0,1,label2,summary_sentences
Two important challenges in reinforcement learning (RL) are the problems of representation learning and of automatic discovery of skills.,1. Introduction,[0],[0]
"Proto-value functions (PVFs) are a well-known solution for the problem of representation learning (Mahadevan, 2005; Mahadevan & Maggioni, 2007); while the problem of skill discovery is generally posed under the options framework (Sutton et al., 1999; Precup, 2000), which models skills as options.
",1. Introduction,[0],[0]
"In this paper, we tie together representation learning and option discovery by showing how PVFs implicitly define options.",1. Introduction,[0],[0]
One of our main contributions is to introduce the concepts of eigenpurpose and eigenbehavior.,1. Introduction,[0],[0]
Eigenpurposes are intrinsic reward functions that incentivize the agent to traverse the state space by following the principal directions of the learned representation.,1. Introduction,[0],[0]
"Each intrinsic reward function leads to a different eigenbehavior, which is
1University of Alberta 2Google DeepMind.",1. Introduction,[0],[0]
"Correspondence to: Marlos C. Machado <machado@ualberta.ca>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
the optimal policy for that reward function.,1. Introduction,[0],[0]
In this paper we introduce an algorithm for option discovery that leverages these ideas.,1. Introduction,[0],[0]
"The options we discover are task-independent because, as PVFs, the eigenpurposes are obtained without any information about the environment’s reward structure.",1. Introduction,[0],[0]
"We first present these ideas in the tabular case and then show how they can be generalized to the function approximation case.
",1. Introduction,[0],[0]
"Exploration, while traditionally a separate problem from option discovery, can also be addressed through the careful construction of options (McGovern & Barto, 2001; Şimşek et al., 2005; Solway et al., 2014; Kulkarni et al., 2016).",1. Introduction,[0],[0]
"In this paper, we provide evidence that not all options capable of accelerating planning are useful for exploration.",1. Introduction,[0],[0]
We show that options traditionally used in the literature to speed up planning hinder the agents’ performance if used for random exploration during learning.,1. Introduction,[0],[0]
"Our options have two important properties that allow them to improve exploration: (i) they operate at different time scales, and (ii) they can be easily sequenced.",1. Introduction,[0],[0]
Having options that operate at different time scales allows agents to make finely timed actions while also decreasing the likelihood the agent will explore only a small portion of the state space.,1. Introduction,[0],[0]
"Moreover, because our options are defined across the whole state space, multiple options are available in every state, which allows them to be easily sequenced.",1. Introduction,[0],[0]
"We generally indicate random variables by capital letters (e.g., R
t ), vectors by bold letters (e.g., ✓), functions by lowercase letters (e.g., v), and sets by calligraphic font (e.g., S).",2. Background,[0],[0]
"In the RL framework (Sutton & Barto, 1998), an agent aims to maximize cumulative reward by taking actions in an environment.",2.1. Reinforcement Learning,[0],[0]
These actions affect the agent’s next state and the rewards it experiences.,2.1. Reinforcement Learning,[0],[0]
We use the MDP formalism throughout this paper.,2.1. Reinforcement Learning,[0],[0]
"An MDP is a 5-tuple hS,A, r, p, i.",2.1. Reinforcement Learning,[0],[0]
"At time t the agent is in state s
t 2 S where it takes action a
t 2 A that leads to the next state s t+1 2 S according to the transition probability kernel p(s0|s, a), which encodes Pr(S t+1 = s 0|S t = s,A t
= a).",2.1. Reinforcement Learning,[0],[0]
The agent also observes a reward R t+1 ⇠,2.1. Reinforcement Learning,[0],[0]
"r(s, a).",2.1. Reinforcement Learning,[0],[0]
"The agent’s goal is to learn a
policy µ : S ⇥",2.1. Reinforcement Learning,[0],[0]
A !,2.1. Reinforcement Learning,[0],[0]
"[0, 1] that maximizes the expected discounted return G
t
.",2.1. Reinforcement Learning,[0],[0]
"= E p,µ ⇥P1 k=0",2.1. Reinforcement Learning,[0],[0]
"k R t+k+1|st ⇤ , where
2 [0, 1) is the discount factor.
",2.1. Reinforcement Learning,[0],[0]
"It is common to use the policy improvement theorem (Bellman, 1957) when learning to maximize G
t .",2.1. Reinforcement Learning,[0],[0]
"One technique is to alternate between solving the Bellman equations for the action-value function q
µk(s, a),
q µk(s, a) .",2.1. Reinforcement Learning,[0],[0]
"= E µk,p
⇥",2.1. Reinforcement Learning,[0],[0]
"G
t |S",2.1. Reinforcement Learning,[0],[0]
"t = s,A t = a
⇤
=
X
s 0 ,r
p(s 0 , r|s, a) ⇥",2.1. Reinforcement Learning,[0],[0]
"r +
X
a
0
µ
k
(a 0|s0)q µk(s 0 , a 0 )
",2.1. Reinforcement Learning,[0],[0]
"⇤
and making the next policy, µ k+1",2.1. Reinforcement Learning,[0],[0]
", greedy w.r.t. qµk ,
µ k+1 .",2.1. Reinforcement Learning,[0],[0]
=,2.1. Reinforcement Learning,[0],[0]
"argmax
a2A q µk(s, a),
until converging to an optimal policy µ⇤.
",2.1. Reinforcement Learning,[0],[0]
Sometimes it is not feasible to learn a value for each stateaction pair due to the size of the state space.,2.1. Reinforcement Learning,[0],[0]
"Generally, this is addressed by parameterizing q
µ (s, a) with a set of weights ✓ 2 Rn such that q
µ (s, a) ⇡ q µ (s, a,✓).",2.1. Reinforcement Learning,[0],[0]
"It is common to approximate q
µ through a linear function, i.e., q
µ (s, a,✓) =",2.1. Reinforcement Learning,[0],[0]
"✓> (s, a), where (s, a) denotes a linear feature representation of state s when taking action a.",2.1. Reinforcement Learning,[0],[0]
The options framework extends RL by introducing temporally extended actions called skills or options.,2.2. The Options Framework,[0],[0]
An option ! is a 3-tuple !,2.2. The Options Framework,[0],[0]
"= hI,⇡, T i where I 2 S denotes the option’s initiation set, ⇡ : A⇥S !",2.2. The Options Framework,[0],[0]
"[0, 1] denotes the option’s policy, and T 2 S denotes the option’s termination set.",2.2. The Options Framework,[0],[0]
After the agent decides to follow option !,2.2. The Options Framework,[0],[0]
"from a state in I, actions are selected according to ⇡ until the agent reaches a state in T .",2.2. The Options Framework,[0],[0]
"Intuitively, options are higher-level actions that extend over several time steps, generalizing MDPs to semiMarkov decision processes (SMDPs) (Puterman, 1994).
",2.2. The Options Framework,[0],[0]
"Traditionally, options capable of moving agents to bottleneck states are sought after.",2.2. The Options Framework,[0],[0]
"Bottleneck states are those states that connect different densely connected regions of the state space (e.g., doorways) (Şimşek & Barto, 2004; Solway et al., 2014).",2.2. The Options Framework,[0],[0]
"They have been shown to be very efficient for planning as these states are the states most frequently visited when considering the shortest distance between any two states in an MDP (Solway et al., 2014).",2.2. The Options Framework,[0],[0]
"Proto-value functions (PVFs) are learned representations that capture large-scale temporal properties of an environment (Mahadevan, 2005; Mahadevan & Maggioni, 2007).",2.3. Proto-Value Functions,[0],[0]
"They are obtained by diagonalizing a diffusion model, which is constructed from the MDP’s transition matrix.",2.3. Proto-Value Functions,[0],[0]
"A diffusion model captures information flow on a graph, and
it is commonly defined by the combinatorial graph Laplacian matrix L = D A, where A is the graph’s adjacency matrix and D the diagonal matrix whose entries are the row sums of A. Notice that the adjacency matrix A easily generalizes to a weight matrix W .",2.3. Proto-Value Functions,[0],[0]
"PVFs are defined to be the eigenvectors obtained after the eigendecomposition of L. Different diffusion models can be used to generate PVFs, such as the normalized graph Laplacian L = D 12 (D A)D 12 , which we use in this paper.",2.3. Proto-Value Functions,[0],[0]
"PVFs capture the large-scale geometry of the environment, such as symmetries and bottlenecks.",3. Option Discovery through the Laplacian,[0],[0]
"They are task independent, in the sense that they do not use information related to reward functions.",3. Option Discovery through the Laplacian,[0],[0]
"Moreover, they are defined over the whole state space since each eigenvector induces a realvalued mapping over each state.",3. Option Discovery through the Laplacian,[0],[0]
We can imagine that options with these properties should also be useful.,3. Option Discovery through the Laplacian,[0],[0]
"In this section we show how to use PVFs to discover options.
",3. Option Discovery through the Laplacian,[0],[0]
Let us start with an example.,3. Option Discovery through the Laplacian,[0],[0]
Consider the traditional 4- room domain depicted in Figure 1c.,3. Option Discovery through the Laplacian,[0],[0]
Gray squares represent walls and white squares represent accessible states.,3. Option Discovery through the Laplacian,[0],[0]
"Four actions are available: up, down, right, and left.",3. Option Discovery through the Laplacian,[0],[0]
The transitions are deterministic and the agent is not allowed to move into a wall.,3. Option Discovery through the Laplacian,[0],[0]
"Ideally, we would like to discover options that move the agent from room to room.",3. Option Discovery through the Laplacian,[0],[0]
"Thus, we should be able to automatically distinguish between the different rooms in the environment.",3. Option Discovery through the Laplacian,[0],[0]
"This is exactly what PVFs do, as depicted in Figure 2 (left).",3. Option Discovery through the Laplacian,[0],[0]
"Instead of interpreting a PVF as a basis function, we can interpret the PVF in our example as a desire to reach the highest point of the plot, corresponding to the centre of the room.",3. Option Discovery through the Laplacian,[0],[0]
"Because the sign of an eigenvector is arbitrary, a PVF can also be interpreted as a desire to reach the lowest point of the plot, corresponding to the opposite room.",3. Option Discovery through the Laplacian,[0],[0]
"In this paper we use the eigenvectors in both directions (i.e., both signs).
",3. Option Discovery through the Laplacian,[0],[0]
An eigenpurpose formalizes the interpretation above by defining an intrinsic reward function.,3. Option Discovery through the Laplacian,[0],[0]
"We can see it as defining a purpose for the agent, that is, to maximize the discounted sum of these rewards.
",3. Option Discovery through the Laplacian,[0],[0]
Definition 3.1 (Eigenpurpose).,3. Option Discovery through the Laplacian,[0],[0]
"An eigenpurpose is the intrinsic reward function re
i
(s, s 0 ) of a proto-value function
e 2 R|S| such that
r e i (s, s 0 )",3. Option Discovery through the Laplacian,[0],[0]
"= e > ( (s0) (s)), (1)
where (x) denotes the feature representation of state x.
Notice that an eigenpurpose, in the tabular case, can be written as re
i
(s, s 0 )",3. Option Discovery through the Laplacian,[0],[0]
"= e[s 0 ] e[s].
",3. Option Discovery through the Laplacian,[0],[0]
"We can now define a new MDP to learn the option associated with the purpose, Me
i = hS,A[{?}, re i , p, i, where
the reward function is defined as in (1) and the action set is augmented by the action terminate (?), which allows the agent to leave Me
i without any cost.",3. Option Discovery through the Laplacian,[0],[0]
The state space and the transition probability kernel remain unchanged from the original problem.,3. Option Discovery through the Laplacian,[0],[0]
"The discount rate can be chosen arbitrarily, although it impacts the timescale the option encodes.
",3. Option Discovery through the Laplacian,[0],[0]
"With Me i we define a new state-value function ve ⇡ (s), for policy ⇡, as the expected value of the cumulative discounted intrinsic reward if the agent starts in state s and follows policy ⇡ until termination.",3. Option Discovery through the Laplacian,[0],[0]
"Similarly, we define a new action-value function qe
⇡ (s, a) as the expected value of the cumulative discounted intrinsic reward if the agent starts in state s, takes action a, and then follows policy ⇡ until termination.",3. Option Discovery through the Laplacian,[0],[0]
"We can also describe the optimal value function for any eigenpurpose obtained through e:
v e ⇤(s) = max
⇡
v e ⇡ (s) and qe⇤(s, a) = max ⇡ q e ⇡ (s, a).
",3. Option Discovery through the Laplacian,[0],[0]
"These definitions naturally lead us to eigenbehaviors.
",3. Option Discovery through the Laplacian,[0],[0]
Definition 3.2 (Eigenbehavior).,3. Option Discovery through the Laplacian,[0],[0]
An eigenbehavior is a policy e : S !,3. Option Discovery through the Laplacian,[0],[0]
"A that is optimal with respect to the eigenpurpose re
i
, i.e., e(s) = argmax a2A",3. Option Discovery through the Laplacian,[0],[0]
"q e ⇤(s, a).
",3. Option Discovery through the Laplacian,[0],[0]
"Finding the optimal policy ⇡e⇤ now becomes a traditional RL problem, with a different reward function.",3. Option Discovery through the Laplacian,[0],[0]
"Importantly, this reward function tends to be dense, avoiding challenging situations due to exploration issues.",3. Option Discovery through the Laplacian,[0],[0]
"In this paper we use policy iteration to solve for an optimal policy.
",3. Option Discovery through the Laplacian,[0],[0]
"If each eigenpurpose defines an option, its corresponding eigenbehavior is the option’s policy.",3. Option Discovery through the Laplacian,[0],[0]
"Thus, we need to define the option’s initiation and termination set.",3. Option Discovery through the Laplacian,[0],[0]
"An option should be available in every state where it is possible to achieve its purpose, and to terminate when it is achieved.
",3. Option Discovery through the Laplacian,[0],[0]
"When defining the MDP to learn the option, we augmented the agent’s action set with the terminate action, allowing the agent to interrupt the option anytime.",3. Option Discovery through the Laplacian,[0],[0]
"We want options to terminate when the agent achieves its purpose, i.e., when it is unable to accumulate further positive intrinsic rewards.",3. Option Discovery through the Laplacian,[0],[0]
"With the defined reward function, this happens when the agent reaches the state with largest value in the eigenpurpose (or a local maximum when < 1).",3. Option Discovery through the Laplacian,[0],[0]
Any subsequent reward will be negative.,3. Option Discovery through the Laplacian,[0],[0]
"We are able to formalize this con-
dition by defining q",3. Option Discovery through the Laplacian,[0],[0]
"(s,?)",3. Option Discovery through the Laplacian,[0],[0]
.= 0,3. Option Discovery through the Laplacian,[0],[0]
"for all e. When the terminate action is selected, control is returned to the higher level policy (Dietterich, 2000).",3. Option Discovery through the Laplacian,[0],[0]
"An option following a policy e terminates when qe
(s, a)  0 for all a 2 A.",3. Option Discovery through the Laplacian,[0],[0]
"We define the initiation set to be all states in which there exists an action a 2 A such that qe
(s, a) > 0.",3. Option Discovery through the Laplacian,[0],[0]
"Thus, the option’s policy is ⇡e(s) =",3. Option Discovery through the Laplacian,[0],[0]
"argmax
a2A",3. Option Discovery through the Laplacian,[0],[0]
"[{?} q e ⇡ (s, a).",3. Option Discovery through the Laplacian,[0],[0]
We refer to the options discovered with our approach as eigenoptions.,3. Option Discovery through the Laplacian,[0],[0]
"The eigenoption corresponding to the example at the beginning of this section is depicted in Figure 2 (right).
",3. Option Discovery through the Laplacian,[0],[0]
"For any eigenoption, there is always at least one state in which it terminates, as we now show.",3. Option Discovery through the Laplacian,[0],[0]
Theorem 3.1 (Option’s Termination).,3. Option Discovery through the Laplacian,[0],[0]
"Consider an eigenoption o = hI
o
,⇡
o , T o",3. Option Discovery through the Laplacian,[0],[0]
i and < 1.,3. Option Discovery through the Laplacian,[0],[0]
"Then, in an MDP with finite state space, T
o
is nonempty.
",3. Option Discovery through the Laplacian,[0],[0]
Proof.,3. Option Discovery through the Laplacian,[0],[0]
"We can write the Bellman equation in the matrix form: v = r+ Tv, where v is a finite column vector with one entry per state encoding its value function.",3. Option Discovery through the Laplacian,[0],[0]
"From (1) we have r = Tw w with w = (s)>e, where e denotes the eigenpurpose of interest.",3. Option Discovery through the Laplacian,[0],[0]
"Therefore:
v +w = Tw + Tv
= (1 )Tw + T (v +w) = (1 )(I T ) 1Tw.
",3. Option Discovery through the Laplacian,[0],[0]
||v +w||1 = (1 )||(I,3. Option Discovery through the Laplacian,[0],[0]
T ),3. Option Discovery through the Laplacian,[0],[0]
1Tw||1 ||v +w||1  (1 )||(I,3. Option Discovery through the Laplacian,[0],[0]
"T ) 1T ||1||w||1
||v +w||1  (1 ) 1 (1 ) ||w||1
||v +w||1  ||w||1
We can shift w by any finite constant without changing the reward, i.e., Tw w = T (w+ ) (w+ )",3. Option Discovery through the Laplacian,[0],[0]
"because T1 = 1 since P j T i,j
= 1.",3. Option Discovery through the Laplacian,[0],[0]
"Hence, we can assume w 0.",3. Option Discovery through the Laplacian,[0],[0]
"Let s ⇤ = argmax
s
w
s ⇤ , so that w s ⇤ = ||w||1.",3. Option Discovery through the Laplacian,[0],[0]
"Clearly vs⇤ 
0, otherwise ||v +w||1 |vs⇤ +",3. Option Discovery through the Laplacian,[0],[0]
ws⇤ | = vs⇤ +,3. Option Discovery through the Laplacian,[0],[0]
"ws⇤ > w
s ⇤ = ||w||1, arriving at a contradiction.
",3. Option Discovery through the Laplacian,[0],[0]
This result is applicable in both the tabular and linear function approximation case.,3. Option Discovery through the Laplacian,[0],[0]
An algorithm that does not rely on knowing the underlying graph is provided in Section 5.,3. Option Discovery through the Laplacian,[0],[0]
"We used three MDPs in our empirical study (c.f. Figure 1): an open room, an I-Maze, and the 4-room domain.",4. Empirical Evaluation,[0],[0]
Their transitions are deterministic and gray squares denote walls.,4. Empirical Evaluation,[0],[0]
"Agents have access to four actions: up, down, right, and left.",4. Empirical Evaluation,[0],[0]
"When an action that would have taken the agent into a wall is chosen, the agent’s state does not change.",4. Empirical Evaluation,[0],[0]
"We demonstrate three aspects of our framework:1
• How the eigenoptions present specific purposes.",4. Empirical Evaluation,[0],[0]
"Interestingly, options leading to bottlenecks are not the first ones we discover.
",4. Empirical Evaluation,[0],[0]
"• How eigenoptions improve exploration by reducing the expected number of steps required to navigate between any two states.
",4. Empirical Evaluation,[0],[0]
• How eigenoptions help agents to accumulate reward faster.,4. Empirical Evaluation,[0],[0]
We show how few options may hurt the agents’ performance while enough options speed up learning.,4. Empirical Evaluation,[0],[0]
"In the PVF theory, the “smoothest” eigenvectors, corresponding to the smallest eigenvalues, are preferred (Mahadevan & Maggioni, 2007).",4.1. Discovered Options,[0],[0]
"The same intuition applies to eigenoptions, with the eigenpurposes corresponding to the smallest eigenvalues being preferred.",4.1. Discovered Options,[0],[0]
"Figures 3, 4, and 5 depict the first eigenoptions discovered in the three domains used for evaluation.
",4.1. Discovered Options,[0],[0]
"Eigenoptions do not necessarily look for bottleneck states, 1Python code can be found at: https://github.com/mcmachado/options
allowing us to apply our algorithm in many environments in which there are no obvious, or meaningful, bottlenecks.",4.1. Discovered Options,[0],[0]
"We discover meaningful options in these environments, such as walking down a corridor, or going to the corners of an open room.",4.1. Discovered Options,[0],[0]
"Interestingly, doorways are not the first options we discover in the 4-room domain (the fifth eigenoption is the first to terminate at the entrance of a doorway).",4.1. Discovered Options,[0],[0]
"In the next sections we provide empirical evidence that eigenoptions are useful, and often more so than bottleneck options.",4.1. Discovered Options,[0],[0]
"A major challenge for agents to explore an environment is to be decisive, avoiding the dithering commonly observed in random walks (Machado & Bowling, 2016; Osband et al., 2016).",4.2. Exploration,[0],[0]
Options provide such decisiveness by operating in a higher level of abstraction.,4.2. Exploration,[0],[0]
"Agents performing a random walk, when equipped with options, are expected to cover larger distances in the state space, navigating back and forth between subgoals instead of dithering around the starting state.",4.2. Exploration,[0],[0]
"However, options need to satisfy two conditions to improve exploration: (1) they have to be available in several parts of the state space, ensuring the agent always has access to many different options; and (2) they have to operate at different time scales.",4.2. Exploration,[0],[0]
"For instance, in the 4-room domain, it is unlikely an agent randomly selects enough primitive actions leading it to a corner if all options move the agent between doorways.",4.2. Exploration,[0],[0]
"An important result in this section is to show that it is very unlikely for an agent to explore the whole environment if it keeps going back and forth between similar high-level goals.
",4.2. Exploration,[0],[0]
Eigenoptions satisfy both conditions.,4.2. Exploration,[0],[0]
"As demonstrated in Section 4.1, eigenoptions are often defined in the whole state space, allowing sequencing.",4.2. Exploration,[0],[0]
"Moreover, PVFs can be seen as a “frequency” basis, with different PVFs being associated with different frequencies (Mahadevan & Maggioni, 2007).",4.2. Exploration,[0],[0]
"The corresponding eigenoptions also operate
at different frequencies, with the length of a trajectory until termination varying.",4.2. Exploration,[0],[0]
This behavior can be seen when comparing the second and fourth eigenoptions in the 10 ⇥ 10 grid (Figure 3).,4.2. Exploration,[0],[0]
"The fourth eigenoption terminates, on expectation, twice as often as the second eigenoption.
",4.2. Exploration,[0],[0]
In this section we show that eigenoptions improve exploration.,4.2. Exploration,[0],[0]
"We do so by introducing a new metric, which we call diffusion time.",4.2. Exploration,[0],[0]
Diffusion time encodes the expected number of steps required to navigate between two states randomly chosen in the MDP while following a random walk.,4.2. Exploration,[0],[0]
A small expected number of steps implies that it is more likely that the agent will reach all states with a random walk.,4.2. Exploration,[0],[0]
"We discuss how this metric can be computed in the Appendix.
Figure 6 depicts, for our the three environments, the diffusion time with options and the diffusion time using only primitive actions.",4.2. Exploration,[0],[0]
"We add options incrementally in order of increasing eigenvalue when computing the diffusion time for different sets of options.
",4.2. Exploration,[0],[0]
"The first options added hurt exploration, but when enough options are added, exploration is greatly improved when compared to a random walk using only primitive actions.",4.2. Exploration,[0],[0]
"The fact that few options hurt exploration may be surprising at first, based on the fact that few useful options are generally sought after in the literature.",4.2. Exploration,[0],[0]
"However, this is a major difference between using options for planning and for learning.",4.2. Exploration,[0],[0]
"In planning, options shortcut the agents’ trajectories, pruning the search space.",4.2. Exploration,[0],[0]
All other actions are still taken into consideration.,4.2. Exploration,[0],[0]
"When exploring, a uniformly random policy over options and primitive actions skews where
agents spend their time.",4.2. Exploration,[0],[0]
"Options that are much longer than primitive actions reduce the likelihood that an agent will deviate much from the options’ trajectories, since sampling an option may undo dozens of primitive actions.",4.2. Exploration,[0],[0]
"This biasing is often observed when fewer options are available.
",4.2. Exploration,[0],[0]
The discussion above can be made clearer with an example.,4.2. Exploration,[0],[0]
"In the 4-room domain, if the only options available are those leading the agent to doorways (c.f. Appendix), it is less likely the agent will reach the outer corners.",4.2. Exploration,[0],[0]
To do so the agent would have to select enough consecutive primitive actions without sampling an option.,4.2. Exploration,[0],[0]
"Also, it is very likely agents will be always moving between rooms, never really exploring inside a room.",4.2. Exploration,[0],[0]
These issues are mitigated with eigenoptions.,4.2. Exploration,[0],[0]
"The first eigenoptions lead agents to individual rooms, but other eigenoptions operate in different time scales, allowing agents to explore different parts of rooms.
",4.2. Exploration,[0],[0]
"Figure 6d supports the intuition that options leading to bottleneck states are not sufficient, by themselves, for exploration.",4.2. Exploration,[0],[0]
It shows how the diffusion time in the 4-room domain is increased when only bottleneck options are used.,4.2. Exploration,[0],[0]
"As in the PVF literature, the ideal number of options to be used by an agent can be seen as a model selection problem.",4.2. Exploration,[0],[0]
We now illustrate the usefulness of our options when the agent’s goal is to accumulate reward.,4.3. Accumulating Rewards,[0],[0]
We also study the impact of an increasing number of options in such a task.,4.3. Accumulating Rewards,[0],[0]
"In these experiments, the agent starts at the bottom left cor-
ner and its goal is to reach the top right corner.",4.3. Accumulating Rewards,[0],[0]
"The agent observes a reward of 0 until the goal is reached, when it observes a reward of +1.",4.3. Accumulating Rewards,[0],[0]
"We used Q-Learning (Watkins & Dayan, 1992) (↵ = 0.1, = 0.9) to learn a policy over primitive actions.",4.3. Accumulating Rewards,[0],[0]
"The behavior policy chooses uniformly over primitive actions and options, following them until termination.",4.3. Accumulating Rewards,[0],[0]
"Figure 7 depicts, after learning for a given number of episodes, the average over 100 trials of the agents’ final performance.",4.3. Accumulating Rewards,[0],[0]
"Episodes were 100 time steps long, and we learned for 250 episodes in the 10 ⇥ 10 grid and in the I-Maze, and for 500 episodes in the 4-room domain.
",4.3. Accumulating Rewards,[0],[0]
In most scenarios eigenoptions improve performance.,4.3. Accumulating Rewards,[0],[0]
"As in the previous section, exceptions occur when only a few options are added to the agent’s action set.",4.3. Accumulating Rewards,[0],[0]
The best results were obtained using 64 options.,4.3. Accumulating Rewards,[0],[0]
"Despite being an additional parameter, our results show that the agent’s performance is fairly robust across different numbers of options.
",4.3. Accumulating Rewards,[0],[0]
Eigenoptions are task-independent by construction.,4.3. Accumulating Rewards,[0],[0]
Additional results in the appendix show how the same set of eigenoptions is able to speed-up learning in different tasks.,4.3. Accumulating Rewards,[0],[0]
"In the appendix we also compare eigenoptions to random options, that is, options that use a random state as subgoal.",4.3. Accumulating Rewards,[0],[0]
So far we have assumed that agents have access to the adjacency matrix representing the underlying MDP.,5. Approximate Option Discovery,[0],[0]
"However, in practical settings this is generally not true.",5. Approximate Option Discovery,[0],[0]
"In fact, the number of states in these settings is often so large that agents rarely visit the same state twice.",5. Approximate Option Discovery,[0],[0]
"These problems are generally tackled with sample-based methods and some sort of function approximation.
",5. Approximate Option Discovery,[0],[0]
In this section we propose a sample-based approach for option discovery that asymptotically discovers eigenoptions.,5. Approximate Option Discovery,[0],[0]
We then extend this algorithm to linear function approximation.,5. Approximate Option Discovery,[0],[0]
We provide anecdotal evidence in Atari 2600 games that this relatively naı̈ve sample-based approach to function approximation discovers purposeful options.,5. Approximate Option Discovery,[0],[0]
"In the online setting, agents must sample trajectories.",5.1. Sample-based Option Discovery,[0],[0]
"Naturally, one can sample trajectories until one is able to perfectly construct the MDP’s adjacency matrix, as suggested by Mahadevan & Maggioni (2007).",5.1. Sample-based Option Discovery,[0],[0]
"However, this approach does not easily extend to linear function approximation.",5.1. Sample-based Option Discovery,[0],[0]
"In this section we provide an approach that does not build the adjacency matrix allowing us to extend the concept of eigenpurposes to linear function approximation.
",5.1. Sample-based Option Discovery,[0],[0]
"In our algorithm, a sample transition is added to a matrix T if it was not previously encountered.",5.1. Sample-based Option Discovery,[0],[0]
"The transition is added as the difference between the current and previous observations, i.e., (s0) (s).",5.1. Sample-based Option Discovery,[0],[0]
"In the tabular case we define (s) to be the one-hot encoding of state s. Once enough transitions have been sampled, we perform a singular value decomposition on the matrix T such that T = U⌃V
>.",5.1. Sample-based Option Discovery,[0],[0]
"We use the columns of V , which correspond to the right-eigenvectors of T , to generate the eigenpurposes.",5.1. Sample-based Option Discovery,[0],[0]
"The intrinsic reward and the termination criterion for an eigenbehavior are the same as before.
",5.1. Sample-based Option Discovery,[0],[0]
Matrix T is known as the incidence matrix.,5.1. Sample-based Option Discovery,[0],[0]
"If all transitions in the graph are sampled once, for tabular representations, this algorithm discovers the same options we obtain with the combinatorial Laplacian.",5.1. Sample-based Option Discovery,[0],[0]
"The theorem below states the equivalence between the obtained eigenpurposes.
",5.1. Sample-based Option Discovery,[0],[0]
Theorem 5.1.,5.1. Sample-based Option Discovery,[0],[0]
"Consider the SVD of T = U T ⌃ T V > T , with each row of T consisting of the difference between observations, i.e., (s0) (s).",5.1. Sample-based Option Discovery,[0],[0]
"In the tabular case, if all transitions in the MDP have been sampled once, the orthonormal eigenvectors of L are the columns of V >
T
.
Proof.",5.1. Sample-based Option Discovery,[0],[0]
"Given the SVD decomposition of a matrix A = U⌃V
>, the columns of V are the eigenvectors of A > A (Strang, 2005).",5.1. Sample-based Option Discovery,[0],[0]
"We know that T>T = 2L, where L = D W (Lemma 5.1, c.f. Appendix).",5.1. Sample-based Option Discovery,[0],[0]
"Thus, the columns of V
T are the eigenvectors of T>T , which can be rewritten as 2(D W ).",5.1. Sample-based Option Discovery,[0],[0]
"Therefore, the columns of V
T are also the eigenvectors of L.
There is a trade-off between reconstructing the adjacency matrix and constructing the incidence matrix.",5.1. Sample-based Option Discovery,[0],[0]
"In MDPs in which states are sparsely connected, such as the I-Maze, the latter is preferred since it has fewer transitions than states.",5.1. Sample-based Option Discovery,[0],[0]
"However, what makes this result interesting is the fact that our algorithm can be easily generalized to linear function approximation.",5.1. Sample-based Option Discovery,[0],[0]
An adjacency matrix is not very useful when the agent has access only to features of the state.,5.2. Function Approximation,[0],[0]
"However, we can use the intuition about the incidence matrix to propose an algorithm compatible with linear function approximation.
",5.2. Function Approximation,[0],[0]
"In fact, to apply the algorithm proposed in the previous section, we just need to define what constitutes a new transition.",5.2. Function Approximation,[0],[0]
"We define two vectors, t and t0, to be identical if and only if t t0 = 0.",5.2. Function Approximation,[0],[0]
We then use a set data structure to avoid duplicates when storing (s0) (s).,5.2. Function Approximation,[0],[0]
"This is a naı̈ve approach, but it provides encouraging evidence eigenoptions generalize to linear function approximation.",5.2. Function Approximation,[0],[0]
"We expect more involved methods to perform even better.
",5.2. Function Approximation,[0],[0]
"We tested our method in the ALE (Bellemare et al., 2013).",5.2. Function Approximation,[0],[0]
"The agent’s representation consists of the emulator’s RAM state (1,024 bits).",5.2. Function Approximation,[0],[0]
"The final incidence matrix in which we ran the SVD had 25,000 rows, which we sampled uniformly from the set of observed transitions.",5.2. Function Approximation,[0],[0]
"We provide further details of the experimental setup in the appendix.
",5.2. Function Approximation,[0],[0]
"In the tabular case we start selecting eigenpurposes generated by the eigenvectors with smallest eigenvalue, because these are the “smoothest” ones.",5.2. Function Approximation,[0],[0]
"However, it is not clear such intuition holds here because we are in the function approximation setting and the matrix of transitions does not contain all possible transitions.",5.2. Function Approximation,[0],[0]
"Therefore, we analyzed, for each game, all 1,024 discovered options.
",5.2. Function Approximation,[0],[0]
We approximate these options greedily ( = 0) with the ALE emulator’s look-ahead.,5.2. Function Approximation,[0],[0]
"The next action a0 for an eigenpurpose e is selected as argmax b2A R s 0 p(s 0|s, b) re i (s, s 0 ).
",5.2. Function Approximation,[0],[0]
"Even with such a myopic action selection mechanism we
were able to obtain options that clearly demonstrate intent.",5.2. Function Approximation,[0],[0]
"In FREEWAY, a game in which a chicken is expected to cross the road while avoiding cars, we observe options in which the agent clearly wants to reach a specific lane in the street.",5.2. Function Approximation,[0],[0]
Figure 8 (left) depicts where the chicken tends to be when the option is executed.,5.2. Function Approximation,[0],[0]
On the right we see a histogram representing the chicken’s height during an episode.,5.2. Function Approximation,[0],[0]
"We can clearly see how the chicken’s height varies for different options, and how a random walk over primitive actions (rand) does not explore the environment properly.",5.2. Function Approximation,[0],[0]
"Remarkably, option #445 scores 28 points at the end of the episode, without ever explicitly taking the reward signal into consideration.",5.2. Function Approximation,[0],[0]
"This performance is very close to those obtained by state-of-the-art algorithms.
",5.2. Function Approximation,[0],[0]
"In MONTEZUMA’S REVENGE, a game in which the agent needs to navigate through a room to pickup a key so it can open a door, we also observe the agent having the clear intent of reaching particular positions on the screen, such as staircases, ropes and doors (Figure 9).",5.2. Function Approximation,[0],[0]
"Interestingly, the options we discover are very similar to those handcrafted by Kulkarni et al. (2016) when evaluating the usefulness of options to tackle such a game.",5.2. Function Approximation,[0],[0]
A video of the highlighted options can be found online.2,5.2. Function Approximation,[0],[0]
Most algorithms for option discovery can be seen as topdown approaches.,6. Related Work,[0],[0]
"Agents use trajectories leading to informative rewards3 as a starting point, decomposing and refining them into options.",6. Related Work,[0],[0]
"There are many approaches based on this principle, such as methods that use the observed rewards to generate intrinsic rewards leading to new value functions (e.g., McGovern & Barto, 2001; Menache et al., 2002; Konidaris & Barto, 2009), methods that use the observed rewards to climb a gradient (e.g., Mankowitz et al., 2016; Vezhnevets et al., 2016; Bacon et al., 2017), or to do
2 https://youtu.be/2BVicx4CDWA
3We define an informative reward to be the signal that informs the agent it has reached a goal.",6. Related Work,[0],[0]
"For example, when trying to escape from a maze, we consider 0 to be an informative reward if the agent observes rewards of value 1 in every time step it is inside the maze.",6. Related Work,[0],[0]
"A different example is a positive reward observed by an agent that typically observes rewards of value 0.
",6. Related Work,[0],[0]
"probabilistic inference (Daniel et al., 2016).",6. Related Work,[0],[0]
"However, such approaches are not applicable in large state spaces with sparse rewards.",6. Related Work,[0],[0]
"If informative rewards are unlikely to be found by an agent using only primitive actions, requiring long or specific sequences of actions, options are equally unlikely to be discovered.
",6. Related Work,[0],[0]
"Our algorithm can be seen as a bottom-up approach, in which options are constructed before the agent observes any informative reward.",6. Related Work,[0],[0]
These options are composed to generate the desired policy.,6. Related Work,[0],[0]
"Options discovered this way tend to be independent of an agent’s intention, and are potentially useful in many different tasks (Gregor et al., 2016).",6. Related Work,[0],[0]
"Such options can also be seen as being useful for exploration by allowing agents to commit to a behavior for an extended period of time (Machado & Bowling, 2016).",6. Related Work,[0],[0]
"Among the approaches to discover options without using extrinsic rewards are the use of global or local graph centrality measures (Şimşek & Barto, 2004; Şimşek et al., 2005; Şimşek & Barto, 2008) and clustering of states (Mannor et al., 2004; Bacon, 2013; Lakshminarayanan et al., 2016).",6. Related Work,[0],[0]
"Interestingly, Şimşek et al. (2005) and Lakshminarayanan et al. (2016) also use the graph Laplacian in their algorithm, but to identify bottleneck states.
",6. Related Work,[0],[0]
Baranes & Oudeyer (2013) and Moulin-Frier & Oudeyer (2013) show how one can build policies to explicitly assist agents to explore the environment.,6. Related Work,[0],[0]
The proposed algorithms self-generate subgoals in order to maximize learning progress.,6. Related Work,[0],[0]
The policies built can be seen as options.,6. Related Work,[0],[0]
"Recently, Solway et al. (2014) proved that “optimal hierarchy minimizes the geometric mean number of trial-and-error attempts necessary for the agent to discover the optimal policy for any selected task (...)”.",6. Related Work,[0],[0]
"Our experiments confirm this result, although we propose diffusion time as a different metric to evaluate how options improve exploration.
",6. Related Work,[0],[0]
The idea of discovering options by learning to control parts of the environment is also related to our work.,6. Related Work,[0],[0]
"Eigenpurposes encode different rates of change in the agents representation of the world, while the corresponding options aim at maximizing such change.",6. Related Work,[0],[0]
Others have also proposed ways to discover options based on the idea of learning to control the environment.,6. Related Work,[0],[0]
"Hengst (2002), for instance, proposes an algorithm that explicitly models changes in the variables that form the agent’s representation.",6. Related Work,[0],[0]
"Recently, Gregor et al. (2016) proposed an algorithm in which agents discover options by maximizing a notion of empowerment (Salge et al., 2014), where the agent aims at getting to states with a maximal set of available intrinsic options.
",6. Related Work,[0],[0]
"Continual Curiosity driven Skill Acquisition (CCSA) (Kompella et al., In Press) is the closest approach to ours.",6. Related Work,[0],[0]
CCSA also discovers skills that maximize an intrinsic reward obtained by some extracted representation.,6. Related Work,[0],[0]
"While we use PVFs, CCSA uses Incremental Slow Feature Analysis
(SFA) (Kompella et al., 2011) to define the intrinsic reward function.",6. Related Work,[0],[0]
"Sprekeler (2011) has shown that, given a specific choice of adjacency function, PVFs are equivalent to SFA (Wiskott & Sejnowski, 2002).",6. Related Work,[0],[0]
SFA becomes an approximation of PVFs if the function space used in the SFA does not allow arbitrary mappings from the observed data to an embedding.,6. Related Work,[0],[0]
"Our method differs in how we define the initiation and termination sets, as well as in the objective being maximized.",6. Related Work,[0],[0]
"CCSA acquires skills that produce a large variation in the slow-feature outputs, leading to options that seek for bottlenecks.",6. Related Work,[0],[0]
"Our approach does not seek for bottlenecks, focusing on traversing different directions of the learned representation.",6. Related Work,[0],[0]
"Being able to properly abstract MDPs into SMDPs can reduce the overall expense of learning (Sutton et al., 1999; Solway et al., 2014), mainly when the learned options are reused in multiple tasks.",7. Conclusion,[0],[0]
"On the other hand, the wrong hierarchy can hinder the agents’ learning process, moving the agent away from desired goal states.",7. Conclusion,[0],[0]
"Current algorithms for option discovery often depend on an initial informative reward signal, which may not be readily available in large MDPs.",7. Conclusion,[0],[0]
"In this paper, we introduced an approach that is effective in different environments, for a multitude of tasks.
",7. Conclusion,[0],[0]
"Our algorithm uses the graph Laplacian, being directly related to the concept of proto-value functions.",7. Conclusion,[0],[0]
The learned representation informs the agent what are meaningful options to be sought after.,7. Conclusion,[0],[0]
The discovered options can be seen as traversing each one of the dimensions in the learned representation.,7. Conclusion,[0],[0]
We believe successful algorithms in the future will be able to simultaneously discover representations and options.,7. Conclusion,[0],[0]
"Agents will use their learned representation to discover options, which will be used to further explore the environment, improving the agent’s representation.
",7. Conclusion,[0],[0]
"Interestingly, the options first discovered by our approach do not necessarily find bottlenecks, which are commonly sought after.",7. Conclusion,[0],[0]
"In this paper we showed how bottleneck options can hinder exploration strategies if naively added to the agent’s action set, and how the options we discover can help an agent to explore.",7. Conclusion,[0],[0]
"Also, we have shown how the discovered options can be used to accumulate reward in a multitude of tasks, leveraging their exploratory properties.
",7. Conclusion,[0],[0]
There are several exciting avenues for future work.,7. Conclusion,[0],[0]
"As noted, SFA can be seen as an approximation to PVFs.",7. Conclusion,[0],[0]
It would be interesting to compare such an approach to eigenoptions.,7. Conclusion,[0],[0]
It would also be interesting to see if the options we discover can be generated incrementally and with incomplete graphs.,7. Conclusion,[0],[0]
"Finally, one can also imagine extensions to the proposed algorithm where a hierarchy of options is built.",7. Conclusion,[0],[0]
"The authors would like to thank Will Dabney, Rémi Munos and Csaba Szepesvári for useful discussions.",Acknowledgements,[0],[0]
This work was supported by grants from Alberta Innovates Technology Futures and the Alberta Machine Intelligence Institute (Amii).,Acknowledgements,[0],[0]
Computing resources were provided by Compute Canada through CalculQuébec.,Acknowledgements,[0],[0]
Representation learning and option discovery are two of the biggest challenges in reinforcement learning (RL).,abstractText,[0],[0]
Proto-value functions (PVFs) are a well-known approach for representation learning in MDPs.,abstractText,[0],[0]
In this paper we address the option discovery problem by showing how PVFs implicitly define options.,abstractText,[0],[0]
"We do it by introducing eigenpurposes, intrinsic reward functions derived from the learned representations.",abstractText,[0],[0]
The options discovered from eigenpurposes traverse the principal directions of the state space.,abstractText,[0],[0]
They are useful for multiple tasks because they are discovered without taking the environment’s rewards into consideration.,abstractText,[0],[0]
"Moreover, different options act at different time scales, making them helpful for exploration.",abstractText,[0],[0]
We demonstrate features of eigenpurposes in traditional tabular domains as well as in Atari 2600 games.,abstractText,[0],[0]
A Laplacian Framework for Option Discovery in Reinforcement Learning,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632–642, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
"The semantic concepts of entailment and contradiction are central to all aspects of natural language meaning (Katz, 1972; van Benthem, 2008), from the lexicon to the content of entire texts.",1 Introduction,[0],[0]
"Thus, natural language inference (NLI) — characterizing and using these relations in computational systems (Fyodorov et al., 2000; Condoravdi et al., 2003; Bos and Markert, 2005; Dagan et al., 2006; MacCartney and Manning, 2009) — is essential in tasks ranging from information retrieval to semantic parsing to commonsense reasoning.
",1 Introduction,[0],[0]
"NLI has been addressed using a variety of techniques, including those based on symbolic logic, knowledge bases, and neural networks.",1 Introduction,[0],[0]
"In recent years, it has become an important testing ground
for approaches employing distributed word and phrase representations.",1 Introduction,[0],[0]
"Distributed representations excel at capturing relations based in similarity, and have proven effective at modeling simple dimensions of meaning like evaluative sentiment (e.g., Socher et al. 2013), but it is less clear that they can be trained to support the full range of logical and commonsense inferences required for NLI (Bowman et al., 2015; Weston et al., 2015b; Weston et al., 2015a).",1 Introduction,[0],[0]
"In a SemEval 2014 task aimed at evaluating distributed representations for NLI, the best-performing systems relied heavily on additional features and reasoning capabilities (Marelli et al., 2014a).
",1 Introduction,[0],[0]
"Our ultimate objective is to provide an empirical evaluation of learning-centered approaches to NLI, advancing the case for NLI as a tool for the evaluation of domain-general approaches to semantic representation.",1 Introduction,[0],[0]
"However, in our view, existing NLI corpora do not permit such an assessment.",1 Introduction,[0],[0]
"They are generally too small for training modern data-intensive, wide-coverage models, many contain sentences that were algorithmically generated, and they are often beset with indeterminacies of event and entity coreference that significantly impact annotation quality.
",1 Introduction,[0],[0]
"To address this, this paper introduces the Stanford Natural Language Inference (SNLI) corpus, a collection of sentence pairs labeled for entailment, contradiction, and semantic independence.",1 Introduction,[0],[0]
"At 570,152 sentence pairs, SNLI is two orders of magnitude larger than all other resources of its type.",1 Introduction,[0],[0]
"And, in contrast to many such resources, all of its sentences and labels were written by humans in a grounded, naturalistic context.",1 Introduction,[0],[0]
"In a separate validation phase, we collected four additional judgments for each label for 56,941 of the examples.",1 Introduction,[0],[0]
"Of these, 98% of cases emerge with a threeannotator consensus, and 58% see a unanimous consensus from all five annotators.
",1 Introduction,[0],[0]
"In this paper, we use this corpus to evaluate
632
a variety of models for natural language inference, including rule-based systems, simple linear classifiers, and neural network-based models.",1 Introduction,[0],[0]
We find that two models achieve comparable performance: a feature-rich classifier model and a neural network model centered around a Long Short-Term Memory network (LSTM; Hochreiter and Schmidhuber 1997).,1 Introduction,[0],[0]
"We further evaluate the LSTM model by taking advantage of its ready support for transfer learning, and show that it can be adapted to an existing NLI challenge task, yielding the best reported performance by a neural network model and approaching the overall state of the art.",1 Introduction,[0],[0]
"To date, the primary sources of annotated NLI corpora have been the Recognizing Textual Entailment (RTE) challenge tasks.1 These are generally high-quality, hand-labeled data sets, and they have stimulated innovative logical and statistical models of natural language reasoning, but their small size (fewer than a thousand examples each) limits their utility as a testbed for learned distributed representations.",2 A new corpus for NLI,[0],[0]
"The data for the SemEval 2014 task called Sentences Involving Compositional Knowledge (SICK) is a step up in terms of size, but only to 4,500 training examples, and its partly automatic construction introduced some spurious patterns into the data (Marelli et al. 2014a, §6).",2 A new corpus for NLI,[0],[0]
"The Denotation Graph entailment set (Young et al., 2014) contains millions of examples of entailments between sentences and artificially constructed short phrases, but it was labeled using fully automatic methods, and is noisy enough that it is probably suitable only as a source of sup-
1http://aclweb.org/aclwiki/index.php?",2 A new corpus for NLI,[0],[0]
"title=Textual_Entailment_Resource_Pool
plementary training data.",2 A new corpus for NLI,[0],[0]
"Outside the domain of sentence-level entailment, Levy et al. (2014) introduce a large corpus of semi-automatically annotated entailment examples between subject–verb– object relation triples, and the second release of the Paraphrase Database (Pavlick et al., 2015) includes automatically generated entailment annotations over a large corpus of pairs of words and short phrases.
",2 A new corpus for NLI,[0],[0]
Existing resources suffer from a subtler issue that impacts even projects using only humanprovided annotations: indeterminacies of event and entity coreference lead to insurmountable indeterminacy concerning the correct semantic label (de Marneffe et al. 2008,2 A new corpus for NLI,[0],[0]
§4.3; Marelli et al. 2014b).,2 A new corpus for NLI,[0],[0]
"For an example of the pitfalls surrounding entity coreference, consider the sentence pair A boat sank in the Pacific Ocean and A boat sank in the Atlantic Ocean.",2 A new corpus for NLI,[0],[0]
"The pair could be labeled as a contradiction if one assumes that the two sentences refer to the same single event, but could also be reasonably labeled as neutral if that assumption is not made.",2 A new corpus for NLI,[0],[0]
"In order to ensure that our labeling scheme assigns a single correct label to every pair, we must select one of these approaches across the board, but both choices present problems.",2 A new corpus for NLI,[0],[0]
"If we opt not to assume that events are coreferent, then we will only ever find contradictions between sentences that make broad universal assertions, but if we opt to assume coreference, new counterintuitive predictions emerge.",2 A new corpus for NLI,[0],[0]
"For example, Ruth Bader Ginsburg was appointed to the US Supreme Court and I had a sandwich for lunch today would unintuitively be labeled as a contradiction, rather than neutral, under this assumption.
",2 A new corpus for NLI,[0],[0]
"Entity coreference presents a similar kind of indeterminacy, as in the pair A tourist visited New
York and A tourist visited the city.",2 A new corpus for NLI,[0],[0]
"Assuming coreference between New York and the city justifies labeling the pair as an entailment, but without that assumption the city could be taken to refer to a specific unknown city, leaving the pair neutral.",2 A new corpus for NLI,[0],[0]
"This kind of indeterminacy of label can be resolved only once the questions of coreference are resolved.
",2 A new corpus for NLI,[0],[0]
"With SNLI, we sought to address the issues of size, quality, and indeterminacy.",2 A new corpus for NLI,[0],[0]
"To do this, we employed a crowdsourcing framework with the following crucial innovations.",2 A new corpus for NLI,[0],[0]
"First, the examples were grounded in specific scenarios, and the premise and hypothesis sentences in each example were constrained to describe that scenario from the same perspective, which helps greatly in controlling event and entity coreference.2 Second, the prompt gave participants the freedom to produce entirely novel sentences within the task setting, which led to richer examples than we see with the more proscribed string-editing techniques of earlier approaches, without sacrificing consistency.",2 A new corpus for NLI,[0],[0]
"Third, a subset of the resulting sentences were sent to a validation task aimed at providing a highly reliable set of annotations over the same data, and at identifying areas of inferential uncertainty.",2 A new corpus for NLI,[0],[0]
We used Amazon Mechanical Turk for data collection.,2.1 Data collection,[0],[0]
"In each individual task (each HIT), a worker was presented with premise scene descriptions from a pre-existing corpus, and asked to supply hypotheses for each of our three labels— entailment, neutral, and contradiction—forcing the data to be balanced among these classes.
",2.1 Data collection,[0],[0]
The instructions that we provided to the workers are shown in Figure 1.,2.1 Data collection,[0],[0]
"Below the instructions were three fields for each of three requested sentences, corresponding to our entailment, neutral, and contradiction labels, a fourth field (marked optional) for reporting problems, and a link to an FAQ page.",2.1 Data collection,[0],[0]
That FAQ grew over the course of data collection.,2.1 Data collection,[0],[0]
"It warned about disallowed techniques (e.g., reusing the same sentence for many different prompts, which we saw in a few cases), provided guidance concerning sentence length and
2 Issues of coreference are not completely solved, but greatly mitigated.",2.1 Data collection,[0],[0]
"For example, with the premise sentence A dog is lying in the grass, a worker could safely assume that the dog is the most prominent thing in the photo, and very likely the only dog, and build contradicting sentences assuming reference to the same dog.
complexity (we did not enforce a minimum length, and we allowed bare NPs as well as full sentences), and reviewed logistical issues around payment timing.",2.1 Data collection,[0],[0]
"About 2,500 workers contributed.
",2.1 Data collection,[0],[0]
"For the premises, we used captions from the Flickr30k corpus (Young et al., 2014), a collection of approximately 160k captions (corresponding to about 30k images) collected in an earlier crowdsourced effort.3",2.1 Data collection,[0],[0]
"The captions were not authored by the photographers who took the source images, and they tend to contain relatively literal scene descriptions that are suited to our approach, rather than those typically associated with personal photographs (as in their example: Our trip to the Olympic Peninsula).",2.1 Data collection,[0],[0]
"In order to ensure that the label for each sentence pair can be recovered solely based on the available text, we did not use the images at all during corpus collection.
",2.1 Data collection,[0],[0]
"Table 2 reports some key statistics about the collected corpus, and Figure 2 shows the distributions of sentence lengths for both our source hypotheses and our newly collected premises.",2.1 Data collection,[0],[0]
"We observed that while premise sentences varied considerably in length, hypothesis sentences tended to be as
3 We additionally include about 4k sentence pairs from a pilot study in which the premise sentences were instead drawn from the VisualGenome corpus (under construction; visualgenome.org).",2.1 Data collection,[0],[0]
"These examples appear only in the training set, and have pair identifiers prefixed with vg in our corpus.
Data set sizes: Training pairs 550,152 Development pairs 10,000 Test pairs 10,000
Sentence length:",2.1 Data collection,[0],[0]
"Premise mean token count 14.1 Hypothesis mean token count 8.3
Parser output:",2.1 Data collection,[0],[0]
"Premise ‘S’-rooted parses 74.0% Hypothesis ‘S’-rooted parses 88.9% Distinct words (ignoring case) 37,026
Table 2:",2.1 Data collection,[0],[0]
Key statistics for the raw sentence pairs in SNLI.,2.1 Data collection,[0],[0]
"Since the two halves of each pair were collected separately, we report some statistics for both.
short as possible while still providing enough information to yield a clear judgment, clustering at around seven words.",2.1 Data collection,[0],[0]
"We also observed that the bulk of the sentences from both sources were syntactically complete rather than fragments, and the frequency with which the parser produces a parse rooted with an ‘S’ (sentence) node attests to this.",2.1 Data collection,[0],[0]
"In order to measure the quality of our corpus, and in order to construct maximally useful testing and development sets, we performed an additional round of validation for about 10% of our data.",2.2 Data validation,[0],[0]
"This validation phase followed the same basic form as the Mechanical Turk labeling task used to label the SICK entailment data: we presented workers with pairs of sentences in batches of five, and asked them to choose a single label for each pair.",2.2 Data validation,[0],[0]
"We supplied each pair to four annotators, yielding five labels per pair including the label used by the original author.",2.2 Data validation,[0],[0]
"The instructions were similar to the instructions for initial data collection shown in Figure 1, and linked to a similar FAQ.",2.2 Data validation,[0],[0]
"Though we initially used a very restrictive qualification (based on past approval rate) to select workers for the validation task, we nonetheless discovered (and deleted) some instances of random guessing in an early batch of work, and subsequently instituted a fully closed qualification restricted to about 30 trusted workers.
",2.2 Data validation,[0],[0]
"For each pair that we validated, we assigned a gold label.",2.2 Data validation,[0],[0]
"If any one of the three labels was chosen by at least three of the five annotators, it was
LHS RHS 0 0 0",2.2 Data validation,[0],[0]
"1 1 39 1 2 42 1011 1/2/00 3 156 7980 3 4 1095 29471 4 5 3882 61196 5 6 12120 74094 6 7 26514 93600 7 8 37434 85851 8 9 44028 61359 9 10 49245 46711 10 11 50919 33241 11 12 48363 22844 12 13 43314 15994 13 14 38121 11047 14 15 33183 7601 5 16 27621 5312 16 17 23250 3732 17 18 20247 2631 18 19 18513 1878 19 20 16386 1325 20 21 13746 911 21 22 12066 642 22 23 9183 449 23 24 7131 357 24
25 6198 217 25
26 5007 168 26
27 3963 138 27
28 3438 84 28
29 2631 67 29
30 1959 46 30 31 1956 26 31 32 1434 31 32 33 1086 23 33 34 912 16 34 35 897 19 35 36 774 8 36 37 453 12 37 38 618 4 38 39 291 5 39 40 330 2 40 41 249 4 41 42 180 2 42 43 225 1 43 44 162 1 44 45 108 1 48 46 87 1 51 47 60 2 55 48 36 1 56 49 90 1 60 50 21 1 62 51 66 52 51 53 36 54 24 55 63 56 18 57 15 58 6 59 27 60 6 61 3 62 3 63 3 64 6 65 3 66 3 67 6 68 6 69 18 70 15 71 3 72 15 73 3 75 15 79 3 82 15
0 10,000 20,000 30,000 40,000 50,000 60,000 70,000 80,000 90,000 100,000
0 5 10 15 20 25 30 35 40
N um
be r
of se
nt en
ce s
Sentence length (tokens)
",2.2 Data validation,[0],[0]
"Premise Hypothesis
Figure 2: The distribution of sentence length.
chosen as the gold label.",2.2 Data validation,[0],[0]
"If there was no such consensus, which occurred in about 2% of cases, we assigned the placeholder label ‘-’.",2.2 Data validation,[0],[0]
"While these unlabeled examples are included in the corpus distribution, they are unlikely to be helpful for the standard NLI classification task, and we do not include them in either training or evaluation in the experiments that we discuss in this paper.
",2.2 Data validation,[0],[0]
The results of this validation process are summarized in Table 3.,2.2 Data validation,[0],[0]
"Nearly all of the examples received a majority label, indicating broad consensus about the nature of the data and categories.",2.2 Data validation,[0],[0]
The gold-labeled examples are very nearly evenly distributed across the three labels.,2.2 Data validation,[0],[0]
"The Fleiss κ scores (computed over every example with a full five annotations) are likely to be conservative given our large and unevenly distributed pool of annotators, but they still provide insights about the levels of disagreement across the three semantic classes.",2.2 Data validation,[0],[0]
This disagreement likely reflects not just the limitations of large crowdsourcing efforts but also the uncertainty inherent in naturalistic NLI.,2.2 Data validation,[0],[0]
"Regardless, the overall rate of agreement is extremely high, suggesting that the corpus is sufficiently high quality to pose a challenging but realistic machine learning task.",2.2 Data validation,[0],[0]
Table 1 shows a set of randomly chosen validated examples from the development set with their labels.,2.3 The distributed corpus,[0],[0]
"Qualitatively, we find the data that we collected draws fairly extensively on commonsense knowledge, and that hypothesis and premise sentences often differ structurally in significant ways, suggesting that there is room for improvement beyond superficial word alignment models.",2.3 The distributed corpus,[0],[0]
"We also find the sentences that we collected to be largely
635
fluent, correctly spelled English, with a mix of full sentences and caption-style noun phrase fragments, though punctuation and capitalization are often omitted.
",2.3 The distributed corpus,[0],[0]
"The corpus is available under a CreativeCommons Attribution-ShareAlike license, the same license used for the Flickr30k source captions.",2.3 The distributed corpus,[0],[0]
"It can be downloaded at: nlp.stanford.edu/projects/snli/
Partition We distribute the corpus with a prespecified train/test/development split.",2.3 The distributed corpus,[0],[0]
The test and development sets contain 10k examples each.,2.3 The distributed corpus,[0],[0]
"Each original ImageFlickr caption occurs in only one of the three sets, and all of the examples in the test and development sets have been validated.
",2.3 The distributed corpus,[0],[0]
"Parses The distributed corpus includes parses produced by the Stanford PCFG Parser 3.5.2 (Klein and Manning, 2003), trained on the standard training set as well as on the Brown Corpus (Francis and Kucera 1979), which we found to improve the parse quality of the descriptive sentences and noun phrases found in the descriptions.",2.3 The distributed corpus,[0],[0]
The most immediate application for our corpus is in developing models for the task of NLI.,3 Our data as a platform for evaluation,[0],[0]
"In par-
ticular, since it is dramatically larger than any existing corpus of comparable quality, we expect it to be suitable for training parameter-rich models like neural networks, which have not previously been competitive at this task.",3 Our data as a platform for evaluation,[0],[0]
"Our ability to evaluate standard classifier-base NLI models, however, was limited to those which were designed to scale to SNLI’s size without modification, so a more complete comparison of approaches will have to wait for future work.",3 Our data as a platform for evaluation,[0],[0]
"In this section, we explore the performance of three classes of models which could scale readily: (i) models from a well-known NLI system, the Excitement Open Platform; (ii) variants of a strong but simple feature-based classifier model, which makes use of both unlexicalized and lexicalized features, and (iii) distributed representation models, including a baseline model and neural network sequence models.",3 Our data as a platform for evaluation,[0],[0]
"The first class of models is from the Excitement Open Platform (EOP, Padó et al. 2014; Magnini et al. 2014)—an open source platform for RTE research.",3.1 Excitement Open Platform models,[0],[0]
EOP is a tool for quickly developing NLI systems while sharing components such as common lexical resources and evaluation sets.,3.1 Excitement Open Platform models,[0],[0]
"We evaluate on two algorithms included in the distribution: a simple edit-distance based algorithm and a classifier-based algorithm, the latter both in a bare form and augmented with EOP’s full suite of lexical resources.
",3.1 Excitement Open Platform models,[0],[0]
"Our initial goal was to better understand the difficulty of the task of classifying SNLI corpus inferences, rather than necessarily the performance of a state-of-the-art RTE system.",3.1 Excitement Open Platform models,[0],[0]
"We approached this by running the same system on several data sets: our own test set, the SICK test data, and the standard RTE-3 test set (Giampiccolo et al., 2007).",3.1 Excitement Open Platform models,[0],[0]
We report results in Table 4.,3.1 Excitement Open Platform models,[0],[0]
"Each of the models
was separately trained on the training set of each corpus.",3.1 Excitement Open Platform models,[0],[0]
All models are evaluated only on 2-class entailment.,3.1 Excitement Open Platform models,[0],[0]
"To convert 3-class problems like SICK and SNLI to this setting, all instances of contradiction and unknown are converted to nonentailment.",3.1 Excitement Open Platform models,[0],[0]
"This yields a most-frequent-class baseline accuracy of 66% on SNLI, and 71% on SICK.",3.1 Excitement Open Platform models,[0],[0]
"This is intended primarily to demonstrate the difficulty of the task, rather than necessarily the performance of a state-of-the-art RTE system.",3.1 Excitement Open Platform models,[0],[0]
"The edit distance algorithm tunes the weight of the three caseinsensitive edit distance operations on the training set, after removing stop words.",3.1 Excitement Open Platform models,[0],[0]
"In addition to the base classifier-based system distributed with the platform, we train a variant which includes information from WordNet (Miller, 1995) and VerbOcean (Chklovski and Pantel, 2004), and makes use of features based on tree patterns and dependency tree skeletons (Wang and Neumann, 2007).",3.1 Excitement Open Platform models,[0],[0]
"Unlike the RTE datasets, SNLI’s size supports approaches which make use of rich lexicalized features.",3.2 Lexicalized Classifier,[0],[0]
We evaluate a simple lexicalized classifier to explore the ability of non-specialized models to exploit these features in lieu of more involved language understanding.,3.2 Lexicalized Classifier,[0],[0]
"Our classifier implements 6 feature types; 3 unlexicalized and 3 lexicalized:
1.",3.2 Lexicalized Classifier,[0],[0]
"The BLEU score of the hypothesis with respect to the premise, using an n-gram length between 1 and 4.
2.",3.2 Lexicalized Classifier,[0],[0]
"The length difference between the hypothesis and the premise, as a real-valued feature.
3.",3.2 Lexicalized Classifier,[0],[0]
"The overlap between words in the premise and hypothesis, both as an absolute count and a percentage of possible overlap, and both over all words and over just nouns, verbs, adjectives, and adverbs.
4.",3.2 Lexicalized Classifier,[0],[0]
"An indicator for every unigram and bigram in the hypothesis.
5.",3.2 Lexicalized Classifier,[0],[0]
"Cross-unigrams: for every pair of words across the premise and hypothesis which share a POS tag, an indicator feature over the two words.
",3.2 Lexicalized Classifier,[0],[0]
6.,3.2 Lexicalized Classifier,[0],[0]
Cross,3.2 Lexicalized Classifier,[0],[0]
"-bigrams: for every pair of bigrams across the premise and hypothesis which share a POS tag on the second word, an indicator feature over the two bigrams.
",3.2 Lexicalized Classifier,[0],[0]
"We report results in Table 5, along with ablation studies for removing the cross-bigram features (leaving only the cross-unigram feature) and
for removing all lexicalized features.",3.2 Lexicalized Classifier,[0],[0]
"On our large corpus in particular, there is a substantial jump in accuracy from using lexicalized features, and another from using the very sparse cross-bigram features.",3.2 Lexicalized Classifier,[0],[0]
The latter result suggests that there is value in letting the classifier automatically learn to recognize structures like explicit negations and adjective modification.,3.2 Lexicalized Classifier,[0],[0]
"A similar result was shown in Wang and Manning (2012) for bigram features in sentiment analysis.
",3.2 Lexicalized Classifier,[0],[0]
It is surprising that the classifier performs as well as it does without any notion of alignment or tree transformations.,3.2 Lexicalized Classifier,[0],[0]
"Although we expect that richer models would perform better, the results suggest that given enough data, cross bigrams with the noisy part-of-speech overlap constraint can produce an effective model.",3.2 Lexicalized Classifier,[0],[0]
SNLI is suitably large and diverse to make it possible to train neural network models that produce distributed representations of sentence meaning.,3.3 Sentence embeddings and NLI,[0],[0]
"In this section, we compare the performance of three such models on the corpus.",3.3 Sentence embeddings and NLI,[0],[0]
"To focus specifically on the strengths of these models at producing informative sentence representations, we use sentence embedding as an intermediate step in the NLI classification task: each model must produce a vector representation of each of the two sentences without using any context from the other sentence, and the two resulting vectors are then passed to a neural network classifier which predicts the label for the pair.",3.3 Sentence embeddings and NLI,[0],[0]
"This choice allows us to focus on existing models for sentence embedding, and it allows us to evaluate the ability of those models to learn useful representations of meaning (which may be independently useful for subsequent tasks), at the cost of excluding from con-
sideration possible strong neural models for NLI that directly compare the two inputs at the word or phrase level.
",3.3 Sentence embeddings and NLI,[0],[0]
"Our neural network classifier, depicted in Figure 3 (and based on a one-layer model in Bowman et al. 2015), is simply a stack of three 200d tanh layers, with the bottom layer taking the concatenated sentence representations as input and the top layer feeding a softmax classifier, all trained jointly with the sentence embedding model itself.
",3.3 Sentence embeddings and NLI,[0],[0]
"We test three sentence embedding models, each set to use 100d phrase and sentence embeddings.",3.3 Sentence embeddings and NLI,[0],[0]
Our baseline sentence embedding model simply sums the embeddings of the words in each sentence.,3.3 Sentence embeddings and NLI,[0],[0]
"In addition, we experiment with two simple sequence embedding models: a plain RNN and an LSTM RNN (Hochreiter and Schmidhuber, 1997).
",3.3 Sentence embeddings and NLI,[0],[0]
"The word embeddings for all of the models are initialized with the 300d reference GloVe vectors (840B token version, Pennington et al. 2014) and fine-tuned as part of training.",3.3 Sentence embeddings and NLI,[0],[0]
"In addition, all of the models use an additional tanh neural network layer to map these 300d embeddings into the lower-dimensional phrase and sentence embedding space.",3.3 Sentence embeddings and NLI,[0],[0]
"All of the models are randomly initialized using standard techniques and trained using AdaDelta (Zeiler, 2012) minibatch SGD until performance on the development set stops improving.",3.3 Sentence embeddings and NLI,[0],[0]
"We applied L2 regularization to all models, manually tuning the strength coefficient λ for each, and additionally applied dropout (Srivastava et al., 2014) to the inputs and outputs of the sen-
tence embedding models (though not to its internal connections) with a fixed dropout rate.",3.3 Sentence embeddings and NLI,[0],[0]
"All models were implemented in a common framework for this paper, and the implementations will be made available at publication time.
",3.3 Sentence embeddings and NLI,[0],[0]
The results are shown in Table 6.,3.3 Sentence embeddings and NLI,[0],[0]
"The sum of words model performed slightly worse than the fundamentally similar lexicalized classifier— while the sum of words model can use pretrained word embeddings to better handle rare words, it lacks even the rudimentary sensitivity to word order that the lexicalized model’s bigram features provide.",3.3 Sentence embeddings and NLI,[0],[0]
"Of the two RNN models, the LSTM’s more robust ability to learn long-term dependencies serves it well, giving it a substantial advantage over the plain RNN, and resulting in performance that is essentially equivalent to the lexicalized classifier on the test set (LSTM performance near the stopping iteration varies by up to 0.5% between evaluation steps).",3.3 Sentence embeddings and NLI,[0],[0]
"While the lexicalized model fits the training set almost perfectly, the gap between train and test set accuracy is relatively small for all three neural network models, suggesting that research into significantly higher capacity versions of these models would be productive.",3.3 Sentence embeddings and NLI,[0],[0]
Figure 4 shows a learning curve for the LSTM and the lexicalized and unlexicalized feature-based models.,3.4 Analysis and discussion,[0],[0]
"It shows that the large size of the corpus is crucial to both the LSTM and the lexicalized model, and suggests that additional data would yield still better performance for both.",3.4 Analysis and discussion,[0],[0]
"In addition, though the LSTM and the lexicalized model show similar performance when trained on the current full corpus, the somewhat steeper slope for the LSTM hints that its ability to learn arbitrarily structured representations of sentence meaning may give it an advantage over the more constrained lexicalized model on still larger datasets.
",3.4 Analysis and discussion,[0],[0]
"We were struck by the speed with which the lexicalized classifier outperforms its unlexicalized
33.33 40.08 47.63 57.72 67.42 71.88 73.95 76.78 78.22
counterpart.",3.4 Analysis and discussion,[0],[0]
"With only 100 training examples, the cross-bigram classifier is already performing better.",3.4 Analysis and discussion,[0],[0]
"Empirically, we find that the top weighted features for the classifier trained on 100 examples tend to be high precision entailments; e.g., playing → outside (most scenes are outdoors), a banana → person eating.",3.4 Analysis and discussion,[0],[0]
"If relatively few spurious entailments get high weight—as it appears is the case— then it makes sense that, when these do fire, they boost accuracy in identifying entailments.
",3.4 Analysis and discussion,[0],[0]
There are revealing patterns in the errors common to all the models considered here.,3.4 Analysis and discussion,[0],[0]
"Despite the large size of the training corpus and the distributional information captured by GloVe initialization, many lexical relationships are still misanalyzed, leading to incorrect predictions of independent, even for pairs that are common in the training corpus like beach/surf and sprinter/runner.",3.4 Analysis and discussion,[0],[0]
"Semantic mistakes at the phrasal level (e.g., predicting contradiction for A male is placing an order in a deli/A man buying a sandwich at a deli) indicate that additional attention to compositional semantics would pay off.",3.4 Analysis and discussion,[0],[0]
"However, many of the persistent problems run deeper, to inferences that depend on world knowledge and contextspecific inferences, as in the entailment pair A race car driver leaps from a burning car/A race car driver escaping danger, for which both the lexicalized classifier and the LSTM predict neutral.",3.4 Analysis and discussion,[0],[0]
"In other cases, the models’ attempts to shortcut
this kind of inference through lexical cues can lead them astray.",3.4 Analysis and discussion,[0],[0]
"Some of these examples have qualities reminiscent of Winograd schemas (Winograd, 1972; Levesque, 2013).",3.4 Analysis and discussion,[0],[0]
"For example, all the models wrongly predict entailment for A young girl throws sand toward the ocean/A girl can’t stand the ocean, presumably because of distributional associations between throws and can’t stand.
",3.4 Analysis and discussion,[0],[0]
Analysis of the models’ predictions also yields insights into the extent to which they grapple with event and entity coreference.,3.4 Analysis and discussion,[0],[0]
"For the most part, the original image prompts contained a focal element that the caption writer identified with a syntactic subject, following information structuring conventions associating subjects and topics in English (Ward and Birner, 2004).",3.4 Analysis and discussion,[0],[0]
"Our annotators generally followed suit, writing sentences that, while structurally diverse, share topic/focus (theme/rheme) structure with their premises.",3.4 Analysis and discussion,[0],[0]
"This promotes a coherent, situation-specific construal of each sentence pair.",3.4 Analysis and discussion,[0],[0]
"This is information that our models can easily take advantage of, but it can lead them astray.",3.4 Analysis and discussion,[0],[0]
"For instance, all of them stumble with the amusingly simple case A woman prepares ingredients for a bowl of soup/A soup bowl prepares a woman, in which prior expectations about parallelism are not met.",3.4 Analysis and discussion,[0],[0]
"Another headline example of this type is A man wearing padded arm protection is being bitten by a German shepherd dog/A man bit a dog, which all the models wrongly diagnose as entailment, though the sentences report two very different stories.",3.4 Analysis and discussion,[0],[0]
A model with access to explicit information about syntactic or semantic structure should perform better on cases like these.,3.4 Analysis and discussion,[0],[0]
"To the extent that successfully training a neural network model like our LSTM on SNLI forces that model to encode broadly accurate representations of English scene descriptions and to build an entailment classifier over those relations, we should expect it to be readily possible to adapt the trained model for use on other NLI tasks.",4 Transfer learning with SICK,[0],[0]
"In this section, we evaluate on the SICK entailment task using a simple transfer learning method (Pratt et al., 1991) and achieve competitive results.
",4 Transfer learning with SICK,[0],[0]
"To perform transfer, we take the parameters of the LSTM RNN model trained on SNLI and use them to initialize a new model, which is trained from that point only on the training portion of SICK.",4 Transfer learning with SICK,[0],[0]
"The only newly initialized parameters are
softmax layer parameters and the embeddings for words that appear in SICK, but not in SNLI (which are populated with GloVe embeddings as above).",4 Transfer learning with SICK,[0],[0]
"We use the same model hyperparameters that were used to train the original model, with the exception of the L2 regularization strength, which is re-tuned.",4 Transfer learning with SICK,[0],[0]
We additionally transfer the accumulators that are used by AdaDelta to set the learning rates.,4 Transfer learning with SICK,[0],[0]
"This lowers the starting learning rates, and is intended to ensure that the model does not learn too quickly in its first few epochs after transfer and destroy the knowledge accumulated in the pre-transfer phase of training.
",4 Transfer learning with SICK,[0],[0]
The results are shown in Table 7.,4 Transfer learning with SICK,[0],[0]
"Training on SICK alone yields poor performance, and the model trained on SNLI fails when tested on SICK data, labeling more neutral examples as contradictions than correctly, possibly as a result of subtle differences in how the labeling task was presented.",4 Transfer learning with SICK,[0],[0]
"In contrast, transferring SNLI representations to SICK yields the best performance yet reported for an unaugmented neural network model, surpasses the available EOP models, and approaches both the overall state of the art at 84.6% (Lai and Hockenmaier, 2014) and the 84% level of interannotator agreement, which likely represents an approximate performance ceiling.",4 Transfer learning with SICK,[0],[0]
"This suggests that the introduction of a large high-quality corpus makes it possible to train representation-learning models for sentence meaning that are competitive with the best hand-engineered models on inference tasks.
",4 Transfer learning with SICK,[0],[0]
"We attempted to apply this same transfer evaluation technique to the RTE-3 challenge, but found that the small training set (800 examples) did not allow the model to adapt to the unfamiliar genre of text used in that corpus, such that no training configuration yielded competitive performance.",4 Transfer learning with SICK,[0],[0]
Further research on effective transfer learning on small data sets with neural models might facilitate improvements here.,4 Transfer learning with SICK,[0],[0]
"Natural languages are powerful vehicles for reasoning, and nearly all questions about meaningfulness in language can be reduced to questions of entailment and contradiction in context.",5 Conclusion,[0],[0]
"This suggests that NLI is an ideal testing ground for theories of semantic representation, and that training for NLI tasks can provide rich domain-general semantic representations.",5 Conclusion,[0],[0]
"To date, however, it has not been possible to fully realize this potential due to the limited nature of existing NLI resources.",5 Conclusion,[0],[0]
"This paper sought to remedy this with a new, largescale, naturalistic corpus of sentence pairs labeled for entailment, contradiction, and independence.",5 Conclusion,[0],[0]
"We used this corpus to evaluate a range of models, and found that both simple lexicalized models and neural network models perform well, and that the representations learned by a neural network model on our corpus can be used to dramatically improve performance on a standard challenge dataset.",5 Conclusion,[0],[0]
We hope that SNLI presents valuable training data and a challenging testbed for the continued application of machine learning to semantic representation.,5 Conclusion,[0],[0]
"We gratefully acknowledge support from a Google Faculty Research Award, a gift from Bloomberg L.P., the Defense Advanced Research Projects Agency (DARPA)",Acknowledgments,[0],[0]
Deep Exploration and Filtering of Text (DEFT) Program under Air Force Research Laboratory (AFRL) contract no.,Acknowledgments,[0],[0]
"FA875013-2-0040, the National Science Foundation under grant no.",Acknowledgments,[0],[0]
"IIS 1159679, and the Department of the Navy, Office of Naval Research, under grant no.",Acknowledgments,[0],[0]
N00014-10-1-0109.,Acknowledgments,[0],[0]
"Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of Google, Bloomberg L.P., DARPA, AFRL NSF, ONR, or the US government.",Acknowledgments,[0],[0]
We also thank our many excellent Mechanical Turk contributors.,Acknowledgments,[0],[0]
"Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations.",abstractText,[0],[0]
"However, machine learning research in this area has been dramatically limited by the lack of large-scale resources.",abstractText,[0],[0]
"To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning.",abstractText,[0],[0]
"At 570K pairs, it is two orders of magnitude larger than all other resources of its type.",abstractText,[0],[0]
"This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.",abstractText,[0],[0]
A large annotated corpus for learning natural language inference,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1237–1247 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1114",text,[0],[0]
"Natural language processing (NLP) plays an important role in artificial intelligence, which has been extensively studied for many decades.",1 Introduction,[0],[0]
"Conventional NLP techniques include the rule-based symbolic approaches widely used about two decades ago, and the more recent statistical approaches relying on feature engineering and statistical models.",1 Introduction,[0],[0]
"In the recent years, deep learning approach has achieved huge successes in many applications, ranging from speech recognition to image classification.",1 Introduction,[0],[0]
"It is drawing increasing attention in the NLP community.
",1 Introduction,[0],[0]
"In this paper, we are interested in a fundamental problem in NLP, namely named entity recognition (NER) and mention detection (MD).",1 Introduction,[0],[0]
"NER and MD are very challenging tasks in NLP, laying the foundation of almost every NLP application.",1 Introduction,[0],[0]
"NER and MD are tasks of identifying entities (named and/or nominal) from raw text, and classifying the detected entities into one of the pre-defined categories such as person (PER), organization (ORG), location (LOC), etc.",1 Introduction,[0],[0]
"Some tasks focus on named entities only, while the others also detect nominal mentions.",1 Introduction,[0],[0]
"Moreover, nested mentions may need to be extracted too.",1 Introduction,[0],[0]
"For example,
[Sue]PER and her [brother]PER N studied in [University of [Toronto]LOC ]ORG.
where Toronto is a LOC entity, embedded in another longer ORG entity University of Toronto.
",1 Introduction,[0],[0]
"Similar to many other NLP problems, NER and MD is formulated as a sequence labeling problem, where a tag is sequentially assigned to each word in the input sentence.",1 Introduction,[0],[0]
"It has been extensively studied in the NLP community (Borthwick et al., 1998).",1 Introduction,[0],[0]
The core problem is to model the conditional probability of an output sequence given an arbitrary input sequence.,1 Introduction,[0],[0]
"Many hand-crafted features are combined with statistical models, such as conditional random fields (CRFs) (Nguyen et al., 2010), to compute conditional probabilities.",1 Introduction,[0],[0]
"More recently, some popular neural networks, including convolutional neural networks (CNNs) and recurrent neural networks (RNNs), are proposed to solve sequence labelling problems.",1 Introduction,[0],[0]
"In the inference stage, the learned models compute the conditional probabilities and the output sequence is generated by the Viterbi decoding algorithm (Viterbi, 1967).
",1 Introduction,[0],[0]
"In this paper, we propose a novel local detection approach for solving NER and MD problems.",1 Introduction,[0],[0]
"The idea can be easily extended to many other se-
1237
quence labeling problems, such as chunking, partof-speech tagging (POS).",1 Introduction,[0],[0]
"Instead of globally modeling the whole sequence in training and jointly decode the entire output sequence in test, our method examines all word segments (up to a certain length) in a sentence.",1 Introduction,[0],[0]
A word segment will be examined individually based on the underlying segment itself and its left and right contexts in the sentence so as to determine whether this word segment is a valid named entity and the corresponding label if it is.,1 Introduction,[0],[0]
This approach conforms to the way human resolves an NER problem.,1 Introduction,[0],[0]
"Given any word fragment and its contexts in a sentence or paragraph, people accurately determine whether this word segment is a named entity or not.",1 Introduction,[0],[0]
People rarely conduct a global decoding over the entire sentence to make such a decision.,1 Introduction,[0],[0]
The key to making an accurate local decision for each individual fragment is to have full access to the fragment itself as well as its complete contextual information.,1 Introduction,[0],[0]
The main pitfall to implement this idea is that we can not easily encode the segment and its contexts in models since they are of varying lengths in natural languages.,1 Introduction,[0],[0]
Many feature engineering techniques have been proposed but all of these methods will inevitably lead to information loss.,1 Introduction,[0],[0]
"In this work, we propose to use a recent fixed-size encoding method, namely fixed-size ordinally forgetting encoding (FOFE) (Zhang et al., 2015a,b), to solve this problem.",1 Introduction,[0],[0]
The FOFE method is a simple recursive encoding method.,1 Introduction,[0],[0]
FOFE theoretically guarantees (almost) unique and lossless encoding of any variable-length sequence.,1 Introduction,[0],[0]
"The left and the right contexts for each word segment are encoded by FOFE method, and then a simple neural network can be trained to make a precise recognition for each individual word segment based on the fixed-size presentation of the contextual information.",1 Introduction,[0],[0]
This FOFE-based local detection approach is more appealing to NER and MD.,1 Introduction,[0],[0]
"Firstly, feature engineering is almost eliminated.",1 Introduction,[0],[0]
"Secondly, under this local detection framework, nested mention is handled with little modification.",1 Introduction,[0],[0]
"Next, it makes better use of partially-labeled data available from many application scenarios.",1 Introduction,[0],[0]
Sequence labeling model requires all entities in a sentence to be labeled.,1 Introduction,[0],[0]
"If only some (not all) entities are labeled, it is not effective to learn a sequence labeling model.",1 Introduction,[0],[0]
"However, every single labeled entity, along with its contexts, may be used to learn the proposed model.",1 Introduction,[0],[0]
"At last, due to the simplicity of
FOFE, simple neural networks, such as multilayer perceptrons, are sufficient for recognition.",1 Introduction,[0],[0]
These models are much faster to train and easier to tune.,1 Introduction,[0],[0]
"In the test stage, all possible word segments from a sentence may be packed into a mini-batch, jointly recognized in parallel on GPUs.",1 Introduction,[0],[0]
"This leads to a very fast decoding process as well.
",1 Introduction,[0],[0]
"In this paper, we have applied this FOFE-based local detection approach to several popular NER and MD tasks, including the CoNLL 2003 NER task and TAC-KBP2015 and TAC-KBP2016 Trilingual Entity Discovery and Linking (EDL) tasks.",1 Introduction,[0],[0]
Our proposed method has yielded strong performance in all of these examined tasks.,1 Introduction,[0],[0]
It has been a long history of research involving neural networks (NN).,2 Related Work,[0],[0]
"In this section, we briefly review some recent NN-related research work in NLP, which may be relevant to our work.
",2 Related Work,[0],[0]
"The success of word embedding (Mikolov et al., 2013; Liu et al., 2015) encourages researchers to focus on machine-learned representation instead of heavy feature engineering in NLP.",2 Related Work,[0],[0]
"Using word embedding as the typical feature representation for words, NNs become competitive to traditional approaches in NER.",2 Related Work,[0],[0]
"Many NLP tasks, such as NER, chunking and part-of-speech (POS) tagging can be formulated as sequence labeling tasks.",2 Related Work,[0],[0]
"In (Collobert et al., 2011), deep convolutional neural networks (CNN) and conditional random fields (CRF) are used to infer NER labels at a sentence level, where they still use many hand-crafted features to improve performance, such as capitalization features explicitly defined based on first-letter capital, non-initial capital and so on.
",2 Related Work,[0],[0]
"Recently, recurrent neural networks (RNNs) have demonstrated the ability in modeling sequences (Graves, 2012).",2 Related Work,[0],[0]
Huang et al. (2015) built on the previous CNN-CRF approach by replacing CNNs with bidirectional Long Short-Term Memory (B-LSTM).,2 Related Work,[0],[0]
"Though they have reported improved performance, they employ heavy feature engineering in that work, most of which is language-specific.",2 Related Work,[0],[0]
"There is a similar attempt in (Rondeau and Su, 2016) with full-rank CRF.",2 Related Work,[0],[0]
"CNNs are used to extract character-level features automatically in (dos Santos et al., 2015).
",2 Related Work,[0],[0]
Gazetteer is a list of names grouped by the predefined categories.,2 Related Work,[0],[0]
"Gazetteer is shown to be one of the most effective external knowledge sources
to improve NER performance (Sang and Meulder, 2003).",2 Related Work,[0],[0]
"Thus, gazetteer is widely used in many NER systems.",2 Related Work,[0],[0]
"In (Chiu and Nichols, 2016), stateof-the-art performance on a popular NER task, i.e., CoNLL2003, is achieved by incorporating a large gazetteer.",2 Related Work,[0],[0]
"Different from previous ways to use a set of bits to indicate whether a word is in gazetteer or not, they have encoded a match in BIOES (Begin, Inside, Outside, End, Single) annotation, which captures positional information.
",2 Related Work,[0],[0]
"Interestingly enough, none of these recent successes in NER was achieved by a vanilla RNN.",2 Related Work,[0],[0]
"Rather, these successes are often established by sophisticated models combining CNNs, LSTMs and CRFs in certain ways.",2 Related Work,[0],[0]
"In this paper, based on recent work in (Zhang et al., 2015a,b) and (Zhang et al., 2016), we propose a novel but simple solution to NER by applying DNN on top of FOFEbased features.",2 Related Work,[0],[0]
"This simpler approach can achieve performance very close to state-of-the-art on various NER and MD tasks, without using any external knowledge or feature engineering.",2 Related Work,[0],[0]
"In this section, we will briefly review some background techniques, which are important to our proposed NER and mention detection approach.",3 Preliminary,[0],[0]
"It is well known that neural network is a universal approximator under certain conditions (Hornik, 1991).",3.1 Deep Feedforward Neural Networks,[0],[0]
A feedforward neural network (FFNN) is a weighted graph with a layered architecture.,3.1 Deep Feedforward Neural Networks,[0],[0]
Each layer is composed of several nodes.,3.1 Deep Feedforward Neural Networks,[0],[0]
Successive layers are fully connected.,3.1 Deep Feedforward Neural Networks,[0],[0]
Each node applies a function on the weighted sum of the lower layer.,3.1 Deep Feedforward Neural Networks,[0],[0]
An NN can learn by adjusting its weights in a process called back-propagation.,3.1 Deep Feedforward Neural Networks,[0],[0]
The learned NN may be used to generalize and extrapolate to new inputs that have not been seen during training.,3.1 Deep Feedforward Neural Networks,[0],[0]
FFNN is a powerful computation model.,3.2 Fixed-size Ordinally Forgetting Encoding,[0],[0]
"However, it requires fixed-size inputs and lacks the ability of capturing long-term dependency.",3.2 Fixed-size Ordinally Forgetting Encoding,[0],[0]
"Because most NLP problems involves variablelength sequences of words, RNNs/LSTMs are more popular than FFNNs in dealing with these problems.",3.2 Fixed-size Ordinally Forgetting Encoding,[0],[0]
"The Fixed-size Ordinally Forgetting Encoding (FOFE), originally proposed in (Zhang et al., 2015a,b), nicely overcomes the limitations
of FFNNs because it can uniquely and losslessly encode a variable-length sequence of words into a fixed-size representation.
",3.2 Fixed-size Ordinally Forgetting Encoding,[0],[0]
"Give a vocabulary V , each word can be represented by a one-hot vector.",3.2 Fixed-size Ordinally Forgetting Encoding,[0],[0]
FOFE mimics bag-ofwords (BOW) but incorporates a forgetting factor to capture positional information.,3.2 Fixed-size Ordinally Forgetting Encoding,[0],[0]
It encodes any sequence of variable length composed by words in V .,3.2 Fixed-size Ordinally Forgetting Encoding,[0],[0]
"Let S = w1, w2, w3, ..., wT denote a sequence of T words from V , and et be the one-hot vector of the t-th word in S, where 1 ≤ t ≤ T .",3.2 Fixed-size Ordinally Forgetting Encoding,[0],[0]
"The FOFE of each partial sequence zt from the first word to the t-th word is recursively defined as:
zt = { 0, if t = 0 α · zt−1 + et, otherwise
(1)
where the constant α is called forgetting factor, and it is picked between 0 and 1 exclusively.",3.2 Fixed-size Ordinally Forgetting Encoding,[0],[0]
"Obviously, the size of zt is |V |, and it is irrelevant to the length of original sequence, T .
",3.2 Fixed-size Ordinally Forgetting Encoding,[0],[0]
Here’s an example.,3.2 Fixed-size Ordinally Forgetting Encoding,[0],[0]
"Assume that we have three words in our vocabulary, e.g. A, B, C, whose one-hot representations are [1, 0, 0], [0, 1, 0] and [0, 0, 1] respectively.",3.2 Fixed-size Ordinally Forgetting Encoding,[0],[0]
"When calculating from left to right, the FOFE for the sequence “ABC” is [α2, α, 1] and that of “ABCBC” is [α4, α+α3, 1+ α2].
",3.2 Fixed-size Ordinally Forgetting Encoding,[0],[0]
"The word sequences can be unequivocally recovered from their FOFE representations (Zhang et al., 2015a,b).",3.2 Fixed-size Ordinally Forgetting Encoding,[0],[0]
"The uniqueness of FOFE representation is theoretically guaranteed by the following two theorems:
Theorem 1.",3.2 Fixed-size Ordinally Forgetting Encoding,[0],[0]
If the forgetting factor α satisfies 0,3.2 Fixed-size Ordinally Forgetting Encoding,[0],[0]
"< α ≤ 0.5, FOFE is unique for any countable vocabulary V and any finite value T .
",3.2 Fixed-size Ordinally Forgetting Encoding,[0],[0]
Theorem 2.,3.2 Fixed-size Ordinally Forgetting Encoding,[0],[0]
"For 0.5 < α < 1, given any finite value T and any countable vocabulary V , FOFE is almost unique everywhere, except only a finite set of countable choices of α.
",3.2 Fixed-size Ordinally Forgetting Encoding,[0],[0]
"Though in theory uniqueness is not guaranteed when α is chosen from 0.5 to 1, in practice the chance of hitting such scenarios is extremely slim, almost impossible due to quantization errors in the system.",3.2 Fixed-size Ordinally Forgetting Encoding,[0],[0]
"Furthermore, in natural languages, normally a word does not appear repeatedly within a near context.",3.2 Fixed-size Ordinally Forgetting Encoding,[0],[0]
"Simply put, FOFE is capable of uniquely encoding any sequence of arbitrary length, serving as a fixed-size but theoretically lossless representation for any sequence.",3.2 Fixed-size Ordinally Forgetting Encoding,[0],[0]
Kim et al. (2016) model morphology in the character level since this may provide some additional advantages in dealing with unknown or out-ofvocabulary (OOVs) words in a language.,3.3 Character-level Models in NLP,[0],[0]
"In the literature, convolutional neural networks (CNNs) have been widely used as character-level models in NLP (Kim et al., 2016).",3.3 Character-level Models in NLP,[0],[0]
A trainable character embedding is initialized based on a set of possible characters.,3.3 Character-level Models in NLP,[0],[0]
"When a word fragment comes, character vectors are retrieved according to its spelling to construct a matrix.",3.3 Character-level Models in NLP,[0],[0]
This matrix can be viewed as a single-channel image.,3.3 Character-level Models in NLP,[0],[0]
"CNN is applied to generate a more abstract representation of the word fragment.
",3.3 Character-level Models in NLP,[0],[0]
The above FOFE method can be easily extended to model character-level feature in NLP.,3.3 Character-level Models in NLP,[0],[0]
"Any word, phrase or fragment can be viewed as a sequence of characters.",3.3 Character-level Models in NLP,[0],[0]
"Based on a pre-defined set of all possible characters, we apply the same FOFE method to encode the sequence of characters.",3.3 Character-level Models in NLP,[0],[0]
"This always leads to a fixed-size representation, irrelevant to the number of characters in question.",3.3 Character-level Models in NLP,[0],[0]
"For example, a word fragment of “Walmart” may be viewed as a sequence of seven characters: ‘W’, ‘a’, ‘l’, ‘m’, ‘a’, ‘r’, ‘t’.",3.3 Character-level Models in NLP,[0],[0]
The FOFE codes of character sequences are always fixed-sized and they can be directly fed to an FFNN for morphology modeling.,3.3 Character-level Models in NLP,[0],[0]
"As described above, our FOFE-based local detection approach for NER, called FOFE-NER hereafter, is motivated by the way how human actually infers whether a word segment in text is an entity or mention, where the entity types of the
other entities in the same sentence is not a must.",4 FOFE-based Local Detection for NER,[0],[0]
"Particularly, the dependency between adjacent entities is fairly weak in NER.",4 FOFE-based Local Detection for NER,[0],[0]
"Whether a fragment is an entity or not, and what class it may belong to, largely depend on the internal structure of the fragment itself as well as the left and right contexts in which it appears.",4 FOFE-based Local Detection for NER,[0],[0]
"To a large extent, the meaning and spelling of the underlying fragment are informative to distinguish named entities from the rest of the text.",4 FOFE-based Local Detection for NER,[0],[0]
"Contexts play a very important role in NER or MD when it involves multi-sense words/phrases or out-of-vocabulary (OOV) words.
",4 FOFE-based Local Detection for NER,[0],[0]
"As shown in Figure 1, our proposed FOFENER method will examine all possible fragments in text (up to a certain length) one by one.",4 FOFE-based Local Detection for NER,[0],[0]
"For each fragment, it uses the FOFE method to fully encode the underlying fragment itself, its left context and right context into some fixed-size representations, which are in turn fed to an FFNN to predict whether the current fragment is NOT a valid entity mention (NONE), or its correct entity type (PER, LOC, ORG and so on) if it is a valid mention.",4 FOFE-based Local Detection for NER,[0],[0]
This method is appealing because the FOFE codes serves as a theoretically lossless representation of the hypothesis and its full contexts.,4 FOFE-based Local Detection for NER,[0],[0]
"FFNN is used as a universal approximator to map from text to the entity labels.
",4 FOFE-based Local Detection for NER,[0],[0]
"In this work, we use FOFE to explore both word-level and character-level features for each fragment and its contexts.",4 FOFE-based Local Detection for NER,[0],[0]
"FOFE-NER generates several word-level features for each fragment hypothesis and its left and right contexts as follows:
• Bag-of-word (BoW) of the fragment, e.g.
bag-of-word vector of ‘Toronto’, ‘Maple’ and ‘Leafs’ in Figure 1.
",4.1 Word-level Features,[0],[0]
"• FOFE code for left context including the fragment, e.g. FOFE code of the word sequence of “... puck from space for the Toronto Maple Leafs” in Figure 1.
",4.1 Word-level Features,[0],[0]
"• FOFE code for left context excluding the fragment, e.g. the FOFE code of the word sequence of “... puck from space for the” in Figure 1..
• FOFE code for right context including the fragment, e.g. the FOFE code of the word sequence of “... against opener home ’ Leafs Maple Toronto” in Figure 1.
",4.1 Word-level Features,[0],[0]
"• FOFE code for right context excluding the fragment, e.g. the FOFE code of the word sequence of “... against opener home ” in Figure 1.
",4.1 Word-level Features,[0],[0]
"Moreover, all of the above word features are computed for both case-sensitive words in raw text as well as case-insensitive words in normalized lower-case text.",4.1 Word-level Features,[0],[0]
"These FOFE codes are projected to lower-dimension dense vectors based on two projection matrices, Ws and Wi, for casesensitive and case-insensitive FOFE codes respectively.",4.1 Word-level Features,[0],[0]
"These two projection matrices are initialized by word embeddings trained by word2vec, and fine-tuned during the learning of the neural networks.
",4.1 Word-level Features,[0],[0]
"Due to the recursive computation of FOFE codes in eq.(1), all of the above FOFE codes can be jointly computed for one sentence or document in a very efficient manner.",4.1 Word-level Features,[0],[0]
"On top of the above word-level features, we also augment character-level features for the underlying segment hypothesis to further model its morphological structure.",4.2 Character-level Features,[0],[0]
"For the example in Figure 1, the current fragment, Toronto Maple Leafs, is considered as a sequence of case-sensitive characters, i.e. “{‘T’, ‘o’, ..., ‘f’ , ‘s’ }”, we then add the following character-level features for this fragment: • Left-to-right FOFE code of the character se-
quence of the underlying fragment.",4.2 Character-level Features,[0],[0]
"That is the FOFE code of the sequence, “‘T’, ‘o’, ..., ‘f’ , ‘s’ ”.
",4.2 Character-level Features,[0],[0]
•,4.2 Character-level Features,[0],[0]
Right-to-left FOFE code of the character sequence of the underlying fragment.,4.2 Character-level Features,[0],[0]
"That is
the FOFE code of the sequence, “‘s’ , ‘f’ , ..., ‘o’, ‘T’ ”.
",4.2 Character-level Features,[0],[0]
"These case-sensitive character FOFE codes are also projected by another character embedding matrix, which is randomly initialized and finetuned during model training.
",4.2 Character-level Features,[0],[0]
"Alternatively, we may use the character CNNs, as described in Section 3.3, to generate characterlevel features for each fragment hypothesis as well.",4.2 Character-level Features,[0],[0]
"Obviously, the above FOFE-NER model will take each sentence of words, S =",5 Training and Decoding Algorithm,[0],[0]
"[w1, w2, w3, ..., wm], as input, and examine all continuous subsequences [wi, wi+1, wi+2, ..., wj ] up to n words in S for possible entity types.",5 Training and Decoding Algorithm,[0],[0]
"All sub-sequences longer than n words are considered as non-entities in this work.
",5 Training and Decoding Algorithm,[0],[0]
"When we train the model, based on the entity labels of all sentences in the training set, we will generate many sentence fragments up to n words.",5 Training and Decoding Algorithm,[0],[0]
"These fragments fall into three categories: • Exact-match with an entity label, e.g., the
fragment “Toronto Maple Leafs” in the previous example.
",5 Training and Decoding Algorithm,[0],[0]
"• Partial-overlap with an entity label, e.g., “for the Toronto”.
",5 Training and Decoding Algorithm,[0],[0]
"• Disjoint with all entity label, e.g. “from space for”.
",5 Training and Decoding Algorithm,[0],[0]
"For all exact-matched fragments, we generate the corresponding outputs based on the types of the matched entities in the training set.",5 Training and Decoding Algorithm,[0],[0]
"For both partial-overlap and disjoint fragments, we introduce a new output label, NONE, to indicate that these fragments are not a valid entity.",5 Training and Decoding Algorithm,[0],[0]
"Therefore, the output nodes in the neural networks contains all entity types plus a rejection option denoted as NONE.
",5 Training and Decoding Algorithm,[0],[0]
"During training, we implement a producerconsumer software design such that a thread fetches training examples, computes all FOFE codes and packs them as a mini-batch while the other thread feeds the mini-batches to neural networks and adjusts the model parameters and all projection matrices.",5 Training and Decoding Algorithm,[0],[0]
"Since “partial-overlap” and “disjoint” significantly outnumber “exact-match”, they are down-sampled so as to balance the data set.
",5 Training and Decoding Algorithm,[0],[0]
"During inference, all fragments not longer than
n words are all fed to FOFE-NER to compute their scores over all entity types.",5 Training and Decoding Algorithm,[0],[0]
"In practice, these fragments can be packed as one mini-batch so that we can compute them in parallel on GPUs.",5 Training and Decoding Algorithm,[0],[0]
"As the NER result, the FOFE-NER model will return a subset of fragments only if: i) they are recognized as a valid entity type (not NONE); AND ii) their NN scores exceed a global pruning threshold.
Occasionally, some partially-overlapped or nested fragments may occur in the above pruned prediction results.",5 Training and Decoding Algorithm,[0],[0]
"We can use one of the following simple post-processing methods to remove overlappings from the final results:
1. highest-first: We check every word in a sentence.",5 Training and Decoding Algorithm,[0],[0]
"If it is contained by more than one fragment in the pruned results, we only keep the one with the maximum NN score and discard the rest.
2.",5 Training and Decoding Algorithm,[0],[0]
longest-first: We check every word in a sentence.,5 Training and Decoding Algorithm,[0],[0]
"If it is contained by more than one fragment in the pruned results, we only keep the longest fragment and discard the rest.
",5 Training and Decoding Algorithm,[0],[0]
"Either of these strategies leads to a collection of non-nested, non-overlapping, non-NONE entity labels.
",5 Training and Decoding Algorithm,[0],[0]
"In some tasks, it may require to label all nested entities.",5 Training and Decoding Algorithm,[0],[0]
This has imposed a big challenge to the sequence labeling methods.,5 Training and Decoding Algorithm,[0],[0]
"However, the above post-processing can be slightly modified to generate nested entities’ labels.",5 Training and Decoding Algorithm,[0],[0]
"In this case, we first run either highest-first or longest-first to generate the first round result.",5 Training and Decoding Algorithm,[0],[0]
"For every entity survived in this round, we will recursively run either highestfirst or longest-first on all entities in the original set, which are completely contained by it.",5 Training and Decoding Algorithm,[0],[0]
This will generate more prediction results.,5 Training and Decoding Algorithm,[0],[0]
This process may continue to allow any levels of nesting.,5 Training and Decoding Algorithm,[0],[0]
"For example, for a sentence of “w1 w2 w3 w4 w5”, if the model first generates the prediction results after the global pruning, as [“w2w3”, PER, 0.7], [“w3w4”, LOC, 0.8], [“w1w2w3w4”, ORG, 0.9], if we choose to run highest-first, it will generate the first entity label as [“w1w2w3w4”, ORG, 0.9].",5 Training and Decoding Algorithm,[0],[0]
"Secondly, we will run highest-first on the two fragments that are completely contained by the first one, i.e., [“w2w3”, PER, 0.7], [“w3w4”, LOC, 0.8], then we will generate the second nested entity label as [“w3w4”, LOC, 0.8].",5 Training and Decoding Algorithm,[0],[0]
"Fortunately, in any real NER and MD tasks, it is pretty rare to have overlapped predictions in the NN outputs.
",5 Training and Decoding Algorithm,[0],[0]
"Therefore, the extra expense to run this recursive post-processing method is minimal.",5 Training and Decoding Algorithm,[0],[0]
"As we know, CRF brings marginal performance gain to all taggers (but not limited to NER) because of the dependancies (though fairly weak) between entity types.",6 Second-Pass Augmentation,[0],[0]
We may easily add this level of information to our model by introducing another pass of FOFE-NER.,6 Second-Pass Augmentation,[0],[0]
"We call it 2nd-pass FOFENER.
",6 Second-Pass Augmentation,[0],[0]
"In 2nd-pass FOFE-NER, another set of model is trained on outputs from the first-pass FOFENER, including all predicted entities.",6 Second-Pass Augmentation,[0],[0]
"For example, given a sentence
S =",6 Second-Pass Augmentation,[0],[0]
"[w1, w2, ...wi, ...wj , ...wn]
and an underlying word segment [wi, ..., wj ] in the second pass, every predicted entity outside this segment is substituted by its entity type predicted from the first pass.",6 Second-Pass Augmentation,[0],[0]
"For example, in the first pass, a sentence like “Google has also recruited Fei-Fei Li, director of the AI lab at Stanford University.” is predicted as: “<ORG> has also recruited FeiFei Li, director of the AI lab at<ORG>.”",6 Second-Pass Augmentation,[0],[0]
"In 2ndpass FOFE-NER, when examining the segment “Fei-Fei Li”, the predicted entity types <ORG> are used to replace the actual named entities.",6 Second-Pass Augmentation,[0],[0]
"The 2nd-pass FOFE-NER model is trained on the outputs of the first pass, where all detected entities are replaced by their predicted types as above.
",6 Second-Pass Augmentation,[0],[0]
"During inference, the results returned by the 1st-pass model are substituted in the same way.",6 Second-Pass Augmentation,[0],[0]
"The scores for each hypothesis from 1st-pass model and 2nd-pass model are linear interpolated and then decoded by either highest-first or longestfirst to generate the final results of 2nd-pass FOFE-NER.
",6 Second-Pass Augmentation,[0],[0]
"Obviously, 2nd-pass FOFE-NER may capture the semantic roles of other entities while filtering out unwanted constructs and sparse combonations.",6 Second-Pass Augmentation,[0],[0]
"On the other hand, it enables longer context expansion, since FOFE memorizes contextual information in an unselective decaying fashion.",6 Second-Pass Augmentation,[0],[0]
"In this section, we evaluate the effectiveness of our proposed methods on several popular NER and MD tasks, including CoNLL 2003 NER task and TAC-KBP2015 and TAC-KBP2016 Trilingual Entity Discovery and Linking (EDL) tasks.
",7 Experiments,[0],[0]
We have made our codes available at https:// github.com/xmb-cipher/fofe-ner for readers to reproduce the results in this paper.,7 Experiments,[0],[0]
"The CoNLL-2003 dataset (Sang and Meulder, 2003) consists of newswire from the Reuters RCV1 corpus tagged with four types of nonnested named entities: location (LOC), organization (ORG), person (PER), and miscellaneous (MISC).
",7.1 CoNLL 2003 NER task,[0],[0]
"The top 100,000 words, are kept as vocabulary, including punctuations.",7.1 CoNLL 2003 NER task,[0],[0]
"For the case-sensitive embedding, an OOV is mapped to <unk> if it contains no upper-case letter and <UNK> otherwise.",7.1 CoNLL 2003 NER task,[0],[0]
We perform grid search on several hyperparameters using a held-out dev set.,7.1 CoNLL 2003 NER task,[0],[0]
Here we summarize the set of hyper-parameters used in our experiments: i),7.1 CoNLL 2003 NER task,[0],[0]
"Learning rate: initially set to 0.128 and is multiplied by a decay factor each epoch so that it reaches 1/16 of the initial value at the end of the training; ii) Network structure: 3 fully-connected layers of 512 nodes with ReLU activation, randomly initialized based on a uniform distribution between − √ 6
Ni+No and
√ 6
Ni+No (Glorot et al., 2011); iii) Character embeddings: 64 dimensions, randomly initialized.",7.1 CoNLL 2003 NER task,[0],[0]
iv) mini-batch: 512; v),7.1 CoNLL 2003 NER task,[0],[0]
"Dropout rate: initially set to 0.4, slowly decreased during training until it reaches 0.1 at the end.",7.1 CoNLL 2003 NER task,[0],[0]
"vi) Number of epochs: 128; vii)Embedding matrices case-sensitive and caseinsensitive word embeddings of 256 dimensions, trained from Reuters RCV1; viii)",7.1 CoNLL 2003 NER task,[0],[0]
We stick to the official data train-dev-test partition.,7.1 CoNLL 2003 NER task,[0],[0]
ix),7.1 CoNLL 2003 NER task,[0],[0]
Forgetting factor α = 0.5.,7.1 CoNLL 2003 NER task,[0],[0]
"1
We have investigated the performance of our method on the CoNLL-2003 dataset by using different combinations of the FOFE features (both word-level and character-level).",7.1 CoNLL 2003 NER task,[0],[0]
The detailed comparison results are shown in Table 1.,7.1 CoNLL 2003 NER task,[0],[0]
"In Table 2, we have compared our best performance with some top-performing neural network systems on this task.",7.1 CoNLL 2003 NER task,[0],[0]
"As we can see from Table 2, our system (highest-first decoding) yields very strong performance (90.85 in F1 score) in this task, outperforming most of neural network models reported on this
1The choice of the forgetting factor α is empirical.",7.1 CoNLL 2003 NER task,[0],[0]
"We’ve evaluatedα = 0.5, 0.6, 0.7, 0.8 on a development set in some early experiments.",7.1 CoNLL 2003 NER task,[0],[0]
It turns out that α = 0.5 is the best.,7.1 CoNLL 2003 NER task,[0],[0]
"As a result, α = 0.5 is used for all NER/MD tasks throughout this paper.
dataset.",7.1 CoNLL 2003 NER task,[0],[0]
"More importantly, we have not used any hand-crafted features in our systems, and all features (either word or char level) are automatically derived from the data.",7.1 CoNLL 2003 NER task,[0],[0]
Highest-first and longestfirst perform similarly.,7.1 CoNLL 2003 NER task,[0],[0]
"In (Chiu and Nichols, 2016)2, a slightly better performance (91.62 in F1 score) is reported but a customized gazetteer is used in theirs.",7.1 CoNLL 2003 NER task,[0],[0]
"Given a document collection in three languages (English, Chinese and Spanish), the KBP2015 trilingual EDL task (Ji et al., 2015) requires to automatically identify entities (including nested entities) from a source collection of textual documents in multiple languages as in Table 3, and classify them into one of the following pre-defined five types: Person (PER), Geo-political Entity (GPE), Organization (ORG), Location (LOC) and Facility (FAC).",7.2 KBP2015 EDL Task,[0],[0]
"The corpus consists of news articles and discussion forum posts published in recent years, related but non-parallel across languages.
",7.2 KBP2015 EDL Task,[0],[0]
Three models are trained and evaluated independently.,7.2 KBP2015 EDL Task,[0],[0]
"Unless explicitly listed, hyperparameters follow those used for CoNLL2003 as described in section 7.1 and 2nd-pass model is not used.",7.2 KBP2015 EDL Task,[0],[0]
"Three sets of word embeddings of 128 dimensions are derived from English Gigaword (Parker et al., 2011), Chinese Gigaword (Graff and Chen, 2005) and Spanish Gigaword (Mendonca et al., 2009) respectively.",7.2 KBP2015 EDL Task,[0],[0]
Some language-specific modifications are made:,7.2 KBP2015 EDL Task,[0],[0]
"• Chinese: Because Chinese segmentation is
not reliable, we label Chinese at character level.",7.2 KBP2015 EDL Task,[0],[0]
The analogous roles of case-sensitive word-embedding and case-sensitive wordembedding are played by character embedding and word-embedding in which the character appears.,7.2 KBP2015 EDL Task,[0],[0]
"Neither Char FOFE features nor Char CNN features are used for Chinese.
",7.2 KBP2015 EDL Task,[0],[0]
• Spanish:,7.2 KBP2015 EDL Task,[0],[0]
Character set of Spanish is a super set of that of English.,7.2 KBP2015 EDL Task,[0],[0]
"When building character-level features, we use the mod function to hash each character’s UTF8 encoding into a number between 0 (inclusive) and 128 (exclusive).
",7.2 KBP2015 EDL Task,[0],[0]
"As shown in Table 4, our FOFE-based local detection method has obtained fairly strong perfor-
2In their work, they have used a combination of trainingset and dev-set to train the model, differing from all other systems (including ours) in Table 2.
mance in the KBP2015 dataset.",7.2 KBP2015 EDL Task,[0],[0]
"The overall trilingual entity discovery performance is slightly better than the best systems participated in the official KBP2015 evaluation, with 73.9 vs. 72.4 as measured by F1 scores.",7.2 KBP2015 EDL Task,[0],[0]
Outer and inner decodings are longest-first and highest-first respectively.,7.2 KBP2015 EDL Task,[0],[0]
"In KBP2016, the trilingual EDL task is extended to detect nominal mentions of all 5 entity types for all three languages.",7.3 KBP2016 EDL task,[0],[0]
"In our experiments, for simplicity, we treat nominal mention types as some extra entity types and detect them along with named entities together with a single model.",7.3 KBP2016 EDL task,[0],[0]
No official training set is provided in KBP2016.,7.3.1 Data Description,[0],[0]
We make use of three sets of training data:,7.3.1 Data Description,[0],[0]
"• Training and evaluation data in KBP2015:
as described in 7.2
• Machine-labeled Wikipedia (WIKI): When terms or names are first mentioned in a Wikipedia article they are often linked to the corresponding Wikipedia page by hyperlinks, which clearly highlights the possible named entities with well-defined boundary in the text.",7.3.1 Data Description,[0],[0]
We have developed a program to automatically map these hyperlinks into KBP annotations by exploring the infobox (if existing) of the destination page and/or examining the corresponding Freebase types.,7.3.1 Data Description,[0],[0]
"In this way, we have created a fairly large amount of weakly-supervised trilingual training data for the KBP2016 EDL task.",7.3.1 Data Description,[0],[0]
"Meanwhile, a gazeteer is created and used in KBP2016.
",7.3.1 Data Description,[0],[0]
"• In-house dataset: A set of 10,000 English and Chinese documents is manually labeled using some annotation rules similar to the KBP 2016 guidelines.
",7.3.1 Data Description,[0],[0]
"We split the available data into training, validation and evaluation sets in a ratio of 90:5:5.",7.3.1 Data Description,[0],[0]
"The models are trained for 256 epochs if the in-house data is not used, and 64 epochs otherwise.",7.3.1 Data Description,[0],[0]
"In our first set of experiments, we investigate the effect of using different training data sets on the final entity discovery performance.",7.3.2 Effect of various training data,[0],[0]
Different training runs are conducted on different combinations of the aforementioned data sources.,7.3.2 Effect of various training data,[0],[0]
"In Table 6, we have summarized the official English entity dis-
covery results from several systems we submitted to KBP2016 EDL evaluation round I and II.",7.3.2 Effect of various training data,[0],[0]
"The first system, using only the KBP2015 data to train the model, has achieved 0.697 in F1 score in the official KBP2016 English evaluation data.",7.3.2 Effect of various training data,[0],[0]
"After adding the weakly labeled data, WIKI, we can see the entity discovery performance is improved to 0.718 in F1 score.",7.3.2 Effect of various training data,[0],[0]
"Moreover, we can see that it yields even better performance by using the KBP2015 data and the in-house data sets to train our models, giving 0.750 in F1 score.",7.3.2 Effect of various training data,[0],[0]
The official best results of our system are summarized in Table 5.,7.3.3 The official trilingual EDL performance in KBP2016,[0],[0]
We have broken down the system performance according to different languages and categories of entities (named or nominal).,7.3.3 The official trilingual EDL performance in KBP2016,[0],[0]
"Our system, achieving 0.718 in F1 score in the KBP2016 trilingual EDL track, ranks second among all participants.",7.3.3 The official trilingual EDL performance in KBP2016,[0],[0]
"Note that our result is produced by a single system while the top system is a combination of two different models, each of which is based on 5-fold cross-validation (Liu et al., 2016).",7.3.3 The official trilingual EDL performance in KBP2016,[0],[0]
"In this paper, we propose a novel solution to NER and MD by applying FFNN on top of FOFE features.",8 Conclusion,[0],[0]
"This simple local-detection based approach has achieved almost state-of-the-art performance on various NER and MD tasks, without using any external knowledge or feature engineering.",8 Conclusion,[0],[0]
"This work is supported mainly by a research donation from iFLYTEK Co., Ltd., Hefei, China, and partially by a discovery grant from Natural Sciences and Engineering Research Council (NSERC) of Canada.",Acknowledgement,[0],[0]
"In this paper, we study a novel approach for named entity recognition (NER) and mention detection (MD) in natural language processing.",abstractText,[0],[0]
"Instead of treating NER as a sequence labeling problem, we propose a new local detection approach, which relies on the recent fixed-size ordinally forgetting encoding (FOFE) method to fully encode each sentence fragment and its left/right contexts into a fixedsize representation.",abstractText,[0],[0]
"Subsequently, a simple feedforward neural network (FFNN) is learned to either reject or predict entity label for each individual text fragment.",abstractText,[0],[0]
"The proposed method has been evaluated in several popular NER and MD tasks, including CoNLL 2003 NER task and TAC-KBP2015 and TAC-KBP2016 Tri-lingual Entity Discovery and Linking (EDL) tasks.",abstractText,[0],[0]
Our method has yielded pretty strong performance in all of these examined tasks.,abstractText,[0],[0]
This local detection approach has shown many advantages over the traditional sequence labeling methods.,abstractText,[0],[0]
A Local Detection Approach for Named Entity Recognition and Mention Detection,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 652–662 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics
We introduce MeSys, a meaning-based approach, for solving English math word problems (MWPs) via understanding and reasoning in this paper. It first analyzes the text, transforms both body and question parts into their corresponding logic forms, and then performs inference on them. The associated context of each quantity is represented with proposed role-tags (e.g., nsubj, verb, etc.), which provides the flexibility for annotating an extracted math quantity with its associated context information (i.e., the physical meaning of this quantity). Statistical models are proposed to select the operator and operands. A noisy dataset is designed to assess if a solver solves MWPs mainly via understanding or mechanical pattern matching. Experimental results show that our approach outperforms existing systems on both benchmark datasets and the noisy dataset, which demonstrates that the proposed approach understands the meaning of each quantity in the text more.",text,[0],[0]
"The math word problem (MWP) (see Figure 1) is frequently chosen to study natural language understanding and simulate human problem solving (Bakman, 2007; Hosseini et al., 2014; Liang et al., 2016) for the following reasons: (1) the answer to the MWP cannot be simply extracted by performing keyword/pattern matching.",1 Introduction,[0],[0]
"It thus shows the merit of understanding and inference.
",1 Introduction,[0],[0]
"(2) An MWP usually possesses less complicated syntax and requires less amount of domain knowledge, so the researchers can focus on the task of understanding and reasoning.",1 Introduction,[0],[0]
(3) The body part of MWP that provides the given information for solving the problem consists of only a few sentences.,1 Introduction,[0],[0]
The understanding and reasoning procedures thus could be more efficiently checked.,1 Introduction,[0],[0]
"(4) The MWP solver has its own applications such as Computer Math Tutor (for students in primary school) and Helper for Math in Daily Life (for adults who are not good in solving mathematics related real problems).
",1 Introduction,[0],[0]
"According to the approaches used to identify entities, quantities, and to select operations and operands, previous MWP solvers can be classified into: (1) Rule-based approaches (Mukherjee and Garain, 20081; Hosseini et al., 2014), which make all related decisions based on a set of rules; (2) Purely statistics-based approaches (Kushman et al., 2014; Roy et al., 2015; Zhou et al., 2015; Upadhyay et al., 2016), in which all related decisions are done via a statistical classifier; (3) DNNbased approaches (Ling et al., 2017; Wang et al., 2017), which map the given text into the corresponding math operation/equation via a DNN; and (4) Mixed approaches, which identify entities and quantities with rules, yet, decide operands and operations via statistical/DNN classifiers.",1 Introduction,[0],[0]
"This category can be further divided into two subtypes: (a) Without understanding (Roy and Roth, 2015; Koncel-Kedziorski et al., 2015; Huang et al., 2017; Shrivastava et al., 2017), which does not check the entity-attribute consistency between each quantity and the target of the given question; and (b) With understanding (Lin et al., 2015; Mitra and Baral, 2016; Roy and Roth, 2017), which also checks the entity-attribute consistency while solving the problem.",1 Introduction,[0],[0]
1,1 Introduction,[0],[0]
"It is a survey paper which reviews most of the rule-based approaches before 2008.
",1 Introduction,[0],[0]
"652
However, a widely covered rule-set is difficult to construct for the rule-based approach.",1 Introduction,[0],[0]
"Also, it is awkward in resolving ambiguity problem.",1 Introduction,[0],[0]
"In contrast, the performance of purely statistics-based approaches deteriorates significantly when the MWP includes either irrelevant information or information gaps (Hosseini et al., 2014), as it is solved without first understanding the meaning.
",1 Introduction,[0],[0]
"For the category (4a), since the physical meaning is only implicitly utilized and the result is not generated via inference, it would be difficult to explain how the answer is obtained in a human comprehensible way.",1 Introduction,[0],[0]
"Therefore, the categories (2), (3) and (4a) belong to the less favored direct translation",1 Introduction,[0],[0]
"approach2 (Pape, 2004).
",1 Introduction,[0],[0]
"In contrast, the approaches of (4b) can avoid the problems mentioned above.",1 Introduction,[0],[0]
"However, among them, Mitra and Baral (2016) merely handled Addition and Subtraction.",1 Introduction,[0],[0]
Only the meaning-based framework proposed by Lin et al. (2015) can handle general MWPs via understanding and reasoning.,1 Introduction,[0],[0]
"Therefore, it is possible to explain how the answer is obtained in a human comprehensible way (Huang et al., 2015).",1 Introduction,[0],[0]
"However, although their design looks promising, only a few Chinese MWPs had been tested and performance was not evaluated.",1 Introduction,[0],[0]
"Accordingly, it is hard to make a fair comparison between their approach and other state-of-the-art methods.",1 Introduction,[0],[0]
"In addition, in their prototype system, the desired operands of arithmetic operations are identified with predefined lexicosyntactic patterns and ad-hoc rules.",1 Introduction,[0],[0]
"Reusing the patterns/rules designed for Chinese in another language is thus difficult even if it is possible.
",1 Introduction,[0],[0]
"In this paper, we adopt the framework proposed by Lin et al. (2015) to solve English MWPs (for its potential in solving difficult/complex MWPs and providing more human comprehensible explanations).",1 Introduction,[0],[0]
"Additionally, we make the following improvements: (1) A new statistical model is proposed to select operands for arithmetic operations, and its model parameters can be automatically learnt via weakly supervised learning (Artzi and Zettlemoyer, 2013).",1 Introduction,[0],[0]
(2) A new informative and robust feature-set is proposed to select the desired arithmetic operation.,1 Introduction,[0],[0]
(3) We show the proposed approach significantly outperforms other existing systems on the common benchmark datasets reported in the literature.,1 Introduction,[0],[0]
"(4) A noisy dataset with 2 According to (Pape, 2004), the meaning-based approach of solving MWPs achieves the best performance among various behaviors adopted by middle school children.
more irrelevant quantities in MWPs is created and released.",1 Introduction,[0],[0]
It could be used to check if an approach really understands what a given MWP looks for.,1 Introduction,[0],[0]
(5) An experiment is conducted to compare various approaches on this new dataset.,1 Introduction,[0],[0]
The superior performance of our system demonstrates that the proposed meaning-based approach has good potential in handling difficult/complex MWPs.,1 Introduction,[0],[0]
"The adopted meaning-based framework (Lin et al., 2015) is a pipeline with following four stages (see Figure 2): (1) Language Analysis, (2) Solution Type Identification, (3) Logic Form Transformation and (4) Logic Inference.",2 System Description,[0],[0]
"We use the Stanford CoreNLP suite (Manning et al., 2014) as the language analysis module.",2 System Description,[0],[0]
The other three modules are briefly described below.,2 System Description,[0],[0]
"Last, we adopt the weakly supervised learning (Artzi and Zettlemoyer, 2013; Kushman et al., 2014) to automatically learn the model parameters without manually annotating each MWP with the adopted solution type and selected operands benchmark.",2 System Description,[0],[0]
"After language analysis, each MWP is assigned with a specific solution type (such as Addition, Multiplication, etc.) which indicates the stereotype math operation pattern that should be adopted to solve this problem.",2.1 Solution Type Identification (STI),[0],[0]
We classify the English MWPs released by Hosseini et al. (2014) and Roy and Roth (2015) into 6 different types:,2.1 Solution Type Identification (STI),[0],[0]
"Addition, Subtraction, Multiplication, Division, Sum and TVQ-F3.",2.1 Solution Type Identification (STI),[0],[0]
"An SVM (Chang and Lin, 2011) is used to identify the solution type with 26 features.",2.1 Solution Type Identification (STI),[0],[0]
"Most of them are derived from some important properties associated with each quantity.
",2.1 Solution Type Identification (STI),[0],[0]
"3 TVQ-F means to get the final state of a Time-VariantQuantity that involves both Addition and Subtraction.
",2.1 Solution Type Identification (STI),[0],[0]
"In addition to the properties Entity4 and Verb (Hosseini et al., 2014) associated with the quantity, we also introduce a new property Time which encodes the tense and aspect of a verb into an integer to specify a point in the timeline.",2.1 Solution Type Identification (STI),[0],[0]
"We assign 2, 4, and 6 to the tenses Past, Present and Future, respectively, and then adjust it with the aspectvalues -1, 0 and 1 for Perfect, Simple, and Progressive, respectively.
",2.1 Solution Type Identification (STI),[0],[0]
Another property Anchor is associated with the unknown quantity asked in the question sentence.,2.1 Solution Type Identification (STI),[0],[0]
"If the subject of the question sentence is a noun phrase (e.g., “how many apples does John have?”), Anchor is the subject (i.e., John).",2.1 Solution Type Identification (STI),[0],[0]
"If the subject is an expletive nominal (e.g. “how many apples are there in the box?”), then Anchor is the associated nominal modifier nmod (i.e., “box”).",2.1 Solution Type Identification (STI),[0],[0]
"Otherwise, Anchor is set to “Unknown”.
",2.1 Solution Type Identification (STI),[0],[0]
"Inspired by (Hosseini et al., 2014), we transform Verb to Verb-Class (VC) which is positive, negative or stative.",2.1 Solution Type Identification (STI),[0],[0]
A verb is positive/negative if it increases/decreases the associated quantity of the subject.,2.1 Solution Type Identification (STI),[0],[0]
"For example, in the sentence “Tom borrowed 3 dollars from Mike”, the verb is positive because the money of subject “Tom” increases.
",2.1 Solution Type Identification (STI),[0],[0]
"However, a positive verb does not always imply the Addition operation.",2.1 Solution Type Identification (STI),[0],[0]
"If the question is “How much money does Mike have now?” for the above body sentence, the operation should be Subtraction.",2.1 Solution Type Identification (STI),[0],[0]
"Two new properties Anchor-Role (AR) and Action (A) are thus proposed: ARi indicates the role that Anchor associated with qi, and is set to nsubj/obj/nmod/φ.",2.1 Solution Type Identification (STI),[0],[0]
"Ai is determined by following rules: (1) Ai=positive if (VCi, ARi) is either (positive, nsubj) or (negative, obj/nmod).",2.1 Solution Type Identification (STI),[0],[0]
"(2) Ai=negative if (VCi, ARi) is either (negative,
4",2.1 Solution Type Identification (STI),[0],[0]
"In our works, the term “Entity” also includes the unit of the quantity (e.g., “cup of coffee”).
nsubj) or (positive, obj/nmod).",2.1 Solution Type Identification (STI),[0],[0]
"(3) Otherwise, Ai=VCi.
To rule out the noisy quantities introduced by irrelevant information, we further associate each known quantity with the property Relevance (R) according to the unknown quantity asked in the question sentence.",2.1 Solution Type Identification (STI),[0],[0]
"Let qi denote the i-th known quantity, Ei denote the entity of qi, Xi denote the property X of qi, qU denote the unknown quantity asked, and XU denote the property X of qU. Ri is specified with following rules: (1) Ri=2",2.1 Solution Type Identification (STI),[0],[0]
(DirectlyRelated) if either {Anchor is Unknown & Ei entails EU} or {Anchor is not Unknown & ARi≠φ & Ei entails EU} (2) Ri=1,2.1 Solution Type Identification (STI),[0],[0]
"(Indirectly-Related) if there is a qj which maps5 to qi and Rj=2 (i.e., qj is Directly-Related).",2.1 Solution Type Identification (STI),[0],[0]
"(3) Ri=0 (Unrelated) otherwise.
",2.1 Solution Type Identification (STI),[0],[0]
The solution type is identified by an SVM based on 26 binary features.,2.1 Solution Type Identification (STI),[0],[0]
"Let the symbols p, n, s, A, E, R, T, V, SB, SQ and wQ stand for positive, negative, stative, Action, Entity, Relevance, Time, Verb, “a body sentence”, “the question sentence” and “a word in question sentence” respectively.",2.1 Solution Type Identification (STI),[0],[0]
"Also, let I(x) be the indicator function to check if x is true.",2.1 Solution Type Identification (STI),[0],[0]
"The 26 features are briefly described as follows:
(1) VCU=p; (2) ∃Ri=2 s.t.",2.1 Solution Type Identification (STI),[0],[0]
Ai=p; (3) ∃Ri=2 s.t.,2.1 Solution Type Identification (STI),[0],[0]
Ai=n; (4) ∃Ri=2 s.t.,2.1 Solution Type Identification (STI),[0],[0]
Ai=s; (5) ∑𝑖𝑖,2.1 Solution Type Identification (STI),[0],[0]
I( Ri =2) > 2; (6) ∑𝑖𝑖,2.1 Solution Type Identification (STI),[0],[0]
"I( Ri=2 & Ai ∈{p, n} )",2.1 Solution Type Identification (STI),[0],[0]
= 2; (7) ∃Ri=2 s.t.,2.1 Solution Type Identification (STI),[0],[0]
Ai=p & TU<Ti; (8) ∃Ri=2 s.t.,2.1 Solution Type Identification (STI),[0],[0]
Ai=n & TU<Ti; (9) ∃Ri=2 s.t.,2.1 Solution Type Identification (STI),[0],[0]
Ai=s & Ti=max Tj; (10) ∃Ri=2 s.t.,2.1 Solution Type Identification (STI),[0],[0]
Ai=s & Ti<TU; (11) TU ≥ max Ti; (12) TU ≤,2.1 Solution Type Identification (STI),[0],[0]
"min Ti; (13) ∀Ri=2, Vi are the same; (14) ∀Ri=2",2.1 Solution Type Identification (STI),[0],[0]
"s.t. Ti=TU; (15) ∀Ri=2, Ti are the same; (16) ∃Ri=2, ∃Rj=1 s.t. qi maps to qj & qi > qj;
5",2.1 Solution Type Identification (STI),[0],[0]
"That is, 𝑞𝑞𝑖𝑖 is linked to a directly-related quantity 𝑞𝑞𝑗𝑗 under an expression such as “2 pencils weigh 30 grams”.
(17) ∃Ri=2, ∃Rj=1 s.t. qi maps to qj & qi is associated with a word “each/every/per/a/an”; (18) ∃Ri=2, ∃Rj=1 s.t. qi maps to qj & qj is associated with a word “each/every/per/a/an”; (19) ∃qi, qj, qk s.t. Ri = Rj = Rk =2 & Vi = Vj = Vk; (20) ∃wQ ∈{total, in all, altogether, sum}; (21) ∃wQ ∈{more, than} or ∃wQ s.t. wQ-POS=RBR; (22) ∃wQ =“left”; (23).",2.1 Solution Type Identification (STI),[0],[0]
∃qi appears in SQ; (24) “the rest V EU” appears in SB (V for any verb); (25) “each NN” appears in SQ (NN for any noun); (26) AnchorU,2.1 Solution Type Identification (STI),[0],[0]
is Unknown/nmod & VCU = s.,2.1 Solution Type Identification (STI),[0],[0]
"The results of language analysis are transformed into a logic form, which is expressed with the first-order logic (FOL) formalism (Russell and Norvig, 2009).",2.2 Logic Form Transformation (LFT),[0],[0]
Figure 3 shows how to transform the sentence (a) “Pack 100 candies into 5 boxes.” into the corresponding logic form (d).,2.2 Logic Form Transformation (LFT),[0],[0]
"First, the dependency tree (b) is transformed into the semantic representation tree (c) adopted by Lin et al., (2015).",2.2 Logic Form Transformation (LFT),[0],[0]
"Afterwards, according to the procedure proposed in (Lin et al., 2015), the domaindependent logic expressions are generated in (d).
",2.2 Logic Form Transformation (LFT),[0],[0]
"The domain-dependent logic expressions are related to crucial generic math facts, such as quantities and relations between quantities.",2.2 Logic Form Transformation (LFT),[0],[0]
"The FOL function quan(quanid, unit6,entity)=number is for describing the quantity fact.",2.2 Logic Form Transformation (LFT),[0],[0]
The first argument denotes its unique identifier.,2.2 Logic Form Transformation (LFT),[0],[0]
The other arguments and the function value describe its meaning.,2.2 Logic Form Transformation (LFT),[0],[0]
"Another FOL predicate qmap(mapid, quanid1, quanid2) (denotes the mapping from quanid1 to quanid2) is for describing a relation between two quantity facts, where the first argument is a unique identifier to represent this relation.
",2.2 Logic Form Transformation (LFT),[0],[0]
"The role-tags (e.g., verb, dobj, etc.) associated with quanid and mapid denote entity attributes (i.e., the physical meaning of the quantity), are created to help the logic inference module find the 6 This second argument denotes the associated unit used to count the entity.",2.2 Logic Form Transformation (LFT),[0],[0]
"It is set to “#” if the unit of the entity is not specified.
solution.",2.2 Logic Form Transformation (LFT),[0],[0]
"For example, quan(q2,#,box) = 5 & verb(q2,pack) &… means that q2 is the quantity of boxes being packed.",2.2 Logic Form Transformation (LFT),[0],[0]
"With those role-tags, the system can select the operands more reliably, and the inference engine can also derive new quantities to solve complex MWPs which require multi-step arithmetic operations (see section 2.3).
",2.2 Logic Form Transformation (LFT),[0],[0]
The question in the MWP is also transformed into an FOL-like utility function according to the solution type to ask the logic inference module to find out the answer.,2.2 Logic Form Transformation (LFT),[0],[0]
"For example, the utility function instance Division(quan(q1, #, candy), quan(q2, #, box)) asks the inference module to divide “100 candies” by “5 boxes”.",2.2 Logic Form Transformation (LFT),[0],[0]
"Since associated operands must be specified before calling those utility functions, a statistical model (see section 2.4) is used to identify the appropriate quantities.",2.2 Logic Form Transformation (LFT),[0],[0]
"The logic inference module adopts the inference engine from (Lin et al., 2015).",2.3 Logic Inference,[0],[0]
Figure 4 shows how it uses inference rules to derive new facts from the initial facts directly provided from the description.,2.3 Logic Inference,[0],[0]
The MWP (a) provides some facts (b) generated from the LFT module.,2.3 Logic Inference,[0],[0]
"An inference rule (c) 7 , which implements the common sense that people must pay money to buy something, is unified with the given facts (b) and derives new facts (d).",2.3 Logic Inference,[0],[0]
"The facts associated with q6 can be interpreted as “Mary paid 0.5 dollar for two puddings”.
",2.3 Logic Inference,[0],[0]
"The inference engine (IE) also provides 5 utility functions, including Addition, Subtraction, Multiplication and Division, and Sum.",2.3 Logic Inference,[0],[0]
The first four utilities all return a value by performing the named math operation on its two input arguments.,2.3 Logic Inference,[0],[0]
"On the other hand, Sum(function,condition) returns the sum of the values of FOL function instances which can be unified with the first argument (i.e., function) and satisfy the second argument (i.e., condition).",2.3 Logic Inference,[0],[0]
"For example, according to 7 In the inference rule, $q is a meta symbol to ask the inference engine to generate a unique identifier for the newly derived quantity fact.
",2.3 Logic Inference,[0],[0]
(a) A sandwich is priced at $0.75.,2.3 Logic Inference,[0],[0]
A pudding is priced at $0.25.,2.3 Logic Inference,[0],[0]
Tim bought 2 sandwiches and 4 puddings.,2.3 Logic Inference,[0],[0]
Mary bought 2 puddings.,2.3 Logic Inference,[0],[0]
"How much money should Tim pay?
(b) …price(sandwich,0.75)&price(pudding,0.25)… quan(q1,#,sandwich)=2&verb(q1,buy)&nsubj(q1,Tim)… quan(q2,#,pudding)=4&verb(q2,buy)&nsubj(q2,Tim)… quan(q3,#,pudding)=2&verb(q3,buy)&nsubj(q3,Mary)… ASK Sum(quan(?q,dollar,#),verb(?q,pay)&nsubj(?q,Tim))
",2.3 Logic Inference,[0],[0]
"(c) quan(?q,?u,?o)&verb(?q,buy)&nsubj(?q,?a)&price(?o,?p)  quan($q,dollar,#)=quan(?q,?u,?o)×?p & verb($q,pay) & nsubj($q,?a) (d) quan(q4,dollar,#)=1.5&verb(q4,pay)&nsubj(q4,Tim)… quan(q5,dollar,#)=1&verb(q5,pay)&nsubj(q5,Tim)…
quan(q6,dollar,#)=0.5&verb(q6,pay)&nsubj(q6,Mary)
",2.3 Logic Inference,[0],[0]
"Figure 2: A logic inference example
(a) A sandwich is priced at $0.75.",2.3 Logic Inference,[0],[0]
A pudding is priced at $0.25.,2.3 Logic Inference,[0],[0]
Tim bought 2 sandwiches and 4 puddings.,2.3 Logic Inference,[0],[0]
Mary bought 2 puddings.,2.3 Logic Inference,[0],[0]
"How much money should Tim pay?
(b) price(sandwich,0.75)&price(pudding,0.25) quan(q1,#,sandwich)=2&verb(q1,buy)&nsubj(q1,Tim) quan(q2,#,pudding)=4&verb(q2,buy)&nsubj(q2,Tim) quan(q3,#,pudding)=2&verb(q3,buy)&nsubj(q3,Mary) ASK Sum(quan(?q,dollar,#),verb(?q,pay)&nsubj(?q,Tim))
",2.3 Logic Inference,[0],[0]
"(c) quan(?q,?u,?o)&verb(?q,buy)&nsubj(?q,?a)&price(?o,?p) quan($q,dollar,#)=quan(?q,?u,?o)×?p & verb($q,pay) & nsubj($q,?a) (d) quan(q4,dollar,#)=1.5&verb(q4,pay)&nsubj(q4,Tim)",2.3 Logic Inference,[0],[0]
"quan(q5,dollar,#)=1&verb(q5,pay)&nsubj(q5,Tim)
",2.3 Logic Inference,[0],[0]
"quan(q6,dollar,#)=0.5&verb(q6,pay)&nsubj(q6,Mary)
",2.3 Logic Inference,[0],[0]
"Figure 4: A logic inference example
the last line in Figure 4(b), three newly derived quantity facts q4, q5 and q6 (in 4(d)) can be unified with the first argument quan(?q,dollar,#) in 4(c), but only q4 and q5 satisfy the second argument verb(?q,pay)&nsubj(?q,Tim).",2.3 Logic Inference,[0],[0]
"As a result, the answer 2.5 is returned by taking sum on the values of the quantity facts quan(q4,dollar,#) and quan(q5,dollar,#).",2.3 Logic Inference,[0],[0]
The most error-prone part in the LFT module is instantiating the utility function of math operation especially if many irrelevant quantity facts appear in the given MWP.,2.4 Probabilistic Operand Selection,[0],[0]
Figure 5 shows the LFT module needs to select two quantity facts (among 4) for Addition.,2.4 Probabilistic Operand Selection,[0],[0]
"Please note that the question quantity qQ, transformed from “how many flowers”, is not a candidate for operand selection.
",2.4 Probabilistic Operand Selection,[0],[0]
"Lin et al., (2015) used predefined lexicosyntactic patterns and ad-hoc rules to instantiate utility functions.",2.4 Probabilistic Operand Selection,[0],[0]
"However, their rule-based approach fails when the MWP involves more quantities.",2.4 Probabilistic Operand Selection,[0],[0]
"Therefore, we propose a statistical model to select operands for the utility functions Addition, Subtraction, Multiplication and Division.",2.4 Probabilistic Operand Selection,[0],[0]
"The operand selection procedure can be regarded as finding the most likely configuration (𝑜𝑜1𝑛𝑛, 𝑟𝑟), where 𝑜𝑜1𝑛𝑛 = 𝑜𝑜1,⋯ , 𝑜𝑜𝑛𝑛 is a sequence of random indicators which denote if the corresponding quantity will be selected as an operand, and 𝑟𝑟 is a tri-state variable to represent the relation between the values of two operands (i.e., 𝑟𝑟 = −1, 0 or 1 ; which denote that the first operand is less than, equal to, or greater than the second operand, respectively).",2.4 Probabilistic Operand Selection,[0],[0]
"Given a solution type 𝑠𝑠, the MWP logic expressions 𝕃𝕃 and the 𝑛𝑛 quantities 𝑞𝑞1𝑛𝑛 = 𝑞𝑞1,⋯ , 𝑞𝑞𝑛𝑛 in 𝕃𝕃.",2.4 Probabilistic Operand Selection,[0],[0]
"The procedure is formulated as:
𝑃𝑃(𝑟𝑟, 𝑜𝑜1𝑛𝑛|𝑞𝑞1𝑛𝑛,𝕃𝕃, 𝑠𝑠) ≈ 𝑃𝑃(𝑟𝑟|𝑠𝑠) × 𝑃𝑃(𝑜𝑜1𝑛𝑛|𝑞𝑞1𝑛𝑛,𝕃𝕃, 𝑠𝑠), (1)
𝑃𝑃(𝑟𝑟|𝑠𝑠) simply refers to Relative Frequency (as it has only a few parameters and we have enough training samples). 𝑃𝑃(𝑜𝑜1𝑛𝑛|𝑞𝑞1𝑛𝑛,𝕃𝕃",2.4 Probabilistic Operand Selection,[0],[0]
", 𝑠𝑠) is further derived as:
𝑃𝑃(𝑜𝑜1𝑛𝑛|𝑞𝑞1𝑛𝑛,𝕃𝕃, 𝑠𝑠) ≈ ∏ 𝑃𝑃(𝑜𝑜𝑖𝑖|𝑞𝑞𝑖𝑖 ,𝕃𝕃, 𝑠𝑠)𝑛𝑛𝑖𝑖=1 ≈ ∏ 𝑃𝑃�𝑜𝑜𝑖𝑖�Φ(𝑞𝑞𝑖𝑖 ,𝕃𝕃, 𝑠𝑠)�,𝑛𝑛𝑖𝑖=1 (2)
where Φ(∙) is a feature extraction function to map 𝑞𝑞𝑖𝑖 and its context into a feature vector.",2.4 Probabilistic Operand Selection,[0],[0]
"Here, the probabilistic factor 𝑃𝑃�𝑜𝑜𝑖𝑖�Φ(𝑞𝑞𝑖𝑖,𝕃𝕃, 𝑠𝑠)� is obtained via an SVM classifier (Chang and Lin, 2011).",2.4 Probabilistic Operand Selection,[0],[0]
"Φ(∙) extracts total 25 features (specified as follows, and 24 of them are binary) for 𝑞𝑞𝑖𝑖. The following 11 of them are independent on the question in the MWP:
1.",2.4 Probabilistic Operand Selection,[0],[0]
"Four features to indicate if 𝑠𝑠 is Addition, Subtraction, Multiplication or Division.",2.4 Probabilistic Operand Selection,[0],[0]
2.,2.4 Probabilistic Operand Selection,[0],[0]
A feature to indicate if 𝑞𝑞𝑖𝑖 is within a qmap(…).,2.4 Probabilistic Operand Selection,[0],[0]
3.,2.4 Probabilistic Operand Selection,[0],[0]
A feature to indicate if 𝑞𝑞𝑖𝑖 = 1. 4.,2.4 Probabilistic Operand Selection,[0],[0]
"Five features to indicate if 𝑛𝑛 < 2, 𝑛𝑛 = 2, 𝑛𝑛 =
3, 𝑛𝑛 = 4 or 𝑛𝑛 > 4; where 𝑛𝑛 is the number of quantities in Eq (1).
Φ(∙) also extracts features by matching the logic expressions of 𝑞𝑞𝑖𝑖 with those of question quantity qQ to check the role-tag consistencies between 𝑞𝑞𝑖𝑖 and qQ. Another fourteen features are extracted with three indicator functions 𝐼𝐼𝑚𝑚(⋅), 𝐼𝐼𝑒𝑒(⋅), 𝐼𝐼∃(⋅) and one tri-state function 𝑇𝑇𝑚𝑚(⋅) as follows:
[ 𝐼𝐼𝑚𝑚(𝑞𝑞𝑖𝑖 , qQ, entity), 𝐼𝐼𝑒𝑒(𝑞𝑞𝑖𝑖 , qQ, entity), 𝐼𝐼𝑚𝑚(𝑞𝑞𝑖𝑖 , qQ, verb), 𝐼𝐼𝑒𝑒(𝑞𝑞𝑖𝑖 , qQ, verb), 𝐼𝐼∃(qQ, nsubj),𝑇𝑇𝑚𝑚(𝑞𝑞𝑖𝑖 , qQ, nsubj), 𝐼𝐼∃(qQ, modifier), 𝐼𝐼𝑚𝑚(𝑞𝑞𝑖𝑖 , qQ, modifier), 𝐼𝐼∃(qQ, place), 𝐼𝐼𝑚𝑚(𝑞𝑞𝑖𝑖 , qQ, place), 𝐼𝐼∃(qQ, temporal), 𝐼𝐼𝑚𝑚(𝑞𝑞𝑖𝑖 , qQ, temporal), 𝐼𝐼∃(qQ, xcomp), 𝐼𝐼𝑚𝑚(𝑞𝑞𝑖𝑖 , qQ, xcomp)",2.4 Probabilistic Operand Selection,[0],[0]
"]
where the indicator functions 𝐼𝐼𝑚𝑚(𝑥𝑥,𝑦𝑦, 𝑧𝑧) checks if the 𝑧𝑧 of 𝑥𝑥 matches the 𝑧𝑧 of 𝑦𝑦, 𝐼𝐼𝑒𝑒(𝑥𝑥,𝑦𝑦, 𝑧𝑧) checks if the 𝑧𝑧 of 𝑥𝑥 entails the 𝑧𝑧 of 𝑦𝑦 and 𝐼𝐼∃(𝑦𝑦, 𝑧𝑧) checks if the 𝑧𝑧 of 𝑦𝑦 exists.",2.4 Probabilistic Operand Selection,[0],[0]
"𝑇𝑇𝑚𝑚(𝑞𝑞𝑖𝑖, qQ, nsubj) returns “exactmatch” (if nsubj of 𝑞𝑞𝑖𝑖 matches nsubj of qQ ), “quasi-match” (if nsubj of qQ does not exist or is a plural pronoun), and “unmatch”.",2.4 Probabilistic Operand Selection,[0],[0]
𝐼𝐼𝑒𝑒(⋅) uses the WordNet hypernym and hyponym relationship to judge whether one entity/verb entails another one or not via checking if they are in an inherited hypernym-path in WordNet.,2.4 Probabilistic Operand Selection,[0],[0]
"The entity, verb and nsubj of a quantity are determined according to the logic expressions.",2.4 Probabilistic Operand Selection,[0],[0]
"The modifier, place, temporal and xcomp of a quantity are extracted from the dependency tree with some lexico-syntactic patterns.",2.4 Probabilistic Operand Selection,[0],[0]
"For example, the modifier and place of the quantity in the sentence “There are 30 red flowers in the garden.”",2.4 Probabilistic Operand Selection,[0],[0]
are “red” and “garden” respectively.,2.4 Probabilistic Operand Selection,[0],[0]
"The temporal
and xcomp of a quantity are extracted according to the dependency relations “tmod” (i.e., temporal modifier) and “xcomp” (i.e., open clausal complement), respectively.",2.4 Probabilistic Operand Selection,[0],[0]
The AI2 dataset provided by Hosseini et al. (2014) and the IL dataset released by Roy and Roth (2015) are adopted to compare our approach with other state-of-the-art methods.,3 Datasets for Performance Evaluation,[0],[0]
"The AI2 dataset has 395 MWPs on addition and subtraction, with 121 MWPs containing irrelevant information (Hosseini et al., 2014).",3 Datasets for Performance Evaluation,[0],[0]
It is the most popular one for comparing different approaches.,3 Datasets for Performance Evaluation,[0],[0]
"On the other hand, the IL dataset consists of 562 elementary MWPs which can be solved by one of the four arithmetic operations (i.e., +, −, ×, and ÷) without any irrelevant quantity.",3 Datasets for Performance Evaluation,[0],[0]
"It is the first publicly available dataset for comparing performances that covers all four arithmetic operations.
",3 Datasets for Performance Evaluation,[0],[0]
"However, the difficulty of solving an MWP depends not only on the number of arithmetic operations required, but also on how many irrelevant quantities inside, and even on how the quantities are described.",3 Datasets for Performance Evaluation,[0],[0]
One way to test if a proposed approach solves the MWPs with understanding is to check whether it is robust to those irrelevant quantities.,3 Datasets for Performance Evaluation,[0],[0]
"Therefore, it is desirable to have a big enough dataset that contains irrelevant quantities which are created under different situations (e.g., confusing with an irrelevant agent, entity, or modifier, etc.) and allow us to probe the system weakness from different angles.",3 Datasets for Performance Evaluation,[0],[0]
We thus create a new dataset with more irrelevant quantities8.,3 Datasets for Performance Evaluation,[0],[0]
"But before we do that, we need to know how difficult the task of solving the given MWPs is.",3 Datasets for Performance Evaluation,[0],[0]
"Therefore, we first propose a way to measure how easy that a system solves the problem by simply guessing.",3 Datasets for Performance Evaluation,[0],[0]
"We propose to adopt the Perplexity to measure the task difficulty, which evaluates how likely a solver will get the correct answer by guessing.",3.1 Perplexity-flavor Measure,[0],[0]
"Every MWP in the datasets can be associated with a solution expression template, such as “⊡ + ⊡” or “⊡−⊡”, where the symbol ⊡ represents a slot to hold a quantity.",3.1 Perplexity-flavor Measure,[0],[0]
The solution can be obtained by placing correct quantities at appropriate slots.,3.1 Perplexity-flavor Measure,[0],[0]
"A 8 The IL dataset does not include any irrelevant information; on the other hand, the AI2 dataset only contains 121 MWPS with irrelevant information (but not systematically created).
",3.1 Perplexity-flavor Measure,[0],[0]
random baseline is to solve an MWP by guessing.,3.1 Perplexity-flavor Measure,[0],[0]
"It first selects a solution expression template according to the prior distribution of the templates and then places quantities into the selected template according to the uniform distribution.
",3.1 Perplexity-flavor Measure,[0],[0]
The expected accuracy of the random baseline on solving an MWP is a trivial combination and permutation exercise9.,3.1 Perplexity-flavor Measure,[0],[0]
"For example, the expected accuracy of solving an MWP associated with “⊡ + ⊡” template is 𝑝𝑝⊡+⊡ ×",3.1 Perplexity-flavor Measure,[0],[0]
"𝐶𝐶𝑛𝑛 2 −1 , where the factor 𝑝𝑝⊡+⊡ denotes the prior probability of the template “⊡ + ⊡” and 𝑛𝑛 is the total number of quantities (including irrelevant ones) in the MWP.",3.1 Perplexity-flavor Measure,[0],[0]
"On the other hand, expected accuracy of solving an MWP associated with “⊡−⊡”10 template is 𝑝𝑝⊡−⊡ × 𝑃𝑃𝑛𝑛 2 −1 .",3.1 Perplexity-flavor Measure,[0],[0]
Let 𝐴𝐴𝑖𝑖 denote the expected accuracy of solving the 𝑖𝑖-th MWP in a dataset.,3.1 Perplexity-flavor Measure,[0],[0]
"The accuracy of the random baseline on the dataset of size 𝑁𝑁 is then computed as 𝐴𝐴 = (1/𝑁𝑁) × ∑ 𝐴𝐴𝑖𝑖𝑁𝑁𝑖𝑖=1 .
",3.1 Perplexity-flavor Measure,[0],[0]
"The word “Accuracy” comprises the opposite sense of the word “Perplexity”11 (i.e., in the sense of how hard a prediction problem is).",3.1 Perplexity-flavor Measure,[0],[0]
"The lower the Accuracy is, the higher the Perplexity is.",3.1 Perplexity-flavor Measure,[0],[0]
"Therefore, we transform the Accuracy measure into a Perplexity-Flavor measure (PP) via the formula: PP = 2− log2 𝐴𝐴 For instance, the Perplexity-Flavor measures of AI2 and IL datasets are 4.46 and 8.32 respectively.",3.1 Perplexity-flavor Measure,[0],[0]
"Human Math/Science tests have been considered more suitable for judging AI progress than Turing test (Clark and Etzioni, 2016).",3.2 Noisy Dataset,[0],[0]
"In our task, solving MWPs is mainly regarded as a test for intelligence (not just for creating a Math Solver package).",3.2 Noisy Dataset,[0],[0]
"By injecting various irrelevant quantities into original MWPs, a noisy dataset is thus created to assess if a solver solves the MWPs mainly via understanding or via mechanical/statistical pattern matching.",3.2 Noisy Dataset,[0],[0]
"If a system solves an MWP mainly via pattern matching, it would have difficulty in solving a similar MWP augmented from the original one with some irrelevant quantities.",3.2 Noisy Dataset,[0],[0]
"Therefore, we first create a noisy dataset by selecting some 9 Let 𝐶𝐶𝑛𝑛 𝑘𝑘 denote 𝑘𝑘-combinations of 𝑛𝑛 and 𝑃𝑃𝑛𝑛 𝑘𝑘 denote 𝑘𝑘- permutations of 𝑛𝑛. 10",3.2 Noisy Dataset,[0],[0]
"We assume the operands have different values and, therefore, they are not permutable for the subtraction operator.",3.2 Noisy Dataset,[0],[0]
11,3.2 Noisy Dataset,[0],[0]
"The Perplexity of a uniform distribution over k discrete events (such as casting a fair k-sided dice) is k.
MWPs that can be correctly solved, and then augmenting each of them with an additional noisy sentence which involves an irrelevant quantity.",3.2 Noisy Dataset,[0],[0]
"This dataset is created to examine if the solver knows that this newly added quantity is irrelevant.
",3.2 Noisy Dataset,[0],[0]
Figure 6 shows how we inject noise into an MWP (a).,3.2 Noisy Dataset,[0],[0]
"(a.1) is created by associating an irrelevant quantity to a new subject (i.e., Mary).",3.2 Noisy Dataset,[0],[0]
Here the ellipse symbol “…” denotes unchanged text.,3.2 Noisy Dataset,[0],[0]
"(a.2) is obtained by associating an irrelevant quantity to a new entity (i.e., books).",3.2 Noisy Dataset,[0],[0]
"In addition, we also change modifiers (such as yellow, red, …) to add new noisy sentence (not shown here).",3.2 Noisy Dataset,[0],[0]
"Since the noisy dataset is not designed to assess the lexicon coverage rate of a solver, we reuse the words in the original dataset as much as possible while adding new subjects, entities and modifiers.
136 MWPs that both Illinois Math Solver 12 (Roy and Roth, 2016) and our system can correctly solve are selected from the AI2 and IL datasets.",3.2 Noisy Dataset,[0],[0]
This subset is denoted as OSS (Original Sub-Set).,3.2 Noisy Dataset,[0],[0]
"Afterwards, based on the 136 MWPs of OSS, we create a noisy dataset of 396 MWPs by adding irrelevant quantities.",3.2 Noisy Dataset,[0],[0]
This noisy dataset is named as NDS13.,3.2 Noisy Dataset,[0],[0]
"Table 1 lists the size of MWPs, Perplexities (PP), and the average numbers of quantities in each MWP of these two datasets.",3.2 Noisy Dataset,[0],[0]
"We compare our approach with (Roy and Roth, 2015) and (Roy and Roth, 2017) because they achieved the state-of-the-art performance on the IL dataset.",4 Experimental Results and Discussion,[0],[0]
"In the approach of (Roy and Roth, 2015), each quantity in the MWP was associated with a quantity schema whose attributes are extracted from the context of the quantity.",4 Experimental Results and Discussion,[0],[0]
"Based on the attributes, several statistical classifiers were used to select operands and determine the operator.",4 Experimental Results and Discussion,[0],[0]
"They also reported the performances on the AI2 dataset to compare their approach with those 12 We submit MWPs to Illinois Math Solver (https://cogcomp.cs.illinois.edu/page/demo_view/Math) in May and June, 2017. 13",4 Experimental Results and Discussion,[0],[0]
The noisy dataset can be downloaded from https://github.com /chaochun/nlu-mwp-noise-dataset.,4 Experimental Results and Discussion,[0],[0]
"It includes 102 Addition, 147 Subtraction, 101 Multiplication and 46 Division MWPs.
of others (e.g., Kushman et al. (2014), which is a purely statistical approach that aligns the text with various pre-extracted equation templates).",4 Experimental Results and Discussion,[0],[0]
"Roy and Roth (2017) further introduced the concept of Unit Dependency Graphs to reinforce the consistency of physical units among selected operands associated with the same operator.
",4 Experimental Results and Discussion,[0],[0]
"To compare the performance of the statistical method with the DNN approach, we only implement a Bi-directional RNN-based Solution Type Identifier (as our original statistical Operand Selector is relatively much better).",4 Experimental Results and Discussion,[0],[0]
"It consists of a word embedding layer (for both body and question parts), and a bidirectional GRU layer as an encoder.",4 Experimental Results and Discussion,[0],[0]
"We apply the attention mechanism to scan all hidden state sequence of body by the last hidden state of question to pay more attention to those more important (i.e., more similar between the body and the question) words.",4 Experimental Results and Discussion,[0],[0]
"Lastly, it outputs the solution type by a softmax function.",4 Experimental Results and Discussion,[0],[0]
"We train it for 100 epochs, with mini-batch-size = 128 and learning-rate = 0.001; the number of nodes in the hidden layer is 200, and the drop-out rate is 0.714.
",4 Experimental Results and Discussion,[0],[0]
"We follow the same n-fold cross-validation evaluation setting adopted in (Roy and Roth, 2015) exactly.",4 Experimental Results and Discussion,[0],[0]
"Therefore, various performances could be directly compared.",4 Experimental Results and Discussion,[0],[0]
"Table 2 lists the accuracies of different systems in solving the MWPs 14 Since the dataset is not large enough for splitting a development set, we choose those hyper parameters based on the test set in coarse grain.",4 Experimental Results and Discussion,[0],[0]
"Therefore, the DNN performance we show here might be a bit optimistic.
of various datasets.",4 Experimental Results and Discussion,[0],[0]
"The performance of (Roy and Roth, 2017) system is directly delivered by their code15.",4 Experimental Results and Discussion,[0],[0]
"The last two rows are extracted from (Roy and Roth, 2015).",4 Experimental Results and Discussion,[0],[0]
"The results show that our performances of the statistical approach significantly outperform that of our DNN approach and other systems on every dataset.
",4 Experimental Results and Discussion,[0],[0]
The performances of STI and LFT modules are listed in Table 3.,4 Experimental Results and Discussion,[0],[0]
"As described in section 2, the benchmark for both solution type and the operand selection benchmark are automatically determined by weakly supervised learning.",4 Experimental Results and Discussion,[0],[0]
"The first and second rows of Table 3 show the solution type accuracies of our statistical and DNN approaches, respectively.",4 Experimental Results and Discussion,[0],[0]
The third row shows the operand selection accuracy obtained by given the solution type benchmark.,4 Experimental Results and Discussion,[0],[0]
"Basically, LFT accuracies are from 92% to 95%, and the system accuracies are dominated by STI.",4 Experimental Results and Discussion,[0],[0]
"We analyzed errors resulted from our statistical STI on AI2 and IL datasets, respectively.",4 Experimental Results and Discussion,[0],[0]
"For AI2, major errors come from: (1) failure of ruling out some irrelevant quantities (40%), and (2) making confusion between TVQ-F and Sum these two solution types (20%) for those cases that only involve addition operation (however, both types would return the same answer).",4 Experimental Results and Discussion,[0],[0]
"For IL, major errors come from: (1) requiring additional information (35%), and (2) not knowing PartWhole relation (17%).",4 Experimental Results and Discussion,[0],[0]
"Table 4 shows a few examples for different STI error types.
",4 Experimental Results and Discussion,[0],[0]
The left-half of Table 5 shows the performances on the OSS and NDS datasets.,4 Experimental Results and Discussion,[0],[0]
"Recall that OSS is created by selecting some MWPs which both Illinois Math Solver (Roy and Roth, 2016) and our system16 can correctly solve.",4 Experimental Results and Discussion,[0],[0]
"Therefore, both systems have 100% accuracy in solving the OSS dataset.",4 Experimental Results and Discussion,[0],[0]
"However, these two systems behave very differently while solving the noisy dataset.",4 Experimental Results and Discussion,[0],[0]
The much higher accuracy of our system on the noisy dataset shows that our meaning-based approach understands the meaning of each quantity more.,4 Experimental Results and Discussion,[0],[0]
"Therefore, it is less confused17 with the irrelevant quantities.
",4 Experimental Results and Discussion,[0],[0]
One MWP in the noisy dataset that confuses Illinois Math Solver (IMS) is “Tom has 9 yellow balloons.,4 Experimental Results and Discussion,[0],[0]
Sara has 8 yellow balloons.,4 Experimental Results and Discussion,[0],[0]
Bob has 5 yellow flowers.,4 Experimental Results and Discussion,[0],[0]
How many yellow balloons do 15 https://github.com/CogComp/arithmetic. 16,4 Experimental Results and Discussion,[0],[0]
"In evaluating the performances on OSS and NDS datasets, our system is trained on the folds 2-5 of the IL dataset.",4 Experimental Results and Discussion,[0],[0]
"17 Since the gap between two different types of approaches is quite big, those 396 examples on OSS and 196 examples on NDS are sufficient to confirm the conclusion.
",4 Experimental Results and Discussion,[0],[0]
"they have in total?”, where the underlined sentence is the added noisy sentence.",4 Experimental Results and Discussion,[0],[0]
"The solver sums all quantities and gives the wrong answer 22, which reveals that IMS cannot understand that the quantity “5 yellow flowers” is irrelevant to the question “How many yellow balloons?”.",4 Experimental Results and Discussion,[0],[0]
"On the contrary, our system avoids this mistake.
",4 Experimental Results and Discussion,[0],[0]
"Although the meaning of each quantity is explicitly checked in our LFT module, our system still cannot correctly solve all MWPs in NDS.",4 Experimental Results and Discussion,[0],[0]
"The error analysis reveals that the top-4 error sources are STI, LFT, CoreNLP and incorrect problem construction (for 27%, 27%, 18%, 18%), which indicates that our STI and LFT still cannot completely prevent the damage caused from the noisy sentences (which implies that more consistency check for quantity meaning should be done).",4 Experimental Results and Discussion,[0],[0]
"The remaining errors are due to incorrect quantity extraction, lacking common-sense or not knowing entailment relationship between two entities.
",4 Experimental Results and Discussion,[0],[0]
A similar experiment is performed to check if the DNN approach will be affected by the noisy information more.,4 Experimental Results and Discussion,[0],[0]
We first select 124 MWPs (denoted as OSS′) from OSS that can be correctly solved by both our statistical and DNN approaches and then filter out 350 derived MWPs (denotes as NDS′) from NDS.,4 Experimental Results and Discussion,[0],[0]
"The right-half of Table 5 shows that the performance of the DNN approach drops more than the statistical approach does in the noisy dataset, which indicates that our statistical approach is less sensitive to the irrelevant quantities and more close to human’s approach.",4 Experimental Results and Discussion,[0],[0]
"To the best of our knowledge, MWP solvers proposed before 2014 all adopted the rule-based approach.",5 Related Work,[0],[0]
Mukherjee and Garain (2008) had given a good survey for all related approaches before 2008.,5 Related Work,[0],[0]
"Afterwards, Ma et al. (2010) proposed a MSWPAS system to simulate human arithmetic multi-step addition and subtraction behavior without evaluation.",5 Related Work,[0],[0]
"Besides, Liguda and Pfeiffer (2012) proposed a model based on augmented semantic networks, and claimed that it could solve multi-step MWPs and complex equation systems and was more robust to irrelevant information (also no evaluation).
",5 Related Work,[0],[0]
"Recently, Hosseini et al. (2014) proposed a Container-Entity based approach, which solved the MWP with a sequence of state transition.",5 Related Work,[0],[0]
"And Kushman et al. (2014) proposed the first statistical approach, which heuristically extracts some algebraic templates from labeled equations, and then aligns them with the given sentence.",5 Related Work,[0],[0]
"Since no semantic analysis is conducted, the performance is quite limited.
",5 Related Work,[0],[0]
"In more recent researches (Roy and Roth, 2015; Koncel-Kedziorski et al., 2015; Roy and Roth, 2017), quantities in an MWP were associated with attributes extracted from their contexts.",5 Related Work,[0],[0]
"Based on the attributes, several statistical classifiers were used to select operands and determine operators to solve multi-step MWPs.",5 Related Work,[0],[0]
"Since the physical meaning of each quantity is not explicitly considered in getting the answer, the reasoning process cannot be explained in a human comprehensible way.",5 Related Work,[0],[0]
"Besides, Shi et al. (2015) attacked the number word problem, which only deal with numbers, with a semantic parser.",5 Related Work,[0],[0]
"Mitra and Baral (2016) mapped MWPs into three types of problems, including Part-Whole, Change and Comparison.",5 Related Work,[0],[0]
Each problem was associated with a generic formula.,5 Related Work,[0],[0]
They used a log-linear model to determine how to instantiate the formula with quantities and solve the only one Unknown variable.,5 Related Work,[0],[0]
They achieved the best performance on the AI2 dataset.,5 Related Work,[0],[0]
"However, their approach cannot handle Multiplication or Division related MWPs.",5 Related Work,[0],[0]
"Recently, DNN-based approaches (Ling et al, 2017; Wang et al, 2017) have emerged.",5 Related Work,[0],[0]
"However, they only attacked algebraic word problems, and required a very large training-set.
",5 Related Work,[0],[0]
"Our proposed approach mainly differs from those previous approaches in combining the statistical framework with logic inference, and also in
adopting the meaning-based statistical approach for selecting the desired operands.",5 Related Work,[0],[0]
"A meaning-based logic form represented with role-tags (e.g., nsubj, verb, etc.) is first proposed to associate the extracted math quantity with its physical meaning, which then can be used to identify the desired operands and filter out irrelevant quantities.",6 Conclusion,[0],[0]
"Afterwards, a statistical framework is proposed to perform understanding and reasoning based on those logic expressions.",6 Conclusion,[0],[0]
"We further compare the performance with a typical DNN approach, the results show the proposed approach is still better.",6 Conclusion,[0],[0]
"We will try to integrate domain concepts into the DNN approach to improve the learning efficiency in the future.
",6 Conclusion,[0],[0]
The main contributions of our work are: (1) Adopting a meaning-based approach to solve English math word problems and showing its superiority over other state-of-the-art systems on common datasets.,6 Conclusion,[0],[0]
(2) Proposing a statistical model to select operands by explicitly checking the meanings of quantities against the meaning of the question sentence.,6 Conclusion,[0],[0]
(3) Designing a noisy dataset to test if a system solves the problems by understanding.,6 Conclusion,[0],[0]
(4) Proposing a perplexity-flavor measure to assess the complexity of a dataset.,6 Conclusion,[0],[0]
"We introduce MeSys, a meaning-based approach, for solving English math word problems (MWPs) via understanding and reasoning in this paper.",abstractText,[0],[0]
"It first analyzes the text, transforms both body and question parts into their corresponding logic forms, and then performs inference on them.",abstractText,[0],[0]
"The associated context of each quantity is represented with proposed role-tags (e.g., nsubj, verb, etc.), which provides the flexibility for annotating an extracted math quantity with its associated context information (i.e., the physical meaning of this quantity).",abstractText,[0],[0]
Statistical models are proposed to select the operator and operands.,abstractText,[0],[0]
A noisy dataset is designed to assess if a solver solves MWPs mainly via understanding or mechanical pattern matching.,abstractText,[0],[0]
"Experimental results show that our approach outperforms existing systems on both benchmark datasets and the noisy dataset, which demonstrates that the proposed approach understands the meaning of each quantity in the text more.",abstractText,[0],[0]
A Meaning-based Statistical English Math Word Problem Solver,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 221–232 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics
Resolving abstract anaphora is an important, but difficult task for text understanding. Yet, with recent advances in representation learning this task becomes a more tangible aim. A central property of abstract anaphora is that it establishes a relation between the anaphor embedded in the anaphoric sentence and its (typically non-nominal) antecedent. We propose a mention-ranking model that learns how abstract anaphors relate to their antecedents with an LSTM-Siamese Net. We overcome the lack of training data by generating artificial anaphoric sentence– antecedent pairs. Our model outperforms state-of-the-art results on shell noun resolution. We also report first benchmark results on an abstract anaphora subset of the ARRAU corpus. This corpus presents a greater challenge due to a mixture of nominal and pronominal anaphors and a greater range of confounders. We found model variants that outperform the baselines for nominal anaphors, without training on individual anaphor data, but still lag behind for pronominal anaphors. Our model selects syntactically plausible candidates and – if disregarding syntax – discriminates candidates using deeper features.",text,[0],[0]
"Current research in anaphora (or coreference) resolution is focused on resolving noun phrases referring to concrete objects or entities in the real
†Leo Born, Juri Opitz and Anette Frank contributed equally to this work.
",1 Introduction,[0],[0]
"world, which is arguably the most frequently occurring type.",1 Introduction,[0],[0]
"Distinct from these are diverse types of abstract anaphora (AA) (Asher, 1993) where reference is made to propositions, facts, events or properties.",1 Introduction,[0],[0]
"An example is given in (1) below.1
While recent approaches address the resolution of selected abstract shell nouns (Kolhatkar and Hirst, 2014), we aim to resolve a wide range of abstract anaphors, such as the NP this trend in (1), as well as pronominal anaphors (this, that, or it).
",1 Introduction,[0],[0]
"Henceforth, we refer to a sentence that contains an abstract anaphor as the anaphoric sentence (AnaphS), and to a constituent that the anaphor refers to as the antecedent (Antec) (cf. (1)).
",1 Introduction,[0],[0]
"(1) Ever-more powerful desktop computers, designed with one or more microprocessors as their ”brains”, are expected to increasingly take on functions carried out by more expensive minicomputers and mainframes. ”",1 Introduction,[0],[0]
"[Antec The guys that make traditional hardware are really being obsoleted by microprocessor-based machines]”, said Mr. Benton.",1 Introduction,[0],[0]
"[AnaphS As a result of this trendAA, longtime powerhouses HP, IBM and Digital Equipment Corp. are scrambling to counterattack with microprocessor-based systems of their own.]
",1 Introduction,[0],[0]
A major obstacle for solving this task is the lack of sufficient amounts of annotated training data.,1 Introduction,[0],[0]
We propose a method to generate large amounts of training instances covering a wide range of abstract anaphor types.,1 Introduction,[0],[0]
"This enables us to use neural methods which have shown great success in related tasks: coreference resolution (Clark and Manning, 2016a), textual entailment (Bowman et al., 2016), learning textual similarity (Mueller and Thyagarajan, 2016), and discourse relation sense classification (Rutherford et al., 2017).
",1 Introduction,[0],[0]
"Our model is inspired by the mention-ranking model for coreference resolution (Wiseman et al., 2015; Clark and Manning, 2015, 2016a,b) and combines it with a Siamese Net (Mueller and Thyagarajan, 2016), (Neculoiu et al., 2016) for
1Example drawn from ARRAU (Uryupina et al., 2016).
",1 Introduction,[0],[0]
"221
learning similarity between sentences.",1 Introduction,[0],[0]
"Given an anaphoric sentence (AntecS in (1)) and a candidate antecedent (any constituent in a given context, e.g. being obsoleted by microprocessor-based machines in (1)), the LSTM-Siamese Net learns representations for the candidate and the anaphoric sentence in a shared space.",1 Introduction,[0],[0]
These representations are combined into a joint representation used to calculate a score that characterizes the relation between them.,1 Introduction,[0],[0]
The learned score is used to select the highest-scoring antecedent candidate for the given anaphoric sentence and hence its anaphor.,1 Introduction,[0],[0]
We consider one anaphor at a time and provide the embedding of the context of the anaphor and the embedding of the head of the anaphoric phrase to the input to characterize each individual anaphor – similar to the encoding proposed by Zhou and Xu (2015) for individuating multiply occurring predicates in SRL.,1 Introduction,[0],[0]
With deeper inspection we show that the model learns a relation between the anaphor in the anaphoric sentence and its antecedent.,1 Introduction,[0],[0]
"Fig. 1 displays our architecture.
",1 Introduction,[0],[0]
"In contrast to other work, our method for generating training data is not confined to specific types of anaphora such as shell nouns (Kolhatkar and Hirst, 2014) or anaphoric connectives (Stede and Grishina, 2016).",1 Introduction,[0],[0]
It produces large amounts of instances and is easily adaptable to other languages.,1 Introduction,[0],[0]
"This enables us to build a robust, knowledge-lean model for abstract anaphora resolution that easily extends to multiple languages.
",1 Introduction,[0],[0]
We evaluate our model on the shell noun resolution dataset of Kolhatkar et al. (2013b) and show that it outperforms their state-of-the-art results.,1 Introduction,[0],[0]
"Moreover, we report results of the model (trained on our newly constructed dataset) on unrestricted abstract anaphora instances from the ARRAU corpus (Poesio and Artstein, 2008; Uryupina et al., 2016).",1 Introduction,[0],[0]
"To our knowledge this provides the first state-of-the-art benchmark on this data subset.
",1 Introduction,[0],[0]
Our TensorFlow2 implementation of the model and scripts for data extraction are available at: https://github.com/amarasovic/ neural-abstract-anaphora.,1 Introduction,[0],[0]
"Abstract anaphora has been extensively studied in linguistics and shown to exhibit specific properties in terms of semantic antecedent types, their degrees of abstractness, and general dis-
2Abadi et al. (2015)
course properties (Asher, 1993; Webber, 1991).",2 Related and prior work,[0],[0]
"In contrast to nominal anaphora, abstract anaphora is difficult to resolve, given that agreement and lexical match features are not applicable.",2 Related and prior work,[0],[0]
"Annotation of abstract anaphora is also difficult for humans (Dipper and Zinsmeister, 2012), and thus, only few smaller-scale corpora have been constructed.",2 Related and prior work,[0],[0]
"We evaluate our models on a subset of the ARRAU corpus (Uryupina et al., 2016) that contains abstract anaphors and the shell noun corpus used in Kolhatkar et al. (2013b).3 We are not aware of other freely available abstract anaphora datasets.
",2 Related and prior work,[0],[0]
Little work exists for the automatic resolution of abstract anaphora.,2 Related and prior work,[0],[0]
"Early work (Eckert and Strube, 2000; Strube and Müller, 2003; Byron, 2004; Müller, 2008) has focused on spoken language, which exhibits specific properties.",2 Related and prior work,[0],[0]
"Recently, event coreference has been addressed using feature-based classifiers (Jauhar et al., 2015; Lu and Ng, 2016).",2 Related and prior work,[0],[0]
"Event coreference is restricted to a subclass of events, and usually focuses on coreference between verb (phrase) and noun (phrase) mentions of similar abstractness levels (e.g. purchase – acquire) with no special focus on (pro)nominal anaphora.",2 Related and prior work,[0],[0]
"Abstract anaphora typically involves a full-fledged clausal antecedent that is referred to by a highly abstract (pro)nominal anaphor, as in (1).
",2 Related and prior work,[0],[0]
Rajagopal et al. (2016) proposed a model for resolution of events in biomedical text that refer to a single or multiple clauses.,2 Related and prior work,[0],[0]
"However, instead of selecting the correct antecedent clause(s) (our task) for a given event, their model is restricted to classifying the event into six abstract categories: this these changes, responses, analysis, context, finding, observation, based on its surrounding context.",2 Related and prior work,[0],[0]
"While related, their task is not comparable to the full-fledged abstract anaphora resolution task, since the events to be classified are known to be coreferent and chosen from a set of restricted abstract types.
",2 Related and prior work,[0],[0]
More related to our work is Anand and Hardt (2016) who present an antecedent ranking account for sluicing using classical machine learning based on a small training dataset.,2 Related and prior work,[0],[0]
"They employ features modeling distance, containment, discourse structure, and – less effectively – content and lexical correlates.4
Closest to our work is Kolhatkar et al. (2013b)
",2 Related and prior work,[0],[0]
3We thank the authors for making their data available.,2 Related and prior work,[0],[0]
"4Their data set was not publicized.
",2 Related and prior work,[0],[0]
"(KZH13) and Kolhatkar and Hirst (2014) (KH14) on shell noun resolution, using classical machine learning techniques.",2 Related and prior work,[0],[0]
"Shell nouns are abstract nouns, such as fact, possibility, or issue, which can only be interpreted jointly with their shell content (their embedded clause as in (2) or antecedent as in (3)).",2 Related and prior work,[0],[0]
KZH13 refer to shell nouns whose antecedent occurs in the prior discourse as anaphoric shell nouns (ASNs),2 Related and prior work,[0],[0]
"(cf. (3)), and cataphoric shell nouns (CSNs) otherwise (cf. (2)).5
(2) Congress has focused almost solely on the fact that [special education is expensive - and that it takes away money from regular education.",2 Related and prior work,[0],[0]
"]
(3) Environmental Defense",2 Related and prior work,[0],[0]
[...] notes that [Antec Mowing the lawn with a gas mower produces as much pollution [...] as driving a car 172 miles.],2 Related and prior work,[0],[0]
"[AnaphS This fact may [...] explain the recent surge in the sales of [...] old-fashioned push mowers [...]].
KZH13 presented an approach for resolving six typical shell nouns following the observation that CSNs are easy to resolve based on their syntactic structure alone, and the assumption that ASNs share linguistic properties with their embedded (CSN) counterparts.",2 Related and prior work,[0],[0]
"They manually developed rules to identify the embedded clause (i.e. cataphoric antecedent) of CSNs and trained SVMrank (Joachims, 2002) on such instances.",2 Related and prior work,[0],[0]
The trained SVMrank model is then used to resolve ASNs.,2 Related and prior work,[0],[0]
"KH14 generalized their method to be able to create training data for any given shell noun, however, their method heavily exploits the specific properties of shell nouns and does not apply to other types of abstract anaphora.
",2 Related and prior work,[0],[0]
Stede and Grishina (2016) study a related phenomenon for German.,2 Related and prior work,[0],[0]
They examine inherently anaphoric connectives (such as demzufolge – according to which) that could be used to access their abstract antecedent in the immediate context.,2 Related and prior work,[0],[0]
"Yet, such connectives are restricted in type, and the study shows that such connectives are often ambiguous with nominal anaphors and require sense disambiguation.",2 Related and prior work,[0],[0]
"We conclude that they cannot be easily used to acquire antecedents automatically.
",2 Related and prior work,[0],[0]
"In our work, we explore a different direction: we construct artificial training data using a general pattern that identifies embedded sentence constituents, which allows us to extract relatively secure training data for abstract anaphora that captures a wide range of anaphora-antecedent rela-
5We follow this terminology for their approach and data representation.
tions, and apply this data to train a model for the resolution of unconstrained abstract anaphora.
",2 Related and prior work,[0],[0]
Recent work in entity coreference resolution has proposed powerful neural network-based models that we will adapt to the task of abstract anaphora resolution.,2 Related and prior work,[0],[0]
"Most relevant for our task is the mention-ranking neural coreference model proposed in Clark and Manning (2015), and their improved model in Clark and Manning (2016a), which integrates a loss function (Wiseman et al., 2015) which learns distinct feature representations for anaphoricity detection and antecedent ranking.
",2 Related and prior work,[0],[0]
Siamese Nets distinguish between similar and dissimilar pairs of samples by optimizing a loss over the metric induced by the representations.,2 Related and prior work,[0],[0]
"It is widely used in vision (Chopra et al., 2005), and in NLP for semantic similarity, entailment, query normalization and QA (Mueller and Thyagarajan, 2016; Neculoiu et al., 2016; Das et al., 2016).",2 Related and prior work,[0],[0]
"Given an anaphoric sentence s with a marked anaphor (mention) and a candidate antecedent c, the mention-ranking (MR) model assigns the pair (c, s) a score, using representations produced by an LSTM-Siamese Net.",3 Mention-Ranking Model,[0],[0]
The highest-scoring candidate is assigned to the marked anaphor in the anaphoric sentence.,3 Mention-Ranking Model,[0],[0]
"Fig. 1 displays the model.
",3 Mention-Ranking Model,[0],[0]
"We learn representations of an anaphoric sentence s and a candidate antecedent c using a bidirectional Long Short-Term Memory (Hochreiter and Schmidhuber, 1997; Graves and Schmidhuber, 2005).",3 Mention-Ranking Model,[0],[0]
"One bi-LSTM is applied to the anaphoric sentence s and a candidate antecedent c, hence the term siamese.",3 Mention-Ranking Model,[0],[0]
"Each word is represented with a vector wi constructed by concatenating embeddings of the word, of the context of the anaphor (average of embeddings of the anaphoric phrase, the previous and the next word), of the head of the anaphoric phrase6, and, finally, an embedding of the constituent tag of the candidate, or the S constituent tag if the word is in the anaphoric sentence.",3 Mention-Ranking Model,[0],[0]
"For each sequence s or c, the word vectors wi are sequentially fed into the bi-LSTM, which produces outputs from the forward pass, −→ hi, and outputs ←−",3 Mention-Ranking Model,[0],[0]
hi from the backward pass.,3 Mention-Ranking Model,[0],[0]
The final output of the i-th word is defined as hi =,3 Mention-Ranking Model,[0],[0]
[ ←− hi ; −→ hi ].,3 Mention-Ranking Model,[0],[0]
"To get a representation of the full sequence, hs or hc, all outputs are averaged, except for those that correspond to padding tokens.
",3 Mention-Ranking Model,[0],[0]
"6Henceforth we refer to it as embedding of the anaphor.
",3 Mention-Ranking Model,[0],[0]
"To prevent forgetting the constituent tag of the sequence, we concatenate the corresponding tag embedding with hs or hc (we call this a shortcut for the tag information).",3 Mention-Ranking Model,[0],[0]
"The resulting vector is fed into a feed-forward layer of exponential linear units (ELUs) (Clevert et al., 2016) to produce the final representation h̃s or h̃c of the sequence.
",3 Mention-Ranking Model,[0],[0]
"From h̃c and h̃s we compute a vector hc,s =",3 Mention-Ranking Model,[0],[0]
[|h̃c − h̃s|; h̃c h̃s],3 Mention-Ranking Model,[0],[0]
"(Tai et al., 2015), where |–| denotes the absolute values of the element-wise subtraction, and the element-wise multiplication.",3 Mention-Ranking Model,[0],[0]
"Then hc,s is fed into a feed-forward layer of ELUs to obtain the final joint representation, h̃c,s, of the pair (c, s).",3 Mention-Ranking Model,[0],[0]
"Finally, we compute the score for the pair (c, s) that represents relatedness between them, by applying a single fully connected linear layer to the joint representation:
score(c, s) = W h̃c,s + b ∈ R, (1) where W is a 1 × d weight matrix, and d the dimension of the vector h̃c,s.
We train the described mention-ranking model with the max-margin training objective from Wiseman et al. (2015), used for the antecedent ranking subtask.",3 Mention-Ranking Model,[0],[0]
"Suppose that the training set D = {(ai, si, T (ai),N (ai)}ni=1, where ai is the i-th abstract anaphor, si the corresponding anaphoric sentence, T (ai) the set of antecedents of ai and N (ai) the set of candidates that are not antecedents (negative candidates).",3 Mention-Ranking Model,[0],[0]
"Let t̃i = arg maxt∈T (ai) score(ti, si) be the highest scor-
ing antecedent of ai.",3 Mention-Ranking Model,[0],[0]
"Then the loss is given by n∑ i=1 max(0, max c∈N (ai) {1+score(c, si)−score(t̃i, si)}).",3 Mention-Ranking Model,[0],[0]
"We create large-scale training data for abstract anaphora resolution by exploiting a common construction, consisting of a verb with an embedded sentence (complement or adverbial) (cf. Fig. 2).",4 Training data construction,[0],[0]
"We detect this pattern in a parsed corpus, ’cut off’ the S′ constituent and replace it with a suitable anaphor to create the anaphoric sentence (AnaphS), while S yields the antecedent (Antec).",4 Training data construction,[0],[0]
"This method covers a wide range of anaphoraantecedent constellations, due to diverse semantic or discourse relations that hold between the clause hosting the verb and the embedded sentence.
",4 Training data construction,[0],[0]
"First, the pattern applies to verbs that embed sentential arguments.",4 Training data construction,[0],[0]
"In (4), the verb doubt establishes a specific semantic relation between the embedding sentence and its sentential complement.
",4 Training data construction,[0],[0]
"(4) He doubts [S′ [S a Bismarckian super state will emerge that would dominate Europe], but warns of ”a risk of profound change in the [..] European Community from a Germany that is too strong, even if democratic”].
",4 Training data construction,[0],[0]
"From this we extract the artificial antecedent A Bismarckian super state will emerge that would dominate Europe, and its corresponding anaphoric sentence He doubts this, but warns of ”a risk of profound change ... even if democratic”, which we construct by randomly choosing one of a predefined set of appropriate anaphors (here: this, that, it),",4 Training data construction,[0],[0]
cf. Table 1.,4 Training data construction,[0],[0]
"The second row in Table 1 is used when the head of S′ is filled by an overt complementizer (doubts that), as opposed to (4).",4 Training data construction,[0],[0]
"The remaining rows in Table 1 apply to adverbial clauses of different types.
",4 Training data construction,[0],[0]
"Adverbial clauses encode specific discourse relations with their embedding sentences, often indicated by their conjunctions.",4 Training data construction,[0],[0]
"In (5), for example, the causal conjunction as relates a cause (embedded sentence) and its effect (embedding sentence):
(5) There is speculation that property casualty firms will sell even more munis",4 Training data construction,[0],[0]
[S′ as [S they scramble to raise cash to pay claims related to Hurricane Hugo,4 Training data construction,[0],[0]
[..],4 Training data construction,[0],[0]
"]].
We randomly replace causal conjunctions because, as with appropriately adjusted anaphors, e.g. because of that, due to this or therefore that make the causal relation explicit in the anaphor.7
Compared to the shell noun corpus of KZH13, who made use of a carefully constructed set of extraction patterns, a downside of our method is that our artificially created antecedents are uniformly of type",4 Training data construction,[0],[0]
"S. However, the majority of abstract anaphora antecedents found in the existing datasets are of type S. Also, our models are intended to induce semantic representations, and so we expect syntactic form to be less critical, compared to a feature-based model.8",4 Training data construction,[0],[0]
"Finally, the general extraction pattern in Fig. 2, covers a much wider range of anaphoric types.
",4 Training data construction,[0],[0]
"Using this method we generated a dataset of artificial anaphoric sentence–antecedent pairs from the WSJ part of the PTB Corpus (Marcus et al., 1993), automatically parsed using the Stanford Parser (Klein and Manning, 2003).",4 Training data construction,[0],[0]
"We evaluate our model on two types of anaphora: (a) shell noun anaphora and (b) (pro)nominal abstract anaphors extracted from ARRAU.
",5.1 Datasets,[0],[0]
a. Shell noun resolution dataset.,5.1 Datasets,[0],[0]
"For comparability we train and evaluate our model for shell noun resolution, using the original training (CSN) and test (ASN) corpus of Kolhatkar et al. (2013a,b).9
7In case of ambiguous conjunctions (e.g. as interpreted as causal or temporal), we generally choose the most frequent interpretation.
",5.1 Datasets,[0],[0]
"8This also alleviates problems with languages like German, where (non-)embedded sentences differ in surface position of the finite verb.",5.1 Datasets,[0],[0]
"We can either adapt the order or ignore it, when producing anaphoric sentence – antecedent pairs.
",5.1 Datasets,[0],[0]
"9We thank the authors for providing the available data.
",5.1 Datasets,[0],[0]
"We follow the data preparation and evaluation protocol of Kolhatkar et al. (2013b) (KZH13).
",5.1 Datasets,[0],[0]
The CSN corpus was constructed from the NYT corpus using manually developed patterns to identify the antecedent of cataphoric shell nouns (CSNs).,5.1 Datasets,[0],[0]
"In KZH13, all syntactic constituents of the sentence that contains both the CSN and its antecedent were considered as candidates for training a ranking model.",5.1 Datasets,[0],[0]
Candidates that differ from the antecedent in only one word or one word and punctuation were as well considered as antecedents10.,5.1 Datasets,[0],[0]
To all other candidates we refer to as negative candidates.,5.1 Datasets,[0],[0]
"For every shell noun, KZH13 used the corresponding part of the CSN data to train SVMrank.
",5.1 Datasets,[0],[0]
The ASN corpus serves as the test corpus.,5.1 Datasets,[0],[0]
"It was also constructed from the NYT corpus, by selecting anaphoric instances with the pattern ”this 〈shell noun〉” for all covered shell nouns.",5.1 Datasets,[0],[0]
"For validation, Kolhatkar et al. (2013a) crowdsourced annotations for the sentence which contains the antecedent, which KZH13 refer to as a broad region.",5.1 Datasets,[0],[0]
Candidates for the antecedent were obtained by using all syntactic constituents of the broad region as candidates and ranking them using the SVMrank model trained on the CSN corpus.,5.1 Datasets,[0],[0]
The top 10 ranked candidates were presented to the crowd workers and they chose the best answer that represents the ASN antecedent.,5.1 Datasets,[0],[0]
The workers were encouraged to select None when they did not agree with any of the displayed answers and could provide information about how satisfied they were with the displayed candidates.,5.1 Datasets,[0],[0]
"We consider this dataset as gold, as do KZH13, although it may be biased towards the offered candidates.11
b. Abstract anaphora resolution data set.",5.1 Datasets,[0],[0]
"We use the automatically constructed data from the WSJ corpus (Section 4) for training.12 Our test data for unrestricted abstract anaphora resolution is obtained from the ARRAU corpus (Uryupina et al., 2016).",5.1 Datasets,[0],[0]
"We extracted all abstract anaphoric instances from the WSJ part of ARRAU that are marked with the category abstract or plan,13 and call the subcorpus ARRAU-AA.
10We obtained this information from the authors directly.",5.1 Datasets,[0],[0]
"11The authors provided us with the workers’ annotations of the broad region, antecedents chosen by the workers and links to the NYT corpus.",5.1 Datasets,[0],[0]
"The extraction of the anaphoric sentence and the candidates had to be redone.
",5.1 Datasets,[0],[0]
12We excluded any documents that are part of ARRAU.,5.1 Datasets,[0],[0]
"13ARRAU distinguishes abstract anaphors and (mostly)
pronominal anaphors referring to an action or plan, as plan.
",5.1 Datasets,[0],[0]
Candidates extraction.,5.1 Datasets,[0],[0]
"Following KZH13, for every anaphor we create a list of candidates by extracting all syntactic constituents from sentences which contain antecedents.",5.1 Datasets,[0],[0]
"Candidates that differ from antecedents in only one word, or one word and punctuation, were as well considered as antecedents.",5.1 Datasets,[0],[0]
"Constituents that are not antecedents are considered as negative candidates.
",5.1 Datasets,[0],[0]
Data statistics.,5.1 Datasets,[0],[0]
"Table 2 gives statistics of the datasets: the number of anaphors (row 1), the median length (in tokens) of antecedents (row 2), the median length (in tokens) for all anaphoric sentences (row 3), the median of the number of antecedents and candidates that are not antecedents (negatives) (rows 4–5), the number of pronominal and nominal anaphors (rows 6–7).",5.1 Datasets,[0],[0]
"Both training sets, artificial and CSN, have only one possible antecedent for which we accept two minimal variants differing in only one word or one word and punctuation.",5.1 Datasets,[0],[0]
"On the contrary, both test sets by design allow annotation of more than one antecedent that differ in more than one word.",5.1 Datasets,[0],[0]
"Every anaphor in the artificial training dataset is pronominal, whereas anaphors in CSN and ASN are nominal only.",5.1 Datasets,[0],[0]
"ARRAU-AA has a mixture of nominal and pronominal anaphors.
",5.1 Datasets,[0],[0]
Data pre-,5.1 Datasets,[0],[0]
processing.,5.1 Datasets,[0],[0]
Other details can be found in Supplementary Materials.,5.1 Datasets,[0],[0]
"Following KZH13, we report success@n (s@n), which measures whether the antecedent, or a candidate that differs in one word14, is in the first n ranked candidates, for n ∈ {1, 2, 3, 4}.",5.2 Baselines and evaluation metrics,[0],[0]
"Additionally, we report the preceding sentence baseline
14We obtained this information in personal communication with one of the authors.
(PSBL) that chooses the previous sentence for the antecedent and TAGbaseline (TAGBL) that randomly chooses a candidate with the constituent tag label in {S, VP, ROOT, SBAR}.",5.2 Baselines and evaluation metrics,[0],[0]
For TAGBL we report the average of 10 runs with 10 fixed seeds.,5.2 Baselines and evaluation metrics,[0],[0]
"PSBL always performs worse than the KZH13 model on the ASN, so we report it only for ARRAU-AA.",5.2 Baselines and evaluation metrics,[0],[0]
Hyperparameters tuning.,5.3 Training details for our models,[0],[0]
"We recorded performance with manually chosen HPs and then tuned HPs with Tree-structured Parzen Estimators (TPE) (Bergstra et al., 2011)15.",5.3 Training details for our models,[0],[0]
TPE chooses HPs for the next (out of 10) trails on the basis of the s@1 score on the devset.,5.3 Training details for our models,[0],[0]
As devsets we employ the ARRAUAA corpus for shell noun resolution and the ASN corpus for unrestricted abstract anaphora resolution.,5.3 Training details for our models,[0],[0]
For each trial we record performance on the test set.,5.3 Training details for our models,[0],[0]
We report the best test s@1 score in 10 trials if it is better than the scores from default HPs.,5.3 Training details for our models,[0],[0]
The default HPs and prior distributions for HPs used by TPE are given below.,5.3 Training details for our models,[0],[0]
"The (exact) HPs we used can be found in Supplementary Materials.
",5.3 Training details for our models,[0],[0]
Input representation.,5.3 Training details for our models,[0],[0]
"To construct word vectors wi as defined in Section 3, we used 100-dim.",5.3 Training details for our models,[0],[0]
"GloVe word embeddings pre-trained on the Gigaword and Wikipedia (Pennington et al., 2014), and did not fine-tune them.",5.3 Training details for our models,[0],[0]
"Vocabulary was built from the words in the training data with frequency in {3, U(1, 10)}, and OOV words were replaced with an UNK token.",5.3 Training details for our models,[0],[0]
"Embeddings for tags are initialized with values drawn from the uniform distribution U(− 1√
d+t , 1√ d+t
) , where t is the number of
tags16 and d ∈ {50, qlog-U(30, 100)} the size of the tag embeddings.17 We experimented with removing embeddings for tag, anaphor and context.
",5.3 Training details for our models,[0],[0]
Weights initialization.,5.3 Training details for our models,[0],[0]
"The size of the LSTMs hidden states was set to {100, qlog-U(30, 150)}.",5.3 Training details for our models,[0],[0]
"We initialized the weight matrices of the LSTMs with random orthogonal matrices (Henaff et al., 2016), all other weight matrices with the initialization proposed in He et al. (2015).",5.3 Training details for our models,[0],[0]
"The first feed-forward layer size is set to a value in {400, qlog-U(200, 800)}, the second to a value in {1024, qlog-U(400, 2000)}.",5.3 Training details for our models,[0],[0]
"Forget biases in the LSTM were initialized with 1s (Józefowicz et al., 2015), all other biases with 0s.
15https://github.com/hyperopt/hyperopt.",5.3 Training details for our models,[0],[0]
16We used a list of tags obtained from the Stanford Parser.,5.3 Training details for our models,[0],[0]
"17qlog-U is the so-called qlog-uniform distribution.
",5.3 Training details for our models,[0],[0]
Optimization.,5.3 Training details for our models,[0],[0]
"We trained our model in minibatches using Adam (Kingma and Ba, 2015) with the learning rate of 10−4 and maximal batch size 64.",5.3 Training details for our models,[0],[0]
"We clip gradients by global norm (Pascanu et al., 2013), with a clipping value in {1.0, U(1, 100)}.",5.3 Training details for our models,[0],[0]
"We train for 10 epochs and choose the model that performs best on the devset.
Regularization.",5.3 Training details for our models,[0],[0]
"We used the l2-regularization with λ ∈ {10−5, log-U(10−7, 10−2)}.",5.3 Training details for our models,[0],[0]
"Dropout (Srivastava et al., 2014) with a keep probability kp ∈ {0.8, U(0.5, 1.0)} was applied to the outputs of the LSTMs, both feed-forward layers and optionally to the input with kp ∈ U(0.8, 1.0).",5.3 Training details for our models,[0],[0]
Table 3 provides the results of the mentionranking model (MR-LSTM) on the ASN corpus using default HPs.,6.1 Results on shell noun resolution dataset,[0],[0]
"Column 2 states which model produced the results: KZH13 refers to the best reported results in Kolhatkar et al. (2013b) and TAGBL is the baseline described in Section 5.2.
",6.1 Results on shell noun resolution dataset,[0],[0]
"In terms of s@1 score, MR-LSTM outperforms both KZH13’s results and TAGBL without even necessitating HP tuning.",6.1 Results on shell noun resolution dataset,[0],[0]
"For the outlier reason we tuned HPs (on ARRAU-AA) for different variants of the architecture: the full architecture, without embedding of the context of the anaphor (ctx), of the anaphor (aa), of both constituent tag em-
bedding and shortcut (tag,cut), dropping only the shortcut (cut), using only word embeddings as input (ctx,aa,tag,cut), without the first (ffl1) and second (ffl2) layer.",6.1 Results on shell noun resolution dataset,[0],[0]
"From Table 4 we observe: (1) with HPs tuned on ARRAU-AA, we obtain results well beyond KZH13, (2) all ablated model variants perform worse than the full model, (3) a large performance drop when omitting syntactic information (tag,cut) suggests that the model makes good use of it.",6.1 Results on shell noun resolution dataset,[0],[0]
"However, this could also be due to a bias in the tag distribution, given that all candidates stem from the single sentence that contains antecedents.",6.1 Results on shell noun resolution dataset,[0],[0]
"The median occurrence of the S tag among both antecedents and negative candidates is 1, thus the model could achieve 50.00 s@1 by picking S-type constituents, just as TAGBL achieves 42.02 for reason and 48.66 for possibility.
",6.1 Results on shell noun resolution dataset,[0],[0]
Tuning of HPs gives us insight into how different model variants cope with the task.,6.1 Results on shell noun resolution dataset,[0],[0]
"For example, without tuning the model with and without syntactic information achieves 71.27 and 19.68 (not shown in table) s@1 score, respectively, and with tuning: 87.78 and 68.10.",6.1 Results on shell noun resolution dataset,[0],[0]
"Performance of 68.10 s@1 score indicates that the model is able to learn without syntactic guidance, contrary to the 19.68 s@1 score before tuning.",6.1 Results on shell noun resolution dataset,[0],[0]
"Table 5 shows the performance of different variants of the MR-LSTM with HPs tuned on the ASN corpus (always better than the default HPs), when evaluated on 3 different subparts of the ARRAUAA: all 600 abstract anaphors, 397 nominal and 203 pronominal ones.",6.2 Results on the ARRAU corpus,[0],[0]
"HPs were tuned on the ASN corpus for every variant separately, without shuffling of the training data.",6.2 Results on the ARRAU corpus,[0],[0]
"For the best performing variant, without syntactic information (tag,cut), we report the results with HPs that yielded the best s@1 test score for all anaphors (row 4), when training with those HPs on shuffled training data (row 5), and with HPs that yielded the best s@1
score for pronominal anaphors (row 6).",6.2 Results on the ARRAU corpus,[0],[0]
"The MR-LSTM is more successful in resolving nominal than pronominal anaphors, although the training data provides only pronominal ones.",6.2 Results on the ARRAU corpus,[0],[0]
"This indicates that resolving pronominal abstract anaphora is harder compared to nominal abstract anaphora, such as shell nouns.",6.2 Results on the ARRAU corpus,[0],[0]
"Moreover, for shell noun resolution in KZH13’s dataset, the MR-LSTM achieved s@1 scores in the range 76.09–93.14, while the best variant of the model achieves 51.89 s@1 score for nominal anaphors in ARRAU-AA.",6.2 Results on the ARRAU corpus,[0],[0]
"Although lower performance is expected, since we do not have specific training data for individual nominals in ARRAU-AA, we suspect that the reason for better performance for shell noun resolution in KZH13 is due to a larger number of positive candidates in ASN (cf. Table 2, rows: antecedents/negatives).
",6.2 Results on the ARRAU corpus,[0],[0]
We also note that HPs that yield good performance for resolving nominal anaphors are not necessarily good for pronominal ones (cf. rows 4–6 in Table 5).,6.2 Results on the ARRAU corpus,[0],[0]
"Since the TPE tuner was tuned on the nominal-only ASN data, this suggest that it would be better to tune HPs for pronominal anaphors on a different dataset or stripping the nouns in ASN.
",6.2 Results on the ARRAU corpus,[0],[0]
"Contrary to shell noun resolution, omitting syntactic information boosts performance in ARRAUAA.",6.2 Results on the ARRAU corpus,[0],[0]
"We conclude that when the model is provided with syntactic information, it learns to pick S-type candidates, but does not continue to learn deeper features to further distinguish them or needs more data to do so.",6.2 Results on the ARRAU corpus,[0],[0]
"Thus, the model is not able to point to exactly one antecedent, resulting in a lower s@1 score, but does well in picking a few good candidates, which yields good s@2-4 scores.",6.2 Results on the ARRAU corpus,[0],[0]
"This is what we can observe from row 2 vs. row 6 in Table 5: the MR-LSTM without context embedding
(ctx) achieves a comparable s@2 score with the variant that omits syntactic information, but better s@3-4 scores.",6.2 Results on the ARRAU corpus,[0],[0]
"Further, median occurrence of tags not in {S, VP, ROOT, SBAR} among top-4 ranked candidates is 0 for the full architecture, and 1 when syntactic information is omitted.",6.2 Results on the ARRAU corpus,[0],[0]
"The need for discriminating capacity of the model is more emphasized in ARRAU-AA, given that the median occurrence of S-type candidates among negatives is 2 for nominal and even 3 for pronominal anaphors, whereas it is 1 for ASN.",6.2 Results on the ARRAU corpus,[0],[0]
"This is in line with the lower TAGBL in ARRAU-AA.
",6.2 Results on the ARRAU corpus,[0],[0]
"Finally, not all parts of the architecture contribute to system performance, contrary to what is observed for reason.",6.2 Results on the ARRAU corpus,[0],[0]
"For nominal anaphors, the anaphor (aa) and feed-forward layers (ffl1, ffl2) are beneficial, for pronominals only the second ffl.",6.2 Results on the ARRAU corpus,[0],[0]
"We finally analyze deeper aspects of the model: (1) whether a learned representation between the anaphoric sentence and an antecedent establishes a relation between a specific anaphor we want to resolve and the antecedent and (2) whether the maxmargin objective enforces a separation of the joint representations in the shared space.
",6.3 Exploring the model,[0],[0]
(1) We claim that by providing embeddings of both the anaphor and the sentence containing the anaphor we ensure that the learned relation between antecedent and anaphoric sentence is dependent on the anaphor under consideration.,6.3 Exploring the model,[0],[0]
Fig. 3 illustrates the heatmap for an anaphoric sentence with two anaphors.,6.3 Exploring the model,[0],[0]
The i-th column of the heatmap corresponds to absolute differences between the output of the bi-LSTM for the i-th word in the anaphoric sentence when the first vs. second anaphor is resolved.,6.3 Exploring the model,[0],[0]
"Stronger color indi-
cates larger difference, the blue rectangle represents the column for the head of the first anaphor, the dashed blue rectangle the column for the head of the second anaphor.",6.3 Exploring the model,[0],[0]
"Clearly, the representations differ when the first vs. second anaphor is being resolved and consequently, joint representations with an antecedent will differ too.
",6.3 Exploring the model,[0],[0]
(2) It is known that the max-margin objective separates the best-scoring positive candidate from the best-scoring negative candidate.,6.3 Exploring the model,[0],[0]
"To investigate what the objective accomplishes in the MRLSTM model, we analyze the joint representations of candidates and the anaphoric sentence (i.e., outputs of ffl2) after training.",6.3 Exploring the model,[0],[0]
"For a randomly chosen instance from ARRAU-AA, we plotted outputs of ffl2 with the tSNE algorithm (v.d. Maaten and Hinton, 2008).",6.3 Exploring the model,[0],[0]
Fig. 4 illustrates that the joint representation of the first ranked candidate and the anaphoric sentence is clearly separated from other joint representations.,6.3 Exploring the model,[0],[0]
This shows that the maxmargin objective separates the best scoring positive candidate from the best scoring negative candidate by separating their respective joint representations with the anaphoric sentence.,6.3 Exploring the model,[0],[0]
"We presented a neural mention-ranking model for the resolution of unconstrained abstract anaphora, and applied it to two datasets with different types of abstract anaphora: the shell noun dataset and a subpart of ARRAU with (pro)nominal abstract anaphora of any type.",7 Conclusions,[0],[0]
To our knowledge this work is the first to address the unrestricted abstract anaphora resolution task with a neural network.,7 Conclusions,[0],[0]
"Our model also outperforms state-of-the-art results on the shell noun dataset.
",7 Conclusions,[0],[0]
"In this work we explored the use of purely artificially created training data and how far it can bring
us.",7 Conclusions,[0],[0]
"In future work, we plan to investigate mixtures of (more) artificial and natural data from different sources (e.g. ASN, CSN).
",7 Conclusions,[0],[0]
"On the more challenging ARRAU-AA, we found model variants that surpass the baselines for the entire and the nominal part of ARRAU-AA, although we do not train models on individual (nominal) anaphor training data like the related work for shell noun resolution.",7 Conclusions,[0],[0]
"However, our model still lags behind for pronominal anaphors.",7 Conclusions,[0],[0]
"Our results suggest that models for nominal and pronominal anaphors should be learned independently, starting with tuning of HPs on a more suitable devset for pronominal anaphors.
",7 Conclusions,[0],[0]
"We show that the model can exploit syntactic information to select plausible candidates, but that when it does so, it does not learn how to distinguish candidates of equal syntactic type.",7 Conclusions,[0],[0]
"By contrast, if the model is not provided with syntactic information, it learns deeper features that enable it to pick the correct antecedent without narrowing down the choice of candidates.",7 Conclusions,[0],[0]
"Thus, in order to improve performance, the model should be enforced to first select reasonable candidates and then continue to learn features to distinguish them, using a larger training set that is easy to provide.
",7 Conclusions,[0],[0]
"In future work we will design such a model, and offer it candidates chosen not only from sentences containing the antecedent, but the larger context.",7 Conclusions,[0],[0]
This work has been supported by the German Research Foundation as part of the Research Training Group Adaptive Preparation of Information from Heterogeneous Sources (AIPHES) under grant No.,Acknowledgments,[0],[0]
GRK 1994/1.,Acknowledgments,[0],[0]
We would like to thank anonymous reviewers for useful comments and especially thank Todor Mihaylov for the model implementations advices and everyone in the Computational Linguistics Group for helpful discussion.,Acknowledgments,[0],[0]
"Resolving abstract anaphora is an important, but difficult task for text understanding.",abstractText,[0],[0]
"Yet, with recent advances in representation learning this task becomes a more tangible aim.",abstractText,[0],[0]
A central property of abstract anaphora is that it establishes a relation between the anaphor embedded in the anaphoric sentence and its (typically non-nominal) antecedent.,abstractText,[0],[0]
We propose a mention-ranking model that learns how abstract anaphors relate to their antecedents with an LSTM-Siamese Net.,abstractText,[0],[0]
We overcome the lack of training data by generating artificial anaphoric sentence– antecedent pairs.,abstractText,[0],[0]
Our model outperforms state-of-the-art results on shell noun resolution.,abstractText,[0],[0]
We also report first benchmark results on an abstract anaphora subset of the ARRAU corpus.,abstractText,[0],[0]
This corpus presents a greater challenge due to a mixture of nominal and pronominal anaphors and a greater range of confounders.,abstractText,[0],[0]
"We found model variants that outperform the baselines for nominal anaphors, without training on individual anaphor data, but still lag behind for pronominal anaphors.",abstractText,[0],[0]
Our model selects syntactically plausible candidates and – if disregarding syntax – discriminates candidates using deeper features.,abstractText,[0],[0]
A Mention-Ranking Model for Abstract Anaphora Resolution,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 818–827 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1076",text,[0],[0]
This paper presents a minimal but surprisingly effective span-based neural model for constituency parsing.,1 Introduction,[0],[0]
"Recent years have seen a great deal of interest in parsing architectures that make use of recurrent neural network (RNN) representations of input sentences (Vinyals et al., 2015).",1 Introduction,[0],[0]
"Despite evidence that linear RNN decoders are implicitly able to respect some nontrivial well-formedness constraints on structured outputs (Graves, 2013), researchers have consistently found that the best performance is achieved by systems that explicitly require the decoder to generate well-formed tree structures (Chen and Manning, 2014).
",1 Introduction,[0],[0]
There are two general approaches to ensuring this structural consistency.,1 Introduction,[0],[0]
The most common is to encode the output as a sequence of operations within a transition system which constructs trees incrementally.,1 Introduction,[0],[0]
"This transforms the parsing problem back into a sequence-to-sequence problem, while making it easy to force the decoder to take only actions guaranteed to produce well-formed
outputs.",1 Introduction,[0],[0]
"However, transition-based models do not admit fast dynamic programs and require careful feature engineering to support exact search-based inference (Thang et al., 2015).",1 Introduction,[0],[0]
"Moreover, models with recurrent state require complex training procedures to benefit from anything other than greedy decoding (Wiseman and Rush, 2016).
",1 Introduction,[0],[0]
"An alternative line of work focuses on chart parsers, which use log-linear or neural scoring potentials to parameterize a tree-structured dynamic program for maximization or marginalization (Finkel et al., 2008; Durrett and Klein, 2015).",1 Introduction,[0],[0]
"These models enjoy a number of appealing formal properties, including support for exact inference and structured loss functions.",1 Introduction,[0],[0]
"However, previous chart-based approaches have required considerable scaffolding beyond a simple well-formedness potential, e.g. pre-specification of a complete context-free grammar for generating output structures and initial pruning of the output space with a weaker model (Hall et al., 2014).",1 Introduction,[0],[0]
"Additionally, we are unaware of any recent chartbased models that achieve results competitive with the best transition-based models.
",1 Introduction,[0],[0]
"In this work, we present an extremely simple chart-based neural parser based on independent scoring of labels and spans, and show how this model can be adapted to support a greedy topdown decoding procedure.",1 Introduction,[0],[0]
"Our goal is to preserve the basic algorithmic properties of span-oriented (rather than transition-oriented) parse representations, while exploring the extent to which neural representational machinery can replace the additional structure required by existing chart parsers.",1 Introduction,[0],[0]
"On the Penn Treebank, our approach outperforms a number of recent models for chart-based and transition-based parsing—including the state-ofthe-art models of Cross and Huang (2016) and Liu and Zhang (2016)—achieving an F1 score of 91.79.",1 Introduction,[0],[0]
"We additionally obtain a strong F1 score of 82.23 on the French Treebank.
818",1 Introduction,[0],[0]
A constituency tree can be regarded as a collection of labeled spans over a sentence.,2 Model,[0],[0]
"Taking this view as a guiding principle, we propose a model with two components, one which assigns scores to span labels and one which assigns scores directly to span existence.",2 Model,[0],[0]
"The former is used to determine the labeling of the output, and the latter provides its structure.
",2 Model,[0],[0]
At the core of both of these components is the issue of span representation.,2 Model,[0],[0]
"Given that a span’s correct label and its quality as a constituent depend heavily on the context in which it appears, we naturally turn to recurrent neural networks as a starting point, since they have previously been shown to capture contextual information suitable for use in a variety of natural language applications (Bahdanau et al., 2014; Wang et al., 2015)
",2 Model,[0],[0]
"In particular, we run a bidirectional LSTM over the input to obtain context-sensitive forward and backward encodings for each position i, denoted by fi and bi, respectively.",2 Model,[0],[0]
"Our representation of the span (i, j) is then the concatenatation the vector differences fj",2 Model,[0],[0]
− fi and bi − bj .,2 Model,[0],[0]
"This corresponds to a bidirectional version of the LSTMMinus features first proposed by Wang and Chang (2016).
",2 Model,[0],[0]
"On top of this base, our label and span scoring functions are implemented as one-layer feedforward networks, taking as input the concatenated span difference and producing as output either a vector of label scores or a single span score.",2 Model,[0],[0]
"More formally, letting sij denote the vector representation of span (i, j), we define
slabels(i, j) = V`g(W`sij + b`),
sspan(i, j) = v > s g(Wssij + bs),
where g denotes an elementwise nonlinearity.",2 Model,[0],[0]
"For notational convenience, we also let the score of an individual label ` be denoted by
slabel(i, j, `) =",2 Model,[0],[0]
"[slabels(i, j)]`,
where the right-hand side is the corresponding element of the label score vector.
",2 Model,[0],[0]
"One potential issue is the existence of unary chains, corresponding to nested labeled spans with the same endpoints.",2 Model,[0],[0]
We take the common approach of treating these as additional atomic labels alongside all elementary nonterminals.,2 Model,[0],[0]
"To accommodate n-ary trees, our inventory additionally includes a special empty label ∅ used for spans that
are not themselves full constituents but arise during the course of implicit binarization.
",2 Model,[0],[0]
Our model shares several features in common with that of Cross and Huang (2016).,2 Model,[0],[0]
"In particular, our representation of spans and the form of our label scoring function were directly inspired by their work, as were our handling of unary chains and our use of an empty label.",2 Model,[0],[0]
"However, our approach differs in its treatment of structural decisions, and consequently, the inference algorithms we describe below diverge significantly from their transition-based framework.",2 Model,[0],[0]
Our basic model is compatible with traditional chart-based dynamic programming.,3 Chart Parsing,[0],[0]
"Representing a constituency tree T by its labeled spans,
T := {(`t, (it, jt)) : t = 1, . . .",3 Chart Parsing,[0],[0]
", |T |},
we define the score of a tree to be the sum of its constituent label and span scores,
stree(T ) = ∑
(`,(i,j))∈T [slabel(i, j, `) +",3 Chart Parsing,[0],[0]
"sspan(i, j)] .
",3 Chart Parsing,[0],[0]
"To find the tree with the highest score for a given sentence, we use a modified CKY recursion.",3 Chart Parsing,[0],[0]
"As with classical chart parsing, the running time of our procedure is O(n3) for a sentence of length n.",3 Chart Parsing,[0],[0]
"The base case is a span (i, i + 1) consisting of a single word.",3.1 Dynamic Program for Inference,[0],[0]
"Since every valid tree must include all singleton spans, possibly with an empty label, we need not consider the span score in this case and perform only a single maximization over the choice of label:
sbest(i, i + 1) = max ` [slabel(i, i + 1, `)] .
",3.1 Dynamic Program for Inference,[0],[0]
"For a general span (i, j), we define the score of the split (i, k, j) as the sum of its subspan scores,
ssplit(i, k, j) =",3.1 Dynamic Program for Inference,[0],[0]
"sspan(i, k) +",3.1 Dynamic Program for Inference,[0],[0]
"sspan(k, j).",3.1 Dynamic Program for Inference,[0],[0]
"(1)
For convenience, we also define an augmented split score incorporating the scores of the corresponding subtrees,
s̃split(i, k, j) = ssplit(i, k, j)
+ sbest(i, k) + sbest(k, j).
",3.1 Dynamic Program for Inference,[0],[0]
"Using these quantities, we can then write the general joint label and split decision as
sbest(i, j) = max `,k",3.1 Dynamic Program for Inference,[0],[0]
"[slabel(i, j, `) + s̃split(i, k, j)] .
(2)
Because our model assigns independent scores to labels and spans, this maximization decomposes into two disjoint subproblems, greatly reducing the size of the state space:
sbest(i, j) = max ` [slabel(i, j, `)]
+ max k",3.1 Dynamic Program for Inference,[0],[0]
"[s̃split(i, k, j)] .
We also note that the span scores sspan(i, j) for each span (i, j) in the sentence can be computed once at the beginning of the procedure and shared across different subproblems with common left or right endpoints, allowing for a quadratic rather than cubic number of span score computations.",3.1 Dynamic Program for Inference,[0],[0]
Training the model under this inference scheme is accomplished using a margin-based approach.,3.2 Margin Training,[0],[0]
"When presented with an example sentence and its corresponding parse tree T ∗, we compute the best prediction under the current model using the above dynamic program,
T̂ = argmax T",3.2 Margin Training,[0],[0]
"[stree(T )] .
",3.2 Margin Training,[0],[0]
"If T̂ = T ∗, then our prediction was correct and no changes need to be made.",3.2 Margin Training,[0],[0]
"Otherwise, we incur a hinge penalty of the form
max ( 0, 1− stree(T ∗)",3.2 Margin Training,[0],[0]
"+ stree(T̂ ) )
to encourage the model to keep a margin of at least 1 between the gold tree and the best alternative.",3.2 Margin Training,[0],[0]
"The loss to be minimized is then the sum of penalties across all training examples.
",3.2 Margin Training,[0],[0]
"Prior work has found that it can be beneficial in a variety of applications to incorporate a structured loss function into this margin objective, replacing the hinge penalty above with one of the form
max ( 0, ∆(T̂ , T ∗)− stree(T ∗)",3.2 Margin Training,[0],[0]
"+ stree(T̂ ) )
",3.2 Margin Training,[0],[0]
for a loss function ∆ that measures the similarity between the prediction T̂ and the reference T ∗.,3.2 Margin Training,[0],[0]
Here we take ∆ to be a Hamming loss on labeled spans.,3.2 Margin Training,[0],[0]
"To incorporate this loss into the training objective, we modify the dynamic program of
Section 3.1 to support loss-augmented decoding (Taskar et al., 2005).",3.2 Margin Training,[0],[0]
"Since the label decisions are isolated from the structural decisions, it suffices to replace every occurrence of the label scoring function slabel(i, j, `) by
slabel(i, j, `)",3.2 Margin Training,[0],[0]
"+ 1(` 6= `∗ij),
where `∗ij is the label of span (i, j) in the gold tree T ∗.",3.2 Margin Training,[0],[0]
"This has the effect of requiring larger margins between the gold tree and predictions that contain more mistakes, offering a greater degree of robustness and better generalization.",3.2 Margin Training,[0],[0]
"While we have so far motivated our model from the perspective of classical chart parsing, it also allows for a novel inference algorithm in which trees are constructed greedily from the top down.",4 Top-Down Parsing,[0],[0]
"At a high level, given a span, we independently assign it a label and pick a split point, then repeat this process for the left and right subspans; the recursion bottoms out with length-one spans that can no longer be split.",4 Top-Down Parsing,[0],[0]
"Figure 1 gives an illustration of the process, which we describe in more detail below.
",4 Top-Down Parsing,[0],[0]
"The base case is again a singleton span (i, i+1), and follows the same form as the base case for the chart parser.",4 Top-Down Parsing,[0],[0]
"In particular, we select the label ̂̀that satisfies
̂̀= argmax ` [slabel(i, i + 1, `)] ,
omitting span scores from consideration since singleton spans cannot be split.
",4 Top-Down Parsing,[0],[0]
"To construct a tree over a general span (i, j), we aim to solve the maximization problem
(̂̀, k̂) =",4 Top-Down Parsing,[0],[0]
"argmax `,k",4 Top-Down Parsing,[0],[0]
"[slabel(i, j, `) + ssplit(i, k, j)] ,
where ssplit(i, k, j) is defined as in Equation (1).",4 Top-Down Parsing,[0],[0]
"The independence of our label and span scoring functions again yields the decomposed form
̂̀= argmax `",4 Top-Down Parsing,[0],[0]
"[slabel(i, j, `)] ,
k̂ = argmax k
[ssplit(i, k, j)] , (3)
leading to a significant reduction in the size of the state space.
To generate a tree for the whole sentence, we call this procedure on the full sentence span (0, n) and return the result.",4 Top-Down Parsing,[0],[0]
"As there are O(n) spans each
requiring one label evaluation and at most n − 1 split point evaluations, the running time of the procedure is O(n2).
",4 Top-Down Parsing,[0],[0]
"The algorithm outlined here bears a strong resemblance to the chart parsing dynamic program discussed in Section 3, but differs in one key aspect.",4 Top-Down Parsing,[0],[0]
"When performing inference from the bottom up, we have already computed the scores of all of the subtrees below the current span, and we can take this knowledge into consideration when selecting a split point.",4 Top-Down Parsing,[0],[0]
"In contrast, when producing a tree from the top down, we can only select a split point based on top-level evaluations of span quality, without knowing anything about the subtrees that will be generated below them.",4 Top-Down Parsing,[0],[0]
"This difference is manifested in the augmented split score s̃split used in the definition of sbest in Equation (2), where the scores of the subtrees associated with a split point are included in the chart recursion but necessarily excluded from the top-down recursion.
",4 Top-Down Parsing,[0],[0]
"While this apparent deficiency may be a cause for concern, we demonstrate the surprising empirical result in Section 6 that there is no loss in per-
formance when moving from the globally-optimal chart parser to the greedy top-down procedure.",4 Top-Down Parsing,[0],[0]
"As with the chart parsing formulation, we also use a margin-based method for learning under the topdown model.",4.1 Margin Training,[0],[0]
"However, rather than requiring separation between the scores of full trees, we instead enforce a local margin at every decision point.
",4.1 Margin Training,[0],[0]
"For a span (i, j) occurring in the gold tree, let `∗ and k∗ represent the correct label and split point, and let ̂̀and k̂ be the predictions made by computing the maximizations in Equation (3).",4.1 Margin Training,[0],[0]
"If ̂̀ 6= `∗, meaning the prediction is incorrect, we incur a hinge penalty of the form
max ( 0, 1− slabel(i, j, `∗) +",4.1 Margin Training,[0],[0]
"slabel(i, j, ̂̀) ) .
",4.1 Margin Training,[0],[0]
"Similarly, if k̂ 6= k∗, we incur a hinge penalty of the form
max ( 0, 1− ssplit(i, k∗, j) + ssplit(i, k̂, j) ) .
",4.1 Margin Training,[0],[0]
"To obtain the loss for a given training example, we trace out the actions corresponding to the gold tree and accumulate the above penalties over all decision points.",4.1 Margin Training,[0],[0]
"As before, the total loss to be minimized is the sum of losses across all training examples.
",4.1 Margin Training,[0],[0]
"Loss augmentation is also beneficial for the local decisions made by the top-down model, and can be implemented in a manner akin to the one discussed in Section 3.2.",4.1 Margin Training,[0],[0]
"The hinge penalties given above are only defined for spans (i, j) that appear in the example tree.",4.2 Training with Exploration,[0],[0]
"The model must therefore be constrained at training time to follow decisions that exactly reproduce the gold tree, since supervision cannot be provided otherwise.",4.2 Training with Exploration,[0],[0]
"As a result, the model is never exposed to its mistakes, which can lead to a lack of calibration and poor performance at test time.
",4.2 Training with Exploration,[0],[0]
"To circumvent this issue, a dynamic oracle can be defined to inform the model about correct behavior even after it has deviated from the gold tree.",4.2 Training with Exploration,[0],[0]
"Cross and Huang (2016) propose such an oracle for a related transition-based parsing system, and prove its optimality for the F1 metric on labeled spans.",4.2 Training with Exploration,[0],[0]
"We adapt their result here to obtain a dynamic oracle for the present model with similar guarantees.
",4.2 Training with Exploration,[0],[0]
"The oracle for labeling decisions carries over without modification: the correct label for a span is the label assigned to that span if it is part of the gold tree, or the empty label ∅ otherwise.
",4.2 Training with Exploration,[0],[0]
"For split point decisions, the oracle can be broken down into two cases.",4.2 Training with Exploration,[0],[0]
"If a span (i, j) appears as a constituent in the gold tree T , we let b(i, j) denote the collection of its interior boundary points.",4.2 Training with Exploration,[0],[0]
"For example, if the constituent over (1, 7) has children spanning (1, 3), (3, 6), and (6, 7), then we would have the two interior boundary points, b(1, 7) =",4.2 Training with Exploration,[0],[0]
"{3, 6}.",4.2 Training with Exploration,[0],[0]
The oracle for a span appearing in the gold tree is then precisely the output of this function.,4.2 Training with Exploration,[0],[0]
"Otherwise, for spans (i, j) not corresponding to gold constituents, we must instead identify the smallest enclosing gold constituent:
(i∗, j∗) = min{(i′, j′) ∈ T : i′ ≤",4.2 Training with Exploration,[0],[0]
"i < j ≤ j′}, where the minimum is taken with respect to the partial ordering induced by span length.",4.2 Training with Exploration,[0],[0]
"The output of the oracle is then the set of interior boundary points of this enclosing span that also lie inside the original, {k ∈ b(i∗, j∗) : i < k",4.2 Training with Exploration,[0],[0]
"< j}.
",4.2 Training with Exploration,[0],[0]
"The proof of correctness is similar to the proof in Cross and Huang (2016); we refer to the Dynamic Oracle section in their paper for a more detailed discussion.
",4.2 Training with Exploration,[0],[0]
"As presented, the dynamic oracle for split point decisions returns a collection of one or more splits rather than a single correct answer.",4.2 Training with Exploration,[0],[0]
"Any of these is a valid choice, with different splits corresponding to different binarizations of the original n-ary tree.",4.2 Training with Exploration,[0],[0]
"We choose to use the leftmost split point for consistency in our implementation, but remark that the oracle split with the highest score could also be chosen at training time to allow for additional flexibility.
",4.2 Training with Exploration,[0],[0]
"Having defined the dynamic oracle for our system, we note that training with exploration can be implemented by a single modification to the procedure described in Section 4.1.",4.2 Training with Exploration,[0],[0]
"Local penalties are accumulated as before, but instead of tracing out the decisions required to produce the gold tree, we instead follow the decisions predicted by the model.",4.2 Training with Exploration,[0],[0]
"In this way, supervision is provided at states within the prediction procedure that are more likely to arise at test time when greedy inference is performed.",4.2 Training with Exploration,[0],[0]
The model presented in Section 2 is designed to be as simple as possible.,5 Scoring and Loss Alternatives,[0],[0]
"However, there are many variations of the label and span scoring functions that could be explored; we discuss some of the options here.",5 Scoring and Loss Alternatives,[0],[0]
"Our basic model treats the empty label, elementary nonterminals, and unary chains each as atomic units, obscuring similarities between unary chains and their component nonterminals or between different unary chains with common prefixes or suffixes.",5.1 Top-Middle-Bottom Label Scoring,[0],[0]
"To address this lack of structure, we consider an alternative scoring scheme in which labels are predicted in three parts: a top nonterminal, a middle unary chain, and a bottom nonterminal (each of which is possibly empty).1",5.1 Top-Middle-Bottom Label Scoring,[0],[0]
"This not only allows for parameter sharing across labels with common subcomponents, but also has the added benefit of allowing the model to produce novel unary chains at test time.
",5.1 Top-Middle-Bottom Label Scoring,[0],[0]
"1In more detail, ∅ decomposes as (∅, ∅, ∅), X decomposes as (X , ∅, ∅), X–Y decomposes as (X , ∅, Y ), and X–Z1– · · · –Zk–Y decomposes as (X , Z1– · · · –Zk, Y ).
",5.1 Top-Middle-Bottom Label Scoring,[0],[0]
"More precisely, we introduce the decomposition
slabel(i, j, (`t, `m, `b))",5.1 Top-Middle-Bottom Label Scoring,[0],[0]
"=
stop(i, j, `t) +",5.1 Top-Middle-Bottom Label Scoring,[0],[0]
"smiddle(i, j, `m) +",5.1 Top-Middle-Bottom Label Scoring,[0],[0]
"sbottom(i, j, `b),
where stop, smiddle, and sbottom are independent one-layer feedforward networks of the same form as slabel that output vectors of scores for all label tops, label middle chains, and label bottoms encountered in the training corpus, respectively.",5.1 Top-Middle-Bottom Label Scoring,[0],[0]
"The best label for a span (i, j) is then computed by solving the maximization problem
max `t,`m,`b [slabel(i, j, (`t, `m, `b))] ,
which decomposes into three independent subproblems corresponding to the three label components.",5.1 Top-Middle-Bottom Label Scoring,[0],[0]
"The final label is obtained by concatenating `t, `m, and `b, with empty components being omitted from the concatenation.",5.1 Top-Middle-Bottom Label Scoring,[0],[0]
The basic model uses the same span scoring function sspan to assign a score to the left and right subspans of a given span.,5.2 Left and Right Span Scoring,[0],[0]
"One simple extension is to replace this by a pair of distinct left and right feedforward networks of the same form, giving the decomposition
ssplit(i, k, j) = sleft(i, k) + sright(k, j).",5.2 Left and Right Span Scoring,[0],[0]
"Since span scores are only used to score splits in our model, we also consider directly scoring a split by feeding the concatenation of the span representations of the left and right subspans through a single feedforward network, giving
ssplit(i, k, j) = v > s g (Ws[sik; skj ] + bs) .
",5.3 Span Concatenation Scoring,[0],[0]
"This is similar to the structural scoring function used by Cross and Huang (2016), although whereas they additionally include features for the outside spans (0, i) and (j, n) in their concatenation, we omit these from our implementation, finding that they do not improve performance.",5.3 Span Concatenation Scoring,[0],[0]
"Inspired by the success of deep biaffine scoring in recent work by Dozat and Manning (2016) for dependency parsing, we also consider a split scoring function of a similar form for our model.",5.4 Deep Biaffine Span Scoring,[0],[0]
"Specifically, we let hik = fleft(sik) and hkj =
fright(skj) be deep left and right span representations obtained by passing the child vectors through corresponding left and right feedforward networks.",5.4 Deep Biaffine Span Scoring,[0],[0]
"We then define the biaffine split scoring function
ssplit(i, k, j) = h >",5.4 Deep Biaffine Span Scoring,[0],[0]
"ikWshkj + v > lefthik + v > righthkj ,
which consists of the sum of a bilinear form between the two hidden representations together with two inner products.",5.4 Deep Biaffine Span Scoring,[0],[0]
The three-way label scoring scheme described in Section 5.1 offers one path towards the incorporation of label structure into the model.,5.5 Structured Label Loss,[0],[0]
We additionally consider a structured Hamming loss on labels.,5.5 Structured Label Loss,[0],[0]
"More specifically, given two labels `1 and `2 consisting of zero or more nonterminals, we define the loss as |`1 \ `2|+ |`2 \ `1|, treating each label as a multiset of nonterminals.",5.5 Structured Label Loss,[0],[0]
This structured loss can be incorporated into the training process using the methods described in Sections 3.2 and 4.1.,5.5 Structured Label Loss,[0],[0]
We first describe the general setup used for our experiments.,6 Experiments,[0],[0]
"We use the Penn Treebank (Marcus et al., 1993) for our English experiments, with standard splits of sections 2-21 for training, section 22 for development, and section 23 for testing.",6 Experiments,[0],[0]
"We use the French Treebank from the SPMRL 2014 shared task (Seddah et al., 2014) with its provided splits for our French experiments.",6 Experiments,[0],[0]
"No token preprocessing is performed, and only a single <UNK> token is used for unknown words at test time.",6 Experiments,[0],[0]
The inputs to our system are concatenations of 100-dimensional word embeddings and 50-dimensional part-of-speech embeddings.,6 Experiments,[0],[0]
"In the case of the French Treebank, we also include 50-dimensional embeddings of each morphological tag.",6 Experiments,[0],[0]
"We use automatically predicted tags for training and testing, obtaining predicted part-ofspeech tags for the Penn Treebank using the Stanford tagger (Toutanova et al., 2003) with 10-way jackknifing, and using the provided predicted partof-speech and morphological tags for the French Treebank.",6 Experiments,[0],[0]
Words are replaced by <UNK> with probability 1/(1+freq(w)),6 Experiments,[0],[0]
"during training, where freq(w) is the frequency of w in the training data.
",6 Experiments,[0],[0]
We use a two-layer bidirectional LSTM for our base span features.,6 Experiments,[0],[0]
"Dropout with a ratio selected from {0.2, 0.3, 0.4} is applied to all non-recurrent
connections of the LSTM, including its inputs and outputs.",6 Experiments,[0],[0]
"We tie the hidden dimension of the LSTM and all feedforward networks, selecting a size from {150, 200, 250}.",6 Experiments,[0],[0]
"All parameters (including word and tag embeddings) are randomly initialized using Glorot initialization (Glorot and Bengio, 2010), and are tuned on development set performance.",6 Experiments,[0],[0]
"We use the Adam optimizer (Kingma and Ba, 2014) with its default settings for optimization, with a batch size of 10.",6 Experiments,[0],[0]
"Our system is implemented in C++ using the DyNet neural network library (Neubig et al., 2017).
",6 Experiments,[0],[0]
We begin by training the minimal version of our proposed chart and top-down parsers on the Penn Treebank.,6 Experiments,[0],[0]
"Out of the box, we obtain test F1 scores of 91.69 for the chart parser and 91.58 for the topdown parser.",6 Experiments,[0],[0]
"The higher of these matches the recent state-of-the-art score of 91.7 reported by Liu and Zhang (2016), demonstrating that our simple neural parsing system is already capable of achieving strong results.
",6 Experiments,[0],[0]
"Building on this, we explore the effects of different split scoring functions when using either the basic 0-1 label loss or the structured label loss discussed in Section 5.5.",6 Experiments,[0],[0]
"Our results are presented in Tables 1a and 1b.
",6 Experiments,[0],[0]
"We observe that regardless of the label loss, the minimal and deep biaffine split scoring schemes perform a notch below the left-right and concatenation scoring schemes.",6 Experiments,[0],[0]
"That the minimal scoring scheme performs worse than the left-right scheme is unsurprising, since the latter is a strict generalization of the former.",6 Experiments,[0],[0]
"It is evident, however,
that joint scoring of left and right subspans is not required for strong results—in fact, the left-right scheme which scores child subspans in isolation slightly outperforms the concatenation scheme in all but one case, and is stronger than the deep biaffine scoring function across the board.
",6 Experiments,[0],[0]
"Comparing results across the choice of label loss, however, we find that fewer trends are apparent.",6 Experiments,[0],[0]
"The scores obtained by training with a 0-1 loss are all within 0.1 of those obtained using a structured Hamming loss, being slightly higher in four out of eight cases and slightly lower in the other half.",6 Experiments,[0],[0]
"This leads us to conclude that the more elementary approach is sufficient when selecting atomic labels from a fixed inventory.
",6 Experiments,[0],[0]
We also perform the same set of experiments under the setting where the top-middle-bottom label scoring function described in Section 5.1 is used in place of an atomic label scoring function.,6 Experiments,[0],[0]
"These results are shown in Tables 1c and 1d.
",6 Experiments,[0],[0]
"A priori, we might expect that exposing additional structure would allow the model to make better predictions, but on the whole we find that the scores in this set of experiments are worse than those in the previous set.",6 Experiments,[0],[0]
"Trends similar to before hold across the different choices of scoring functions, though in this case the minimal setting has scores closer to those of the left-right setting, even exceeding its performance in the case of a chart parser with a 0-1 label loss.
",6 Experiments,[0],[0]
"Our final test results are given in Table 2, along with the results of other recent single-model parsers trained without external parse data.",6 Experiments,[0],[0]
"We
achieve a new state-of-the-art F1 score of 91.79 with our best model.",6 Experiments,[0],[0]
"Interestingly, we observe that our parsers have a noticeably higher gap between precision and recall than do other top parsers, likely owing to the structured label loss which penalizes mismatching nonterminals more heavily than it does a nonterminal and empty label mismatch.",6 Experiments,[0],[0]
"In addition, there is little difference between the best top-down model and the best chart model, indicating that global normalization is not required to achieve strong results.",6 Experiments,[0],[0]
"Processing one sentence at a time on a c4.4xlarge Amazon EC2 instance, our best chart and top-down parsers operate at speeds of 20.3 sentences per second and 75.5 sentences per second, respectively, as measured on the test set.
",6 Experiments,[0],[0]
"We additionally train parsers on the French Treebank using the same settings from our English experiments, selecting the best model of each type based on development performance.",6 Experiments,[0],[0]
We list our test results along with those of several other recent papers in Table 3.,6 Experiments,[0],[0]
"Although we fall short of the scores obtained by Cross and Huang (2016), we achieve competitive performance relative to the neural CRF parser of Durrett and Klein (2015).",6 Experiments,[0],[0]
"Many early successful approaches to constituency parsing focused on rich modeling of correlations in the output space, typically by engineering proabilistic context-free grammars with state spaces enriched to capture long-distance dependencies and lexical phenomena (Collins, 2003; Klein and Manning, 2003; Petrov and Klein, 2007).",7 Related Work,[0],[0]
"By contrast, the approach we have described here continues a recent line of work on direct modeling of correlations in the input space, by using rich feature representations to parameterize local potentials that interact with a comparatively unconstrained structured decoder.",7 Related Work,[0],[0]
"As noted in the introduction, this class of feature-based tree scoring functions can be implemented with either a linear transition system (Chen and Manning, 2014) or a global decoder (Finkel et al., 2008).",7 Related Work,[0],[0]
"Kiperwasser and Goldberg (2016) describe an approach closely related to ours but targeted at dependency formalisms, and which easily accommodates both sparse log-linear scoring models (Hall et al., 2014) and deep neural potentials (Henderson, 2004; Ballesteros et al., 2016).
",7 Related Work,[0],[0]
"The best-performing constituency parsers in the last two years have largely been transition-based rather than global; examples include the models of Dyer et al. (2016), Cross and Huang (2016) and Liu and Zhang (2016).",7 Related Work,[0],[0]
"The present work takes many of the insights developed in these models (e.g. the recurrent representation of spans (Kiperwasser and Goldberg, 2016), and the use of a dynamic oracle and exploration policy during training (Goldberg and Nivre, 2013)) and extends these insights to span-oriented models, which support a wider range of decoding procedures.",7 Related Work,[0],[0]
"Our approach differs from other recent chart-based neural models (e.g. Durrett and Klein (2015)) in the use of a recurrent input representation, structured loss function, and comparatively simple parameterization of the scoring function.",7 Related Work,[0],[0]
"In addition to the globally optimal decoding procedures for which these models were designed, and in contrast to the left-to-right decoder typically employed by transition-based models, our model admits an additional greedy top-to-bottom inference procedure.",7 Related Work,[0],[0]
"We have presented a minimal span-oriented parser that uses a recurrent input representation to score
trees with a sum of independent potentials on their constituent spans and labels.",8 Conclusion,[0],[0]
Our model supports both exact chart-based decoding and a novel top-down inference procedure.,8 Conclusion,[0],[0]
"Both approaches achieve state-of-the-art performance on the Penn Treebank, and our best model achieves competitive performance on the French Treebank.",8 Conclusion,[0],[0]
"Our experiments show that many of the key insights from recent neural transition-based approaches to parsing can be easily ported to the chart parsing setting, resulting in a pair of extremely simple models that nonetheless achieve excellent performance.",8 Conclusion,[0],[0]
We would like to thank Nick Altieri and the anonymous reviewers for their valuable comments and suggestions.,Acknowledgments,[0],[0]
MS is supported by an NSF Graduate Research Fellowship.,Acknowledgments,[0],[0]
JA is supported by a Facebook graduate fellowship and a Berkeley AI / Huawei fellowship.,Acknowledgments,[0],[0]
"In this work, we present a minimal neural model for constituency parsing based on independent scoring of labels and spans.",abstractText,[0],[0]
"We show that this model is not only compatible with classical dynamic programming techniques, but also admits a novel greedy top-down inference algorithm based on recursive partitioning of the input.",abstractText,[0],[0]
"We demonstrate empirically that both prediction schemes are competitive with recent work, and when combined with basic extensions to the scoring model are capable of achieving state-of-the-art single-model performance on the Penn Treebank (91.79 F1) and strong performance on the French Treebank (82.23 F1).",abstractText,[0],[0]
A Minimal Span-Based Neural Constituency Parser,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1318–1328 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1318",text,[0],[0]
"Temporal relation (TempRel) extraction is an important task for event understanding, and it has drawn much attention in the natural language processing (NLP) community recently (UzZaman et al., 2013; Chambers et al., 2014; Llorens et al., 2015; Minard et al., 2015; Bethard et al., 2015, 2016, 2017; Leeuwenberg and Moens, 2017; Ning et al., 2017, 2018a,b).
",1 Introduction,[0],[0]
"Initiated by TimeBank (TB) (Pustejovsky et al., 2003b), a number of TempRel datasets have been collected, including but not limited to the verbclause augmentation to TB (Bethard et al., 2007),
1The dataset is publicly available at https:// cogcomp.org/page/publication_view/834.
",1 Introduction,[0],[0]
"TempEval1-3 (Verhagen et al., 2007, 2010; UzZaman et al., 2013), TimeBank-Dense (TB-Dense) (Cassidy et al., 2014), EventTimeCorpus (Reimers et al., 2016), and datasets with both temporal and other types of relations (e.g., coreference and causality) such as CaTeRs (Mostafazadeh et al., 2016) and RED (O’Gorman et al., 2016).",1 Introduction,[0],[0]
"These datasets were annotated by experts, but most still suffered from low inter-annotator agreements (IAA).",1 Introduction,[0],[0]
"For instance, the IAAs of TB-Dense, RED and THYME-TimeML (Styler IV et al., 2014) were only below or near 60% (given that events are already annotated).",1 Introduction,[0],[0]
"Since a low IAA usually indicates that the task is difficult even for humans (see Examples 1-3), the community has been looking into ways to simplify the task, by reducing the label set, and by breaking up the overall, complex task into subtasks (e.g., getting agreement on which event pairs should have a relation, and then what that relation should be) (Mostafazadeh et al., 2016; O’Gorman et al., 2016).",1 Introduction,[0],[0]
"In contrast to other existing datasets, Bethard et al. (2007) achieved an agreement as high as 90%, but the scope of its annotation was narrowed down to a very special verb-clause structure.
(e1, e2), (e3, e4), and (e5, e6): TempRels that are difficult even for humans.",1 Introduction,[0],[0]
Note that only relevant events are highlighted here.,1 Introduction,[0],[0]
Example 1: Serbian police tried to eliminate the proindependence Kosovo Liberation Army and (e1:restore) order.,1 Introduction,[0],[0]
At least 51 people were (e2:killed) in clashes between Serb police and ethnic Albanians in the troubled region.,1 Introduction,[0],[0]
"Example 2: Service industries (e3:showed) solid job gains, as did manufacturers, two areas expected to be hardest (e4:hit) when the effects of the Asian crisis hit the American economy.",1 Introduction,[0],[0]
"Example 3: We will act again if we have evidence he is (e5:rebuilding) his weapons of mass destruction capabilities, senior officials say.",1 Introduction,[0],[0]
"In a bit of television diplomacy, Iraq’s deputy foreign minister (e6:responded) from Baghdad in less than one hour, saying that . . .
",1 Introduction,[0],[0]
"This paper proposes a new approach to handling
these issues in TempRel annotation.",1 Introduction,[0],[0]
"First, we introduce multi-axis modeling to represent the temporal structure of events, based on which we anchor events to different semantic axes; only events from the same axis will then be temporally compared (Sec. 2).",1 Introduction,[0],[0]
"As explained later, those event pairs in Examples 1-3 are difficult because they represent different semantic phenomena and belong to different axes.",1 Introduction,[0],[0]
"Second, while we represent an event pair using two time intervals (say, [t1start, t 1 end] and [t 2 start, t 2 end]), we suggest that comparisons involving end-points (e.g., t1end vs. t2end) are typically more difficult than comparing start-points (i.e., t1start vs. t2start); we attribute this to the ambiguity of expressing and perceiving durations of events (Coll-Florit and Gennari, 2011).",1 Introduction,[0],[0]
"We believe that this is an important consideration, and we propose in Sec. 3 that TempRel annotation should focus on start-points.",1 Introduction,[0],[0]
"Using the proposed annotation scheme, a pilot study done by experts achieved a high IAA of .84 (Cohen’s Kappa) on a subset of TB-Dense, in contrast to the conventional 60’s.
",1 Introduction,[0],[0]
"In addition to the low IAA issue, TempRel annotation is also known to be labor intensive.",1 Introduction,[0],[0]
"Our third contribution is that we facilitate, for the first time, the use of crowdsourcing to collect a new, high quality (under multiple metrics explained later) TempRel dataset.",1 Introduction,[0],[0]
"We explain how the crowdsourcing quality was controlled and how vague relations were handled in Sec. 4, and present some statistics and the quality of the new dataset in Sec. 5.",1 Introduction,[0],[0]
"A baseline system is also shown to achieve much better performance on the new dataset, when compared with system performance in the literature (Sec. 6).",1 Introduction,[0],[0]
"The paper’s results are very encouraging and hopefully, this work would significantly benefit research in this area.",1 Introduction,[0],[0]
"Given a set of events, one important question in designing the TempRel annotation task is: which pairs of events should have a relation?",2 Temporal Structure of Events,[0],[0]
The answer to it depends on the modeling of the overall temporal structure of events.,2 Temporal Structure of Events,[0],[0]
"TimeBank (Pustejovsky et al., 2003b) laid the foundation for many later TempRel corpora, e.g., (Bethard et al., 2007; UzZaman et al., 2013; Cas-
sidy et al., 2014).2",2.1 Motivation,[0],[0]
"In TimeBank, the annotators were allowed to label TempRels between any pairs of events.",2.1 Motivation,[0],[0]
"This setup models the overall structure of events using a general graph, which made annotators inadvertently overlook some pairs, resulting in low IAAs and many false negatives.
",2.1 Motivation,[0],[0]
Example 4: Dense Annotation Scheme.,2.1 Motivation,[0],[0]
Serbian police (e7:tried) to (e8:eliminate) the proindependence Kosovo Liberation Army and (e1:restore) order.,2.1 Motivation,[0],[0]
At least 51 people were (e2:killed) in clashes between Serb police and ethnic Albanians in the troubled region.,2.1 Motivation,[0],[0]
"Given 4 NON-GENERIC events above, the dense scheme presents 6 pairs to annotators one by one: (e7, e8), (e7, e1), (e7, e2), (e8, e1), (e8, e2), and (e1, e2).",2.1 Motivation,[0],[0]
"Apparently, not all pairs are well-defined, e.g., (e8, e2) and (e1, e2), but annotators are forced to label all of them.
",2.1 Motivation,[0],[0]
"To address this issue, Cassidy et al. (2014) proposed a dense annotation scheme, TB-Dense, which annotates all event pairs within a sliding, two-sentence window (see Example 4).",2.1 Motivation,[0],[0]
"It requires all TempRels between GENERIC3 and NON-GENERIC events to be labeled as vague, which conceptually models the overall structure by two disjoint time-axes: one for the NONGENERIC and the other one for the GENERIC.
",2.1 Motivation,[0],[0]
"However, as shown by Examples 1-3 in which the highlighted events are NON-GENERIC, the TempRels may still be ill-defined: In Example 1, Serbian police tried to restore order but ended up with conflicts.",2.1 Motivation,[0],[0]
"It is reasonable to argue that the attempt to e1:restore order happened before the conflict where 51 people were e2:killed; or, 51 people had been killed but order had not been restored yet, so e1:restore is after e2:killed.",2.1 Motivation,[0],[0]
"Similarly, in Example 2, service industries and manufacturers were originally expected to be hardest e4:hit but actually e3:showed gains, so e4:hit is before e3:showed; however, one can also argue that the two areas had showed gains but had not been hit, so e4:hit is after e3:showed.",2.1 Motivation,[0],[0]
"Again, e5:rebuilding is a hypothetical event: “we will act if rebuilding is true”.",2.1 Motivation,[0],[0]
"Readers do not know for sure if “he is already rebuilding weapons but we have no evidence”, or “he will be building weapons in the future”, so annotators may disagree on the relation between e5:rebuilding and e6:responded.",2.1 Motivation,[0],[0]
"Despite, importantly, minimizing missing annota-
2EventTimeCorpus (Reimers et al., 2016) is based on TimeBank, but aims at anchoring events onto explicit time expressions in each document rather than annotating TempRels between events, which can be a good complementary to other TempRel datasets.
",2.1 Motivation,[0],[0]
"3For example, lions eat meat is GENERIC.
tions, the current dense scheme forces annotators to label many such ill-defined pairs, resulting in low IAA.",2.1 Motivation,[0],[0]
"Arguably, an ideal annotator may figure out the above ambiguity by him/herself and mark them as vague, but it is not a feasible requirement for all annotators to stay clear-headed for hours; let alone crowdsourcers.",2.2 Multi-Axis Modeling,[0],[0]
"What makes things worse is that, after annotators spend a long time figuring out these difficult cases, whether they disagree with each other or agree on the vagueness, the final decisions for such cases will still be vague.
",2.2 Multi-Axis Modeling,[0],[0]
"As another way to handle this dilemma, TBDense resorted to a 80% confidence rule: annotators were allowed to choose a label if one is 80% sure that it was the writer’s intent.",2.2 Multi-Axis Modeling,[0],[0]
"However, as pointed out by TB-Dense, annotators are likely to have rather different understandings of 80% confidence and it will still end up with disagreements.
",2.2 Multi-Axis Modeling,[0],[0]
"In contrast to these annotation difficulties, humans can easily grasp the meaning of news articles, implying a potential gap between the difficulty of the annotation task and the one of understanding the actual meaning of the text.",2.2 Multi-Axis Modeling,[0],[0]
"In Examples 1-3, the writers did not intend to explain the TempRels between those pairs, and the original annotators of TimeBank4 did not label relations between those pairs either, which indicates that both writers and readers did not think the TempRels between these pairs were crucial.",2.2 Multi-Axis Modeling,[0],[0]
"Instead, what is crucial in these examples is that “Serbian police tried to restore order but killed 51 people”, that “two areas were expected to be hit but showed gains”, and that “if he rebuilds weapons then we will act.”",2.2 Multi-Axis Modeling,[0],[0]
"To “restore order”, to be “hardest hit”, and “if he was rebuilding” were only the intention of police, the opinion of economists, and the condition to act, respectively, and whether or not they actually happen is not the focus of those writers.
",2.2 Multi-Axis Modeling,[0],[0]
This discussion suggests that a single axis is too restrictive to represent the complex structure of NON-GENERIC events.,2.2 Multi-Axis Modeling,[0],[0]
"Instead, we need a modeling which is more restrictive than a general graph so that annotators can focus on relation annotation (rather than looking for pairs first), but also more flexible than a single axis so that ill-defined
4Recall that they were given the entire article and only salient relations would be annotated.
",2.2 Multi-Axis Modeling,[0],[0]
relations are not forcibly annotated.,2.2 Multi-Axis Modeling,[0],[0]
"Specifically, we need axes for intentions, opinions, hypotheses, etc.",2.2 Multi-Axis Modeling,[0],[0]
in addition to the main axis of an article.,2.2 Multi-Axis Modeling,[0],[0]
"We thus argue for multi-axis modeling, as defined in Table 1.",2.2 Multi-Axis Modeling,[0],[0]
"Following the proposed modeling, Examples 1-3 can be represented as in Fig. 1.",2.2 Multi-Axis Modeling,[0],[0]
"This modeling aims at capturing what the author has explicitly expressed and it only asks annotators to look at comparable pairs, rather than forcing them to make decisions on often vaguely defined pairs.
",2.2 Multi-Axis Modeling,[0],[0]
"In practice, we annotate one axis at a time: we first classify if an event is anchorable onto a given axis (this is also called the anchorability annotation step); then we annotate every pair of anchorable events (i.e., the relation annotation step); finally, we can move to another axis and repeat the two steps above.",2.2 Multi-Axis Modeling,[0],[0]
Note that ruling out cross-axis relations is only a strategy we adopt in this paper to separate well-defined relations from ill-defined relations.,2.2 Multi-Axis Modeling,[0],[0]
"We do not claim that cross-axis relations are unimportant; instead, as shown in Fig. 2, we think that cross-axis relations are a different semantic phenomenon that requires additional investigation.",2.2 Multi-Axis Modeling,[0],[0]
"There have been other proposals of temporal structure modelings (Bramsen et al., 2006; Bethard et al., 2012), but in general, the semantic phenomena handled in our work are very different and complementary to them.",2.3 Comparisons with Existing Work,[0],[0]
"(Bramsen et al., 2006) introduces “temporal segments” (a fragment of text that does not exhibit abrupt changes) in the medical domain.",2.3 Comparisons with Existing Work,[0],[0]
"Similarly, their temporal segments can also be considered as a special temporal structure modeling.",2.3 Comparisons with Existing Work,[0],[0]
"But a key difference is that (Bramsen et al., 2006) only annotates inter-segment relations, ignoring intra-segment ones.",2.3 Comparisons with Existing Work,[0],[0]
"Since those segments are usually large chunks of text, the semantics handled in (Bramsen et al., 2006) is in a very coarse granularity (as pointed out by (Bramsen et al., 2006)) and is thus different from ours.
",2.3 Comparisons with Existing Work,[0],[0]
"(Bethard et al., 2012) proposes a tree structure for children’s stories, which “typically have simpler temporal structures”, as they pointed out.",2.3 Comparisons with Existing Work,[0],[0]
"Moreover, in their annotation, an event can only be linked to a single nearby event, even if multiple nearby events may exist, whereas we do not have such restrictions.
",2.3 Comparisons with Existing Work,[0],[0]
"In addition, some of the semantic phenomena in Table 1 have been discussed in existing work.",2.3 Comparisons with Existing Work,[0],[0]
Here we compare with them for a better positioning of the proposed scheme.,2.3 Comparisons with Existing Work,[0],[0]
TB-Dense handled the incomparability between main-axis events and HYPOTHESIS/NEGATION by treating an event as having occurred if the event is HYPOTHESIS/NEGATION.5,2.3.1 Axis Projection,[0],[0]
"In our multiaxis modeling, the strategy adopted by TB-Dense falls into a more general approach, “axis projection”.",2.3.1 Axis Projection,[0],[0]
"That is, projecting events across different axes to handle the incomparability between any two axes (not limited to HYPOTHESIS/NEGATION).",2.3.1 Axis Projection,[0],[0]
"Axis projection works well for certain event pairs like Asian crisis and e4:hardest hit in Example 2: as in Fig. 1, Asian crisis is before expected, which is again before e4:hardest hit, so Asian crisis is before e4:hardest hit.
",2.3.1 Axis Projection,[0],[0]
"Generally, however, since there is no direct evidence that can guide the projection, annotators may have different projections (imagine projecting e5:rebuilding onto the main axis: is it in the past or in the future?).",2.3.1 Axis Projection,[0],[0]
"As a result, axis projec-
5In the case of Example 3, it is to treat rebuilding as actually happened and then link it to responded.
tion requires many specially designed guidelines or strong external knowledge.",2.3.1 Axis Projection,[0],[0]
"Annotators have to rigidly follow the sometimes counter-intuitive guidelines or “guess” a label instead of looking for evidence in the text.
",2.3.1 Axis Projection,[0],[0]
"When strong external knowledge is involved in axis projection, it becomes a reasoning process and the resulting relations are a different type.",2.3.1 Axis Projection,[0],[0]
"For example, a reader may reason that in Example 3, it is well-known that they did “act again”, implying his e5:rebuilding had happened and is before e6:responded.",2.3.1 Axis Projection,[0],[0]
Another example is in Fig. 2.,2.3.1 Axis Projection,[0],[0]
"It is obvious that relations based on these projections are not the same with and more challenging than those same-axis relations, so in the current stage, we should focus on same-axis relations only.",2.3.1 Axis Projection,[0],[0]
"Another prominent difference to earlier work is the introduction of orthogonal axes, which has not been used in any existing work as we know.",2.3.2 Introduction of the Orthogonal Axes,[0],[0]
"A special property is that the intersection event of two axes can be compared to events from both, which can sometimes bridge events, e.g., in Fig. 1, Asian crisis is seemingly before hardest hit due to their connections to expected.",2.3.2 Introduction of the Orthogonal Axes,[0],[0]
"Since Asian crisis is on the main axis, it seems that e4:hardest hit is on the main axis as well.",2.3.2 Introduction of the Orthogonal Axes,[0],[0]
"However, the “hardest hit” in “Asian crisis before hardest hit” is only a projection of the original e4:hardest hit onto the real axis and is valid only when this OPINION is true.
",2.3.2 Introduction of the Orthogonal Axes,[0],[0]
"Nevertheless, OPINIONS are not always true and INTENTIONS are not always fulfilled.",2.3.2 Introduction of the Orthogonal Axes,[0],[0]
"In Example 5, e9:sponsoring and e10:resolve are the opinions of the West and the speaker, respectively; whether or not they are true depends on the au-
thors’ implications or the readers’ understandings, which is often beyond the scope of TempRel annotation.6 Example 6 demonstrates a similar situation for INTENTIONS: when reading the sentence of e11:report, people are inclined to believe that it is fulfilled.",2.3.2 Introduction of the Orthogonal Axes,[0],[0]
"But if we read the sentence of e12:report, we have reason to believe that it is not.",2.3.2 Introduction of the Orthogonal Axes,[0],[0]
"When it comes to e13:tell, it is unclear if everyone told the truth.",2.3.2 Introduction of the Orthogonal Axes,[0],[0]
"The existence of such examples indicates that orthogonal axes are a better modeling for INTENTIONS and OPINIONS.
",2.3.2 Introduction of the Orthogonal Axes,[0],[0]
Example 5: Opinion events may not always be true.,2.3.2 Introduction of the Orthogonal Axes,[0],[0]
He is ostracized by the West for (e9:sponsoring) terrorism.,2.3.2 Introduction of the Orthogonal Axes,[0],[0]
We need to (e10:resolve) the deep-seated causes that have resulted in these problems.,2.3.2 Introduction of the Orthogonal Axes,[0],[0]
Example 6: Intentions may not always be fulfilled.,2.3.2 Introduction of the Orthogonal Axes,[0],[0]
A passerby called the police to (e11:report) the body.,2.3.2 Introduction of the Orthogonal Axes,[0],[0]
A passerby called the police to (e12:report) the body.,2.3.2 Introduction of the Orthogonal Axes,[0],[0]
"Unfortunately, the line was busy.",2.3.2 Introduction of the Orthogonal Axes,[0],[0]
I asked everyone to (e13:tell) the truth.,2.3.2 Introduction of the Orthogonal Axes,[0],[0]
"Event modality have been discussed in many existing event annotation schemes, e.g., Event Nugget (Mitamura et al., 2015), Rich ERE (Song et al., 2015), and RED.",2.3.3 Differences from Factuality,[0],[0]
"Generally, an event is classified as Actual or Non-Actual, a.k.a. factuality (Saurı́ and Pustejovsky, 2009; Lee et al., 2015).
",2.3.3 Differences from Factuality,[0],[0]
"The main-axis events defined in this paper seem to be very similar to Actual events, but with several important differences: First, future events are Non-Actual because they indeed have not happened, but they may be on the main axis.",2.3.3 Differences from Factuality,[0],[0]
"Second, events that are not on the main axis can also be Actual events, e.g., intentions that are fulfilled, or opinions that are true.",2.3.3 Differences from Factuality,[0],[0]
"Third, as demonstrated by Examples 5-6, identifying anchorability as defined in Table 1 is relatively easy, but judging if an event actually happened is often a high-level understanding task that requires an understanding of the entire document or external knowledge.
",2.3.3 Differences from Factuality,[0],[0]
Interested readers are referred to Appendix B for a detailed analysis of the difference between Anchorable (onto the main axis) and Actual on a subset of RED.,2.3.3 Differences from Factuality,[0],[0]
"All existing annotation schemes adopt the interval representation of events (Allen, 1984) and there
6For instance, there is undoubtedly a causal link between e9:sponsoring and ostracized.
are 13 relations between two intervals (for readers who are not familiar with it, please see Fig. 4 in the appendix).",3 Interval Splitting,[0],[0]
"To reduce the burden of annotators, existing schemes often resort to a reduced set of the 13 relations.",3 Interval Splitting,[0],[0]
"For instance, Verhagen et al. (2007) merged all the overlap relations into a single relation, overlap.",3 Interval Splitting,[0],[0]
Bethard et al. (2007); Do et al. (2012); O’Gorman et al. (2016) all adopted this strategy.,3 Interval Splitting,[0],[0]
"In Cassidy et al. (2014), they further split overlap into includes, included and equal.
",3 Interval Splitting,[0],[0]
"Let [t1start, t1end] and [t 2 start, t 2 end] be the time intervals of two events (with the implicit assumption that tstart  tend).",3 Interval Splitting,[0],[0]
"Instead of reducing the relations between two intervals, we try to explicitly compare the time points (see Fig. 3).",3 Interval Splitting,[0],[0]
"In this way, the label set is simply before, after and equal,7 while the expressivity remains the same.",3 Interval Splitting,[0],[0]
"This interval splitting technique has also been used in (Raghavan et al., 2012).
",3 Interval Splitting,[0],[0]
"In addition to same expressivity, interval splitting can provide even more information when the relation between two events is vague.",3 Interval Splitting,[0],[0]
"In the conventional setting, imagine that the annotators find that the relation between two events can be either before or before and overlap.",3 Interval Splitting,[0],[0]
"Then the resulting annotation will have to be vague, although the annotators actually agree on the relation between t1start and t2start.",3 Interval Splitting,[0],[0]
"Using interval splitting, however, such information can be preserved.
",3 Interval Splitting,[0],[0]
An obvious downside of interval splitting is the increased number of annotations needed (4 point comparisons vs. 1 interval comparison).,3 Interval Splitting,[0],[0]
"In practice, however, it is usually much fewer than 4 comparisons.",3 Interval Splitting,[0],[0]
"For example, when we see t1end < t 2 start (as in Fig. 3), the other three can be skipped because they can all be inferred.",3 Interval Splitting,[0],[0]
"Moreover, although the number of annotations is increased, the work load for human annotators may still be the same, because even in the conventional scheme, they still need to think of the relations between start- and
7We will discuss vague in Sec. 4.
end-points before they can make a decision.",3 Interval Splitting,[0],[0]
"During our pilot annotation, the annotation quality dropped significantly when the annotators needed to reason about relations involving end-points of events.",3.1 Ambiguity of End-Points,[0],[0]
Table 2 shows four metrics of task difficulty when only t1start vs. t2start or t1end vs. t 2 end are annotated.,3.1 Ambiguity of End-Points,[0],[0]
Non-anchorable events were removed for both jobs.,3.1 Ambiguity of End-Points,[0],[0]
"The first two metrics, qualifying pass rate and survival rate are related to the two quality control protocols (see Sec. 4.1 for details).",3.1 Ambiguity of End-Points,[0],[0]
"We can see that when annotating the relations between end-points, only one out of ten crowdsourcers (11%) could successfully pass our qualifying test; and even if they had passed it, half of them (56%) would have been kicked out in the middle of the task.",3.1 Ambiguity of End-Points,[0],[0]
"The third line is the overall accuracy on gold set from all crowdsourcers (excluding those who did not pass the qualifying test), which drops from 67% to 37% when annotating end-end relations.",3.1 Ambiguity of End-Points,[0],[0]
The last line is the average response time per annotation and we can see that it takes much longer to label an end-end TempRel (52s) than a start-start TempRel (33s).,3.1 Ambiguity of End-Points,[0],[0]
"This important discovery indicates that the TempRels between end-points is probably governed by a different linguistic phenomenon.
",3.1 Ambiguity of End-Points,[0],[0]
We hypothesize that the difficulty is a mixture of how durative events are expressed (by authors) and perceived (by readers) in natural language.,3.1 Ambiguity of End-Points,[0],[0]
"In cognitive psychology, Coll-Florit and Gennari (2011) discovered that human readers take longer to perceive durative events than punctual events, e.g., owe 50 bucks vs. lost 50 bucks.",3.1 Ambiguity of End-Points,[0],[0]
"From the writer’s standpoint, durations are usually fuzzy (Schockaert and De Cock, 2008), or assumed to be a prior knowledge of readers (e.g., college takes 4 years and watching an NBA game takes a few hours), and thus not always written explicitly.",3.1 Ambiguity of End-Points,[0],[0]
"Given all these reasons, we ignore the comparison of end-points in this work, although event duration is indeed, another important task.",3.1 Ambiguity of End-Points,[0],[0]
"To summarize, with the proposed multi-axis modeling (Sec. 2) and interval splitting (Sec. 3), our annotation scheme is two-step.",4 Annotation Scheme Design,[0],[0]
"First, we mark every event candidate as being temporally Anchorable or not (based on the time axis we are working on).",4 Annotation Scheme Design,[0],[0]
"Second, we adopt the dense annotation scheme to label TempRels only between Anchorable events.",4 Annotation Scheme Design,[0],[0]
"Note that we only work on verb events in this paper, so non-verb event candidates are also deleted in a preprocessing step.",4 Annotation Scheme Design,[0],[0]
"We design crowdsourcing tasks for both steps and as we show later, high crowdsourcing quality was achieved on both tasks.",4 Annotation Scheme Design,[0],[0]
"In this section, we will discuss some practical issues.",4 Annotation Scheme Design,[0],[0]
We take advantage of the quality control feature in CrowdFlower in our crowdsourcing jobs.,4.1 Quality Control for Crowdsourcing,[0],[0]
"For any job, a set of examples are annotated by experts beforehand, which is considered gold and will serve two purposes.",4.1 Quality Control for Crowdsourcing,[0],[0]
(i) Qualifying test: Any crowdsourcer who wants to work on this job has to pass with 70% accuracy on 10 questions randomly selected from the gold set.,4.1 Quality Control for Crowdsourcing,[0],[0]
"(ii) Surviving test: During the annotation process, questions from the gold set will be randomly given to crowdsourcers without notice, and one has to maintain 70% accuracy on the gold set till the end of the annotation; otherwise, he or she will be forbidden from working on this job anymore and all his/her annotations will be discarded.",4.1 Quality Control for Crowdsourcing,[0],[0]
"At least 5 different annotators are required for every judgement and by default, the majority vote will be the final decision.",4.1 Quality Control for Crowdsourcing,[0],[0]
How to handle vague relations is another issue in temporal annotation.,4.2 Vague Relations,[0],[0]
"In non-dense schemes, annotators usually skip the annotation of a vague pair.",4.2 Vague Relations,[0],[0]
"In dense schemes, a majority agreement rule is applied as a postprocessing step to back off a decision to vague when annotators cannot pass a majority vote (Cassidy et al., 2014), which reminds us that annotators often label a vague relation as non-vague due to lack of thinking.
",4.2 Vague Relations,[0],[0]
We decide to proactively reduce the possibility of such situations.,4.2 Vague Relations,[0],[0]
"As mentioned earlier, our label set for t1start vs. t2start is before, after, equal and vague.",4.2 Vague Relations,[0],[0]
We ask two questions: Q1=Is it possible that t1start is before t2start?,4.2 Vague Relations,[0],[0]
Q2=Is it possible that t2start is before t1start?,4.2 Vague Relations,[0],[0]
"Let the an-
swers be A1 and A2.",4.2 Vague Relations,[0],[0]
"Then we have a oneto-one mapping as follows: A1=A2=yes7!vague, A1=A2=no7!equal, A1=yes, A2=no 7!before, and A1=no, A2=yes 7!after.",4.2 Vague Relations,[0],[0]
"An advantage is that one will be prompted to think about all possibilities, thus reducing the chance of overlook.
",4.2 Vague Relations,[0],[0]
"Finally, the annotation interface we used is shown in Appendix C.",4.2 Vague Relations,[0],[0]
"In this section, we first focus on annotations on the main axis, which is usually the primary storyline and thus has most events.",5 Corpus Statistics and Quality,[0],[0]
"Before launching the crowdsourcing tasks, we checked the IAA between two experts on a subset of TB-Dense (about 100 events and 400 relations).",5 Corpus Statistics and Quality,[0],[0]
A Cohen’s Kappa of .85 was achieved in the first step: anchorability annotation.,5 Corpus Statistics and Quality,[0],[0]
"Only those events that both experts labeled Anchorable were kept before they moved onto the second step: relation annotation, for which the Cohen’s Kappa was .90 for Q1 and .87 for Q2.",5 Corpus Statistics and Quality,[0],[0]
"Table 3 furthermore shows the distribution, Cohen’s Kappa, and F1 of each label.",5 Corpus Statistics and Quality,[0],[0]
"We can see the Kappa and F1 of vague (=.75, F1=.81) are generally lower than those of the other labels, confirming that temporal vagueness is a more difficult semantic phenomenon.",5 Corpus Statistics and Quality,[0],[0]
"Nevertheless, the overall IAA shown in Table 3 is a significant improvement compared to existing datasets.
",5 Corpus Statistics and Quality,[0],[0]
"With the improved IAA confirmed by experts, we sequentially launched the two-step crowdsourcing tasks through CrowdFlower on top of the same 36 documents of TB-Dense.",5 Corpus Statistics and Quality,[0],[0]
"To evaluate how well the crowdsourcers performed on our task, we calculate two quality metrics: accuracy on the gold set and the Worker Agreement with Aggregate (WAWA).",5 Corpus Statistics and Quality,[0],[0]
WAWA indicates the average number of crowdsourcers’ responses agreed with the aggregate answer (we used majority aggregation for each question).,5 Corpus Statistics and Quality,[0],[0]
"For example, if N individual responses were obtained in total, and n of them were correct when compared to the aggregate answer, then WAWA is simply n/N .",5 Corpus Statistics and Quality,[0],[0]
"In the first step,
crowdsourcers labeled 28% of the events as NonAnchorable to the main axis, with an accuracy on the gold of .86 and a WAWA of .79.
With Non-Anchorable events filtered, the relation annotation step was launched as another crowdsourcing task.",5 Corpus Statistics and Quality,[0],[0]
"The label distribution is b=.50, a=.28, e=.03, and v=.19 (consistent with Table 3).",5 Corpus Statistics and Quality,[0],[0]
"In Table 4, we show the annotation quality of this step using accuracy on the gold set and WAWA.",5 Corpus Statistics and Quality,[0],[0]
"We can see that the crowdsourcers achieved a very good performance on the gold set, indicating that they are consistent with the authors who created the gold set; these crowdsourcers also achieved a high-level agreement under the WAWA metric, indicating that they are consistent among themselves.",5 Corpus Statistics and Quality,[0],[0]
"These two metrics indicate that the annotation task is now well-defined and easy to understand even by non-experts.
",5 Corpus Statistics and Quality,[0],[0]
We continued to annotate INTENTION and OPINION which create orthogonal branches on the main axis.,5 Corpus Statistics and Quality,[0],[0]
"In the first step, crowdsourcers achieved an accuracy on gold of .82 and a WAWA of .89.",5 Corpus Statistics and Quality,[0],[0]
"Since only 16% of the events are in this category and these axes are usually very short (e.g., allocate funds to build a museum.), the annotation task is relatively small and two experts took the second step and achieved an agreement of .86 (F1).
",5 Corpus Statistics and Quality,[0],[0]
We name our new dataset MATRES for MultiAxis Temporal RElations for Start-points.,5 Corpus Statistics and Quality,[0],[0]
Each individual judgement cost us $0.01 and MATRES in total cost about $400 for 36 documents.,5 Corpus Statistics and Quality,[0],[0]
"To get another checkpoint of the quality of the new dataset, we compare with the annotations of TBDense.",5.1 Comparison to TB-Dense,[0],[0]
"TB-Dense has 1.1K verb events, between which 3.4K event-event (EE) relations are annotated.",5.1 Comparison to TB-Dense,[0],[0]
"In the new dataset, 72% of the events (0.8K) are anchored onto the main axis, resulting in 1.6K EE relations, and 16% (0.2K) are anchored onto orthogonal axes, resulting in 0.2K EE relations.
",5.1 Comparison to TB-Dense,[0],[0]
The following comparison is based on the 1.8K EE relations in common.,5.1 Comparison to TB-Dense,[0],[0]
"Moreover, since TB-Dense annotations are for intervals instead of start-points only, we converted TB-Dense’s interval relations to start-point relations (e.g., if A includes B, then tAstart is before tBstart).
",5.1 Comparison to TB-Dense,[0],[0]
The confusion matrix is shown in Table 5.,5.1 Comparison to TB-Dense,[0],[0]
"A few remarks about how to understand it: First, when TB-Dense labels before or after, MATRES also has a high-probability of having the same label (b=455/513=.89, a=309/438=.71); when MATRES labels vague, TB-Dense is also very likely to label vague (v=192/312=.62).",5.1 Comparison to TB-Dense,[0],[0]
This indicates the high agreement level between the two datasets if the interval- or point-based annotation difference is ruled out.,5.1 Comparison to TB-Dense,[0],[0]
"Second, many vague relations in TB-Dense are labeled as before, after or equal in MATRES.",5.1 Comparison to TB-Dense,[0],[0]
"This is expected because TB-Dense annotates relations between intervals, while MATRES annotates start-points.",5.1 Comparison to TB-Dense,[0],[0]
"When durative events are involved, the problem usually becomes more difficult and interval-based annotation is more likely to label vague (see earlier discussions in Sec. 3).",5.1 Comparison to TB-Dense,[0],[0]
"Example 7 shows three typical cases, where e14:became, e17:backed, e18:rose and e19:extending can be considered durative.",5.1 Comparison to TB-Dense,[0],[0]
"If only their start-points are considered, the crowdsourcers were correct in labeling e14 before e15, e16 after e17, and e18 equal to e19, although TBDense says vague for all of them.",5.1 Comparison to TB-Dense,[0],[0]
"Third, equal seems to be the relation that the two dataset mostly disagree on, which is probably due to crowdsourcers’ lack of understanding in time granularity and event coreference.",5.1 Comparison to TB-Dense,[0],[0]
"Although equal relations only constitutes a small portion in all relations, it needs further investigation.",5.1 Comparison to TB-Dense,[0],[0]
"We develop a baseline system for TempRel extraction on MATRES, assuming that all the events and axes are given.",6 Baseline System,[0],[0]
"The following commonly-
Example 7: Typical cases that TB-Dense annotated vague but MATRES annotated before, after, and equal, respectively.",6 Baseline System,[0],[0]
"At one point , when it (e14:became) clear controllers could not contact the plane, someone (e15:said) a prayer.",6 Baseline System,[0],[0]
"TB-Dense: vague; MATRES: before The US is bolstering its military presence in the gulf, as President Clinton (e16:discussed) the Iraq crisis with the one ally who has (e17:backed) his threat of force, British prime minister Tony Blair.",6 Baseline System,[0],[0]
TB-Dense: vague; MATRES: after Average hourly earnings of nonsupervisory employees (e18:rose) to $12.51.,6 Baseline System,[0],[0]
"The gain left wages 3.8 percent higher than a year earlier, (e19:extending) a trend that has given back to workers some of the earning power they lost to inflation in the last decade.",6 Baseline System,[0],[0]
"TB-Dense: vague; MATRES: equal
used features for each event pair are used: (i)",6 Baseline System,[0],[0]
The part-of-speech (POS) tags of each individual event and of its neighboring three words.,6 Baseline System,[0],[0]
(ii) The sentence and token distance between the two events.,6 Baseline System,[0],[0]
"(iii) The appearance of any modal verb between the two event mentions in text (i.e., will, would, can, could, may and might).",6 Baseline System,[0],[0]
"(iv) The appearance of any temporal connectives between the two event mentions (e.g., before, after and since).",6 Baseline System,[0],[0]
"(v) Whether the two verbs have a common synonym from their synsets in WordNet (Fellbaum, 1998).",6 Baseline System,[0],[0]
(vi) Whether the input event mentions have a common derivational form derived from WordNet.,6 Baseline System,[0],[0]
(vii),6 Baseline System,[0],[0]
"The head words of the preposition phrases that cover each event, respectively.",6 Baseline System,[0],[0]
"And (viii) event properties such as Aspect, Modality, and Polarity that come with the TimeBank dataset and are commonly used as features.
",6 Baseline System,[0],[0]
The proposed baseline system uses the averaged perceptron algorithm to classify the relation between each event pair into one of the four relation types.,6 Baseline System,[0],[0]
"We adopted the same train/dev/test split of TB-Dense, where there are 22 documents in train, 5 in dev, and 9 in test.",6 Baseline System,[0],[0]
"Parameters were tuned on the train-set to maximize its F1 on the dev-set, after which the classifier was retrained on the union of train and dev.",6 Baseline System,[0],[0]
A detailed analysis of the baseline system is provided in Table 6.,6 Baseline System,[0],[0]
"The performance on equal and vague is lower than on before and after, probably due to shortage in these labels in the training data and the inherent difficulty in event coreference and temporal vagueness.",6 Baseline System,[0],[0]
"We can see, though, that the overall performance on MATRES is much better than those in the literature for TempRel extraction, which used to be in the low 50’s (Chambers et al., 2014; Ning et al., 2017).",6 Baseline System,[0],[0]
"The same system was also retrained
and tested on the original annotations of TB-Dense (Line “Original”), which confirms the significant improvement if the proposed annotation scheme is used.",6 Baseline System,[0],[0]
"Note that we do not mean to say that the proposed baseline system itself is better than other existing algorithms, but rather that the proposed annotation scheme and the resulting dataset lead to better defined machine learning tasks.",6 Baseline System,[0],[0]
"In the future, more data can be collected and used with advanced techniques such as ILP (Do et al., 2012), structured learning (Ning et al., 2017) or multi-sieve (Chambers et al., 2014).",6 Baseline System,[0],[0]
"This paper proposes a new scheme for TempRel annotation between events, simplifying the task by focusing on a single time axis at a time.",7 Conclusion,[0],[0]
"We have also identified that end-points of events is a major source of confusion during annotation due to reasons beyond the scope of TempRel annotation, and proposed to focus on start-points only and handle the end-points issue in further investigation (e.g., in event duration annotation tasks).",7 Conclusion,[0],[0]
"Pilot study by expert annotators shows significant IAA improvements compared to literature values, indicating a better task definition under the proposed scheme.",7 Conclusion,[0],[0]
"This further enables the usage of crowdsourcing to collect a new dataset, MATRES, at a lower time cost.",7 Conclusion,[0],[0]
"Analysis shows that MATRES, albeit crowdsourced, has achieved a reasonably good agreement level, as confirmed by its performance on the gold set (agreement with the authors), the WAWA metric (agreement with the crowdsourcers themselves), and consistency with TB-Dense (agreement with an existing dataset).",7 Conclusion,[0],[0]
"Given the fact that existing schemes suffer from low IAAs and lack of data, we hope that the findings in this work would
provide a good start towards understanding more sophisticated semantic phenomena in this area.",7 Conclusion,[0],[0]
"We thank Martha Palmer, Tim O’Gorman, Mark Sammons and all the anonymous reviewers for providing insightful comments and critique in earlier stages of this work.",Acknowledgements,[0],[0]
"This research is supported in part by a grant from the Allen Institute for Artificial Intelligence (allenai.org); the IBM-ILLINOIS Center for Cognitive Computing Systems Research (C3SR) - a research collaboration as part of the IBM AI Horizons Network; by DARPA under agreement number FA8750-132-0008; and by the Army Research Laboratory (ARL) under agreement W911NF-09-2-0053 (the ARL Network Science CTA).
",Acknowledgements,[0],[0]
The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon.,Acknowledgements,[0],[0]
"The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA, of the Army Research Laboratory or the U.S. Government.",Acknowledgements,[0],[0]
"Any opinions, findings, conclusions or recommendations are those of the authors and do not necessarily reflect the view of the ARL.",Acknowledgements,[0],[0]
"Existing temporal relation (TempRel) annotation schemes often have low interannotator agreements (IAA) even between experts, suggesting that the current annotation task needs a better definition.",abstractText,[0],[0]
This paper proposes a new multi-axis modeling to better capture the temporal structure of events.,abstractText,[0],[0]
"In addition, we identify that event end-points are a major source of confusion in annotation, so we also propose to annotate TempRels based on start-points only.",abstractText,[0],[0]
A pilot expert annotation effort using the proposed scheme shows significant improvement in IAA from the conventional 60’s to 80’s (Cohen’s Kappa).,abstractText,[0],[0]
This better-defined annotation scheme further enables the use of crowdsourcing to alleviate the labor intensity for each annotator.,abstractText,[0],[0]
We hope that this work can foster more interesting studies towards event understanding.1,abstractText,[0],[0]
A Multi-Axis Annotation Scheme for Event Temporal Relations,title,[0],[0]
"Proceedings of NAACL-HLT 2013, pages 673–679, Atlanta, Georgia, 9–14 June 2013. c©2013 Association for Computational Linguistics",text,[0],[0]
"In language, stylistic variation is a reflection of various contextual factors, including the backgrounds of and relationship between the parties involved.",1 Introduction,[0],[0]
"Although in the context of prescriptive linguistics (Strunk and White, 1979), style is often assumed to be a matter of aesthetics, the stylistic intuitions of language users are inextricably linked to the conventions of register and genre (Biber and Conrad, 2009).",1 Introduction,[0],[0]
"Intentional or not, stylistic differences play a role in numerous NLP tasks.",1 Introduction,[0],[0]
"Examples include genre classification (Kessler et al., 1997), author profiling (Garera and Yarowsky, 2009; Rosenthal and McKeown, 2011), social relationship classification (Peterson et al., 2011), sentiment analysis (Wilson et al., 2005), readability classification (Collins-Thompson and Callan, 2005), and text generation (Hovy, 1990; Inkpen and Hirst, 2006).",1 Introduction,[0],[0]
"Following the classic work of Biber (1988), computational modeling of style has often focused on textual statistics and the frequency of function words and syntactic categories.",1 Introduction,[0],[0]
"When content words are considered, they are often limited to manually-constructed lists (Argamon
et al., 2007), or used as individual features for supervised classification, which can be confounded by topic (Petrenz and Webber, 2011) or fail in the face of lexical variety.",1 Introduction,[0],[0]
"Our interest is models that offer broad lexical coverage of human-identifiable stylistic variation.
",1 Introduction,[0],[0]
"Research most similar to ours has focused on classifying the lexicon in terms of individual aspects relevant to style (e.g. formality, specificity, readability) (Brooke et al., 2010; Pan and Yang, 2010; Kidwell et al., 2009) and a large body of research on the induction of polarity lexicons, in particular from large corpora (Turney, 2002; Kaji and Kitsuregawa, 2007; Velikovich et al., 2010).",1 Introduction,[0],[0]
"Our work is the first to represent multiple dimensions of style in a single statistical model, adapting latent Dirichlet allocation (Blei et al., 2003), a Bayesian ‘topic’ model, to our stylistic purposes; as such, our approach also follows on recent interest in the interpretability of topic-model topics (Chang et al., 2009; Newman et al., 2011).",1 Introduction,[0],[0]
"We show that our model can be used for acquisition of stylistic lexicons, and we also evaluate the model relative to theories of register variation and the expected stylistic character of particular genres.",1 Introduction,[0],[0]
"In English manuals of style and other prescriptivist texts (Fowler and Fowler, 1906; Gunning, 1952; Follett, 1966; Strunk and White, 1979; Kane, 1983; Hayakawa, 1994), writers are urged to pay attention to various aspects of lexical style, including elements such as clarity, familiarity, readability, for-
673
mality, fanciness, colloquialness, specificity, concreteness, objectivity, and naturalness; these stylistic categories reflect common aesthetic judgments about language.",2.1 Linguistic foundations,[0],[0]
"In descriptive studies of register, some researchers have posited a few fixed styles (Joos, 1961) or a small, discrete set of situational constraints which determine style and register (Crystal and Davy, 1969; Halliday and Hasan, 1976); by contrast, the applied approach of Biber (1988) and theoretical framework of Leckie-Tarry (1995) offer a more continuous interpretation of register variation.
",2.1 Linguistic foundations,[0],[0]
"In Biber’s approach, functional dimensions such as Involved vs. Informational, Argumentative vs. Non-argumentative, and Abstract vs. non-Abstract are derived in an unsupervised manner from a mixed-genre corpus, with the labels assigned depending on where features (a small set of known indicators of register) and genres fall on each spectrum.",2.1 Linguistic foundations,[0],[0]
"The theory of Leckie-Tarry posits a single main cline of register with one pole (the oral pole) reflecting a full reliance on the context of the linguistic situation, and the other (the literate pole) reflecting a reliance on cultural knowledge.",2.1 Linguistic foundations,[0],[0]
"The more specific elements of register are represented as subclines which are strongly influenced by this main cline, creating probabilistic relationships between related dimensions (Birch, 1995).
",2.1 Linguistic foundations,[0],[0]
"For the present study, we have chosen 3 dimensions (6 styles) which are clearly represented in the lexicon, which are discussed often in the relevant literature, and which fit well into the Leckie-Tarry conception of related subclines: colloquial vs. literary, concrete vs. abstract, and subjective vs. objective.",2.1 Linguistic foundations,[0],[0]
"In addition to a negative correlation between opposing styles, we also expect a positive correlation between stylistic aspects that tend toward the same main pole, situational (i.e. colloquial, concrete, subjective) or cultural (i.e. literary, abstract, objective).",2.1 Linguistic foundations,[0],[0]
These correlations can potentially interfere with accurate lexical acquisition.,2.1 Linguistic foundations,[0],[0]
"Our main model is an adaption of the popular latent Dirichlet allocation topic model (Blei et al., 2003), with each of the 6 styles corresponding to a topic.",2.2 Implementation,[0],[0]
"Briefly, latent Dirichlet allocation (LDA) is a generative Bayesian model: for each document d, a distribution of topics θd is drawn from a Dirichlet prior
(with parameter α).",2.2 Implementation,[0],[0]
"For each topic z, there is a probability distribution βz1 corresponding to the probability of that topic generating any given word in the vocabulary.",2.2 Implementation,[0],[0]
"Words in document d are generated by first selecting a topic z randomly according to θd , and then randomly selecting a word w according to βz.",2.2 Implementation,[0],[0]
"An extension of LDA, the correlated topic model (CTM) (Blei and Lafferty, 2007), supposes a more complex representation of topics: given a matrix Σ representing the covariance between topics and µ representing the means, for each document a topic distribution η (analogous to θ ) is drawn from the logistic normal distribution.",2.2 Implementation,[0],[0]
"Given a corpus, good estimates for the relevant parameters can be derived using Bayesian inference.
",2.2 Implementation,[0],[0]
For both LDA and CTM we use the original variational Bayes implementation of Blei.,2.2 Implementation,[0],[0]
"Variational Bayes (VB) works by approximating the true posterior with a simpler distribution, minimizing the Kullback-Leibler divergence between the two through iterative updates of specially-introduced free variables.",2.2 Implementation,[0],[0]
The mathematical and algorithmic details are omitted here; see Blei et al. (2003; 2007).,2.2 Implementation,[0],[0]
"Our early investigations used an online, batch version of LDA (Hoffman et al., 2010), which is more appropriate for large corpora because it requires only a single iteration over the dataset.",2.2 Implementation,[0],[0]
"We discovered, however, that batch models were markedly inferior to more traditional models for our purposes because the influence of the initial model diminishes too quickly; here, we need particular topics in the model to correspond to particular styles, and we accomplish this by seeding the model with known instances of each style (see Section 3).",2.2 Implementation,[0],[0]
"Specifically, our initial β consists of distributions where the entire probability mass is divided amongst the seeds for each corresponding topic, and a full iteration over the corpus occurs before β is updated.",2.2 Implementation,[0],[0]
"Typically, LDA iterates over the corpus until a convergence requirement is met, but in this case this is neither practical (due to the size of our corpus) nor necessarily desirable; the diminishing effects of the initial seeding means that the model may not stabilize, in terms of its likelihood, until after it has shifted away from our desired stylistic dimensions towards some other
1Some versions of LDA smooth this distribution using a Dirichlet prior; here, though, we use the original formulation from Blei (2003), which does not.
variation in the data.",2.2 Implementation,[0],[0]
"Therefore, we treat the optimal number of iterations as a variable to investigate.
",2.2 Implementation,[0],[0]
"The model is trained on a 1 million text portion of the 2009 version of the ICWSM Spinn3r dataset (Burton et al., 2009), a corpus of blogs we have previously used for formality lexicon induction (Brooke et al., 2010).",2.2 Implementation,[0],[0]
"Since our method relies on cooccurrence, we followed our earlier work in using only texts with at least 100 different word types.",2.2 Implementation,[0],[0]
"All words were tokenized and converted to lower-case, with no further lemmatization.",2.2 Implementation,[0],[0]
"Following Hoffman et al. (2010), we initialized the α of our models to 1/k where k is the number of topics.",2.2 Implementation,[0],[0]
Otherwise we used the default settings; when they overlap they were identical for the LDA and CTM models.,2.2 Implementation,[0],[0]
Our primary evaluation is based on the stylistic induction of held-out seed words.,3 Lexicon Induction,[0],[0]
The words were collected from various sources by the first author and further reviewed by the second; we are both native speakers of English with significant experience in English linguistics.,3 Lexicon Induction,[0],[0]
"Included words had to be clear, extreme members of their stylistic category, with little or no ambiguity with respect to their style.",3 Lexicon Induction,[0],[0]
"The colloquial seeds consist of English slang terms and acronyms, e.g. cuz, gig, asshole, lol.",3 Lexicon Induction,[0],[0]
"The literary seeds were primarily drawn from web sites which explain difficult language in texts such as the Bible and Lord of the Rings; examples include behold, resplendent, amiss, and thine.",3 Lexicon Induction,[0],[0]
"The concrete seeds all denote objects and actions strongly rooted in the physical world, e.g. shove and lamppost, while the abstract seeds all involve concepts which require significant human psychological or cultural knowledge to grasp, for instance patriotism and nonchalant.",3 Lexicon Induction,[0],[0]
"For our subjective seeds, we used an edited list of strongly positive and negative terms from a manually-constructed sentiment lexicon (Taboada et al., 2011), e.g. gorgeous and depraved, and for our objective set we selected words from sets of nearsynonyms where one was clearly an emotionallydistant alternative, e.g. residence (for home), jocular (for funny) and communicable (for contagious).",3 Lexicon Induction,[0],[0]
"We filtered initial lists to 150 of each type, removing words which did not appear in the corpus or which occurred in multiple lists.",3 Lexicon Induction,[0],[0]
"For evaluation we
used stratified 3-fold crossvalidation, averaged over 5 different (3-way) splits of the seeds, with the same splits used for all evaluated conditions.
",3 Lexicon Induction,[0],[0]
"Given two sets of opposing seeds, we follow our earlier work in evaluating our performance in terms of the number of pairings of seeds from each set which have the expected stylistic relationship relative to each other (the guessing baseline is 0.5).",3 Lexicon Induction,[0],[0]
"Given a word w and two opposing styles (topics) p and n, we place w on the PN dimension according to the β of our trained model as follows:
PNw = βpw−βnw βpw",3 Lexicon Induction,[0],[0]
"+βnw
The normalization is important because otherwise more-common words would tend to have higher PN’s, when in fact the opposite is true (rare words tend to be more stylistically prominent).",3 Lexicon Induction,[0],[0]
"We then calculate pairwise accuracy as the percentage of pairs 〈wp,wn〉 (wp ∈ Pseeds and wn ∈ Nseeds) where PNwp > PNwn .",3 Lexicon Induction,[0],[0]
"However, this metric does not address the case where the degree of a word in one stylistic dimension is overestimated because of its status on a parallel dimension.",3 Lexicon Induction,[0],[0]
"Two more-holistic alternatives are total accuracy, the percentage of seeds for which the highest βtw is the topic t for which w is a seed (guessing baseline is 0.17), and the average rank of the correct t as ordered by βtw (in the range 1–6, guessing baseline is 3.5); the latter is more forgiving of near misses.
",3 Lexicon Induction,[0],[0]
We tested a few options which involved straightforward modifications to model training.,3 Lexicon Induction,[0],[0]
"Standard LDA produces all tokens in the document, but when dealing with style rather than topic, the number of times a word appears is much less relevant (Brooke et al., 2010).",3 Lexicon Induction,[0],[0]
"Our binary model assumes an LDA that generates types, not tokens.2 A key comparison
2At the theoretical level, this move is admittedly problematic, since our LDA model is thus being trained under the assumption that texts with multiple instances of the same type can be generated, when of course such texts cannot by definition exist.",3 Lexicon Induction,[0],[0]
"We might address this by moving to Bayesian models with very different generative assumptions, e.g. the spherical topic model (Reisinger et al., 2010), but these methods involve a significant increase of computational complexity and we believe that on a practical level there are no real negatives associated with directly using a binary representation as input to LDA; in fact, we are avoiding what appears to be a much more serious problem, burstiness (Doyle and Elkan, 2009), i.e. the fact that
here is with a combined LDA model (combo), an amalgamation of three independently trained 2-topic models, one for each dimension; this tests our key hypothesis that training dimensions of style together is beneficial.",3 Lexicon Induction,[0],[0]
"Finally, we test against the correlated topic model (CTM), which offers an explicit representation of style correlation, but which has done poorly with respect to interpretability, despite offering better perplexity (Chang et al., 2009).
",3 Lexicon Induction,[0],[0]
The results of the lexicon induction evaluation are in Table 1.,3 Lexicon Induction,[0],[0]
"Since the number of optimal iterations varies, we report the result from the best of the first five iterations, as measured by total accuracy; the best iteration is shown in parenthesis.",3 Lexicon Induction,[0],[0]
"In general, all the results are high enough—we are reliably above 90% for the pairwise task, and above 50% for the 6-way task—for us to conclude with some confidence that our model is capturing a significant amount of stylistic variation.",3 Lexicon Induction,[0],[0]
"As predicted, using words as boolean features had a net positive gain, consistent across all of our metrics, though this effect was not as marked as we have seen previously.",3 Lexicon Induction,[0],[0]
"The model with independent training of each dimension (combo) did noticeably worse, supporting our conclusion that a multidimensional approach is warranted here.",3 Lexicon Induction,[0],[0]
"Particularly striking is the much larger drop in overall accuracy as compared to pairwise accuracy, which suggests that the combo model is capturing the general trends but not distinguishing correlated styles as well.",3 Lexicon Induction,[0],[0]
"However, the most complex model, the CTM, actually does slightly worse than the combo, which was contrary to our expectations but nonetheless consistent with previous work on the interpretability of topic models.",3 Lexicon Induction,[0],[0]
"The performance of the full LDA models benefited from a second itera-
traditional LDA is influenced too much by multiple instances of the same word.
tion, but this was not true of combo LDA or CTM, and the performance of all models dropped after the second iteration.
",3 Lexicon Induction,[0],[0]
"An analysis of individual errors reveals, unsurprisingly, that most of the errors occur across styles on the same pole; by far the largest single common misclassification is objective words to abstract.",3 Lexicon Induction,[0],[0]
"Of the words that consistently show this misclassification across the runs, many of them, e.g. animate, aperture, encircle, and constrain are clearly errors (if anything, these words tend towards concreteness), but in other cases the word in question is arguably also fairly abstract, e.g. categorize and predominant, and might not be labeled an error at all.",3 Lexicon Induction,[0],[0]
"Other signs that our model might be doing better than our total accuracy metric gives it credit for: many of the subjective words that are consistently mislabeled as literary have an exaggerated, literary feel, e.g. jubilant, grievous, and malevolent.",3 Lexicon Induction,[0],[0]
Our secondary analysis involved evaluating the θ ’s of our best configuration (based on average pairwise and total accuracy) on other texts.,4 Text-level Analysis,[0],[0]
"After training, we carried out inference on the BNC corpus, averaging the resulting θ ’s to see which styles are associated with which genres.",4 Text-level Analysis,[0],[0]
Appearances of the seed terms for each model were disregarded during this process; only the induced part of the lexicon was used.,4 Text-level Analysis,[0],[0]
"The average differences relative to the mean across the various stylistic dimensions (as measured by the probabilities in θ ) are given for a selection of genres in Table 2.
",4 Text-level Analysis,[0],[0]
"The most obvious pattern in table 2 is the dominance of the medium: all written genres are positive for our styles on the ‘cultural’ pole and negative for styles on the ‘situational’ pole and the opposite is
true for spoken genres.",4 Text-level Analysis,[0],[0]
"The magnitude of this effect is more difficult to interpret: though it is clear why fiction should sit on the boundary (since it contains spoken dialogue), the appearance of news at the written extreme is odd, though it might be due to the fact that news blogs are the most prevalent formal genre in the training corpus.
",4 Text-level Analysis,[0],[0]
"However, if we ignore magnitude and focus on the relative ratios of the stylistic differences for styles on the same pole, we can identify some individual stylistic effects among genres within the same medium.",4 Text-level Analysis,[0],[0]
"Relative to the other written genres, for instance, fiction is, sensibly, more literary and much less objective, while academic texts are much more abstract and objective; for the other two written genres, the spread is more even, though relative to religious texts, news is more objective.",4 Text-level Analysis,[0],[0]
"At the situational pole, fiction also stands out, being much more colloquial and concrete than other written genres.",4 Text-level Analysis,[0],[0]
"Predictably, if we consider again the ratios across styles, conversation is the most colloquial genre here, though the difference is subtle.
",4 Text-level Analysis,[0],[0]
"We carried out a correlation analysis of the LDAreduced styles of all texts in the BNC and, consistent with the genre results in Table 2, found a strong positive correlation for all styles on the same main pole, averaging 0.83.",4 Text-level Analysis,[0],[0]
"The average negative correlation between opposing poles is even higher, −0.88.",4 Text-level Analysis,[0],[0]
This supports the Leckie-Tarry formulation.,4 Text-level Analysis,[0],[0]
"The independence assumptions of the LDA model did not prevent strong correlations from forming between these distinct yet clearly interrelated dimensions; if anything, the correlations are stronger than we would have predicted.",4 Text-level Analysis,[0],[0]
We have introduced a Bayesian model of stylistic variation.,5 Conclusion,[0],[0]
"Topic models like LDA are often evaluated using information-theoretic measures, but our emphasis has been on interpretibility: at the word level we can use the model to induce stylistic lexicons which correspond to human judgement, and at the text level we can use it distinguish genres in expected ways.",5 Conclusion,[0],[0]
"Another theme has been to offer evidence that indeed a multi-dimensional approach is strongly warranted: importantly, our results indicate that separate unidimensional models of style are inferior for identifying the core stylistic character of each word, and in our secondary analysis we found strong correlations among styles attributable to the situational/cultural dichotomy.",5 Conclusion,[0],[0]
"However, an off-theshelf model that integrates correlation among topics did not outperform basic LDA.
",5 Conclusion,[0],[0]
"One advantage of a Bayesian approach is in the flexibility of the model: there are any number of other interesting possible extensions at both the θ and β levels of the model, including alternative approaches to correlation (Li and McCallum, 2006).",5 Conclusion,[0],[0]
"Beyond Bayesian models, vector space and graphical approaches should be compared.",5 Conclusion,[0],[0]
"More work is clearly needed to improve evaluation: some of our seeds could fall into multiple stylistic categories, so a more detailed annotation would be useful.",5 Conclusion,[0],[0]
This work was financially supported by the Natural Sciences and Engineering Research Council of Canada.,Acknowledgements,[0],[0]
"We adapt the popular LDA topic model (Blei et al., 2003) to the representation of stylistic lexical information, evaluating our model on the basis of human-interpretability at the word and text level.",abstractText,[0],[0]
"We show, in particular, that this model can be applied to the task of inducing stylistic lexicons, and that a multi-dimensional approach is warranted given the correlations among stylistic dimensions.",abstractText,[0],[0]
A Multi-Dimensional Bayesian Approach to Lexical Style,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 799–809 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
799",text,[0],[0]
"When we use supervised learning to solve Natural Language Processing (NLP) problems, we typically train an individual model for each task with task-specific labeled data.",1 Introduction,[0],[0]
"However, our target task may be intrinsically linked to other tasks.",1 Introduction,[0],[0]
"For example, Part-of-speech (POS) tagging and Name Tagging can both be considered as sequence labeling; Machine Translation (MT) and Abstractive Text Summarization both require the ability to understand the source text and generate natural language sentences.",1 Introduction,[0],[0]
"Therefore, it is valuable to transfer knowledge from related tasks to the target task.",1 Introduction,[0],[0]
"Multi-task Learning (MTL) is one of
∗*",1 Introduction,[0],[0]
"Part of this work was done when the first author was on an internship at Facebook.
1The code of our model is available at https://github.",1 Introduction,[0],[0]
"com/limteng-rpi/mlmt
the most effective solutions for knowledge transfer across tasks.",1 Introduction,[0],[0]
"In the context of neural network architectures, we usually perform MTL by sharing parameters across models (Ruder, 2017).
",1 Introduction,[0],[0]
"Previous studies (Collobert and Weston, 2008; Dong et al., 2015; Luong et al., 2016; Liu et al., 2018; Yang et al., 2017) have proven that MTL is an effective approach to boost the performance of related tasks such as MT and parsing.",1 Introduction,[0],[0]
"However, most of these previous efforts focused on tasks and languages which have sufficient labeled data but hit a performance ceiling on each task alone.",1 Introduction,[0],[0]
"Most NLP tasks, including some well-studied ones such as POS tagging, still suffer from the lack of training data for many low-resource languages.",1 Introduction,[0],[0]
"According to Ethnologue2, there are 7, 099 living languages in the world.",1 Introduction,[0],[0]
"It is an unattainable goal to annotate data in all languages, especially for tasks with complicated annotation requirements.",1 Introduction,[0],[0]
"Furthermore, some special applications (e.g., disaster response and recovery) require rapid development of NLP systems for extremely low-resource languages.",1 Introduction,[0],[0]
"Therefore, in this paper, we concentrate on enhancing supervised models in low-resource settings by borrowing knowledge learned from related high-resource languages and tasks.
",1 Introduction,[0],[0]
"In (Yang et al., 2017), the authors simulated a low-resource setting for English and Spanish by downsampling the training data for the target task.",1 Introduction,[0],[0]
"However, for most low-resource languages, the data sparsity problem also lies in related tasks and languages.",1 Introduction,[0],[0]
"Under such circumstances, a single transfer model can only bring limited improvement.",1 Introduction,[0],[0]
"To tackle this issue, we propose a multi-lingual multi-task architecture which combines different transfer models within a unified architecture through two levels of parameter sharing.",1 Introduction,[0],[0]
"In the first level, we share character embeddings,
2https://www.ethnologue.com/guides/ how-many-languages
character-level convolutional neural networks, and word-level long-short term memory layer across all models.",1 Introduction,[0],[0]
These components serve as a basis to connect multiple models and transfer universal knowledge among them.,1 Introduction,[0],[0]
"In the second level, we adopt different sharing strategies for different transfer schemes.",1 Introduction,[0],[0]
"For example, we use the same output layer for all Name Tagging tasks to share task-specific knowledge (e.g., I-PER3 should not be assigned to the first word in a sentence).
",1 Introduction,[0],[0]
"To illustrate our idea, we take sequence labeling as a case study.",1 Introduction,[0],[0]
"In the NLP context, the goal of sequence labeling is to assign a categorical label (e.g., POS tag) to each token in a sentence.",1 Introduction,[0],[0]
"It underlies a range of fundamental NLP tasks, including POS Tagging, Name Tagging, and chunking.
",1 Introduction,[0],[0]
Experiments show that our model can effectively transfer various types of knowledge from different auxiliary tasks and obtains up to 50.5% absolute F-score gains on Name Tagging compared to the mono-lingual single-task baseline.,1 Introduction,[0],[0]
"Additionally, our approach does not rely on a large amount of auxiliary task data to achieve the improvement.",1 Introduction,[0],[0]
"Using merely 1% auxiliary data, we already obtain up to 9.7% absolute gains in Fscore.",1 Introduction,[0],[0]
The goal of sequence labeling is to assign a categorical label to each token in a given sentence.,2.1 Basic Architecture,[0],[0]
"Though traditional methods such as Hidden Markov Models (HMMs) and Conditional Random Fields (CRFs) (Lafferty et al., 2001; Ratinov and Roth, 2009; Passos et al., 2014) achieved high performance on sequence labeling tasks, they typically relied on hand-crafted features, therefore it is difficult to adapt them to new tasks or languages.",2.1 Basic Architecture,[0],[0]
"To avoid task-specific engineering, (Collobert et al., 2011) proposed a feed-forward neural network model that only requires word embeddings trained on a large scale corpus as features.",2.1 Basic Architecture,[0],[0]
"After that, several neural models based on the combination of long-short term memory (LSTM) and CRFs (Ma and Hovy, 2016; Lample et al., 2016; Chiu and Nichols, 2016) were proposed and
3We adopt the BIOES annotation scheme.",2.1 Basic Architecture,[0],[0]
"Prefixes B-, I, E-, and S- represent the beginning of a mention, inside of a mention, the end of a mention and a single-token mention respectively.",2.1 Basic Architecture,[0],[0]
"The O tag is assigned to a word which is not part of any mention.
achieved better performance on sequence labeling tasks.
",2.1 Basic Architecture,[0],[0]
"LSTM-CRFs-based models are well-suited for multi-lingual multi-task learning for three reasons: (1) They learn features from word and character embeddings and therefore require little feature engineering; (2) As the input and output of each layer in a neural network are abstracted as vectors, it is fairly straightforward to share components between neural models; (3) Character embeddings can serve as a bridge to transfer morphological and semantic information between languages with identical or similar scripts, without requiring cross-lingual dictionaries or parallel sentences.
",2.1 Basic Architecture,[0],[0]
"Therefore, we design our multi-task multilingual architecture based on the LSTM-CNNs model proposed in (Chiu and Nichols, 2016).",2.1 Basic Architecture,[0],[0]
The overall framework is illustrated in Figure 1.,2.1 Basic Architecture,[0],[0]
"First, each word wi is represented as the combination xi of two parts, word embedding and character feature vector, which is extracted from character embeddings of the characters in wi using convolutional neural networks (CharCNN).",2.1 Basic Architecture,[0],[0]
"On top of that, a bidirectional LSTM processes the sequence x = {x1, x2, ...} in both directions and encodes each word and its context into a fixed-size vector hi.",2.1 Basic Architecture,[0],[0]
"Next, a linear layer converts hi to a score vector yi, in which each component represents the predicted score of a target tag.",2.1 Basic Architecture,[0],[0]
"In order to model correlations between tags, a CRFs layer is added at the top to generate the best tagging path for the whole sequence.",2.1 Basic Architecture,[0],[0]
"In the CRFs layer, given an input sentence x of length L and the output of the linear layer y, the score of a sequence of tags z is
defined as:
S(x,y, z) = L∑
t=1
(Azt−1,zt + yt,zt),
where A is a transition matrix in which Ap,q represents the binary score of transitioning from tag p to tag q, and yt,z represents the unary score of assigning tag z to the t-th word.",2.1 Basic Architecture,[0],[0]
"Given the ground truth sequence of tags z, we maximize the following objective function during the training phase:
O = logP (z|x)",2.1 Basic Architecture,[0],[0]
"= S(x,y, z)− log ∑ z̃∈Z eS(x,y,z̃),
where Z is the set of all possible tagging paths.",2.1 Basic Architecture,[0],[0]
We emphasize that our actual implementation differs slightly from the LSTM-CNNs model.,2.1 Basic Architecture,[0],[0]
"We do not use additional word- and characterlevel explicit symbolic features (e.g., capitalization and lexicon) as they may require additional language-specific knowledge.",2.1 Basic Architecture,[0],[0]
"Additionally, we transform character feature vectors using highway networks (Srivastava et al., 2015), which is reported to enhance the overall performance by (Kim et al., 2016) and (Liu et al., 2018).",2.1 Basic Architecture,[0],[0]
Highway networks is a type of neural network that can smoothly switch its behavior between transforming and carrying information.,2.1 Basic Architecture,[0],[0]
"MTL can be employed to improve performance on multiple tasks at the same time, such as MT and parsing in (Luong et al., 2016).",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"However, in our scenario, we only focused on enhancing the performance of a low-resource task, which is our target task or main task.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
Our proposed architecture aims to transfer knowledge from a set of auxiliary tasks to the main task.,2.2 Multi-task Multi-lingual Architecture,[0],[0]
"For simplicity, we refer to a model of a main (auxiliary) task as a main (auxiliary) model.
",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"To jointly train multiple models, we perform multi-task learning using parameter sharing.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"Let Θi be the set of parameters for model mi and Θi,j = Θi ∩Θj be the shared parameters between mi and mj .",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"When optimizing model mi, we update Θi and hence Θi,j .",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"In this way, we can partially train model mj as Θi,j ⊆ Θj .",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"Previously, each MTL model generally uses a single transfer scheme.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"In order to merge different transfer models into a unified architecture, we employ two levels of parameter sharing as follows.
",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"On the first level, we construct the basis of the architecture by sharing character embeddings, CharCNN and bidirectional LSTM among all models.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"This level of parameter sharing aims to provide universal word representation and feature extraction capability for all tasks and languages.
",2.2 Multi-task Multi-lingual Architecture,[0],[0]
Character Embeddings and Character-level CNNs.,2.2 Multi-task Multi-lingual Architecture,[0],[0]
"Character features can represent morphological and semantic information; e.g., the English morpheme dis- usually indicates negation and reversal as in “disagree” and “disapproval”.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"For low-resource languages lacking in data to suffice the training of high-quality word embeddings, character embeddings learned from other languages may provide crucial information for labeling, especially for rare and out-of-vocabulary words.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
Take the English word “overflying” (flying over) as an example.,2.2 Multi-task Multi-lingual Architecture,[0],[0]
"Even if it is rare or absent in the corpus, we can still infer the word meaning from its suffix over- (above), root fly, and prefix -ing (present participle form).",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"In our architecture, we share character embeddings and the CharCNN between languages with identical or similar scripts to enhance word representation for low-resource languages.
",2.2 Multi-task Multi-lingual Architecture,[0],[0]
Bidirectional LSTM.,2.2 Multi-task Multi-lingual Architecture,[0],[0]
"The bidirectional LSTM layer is essential to extract character, word, and contextual information from a sentence.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"However, with a large number of parameters, it cannot be fully trained only using the low-resource task data.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"To tackle this issue, we share the bidirectional LSTM layer across all models.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"Bear in mind that because our architecture does not require aligned cross-lingual word embeddings, sharing this layer across languages may confuse the model as it equally handles embeddings in different spaces.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"Nevertheless, under low-resource circumstances, data sparsity is the most critical factor that affects the performance.
",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"On top of this basis, we adopt different parameter sharing strategies for different transfer schemes.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"For cross-task transfer, we use the same word embedding matrix across tasks so that they can mutually enhance word representations.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"For cross-lingual transfer, we share the linear layer and CRFs layer among languages to transfer taskspecific knowledge, such as the transition score between two tags.
",2.2 Multi-task Multi-lingual Architecture,[0],[0]
Word Embeddings.,2.2 Multi-task Multi-lingual Architecture,[0],[0]
"For most words, in addition to character embeddings, word embeddings are still crucial to represent semantic informa-
tion.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
We use the same word embedding matrix for tasks in the same language.,2.2 Multi-task Multi-lingual Architecture,[0],[0]
The matrix is initialized with pre-trained embeddings and optimized as parameters during training.,2.2 Multi-task Multi-lingual Architecture,[0],[0]
"Thus, task-specific knowledge can be encoded into the word embeddings by one task and subsequently utilized by another one.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"For a low-resource language even without sufficient raw text, we mix its data with a related high-resource language to train word embeddings.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"In this way, we merge both corpora and hence their vocabularies.
",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"Recently, Conneau et al. (2017) proposed a domain-adversarial method to align two monolingual word embedding matrices without crosslingual supervision such as a bilingual dictionary.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"Although cross-lingual word embeddings are not required, we evaluate our framework with aligned embeddings generated using this method.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"Experiment results show that the incorporation of crosslingual embeddings substantially boosts the performance under low-resource settings.
",2.2 Multi-task Multi-lingual Architecture,[0],[0]
Linear Layer and CRFs.,2.2 Multi-task Multi-lingual Architecture,[0],[0]
"As the tag set varies from task to task, the linear layer and CRFs can only be shared across languages.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
We share these layers to transfer task-specific knowledge to the main model.,2.2 Multi-task Multi-lingual Architecture,[0],[0]
"For example, our model corrects [SPER Charles]",2.2 Multi-task Multi-lingual Architecture,[0],[0]
[S-PER Picqué] to [B-PER Charles],2.2 Multi-task Multi-lingual Architecture,[0],[0]
[E-PER Picqué] because the CRFs layer fully trained on other languages assigns a low score to the rare transition S-PER→S-PER and promotes B-PER→E-PER.,2.2 Multi-task Multi-lingual Architecture,[0],[0]
"In addition to the shared linear layer, we add an unshared language-specific linear layer to allow the model to behave differently
toward some features for different languages.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"For example, the suffix -ment usually indicates nouns in English whereas indicates adverbs in French.
",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"We combine the output of the shared linear layer yu and the output of the language-specific linear layer ys using:
y = g ⊙ ys",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"+ (1− g)⊙ yu,
where g = σ(W gh + bg).",2.2 Multi-task Multi-lingual Architecture,[0],[0]
W g,2.2 Multi-task Multi-lingual Architecture,[0],[0]
and bg are optimized during training.,2.2 Multi-task Multi-lingual Architecture,[0],[0]
h is the LSTM hidden states.,2.2 Multi-task Multi-lingual Architecture,[0],[0]
"As W g is a square matrix, y, ys, and yu have the same dimension.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"Although we only focus on sequence labeling in this work, our architecture can be adapted for many NLP tasks with slight modification.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"For example, for text classification tasks, we can take the last hidden state of the forward LSTM as the sentence representation and replace the CRFs layer with a Softmax layer.
",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"In our model, each task has a separate object function.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"To optimize multiple tasks within one model, we adopt the alternating training approach in (Luong et al., 2016).",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"At each training step, we sample a task di with probability ri∑
j rj , where ri
is the mixing rate value assigned to di.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"In our experiments, instead of tuning ri, we estimate it by:
ri = µiζi √ Ni ,
where µi is the task coefficient, ζi is the language coefficient, and Ni is the number of training examples.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"µi (or ζi) takes the value 1 if the task
(or language) of di is the same as that of the target task; Otherwise it takes the value 0.1.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"For example, given English Name Tagging as the target task, the task coefficient µ and language coefficient ζ of Spanish Name Tagging are 0.1 and 1 respectively.
",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"While assigning lower mixing rate values to auxiliary tasks, this formula also takes the amount of data into consideration.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"Thus, auxiliary tasks receive higher probabilities to reduce overfitting when we have a smaller amount of main task data.",2.2 Multi-task Multi-lingual Architecture,[0],[0]
"For Name Tagging, we use the following data sets: Dutch (NLD) and Spanish (ESP) data from the CoNLL 2002 shared task (Tjong Kim Sang, 2002), English (ENG) data from the CoNLL 2003 shared task (Tjong Kim Sang and De Meulder, 2003), Russian (RUS) data from LDC2016E95 (Russian Representative Language Pack), and Chechen (CHE) data from TAC KBP 2017 10-Language EDL Pilot Evaluation Source Corpus4.",3.1 Data Sets,[0],[0]
"We select Chechen as another target language in addition to Dutch and Spanish because it is a truly under-resourced language and its related language, Russian, also lacks NLP resources.
",3.1 Data Sets,[0],[0]
"For POS Tagging, we use English, Dutch, Spanish, and Russian data from the CoNLL 2017 shared task (Zeman et al., 2017; Nivre et al., 2017).",3.1 Data Sets,[0],[0]
"In this data set, each token is annotated with two POS tags, UPOS (universal POS tag) and XPOS (language-specific POS tag).",3.1 Data Sets,[0],[0]
We use UPOS because it is consistent throughout all languages.,3.1 Data Sets,[0],[0]
We use 50-dimensional pre-trained word embeddings and 50-dimensional randomly initialized character embeddings.,3.2 Experimental Setup,[0],[0]
We train word embeddings using the word2vec package5.,3.2 Experimental Setup,[0],[0]
"English, Span-
4https://tac.nist.gov/2017/KBP/data.html 5https://github.com/tmikolov/word2vec
ish, and Dutch embeddings are trained on corresponding Wikipedia articles (2017-12-20 dumps).",3.2 Experimental Setup,[0],[0]
Russian embeddings are trained on documents in LDC2016E95.,3.2 Experimental Setup,[0],[0]
Chechen embeddings are trained on documents in TAC KBP 2017 10-Language EDL Pilot Evaluation Source Corpus.,3.2 Experimental Setup,[0],[0]
"To learn a mapping between mono-lingual word embeddings and obtain cross-lingual embeddings, we use the unsupervised model in the MUSE library6 (Conneau et al., 2017).",3.2 Experimental Setup,[0],[0]
"Although word embeddings are fine-tuned during training, we update the embedding matrix in a sparse way and thus do not have to update a large number of parameters.
",3.2 Experimental Setup,[0],[0]
"We optimize parameters using Stochastic Gradient Descent with momentum, gradient clipping and exponential learning rate decay.",3.2 Experimental Setup,[0],[0]
"At step t, the learning rate αt is updated using αt = α0 ∗ ρt/T , where α0 is the initial learning rate, ρ is the decay rate, and T is the decay step.7 To reduce overfitting, we apply Dropout (Srivastava et al., 2014) to the output of the LSTM layer.
",3.2 Experimental Setup,[0],[0]
"We conduct hyper-parameter optimization by exploring the space of parameters shown in Table 2 using random search (Bergstra and Bengio, 2012).",3.2 Experimental Setup,[0],[0]
"Due to time constraints, we only perform parameter sweeping on the Dutch Name Tagging task with 200 training examples.",3.2 Experimental Setup,[0],[0]
We select the set of parameters that achieves the best performance on the development set and apply it to all models.,3.2 Experimental Setup,[0],[0]
"In Figure 3, 4, and 5, we compare our model with the mono-lingual single-task LSTM-CNNs model (denoted as baseline), cross-task transfer model, and cross-lingual transfer model in low-resource settings with Dutch, Spanish, and Chechen Name Tagging as the main task respectively.",3.3 Comparison of Different Models,[0],[0]
"We use English as the related language for Dutch and Spanish, and use Russian as the related language for
6https://github.com/facebookresearch/MUSE 7Momentum β, gradient clipping threshold, ρ, and T are
set to 0.9, 5.0, 0.9, and 10000 in the experiments.
Chechen.",3.3 Comparison of Different Models,[0],[0]
"For cross-task transfer, we take POS Tagging as the auxiliary task.",3.3 Comparison of Different Models,[0],[0]
"Because the CoNLL 2017 data does not include Chechen, we only use Russian POS Tagging and Russian Name Tagging as auxiliary tasks for Chechen Name Tagging.
",3.3 Comparison of Different Models,[0],[0]
We take Name Tagging as the target task for three reasons: (1) POS Tagging has a much lower requirement for the amount of training data.,3.3 Comparison of Different Models,[0],[0]
"For example, using only 10 training sentences, our baseline model achieves 75.5% and 82.9% prediction accuracy on Dutch and Spanish; (2) Compared to POS Tagging, Name Tagging has been considered as a more challenging task; (3) Existing POS Tagging resources are relatively richer than Name Tagging ones; e.g., the CoNLL 2017 data set provides POS Tagging training data for 45 languages.",3.3 Comparison of Different Models,[0],[0]
"Name Tagging also has a higher annotation cost as its annotation guidelines are usually more complicated.
",3.3 Comparison of Different Models,[0],[0]
We can see that our model substantially outperforms the mono-lingual single-task baseline model and obtains visible gains over single transfer models.,3.3 Comparison of Different Models,[0],[0]
"When trained with less than 50 main tasks training sentences, cross-lingual transfer consistently surpasses cross-task transfer, which is not surprising because in the latter scheme, the linear layer and CRFs layer of the main model are not shared with other models and thus cannot be fully trained with little data.
",3.3 Comparison of Different Models,[0],[0]
"Because there are only 20,400 sentences in Chechen documents, we also experiment with the data augmentation method described in Section 2.2 by training word embeddings on a mixture of Russian and Chechen data.",3.3 Comparison of Different Models,[0],[0]
This method yields additional 3.5%-10.0% absolute F-score gains.,3.3 Comparison of Different Models,[0],[0]
We also experiment with transferring from English to Chechen.,3.3 Comparison of Different Models,[0],[0]
"Because Chechen uses Cyrillic alphabet , we convert its data set to Latin script.",3.3 Comparison of Different Models,[0],[0]
"Surprisingly, although these two languages are not close, we get more improvement by using English as the auxiliary language.
",3.3 Comparison of Different Models,[0],[0]
"In Table 3, we compare our model with state-ofthe-art models using all Dutch or Spanish Name Tagging data.",3.3 Comparison of Different Models,[0],[0]
"Results show that although we design this architecture for low-resource settings, it also achieves good performance in high-resource settings.",3.3 Comparison of Different Models,[0],[0]
"In this experiment, with sufficient training data for the target task, we perform another round of parameter sweeping.",3.3 Comparison of Different Models,[0],[0]
We increase the embedding sizes and LSTM hidden state size to 100 and 225 respectively.,3.3 Comparison of Different Models,[0],[0]
"In Table 4, we compare Name Tagging results from the baseline model and our model, both trained with 100 main task sentences.
",3.4 Qualitative Analysis,[0],[0]
"The first three examples show that shared character-level networks can transfer different levels of morphological and semantic information.
",3.4 Qualitative Analysis,[0],[0]
"In example #1, the baseline model fails to identify “Palestijnen”, an unseen word in the Dutch data, while our model can recognize it because the shared CharCNN represents it in a way similar to its corresponding English word “Palestinians”, which occurs 20 times.",3.4 Qualitative Analysis,[0],[0]
"In addition to mentions, the shared CharCNN can also improve representations of context words, such as “staat” (state) in the example.",3.4 Qualitative Analysis,[0],[0]
"For some words dissimilar to corresponding English words, the CharCNN may enhance their word representations by transferring morpheme-level knowledge.",3.4 Qualitative Analysis,[0],[0]
"For example, in sentence #2, our model is able to identify “Rusland” (Russia) as the suffix -land is usually associated with location names in the English data; e.g., Finland.",3.4 Qualitative Analysis,[0],[0]
"Furthermore, the CharCNN is capable of capturing some word-level patterns, such as capitalized hyphenated compound and acronym as example #3 shows.",3.4 Qualitative Analysis,[0],[0]
"In this sentence, neither “PMScentra” nor “MST” can be found in auxiliary task data, while we observe a number of similar expressions, such as American-style and LDP.
",3.4 Qualitative Analysis,[0],[0]
The transferred knowledge also helps reduce overfitting.,3.4 Qualitative Analysis,[0],[0]
"For example, in sentence #4, the baseline model mistakenly tags “sección” (section) and “consellerı́a” (department) as organizations because their capitalized forms usually appear in Spanish organization names.",3.4 Qualitative Analysis,[0],[0]
"With knowledge learned in auxiliary tasks that a lowercased word is rarely tagged as a proper noun, our model is able to avoid overfitting and correct these errors.",3.4 Qualitative Analysis,[0],[0]
"Sentence #5 shows an opposite situation, where the capitalized word “campesinos” (farm worker) never appears in Spanish names.
",3.4 Qualitative Analysis,[0],[0]
"In Table 5, we show differences between cross-
lingual transfer and cross-task transfer.",3.4 Qualitative Analysis,[0],[0]
"Although the cross-task transfer model recognizes “Ingeborg Marx” missed by the baseline model, it mistakenly assigns an S-PER tag to “Marx”.",3.4 Qualitative Analysis,[0],[0]
"Instead, from English Name Tagging, the cross-lingual transfer model borrows task-specific knowledge through the shared CRFs layer that (1) B-PER→SPER is an invalid transition, and (2) even if we assign S-PER to “Ingeborg”, it is rare to have continuous person names without any conjunction or punctuation.",3.4 Qualitative Analysis,[0],[0]
"Thus, the cross-lingual model promotes the sequence B-PER→E-PER.
",3.4 Qualitative Analysis,[0],[0]
"In Figure 6, we depict the change of tag distribution with the number of training sentences.",3.4 Qualitative Analysis,[0],[0]
"When trained with less than 100 sentences, the baseline model only correctly predicts a few tags dominated by frequent types.",3.4 Qualitative Analysis,[0],[0]
"By contrast, our model has a visibly higher recall and better predicts infrequent tags, which can be attributed to the implicit data augmentation and inductive bias introduced by MTL (Ruder, 2017).",3.4 Qualitative Analysis,[0],[0]
"For example, if all location names in the Dutch training data are single-token ones, the baseline model will inevitably overfit to the tag S-LOC and possibly label “Caldera de Taburiente” as [S-LOC Caldera]",3.4 Qualitative Analysis,[0],[0]
[S-LOC de],3.4 Qualitative Analysis,[0],[0]
"[S-LOC Taburiente], whereas with the shared CRFs layer fully trained on English Name Tagging, our model prefers B-LOC→I-LOC→ELOC, which receives a higher transition score.",3.4 Qualitative Analysis,[0],[0]
"In order to quantify the contributions of individual components, we conduct ablation studies on Dutch Name Tagging with different numbers of training sentences for the target task.",3.5 Ablation Studies,[0],[0]
"For the basic model, we we use separate LSTM layers and
model (B), and result of our model (A).",3.5 Ablation Studies,[0],[0]
"The GREEN ( RED ) highlight indicates a correct (incorrect) tag.
remove the character embeddings, highway networks, language-specific layer, and",3.5 Ablation Studies,[0],[0]
Dropout layer.,3.5 Ablation Studies,[0],[0]
"As Table 6 shows, adding each component usually enhances the performance (F-score, %), while the impact also depends on the size of the target task data.",3.5 Ablation Studies,[0],[0]
"For example, the language-specific layer slightly impairs the performance with only 10 training sentences.",3.5 Ablation Studies,[0],[0]
"However, this is unsurpris-
ing as it introduces additional parameters that are only trained by the target task data.",3.5 Ablation Studies,[0],[0]
"For many low-resource languages, their related languages are also low-resource.",3.6 Effect of the Amount of Auxiliary Task Data,[0],[0]
"To evaluate our model’s sensitivity to the amount of auxiliary task data, we fix the size of main task data and downsample all auxiliary task data with sample rates from 1% to 50%.",3.6 Effect of the Amount of Auxiliary Task Data,[0],[0]
"As Figure 7 shows, the performance goes up when we raise the sample rate from
1% to 20%.",3.6 Effect of the Amount of Auxiliary Task Data,[0],[0]
"However, we do not observe significant improvement when we further increase the sample rate.",3.6 Effect of the Amount of Auxiliary Task Data,[0],[0]
"By comparing scores in Figure 3 and Figure 7, we can see that using only 1% auxiliary data, our model already obtains 3.7%-9.7% absolute F-score gains.",3.6 Effect of the Amount of Auxiliary Task Data,[0],[0]
"Due to space limitations, we only show curves for Dutch Name Tagging, while we observe similar results on other tasks.",3.6 Effect of the Amount of Auxiliary Task Data,[0],[0]
"Therefore, we may conclude that our model does not heavily rely on the amount of auxiliary task data.",3.6 Effect of the Amount of Auxiliary Task Data,[0],[0]
"Multi-task Learning has been applied in different NLP areas, such as machine translation (Luong et al., 2016; Dong et al., 2015; Domhan and Hieber, 2017), text classification (Liu et al., 2017), dependency parsing (Peng et al., 2017), textual entailment (Hashimoto et al., 2017), text summarization (Isonuma et al., 2017) and sequence labeling (Collobert and Weston, 2008; Søgaard and Goldberg, 2016; Rei, 2017; Peng and Dredze, 2017; Yang et al., 2017; von Däniken and Cieliebak, 2017; Aguilar et al., 2017; Liu et al., 2018)
Collobert and Weston (2008) is an early attempt that applies MTL to sequence labeling.",4 Related Work,[0],[0]
"The authors train a CNN model jointly on POS Tagging, Semantic Role Labeling, Name Tagging, chunking, and language modeling using parameter sharing.",4 Related Work,[0],[0]
"Instead of using other sequence labeling tasks, Rei (2017) and Liu et al. (2018) take language modeling as the secondary training objective to extract semantic and syntactic knowledge from large scale raw text without additional supervision.",4 Related Work,[0],[0]
"In (Yang et al., 2017), the authors propose three transfer models for crossdomain, cross-application, and cross-lingual trans-
fer for sequence labeling, and also simulate a lowresource setting by downsampling the training data.",4 Related Work,[0],[0]
"By contrast, we combine cross-task transfer and cross-lingual transfer within a unified architecture to transfer different types of knowledge from multiple auxiliary tasks simultaneously.",4 Related Work,[0],[0]
"In addition, because our model is designed for lowresource settings, we share components among models in a different way (e.g., the LSTM layer is shared across all models).",4 Related Work,[0],[0]
"Differing from most MTL models, which perform supervisions for all tasks on the outermost layer, (Søgaard and Goldberg, 2016) proposes an MTL model which supervised tasks at different levels.",4 Related Work,[0],[0]
It shows that supervising low-level tasks such as POS Tagging at lower layer obtains better performance.,4 Related Work,[0],[0]
We design a multi-lingual multi-task architecture for low-resource settings.,5 Conclusions and Future Work,[0],[0]
We evaluate the model on sequence labeling tasks with three language pairs.,5 Conclusions and Future Work,[0],[0]
Experiments show that our model can effectively transfer different types of knowledge to improve the main model.,5 Conclusions and Future Work,[0],[0]
"It substantially outperforms the mono-lingual single-task baseline model, cross-lingual transfer model, and crosstask transfer model.
",5 Conclusions and Future Work,[0],[0]
"The next step of this research is to apply this architecture to other types of tasks, such as Event Extract and Semantic Role Labeling that involve structure prediction.",5 Conclusions and Future Work,[0],[0]
We also plan to explore the possibility of integrating incremental learning into this architecture to adapt a trained model for new tasks rapidly.,5 Conclusions and Future Work,[0],[0]
This work was supported by the U.S. DARPA LORELEI Program No. HR0011-15-C-0115 and U.S. ARL NS-CTA No.,Acknowledgments,[0],[0]
W911NF-09-2-0053.,Acknowledgments,[0],[0]
"The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Government.",Acknowledgments,[0],[0]
The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on.,Acknowledgments,[0],[0]
We propose a multi-lingual multi-task architecture to develop supervised models with a minimal amount of labeled data for sequence labeling.,abstractText,[0],[0]
"In this new architecture, we combine various transfer models using two layers of parameter sharing.",abstractText,[0],[0]
"On the first layer, we construct the basis of the architecture to provide universal word representation and feature extraction capability for all models.",abstractText,[0],[0]
"On the second level, we adopt different parameter sharing strategies for different transfer schemes.",abstractText,[0],[0]
"This architecture proves to be particularly effective for low-resource settings, when there are less than 200 training sentences for the target task.",abstractText,[0],[0]
"Using Name Tagging as a target task, our approach achieved 4.3%-50.5% absolute Fscore gains compared to the mono-lingual single-task baseline model.",abstractText,[0],[0]
1,abstractText,[0],[0]
A Multi-lingual Multi-task Architecture for Low-resource Sequence Labeling,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 758–763 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
758",text,[0],[0]
"Sentiment classification is an important task of natural language processing (NLP), aiming to classify the sentiment polarity of a given text as positive, negative, or more fine-grained classes.",1 Introduction,[0],[0]
"It has obtained considerable attention due to its broad applications in natural language processing (Hao et al., 2012; Gui et al., 2017).",1 Introduction,[0],[0]
"Most existing studies set up sentiment classifiers using supervised machine learning approaches, such as support vector machine (SVM) (Pang et al., 2002), convolutional neural network (CNN) (Kim, 2014; Bonggun et al., 2017), long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997; Qian et al., 2017), Tree-LSTM (Tai et al., 2015), and attention-based methods (Zhou et al., 2016; Yang et al., 2016; Lin et al., 2017; Du et al., 2017).
",1 Introduction,[0],[0]
"Despite the remarkable progress made by the
previous work, we argue that sentiment analysis still remains a challenge.",1 Introduction,[0],[0]
"Sentiment resources including sentiment lexicon, negation words, intensity words play a crucial role in traditional sentiment classification approaches (Maks and Vossen, 2012; Duyu et al., 2014).",1 Introduction,[0],[0]
"Despite its usefulness, to date, the sentiment linguistic knowledge has been underutilized in most recent deep neural network models (e.g., CNNs and LSTMs).
",1 Introduction,[0],[0]
"In this work, we propose a Multi-sentimentresource Enhanced Attention Network (MEAN) for sentence-level sentiment classification to integrate many kinds of sentiment linguistic knowledge into deep neural networks via multi-path attention mechanism.",1 Introduction,[0],[0]
"Specifically, we first design a coupled word embedding module to model the word representation from character-level and word-level semantics.",1 Introduction,[0],[0]
This can help to capture the morphological information such as prefixes and suffixes of words.,1 Introduction,[0],[0]
"Then, we propose a multisentiment-resource attention module to learn more comprehensive and meaningful sentiment-specific sentence representation by using the three types of sentiment resource words as attention sources attending to the context words respectively.",1 Introduction,[0],[0]
"In this way, we can attend to different sentimentrelevant information from different representation subspaces implied by different types of sentiment sources and capture the overall semantics of the sentiment, negation and intensity words for sentiment prediction.
",1 Introduction,[0],[0]
The main contributions of this paper are summarized as follows.,1 Introduction,[0],[0]
"First, we design a coupled word embedding obtained from character-level embedding and word-level embedding to capture both the character-level morphological information and word-level semantics.",1 Introduction,[0],[0]
"Second, we propose a multi-sentiment-resource attention module to learn more comprehensive sentiment-specific sentence representation from multiply subspaces
implied by three kinds of sentiment resources including sentiment lexicon, intensity words, negation words.",1 Introduction,[0],[0]
"Finally, the experimental results show that MEAN consistently outperforms competitive methods.",1 Introduction,[0],[0]
"Our proposed MEAN model consists of three key components: coupled word embedding module, multi-sentiment-resource attention module, sentence classifier module.",2 Model,[0],[0]
"In the rest of this section, we will elaborate these three parts in details.",2 Model,[0],[0]
"To exploit the sentiment-related morphological information implied by some prefixes and suffixes of words (such as “Non-”, “In-”, “Im-”), we design a coupled word embedding learned from character-level embedding and word-level embedding.",2.1 Coupled Word Embedding,[0],[0]
"We first design a character-level convolution neural network (Char-CNN) to obtain characterlevel embedding (Zhang et al., 2015).",2.1 Coupled Word Embedding,[0],[0]
"Different from (Zhang et al., 2015), the designed CharCNN is a fully convolutional network without max-pooling layer to capture better semantic information in character chunk.",2.1 Coupled Word Embedding,[0],[0]
"Specifically, we first input one-hot-encoding character sequences to a 1 × 1 convolution layer to enhance the semantic nonlinear representation ability of our model (Long et al., 2015), and the output is then fed into a multi-gram (i.e. different window sizes) convolution layer to capture different local character chunk information.",2.1 Coupled Word Embedding,[0],[0]
"For word-level embedding, we use pre-trained word vectors, GloVe (Pennington et al., 2014), to map each word to a lowdimensional vector space.",2.1 Coupled Word Embedding,[0],[0]
"Finally, each word is represented as a concatenation of the characterlevel embedding and word-level embedding.",2.1 Coupled Word Embedding,[0],[0]
"This is performed on the context words and the three types of sentiment resource words 1, resulting in four final coupled word embedding matrices: the W c =",2.1 Coupled Word Embedding,[0],[0]
"[wc1, ..., w c t ] ∈",2.1 Coupled Word Embedding,[0],[0]
"Rd×t for context words, the W s =",2.1 Coupled Word Embedding,[0],[0]
"[ws1, ..., w s m] ∈ Rd×m for sentiment words, the W i =",2.1 Coupled Word Embedding,[0],[0]
"[wi1, ..., w i k] ∈ Rd×k for intensity words, the Wn = [wn1 , ..., w n p ] ∈",2.1 Coupled Word Embedding,[0],[0]
Rd×p for negation words.,2.1 Coupled Word Embedding,[0],[0]
"Here, t,m, k, p are the length of the corresponding items respectively, and d is the embedding dimension.",2.1 Coupled Word Embedding,[0],[0]
"Each W is normalized to better calculate the following word correlation.
",2.1 Coupled Word Embedding,[0],[0]
"1To be precise, sentiment resource words include sentiment words, negation words and intensity words.",2.1 Coupled Word Embedding,[0],[0]
"After obtaining the coupled word embedding, we propose a multi-sentiment-resource attention mechanism to help select the crucial sentimentresource-relevant context words to build the sentiment-specific sentence representation.",2.2 Multi-sentiment-resource Attention Module,[0],[0]
"Concretely, we use the three kinds of sentiment resource words as attention sources to attend to the context words respectively, which is beneficial to capture different sentiment-relevant context words corresponding to different types of sentiment sources.",2.2 Multi-sentiment-resource Attention Module,[0],[0]
"For example, using sentiment words as attention source attending to the context words helps form the sentiment-word-enhanced sentence representation.",2.2 Multi-sentiment-resource Attention Module,[0],[0]
"Then, we combine the three kinds of sentiment-resource-enhanced sentence representations to learn the final sentiment-specific sentence representation.",2.2 Multi-sentiment-resource Attention Module,[0],[0]
"We design three types of attention mechanisms: sentiment attention, intensity attention, negation attention to model the three kinds of sentiment resources, respectively.",2.2 Multi-sentiment-resource Attention Module,[0],[0]
"In the following, we will elaborate the three types of attention mechanisms in details.
",2.2 Multi-sentiment-resource Attention Module,[0],[0]
"First, inspired by (Xiong et al.), we expect to establish the word-level relationship between the context words and different kinds of sentiment resource words.",2.2 Multi-sentiment-resource Attention Module,[0],[0]
"To be specific, we define the dot products among the context words and the three kinds of sentiment resource words as correlation matrices.",2.2 Multi-sentiment-resource Attention Module,[0],[0]
"Mathematically, the detailed formulation is described as follows.
",2.2 Multi-sentiment-resource Attention Module,[0],[0]
M s = (W c)T ·W s ∈ Rt×m (1) M i =,2.2 Multi-sentiment-resource Attention Module,[0],[0]
(W c)T ·W i ∈ Rt×k (2),2.2 Multi-sentiment-resource Attention Module,[0],[0]
"Mn = (W c)T ·Wn ∈ Rt×p (3)
whereM s,M i,",2.2 Multi-sentiment-resource Attention Module,[0],[0]
"Mn are the correlation matrices to measure the relationship among the context words and the three kinds of sentiment resource words, representing the relevance between the context words and the sentiment resource word.
",2.2 Multi-sentiment-resource Attention Module,[0],[0]
"After obtaining the correlation matrices, we can compute the sentiment-resource-relevant context word representations Xcs ,",2.2 Multi-sentiment-resource Attention Module,[0],[0]
"X c i , X c n",2.2 Multi-sentiment-resource Attention Module,[0],[0]
by the dot products among the context words and different types of corresponding correlation matrices.,2.2 Multi-sentiment-resource Attention Module,[0],[0]
"Meanwhile, we can also obtain the context-wordrelevant sentiment word representation matrix Xs by the dot product between the correlation matrix M s and the sentiment words W s, the context-
word-relevant intensity word representation matrix Xi by the dot product between the intensity words W i and the correlation matrix M i, the context-word-relevant negation word representation matrix Xn by the dot product between the negation words Wn and the correlation matrix Mn.",2.2 Multi-sentiment-resource Attention Module,[0],[0]
"The detailed formulas are presented as follows:
Xcs = W cM s, Xs = W s(M",2.2 Multi-sentiment-resource Attention Module,[0],[0]
s)T (4) Xci = W cM,2.2 Multi-sentiment-resource Attention Module,[0],[0]
"i, Xi = W i(M i)T (5) Xcn = W cMn, Xn = Wn(Mn)T (6)
",2.2 Multi-sentiment-resource Attention Module,[0],[0]
"The final enhanced context word representation matrix is computed as:
Xc = Xcs",2.2 Multi-sentiment-resource Attention Module,[0],[0]
"+X c i +X c n. (7)
",2.2 Multi-sentiment-resource Attention Module,[0],[0]
"Next, we employ four independent GRU networks (Chung et al., 2015) to encode hidden states of the context words and the three types of sentiment resource words, respectively.",2.2 Multi-sentiment-resource Attention Module,[0],[0]
"Formally, given the word embedding Xc, Xs, Xi, Xn, the hidden state matrices Hc, Hs, H i, Hn can be obtained as follows:
Hc = GRU(Xc) (8)
Hs = GRU(Xs) (9)
H i = GRU(Xi) (10)
",2.2 Multi-sentiment-resource Attention Module,[0],[0]
"Hn = GRU(Xn) (11)
",2.2 Multi-sentiment-resource Attention Module,[0],[0]
"After obtaining the hidden state matrices, the sentiment-word-enhanced sentence representation o1 can be computed as:
o1 = t∑ i=1",2.2 Multi-sentiment-resource Attention Module,[0],[0]
"αih c i , q s = m∑ i=1",2.2 Multi-sentiment-resource Attention Module,[0],[0]
hsi/m (12) β([hci ; qs]),2.2 Multi-sentiment-resource Attention Module,[0],[0]
= u T s tanh(Ws[h c,2.2 Multi-sentiment-resource Attention Module,[0],[0]
i ; qs]) (13) αi = exp(β([hci ; qs]))∑t i=1,2.2 Multi-sentiment-resource Attention Module,[0],[0]
exp(β([h,2.2 Multi-sentiment-resource Attention Module,[0],[0]
c i ; qs])),2.2 Multi-sentiment-resource Attention Module,[0],[0]
"(14)
where qs denotes the mean-pooling operation towards Hs, β is the attention function that calculates the importance of the i-th word hci in the context and αi indicates the importance of the ith word in the context, us and Ws are learnable parameters.
",2.2 Multi-sentiment-resource Attention Module,[0],[0]
"Similarly, with the hidden states H i and Hn for the intensity words and the negation words as attention sources, we can obtain the intensityword-enhanced sentence representation o2 and the
negation-word-enhanced sentence representation o3.",2.2 Multi-sentiment-resource Attention Module,[0],[0]
"The final comprehensive sentiment-specific sentence representation õ is the composition of the above three sentiment-resource-specific sentence representations o1, o2, o3:
õ",2.2 Multi-sentiment-resource Attention Module,[0],[0]
"= [o1, o2, o3] (15)",2.2 Multi-sentiment-resource Attention Module,[0],[0]
"After obtaining the final sentence representation õ, we feed it to a softmax layer to predict the sentiment label distribution of a sentence:
ŷ = exp(W̃o T õ + b̃o)∑C
i=1",2.3 Sentence Classifier,[0],[0]
exp(W̃o,2.3 Sentence Classifier,[0],[0]
"T õ + b̃o)
(16)
where ŷ is the predicted sentiment distribution of the sentence, C is the number of sentiment labels, W̃o and b̃o are parameters to be learned.
",2.3 Sentence Classifier,[0],[0]
"For model training, our goal is to minimize the cross entropy between the ground truth and predicted results for all sentences.",2.3 Sentence Classifier,[0],[0]
"Meanwhile, in order to avoid overfitting, we use dropout strategy to randomly omit parts of the parameters on each training case.",2.3 Sentence Classifier,[0],[0]
"Inspired by (Lin et al., 2017), we also design a penalization term to ensure the diversity of semantics from different sentiment-resourcespecific sentence representations, which reduces information redundancy from different sentiment resources attention.",2.3 Sentence Classifier,[0],[0]
"Specifically, the final loss function is presented as follows:
L(ŷ, y) =",2.3 Sentence Classifier,[0],[0]
− N∑ i=1,2.3 Sentence Classifier,[0],[0]
C∑,2.3 Sentence Classifier,[0],[0]
j=1,2.3 Sentence Classifier,[0],[0]
yji log(ŷ j i ) + λ,2.3 Sentence Classifier,[0],[0]
"( ∑ θ∈Θ θ2)
(17)
+ µ||ÕÕT",2.3 Sentence Classifier,[0],[0]
"− ψI||2F Õ =[o1; o2; o3] (18)
where yji is the target sentiment distribution of the sentence, ŷji is the prediction probabilities, θ denotes each parameter to be regularized, Θ is parameter set, λ is the coefficient for L2 regularization, µ is a hyper-parameter to balance the three terms, ψ is the weight parameter, I denotes the the identity matrix and ||.||F denotes the Frobenius norm of a matrix.",2.3 Sentence Classifier,[0],[0]
"Here, the first two terms of the loss function are cross-entropy function of the predicted and true distributions and L2 regularization respectively, and the final term is a penalization term to encourage the diversity of sentiment sources.",2.3 Sentence Classifier,[0],[0]
Movie Review (MR)2 and Stanford Sentiment Treebank (SST)3 are used to evaluate our model.,3.1 Datasets and Sentiment Resources,[0],[0]
"MR dataset has 5,331 positive samples and 5,331 negative samples.",3.1 Datasets and Sentiment Resources,[0],[0]
"We adopt the same data split as in (Qian et al., 2017).",3.1 Datasets and Sentiment Resources,[0],[0]
"SST consists of 8,545 training samples, 1,101 validation samples, 2210 test samples.",3.1 Datasets and Sentiment Resources,[0],[0]
"Each sample is marked as very negative, negative, neutral, positive, or very positive.",3.1 Datasets and Sentiment Resources,[0],[0]
"Sentiment lexicon combines the sentiment words from both (Qian et al., 2017) and (Hu and Liu, 2004), resulting in 10,899 sentiment words in total.",3.1 Datasets and Sentiment Resources,[0],[0]
We collect negation and intensity words manually as the number of these words is limited.,3.1 Datasets and Sentiment Resources,[0],[0]
"In order to comprehensively evaluate the performance of our model, we list several baselines for sentence-level sentiment classification.
RNTN:",3.2 Baselines,[0],[0]
"Recursive Tensor Neural Network (Socher et al., 2013) is used to model correlations between different dimensions of child nodes vectors.
",3.2 Baselines,[0],[0]
LSTM/Bi-LSTM:,3.2 Baselines,[0],[0]
"Cho et al. (2014) employs Long Short-Term Memory and the bidirectional variant to capture sequential information.
",3.2 Baselines,[0],[0]
Tree-LSTM:,3.2 Baselines,[0],[0]
"Memory cells was introduced by Tree-Structured Long Short-Term Memory (Tai et al., 2015) and gates into tree-structured neural network, which is beneficial to capture semantic relatedness by parsing syntax trees.
",3.2 Baselines,[0],[0]
"CNN: Convolutional Neural Networks (Kim, 2014) is applied to generate task-specific sentence representation.
",3.2 Baselines,[0],[0]
NCSL:,3.2 Baselines,[0],[0]
"Teng et al. (2016) designs a Neural Context-Sensitive Lexicon (NSCL) to obtain prior sentiment scores of words in the sentence.
",3.2 Baselines,[0],[0]
LR-Bi-LSTM:,3.2 Baselines,[0],[0]
"Qian et al. (2017) imposes linguistic roles into neural networks by applying linguistic regularization on intermediate outputs with KL divergence.
",3.2 Baselines,[0],[0]
"Self-attention: Lin et al. (2017) proposes a selfattention mechanism to learn structured sentence embedding.
2http://www.cs.cornell.edu/people/ pabo/movie-review-data/
3https://nlp.stanford.edu/sentiment/we train the model on both phrases and sentences but only test on sentences
ID-LSTM: (Tianyang et al., 2018) uses reinforcement learning to learn structured sentence representation for sentiment classification.",3.2 Baselines,[0],[0]
"In our experiments, the dimensions of characterlevel embedding and word embedding (GloVe) are both set to 300.",3.3 Implementation Details,[0],[0]
"Kernel sizes of multi-gram convolution for Char-CNN are set to 2, 3, respectively.",3.3 Implementation Details,[0],[0]
"All the weight matrices are initialized as random orthogonal matrices, and we set all the bias vectors as zero vectors.",3.3 Implementation Details,[0],[0]
"We optimize the proposed model with RMSprop algorithm, using mini-batch training.",3.3 Implementation Details,[0],[0]
The size of mini-batch is 60.,3.3 Implementation Details,[0],[0]
"The dropout rate is 0.5, and the coefficient λ of L2 normalization is set to 10−5. µ is set to 10−4. ψ is set to 0.9.",3.3 Implementation Details,[0],[0]
"When there are not sentiment resource words in the sentences, all the context words are treated as sentiment resource words to implement the multi-path self-attention strategy.",3.3 Implementation Details,[0],[0]
"In our experiments, to be consistent with the recent baseline methods, we adopt classification accuracy as evaluation metric.",3.4 Experiment Results,[0],[0]
We summarize the experimental results in Table 1.,3.4 Experiment Results,[0],[0]
Our model has robust superiority over competitors and sets stateof-the-art on MR and SST datasets.,3.4 Experiment Results,[0],[0]
"First, our model brings a substantial improvement over the methods that do not leverage sentiment linguistic knowledge (e.g., RNTN, LSTM, BiLSTM, CNN and ID-LSTM) on both datasets.",3.4 Experiment Results,[0],[0]
This verifies the effectiveness of leveraging sentiment linguistic resource with the deep learning algorithms.,3.4 Experiment Results,[0],[0]
"Second, our model also consistently outperforms LR-Bi-LSTM which integrates linguistic roles of sentiment, negation and intensity words into neural networks via the linguistic regularization.",3.4 Experiment Results,[0],[0]
"For example, our model achieves 2.4% improvements over the MR dataset and 0.8% improvements over the SST dataset compared to LR-Bi-LSTM.",3.4 Experiment Results,[0],[0]
"This is because that MEAN designs attention mechanisms to leverage sentiment resources efficiently, which utilizes the interactive information between context words and sentiment resource words.
",3.4 Experiment Results,[0],[0]
"In order to analyze the effectiveness of each component of MEAN, we also report the ablation test in terms of discarding character-level embedding (denoted as MEAN w/o CharCNN) and sentiment words/negation words/intensity words (denoted as MEAN w/o sentiment words/negation words/intensity words).",3.4 Experiment Results,[0],[0]
"All the tested factors con-
tribute greatly to the improvement of the MEAN.",3.4 Experiment Results,[0],[0]
"In particular, the accuracy decreases sharply when discarding the sentiment words.",3.4 Experiment Results,[0],[0]
This is within our expectation since sentiment words are vital when classifying the polarity of the sentences.,3.4 Experiment Results,[0],[0]
"In this paper, we propose a novel Multi-sentimentresource Enhanced Attention Network (MEAN) to enhance the performance of sentence-level sentiment analysis, which integrates the sentiment linguistic knowledge into the deep neural network.",4 Conclusion,[0],[0]
This work was supported in part by the Research Fund for the development of strategic emerging industries by ShenZhen city (No.JCYJ20160301151844537 and No. JCYJ20160331104524983).,Acknowledgements,[0],[0]
Deep learning approaches for sentiment classification do not fully exploit sentiment linguistic knowledge.,abstractText,[0],[0]
"In this paper, we propose a Multi-sentiment-resource Enhanced Attention Network (MEAN) to alleviate the problem by integrating three kinds of sentiment linguistic knowledge (e.g., sentiment lexicon, negation words, intensity words) into the deep neural network via attention mechanisms.",abstractText,[0],[0]
"By using various types of sentiment resources, MEAN utilizes sentiment-relevant information from different representation subspaces, which makes it more effective to capture the overall semantics of the sentiment, negation and intensity words for sentiment prediction.",abstractText,[0],[0]
The experimental results demonstrate that MEAN has robust superiority over strong competitors.,abstractText,[0],[0]
A Multi-sentiment-resource Enhanced Attention Network for Sentiment Classification,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 884–895 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1082",text,[0],[0]
"What does it mean to be welcoming or standoffish, light-hearted or cynical?",1 Introduction,[0],[0]
"Such interactional styles are performed primarily with language, yet little is known about how linguistic resources are arrayed to create these social impressions.",1 Introduction,[0],[0]
"The sociolinguistic concept of interpersonal stancetaking attempts to answer this question, by providing a conceptual framework that accounts for a range of interpersonal phenomena, subsuming formality, politeness, and subjectivity (Du Bois, 2007).1 This
1Stancetaking is distinct from the notion of stance which corresponds to a position in a debate (Walker et al., 2012).",1 Introduction,[0],[0]
"Similarly, Freeman et al. (2014) correlate phonetic features with the strength of such argumentative stances.
framework has been applied almost exclusively through qualitative methods, using close readings of individual texts or dialogs to uncover how language is used to position individuals with respect to their interlocutors and readers.
",1 Introduction,[0],[0]
We attempt the first large-scale operationalization of stancetaking through computational methods.,1 Introduction,[0],[0]
"Du Bois (2007) formalizes stancetaking as a multi-dimensional construct, reflecting the relationship of discourse participants to (a) the audience or interlocutor; (b) the topic of discourse; (c) the talk or text itself.",1 Introduction,[0],[0]
"However, the multidimensional nature of stancetaking poses problems for traditional computational approaches, in which labeled data is obtained by relying on annotator intuitions about scalar concepts such politeness (Danescu-Niculescu-Mizil et al., 2013) and formality (Pavlick and Tetreault, 2016).
",1 Introduction,[0],[0]
"Instead, our approach is based on a theoretically-guided application of unsupervised learning, in the form of factor analysis, applied to lexical features.",1 Introduction,[0],[0]
"Stancetaking is characterized in large part by an array of linguistic features ranging from discourse markers such as actually to backchannels such as yep (Kiesling, 2009).",1 Introduction,[0],[0]
"We therefore first compile a lexicon of stance markers, combining prior lexicons from Biber and Finegan (1989) and the Switchboard Dialogue Act Corpus (Jurafsky et al., 1998).",1 Introduction,[0],[0]
We then extend this lexicon to the social media domain using word embeddings.,1 Introduction,[0],[0]
"Finally, we apply multi-dimensional analysis of co-occurrence patterns to identify a small set of stance dimensions.
",1 Introduction,[0],[0]
"To measure the internal coherence (construct validity) of the stance dimensions, we use a word
884
intrusion task (Chang et al., 2009) and a set of preregistered hypotheses.",1 Introduction,[0],[0]
"To measure the utility of the stance dimensions, we perform a series of extrinsic evaluations.",1 Introduction,[0],[0]
A predictive evaluation shows that the membership of online communities is determined in part by the interactional stances that predominate in those communities.,1 Introduction,[0],[0]
"Furthermore, the induced stance dimensions are shown to align with annotations of politeness and formality.
",1 Introduction,[0],[0]
"Contributions We operationalize the sociolinguistic concept of stancetaking as a multidimensional framework, making it possible to measure at scale.",1 Introduction,[0],[0]
"Specifically,
• we contribute a lexicon of stance markers based on prior work and adapted to the genre of online interpersonal discourse;
• we group stance markers into latent dimensions; • we show that these stance dimensions are inter-
nally coherent;
• we demonstrate that the stance dimensions predict and correlate with social phenomena.2",1 Introduction,[0],[0]
"From a theoretical perspective, we build on prior work on interactional meaning in language.",2 Related Work,[0],[0]
"Methodologically, our paper relates to prior work on lexicon-based analysis and contrastive studies of social media communities.",2 Related Work,[0],[0]
"In computational sociolinguistics (Nguyen et al., 2016), language variation has been studied primarily in connection with macro-scale social variables, such as age (Argamon et al., 2007; Nguyen et al., 2013), gender (Burger et al., 2011; Bamman et al., 2014), race (Eisenstein et al., 2011; Blodgett et al., 2016), and geography (Eisenstein et al., 2010).",2.1 Linguistic Variation and Social Meaning,[0],[0]
"This parallels what Eckert (2012) has called the “first wave” of language variation studies in sociolinguistics, which also focused on macro-scale variables.
",2.1 Linguistic Variation and Social Meaning,[0],[0]
"More recently, sociolinguists have dedicated increased attention to situational and stylistic variation, and the interactional meaning that such variation can convey (Eckert and Rickford, 2001).",2.1 Linguistic Variation and Social Meaning,[0],[0]
"This linguistic research can be aligned with computational efforts to quantify phenomena such
2Lexicons and stance dimensions are available at https://github.com/umashanthi-research/ multidimensional-stance-lexicon
as subjectivity (Riloff and Wiebe, 2003), sentiment (Wiebe et al., 2005), politeness (DanescuNiculescu-Mizil et al., 2013), formality (Pavlick and Tetreault, 2016), and power dynamics (Prabhakaran et al., 2012).",2.1 Linguistic Variation and Social Meaning,[0],[0]
"While linguistic research on interactional meaning has focused largely on qualitative methodologies such as discourse analysis (e.g., Bucholtz and Hall, 2005), these computational efforts have made use of crowdsourced annotations to build large datasets of, for example, polite and impolite text.",2.1 Linguistic Variation and Social Meaning,[0],[0]
"These annotation efforts draw on the annotators’ intuitions about the meaning of these sociolinguistic constructs.
",2.1 Linguistic Variation and Social Meaning,[0],[0]
"Interpersonal stancetaking represents an attempt to unify concepts such as sentiment, politeness, formality, and subjectivity under a single theoretical framework (Jaffe, 2009; Kiesling, 2009).",2.1 Linguistic Variation and Social Meaning,[0],[0]
"The key idea, as articulated by Du Bois (2007), is that stancetaking captures the speaker’s relationship to (a) the topic of discussion, (b) the interlocutor or audience, and (c) the talk (or writing) itself.",2.1 Linguistic Variation and Social Meaning,[0],[0]
Various configurations of these three legs of the “stance triangle” can account for a range of phenomena.,2.1 Linguistic Variation and Social Meaning,[0],[0]
"For example, epistemic stance relates to the speaker’s certainty about what is being expressed, while affective stance indicates the speaker’s emotional position with respect to the content (Ochs, 1993).
",2.1 Linguistic Variation and Social Meaning,[0],[0]
"The framework of stancetaking has been widely adopted in linguistics, particularly in the discourse analytic tradition, which involves close reading of individual texts or conversations (Kärkkäinen, 2006; Keisanen, 2007; Precht, 2003; White, 2003).",2.1 Linguistic Variation and Social Meaning,[0],[0]
"But despite its strong theoretical foundation, we are aware of no prior efforts to operationalize stancetaking at scale.",2.1 Linguistic Variation and Social Meaning,[0],[0]
Since annotators may not have strong intuitions about stance — in the way that they do about formality and politeness — we cannot rely on the annotation methodologies employed in prior work.,2.1 Linguistic Variation and Social Meaning,[0],[0]
"We take a different approach, performing a multidimensional analysis of the distribution of likely stance markers.",2.1 Linguistic Variation and Social Meaning,[0],[0]
Our operationalization of stancetaking is based on the induction of lexicons of stance markers.,2.2 Lexicon-based Analysis,[0],[0]
"The lexicon-based methodology is related to earlier work from social psychology, such as the General Inquirer (Stone, 1966) and LIWC (Tausczik and Pennebaker, 2010).",2.2 Lexicon-based Analysis,[0],[0]
"In LIWC, the basic categories were identified first, based on psychological
constructs (e.g., positive emotion, cognitive processes, drive to power) and syntactic groupings of words and phrases (e.g., pronouns, prepositions, quantifiers).",2.2 Lexicon-based Analysis,[0],[0]
"The lexicon designers then manually contructed lexicons for each category, augmenting their intuitions by using distributional statistics to suggest words that may have been missed (Pennebaker et al., 2015).",2.2 Lexicon-based Analysis,[0],[0]
"In contrast, we follow the approach of Biber (1991), using multidimensional analysis to identify latent groupings of markers based on co-occurrence statistics.",2.2 Lexicon-based Analysis,[0],[0]
We then use crowdsourcing and extrinsic comparisons to validate the coherence of these dimensions.,2.2 Lexicon-based Analysis,[0],[0]
"Social media platforms such as Reddit, Stack Exchange, and Wikia can be considered multicommunity environments, in that they host multiple subcommunities with distinct social and linguistic properties.",2.3 Multicommunity Studies,[0],[0]
"Such subcommunities can be contrasted in terms of topics (Adamic et al., 2008; Hessel et al., 2014) and social networks (Backstrom et al., 2006).",2.3 Multicommunity Studies,[0],[0]
"Our work focuses on Reddit, emphasizing community-wide differences in norms for interpersonal interaction.",2.3 Multicommunity Studies,[0],[0]
"In the same vein, Tan and Lee (2015) attempt to characterize stylistic differences across subreddits by focusing on very common words and parts-of-speech; Tran and Ostendorf (2016) use language models and topic models to measure similarity across threads within a subreddit.",2.3 Multicommunity Studies,[0],[0]
One distinction of our approach is that the use of multidimensional analysis gives us interpretable dimensions of variation.,2.3 Multicommunity Studies,[0],[0]
This makes it possible to identify the specific interpersonal features that vary across communities.,2.3 Multicommunity Studies,[0],[0]
"Reddit, one of the internet’s largest social media platforms, is a collection of subreddits organized around various topics of interest.",3 Data,[0],[0]
"As of January 2017, there were more than one million subreddits and nearly 250 million users, discussing topics ranging from politics (r/politics) to horror stories (r/nosleep).3 Although Reddit was originally designed for sharing hyperlinks, it also provides the ability to post original textual content, submit comments, and vote on content quality (Gilbert, 2013).",3 Data,[0],[0]
"Reddit’s conversation-like threads are therefore well suited for the study of interpersonal social and linguistic phenomena.
",3 Data,[0],[0]
"3http://redditmetrics.com/
For example, the following are two comments from the subreddit r/malefashionadvice, posted in response to a picture posted by a user asking for fashion advise.
U1: “I think the beard looks pretty good.",3 Data,[0],[0]
Definitely not the goatee.,3 Data,[0],[0]
"Clean shaven is always the safe option.”
U2: “Definitely the beard.",3 Data,[0],[0]
"But keep it trimmed.”
The phrases in bold face are markers of stance, indicating a evaluative stance.",3 Data,[0],[0]
The following example is a part of a thread in the subreddit r/photoshopbattles where users discuss an edited image posted by the original poster OP.,3 Data,[0],[0]
"The phrases in bold face are markers of stance, indicating an involved and interactional stance.
",3 Data,[0],[0]
U3: “Ha ha awesome!”,3 Data,[0],[0]
"U4: ‘‘are those..... furries?”
",3 Data,[0],[0]
"OP: “yes, sir.",3 Data,[0],[0]
They are!”,3 Data,[0],[0]
U4: “Oh cool.,3 Data,[0],[0]
"That makes sense!”
",3 Data,[0],[0]
"We used an archive of 530 million comments posted on Reddit in 2014, retrieved from the public archive of Reddit comments.4",3 Data,[0],[0]
"This dataset consists of each post’s textual content, along with metadata that identifies the subreddit, thread, author, and post creation time.",3 Data,[0],[0]
More statistics about the full dataset are shown in Table 1.,3 Data,[0],[0]
"Interpersonal stancetaking can be characterized in part by an array of linguistic features such as hedges (e.g., might, kind of ), discourse markers (e.g., actually, I mean), and backchannels (e.g., yep, um).",4 Stance Lexicon,[0],[0]
"Our analysis focuses on these markers, which we collect into a lexicon.",4 Stance Lexicon,[0],[0]
"We began with a seed lexicon of stance markers from Biber and Finegan (1989), who compiled an
4https://archive.org/details/2015_",4.1 Seed lexicon,[0],[0]
"reddit_comments_corpus
extensive list by surveying dictionaries, previous studies on stance, and texts in several genres of English.",4.1 Seed lexicon,[0],[0]
"This list includes certainty adverbs (e.g., actually, of course, in fact), affect markers (e.g., amazing, thankful, sadly), and hedges (e.g., kind of, maybe, something like) among other adverbial, adjectival, verbal, and modal markers of stance.",4.1 Seed lexicon,[0],[0]
"In total, this list consists of 448 stance markers.
",4.1 Seed lexicon,[0],[0]
The Biber and Finegan (1989) lexicon is primarily based on written genres from the pre-social media era.,4.1 Seed lexicon,[0],[0]
"Our dataset — like much of the recent work in this domain — consists of online discussions, which differ significantly from printed texts (Eisenstein, 2013).",4.1 Seed lexicon,[0],[0]
"One difference is that online discussions contain a number of dialog act markers that are characteristic of spoken language, such as oh yeah, nah,",4.1 Seed lexicon,[0],[0]
wow.,4.1 Seed lexicon,[0],[0]
"We accounted for this by adding 74 dialog act markers from the Switchboard Dialog Act Corpus (Jurafsky et al., 1998).",4.1 Seed lexicon,[0],[0]
"The final seed lexicon consists of 517 unique markers, from these two sources.",4.1 Seed lexicon,[0],[0]
"Note that the seed lexicon also includes markers that contain multiple tokens (e.g. kind of, I know).",4.1 Seed lexicon,[0],[0]
"Online discussions differ not only from written texts, but also from spoken discussions, due to their use of non-standard vocabulary and spellings.",4.2 Lexicon expansion,[0],[0]
"To measure stance accurately, these genre differences must be accounted for.",4.2 Lexicon expansion,[0],[0]
We therefore expanded the seed lexicon using automated techniques based on distributional statistics.,4.2 Lexicon expansion,[0],[0]
"This is similar to prior work on the expansion of sentiment lexicons (Hatzivassiloglou and McKeown, 1997; Hamilton et al., 2016).
",4.2 Lexicon expansion,[0],[0]
Our lexicon expansion approach used word embeddings to find words that are distributionally similar to those in the seed set.,4.2 Lexicon expansion,[0],[0]
"We trained word embeddings on a corpus of 25 million Reddit comments and a vocabulary of 100K most frequent words on Reddit using the structured skip-gram models of both WORD2VEC (Mikolov et al., 2013) and WANG2VEC (Ling et al., 2015) with default parameters.",4.2 Lexicon expansion,[0],[0]
The WANG2VEC method augments WORD2VEC by accounting for word order information.,4.2 Lexicon expansion,[0],[0]
"We found the similarity judgments obtained from WANG2VEC to be qualitatively more meaningful, so we used these embeddings to construct the expanded lexicon.5
5We used the following default parameters: 100 dimensions, a window size of five, a negative sampling size of ten, five-epoch iterations, and a sub-sampling rate of 10−4.
",4.2 Lexicon expansion,[0],[0]
"To perform lexicon expansion, we constructed a dictionary of candidate terms, consisting of all unigrams that occur with a frequency rate of at least 10−7 in the Reddit comment corpus.",4.2 Lexicon expansion,[0],[0]
"Then, for each single-token marker in the seed lexicon, we identified all terms from the candidate set whose embedding has cosine similarity of at least 0.75 with respect to the seed marker.6 Table 2 shows examples of seed markers and related terms we extracted from word embeddings.",4.2 Lexicon expansion,[0],[0]
"Through this procedure, we identified 228 additional markers based on similarity to items in the seed list from Biber and Finegan (1989), and 112 additional markers based on the seed list of dialog acts.",4.2 Lexicon expansion,[0],[0]
"In total, our stance lexicon contains 812 unique markers.",4.2 Lexicon expansion,[0],[0]
"To summarize the main axes of variation across the lexicon of stance markers, we apply a multidimensional analysis (Biber, 1992) to the distributional statistics of stance markers across subreddit communities.",5 Linguistic Dimensions of Stancetaking,[0],[0]
"Each dimension of variation can then be viewed as a spectrum, characterized by the stance markers and subreddits that are associated with the positive and negative extremes.",5 Linguistic Dimensions of Stancetaking,[0],[0]
"Multidimensional analysis is based on singular value decomposition, which has been applied successfully to a wide range of problems in natural language processing and information retrieval (e.g., Landauer et al., 1998).",5 Linguistic Dimensions of Stancetaking,[0],[0]
"While Bayesian topic models are an appealing alternative, singular value decomposition is fast and deterministic, with a minimal number of tuning parameters.
",5 Linguistic Dimensions of Stancetaking,[0],[0]
"6We tried different thresholds on the similarity value and the corpus frequency, and the reported values were chosen based on the quality of the resulting related terms.",5 Linguistic Dimensions of Stancetaking,[0],[0]
This was done prior to any of the validations or extrinsic analyses described later in the paper.,5 Linguistic Dimensions of Stancetaking,[0],[0]
Our analysis is based on the co-occurrence of stance markers and subreddits.,5.1 Extracting Stance Dimensions,[0],[0]
"This is motivated by our interest in comparisons of the interactional styles of online communities within Reddit, and by the premise that these distributional differences reflect socially meaningful communicative norms.",5.1 Extracting Stance Dimensions,[0],[0]
"A pilot study applied the same technique to the cooccurrence of stance markers and individual authors, and the resulting dimensions appeared to be less stylistically coherent.
",5.1 Extracting Stance Dimensions,[0],[0]
"Singular value decomposition is often used in combination with a transformation of the cooccurrence counts by pointwise mutual information (Bullinaria and Levy, 2007).",5.1 Extracting Stance Dimensions,[0],[0]
This transformation ensures that each cell in the matrix indicates how much more likely a stance marker is to cooccur with a given subreddit than would happen by chance under an independence assumption.,5.1 Extracting Stance Dimensions,[0],[0]
"Because negative PMI values tend to be unreliable, we use positive PMI (PPMI), which involves replacing all negative PMI values with zeros (Niwa and Nitta, 1994).",5.1 Extracting Stance Dimensions,[0],[0]
"Therefore, we obtain stance dimensions by applying singular value decomposition to the matrix constructed as follows:
Xm,s =
( log Pr(marker = m, subreddit = s)
Pr(marker = m) Pr(subreddit = s)
)
+
.
",5.1 Extracting Stance Dimensions,[0],[0]
Truncated singular value decomposition performs the approximate factorization X,5.1 Extracting Stance Dimensions,[0],[0]
"≈ UΣV >, where each row of the matrix U is a k-dimensional description of each stance marker, and each row of V is a k-dimensional description of each subreddit.",5.1 Extracting Stance Dimensions,[0],[0]
"We included the 7,589 subreddits that received at least 1,000 comments in 2014.",5.1 Extracting Stance Dimensions,[0],[0]
"From the SVD analysis, we extracted the six principal latent dimensions that explain the most variation in our dataset.7",5.2 Results: Stance Dimensions,[0],[0]
The decision to include only the first six dimensions was based on the strength of the singular values corresponding to the dimensions.,5.2 Results: Stance Dimensions,[0],[0]
Table 3 shows the top five stance markers for each extreme of the six dimensions.,5.2 Results: Stance Dimensions,[0],[0]
"The stance dimensions convey a range of concepts, such as involved versus informational language, narrative
7Similar to factor analysis, the top few dimensions of SVD explain the most variation, and tend to be most interpretable.",5.2 Results: Stance Dimensions,[0],[0]
"A scree plot (Cattell, 1966) showed that the amount of variation explained dropped after the top six dimensions, and qualitative interpretation showed that the remaining dimension were less interpretable.
versus dialogue-oriented writing, standard versus non-standard variation, and positive versus negative affect.",5.2 Results: Stance Dimensions,[0],[0]
Figure 1 shows the distribution of subreddits along two of these dimensions.,5.2 Results: Stance Dimensions,[0],[0]
Evaluating model output against gold-standard annotations is appropriate when there is some notion of a correct answer.,6 Construct Validity,[0],[0]
"As stancetaking is a multidimensional concept, we have taken an unsupervised approach.",6 Construct Validity,[0],[0]
"Therefore, we use evaluation techniques based on the notion of validity, which is the extent to which the operationalization of a construct truly captures the intended quantity or concept.",6 Construct Validity,[0],[0]
"Validation techniques for unsupervised content analysis are widely found in the social science literature (Weber, 1990; Quinn et al., 2010) and have also been recently used in the NLP and machine learning communities (e.g., Chang et al., 2009; Murphy et al., 2012; Sim et al., 2013).
",6 Construct Validity,[0],[0]
We used several methods to validate the stance dimensions extracted from the corpus of Reddit comments.,6 Construct Validity,[0],[0]
"This section describes intrinsic evaluations, which test whether the extracted stance dimensions are linguistically coherent and mean-
ingful, thereby testing the construct or content validity of the proposed stance dimensions (Quinn et al., 2010).",6 Construct Validity,[0],[0]
Extrinsic evaluations are presented in section 7.,6 Construct Validity,[0],[0]
A word intrusion task is used to measure the coherence and interpretability of a group of words.,6.1 Word Intrusion Task,[0],[0]
"Human raters are presented with a list of terms, all but one of which are selected from a target concept; their task is to identify the intruder.",6.1 Word Intrusion Task,[0],[0]
"If the target concept is internally coherent, human raters should be able to perform this task accurately; if not, their selections should be random.",6.1 Word Intrusion Task,[0],[0]
"Word intrusion tasks have previously been used to validate the interpretability of topic models (Chang et al., 2009) and vector space models (Murphy et al., 2012).
",6.1 Word Intrusion Task,[0],[0]
"We deployed a word intrusion task on Amazon Mechanical Turk (AMT), in which we presented the top four stance markers from one end of a dimension, along with an intruder marker selected from the top four markers of the opposite end of that dimension.",6.1 Word Intrusion Task,[0],[0]
"In this way, we created four word intrusion tasks for each end of each dimension.",6.1 Word Intrusion Task,[0],[0]
The main reason for including only the top four words in each dimension is the expense of conducting crowd-sourced evaluations.,6.1 Word Intrusion Task,[0],[0]
"In the most relevant prior work, Chang et al. (2009) used the top five words from each topic in their evaluation of topic models.
",6.1 Word Intrusion Task,[0],[0]
"Worker selection We required that the AMT workers (“turkers”) have completed a minimum of 1,000 HITs and have at least 95% approval rate
Furthermore, because our task is based on analysis of English language texts, we required the turkers to be native speakers of English living in one of the majority English speaking countries.",6.1 Word Intrusion Task,[0],[0]
"As a further requirement, we required the turkers to obtain a qualification which involves an English comprehension test similar to the questions in standardized English language tests.",6.1 Word Intrusion Task,[0],[0]
"These requirements are based on best practices identified by CallisonBurch and Dredze (2010).
",6.1 Word Intrusion Task,[0],[0]
"Task specification Each AMT human intelligence task (HIT) consists of twelve word intrusion tasks, one for each end of the six dimensions.",6.1 Word Intrusion Task,[0],[0]
"We provided minimal instructions regarding the task, and did not provide any examples, to avoid introducing bias.8 As a further quality control, each HIT included three questions which ask the turkers to pick the best synonym for a given word from a list of five answers, where one answer was clearly correct; Turkers who gave incorrect answers were to be excluded, but this situation did not arise in practice.",6.1 Word Intrusion Task,[0],[0]
"Altogether each HIT consists of 15 questions, and was paid US$1.50.",6.1 Word Intrusion Task,[0],[0]
"Five different turkers performed each HIT.
Results We measured the interrater reliability using Krippendorf’s α (Krippendorff, 2007) and the model precision metric of Chang et al. (2009).",6.1 Word Intrusion Task,[0],[0]
Results on both metrics were encouraging.,6.1 Word Intrusion Task,[0],[0]
"We obtained a value of α = 0.73, on a scale where
8The prompt for the word intrusions task was: “Select the intruder word/phrase: you will be given a list of five English words/phrases and asked to pick the word/phrase that is least similar to the other four words/phrases when used in online discussion forums.”
α = 0 indicates chance agreement and α = 1 indicates perfect agreement.",6.1 Word Intrusion Task,[0],[0]
The model precision was 0.82; chance precision is 0.20.,6.1 Word Intrusion Task,[0],[0]
"To offer a sense of typical values for this metric, Chang et al. (2009) report model precisions in the range 0.7–0.83 in their analysis of topic models.",6.1 Word Intrusion Task,[0],[0]
"Overall, these results indicate that the multi-dimensional analysis has succeeded at identifying dimensions that reflect natural groupings of stance markers.",6.1 Word Intrusion Task,[0],[0]
Content validity was also assessed using a set of pre-registered hypotheses.,6.2 Pre-registered Hypotheses,[0],[0]
The practice of preregistering hypotheses before an analysis and testing the correctness is widely used in the social sciences; it was adopted by Sim et al. (2013) to evaluate the induction of political ideological models from text.,6.2 Pre-registered Hypotheses,[0],[0]
"Before performing the mutidimensional analysis, we identified two groups of hypotheses that are expected to hold with respect to the latent stancetaking dimensions using our prior linguistic knowledge:
",6.2 Pre-registered Hypotheses,[0],[0]
• Hypothesis I: Stance markers that are synonyms should not appear on the opposite ends of a stance dimension.,6.2 Pre-registered Hypotheses,[0],[0]
• Hypothesis II:,6.2 Pre-registered Hypotheses,[0],[0]
"If at least one stance marker
from a predefined stance feature group (defined below) appears on one end of a stance dimension, then other markers from the same feature group will tend not to appear at the opposite end of the same dimension.",6.2 Pre-registered Hypotheses,[0],[0]
"For each marker in our stance lexicon, we extracted synonyms from Wordnet, focusing on markers that appear in only one Wordnet synset, and not including pairs in which one term was an inflection of the other.9 Our final list contains 73 synonym pairs (e.g., eventually/finally, grateful/thankful, yea/yeah).",6.2.1 Synonym Pairs,[0],[0]
"Of these pairs, there were 59 cases in which both terms appeared in either the top or bottom 200 positions of a stance dimension.",6.2.1 Synonym Pairs,[0],[0]
"In 51 of these cases (86%), the two terms appeared on the same side of the dimension.",6.2.1 Synonym Pairs,[0],[0]
"The chance rate would be 50%, so this supports Hypothesis I and
9It is possible that inflections are semantically similar, because by definition they are changes in the form of a word to mark distinctions such as tense, person, or number.",6.2.1 Synonym Pairs,[0],[0]
"However, different inflections of a single word form might be used to mark different stances (e.g., some stances might be associated with the past while others might be associated with the present or future).
",6.2.1 Synonym Pairs,[0],[0]
further validates the stance dimensions.,6.2.1 Synonym Pairs,[0],[0]
More details of the results are shown in Table 4.,6.2.1 Synonym Pairs,[0],[0]
"Note that synonym pairs may differ in aspects such as formality (e.g., said/informed, want/desire), which is one of the main dimensions of stancetaking.",6.2.1 Synonym Pairs,[0],[0]
"Therefore, perfect support for Hypothesis I is not expected.",6.2.1 Synonym Pairs,[0],[0]
"Biber and Finegan (1989) group stance markers into twelve “feature groups”, such as certainty adverbs, doubt adverbs, affect expressions, and hedges.",6.2.2 Stance Feature Groups,[0],[0]
"Ideally, the stance dimensions should preserve these groupings.",6.2.2 Stance Feature Groups,[0],[0]
"To test this, for each of the seven feature groups with at least ten stance markers in the lexicon, we counted the number of terms appearing among the top 200 positions in both ends (high/low) of each dimension.",6.2.2 Stance Feature Groups,[0],[0]
"Under the null hypothesis, the stance dimensions are random with respect to the feature groups, so we would expect roughly an equal number of markers on both ends.",6.2.2 Stance Feature Groups,[0],[0]
"As shown in Table 5, for five of the seven feature groups, it is possible to reject the null hypothesis at p < .007, which is the significance threshold at α = 0.05, after correcting for multiple comparisons using the Bonferroni correction.",6.2.2 Stance Feature Groups,[0],[0]
This indicates that the stance dimensions are aligned with predefined stance feature groups.,6.2.2 Stance Feature Groups,[0],[0]
The evaluations in the previous section test internal validity; we now describe evaluations testing whether the stance dimensions are relevant to external social and interactional phenomena.,7 Extrinsic Evaluations,[0],[0]
"Online communities can be considered as communities of practice (Eckert and McConnell-Ginet, 1992), where members come together to engage in shared linguistic practices.",7.1 Predicting Cross-posting,[0],[0]
"These practices
evolve simultaneously with membership, coalescing into shared norms.",7.1 Predicting Cross-posting,[0],[0]
"The memberships of multiple subreddits on the same topic (e.g., r/science and r/askscience) often do not overlap considerably.",7.1 Predicting Cross-posting,[0],[0]
"Therefore we hypothesize that users of Reddit have preferred interactional styles, and that participation in subreddit communities is governed not only by topic interest, but also by these interactional preferences.",7.1 Predicting Cross-posting,[0],[0]
"The proposed stancetaking dimensions provide a simple measure of interactional style, allowing us to test whether it is predictive of community membership decisions.
",7.1 Predicting Cross-posting,[0],[0]
"Classification task We design a classification task, in which the goal is to determine whether a pair of subreddits is high-crossover or lowcrossover.",7.1 Predicting Cross-posting,[0],[0]
"In high-crossover subreddit pairs, individuals are especially likely to participate in both.",7.1 Predicting Cross-posting,[0],[0]
"For the purpose of this evaluation, individuals are considered to participate in a subreddit if they contribute posts or comments.",7.1 Predicting Cross-posting,[0],[0]
We compute the pointwise mutual information (PMI) with respect to cross-participation among the 100 most popular subreddits.,7.1 Predicting Cross-posting,[0],[0]
"For each subreddit s, we identify the five highest and lowest PMI pairs 〈s, t〉, and add these to the high-crossover and low-crossover sets, respectively.",7.1 Predicting Cross-posting,[0],[0]
Example pairs are shown in Table 6.,7.1 Predicting Cross-posting,[0],[0]
"After eliminating redundant pairs, we identify 437 unique high-crossover pairs, and 465 unique lowcrossover pairs.",7.1 Predicting Cross-posting,[0],[0]
"All evaluations are based on multiple random training/test splits over this dataset.
",7.1 Predicting Cross-posting,[0],[0]
Classification approaches A simple classification approach is to predict that subreddits with similar text will have high crossover.,7.1 Predicting Cross-posting,[0],[0]
"We measure similarity using TF-IDF weighted cosine similarity, using two possible lexicons: the 8,000 most frequent words on reddit (BOW), and the stance lexicon (STANCE MARKERS).",7.1 Predicting Cross-posting,[0],[0]
"The similarity threshold between high-crossover and low-
crossover pairs was estimated on the training data.",7.1 Predicting Cross-posting,[0],[0]
"We also tested the relevance of multi-dimensional analysis, by applying SVD to both lexicons.",7.1 Predicting Cross-posting,[0],[0]
"For each pair of subreddits, we computed a feature set of the absolute difference across the top six latent dimensions, and applied a logistic regression classifier.",7.1 Predicting Cross-posting,[0],[0]
"Regularization was tuned by internal crossvalidation.
",7.1 Predicting Cross-posting,[0],[0]
Results Table 7 shows average accuracies for these models.,7.1 Predicting Cross-posting,[0],[0]
"The stance-based SVD features are considerably more accurate than the BOWbased SVD features, indicating that interactional style does indeed predict cross-posting behavior.10 Both are considerably more accurate than the bagof-words models based on cosine similarity.",7.1 Predicting Cross-posting,[0],[0]
The utility of the induced stance dimensions depends on their correlation with social phenomena of interest.,7.2 Politeness and Formality,[0],[0]
Prior work has used crowdsourcing to annotate texts for politeness and formality.,7.2 Politeness and Formality,[0],[0]
"We now evaluate the stancetaking properties of these annotated texts.
",7.2 Politeness and Formality,[0],[0]
"Data We used the politeness corpus of Wikipedia edit requests from Danescu-NiculescuMizil et al. (2013), which includes the textual content of the edit requests, along with scalar annotations of politeness.",7.2 Politeness and Formality,[0],[0]
"Following the original
10We use BOW+SVD as the most comparable contentbased alternative to our stancetaking dimensions.",7.2 Politeness and Formality,[0],[0]
"While there may be more accurate discriminative approaches, our goal is a direct comparison of stance and content-based features, not an exhaustive comparison of classification approaches.
",7.2 Politeness and Formality,[0],[0]
"authors, we compare the text for the messages ranked in the first and fourth quartiles of politeness scores.",7.2 Politeness and Formality,[0],[0]
"For formality, we used the corpus from Pavlick and Tetreault (2016), focusing on the blogs domain, which is most similar to our domain of Reddit.",7.2 Politeness and Formality,[0],[0]
Each sentence in this corpus was annotated for formality levels from −3 to +3.,7.2 Politeness and Formality,[0],[0]
"We considered only the sentences with mean formality score greater than +1 (more formal) and less than −1 (less formal).
Stance dimensions For each document in the above datasets, we compute the stance properties, as follows: for each dimension, we compute the total frequency of the hundred most positive terms and the hundred most negative terms, and then take the difference.",7.2 Politeness and Formality,[0],[0]
Instances containing no terms from either list are excluded.,7.2 Politeness and Formality,[0],[0]
"We focus on stance dimensions two and five (summarized in Table 3), because they appeared to be most relevant to politeness and formality.",7.2 Politeness and Formality,[0],[0]
Dimension two contrasts informational and argumentative language against emotional and non-standard language.,7.2 Politeness and Formality,[0],[0]
"Dimension five contrasts positive and formal language against non-standard and somewhat negative language.
",7.2 Politeness and Formality,[0],[0]
Results A kernel density plot of the resulting differences is shown in Figure 2.,7.2 Politeness and Formality,[0],[0]
"The effect sizes of the resulting differences are quantified using Cohen’s d statistic (Cohen, 1988).",7.2 Politeness and Formality,[0],[0]
"Effect sizes for all differences are between 0.3 and 0.4, indicating small-to-medium effects — except for the evaluation of formality on dimension five, where the effect size is close to zero.",7.2 Politeness and Formality,[0],[0]
"The relatively modest effect sizes are unsurprising, given the short length of the texts.",7.2 Politeness and Formality,[0],[0]
"However, these differences lend insight to the relationship between formality and politeness, which may seem to be closely related concepts.",7.2 Politeness and Formality,[0],[0]
"On dimension two, it is possible to be polite while using non-standard language such as hehe and awww, so long as the sentiment expressed is positive; however, these markers are not consistent with formality.",7.2 Politeness and Formality,[0],[0]
"On dimension five, we see that positive sentiment terms such as lovely and stunning are consistent with politeness, but not with formality.",7.2 Politeness and Formality,[0],[0]
"Indeed, the distribution of dimension five indicates that both ends of dimension five are consistent only with informal texts.
",7.2 Politeness and Formality,[0],[0]
"Overall, these results indicate that interactional phenomena such as politeness and formality are reflected in our stance dimensions, which are induced in an unsupervised manner.",7.2 Politeness and Formality,[0],[0]
"Future work
may consider the utility of these stance dimensions to predict these social phenomena, particularly in cross-domain settings where lexical classifiers may overfit.",7.2 Politeness and Formality,[0],[0]
Stancetaking provides a general perspective on the various linguistic phenomena that structure social interactions.,8 Conclusion,[0],[0]
"We have identified a set of several hundred stance markers, building on previouslyidentified lexicons by using word embeddings to perform lexicon expansion.",8 Conclusion,[0],[0]
"We then used multidimensional analysis to group these markers into stance dimensions, which we show to be internally coherent and extrinsically useful.",8 Conclusion,[0],[0]
"Our hope is that these stance dimensions will be valuable as a convenient building block for future research on interactional meaning.
",8 Conclusion,[0],[0]
Acknowledgments Thanks to the anonymous reviewers for their useful and constructive feedback on our submission.,8 Conclusion,[0],[0]
"This research was supported by Air Force Office of Scientific Research award FA9550-14-1-0379, by National Institutes of Health award R01-GM112697, and by the National Science Foundation awards 1452443 and 1111142.",8 Conclusion,[0],[0]
"We thank Tyler Schnoebelen for helpful discussions; C.J. Hutto, Tanushree Mitra, and Sandeep Soni for assistance with Mechanical Turk experiments; and Ian Stewart for assistance with creating word embeddings.",8 Conclusion,[0],[0]
"We also thank the Mechanical Turk workers for performing the word intrusion task, and for feedback on a pilot task.",8 Conclusion,[0],[0]
"The sociolinguistic construct of stancetaking describes the activities through which discourse participants create and signal relationships to their interlocutors, to the topic of discussion, and to the talk itself.",abstractText,[0],[0]
"Stancetaking underlies a wide range of interactional phenomena, relating to formality, politeness, affect, and subjectivity.",abstractText,[0],[0]
"We present a computational approach to stancetaking, in which we build a theoretically-motivated lexicon of stance markers, and then use multidimensional analysis to identify a set of underlying stance dimensions.",abstractText,[0],[0]
"We validate these dimensions intrinsically and extrinsically, showing that they are internally coherent, match pre-registered hypotheses, and correlate with social phenomena.",abstractText,[0],[0]
A Multidimensional Lexicon for Interpersonal Stancetaking,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 753–762 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1070",text,[0],[0]
"One of the most successful approaches to grammatical error correction (GEC) is to cast the problem as (monolingual) machine translation (MT), where we translate from possibly ungrammatical English sentences to corrected ones (Brockett et al., 2006; Gao et al., 2010; Junczys-Dowmunt and Grundkiewicz, 2016).",1 Introduction,[0],[0]
"Such systems, which are based on phrasebased MT models that are typically trained on large sets of sentence-correction pairs, can correct global errors such as word order and usage and local errors in spelling and inflection.",1 Introduction,[0],[0]
"The approach has proven superior to systems based on local classifiers that can only fix focused errors in prepositions, determiners, or inflected forms (Rozovskaya and Roth, 2016).
∗This work was conducted while the third author worked at Microsoft Research.
",1 Introduction,[0],[0]
"Recently, neural machine translation (NMT) systems have achieved substantial improvements in translation quality over phrase-based MT systems (Sutskever et al., 2014; Bahdanau et al., 2015).",1 Introduction,[0],[0]
"Thus, there is growing interest in applying neural systems to GEC (Yuan and Briscoe, 2016; Xie et al., 2016).",1 Introduction,[0],[0]
"In this paper, we significantly extend previous work, and explore new neural models to meet the unique challenges of GEC.
",1 Introduction,[0],[0]
The core component of most NMT systems is a sequence-to-sequence (S2S) model which encodes a sequence of source words into a vector and then generates a sequence of target words from the vector.,1 Introduction,[0],[0]
"Unlike the phrase-based MT models, the S2S model can capture long-distance, or even global, word dependencies, which are crucial to correcting global grammatical errors and helping users achieve native speaker fluency (Sakaguchi et al., 2016).",1 Introduction,[0],[0]
"Thus, the S2S model is expected to perform better on GEC than phrase-based models.",1 Introduction,[0],[0]
"However, as we will show in this paper, to achieve the best performance on GEC, we still need to extend the standard S2S model to address several task-specific challenges, which we will describe below.
",1 Introduction,[0],[0]
"First, a GEC model needs to deal with an extremely large vocabulary that consists of a large number of words and their (mis)spelling variations.",1 Introduction,[0],[0]
"Second, the GEC model needs to capture structure at different levels of granularity in order to correct errors of different types.",1 Introduction,[0],[0]
"For example, while correcting spelling and local grammar errors requires only word-level or sub-word level information, e.g., violets→ violates (spelling) or violate→ violates (verb form), correcting errors in word order or usage requires global semantic relationships among phrases and words.
",1 Introduction,[0],[0]
"Standard approaches in neural machine translation, also applied to grammatical error correction by Yuan and Briscoe (2016), address the large vocabulary problem by restricting the vocabulary to a limited number of high-frequency words and re-
753
sorting to standard word translation dictionaries to provide translations for the words that are out of the vocabulary (OOV).",1 Introduction,[0],[0]
"However, this approach often fails to take into account the OOVs in context for making correction decisions, and does not generalize well to correcting words that are unseen in the parallel training data.",1 Introduction,[0],[0]
"An alternative approach, proposed by Xie et al. (2016), applies a character-level sequence to sequence neural model.",1 Introduction,[0],[0]
"Although the model eliminates the OOV issue, it cannot effectively leverage word-level information for GEC, even if it is used together with a separate word-based language model.
",1 Introduction,[0],[0]
"Our solution to the challenges mentioned above is a novel, hybrid neural model with nested attention layers that infuse both word-level and character-level information.",1 Introduction,[0],[0]
The architecture of the model is illustrated in Figure 1.,1 Introduction,[0],[0]
The word-level information is used for correcting global grammar and fluency errors while the character-level information is used for correcting local errors in spelling or inflected forms.,1 Introduction,[0],[0]
Contextual information is crucial for GEC.,1 Introduction,[0],[0]
"Using the proposed model, by combining embedding vectors and attention at both word and character levels, we model all contextual words, including OOVs, in a unified context vector representation.",1 Introduction,[0],[0]
"In particular, as we will discuss in Section 5, the character-level attention layer captures most useful information for correcting local errors that involve small edits in orthography.
",1 Introduction,[0],[0]
Our model differs substantially from the wordlevel S2S model of Yuan and Briscoe (2016) and the character-level S2S model of Xie et al. (2016) in the way we infuse information at both the word level and the character level.,1 Introduction,[0],[0]
"We extend the wordcharacter hybrid model of Luong and Manning (2016), which was originally developed for machine translation, by introducing a character attention layer.",1 Introduction,[0],[0]
"This allows the model to learn substitution patterns at both the character level and the word level in an end-to-end fashion, using sentencecorrection pairs.
",1 Introduction,[0],[0]
"We validate the effectiveness of our model on the CoNLL-14 benchmark dataset (Ng et al., 2014).",1 Introduction,[0],[0]
"Results show that the proposed model outperforms all previous neural models for GEC, including the hybrid model of Luong and Manning (2016), which we apply to GEC for the first time.",1 Introduction,[0],[0]
"When integrated with a large word-based n-gram language model, our GEC system achieves an F0.5 of 45.15 on CoNLL-14, substantially exceeding the previ-
ously reported top performance of 40.56 achieved by using a neural model and an external language model (Xie et al., 2016).",1 Introduction,[0],[0]
A variety of classifier-based and MT-based techniques have been applied to grammatical error correction.,2 Related Work,[0],[0]
The CoNLL-14 shared task overview paper of Ng et al. (2014) provides a comparative evaluation of approaches.,2 Related Work,[0],[0]
"Two notable advances after the shared task have been in the areas of combining classifiers and phrase-based MT (Rozovskaya and Roth, 2016) and adapting phrase-based MT to the GEC task (Junczys-Dowmunt and Grundkiewicz, 2016).",2 Related Work,[0],[0]
The latter work has reported the highest performance to date on the task of 49.5 in F0.5 score on the CoNLL-14 test set.,2 Related Work,[0],[0]
"This method integrates discriminative training toward the task-specific evaluation function, a rich set of features, and multiple large language models.",2 Related Work,[0],[0]
Neural approaches to the task are less explored.,2 Related Work,[0],[0]
"We believe that the advances from Junczys-Dowmunt and Grundkiewicz (2016) are complementary to the ones we propose for neural MT, and could be integrated with neural models to achieve even higher performance.
",2 Related Work,[0],[0]
"Two prior works explored sequence to sequence neural models for GEC (Xie et al., 2016; Yuan and Briscoe, 2016), while Chollampatt et al. (2016) integrated neural features in a phrase-based system for the task.",2 Related Work,[0],[0]
"Neural models were also applied to the related sub-task of grammatical error identification (Schmaltz et al., 2016).",2 Related Work,[0],[0]
"Yuan and Briscoe (2016) demonstrated the promise of neural MT for GEC but did not adapt the basic sequence-to-sequence with attention to its unique challenges, falling back to traditional word-alignment models to address vocabulary coverage with a post-processing heuristic.",2 Related Work,[0],[0]
"Xie et al. (2016) built a character-level sequence
to sequence model, which achieves open vocabulary and character-level modeling, but has difficulty with global word-level decisions.
",2 Related Work,[0],[0]
"The primary focus of our work is integration of character and word-level reasoning in neural models for GEC, to capture global fluency errors and local errors in spelling and closely related morphological variants, while obtaining open vocabulary coverage.",2 Related Work,[0],[0]
This is achieved with the help of character and word-level encoders and decoders with two nested levels of attention.,2 Related Work,[0],[0]
Our model is inspired by advances in sub-word level modeling in neural machine translation.,2 Related Work,[0],[0]
We build mostly on the hybrid model of Luong and Manning (2016) to expand its capability to correct rare words by fine-grained character-level attention.,2 Related Work,[0],[0]
We directly compare our model to the one of Luong and Manning (2016) on the grammar correction task.,2 Related Work,[0],[0]
"Alternative methods for MT include modeling of word pieces to achieve open vocabulary (Sennrich et al., 2016), and more recently, fully character-level modeling (Lee et al., 2017).",2 Related Work,[0],[0]
None of these models integrate two nested levels of attention although an empirical evaluation of these approaches for GEC would also be interesting.,2 Related Work,[0],[0]
"Our model is hybrid, and uses both word-level and character-level representations.",3 Nested Attention Hybrid Model,[0],[0]
"It consists of a word-based sequence-to-sequence model as a backbone, and additional character-level encoder, decoder, and attention components, which focus on words that are outside the word-level model’s vocabulary.",3 Nested Attention Hybrid Model,[0],[0]
The word-based backbone closely follows the basic neural sequence-to-sequence architecture with attention as proposed by Bahdanau et al. (2015) and applied to grammatical error correction by Yuan and Briscoe (2016).,3.1 Word-based sequence-to-sequence model as backbone,[0],[0]
"For completeness, we give a sketch here.",3.1 Word-based sequence-to-sequence model as backbone,[0],[0]
"It uses recurrent neural networks to encode the input sentence and to decode the output sentence.
",3.1 Word-based sequence-to-sequence model as backbone,[0],[0]
"Given a sequence of embedding vectors, corresponding to a sequence of input words x:
x = (x1, . . .",3.1 Word-based sequence-to-sequence model as backbone,[0],[0]
", xT ), (1)
the encoder creates a corresponding context-
specific sequence of hidden state vectors e:
e = (h1, . . .",3.1 Word-based sequence-to-sequence model as backbone,[0],[0]
", hT )
",3.1 Word-based sequence-to-sequence model as backbone,[0],[0]
"The hidden state ht at time t is computed as: ft = GRUencf (ft−1, xt) , bt = GRUencb(bt+1, xt), ht =",3.1 Word-based sequence-to-sequence model as backbone,[0],[0]
"[ft; bt], where GRUencf and GRUencb stand for gated recurrent unit functions as described in Cho et al. (2014).",3.1 Word-based sequence-to-sequence model as backbone,[0],[0]
"We use the symbol GRU with different subscripts to represent GRU functions using different sets of parameters (for example, we used the encf and encb subscripts to denote the parameters of the forward and backward word-level encoder units.)
",3.1 Word-based sequence-to-sequence model as backbone,[0],[0]
"The decoder network is also an RNN using GRU units, and defines a sequence of hidden states d̄1, . . .",3.1 Word-based sequence-to-sequence model as backbone,[0],[0]
", d̄S used to define the probability of an output sequence y1, . . .",3.1 Word-based sequence-to-sequence model as backbone,[0],[0]
", yS as follows:
The context vector cs at time step s is computed as follows:
cs = T∑
j=1
αsjhj (2)
where: αsk =
usk∑T j=1 usj
(3)
usk = φ1(ds) Tφ2(hk) (4)
Here φ1 and φ2 denote feedforward linear transformations followed by a tanh nonlinearity.",3.1 Word-based sequence-to-sequence model as backbone,[0],[0]
"The next hidden state d̄s is then defined as:
ds = GRUdec( ¯ds−1, ys−1),
d̄s = ReLU(W",3.1 Word-based sequence-to-sequence model as backbone,[0],[0]
"[cs; ds])
where ys−1 is the embedding of the output token at time s-1.",3.1 Word-based sequence-to-sequence model as backbone,[0],[0]
"ReLU indicates rectified linear units (Hahnloser et al., 2000).
",3.1 Word-based sequence-to-sequence model as backbone,[0],[0]
"The probability of each target word ys is computed as: p(ys|y<s,x) = softmax(g(d̄s)), where g is a function that maps the decoder state into a vector of size the dimensionality of the target vocabulary.
",3.1 Word-based sequence-to-sequence model as backbone,[0],[0]
"The model is trained by minimizing the crossentropy loss, which for a given (x,y) pair is:
Loss(x,y) =",3.1 Word-based sequence-to-sequence model as backbone,[0],[0]
"− S∑
s=1
log p(ys|y<s,x) (5)
",3.1 Word-based sequence-to-sequence model as backbone,[0],[0]
"For parallel training data C, the loss is:
Loss = − ∑
(x,y)∈C
S∑
s=1
log p(ys|y<s,x)",3.1 Word-based sequence-to-sequence model as backbone,[0],[0]
"The word-level backbone models a limited vocabulary of source and target words, and represents out-of-vocabulary tokens with special UNK symbols.",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"In the standard word-level NMT approach, valuable information is lost for source OOV words and target OOV words are predicted using postprocessing heuristics.
",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"Hybrid encoder
Our hybrid architecture overcomes the loss of source information in the word-level backbone by building up compositional representations of the source OOV words using a character-level recurrent neural network with GRU units.",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"These representations are used in place of the special source UNK embeddings in the backbone, and contribute to the contextual encoding of all source tokens.
",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"For example, a three word input sentence where the last term is out-of-vocabulary will be represented as the following vector of embeddings in the word-level model: x = (x1, x2, x3), where x3 would be the embedding for the UNK symbol.
",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"The hybrid encoder builds up a word embedding for the third word based on its character sequence: xc1, . . .",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
", x c M .",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"The encoder computes a sequence of hidden states ec for this character sequence, by a forward character-level GRU network:
ec = (h c 1, . . .",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
", h c M ), (6)
",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
The last state hcM is used as an embedding of the unknown word.,3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"The sequence of embeddings for our example three-word sequence becomes: x = (x1, x2, h c M ).",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
We use the same dimensionality for word embedding vectors xi and composed character sequence vectors hcM to ensure the two ways to define embeddings are compatible.,3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"Our hybrid source encoder architecture is similar to the one proposed by Luong and Manning (2016).
",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"Nested attention hybrid decoder
In traditional word-based sequence-to-sequence models special target UNK tokens are used to represent outputs that are outside the target vocabulary.",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"A post-processing UNK-replacement method is then used (Cho et al., 2015; Yuan and Briscoe, 2016) to replace these special tokens with target words.",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"The hybrid model of (Luong and Manning, 2016) uses a jointly trained character-level decoder
to generate target words corresponding to UNK tokens, and outperforms the traditional approach in the machine translation task.
",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"However, unlike machine translation, models for grammar correction conduct “translation” in the same language, and often need to apply a small number of local edits to the character sequence of a source word corresponding to the target UNK word.",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"For example, rare but correct words such as entity names need to be copied as is, and local spelling errors or errors in inflection need to be corrected.",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"The architecture of Luong and Manning (2016) does not have direct access to a source character sequence, but only uses a single fixed-dimensionality embedding of source unknown words aggregated with additional contextual information from the source.
",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"To address the needs of the grammatical error correction task, we propose a novel hybrid decoder with two nested levels of attention: word level and character-level.",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"The character-level attention serves to provide the decoder with direct access to the relevant source character sequence.
",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"More specifically, the probability of each target word is defined as follows: For words in the target vocabulary, the probability is defined by the wordlevel backbone.",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"For words outside the vocabulary, the probability of each token is the probability of UNK according to the backbone, multiplied by the probability of the word’s character sequence.
",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
The probability of the target character sequence corresponding to an UNK token at position s in the target is defined using a character-level decoder.,3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"As in Luong and Manning (2016), the “separate path” architecture is used to capture the relevant context and define the initial state for the character-level decoder:
d̂s = ReLU(Ŵ",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"[cs; ds])
where Ŵ are parameters different from W , and d̂s is not used by the word-level model in predicting the subsequent tokens, but is only used to initialize the character-level decoder.
",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"To be able to attend to the relevant source character sequence when generating the target character sequence, we use the concept of hard attention (Xu et al., 2015), but use an arg-max approximation for inference instead of sampling.",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"A similar approach to represent discrete hidden structure in a variety of architectures is used in Kong et al. (2017).
",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"The source index zs corresponding to the target
position s is defined according to the word-level attention model:
zs = arg max k∈0...",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"T−1 αsk
where αsk are the intermediate outputs of the word-level attention model we described in Eq.(3).
",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"The character-level decoder generates a character sequence yc = (yc1, . . .",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
", y c N ), conditioned on the initial vector d̂s and the source index",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
zs.,3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"The characters are generated using a hidden state vector dcn at each time step, via a softmax(gc(dcn)), where gc maps the state to the target character vocabulary space.
",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"If the source word xzs is in the source vocabulary, the model is analogous to the one of Luong and Manning (2016) and does not use characterlevel attention: the source context is available only in aggregated form to initialize the state of the decoder.",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"The state dcn for step n in the characterlevel decoder is defined as follows, where GRUcdec are parameters for the gated recurrent cell of this decoder:
dcn = { GRUcdec(d̂s, ycn−1) n",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
= 0,3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"GRUcdec(dcn−1, ycn−1) n > 0
",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"In contrast, if the corresponding token in the source xzs is also an out-of-vocabulary word, we define a second nested level of character attention and use it in the character-level decoder.",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
The character-level attention focuses on individual characters from the source word xzs .,3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"If ec are the source character hidden vectors computed as in Eq.(6), the recurrence equations for the characterlevel decoder with nested attention are:
¯dcn = ReLU(Wc[ccn; dcn])
dcn = { GRUcdecNested(d̂s, ycn−1) n = 0",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"GRUcdecNested( ¯dcn−1, ycn−1) n > 0
where ccn is the context vector obtained using character-level attention on the sequence ec and the last state of the character-level decoder dcn, computed following equations 2, 3 and 4, but using a different set of parameters.
",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"These equations show that the character-level decoder with nested attention can use both the wordlevel state d̂s, and the character-level context ccn
and hidden state dcn to perform global and local editing operations.
",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"Since we introduced two architectures for the character-level decoder depending on whether the source word xzs is OOV, the combined loss function is defined as follows for end-to-end training:
Losstotal = Lossw + αLossc1 + βLossc2
Here Lossw is the standard word-level loss in Eq.(5); character level losses Lossc1 and Lossc2 are losses for target OOV words corresponding to source known and unknown tokens, respectively.",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"α and β are hyper-parameters to balance the loss terms.
",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"As seen, our proposed nested attention hybrid model uses character-level attention only when both a predicted target word and its corresponding source input word are OOV.",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"While the model can be naturally generalized to integrate characterlevel attention for known words, the original hybrid model proposed by Luong and Manning (2016) does not use any character-level information for known words.",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"Thus for a controlled evaluation of the impact of the addition of character-level attention only, in this paper we limit character-level attention to OOV words, which already use characters as a basis for building their embedding vectors.",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"A thorough investigation of the impact of characterlevel information in the encoder, attention, and decoder for known words as well is an interesting topic for future research.
",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
Decoding for word-level and hybrid models Beam-search is used to decode hypotheses according to the word-level backbone model.,3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
"For the hybrid model architecture, word-level beam search is conducted first; for each target UNK token, character-level beam-search is used to generate a corresponding target word.",3.2 Hybrid encoder and decoder with two nested levels of attention,[0],[0]
We use standard publicly available datasets for training and evaluation.,4.1 Dataset and Evaluation,[0],[0]
"One data source is the NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013), which is provided as a training set for the CoNLL-13 and CoNLL-14 shared tasks.",4.1 Dataset and Evaluation,[0],[0]
"From the original corpus of size about 60K parallel sentences, we randomly selected close to 5K sentence pairs for use as a validation set, and 45K parallel sentences for use in training.",4.1 Dataset and Evaluation,[0],[0]
"A second data source
Training Validation Development Test #Sent pairs 2,608,679 4,771 1,381 1,312
is the Cambridge Learner Corpus (CLC) (Nicholls, 2003), from which we extracted a substantially larger set of parallel sentences.",4.1 Dataset and Evaluation,[0],[0]
"Finally, we used additional training examples from the Lang-8 Corpus of Learner English v1.0 (Tajiri et al., 2012).",4.1 Dataset and Evaluation,[0],[0]
"As Lang-8 data is crowd-sourced, we used heuristics to filter out noisy examples: we removed sentences longer than 100 words and sentence pairs where the correction was substantially shorter than the input text.",4.1 Dataset and Evaluation,[0],[0]
"Table 2 shows the number of sentence pairs from each source used for training.
",4.1 Dataset and Evaluation,[0],[0]
"We evaluate the performance of the models on the standard sets from the CoNLL-14 shared task (Ng et al., 2014).",4.1 Dataset and Evaluation,[0],[0]
"We report final performance on the CoNLL-14 test set without alternatives, and analyze model performance on the CoNLL-13 development set (Dahlmeier et al., 2013).",4.1 Dataset and Evaluation,[0],[0]
We use the development and validation sets for model selection.,4.1 Dataset and Evaluation,[0],[0]
The sizes of all datasets in number of sentences are shown in Table 1.,4.1 Dataset and Evaluation,[0],[0]
"We report performance in F0.5-measure, as calculated by the m2scorer— the official implementation of the scoring metric in the shared task.",4.1 Dataset and Evaluation,[0],[0]
1,4.1 Dataset and Evaluation,[0],[0]
"Given system outputs and gold-standard edits, m2scorer computes the F0.5 measure of a set of system edits against a set of gold-standard edits.",4.1 Dataset and Evaluation,[0],[0]
"We evaluate our model in comparison to the strong baseline of a word-based neural sequenceto-sequence model with attention, with postprocessing for handling out-of-vocabulary words (Yuan and Briscoe, 2016); we refer to this model as word NMT+UNK replacement.",4.2 Baseline,[0],[0]
"Like Yuan and Briscoe (2016), we use a traditional wordalignment model (GIZA++) to derive a wordcorrection lexicon from the parallel training set.",4.2 Baseline,[0],[0]
"However, in decoding, we don’t use GIZA++ to find the corresponding source word for each tar-
1http://www.comp.nus.edu.sg/˜nlp/sw/ m2scorer.tar.gz
get OOV, but follow Cho et al. (2015), Section 3.3 to use the NMT system’s attention weights instead.",4.2 Baseline,[0],[0]
"The target OOV is then replaced by the most likely correction of the source word from the wordcorrection lexicon, or by the source word itself if there are no available corrections.",4.2 Baseline,[0],[0]
"The embedding size for all word and characterlevel encoders and decoders is set to 1000, and the hidden unit size is also 1000.",4.3 Training Details and Results,[0],[0]
"To reproduce the model of Yuan and Briscoe (2016), we selected the word vocabulary for the baseline by choosing the 30K most frequent words in the source and target respectively to form the source and target vocabularies.",4.3 Training Details and Results,[0],[0]
"In preliminary experiments for the hybrid models, we found that selecting the same vocabulary of 30K words for the source and target based on combined frequency was better (.003 in F0.5) and use that method for vocabulary selection instead.",4.3 Training Details and Results,[0],[0]
"However, there was no gain observed by using such a vocabulary selection method in the baseline.",4.3 Training Details and Results,[0],[0]
"Although the source and target vocabularies in the hybrid models are the same, like in the word-level model, the embedding parameters for source and target words are not shared.
",4.3 Training Details and Results,[0],[0]
The hyper-parameters for the losses in our models are selected based on the development set and set as follows: α = β = 0.5.,4.3 Training Details and Results,[0],[0]
"All models are trained with mini-batch size of 128 (batches are shuffled), initial learning rate of 0.0003 and a 0.95 decay ratio if the cost increases in two consecutive 100 iterations.",4.3 Training Details and Results,[0],[0]
"The gradient is rescaled whenever its norm exceeds 10, and dropout is used with a probability of 0.15.",4.3 Training Details and Results,[0],[0]
"Parameters are uniformly ini-
tialized in [− √
(3)√ 1000 ,
√ (3)√
1000 ].
",4.3 Training Details and Results,[0],[0]
We perform inference on the validation set every 5000 iterations to log word-level cost and characterlevel costs; we save parameter values for the model every 10000 iterations as well as the end of each epoch.,4.3 Training Details and Results,[0],[0]
The stopping point for training is selected based on development set F0.5 among the top 20 parameter sets with best validation set value of the loss function.,4.3 Training Details and Results,[0],[0]
Training of the nested attention hybrid model takes approximately five days on a Tesla k40m GPU.,4.3 Training Details and Results,[0],[0]
"The basic hybrid model trains in around four days and the word-level backbone trains in approximately three days.
",4.3 Training Details and Results,[0],[0]
Table 3 shows the performance of the baseline and our nested attention hybrid model on the development and test sets.,4.3 Training Details and Results,[0],[0]
"In addition to the word-level
baseline, we include the performance of a hybrid model with a single level of attention, which follows the work of Luong and Manning (2016) for machine translation, and is the first application of a hybrid word/character-level model to grammatical error correction.",4.3 Training Details and Results,[0],[0]
"Based on hyper-parameter selection, the character-level component weight of the loss is α = 1 for the basic hybrid model.
",4.3 Training Details and Results,[0],[0]
"As shown in Table 3, our implementation of the word NMT+UNK replacement baseline approaches the performance of the one reported in Yuan and Briscoe (2016) (38.77 versus 39.9).",4.3 Training Details and Results,[0],[0]
We attribute the difference to differences in the training set and the word-alignment methods used.,4.3 Training Details and Results,[0],[0]
Our reimplementation serves to provide a controlled experimental evaluation of the impact of hybrid models and nested attention on the GEC task.,4.3 Training Details and Results,[0],[0]
"As seen, our nested attention hybrid model substantially improves upon the baseline, achieving a gain of close to 3 points on the test set.",4.3 Training Details and Results,[0],[0]
"The hybrid word/character model with a single level of attention brings a large improvement as well, showing the importance of character-level information for this task.",4.3 Training Details and Results,[0],[0]
We delve deeper into the impact of nested attention for the hybrid model in Section 5.,4.3 Training Details and Results,[0],[0]
"The value of large language models for grammatical error correction is well known, and such models have been used in classifier and MT-based systems.",4.4 Integrating a Web-scale Language Model,[0],[0]
"To establish the potential of such models in word-based neural sequence-to-sequence systems, we integrate a web-scale count-based language model.",4.4 Integrating a Web-scale Language Model,[0],[0]
"In particular, we use the modified Kneser-Ney 5-gram language model trained from Common Crawl (Buck et al., 2014), made available for download by Junczys-Dowmunt and Grundkiewicz (2016).
",4.4 Integrating a Web-scale Language Model,[0],[0]
Candidates generated by neural models are reranked using the following linear interpolation of log probabilities: sy|x,4.4 Integrating a Web-scale Language Model,[0],[0]
=,4.4 Integrating a Web-scale Language Model,[0],[0]
logPNN (y|x) + λ,4.4 Integrating a Web-scale Language Model,[0],[0]
logPLM (y).,4.4 Integrating a Web-scale Language Model,[0],[0]
Here λ is a hyper-parameter that balances the weights of the neural network model and the language model.,4.4 Integrating a Web-scale Language Model,[0],[0]
"We tuned λ separately
for each neural model variant, by exploring values in the range [0.0, 2.0] with step size 0.1, and selecting according to development set F0.5.",4.4 Integrating a Web-scale Language Model,[0],[0]
"The selected values of λ are: 1.6 for word NMT + UNK replacement and 1.0 for the nested attention model.
",4.4 Integrating a Web-scale Language Model,[0],[0]
Table 4 shows the impact of the LM when combined with the neural models implemented in this work.,4.4 Integrating a Web-scale Language Model,[0],[0]
"The table also lists the results reported by Xie et al. (2016), for their character-level neural model combined with a large word-level language model.",4.4 Integrating a Web-scale Language Model,[0],[0]
"Our best results exceed the ones reported in the prior work by more than 4 points, although we should note that Xie et al. (2016) used a smaller parallel data set for training.",4.4 Integrating a Web-scale Language Model,[0],[0]
We analyze the impact of sub-word level information and the two nested levels of attention in more detail by looking at the performance of the models on different segments of the data.,5 Analysis,[0],[0]
"In particular, we analyze the performance of the models on sentences containing OOV source words versus ones without OOV words, and corrections to orthographically similar versus dissimilar word forms.",5 Analysis,[0],[0]
We present a comparative performance analysis of models on the CoNLL-13 development set.,5.1 Performance by Segment: OOV versus Non-OOV,[0],[0]
"First, we divide the set into two segments: OOV and NonOOV, based on whether there is at least one OOV word in the given source input.",5.1 Performance by Segment: OOV versus Non-OOV,[0],[0]
Table 5 shows that both hybrid architectures substantially outperform the word-level model in both segments of the data.,5.1 Performance by Segment: OOV versus Non-OOV,[0],[0]
The additional nested character-level attention of our hybrid model brings a sizable improvement over the basic hybrid model in the OOV segment and a small degradation in the non-OOV segment.,5.1 Performance by Segment: OOV versus Non-OOV,[0],[0]
"We should note that in future work characterlevel attention can be added for non-OOV source words in the nested attention model, which could improve performance on this segment as well.
",5.1 Performance by Segment: OOV versus Non-OOV,[0],[0]
"Table 6 shows an example where the nested attention hybrid model successfully corrects a misspelling resulting in an OOV word on the source, whereas the baseline word-level system simply copies the source word without fixing the error (since this particular error is not observed in the parallel training set).",5.1 Performance by Segment: OOV versus Non-OOV,[0],[0]
"To analyze more precisely the impact of the additional character-level attention introduced by our design, we continue to investigate the OOV segment in more detail.
",5.2 Impact of Nested Attention on Different Error Types,[0],[0]
"The concept of edit, which is also used by the official M2 score metric, is defined as a minimal pair of corresponding sub-strings in a source sentence and a correction.",5.2 Impact of Nested Attention on Different Error Types,[0],[0]
"For example, in the sentence fragment pair: “Even though there is a risk of causing harms to someone, people still are prefers to keep their pets without a leash.”",5.2 Impact of Nested Attention on Different Error Types,[0],[0]
"→ “Even though there is a risk of causing harm to someone, people still prefer to keep their pets without a leash.”",5.2 Impact of Nested Attention on Different Error Types,[0],[0]
", the minimal edits are “harms→ harm” and “are prefers→ prefer”.",5.2 Impact of Nested Attention on Different Error Types,[0],[0]
"The F0.5 score is computed using weighted precision and recall of the set of a system’s edits against one or more sets of reference edits.
",5.2 Impact of Nested Attention on Different Error Types,[0],[0]
"For our in-depth analysis, we classify edits in the OOV segment into two types: small changes and large changes, based on whether the source and target phrase of the edit are orthographically similar or not.",5.2 Impact of Nested Attention on Different Error Types,[0],[0]
"More specifically, we say that the target and
source phrases are orthographically similar, iff: the character edit distance is at most 2 and the source or target is at most 8 characters long, or edit ratio < 0.25, where edit ratio = character edit distancemin(len(src),len(tar))+0.1 , len(∗) denotes number of characters in ∗, and src and tgt denote the pairs in the edit.",5.2 Impact of Nested Attention on Different Error Types,[0],[0]
"There are 307 gold edits in the “small changes” portion of the CoNLL-13 OOV segment, and 481 gold edits in the “large changes” portion.
",5.2 Impact of Nested Attention on Different Error Types,[0],[0]
Our hypothesis is that the additional characterlevel attention layer is particularly useful to model edits among orthographically similar words.,5.2 Impact of Nested Attention on Different Error Types,[0],[0]
Table 7 contrasts the impact of character-level attention on the two portions of the data.,5.2 Impact of Nested Attention on Different Error Types,[0],[0]
"We can see that the gains in the “small changes” portion are indeed quite large, indicating that the fine-grained character-level attention empowers the model to more accurately correct confusions among phrases with high character-level similarity.",5.2 Impact of Nested Attention on Different Error Types,[0],[0]
The impact in the “large changes” portion is slightly positive in precision and slightly negative in recall.,5.2 Impact of Nested Attention on Different Error Types,[0],[0]
"Thus most of the benefit of the additional character-level attention stems from improvements in the “small changes” portion.
",5.2 Impact of Nested Attention on Different Error Types,[0],[0]
Table 8 shows an example input which illustrates the precision gain of the nested attention hybrid model.,5.2 Impact of Nested Attention on Different Error Types,[0],[0]
The input sentence has a source OOV word which is correct.,5.2 Impact of Nested Attention on Different Error Types,[0],[0]
"The hybrid model introduces an error in this word, because it uses only a single source context vector, aggregating the characterlevel embedding of the source OOV word together with other source words.",5.2 Impact of Nested Attention on Different Error Types,[0],[0]
"The additional characterlevel attention layer in the nested hybrid model enables the correct copying of this long source OOV word, without employing the heuristic mechanism of the word-level NMT system.",5.2 Impact of Nested Attention on Different Error Types,[0],[0]
We have introduced a novel hybrid neural model with two nested levels of attention: word-level and character-level.,6 Conclusions,[0],[0]
The model addresses the unique challenges of the grammatical error correction task and achieves the best reported results on the CoNLL-14 benchmark among fully neural systems.,6 Conclusions,[0],[0]
"Our nested attention hybrid model deeply combines the strengths of word and character level information in all components of an end-to-end neural model: the encoder, the attention layers, and the decoder.",6 Conclusions,[0],[0]
This enables it to correct both global wordlevel and local character-level errors in a unified way.,6 Conclusions,[0],[0]
The new architecture contributes substantial improvement in correction of confusions among rare or orthographically similar words compared to word-level sequence-to-sequence and non-nested hybrid models.,6 Conclusions,[0],[0]
"We would like to thank the ACL reviewers for their insightful suggestions, Victoria Zayats for her help with reproducing the baseline word-level NMT system and Yu Shi, Daxin Jiang and Michael Zeng for the helpful discussions.",Acknowledgements,[0],[0]
"Grammatical error correction (GEC) systems strive to correct both global errors in word order and usage, and local errors in spelling and inflection.",abstractText,[0],[0]
"Further developing upon recent work on neural machine translation, we propose a new hybrid neural model with nested attention layers for GEC.",abstractText,[0],[0]
"Experiments show that the new model can effectively correct errors of both types by incorporating word and character-level information, and that the model significantly outperforms previous neural models for GEC as measured on the standard CoNLL14 benchmark dataset.",abstractText,[0],[0]
"Further analysis also shows that the superiority of the proposed model can be largely attributed to the use of the nested attention mechanism, which has proven particularly effective in correcting local errors that involve small edits in orthography.",abstractText,[0],[0]
A Nested Attention Neural Hybrid Model for Grammatical Error Correction,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1066–1076 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1066",text,[0],[0]
"The International Classification of Diseases (ICD) is a healthcare classification system maintained by the World Health Organization (Organization et al., 1978).",1 Introduction,[0],[0]
"It provides a hierarchy of diagnostic codes of diseases, disorders, injuries, signs, symptoms, etc.",1 Introduction,[0],[0]
"It is widely used for reporting diseases and health conditions, assisting in medical reimbursement decisions, collecting morbidity and mortality statistics, to name a few.
",1 Introduction,[0],[0]
"While ICD codes are important for making clinical and financial decisions, medical coding
– which assigns proper ICD codes to a patient visit – is time-consuming, error-prone, and expensive.",1 Introduction,[0],[0]
"Medical coders review the diagnosis descriptions written by physicians in the form of textual phrases and sentences, and (if necessary) other information in the electronic health record of a clinical episode, then manually attribute the appropriate ICD codes by following the coding guidelines (O’malley et al., 2005).",1 Introduction,[0],[0]
Several types of errors frequently occur.,1 Introduction,[0],[0]
"First, the ICD codes are organized in a hierarchical structure.",1 Introduction,[0],[0]
"For a node representing a disease C, the children of this node represent the subtypes of C. In many cases, the difference between disease subtypes is very subtle.",1 Introduction,[0],[0]
It is common that human coders select incorrect subtypes.,1 Introduction,[0],[0]
"Second, when writing diagnosis descriptions, physicians often utilize abbreviations and synonyms, which causes ambiguity and imprecision when the coders are matching ICD codes to those descriptions (Sheppard et al., 2008).",1 Introduction,[0],[0]
"Third, in many cases, several diagnosis descriptions are closely related and should be mapped to a single ICD code.",1 Introduction,[0],[0]
"However, unexperienced coders may code each disease separately.",1 Introduction,[0],[0]
Such errors are called unbundling.,1 Introduction,[0],[0]
"The cost incurred by coding errors and the financial investment spent on improving coding quality are estimated to be $25 billion per year in the US (Lang, 2007; Farkas and Szarvas, 2008).
",1 Introduction,[0],[0]
"To reduce coding errors and cost, we aim at building an ICD coding model which automatically and accurately translates the free-text diagnosis descriptions into ICD codes.",1 Introduction,[0],[0]
"To achieve this goal, several technical challenges need to be addressed.",1 Introduction,[0],[0]
"First, there exists a hierarchical structure among the ICD codes.",1 Introduction,[0],[0]
This hierarchy can be leveraged to improve coding accuracy.,1 Introduction,[0],[0]
"On one hand, if code A and B are both children of C, then it is unlikely to simultaneously assign A and B to a patient.",1 Introduction,[0],[0]
"On the other hand, if the distance be-
tween A and B in the code tree is smaller than that between A and C and we know A is the correct code, then B is more likely to be a correct code than C, since codes with smaller distance are more clinically relevant.",1 Introduction,[0],[0]
How to explore this hierarchical structure for better coding is technically demanding.,1 Introduction,[0],[0]
"Second, the diagnosis descriptions and the textual descriptions of ICD codes are written in quite different styles even if they refer to the same disease.",1 Introduction,[0],[0]
"In particular, the textual description of an ICD code is formally and precisely worded, while diagnosis descriptions are usually written by physicians in an informal and ungrammatical way, with telegraphic phrases, abbreviations, and typos.",1 Introduction,[0],[0]
"Third, it is required that the assigned ICD codes are ranked according to their relevance to the patient.",1 Introduction,[0],[0]
How to correctly determine this order is technically nontrivial.,1 Introduction,[0],[0]
"Fourth, as stated earlier, there does not necessarily exist an one-toone mapping between diagnosis descriptions and ICD codes, and human coders should consider the overall health condition when assigning codes.",1 Introduction,[0],[0]
"In many cases, two closely related diagnosis descriptions need to be mapped onto a single combination ICD code.",1 Introduction,[0],[0]
"On the other hand, physicians may write two health conditions into one diagnosis description which should be mapped onto two ICD codes under such circumstances.
",1 Introduction,[0],[0]
"Contributions In this paper, we design a neural architecture to automatically perform ICD coding given the diagnosis descriptions.",1 Introduction,[0],[0]
"Specifically, we make the following contributions:
• We propose a tree-of-sequences LSTM architecture to simultaneously capture the hierarchical relationship among codes and the semantics of each code.
",1 Introduction,[0],[0]
"• We use an adversarial learning approach to reconcile the heterogeneous writing styles of diagnosis descriptions and ICD code descriptions.
",1 Introduction,[0],[0]
"• We use isotonic constraints to preserve the importance order among codes and develop an algorithm based on ADMM and isotonic projection to solve the constrained problem.
",1 Introduction,[0],[0]
"• We use an attentional matching mechanism to perform many-to-one and one-to-many mappings between diagnosis descriptions and codes.
",1 Introduction,[0],[0]
"• On a clinical datasets with 59K patient visits, we demonstrate the effectiveness of the proposed methods.
",1 Introduction,[0],[0]
The rest of the paper is organized as follows.,1 Introduction,[0],[0]
Section 2 introduces related works.,1 Introduction,[0],[0]
Section 3 and 4 present the dataset and methods.,1 Introduction,[0],[0]
Section 5 gives experimental results.,1 Introduction,[0],[0]
Section 6 presents conclusions and discussions.,1 Introduction,[0],[0]
"Larkey and Croft (1996) studied the automatic assignment of ICD-9 codes to dictated inpatient discharge summaries, using a combination of three classifiers: k-nearest neighbors, relevance feedback, and Bayesian independence classifiers.",2 Related Works,[0],[0]
This method assigns a single code to each patient visit.,2 Related Works,[0],[0]
"However, in clinical practice, each patient is usually assigned with multiple codes.",2 Related Works,[0],[0]
Franz et al. (2000) investigated the automated coding of German-language free-text diagnosis phrases.,2 Related Works,[0],[0]
This approach performs one-to-one mapping between diagnosis descriptions and ICD codes.,2 Related Works,[0],[0]
"This is not in accordance with the coding practice where one-to-many and many-to-one mappings widely exist (O’malley et al., 2005).",2 Related Works,[0],[0]
Pestian et al. (2007) studied the assignment of ICD-9 codes to radiology reports.,2 Related Works,[0],[0]
Kavuluru et al. (2013) proposed an unsupervised ensemble approach to automatically perform ICD-9 coding based on textual narratives in electronic health records (EHRs),2 Related Works,[0],[0]
"Kavuluru et al. (2015) developed multi-label classification, feature selection, and learning to rank approaches for ICD-9 code assignment of in-patient visits based on EHRs.",2 Related Works,[0],[0]
Koopman et al. (2015) explored the automatic ICD-10 classification of cancers from free-text death certificates.,2 Related Works,[0],[0]
"These methods did not consider the hierarchical relationship or importance order among codes.
",2 Related Works,[0],[0]
"The tree LSTM network was first proposed by (Tai et al., 2015) to model the constituent or dependency parse trees of sentences.",2 Related Works,[0],[0]
Teng and Zhang (2016) extended the unidirectional tree LSTM to a bidirectional one.,2 Related Works,[0],[0]
Xie and Xing (2017) proposed a sequence-of-trees LSTM network to model a passage.,2 Related Works,[0],[0]
"In this network, a sequential LSTM is used to compose a sequence of tree LSTMs.",2 Related Works,[0],[0]
The tree LSTMs are built on the constituent parse trees of individual sentences and the sequential LSTM is built on the sequence of sentences.,2 Related Works,[0],[0]
Our proposed tree-of-sequences LSTM network differs from the previous works in twofold.,2 Related Works,[0],[0]
"First, it is applied to a code tree to capture the hierarchical relationship among codes.",2 Related Works,[0],[0]
"Second, it uses a tree LSTM to compose a hierarchy
Diagnosis Descriptions 1.",2 Related Works,[0],[0]
Prematurity at 35 4/7 weeks gestation 2.,2 Related Works,[0],[0]
Twin number two of twin gestation 3.,2 Related Works,[0],[0]
"Respiratory distress secondary to transient tachypnea
of the newborn 4.",2 Related Works,[0],[0]
Suspicion for sepsis ruled out Assigned ICD Codes 1.,2 Related Works,[0],[0]
"V31.00 (Twin birth, mate liveborn, born in hospital, delivered without mention of cesarean section) 2.",2 Related Works,[0],[0]
765.18,2 Related Works,[0],[0]
"(Other preterm infants, 2,000-2,499 grams) 3.",2 Related Works,[0],[0]
775.6 (Neonatal hypoglycemia) 4.,2 Related Works,[0],[0]
770.6 (Transitory tachypnea of newborn) 5.,2 Related Works,[0],[0]
V29.0,2 Related Works,[0],[0]
(Observation for suspected infectious condition) 6.,2 Related Works,[0],[0]
"V05.3 (Need for prophylactic vaccination and inoculation
against viral hepatitis)
Table 1:",2 Related Works,[0],[0]
The diagnosis descriptions of a patient visit and the assigned ICD codes.,2 Related Works,[0],[0]
Inside the parentheses are the descriptions of the codes.,2 Related Works,[0],[0]
"The codes are ranked according to descending importance.
of sequential LSTMs.",2 Related Works,[0],[0]
"Adversarial learning (Goodfellow et al., 2014) has been widely applied to image generation (Goodfellow et al., 2014), domain adaption (Ganin and Lempitsky, 2015), feature learning (Donahue et al., 2016), text generation (Yu et al., 2017), to name a few.",2 Related Works,[0],[0]
"In this paper, we use adversarial learning for mitigating the discrepancy among the writing styles of a pair of sentences.
",2 Related Works,[0],[0]
"The attention mechanism was widely used in machine translation (Bahdanau et al., 2014), image captioning (Xu et al., 2015), reading comprehension (Seo et al., 2016), text classification (Yang et al., 2016), etc.",2 Related Works,[0],[0]
"In this work, we compute attention between sentences to perform many-to-one and one-to-many mappings.",2 Related Works,[0],[0]
"We performed the study on the publicly available MIMIC-III dataset (Johnson et al., 2016), which contains de-identified electronic health records (EHRs) of 58,976 patient visits in the Beth Israel Deaconess Medical Center from 2001 to 2012.",3 Dataset and Preprocessing,[0],[0]
"Each EHR has a clinical note called discharge summary, which contains multiple sections of information, such as ‘discharge diagnosis’, ‘past medical history’, etc.",3 Dataset and Preprocessing,[0],[0]
"From the ‘discharge diagnosis’ and ‘final diagnosis’ sections, we extracted the diagnosis descriptions (DDs) written by physicians.",3 Dataset and Preprocessing,[0],[0]
"Each DD is a short phrase or a sentence, articulating a certain disease or condition.",3 Dataset and Preprocessing,[0],[0]
Medical coders perform ICD coding mainly based on DDs.,3 Dataset and Preprocessing,[0],[0]
"Following such a practice, in this paper, we set the inputs of the automated coding model to be
the DDs while acknowledging that other information in the EHRs is also valuable and is referred to by coders for code assignment.",3 Dataset and Preprocessing,[0],[0]
"For simplicity, we leave the incorporation of non-DD information to future study.
",3 Dataset and Preprocessing,[0],[0]
"Each patient visit is assigned with a list of ICD codes, ranked in descending order of importance and relevance.",3 Dataset and Preprocessing,[0],[0]
"For each visit, the number of codes is usually not equal to the number of diagnosis descriptions.",3 Dataset and Preprocessing,[0],[0]
These ground-truth codes serve as the labels to train our coding model.,3 Dataset and Preprocessing,[0],[0]
"The entire dataset contains 6,984 unique codes, each of which has a textual description, describing a disease, symptom, or condition.",3 Dataset and Preprocessing,[0],[0]
The codes are organized into a hierarchy where the top-level codes correspond to general diseases while the bottom-level ones represent specific diseases.,3 Dataset and Preprocessing,[0],[0]
"In the code tree, children of a node represent subtypes of a disease.",3 Dataset and Preprocessing,[0],[0]
Table 1 shows the DDs and codes of an exemplar patient.,3 Dataset and Preprocessing,[0],[0]
"In this section, we present a neural architecture for ICD coding.",4 Methods,[0],[0]
Figure 1 shows the overview of our approach.,4.1 Overview,[0],[0]
The proposed ICD coding model consists of five modules.,4.1 Overview,[0],[0]
The model takes the ICD-code tree and diagnosis descriptions (DDs) of a patient as inputs and assigns a set of ICD codes to the patient.,4.1 Overview,[0],[0]
The encoder of DDs generates a latent representation vector for a DD.,4.1 Overview,[0],[0]
"The encoder of ICD codes is a tree-of-sequences long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) network.",4.1 Overview,[0],[0]
"It takes the textual descriptions of the ICD codes and their hierarchical structure as in-
puts and produces a latent representation for each code.",4.1 Overview,[0],[0]
The representation aims at simultaneously capturing the semantics of each code and the hierarchical relationship among codes.,4.1 Overview,[0],[0]
"By incorporating the code hierarchy, the model can avoid selecting codes that are subtypes of the same disease and promote the selection of codes that are clinically correlated.",4.1 Overview,[0],[0]
"The writing styles of DDs and code descriptions (CDs) are largely different, which makes the matching between a DD and a CD error-prone.",4.1 Overview,[0],[0]
"To address this issue, we develop an adversarial learning approach to reconcile the writing styles.",4.1 Overview,[0],[0]
"On top of the latent representation vectors of the descriptions, we build a discriminative network to distinguish which ones are DDs and which are CDs.",4.1 Overview,[0],[0]
The encoders of DDs and CDs try to make such a discrimination impossible.,4.1 Overview,[0],[0]
"By doing this, the learned representations are independent of the writing styles and facilitate more accurate matching.",4.1 Overview,[0],[0]
The representations of DDs and CDs are fed into an attentional matching module to perform code assignment.,4.1 Overview,[0],[0]
This attentional mechanism allows multiple DDs to be matched to a single code and allows a single DD to be matched to multiple codes.,4.1 Overview,[0],[0]
"During training, we incorporate the order of importance among codes as isotonic constraints.",4.1 Overview,[0],[0]
These constraints regulate the model’s weight parameters so that codes with higher importance are given larger prediction scores.,4.1 Overview,[0],[0]
This section introduces the encoder of ICD codes.,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
Each code has a description (a sequence of words) that tells the semantics of this code.,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"We use a sequential LSTM (SLSTM) (Hochreiter and Schmidhuber, 1997) to encode this description.",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"To capture the hierarchical relationship among codes, we build a tree LSTM (TLSTM) (Tai et al., 2015) along the code tree.",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"At each TLSTM node, the input vector is the latent representation generated
by the SLSTM.",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"Combining these two types of LSTMs together, we obtain a tree-of-sequences LSTM network (Figure 2).
",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"Sequential LSTM A sequential LSTM (SLSTM) (Hochreiter and Schmidhuber, 1997) network is a special type of recurrent neural network that (1) learns the latent representation (which usually reflects certain semantic information) of words, and (2) models the sequential structure among words.",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"In the word sequence, each word t is allocated with an SLSTM unit, which consists of the following components: an input gate it, a forget gate ft, an output gate ot, a memory cell ct, and a hidden state st.",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"These components (vectors) are computed as follows:
it = σ(W",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
(i)st−1 + U (i)xt + b,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
(i)),4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
ft = σ(W (f)st−1 + U (f)xt + b (f)),4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
ot = σ(W (o)st−1 + U (o)xt + b (o)),4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
ct,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
= it tanh(W(c)st−1 + U(c)xt + b(c)),4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"+ft ct−1 st = ot tanh(ct) (1) where xt is the embedding vector of word t. W, U are component-specific weight matrices and b are bias vectors.
",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"Tree-of-sequences LSTM We use a bidirectional tree LSTM (TLSTM) (Tai et al., 2015; Xie and Xing, 2017) to capture the hierarchical relationships among codes.",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
The inputs of this LSTM include the code hierarchy and hidden states of individual codes produced by the SLSTMs.,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"It consists of a bottom-up TLSTM and a top-down TLSTM, which produce two hidden states h↑ and h↓ at each node in the tree.
",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"In the bottom-up TLSTM, an internal node (representing a code C, having M children) is comprised of these components: an input gate i↑, an output gate o↑, a memory cell c↑, a hidden state h↑ and M child-specific forget gates {f (m) ↑ } M m=1 where f (m)↑ corresponds to the m-th child.",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"The transition equations among components are:
i↑ = σ",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
( ∑M m=1,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"W (i,m) ↑",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
h (m) ↑ +,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
U (i)s + b,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
(i) ↑ ),4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"∀m, f (m)↑ = σ(W",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"(f,m) ↑ h",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
(m) ↑ +,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"U (f,m)s + b",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"(f,m) ↑ )
",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
o↑ = σ,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
( ∑M m=1,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"W (o,m) ↑",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
h (m) ↑ +,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"U (o)s + b (o) ↑ )
",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"u↑ = tanh( ∑M m=1 W (u,m) ↑",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
h (m) ↑ +,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
U (u)s + b,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"(u) ↑ )
",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
c↑ = i↑ u↑ + ∑M m=1,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
f (m) ↑,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
c (m) ↑,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
h↑ =,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
o↑ tanh(c↑),4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"(2)
where s is the SLSTM hidden state that encodes the description of code C; {h(m)↑ } M m=1 and {c(m)↑ } M m=1 are the bottom-up TLSTM hidden states and memory cells of the children.",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"W, U, b are component-specific weight matrices and bias vectors.",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"For a leaf node having no children, its only input is the SLSTM hidden state s and no forget gates are needed.
",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"In the top-down TLSTM, for a non-root node, it has such components: an input gate i↓, a forget gate f↓, an output gate o↓, a memory cell c↓ and a hidden state",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
h↓.,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"The transition equations are:
i↓ = σ(W",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
(i) ↓,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
h (p) ↓,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
+ b (i) ↓ ),4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
f↓ = σ(W (f) ↓,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
h (p) ↓,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
+ b,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
(f) ↓ ),4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
o↓,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
= σ(W (o) ↓,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
h (p) ↓,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
+ b (o) ↓ ) u↓ =,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
tanh(W (u) ↓,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
h (p) ↓,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
+ b (u) ↓ ) c↓ = i↓ u↓ + f↓,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"c (p) ↓ h↓ = o↓ tanh(c↓)
(3)
where h(p)↓ and c (p) ↓ are the top-down TLSTM hidden state and memory cell of the parent of this node.",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"For the root node which has no parent, h↓ cannot be computed using the above equations.",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"Instead, we set h↓ to h↑ (the bottom-up TLSTM hidden state generated at the root node).",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"h↑ captures the semantics of all codes in this hierarchy, which is then propagated downwards to each individual code via the top-down TLSTM dynamics.
",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
We concatenate the hidden states of the two directions to obtain the bidirectional TLSTM encoding of each code h =,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
[h↑;h↓].,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"The bottom-up TLSTM composes the semantics of children (representing sub-diseases) and merge them into the current node, which hence captures child-to-parent relationship.",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"The top-down TLSTM makes each node inherit the semantics of its parent, which captures parent-to-child relation.",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"As a result, the hierarchical relationship among codes are encoded in the hidden states.
",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"For the diagnosis descriptions of a patient, we use an SLSTM network to encode each description individually.",4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
The weight parameters of this SLSTM are tied with those of the SLSTM used for encoding code descriptions.,4.2 Tree-of-Sequences LSTM Encoder,[0],[0]
"Next, we introduce how to map the DDs to codes.",4.3 Attentional Matching,[0],[0]
"We denote the hidden representations of DDs and codes as {hm}Mm=1 and {un}Nn=1 respectively, where M is the number of DDs of one patient and
N is the total number of codes in the dataset.",4.3 Attentional Matching,[0],[0]
The mapping from DDs to codes is not one-to-one.,4.3 Attentional Matching,[0],[0]
"In many cases, a code is assigned only when a certain combination of K (1 < K ≤ M ) diseases simultaneously appear within the M DDs and the value of K depends on this code.",4.3 Attentional Matching,[0],[0]
"Among the K diseases, their importance of determining the assignment of this code is different.",4.3 Attentional Matching,[0],[0]
"For the rest M −K DDs, we can consider their importance score to be zero.",4.3 Attentional Matching,[0],[0]
"We use a soft-attention mechanism (Bahdanau et al., 2014) to calculate these importance scores.",4.3 Attentional Matching,[0],[0]
"For a code un, the importance of a DD hm to un is calculated as anm = u>nhm.",4.3 Attentional Matching,[0],[0]
We normalize the scores {anm}Mm=1 of all DDs into a probabilistic simplex using the softmax operation: ãnm = exp(anm)/ ∑M l=1 exp(anl).,4.3 Attentional Matching,[0],[0]
"Given these normalized importance scores {ãnm}Mm=1, we use them to weight the representations of DDs and get a single attentional vector of the M DDs: ĥn = ∑M m=1 ãnmhm.",4.3 Attentional Matching,[0],[0]
"Then we concatenate ĥn and un, and use a linear classifier to predict the probability that code n should be assigned: pn = sigmoid(w>n",4.3 Attentional Matching,[0],[0]
"[ĥn;un] + bn), where the coefficients wn and bias bn are specific to code",4.3 Attentional Matching,[0],[0]
"n.
We train the weight parameters Θ of the proposed model using the data of L patient visits.",4.3 Attentional Matching,[0],[0]
"Θ includes the sequential LSTM weights Ws, tree LSTM weights Wt and weights Wp in the final prediction layer.",4.3 Attentional Matching,[0],[0]
Let c(l) ∈ RN be a binary vector where c(l)n = 1 if the n-th code is assigned to this patient and c(l)n = 0,4.3 Attentional Matching,[0],[0]
if otherwise.,4.3 Attentional Matching,[0],[0]
"Θ can be learned by minimizing the following prediction loss:
minΘ Lpred(Θ) = L∑",4.3 Attentional Matching,[0],[0]
l=1,4.3 Attentional Matching,[0],[0]
"N∑ n=1 CE(p(l)n , c (l) n )",4.3 Attentional Matching,[0],[0]
"(4)
where p(l)n is the predicted probability that code n is assigned to patient visit l and p(l)n is a function of Θ. CE(·, ·) is the cross-entropy loss.",4.3 Attentional Matching,[0],[0]
"We use an adversarial learning (Goodfellow et al., 2014) approach to reconcile the different writing styles of diagnosis descriptions (DDs) and code descriptions (CDs).",4.4 Adversarial Reconciliation of Writing Styles,[0],[0]
"The basic idea is: after encoded, if a description cannot be discerned to be a DD or a CD, then the difference in their writing styles is eliminated.",4.4 Adversarial Reconciliation of Writing Styles,[0],[0]
"We build a discriminative network which takes the encoding vector of a description as input and tries to identify it as a DD
or CD.",4.4 Adversarial Reconciliation of Writing Styles,[0],[0]
The encoders of DDs and CDs adjust their weight parameters so that such a discrimination is difficult to be achieved by the discriminative network.,4.4 Adversarial Reconciliation of Writing Styles,[0],[0]
"Consider all the descriptions {tr, yr}Rr=1 where tr is a description and yr is a binary label.",4.4 Adversarial Reconciliation of Writing Styles,[0],[0]
yr = 1 if tr is a DD and yr = 0,4.4 Adversarial Reconciliation of Writing Styles,[0],[0]
if otherwise.,4.4 Adversarial Reconciliation of Writing Styles,[0],[0]
Let f(tr;Ws) denote the sequential LSTM (SLSTM) encoder parameterized by Ws.,4.4 Adversarial Reconciliation of Writing Styles,[0],[0]
This encoder is shared by the DDs and CDs.,4.4 Adversarial Reconciliation of Writing Styles,[0],[0]
"Note that for CDs, a tree LSTM is further applied on top of the encodings produced by the SLSTM.",4.4 Adversarial Reconciliation of Writing Styles,[0],[0]
We use the SLSTM encoding vectors of CDs as the input of the discriminative network rather than using the TLSTM encodings since the latter are irrelevant to writing styles.,4.4 Adversarial Reconciliation of Writing Styles,[0],[0]
Let g(f(tr;Ws);Wd) denote the discriminative network parameterized by Wd.,4.4 Adversarial Reconciliation of Writing Styles,[0],[0]
It takes the encoding vector f(tr;Ws) as input and produces the probability that tr is a DD.,4.4 Adversarial Reconciliation of Writing Styles,[0],[0]
"Adversarial learning is performed by solving this problem:
",4.4 Adversarial Reconciliation of Writing Styles,[0],[0]
"max Ws min Wd Ladv = R∑ r=1 CE(g(f(tr;Ws);Wd), yr)
(5)",4.4 Adversarial Reconciliation of Writing Styles,[0],[0]
The discriminative network tries to differentiate DDs from CDs by minimizing this classification loss while the encoder maximizes this loss so that DDs and CDs are not distinguishable.,4.4 Adversarial Reconciliation of Writing Styles,[0],[0]
"Next, we incorporate the importance order among ICD codes.",4.5 Isotonic Constraints,[0],[0]
"For the D(l) codes assigned to patient l, without loss of generality, we assume the order is 1 2 · · · D(l) (the order is given by human coders as ground-truth in the MIMIC-III dataset).",4.5 Isotonic Constraints,[0],[0]
"We use the predicted probability pi (1 ≤ i ≤ D(l)) defined in Section 4.3 to characterize the importance of code i. To incorporate the order, we impose an isotonic constraint on the probabilities: p
(l) 1 p (l) 2 · · · p (l) D(l) , and solve the following
problem:
minΘ Lpred(Θ) + maxWd(−λLadv(Ws,Wd))",4.5 Isotonic Constraints,[0],[0]
"s.t. p
(l) 1 p (l) 2 · · · p (l)
D(l)
∀l = 1, · · · , L (6)
where the probabilities p(l)i are functions of Θ and λ is a tradeoff parameter.
",4.5 Isotonic Constraints,[0],[0]
"We develop an algorithm based on the alternating direction method of multiplier (ADMM) (Boyd et al., 2011) to solve the problem defined in Eq.(6).",4.5 Isotonic Constraints,[0],[0]
"Let p(l) be a |D(l)|-dimensional
vector where the i-th element is p(l)i .",4.5 Isotonic Constraints,[0],[0]
"We first write the problem into an equivalent form
minΘ Lpred(Θ)",4.5 Isotonic Constraints,[0],[0]
"+ maxWd(−λLadv(Ws,Wd))",4.5 Isotonic Constraints,[0],[0]
"s.t. p(l) = q(l)
q",4.5 Isotonic Constraints,[0],[0]
"(l) 1 q (l) 2 · · · q (l) |D(l)| ∀l = 1, · · · , L
(7) Then we write down the augmented Lagrangian
min Θ,q,v Lpred(Θ)",4.5 Isotonic Constraints,[0],[0]
"+ maxWd(−λLadv(Ws,Wd))
",4.5 Isotonic Constraints,[0],[0]
+〈p(l),4.5 Isotonic Constraints,[0],[0]
"− q(l),v(l)〉+ ρ2‖p (l)",4.5 Isotonic Constraints,[0],[0]
"− q(l)‖22
s.t. q",4.5 Isotonic Constraints,[0],[0]
"(l) 1 q (l) 2 · · · q (l)
|D(l)| ∀l = 1, · · · , L
(8) We solve this problem by alternating between {p(l)}Ll=1, {q(l)}Ll=1 and {v(l)}Ll=1 The subproblem defined over q(l) is
minq(l) −〈q(l),v(l)〉+ ρ 2‖p (l)",4.5 Isotonic Constraints,[0],[0]
"− q(l)‖22 s.t. q
(l) 1 q (l) 2 · · · q (l) |D(l)| (9)
which is an isotonic projection problem and can be solved via the algorithm proposed in (Yu and Xing, 2016).",4.5 Isotonic Constraints,[0],[0]
"With {q(l)}Ll=1 and {v(l)}Ll=1 fixed, the sub-problem is minΘ Lpred(Θ) + maxWd(−λLadv(Ws,Wd))",4.5 Isotonic Constraints,[0],[0]
which can be solved using stochastic gradient descent (SGD).,4.5 Isotonic Constraints,[0],[0]
The update of v(l) is simple: v(l) = v(l) + ρ(p(l)−q(l)).,4.5 Isotonic Constraints,[0],[0]
"In this section, we present experiment results.",5 Experiments,[0],[0]
"Out of the 6,984 unique codes, we selected 2,833 codes that have the top frequencies to perform the study.",5.1 Experimental Settings,[0],[0]
We split the data into a train/validation/test dataset with 40k/7k/12k patient visits respectively.,5.1 Experimental Settings,[0],[0]
The hyperparameters were tuned on the validation set.,5.1 Experimental Settings,[0],[0]
"The SLSTMs were bidirectional and dropout with 0.5 probability (Srivastava et al., 2014) was used.",5.1 Experimental Settings,[0],[0]
The size of hidden states in all LSTMs was set to 100.,5.1 Experimental Settings,[0],[0]
The word embeddings were trained on the fly and their dimension was set to 200.,5.1 Experimental Settings,[0],[0]
The tradeoff parameter λ was set to 0.1.,5.1 Experimental Settings,[0],[0]
The parameter ρ in the ADMM algorithm was set to 1.,5.1 Experimental Settings,[0],[0]
"In the SGD algorithm for solving minΘ Lpred(Θ)+maxWd(−λLadv(Ws,Wd)), we used the ADAM (Kingma and Ba, 2014) optimizer with an initial learning rate 0.001 and a minibatch size 20.",5.1 Experimental Settings,[0],[0]
"Sensitivity (true positive rate) and
specificity (true negative rate) were used to evaluate the code assignment performance.",5.1 Experimental Settings,[0],[0]
"We calculated these two scores for each individual code on the test set, then took a weighted (proportional to codes’ frequencies) average across all codes.",5.1 Experimental Settings,[0],[0]
"To evaluate the ranking performance of codes, we used normalized discounted cumulative gain (NDCG) (Järvelin and Kekäläinen, 2002).",5.1 Experimental Settings,[0],[0]
We perform ablation study to verify the effectiveness of each module in our model.,5.2 Ablation Study,[0],[0]
"To evaluate module X, we remove it from the model without changing other modules and denote such a baseline by No-X. The comparisons of No-X with the full model are given in Table 2.
",5.2 Ablation Study,[0],[0]
"Tree-of-sequences LSTM To evaluate this module, we compared with the two configurations: (1) No-TLSTM, which removes the tree LSTM and directly uses the hidden states produced by the sequential LSTM as final representations of codes; (2) Bottom-up TLSTM, which removes the hidden states generated by the top-down TLSTM.",5.2 Ablation Study,[0],[0]
"In addition, we compared with four hierarchical classification baselines including (1) hierarchical network (HierNet) (Yan et al., 2015), (2) HybridNet (Hou et al., 2017), (3) branch network (BranchNet) (Zhu and Bain, 2017), (4) label embedding tree (LET) (Bengio et al., 2010), by using them to replace the bidirectional tree LSTM while keeping other modules untouched.",5.2 Ablation Study,[0],[0]
Table 2 shows the average sensitivity and specificity scores achieved by these methods on the test set.,5.2 Ablation Study,[0],[0]
We make the following observations.,5.2 Ablation Study,[0],[0]
"First, removing tree LSTM largely degrades performance: the sensitivity and specificity of No-TLSTM is 0.23 and 0.28 respectively while our full model (which uses bidirectional TLSTM) achieves 0.29 and 0.33 respectively.",5.2 Ablation Study,[0],[0]
The reason is No-TLSTM ignores the hierarchical relationship among codes.,5.2 Ablation Study,[0],[0]
"Second, bottom-up tree LSTM alone performs less well than bidirectional tree LSTM.",5.2 Ablation Study,[0],[0]
"This demonstrates the necessity of the top-down TLSTM, which ensures every two codes are connected by directed paths and can more expressively capture code-relations in the hierarchy.",5.2 Ablation Study,[0],[0]
"Third, our method outperforms the four baselines.",5.2 Ablation Study,[0],[0]
"The possible reason is our method directly builds codes’ hierarchical relationship into their representations while the baselines perform representation-learning and relationship-capturing
separately.
",5.2 Ablation Study,[0],[0]
"Next, we present some qualitative results.",5.2 Ablation Study,[0],[0]
"For a patient (admission ID 147798) having a DD ‘E Coli urinary tract infection’, without using tree LSTM, two sibling codes 585.2 (chronic kidney disease, stage II (mild)) – which is the groundtruth – and 585.4 (chronic kidney disease, stage IV (severe)) are simultaneously assigned possibly because their textual descriptions are very similar (only differ in the level of severity).",5.2 Ablation Study,[0],[0]
This is incorrect because 585.2 and 585.4 are the children of 585 (chronic kidney disease) and the severity level of this disease cannot simultaneously be mild and severe.,5.2 Ablation Study,[0],[0]
"After tree LSTM is added, the false prediction of 585.4 is eliminated, which demonstrates the effectiveness of tree LSTM in incorporating one constraint induced by the code hierarchy: among the nodes sharing the same parent, only one should be selected.
",5.2 Ablation Study,[0],[0]
"For patient 197205, No-TLSTM assigns the following codes: 462 (subacute sclerosing panencephalitis), 790.29 (other abnormal glucose), 799.9 (unspecified viral infection), and 285.21 (anemia in chronic kidney disease).",5.2 Ablation Study,[0],[0]
"Among these codes, the first three are ground-truth and the fourth one is incorrect (the ground-truth is 401.9 (unspecified essential hypertension)).",5.2 Ablation Study,[0],[0]
Adding tree LSTM fixes this error.,5.2 Ablation Study,[0],[0]
The average distance between 401.9 and the rest of ground-truth codes is 6.2.,5.2 Ablation Study,[0],[0]
"For the incorrectly assigned code 285.21, such a distance is 7.9.",5.2 Ablation Study,[0],[0]
"This demonstrates that tree LSTM is able to capture another constraint imposed by the hierarchy: codes with smaller treedistance are more likely to be assigned together.
",5.2 Ablation Study,[0],[0]
"Adversarial learning To evaluate the efficacy of adversarial learning (AL), we remove it from the full model and refer to this baseline as No-AL.",5.2 Ablation Study,[0],[0]
"Specifically, in Eq.(6), the loss term maxWd(−Ladv(Ws,Wd)) is taken away.",5.2 Ablation Study,[0],[0]
"Table 2 shows the results, from which we observe that after AL is removed, the sensitivity and specificity are dropped from 0.29 and 0.33 to 0.26 and 0.31 respectively.",5.2 Ablation Study,[0],[0]
No-AL does not reconcile different writing styles of diagnosis descriptions (DDs) and code descriptions (CDs).,5.2 Ablation Study,[0],[0]
"As a result, a DD and a CD that have similar semantics may be mismatched because their writing styles are different.",5.2 Ablation Study,[0],[0]
"For example, a patient (admission ID 147583) has a DD ‘h/o DVT on anticoagulation’, which contains abbreviation DVT (deep vein thrombosis).",5.2 Ablation Study,[0],[0]
"Due to the presence of this abbreviation, it is difficult to assign a proper code to this DD since the textual descriptions of codes do not contain abbreviations.",5.2 Ablation Study,[0],[0]
"With adversarial learning, our model can correctly map this DD to a ground-truth code: 443.9 (peripheral vascular disease, unspecified).",5.2 Ablation Study,[0],[0]
"Without AL, this code is not selected.",5.2 Ablation Study,[0],[0]
"As another example, a DD ‘coronary artery disease, STEMI, s/p 2 stents placed in RCA’ was given to patient 148532.",5.2 Ablation Study,[0],[0]
"This DD is written informally and ungrammatically, and contains too much detailed information, e.g., ‘s/p 2 stents placed in RCA’.",5.2 Ablation Study,[0],[0]
Such a writing style is quite different from that of CDs.,5.2 Ablation Study,[0],[0]
"With AL, our model successfully matches this DD to a ground-truth code: 414.01 (coronary atherosclerosis of native coronary artery).",5.2 Ablation Study,[0],[0]
"On the contrary, No-AL fails to achieve this.
",5.2 Ablation Study,[0],[0]
"Isotonic constraint (IC) To evaluate this ingredient, we remove the ICs from Eq.(6) during training and denote this baseline as No-IC.",5.2 Ablation Study,[0],[0]
"We use NDCG to measure the ranking performance, which is calculated in the following way.",5.2 Ablation Study,[0],[0]
Consider a testing patient-visit lwhere the ground-truth ICD codes are M(l).,5.2 Ablation Study,[0],[0]
"For any code c, we define the relevance score of c to l as 0 if c /∈ M(l) and as |M(l)| − r(c) if otherwise, where r(c) is the ground-truth rank of c inM(l).",5.2 Ablation Study,[0],[0]
"We rank codes in descending order of their corresponding prediction
probabilities and obtain the predicted rank for each code.",5.2 Ablation Study,[0],[0]
"We calculate the NDCG scores at position 2, 4, 6, 8 based on the relevance scores and predicted ranks, which are shown in Table 3.",5.2 Ablation Study,[0],[0]
"As can be seen, using IC achieves much higher NDCG than NoIC, which demonstrates the effectiveness of IC in capturing the importance order among codes.
",5.2 Ablation Study,[0],[0]
We also evaluate how IC affects the sensitivity and specificity of code assignment.,5.2 Ablation Study,[0],[0]
"As can be seen from Table 2, No-IC degrades the two scores from 0.29 and 0.33 to 0.24 and 0.29 respectively, which indicates that IC is helpful in training a model that can more correctly assign codes.",5.2 Ablation Study,[0],[0]
"This is because IC encourages codes that are highly relevant to the patients to be ranked at top positions, which prevents the selection of irrelevant codes.
",5.2 Ablation Study,[0],[0]
Attentional matching (AM),5.2 Ablation Study,[0],[0]
"In the evaluation of this module, we compare with a baseline – No-AM, which performs an unweighted average of the M DDs:",5.2 Ablation Study,[0],[0]
ĥn = 1M ∑M m=1,5.2 Ablation Study,[0],[0]
"hm, concatenates ĥn with un and feeds the concatenated vector into the final prediction layer.",5.2 Ablation Study,[0],[0]
"From Table 2, we can see our full model (with AM) outperforms No-AM, which demonstrates the effectiveness of attentional matching.",5.2 Ablation Study,[0],[0]
"In determining whether a code should be assigned, different DDs have different importance weights.",5.2 Ablation Study,[0],[0]
"No-AM ignores such weights, therefore performing less well.
",5.2 Ablation Study,[0],[0]
AM can correctly perform many-to-one mapping from multiple DDs to a CD.,5.2 Ablation Study,[0],[0]
"For example, patient 190236 was given two DDs: ‘renal insufficiency’ and ‘acute renal failure’.",5.2 Ablation Study,[0],[0]
"AM maps them to a combined ICD code: 403.91 (hypertensive chronic kidney disease, unspecified, with chronic kidney disease stage V or end stage renal disease), which is in the ground-truth provided by medical coders.",5.2 Ablation Study,[0],[0]
"On the contrary, No-AM fails to assign this code.",5.2 Ablation Study,[0],[0]
"On the other hand, AM is able to correctly map a DD to multiple CDs.",5.2 Ablation Study,[0],[0]
"For example, a DD ‘congestive heart failure, diastolic’ was given to patient 140851.",5.2 Ablation Study,[0],[0]
"AM successfully maps this DD to two codes: (1) 428.0 (congestive heart failure, unspecified); (2) 428.30 (diastolic heart failure, unspecified).",5.2 Ablation Study,[0],[0]
"Without AM, this DD is mapped only to 428.0.",5.2 Ablation Study,[0],[0]
"In addition to evaluating the four modules individually, we also compared our full model with four other baselines proposed by (Larkey and Croft,
1996; Franz et al., 2000; Pestian et al., 2007; Kavuluru et al., 2013, 2015; Koopman et al., 2015) for ICD coding.",5.3 Holistic Comparison with Other Baselines,[0],[0]
Table 2 shows the results.,5.3 Holistic Comparison with Other Baselines,[0],[0]
"As can be seen, our approach achieves much better sensitivity and specificity scores.",5.3 Holistic Comparison with Other Baselines,[0],[0]
The reason that our model works better is two-fold.,5.3 Holistic Comparison with Other Baselines,[0],[0]
"First, our model is based on deep neural network, which has arguably better modeling power than linear methods used in the baselines.",5.3 Holistic Comparison with Other Baselines,[0],[0]
"Second, our model is able to capture the hierarchical relationship and importance order among codes, can alleviate the discrepancy in writing styles and allows flexible many-toone and one-to-many mappings from DDs to CDs.",5.3 Holistic Comparison with Other Baselines,[0],[0]
These merits are not possessed by the baselines.,5.3 Holistic Comparison with Other Baselines,[0],[0]
"In this paper, we build a neural network model for automated ICD coding.",6 Conclusions and Discussions,[0],[0]
Evaluations on the MIMIC-III dataset demonstrate the following.,6 Conclusions and Discussions,[0],[0]
"First, the tree-of-sequences LSTM network effectively discourages the co-selection of sibling codes and promotes the co-assignment of clinicallyrelevant codes.",6 Conclusions and Discussions,[0],[0]
Adversarial learning improves the matching accuracy by alleviating the discrepancy among the writing styles of DDs and CDs.,6 Conclusions and Discussions,[0],[0]
"Third, isotonic constraints promote the correct ranking of codes.",6 Conclusions and Discussions,[0],[0]
"Fourth, the attentional matching mechanism is able to perform many-to-one and one-tomany mappings.
",6 Conclusions and Discussions,[0],[0]
"In the coding practice of human coders, in addition to the diagnosis descriptions, other information contained in nursing notes, lab values, and medical procedures are also leveraged for code assignment.",6 Conclusions and Discussions,[0],[0]
We have initiated preliminary investigation along this line and added two new input sources: (1) the rest of discharge summary and (2) lab values.,6 Conclusions and Discussions,[0],[0]
The sensitivity is improved from 0.29 to 0.32 and the specificity is improved from 0.33 to 0.35.,6 Conclusions and Discussions,[0],[0]
"A full study is ongoing.
",6 Conclusions and Discussions,[0],[0]
"At present, the major limitations of this work include: (1) it does not perform well on infrequent codes; (2) it is less capable of dealing with abbreviations.",6 Conclusions and Discussions,[0],[0]
"We will address these two issues in future by investigating diversity-promoting regularization (Xie et al., 2017) and leveraging an external knowledge base that maps medical abbreviations into their full names.
",6 Conclusions and Discussions,[0],[0]
The proposed methods can be applied to other tasks in NLP.,6 Conclusions and Discussions,[0],[0]
The tree-of-sequences model can be applied for ontology annotation.,6 Conclusions and Discussions,[0],[0]
"It takes the textual descriptions of concepts in the ontology
and their hierarchical structure as inputs and produces a latent representation for each concept.",6 Conclusions and Discussions,[0],[0]
The representations can simultaneously capture the semantics of codes and their relationships.,6 Conclusions and Discussions,[0],[0]
The proposed adversarial reconciliation of writing styles and attentional matching can be applied for knowledge mapping or entity linking.,6 Conclusions and Discussions,[0],[0]
"For example, in tweets, we can use the method to map an informally written mention ‘nbcbightlynews’ to a canonical entity ‘NBC Nightly News’ in the knowledge base.",6 Conclusions and Discussions,[0],[0]
We would like to thank the anonymous reviewers for their very constructive and helpful comments and suggestions.,Acknowledgements,[0],[0]
"Pengtao Xie and Eric P. Xing are supported by National Institutes of Health P30DA035778, Pennsylvania Department of Health BD4BH4100070287, and National Science Foundation IIS1617583.",Acknowledgements,[0],[0]
The International Classification of Diseases (ICD) provides a hierarchy of diagnostic codes for classifying diseases.,abstractText,[0],[0]
Medical coding – which assigns a subset of ICD codes to a patient visit – is a mandatory process that is crucial for patient care and billing.,abstractText,[0],[0]
"Manual coding is time-consuming, expensive, and errorprone.",abstractText,[0],[0]
"In this paper, we build a neural architecture for automated coding.",abstractText,[0],[0]
It takes the diagnosis descriptions (DDs) of a patient as inputs and selects the most relevant ICD codes.,abstractText,[0],[0]
"This architecture contains four major ingredients: (1) tree-ofsequences LSTM encoding of code descriptions (CDs), (2) adversarial learning for reconciling the different writing styles of DDs and CDs, (3) isotonic constraints for incorporating the importance order among the assigned codes, and (4) attentional matching for performing many-toone and one-to-many mappings from DDs to CDs.",abstractText,[0],[0]
We demonstrate the effectiveness of the proposed methods on a clinical datasets with 59K patient visits.,abstractText,[0],[0]
A Neural Architecture for Automated ICD Coding,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 379–389, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
Summarization is an important challenge of natural language understanding.,1 Introduction,[0],[0]
The aim is to produce a condensed representation of an input text that captures the core meaning of the original.,1 Introduction,[0],[0]
Most successful summarization systems utilize extractive approaches that crop out and stitch together portions of the text to produce a condensed version.,1 Introduction,[0],[0]
"In contrast, abstractive summarization attempts to produce a bottom-up summary, aspects of which may not appear as part of the original.
",1 Introduction,[0],[0]
We focus on the task of sentence-level summarization.,1 Introduction,[0],[0]
"While much work on this task has looked at deletion-based sentence compression techniques (Knight and Marcu (2002), among many others), studies of human summarizers show that it is common to apply various other operations while condensing, such as paraphrasing, generalization, and reordering (Jing, 2002).",1 Introduction,[0],[0]
"Past work has modeled this abstractive summarization problem either using linguistically-inspired constraints (Dorr et al., 2003; Zajic et al., 2004) or with syntactic transformations of the input text (Cohn and
Lapata, 2008; Woodsend et al., 2010).",1 Introduction,[0],[0]
"These approaches are described in more detail in Section 6.
",1 Introduction,[0],[0]
We instead explore a fully data-driven approach for generating abstractive summaries.,1 Introduction,[0],[0]
"Inspired by the recent success of neural machine translation, we combine a neural language model with a contextual input encoder.",1 Introduction,[0],[0]
Our encoder is modeled off of the attention-based encoder of Bahdanau et al. (2014) in that it learns a latent soft alignment over the input text to help inform the summary (as shown in Figure 1).,1 Introduction,[0],[0]
Crucially both the encoder and the generation model are trained jointly on the sentence summarization task.,1 Introduction,[0],[0]
The model is described in detail in Section 3.,1 Introduction,[0],[0]
"Our model also incorporates a beam-search decoder as well as additional features to model extractive elements; these aspects are discussed in Sections 4 and 5.
",1 Introduction,[0],[0]
"This approach to summarization, which we call Attention-Based Summarization (ABS), incorporates less linguistic structure than comparable abstractive summarization approaches, but can easily
379
scale to train on a large amount of data.",1 Introduction,[0],[0]
Since our system makes no assumptions about the vocabulary of the generated summary it can be trained directly on any document-summary pair.1,1 Introduction,[0],[0]
"This allows us to train a summarization model for headline-generation on a corpus of article pairs from Gigaword (Graff et al., 2003) consisting of around 4 million articles.",1 Introduction,[0],[0]
"An example of generation is given in Figure 2, and we discuss the details of this task in Section 7.
To test the effectiveness of this approach we run extensive comparisons with multiple abstractive and extractive baselines, including traditional syntax-based systems, integer linear programconstrained systems, information-retrieval style approaches, as well as statistical phrase-based machine translation.",1 Introduction,[0],[0]
Section 8 describes the results of these experiments.,1 Introduction,[0],[0]
Our approach outperforms a machine translation system trained on the same large-scale dataset and yields a large improvement over the highest scoring system in the DUC-2004 competition.,1 Introduction,[0],[0]
We begin by defining the sentence summarization task.,2 Background,[0],[0]
"Given an input sentence, the goal is to produce a condensed summary.",2 Background,[0],[0]
"Let the input consist of a sequence of M words x1, . . .",2 Background,[0],[0]
",xM coming from a fixed vocabulary V of size |V| = V .",2 Background,[0],[0]
"We will represent each word as an indicator vector xi ∈ {0, 1}V for i ∈ {1, . . .",2 Background,[0],[0]
",M}, sentences as a sequence of indicators, and X as the set of possible inputs.",2 Background,[0],[0]
"Furthermore define the notation x[i,j,k] to indicate the sub-sequence of elements i, j, k.
A summarizer takes x as input and outputs a shortened sentence y of length N < M .",2 Background,[0],[0]
"We will assume that the words in the summary also come from the same vocabulary V and that the output is
1In contrast to a large-scale sentence compression systems like Filippova and Altun (2013) which require monotonic aligned compressions.
",2 Background,[0],[0]
"a sequence y1, . .",2 Background,[0],[0]
.,2 Background,[0],[0]
",yN .",2 Background,[0],[0]
"Note that in contrast to related tasks, like machine translation, we will assume that the output length N is fixed, and that the system knows the length of the summary before generation.2
Next consider the problem of generating summaries.",2 Background,[0],[0]
"Define the set Y ⊂ ({0, 1}V , . . .",2 Background,[0],[0]
", {0, 1}V )",2 Background,[0],[0]
"as all possible sentences of length N , i.e. for all i and y ∈ Y , yi is an indicator.",2 Background,[0],[0]
"We say a system is abstractive if it tries to find the optimal sequence from this set Y ,
arg max y∈Y s(x,y), (1)
under a scoring function s : X ×Y 7→ R. Contrast this to a fully extractive sentence summary3 which transfers words from the input:
arg max m∈{1,...M}N s(x,x[m1,...,mN ]), (2)
or to the related problem of sentence compression that concentrates on deleting words from the input:
arg max m∈{1,...",2 Background,[0],[0]
"M}N ,mi−1<mi s(x,x[m1,...,mN ]).",2 Background,[0],[0]
"(3)
While abstractive summarization poses a more difficult generation challenge, the lack of hard constraints gives the system more freedom in generation and allows it to fit with a wider range of training data.
",2 Background,[0],[0]
"In this work we focus on factored scoring functions, s, that take into account a fixed window of previous words:
s(x,y)",2 Background,[0],[0]
"≈ N−1∑ i=0 g(yi+1,x,yc), (4)
2For",2 Background,[0],[0]
"the DUC-2004 evaluation, it is actually the number of bytes of the output that is capped.",2 Background,[0],[0]
"More detail is given in Section 7.
",2 Background,[0],[0]
3Unfortunately the literature is inconsistent on the formal definition of this distinction.,2 Background,[0],[0]
"Some systems self-described as abstractive would be extractive under our definition.
where we define yc , y[i−C+1,...,i] for a window of size C.
In particular consider the conditional logprobability of a summary given the input, s(x,y) = log p(y|x; θ).",2 Background,[0],[0]
"We can write this as:
log p(y|x; θ) ≈ N−1∑ i=0 log p(yi+1|x,yc; θ),
where we make a Markov assumption on the length of the context as size C and assume for i < 1, yi is a special start symbol 〈S〉.
",2 Background,[0],[0]
"With this scoring function in mind, our main focus will be on modelling the local conditional distribution: p(yi+1|x,yc; θ).",2 Background,[0],[0]
"The next section defines a parameterization for this distribution, in Section 4, we return to the question of generation for factored models, and in Section 5 we introduce a modified factored scoring function.",2 Background,[0],[0]
"The distribution of interest, p(yi+1|x,yc; θ), is a conditional language model based on the input sentence",3 Model,[0],[0]
"x. Past work on summarization and compression has used a noisy-channel approach to split and independently estimate a language model and a conditional summarization model (Banko et al., 2000; Knight and Marcu, 2002; Daumé III and Marcu, 2002), i.e.,
arg max y log p(y|x) = arg max y log p(y)p(x|y)
where p(y) and p(x|y) are estimated separately.",3 Model,[0],[0]
Here we instead follow work in neural machine translation and directly parameterize the original distribution as a neural network.,3 Model,[0],[0]
The network contains both a neural probabilistic language model and an encoder which acts as a conditional summarization model.,3 Model,[0],[0]
The core of our parameterization is a language model for estimating the contextual probability of the next word.,3.1 Neural Language Model,[0],[0]
"The language model is adapted from a standard feed-forward neural network language model (NNLM), particularly the class of NNLMs described by Bengio et al. (2003).",3.1 Neural Language Model,[0],[0]
"The full model is:
p(yi+1|yc,x; θ) ∝",3.1 Neural Language Model,[0],[0]
"exp(Vh + Wenc(x,yc)), ỹc =",3.1 Neural Language Model,[0],[0]
"[Eyi−C+1, . . .",3.1 Neural Language Model,[0],[0]
",Eyi], h = tanh(Uỹc).
",3.1 Neural Language Model,[0],[0]
"The parameters are θ = (E,U,V,W) where E ∈ RD×V is a word embedding matrix, U ∈ R(CD)×H , V ∈ RV×H , W ∈ RV×H are weight matrices,4 D is the size of the word embeddings, and h is a hidden layer of size H .",3.1 Neural Language Model,[0],[0]
"The black-box function enc is a contextual encoder term that returns a vector of size H representing the input and current context; we consider several possible variants, described subsequently.",3.1 Neural Language Model,[0],[0]
Figure 3a gives a schematic representation of the decoder architecture.,3.1 Neural Language Model,[0],[0]
Note that without the encoder term this represents a standard language model.,3.2 Encoders,[0],[0]
By incorporating in enc and training the two elements jointly we crucially can incorporate the input text into generation.,3.2 Encoders,[0],[0]
"We discuss next several possible instantiations of the encoder.
",3.2 Encoders,[0],[0]
"Bag-of-Words Encoder Our most basic model simply uses the bag-of-words of the input sentence embedded down to size H , while ignoring properties of the original order or relationships between neighboring words.",3.2 Encoders,[0],[0]
"We write this model as:
enc1(x,yc) = p>x̃, p =",3.2 Encoders,[0],[0]
"[1/M, . . .",3.2 Encoders,[0],[0]
", 1/M ], x̃ =",3.2 Encoders,[0],[0]
"[Fx1, . . .",3.2 Encoders,[0],[0]
",FxM ].
Where the input-side embedding matrix F ∈ RH×V is the only new parameter of the encoder and p ∈",3.2 Encoders,[0],[0]
"[0, 1]M is a uniform distribution over the input words.
",3.2 Encoders,[0],[0]
"4Each of the weight matrices U, V, W also has a corresponding bias term.",3.2 Encoders,[0],[0]
"For readability, we omit these terms throughout the paper.
",3.2 Encoders,[0],[0]
For summarization this model can capture the relative importance of words to distinguish content words from stop words or embellishments.,3.2 Encoders,[0],[0]
"Potentially the model can also learn to combine words; although it is inherently limited in representing contiguous phrases.
",3.2 Encoders,[0],[0]
Convolutional Encoder To address some of the modelling issues with bag-of-words we also consider using a deep convolutional encoder for the input sentence.,3.2 Encoders,[0],[0]
"This architecture improves on the bag-of-words model by allowing local interactions between words while also not requiring the context yc while encoding the input.
",3.2 Encoders,[0],[0]
"We utilize a standard time-delay neural network (TDNN) architecture, alternating between temporal convolution layers and max pooling layers.
∀j, enc2(x,yc)j = max i x̃Li,j , (5) ∀i, l ∈ {1, . . .",3.2 Encoders,[0],[0]
"L}, x̃lj = tanh(max{x̄l2i−1, x̄l2i}), (6) ∀i, l ∈ {1, . . .",3.2 Encoders,[0],[0]
"L}, x̄li = Qlx̃l−1[i−Q,...,i+Q], (7) x̃0 =",3.2 Encoders,[0],[0]
"[Fx1, . . .",3.2 Encoders,[0],[0]
",FxM ]. (8)
Where F is a word embedding matrix and QL×H×2Q+1 consists of a set of filters for each layer {1, . . .",3.2 Encoders,[0],[0]
L}.,3.2 Encoders,[0],[0]
"Eq. 7 is a temporal (1D) convolution layer, Eq. 6 consists of a 2-element temporal max pooling layer and a pointwise non-linearity, and final output Eq. 5 is a max over time.",3.2 Encoders,[0],[0]
At each layer x̃ is one half the size of x̄.,3.2 Encoders,[0],[0]
"For simplicity we assume that the convolution is padded at the boundaries, and that M is greater than 2L so that the dimensions are well-defined.
",3.2 Encoders,[0],[0]
"Attention-Based Encoder While the convolutional encoder has richer capacity than bag-ofwords, it still is required to produce a single representation for the entire input sentence.",3.2 Encoders,[0],[0]
A similar issue in machine translation inspired Bahdanau et al. (2014) to instead utilize an attention-based contextual encoder that constructs a representation based on the generation context.,3.2 Encoders,[0],[0]
"Here we note that if we exploit this context, we can actually use a rather simple model similar to bag-of-words:
enc3(x,yc) = p>x̄, p ∝ exp(x̃Pỹ′c), x̃ =",3.2 Encoders,[0],[0]
"[Fx1, . . .",3.2 Encoders,[0],[0]
",FxM ],
ỹ′c =",3.2 Encoders,[0],[0]
"[Gyi−C+1, . . .",3.2 Encoders,[0],[0]
",Gyi],
∀i x̄i = i+Q∑
q=i−Q x̃i/Q.
Where G ∈ RD×V is an embedding of the context, P ∈ RH×(CD) is a new weight matrix parameter mapping between the context embedding and input embedding, and Q is a smoothing window.",3.2 Encoders,[0],[0]
"The full model is shown in Figure 3b.
",3.2 Encoders,[0],[0]
"Informally we can think of this model as simply replacing the uniform distribution in bag-of-words with a learned soft alignment, P, between the input and the summary.",3.2 Encoders,[0],[0]
Figure 1 shows an example of this distribution p as a summary is generated.,3.2 Encoders,[0],[0]
The soft alignment is then used to weight the smoothed version of the input x̄ when constructing the representation.,3.2 Encoders,[0],[0]
"For instance if the current context aligns well with position i then the words xi−Q, . . .",3.2 Encoders,[0],[0]
",xi+Q are highly weighted by the encoder.",3.2 Encoders,[0],[0]
"Together with the NNLM, this model can be seen as a stripped-down version of the attention-based neural machine translation model.5",3.2 Encoders,[0],[0]
The lack of generation constraints makes it possible to train the model on arbitrary input-output pairs.,3.3 Training,[0],[0]
"Once we have defined the local conditional model, p(yi+1|x,yc; θ), we can estimate the parameters to minimize the negative loglikelihood of a set of summaries.",3.3 Training,[0],[0]
"Define this training set as consisting of J input-summary pairs (x(1),y(1)), . . .",3.3 Training,[0],[0]
", (x(J),y(J)).",3.3 Training,[0],[0]
"The negative loglikelihood conveniently factors6 into a term for each token in the summary:
NLL(θ) =",3.3 Training,[0],[0]
"− J∑
j=1
log p(y(j)|x(j); θ),
=",3.3 Training,[0],[0]
"− J∑
j=1 N−1∑ i=1",3.3 Training,[0],[0]
"log p(y (j) i+1|x(j),yc; θ).
",3.3 Training,[0],[0]
We minimize NLL by using mini-batch stochastic gradient descent.,3.3 Training,[0],[0]
"The details are described further in Section 7.
",3.3 Training,[0],[0]
"5To be explicit, compared to Bahdanau et al. (2014)",3.3 Training,[0],[0]
"our model uses an NNLM instead of a target-side LSTM, source-side windowed averaging instead of a source-side bidirectional RNN, and a weighted dot-product for alignment instead of an alignment MLP.
",3.3 Training,[0],[0]
6This is dependent on using the gold standard contexts yc.,3.3 Training,[0],[0]
An alternative is to use the predicted context within a structured or reenforcement-learning style objective.,3.3 Training,[0],[0]
We now return to the problem of generating summaries.,4 Generating Summaries,[0],[0]
"Recall from Eq. 4 that our goal is to find,
y∗ = arg max y∈Y N−1∑ i=0 g(yi+1,x,yc).
",4 Generating Summaries,[0],[0]
"Unlike phrase-based machine translation where inference is NP-hard, it actually is tractable in theory to compute y∗.",4 Generating Summaries,[0],[0]
"Since there is no explicit hard alignment constraint, Viterbi decoding can be applied and requires O(NV C) time to find an exact solution.",4 Generating Summaries,[0],[0]
"In practice though V is large enough to make this difficult.
",4 Generating Summaries,[0],[0]
An alternative approach is to approximate the arg max with a strictly greedy or deterministic decoder.,4 Generating Summaries,[0],[0]
"While decoders of this form can produce very bad approximations, they have shown to be relatively effective and fast for neural MT models (Sutskever et al., 2014).
",4 Generating Summaries,[0],[0]
A compromise between exact and greedy decoding is to use a beam-search decoder (Algorithm 1) which maintains the full vocabulary V while limiting itself to K potential hypotheses at each position of the summary.,4 Generating Summaries,[0],[0]
"The beam-search algorithm is shown here:
Algorithm 1 Beam Search Input: Parameters θ, beam size K, input x Output: Approx.",4 Generating Summaries,[0],[0]
K-best summaries π[0]← { } S = V if abstractive else {xi | ∀i} for i = 0 to N,4 Generating Summaries,[0],[0]
"− 1 do
.",4 Generating Summaries,[0],[0]
Generate Hypotheses N ←,4 Generating Summaries,[0],[0]
"{[y,yi+1]",4 Generating Summaries,[0],[0]
| y ∈,4 Generating Summaries,[0],[0]
"π[i],yi+1 ∈ S} .",4 Generating Summaries,[0],[0]
"Hypothesis Recombination
H ← { y ∈ N | s(y,x) > s(y′,x) ∀y′",4 Generating Summaries,[0],[0]
∈ N s.t.,4 Generating Summaries,[0],[0]
"yc = y′c }
.",4 Generating Summaries,[0],[0]
"Filter K-Max π[i+ 1]← K-arg max
y∈H",4 Generating Summaries,[0],[0]
"g(yi+1,yc,x) +",4 Generating Summaries,[0],[0]
"s(y,x)
end for return π[N ]
As with Viterbi this beam search algorithm is much simpler than beam search for phrase-based MT.",4 Generating Summaries,[0],[0]
Because there is no explicit constraint that each source word be used exactly once there is no need to maintain a bit set,4 Generating Summaries,[0],[0]
and we can simply move from left-to-right generating words.,4 Generating Summaries,[0],[0]
The beam search algorithm requires O(KNV ) time.,4 Generating Summaries,[0],[0]
"From a computational perspective though, each round of beam search is dominated by computing p(yi|x,yc) for each of the K hypotheses.",4 Generating Summaries,[0],[0]
"These
can be computed as a mini-batch, which in practice greatly reduces the factor of K.",4 Generating Summaries,[0],[0]
"While we will see that the attention-based model is effective at generating summaries, it does miss an important aspect seen in the human-generated references.",5 Extension: Extractive Tuning,[0],[0]
"In particular the abstractive model does not have the capacity to find extractive word matches when necessary, for example transferring unseen proper noun phrases from the input.",5 Extension: Extractive Tuning,[0],[0]
"Similar issues have also been observed in neural translation models particularly in terms of translating rare words (Luong et al., 2014).
",5 Extension: Extractive Tuning,[0],[0]
To address this issue we experiment with tuning a very small set of additional features that tradeoff the abstractive/extractive tendency of the system.,5 Extension: Extractive Tuning,[0],[0]
"We do this by modifying our scoring function to directly estimate the probability of a summary using a log-linear model, as is standard in machine translation:
p(y|x; θ, α) ∝",5 Extension: Extractive Tuning,[0],[0]
"exp(α> N−1∑ i=0 f(yi+1,x,yc)).
",5 Extension: Extractive Tuning,[0],[0]
Where α ∈ R5 is a weight vector and f is a feature function.,5 Extension: Extractive Tuning,[0],[0]
"Finding the best summary under this distribution corresponds to maximizing a factored scoring function s,
s(y,x) = N−1∑ i=0 α>f(yi+1,x,yc).
where g(yi+1,x,yc) , α>f(yi+1,x,yc) to satisfy Eq. 4.",5 Extension: Extractive Tuning,[0],[0]
"The function f is defined to combine the local conditional probability with some additional indicator featrues:
f(yi+1,x,yc) =",5 Extension: Extractive Tuning,[0],[0]
"[ log p(yi+1|x,yc; θ), 1{∃j.",5 Extension: Extractive Tuning,[0],[0]
yi+1,5 Extension: Extractive Tuning,[0],[0]
"= xj }, 1{∃j.",5 Extension: Extractive Tuning,[0],[0]
"yi+1−k = xj−k ∀k ∈ {0, 1}}, 1{∃j.",5 Extension: Extractive Tuning,[0],[0]
"yi+1−k = xj−k ∀k ∈ {0, 1, 2}}, 1{∃k > j. yi = xk,yi+1 = xj} ].
",5 Extension: Extractive Tuning,[0],[0]
"These features correspond to indicators of unigram, bigram, and trigram match with the input as well as reordering of input words.",5 Extension: Extractive Tuning,[0],[0]
"Note that setting α = 〈1, 0, . . .",5 Extension: Extractive Tuning,[0],[0]
", 0〉 gives a model identical to standard ABS.
",5 Extension: Extractive Tuning,[0],[0]
"After training the main neural model, we fix θ and tune the α parameters.",5 Extension: Extractive Tuning,[0],[0]
"We follow the statistical machine translation setup and use minimumerror rate training (MERT) to tune for the summarization metric on tuning data (Och, 2003).",5 Extension: Extractive Tuning,[0],[0]
This tuning step is also identical to the one used for the phrase-based machine translation baseline.,5 Extension: Extractive Tuning,[0],[0]
Abstractive sentence summarization has been traditionally connected to the task of headline generation.,6 Related Work,[0],[0]
Our work is similar to early work of Banko et al. (2000) who developed a statistical machine translation-inspired approach for this task using a corpus of headline-article pairs.,6 Related Work,[0],[0]
"We extend this approach by: (1) using a neural summarization model as opposed to a count-based noisy-channel model, (2) training the model on much larger scale (25K compared to 4 million articles), (3) and allowing fully abstractive decoding.
",6 Related Work,[0],[0]
"This task was standardized around the DUC2003 and DUC-2004 competitions (Over et al., 2007).",6 Related Work,[0],[0]
"The TOPIARY system (Zajic et al., 2004) performed the best in this task, and is described in detail in the next section.",6 Related Work,[0],[0]
"We point interested readers to the DUC web page (http://duc.nist. gov/) for the full list of systems entered in this shared task.
",6 Related Work,[0],[0]
"More recently, Cohn and Lapata (2008) give a compression method which allows for more arbitrary transformations.",6 Related Work,[0],[0]
"They extract tree transduction rules from aligned, parsed texts and learn weights on transfomations using a max-margin learning algorithm.",6 Related Work,[0],[0]
Woodsend et al. (2010) propose a quasi-synchronous grammar approach utilizing both context-free parses and dependency parses to produce legible summaries.,6 Related Work,[0],[0]
Both of these approaches differ from ours in that they directly use the syntax of the input/output sentences.,6 Related Work,[0],[0]
"The latter system is W&L in our results; we attempted to train the former system T3 on this dataset but could not train it at scale.
",6 Related Work,[0],[0]
In addition to Banko et al. (2000) there has been some work using statistical machine translation directly for abstractive summary.,6 Related Work,[0],[0]
"Wubben et al. (2012) utilize MOSES directly as a method for text simplification.
",6 Related Work,[0],[0]
Recently Filippova and Altun (2013) developed a strictly extractive system that is trained on a relatively large corpora (250K sentences) of articletitle pairs.,6 Related Work,[0],[0]
"Because their focus is extractive com-
pression, the sentences are transformed by a series of heuristics such that the words are in monotonic alignment.",6 Related Work,[0],[0]
"Our system does not require this alignment step but instead uses the text directly.
",6 Related Work,[0],[0]
Neural MT,6 Related Work,[0],[0]
This work is closely related to recent work on neural network language models (NNLM) and to work on neural machine translation.,6 Related Work,[0],[0]
"The core of our model is a NNLM based on that of Bengio et al. (2003).
",6 Related Work,[0],[0]
"Recently, there have been several papers about models for machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014).",6 Related Work,[0],[0]
"Of these our model is most closely related to the attention-based model of Bahdanau et al. (2014), which explicitly finds a soft alignment between the current position and the input source.",6 Related Work,[0],[0]
Most of these models utilize recurrent neural networks (RNNs) for generation as opposed to feedforward models.,6 Related Work,[0],[0]
We hope to incorporate an RNNLM in future work.,6 Related Work,[0],[0]
We experiment with our attention-based sentence summarization model on the task of headline generation.,7 Experimental Setup,[0],[0]
"In this section we describe the corpora used for this task, the baseline methods we compare with, and implementation details of our approach.",7 Experimental Setup,[0],[0]
"The standard sentence summarization evaluation set is associated with the DUC-2003 and DUC2004 shared tasks (Over et al., 2007).",7.1 Data Set,[0],[0]
"The data for this task consists of 500 news articles from the New York Times and Associated Press Wire services each paired with 4 different human-generated reference summaries (not actually headlines), capped at 75 bytes.",7.1 Data Set,[0],[0]
"This data set is evaluation-only, although the similarly sized DUC-2003 data set was made available for the task.",7.1 Data Set,[0],[0]
"The expectation is for a summary of roughly 14 words, based on the text of a complete article (although we only make use of the first sentence).",7.1 Data Set,[0],[0]
"The full data set is available by request at http://duc.nist.gov/data.html.
",7.1 Data Set,[0],[0]
"For this shared task, systems were entered and evaluated using several variants of the recalloriented ROUGE metric (Lin, 2004).",7.1 Data Set,[0],[0]
"To make recall-only evaluation unbiased to length, output of all systems is cut-off after 75-characters and no bonus is given for shorter summaries.
",7.1 Data Set,[0],[0]
"Unlike BLEU which interpolates various n-gram matches, there are several versions of ROUGE for different match lengths.",7.1 Data Set,[0],[0]
"The DUC evaluation uses ROUGE-1 (unigrams), ROUGE-2 (bigrams), and ROUGE-L (longest-common substring), all of which we report.
",7.1 Data Set,[0],[0]
"In addition to the standard DUC-2014 evaluation, we also report evaluation on single reference headline-generation using a randomly heldout subset of Gigaword.",7.1 Data Set,[0],[0]
"This evaluation is closer to the task the model is trained for, and it allows us to use a bigger evaluation set, which we will include in our code release.",7.1 Data Set,[0],[0]
"For this evaluation, we tune systems to generate output of the average title length.
",7.1 Data Set,[0],[0]
"For training data for both tasks, we utilize the annotated Gigaword data set (Graff et al., 2003; Napoles et al., 2012), which consists of standard Gigaword, preprocessed with Stanford CoreNLP tools (Manning et al., 2014).",7.1 Data Set,[0],[0]
"Our model only uses annotations for tokenization and sentence separation, although several of the baselines use parsing and tagging as well.",7.1 Data Set,[0],[0]
"Gigaword contains around 9.5 million news articles sourced from various domestic and international news services over the last two decades.
",7.1 Data Set,[0],[0]
"For our training set, we pair the headline of each article with its first sentence to create an inputsummary pair.",7.1 Data Set,[0],[0]
"While the model could in theory be trained on any pair, Gigaword contains many spurious headline-article pairs.",7.1 Data Set,[0],[0]
We therefore prune training based on the following heuristic filters: (1) Are there no non-stop-words in common?,7.1 Data Set,[0],[0]
(2) Does the title contain a byline or other extraneous editing marks?,7.1 Data Set,[0],[0]
(3) Does the title have a question mark or colon?,7.1 Data Set,[0],[0]
"After applying these filters, the training set consists of roughly J = 4 million title-article pairs.",7.1 Data Set,[0],[0]
"We apply a minimal preprocessing step using PTB tokenization, lower-casing, replacing all digit characters with #, and replacing of word types seen less than 5 times with UNK.",7.1 Data Set,[0],[0]
We also remove all articles from the time-period of the DUC evaluation.,7.1 Data Set,[0],[0]
"release.
",7.1 Data Set,[0],[0]
The complete input training vocabulary consists of 119 million word tokens and 110K unique word types with an average sentence size of 31.3 words.,7.1 Data Set,[0],[0]
The headline vocabulary consists of 31 million tokens and 69K word types with the average title of length 8.3 words (note that this is significantly shorter than the DUC summaries).,7.1 Data Set,[0],[0]
"On average there are 4.6 overlapping word types between the
headline and the input; although only 2.6 in the first 75-characters of the input.",7.1 Data Set,[0],[0]
"Due to the variety of approaches to the sentence summarization problem, we report a broad set of headline-generation baselines.
",7.2 Baselines,[0],[0]
From the DUC-2004 task we include the PREFIX baseline that simply returns the first 75- characters of the input as the headline.,7.2 Baselines,[0],[0]
"We also report the winning system on this shared task, TOPIARY (Zajic et al., 2004).",7.2 Baselines,[0],[0]
"TOPIARY merges a compression system using linguisticallymotivated transformations of the input (Dorr et al., 2003) with an unsupervised topic detection (UTD) algorithm that appends key phrases from the full article onto the compressed output.",7.2 Baselines,[0],[0]
"Woodsend et al. (2010) (described above) also report results on the DUC dataset.
",7.2 Baselines,[0],[0]
The DUC task also includes a set of manual summaries performed by 8 human summarizers each summarizing half of the test data sentences (yielding 4 references per sentence).,7.2 Baselines,[0],[0]
We report the average inter-annotater agreement score as REFERENCE.,7.2 Baselines,[0],[0]
"For reference, the best human evaluator scores 31.7 ROUGE-1.
",7.2 Baselines,[0],[0]
We also include several baselines that have access to the same training data as our system.,7.2 Baselines,[0],[0]
"The first is a sentence compression baseline COMPRESS (Clarke and Lapata, 2008).",7.2 Baselines,[0],[0]
This model uses the syntactic structure of the original sentence along with a language model trained on the headline data to produce a compressed output.,7.2 Baselines,[0],[0]
"The syntax and language model are combined with a set of linguistic constraints and decoding is performed with an ILP solver.
",7.2 Baselines,[0],[0]
"To control for memorizing titles from training, we implement an information retrieval baseline, IR.",7.2 Baselines,[0],[0]
"This baseline indexes the training set, and gives the title for the article with highest BM-25 match to the input (see Manning et al. (2008)).
",7.2 Baselines,[0],[0]
"Finally, we use a phrase-based statistical machine translation system trained on Gigaword to produce summaries, MOSES+ (Koehn et al., 2007).",7.2 Baselines,[0],[0]
"To improve the baseline for this task, we augment the phrase table with “deletion” rules mapping each article word to , include an additional deletion feature for these rules, and allow for an infinite distortion limit.",7.2 Baselines,[0],[0]
"We also explicitly tune the model using MERT to target the 75- byte capped ROUGE score as opposed to standard
BLEU-based tuning.",7.2 Baselines,[0],[0]
"Unfortunately, one remaining issue is that it is non-trivial to modify the translation decoder to produce fixed-length outputs, so we tune the system to produce roughly the expected length.",7.2 Baselines,[0],[0]
"For training, we use mini-batch stochastic gradient descent to minimize negative log-likelihood.",7.3 Implementation,[0],[0]
"We use a learning rate of 0.05, and split the learning rate by half if validation log-likelihood does not improve for an epoch.",7.3 Implementation,[0],[0]
Training is performed with shuffled mini-batches of size 64.,7.3 Implementation,[0],[0]
The minibatches are grouped by input length.,7.3 Implementation,[0],[0]
"After each epoch, we renormalize the embedding tables (Hinton et al., 2012).",7.3 Implementation,[0],[0]
"Based on the validation set, we set hyperparameters as D = 200, H = 400, C = 5, L = 3, and Q = 2.
",7.3 Implementation,[0],[0]
Our implementation uses the Torch numerical framework (http://torch.ch/) and will be openly available along with the data pipeline.,7.3 Implementation,[0],[0]
"Crucially, training is performed on GPUs and would be intractable or require approximations otherwise.",7.3 Implementation,[0],[0]
"Processing 1000 mini-batches with D = 200, H = 400 requires 160 seconds.",7.3 Implementation,[0],[0]
"Best validation accuracy is reached after 15 epochs through the data, which requires around 4 days of training.
",7.3 Implementation,[0],[0]
"Additionally, as described in Section 5 we apply a MERT tuning step after training using the DUC2003 data.",7.3 Implementation,[0],[0]
"For this step we use Z-MERT (Zaidan, 2009).",7.3 Implementation,[0],[0]
We refer to the main model as ABS and the tuned model as ABS+.,7.3 Implementation,[0],[0]
Our main results are presented in Table 1.,8 Results,[0],[0]
"We run experiments both using the DUC-2004 evaluation data set (500 sentences, 4 references, 75 bytes) with all systems and a randomly held-out
Gigaword test set (2000 sentences, 1 reference).",8 Results,[0],[0]
"We first note that the baselines COMPRESS and IR do relatively poorly on both datasets, indicating that neither just having article information or language model information alone is sufficient for the task.",8 Results,[0],[0]
"The PREFIX baseline actually performs surprisingly well on ROUGE-1 which makes sense given the earlier observed overlap between article and summary.
",8 Results,[0],[0]
"Both ABS and MOSES+ perform better than TOPIARY, particularly on ROUGE-2 and ROUGE-L in DUC.",8 Results,[0],[0]
"The full model ABS+ scores the best on these tasks, and is significantly better based on the default ROUGE confidence level than TOPIARY on all metrics, and MOSES+ on ROUGE-1 for DUC as well as ROUGE-1 and ROUGE-L for Gigaword.",8 Results,[0],[0]
"Note that the additional extractive features bias the system towards retaining more input words, which is useful for the underlying metric.
",8 Results,[0],[0]
Next we consider ablations to the model and algorithm structure.,8 Results,[0],[0]
Table 2 shows experiments for the model with various encoders.,8 Results,[0],[0]
"For these experiments we look at the perplexity of the system as a language model on validation data, which controls for the variable of inference and tuning.",8 Results,[0],[0]
The NNLM language model with no encoder gives a gain over the standard n-gram language model.,8 Results,[0],[0]
Including even the bag-of-words encoder reduces perplexity number to below 50.,8 Results,[0],[0]
"Both the convolutional encoder and the attention-based encoder further reduce the perplexity, with attention giving a value below 30.
",8 Results,[0],[0]
"We also consider model and decoding ablations on the main summary model, shown in Table 3.",8 Results,[0],[0]
"These experiments compare to the BoW encoding models, compare beam search and greedy decoding, as well as restricting the system to be com-
plete extractive.",8 Results,[0],[0]
"Of these features, the biggest impact is from using a more powerful encoder (attention versus BoW), as well as using beam search to generate summaries.",8 Results,[0],[0]
"The abstractive nature of the system helps, but for ROUGE even using pure extractive generation is effective.
",8 Results,[0],[0]
Finally we consider example summaries shown in Figure 4.,8 Results,[0],[0]
"Despite improving on the baseline scores, this model is far from human performance on this task.",8 Results,[0],[0]
"Generally the models are good at picking out key words from the input, such as names and places.",8 Results,[0],[0]
"However, both models will reorder words in syntactically incorrect ways, for instance in Sentence 7 both models have the wrong subject.",8 Results,[0],[0]
"ABS often uses more interesting re-wording, for instance new nz pm after election in Sentence 4, but this can also lead to attachment mistakes such a russian oil giant chevron in Sentence 11.",8 Results,[0],[0]
"We have presented a neural attention-based model for abstractive summarization, based on recent developments in neural machine translation.",9 Conclusion,[0],[0]
We combine this probabilistic model with a generation algorithm which produces accurate abstractive summaries.,9 Conclusion,[0],[0]
"As a next step we would like to further improve the grammaticality of the summaries in a data-driven way, as well as scale this system to generate paragraph-level summaries.",9 Conclusion,[0],[0]
Both pose additional challenges in terms of efficient alignment and consistency in generation.,9 Conclusion,[0],[0]
"Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build.",abstractText,[0],[0]
"In this work, we propose a fully data-driven approach to abstractive sentence summarization.",abstractText,[0],[0]
Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence.,abstractText,[0],[0]
"While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data.",abstractText,[0],[0]
The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines.,abstractText,[0],[0]
A Neural Attention Model for Sentence Summarization,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 1446–1459 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
"The task of named entity recognition (NER) involves the extraction from text of names of entities pertaining to semantic types such as person (PER), location (LOC) and geo-political entity (GPE).",1 Introduction,[0],[0]
"NER has drawn the attention of many researchers as the first step towards NLP applications such as entity linking (Gupta et al., 2017), relation extraction (Miwa and Bansal, 2016), event
1Code is available at https://github.com/ meizhiju/layered-bilstm-crf
extraction (Feng et al., 2016) and co-reference resolution (Fragkou, 2017; Stone and Arora, 2017).
",1 Introduction,[0],[0]
"Due to the properties of natural language, many named entities contain nested entities: embedded names which are included in other entities, illustrated in Figure 1.",1 Introduction,[0],[0]
"This phenomenon is quite common in many domains (Alex et al., 2007; Byrne, 2007; Wang, 2009; Màrquez et al., 2007).",1 Introduction,[0],[0]
"However, much of the work on NER copes only with non-nested entities which are also called flat entities and neglects nested entities.",1 Introduction,[0],[0]
"This leads to loss of potentially important information, with negative impacts on subsequent tasks.
",1 Introduction,[0],[0]
"Traditional approaches to NER mainly involve two types of approaches: supervised learning (Ling and Weld, 2012; Marcińczuk, 2015; Leaman and Lu, 2016) and hybrid approaches (Bhasuran et al., 2016; Rocktäschel et al., 2012; Leaman et al., 2015) that combine supervised learning with rules.",1 Introduction,[0],[0]
Such approaches require either domain knowledge or heavy featureengineering.,1 Introduction,[0],[0]
"Recent advances in neural networks enable NER without depending on external knowledge resources through automated learning highlevel and abstract features from text (Lample et al., 2016; Ma and Hovy, 2016; Pahuja et al., 2017; Strubell et al., 2017).
",1 Introduction,[0],[0]
"In this paper, we propose a novel dynamic neural model for nested entity recognition, without relying on any external knowledge resources or linguistics features.",1 Introduction,[0],[0]
"Our model enables sequentially
1446
O B-DNA I-DNA I-DNA I-DNA O
Gold labels
Outer
stacking flat NER layers from bottom to up and identifying entities in an end-to-end manner.",1 Introduction,[0],[0]
"The number of stacked layers depends on the level of entity nesting and dynamically adjusts to the input sequences as the nested level varies from different sequences.
",1 Introduction,[0],[0]
"Given a sequence of words, our model first represents each word using a low-dimensional vector concatenated from its corresponding word and character sequence embeddings.",1 Introduction,[0],[0]
"Taking the sequence of the word representation as input, our flat NER layer enables capturing context representation by a long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) layer.",1 Introduction,[0],[0]
The context representation is then fed to a CRF layer for label prediction.,1 Introduction,[0],[0]
"Subsequently, the context representation from the LSTM layer is merged to build representation for each detected entity, which is used as the input for the next flat NER layer.",1 Introduction,[0],[0]
Our model stops detecting entities if no entities are predicted by the current flat NER layer.,1 Introduction,[0],[0]
"Through stacking flat NER layers in order, we are able to extract entities from inside to outside with sharing parameters among the different LSTM layers and CRF layers.
",1 Introduction,[0],[0]
We gain 3.9 and 9.1 percentage point improvements regarding F-score over the state-of-the-art feature-based model on two nested entity corpora:,1 Introduction,[0],[0]
"GENIA (Kim et al., 2003) and ACE2005 (Walker et al., 2006), and analyze contributions of inner entities to outer entity detection, drawing several key conclusions.
",1 Introduction,[0],[0]
"In addition, experiments are conducted on a flatly annotated corpora JNLPBA (Kim et al.,
2004).",1 Introduction,[0],[0]
"Our model can be a complete NER model as well for flat entities, on the condition that it is trained on annotations that do not account for nested entities.",1 Introduction,[0],[0]
We obtain 75.55% in terms of Fscore that is comparable to the state-of-the-art performance.,1 Introduction,[0],[0]
Our nested NER model is designed based on a sequential stack of flat NER layers that detects nested entities in an end-to-end manner.,2 Neural Layered Model,[0],[0]
Figure 2 provides the overview of our model.,2 Neural Layered Model,[0],[0]
Our flat NER layers are inspired by the state-of-theart model proposed in Lample et al. (2016).,2 Neural Layered Model,[0],[0]
The layer utilizes one single bidirectional LSTM layer to represent word sequences and predict flat entities by putting one single CRF layer on top of the LSTM layer.,2 Neural Layered Model,[0],[0]
"Therefore, we refer to our model as Layered-BiLSTM-CRF model.",2 Neural Layered Model,[0],[0]
"If any entities are predicted, a new flat NER layer is introduced and the word sequence representation of each detected entity by the current flat NER layer is merged to compose a representation for the entity, which is then passed on to the new flat NER layer as its input.",2 Neural Layered Model,[0],[0]
"Otherwise, the model terminates stacking and hence finishes entity detection.
",2 Neural Layered Model,[0],[0]
"In this section, we provide a brief description of the model architecture: the flat NER layers and their stacking, the embedding layer and their training.",2 Neural Layered Model,[0],[0]
A flat NER layer consists of an LSTM layer and a CRF layer.,2.1 Flat NER layer,[0],[0]
"The LSTM layer captures the bidi-
rectional context representation of sequences and subsequently feeds it to the CRF layer to globally decode label sequences.
",2.1 Flat NER layer,[0],[0]
"LSTM is a variant of recurrent neural networks (RNNs) (Goller and Kuchler, 1996) that incorporates a memory cell to remember the past information for a long period of time.",2.1 Flat NER layer,[0],[0]
"This enables capturing long dependencies, thus reducing the gradient vanishing/explosion problem existing in RNNs.",2.1 Flat NER layer,[0],[0]
We employ bidirectional LSTM with no peephole connection.,2.1 Flat NER layer,[0],[0]
"We refer the readers to Hochreiter and Schmidhuber (1997) for more details of LSTM used in our work.
",2.1 Flat NER layer,[0],[0]
CRFs are used to globally predict label sequences for any given sequences.,2.1 Flat NER layer,[0],[0]
"Given an input sequence X = (x1, x2, . . .",2.1 Flat NER layer,[0],[0]
", xn) which is the output from the LSTM layer, we maximize the logprobability during training.",2.1 Flat NER layer,[0],[0]
"In decoding, we set transition costs between illegal transitions, e.g., transition from O to I-PER, as infinite to restrict illegal labels.",2.1 Flat NER layer,[0],[0]
"The expected label sequence y = (y1, y2, . . .",2.1 Flat NER layer,[0],[0]
", yn) is predicted based on maximum scores in decoding.",2.1 Flat NER layer,[0],[0]
"We stack a flat NER layer on the top of the current flat NER layer, aiming to extract outer entities.",2.2 Stacking flat NER layers,[0],[0]
"Concretely, we merge and average current context representation of the regions composed in the detected entities, as described in the following equation:
mi = 1 end− start+",2.2 Stacking flat NER layers,[0],[0]
"1 end∑
i=start
zi, (1)
where zi denotes the representation of the i-th word from the flat NER layer, and mi is the merged representation for an entity.",2.2 Stacking flat NER layers,[0],[0]
The region starts from a position start and ends at a position end of the sequence.,2.2 Stacking flat NER layers,[0],[0]
"This merged representation of detected entities allows us to treat each detected entity as a single token, and hence we are able to make the most of inner entity information to encourage outer entity recognition.",2.2 Stacking flat NER layers,[0],[0]
"If the region is detected as a non-entity, we keep the representation without any processing.",2.2 Stacking flat NER layers,[0],[0]
The processed context representation of the flat NER layer is used as the input for the next flat NER layer.,2.2 Stacking flat NER layers,[0],[0]
"The input for the first NER layer is different from the remaining flat NER layers since the first layer
has no previous layers.",2.3 Embedding layer,[0],[0]
We thus represent each word by concatenating character sequence embeddings and word embeddings for the first flat NER layer.,2.3 Embedding layer,[0],[0]
"Figure 3 describes the architecture of the embedding layer to produce word representation.
",2.3 Embedding layer,[0],[0]
"Following the successes of Ma and Hovy (2016) and Lample et al. (2016) in utilizing character embeddings on the flat NER task, we also represent each word with its character sequence to capture the orthographic and morphological features of the word.",2.3 Embedding layer,[0],[0]
Each character is mapped to a randomly initialized vector through a character lookup table.,2.3 Embedding layer,[0],[0]
"We feed the character vectors comprising a word to a bidirectional LSTM layer and concatenate the forward and backward representation to obtain the word-level embedding.
Differently from the character sequence embeddings, the pretrained word embeddings are used to initialize word embeddings.",2.3 Embedding layer,[0],[0]
"When evaluating or applying the model, words that are outside of the pretrained embeddings and training dataset are mapped to an unknown (UNK) embedding, which is randomly initialized during training.",2.3 Embedding layer,[0],[0]
"To train the UNK embedding, we replace words whose frequency is 1 in the training dataset with the UNK embedding with a probability 0.5.",2.3 Embedding layer,[0],[0]
"We prepare the gold labels based on the conventional BIO (Beginning, Inside, Out of entities) tagging scheme to represent a label attached to each word.
",2.4 Training,[0],[0]
"As our model detects entities from inside to outside, we keep the same order in preparing the gold labels for each word sequence.",2.4 Training,[0],[0]
We call it the detection order rule.,2.4 Training,[0],[0]
"Meantime, we define that each entity region in the sequence can only be tagged once with the same entity type, referred to as the non-duplicate rule.",2.4 Training,[0],[0]
"For instance, in Figure 2, “interleukin-2” is tagged first while “interleukin2 receptor alpha gene” is subsequently tagged following the above two rules.",2.4 Training,[0],[0]
"When assigning the label O to non-entity regions, we only follow the detection order rule.",2.4 Training,[0],[0]
"As a result, two gold label sequences {O, B-Protein, O, O, O, O} and {O, B-DNA, I-DNA, I-DNA, I-DNA, O} are assigned to the given word sequence “Mouse interleukin-2 receptor alpha gene expression” as shown in Figure 2.",2.4 Training,[0],[0]
"With these rules, the number of labels for each word equals the nested level of entities in the given word sequence.
",2.4 Training,[0],[0]
"We employ mini-batch training and update the model parameters using back-propagation through time (BPTT) (Werbos, 1990) with Adam (Kingma and Ba, 2014).",2.4 Training,[0],[0]
"The model parameters include weights, bias, transition costs, and embeddings of characters.",2.4 Training,[0],[0]
"We disable updating the word embeddings.2 During training, early stopping, L2regularization and dropout (Hinton et al., 2012) are used to prevent overfitting.",2.4 Training,[0],[0]
Dropout is employed to the input of each flat NER layer.,2.4 Training,[0],[0]
"Hyperparameters including batch size, number of hidden units in LSTM, character dimensions, dropout rate, Adam learning rate, gradient clipping and weight decay (L2) are all tuned with Bayesian optimization (Snoek et al., 2012).",2.4 Training,[0],[0]
"We employed three datasets for evaluation: GENIA3 (Kim et al., 2003), ACE20054",3 Evaluation Settings,[0],[0]
"(Walker et al., 2006) and JNLPBA5 (Kim et al., 2004).",3 Evaluation Settings,[0],[0]
We briefly explain the data and task settings and then introduce model and experimental settings.,3 Evaluation Settings,[0],[0]
"We performed nested entity extraction experiments on GENIA and ACE2005 while we con-
2We tried updating and disabling updating word embeddings.",3.1 Data and Task Settings,[0],[0]
"The former trial did not work.
",3.1 Data and Task Settings,[0],[0]
"3http://www.geniaproject.org/ genia-corpus/term-corpus
4https://catalog.ldc.upenn.edu/ ldc2006t06
5http://www.nactem.ac.uk/tsujii/GENIA/ ERtask/report.html
ducted flat entity extraction on the JNLPBA dataset.",3.1 Data and Task Settings,[0],[0]
"For the details of data statistics and preprocessing, please refer to the supplementary materials.
",3.1 Data and Task Settings,[0],[0]
"GENIA involves 36 fine-grained entity categories among total 2,000 MEDLINE abstracts.",3.1 Data and Task Settings,[0],[0]
"Following the same task settings as in Finkel and Manning (2009) and Lu and Roth (2015), we collapsed all DNA subcategories as DNA.",3.1 Data and Task Settings,[0],[0]
"The same setting was applied to RNA, protein, cell line and cell type categories.",3.1 Data and Task Settings,[0],[0]
"We used same test portion as Finkel and Manning (2009), Lu and Roth (2015) and Muis and Lu (2017) for the direct comparison.
ACE2005 contains 7 fine-grained entity categories.",3.1 Data and Task Settings,[0],[0]
"We made same modifications described in Lu and Roth (2015) and Muis and Lu (2017) by keeping files from bn, bw, nw and wl and spitting them into training, development and testing datasets at random following same ratio 8:1:1, respectively.
",3.1 Data and Task Settings,[0],[0]
JNLPBA defines both training and testing datasets.,3.1 Data and Task Settings,[0],[0]
"These two datasets are composed of 2,000 and 404 MEDLINE abstracts, respectively.",3.1 Data and Task Settings,[0],[0]
JNLPBA is originally from the GENIA corpus.,3.1 Data and Task Settings,[0],[0]
"However, only flat and topmost entities in JNLPBA are kept while nested and discontinuous entities are removed.",3.1 Data and Task Settings,[0],[0]
"Like our preprocessing on the GENIA corpus, subcategories are collapsed and only 5 entity types are finally reserved.",3.1 Data and Task Settings,[0],[0]
"We randomly chose the 90% sentences of the original training dataset as our training dataset and the remaining as our development dataset.
",3.1 Data and Task Settings,[0],[0]
"Precision (P), recall (R) and F-score (F) were used for the evaluation metrics in our tasks.",3.1 Data and Task Settings,[0],[0]
"We define that if the numbers of gold entities and predictions are all zeros, the evaluation metrics all equal one hundred percent.",3.1 Data and Task Settings,[0],[0]
"Our model was implemented with Chainer6 (Tokui et al., 2015).",3.2 Model and Experimental Settings,[0],[0]
"We initialized word embeddings in GENIA and JNLPBA with the pretrained embeddings trained on MEDLINE abstracts (Chiu et al., 2016).",3.2 Model and Experimental Settings,[0],[0]
"For ACE2005, we initialized each word with the pretrained embeddings which are trained by Miwa and Bansal (2016).",3.2 Model and Experimental Settings,[0],[0]
"Except for the word embeddings, parameters of word embeddings were initialized with a normal distribution.",3.2 Model and Experimental Settings,[0],[0]
"For LSTM, we initialized hidden states, cell state and all the bias terms as 0 except for the forget gate
6https://chainer.org/
bias that was set as 1.",3.2 Model and Experimental Settings,[0],[0]
"For other hyper-parameters, we chose the best hyper-parameters via Bayesian optimization.",3.2 Model and Experimental Settings,[0],[0]
"We refer the readers to the supplemental material for the settings of the hyperparameters of the models and Bayesian optimization.
",3.2 Model and Experimental Settings,[0],[0]
"For ablation tests, we compared with our layered-BiLSTM-CRF model with two models that produce the input for next flat NER layer in different ways.",3.2 Model and Experimental Settings,[0],[0]
The first model is called layeredBiLSTM-CRF w/o layered out-of-entities which uses the input of the current flat NER layer for out-of-entity words.,3.2 Model and Experimental Settings,[0],[0]
We name the second model as layered-BiLSTM-CRF w/o layered LSTM as it skips all intermediate LSTM layers and only uses output of embedding layer to build the input for the next flat NER layer.,3.2 Model and Experimental Settings,[0],[0]
"Please refer to supplemental material for the introduced two models.7
To investigate the effectiveness of our model on different nested levels of entities, we evaluated the model performance on each flat NER layer on GENIA and ACE2005 test datasets.8",3.2 Model and Experimental Settings,[0],[0]
"When calculating precision and recall measurements, we collected the predictions and gold entities from the corresponding flat NER layer.",3.2 Model and Experimental Settings,[0],[0]
"Since predicted entities on a specific flat NER layer might be from other flat NER layers, we defined extended precision (EP), extended recall (ER) and extended Fscore (EF) to measure the performance.",3.2 Model and Experimental Settings,[0],[0]
"We calculated EP by comparing the predicted entities in a specific flat NER layer with all the gold entities, and ER by comparing the gold entities in a specific flat NER layer with all the predicted entities.",3.2 Model and Experimental Settings,[0],[0]
"EF was calculated in the same way with F.
In addition to experiments on nested GENIA and ACE2005 datasets, flat entity recognition was conducted on the JNLPBA dataset.",3.2 Model and Experimental Settings,[0],[0]
We trained our flat model that only kept the first flat NER layer and removed the following stacking layers.,3.2 Model and Experimental Settings,[0],[0]
"We follow the hyper-parameters settings by Lample et al. (2016) for this evaluation.
",3.2 Model and Experimental Settings,[0],[0]
7We examined the contributions of predicted labels of the current flat NER layer to the next flat NER layer.,3.2 Model and Experimental Settings,[0],[0]
"For this, we introduced label embeddings into each test by combining the embedding with context representation.",3.2 Model and Experimental Settings,[0],[0]
"Experiments show that appending label embedding hurts the performance of our model while gain slight improvements in the rest 2 models on development datasets.
8We removed entities which were predicted in previous flat NER layers during evaluation.",3.2 Model and Experimental Settings,[0],[0]
Table 1 presents the comparisons of our model with related work including the state-of-the-art feature-based model by Muis and Lu (2017).,4.1 Nested NER,[0],[0]
"Our model outperforms the state-of-the-art models with 74.7% and 72.2% in terms of F-score, achieving the new state-of-the-art in the nested NER tasks.",4.1 Nested NER,[0],[0]
"For GENIA, our model gained more improvement in terms of recall with enabling extract more nested entities without reducing precision.",4.1 Nested NER,[0],[0]
"On ACE2005, we improved recall with 12.2 percentage points and obtained 5.1% relative error reductions.",4.1 Nested NER,[0],[0]
"Compared with GENIA, our model gained more improvements in ACE2005 in terms of F-score.",4.1 Nested NER,[0],[0]
Two possible reasons account for it.,4.1 Nested NER,[0],[0]
One reason is that ACE2005 contains more deeper nested entities (maximum nested level is 5) than GENIA (maximum nested level is 3) on the test dataset.,4.1 Nested NER,[0],[0]
This allows our model to capture the potentially ‘nested’ relations among nested entities.,4.1 Nested NER,[0],[0]
"The other reason is that ACE2005 has more nested entities (37.45%) compared with GENIA (21.56%).
",4.1 Nested NER,[0],[0]
"Table 2 shows the results of models on the development datasets of GENIA and ACE2005, respectively.",4.1 Nested NER,[0],[0]
"From this table, we can see that our model, which only utilizes context representation for preparation of input for the next flat NER layer, performs better than the rest two models.",4.1 Nested NER,[0],[0]
This demonstrates that introducing input of the current flat NER layer such as skipping either representation for any non-entity or words or all intermediate LSTM layers hurts performance.,4.1 Nested NER,[0],[0]
"Compared with the layered-BiLSTM-CRF model, the drop of the performance in the layered-BiLSTM-CRF w/o layered out-of-entities model reflects the skip of representation for out-of-entity words leads to the decline in performance.",4.1 Nested NER,[0],[0]
This is because the representation of non-entity words didn’t incorporate the current context representation as we used the input rather than the output to represent them.,4.1 Nested NER,[0],[0]
"By analogy, the layered BiLSTM-CRF w/o layer LSTM model skips representation for both entities and non-entity words, resulting in worse performance.",4.1 Nested NER,[0],[0]
"This is because, when skipping all intermediate LSTM layers, input of the first flat NER layer, i.e., word embeddings, is passed to the remaining flat NER layers.",4.1 Nested NER,[0],[0]
"Since word embeddings do not contain context representation, we fail to incorporate the context representation when we use
the word embeddings as the input for the flat NER layers.",4.1 Nested NER,[0],[0]
"Therefore, we have no chance to take advantage of the context representation and instead we only manage to use the word embeddings as the input for flat NER layers in this case.
",4.1 Nested NER,[0],[0]
"Table 3 and Table 4 describe the performance for each entity type in GENIA and ACE2005 test datasets, respectively.",4.1 Nested NER,[0],[0]
"In GENIA, our model performed best in recognizing entities with type RNA.
",4.1 Nested NER,[0],[0]
This is because most of the entities pertaining to RNA mainly end up either with “mRNA” or RNA.,4.1 Nested NER,[0],[0]
These two words are informative indicators of RNA entities.,4.1 Nested NER,[0],[0]
"For entities in rest entity types, their performances are close to the overall performance.",4.1 Nested NER,[0],[0]
One possible reason is that there are many instances to model them.,4.1 Nested NER,[0],[0]
"This also accounts for the high performances of entity types such as PER, GPE in ACE2005.",4.1 Nested NER,[0],[0]
The small amounts of instances of entity types like FAC in ACE2005 is one reason for their under overall performances.,4.1 Nested NER,[0],[0]
"We refer readers to supplemental material for statistics details.
",4.1 Nested NER,[0],[0]
"When evaluating our model on top level which contains only outermost entities, the precision, recall and F-score were 78.19%, 75.17% and 76.65% on GENIA test dataset.",4.1 Nested NER,[0],[0]
"For ACE2005, the corresponding precision, recall and F-score were 68.37%, 68.57% and 68.47%.",4.1 Nested NER,[0],[0]
"Compared with the overall performance listed in Table 1, we obtained higher top level performance on GENIA but lower performance in ACE2005.",4.1 Nested NER,[0],[0]
"We discuss details of this phenomena in the following tables.
",4.1 Nested NER,[0],[0]
Table 5 shows the performances of each flat NER layer in GENIA test dataset.,4.1 Nested NER,[0],[0]
"Among all the stacking flat NER layers, our model resulted in the best performance regarding standard evaluation metrics on the first flat NER layer which contains the predictions for the gold innermost entities.",4.1 Nested NER,[0],[0]
"When the model went to deeper flat NER layers, the performance dropped gradually as the number of gold entities decreased.",4.1 Nested NER,[0],[0]
"However, the performance for predictions on each flat
NER layer was different in terms of extended evaluation metrics.",4.1 Nested NER,[0],[0]
"For the first two flat NER layers, performance of extended evaluation is better than the performance of standard evaluation.",4.1 Nested NER,[0],[0]
It indicates that gold entities correspond to some of the predictions on the specific flat NER layer are from other flat NER layers.,4.1 Nested NER,[0],[0]
This may lead to the zero performances for the last flat NER layer.,4.1 Nested NER,[0],[0]
"In addition, performance on the second flat NER layer was higher than it was on the first flat NER layer in terms of extended F-score.",4.1 Nested NER,[0],[0]
"This demonstrates that our model is able to obtain higher performance on top level of entities than innermost entities.
",4.1 Nested NER,[0],[0]
Table 6 lists the results of each flat NER layer on ACE2005 test dataset.,4.1 Nested NER,[0],[0]
"Similar to GENIA, the first flat NER layer achieved better performance than the rest flat NER layers.",4.1 Nested NER,[0],[0]
Performances decreased in a bottom-to-up manner regarding model architecture.,4.1 Nested NER,[0],[0]
"This phenomena was the same with the extended evaluation performances, which reflects that some of the predictions in a specific flat NER layer were detected in other flat NER layers.",4.1 Nested NER,[0],[0]
"Unlike rising tendency (except last flat NER layer) regarding extend F-score in GENIA, performance in ACE2005 was in downtrend.",4.1 Nested NER,[0],[0]
This accounts for the fact that F-score on top level was lower than it on the fist flat NER layer.,4.1 Nested NER,[0],[0]
"Even though the decline trend in extended F-score, the first flat NER layer contained the largest proportion of predictions for the gold entities, the overall performance on all nested entities showed in Table 1 was still high.",4.1 Nested NER,[0],[0]
"Unlike GENIA, our model in ACE2005 stopped before reaching the maximum nested level of entities.",4.1 Nested NER,[0],[0]
It indicates our model failed to model the appropriate nested levels.,4.1 Nested NER,[0],[0]
"This is one of the reasons that account for the zero predictions on the last
flat NER layer.",4.1 Nested NER,[0],[0]
One reason is that our model The sparse instances on the high nested levels could be another reason that resulted in the zero performances on the last flat NER layer.,4.1 Nested NER,[0],[0]
"Compared with the state-of-the-art work on JNLPBA (Gridach, 2017) which achieved 75.87% in terms of F-score, our model obtained 75.55% in F-score.",4.2 Flat NER,[0],[0]
"Since both the model by Gridach (2017) and our flat model are based on Lample et al. (2016), so it is reasonable that both models were able to get comparable performance.",4.2 Flat NER,[0],[0]
We showed the error types and their statistics both for all nested entities and each flat NER layer on GENIA and ACE2005 test datasets.,4.3 Error analysis,[0],[0]
"From ACE2005 test dataset, 28% of predictions were incorrect in 200 sentences which were selected at random.",4.3 Error analysis,[0],[0]
"Among these errors, 39% of them were because their text spans were assigned with other entity types.",4.3 Error analysis,[0],[0]
We call this type of errors type error.,4.3 Error analysis,[0],[0]
The main reason is that most of them are pronouns and co-refer to other entities which are absent in the sentence.,4.3 Error analysis,[0],[0]
"Taking this sentence “whether that is true now, we can not say” as an example, “we” is annotated as ORG while our model labeled it as PER.",4.3 Error analysis,[0],[0]
Lack of context information such as the absence of co-referent entities leads our model to make the wrong decisions.,4.3 Error analysis,[0],[0]
"In addition, 30% of the errors were caused by that incorrect predictions were predicted as only parts of gold entities with correct entity types.",4.3 Error analysis,[0],[0]
This error type is referred to as partial prediction error.,4.3 Error analysis,[0],[0]
"This might be due to these gold entities tend to clauses or inde-
pendent sentences, thus possibly containing many modifiers.",4.3 Error analysis,[0],[0]
"For example, in this sentence “A man who has been to Baghdad many times and can tell us with great knowledge exactly what it’s going to be like to fight on those avenues in that sprawling city of Baghdad - Judy .”, “A man who has been to Baghdad many times and can tell us with great knowledge exactly what it’s going to be like to fight on those avenues in that sprawling city of Baghdad” is annotated as PER while our model could only extract “A man who has been to Baghdad many times” and predicted it as PER.
",4.3 Error analysis,[0],[0]
"Errors on the first flat NER layer, we got 41% in type error and 11% of partial prediction error, respectively.",4.3 Error analysis,[0],[0]
"Apart from this, our model recognized predictions from other flat NER layers, leading to 5% errors.",4.3 Error analysis,[0],[0]
We define this error type as layer error.,4.3 Error analysis,[0],[0]
"Unlike the first flat NER layer, 26% of errors were caused by layer error.",4.3 Error analysis,[0],[0]
"Additionally, 17% of the errors belong to type error.",4.3 Error analysis,[0],[0]
"In particular, 22% errors were due to the type error.",4.3 Error analysis,[0],[0]
"As for the last flat NER layer, 40% errors were caused by partial prediction error.",4.3 Error analysis,[0],[0]
The rest errors were different from the mentioned error types.,4.3 Error analysis,[0],[0]
One possible reason is that we have less gold entities to train this flat NER layer compared with previous flat NER layers.,4.3 Error analysis,[0],[0]
"Another reason might be the error propagation.
",4.3 Error analysis,[0],[0]
"Similarly, 200 sentences were randomly selected from GENIA test dataset.",4.3 Error analysis,[0],[0]
We got 20% errors of predictions in the subset.,4.3 Error analysis,[0],[0]
"Among these errors, 17% and 24% of errors were separately due to type error and partial prediction error.",4.3 Error analysis,[0],[0]
"In addition, 24% of the predictions on the first flat NER layer were incorrect.",4.3 Error analysis,[0],[0]
"Among them, the top error types were layer error, partial prediction error and type error, accounting for 21%, 18% and 13%, respectively.",4.3 Error analysis,[0],[0]
Errors on the second flat NER layer were mainly caused by type error and the and partial prediction error.,4.3 Error analysis,[0],[0]
"The success of neural networks has boosted the performance of flat named NER in different domains (Lample et al., 2016; Ma and Hovy, 2016; Gridach, 2017; Strubell et al., 2017).",5 Related Work,[0],[0]
"Such models achieved the state of the art without any handcrafted features and external knowledge resources.
",5 Related Work,[0],[0]
"Contrary to flat NER, much fewer attempts have emphasized the nested entity recognition.",5 Related Work,[0],[0]
"Existing approaches to nested NER (Shen et al., 2003; Alex et al., 2007; Finkel and Manning, 2009; Lu
and Roth, 2015; Xu and Jiang, 2016; Muis and Lu, 2017) mainly rely on hand-crafted features.",5 Related Work,[0],[0]
They also failed to take advantage of the dependencies among nested entities.,5 Related Work,[0],[0]
"Our model enables capturing dependencies and automatic learning highlevel abstract features from texts.
",5 Related Work,[0],[0]
Early work regarding nested NER involve mainly hybrid systems that combined rules with supervised learning algorithms.,5 Related Work,[0],[0]
"For example, Shen et al. (2003), Zhou et al. (2004) and Zhang et al. (2004) employed a Hidden Markov Model to GENIA to extract inner entities and then used rule-based methods to obtain the outer entities.",5 Related Work,[0],[0]
"Furthermore, Gu (2006) extracted nested entities based on SVM which were trained separately on both inner entities and outermost entities without putting the hidden relations between nested entities into consideration.",5 Related Work,[0],[0]
All these methods failed to capture the dependencies between nested entities.,5 Related Work,[0],[0]
One trial work is that Alex et al. (2007) separately built a inside-out and outside-in layered CRFs which were able to use the current guesses as the input for next layer.,5 Related Work,[0],[0]
"They also cascaded separate CRFs of each entity type by using output from previous CRFs as features of current CRFs, yielding best performance in their work.",5 Related Work,[0],[0]
"One of the main drawbacks in the cascading approach was that it failed to handle nested entities sharing the same entity type, which were quite common in natural languages.
",5 Related Work,[0],[0]
Finkel and Manning (2009) proposed a discriminative constituency tree to represent each sentence where the root node was used for connection.,5 Related Work,[0],[0]
All entities were treated as phrases and represented as subtrees following the whole tree structure.,5 Related Work,[0],[0]
"Unlike our linguistic features independent model, Finkel and Manning (2009) used a CRFbased approach driven by entity-level features to detect nested entities
Later on, Lu and Roth (2015) built hyper-graphs that allow edges to connect multiple nodes to represent both the nested entities and their references (a.k.a. mentions).",5 Related Work,[0],[0]
"One issue in their approach is the spurious structures of hyper-graphs as they enumerate combinations of nodes, types and boundaries to represent entities.",5 Related Work,[0],[0]
"In addition, they fail to encode the dependencies among embedded entities using hyper-graphs.",5 Related Work,[0],[0]
"In contrast, our model enables nested entity representation by merging representation of multiple tokens composed in the entity and considers it as the longer
entity representation.",5 Related Work,[0],[0]
"This allows us to represent outer entities based on inner entity representation, thus managing to capture the relations between inner and outer entities, and hence overcoming the spurious entity structure problem.
",5 Related Work,[0],[0]
"As an improvement in overcoming spurious structure issue in Lu and Roth (2015), Muis and Lu (2017) further incorporated mention separators along with features to yield better performance on nested entities.",5 Related Work,[0],[0]
Both Lu and Roth (2015) and Muis and Lu (2017) rely on hand-crafted features to extract nested entities without incorporating hidden dependencies in nested entities.,5 Related Work,[0],[0]
"In contrast, we make the most of dependencies of nested entities in our model to encourage outer entity recognition by automatic learning of high-level and abstract features from sequences.
",5 Related Work,[0],[0]
Shared tasks dealing with nested entities like SemEval-2007 Task 99 and GermEval-201410 were held in order to advance the state-of-theart on this issue.,5 Related Work,[0],[0]
"Additionally, as subtasks in KBP 201511 and KBP 201612, one of the aims in tri-lingual Entity Discovery and Linking Track (EDL) track was extracting nested entities from textual documents varying from English, Chinese and Spanish.",5 Related Work,[0],[0]
"Following this task, Xu and Jiang (2016) firstly developed a new tagging scheme which is based on fixed-size ordinally-forgetting encoding (FOFE) method for text fragment representation.",5 Related Work,[0],[0]
All the entities along their contexts were represented using this novel tagging scheme.,5 Related Work,[0],[0]
"Different from the extensively used LSTM-RNNs in sequence labeling task, a feed-forward neural network was used to predict labels on entity level for each fragment in any of given sequences.",5 Related Work,[0],[0]
"Additionally, Li et al. (2017) used the model proposed in Lample et al. (2016) to the extract both flat entities and components composed in nested and discontinuous entities.",5 Related Work,[0],[0]
Another BiLSTM was applied to combine the components to get nested and discontinuous entities.,5 Related Work,[0],[0]
"However, these methods failed to capture and utilize the inner entity representation to facilitate outer entity detection.
9http://nlp.cs.swarthmore.edu/semeval/ tasks/index.php
10https://sites.google.com/site/ germeval2014ner/
11https://tac.nist.gov//2015/KBP/ 12https://tac.nist.gov//2016/KBP/",5 Related Work,[0],[0]
This paper presented a dynamic layered model which takes full advantage of inner entity information to encourage outer entity recognition in an end-to-end manner.,6 Conclusion,[0],[0]
"Our model is based on a flat NER layer consisting of LSTM and CRF, so our model is able to capture context representation of input sequences and globally decode predicted labels at a flat NER layer without relying on feature-engineering.",6 Conclusion,[0],[0]
Our model automatically stacks the flat NER layers with sharing the parameters of LSTM and CRF in the layers.,6 Conclusion,[0],[0]
"The stacking continues until the current flat NER layer predicts sequences as all outside of entities, which enables stopping dynamically stacked flat NER layers.",6 Conclusion,[0],[0]
"Each flat NER layer receives the merged context representation as input for outer entity recognition, based on the predicted entities from the previous flat NER layer.",6 Conclusion,[0],[0]
"With this dynamic endto-end model, our model is able to outperform existing models, achieving the-state-of-art on two nested NER tasks.",6 Conclusion,[0],[0]
"In addition, the model can be flexibly simplified as a flat NER model by removing components cascaded after the first NER layer.
",6 Conclusion,[0],[0]
"Extensive evaluation shows that utilization of inner entities significantly encourages outer entities detection with improvements of 3.9 and 9.1 percentage points in F-score on GENIA and ACE2005, respectively.",6 Conclusion,[0],[0]
"Additionally, utilization of only current context representation contributes to the performance improvement than use of context representation from multi-layers.",6 Conclusion,[0],[0]
We thank the anonymous reviewers for their valuable comments.,Acknowledgments,[0],[0]
The first author is financially supported by the University of Manchesters 2016 Presidents Doctoral Scholar Award.,Acknowledgments,[0],[0]
Sophia Ananiadou acknowledges BBSRC BB/P025684/1 Japan Partnering Award and BB/M006891/1 Empathy.,Acknowledgments,[0],[0]
This research has also been carried out with funding from AIRC/AIST and results obtained from a project commissioned by the New Energy and Industrial Technology Development Organization (NEDO).,Acknowledgments,[0],[0]
"Statistics of GENIA, ACE2005 and JNLPBA are described in Tables 7, 8 and 9, respectively.
",A Data Statistics and Preprocessing,[0],[0]
"We used NERSuite (Cho et al., 2010) for GENIA to perform tokenization while Stanford CoreNLP",A Data Statistics and Preprocessing,[0],[0]
"(Manning et al., 2014) was used for ACE2005.",A Data Statistics and Preprocessing,[0],[0]
"The JNLPBA dataset has already been went through tokenization and sentence splitting, so we did not apply any preprocessing.
",A Data Statistics and Preprocessing,[0],[0]
"For GENIA, we had to manually revolve the following two issues, in addition to the above preprocessing.
",A Data Statistics and Preprocessing,[0],[0]
One of the issues we had in this corpus is the removal of discontinuous entities during parsing.,A Data Statistics and Preprocessing,[0],[0]
"In provided GENIA XML file, each flat entity is annotated with ‘lex’ (lexical) and ‘sem’ (semantics) attributes while discontinuous and nested entities may have none, one or two attributes when these entities embed with each other, making it difficult to extract the strictly nested ones.",A Data Statistics and Preprocessing,[0],[0]
"Taken the text “recombinant human nm23-H1, -H2, mouse nm23-M1, and -M2 proteins” as an example, there are six discontinuous entities, “recombinant human nm23-H1 protein”, “recombinant human
H2 protein”, “recombinant mouse nm23-M1 protein”, “recombinant mouse nm23-M2 protein”, “mouse nm23-M2” and “human nm23-H2”, and two nested entities, “mouse nm23-M1” and “human nm23-H1”.",A Data Statistics and Preprocessing,[0],[0]
"We extract these nested entities based on symbol * appeared ‘lex’ attribute which
is an connection indicator of the separated texts in discontinuous entities.",A Data Statistics and Preprocessing,[0],[0]
"Meanwhile, each of the
separated texts has no ‘sem’ attribute unless itself is an innermost entity.",A Data Statistics and Preprocessing,[0],[0]
"Unfortunately, there are some inconsistent cases such as “c-fos and c-jun transcripts” where symbol * should be in the ‘lex’ attribute as the discontinuous entity “c-fos transcript” is connected by “c-fos” and “transcript” while “c-jun transcript” is connected by “c-jun” and “transcript”.",A Data Statistics and Preprocessing,[0],[0]
These two entities share the same text “transcript”.,A Data Statistics and Preprocessing,[0],[0]
"However, each of them is annotated with two attributes: ‘lex’ and ‘sem’, following the same annotation for flat entities.",A Data Statistics and Preprocessing,[0],[0]
"Although it is possible to ignore the latter entity based on ‘lex’ attribute and its belonging sentence, this rule fails to deal with entity “c-jun gene” in the example of “c-fos and c-jun genes” as the ‘lex’ of “c-jun gene” is mistaken as “c-jun genes”.",A Data Statistics and Preprocessing,[0],[0]
"Therefore, in this case, we ignored “c-fos transcript” and instead kept the “c-jun transcripts” as a flat entity.
",A Data Statistics and Preprocessing,[0],[0]
Another issue is the incomplete tokenization.,A Data Statistics and Preprocessing,[0],[0]
"The label assignment to one word was conducted on the word-level instead of character level, but there are entities that correspond to parts of words.",A Data Statistics and Preprocessing,[0],[0]
"An example is “NF-YA subunit”, which contains two protein entities: “NF-Y” and “A subunit”.",A Data Statistics and Preprocessing,[0],[0]
"To cope with this problem, we treat both two entities as false negative entities in training dataset as there are only 13 such entities in the training data set.",A Data Statistics and Preprocessing,[0],[0]
The hyper-parameters which were tuned for our model are listed in Table 10 and Table 11.,B Bayesian Optimization Setting,[0],[0]
These hyper-parameters are tuned by Bayesian optimization with the hyper parameters listed in Table 12.,B Bayesian Optimization Setting,[0],[0]
"Figure 4 shows the model architecture when we skip all intermediate LSTM layers and only word embeddings are used to produce the input for the next flat NER layer.
",C Model Structure,[0],[0]
Figure 5 describes the model architecture when we skip the representation of non-entity words to prepare the input for the next flat NER layer.,C Model Structure,[0],[0]
"Concretely, we merge and average representation following Equation 1.",C Model Structure,[0],[0]
"For the predicted non-entity words, however, we skip the LSTM layer and directly use their corresponding representation from the input rather than the output context representation.",C Model Structure,[0],[0]
Entity mentions embedded in longer entity mentions are referred to as nested entities.,abstractText,[0],[0]
"Most named entity recognition (NER) systems deal only with the flat entities and ignore the inner nested ones, which fails to capture finer-grained semantic information in underlying texts.",abstractText,[0],[0]
"To address this issue, we propose a novel neural model to identify nested entities by dynamically stacking flat NER layers.",abstractText,[0],[0]
Each flat NER layer is based on the state-ofthe-art flat NER model that captures sequential context representation with bidirectional long short-term memory (LSTM) layer and feeds it to the cascaded CRF layer.,abstractText,[0],[0]
Our model merges the output of the LSTM layer in the current flat NER layer to build new representation for detected entities and subsequently feeds them into the next flat NER layer.,abstractText,[0],[0]
"This allows our model to extract outer entities by taking full advantage of information encoded in their corresponding inner entities, in an inside-to-outside way.",abstractText,[0],[0]
Our model dynamically stacks the flat NER layers until no outer entities are extracted.,abstractText,[0],[0]
"Extensive evaluation shows that our dynamic model outperforms state-ofthe-art feature-based systems on nested NER, achieving 74.7% and 72.2% on GENIA and ACE2005 datasets, respectively, in terms of Fscore.1",abstractText,[0],[0]
A Neural Layered Model for Nested Named Entity Recognition,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4328–4339 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
4328",text,[0],[0]
Coherence is a key factor that distinguishes well-written texts from random collections of sentences.,1 Introduction,[0],[0]
A potential application of coherence models is text quality assessment.,1 Introduction,[0],[0]
"Examples include readability assessment (Pitler and Nenkova, 2008; Li and Hovy, 2014) and essay scoring (Miltsakaki and Kukich, 2004; Burstein et al., 2010).",1 Introduction,[0],[0]
"Here, we address the problem of local coherence modeling, which captures text relatedness at the level of sentence-to-sentence transitions.
",1 Introduction,[0],[0]
Several approaches to local coherence modeling have been proposed.,1 Introduction,[0],[0]
"Entity-based methods principally relate adjacent sentences by means of entities, which are mentioned as noun phrases, NPs, in sentences (Barzilay and Lapata, 2008; Elsner and Charniak, 2011; Guinaudeau and Strube,
∗This author is currently employed by the Ubiquitous Knowledge Processing (UKP) Lab, Technische Universität Darmstadt, https://www.ukp.tu-darmstadt.de.
2013; Tien Nguyen and Joty, 2017).",1 Introduction,[0],[0]
"Lexical models connect sentences based on semantic relations between words in sentences (Beigman Klebanov and Shamir, 2006; Heilman et al., 2007; Mesgar and Strube, 2016).",1 Introduction,[0],[0]
Both of these approaches suffer from different weaknesses.,1 Introduction,[0],[0]
"The entity-based models require an entity detection system, a coreference model, and a syntactic parser.",1 Introduction,[0],[0]
These subsystems need to be perfect to gain the best performance of entity-based coherence models.,1 Introduction,[0],[0]
"The weakness of the lexical models is that they consider words independently, i.e. regardless of context in that words appear.",1 Introduction,[0],[0]
"More concretely, such lexical models take sentences as a bag of words.",1 Introduction,[0],[0]
"Recent deep learning coherence work (Li and Hovy, 2014; Li and Jurafsky, 2017) adopts recursive and recurrent neural networks for computing semantic vectors for sentences.",1 Introduction,[0],[0]
"Coherence models that use recursive neural networks suffer from a severe dependence on external resources, e.g. a syntactic parser to construct their recursion structure.",1 Introduction,[0],[0]
Coherence models that purely rely on the recurrent neural networks process words sequentially within a text.,1 Introduction,[0],[0]
"However, in such models, long-distance dependencies between words cannot be captured effectively due to the limits of the memorization capability of recurrent networks.
",1 Introduction,[0],[0]
Our motivation is to overcome these limitations.,1 Introduction,[0],[0]
"We use the advantages of distributional representations in order to, first, identify and represent salient semantic information that connects sentences, and second, extract patterns of changes in such information as a text progresses.",1 Introduction,[0],[0]
"By representing words of sentences with their pre-trained embeddings, we take lexical semantic relations between words into account.",1 Introduction,[0],[0]
We employ a Recurrent Neural Network (RNN) layer to combine information in word embeddings and actual context information of words in sentences.,1 Introduction,[0],[0]
"Our model encodes salient information that relates two adja-
cent sentences based on the two most similar RNN states in sentences.",1 Introduction,[0],[0]
We accumulate two identified RNN states to represent semantic information that connects two adjacent sentences.,1 Introduction,[0],[0]
We encode pattern of semantic information changes across sentences in a text by a convolutional neural network to represent coherence.,1 Introduction,[0],[0]
Our end-to-end coherence model is superior to previous work because it relates sentences based on two semantic information states in sentences that are highly similar.,1 Introduction,[0],[0]
So it does not need extra tools such as coreference resolution systems.,1 Introduction,[0],[0]
"Furthermore, our model incorporates words in their sentence context and models (roughly) distant relations between words.
",1 Introduction,[0],[0]
We evaluate our model on two tasks: readability assessment and essay scoring.,1 Introduction,[0],[0]
"Both have been frequently used for coherence evaluation (Barzilay and Lapata, 2008; Miltsakaki and Kukich, 2004).",1 Introduction,[0],[0]
Readability assessment is a ranking task where we compare the rankings given by the model against human judgments.,1 Introduction,[0],[0]
"Essay scoring is a regression task, in which we investigate if the combination of coherence vectors produced by our model and other essay scoring features proposed by Phandi et al. (2015) improves the performance of the essay scorer.",1 Introduction,[0],[0]
"The experimental results show that our model achieves the state-of-the-art result for readability assessment on the examined dataset (De Clercq and Hoste, 2016); and the combination of our coherence features with other essay scoring features significantly improves the performance of the examined essay scorer (Phandi et al., 2015).",1 Introduction,[0],[0]
"Early work on coherence captures different types of relations: entity-based (Grosz et al., 1995; Barzilay and Lapata, 2008), lexical-based (Beigman Klebanov and Flor, 2013; Somasundaran et al., 2014; Zhang et al., 2015), etc.",2 Related Work,[0],[0]
"Among these models, the entity-grid model (Barzilay and Lapata, 2005, 2008) has received a lot of attention.",2 Related Work,[0],[0]
"In this model, entities are defined, heuristically, by applying a string match over head nouns of all NPs in a text.",2 Related Work,[0],[0]
"The model, then, defines all possible changes over syntactic roles of entities in adjacent sentences as coherence patterns.",2 Related Work,[0],[0]
"The entity-grid model has been extended both by expanding its entity extraction phase (Elsner and Charniak, 2011; Feng and Hirst, 2012) and by defining other types of patterns (Lin et al., 2011; Louis and Nenkova, 2012; Ji and Eisenstein, 2014; Guinaudeau and
Strube, 2013).",2 Related Work,[0],[0]
"Recently, Tien Nguyen and Joty (2017) fed entity grid representations of texts to a convolutional neural network (CNN) in order to overcome the limitation of predefined coherence patterns and extract patterns automatically.",2 Related Work,[0],[0]
"However, all of these models limit relations between sentences to entities that are shared by sentences.",2 Related Work,[0],[0]
This makes the performance of these models dependent on the performance of other tools like coreference resolution systems and syntactic parsers.,2 Related Work,[0],[0]
"Our coherence model, in contrast, is based on relations between any embedded semantic information in sentences, and does not require entity annotations.",2 Related Work,[0],[0]
A similar approach to ours is proposed by Mesgar and Strube (2016).,2 Related Work,[0],[0]
Their approach encodes lexical relations between sentences in a text via a graph.,2 Related Work,[0],[0]
"Sentences are encoded by nodes, and lexical semantic relations between sentences are represented by edges.",2 Related Work,[0],[0]
Coherence patterns are obtained by applying a subgraph mining method to graph representations of all texts in a corpus.,2 Related Work,[0],[0]
This model involves words individually and independent of their sentence context.,2 Related Work,[0],[0]
Our model uses a RNN layer over words in sentences to incorporate context information.,2 Related Work,[0],[0]
Our approach for extracting coherence patterns also differs from this model as we employ CNNs rather than graph mining.,2 Related Work,[0],[0]
Li and Hovy (2014) model sentences as vectors derived from RNNs and train a feed-forward neural network that takes an input window of sentence vectors and assigns a probability which represents the coherence of the sentences in the window.,2 Related Work,[0],[0]
Text coherence is evaluated by sliding the window over sentences and aggregating their coherence probabilities.,2 Related Work,[0],[0]
"Similarly, Li and Jurafsky (2017) study the same model at a larger scale and use a sequence-to-sequence approach in which the model is trained to generate the next sentence given the current sentence and vice versa.",2 Related Work,[0],[0]
Our approach differs from these methods; we represent coherence by a vector of coherence patterns.,2 Related Work,[0],[0]
"Moreover, our model takes distant relations between words in a text into account by relating two semantic states of sentences that are highly similar.",2 Related Work,[0],[0]
Lai and Tetreault (2018) compare the performance of the aforementioned coherence models on texts from different domains.,2 Related Work,[0],[0]
"They conclude that the neural coherence models, which are explained above, surpass examined nonneural coherence models such as the entity-based models and the lexical-based model.",2 Related Work,[0],[0]
"Unlike their
evaluation method, which predicts the coherence level of a text, we rank two texts with respect to their coherence levels for the readability assessment task.",2 Related Work,[0],[0]
"We also show that integrating our coherence model into an essay scorer improves its performance.
",2 Related Work,[0],[0]
"An important task for evaluating a coherence model is readability assessment (Li and Hovy, 2014; Petersen et al., 2015; Todirascu et al., 2016).",2 Related Work,[0],[0]
"The more coherent a text, the faster to read and easier to understand it is.",2 Related Work,[0],[0]
"Early readability formulas were based on superficial text features such as average word lengths (Kincaid et al., 1975).",2 Related Work,[0],[0]
"These formulas systematically ignore many important factors that affect readability such as discourse coherence (Barzilay and Lapata, 2008).",2 Related Work,[0],[0]
"Schwarm and Ostendorf (2005) and Feng et al. (2010) recast readability assessment as a ranking task, and employ different semantic (e.g. language model perplexity scores) and syntactic (e.g. the average number of NPs) features to solve this task.",2 Related Work,[0],[0]
Pitler and Nenkova (2008) show that discourse coherence features are more informative than other features for ranking texts with respect to their readability.,2 Related Work,[0],[0]
"Following the related work on coherence modeling (Barzilay and Lapata, 2008; Mesgar and Strube, 2015), we evaluate our coherence model on this task.
",2 Related Work,[0],[0]
"Another popular task for evaluating coherence models is essay scoring (Beigman Klebanov and Flor, 2013; Somasundaran et al., 2014).",2 Related Work,[0],[0]
"Miltsakaki and Kukich (2004) employ an essay scoring system to examine whether local coherence features, as defined by a measure of Centering Theory’s Rough-Shift transitions (Grosz et al., 1995), might be a significant contributor to the evaluation of essays.",2 Related Work,[0],[0]
They show that adding such features to their essay scorer improves its performance significantly.,2 Related Work,[0],[0]
"Burstein et al. (2010) specifically focus on the impact of entity transition features, as proposed by the entity-grid model for coherence modeling, on the essay scoring task.",2 Related Work,[0],[0]
"They demonstrate that by combining these features with other features related to grammar errors and word usage, the performance of their automated essay scoring system improves.",2 Related Work,[0],[0]
"Likewise, we combine our coherence vectors with other features that are used by a strong essay scorer (Phandi et al., 2015) and show that our coherence vectors improve the performance of this system significantly.
",2 Related Work,[0],[0]
"e0 e1 e2
E0
e3 e4 e5
E1
e6 e7 e8
E2
e9 e10 e12
E3
e13 e14 e15
E4
LOOKUP LOOKUP",2 Related Work,[0],[0]
LOOKUP,2 Related Work,[0],[0]
LOOKUP,2 Related Work,[0],[0]
"LOOKUP
LSTM LSTM LSTM LSTM LSTM
h00 h 1 0 h 2 0 h 0 1 h 1 1 h 2 1 h 0 2 h 1 2 h 2 2 h 0 3 h 1 3 h 2 3 h 0 4 h 1 4 h 2 4
",2 Related Work,[0],[0]
"H0 H1 H2 H3 H4
�f1 �f2 �f3 �f4
d23
CNN
�p",2 Related Work,[0],[0]
"In this section, we describe details of our model.",3 Coherence Model,[0],[0]
"First, we explain how we encode words in their context (Section 3.1).",3 Coherence Model,[0],[0]
"Then we show how we relate sentences (Section 3.2), and finally we explain how we represent coherence based on sentence relations (Section 3.3).",3 Coherence Model,[0],[0]
"A general formulation of our model is a parametric function, �p = Lθ (d), where d is an input document, θ indicates parameters of neural modules, and �p is a vector representation for the coherence of d. Figure 1 illustrates our model.",3 Coherence Model,[0],[0]
We use a lookup table to associate all words in the vocabulary with word embeddings.,3.1 Word and Context Representations,[0],[0]
"The lookup table is initialized by existing pre-trained word embeddings because they capture lexical semantic re-
lations between words.",3.1 Word and Context Representations,[0],[0]
"For sentence si, the lookup table returns matrix Ei whose rows are embeddings of words in si.",3.1 Word and Context Representations,[0],[0]
"A weakness of former lexical coherence models (Somasundaran et al., 2014; Mesgar and Strube, 2016) is that they only rely on semantic relations between words in sentences, regardless of the current context of words.",3.1 Word and Context Representations,[0],[0]
"In order to overcome this limitation, we use a standard unidirectional1 RNN with Long Short-Term Memory (LSTM) cells to encode the current context of words in sentences.",3.1 Word and Context Representations,[0],[0]
"For embedding matrix Ei:
Hi = LSTM � Ei, h n−1 i−1 � ,
where Hi is a list of LSTM states, and hn−1i−1 is the last LSTM state of sentence si−1.",3.1 Word and Context Representations,[0],[0]
Parameter n is the number of words in a sentence.,3.1 Word and Context Representations,[0],[0]
We take state vector hji ∈,3.1 Word and Context Representations,[0],[0]
"Hi as a representation of its input word embedding, ej , that is combined with its preceding word vectors in sentence si.",3.1 Word and Context Representations,[0],[0]
"For sake of brevity, the details of LSTM formulations are explained in Appendix A.",3.1 Word and Context Representations,[0],[0]
The relation between sentences is encoded by the most similar semantic states of sentences.,3.2 Sentence Relation Representations,[0],[0]
"Given two adjacent sentences, two of their LSTM states that have the highest similarity are selected to connect them.",3.2 Sentence Relation Representations,[0],[0]
Those LSTM states refer to the salient semantic information that is shared between sentences.,3.2 Sentence Relation Representations,[0],[0]
"To model this, we follow attention components in neural language models (Bahdanau et al., 2014; Vaswani et al., 2017) where the similarity between the last LSTM state and each of its preceding states is computed to measure the amount of attention that the model should give to its preceding context for generating the next word.",3.2 Sentence Relation Representations,[0],[0]
"More formally, for two adjacent sentences si and si−1, one LSTM state in Hi and one LSTM state in Hi−1 that have the maximum similarity are selected to represent the relation between the sentences:
(�u,�v) =",3.2 Sentence Relation Representations,[0],[0]
argmax,3.2 Sentence Relation Representations,[0],[0]
(�hm∈Hi) (,3.2 Sentence Relation Representations,[0],[0]
"�hn∈Hi−1)
(sim(�hm,�hn)),
where Hi and Hi−1 are LSTM states corresponding to sentences si and si−1.",3.2 Sentence Relation Representations,[0],[0]
"The similarity function, sim, returns the absolute value of the dot
1We use unidirectional RNN to model the way that an English text is read.
product between input vectors,
sim(�hm,�hn) =",3.2 Sentence Relation Representations,[0],[0]
"|�hm · �hn|, (1)
where the function |.| computes the absolute value of its input2.",3.2 Sentence Relation Representations,[0],[0]
"We use the dot product function because in practice it enables our model to calculate the above equations efficiently in parallel and in matrix-space, i.e., directly on Hi and Hi−1.",3.2 Sentence Relation Representations,[0],[0]
"Since this is the details of implementation, we explain matrix-based equations in Appendix B.",3.2 Sentence Relation Representations,[0],[0]
"The absolute value in the similarity function is used to encode semantic relatedness between associated information with vectors, which is independent of the sign of the similarity function (Manning and Schütze, 1999).
",3.2 Sentence Relation Representations,[0],[0]
We represent semantic information that relates two adjacent sentences by accumulating its selected LSTM states in the corresponding sentences.,3.2 Sentence Relation Representations,[0],[0]
"Since averaging in the vector space is an effective way to accumulate information represented in some vectors (Iyyer et al., 2015; Wieting et al., 2016), we compute the average of two identified vectors among the LSTM states of two adjacent sentences to represent semantic information shared by the sentences.",3.2 Sentence Relation Representations,[0],[0]
"More concretely, the vector representation of what relates sentence si to its immediately preceding sentence is obtained by averaging a vector of Hi and a vector of Hi−1 that are identified as highly similar:
�fi = avg(�u,�v) = �u+ �v
2 ,
where �u and �v are selected vectors.",3.2 Sentence Relation Representations,[0],[0]
�fi is the vector representation of what connects si to its immediately preceding sentence.,3.2 Sentence Relation Representations,[0],[0]
"Since sentences in a coherent text are about similar topics and share some semantic information, we compute semantic similarity between adjacent information states, i.e. �fis, to capture how they are changing through a text.",3.3 Coherence Representations,[0],[0]
"We propose to encode changes by a continuous value between 0 and 1, where 1 shows that there is no change and 0 indicates that there is a big semantic drift in a text.",3.3 Coherence Representations,[0],[0]
Any value in between depicts how far a text is semantically changing.,3.3 Coherence Representations,[0],[0]
"Given two adjacent vectors �fi and �fi+1, the degree of continuity between them is:
2In practice, the absolute function is implemented as g(z)",3.3 Coherence Representations,[0],[0]
"= max(0, z)−min(0, z) to be differentiable.
",3.3 Coherence Representations,[0],[0]
"d = sim(�fi, �fi+1)
l ,
where l is the length of input vectors, which is used to prevent large numbers (Vaswani et al., 2017), and sim is the similarity function (Section 3.2).",3.3 Coherence Representations,[0],[0]
"The task of this layer is to check if the salient information that is shared by two adjacent sentences is salient in the subsequent sentence or not.
",3.3 Coherence Representations,[0],[0]
The last layer of our model is a convolutional layer to automatically extract and represent patterns of semantic changes in a text.,3.3 Coherence Representations,[0],[0]
"CNNs have proven useful for various NLP tasks (Collobert et al., 2011; Kim, 2014; Kalchbrenner et al., 2014; Cheng and Lapata, 2016) because of their effectiveness in identifying patterns in their input (Xu et al., 2015).",3.3 Coherence Representations,[0],[0]
"In the case of coherence, the convolution layer can identify coherence patterns that correlate with final tasks (Tien Nguyen and Joty, 2017).",3.3 Coherence Representations,[0],[0]
We use a temporal narrow convolution by applying a kernel filter k of width h to a window of h adjacent transitions over sentences to produce a new coherence feature.,3.3 Coherence Representations,[0],[0]
"This filter is applied to each possible window of transitions in a text to produce a feature map �p, which is a coherence vector.",3.3 Coherence Representations,[0],[0]
"Since we use a standard convolution layer, we explain details of the CNN formulations in Appendix C.",3.3 Coherence Representations,[0],[0]
"In our experiments, we consider two variants of our model:",3.4 Variants of Our Model,[0],[0]
CohLSTM that is the full version of our model as described above; and CohEmb that is an ablation.,3.4 Variants of Our Model,[0],[0]
"CohEmb has no RNN layer, so the model is built directly on word embeddings.",3.4 Variants of Our Model,[0],[0]
"In this model, relations between sentences are made over only content words by eliminating all stop words.",3.4 Variants of Our Model,[0],[0]
Model configurations.,4 Implementation Details,[0],[0]
Our model is implemented in PyTorch3 with CUDA 8.0 support.,4 Implementation Details,[0],[0]
In preprocessing we apply zero-padding to all sentences and documents to make their length equal.,4 Implementation Details,[0],[0]
The vocabulary is limited to the 4000 most frequent words in the training data and all other words are replaced with the unknown token.,4 Implementation Details,[0],[0]
"We use the pre-trained word embeddings released by Zou et al. (2013), which are employed by
3https://pytorch.org
state-of-the-art essay scoring systems.",4 Implementation Details,[0],[0]
"The dimensions of word embeddings and LSTM cells are 50 and 300, respectively.",4 Implementation Details,[0],[0]
The convolution layer uses one filter with size 4.,4 Implementation Details,[0],[0]
"However, optimizing hyperparameters for each task may lead to better performance.",4 Implementation Details,[0],[0]
"For selecting two vectors with the highest similarity from the LSTM states of two adjacent sentences, we capture the similarity between any pair of LSTM states of the sentences as an element in a vector, and then apply a max-pooling layer to this vector of similarities to identify the pair with maximally similar LSTM states.",4 Implementation Details,[0],[0]
Selected LSTM states are used for representing salient information shared by the sentences.,4 Implementation Details,[0],[0]
"In CohEmb, stop words are removed by the SMART English stop word list (Salton, 1971).
",4 Implementation Details,[0],[0]
Training setup.,4 Implementation Details,[0],[0]
We set the mini-batch size to 32 and train the network for 100 epochs.,4 Implementation Details,[0],[0]
At each epoch we evaluate the model on the validation set and select the one with the best performance for test evaluations.,4 Implementation Details,[0],[0]
"We optimize with Adam, with an initial learning rate of 0.01.",4 Implementation Details,[0],[0]
Word vectors are updated during training.,4 Implementation Details,[0],[0]
The dropout method with rate 0.5 is employed for regularization.,4 Implementation Details,[0],[0]
Loss functions are specifically defined for each task.,4 Implementation Details,[0],[0]
"We evaluate our model on two downstream tasks: readability assessment (Section 5.1), in which coherence representations of documents are mapped to coherence scores, and then documents are ranked based on these scores; and essay scoring (Section 5.2), in which the coherence representation of an essay is combined with other features for essay scoring to quantify the quality of the essay.",5 Experiments,[0],[0]
Readability assessment – How difficult is a text to read and understand? – depends on many factors one of which is coherence.,5.1 Readability Assessment,[0],[0]
Texts that are more coherent are supposed to be faster to read and easier to understand.,5.1 Readability Assessment,[0],[0]
"Following earlier research on local coherence (Barzilay and Lapata, 2008; Pitler and Nenkova, 2008; Guinaudeau and Strube, 2013), we evaluate our coherence model on this task by ranking texts with respect to readability, instead of predicting readability scores.",5.1 Readability Assessment,[0],[0]
"More formally, we approach readability assessment as follows:",5.1 Readability Assessment,[0],[0]
"Given a text-pair, which text is easier to read?
",5.1 Readability Assessment,[0],[0]
Compared models.,5.1 Readability Assessment,[0],[0]
"We compare the two variants of our model as described in Section 3.4 with two following state-of-the-art systems:
Mesgar and Strube (2016).",5.1 Readability Assessment,[0],[0]
"This is a graph-based coherence model, in which nodes of a graph indicate sentences of a text, and an edge between two sentence nodes represents the existence of a lexico-semantic relation between two words in the sentences.",5.1 Readability Assessment,[0],[0]
Semantic relations between words are measured by the absolute value of the cosine function over their corresponding pre-trained word embeddings.,5.1 Readability Assessment,[0],[0]
If the similarity value for two word vectors is below a certain threshold4 then the connection between these two words is omitted.,5.1 Readability Assessment,[0],[0]
"Given the graph representation of a text, its coherence is encoded as a vector whose elements are frequencies of different subgraphs in the graph.",5.1 Readability Assessment,[0],[0]
The size of subgraphs is defined by the number of their nodes and is set to five.,5.1 Readability Assessment,[0],[0]
Subgraphs are extracted by a random sampling approach.,5.1 Readability Assessment,[0],[0]
We choose this model for comparison because its intuition is similar to our model.,5.1 Readability Assessment,[0],[0]
"However, this model suffers from the following limitations: word embeddings are considered independently, not in their current context; and a manual threshold is used for connection filtering.",5.1 Readability Assessment,[0],[0]
"We overcome these two weaknesses using the RNN and CNN layers in our model, respectively.
",5.1 Readability Assessment,[0],[0]
De Clercq and Hoste (2016).,5.1 Readability Assessment,[0],[0]
This is the state-of-the-art readability system on the examined dataset.,5.1 Readability Assessment,[0],[0]
It uses a rich set of readability features ranging from surface to semantic text features.,5.1 Readability Assessment,[0],[0]
The ranking is performed by LibSVM in their model.,5.1 Readability Assessment,[0],[0]
"We report their best performance that is achieved by extensive feature engineering and SVM’s parameter optimization.
",5.1 Readability Assessment,[0],[0]
Experimental setup.,5.1 Readability Assessment,[0],[0]
"In Section 3, we formulated our model as �p = Lθ (d) where θ represents parameters of the neural modules (i.e. the CNN and RNN layers) in our model.",5.1 Readability Assessment,[0],[0]
"For this task, we use an output layer to map coherence vector �p to score s which quantifies the degree of the perceived coherence of document d. Formally, the output layer is sd = �u · �p + b where �u and b are the weight vector and bias, respectively.",5.1 Readability Assessment,[0],[0]
"Let document d be more readable than document d�, then the model should ideally produce sd > s�d. We train the parameters of the model by a pairwise
4Like Mesgar and Strube (2016), we set this threshold to 0.9.
ranking approach and define the loss function as:
loss = max {0, 1− sd + sd�} .",5.1 Readability Assessment,[0],[0]
"The parameters of the model are shared to obtain the scores for texts in pair (d, d�).
",5.1 Readability Assessment,[0],[0]
Data.,5.1 Readability Assessment,[0],[0]
We use the readability dataset proposed by De Clercq et al. (2014).,5.1 Readability Assessment,[0],[0]
"It consists of 105 texts collected from the British National Corpus and Wikipedia in four different genres: administrative (e.g. reports and surveys), informative (e.g. articles of newspapers and Wikipedia entries), instructive (e.g. user manuals and guidelines), and miscellaneous (e.g., very technical texts and children’s literature).",5.1 Readability Assessment,[0],[0]
The average number of sentences is about 12 per text.,5.1 Readability Assessment,[0],[0]
"10, 907 pairs of texts are labeled with five fine-grained categories: {−100,−50, 0, 50, 100} indicating that the first text in a pair is respectively much easier, somewhat easier, equally difficult, somewhat difficult, more difficult to read than the second text in the pair.",5.1 Readability Assessment,[0],[0]
Labels of text-pairs are assigned by human judges.,5.1 Readability Assessment,[0],[0]
"Similar to De Clercq and Hoste (2016), we evaluate on the positive and negative labels as two sets of classes resulting in 6, 290 text-pairs in total.",5.1 Readability Assessment,[0],[0]
The original readability dataset does not provide any standard training/validation/test sets.,5.1 Readability Assessment,[0],[0]
"We apply 5-fold cross-validation over this dataset.
",5.1 Readability Assessment,[0],[0]
Evaluation metric.,5.1 Readability Assessment,[0],[0]
"The quality of a model is measured in terms of accuracy, which is the fraction of pairs that are correctly ranked by a model divided by the total number of document-pairs.",5.1 Readability Assessment,[0],[0]
We report the average accuracy over all runs of cross-validation as the final result.,5.1 Readability Assessment,[0],[0]
"We perform a paired t-test to determine if improvements are statistically significant (p < .05).
Results.",5.1 Readability Assessment,[0],[0]
"Table 1 summarizes the results of different systems for the readability assessment task.
",5.1 Readability Assessment,[0],[0]
"CohEmb significantly outperforms the graph-based coherence model proposed by
Mesgar and Strube (2016) by a large margin (6%), showing that our model captures coherence better than their model.",5.1 Readability Assessment,[0],[0]
"In our model, the CNN layer automatically learns which connections are important to be considered for coherence patterns, whereas this is performed in Mesgar and Strube (2016) by defining a threshold for eliminating connections.
",5.1 Readability Assessment,[0],[0]
"CohLSTM significantly outperforms both the coherence model proposed by Mesgar and Strube (2016) and the CohEmb model by 11% and 5%, respectively, and defines a new state-of-the-art on this dataset.",5.1 Readability Assessment,[0],[0]
"CohLSTM, unlike Mesgar and Strube (2016)’s model and CohEmb, considers words of sentences in their sentence context.",5.1 Readability Assessment,[0],[0]
"This supports our intuition that actual context information of words contributes to the perceived coherence of texts.
CohLSTM, which captures exclusively local coherence, even outperforms the readability system proposed by De Clercq and Hoste (2016), which relies on a wide range of lexical, syntactic and semantic features.",5.1 Readability Assessment,[0],[0]
One part of the student assessment process is essay writing where students are asked to write an essay about a given topic known as a prompt.,5.2 Essay Scoring,[0],[0]
An essay scoring system assigns an essay a score reflecting the quality of the essay.,5.2 Essay Scoring,[0],[0]
The quality of an essay depends on various factors including coherence.,5.2 Essay Scoring,[0],[0]
"Following previous studies (Miltsakaki and Kukich, 2004; Lei et al., 2014; Somasundaran et al., 2014; Zesch et al., 2015), we approach this task by combining the coherence vector produced by our model and the feature vector developed by an open-source essay scorer to represent an essay by a vector.",5.2 Essay Scoring,[0],[0]
"The final vector representation of an essay, �x, is mapped to a score by a simple neural regression method as follows:
s = sigmoid(�u · �x+ b),
where �u and b are the weight vector and the bias, respectively.",5.2 Essay Scoring,[0],[0]
"We exactly define vector �x for different examined systems, where we explain compared models for essay scoring.
",5.2 Essay Scoring,[0],[0]
Compared models.,5.2 Essay Scoring,[0],[0]
"We compare variations of our model (Section 3.4) with the following models:
EASE (BLRR).",5.2 Essay Scoring,[0],[0]
"As a baseline we use an open-source essay scoring system, Enhanced AI
Scoring Engine5 (EASE) (Phandi et al., 2015).",5.2 Essay Scoring,[0],[0]
This system was ranked third among all 154 participating teams in the Automated Student Assessment Prize (ASAP) competition and is the best among all open-source participating systems.,5.2 Essay Scoring,[0],[0]
"It employs Bayesian Linear Ridge Regression (BLRR) as its regression method applied to a set of linguistic features grouped in four categories: (i) Frequency-based features: such as the number of characters, the number of words, the number of commas, etc; (ii) POS-based features: the number of POS n-grams; (iii) Word overlap with prompt; (iv) Bag of n-grams: the number of uni-grams and bi-grams.
EASE.",5.2 Essay Scoring,[0],[0]
The difference between this system and EASE (BLRR) is in the employed regression method.,5.2 Essay Scoring,[0],[0]
This system uses a neural regression method as described above.,5.2 Essay Scoring,[0],[0]
"In order to have a similar experimental settings for this task, here, we use feature vectors generated by Phandi et al. (2015) to train our neural regression system.",5.2 Essay Scoring,[0],[0]
"The input of the neural regression function is a nonlinear transformation of the feature vector produced by EASE, �f .",5.2 Essay Scoring,[0],[0]
"Therefore �x = tanh(�w · �f + b).
EASE & CohEmb.",5.2 Essay Scoring,[0],[0]
"This model combines the feature vector computed by EASE, �f , and the coherence vector produced by CohEmb, �p, to have a more reliable representation of an essay.",5.2 Essay Scoring,[0],[0]
"More concretely, the input to our regression function, �x, is obtained as follows:
�h1 = tanh(�w1 · �f + b1), �h2 = �h1 ⊕ �p, �x = tanh(�w2 · �h2 + b2),
where ⊕ indicates the concatenation operatation.",5.2 Essay Scoring,[0],[0]
EASE & CohLSTM.,5.2 Essay Scoring,[0],[0]
The structure of this model is the same as the EASE & CohEmb structure.,5.2 Essay Scoring,[0],[0]
"But the input coherence vector, �p, is produced by CohLSTM.
",5.2 Essay Scoring,[0],[0]
Dong et al. (2017).,5.2 Essay Scoring,[0],[0]
"It is a sentence-document model, which is especially designed for this task.",5.2 Essay Scoring,[0],[0]
"It first encodes each sentence by a vector, which represents whole sentence meanings, and then use an RNN to embed vectors of sentences into a document vector.
",5.2 Essay Scoring,[0],[0]
Experimental setup.,5.2 Essay Scoring,[0],[0]
"The size of the input vector for the regression method, �x, is fixed to 100 and its output size is fixed to 1.",5.2 Essay Scoring,[0],[0]
"Dimensions of other parameters, �w1 and �w2, are set accordingly.",5.2 Essay Scoring,[0],[0]
"The
5https://github.com/edx/ease
loss function is the Mean Squared Error (MSE) between human scores, �H , and scores predicted by our system, �S:
MSE( �H, �S) = 1
N
�",5.2 Essay Scoring,[0],[0]
"( �H − �S)2.
",5.2 Essay Scoring,[0],[0]
"The models are compared for each prompt by running 5-fold cross-validation (Dong et al., 2017).
",5.2 Essay Scoring,[0],[0]
Data.,5.2 Essay Scoring,[0],[0]
We apply our model to a dataset used in the Automated Student Assessment Prize (ASAP) competition run by Kaggle6.,5.2 Essay Scoring,[0],[0]
The essays are associated with scores given by humans and categorized in eight prompts.,5.2 Essay Scoring,[0],[0]
Each prompt can be interpreted as a different essay topic along with different genres.,5.2 Essay Scoring,[0],[0]
"Table 2 summarizes some properties of this dataset.
",5.2 Essay Scoring,[0],[0]
Evaluation metric.,5.2 Essay Scoring,[0],[0]
ASAP adopted Quadratic Weighted Kappa (QWK) as the official evaluation metric.,5.2 Essay Scoring,[0],[0]
This metric measures the agreement between scores predicted by a system and scores assigned by humans.,5.2 Essay Scoring,[0],[0]
QWK considers chance agreements and penalizes large disagreements more than small agreements.,5.2 Essay Scoring,[0],[0]
We use an implementation of QWK that is described in Taghipour and Ng (2016).,5.2 Essay Scoring,[0],[0]
The formulation of QWK are explained in Appendix D. The final reported QWK is the average over QWKs of all prompts.,5.2 Essay Scoring,[0],[0]
"We perform a paired t-test to determine if improvements are statistically significant (p < .05).
Results.",5.2 Essay Scoring,[0],[0]
"Table 3 shows the results of different systems for the essay scoring task.
",5.2 Essay Scoring,[0],[0]
"Both EASE & CohEmb, and EASE & CohLSTM significantly improve EASE, confirming that our proposed representation for coherence is beneficial for essay scoring and
6https://www.kaggle.com/c/asap-aes/ data
improves the performance of the examined essay scoring system.",5.2 Essay Scoring,[0],[0]
"Our model does not beat the state-of-the-art essay scoring system (Dong et al., 2017), which is especially designed for this task and is tuned on this dataset.",5.2 Essay Scoring,[0],[0]
This model learns a vector representation for an input essay so that the vector performs the best for this regression task.,5.2 Essay Scoring,[0],[0]
"In contrast, the core of our best performing essay scoring system, i.e. EASE & CohLSTM, is the feature vector generated by EASE, which has less modeling capacity than a deep learning model like the model proposed by Dong et al. (2017).",5.2 Essay Scoring,[0],[0]
"The reason that we combine our coherence model with EASE, rather than the model proposed by Dong et al. (2017), is that EASE has no notion of coherence.",5.2 Essay Scoring,[0],[0]
"By combining our coherence model with it, we examine if our coherence vector improves its performance or not.
",5.2 Essay Scoring,[0],[0]
"Surprisingly, EASE & CohLSTM works on par with EASE & CohEmb.",5.2 Essay Scoring,[0],[0]
"To gain a better insight, we ablate EASE feature vectors and compare the performance of the coherence models, i.e., CohLSTM, and CohEmb.",5.2 Essay Scoring,[0],[0]
"Of course, coherence vectors on their own are not sufficient for predicting essay scores but this setup shows how much each variant of our model contributes to this task.",5.2 Essay Scoring,[0],[0]
"The two last rows in Table 3 show the results.
",5.2 Essay Scoring,[0],[0]
"CohLSTM outperforms CohEmb on all prompts, which matches the results for readability assessment.",5.2 Essay Scoring,[0],[0]
"This confirms our intuition that integrating the information of the current context of words contributes to coherence measurement.
",5.2 Essay Scoring,[0],[0]
"In terms of average QWK, CohLSTM works similar to EASE; however they behave differently on different prompts.",5.2 Essay Scoring,[0],[0]
"The largest improvement for CohLSTM, with respect to EASE, is obtained on prompt 7 and 8.",5.2 Essay Scoring,[0],[0]
"These two prompts ask for stories about laughter and patience, so corresponding essays can be categorized in the narrative genre (see Table 2).",5.2 Essay Scoring,[0],[0]
"The guidelines of these two prompts, which are publicly available in the Kaggle data, ask human annotators to assign the highest score to essays that are coherent and hold the attention of readers through an essay.",5.2 Essay Scoring,[0],[0]
"This is what our model captures: the sequence of semantic changes in a text, or coherence.
",5.2 Essay Scoring,[0],[0]
"On prompt 5, in contrast, we see the largest deterioration in performance of CohLSTM in comparison to EASE.",5.2 Essay Scoring,[0],[0]
This prompt asks students to describe the mood created by the author of a memoir.,5.2 Essay Scoring,[0],[0]
"Essays are expected to contain specific infor-
mation from the memoir so that an essay with the highest score has the highest coverage of all relevant and specific information from the memoir.",5.2 Essay Scoring,[0],[0]
"Therefore, mentioning the details of the memoir in essays of prompt 5 is more important than coherence for this prompt.",5.2 Essay Scoring,[0],[0]
"This also shows that our model exclusively captures the coherence of a text, which is the goal of this paper.",5.2 Essay Scoring,[0],[0]
We developed a local coherence model that encodes patterns of changes in what semantically relates adjacent sentences.,6 Conclusions,[0],[0]
The main novelty of our approach is that it defines sentence connections based on any semantic concept in sentences.,6 Conclusions,[0],[0]
"In this sense, our model goes beyond entity-based coherence models, which need extra dependencies such as coreference resolution systems.",6 Conclusions,[0],[0]
"Moreover, in contrast to lexical cohesion models, which take words individually, our model encodes words in their sentence context.",6 Conclusions,[0],[0]
Our model relates sentences by means of distant relations between word representations.,6 Conclusions,[0],[0]
The most similar LSTM states in two adjacent sentences are selected to encode the salient semantic concept that relates the sentences.,6 Conclusions,[0],[0]
"The model finally employs a convolutional layer to extract and represent patterns of topic changes across sentences in a text as a coherence vector.
",6 Conclusions,[0],[0]
We evaluate coherence vectors generated by our model on the readability assessment and essay scoring tasks.,6 Conclusions,[0],[0]
"On the former, our model achieves new state-of-the-art results.",6 Conclusions,[0],[0]
"On the latter, it significantly improves the performance of a strong essay scorer.",6 Conclusions,[0],[0]
"We believe the reason that our system works is that it learns which semantic concepts of sentences should be used to relate sentences, and which information about concepts is required to model sentence-to-sentence transitions.",6 Conclusions,[0],[0]
"In future
work we intend to run qualitative experiments on patterns that are extracted by our model to see if they are also linguistically interpretable.",6 Conclusions,[0],[0]
This work has been supported by the German Research Foundation (DFG) as part of the Research Training Group Adaptive Preparation of Information from Heterogeneous Sources (AIPHES) under grant No.,Acknowledgments,[0],[0]
"GRK 1994/1 and the Klaus Tschira Foundation, Heidelberg, Germany.",Acknowledgments,[0],[0]
"We thank Mohammad Taher Pilehvar, Ines Rehbein, and Mark-Christoph Müller for their valuable feedback on earlier drafts of this paper.",Acknowledgments,[0],[0]
We also thank anonymous reviewers for their useful suggestions for improving the quality of the paper.,Acknowledgments,[0],[0]
We propose a local coherence model that captures the flow of what semantically connects adjacent sentences in a text.,abstractText,[0],[0]
We represent the semantics of a sentence by a vector and capture its state at each word of the sentence.,abstractText,[0],[0]
"We model what relates two adjacent sentences based on the two most similar semantic states, each of which is in one of the sentences.",abstractText,[0],[0]
"We encode the perceived coherence of a text by a vector, which represents patterns of changes in salient information that relates adjacent sentences.",abstractText,[0],[0]
"Our experiments demonstrate that our approach is beneficial for two downstream tasks: Readability assessment, in which our model achieves new state-of-the-art results; and essay scoring, in which the combination of our coherence vectors and other taskdependent features significantly improves the performance of a strong essay scorer.",abstractText,[0],[0]
A Neural Local Coherence Model for Text Quality Assessment,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 209–216 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-2033",text,[0],[0]
"Many services such as web search (Leung et al., 2010), recommender systems (Ho et al., 2012), targeted advertising (Lim and Datta, 2013), and rapid disaster response (Ashktorab et al., 2014) rely on the location of users to personalise information and extract actionable knowledge.",1 Introduction,[0],[0]
"Explicit user geolocation metadata (e.g. GPS tags, WiFi footprint, IP address) is not usually available to third-party consumers, giving rise to the need for geolocation based on profile data, text content, friendship graphs (Jurgens et al., 2015) or some combination of these (Rahimi et al., 2015b,a).",1 Introduction,[0],[0]
"The strong geographical bias, most obviously at the language level (e.g. Finland vs. Japan), and more subtly at the dialect level (e.g. in English used in north-west England vs. north-east USA vs. Texas, USA), clearly reflected in language use in social media services such as Twitter, has been used extensively either for geolocation of users (Eisenstein et al., 2010; Roller et al., 2012; Rout et al., 2013; Han et al., 2014; Wing and Baldridge, 2014) or dialectology (Cook et al., 2014; Eisenstein, 2015).",1 Introduction,[0],[0]
"In these methods, a user is often represented by the concatenation of their tweets, and the geolocation model is trained on a very small percentage of explicitly geotagged
tweets, noting the potential biases implicit in geotagged tweets (Pavalanathan and Eisenstein, 2015).
",1 Introduction,[0],[0]
"Lexical dialectology is (in part) the converse of user geolocation (Eisenstein, 2015): given text associated with a variety of regions, the task is to identify terms that are distinctive of particular regions.",1 Introduction,[0],[0]
"The complexity of the task is two-fold: (1) localised named entities (e.g. sporting team names) are not of interest; and (2) without semantic knowledge it is difficult to detect terms that are in general use but have a special meaning in a region.
",1 Introduction,[0],[0]
In this paper we propose a text-based geolocation method based on neural networks.,1 Introduction,[0],[0]
"Our contributions are as follows: (1) we achieve state-of-the-art results on benchmark Twitter geolocation datasets; (2) we show that the model is less sensitive to the specific location discretisation method; (3) we release the first broad-coverage dataset for evaluation of lexical dialectology models; (4) we incorporate our text-based model into a network-based model (Rahimi et al., 2015a) and improve the performance utilising both network and text; and (5) we use the model’s embeddings for extraction of local terms and show that it outperforms two baselines.",1 Introduction,[0],[0]
Related work on Twitter user geolocation falls into two categories: text-based and network-based methods.,2 Related Work,[0],[0]
"Text-based methods make use of the geographical biases of language use, and networkbased methods rely on the geospatial homophily of user–user interactions.",2 Related Work,[0],[0]
"In both cases, the assumption is that users who live in the same geographic area share similar features (linguistic or interactional).",2 Related Work,[0],[0]
"Three main text-based approaches are: (1) the use of gazetteers (Lieberman et al., 2010; Quercini et al., 2010); (2) unsupervised text clustering based on topic models or similar (Eisenstein et al., 2010; Hong et al., 2012; Ahmed et al.,
209
2013); and (3) supervised classification (Ding et al., 2000; Backstrom et al., 2008; Cheng et al., 2010; Hecht et al., 2011; Kinsella et al., 2011; Wing and Baldridge, 2011; Han et al., 2012;",2 Related Work,[0],[0]
"Rout et al., 2013), which unlike gazetteers can be applied to informal text and compared to topic models, scales better.",2 Related Work,[0],[0]
"The classification models often rely on less than 1% of geotagged tweets for supervision and discretise real-valued coordinates into equalsized grids (Serdyukov et al., 2009), administrative regions (Cheng et al., 2010; Hecht et al., 2011; Kinsella et al., 2011; Han et al., 2012, 2014), or flat (Wing and Baldridge, 2011) or hierarchical k-d tree clusters (Wing and Baldridge, 2014).",2 Related Work,[0],[0]
"Network-based methods also use either real-valued coordinates (Jurgens et al., 2015) or discretised regions (Rahimi et al., 2015a) as labels, and use label propagation over the interaction graph (e.g. @-mentions).",2 Related Work,[0],[0]
"More recent methods have focused on representation learning by using sparse coding (Cha et al., 2015) or neural networks (Liu and Inkpen, 2015), utilising both text and network information (Rahimi et al., 2015a).
",2 Related Work,[0],[0]
"Dialect is a variety of language shared by a group of speakers (Wolfram and Schilling, 2015).",2 Related Work,[0],[0]
Our focus here is on geographical dialects which are spoken (and written in social media) by people from particular areas.,2 Related Work,[0],[0]
"The traditional approach to dialectology is to find the geographical distribution of known lexical alternatives (e.g. you, yall and yinz: (Labov et al., 2005; Nerbonne et al., 2008; Gonçalves and Sánchez, 2014; Doyle, 2014; Huang et al., 2015; Nguyen and Eisenstein, 2016)), the shortcoming of which is that the alternative lexical variables must be known beforehand.",2 Related Work,[0],[0]
"There have also been attempts to automatically identify such words from geotagged documents (Eisenstein et al., 2010; Ahmed et al., 2013; Cook et al., 2014; Eisenstein, 2015).",2 Related Work,[0],[0]
"The main idea is to find lexical variables that are disproportionately distributed in different locations either via model-based or statistical methods (Monroe et al., 2008).",2 Related Work,[0],[0]
"There is a research gap in evaluating the geolocation models in terms of their usability in retrieving dialect terms given a geographic region.
",2 Related Work,[0],[0]
"We use a text-based neural approach trained on geotagged Twitter messages that: (a) given a geographical region, identifies the associated lexical terms; and (b) given a text, predicts its location.",2 Related Work,[0],[0]
"We use three existing Twitter user geolocation datasets: (1) GEOTEXT (Eisenstein et al., 2010), (2) TWITTER-US (Roller et al., 2012), and (3) TWITTER-WORLD (Han et al., 2012).",3 Data,[0],[0]
These datasets have been used widely for training and evaluation of geolocation models.,3 Data,[0],[0]
"They are all prepartitioned into training, development and test sets.",3 Data,[0],[0]
"Each user is represented by the concatenation of their tweets, and labeled with the latitude/longitude of the first collected geotagged tweet in the case of GEOTEXT and TWITTER-US, and the centre of the closest city in the case of TWITTER-WORLD.1 GEOTEXT and TWITTER-US cover the continental US, and TWITTER-WORLD covers the whole world, with 9k, 449k and 1.3m users, respectively as shown in Figure 1.2 DAREDS is a dialect-term dataset novel to this research, created from the Dictionary of American Regional English (DARE) (Cassidy et al., 1985).",3 Data,[0],[0]
"DARE consists of dialect regions, their terms and meaning.3",3 Data,[0],[0]
"It is based on dialectal surveys from different regions of the U.S., which are then postprocessed to identify dialect regions and terms.",3 Data,[0],[0]
"In order to construct a dataset based on DARE, we downloaded the web version of DARE, cleaned it, and removed multiword expressions and highly-frequent words (any word which occurred in the top 50k most frequent words, based on a word frequency list (Norvig, 2009).",3 Data,[0],[0]
"For dialect regions that don’t correspond to a single state or set of cities (e.g. South), we mapped it to the most populous cities within each region.",3 Data,[0],[0]
"For example, within the Pacific Northwest dialect region, we manually extracted the most populous cities (Seattle, Tacoma, Portland, Salem, Eugene) and added those cities to DAREDS as subregions.
",3 Data,[0],[0]
The resulting dataset (DAREDS) consists of around 4.3k dialect terms from 99 U.S. dialect regions.,3 Data,[0],[0]
DAREDS is the largest standardised dialectology dataset.,3 Data,[0],[0]
"We use a multilayer perceptron (MLP) with one hidden layer as our location classifier, where the
1The decision as to how a given user is labeled was made by the creators of the original datasets, and has been preserved in this work, despite misgivings about the representativeness of the label for some users.
2The datasets can be obtained from https://github. com/utcompling/textgrounder
3http://www.daredictionary.com/
input is l2 normalised bag-of-words features for a given user.",4 Methods,[0],[0]
"We exclude @-mentions, words with document frequency less than 10, and stop words.",4 Methods,[0],[0]
"The output is either a k-d tree leaf node or k-means discretisation of real-valued coordinates of training locations, the output of which is visualised for TWITTER-US in Figure 2.",4 Methods,[0],[0]
"The hidden layer output provides word (and phrase, as bags of words) embeddings for dialectal analysis.
",4 Methods,[0],[0]
"The number of regions, regularisation strength, hidden layer and mini-batch size are tuned over development data and set to (32, 10−5, 896, 100), (256, 10−6, 2048, 10000) and (930, 10−6, 3720, 10000) for GEOTEXT, TWITTER-US and TWITTER-WORLD, respectively.",4 Methods,[0],[0]
"The parameters are optimised using Adamx (Kingma and Ba, 2014) using Lasagne/Theano (Theano Development Team, 2016).",4 Methods,[0],[0]
"Following Cheng (2010) and Eisenstein (2010), we evaluated the geolocation model using mean and median error in km (“Mean” and “Median” resp.)",4 Methods,[0],[0]
and accuracy within 161km of the actual location (“Acc@161”).,4 Methods,[0],[0]
"Note that lower numbers are better for Mean and Median, and higher numbers better for Acc@161.
",4 Methods,[0],[0]
"4The results reported in Rahimi et al. (2015b; 2015a) for TWITTER-WORLD were over a superset of the dataset; the results reported here are based on the actual dataset.
",4 Methods,[0],[0]
"While the focus of this paper is text-based user geolocation, state-of-the-art results for the three datasets have been achieved with hybrid text+network-based models, where the predictions of the text-based model are fed into a mention network as “dongle” nodes to each user node, providing a personalised geolocation prior for each user (Rahimi et al., 2015a).",4 Methods,[0],[0]
"Note that it would, of course, be possible to combine text and network information in a joint deep learning model (Yang et al., 2016; Kipf and Welling, 2016), which we leave to future work (noting that scalability will potentially be a major issue for the larger datasets).
",4 Methods,[0],[0]
"To test the applicability of the model’s embeddings in dialectology, we created DAREDS.",4 Methods,[0],[0]
The output of the hidden layer of the model is used as embeddings for both location names and dialect terms.,4 Methods,[0],[0]
"Given a dialect region name, we retrieve its nearest neighbours in the embedding space, and compare them to dialect terms associated with that location.",4 Methods,[0],[0]
"We also compare the quality of the embeddings with pre-trained word2vec embeddings and the embeddings from the output layer of LR (logistic regression) (Rahimi et al., 2015b) as baselines.",4 Methods,[0],[0]
"Regions in DAREDS can be very broad (e.g. SouthWest), meaning that words associated with those locations will be used across a large number
GEOTEXT TWITTER-US TWITTER-WORLD
Acc@161 Mean Median Acc@161 Mean Median Acc@161 Mean Median TEXT-BASED METHODS
Proposed method (MLP + k-d tree) 38 844 389 54 554 120 34 1456 415 Proposed method (MLP + k-means) 40 856 380 55 581 91 36 1417 373
(Rahimi et al., 2015b) (LR) 38 880 397 50 686 159 32 1724 530 (Wing and Baldridge, 2014) (uniform) — — — 49 703 170 32 1714 490 (Wing and Baldridge, 2014) (k-d tree) — — — 48 686 191 31 1669 509 (Melo and Martins, 2015) — — — — 702 208 — 1507 502 (Cha et al., 2015) — 581 425 — — — — — — (Liu and Inkpen, 2015) — — — — 733 377 — — —
NETWORK-BASED METHODS
(Rahimi et al., 2015a)",4 Methods,[0],[0]
"MADCEL-W 58 586 60 54 705 116 45 2525 279
TEXT+NETWORK-BASED METHODS
of cities contained within that region.",4 Methods,[0],[0]
"We generate a region-level embedding by simply taking the city names associated with the region, and feeding them as BoW input for LR and MLP and averaging their embeddings for word2vec.",4 Methods,[0],[0]
"We evaluate the retrieved terms by computing recall of DAREDS terms existing in TWITTER-US (1071 terms) at k ∈ {0.05%, 0.1%, 0.2%, 0.5%, 1%, 2%, 5%} of vocabulary size.",4 Methods,[0],[0]
The code and the DAREDS dataset are available at https://github.,4 Methods,[0],[0]
com/afshinrahimi/acl2017.,4 Methods,[0],[0]
The performance of the text-based MLP model with k-d tree and k-means discretisation over the three datasets is shown in Table 1.,5.1 Geolocation,[0],[0]
"The results are also compared with state-of-the-art text-based methods based on a flat (Rahimi et al., 2015b; Cha et al., 2015) or hierarchical (Wing and Baldridge, 2014; Melo and Martins, 2015; Liu and Inkpen, 2015) geospatial representation.",5.1 Geolocation,[0],[0]
Our method outperforms both the flat and hierarchical text-based models by a large margin.,5.1 Geolocation,[0],[0]
"Comparing the two discretisation strategies, k-means outperforms k-d tree by a reasonable margin.",5.1 Geolocation,[0],[0]
"We also incorporated the MLP predictions into a network-based model based on the method of Rahimi et al. (2015a), and improved upon their work.",5.1 Geolocation,[0],[0]
"We analysed the Median error of MLP (k-d tree) over the development users of TWITTER-US in each of the U.S. states as shown
in Figure 3.",5.1 Geolocation,[0],[0]
"The error is highest in states with lower training coverage (e.g. Maine, Montana, Wisconsin, Iowa and Kansas).",5.1 Geolocation,[0],[0]
We also randomly sampled 50 development samples from the 1000 samples with highest prediction errors to check the biases of the model.,5.1 Geolocation,[0],[0]
Most of the errors are the result of geolocating users from Eastern U.S. in Western U.S. particularly in Los Angeles and San Francisco.,5.1 Geolocation,[0],[0]
"We quantitatively tested the quality of the geographical embeddings by calculating the micro-average recall of the k-nearest dialect terms (in terms of the proportion of retrieved dialect terms) given a dialect region, as shown in Figure 4.",5.2 Dialectology,[0],[0]
"Recall at 0.5% is about 3.6%, meaning that we were able to retrieve 3.6% of the dialect terms given the dialect region name in the geographical embedding space.",5.2 Dialectology,[0],[0]
"The embeddings slightly outperform the output layer of logistic regression (LR) (Rahimi et al., 2015b)
and word2vec pre-trained embeddings, but there is still substantial room for improvement.
",5.2 Dialectology,[0],[0]
"Our model is slightly better than both baselines, and can retrieve 3.6% of the correct dialect terms given the region name at 0.5% of the total vocabulary, noting the significant performance gap left for future research.",5.2 Dialectology,[0],[0]
"It is worth noting that the retrieved terms that are not included in DAREDS are not irrelevant: many of them are toponyms (e.g. city names, rivers, companies, or companies) associated with the given region which are not of interest in dialectology.",5.2 Dialectology,[0],[0]
"Equally, some are terms that don’t exist in the DARE dictionary but might be of interest for dialectologists because language use in social media is so dynamic that they won’t be captured by traditional survey-like approaches.",5.2 Dialectology,[0],[0]
"A major shortcoming of this work is that it doesn’t
incorporate sense distinctions and so can’t recover dialect terms that are uniformly distributed but have an idiomatic usage in a particular region.",5.2 Dialectology,[0],[0]
"We proposed a new text geolocation model based on the multilayer perceptron (MLP), and evaluated it over three benchmark Twitter geolocation datasets.",6 Conclusion and Future Work,[0],[0]
We achieved state-of-the-art text-based results over all datasets.,6 Conclusion and Future Work,[0],[0]
We used the parameters of the hidden layer of the neural network as word and phrase embeddings.,6 Conclusion and Future Work,[0],[0]
"We performed a nearest neighbour search on a sample of city names and dialect terms, and showed that the embeddings can be used both to discover dialect terms from a geographic area and to find the geographic area a dialect term is spoken.",6 Conclusion and Future Work,[0],[0]
"To evaluate the geographical embeddings quantitatively, we created DAREDS, a machine-readable version of the DARE dictionary and compared the performance of dialect term retrieval given dialect region name in terms of recall (Figure 4), and compared the performance to the performance in pre-trained word2vec and LR embeddings.",6 Conclusion and Future Work,[0],[0]
We thank the anonymous reviewers for their insightful comments and valuable suggestions.,Acknowledgments,[0],[0]
"This work was funded in part by the Australian Government Research Training Program Scholarship, and the Australian Research Council.",Acknowledgments,[0],[0]
"We propose a simple yet effective textbased user geolocation model based on a neural network with one hidden layer, which achieves state of the art performance over three Twitter benchmark geolocation datasets, in addition to producing word and phrase embeddings in the hidden layer that we show to be useful for detecting dialectal terms.",abstractText,[0],[0]
"As part of our analysis of dialectal terms, we release DAREDS, a dataset for evaluating dialect term detection methods.",abstractText,[0],[0]
A Neural Model for User Geolocation and Lexical Dialectology,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4704–4710 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
4704",text,[0],[0]
Reading involves the integration of noisy perceptual evidence with probabilistic expectations about the likely contents of the text.,1 Introduction,[0],[0]
"Words that are consistent with these expectations are identified more quickly (Ehrlich and Rayner, 1981; Smith and Levy, 2013).",1 Introduction,[0],[0]
"For the reader’s expectations to be maximally effective, they should not only reflect the reader’s past experience with the language (Hale, 2001; MacDonald and Christiansen, 2002), but should also be adapted to the current context.",1 Introduction,[0],[0]
"Optimal adaptation would reflect properties of the text being read, such as genre, topic and writer identity, as well as the general tendency for recently used words and syntactic structures to be reused with higher probability (Bock, 1986; Church, 2000; Dubey et al., 2006).
",1 Introduction,[0],[0]
"Several studies have suggested that readers do in fact adapt their lexical and syntactic predictions to the current context (Otten and Van Berkum, 2008; Fine et al., 2013; Fine and Jaeger, 2016).1 For example, Fine and Jaeger investigated the processing of “garden path” sentences such as (1):
1Recently, Harrington Stack et al. (2018) questioned the robustness of the results of Fine et al. (2013).
",1 Introduction,[0],[0]
"(1) The experienced soldiers warned about the dangers conducted the midnight raid.
",1 Introduction,[0],[0]
The word warned in (1) is initially ambiguous between a main verb interpretation (the soldiers were doing the warning) and a reduced relative clause interpretation (the soldiers were being warned).,1 Introduction,[0],[0]
"When the word conducted is reached, this ambiguity is resolved in favor of the reduced relative parse.",1 Introduction,[0],[0]
Reduced relatives are infrequent constructions.,1 Introduction,[0],[0]
"This makes the disambiguating word conducted unexpected, causing it to be read more slowly than it would be in a context such as (2), in which the words who were indicate early on that only the relative clause parse is possible:
(2) The experienced soldiers who were warned about the dangers conducted the midnight raid.
",1 Introduction,[0],[0]
Fine,1 Introduction,[0],[0]
and Jaeger included a large proportion of reduced relatives in their experiment.,1 Introduction,[0],[0]
"As the experiment progressed, the cost of disambiguation in favor of the reduced relative interpretation decreased, suggesting that readers had come to expect a construction that is normally infrequent.
",1 Introduction,[0],[0]
"Human syntactic expectations have been successfully modeled with syntax-based language models (Hale, 2001; Levy, 2008; Roark et al., 2009).",1 Introduction,[0],[0]
"Recently, language models (LMs) based on recurrent neural networks (RNNs) have been shown to make adequate syntactic predictions (Linzen et al., 2016; Gulordava et al., 2018), and to make comparable reading time predictions to syntax-based LMs (van Schijndel and Linzen, 2018).",1 Introduction,[0],[0]
"In this paper, we propose a simple way to continuously adapt a neural LM, and test the method’s psycholinguistic plausibility.",1 Introduction,[0],[0]
We show that LM adaptation significantly improves our ability to predict human reading times using the LM.,1 Introduction,[0],[0]
"Follow-up experiments with controlled materials show that the LM adapts not only to specific
vocabulary items but also to abstract syntactic constructions, as humans do.",1 Introduction,[0],[0]
"We use a simple method to adapt our LM: at the end of each new test sentence, we update the parameters of the LM based on its cross-entropy loss when predicting that sentence; the new weights are then used to predict the next test sentence.2",2 Method,[0],[0]
"Our baseline LM is a long short-term memory (LSTM; Hochreiter and Schmidhuber, 1997) language model trained on 90 million words of English Wikipedia by Gulordava et al. (2018) (see Supplementary Materials for details).",2 Method,[0],[0]
"For adaptation, we keep the learning rate of 20 used by Gulordava et al. (the gradient is multiplied by this learning rate during weight updates).",2 Method,[0],[0]
"We examine the effect of this parameter in Section 5.2.
",2 Method,[0],[0]
"We tested the model on the Natural Stories Corpus (Futrell et al., 2018), which has 10 narratives with self-paced reading times from 181 native English speakers.",2 Method,[0],[0]
There are two narrative genres in the corpus: fairy tales (seven texts) and documentary accounts (three texts).,2 Method,[0],[0]
We first measured how well the adaptive model predicted upcoming words.,3 Linguistic accuracy,[0],[0]
"We report the model’s perplexity, a quantity which is lower when the LM assigns higher probabilities to the words that in fact occurred.",3 Linguistic accuracy,[0],[0]
"We adapted the model to the first k sentences of each text, then tested it on sentence k+1, for all k. Adaptation dramatically improved test perplexity compared to the non-adaptive version of the model (86.99 vs. 141.49).
",3 Linguistic accuracy,[0],[0]
We next adapted the model to each genre separately.,3 Linguistic accuracy,[0],[0]
"If the model adapts to stylistic or syntactic patterns, we might expect adaptation to be more helpful in the fairy tale than the documentary genre: the Wikipedia corpus that the LM was originally trained on is likely to be more similar in style to the documentary genre.",3 Linguistic accuracy,[0],[0]
"Consistent with this hypothesis, the documentary texts benefited less from adaptation (99.33 to 73.20) than the fairy tales (160.05 to 86.47), though the fact that both saw improvement from adaptation suggests that text-specific adaptation is beneficial even if the genre is similar to the training genre.
",3 Linguistic accuracy,[0],[0]
"2Our code is publicly available at: https://github. com/vansky/neural-complexity.git
Each genre consists of multiple texts.",3 Linguistic accuracy,[0],[0]
"Does adaptation to a particular text lead to catastrophic forgetting (McCloskey and Cohen, 1989), such that the LM overfits to the text and forgets its more general knowledge acquired from the Wikipedia training corpus?",3 Linguistic accuracy,[0],[0]
"This was not the case; in fact, adapting to the entirety of each genre without reverting to the baseline model after each text led to a very slightly better perplexity (fairytales: 86.47, documentaries: 73.20) compared with a setting in which the LM was reverted after each text (fairytales: 86.61, documentaries: 73.63).",3 Linguistic accuracy,[0],[0]
We next tested whether our adaptive LM matches human expectations better than a non-adaptive model.,4 Modeling human expectations,[0],[0]
"Since each reader saw the texts in a different order, we adapted the LM to each text separately: after each story, we reverted to the initial Wikipedia-trained LM and restarted adaptation on the next text.",4 Modeling human expectations,[0],[0]
"If anything, this likely resulted in a conservative estimate of the benefit of adaptation compared to a model that adapts continuously across multiple stories from the same genre, as humans might do.3
We used surprisal as a linking function between the LM’s predictions and human reading times
3We do not distinguish between priming and adaptation in this paper.",4 Modeling human expectations,[0],[0]
"While it may be tempting to think of the LSTM memory cell as a model of priming and of the weight updates as a model of adaptation, Bock and Griffin (2000) provide evidence that priming cannot simply be a function of residual activation and that priming can be driven by longer-term learning (see Tooley and Traxler (2010) for more discussion on priming vs. adaptation).
",4 Modeling human expectations,[0],[0]
"(Hale, 2001; Smith and Levy, 2013).",4 Modeling human expectations,[0],[0]
"Surprisal quantifies how unpredictable each word (wi) is given the preceding words:
surprisal(wi) = −log",4 Modeling human expectations,[0],[0]
"P(wi | w1...wi−1) (1)
We fit the self-paced reading times in the Natural Stories Corpus with linear mixed effects models (LMEMs), a generalization of linear regression (see Supplementary Materials for details).
",4 Modeling human expectations,[0],[0]
"In line with previous work, non-adaptive surprisal was a significant predictor of reading times (p < 0.001) when the model only included other baseline factors (Table 1, Top).",4 Modeling human expectations,[0],[0]
"Adaptive surprisal was a significant predictor of reading times (p < 0.001) over non-adaptive surprisal and all baseline factors (Table 1, Bottom).",4 Modeling human expectations,[0],[0]
"Crucially, nonadaptive surprisal was no longer a significant predictor of reading times once adaptive surprisal was included.",4 Modeling human expectations,[0],[0]
This indicates that the predictions of the adaptive model subsume the predictions of the non-adaptive one.,4 Modeling human expectations,[0],[0]
We have shown that LM adaptation improves our ability to model human expectations as reflected in a self-paced reading time corpus.,5 Does the model adapt to syntax?,[0],[0]
"How much of this improvement is due to adaptation of the model’s syntactic representations (Bacchiani et al., 2006; Dubey et al., 2006) and how much is simply due to the model assigning a higher probability to words that have recently occurred (Kuhn and de Mori, 1990; Church, 2000)?",5 Does the model adapt to syntax?,[0],[0]
"We address this ques-
tion using two syntactic phenomena: reduced relative clauses and the dative alternation.",5 Does the model adapt to syntax?,[0],[0]
We adapted the model independently to random orderings of the critical and filler stimuli used in Experiment 3 of Fine and Jaeger (2016);4 this experiment (described in the Introduction) contained a much higher proportion of reduced relative clauses than their general distribution in English.,5.1 Reduced relative clauses,[0],[0]
We used surprisal as our proxy for reading times.,5.1 Reduced relative clauses,[0],[0]
"Following Fine and Jaeger, we took the mean surprisal over three words in each ambiguous sentence: the disambiguating word and the following two words (e.g., conducted the midnight in example (1)).",5.1 Reduced relative clauses,[0],[0]
"To estimate the magnitude of the syntactic disambiguation penalty while also controlling for lexical content, we subtracted this quantity from the mean surprisal over the exact same words in the paired unambiguous sentence (2).",5.1 Reduced relative clauses,[0],[0]
"Linear regression showed that the disambiguation penalty decreased as the model was exposed to more critical items (item order coefficient: β̂ = −0.0804, p < 0.001), indicating that the LM was adapting to reduced relatives, a syntactic construction without any lexical content.
",5.1 Reduced relative clauses,[0],[0]
"In order to compare our findings more directly with the results given by Fine and Jaeger (2016) (shown in Figure 1), we mimicked their method of plotting reading times.",5.1 Reduced relative clauses,[0],[0]
"First, we fit a linear model of the mean surprisal of each disambiguating region with the number of trials the model had seen in the experiment thus far to account for a general trend of subjects speeding up over the course
4See details in the Supplementary Materials.
of the experiment.",5.1 Reduced relative clauses,[0],[0]
"Then, we plotted the mean residual model surprisal that was left in the disambiguating region in both the ambiguous and unambiguous conditions as the experiment progressed.",5.1 Reduced relative clauses,[0],[0]
The shape of our model’s adaptation to the reduced relative construction (upper curve in Figure 2) matched the human results reported by Fine and Jaeger.,5.1 Reduced relative clauses,[0],[0]
"Like humans, the model showed an initially large adaptation effect, followed by more gradual adaptation thereafter.",5.1 Reduced relative clauses,[0],[0]
Both humans and our model continued to adapt over all the items rather than just at the beginning of the experiment.,5.1 Reduced relative clauses,[0],[0]
"Also like humans, the model’s response to unambiguous items did not change significantly over the course of the experiment (p = 0.91).",5.1 Reduced relative clauses,[0],[0]
"Dative events can be expressed using two roughly equivalent English constructions:
(3) a. Prepositional object (PO): The boy threw the ball to the dog.
",5.2 The dative alternation,[0],[0]
"b. Double object (DO): The boy threw the dog the ball.
",5.2 The dative alternation,[0],[0]
"Work in psycholinguistics has shown that recent experience with one of these variants increases the probability of producing that variant (Bock, 1986; Kaschak et al., 2006) as well as the likelihood of predicting it in reading (Tooley and Bock, 2014).",5.2 The dative alternation,[0],[0]
"To test whether our adaptation method can reproduce this behavior, we generated 200 pairs of dative sentences similar to (3).",5.2 The dative alternation,[0],[0]
"We shuffled 100 DO sentences into 1000 filler sentences sampled from the Wikitext-2 training corpus (Merity et al., 2016) and adapted the model to these 1100 sentences.",5.2 The dative alternation,[0],[0]
"We then froze the weights of the adapted model and tested its predictions for two types of sentences: the PO counterparts of the DO sentences in the adaptation set, which shared the vocabulary of the adaptation set but differed in syntax; and 100 new DO sentences, which shared syntax but no content words with the adaptation set.5
An additional goal of this experiment was to examine the effect of learning rate on adaptation.",5.2 The dative alternation,[0],[0]
During adaptation the model performs a single parameter update after each sentence and does not train until convergence with gradual reduction of the learning rate as would normally be the case during LM training.,5.2 The dative alternation,[0],[0]
"Consequently, the learning
5For additional details as well as the reverse setting (adaptation to PO), see Supplementary Materials.
rate parameter crucially determines the amount of adaptation the model can undertake after each sentence.",5.2 The dative alternation,[0],[0]
"If the learning rate is very low, adaptation will not have any effect; if it is too high, either the model will overfit after each update and will not generalize well, or the model will forget its trained representation as it overshoots the targeted minima.",5.2 The dative alternation,[0],[0]
The optimal rate may differ between lexical and syntactic adaptation.,5.2 The dative alternation,[0],[0]
"Our experiments thus far all used the same learning rate as our original model (20); here, we varied the learning rate on a logarithmic scale between 0.002 and 200.
",5.2 The dative alternation,[0],[0]
The results of this experiment are shown in Figure 3.,5.2 The dative alternation,[0],[0]
The model successfully adapted to the DO construction as well as to the vocabulary of the adaptation sentences.,5.2 The dative alternation,[0],[0]
"This was the case for all of the learning rates except for 200, which resulted in enormous perplexity on both sentence types.",5.2 The dative alternation,[0],[0]
"Both lexical and syntactic adaptation were most successful when the learning rate was around 2, with perplexity reductions of 94% for lexical adaptation and 84% for syntactic adaptation.
",5.2 The dative alternation,[0],[0]
Syntactic adaption was penalized at higher learning rates more than lexical adaptation (compare learning rates of 2 and 20).,5.2 The dative alternation,[0],[0]
"This fragility of syntactic adaptation likely stems from the fact that the model can directly observe the relevant vocabulary but syntax is latent and must be inferred from multiple similar sentences, a generalization which is impeded by overfitting at higher learning rates.",5.2 The dative alternation,[0],[0]
Our analysis of the Natural Stories corpus did not indicate that the model suffered from catastrophic forgetting.,6 Testing for catastrophic forgetting,[0],[0]
"Yet the Natural Stories corpus contained only two genres; to address the issue of catastrophic forgetting more systematically, we used the premise sentences from the MultiNLI corpus (Williams et al., 2018) — a total of 2000 sentences for each of 10 genres.
",6 Testing for catastrophic forgetting,[0],[0]
"For each genre pair G1 and G2 (omitting cases where G1 = G2), we first adapted the baseline Wikipedia model to 1000 sentences of G1 using a learning rate of 2 (shown to be optimal in Section 5.2).",6 Testing for catastrophic forgetting,[0],[0]
We then adapted the model to 1000 sentences of G2.,6 Testing for catastrophic forgetting,[0],[0]
"Finally, we froze the model’s weights and tested its perplexity on the 1000 heldout sentences from G1.
",6 Testing for catastrophic forgetting,[0],[0]
The results averaged across all pairs of genres are plotted in Figure 4.,6 Testing for catastrophic forgetting,[0],[0]
"Unsurprisingly, the model performed best on G1 immediately after adapting to it (middle bar).",6 Testing for catastrophic forgetting,[0],[0]
"Crucially, even after adapting to 1000 sentences of G2 after its last exposure to G1 (right bar), it still modeledG1 much better than the non-adapted model (left bar).",6 Testing for catastrophic forgetting,[0],[0]
These results suggest that catastrophic forgetting is not a concern even with a relatively large amount of data.,6 Testing for catastrophic forgetting,[0],[0]
"Adaptation greatly improved an RNN LM’s word prediction accuracy, in line with other work on LM adaptation (Kneser and Steinbiss, 1993).",7 Discussion,[0],[0]
"We showed that the adapted model was psycholinguistically plausible, in two senses.",7 Discussion,[0],[0]
"First, it improved the correlation between surprisal derived from the model and human reading times, sug-
gesting that the model generated more human-like expectations.",7 Discussion,[0],[0]
"Second, using materials that teased apart lexical content from syntax, we showed that the model adapted both its lexical and its syntactic predictions, in line with findings from human experiments.",7 Discussion,[0],[0]
"Finally, as in other neural-network based models in psychology (Chang et al., 2006), our gradient-based updates naturally incorporate the error-driven nature of syntactic adaptation; while we did not demonstrate this in the current paper, we hypothesize that our model will reproduce the finding that more surprising words lead to greater adaptation (Jaeger and Snider, 2013).
",7 Discussion,[0],[0]
The simplicity of our adaptation method makes it attractive for use in modeling human expectations.,7 Discussion,[0],[0]
"Since adaptive surprisal is strictly superior to non-adaptive surprisal in modeling reading times, it would be a stronger baseline in analyses that aim to demonstrate the contribution of factors other than predictability.
",7 Discussion,[0],[0]
"We used a simple neural adaptation approach, where we performed continuous gradient updates based on the prediction error on the adaptation sentences (see also Krause et al., 2017).",7 Discussion,[0],[0]
"An alternative approach to neural LM adaptation uses recent RNN states in conjunction with the current state to make word predictions (Grave et al., 2017; Merity et al., 2017); a comparison of the two methods using our paradigms may provide insight into their relative strengths and weaknesses.
",7 Discussion,[0],[0]
"Finally, we reverted to the base model after the end of each text in our experiments, forgetting any text-specific adaptation.",7 Discussion,[0],[0]
This mimics the effect of a participant leaving an experiment that had an unusual distribution of syntactic constructions and reverting to their standard expectations.,7 Discussion,[0],[0]
"In practice, however, humans are able to generalize from prior experience when they begin adapting to a new speaker or text if it is similar in some way to their previous experiences.",7 Discussion,[0],[0]
"For example, the model of Jaech and Ostendorf (2018) adapts to environmental factors, so it could potentially draw on independent experiences with female speakers and with lawyer speech in order to initialize a model of adaptation to a new female lawyer (see also Mikolov and Zweig, 2012; Kleinschmidt, 2018).",7 Discussion,[0],[0]
The psycholinguistic plausibility of these models can be tested in future work.,7 Discussion,[0],[0]
It has been argued that humans rapidly adapt their lexical and syntactic expectations to match the statistics of the current linguistic context.,abstractText,[0],[0]
We provide further support to this claim by showing that the addition of a simple adaptation mechanism to a neural language model improves our predictions of human reading times compared to a non-adaptive model.,abstractText,[0],[0]
We analyze the performance of the model on controlled materials from psycholinguistic experiments and show that it adapts not only to lexical items but also to abstract syntactic structures.,abstractText,[0],[0]
A Neural Model of Adaptation in Reading,title,[0],[0]
"Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 196–205, Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics
We present a novel response generation system that can be trained end to end on large quantities of unstructured Twitter conversations. A neural network architecture is used to address sparsity issues that arise when integrating contextual information into classic statistical models, allowing the system to take into account previous dialog utterances. Our dynamic-context generative models show consistent gains over both context-sensitive and non-context-sensitive Machine Translation and Information Retrieval baselines.",text,[0],[0]
"Until recently, the goal of training open-domain conversational systems that emulate human conversation has seemed elusive.",1 Introduction,[0],[0]
"However, the vast quantities of conversational exchanges now available on social media websites such as Twitter and Reddit raise the prospect of building data-driven models that can begin to communicate conversationally.",1 Introduction,[0],[0]
"The work of Ritter et al. (2011), for example, demonstrates that a response generation system can be constructed from Twitter conversations using statistical machine translation techniques, where a status post by a Twitter user is “translated” into a plausible looking response.
",1 Introduction,[0],[0]
"However, an approach such as that presented in Ritter et al. (2011) does not address the challenge of
*The entirety of this work was conducted while at Microsoft Research.
†Corresponding authors: Alessandro Sordoni (sordonia@iro.umontreal.ca) and Michel Galley (mgalley@microsoft.com).
generating responses that are sensitive to the context of the conversation.",1 Introduction,[0],[0]
"Broadly speaking, context may be linguistic or involve grounding in the physical or virtual world, but we here focus on linguistic context.",1 Introduction,[0],[0]
The ability to take into account previous utterances is key to building dialog systems that can keep conversations active and engaging.,1 Introduction,[0],[0]
Figure 1 illustrates a typical Twitter dialog where the contextual information is crucial: the phrase “good luck” is plainly motivated by the reference to “your game” in the first utterance.,1 Introduction,[0],[0]
"In the MT model, such contextual sensitivity is difficult to capture; moreover, naive injection of context information would entail unmanageable growth of the phrase table at the cost of increased sparsity, and skew towards rarely-seen context pairs.",1 Introduction,[0],[0]
"In most statistical approaches to machine translation, phrase pairs do not share statistical weights regardless of their intrinsic semantic commonality.
",1 Introduction,[0],[0]
We propose to address the challenge of contextsensitive response generation by using continuous representations or embeddings of words and phrases to compactly encode semantic and syntactic similarity.,1 Introduction,[0],[0]
"We argue that embedding-based models af-
196
ford flexibility to model the transitions between consecutive utterances and to capture long-span dependencies in a domain where traditional word and phrase alignment is difficult (Ritter et al., 2011).",1 Introduction,[0],[0]
"To this end, we present two simple, context-sensitive response-generation models utilizing the Recurrent Neural Network Language Model (RLM) architecture of (Mikolov et al., 2010).",1 Introduction,[0],[0]
"These models first encode past information in a hidden continuous representation, which is then decoded by the RLM to promote plausible responses that are simultaneously fluent and contextually relevant.",1 Introduction,[0],[0]
"Unlike typical complex task-oriented multi-modular dialog systems (Young, 2002; Stent and Bangalore, 2014), our architecture is completely data-driven and can easily be trained end-to-end using unstructured data without requiring human annotation, scripting, or automatic parsing.
",1 Introduction,[0],[0]
This paper makes the following contributions.,1 Introduction,[0],[0]
We present a neural network architecture for response generation that is both context-sensitive and datadriven.,1 Introduction,[0],[0]
"As such, it can be trained from end to end on massive amounts of social media data.",1 Introduction,[0],[0]
"To our knowledge, this is the first application of a neural-network model to open-domain response generation, and we believe that the present work will lay groundwork for more complex models to come.",1 Introduction,[0],[0]
We additionally introduce a novel multi-reference extraction technique that shows promise for automated evaluation.,1 Introduction,[0],[0]
"Our work naturally lies in the path opened by Ritter et al. (2011), but we generalize their approach by exploiting information from a larger context.",2 Related Work,[0],[0]
Ritter et al. and our work represent a radical paradigm shift from other work in dialog.,2 Related Work,[0],[0]
"More traditional dialog systems typically tease apart dialog management (Young, 2002) from response generation (Stent and Bangalore, 2014), while our holistic approach can be considered a first attempt to accomplish both tasks jointly.",2 Related Work,[0],[0]
"While there are previous uses of machine learning for response generation (Walker et al., 2003), dialog state tracking (Young et al., 2010), and user modeling (Georgila et al., 2006), many components of typical dialog systems remain hand-coded: in particular, the labels and attributes defining dialog states.",2 Related Work,[0],[0]
"In contrast, the dialog state in our neural network model is completely latent and directly optimized towards end-to-end performance.",2 Related Work,[0],[0]
"In this sense,
we believe the framework of this paper is a significant milestone towards more data-driven and less hand-coded dialog processing.
",2 Related Work,[0],[0]
"Continuous representations of words and phrases estimated by neural network models have been applied on a variety of tasks ranging from Information Retrieval (IR) (Huang et al., 2013; Shen et al., 2014), Online Recommendation (Gao et al., 2014b), Machine Translation (MT) (Auli et al., 2013; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014), and Language Modeling (LM) (Bengio et al., 2003; Collobert and Weston, 2008).",2 Related Work,[0],[0]
"Gao et al. (2014a) successfully use an embedding model to refine the estimation of rare phrase-translation probabilities, which is traditionally affected by sparsity problems.",2 Related Work,[0],[0]
"Robustness to sparsity is a crucial property of our method, as it allows us to capture context information while avoiding unmanageable growth of model parameters.
",2 Related Work,[0],[0]
"Our work extends the Recurrent Neural Network Language Model (RLM) of (Mikolov et al., 2010), which uses continuous representations to estimate a probability function over natural language sentences.",2 Related Work,[0],[0]
"We propose a set of conditional RLMs where contextual information (i.e., past utterances) is encoded in a continuous context vector to help generate the response.",2 Related Work,[0],[0]
Our models differ from most previous work in the way the context vector is constructed.,2 Related Work,[0],[0]
"For example, Mikolov and Zweig (2012) and Auli et al. (2013) use a pre-trained topic model.",2 Related Work,[0],[0]
"In our models, the context vector is learned along with the conditional RLM that generates the response.",2 Related Work,[0],[0]
"Additionally, the learned context encodings do not exclusively capture contentful words.",2 Related Work,[0],[0]
"Indeed, even “stop words” can carry discriminative power in this task; for example, all words in the utterance “how are you?” are commonly characterized as stop words, yet this is a contentful dialog utterance.",2 Related Work,[0],[0]
"We give a brief overview of the Recurrent Language Model (RLM) (Mikolov et al., 2010) architecture that our models extend.",3 Recurrent Language Model,[0],[0]
"A RLM is a generative model of sentences, i.e., given sentence s = s1, . . .",3 Recurrent Language Model,[0],[0]
", sT , it estimates:
p(s) = T∏ t=1 p(st|s1, . . .",3 Recurrent Language Model,[0],[0]
", st−1).",3 Recurrent Language Model,[0],[0]
"(1)
The model architecture is parameterized by three weight matrices, ΘRNN = 〈Win,Wout,Whh〉: an input matrixWin, a recurrent matrixWhh and an output matrix Wout, which are usually initialized randomly.",3 Recurrent Language Model,[0],[0]
The rows of the input matrix Win ∈,3 Recurrent Language Model,[0],[0]
RV×K contain the K-dimensional embeddings for each word in the language vocabulary of size V .,3 Recurrent Language Model,[0],[0]
"Let us denote by st both the vocabulary token and its one-hot representation, i.e., a zero vector of dimensionality V with a 1 corresponding to the index of the st token.",3 Recurrent Language Model,[0],[0]
The embedding for st is then obtained by s>t Win.,3 Recurrent Language Model,[0],[0]
The recurrent matrix Whh ∈ RK×K keeps a history of the subsequence that has already been processed.,3 Recurrent Language Model,[0],[0]
"The output matrix Wout ∈ RK×V projects the hidden state ht into the output layer ot, which has an entry for each word in the vocabulary V .",3 Recurrent Language Model,[0],[0]
This value is used to generate a probability distribution for the next word in the sequence.,3 Recurrent Language Model,[0],[0]
"Specifically, the forward pass proceeds with the following recurrence, for t = 1, . . .",3 Recurrent Language Model,[0],[0]
", T :
ht = σ(s>t Win + h > t−1Whh), ot = h > t",3 Recurrent Language Model,[0],[0]
"Wout (2)
where σ is a non-linear function applied elementwise, in our case the logistic sigmoid.",3 Recurrent Language Model,[0],[0]
"The recurrence is seeded by setting h0 = 0, the zero vector.",3 Recurrent Language Model,[0],[0]
"The probability distribution over the next word given the previous history is obtained by applying the softmax activation function:
P (st = w|s1, . . .",3 Recurrent Language Model,[0],[0]
", st−1) = exp(otw)∑V v=1 exp(otv) .",3 Recurrent Language Model,[0],[0]
"(3)
The RLM is trained to minimize the negative loglikelihood of the training sentence s:
L(s) =",3 Recurrent Language Model,[0],[0]
"− T∑ t=1 logP (st|s1, . . .",3 Recurrent Language Model,[0],[0]
", st−1).",3 Recurrent Language Model,[0],[0]
"(4)
The recurrence is unrolled backwards in time using the back-propagation through time (BPTT) algorithm (Rumelhart et al., 1988), and gradients are accumulated over multiple time-steps.",3 Recurrent Language Model,[0],[0]
"We distinguish three linguistic entities in a conversation between two users A and B: the context1 c,
1In this work, the context is purely linguistic, but future work might integrate further contextual information, e.g., geographical location, time information, or other forms of grounding.
",4 Context-Sensitive Models,[0],[0]
the message m and response r.,4 Context-Sensitive Models,[0],[0]
"The context c represents a sequence of past dialog exchanges of any length; then B emits a message m to which A reacts by formulating its response r (see Figure 1).
",4 Context-Sensitive Models,[0],[0]
"We use three context-based generation models to estimate a generation model of the response r, r = r1, . . .",4 Context-Sensitive Models,[0],[0]
", rT , conditioned on past information c and m:
p(r|c,m) = T∏ t=1 p(rt|r1, . . .",4 Context-Sensitive Models,[0],[0]
", rt−1, c,m).",4 Context-Sensitive Models,[0],[0]
"(5)
These three models differ in the manner in which they compose the context-message pair (c,m).",4 Context-Sensitive Models,[0],[0]
"In our first model, dubbed RLMT, we straightforwardly concatenate each utterance c, m, r into a single sentence s and train the RLM to minimize L(s).",4.1 Tripled Language Model,[0],[0]
"Given c and m, we compute the probability of the response as follows: we perform the forward propagation over the known utterances c andm to obtain a hidden state encoding useful information about previous utterances.",4.1 Tripled Language Model,[0],[0]
"Subsequently, we compute the likelihood of the response from that hidden state.
",4.1 Tripled Language Model,[0],[0]
"An issue with this simple approach is that the concatenated sentence s will be very long on average, especially if the context comprises multiple utterances.",4.1 Tripled Language Model,[0],[0]
"Modelling such long-range dependencies with an RLM is difficult and is still considered an open problem (Pascanu et al., 2013).",4.1 Tripled Language Model,[0],[0]
We will consider RLMT as an additional context-sensitive baseline for the models we present next.,4.1 Tripled Language Model,[0],[0]
The above limitation of RLMT can be addressed by strengthening the context bias.,4.2 Dynamic-Context Generative Model I,[0],[0]
"In our second model (DCGM-I), the context and the message are encoded
DCGM-I DCGM-II
into a fixed-length vector representation the is used by the RLM to decode the response.",4.2 Dynamic-Context Generative Model I,[0],[0]
This is illustrated in Figure 3 (left).,4.2 Dynamic-Context Generative Model I,[0],[0]
"First, we consider c andm as a single sentence and compute a single bag-of-words representation bcm ∈ RV .",4.2 Dynamic-Context Generative Model I,[0],[0]
"Then, bcm is provided as input to a multilayered non-linear forward architecture that produces a fixed-length representation that is used to bias the recurrent state of the decoder RLM.",4.2 Dynamic-Context Generative Model I,[0],[0]
"At training time, both the context encoder and the RLM decoder are learned so as to minimize the negative log-probability of the generated response.
",4.2 Dynamic-Context Generative Model I,[0],[0]
"The parameters of the model are ΘDCGM-I = 〈Win,Whh,Wout, {W `f}L`=1〉, where {W `f}L`=1 are the weights for the L layers of the feed-forward context networks.",4.2 Dynamic-Context Generative Model I,[0],[0]
"The fixed-length context vector kL is obtained by forward propagation of the network:
k1 = b>cmW",4.2 Dynamic-Context Generative Model I,[0],[0]
"1f k` = σ(k>`−1W ` f ) for ` = 2, · · · , L
(6)
",4.2 Dynamic-Context Generative Model I,[0],[0]
The rows of W 1f contain the embeddings of the vocabulary.2 These are different from those employed in the RLM and play a crucial role in promoting the specialization of the context encoder to a distinct task.,4.2 Dynamic-Context Generative Model I,[0],[0]
"The hidden layer of the decoder RLM takes the
2Notice that the first layer of the encoder network is linear.",4.2 Dynamic-Context Generative Model I,[0],[0]
"We found that this helps learning the embedding matrix as it reduces the vanishing gradient effect partially due to stacking of squashing non-linearities (Pascanu et al., 2013).
",4.2 Dynamic-Context Generative Model I,[0],[0]
"following form:
ht = σ(h>t−1Whh + kL + s > t Win) (7a)
",4.2 Dynamic-Context Generative Model I,[0],[0]
ot = h>t,4.2 Dynamic-Context Generative Model I,[0],[0]
"Wout (7b)
p(st+1|s1, . . .",4.2 Dynamic-Context Generative Model I,[0],[0]
", st−1, c,m) = softmax(ot) (7c)",4.2 Dynamic-Context Generative Model I,[0],[0]
This model conditions on the previous utterances via biasing the hidden layer state on the context representation kL.,4.2 Dynamic-Context Generative Model I,[0],[0]
Note that the context representation does not change through time.,4.2 Dynamic-Context Generative Model I,[0],[0]
This is useful because: (a) it forces the context encoder to produce a representation general enough to be useful for generating all words in the response and (b) it helps the RLM decoder to remember context information when generating long responses.,4.2 Dynamic-Context Generative Model I,[0],[0]
"Because DCGM-I does not distinguish between c and m, that model has the propensity to underestimate the strong dependency that holds between m and r.",4.3 Dynamic-Context Generative Model II,[0],[0]
Our third model (DCGM-II) addresses this issue by concatenating the two linear mappings of the bag-ofwords representations bc and bm in the input layer of the feed-forward network representing c and m (see Figure 3 right).,4.3 Dynamic-Context Generative Model II,[0],[0]
"Concatenating continuous representations prior to deep architectures is a common strategy to obtain order-sensitive representations (Bengio et al., 2003; Devlin et al., 2014).
",4.3 Dynamic-Context Generative Model II,[0],[0]
"The forward equations for the context encoder are:
k1 =",4.3 Dynamic-Context Generative Model II,[0],[0]
"[b>c W 1f , b > mW 1 f ],
k` = σ(k>`−1W ` f ) for ` = 2, · · · , L
(8)
where [x, y] denotes the concatenation of x and y vectors.",4.3 Dynamic-Context Generative Model II,[0],[0]
"In DCGM-II, the bias on the recurrent hidden state and the probability distribution over the next token are computed as described in Eq. 7.",4.3 Dynamic-Context Generative Model II,[0],[0]
"For computational efficiency and to alleviate the burden of human evaluators, we restrict the context sequence c to a single sentence.",5.1 Dataset Construction,[0],[0]
"Hence, our dataset is composed of “triples” τ ≡",5.1 Dataset Construction,[0],[0]
"(cτ ,mτ , rτ ) consisting of three sentences.",5.1 Dataset Construction,[0],[0]
"We mined 127M context-messageresponse triples from the Twitter FireHose, covering the 3-month period June 2012 through August 2012.
",5.1 Dataset Construction,[0],[0]
Only those triples where context and response were generated by the same user were extracted.,5.1 Dataset Construction,[0],[0]
"To minimize noise, we selected triples that contained at least one frequent bigram that appeared more than 3 times in the corpus.",5.1 Dataset Construction,[0],[0]
This produced a corpus of 29M Twitter triples.,5.1 Dataset Construction,[0],[0]
"Additionally, we hired crowdsourced raters to evaluate approximately 33K candidate triples.",5.1 Dataset Construction,[0],[0]
Judgments on a 5-point scale were obtained from 3 raters apiece.,5.1 Dataset Construction,[0],[0]
This yielded a set of 4232 triples with a mean score of 4 or better that was then randomly binned into a tuning set of 2118 triples and a test set of 2114 triples3.,5.1 Dataset Construction,[0],[0]
"The mean length of responses in these sets was approximately 11.5 tokens, after cleanup (e.g., stripping of emoticons), including punctuation.",5.1 Dataset Construction,[0],[0]
"We evaluate all systems using BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005), and supplement these results with more targeted human pairwise comparisons in Section 6.3.",5.2 Automatic Evaluation,[0],[0]
A major challenge in using these automated metrics for response generation is that the set of reasonable responses in our task is potentially vast and extremely diverse.,5.2 Automatic Evaluation,[0],[0]
The dataset construction method just described yields only a single reference for each status.,5.2 Automatic Evaluation,[0],[0]
"Accordingly, we extend the set of references using an IR approach to mine potential responses, after which we have human judges rate their appropriateness.",5.2 Automatic Evaluation,[0],[0]
"As we see in Section 6.3, it turns out that by optimizing systems towards BLEU using mined multi-references, BLEU rankings align well with human judgments.",5.2 Automatic Evaluation,[0],[0]
"This lays groundwork for interesting future correlation studies.
",5.2 Automatic Evaluation,[0],[0]
Multi-reference extraction We use the following algorithm to better cover the space of reasonable responses.,5.2 Automatic Evaluation,[0],[0]
Given a test triple τ ≡,5.2 Automatic Evaluation,[0],[0]
"(cτ ,mτ , rτ ), our goal is to mine other responses {rτ̃} that fit the context and message pair (cτ ,mτ ).",5.2 Automatic Evaluation,[0],[0]
"To this end, we first select a set of 15 candidate triples {τ̃} using an IR
3The Twitter ids of the tuning and test sets along with the code for the neural network models may be obtained from http://research.microsoft.com/convo/
system.",5.2 Automatic Evaluation,[0],[0]
The IR system is calibrated in order to select candidate triples τ̃ for which both the message mτ̃ and the response rτ̃ are similar to the original message mτ and response rτ .,5.2 Automatic Evaluation,[0],[0]
"Formally, the score of a candidate triple is:
s(τ̃ , τ) =",5.2 Automatic Evaluation,[0],[0]
"d(mτ̃ ,mτ ) (αd(rτ̃ , rτ )+(1−α) ), (9)
where d is the bag-of-words BM25 similarity function (Robertson et al., 1995), α controls the impact of the similarity between the responses and is a smoothing factor that avoids zero scores for candidate responses that do not share any words with the reference response.",5.2 Automatic Evaluation,[0],[0]
We found that this simple formula provided references that were both diverse and plausible.,5.2 Automatic Evaluation,[0],[0]
"Given a set of candidate triples {τ̃}, human evaluators are asked to rate the quality of the response within the new triples {(cτ ,mτ , rτ̃ )}.",5.2 Automatic Evaluation,[0],[0]
"After human evaluation, we retain the references for which the score is 4 or better on a 5 point scale, resulting in 3.58 references per example on average (Table 1).",5.2 Automatic Evaluation,[0],[0]
The average lengths for the responses in the multi-reference tuning and test sets are 8.75 and 8.13 tokens respectively.,5.2 Automatic Evaluation,[0],[0]
"The response generation systems evaluated in this paper are parameterized as log-linear models in a framework typical of statistical machine translation (Och and Ney, 2004).",5.3 Feature Sets,[0],[0]
"These log-linear models comprise the following feature sets:
MT MT features are derived from a large response generation system built along the lines of Ritter et al. (2011), which is based on a phrase-based MT decoder similar to Moses (Koehn et al., 2007).",5.3 Feature Sets,[0],[0]
"Our MT feature set includes the following features that are common in Moses: forward and backward maximum likelihood “translation” probabilities, word and phrase penalties, linear distortion, and a modified Kneser-Ney language model (Kneser and Ney, 1995) trained on Twitter responses.",5.3 Feature Sets,[0],[0]
"For the translation probabilities, we built a very large phrase table of 160.7 million entries by first filtering out Twitterisms (e.g., long sequences of vowels, hashtags), and then selecting candidate phrase pairs using Fisher’s exact test (Ritter et al., 2011).",5.3 Feature Sets,[0],[0]
"We also included MT decoder features specifically motivated by the response generation task: Jaccard distance between source and
target phrase, Fisher’s exact probability, and a score relating the lengths of source and target phrases.
",5.3 Feature Sets,[0],[0]
"IR We also use an IR feature built from an index of triples, whose implementation roughly matches the IRstatus approach described in Ritter et al. (2011): For a test triple τ , we choose rτ̃",5.3 Feature Sets,[0],[0]
as the candidate response iff,5.3 Feature Sets,[0],[0]
"τ̃ = arg maxτ̃ d(mτ ,mτ̃ ).
",5.3 Feature Sets,[0],[0]
CMM Neither MT nor IR traditionally take into account contextual information.,5.3 Feature Sets,[0],[0]
"Therefore, we take into consideration context and message matches (CMM), i.e., exact matches between c, m and r.",5.3 Feature Sets,[0],[0]
We define 8 features as the [1-4]-gram matches between c and the candidate reply r and the [1-4]-gram matches between m and the candidate reply r.,5.3 Feature Sets,[0],[0]
"These exact matches help capture and promote contextual information in the replies.
",5.3 Feature Sets,[0],[0]
"RLMT, DCGM-I, DCGM-II We consider the RLM trained on the concatenated triples, denoted as RLMT (Section 4.1), to be a context-sensitive RLM baseline.",5.3 Feature Sets,[0],[0]
Each neural network model contributes an additional feature corresponding to the likelihood of the candidate response given context and message.,5.3 Feature Sets,[0],[0]
The proposed models are trained on a 4M subset of the triple data.,5.4 Model Training,[0],[0]
The vocabulary consists of the most frequent V = 50K words.,5.4 Model Training,[0],[0]
"In order to speed up training, we use the Noise-Contrastive Estimation (NCE) loss, which avoids repeated summations over V by approximating the probability of the target word (Gutmann and Hyvärinen, 2010).",5.4 Model Training,[0],[0]
"Parameter optimization is done using Adagrad (Duchi et al., 2011) with a mini-batch size of 100 and a learning rate α = 0.1, which we found to work well on held-out data.",5.4 Model Training,[0],[0]
"In order to stabilize learning, we clip the gradients to a fixed range [−10, 10], as suggested in Mikolov et al. (2010).",5.4 Model Training,[0],[0]
"All the parameters of the neural models are sampled from a normal distribution N (0, 0.01) while the recurrent weight Whh is initialized as a
random orthogonal matrix and scaled by 0.01.",5.4 Model Training,[0],[0]
"To prevent over-fitting, we evaluate performance on a held-out set during training and stop when the objective increases.",5.4 Model Training,[0],[0]
"The size of the RLM hidden layer is set to K = 512, where the context encoder is a 512, 256, 512 multilayer network.",5.4 Model Training,[0],[0]
The bottleneck in the middle compresses context information that leads to similar responses and thus achieves better generalization.,5.4 Model Training,[0],[0]
The last layer embeds the context vector into the hidden space of the decoder RLM.,5.4 Model Training,[0],[0]
We evaluate the proposed models by rescoring the n-best candidate responses obtained using the MT phrase-based decoder and the IR system.,5.5 Rescoring Setup,[0],[0]
"In contrast to MT, the candidate responses provided by IR have been created by humans and are less affected by fluency issues.",5.5 Rescoring Setup,[0],[0]
The different n-best lists will provide a comprehensive testbed for our experiments.,5.5 Rescoring Setup,[0],[0]
"First, we augment the n-best list of the tuning set with the scores of the model of interest.",5.5 Rescoring Setup,[0],[0]
"Then, we run an iteration of MERT (Och, 2003) to estimate the log-linear weights of the new features.",5.5 Rescoring Setup,[0],[0]
"At test time, we rescore the test n-best list with the new weights.",5.5 Rescoring Setup,[0],[0]
Table 2 shows the expected upper and lower bounds for this task as suggested by BLEU scores for human responses and a random response baseline.,6.1 Lower and Upper Bounds,[0],[0]
The RANDOM system comprises responses randomly extracted from the triples corpus.,6.1 Lower and Upper Bounds,[0],[0]
"HUMAN is computed by choosing one reference amongst the multi-reference set for each context-status pair.4 Although the scores are lower than those usually reported in SMT tasks, the ranking of the three systems is unambiguous.",6.1 Lower and Upper Bounds,[0],[0]
"The results of automatic evaluation using BLEU and METEOR are presented in Table 3, where some broad patterns emerge.",6.2 BLEU and METEOR,[0],[0]
"First, both metrics indicate that a phrase-based MT decoder outperforms a purely IR approach.",6.2 BLEU and METEOR,[0],[0]
"Second, adding CMM features
4For the human score, we compute corpus-level BLEU with a sampling scheme that randomly leaves out one reference - the human sentence to score - for each reference set.",6.2 BLEU and METEOR,[0],[0]
"This sampling scheme (repeated with 100 trials) is also applied for the MT and RANDOM system so as to make BLEU scores comparable.
to the baseline systems helps.",6.2 BLEU and METEOR,[0],[0]
"Third, the neural network models contribute measurably to improvement: RLMT and DCGM models outperform baselines, and DCGM models provide more consistent gains than RLMT.
MT vs. IR BLEU and METEOR scores indicate that the phrase-based MT decoder outperforms a purely IR approach, despite the fact that IR proposes fluent human generated responses.",6.2 BLEU and METEOR,[0],[0]
This may be because the IR model only loosely captures important patterns between message and response: It ranks candidate responses solely by the similarity of their message with the message of the test triple (§5.3).,6.2 BLEU and METEOR,[0],[0]
"As a result, the top ranked response is likely to drift from the purpose of the original conversation.",6.2 BLEU and METEOR,[0],[0]
"The MT approach, by contrast, more directly models statistical patterns between message and response.
",6.2 BLEU and METEOR,[0],[0]
"CMM MT+CMM, totaling 17 features (9 from MT + 8 CMM), improves 0.38 BLEU points, a 9.5% relative improvement, over the baseline MT model.",6.2 BLEU and METEOR,[0],[0]
"IR+CMM, with 10 features (IR + word penalty + 8 CMM), benefits even more, attaining 1.8 BLEU points and 1.5 METEOR points over the IR baseline.",6.2 BLEU and METEOR,[0],[0]
Figure 4 (a) and (b) plots the magnitude of the learned CMM feature weights for MT+CMM and IR+CMM.,6.2 BLEU and METEOR,[0],[0]
CMM features help in both these hypothesis spaces and especially on the IR n-best list.,6.2 BLEU and METEOR,[0],[0]
"Figure 4 (b) supports the hypothesis formulated in the previous paragraph: Since IR solely captures intermessage similarities, the matches between message and response are important, while context matches help in providing additional gains.",6.2 BLEU and METEOR,[0],[0]
"The phrase-based statistical patterns captured by the MT system do a
good job in explaining away 1-gram and 2-gram message matches (Figure 4 (a)) and the performance gain mainly comes from context matches.",6.2 BLEU and METEOR,[0],[0]
"On the other hand, we observe that 4-gram matches may be important in selecting appropriate responses.",6.2 BLEU and METEOR,[0],[0]
"Inspection of the tuning set reveals instances where responses contain long subsequences of their corresponding messages, e.g., m = “good night best friend, I love you”, r =",6.2 BLEU and METEOR,[0],[0]
"“I love you too, good night best friend”.",6.2 BLEU and METEOR,[0],[0]
"Although infrequent, such higher-order n-gram matches, when they occur, may provide a more robust signal of the quality of the response than 1- and 2-gram matches, given the highly conversational nature of our dataset.
RLMT and DCGM Both RLMT and DCGM models outperform their respective MT and IR baselines.",6.2 BLEU and METEOR,[0],[0]
"Both models also exhibit similar performance and show improvements over the MT+CMM models, albeit using a lower dimensional feature space.",6.2 BLEU and METEOR,[0],[0]
"We believe that their similar performance is due to the limited diversity of MT n-best list together with gains in fluency stemming from the strong language
model provided by the RLM.",6.2 BLEU and METEOR,[0],[0]
"In the case of IR models, on the other hand, there is more headroom for improvement and fluency is already guaranteed.",6.2 BLEU and METEOR,[0],[0]
Any gains must come from context and message matches.,6.2 BLEU and METEOR,[0],[0]
"Hence, RLMT underperforms with respect to both DCGM and IR+CMM.",6.2 BLEU and METEOR,[0],[0]
"The DCGM models appear to have better capacity to retain contextual information and thus achieve similar performance to IR+CMM despite their lack of exact n-gram match information.
",6.2 BLEU and METEOR,[0],[0]
"In the present experimental setting, no striking performance difference can be observed between the two versions of the DCGM architecture.",6.2 BLEU and METEOR,[0],[0]
"If multiple sequences were used as context, we expect that the DCGM-II model would likely benefit more owing to the separate encoding of message and context.
",6.2 BLEU and METEOR,[0],[0]
DCGM+CMM,6.2 BLEU and METEOR,[0],[0]
We also investigated whether mixing exact CMM n-gram overlap with semantic information encoded by the DCGM models can bring additional gains.,6.2 BLEU and METEOR,[0],[0]
DCGM-{I-II}+CMM systems each totaling 10 features show increases of up to 0.48 BLEU points over MT+CMM and up to 0.88 BLEU over the model based on Ritter et al. (2011).,6.2 BLEU and METEOR,[0],[0]
METEOR improvements similarly align with BLEU improvements both for MT and IR lists.,6.2 BLEU and METEOR,[0],[0]
"We take this as evidence that CMM exact matches and DCGM semantic matches interact positively, a finding that comports with Gao et al. (2014a), who show that semantic relationships mined through phrase embeddings correlate positively with classic co-occurrencebased estimations.",6.2 BLEU and METEOR,[0],[0]
"Analysis of CMM feature weights in Figure 4 (c) and (d) suggests that 1-gram matches are explained away by the DCGM model, but that higher order matches are important.",6.2 BLEU and METEOR,[0],[0]
"It appears that DCGM models might be improved by preserving
word-order information in context and message encodings.",6.2 BLEU and METEOR,[0],[0]
Human evaluation was conducted using crowdsourced annotators.,6.3 Human Evaluation,[0],[0]
Annotators were asked to compare the quality of system output responses pairwise (“Which is better?”) in relation to the context and message strings in the 2114 item test set.,6.3 Human Evaluation,[0],[0]
"Identical strings were held out, so that the annotators only saw those outputs that differed.",6.3 Human Evaluation,[0],[0]
"Paired responses were presented in random order to the annotators, and each pair of responses was judged by 5 annotators.
",6.3 Human Evaluation,[0],[0]
"Table 4 summarizes the results of human evaluation, giving the difference in mean scores (pairwise preference margin) between systems and 95% confidence intervals generated using Welch’s t-test.",6.3 Human Evaluation,[0],[0]
Identical strings not shown to raters are incorporated with an automatically assigned score of 0.5.,6.3 Human Evaluation,[0],[0]
"The pattern in these results is clear and consistent: context-sensitive systems (+CMM) outperform non-context-sensitive systems, with preference gains as high as approximately 5.3% in the case of DCGM-II+CMM versus IR, and about 3.1% in the case of DCGM-II+CMM versus MT.",6.3 Human Evaluation,[0],[0]
"Similarly, context-sensitive DCGM systems outperform non-DCGM context-sensitive systems by 1.5% (MT) and 2.3% (IR).",6.3 Human Evaluation,[0],[0]
These results are consistent with the automated BLEU rankings and confirm that our best performing DCGM models outperform both raw baseline and the context-sensitive baseline using CMM features.,6.3 Human Evaluation,[0],[0]
"Table 5 provides examples of responses generated on the tuning corpus by the MT-based DCGM-II+CMM system, our best system in terms of both BLEU and human evaluation.",6.4 Discussion,[0],[0]
Responses from this system are on average shorter (8.95 tokens) than the original human responses in the tuning set (11.5 tokens).,6.4 Discussion,[0],[0]
"Overall, the outputs tend to be generic or commonplace, but are often reasonably plausible in the context as in examples 1-3, especially where context and message contain common conversational elements.",6.4 Discussion,[0],[0]
Example 2 illustrates the impact of context-sensitivity: the word “book” in the response is not found in the message.,6.4 Discussion,[0],[0]
"Nonetheless, longer generated responses are apt to degrade both syntactically and in terms of content.",6.4 Discussion,[0],[0]
"We notice that longer responses are likely to present
information that conflicts either internally within the response itself, or is at odds with the context, as in examples 4-5.",6.4 Discussion,[0],[0]
"This is not unsurprising, since our model lacks mechanisms both for reflecting agent intent in the response and for maintaining consistency with respect to sentiment polarity.",6.4 Discussion,[0],[0]
"Longer context and message components may also result in responses that wander off-topic or lapse into incoherence as in 6-8, especially when relatively low frequency unigrams (“bass”, “threat”) are echoed in the response.",6.4 Discussion,[0],[0]
"In general, we expect that larger datasets and incorporation of more extensive contexts into the model will help yield more coherent results in these cases.",6.4 Discussion,[0],[0]
"Consistent representation of agent intent is outside the scope of this work, but will likely remain a significant challenge.",6.4 Discussion,[0],[0]
"We have formulated a neural network architecture for data-driven response generation trained from social media conversations, in which generation of responses is conditioned on past dialog utterances that provide contextual information.",7 Conclusion,[0],[0]
We have proposed a novel multi-reference extraction technique allowing for robust automated evaluation using standard SMT metrics such as BLEU and METEOR.,7 Conclusion,[0],[0]
"Our context-sensitive models consistently outperform both context-independent and context-sensitive baselines by up to 11% relative improvement in
BLEU in the MT setting and 24% in the IR setting, albeit using a minimal number of features.",7 Conclusion,[0],[0]
"As our models are completely data-driven and self-contained, they hold the potential to improve fluency and contextual relevance in other types of dialog systems.
",7 Conclusion,[0],[0]
Our work suggests several directions for future research.,7 Conclusion,[0],[0]
We anticipate that there is much room for improvement if we employ more complex neural network models that take into account word order within the message and context utterances.,7 Conclusion,[0],[0]
Direct generation from neural network models is an interesting and potentially promising next step.,7 Conclusion,[0],[0]
Future progress in this area will also greatly benefit from thorough study of automated evaluation metrics.,7 Conclusion,[0],[0]
"We thank Alan Ritter, Ray Mooney, Chris Quirk, Lucy Vanderwende, Susan Hendrich and Mouni Reddy for helpful discussions, as well as the three anonymous reviewers for their comments.",Acknowledgments,[0],[0]
We present a novel response generation system that can be trained end to end on large quantities of unstructured Twitter conversations.,abstractText,[0],[0]
"A neural network architecture is used to address sparsity issues that arise when integrating contextual information into classic statistical models, allowing the system to take into account previous dialog utterances.",abstractText,[0],[0]
Our dynamic-context generative models show consistent gains over both context-sensitive and non-context-sensitive Machine Translation and Information Retrieval baselines.,abstractText,[0],[0]
A Neural Network Approach to Context-Sensitive Generation of Conversational Responses,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 339–348, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
"Dependency parsing is an important task for Natural Language Processing (NLP) with application to text classification (Özgür and Güngör, 2010), relation extraction (Bunescu and Mooney, 2005), question answering (Cui et al., 2005), statistical machine translation (Xu et al., 2009), and sentiment analysis (Socher et al., 2013).",1 Introduction,[0],[0]
"A mature parser normally requires a large treebank for training, yet such resources are rarely available and are costly to build.",1 Introduction,[0],[0]
"Ideally, we would be able to construct a high quality parser with less training data, thereby enabling accurate parsing for lowresource languages.
",1 Introduction,[0],[0]
"In this paper we formalize the dependency parsing task for a low-resource language as a domain adaptation task, in which a target resource-poor language treebank is treated as in-domain, while a much larger treebank in a high-resource language forms the out-of-domain data.",1 Introduction,[0],[0]
"In this way, we can apply well-understood domain adaptation techniques to the dependency parsing task.",1 Introduction,[0],[0]
"However, a crucial requirement for domain adaptation is that the in-domain and out-of-domain data have
compatible representations.",1 Introduction,[0],[0]
"In applying our approach to data from several languages, we must learn such a cross-lingual representation.",1 Introduction,[0],[0]
Here we frame this representation learning as part of a neural network training.,1 Introduction,[1.0],['Here we frame this representation learning as part of a neural network training.']
The underlying hypothesis for the joint learning is that there are some shared-structures across languages that we can exploit.,1 Introduction,[0],[0]
"This hypothesis is motivated by the excellent results of the cross-lingual application of unlexicalised parsing (McDonald et al., 2011), whereby a delexicalized parser constructed on one language is applied directly to another language.
",1 Introduction,[0],[0]
Our approach works by jointly training a neural network dependency parser to model the syntax in both a source and target language.,1 Introduction,[1.0],['Our approach works by jointly training a neural network dependency parser to model the syntax in both a source and target language.']
"Many of the parameters of the source and target language parsers are shared, except for a small handful of language-specific parameters.",1 Introduction,[0],[0]
"In this way, the information can flow back and forth between languages, allowing for the learning of a compatible cross-lingual syntactic representation, while also allowing the parsers to mutually correct one another’s errors.",1 Introduction,[1.0],"['In this way, the information can flow back and forth between languages, allowing for the learning of a compatible cross-lingual syntactic representation, while also allowing the parsers to mutually correct one another’s errors.']"
"We include some language-specific components, in order to better model the lexicon of each language and allow learning of the syntactic idiosyncrasies of each language.",1 Introduction,[1.0],"['We include some language-specific components, in order to better model the lexicon of each language and allow learning of the syntactic idiosyncrasies of each language.']"
"Our experiments show that this outperforms a purely supervised setting, on both small and large data conditions, with a gain as high as 10% for small training sets.",1 Introduction,[1.0],"['Our experiments show that this outperforms a purely supervised setting, on both small and large data conditions, with a gain as high as 10% for small training sets.']"
"Our proposed joint training method also out-performs the conventional cascade approach where the parameters between source and target languages are related together through a regularization term (Duong et al., 2015).
",1 Introduction,[0],[0]
"Our model is flexible, allowing easy incorporation of peripheral information.",1 Introduction,[0],[0]
"For example, assuming the presence of a small bilingual dictionary is befitting of a low-resource setting, as this is prototypically one of the first artifacts generated by field linguists.",1 Introduction,[0],[0]
"We incorporate a bilingual dictionary as a set of soft constraints on the
339
model, such that it learns similar representations for each word and its translation(s).",1 Introduction,[0],[0]
"For example, the representation of house in English should be close to haus in German.",1 Introduction,[0],[0]
"We empirically show that adding a bilingual dictionary improves parser performance, particularly when target data is limited.
",1 Introduction,[0],[0]
The final contribution of the paper concerns the learned word embeddings.,1 Introduction,[0],[0]
"We demonstrate that these encode meaningful syntactic phenomena, both in terms of the observable clusters and through a verb classification task.",1 Introduction,[0],[0]
The code for this paper is published as an open source project.1,1 Introduction,[0],[0]
"This work is motivated by the idea of delexicalized parsing, in which a parser is built without any lexical features and trained on a treebank for a resource-rich source language (Zeman et al., 2008).",2 Related Work,[0],[0]
It is then applied directly to parse sentences in the target resource-poor languages.,2 Related Work,[0],[0]
"Delexicalized parsing relies on the fact that identical part-ofspeech (POS) inventories are highly informative of dependency relations, and that there exists shared dependency structures across languages.
",2 Related Work,[0],[0]
Building a dependency parser for a resourcepoor language usually starts with the delexicalized parser and then uses other resources to refine the model.,2 Related Work,[0],[0]
McDonald et al. (2011) and Ma and Xia (2014) exploited parallel data as the bridge to transfer constraints from the source resourcerich language to the target resource-poor languages. Täckström,2 Related Work,[0],[0]
et al. (2012) also used parallel data to induce cross-lingual word clusters which added as features for their delexicalized parser.,2 Related Work,[0],[0]
Durrett et al. (2012) constructed the set of language-independent features and used a bilingual dictionary as the bridge to transfer these features from source to target language. Täckström,2 Related Work,[0],[0]
"et al. (2013) additionally used high-level linguistic features extracted from the World Atlas of Language Structures (WALS) (Dryer and Haspelmath, 2013).
",2 Related Work,[0],[0]
"For low-resource languages, no large parallel corpus is available.",2 Related Work,[0],[0]
"Some linguists are dependency-annotating small amounts of field data, e.g. for Karuk, a nearly-extinct language of Northwest California (Garrett et al., 2013).",2 Related Work,[0],[0]
"Accordingly, we adopt a different resource require-
1http://github.com/longdt219/ universal_dependency_parser
ment: a small treebank in the target low-resource language.
",2 Related Work,[0],[0]
"Domain adaptation or joint-training is a different branch of research, and falls outside the scope of this paper.",2 Related Work,[0],[0]
"Nevertheless, we would like to contrast our work with Senna (Collobert et al., 2011), a neural network framework to perform a variety of NLP tasks such as part-of-speech (POS) tagging, named entity recognition (NER), chunking, and so forth.",2 Related Work,[0],[0]
Both approaches exploit common linguistic properties of the data through joint learning.,2 Related Work,[0],[0]
"However, Collobert et al’s goal is to find a single input representation that can work well for many tasks.",2 Related Work,[0],[0]
Our goal is different: we allow the joint-training inputs to be different but constrain the parameter weights in the upper layer to be identical.,2 Related Work,[0],[0]
"Consequently, our method applies to the task where inputs are different, possibly from different languages or domains.",2 Related Work,[0],[0]
Their method applies for different tasks in the same language/domain where the inputs are fairly similar.,2 Related Work,[0],[0]
This section describes the monolingual neural network dependency parser structure of Chen and Manning (2014).,2.1 Supervised Neural Network Parser,[0],[0]
"This parser achieves excellent performance, and has a highly flexible formulation allowing auxilliary inputs.",2.1 Supervised Neural Network Parser,[0],[0]
"The model is based on a transition-based dependency parser (Nivre, 2006) formulated as a neural-network classifier to decide which transition to apply to each parsing state configuration.2 That is, for each configuration, the selected list of words, POS tags and labels from the Stack, Queue and Arcs are extracted.",2.1 Supervised Neural Network Parser,[1.0],"['The model is based on a transition-based dependency parser (Nivre, 2006) formulated as a neural-network classifier to decide which transition to apply to each parsing state configuration.2 That is, for each configuration, the selected list of words, POS tags and labels from the Stack, Queue and Arcs are extracted.']"
"Each word, POS and label is mapped into a lowdimension vector representation using an embedding matrix, which is then fed into a two-layer neural network classifier to predict the next parsing action.",2.1 Supervised Neural Network Parser,[0],[0]
"The set of parameters for the model is E = {Eword, Epos, Earc} for the embedding layer, W1 for the fully connected cubic hidden layer and W2 for the softmax output layer.",2.1 Supervised Neural Network Parser,[0],[0]
"The model prediction function is
P (Y |X = ~x,W1,W2, E) = softmax ( W2 × cube(W1",2.1 Supervised Neural Network Parser,[0],[0]
"× Φ [~x,E]) )",2.1 Supervised Neural Network Parser,[0],[0]
"(1)
2Our approach is focused on a technique for transfer learning which can be more widely applied to other types of dependency parser (and models, generally) regardless of whether they are transition-based or graph-based.
where cube is a non-linear activation function, Φ is the embedding function that returns a vector representation of parsing state x using an embedding matrix E. We refer the reader to Chen and Manning (2014) for a more detailed description.",2.1 Supervised Neural Network Parser,[0],[0]
"We assume a small treebank in a target resourcepoor language, as well as a larger treebank in the source language.",3 A Joint Interlingual Model,[0],[0]
"Our objective is to learn a model of both languages, subject to the constraint that both models are similar overall, while allowing for some limited language variability.",3 A Joint Interlingual Model,[0],[0]
"Instead of just training two different parsers on source and then on target, we train them jointly, in order to learn an interlingual parser.",3 A Joint Interlingual Model,[0],[0]
"This allows the method to take maximum advantage of the limited treebank data available, resulting in highly accurate predicted parses.
",3 A Joint Interlingual Model,[0],[0]
"Training a monolingual parser as described in section 2.1 requires optimizing the simple cross-entropy learning objective,",3 A Joint Interlingual Model,[0],[0]
"L = −∑|D|i=1 logP (Y = ~y(i)|X = ~x(i)), where P (Y |X) is given by equation 1 and D = {~x(i), ~y(i)}ni=1 is the training data.",3 A Joint Interlingual Model,[0],[0]
"Joint training of a parser over the source and target languages can be achieved by simply adding two such cross-entropy objectives, i.e.,
Ljoint = − |Ds|∑ i=1",3 A Joint Interlingual Model,[0],[0]
logP (Ys = ~y(i)s,3 A Joint Interlingual Model,[0],[0]
"|Xs = ~x(i)s )
",3 A Joint Interlingual Model,[0],[0]
− |Dt|∑ i=1,3 A Joint Interlingual Model,[0],[0]
"logP (Yt = ~y (i) t |Xt = ~x(i)t ) , (2)
where the training data, D = Ds ∪Dt, comprises data in both the source and target language.",3 A Joint Interlingual Model,[0],[0]
However training the model according to equation 2 will result in two independent parsers.,3 A Joint Interlingual Model,[0],[0]
"To enforce similarity between the two parsers, we adopt parameter sharing: the neural network parameters, W1 and W2, are identical in both parsers.",3 A Joint Interlingual Model,[0],[0]
"Thereby
P (Yα|Xα = ~x) = P (Y |X = ~x,W1,W2, Eα) ,
where the subscript α ∈ {s, t} denotes the source or target language.",3 A Joint Interlingual Model,[0],[0]
"We allow the embedding matrix Eα to differ in order to accommodate language-specific features, in terms of the representations of lexical types, Ewords , part-of-speech, E
pos s and dependency arc labels Earcs .",3 A Joint Interlingual Model,[0],[0]
"This reflects
the fact that different languages have different lexicon, parts-of-speech often exhibit different roles, and dependency edges serve different functions, e.g. in Korean a static verb can serve as an adjective (Kim, 2001).",3 A Joint Interlingual Model,[0],[0]
"During training, the languagespecific errors are back propagated through different branches according to the language, guiding learning towards an interlingual representation that informs parsing decisions in both languages.",3 A Joint Interlingual Model,[0],[0]
"The set of parameters for the model is W1,W2, Es, Et where Es, Et are the embedding matrices for the source and target languages.
",3 A Joint Interlingual Model,[0.9999999719820645],"['The set of parameters for the model is W1,W2, Es, Et where Es, Et are the embedding matrices for the source and target languages.']"
"Generally speaking, we can understand the model as building the universal dependency parser that parses the universal language.",3 A Joint Interlingual Model,[0],[0]
"Specifically, the model is the combination of two parts: the universal part (W1,W2) that is shared between the languages, and the conversion part (Es, Et) that maps a language-specific representation into the universal language.",3 A Joint Interlingual Model,[1.0],"['Specifically, the model is the combination of two parts: the universal part (W1,W2) that is shared between the languages, and the conversion part (Es, Et) that maps a language-specific representation into the universal language.']"
"Naturally, we could stack several non-linear layers in the conversion components such that the model can better transform the input into the universal representation; we leave this exploration for future work.",3 A Joint Interlingual Model,[0],[0]
"Currently, our cross-lingual word embeddings are meaningful for a pair of source and target languages.",3 A Joint Interlingual Model,[0],[0]
"However, our model can easily be used for joint training over k > 2 languages.",3 A Joint Interlingual Model,[0],[0]
"We also leave this avenue of enquiry for future work
One concern from equation 2 is that when the source language treebank",3 A Joint Interlingual Model,[0],[0]
"Ds is much bigger than the target language treebank Dt, it is likely to dominate, and consequently, learning will mainly focus on optimizing the source language parser.",3 A Joint Interlingual Model,[0],[0]
"We adjust for this disparity by balancing the two datasets,Ds andDt, during training.",3 A Joint Interlingual Model,[0],[0]
"When selecting mini-batches for online gradient updates, we select an equal number of classification instances from the source and target languages.",3 A Joint Interlingual Model,[0],[0]
"Thus, for each step |Ds| = |Dt|, effectively reweighting the cross-entropy components in (2) to ensure parity between the languages.
",3 A Joint Interlingual Model,[0],[0]
"The other concern is over-fitting, especially when we only have a small treebank in the target language.",3 A Joint Interlingual Model,[0],[0]
"As suggested by Chen and Manning (2014), we apply drop-out, a form of regularization for both source and target language.",3 A Joint Interlingual Model,[0],[0]
"That is, we randomly drop some of the activation units from both hidden layer and input layer.",3 A Joint Interlingual Model,[0],[0]
"Following Srivastava et al. (2014), we randomly dropout 20% of the input layer and 50% of the hid-
den layer.",3 A Joint Interlingual Model,[0],[0]
"Empirically, we observe a substantial improvement applying dropout to the model over MLE or l2 regularization.",3 A Joint Interlingual Model,[0],[0]
"Our model is flexible, enabling us to freely add additional components.",3.1 Incorporating a Dictionary,[1.0],"['Our model is flexible, enabling us to freely add additional components.']"
"In this section, we assume the presence of a bilingual dictionary between the source and target language.",3.1 Incorporating a Dictionary,[0],[0]
"We seek to incorporate this dictionary as a part of model learning, to encode the intuition that if two lexical items are translations of one another, the parser should treat them similarly.3 Recall that the mapping layer is the combination of word, pos and arc embeddings, i.e., Eα = {Ewordα , Eposα , Earcα }.",3.1 Incorporating a Dictionary,[0],[0]
"We can easily add bilingual dictionary constraints to the model in the form of regularization to minimize the l2 distance between word representations, i.e.,∑
〈i,j〉∈D ‖Eword(i)s",3.1 Incorporating a Dictionary,[0],[0]
"− Eword(j)t ‖2F , where D comprises translation pairs, word(i) and word(j).
",3.1 Incorporating a Dictionary,[0],[0]
"When the languages share the same POS tagset and arc set,4 we can also add further constraints such as their language-specific embeddings be close together.",3.1 Incorporating a Dictionary,[0],[0]
"This results a regularised training objective,
Ldict = Ljoint−λ",3.1 Incorporating a Dictionary,[0],[0]
"( ∑ 〈i,j〉∈D ‖Eword(i)s −Eword(j)t ‖2F
+ ‖Eposs − Epost ‖2F + ‖Earcs",3.1 Incorporating a Dictionary,[0],[0]
"− Earct ‖2F ) , (3)
where λ ∈",3.1 Incorporating a Dictionary,[0],[0]
"[0,∞] controls to what degree we bind these words or pos tags or arc labels together, with high λ tying the parameters and small λ allowing independent learning.",3.1 Incorporating a Dictionary,[0],[0]
We expect the best value of λ to fall somewhere between these extremes.,3.1 Incorporating a Dictionary,[0],[0]
"Finally, we use a mini-batch size of 1000 instance pairs and adaptive learning rate trainer, adagrad (Duchi et al., 2011) to build our two separate models corresponding to equations 2 and 3.",3.1 Incorporating a Dictionary,[0],[0]
"In this section, we compare our joint training approach with baseline methods of supervised learning in the target language, and cascaded learning of source and target parsers.
",4 Experiments,[0],[0]
"3However, this is not always the case.",4 Experiments,[0],[0]
"For example, modal or auxiliary verbs in English often have no translations in different languages or map to words with different syntactic functions.
",4 Experiments,[0],[0]
4As was the case for our experiments.,4 Experiments,[0],[0]
"We experiment with the Universal Dependency Treebank (UDT) V1.0 (Nivre et al., 2015), simulating low resource settings.5",4.1 Dataset,[0],[0]
This treebank has many desirable properties for our model: the dependency types (arc labels set) and coarse POS tagset are the same across languages.,4.1 Dataset,[0],[0]
This removes the need for mapping the source and target language tagsets to a common tagset.,4.1 Dataset,[0],[0]
"Moreover, the dependency types are also common across languages allowing evaluation of the labelled attachment score (LAS).",4.1 Dataset,[1.0],"['Moreover, the dependency types are also common across languages allowing evaluation of the labelled attachment score (LAS).']"
"The treebank covers 10 languages,6 with some languages very highly resourced—Czech, French and Spanish have 400k tokens—and only modest amounts of data for other languages—Hungarian and Irish have only around 25k tokens.",4.1 Dataset,[0],[0]
"Cross-lingual models assume English as the source language, for which we have a large treebank, and only a small treebank of 3k tokens exists in each target language, simulated by subsampling the corpus.",4.1 Dataset,[0],[0]
"We compare our approach to a baseline interlingual model based on the same parsing algorithm as presented in section 2.1, but with cascaded training (Duong et al., 2015).",4.2 Baseline Cascade Model,[0],[0]
"This works by first learning the source language parser, and then training the target language parser using a regularization term to minimise the distance between the parameters of the target parser and the source parser (which is fixed).",4.2 Baseline Cascade Model,[0],[0]
"In this way, some structural information from the source parser can be used in the target parser, however it is likely that the representation will be overly biased towards the source language and consequently may not prove as useful for modelling the target.",4.2 Baseline Cascade Model,[0],[0]
"While the Epos and Earc are randomly initialized, we initialize both the source and target language word embeddings Ewords , E word t of our neural network models with pre-trained embeddings.",4.3 Monolingual Word Embeddings,[0],[0]
"This is an advantage since we can incorporate the monolingual data which is often available, even for
5Evaluating on truly resource-poor languages would be preferable to simulation.",4.3 Monolingual Word Embeddings,[0],[0]
"However for ease of training and evaluation, which requires a small treebank in the target language, we simulate the low-resource setting using a small part of the UDT.
6Czech (cs), English (en), Finnish (fi), French (fr), German (de), Hungarian (hu), Irish (ga), Italian (it), Spanish (es), Swedish (sv).
resource-poor languages.",4.3 Monolingual Word Embeddings,[0],[0]
"We collect monolingual data for each language from the Machine Translation Workshop (WMT) data,7",4.3 Monolingual Word Embeddings,[0],[0]
"Europarl (Koehn, 2005) and EU Bookshop Corpus (Skadiņš et al., 2014).",4.3 Monolingual Word Embeddings,[0],[0]
"The size of monolingual data also varies significantly, with as much as 400 million tokens for English and German, and as few as 4 million tokens for Irish.",4.3 Monolingual Word Embeddings,[0],[0]
"We use the skip-gram model (Mikolov et al., 2013b) to induce 50-dimensional word embeddings.",4.3 Monolingual Word Embeddings,[0],[0]
"For the extended model as described in section 3.1, we also need a bilingual dictionary.",4.4 Bilingual Dictionary,[0],[0]
"We extract dictionaries from PanLex (Kamholz et al., 2014) which currently covers around 1300 language varieties and about 12 million expressions.",4.4 Bilingual Dictionary,[0],[0]
This dataset is growing and aims at covering all languages in the world and up to 350 million expressions.,4.4 Bilingual Dictionary,[0],[0]
"The translations in PanLex come from various sources such as glossaries, dictionaries, automatic inference from other languages, etc.",4.4 Bilingual Dictionary,[0],[0]
"Naturally, the bilingual dictionary size varies greatly among resource-poor and resource-rich languages.",4.4 Bilingual Dictionary,[0],[0]
Joint training with a dictionary (see equation 3) includes a regularization sensitivity parameter λ.,4.5 Regularization Parameter Tuning,[0],[0]
"This parameter controls to what extent we should bind the source words and their target translation, common POS tags and arcs together.",4.5 Regularization Parameter Tuning,[0],[0]
In this section we measure the sensitivity of our approach with respect to this parameter.,4.5 Regularization Parameter Tuning,[0],[0]
"In a real world sce-
7http://www.statmt.org/wmt14/
nario, getting development data to tune this parameter is difficult.",4.5 Regularization Parameter Tuning,[0],[0]
"Thus, we want a parameter that can work well cross-lingually.",4.5 Regularization Parameter Tuning,[0],[0]
"To simulate this, we only tune the parameter on one language and apply it directly to different languages.",4.5 Regularization Parameter Tuning,[0],[0]
"We trained on a small Swedish treebank with 1k tokens, testing several different values of λ.",4.5 Regularization Parameter Tuning,[0],[0]
We evaluated on the Swedish development dataset.,4.5 Regularization Parameter Tuning,[0],[0]
Figure 1 shows the labelled attachment score (LAS) for different λ.,4.5 Regularization Parameter Tuning,[0],[0]
It’s clearly visible that λ = 0.0001 gives the maximum LAS on the development set.,4.5 Regularization Parameter Tuning,[0],[0]
"Thus, we use this value for all the experiments involving a dictionary hereafter.",4.5 Regularization Parameter Tuning,[0],[0]
For our initial experiments we assume that we have only a small target treebank with 3000 tokens (around 200 sentences).,4.6 Results,[1.0],['For our initial experiments we assume that we have only a small target treebank with 3000 tokens (around 200 sentences).']
Ideally the much larger source language (English) treebank should be able to improve parser performance versus simple supervised learning on such a small collection.,4.6 Results,[0],[0]
"We apply the joint model (equation 2) and joint model with the dictionary constraints (equation 3) for each target language,
The results are reported in Table 1.",4.6 Results,[0],[0]
"The supervised neural network dependency parser performed worst, as expected, and the baseline cascade model consistently outperformed the supervised model on all languages by an average margin of 5.6% (absolute).8 The joint model also consistently out-performed both baselines giving a further 1.9% average improvement over the cascade.",4.6 Results,[1.0],"['The supervised neural network dependency parser performed worst, as expected, and the baseline cascade model consistently outperformed the supervised model on all languages by an average margin of 5.6% (absolute).8 The joint model also consistently out-performed both baselines giving a further 1.9% average improvement over the cascade.']"
"This was despite the fact that the cascaded model had the benefit of tuning for the regularization parameters on a development corpus, while the joint model had no parameter tuning.",4.6 Results,[0],[0]
"Note that the improvement varies substantially across languages, and is largest for Czech but is only minor for Swedish.",4.6 Results,[0],[0]
"The joint model with the bilingual dictionary outperforms the joint model, however, the improvement is modest (0.7%).",4.6 Results,[0],[0]
"Nevertheless, this model gives substantial improvements compared with the cascaded and the supervised model (2.6% and 8.2%).",4.6 Results,[0],[0]
"In section 4.6, we used a 3k token treebank in the target language.",5.1 Learning Curve,[0],[0]
"What if we have more or less
8We use absolute percentage comparisons herein.
target language data?",5.1 Learning Curve,[0],[0]
Figure 2 shows the learning curve with respect to various models on different data sizes averaged over all target languages.,5.1 Learning Curve,[1.0],['Figure 2 shows the learning curve with respect to various models on different data sizes averaged over all target languages.']
"For small datasets of 1k training tokens, the cascaded model, joint model and joint + dict model performed similarly well, out-performing the supervised model by about 10% (absolute).",5.1 Learning Curve,[1.0],"['For small datasets of 1k training tokens, the cascaded model, joint model and joint + dict model performed similarly well, out-performing the supervised model by about 10% (absolute).']"
"With more training data, we see interesting changes to the relative performance of the different models.",5.1 Learning Curve,[0],[0]
"While the baseline cascade model still outperforms the supervised model, the improvement is diminishing, and by 15k, the difference is only 2.9%.",5.1 Learning Curve,[1.0],"['While the baseline cascade model still outperforms the supervised model, the improvement is diminishing, and by 15k, the difference is only 2.9%.']"
"On the other hand, compared with the supervised model, the joint and joint + dict models perform consistently well at all sizes, maintaining an 8% lead at 15k.",5.1 Learning Curve,[0],[0]
"This shows the superiority of joint training compared with single language training.
",5.1 Learning Curve,[0],[0]
"To understand this pattern of performance differences for the cascade versus the joint model, one needs to consider the cascade model formulation.",5.1 Learning Curve,[0],[0]
"In this approach, the target language parameters are tied (softly) with the source language
parameters through regularization.",5.1 Learning Curve,[1.00000003911215],"['In this approach, the target language parameters are tied (softly) with the source language parameters through regularization.']"
"This is a benefit for small datasets, providing a smoothing function to limit overtraining.",5.1 Learning Curve,[0],[0]
"However, when we have more training data, these constraints limit the capacity of the model to describe the target data.",5.1 Learning Curve,[0],[0]
"This is compounded by the problem that the source representation may not be appropriate for modelling the target language, and there is no way to correct for this.",5.1 Learning Curve,[0],[0]
"In contrast the joint model learns a mutually compatible representation automatically during joint training.
",5.1 Learning Curve,[1.000000024258561],['In contrast the joint model learns a mutually compatible representation automatically during joint training.']
The performance results for the joint model with and without the dictionary are similar overall.,5.1 Learning Curve,[0],[0]
"Only on small datasets (1k, 3k), is the difference notable.",5.1 Learning Curve,[0],[0]
"From 5k tokens, the bilingual dictionary doesn’t confer additional information, presumably as there is sufficient data for learning syntactic word representations.",5.1 Learning Curve,[0],[0]
"Moreover, translation entries exist between syntactically related word types as well as semantically related pairs, with the latter potentially limiting the beneficial effect of the dictionary.
",5.1 Learning Curve,[0],[0]
"When training on all the target language data, the supervised model does well, surpassing the cascade model.",5.1 Learning Curve,[0],[0]
"Surprisingly, the joint models outperform slightly, yielding a 0.4% improvement.",5.1 Learning Curve,[0],[0]
"This is an interesting observation suggesting that our method has potential for use not only for low resource problems, but also high resource settings.",5.1 Learning Curve,[0],[0]
"In the above experiments, we used the universal POS tagset for all the languages in the corpus.",5.2 Different Tagsets,[0],[0]
"However, for some languages,9 the UDT also provides language specific POS tags.",5.2 Different Tagsets,[0],[0]
We use this data to test the relative performance of the model using a universal tagset cf.,5.2 Different Tagsets,[0],[0]
language specific tagsets.,5.2 Different Tagsets,[0],[0]
"In this experiment, we applied the same joint model (see §3) but with a language specific tagset instead of UPOS for these languages.",5.2 Different Tagsets,[0],[0]
"We expect the joint
9en, cs, fi, ga, it and sv.
model to automatically learn to project the different tagsets into a common space, i.e., implicitly learn a tagset mapping between languages.",5.2 Different Tagsets,[0],[0]
Figure 3 shows the learning curve comparing the joint model with the two types of POS tagsets.,5.2 Different Tagsets,[0],[0]
"For the small dataset, it is clear that the data is insufficient for the model to learn a good tagset mapping, especially for a morphologically rich language like Czech.",5.2 Different Tagsets,[0],[0]
"However, with more data, the model is better able to learn the tagset mapping as part of joint training.",5.2 Different Tagsets,[1.0],"['However, with more data, the model is better able to learn the tagset mapping as part of joint training.']"
"Beyond 15k tokens, the joint model using the language specific POS tagset outperforms UPOS.",5.2 Different Tagsets,[1.0],"['Beyond 15k tokens, the joint model using the language specific POS tagset outperforms UPOS.']"
"Clearly there is some information lost in the UPOS tagset, although the UPOS mapping simultanously provides implicit linguistic supervision.",5.2 Different Tagsets,[0],[0]
"This explains why the UPOS might be useful in small data scenarios, but detrimental at scale.",5.2 Different Tagsets,[0],[0]
Using all the target data (“All”) the language specific POS provides a 1% (absolute) gain over UPOS.,5.2 Different Tagsets,[0],[0]
"As described in section 3, we can consider our joint model as the combination of two parts: a universal parser and a language-specific embedding Es or Et that converts the source and target language into the universal representation.",5.3 Universal Representation,[0],[0]
We now seek to analyse qualitatively this universal representation through visualization.,5.3 Universal Representation,[0],[0]
"For this purpose we use a joint model of English and French, using all the available French treebank (more than 350k
tokens) as well as a bilingual dictionary.10 Figure 4 shows the t-SNE (Van Der Maaten, 2014) projection of the 50 dimensional word embeddings in both languages.",5.3 Universal Representation,[0],[0]
We can see that English and French are mixed nicely together.,5.3 Universal Representation,[1.0],['We can see that English and French are mixed nicely together.']
"The colouring denotes the POS tag, showing clearly that the words with similar POS tags are grouped together regardless of languages.",5.3 Universal Representation,[0],[0]
"This is partially understandable since word embeddings for dependency parsing need to convey the dependency context rather than surrounding words, as in most distributional embedding models.",5.3 Universal Representation,[0],[0]
"Words having similar dependency relation should be grouped together as they are treated similarly by the parser.
",5.3 Universal Representation,[0],[0]
"Some of the learned cross-lingual wordembeddings are shown in Table 2, which includes the five nearest neighbours to selected English words according to the monolingual word embedding (section 4.3) and our cross-lingual dependency word embeddings, trained using PanLex.",5.3 Universal Representation,[1.0],"['Some of the learned cross-lingual wordembeddings are shown in Table 2, which includes the five nearest neighbours to selected English words according to the monolingual word embedding (section 4.3) and our cross-lingual dependency word embeddings, trained using PanLex.']"
The monolingual sets appear to be strongly characterised by distributional similarity.,5.3 Universal Representation,[0],[0]
"The crosslingual embeddings display greater semantic similarity, while being more variable morphosyntactically.",5.3 Universal Representation,[0],[0]
"In many cases, the top five words of English and French are translations of each other, but with varying inflectional endings in the French forms.",5.3 Universal Representation,[0],[0]
"For example, “buy” vs “vendez” or “invest” vs “in-
10We also visualized the cross-lingual word embeddings without the dictionary, however the results were rather odd.",5.3 Universal Representation,[0],[0]
"Although we saw coherent POS clusters, the two languages were largely disjoint.",5.3 Universal Representation,[0],[0]
"We speculate that many components of the embeddings are use for only one language, and these outnumber the shared components, and thus more careful projection is needed for meaningful visualisation.
vestir”.",5.3 Universal Representation,[0],[0]
This is a direct consequence of incorporating the bilingual lexicon.,5.3 Universal Representation,[0],[0]
"Moreover, the top five closest words of both English and French mostly have the same part of speech.",5.3 Universal Representation,[0],[0]
"This is consistent with the finding in Figure 4.
",5.3 Universal Representation,[0],[0]
Levin (1993) has shown that there is a strong connection between a verb’s meaning and its syntactic behaviour.,5.3 Universal Representation,[0],[0]
"We compare the English side of our cross-lingual dependency based word embeddings with various other pre-trained monolingual English word embeddings and our monolingual embedding (section 4.3) on Verb-143 dataset (Baker et al., 2014).",5.3 Universal Representation,[0],[0]
This dataset contains 143 pairs of verbs that are manually given score from 1 to 10 according to the meaning similarity.,5.3 Universal Representation,[1.0],['This dataset contains 143 pairs of verbs that are manually given score from 1 to 10 according to the meaning similarity.']
"Table 3 shows the Pearson correlation
with human judgment for our embeddings and other pre-trained embeddings.",5.3 Universal Representation,[0],[0]
"As expected, our cross-lingual embeddings out-perform others embeddings on this dataset.",5.3 Universal Representation,[0],[0]
"This is partly because the syntactic behaviour is well encoded in our word embeddings through dependency relation.
",5.3 Universal Representation,[0],[0]
"Our embeddings encode not just cross-lingual correspondences, but also capture dependency relations which we expect might be beneficial for other NLP tasks based on dependency parsing, e.g., cross-lingual semantic role labelling where long-distance relationship can be captured by word embedding.",5.3 Universal Representation,[1.0],"['Our embeddings encode not just cross-lingual correspondences, but also capture dependency relations which we expect might be beneficial for other NLP tasks based on dependency parsing, e.g., cross-lingual semantic role labelling where long-distance relationship can be captured by word embedding.']"
"In this paper, we present a training method for building a dependency parser for a resourcepoor language using a larger treebank in a highresource language.",6 Conclusion,[0],[0]
"Our approach takes advantage of the shared structure among languages to learn a universal parser and language-specific mappings to the lexicon, parts of speech and dependency arcs.",6 Conclusion,[0],[0]
"Compared with supervised learning, our joint model gives a consistent 8-10% improvement over several different datasets in simulation lowresource scenarios.",6 Conclusion,[1.0],"['Compared with supervised learning, our joint model gives a consistent 8-10% improvement over several different datasets in simulation lowresource scenarios.']"
"Interestingly, some small but consistent gains are still realised by joint crosslingual training even on large complete treebanks.",6 Conclusion,[0],[0]
This suggests that our approach has utility not just in low resource settings.,6 Conclusion,[0],[0]
"Our joint model is flexible, allowing the incorporation of a bilingual dictionary, which results in small improvements particularly for tiny training scenarios.
",6 Conclusion,[0],[0]
"As the side-effect of training our joint model, we obtain cross-lingual word embeddings specialized for dependency parsing.",6 Conclusion,[1.0],"['As the side-effect of training our joint model, we obtain cross-lingual word embeddings specialized for dependency parsing.']"
"We expect these embeddings to be beneficial to other syntatic and se-
mantic tasks.",6 Conclusion,[0.9999999784729381],['We expect these embeddings to be beneficial to other syntatic and se- mantic tasks.']
"In future work, we plan to extend joint training to several languages, and further explore the idea of learning and exploiting crosslingual embeddings.",6 Conclusion,[1.0],"['In future work, we plan to extend joint training to several languages, and further explore the idea of learning and exploiting crosslingual embeddings.']"
This work was supported by the University of Melbourne and National ICT Australia (NICTA).,Acknowledgments,[0],[0]
Trevor Cohn is the recipient of an Australian Research Council Future Fellowship (project number FT130101105).,Acknowledgments,[0],[0]
"Accurate dependency parsing requires large treebanks, which are only available for a few languages.",abstractText,[0],[0]
We propose a method that takes advantage of shared structure across languages to build a mature parser using less training data.,abstractText,[0],[0]
"We propose a model for learning a shared “universal” parser that operates over an interlingual continuous representation of language, along with language-specific mapping components.",abstractText,[0],[0]
"Compared with supervised learning, our methods give a consistent 8-10% improvement across several treebanks in low-resource simulations.",abstractText,[0],[0]
A Neural Network Model for Low-Resource Universal Dependency Parsing,title,[0],[0]
