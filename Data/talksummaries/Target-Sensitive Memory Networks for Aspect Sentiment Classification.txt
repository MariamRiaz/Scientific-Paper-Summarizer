32	28	We will give a theoretical insight about this problem with MNs in Section 3.
33	29	In this work, we aim to solve this problem.
34	38	To distinguish it from the aforementioned targetedcontext detection problem as shown by sentence (1), we refer to the problem in (2), (3) and (4) as the target-sensitive sentiment (or target-dependent sentiment) problem, which means that the sentiment polarity of a detected/attended context word is conditioned on the target and cannot be directly inferred from the context word alone, unlike “excellent” and “ridiculous”.
35	29	To address this problem, we propose target-sensitive memory networks (TMNs), which can capture the sentiment interaction between targets and contexts.
39	75	The model design follows previous studies (Sukhbaatar et al., 2015; Tang et al., 2016) except that a different attention alignment function is used (shown in Eq.
42	50	Input Representation: Given a target aspect t, an embedding matrix A is used to convert t into a vector representation, vt (vt = At).
47	68	Attention: Attention can be obtained based on the above input representation.
49	64	In this manner, attention α = {α1, α2, ..αn} is represented as a vector of probabilities, indicating the weight/importance of context words towards a given target.
50	24	Output Representation: Another embedding matrixC is used for generating the individual (output) continuous vector ci (ci = Cxi) for each context word xi.
56	61	The analysis can be generalized to many existing MNs as long as their improvements are on attention α only.
60	97	They are linearly combined to determine the final sentiment score s. This can be problematic in ASC.
61	30	First, an aspect word often expresses no sentiment, for example, “screen”.
67	20	We also have two possible context words “high” and “low” (denoted as h and l).
68	21	As these two sentiment words can modify both aspects, we can construct four snippets “high price”, “low price”, “high resolution” and “low resolution”.
77	39	For example, in sentences (2) and (3) in Section 1, when price is targeted, the main attention will be placed on “high”.
86	29	For example, in the sentence “the price is high”, adjusting the weights of context words “the” and “is” will neither help solve the problem nor be intuitive to do so.
89	26	Non-linear Projection (NP): This is the first approach that utilizes a non-linear projection to capture the interplay between an aspect and its context.
99	33	We thus name it Contextual Non-linear Projection (CNP).
104	17	However, one potential disadvantage is that this setting uses the same set of vector representations (learned by embeddings C) for multiple purposes, i.e., to learn output (context) representations and to capture the interplay between contexts and aspects.
109	16	Interaction Term (IT): The third approach is to formulate explicit target-context sentiment interaction terms.
118	49	The key advantage is that now the sentiment effect is explicitly dependent on its target and context.
119	38	For example, 〈dh, dp〉 can help shift the final sentiment to negative and 〈dh, dr〉 can help shift it to positive.
129	16	Joint Coupled Interaction (JCI): A natural variant of the above model is to replace ei with ci, which means to learn a joint output representation.
139	24	(b) TCS interaction can be calculated by other modeling functions.
141	17	(c) One may ask whether we can use fewer embeddings or just use one universal embedding to replace A, C and D (the definition of D can be found in the introduction of IT).
144	23	But merging D and A/C produces poor results because they essentially function with different purposes.
147	30	Although adding non-linear transformation to it may further improve model performance, the individual sentiment effect from each context will become untraceable, i.e., losing some interpretability.
148	124	In order to show the effectiveness of learning TCS interaction and for analysis purpose, we do not use it in this work.
150	25	Loss function: The proposed models are all trained in an end-to-end manner by minimizing the cross entropy loss.
151	163	Let us denote a sentence and a target aspect as x and t respectively.
