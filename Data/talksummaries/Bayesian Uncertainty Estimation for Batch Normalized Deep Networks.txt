26	2	We also show that the uncertainty quality of MCBN is on par with other recent approximate Bayesian networks.
43	2	We continue by showing that a batch normalized deep network can be seen as an approximate Bayesian model.
45	2	Finally, we describe the procedure for estimating the uncertainty of a batch normalized network’s output.3
52	3	We use the same MC estimate as in (Gal, 2016) (explained in Appendix Section 1.1), such that one realized ω̂i is taken for each sample i 5.
60	2	For FC layers, it converts a unit’s input hu in the following way: ĥu = hu − E[hu]√ Var[hu] where the expectations are computed over the training set during evaluation, and mini-batch during training (in deep networks, the weight matrices are often optimized using back-propagated errors calculated on mini-batches of data)7.
61	3	Therefore, during training, the estimated mean and variance on the mini-batch B is used, which we denote by µB and σB respectively.
64	3	If the loss l is cross-entropy for classification or sum-of-squares for regression problems (assuming i.i.d.
75	1	(2) to be equivalent up to a scaling factor.
81	5	In Appendix Section 1.4 we explore this with L2-regularization, also called weight decay (Ω(θ) = λ ∑ l=1:L ||W l||2).
82	2	We find that unlike in MCDO (Gal, 2016), some simplifying assumptions are necessary to reconcile the VA and BN objectives with weight decay: no scale and shift applied to BN layers, uncorrelated units in each layer, BN applied on all layers, and large N and M .
83	3	This corresponds to a wide and narrow distribution on BN units’ means and std.
85	4	Due to its popularity in deep learning, our experiments in Section 4 are performed with weight decay.
89	2	Sampling ω̂i from the training set, and keeping the size of B consistent with the mini-batch size used during training, ensures that qθ(ω) during inference remains identical to the approximate posterior optimized during training.
90	2	The network is trained just as a regular BN network, but instead of replacing ω = {µ1:LB ,σ1:LB } with population values from D for inference, we update these parameters stochastically, once for each forward pass.9 Pseudocode for estimating predictive mean and variance is given in Algorithm 1.
91	1	Algorithm 1 MCBN Algorithm Input: sample x, number of inferences T , batchsize b Output: mean prediction ŷ, predictive uncertainty σ2 1: y = {} 2: loop for T iterations 3: B ∼ D // mini batch 4: ω̂ = {µB ,σB} // mini batch mean and variance 5: y = y ∪ fω̂(x) 6: end loop 7: ŷ = E[y] 8: σ2 = Cov[y] + τ−1I // for regression
98	2	To improve the interpretability of the metrics, we propose to normalize them by upper and lower bounds.
139	1	epochs was optimized, the batch size was set to 100, and early stopping test performed each epoch (compared to every 20th for MCBN, MCDO).
143	2	For each split, test set evaluation was done 5 times with different seeds.
145	1	For the image classification test we use CIFAR10 (Krizhevsky & Hinton, 2009) which includes 10 object classes with 5,000 and 1,000 images in the training and test sets, respectively.
148	3	Code for reproducing our experiments is available at https://github.com/icml-mcbn/mcbn.
149	1	The regression experiment comparing uncertainty quality is summarized in Table 1.
151	1	Further details are provided in Appendix Section 1.7.
153	1	Data are sorted by estimated uncertainty in the x-axis.
185	2	Looking closer, MCBN outperforms MCDO and MNF in more cases than not, measured by CRPS.
201	2	Furthermore, we can use the Gaussian estimate of the BN statistics as discussed previously, which makes memory and computation extremely efficient.
204	2	Our experiments show that MCBN yields a significant improvement over the optimized constant uncertainty baseline, on par with MCDO and MNF.
205	18	Our evaluation also suggests new normalized metrics based on useful upper and lower bounds, and a new visualization which provides an intuitive explanation of uncertainty quality.
206	31	Finally, it should be noted that over the past few years, batch normalization has become an integral part of most – if not all – cutting edge deep networks.
207	152	We have shown that it is possible to obtain meaningful uncertainty estimates from existing models without modifying the network or the training procedure.
208	152	With a few lines of code, robust uncertainty estimates can be obtained by computing the variance of multiple stochastic forward passes through an existing network.
