{"sections": [{"heading": "1. Introduction", "text": "This paper presents a novel framework that enables an exact, nonasymptotic, and closed-form analysis of the parameter estimation error under the Rasch model. The Rasch model was proposed in 1960 for modeling the responses of students/users to test/survey items (Rasch, 1960), and has enjoyed great success in applications including (but not limited to) psychometrics (van der Linden & Hambleton, 2013), educational tests (Lan et al., 2016), crowdsourcing (Whitehill et al., 2009), public health (Cappelleri et al., 2014), and even market and financial research (Schellhorn & Sharma, 2013; Brzezin\u0301ska, 2016). Mathematically, the (dichotomous) Rasch model, also known as the 1PL item\n1Department of Electrical Engineering, Princeton University 2Purdue University 3School of Electrical and Computer Engineering, Cornell University. Correspondence to: Andrew S. Lan <andrew.lan@princeton.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nresponse theory (IRT) model (Lord, 1980), is given by\np(Yu,i = 1) = \u03a6(au \u2212 di), (1)\nwhere Yu,i \u2208 {\u22121,+1} denotes the response of user u to item i, where +1 stands for a correct response and\u22121 stands for an incorrect response. The parameters au \u2208 R model the scalar abilities of users u = 1, . . . , U and the parameters di \u2208 R model the scalar difficulties of items i = 1, . . . , Q. The function \u03a6(x) = \u222b x \u2212\u221eN (t; 0, 1)dt, often referred to as the inverse probit link function1, is the cumulative distribution function of a standard normal random variable, where N (t; 0, 1) denotes the probability density function of a standard normal random variable evaluated at t.\nThe literature describes a range of parameter estimation methods under the Rasch model and related IRT models; see (Baker & Kim, 2004) for an overview. However, existing analytical results for the associated parameter estimation error are limited; see (Tsutakawa & Johnson, 1990) for an example. The majority of existing results have been proposed in the psychometrics and educational measurement literature; see, e.g., (Carroll et al., 2006) for a survey. The proposed analysis tools rely, for example, on multiple imputation (Yang et al., 2012) or Markov chain Monte Carlo (MCMC) techniques (Patz & Junker, 1999), and are thus not analytical. Hence, their accuracy strongly depends on the available data.\nOther analysis tools use the Fisher information matrix (Zhang et al., 2011; Yang et al., 2012) to obtain lower bounds on the estimation error. Such methods are of asymptotic nature, i.e., they yield accurate results only when the number of users and items tend to infinity. For real-world settings with limited data, these bounds are typically loose; As an example, in computerized adaptive testing (CAT) (Chang & Ying, 2009), a user enters the system and starts responding to items. The system maintains an estimate of their ability parameter, and adaptively selects the next-best item to assign to the user that is most informative of the ability estimate. Calculating the informativeness of each item requires an analysis of the uncertainty in the ability\n1While some publications assume the inverse logit link function, i.e., the sigmoid \u03a6(x) = 1\n1+e\u2212x , in most real-world applications the choice of the link function has no significant performance impact. In what follows, we will focus on the inverse probit link function for reasons that will be discussed in Section 3.\nestimate. Initially, after the user has only responded to a few items, these asymptotic methods lead to highly inaccurate analyses, which may lead to poor item selections.\nAnother family of analysis tools relies on concentration inequalities and yield probabilistic bounds, i.e., bounds that hold with high probability (Bunea, 2008; Filippi et al., 2010). Such results are often impractical in real-world applications. However, an exact analysis of the estimation error of the Rasch model is critical to ensure the a certain degree of reliability of assessment scores in tests (Thompson, 2002)."}, {"heading": "1.1. Contributions", "text": "We propose a novel framework for the Rasch model that enables an exact, nonasymptotic, and closed-form analysis of the parameter estimation error. To this end, we generalize a recently-proposed linear estimator for binary regression (Lan et al., 2018) to the Rasch model, which enables us to derive a sharp upper bound on the mean squared error (MSE) of model parameter estimates. Our analytical results are in stark contrast to existing analytical results which either provide loose lower bounds or are asymptotic in nature, rendering them impractical in real-world applications.\nTo demonstrate the efficacy of our framework, we provide experimental results on both synthetic and real-world data. First, using synthetic data, we show that our upper bound on the MSE is (often significantly) tighter than the Fisher information-based lower bound, especially when the problem size is small and when the data is noisy. Therefore, our framework enables a more accurate analysis of the estimation error in real-world settings. Second, using real-world student question response and user movie rating datasets, we show that our linear estimator achieves competitive predictive performance to more sophisticated, nonlinear estimators for which sharp performance guarantees are unavailable."}, {"heading": "2. Rasch Model and Probit Regression", "text": "The Rasch model in (1) can be written in equivalent matrixvector form as follows (Hoff, 2009):\ny = sign(Dx + w). (2)\nHere, the UQ-dimensional vector y \u2208 {\u22121,+1}UQ contains all user responses to all items, the Rasch model matrix D = [1Q\u2297 IU\u00d7U , IQ\u00d7Q\u22971U ] is constructed with the Kronecker product operator \u2297, identity matrices I, all-ones vectors 1, and the vector xT = [aT ,\u2212dT ] to be estimated consists of the user abilities a \u2208 RU and item difficulties d \u2208 RQ. The \u201cnoise\u201d vector w contains i.i.d. standard normal random variables. In this equivalent form, parameter estimation under the Rasch model can be casted as a probit regression problem (Bliss, 1935), for which numerous estimators have been proposed in the past."}, {"heading": "2.1. Estimators for Probit Regression", "text": "The two most prominent estimators for probit regression are the posterior mean (PM) estimator, given by\nx\u0302PM = Ex[x|y] = \u222b RN xp(x|y)dx, (3)\nand the maximum a-posteriori (MAP) estimator, given by\nx\u0302MAP = arg min x\u2208RN\n\u2212 \u2211M m=1 log(\u03a6(ymd T mx)) + 1 2x TC\u22121x x.\nHere, p(x|y) denotes the posterior probability of the vector x given the observations y under the model (2), dTm denotes the mth row of the matrix of covariates D, and Cx denotes the covariance matrix of the multivariate Gaussian prior on x. A special case of the MAP estimator is the wellknown maximum likelihood (ML) estimator, which does not impose a prior distribution on x.\nThe PM estimator is optimal in terms of minimizing the MSE of the estimated parameters, which is defined as\nMSE(x\u0302) = Ex,w [ \u2016x\u2212 x\u0302\u20162 ] . (4)\nHowever, there are no simple methods to evaluate the expectation in (3) under the probit model. Thus, one typically resorts to Markov chain Monte Carlo (MCMC) methods (Albert & Chib, 1993) to perform PM estimation, which can be computationally intensive. In contrast to the PM estimator, MAP and ML estimation is generally less complex since it can be implemented using standard convex optimization algorithms (Nocedal & Wright, 2006; Hastie et al., 2010; Goldstein et al., 2014). On the flipside, MAP and ML estimation is not optimal in terms of minimizing the MSE in (4). In contrast to such well-established, nonlinear estimators, we build our framework on the family of linear estimators recently proposed in (Lan et al., 2018). There, a linear minimum MSE (L-MMSE) estimator was proposed for a certain class of probit regression problems. This L-MMSE estimator was found to perform on par with the PM estimator and outperforms the MAP estimator in terms of the MSE for certain settings, while enabling an exact and nonasymptotic analysis of the MSE."}, {"heading": "2.2. Analytical Performance Guarantees", "text": "In the statistical estimation literature, there exists numerous analytical results characterizing the estimation errors for binary regression problems in the asymptotic setting. For example, (Brillinger, 1982) shows that least squares estimation is particularly effective when the design matrix D has i.i.d. Gaussian entries and the number of observations approaches infinity; in this case, its performance was shown to differ from that of the PM estimator only by a constant factor. Recently, (Thrampoulidis et al., 2015) provides a related analysis in the case that the parameter vector x is\nsparse. Another family of probabilistic results relies on the asymptotic normality property of ML estimators, either in the standard (dense) setting (Gourieroux & Monfort, 1981; Fahrmeir & Kaufmann, 1985) or the sparse setting (Bunea, 2008; Bach, 2010; Ravikumar et al., 2010; Plan & Vershynin, 2013), providing bounds on the MSE with high probability. Since numerous real-world applications, such as the Rasch model, rely on deterministic, structured matrices and have small problem dimensions, existing analytical performance bounds are often loose; see Section 4 for experiments that support this claim."}, {"heading": "3. Main Results", "text": "Our main result is as follows; the proof is given in Appendix A.\nTheorem 1. Assume that x \u223c N (x\u0304,Cx) with mean vector x\u0304 and positive definite covariance matrix Cx, and assume that the vector w contains i.i.d. standard normal random variables. Consider the general probit regression model\ny = sign(Dx + m + w), (5)\nwhere D is a given matrix of covariates and m is a given bias vector. Then, the L-MMSE estimate is given by\nx\u0302L-MMSE = ETC\u22121y y + b,\nwhere we use the following quantities:\nE=2diag(N (c; 0,1) diag(Cz)\u2212 1 2 )DCx\nc = z\u0304 diag(Cz)\u22121/2\nz\u0304 = Dx\u0304 + m\nCz = DCxD T + I\nCy = 2(\u03a62(c1 T ,1cT ;R) + \u03a62(\u2212c1T ,\u22121cT ;R))\n\u2212 1M\u00d7M \u2212 y\u0304y\u0304T\nR = diag(diag(Cz) \u22121/2)Czdiag(diag(Cz) \u22121/2)\ny\u0304=\u03a6(c)\u2212 \u03a6(\u2212c) b= x\u0304\u2212ETC\u22121y y\u0304.\nHere, \u03a62(x, y, \u03c1) denotes the cumulative density of a twodimensional zero-mean Gaussian distribution with covariance matrix [1 \u03c1; \u03c1 1] with \u03c1 \u2208 [0, 1), defined as\n\u03a62(x, y; \u03c1) = \u222b x \u2212\u221e \u222b y \u2212\u221e\n1 2\u03c0 \u221a 1\u2212 \u03c12 e \u2212 s 2\u22122\u03c1st+t2 2(1\u2212\u03c12) dtds\nand is applied element-wise on matrices. Furthermore, the associated estimation MSE is given by\nMSE(x\u0302L-MMSE) = tr(Cx \u2212ETC\u22121y E).\nWe note that the linear estimator developed in (Lan et al., 2018, Thm. 1) is a special case of our result with x\u0304 = 0 and\nm = 0. As we will show below, including both of these terms will be essential for our analysis.\nRemark 1. We exclusively focus on probit regression since the matrices E and Cy exhibit tractable expressions under this model. We are unaware of any closed-form expressions for these quantities in the logistic regression case.\nAs an immediate consequence of the fact that the PM estimator minimizes the MSE, we can use Theorem 1 to obtain the following upper bound on the MSE of the PM estimator.\nCorollary 2. The MSE of the PM estimator is upperbounded as follows:\nMSE(xPM) \u2264 MSE(x\u0302L-MMSE). (6)\nAs we will demonstrate in Section 4, this upper bound on the MSE turns out to be surprisingly sharp for a broad range of parameters and problem settings.\nWe now specialize Theorem 1 for the Rasch model and use Corollary 2 to analyze the associated MSE. We divide our results into two cases: (i) both the user abilities and item difficulties are unknown and (ii) one of the two sets of parameters is known and the other is unknown. Due to symmetry in the Rasch model, we will present our results with unknown/known item difficulties while the user abilities are unknown and to be estimated; a corresponding analysis on the estimation error of item parameters follows immediately."}, {"heading": "3.1. First Case: Unknown Item Parameters", "text": "We now analyze the case in which both the user abilities and item difficulties are unknown and need to be estimated. In practice, this scenario is relevant if a new set of items are deployed with little or no prior knowledge on their difficulty parameters. We assume that there is no missing data, i.e., we observe all user responses to all items.2 In the psychometrics literature (see, e.g., (Linacre, 1999)), one typically assumes that the entries of the ability a and difficulty vectors d are i.i.d. zero-mean Gaussian with variance \u03c32a and \u03c32d, respectively, i.e., au \u223c N (0, \u03c32a) and di \u223c N (0, \u03c32d), which can be included in our model assumptions. Thus, we can leverage the special structure of the Rasch model, since it corresponds to a special case of the generic probit regression model in (5) with D = [1Q\u2297 IU\u00d7U , IQ\u00d7Q\u22971U ] and m = 0. We have the following result on the MSE of the L-MMSE estimator; the proof is given in Appendix B.\nTheorem 3. Assume that \u03c32a = \u03c32d = \u03c32x and the covariance matrix of x is Cx = \u03c32xI(U+Q)\u00d7(U+Q). Let\ns = 2\n\u03c0 arcsin\n( \u03c32x\n2\u03c32x + 1\n) .\n2Our analysis can readily be generalized to missing data; the results, however, depend on the missing data pattern.\nThen, the MSE of the L-MMSE estimator of user abilities under the Rasch model is given by\nMSEa = Ex,w [ (au \u2212 a\u0302u)2 ] =\n\u03c32x\n( 1\u2212 2\n\u03c0 \u03c32x 2\u03c32x + 1 sQ(Q+ U \u2212 3) + 1 (s(Q\u2212 2) + 1)(s(Q+ U \u2212 2) + 1)\n) .\n(7)\nTo the best of our knowledge, Theorem 3 is the first exact, nonasymptotic, and closed-form analysis of the MSE of a parameter estimation method for the Rasch model. From (7), we see that if \u03c32x is held constant, then the relationship between MSEa and the numbers of users (U ) and items (Q) is given by the ratio of two second-order polynomials. If the signal-to-noise ratio (SNR) is low (or, equivalently, the data is noisy), i.e., \u03c32x \u03c32n, then we have \u03c32x 2\u03c32x+1 \u2248 0 and hence, s = 2\u03c0 arcsin( \u03c32x 2\u03c32x+1 ) \u2248 0. In this case, we have MSEa \u2248 \u03c32x, i.e., increasing the number of users/items does not affect the accuracy of the ability and difficulty parameters of users and items; this behavior is as expected.\nWhen U,Q\u2192\u221e, the MSE satisfies MSEa \u2192 \u03c32x ( 1\u2212 \u03c3 2 x\n2\u03c32x + 1 arcsin\u22121\n( \u03c32x\n2\u03c32x + 1\n)) , (8)\nwhich is a non-negative quantity. This result implies that the L-MMSE estimator has a residual MSE even as the number of users/items grows large. More specifically, since x \u2264 arcsin(x) for x \u2208 [0, 1], this residual error approaches \u03c32x(1 \u2212 3\u03c0 ) at high values of SNR. We note, however, this result does not imply that the L-MMSE estimator is not consistent under the Rasch model, since the number of parameters to be estimated (U +Q) grows with the number of the observations (UQ) instead of remaining constant.\nRemark 2. The above MSE analysis is data-independent, in contrast to error estimates that rely on the responses y (which is, for example, the case for method in (Carroll et al., 2006)). This fact implies that our result provides an error estimate before observing y. Thus, Theorem 3 provides guidelines on how many items to include and how many users to recruit for a study, given a desired MSE level on the user ability and item difficulty parameter estimates."}, {"heading": "3.2. Second Case: Known Item Difficulties", "text": "We now analyze the case in which the user abilities are unknown and need to be estimated; the item difficulties (d) are given. In practice, this scenario is relevant if a large number of users previously responded to a set of items so that a good estimate of the item difficulties is available. Let a denote the scalar ability parameter of an user. Then, their responses to items are modeled as\np(y = 1) = \u03a6(1Qa\u2212 d).\nThe following result follows from Theorem 1 by setting x = a, x\u0304 = x\u0304, Cx = \u03c32x, D = 1Q, and m = \u2212d. Corollary 4. Assume that a \u223c N (x\u0304, \u03c32x). Then, the LMMSE estimate of user ability is given by\na\u0302 = eTC\u22121y y + b,\nwhere\ne=2 \u03c32x\u221a \u03c32x + 1 N (c; 0, 1) c = z\u0304 diag(Cz)\u22121/2\nz\u0304 = x\u03041Q \u2212 d Cz = \u03c3 2 x1Q\u00d7Q + I\ny\u0304 = \u03a6(c)\u2212 \u03a6(\u2212c) Cy = 2(\u03a62(c1\nT,1cT ,R) + \u03a62(\u2212c1T ,\u22121cT ,R)) \u2212 1M\u00d7M \u2212 y\u0304y\u0304T\nR = diag(diag(Cz) \u22121/2)Czdiag(diag(Cz) \u22121/2.\nThe MSE of the user ability estimate is given by MSE(a\u0302) = \u03c32x \u2212 eTC\u22121y e."}, {"heading": "4. Numerical Results", "text": "We now experimentally demonstrate the efficacy of the proposed framework. First, we use synthetically generated data to numerically compare our L-MMSE-based upper bound on the MSE of the PM estimator to the widely-used lower bound based on Fisher information (Zhang et al., 2011; Yang et al., 2012). We then use several real-world collaborative filtering datasets to show that the L-MMSE estimator achieves comparable predictive performance to that of the PM and MAP estimators."}, {"heading": "4.1. Experiments with Synthetic Data", "text": "We start with synthetic data to demonstrate the exact and nonasymptotic nature of our analytical MSE expressions."}, {"heading": "4.1.1. FIRST CASE: UNKNOWN ITEM PARAMETERS", "text": "Experimental Setup We vary the number of users U \u2208 {20, 50, 100} and the number of items Q \u2208 {20, 50, 100, 200}. We generate the user ability and item difficulty parameters from zero-mean Gaussian distributions with variance \u03c32x = \u03c3 2 a = \u03c3 2 d. We vary \u03c3 2 x so that the signalto-noise ratio (SNR) corresponds to {\u221210, 0, 10} decibels (dB). We then randomly generate the response from each user to each item, Yu,i, according to (1). We repeat these experiments for 1, 000 random instances of user and item parameters and responses, and report the averaged results.\nWe compute the L-MMSE-based upper bound on the MSE of the PM estimator using Theorem 1 and the Fisher\ninformation-based lower bound using the method detailed in (Zhang et al., 2011; Yang et al., 2012). Since the calculation of the Fisher information matrix requires the true values of the user ability and item difficulty parameters (which are to be estimated in practice), we use the PM estimates of these parameters instead. We also calculate the empirical parameter estimation MSEs of the L-MMSE and PM estimators. To this end, we use a standard Gibbs sampling procedure (Albert & Chib, 1993); we use the mean of the generated samples over 20, 000 iterations as the PM estimate after a burn-in phase of 10, 000 iterations. We then use these estimates to calculate the empirical MSE.\nResults and Discussion Fig. 1 shows the empirical MSEs of the L-MMSE and PM estimators, together with the L-MMSE-based upper bound and the Fisher informationbased lower bound on the MSE of the PM estimator, for every problem size and every SNR. First, we see that the analytical and empirical MSEs of the L-MMSE estimator match perfectly, which confirms that our analytical MSE expressions are exact. We also see that for low SNR (i.e., the first row of Fig. 1), our L-MMSE upper bound on the MSE of the PM estimator is tight. Moreover, at all noise levels,\nthe L-MMSE-based upper bound is tighter at small problem sizes, while the Fisher information-based lower bound is tighter at very large problem sizes and at high SNR.\nThese results confirm that our L-MMSE-based upper bound on the MSE is nonasymptotic, while the Fisher informationbased lower bound is asymptotic and thus only tight at very large problem sizes. Therefore, the L-MMSE-based upper bound is more practical than the Fisher informationbased lower bound in real-world applications, especially for situations like the initial phase of CAT when the number of items a user has responded to is small."}, {"heading": "4.1.2. CASE TWO: KNOWN ITEM PARAMETERS", "text": "Experimental Setup In this experiment, we randomly generate the item parameters from the standard normal distribution (\u03c32d = 1) and treat these parameters as known; we then estimate the user ability parameters via Theorem 4. The rest of the experimental setup remains unchanged.\nResults and Discussion Fig. 2 shows the empirical MSEs of the L-MMSE and PM estimators, together with the L-MMSE-based upper bound and the Fisher information-\nbased lower bound on the MSE of the PM estimator, for every problem size and every SNR. We see that the analytical and empirical MSEs of the L-MMSE estimator match. We also see that the L-MMSE-based upper bound on the MSE is tighter than the Fisher information-based lower bound at low SNR levels (\u221210 dB and 1 dB), and especially when the problem size is small (less than 50 items). These results further confirm that our L-MMSE-based upper bound on the MSE is nonasymptotic, and is thus practical in the \u201ccold-start\u201d setting of recommender systems."}, {"heading": "4.2. Experiments with Real-World Data", "text": "We now test the performance of the proposed L-MMSE estimator using a variety of real-world datasets. Since the noise model in real-world datasets is generally unknown, we also consider the performance of MAP estimation using the inverse logit link function (Logit-MAP).\nDatasets We perform our experiments using a range of collaborative filtering datasets. These datasets are matrices that contain binary-valued ratings (or graded responses) of users (or students) to movies (or items). For these datasets, we use the probit Rasch model. The datasets include (i) \u201cMT\u201d, which consists of students\u2019 binary-valued (correct/incorrect) graded responses to questions in a highschool algebra test, with U = 99 students\u2019 3, 366 responses to Q = 34 questions, (ii) \u201cSS\u201d, which consists of student responses in a signals and systems course, with U = 92 students\u2019 5, 692 responses to Q = 203 questions, (iii) \u201cedX\u201d, which consists of student responses in an edX course, with U = 3241 students\u2019 177, 181 responses to Q = 191 questions, and (iv) \u201cML\u201d, a processed version of the ml-100k dataset from the Movielens project (Herlocker et al., 1999), with 37, 175 integer-valued ratings by U = 943 users to Q = 1152 movies. We adopt the procedure used in (Davenport et al., 2014) to transform the dataset into binary values by comparing each rating to the overall average rating.\nExperimental Setup We evaluate the prediction performance of the L-MMSE, MAP, PM, and Logit-MAP estimators using ten-fold cross validation. We randomly divide the\nentire dataset into ten equally-partitioned folds (of user-item response pairs), leave out one fold as the held-out testing set and use the other folds as the training set. We then use the training set to estimate the learner abilities au and item difficulties di, and use these estimates to predict user responses on the test set. We tune the prior variance parameter \u03c32x using a separate validation set (one fold in the training set). To assess the performance of these estimators, we use two common metrics in binary classification problems: prediction accuracy (ACC), which is simply the portion of correct predictions, and area under the receiver operating characteristic curve (AUC) (Jin & Ling, 2005). Both metrics have range in [0, 1], with larger values indicating better predictive performance.\nResults and Discussion Tables 1 and 2 show the mean and standard deviation of the performance of each estimator on both metrics across each fold. We observe that the performance of the considered estimators are comparable on the ACC metric, while the L-MMSE estimator performs slightly worse than the MAP, PM, and Logit-MAP estimators for most datasets on the AUC metric.\nWe find it quite surprising that a well-designed linear estimator performs on par with more sophisticated nonlinear estimators on these real-world datasets. We also note that the L-MMSE estimator is more computationally efficient than the PM estimator. As an example, on the MT and ML datasets, one run of the L-MMSE estimator takes 0.23s and 79s, respectively, while one run of the PM estimator takes 1.9s and 528s (2, 000 and 10, 000 iterations required for convergence) on a standard laptop computer. These observations suggest that the L-MMSE estimator is computationally efficient and thus scales favorably to large datasets."}, {"heading": "5. Conclusions", "text": "We have generalized a recently proposed linear estimator for probit regression and applied the method to the classic Rasch model in item response analysis. We have shown that the L-MMSE estimator enables an exact, closed-form, and nonasymptotic MSE analysis, which is in stark contrast to existing analytical results which are asymptotic, probabilis-\nL-MMSE MAP PM Logit-MAP\nMT 0.840\u00b1 0.016 0.843\u00b1 0.015 0.843\u00b1 0.015 0.842\u00b1 0.015 SS 0.800\u00b1 0.014 0.803\u00b1 0.013 0.803\u00b1 0.013 0.802\u00b1 0.013 edX 0.900\u00b1 0.004 0.909\u00b1 0.004 0.909\u00b1 0.004 0.909\u00b1 0.004 ML 0.755\u00b1 0.005 0.756\u00b1 0.004 0.756\u00b1 0.004 0.756\u00b1 0.004\ntic, or loose. As a result, we have shown that the nonasymptotic, L-MMSE-based upper bound on the parameter estimation error of the PM estimator under the Rasch model can be tighter than the common Fisher information-based asymptotic lower bound, especially in practical settings. An avenue of future work is to apply our analysis to models that are more sophisticated than the Rasch model, e.g., the latent factor model in (Lan et al., 2014)."}, {"heading": "A. Proof of Theorem 4", "text": "Let z = Dx+m+w. Thus, z \u223c N (Dx\u0304+m,DCxDT + I) := N (z\u0304,Cz). The L-MMSE estimator for x has the general form of x\u0302L-MMSE = Wy + b, where W = EC\u22121y and b = x\u0304\u2212Wy\u0304, with\nCy =E [ (y\u2212y\u0304)(y\u2212y\u0304)T ] =E [ yyT ] \u2212y\u0304y\u0304T :=C\u0303y\u2212y\u0304y\u0304T\nand E = E [ (y \u2212 y\u0304)(x\u2212 x\u0304)T ] =E [ yxT ] \u2212y\u0304x\u0304T :=E\u0303\u2212y\u0304x\u0304T .\nWe need to evaluate three quantities, y\u0304, C\u0303y, and E\u0303.\nWe start with y\u0304. Its ith entry is given by\ny\u0304i = \u222b \u221e \u2212\u221e sign(zi)N (zi; z\u0304i, [Cz]i,i)dzi\n=\u2212 \u222b 0 \u2212\u221e N (zi; z\u0304i, [Cz]i,i)dzi+ \u222b \u221e 0 N (zi; z\u0304i, [Cz]i,i)dzi\n= \u03a6 ( z\u0304i\u221a\n[Cz]i,i\n) \u2212 \u03a6 ( \u2212 z\u0304i\u221a\n[Cz]i,i\n) .\nNext, we calculate C\u0303y. Its (i, j)th entry is given by [C\u0303y]i,j =\u222b \u221e \u2212\u221e \u222b \u221e \u2212\u221e sign(zi) sign(zj)N ([ zi zj\n] ;[ z\u0304i\nz\u0304j\n] , [ [Cz]i,i [Cz]i,j\n[Cz]j,i [Cz]j,j\n]) dzjdzi\n(a) = \u222b \u221e \u2212\u221e \u222b \u221e \u2212\u221e sign ( zi + z\u0304i\u221a [Cz]i,i ) sign ( zj + z\u0304j\u221a [Cz]j,j )\nN ([ zi\nzj\n] ;0, [ 1 \u03c1 \u03c1 1 ]) dzjdzi\n=\n\u222b \u2212 z\u0304i\u221a [Cz]i,i\n\u2212\u221e\n\u222b \u2212 z\u0304j\u221a [Cz]j,j \u2212\u221e N ([ zi zj ] ;0, [ 1 \u03c1 \u03c1 1 ]) dzjdzi\ufe38 \ufe37\ufe37 \ufe38\nv1\n+ \u222b \u221e \u2212 z\u0304i\u221a\n[Cz]i,i\n\u222b \u221e \u2212\nz\u0304j\u221a [Cz]j,j\nN ([\nzi zj\n] ;0, [\n1 \u03c1 \u03c1 1 ]) dzjdzi\ufe38 \ufe37\ufe37 \ufe38\nv2 \u2212 \u222b \u2212 z\u0304i\u221a [Cz]i,i\n\u2212\u221e\n\u222b \u221e \u2212\nz\u0304j\u221a [Cz]j,j\nN ([ zi\nzj\n] ;0, [ 1 \u03c1 \u03c1 1 ]) dzjdzi\ufe38 \ufe37\ufe37 \ufe38\nv3 \u2212 \u222b \u221e \u2212 z\u0304i\u221a\n[Cz]i,i\n\u222b \u2212 z\u0304j\u221a [Cz]j,j \u2212\u221e N ([ zi zj ] ;0, [ 1 \u03c1 \u03c1 1 ]) dzjdzi\ufe38 \ufe37\ufe37 \ufe38\nv4\n(b) = 2(v1 + v2)\u2212 1\n= 2 ( \u03a62 ( z\u0304i\u221a\n[Cz]i,i , z\u0304j\u221a [Cz]j,j , \u03c1\n)\n+ \u03a62\n( \u2212 z\u0304i\u221a\n[Cz]i,i ,\u2212 z\u0304j\u221a [Cz]j,j , \u03c1\n)) \u2212 1,\nwhere we have used (a) change of variable zi\u2212z\u0304i\u221a [Cz]i,i \u2192 zi and (b) the fact that v1 +v2 +v3 +v4 = 1. The computation of E\u0303 follows from that in (Lan et al., 2018) and is omitted."}, {"heading": "B. Proof of Theorem 3", "text": "Recall that the expression for the MSE is tr(Cx \u2212 ETC\u22121y E), the critical part is to evaluate E\nTC\u22121y E. We begin by evaluating C\u22121y . For the Rasch model, we have\nD = [1Q\u2297 IU\u00d7U , IQ\u00d7Q\u22971U ]. Therefore, since Cx = \u03c32xIU+Q, we have\nCz = DCxD T + IUQ\u00d7UQ = \u03c3 2 xDD T + IUQ\u00d7UQ = \u03c32x[1Q\u2297 IU\u00d7U IQ\u00d7Q\u22971U ] [\n1TQ\u2297 IU\u00d7U IQ\u00d7Q\u22971TU ] + IUQ\u00d7UQ\n= \u03c32x(1Q\u00d7Q\u2297 IU\u00d7U + IQ\u00d7Q\u22971U\u00d7U ) + IUQ\u00d7UQ,\nwhere 1U\u00d7U denotes an all-one matrix with size U \u00d7 U . Therefore, we see that the UQ\u00d7 UQ matrix Cz consists of three parts: (i) Q copies of the all-ones matrix \u03c32x1U\u00d7U in its diagonal U \u00d7U blocks, (ii) copies of the matrix \u03c32xIU\u00d7U in every other off-diagonal U\u00d7U block, plus (iii) a diagonal matrix IUQ\u00d7UQ. Therefore, its diagonal elements are 2\u03c32x + 1 and its non-zero off-diagonal elements are \u03c32x.\nAs detailed in (Lan et al., 2018, (7)), one can show that\nCy = 2\n\u03c0 arcsin(diag(diag(Cz)\n\u22121/2)Cz\n\u00d7 diag(diag(Cz)\u22121/2)),\nwe have that the term inside the arcsin function has the same structure as Cz, with diagonal entries of 1 and nonzero off-diagonal entries as \u03c3 2 x\n2\u03c32x+1 . Therefore, Cy also has\nthe same structure, with diagonal entries of 1 and non-zero off-diagonal entries as\ns = 2\n\u03c0 arcsin\n( \u03c32x\n2\u03c32x + 1\n) .\nSince C\u22121y satisfies CyC \u22121 y = IUQ\u00d7UQ, it is easy to see that the entries of C\u22121y only contain three distinct values (denoted by a, b, and c), and consists of two parts: (i) Q copies of a U \u00d7 U matrix with a on its diagonal, b everywhere else, in its diagonal blocks, and (ii) copies of a U \u00d7U matrix with c on its diagonal, d everywhere else, in its other blocks. We next compute a, b, c, and d.\nThe first column of C\u22121y is given by\n[a, b11\u00d7U\u22121, c, d11\u00d7U\u22121, c, d11\u00d7U\u22121, . . .] T .\nSince its inner product with the first row of Cy is one (since CyC \u22121 y = IUQ\u00d7UQ), we get\na+ (U \u2212 1)sb+ (Q\u2212 1)sc = 1.\nSimilarly, its inner products with the second, (U + 1)\u2212 th, and (U + 2)-th rows are all zero; this gives\nsa+ ((U \u2212 2)s+ 1)b+ (Q\u2212 1)sd = 0, sa+ ((Q\u2212 2)s+ 1)c+ (U \u2212 1)sd = 0,\nsb+ sc+ ((U +Q\u2212 4)s+ 1)d = 0.\nSolving the linear system given by these four equations results in\na= (3U2+3Q2\u2212U2Q\u2212UQ2+8UQ\u221215U\u221215Q+20)s3\nr\n+ (\u2212U2 \u2212Q2 \u2212 3UQ+ 11U + 11Q\u2212 22)s2\nr\n+ (\u22122U \u2212 2Q+ 8)s\u2212 1\nr\nb= (UQ+Q2 \u2212 3U \u2212 5Q+ 8)s3+(U + 2Q\u2212 6)s2+s\nr\nc= (UQ+ U2 \u2212 5U \u2212 3Q+ 8)s3+(2U +Q\u2212 6)s2+s\nr\nd= \u2212(U +Q\u2212 4)s3 \u2212 2s2\nr , (9)\nwhere\nr=(2s\u22121)((U\u22122)s+1)((Q\u22122)s+1)((Q+U\u22122)s+1).\nNow, let A be the N \u00d7N matrix with c on its diagonal and d everywhere else, B denote the matrix with a \u2212 c on its diagonal and b\u2212 d everywhere else, we can write C\u22121y as\nC\u22121y = 1Q\u00d7Q\u2297A + IQ\u00d7Q\u2297B. (10)\nOur second task is to evaluate E. Since\nE =\n\u221a 2\n\u03c0 diag(diag(Cz)\n\u22121/2)DCx =\n\u221a 2\n\u03c0 \u03c32x\u221a 2\u03c32x + 1 D\n= \u03c32x\u221a\n2\u03c32x + 1 [1Q\u2297 IU\u00d7U IQ\u00d7Q\u22971U ],\nwe have\nETC\u22121y E = 2\n\u03c0 \u03c34x 2\u03c32x + 1 [ 1TQ\u2297 IU\u00d7U IQ\u00d7Q\u22971TU ] \u00d7 (1Q\u00d7Q\u2297A + IQ\u00d7Q\u2297B)[1Q\u2297 IU\u00d7U IQ\u00d7Q\u22971U ]\n= 2\n\u03c0 \u03c34x 2\u03c32x + 1 Q(QA + B),\nwhere we have used (X\u2297Y)(U\u2297V) = (XU)\u2297(YV).\nTherefore, the value of entry (1, 1) in ETC\u22121y E, i.e., the MSE of the user ability parameter estimates, is given by\n2\n\u03c0 \u03c34x 2\u03c32x + 1 Q(a+ (Q\u2212 1)c) =\n\u03c32x\n( 1\u2212 2\n\u03c0 \u03c32x 2\u03c32x + 1 sQ(Q+ U \u2212 3) + 1 (s(Q\u2212 2) + 1)(s(Q+ U \u2212 2) + 1)\n) ,\nwhere we have used (9), thus completing the proof."}, {"heading": "Acknowledgments", "text": "C. Studer was supported in part by Xilinx, Inc. and by the US National Science Foundation (NSF) under grants ECCS1408006, CCF-1535897, CCF-1652065, and CNS-1717559."}], "year": 2018, "references": [{"title": "Bayesian analysis of binary and polychotomous response data", "authors": ["J.H. Albert", "S. Chib"], "venue": "J. Am. Stat. Assoc.,", "year": 1993}, {"title": "Self-concordant analysis for logistic regression", "authors": ["F. Bach"], "venue": "Electron. J. Stat.,", "year": 2010}, {"title": "Item Response Theory: Parameter Estimation Techniques", "authors": ["F.B. Baker", "S.H. Kim"], "venue": "Marcel Dekker Inc.,", "year": 2004}, {"title": "The calculation of the dosage-mortality curve", "authors": ["C. Bliss"], "venue": "Ann. Appl. Biol.,", "year": 1935}, {"title": "A Festschrift for Erich L. Lehmann, chapter \u201cA generalized linear model with \u201cGaussian", "authors": ["D. Brillinger"], "venue": "regressor variables\u201d,", "year": 1982}, {"title": "Latent variable modelling and item response theory analyses in marketing research", "authors": ["J. Brzezi\u0144ska"], "venue": "J. University of Szczecin,", "year": 2016}, {"title": "Honest variable selection in linear and logistic regression models via", "authors": ["F. Bunea"], "venue": "penalization. Electron. J. Stat.,", "year": 2008}, {"title": "Overview of classical test theory and item response theory for the quantitative assessment of items in developing patient-reported outcomes measures", "authors": ["J.C. Cappelleri", "J.J. Lundy", "R.D. Hays"], "venue": "Clinical Therapeutics,", "year": 2014}, {"title": "Measurement Error in Nonlinear Models: A Modern Perspective", "authors": ["R.J. Carroll", "D. Ruppert", "L.A. Stefanski", "C.M. Crainiceanu"], "venue": "CRC press,", "year": 2006}, {"title": "Nonlinear sequential designs for logistic item response theory models with applications to computerized adaptive tests", "authors": ["H. Chang", "Z. Ying"], "venue": "The Annals of Statistics,", "year": 2009}, {"title": "Consistency and asymptotic normality of the maximum likelihood estimator in generalized linear models", "authors": ["L. Fahrmeir", "H. Kaufmann"], "venue": "Ann. Stat.,", "year": 1985}, {"title": "Parametric bandits: The generalized linear case", "authors": ["S. Filippi", "O. Cappe", "A. Garivier", "C. Szepesv\u00e1ri"], "venue": "In Proc. Adv. Neural Info. Proc. Syst.,", "year": 2010}, {"title": "A field guide to forwardbackward splitting with a FASTA implementation", "authors": ["T. Goldstein", "C. Studer", "R. Baraniuk"], "venue": "arXiv preprint:", "year": 2014}, {"title": "Asymptotic properties of the maximum likelihood estimator in dichotomous logit models", "authors": ["C. Gourieroux", "A. Monfort"], "venue": "J. Econometrics,", "year": 1981}, {"title": "An algorithmic framework for performing collaborative filtering", "authors": ["J. Herlocker", "J. Konstan", "A. Borchers", "J. Riedl"], "venue": "In Proc. Ann. Intl. Conf. Res. Develop. Inf. Retrieval,", "year": 1999}, {"title": "Sparse factor analysis for learning and content analytics", "authors": ["A.S. Lan", "A.E. Waters", "C. Studer", "R.G. Baraniuk"], "venue": "J. Mach. Learn. Res.,", "year": 2014}, {"title": "Dealbreaker: A nonlinear latent variable model for educational data", "authors": ["A.S. Lan", "T. Goldstein", "R.G. Baraniuk", "C. Studer"], "venue": "In Proc. Intl. Conf. Mach. Learn.,", "year": 2016}, {"title": "Linearized binary regression", "authors": ["A.S. Lan", "M. Chiang", "C. Studer"], "venue": "arXiv preprint:", "year": 2018}, {"title": "Understanding Rasch measurement: Estimation methods for Rasch measures", "authors": ["J.M. Linacre"], "venue": "J. Outcome Meas.,", "year": 1999}, {"title": "Applications of Item Response Theory to Practical Testing Problems", "authors": ["F. Lord"], "venue": "Erlbaum Associates,", "year": 1980}, {"title": "A straightforward approach to Markov chain Monte Carlo methods for item response models", "authors": ["R.J. Patz", "B.W. Junker"], "venue": "J. Educ. Behav. Stat.,", "year": 1999}, {"title": "Robust 1-bit compressed sensing and sparse logistic regression: A convex programming approach", "authors": ["Y. Plan", "R. Vershynin"], "venue": "IEEE Trans. Inf. Th.,", "year": 2013}, {"title": "Studies in Mathematical Psychology: I. Probabilistic Models for Some Intelligence and Attainment Tests", "authors": ["G. Rasch"], "venue": "Nielsen & Lydiche,", "year": 1960}, {"title": "High-dimensional Ising model selection using `1-regularized logistic regression", "authors": ["P. Ravikumar", "M. Wainwright", "J. Lafferty"], "venue": "Ann. Stat.,", "year": 2010}, {"title": "Using the rasch model to rank firms by managerial ability", "authors": ["C. Schellhorn", "R. Sharma"], "venue": "Managerial Finance,", "year": 2013}, {"title": "Score reliability: Contemporary thinking on reliability issues", "authors": ["B. Thompson"], "venue": "Sage Publications,", "year": 2002}, {"title": "Lasso with nonlinear measurements is equivalent to one with linear measurements", "authors": ["C. Thrampoulidis", "E. Abbasi", "B. Hassibi"], "venue": "In Proc. Adv. Neural Info. Proc. Syst.,", "year": 2015}, {"title": "The effect of uncertainty of item parameter estimation on ability estimates", "authors": ["R.K. Tsutakawa", "J.C. Johnson"], "year": 1990}, {"title": "Whose vote should count more: Optimal integration of labels from labelers of unknown expertise", "authors": ["J. Whitehill", "T. Wu", "J. Bergsma", "J.R. Movellan", "P.L. Ruvolo"], "venue": "In Proc. Adv. Neural Info. Proc. Syst.,", "year": 2009}, {"title": "Characterizing sources of uncertainty in item response theory scale scores", "authors": ["J.S. Yang", "M. Hansen", "L. Cai"], "venue": "Educ. Psychol. Meas.,", "year": 2012}, {"title": "Investigating the impact of uncertainty about item parameters on ability estimation", "authors": ["J. Zhang", "M. Xie", "X. Song", "T. Lu"], "year": 2011}], "id": "SP:7db004853a362b675c5b4c71ac1957a14a2cebd2", "authors": [{"name": "Andrew S. Lan", "affiliations": []}, {"name": "Mung Chiang", "affiliations": []}, {"name": "Christoph Studer", "affiliations": []}], "abstractText": "The Rasch model is widely used for item response analysis in applications ranging from recommender systems to psychology, education, and finance. While a number of estimators have been proposed for the Rasch model over the last decades, the available analytical performance guarantees are mostly asymptotic. This paper provides a framework that relies on a novel linear minimum mean-squared error (L-MMSE) estimator which enables an exact, nonasymptotic, and closed-form analysis of the parameter estimation error under the Rasch model. The proposed framework provides guidelines on the number of items and responses required to attain low estimation errors in tests or surveys. We furthermore demonstrate its efficacy on a number of real-world collaborative filtering datasets, which reveals that the proposed L-MMSE estimator performs on par with state-of-the-art nonlinear estimators in terms of predictive performance.", "title": "An Estimation and Analysis Framework for the Rasch Model"}