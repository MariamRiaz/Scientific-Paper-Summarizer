{"sections": [{"heading": "1. Introduction", "text": "We consider the following optimization problem\nmin z\u2208RM g(z) := N\u2211 i=1 fi(z), (1)\nwhere each fi, i \u2208 {1, \u00b7 \u00b7 \u00b7 , N} := [N ] is a nonconvex cost function, and we assume that it is smooth and has Lipschitz continuous gradient.\nSuch a finite sum problem plays a central role in machine learning and signal/information processing (Cevher et al., 2014; Hong et al., 2016). In particular, in the class of empirical risk minimization (ERM) problem, z represents the feature vectors to be learned, and each fi can represent: 1) a mini-batch of (possibly nonconvex) loss functions modeling data fidelity (Antoniadis et al., 2009); 2) nonconvex activation functions of neural networks (Allen-Zhu & Hazan,\n1Department of Industrial and Manufacturing Systems Engineering, Iowa State University, Ames, IA, USA 2College of Information Science and Electronic Engineering, Zhejiang University, China. Correspondence to: Mingyi Hong <mingyi@iastate.edu>, Davood Hajinezhad <dhaji@iastate.edu>, Ming-Min Zhao <zmmblack@zju.edu.cn>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\n2016); 3) nonconvex utility functions used in applications such as resource allocation (Bjornson & Jorswieck, 2013). Recently, a number of works in machine learning community have been focused on designing fast algorithms for solving problem (1) in centralized setting; e.g., SAG (Defazio et al., 2014), SAGA (Schmidt et al., 2013), and SVRG (Johnson & Zhang, 2013) for convex problems, and (Reddi et al., 2016; Allen-Zhu & Hazan, 2016; Hajinezhad et al., 2016b; Rahimpour et al., 2016) for nonconvex problems.\nIn this work, we are interested in designing algorithms that solve problem (1) in a distributed manner. In particular, we focus on the scenario where each fi (or equivalently, each subset of data points in the ERM problem) is available locally at a given computing node i \u2208 [N ], and the nodes are connected via a network. Clearly, such distributed optimization and learning scenario is important for machine learning, because in contemporary applications such as document topic modeling and/or social network data analysis, oftentimes data corporas are stored in geographically distributed locations without any central controller managing the entire network of nodes; see (Forero et al., 2010; Yan et al., 2013; Rahmani & Atia, 2015; Aybat & Hamedani, 2016).\nRelated Works. Distributed convex optimization and learning has been thoroughly investigated in the literature. In (Nedic & Ozdaglar, 2009b), the authors propose a distributed subgradient algorithm (DSG), which allows the agents to jointly optimize problem (1). Subsequently, many variants of DSG have been proposed, either with special assumptions on the underlying graph, or having additional structures of the problem; see, e.g., (Lobel & Ozdaglar, 2011; Lobel et al., 2011; Nedic & Olshevsky, 2015). The rate of convergence for DSG is O(log(r)/ \u221a r) under certain diminishing stepsize rules. Recently, a number of algorithms such as the exact first-order algorithm (EXTRA) (Shi et al., 2014) and DLM (Ling et al., 2015) have been proposed, which use constant stepsize and achieve faster O(1/r) rate for convex problems. Recent works that apply distributed optimization algorithms to machine learning applications include (Scardapane et al., 2016; Aybat & Hamedani, 2016; Scardapane & Lorenzo, 2016).\nOn the other hand, there has been little work for dis-\ntributed optimization and learning when the objective function involves nonconvex problems. A dual subgradient method has been proposed in (Zhu & Martinez, 2010), which relaxes the exact consensus constraint. In (Bianchi & Jakubowicz, 2013) a stochastic projection algorithm using diminishing stepsizes has been proposed. An ADMM based algorithm has been presented in (Hong et al., 2014; Hajinezhad & Hong, 2015) for a special type of problem called global consensus, where all distributed nodes are directly connected to a central controller. Utilizing certain convexification decomposition technique the authors of (Lorenzo & Scutari, 2016) designed an algorithm named NEXT, which converges to the set of stationary solutions when using diminishing stepsizes. To the best of our knowledge, no multi agent distributed algorithm is able to guarantee global sublinear convergence rate for problem (1).\nOur Contributions. In this work, we propose a proximal primal-dual algorithm (Prox-PDA) for problem (1) over an undirected connected network. We show that Prox-PDA converges to the set of stationary solutions of problem (1) (satisfying the first-order optimality condition) in a globally sublinear manner. We also show that Prox-PDA can be extended in several directions to improve its practical performance. To the best of our knowledge, this is the first algorithm that is capable of achieving global sublinear convergence rate for distributed non-convex optimization.\nFurther, our work reveals an interesting connection between the primal-dual based algorithm Prox-PDA and the primal-only fast distributed algorithms such as EXTRA (Shi et al., 2014). Such new insight of the connection between primal-dual and primal-only algorithms could be of independent interest for the optimization community. Finally, we generalize the theory for Prox-PDA based algorithms to a challenging distributed matrix factorization problem."}, {"heading": "2. System Model", "text": "Define a graph G := {V, E}, where V and E are the node and edge sets; Let |V| = N and |E| = E. Each node v \u2208 V represents an agent in the network, and each edge eij = (i, j) \u2208 E indicates that node i and j are neighbors; see Fig.1(Left). Assume that each node i can only communicate with its immediate neighbors, defined as Ni := {j | (i, j) \u2208 V}, with |Ni| = di. The distributed version of problem (1) is given as below\nmin xi\u2208RM f(x) := N\u2211 i=1 fi(xi), s.t. xi = xj , \u2200 (i, j) \u2208 E . (2)\nClearly the above problem is equivalent to (1) as long as G is connected. For notational simplicity, define x := {xi} \u2208 RNM\u00d71, and Q := N \u00d7M .\nTo proceed, let us introduce a few useful quantities related to graph G.\n\u2022 The incidence matrix A\u0303 \u2208 RE\u00d7N is a matrix with entires A\u0303(k, i) = 1 and A\u0303(k, j) = \u22121 if k = (i, j) \u2208 E with j > i, and all the rest of the entries being zero. For example, for the network in Fig.1 (Left); the incidence matrix is given in Fig.1 (Right). Define the extended incidence matrix as\nA := A\u0303\u2297 IM \u2208 REM\u00d7Q, (3)\nwhere \u2297 denotes the Kronecker product.\n\u2022 The Degree matrix D\u0303 \u2208 RN\u00d7N is given by D\u0303 := diag[d1, \u00b7 \u00b7 \u00b7 , dN ]; Let D := D\u0303 \u2297 IM \u2208 RQ\u00d7Q.\n\u2022 The signed and the signless Laplacian matrices (denoted as L\u2212 and L+ respectively), are given below\nL\u2212 := A>A \u2208 RQ\u00d7Q, L+ := 2D \u2212A>A \u2208 RQ\u00d7Q. (4)\nUsing the above notations, one can verify that problem (2) can be written in the following compact form\nmin x\u2208RQ f(x), s.t. Ax = 0. (5)"}, {"heading": "3. The Prox-PDA Algorithm", "text": "The proposed algorithm builds upon the classical augmented Lagrangian (AL) method (Bertsekas, 1982; Powell, 1969). Let us define the AL function for (5) as\nL\u03b2(x, \u00b5) = f(x) + \u3008\u00b5,Ax\u3009+ \u03b2\n2 \u2016Ax\u20162, (6)\nwhere \u00b5 \u2208 RQ is the dual variable; \u03b2 > 0 is a penalty parameter. Let B \u2208 RQ\u00d7Q be some arbitrary matrix to be determined shortly. Then the proposed algorithm is given in the table below (Algorithm 1). In Prox-PDA, the primal iteration (7a) minimizes the augmented Lagrangian plus a proximal term \u03b22 \u2016x \u2212 x\nr\u20162BTB . We emphasize that the proximal term is critical in both the algorithm implementation and the analysis. It is used to ensure the following key properties: (1). The primal problem is strongly convex; (2). The primal problem is decomposable over different network nodes, hence distributedly implementable. To see the first point, suppose BTB is chosen such that ATA + BTB IQ, and that f(x) has Lipschitz gradient. Then by a result in (Zlobec, 2005)[Theorem 2.1], we know\nAlgorithm 1 The Prox-PDA Algorithm\n1: At iteration 0, initialize \u00b50 = 0 and x0 \u2208 RQ. 2: At each iteration r + 1, update variables by:\nxr+1 = arg min x\u2208RQ f(x) + \u3008\u00b5r, Ax\u3009+ \u03b2 2 \u2016Ax\u20162\n+ \u03b2\n2 \u2016x\u2212 xr\u20162BTB ; (7a)\n\u00b5r+1 = \u00b5r + \u03b2Axr+1. (7b)\nthat there exists \u03b2 > 0 large enough such that the objective function of (7a) is strongly convex. To see the second point, Let B := |A|, where the absolute value is taken for each component of A. It can be verified that BTB = L+, and step (7a) becomes\nxr+1 = argmin x N\u2211 i=1 fi(xi) + \u3008\u00b5r, Ax\u3009\n+ \u03b2 2 xTL\u2212x+ \u03b2 2 (x\u2212 xr)TL+(x\u2212 xr)\n= argmin x N\u2211 i=1 fi(xi) + \u3008\u00b5r, Ax\u3009+ \u03b2xTDx\u2212 \u03b2xTL+xr.\nClearly this problem is separable over the nodes, therefore it can be solved completely distributedly."}, {"heading": "4. The Convergence Analysis", "text": "In this section we provide convergence analysis for Algorithm 1. The key in the analysis is the construction of a novel potential function, which decreases at every iteration of the algorithm. In particular, the constructed potential function is a conic combination of the AL function and the size of the violation of the consensus constraint, therefore it measures the progress of both the primal and dual updates.\nWe first state our main assumptions below.\n[A1.] The function f(x) is differentiable and has Lipschitz continuous gradient, i.e.,\n\u2016\u2207f(x)\u2212\u2207f(x)\u2016 \u2264 L\u2016x\u2212 y\u2016, \u2200 x, y \u2208 RQ.\nFurther assume that ATA+BTB IQ.\n[A2.] There exists a constant \u03b4 > 0 such that\n\u2203 f > \u2212\u221e, s.t. f(x) + \u03b4 2 \u2016Ax\u20162 \u2265 f, \u2200 x \u2208 RQ.\nWithout loss of generality we will assume that f = 0. Below we provide a few nonconvex smooth functions that satisfy our assumptions, all of which are commonly used as activation functions for neural networks.\n\u2022 The sigmoid function sigmoid(x) = 11+e\u2212x . \u2022 The arctan and tanh function. \u2022 The logit function logit(x) = e x\nex+1 ."}, {"heading": "4.1. The Analysis Steps", "text": "Below we provide the analysis of Prox-PDA. First we provide a bound on the size of the constraint violation using a quantity related to the primal iterates. Let \u03c3min denotes the smallest non-zero eigenvalue of ATA, and we define wr := (xr+1\u2212xr)\u2212(xr\u2212xr\u22121) for notational simplicity. We have the following result. Lemma 1 Suppose Assumptions [A1] and [A2] are satisfied. Then the following is true for Prox-PDA\n1 \u03b2 \u2016\u00b5r+1 \u2212 \u00b5r\u20162\n\u2264 2L 2\n\u03b2\u03c3min \u2225\u2225xr \u2212 xr+1\u2225\u22252 + 2\u03b2 \u03c3min \u2016BTBwr\u20162. (8)\nThen we bound the descent of the AL function. Lemma 2 Suppose Assumptions [A1] and [A2] are satisfied. Then the following is true for Algorithm 1\nL\u03b2(x r+1, \u00b5r+1)\u2212 L\u03b2(xr, \u00b5r) \u2264 2\u03b2\u2016BTB\u2016 \u03c3min \u2016wr\u20162BTB\n\u2212 ( \u03b2 \u2212 L\n2 \u2212 2L\n2\n\u03b2\u03c3min\n) \u2016xr+1 \u2212 xr\u20162. (9)\nA key observation from Lemma 2 is that no matter how large \u03b2 is, the rhs of (9) cannot be made negative. This observation suggests that in contrast to (Hong et al., 2014; Hajinezhad et al., 2016a) the augmented Lagrangian alone cannot serve as the potential function for Prox-PDA. In search for an appropriate potential function, we need a new object that is decreasing in the order of \u03b2 \u2016wr\u20162BTB . The following lemma shows that the descent of the sum of the constraint violation and the proximal term has the desired property.\nLemma 3 Suppose Assumption [A1] is satisfied. Then the following is true\n\u03b2\n2\n( \u2016Axr+1\u20162 + \u2016xr+1 \u2212 xr\u20162BTB ) \u2264 L\u2016xr+1 \u2212 xr\u20162 + \u03b2\n2\n( \u2016Axr\u20162 + \u2016xr \u2212 xr\u22121\u20162BTB ) \u2212 \u03b2\n2\n( \u2016wr\u20162BTB + \u2016A(x r+1 \u2212 xr)\u20162 ) . (10)\nIt is interesting to observe that the new object, \u03b2/2 ( \u2016Axr+1\u20162 + \u2016xr+1 \u2212 xr\u20162BTB ) , increases in L\u2016xr+1 \u2212 xr\u20162 and decreases in \u03b2/2\u2016wr\u20162BTB , while the AL behaves in an opposite manner (cf. Lemma 2). More importantly, in our new object, the constant in front of \u2016xr+1 \u2212 xr\u20162 is independent of \u03b2. Although neither of these two objects decreases by itself, quite surprisingly, a proper conic combination of these two objects decreases at every iteration of Prox-PDA. To precisely state the claim, let us define the potential function for Algorithm 1 as\nPc,\u03b2(x r+1, xr, \u00b5r+1) := L\u03b2(x r+1, \u00b5r+1)\n+ c\u03b2\n2\n( \u2016Axr+1\u20162 + \u2016xr+1 \u2212 xr\u20162BTB ) , (11)\nwhere c > 0 is some constant to be determined later. We have the following result.\nLemma 4 Suppose the assumptions made in Lemmas 1 \u2013 3 are satisfied. Then we have the following\nPc,\u03b2(x r+1, xr, \u00b5r+1) \u2264 Pc,\u03b2(xr, xr\u22121, \u00b5r) \u2212 ( \u03b2 \u2212 L\n2 \u2212 2L\n2\n\u03b2\u03c3min \u2212 cL\n) \u2016xr+1 \u2212 xr\u20162\n\u2212 ( c\u03b2\n2 \u2212 2\u03b2\u2016B TB\u2016F \u03c3min\n) \u2016wr\u20162BTB . (12)\nFrom the above analysis, it is easy to see that as long as c and \u03b2 are sufficiently large, the potential function decreases at each iteration of Prox-PDA. Below we derive the precise bounds for c and \u03b2. First, a sufficient condition for c is given below (note, that \u03b4 > 0 is defined in Assumption [A2])\nc \u2265 max { \u03b4\nL , 4\u2016BTB\u2016F \u03c3min\n} . (13)\nSecond, for any given c, we need \u03b2 to satisfy \u03b2\u2212L2 \u2212 2L2\n\u03b2\u03c3min \u2212 cL > 0, which implies the following\n\u03b2 > L\n2 2c+ 1 +\u221a(2c+ 1)2 + 16L2 \u03c3min  . (14) We conclude that if both (13) and (14) are satisfied, then the potential function Pc,\u03b2(xr+1, xr, \u00b5r+1) decreases at every iteration.\nOur next step shows that by using the particular choices of c and \u03b2 in (13) and (14), the constructed potential function is lower bounded.\nLemma 5 Suppose [A1] - [A2] are satisfied, and (c, \u03b2) are chosen according to (13) and (14). Then the following statement holds true\n\u2203 P > \u2212\u221e s.t. Pc,\u03b2(xr+1, xr, \u00b5r+1) \u2265 P , \u2200 r > 0. Now we are ready to present the main result of this section. To this end, define Q(xr+1, \u00b5r) as the optimality gap of problem (5), given by\nQ(xr+1, \u00b5r) := \u2016\u2207xL\u03b2(xr+1, \u00b5r)\u20162 + \u2016Axr+1\u20162. (15)\nIt is easy to see that Q(xr+1, \u00b5r) \u2192 0 implies that any limit point (x\u2217, \u00b5\u2217), if it exists, is a KKT point of (5) that satisfies the following conditions\n0 = \u2207f(x\u2217) +AT\u00b5\u2217, Ax\u2217 = 0. (16)\nIn the following we show that the gap Q(\u00b7) not only decreases to zero, but does so in a sublinear manner.\nTheorem 1 Suppose Assumption A and the conditions (13) and (14) are satisfied. Then we have \u2022 Eventual Consensus:\nlim r\u2192\u221e \u00b5r+1 \u2212 \u00b5r \u2192 0, lim r\u2192\u221e Axr \u2192 0.\n\u2022 Convergence to Stationary Points: Every limit point of the iterates {xr, \u00b5r} generated by Algorithm 1 converges to a KKT point of problem (5). Further, Q(xr+1, \u00b5r)\u2192 0. \u2022 Sublinear Convergence Rate: For any given \u03d5 > 0, let us define T to be the first time that the optimality gap reaches below \u03d5, i.e.,\nT := arg min r Q(xr+1, \u00b5r) \u2264 \u03d5.\nThen for some \u03bd > 0, we have \u03d5 \u2264 \u03bdT\u22121 . That is, the optimality gap Q(xr+1, \u00b5r) converges sublinearly."}, {"heading": "5. Variants of Prox-PDA", "text": "In this section, we discuss two important extensions of the Prox-PDA, one allows the x-problem (7a) to be solved inexactly, while the second allows the use of increasing penalty parameter \u03b2. In many practical applications, exactly minimizing the augmented Lagrangian may not be easy. Therefore, we propose the proximal gradient primaldual algorithm (Prox-GPDA), whose main steps are given below\nxr+1 = arg min x\u2208RQ \u3008\u2207f(xr), x\u2212 xr\u3009+ \u3008\u00b5r, Ax\u3009\n+ \u03b2 2 \u2016Ax\u20162 + \u03b2 2 \u2016x\u2212 xr\u20162BTB ; (17) \u00b5r+1 = \u00b5r + \u03b2Axr+1. (18)\nThe analysis of this algorithm follows similar steps as that for Prox-PDA. For detailed discussion see (Hong, 2016).\nOur second variant do not require to explicitly compute the bound for \u03b2 given in (14). Indeed, the bounds on \u03b2 derived in the previous sections are the worst case bounds, and algorithms that use stepsizes that strictly satisfy such bounds may be slow at the beginning. In practice, one may prefer to start with a small penalty parameter and gradually increase it. We denote this algorithm by Prox-PDA-IP, whose main steps are given below\nxr+1 = arg min x\u2208RQ f(x) + \u3008\u00b5r, Ax\u3009\n+ \u03b2r+1 2 \u2016Ax\u20162 + \u03b2 r+1 2 \u2016x\u2212 xr\u20162BTB ; (19)\n\u00b5r+1 = \u00b5r + \u03b2r+1Axr+1. (20)\nNote that one can also replace f(x) in (19) by \u3008\u2207f(xr), x\u2212 xr\u3009 to obtain a similar variant for Prox-GPDA denoted by Prox-GPDA-IP. Throughout this section we will still as-\nsume that Assumption A holds true. Further, we will assume that \u03b2r satisfies the following conditions\n1\n\u03b2r \u2192 0, \u221e\u2211 r=1 1 \u03b2r =\u221e, \u03b2r+1 \u2265 \u03b2r,\nmax r\n(\u03b2r+1 \u2212 \u03b2r) < \u03ba, for some finite \u03ba > 0. (21)\nAlso without loss of generality we will assume that\nBTB 0, and \u2016BTB\u2016F > 1. (22)\nNote that this is always possible, by adding an identity matrix to BTB if necessary.\nThe analysis for Prox-PDA-IP is long and technical, therefore we refer the readers to (Hong, 2016). The key step is to construct a new potential function, given below\nP\u03b2r+1,c(x r+1, xr, \u00b5r+1) = L\u03b2r+1(x r+1, \u00b5r+1)\n+ c\u03b2r+1\u03b2r 2 \u2016Axr+1\u20162 + c\u03b2 r+1\u03b2r 2 \u2016xr \u2212 xr+1\u20162BTB .\nThe insight here is that in order to achieve the desired descent, in the potential function the coefficients for \u2016xr+1\u2212 xr\u20162BTB and \u2016Ax\nr+1\u20162 should be proportional to O ( (\u03b2r)2 ) . We have the following theorem regarding to the convergence of Prox-PDA-IP.\nTheorem 2 Suppose Assumption A and (21) are satisfied. Suppose that B is selected such that (22) holds true. Then the following hold for Prox-PDA-IP\n\u2022 Eventual Consensus: lim r\u2192\u221e \u00b5r+1 \u2212 \u00b5r \u2192 0, lim r\u2192\u221e Axr \u2192 0.\n\u2022 Convergence to KKT Points: Every limit point of the iterates {xr, \u00b5r} generated by Prox-PDA-IP converges to a KKT point of problem (5). Further, Q(xr+1, \u00b5r)\u2192 0."}, {"heading": "6. Connections and Discussions", "text": "In this section we present an interesting observation which establishes links between the so-called EXTRA algorithm (Shi et al., 2014) (developed for distributed, but convex optimization) and the Prox-GPDA.\nSpecifically, the optimality condition of the x-update step (17) is given by\n\u2207f(xr) +AT (\u00b5r + \u03b2Axr+1) + \u03b2(BTB(xr+1 \u2212 xr)) = 0.\nUtilizing the fact that ATA = L\u2212, BTB = L+ and L+ + L\u2212 = 2D, we have\n\u2207f(xr) +AT\u00b5r + 2\u03b2Dxr+1 \u2212 \u03b2L+xr = 0.\nSubtracting the same equation evaluated at the previous iteration, we obtain\n\u2207f(xr)\u2212\u2207f(xr\u22121) + \u03b2L\u2212xr + 2\u03b2D(xr+1 \u2212 xr) \u2212 \u03b2L+(xr \u2212 xr\u22121) = 0,\nwhere we have used the fact that AT (\u00b5r \u2212 \u00b5r\u22121) = \u03b2ATAxr = \u03b2L\u2212xr. Rearranging terms, we have\nxr+1 = xr \u2212 1 2\u03b2 D\u22121\n( \u2207f(xr)\u2212\u2207f(xr\u22121) ) + 1\n2 D\u22121(L+ \u2212 L\u2212)xr \u2212 1 2 D\u22121L+xr\u22121\n= xr \u2212 1 2\u03b2 D\u22121\n( \u2207f(xr)\u2212\u2207f(xr\u22121) ) +Wxr\n\u2212 1 2 (I +W )xr\u22121, (23)\nwhere in the last equality we have defined the weight matrix W := 12D\n\u22121(L+ \u2212 L\u2212), which is a row stochastic matrix.\nIteration (23) has the same form as the EXTRA algorithm given in (Shi et al., 2014), therefore we can conclude that EXTRA is a special case of Prox-GPDA. Moreover, by appealing to our analysis in Section 5, it readily follows that EXTRA works for the nonconvex distributed optimization problem as well.\nWe remark that each node i can distributedly implement iteration (23) by performing the following\nxr+1i = x r i \u2212\n1\n2\u03b2di\n( \u2207fi(xri )\u2212\u2207fi(xr\u22121i ) ) + \u2211\nj\u2208N (i)\n1 di xrj \u2212 1 2  \u2211 j\u2208N (i) 1 di xr\u22121j + x r\u22121 i  . (24) Clearly, at iteration r + 1, besides the local gradient information, node i only needs the aggregated information from its neighbors, \u2211 j\u2208N (i) x r j . Therefore the algorithm is distributedly implementable."}, {"heading": "7. Distributed Matrix Factorization", "text": "In this section we study a variant of the Prox-PDA/ProxPDA-IP for the following distributed matrix factorization problem (Ling et al., 2012)\nmin X,Y\n1 2 \u2016XY \u2212 Z\u20162F + \u03b7\u2016X\u20162F + h(Y ) (25)\n= N\u2211 i=1 1 2 \u2016Xyi \u2212 zi\u20162 + \u03b3\u2016X\u20162F + hi(yi),\ns.t. \u2016yi\u20162 \u2264 \u03c4, \u2200 i\nwhere X \u2208 RM\u00d7K , Y \u2208 RK\u00d7N ; for each i, yi \u2208 RK consists of one column of Y ; Z \u2208 RM\u00d7N is some known matrix; h(Y ) := \u2211N i=1 hi(yi) is some convex but possibly nonsmooth penalization term; \u03b7 > 0 is some given constant; for notation simplicity we have defined \u03b3 := 1/N\u03b7.\nIt is easy to extend the above formulation to the case where Y and Z both have NP columns, and each yi and zi consists of P columns of Y and Z respectively.\nWe assume that h(Y ) is lower bounded over dom (h). One application of problem (25) is the distributed sparse dictionary learning problem where X is the dictionary to be learned, each zi is a training data sample, and each yi is the sparse coefficient corresponding to the particular training sample zi. The constraint \u2016yi\u20162 \u2264 \u03c4 simply says that the size of the coefficient must be bounded.\nConsider a distributed scenario where N agents form a graph {V, E}, each having a column of Y . We reformulate problem (25) as\nmin {Xi},{yi} N\u2211 i=1 ( 1 2 \u2016Xiyi \u2212 zi\u20162 + hi(yi) + \u03b3\u2016Xi\u20162F ) s.t. \u2016yi\u20162 \u2264 \u03c4, \u2200 i Xi = Xj , \u2200 (i, j) \u2208 E .\nLet us stack all the variables Xi, and define X := [X1;X2; \u00b7 \u00b7 \u00b7 ;XN ] \u2208 RNM\u00d7K . Define the block signed incidence matrix as A = A\u0303 \u2297 IM \u2208 REM\u00d7NM , where A is the standard graph incidence matrix. Define the block signless incidence matrix B \u2208 REM\u00d7NM similarly. If the graph is connected, then the condition AX = 0 implies network-wide consensus. We formulate the distributed matrix factorization problem as\nmin {Xi},{yi} f(X, Y ) + h(Y )\n:= N\u2211 i=1 ( 1 2 \u2016Xiyi \u2212 zi\u20162 + \u03b3\u2016Xi\u20162F + hi(yi) ) s.t. \u2016yi\u20162 \u2264 \u03c4, \u2200 i AX = 0. (26)\nClearly the above problem does not satisfy Assumption A, because the objective function is not smooth, and neither \u2207Xf(X, Y ) nor \u2207Y f(X, Y ) is Lipschitz continuous. The latter fact poses significant difficulty in algorithm development and analysis. Define the block-signed/signless Laplacians as\nL\u2212 = ATA, L+ = BTB. (27)\nThe AL function for the above problem is given by\nL\u03b2(X, Y,\u2126) = N\u2211 i=1 ( 1 2 \u2016Xiyi \u2212 zi\u20162 + \u03b3\u2016Xi\u20162F + hi(yi) ) + \u3008\u2126,AX\u3009+ \u03b2\n2 \u3008AX,AX\u3009, (28)\nwhere \u2126 := {\u2126e} \u2208 REM\u00d7K is the matrix of the dual variable, with \u2126e \u2208 RM\u00d7K being the dual variable for the consensus constraint on link e, i.e, Xi = Xj , e = (i, j).\nLet us generalize Algorithm 1 for distributed matrix factorization given in Algorithm 2. In Algorithm 2 we have introduced a sequence {\u03b8ri \u2265 0} which measures the size\nAlgorithm 2 Prox-PDA for Distr. Matrix Factorization\n1: At iteration 0, initialize \u21260 = 0, and X0, y0 2: At each iteration r + 1, update variables by:\n\u03b8ri = \u2016Xri yri \u2212 zi\u20162, \u2200 i; (29a)\nyr+1i = arg min \u2016yi\u20162\u2264\u03c4\n1 2 \u2016Xri yi \u2212 zi\u20162 + hi(yi)\n+ \u03b8ri 2 \u2016yi \u2212 yri \u20162, \u2200 i; (29b)\nXr+1 = argmin X f(X, Y r+1) + \u3008\u2126r,AX\u3009 (29c)\n+ \u03b2 2 \u3008AX,AX\u3009+ \u03b2 2 \u3008B(X \u2212Xr),B(X \u2212Xr)\u3009;\n\u2126r+1 = \u2126r + \u03b2AXr+1. (29d)\nof the local factorization error. We note that including the proximal term \u03b8 r i\n2 \u2016yi \u2212 y r i \u20162 is the key to achieve conver-\ngence for Algorithm 2.\nLet us comment on the distributed implementation of the algorithm. First note that the y subproblem (29b) is naturally distributed to each node, that is, only local information is needed to perform the update. Second, the X subproblem (29c) can also be decomposed into N subproblems, one for each node. To be more precise, let us examine the terms in (29c) one by one. First, the term f(X, Y r+1) =\u2211N i=1 ( 1 2\u2016Xiy r+1 i \u2212 zi\u20162 + hi(y r+1 i ) + \u03b3\u2016Xi\u20162F ) , hence it is decomposable. Second, the term \u3008\u2126r,AX\u3009 can be expressed as\n\u3008\u2126r,AX\u3009 = N\u2211 i=1 \u2211 e\u2208U(i) \u3008\u2126re, Xi\u3009 \u2212 \u2211 e\u2208H(i) \u3008\u2126re, Xi\u3009\nwhere the sets U(i) and H(i) are defined as U(i) := {e | e = (i, j) \u2208 E , i \u2265 j}, H(i) := {e | e = (i, j) \u2208 E , j \u2265 i}.\nSimilarly, we have \u3008BXr,BX\u3009 = N\u2211 i=1\n\u2329 Xi, diX r i + \u2211 j\u2208N(i) Xrj \u232a \u03b2 2 (\u3008AX,AX\u3009+ \u3008BX,BX\u3009)\n= \u03b2\u3008DX,X\u3009 = \u03b2 N\u2211 i=1 di\u2016Xi\u20162F ,\nwhere D := D\u0303 \u2297 IM \u2208 RNM\u00d7NM with D\u0303 being the degree matrix. It is easy to see that the X subproblem (29c) is separable over the distributed agents. Finally, one can verify that the \u2126 update step (29d) can be implemented by each edge e \u2208 E as follows\n\u2126r+1e = \u2126 r e + \u03b2 ( Xr+1i \u2212X r+1 j ) , e = (i, j), i \u2265 j.\nTo show convergence rate of the algorithm, we need the following definition Q(Xr+1, Y r+1,\u2126r) := \u03b2\u2016AXr+1\u20162 + \u2016[Zr+11 ;Z r+1 2 ]\u2016 2,\nwhere we have defined Zr+11 := \u2207XL\u03b2(X r+1, Y r+1,\u2126r); Zr+12 := Y r+1\n\u2212 proxh+\u03b9(Y) [ Y r+1 \u2212\u2207Y ( L\u03b2(X r+1, Y r+1,\u2126r)\u2212 h(Y ) )] .\nIn the above expression, we have used Y :=\u22c3 i { \u2016yi\u20162 \u2264 \u03c4 } to denote the feasible set of Y , and used \u03b9(Y) to denote the indicator function of such set. Similarly as in Section 4, we can show that Q(Xr+1, Y r+1,\u2126r) \u2192 0 implies that every limit point of (Xr+1, Y r+1,\u2126r) is a KKT point of problem (26).\nNext we present the main convergence analysis for Algorithm 2. The proof is long and technical, therefore we refer the readers to (Hong, 2016).\nTheorem 3 Consider using Algorithm 2 to solve the distributed matrix factorization problem (26). Suppose that h(Y ) is lower bounded over dom h(x), and that the penalty parameter \u03b2, together with two positive constants c and d, satisfies the following conditions\n\u03b2 + 2\u03b3 2 \u2212 8(\u03c4\n2 + 4\u03b32)\n\u03b2\u03c3min \u2212 cd 2 > 0,\n1 2 \u2212 8 \u03c3min\u03b2 \u2212 c d > 0, 1 2 \u2212 8\u03c4 \u03c3min\u03b2 \u2212 c\u03c4 d > 0,\nc\u03b2 2 \u2212 2\u03b2\u2016B TB\u2016 \u03c3min > 0.\n(30)\nThen in the limit, consensus will be achieved, i.e.,\nlim r\u2192\u221e\n\u2016Xri \u2212Xrj \u2016 = 0, \u2200 (i, j) \u2208 E .\nFurther, the sequences {Xr+1} and {\u2126r+1} are both bounded, and every limit point generated by Algorithm 2 is a KKT point of problem (25).\nAdditionally, Algorithm 2 converges sublinearly. Specifically, for any given \u03d5 > 0, define T to be the first time that the optimality gap reaches below \u03d5, i.e.,\nT := argmin r\nQ(Xr+1, Y r+1,\u2126r) \u2264 \u03d5.\nThen for some constant \u03bd > 0 we have \u03d5 \u2264 \u03bdT\u22121 . We can see that it is always possible to find the tuple {\u03b2, c, d > 0} that satisfies (30): c can be solely determined by the last inequality; for fixed c, the constant d needs to be chosen large enough such that 1/2 \u2212 cd > 0 and 1/2 \u2212 c\u03c4d > 0 are satisfied. After c and d are fixed, one can always choose \u03b2 large enough to satisfy the first three conditions. In practice, we typically prefer to choose \u03b2 as small as possible to improve the convergence speed. Therefore empirically one can start with (for some small \u03bd > 0): c = 4\u2016B\nTB\u2016 \u03c3min + \u03bd, d = max{4, 2c\u03c4}, and then\ngradually increase d to find an appropriate \u03b2 that satisfies the first three conditions.\nWe remark that Algorithm 2 can be extended to the case with increasing penalty. Due to the space limitation we omit the details here."}, {"heading": "8. Numerical Results", "text": "In this section, we demonstrate the performance of the proposed algorithms. All experiments are performed in Matlab (2016b) on a desktop with an Intel Core(TM) i5-4690 CPU (3.50 GHz) and 8GB RAM running Windows 7."}, {"heading": "8.1. Distributed Binary Classification", "text": "In this subsection, we study the problem of binary classification using nonconvex regularizers in the mini-bach setup i.e. each node stores b (batch size) data points, and each component function is given by\nfi(xi) = 1\nNb [ b\u2211 j=1 log(1 + exp(\u2212yijxTi vij)) + M\u2211 k=1 \u03bb\u03b1x2i,k 1 + \u03b1x2i,k ]\nwhere vij \u2208 RM and yij \u2208 {1,\u22121} are the feature vector and the label for the jth date point in ith agent (Antoniadis et al., 2009). We use the parameter settings of \u03bb = 0.001, \u03b1 = 1 and M = 10. We randomly generated 100, 000 data points and distribute them into N = 20 nodes (i.e. b = 5000). We use the optimality gap (opt-gap) and constraint violation (con-vio), displayed below, to measure the quality of the solution generated by different algorithms\nopt-gap := \u2225\u2225\u2225\u2225 N\u2211 i=1 \u2207fi(zi) \u2225\u2225\u2225\u22252 + \u2016Ax\u20162, con-vio = \u2016Ax\u20162.\nWe compare the the Prox-GPDA, and Prox-GPDA-IP with the distributed subgradient (DSG) method (Nedic & Ozdaglar, 2009a;b) (which is only known to work for convex cases) and the Push-sum algorithm (Tatarenko & Touri, 2015). The performance of all three algorithms in terms of the consensus error and the optimality gap (averaged over 30 problem instances) are presented in Fig. 2. The penalty parameter for Prox-GPDA is chosen such that satisfy (14), and \u03b2r for Prox-GPDA-IP is set as 0.05 log(r), the stepsizes of the DSG algorithm and the Push-sum algorithm are chosen as 1/0.05 log(r) and 1/r, respectively. Note that these parameters are tuned for each algorithm to achieve the best results. All Algorithms will stop after 1000 iterations. It can be observed that the Prox-GPDA with constant stepsize outperforms other algorithms. The Push-sum algorithm does not seem to converge within 1000 iterations.\nTo see more results, we compare different algorithms with different number of agents in the network (N ). The problem and the algorithms setup are set as the previous case. We measure the optimality gap as well as the constraint violation and the results are respectively reported in Table 1 and Table 2. In the tables Alg1, Alg2, Alg3, Alg4\ndenote Prox-GPDA, Prox-GPDA-IP, DGS, and Push-sum algorithms respectively. As seen, the performance of the proposed algorithms (Alg1, Alg2) are significantly better than DGS and Push-Sum."}, {"heading": "8.2. Distributed Matrix Factorization", "text": "In this section we consider the distributed matrix factorization problem (25). The training data is constructed\nby randomly extracting 300 overlapping patches from the 512 \u00d7 512 image of barbara.png, each with size 16 \u00d7 16 pixels. Each of the extracted patch is vectorized, resulting a training data set Z of size 256 \u00d7 300. We consider a network of N = 10 agents, and the columns of Z are evenly distributed among the agents (each having P = 30 columns). We compare Prox-PDA-IP with the EXTRAAO algorithm proposed in (H.-T. Wai & Scaglione, 2015). Note that the EXTRA-AO is also designed for a similar distributed matrix factorization problem and it works well in practice. However, it does not have formal convergence proof. We initialize both algorithms with X being the 2D discrete cosine transform (DCT) matrix. We set \u03b3 = 0.05, \u03c4 = 105 and \u03b2 = 0.001r, and the results are averaged over 30 problem instances. The stepsizes of the EXTRAAO is set as \u03b1AO = 0.03 and \u03b2AO = 0.002. In Fig. 3, we compare the performance of the proposed Prox-PDAIP and the EXTRA-AO. It can be observed that our proposed algorithm converges faster than the EXTRA-AO. We have observed that the EXTRA-AO does have reasonably good practical performance, however it lacks formal convergence proof."}, {"heading": "Acknowledgment", "text": "The authors supported by National Natural Science Foundation (Grant No. CCF-1526078)."}], "year": 2017, "references": [{"title": "Variance Reduction for Faster NonConvex Optimization", "authors": ["Z. Allen-Zhu", "E. Hazan"], "venue": "In Proceedings of the 33rd International Conference on Machine Learning,", "year": 2016}, {"title": "Penalized likelihood regression for generalized linear models with non-quadratic penalties", "authors": ["A. Antoniadis", "I. Gijbels", "M. Nikolova"], "venue": "Annals of the Institute of Statistical Mathematics,", "year": 2009}, {"title": "A primal-dual method for conic constrained distributed optimization problems", "authors": ["Aybat", "N-S", "Hamedani", "E-Y"], "venue": "Advances in Neural Information Processing Systems,", "year": 2016}, {"title": "Constrained Optimization and Lagrange Multiplier Method", "authors": ["D.P. Bertsekas"], "year": 1982}, {"title": "Convergence of a multi-agent projected stochastic gradient algorithm for non-convex optimization", "authors": ["P. Bianchi", "J. Jakubowicz"], "venue": "IEEE Transactions on Automatic Control,", "year": 2013}, {"title": "Optimal resource allocation in coordinated multi-cell systems", "authors": ["E. Bjornson", "E. Jorswieck"], "venue": "Foundations and Trends in Communications and Information Theory,", "year": 2013}, {"title": "Convex optimization for big data: Scalable, randomized, and parallel algorithms for big data analytics", "authors": ["V. Cevher", "S. Becker", "M. Schmidt"], "venue": "IEEE Signal Processing Magazine,", "year": 2014}, {"title": "Saga: A fast incremental gradient method with support for non-strongly convex composite objectives", "authors": ["A. Defazio", "F. Bach", "S. Lacoste-Julien"], "venue": "In The Proceeding of NIPS,", "year": 2014}, {"title": "Consensusbased distributed support vector machines", "authors": ["P.A. Forero", "A. Cano", "G.B. Giannakis"], "venue": "Journal of Machine Learning Research,", "year": 2010}, {"title": "A consensus-based decentralized algorithm for non-convex optimization with application to dictionary learning", "authors": ["H.-T. Wai", "T.-H. Chang", "A. Scaglione"], "venue": "In the Proceedings of the IEEE ICASSP,", "year": 2015}, {"title": "Nonconvex alternating direction method of multipliers for distributed sparse principal component analysis", "authors": ["D. Hajinezhad", "M. Hong"], "venue": "In IEEE Global Conference on Signal and Information Processing (GlobalSIP). IEEE,", "year": 2015}, {"title": "Nonnegative matrix factorization using admm: Algorithm and convergence analysis", "authors": ["D. Hajinezhad", "Chang", "T-H", "X. Wang", "Q. Shi", "M. Hong"], "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "year": 2016}, {"title": "Nestt: A nonconvex primal-dual splitting method for distributed and stochastic optimization", "authors": ["D. Hajinezhad", "M. Hong", "T. Zhao", "Z. Wang"], "venue": "In Advances in Neural Information Processing Systems", "year": 2016}, {"title": "Decomposing linearly constrained nonconvex problems by a proximal primal dual approach: Algorithms, convergence, and applications", "authors": ["M. Hong"], "venue": "arXiv preprint arXiv:1604.00543,", "year": 2016}, {"title": "Convergence analysis of alternating direction method of multipliers for a family of nonconvex problems", "authors": ["M. Hong", "Luo", "Z.-Q", "M. Razaviyayn"], "year": 2014}, {"title": "A unified algorithmic framework for block-structured optimization involving big data", "authors": ["M. Hong", "M. Razaviyayn", "Luo", "Z.-Q", "Pang", "J.-S"], "venue": "IEEE Signal Processing Magazine,", "year": 2016}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "authors": ["R. Johnson", "T. Zhang"], "venue": "In the Proceedings of the Neural Information Processing (NIPS)", "year": 2013}, {"title": "Decentralized low-rank matrix completion", "authors": ["Q. Ling", "Y. Xu", "W. Yin", "Z. Wen"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "year": 2012}, {"title": "DLM: Decentralized linearized alternating direction method of multipliers", "authors": ["Q. Ling", "W. Shi", "G. Wu", "A. Ribeiro"], "venue": "IEEE Transactions on Signal Processing,", "year": 2015}, {"title": "Distributed subgradient methods for convex optimization over random networks", "authors": ["I. Lobel", "A. Ozdaglar"], "venue": "Automatic Control, IEEE Transactions on,", "year": 2011}, {"title": "Distributed multi-agent optimization with state-dependent communication", "authors": ["I. Lobel", "A. Ozdaglar", "D. Feijer"], "venue": "Mathematical Programming,", "year": 2011}, {"title": "Next: In-network nonconvex optimization", "authors": ["Lorenzo", "P. Di", "G. Scutari"], "venue": "IEEE Transactions on Signal and Information Processing over Networks,", "year": 2016}, {"title": "Distributed optimization over timevarying directed graphs", "authors": ["A. Nedic", "A. Olshevsky"], "venue": "IEEE Transactions on Automatic Control,", "year": 2015}, {"title": "Cooperative distributed multi-agent optimization. In Convex Optimization in Signal Processing and Communications", "authors": ["A. Nedic", "A. Ozdaglar"], "year": 2009}, {"title": "Distributed subgradient methods for multi-agent optimization", "authors": ["A. Nedic", "A. Ozdaglar"], "venue": "IEEE Transactions on Automatic Control,", "year": 2009}, {"title": "An efficient method for nonlinear constraints in minimization problems", "authors": ["M.M.D. Powell"], "year": 1969}, {"title": "Distributed object recognition in smart camera networks", "authors": ["Rahimpour", "Alireza", "Taalimi", "Ali", "Luo", "Jiajia", "Qi", "Hairong"], "venue": "In IEEE International Conference on Image Processing,", "year": 2016}, {"title": "A decentralized approach to robust subspace recovery", "authors": ["M. Rahmani", "G. Atia"], "venue": "In 2015 53rd Annual Allerton Conference on Communication, Control, and Computing (Allerton),", "year": 2015}, {"title": "Fast incremental method for nonconvex optimization", "authors": ["S. Reddi", "S. Sra", "B. Poczos", "A. Smola"], "venue": "arXiv preprint arXiv:1603.06159,", "year": 2016}, {"title": "A framework for parallel and distributed training of neural networks", "authors": ["S. Scardapane", "Lorenzo", "P. Di"], "venue": "arXiv preprint arXiv:1610.07448,", "year": 2016}, {"title": "Distributed semi-supervised support vector machines", "authors": ["S. Scardapane", "R. Fierimonte", "Lorenzo", "P. Di", "M. Panella", "A. Uncini"], "venue": "Neural Networks,", "year": 2016}, {"title": "Minimizing finite sums with the stochastic average gradient", "authors": ["M. Schmidt", "Roux", "N. Le", "F. Bach"], "year": 2013}, {"title": "Extra: An exact first-order algorithm for decentralized consensus optimization", "authors": ["W. Shi", "Q. Ling", "G. Wu", "W. Yin"], "venue": "SIAM Journal on Optimization,", "year": 2014}, {"title": "Non-convex distributed optimization", "authors": ["T. Tatarenko", "B. Touri"], "venue": "arXiv Preprint:", "year": 2015}, {"title": "Distributed autonomous online learning: Regrets and intrinsic privacy-preserving properties", "authors": ["F. Yan", "S. Sundaram", "S.V.N. Vishwanathan", "Y. Qi"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "year": 2013}, {"title": "An approximate dual subgradient algorithm for multi-agent non-convex optimization", "authors": ["M. Zhu", "S. Martinez"], "venue": "IEEE Conference on Decision and Control (CDC),", "year": 2010}, {"title": "On the liu-floudas convexification of smooth programs", "authors": ["S. Zlobec"], "venue": "Journal of Global Optimization,", "year": 2005}], "id": "SP:7797ec742b3d17d96a1e60c77d20eaafe10b0f9c", "authors": [{"name": "Mingyi Hong", "affiliations": []}, {"name": "Davood Hajinezhad", "affiliations": []}, {"name": "Ming-Min Zhao", "affiliations": []}], "abstractText": "In this paper we consider nonconvex optimization and learning over a network of distributed nodes. We develop a Proximal Primal-Dual Algorithm (Prox-PDA), which enables the network nodes to distributedly and collectively compute the set of first-order stationary solutions in a global sublinear manner [with a rate of O(1/r), where r is the iteration counter]. To the best of our knowledge, this is the first algorithm that enables distributed nonconvex optimization with global sublinear rate guarantees. Our numerical experiments also demonstrate the effectiveness of the proposed algorithm.", "title": "Prox-PDA: The Proximal Primal-Dual Algorithm for Fast Distributed Nonconvex Optimization and Learning Over Networks"}