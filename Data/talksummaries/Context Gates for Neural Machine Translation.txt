25	10	Experimental results show that introducing context gates leads to an average improvement of +2.3 BLEU points over a standard attention-based NMT system (Bahdanau et al., 2015).
40	23	Specifically, the source representation si stands for source context, which embeds the information from the source sentence.
44	32	Reducing the effect of target context (i.e., the lines (1.0, 0.8) and (1.0, 0.5)) results in longer translations, while reducing the effect of source context (i.e., the lines (0.8, 1.0) and (0.5, 1.0)) leads to shorter translations.
45	40	When halving the effect of the target context, most of the generated translations reach the maximum length, which is three times the length of source sentence in this work.
51	17	NMT lacks a mechanism that guarantees that each source word is translated.4 The decoding state implicitly models the notion of “coverage” by recurrently reading the time-dependent source context si.
54	23	As shown in Table 1, NMT can get stuck in an infinite loop repeatedly generating a phrase due to the overwhelming influence of the source context.
59	57	Intuitively, at each decoding step i, the context gate looks at input signals from both the source (i.e., si) and target (i.e., ti−1 and yi−1) sides, and outputs a number between 0 and 1 for each element in the input vectors, where 1 denotes “completely transferring this” while 0 denotes “completely ignoring this”.
63	29	Again, m, n and n′ are the dimensions of word embedding, decoding state, and source representation, respectively.
64	10	Note that zi has the same dimensionality as the transferred input signals (e.g., Csi), and thus each element in the input vectors has its own weight.
69	10	Accordingly, the gate assigns higher weights to the source context and lower weights to the target context and then feeds them into the decoding activation layer.
120	26	For efficient training of the neural networks, we limited the source and target vocabularies to the most frequent 30K words in Chinese and English, covering approximately 97.7% and 99.3% of the data in the two languages respectively.
135	11	Over GroundHog (vanilla) We first carried out experiments on a simple decoder without gating function (Rows 2 and 3), to better estimate the impact of context gates.
145	66	Finally, our best model (Row 7) outperforms the SMT baseline system using the same data (Row 1) by 3.3 BLEU points.
148	49	Two human evaluators were asked to compare the translations of 200 source sentences randomly sampled from the test sets without knowing which system produced each translation.
149	51	Table 3 shows the results of subjective evaluation.
150	12	The two human evaluators made similar judgments: in adequacy, around 30% of GroundHog translations are worse, 52% are equal, and 18% are better; while in fluency, around 29% are worse, 52% are equal, and 19% are better.
152	54	Following Tu et al. (2016), we used the alignment error rate (AER) (Och and Ney, 2003) and its variant SAER to measure the alignment quality: SAER = 1− |MA ×MS |+ |MA ×MP ||MA|+ |MS | where A is a candidate alignment, and S and P are the sets of sure and possible links in the reference alignment respectively (S ⊆ P ).
157	17	One possible reason is that better estimated decoding states (from the context gate) and coverage information help to produce more concentrated alignments, as shown in Figure 6.
158	12	Table 5 shows a detailed analysis of architecture components measured in BLEU scores.
163	12	We follow Bahdanau et al. (2015) and group sentences of similar lengths together.
169	29	In other words, translations that contain higher zi (i.e., source context contributes more than target context) at many time steps are better in translation performance.
170	27	We used the mean of the sequence z1, .
171	10	, zI as the gate weight of each sentence.
172	21	We calculated the Pearson Correlation between the sentence-level gate weight and the corresponding improvement on translation performance (i.e., BLEU, adequacy, and fluency scores),9 as shown in Table 6.
173	19	We observed that context gate weight is positively correlated with translation performance improvement and that the correlation is higher on long sentences.
177	10	Integrating context gates improves the translation adequacy: this is exactly the peak days British people buying the supermarket .
180	15	at that time , the closure of 14 supermarkets made the biggest supermarket of britain lose millions of pounds of sales income .
184	19	Experimental results show that NMT with context gates achieves consistent and significant improvements in translation quality over different NMT models.
185	17	Context gates are in principle applicable to all sequence-to-sequence learning tasks in which information from the source sequence is transformed to the target sequence (corresponding to adequacy) and the target sequence is generated (corresponding to fluency).
187	48	It is also necessary to validate the effectiveness of our approach on more language pairs and other NMT architectures (e.g., using LSTM as well as GRU, or multiple layers).
