1	17	However, there are still many open problems in NMT (Koehn and Knowles, 2017).
2	23	One particular issue is mistranslation of rare words.
3	17	For example, consider the Uzbek sentence: Source: Ammo muammolar hali ko’p, deydi amerikalik olim Entoni Fauchi.
7	16	All three surnames occur in the training data with reference to immunologists: Fauci is the director of the National Institute of Allergy and Infectious Diseases, Margaret (not James) Chan is the former director of the World Health Organization, and Edward Jenner invented smallpox vaccine.
37	43	But because it is multiplied by cos θWe,h̃, it has a stronger effect on words whose embeddings have direction similar to h̃, and less effect or even a negative effect on words in other directions.
39	63	For example, returning to the example from Section 1, these terms are: e ‖We‖ ‖h̃‖ cos θWe,h̃ be logit Chan 5.25 19.5 0.144 −1.53 13.2 Fauci 4.69 19.5 0.154 −1.35 12.8 Jenner 5.23 19.5 0.120 −1.59 10.7 Observe that cos θWe,h̃ and even be both favor the correct output word Fauci, whereas ‖We‖ favors the more frequent, but incorrect, word Chan.
40	18	The most frequently-mentioned immunologist trumps other immunologists.
41	39	To solve this issue, we propose to fix the norm of all target word embeddings to some value r. Followingthe weight normalization approach of Salimans and Kingma (2016), we reparameterize We as r ve ‖ve‖ , but keep r fixed.
43	14	We compared both approaches on a development set and found that replacing h̃t in equation (1) with r h̃t ‖h̃t‖ indeed performs better, as shown in Table 1.
45	51	This could make the model prone to generate a target word that fits the context but doesn’t necessarily correspond to the source word(s).
49	46	We propose instead to use a simple feedforward neural network (FFNN) that is trained jointly with the rest of the NMT model to generate a target word based directly on the source word(s).
50	18	,m) be the embeddings of the source words.
52	78	Then we use a one-hidden-layer FFNN with skip connections (He et al., 2016): h`t = tanh(W f ` t ) + f ` t and combine its output with the decoder output to get the predictive distribution over output words at time step t: p(yt | y<t, x) = softmax(Woh̃t + bo + W`h`t + b`).
62	26	In addition, we compared against two other baseline systems: Moses: The Moses phrase-based translation system (Koehn et al., 2007), trained on the same data as the NMT systems, with the same maximum sentence length of 50.
69	20	Model For all NMT systems, we fed the source sentences to the encoder in reverse order during both training and testing, following Luong et al. (2015a).
94	20	Integrating the lexical module (fixnorm+lex) adds in further gains.
95	14	Our fixnorm+lex models surpass Moses on all tasks except Urdu- and Hausa-English, where it is 1.6 and 0.7 BLEU short respectively.
96	51	The method of Arthur et al. (2016) does improve over the baseline NMT on most language pairs, but not by as much and as consistently as our models, and often not as well as Moses.
97	23	Unfortunately, we could not replicate their approach for English-Japanese (KFTT) because the lexical table was too large to fit into the computational graph.
101	14	In the Uzbek example (top), untied and tied have confused 34 with UNK and 700, while in the Turkish one (middle), they incorrectly output other proper names, Afghan and Myanmar, for the proper name Kenya.
104	20	We can see that our fixnorm approach does not completely solve the mistranslation issue, since it translates Entoni Fauchi to UNK UNK (which is arguably better than James Chan).
105	17	On the other hand, fixnorm+lex gets this right.
106	64	To better understand how the lexical module helps in this case, we look at the top five translations for the word Fauci in fixnorm+lex: e cos θWe,h̃ cos θW le,hl be + b l e logit Fauci 0.522 0.762 −8.71 7.0 UNK 0.566 −0.009 −1.25 5.6 Anthony 0.263 0.644 −8.70 2.4 Ahmedova 0.555 0.173 −8.66 0.3 Chan 0.546 0.150 −8.73 −0.2 As we can see, while cos θWe,h̃ might still be confused between similar words, cos θW le,hl significantly favors Fauci.
111	87	As we can see in Figure 2, because of the alignment shift, both tied and fixnorm incorrectly replace the two unknown words (in bold) with But Deutsche instead of Deutsche Telekom.
112	15	In contrast, under fixnorm+lex and the model of Arthur et al. (2016), the alignment is corrected, causing the UNKs to be replaced with the correct source words.
117	51	As expected, the lexical distribution is sparse, with a few top translations accounting for the most probability mass.
118	22	Byte-Pair-Encoding (BPE) (Sennrich et al., 2016) is commonly used in NMT to break words into word-pieces, improving the translation of rare words.
141	17	In this paper, we have presented two simple yet effective changes to the output layer of a NMT model.
142	18	Both of these changes improve translation quality substantially on low-resource language pairs.
144	21	We conclude that NMT, equipped with the methods demonstrated here, is a more viable choice for low-resource translation than before, and are optimistic that NMT’s repertoire will continue to grow.
