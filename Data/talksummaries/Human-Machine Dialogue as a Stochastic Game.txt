20	35	This model extends the MDP paradigm to multi-player interactions and allows learning jointly the strategies of both agents (the user and the DM), which leads to the best system strategy in the face of the optimal user/adversary (in terms of his/her goal).
21	8	This paradigm models both co-adaptation and possible non-cooperativness.
22	44	Unlike models based on standard game theory (Caelen and Xuereb, 2011), Stochastic Games allow to learn from data.
25	8	This new paradigm is also very different from MARL methods proposed in previous work (Chandramohan et al., 2012b; Georgila et al., 2014; Efstathiou and Lemon, 2014) since optimization is jointly performed instead of alternatively optimizing each agent, considering the other can stay stationary for a while.
26	16	Although experiments are only concerned with purely adversarial tasks (Zero-Sum games), we show that it could be naturally extended to collaborative tasks (general sum games) (Prasad et al., 2015).
28	8	As said before, human-machine dialogue has been modeled as an (PO)MDP to make it suitable for automatic strategy learning (Levin and Pieraccini, 1997; Young et al., 2013).
29	18	In this framework, the dialogue is seen as a turn-taking process in which two agents (a user and a DM) interact through a noisy channel (ASR, NLU) to exchange information.
45	33	Thus, by taking at each state the action maximizing those values, one finds the optimal policy.
47	22	Stochastic Games (Filar and Vrieze, 1996; Neyman and Sorin, 2003), introduced in (Shapley, 1953), are a natural extension of MDPs to the Multi-Agent setting.
48	253	A discounted Stochastic Game (SG) is a tuple 〈D,S,A, T ,R, γ〉 where: D = {1, ..., n} represents the set of agents, S the discrete set of environment states, A = ×i∈DAi the joint action set, where for all i = 1, ..., n, Ai is the discrete set of actions available to the ith agent, T : S × A × S → [0, 1] the state transition probability function, R = ×i∈DRi the joint reward function, where for all i = 1, ..., n, Ri : S ×A→ R is the reward function of agent i.
66	10	It is therefore not possible to define in the general case a strategy optimal against every other strategy.
68	11	Agent i plays a Best Response σi against the other players’ joint strategy σ−i if σi is optimal given σ−i.
97	18	In such a setting, the aim of the agents is not to find a Nash Equilibrium (it is therefore not an SG algorithm) but to do as good as possible in this environment (and as a consequence, it may lead to a Nash Equilibrium).
118	18	At first sight, it seems reasonable to think that if two RL agents, previously trained to reach an optimal strategy, interact with each other, it would result in ”optimal” dialogues.
119	37	Each agent would be optimal given the environment it’s been trained on, but given another environment, nothing can be said about the learnt policy.
120	10	Furthermore, if two DMs are trained together with traditional RL techniques, no convergence is guaranteed since, as seen above, nonstationarities emerge.
122	47	Jointly optimizing RL-agents in the framework of Stochastic Games finds a Nash Equilibrium.
123	9	This guarantees both strategies to be optimal and this makes a fundamental difference with previous work (Chandramohan et al., 2012b; Georgila et al., 2014; Efstathiou and Lemon, 2014).
124	27	In the next section, we illustrate how dialogue may be modeled by a Stochastic Game, how transitions and reward functions depend on the policy of both agents.
125	27	We propose now a Zero-Sum dialogue game where agents have to drive efficiently the dialogue to gather information quicker than their opponent.
126	18	In this example, human user (Agent 1) and DM (Agent 2) are modeled with MDPs: each of them has a goal encoded into reward functions R1 and R2 (they may depend on the joint action).
127	176	The task involves two agents, each of them receives a random secret number and aims at guessing the other agent’s number.
128	52	They are adversaries: if one wins, the other one loses as much.
129	129	To find the secret number out, agents may perform one of the following actions: ask, answer, guess, ok, confirm and listen.
130	103	During a dialogue turn, the agent asking the question is called the guesser and the one answering is the opponent.
132	14	The opponent is forced to answer the truth.
133	58	To show that it has understood the answer, the agent says ok and releases then the turn to its adversary, which endorses the guesser’s role.
135	27	This simulates ASR and NLU errors arising in real SDSs.
138	7	- My number is greater than y.
139	17	When such an error arises, Agent 1 is allowed to ask another question instead of just saying ok.
