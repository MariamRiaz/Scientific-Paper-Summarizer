1	23	The argument mining task includes the tightly-knit subproblems of classifying propositions into elementary unit types and detecting argumentative relations between the elementary units.
2	72	The desired output is a document argumentation graph structure, such as the one in Figure 1, where propositions are denoted by letter subscripts, and the associated argumentation graph shows their types and support relations between them.
3	19	Most annotation and prediction efforts in argument mining have focused on tree or forest structures (Peldszus and Stede, 2015; Stab and Gurevych, 2016), constraining argument structures to form one or more trees.
8	19	By formulating argument mining as inference in a factor graph (Kschischang et al., 2001), our model (described in Section 4) can account for correlations between the two tasks, can consider second order link structures (e.g., in Figure 1, c → b → a), and can impose arbitrary constraints (e.g., transitivity).
12	18	Firstly, on our new Cornell eRulemaking Corpus – CDCP,2 consisting of argument annotations on comments from an eRulemaking discussion forum, where links don’t always form trees (Figure 1 shows an abridged example comment, and Section 3 describes the dataset in more detail).
16	15	For insight into the strengths and weaknesses of the proposed models, as well as into the differences between SVM and RNN parameterizations, we perform an error analysis in Section 5.1.
29	32	We release a new argument mining dataset consisting of user comments about rule proposals regarding Consumer Debt Collection Practices (CDCP) by the Consumer Financial Protection Bureau collected from an eRulemaking website, http:// regulationroom.org.
44	17	We reserve a test set of 150 documents (973 propositions, 272 links) from CDCP, and use the provided 80-document test split from UKP (1266 propositions, 809 links).
45	26	Binary and multi-class classification have been applied with some success to proposition and link prediction separately, but we seek a way to jointly learn the argument mining problem at the document level, to better model contextual dependencies and constraints.
49	15	This makes the argmax in Equation 1 impossible to evaluate by enumeration, so it is desirable to find models that decompose over smaller units and dependencies between them; for instance, as factor graphs.
55	18	Each proposition a and each link a → b has a corresponding random variable in the factor graph (the circles in Figure 2).
56	57	To encode the model’s belief in each possible value for these variables, we parametrize the unary factors (gray boxes in Figure 2) with unary potentials: φ(a) ∈ R|P| is a score of ya for each possible proposition type.
63	38	Local argumentation graph structures such as a → b → c might be modeled better together rather than through separate link factors for a → b and b → c. As in higher-order structured models for semantic and dependency parsing (Martins et al., 2013; Martins and Almeida, 2014), we implement three types of second order factors: grandparent (a → b → c), sibling (a ← b → c), and co-parent (a → b ← c).
80	17	While exact inference methods are generally unavailable, our setting is perfectly suited for the Alternating Directions Dual Decomposition (AD3) algorithm: approximate inference on expressive factor graphs with overlapping factors, logic constraints, and generic factors (e.g., directed tree factors) defined through maximization oracles (Martins et al., 2015).
84	17	We also find that after around 15 training iterations with fractional solutions, over 99% of inference calls are integral.
94	21	Link features are lexical (unigrams), syntactic (POS and productions), structural (token statistics, proposition statistics and location features), hand-crafted indicators, discourse triples, PMI, and shared noun counts.
97	21	Our higher-order factor features are: same sentence indicators (for all 3 and for each pair), proposition order (one for each of the 6 possible orderings), Jaccard similarity (between all 3 and between each pair), presence of any shared nouns (between all 3 and between each pair), and shared noun ratios: nouns shared by all 3 divided by total nouns in each proposition and each pair, and shared nouns between each pair with respect to each proposition.
101	38	Neural network methods have proven effective for natural language problems even with minimalto-no feature engineering.
103	13	We extract embeddings of all words with a corpus frequency > 1, initialized with GloVe word vectors.
122	24	Since the RNNweights are shared, this is a form of multi-task learning.
124	35	We evaluate our proposed models on both datasets.
141	22	While structured inference certainly helps baselines by preventing invalid structures such as cycles, it still depends on local decisions, losing to fully structured training in cases where joint proposition and link decisions are needed.
143	40	]b Otherwise, [ it would cause too much cost from taxpayers and the government.
144	43	]c Indeed, no reasons are provided, but baseline are misled by the connectives: the SVM baseline outputs that b and c are PREMISEs supporting the CLAIM a.
145	23	The full structured SVM combines the two tasks and correctly recognizes the link structure.
153	136	We introduce an argumentation parsing model based on AD3 relaxed inference in expressive factor graphs, experimenting with both linear struc- tured SVMs and structured RNNs, parametrized with higher-order factors and link structure constraints.
154	27	We demonstrate our model on a new argumentation mining dataset with more permissive argument structure annotation.
155	27	Our model also achieves state-of-the-art link prediction performance on the UKP essays dataset.
156	69	Stab and Gurevych (2016) found polynomial kernels useful for modeling feature interactions, but kernel structured SVMs scale poorly, we intend to investigate alternate ways to capture feature interactions.
157	100	While we focus on monological argumentation, our model could be extended to dialogs, for which argumentation theory thoroughly motivates non-tree structures (Afantenos and Asher, 2014).
