28	48	In Section 2 we will outline related work on KG embedding, multi-hop reasoning, and variational auto-encoder.
29	20	We describe our variational knowledge reasoner DIVA in Section 3.
30	22	Experimental results are presented in Section 4, and we conclude in Section 5.
51	51	Let E be the set of entities and R be the set of relations.
53	49	Next, in order to tackle this classification problem, we assume that there is a latent representation for given entity pair in the KG, i.e. the collection of linked paths, these hidden variables can reveal the underlying semantics between these two entities.
54	23	Therefore, the link classification problem can be decomposed into two modules – acquire underlying paths (Path Finder) and infer relation from la- tent representation (Path Reasoner).
55	17	Path Finder The state-of-the-art approach (Xiong et al., 2017; Das et al., 2018) is to view this process as a Markov Decision Process (MDP).
56	24	A tuple < S,A, P > is defined to represent the MDP, where S denotes the current state, e.g. the current node in the knowledge graph, A is the set of available actions, e.g. all the outgoing edges from the state, while P is the transition probability describing the state transition mechanism.
63	16	However, this evidence probability is intractable since it requires summing over the whole latent link space.
64	57	Therefore, we propose to maximize its variational lower bound as follows: ELBO = E L∼qϕ(L|r,(es,ed)) [log pθ(r|L)]− KL(qϕ(L|r, (es, ed))||pβ(L|(es, ed))) (2) Specifically, the ELBO (Kingma and Welling, 2013) is composed of three different terms – likelihood pθ ( r|L), prior pβ ( L|(es, et)), and posterior qϕ ( L|(es, ed), r).
71	15	Then we design three convolution layers with window size of (1× 2E), (2× 2E), (3× 2E), input channel size 1 and filter size D. After the convolution layer, we use (N × 1), (N − 1 × 1), (N − 2 × 1) to max pool the convolution feature map.
73	20	Finally, we use two-layered MLP with intermediate hidden size of M to output a softmax distribution over all the relations set R. F = f(f1, f2, · · · , fN ) p(r|L; θ) = softmax(WrF + br) (3) where f denotes the convolution and max-pooling function applied to extract reasoning path feature F , and Wr, br denote the weights and bias for the output feed-forward neural network.
74	12	Here we formulate the path finder p(L|(es, ed)) as an MDP problem, and recursively predict actions (an outgoing relationentity edge (a, e)) in every time step based on the previous history ht−1 as follows: ct = ReLU(Wh[ht; ed] + bh) p((at+1, et+1)|ht, β) = softmax(Atct) (4) where the ht ∈ RH denotes the history embedding, ed ∈ RE denotes the entity embedding, At ∈ R|A|×2E is outgoing matrix which stacks the concatenated embeddings of all outgoing edges and |A| denotes the number of outgoing edge, we use Wh and bh to represent the weight and bias of the feed-forward neural network outputting feature vector ct ∈ R2E .
75	12	The history embedding ht is obtained using an LSTM network (Hochreiter and Schmidhuber, 1997) to encode all the previous decisions as follows: ht = LSTM(ht−1, (at, et)) (5) As shown in Figure 3, the LSTM-based path finder interacts with the KG in every time step and decides which outgoing edge (at+1, et+1) to follow, search procedure will terminate either the target node is reached or the maximum step is reached.
77	19	The main difference lies in the fact that posterior approximator is aware of the relation r, therefore making more relevant decisions.
80	37	In order to maximize the ELBO with respect to the neural network models described above, we follow VAE (Kingma and Welling, 2013) to interpret the negative ELBO as two separate losses and minimize these them jointly using a gradient descent: Reconstruction Loss.
81	40	Here we name the first term of negative ELBO as reconstruction loss: JR = E L∼qϕ(L|r,(es,ed)) [− log pθ(r|L)] (7) this loss function is motivated to reconstruct the relationR from the latent variable L sampled from approximate posterior, optimizing this loss function jointly can not only help the approximate posterior to obtain paths unique to particular relation r, but also teaches the path reasoner to reason over multiple hops and predict the correct relation.
82	57	We name the second term as KL-divergence loss: JKL = KL(qϕ(L|r, (es, ed))|pβ(L|(es, ed))) (8) this loss function is motivated to push the prior distribution towards the posterior distribution.
84	53	During testtime when we have no knowledge about relation, we use path finder to replace posterior approximator to search for high-quality paths.
88	17	In practice, we found that the KL-reward term log pβqϕ causes severe instability during training, so we finally leave this term out by setting wKL as 0.
91	55	1: procedure TRAINING & TESTING 2: Train: 3: for episode← 1 to N do 4: Rollout K paths from posterior pϕ 5: if Train-Posterior then 6: ϕ← ϕ− η × ∂Lr∂ϕ 7: else if Train-Likelihood then 8: θ ← θ − η × ∂Lr∂θ 9: else if Train-Prior then 10: β ← β − η × ∂LKL∂β 11: end if 12: end for 13: Test MAP: 14: Restore initial parameters θ, β 15: Given sample (es, rq, (e1, e2, · · · , en)) 16: Li ← BeamSearch(pβ(L|es, ei)) 17: Si ← 1|Li| ∑ l∈Li pθ(rq|l) 18: Sort Si and find positive rank ra+ 19: MAP ← 1 1+ra+ 20: end procedure which indicates “no-relation” between two entities.
98	42	Unlike these two models assigning heuristic rewards to the policy, our model assigns model-based reward log pθ(r|L), which is known to be more sophisticated and considers more implicit factors to distinguish between good and bad paths.
101	36	By the active interactions and collaborations of two models, DIVA is able to comprehend more complex inference scenarios and handle more noisy environments.
102	22	To evaluate the performance of DIVA, we explore the standard link prediction task on two differentsized KG datasets and compare with the state-ofthe-art algorithms.
103	55	Link prediction is to rank a list of target entities (e−1 , e − 2 , · · · , e+n ) given a query entity eq and query relation rq.
105	34	We perform experiments on two datasets, and the details of the statistics are described in Table 1.
106	77	The samples of FB15k-237 (Toutanova et al., 2015) are sampled from FB15k (Bordes et al., 2013), here we follow DeepPath (Xiong et al., 2017) to select 20 relations including Sports, Locations, Film, etc.
109	13	For each query rq, we remove all the triples with rq and r−1q during reasoning.
111	26	During testing, we use a beam of 5 to approximate the whole search space for path finder.
112	19	We follow MINERVA (Das et al., 2018) to set the maximum reasoning length to 3, which lowers the burden for the path-reasoner model.
