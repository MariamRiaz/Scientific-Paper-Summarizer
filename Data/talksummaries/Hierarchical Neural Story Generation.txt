0	61	Story-telling is on the frontier of current text generation technology: stories must remain thematically consistent across the complete document, requiring modeling very long range dependencies; stories require creativity; and stories need a high level plot, necessitating planning ahead rather than word-by-word generation (Wiseman et al., 2017).
1	67	We tackle the challenges of story-telling with a hierarchical model, which first generates a sentence called the prompt describing the topic for the story, and then conditions on this prompt when generating the story.
4	75	We find that standard sequence-to-sequence (seq2seq) models (Sutskever et al., 2014) applied to hierarchical story generation are prone to degenerating into language models that pay little attention to the writing prompt (a problem that has been noted in other domains, such as dialogue response generation (Li et al., 2015a)).
6	41	To improve the relevance of the generated story to its prompt, we introduce a fusion mechanism (Sriram et al., 2017) where our model is trained on top of an pre-trained seq2seq model.
17	24	We collect a hierarchical story generation dataset1 from Reddit’s WRITINGPROMPTS forum.2 WRITINGPROMPTS is a community where online users inspire each other to write by submitting story premises, or prompts, and other users freely respond.
30	43	Recurrent and convolutional networks have successfully modeled sentences (Jozefowicz et al., 2016; Dauphin et al., 2017), but accurately modeling several paragraphs is an open problem.
36	28	The prompt gives a sketch of the structure of the story.
37	35	Second, we use a seq2seq model to generate a story that follows the premise.
42	36	In the Conv seq2seq model, the encoder and decoder are connected with attention modules (Bahdanau et al., 2015) that perform a weighted sum of encoder outputs, using attention at each layer of the decoder.
48	25	We show that gating lends the self-attention mechanism crucial capacity to make fine-grained selections.
49	38	Multi-Scale Attention: Further, we propose to have each head operating at a different time scale, depicted in Figure 2.
58	87	We find that seq2seq models ignore the prompt and focus solely on modeling the stories, because the local dependencies required for language modeling are easier to model than the subtle dependencies between prompt and story.
60	49	We train a seq2seq model that has access to the hidden states of a pretrained seq2seq model.
61	23	Doing so can be seen as a type of boosting or residual learning that allows the second model to focus on what the first model failed to learn—such as conditioning on the prompt.
63	89	The cold fusion mechanism of Sriram et al. (2017) pretrains a language model and subsequently trains a seq2seq model with a gating mechanism that learns to leverage the final hidden layer of the language model during seq2seq training.
104	65	Completely random sampling can introduce very unlikely words, which can damage generation as the model has not seen such mistakes at training time.
121	25	For human evaluation, we use Amazon Mechanical Turk to conduct a triple pairing task.
124	36	The stories and their corresponding prompts are shuffled, and human evaluators are asked to select the correct pairing for all three prompts.
139	31	In comparison, ensembling has no effect on people’s ability to associate stories with a prompt, but adding model fusion leads improves the pairing accuracy of the human judges by 7%.
143	41	Figure 5 shows that the fusion model can match the performance of nearest neighbour search in terms of the connection between the story and prompt.
148	34	In contrast, the baseline Conv seq2seq model copies 10.2 words on average and the KNN baseline copies all 150 words from a story in the training set.
154	85	For example, can’t is tokenized to ca n’t, and the model occasionally produces the first token but misses the second.
155	72	A similar error is after one line of dialogue, the model may move to another line of dialogue without generating a newline token.
158	52	In the generation of prompts using the GCNN language model, we find that prompts are fairly generic compared to human prompts.
161	39	In contrast, many of the human prompts are very unique (e.g. prompting stories in fantasy worlds such as Harry Potter and Game of Thrones) and the language model rarely produces the specific vocabulary required by these settings.
163	136	We further look at the usage of the self-attention layers within the decoder.
164	199	While they could be leveraged to look at words generated very far in the past, at many timesteps the selfattention focuses on the recent past.
165	70	We have collected the first dataset for creative text generation based on short writing prompts.
166	131	This new dataset pushes the boundaries of text generation by requiring longer range dependencies and conditioning on an abstract premise.
167	168	Building on this dataset, we show through automatic and human evaluation that novel hierarchical models, self-attention mechanisms and model fusion significantly improves the fluency, topicality, and overall quality of the generated stories.
