3	36	Despite strong theoretical foundations and plethora of normative theories, such as Walton’s schemes and their critical questions (Walton, 1989), an ideal model of critical discussion in the pragma-dialectic view (Van Eemeren and Grootendorst, 1987), or research into fallacies (Boudry et al., 2015), assessing qualitative criteria of everyday argumentation represents a challenge for argumentation scholars and practitioners (Weltzer-Ward et al., 2009; Swanson et al., 2015; Rosenfeld and Kraus, 2015).
4	36	Addressing qualitative aspects of arguments has recently started gaining attention in the field of computational argumentation.
8	42	First, can we assess what makes an argument convincing in a purely empirical fashion as opposite to theoretical normative approaches?
10	20	To address these questions, we exploit our recently introduced UKPConvArg1 corpus (Habernal and Gurevych, 2016).
17	19	physical education should be mandatory cuhz 112,000 people have died in the year 2011 so far and it’s because of the lack of physical activity and people are becoming obese!!!!
22	61	Our main contributions are: (1) We propose empirically inspired labels of quality properties of Web arguments and design a hierarchical annotation scheme.
41	37	Arguments from each of the 32 debate sides are connected into a set of argument pairs, and each argument pair is annotated with a binary relation (argument A is more/less convincing than argument B), resulting in total into 11,650 argument pairs.
42	47	Annotations performed by Habernal and Gurevych (2016) also contain several reasons written by crowd-workers that explain why a particular argument is more or less convincing; see an example in Figure 1.
43	36	As these reasons were written in an uncontrolled setting, they naturally reflect the main properties of argument quality in a downstream task, which is to decide which argument from a pair is more convincing.
52	50	Having a set of ≈ 25 distinct labels, we ran two larger studies on Amazon Mechanical Turk (AMT), each with 500 reason units and 10 workers.
55	125	As the number of classes is too big for non-expert crowd workers, we developed a hierarchical annotation process guided by questions that narrow down the final class decision.
56	39	The scheme is depicted in Figure 2.4 Workers were shown only the reason units without seeing the original arguments.
60	23	We employed MACE (Hovy et al., 2013) for gold label and worker competence estimation with 95% threshold to ignore the less confident labels.
75	18	After all cleaning procedures, annotations from reason units were mapped back to argument pairs, resulting into a multi-label annotation of one or both arguments from the given pair.
79	26	Table 1 shows number of labels per argument pairs; about a half of the argument pairs have only one label.
81	80	This is not surprising, as this label was used for reason units pointing out that the more convincing argument provided more reasons, details, information or better reasoning – a feature inherent to argumentation seen as giving reasons (Freeley and Steinberg, 2008).
83	19	We designed a validation study, in which workers were shown the original argument pair and two sets of labels.
86	24	For example, for the argument pair from Figure 1, one set of shown labels would be {C8-1, C9-3, C5-2, C6-1} (the correct set) while the other ‘distracting’ set would be {C8-1, C9-3, C5-1, C7-3} .
111	23	Here we can see that the error rate of the most confident predicted label is about 30%, while human performed similarly by choosing from a two different label sets in a binary settings, so their task was inherently harder.
114	31	We suspect two causes, first, the highly skewed distribution of labels (see Figure 3) and, second, insufficient training data sizes where 13 classes have less than 1k training examples (while Goodfellow et al. (2016) recommend at least 5k instances per class).
125	22	We also add another baseline model that employs SVM with RBF kernel5 and a rich set of linguistically motivated features, similarly to (Habernal and Gurevych, 2016).
126	27	The feature set includes uni- and bi-gram presence, ratio of adjective and adverb endings that may signalize neuroticism (Corney et al., 2002), contextuality measure (Heylighen and Dewaele, 2002), dependency tree depth, ratio of exclamation or quotation marks, ratio of modal verbs, counts of several named entity types, ratio of past vs. future tense verbs, POS n-grams, presence of dependency tree production rules, seven different readability measures (e.g., Ari (Senter and Smith, 1967), Coleman-Liau (Coleman and Liau, 1975), Flesch (Flesch, 1948), and others), five sentiment scores (from very negative to very positive) (Socher et al., 2013), spell-checking using standard Unix words, ratio of superlatives, and some surface features such as sentence lengths, longer words count, etc.6 It results into a sparse 60k-dimensional feature vector space.
128	29	Both deep learning models significantly outperform the baseline, yielding Macro-F1 score about 0.35.
134	121	While the problem of not catching off-topic arguments can be probably modeled by incorporating the debate description or some sort of debate topic model into the attention vector, the more common issue of non-sense arguments or fallacious arguments (which seem like actual arguments on the first view) needs much deeper understanding of realworld knowledge, logic, and reasoning.
135	27	This paper presented a novel task in the field of computational argumentation, namely empirical assessment of reasons for argument convincingness.
136	33	We created a new large benchmark data set by utilizing a new annotation scheme and several filtering strategies for crowdsourced data.
137	66	Then we tackled two challenging tasks, namely multi-label classification of argument pairs in order to reveal qualitative properties of the arguments, and predicting flaws in the less convincing argument from the given argument pair.
138	45	We performed all evaluations in a cross-domain scenario and experimented with feature-rich SVM and two state-of-the-art neural network models.
139	29	The results are promising but show that the task is inherently complex as it requires deep reasoning about the presented arguments that goes beyond capabilities of the current computational models.
140	230	By releasing the UKPConvArg2 data and code to the community, we believe more progress can be made in this direction in the near future.
