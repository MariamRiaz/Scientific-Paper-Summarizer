2	45	While both simple and surprisingly accurate, NMT systems typically need to have very high capacity in order to perform well: Sutskever et al. (2014) used a 4-layer LSTM with 1000 hidden units per layer (herein 4×1000) and Zhou et al. (2016) obtained state-of-the-art results on English → French with a 16-layer LSTM with 512 units per layer.
11	66	In this work, we investigate knowledge distillation in the context of neural machine translation.
13	27	For NMT, while the model is trained on multi-class prediction at the word-level, it is tasked with predicting complete sequence outputs conditioned on previous decisions.
27	26	We generally assume that the teacher has previously been trained, and that we are estimating parameters for the student.
34	78	Instead of minimizing cross-entropy with the observed data, we instead minimize the cross-entropy with the teacher’s probability distribution, LKD(θ; θT ) =− |V|∑ k=1 q(y = k |x; θT )× log p(y = k |x; θ) where θT parameterizes the teacher distribution and remains fixed.
37	26	Since this new objective has no direct term for the training data, it is common practice to interpolate between the two losses, L(θ; θT ) = (1− α)LNLL(θ) + αLKD(θ; θT ) where α is mixture parameter combining the one-hot distribution and the teacher distribution.
45	28	Word-level knowledge distillation allows transfer of these local word distributions.
49	24	The sequence-level negative loglikelihood for NMT then involves matching the onehot distribution over all complete sequences, LSEQ-NLL = − ∑ t∈T 1{t = y} log p(t | s) = − J∑ j=1 |V|∑ k=1 1{yj = k} log p(tj = k | s, t<j) = LWORD-NLL where y = [y1, .
53	37	However, instead of using a single word prediction, we use q(t | s) to represent the teacher’s sequence distribution over the sample space of all possible sequences, LSEQ-KD = − ∑ t∈T q(t | s) log p(t | s) Note that LSEQ-KD is inherently different from LWORD-KD, as the sum is over an exponential number of terms.
59	30	Using the mode seems like a poor approximation for the teacher distribution q(t | s), as we are approximating an exponentially-sized distribution with a single sample.
68	42	Next we consider integrating the training data back into the process, such that we train the student model as a mixture of our sequence-level teachergenerated data (LSEQ-KD) with the original training data (LSEQ-NLL), L = (1− α)LSEQ-NLL + αLSEQ-KD = −(1− α) log p(y | s)− α ∑ t∈T q(t | s) log p(t | s) where y is the gold target sequence.
74	63	Local updating suggests selecting a training sequence which is close to y and has high probability under the teacher model, ỹ = argmax t∈T sim(t,y)q(t | s) where sim is a function measuring closeness (e.g. Jaccard similarity or BLEU (Papineni et al., 2002)).
75	27	Following local updating, we can approximate this sequence by running beam search and choosing ỹ ≈ argmax t∈TK sim(t,y) where TK is the K-best list from beam search.
90	60	Sequence-Level Knowledge Distillation (Seq-KD) Student is trained on the teacher-generated data, which is the result of running beam search and taking the highest-scoring sequence with the teacher model.
92	40	Sequence-Level Interpolation (Seq-Inter) Student is trained on the sequence on the teacher’s beam that had the highest BLEU (beam size K = 35).
100	39	Sequence-level interpolation (Seq-Inter), in addition to improving models trained via Word-KD and Seq-KD, also improves upon the original teacher model that was trained on the actual data but finetuned towards Seq-Inter data (Baseline + Seq-Inter).
102	34	We hypothesize that sequence-level knowledge distillation is effective because it allows the student network to only model relevant parts of the teacher distribution (i.e. around the teacher’s mode) instead of ‘wasting’ parameters on trying to model the entire space of translations.
105	48	This also explains the success of greedy decoding for Seq-KD models—since we are only modeling around the teacher’s mode, the student’s distribution is more peaked and therefore the argmax is much easier to find.
112	35	We use a GeForce GTX Titan X for GPU and a Samsung Galaxy 6 smartphone.
113	134	We find that we can run the student model 10 times faster with greedy decoding than the teacher model with beam search on GPU (1051.3 vs 101.9 words/sec), with similar performance.
114	35	Although knowledge distillation enables training faster models, the number of parameters for the student models is still somewhat large (Table 1: Params), due to the word embeddings which dominate most of the parameters.10 For example, on the 2 × 500 English → German model the word embeddings account for approximately 63% (50m out of 84m) of the parameters.
115	40	The size of word embeddings have little impact on run-time as the word embedding layer is a simple lookup table that only affects the first layer of the model.
117	54	Weight pruning for NMT was recently investigated by See et al. (2016), who found that up to 80 − 90% of the parameters in a large NMT model can be pruned with little loss in performance.
118	52	We take our best English→ German student model (2× 500 Seq-KD + Seq-Inter) and prune x% of the parameters by removing the weights with the lowest absolute values.
119	63	We then retrain the pruned model on Seq-KD data with a learning rate of 0.2 and fine-tune towards Seq-Inter data with a learning rate of 0.1.
120	200	As observed by See et al. (2016), retraining proved to be crucial.
122	48	Our findings suggest that compression benefits achieved through weight pruning and knowledge distillation are orthogonal.11 Pruning 80% of the weight in the 2 × 500 student model results in a model with 13× fewer parameters than the original teacher model with only a decrease of 0.4 BLEU.
123	46	While pruning 90% of the weights results in a more appreciable decrease of 1.0 BLEU, the model is drastically smaller with 8m parameters, which is 26× fewer than the original teacher model.
124	30	• For models trained with word-level knowledge distillation, we also tried regressing the student network’s top-most hidden layer at each time step to the teacher network’s top-most hidden layer as a pretraining step, noting that Romero et al. (2015) obtained improvements with a similar technique on feed-forward models.
142	46	We anticipate that methods described in this paper can be used to similarly train smaller models in other domains.
