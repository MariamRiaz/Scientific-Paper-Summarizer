{"sections": [{"text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 644\u2013649 Melbourne, Australia, July 15 - 20, 2018. c\u00a92018 Association for Computational Linguistics\n644"}, {"heading": "1 Introduction", "text": "Automatically extracting common sense from text is a long-standing challenge in natural language processing (Schubert, 2002; Van Durme and Schubert, 2008; Vanderwende, 2005). As argued by Forbes and Yejin (2017), typical language use may reflect common sense, but the commonsense knowledge itself is not often explicitly stated, due to reporting bias (Gordon and Van Durme, 2013). Thus, additional human knowledge or annotated training data are often used to help systems learn common sense.\nIn this paper, we study methods for reducing the amount of human input needed to learn common sense. Specifically, we focus on learning relative comparisons of (one-dimensional) object properties, such as the fact that a cantaloupe is more round than a hammer. Methods for learning this kind of common sense have been developed previously (e.g. Forbes and Choi, 2017), but the best-performing methods in that previous work requires dozens of manually-annotated frames for each comparison property, to connect the property to how it is indirectly reflected in text\u2014e.g., if\ntext asserts that \u201cx carries y,\u201d this implies that x is probably larger than y.\nOur architecture for relative comparisons follows the zero-shot learning paradigm (Palatucci et al., 2009). It takes the form of a neural network that compares a projection of embeddings for each of two objects (e.g. \u201celephant\u201d and \u201ctiger\u201d) to the embeddings for the two poles of the target dimension of comparison (e.g., \u201cbig\u201d and \u201dsmall\u201d for the size property). The projected object embeddings are trained to be closer to the appropriate pole, using a small training set of hand-labeled comparisons. Our experiments reveal that our architecture outperforms previous work, despite using less annotated data. Further, because our architecture takes the property (pole) labels as arguments, it can extend to the zero-shot setting in which we evaluate on properties not seen in training. We find that in zero-shot, our approach outperforms baselines and comes close to supervised results, but providing labels for both poles of the relation rather than just one is important. Finally, because the number of properties we wish to learn is large, we experiment with active learning (AL) over a larger property space. We show that synthesizing AL queries can be effective using an approach that explicitly models which comparison questions are nonsensical (e.g., is Batman taller than Democracy?). We release our code base and a new commonsense data set to the research community.1"}, {"heading": "2 Problem Definition and Methods", "text": "We define the task of comparing object properties in two different ways: a three-way classification task, and a four-way classification task. In the three-way classification task, we want to estimate the following conditional probability:\nP (L|O1,O2,Property),L \u2208 { < , > , \u2248 }. 1https://github.com/yangyiben/PCE\nFor example, Prob(An elephant is larger than a dog) can be expressed as P (L = > |O1 = \u201delephant\u201d,O2 = \u201ddog\u201d,Property = \u201dsize\u201d). The three-way classification task has been explored in previous work (Forbes and Choi, 2017) and is only performed on triples where both objects have the property, so that the comparison is meaningful. In applications, however, we may not know in advance which comparisons are meaningful. Thus, we also define a four-way classification task to include \u201dnot applicable\u201d as the fourth label, so that inference can be performed on any objectproperty triples. In the four-way task, the system is tasked with identifying the nonsensical comparisons. Formally, we want to estimate the following conditional probability:"}, {"heading": "P (L|O1,O2,Property),L\u2208{ < , > , \u2248 , N/A }.", "text": ""}, {"heading": "2.1 Three-way Model", "text": "For each comparison property, we pick an adjective and its antonym to represent the { < , > } labels. For example, for the property size, we pick \u201dbig\u201d and \u201dsmall\u201d. The adjective \u201dsimilar\u201d serves as the label for \u2248 for all properties. Under this framework, a relative comparison question, for instance, \u201dIs a dog bigger than an elephant?\u201d, can be formulated as a quintuple query to the model, namely {dog, elephant, small, similar, big}. Denoting the word embeddings for tokens in a quintuple query as X , Y , R<, R\u2248, R>, our three-way model is defined as follows:\nP (L = s|Q) = softmax(Rs \u00b7 \u03c3((X \u2295 Y )W )),\nfor s \u2208 {<, >, \u2248}, where Q is an quintuple query, \u03c3(\u00b7) is an activation function and W is a learnable weight matrix. The symbol \u2295 represents concatenation. We refer to this method as PCE (Property Comparison from Embeddings) for the 3-way task. We also experiment with generating label representations from just a single adjective (property) embedding R<, namely R\u2248 = \u03c3(R<W2), R> = \u03c3(R<W3). We refer to this simpler method as PCE(one-pole).\nWe note that in both the three- and four-way settings, the question \u201dA>B?\u201d is equivalent to \u201dB<A?\u201d. We leverage this fact at test time by feeding our network a reversed object pair, and taking the average of the aligned network outputs before the softmax layer to reduce prediction variance. We refer to our model without this technique as PCE(no reverse).\nThe key distinction of our method is that it learns a projection from the object word embedding space to the label embedding space. This allows the model to leverage the property label embeddings to perform zero-shot prediction on properties not observed in training. For example, from a training example \u201ddogs are smaller than elephants\u201d, the model will learn a projection that puts \u201ddogs\u201d relatively closer to \u201dsmall,\u201d and far from \u201dbig\u201d and \u201dsimilar.\u201d Doing so may also result in projecting \u201ddog\u201d to be closer to \u201dlight\u201d than to \u201dheavy,\u201d such that the model is able to predict \u201ddogs are lighter than elephants\u201d despite never being trained on any weight comparison examples."}, {"heading": "2.2 Four-way Model", "text": "Our four-way model is the same as our three-way model, with an additional module to learn whether the comparison is applicable. Keeping the other output nodes unchanged, we add an additional component into the softmax layer to output the probability of \u201dN/A\u201d:\nhx = \u03c3(XWa), hy = \u03c3(YWa), Ai = hi \u00b7R> + hi \u00b7R<, P (L = N/A |Q) \u221d exp(Ax +Ay)."}, {"heading": "2.3 Synthesis for Active Learning", "text": "We propose a method to synthesize informative queries to pose to annotators, a form of active learning (Settles, 2009). We use the common heuristic that an informative training example will have a high uncertainty in the model\u2019s predictive distribution. We adopt the confidence measure (Culotta and McCallum, 2005) to access the uncertainty of a given example:\nUncertainty(x) = 1\u2212max y P (y|x,Dtrain).\nGood candidates for acquisition should have high uncertainty measure, but we also want to avoid querying outliers. As the vocabulary is finite, it is possible to evaluate the uncertainty measures for all possible inputs to synthesize the most uncertain query. However, such a greedy policy is expensive and prone to selecting outliers. Hence, we adopt a sampling based synthesis strategy: at each round, we generate one random object pair per property, and query the one that achieves the highest uncertainty measure.\nA classical difficulty faced by synthesis approaches to active learning is that they may pro-\nduce unnatural queries that are difficult for a human to label (Baum and Lang, 1992). However, our task formulation includes \u201dsimilar\u201d and \u201dN/A\u201d classes that encompass many of the more difficult or confusing comparisons, which we believe aids the effectiveness of the synthesis approach."}, {"heading": "3 Experiments", "text": "We now present our experimental results on both the three-way and four-way tasks."}, {"heading": "3.1 Data Sets", "text": "We test our three-way model on the VERB PHYSICS data set from (Forbes and Choi, 2017). As there are only 5 properties in VERB PHYSICS, we also develop a new data set we call PROPERTY COMMON SENSE. We select 32 commonsense properties to form our property set (e.g., value, roundness, deliciousness, intelligence, etc.). We extract object nouns from the McRae Feature Norms dataset (McRae et al., 2005) and add selected named entities to form a object vocabulary of 689 distinct objects. We randomly generate 3148 object-property triples, label them and reserve 45% of the data for the test set. We further add 5 manually-selected applicable comparison examples per property to our test set, in order to make sure each property has some applicable testing examples. To verify the labeling, we have a second annotator redundantly label 200 examples and find a Cohen\u2019s Kappa of 0.64, which indicates good annotator agreement (we analyze the source of the disagreements in Section 4.1). The training set is used for the passive learning and pool-based active learning, and a human oracle provides labels in the synthesis active learning setting."}, {"heading": "3.2 Experimental Setup", "text": "We experiment with three types of embeddings: GloVe, normalized 300-dimensional embeddings trained on a corpus of 6B tokens (Pennington et al., 2014) (the F&C method (Forbes and Choi, 2017) uses the 100-dimensional version, as it achieves the highest validation accuracy for their methods); Word2vec, normalized 300- dimensional embeddings trained on 100B tokens (Mikolov et al., 2013); and LSTM, the normalized 1024-dimensional weight matrix from the softmax layer of the Google 1B LSTM language model (Jozefowicz et al., 2016).\nFor training PCE, we use an identity activation function and apply 50% dropout. We use the Adam optimizer with default settings to train the models for 800 epochs, minimizing cross entropy loss. For zero-shot learning, we adopt a hold-oneproperty-out scheme to test our models\u2019 zero-shot performance.\nFinally, for active learning, we use Word2vec embeddings. All the models are trained on 200 random training examples to warm up. We train for 20 epochs after each label acquisition. To smooth noise, we report the average of 20 different runs of random (passive learning) and least confident (LC) pool-based active learning (Culotta and McCallum, 2005) baselines. We report the average of only 6 runs for an expected model change (EMC) pool-based active learning (Cai et al., 2013) baseline due to its high computational cost, and of only 2 runs for our synthesis active learning approach due to its high labeling cost. The pool size is 1540 examples."}, {"heading": "3.3 Results", "text": "In Table 1, we compare the performance of the three-way PCE model against the existing state of the art on the VERB PHYSICS data set. The use of LSTM embeddings in PCE yields the best accuracy for all properties. Across all embedding choices, PCE performs as well or better than F&C, despite the fact that PCE does not use the annotated frames that F&C requires (approximately 188 labels per property). Thus, our approach matches or exceeds the performance of previous work using significantly less annotated knowledge. The lower performance of \u201dno reverse\u201d shows that the simple method of averaging over the reversed object pair is effective.\nTable 2 evaluates our models on properties not seen in training (zero-shot learning). We compare against a random baseline, and an Emb-Similarity baseline that classifies based on the cosine similarity of the object embeddings to the pole label embeddings (i.e., without the projection layer in PCE). PCE outperforms the baselines. Although the one-pole method was shown to perform similarly to the two-pole method for properties seen in training (Table 1), we see that for zero-shot learning, using two poles is important.\nIn Table 3, we show that our four-way models with different embeddings beat both the majority and random baselines on the PROPERTY\nCOMMON SENSE data. Here, the LSTM embeddings perform similarly to the Word2vec embeddings, perhaps because the PROPERTY COMMON SENSE vocabulary consists of less frequent nouns than in VERB PHYSICS. Thus, the Word2vec embeddings are able to catch up due to their larger vocabulary and much larger training corpus.\nFinally, in Figure 1, we evaluate in the active learning setting. The synthesis approach performs best, especially later in training when the training pool for the pool-based methods has only uninformative examples remaining. Figure 2 helps explain the relative advantage of the synthesis approach: it is able to continue synthesizing informative (uncertain) queries throughout the entire training run."}, {"heading": "4 Discussion", "text": ""}, {"heading": "4.1 Sources of annotator disagreement", "text": "As noted above, we found a \u201cgood\u201d level of agreement (Cohen\u2019s Kappa of 0.64) for our PROPERTY COMMON SENSE data, which is lower than one might expect for task aimed at common sense. We\nanalyzed the disagreements and found that they stem from two sources of subjectivity in the task. The first is that different labelers may have different thresholds for what counts as similar\u2014a spider and an ant might be marked similar in size for one labeler, but not for another labeler. In our data, 58% of the disagreements are cases in which one annotator marks similar while the other says not similar. The second is that different labelers have different standards for whether a comparison is N/A. For example, in our data set, one labeler labels that a toaster is physically stronger than alcohol, and the other labeler says the comparison is N/A. 37% of our disagreements are due to this type of subjectivity. The above two types of subjectivity account for almost all disagreements\n(95%), and the remaining 5% are due to annotation errors (one of the annotators makes mistake)."}, {"heading": "4.2 Model Interpretation", "text": "Since we adopt an identity activation function and a single layer design, it is possible to simplify the mathematical expression of our model to make it more interpretable. After accounting for model averaging, we have the following equality:\nP (L =< |Q) \u221d exp(R< \u00b7 ((X \u2295 Y )W ) +R> \u00b7 ((Y \u2295X)W ))\n= exp(RT<(XW1 + YW2) +R T >(YW1 +XW2)) \u221d exp((R< \u2212R>)T (XW1 +XW2)),\nwhere W = W1 \u2295W2. So we can define a score of \u201dR<\u201d for a object with embedding X as the following:\nscore(X,R<) = (R< \u2212R>)T (XW1 +XW2).\nAn object with a higher score for R< is more associated with the R< pole than the R> one. For example, score(\u201delephant\u201d,\u201dsmall\u201d) represents how small an elephant is\u2014a larger score indicates a smaller object. Table 4 shows smallness scores for 5 randomly picked objects from the VERB PHYSICS data set. PCE tends to assign higher scores to the smaller objects in the set."}, {"heading": "4.3 Sensitivity to pole labels", "text": "PCE requires labels for the poles of the target object property. Table 5 presents a limited sensitivity\nanalysis to pole labels, evaluating the test accuracy of PCE as the pole label varies among different combinations of synonyms for the size and speed relations. We evaluate in both the trained setting (comparable to the results in Table 1) and the zero-shot setting (comparable to Table 2). We see that the trained accuracy remains essentially unchanged for different pole labels. In the zeroshot setting, all combinations achieve accuracy that beats the baselines in Table 2, but the accuracy value is somewhat sensitive to the choice of pole label. Exploring how to select pole labels and experimenting with richer pole representations such as textual definitions are items of future work."}, {"heading": "5 Conclusion", "text": "In this paper, we presented a method for extracting commonsense knowledge from embeddings. Our experiments demonstrate that the approach is effective at performing relative comparisons of object properties using less hand-annotated knowledge than in previous work. A synthesis active learner was found to boost accuracy, and further experiments with this approach are an item of future work."}, {"heading": "Acknowledgments", "text": "This work was supported in part by NSF Grant IIS-1351029. We thank the anonymous reviewers for helpful comments."}], "year": 2018, "references": [{"title": "Query learning can work poorly when a human oracle is used", "authors": ["Eric B Baum", "Kenneth Lang."], "venue": "International joint conference on neural networks, volume 8, page 8.", "year": 1992}, {"title": "Maximizing expected model change for active learning in regression", "authors": ["W. Cai", "Y. Zhang", "J. Zhou."], "venue": "2013 IEEE 13th International Conference on Data Mining, pages 51\u201360.", "year": 2013}, {"title": "Reducing labeling effort for structured prediction tasks", "authors": ["Aron Culotta", "Andrew McCallum."], "venue": "AAAI, pages 746\u2013751. AAAI Press / The MIT Press.", "year": 2005}, {"title": "Verb physics: Relative physical knowledge of actions and objects", "authors": ["Maxwell Forbes", "Yejin Choi."], "venue": "arXiv preprint arXiv:1706.03799.", "year": 2017}, {"title": "Reporting bias and knowledge acquisition", "authors": ["Jonathan Gordon", "Benjamin Van Durme."], "venue": "Proceedings of the 2013 workshop on Automated knowledge base construction, pages 25\u201330. ACM.", "year": 2013}, {"title": "Exploring the limits of language modeling", "authors": ["Rafal Jozefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu."], "venue": "arXiv preprint arXiv:1602.02410.", "year": 2016}, {"title": "Semantic feature production norms for a large set of living and nonliving things", "authors": ["Ken McRae", "George S. Cree", "Mark S. Seidenberg", "Chris Mcnorgan."], "venue": "Behavior Research Methods, 37(4):547\u2013559.", "year": 2005}, {"title": "Distributed representations of words and phrases and their compositionality", "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Ad-", "year": 2013}, {"title": "Zero-shot learning with semantic output codes", "authors": ["Mark Palatucci", "Dean Pomerleau", "Geoffrey E Hinton", "Tom M Mitchell."], "venue": "Y. Bengio, D. Schuurmans, J. D. Lafferty, C. K. I. Williams, and A. Culotta, editors, Advances in Neural Information Pro-", "year": 2009}, {"title": "Glove: Global vectors for word representation", "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u2013 1543.", "year": 2014}, {"title": "Can we derive general world knowledge from texts? In Proceedings of the second international conference on Human Language Technology Research, pages 94\u201397", "authors": ["Lenhart Schubert."], "venue": "Morgan Kaufmann Publishers Inc.", "year": 2002}, {"title": "Active learning literature survey", "authors": ["B. Settles."], "venue": "Computer Sciences Technical Report 1648, University of Wisconsin\u2013Madison.", "year": 2009}, {"title": "Open knowledge extraction through compositional language processing", "authors": ["Benjamin Van Durme", "Lenhart Schubert."], "venue": "Proceedings of the 2008 Conference on Semantics in Text Processing, pages 239\u2013254. Association for Computational Linguis-", "year": 2008}, {"title": "Volunteers created the web", "authors": ["Lucy Vanderwende."], "venue": "AAAI Spring Symposium: Knowledge Collection from Volunteer Contributors, pages 84\u201390.", "year": 2005}], "id": "SP:9ef26e9280a11d3012d01c3e219504cfb40f9dbd", "authors": [{"name": "Yiben Yang", "affiliations": []}, {"name": "Larry Birnbaum", "affiliations": []}, {"name": "Ji-Ping Wang", "affiliations": []}, {"name": "Doug Downey", "affiliations": []}], "abstractText": "Intelligent systems require common sense, but automatically extracting this knowledge from text can be difficult. We propose and assess methods for extracting one type of commonsense knowledge, object-property comparisons, from pretrained embeddings. In experiments, we show that our approach exceeds the accuracy of previous work but requires substantially less hand-annotated knowledge. Further, we show that an active learning approach that synthesizes common-sense queries can boost accuracy.", "title": "Extracting Commonsense Properties from Embeddings with Limited Human Guidance"}