31	1	F(x) is the Fourier transform that maps x from the spatial domain to the frequency domain, while F 1(x) is the inverse operator which maps F(x) back to x.
39	1	Let x̃i ⌘ F(xi), D̃(:, k) ⌘ F(D(:, k)), and Z̃i(:, k) ⌘ F(Zi(:, k)) be the Fourier-transformed counterparts of xi, D(:, k) and Zi(:, k).
60	1	Moreover, Ht and Gt can be updated incrementally.
70	1	We propose to represent xi as: xi = KX k=1 RX r=1 Wi(r, k)B(:, r) !
86	2	All the proofs are in the Appendix.
94	2	, N. Let B̃(:, r) ⌘ F(B(:, r)), where B(:, r) 2 RM is zeropadded to be P -dimensional.
111	1	the two blocks can be performed separately as: (proxIW(·)(W ), prox k·k1(Z)) (Parikh & Boyd, 2014).
113	2	The whole procedure, which will be called “Sampledependent Convolutional Sparse Coding (SCSC)”, is shown in Algorithm 1.
115	1	Its per-iteration time complexity is O(RKP + RP logP ), where the O(RKP ) term is due to gradient computation, and O(RP logP ) is due to FFT/inverse FFT.
116	1	Table 1 compares its complexities with those of the other online and distributed CSC algorithms.
117	1	As can be seen, SCSC has much lower time and space complexities as R⌧ K.
120	1	We use the default training and testing splits provided in (Bristow et al., 2013).
124	1	Following (Heide et al., 2015; Choudhury et al., 2017; Papyan et al., 2017; Wang et al., 2018), we set the filter size M as 11⇥11, and the regularization parameter Algorithm 1 Sample-dependent CSC (SCSC).
125	1	, T do 3: draw xt from {xi}; 4: x̃t = F(xt); 5: obtain Wt, Zt using niAPG; 6: for r = 1, 2, .
131	1	The experiment is repeated five times with different dictionary initializations.
134	1	Experiments are performed on Fruit and City.
158	1	In this Section, we demonstrate that this can lead to better performance.
159	1	We compare SCSC with two most recent batch and online CSC methods, namely, slice-based CSC (SBCSC) (Papyan et al., 2017) and OCSC.
162	1	As can be seen, a larger K consistently leads to better performance for all methods.
166	1	We set R = 10 (i.e., CR = 100) for SCSC.
169	1	Figure 6 shows convergence of the testing PSNR with clock time.
172	1	Next, we perform experiments on the two large data sets, CIFAR-10 and Flower.
176	1	On Flower, K is still 300 for SCSC.
179	1	In both cases, SCSC significantly outperforms OCSC.
220	3	As can be seen, the violation does not go to zero, which indicates that ADMM does not converge.
221	4	In this paper, we proposed a novel CSC extension, in which each sample has its own sample-dependent dictionary constructed from a small set of shared base filters.
222	13	Using online learning, the model can be efficiently updated with low time and space complexities.
223	34	Extensive experiments on a variety of data sets including large image data sets and Figure 8.
224	144	Convergence of niAPG and ADMM on solving (16).
225	142	higher-dimensional data sets all demonstrate its efficiency and scalability.
