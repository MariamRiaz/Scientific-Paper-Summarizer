{"sections": [{"heading": "1. Introduction", "text": "Tensors have long been successfully used in several disciplines, including neuroscience, phylogenetics, statistics, signal processing, computer vision, and data mining. They are used to model multi-relational or multi-modal data, and their decompositions often reveal some underlying structures behind the observed data. See (Kolda & Bader, 2009) for a survey of such results. Recently, they have found applications in machine learning, particularly for learning various latent variable models (Anandkumar et al., 2012; Chaganty & Liang, 2014; Anandkumar et al., 2014a).\nOne popular decomposition method in such applications is the CP (Candecomp/Parafac) decomposition, which decomposes the given tensor as a sum of rank-one components. This is similar to the singular value decomposition (SVD) of matrices, and a popular approach for SVD is the power method, which is well-understood and has nice theoretical guarantee. As tensors can be seen as generalization of matrices to higher orders, one would hope that a natural generalization of the power method to tensors could inherit the success from the matrix case. However, the situation turns out to be much more complicated for tensors (see e.g. the discussion in (Anandkumar et al., 2014a)),\n1Academia Sinica, Taiwan. Correspondence to: Po-An Wang <poanwang@iis.sinica.edu.tw>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nand in fact several problems related to tensor decomposition are known to be NP-hard (Hillar & Lim, 2013). Nevertheless, when the given tensor has some additional structure, the tensor decomposition problem becomes tractable again. In particular, for tensors having orthogonal decomposition, Anandkumar et al. (2014a) provided an efficient algorithm based on the tensor power method with theoretical guarantee. Still, as we will discuss later in Section 2, the seemingly subtle change of going from matrices to tensors makes some significant differences for the power method.\nThe first is that while the matrix power method can guarantee that a randomly selected initial vector will almost surely converge to the top singular vector, we have much less control of where the convergence goes in the tensor case. Consequently, most previous works based on the tensor power method with theoretical guarantee, such as (Anandkumar et al., 2014a;b; Wang & Anandkumar, 2016), require much more complicated procedures. In particular, they can only find the top k eigenvectors one by one, each time with the power method applied to a modified tensor, deflated from the original tensor according to the previously found vectors. Moreover, to find each vector, they need to sample several initial vectors and apply the power method on all of them, before selecting just one from them. In contrast, algorithms for matrices such as (Mitliagkas et al., 2013; Hardt & Price, 2014) are much simpler, as they can find the k vectors simultaneously by applying the power method only on k random initial vectors. The second difference, on the other hand, has a beneficial effect, which allows the tensor power method to converge exponentially faster than the matrix one when starting from good initial vectors. Then a natural question is: can we inherit the best of both worlds? Namely, is it possible to have a simple algorithm which can find the k eigenvectors of a tensor simultaneously and converge faster than that for matrices?\nOur Results. As in previous works, we consider the slightly harder scenario in which we only have access to a noisy version of the tensor we want to decompose. This arises in applications such as learning latent variable models, in which the tensor we have access to is obtained from some empirical average of the observed data. Our main contribution is to answer the above question affirmatively.\nFirst, we consider the batch setting in which we assume\nthat the given noisy tensor is stored somewhere and can be accessed whenever we want to. In this setting, we identify a sufficient condition such that if we have k initial vectors satisfying this condition, then we can apply the tensor power method on them simultaneously, which will come within some distance \u03b5 to the eigenvectors in O(log log 1\u03b5 ) iterations, with parameters related to eigenvalues considered as constant. To apply such a result, we need an efficient way to find such initial vectors. We show how to do this by choosing a good direction to project the tensor down to a matrix while preserving the eigengaps, and then applying the matrix power method for only a few iterations just to obtain vectors meeting that sufficient condition. The number of iterations needed here is only O(log d), independent of \u03b5, where d is the dimension of the eigenvectors.\nThe result stated above is for orthogonal tensors. On the other hand, it is known that an nonorthogonal tensor with linearly independent eigenvectors can be converted into an orthogonal one with the help of some whitening matrix. However, previous works usually pay little attention on how to find such a whitening matrix efficiently. According to (Anandkumar et al., 2014a), one way is via SVD on some second moment matrix, but doing this using the matrix power method would take longer to converge compared to the tensor power method which would then be applied on the whitened tensor. Our second contribution is to provide an efficient way to find a whitening matrix, by simply applying only one iteration of the matrix power method.\nWhile most previous works on tensor decomposition focus on the batch setting, storing even a tensor of order three requires \u2126(d3) space, which is infeasible for a large d. We show to avoid this in the streaming setting, with a stream of data arriving one at a time, which is the only source of information about the tensor. We provide a streaming algorithm using only O(kd) space, which is the smallest possible, just enough to store the k eigenvectors of dimension d. To achieve an approximation error \u03b5, the total number of samples we need is O(kd log d+ 1\u03b52 log(d log 1 \u03b5 )).\nRelated Works There is a huge literature on tensor decomposition, and it is beyond the scope of this paper to give a comprehensive survey. Thus, we only compare our results to the most related ones, particularly those based on the power method. While different works may focus on different aspects, we are most interested in understanding how the error parameter \u03b5 affects various performance measures, having in mind a small \u03b5.\nFirst, the batch algorithm of Anandkumar et al. (2014a), using a better analysis in (Wang & Anandkumar, 2016), runs in time about O((k2 log k)(log log 1\u03b5 )), which can be made to run in O(k(log log 1\u03b5 )) iterations in parallel, while ours are O(k log log 1\u03b5 ) and O(log log 1 \u03b5 ), respectively. On\nthe other hand, one advantage of their algorithm is that its running time does not depend on the eigengaps, while ours has the dependence hidden above as some constant.\nIn the streaming setting, Wang & Anandkumar (2016) provided an algorithm using O(dk log k) memory and O( k\u03b52 log( d \u03b5 )) samples, while ours only uses O(dk) memory and O( 1\u03b52 log(d log log 1 \u03b5 )) samples.\n1 Nevertheless, the sample complexity of Wang & Anandkumar (2016) is also independent of the eigengaps, while ours has the dependence hidden above as a constant factor.\nAs one can see, our algorithms, which find the k eigenvectors simultaneously, allow us to save a factor of k in the time complexity and the sample complexity, although our bounds may become worse when the eigengaps are small. Thus, our algorithms can be seen as new options for users to choose from, depending on the data they are given.\nAlthough not directed related, let us also compare to previous works on SVD. Two related ones, both based on the simultaneous matrix power method, are the batch algorithm of (Hardt & Price, 2014) which converges in O(log d\u03b5 ) iterations, and the streaming algorithm of (Li et al., 2016) which requires O( 1\u03b52 log(d log 1 \u03b5 )) samples. Both bounds are worse than ours and also depend on the eigengaps. Thus, although one approach for orthogonal tensor decomposition is to reduce it to a matrix SVD problem, this does not appear to result in better performance than ours.\nFinally, comparisons of the tensor power method with other approaches can be found in works such as (Anandkumar et al., 2014a; Wang & Anandkumar, 2016). For example, the online SGD approach of (Ge et al., 2015) works only for tensors of even orders and its sample complexity has a poor dependency on the dimension d.\nOrganization of the paper. First, we provide some preliminaries in Section 2. Then we present our batch algorithm for orthogonal and symmetric tensors of order three in Section 3, and then for general orthogonal tensors in Section 4. In Section 5, we introduce our whitening procedure for nonorthogonal but symmetry tensors. Finally, in Section 6, we present our algorithm for the streaming setting. Due to the space limitation, we will move all our proofs to the appendix in the supplementary material."}, {"heading": "2. Preliminaries", "text": "Let us first introduce some notations and definitions which we will use later. Let R denote the set of real numbers and N the set of positive integers. Let N (0, 1) denote the standard normal distribution with mean 0 and variance 1,\n1We use a different input distribution from theirs. The bound listed here is modified from theirs according to our distribution.\nand let N d(0, 1), for d \u2208 N, denote the d-variate one which has each of its d dimensions sampled independently from N (0, 1). For d \u2208 N, let [d] denote the set {1, . . . , d}. For a vector x, let \u2016x\u2016 denote its L2 norm. For d \u2208 N, let Id denote the d \u00d7 d identity matrix. For a matrix A \u2208 Rd\u00d7k, let Ai, for i \u2208 [k], denote its i-th column, and let Ai,j , for j \u2208 [d], be the j-th entry of Ai. Moreover, for a matrix A, let A> denote its transpose, and define its norm as \u2016A\u2016 = maxx\u2208Rk \u2016A\u00b7x\u2016 \u2016x\u2016 , using the convention that 0 0 = 0.\nTensors are the focus of our paper, which can be seen as generalization of matrices to higher orders. For simplicity of presentation, we will use symmetric tensors of order three as examples in the following definitions. A real tensor T of order three can be seen as an three-dimensional array in Rd\u00d7d\u00d7d, for some d \u2208 N, with its (i, j, k)-th entry denoted as Ti,j,k. For such a tensor T and three matrices A \u2208 Rd\u00d7m1 , B \u2208 Rd\u00d7m2 , C \u2208 Rd\u00d7m3 , let T (A,B,C) be the tensor in Rm1\u00d7m2\u00d7m3 , with its (a, b, c)th entry defined as \u2211 i,j,k\u2208[d] Ti,j,kAa,iBb,jCc,k. The norm of a tensor T we will use is the operator norm: \u2016T\u2016 = maxx,y,z\u2208Rd |T (x,y,z)| \u2016x\u2016\u2016y\u2016\u2016z\u2016 .\nThe tensor decomposition problem. In this problem, there is a tensor T with some unknown decomposition\nT = \u2211 i\u2208[d] \u03bbi \u00b7 ui \u2297 ui \u2297 ui,\nwith \u03bbi \u2265 0 and ui \u2208 Rd for any i \u2208 [d]. Then given some k \u2208 [d] and \u03b5 \u2208 (0, 1), our goal is to find \u03bb\u0302i and u\u0302i with\n|\u03bb\u0302i \u2212 \u03bbi| \u2264 \u03b5 and \u2016u\u0302i \u2212 ui\u2016 \u2264 \u03b5, for every i \u2208 [k].\nWe will assume that\u2211 i\u2208[d] \u03bbi \u2264 1 and \u2200i \u2208 [k] : \u03bbi > \u03bbi+1. (1)\nAs in previous works, we consider a slightly harder version of the problem, in which we only have access to some noisy version of T , instead of the noiseless T . We will consider the following two settings. In the batch setting, we have access to some T\u0304 = T + \u03a6 for the whole time, for some perturbation tensor \u03a6. In the streaming setting, we have a stream of data points x1, x2, . . . arriving one by one, which provide the only information we have about T , with each x\u03c4 \u2208 Rd allowing us to compute some T\u0304\u03c4 with mean E[T\u0304 ] = T . In this streaming setting, we are particularly interested in the case of a large d which prohibits one to store a tensor of size d3 in memory.\nPower Method: Matrices versus Tensors. Note that a tensor of order two is just a matrix, and a popular approach for decomposing matrices is the so-called power\nmethod, which works as follows. Suppose we are given a d \u00d7 d matrix M = \u2211 i\u2208[d] \u03bbi \u00b7 ui \u2297 ui, with nonnegative \u03bb1 > \u03bb2 \u2265 \u00b7 \u00b7 \u00b7 and orthonormal vectors u1, . . . , ud. The power method starts with some q(0) = \u2211 i\u2208[d] ci \u00b7 ui, usually chosen randomly, and then repeatedly performs the update q(t) = M \u00b7 q(t\u22121), which results in\nq(t) = \u2211 i\u2208[d] \u03bbi ( u>i q (t\u22121) ) \u00b7 ui = \u2211 i\u2208[d] ( \u03bbtici ) \u00b7 ui.\nNote that for any i 6= 1, as \u03bbi < \u03bb1, the coefficient \u03bbtici will soon become much smaller than the coefficient \u03bbt1c1 if c1 is not too small, which is likely to happen for a randomly chosen q(0). This has the effect that after normalization, q(t)/\u2016q(t)\u2016 approaches u1 quickly.\nNow consider a tensor T = \u2211\ni\u2208[d] \u03bbi \u00b7 ui \u2297 ui \u2297 ui, with nonnegative \u03bb1 > \u03bb2 \u2265 \u00b7 \u00b7 \u00b7 and orthonormal vectors u1, . . . , ud. The tensor version of the power method again starts from a randomly chosen q(0) =\u2211\ni\u2208[d] ci \u00b7 ui, but now repeatedly performs the update q(t) = T (Id, q (t\u22121), q(t\u22121)), which in turn results in\nq(t) = \u2211 i\u2208[d] \u03bbi ( u>i q (t\u22121) )2 \u00b7 ui = \u2211 i\u2208[d] \u03bb\u22121i (\u03bbici) 2t \u00b7 ui.\nThe coefficient of each ui now has a different form from the matrix case, and this leads to the following two effects.\nFirst, one now has much less control on what q(t)/\u2016q(t)\u2016 converges to. In fact, it can converge to any ui 6= u1 if ui has the largest value of \u03bbi|ci|, which happens with a good probability if \u03bbi is not much smaller than \u03bb1. Consequently, to find the top k vectors u1, . . . , uk, previous works based on the power method all need much more complicated procedures (Anandkumar et al., 2014a), compared to those for matrices, as discussed in the introduction.\nOn the other hand, the different form of q(t) has the beneficial effect that the convergence is now exponentially faster than in the matrix case. More precisely, if \u03bbi|ci| < \u03bbj |cj |, than the gap between the coefficients (\u03bbici)2 t and (\u03bbjcj)2 t is now amplified much faster. We will show how to inherit this nice property of faster convergence but at the same time avoid the difficulty discussed above."}, {"heading": "3. Orthogonal and Symmetric Tensors of Order Three", "text": "In this section, we focus on the special case in which the tensors to be decomposed are orthogonal, symmetric, and of order three. Formally, there is an underlying tensor\nT = \u2211 i\u2208[d] \u03bbi \u00b7 ui \u2297 ui \u2297 ui,\nAlgorithm 1 Robust tensor power method Input: Tensor T\u0304 \u2208 Rd\u00d7d\u00d7d and parameters k, L, S,N . Initialization Phase: Sample w1, . . . , wL, Y (0) 1 , . . . , Y (0) k \u223c N d(0, 1).\nCompute w\u0304 = 1L \u2211\nj\u2208[L] T\u0304 (Id, wj , wj). Compute M\u0304 = T\u0304 (Id, Id, w\u0304). Factorize Y (0) as Z(0) \u00b7R(0) by QR decomposition. for s = 1 to S do\nCompute Y (s) = M\u0304 \u00b7 Z(s\u22121). Factorize Y (s) as Z(s) \u00b7R(s) by QR decomposition.\nend for Tensor Power Phase: Let Q(0) = Z(S). for t = 1 to N do\nCompute Y (t)j = T\u0304 (Id, Q (t\u22121) j , Q (t\u22121) j ), \u2200j \u2208 [k].\nFactorize Y (t) as Q(t) \u00b7R(t) by QR decomposition. end for Output: u\u0302j = Q (N) j and \u03bb\u0302j = T\u0304 (u\u0302j , u\u0302j , u\u0302j), \u2200j \u2208 [k].\nwith orthonormal vectors ui\u2019s and real \u03bbi\u2019s satisfying the condition (1). Then given k \u2208 [d] and \u03b5 \u2208 (0, 1), our goal is to find approximates to those \u03bbi and ui within distance \u03b5, but we only have access to some noisy tensor T\u0304 = T + \u03a6 for some symmetric perturbation tensor \u03a6.\nOur algorithm is given in Algorithm 1, which consists of two phases: the initialization phase and the tensor power phase. The main phase is the tensor power phase, which we will discuss in detail in Subsection 3.1. For our tensor power phase to work, it needs to have a good starting point. This is provided by the initialization phase, which we will discuss in detail in Subsection 3.2. Through these two subsections, we will prove Theorem 1 below, which summarizes the performance of our algorithm, according to the following parameters of the tensor:\n\u03b3 = min i\u2208[k] \u03bb2i \u2212 \u03bb2i+1 \u03bb2i and \u2206 = min i\u2208[k] \u03bbi \u2212 \u03bbi+1 4 .\nTheorem 1. Suppose \u03b5 \u2264 \u03bbk2 and the perturbation tensor has the bound \u2016\u03a6\u2016 \u2264 min{ \u2206\u03b5\n2 \u221a k , \u22063d ,\n\u03b10\u2206 2\n\u221a dk } for a small enough constant \u03b10. Then for some L = O( 1\u03b32 log d), S = O( 1\u03b3 log d), and N = O(log( 1 \u03b3 log 1 \u03b5 )), our Algorithm 1 with high probability will output u\u0302i and \u03bb\u0302i with \u2016u\u0302i\u2212ui\u2016 \u2264 \u03b5 and |\u03bb\u0302i \u2212 \u03bbi| \u2264 \u03b5 for every i \u2208 [k].\nLet us make some remarks about the theorem. First, the L samples are used to compute w\u0304 and M\u0304 , which can be done in a parallel way. Second, our parameter \u03b3 is related to a parameter \u03b3\u2032 = mini\u2208[k] \u03bbi\u2212\u03bbi+1 \u03bbi\nused in (Hardt & Price, 2014), and it is easy to verify that \u03b3 \u2265 \u03b3\u2032. Thus, our algorithm for tensors converges in O( 1\u03b3 log d + log( 1 \u03b3 log 1 \u03b5 )) rounds, which is faster than the O( 1\u03b3\u2032 log d + 1 \u03b3\u2032 log 1 \u03b5 )\nrounds of (Hardt & Price, 2014) for matrices. Note that our dependence on the error parameter \u03b5 is exponentially smaller than that of (Hardt & Price, 2014), which means that for a small \u03b5, we can decompose tensors much faster than matrices. Finally, compared to previous works on tensors, our convergence time, for a small \u03b5 is about O(log log 1\u03b5 ) while those in (Anandkumar et al., 2014a; Wang & Anandkumar, 2016) are at least \u2126(k log log 1\u03b5 )."}, {"heading": "3.1. Our Robust Tensor Power Method", "text": "The tensor power phase of our Algorithm 1 is based on our version of the tensor power method, which works as follows. At each step t, we maintain a d \u00d7 k matrix Q(t) with columns Q(t)1 , . . . , Q (t) k as our current estimators for u1, . . . , uk, which is obtained by updating the previous estimators with the following two operations.\nThe main operation is to apply the noisy tensor T\u0304 on them simultaneously to get a d\u00d7 k matrix Y (t) with its j-th column computed as Y (t)j = T\u0304 (Id, Q (t\u22121) j , Q (t\u22121) j ), which equals \u2211 i\u2208[d] \u03bbi ( u>i Q (t\u22121) j )2 \u00b7 ui + \u03a6\u0302(t)j ,\nfor \u03a6\u0302(t)j = \u03a6(Id, Q (t\u22121) j , Q (t\u22121) j ). This implies that\n\u2200i \u2208 [d] : u>i Y (t) j = \u03bbi ( u>i Q (t\u22121) j )2 + u>i \u03a6\u0302 (t) j , (2)\nwhich shows the progress made by this operation.\nThe second operation is to orthogonalize Y (t) as\nY (t) = Q(t) \u00b7R(t),\nby the QR decomposition via the Gram-Schmidt process, to obtain a d\u00d7 k matrix Q(t) with columns Q(t)1 , . . . , Q (t) k , which then become our new estimators. As we will show in Lemma 1 below, given a small enough \u2016\u03a6\u2016, if we start with a full-rank Q(0), then each Q(t) also has full rank and consists of orthonormal columns, and each R(t) is invertible. Moreover, although we apply the QR decomposition on the whole matrix Y (t) to obtain the matrix Q(t), it has the effect that for any m \u2208 [k], the first m columns of Q(t) can be seen as obtained from the first m columns of Y (t) by a QR decomposition. This property is needed in our Lemma 1 and Theorem 2 below to guarantee the simultaneous convergence of Q(t)i to ui for every i \u2208 [k].\nBefore stating Lemma 1 which guarantees the progress we make at each step, let us prepare some notations first. For a d \u00d7 k matrix A and some m \u2208 [k], let A[m] denote the d \u00d7m matrix containing the first m columns of A. Let U denote the d\u00d7 k matrix with the target vector ui as its i\u2019th column. For a d\u00d7 k matrix Q and some m \u2208 [k], define\ncosm(Q) = min y\u2208Rm\n\u2225\u2225\u2225U>[m] \u00b7Q[m] \u00b7 y\u2225\u2225\u2225 /\u2016Q[m] \u00b7 y\u2016,\nwhich equals the cosine of the m\u2019th principal angle between the column spaces of U[m] and Q[m], let sinm(Q) =\u221a\n1\u2212 cos2m(Q), and let us use as the error measure\ntanm(Q) = sinm(Q)/ cosm(Q).\nMore information about the principal angles can be found in, e.g., (Golub & Van Loan, 1996). Then we have the following lemma, which we prove in Appendix B.1.\nLemma 1. Fix any m \u2208 [k] and t \u2265 0. Let \u03a6\u0302(t)[m] denote the d\u00d7m matrix with \u03a6\u0302(t)j = \u03a6(Id, Q (t\u22121) j , Q (t\u22121) j ) as its j\u2019th column, and suppose\u2225\u2225\u2225\u03a6\u0302(t)[m]\u2225\u2225\u2225 < \u2206 \u00b7min{\u03b2, cos2m(Q(t\u22121))} , (3) for some \u03b2 > 0. Then for \u03c1 = maxi\u2208[k](\n\u03bbi+1 \u03bbi ) 1 4 , we have\ntanm(Q (t)) \u2264 max { \u03b2,max{\u03b2, \u03c1} \u00b7 tan2m(Q(t\u22121)) } .\nObserve that the guarantee provided by the lemma above has a similar form as that in (Hardt & Price, 2014) for matrices. The main difference is that here in the tensor case, we have the error measure essentially squared after each step, which has the following two implications. First, to guarantee that the error is indeed reduced, we need tanm(Q\n(t\u22121)) to be small enough (say, less than one), unlike in the matrix case. Next, if we indeed have a small enough tanm(Q(t\u22121)), then the error can be reduced in a much faster rate than in the matrix case. Another difference is that here we provide the guarantee for all the k submatrices Q(t)[m], for m \u2208 [k], instead of just one matrix Q\n(t). This allows us to show the simultaneous convergence of each column Q(t)i to the target vector ui for every i \u2208 [k], as given in the following, which we prove in Appendix B.2.\nTheorem 2. For any \u03b5 \u2208 (0, \u03bbk2 ), there exists some N \u2264 O(log( 1\u03b3 log 1 \u03b5 )) such that the following holds. Suppose the perturbation is bounded by \u2016\u03a6\u2016 \u2264 \u2206\u03b5 2 \u221a k and we start from some initial Q(0) with tanm Q(0) \u2264 1 for every m \u2208 [k]. Then for any t \u2265 N , with u\u0302i = Q(t)i and \u03bb\u0302i = T\u0304 (u\u0302i, u\u0302i, u\u0302i), we have \u2016ui \u2212 u\u0302i\u2016 \u2264 \u03b5 and |\u03bbi \u2212 \u03bb\u0302i| \u2264 \u03b5, for every i \u2208 [k].\nNote that the convergence rate guaranteed by the theorem above is exponentially faster than that in (Hardt & Price, 2014) for matrices, assuming that we indeed can have such a good initial Q(0) to start with. In the next subsection, we show how it can be found efficiently."}, {"heading": "3.2. Initialization Procedure", "text": "Our approach for finding a good initialization is to project the tensor down to a matrix and apply the matrix power\nmethod for only a few steps just to make the tangents less than one. Although we could continue applying the matrix power method till reaching the much smaller target bound \u03b5, this would take exponentially longer than by switching to the tensor power method as we actually do.\nAs mentioned above, we would first like to project the tensor T\u0304 down to a matrix. A naive approach is to sample a random vector w\u0304 and take the matrix T\u0304 (Id, Id, w\u0304) \u2248 T (Id, Id, w\u0304) = \u2211 i\u2208[d] \u03bbi(u > i w\u0304) \u00b7 ui \u2297 ui. However, this may mess up the gaps between eigenvalues, which are needed to guarantee the convergence rate of the matrix power method. The reason is that as each u>i w\u0304 has mean zero, the coefficient \u03bbi(u>i w\u0304) also has mean zero and thus has a good chance of coming very close to others. To preserve the gaps, we would like to have u>i w\u0304 \u2265 u>i+1w\u0304 for each i with high probability. To achieve this, let us first imagine sampling a random w \u2208 Rd from N d(0, 1), and computing the vector w\u0304 = T\u0304 (Id, w, w), which is close to\nT (Id, w, w) = \u2211 i\u2208[d] \u03bbi(u > i w) 2 \u00b7 ui.\nThen one can show that for every i, E[(u>i w)2] = 1, so that E[w\u0304] \u2248 E[T (Id, w, w)] = \u2211 i\u2208[d] \u03bbiui, and\nu>i E[w\u0304] \u2248 \u03bbi > \u03bbi+1 \u2248 u>i+1 E[w\u0304].\nHowever, we want the gap-preserving guarantee to be in high probability, instead in expectation. Thus we go further by sampling not just one, but some number L of vectors w1, . . . , wL independently from the distribution N d(0, 1), and then taking the average\nw\u0304 = 1\nL \u2211 j\u2208[L] T\u0304 (Id, wj , wj). (4)\nThe following lemma shows that such a w\u0304 is likely to have u>i w\u0304 \u2248 \u03bbi, which we prove in Appendix B.3. Lemma 2. Suppose we have T\u0304 = T + \u03a6 with \u2016\u03a6\u2016 \u2264 \u22063d . Then for some L \u2264 O( 1\u03b32 log d), the vector w\u0304 computed according to (4) with high probability satisfies\u2223\u2223u>i w\u0304 \u2212 \u03bbi\u2223\u2223 \u2264 14 (\u03bbi\u03b3 + 2\u2206) for every i \u2208 [k]. (5) With this w\u0304, we compute the matrix M\u0304 = T\u0304 (Id, Id, w\u0304). As shown by the following lemma, which we prove in Appendix B.4, M\u0304 is close to a matrix with ui\u2019s as eigenvectors and good gaps between eigenvalues.\nLemma 3. Suppose we have T\u0304 = T + \u03a6. Then for any w\u0304 satisfying the condition (5) in Lemma 2, the matrix M\u0304 = T\u0304 (Id, Id, w\u0304) can be decomposed as\nM\u0304 = \u2211 i\u2208[d] \u03bb\u0304i \u00b7 ui \u2297 ui + \u03a6\u0304,\nfor some \u03bb\u0304i\u2019s with \u03bb\u0304i \u2212 \u03bb\u0304i+1 \u2265 \u22062, for i \u2208 [k], and \u03a6\u0304 = \u03a6(Id, Id, w\u0304) with \u2016\u03a6\u0304\u2016 \u2264 2\u2016\u03a6\u2016.\nWith such a matrix M\u0304 , we next apply the matrix power method of (Hardt & Price, 2014) to find good approximates to its eigenvectors. More precisely, we sample an initial matrix Y (0) \u2208 Rd\u00d7k by choosing each of its column independently according to the distribution N d(0, 1), and factorize it as Y (0) = Z(0) \u00b7 R(0) by QR decomposition via the Gram-Schmidt process. Then at step s \u2265 1, we multiply the previous estimate Z(s\u22121) by M\u0304 to obtain Y (s) = M\u0304 \u00b7 Z(s\u22121), factorize it as Y (s) = Z(s) \u00b7 R(s) by QR decomposition via the Gram-Schmidt process, and then take the orthonormal Z(s) as the new estimate. The following lemma shows the number of steps needed to find a good enough Z(s). Lemma 4. Suppose we are given a matrix M\u0304 having the decomposition described in Lemma 3, with \u2016\u03a6\u0304\u2016 \u2264 2\u03b10\u2206\n2 \u221a dk\nfor a small enough constant \u03b10. Then there exists some S \u2264 O( 1\u03b3 log d) such that with high probability, we have tanm(Z (s)) \u2264 1 for every m \u2208 [k] whenever s \u2265 S.\nThis together with the previous two lemmas guarantee that given T\u0304 = T +\u03a6, with \u2016\u03a6\u2016 \u2264 min{ \u22063d , \u03b10\u2206 2 \u221a dk\n}, for a small enough constant \u03b10, we can obtain with high probability a good Z(S) which can be used as the initial Q(0) for our tensor power phase. Combining this with Theorem 2 in the previous subsection, we then have our Theorem 1 given at the beginning of the section."}, {"heading": "4. General Orthogonal Tensors", "text": "In the previous section, we consider tensors which are orthogonal, symmetric, and of order three. In this section, we show how to extend our results for general orthogonal tensors, to deal with higher orders first and then asymmetry."}, {"heading": "4.1. Higher-Order Tensors", "text": "To handle orthogonal and symmetric tensors of any order, only the initialization procedure needs to be modified. First, for tensors of any odd order, a straightforward modification is as follows. Take for example a tensor of order 2r + 1. Now we simply compute\nw\u0304 = 1\nL \u2211 j\u2208[L] T\u0304 (Id, wj , . . . , wj),\nwith 2r copies of wj , and similarly to Lemma 2, one can show that w\u0304 is likely to be close to the vector \u2211 i\u2208[d] \u03bbi \u00b7ui. With such a vector w\u0304, one can show that the matrix\nM\u0304 = T\u0304 (Id, Id, w\u0304, . . . , w\u0304), with 2r \u2212 1 copies of w\u0304, is close to the matrix \u2211\ni\u2208[d] \u03bb 2r i \u00b7\nui \u2297 ui, similarly to Lemma 3. Then the rest is the same\nas that in the previous section. Note that this approach has the eigenvalues decreased exponentially in r. A different approach avoiding this is to compute M\u0304 directly as\nM\u0304 = 1\nL \u2211 j\u2208[L] ( T\u0304 (Id, Id, wj , . . . , wj) )2 ,\nwhich is close to\n1\nL \u2211 j\u2208[L] (T (Id, Id, wj , . . . , wj)) 2\n= 1\nL \u2211 j\u2208[L] \u2211 i\u2208[d] \u03bb2i ( u>i wj )4r\u22122 \u00b7 ui \u2297 ui =\n\u2211 i\u2208[d] \u03bb2i 1 L \u2211 j\u2208[L] ( u>i wj )4r\u22122 \u00b7 ui \u2297 ui. Then one can again show that such a matrix M\u0304 is likely to be close to the matrix \u2211 i\u2208[d] \u03bb 2 i \u00b7 ui \u2297 ui.\nTo handle tensors of even orders, the initialization is slightly different but the idea is similar. Given a tensor of order 2r, we again sample vectors w1, . . . , wL as before, but now we compute the matrix directly as\nM\u0304 = 1\nL \u2211 j\u2208[L] T\u0304 (Id, Id, wj , . . . , wj),\nwith 2r \u2212 2 copies of wj . As before, one can show that the matrix M\u0304 is likely to be close to the matrix \u2211 i\u2208[d] \u03bbi \u00b7 ui\u2297ui. Then again we can apply the matrix power method on M\u0304 and obtain a good initialization for the tensor power method as before. Note that now the eigenvalues are no longer squared, and the previous requirement on \u2016\u03a6\u2016 can be slightly relaxed, with the dependence on \u22062 being replaced by \u2206."}, {"heading": "4.2. Asymmetric Tensors", "text": "For simplicity of presentation, let us focus on the third order case; the extension to higher orders is straightforward. That is, now the underlying tensor has the form\nT = \u2211 i\u2208[d] \u03bbi \u00b7 ai \u2297 bi \u2297 ci,\nwith nonnegative \u03bbi\u2019s satisfying the condition (1), together with three sets of orthonormal vectors of ai\u2019s, bi\u2019s, and ci\u2019s. As before, we only have access to a noisy version T\u0304 of T .\nThe main modification of our algorithm is again to the initialization procedure, but the idea is also similar. To find a good initial matrix A for ai\u2019s, we sample w1, . . . , wL independently from N d(0, 1), and now compute the matrix\nM\u0304 = 1\nL \u2211 j\u2208[L] ( T\u0304 (Id, Id, wj) ) ( T\u0304 (Id, Id, wj) )> .\nAs before, it is not hard to show that\nM\u0304 \u2248 \u2211 i\u2208[d] 1 L \u2211 j\u2208[L] \u03bb2i ( c>i wj )2 \u00b7 ai \u2297 ai, which is close to the matrix \u2211 i\u2208[d] \u03bb 2 i \u00b7 ai \u2297 ai with high probability. From the matrix M\u0304 , we can again apply the matrix power method to find a good initial matrix A. Similarly, we can find good initial matrices B and C for bi\u2019s and ci\u2019s, respectively.\nNext, with such matrices, we would like to apply the tensor power method, which we modify as follows. Now at each step t, we take previous estimates A(t\u22121), B(t\u22121), C(t\u22121), and compute X(t)i = T\u0304 (Id, B (t\u22121) i , C (t\u22121) i ), Y (t) i = T\u0304 (A (t\u22121) i , Id, C (t\u22121) i ), Z (t) i = T\u0304 (A (t\u22121) i , B (t\u22121) i , Id), for i \u2208 [k], followed by orthonormalizing X(t), Y (t), Z(t) to obtain the new estimates A(t), B(t), C(t) via QRdecomposition. It is not hard to show that the resulting algorithm has a similar convergence rate as our Algorithm 1."}, {"heading": "5. Nonorthogonal but Symmetric Tensors", "text": "In the previous section, we consider general orthogonal tensors, which can be asymmetric. In this section, we consider non-orthogonal tensors which are symmetric. We remark that for some latent variable models such as the multi-view model, the corresponding asymmetric tensors can be converted into symmetric ones (Anandkumar et al., 2014a), so that our result here can still be applied. For simplicity of exposition, let us again focus on the case of order three, so that the given tensor has the form T =\u2211\ni\u2208[d] \u03bbi \u00b7vi\u2297vi\u2297vi, but the vectors vi\u2019s are no longer assumed to be orthogonal to each other. Still we assume them to be linearly independent, and we again assume without loss of generality that \u2016T\u2016 \u2264 1 and \u2016vi\u2016 = 1 for each i. In addition, let us assume, as in previous works, that \u03bbj = 0 for j \u2265 k + 1.2\nFollowing (Anandkumar et al., 2014a), we would like to whiten such a tensor T into an orthogonal one, so that we can then apply our Algorithm 1. More precisely, our goal is to find a d\u00d7 k matrix W such that the tensor T (W,W,W ) becomes orthogonal. As in (Anandkumar et al., 2014a), assume that we also have available a matrix\nM = \u2211 i\u2208[k] \u03bbi \u00b7 vi \u2297 vi.3\nThen for a whitening matrix, it suffices to find some W\n2This assumption is not necessary. We assume it just to simplify the first step of our algorithm given below. Without it, we can simply replace that step by the matrix power method used in our Algorithm 1, which takes more steps but can still do the job.\n3More generally, the weights \u03bbi in M are allowed to differ from those in T , but for simplicity we assume they are the same.\nsuch that W>MW = Ik. The reason is that\nIk = W >MW = \u2211 i\u2208[k] \u03bbi \u00b7 ( W>vi ) \u2297 ( W>vi ) ,\nwhich implies that the vectors \u221a \u03bbiW\n>vi, for i \u2208 [k], are orthonormal. Then the tensor T (W,W,W ) equals\u2211\ni\u2208[k] \u03bbi \u00b7 (W>vi)\u22973 = \u2211 i\u2208[k] 1\u221a \u03bbi \u00b7 (\u221a \u03bbiW >vi )\u22973 ,\nwhich has an orthogonal decomposition.\nAccording to (Anandkumar et al., 2014a), one way to find such a W is to do the spectral decomposition of M as U\u039bU>, with eigenvectors as columns of U , and let W = U\u039b\u2212 1 2 . However, we will not take this approach, because finding a good approximate to U by the matrix power method would take longer to converge than the tensor power method which we will later apply to the whitened tensor. Our key observation is that it suffices to find a d\u00d7k matrix Q such that the matrix P = Q>MQ is invertible, since we can then let W = QP\u2212 1 2 and have\nW>MW = P\u2212 1 2Q>MQP\u2212 1 2 = Ik.\nWith such a W , the tensor T (W,W,W ) becomes orthogonal, so that we can decompose it4 to obtain \u03c3i = 1\u221a\u03bbi and ui = \u221a \u03bbiW\n>vi, from which we can recover \u03bbi = 1\u03c32i and\nvi = \u03c3iQP 1 2ui if Q has orthonormal columns.\nAs before, we consider a similar setting in which we only have access to a noisy M\u0304 = M + \u03a6\u0304, for some symmetric perturbation matrix \u03a6\u0304, in addition to the noisy tensor T\u0304 = T+\u03a6. Then our algorithm for finding the whitening matrix consists of the following two steps:\n1. Sample a random matrix Z \u2208 Rd\u00d7k with orthonormal columns, compute Y\u0304 = M\u0304Z, and factorize it as Y\u0304 = QR\u0304 by a QR decomposition.\n2. Compute P\u0304 = Q>M\u0304Q and output W\u0304 = QP\u0304\u2212 1 2 as\nthe whitening matrix.\nWe analyze our algorithm in the following. First note that Q is computed in the same way as we compute Z(1) in Algorithm 1, and with \u03bbk+1 = 0 we are likely to have tank(Q) \u2248 0 so that the matrix P = Q>MQ is invertible. Formally, we have the following, which we prove in Appendix C.1.\nLemma 5. Suppose \u2016\u03a6\u0304\u2016 \u2264 \u03b10\u03bbk\u221a dk for a small enough constant \u03b10. Then with high probability we have \u03c3max(P ) \u2264 \u03bb1 and \u03c3min(P ) \u2265 \u03bbk2 .\n4To apply our Algorithm 1, we need to scale it properly, say by a factor of \u221a \u03bbk/k to make its norm at most one.\nNext, with a small enough \u2016\u03a6\u0304\u2016, if P is invertible, then so is P\u0304 , and moreover, we have P\u0304\u2212 1 2 \u2248 P\u2212 12 . This is shown in the following, which we prove in Appendix C.2.\nLemma 6. Fix any \u2208 (0, 1) and suppose we have \u03c3min(P ) \u2265 2 and \u2016\u03a6\u0304\u2016 \u2264 . Then P\u0304 is invertible and \u2016P\u0304\u2212 12 \u2212 P\u2212 12 \u2016 \u2264 2 (\u03c3min(P ))\u22122(\u03c3max(P )) 1 2 .\nThen, with a good P\u0304\u2212 1 2 , we can obtain a good W\u0304 and have T\u0304 (W\u0304 , W\u0304 , W\u0304 ) close to T (W,W,W ) which has an orthogonal decomposition. This is shown in the following, which we prove in Appendix C.3.\nTheorem 3. Fix any \u03b5 \u2208 (0, \u03bbk4 ) and suppose we have \u2016\u03a6\u2016 \u2264 \u03b10\u03bb 3 2\nk \u03b5 and \u2016\u03a6\u0304\u2016 \u2264 \u03b10\u03b5min{ \u03bbk\u221a dk , \u03bb3k\u221a \u03bb1 }, for a\nsmall enough constant \u03b10. Then with high probability we have \u2016T\u0304 (W\u0304 , W\u0304 , W\u0304 )\u2212 T (W,W,W )\u2016 \u2264 \u03b5."}, {"heading": "6. Streaming setting", "text": "In the previous sections, we consider the batch setting in which the tensor T\u0304 is assumed to be stored somewhere which can be accessed whenever we want to. However, storing such a tensor, say of order three, requires a space complexity of \u2126(d3), which becomes impractical even for a moderate value of d. In this section, we study the possibility of achieving a space complexity of O(kd), which is the least amount of memory needed just to store the k vectors in Rd. More precisely, we consider the streaming setting, in which there is a stream of vectors x1, x2, . . . arriving one at a time. We assume that each vector x is sampled independently from some distribution over Rd, with \u2016x\u2016 \u2264 1 and some function g : Rd \u2192 Rd\u00d7d\u00d7d such that\n\u2022 E[g(x)] = T , and given x, u, v \u2208 Rd, g(x)(Id, u, v) can be computed in O(d) space.\nSuch a function g is known to exist for some latent variable models (Ge et al., 2015; Wang & Anandkumar, 2016). Given such a function, our algorithms in previous sections can all be converted to work in the streaming setting using O(kd) space. This is because all our operations involving tensors have the form T\u0304 (Id, u, v), for some u, v \u2208 Rd, which can be realized as(\n1 |J | \u2211 t\u2208J g(xt)\n) (Id, u, v) = 1\n|J | \u2211 t\u2208J (g(xt) (Id, u, v)) ,\nfor a collection J of samples, with the righthand side above clearly computable in O(kd) space.5 Then depending on the distance we want between T\u0304 = 1|J| \u2211 t\u2208J g(xt) and T ,\n5This also includes the initialization phase in which we now do not store the matrix M\u0304 explicitly but instead replace the operation M\u0304Zi by T\u0304 (Id, Zi, w\u0304).\nwe can choose a proper size for J . In fact, to save the total number of samples, we can follow the approach of (Li et al., 2016) by choosing different sizes in different iterations of the matrix or tensor power method.\nFollowing (Wang & Anandkumar, 2016), let us take the specific case with g(x) = x\u2297 x\u2297 x as a concrete example and focus on the orthogonal case studied in Section 3; it is not hard to convert other algorithms of ours to the streaming setting. One can show that in this specific case, we have E[x] = \u2211 i\u2208[d] \u03bbiui so that there is a more efficient way to find a vector w\u0304 for producing the matrix M\u0304 in the initialization phase. Formally, we have the following lemma, which we prove in Appendix D.1.\nLemma 7. There is an algorithm using O(d) space and O( log k\u22062 ) samples to find some w\u0304 \u2208 R\nd satisfying the condition (5) in Lemma 2 with high probability.\nWith such a vector w\u0304, we can then use the streaming algorithm of (Li et al., 2016) to find a good initial matrix Z for the later tensor power phase. Formally, we have the following lemma, which we prove in Appendix D.2.\nLemma 8. Given w\u0304 from Lemma 7, we can use O(kd) space and O(kd log d \u03b3\n\u22064\u03b3 ) samples to find some Z \u2208 R d\u00d7k\nwith tanm(Z) < 1, for any m \u2208 [k], with high probability.\nHaving such a matrix Z, we can proceed to the tensor power phase. Borrowing again the idea from (Li et al., 2016), let us partition the incoming data into blocks of increasing sizes, with the t\u2019th block Jt used to carry out one tensor power iteration Y (t)i = T\u0304 (t)(Id, Q (t\u22121) i , Q (t\u22121) i ), for\ni \u2208 [k], of Algorithm 1, with T\u0304 (t) = 1|Jt| \u2211 \u03c4\u2208Jt g(x\u03c4 ). Instead of preparing this T\u0304 (t) and then computing each Y (t)i , we now go through |Jt| steps of updates:\n\u2022 For \u03c4 \u2208 Jt do: Y (t)i = Y (t) i + 1 |Jt| (x > \u03c4 Q (t\u22121) i ) 2x\u03c4 .\nThe block sizes are chosen carefully to keep \u2016T\u0304 (t) \u2212 T\u2016 small enough so that we can have tanm(Q(t)) decreased in a desirable rate. Here, we choose the parameters\n\u03b2t = max { \u03c12 t\u22121, \u03b5\n2\n} and |Jt| = c0 log(dt)\n\u22062\u03b22t , (6)\nfor a large enough constant c0, to make the condition (3) in Lemma 1 hold with high probability so that we have tanm(Q\n(t)) \u2264 \u03b2t. In Appendix D.3, we summarize our algorithm and prove the following theorem.\nTheorem 4. Given \u03b5 \u2208 (0, \u03bbk2 ), with high probability we can find \u03bb\u0302i, u\u0302i with |\u03bb\u0302i \u2212 \u03bbi|, \u2016u\u0302i \u2212 ui\u2016 \u2264 \u03b5, for any i \u2208 [k], using O(kd) space and O(kd log d \u03b3\n\u22064\u03b3 + log(d log( 1\u03b3 log 1 \u03b5 ))\n\u22062\u03b3\u03b52 ) samples."}], "year": 2017, "references": [{"title": "A method of moments for mixture models and hidden markov models", "authors": ["Anandkumar", "Animashree", "Hsu", "Daniel J", "Kakade", "Sham M"], "venue": "In COLT,", "year": 2012}, {"title": "Tensor decompositions for learning latent variable models", "authors": ["Anandkumar", "Animashree", "Ge", "Rong", "Hsu", "Daniel", "Kakade", "Sham M", "Telgarsky", "Matus"], "venue": "Journal of Machine Learning Research,", "year": 2014}, {"title": "Guaranteed non-orthogonal tensor decomposition via alternating rank-1 updates", "authors": ["Anandkumar", "Animashree", "Ge", "Rong", "Janzamin", "Majid"], "venue": "arXiv preprint arXiv:1402.5180,", "year": 2014}, {"title": "Estimating latent-variable graphical models using moments and likelihoods", "authors": ["Chaganty", "Arun Tejasvi", "Liang", "Percy"], "venue": "In ICML, pp", "year": 2014}, {"title": "Escaping from saddle pointsonline stochastic gradient for tensor decomposition", "authors": ["Ge", "Rong", "Huang", "Furong", "Jin", "Chi", "Yuan", "Yang"], "venue": "In Proceedings of The 28th Conference on Learning Theory, pp", "year": 2015}, {"title": "Matrix Computation", "authors": ["Golub", "Gene H", "Van Loan", "Charles F"], "year": 1996}, {"title": "The noisy power method: A meta algorithm with applications", "authors": ["Hardt", "Moritz", "Price", "Eric"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2014}, {"title": "Most tensor problems are NP-hard", "authors": ["Hillar", "Christopher J", "Lim", "Lek-Heng"], "venue": "Journal of the ACM (JACM),", "year": 2013}, {"title": "Tensor decompositions and applications", "authors": ["Kolda", "Tamara G", "Bader", "Brett W"], "venue": "SIAM review,", "year": 2009}, {"title": "Rivalry of two families of algorithms for memory-restricted streaming PCA", "authors": ["Li", "Chun-Liang", "Lin", "Hsuan-Tien", "Lu", "Chi-Jen"], "venue": "In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics (AISTATS),", "year": 2016}, {"title": "Memory limited, streaming PCA", "authors": ["Mitliagkas", "Ioannis", "Caramanis", "Constantine", "Jain", "Prateek"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2013}, {"title": "Online and differentially-private tensor decomposition", "authors": ["Wang", "Yining", "Anandkumar", "Anima"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2016}], "id": "SP:416c15e0bcd9e0a0a4ffd329d42a01667c852243", "authors": [{"name": "Po-An Wang", "affiliations": []}, {"name": "Chi-Jen Lu", "affiliations": []}], "abstractText": "Tensor decomposition is an important problem with many applications across several disciplines, and a popular approach for this problem is the tensor power method. However, previous works with theoretical guarantee based on this approach can only find the top eigenvectors one after one, unlike the case for matrices. In this paper, we show how to find the eigenvectors simultaneously with the help of a new initialization procedure. This allows us to achieve a better running time in the batch setting, as well as a lower sample complexity in the streaming setting.", "title": "Tensor Decomposition via Simultaneous Power Iteration"}