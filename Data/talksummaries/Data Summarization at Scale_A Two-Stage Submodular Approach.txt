3	16	However, modern datasets are growing so large that even linear time solutions can be computationally expensive.
4	49	Ideally, we want to find a sublinear summary of the given dataset so that optimizing these related functions over this reduced subset is nearly as effective, but not nearly as expensive, as optimizing them over the full dataset.
6	25	Even if they discretize the potential waiting locations to just include points at which pick-ups have occurred in the past, there are still hundreds of thousands, if not millions, of locations to consider.
10	42	On the other hand, major tourist destinations like Times Square will probably be busy year-round.
13	48	In more mathematical terms, consider some unknown distribution of functions D and a ground set Ω of n elements to pick from.
20	23	In this paper, we build on existing work to provide solutions for two-stage submodular maximization in both the streaming and distributed settings.
46	25	This makes the problem intractable for any reasonable number of elements, let alone the billions of elements that are common in modern datasets.
73	11	In this paper, we combine these two approaches in a novel and non trivial way in order to design a streaming algorithm (called REPLACEMENT-STREAMING) for the two-stage submodular maximization problem.
74	17	The goal of REPLACEMENT-STREAMING is to pick a set S of at most ` elements from the data stream, where we keep sets Ti ⊆ S, 1 ≤ i ≤ m as the solutions for functions fi.
83	15	That is, ∇i(x,A) tells us how much we can increase the value of fi(A) by either Algorithm 1 EXCHANGE 1: Input: u, S, {Ti}, τ and α {∇i terms use α} 2: if |S| < ` then 3: if 1m ∑m i=1∇i(u, Ti) ≥ τ then 4: S ← S + u 5: for 1 ≤ i ≤ m do 6: if∇i(u, Ti) > 0 then 7: if |Ti| < k then 8: Ti ← Ti + u 9: else 10: Ti ← Ti + u− REP(u, Ti) adding x to A (if |A| < k) or optimally swapping it in (if |A| = k).
116	9	So we would need to make one pass over the data to learn δ, which is not possible in the streaming setting.
120	10	This is due to the fact that a newly instantiated threshold τ could potentially have already seen elements with additive value of τ/(β`).
134	32	Specifically, Barbosa et al. (2015) proved that the following simple procedure results in a distributed algorithm with a constant factor approximation guarantee: (i) randomly split the data amongst M machines, (ii) run the classical greedy on each machine and pass outputs to a central machine, (iii) run another instance of the greedy algorithm over the union of all the collected outputs from all M machines, and (iv) output the maximizing set amongst all the collected solutions.
156	24	This dataset has images containing objects from 20 different classes, ranging from birds to boats.
163	10	We want to optimize the following monotone submodular functions: fi(S) = Li({e0}) − Li(S ∪ {e0}), where Li(S) = 1 |Ωi| ∑ x∈Ωi miny∈Si d(x, y).
168	37	With this in mind, we will compare our streaming algorithm REPLACEMENT-STREAMING against the non-streaming baseline of REPLACEMENT-GREEDY.
169	12	We also compare against a heuristic streaming baseline that we call STREAMSUM.
174	13	The graphs are organized so that each column shows the effects of varying a particular parameter, with the objective value being shown in the top row and the running time in the bottom row.
183	17	Figure 3 shows some sample images selected by REPLACEMENT-GREEDY (top) and REPLACEMENT- STREAMING (bottom).
185	16	For example, both images in the second column contain bikes and people; while in the third column, both images contain sheep.
198	28	Figure 4(a) shows the center points of the m = 20 randomly selected regions, overlaid on top of a heat map of all the customer pick-up locations.
203	14	We will also compare our algorithms against a heuristic baseline that we call DISTRIBUTED-GREEDY.
204	21	This baseline will first select ` elements using the greedy distributed framework introduced by Mirzasoleiman et al. (2013), and then greedily optimize each fi over these ` elements.
207	14	In Figure 4(b), we graph the average distance from each customer to his closest driver, which we will refer to as the cost.
211	11	Overall, we see that while all three algorithms have very comparable costs, DISTRIBUTED-FAST is significantly faster than the others.
213	11	, fm, in this experiment we also evaluate the utility of our summary on new functions drawn from the same distribution.
214	10	That is, using the regions shown in Figure 4(a), each algorithm will select a subset S of potential waiting locations.
215	15	Using only these reduced subsets, we then greedily select k waiting locations for each of the twenty new regions shown in 4(d).
219	34	Note that 4(f) is demonstrating how long each algorithm takes to construct a size ` summary, not how long it is taking to optimize over this summary.
220	20	To satisfy the need for scalable data summarization algorithms, this paper focused on the two-stage submodular maximization framework and provided the first streaming and distributed solutions to this problem.
