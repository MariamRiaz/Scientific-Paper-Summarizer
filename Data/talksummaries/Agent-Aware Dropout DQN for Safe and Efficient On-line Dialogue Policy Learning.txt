3	17	At the early research, the spoken dialogue systems assume observable dialogue states.
9	28	A well-founded theory for this is the partially observable Markov decision process (POMDP) (Kaelbling et al., 1998), which can provide robustness to errors from the input module and automatic policy optimization by reinforcement learning.
10	28	Most POMDP based policy learning research is usually carried out using either user simulator or employed users (Williams and Young, 2007; Young et al., 2010).
13	70	Recently, Chen et al. (2017) proposed two qualitative metrics 1 to measure on-line policy learning: safety and efficiency.
17	9	In light of above, Chen et al. (2017) proposed a safe and efficient on-line policy optimization framework, i.e. companion teaching (CT), in which a human teacher is added in the classic POMDP.
21	27	Based on CT, companion learning (CL) framework is proposed to integrate rule-based policy and RL-based policy, resulting in safe and efficient on-line policy learning.
23	15	There are a few differences between these two kinds of teachers.
58	8	The action from control module is then transmitted to the output module, which generates the nature text and audio.
61	8	In the CL framework, there are two things that matter: one is when to consult the teacher, another is how to use the teacher’s experiences.
105	15	To obtain the uncertainty, similar with that at train phrase the dropout is enabled at test phrase.
109	11	Instead, we proposed a novel method to measure the certainty of the decision of student policy at t-th turn.
112	30	The action astut that should be taken in the belief state bt is the one with the largest percentage of the votes, and the corresponding percentage is defined as certainty ct.
115	39	2: for i = 1, N do 3: qi← DropoutQNetwork(bt) 4: ati ← arg maxj qij 5: p[ati]← p[ati] + 1/N 6: end for 7: ct ← maxj pj 8: astut ← arg maxj pj 9: return astut , ct At the end of e-th dialogue, the average certainty of all turns is computed, i.e. Ce = 1 Te ∑Te t=0 ct, where Te is the number of turns in e-th dialogue.
119	26	If Ce in all successive W dialogues are greater then a threshold Cth as shown in Figure 2, it’s assumed that the student reaches a point where it is confident enough with its own decision steadily.
133	15	Here, we use the ordered propositional logic rules, which can be easily translated into IF-THEN rules.
139	10	32: end for 33: end for 34: return θ Algorithm 3 Companion Function Companion(C) Require: The average certainty memory C at e-th dialogue and the moving window size W .
159	19	Our experiments have three objectives: (1) Comparing our proposed dropout DQN in Algorithm 1 with some baselines when there is no teacher.
161	19	(3) Visually analyzing the differences in behaviors between the rule-based teacher policy and the optimized student policy.
163	17	The purpose of the user’s interacting with SDS is to find restaurant information in the Cambridge (UK) area (Henderson and Thomson, 2014).
168	29	At the end of the dialogue, a reward of +1 is given for dialogue success.
172	25	In this section, four policies without teaching are compared: • DQN: A vanilla deep Q-Network (Mnih et al., 2015) which has two hidden layers, each with 128 nodes.
180	15	However, Dropout DQN 1 seems to suffer premature and sub-optimal convergence, while our proposed Dropout DQN 32, whose decision is based on multi votes (algorithm 1), can result in improvement of efficiency and better final performance.
183	28	In this section, four methods of teaching by the rule-based policy are compared: • EA: 500 dialogues are taught with EA at the beginning (Chen et al., 2017).
185	8	These examples are used to pre-train the actor network with supervised learning.
189	16	As can be seen in Figure 4, there is a big dip in the performance of A2C PreTrain.
193	9	Moreover, except for safety, CL AAD can boost the efficiency, which benefits from the agent-aware experience replay.
212	13	This paper has proposed a companion learning framework to unify rule-based policy and RLbased policy.
214	74	Based on the uncertainty estimated using a dropout Q-Network, a companion strategy is proposed to control when the student policy directly consults rules and how often the student policy learns from the teacher’s experiences.
215	90	Simulation experiments showed that our proposed framework can significantly improve both safety and efficiency of on-line policy optimization.
216	59	Additionally, we visually analyzed the differences in behaviors between the rule-based teacher policy and the optimized student policy, which gave us some inspirations to refine the rules.
