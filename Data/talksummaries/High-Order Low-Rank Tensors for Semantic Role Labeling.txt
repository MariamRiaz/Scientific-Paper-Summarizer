1	27	For instance, the top performing system on the CoNLL–2009 shared task employs over 50 language-specific templates for feature generation (Che et al., 2009).
3	19	However, finding compact, informative templates is difficult since the relevant signal may be spread over many correlated features.
23	19	As a baseline, we use a simple SRL model that relies on a minimal set of standard features.
56	27	For each predicate pi (e.g., “holding”), our goal is to predict tuples (pi, aij , rij) specifying the semantic dependency arcs, where aij ∈ x is one argument (e.g., “meetings”), and rij is the corresponding semantic role label (e.g., A1).
66	45	For the arc-factored model, there are mainly four types of atomic information that define the arc features in φ(p, a, r): (a) the predicate token p (and its local context); (b) the argument token a (and its local context); (c) the dependency label path that connects p and a in the syntactic tree; (d) the semantic role label r of the arc.
71	81	We characterize each semantic arc (p, a, r) using the cross-product of atomic feature vectors associated with the four types of information described above: the predicate vector φ(p), the argument vector φ(a), the dependency path vector φ(path) and the semantic role label vector φ(r).
81	46	(3) Here k is a small constant, P,Q ∈ Rk×n, R ∈ Rk×m and S ∈ Rk×l are parameter matrices, and P (i) (and similarly Q(i), R(i) and S(i)) represents the i-th row vector of matrix P .
95	29	That is, for the given sentence x̂ and the corresponding syntactic tree ˆysyn, we adjust parameter values to separate gold semantic parse and other incorrect alternatives: ∀zsem ∈ Z(x̂, ˆysyn) : Ssem(x̂, ˆysyn, ˆzsem) ≥ Ssem(x̂, ˆysyn, zsem) + cost( ˆzsem, zsem) (4) where Z(x̂, ˆysyn) represent the set of all possible semantic parses, and cost( ˆzsem, zsem) is a non-negative function representing the structural difference between ˆzsem and zsem.
98	17	The parameters are updated successively after each training sentence.
102	29	Since this loss function is neither linear nor convex with respect to the parameters θ (more precisely the low-rank component matrices P , Q, R and S), we can use the same alternating passive-aggressive (PA) update strategy in our previous work (Lei et al., 2014) to update one parameter matrix at one time while fixing the other matrices.
106	16	Although our scoring function Ssem(·) is not linear, we can simply approximate it with its first-order Taylor expansion: S(x,y, z; θ + ∆θ) ≈ S(x,y, z; θ) + dS dθ ·∆θ In fact, by plugging this into the hinge loss function and the quadratic optimization problem, we get a joint closed-form update which can be simply described as, ∆θ = max { C, loss(θ) ‖gθ‖2 } gθ where gθ = dS dθ (x̂, ˆysyn, ˆzsem)− dS dθ (x̂, ˆysyn, ˜zsem), and C is a regularization hyper-parameter controlling the maximum step size of each update.
110	41	Since the scoring and loss function with high-order tensor components is highly non-convex, our model performance can be impacted by the initialization of the matrices P , Q, R and S. In addition to intializing these low-rank components randomly, we also experiment with a strategy to provide a good guess of the low-rank tensor.
111	27	First, note that the traditional manually-selected feature set (i.e., φ(p, a, r) in our notation) is an expressive and informative subset of the huge feature expansion covered in the feature tensor.
112	18	We can train our model using only the manual feature set and then use the corresponding feature weights w to intialize the tensor.
114	40	We then try to find a low-rank approximation of sparse tensor T by approximately minimizing the squared error: min P,Q,R,S ‖T − ∑ i P (i)⊗Q(i)⊗R(i)⊗ S(i)‖22 In the low-rank dependency parsing work (Lei et al., 2014), this is achieved by unfolding the sparse tensor T into a n× nml matrix and taking the SVD to get the top low-rank components.
115	24	Unfortunately this strategy does not apply in our case (and other high-order tensor cases) because even the number of columns in the unfolded matrix is huge, nml > 1011, and simply taking the SVD would fail because of memory limits.
117	34	power iteration (De Lathauwer et al., 1995), to incrementally obtain the most important rank-1 component one-by-one – P (i), Q(i), R(i) and S(i) for each i = 1..k. This method is a very simple iterative algorithm and is used to find the largest eigenvalues and eigenvectors (or singular values and vectors in SVD case) of a matrix.
131	15	In addition, a subset of the Brown corpus is used as the secondary out-of-domain test set, in order to evaluate how well the model generalizes to a different domain.
154	17	We single out performance on English corpora because these datasets are most commonly used for system evalutation.
159	32	Table 4 shows the results of our system on other languages in the CoNLL-2009 shared task.
160	50	Out of five languages, our model rivals the best performing system on three languages, achieving statistically significant gains on English and Chinese.
161	15	Note that our model uses the same feature configuration for all the languages.
163	24	Results in Table 3 and 4 also highlight the contribution of the tensor to the model performance, which is consistent across languages.
164	40	Without the tensor component, our system trails the top two performing systems.
166	49	The mode of the tensor also contributes to the performance – the 4-way tensor model performs better than the 3-way counterpart, demonstrating the importance of modeling the interactions between dependency paths and semantic role labels.
168	18	The initialization based on the power method yields superior results compared to random initialization, for both 3-way and 4-way tensors.
172	32	For instance, the highest gain achieved by Roth and Woodsend (2014) when the embeddings of the arguments are averaged is 0.5%, compared to 1.6% obtained by our model.
173	36	In this paper we introduce a tensor-based approach to SRL that induces a compact feature representation for words and their relations.
175	83	Augmenting a simple, yet competitive SRL model with the tensor component yields significant performance gains.
176	79	We demonstrate that our full model outperforms the best performing systems on the CoNLL-2009 shared task.
