{"sections": [{"text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 162\u2013171 Brussels, Belgium, October 31 - November 4, 2018. c\u00a92018 Association for Computational Linguistics\n162\nis proposed to temporally capture the evolving fine-grained frame-by-word interactions between video and sentence. TGN sequentially scores a set of temporal candidates ended at each frame based on the exploited frameby-word interactions, and finally grounds the segment corresponding to the sentence. Unlike traditional methods treating the overlapping segments separately in a sliding window fashion, TGN aggregates the historical information and generates the final grounding result in one single pass. We extensively evaluate our proposed TGN on three public datasets with significant improvements over the stateof-the-arts. We further show the consistent effectiveness and efficiency of TGN through an ablation study and a runtime test."}, {"heading": "1 Introduction", "text": "We examine the task of Natural Sentence Grounding in Video (NSGV). Given an untrimmed video and a natural sentence, the goal is to determine the start and end timestamps of the segment in the video which corresponds to the given sentence, as shown in Figure 1 (a). Comparing with the other video researches, such as bidirectional video-sentence retrieval (Xu et al., 2015b), video attractiveness prediction (Chen et al., 2018, 2016), and video captioning (Pasunuru and Bansal, 2017; Wang et al., 2018a,b), NSGV needs to model not only the characteristics of sentence and video but also the fine-grained interactions between the two modalities, which is even more challenging.\n\u2217 Work done while Jingyuan Chen and Xinpeng Chen were Research Interns with Tencent AI Lab.\n1 The project homepage is https:// jingyuanchen.github.io/archive/tgn.html.\nRecently, several related works (Gao et al., 2017; Hendricks et al., 2017) leverage one temporal sliding window approach over video sequences to generate video segment candidates, which are then independently combined (Gao et al., 2017) or compared (Hendricks et al., 2017) with the given sentence to make the grounding prediction. Although the existing works have achieved promising performances, they are still suffering from inferior effectiveness and efficiency. First, existing methods project the video segment and sentence into one common space, as shown in Figure 1 (b),\nwhere the two generated embedding vectors are used to perform the matching between video segment and sentence. Such a matching is only performed in the global segment and sentence level and thus not expressive enough, which ignores the fine-grained matching relations between video frames and the words in sentences. Second, in order to handle the diverse temporal scales and locations of the candidate segments, exhaustive matching between the large amount of overlapping segments and the sentence is required. As such, the sliding window methods are very computationally expensive.\nIn order to tackle the above two limitations, we introduce a novel Temporal GroundNet (TGN) model, the first dynamic single-stream deep architecture for the NSGV task that takes full advantage of fine-grained interactions between video frames and words in a sentence, as shown in Figure 1 (c). TGN sequentially processes video frames, where at each time step we rely on a novel multimodal interactor to exploit the evolving fine-grained frameby-word interactions. Then, TGN works on the yielded interaction status to simultaneously score a set of temporal candidates of multiple scales and finally localize the video segment that corresponds to the sentence. More importantly, our proposed TGN is able to analyze an untrimmed video frame by frame without resorting to handling overlapping temporal video segments."}, {"heading": "2 Related Work", "text": ""}, {"heading": "2.1 Grounding Natural Language in Image", "text": "Grounding natural language in image is also known as natural language object retrieval. The task is to localize an image region described by natural language, which involves comprehending and modeling different spatial contexts, such as spatial configurations (Hu et al., 2016), attributes (Yu et al., 2018), and relationships between objects (Hu et al., 2017). Specifically, the task is usually formulated as a ranking problem over a set of candidate regions in a given image, where candidate spatial locations come from region proposal methods (Uijlings et al., 2013; Jie et al., 2016b,a; Ren et al., 2017) such as EdgeBox (Zitnick and Dolla\u0301r, 2014). Earlier studies (Mao et al., 2016; Rohrbach et al., 2016) score the generated candidate regions according to their appearances and spatial features along with features of the entire image. However, these meth-\nods fail to incorporate the interactions between objects, because the scoring process of each region proposal is isolated. More recent studies (Hu et al., 2017; Nagaraja et al., 2016) improve the performance with the aid of modeling relationships between objects."}, {"heading": "2.2 Grounding Natural Language in Video", "text": "Analogous to spatial grounding in image, this work studies a similar problem\u2014temporal natural language grounding in video. Earlier works (Yu and Siskind, 2013; Lin et al., 2014) learn the semantics of sentences, which are then matched to visual concepts via exploiting object appearance, motion and spatial relationships. However, they are limited to a small set of objects. Recently, larger datasets (Gao et al., 2017; Hendricks et al., 2017) are constructed to support more flexible groundings. The methods proposed in (Gao et al., 2017; Hendricks et al., 2017) learn a common embedding space shared by video segment features and sentence representations, in which their similarities are measured. Specifically, moment context network (MCN) (Hendricks et al., 2017) learns a shared embedding for video clip-level features and language features. The video features integrate local video features, global features, and temporal endpoint features. Cross-modal temporal regression localizer (CTRL) (Gao et al., 2017) contains four modules, specifically a visual encoder extracting clip-level features with context, a sentence encoder yielding its embedding through LSTM, a multimodal processing network generating the fused representations via element-wise operations, and a temporal regression network producing the alignment scores and location offsets. One limitation of those common space matching methods is that the video segment generation process is computationally expensive, as they carry out overlapping sliding window matching (Gao et al., 2017) or exhaustive search (Hendricks et al., 2017). Another weakness is that they exploit the relationships between textual and visual modalities by conducting a simple concatenation (Gao et al., 2017) or measuring a squared distance loss (Hendricks et al., 2017), which ignores the evolving fine-grained video-sentence interactions. In this paper, a novel model TGN is proposed to deal with the aforementioned limitations for the task of natural sentence grounding in video."}, {"heading": "3 Approach", "text": "Given a long and untrimmed video sequence V and a natural sentence S, the NSGV task is to localize a video segment Vs = {ft}tet=tb from V , beginning at tb and ending at te, which corresponds to and expresses the same semantic meaning as the given sentence S. In order to perform the grounding, each video is represented as V = {ft}Tt=1, where T is the total number of frames and ft denotes the feature representation of the t-th video frame. Similarly, each sentence is represented as S = {wn}Nn=1, where wn is the embedding vector of the n-th word in the sentence andN denotes the total number of words.\nWe propose a novel model, namely Temporal GroundNet (TGN), to tackle the NSGV problem. As illustrated in Figure 2, TGN consists of three modules. 1) Encoder: visual and textual encoders are used to compose the video frame representations and word embeddings, respectively. 2) Interactor: a multimodal interactor learns the frame-byword interactions between the video and sentence. 3) Grounder: a grounder generates the temporal localization in one single pass. Please note that these three modules are fully coupled together, which can thus be trained in an end-to-end fashion."}, {"heading": "3.1 Encoder", "text": "With the obtained video frame features V = {ft}Tt=1 and word embeddings of the sentence S = {wn}Nn=1, we employ two long shortterm memory networks (LSTMs) (Hochreiter and Schmidhuber, 1997) to sequentially process the two different modalities, i.e., video and sentence, independently. Specifically, one LSTM sequentially models the video V , yielding the hidden states {hvt }Tt=1, while the other LSTM processes the sequential words in the sentence S, resulting in its corresponding hidden states {hsn}Nn=1. Owing to natural behaviors and characteristics of LSTMs, both {hvt }Tt=1 and {hsn}Nn=1 can encode and aggregate the contextual evidences (Wang and Jiang, 2016b) from the sequential video frame representations and word embeddings of the sentence, respectively, meanwhile casting aside the irrelevant information."}, {"heading": "3.2 Interactor", "text": "Based on the hidden states of the video and sentence yielded from the leveraged encoders, we de-\nsign a multimodal interactor to perform the frameby-word interactions between the video and sentence. First, the frame-specific sentence feature is generated through summarizing the sentence hidden states by considering their relationships with the specific video frame at each time step. Afterwards, an interaction LSTM, dubbed i-LSTM, is performed to aggregate frame-by-word interactions."}, {"heading": "3.2.1 Frame-Specific Sentence Feature", "text": "Directly operating on the clip-level and sentencelevel features generated by the encoders cannot well exploit the frame-by-word relationships between video and sentence that evolve over time. Inspired by (Wang and Jiang, 2016a; Feng et al., 2018), we introduce one novel frame-specific sentence feature, which adaptively summarizes the hidden states of the sentence {hsn}Nn=1 with respect to the t-th video frame:\nHst = N\u2211\nn=1\n\u03b1nt h s n, (1)\nwhere Hst denotes the summarized sentence representation specified by the t-th video frame. At each time step t, we utilize the hidden state hvt to\nselectively attend the words and summarize them accordingly. The attention weight \u03b1nt encodes the degree to which the n-th word in the sentence is aligned with the t-th video frame. As the processing of video frames proceeds, the attention weights dynamically change regarding to the current video frame. As such, the generated framespecific sentence features {Hst}Tt=1 consider the frame-by-word relationships between all the video frames and all the words in the sentence.\nAs the generation of frame-specific sentence feature is deeply coupled with the following interaction LSTM, we will explain the calculation of the attention weight \u03b1nt later.\n3.2.2 Interaction LSTM (i-LSTM) In order to accurately ground the sentence in a video, the multimodal interation behaviors between the video and sentence need to be comprehensively modeled. Previous approaches on multimodal interactions were limited to concatenation (Zhu et al., 2016), element-wise product or sum (Gao et al., 2017), and bilinear pooling (Fukui et al., 2016). These methods are not expressive enough since they ignore the evolving fine-grained interactions across video and sentence, particularly the frame-by-word interactions. In this paper, we propose a novel multimodal interaction model, which is realized by LSTM. We term it interaction LSTM (i-LSTM), which sequentially processes the video sequence frame by frame, holding deep interactions with the words in the sentence.\nIn order to well capture the complicated temporal interactions between the video and sentence, at each time step t, the input of the i-LSTM is formed by concatenating the t-th video hidden state hvt and the t-th frame-specific sentence feature Hst as: rt = h v t \u2016 Hst . rt is then fed into the i-LSTM unit to yield the t-th intermediate interaction status between the video and sentence:\nhrt = i-LSTM(rt,h r t\u22121), (2)\nwhere hrt is the yielded hidden state, encoding the fine-grained interactions between the word and video frame. hrt will be further used to perform the grounding process. Due to the inherent properties and characteristics of LSTMs, important cues regarding to grounding up to the current stage will be \u201cremembered\u201d, while non-essential ones will be \u201cforgotten\u201d.\nNow we go back to the generation of attention weight \u03b1nt in Eq. (1), based on the obtained vi-\nsual hidden states hvt and textual hidden state h s n as well as the yielded interaction status hrt\u22121 in the previous step. The widely used soft-attention mechanism (Xu et al., 2015a; Chen et al., 2017) is used to generate the attention weights in a frameby-word manner. As aforementioned, the i-LSTM models the evolving frame-by-word interactions between the sentence and video. Therefore, the attention weight between the n-th word hsn and the t-th video frame hvt is determined by not only the content of the video and sentence but also their interaction status. Thus, we design one network to compute the relevance score of one video frame with respect to each word:\n\u03b2nt = w \u1d40 tanh(WShsn+W V hvt +W Rhrt\u22121+b)+c, (3)\nwhere vector w, matrices W\u2217, bias vector b, and bias c are the network parameters to be learned. hrt\u22121 is the hidden state of the i-LSTM at t \u2212 1 time step. The final word-level attention weights are obtained by:\n\u03b1nt = exp(\u03b2nt )\u2211N j=1 exp(\u03b2 j t ) . (4)\nThe obtained attention weight \u03b1nt is thereafter to generate the frame-specific sentence feature as in Eq. (1)."}, {"heading": "3.3 Grounder", "text": "In this section, we introduce the grounder, which works on the yielded interaction status hrt from i-LSTM, to localize the video segment that corresponds to the sentence. Our proposed grounder works in one single pass without introducing overlapping sliding windows, which thus results in a fast runtime. As shown in Figure 2, at each time step t, the grounder efficiently scores a set of K grounding candidates by considering multiple time scales (Buch et al., 2017) that end at time step t. Specifically, we use different K for different datasets, which is determined by the distribution of the lengths of all ground-truth groundings in a certain dataset. To simplify the following discussions, the lengths of K time scales are assumed to be an arithmetic sequence with the common difference \u03b4 and all the temporal candidates are sorted by increasing lengths. In other words, the length of the k-th candidate is k\u03b4. Note that all grounding candidates considered at time t have a fixed ending boundary.\nSpecifically, at each time step t, the grounder will classify each temporal candidate in consideration as a positive grounding or a negative one with respect to the given sentence. Considering multiple time scales, the grounder will generate the confidence scores Ct = (c1t , c 2 t , ..., c K t ) that correspond to the set of K visual grounding candidates, all ending at time step t. The hidden state hrt generated by i-LSTM at time t, representing the interaction status between the sentence and video sequence up to the current position, is naturally suited to yield the confidence scores for the different time scales ending at time step t. In this paper, the confidence scores, indicating the sentence grounding, are generated by a fullyconnected layer with sigmoid nonlinearity:\nCt = \u03c3(W Khrt + b r t ), (5)\nwhere WK and brt are the corresponding parameters, and \u03c3 denotes the nonlinear sigmoid function."}, {"heading": "3.4 Training", "text": "The training samples collected in X for NSGV are video-sentence pairs. Specifically, each video V is temporally associated with a set of sentence annotations: A = {(Si, tbi , tei )}Mi=1, where M is the number of annotated sentences of the video, and Si is a sentence description of a video clip, with tbi and t e i indicating the beginning and ending time in the video. Each training sample corresponds to a ground-truth matrix y \u2208 RT\u00d7K with binary entries. We use ykt to denote the (t, k)-th entry of the ground-truth matrix. ykt is interpreted as whether the k-th grounding candidate at time step t corresponds to the given natural sentence. Concretely, the entry ykt is set as 1, indicating that the corresponding video segment (ends at time step t with length k\u03b4) has a temporal Intersection-over-Union (IoU) with (tb, te) larger than a threshold \u03b8. Otherwise ykt is set as 0.\nFor a training pair (V, S) \u2208 X , the objective at time step t is given by a weighted binary cross entropy loss L(t, V, S):\n\u2212 K\u2211 k=1 wk0y k t log c k t +w k 1(1\u2212ykt ) log(1\u2212 ckt ), (6)\nwhere the weights wk0 and w k 1 are calculated according to the frequencies of positive and negative samples in the training set with length k\u03b4. ykt is the ground-truth value and ckt denotes the prediction results by our proposed model.\nOur TGN backpropagates at every time step t to learn all the parameters of the fully-coupled three modules: encoder, interactor, and grounder. The objective of all training video-sentence pairs X is defined as:\nLX = \u2211\n(V,S)\u2208X T\u2211 t=1 L(t, V, S). (7)"}, {"heading": "3.5 Inference", "text": "During the inference stage, given a testing video V and a sentence S, the textual and visual encoders first generate hidden states for each word and video frame, respectively. Then, the interactor sequentially goes through the video frame by frame to yield the frame-by-word interaction status. At each position t, a K-dimensional score vector Ct is generated by the grounder. Therefore, after processing the last frame in the video, a T \u00d7 K score matrix is obtained for the whole video, with the (t, k)-th entry in the matrix indicating the probability that the video segment ended at position t with length k\u03b4 in video V corresponds to sentence S. Eventually, the evaluation is reduced to a ranking problem over all the grounding candidates based on the generated scores."}, {"heading": "4 Experiments", "text": "In this section, we evaluate the effectiveness of our proposed TGN on the NSGV task. We begin by describing the datasets used for evaluation, followed by the introduction of the experimental settings including the baselines, configurations, as well as the evaluation metrics. Afterwards, we demonstrate the effectiveness of TGN by comparing with the state-of-the-art approaches and efficiency through a runtime test."}, {"heading": "4.1 Datasets", "text": "We experiment on three publicly accessible datasets: DiDeMo (Hendricks et al., 2017), TACoS (Regneri et al., 2013), and ActivityNet Captions (Fabian Caba Heilbron and Niebles, 2015). These datasets consist of videos as well as their associated temporally annotated sentences. DiDeMo2 consists of 10464 25-50 second long videos. The same split provided by (Hendricks et al., 2017) is used for a fair comparison, with 33008, 4180, and 4022 video-sentence pairs for training, validation, and testing, respectively.\n2https://goo.gl/JpbAhg.\nTACoS3 consists of 127 videos selected from the MPII Cooking Composite Activities video corpus (Rohrbach et al., 2012). The same split as in (Gao et al., 2017) is used, consisting of 10146, 4589, and 4083 video-sentence pairs for training, validation, and testing, respectively. ActivityNet Captions4 consists of 19, 209 videos amounting to 849 hours. The public split is used for our experiments, which has 37421, 17505, and 17031 video-sentence pairs for training, validation, and testing, respectively."}, {"heading": "4.2 Experimental Settings", "text": ""}, {"heading": "4.2.1 Baselines", "text": "We compare our proposed TGN against the following two state-of-the-art models, specifically, the MCN (Hendricks et al., 2017), CTRL (Gao et al., 2017), visual-semantic alignment with LSTM (VSA-RNN) (Karpathy and Li, 2015), and visual-semantic alignment with skip thought vector (VSA-STV) (Kiros et al., 2015). For fair comparisons, we compare the results of MCN on DiDeMo and the results of CTRL, VSA-RNN, VSA-STV on TACoS reported in their papers."}, {"heading": "4.2.2 Evaluation Metrics", "text": "A grounding of one natural sentence in a video is considered as \u201ccorrect\u201d if its temporal IoU with the ground-truth boundary is above a threshold \u03b8. To be consistent with the baselines, we adopt R@N , IoU=\u03b8, and mean IoU (mIoU) as our evaluation metrics. R@N , IoU=\u03b8 represents the percentage of testing samples which have at least one of the top-N results with IoU larger than \u03b8. mIoU means the average IoU over all testing samples."}, {"heading": "4.2.3 Configurations", "text": "Generally, the video frame features are usually extracted with a time resolution. For the videos in DiDeMo and TACoS, we sample every 5 second as done by (Hendricks et al., 2017). As the videos in DiDeMo are 25-30 second long, the video feature length is reduced to 6. For videos in ActivityNet Captions, we sample every second. To extract visual features, we consider both appearance and optical flow features. Specifically, we study four widely-used visual features: VGG16 (Simonyan and Zisserman, 2014), C3D (Tran et al., 2015), Inception-V4 (Szegedy et al., 2017), and optical flow (Wang et al., 2016). Please note that when\n3https://goo.gl/ajmsva. 4https://goo.gl/N355bG.\ncomparing with specific baseline methods, we use the same features as baseline methods, specifically, VGG16 and optical flow for MCN and C3D for CTRL, VSA-RNN, and VSA-STV.\nFor sentences, we tokenize each sentence by Stanford CoreNLP (Manning et al., 2014) and use the 300-D word embeddings from GloVe (Pennington et al., 2014) to initialize the models. The words not found in GloVe are initialized as zero vectors. The hidden state dimensions of all LSTMs (including the video, sentence, and interaction LSTMs) are set as 512. We use the Adam (Kingma and Ba, 2014) optimizer with \u03b21 = 0.5 and \u03b22 = 0.999. The initial learning rate is set to 0.001. We train the network for 200 iterations, and the learning rate is gradually decayed over time. The mini-batch size is set to 64."}, {"heading": "4.3 Experimental Results and Analysis", "text": ""}, {"heading": "4.3.1 Comparisons with State-of-the-Arts", "text": "Experiments on DiDeMo. Table 1 illustrates the performance comparisons on the DiDeMo dataset. In addition to MCN, we also compare with the baseline Moment Frequency Prior (MFP) in (Hendricks et al., 2017), which selects segments corresponding to the positions of videos in the training dataset with most annotations. First, TGN with different features can significantly outperforms the \u201cprior baseline\u201d MFP, which retrieves segments corresponding to the most common start and end points in the dataset. Second, it can be observed that with the same visual features, specifically VGG16 and optical flow, TGN significantly outperforms MCN. And the performance of TGN with optical flow is better than that with VGG16. One possible reason is that the videos in DiDeMo are relatively short, which only contain a single event. In such a case, the action information plays\na more critical role. This finding is also consistent with (Hendricks et al., 2017). By fusing the results obtained by VGG16 and optical flow together, the performance can be further boosted, as demonstrated by TGN-Fusion and MCN-Fusion. Third, MCN introduces the temporal endpoint feature (TEF) as prior knowledge, which indicates when a segment occurs in a video. With TEF, the performance of MCN can be significantly improved. However, it is still inferior to our proposed TGN.\nMCN is designed as an enumeration-based approach. Each video in the DiDeMo dataset is split into six five-second chunks which are considered as the time unit for localization. Therefore, in total there are only C27 = 7 \u00d7 6/2 = 21 different ways of localization for DiDeMo videos. Therefore, although MCN can be effectively applied to videos with several chunks due to the small search space, it is not practical for untrimmed long videos. In the Section 4.3.3, we will evaluate and compare the efficiencies of MCN, CTRL, and our proposed TGN.\nExperiments on TACoS. Table 2 illustrates the experimental results on TACoS. First, it can be observed that CTRL performs much better than VSA-RNN and VSA-STV. The reasons lie in twofold (Gao et al., 2017). On one hand, CTRL utilizes a multilayer alignment network to learn better alignment. On the other hand, VSA-RNN and VSA-STV do not encode temporal context information of video. Second, with the same visual feature, specifically C3D, TGN-C3D significantly outperforms CTRL-C3D. This is due to the fact that TGN exploits not only the contextual information but also the fine-grained interaction behaviors. More concretely, TGN considers the frameby-word correlations by introducing an attentive combinations of the words in the sentence, where each weight encodes the degree to which the word is aligned with each specific frame. This mechanism is beneficial to capturing the informative se-\nmantics in the sentences for alignment.\nExperiments on ActivityNet Captions. Besides the two benchmarks, we also evaluate our model on the ActivityNet Captions dataset. Different CNNs are used to encode video visual information. Specifically, we consider VGG16, C3D, and Inception-V4. The results are included in Table 3. First, our proposed TGN can perform effectively on long untrimmed videos. Second, Inception-V4 performs generally better than VGG16 and C3D, which is consistent with the finding in (Canziani et al., 2016). Therefore, more powerful visual representations of video features will undoubtedly improve the the performance of our proposed TGN on the NSGV task.\nSome qualitative results of our proposed TGN on ActiveityNet Captions dataset is illustrated in Figure 3. It can be observed that with different visual features, different grounding results are obtained. For the first and second examples, TGN with VGG16 and Inception-V4 generates more accurate groundings than that with C3D, while TGN with C3D yields more accurate grounding results for the third example. More specifically, our proposed TGN with VGG16 and Inception-V4 can well identify the visual information related with the sentence, i.e. \u201cA man in a red shirt claps his hands\u201d."}, {"heading": "4.3.2 Effect of Frame-by-Word Attention", "text": "We examine the effect of the frame-by-word attention in interactor. We ablate TGN into two other methods. 1) NA: There is no attention layer in this model. After obtaining the sequential hidden states of the sentence, mean pooling is used to generate the representation for the whole sentence.\nThen the generated representation is concatenated with video representation, based on which the scores for multiple grounding candidates are predicted. 2) NM: The idea of generating framespecific sentence feature is still reserved in the NM model. The difference between NM and TGN is that there is no interaction LSTM in NM. Specifically, when calculating the attention weight for each word as in Eq. (3), the hidden state hrt\u22121 indicating the interaction status is not incorporated.\nThe quantitative results are displayed in Table 4. First, when the attention mechanism is applied (NM), the performance is improved as compared with utilizing mean pooling (NA) for sentence features. The better performance demonstrates that our assumption about the evolving frame-by-word correlations between two modalities is reasonable. This also indicates that it is necessary to discriminate the contribution of each word in a sentence\nto perform the NSGV task. Second, utilizing the interaction LSTM module (TGN) achieves better performance than simply concatenating the video representation and the attentive sentence representation (NM). This result indicates that the interaction LSTM yields better interaction status between these two modalities, which can thereby benefit the final grounding.\nWe provide some qualitative examples in Figure 4 for a better understanding of the frame-byword attention. Meanwhile, the grounding results yielded by TGN-Fusion (considering both VGG16 and optical flow) are also illustrated. This experiment is designed to verify whether the frameby-word attention mechanism in interactor is useful to highlight the representative concepts in the sentence. The attention weights \u03b1 for two testing samples in DiDeMo are illustrated in Figure 4, where the darker the color is, the larger\nthe attention weight is. It can be observed that some words well match the frames. For example, in Figure 4 (a), the concept \u201cforest\u201d appears across all the video frames presenting an evenly distributed attention weights, while the other concept \u201cwaterfall\u201d only presents in the first two frames. In addition to nouns, the adjective \u201cblue\u201d in Figure 4 (b) also receives relatively higher attention weights in relevant frames. Lastly, for stop words like \u201ca\u201d, \u201cthe\u201d and \u201cin\u201d, their attention weights, which are very small, also present an even distribution."}, {"heading": "4.3.3 Efficiency", "text": "We evaluate the efficiency of our proposed TGN, by comparing its runtime with MCN and CTRL on a Tesla M40 GPU. The efficiency is measured by frames per second (FPS) as shown in Table 5. Please not that the feature extraction time is excluded. It can be observed that our TGN model achieves much faster processing speeds, with 1,363 fps vs. 562 and 286 for CTRL and MCN, respectively. The reason mainly attributes to that the proposed TGN only process each video in one single pass without processing overlapped sliding windows."}, {"heading": "5 Conclusion", "text": "In this paper, we focused on the task of natural sentence grounding in video that is believed to offer a comprehensive understanding of bridging computer vision and natural language processing. Towards this task, we proposed an end-to-end Temporal GroundNet (TGN) by incorporating the evolving fine-grained frame-by-word interactions across video-sentence modalities to generate a visual grounding tailored to each given natural sentence. Moreover, TGN performs efficiently, which only needs to process the video sequence in one single pass. Extensive experiments on three realworld datasets clearly demonstrate the effectiveness and efficiency of the proposed TGN."}], "year": 2018, "references": [{"title": "Sst: Singlestream temporal action proposals", "authors": ["Shyamal Buch", "Victor Escorcia", "Chuanqi Shen", "Bernard Ghanem", "Juan Carlos Niebles."], "venue": "CVPR, pages 5534\u2013 5542.", "year": 2017}, {"title": "An analysis of deep neural network models for practical applications", "authors": ["Alfredo Canziani", "Adam Paszke", "Eugenio Culurciello."], "venue": "CoRR, abs/1605.07678.", "year": 2016}, {"title": "Micro tells macro: Predicting the popularity of micro-videos via a transductive model", "authors": ["Jingyuan Chen", "Xuemeng Song", "Liqiang Nie", "Xiang Wang", "Hanwang Zhang", "Tat-Seng Chua."], "venue": "MM, pages 898\u2013907.", "year": 2016}, {"title": "Attentive collaborative filtering: Multimedia recommendation with item- and component-level attention", "authors": ["Jingyuan Chen", "Hanwang Zhang", "Xiangnan He", "Liqiang Nie", "Wei Liu", "Tat-Seng Chua."], "venue": "SIGIR, pages 335\u2013344.", "year": 2017}, {"title": "Fine-grained video attractiveness prediction using multimodal deep learning on a large real-world dataset", "authors": ["Xinpeng Chen", "Jingyuan Chen", "Lin Ma", "Jian Yao", "Wei Liu", "Jiebo Luo", "Tong Zhang."], "venue": "WWW.", "year": 2018}, {"title": "Activitynet: A large-scale video benchmark for human activity understanding", "authors": ["Bernard Ghanem Fabian Caba Heilbron", "Victor Escorcia", "Juan Carlos Niebles."], "venue": "CVPR, pages 961\u2013970.", "year": 2015}, {"title": "Video re-localization", "authors": ["Yang Feng", "Lin Ma", "Wei Liu", "Tong Zhang", "Jiebo Luo."], "venue": "ECCV.", "year": 2018}, {"title": "Multimodal compact bilinear pooling for visual question answering and visual grounding", "authors": ["Akira Fukui", "Dong Huk Park", "Daylen Yang", "Anna Rohrbach", "Trevor Darrell", "Marcus Rohrbach."], "venue": "EMNLP, pages 457\u2013468.", "year": 2016}, {"title": "TALL: temporal activity localization via language query", "authors": ["Jiyang Gao", "Chen Sun", "Zhenheng Yang", "Ram Nevatia."], "venue": "ICCV, pages 5277\u20135285.", "year": 2017}, {"title": "Localizing moments in video with natural language", "authors": ["Lisa Anne Hendricks", "Oliver Wang", "Eli Shechtman", "Josef Sivic", "Trevor Darrell", "Bryan C. Russell."], "venue": "ICCV, pages 5804\u20135813.", "year": 2017}, {"title": "Long shortterm memory", "authors": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation, 9(8):1735\u20131780.", "year": 1997}, {"title": "Modeling relationships in referential expressions with compositional modular networks", "authors": ["Ronghang Hu", "Marcus Rohrbach", "Jacob Andreas", "Trevor Darrell", "Kate Saenko."], "venue": "CVPR, pages 4418\u20134427.", "year": 2017}, {"title": "Natural language object retrieval", "authors": ["Ronghang Hu", "Huazhe Xu", "Marcus Rohrbach", "Jiashi Feng", "Kate Saenko", "Trevor Darrell."], "venue": "CVPR, pages 4555\u20134564.", "year": 2016}, {"title": "Tree-structured reinforcement learning for sequential object localization", "authors": ["Zequn Jie", "Xiaodan Liang", "Jiashi Feng", "Xiaojie Jin", "Wen Lu", "Shuicheng Yan."], "venue": "NIPS, pages 127\u2013135.", "year": 2016}, {"title": "Scale-aware pixelwise object proposal networks", "authors": ["Zequn Jie", "Xiaodan Liang", "Jiashi Feng", "Wen Feng Lu", "Francis Eng Hock Tay", "Shuicheng Yan."], "venue": "IEEE Trans. Image Processing, 25(10):4525\u20134539.", "year": 2016}, {"title": "Deep visual-semantic alignments for generating image descriptions", "authors": ["Andrej Karpathy", "Fei-Fei Li."], "venue": "CVPR, pages 3128\u20133137.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "authors": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "CoRR, abs/1412.6980.", "year": 2014}, {"title": "Skip-thought vectors", "authors": ["Ryan Kiros", "Yukun Zhu", "Ruslan Salakhutdinov", "Richard S. Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler."], "venue": "NIPS, pages 3294\u2013 3302.", "year": 2015}, {"title": "Visual semantic search: Retrieving videos via complex textual queries", "authors": ["Dahua Lin", "Sanja Fidler", "Chen Kong", "Raquel Urtasun."], "venue": "CVPR, pages 2657\u20132664.", "year": 2014}, {"title": "The stanford corenlp natural language processing toolkit", "authors": ["Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Rose Finkel", "Steven Bethard", "David McClosky."], "venue": "ACL, pages 55\u201360. The Association for Computer Linguistics.", "year": 2014}, {"title": "Generation and comprehension of unambiguous object descriptions", "authors": ["Junhua Mao", "Jonathan Huang", "Alexander Toshev", "Oana Camburu", "Alan L. Yuille", "Kevin Murphy."], "venue": "CVPR, pages 11\u201320.", "year": 2016}, {"title": "Modeling context between objects for referring expression understanding", "authors": ["Varun K. Nagaraja", "Vlad I. Morariu", "Larry S. Davis."], "venue": "ECCV, pages 792\u2013807.", "year": 2016}, {"title": "Multi-task video captioning with video and entailment generation", "authors": ["Ramakanth Pasunuru", "Mohit Bansal."], "venue": "ACL, pages 1273\u20131283.", "year": 2017}, {"title": "Glove: Global vectors for word representation", "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "EMNLP, pages 1532\u20131543.", "year": 2014}, {"title": "Grounding action descriptions in videos", "authors": ["Michaela Regneri", "Marcus Rohrbach", "Dominikus Wetzel", "Stefan Thater", "Bernt Schiele", "Manfred Pinkal."], "venue": "TACL, 1:25\u201336.", "year": 2013}, {"title": "Grounding of textual phrases in images by reconstruction", "authors": ["Anna Rohrbach", "Marcus Rohrbach", "Ronghang Hu", "Trevor Darrell", "Bernt Schiele."], "venue": "ECCV, pages 817\u2013834.", "year": 2016}, {"title": "Script data for attribute-based recognition of composite activities", "authors": ["Marcus Rohrbach", "Michaela Regneri", "Mykhaylo Andriluka", "Sikandar Amin", "Manfred Pinkal", "Bernt Schiele."], "venue": "ECCV, pages 144\u2013157.", "year": 2012}, {"title": "Very deep convolutional networks for large-scale image recognition", "authors": ["Karen Simonyan", "Andrew Zisserman."], "venue": "CoRR, abs/1409.1556.", "year": 2014}, {"title": "Inception-v4, inception-resnet and the impact of residual connections on learning", "authors": ["Christian Szegedy", "Sergey Ioffe", "Vincent Vanhoucke", "Alexander A. Alemi."], "venue": "AAAI, pages 4278\u20134284.", "year": 2017}, {"title": "Learning spatiotemporal features with 3d convolutional networks", "authors": ["Du Tran", "Lubomir D. Bourdev", "Rob Fergus", "Lorenzo Torresani", "Manohar Paluri."], "venue": "ICCV, pages 4489\u20134497.", "year": 2015}, {"title": "Selective search for object recognition", "authors": ["Jasper R.R. Uijlings", "Koen E.A. van de Sande", "Theo Gevers", "Arnold W.M. Smeulders."], "venue": "International Journal of Computer Vision, 104(2):154\u2013171.", "year": 2013}, {"title": "Reconstruction network for video captioning", "authors": ["Bairui Wang", "Lin Ma", "Wei Zhang", "Wei Liu."], "venue": "CVPR.", "year": 2018}, {"title": "Bidirectional attentive fusion with context gating for dense video captioning", "authors": ["Jingwen Wang", "Wenhao Jiang", "Lin Ma", "Wei Liu", "Yong Xu."], "venue": "CVPR.", "year": 2018}, {"title": "Temporal segment networks: Towards good practices for deep action recognition", "authors": ["Limin Wang", "Yuanjun Xiong", "Zhe Wang", "Yu Qiao", "Dahua Lin", "Xiaoou Tang", "Luc Van Gool."], "venue": "ECCV, pages 20\u201336.", "year": 2016}, {"title": "Learning natural language inference with LSTM", "authors": ["Shuohang Wang", "Jing Jiang."], "venue": "NAACL, pages 1442\u2013 1451.", "year": 2016}, {"title": "Machine comprehension using match-lstm and answer pointer", "authors": ["Shuohang Wang", "Jing Jiang."], "venue": "CoRR, abs/1608.07905.", "year": 2016}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "authors": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C. Courville", "Ruslan Salakhutdinov", "Richard S. Zemel", "Yoshua Bengio."], "venue": "ICML, pages 2048\u20132057.", "year": 2015}, {"title": "Jointly modeling deep video and compositional text to bridge vision and language in a unified framework", "authors": ["Ran Xu", "Caiming Xiong", "Wei Chen", "Jason J. Corso."], "venue": "AAAI, pages 2346\u20132352.", "year": 2015}, {"title": "Grounded language learning from video described with sentences", "authors": ["Haonan Yu", "Jeffrey Mark Siskind."], "venue": "ACL, pages 53\u201363.", "year": 2013}, {"title": "Mattnet: Modular attention network for referring expression comprehension", "authors": ["Licheng Yu", "Zhe Lin", "Xiaohui Shen", "Jimei Yang", "Xin Lu", "Mohit Bansal", "Tamara L. Berg."], "venue": "CoRR, abs/1801.08186.", "year": 2018}, {"title": "Visual7w: Grounded question answering in images", "authors": ["Yuke Zhu", "Oliver Groth", "Michael S. Bernstein", "Li FeiFei."], "venue": "CVPR, pages 4995\u20135004.", "year": 2016}, {"title": "Edge boxes: Locating object proposals from edges", "authors": ["C. Lawrence Zitnick", "Piotr Doll\u00e1r."], "venue": "ECCV, pages 391\u2013405.", "year": 2014}], "id": "SP:452aca244ef62a533d8b46a54c6212fe9fa3ce9a", "authors": [{"name": "Jingyuan Chen", "affiliations": []}, {"name": "Xinpeng Chen", "affiliations": []}, {"name": "Lin Ma", "affiliations": []}, {"name": "Zequn Jie", "affiliations": []}, {"name": "Tat-Seng Chua", "affiliations": []}], "abstractText": "We introduce an effective and efficient method that grounds (i.e., localizes) natural sentences in long, untrimmed video sequences. Specifically, a novel Temporal GroundNet (TGN)1 is proposed to temporally capture the evolving fine-grained frame-by-word interactions between video and sentence. TGN sequentially scores a set of temporal candidates ended at each frame based on the exploited frameby-word interactions, and finally grounds the segment corresponding to the sentence. Unlike traditional methods treating the overlapping segments separately in a sliding window fashion, TGN aggregates the historical information and generates the final grounding result in one single pass. We extensively evaluate our proposed TGN on three public datasets with significant improvements over the stateof-the-arts. We further show the consistent effectiveness and efficiency of TGN through an ablation study and a runtime test.", "title": "Temporally Grounding Natural Sentence in Video"}