0	30	Semantic role labeling (SRL), namely semantic parsing, is a shallow semantic parsing task, which aims to recognize the predicate-argument structure of each predicate in a sentence, such as who did what to whom, where and when, etc.
3	56	There are two formulizations for semantic predicate-argument structures, one is based on constituents (i.e., phrase or span), the other is based on dependencies.
6	42	In prior work of SRL, considerable attention has been paid to feature engineering that struggles to capture sufficient discriminative information, while neural network models are capable of extracting features automatically.
7	19	In particular, syntactic information, including syntactic tree feature, has been show extremely beneficial to SRL since a larger scale of empirical verification of Punyakanok et al. (2008).
9	19	To alleviate the above issues, Marcheggiani et al. (2017) propose a simple but effective model for dependency SRL without syntactic input.
10	40	It seems that neural SRL does not have to rely on syntactic features, contradicting with the belief that syntax is a necessary prerequisite for SRL as early as Gildea and Palmer (2002).
25	25	The overall role labeling model is depicted in Figure 1.
34	20	Afterwards, reset the current node to its syntactic head and repeat the previous step till the root of the tree.
39	22	Algorithm 1 k-order argument pruning algorithm Input: A predicate p, the root node r given a syn- tactic dependency tree T , the order k Output: The set of argument candidates S 1: initialization set p as current node c, c = p 2: for each descendant ni of c in T do 3: if D(c, ni) ≤ k and ni /∈ S then 4: S = S + ni 5: end if 6: end for 7: find the syntactic head ch of c, and let c = ch 8: if c = r then 9: S = S + r 10: else 11: goto step 2 12: end if 13: return argument candidates set S First, previous standard pruning algorithm may hurt the argument coverage too much, even though indeed arguments usually tend to surround their predicate in a close distance.
74	27	Figure 3 shows changing curves of coverage and reduction following k on the English train set.
77	32	Accordingly, the first-order pruning reduces more than 50% candidates at the cost of missing 5.5% true ones on average, and the second-order prunes about 40% candidates with nearly 2.0% loss.
91	18	Even though we use the same parameters as for English, our model also outperforms the best reported results by 0.3% (syntax-aware) and 0.6% (syntax-agnostic) in F1 scores.
108	17	In Figure 5, we give an example sequence with the labels for the given sentence.
109	18	We also report results of our end-to-end model on CoNLL-2009 test set with syntax-aware and syntax-agnostic settings.
112	22	For a full SRL task, the predicate identification subtask is also indispensable, which has been included in CoNLL-2008 shared task.
125	30	Syntactic Input In order to obtain different syntactic inputs, we design a faulty syntactic tree generator (refer to STG hereafter), which is able to produce random errors in the output parse tree like a true parser does.
126	30	To simplify implementation, we construct a new syntactic tree based on the gold standard parse tree.
129	18	However, the score is influenced by the quality of syntactic input to some extent, leading to unfaithfully reflecting the competence of syntax-based SRL system.
131	22	To normalize the semantic score relative to syntactic parse, we take into account additional evaluation measure to estimate the actual overall performance of SRL.
133	25	Table 9 reports the performance of existing models7 in term of Sem-F1/LAS ratio on CoNLL2009 English test set.
134	18	Interestingly, even though our system has significantly lower scores than others by 3.8% LAS in syntactic components, we obtain the highest results both on Sem-F1 and the Sem-F1/LAS ratio, respectively.
135	28	These results show that our SRL component is relatively much stronger.
139	113	We also perform our first and tenth order pruning models with different erroneous syntactic inputs generated from STG and evaluate their per- formance using the Sem-F1/LAS ratio.
140	21	Figure 6 shows Sem-F1 scores at different quality of syntactic parse inputs on the English test set whose LAS varies from 85% to 100%.
141	66	Compared to previous state-of-the-arts (Marcheggiani and Titov, 2017).
142	49	Our tenth-order pruning model gives quite stable SRL performance no matter the syntactic input quality varies in a broad range, while our firstorder pruning model yields overall lower results (1-5% F1 drop), owing to missing too many true arguments.
144	20	Furthermore, it indicates that our model with an accurate enough syntactic input as Marcheggiani and Titov (2017), namely, 90% LAS, will give a Sem-F1 exceeding 90% for the first time in the research timeline of semantic role labeling.
163	43	In addition, we consider the Sem-F1/LAS ratio as a mean of evaluating syntactic contribution to SRL, and true performance of SRL independent of the quality of syntactic parser.
164	55	Though we again confirm the importance of syntax to SRL with empirical experiments, we are aware that since (Pradhan et al., 2005), the gap between syntax-aware and syntax-agnostic SRL has been greatly reduced, from as high as 10% to only 1-2% performance loss in this work.
165	71	However, maybe we will never reach a satisfying conclusion, as whenever one proposes a syntax-agnostic SRL system which can outperform all syntax-aware ones at then, always there comes argument that you have never fully explored creative new method to effectively exploit the syntax input.
