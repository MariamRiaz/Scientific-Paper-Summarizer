40	9	,DN} by using a deep neural network (DNN).
46	5	Unfortunately, the computation of the posterior is challenging in deep models.
52	8	Due to these issues, Bayesian inference in deep learning is computationally challenging.
57	5	These approaches make use of existing codebases for adaptive learning-rate methods to perform VI, which can handle many network architectures and can scale well to large datasets.
59	4	Compared to MLE, the memory and computation costs increase because the number of parameters to be optimized is doubled and we now have two vectors µ and σ to estimate.
65	8	We derive our algorithm by approximating a natural-gradient method and then using a naturalmomentum method.
83	3	Since the scaling vector st contains an online estimate of the diagonal of the Hessian, we call this the “variational online-Newton” (VON) method.
88	4	In the next section, we propose a simple fix to this problem by using an approximation.
89	5	To avoid negative variances in the VON update, we propose to use the Generalized Gauss-Newton (GGN) approximation (Schraudolph, 2002; Martens, 2014; Graves, 2011): ∇2θjθjf(θ) ≈ 1 M ∑ i∈M [ ∇θjfi(θ) ]2 := ĥj(θ), (9) where θj is the j’th element of θ.
93	7	The GGN approximation is proposed by Graves (2011) for mean-field Gaussian VI to derive a fast gradient-based method (see Eq.
95	6	How good is this approximation?
100	3	This is because these codebases are optimized to directly compute the sum of the gradients over minibatches, and do not support computation of individual gradients as required in (9).
104	4	(11) Compared to the GGN which computes the sum of squaredgradients, this approximation instead computes the square of the sum.
111	9	The resulting update is very similar to the RMSprop update: Vprop: µt+1 = µt − αt (ĝ(θt)+λ̃µt)/( √ st+1 + λ̃), st+1 = (1− βt)st + βt [ĝ(θt) ◦ ĝ(θt)] , (14) where θt ∼ N (θ|µt,σ2t ) with σ2t := 1/[N(st + λ̃)].
113	5	The Vprop update resembles RMSprop but with three differences (highlighted in red).
116	3	The variance is also the uncertainty estimates.
119	5	Finally, the third difference is that the constant δ in RMSprop is replaced by λ̃.
121	8	Taking square of a sum leads to a sum with M2 terms which, depending on the correlations between the individual gradients, would either shrink or expand the estimate.
125	3	Suppose minibatchesM are sampled from the uniform distribution p(M) over all ( N M ) minibatches, and denote a minibatch gradient by ĝj(θ;M), then the expected value of the GM approximation is the following, Ep(M) [ ĝj(θ;M)2 ] = whj(θ) + (1− w)[gj(θ)]2, (15) where w = 1M (N −M)/(N − 1).
145	10	Vprop and Vadam perform variational inference, but they can be modified to perform optimization instead of inference.
146	6	We now derive such an algorithm which turns out to be a variational version of AdaGrad.
148	6	In this framework, instead of directly minimizing F (θ), we minimize its expectation Eq [F (θ)] under a distribution q(θ) := N (θ|µ,σ2) with respect to µ and σ2.
170	3	In this experiment, we compare the posterior approximations found with our algorithms to the optimal variational approximation that minimizes the variational objective.
172	8	Since our goal is to compare the accuracy of posterior approximations and not the speed of convergence, we run both the methods for many iterations with a small learning rate to make sure that they converge.
178	11	First, the negative of the variational objective on the training data (the evidence lower-bound or ELBO), log-loss on the test data, and the symmetric KL distance between MF-Exact and the approximation found by a method.
213	7	In this paper, we present new VI algorithms which are as simple to implement and execute as algorithms for MLE.
215	14	The resulting algorithms can be implemented within Adam with minimal changes.
216	19	Our empirical findings confirm that our proposed algorithms obtain comparable uncertainty estimates to existing VI methods, but require less computational and implementation effort6.
217	134	An interesting direction we hope to pursue in the future is to generalize our natural-gradient approach to other types of approximation, e.g., exponetial-family distributions and their mixtures.
218	164	We would also like to further explore the application to areas such as RL and stochastic optimization.
