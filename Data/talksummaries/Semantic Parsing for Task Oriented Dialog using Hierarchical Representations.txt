0	18	Intelligent personal assistants are now ubiquitous, but modeling the semantics of complex compositional natural language queries remains challenging.
1	151	Typical systems classify the intent of a query (e.g. GET DIRECTIONS) and tag the necessary slots (e.g. San Francisco) (Mesnil et al., 2013; Liu and Lane, 2016).
2	68	It is difficult for such representations to adequately represent nested queries such as “Driving directions to the Eagles game”, which is composed of GET DIRECTIONS and GET EVENT intents.
4	64	We introduce a Task Oriented Parsing (TOP) representation for intent-slot based dialog systems.
5	51	This hierarchical representation is expressive enough to capture the semantics of complex nested queries, but is easier to annotate and parse than alternative representations such as logical forms or dependency graphs.
10	38	A hierarchical semantic representation for task oriented dialog systems that can model compositional and nested queries.
13	26	We show that the representation is learnable by standard algorithms.
25	19	Executing queries such as those in Figure 1 is straightforward because of the explicit tagging of the outer location slot: first we fetch ‘the Eagles game’ event (or the relevant coffee shop), extract the location, and pass it as the destination slot to the navigation domain intent.
33	17	We found that more general representations are required for only 0.3% of queries.
36	12	• Efficient and Accurate Parsing Since our representation closely resembles syntactic trees, we can easily re-use models from the large literature on constituency parsing.
37	16	• Execution Our approach can be seen as a simple generalization of traditional dialog systems, meaning that existing infrastructure can easily be adapted to execute the intents.
41	20	If all three annotators disagreed then we discarded the utterance and its annotations.
43	26	We also compared percentage of utterances that were resolved after 2 annotations for depth ≤ 2 (traditional slot filling) and for depth > 2 (compositional): 68.87% vs 62.03%, noting that the agreement rate is similar.
50	13	We experiment with two types of models: standard sequence-to-sequence learning models, and a model adapted from syntactic parsing, Recurrent Neural Network Grammars (Dyer et al., 2016) (RNNG).
51	28	RNNG is a top-down transition-based parser and was originally proposed for parsing syntax trees and language modeling.
52	18	We trained the RNNG parser discriminatively and not generatively to reduce training time of the model.
53	17	While sequence-to-sequence learning can model arbitrary sequence transduction, we hypothesize that parsing models like RNNG, which can only output well-formed trees, will give better inductive bias and flexibility for predicting compositional and cross-domains scenarios on the fly, particularly for domains with less training data available.
54	45	We briefly review the RNNG model – The parse tree is constructed using a sequence of transitions, or ‘actions’.
60	14	We compared RNNG with implementations of sequence-to-sequence models using CNNs (Gehring et al., 2017), LSTMs (Wiseman and Rush, 2016) and Transformer networks (Vaswani et al., 2017) in fairseq4.
76	23	The experimental results in Table 1 and Figure 3 show that existing approaches for syntactic parsing, such as RNNG, perform well for this task, achieving perfect outputs on over 75% of queries.
82	31	The seq2seq results in Table 1 are accuracy for the top prediction when a beam of size 5 was run and the RNNG results are for greedy inference.
83	18	For RNNG, the accuracy of top prediction did not change much when a beam of size 5 was run.
85	15	For seq2seqCNN, the Top-3 score was 88.08 and Top-5 score was 90.21.
86	30	For seq2seq-LSTM, Top-3 was 86.55 and Top-5 was 88.76.
88	86	These top-k predictions could be used by a hypothesis ranker downstream, which can take into account agent capabilities.
93	22	While sequence-to-sequence models have shown strong parsing performance when trained on very large amounts of data (Vinyals et al., 2015); in our setting the inductive bias provided by the RNNG model is crucial to achieving high performance.
94	34	The model has several useful biases, such as guaranteeing a well-formed output tree, and shortening the dependencies between intents and their slots.
95	61	A further advantage of RNNG is that inference has linear time complexity, whereas seq2seq models are quadratic because attention is recomputed at every time step.
108	66	We are releasing a large dataset of annotated utterances at http://fb.
109	37	The representation allows the use of existing constituency parsing algorithms, resulting in higher accuracy than sequence-to-sequence models.
