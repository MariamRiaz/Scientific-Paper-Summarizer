0	94	The problem of finding the mixing matrix A from a set of observation vectors y in the model y = Ax (1) is only solvable if one can benefit from strong hypotheses on the signal vector x.
1	40	For instance, one may assume that the entries of x are statistically independent, which results in a class of methods refered to as independent component analysis (ICA) (Hyvärinen et al., 2004).
2	31	A more recent trend is to assume that the vector x is sparse, so that the recovery can be recast as a deterministic dictionary learning problem, the prototypical example being sparse component analysis (SCA) (Gribonval & Lesage, 2006; Aharon et al., 2006; Spielman et al., 2012).
4	31	Prior work: In the literature, ICA precedes SCA and can be traced back to (Herault & Jutten, 1986).
5	22	In fact, ICA constitutes the non-Gaussian generalization of the much older principal component analysis (PCA), which is widely used in classical signal processing.
22	31	By decreasing α, the distribution becomes more heavy-tailed and thus the signal becomes more sparse (the effect of α is illustrated in Figure 1).
25	13	Main contributions: Our main contribution in this paper is a new dictionary learning algorithm based on the signal modeling mentioned above.
37	24	Thus, X is a deterministic matrix,X is a random matrix and x is a random variable.
41	17	As their name suggests, α-stable variables share the property of stability under linear combination (Nikias & Shao, 1995); i.e., if X1, .
43	22	(4) In other words, the random variable Y is an SαS random variable with dispersion γ ‖a‖αα where ‖a‖α = ( |a1|α + · · · + |an|α ) 1 α is the α-(pseudo)norm of the vector a = (a1, .
45	34	The other property of SαS random variables with α < 2 is their heavy-tailed PDF.
48	15	Also, note that a smaller α results in heavier tails.
74	15	If α ∈ (0, 2) and B is an m× n matrix for which we have ‖A>u‖αα = ‖B>u‖αα (10) for all u ∈ Rm, then B is equal to A up to negation and permutation of its columns.
77	14	This theorem suggests that in order to find A all we need is to find γ(u) for u ∈ Rm.
78	14	Intuitively, we can say that as A has a finite number of parameters (entries), A is identifiable based on the knowledge of γ(u) for an appropriate finite set of vectors u = u1, .
95	20	Note that the dispersion parameter γ in Equation (14) does not need to be set as it will be automatically merged into the learned dictionary.
96	23	Recall that there are well-known methods for estimating α from data; among which we use α̂(u) = ( 6 π2K K∑ k=1 ( log |u>yk| − log κ̂(u) )2 − 1 2 )− 12 (15) from (Achim et al., 2015), where log κ̂(u) = 1 K K∑ k=1 log |u>yk|.
101	58	This is in contrast with most existing cost functions that have parameters one must set.
105	16	The cost function in Equation (13) is non-convex in B.
106	26	In order to avoid getting trapped in local minima, we iteratively change the cost function inside the gradient descent algorithm.
114	14	,uL yields a non-convex cost function with different local minima.
132	14	We run two types of the experiments: We first test the algorithm on synthetic SαS data and then we test it on real images.
136	34	We compare our algorithm with three commonly used algorithms that are available in the Python package SPAMS2.
137	30	These constrained optimization problems3 are as follows: 1.
150	13	Intuitively, the precision of the estimation increases with the number of samples K, and, as K goes to infinity, the estimation error goes to zero, which ultimately gives the exact A.
154	12	Moreover, we see that the average correlation is an increasing function of K, as expected.
163	15	Also, its average learning time is typically much less than the others, except for `2/`1 which does not find the correct dictionary at best in 10% of the time.
164	14	The range of α that was observed in our experiments is α ∈ [1, 1.6], which is also the range where our algorithm works well and which is interesting for many applications including image processing.
173	17	Missing pixel recovery: In this experiment, we reconstruct an image from 50% of its pixels.
181	28	As we see, SparsDT outperforms the other methods by at least 0.6 dB.
