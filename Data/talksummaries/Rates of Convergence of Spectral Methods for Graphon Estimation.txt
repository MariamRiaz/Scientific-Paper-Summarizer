0	28	Many modern systems and datasets can be represented as networks with vertices denoting the objects and edges (possibly weighted or labelled) encoding their interactions.
2	9	A key task in network analysis is to estimate the underlying network generating mechanism, i.e., how the edges are formed in a network.
3	19	It is useful for many important applications such as studying network evolution over time (Pensky, 2016), predicting missing links in networks (Miller et al., 2009; Airoldi et al., 2013; Gao et al., 2016), learning hidden user prefererences in recommender systems (Song et al., 2016), and correcting errors in crowdsourcing systems (Lee & Shah, 2017).
8	22	We assume the network is generated according to the graphon model (Lovász & Szegedy, 2006).
9	52	Concretely, given n vertices, the edges are generated independently, connecting each pair of two distinct vertices i and j with a probability Mij = f(xi, xj), (1) where xi ∈ X is the latent feature vector of vertex i that captures various characteristics of vertex i; f : X × X → [0, 1] is a symmetric function called graphon.
11	26	from a measurable space X according to a probability distribution µ. Graphon model captures a key characteristic of real networks, that is, the edge connections are dependent on latent features of vertices rather than specific vertex identities.
12	13	Graphon model was originally developed as a limit of a sequence of graphs with growing sizes (Lovász, 2012), and has been applied to various network analysis problems ranging from testing graph properties to counting homomorphisms to charactering distances between two graphs (Lovász, 2012; Borgs et al., 2008; 2012) to detecting communities (Bickel & Chen, 2009).
13	21	Graphon model encompasses many existing network models as special cases.
14	8	Setting f to be a constant p, it gives rise to Erdős-Rényi random graphs, where each edge is formed independently with probability p. In the case where f is a step function or X is a discrete set, the model specializes to the stochastic block model (Holland et al., 1983), where each vertex belongs to a community, and the edge probability between i and j depends only on which communities they are in.
15	42	If X is a Euclidean space of dimension d and f(xi, xj) is a function of the Euclidean distance ‖xi−xj‖, then the grahon model reduces to the latent space model (Hoff et al., 2002; Handcock et al., 2007).
16	22	To capture a partial observation of the network, we assume every edge is observed independently with probability ρ ∈ [0, 1], where ρ = ρn may converge to 0 as n → ∞.
17	12	Given the resulting observed graph, the problem of interest is to estimate the underlying network generating mechanism – the graphon f .
18	5	However, without observing xi’s, there is no way to uniquely identify f. To overcome this identifiability issue, we follow the prior work (Gao et al., 2015) and consider estimating f under the expected empirical loss1: 1 n2 E  ∑ i,j∈[n] ( f̂(xi, xj)− f(xi, xj) )2 .
19	10	This is equivalent to estimating the edge probability matrix M under the mean squared error (Gao et al., 2015): MSE(M̂) , 1 n2 E [∥∥∥M̂ −M∥∥∥2 F ] , (2) where M̂ij = f̂(xi, xj).
23	10	The minimax estimation error depends on the smoothness of graphon f , the structure of latent space (X , µ), and the observation probability ρ.
25	12	Various procedures have been proposed and analyzed (Gao et al., 2015; Klopp et al., 2015; Gao et al., 2016; Wolfe & Olhede, 2013; Airoldi et al., 2013; Yang et al., 2014; Chan & Airoldi, 2014; Cai et al., 2014; Zhang et al., 2015; Borgs et al., 2015a; Klopp & Verzelen, 2017; Borgs et al., 2017).
28	5	For fully observed graphons with f being Hölder smooth on X = [0, 1] and ρ = 1, the minimax error rate turns out be n−1 log k + n−2α/(α+1), where α is the smoothness index of f .
37	7	This raises a fundamental question: Is there a polynomial-time estimator that is guaranteed to achieve the minimax optimal rate?
38	31	In this paper, we provide a partial answer to this question by analyzing the universal singular value thresholding (USVT) algorithm proposed by Chatterjee (Chatterjee, 2015).
39	40	The universal singular value thresholding is a simple and versatile method for structured matrix estimation and has been applied to a variety of different problems such as rank- ing (Shah et al., 2016).
40	25	It truncates the singular values of A at a threshold slightly above the spectral norm ‖A−E [A] ‖, and estimates M by a properly rescaled A after truncation.
41	23	It is computationally efficient, however, its performance guarantee established in (Chatterjee, 2015) requires the total number of observed edges to be much larger than n(2d+2)/(d+2) to attain a vanishing MSE.
42	39	In contrast, our improved performance bound shows that the total number of observed edges only needs to be a constant factor larger than n log n, irrespective of the latent space dimension d. More formally, by assuming the average vertex degree is at least logarithmic in n, i.e., nρ = Ω(log n), and X is a compact subset in Rd, the mean-squared error rate of USVT is shown to be upper bounded by (nρ)−2α/(2α+d), when f belongs to either α-smooth Hölder function class H(α,L) or α-smooth Sobolev space S(α,L).
43	65	This convergence rate of USVT is approaching the minimax optimal rate log(nρ)/(nρ) provided in (Gao et al., 2015) for d = 1, as f becomes smoother, i.e., α increases.
44	35	In fact, we show that if f is analytic with infinitely many times differentiability2, then the error rate is upper bounded by logd(nρ)/(nρ).
46	16	Based on compelling but non-rigorous statistical physics arguments, it is believed that no polynomial-time algorithms are able to detect the communities between the KS-threshold and IT-threshold (Moore, 2017).
48	9	During the preparation of this manuscript, we became aware of an earlier arXiv preprint (Klopp & Verzelen, 2017)[Proposition 4] which also derives the error rate of k/(nρ).
49	7	Our proof incorporates three interesting ingredients.
50	37	One is a characterization of the estimation error of USVT in terms of the tail of eigenvalues of M , and the spectral norm of the noise perturbation ‖A − E [A|M ] ‖, see e.g., (Shah et al., 2016)[Lemma 3].
51	9	The second one is a high-probability upper bound on ‖A− E [M |A] ‖ obtained from matrix concentration inequalities initially developed by (Feige & Ofek, 2005).
