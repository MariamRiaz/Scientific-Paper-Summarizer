2	59	To solve this problem, the transfer leaning method (Arnold et al., 2007) have been used to make use of samples from a resource rich source language to a resource scarce target language, also known as cross language opinion analysis (CLOA).
3	44	In transductive transfer learning (TTL) where the source language has labeled data and the target language has only unlabeled data, an algorithm needs to select samples from the unlabeled target language as the training data and assign them with class labels using some estimated confidence.
4	17	These labeled samples in the target language, referred to as the transferred samples, also have a probability of being misclassified.
5	56	During es class noise which accumulates, resulting in a so called negative transfer that affects the classification performance.
6	10	In this paper, we propose a novel method aimed at reducing class noise for TTL in CLOA.
7	12	The basic idea is to utilize transferred samples with high quality to identify those negative transfers and remove them as class noise to reduce noise accumulation in future training iterations.
9	24	More significantly, our system shows a monotonic increasing trend in performance when more training data are used beating the performance degradation curse of most transfer learning methods when training data reaches certain size.
11	50	Section 2 introduces related works in transfer learning, cross lingual opinion analysis, and class noise detection technology.
12	13	Section 3 presents our algorithm.
33	22	The basic idea is to first select high quality labeled samples after certain iterations as indicator to detect class noise in transferred samples.
36	18	Two questions must be answered in this approach: (1) how to measure the quality of transferred samples, and (2) how to utilize high quality labeled samples to detect class noise in training data.
50	10	We then estimate the least error boundary before and after each T to measure the quality of transferred samples during each T. If the least error boundary is reduced, it means that transferred samples used in this period are of high quality and can improve the performance.
52	39	For formula (2) to work, we need to know the class noise rate Î· to calculate the error boundary.
54	32	Consider a KNN graph on the transferred samples using any similarity metric, for example, cosine similarity, for any two connected vertex ( )and ( ) in the graph from samples to classes, the edge weight is given by: ( ) ( ) Furthermore, a sign function for the two vertices ( )and ( ), is defined as: { ( ) According to the manifold assumption, the conditional probability ( | ) can be approximated by the frequency of ( ) which is equal to ( ).
67	41	If the newly added samples are of high quality, they can be used to detect class noise in transferred training data.
68	10	Otherwise, transfer learning should stop.
72	14	Let T denote quality estimate period T in terms of iteration numbers.
73	47	The transfer process select k samples in each iteration.
74	20	When one period of transfer process finishes, the negative transfer detection will estimate the quality by comparing and either select the new transferred samples or remove class noise accumulated up to this iteration.
75	13	The proposed approach is evaluated on the NLP&CC 2013 cross-lingual opinion analysis (in short, NLP&CC) dataset 1 .
76	18	In the training set, there are 12,000 labeled English Amazon.com products reviews, denoted by Train_ENG, and 120 labeled Chinese product reviews, denoted as Train_CHN, from three categories, DVD, BOOK, MUSIC.
78	12	In the testing set, there are 12,000 Chinese product reviews (shown in Table.1).
79	16	This dataset is designed to evaluate the CLOA algorithm which uses Train_CHN, Train_ENG and Dev_CHN to train a classifier for Test_CHN.
92	52	Although the improvement does not seem large, our algorithm shows a different behavior in that it can continue to make use of available training data to improve the system performance.
98	13	Another experiment is conducted to compare the performance of our proposed transfer learning based approach with supervised learning.
99	15	Here, the achieved performance of 3-folder cross validation are given in The accuracy of our approach is only 1.0% lower than the supervised learning using 2/3 of Test_CHN.
100	17	In the BOOK subset, our approach achieves match result.
113	35	Experimental results also show that our approach can obtain better performance when the transferred samples are added incrementally, which in previous works would decrease the system performance.
117	16	Machine Learning, 2(4): 343-370.
118	22	Arnold, A., Nallapati, R., Cohen, W. W. 2007.
