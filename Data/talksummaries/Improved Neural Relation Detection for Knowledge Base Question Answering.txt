147	1	Following Yih et al. (2016), we use S-MART (Yang and Chang, 2015) entity-linking outputs.7 In order to evaluate the relation detection models, we create a new relation detection task from the WebQSP data set.8 For each question and its labeled semantic parse: (1) we first select the topic entity from the parse; and then (2) select all the relations and relation chains (length  2) connected to the topic entity, and set the corechain labeled in the parse as the positive label and all the others as the negative examples.
150	1	All word vectors are initialized with 300-d pretrained word embeddings (Mikolov et al., 2013).
163	1	Second, residual learning helps hierarchical matching compared to weighted-sum and attention-based baselines (see Section 4.3).
170	1	We believe this is because the LSTM relation encoder can better learn the composition of chains of relations in WebQSP, as it is better at dealing with longer dependencies.
172	1	We use WebQSP for the analysis purposes.
174	1	This is evidenced by that during training one layer usually gets a weight close to 0 thus is ignored.
176	1	It also gives much lower training accuracy (91.94%) compared to HR-BiLSTM (95.67%), suffering from training difficulty.
177	1	Second, compared to our deep BiLSTM with shortcut connections, we have the hypothesis that for KB relation detection, training deep BiLSTMs is more difficult without shortcut connections.
178	1	Our experiments suggest that deeper BiLSTM does not always result in lower training accuracy.
179	1	In the experiments a two-layer BiLSTM converges to 94.99%, even lower than the 95.25% achieved by a single-layer BiLSTM.
180	1	Under our setting the twolayer model captures the single-layer model as a special case (so it could potentially better fit the training data), this result suggests that the deep BiLSTM without shortcut connections might suffers more from training difficulty.
185	1	This proves that the residual and deep structures both contribute to the good performance of HR-BiLSTM.
192	1	Since the reranking step relies on the relation detection models, this shows that our HR-BiLSTM model contributes to the good performance in multiple ways.
193	1	Appendix C gives the detailed performance of the re-ranking step.
196	1	Finally, like STAGG, which uses multiple relation detectors (see Yih et al. (2015) for the three models used), we also try to use the top-3 relation detectors from Section 6.2.
204	1	Constraint-based question answering with knowledge graph.
205	1	In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers.
206	1	The COLING 2016 Organizing Committee, Osaka, Japan, pages 2503–2514.
207	1	Hannah Bast and Elmar Haussmann.
212	1	Association for Computational Linguistics, Seattle, Washington, USA, pages 1533– 1544.
214	1	arXiv preprint arXiv:1506.02075 .
215	1	Translating embeddings for modeling multirelational data.
219	1	In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).
225	1	arXiv preprint arXiv:1604.00727 .
226	1	Improved relation extraction with feature-rich compositional embedding models.
229	1	Deep residual learning for image recognition.
232	1	arXiv preprint arXiv:1611.00020 .
297	11	Attentionbased bidirectional long short-term memory networks for relation classification.
298	68	In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers).
299	67	Association for Computational Linguistics, Berlin, Germany, pages 207–212.
