{"sections": [{"text": "Keywords: screening, SVM, dual problem, optimization, classification\n\u2217. The first two authors contribute equally. \u2020. Corresponding author.\nc\u00a92019 Bin Hong, Weizhong Zhang, Wei Liu, Jieping Ye, Deng Cai, Xiaofei He and Jie Wang.\nLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at http://jmlr.org/papers/v20/17-723.html.\nar X\niv :1\n60 7."}, {"heading": "1. Introduction", "text": "Sparse support vector machine (SVM) (Bi et al., 2003; Wang et al., 2006) is a powerful technique that can simultaneously perform classification by margin maximization and variable selection via `1-norm penalty. The last few years have witnessed many successful applications of sparse SVMs, such as text mining (Joachims, 1998; Yoshikawa et al., 2014), bioinformatics (Narasimhan and Agarwal, 2013), and image processing (Mohr and Obermayer, 2004; Kotsia and Pitas, 2007). Many algorithms (Hastie et al., 2004; Fan et al., 2008; Catanzaro et al., 2008; Hsieh et al., 2008; Shalev-Shwartz et al., 2011) have been proposed to efficiently solve sparse SVM problems. However, the applications of sparse SVMs to large-scale learning problems, which involve a huge number of samples and extremely high-dimensional features, still remain challenging.\nAn emerging technique, called screening (El Ghaoui et al., 2012), has been shown to be promising in accelerating the training processes of large-scale sparse learning models. The essential idea of screening is to quickly identify the zero coefficients in the sparse solutions without solving any optimization problems such that the corresponding features or samples\u2014that are called inactive features or samples\u2014can be removed from the training phase. Then, we only need to perform optimization on the reduced data sets instead of the full data sets, leading to a substantial saving in the computational cost. Here, we need to emphasize that screening differs greatly from feature selection, although they look similar at the first glance. To be precise, screening is devoted to accelerating the training processes of many sparse models including Lasso, sparse SVMs, etc., while feature selection is the goal of these models. In the past few years, many screening methods are proposed for a large set of sparse learning techniques, such as Lasso (Tibshirani et al., 2012; Xiang and Ramadge, 2012; Wang et al., 2015), group Lasso (Ndiaye et al., 2016), `1-regularized logistic regression (Wang et al., 2014b), and SVMs (Ogawa et al., 2013). Most of the existing methods are in the same framework, i.e., estimating the optimum of the dual (resp. primal) problem and then developing the screening rules based on the estimations to infer which components of the primal (resp. dual) optimum are zero from the KKT conditions. The main differences among them are the techniques they use to develop their optima estimations and rules and the different sparse models they focus on. For example, SAFE (El Ghaoui et al., 2012) estimates the dual optimum by calculating the optimal value\u2019s lower bound of the dual problem. The Lasso screening method (Wang et al., 2015) estimates the optimum based on the non-expensiveness (Bauschke et al., 2011) of the projection operator by noting that its dual problem boils down to finding the projection of a given point on a convex set. Moreover, a screening method is called static if it triggers its screening rules before training, and dynamic if during the training process. Empirical studies indicate that screening methods can lead to orders of magnitude of speedup in computation time.\nHowever, most existing screening methods study either feature screening or sample screening individually and their applications have very different scenarios. Specifically, to achieve better performance (say, in terms of speedup), we favor feature screening methods when the number of features p is much larger than the sample size n, while sample screening methods are preferable when n p. Note that there is another class of sparse learning techniques, like sparse SVMs, which induce sparsities in both feature and sample spaces. All these screening methods tend to be helpless in accelerating the training process of these\nmodels with large n and p. We cannot address this problem by simply combining the existing feature and sample screening methods either. The reason is that they could mistakenly discard relevant data as they are specifically designed for different sparse models. Recently, Shibagaki et al. (2016) considered this issue and proposed a method to simultaneously identify the inactive features and samples in a dynamic manner, and they trigger their testing rule when there is a sufficient decrease in the duality gap during the training process. Thus, the method in Shibagaki et al. 2016 can discard more and more inactive features and samples as the training proceeds and one has small-scale problems to solve in the late stage of the training process. Nevertheless, the overall speedup is limited as the problems\u2019 size can be large in the early stage of the training process. To be specific, the method in Shibagaki et al. 2016 depends heavily on the duality gap during the training process. The duality gap in the early stage can always be large, which makes the dual and primal estimations inaccurate and finally results in ineffective screening rules. Hence, it is essentially solving a large problem in the early stage. This will be verified in the experimental comparisons in Section 6 and similar results can also be found in the recent work (Massias et al., 2018), which merely focuses on Lasso.\nIn this paper, to address the limitations in the dynamic screening method, we propose a novel screening method that can Simultaneously identify Inactive Features and Samples (SIFS) for sparse SVMs in a static manner, and we only need to perform SIFS once before (instead of during) training. Thus, we only need to run the training algorithm on small-scale problems. The major technical challenge in developing SIFS is that we need to accurately estimate the primal and dual optima. The more accurate the estimations are, the more effective SIFS is in detecting inactive features and samples. Thus, our major technical contribution is a novel framework, which is based on the strong convexity of the primal and dual problems of sparse SVMs for deriving accurate estimations of the primal and dual optima (see Section 3). Another appealing feature of SIFS is the so-called synergistic effect (Shibagaki et al., 2016). Specifically, the proposed SIFS consists of two parts, i.e., Inactive Feature Screening (IFS) and Inactive Sample Screening (ISS). We show that discarding inactive features (resp. samples) identified by IFS (resp. ISS) leads to a more accurate estimation of the primal (resp. dual) optimum, which in turn dramatically enhances the capability of ISS (resp. IFS) in detecting inactive samples (resp. features). Thus, SIFS applies IFS and ISS in an alternating manner until no more inactive features and samples can be identified, leading to much better performance in scaling up large-scale problems than the application of ISS or IFS individually. Moreover, SIFS (see Section 4) is safe in the sense that the detected features and samples are guaranteed to be absent from the sparse representations. To the best of our knowledge, SIFS is the first static screening rule for sparse SVMs, which is able to simultaneously detect inactive features and samples. Experiments (see Section 6) on both synthetic and real data sets demonstrate that SIFS significantly outperforms the state-of-the-art (Shibagaki et al., 2016) in improving the training efficiency of sparse SVMs and the speedup can be orders of magnitude.\nTo demonstrate the flexibility of our proposed method SIFS, we extend its idea to multi-class sparse support vector machine (see Section 5), which also induces sparsities in both feature and sample spaces. Although multi-class sparse SVM has a very different structure and is more complex than sparse SVM, we will see that its dual problem has some similar properties (especially the strong convexity) with those of sparse SVM. Recall that\nSIFS we developed for sparse SVMs is mainly based on the strong convexity of the primal and dual problems. Therefore, the idea of SIFS is also applicable for multi-class sparse SVMs. Experimental results show that the speedup gained by SIFS in multi-class sparse SVMs can also be orders of magnitude.\nFor the convenience of presentation, we postpone the detailed proofs of all the theorems in this paper to the appendix. At last, we should point out that this journal paper is an extension of our own previous work (Zhang et al., 2017) published at the International Conference on Machine Learning (ICML) 2017.\nNotations: Let \u2016 \u00b7 \u20161, \u2016 \u00b7 \u2016, and \u2016 \u00b7 \u2016\u221e be `1, `2, and `\u221e norms, respectively. We denote the inner product between vectors x and y by \u3008x,y\u3009, and the i-th component of x by [x]i. Let [p] = {1, 2..., p} for a positive integer p. Given a subset J := {j1, ..., jk} of [p], let |J | be the cardinality of J . For a vector x, let [x]J = ([x]j1 , ..., [x]jk)>. For a matrix X, let [X]J = (xj1 , ...,xjk) and J [X] = ((x j1)>, ..., (xjk)>)>, where xi and xj are the i th row and jth column of X, respectively. For a scalar t, we denote max{0, t} by [t]+. Let ek \u2208 RK be the index vector, that is, [ek]i = 1 if i = k, otherwise [ek]i = 0. At last, we denote the set of nonnegative real numbers as R+."}, {"heading": "2. Basics and Motivations", "text": "In this section, we briefly review some basics of sparse SVMs and then motivate SIFS via the KKT conditions. Specifically, we focus on an `1-regularized SVM with a smoothed hinge loss, which takes the form of\nmin w\u2208Rp\nP (w;\u03b1, \u03b2) = 1\nn n\u2211 i=1 `(1\u2212 \u3008x\u0304i,w\u3009) + \u03b1 2 \u2016w\u20162 + \u03b2||w||1, (P\u2217)\nwhere w \u2208 Rp is the parameter vector to be estimated, {xi, yi}ni=1 is the training set, xi \u2208 Rp, yi \u2208 {\u22121,+1}, x\u0304i = yixi, \u03b1 and \u03b2 are two positive parameters, and `(\u00b7) : R \u2192 R+ is the smoothed hinge loss, i.e.,\n`(t) =  0, if t < 0, t2\n2\u03b3 , if 0 \u2264 t \u2264 \u03b3, t\u2212 \u03b32 , if t > \u03b3,\n(1)\nwhere \u03b3 \u2208 (0, 1).\nRemark 1 We use the smoothed hinge loss instead of the vanilla one in order to make the objective of the Lagrangian dual problem of (P\u2217) strongly convex, which is needed in developing our accurate optima estimations. We should point out that the smoothed hinge loss is a pretty good approximation to the vanilla one with strong theoretical guarantees. See Section 5 in Shalev-Shwartz and Zhang 2016 for the details.\nWe present the Lagrangian dual problem of problem (P\u2217) and the KKT conditions in the following theorem, which play a fundamentally important role in developing our screening rules. We will not provide its proof in the appendix since it follows from Fenchel Duality (see Corollary 31.2.1 in Rockafellar, 1970) and a similar result can be found in Shibagaki et al. 2016.\nTheorem 2 (Rockafellar, 1970) Let X\u0304 = (x\u03041, x\u03042, ..., x\u0304n) and S\u03b2(\u00b7) be the soft-thresholding operator (Hastie et al., 2015), i.e., [S\u03b2(u)]i = sign([u]i)(|[u]i| \u2212 \u03b2)+. Then, for problem (P\u2217), the followings hold: (i) the dual problem of (P\u2217) is\nmin \u03b8\u2208[0,1]n\nD(\u03b8;\u03b1, \u03b2) = 1\n2\u03b1 \u2225\u2225\u2225\u2225S\u03b2 ( 1nX\u0304\u03b8 )\u2225\u2225\u2225\u22252 + \u03b32n\u2016\u03b8\u20162 \u2212 1n\u30081, \u03b8\u3009, (D\u2217)\nwhere 1 \u2208 Rn is a vector with all components equal to 1; (ii) denote the optima of (P\u2217) and (D\u2217) by w\u2217(\u03b1, \u03b2) and \u03b8\u2217(\u03b1, \u03b2), respectively, then,\nw\u2217(\u03b1, \u03b2) = 1 \u03b1 S\u03b2 ( 1 n X\u0304\u03b8\u2217(\u03b1, \u03b2) ) , (KKT-1)\n[\u03b8\u2217(\u03b1, \u03b2)]i =  0, if 1\u2212 \u3008x\u0304i,w\u2217(\u03b1, \u03b2)\u3009 < 0; 1, if 1\u2212 \u3008x\u0304i,w\u2217(\u03b1, \u03b2)\u3009 > \u03b3; 1 \u03b3 (1\u2212 \u3008x\u0304i,w \u2217(\u03b1, \u03b2)\u3009), otherwise. (KKT-2)\nAccording to KKT-1 and KKT-2, we define 4 index sets: F = { j \u2208 [p] : 1\nn |[X\u0304\u03b8\u2217(\u03b1, \u03b2)]j | \u2264 \u03b2\n} ,\nR = {i \u2208 [n] : 1\u2212 \u3008w\u2217(\u03b1, \u03b2), x\u0304i\u3009 < 0}, E = {i \u2208 [n] : 1\u2212 \u3008w\u2217(\u03b1, \u03b2), x\u0304i\u3009 \u2208 [0, \u03b3]}, L = {i \u2208 [n] : 1\u2212 \u3008w\u2217(\u03b1, \u03b2), x\u0304i\u3009 > \u03b3},\nwhich imply that (i) i \u2208 F \u21d2 [w\u2217(\u03b1, \u03b2)]i = 0, (ii) { i \u2208 R \u21d2 [\u03b8\u2217(\u03b1, \u03b2)]i = 0, i \u2208 L \u21d2 [\u03b8\u2217(\u03b1, \u03b2)]i = 1.\n(R)\nThus, we call the j-th feature inactive if j \u2208 F . The samples in E are the so-called support vectors and we call the samples in R and L inactive samples.\nSuppose that we are given subsets of F , R, and L. Then by the rules in (R), we can see that many coefficients of w\u2217(\u03b1, \u03b2) and \u03b8\u2217(\u03b1, \u03b2) are known. Thus, we may have much fewer unknowns to solve and the problem size can be dramatically reduced. We formalize this idea in Lemma 3.\nLemma 3 Given index sets F\u0302 \u2286 F , R\u0302 \u2286 R, and L\u0302 \u2286 L, the followings hold: (i) [w\u2217(\u03b1, \u03b2)]F\u0302 = 0, [\u03b8 \u2217(\u03b1, \u03b2)]R\u0302 = 0, [\u03b8 \u2217(\u03b1, \u03b2)]L\u0302 = 1; (ii) let D\u0302 = R\u0302 \u222a L\u0302, G\u03021 = F\u0302c [X\u0304]D\u0302c , and G\u03022 = F\u0302c [X\u0304]L\u0302, where F\u0302 c = [p] \\ F\u0302 , D\u0302c = [n] \\ D\u0302, and L\u0302c = [n] \\ L\u0302. Then, [\u03b8\u2217(\u03b1, \u03b2)]D\u0302c solves the following scaled dual problem:\nmin \u03b8\u0302\u2208[0,1]|D\u0302c| { 1 2\u03b1 \u2225\u2225\u2225\u2225S\u03b2 ( 1nG\u03021\u03b8\u0302 + 1nG\u030221 )\u2225\u2225\u2225\u22252 + \u03b32n\u2016\u03b8\u0302\u20162 \u2212 1n\u30081, \u03b8\u0302\u3009}; (scaled-D\u2217)\n(iii) suppose that \u03b8\u2217(\u03b1, \u03b2) is known, then,\n[w\u2217(\u03b1, \u03b2)]F\u0302c = 1 \u03b1 S\u03b2 ( 1 n F\u0302 c [X\u0304]\u03b8 \u2217(\u03b1, \u03b2) ) .\nLemma 3 indicates that, if we can identify index sets F\u0302 and D\u0302 and the cardinalities of F\u0302c and D\u0302c are much smaller than the feature dimension p and the sample size n, we only need to solve problem (scaled-D\u2217) that may be much smaller than problem (D\u2217) to exactly recover the optima w\u2217(\u03b1, \u03b2) and \u03b8\u2217(\u03b1, \u03b2) without sacrificing any accuracy.\nHowever, we cannot directly apply Rules in (R) to identify subsets of F , R, and L, as they require the knowledge of w\u2217(\u03b1, \u03b2) and \u03b8\u2217(\u03b1, \u03b2) that are usually unavailable. Inspired by the idea in El Ghaoui et al. 2012, we can first estimate regions W and \u0398 that contain w\u2217(\u03b1, \u03b2) and \u03b8\u2217(\u03b1, \u03b2), respectively. Then, by denoting\nF\u0302 := { j \u2208 [p] : max\n\u03b8\u2208\u0398 {\u2223\u2223\u2223\u2223 1n [X\u0304\u03b8]j \u2223\u2223\u2223\u2223} \u2264 \u03b2} , (2)\nR\u0302 := { i \u2208 [n] : max\nw\u2208W {1\u2212 \u3008w, x\u0304i\u3009} < 0\n} , (3)\nL\u0302 := { i \u2208 [n] : min\nw\u2208W {1\u2212 \u3008w, x\u0304i\u3009} > \u03b3\n} , (4)\nsince it is easy to know that F\u0302 \u2282 F , R\u0302 \u2282 R, and L\u0302 \u2282 L, the rules in (R) can be relaxed as:\n(i) j \u2208 F\u0302 \u21d2 [w\u2217(\u03b1, \u03b2)]j = 0, (R1)\n(ii) { i \u2208 R\u0302 \u21d2 [\u03b8\u2217(\u03b1, \u03b2)]i = 0, i \u2208 L\u0302 \u21d2 [\u03b8\u2217(\u03b1, \u03b2)]i = 1.\n(R2)\nIn view of Rules R1 and R2, we sketch the development of SIFS as follows.\nStep 1: Derive estimations W and \u0398 such that w\u2217(\u03b1, \u03b2) \u2208 W and \u03b8\u2217(\u03b1, \u03b2) \u2208 \u0398, respectively.\nStep 2: Develop SIFS by deriving the relaxed screening rules R1 and R2, i.e., by solving the optimization problems in Eqs. (2), (3) and (4)."}, {"heading": "3. Estimate the Primal and Dual Optima", "text": "In this section, we first show that the primal and dual optima admit closed-form solutions for specific values of \u03b1 and \u03b2 (Section 3.1). Then, in Sections 3.2 and 3.3, we present accurate estimations of the primal and dual optima, respectively. We would like to point out that in order to extend the optimum estimation results below to multi-class sparse SVM and avoid redundancy, we consider the optimum estimations for the primal and dual problems in more general forms."}, {"heading": "3.1 Effective Intervals of Parameters \u03b1 and \u03b2", "text": "Below we show two things. One is that if the value of \u03b2 is sufficiently large, no matter what \u03b1 is, the primal solution is 0. The other is for any \u03b2, if \u03b1 is large enough, the primal and dual optima admit closed-form solutions.\nLemma 4 Let \u03b2max = \u2016 1nX\u03041\u2016\u221e and \u03b1max(\u03b2) = 1 1\u2212\u03b3 maxi\u2208[n] { \u3008x\u0304i,S\u03b2( 1nX\u03041)\u3009 } . Then, (i) for \u03b1 > 0 and \u03b2 \u2265 \u03b2max, we have\nw\u2217(\u03b1, \u03b2) = 0, \u03b8\u2217(\u03b1, \u03b2) = 1;\n(ii) for all \u03b1 \u2208 [max{\u03b1max(\u03b2), 0},\u221e) \u2229 (0,\u221e), we have\nw\u2217(\u03b1, \u03b2) = 1 \u03b1 S\u03b2 ( 1 n X\u03041 ) , \u03b8\u2217(\u03b1, \u03b2) = 1. (5)\nBy Lemma 4, we only need to consider the cases where \u03b2 \u2208 (0, \u03b2max] and \u03b1 \u2208 (0, \u03b1max(\u03b2)]."}, {"heading": "3.2 Primal Optimum Estimation", "text": "In Section 1, we mention that the proposed SIFS consists of IFS and ISS, which can identify the inactive features and samples, respectively. We also mention that an alternating application of IFS and ISS can improve the estimation accuracy of the primal and dual optimum estimations, which can in turn make ISS and IFS more effective in identifying inactive samples and features, respectively. We would like to show that discarding inactive features by IFS leads to a more accurate estimation of the primal optimum.\nWe consider the following general problem (g-P\u2217):\nmin w\u2208Rp\nP (w;\u03b1, \u03b2) = L(w) + \u03b1\n2 \u2016w\u20162 + \u03b2||w||1, (g-P\u2217)\nwhere L(w) : Rp \u2192 R+ is smooth and convex. Exploiting the strong convexity of the objective function, we obtain the optimum estimation in the following lemma.\nLemma 5 Suppose that the optimum w\u2217(\u03b10, \u03b20) of problem (g-P \u2217) at (\u03b10, \u03b20) with \u03b20 \u2208 (0, \u03b2max] and \u03b10 \u2208 (0, \u03b1max(\u03b20)] is known. Consider problem (g-P\u2217) with parameters \u03b1 > 0 and \u03b20. Let F\u0302 be the index set satisfying [w\u2217(\u03b1, \u03b20)]F\u0302 = 0 and define\nc = \u03b10 + \u03b1\n2\u03b1 [w\u2217(\u03b10, \u03b20)]F\u0302c , (6)\nr2 = (\u03b10 \u2212 \u03b1)2\n4\u03b12 \u2016w\u2217(\u03b10, \u03b20)\u20162 \u2212\n(\u03b10 + \u03b1) 2\n4\u03b12 \u2016[w\u2217(\u03b10, \u03b20)]F\u0302\u2016 2. (7)\nThen, the following holds:\n[w\u2217(\u03b1, \u03b20)]F\u0302c \u2208 {w : \u2016w \u2212 c\u2016 \u2264 r}.\nFor problem (P\u2217), let F\u0302 be the index set of inactive features identified by previous IFS steps. We have [w\u2217(\u03b1, \u03b20)]F\u0302 = 0. Hence, we only need to find an estimation for [w\u2217(\u03b1, \u03b20)]F\u0302c . Since problem (P \u2217) is a special case of problem (g-P\u2217), given the reference solution w\u2217(\u03b10, \u03b20) and the set F\u0302 , from Lemma 5 we have:\n[w\u2217(\u03b1, \u03b20)]F\u0302c \u2208 W := {w : \u2016w \u2212 c\u2016 \u2264 r}, (8)\nwhere c and r are defined in Eqs. (6) and (7), respectively. It shows that [w\u2217(\u03b1, \u03b20)]F\u0302c lies in a ball of radius r centered at c. Note that, before we perform IFS, the set F\u0302 is empty and the second term on the right hand side of Eq. (7) is thus 0. If we apply IFS multiple times alternatively with ISS, the set F\u0302 will be monotonically increasing. Thus, Eq. (7) implies that the radius will be monotonically decreasing, leading to a more accurate primal optimum estimation."}, {"heading": "3.3 Dual Optimum Estimation", "text": "We consider a general dual problem (g-D\u2217) below:\nmin \u03b8\u2208[0,1]n\nD(\u03b8;\u03b1, \u03b2) = 1\n2\u03b1 f\u03b2(\u03b8) +\n\u03b3 2n \u2016\u03b8\u20162 \u2212 1 n \u3008v, \u03b8\u3009, (g-D\u2217)\nwhere v \u2208 Rn and f\u03b2(\u03b8) : Rn \u2192 R+ is smooth and convex. It is obvious that problem (g-D\u2217) can be reduced to problem (D\u2217) by letting f\u03b2(\u03b8) = \u2225\u2225S\u03b2 ( 1nX\u0304\u03b8)\u2225\u22252 and v = 1. The lemma below gives an optimum estimation of problem (g-D\u2217) based on the strong convexity of its objective function.\nLemma 6 Suppose that the optimum \u03b8\u2217(\u03b10, \u03b20) of problem (g-D \u2217) with \u03b20 \u2208 (0, \u03b2max] and \u03b10 \u2208 (0, \u03b1max(\u03b20)] is known. Consider problem (g-D\u2217) at (\u03b1, \u03b20) with \u03b1 > 0 and let R\u0302 and L\u0302 be two index sets satisfying [\u03b8\u2217(\u03b1, \u03b20)]R\u0302 = 0 and [\u03b8 \u2217(\u03b1, \u03b20)]L\u0302 = 1, respectively. We denote D\u0302 = R\u0302 \u222a L\u0302 and define\nc = \u03b1\u2212 \u03b10\n2\u03b3\u03b1 [v]D\u0302c +\n\u03b10 + \u03b1\n2\u03b1 [\u03b8\u2217(\u03b10, \u03b20)]D\u0302c , (9)\nr2 =( \u03b1\u2212 \u03b10\n2\u03b1 )2||\u03b8\u2217(\u03b10, \u03b20)\u2212\n1 \u03b3 v||2 \u2212 ||1\u2212 \u03b1\u2212 \u03b10 2\u03b3\u03b1 [v]L\u0302 \u2212 \u03b10 + \u03b1 2\u03b1 [\u03b8\u2217(\u03b10, \u03b20)]L\u0302|| 2\n\u2212 ||\u03b1\u2212 \u03b10 2\u03b3\u03b1 [v]R\u0302 + \u03b10 + \u03b1 2\u03b1 [\u03b8\u2217(\u03b10, \u03b20)]R\u0302|| 2. (10)\nThen, the following holds:\n[\u03b8\u2217(\u03b1, \u03b20)]D\u0302c \u2208 {\u03b8 : \u2016\u03b8 \u2212 c\u2016 \u2264 r}.\nWe now turn back to problem (D\u2217). As it is a special case of problem (g-D\u2217), given the reference solution \u03b8\u2217(\u03b10, \u03b20) at (\u03b10, \u03b20) and the index sets of inactive samples identified by the previous ISS steps R\u0302 and L\u0302, using Lemma 6, we can obtain:\n[\u03b8\u2217(\u03b1, \u03b20)]D\u0302c \u2208 \u0398 := {\u03b8 : \u2016\u03b8 \u2212 c\u2016 \u2264 r}, (11)\nwhere c and r are defined by Eqs. (9) and (10) with v = 1, respectively. Therefore, [\u03b8\u2217(\u03b1, \u03b20)]D\u0302c lies in the ball \u0398. In view of Eq. (10), the index sets L\u0302 and R\u0302 monotonically increase and hence the last two terms on the right hand side of Eq. (10) monotonically increase when we perform ISS multiple times (alternating with IFS), which implies that the ISS steps can reduce the radius and thus improve the dual optimum estimation.\nIn addition, from both Lemmas 5 and 6, we can see that the radii of W and \u0398 can be potentially large when \u03b1 is very small, which may affect our estimation accuracy. We can sidestep this issue by letting the ratio \u03b1/\u03b10 be a constant. That is why we space the values of \u03b1 equally at the logarithmic scale on the parameter value path in the experiments.\nRemark 7 To estimate the optima w\u2217(\u03b1, \u03b20) and \u03b8 \u2217(\u03b1, \u03b20) of problems (P \u2217) and (D\u2217) using Lemmas 5 and 6, we have a free reference solution pair w\u2217(\u03b10, \u03b20) and \u03b8\n\u2217(\u03b10, \u03b20) with \u03b10 = \u03b1max(\u03b20). The reason is that w \u2217(\u03b10, \u03b20) and \u03b8 \u2217(\u03b10, \u03b20) admit closed-form solutions in this setting (see Lemma 4)."}, {"heading": "4. The Proposed SIFS Screening Rule", "text": "We first present the IFS and ISS rules in Sections 4.1 and 4.2, respectively. Then, in Section 4.3, we develop the SIFS screening rule by an alternating application of IFS and ISS."}, {"heading": "4.1 Inactive Feature Screening (IFS)", "text": "Suppose that w\u2217(\u03b10, \u03b20) and \u03b8 \u2217(\u03b10, \u03b20) are known, we derive IFS to identify inactive features for problem (P\u2217) at (\u03b1, \u03b20) by solving the optimization problem in Eq. (2) (see Section A.5 in the appendix):\nsi(\u03b1, \u03b20) = max \u03b8\u2208\u0398\n{ 1\nn |\u3008[x\u0304i]D\u0302c , \u03b8\u3009+ \u3008[x\u0304\ni]L\u0302,1\u3009| } , i \u2208 F\u0302c, (12)\nwhere \u0398 is given by Eq. (11) and F\u0302 and D\u0302 = R\u0302 \u222a L\u0302 are the index sets of inactive features and samples that have been identified in previous screening processes, respectively. The next result shows the closed-form solution of problem (12).\nLemma 8 Consider problem (12). Let c and r be given by Eq. (9) and Eq. (10) with v = 1. Then, for all i \u2208 F\u0302c, we have\nsi(\u03b1, \u03b20) = 1\nn (|\u3008[x\u0304i]D\u0302c , c\u3009+ \u3008[x\u0304 i]L\u0302,1\u3009|+ \u2016[x\u0304 i]D\u0302c\u2016r).\nWe are now ready to present the IFS rule.\nTheorem 9 [Feature screening rule IFS] Consider problem (P\u2217). We suppose that w\u2217(\u03b10, \u03b20) and \u03b8\u2217(\u03b10, \u03b20) are known. Then:\n(1) the feature screening rule IFS takes the form of\nsi(\u03b1, \u03b20) \u2264 \u03b20 \u21d2 [w\u2217(\u03b1, \u03b20)]i = 0,\u2200i \u2208 F\u0302c; (IFS)\n(2) we can update the index set F\u0302 by\nF\u0302 \u2190 F\u0302 \u222a\u2206F\u0302 with \u2206F\u0302 = {i : si \u2264 \u03b20, i \u2208 F\u0302c}. (13)\nRecall that (see Lemma 6) previous sample screening results give us a tighter dual estimation, i.e., a smaller feasible region \u0398 for problem (12), which results in a smaller si(\u03b1, \u03b20). It finally brings about a more powerful feature screening rule IFS. This is the so called synergistic effect."}, {"heading": "4.2 Inactive Sample Screening (ISS)", "text": "Similar to IFS, we derive ISS to identify inactive samples by solving the optimization problems in Eq. (3) and Eq. (4) (see Section A.7 in the appendix for details):\nui(\u03b1, \u03b20) = max w\u2208W\n{1\u2212 \u3008[x\u0304i]F\u0302c ,w\u3009}, i \u2208 D\u0302 c, (14)\nli(\u03b1, \u03b20) = min w\u2208W\n{1\u2212 \u3008[x\u0304i]F\u0302c ,w\u3009}, i \u2208 D\u0302 c, (15)\nwhereW is given by Eq. (8) and F\u0302 and D\u0302 = R\u0302\u222a L\u0302 are the index sets of inactive features and samples that have been identified in previous screening processes. We show that problems (14) and (15) admit closed-form solutions.\nLemma 10 Consider problems (14) and (15). Let c and r be given by Eq. (6) and Eq. (7). Then,\nui(\u03b1, \u03b20) = 1\u2212 \u3008[x\u0304i]F\u0302c , c\u3009+ \u2016[x\u0304i]F\u0302c\u2016r, i \u2208 D\u0302 c,\nli(\u03b1, \u03b20) = 1\u2212 \u3008[x\u0304i]F\u0302c , c\u3009 \u2212 \u2016[x\u0304i]F\u0302c\u2016r, i \u2208 D\u0302 c.\nWe are now ready to present the ISS rule.\nTheorem 11 [Sample screening rule ISS] Consider problem (D\u2217) and suppose that w\u2217(\u03b10, \u03b20) and \u03b8\u2217(\u03b10, \u03b20) are known, then:\n(1) the sample screening rule ISS takes the form of\nui(\u03b1, \u03b20) < 0\u21d2 [\u03b8\u2217(\u03b1, \u03b20)]i = 0, li(\u03b1, \u03b20) > \u03b3 \u21d2 [\u03b8\u2217(\u03b1, \u03b20)]i = 1, \u2200i \u2208 D\u0302c; (ISS)\n(2) we can update the index sets R\u0302 and L\u0302 by\nR\u0302 \u2190 R\u0302 \u222a\u2206R\u0302 with \u2206R\u0302 = {i : ui(\u03b1, \u03b20) < 0, i \u2208 D\u0302c}, (16) L\u0302 \u2190 L\u0302 \u222a\u2206L\u0302 with \u2206L\u0302 = {i : li(\u03b1, \u03b20) > \u03b3, i \u2208 D\u0302c}. (17)\nThe synergistic effect also exists here. Recall that (see Lemma 5), previous feature screening results lead to a smaller feasible region W for problems (14) and (15), which results in smaller ui(\u03b1, \u03b20) and larger li(\u03b1, \u03b20). It finally leads us to a more accurate sample screening rule ISS."}, {"heading": "4.3 The Proposed SIFS Rule by An Alternating Application of IFS and ISS", "text": "In real applications, the optimal parameter values of \u03b1 and \u03b2 are usually unknown. To determine appropriate parameter values, common approaches, like cross validation and stability selection, need to solve the model over a grid of parameter values {(\u03b1i,j , \u03b2j) : i \u2208 [M ], j \u2208 [N ]} with \u03b2max > \u03b21 > ... > \u03b2N > 0 and \u03b1max(\u03b2j) > \u03b11,j > ... > \u03b1M,j > 0. This can be very time-consuming. Inspired by Strong Rule (Tibshirani et al., 2012) and SAFE (El Ghaoui et al., 2012), we develop a sequential version of SIFS in Algorithm 1. Specifically, given the primal and dual optima w\u2217(\u03b1i\u22121,j , \u03b2j) and \u03b8\n\u2217(\u03b1i\u22121,j , \u03b2j) at (\u03b1i\u22121,j , \u03b2j), we apply SIFS to identify the inactive features and samples for problem (P\u2217) at (\u03b1i,j , \u03b2j). Then, we perform training on the reduced data set and solve the primal and dual optima at (\u03b1i,j , \u03b2j). We repeat this process until we solve problem (P\u2217) at all pairs of parameter values.\nNote that we insert \u03b10,j into every sequence {\u03b1i,j : i \u2208 [M ]} ( see line 1 in Algorithm 1) to obtain a closed-form solution as the first reference solution. In this way, we can avoid solving problem at (\u03b11,j , \u03b2j), j \u2208 [N ] directly (without screening), which is time consuming. At last, we would like to point out that the values {(\u03b1i,j , \u03b2j) : i \u2208 [M ], j \u2208 [N ]} in SIFS can be specified by users arbitrarily.\nAlgorithm 1 SIFS\n1: Input: \u03b2max > \u03b21 > ... > \u03b2N > 0 and \u03b1max(\u03b2j) = \u03b10,j > \u03b11,j > ... > \u03b1M,j > 0. 2: for j = 1 to N do 3: Compute the first reference solution w\u2217(\u03b10,j , \u03b2j) and \u03b8\n\u2217(\u03b10,j , \u03b2j) using the close-form formulas in Eq. (5).\n4: for i = 1 to M do 5: Initialization: F\u0302 = R\u0302 = L\u0302 = \u2205. 6: repeat 7: Run sample screening using rule ISS based on w\u2217(\u03b1i\u22121,j , \u03b2j). 8: Update the inactive sample sets R\u0302 and L\u0302: R\u0302 \u2190 R\u0302 \u222a\u2206R\u0302 and L\u0302 \u2190 L\u0302 \u222a\u2206L\u0302. 9: Run feature screening using rule IFS based on \u03b8\u2217(\u03b1i\u22121,j , \u03b2j).\n10: Update the inactive feature set F\u0302 : F\u0302 \u2190 F\u0302 \u222a\u2206F\u0302 . 11: until No new inactive features or samples are identified 12: Compute w\u2217(\u03b1i,j , \u03b2j) and \u03b8\n\u2217(\u03b1i,j , \u03b2j) by solving the scaled problem. 13: end for 14: end for 15: Output:w\u2217(\u03b1i,j , \u03b2j) and \u03b8 \u2217(\u03b1i,j , \u03b2j), i \u2208 [M ], j \u2208 [N ].\nSIFS applies ISS and IFS in an alternating manner to reinforce their capability in identifying inactive samples and features. In Algorithm 1, we apply ISS first. Of course, we can also apply IFS first. The theorem below demonstrates that the orders have no impact on the performance of SIFS.\nTheorem 12 Given the optimal solutions w\u2217(\u03b1i\u22121,j , \u03b2j) and \u03b8 \u2217(\u03b1i\u22121,j , \u03b2j) at (\u03b1i\u22121,j , \u03b2j) as the reference solution pair at (\u03b1i,j , \u03b2j) for SIFS, we assume SIFS with ISS first stops after applying IFS and ISS for s times and denote the identified inactive features and samples as F\u0302As , R\u0302As , and L\u0302As . Similarly, when we apply IFS first, the results are denoted as F\u0302Bt , R\u0302Bt , and L\u0302Bt . Then, the followings hold: (1) F\u0302As = F\u0302Bt , R\u0302As = R\u0302Bt , and L\u0302As = L\u0302Bt . (2) with different orders of applying ISS and IFS, the difference between the times of ISS and IFS we need to apply in SIFS can never be larger than 1, that is, |s\u2212 t| \u2264 1."}, {"heading": "4.4 Discussion", "text": "After developing the proposed method SIFS above, we now turn to the related work discussion in order to highlight the differences between SIFS and the existing methods and also the novelty of our method, although we have mentioned some of them in the introduction section. We divide the previous work into two parts: screening for sparse SVM and for other sparse learning models.\nTo the best of our knowledge, the screening method in Shibagaki et al. 2016 is the only method besides our SIFS, which can simultaneously identify the inactive features and samples for sparse SVM. There are mainly three big differences between SIFS and this method. First, the techniques used in the optima estimations of SIFS and Shibagaki\net al. 2016 are quite different. To be specific, the estimations in SIFS are developed by exploiting the reference solution pair and carefully studying the strong convexity of the objective functions and the optimum conditions of problems (P\u2217) and (D\u2217) at the current and reference parameter value pairs (see Lemmas 5 and 6 and also their proofs for the details). In contrast, Shibagaki et al. (2016) estimated the optima heavily based on the duality gap. As we mentioned in the introduction section, duality gap is usually large in the early stages and decreases gradually, which weakens its capability in the early stages and leads to a limited overall speedup. Second, algorithmically, Shibagaki et al. (2016) is dynamic while our method is static. In other words, Shibagaki et al. (2016) identifies the inactive features and samples during the training process, and in our method, we do this before the training process (see steps 6 to 11 in Algorithm 1), which means Shibagaki et al. (2016) needs to apply its screening rules for many times while we trigger SIFS only once. These two technical and algorithmic differences make our method outperform Shibagaki et al. (2016), which will be verified in the experimental section. At last, we theoretically prove that in the static scenario the orders of applying the feature and sample screening rules have no impact on the final performance, while Shibagaki et al. (2016) did not give the theoretical result accordingly in dynamic screening.\nThere are mainly three big differences between our SIFS and existing methods for other sparse learning models. First, these existing methods identify either features or samples individually and would be helpless in real applications involving a huge number of samples and extremely high-dimensional features, while SIFS identifies features and samples simultaneously. Second, technically, SIFS can incorporate the feature (resp. sample) screening results of the previous steps as the prior knowledge into the primal (reps. dual) optimum estimation to obtain a more accurate estimation. This is verified with a strong theoretical guarantee (see Lemmas 5 and 6). At last, we give a theoretical proof (see Sections 4.1 and 4.2) to show the existence of the synergistic effect between feature and sample screening for the first time in the static scenario.\nFinally, we would like to point out that although the key ideas used in some of the screening methods including SIFS seem to be similar, they are developed specifically for the sparse models they focus on, which makes it nontrivial and even very difficult to reduce one method to another by simply resetting the loss and regularizer of the model. For example, SIFS cannot be reduced to Wang et al. (2014b) by setting \u03b1 = 0 and letting loss be the logistic loss, although both of their dual optimum estimations are based on the strong convexity of the objective. The reason is that they use the strong convexity in quite different ways due to their different dual problems including the feasible regions and the expressions of the dual objectives (see Theorem 2 in Wang et al., 2014b, Lemma 6 above, and their proofs for the details). Moreover, we cannot reduce SIFS to the methods (Ogawa et al., 2013; Wang et al., 2014a) for SVM by setting \u03b2 to be 0, since they are based on the convexity of the objective while SIFS exploits the strong convexity."}, {"heading": "5. SIFS for Multi-class Sparse SVMs", "text": "In this section, we consider extending SIFS to multi-class sparse SVMs. We will briefly review the basics of multi-class sparse SVMs and then derive a series of theorems as we did\nfor sparse SVMs above. Finally, we will present the detailed screening rule for multi-class sparse SVMs."}, {"heading": "5.1 Basics of Multi-class Sparse SVMs", "text": "We focus on the `1-regularized multi-class SVM with a smoothed hinge loss, which takes the form of\nmin w\u2208RKp\nP (w;\u03b1, \u03b2) = 1\nn n\u2211 i=1 `i(w) + \u03b1 2 \u2016w\u20162 + \u03b2||w||1, (m-P\u2217)\nwhere w = [w1; w2; ...; wK ] \u2208 RKp is the parameter vector to be estimated with wk \u2208 Rp, k = 1, ...,K. The loss function `i(w) = \u2211 k 6=yi `(w > k xi \u2212w>yixi + 1), with {xi, yi} n i=1 is the training data set of K classes, xi \u2208 Rp, yi \u2208 {1, ...,K}, and `(\u00b7) is the smoothed hinge loss defined in Eq. (1).\nThe following theorem presents the Lagrangian dual problem of (m-P\u2217) and the KKT conditions, which are essential for developing the screening rules.\nTheorem 13 For each sample (xi, yi), we define ui = 1\u2212eyi \u2208 RK and Xi = [X1i ,X2i , ...,XKi ] \u2208 RKp\u00d7K , where Xki = vec(xi(ek \u2212 eyi)>) \u2208 RKp. Let u = [u1; ...; un] \u2208 RKn and X = [X1,X2, ...,Xn] \u2208 RKp\u00d7Kn. Then, for the problem (m-P\u2217), the followings hold: (i) the dual problem of (m-P\u2217) is\nmin \u03b8\u2208[0,1]Kn\nD(\u03b8;\u03b1, \u03b2) = 1\n2\u03b1 \u2225\u2225\u2225\u2225S\u03b2 ( 1nX\u03b8 )\u2225\u2225\u2225\u22252 + \u03b32n\u2016\u03b8\u20162 \u2212 1n\u3008u, \u03b8\u3009, (m-D\u2217)\nwhere \u03b8 = [\u03b81; ...; \u03b8n] with \u03b8i \u2208 [0, 1]K , i = 1, ..., n; (ii) denote the optima of problems (m-P\u2217) and (m-D\u2217) by w\u2217(\u03b1, \u03b2) and \u03b8\u2217(\u03b1, \u03b2), respectively, then,\nw\u2217(\u03b1, \u03b2) = 1 \u03b1 S\u03b2 ( \u2212 1 n X\u03b8\u2217(\u03b1, \u03b2) ) , (m-KKT-1)\n[\u03b8\u2217i (\u03b1, \u03b2)]k =  0, if \u3008Xki ,w\u2217(\u03b1, \u03b2)\u3009+ [ui]k \u2264 0; 1, if \u3008Xki ,w\u2217(\u03b1, \u03b2)\u3009+ [ui]k \u2265 \u03b3; 1 \u03b3 (\u3008X k i ,w \u2217(\u03b1, \u03b2)\u3009+ [ui]k), otherwise; k = 1, 2, ...,K. (m-KKT-2)\nAs we did in Section 2, we can also define 3 index sets here: F = {\n(k, j) \u2208 [K]\u00d7 [p] : 1 n |[X\u03b8\u2217(\u03b1, \u03b2)]k,j | \u2264 \u03b2\n} ,\nR = { (i, k) \u2208 [n]\u00d7 [K] : \u3008Xki ,w\u2217(\u03b1, \u03b2)\u3009+ [ui]k \u2264 0 } ,\nL = { (i, k) \u2208 [n]\u00d7 [K] : \u3008Xki ,w\u2217(\u03b1, \u03b2)\u3009+ [ui]k \u2265 \u03b3 } ,\nwhich imply that\n(i) (k, j) \u2208 F \u21d2 [w\u2217k(\u03b1, \u03b2)]j = 0,\n(ii) { (i, k) \u2208 R \u21d2 [\u03b8\u2217i (\u03b1, \u03b2)]k = 0, (i, k) \u2208 L \u21d2 [\u03b8\u2217i (\u03b1, \u03b2)]k = 1.\n(m-R)\nSuppose that we are given three subsets of F ,R, and L, then we can infer the values of many coefficients of w\u2217(\u03b1, \u03b2) and \u03b8\u2217(\u03b1, \u03b2) via Rule m-R. The lemma below shows that the rest coefficients of w\u2217(\u03b1, \u03b2) and \u03b8\u2217(\u03b1, \u03b2) can be recovered by solving a small sized problem.\nLemma 14 Given index sets F\u0302 \u2286 F , R\u0302 \u2286 R, and L\u0302 \u2282 L, the followings hold: (i) [w\u2217(\u03b1, \u03b2)]F\u0302 = 0, [\u03b8 \u2217(\u03b1, \u03b2)]R\u0302 = 0, [\u03b8 \u2217(\u03b1, \u03b2)]L\u0302 = 1; (ii) let D\u0302 = R\u0302 \u222a L\u0302, G\u03021 = F\u0302c [X]D\u0302c, and G\u03022 = F\u0302c [X]L\u0302, where F\u0302 c = [K] \u00d7 [p] \\ F\u0302 , D\u0302c = [n]\u00d7 [K] \\ D\u0302, L\u0302c = [n]\u00d7 [K] \\ L\u0302 and u\u0302 = [u]D\u0302c , then, [\u03b8 \u2217(\u03b1, \u03b2)]D\u0302c solves the following scaled dual problem:\nmin \u03b8\u0302\u2208[0,1]|D\u0302c| { 1 2\u03b1 \u2225\u2225\u2225\u2225S\u03b2 ( 1nG\u03021\u03b8\u0302 + 1nG\u030221 )\u2225\u2225\u2225\u22252 + \u03b32n\u2016\u03b8\u0302\u20162 \u2212 1n\u3008u\u0302, \u03b8\u0302\u3009}; (m-scaled-D\u2217)\n(iii) suppose that \u03b8\u2217(\u03b1, \u03b2) is known, then,\n[w\u2217(\u03b1, \u03b2)]F\u0302c = 1 \u03b1 S\u03b2 ( \u2212 1 n F\u0302 c [X]\u03b8 \u2217(\u03b1, \u03b2) ) .\nGiven two estimations W and \u0398 for w\u2217(\u03b1, \u03b2) and \u03b8\u2217(\u03b1, \u03b2), we can define three subsets of F ,R, and L below as we did in the binary case to relax Rule m-R into the applicable version:\nF\u0302 = {\n(k, j) \u2208 [K]\u00d7 [p] : max \u03b8\u2208\u0398\n{ 1\nn |[X\u03b8]k,j |\n} \u2264 \u03b2 } ,\nR\u0302 = {\n(i, k) \u2208 [n]\u00d7 [K] : max w\u2208W\n{ \u3008Xki ,w\u3009+ [ui]k } \u2264 0 } ,\nL\u0302 = {\n(i, k) \u2208 [n]\u00d7 [K] : min w\u2208W\n{ \u3008Xki ,w\u2217(\u03b1, \u03b2)\u3009+ [ui]k\u3009 } \u2265 \u03b3 } ."}, {"heading": "5.2 Estimate the Primal and Dual Optima", "text": "We first derive the effective intervals of the parameters \u03b1 and \u03b2.\nLemma 15 Let \u03b2max = \u2016 1nXu\u2016\u221e and \u03b1max(\u03b2) = 1 1\u2212\u03b3 max(i,k)\u2208[n]\u00d7[K] { \u3008Xki ,S\u03b2( 1nXu)\u3009 } . Then: (i) for \u03b1 > 0 and \u03b2 \u2265 \u03b2max, we have\nw\u2217(\u03b1, \u03b2) = 0, \u03b8\u2217(\u03b1, \u03b2) = u;\n(ii) for all \u03b1 \u2208 [max{\u03b1max(\u03b2), 0},\u221e) \u2229 (0,\u221e), we have\nw\u2217(\u03b1, \u03b2) = 1 \u03b1 S\u03b2 ( \u2212 1 n Xu ) , \u03b8\u2217(\u03b1, \u03b2) = u. (18)\nHence, we only need to consider the cases with \u03b2 \u2208 (0, \u03b2max] and \u03b1 \u2208 (0, \u03b1max(\u03b2)]. Since problem (m-P\u2217) is a special case of problem (g-P\u2217) with L(w) = 1m \u2211n i=1 `i(w), given the reference solution w\u2217(\u03b10, \u03b20) and the index set F\u0302 of the inactive features identified by the previous IFS steps, we can apply Lemma 5 to obtain the estimation for w\u2217(\u03b1, \u03b20)]F\u0302c :\n[w\u2217(\u03b1, \u03b20)]F\u0302c \u2208 W := {w : \u2016w \u2212 c\u2016 \u2264 r}, (19)\nwhere c and r are defined in Eqs. (6) and (7), respectively. Moreover, noting that problem (m-D\u2217) is also a special case of problem (g-D\u2217), given the reference solution \u03b8\u2217(\u03b10, \u03b20) and the index sets of inactive samples identified by the previous ISS steps R\u0302 and L\u0302, we can obtain an estimation for [\u03b8\u2217(\u03b1, \u03b20)]D\u0302c from Lemma 6:\n[\u03b8\u2217(\u03b1, \u03b20)]D\u0302c \u2208 \u0398 := {\u03b8 : \u2016\u03b8 \u2212 c\u2016 \u2264 r}, (20)\nwhere c and r are defined by Eqs. (9) and (10) with v = u, respectively."}, {"heading": "5.3 Screening Rule SIFS", "text": "Given the optima w\u2217(\u03b10, \u03b20) and \u03b8 \u2217(\u03b10, \u03b20), to derive the feature screening rule IFS, we need to solve the optimization problem below first:\ns(k,j)(\u03b1, \u03b20) = max \u03b8\u2208\u0398\n{ 1\nn |\u3008[Xk,j ]D\u0302c , \u03b8\u3009+ \u3008[Xk,j ]L\u0302,1\u3009|\n} , (k, j) \u2208 F\u0302c, (21)\nwhere Xk,j is the ((k \u2212 1)p+ j)-th row of X, \u0398 is given by Eq. (20), and F\u0302 and D\u0302 = R\u0302 \u222a L\u0302 are the index sets of the inactive features and samples identified in the previous screening process. We notice that this problem has exactly the same form as problem (12). Hence, from Lemma 8 we can obtain its closed-form solution directly. Now, from Rule m-R, we can obtain our IFS rule below.\n\u2022 The feature screening rule IFS takes the form of\ns(k,j)(\u03b1, \u03b20) \u2264 \u03b20 \u21d2 [w\u2217k(\u03b1, \u03b20)]j = 0, \u2200(k, j) \u2208 F\u0302c. (IFS)\n\u2022 The index set F\u0302 can be updated by\nF\u0302 \u2190 F\u0302 \u222a\u2206F\u0302 with \u2206F\u0302 = {(k, j) : s(k,j) \u2264 \u03b20, (k, j) \u2208 F\u0302c}. (22)\nSimilarly, we need to solve the following problems to derive our sample screening rule ISS:\nu(i,k)(\u03b1, \u03b20) = max w\u2208W\n{\u3008[Xki ]F\u0302c ,w\u3009+ [ui]k}, (i, k) \u2208 D\u0302 c, (23)\nl(i,k)(\u03b1, \u03b20) = min w\u2208W\n{\u3008[Xki ]F\u0302c ,w\u3009+ [ui]k}, (i, k) \u2208 D\u0302 c, (24)\nwhere W is given by Eq. (19). We find that they can be solved by Lemma 10 directly. Therefore, we can obtain our sample screening rule ISS below.\n\u2022 The sample screening rule ISS takes the form of\nu(i,k)(\u03b1, \u03b20) \u2264 0\u21d2 [\u03b8\u2217i (\u03b1, \u03b20)]k = 0, l(i,k)(\u03b1, \u03b20) \u2265 \u03b3 \u21d2 [\u03b8\u2217i (\u03b1, \u03b20)]k = 1, \u2200(i, k) \u2208 D\u0302c. (ISS)\n\u2022 We can update the index sets R\u0302 and L\u0302 by\nR\u0302 \u2190 R\u0302 \u222a\u2206R\u0302 with \u2206R\u0302 = {(i, k) : u(i,k)(\u03b1, \u03b20) \u2264 0, (i, k) \u2208 D\u0302c}, (25) L\u0302 \u2190 L\u0302 \u222a\u2206L\u0302 with \u2206L\u0302 = {(i, k) : l(i,k)(\u03b1, \u03b20) \u2265 \u03b3, (i, k) \u2208 D\u0302c}. (26)\nThe same as we did sparse SVM, we can also develop SIFS to reinforce the capabilities of ISS and IFS by applying them alternatively. The framework of SIFS for solving the model over a grid of parameter values here is the same as that in the case of sparse SVM, i.e. Algorithm 1, except for the different rules IFS and ISS, and the updates of R\u0302, L\u0302, and F\u0302 .\nIn this version of SIFS, the order of applying IFS and ISS also has no impact on the final performance. Since the form of the theorem and its proof are nearly the same as that of Theorem 12, to avoid redundancy, we omit them in this section."}, {"heading": "6. Experiments", "text": "We evaluate SIFS on both synthetic and real data sets in terms of three measurements: speedup, scaling ratio, and rejection ratio. Speedup is given by the ratio of the running time of the solver without screening to that with screening.\nFor sparse SVMs, scaling ratio is defined as 1 \u2212 (n\u2212n\u0303)(p\u2212p\u0303)np , where n\u0303, p\u0303, n, and p are the numbers of inactive samples and features identified by SIFS, sample size, and feature dimension of the data set. From steps 6 to 11 in Algorithm 1, we can see that we trigger the rules ISS and IFS repetitively until no new features and samples are identified. We adopt the rejection ratios of the i-th triggering of ISS and IFS defined as n\u0303in0 and p\u0303i p0\nto evaluate their performances in each triggering, where n\u0303i and p\u0303i are the numbers of inactive samples and features identified in the i-th triggering of ISS and IFS. n0 and p0 are the numbers of inactive samples and features.\nFor multi-class sparse SVMs, we let scaling ratio be 1\u2212 (Kn\u2212n\u0303)(Kp\u2212p\u0303) K2np\n, where n\u0303 = |R\u0302\u222a L\u0302|, p\u0303 = |F\u0302 |, and n and p are the sample size and the feature dimension of the data set. The rejection ratios of the i-th triggering of ISS and IFS are n\u0303in0 and p\u0303i p0 , respectively, where p0 = |F|, n0 = |R \u222a L|, p\u0303i = |\u2206F\u0302 | and n\u0303i = |\u2206R\u0302 \u222a \u2206L\u0302| with \u2206F\u0302 ,\u2206R\u0302, and \u2206L\u0302 are the increments of F\u0302 , R\u0302 and L\u0302 in the i-th triggering of the rules IFS and ISS. Please see Eqs. (22), (25), and (26) for the detailed definitions of \u2206F\u0302 ,\u2206R\u0302, and \u2206L\u0302.\nRecall that, we can integrate SIFS with any solvers for problem (P\u2217). In this experiment, we use Accelerated Proximal Stochastic Dual Coordinate Ascent (AProx-SDCA) (ShalevShwartz and Zhang, 2016) as a baseline, as it is one of the state-of-the-arts. We choose the state-of-art screening method for sparse SVMs in Shibagaki et al. 2016 as another baseline only in the experiments of sparse SVMs, since it cannot be applied in multi-class sparse SVMs. As we mentioned in the introduction section that screening differs greatly from feature selection methods, it is not appropriate to make comparisons with feature selection methods.\nFor each data set, we solve problem (P\u2217) at a grid of turning parameter values. Specifically, we first compute \u03b2max by Lemma 4 and then select 10 values of \u03b2 that are equally spaced at the logarithmic scale of \u03b2/\u03b2max from 1 to 0.05. Then, for each value of \u03b2, we first compute \u03b1max(\u03b2) by Lemma 4 and then select 100 values of \u03b1 that are equally spaced at the logarithmic scale of \u03b1/\u03b1max(\u03b2) from 1 to 0.01. Thus, for each data set, we solve problem (P\u2217) at 1, 000 pairs of parameter values in total. We write the code in C++ along with Eigen library for numerical computations. We perform all the computations on a single core of Intel(R) Core(TM) i7-5930K 3.50GHz, 128GB MEM."}, {"heading": "6.1 Simulation Studies with Spare SVMs", "text": "We evaluate SIFS on 3 synthetic data sets named syn1, syn2, and syn3 with sample and feature size (n, p) \u2208 {(10000, 1000), (10000, 10000), (1000, 10000)}. We present each data point as x = [x1; x2] with x1 \u2208 R0.02p and x2 \u2208 R0.98p. We use Gaussian distributions G1 = N(u, 0.75I),G2 = N(\u2212u, 0.75I), and G3 = N(0, 1) to generate the data points, where u = 1.51 and I \u2208 R0.02p\u00d70.02p is the identity matrix. To be precise, x1 for positive and negative points are sampled from G1 and G2, respectively. For each entry in x2, it has chance \u03b7 = 0.02 to be sampled from G3 and chance 1\u2212 \u03b7 to be 0.\nFig. 1 shows the scaling ratios by ISS, IFS, and SIFS on the synthetic data sets at 1, 000 parameter values. We can see that IFS is more effective in scaling problem size than ISS, with scaling ratios roughly 98% against 70\u2212 90%. Moreover, SIFS, which is an alternating application of IFS and ISS, significantly outperforms ISS and IFS, with scaling ratios roughly 99.9%. This high scaling ratios imply that SIFS can lead to a significant speedup.\nDue to the space limitation, we only report the rejection ratios of SIFS on syn2. Other results can be found in the appendix. Fig. 2 shows that SIFS can identify most of the inactive features and samples. However, few features and samples are identified in the second and later triggerings of ISS and IFS. The reason may be that the task here is so simple that one triggering is enough.\nTable 1 reports the running times of solver without and with IFS, ISS and SIFS for solving problem (P\u2217) at 1, 000 pairs of parameter values. We can see that SIFS leads to\nsignificant speedups, that is, up to 76.8 times. Taking syn2 for example, without SIFS, the solver takes more than two hours to solve problem (P\u2217) at 1, 000 pairs of parameter values. However, combined with SIFS, the solver only needs less than three minutes for solving the same set of problems. From the theoretical analysis in Shalev-Shwartz and Zhang 2016 for AProx-SDCA, we can see that its computational complexity rises proportionately to the sample size n and the feature dimension p. From this theoretical result, we can see that the results in Fig. 1 are roughly consistent with the speedups we achieved shown in Table 1."}, {"heading": "6.2 Experiments with Sparse SVMs on Real Datasets", "text": "In this experiment, we evaluate the performance of SIFS on 5 large-scale real data sets: real-sim, rcv1-train, rcv1-test, url, and kddb, which are all collected from the project page of LibSVM (Chang and Lin, 2011). See Table 2 for a brief summary. We note that, the kddb data set has about 20 million samples with 30 million features.\nRecall that, SIFS detects the inactive features and samples in a static manner, i.e., we perform SIFS only once before the training process and hence the size of the problem we need to perform optimization on is fixed. However, the method in Shibagaki et al. 2016 detects inactive features and samples in a dynamic manner (Bonnefoy et al., 2014), i.e.,\nthey perform their method along with the training process and thus the size of the problem would keep decreasing during the iterations. Thus, comparing SIFS with this baseline in terms of the rejection ratios is inapplicable. We compare the performance of SIFS with the method in Shibagaki et al. 2016 in terms of speedup, i.e., the speedup gained by these two methods in solving problem (P\u2217) at 1, 000 pairs of parameter values. The code of the method in Shibagaki et al. 2016 is obtained from (https://github.com/husk214/s3fs).\nFig. 3 shows the rejection ratios of SIFS on the real-sim data set (other results are in the appendix). In Fig. 3, we can see that some inactive features and samples are identified in the 2nd and 3rd triggerings of ISS and IFS, which verifies the necessity of the alternating application of ISS and IFS. SIFS is efficient since it always stops in 3 times of triggering. In addition, most of (> 98%) the inactive features can be identified in the 1st triggering of IFS while identifying inactive samples needs to apply ISS two or more times. It may due to two reasons: 1) we run ISS first, which reinforces the capability of IFS due to the synergistic effect (see Sections 4.1 and 4.2), referring to the analysis below for further verification; 2) feature screening here may be easier than sample screening.\nTable 3 reports the running times of solver without and with the method in Shibagaki et al. 2016 and SIFS for solving problem (P\u2217) at 1, 000 pairs of parameter values on real data\nsets. The speedup gained by SIFS is up to 300 times on real-sim, rcv1-train and rcv1-test. Moreover, SIFS significantly outperforms the method in Shibagaki et al. 2016 in terms of speedup\u2014by about 30 to 40 times faster on the aforementioned three data sets. For data sets url and kddb, we do not report the results of the solver as the sizes of the data sets are huge and the computational cost is prohibitive. Instead, we can see that the solver with SIFS is about 25 times faster than the solver with the method in Shibagaki et al. 2016 on both data sets url and kddb. Let us take the data set kddb as an example. The solver with SIFS takes about 13 hours to solve problem (P\u2217) for 1, 000 pairs of parameter values, while the solver with the method in Shibagaki et al. 2016 needs 11 days to finish the same task.\nVerification of the Synergy Effect In Fig. 3, SIFS performs ISS (sample screening) first, while in Fig. 4, it performs IFS (feature screening) first. All the rejection ratios of the 1st triggering of IFS in Fig. 3(a)-3(d) where SIFS performs ISS first are much higher than (at least equal to) those in Fig. 4(a)-4(d) where SIFS performs IFS first. In turn, all the rejection ratios of the 1st triggering of ISS in Fig. 4(e)-4(h) where SIFS performs IFS\nfirst are also much higher than those in Fig. 3(e)-3(h) where SIFS performs ISS first. This demonstrates that the screening result of ISS can reinforce the capability of IFS and vice versa, which is the so called synergistic effect. At last, in Fig. 4 and Fig. 3, we can see that the overall rejection ratios at the end of SIFS are exactly the same. Hence, no matter which rule (ISS or IFS) we perform first in SIFS, SIFS has the same screening performances in the end, which verifies the conclusion we present in Theorem 12.\nPerformance in Solving Single Problems In the experiments above, we solve problem (P\u2217) at a grid of turning parameter values. This setting is meaningful and it arises naturally in various cases, such as cross validation (Kohavi, 1995) and feature selection (Meinshausen and Bu\u0308hlmann, 2010; Yang et al., 2015). Therefore, the results above demonstrate that our method would be helpful in such real applications. We notice that sometimes one may be interested in the model at a specific parameter value pair. To this end, we now evaluate the performance of our method in solving a single problem. Due to the space limitation, we solve sparse SVM at 16 specific parameter values pairs on the real-sim data set for examples, To be precise, \u03b2 \u2208 {0.05, 0.10, 0.5, 0.9} and each \u03b2 has 4 values of \u03b1 satisfying \u03b1/\u03b1max(\u03b2) \u2208 {0.05, 0.1, 0.5, 0.9}. For each (\u03b1i, \u03b2i), we construct a parameter value path {(\u03b1j,i, \u03b2i), j = 0, ...,M} with \u03b1j,i equally spaced at the logarithmic scale of \u03b1j,i/\u03b1max(\u03b2i) between 1 and \u03b1i/\u03b1max(\u03b2i), which implies that \u03b1max(\u03b2i) = \u03b10,i > \u03b11,i > ... > \u03b1M,i = \u03b1i. Then we use AProx-SDCA integrated with SIFS to solve problem (P\u2217) at the parameter value pairs on the path one by one and report the total time cost. It can be expected that for \u03b1i far from \u03b1max(\u03b2i), we need more points on the parameter value path in order to obtain higher rejection ratios and more significant speedups of SIFS and otherwise we need fewer ones. In this experiment, we fix M = 50 at each \u03b1i for convenience. For comparison, we solve the problem at (\u03b1i, \u03b2i) using AProx-SDCA with random initializations, which would be faster than solving all the problems on the path. Table 4 reports the speedups achieved by SIFS at these parameter value pairs. It shows that SIFS can still obtain significant speedups. In addition, compared with the results in Tables 1 and 3, we can see that our method is much better at solving a problem sequence than solving a single problem."}, {"heading": "6.3 Experiments with Multi-class Sparse SVMs", "text": "We evaluate SIFS for multi-class Sparse SVMs on 3 synthetic data sets (syn-multi1, synmulti2, and syn-multi3) and 2 real data sets (news20 and rcv1-multiclass). Their statistics are given in Table 5.\nThe synthetic data sets are generated in a similar way as we did in Section 6.1. Each of them has K = 5 classes and each class has n/K samples. The data points of them can be written as x = [x1; x2], where x1 = [x 1 1; ...; x K 1 ] \u2208 R0.02p with xk1 \u2208 R0.02p/Kand x2 \u2208 R0.98p. If x belongs to the k-th class, then xk1 is sampled from a Gaussian distribution\nG = N(u, 0.75I) with u = 1.51, I \u2208 R(0.02p/K)\u00d7(0.02p/K) and other components in x1 are from N(0, 1). Each entry of x2 has chance \u03b7 = 0.2 to be sampled from distribution N(0, 1) and chance 1\u2212 \u03b7 to be 0.\nFig. 5 shows the scaling ratios of ISS, IFS, and SIFS on the synthetic data sets at 1, 000 pairs of parameter values. Similar to the results in sparse SVMs, SIFS totally surpasses IFS and ISS, with scaling ratios larger than 98% at any parameter value pair against 70\u2212 90%. Thus, we can expect significant speedups by integrating SIFS with the solver.\nFig. 6 and 7 present the rejection ratios of SIFS on the news20 data set (the results of other data sets can be found in the appendix). It indicates that, by alternatively applying IFS and ISS, we can finally identify most of the inactive samples (> 95%) and features (> 99%). The synergistic effect between IFS and ISS can also be found in these figures.\nAs a result of excellent performance of SIFS in identifying inactive features and samples, we observe significant speedups gained by SIFS in Table 6, which are up to 200 times on news20 and 300 times on syn-multi3. Specifically, on news20, the solver without any\nscreening method takes about 97 hours to solve the problem at 1, 000 pairs of parameter values; however, integrated with SIFS, the solver only needs less than 0.5 hour for the same task. Moreover, Table 6 also indicates that the speedup gained by SIFS is much more significant than that gained by IFS and ISS. This demonstrates again the great benefit of alternatively applying IFS and ISS in SIFS. At last, we notice that the speedup gained by\nSIFS and IFS on syn-multi3 is far greater than those gained on syn-multi1 and syn-multi2. The reason is that the convergence rate of the solver in our problem may depend on the condition number of the data matrix. The dimension of w in syn-multi3 is much larger than its sample size (50, 000 over 1, 000), which leads to a large condition number and a bad convergence rate. However, SIFS and IFS can remove the inactive features from the model, which can greatly improve the convergence rate of the solver by decreasing the condition number. This point can also be supported by the fact in the experiment on syn-multi3 that the solver with SIFS or IFS needs much fewer iterations to converge to the optima than that without SIFS or IFS."}, {"heading": "7. Conclusions", "text": "In this paper, we proposed a novel data reduction method SIFS to simultaneously identify inactive features and samples for sparse SVMs. Our major contribution is a novel framework for an accurate estimation of the primal and dual optima based on strong convexity. To the best of our knowledge, the proposed SIFS is the first static screening method that is able to simultaneously identify inactive features and samples for sparse SVMs. An appealing feature of SIFS is that all detected features and samples are guaranteed to be irrelevant to the outputs. Thus, the model learned on the reduced data is exactly identical to that learned on the full data. To show the flexibility of the proposed SIFS, we extended it to multi-class sparse SVMs. The experimental results demonstrate that, for both sparse SVMs and multi-class sparse SVMs, SIFS can dramatically reduce the problem size and the resulting speedup can be orders of magnitude. We plan to generalize SIFS to more complicated models, e.g., SVMs with a structured sparsity-inducing penalty."}, {"heading": "Acknowledgments", "text": "This work was supported by the National Basic Research Program of China (973 Program) under Grant 2013CB336500, National Natural Science Foundation of China under Grant 61233011, and National Youth Top-notch Talent Support Program."}, {"heading": "Appendix A. Detailed Proofs and More Experimental Results", "text": "In this appendix, we first present the detailed proofs of all the theorems in the main paper and then report the rest experimental results which are omitted in the experimental section."}, {"heading": "A.1 Proof for Lemma 3", "text": "Proof of Lemma 3: 1) It is the immediate conclusion of the analysis above. 2) After feature screening, the primal problem (P\u2217) is scaled into:\nmin w\u0303\u2208R|F\u0302c|\n\u03b1 2 ||w\u0303||2 + \u03b2||w\u0303||1 + 1 n n\u2211 i=1 `(1\u2212 \u3008[x\u0304i]F\u0302c , w\u0303\u3009). (scaled-P \u2217-1)\nThus, we can easily derive out the dual problem of (scaled-P \u2217-1):\nmin \u03b8\u0303\u2208[0,\u03b1] n D\u0303(\u03b8\u0303;\u03b1, \u03b2) =\n1\n2\u03b1 ||S\u03b2(\n1 n F\u0302 c [X\u0304]\u03b8\u0303)||2 + \u03b3 2n ||\u03b8\u0303||2 \u2212 1 n \u30081, \u03b8\u0303\u3009, (scaled-D\u2217-1)\nand the KKT conditions:\nw\u0303\u2217(\u03b1, \u03b2) = 1\n\u03b1 S\u03b2(\n1 n F\u0302 c [X\u0304]\u03b8\u0303 \u2217(\u03b1, \u03b2)), (scaled-KKT-1)\n[\u03b8\u0303\u2217(\u03b1, \u03b2)]i =  0, if 1\u2212 \u3008[x\u0304i]F\u0302c , w\u0303 \u2217(\u03b1, \u03b2)\u3009 < 0, 1 \u03b3 (1\u2212 \u3008[x\u0304i]F\u0302c , w\u0303 \u2217(\u03b1, \u03b2)), if 0 \u2264 1\u2212 \u3008[x\u0304i]F\u0302c , w\u0303 \u2217(\u03b1, \u03b2) \u2264 \u03b3,\n1, if 1\u2212 \u3008[x\u0304i]F\u0302c , w\u0303 \u2217(\u03b1, \u03b2) > \u03b3,\n(scaled-KKT-2)\nThen, it is obvious that w\u0303\u2217(\u03b1, \u03b2) = [w\u2217(\u03b1, \u03b2)]F\u0302c , since essentially, problem (scaled-P \u2217-1) can be derived by substituting 0 to the weights for the eliminated features in problem (P\u2217) and optimizing over the rest weights. Since the solutions w\u2217(\u03b1, \u03b2) and \u03b8\u2217(\u03b1, \u03b2) satisfy the conditions (KKT-1) and (KKT-2) and \u3008[x\u0304i]F\u0302c , w\u0303\n\u2217(\u03b1, \u03b2)\u3009 = \u3008x\u0304i,w\u2217(\u03b1, \u03b2)\u3009 for all i , we know that w\u0303\u2217(\u03b1, \u03b2) and \u03b8\u2217(\u03b1, \u03b2) satisfy the conditions (scaled-KKT-1) and (scaled-KKT-2). Thus they are the solutions of problems (scaled-P \u2217-1) and (scaled-D\u2217-1). Then, due to the uniqueness of the solution of problem (scaled-D\u2217-1), we have\n\u03b8\u2217(\u03b1, \u03b2) = \u03b8\u0303\u2217(\u03b1, \u03b2). (27)\nFrom 1) we have [\u03b8\u0303\u2217(\u03b1, \u03b2)]R\u0302c = 0 and [\u03b8\u0303 \u2217(\u03b1, \u03b2)]L\u0302c = 1. Therefore, from the dual problem\n(scaled-D\u2217), we can see that [\u03b8\u0303\u2217(C,\u03b1)]D\u0302c can be recovered from the following problem:\nmin \u03b8\u0302\u2208[0,1] |D\u0302c|\n1\n2\u03b1 ||S\u03b2(\n1 n G\u03021\u03b8\u0302 + 1 n G\u030221)||2 + \u03b3 2n ||\u03b8\u0302||2 \u2212 1 n \u30081, \u03b8\u0302\u3009.\nSince [\u03b8\u0303\u2217(\u03b1, \u03b2)]D\u0302c = [\u03b8 \u2217(\u03b1, \u03b2)]D\u0302c , the proof is therefore completed."}, {"heading": "A.2 Proof for Lemma 4", "text": "Proof of Lemma 4:\n(i) We prove this lemma by verifying that the solutions w\u2217(\u03b1, \u03b2) = 0 and \u03b8\u2217(\u03b1, \u03b2) = 1 satisfy the conditions (KKT-1) and (KKT-2).\nFirstly, since \u03b2 \u2265 \u03b2max = || 1nX\u03041||\u221e, we have S\u03b2( 1 nX\u03041) = 0. Thus w \u2217(\u03b1, \u03b2) = 0 and \u03b8\u2217(\u03b1, \u03b2) = 1 satisfy the condition (KKT-1).\nThen, for all i \u2208 [n], we have\n1\u2212 \u3008x\u0304i,w\u2217(\u03b1, \u03b2)\u3009 = 1\u2212 0 > \u03b3.\nThus, w\u2217(\u03b1, \u03b2) = 0 and \u03b8\u2217(\u03b1, \u03b2) = 1 satisfy the condition (KKT-2). Hence, they are the solutions of the primal problem (P\u2217) and the dual problem (D\u2217), respectively.\n(ii) Similar to the proof of (i), we prove this by verifying that the solutions w\u2217(\u03b1, \u03b2) = 1 \u03b1S\u03b2( 1 nX\u0304\u03b8 \u2217(\u03b1, \u03b2)) and \u03b8\u2217(\u03b1, \u03b2) = 1 satisfy the conditions (KKT-1) and (KKT-2).\n1. Case 1: \u03b1max(\u03b2) \u2264 0. Then for all \u03b1 > 0, we have\nmin i\u2208[n] {1\u2212 \u3008x\u0304i,w\u2217(\u03b1, \u03b2)\u3009}\n= min i\u2208[n] {1\u2212 1 \u03b1 \u3008x\u0304i,S\u03b2( 1 n X\u0304\u03b8\u2217(\u03b1, \u03b2))\u3009} = min i\u2208[n] {1\u2212 1 \u03b1 \u3008x\u0304i,S\u03b2( 1 n X\u03041)\u3009}\n=1\u2212 1 \u03b1 max i\u2208[n] \u3008x\u0304i,S\u03b2( 1 n X\u03041)\u3009 = 1\u2212 (1\u2212 \u03b3) 1 \u03b1 \u03b1max(\u03b2)\n\u22651 > \u03b3.\nThen, L = [n], and w\u2217(\u03b1, \u03b2) = 1\u03b1S\u03b2( 1 nX\u0304\u03b8 \u2217(\u03b1, \u03b2)) and \u03b8\u2217(\u03b1, \u03b2) = 1 satisfy the conditions (KKT-1) and (KKT-2). Hence, they are the optimal solution of the primal and dual problems (P\u2217) and (D\u2217).\n2. Case 2: \u03b1max(\u03b2) > 0. Then for any \u03b1 \u2265 \u03b1max(\u03b2), we have\nmin i\u2208[n] {1\u2212 \u3008x\u0304i,w\u2217(\u03b1, \u03b2)\u3009}\n= min i\u2208[n] {1\u2212 1 \u03b1 \u3008x\u0304i,S\u03b2( 1 n X\u0304\u03b8\u2217(\u03b1, \u03b2))\u3009} = min i\u2208[n] {1\u2212 1 \u03b1 \u3008x\u0304i,S\u03b2( 1 n X\u03041)\u3009}\n=1\u2212 1 \u03b1 max i\u2208[n] \u3008x\u0304i,S\u03b2( 1 n X\u03041)\u3009 = 1\u2212 (1\u2212 \u03b3) 1 \u03b1 \u03b1max(\u03b2) \u2265 1\u2212 (1\u2212 \u03b3) = \u03b3.\nThus, E \u222a L = [n], and w\u2217(\u03b1, \u03b2) = 1\u03b1S\u03b2( 1 nX\u0304\u03b8 \u2217(\u03b1, \u03b2)) and \u03b8\u2217(\u03b1, \u03b2) = 1 satisfy the conditions (KKT-1) and (KKT-2). Hence, they are the optimal solution of the primal and dual problems (P\u2217) and (D\u2217).\nThe proof is complete."}, {"heading": "A.3 Proof for Lemma 5", "text": "Proof of Lemma 5: Due to the \u03b1-strong convexity of the objective P (w;\u03b1, \u03b2), we have\nP (w\u2217(\u03b10, \u03b20);\u03b1, \u03b20) \u2265 P (w\u2217(\u03b1, \u03b20);\u03b1, \u03b20) + \u03b1 2 ||w\u2217(\u03b10, \u03b20)\u2212w\u2217(\u03b1, \u03b20)||2, P (w\u2217(\u03b1, \u03b20);\u03b10, \u03b20) \u2265 P (w\u2217(\u03b10, \u03b20);\u03b10, \u03b20) + \u03b10 2 ||w\u2217(\u03b10, \u03b20)\u2212w\u2217(\u03b1, \u03b20)||2,\nwhich are equivalent to\n\u03b1 2 ||w\u2217(\u03b10, \u03b20)||2 + \u03b20||w\u2217(\u03b10, \u03b20)||1 + L(w\u2217(\u03b10, \u03b20)) \u2265 \u03b1 2 ||w\u2217(\u03b1, \u03b20)||2 + \u03b20||w\u2217(\u03b1, \u03b20)||1 + L(w\u2217(\u03b1, \u03b20)) + \u03b1 2 ||w\u2217(\u03b10, \u03b20)\u2212w\u2217(\u03b1, \u03b20)||2, \u03b10 2 ||w\u2217(\u03b1, \u03b20)||2 + \u03b20||w\u2217(\u03b1, \u03b20)||1 + L(w\u2217(\u03b1, \u03b20)) \u2265 \u03b10 2 ||w\u2217(\u03b10, \u03b20)||2 + \u03b20||w\u2217(\u03b10, \u03b20)||1 + L(w\u2217(\u03b10, \u03b20)) + \u03b10 2 ||w\u2217(\u03b10, \u03b20)\u2212w\u2217(\u03b1, \u03b20)||2.\nAdding the above two inequalities together, we obtain\n\u03b1\u2212 \u03b10 2 ||w\u2217(\u03b10, \u03b20)||2 \u2265 \u03b1\u2212 \u03b10 2 ||w\u2217(\u03b1, \u03b20)||2 + \u03b10 + \u03b1 2 ||w\u2217(\u03b10, \u03b20)\u2212w\u2217(\u03b1, \u03b20)||2 \u21d2 ||w\u2217(\u03b1, \u03b20)\u2212 \u03b10 + \u03b1\n2\u03b1 w\u2217(\u03b10, \u03b20)||2 \u2264\n(\u03b1\u2212 \u03b10)2\n4\u03b12 ||w\u2217(\u03b10, \u03b20)||2. (28)\nSubstituting the prior that [w\u2217(\u03b1, \u03b20)]F\u0302 = 0 into Eq. (28), we obtain\n||[w\u2217(\u03b1, \u03b20)]F\u0302c \u2212 \u03b10 + \u03b1 2\u03b1 [w\u2217(\u03b10, \u03b20)]F\u0302c || 2\n\u2264(\u03b1\u2212 \u03b10) 2\n4\u03b12 ||w\u2217(\u03b10, \u03b20)||2 \u2212\n(\u03b10 + \u03b1) 2\n4\u03b12 ||[w\u2217(\u03b10, \u03b20)]F\u0302 || 2.\nThe proof is complete."}, {"heading": "A.4 Proof for Lemma 6", "text": "Proof of Lemma 6: Firstly, we need to extend the definition of D(\u03b8;\u03b1, \u03b2) to Rn:\nD\u0303(\u03b8;\u03b1, \u03b2) =\n{ D(\u03b8;\u03b1, \u03b2), if \u03b8 \u2208 [0, 1]n,\n+\u221e, otherwise. (29)\nDue to the strong convexity of objective D\u0303(\u03b8;\u03b1, \u03b2), we have\nD\u0303(\u03b8\u2217(\u03b10, \u03b20), \u03b1, \u03b20) \u2265 D\u0303(\u03b8\u2217(\u03b1, \u03b20), \u03b1, \u03b20) + \u03b3 2n ||\u03b8\u2217(\u03b10, \u03b20)\u2212 \u03b8\u2217(\u03b1, \u03b20)||2, D\u0303(\u03b8\u2217(\u03b1, \u03b20), \u03b10, \u03b20) \u2265 D\u0303(\u03b8\u2217(\u03b10, \u03b20), \u03b10, \u03b20) + \u03b3\n2n ||\u03b8\u2217(\u03b10, \u03b20)\u2212 \u03b8\u2217(\u03b1, \u03b20)||2.\nSince \u03b8\u2217(\u03b10, \u03b20), \u03b8 \u2217(\u03b1, \u03b20) \u2208 [0, 1]n, the above inequalities are equivalent to\n1\n2\u03b1 f\u03b20(\u03b8\n\u2217(\u03b10, \u03b20)) + \u03b3 2n ||\u03b8\u2217(\u03b10, \u03b20)||2 \u2212 1 n \u30081, \u03b8\u2217(\u03b10, \u03b20)\u3009\n\u2265 1 2\u03b1 f\u03b20(\u03b8 \u2217(\u03b1, \u03b20)) + \u03b3 2n ||\u03b8\u2217(\u03b1, \u03b20)||2 \u2212 1 n \u30081, \u03b8\u2217(\u03b1, \u03b20)\u3009+ \u03b3 2n ||\u03b8\u2217(\u03b10, \u03b20)\u2212 \u03b8\u2217(\u03b1, \u03b20)||2,\n1\n2\u03b10 f\u03b20(\u03b8\n\u2217(\u03b1, \u03b20)) + \u03b3 2n ||\u03b8\u2217(\u03b1, \u03b20)||2 \u2212 1 n \u30081, \u03b8\u2217(\u03b1, \u03b20)\u3009\n\u2265 1 2\u03b10 f\u03b20(\u03b8 \u2217(\u03b10, \u03b20)) + \u03b3 2n ||\u03b8\u2217(\u03b10, \u03b20)||2 \u2212 1 n \u30081, \u03b8\u2217(\u03b10, \u03b20)\u3009+ \u03b3 2n ||\u03b8\u2217(\u03b10, \u03b20)\u2212 \u03b8\u2217(\u03b1, \u03b20)||2.\nAdding the above two inequalities, we obtain\n\u03b3(\u03b1\u2212 \u03b10) 2n ||\u03b8\u2217(\u03b10, \u03b20)||2 \u2212 \u03b1\u2212 \u03b10 n \u30081, \u03b8\u2217(\u03b10, \u03b20)\u3009 \u2265 \u03b3(\u03b1\u2212 \u03b10) 2n ||\u03b8\u2217(\u03b1, \u03b20)||2 \u2212 \u03b1\u2212 \u03b10 n \u30081, \u03b8\u2217(\u03b1, \u03b20)\u3009+ \u03b3(\u03b10 + \u03b1) 2n ||\u03b8\u2217(\u03b10, \u03b20)\u2212 \u03b8\u2217(\u03b1, \u03b20)||2.\nThat is equivalent to\n||\u03b8\u2217(\u03b1, \u03b20)||2 \u2212 \u3008 \u03b1\u2212 \u03b10 \u03b3\u03b1 1 + \u03b10 + \u03b1 \u03b1 \u03b8\u2217(\u03b10, \u03b20), \u03b8 \u2217(\u03b1, \u03b20)\u3009\n\u2264 \u2212\u03b10 \u03b1 ||\u03b8\u2217(\u03b10, \u03b20)||2 \u2212 \u03b1\u2212 \u03b10 \u03b3\u03b1 \u30081, \u03b8\u2217(\u03b10, \u03b20)\u3009. (30)\nThat is\n||\u03b8\u2217(\u03b1, \u03b20)\u2212 ( \u03b1\u2212 \u03b10\n2\u03b3\u03b1 1 +\n\u03b10 + \u03b1\n2\u03b1 \u03b8\u2217(\u03b10, \u03b20))||2 \u2264 ( \u03b1\u2212 \u03b10 2\u03b1 )2||\u03b8\u2217(\u03b10, \u03b20)\u2212 1 \u03b3 1||2. (31)\nSubstituting the priors that [\u03b8\u2217(\u03b1, \u03b20)]R\u0302 = 0 and [\u03b8 \u2217(\u03b1, \u03b20)]L\u0302 = 1 into Eq. (31), we have\n||[\u03b8\u2217(\u03b1, \u03b20)]D\u0302c \u2212 ( \u03b1\u2212 \u03b10\n2\u03b3\u03b1 1 +\n\u03b10 + \u03b1\n2\u03b1 [\u03b8\u2217(\u03b10, \u03b20)]D\u0302c)|| 2\n\u2264 (\u03b1\u2212 \u03b10 2\u03b1 )2||\u03b8\u2217(\u03b10, \u03b20)\u2212 1 \u03b3 1||2 \u2212 ||(2\u03b3 \u2212 1)\u03b1+ \u03b10 2\u03b3\u03b1 1\u2212 \u03b10 + \u03b1 2\u03b1 [\u03b8\u2217(\u03b10, \u03b20)]L\u0302|| 2 \u2212 ||\u03b1\u2212 \u03b10 2\u03b3\u03b1 1 + \u03b10 + \u03b1 2\u03b1 [\u03b8\u2217(\u03b10, \u03b20)]R\u0302|| 2.\nThe proof is complete."}, {"heading": "A.5 Proof for Lemma 8", "text": "Before the proof of Lemma 8, we should prove that the optimization problem in (2) is equivalent to\nsi(\u03b1, \u03b20) = max \u03b8\u2208\u0398\n{ 1\nn |\u3008[x\u0304i]D\u0302c , \u03b8\u3009+ \u3008[x\u0304\ni]L\u0302,1\u3009| } , i \u2208 F\u0302c. (32)\nTo avoid notational confusion, we denote the feasible region \u0398 and \u03b8 in Eq. (2) as \u0398\u0303 and \u03b8\u0303, respectively. Then,\nmax \u03b8\u0303\u2208\u0398\u0303 {\u2223\u2223\u2223\u2223 1n [X\u0304\u03b8\u0303]i \u2223\u2223\u2223\u2223} = max \u03b8\u0303\u2208\u0398\u0303 { 1 n \u2223\u2223\u2223x\u0304i\u03b8\u0303\u2223\u2223\u2223} = max\n\u03b8\u0303\u2208\u0398\u0303\n{ 1\nn \u2223\u2223\u2223[x\u0304i]D\u0302c [\u03b8\u0303]D\u0302c + [x\u0304i]L\u0302[\u03b8\u0303]L\u0302 + [x\u0304i]R\u0302[\u03b8\u0303]R\u0302\u2223\u2223\u2223} = max\n\u03b8\u0303\u2208\u0398\u0303\n{ 1\nn |\u3008[x\u0304i]D\u0302c , [\u03b8\u0303]D\u0302c\u3009+ \u3008[x\u0304\ni]L\u0302,1\u3009| }\n= max \u03b8\u2208\u0398\n{ 1\nn |\u3008[x\u0304i]D\u0302c , \u03b8\u3009+ \u3008[x\u0304\ni]L\u0302,1\u3009| } = si(\u03b1, \u03b20).\nThe last two equations hold since [\u03b8]L\u0302 = 1, [\u03b8]R\u0302 = 0, and [\u03b8D\u0302c ] \u2208 \u0398. Proof of Lemma 8:\nsi(\u03b1, \u03b20) = max \u03b8\u2208B(c,r) { 1 n |\u3008[x\u0304i]D\u0302c , \u03b8\u3009+ \u3008[x\u0304 i]L\u0302,1\u3009|}\n= max \u03b7\u2208B(0,r) { 1 n |\u3008[x\u0304i]D\u0302c , c\u3009+ \u3008[x\u0304 i]L\u0302,1\u3009+ \u3008[x\u0304 i]D\u0302c , \u03b7\u3009|}\n= 1\nn\n( |\u3008[x\u0304i]D\u0302c , c\u3009+ \u3008[x\u0304 i]L\u0302,1\u3009|+ \u2016[x\u0304 i]D\u0302c\u2016r ) .\nThe last equality holds since \u2212\u2016[x\u0304i]D\u0302c\u2016r \u2264 \u3008[x\u0304 i]D\u0302c , \u03b7\u3009 \u2264 \u2016[x\u0304 i]D\u0302c\u2016r. The proof is complete."}, {"heading": "A.6 Proof for Theorem 9", "text": "Proof of Theorem 9: (1) It can be obtained from the rule R1. (2) It is from the definition of F\u0302 ."}, {"heading": "A.7 Proof for Lemma 10", "text": "Firstly, we need to point out that the optimization problems in Eqs. (3) and (4) are equivalent to the problems:\nui(\u03b1, \u03b20) = max w\u2208W\n{1\u2212 \u3008[x\u0304i]F\u0302c ,w\u3009}, i \u2208 D\u0302 c,\nli(\u03b1, \u03b20) = min w\u2208W\n{1\u2212 \u3008[x\u0304i]F\u0302c ,w\u3009}, i \u2208 D\u0302 c.\nThey follow from the fact that [w]F\u0302c \u2208 W and\n{1\u2212 \u3008w, x\u0304i\u3009} ={1\u2212 \u3008[w]F\u0302c , [x\u0304i]F\u0302c\u3009 \u2212 \u3008[w]F\u0302 , [x\u0304i]F\u0302 \u3009} ={1\u2212 \u3008[w]F\u0302c , [x\u0304i]F\u0302c\u3009} (since [w]F\u0302 = 0).\nProof of Lemma 10:\nui(\u03b1, \u03b20) = max w\u2208B(c,r) {1\u2212 \u3008[x\u0304i]F\u0302c ,w\u3009}\n= max \u03b7\u2208B(0,r) {1\u2212 \u3008[x\u0304i]F\u0302c , c\u3009 \u2212 \u3008[x\u0304i]F\u0302c , \u03b7\u3009}\n=1\u2212 \u3008[x\u0304i]F\u0302c , c\u3009+ max \u03b7\u2208B(0,r) {\u2212\u3008[x\u0304i]F\u0302c , \u03b7\u3009}\n=1\u2212 \u3008[x\u0304i]F\u0302c , c\u3009+ \u2016[x\u0304i]F\u0302c\u2016r.\nli(\u03b1, \u03b20) = min w\u2208B(c,r) {1\u2212 \u3008[x\u0304i]F\u0302c ,w\u3009}\n= min \u03b7\u2208B(0,r) {1\u2212 \u3008[x\u0304i]F\u0302c , c\u3009 \u2212 \u3008[x\u0304i]F\u0302c , \u03b7\u3009}\n=1\u2212 \u3008[x\u0304i]F\u0302c , c\u3009+ min \u03b7\u2208B(0,r) {\u2212\u3008[x\u0304i]F\u0302c , \u03b7\u3009}\n=1\u2212 \u3008[x\u0304i]F\u0302c , c\u3009 \u2212 \u2016[x\u0304i]F\u0302c\u2016r.\nThe proof is complete."}, {"heading": "A.8 Proof for Theorem 11", "text": "Proof of Theorem 11: (1) It can be obtained from the rule R2. (2) It is from the definitions of R\u0302 and L\u0302."}, {"heading": "A.9 Proof for Theorem 12", "text": "Proof of Theorem 12: (1) Given the reference solutions pair w\u2217(\u03b1i\u22121,j , \u03b2j) and \u03b8\n\u2217(\u03b1i\u22121,j , \u03b2j), if we do ISS first in SIFS and apply ISS and IFS for infinite times. If after s times of triggering, no new inactive features or samples are identified, then we can denote the sequence of F\u0302 , R\u0302, and L\u0302 as:\nF\u0302A0 = R\u0302A0 = L\u0302A0 = \u2205 ISS\u2212\u2192 F\u0302A1 , R\u0302A1 , L\u0302A1 IFS\u2212\u2192 F\u0302A2 , R\u0302A2 , L\u0302A2 ISS\u2212\u2192 ...F\u0302As , R\u0302As , L\u0302As IFS/ISS\u2212\u2192 ... (33) with F\u0302As = F\u0302As+1 = F\u0302As+2 = ..., R\u0302As = R\u0302As+1 = R\u0302As+2 = ... and L\u0302As = L\u0302As+1 = L\u0302As+2 = ... (34)\nIn the same way, if we do IFS first in SIFS and no new inactive features or samples are identified after t times of triggering of ISS and IFS, then the sequence can be denoted as:\nF\u0302B0 = R\u0302B0 = L\u0302B0 = \u2205 IFS\u2212\u2192 F\u0302B1 , R\u0302B1 , L\u0302B1 ISS\u2212\u2192 F\u0302B2 , R\u0302B2 , L\u0302B2 IFS\u2212\u2192 ...F\u0302Bt , R\u0302Bt , L\u0302Bt IFS/ISS\u2212\u2192 ... (35) with F\u0302Bt = F\u0302Bt+1 = F\u0302Bt+2 = ..., R\u0302Bt = R\u0302Bt+1 = R\u0302Bt+2 = ...and L\u0302Bt = L\u0302Bt+1 = L\u0302Bt+2 = ... (36)\nWe first prove that F\u0302Bk \u2286 F\u0302Ak+1, R\u0302Bk \u2286 R\u0302Ak+1 and L\u0302Bk \u2286 L\u0302Ak+1 hold for all k \u2265 0 by induction.\n1) When k = 0, the equalities F\u0302B0 \u2286 F\u0302A1 , R\u0302B0 \u2286 R\u0302A1 and L\u0302B0 \u2286 L\u0302A1 hold since F\u0302B0 = R\u0302B0 = L\u0302B0 = \u2205.\n2) If F\u0302Bk \u2286 F\u0302Ak+1, R\u0302Bk \u2286 R\u0302Ak+1 and L\u0302Bk \u2286 L\u0302Ak+1 hold, by the synergistic effect of ISS and IFS, we have that F\u0302Bk+1 \u2286 F\u0302Ak+2, R\u0302Bk+1 \u2286 R\u0302Ak+2 and L\u0302Bk+1 \u2286 L\u0302Ak+2 hold.\nThus, F\u0302Bk \u2286 F\u0302Ak+1, R\u0302Bk \u2286 R\u0302Ak+1 and L\u0302Bk \u2286 L\u0302Ak+1 hold for all k \u2265 0. Similar to the analysis in (1), we can also prove that F\u0302Ak \u2286 F\u0302Bk+1, R\u0302Ak \u2286 R\u0302Bk+1 and\nL\u0302Ak \u2286 L\u0302Bk+1 hold for all k \u2265 0. Combining (1) and (2), we can acquire\nF\u0302B0 \u2286 F\u0302A1 \u2286 F\u0302B2 \u2286 F\u0302A3 .... (37) F\u0302A0 \u2286 F\u0302B1 \u2286 F\u0302A2 \u2286 F\u0302B3 .... (38) R\u0302B0 \u2286 R\u0302A1 \u2286 R\u0302B2 \u2286 R\u0302A3 .... (39) R\u0302A0 \u2286 R\u0302B1 \u2286 R\u0302A2 \u2286 R\u0302A3 .... (40) L\u0302B0 \u2286 L\u0302A1 \u2286 L\u0302B2 \u2286 L\u0302A3 .... (41) L\u0302A0 \u2286 L\u0302B1 \u2286 L\u0302A2 \u2286 L\u0302B3 .... (42)\nBy the first equality of Eqs. (34), (37), and (38), we can obtain F\u0302Ap = F\u0302Bt . Similarly, we can obtain R\u0302Ap = R\u0302Bt and L\u0302Ap = L\u0302Bt .\n(2) If p is odd, then by Eqs. (37), (39, and (41), we have F\u0302As \u2286 F\u0302Bp+1, R\u0302As \u2286 R\u0302Bp+1, and L\u0302As \u2286 L\u0302Bp+1. Thus q \u2264 p+ 1.\nElse if p is even, then by Eqs. (38), (40), and (42), we have F\u0302As \u2286 F\u0302Bp+1, R\u0302As \u2286 R\u0302Bp+1, and L\u0302As \u2286 L\u0302Bp+1. Thus q \u2264 p+ 1.\nDoing the same analysis for q, we can obtain p \u2264 q + 1. Hence, |p\u2212 q| \u2264 1. The proof is complete."}, {"heading": "A.10 Proof for Theorem 13", "text": "Proof of Theorem 13:\nWe notice (Xki ) >w = w>k xi\u2212w>yixi. By denoting \u03c6i(a) = \u2211K k=1 `([ui]k + [a]k), it is easy\nto verify that `i(w) = \u03c6i(X > i w). Thus, the problem (m-P \u2217) can be rewritten as:\nmin w\u2208RKp\nP (w;\u03b1, \u03b2) = 1\nn n\u2211 i=1 \u03c6i(X > i w) + \u03b1 2 \u2016w\u20162 + \u03b2||w||1.\nLet zi = X > i w. The primal problem (P \u2217) is then equivalent to\nmin w\u2208Rp,z\u2208Rn\n\u03b1 2 ||w||2 + \u03b2||w||1 + 1 n n\u2211 i=1 \u03c6i(zi),\ns.t. zi = X > i w, i = 1, 2, .., n.\nThe Lagrangian then becomes\nL(w, z, \u03b8) = \u03b1\n2 ||w||2 + \u03b2||w||1 +\n1\nn n\u2211 i=1 \u03c6i(zi) + 1 n n\u2211 i=1 \u3008X>i w \u2212 zi, \u03b8i\u3009\n= \u03b1\n2 ||w||2 + \u03b2||w||1 +\n1\nn \u3008X\u03b8,w\u3009\ufe38 \ufe37\ufe37 \ufe38\n:=f1(w)\n+ 1\nn n\u2211 i=1\n(\u03c6i(zi)\u2212 \u3008zi, \u03b8i\u3009)\ufe38 \ufe37\ufe37 \ufe38 :=f2(z) . (43)\nWe first consider the subproblem minw L(w, z, \u03b8):\n0 \u2208 \u2202wL(w, z, \u03b8) = \u2202wf1(w) = \u03b1w + 1\nn X\u03b8 + \u03b2\u2202||w||1 \u21d4\n1 n X\u03b8 \u2208 \u2212\u03b1w \u2212 \u03b2\u2202||w||1 \u21d2 w = \u2212 1 \u03b1 S\u03b2( 1 n X\u03b8). (44)\nBy substituting Eq. (44) into f1(w), we obtain\nf1(w) = \u03b1\n2 ||w||2 + \u03b2||w||1 \u2212 \u3008\u03b1w + \u03b2\u2202||w||1,w\u3009 = \u2212\n\u03b1 2 ||w||2 = \u2212 1 2\u03b1 ||S\u03b2( 1 n X\u03b8)||2. (45)\nThen, we consider the problem minz L(w, z, \u03b8):\n0 =\u2207[zi]kL(w, z, \u03b8) = \u2207[zi]kf2(z) =  \u2212 1n [\u03b8i]k, if [zi]k + [ui]k < 0, 1 \u03b3n([zi]k + [ui]k)\u2212 1 n [\u03b8i]k, if 0 \u2264 [zi]k + [ui]k \u2264 \u03b3,\n1 n \u2212 1 n [\u03b8i]k, if [zi]k + [ui]k > \u03b3.\n\u21d2[\u03b8i]k =  0, if [zi]k + [ui]k < 0, 1 \u03b3 ([zi]k + [ui]k), if 0 \u2264 [zi]k + [ui]k \u2264 \u03b3,\n1, if [zi]k + [ui]k > \u03b3.\n(46)\nThus, we have\nf2(z) =\n{ \u2212 \u03b32n ||\u03b8||\n2 + 1n\u3008u, \u03b8\u3009, if [\u03b8i]k \u2208 [0, 1], \u2200i \u2208 [n], k \u2208 [K], \u2212\u221e, otherwise . (47)\nCombining Eqs.(43), (45), and (47), we obtain the dual problem:\nmin \u03b8\u2208[0,1] Kn\n1\n2\u03b1 ||S\u03b2(\n1 n X\u03b8)||2 + \u03b3 2n ||\u03b8||2 \u2212 1 n \u3008u, \u03b8\u3009."}, {"heading": "A.11 Proof for Lemma 14", "text": "Lemma 14 can be proved quite similarly with Lemma 3. Therefore, we omit this proof here."}, {"heading": "A.12 Proof for Lemma 15", "text": "Proof of Lemma 15: (i) We prove this lemma by verifying that the solutions w\u2217(\u03b1, \u03b2) = 0 and \u03b8\u2217(\u03b1, \u03b2) = u satisfy the conditions (m-KKT-1) and (m-KKT-2). Firstly, since \u03b2 \u2265 \u03b2max = || 1nXu||\u221e, we have S\u03b2( 1 nXu) = 0. Thus, w\n\u2217(\u03b1, \u03b2) = 0 and \u03b8\u2217(\u03b1, \u03b2) = u satisfy the condition (m-KKT-1). Then, for all i \u2208 [n], we have\n\u3008Xki ,w\u2217(\u03b1, \u03b2)\u3009+ [ui]k = [ui]k = { 1, if k 6= yi; 0, if k = yi; \u2200i = 1, ..., n, k = 1, ...,K.\nThus, w\u2217(\u03b1, \u03b2) = 0 and \u03b8\u2217(\u03b1, \u03b2) = u satisfy the condition (m-KKT-2). Hence, they are the solutions of the primal problem (m-P\u2217) and the dual problem (m-D\u2217), respectively.\n(ii) Similar to the proof of (i), we prove this by verifying that the solutions w\u2217(\u03b1, \u03b2) = 1 \u03b1S\u03b2(\u2212 1 nXu) and \u03b8 \u2217(\u03b1, \u03b2) = u satisfy the conditions (m-KKT-1) and (m-KKT-2).\n1. Case 1: \u03b1max(\u03b2) \u2264 0. Then for all \u03b1 > 0, we have\nif k = yi, \u3008Xki ,w\u2217(\u03b1, \u03b2)\u3009+ uki = uki = 0, if k 6= yi, \u3008Xki ,w\u2217(\u03b1, \u03b2)\u3009+ uki = 1\u2212 \u3008Xki , 1\n\u03b1 S\u03b2(\n1 n Xu)\u3009\n= 1\u2212 1 \u03b1 \u3008Xki ,S\u03b2( 1 n Xu)\u3009 \u2265 1 > \u03b3.\nThus, w\u2217(\u03b1, \u03b2) = 1\u03b1S\u03b2(\u2212 1 nXu) and \u03b8 \u2217(\u03b1, \u03b2) = u satisfy the conditions (m-KKT-1) and (m-KKT-2). Hence, they are the optimal solution of problems (m-P\u2217) and (m-D\u2217).\n2. Case 2: \u03b1max(\u03b2) > 0. Then for any \u03b1 \u2265 \u03b1max(\u03b2), we have\nif k = yi, \u3008Xki ,w\u2217(\u03b1, \u03b2)\u3009+ uki = uki = 0, if k 6= yi, \u3008Xki ,w\u2217(\u03b1, \u03b2)\u3009+ uki = 1\u2212 \u3008Xki , 1\n\u03b1 S\u03b2(\n1 n Xu)\u3009\n= 1\u2212 1 \u03b1 \u3008Xki ,S\u03b2( 1 n Xu)\u3009 \u2265 1\u2212 \u03b1max(\u03b2) \u03b1 (1\u2212 \u03b3) \u2265 1\u2212 (1\u2212 \u03b3) = \u03b3.\nThus, w\u2217(\u03b1, \u03b2) = 1\u03b1S\u03b2(\u2212 1 nXu) and \u03b8 \u2217(\u03b1, \u03b2) = u satisfy the conditions (m-KKT-1) and (m-KKT-2). Hence, they are the optimal solutions of the primal and dual problems (m-P\u2217) and (m-D\u2217).\nThe proof is complete."}, {"heading": "A.13 More Experimental Results", "text": "Below, we report the rejection ratios of SIFS on syn1 (Fig. 8), syn3 (Fig. 9), rcv1-train (Fig. 10), rcv1-test(Fig. 11), url (Fig. 12), kddb (Fig. 13), syn-multi1 (Fig. 14), syn-multi3 (Fig. 15), and rcv1-multiclass (Fig. 16), which are omitted in the main paper. In all the figures, the first row presents the rejection ratios of feature screening and the second row presents those of sample screening."}], "year": 2019, "references": [{"title": "Convex analysis and monotone operator theory in Hilbert spaces, volume 408", "authors": ["Heinz H Bauschke", "Patrick L Combettes"], "year": 2011}, {"title": "Dimensionality reduction via sparse support vector machines", "authors": ["Jinbo Bi", "Kristin Bennett", "Mark Embrechts", "Curt Breneman", "Minghu Song"], "venue": "The Journal of Machine Learning Research,", "year": 2003}, {"title": "A dynamic screening principle for the lasso", "authors": ["Antoine Bonnefoy", "Valentin Emiya", "Liva Ralaivola", "R\u00e9mi Gribonval"], "venue": "In Signal Processing Conference (EUSIPCO),", "year": 2014}, {"title": "Fast support vector machine training and classification on graphics processors", "authors": ["Bryan Catanzaro", "Narayanan Sundaram", "Kurt Keutzer"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "year": 2008}, {"title": "Libsvm: a library for support vector machines", "authors": ["Chih-Chung Chang", "Chih-Jen Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST),", "year": 2011}, {"title": "Safe feature elimination in sparse supervised learning", "authors": ["Laurent El Ghaoui", "Vivian Viallon", "Tarek Rabbani"], "venue": "Pacific Journal of Optimization,", "year": 2012}, {"title": "Liblinear: A library for large linear classification", "authors": ["Rong-En Fan", "Kai-Wei Chang", "Cho-Jui Hsieh", "Xiang-Rui Wang", "Chih-Jen Lin"], "venue": "The Journal of Machine Learning Research,", "year": 2008}, {"title": "The entire regularization path for the support vector machine", "authors": ["Trevor Hastie", "Saharon Rosset", "Robert Tibshirani", "Ji Zhu"], "venue": "The Journal of Machine Learning Research,", "year": 2004}, {"title": "Statistical learning with sparsity: the lasso and generalizations", "authors": ["Trevor Hastie", "Robert Tibshirani", "Martin Wainwright"], "year": 2015}, {"title": "A dual coordinate descent method for large-scale linear svm", "authors": ["Cho-Jui Hsieh", "Kai-Wei Chang", "Chih-Jen Lin", "S Sathiya Keerthi", "Sellamanickam Sundararajan"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "year": 2008}, {"title": "Text categorization with support vector machines: Learning with many relevant features", "authors": ["Thorsten Joachims"], "year": 1998}, {"title": "A study of cross-validation and bootstrap for accuracy estimation and model selection", "authors": ["Ron Kohavi"], "venue": "In International Joint Conference on Artificial Intelligence,", "year": 1995}, {"title": "Facial expression recognition in image sequences using geometric deformation features and support vector machines", "authors": ["Irene Kotsia", "Ioannis Pitas"], "venue": "Image Processing, IEEE Transactions on,", "year": 2007}, {"title": "Celer: a fast solver for the lasso with dual extrapolation", "authors": ["Mathurin Massias", "Joseph Salmon", "Alexandre Gramfort"], "venue": "In International Conference on Machine Learning,", "year": 2018}, {"title": "Stability selection", "authors": ["Nicolai Meinshausen", "Peter B\u00fchlmann"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "year": 2010}, {"title": "A topographic support vector machine: Classification using local label configurations", "authors": ["Johannes Mohr", "Klaus Obermayer"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2004}, {"title": "Svm pauc tight: a new support vector method for optimizing partial auc based on a tight convex upper bound", "authors": ["Harikrishna Narasimhan", "Shivani Agarwal"], "venue": "In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining,", "year": 2013}, {"title": "Gap safe screening rules for sparse-group lasso", "authors": ["Eugene Ndiaye", "Olivier Fercoq", "Alexandre Gramfort", "Joseph Salmon"], "venue": "Advances in Neural Information Processing Systems", "year": 2016}, {"title": "Safe screening of non-support vectors in pathwise svm computation", "authors": ["Kohei Ogawa", "Yoshiki Suzuki", "Ichiro Takeuchi"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "year": 2013}, {"title": "Convex analysis", "authors": ["Ralph Tyrell Rockafellar"], "venue": "Princeton university press,", "year": 1970}, {"title": "Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization", "authors": ["Shai Shalev-Shwartz", "Tong Zhang"], "venue": "Mathematical Programming,", "year": 2016}, {"title": "Pegasos: Primal estimated sub-gradient solver for svm", "authors": ["Shai Shalev-Shwartz", "Yoram Singer", "Nathan Srebro", "Andrew Cotter"], "venue": "Mathematical programming,", "year": 2011}, {"title": "Simultaneous safe screening of features and samples in doubly sparse modeling", "authors": ["Atsushi Shibagaki", "Masayuki Karasuyama", "Kohei Hatano", "Ichiro Takeuchi"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "year": 2016}, {"title": "Strong rules for discarding predictors in lasso-type problems", "authors": ["Robert Tibshirani", "Jacob Bien", "Jerome Friedman", "Trevor Hastie", "Noah Simon", "Jonathan Taylor", "Ryan J Tibshirani"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "year": 2012}, {"title": "Scaling svm and least absolute deviations via exact data reduction", "authors": ["Jie Wang", "Peter Wonka", "Jieping Ye"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "year": 2014}, {"title": "A safe screening rule for sparse logistic regression", "authors": ["Jie Wang", "Jiayu Zhou", "Jun Liu", "Peter Wonka", "Jieping Ye"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2014}, {"title": "Lasso screening rules via dual polytope projection", "authors": ["Jie Wang", "Peter Wonka", "Jieping Ye"], "venue": "Journal of Machine Learning Research,", "year": 2015}, {"title": "The doubly regularized support vector machine", "authors": ["Li Wang", "Ji Zhu", "Hui Zou"], "venue": "Statistica Sinica,", "year": 2006}, {"title": "Fast lasso screening tests based on correlations", "authors": ["Zhen James Xiang", "Peter J Ramadge"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "year": 2012}, {"title": "Detecting genetic risk factors for alzheimer\u2019s disease in whole genome sequence data via lasso screening", "authors": ["Tao Yang", "Jie Wang", "Qian Sun", "Derrek P Hibar", "Neda Jahanshad", "Li Liu", "Yalin Wang", "Liang Zhan", "Paul M Thompson", "Jieping Ye"], "venue": "In Biomedical Imaging (ISBI),", "year": 2015}, {"title": "Latent support measure machines for bag-of-words data classification", "authors": ["Yuya Yoshikawa", "Tomoharu Iwata", "Hiroshi Sawada"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2014}, {"title": "Scaling up sparse support vector machine by simultaneous feature and sample reduction", "authors": ["Weizhong Zhang", "Bin Hong", "Wei Liu", "Jieping Ye", "Deng Cai", "Xiaofei He", "Jie Wang"], "venue": "In Proceedings of The 34th International Conference on Machine Learning,", "year": 2017}], "id": "SP:4605edac92e29b169e7226bd450abe6def6b3982", "authors": [{"name": "Bin Hong", "affiliations": []}, {"name": "Weizhong Zhang", "affiliations": []}, {"name": "Wei Liu", "affiliations": []}, {"name": "Jieping Ye", "affiliations": []}, {"name": "Deng Cai", "affiliations": []}, {"name": "Xiaofei He", "affiliations": []}, {"name": "Jie Wang", "affiliations": []}], "abstractText": "Sparse support vector machine (SVM) is a popular classification technique that can simultaneously learn a small set of the most interpretable features and identify the support vectors. It has achieved great successes in many real-world applications. However, for large-scale problems involving a huge number of samples and ultra-high dimensional features, solving sparse SVMs remains challenging. By noting that sparse SVMs induce sparsities in both feature and sample spaces, we propose a novel approach, which is based on accurate estimations of the primal and dual optima of sparse SVMs, to simultaneously identify the inactive features and samples that are guaranteed to be irrelevant to the outputs. Thus, we can remove the identified inactive samples and features from the training phase, leading to substantial savings in the computational cost without sacrificing the accuracy. Moreover, we show that our method can be extended to multi-class sparse support vector machines. To the best of our knowledge, the proposed method is the first static feature and sample reduction method for sparse SVMs and multi-class sparse SVMs. Experiments on both synthetic and real data sets demonstrate that our approach significantly outperforms state-of-the-art methods and the speedup gained by our approach can be orders of magnitude.", "title": "Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction"}