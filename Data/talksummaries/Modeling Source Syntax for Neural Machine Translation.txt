0	24	Recently the sequence to sequence model (seq2seq) in neural machine translation (NMT) has achieved certain success over the state-ofthe-art of statistical machine translation (SMT) on various language pairs (Bahdanau et al., 2015; Jean et al., 2015; Luong et al., 2015; Luong and Manning, 2015).
1	49	However, Shi et al. (2016) show that the seq2seq model still fails to capture a lot of deep structural details, even though it is capable of learning certain implicit source syntax from sentence-aligned parallel corpus.
4	21	In this paper, we show that syntax can be well exploited in NMT explicitly by taking advantage of source-side syntax to improve the translation accuracy.
10	68	Statistics on our development set show that one forth of Chinese noun phrases are translated into discontinuous phrases in English, indicating the substantial disrespect of syntax in NMT translation.1 Figure 1 (b) shows another example with over translation, where the noun phrase 两/liang 个/ge 女孩/nvhai is translated twice in English.
13	16	Alternatively, we address how to incorporate explicitly the source syntax to improve the NMT translation accuracy with the expectation of alleviating the issues above in general.
21	19	Such bi-RNN encodes not only the word itself but also its left and right context, which can provide important evidence for its translation.
29	20	Our purpose is to inform the NMT model the structural context of each word in its corresponding parse tree with the goal that the learned annotation vectors (h1, ..., hm) encode not I love dogs only the information of words and their surroundings, but also structural context in the parse tree.
33	18	For example, Figure 3(c) shows the structural label sequence of Figure 3(b) in a simple way following a depth-first traversal order.
35	35	There is no doubt that the structural label sequence is much longer than its word sequence.
39	14	Likewise, w2 maps to l5 and w3 to l7.
40	15	That is to say, we use l3’s learned annotation vector as w1’s label annotation vector.
42	50	In the next, we first propose two different encoders to augment word annotation vector with its corresponding label annotation vector, each of which consists of two RNNs 3: in one encoder, the two RNNs work independently (i.e., Parallel RNN Encoder) while in another encoder the two RNNs work in a hierarchical way (i.e., Hierarchical RNN Encoder).
48	21	On the other hand, the structural label RNN takes the structural label sequence of the word sequence as input and obtains a label annotation vector for each label.
52	21	We put the word RNN in the upper layer because each item in the word sequence can map into an item in the structural label sequence, while this does not hold if the order of the two RNNs is reversed.
74	24	All the other settings are the same as in Bahdanau et al.(2015).
83	17	For translation, a beam search with size 10 is employed.
91	25	When running on a single GPU GeForce GTX 1080, the baseline model speeds 153 minutes per epoch with 14K updates while the proposed structural label RNNs in both Parallel RNN and Hierarchical RNN systems only increases the training time by about 6% (thanks to the small size of structural label embeddings and annotation vectors), and the Mixed RNN system spends 26% more training time to cater the triple sized input sequence.
92	39	Comparison with the baseline NMT model (RNNSearch) While all the three proposed NMT models outperform RNNSearch, the Parallel RNN system and the Hierarchical RNN system achieve similar accuracy (e.g., 36.6 v.s.
97	16	This is very consistent with other studies on Chinese-to-English translation (Mi et al., 2016; Tu et al., 2017b; Wang et al., 2017).
105	48	We think that the bad behavior of NMT systems towards long sentences (e.g., length of 50) is due to the following two reasons: (1) the maximum source sentence length limit is set as 50 in training, 9 making the learned models not ready to translate sentences over the maximum length limit; (2) NMT systems tend to stop early for long input sentences.
109	26	To evaluate alignment performance, we report the alignment error rate (AER) (Och and Ney, 2003) in Table 2.
115	46	There are some phrases, such as noun phrases (NPs), prepositional phrases (PPs) that we usually expect to have a continuous translation.
121	19	To estimate the over translation generated by NMT, we propose ratio of over translation (ROT): ROT = ∑ wi t(wi) |w| (1) where |w| is the number of words in consideration, t(wi) is the times of over translation for word wi.
137	122	For example, if misunderstand/VB is divided into units of mis and understand, we construct substructure (VB (VB-F mis) (VB-I understand)).
155	15	In this paper, we have investigated whether and how source syntax can explicitly help NMT to improve its translation accuracy.
156	46	To obtain syntactic knowledge, we linearize a parse tree into a structural label sequence and let the model automatically learn useful information through it.
157	26	Specifically, we have described three different models to capture the syntax knowledge, i.e., Parallel RNN, Hierarchical RNN, and Mixed RNN.
159	23	It is also interesting to note that the simplest model (i.e., Mixed RNN) achieves the best performance, resulting in obtaining significant improvements of 1.4 BLEU points on NIST MT 02 to 05.
161	16	Our analysis shows that there is still much room for NMT translation to be consistent with source syntax.
162	24	In our future work, we expect several developments that will shed more light on utilizing source syntax, e.g., designing novel syn- tactic features (e.g., features showing the syntactic role that a word is playing) for NMT, and employing the source syntax to constrain and guild the attention models.
