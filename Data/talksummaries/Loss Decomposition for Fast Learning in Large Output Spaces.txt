0	34	Large output spaces are ubiquitous in several machine learning problems today: for example, extreme multiclass or multilabel classification problems with many classes, language modeling with big vocabularies, or metric learning with a large number of pairwise distance constraints.
1	12	In all such problems, a key bottleneck in training models is evaluation of the loss function and its gradient.
5	33	One body of work imposes structure over the output space, such as low-rank (Yu et al., 2014), treestructure (Prabhu & Varma, 2014), locally low-rank (Bhatia et al., 2015), or hierarchical factorization (Morin & Bengio, 2005; Mnih & Hinton, 2009).
26	22	Let X denote the input space and Y the output space, and let K := |Y|.
34	16	The key to our method for reducing the complexity of loss and gradient evaluation will be the following linear structural assumption on the class of scoring functions F : there is an embedding dimension parameter D ∈ N such that for every f ∈ F , we can associate a weight matrix W ∈ RK×D and feature map φ : X → RD so that for all x ∈ X , f(x) = Wφ(x).
38	9	The main challenge here is to construct data structures that preprocess the matrix W so that good approximations to the loss f(xi,Pi) and its gradient can be computed without computing the vector f(x) entirely: i.e. we desire sublinear (in K) time computation of such approximations given access to an appropriate data structure.
40	18	In extreme classification problems, popular classification loss functions include Cross-Entropy Loss L(z,P) := ∑ k∈P log (∑K j=1 exp(zj) ) − zk (2) and Max-Margin Loss L(z,P) := [ max k∈P,j∈N zj − zk + 1 ] + .
50	19	(7) It is easy to see that such scoring functions satisfy the structural assumption (1): for the scoring function f given by the squared Mahalanobis distance parameterized by ψ and M , the matrix W consists of the rows 〈−ψ(y)>Mψ(y), 2ψ(y)>M ,−1〉 for each y ∈ Y , and φ(x) = 〈1,ψ(x)>,ψ(x)>Mψ(x)〉>.
54	63	This clearly fits the structural assumption (1): the rows of the matrixW are the embeddings φ(y) for all y ∈ X .
57	15	2: Construct a sparse approximation z̃ for f(x) by setting z̃k = f(x)k for k ∈ S ∪P , and z̃k = 0 for k 6∈ S ∪P .
61	64	Similarly, for the Cross-Entropy loss (2), the coordinates of the gradient corresponding to the negative classes are dominated by the ones with the highest score; the gradient coordinates decrease exponentially as the scores decrease.
65	26	Then for each sample x, we can compute the highest scores by querying this data structure with the target vector φ(x) and some reasonable threshold τ , computing approximations to the loss and gradient from the returned vectors (and treating all other scores as 0).
69	12	The main difficulty in applying this approach in practice is the curse of dimensionality: the dependence on D is exponential for exact methods, and even for approximate methods, such as Locality-Sensitive Hashing, the cost still implicitly depends on the dimension as points become far apart when the intrinsic dimensionality is high (Li & Malik, 2017).
72	34	In order to apply and analyze the technique, we need the loss functions to be smooth (i.e. have Lipschitz continuous gradients).
73	37	For non-smooth losses like Max-Margin loss (3), we apply Nesterov’s smoothing technique (Nesterov, 2005), which constructs a surrogate loss function with guaranteed approximation quality by adding a strongly convex term to the Fenchel conjugate of the loss: Lµ(z) := max α 〈z,α〉 − ( L∗(α) + µ 2 ‖α‖2 ) .
74	21	(10) Here, µ is a smoothing parameter that ensures that the surrogate loss has 1µ Lipschitz continuous gradients while approximating the original loss function to withinO(µ).
76	34	denotes the projection onto the bi-simplex C = {α | ∑ k∈N αk = ∑ k∈P −αk ≤ 1, αN ≥ 0, αP ≤ 0}.
77	16	The Smoothed Max-Margin loss and its gradient can again be computed using the largest few scores.
78	28	We now describe our loss decomposition method.
80	21	In this section, we will keep (x,P) fixed, and we will drop the dependence on P in L for convenience and simply use the notation L(f(x)) and∇L(f(x)).
83	10	,W (B) be the corresponding block partitioning of W obtained by grouping together the columns corresponding to the coordinates in each block.
84	75	Similarly, let φ(1)(x),φ(2)(x), .
85	64	,φ(B)(x) be the conformal partitioning of the coordinates of φ(x).
86	49	Now define the overall score vector z := f(x) = Wφ(x), and per-chunk score vectors zj = W (j)φ(j)(x), for j ∈ [B].
87	27	Then we have z = ∑B j=1 zj , in other words, we have a decomposition of the score vector.
88	26	The following theorem states that the loss of a decomposable score vector can itself be decomposed into several parts connected through a set of message variables.
89	12	This theorem is key to decoupling the variables into lower dimensional chunks that can be optimized separately via an efficient MIPS data structure.
94	39	On the other hand, if we set λj = 1Bz− zj for all j ∈ [B], we have L(z) = 1B ∑B j=1 L(B(zj + λj)).
95	8	Theorem (1) is the basis for our algorithm for computing approximations to the loss and its gradient.
96	17	This approximation is computed by approximately solving the convex minimization problem (12) without computing the whole score vector z, using a form of descent method on the λj variables (which we refer to as “message passing”).
