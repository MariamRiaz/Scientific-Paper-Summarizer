0	90	Procedural text is ubiquitous (e.g., scientific protocols, news articles, how-to guides, recipes), but is challenging to comprehend because of the dynamic nature of the world being described.
1	145	Comprehending such text requires a model of the actions described in the text and the state changes they produce, so that questions about the states of entities at different timepoints can be answered (Bosselut et al., 2018).
3	24	Recent work – such as EntNet (Henaff et al., 2017), QRN (Seo et al., 2017b), ProLocal/ProGlobal (Dalvi et al., ∗*Niket Tandon and Bhavana Dalvi Mishra contributed equally to this work.
4	15	Procedural Text: How hydroelectric electricity is generated: 1 Water flows downwards thanks to gravity.
5	11	2 The moving water spins the turbines in the power plant.
41	13	Given: • A paragraph of procedural text S = an ordered set of sentences {s1, ..., sT } describing a sequence of actions1 about a given topic (a word or phrase).
42	15	• A set of entities E = {e j} representing the en- tities mentioned in the procedure or process.
43	15	Each entity e j is denoted by the set of its mentions in the paragraph, e.g., {leaf, leaves} • A set of properties P = {pk} of entities to be tracked (e.g., location, existence) predict: • The state of each entity e j after each sentence sk, where an entity’s state is the values of all its properties {pk}.
44	13	For example, in Figure 2, the state of the water after step 2 is {location(water) = turbine; exists(water) = true}.
47	13	In the state tracking task of Bosselut et al. (2018), six properties (temperature, shape, etc.)
50	38	ProPara contains 488 paragraphs (3100 sentences) of a particular genre of procedural text, namely science processes (e.g., how hydroelectricity is generated).
56	26	For example, for the ProPara dataset, we model K = 4 types of state change: move, create, destroy, and none.
59	43	If a parameterized state change is predicted, then the model also must predict its parameter values from the paragraph.
64	9	For each sentence and entity, the encoder first uses a bidirectional LSTM to encode the sentence and indicator variables identifying which entity is currently being considered (Figure 3).
72	13	The encoder operates over every (st, e j) ∈ S × E pair to create an encoded representation ct j of the action described by sentence st, as applied to entity e j.
74	12	This novel feature allows us to model different effects on different entities by the same action.
84	56	To decode the action vectors ct j into their resulting state changes they imply, each is passed through a feedforward layer to generate logit(π jt ), a set of logistic activations over the K possible state changes π j t for entity e j in sentence st. (For ProPara, there are K = 4 possible state changes: move, create, destroy, none).
85	17	These logits denote how likely each state change π jt is for entity e j at sentence st.
92	55	While hard constraints remove impossible state change predictions, there may also be other state changes that are implausible with respect to background knowledge.
96	21	We add this bias as an additional term (the second term below) when scoring the addition of πt+1 to the sequence so far Πt: φ′(πt+1) = |E|∑ j=1 ( λ logit(π jt+1) + (1 − λ) log P(π jt+1|e j, topic) ) (6) where λ is a learned parameter controlling the degree of bias.
107	79	By formulating procedural text comprehension as a structured prediction task, we can introduce commonsense knowledge as hard and soft constraints into the model, allowing nonsensical and unlikely predictions to be avoided, and allowing the system to recover from early mistakes.
108	10	Hard constraints are introduced by defining the (boolean) function over a candidate sequence of state changes: allowable(Π) used in Equation 4.
110	152	The first three below are based on basic “laws of physics” or commonsense (CS) and are universally applicable: CS-1: An entity must exist before it can be moved or destroyed CS-2: An entity cannot be created if it already exists CS-3: An entity cannot change until it is mentioned in the paragraph The next three constraints are observed in the training data: D-1: Maximum number of toggles for an entity between Exists and not Exist ≤ fmax_toggles D-2: Max fraction of entities that are changed per sentence ≤ fentities_per_sentence D-3: Max fraction of sentences in which an entity changes ≤ fsentences_per_entity The thresholds used in D-1, D-2 and D-3 are hyperparameters that can be tuned on the dev set.
111	108	Soft constraints are introduced by defining the prior probabilities used in Equation 6: P(π j|e j, topic) that entity e j undergoes state change π j in a sentence of text about topic.
112	68	These probabilities are used to re-rank the candidate event sequences during decoding (see Equation 6).
115	28	For a given state change π j, entity e j, and topic, we first gather a corpus of Web sentences mentioning that topic (using Bing search APIs), then count the number of times x that the entity is described as undergoing that state change (e.g., that water is said to MOVE).
117	17	We then use an existing rulebase, derived from VerbNet, that contains rules that map SRL frames to state changes, e.g., e1/ARG0 “absorbs”/VERB e2/ARG1 =⇒ e2 MOVES (Clark et al., 2018).
118	16	Although the rules and SRL labels are incomplete and noisy, redundancy in the corpus provides some robustness when estimating the frequency x.
122	24	Note that our model architecture itself is agnostic to the source and quantity of hard and soft constraints.
125	50	Given a paragraph and set of entities as input, the task is to answer four templated questions, whose answers are deterministically computed from the state change sequence: Q1.
