1	22	A common approach to deal with the high amount of variance often found in this type of data is to perform spelling normalization (Piotrowski, 2012), which is the mapping of historical spelling variants to standardized/modernized forms (e.g. vnd→ und ‘and’).
3	15	Nevertheless, we explore framing the spelling normalization task as a character-based sequence-to-sequence transduction problem, and use encoder–decoder recurrent neural networks (RNNs) to induce our transduction models.
4	51	This is similar to models that have been proposed for neural machine translation (e.g., Cho et al. (2014)), so essentially, our approach could also be considered a specific case of character-based neural machine translation.
7	31	Furthermore, we explore using an auxiliary task for which data is more readily available, namely grapheme-tophoneme mapping (word pronunciation), to regularize the induction of the normalization models.
9	16	Contributions Our contributions are as follows: • We are, to the best of our knowledge, the first to propose and evaluate encoder-decoder architectures for historical text normalization.
10	17	• We evaluate several such architectures across 44 datasets of Early New High German.
13	13	332 • We analyze the above architectures and show that the MTL architecture learns attention from the auxiliary task, making the attention mechanism largely redundant.
15	17	In sum, we both push the state-of-the-art in historical text normalization and present an analysis that, we believe, brings us a step further in understanding the benefits of multi-task learning.
16	26	Normalization For the normalization task, we use a total of 44 texts from the Anselm corpus (Dipper and Schultz-Balluff, 2013) of Early New High German.1 The corpus is a collection of manuscripts and prints of the same core text, a religious treatise.
18	30	For example, the modern German word Frau ‘woman’ can be spelled as fraw/vraw (Me), frawe (N2), frauwe (St), fraüwe (B2), frow (Stu), vrowe (Ka), vorwe (Sa), or vrouwe (B), among others.2 All texts in the Anselm corpus are manually annotated with gold-standard normalizations following guidelines described in Krasselt et al. (2015).
20	31	Nonetheless, the remaining 44 texts are still quite short for machine-learning standards, ranging from about 4,200 to 13,200 tokens, with an average length of 7,350 tokens.
49	29	For the lexicon, we use all word forms from CELEX (cf.
57	22	If a = (a1, ..., an) is the encoder’s output and ht is the decoder’s hidden state at timestep t, we first calculate a context vector ẑt as a weighted combination of the output vectors ai: ẑt = n∑ i=1 αiai (1) The weights αi are derived by feeding the encoder’s output and the decoder’s hidden state from the previous timestep into a multilayer perceptron, called the attention model (fatt): α = softmax(fatt(a, ht−1)) (2) We then modify the decoder by conditioning its internal states not only on the previous hidden state ht−1 and the previously predicted output character yt−1, but also on the context vector ẑt: it = σ(Wi[ht−1, yt−1, ẑt] + bi) ft = σ(Wf [ht−1, yt−1, ẑt] + bf ) ot = σ(Wo[ht−1, yt−1, ẑt] + bo) gt = tanh(Wg[ht−1, yt−1, ẑt] + bg) ct = ft ct−1 + it gt ht = ot tanh(ct) (3) In Eq.
64	34	This choice is motivated by the relationship between phonology and orthography, in particular the observation that spelling variation often stems from phonological variation.
65	32	We train our multi-task learning architecture by alternating between the two tasks, sampling one instance of the auxiliary task for each training sample of the main task.
72	63	For the final evaluation, we set the size of the embedding and the recurrent LSTM layers to 128, applied a dropout of 0.3 to the input of each recurrent layer, and trained the model on mini-batches with 50 samples each for a total of 50 epochs (in the multi-task learning setup, mini-batches contain 50 samples of each task, and epochs are counted by the size of the training set for the main task only).
95	48	5: That multi-task learning can induce strategies for focusing attention comparable to attention strategies for recurrent neural networks.
100	49	In some cases (such as gewarnet ‘warned’), only the models with attention or multi-task learning produce the correct normalization, but even when they are wrong, they often agree on the prediction (e.g. dicke, herzel).
125	23	We find that out of the 210.9 word errors that the base model produces on average across all test sets (comprising 1,000 tokens each), attention resolves 47.7, while multi-task learning resolves an average of 45.4 errors.
132	29	Therefore, if two models produce similar saliency matrices for a given input/output pair, they have learned to focus on similar parts of the input during the prediction.
133	23	Our hypothesis is that the attentional and the multi-task learning model should be more similar in terms of saliency scores than either of them compared to the base model.
134	82	Figure 5 shows a plot of the saliency matrices generated from the word pair czeychen – zeichen ‘sign’.
135	31	Here, the scores for the attentional and the MTL model indeed correlate by ρ = 0.615, while those for the base model do not correlate with either of them.
136	40	A systematic analysis across 19,000 word pairs (where all models agree on the output) shows that this effect only holds for longer input sequences (≥ 7 characters), with a mean ρ = 0.303 (±0.177) for attentional vs. MTL model, while the base model correlates with either of them by ρ < 0.21.
148	42	Specifically, we demonstrated the aptitude of multi-task learning to mitigate the shortage of training data for the named task.
149	16	We included a multifaceted analysis of the effects that MTL introduces to our models and the resemblance that it bears to attention mechanisms.
150	53	We believe that this analysis is a valuable contribution to the understanding of MTL approaches also beyond spelling normalization, and we are confident that our observations will stimulate further research into the relationship between MTL and attention.
152	41	Currently, we only consider word forms in isolation, which is problematic for ambiguous cases (such as jn, which can normalize to in ‘in’ or ihn ‘him’) and conceivably makes the task harder for others.
153	84	Reranking the predictions with a language model could be one possible way to improve on this.
155	36	Such an approach could also deal with the issue of tokenization differences between the historical and the modern text, which is another challenge often found in datasets of historical text.
