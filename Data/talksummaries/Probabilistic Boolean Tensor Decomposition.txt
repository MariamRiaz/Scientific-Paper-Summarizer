4	33	Examples include network interactions at different time-points, gene-gene interactions for different individuals or any two-way data under different experimental conditions.
7	43	(1) Here, x[n] is a single tensor entry and [n] denotes a tuple of indices [n1, .
9	28	(1) says that an observation is 1, if and only if there exist one or more latent dimensions in which all corresponding factors are 1.
10	58	This leads to easily interpretable factor matrices, Fk, where each latent dimension denotes a subset of entries along mode k, for instance subsets of objects, properties and conditions that occur jointly in the data.
11	90	This can be thought of as a particular type of canonical polyadic (CP) decomposition, as illustrated in Fig.
12	81	A different graphical intuition starts from the factor rows and considers the 3-way Boolean tensor product as a three-stage template assignment procedure.
13	47	Rows of a factor matrix F1 are one-dimensional binary templates of size of the first tensor mode.
15	36	In the same manner, the third factor matrix, F3, indicates which disjunction of these 2D patterns appears in each slice of the tensor, and so forth.
18	59	In contrast to previous approaches, the probabilistic framework readily treats missing data, allows for tensor completion and integration of prior information.
19	26	Importantly, the latent representations are amenable to informative and intuitive interpretation.
31	43	Each tensor entry, x[n] = x[n1,...,nK ], is a Bernoulli random variable that equals 1 with a probability greater 12 if the following holds true ∃ l : fnl = 1 ∀ n ∈ [n1, .
33	87	(3) The term inside parenthesis evaluates to 1 if the condition in eq.
49	41	This naturally modelled by the noisy-OR.
53	105	Even if we were to believe in the independent inhibition of latent causes, usually only one latent cause triggers an event.
67	37	The second term is a product of the co-parents in all other latent dimensions and evaluates to zero if any of them explains away observation x[n].
77	53	Algorithm 1 Computation of the full conditional of fnkl m = 0 // initialise integer count for the sum in eq.
81	19	continue (next [n]) end for end for m = m + x̃nd end for p(fnkl|.)
82	20	= ( 1 + exp ( −λ · f̃nkl ·m ))−1 After a sampling all factor entries from their full conditional, we set the noise-parameter, λ, to its conditional MAP estimate and repeat until convergence before drawing posterior samples.
87	30	We see that a uniform prior, p(σ(λ)) = Beta(α=1, β=1), corresponds to applying Laplace’s rule of succession to the maximum likelihood estimate.
101	22	Yet another alternative is to compute the estimate from the factor matrix mean, thus taking posterior uncertainty but no higher-order correlations into account.
102	36	Denoting the posterior mean of a factor entry as f̂ , we have p(x[n] = 1|.)
105	35	We simulate random 3-way tensors, X , of size 20×20×20 and vary rank L, expected density E(X) and noise-level.
108	41	We introduce noise by flipping each entry in the tensor with a given probability.
116	78	2(b) compares the reconstruction accuracies of the posterior predictive compared to the reconstruction based solely on the MAP estimates of each factor.
117	39	We can see, that posterior averaging plays an important role only in scenarios with high noise levels.
120	26	2(a), that dbtf features a similar or higher reconstruction accuracy with respect to the noisy training data.
168	37	4(a) shows posterior means of the time-specific and (one of the two equivalent) individual-specific factor matrices.
178	27	We partition the time-points into weeks of the course and the seating positions into six regions front/back - left/centre/right.
192	32	(c) Representations of cancer patients – Each column corresponds to one out of approximately 8,000 cancer patients and indicates which of the latent properties of relative expression among approximately 2,000 genes they exhibit.
223	44	15 as well as by the AGE-RAGE signalling pathway recovered in Dim.
