0	11	The recent proliferation of malicious fake news in social media has been a source of widespread concern.
1	66	Given that more than 62% of U.S. adults turn to social media for news, with 18% doing so often, fake news can have potential real-world consequences on a large scale (Gottfried & Shearer, 2016).
2	30	For example, within the final three months of the 2016 U.S. presidential election, news stories that favored either of the two nominees–later proved to be fake– were shared over 37 million times on Facebook alone, and over half of those who recalled seeing fake news stories believed them (Allcott & Gentzkow, 2017).
3	11	An analysis by Buzzfeed News shows that the top 20 false election stories from hoax websites generated nearly 1.5 million more user engagement activities on Facebook than the top 20 stories from reputable major news outlets (Silverman, 2016).
5	12	Policies to counter fake news can be categorized by the level of manual oversight and the aggressiveness of action required.
8	10	Such direct action on the offending news requires a high degree of human oversight, which can be costly and slow and also may violate civil rights.
19	15	We present the first formulation of fake news mitigation as the problem of optimal point process intervention in a network.
29	19	Steering user activities by adding external incentives to the exogenous intensity of Hawkes processes was first considered in (Farajtabar et al., 2014).
45	19	Let t` be the time of the `-th event, then the Hawkes process can be represented by the counting process N(t) = P t`t h(t t`) that tracks the number of events up to time t, where h(t) is the standard Heaviside function such that h(t) = 1 if t 0 and = 0 if t < 0.
46	53	The conditional intensity function of a point process is defined as the probability of observing an event in an infinitesimal window given the history.
51	17	, Nn(t) that can also mutually excite one another, and the conditional intensity (t) := 1(t), .
55	78	We model the activities of both fake news and mitigation events as MHP in the network.
56	76	Basically, MHP is a networked point process model with dependent dimensions (nodes), and can capture the underlying dynamics of networks and activities (Blundell et al., 2012; Xu et al., 2016; Guo et al., 2015).
57	28	, Fn(t) > 2 Nn0 , where Fi(t) counts the number of times user i shares a piece of news from the fake campaign up to time t. Similarly, define M(t) = M1(t), .
59	24	Correspondingly, we have 2 intensity functions: M (t) = ( M 1 (t), .
73	10	For each stage k, xk (defined later) is the state of the whole MDP that encodes all the information from previous stages and uk is the current control imposed at this stage.
74	27	Let Mki (t;xk, uk) := P j bij R t ⌧k dMj(s) be the number of times user i is exposed to the mitigation campaign by time t 2 [⌧k, ⌧k+1) within stage k, then the goal is to steer the expected total number of exposure Mki (t;xk, uk) using uk, s.t.
75	14	the sum of reward functions R(x k , u k ) (rigorously defined in sec.
76	29	By observing the counting process in previous stages (summarized in a sequence of xk) and taking the future uncertainty into account, the control problem is to design a policy ⇡ such that the controls uk = ⇡(xk) can maximize the total discounted objective E[ P1 k=0 k R k ], where 2 (0, 1] is the discount rate and Rk is the observed reward at stage k. In addition, we may have constraints on the amount of control, such as a budget constraint on the sum of all interventions to users at each stage, or a cap over the amount of intensity a user can handle.
82	18	In this paper, we consider two types of reward functions R(x, u): 1) Correlation Maximization: One possible way is to require correlation between mitigation exposures and fake news exposures: people exposed more to fake news should also be exposed more to the true news, to counter the fake news campaign.
109	23	The optimal value function satisfies the Bellman equation: V ⇡ (x) = E[R(x,⇡(x))] + E[V ⇡(x0)], (5) where x0 is the next state after taking action based on policy ⇡ at state x.
121	17	Algorithm 2 Real-time fake news mitigation Input: network A, learned w⇡ , feature (·), discount repeat Observe state x of the network activities u = argmaxa{E[R(x, a)] + E[V ⇡(x0)|a,w⇡]} Add u to base exogenous intensity µ and generate mitigation event times {ti} using point process model Create posts at times {ti} using campaigner accounts until end of campaign Let (xs) = s 2 RD, E[ (x0s)] = 0s 2 RD, and r ⇡ s = E[R(xs,⇡(xs))] 2 R. Then define matrices of current features = [ >1 ; .
140	26	Yet, in our setup, learning the value function is sufficient, by writing the action-value function as Q⇡(x, u) = E[R(x, u)+V ⇡(x0)], and observing that the learned model of the multivariate Hawkes process enables analytical computation of the expectation (see Appendix C for details): E[V ⇡(x0)] = nX i=1 L 1X l=1 w ⇡ ln+iz k 1 M,(l 1)n+i + w ⇡ nL+ln+iz k 1 F,(l 1)n+i + nX i=1 wiE[zkM,i] + wnL+iE[zkF,i] + w⇡2nL+1, E[R(x, u)] = 1 n E[zkM ]>B>B E[zkF ], % correlation E[R(x, u)] = 1 n E[zkM > B > B z k M ] 1 n E[zkF > B > B z k F ] + 2 n E[zkM ]>B>B E[zkF ].
147	51	We evaluate our fake news mitigation framework by both simulated and real-time real-world experiments and show our approach, Least-squares Temporal Difference (LTD), significantly outperforms several state-of-the-art methods and alternatives: CEC (an approximate dynamic programming), OPL (an open loop optimization), CLS (a centrality based measure), EXP (an exposure based centrality measure), and RND (the random policy).
195	19	Using five Twitter accounts, each of which made five posts on machine learning topics at random times per day for a span of two months (Nov.-Dec. 2016), we accumulated a network of 1894 real users with 23407 directed edges in total.
204	17	So, to save time in real interventions, we only test CEC from the first and EXP from the second pair, and compare them with the random policy (RND) and with our algorithm (LTD).
206	10	6 shows the performance of our results compared to competitors.
211	20	This means that we decreased the difference in exposure to the two processes to less 2.6 per user, which is considerable improvement.
216	92	In addition to retweets, users can also “like” a post, indicating that they were exposed to fake or real news; while we measured this, we did not include it in the reward.
217	40	Future experiments can use these two observations to widen the experimental scope and more accurately measure the effectiveness of a mitigation strategy.
