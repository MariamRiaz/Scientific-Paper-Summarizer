2	10	RNNs can be ‘unwrapped’ and thought of as very deep feedforward networks, with weights shared between each layer.
3	34	Computation proceeds one step at a time, like the trajectory of an ordinary differential equation when solving an initial value problem.
7	47	Changing the value of the forcing function (analogously, of an input sequence element) at any point in the sequence will affect the values everywhere else.
8	13	The bidirectional recurrent network (Schuster and Paliwal, 1997) attempts to addresses this problem by creating a network with two recurrent hidden states – one that progresses in the forward direction and one that progresses in the reverse.
9	10	This allows information to flow in both directions, but each state can only consider information from one direction.
11	12	We provide a novel mechanism that is able to process information in both directions, with the motivation being a program which iterates over itself until convergence.
25	12	Computation proceeds linearly, with each next state depending only on inputs and previously computed hidden states.
27	25	This leads to an implicit set of equations for the entire sequence of hidden states, which can be thought of as a single tensor H: H = [h1, h2, .
28	21	, hn] This yields a system of nonlinear equations.
32	21	In this setup, we have the following variables: data X labels Y parameters θ and functions: input layer transformation ξ = g(θ,X) implicit hidden layer def.
33	39	H = F (θ, ξ,H) loss function L = `(θ,H, Y ) Our implicit definition function, F , is made up of local state transitions and forms a system of nonlinear equations that require solving, denoting n as the length of the input sequence and hs, he as boundary states: h1 = f(hs, h2, ξ1) .
35	67	We computed this via an approximate Newton solve, where we successively refine an approximation Hn of H: Hn+1 = Hn − (I −∇HF )−1(Hn − F (Hn)) Let k be the dimension of a single hidden state.
36	16	(I −∇HF ) is a sparse matrix, since∇HF is zero except for k pairs of n × n block matrices, corresponding to the influence of the left and right neighbors of each state.
39	7	In order to train the model, we perform stochastic gradient descent.
40	50	We take the gradient of the loss function: ∇θL = ∇θ`+∇H`∇θH The gradient of the hidden units with respect to the parameters can found via the implicit definition: ∇θH = ∇θF +∇HF∇θH +∇ξF∇θξ = (I −∇HF )−1 (∇θF +∇ξF∇θξ) where the factorization follows from the noting that (I −∇HF )∇θH = ∇θF +∇ξF∇θξ.
45	62	The switch variable s is determined by a competition between two sigmoidal units sp and sn, representing the contributions of the previous and next hidden states, respectively.
55	18	For this reason, we found it helpful to run Newton’s method from two separate initializations for each element in our batch, one selected randomly and the other set to a “one-step” approximation: Hidden states of a traditional GRU were computed in both forward (hfi ) and reverse (hbi ) directions, and hi was initialized to f(hfi−1, h b i+1, ξi).
59	23	Our task was to find the point at which a random walk, in the spirit of the Wiener Process (Durrett, 2010), changes from a zero to nonzero mean.
73	31	The INN outperforms the best b-LSTM in the more challenging cases where the bias size b is small.
75	21	Part-of-speech tagging fits naturally in the sequence labeling framework, and has the advantage of a standard dataset that we can use to compare our network with other techniques.
76	5	To train a partof-speech tagger, we simply let L be a softmax layer transforming each hidden unit output into a part of speech tag.
77	16	Our input encoding ξ, is a concatenation of three sets of features, adapted from (Huang et al., 2015): first, word vectors for 39,000 case-insensitive vocabulary words; second, six additional ‘word vector’ components indicating the presence of the top-2000 most common prefixes and suffixes of words, for affix lengths 2 to 4; and finally, eight other binary features to indicate the presence of numbers, symbols, punctuation, and more rich case data.
78	18	We trained the Part of Speech (POS) tagger on the Penn Treebank Wall Street Journal corpus (Marcus et al., 1993), blocks 0-18, validated on 19-21, and tested on 22-24, per convention.
91	39	However, our matrix is nonsymmetric, and we expect κ to vary from problem to problem.
93	58	For the random walk experiment with b = 0.5, we found the the average run time for a given sequence length to be approximately 0.17n1.8, with r2 = 0.994.
94	68	Note that the exponent would have been larger had we not truncated the number of BiCG-STAB iterations to 40, as the inner iteration frequently hit this limit for larger n. However, the average number of Newton iterations did not go above 10, indicating that exiting early from the BiCG-STAB loop did not prevent the Newton solver from converging.
95	73	Run times for the other random walk experiments were very similar, indicating run time does not depend on b; However, for the POS task runtime was 0.29n1.3, with r2 = 0.910.
97	72	In future work, we intend to consider implicit variations of other architectures, such as the LSTM, as well as additional, more challenging, and/or data-rich applications.
98	67	We also plan to explore ways to speed up the computation of (I−∇HF )−1.
99	77	Potential speedups include approximating the hidden state values by reducing the number of Newton and/or BiCG-STAB iterations, using cached previous solutions as initial values, and modifying the gradient update strategy to keep the batch full at every Newton iteration.
