{"sections": [{"text": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1171\u20131181 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1108"}, {"heading": "1 Introduction", "text": "Document summarization is a task to generate a fluent, condensed summary for a document, and keep important information. As a useful technique to alleviate the information overload people are facing today, document summarization has been extensively investigated. Efforts on document summarization can be categorized to extractive and abstractive methods. Extractive methods produce the summary of a document by extracting sentences from the original document. They have the advantage of producing fluent sentences and\npreserving the meaning of original documents, but also inevitably face the drawbacks of information redundancy and incoherence between sentences. Moreover, extraction is far from the way humans write summaries.\nOn the contrary, abstractive methods are able to generate better summaries with the use of arbitrary words and expressions, but generating abstractive summaries is much more difficult in practice. Abstractive summarization involves sophisticated techniques including meaning representation, content organization, and surface realization. Each of these techniques has large space to be improved (Yao et al., 2017). Due to the immaturity of natural language generation techniques, fully abstractive approaches are still at the beginning and cannot always ensure grammatical abstracts.\nRecent neural networks enable an end-to-end framework for natural language generation. Success has been witnessed on tasks like machine translation and image captioning, together with the abstractive sentence summarization (Rush et al., 2015). Unfortunately, the extension of sentence abstractive methods to the document summarization task is not straightforward. Encoding and decoding for a long sequence of multiple sentences, currently still lack satisfactory solutions (Yao et al., 2017). Recent abstractive document summarization models are yet not able to achieve convincing performance, with a considerable gap from extractive methods.\nIn this paper, we review the key factors of document summarization, i.e., the saliency, fluency, coherence, and novelty requirements of the generated summary. Fluency is what neural generation models are naturally good at, but the other factors are less considered in previous neural abstractive models. A recent study (Chen et al., 2016) starts to consider the factor of novelty, using a distraction mechanism to avoid redundancy. As far as we\n1171\nknow, however, saliency has not been addressed by existing neural abstractive models, despite its importance for summary generation.\nIn this work, we study how neural summarization models can discover the salient information of a document. Inspired by the graph-based extractive summarization methods, we introduce a novel graph-based attention mechanism in the encoderdecoder framework. Moreover, we investigate the challenges of accepting and generating long sequences for sequence-to-sequence (seq2seq) models, and propose a new hierarchical decoding algorithm with a reference mechanism to generate the abstractive summaries. The proposed method is able to tackle the constraints of saliency, nonredundancy, information correctness, and fluency under a unified framework.\nWe conduct experiments on two large-scale corpora with human generated summaries. Experimental results demonstrate that our approach consistently outperforms previous neural abstractive summarization models, and is also competitive with state-of-the-art extractive methods.\nWe organize the paper as follows. Section 2 introduces related work. Section 3 describes our method. In Section 4 we present the experiments and have discussion. Finally in Section 5 we conclude this paper."}, {"heading": "2 Related Work", "text": ""}, {"heading": "2.1 Extractive Summarization Methods", "text": "Document summarization can be categorized to extractive methods and abstractive methods. Extractive methods extract sentences from the original document to form the summary. Notable early works include (Edmundson, 1969; Carbonell and Goldstein, 1998; McDonald, 2007). In recent years much progress has also been made under traditional extractive frameworks (Li et al., 2013; Dasgupta et al., 2013; Nishikawa et al., 2014).\nNeural networks have also been widely investigated on the extractive summarization task. Earlier works explore to use deep learning techniques in the traditional framework (Kobayashi et al., 2015; Yin and Pei, 2015; Cao et al., 2015a,b). More recent works predict the extraction of sentences in a more data-driven way. Cheng and Lapata (2016) propose an encoder-decoder approach where the encoder learns the representation of sentences and documents while the decoder classifies each sentence using an attention mechanism. Nal-\nlapati et al. (2017) propose a recurrent neural network (RNN)-based sequence model for extractive summarization of documents. Neural sentence extractive models are able to leverage large-scale training data and achieve performance better than traditional extractive summarization methods."}, {"heading": "2.2 Abstractive Summarization Methods", "text": "Abstractive summarization aims at generating the summary based on understanding the input text. It involves multiple subproblems like simplification, paraphrasing, and fusion. Previous research is mostly restricted in one or a few of the subproblems or specific domains (Woodsend and Lapata, 2012; Thadani and McKeown, 2013; Cheung and Penn, 2014; Pighin et al., 2014; Sun et al., 2015).\nAs for neural network models, success is achieved on sentence abstractive summarization. Rush et al. (2015) train a neural attention model on a large corpus of news documents and their headlines, and later Chopra et al. (2016) extend their work with an attentive recurrent neural network framework. Nallapati et al. (2016) introduce various effective techniques in the RNN seq2seq framework. These neural sentence abstraction models are able to achieve state-of-the-art results on the DUC competition of generating headlinelevel summaries for news documents.\nSome recent works investigate neural abstractive models on the document summarization task. Cheng and Lapata (2016) also adopt a word extraction model, which is restricted to use the words of the source document to generate a summary, although the performance is much worse than the sentence extractive model. Nallapati et al. (2016) extend the sentence summarization model by trying a hierarchical attention architecture and a limited vocabulary during the decoding phase. However these models still investigate few properties of the document summarization task. Chen et al. (2016) first attempt to explore the novelty factor of summarization, and propose a distraction-based attentional model. Unfortunately these state-ofthe-art neural abstractive summarization models are still not competitive to extractive methods, and there are several problems remain to be solved."}, {"heading": "3 Our Method", "text": ""}, {"heading": "3.1 Overview", "text": "In this section we introduce our method. We adopt an encoder-decoder framework, which is\nwidely used in machine translation (Bahdanau et al., 2014) and dialog systems (Mou et al., 2016), etc. In particular, we use a hierarchical encoderdecoder framework similar to (Li et al., 2015), as shown in Figure 1. The main distinction of this work is that we introduce a graph-based attention mechanism which is illustrated in Figure 1b, and we propose a hierarchical decoding algorithm with a reference mechanism to tackle the difficulty of abstractive summary generation. In the following parts, we will first introduce the encoder-decoder framework, and then describe the graph-based attention and the hierarchical decoding algorithm."}, {"heading": "3.2 Encoder", "text": "The goal of the encoder is to map the input document to a vector representation. A document d is a sequence of sentences d = {si}, and a sentence si is a sequence of words si = {wi,k}. Each word wi,k is represented by its distributed representation ei,k, which is mapped by a word embedding matrix Ev. We adopt a hierarchical encoder framework, where we use a word encoder encword to encode the words of a sentence si into the sentence representation, and use a sentence encoder encsent to encode the sentences of a document d into the document representation. The input to the word encoder is the word sequence of a sentence, appended with an \u201c<eos>\u201d token indicating the end of a sentence. The word encoder sequentially updates its hidden state after receiving each word, as hi,k = encword(hi,k\u22121, ei,k). The last hidden state (after the word encoder receives \u201c<eos>\u201d) is denoted as hi,\u22121, and used as the embedding representation of the sentence si, denoted as xi. A sentence encoder is used to sequentially receive the embeddings of the sentences, given by hi = encsent(hi\u22121,xi). A pseudo sentence of an \u201c<eod>\u201d token is appended at the end of the document to indicate the end of the whole document. The hidden state after the sentence encoder receives \u201c<eod>\u201d is treated as the representation of the input document c = h\u22121.\nWe use the Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) as both the word encoder encword and sentence encoder encsent. In particular, we adopt the variant of LSTM structure in (Graves, 2013)."}, {"heading": "3.3 Decoder with Attention", "text": "The decoder is used to generate output sentences {s\u2032j} according to the representation of the input\nsentences. We also use an LSTM-based hierarchical decoder framework to generate the summary, because the summary typically comprises several sentences. The sentence decoder decsent receives the document representation c as the initial state h \u2032 0 = c, and predicts the sentence representations sequentially, by h \u2032 j = decsent(h \u2032 j\u22121,x \u2032 j\u22121), where x \u2032 j\u22121 is the encoded representation of the previously generated sentence s \u2032 j\u22121. The word decoder decword receives a sentence representation h \u2032 j as the initial state h \u2032 j,0 = h \u2032 j , and predicts the word representations sequentially, by h \u2032 j,k = decword(h \u2032 j,k\u22121, ej,k\u22121), where ej,k\u22121 is the embedding of the previously generated word. The predicted word representations are mapped to vectors of the vocabulary size dimension, and then normalized by a softmax layer as the probability distribution of generating the words in the vocabulary. A word decoder stops when it generates the \u201c<eos>\u201d token and similarly the sentence decoder stops when it generates the \u201c<eod>\u201d token.\nIn primitive decoder models, c is the same for generating all the output words, which requires c to be a sufficient representation for the whole input sequence. The attention mechanism (Bahdanau et al., 2014) is usually introduced to alleviate the burden of remembering the whole input sequence, and to allow the decoder to pay different attention to different parts of input at different generation states. The attention mechanism sets a different cj when generating sentence j, by cj = \u2211 i \u03b1 j ihi. \u03b1ji indicates how much the i-th original sentence si contributes to generating the j-th sentence. \u03b1 j i is usually computed as:\n\u03b1ji = e \u03b7 ( hi,h \u2032 j )\n\u2211 l e \u03b7(hl,h\u2032j)\n(1)\nwhere \u03b7 is the function modeling the relation between hi and h \u2032 j . \u03b7 can be defined using various functions including \u03b7 (a,b) = aTb, \u03b7 (a,b) = aTMb, and even a non-linear function achieved by a multi-layer neural network. In this paper we use \u03b7 (a,b) = aTMb where M is a parameter matrix."}, {"heading": "3.4 Graph-based Attention Mechanism", "text": "Traditional attention computes the importance score of a sentence si, when generating sentence s \u2032 j , according to the relation between the hidden state hi and current decoding state h \u2032 j , as shown\nin Figure 1a. This attention mechanism is useful in scenarios like machine translation and image captioning, because the model is able to learn a relevance mapping between the input and output. However, for document summarization, it is not easy for the model to learn how to summarize the salient information of a document, i.e., which sentences are more important to a document.\nTo tackle this challenge, we learn from graphbased extractive summarization models TextRank (Mihalcea and Tarau, 2004) and LexRank (Erkan and Radev, 2004), which are based on the PageRank (Page et al., 1999) algorithm. These unsupervised graph-based models show good ability to identify important sentences in a document. The underlying idea is that a sentence is important in a document if it is heavily linked with many important sentences (Wan, 2010).\nIn graph-based extractive summarization, a graph G is constructed to rank the original sentences. The vertices V are the set of n sentences to be considered, and the edges E are the relations between the sentences, which are typically modeled by the similarity of sentences. Let W \u2208 Rn\u00d7n be the adjacent matrix. Then the saliency scores of the sentences are determined by making use of the global information on the graph recursively, as:\nf (t+ 1) = \u03bbWD\u22121f(t) + (1\u2212 \u03bb)y (2)\nwhere f = [f1, . . . , fn] \u2208 Rn denotes the rank scores of the n sentences. f(t) denotes the rank scores at the t-th iteration. D is a diagonal matrix with its (i, i)-element equal to the sum of the i-th column of W . Assume we use hi as the representation of si, and W (i, j) = hTi Mhj , where M is a parameter matrix to be learned. \u03bb is a damping\nfactor. y \u2208 Rn with all elements equal to 1/n. The solution of f can be calculated using the closedform:\nf = (1\u2212 \u03bb)(I \u2212 \u03bbWD\u22121)\u22121y (3) In the graph model, the importance score of a sentence si is determined by the relation between hi and the {hl} of all other sentences. Relatively, in traditional attention mechanisms, the importance (attention) score \u03b1ji is determined by the relation between hi and h \u2032 j , regardless of other original sentences. In our model we hope to combine the two effects, and compute the rank scores of the original sentences regarding h\n\u2032 j , so that the\nimportance scores of original sentences are different when decoding different state h\n\u2032 j , denoted\nby f j . In our model we use the scores f j to compute the attention. Therefore, h\n\u2032 j should be\nconsidered in the graph model. Inspired by the query-focused graph-based extractive summarization model (Wan et al., 2007), we realize this by applying the idea of topic-sensitive PageRank (Haveliwala, 2002), which is to rank the sentences with the concern of their relevance to the topic. We treat the current decoding state h\n\u2032 j as the topic and\nadd it into the graph as the 0-th pseudo-sentence. Given a topic T , the topic-sensitive PageRank is similar to Eq. 3 except that y becomes:\nyT = { 1 |T | i \u2208 T 0 i /\u2208 T\n(4)\nTherefore yT is always a one hot vector and only y0 = 1, indicating the 0-th sentence is s \u2032 j . Denote W j as the new adjacent matrix added with h \u2032 j , and D\nj as the new diagonal matrix corresponding to W j . Then the convergence score vector f j contains the importance scores for all the\ninput sentences when generating sentence s \u2032 j , as:\nf j = (1\u2212 \u03bb)(I \u2212 \u03bbW jDj\u22121)\u22121yT (5)\nThe new scores f j can be used to compute the graph-based attention when decoding h\n\u2032 j , to find\nthe sentences which are both globally important and relevant to current decoding state h\n\u2032 j . In-\nspired by (Chen et al., 2016) we adopt a distraction mechanism to compute the final attention value \u03b1ji , which subtracts the rank scores of the previous step, to penalize the model from attending to previously attended sentences, and also help to normalize the ranked scores f j . The graph-based attention is finally computed as:\n\u03b1ji = max(f ji \u2212 f j\u22121 i , 0)\u2211\nl ( max(f jl \u2212 f j\u22121 l , 0) ) (6)\nwhere f0 is initialized with all elements equal to 1/n. The graph-based attention will only focus on those sentences ranked higher over the previous decoding step, so that it concentrates more on the sentences which are both salient and novel. Both Eq. 5 and Eq. 6 are differentiable; thus we can use the graph-based attention function Eq. 6 to replace the traditional attention function Eq. 1, and the neural model using the graph-based attention can also be trained using traditional gradientbased methods."}, {"heading": "3.5 Model Training", "text": "The loss function L of the model is the negative log likelihood of generating summaries over the training set D:\nL = \u2211\n(Y,X)\u2208D \u2212 log p(Y |X; \u03b8) (7)\nwhere X = { x1, . . . , x|X| } and Y ={\ny1, . . . , y|Y | }\ndenote the word sequences of a document and its summary respectively, including the \u201c<eos>\u201d and \u201c<eod>\u201d tokens for structure information. Then\nlog p(Y |X; \u03b8) = |Y |\u2211\n\u03c4=1\nlog p (y\u03c4 | {y1, . . . , y\u03c4\u22121} , c; \u03b8)\n(8) and log p (y\u03c4 | {y1, . . . , y\u03c4\u22121} , c; \u03b8) is modeled by the LSTM encoder and decoder. We use the Adamax (Kingma and Ba, 2014) gradient-based\noptimization method to optimize the model parameters \u03b8."}, {"heading": "3.6 Decoding Algorithm", "text": "We find there are several problems during the generation of summary, including out-of-vocabulary (OOV) words, information incorrectness, error accumulation and repetition. These problems make the generated abstractive summaries far from satisfactory. In this work, we propose a hierarchical decoding algorithm with a reference mechanism to tackle these difficulties, which effectively improves the quality of generated summaries.\nAs OOV words frequently occur in name entities, we can first identify the entities of a document using NLP toolkit like Stanford CoreNLP1. Then we prefix every entity with an \u201c@entity\u201d token and a number indicating how many words the entity has. We hope the entity prefixes can help better deal with entities which have more than one word, and help improve the accuracy of recovering OOV words in entities. After decoding we recover the OOV words by matching entities in the original document according to the contexts.\nFor the hierarchical decoder, a major challenge is that same sentences or phrases are often repeated in the output. A beam search strategy may help to alleviate the repetition in a sentence, but the repetition in the whole generated summary is remained a problem. The word-level beam search is not easy to be extended to the sentence level. The reason is that the K-best sentences generated by a word decoder will mostly be similar to each other, which is also noticed by Li et al. (2016).\nIn this paper we propose a hierarchical beam search algorithm with a reference mechanism. The hierarchical algorithm comprises K-best word-level beam search and N -best sentencelevel beam search. At the word level, the only difference to vanilla beam search is that we add an additional term to the score p\u0303(y\u03c4 ) of generating word y\u03c4 , and now score(y\u03c4 ) = p\u0303(y\u03c4 ) + \u03b3 (ref(Y\u03c4\u22121 + y\u03c4 , s\u2217)\u2212 ref(Y\u03c4\u22121, s\u2217)), where Y\u03c4\u22121 = {y1, . . . , y\u03c4\u22121} and p\u0303(y\u03c4 ) = log p (y\u03c4 |Y\u03c4\u22121, c; \u03b8). s\u2217 is an original sentence to refer to. ref is a function which calculates the ratio of bigram overlap between two texts. The added term aims to favor the generated word y\u03c4 with improving the bigram overlap between current generated summary Y\u03c4\u22121 and the target orig-\n1http://stanfordnlp.github.io/CoreNLP/\ninal sentence s\u2217. At the word decoder level, the reference mechanism helps to both improve the information correctness and avoid redundancy. Because the reference score is based on the bigram overlap improvement to the whole generated summary Y\u03c4\u22121, the awareness of previously generated sentences also helps alleviate sentence-level redundancy. A factor \u03b3 is introduced to control the influence of the reference mechanism. Note that because of the non-optimal search, the generated sentence will still be different to the original sentence even with an extremely large \u03b3.\nAt the sentence level, N -best sentence beam is to keep the N generated sentences by referring to N different original sentences, which have the highest attention scores and have not been used as a reference. With referring to N different sentences, the N candidate sentences are guaranteed diverse. Sentence-level beam search is realized by maximizing the accumulated score of all the sentences generated."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Dataset", "text": "We conduct experiments on two large-scale corpora of CNN and DailyMail, which have been widely used in neural document summarization tasks. The corpora are originally constructed in (Hermann et al., 2015) by collecting human generated abstractive highlights from the news stories in the CNN and DailyMail website. The statistics and split of the two datasets are listed in Table 1."}, {"heading": "4.2 Implementation", "text": "We use the corpora which are already provided with labeled entities (Nallapati et al., 2016). The documents and summaries are first lowercased and tokenized, and all digit characters are replaced with the \u201c#\u201d symbol, similar to (Nallapati et al., 2016, 2017). We keep the 40,000 most frequently occurring words and other words are replaced with the \u201c<OOV>\u201d token.\nWe use Theano2 for implementation. For the word encoder and decoder we use three layers of LSTM, and for the sentence encoder and decoder we use one layer of LSTM. The dimension of hidden vectors are all 512. We use pre-trained GloVe (Pennington et al., 2014) vectors3 for the initialization of word vectors, which will be further trained in the model. The dimension of word vectors is 100. \u03bb is set to 0.9. The parameters of Adamax are set to those provided in (Kingma and Ba, 2014). The batch size is set to 8 documents, and an epoch is set containing 10,000 randomly sampled documents. Convergence is reached within 200 epochs on the DailyMail dataset and 120 epochs on the CNN dataset. It takes about one day for every 30 epochs on a GTX-1080 GPU card. \u03b3 is tuned on the validation set and the best choice is 300. The beam sizes for word decoder and sentence decoder are 15 and 2, respectively."}, {"heading": "4.3 Evaluation", "text": "We adopt the widely used ROUGE (Lin, 2004) toolkit for evaluation. We first compare with the reported results in (Chen et al., 2016) including various traditional extractive methods and a state-of-the-art abstractive model (DistractionM3) on the CNN dataset, as shown in Table 2. Uni-GRU is a non-hierarchical seq2seq baseline model. In Table 3 we compare our method with the results of state-of-the-art neural summarization methods reported in recent papers. Extractive models include NN-SE (Cheng and Lapata, 2016) and SummaRuNNer (Nallapati et al., 2017), while SummaRuNNer-abs is also an extractive model similar to SummaRuNNer but is trained directly on the abstractive summaries. Moreover, we include several baselines for comparison, including the baselines reported in (Cheng and Lapata, 2016) although they are tested on 500 samples of the test set. LREG is a feature based method using linear regression. NN-ABS is a neural abstractive baseline which is a simple hierarchical extension of (Rush et al., 2015). NN-WE is the abstractive model which restricts the generation of words from the original document. Lead-3 is a strong extractive baseline that uses the lead three sentences as the summary.\nIn Table 4 we compare our model with the abstractive attentional encoder-decoder models in\n2https://github.com/Theano/Theano 3http://nlp.stanford.edu/projects/glove\n(Nallapati et al., 2016), which leverage several effective techniques and achieve state-of-the-art performance on sentence abstractive summarization tasks. The words-lvt2k and words-lvt2k-ptr are flat models and words-lvt2k-hieratt is a hierarchical extension.\nResults in Table 2 show our abstractive method is able to outperform traditional extractive methods and the distraction-based abstractive model. The results in Tables 3 and 4 show that our method has considerable improvement over neural abstractive baselines, and is able to outperform stateof-the-art neural extractive methods. An interesting observation is the results of the hierarchical model in Table 4 are lower than the flat models, which may demonstrate the difficulty for a traditional attention model to identify the important information in a document.\nWe also conducted human evaluation on 20 random samples from the DailyMail test set and compared the summaries generated by our method with the outputs of Lead-3, NN-SE (Cheng and\nLapata, 2016) and Distraction (Chen et al., 2016). The output summaries of NN-SE are provided by the authors, and the output summaries of Distraction are achieved by running the code provided by the authors on the DailyMail dataset. Three participants were asked to compare the generated summaries with the human summaries, and assess each summary from four independent perspectives: (1) How informative the summary is? (2) How concise the summary is? (3) How coherent (between sentences) the summary is? (4) How fluent, grammatical the sentences of a summary are? Each property is assessed with a score from 1 (worst) to 5 (best). The average results are presented in Table 5.\nAs shown in Table 5, our method consistently outperforms the previous state-of-the-art abstractive method Distraction. Compared with extractive methods, our method is able to generate more informative and concise summaries, which shows the advantage of abstractive methods. The Distraction method in fact usually produces the shortest summaries, but the conciseness score is low mainly because sometimes it generates repeated sentences. The repetition also causes Distraction to achieve a low coherence score. Concerning coherence and fluency, our abstractive method achieves slightly better scores than NN-SE, while not surprisingly Lead-3 gets the best scores. The fluency scores show the good ability of the abstractive model to generate fluent and grammatical sentences."}, {"heading": "4.4 Model Validation", "text": "We conduct experiments to see how the model\u2019s performance is affected by the choice of the hyperparameters. For efficiency we test on 500 random samples from the DailyMail test set. Figure 2a shows the maximum average Rouge-2 F1-score achieved when the model is trained using different \u03bb values within 200 and 300 epochs. When using a larger \u03bb, the performance is better and the convergence is faster. When \u03bb = 1.0 the model fails to train because of running into a singular matrix.\nFigure 2b shows the results achieved when using different \u03b3 values in the hierarchical decoding algorithm. \u03b3 = 0 is the baseline of the traditional decoding algorithm which does not refer to the original document. The poor results indicate that even the model is able to learn to identify the salient information in the original document, the performance is limited by the model\u2019s ability of generating a long output sequence. That may be a reason why simple extensions of seq2seq models fail on the abstractive document summarization task. The performance is significantly improved using a reasonable \u03b3, and the optimal \u03b3 value is consistent with the one chosen on the validation set. When using an extremely large \u03b3, the permanence begins to decrease, because the model will copy too much from the original document, and at this time the generated text also becomes less fluent. Results show that introducing the reference mechanism in the hierarchical beam search is very effective. The \u03b3 factor significantly affects the results, but the optimal value is easy to be decided on a validation set.\nWe also conduct ablation experiments on the CNN dataset to verify the effectiveness of the proposed model. Results on the CNN test set are shown in Table 6. \u201cw/o GraphAtt\u201d is to replace\nthe graph-based attention by a traditional attention function. \u201cw/o SentenceBeam\u201d is to remove the sentence-level beam search. \u201cw/o BeamSearch\u201d is to remove both the sentence-level and word-level beam search, and use a greedy decoding algorithm with the reference mechanism. As seen from Table 6, the graph-based attention mechanism is significantly better than traditional attention mechanism for the document summarization task. Beam search helps significantly improve the generated summaries. Our proposed decoding algorithm enables a sentence-level beam search, which helps improve the generated summaries with multiple sentences."}, {"heading": "4.5 Case Study", "text": "We show the case study of a sample4 from the DailyMail test set in Figure 3. We show the \u201c@entity\u201d and number here although they are removed in the evaluation. We compare our result with the output by a model using traditional attention as Baseline Attention. We also show the output generated by a Baseline Decoder, which sets \u03b3 = 0 and does not use the sentence-level beam search, to study the difficulty for a traditional decoder to generate multiple sentences. Many observations can be found in Figure 3. The lead three sentences mainly focus on the money information and are not sufficient. As for the Baseline Decoder, first it usually ends the generation too early. The \u201c<eod>\u201d token indicates where the original output stops. When we force the decoder not to end here, the model shows the ability to continue producing the important information. However, two flaws are presented. First is the repetition of \u201c## - year -\n4The original story and highlights can be found at http://www.dailymail.co.uk/news/article-3041766/Benefitscheat-pocketed-17-000-taxpayers-money.html\nold\u201d. Because the word decoder is unaware of the history generated sentences, it repeats generating the sequence as the subject all the time. Second, more importantly, is the information incorrectness. The \u201c## - month - old\u201d is not appropriate to describe the heroine, and the \u201csix - month prison sentence\u201d is in fact \u201cthree months\u201d. Information incorrectness occurs because, for a decoder, it aims at generating a fluent sentence according to the input representation. However, no favor of consistent with the original input is concerned. The proposed hierarchical decoding algorithm helps to alleviate the two problems. The awareness of all the generated sentences helps prevent from always generating some important information. The favor of bigram overlapping with the original sentences helps generate more correct sentences. For example the model is able to correctly distinguish between the \u201cthree-month sentence\u201d and the \u201c##- month suspend\u201d. In conclusion, our method is able to identify the most important information in the original document, and the decoding algorithm we propose is able to generate a more discourse-fluent and information-correct abstractive summary.\nThe visualization of the graph-based attention when our method generates the presented example\nis shown in Figure 4. It seems that the graph-based attention mechanism is able to find the important sentences in the input document, and the distraction mechanism makes the decoder focus on different sentences during decoding. Gradually the decoder attends to \u201c<eod>\u201d until it stops."}, {"heading": "5 Conclusion and Future Work", "text": "In this paper we tackle the challenging task of abstractive document summarization, which is still less investigated to date. We study the difficulty of the abstractive document summarization task, and address the need of finding salient content from the original document, which is overlooked by previous studies. We propose a novel graph-based attention mechanism in a hierarchical encoderdecoder framework, and propose a hierarchical beam search algorithm to generate multi-sentence summary. Extensive experiments verify the effectiveness of the proposed method. Experimental results on two large-scale datasets demonstrate our method achieves state-of-the-art abstractive document summarization performance. It is also able to achieve competitive results with state-of-the-art neural extractive summarization models.\nThere is lots of future work we can do. An appealing direction is to investigate the neural abstractive method on the multi-document summarization task, which is more challenging and lacks training data. Further endeavor may be needed."}, {"heading": "Acknowledgments", "text": "This work was supported by 863 Program of China (2015AA015403), NSFC (61331011), and Key Laboratory of Science, Technology and Standard in Press Industry (Key Laboratory of Intelligent Press Media Technology). We thank the anonymous reviewers for helpful comments and Xinjie Zhou, Jianmin Zhang for doing human evaluation. Xiaojun Wan is the corresponding author."}], "year": 2017, "references": [{"title": "Neural machine translation by jointly learning to align and translate", "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473 .", "year": 2014}, {"title": "Ranking with recursive neural networks and its application to multi-document summarization", "authors": ["Ziqiang Cao", "Furu Wei", "Li Dong", "Sujian Li", "Ming Zhou."], "venue": "Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, January 25-", "year": 2015}, {"title": "Learning summary prior representation for extractive summarization", "authors": ["Ziqiang Cao", "Furu Wei", "Sujian Li", "Wenjie Li", "Ming Zhou", "Houfeng Wang."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and", "year": 2015}, {"title": "The use of mmr, diversity-based reranking for reordering documents and producing summaries", "authors": ["Jaime G. Carbonell", "Jade Goldstein."], "venue": "SIGIR \u201998: Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Develop-", "year": 1998}, {"title": "Distraction-based neural networks for document summarization", "authors": ["Qian Chen", "Xiaodan Zhu", "Zhen-Hua Ling", "Si Wei", "Hui Jiang."], "venue": "Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence (IJCAI-16). pages 2754\u20132760.", "year": 2016}, {"title": "Neural summarization by extracting sentences and words", "authors": ["Jianpeng Cheng", "Mirella Lapata."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association", "year": 2016}, {"title": "Unsupervised sentence enhancement for automatic summarization", "authors": ["Kit Jackie Chi Cheung", "Gerald Penn."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association", "year": 2014}, {"title": "Abstractive sentence summarization with attentive recurrent neural networks", "authors": ["Sumit Chopra", "Michael Auli", "M. Alexander Rush."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational", "year": 2016}, {"title": "Summarization through submodularity and dispersion", "authors": ["Anirban Dasgupta", "Ravi Kumar", "Sujith Ravi."], "venue": "Proceedings of the 51st Annual", "year": 2013}, {"title": "New methods in automatic extracting", "authors": ["Harold P Edmundson."], "venue": "Journal of the ACM (JACM) 16(2):264\u2013285.", "year": 1969}, {"title": "Lexrank: Graph-based lexical centrality as salience in text summarization", "authors": ["G\u00fcnes Erkan", "Dragomir R. Radev."], "venue": "J. Artif. Intell. Res. (JAIR) 22:457\u2013 479.", "year": 2004}, {"title": "Generating sequences with recurrent neural networks", "authors": ["Alex Graves."], "venue": "arXiv preprint arXiv:1308.0850 .", "year": 2013}, {"title": "Topic-sensitive pagerank", "authors": ["Taher H. Haveliwala."], "venue": "Proceedings of the Eleventh International World Wide Web Conference, WWW 2002, May 7-11, 2002, Honolulu, Hawaii. pages 517\u2013526.", "year": 2002}, {"title": "Teaching machines to read and comprehend", "authors": ["Karl Moritz Hermann", "Tom\u00e1s Kocisk\u00fd", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Advances in Neural Information Processing Systems 28: Annual", "year": 2015}, {"title": "Long short-term memory", "authors": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation 9(8):1735\u20131780.", "year": 1997}, {"title": "Adam: A method for stochastic optimization", "authors": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980 .", "year": 2014}, {"title": "Summarization based on embedding distributions", "authors": ["Hayato Kobayashi", "Masaki Noguchi", "Taichi Yatsuka."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for", "year": 2015}, {"title": "Using supervised bigram-based ilp for extractive summarization", "authors": ["Chen Li", "Xian Qian", "Yang Liu."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association", "year": 2013}, {"title": "A hierarchical neural autoencoder for paragraphs and documents", "authors": ["Jiwei Li", "Thang Luong", "Dan Jurafsky."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference", "year": 2015}, {"title": "A simple, fast diverse decoding algorithm for neural generation", "authors": ["Jiwei Li", "Will Monroe", "Dan Jurafsky."], "venue": "arXiv preprint arXiv:1611.08562 .", "year": 2016}, {"title": "Rouge: A package for automatic evaluation of summaries", "authors": ["Chin-Yew Lin."], "venue": "Text Summarization Branches Out: Proceedings of the ACL-04 workshop. Barcelona, Spain, volume 8.", "year": 2004}, {"title": "A study of global inference algorithms in multi-document summarization", "authors": ["Ryan T. McDonald."], "venue": "Advances in Information Retrieval, 29th European Conference on IR Research, ECIR 2007, Rome, Italy, April 2-5, 2007, Proceedings. pages 557\u2013564.", "year": 2007}, {"title": "Textrank: Bringing order into text", "authors": ["Rada Mihalcea", "Paul Tarau."], "venue": "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing. pages 404\u2013411.", "year": 2004}, {"title": "Sequence to backward and forward sequences: A content-introducing approach to generative short-text conversation", "authors": ["Lili Mou", "Yiping Song", "Rui Yan", "Ge Li", "Lu Zhang", "Zhi Jin."], "venue": "Proceedings of COLING 2016, the 26th International Conference", "year": 2016}, {"title": "Summarunner: A recurrent neural network based sequence model for extractive summarization of documents", "authors": ["Ramesh Nallapati", "Feifei Zhai", "Bowen Zhou."], "venue": "Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February 4-9,", "year": 2017}, {"title": "Abstractive text summarization using sequenceto-sequence rnns and beyond", "authors": ["Ramesh Nallapati", "Bowen Zhou", "Cicero dos Santos", "Caglar Gulcehre", "Bing Xiang."], "venue": "Proceedings of The 20th SIGNLL Conference on Computa-", "year": 2016}, {"title": "Learning to generate coherent summary with discriminative hidden semi-markov model", "authors": ["Hitoshi Nishikawa", "Kazuho Arita", "Katsumi Tanaka", "Tsutomu Hirao", "Toshiro Makino", "Yoshihiro Matsuo."], "venue": "Proceedings of COLING 2014. Dublin", "year": 2014}, {"title": "The pagerank citation ranking: Bringing order to the web", "authors": ["Lawrence Page", "Sergey Brin", "Rajeev Motwani", "Terry Winograd."], "venue": "Technical report, Stanford InfoLab.", "year": 1999}, {"title": "Glove: Global vectors for word representation", "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association", "year": 2014}, {"title": "Modelling events through memory-based, open-ie patterns for abstractive summarization", "authors": ["Daniele Pighin", "Marco Cornolti", "Enrique Alfonseca", "Katja Filippova."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computa-", "year": 2014}, {"title": "A neural attention model for abstractive sentence summarization", "authors": ["M. Alexander Rush", "Sumit Chopra", "Jason Weston."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Associa-", "year": 2015}, {"title": "Event-driven headline generation", "authors": ["Rui Sun", "Yue Zhang", "Meishan Zhang", "Donghong Ji."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Lan-", "year": 2015}, {"title": "Supervised sentence fusion with single-stage inference", "authors": ["Kapil Thadani", "Kathleen McKeown."], "venue": "Proceedings of the Sixth International Joint Conference on Natural Language Processing. Asian Federation of Natural Language Processing, pages 1410\u2013", "year": 2013}, {"title": "Towards a unified approach to simultaneous single-document and multi-document summarizations", "authors": ["Xiaojun Wan."], "venue": "Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010). Coling 2010 Organizing Committee,", "year": 2010}, {"title": "Manifold-ranking based topic-focused multidocument summarization", "authors": ["Xiaojun Wan", "Jianwu Yang", "Jianguo Xiao."], "venue": "IJCAI 2007, Proceedings of the 20th International Joint Conference on Artificial Intelligence, Hyderabad, India, January 6-", "year": 2007}, {"title": "Multiple aspect summarization using integer linear programming", "authors": ["Kristian Woodsend", "Mirella Lapata."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language", "year": 2012}, {"title": "Recent advances in document summarization", "authors": ["Jin-ge Yao", "Xiaojun Wan", "Jianguo Xiao."], "venue": "Knowledge and Information Systems .", "year": 2017}, {"title": "Optimizing sentence modeling and selection for document summarization", "authors": ["Wenpeng Yin", "Yulong Pei."], "venue": "Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015, Buenos Aires, Argentina, July 25-31,", "year": 2015}], "id": "SP:c624c38e53f321a6df2d16bd707499ce744ca114", "authors": [{"name": "Jiwei Tan", "affiliations": []}, {"name": "Xiaojun Wan", "affiliations": []}, {"name": "Jianguo Xiao", "affiliations": []}], "abstractText": "Abstractive summarization is the ultimate goal of document summarization research, but previously it is less investigated due to the immaturity of text generation techniques. Recently impressive progress has been made to abstractive sentence summarization using neural models. Unfortunately, attempts on abstractive document summarization are still in a primitive stage, and the evaluation results are worse than extractive methods on benchmark datasets. In this paper, we review the difficulties of neural abstractive document summarization, and propose a novel graph-based attention mechanism in the sequence-to-sequence framework. The intuition is to address the saliency factor of summarization, which has been overlooked by prior works. Experimental results demonstrate our model is able to achieve considerable improvement over previous neural abstractive models. The data-driven neural abstractive method is also competitive with state-of-the-art extractive methods.ive summarization is the ultimate goal of document summarization research, but previously it is less investigated due to the immaturity of text generation techniques. Recently impressive progress has been made to abstractive sentence summarization using neural models. Unfortunately, attempts on abstractive document summarization are still in a primitive stage, and the evaluation results are worse than extractive methods on benchmark datasets. In this paper, we review the difficulties of neural abstractive document summarization, and propose a novel graph-based attention mechanism in the sequence-to-sequence framework. The intuition is to address the saliency factor of summarization, which has been overlooked by prior works. Experimental results demonstrate our model is able to achieve considerable improvement over previous neural abstractive models. The data-driven neural abstractive method is also competitive with state-of-the-art extractive methods.", "title": "Abstractive Document Summarization with a Graph-Based Attentional Neural Model"}