{"sections": [{"text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1237\u20131246 Brussels, Belgium, October 31 - November 4, 2018. c\u00a92018 Association for Computational Linguistics\n1237"}, {"heading": "1 Introduction", "text": "The current leading perspective on temporal information extraction regards three phases: (1) a temporal entity recognition phase, extracting events (blue boxes in Fig. 1) and their attributes, and extracting temporal expressions (green boxes), and normalizing their values to dates or durations, (2) a relation extraction phase, where temporal links (TLinks) among those entities, and between events and the document-creation time (DCT) are found (arrows in Fig. 1, left). And (3), construction of a time-line (Fig. 1, right) from the extracted temporal links, if they are temporally consistent. Much research concentrated on the first two steps, but very little research looks into step 3, time-line construction, which is the focus of this work.\nIn this paper, we propose a new time-line construction paradigm that evades phase 2, the relation extraction phase, because in the classical\nparadigm temporal relation extraction comes with many difficulties in training and prediction that arise from the fact that for a text with n temporal entities (events or temporal expressions) there are n2 possible entity pairs, which makes it likely for annotators to miss relations, and makes inference slow as n2 pairs need to be considered. Temporal relation extraction models consistently give lower performance than those in the entity recognition phase (UzZaman et al., 2013; Bethard et al., 2016, 2017), introducing errors in the time-line construction pipe-line.\nThe ultimate goal of our proposed paradigm is to predict from a text in which entities are already detected, for each entity: (1) a probability distribution on the entity\u2019s starting point, and (2) another distribution on the entity\u2019s duration. The probabilistic aspect is crucial for time-line based decision making. Constructed time-lines allow for further quantitative reasoning with the temporal information, if this would be needed for certain applications.\nAs a first approach towards this goal, in this paper, we propose several initial time-line models in this paradigm, that directly predict - in a linear fashion - start points and durations for each entity, using text with annotated temporal entities as input (shown in Fig. 1). The predicted start points and durations constitute a relative time-line, i.e. a total order on entity start and end points. The time-line is relative, as start and duration values cannot (yet) be mapped to absolute calender dates or durations expressed in seconds. It represents the relative temporal order and inclusions that temporal entities have with respect to each other by the quantitative start and end values of the entities. Relative time-lines are a first step toward our goal, building models that predict statistical absolute time-lines. To train our relative time-line models, we define novel loss functions that exploit TimeML-style an-\nnotations, used in most existing temporal corpora. This work leads to the following contributions:\n\u2022 A new method to construct a relative time-line from a set of temporal relations (TL2RTL).\n\u2022 Two new models that, for the first time, directly predict (relative) time-lines - in linear complexity - from entity-annotated texts without doing a form of temporal relation extraction (S-TLM & C-TLM).\n\u2022 Three new loss functions based on the mapping between Allen\u2019s interval algebra and the end-point algebra to train time-line models from TimeML-style annotations.\nIn the next sections we will further discuss the related work on temporal information extraction. We will describe the models and training losses in detail, and report on conducted experiments."}, {"heading": "2 Related Work", "text": ""}, {"heading": "2.1 Temporal Information Extraction", "text": "The way temporal information is conveyed in language has been studied for a long time. It can be conveyed directly through verb tense, explicit temporal discourse markers (e.g. during or afterwards) (Derczynski, 2017) or temporal expressions such as dates, times or duration expressions (e.g. 10-05-2010 or yesterday). Temporal information is also captured in text implicitly, through background knowledge about, for example, duration of events mentioned in the text (e.g. even without context, walks are usually shorter than journeys).\nMost temporal corpora are annotated with TimeML-style annotations, of which an example is shown in Fig 1, indicating temporal entities, their attributes, and the TLinks among them.\nThe automatic extraction of TimeML-style temporal information from text using machine learning was first explored by Mani et al. (2006). They proposed a multinomial logistic regression classifier to predict the TLinks between entities. They also noted the problem of missed TLinks by annotators, and experimented with using temporal reasoning (temporal closure) to expand their training data.\nSince then, much research focused on further improving the pairwise classification models, by\nexploring different types of classifiers and features, such as (among others) logistic regression and support vector machines (Bethard, 2013; Lin et al., 2015), and different types of neural network models, such as long short-term memory networks (LSTM) (Tourille et al., 2017; Cheng and Miyao, 2017), and convolutional neural networks (CNN) (Dligach et al., 2017). Moreover, different sievebased approaches were proposed (Chambers et al., 2014; Mirza and Tonelli, 2016), facilitating mixing of rule-based and machine learning components.\nTwo major issues shared by these existing approaches are: (1) models classify TLinks in a pairwise fashion, often resulting in an inference complexity of O(n2), and (2) the pair-wise predictions are made independently, possibly resulting in prediction of temporally inconsistent graphs. To address the second, additional temporal reasoning can be used at the cost of computation time, during inference (Chambers and Jurafsky, 2008; Denis and Muller, 2011; Do et al., 2012), or during both training and inference (Yoshikawa et al., 2009; Laokulrat et al., 2015; Ning et al., 2017; Leeuwenberg and Moens, 2017). In this work, we circumvent these issues, as we predict time-lines - in linear time complexity - that are temporally consistent by definition."}, {"heading": "2.2 Temporal Reasoning", "text": "Temporal reasoning plays a central role in temporal information extraction, and there are roughly two approaches: (1) Reasoning directly with Allen\u2019s interval relations (shown in Table 1), by constructing rules like: If event X occurs before Y, and event Y before Z then X should happen before Z (Allen, 1990). Or (2), by first mapping the temporal interval expressions to expressions about interval end-points (start and endings of entities) (Vilain et al., 1990). An example of such mapping is that If event X occurs before Y then the end of X should be before the start of Y. Then reasoning can be done with end-points in a point algebra, which has only three point-wise relations (=, <,>), making reasoning much more efficient compared to reasoning with Allen\u2019s thirteen interval relations.\nMapping interval relations to point-wise expressions has been exploited for model inference by Denis and Muller (2011), and for evaluation by UzZaman and Allen (2011). In this work, we ex-\nploit it for the first time for model training, in our loss functions."}, {"heading": "3 Models", "text": "We propose two model structures for direct time-line construction: (1) a simple contextindependent model (S-TLM), and (2) a contextual model (C-TLM). Their structures are shown in Fig. 2. Additionally, we propose a method to construct relative time-lines from a set of (extracted) TLinks (TL2RTL). In this section we first explain the first two direct models S-TLM and C-TLM, and afterwards the indirect method TL2RTL."}, {"heading": "3.1 Direct Time-line Models", "text": ""}, {"heading": "Word representation", "text": "In both S-TLM and C-TLM, words are represented as a concatenation of a word embedding, a POS embedding, and a Boolean feature vector containing entity attributes such as the type, class, aspect, following (Do et al., 2012). Further details on these are given in the experiments section."}, {"heading": "Simple Time-line Model (S-TLM)", "text": "For the simple context-independent time-line model, each entity is encoded by the word representation of the last word of the entity (generally the most important). From this representation we have a linear projection to the duration d, and the start s. S-TLM is shown by the dotted edges in Fig 2. An advantage of S-TLM is that it has very few parameters, and each entity can be placed on the time-line independently of the others, allowing parallelism during prediction. The downside is that S-TLM is limited in its use of contextual information."}, {"heading": "Contextual Time-line Model (C-TLM)", "text": "To better exploit the entity context we also propose a contextual time-line model C-TLM (solid edges in Fig 2), that first encodes the full text using two bi-directional recurrent neural networks, one for entity starts (BiRNNs), and one for entity durations (BiRNNd).1 On top of the encoded text we learn two linear mappings, one from the BiRNNd output of the last word of the entity mention to its duration d, and similarly for the start time, from the BiRNNs output to the entity\u2019s start s."}, {"heading": "Predicting Start, Duration, and End", "text": "Both proposed models use linear mappings2 to predict the start value si and duration di for the encoded entity i. By summing start si and duration di we can calculate the entity\u2019s end-point ei.\nei = si +max(di, dmin) (1)\nPredicting durations rather than end-points makes it easy to control that the end-point lies after the start-point by constraining the duration di by a constant minimum duration value dmin above 0, as shown in Eq. 1."}, {"heading": "Modeling Document-Creation Time", "text": "Although the DCT is often not found explicitly in the text, it is an entity in TimeML, and has TLinks to other entities. We model it by assigning it a text-independent start sDCT and duration dDCT.\nStart sDCT is set as a constant (with value 0). This way the model always has the same reference point, and can learn to position the entities w.r.t. the DCT on the time-line.\n1We also experimented with sharing weights among BiRNNd and BiRNNs. In our experiments, this gave worse performance, so we propose to keep them separate.\n2Adding more layers did not improve results.\nIn contrast, DCT duration dDCT is modeled as a single variable that is learned (initialized with 1). Since multiple entities may be included in the DCT, and entities have a minimum duration dmin, a constant dDCT could possibly prevent the model from fitting all entities in the DCT. Modeling dDCT as a variable allows growth of dDCT and averts this issue.3"}, {"heading": "Training Losses", "text": "We propose three loss functions to train time-line models from TimeML-style annotations: a regular time-line loss L\u03c4 , and two slightly expanded discriminative time-line losses, L\u03c4ce and L\u03c4h."}, {"heading": "Regular Time-line Loss (L\u03c4 )", "text": "Ground-truth TLinks can be seen as constraints on correct positions of entities on a time-line. The regular time-line loss L\u03c4 expresses the degree to which these constraints are met for a predicted time-line. If all TLinks are satisfied in the timeline for a certain text, L\u03c4 will be 0 for that text.\nAs TLinks relate entities (intervals), we first convert the TLinks to expressions that relate the start and end points of entities. How each TLink is translated to its corresponding point-algebraic constraints is given in Table 1, following Allen (1990).\nAs can be seen in the last column there are only two point-wise operations in the point-algebraic constraints: an order operation (<), and an equality operation (=). To model to what degree each point-wise constraint is met, we employ hinge losses, with a margin m\u03c4 , as shown in Eq. 2.\n3Other combinations of modeling sDCT and dDCT as variable or constant decreased performance.\n4No TLink for Allen\u2019s overlap relation is present in TimeML, also concluded by UzZaman and Allen (2011).\nTo explain the intuition and notation: If we have a point-wise expression \u03be of the form x < y (first case of Eq. 2), then the predicted point x\u0302 should be at least a distance m\u03c4 smaller (or earlier on the time-line) than predicted point y\u0302 in order for the loss to be 0. Otherwise, the loss represents the distance x\u0302 or y\u0302 still has to move to make x\u0302 smaller than y\u0302 (and satisfy the constraint). For the second case, if \u03be is of the form x = y, then point x\u0302 and y\u0302 should lie very close to each other, i.e. at most a distance m\u03c4 away from each other. Any distance further than the margin m\u03c4 is counted as loss. Notice that if we set margin m\u03c4 to 0, the second case becomes an L1 loss |x\u0302 \u2212 y\u0302|. However, we use a small margin m\u03c4 to promote some distance between ordered points and prevent con-\nfusion with equality. Fig. 3 visualizes the loss for three TLinks.\nLp(\u03be|t, \u03b8) = { max(x\u0302+m\u03c4 \u2212 y\u0302, 0) iff x < y max(|x\u0302\u2212 y\u0302| \u2212m\u03c4 , 0) iff x = y\n(2)\nThe total time-line loss L\u03c4 (t|\u03b8) of a model with parameters \u03b8 on text t with ground-truth TLinks R(t), is the sum of the TLink-level losses of all TLinks r \u2208 R(t). Each TLink-level loss Lr(r|t, \u03b8) for TLink r is the sum of the pointwise losses Lp(\u03be|t, \u03b8) of the corresponding pointalgebraic constraints \u03be \u2208 IPA(r) from Table 1.5\nLr(r|t, \u03b8) = \u2211\n\u03be\u2208IPA(r)\nLp(\u03be|t, \u03b8) (3)\nL\u03c4 (t, \u03b8) = \u2211 r\u2208R(t) Lr(r|t, \u03b8) (4)"}, {"heading": "Discriminative Time-line Losses", "text": "To promote a more explicit difference between the relations on the time-line we introduce two discriminative loss functions, L\u03c4ce and L\u03c4h, which build on top of Lr. Both discriminative loss functions use an intermediate score S(r|t, \u03b8) for each TLink r based on the predicted time-line. As scoring function, we use the negativeLr loss, as shown in Eq. 5.\nS(r|t, \u03b8) = \u2212Lr(r|t, \u03b8) (5) 5The TLink during and its inverse are mapped to simulta-\nneous, following the evaluation of TempEval-3.\nThen, a lower time-line loss Lr(r|t, \u03b8) results in a higher score for relation type r. Notice that the maximum score is 0, as this is the minimum Lr."}, {"heading": "Probabilistic Loss (L\u03c4ce)", "text": "Our first discriminative loss is a cross-entropy based loss. For this the predicted scores are normalized using a soft-max over the possible relation types (TL). The resulting probabilities are used to calculate a cross-entropy loss, shown in Eq. 6. This way, the loss does not just promote the correct relation type but also distantiates from the other relation types.\nL\u03c4ce(t|\u03b8) = \u2211 r\u2208R(t) r \u00b7 log ( eS(r|t,\u03b8)\u2211 r\u2032\u2208TL e S(r\u2032|t,\u03b8) ) (6)"}, {"heading": "Ranking Loss (L\u03c4h)", "text": "When interested in discriminating relations on the time-line, we want the correct relation type to have the highest score from all possible relation types TL. To represent this perspective, we also define a ranking loss with a score margin mh in Eq. 7.\nL\u03c4h(t|\u03b8) =\u2211 r\u2208R(t) \u2211 r\u2032\u2208TL\\{r} max(S(r\u2032|t, \u03b8)\u2212S(r|t, \u03b8)+mh, 0)\n(7)"}, {"heading": "Training Procedure", "text": "S-TLM and C-TLM are trained by by iterating through the training texts, sampling mini-batches of 32 annotated TLinks. For each batch we (1) perform a forward pass, (2) calculate the total loss (for one of the loss functions), (3) derive gradients using Adam6 (Kingma and Ba, 2014), and (4) update the model parameters \u03b8 via back-propagation. After each epoch we shuffle the training texts. As stopping criteria we use early stopping (Morgan and Bourlard, 1990), with a patience of 100 epochs and a maximum number of 1000 epochs."}, {"heading": "3.2 From TLinks to Time-lines (TL2RTL)", "text": "To model the indirect route, we construct a novel method, TL2RTL, that predicts relative time lines from a subset of TLinks, shown in Fig 1. One can choose any method to obtain a set of TLinks R(t) from a text t, serving as input to TL2RTL.\n6Using the default parameters from the paper.\nTL2RTL constructs a relative time-line, by assigning start and end values to each temporal entity, such that the resulting time-line satisfies the extracted TLinksR(t) by minimizing a loss function that is 0 when the extracted TLinks are satisfied. TL2RTL on itself is a method and not a model. The only variables over which it optimizes the loss are the to be assigned starts and duration values.\nIn detail, for a text t, with annotated entities E(t), we first extract a set of TLinks R(t). In this work, to extract TLinks, we use the current state-of-the-art structured TLink extraction model by Ning et al. (2017). Secondly, we assign a start variable si, and duration variable di to each entity i \u2208 E(t). Similar to S-TLM and C-TLM, for each i \u2208 E(t), di is bounded by a minimum duration dmin to ensure start si always lies before end ei. Also, we model the DCT start sDCT as a constant, and its duration dDCT as a variable. Then we minimize one of the loss functions L\u03c4 , L\u03c4ce, or L\u03c4h on the extracted TLinks R(t), obtaining three TL2RTL variants, one for each loss. If the initially extracted set of TLinks R(t) is consistent, and the loss is minimized sufficiently, all si and di form a relative time-line that satisfies the TLinks R(t), but from which we can now also derive consistent TLinks for any entity pair, also the pairs that were not in R(t). To minimize the loss we use Adam for 10k epochs until the loss is zero for each document.7"}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Evaluation and Data", "text": "Because prediction of relative time-lines trained on TimeML-style annotations is new, we cannot compare our model directly to relation extraction or classification models, as the latter do not provide completely temporally consistent TLinks for all possible entity pairs, like the relative timelines do. Neither can we compare directly to existing absolute time-line prediction models such as Reimers et al. (2018) because they are trained on different data with a very different annotation scheme.\nTo evaluate the quality of the relative time-line models in a fair way, we use TimeML-style test sets as follows: (1) We predict a time-line for each test-text, and (2) we check for all ground-truth an-\n7For some documents the extracted TLinks were temporally inconsistent, resulting in a non-zero loss. Nevertheless, > 96% of the extracted TLinks were satisfied.\nnotated TLinks that are present in the data, what would be the derived relation type based on the predicted time-line, which is the relation type that gives the lowest time-line loss Lr. This results in a TLink assignment for each annotated pair in the TimeML-style reference data, and therefor we can use similar metrics. As evaluation metric we employ the temporal awareness metric, used in TempEval-3, which takes into account temporal closure (UzZaman et al., 2013). Notice that although we use the same metric, comparison against relation classification systems would be unfair, as our model assigns consistent labels to all pairs, whereas relation classification systems do not.\nFor training and evaluation we use two data splits, TE\u2021 and TD\u2021, exactly following Ning et al. (2017). Some statistics about the data are shown in Table 2.8 The splits are constituted from various smaller datasets: the TimeBank (TB) (Pustejovsky et al., 2002), the AQUANT dataset (AQ), and the platinum dataset (PT) all from TempEval-3 (UzZaman et al., 2013). And, the TimeBank Dense (Cassidy et al., 2014) , and the Verb-Clause dataset (VC) (Bethard et al., 2007)."}, {"heading": "4.2 Hyper-parameters and Preprocessing", "text": "Hyper-parameters shared in all settings can be found in Table 3. The following hyper-parameters are tuned using grid search on a development set (union of TB and AQ): dmin is chosen from {1, 0.1, 0.01}, m\u03c4 from {0, 0.025, 0.05, 0.1}, \u03b1d from {0, 0.1, 0.2, 0.4, 0.8}, and \u03b1rnn from {10, 25, 50}. We use LSTM (Hochreiter and Schmidhuber, 1997) as RNN units9 and employ 50-dimensional GloVe word-embeddings pre-trained10 on 6B words (Wikipedia and NewsCrawl) to initialize the models\u2019 word embeddings.\nWe use very simple tokenization and consider punctuation11 or newline tokens as individual tokens, and split on spaces. Additionally, we lowercase the text and use the Stanford POS Tagger (Toutanova et al., 2003) to obtain POS.\n8We explicitly excluded all test documents from training as some corpora annotated the same documents.\n9We also experimented with GRU as RNN type, obtaining similar results.\n10https://nlp.stanford.edu/projects/glove 11, ./\\\"\u2019=+-;:()!?<>%&$*|[]{}"}, {"heading": "5 Results", "text": "We compared our three proposed models for the three loss functions L\u03c4 , L\u03c4ce, and L\u03c4h, and their linear (unweighted) combination L\u2217, on TE3\u2021 and TD\u2021, for which the results are shown in Table 4.\nA trend that can be observed is that overall performance on TD\u2021 is higher than that of TE3\u2021, even though less documents are used for training. We inspected why this is the case, and this is caused by a difference in class balance between both test sets. In TE3\u2021 there are many more TLinks of type simultaneous (12% versus 3%), which are very\ndifficult to predict, resulting in lower scores for TE3\u2021 compared to TD\u2021. The difference in performance between the datasets is probably also be related to the dense annotation scheme of TD\u2021 compared to the sparser annotations of TE3\u2021, as dense annotations give a more complete temporal view of the training texts. For TL2RTL better TLink extraction12 is also propagated into the final timeline quality.\nIf we compare loss functions L\u03c4 , L\u03c4ce, and L\u03c4h, and combination L\u2217, it can be noticed that, although all loss functions seem to give fairly similar performance, L\u03c4 gives the most robust results (never lowest), especially noticeable for the smaller dataset TD\u2021. This is convenient, because L\u03c4 is fastest to compute during training, as it requires no score calculation for each TLink type. L\u03c4 is also directly interpretable on the timeline. The combination of losses L\u2217 shows mixed results, and has lower performance for S-TLM and C-TLM, but better performance for TL2RTL. However, it is slowest to compute, and less interpretable, as it is a combined loss.\nMoreover, we can clearly see that on TE3\u2021, CTLM performs better than the indirect models, across all loss functions. This is a very interesting result, as C-TLM is an order of complexity faster in prediction speed compared to the indirect models (O(n) compared to O(n2) for a text with n entities).13 We further explore why this is the case through our error analysis in the next section.\nOn TD\u2021, the indirect models seem to perform slightly better. We suspect that the reason for this is that C-TLM has more parameters (mostly the LSTM weights), and thus requires more data (TD\u2021 has much fewer documents than TE3\u2021) compared to the indirect methods. Another result supporting this hypothesis is the fact that the difference between C-TLM and S-TLM is small on the smaller\n12F1 of 40.3 for TE3\u2021 and 48.5 for TD\u2021 (Ning et al., 2017) 13We do not directly compare prediction speed, as it would result in unfair evaluation because of implementation differences. However, currently, C-TLM predicts at\u223c100 w/s incl. POS tagging, and \u223c2000 w/s without. When not using POS, overall performance decreases consistently with 2-4 points.\nTD\u2021, indicating that C-TLM does not yet utilize contextual information from this dataset, whereas, in contrast, on TE3\u2021, the larger dataset, C-TLM clearly outperforms S-TLM across all loss functions, showing that when enough data is available C-TLM learns good LSTM weights that exploit context substantially."}, {"heading": "6 Error Analysis", "text": "We compared predictions of TL2RTL(L\u03c4 ) with those of C-TLM (L\u03c4 ), the best models of each paradigm. In Table 4, we show the confusion matrices of both systems on TE3\u2021.\nWhen looking at the overall pattern in errors, both models seem to make similar confusions on both datasets (TD\u2021 was excluded for space constraints). Overall, we find that simultaneous is the most violated TLink for both models. This can be explained by two reasons: (1) It is the least frequent TLink in both datasets. And (2), simultaneous entities are often co-referring events. Event co-reference resolution is a very difficult task on its own.\nWe also looked at the average token-distance between arguments of correctly satisfied TLinks by the time-lines of each model. For TL2RTL (L\u03c4 ) this is 13 tokens, and for C-TLM (L\u03c4 ) 15. When looking only at the TLinks that C-TLM (L\u03c4 ) satisfied and TL2RTL (L\u03c4 ) did not, the average distance is 21. These two observations suggest that the direct C-TLM (L\u03c4 ) model is better at positioning entities on the time-line that lie further away from each other in the text. An explanation for this can be error propagation of TLink extraction to the time-line construction, as the pairwise TLink extraction of the indirect paradigm extracts TLinks in a contextual window, to prune the O(n2) number of possible TLink candidates. This\nconsequently prevents TL2RTL to properly position distant events with respect to each other.\nTo get more insight in what the model learns we calculated mean durations and mean starts of CTLM (L\u03c4 ) predictions. Table 5 contains examples from the top-shortest, and top-longest duration assignments and earliest and latest starting points. We observe that events that generally have more events included are assigned longer duration and vice versa. And, events with low start values are in the past tense and events with high start values are generally in the present (or future) tense."}, {"heading": "7 Discussion", "text": "A characteristic of our model is that it assumes that all events can be placed on a single timeline, and that it does not assume that unlabeled pairs are temporally unrelated. This has big advantages: it results in fast prediction, and missed annotation do not act as noise to the training, as they do for pairwise models. Ning et al. (2018) argue that actual, negated, hypothesized, expected or opinionated events should possibly be annotated\non separate time-axis. We believe such multi-axis representations can be inferred from the generated single time-lines if hedging information is recognized."}, {"heading": "8 Conclusions", "text": "This work leads to the following three main contributions14: (1) Three new loss functions that connect the interval-based TimeML-annotations to points on a time-line, (2) A new method, TL2RTL, to predict relative time-lines from a set of predicted temporal relations. And (3), most importantly, two new models, S-TLM and C-TLM, that \u2013 to our knowledge for the first time \u2013 predict (relative) time-lines in linear complexity from text, by evading the computationally expensive (often O(n2)) intermediate relation extraction phase in earlier work. From our experiments, we conclude that the proposed loss functions can be used effectively to train direct and indirect relative time-line models, and that, when provided enough data, the \u2013 much faster \u2013 direct model C-TLM outperforms the indirect method TL2RTL.\nAs a direction for future work, it would be very interesting to extend the current models, diving further into direct time-line models, and learn to predict absolute time-lines, i.e. making the time-lines directly mappable to calender dates and times, e.g. by exploiting complementary data sources such as the EventTimes Corpus (Reimers et al., 2016) and extending the current loss functions accordingly. The proposed models also provide a good starting point for research into probabilistic time-line models, that additionally model the (un)certainty of the predicted positions and durations of the entities."}, {"heading": "Acknowledgments", "text": "The authors thank Geert Heyman and the reviewers for their constructive comments which helped us to improve the paper. This work was funded by the KU Leuven C22/15/16 project \u201dMAchine Reading of patient recordS (MARS)\u201d, and by the IWT-SBO 150056 project \u201dACquiring CrUcial Medical information Using LAnguage TEchnology\u201d (ACCUMULATE).\n14Code is available at: liir.cs.kuleuven.be/software.php"}], "year": 2018, "references": [{"title": "Maintaining knowledge about temporal intervals", "authors": ["James F Allen."], "venue": "Readings in Qualitative Reasoning about Physical Systems, pages 361\u2013372.", "year": 1990}, {"title": "ClearTK-TimeML: A minimalist approach to tempeval 2013", "authors": ["Steven Bethard."], "venue": "Proceedings of SemEval, volume 2, pages 10\u201314. ACL.", "year": 2013}, {"title": "Timelines from text: Identification of syntactic temporal relations", "authors": ["Steven Bethard", "James H. Martin", "Sara Klingenstein."], "venue": "Proceedings of ICSC, pages 11\u201318.", "year": 2007}, {"title": "Semeval-2016 task 12: Clinical tempeval", "authors": ["Steven Bethard", "Guergana Savova", "Wei-Te Chen", "Leon Derczynski", "James Pustejovsky", "Marc Verhagen."], "venue": "Proceedings of SemEval, pages 1052\u20131062. ACL.", "year": 2016}, {"title": "SemEval-2017 Task 12: Clinical TempEval", "authors": ["Steven Bethard", "Guergana Savova", "Martha Palmer", "James Pustejovsky."], "venue": "Proceedings of SemEval, pages 565\u2013572. ACL.", "year": 2017}, {"title": "An annotation framework for dense event ordering", "authors": ["Taylor Cassidy", "Bill McDowell", "Nathanael Chambers", "Steven Bethard."], "venue": "Proceedings of ACL, pages 501\u2013506. ACL.", "year": 2014}, {"title": "Dense event ordering with a multi-pass architecture", "authors": ["Nathanael Chambers", "Taylor Cassidy", "Bill McDowell", "Steven Bethard."], "venue": "Transactions of the Association for Computational Linguistics, 2:273\u2013 284.", "year": 2014}, {"title": "Jointly combining implicit constraints improves temporal ordering", "authors": ["Nathanael Chambers", "Dan Jurafsky."], "venue": "Proceedings of EMNLP, pages 698\u2013 706. ACL.", "year": 2008}, {"title": "Classifying temporal relations by bidirectional LSTM over dependency paths", "authors": ["Fei Cheng", "Yusuke Miyao."], "venue": "Proceedings of ACL, volume 2, pages 1\u20136. ACL.", "year": 2017}, {"title": "Predicting globally-coherent temporal structures from texts via endpoint inference and graph decomposition", "authors": ["Pascal Denis", "Philippe Muller."], "venue": "Proceedings of IJCAI, pages 1788\u20131793.", "year": 2011}, {"title": "Automatically Ordering Events and Times in Text, volume 677", "authors": ["Leon RA Derczynski."], "venue": "Springer.", "year": 2017}, {"title": "Neural temporal relation extraction", "authors": ["Dmitriy Dligach", "Timothy Miller", "Chen Lin", "Steven Bethard", "Guergana Savova."], "venue": "Proceedings of EACL, volume 2, pages 746\u2013751.", "year": 2017}, {"title": "Joint inference for event timeline construction", "authors": ["Quang Xuan Do", "Wei Lu", "Dan Roth."], "venue": "Proceedings of EMNLP-CoNLL, pages 677\u2013687. ACL.", "year": 2012}, {"title": "Long short-term memory", "authors": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation, 9(8):1735\u20131780.", "year": 1997}, {"title": "Adam: A method for stochastic optimization", "authors": ["Diederik P Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980.", "year": 2014}, {"title": "Stacking approach to temporal relation classification", "authors": ["Natsuda Laokulrat", "Makoto Miwa", "Yoshimasa Tsuruoka."], "venue": "Journal of Natural Language Processing, 22(3):171\u2013196.", "year": 2015}, {"title": "Structured learning for temporal relation extraction from clinical records", "authors": ["Artuur Leeuwenberg", "Marie-Francine Moens."], "venue": "Proceedings of EACL, volume 1, pages 1150\u20131158. ACL.", "year": 2017}, {"title": "Multilayered temporal modeling for the clinical domain", "authors": ["Chen Lin", "Dmitriy Dligach", "Timothy A Miller", "Steven Bethard", "Guergana K Savova."], "venue": "Journal of the American Medical Informatics Association, 23(2):387\u2013395.", "year": 2015}, {"title": "Machine learning of temporal relations", "authors": ["Inderjeet Mani", "Marc Verhagen", "Ben Wellner", "Chong Min Lee", "James Pustejovsky."], "venue": "Proceedings of COLING-ACL, pages 753\u2013760. ACL.", "year": 2006}, {"title": "CATENA : Causal and temporal relation extraction from natural language texts", "authors": ["Paramita Mirza", "Sara Tonelli."], "venue": "Proceedings of COLING, pages 64\u201375.", "year": 2016}, {"title": "Generalization and parameter estimation in feedforward nets: Some experiments", "authors": ["Nelson Morgan", "Herv\u00e9 Bourlard."], "venue": "Advances in Neural Information Processing Systems.", "year": 1990}, {"title": "A structured learning approach to temporal relation extraction", "authors": ["Qiang Ning", "Zhili Feng", "Dan Roth."], "venue": "Proceedings of EMNLP, pages 1038\u20131048.", "year": 2017}, {"title": "A multiaxis annotation scheme for event temporal relations", "authors": ["Qiang Ning", "Hao Wu", "Dan Roth."], "venue": "Proceedings of ACL, pages 1318\u20131328. ACL.", "year": 2018}, {"title": "Temporal anchoring of events for the timebank corpus", "authors": ["Nils Reimers", "Nazanin Dehghani", "Iryna Gurevych."], "venue": "Proceedings of ACL, pages 2195\u2013 2204.", "year": 2016}, {"title": "Event time extraction with a decision tree of neural classifiers", "authors": ["Nils Reimers", "Nazanin Dehghani", "Iryna Gurevych."], "venue": "Transactions of the Association for Computational Linguistics, 6:77\u201389.", "year": 2018}, {"title": "Neural architecture for temporal relation extraction: A Bi-LSTM approach for detecting narrative containers", "authors": ["Julien Tourille", "Olivier Ferret", "Aurelie Neveol", "Xavier Tannier."], "venue": "Proceedings of ACL, pages 224\u2013230.", "year": 2017}, {"title": "Feature-rich partof-speech tagging with a cyclic dependency network", "authors": ["Kristina Toutanova", "Dan Klein", "Christopher D Manning", "Yoram Singer."], "venue": "Proceedings of NAACL-HLT, pages 173\u2013 180. ACL.", "year": 2003}, {"title": "Temporal evaluation", "authors": ["Naushad UzZaman", "James F. Allen."], "venue": "Proceedings of ACL, HLT \u201911, pages 351\u2013356, Stroudsburg, PA, USA. ACL.", "year": 2011}, {"title": "Semeval-2013 task 1: Tempeval-3: Evaluating time expressions, events, and temporal relations", "authors": ["Naushad UzZaman", "Hector Llorens", "Leon Derczynski", "Marc Verhagen", "James Allen", "James Pustejovsky."], "venue": "Second joint conference on lexical and", "year": 2013}, {"title": "Constraint propagation algorithms for temporal reasoning: A revised report", "authors": ["Marc Vilain", "Henry Kautz", "Peter Van Beek."], "venue": "Readings in Qualitative Reasoning about Physical Systems, pages 373\u2013381. Elsevier.", "year": 1990}, {"title": "Jointly identifying temporal relations with markov logic", "authors": ["Katsumasa Yoshikawa", "Sebastian Riedel", "Masayuki Asahara", "Yuji Matsumoto."], "venue": "Proceedings of ACL-IJCNLP, pages 405\u2013413. ACL.", "year": 2009}], "id": "SP:bbad0b301561c9b44a43f2880b29f143dc7297ba", "authors": [{"name": "Artuur Leeuwenberg", "affiliations": []}], "abstractText": "The current leading paradigm for temporal information extraction from text consists of three phases: (1) recognition of events and temporal expressions, (2) recognition of temporal relations among them, and (3) time-line construction from the temporal relations. In contrast to the first two phases, the last phase, time-line construction, received little attention and is the focus of this work. In this paper, we propose a new method to construct a linear time-line from a set of (extracted) temporal relations. But more importantly, we propose a novel paradigm in which we directly predict start and end-points for events from the text, constituting a time-line without going through the intermediate step of prediction of temporal relations as in earlier work. Within this paradigm, we propose two models that predict in linear complexity, and a new training loss using TimeML-style annotations, yielding promising results.", "title": "Temporal Information Extraction by Predicting Relative Time-lines"}