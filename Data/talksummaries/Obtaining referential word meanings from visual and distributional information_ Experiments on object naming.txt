0	75	Expressions referring to objects in visual scenes typically include a word naming the type of the object: E.g., house in Figure 1 (a), or, as a very general type, thingy in Figure 1 (d).
1	37	Determining such a name is a crucial step for referring expression generation (REG) systems, as many other decisions concerning, e.g., the selection of attributes follow from it (Dale and Reiter, 1995; Krahmer and Van Deemter, 2012).
2	44	For a long time, however, research on REG mostly assumed the availability of symbolic representations of ref- erent and scene, and sidestepped questions about how speakers actually choose these names, due to the lack of models capable of capturing what a word like house refers to in the real world.
3	33	Recent advances in image processing promise to fill this gap, with state-of-the-art computer vision systems being able to classify images into thousands of different categories (e.g. Szegedy et al. (2015)).
4	32	However, classification is not naming (Ordonez et al., 2016).
5	33	Standard object classification schemes are inherently “flat”, and treat object labels as mutually exclusive (Deng et al., 2014).
6	23	A state-of-the-art object recognition system would be trained to classify the object in e.g. Figure 1 (a) as either house or building, ignoring the lexical similarity between these two names.
7	45	In contrast, humans seem to be more flexible as to the chosen level of generality.
8	28	Depending on the prototypicality of the object to name, and possibly other visual properties, a general name might be more or less appropriate.
9	27	For instance, a robin can be named bird, but a penguin is better referred 243 to as “penguin” (Rosch, 1978); along the same lines, the rather unusual building in Figure 1 (c) that is not easy to otherwise categorise was named “structure”.
12	10	Thus, under the assumption that such semantic spaces represent, in some form at least, taxonomic knowledge, this makes labels on different levels of specificity available for a given object.
13	16	Moreover, if the mapping is sufficiently general, it should be able to map objects to an appropriate label, even if during training of the mapping this label has not been seen (zero-shot learning).
14	11	While cross-modal transfer seems to be a conceptually attractive model for learning object names, it is based on an important assumption that, in our view, has not received sufficient attention in previous works: it assumes that a given distributional vector space constitutes an optimal target representation that visual instances of objects can be mapped to.
70	17	This model will be called TRANSFER below.
76	23	Another approach is to keep visual and distributional information separate, by training a separate visual classifier for each word w in the vocabulary.
81	17	The model predicts names directly, without links into a distributional space.
82	32	In order to extend the model’s vocabulary for zero-shot learning, we follow Norouzi et al. (2013) and associate the top n words with their corresponding distributional vector and compute the convex combination of these vectors.
83	10	Then, in parallel to cross-modal mapping, we retrieve the nearest neighbors of the combined embedding from the distributional space.
84	17	Thus, with this model, we use two different modes of decoding: one that projects into distributional space, one that only applies the available word classifiers.
88	39	Finally, we implement an approach that combines ideas from cross-modal mapping with the WAC model: we train individual predictors for each word in the vocabulary, but, during training, we exploit lexical similarity relations encoded in a distributional space.
89	16	Instead of treating a word as a binary classifier, we annotate its training instances with a fine-grained similarity signal according to their object names.
99	10	This seems to suggest that lexical or at least distributional knowledge is detrimental when learning what a word refers to in the real world and that referential meaning should potentially be learned from visual object representation only.
102	47	This amounts to 6208 image regions for testing and 73K instances for training.
103	29	Results Table 1 shows accuracies in the object naming task for the TRANSFER, WAC and SIMWAP models according to their accuracies in the top n, including two variants of WAC where its top 5 and top 10 predictions are projected into the distributional space.
106	44	Thus, looking at accuracies for the top (two) predictions, the various models that link referential meaning to word representations in the distributional space all perform slightly worse than the plain WAC model, i.e. individual word classifiers trained on visual features only.
111	9	In order to get more insight into why the TRANSFER and SIM-WAP models produce slightly worse results than individual visual word classifiers, we now test to what extent the different models are complementary and combine them by aggregating over their naming predictions.
112	14	If the models are complementary, their combination should lead to more confident and accurate naming decisions.
114	16	During testing, we apply all models to an image region and consider words ranked among the top 10.
130	46	This parallels findings by Frome et al. (2013) who found that their transfer-based object recognition made “semantically more reasonable” errors than a standard convolutional network while not improving accuracies for object recognition, see discussion in Section 2.
132	46	Zarrieß and Schlangen (2016)), would be an interesting direction for more detailed investigation here.
