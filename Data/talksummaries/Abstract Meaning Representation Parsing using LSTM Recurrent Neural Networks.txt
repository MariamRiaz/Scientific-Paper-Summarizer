6	20	Sentence meanings in AMR are represented in the form of graphs consisting of concepts (nodes) connected by labeled relations (edges).
45	12	Features based on the sentence and these subgraphs are then processed by a pair of B-LSTM networks to compute the probabilities of relations between all subgraphs.
51	39	Likewise, a span of words can be mapped to a small connected subgraph, such as the single word span France which is mapped to a subgraph composed of two concepts connected by a name relation.
52	12	(see the shaded section of Figure 1a).
62	14	All input features for the five networks correspond to the sequence of words in the input sentence, and are presented to the networks as indices into lookup tables.
71	39	Named Entity Recognition can be valuable input to a parser, and state-of-the-art NER systems can be created using convolutional neural networks (Collobert et al., 2011) or LSTM (Chiu and Nichols, 2015) aided by information from gazetteers.
74	14	We then encode the classified named entities output from the wikifier as feature embeddings, which are used by the SG Network.
75	24	The features used as input to the SG network are: • word: 45Kx302, the word embeddings • suffix: 430x5, embeddings based on the final two letters of each word.
77	45	• NER: 5x5, embeddings indexed by NER from the Wikifier, ’O’, ’LOC’, ’ORG’, ’PER’ or ’MISC’.
78	72	The SG Network produces probabilities for 46 BIOES tagged subgraph types, and the highest probability tag is chosen for each word, as shown for the example sentence in Table 1.
79	61	The AMR concepts (nodes) are connected by relations (arcs).
80	47	We found it convenient to distinguish predicate argument relations, or ”Args” from other relations, which we call ”Nargs”.
81	29	For example, see ARG0 and ARG1 relations in Figure 1a are ”Args”, compared with the name, degree, mod, or quant relations which are ”Nargs”.
105	63	The Subgraphs are connected to one another by applying a greedy algorithm, which repeatedly selects the most probable edge from the Pargs and Pnargs matrices and adds the edge to the graph description.
109	18	Expressed as a step by step procedure, we first define pconnect as the probability threshold at which to require graph component spanning, and we repeat the following, until any two subgraphs in the graph are connected by at least one path.
126	14	A practical graph comparison program called Smatch (Cai and Knight, 2013) is used to consistently evaluate AMR parsers.
127	18	The smatch python script provides an F1 evaluation metric for whole-sentence semantic graph analysis by comparing sets of triples which describe portions of the graphs, and uses a hill climbing algorithm for efficiency.
131	34	For each of the five networks, we used the LDC2015E86 training split to train parameters, and periodically interrupted training to run the dev split (forward) in order to monitor performance.
137	27	We report the statistics for smatch results of the ”test” and ”eval” datasets for 12 trained systems in Table 3.
138	24	The top five scores for Semeval 2016 task 8, representing the previous state-of-the-art, are shown for context.
140	12	The best performing systems for in-domain (dev and test) data correlated well with the best ones for the out-of-domain (eval) data, although the scores for the eval dataset were lower overall.
147	29	We have shown that B-LSTM neural networks can be used as the basis for a graph based semantic parser.
148	27	Our AMR parser effectively exploits the ability of B-LSTM networks to learn to selectively extract information from words separated by long distances in a sentence, and to build up higher level representations by rejecting or remembering important information during sequence processing.
154	31	But it would also be fairly easy to add gazetteer information to the network features in order to remove the need for NER preprocessing.
155	11	Therefore, the wikification subtask is the only portion of the parser which requires any pre-processing at all.
158	34	A better approach to disambiguation is to consider predicates separately, solving for a set of coefficients for each verb found in the training set.
159	37	A general set of model parameters could then be used to handle unseen examples.
160	90	Likewise, high level ARGs like ARG2 and ARG3 don’t generalize very well among different predicates, and ARG inference accuracy could be improved with predicatespecific network parameters for the most common cases.
161	45	The alignment between concepts and words is not a reliable, direct mapping: some concepts cannot be grounded to words, some are ambiguous, and automatic aligners tend to have high error rates relative to human aligning judgements.
162	69	Improvements in the quality of the alignment in training data would improve parsing results.
