4	23	Several works attempt to extend recurrent neural networks to work on trees (see Section 8 for a brief overview), giving rise to the so-called recursive neural networks (Goller and Kuchler, 1996; Socher et al., 2010).
6	9	Other attempts allow arbitrary branching factors, at the expense of ignoring the order of the modifiers.
8	12	Our tree encoder uses recurrent neural networks as a building block: we model the left and right sequences of modifiers using RNNs, which are composed in a recursive manner to form a tree (Section 3).
9	24	We use our tree representation for encoding the partially-built parse trees in a greedy, bottom-up dependency parser which is based on the easy-first transition-system of Goldberg and Elhadad (2010).
10	15	Using the Hierarchical Tree LSTM representation, and without using any external embeddings, our parser achieves parsing accuracies of 92.6 UAS and 90.2 LAS on the PTB (Stanford dependencies) and 86.1 UAS and 84.4 LAS on the Chinese treebank, while relying on greedy decoding.
11	96	To the best of our knowledge, this is the first work to demonstrate competitive parsing accuracies for full-scale parsing while relying solely on recursive, compositional tree representations, and without using a reranking framework.
13	26	While the parsing experiments demonstrate the suitability of our representation for capturing the structural elements in the parse tree that are useful for predicting parsing decisions, we are interested in exploring the use of the RNN-based compositional vector representation of parse trees also for seman- 445 Transactions of the Association for Computational Linguistics, vol.
15	9	tic tasks such as sentiment analysis (Socher et al., 2013b; Tai et al., 2015), sentence similarity judgements (Marelli et al., 2014) and textual entailment (Bowman et al., 2015).
16	33	A dependency-based syntactic representation is centered around syntactic modification relations between head words and modifier words.
18	70	A dependency tree over a sentence with n words w1, .
19	8	, wn can be represented as a list of n pairs of the form (h,m), where 0 ≤ h ≤ n and 1 ≤ m ≤ n. Each such pair represents an edge in the tree in which h is the index of a head word (including the special ROOT node 0), and m is the index of a modifier word.
23	80	Looking at trees in the PTB training set we find that 94% of the trees have a height of at most 10, and 49% of the trees a height of at most 6.
25	48	Recurrent neural networks (RNNs), first proposed by Elman (1990) are statistical learners for modeling sequential data.
26	78	In this work, we use the RNN abstraction as a building block, and recursively combine several RNNs to obtain our tree representation.
27	35	We briefly describe the RNN abstraction below.
28	13	For further detail on RNNs, the reader is referred to sources such as (Goldberg, 2015; Bengio and Courville, 2016; Cho, 2015).
30	11	, xn (xi ∈ Rdin), and produces a sequence of state vec- tors (also called output vectors) y1, .
33	112	, yn−1, the RNN can be thought of as encoding the sequence x1, .
40	61	The functions N and O defining the RNN are parameterized by parameters θ (matrices and vectors), which are trained from data.
41	16	Specifically, one is usually interested in using some of the outputs yi for making predictions.
42	12	The RNN is trained such that the encoding yi is good for the prediction task.
43	9	That is, the RNN learns which aspects of the sequence x1, .
47	46	In this work we use the Long Short Term Memory (LSTM) variant (Hochreiter and Schmidhuber, 1997) which is shown to be a very capable sequence learner.
48	27	However, our algorithm and encoding method do not rely on any specific property of the LSTM architecture, and the LSTM can be transparently switched for any other RNN variant.
50	367	We assume trees in which the children are ordered and there are kl ≥ 0 children before the parent node (left children) and kr ≥ 0 children after it (right children).
51	10	Such trees correspond well to dependency tree structures.
52	10	We refer to the parent node as a head, and to its children as modifiers.
57	15	The first input to each RNN is the vector representation of the head word, and the last input is the vector representation of the left-most or the right-most modifier.
58	7	The node’s representation is then a concatenation of the RNN encoding of the left-modifiers with the RNN encoding of the right-modifiers.
60	19	t R L t.r1 R t.r2 R t.r3 R t.r4 R t t.l1 L t.l2 L t.l3 L enc(t) RNNR RNNL concatenate and compress More formally, consider a node t. Let i(t) be the sentence index of the word corresponding to the head node t, and let vi be a vector corresponding to the ith word in the sentence (this vector captures information such as the word form and its part of speech tag, and will be discussed shortly).
