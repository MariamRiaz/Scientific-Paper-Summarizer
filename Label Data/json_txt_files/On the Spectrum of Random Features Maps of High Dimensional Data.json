{"sections": [{"heading": "1. Introduction", "text": "Finding relevant features is one of the key steps for solving a machine learning problem. To this end, the backpropagation algorithm is probably the best-known method, with which superhuman performances are commonly achieved for specific tasks in applications of computer vision (Krizhevsky et al., 2012) and many others (Schmidhuber, 2015). But data-driven approaches such as the backpropagation method, in addition to being computationally demanding, fail to cope with limited amounts of available training data.\nOne successful alternative in this regard is the use of \u201crandom features\u201d, exploited both in feed-forward neural networks (Huang et al., 2012; Scardapane & Wang, 2017), in large-scale kernel estimation (Rahimi & Recht, 2008; Vedaldi & Zisserman, 2012) and more recently in random sketching schemes (Keriven et al., 2016). Random feature maps consist in projections randomly exploring the set of nonlinear representations of the data, hopefully extracting\n1Laboratoire des Signaux et Syste\u0300mes (L2S), CentraleSupe\u0301lec, Universite\u0301 Paris-Saclay, France; 2G-STATS Data Science Chair, GIPSA-lab, University Grenobles-Alpes, France. Correspondence to: Zhenyu Liao <zhenyu.liao@l2s.centralesupelec.fr>, Romain Couillet <romain.couillet@centralesupelec.fr>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nfeatures relevant to some given task. The nonlinearities make these representations more mighty but meanwhile theoretically more difficult to analyze and optimize.\nInfinitely large random features maps are nonetheless well understood as they result in (asymptotically) equivalent kernels, the most popular example being random Fourier features and their limiting radial basis kernels (Rahimi & Recht, 2008). Beyond those asymptotic results, recent advances in random matrix theory give rise to unexpected simplification on the understanding of the finite-dimensional version of these kernels, i.e., when the data number and size are large but of similar order as the random feature vector size (El Karoui et al., 2010; Couillet et al., 2016). Following the same approach, in this work, we perform a spectral analysis on the Gram matrix of the random feature matrices. This matrix is of key relevance in many associated machine learning methods (e.g., spectral clustering (Ng et al., 2002) and kernel SVM (Scho\u0308lkopf & Smola, 2002)) and understanding its spectrum casts an indispensable light on their asymptotic performances. In the remainder of the article, we shall constantly consider spectral clustering as a concrete example of application; however, similar analyses can be performed for other types of random feature-based algorithms.\nOur contribution is twofold. From a random matrix theory perspective, it is a natural extension of the sample covariance matrix analysis (Silverstein & Bai, 1995) to a nonlinear setting and can also be seen as the generalization of the recent work of (Pennington & Worah, 2017) to a more practical data model. From a machine learning point of view, we describe quantitatively the mutual influence of different nonlinearities and data statistics on the resulting random feature maps. More concretely, based on the ratio of two coefficients from our analysis, commonly used activation functions are divided into three classes: means-oriented, covariance-oriented and balanced, which eventually allows one to choose the activation function with respect to the statistical properties of the data (or task) at hand, with a solid theoretical basis.\nWe show by experiments that our results, applicable theoretically only to Gaussian mixture data, show an almost perfect match when applied to some real-world datasets. We are thus optimistic that our findings, although restricted to Gaussian assumptions on the data model, can be applied to\na larger set of problems beyond strongly structured ones.\nNotations: Boldface lowercase (uppercase) characters stand for vectors (matrices), and non-boldface scalars respectively. 1T is the column vector of ones of size T , and IT the T \u21e5T identity matrix. The notation (\u00b7)T denotes the transpose operator. The norm k \u00b7 k is the Euclidean norm for vectors and the operator norm for matrices.\nIn the remainder of this article, we introduce the objects of interest and necessary preliminaries in Section 2. Our main results on the spectrum of random feature maps will be presented in Section 3, followed by experiments on two types of classification tasks in Section 4. The article closes on concluding remarks and envisioned extensions in Section 5."}, {"heading": "2. Problem Statement and Preliminaries", "text": "Let x1, . . . ,xT 2 Rp be independent data vectors, each belonging to one of K distribution classes C1, . . . , CK . Class Ca has cardinality Ta, for all a 2 {1, . . . ,K}. We assume that the data vector xi follows a Gaussian mixture model1, i.e.,\nxi = \u00b5a/ p p+ !i\nwith !i \u21e0 N (0,Ca/p) for some mean \u00b5a 2 Rp and covariance Ca 2 Rp\u21e5p of associated class Ca. We denote the data matrix X = \u21e5 x1, . . . ,xT \u21e4 2 Rp\u21e5T of size T by cascading all xi as column vectors. To extract random features, X is premultiplied by some random matrix W 2 Rn\u21e5p with i.i.d. entries and then applied entry-wise some nonlinear activation function (\u00b7) to obtain the random feature matrix \u2303 \u2318 (WX) 2 Rn\u21e5T , whose columns are simply (Wxi) the associated random feature of xi.\nIn this article, we focus on the Gram matrix G \u2318 1 n \u2303T\u2303 of the random features, the entry (i, j) of which is given by\nGij = 1\nn (Wxi)\nT (Wxj) =\n1\nn\nnX\nk=1\n(wT k xi) (w T k xj)\nwith wT k the k-th row of W. Note that all wk follow the same distribution, so that taking expectation over w \u2318 wk of the above equation one results in the average kernel matrix , with the (i, j) entry of which given by\n(xi,xj) = EwGij = Ew (wTxi) (wTxj). (1)\nWhen the entries of W follow a standard Gaussian distribution, one can compute the generic form (a,b) = Ew (wTa) (wTb) by applying the integral trick from (Williams, 1997), for a large set of nonlinear functions (\u00b7) and arbitrary vector a,b of appropriate dimension. We list the results for commonly used functions in Table 1.\n1We normalize the data by p 1/2 to guarantee that kxik = O(1) with high probability when kCak = O(1).\nSince the Gram matrix G describes the correlation of data in the feature space, it is natural to recenter G, and thus by pre- and post-multiplying a projection matrix P \u2318 IT 1T 1T1 T T . In the case of , we get\nc \u2318 P P.\nIn the recent line of works (Louart et al., 2018; Pennington & Worah, 2017), it has been shown that the large dimensional (large n, p, T ) characterization of G, in particular its eigenspectrum, is fully determined by and the ratio n/p. For instance, by defining the empirical spectral distribution of Gc = PGP as \u21e2Gc(x) \u2318 1T P T\ni=1 1 ix(x), with 1, . . . , T the eigenvalues of Gc, it has been shown in (Louart et al., 2018) that, as n, p, T ! 1, \u21e2G(x) almost surely converges to a non-random distribution \u21e2(x), referred to as the limiting spectral distribution of Gc such that\n\u21e2(x) = 1\n\u21e1 lim y!0+\nZ x\n1 = [m(t+ iy)] dt.\nwith m(z) the associated Stieltjes transform given by\nm(z) = 1\nn trQ(z), Q(z) \u2318\n\u2713 c\n1 + (z) zIT\n\u25c6 1\nwith (z) the unique solution of (z) = 1 n tr ( cQ(z)).\nAs a consequence, in the objective of understanding the asymptotic behavior of Gc as n, p, T are simultaneously large, we shall focus our analysis on c. To this end, the following assumptions will be needed throughout the paper.\nAssumption 1 (Growth rate). As T ! 1,\n1) p/T ! c0 2 (0,1),\n2) for each a 2 {1, . . . ,K}, Ta/T ! ca 2 (0, 1),\n3) k\u00b5ak = O(1), 4) let C \u2318 P K\na=1 Ta T Ca and for a 2 {1, . . . ,K}, C a \u2318\nCa C , then kCak = O(1) and tr(C a)/ p p = O(1),\n5) for technical convenience we assume in addition that \u2327 \u2318 tr (C ) /p converges in (0,1).\nAssumption 1 ensures that the information of means or covariances is neither too simple nor impossible to be extracted from the data, as investigated in (Couillet et al., 2016).\nLet us now introduce the key steps of our analysis. Under Assumption 1, note that for xi 2 Ca and xj 2 Cb, i 6= j,\nxT i xj = ! T i !j| {z }\nO(p 1/2)\n+\u00b5T a \u00b5b/p+ \u00b5 T a !j/\np p+ \u00b5T\nb !i/\np p\n| {z } O(p 1)\nwhich allows one to perform a Taylor expansion around 0 as p, T ! 1, to give a reasonable approximation of nonlinear functions of xT\ni xj , such as those appearing in ij (see again\nTable 1). For i = j, one has instead\nkxik2 = k!ik2| {z } O(1)\n+ k\u00b5ak2/p+ 2\u00b5Ta!i/ p p\n| {z } O(p 1)\n.\nFrom E!i [k!ik2] = tr(Ca)/p it is convenient to further write k!ik2 = tr(Ca)/p + k!ik2 tr(Ca)/p , where tr(Ca)/p = O(1) and k!ik2 tr(Ca)/p = O(p 1/2). By definition \u2327 \u2318 tr(C )/p = O(1) and exploiting again Assumption 1 one results in,\nkxik2 = \u2327|{z} O(1) +tr(C a )/p+ k!ik2 tr(Ca)/p| {z }\nO(p 1/2)\n+ k\u00b5ak2/p+ 2\u00b5Ta!i/ p p\n| {z } O(p 1)\nwhich allows for a Taylor expansion of nonlinear functions of kxik2 around \u2327 , as has been done for xTi xj .\nFrom Table 1, it appears that, for every listed (\u00b7), (xi,xj) is a smooth function of xT\ni xj and kxik, kxjk, despite\ntheir possible discontinuities (e.g., the ReLU function and (t) = |t|). The above results thus allow for an entry-wise Taylor expansion of the matrix in the large p, T limit.\nA critical aspect of the analysis where random matrix theory comes into play now consists in developing as a sum of matrices arising from the Taylor expansion and ignoring terms that give rise to a vanishing operator norm,\nso as to find an asymptotic equivalent matrix \u0303 such that k \u0303k ! 0 as p, T ! 1, as described in detail in the following section. This analysis provides a simplified asymptotically equivalent expression for with all nonlinearities removed, which is the crux of the present study."}, {"heading": "3. Main Results", "text": "In the remainder of this article, we shall use the following notations for random elements,\n\u2326 \u2318 \u21e5 !1, . . . ,!T \u21e4 , \u2318 k!ik2 Ek!ik2 T i=1\nsuch that \u2326 2 Rp\u21e5T , 2 RT . For deterministic elements2,\nM \u2318 \u21e5 \u00b51, . . . ,\u00b5K \u21e4 2 Rp\u21e5K , t \u2318 {trC a / p p}K\na=1\nJ \u2318 \u21e5 j1, . . . , jK \u21e4 2 RT\u21e5K ,S \u2318 {tr(CaCb)/p}Ka,b=1\nwith t 2 RK , S 2 RK\u21e5K and ja 2 RT denotes the canonical vector of class Ca such that (ja)i = xi2Ca . Theorem 1 (Asymptotic Equivalent of c). Let Assumption 1 hold and c be defined as c \u2318 P P, with given in (1). Then, as T ! 1, for all (\u00b7) given in Table 1,3\nk c \u0303ck ! 0 2As a reminder here, M stands for means, t accounts for (difference in) traces while S for the \u201cshapes\u201d of covariances. 3For all functions (\u00b7) listed in Table 1 we identified a \u201cpattern\u201d in the structure of \u0303c, which then led to Theorem 1 and Table 2. This two-step approach does not yet allow to justify whether this pattern goes beyond these (listed) functions; hence Theorem 1 is stated so far solely for these functions.\nOn the Spectrum of Random Features Maps of High Dimensional Data\nalmost surely, with \u0303c = P\u0303P and \u0303 \u2318 d1 \u2713 \u2326+M\nJT p p\n\u25c6T\u2713 \u2326+M\nJT p p\n\u25c6 +d2UBU T+d0IT\nwhere we recall that P \u2318 IT 1T 1T1 T T and\nU \u2318 h Jp p , i , B \u2318\n ttT + 2S t\ntT 1\nwith the coefficients d0, d1, d2 given in Table 2.\nWe refer the readers to Section A in Supplementary Material for a detailed proof of Theorem 1.\n0 0.2 0.4 0.6 0.8 1 1.2\nEigenvalues of c\n0 0.2 0.4 0.6 0.8 1 1.2\nEigenvalues of c\n0 0.2 0.4 0.6 0.8 1 1.2\nEigenvalues of \u0303c\n0 0.2 0.4 0.6 0.8 1 1.2\nEigenvalues of \u0303c\n0.1 0.05\n0 0.05 0.1\nLeading eigenvector of c Leading eigenvector of \u0303c\nC1 C2\n0.1 0.05\n0 0.05 0.1\nLeading eigenvector of c Leading eigenvector of \u0303c\nC1 C2\nFigure 2. Leading eigenvector of c and \u0303c in the settings of Figure 1, with j1 = \u21e5 1T1 ;0T2 \u21e4 and j2 = \u21e5 0T1 ;1T2 \u21e4 .\nTheorem 1 tells us as a corollary (see Corollary 4.3.15 in (Horn & Johnson, 2012)) that the maximal difference between the eigenvalues of c and \u0303c vanishes asymptotically as p, T ! 1, as confirmed in Figure 1. Similarly the distance between the \u201cisolated eigenvectors4\u201d also vanishes, as seen in Figure 2. This is of tremendous importance as the determination of the leading eigenvalues and eigenvectors of c (that contain crucial information for clustering, for example) can be studied from the equivalent problem performed on \u0303c and becomes mathematically more tractable.\n4Eigenvectors that correspond to the eigenvalues found at a non-vanishing distance from the other eigenvalues.\nOn closer inspection of Theorem 1, the matrix \u0303 is expressed as the sum of three terms, weighted respectively by the three coefficients d0, d1 and d2, that depend on the nonlinear function (\u00b7) via Table 2. Note that the statistical structure of the data {xi}Ti=1 (namely the means in M and the covariances in t and S) is perturbed by random fluctuations (\u2326 and ) and it is thus impossible to get rid of these noisy terms by wisely choosing the function (\u00b7). This is in sharp contrast to (Couillet et al., 2016) where it is shown that more general kernels (i.e., not arising from random feature maps) allow for a more flexible treatment of information versus noise.\nHowever, there does exist a balance between the means and covariances, that provides some instructions in the appropriate choice of the nonlinearity. From Table 2, the functions (\u00b7) can be divided into the following three groups:\nBefore entering into a more detailed discussion of Theorem 1, first note importantly that, for practical interests, the quantity \u2327 can be estimated consistently from the data, as described in the following lemma. Lemma 1 (Consistent estimator of \u2327 ). Let Assumption 1 hold and recall the definition \u2327 \u2318 tr (C ) /p. Then, as T ! 1, with probability 1\n1\nT\nTX\ni=1\nkxik2 \u2327 ! 0.\nProof. Since\n1\nT\nTX\ni=1\nkxik2 = 1\nT\nKX\na=1\nTaX\ni=1\n1 p k\u00b5ak2 2 p p \u00b5T a !i + k!ik2,\nwith Assumption 1 we have 1 T\nP K\na=1\nP Ta\ni=1 1 p k\u00b5ak2 =\nO(p 1). The term 1 T\nP K\na=1\nP Ta\ni=1 2p p \u00b5T a !i is a linear com-\nbination of independent zero-mean Gaussian variables and vanishes with probability 1 as p, T ! 1 with Chebyshev\u2019s inequality and the Borel-Cantelli lemma. Ultimately by the strong law of large numbers, we have 1\nT\nP T\ni=1 k!ik2 \u2327 ! 0 almost surely, which concludes the proof.\nFrom a practical aspect, a few remarks on the conclusions of Theorem 1 can be made. Remark 1 (Constant shift in feature space). For (t) = &2t\n2 + &1t+ &0, note the absence of &0 in Table 2, meaning that the constant of the quadratic function does not affect the spectrum of the feature matrix. More generally, it can be shown through the integral trick of (Williams, 1997) that the function (t) + c for some constant shift c gives the same matrix c as the original function (t).\nA direct consequence of Remark 1 is that the coefficients d0, d1, d2 of the function sign(t) are four times those of 1t>0, as a result of the fact that sign(t) = 2 \u00b7 1t>0 1. Constant shifts have, as such, no consequence in classification or clustering applications. Remark 2 (Universality of quadratic and Leaky ReLU functions). Ignoring the coefficient d0 that gives rise to a constant shift of all eigenvalues of \u0303c and thus of no practical relevance, observe from Table 2 that by tuning the parameters of the quadratic and Leaky ReLU functions (LReLU(t)), one can select arbitrary positive value for the ratio d1/d2, while the other listed functions have constraints linking d1 to d2.\nFollowing the discussions in Remark 2, the parameters &+, & of the LReLU, as well as &1, &2 of the quadratic function, essentially act to balance the weights of means\nand covariances in the mixture model of the data. More precisely, as &+\n& ! 1 or &2 &1, more emphasis is set on\nthe \u201cdistance\u201d between covariance matrices while &+ & ! 1 or &1 &2 stresses the differences in means.\nIn Figure 5, spectral clustering on four classes of Gaussian data is performed: N (\u00b51,C1), N (\u00b51,C2), N (\u00b52,C1) and N (\u00b52,C2) with the LReLU function that takes different values for &+ and & . For a = 1, 2, \u00b5a =\u21e5 0a 1; 5;0p a \u21e4 and Ca = \u21e3 1 + 15(a 1)p\np\n\u2318 Ip. By choosing\n&+ = & = 1 (equivalent to (t) = |t|) and &+ = & = 1 (equivalent to the linear map (t) = t), with the leading two eigenvectors we always recover two classes instead of four, as each setting of parameters only allows for a part of the statistical information of the data to be used for clustering. However, by taking &+ = 1, & = 0 (the ReLU function) we distinguish all four classes in the leading two eigenvectors, to which the k-means method can then be applied for final classification, as shown in Figure 6.\nOf utmost importance for random feature-based spectral methods (such as kernel spectral clustering discussed above (Ng et al., 2002)) is the presence of informative eigenvectors in the spectrum of G, and thus of c. To gain a deeper understanding on the spectrum of c, one can rewrite \u0303 in the more compact form,\n\u0303 = d1\u2326 T\u2326+VAVT + d0IT (2)\nwhere\nV \u2318 h\nJp p , ,\u2326TM\ni , A \u2318\n2 4 A11 d2t d1IK d2tT d2 0 d1IK 0 0 3 5\nwith A11 \u2318 d1MTM + d2(ttT + 2S), that is akin to the so-called \u201cspiked model\u201d in the random matrix literature (Baik et al., 2005), as it equals, if d1 6= 0, the sum of some standard (noise-like) random matrix \u2326T\u2326, and a low rank (here up to 2K + 1) informative matrix VAVT, that may induce some isolated eigenvalues outside the main bulk of eigenvalues in the spectrum of \u0303c, as shown in Figure 1.\nThe eigenvectors associated to these eigenvalues often contain crucial information about the data statistics (the classes in a classification settings). In particular, note that the matrix V contains the canonical vector ja of class Ca and we thus hope to find some isolated eigenvector of c aligned to ja that can be directly used to perform clustering. Intuitively speaking, if the matrix A contains sufficient energy (has sufficiently large operator norm), the eigenvalues associated to the small rank matrix VAVT may jump out from the main bulk of \u2326T\u2326 and becomes \u201cisolated\u201d as in Figure 1, referred to as the phase transition phenomenon in the random matrix literature (Baik et al., 2005). The associated eigenvectors then tend to align to linear combinations of the canonical vectors ja as seen in Figure 5-6. This alignment between the isolated eigenvectors and ja is essentially measured by the amplitude of the eigenvalues of the matrix A11, or more concretely, the statistical differences of the data (namely, t, S and M). Therefore, a good adaptation of the ratio d1/d2 ensures the (asymptotic) detectability of different classes from the spectrum of c.\nOn the Spectrum of Random Features Maps of High Dimensional Data"}, {"heading": "4. Numerical Validations", "text": "We complete this article by showing that our theoretical results, derived from Gaussian mixture models, show an unexpected close match in practice when applied to some real-world datasets. We consider two different types of classification tasks: one on handwritten digits of the popular MNIST (LeCun et al., 1998) database (number 6 and 8), and the other on epileptic EEG time series data (Andrzejak et al., 2001) (set B and E). These two datasets are typical examples of means-dominant (handwritten digits recognition) and covariances-dominant (EEG times series classification) tasks. This is numerically confirmed in Table 3. Python 3 codes to reproduce the results in this section are available at https://github.com/Zhenyu-LIAO/RMT4RFM.\nTable 3. Empirical estimation of (normalized) differences in means and covariances of the MNIST (Figure 7 and 8) and epileptic EEG (Figure 9 and 10) datasets.\nkMTMk kttT + 2Sk MNIST DATA 172.4 86.0 EEG DATA 1.2 182.7"}, {"heading": "4.1. Handwritten digits recognition", "text": "We perform random feature-based spectral clustering on data matrices that consist of T = 32, 64 and 128 randomly selected vectorized images of size p = 784 from the MNIST dataset. The \u201ctrue\u201d means and covariances are empirically obtained from the full set of 11 769 MNIST images (5 918 images of number 6 and 5 851 of number 8) to construct the matrix \u0303c as per Theorem 1. Comparing the matrix c built from the data and the theoretically equivalent \u0303c obtained as if the data were Gaussian with the (empirically) computed means and covariances, we observe an extremely close fit in the behavior of the eigenvalues in Figure 7, as well of the leading eigenvector in Figure 8. The k-means method is then applied to the leading two eigenvectors of the matrix Gc 2 RT\u21e5T that consists of n = 32 random features to perform unsupervised classification, with resulting accuracies (averaged over 50 runs) reported in Table 4. As remarked from Table 3, the mean-oriented (t) functions are expected to outperform the covariance-oriented ones in this task, which is consistent with the results in Table 4."}, {"heading": "4.2. EEG time series classification", "text": "The epileptic EEG dataset5, developed by the University of Bonn, Germany, is described in (Andrzejak et al., 2001). The dataset consists of five subsets (denoted A-E), each containing 100 single-channel EEG segments of 23.6-sec\n5 http://www.meb.unibonn.de/epileptologie/\nscience/physik/eegdata.html.\n1 2 3 4 5 6 0\n0.5\n1\n1.5 Eigenvalues of c Eigenvalues of \u0303c\n1 2 3 4 5 6 0\n0.5\n1 1.5 Eigenvalues of c Eigenvalues of \u0303c\n1 2 3 4 5 6 0\n0.5\n1 1.5 Eigenvalues of c Eigenvalues of \u0303c\nFigure 7. Eigenvalue distribution of c and \u0303c for the MNIST data, with the ReLU function, p = 784, T = 128 and c1 = c2 = 12 , with j1 = \u21e5 1T1 ;0T2 \u21e4 and j2 = \u21e5 0T1 ;1T2 \u21e4 . Expectation estimated by averaging over 500 realizations of W.\nLeading eigenvector for MNIST data Simulation: mean/std for MNIST data\nTheory: mean/std for Gaussian data\nC1 C2\nFigure 8. Leading eigenvector of c for the MNIST and Gaussian mixture data with a width of \u00b11 standard deviations (generated from 500 trials) in the settings of Figure 7.\nduration. Sets A and B were collected from surface EEG recordings of five healthy volunteers, while sets C, D and E were collected from the EEG records of the pre-surgical diagnosis of five epileptic patients. Here we perform random feature-based spectral clustering on T = 32, 64 and 128 randomly picked EEG segments of length p = 100 from the dataset. Means and covariances are empirically estimated from the full set (4 097 segments of set B and 4 097 segments of set E). Similar behavior of eigenpairs as for Gaussian mixture models is once more observed in Figure 9 and 10. After k-means classification on the leading two eigenvectors of the (centered) Gram matrix composed of n = 32 random features, the accuracies (averaged over 50 runs) are reported in Table 5.\nAs opposed to the MNIST image recognition task, from Table 5 it is easy to check that the covariance-oriented functions (i.e., (t) = |t|, cos(t) and exp( t2/2)) far outperform any other with almost perfect classification accuracies. It is particularly interesting to note that the popular ReLU function is suboptimal in both tasks, but never performs very badly, thereby offering a good risk-performance tradeoff."}, {"heading": "5. Conclusion", "text": "In this article, we have provided a theoretical analysis on random feature-based spectral algorithms for large dimensional data, providing a better understanding of the precise mechanism underlying these methods. Our results show a quite simple relation between the nonlinear function involved in the random feature map (only through two scalars\nd1 and d2) and the capacity of the latter to discriminate data upon their means and covariances. In obtaining this result, we demonstrated that point-wise nonlinearities can be incorporated into a classical Taylor expansion as a consequence of the concentration phenomenon in high dimensional space. This result was then validated through experimental classification tasks on the MNIST and EEG datasets.\nAlthough Theorem 1 is stated here solely for the functions listed in Table 1, our current line of investigation consists in directly linking the activation function (\u00b7) and the coefficients d0, d1 and d2 in Table 2 so as to generalize our results and to provide more insights into the attributes of a function that makes it mean- or covariance-oriented; this undertaking is however more technically demanding but still likely achievable through the extension of existing results related to the work of (Cheng & Singer, 2013).\nFrom a point of view of clustering, the crucial information to distinguish different classes is contained in the isolated eigenvalue/eigenvector pairs as shown in (2), the asymptotic behavior of these pairs, as well as their significance for clustering are technically reachable within the analysis framework presented in this paper. When W follows a non-Gaussian distribution, or when different nonlinearities are combined (e.g., cos+ sin to get the Gaussian kernel (Rahimi & Recht, 2008)), obtaining the equivalent kernel in the large p, T regime would be a key enabler to gain a deeper understanding under these more elaborate settings.\nBesides, this paper can be taken as a first step of the random matrix-based understanding of various learning methods using random features, for example the randomly designed deep neural networks (Lillicrap et al., 2016), the nonlinear activation of which being the main difficulty for a thorough analysis. Moreover, along with recent advances in random matrix analysis (Tiomoko Ali & Couillet, 2016), the hyperparameter d1/d2 of utmost importance envisions to be consistently estimated and thus allows for an efficient tuning technique for all nonlinear random feature-based methods."}, {"heading": "Acknowledgments", "text": "We thank the anonymous reviewers for their comments and constructive suggestions. We would like to acknowledge this work is supported by the ANR Project RMT4GRAPH (ANR-14-CE28-0006) and the Project DeepRMT of La Fondation Supe\u0301lec."}], "year": 2018, "references": [{"title": "Indications of nonlinear deterministic and finite-dimensional structures in time series of brain electrical activity: Dependence on recording region and brain state", "authors": ["R.G. Andrzejak", "K. Lehnertz", "F. Mormann", "C. Rieke", "P. David", "C.E. Elger"], "venue": "Physical Review E,", "year": 1907}, {"title": "Phase transition of the largest eigenvalue for nonnull complex sample covariance matrices", "authors": ["J. Baik", "G.B. Arous", "S P\u00e9ch\u00e9"], "venue": "The Annals of Probability,", "year": 2005}, {"title": "The spectrum of random innerproduct kernel matrices", "authors": ["X. Cheng", "A. Singer"], "venue": "Random Matrices: Theory and Applications,", "year": 2013}, {"title": "Kernel spectral clustering of large dimensional data", "authors": ["R. Couillet", "F Benaych-Georges"], "venue": "Electronic Journal of Statistics,", "year": 2016}, {"title": "The spectrum of kernel random matrices", "authors": ["N El Karoui"], "venue": "The Annals of Statistics,", "year": 2010}, {"title": "Extreme learning machine for regression and multiclass classification", "authors": ["Huang", "G.-B", "H. Zhou", "X. Ding", "R. Zhang"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics),", "year": 2012}, {"title": "Sketching for large-scale learning of mixture models", "authors": ["N. Keriven", "A. Bourrier", "R. Gribonval", "P. P\u00e9rez"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "year": 2016}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "authors": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "year": 2012}, {"title": "Random synaptic feedback weights support error backpropagation for deep learning", "authors": ["T.P. Lillicrap", "D. Cownden", "D.B. Tweed", "C.J. Akerman"], "venue": "Nature communications,", "year": 2016}, {"title": "A random matrix approach to neural networks", "authors": ["C. Louart", "Z. Liao", "R Couillet"], "venue": "The Annals of Applied Probability,", "year": 2018}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "authors": ["A.L. Maas", "A.Y. Hannun", "A.Y. Ng"], "venue": "In Proc. ICML,", "year": 2013}, {"title": "Distribution of eigenvalues for some sets of random matrices", "authors": ["V.A. Mar\u010denko", "L.A. Pastur"], "venue": "Mathematics of the USSR-Sbornik,", "year": 1967}, {"title": "On spectral clustering: Analysis and an algorithm", "authors": ["A.Y. Ng", "M.I. Jordan", "Y. Weiss"], "venue": "In Advances in neural information processing systems,", "year": 2002}, {"title": "Nonlinear random matrix theory for deep learning", "authors": ["J. Pennington", "P. Worah"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2017}, {"title": "Random features for large-scale kernel machines", "authors": ["A. Rahimi", "B. Recht"], "venue": "In Advances in neural information processing systems,", "year": 2008}, {"title": "Randomness in neural networks: an overview", "authors": ["S. Scardapane", "D. Wang"], "venue": "Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery,", "year": 2017}, {"title": "Deep learning in neural networks: An overview", "authors": ["J. Schmidhuber"], "venue": "Neural networks,", "year": 2015}, {"title": "Learning with kernels: support vector machines, regularization, optimization, and beyond", "authors": ["B. Sch\u00f6lkopf", "A.J. Smola"], "venue": "MIT press,", "year": 2002}, {"title": "On the empirical distribution of eigenvalues of a class of large dimensional random matrices", "authors": ["J.W. Silverstein", "Z. Bai"], "venue": "Journal of Multivariate analysis,", "year": 1995}, {"title": "Spectral community detection in heterogeneous large networks", "authors": ["H. Tiomoko Ali", "R. Couillet"], "venue": "arXiv preprint arXiv:1611.01096,", "year": 2016}, {"title": "Efficient additive kernels via explicit feature maps", "authors": ["A. Vedaldi", "A. Zisserman"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "year": 2012}, {"title": "Computing with infinite networks", "authors": ["C.K. Williams"], "venue": "Advances in neural information processing systems,", "year": 1997}], "id": "SP:fc5974757c617b5fae14bfa6fe3310236de91ee3", "authors": [{"name": "Zhenyu Liao", "affiliations": []}, {"name": "Romain Couillet", "affiliations": []}], "abstractText": "Random feature maps are ubiquitous in modern statistical machine learning, where they generalize random projections by means of powerful, yet often difficult to analyze nonlinear operators. In this paper, we leverage the \u201cconcentration\u201d phenomenon induced by random matrix theory to perform a spectral analysis on the Gram matrix of these random feature maps, here for Gaussian mixture models of simultaneously large dimension and size. Our results are instrumental to a deeper understanding on the interplay of the nonlinearity and the statistics of the data, thereby allowing for a better tuning of random featurebased techniques.", "title": "On the Spectrum of Random Features Maps of High Dimensional Data"}