0	24	Fine-grained opinion mining involves identifying the opinion holder who expresses the opinion, detecting opinion expressions, measuring their intensity and sentiment, and identifying the target or aspect of the opinion (Wiebe et al., 2005).
1	42	For example, in the sentence “John says, the hard disk is very noisy”, John, the opinion holder, expresses a very negative (i.e., sentiment with intensity) opinion towards the target “hard disk” using the opinionated expression “very noisy”.
3	16	The tasks in fine-grained opinion mining can be regarded as either a token-level sequence labeling problem or as a semantic compositional task at the sequence (e.g., phrase) level.
4	20	For example, identifying opinion holders, opinion expressions and opinion targets can be formulated as a token-level sequence tagging problem, where the task is to label each word in a sentence using the conventional BIO tagging scheme.
7	52	Conditional random fields (CRFs) (Lafferty et al., 2001) have been quite successful for different fine-grained opinion mining tasks, e.g., opinion expression extraction (Yang and Cardie, 2012).
14	32	Motivated by the recent success of deep learning, in this paper we propose a general class of models based on RNN architecture and word embeddings, that can be successfully applied to finegrained opinion mining tasks without any taskspecific feature engineering effort.
15	18	We experiment with several important RNN architectures including Elman-RNN, Jordan-RNN, long short term memory (LSTM) and their variations.
17	15	The RNN models then fine-tune the word vectors during training to learn task-specific embeddings.
53	21	Formally, the probability of k-th label in the output for classification into K classes: P (yt = k|s, θ) = exp (w T k ht)∑K k=1 exp (w T k ht) (1) where, ht = φ(xt) defines the transformations of xt through the hidden layers, and wk are the weights from the last hidden layer to the output layer.
73	35	A memory block is composed of four elements: (i) a memory cell c (i.e., a neuron) with a self-connection, (ii) an input gate i to control the flow of input signal into the neuron, (iii) an output gate o to control the effect of the neuron activation on other neurons, and (iv) a forget gate f to allow the neuron to adaptively reset its current state through the self-connection.
80	17	In our example sentence (Table 1), to correctly tag the word hard as a B-TARG, it is beneficial for the RNN to know that the next word is disk.
88	18	In an Elman-type bidirectional RNN (Fig.
90	26	The concatenated vector ht = [ −→ ht, ←− ht] is passed to the output layer.
104	31	As shown in Figure 2b, this can be done in our RNNs by feeding these additional features directly to the output layer, and learn their associated weights in training.
109	25	Each word in the input layer is represented by M features, each of which has an embedding vector associated with it in a lookup table.
113	21	They released their pre-trained 300-dimensional word embeddings (vocabulary size 3M ) trained by the skip-gram model on part of Google news dataset containing about 100 billion words.4
117	26	Datasets: In our experiments, we use the two review datasets provided by the SemEval-2014 task 4: aspect-based sentiment analysis evaluation campaign (Pontiki et al., 2014), namely the Laptop and the Restaurant datasets.
128	31	The features used in the baseline model include the current word, its POS tag, its prefixes and suffixes between one to four characters, its position, its stylistics (e.g., case, digit, symbol, alphanumeric), and its context (i.e., the same features for the two preceding and the two following words).
156	36	The contribution of embeddings in CRF is also validated by the 10-fold results in Table 4 (see first two rows), where SENNA embeddings yield significant improvements – absolute 1.47% on Laptop (p < 0.03) and absolute 1.24% on Restaurant (p < 0.01).
163	33	Comparison among RNN Models: A comparison among the RNN models in Table 3 tells that Elman RNN generally outperforms Jordan RNN.
185	16	For example, the absolute gains for fine-tuning SENNA embeddings in Elman RNN are 13.01% in Laptop and 4.11% in Restaurant.
187	35	Comparison with SemEval-2014 Systems: When our RNN results are compared with the top performing systems in the SemEval-2014 (last two rows in Table 3), we see that RNNs without using any linguistic features achieve the second best results on both Laptop and Restaurant datasets.
190	64	achieves the best results on the Laptop dataset.
192	94	We used pre-trained word embeddings from three external sources in different RNN architectures including Elman-type, Jordantype, LSTM and their several variations.
193	31	Our results on the opinion target extraction task demonstrate that word embeddings improve the performance of both CRF and RNN models, however, fine-tuning them in RNNs on the task gives the best results.
194	122	RNNs outperform CRFs, even when they use word embeddings as the only features.
195	16	Incorporating simple linguistic features into RNNs improves the performance further.
196	25	Our best results with LSTM RNN outperform the top performing system on the Laptop dataset and achieve the second best on the Restaurant dataset in SemEval-2014 evaluation campaign.
198	61	In the future, we would like apply our models to other fine-grained opinion mining tasks including opinion expression detection and characterizing the intensity and sentiment of the opinion expressions.
199	78	We would also like to explore to what extent these tasks can be jointly modeled in an RNN-based multi-task learning framework.
