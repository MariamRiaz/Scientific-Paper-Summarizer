0	16	The aspect-based sentiment analysis (ABSA) task is to identify opinions expressed towards specific entities such as laptop or attributes of entities such as price (Liu, 2012a).
1	65	This task involves three subtasks: Aspect Term Extraction (ATE), Aspect Polarity Detection and Aspect Category Detection.
3	55	One of most important characteristics is that opinion words can provide indicative clues for aspect detection since opinion words should co-occur with aspect words.
5	108	Some works tackling the ATE task ignore the consideration of opinion words and just focus on aspect term modeling and learning (Jin et al., 2009; Jakob and Gurevych, 2010; Toh and Wang, 2014; Chernyshevich, 2014; Manek et al., 2017; San Vicente et al., 2015; Liu et al., 2015; Poria et al., 2016; Toh and Su, 2016; Yin et al., 2016).
12	5	As unsupervised or partially supervised frameworks cannot take the full advantages of aspect annotations commonly found in the training data, the above methods lead to deficiency in leveraging the data.
17	9	We propose MIN (Memory Interaction Network), a novel LSTM-based deep multi-task learning framework for the ATE task.
18	17	Two LSTMs with extended memory are designed for handling 2886 the extraction tasks of aspects and opinions.
19	42	The aspect-opinion relationship is established based on neural memory interactions between aspect extraction and opinion extraction where the global indicator score of opinion terms and local positional relevance between aspects and opinions are considered.
20	19	To ensure that aspects are from sentimental sentences, MIN employs a third LSTM for sentimental sentence classification facilitating more accurate aspect term extraction.
23	17	The ATE task is treated as a sequence labeling task with BIO tagging scheme and the set of aspect tags for the word wt is yAt ∈ {B, I,O}, where B, I,O represent beginning of, inside and outside of the aspect span respectively.
25	7	In our multi-task learning framework, three tasks are involved: (1) aspect term extraction (ATE), (2) opinion word extraction and (3) sentimental sentence classification.
30	12	The first component of our framework MIN is composed of A-LSTM and O-LSTM.
31	21	Both LSTMs have extended memories for task-level memory interactions.
32	6	A-LSTM involves a large aspect memory HAt ∈ Rnm×dim A h and an opinion summary vectormOt ∈ Rdim O h whereHAt contains nm pieces of aspect hidden states of dimension dimAh and m O t is distilled from H O t .
39	10	In the memory-enhanced A-LSTM and OLSTM, we manually design three kinds of operations: (1) READ to select nm pieces of aspect (opinion) hidden states from the past memories and build HAt (H O t ); (2) DIGEST to distill an aspect (opinion)-specific summary mAt (m O t ) from HAt (H O t ) where influences of opinion terms and relative positions of inputs are considered; (3) INTERACT to perform interaction between ALSTM and O-LSTM using the task specific summaries (i.e., mAt and m O t ).
44	17	Therefore, MIN selects memory segments temporally related to wt.
59	9	From Equation 3,mOt is dominated by the associated memory segment of wt−i that obtains the high combined weights.
62	4	[:] denotes vector concatenation operation.
68	30	The work flow of memory interaction and the update process of the internal memories in OLSTM are kept same with those in A-LSTM except the DIGEST operation.
71	5	The design and the process of the memory update in this component are similar to that in Jozefowicz et al. (2015).
73	116	To avoid this kind of error, we add a constraint that an aspect term should come from sentimental sentence.
74	14	Specifically, S-LSTM learns the sentimental representation hST of the sentence and then feeds it in aspect prediction as a soft constraint: P (yAt |xt) = softmax(WAfc([hAt : hST ])) (5) where WAfc denotes the weight matrix of the fullyconnected softmax layer.
77	129	For ALSTM and O-LSTM, we use the token-level cross-entropy error between the predicted distribution P (yTt |xt) and the gold standard distribution P (yT ,gt |xt) as the loss function (T ∈ {A,O}): Loss(T ) = − 1 N ∗ T N∑ i=1 T∑ t=1 P (Y T ,gi,t |Xi,t) log[P (Y Ti,t |Xi,t)] (6) For S-LSTM, sentence-level cross entropy error are employed to calculate the corresponding loss: Loss(S) = − 1 N N∑ i=1 P (Y S,gi |Xi) log[P (Y Si |Xi)] (7) Then, losses from different LSTMs are combined to form the training objective of the MIN framework:
78	9	We conduct experiments on two benchmark datasets from SemEval ABSA challenge (Pontiki et al., 2014, 2016) as shown in Table 1.
79	15	D1 (SemEval 2014) contains reviews from the laptop domain and D2 (SemEval 2016) contains reviews from the restaurant domain.
80	5	In these datasets, aspect terms have been labeled and sentences containing at least one golden truth aspect are regarded as sentimental sentences.
84	4	• Semi-CRF: First-order semi-Markov conditional random fields (Sarawagi et al., 2004) and the feature template in Cuong et al. (2014) is adopted.
85	34	• IHS RD (Chernyshevich, 2014), NLANGP (Toh and Su, 2016): Best systems in ATE subtask in SemEval ABSA challenges (Pontiki et al., 2014, 2016).
90	22	For datasets in the restaurant domain, we train word embeddings of dimension 200 with word2vec (Mikolov et al., 2013) on Yelp reviews5.
91	17	For those in laptop domain, we use pre-trained glove.840B.300d6.
