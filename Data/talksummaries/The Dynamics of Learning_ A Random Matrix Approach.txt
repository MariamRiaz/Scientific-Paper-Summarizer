0	21	Deep neural networks trained with backpropagation have commonly attained superhuman performance in applications of computer vision (Krizhevsky et al., 2012) and many others (Schmidhuber, 2015) and are thus receiving an unprecedented research interest.
2	38	One of the salient features of deep networks today is that they often have far more model parameters than the number of training samples that they are trained on, but meanwhile some of the models still exhibit remarkably good generalization performance when applied to unseen data of similar nature, while others generalize poorly in exactly the same setting.
4	9	To answer such a question, statistical learning theory has proposed interpretations from the viewpoint of system complexity (Vapnik, 2013; Bartlett & Mendelson, 2002; Poggio et al., 2004).
7	7	Inspired by the recent line of works (Saxe et al., 2013; Advani & Saxe, 2017), in this article we introduce a random matrix framework to analyze the training and, more importantly, the generalization performance of neural networks, trained by gradient descent.
8	8	Preliminary results established from a toy model of two-class classification on a single-layer linear network are presented, which, despite their simplicity, shed new light on the understanding of many important aspects in training neural nets.
19	7	In the remainder of the article, we introduce the problem of interest and recall the results of (Saxe et al., 2013) in Section 2.
20	11	After a brief overview of basic concepts and methods to be used throughout the article in Section 3, our main results on the training and generalization performance of the network are presented in Section 4, followed by a thorough discussion in Section 5 and experiments on the popular MNIST database (LeCun et al., 1998) in Section 6.
22	16	,xn 2 Rp be independent vectors drawn from two distribution classes C1 and C2 of cardinality n1 and n2 (thus n1 + n2 = n), respectively.
24	21	In the context of a binary classification problem, one takes the label yi = 1 for xi 2 C1 and yj = 1 for xj 2 C2 to distinguish the two classes.
30	24	Following the previous works of (Saxe et al., 2013; Advani & Saxe, 2017), when the learning rate ↵ is small, wt+1 and wt are close to each other so that by performing a continuous-time approximation, one obtains the following differential equation @w(t) @t = ↵@L(w) @w = ↵ n X y XTw(t) the solution of which is given explicitly by w(t) = e ↵t n XX T w0 + ⇣ Ip e ↵t n XX T ⌘ (XXT) 1Xy (1) if one assumes that XXT is invertible (only possible in the case p < n), with w0 ⌘ w(t = 0) the initialization of the weight vector; we recall the definition of the exponential of a matrix 1nXX T given by the power series e 1nXX T = P1 k=0 1 k!
31	28	( 1 nXX T)k = Ve⇤VT, with the eigendecomposition of 1nXX T = V⇤VT and e⇤ is a diagonal matrix with elements equal to the exponential of the elements of ⇤.
35	52	entries and that there is no linking structure between the data and associated targets in such a way that the “true” weight vector w̄ to be learned is independent of X so as to simplify the analysis.
44	40	For certain simple distributions of M, one may define a so-called deterministic equivalent (Hachem et al., 2007; Couillet & Debbah, 2011) Q̄M for QM, which is a deterministic matrix such that for all A 2 Rn⇥n and all a,b 2 Rn of bounded (spectral and Euclidean, respectively) norms, 1n tr (AQM) 1 n tr AQ̄M !
45	21	As such, deterministic equivalents allow to transfer random spectral properties of M in the form of deterministic limiting quantities and thus allows for a more detailed investigation.
49	6	With Cauchy’s integral formula, one is able to evaluate more sophisticated functionals of the random matrix M. For example, for f(M) ⌘ aTeMb one has f(M) = 1 2⇡i I exp(z)aTQM(z)b dz with a positively oriented path circling around all the eigenvalues of M. Moreover, from the previous subsection one knows that the bilinear form aTQM(z)b is asymptotically close to a non-random quantity aTQ̄M(z)b.
56	7	entries of zero mean, variance 2 /p for some > 0 and finite fourth moment.
57	5	We first focus on the generalization performance, i.e., the average performance of the trained classifier taking as input an unseen new datum x̂ drawn from class C1 or C2.
62	6	For µTw(t), with Cauchy’s integral formula we have µTw(t) = µTe ↵t n XX T w0 + µ T ⇣ Ip e ↵t n XX T ⌘ wLS = 1 2⇡i I ft(z)µ T ✓ 1 n XXT zIp ◆ 1 w0 dz 1 2⇡i I 1 ft(z) z µT ✓ 1 n XXT zIp ◆ 1 1 n Xy dz with ft(z) ⌘ exp( ↵tz), for a positive closed path circling around all eigenvalues of 1nXX T. Note that the data matrix X can be rewritten as X = µjT1 + µjT2 + Z = µyT + Z with Z ⌘ ⇥ z1, .
71	12	Since 1nXX T and 1nX TX have the same eigenvalues except for additional zero eigenvalues for the larger matrix, the path remains unchanged (as we demand that contains the origin) and hence Theorem 1 holds true for both p < n and p > n. The case p = n can be obtained by continuity arguments.
74	9	Note that the i-th entry of XTw(t) is given by the bilinear form eTi XTw(t), with ei the canonical vector with unique non-zero entry [ei]i = 1.
78	6	0 almost surely, with E⇤ ⌘ 1 2⇡i I 1 ft(z) z dz (kµk2 + c)m(z) + 1 V⇤ ⌘ 1 2⇡i I " 1 z (1 ft(z)) 2 (kµk2 + c)m(z) + 1 2 f 2 t (z)zm(z) # dz.
79	9	In Figure 1 we compare finite dimensional simulations with theoretical results obtained from Theorem 1 and 2 and observe a very close match, already for not too large n, p. As t grows large, the generalization error first drops rapidly with the training error, then goes up, although slightly, while the training error continues to decrease to zero.
81	28	Training and generalization performance for µ = [2;0p 1], p = 256, n = 512, 2 = 0.1, ↵ = 0.01 and c1 = c2 = 1/2.
82	11	Results obtained by averaging over 50 runs.
83	7	X and performs badly on unseen ones.
84	12	To avoid overfitting, one effectual approach is to apply regularization strategies (Bishop, 2007), for example, to “early stop” (at t = 100 for instance in the setting of Figure 1) in the training process.
85	51	However, this introduces new hyperparameters such as the optimal stopping time topt that is of crucial importance for the network performance and is often tuned through cross-validation in practice.
86	41	Theorem 1 and 2 tell us that the training and generalization performances, although being random themselves, have asymptotically deterministic behaviors described by (E⇤, V⇤) and (E, V ), respectively, which allows for a deeper understanding on the choice of topt, since E, V are in fact functions of t via ft(z) ⌘ exp( ↵tz).
90	12	We rewrite the path (that contains all eigenvalues of 1nXX T) as the sum of two paths b and s, that circle around the main bulk and the isolated eigenvalue (if any), respectively.
92	66	m(z) ⌘ m̌( ) exists (Silverstein & Choi, 1995) and follow the idea in (Bai & Silverstein, 2008) by choosing the contour b to be a rectangle with sides parallel to the axes, intersecting the real axis at 0 and + and the horizontal sides being a distance " !
