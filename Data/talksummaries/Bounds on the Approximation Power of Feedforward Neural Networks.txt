1	23	An important problem then is to determine the smallest neural network for a given task and accuracy.
2	62	The standard guideline is the approximation power (variously known as expressiveness) of the network which quantifies the size of the neural network, typically in terms of depth and width, in order to approximate a class of functions within a given error.
3	37	In particular, several works provided evidence that deeper networks perform better than shallow ones, given a fixed number of hidden units (Bianchini & Scarselli, 2014; Delalleau & Bengio, 2011; Liang & Srikant, 2017; Mhaskar et al., 2016; Pascanu et al., 2014; Telgarsky, 2015; 2016; Yarotsky, 2017).1 A popular activation function is the rectified linear unit (ReLU), partly because of its low complexity when coupled with backpropagation training (Krizhevsky et al., 2012).
4	5	It has, therefore, become of interest to determine the power of neural networks with ReLU’s and, more generally, with piecewise linear activation functions.
5	3	Determining the capacity of a neural networks with a piecewise linear activation function typically involves two steps.
6	102	First, evaluate the number of linear pieces (or break points) that the network can produce and, second, tie this number to the approximation error.
7	27	The works (Montufar et al., 2014; Pascanu et al., 2014) recently showed that a linear increase in depth results in an exponential growth in the number of linear pieces as opposed to width which results only in a polynomial growth.
8	31	Accordingly, the approximation capacity exhibits a similar tradeoff between depth and width.
9	156	For related works with respect to classification error see (Telgarsky, 2015; 2016) and with respect to function approximation error see (Liang & Srikant, 2017; Mhaskar et al., 2016; Yarotsky, 2017).
10	47	In this paper we consider general feedforward neural networks with piecewise linear activation functions and establish bounds on the size of the network in terms of the approximation error, the depth d, the width, and the dimension of the input space to approximate a given function.
11	21	We first establish an improved upper bound on the number of break points that such a network can produce which is a multiplicative factor dd smaller than the currently best known from (Yarotsky, 2017).
12	61	This upper bound is obtained by investigating neuron state transitions as introduced in (Raghu et al., 2017).
13	27	Combining this upper bound with lower bounds in terms of error and dimension, we obtain necessary conditions on the depth, width, error, and dimension for a neural network to approximate a given function.
14	17	These bounds significantly improve on the corresponding state-of-the-art bounds for certain classes of functions (Theorems 1,2 and Corollaries 1,2,3).
15	40	The second contribution of the paper (Theorem 3) is an upper bound on the difference of two neural networks with identical weights but different activation functions.
16	38	This problem is related to “activation function simulation” investigated in (DasGupta & Schnitger, 1993) which leverages network topology to compensate a change in activation function.
17	7	The paper is organized as follows.
20	23	Finally, Section 5 contains the proofs.
21	24	Throughout the paper R denotes a compact convex set in Rn, n ≥ 1, and Fσ denotes the set of feedforward neural networks with input R, output R, and activation function σ : R → R. Feedforward here refers to the fact that the neural network contains no cycles; connections are allowed between non-neighbouring layers.
28	11	Definition 1 (Depth and width).
29	12	Given a neural network f ∈ Fσ, the depth of a hidden unit h ∈ Hf , denoted as df (h), is the length of the longest path from any i ∈ If to h. The depth of f is df def = max { df (h) ∣∣h ∈ Hf}.
30	12	The set of hidden units with depth i is Hif def = { h ∈ Hf ∣∣df (h) = i}.
39	17	Finally, we let B̄x→y(f) def = Bx→y(f) + 1.
