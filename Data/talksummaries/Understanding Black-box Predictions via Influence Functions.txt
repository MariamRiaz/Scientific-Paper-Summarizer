0	59	A key question often asked of machine learning systems is “Why did the system make this prediction?” We want models that are not just high-performing but also explainable.
2	13	However, the best-performing models in many domains — e.g., deep neural networks for image and speech recognition (Krizhevsky et al., 2012) — are complicated, blackbox models whose predictions seem hard to explain.
3	66	Work on interpreting these black-box models has focused on understanding how a fixed model leads to particular predictions, e.g., by locally fitting a simpler model around the test point (Ribeiro et al., 2016) or by perturbing the test point to see how the prediction changes (Simonyan et al., 2013; Li et al., 2016b; Datta et al., 2016; Adler et al., 2016).
5	17	In this paper, we tackle this question by tracing a model’s predictions through its learning algorithm and back to the training data, where the model parameters ultimately derive from.
13	32	Influence functions capture the core idea of studying models through the lens of their training data.
14	16	We show that they are a versatile tool that can be applied to a wide variety of seemingly disparate tasks: understanding model behavior, debugging models, detecting dataset errors, and creating visually-indistinguishable adversarial training examples that can flip neural network test predictions, the training set analogue of Goodfellow et al. (2015).
21	15	Our goal is to understand the effect of training points on a model’s predictions.
33	37	Let us develop a finer-grained notion of influence by studying a different counterfactual: how would the model’s predictions change if a training input were modified?
53	18	We highlight two key differences from x · xtest.
54	40	First, σ(−yθ>x) gives points with high training loss more influence, revealing that outliers can dominate the model parameters.
55	29	Second, the weighted covariance matrix H−1 θ̂ measures the “resistance” of the other training points to the removal of z; if ∇θL(z, θ̂) points in a direction of little variation, its influence will be higher since moving in that direction will not significantly increase the loss on other training points.
56	19	As we show in Fig 1, these differences mean that influence functions capture the effect of model training much more accurately than nearest neighbors.
59	58	With n training points and θ ∈ Rp, this requires O(np2 + p3) operations, which is too expensive for models like deep neural networks with millions of parameters.
61	39	The first problem is well-studied in second-order optimization.
62	26	The idea is to avoid explicitly computing H−1 θ̂ ; in- stead, we use implicit Hessian-vector products (HVPs) to efficiently approximate stest def = H−1 θ̂ ∇θL(ztest, θ̂) and then compute Iup,loss(z, ztest) = −stest · ∇θL(z, θ̂).
63	24	This also solves the second problem: for each test point of interest, we can precompute stest and then efficiently compute −stest · ∇θL(zi, θ̂) for each training point zi.
69	24	With large datasets, standard CG can be slow; each iteration still goes through all n training points.
95	19	Here, λ is a damping term that we add ifHθ̃ has negative eigenvalues; this corresponds to adding L2 regularization on θ.
100	31	What happens when the derivatives of the loss, ∇θL and ∇2θL, do not exist?
109	150	By telling us the training points “responsible” for a given prediction, influence functions reveal insights about how models rely on and extrapolate from the training data.
110	71	In this section, we show that two models can make the same correct predictions but get there in very different ways.
111	55	We compared (a) the state-of-the-art Inception v3 network (Szegedy et al., 2016) with all but the top layer frozen6 to (b) an SVM with an RBF kernel on a dog vs. fish image classification dataset we extracted from ImageNet (Russakovsky et al., 2015), with 900 training examples for each class.
112	110	Freezing neural networks in this way is not uncommon in computer vision and is equivalent to training a logistic regression model on the bottleneck features (Donahue et al., 2014).
114	13	As expected, Iup,loss in the RBF SVM varied inversely with raw pixel distance, with training images far from the test image in pixel space having almost no influence.
118	66	In contrast, in the Inception network, fish and dogs could be helpful or harmful for correctly classifying the test image as a fish; in fact, some of the most helpful training images were dogs that, to the model, looked very different from the test fish (Fig 4-Top).
119	35	In this section, we show that models that place a lot of influence on a small number of points can be vulnerable to training input perturbations, posing a serious security risk in real-world ML systems where attackers can influence the training data (Huang et al., 2011).
120	25	Recent work has generated adversarial test images that are visually indistinguish- able from real test images but completely fool a classifier (Goodfellow et al., 2015; Moosavi-Dezfooli et al., 2016).
122	17	To the best of our knowledge, this is the first proof-of-concept that visually-indistinguishable training attacks can be executed on otherwise highly-accurate neural networks.
123	31	The key idea is that Ipert,loss(z, ztest) tells us how to modify training point z to most increase the loss on ztest.
124	43	Concretely, for a target test image ztest, we can construct z̃i, an adversarial version of a training image zi, by initializing z̃i := zi and then iterating z̃i := Π(z̃i + α sign(Ipert,loss(z̃i, ztest))), where α is the step size and Π projects onto the set of valid images that share the same 8- bit representation with zi.
