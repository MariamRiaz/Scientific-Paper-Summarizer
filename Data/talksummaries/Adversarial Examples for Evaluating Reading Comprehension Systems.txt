21	28	Each model has a single and an ensemble version, yielding four systems in total.
27	40	To determine whether existing models have learned much beyond such simple patterns, we introduce adversaries that confuse deficient models by altering test examples.
31	64	While standard test error measures the fraction of the test distribution over which the model gets the correct answer, the adversarial accuracy measures the fraction over which the model is robustly correct, even in the face of adversarially-chosen alterations.
32	32	For this quantity to be meaningful, the adversary must satisfy two basic requirements: first, it should always generate (p′, q′, a′) tuples that are valid—a human would judge a′ as the correct answer to q′ given p′.
34	60	In image classification, adversarial examples are commonly generated by adding an imperceptible amount of noise to the input (Szegedy et al., 2014; Goodfellow et al., 2015).
35	47	These perturbations do not change the semantics of the image, but they can change the predictions of models that are oversensitive to semantics-preserving changes.
37	183	However, high-precision paraphrase generation is challenging, as most edits to a sentence do actually change its meaning.
38	20	Instead of relying on paraphrasing, we use perturbations that do alter semantics to build concatenative adversaries, which generate examples of the form (p + s, q, a) for some sentence s. In other words, concatenative adversaries add a new sentence to the end of the paragraph, and leave the question and answer unchanged.
39	26	Valid adversarial examples are precisely those for which s does not contradict the correct answer; we refer to such sentences as being compatible with (p, q, a).
59	20	In our running example, the correct answer was not tagged as a named entity, and has the POS tag NNP, which corresponds to the fake answer “Central Park.” In Step 3, we combine the altered question and fake answer into declarative form, using a set of roughly 50 manually-defined rules over CoreNLP constituency parses.
83	403	Without this assumption, we would have to rely on something like the F1 score of the argmax prediction, which is piecewise constant and therefore harder to optimize.
85	28	We do not do anything to ensure that the sentences generated by this search procedure do not contradict the original answer.
91	62	For all experiments, we measure adversarial F1 score (Rajpurkar et al., 2016) across 1000 randomly sampled examples from the SQuAD development set (the test set is not publicly available).
95	27	Each model incurred a significant accuracy drop under every form of adversarial evaluation.
104	98	We hypothesize that Mnemonic Reader’s self-alignment layer, which helps model long-distance relationships between parts of the paragraph, makes it better at locating all pieces of evidence that support the correct answer.
115	48	Even if we showed the same original example to five sets of three crowdworkers, chances are that at least one of the five groups would make a mistake, just because humans naturally err.
117	96	Next, we sought to better understand the behavior of our four main models under adversarial evaluation.
133	30	Humans predicted from the adversarial sentence on only 27.3% of these error cases, which confirms that many errors are normal mistakes unrelated to adversarial sentences.
136	19	For example, on a question about the “Kalven Report”, the adversarial sentence discussed “The statement Kalven cited” instead; in another case, the question, “How does Kenya curb corruption?” was met by the unhelpful sentence, “Tanzania is curbing corruption” (the model simply answered, “corruption”).
138	105	First, we found that models do well when the question has an exact n-gram match with the original paragraph.
144	24	For example, 32.7% of the questions in BiDAF Ensemble successes were 8 words or shorter, compared to only 11.8% for model failures.
146	60	For long questions, changing one word leaves many others unchanged, so the adversarial sentence still has many words in common with the question.
147	33	For short questions, changing one content word may be enough to make the adversarial sentence completely irrelevant.
148	27	In computer vision, adversarial examples that fool one model also tend to fool other models (Szegedy et al., 2014; Moosavi-Dezfooli et al., 2017); we investigate whether the same pattern holds for us.
149	184	Examples from ADDONESENT clearly do transfer across models, since ADDONESENT always adds the same adversarial sentence regardless of model.
155	21	Due to the prohibitive cost of running ADDSENT or ADDANY on the entire training set, we instead ran only Steps 1-3 of ADDSENT (everything except crowdsourcing) to generate a raw adversarial sentence for each training example.
156	34	We then trained the BiDAF model from scratch on the union of these examples and the original training data.
160	40	To demonstrate this, we created a variant of ADDSENT called ADDSENTMOD, which differs from ADDSENT in two ways: it uses a different set of fake answers (e.g., PERSON named entities map to “Charles Babbage” instead of “Jeff Dean”), and it prepends the adversarial sentence to the beginning of the paragraph instead of appending it to the end.
161	44	The retrained model does almost as badly as the original one on ADDSENTMOD, suggesting that it has just learned to ignore the last sentence and reject the fake answers that ADDSENT usually proposed.
162	112	In order for training on adversarial examples to actually improve the model, more care must be taken to ensure that the model cannot overfit the adversary.
