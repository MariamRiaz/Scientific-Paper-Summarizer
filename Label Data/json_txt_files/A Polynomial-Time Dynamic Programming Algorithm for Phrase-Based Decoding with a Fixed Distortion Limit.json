{"sections": [{"heading": "1 Introduction", "text": "Phrase-based translation models (Koehn et al., 2003; Och and Ney, 2004) are widely used in statistical machine translation. The decoding problem for phrase-based translation models is known to be difficult: the results from Knight (1999) imply that in the general case decoding of phrase-based translation models is NP-complete.\nThe complexity of phrase-based decoding comes from reordering of phrases. In practice, however, various constraints on reordering are often imposed in phrase-based translation systems. A common constraint is a \u201cdistortion limit\u201d, which places a hard constraint on how far phrases can move. The complexity of decoding with such a distortion limit is an open question: the NP-hardness result from Knight\n\u2217On leave from Columbia University.\n(1999) applies to a phrase-based model with no distortion limit.\nThis paper describes an algorithm for phrasebased decoding with a fixed distortion limit whose runtime is linear in the length of the sentence, and for a fixed distortion limit is polynomial in other factors. More specifically, for a hard distortion limit d, and sentence length n, the runtime is O(nd!lhd+1), where l is a bound on the number of phrases starting at any point in the sentence, and h is related to the maximum number of translations for any word in the source language sentence.\nThe algorithm builds on the insight that decoding with a hard distortion limit is related to the bandwidth-limited traveling salesman problem (BTSP) (Lawler et al., 1985). The algorithm is easily amenable to beam search. It is quite different from previous methods for decoding of phrase-based models, potentially opening up a very different way of thinking about decoding algorithms for phrasebased models, or more generally for models in statistical NLP that involve reordering."}, {"heading": "2 Related Work", "text": "Knight (1999) proves that decoding of word-to-word translation models is NP-complete, assuming that there is no hard limit on distortion, through a reduction from the traveling salesman problem. Phrasebased models are more general than word-to-word models, hence this result implies that phrase-based decoding with unlimited distortion is NP-complete.\nPhrase-based systems can make use of both reordering constraints, which give a hard \u201cdistortion limit\u201d on how far phrases can move, and reordering models, which give scores for reordering steps, often penalizing phrases that move long distances. Moses (Koehn et al., 2007b) makes use of a distortion limit, and a decoding algorithm that makes use\n59\nTransactions of the Association for Computational Linguistics, vol. 5, pp. 59\u201371, 2017. Action Editor: Holger Schwenk. Submission batch: 10/2016; Revision batch: 11/2016; Published 2/2017.\nc\u00a92017 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license.\nof bit-strings representing which words have been translated. We show in Section 5.2 of this paper that this can lead to at least 2n/4 bit-strings for an input sentence of length n, hence an exhaustive version of this algorithm has worst-case runtime that is exponential in the sentence length. The current paper is concerned with decoding phrase-based models with a hard distortion limit.\nVarious other reordering constraints have been considered. Zens and Ney (2003) and Zens et al. (2004) consider two types of hard constraints: the IBM constraints, and the ITG (inversion transduction grammar) constraints from the model of Wu (1997). They give polynomial time dynamic programming algorithms for both of these cases. It is important to note that the IBM and ITG constraints are different from the distortion limit constraint considered in the current paper. Decoding algorithms with ITG constraints are further studied by Feng et al. (2010) and Cherry et al. (2012).\nKumar and Byrne (2005) describe a class of reordering constraints and models that can be encoded in finite state transducers. Lopez (2009) shows that several translation models can be represented as weighted deduction problems and analyzes their complexities.1 Koehn et al. (2003) describe a beamsearch algorithm for phrase-based decoding that is in widespread use; see Section 5 for discussion.\nA number of reordering models have been proposed, see for example Tillmann (2004), Koehn et al. (2007a) and Galley and Manning (2008).\nDeNero and Klein (2008) consider the phrase alignment problem, that is, the problem of finding an optimal phrase-based alignment for a sourcelanguage/target-language sentence pair. They show that in the general case, the phrase alignment problem is NP-hard. It may be possible to extend the techniques in the current paper to the phrasealignment problem with a hard distortion limit.\nVarious methods for exact decoding of phrasebased translation models have been proposed. Zaslavskiy et al. (2009) describe the use of travel-\n1An earlier version of this paper states the complexity of decoding with a distortion limit as O(I32d) where d is the distortion limit and I is the number of words in the sentence; however (personal communication from Adam Lopez) this runtime is an error, and should be O(2I) i.e., exponential time in the length of the sentence. A corrected version of the paper corrects this.\ning salesman algorithms for phrase-based decoding. Chang and Collins (2011) describe an exact method based on Lagrangian relaxation. Aziz et al. (2014) describe a coarse-to-fine approach. These algorithms all have exponential time runtime (in the length of the sentence) in the worst case.\nGalley and Manning (2010) describe a decoding algorithm for phrase-based systems where phrases can have discontinuities in both the source and target languages. The algorithm has some similarities to the algorithm we propose: in particular, it makes use of a state representation that contains a list of disconnected phrases. However, the algorithms differ in several important ways: Galley and Manning (2010) make use of bit string coverage vectors, giving an exponential number of possible states; in contrast to our approach, the translations are not formed in strictly left-to-right ordering on the source side."}, {"heading": "3 Background: The Traveling Salesman Problem on Bandwidth-Limited Graphs", "text": "This section first defines the bandwidth-limited traveling salesman problem, then describes a polynomial time dynamic programming algorithm for the traveling salesman path problem on bandwidth limited graphs. This algorithm is the algorithm proposed by Lawler et al. (1985)2 with small modifications to make the goal a path instead of a cycle, and to consider directed rather than undirected graphs."}, {"heading": "3.1 Bandwidth-Limited TSPPs", "text": "The input to the problem is a directed graph G = (V,E), where V is a set of vertices and E is a set of directed edges. We assume that V = {1, 2, . . . , n}. A directed edge is a pair (i, j) where i, j \u2208 V , and i 6= j. Each edge (i, j) \u2208 E has an associated weight wi,j . Given an integer k \u2265 1, a graph is bandwidth-limited with bandwidth k if\n\u2200(i, j) \u2208 E, |i\u2212 j| \u2264 k The traveling salesman path problem (TSPP) on\nthe graph G is defined as follows. We will assume that vertex 1 is the \u201csource\u201d vertex and vertex n is the \u201csink\u201d vertex. The TSPP is to find the minimum cost directed path from vertex 1 to vertex n, which passes through each vertex exactly once.\n2The algorithm is based on the ideas of Monien and Sudborough (1981) and Ratliff and Rosenthal (1983)."}, {"heading": "3.2 An Algorithm for Bandwidth-Limited TSPPs", "text": "The key idea of the dynamic-programming algorithm for TSPPs is the definition of equivalence classes corresponding to dynamic programming states, and an argument that the number of equivalence classes depends only on the bandwidth k.\nThe input to our algorithm will be a directed graph G = (V,E), with weights wi,j , and with bandwidth k. We define a 1-n path to be any path from the source vertex 1 to the sink vertex n that visits each vertex in the graph exactly once. A 1-n path is a subgraph (V \u2032, E\u2032) ofG, where V \u2032 = V andE\u2032 \u2286 E.\nWe will make use of the following definition:\nDefinition 1. For any 1-n path H , define Hj to be the subgraph that H induces on vertices 1, 2, . . . j, where 1 \u2264 j \u2264 n.\nThat is, Hj contains the vertices 1, 2, . . . j and the edges in H between these vertices.\nFor a given value for j, we divide the vertices V into three sets Aj , Bj and Cj :\n\u2022 Aj = {1, 2, . . . , (j \u2212 k)} (Aj is the empty set if j \u2264 k). \u2022 Bj = {1 . . . j} \\Aj .3 \u2022 Cj = {j + 1, j + 2, . . . , n}\n(Cj is the empty set if j = n).\nNote that the vertices in subgraph Hj are the union of the sets Aj and Bj . Aj is the empty set if j \u2264 k, but Bj is always non-empty.\nThe following Lemma then applies:\nLemma 1. For any 1-n path H in a graph with bandwidth k, for any 1 \u2264 j \u2264 n, the subgraph Hj has the following properties:\n1. If vertex 1 is in Aj , then vertex 1 has degree one.\n2. For any vertex v \u2208 Aj with v \u2265 2, vertex v has degree two.\n3. Hj contains no cycles.\nProof. The first and second properties are true because of the bandwidth limit. Under the constraint of bandwidth k, any edge (u, v) in H such that\n3For sets X and Y we use the notation X \\ Y to refer to the set difference: i.e., X \\ Y = {x|x \u2208 X and x /\u2208 Y }.\nu \u2208 Aj , must have v \u2208 Aj \u222a Bj = Hj . This follows because if v \u2208 Cj = {j + 1, j + 2, . . . n} and u \u2208 Aj = {1, 2, . . . j \u2212 k}, then |u \u2212 v| > k. Similarly any edge (u, v) \u2208 H such that v \u2208 Aj must have u \u2208 Aj \u222a Bj = Hj . It follows that for any vertex u \u2208 Aj , with u > 1, there are edges (u, v) \u2208 Hj and (v\u2032, u) \u2208 Hj , hence vertex u has degree 2. For vertex u \u2208 Aj with u = 1, there is an edge (u, v) \u2208 Hj , hence vertex u has degree 1. The third property (no cycles) is true because Hj is a subgraph of H , which has no cycles.\nIt follows that each connected component of Hj is a directed path, that the start points of these paths are in the set {1} \u222a Bj , and that the end points of these paths are in the set Bj .\nWe now define an equivalence relation on subgraphs. Two subgraphs Hj and H \u2032j are in the same equivalence class if the following conditions hold (taken from Lawler et al. (1985)):\n1. For any vertex v \u2208 Bj , the degree of v in Hj and H \u2032j is the same. 2. For each path (connected component) in Hj there is a path in H \u2032j with the same start and end points, and conversely.\nThe significance of this definition is as follows. Assume that H\u2217 is an optimal 1-n path in the graph, and that it induces the subgraph Hj on vertices 1 . . . j. Assume that H \u2032j is another subgraph over vertices 1 . . . j, which is in the same equivalence class as Hj . For any subgraph Hj , define c(Hj) to be the sum of edge weights in Hj :\nc(Hj) = \u2211\n(u,v)\u2208Hj wu,v\nThen it must be the case that c(H \u2032j) \u2265 c(Hj). Otherwise, we could simply replace Hj by H \u2032j in H\n\u2217, thereby deriving a new 1-n path with a lower cost, implying that H\u2217 is not optimal.\nThis observation underlies the dynamic programming approach. Define \u03c3 to be a function that maps a subgraph Hj to its equivalence class \u03c3(Hj). The equivalence class \u03c3(Hj) is a data structure that stores the degrees of the vertices inBj , together with the start and end points of each connected component in Hj .\nNext, define \u2206 to be a set of 0, 1 or 2 edges between vertex (j + 1) and the vertices in Bj . For any subgraph Hj+1 of a 1-n path, there is some \u2206, simply found by recording the edges incident to vertex (j + 1). For any Hj , define \u03c4(\u03c3(Hj),\u2206) to be the equivalence class resulting from adding the edges in \u2206 to the data structure \u03c3(Hj). If adding the edges in \u2206 to \u03c3(Hj) results in an ill-formed subgraph\u2014for example, a subgraph that has one or more cycles\u2014 then \u03c4(\u03c3(Hj),\u2206) is undefined. The following recurrence then defines the dynamic program (see Eq. 20 of Lawler et al. (1985)):\n\u03b1(j + 1, S) = min \u2206,S\u2032:\u03c4(S\u2032,\u2206)=S\n( \u03b1(j, S\u2032) + c(\u2206) )\nHere S is an equivalence class over vertices {1 . . . (j+1)}, and \u03b1(S, j+1) is the minimum score for any subgraph in equivalence class S. The min is taken over all equivalence classes S\u2032 over vertices {1 . . . j}, together with all possible values for \u2206."}, {"heading": "4 A Dynamic Programming Algorithm for Phrase-Based Decoding", "text": "We now describe the dynamic programming algorithm for phrase-based decoding with a fixed distortion limit. We first give basic definitions for phrasebased decoding, and then describe the algorithm."}, {"heading": "4.1 Basic Definitions", "text": "Consider decoding an input sentence consisting of words x1 . . . xn for some integer n. We assume that x1 = <s> and xn = </s> where <s> and </s> are the sentence start and end symbols respectively. A phrase-based lexicon specifies a set of possible translations in the form of phrases p = (s, t, e), where s and t are integers such that 1 \u2264 s \u2264 t \u2264 n, and e is a sequence of m \u2265 1 target-language words e1 . . . em. This signifies that words xs . . . xt in the source language have a translation as e1 . . . em in the target language. We use s(p), t(p) and e(p) to refer to the three components of a phrase p = (s, t, e), and e1(p) . . . em(p) to refer to the words in the targetlanguage string e(p). We assume that (1, 1,<s>) and (n, n,</s>) are the only translation entries with s(p) \u2264 1 and t(p) \u2265 n respectively.\nA derivation is then defined as follows: Definition 2 (Derivations). A derivation is a sequence of phrases p1 . . . pL such that\n\u2022 p1 = (1, 1,<s>) and pL = (n, n,</s>). \u2022 Each source word is translated exactly once. \u2022 The distortion limit is satisfied for each pair of phrases pi\u22121, pi, that is:\n|t(pi\u22121) + 1\u2212 s(pi)| \u2264 d \u2200 i = 2 . . . L.\nwhere d is an integer specifying the distortion limit in the model.\nGiven a derivation p1 . . . pL, a target-language translation can be obtained by concatenating the target-language strings e(p1) . . . e(pL).\nThe scoring function is defined as follows:\nf(p1 . . . pL) = \u03bb(e(p1) . . . e(pL)) +\nL\u2211\ni=1\n\u03ba(pi)\n+ L\u2211\ni=2\n\u03b7 \u00d7 |t(pi\u22121) + 1\u2212 s(pi)| (1)\nFor each phrase p, \u03ba(p) is the translation score for the phrase. The parameter \u03b7 is the distortion penalty, which is typically a negative constant. \u03bb(e) is a language model score for the string e. We will assume a bigram language model:\n\u03bb(e1 . . . em) = m\u2211\ni=2\n\u03bb(ei|ei\u22121).\nThe generalization of our algorithm to higher-order n-gram language models is straightforward.\nThe goal of phrase-based decoding is to find y\u2217 = arg maxy\u2208Y f(y) where Y is the set of valid derivations for the input sentence.\nRemark (gap constraint): Note that a common restriction used in phrase-based decoding (Koehn et al., 2003; Chang and Collins, 2011), is to impose an additional \u201cgap constraint\u201d while decoding. See Chang and Collins (2011) for a description. In this case it is impossible to have a dynamicprogramming state where word xi has not been translated, and where word xi+k has been translated, for k > d. This limits distortions further, and it can be shown in this case that the number of possible bitstrings is O(2d) where d is the distortion limit. Without this constraint the algorithm of Koehn et al. (2003) actually fails to produce translations for many input sentences (Chang and Collins, 2011)."}, {"heading": "4.2 The Algorithm", "text": "We now describe the dynamic programming algorithm. Intuitively the algorithm builds a derivation by processing the source-language sentence in strictly left-to-right order. This is in contrast with the algorithm of Koehn et al. (2007b), where the targetlanguage sentence is constructed from left to right.\nThroughout this section we will use \u03c0, or \u03c0i for some integer i, to refer to a sequence of phrases:\n\u03c0 = \u2329 p1 . . . pl \u232a\nwhere each phrase pi = (s(pi), t(pi), e(pi)), as defined in the previous section.\nWe overload the s, t and e operators, so that if \u03c0 = \u2329 p1 . . . pl \u232a , we have s(\u03c0) = s(p1), t(\u03c0) = t(pl), and e(\u03c0) = e(p1) \u00b7e(p2) . . . \u00b7e(pl), where x \u00b7y is the concatenation of strings x and y.\nA derivation H consists of a single phrase sequence \u03c0 = \u2329 p1 . . . pL \u232a :\nH = \u03c0 = \u2329 p1 . . . pL \u232a\nwhere the sequence p1 . . . pL satisfies the constraints in definition 2.\nWe now give a definition of sub-derivations and complement sub-derivations:\nDefinition 3 (Sub-derivations and Complement Sub-derivations). For any H = \u2329 p1 . . . pL \u232a , for any j \u2208 {1 . . . n} such that \u2203 i \u2208 {1 . . . L} s.t. t(pi) = j, the sub-derivation Hj and the complement subderivation H\u0304j are defined as\nHj =\u3008\u03c01 . . . \u03c0r\u3009, H\u0304j = \u3008\u03c0\u03041 . . . \u03c0\u0304r\u3009\nwhere the following properties hold: \u2022 r is an integer with r \u2265 1. \u2022 Each \u03c0i for i = 1 . . . r is a sequence of one or more phrases, where each phrase p \u2208 \u03c0i has t(p) \u2264 j.\n\u2022 Each \u03c0\u0304i for i = 1 . . . (r\u22121) is a sequence of one or more phrases, where each phrase p \u2208 \u03c0\u0304i has s(p) > j.\n\u2022 \u03c0\u0304r is a sequence of zero or more phrases, where each phrase p \u2208 \u03c0\u0304r has s(p) > j. We have zero phrases in \u03c0\u0304r iff j = n where n is the length of the sentence.\n\u2022 Finally, \u03c01 \u00b7 \u03c0\u03041 \u00b7 \u03c02 \u00b7 \u03c0\u03042 . . . \u03c0r \u00b7 \u03c0\u0304r = p1 . . . pL where x \u00b7 y denotes the concatenation of phrase sequences x and y. Note that for any j \u2208 {1 . . . n} such that @i \u2208 {1 . . . L} such that t(pi) = j, the sub-derivation Hj and the complement sub-derivation H\u0304j is not defined.\nThus for each integer j such that there is a phrase in H ending at point j, we can divide the phrases in H into two sets: phrases p with t(p) \u2264 j, and phrases pwith s(p) > j. The sub-derivationHj lists all maximal sub-sequences of phrases with t(p) \u2264 j. The complement sub-derivation H\u0304j lists all maximal sub-sequences of phrases with s(p) > j.\nFigure 1 gives all sub-derivations Hj for the derivation\nH = \u2329\u2329 p1 . . . p7 \u232a\u232a\n= \u2329\u2329 (1, 1,<s>)(2, 3,we must)(4, 4, also)\n(8, 8, take)(5, 6, these criticisms) (7, 7, seriously)(9, 9,</s>) \u232a\u232a\nAs one example, the sub-derivation H7 = \u3008\u03c01, \u03c02\u3009 induced by H has two phrase sequences:\n\u03c01 = \u2329 (1, 1,<s>)(2, 3,we must)(4, 4, also) \u232a \u03c02 = \u2329 (5, 6, these criticisms)(7, 7, seriously) \u232a\nNote that the phrase sequences \u03c01 and \u03c02 give translations for all words x1 . . . x7 in the sentence. There\nare two disjoint phrase sequences because in the full derivation H , the phrase p = (8, 8, take), with t(p) = 8 > 7, is used to form a longer sequence of phrases \u03c01 p \u03c02.\nFor the above example, the complement subderivation H\u03047 is as follows:\n\u03c0\u03041 = \u2329 (8, 8, take) \u232a \u03c0\u03042 = \u2329 (9, 9,</s>) \u232a\nIt can be verified that \u03c01 \u00b7 \u03c0\u03041 \u00b7\u03c02 \u00b7 \u03c0\u03042 = H as required by the definition of sub-derivations and complement sub-derivations.\nWe now state the following Lemma: Lemma 2. For any derivation H = p1 . . . pL, for any j such that \u2203i such that t(pi) = j, the subderivation Hj = \u3008\u03c01 . . . \u03c0r\u3009 satisfies the following properties:\n1. s(\u03c01) = 1 and e1(\u03c01) = <s>. 2. For all positions i \u2208 {1 . . . j}, there exists a\nphrase p \u2208 \u03c0, for some phrase sequence \u03c0 \u2208 Hj , such that s(p) \u2264 i \u2264 t(p). 3. For all i = 2 . . . r, s(\u03c0i) \u2208 {(j \u2212 d+ 2) . . . j} 4. For all i = 1 . . . r, t(\u03c0i) \u2208 {(j \u2212 d) . . . j}\nHere d is again the distortion limit.\nThis lemma is a close analogy of Lemma 1. The proof is as follows: Proof of Property 1: For all values of j, the phrase p1 = (1, 1,<s>) has t(p1) \u2264 j, hence we must have \u03c01 = p1 . . . pk for some k \u2208 {1 . . . L}. It follows that s(\u03c01) = 1 and e1(\u03c01) = <s>. Proof of Property 2: For any position i \u2208 {1 . . . j}, define the phrase (s, t, e) in the derivation H to be the phrase that covers word i; i.e., the phrase such that s \u2264 i \u2264 t. We must have s \u2208 {1 . . . j}, because s \u2264 i and i \u2264 j. We must also have t \u2208 {1 . . . j}, because otherwise we have s \u2264 j < t, which contradicts the assumption that there is some i \u2208 {1 . . . L} such that t(pi) = j. It follows that the phrase (s, t, e) has t \u2264 j, and from the definition of sub-derivations it follows that the phrase is in one of the phrase sequences \u03c01 . . . \u03c0r. Proof of Property 3: This follows from the distortion limit. Consider the complement sub-derivation H\u0304j = \u3008\u03c0\u03041 . . . \u03c0\u0304r\u3009. For the distortion limit to be satisfied, for all i \u2208 {2 . . . r}, we must have\n|t(\u03c0\u0304i\u22121) + 1\u2212 s(\u03c0i)| \u2264 d\nWe must also have t(\u03c0\u0304i\u22121) > j, and s(\u03c0i) \u2264 j, by the definition of sub-derivations. It follows that s(\u03c0i) \u2208 {(j \u2212 d+ 2) . . . j}. Proof of Property 4: This follows from the distortion limit. First consider the case where \u03c0\u0304r is non-empty. For the distortion limit to be satisfied, for all i \u2208 {1 . . . r}, we must have\n|t(\u03c0i) + 1\u2212 s(\u03c0\u0304i)| \u2264 d\nWe must also have t(\u03c0i) \u2264 j, and s(\u03c0\u0304i) > j, by the definition of sub-derivations. It follows that t(\u03c0i) \u2208 {(j \u2212 d) . . . j}.\nNext consider the case where \u03c0\u0304r is empty. In this case we must have j = n. For the distortion limit to be satisfied, for all i \u2208 {1 . . . (r\u22121)}, we must have\n|t(\u03c0i) + 1\u2212 s(\u03c0\u0304i)| \u2264 d\nWe must also have t(\u03c0i) \u2264 j, and s(\u03c0\u0304i) > j, by the definition of sub-derivations. It follows that t(\u03c0i) \u2208 {(j\u2212d) . . . j} for i \u2208 {1 . . . (r\u22121)}. For i = r, we must have t(\u03c0i) = n, from which it again follows that t(\u03c0r) = n \u2208 {(j \u2212 d) . . . j}.\nWe now define an equivalence relation between sub-derivations, which will be central to the dynamic programming algorithm. We define a function \u03c3 that maps a phrase sequence \u03c0 to its signature. The signature is a four-tuple:\n\u03c3(\u03c0) = (s, ws, t, wt).\nwhere s is the start position, ws is the start word, t is the end position and wt is the end word of the phrase sequence. We will use s(\u03c3), ws(\u03c3), t(\u03c3), and wt(\u03c3) to refer to each component of a signature \u03c3.\nFor example, given a phrase sequence\n\u03c0 = \u2329 (1, 1, <s>) (2, 2, we) (4, 4, also) \u232a ,\nits signature is \u03c3(\u03c0) = (1, <s>, 4, also). The signature of a sub-derivation Hj = \u3008\u03c01 . . . \u03c0r\u3009 is defined to be\n\u03c3(Hj) = \u3008\u03c3(\u03c01) . . . \u03c3(\u03c0r)\u3009.\nFor example, with H7 as defined above, we have \u03c3(H7) = \u2329( 1,<s>, 4, also ) , ( 5, these, 7, seriously )\u232a\nTwo partial derivationsHj andH \u2032j are in the same equivalence class iff \u03c3(Hj) = \u03c3(H \u2032j).\nWe can now state the following Lemma:\nLemma 3. Define H\u2217 to be the optimal derivation for some input sentence, and H\u2217j to be a subderivation of H\u2217. Suppose H \u2032j is another subderivation with j words, such that \u03c3(H \u2032j) = \u03c3(H \u2217 j ). Then it must be the case that f(H\u2217j ) \u2265 f(H \u2032j), where f is the function defined in Section 4.1.\nProof. Define the sub-derivation and complement sub-derivation of H\u2217 as\nH\u2217j = \u3008\u03c01 . . . \u03c0r\u3009 H\u0304\u2217j = \u3008\u03c0\u03041 . . . \u03c0\u0304r\u3009\nWe then have\nf(H\u2217) = f(H\u2217j ) + f(H\u0304 \u2217 j ) + \u03b3 (2)\nwhere f(. . .) is as defined in Eq. 1, and \u03b3 takes into account the bigram language modeling scores and the distortion scores for the transitions \u03c01 \u2192 \u03c0\u03041, \u03c0\u03041 \u2192 \u03c02, \u03c02 \u2192 \u03c0\u03042, etc.\nThe proof is by contradiction. Define\nH \u2032j = \u03c0 \u2032 1 . . . \u03c0 \u2032 r\nand assume that f(H\u2217j ) < f(H \u2032 j). Now consider\nH \u2032 = \u03c0\u20321\u03c0\u03041\u03c0 \u2032 2\u03c0\u03042 . . . \u03c0 \u2032 r\u03c0\u0304r\nThis is a valid derivation because the transitions \u03c0\u20321 \u2192 \u03c0\u03041, \u03c0\u03041 \u2192 \u03c0\u20322, \u03c0\u20322 \u2192 \u03c0\u03042 have the same distortion distances as \u03c01 \u2192 \u03c0\u03041, \u03c0\u03041 \u2192 \u03c02, \u03c02 \u2192 \u03c0\u03042, hence they must satisfy the distortion limit.\nWe have\nf(H \u2032) = f(H \u2032j) + f(H\u0304 \u2217 j ) + \u03b3 (3)\nwhere \u03b3 has the same value as in Eq. 2. This follows because the scores for the transitions \u03c0\u20321 \u2192 \u03c0\u03041, \u03c0\u03041 \u2192 \u03c0\u20322, \u03c0\u20322 \u2192 \u03c0\u03042 are identical to the scores for the transitions \u03c01 \u2192 \u03c0\u03041, \u03c0\u03041 \u2192 \u03c02, \u03c02 \u2192 \u03c0\u03042, because \u03c3(H\u2217j ) = \u03c3(H \u2032 j).\nIt follows from Eq. 2 and Eq. 3 that if f(H \u2032j) > f(H\u2217j ), then f(H\n\u2032) > f(H\u2217). But this contradicts the assumption that H\u2217 is optimal. It follows that we must have f(H \u2032j) \u2264 f(H\u2217j ).\nThis lemma leads to a dynamic programming algorithm. Each dynamic programming state consists of an integer j \u2208 {1 . . . n} and a set of r signatures:\nT = (j, {\u03c31 . . . \u03c3r})\nFigure 2 shows the dynamic programming algorithm. It relies on the following functions:\n\u2022 For any state T , \u03b4(T ) is the set of outgoing transitions from state T .\n\u2022 For any state T , for any transition \u2206 \u2208 \u03b4(T ), \u03c4(T,\u2206) is the state reached by transition \u2206 from state T .\n\u2022 For any state T , valid(T ) checks if a resulting state is valid.\n\u2022 For any transition \u2206, score(\u2206) is the score for the transition.\nWe next give full definitions of these functions."}, {"heading": "4.2.1 Definitions of \u03b4(T ) and \u03c4(T,\u2206)", "text": "Recall that for any state T , \u03b4(T ) returns the set\nof possible transitions from state T . In addition \u03c4(T,\u2206) returns the state reached when taking transition \u2206 \u2208 \u03b4(T ).\nGiven the state T = (j, {\u03c31 . . . \u03c3r}), each transition is of the form \u03c81 p\u03c82 where \u03c81, p and \u03c82 are defined as follows:\n\u2022 p is a phrase such that s(p) = j + 1. \u2022 \u03c81 \u2208 {\u03c31 . . . \u03c3r} \u222a {\u03c6}. If \u03c81 6= \u03c6, it must be the\ncase that |t(\u03c81) + 1\u2212 s(p)| \u2264 d and t(\u03c81) 6= n. \u2022 \u03c82 \u2208 {\u03c31 . . . \u03c3r} \u222a {\u03c6}. If \u03c82 6= \u03c6, it must be the\ncase that |t(p) + 1\u2212 s(\u03c82)| \u2264 d and s(\u03c82) 6= 1. \u2022 If \u03c81 6= \u03c6 and \u03c82 6= \u03c6, then \u03c81 6= \u03c82. Thus there are four possible types of transition from a state T = (j, {\u03c31 . . . \u03c3r}): Case 1: \u2206 = \u03c6 p\u03c6. In this case the phrase p is incorporated as a stand-alone phrase. The new state T \u2032 is equal to (j\u2032, {\u03c3\u20321 . . . \u03c3\u2032r+1}) where j\u2032 = t(p), where \u03c3\u2032i = \u03c3i for i = 1 . . . r, and \u03c3 \u2032 r+1 = (s(p), e1(p), t(p), em(p)).\nCase 2: \u2206 = \u03c3i p \u03c6 for some \u03c3i \u2208 {\u03c31 . . . \u03c3r}. In this case the phrase p is appended to the signature \u03c3i. The new state T \u2032 = \u03c4(T,\u2206) is of the form (j\u2032, \u03c3\u20321 . . . \u03c3 \u2032 r), where j\n\u2032 = t(p), where \u03c3i is replaced by (s(\u03c3i), ws(\u03c3i), t(p), em(p)), and where \u03c3\u2032i\u2032 = \u03c3i\u2032 for all i\u2032 6= i. Case 3: \u2206 = \u03c6 p\u03c3i for some \u03c3i \u2208 {\u03c31 . . . \u03c3r}. In this case the phrase p is prepended to the signature \u03c3i. The new state T \u2032 = \u03c4(T,\u2206) is of the form (j\u2032, \u03c3\u20321 . . . \u03c3 \u2032 r), where j\n\u2032 = t(p), where \u03c3i is replaced by (s(p), e1(p), t(\u03c3i), wt(\u03c3i)), and where \u03c3\u2032i\u2032 = \u03c3i\u2032 for all i\u2032 6= i. Case 4: \u2206 = \u03c3i p \u03c3i\u2032 for some \u03c3i, \u03c3i\u2032 \u2208 {\u03c31 . . . \u03c3r}, with i\u2032 6= i. In this case phrase p is appended to signature \u03c3i, and prepended to signature \u03c3i\u2032 , effectively joining the two signatures together. In this case the new state T \u2032 = \u03c4(T,\u2206) is of the form (j\u2032, \u03c3\u20321 . . . \u03c3 \u2032 r\u22121), where signatures \u03c3i and \u03c3i\u2032 are replaced by a new signature (s(\u03c3i), ws(\u03c3i), t(\u03c3i\u2032), wt(\u03c3i\u2032)), and all other signatures are copied across from T to T \u2032.\nFigure 3 gives the dynamic programming states and transitions for the derivation H in Figure 1. For example, the sub-derivation\nH7 = \u2329\u2329 (1, 1,<s>)(2, 3,we must)(4, 4, also) \u232a ,\n\u2329 (5, 6, these criticisms)(7, 7, seriously)\n\u232a\u232a\nwill be mapped to a state\nT = ( 7, \u03c3(H7) )\n= ( 7, { (1,<s>, 4, also), (5, these, 7, seriously) })\nThe transition \u03c31(8, 8, take)\u03c32 from this state leads to a new state,\nT \u2032 = ( 8, { \u03c31 = (1,<s>, 7, seriously) })"}, {"heading": "4.3 Definition of score(\u2206)", "text": "Figure 4 gives the definition of score(\u2206), which incorporates the language model, phrase scores, and distortion penalty implied by the transition \u2206.\n4.4 Definition of valid(T )\nFigure 5 gives the definition of valid(T ). This function checks that the start and end points of each signature are in the set of allowed start and end points given in Lemma 2."}, {"heading": "4.5 A Bound on the Runtime of the Algorithm", "text": "We now give a bound on the algorithm\u2019s run time. This will be the product of terms N and M , where N is an upper bound on the number of states in the dynamic program, and M is an upper bound on the number of outgoing transitions from any state.\nFor any j \u2208 {1 . . . n}, define first(j) to be the set of target-language words that can begin at position j and last(j) to be the set of target-language\nwords that can end at position j.\nfirst(j) = {w : \u2203 p = (s, t, e) s.t. s = j, e1 = w} last(j) = {w : \u2203 p = (s, t, e) s.t. t = j, em = w}\nIn addition, define singles(j) to be the set of phrases that translate the single word at position j:\nsingles(j) = {p : s(p) = j and t(p) = j}\nNext, define h to be the smallest integer such that for all j, |first(j)| \u2264 h, |last(j)| \u2264 h, and |singles(j)| \u2264 h. Thus h is a measure of the maximal ambiguity of any word xj in the input.\nFinally, for any position j, define start(j) to be the set of phrases starting at position j:\nstart(j) = {p : s(p) = j}\nand define l to be the smallest integer such that for all j, |start(j)| \u2264 l. Given these definitions we can state the following result:\nTheorem 1. The time complexity of the algorithm is O(nd!lhd+1).\nTo prove this we need the following definition:\nDefinition 4 (p-structures). For any finite set A of integers with |A| = k, a p-structure is a set of r ordered pairs {(si, ti)}ri=1 that satisfies the following properties: 1) 0 \u2264 r \u2264 k; 2) for each i \u2208 {1 . . . r}, si \u2208 A and ti \u2208 A (both si = ti and si 6= ti are allowed); 3) for each j \u2208 A, there is at most one index i \u2208 {1 . . . r} such that (si = j) or (ti = j) or (si = j and ti = j).\nWe use g(k) to denote the number of unique pstructures for a set A with |A| = k.\nWe then have the following Lemmas:\nLemma 4. The function g(k) satisfies g(0) = 0, g(1) = 2, and the following recurrence for k \u2265 2:\ng(k) = 2g(k \u2212 1) + 2(n\u2212 1)g(k \u2212 2)\nProof. The proof is in Appendix A.\nLemma 5. Consider the function h(k) = k2\u00d7g(k). h(k) is in O((k \u2212 2)!). Proof. The proof is in Appendix B.\nWe can now prove the theorem: Proof of Theorem 1: First consider the number of states in the dynamic program. Each state is of the form (j, {\u03c31 . . . \u03c3r}) where the set {(s(\u03c3i), t(\u03c3i))}ri=1 is a p-structure over the set {1}\u222a {(j \u2212 d) . . . d}. The number of possible values for {(s(\u03c3i), e(\u03c3i))}ri=1 is at most g(d + 2). For a fixed choice of {(s(\u03c3i), t(\u03c3i))}ri=1 we will argue that there are at most hd+1 possible values for {(ws(\u03c3i), wt(\u03c3i))}ri=1. This follows because for each k \u2208 {(j \u2212 d) . . . j} there are at most h possible choices: if there is some i such that s(\u03c3i) = k, and t(\u03c3i) 6= k, then the associated word ws(\u03c3i) is in the set first(k); alternatively if there is some i such that t(\u03c3i) = k, and s(\u03c3i) 6= k, then the associated word wt(\u03c3i) is in the set last(k); alternatively if there is some i such that s(\u03c3i) = t(\u03c3i) = k then the associated words ws(\u03c3i), wt(\u03c3i) must be the first/last word of some phrase in singles(k); alternatively there is no i such that s(\u03c3i) = k or t(\u03c3i) = k, in which case there is no choice associated with position k in the sentence. Hence there are at most h choices associated with each position k \u2208 {(j \u2212 d) . . . j}, giving hd+1 choices in total. Combining these results, and noting that there are\nn choices of the variable j, implies that there are at most ng(d+ 2)hd+1 states in the dynamic program.\nNow consider the number of transitions from any state. A transition is of the form \u03c81p\u03c82 as defined in Section 4.2.1. For a given state there are at most (d + 2) choices for \u03c81 and \u03c82, and l choices for p, giving at most (d+ 2)2l choices in total.\nMultiplying the upper bounds on the number of states and number of transitions for each state gives an upper bound on the runtime of the algorithm as O(ng(d + 2)hd+1(d + 2)2l). Hence by Lemma 5 the runtime is O(nd!lhd+1) time.\nThe bound g(d + 2) over the number of possible values for {(s(\u03c3i), e(\u03c3i))}ri=1 is somewhat loose, as the set of p-structures over {1} \u222a {(j \u2212 d) . . . d} includes impossible values {(si, ti)}ri=1 where for example there is no i such that s(\u03c3i) = 1. However the bound is tight enough to give the O(d!) runtime."}, {"heading": "5 Discussion", "text": "We conclude the paper with discussion of some issues. First we describe how the dynamic programming structures we have described can be used in conjunction with beam search. Second, we give more analysis of the complexity of the widely-used decoding algorithm of Koehn et al. (2003)."}, {"heading": "5.1 Beam Search", "text": "Beam search is widely used in phrase-based decoding; it can also be applied to our dynamic programming construction. We can replace the line\nfor each state T \u2208 Tj\nin the algorithm in Figure 2 with\nfor each state T \u2208 beam(Tj)\nwhere beam is a function that returns a subset of Tj , most often the highest scoring elements of Tj under some scoring criterion. A key question concerns the choice of scoring function \u03b3(T ) used to rank states. One proposal is to define \u03b3(T ) = \u03b1(T ) + \u03b2(T ) where \u03b1(T ) is the score used in the dynamic program, and \u03b2(T ) =\n\u2211 i:ws(\u03c3i)6=<s> \u03bbu(ws(\u03c3i)).\nHere \u03bbu(w) is the score of word w under a unigram language model. The \u03b2(T ) scores allow different states in Tj , which have different words ws(\u03c3i) at\nthe start of signatures, to be comparable: for example it compensates for the case wherews(\u03c3i) is a rare word, which will incur a low probability when the bigram \u3008w ws(\u03c3i)\u3009 for some word w is constructed during search.\nThe \u03b2(T ) values play a similar role to \u201cfuture scores\u201d in the algorithm of Koehn et al. (2003). However in the Koehn et al. (2003) algorithm, different items in the same beam can translate different subsets of the input sentence, making futurescore estimation more involved. In our case all items in Tj translate all words x1 . . . xj inclusive, which may make comparison of different hypotheses more straightforward."}, {"heading": "5.2 Complexity of Decoding with Bit-string Representations", "text": "A common method for decoding phrase-based models, as described in Koehn et al. (2003), is to use beam search in conjunction with a search algorithm that 1) creates the target language string in strictly left-to-right order; 2) uses a bit string with bits bi \u2208 {0, 1} for i = 1 . . . n representing at each point whether word i in the input has been translated. A natural question is whether the number of possible bit strings for a model with a fixed distortion limit d can grow exponentially quickly with respect to the length of the input sentence. This section gives an example that shows that this is indeed the case.\nAssume that our sentence length n is such that (n\u22122)/4 is an integer. Assume as before x1 = <s> and xn = </s>. For each k \u2208 {0 . . . ((n \u2212 2)/4 \u2212 1)}, assume we have the following phrases for the words x4k+2 . . . x4k+5:\n(4k + 2, 4k + 2, uk) (4k + 3, 4k + 3, vk)\n(4k + 4, 4k + 4, wk) (4k + 5, 4k + 5, zk)\n(4k + 4, 4k + 5, yk)\nNote that the only source of ambiguity is for each k whether we use yk to translate the entire phrase x4k+4x4k+5, or whether we use wk and zk to translate x4k+4 and x4k+5 separately.\nWith a distortion limit d \u2265 5, the number of possible bit strings in this example is at least 2(n\u22122)/4. This follows because for any setting of the variables b4k+4 \u2208 {0, 1} for k \u2208 {0 . . . ((n \u2212 2)/4 \u2212 1)},\nthere is a valid derivation p1 . . . pL such that the prefix p1 . . . pl where l = 1 + (n \u2212 2)/4 gives this bit string. Simply choose p1 = (1, 1,<s>) and for l\u2032 \u2208 {0 . . . (n\u2212 2)/4\u2212 1} choose pl\u2032+2 = (4l\u2032+ 4, 4l\u2032+ 5, yi) if b4k+4 = 1, pl\u2032+2 = (4l\u2032 + 5, 4l\u2032 + 5, zi) otherwise. It can be verified that p1 . . . pl is a valid prefix (there is a valid way to give a complete derivation from this prefix). As one example, for n = 10, and b4 = 1 and b8 = 0, a valid derivation is\n(1, 1,<s>)(4, 5, y1)(9, 9, z2)(7, 7, v2)(3, 3, v1)\n(2, 2, u1)(6, 6, u2)(8, 8, w2)(10, 10,</s>)\nIn this case the prefix (1, 1,<s>)(4, 5, y1)(9, 9, z2) gives b4 = 1 and b8 = 0. Other values for b4 and b8 can be given by using (5, 5, z1) in place of (4, 5, y1), and (8, 9, y2) in place of (9, 9, z2), with the following phrases modified appropriately."}, {"heading": "6 Conclusion", "text": "We have given a polynomial-time dynamic programming algorithm for phrase-based decoding with a fixed distortion limit. The algorithm uses a quite different representation of states from previous decoding algorithms, is easily amenable to beam search, and leads to a new perspective on phrase-based decoding. Future work should investigate the effectiveness of the algorithm in practice."}, {"heading": "A Proof of Lemma 4", "text": "Without loss of generality assume A = {1, 2, 3, . . . k}. We have g(1) = 2, because in this case the valid p-structures are {(1, 1)} and \u2205. To calculate g(k) we can sum over four possibilities:\nCase 1: There are g(k \u2212 1) p-structures with si = ti = 1 for some i \u2208 {1 . . . r}. This follows because once si = ti = 1 for some i, there are g(k \u2212 1) possible p-structures for the integers {2, 3, 4 . . . k}. Case 2: There are g(k \u2212 1) p-structures such that si 6= 1 and ti 6= 1 for all i \u2208 {1 . . . r}. This follows because once si 6= 1 and ti 6= 1 for all i, there are g(k \u2212 1) possible p-structures for the integers {2, 3, 4 . . . k}. Case 3: There are (k\u2212 1)\u00d7 g(k\u2212 2) p-structures such that there is some i \u2208 {1 . . . r} with si = 1 and ti 6= 1. This follows because for the i such that\nsi = 1, there are (k\u2212 1) choices for the value for ti, and there are then g(k \u2212 2) possible p-structures for the remaining integers in the set {1 . . . k}/{1, ti}. Case 4: There are (k\u2212 1)\u00d7 g(k\u2212 2) p-structures such that there is some i \u2208 {1 . . . r} with ti = 1 and si 6= 1. This follows because for the i such that ti = 1, there are (k\u2212 1) choices for the value for si, and there are then g(k \u2212 2) possible p-structures for the remaining integers in the set {1 . . . k}/{1, si}.\nSumming over these possibilities gives the following recurrence: g(k) = 2g(k \u2212 1) + 2(k \u2212 1)\u00d7 g(k \u2212 2)"}, {"heading": "B Proof of Lemma 5", "text": "Recall that h(k) = f(k)\u00d7 g(k) where f(k) = k2. Define k0 to be the smallest integer such that for all k \u2265 k0, 2f(k)\nf(k \u2212 1) + 2f(k) f(k \u2212 2) \u00b7 k \u2212 1 k \u2212 3 \u2264 k \u2212 2 (4)\nFor f(k) = k2 we have k0 = 9. Now choose a constant c such that for all k \u2208 {1 . . . (k0 \u2212 1)}, h(k) \u2264 c \u00d7 (k \u2212 2)!. We will prove by induction that under these definitions of k0 and c we have h(k) \u2264 c(k \u2212 2)! for all integers k, hence h(k) is in O((k \u2212 2)!).\nFor values k \u2265 k0, we have h(k) = f(k)g(k)\n= 2f(k)g(k \u2212 1) + 2f(k)(k \u2212 1)g(k \u2212 2) (5)\n= 2f(k) f(k \u2212 1)h(k \u2212 1) + 2f(k) f(k \u2212 2)(k \u2212 1)h(k \u2212 2) \u2264 ( 2cf(k)\nf(k \u2212 1) + 2cf(k) f(k \u2212 2) \u00b7 k \u2212 1 k \u2212 3\n) (k \u2212 3)! (6)\n\u2264 c(k \u2212 2)! (7) Eq. 5 follows from g(k) = 2g(k\u22121)+2(k\u22121)g(k\u2212 2). Eq. 6 follows by the inductive hypothesis that h(k \u2212 1) \u2264 c(k \u2212 3)! and h(k \u2212 2) \u2264 c(k \u2212 4)!. Eq 7 follows because Eq. 4 holds for all k \u2265 k0."}], "year": 2017, "references": [{"title": "Exact decoding for phrase-based statistical machine translation", "authors": ["Wilker Aziz", "Marc Dymetman", "Lucia Specia."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.", "year": 2014}, {"title": "Exact decoding of phrase-based translation models through Lagrangian relaxation", "authors": ["Yin-Wen Chang", "Michael Collins."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 26\u201337. Association for Computational Lin-", "year": 2011}, {"title": "On hierarchical re-ordering and permutation parsing for phrase-based decoding", "authors": ["Colin Cherry", "Robert C Moore", "Chris Quirk."], "venue": "Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 200\u2013209. Association for Computational Lin-", "year": 2012}, {"title": "The complexity of phrase alignment problems", "authors": ["John DeNero", "Dan Klein."], "venue": "Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers, pages 25\u201328. Association for Computational", "year": 2008}, {"title": "An efficient shift-reduce decoding algorithm for phrasedbased machine translation", "authors": ["Yang Feng", "Haitao Mi", "Yang Liu", "Qun Liu."], "venue": "Proceedings of the 23rd International Conference on Computational Linguistics: Posters, pages 285\u2013293. Association for Compu-", "year": 2010}, {"title": "A simple and effective hierarchical phrase reordering model", "authors": ["Michel Galley", "Christopher D Manning."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 848\u2013856. Association for Computational Linguistics.", "year": 2008}, {"title": "Accurate non-hierarchical phrase-based translation", "authors": ["Michel Galley", "Christopher D Manning."], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 966\u2013974.", "year": 2010}, {"title": "Decoding complexity in wordreplacement translation models", "authors": ["Kevin Knight."], "venue": "Computational Linguistics, 25(4).", "year": 1999}, {"title": "Statistical phrase-based translation", "authors": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu."], "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, pages", "year": 2003}, {"title": "Edinburgh system description for the 2005 IWSLT speech translation evaluation", "authors": ["Philipp Koehn", "Amittai Axelrod", "Chris Callison-Burch", "Miles Osborne", "David Talbot."], "venue": "Proceedings of the Second Workshop on Statistical Machine Translation, StatMT \u201907,", "year": 2007}, {"title": "Moses: Open source toolkit for statistical machine translation", "authors": ["Zens", "Chris Dyer", "Ond\u0159ej Bojar", "Alexandra Constantin", "Evan Herbst."], "venue": "Proceedings of the 45th annual meeting of the ACL on interactive poster and demonstration sessions, pages 177\u2013", "year": 2007}, {"title": "Local phrase reordering models for statistical machine translation", "authors": ["Shankar Kumar", "William Byrne."], "venue": "Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 161\u2013168. Association for", "year": 2005}, {"title": "The Traveling Salesman Problem", "authors": ["Eugene Leighton Lawler", "Jan Karel Lenstra", "Alexander Hendrik George Rinnooy Kan", "David Bernard Shmoys."], "venue": "John Wiley & Sons Ltd.", "year": 1985}, {"title": "Translation as weighted deduction", "authors": ["Adam Lopez."], "venue": "Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, pages 532\u2013540. Association for Computational Linguistics.", "year": 2009}, {"title": "Bandwidth constrained NP-complete problems", "authors": ["Burkhard Monien", "Ivan Hal Sudborough."], "venue": "Proceedings of the thirteenth annual ACM symposium on Theory of computing, pages 207\u2013217. ACM.", "year": 1981}, {"title": "The alignment template approach to statistical machine translation", "authors": ["Franz Josef Och", "Hermann Ney."], "venue": "Computational linguistics, 30(4):417\u2013449.", "year": 2004}, {"title": "Orderpicking in a rectangular warehouse: a solvable case of the traveling salesman problem", "authors": ["H Donald Ratliff", "Arnon S Rosenthal."], "venue": "Operations Research, 31(3):507\u2013521.", "year": 1983}, {"title": "A unigram orientation model for statistical machine translation", "authors": ["Christoph Tillmann."], "venue": "Proceedings of HLT-NAACL 2004: Short Papers, pages 101\u2013104. Association for Computational Linguistics.", "year": 2004}, {"title": "Stochastic inversion transduction grammars and bilingual parsing of parallel corpora", "authors": ["Dekai Wu."], "venue": "Computational linguistics, 23(3):377\u2013403.", "year": 1997}, {"title": "Phrase-based statistical machine translation as a traveling salesman problem", "authors": ["Mikhail Zaslavskiy", "Marc Dymetman", "Nicola Cancedda."], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on", "year": 2009}, {"title": "A comparative study on reordering constraints in statistical machine translation", "authors": ["Richard Zens", "Hermann Ney."], "venue": "Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 144\u2013151. Association for Computational Lin-", "year": 2003}, {"title": "Reordering constraints for phrase-based statistical machine translation", "authors": ["Richard Zens", "Hermann Ney", "Taro Watanabe", "Eiichiro Sumita."], "venue": "Proceedings of the 20th international conference on Computational Linguistics, page 205. Association for", "year": 2004}], "id": "SP:cfd09dd798f434cfaa8bc9d94ecb9837bd7de155", "authors": [{"name": "Yin-Wen Chang", "affiliations": []}, {"name": "Michael Collins", "affiliations": []}], "abstractText": "Decoding of phrase-based translation models in the general case is known to be NPcomplete, by a reduction from the traveling salesman problem (Knight, 1999). In practice, phrase-based systems often impose a hard distortion limit that limits the movement of phrases during translation. However, the impact on complexity after imposing such a constraint is not well studied. In this paper, we describe a dynamic programming algorithm for phrase-based decoding with a fixed distortion limit. The runtime of the algorithm is O(nd!lh) where n is the sentence length, d is the distortion limit, l is a bound on the number of phrases starting at any position in the sentence, and h is related to the maximum number of target language translations for any source word. The algorithm makes use of a novel representation that gives a new perspective on decoding of phrase-based models.", "title": "A Polynomial-Time Dynamic Programming Algorithm for Phrase-Based Decoding with a Fixed Distortion Limit"}