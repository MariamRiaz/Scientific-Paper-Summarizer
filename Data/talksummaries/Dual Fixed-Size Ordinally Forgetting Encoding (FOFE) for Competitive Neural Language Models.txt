9	61	In this paper, we propose a simple alteration to FOFE method, which allows to incorporate two forgetting factors into the fixed-size encoding of the variable-length word sequences.
10	20	We name this approach as dual-FOFE.
22	16	Obviously, FOFE can convert any variable-length sequence into a fixed-size code with length equal to the size of vocabulary.
23	64	In regard to uniqueness of FOFE code, the code is said to be (almost) unique under the two theorems (proven in Zhang (2015a)): Theorem 1 If 0 < α ≤ 0.5, then FOFE code is guarantee uniqueness for any values of vocabulary’s size and sequence’s length.
27	5	The idea of FOFE based NN-LMs is to use FOFE to encode the partial history sequence of past words in a sentence, then feed this fixed-size FOFE code to a feedforward neural network as an input to predict next word.
28	12	As shown in Figure 1, the FOFE code could be efficiently computed via time-delayed recursive structure, where the symbol z−1 in the figure represents a unit time delay (or equivalently a memory unit) from zt to zt−1.
32	31	In a 3rd-order FOFE model, all zt, zt−1 and zt−2 are used as inputs to neural networks.
33	108	More recently, the FOFE methods have been successfully applied to many NLP tasks, including word embedding (Sanu et al., 2017), named entity recognition (Xu et al., 2017a), entity discovery and linking (Xu et al., 2016, 2017b).
34	45	The main idea of dual-FOFE is to generate augmented FOFE encoding codes by concatenating two FOFE codes using two different forgetting factors.
35	10	Each of these FOFE codes is still computed in the same way as the mathematical formulation shown in Equation (1).
37	7	As mentioned in the subsection 2.1, the values in a FOFE code are used to encode both the content and the order information in a sequence.
38	4	This is achieved by a recursive encoding method where at each recursive step the code will be multiplied by the forgetting factor (α) whose value is bounded by 0 < α < 1.
41	5	The reason is that that when α is small, the FOFE code (zt) for each word vastly differs from its neighbour in magnitude.
45	37	This is because when α is small, the contribution of a word from the older history may quickly underflow to become irrelevant (i.e. forgotten) when computing the current word.
46	11	In the original FOFE with just a single forgetting factor, we would have to determine the best trade-off between these two benefits.
51	4	However, faraway histories will be gradually forgotten by the recursive calculation in FOFE due to 0 < α < 1 and finite precision in computers.
63	41	As shown in Table 1, all three dual-FOFE FNNLMs, using three pairs of forgetting factors as (0.5, 0.7) and (0.7, 0.9) and (0.5, 0.9), can significantly outperform other traditional models previously reported on this corpus.
64	3	We also note that it is beneficial to include a relatively large forgetting factor, such as 0.9, in the dual FOFE models since such a large alpha may help to memorize much longer context in the inputs.
66	11	It is worth noting that our dual-FOFE models can be extended to incorporate more than two alpha values.
67	6	In fact after we have obtained a strong result supporting our dual-FOFE hypothesis, we have performed additional experiments using three alpha values, the so-called tri-FOFE model.
68	6	The result on Table 1 has shown that the tri-FOFE FNN-LMs still slightly outperforms the dual-FOFE models.
70	18	This leads us to believing that further extension of more alpha values in FOFE would be of limited use.
78	22	When compared with the original FOFE counterpart, the dual-FOFE FNN-LM is able to provide approximately 11% relative improvement in PPL.
79	4	In this paper, we have proposed a new approach of utilizing the fixed-size ordinally-forgetting encoding (FOFE) method for neural network lan- guage models (NN-LMs), known as dual-FOFE.
80	56	As the name implies, this approach involves to produce a new fixed-sized representation for any variable-length sequence from a concatenation of two FOFE codes.
81	110	This would have allowed us to select two values for the forgetting factors.
82	45	One FOFE code with a smaller forgetting factor is responsible for representing the positional information of all words in the sequence while the other using a larger forgetting factor is responsible for modelling the even longer term dependency in far away history.
83	19	Our experiments on both enwiki9 and Google Billion Words (GBW) tasks have both demonstrated the effectiveness of the dual-FOFE modeling approach.
84	14	Experimental results on the challenging GBW corpus have shown that the dual-FOFE FNN-LM has achieved over 10% improvement in perplexity over the original FOFE FNN-LM, without any significant drawback in model and learning complexity.
85	26	When compared with other traditional neural language models, the dual-FOFE FNN-LM has achieved com- petitive performance with significantly lower computational complexity.
