0	86	While parsing has become a relatively mature technology for written text, parser performance on conversational speech lags behind.
1	129	Speech poses challenges for parsing: transcripts may contain errors and lack punctuation; even perfect transcripts can be difficult to handle because of disfluencies (restarts, repetitions, and self-corrections), filled pauses (“um”, “uh”), interjections (“like”), parentheticals (“you know”, “I mean”), and sentence fragments.
4	21	Despite these challenges, speech carries helpful extra information – beyond the words – associated with the prosodic structure of an utterance and encoded via variation in timing and intonation.
5	29	Speakers pause in locations that are correlated with syntactic structure (Grosjean et al., 1979), and listeners use prosodic structure in resolving syntactic ambiguities (Price et al., 1991).
6	12	Prosodic cues also signal disfluencies by marking the interruption point (Shriberg, 1994).
8	34	Our study focuses on this last challenge, aiming to incorporate prosodic cues in a neural parser, handling disfluencies as constituents via a neural attention mechanism.
9	41	A challenge of incorporating prosody in parsing is that multiple acoustic cues interact to signal prosodic structure, including pauses, lengthening, fundamental frequency modulation, and spectral shape.
11	19	The most successful constituent parsers have mapped these features to prosodic boundary posteriors by using labeled training data (Kahn et al., 2005; Hale et al., 2006; Dreyer and Shafran, 2007).
17	21	Our model maps a sequence of word-level input features to a linearized parse output sequence.
23	14	The encoder is a deep long short-term memory recurrent neural network (LSTM-RNN) (Hochreiter and Schmidhuber, 1997) that reads in a word-level inputs,1 represented as a sequence of vectors x = (x1, · · · ,xTs), and outputs high-level features h = (h1, · · · ,hTs) where hi = LSTM(xi,hi−1).2 The parse decoder is also a deep LSTM-RNN that predicts the linearized parse sequence y = (y1, · · · , yTo) as follows: P (y|x) = To∏ t=1 P (yt|h,y<t) In attention-based models, the posterior distribution of the output yt at time step t is given by: P (yt|h,y<t) = softmax(W s[ct;dt] + bs), where vector bs and matrix W s are learnable parameters; ct is referred to as a context vector that summarizes the encoder’s output h; and dt is the decoder hidden state at time step t, which captures the previous output sequence context y<t.
24	44	uit = v > tanh(W 1hi +W 2dt + ba) αt = softmax(ut) ct = Ts∑ i=1 αtihi where vectors v, ba and matrices W 1, W 2 are learnable parameters; ut and αt are the attention score and attention weight vector, respectively, for decoder time step t. The above attention mechanism is only contentbased, i.e., it is only dependent on hi, dt.
25	16	It is not location-aware, i.e., it does not consider the “location” of the previous attention vector.
55	25	Three energy features are extracted from the Kaldi 40-mel-frequency filter bank features: Etotal, the log of total energy normalized by dividing by the speaker side’s max total energy; Elow, the log of total energy in the lower 20 mel-frequency bands, normalized by total energy, and Ehigh, the log of total energy in the higher 20 mel-frequency bands, normalized by total energy.
59	16	The motivation for the multiple filter sizes is to enable the computation of features that capture information on different time scales.
76	13	We report disfluency detection scores primarily as a diagnostic.
94	18	We first show our results on the model using only text (i.e. xi = ei) to establish a strong baseline, on top of which we can add acousticprosodic features.
95	11	We experiment with the contentonly attention model used by Vinyals et al. (2015) and the content+location attention of Chorowski et al. (2015).
103	12	Since our best model is the content+location attention model, we will henceforth refer to it as the “CL-attn” text-only model.
109	27	The text + p + δ + f0/E-CNN model that uses all three types of features has the best performance with a gain of 0.7% over the already-strong text-only baseline.
131	18	We use the Berkeley Parser Analyzer (Kummerfeld et al., 2012) to compare the types of errors made by the different parsers.10 Table 7 presents the relative error reductions over the text-only baseline achieved by the text + p model and our best model for disfluent sentences.
135	20	Figure 3 demonstrates one case where the pause feature helps in correcting a PP attachment error made by a text-only parser.
142	16	The transcript errors mean that the acoustic signal is inconsistent with the “gold” parse tree.
143	30	Below are some examples of “fluent” sentences (according to the Treebank transcripts) with transcription errors, for which prosodic features “hurt” parsing.
144	24	Words that transcribers missed are in brackets and those inserted are underlined.
178	15	The acousticprosodic features provide the largest gains when sentences are disfluent or long, and analysis of error types shows that these features are especially helpful in repairing attachment errors.
183	13	However, it remains an open question as to whether dependency, constituency or other parsing frameworks are better suited to leveraging prosody.
185	43	However, the prosody modeling component relies only on a 1 second lookahead of the current word (for pause binning), so it could be easily incorporated in an incremental parser.
191	21	Most of the data preprocessing code is available at https://github.
193	67	Part of our data preprocessing pipeline also uses https: //github.com/syllog1sm/swbd_tools.
197	13	That is, all of our models are trained on Switchboard conversations sw2000 to sw3000 as well as sw4154 to sw4500.
