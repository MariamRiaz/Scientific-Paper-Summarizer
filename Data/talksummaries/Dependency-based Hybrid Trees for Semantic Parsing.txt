2	20	Semantic parsing aims to transform the natural language sentences into machine interpretable meaning representations automatically.
4	17	Various systems (Zelle and Mooney, 1996; Kate et al., 2005; Zettlemoyer and Collins, 2005; Liang et al., 2011) were proposed over the years to deal with different types of semantic representations.
5	35	Such models include structure-based models (Wong and Mooney, 2006; Lu et al., 2008; Kwiatkowski et al., 2010; Jones et al., 2012) and neural network based models (Dong and Lapata, 2016; Cheng et al., 2017).
6	94	Following various previous research efforts (Wong and Mooney, 2006; Lu et al., 2008; Jones et al., 2012), in this work, we adopt a popular class of semantic formalism – logical forms that can be equivalently represented as tree structures.
7	53	The tree representation of an example MR is shown in the middle of Figure 1.
12	12	Recently, Reddy et al. (2016, 2017) proposed a model to construct logical representations from sentences that are parsed into dependency structures.
49	21	To jointly encode the tree-structured semantics m and a natural language sentence n, we in- troduce our novel dependency-based hybrid tree.
63	10	In a relaxed hybrid tree representation, words and semantic units jointly form a constituency tree-like structure, where the former are leaves and the latter are internal nodes of such a joint representation.
75	31	For example, the semantic unit m4 on the dependency arc from “not” to “through” in our representation can be used to capture their interactions.
81	11	To define the set of allowable dependency-based hybrid tree representation so as to allow us to perform exact inference later, we introduce the dependency patterns as shown in Table 1.
85	39	For the first case, the semantic unit m3 has arity 0, the pattern involved is WW, indicating both the lefthand and right-hand sides of “rivers” (under the dependency arc with semantic unit m3) are just word spans (W, whose length could be zero).
88	31	Based on the dependency patterns, we are able to define the set of all possible allowable dependency-based hybrid tree representations.
89	15	Each representation essentially belongs to a class of projective dependency trees where semantic units appear on the dependency arcs and (some of the) words are selected as nodes.
92	12	We use t to denote a dependency-based hybrid tree (as shown in Figure 2), which jointly encodes both natural language words and the gold meaning representation.
102	22	The algorithms are inspired by the inside-outside style algorithm (Baker, 1979), graph-based dependency parsing (Eisner, 2000; Koo and Collins, 2010; Shi et al., 2017), and the relaxed hybrid tree model (Lu, 2014, 2015).
104	13	The objective function in Equation 2 can be further decomposed into the following form8: L(w) = − ∑ (n,m)∈D log ∑ t∈T (n,m) ew·f(n,m,t) + ∑ (n,m)∈D log ∑ m′,t′∈T (n,m′) ew·f(n,m ′,t′) We can see the first term is essentially the combined score of all the possible latent structures containing the pair (n,m).
105	21	The second term is the combined score for all the possible latent structures containing n. We show how such scores can be calculated in a factorized manner, based on the fact that we can recursively decompose a dependency-based hybrid tree based on the dependency patterns we introduced.
110	44	We use Ci,j,p,m to denote a complete span, where i and j represent the indices of the headword and endpoint, p is the dependency pattern and m is the semantic unit.
113	12	Figure 4a shows that a complete span is constructed from a complete arc span following the dependency patterns in Table 1.
130	53	Our feature design is inspired by the hybrid tree model (Lu, 2015) and graph-based dependency parsing (McDonald et al., 2005).
141	23	The two words are first mapped to word embeddings ep and ec (both of dimension d).
165	17	Our model DEPHT achieves competitive performance and outperforms the previous best system RHT on 6 languages.
168	29	We found 40 instances that are incorrectly predicted by RHT are correctly predicted by DEPHT.
170	45	Figure 5 shows an example of such errors where the relaxed hybrid tree fails to capture the correct alignment.
186	57	From the table, we can see the neural component is effective, which consistently gives better results than DEPHT and the approach that uses word embedding features only.
191	20	In this work, we present a novel dependencybased hybrid tree model for semantic parsing.
192	73	The model captures the underlying semantic information of a sentence as latent dependencies between the natural language words.
193	42	We develop an efficient algorithm for exact inference based on dynamic-programming.
194	21	Extensive experiments on benchmark dataset across 8 different languages demonstrate the effectiveness of our newly proposed representation for semantic parsing.
195	57	Future work includes exploring alternative approaches such as transition-based methods (Nivre et al., 2006; Chen and Manning, 2014) for semantic parsing with latent dependencies, applying our dependency-based hybrid trees on other types of logical representations (e.g., lambda calculus expressions and SQL (Finegan-Dollak et al., 2018)) as well as multilingual semantic parsing (Jie and Lu, 2014; Susanto and Lu, 2017a).
