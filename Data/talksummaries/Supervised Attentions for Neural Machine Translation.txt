1	10	The attention model plays a crucial role in NMT, as it shows which source word(s) the model should focus on in order to predict the next target word.
2	104	However, the attention or alignment quality of NMT is still very low (Mi et al., 2016a; Tu et al., 2016).
3	51	In this paper, we alleviate the above issue by utilizing the alignments (human annotated data or machine alignments) of the training set.
8	7	the encoder employs a bi-directional recurrent neural network to encode the source sentence x = (x1, ..., xl), where l is the sentence length (including the end-of-sentence 〈eos〉), into a sequence of hidden states h = (h1, ..., hl), each hi is a concatenation of a left-to-right −→ hi and a right-to-left ←− hi .
10	42	At each time t, the probability of each word yt from a target vocabulary Vy is: p(yt|h, y∗t−1..y∗1) = g(st, y∗t−1), (1) where g is a two layer feed-forward neural network over the embedding of the previous word y∗t−1, and the hidden state st.
12	16	The training objective is to maximize the conditional log-probability of the correct translation y∗ 2283 given x with respect to the parameters θ θ∗ = argmax θ N∑ n=1 m∑ t=1 log p(y∗nt |xn, y∗nt−1..y∗n1 ), (5) where n is the n-th sentence pair (xn,y∗n) in the training set, N is the total number of pairs.
14	26	However, the accuracy is still far behind the traditional MaxEnt alignment model in terms of alignment F1 score (Mi et al., 2016b; Tu et al., 2016).
15	12	Thus, in this section, we explicitly add an alignment distance to the objective function in Eq.
17	86	Given an alignment matrix A for a sentence pair (x,y) in Figure 2 (a), where we have an end-ofsource-sentence token 〈eos〉 = xl, and we align all the unaligned target words (y∗3 in this example) to 〈eos〉, also we force y∗m (end-of-target-sentence) to be aligned to xl with probability one.
19	12	The first transformation simply normalizes each row.
26	39	In our experiments, we use a shape distribution, where σ = 0.5.
28	9	(6) NMT Objective: We plug Eq.
29	64	(7) There are two parts: translation and alignment, so we can optimize them jointly, or separately (e.g. we first optimize alignment only, then optimize translation).
36	17	We run our experiments on Chinese to English task.
41	15	Our development set is the concatenation of several tuning sets (GALE Dev, P1R6 Dev, and Dev 12) initially released under the DARPA GALE program.
47	8	For each source sentence, the sentencelevel target vocabularies are union of top 2k most frequent target words and the top 10 candidates of the word-to-word/phrase translation tables learned from ‘fast align’ (Dyer et al., 2013).
53	11	Our SMT system is a hybrid syntax-based tree-tostring model (Zhao and Al-onaizan, 2008), a simplified version of the joint decoding (Liu et al., 2009; Cmejrek et al., 2013).
58	7	We tune our system with PRO (Hopkins and May, 2011) to minimize (TER- BLEU)/2 1 on the development set.
60	37	The syntax-based statistical machine translation model achieves an average (TER-BLEU)/2 of 13.36 on three test sets.
61	12	LVNMT system achieves an average (TER-BLEU)/2 of 14.24, which is about 0.9 points worse than Tree-to-string SMT system.
64	14	We test three different alignments: • Zh→ En (one direction of GIZA++), • GDFA (the “grow-diag-final-and” heuristic merge of both directions of GIZA++), • MaxEnt (trained on 67k hand-aligned sentences).
69	7	Only the last row uses the smoothed transformation, all others use the simple transformation.
80	11	Our alignment objective adjusts the translation length to be more in line with the human references accordingly.
84	12	For each target word, we sort the alphas and add the max probability link if it is higher than 0.2.
91	13	in the last row, which significantly improves the F1 by 5 points over the baseline Cov.
93	12	smoothing gives us about 1.7 points gain over J system.
95	8	Together with the results in Table 1, we conclude that adding the alignment cost to the training objective helps both translation and alignment significantly.
96	49	In this paper, we utilize the “supervised” alignments, and put the alignment cost to the NMT objective function.
97	90	In this way, we directly optimize the attention model in a supervised way.
98	99	Experiments show significant improvements in both translation and alignment tasks over a very strong LVNMT system.
