0	74	Open book exams are a common mechanism for assessing human understanding of a subject, where test takers are allowed free access to a relevant book, study guide, or class notes when answering questions.
1	41	In this context, the goal is not to evaluate memorization but a deeper understanding of the material and its application to new situations (Jenkins, 1995; Landsberger, 1996).
2	13	The application, in turn, often requires combining a fact in the book (e.g., metals conduct electricity) with additional common knowledge the test taker is ex- pected to have acquired by this stage (e.g., a suit of armor is made of metal).
3	21	Motivated by this setting, we present a new kind of question answering dataset, OpenBookQA,1 that consists of two parts: Q, a set of 5957 multiple-choice questions, andF , a set of 1326 diverse facts about elementary level science.
4	50	F has three key characteristics of an ‘open book’: (a) it forms the basis for generating Q; (b) it has been deemed central to scientific explanations (Jansen et al., 2018); and (c) by itself, F is generally insufficient to answer questions in Q.
5	140	Faced with a question q ∈ Q, a student or system S is expected retrieve a relevant fact f ∈ F , and appeal to their own common knowledge, KS , when applying f to answer q.
6	15	Here, metals are thermal conductors is a core scientific fact available in F .
7	116	One way to apply this fact to decide whether a steel spoon would let the most heat travel through is to appeal to common knowledge that steel is metallic and heat travels through thermal conductors.
10	62	OpenBookQA questions are challenging as they require multi-hop reasoning with partial context provided by F .
11	28	Specifically, unlike existing datasets for reading comprehension (RC), answering questions on the back of a textbook (TQA),2 as well as question answering over structured knowledge-bases (KBQA), the open book F that comes with OpenBookQA is not self-contained.
12	22	A successful system must therefore go beyond the typical challenges such as paraphrase matching and coreference resolution, without benefiting from the canonicalized and complete information in KBQA.
13	38	Generating interesting open book questions is a difficult task.
15	27	We evaluate a number of existing QA systems for science (without retraining) on OpenBookQA, finding that they perform surprisingly close to the random guessing baseline of 25%.
16	78	Human performance, on the other hand, is close to 92%.3 Motivated by recent findings of gameability of NLP datasets (Gururangan et al., 2018), we also develop and evaluate simple, attention-based, neural baselines including a plausible answer detector (which ignores the question text completely) and an odd-one-out solver.
17	50	These highlight inevitable human bias in any crowdsourced dataset, increasing performance on OpenBookQA to 48%.
18	52	Building upon a recent neural model for incorporating external knowledge in the story cloze setting (Mihaylov and Frank, 2018), we propose a knowledge-aware neural baseline that can utilize both the open book F and common knowledge retrieved from sources such as ConceptNet (Speer et al., 2017).
19	49	While retrieving the most useful pieces of knowledge remains an open challenge, our ‘oracle’ experiments with the fact f used while generating a question q and an interpretation (by the question author) of the additional knowledge k needed for q, provides valuable insight into the nature of this dataset: Facts from the open book F are valuable (5% improvement) but not sufficient.
20	35	Using both f and k increases the accuracy to 76%, but is still far from human level performance, suggesting the need for non-trivial reasoning to combine these facts.
45	54	The OpenBookQA dataset consists of about 6,000 4-way multiple-choice questions, each associated with one core fact from a “book” F of 1326 such facts, and an auxiliary set K of about 6000 additional facts.
51	22	OpenBookQA additionally requires broad common knowledge, which is expected to come from large corpora, such as ConceptNet, Wikipedia, or a corpus with 14M science-related sentences used by some existing baselines.
57	27	Given the “book” F of core facts, the process proceeds as follows, starting with an empty question set Qs and an empty ‘second facts’ set K: 1.
59	14	2. w is asked to think of a second common fact, k, that may be combined with f to derive a new, valid assertion s. 3. w then converts s into a question-answer pair and extends this into a 4-way multiple choice question by adding 3 incorrect answer choices, qmc = (q, {c1, c2, c3, c4}), where one of the ci’s is the unique correct answer.
60	54	The system verifies qmc passes basic checks such as uniformity of answer choices.6 5. w then feeds the multiple-choice question qmc to an information retrieval solver (Clark et al., 5 We used Amazon Mechnical Turk, with workers from North America and with a ‘masters’ level qualification.
63	26	If at least 4 out of 5 workers answer qmc correctly, it is deemed answerable and the process continues.
78	33	In our Dev and Test sets, where |Q| = 500 and |I| = 5, this translates into H(Q) being at least H̃(Q) − 3% with probability over 98.8% and at least H̃(Q) − 2.5% with prob 95.6%; we report the former as our conservative estimate on human performance.
82	39	We analyzed 100 questions in the Train set to capture the kind of common knowledge and reasoning needed.
88	36	ISA: Basic taxonomic facts such as isa(tree, 9Overall, 8140 questions were collected, of which 2183 were discarded in crowdsourcing Step 7. living thing), isa(granite, rock).
89	39	PROPERTY: Properties of objects such as madeof(belt buckle, metal), has(mammals, four legs), contains(lemon juice, citric acid).
91	14	CAUSAL: Causal facts such as causes(adding lemon juice to milk, milk to break down).
92	27	BASIC: General scientific fact that did not fit above, e.g. squirrels eat nuts for food.
