0,1,label2,summary_sentences
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3622–3631 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
3622",text,[0],[0]
"Despite the massive success brought by neural machine translation (NMT, Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017), it has been noticed that the vanilla NMT often lags behind conventional machine translation systems, such as statistical phrase-based translation systems (PBMT, Koehn et al., 2003), for low-resource language pairs (see, e.g., Koehn and Knowles, 2017).",1 Introduction,[0],[0]
"In the past few years, various approaches have been proposed to address this issue.",1 Introduction,[0],[0]
"The first attempts at tackling this problem exploited the availability of monolingual corpora (Gulcehre
* Equal contribution.
",1 Introduction,[0],[0]
"et al., 2015; Sennrich et al., 2015; Zhang and Zong, 2016).",1 Introduction,[0],[0]
"It was later followed by approaches based on multilingual translation, in which the goal was to exploit knowledge from high-resource language pairs by training a single NMT system on a mix of high-resource and low-resource language pairs (Firat et al., 2016a,b; Lee et al., 2016; Johnson et al., 2016; Ha et al., 2016b).",1 Introduction,[0],[0]
"Its variant, transfer learning, was also proposed by Zoph et al. (2016), in which an NMT system is pretrained on a high-resource language pair before being finetuned on a target low-resource language pair.
",1 Introduction,[0],[0]
"In this paper, we follow up on these latest approaches based on multilingual NMT and propose a meta-learning algorithm for low-resource neural machine translation.",1 Introduction,[0],[0]
"We start by arguing that the recently proposed model-agnostic meta-learning algorithm (MAML, Finn et al., 2017) could be applied to low-resource machine translation by viewing language pairs as separate tasks.",1 Introduction,[0],[0]
This view enables us to use MAML to find the initialization of model parameters that facilitate fast adaptation for a new language pair with a minimal amount of training examples (§3).,1 Introduction,[0],[0]
"Furthermore, the vanilla MAML however cannot handle tasks with mismatched input and output.",1 Introduction,[0],[0]
"We overcome this limitation by incorporating the universal lexical representation (Gu et al., 2018b) and adapting it for the meta-learning scenario (§3.3).
",1 Introduction,[0],[0]
We extensively evaluate the effectiveness and generalizing ability of the proposed meta-learning algorithm on low-resource neural machine translation.,1 Introduction,[0],[0]
"We utilize 17 languages from Europarl and Russian from WMT as the source tasks and test the meta-learned parameter initialization against five target languages (Ro, Lv, Fi, Tr and Ko), in all cases translating to English.",1 Introduction,[0],[0]
"Our experiments using only up to 160k tokens in each of the target task reveal that the proposed meta-learning approach outperforms the multilingual translation
approach across all the target language pairs, and the gap grows as the number of training examples decreases.",1 Introduction,[0],[0]
Neural Machine Translation (NMT),2 Background,[0],[0]
"Given a source sentence X = {x1, ..., xT 0}, a neural machine translation model factors the distribution over possible output sentences Y = {y1, ..., yT } into a chain of conditional probabilities with a leftto-right causal structure:
p(Y |X; ✓) = T+1Y
t=1
p(yt|y0:t 1, x1:T 0 ; ✓), (1)
where special tokens y0 (hbosi) and yT+1 (heosi) are used to represent the beginning and the end of a target sentence.",2 Background,[0],[0]
These conditional probabilities are parameterized using a neural network.,2 Background,[0],[0]
"Typically, an encoder-decoder architecture (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015) with a RNN-based decoder is used.",2 Background,[0],[0]
"More recently, architectures without any recurrent structures (Gehring et al., 2017; Vaswani et al., 2017) have been proposed and shown to speed up training while achieving state-of-the-art performance.
",2 Background,[0],[0]
"Low Resource Translation NMT is known to easily over-fit and result in an inferior performance when the training data is limited (Koehn and Knowles, 2017).",2 Background,[0],[0]
"In general, there are two ways for handling the problem of low resource translation: (1) utilizing the resource of unlabeled monolingual data, and (2) sharing the knowledge between low- and high-resource language pairs.",2 Background,[0],[0]
"Many research efforts have been spent on incorporating the monolingual corpora into machine translation, such as multi-task learning (Gulcehre et al., 2015; Zhang and Zong, 2016), back-translation (Sennrich et al., 2015), dual learning (He et al., 2016) and unsupervised machine translation with monolingual corpora only for both sides (Artetxe et al., 2017b; Lample et al., 2017; Yang et al., 2018).
",2 Background,[0],[0]
"For the second approach, prior researches have worked on methods to exploit the knowledge of auxiliary translations, or even auxiliary tasks.",2 Background,[0.9608077859422562],"['Though this belief occurred in about a third of responses, the remaining responses were all proactive ways for the robot to get the conversation back on track (i.e., propose alternative, ask for more, and ask for help).']"
"For instance, Cheng et al. (2016); Chen et al. (2017); Lee et al. (2017); Chen et al. (2018) investigate the use of a pivot to build a translation path between two languages even without any directed resource.",2 Background,[0],[0]
The pivot can be a third language or even an image in multimodal domains.,2 Background,[0],[0]
"When pivots are
not easy to obtain, Firat et al. (2016a); Lee et al. (2016); Johnson et al. (2016) have shown that the structure of NMT is suitable for multilingual machine translation.",2 Background,[0],[0]
"Gu et al. (2018b) also showed that such a multilingual NMT system could improve the performance of low resource translation by using a universal lexical representation to share embedding information across languages.
",2 Background,[0],[0]
"All the previous work for multilingual NMT assume the joint training of multiple high-resource languages naturally results in a universal space (for both the input representation and the model) which, however, is not necessarily true, especially for very low resource cases.
",2 Background,[0],[0]
"Meta Learning In the machine learning community, meta-learning, or learning-to-learn, has recently received interests.",2 Background,[0],[0]
Meta-learning tries to solve the problem of “fast adaptation on new training data.”,2 Background,[0],[0]
"One of the most successful applications of meta-learning has been on few-shot (or oneshot) learning (Lake et al., 2015), where a neural network is trained to readily learn to classify inputs based on only one or a few training examples.",2 Background,[0],[0]
"There are two categories of meta-learning:
1.",2 Background,[0],[0]
"learning a meta-policy for updating model parameters (see, e.g., Andrychowicz et al., 2016; Ha et al., 2016a; Mishra et al., 2017)
2.",2 Background,[0],[0]
"learning a good parameter initialization for fast adaptation (see, e.g., Finn et al., 2017; Vinyals et al., 2016; Snell et al., 2017).
",2 Background,[0],[0]
"In this paper, we propose to use a meta-learning algorithm for low-resource neural machine translation based on the second category.",2 Background,[0],[0]
"More specifically, we extend the idea of model-agnostic metalearning (MAML, Finn et al., 2017) in the multilingual scenario.",2 Background,[0],[0]
"The underlying idea of MAML is to use a set of source tasks T 1, . . .",3 Meta Learning for Low-Resource Neural Machine Translation,[0],[0]
", T K to find the initialization of parameters ✓0 from which learning a target task T 0 would require only a small number of training examples.",3 Meta Learning for Low-Resource Neural Machine Translation,[0],[0]
"In the context of machine translation, this amounts to using many high-resource language pairs to find good initial parameters and training a new translation model on a low-resource language starting from the found initial parame-
ters.",3 Meta Learning for Low-Resource Neural Machine Translation,[0],[0]
"This process can be understood as
✓⇤ = Learn(T 0;MetaLearn(T 1, . . .",3 Meta Learning for Low-Resource Neural Machine Translation,[0],[0]
", T K)).
",3 Meta Learning for Low-Resource Neural Machine Translation,[0],[0]
"That is, we meta-learn the initialization from auxiliary tasks and continue to learn the target task.",3 Meta Learning for Low-Resource Neural Machine Translation,[0],[0]
We refer the proposed meta-learning method for NMT to MetaNMT.,3 Meta Learning for Low-Resource Neural Machine Translation,[0],[0]
See Fig. 1 for the overall illustration.,3 Meta Learning for Low-Resource Neural Machine Translation,[0],[0]
"Given any initial parameters ✓0 (which can be either random or meta-learned),
the prior distribution of the parameters of a desired NMT model can be defined as an isotropic Guassian:
✓i ⇠ N (✓0i , 1/ ),
where 1/ is a variance.",3.1 Learn: language-specific learning,[0],[0]
"With this prior distribution, we formulate the language-specific learning process Learn(DT ; ✓0) as maximizing the logposterior of the model parameters given data DT :
Learn(DT ; ✓0) = argmax ✓ LDT (✓)
= argmax
✓
X
(X,Y )2DT
log p(Y |X, ✓) k✓ ✓0k2,
where we assume p(X|✓) to be uniform.",3.1 Learn: language-specific learning,[0],[0]
The first term above corresponds to the maximum likelihood criterion often used for training a usual NMT system.,3.1 Learn: language-specific learning,[0],[0]
"The second term discourages the newly learned model from deviating too much from the initial parameters, alleviating the issue of overfitting when there is not enough training data.",3.1 Learn: language-specific learning,[0],[0]
"In practice, we solve the problem above by maximizing the first term with gradient-based optimization and early-stopping after only a few update steps.
",3.1 Learn: language-specific learning,[0],[0]
"Thus, in the low-resource scenario, finding a good initialization ✓0 strongly correlates the final performance of the resulting model.",3.1 Learn: language-specific learning,[0],[0]
"We find the initialization ✓0 by repeatedly simulating low-resource translation scenarios using auxiliary, high-resource language pairs.",3.2 MetaLearn,[0],[0]
"Following Finn et al. (2017), we achieve this goal by defining the meta-objective function as
L(✓) =EkEDT k ,D0T k (2)2
64 X
(X,Y )2D0 T k
log p(Y |X;Learn(DT k ; ✓))
3
75 ,
where k ⇠ U({1, . . .",3.2 MetaLearn,[0],[0]
",K}) refers to one metalearning episode, and DT , D0T follow the uniform distribution over T ’s data.
",3.2 MetaLearn,[0],[0]
"We maximize the meta-objective function using stochastic approximation (Robbins and Monro, 1951) with gradient descent.",3.2 MetaLearn,[0],[0]
"For each episode, we uniformly sample one source task at random, T k.",3.2 MetaLearn,[0],[0]
"We then sample two subsets of training examples independently from the chosen task, DT k and D0T k .",3.2 MetaLearn,[0],[0]
We use the former to simulate languagespecific learning and the latter to evaluate its outcome.,3.2 MetaLearn,[0],[0]
"Assuming a single gradient step is taken only the with learning rate ⌘, the simulation is:
✓0k = Learn(DT k ;",3.2 MetaLearn,[0],[0]
✓) =,3.2 MetaLearn,[0],[0]
"✓ ⌘r✓LDT k (✓).
",3.2 MetaLearn,[0],[0]
"Once the simulation of learning is done, we evaluate the updated parameters ✓0k on D 0 T k , The gradient computed from this evaluation, which we refer to as meta-gradient, is used to update the
meta model ✓.",3.2 MetaLearn,[0],[0]
"It is possible to aggregate multiple episodes of source tasks before updating ✓:
✓ ✓ ⌘0 X
k
r✓LD 0 T k (✓0k),
where ⌘0 is the meta learning rate.",3.2 MetaLearn,[0],[0]
"Unlike a usual learning scenario, the resulting model ✓0 from this meta-learning procedure is not necessarily a good model on its own.",3.2 MetaLearn,[0],[0]
It is however a good starting point for training a good model using only a few steps of learning.,3.2 MetaLearn,[0],[0]
"In the context of machine translation, this procedure can be understood as finding the initialization of a neural machine translation system that could quickly adapt to a new language pair by simulating such a fast adaptation scenario using many high-resource language pairs.
",3.2 MetaLearn,[0],[0]
"Meta-Gradient We use the following approximation property
H(x)v ⇡ r(x+ ⌫v) r(x) ⌫
to approximate the meta-gradient:1
r✓LD 0",3.2 MetaLearn,[0],[0]
(✓0),3.2 MetaLearn,[0],[0]
= r✓0LD 0,3.2 MetaLearn,[0],[0]
"(✓0)r✓(✓ ⌘r✓LD(✓))
",3.2 MetaLearn,[0],[0]
= r✓0LD 0,3.2 MetaLearn,[0],[0]
(✓0) ⌘,3.2 MetaLearn,[0],[0]
r✓0LD 0,3.2 MetaLearn,[0],[0]
"(✓0)H✓(LD(✓))
⇡ r✓0LD 0",3.2 MetaLearn,[0],[0]
"(✓0) ⌘
⌫
 r✓LD(✓)
",3.2 MetaLearn,[0],[0]
"✓̂ r✓LD(✓) ✓ ,
where ⌫ is a small constant and
ˆ✓ = ✓ + ⌫r",3.2 MetaLearn,[0],[0]
✓0LD 0,3.2 MetaLearn,[0],[0]
"(✓0).
",3.2 MetaLearn,[0],[0]
"In practice, we find that it is also possible to ignore the second-order term, ending up with the following simplified update rule:
r✓LD 0 (✓0) ⇡ r✓0LD 0",3.2 MetaLearn,[0],[0]
(✓0).,3.2 MetaLearn,[0],[0]
"(3)
1We omit the subscript k for simplicity.
",3.2 MetaLearn,[0],[0]
"Related Work: Multilingual Transfer Learning The proposed MetaNMT differs from the existing framework of multilingual translation (Lee et al., 2016; Johnson et al., 2016; Gu et al., 2018b) or transfer learning (Zoph et al., 2016).",3.2 MetaLearn,[0],[0]
"The latter can be thought of as solving the following problem:
max ✓ Lmulti(✓) =",3.2 MetaLearn,[0],[0]
"Ek
2 4 X
(X,Y )2Dk
log p(Y |X; ✓)
3
5 ,
where Dk is the training set of the k-th task, or language pair.",3.2 MetaLearn,[0],[0]
"The target low-resource language pair could either be a part of joint training or be trained separately starting from the solution ✓0 found from solving the above problem.
",3.2 MetaLearn,[0],[0]
"The major difference between the proposed MetaNMT and these multilingual transfer approaches is that the latter do not consider how learning happens with the target, low-resource language pair.",3.2 MetaLearn,[0],[0]
The former explicitly incorporates the learning process within the framework by simulating it repeatedly in Eq.,3.2 MetaLearn,[0],[0]
(2).,3.2 MetaLearn,[0],[0]
"As we will see later in the experiments, this results in a substantial gap in the final performance on the low-resource task.
",3.2 MetaLearn,[0],[0]
"Illustration In Fig. 2, we contrast transfer learning, multilingual learning and meta-learning using three source language pairs (Fr-En, Es-En and Pt-En) and two target pairs (Ro-En and Lv-En).",3.2 MetaLearn,[0],[0]
"Transfer learning trains an NMT system specifically for a source language pair (Es-En) and finetunes the system for each target language pair (RoEn, Lv-En).",3.2 MetaLearn,[0],[0]
"Multilingual learning often trains a single NMT system that can handle many different language pairs (Fr-En, Pt-En, Es-En), which may or may not include the target pairs (Ro-En, LvEn).",3.2 MetaLearn,[0],[0]
"If not, it finetunes the system for each target pair, similarly to transfer learning.",3.2 MetaLearn,[0],[0]
Both of these however aim at directly solving the source tasks.,3.2 MetaLearn,[0],[0]
"On the other hand, meta-learning trains the NMT system to be useful for fine-tuning on various tasks including the source and target tasks.",3.2 MetaLearn,[0],[0]
"This is done by repeatedly simulating the learning process on
low-resource languages using many high-resource language pairs (Fr-En, Pt-En, Es-En).",3.2 MetaLearn,[0],[0]
I/O mismatch across language pairs One major challenge that limits applying meta-learning for low resource machine translation is that the approach outlined above assumes the input and output spaces are shared across all the source and target tasks.,3.3 Unified Lexical Representation,[0],[0]
"This, however, does not apply to machine translation in general due to the vocabulary mismatch across different languages.",3.3 Unified Lexical Representation,[0],[0]
"In multilingual translation, this issue has been tackled by using a vocabulary of sub-words (Sennrich et al., 2015) or characters (Lee et al., 2016) shared across multiple languages.",3.3 Unified Lexical Representation,[0],[0]
"This surface-level sharing is however limited, as it cannot be applied to languages exhibiting distinct orthography (e.g., IndoEuroepan languages vs. Korean.)
",3.3 Unified Lexical Representation,[0],[0]
"Universal Lexical Representation (ULR) We tackle this issue by dynamically building a vocabulary specific to each language using a keyvalue memory network (Miller et al., 2016; Gulcehre et al., 2018), as was done successfully for low-resource machine translation recently by Gu et al. (2018b).",3.3 Unified Lexical Representation,[0],[0]
"We start with multilingual word embedding matrices ✏kquery 2 R|Vk|⇥d pretrained on large monolingual corpora, where Vk is the vocabulary of the k-th language.",3.3 Unified Lexical Representation,[0],[0]
"These embedding vectors can be obtained with small dictionaries of seed word pairs (Artetxe et al., 2017a; Smith et al., 2017) or in a fully unsupervised manner (Zhang et al., 2017; Conneau et al., 2018).",3.3 Unified Lexical Representation,[0],[0]
"We take one of these languages k0 to build universal lexical representation consisting of a universal embedding matrix ✏u 2 RM⇥d and a corresponding key matrix ✏key 2 RM⇥d, where M < |V 0k|.",3.3 Unified Lexical Representation,[0],[0]
Both ✏kquery and ✏key are fixed during meta-learning.,3.3 Unified Lexical Representation,[0],[0]
"We then compute the language-specific embedding of token x from the language k as the convex sum of the universal embedding vectors by
✏0[x] = MX
i=1
↵i✏u[i],
where ↵i / exp 1⌧ ✏key[i]",3.3 Unified Lexical Representation,[0],[0]
>A✏kquery[x] and ⌧ is set to 0.05.,3.3 Unified Lexical Representation,[0],[0]
"This approach allows us to handle languages with different vocabularies using a fixed number of shared parameters (✏u, ✏key and A.)
",3.3 Unified Lexical Representation,[0],[0]
"Learning of ULR It is not desirable to update the universal embedding matrix ✏u when fine-
tuning on a small corpus which contains a limited set of unique tokens in the target language, as it could adversely influence the other tokens’ embedding vectors.",3.3 Unified Lexical Representation,[0],[0]
"We thus estimate the change to each embedding vector induced by languagespecific learning by a separate parameter ✏k[x]:
✏k[x] = ✏0[x] + ✏k[x].
",3.3 Unified Lexical Representation,[0],[0]
"During language-specific learning, the ULR ✏0[x] is held constant, while only ✏k[x] is updated, starting from an all-zero vector.",3.3 Unified Lexical Representation,[0],[0]
"On the other hand, we hold ✏k[x]’s constant while updating ✏u and A during the meta-learning stage.",3.3 Unified Lexical Representation,[0],[0]
"Target Tasks We show the effectiveness of the proposed meta-learning method for low resource NMT with extremely limited training examples on five diverse target languages: Romanian (Ro) from WMT’16,2 Latvian (Lv), Finnish (Fi), Turkish (Tr) from WMT’17,3 and Korean (Ko) from Korean Parallel Dataset.4 We use the officially provided train, dev and test splits for all these languages.",4.1 Dataset,[0],[0]
The statistics of these languages are presented in Table 1.,4.1 Dataset,[0],[0]
"We simulate the low-resource translation scenarios by randomly sub-sampling the training set with different sizes.
",4.1 Dataset,[0],[0]
"Source Tasks We use the following languages from Europarl5: Bulgarian (Bg), Czech (Cs), Danish (Da), German (De), Greek (El), Spanish (Es), Estonian (Et), French (Fr), Hungarian (Hu), Italian (It), Lithuanian (Lt), Dutch (Nl), Polish (Pl), Portuguese (Pt), Slovak (Sk), Slovene (Sl) and
2 http://www.statmt.org/wmt16/translation-task.html 3 http://www.statmt.org/wmt17/translation-task.html 4 https://sites.google.com/site/koreanparalleldata/ 5 http://www.statmt.org/europarl/
Swedish (Sv), in addition to Russian (Ru)6 to learn the intilization for fine-tuning.",4.1 Dataset,[0],[0]
"In our experiments, different combinations of source tasks are explored to see the effects from the source tasks.
",4.1 Dataset,[0],[0]
Validation We pick either Ro-En or Lv-En as a validation set for meta-learning and test the generalization capability on the remaining target tasks.,4.1 Dataset,[0],[0]
"This allows us to study the strict form of metalearning, in which target tasks are unknown during both training and model selection.
",4.1 Dataset,[0],[0]
"Preprocessing and ULR Initialization As described in §3.3, we initialize the query embedding vectors ✏kquery of all the languages.",4.1 Dataset,[0],[0]
"For each language, we use the monolingual corpora built from Wikipedia7 and the parallel corpus.",4.1 Dataset,[0],[0]
"The concatenated corpus is first tokenized and segmented using byte-pair encoding (BPE, Sennrich et al., 2016), resulting in 40, 000 subwords for each language.",4.1 Dataset,[0],[0]
"We then estimate word vectors using fastText (Bojanowski et al., 2016) and align them across all the languages in an unsupervised way
6 A subsample of approximately 2M pairs from WMT’17.",4.1 Dataset,[0],[0]
"7 We use the most recent Wikipedia dump (2018.5) from
https://dumps.wikimedia.org/backup-index.html.
using MUSE (Conneau et al., 2018) to get multilingual word vectors.",4.1 Dataset,[0],[0]
"We use the multilingual word vectors of the 20,000 most frequent words in English to form the universal embedding matrix ✏u.",4.1 Dataset,[0],[0]
"Model We utilize the recently proposed Transformer (Vaswani et al., 2017) as an underlying NMT system.",4.2 Model and Learning,[0],[0]
"We implement Transformer in this paper based on (Gu et al., 2018a)8 and modify it to use the universal lexical representation from §3.3.",4.2 Model and Learning,[0],[0]
"We use the default set of hyperparameters (dmodel = dhidden = 512, nlayer = 6, nhead = 8, nbatch = 4000, twarmup = 16000) for all the language pairs and across all the experimental settings.",4.2 Model and Learning,[0],[0]
"We refer the readers to (Vaswani et al., 2017; Gu et al., 2018a) for the details of the model.",4.2 Model and Learning,[0],[0]
"However, since the proposed metalearning method is model-agnostic, it can be easily extended to any other NMT architectures, e.g. RNN-based sequence-to-sequence models with attention (Bahdanau et al., 2015).
",4.2 Model and Learning,[0],[0]
8,4.2 Model and Learning,[0],[0]
"https://github.com/salesforce/nonauto-nmt
Learning We meta-learn using various sets of source languages to investigate the effect of source task choice.",4.2 Model and Learning,[0],[0]
"For each episode, by default, we use a single gradient step of language-specific learning with Adam (Kingma and Ba, 2014) per computing the meta-gradient, which is computed by the first-order approximation in Eq.",4.2 Model and Learning,[0],[0]
"(3).
",4.2 Model and Learning,[0],[0]
"For each target task, we sample training examples to form a low-resource task.",4.2 Model and Learning,[0],[0]
"We build tasks of 4k, 16k, 40k and 160k English tokens for each language.",4.2 Model and Learning,[0],[0]
We randomly sample the training set five times for each experiment and report the average score and its standard deviation.,4.2 Model and Learning,[0],[0]
"Each fine-tuning is done on a training set, early-stopped on a validation set and evaluated on a test set.",4.2 Model and Learning,[0],[0]
"In default without notation, datasets of 16k tokens are used.
",4.2 Model and Learning,[0],[0]
"Fine-tuning Strategies The transformer consists of three modules; embedding, encoder and decoder.",4.2 Model and Learning,[0],[0]
"We update all three modules during metalearning, but during fine-tuning, we can selectively tune only a subset of these modules.",4.2 Model and Learning,[0],[0]
"Following (Zoph et al., 2016), we consider three fine-tuning
strategies; (1) fine-tuning all the modules (all), (2) fine-tuning the embedding and encoder, but freezing the parameters of the decoder (emb+enc) and (3) fine-tuning the embedding only (emb).",4.2 Model and Learning,[0],[0]
vs. Multilingual Transfer Learning We metalearn the initial models on all the source tasks using either Ro-En or Lv-En as a validation task.,5 Results,[0],[0]
We also train the initial models to be multilingual translation systems.,5 Results,[0],[0]
"We fine-tune them using the four target tasks (Ro-En, Lv-En, Fi-En and Tr-En; 16k tokens each) and compare the proposed meta-learning strategy and the multilingual, transfer learning strategy.",5 Results,[0],[0]
"As presented in Fig. 3, the proposed learning approach significantly outperforms the multilingual, transfer learning strategy across all the target tasks regardless of which target task was used for early stopping.",5 Results,[0],[0]
We also notice that the emb+enc strategy is most effective for both meta-learning and transfer learning approaches.,5 Results,[0],[0]
"With the proposed meta-learning and emb+enc fine-tuning, the final NMT systems trained using only a fraction of all available training examples achieve 2/3 (Ro-En) and 1/2 (Lv-En, Fi-En and Tr-En) of the BLEU score achieved by the models trained with full training sets.
",5 Results,[0],[0]
"vs. Statistical Machine Translation We also test the same Ro-En datasets with 16, 000 target tokens using the default setting of Phrase-based MT (Moses) with the dev set for adjusting the parameters and the test set for calculating the final performance.",5 Results,[0],[0]
"We obtain 4.79(±0.234) BLEU point, which is higher than the standard NMT performance (0 BLEU).",5 Results,[0],[0]
"It is however still lower than both the multi-NMT and meta-NMT.
",5 Results,[0],[0]
"Impact of Validation Tasks Similarly to training any other neural network, meta-learning still requires early-stopping to avoid overfitting to a
specific set of source tasks.",5 Results,[0],[0]
"In doing so, we observe that the choice of a validation task has nonnegligible impact on the final performance.",5 Results,[0],[0]
"For instance, as shown in Fig. 3, Fi-En benefits more when Ro-En is used for validation, while the opposite happens with Tr-En.",5 Results,[0],[0]
"The relationship between the task similarity and the impact of a validation task must be investigated further in the future.
",5 Results,[0],[0]
"Training Set Size We vary the size of the target task’s training set and compare the proposed meta-learning strategy and multilingual, transfer learning strategy.",5 Results,[0],[0]
We use the emb+enc fine-tuning on Ro-En and Fi-En.,5 Results,[0],[0]
Fig. 4 demonstrates that the meta-learning approach is more robust to the drop in the size of the target task’s training set.,5 Results,[0],[0]
"The gap between the meta-learning and transfer learning grows as the size shrinks, confirming the effectiveness of the proposed approach on extremely lowresource language pairs.
",5 Results,[0],[0]
"Impact of Source Tasks In Table 2, we present the results on all five target tasks obtained while varying the source task set.",5 Results,[0],[0]
We first see that it is always beneficial to use more source tasks.,5 Results,[0],[0]
"Although the impact of adding more source tasks varies from one language to another, there is up to 2⇥ improvement going from one source task to 18 source tasks (Lv-En, Fi-En, Tr-En and Ko-En).",5 Results,[0],[0]
"The same trend can be observed even without any fine-tuning (i.e., unsupervised translation, (Lample et al., 2017; Artetxe et al., 2017b)).",5 Results,[0],[0]
"In addition, the choice of source languages has different implications for different target languages.",5 Results,[0],[0]
"For instance, Ro-En benefits more from {Es, Fr, It, Pt} than from {De, Ru}, while the opposite effect is observed with all the other target tasks.
",5 Results,[0],[0]
Training Curves,5 Results,[0],[0]
The benefit of meta-learning over multilingual translation is clearly demonstrated when we look at the training curves in Fig. 5.,5 Results,[0],[0]
"With the multilingual, transfer learning ap-
",5 Results,[0],[0]
"proach, we observe that training rapidly saturates and eventually degrades, as the model overfits to the source tasks.",5 Results,[0],[0]
MetaNMT,5 Results,[0],[0]
"on the other hand continues to improve and never degrades, as the metaobjective ensures that the model is adequate for fine-tuning on target tasks rather than for solving the source tasks.
",5 Results,[0],[0]
Sample Translations We present some sample translations from the tested models in Table 3.,5 Results,[0],[0]
Inspecting these examples provides the insight into the proposed meta-learning algorithm.,5 Results,[0],[0]
"For instance, we observe that the meta-learned model without any fine-tuning produces a word-by-word translation in the first example (Tr-En), which is due to the successful use of the universal lexcial representation and the meta-learned initialization.",5 Results,[0],[0]
"The system however cannot reorder tokens from Turkish to English, as it has not seen any training example of Tr-En.",5 Results,[0],[0]
"After seeing around 600 sentence pairs (16K English tokens), the model rapidly learns to correctly reorder tokens to form a better translation.",5 Results,[0],[0]
A similar phenomenon is observed in the Ko-En example.,5 Results,[0],[0]
These cases could be found across different language pairs.,5 Results,[0],[0]
"In this paper, we proposed a meta-learning algorithm for low-resource neural machine translation that exploits the availability of high-resource languages pairs.",6 Conclusion,[0],[0]
"We based the proposed algorithm on the recently proposed model-agnostic metalearning and adapted it to work with multiple languages that do not share a common vocabulary using the technique of universal lexcal representation, resulting in MetaNMT.",6 Conclusion,[0],[0]
"Our extensive evaluation, using 18 high-resource source tasks and 5 low-resource target tasks, has shown that the proposed MetaNMT significantly outperforms the existing approach of multilingual, transfer learning in low-resource neural machine translation across all the language pairs considered.
",6 Conclusion,[0],[0]
The proposed approach opens new opportunities for neural machine translation.,6 Conclusion,[0],[0]
"First, it is a principled framework for incorporating various extra sources of data, such as source- and targetside monolingual corpora.",6 Conclusion,[0.954171528344891],"['These results suggest that dialogue systems should present detection of referential ambiguity implicitly, and as a list.']"
"Second, it is a generic framework that can easily accommodate existing and future neural machine translation systems.",6 Conclusion,[0],[0]
This research was supported in part by the Facebook Low Resource Neural Machine Translation Award.,Acknowledgement,[0],[0]
This work was also partly supported by Samsung Advanced Institute of Technology (Next Generation Deep Learning: from pattern recognition to AI) and Samsung Electronics (Improving Deep Learning using Latent Structure).,Acknowledgement,[0],[0]
"KC thanks support by eBay, TenCent, NVIDIA and CIFAR.",Acknowledgement,[0],[0]
"In this paper, we propose to extend the recently introduced model-agnostic meta-learning algorithm (MAML, Finn et al., 2017) for lowresource neural machine translation (NMT).",abstractText,[0],[0]
"We frame low-resource translation as a metalearning problem, and we learn to adapt to low-resource languages based on multilingual high-resource language tasks.",abstractText,[0],[0]
"We use the universal lexical representation (Gu et al., 2018b) to overcome the input-output mismatch across different languages.",abstractText,[0],[0]
"We evaluate the proposed meta-learning strategy using eighteen European languages (Bg, Cs, Da, De, El, Es, Et, Fr, Hu, It, Lt, Nl, Pl, Pt, Sk, Sl, Sv and Ru) as source tasks and five diverse languages (Ro, Lv, Fi, Tr and Ko) as target tasks.",abstractText,[0],[0]
"We show that the proposed approach significantly outperforms the multilingual, transfer learning based approach (Zoph et al., 2016) and enables us to train a competitive NMT system with only a fraction of training examples.",abstractText,[0],[0]
"For instance, the proposed approach can achieve as high as 22.04 BLEU on Romanian-English WMT’16 by seeing only 16,000 translated words (⇠ 600 parallel sentences).",abstractText,[0],[0]
Meta-Learning for Low-Resource Neural Machine Translation,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 375–385 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
"Online platforms have revolutionized the way individuals collect and share information (O’Connor et al., 2010; Lee and Ma, 2012; Bakshy et al., 2015), but the vast bulk of online content is irrelevant or unpalatable to any given individual.",1 Introduction,[0],[0]
"A user interested in political discussion, for instance, might prefer content concerning a specific candidate or issue, and only then if discussed in a positive light without controversy (Adamic and Glance, 2005; Bakshy et al., 2015).
",1 Introduction,[0],[0]
"How do individuals facing such large quantities of superfluous material select which conversations to engage in, and how might we better algorithmically recommend conversations suited to individual users?",1 Introduction,[0],[0]
We approach this problem from a microblog conversation recommendation framework.,1 Introduction,[0],[0]
"Where prior work has focused on the content of individual posts for recommendation (Chen
et al., 2012; Yan et al., 2012; Vosecky et al., 2014; He and Tan, 2015), we examine the entire history and context of a conversation, including both topical content and discourse modes such as agreement, question-asking, argument and other dialogue acts (Ritter et al.,",1 Introduction,[0],[0]
"2010).1 And where Backstrom et al. (2013) leveraged conversation reply structure (such as previous user engagement), their model is unable to predict first entry into new conversations, while ours is able to predict both new
1In this paper, discourse mode refers to a certain type of dialogue act, e.g., agreement or argument.",1 Introduction,[0],[0]
"The discourse structure of a conversation means some combination (or a probability distribution) of discourse modes.
375
and repeated entry into conversations based on a combination of topical and discourse features.
",1 Introduction,[0],[0]
"To illustrate the interplay between topics and discourse, Figure 1 displays two snippets of conversations on Twitter collected during the 2016 United States presidential election.",1 Introduction,[0],[0]
User U1 participates in both conversations.,1 Introduction,[0],[0]
"The first conversation is centered around Clinton, and U1, who is more typically involved with conversations about candidate Sanders, does not return.",1 Introduction,[0],[0]
"In the second conversation, however, U1 is involved in a heated back-and-forth debate, and thus is drawn back to a conversation that they may otherwise have abandoned but for their enjoyment of adversarial discourse.
",1 Introduction,[0],[0]
"Effective conversation prediction and recommendation requires an understanding of both user interests and discourse behaviors, such as agreement, disagreement, inquiry, backchanneling, and emotional reactions.",1 Introduction,[0],[0]
"However, acquiring manual labels for both is a time-consuming process and hard to scale for new datasets.",1 Introduction,[0],[0]
"We instead propose a unified statistical learning framework for conversation recommendation, which jointly learns (1) hidden factors that reflect user interests based on conversation history, and (2) topics and discourse modes in ongoing conversations, as discovered by a novel probabilistic latent variable model.",1 Introduction,[0],[0]
"Our model is built on the success of collaborative filtering (CF) in recommendation systems, where latent dimensions of product ratings or movie reviews are extracted to better capture user preferences (Linden et al., 2003; Salakhutdinov and Mnih, 2008; Wang and Blei, 2011; McAuley and Leskovec, 2013).",1 Introduction,[0],[0]
"To the best of our knowledge, we are the first to model both topics and discourse modes as part of a CF framework and apply it to microblog conversation recommendation.2
Experimental results on two Twitter conversation datasets show that our proposed model yields significantly better performance than state-of-theart post-level recommendation systems.",1 Introduction,[0],[0]
"For example, by leveraging both topical content and discourse structure, our model achieves a mean average precision (MAP) of 0.76 on conversations about the U.S. presidential election, compared with 0.70 by McAuley and Leskovec (2013), which only considers topics.",1 Introduction,[0],[0]
"We further con-
2To ensure the general applicability of our approach to domains lacking such information, we do not utilize external features such as network structure, but it may certainly be added in future, more narrowly targeted applications.
ducted detailed analysis on the latent topics and discourse modes and find that our model can discover reasonable topic and discourse representations, which play an important role in characterizing reply behaviors.",1 Introduction,[0.9611137105565736],"['We should note that some differences between our findings and those of others may in part rest on differences in task and environment, though intrinsic variables such as mental effort will likely persist over different situations.']"
"Finally, we also provide a pilot study on recommendation for first time replies, which shows that our model outperforms comparable recommendation systems.
",1 Introduction,[0],[0]
The rest of this paper is structured as follows.,1 Introduction,[0],[0]
The related work is discussed in Section 2.,1 Introduction,[0],[0]
We then present our microblog conversation recommendation model in Section 3.,1 Introduction,[0],[0]
The experimental setup and results are described in Sections 4 and 5.,1 Introduction,[0],[0]
"Finally, we conclude in Section 6.",1 Introduction,[0],[0]
"Social media has attracted increasing attention in digital communication research (Agichtein et al., 2008; Kwak et al., 2010; Wu et al., 2011).",2 Related Work,[0],[0]
"The problem studied here is closely related to work on recommendation and response prediction in microblogs (Artzi et al., 2012; Hong et al., 2013), where the goal is to predict whether a user will share or reply to a given post.",2 Related Work,[0],[0]
"Existing methods focus on measuring features that reflect personalized user interests, including topics (Hong et al., 2013) and network structures (Pan et al., 2013; He and Tan, 2015).",2 Related Work,[0],[0]
"These features have been investigated under a learning to rank framework (Duan et al., 2010; Artzi et al., 2012), graph ranking models (Yan et al., 2012; Feng and Wang, 2013; Alawad et al., 2016), and neural network-based representation learning methods (Yu et al., 2016).
",2 Related Work,[0],[0]
"Distinguishing from prior work that focuses on post-level recommendation, we tackle the challenges of predicting user reply behaviors at the conversation-level.",2 Related Work,[0],[0]
"In addition, our model not only captures latent factors such as the topical interests of users, but also leverages the automatically learned discourse structure.",2 Related Work,[0],[0]
"Much of the previous work on discourse structure and dialogue acts has relied on labeled data (Jurafsky et al., 1997; Stolcke et al., 2000), while unsupervised approaches have not been applied to the problem of conversation recommendation (Woszczyna and Waibel, 1994; Crook et al., 2009; Ritter et al., 2010; Joty et al., 2011).
",2 Related Work,[0],[0]
"Our work is also in line with conversation modeling for social media discussions (Ritter et al., 2010; Budak and Agrawal, 2013; Louis and Cohen, 2015; Cheng et al., 2017).",2 Related Work,[0],[0]
"Topic modeling
has been employed to identify conversation content on Twitter (Ritter et al., 2010).",2 Related Work,[0],[0]
"In this work, we propose a probabilistic model to capture both topics and discourse modes as latent variables.",2 Related Work,[0],[0]
"A further line of work studies the reposting and reply structure of conversations (Gómez et al., 2011; Laniado et al., 2011; Backstrom et al., 2013; Budak and Agrawal, 2013).",2 Related Work,[0],[0]
"But none of this work distinguishes the rich discourse functions of replies, which is modeled and exploited in our work.",2 Related Work,[0],[0]
Our proposed microblog conversation recommendation framework is based on collaborative filtering and a novel probabilistic graphical model.,3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Concretely, our objective function takes the form:
minL+ µ ·NLL(C |Θ) (1)
",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
This function encodes two types of information.,3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"First, L models user reply preference in a similar fashion to collaborative filtering (CF) (Hu et al., 2008; Pan et al., 2008).",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"It captures topics of interests and discourse structures users are commonly involved (e.g., argumentation), and takes the form of mean square error (MSE) based on user reply history.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"This part is detailed in Section 3.1.
",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"The second term, NLL(C |Θ), denotes the negative log-likelihood of a set of conversations C, with Θ containing all parameters.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"A probabilistic model is described in Section 3.2 that shows how the topical content and discourse structures of conversations are captured by these latent variables.
",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
The hyperparameter µ controls the trade-off between the two effects.,3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"`2 regularization is also added for parameters to avoid model overfitting.
",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"For the rest of this section, we first present the construction of L andNLL(C |Θ) in Sections 3.1 and 3.2.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
We then discuss how these two components can be mutually informed by each other in Section 3.3.,3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Finally, the generative process and parameter learning are described in Section 3.4.
3.1 Reply Preference (L) Our user reply preference modeling is built on the success of collaborative filtering (CF) for product ratings.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"However, classic CF problems, such as product recommendation, generally rely on explicit user feedback.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Unlike user ratings on products, our input lacks explicit feedback from users about negative preferences and nonresponse.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Therefore, we follow one-class Collaborative Filtering (Hu et al., 2008; Pan et al., 2008),
which weights positive instances higher during training and is thus suited to our data.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Formally, for user u and conversation c, we measure reply preference based on the MSE between predicted preference score pu,c and reply history ru,c. ru,c equals 1 if u is in the conversation history; otherwise, it is 0.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"The first term of objective (Eq. 1) takes the following form:
L = |U|∑
u=1
|C|∑
c=1
fu,c · (pu,c − ru,c)2 (2)
where U consists of users {u} and C is a set of conversations {c} in a dataset.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"fu,c is the corresponding weight for a conversation c and a target user u. Intuitively, it has a large value if positive feedback (user replied) is observed.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Therefore, we adapt the formulation from Pan et al. (2008):
fu,c = { s if ru,c = 1 (i.e., user replied) 1 if ru,c = 0
(3)
where s > 1, an integer hyperparameter to be tuned.
",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Inspired by prior models (Koren et al., 2009; McAuley and Leskovec, 2013), we propose the following latent factor model to describe pu,c:
pu,c = λ · γUu · γCc + (1− λ) ·",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
δUu · δCc,3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"+ bu + bc + a (4)
γUu and γ C c are K-dimensional latent vectors that encode topic-specific information (where K is the number of latent topics) for users and conversations.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Specifically, γUu reflects the topical interests of u, with higher value γUu,k indicating greater interest by u in topic k. γCc captures the extents that topics are discussed in conversation c.
Similarly, D-dimensional vectors δUu and δ C c capture discourse structures in shaping reply behaviors (where D is the number of discourse clusters).",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"δUu reflects the discourse behaviors u prefers, such as u1 often enjoys arguments as in the second conversation of Figure 1, while δCc captures the discourse modes used throughout conversation c. By multiplying user and conversation factors, we can measure the corresponding similarity.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"The predicted score pu,c thereby reflects the tendency for a user u to be involved in conversation c.
As pointed out by McAuley and Leskovec (2013), these latent vectors often encode hidden factors that are hard to interpret under a CF framework.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Therefore, in Section 3.2, we present a novel probabilistic model which can extract interpretable topics and discourse modes as word
distributions.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"We then describe how they can be aligned with the latent vectors of γC and δU .
",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Parameter a is an offset parameter, bu and bc are user and conversation biases, and λ ∈",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"[0, 1] serves as the weight for trading offs of topic and discourse factors in reply preference modeling.
",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
3.2 Corpus Likelihood NLL(C |Θ),3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
Here we present a novel probabilistic model that learns coherent word distributions for latent topics and discourse modes of conversations.,3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Formally, we assume that each conversation c ∈ C contains Mc messages, and each message m has Nc,m words.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"We distinguish three latent components – discourse, topic, and background – underlying conversations, each with their own type of word distribution.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"At the corpus level, there are K topics represented by word distribution φTk (k = 1, 2, ...,K), while φDd (d = 1, 2, ..., D) represents the D discourse modes embedded in corpus.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"In addition, we add a background word distribution φB to capture general information (e.g., common words), which do not indicate either discourse or topic information.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"φDd , φ T k , and φ
B are all multinomial word distributions over vocabulary size V .",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Below describes more details.
",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
Message-level Modeling.,3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Our model assigns two types of message-level multinomial variables to each message: zc,m reflects its latent topic and dc,m represents its discourse mode.
",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
Topic assignments.,3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Due to the short nature of microblog posts, we assume each message m in conversation c contains only one topic, indexed as zc,m. This strategy has been proven useful to alleviate data sparsity for topic inference (Quan et al., 2015).",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
We further assume messages in the same conversation would focus on similar topics.,3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"We thus draw topic zc,m ∼ θc, where θc denotes the fractions of topics discussed in conversation c.
Discourse assignments.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"To capture discourse behaviors of u, distribution πu is used to represent the discourse modes in messages posted by u.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"The discourse mode dc,m for message m is then generated from πuc,m , where uc,m is the author of m in c.
Word-level Modeling.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"We aim to separate discourse, topic, and background information for conversations.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Therefore, for each word wc,m,n of message m, a ternary switcher xc,m,n ∈ {DISC, TOPIC,BACK} controls word wc,m,n to
fall into one of the three types: discourse, topic, and background.
",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
Discourse words (DISC) are indicative of the discourse modes of messages.,3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"When xc,m,n = DISC (i.e., wc,m,n is assigned as a discourse word), word wc,m,n is generated from the discourse word distribution φDdc,m where dc,m is discourse assignment to",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"message m.
Topic words (TOPIC) describe the topical focus of a conversation.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"When xc,m,n = TOPIC, wc,m,n is assigned as a topic word and generated from φTzc,m – word distribution given topic of m.
Background words (BACK) capture the general information that is not related to discourse or topic.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"When word wc,m,n is assigned as a background word (xc,m,n = BACK), it is drawn from background distribution φB .
",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Switching among Topic, Discourse, and Background.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"We further assume the word type switcher xc,m,n is sampled from a multinomial distribution which depends on the current discourse mode dc,m. The intuition is that messages of different discourse modes may show different distributions of the three word types.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"For instance, a statement message may contain more content words than a rhetorical question.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Specifically, xc,m,n ∼ Multi(τdc,m), where τd is a 3-dimension stochastic vector that expresses the appearing probabilities of three kinds of words (DISC, TOPIC, BACK), when the discourse assignment is d. Stop words and punctuations are forced to be labeled as discourse or background.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"By explicitly distinguishing different types of words with switcher xc,m,n, we can thus separate word distributions that reflect discourse, topic, and background information.
",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
Likelihood.,3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Based on the message-level and the word-level generation process, the probability of observing words in the given corpus is:
Pr(C |θ,π,φ, τ , z,d,x)
=
C∏
c=1
Mc∏
m=1
θc,zc,mπuc,m,dc,m
× ∏
xc,m,n=BACK
τdc,m,BACKφ B wc,m,n
× ∏
xc,m,n=DISC
τdc,m,DISCφ D dc,m,wc,m,n
× ∏
xc,m,n=TOPIC
τdc,m,TOPICφ T zc,m,wc,m,n
(5)
",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"And we use negative log likelihood to model corpus likelihood effect in Eq. 1, i.e., NLL(C |Θ) =
− log(Pr(C |Θ), where parameters set Θ = {θ,π,φ, τ , z,d,x}.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Latent Variables
As mentioned above, the hidden factors discovered in Section 3.1 lack interpretability, which can be boosted by the learned latent topics and discourse modes in Section 3.2.",3.3 Mutually Informed User Preference and,[0],[0]
"However, it is nontrivial to link the topic-related parameters of γCc to the conversation topic distributions of θc, since the former takes real values from −∞ to +∞ while the latter is a stochastic vector.",3.3 Mutually Informed User Preference and,[0.9595865490370562],"['The focus of this work is on recovery from situated grounding problems, a type of miscommunication that occurs when an agent fails to uniquely map a person’s instructions to its surroundings (Marge and Rudnicky, 2013).']"
"Therefore, we follow the strategy from McAuley and Leskovec (2013) to apply a softmax function over γCc :
θc,k = exp(κT γCc,k)∑K
k′=1 exp(κ T γCc,k′)
(6)
",3.3 Mutually Informed User Preference and,[0],[0]
"We further assume that the discourse mode preference by users, δUu , can also be informed by the discourse mode distribution captured by πu, i.e., a user who enjoys arguments may be willing to participate another.",3.3 Mutually Informed User Preference and,[0],[0]
"So similarly, we define:
πu,d = exp(κDδUu,d)∑D
d′=1 exp(κ DδUu,d′)
(7)
where κT and κD are learnable parameters that control the “peakiness” of the transformation.",3.3 Mutually Informed User Preference and,[0],[0]
"For example, a larger κT indicates a more focused conversation, while a smaller κT means users discuss diverse topics.
",3.3 Mutually Informed User Preference and,[0],[0]
"Finally, softmax transformation is also applied to φTk , φ D d , φ
B , and τd, as done in McAuley and Leskovec (2013), with additional parameters ψTk , ψDd , ψ
B , and χd (as shown in Figure 2).",3.3 Mutually Informed User Preference and,[0],[0]
This is to ensure that the distributions φ∗∗ and τd are stochastic vectors.,3.3 Mutually Informed User Preference and,[0],[0]
"In doing so, these distributions can be learned via optimizing ψ∗∗ and χd, which take any value and thus ensure that the cost function in Eq. 1 is optimized without considering any parameter constraints.",3.3 Mutually Informed User Preference and,[0],[0]
"Our word generation process is displayed in Figure 2 and described as follows:
• Compute topic distribution θc by Eq. 6 •",3.4 Generative Process and Model Learning,[0],[0]
"For message m = 1 to Mc:
– Compute discourse distribution πuc,m by Eq. 7 – Draw topic assignment zc,m ∼Multi(θc) – Draw discourse mode dc,m ∼Multi(πuc,m) –",3.4 Generative Process and Model Learning,[0],[0]
"For word index n = 1 to Nc,m: ∗ Draw word type xc,m,n ∼Multi(τd)
∗",3.4 Generative Process and Model Learning,[0],[0]
"if xc,m,n == BACK:",3.4 Generative Process and Model Learning,[0],[0]
"Draw word wc,m,n ∼Multi(φB) ∗",3.4 Generative Process and Model Learning,[0],[0]
"if xc,m,n == DISC:",3.4 Generative Process and Model Learning,[0],[0]
"Draw word wc,m,n ∼Multi(φDdc,m) ∗",3.4 Generative Process and Model Learning,[0],[0]
"if xc,m,n == TOPIC:",3.4 Generative Process and Model Learning,[0],[0]
"Draw word wc,m,n ∼Multi(φTzc,m)
Parameter Learning.",3.4 Generative Process and Model Learning,[0],[0]
"For learning, we randomly initialize all learnable parameters and then alternate between the following two steps: Step 1.",3.4 Generative Process and Model Learning,[0],[0]
"Fix topic and discourse assignments z and d, and word type switcher x, then optimize the remaining parameters in Eq. 1 by L-BFGS (Nocedal, 1980):
Update a, b, γ∗, δ∗, κ∗, ψ∗, χ = argminL+ µ ·NLL(C |Θ) (8)
Step 2.",3.4 Generative Process and Model Learning,[0],[0]
"Sample topic and discourse assignments z and d at the message level and word type switcher x at the word level, using the distributions, computed according to parameters optimized in step 1:
Sample zc,m, dc,m, xc,m,n with probabilities p(zc,m = k) =",3.4 Generative Process and Model Learning,[0],[0]
"θc,k
p(dc,m = d) = πuc,m,d p(xc,m,n = BACK) = φ B wc,m,nτdc,m,BACK p(xc,m,n = DISC) = φ D dc,m,wc,m,nτdc,m,DISC p(xc,m,n = TOPIC) = φ T zc,m,wc,m,nτdc,m,TOPIC
(9)
Step 2 is analogous to Gibbs Sampling (Griffiths, 2002) in probabilistic graphical models, such as LDA (Blei et al., 2003).",3.4 Generative Process and Model Learning,[0],[0]
"However, distinguishing from previous models, the multinomial distributions in our models are not drawn from a Dirichlet prior.",3.4 Generative Process and Model Learning,[0],[0]
"Instead, they are computed based on the parameters learned in Step 1.
",3.4 Generative Process and Model Learning,[0],[0]
"Our learning process stops when the change of parameters is small (i.e., below a pre-specified
threshold).",3.4 Generative Process and Model Learning,[0],[0]
"Multiple restarts are tried, and similar results are achieved.",3.4 Generative Process and Model Learning,[0],[0]
Datasets.,4 Experimental Setup,[0],[0]
"We collected two microblog conversation datasets from Twitter for experiments3: one contains discussions about the U.S. presidential election (henceforth US Election), the other gathers conversations of diverse topics based on the tweets released by TREC 2011 microblog track (henceforth TREC)4.",4 Experimental Setup,[0],[0]
"US Election was collected from January to June of 2016 using Twitter’s Streaming API5 with a small set of political keywords.6 To recover conversations, Tweet Search API7 was used to retrieve messages with the “inreply-to” relations to collect tweets in a recursive way until full conversations were recovered.
",4 Experimental Setup,[0],[0]
Statistics of the datasets are shown in Table 1.,4 Experimental Setup,[0],[0]
Figure 3 displays the number of conversations individual users participated in.,4 Experimental Setup,[0],[0]
"As can be seen, most users are involved in only a few conversations.",4 Experimental Setup,[0],[0]
"Simply leveraging personal chat history will not produce good performance for conversation
3The datasets are available at http://www.ccs. neu.edu/home/luwang/
4 http://trec.nist.gov/data/tweets/ 5https://developer.twitter.com/
en/docs/tweets/filter-realtime/ api-reference/post-statuses-filter.html
6Keyword list: “trump”, “hillary”, “clinton”, “president”, “politics”, and “election.”
",4 Experimental Setup,[0],[0]
"7https://developer.twitter.com/en/ docs/tweets/search/api-reference/ get-saved_searches-show-id
recommendation.",4 Experimental Setup,[0],[0]
"In our experiments, we predict whether a user will engage in a conversation given the previous messages in that conversation and past conversations the user is involved.",4 Experimental Setup,[0],[0]
"For model training and testing, we divide conversations into three ordered segments, corresponding to training, development, and test sets at 75%, 12.5%, and 12.5%.8
Preprocessing and Hyperparameter Tuning.",4 Experimental Setup,[0],[0]
"For preprocessing, links, mentions (i.e., @username), and hashtags in tweets were replaced with generic tags of “URL”, “MENTION”, and “HASHTAG”.",4 Experimental Setup,[0],[0]
We then utilized the Twitter NLP tool9,4 Experimental Setup,[0],[0]
"(Gimpel et al., 2011; Owoputi et al., 2013) for tokenization and non-alphabetic token removal.",4 Experimental Setup,[0],[0]
We removed stop words and punctuations for all comparisons to ensure comparable performance.,4 Experimental Setup,[0],[0]
"We maintain a vocabulary with the 5,000 most frequent words.
",4 Experimental Setup,[0],[0]
"Our model parameters are tuned on the development set based on grid search, i.e. the parameters that give the lowest value for our objective are selected.",4 Experimental Setup,[0],[0]
"Specifically, the number of discourse modes (D) and topics (K) are tuned to be 10.",4 Experimental Setup,[0],[0]
"The trade-off parameter µ between user preference and corpus negative log-likelihood takes value of 0.1, and λ, the parameter for balancing topic and discourse, is set to 0.5.",4 Experimental Setup,[0],[0]
"Finally, the confidence parameter s takes a value of 200 to give higher weight for positive instances, i.e., a user replied to a conversation.
",4 Experimental Setup,[0],[0]
Evaluation Metrics.,4 Experimental Setup,[0],[0]
"Following prior work on social media post recommendation (Chen et al., 2012; Yan et al., 2012), we treat our task on conversation recommendation as a ranking problem.",4 Experimental Setup,[0],[0]
"Therefore, popular information retrieval evaluation metrics, including precision at K (P@K), mean average precision (MAP) (Manning et al., 2008), and normalized Discounted Cumulative Gain at K (nDCG@K) (Järvelin and Kekäläinen, 2002) are reported.",4 Experimental Setup,[0],[0]
The metrics are computed per user in the dataset and then averaged over all users.,4 Experimental Setup,[0],[0]
"The values range from 0.0 to 1.0, with higher values indicating better performance.
",4 Experimental Setup,[0],[0]
Baselines and Comparisons.,4 Experimental Setup,[0],[0]
"For comparison, we first consider three baselines: 1) ranking
8At least one turn per conversation is retained for training.",4 Experimental Setup,[0],[0]
"It is possible that one user only replies in either development set or test set, but it is rather infrequent.
",4 Experimental Setup,[0],[0]
"9http://www.cs.cmu.edu/˜ark/TweetNLP/
conversations randomly (RANDOM); 2) longer conversations (i.e., more words) ranked higher (LENGTH); 3) conversations with more distinct users ranked higher (POPULARITY).
",4 Experimental Setup,[0],[0]
"We further compare results with three established recommendation models: • OCCF: one-class Collaborative Filtering (Pan et al., 2008), which only considers users’ reply history without modeling content in conversations.",4 Experimental Setup,[0],[0]
• RSVM:,4 Experimental Setup,[0],[0]
"ranking SVM (Joachims, 2002), which ranks conversations for each user with the content and Twitter features as in Duan et al. (2010).",4 Experimental Setup,[0],[0]
"• CTR: messages in one conversation are aggregated into one post and a state-of-the art Collaborative Filtering-based post recommendation model is applied (Chen et al., 2012).
",4 Experimental Setup,[0],[0]
"Finally, we also adapt the “hidden factors as topics” (HFT) model proposed in McAuley and Leskovec (2013) (henceforth ADAPTED HFT).",4 Experimental Setup,[0],[0]
"Because the original model leverages the ratings for all product reviews and does not handle implicit user feedback well, we replace their user preference objective function with ours (Eq. 2).",4 Experimental Setup,[0],[0]
"In this section, we first discuss our main evaluation in Section 5.1.",5 Experimental Results,[0],[0]
"A case study and corresponding discussion are provided in Section 5.2 to provide further insights, which is followed by an analysis of the topics and discourse modes discovered by our model (Section 5.3).",5 Experimental Results,[0],[0]
We also examine our performance on first time replies (Section 5.4).,5 Experimental Results,[0],[0]
"Experimental results are displayed in Table 2, where our model yields statistically significantly better results than baselines and comparisons
(paired t-tests, p < 0.01).",5.1 Conversation Recommendation Results,[0],[0]
"For P@K, we only report P@1, because a significant amount of users participate only in 1 or 2 conversations.",5.1 Conversation Recommendation Results,[0],[0]
"For nDCG@K, different K values are experimented, which results in similar trend, so only nDCG@5 is reported.
",5.1 Conversation Recommendation Results,[0],[0]
"We find that the baselines that rank conversations with simple features (e.g., length or popularity) perform poorly.",5.1 Conversation Recommendation Results,[0],[0]
"This implies that generic algorithms that do not consider conversation content or user preference cannot produce reasonable recommendations.
",5.1 Conversation Recommendation Results,[0],[0]
"Although some non-baseline systems capture content in one way or another, only ADAPTED HFT and our model exploit latent topic models to better represent content in tweets, and outperform other methods.
",5.1 Conversation Recommendation Results,[0],[0]
"Compared to ADAPTED HFT, which only considers latent topics under a collaborative filtering framework, our model extracts both topics and discourse modes as latent variables, and shows superior performance on both datasets.",5.1 Conversation Recommendation Results,[0],[0]
"Our discourse variables go beyond topical content to capture social behaviors that affect user engagement, such as
arguments, question-asking, agreement, and other discourse modes.
",5.1 Conversation Recommendation Results,[0],[0]
Training with Varying Conversation History.,5.1 Conversation Recommendation Results,[0],[0]
"To test the model performance based different levels of user engagement history, we further experiment with varying the length of conversations for training.",5.1 Conversation Recommendation Results,[0],[0]
"Specifically, in addition to using 75% of conversation history, we also extract the first 25% and 50% of history as training.",5.1 Conversation Recommendation Results,[0],[0]
The rest of a conversation is separated equally for development and test.,5.1 Conversation Recommendation Results,[0],[0]
Figure 4 shows the MAP scores for US Election and TREC datasets.,5.1 Conversation Recommendation Results,[0],[0]
"The increasing MAP for all methods as the training history increases indicates that generally, conversation history is essential for recommendation.",5.1 Conversation Recommendation Results,[0],[0]
"Our model performs consistently better over different lengths of conversation histories.
",5.1 Conversation Recommendation Results,[0],[0]
Results for Varying Degree of Data Sparsity.,5.1 Conversation Recommendation Results,[0],[0]
"From Table 1 and Figure 3, we observe that most users in our datasets are involved in only a few conversations.",5.1 Conversation Recommendation Results,[0],[0]
"In order to study the effects of data sparsity on recommendation models, we examine in Figure 5 the MAP scores for users engaged in a varying number of conversations, as measured on the TREC dataset.",5.1 Conversation Recommendation Results,[0],[0]
The results on the US Election dataset have similar distributions.,5.1 Conversation Recommendation Results,[0],[0]
"As we see, the prediction results become worse for users involved in fewer conversations.",5.1 Conversation Recommendation Results,[0],[0]
This indicates that data sparsity serves as a challenge for all recommendation models.,5.1 Conversation Recommendation Results,[0],[0]
We also observe that our model performs consistently better than other models over different degrees of sparsity.,5.1 Conversation Recommendation Results,[0],[0]
"This implies that effectively capturing discourse structure in conversation context is useful to mitigating the effects of
data sparsity on conversation recommendation.",5.1 Conversation Recommendation Results,[0],[0]
Here we present a case study based on the sample conversations in Figure 1.,5.2 Case Study and Discussion,[0],[0]
"Recall that user U1 is interested in conversations about Sanders, and also prefers more argumentative discourse, and thus returns in conversation c2 but not c1.
",5.2 Case Study and Discussion,[0],[0]
"Table 3 shows the predicted scores for the two conversations from OCCF, ADAPTED HFT, and our model (as in Eq. 2).",5.2 Case Study and Discussion,[0],[0]
"Both ADAPTED HFT and our model more accurately recommend c2 over c1, with our model producing a slightly higher recommendation score for c2.
",5.2 Case Study and Discussion,[0],[0]
Table 4 shows the latent dimension values for the learned topics and discourse modes for this user and these two conversations.,5.2 Case Study and Discussion,[0],[0]
"Based on human inspection, topic 1 appears to contain words about Sanders, which is the main topic in conversation c2.",5.2 Case Study and Discussion,[0],[0]
"Topic 2 is about Clinton, which is a dominating topic in conversation c1.",5.2 Case Study and Discussion,[0],[0]
"Our model also picks up user interest in topic 1 (Sanders), and thus assigns γUu1,1 a high value.",5.2 Case Study and Discussion,[0],[0]
"For discourse modes, our model also generates a high score for “argument” discourse (labeled via human inspection) for both the user and c2.",5.2 Case Study and Discussion,[0],[0]
Ablation Study.,5.3 Further Analysis of Topic and Discourse,[0],[0]
"We have shown that joint modeling of topical content and discourse modes produces the superior performance for our model.
",5.3 Further Analysis of Topic and Discourse,[0],[0]
Here we provide an ablation study to examine the relative contributions of those two aspects by setting the trade-off parameter λ to 1.0 (topic only) or 0.0 (discourse only).,5.3 Further Analysis of Topic and Discourse,[0],[0]
"Table 5 shows that topics or discourse individually improve slightly upon the comparison ADAPTED HFT, but only jointly do they improve significantly upon it.
",5.3 Further Analysis of Topic and Discourse,[0],[0]
Topic Coherence.,5.3 Further Analysis of Topic and Discourse,[0],[0]
"To examine the quality of topics found by our model, we use the CV topic coherence score measured via the open-source toolkit Palmetto10, which has been shown to produce evaluation performance comparable to human judgment (Röder et al., 2015).",5.3 Further Analysis of Topic and Discourse,[0],[0]
"Our model achieves topic coherence scores of 0.343 and 0.376 on TREC and US Election datasets, compared to 0.338 and 0.371 for the topics from ADAPTED HFT.
",5.3 Further Analysis of Topic and Discourse,[0],[0]
Sample Discourse Modes.,5.3 Further Analysis of Topic and Discourse,[0],[0]
"While our topic word distributions are relatively unsurprising, of greater interest are the discourse mode word distributions.",5.3 Further Analysis of Topic and Discourse,[0],[0]
Table 6 shows a sample of discourse modes as labeled by human.,5.3 Further Analysis of Topic and Discourse,[0],[0]
"Although this is merely a qualitative human judgment at this point, there does appear to be a notable overlap in discourse modes between the two datasets even though they were learned separately.
",5.3 Further Analysis of Topic and Discourse,[0],[0]
10https://github.com/AKSW/Palmetto/,5.3 Further Analysis of Topic and Discourse,[0],[0]
"From a recommendation perspective, users may be interested in joining new conversations.",5.4 First Time Reply Results,[0],[0]
We thus compare each recommendation system for first time replies.,5.4 First Time Reply Results,[0],[0]
"For each user, we only evaluate for conversations where they are newcomers.",5.4 First Time Reply Results,[0],[0]
"Table 7 shows that, unsurprisingly, all systems perform poorly on this task, though our model performs slightly better.",5.4 First Time Reply Results,[0],[0]
"This suggests that other features, e.g., network structures or other discussion thread features, could usefully be included in future studies that target new conversations.",5.4 First Time Reply Results,[0],[0]
This paper has presented a framework for microblog conversation recommendation via jointly modeling topics and discourse modes.,6 Conclusion,[0],[0]
Experimental results show that our method can outperform competitive approaches that omit user discourse behaviors.,6 Conclusion,[0],[0]
Qualitative analysis shows that our joint model yields meaningful topics and discourse representations.,6 Conclusion,[0],[0]
This work is partly supported by Innovation and Technology Fund (ITF) Project,Acknowledgements,[0],[0]
"No. 6904333, General Research Fund (GRF) Project No. 14232816 (12183516), and National Science Foundation Grant IIS-1566382.",Acknowledgements,[0],[0]
"We thank Shuming Shi, Yan Song, and the three anonymous reviewers for the insightful suggestions on various aspects of this work.",Acknowledgements,[0],[0]
Millions of conversations are generated every day on social media platforms.,abstractText,[0],[0]
"With limited attention, it is challenging for users to select which discussions they would like to participate in.",abstractText,[0],[0]
Here we propose a new method for microblog conversation recommendation.,abstractText,[0],[0]
"While much prior work has focused on postlevel recommendation, we exploit both the conversational context, and user content and behavior preferences.",abstractText,[0],[0]
"We propose a statistical model that jointly captures: (1) topics for representing user interests and conversation content, and (2) discourse modes for describing user replying behavior and conversation dynamics.",abstractText,[0],[0]
Experimental results on two Twitter datasets demonstrate that our system outperforms methods that only model content without considering discourse.,abstractText,[0],[0]
Microblog Conversation Recommendation via Joint Modeling of Topics and Discourse,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 102–112 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"One of the key advantages of word embeddings for natural language processing is that they enable generalization to words that are unseen in labeled training data, by embedding lexical features from large unlabeled datasets into a relatively low-dimensional Euclidean space.",1 Introduction,[0],[0]
"These low-dimensional embeddings are typically trained to capture distributional similarity, so that information can be shared among words that tend to appear in similar contexts.
",1 Introduction,[0],[0]
"However, it is not possible to enumerate the entire vocabulary of any language, and even large unlabeled datasets will miss terms that appear in later applications.",1 Introduction,[0],[0]
The issue of how to handle these out-of-vocabulary (OOV) words poses challenges for embedding-based methods.,1 Introduction,[0],[0]
"These challenges are particularly acute when working with lowresource languages, where even unlabeled data may be difficult to obtain at scale.",1 Introduction,[0],[0]
"A typical solution is to abandon hope, by assigning a single OOV embedding to all terms that do not appear in the unlabeled data.
",1 Introduction,[0],[0]
We approach this challenge from a quasigenerative perspective.,1 Introduction,[0],[0]
"Knowing nothing of a word except for its embedding and its written form, we attempt to learn the former from the latter.",1 Introduction,[0],[0]
"We train a recurrent neural network (RNN) on the character level with the embedding as the target, and use it later to predict vectors for OOV words in any downstream task.",1 Introduction,[0],[0]
"We call this model the MIMICK-RNN, for its ability to read a word’s spelling and mimick its distributional embedding.
",1 Introduction,[0],[0]
"Through nearest-neighbor analysis, we show that vectors learned via this method capture both word-shape features and lexical features.",1 Introduction,[0],[0]
"As a result, we obtain reasonable near-neighbors for OOV abbreviations, names, novel compounds, and orthographic errors.",1 Introduction,[0],[0]
"Quantitative evaluation on the Stanford RareWord dataset (Luong et al., 2013) provides more evidence that these character-based embeddings capture word similarity for rare and unseen words.
",1 Introduction,[0],[0]
"As an extrinsic evaluation, we conduct experiments on joint prediction of part-of-speech tags and morphosyntactic attributes for a diverse set of 23 languages, as provided in the Universal Dependencies dataset (De Marneffe et al., 2014).",1 Introduction,[0],[0]
"Our model shows significant improvement
102
across the board against a single UNK-embedding backoff method, and obtains competitive results against a supervised character-embedding model, which is trained end-to-end on the target task.",1 Introduction,[0],[0]
"In low-resource settings, our approach is particularly effective, and is complementary to supervised character embeddings trained from labeled data.",1 Introduction,[0],[0]
The MIMICK-RNN therefore provides a useful new tool for tagging tasks in settings where there is limited labeled data.,1 Introduction,[0],[0]
Models and code are available at www.github.com/ yuvalpinter/mimick .,1 Introduction,[0],[0]
Compositional models for embedding rare and unseen words.,2 Related Work,[0],[0]
"Several studies make use of morphological or orthographic information when training word embeddings, enabling the prediction of embeddings for unseen words based on their internal structure.",2 Related Work,[0],[0]
Botha and Blunsom (2014) compute word embeddings by summing over embeddings of the morphemes; Luong et al. (2013) construct a recursive neural network over each word’s morphological parse; Bhatia et al. (2016) use morpheme embeddings as a prior distribution over probabilistic word embeddings.,2 Related Work,[0],[0]
"While morphology-based approaches make use of meaningful linguistic substructures, they struggle with names and foreign language words, which include out-of-vocabulary morphemes.",2 Related Work,[0],[0]
"Character-based approaches avoid these problems: for example, Kim et al. (2016) train a recurrent neural network over words, whose embeddings are constructed by convolution over character embeddings; Wieting et al. (2016) learn embeddings of character ngrams, and then sum them into word embeddings.",2 Related Work,[0],[0]
"In all of these cases, the model for composing embeddings of subword units into word embeddings is learned by optimizing an objective over a large unlabeled corpus.",2 Related Work,[0],[0]
"In contrast, our approach is a post-processing step that can be applied to any set of word embeddings, regardless of how they were trained.",2 Related Work,[0],[0]
"This is similar to the “retrofitting” approach of Faruqui et al. (2015), but rather than smoothing embeddings over a graph, we learn a function to build embeddings compositionally.
",2 Related Work,[0],[0]
Supervised subword models.,2 Related Work,[0],[0]
Another class of methods learn task-specific character-based word embeddings within end-to-end supervised systems.,2 Related Work,[0],[0]
"For example, Santos and Zadrozny (2014) build word embeddings by convolution over char-
acters, and then perform part-of-speech (POS) tagging using a local classifier; the tagging objective drives the entire learning process.",2 Related Work,[0],[0]
"Ling et al. (2015) propose a multi-level long shortterm memory (LSTM; Hochreiter and Schmidhuber, 1997), in which word embeddings are built compositionally from an LSTM over characters, and then tagging is performed by an LSTM over words.",2 Related Work,[0],[0]
Plank et al. (2016) show that concatenating a character-level or bit-level LSTM network to a word representation helps immensely in POS tagging.,2 Related Work,[0],[0]
"Because these methods learn from labeled data, they can cover only as much of the lexicon as appears in their labeled training sets.",2 Related Work,[0],[0]
"As we show, they struggle in several settings: lowresource languages, where labeled training data is scarce; morphologically rich languages, where the number of morphemes is large, or where the mapping from form to meaning is complex; and in Chinese, where the number of characters is orders of magnitude larger than in non-logographic scripts.",2 Related Work,[0],[0]
"Furthermore, supervised subword models can be combined with MIMICK, offering additive improvements.
",2 Related Work,[0],[0]
Morphosyntactic attribute tagging.,2 Related Work,[0],[0]
"We evaluate our method on the task of tagging word tokens for their morphosyntactic attributes, such as gender, number, case, and tense.",2 Related Work,[0],[0]
"The task of morpho-syntactic tagging dates back at least to the mid 1990s (Oflazer and Kuruöz, 1994; Hajič and Hladká, 1998), and interest has been rejuvenated by the availability of large-scale multilingual morphosyntactic annotations through the Universal Dependencies (UD) corpus (De Marneffe et al., 2014).",2 Related Work,[0],[0]
"For example, Faruqui et al. (2016) propose a graph-based technique for propagating typelevel morphological information across a lexicon, improving token-level morphosyntactic tagging in 11 languages, using an SVM tagger.",2 Related Work,[0],[0]
"In contrast, we apply a neural sequence labeling approach, inspired by the POS tagger of Plank et al. (2016).",2 Related Work,[0],[0]
"We approach the problem of out-of-vocabulary (OOV) embeddings as a generation problem: regardless of how the original embeddings were created, we assume there is a generative wordformbased protocol for creating these embeddings.",3 MIMICK Word Embeddings,[0],[0]
"By training a model over the existing vocabulary, we can later use that model for predicting the embedding of an unseen word.
",3 MIMICK Word Embeddings,[0],[0]
"Formally: given a language L, a vocabulary V ⊆ L of size V , and a pre-trained embeddings table W ∈ RV×d where each word {wk}Vk=1 is assigned a vector ek of dimension d, our model is trained to find the function f :",3 MIMICK Word Embeddings,[0],[0]
L → Rd such that the projected function f |V approximates the assignments f(wk),3 MIMICK Word Embeddings,[0],[0]
≈ ek.,3 MIMICK Word Embeddings,[0],[0]
"Given such a model, a new word wk∗ ∈ L \ V can now be assigned an embedding ek∗ = f(wk∗).
",3 MIMICK Word Embeddings,[0],[0]
Our predictive function of choice is a Word Type Character Bi-LSTM.,3 MIMICK Word Embeddings,[0],[0]
"Given a word with character sequence w = {ci}n1 , a forward-LSTM and a backward-LSTM are run over the corresponding character embeddings sequence {e(c)i }n1 .",3 MIMICK Word Embeddings,[0],[0]
"Let hnf represent the final hidden vector for the forward-LSTM, and let h0b represent the final hidden vector for the backward-LSTM.",3 MIMICK Word Embeddings,[0],[0]
"The word embedding is computed by a multilayer perceptron:
(1)f(w) = OT · g(Th ·",3 MIMICK Word Embeddings,[0],[0]
"[hnf ; h0b ] + bh) + bT ,
where Th, bh and OT , bT are parameters of affine transformations, and g is a nonlinear elementwise function.",3 MIMICK Word Embeddings,[0],[0]
"The model is presented in Figure 1.
",3 MIMICK Word Embeddings,[0],[0]
The training objective is similar to that of Yin and Schütze (2016).,3 MIMICK Word Embeddings,[0],[0]
"We match the predicted embeddings f(wk) to the pre-trained word embeddings ewk , by minimizing the squared Euclidean distance,
(2)L = ‖f(wk)− ewk‖22 .
",3 MIMICK Word Embeddings,[0],[0]
"By backpropagating from this loss, it is possible to obtain local gradients with respect to the parameters of the LSTMs, the character embeddings, and the output model.",3 MIMICK Word Embeddings,[0],[0]
"The ultimate output of the training phase is the character embeddings matrix C and the parameters of the neural network: M = {C,F,B,Th, bh,OT , bT }, where F,B are the forward and backward LSTM component parameters, respectively.",3 MIMICK Word Embeddings,[0],[0]
"The pretrained embeddings we use in our experiments are obtained from Polyglot (Al-Rfou et al., 2013), a multilingual word embedding effort.",3.1 MIMICK Polyglot Embeddings,[0],[0]
"Available for dozens of languages, each dataset contains 64-dimension embeddings for the 100,000 most frequent words in a language’s training corpus (of variable size), as well as an UNK embedding to be used for OOV words.",3.1 MIMICK Polyglot Embeddings,[0],[0]
"Even with this vocabulary size, querying words from respective UD corpora (train + dev + test) yields high
OOV rates: in at least half of the 23 languages in our experiments (see Section 5), 29.1% or more of the word types do not appear in the Polyglot vocabulary.",3.1 MIMICK Polyglot Embeddings,[0],[0]
"The token-level median rate is 9.2%.1
Applying our MIMICK algorithm to Polyglot embeddings, we obtain a prediction model for each of the 23 languages.",3.1 MIMICK Polyglot Embeddings,[0],[0]
"Based on preliminary testing on randomly selected held-out development sets of 1% from each Polyglot vocabulary (with error calculated as in Equation 2), we set the following hyper-parameters for the remainder of the experiments: character embedding dimension = 20; one LSTM layer with 50 hidden units; 60 training epochs with no dropout; nonlinearity function g =",3.1 MIMICK Polyglot Embeddings,[0],[0]
"tanh.2 We initialize character embeddings randomly, and use DyNet to implement the model (Neubig et al., 2017).
",3.1 MIMICK Polyglot Embeddings,[0],[0]
Nearest-neighbor examination.,3.1 MIMICK Polyglot Embeddings,[0],[0]
"As a preliminary sanity check for the validity of our protocol, we examined nearest-neighbor samples in languages for which speakers were available: English, Hebrew, Tamil, and Spanish.",3.1 MIMICK Polyglot Embeddings,[0],[0]
"Table 1 presents selected English OOV words with
1Some OOV counts, and resulting model performance, may be adversely affected by tokenization differences between Polyglot and UD.",3.1 MIMICK Polyglot Embeddings,[0],[0]
"Notably, some languages such as Spanish, Hebrew and Italian exhibit relational synthesis wherein words of separate grammatical phrases are joined into one form (e.g. Spanish del = de + el, ‘from the-masc.sg.’).",3.1 MIMICK Polyglot Embeddings,[0],[0]
"For these languages, the UD annotations adhere to the sub-token level, while Polyglot does not perform subtokenization.",3.1 MIMICK Polyglot Embeddings,[0],[0]
"As this is a real-world difficulty facing users of out-of-the-box embeddings, we do not patch it over in our implementations or evaluation.
",3.1 MIMICK Polyglot Embeddings,[0],[0]
"2Other settings, described below, were tuned on the supervised downstream tasks.
",3.1 MIMICK Polyglot Embeddings,[0],[0]
their nearest in-vocabulary Polyglot words computed by cosine similarity.,3.1 MIMICK Polyglot Embeddings,[0],[0]
"These examples demonstrate several properties: (a) word shape is learned well (acronyms, capitalizations, suffixes); (b) the model shows robustness to typos (e.g., developiong, corssing); (c) part-of-speech is learned across multiple suffixes (pesky – euphoric, ghastly); (d) word compounding is detected (e.g., lawnmower – bookmaker, postman); (e) semantics are not learned well (as is to be expected from the lack of context in training), but there are surprises (e.g., flatfish – slimy, watery).",3.1 MIMICK Polyglot Embeddings,[0],[0]
"Table 2 presents examples from Hebrew that show learned properties can be extended to nominal morphosyntactic attributes (gender, number – first two examples) and even relational syntactic subword forms such as genetive markers (third example).",3.1 MIMICK Polyglot Embeddings,[0],[0]
Names are learned (fourth example) despite the lack of casing in the script.,3.1 MIMICK Polyglot Embeddings,[0],[0]
"Spanish examples exhibit wordshape and part-of-speech learning patterns with some loose semantics: for example, the plural adjective form prenatales is similar to other familyrelated plural adjectives such as patrimoniales and generacionales.",3.1 MIMICK Polyglot Embeddings,[0],[0]
"Tamil displays some semantic similarities as well: e.g. enjineer (‘engineer’) predicts similarity to other professional terms such as kalviyiyal (‘education’), thozhilnutpa (‘technical’), and iraanuva (‘military’).
",3.1 MIMICK Polyglot Embeddings,[0],[0]
Stanford RareWords.,3.1 MIMICK Polyglot Embeddings,[0],[0]
"The Stanford RareWord evaluation corpus (Luong et al., 2013) focuses on predicting word similarity between pairs involving low-frequency English words, predominantly ones with common morphological affixes.",3.1 MIMICK Polyglot Embeddings,[0],[0]
"As these words are unlikely to be above the cutoff threshold for standard word embedding models, they emphasize the performance on OOV words.
",3.1 MIMICK Polyglot Embeddings,[0],[0]
"For evaluation of our MIMICK model on the RareWord corpus, we trained the Variational Embeddings algorithm (VarEmbed; Bhatia et al., 2016) on a 20-million-token, 100,000- type Wikipedia corpus, obtaining 128-dimension
word embeddings for all words in the test corpus.",3.1 MIMICK Polyglot Embeddings,[0],[0]
"VarEmbed estimates a prior distribution over word embeddings, conditional on the morphological composition.",3.1 MIMICK Polyglot Embeddings,[0],[0]
"For in-vocabulary words, a posterior is estimated from unlabeled data; for outof-vocabulary words, the expected embedding can be obtained from the prior alone.",3.1 MIMICK Polyglot Embeddings,[0],[0]
"In addition, we compare to FastText (Bojanowski et al., 2016), a high-vocabulary, high-dimensionality embedding benchmark.
",3.1 MIMICK Polyglot Embeddings,[0],[0]
"The results, shown in Table 3, demonstrate that the MIMICK RNN recovers about half of the loss in performance incurred by the original Polyglot training model due to out-of-vocabulary words in the “All pairs” condition.",3.1 MIMICK Polyglot Embeddings,[0],[0]
MIMICK also outperforms VarEmbed.,3.1 MIMICK Polyglot Embeddings,[0],[0]
"FastText can be considered an upper bound: with a vocabulary that is 25 times larger than the other models, it was missing words from only 44 pairs on this data.",3.1 MIMICK Polyglot Embeddings,[0],[0]
"The Universal Dependencies (UD) scheme (De Marneffe et al., 2014) features a minimal set of 17 POS tags (Petrov et al., 2012) and supports tagging further language-specific features using attribute-specific inventories.",4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
"For example, a verb in Turkish could be assigned a value for the evidentiality attribute, one which is absent from Danish.",4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
"These additional morphosyntactic attributes are marked in the UD dataset as optional per-token attribute-value pairs.
",4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
"Our approach for tagging morphosyntactic attributes is similar to the part-of-speech tagging model of Ling et al. (2015), who attach a projection layer to the output of a sentence-level bidirectional LSTM.",4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
We extend this approach to morphosyntactic tagging by duplicating this projection layer for each attribute type.,4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
"The input to our multilayer perceptron (MLP) projection network is the hidden state produced for each token in the sentence by an underlying LSTM, and the output is
attribute-specific probability distributions over the possible values for each attribute on each token in the sequence.",4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
"Formally, for a given attribute a with possible values v ∈ Va, the tagging probability for the i’th word in a sentence is given by:
Pr(awi = v) =",4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
"(Softmax(φ(hi)))v , (3)
with
(4)φ(hi) = OaW · tanh(Wah ·",4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
"hi + bah) + baW ,
where hi is the i’th hidden state in the underlying LSTM, and φ(hi) is a two-layer feedforward neural network, with weights Wah and O a W .",4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
We apply a softmax transformation to the output; the value at position v is then equal to the probability of attribute v applying to token wi.,4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
"The input to the underlying LSTM is a sequence of word embeddings, which are initialized to the Polyglot vectors when possible, and to MIMICK vectors when necessary.",4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
"Alternative initializations are considered in the evaluation, as described in Section 5.2.
",4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
Each tagged attribute sequence (including POS tags) produces a loss equal to the sum of negative log probabilities of the true tags.,4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
One way to combine these losses is to simply compute the sum loss.,4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
"However, many languages have large differences in sparsity across morpho-syntactic attributes, as apparent from Table 4 (rightmost column).",4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
"We therefore also compute a weighted sum
loss, in which each attribute is weighted by the proportion of training corpus tokens on which it is assigned a non-NONE value.",4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
"Preliminary experiments on development set data were inconclusive across languages and training set sizes, and so we kept the simpler sum loss objective for the remainder of our study.",4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
"In all cases, part-of-speech tagging was less accurate when learned jointly with morphosyntactic attributes.",4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
This may be because the attribute loss acts as POS-unrelated “noise” affecting the common LSTM layer and the word embeddings.,4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
The morphological complexity and compositionality of words varies greatly across languages.,5 Experimental Settings,[0],[0]
"While a morphologically-rich agglutinative language such as Hungarian contains words that carry many attributes as fully separable morphemes, a sentence in an analytic language such as Vietnamese may have not a single polymorphemic or inflected word in it.",5 Experimental Settings,[0],[0]
"To see whether this property is influential on our MIMICK model and its performance in the downstream tagging task, we select languages that comprise a sample of multiple morphological patterns.",5 Experimental Settings,[0],[0]
"Language family and script type are other potentially influential factors in an orthography-based approach such as ours, and so we vary along these parameters as well.",5 Experimental Settings,[0],[0]
"We also considered language selection recommendations from de Lhoneux and Nivre (2016) and Schluter and Agić (2017).
",5 Experimental Settings,[0],[0]
"As stated above, our approach is built on the Polyglot word embeddings.",5 Experimental Settings,[0],[0]
The intersection of the Polyglot embeddings and the UD dataset (version 1.4) yields 44 languages.,5 Experimental Settings,[0],[0]
"Of these, many are under-annotated for morphosyntactic attributes; we select twenty-three sufficiently-tagged languages, with the exception of Indonesian.3 Table 4 presents the selected languages and their typological properties.",5 Experimental Settings,[0],[0]
"As an additional proxy for mor-
3Vietnamese has no attributes by design; it is a pure analytic language.
",5 Experimental Settings,[0],[0]
"phological expressiveness, the rightmost column shows the proportion of UD tokens which are annotated with any morphosyntactic attribute.",5 Experimental Settings,[0],[0]
"As noted above, we use the UD datasets for testing our MIMICK algorithm on 23 languages4 with the supplied train/dev/test division.",5.1 Metrics,[0],[0]
"We measure partof-speech tagging by overall token-level accuracy.
",5.1 Metrics,[0],[0]
"For morphosyntactic attributes, there does not seem to be an agreed-upon metric for reporting performance.",5.1 Metrics,[0],[0]
Dzeroski et al. (2000) report pertag accuracies on a morphosyntactically tagged corpus of Slovene.,5.1 Metrics,[0],[0]
"Faruqui et al. (2016) report macro-averages of F1 scores of 11 languages from UD 1.1 for the various attributes (e.g., part-ofspeech, case, gender, tense); recall and precision were calculated for the full set of each attribute’s values, pooled together.5",5.1 Metrics,[0],[0]
Agić,5.1 Metrics,[0],[0]
"et al. (2013) report separately on parts-of-speech and morphosyntactic attribute accuracies in Serbian and Croatian, as well as precision, recall, and F1 scores per tag.",5.1 Metrics,[0],[0]
"Georgiev et al. (2012) report token-level accuracy for exact all-attribute tags (e.g. ‘Ncmsh’ for “Noun short masculine singular definite”) in Bulgarian, reaching a tagset of size 680.",5.1 Metrics,[0],[0]
Müller et al. (2013) do the same for six other languages.,5.1 Metrics,[0],[0]
"We report micro F1: each token’s value for each attribute is compared separately with the gold labeling, where a correct prediction is a matching non-NONE attribute/value assignment.",5.1 Metrics,[0],[0]
"Recall and
4When several datasets are available for a language, we use the unmarked corpus.
5Details were clarified in personal communication with the authors.
precision are calculated over the entire set, with F1 defined as their harmonic mean.",5.1 Metrics,[0],[0]
"We implement and test the following models:
No-Char.",5.2 Models,[0],[0]
"Word embeddings are initialized from Polyglot models, with unseen words assigned the Polyglot-supplied UNK vector.",5.2 Models,[0],[0]
"Following tuning experiments on all languages with cased script, we found it beneficial to first back off to the lowercased form for an OOV word if its embedding exists, and only otherwise assign UNK.
MIMICK.",5.2 Models,[0],[0]
"Word embeddings are initialized from Polyglot, with OOV embeddings inferred from a MIMICK model (Section 3) trained on the Polyglot embeddings.",5.2 Models,[0],[0]
"Unlike the No-Char case, backing off to lowercased embeddings before using the MIMICK output did not yield conclusive benefits and thus we report results for the more straightforward no-backoff implementation.
CHAR→TAG.",5.2 Models,[0],[0]
"Word embeddings are initialized from Polyglot as in the No-Char model (with lowercase backoff), and appended with the output of a character-level LSTM updated during training (Plank et al., 2016).",5.2 Models,[0],[0]
"This additional module causes a threefold increase in training time.
",5.2 Models,[0],[0]
Both.,5.2 Models,[0],[0]
"Word embeddings are initialized as in MIMICK, and appended with the CHAR→TAG LSTM.
",5.2 Models,[0],[0]
Other models.,5.2 Models,[0],[0]
"Several non-Polyglot embedding models were examined, all performed substantially worse than Polyglot.",5.2 Models,[0],[0]
"Two of these
are notable: a random-initialization baseline, and a model initialized from FastText embeddings (tested on English).",5.2 Models,[0],[0]
"FastText supplies 300-dimension embeddings for 2.51 million lowercase-only forms, and no UNK vector.6 Both of these embedding models were attempted with and without CHAR→TAG concatenation.",5.2 Models,[0],[0]
"Another model, initialized from only MIMICK output embeddings, performed well only on the language with smallest Polyglot training corpus (Latvian).",5.2 Models,[0],[0]
"A Polyglot model where OOVs were initialized using an averaged embedding of all Polyglot vectors, rather than the supplied UNK vector, performed worse than our No-Char baseline on a great majority of the languages.
",5.2 Models,[0],[0]
"Last, we do not employ type-based tagset restrictions.",5.2 Models,[0],[0]
All tag inventories are computed from the training sets and each tag selection is performed over the full set.,5.2 Models,[0],[0]
"Based on development set experiments, we set the following hyperparameters for all models on all languages: two LSTM layers of hidden size 128, MLP hidden layers of size equal to the number of each attribute’s possible values; momentum stochastic gradient descent with 0.01 learning rate; 40 training epochs (80 for 5K settings) with a dropout rate of 0.5.",5.3 Hyperparameters,[0],[0]
The CHAR→TAG models use 20-dimension character embeddings and a single hidden layer of size 128.,5.3 Hyperparameters,[0],[0]
We report performance in both low-resource and full-resource settings.,6 Results,[0],[0]
"Low-resource training sets were obtained by randomly sampling training sentences, without replacement, until a predefined token limit was reached.",6 Results,[0],[0]
We report the results on the full sets and on N = 5000 tokens in Table 5 (partof-speech tagging accuracy) and Table 6 (morphosyntactic attribute tagging micro-F1).,6 Results,[0],[0]
"Results for additional training set sizes are shown in Figure 2; space constraints prevent us from showing figures for all languages.
MIMICK as OOV initialization.",6 Results,[0],[0]
"In nearly all experimental settings on both tasks, across languages and training corpus sizes, the MIMICK embeddings significantly improve over the Polyglot UNK embedding for OOV tokens on both
6Vocabulary type-level coverage for the English UD corpus: 55.6% case-sensitive, 87.9% case-insensitive.
",6 Results,[0],[0]
POS and morphosyntactic tagging.,6 Results,[0],[0]
"For POS, the largest margins are in the Slavic languages (Russian, Czech, Bulgarian), where word order is relatively free and thus rich word representations are imperative.",6 Results,[0],[0]
"Chinese also exhibits impressive improvement across all settings, perhaps due to the large character inventory (> 12,000), for which a model such as MIMICK can learn well-informed embeddings using the large Polyglot vocabulary dataset, overcoming both word- and characterlevel sparsity in the UD",6 Results,[0],[0]
"corpus.7 In morphosyntactic tagging, gains are apparent for Slavic languages and Chinese, but also for agglutinative languages — especially Tamil and Turkish — where the stable morpheme representation makes it easy for subword modeling to provide a type-level signal.8 To examine the effects on Slavic and agglutinative languages in a more fine-grained view, we present results of multiple training-set size experiments for each model, averaged over five repetitions (with different corpus samples), in Figure 2.
MIMICK vs. CHAR→TAG.",6 Results,[0],[0]
"In several languages, the MIMICK algorithm fares better than the CHAR→TAG model on part-of-speech tagging in low-resource settings.",6 Results,[0],[0]
"Table 7 presents the POS tagging improvements that MIMICK achieves over the pre-trained Polyglot models, with and without CHAR→TAG concatenation, with 10,000 tokens of training data.",6 Results,[0],[0]
"We obtain statistically significant improvements in most languages, even when CHAR→TAG is included.",6 Results,[0],[0]
"These improvements are particularly substantial for test-set tokens outside the UD training set, as shown in the right two columns.",6 Results,[0],[0]
"While test set OOVs are a strength of the CHAR→TAG model (Plank et al., 2016), in many languages there are still considerable improvements to be obtained from the application of MIMICK initialization.",6 Results,[0],[0]
"This suggests that with limited training data, the end-to-end CHAR→TAG model is unable to learn a sufficiently accurate representational mapping from orthography.",6 Results,[0],[0]
"We present a straightforward algorithm to infer OOV word embedding vectors from pre-trained,
7Character coverage in Chinese Polyglot is surprisingly good: only eight characters from the UD dataset are unseen in Polyglot, across more than 10,000 unseen word types.
",7 Conclusion,[0],[0]
8Persian is officially classified as agglutinative but it is mostly so with respect to derivations.,7 Conclusion,[0],[0]
"Its word-level inflections are rare and usually fusional.
limited-vocabulary models, without need to access the originating corpus.",7 Conclusion,[0],[0]
"This method is particularly useful for low-resource languages and tasks with little labeled data available, and in fact is task-agnostic.",7 Conclusion,[0],[0]
"Our method improves performance over word-based models on annotated sequence-tagging tasks for a large variety of languages across dimensions of family, orthography, and morphology.",7 Conclusion,[0],[0]
"In addition, we present a BiLSTM approach for tagging morphosyntactic attributes at the token level.",7 Conclusion,[0],[0]
"In this paper, the MIMICK model was trained using characters as input, but future work may consider the use of other subword units, such as morphemes, phonemes, or even bitmap representations of ideographic characters (Costa-jussà et al., 2017).",7 Conclusion,[0],[0]
"We thank Umashanthi Pavalanathan, Sandeep Soni, Roi Reichart, and our anonymous reviewers for their valuable input.",8 Acknowledgments,[0],[0]
We thank Manaal Faruqui and Ryan McDonald for their help in understanding the metrics for morphosyntactic tagging.,8 Acknowledgments,[0],[0]
The project was supported by project HDTRA1-15-10019 from the Defense Threat Reduction Agency.,8 Acknowledgments,[0],[0]
"Word embeddings improve generalization over lexical features by placing each word in a lower-dimensional space, using distributional information obtained from unlabeled data.",abstractText,[0],[0]
"However, the effectiveness of word embeddings for downstream NLP tasks is limited by out-of-vocabulary (OOV) words, for which embeddings do not exist.",abstractText,[0],[0]
"In this paper, we present MIMICK, an approach to generating OOV word embeddings compositionally, by learning a function from spellings to distributional embeddings.",abstractText,[0],[0]
"Unlike prior work, MIMICK does not require re-training on the original word embedding corpus; instead, learning is performed at the type level.",abstractText,[0],[0]
Intrinsic and extrinsic evaluations demonstrate the power of this simple approach.,abstractText,[0],[0]
"On 23 languages, MIMICK improves performance over a word-based baseline for tagging part-of-speech and morphosyntactic attributes.",abstractText,[0],[0]
It is competitive with (and complementary to) a supervised characterbased model in low-resource settings.,abstractText,[0],[0]
Mimicking Word Embeddings using Subword RNNs,title,[0],[0]
The sheer number and variety of online social networks (OSN) today is staggering.,1. Introduction,[0],[0]
"Although the purpose and the shaping of these networks vary generously, the majority of them has one aspect in common: the value of most OSNs is in its user data and the information that one can infer from the data.",1. Introduction,[0],[0]
"This, unfortunately, results in a big incentive for culprits to intrude OSNs and manipulate their data.",1. Introduction,[0],[0]
"One popular method of intruding and attacking an OSN is referred to as Sybil attack, where the intruder creates a whole bunch of fake (Sybil) accounts that are all under the attacker’s control.",1. Introduction,[0],[0]
"The intruder’s influence over the OSN
1MathPlan, 10587 Berlin, Germany 2Machine Learning Group, Berlin Institute of Technology, 10587 Berlin, Germany 3Berlin Big Data Center 4Max Planck Society 5Korea University.",1. Introduction,[0],[0]
Correspondence to: János,1. Introduction,[0],[0]
Höner <,1. Introduction,[0],[0]
"janos.hoener@campus.tuberlin.de>, Nico Görnitz <nico.goernitz@tu-berlin.de>, KlausRobert Müller <klaus-robert.mueller@tu-berlin.de>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
is multiplied by the number of accounts created, which opens possibilities of manipulation typically for gaining some monetary advantage in the end.
",1. Introduction,[0],[0]
"The term, Sybil attack, was coined by Douceur (2002) who showed that this kind of attack will be always possible unless a trusted agency certifies identities.",1. Introduction,[0],[0]
"Unfortunately, this approach is orthogonal to how OSNs grow.",1. Introduction,[0],[0]
The threshold of registration must be as low as possible to attract as many new users as possible.,1. Introduction,[0],[0]
"On the other hand, Sybil attacks can damage the value of OSNs significantly, which has been proved by the fact that Facebook shares dropped in 2012 after the company revealed that a significant share of its network is made up by Sybil accounts (The Associated Press, 2012).
",1. Introduction,[0],[0]
"There exists a number of “classic” feature-based solutions (Stein et al., 2011; Cao et al., 2012; Stringhini et al., 2010; Yang et al., 2014).",1. Introduction,[0],[0]
"However, up until now, it remains an unsolved problem as those methods can be evaded by cleverly designed attacking schemes (Bilge et al., 2009; Boshmaf et al., 2011; Wagner et al., 2012; Lowd & Meek, 2005) and manual detection is too expensive, time consuming, and simply unfeasible in large OSNs (Cao et al., 2012).
",1. Introduction,[0],[0]
More recent graph-based Sybil detection methods assume that honest (non-Sybil) nodes form a strongly connected subgraph and attackers can establish a limited amount of edges which leads to a sparse cut between the honest subraph and the Sybil nodes.,1. Introduction,[0],[0]
"The majority of the graph-based methods define trusted nodes, which the defender is sure to be honest, and use random walks (Yu et al., 2010; Danezis, 2009; Cao et al., 2012) or other typical graph-based algorithms like breadth-first-search (Tran et al., 2011) and belief propagation (Gong et al., 2014) to convey trust from the trusted nodes.",1. Introduction,[0],[0]
A node is identified as Sybil if sufficient ammount of trust is not delivered to it.,1. Introduction,[0],[0]
"Among random-walk based approaches, SybilRank is known to be the state-ofthe-art, of which the performance is theoretically guaranteed (Cao et al., 2012).",1. Introduction,[0],[0]
"However, the theory holds only under unrealistic topological assumptions of the network.",1. Introduction,[0],[0]
"In this paper, we show that the same theoretical guarantee can be obtained under more realistic situations.
",1. Introduction,[0],[0]
We further dicuss the robustness of the random walk approach against adversarial strategies.,1. Introduction,[0],[0]
"To this end, we formally introduce adversarial settings for graph-based Sybil
detection and derive an optimal attacking strategy that is based on the exploitation of trust leaks.",1. Introduction,[0],[0]
"Based on our analysis, we propose a transductive Sybil ranking (TSR), an integrated approach capable of adjusting edge weights based on sampled trust leaks.",1. Introduction,[0],[0]
We empirically show good performance of TSR against the state-of-the-art baselines on a variety of attacking scenarios using artificially generated data as well as real-world Facebook data.,1. Introduction,[0],[0]
"We are given a graph G = (V,E) consisting of nodes V and pairwise edges E between nodes.",2. Preliminaries,[0],[0]
"We denote GS = (VS , ES) the Sybil sub-graph, GH = (VH , EH) the disjunct honest sub-graph, and VT ✓ VH our trusted (verified nonSybil nodes) random walk seed nodes.",2. Preliminaries,[0],[0]
EA is the set of edges connecting any node in GS and any node in GH .,2. Preliminaries,[0],[0]
"Sybil Rank is considered the state-of-the-art graph-based method to detect Sybil accounts as it outperformed all its contestants (Cao et al., 2012).",2. Preliminaries,[0],[0]
It is also based on random walks and operates solely on the topology of the graph.,2. Preliminaries,[0],[0]
"Sybil Rank starts from the initial distribution {p(i)0 2 [0, 1]}|V |i=1",2. Preliminaries,[0],[0]
"(without superscript refers to a vector containing all elements), in which “trust” is assigned to the known honest nodes VT :
p (i) 0",2. Preliminaries,[0],[0]
"=
( 1
|VT",2. Preliminaries,[0],[0]
"| if vi 2 VT , 0 otherwise.
(1)
Then, it “propagates” the trust via a short (k steps) random walk:
p > k",2. Preliminaries,[0],[0]
= p >,2. Preliminaries,[0],[0]
k,2. Preliminaries,[0],[0]
"1Q = · · · = p>0 Qk , (2)
where Q 2 R|V |⇥|V",2. Preliminaries,[0],[0]
"| is the transition matrix through the edges with Qi,j = ( P j0 1[(i, j 0 ) 2 E]) 1, if (i, j) 2 E, and else 0.",2. Preliminaries,[0],[0]
"It is known that the stationary distribution ⇡ ⌘ p1 is the normalized degree distribution (Behrends, 2000)
⇡ > = ⇣ deg(v1) Vol(V ) , . . .",2. Preliminaries,[0],[0]
", deg(v|V |) Vol(V ) ⌘ , (3)
where deg(v) is the degree of node v, i.e., the number of all incident edges of v, and Vol(V ) = P v2V deg(v) is the sum of the degrees for all nodes in V .",2. Preliminaries,[0],[0]
"SybilRank conpensates the effect of degrees, and use the degree-normalized probability
p (i) = p",2. Preliminaries,[0],[0]
"(i) k /⇡ (i) (4)
as the ranking score, where a high ranking indicates a high probability of being an honest node.
",2. Preliminaries,[0],[0]
"Essentially, SybilRank relies on the assumption that the total number of attacking edges is bounded.",2. Preliminaries,[0],[0]
"Under this assumption, only a small fraction of the trust is propagated
through the sparse cut between the honest network and the Sybil nodes during the short random walk, while ”trust” go through the ”non-trusted” honest nodes through the dense connections within the the honest subgraph.
",2. Preliminaries,[0],[0]
Boshmaf et al. (2016) developed Integro to cope with a larger number of attacking edges.,2. Preliminaries,[0],[0]
"To this end, Integro introduces weights on the edges to bias the random walk, where the weights are determined after its pre-processing step to detect victims.",2. Preliminaries,[0],[0]
Here a victim is defined as a node that established a connection to one of the Sybil nodes.,2. Preliminaries,[0],[0]
"The set of all victim nodes is defined by Vv = {v 2 Vh : 9(v, s) 2 EA}.",2. Preliminaries,[0],[0]
"After the detection step, Integro lowers the weights to all incident edges to the detected victims, which prevents the trust to propagate through victim nodes.",2. Preliminaries,[0],[0]
"As the victims form a natural border between the honest and the Sybil graph, this reduces the overall flow of trust into the Sybil graph.",2. Preliminaries,[0],[0]
Boshmaf et al. found that traditional feature-based classification methods yield good and robust detection of victims.,2. Preliminaries,[0],[0]
"A notable advantage against the feature-based Sybil detection is that, unlike Sybils, victims generally do not behave adversarial, as they don’t have any incentive to ”hide”.",2. Preliminaries,[0],[0]
"More Realistic Assumptions
Cao et al. (2012) gave a security guarantee for SybilRank.",3. SybilRank’s Security Guarantee Under,[0],[0]
Let g := |EA| be the number of attacking edges and n := |V | be the number of all nodes in the graph.,3. SybilRank’s Security Guarantee Under,[0],[0]
their theory relies on the notion of trust leaks.,3. SybilRank’s Security Guarantee Under,[0],[0]
Definition 1.,3. SybilRank’s Security Guarantee Under,[0],[0]
(Trust leaks) Let rk0 = P i2VH p (i) k0 be the trust that remains in the honest graph after k0 random walk steps.,3. SybilRank’s Security Guarantee Under,[0],[0]
We call l = Pk k0=1(rk0+1 rk0) the absolute trust leak.,3. SybilRank’s Security Guarantee Under,[0],[0]
"Assume that the attacking edges are created randomly, following a distribution ↵(EA).",3. SybilRank’s Security Guarantee Under,[0],[0]
We call CH(k 0 ),3. SybilRank’s Security Guarantee Under,[0],[0]
"= E↵(EA)[ rk0+1 rk0 rk0
] the expected relative trust leak.
",3. SybilRank’s Security Guarantee Under,[0],[0]
CH(k 0 ) is actually a constant with respect to k0 under reasonable assumptions on ↵(EA).,3. SybilRank’s Security Guarantee Under,[0],[0]
The following lemma has been proved: Lemma 1.,3. SybilRank’s Security Guarantee Under,[0],[0]
"(Cao et al., 2012) Assume that the graph G is created randomly, following the configuration model (Molloy & Reed, 1995).",3. SybilRank’s Security Guarantee Under,[0],[0]
"Then, the expected relative trust leak in each iteration is given by CH = gvol(VH) .
",3. SybilRank’s Security Guarantee Under,[0],[0]
This leads to a theoretical guarantee of SybilRank.,3. SybilRank’s Security Guarantee Under,[0],[0]
Theorem 1.,3. SybilRank’s Security Guarantee Under,[0],[0]
"(Cao et al., 2012) Assume that the graph G is created randomly, following the configuration model.",3. SybilRank’s Security Guarantee Under,[0],[0]
The total number of Sybils that are ranked higher than nonSybils by SybilRank is O(g log n).,3. SybilRank’s Security Guarantee Under,[0],[0]
"Theorem 1 implies good performance of SybilRank, but
it holds under the assumption that the attacking edges are created in the same process as the honest graph,1 which is not realistic.
",3. SybilRank’s Security Guarantee Under,[0],[0]
"Below, we show that the same guarantee is obtained under the following more realistic assumption: Assumption 1.",3. SybilRank’s Security Guarantee Under,[0],[0]
"The graph G is constructed by the following steps:
1.",3. SybilRank’s Security Guarantee Under,[0],[0]
"Honest graph GH construction: GH is connected, nonbipartite, and scale free, i.e., the degree distribution follows the power law distribution.
",3. SybilRank’s Security Guarantee Under,[0],[0]
2.,3. SybilRank’s Security Guarantee Under,[0],[0]
"Sybil graph GS construction: The topology of GS is arbitrary.
",3. SybilRank’s Security Guarantee Under,[0],[0]
3.,3. SybilRank’s Security Guarantee Under,[0],[0]
Attacking edges EA generation:,3. SybilRank’s Security Guarantee Under,[0],[0]
"The attacking edges are genarated on all possible edges EA ⇢ VS ⇥ VH between the honest and the Sybil subgraphs with equal propability.
",3. SybilRank’s Security Guarantee Under,[0],[0]
"Under Assumption 1, evaluating the expected trust leak is less straightforward.",3. SybilRank’s Security Guarantee Under,[0],[0]
"Nevertheless, we can show that it results in the same formal security guarantee stated in Theorem 1.
",3. SybilRank’s Security Guarantee Under,[0],[0]
"To properly compute the expected trust leak, the following random variables are defined.",3. SybilRank’s Security Guarantee Under,[0],[0]
"Xv counts the number of attacking edges incident to node v, Yv = ⇡(v)
",3. SybilRank’s Security Guarantee Under,[0],[0]
"Xv deg(v,GH)+Xv = ⇡(v) Xv deg(v,G) is the trust leak in node
v and Z = P
",3. SybilRank’s Security Guarantee Under,[0],[0]
v2VH Yv is the total trust leak.,3. SybilRank’s Security Guarantee Under,[0],[0]
Note that here ⇡(v) is the current amount of trust in node v and not the stationary distribution of the random walk.,3. SybilRank’s Security Guarantee Under,[0],[0]
This notation is used to avoid confusion with the probability mass function denoted by P .,3. SybilRank’s Security Guarantee Under,[0],[0]
"From Assumption 1 it follows that Xv is hypergeometrically distributed (Tuckwell, 1995) with the following parameters: the population size: N = |VH⇥VS |, successes: K = |{v}⇥ VS |, and the draws n = |EA|.",3. SybilRank’s Security Guarantee Under,[0],[0]
Let g := |EA| be the number of attacking edges.,3. SybilRank’s Security Guarantee Under,[0],[0]
"Moreover, let nH := |VH",3. SybilRank’s Security Guarantee Under,[0],[0]
| and nS,3. SybilRank’s Security Guarantee Under,[0],[0]
":= |VS | denote the number of honest nodes and Sybil nodes, respectively.
",3. SybilRank’s Security Guarantee Under,[0],[0]
"The probability mass function of Xv is given by P (Xv =
k) =
✓ K
k ◆✓ N K n k ◆ / ✓ N n",3. SybilRank’s Security Guarantee Under,[0],[0]
◆,3. SybilRank’s Security Guarantee Under,[0],[0]
"and according to Tuckwell
(1995), its expected value can be computed by E[Xv] = n
K N = |EA| |{v}⇥VS ||VH⇥VS | = |EA| |VH",3. SybilRank’s Security Guarantee Under,[0],[0]
"| = g nH
.",3. SybilRank’s Security Guarantee Under,[0],[0]
"The final goal is to compute the expected value of Z. The linearity of the expected value yields E[Z] =
P v2VH E[Yv] and for the
expected value of Yv we get
E[Yv] = ⇡(v)deg(v,G) P1 k=0 kP (Xv = k)
= ⇡(v) deg(v,G)E[Xv] = ⇡(v) deg(v,G) g nH .
",3. SybilRank’s Security Guarantee Under,[0],[0]
"1This assumption is not explicitly stated in Cao et al. (2012), but apparent from their derivation.
",3. SybilRank’s Security Guarantee Under,[0],[0]
"Using this result, the expected value of Z becomes E[Z]",3. SybilRank’s Security Guarantee Under,[0],[0]
"=P v2VH E[Yv] = g nH P v2VH ⇡(v) deg(v,G) , where the right hand side still contains a sum that needs to be evaluated individually for each node to compute its actual value.",3. SybilRank’s Security Guarantee Under,[0],[0]
"In order to “average out” this sum, we rely on the assumption that the honest nodes GH is power law-distributed (Barabási, 2009).",3. SybilRank’s Security Guarantee Under,[0],[0]
"To do this, a new random variable Dv is introduced which measures the degree of v. Then, the assumption results in the probability of a node having a degree of d being P (Dv = d) =",3. SybilRank’s Security Guarantee Under,[0],[0]
"d
⇣( ) , where ⇣ is the Riemann zeta function ⇣(s) := P1 n=0 n s (Barabási, 2009).
",3. SybilRank’s Security Guarantee Under,[0],[0]
"With this expression, it is possible to “average out” the exact topology of the graph by computing the expected value with respect to the newly defined random variable Dv:
E[Z] =",3. SybilRank’s Security Guarantee Under,[0],[0]
gnH P1 d=1 P v2VH ⇡(v) d P,3. SybilRank’s Security Guarantee Under,[0],[0]
(,3. SybilRank’s Security Guarantee Under,[0],[0]
"Dv = d)
= g nH P v2VH ⇡(v) P1 d=1 1 d d ⇣( )
= g nH P v2VH ⇡(v) ⇣( ) P1 d=1 d ( +1)
",3. SybilRank’s Security Guarantee Under,[0],[0]
= g nH ⇣( +1) ⇣,3. SybilRank’s Security Guarantee Under,[0],[0]
"( ) P v2VH ⇡(v).| {z }
Total trust in the honest graph
This yields the following lemma.",3. SybilRank’s Security Guarantee Under,[0],[0]
Lemma 2.,3. SybilRank’s Security Guarantee Under,[0],[0]
"Under Assumption 1 the expected relative trust leak in each iteration of the random walk is given by
˜ CH = g
nH ⇣( +1) ⇣( )| {z } =:e
where e < 1 is a constant that depends on the parameter of the assumed power law distribution for the degree distribution.
",3. SybilRank’s Security Guarantee Under,[0],[0]
"Although Lemma 2 gives a different expected relative trust leak from Lemma 1, the fact that the maximum number of connection for each node is bounded in every OSN and therefore O(nH) = O(vol(VH)) leads to the same asymptotic behavior as Theorem 2: Theorem 2.",3. SybilRank’s Security Guarantee Under,[0],[0]
"Under Assuption 1, the total number of Sybils that are ranked higher than non-Sybils by SybilRank is O(g log n).",3. SybilRank’s Security Guarantee Under,[0],[0]
"This result explicitly shows that, asymptotically, SybilRank’s security guarantee remains unchanged even under more realistic Assumption 1.",3. SybilRank’s Security Guarantee Under,[0],[0]
"In this section, we discuss adversarial strategies against graph-based Sybil detection methods.
",4. Adversarial Strategies,[0],[0]
"Attacker’s Action Although attackers in general can take variety of actions, we restricts their action to adding attacking edges.
",4. Adversarial Strategies,[0],[0]
Definition 2 (Attacking strategy).,4. Adversarial Strategies,[0],[0]
"Given an honest graph GH and a Sybil graph GS , an attacking strategy describes the set of attacking edges established by the intruder.
",4. Adversarial Strategies,[0],[0]
"The cost of action is measured by the number of attacking edges.
",4. Adversarial Strategies,[0],[0]
"Attacker’s Knowledge Generally, we focus on adversarial attacks against random walk based approaches.",4. Adversarial Strategies,[0],[0]
"That is, an attacker’s strategy for establishing edges from Sybil nodes to honest nodes in order to cloak an attacker’s Sybil sub-network.",4. Adversarial Strategies,[0],[0]
"For analysis, we assume different levels of knowledge that the attacker has on the defender’s strategy and information:
A.1 Strategy only.
",4. Adversarial Strategies,[0],[0]
"A.2 Strategy + topology.
",4. Adversarial Strategies,[0],[0]
"A.3 Strategy + topology + trusted nodes (positively labeled nodes).
",4. Adversarial Strategies,[0],[0]
"B.1 Strategy + topology + trusted nodes (positively labeled nodes) + untrusted nodes (negatively labeled nodes).
",4. Adversarial Strategies,[0],[0]
"Here, we divided the level of access to inside information for the attacker into two groups.",4. Adversarial Strategies,[0],[0]
"In group A (i.e., A.1, A.2, A.3) attackers are able to gather sophisticated information based on publicly available sources, whereas in group B (i.e., B.1) either some back-channel provides non-public information (e.g. defender marked Sybil nodes based on their analysis), or, the attackers are provided with all information visible to the defenders.
",4. Adversarial Strategies,[0],[0]
"Clearly, it is too hard, if not impossible, to have an out-ofthe-box solution for the setting described in group B and we therefore resort our analysis on the settings in group A.",4. Adversarial Strategies,[0],[0]
"In the first case (A.1), no efficient adversarial strategies for graph-based random walk approaches is possible.",4. Adversarial Strategies,[0],[0]
The attackers must build up sufficient attacking edges to trusted nodes in order to absorb enough trust.,4. Adversarial Strategies,[0],[0]
"In A.3 (and A.2 as a special case) on the other hand, the attacker gained enough information to guide the creation of attacking edges efficiently.",4. Adversarial Strategies,[0],[0]
This paper focuses on this most interesting situation.,4. Adversarial Strategies,[0],[0]
"More specifically, we assume the following: the intruder knows defender’s strategy (algorithm details), the topology of the honest graph, and the set of trusted nodes, i.e., she knows about GH = (VH , EH) and VT .",4. Adversarial Strategies,[0],[0]
Based on that knowledge the intruder can establish attacking edges to honest nodes of her choice with the goal to create an attacking scenario where the applied defense method fails.,4. Adversarial Strategies,[0],[0]
"Although the exact topology of the Sybil graph is not specified any further, for the following results it is assumed that it is designed in a way that suits the intruder well.
",4. Adversarial Strategies,[0],[0]
"Attacker’s Goal Attackers can have various final goals, e.g., spamming honest users to earn money, feeding wrong information to honest nodes, stealing nonpublic information, damaging countries/companies, etc.",4. Adversarial Strategies,[0],[0]
"Depending on the goal, the objective of the optimal strategy can differ.",4. Adversarial Strategies,[0],[0]
"We assume that attacker’s try to maximize their influence and hence, have an inherent need to increase the number of attacking edges.
",4. Adversarial Strategies,[0],[0]
"Random-walk based approaches such as SybilRank and Integro rely on the fact that the absolute trust leak l from the honest graph to the Sybil graph is small (i.e., below the amount needed to reach the stationary distribution within the Sybil sub-graph) which ensures low trust scores for the Sybil nodes.",4. Adversarial Strategies,[0],[0]
"However, if enough trust is being propagated to the Sybil graph, trust values will be close to the stationary distribution in the Sybil graph as well as in the honest graph.",4. Adversarial Strategies,[0],[0]
"Consequently, the degree-normalized ranking values will be similar to the ones in the honest graph, which makes Sybil nodes indistinguishable from honest nodes and therefore disables the detector.",4. Adversarial Strategies,[0],[0]
Definition 3 (Disabling Attacking Strategy).,4. Adversarial Strategies,[0],[0]
Let GH and GS be the honest graph and the Sybil graph.,4. Adversarial Strategies,[0],[0]
Let l : 2E !,4. Adversarial Strategies,[0],[0]
R be the absolute trust leak as a function of an attacking strategy.,4. Adversarial Strategies,[0],[0]
"Then, an attacking strategy EA ⇢ VH ⇥",4. Adversarial Strategies,[0],[0]
"VS is said to be disabling if
l(EA)",4. Adversarial Strategies,[0],[0]
"td, (5) where td is the disabling threshold, which depends on the topology of the Sybil graph and the detection algorithm.
",4. Adversarial Strategies,[0],[0]
"Surely, an attacker does not aim for just any disabling strategy but for the one that comes at the lowest cost.",4. Adversarial Strategies,[0],[0]
"As the cost of an attacking strategy is assumed to be increasing in the number of attacking edges, an optimal/minimal disabling strategy is given by the following definition.",4. Adversarial Strategies,[0],[0]
Definition 4 (Optimal Disabling Strategy).,4. Adversarial Strategies,[0],[0]
"An attacking strategy AE is said to be optimal if it is the solution to the following optimization problem:
min EA⇢VH⇥VS |EA| (6)
s. t. l(EA) td.
To solve this, the disabling threshold td and the trust leak function must be known to the attacker.",4. Adversarial Strategies,[0],[0]
"Ignoring the edge weights (which are unknown to the attacker) the amount of trust needed within the Sybil graph to reach the stationary distribution of the random walk is given by td =P
vi2VS ⇡i = vol(VS) vol(V ) .",4. Adversarial Strategies,[0],[0]
To exactly evaluate l(EA) the entire random walk needs to be simulated which is infeasible for the attacker without knowing its exact length and the edge weights.,4. Adversarial Strategies,[0],[0]
A useful estimate is to consider only the first iteration.,4. Adversarial Strategies,[0],[0]
"The computation of this value is feasible and the trust leak per attacking edge is by far the
largest in the first iteration because all the trust is concentrated in the relatively small subset of trusted nodes VT .",4. Adversarial Strategies,[0],[0]
"The trust leak in the first iteration ˜l(EA) is given by l(EA) = P v2VT (v) deg(v,GH)+(v)
, where (v) is the attacking degree (i.e., the number of attacking edges) of node v.",4. Adversarial Strategies,[0],[0]
This leads to a greedy strategy where the intruder iteratively adds those attacking edges which produce the largest increase in ˜l.,4. Adversarial Strategies,[0],[0]
In the following the term adversarial strategy/attacker refers to this greedy strategy.,4. Adversarial Strategies,[0],[0]
"In this section, we propose our new method and derive its efficient solver.",5. Proposed Method,[0],[0]
"Our method is specifically designed to cope with a large number of attacking edges by minimizing “trust leaks”, that is, minimizing a sampled trust leak by adjusting the edge weights—a missing mechanism for SybilRank and Integro.
",5. Proposed Method,[0],[0]
"Transductive Sybil Ranking Combining the approach of Backstrom & Leskovec (2011) and SybilRank (Cao et al., 2012), our proposed method, called transductive Sybil ranking (TSR), tries to leverage potential prior knowledge, negative labels, to bias a short random walk so that random walk methods work even with the existence of a large number of attacking edges.
",5. Proposed Method,[0],[0]
Assume that all nodes carry attributes and n  |V,5. Proposed Method,[0],[0]
"| nodes are additionally attached with label information, i.e., the defender knows a subset of nodes are honest, and another subset of nodes are sybil.",5. Proposed Method,[0],[0]
"More formally, the defender is given labeled nodes L := {(xi, yi) 2 X ⇥",5. Proposed Method,[0],[0]
"{+1, 1}}ni=1 and unlabeled nodes U := {xi 2 X}|V |i=n+1.",5. Proposed Method,[0],[0]
"Since only the honest nodes can be trusted, VT ✓ {vi 2 V ; yi = +1} holds.
",5. Proposed Method,[0],[0]
"We define an edge feature function u,v between nodes u and v as u,v : X ⇥ X !",5. Proposed Method,[0],[0]
Y .,5. Proposed Method,[0],[0]
"A corresponding parameterized, non-negative scoring function fw : Y !",5. Proposed Method,[0],[0]
"R+ is learned during training and applied as edge weight au,v = fw( u,v) in the weighted adjacency matrix Q 2 R|V |⇥|V",5. Proposed Method,[0],[0]
"|:
Qu,v =
( au,vP x au,x
if (u, v) 2 E, 0 otherwise.
(7)
",5. Proposed Method,[0],[0]
"Throughout our experiments, we restrict ourselves to the following differentiable edge feature function:
fw( u,v) =",5. Proposed Method,[0],[0]
"(1 exp( w> u,v))",5. Proposed Method,[0],[0]
1.,5. Proposed Method,[0],[0]
"(8) Once the transition matrix is fixed, The remaining procedure is the same as SybilRank.",5. Proposed Method,[0],[0]
"Namely, starting form the initial distribution (1), k-steps random walk (2) is applied with the transition matrix (7).",5. Proposed Method,[0],[0]
"After that, the degreenormalized ranking probability (4) is used for classification.",5. Proposed Method,[0],[0]
"However, we are also given negatively labeled nodes,
which are used to train the parameter w of the edge feature function (8), so that p(i) < p(j), 8 i, j 2 {1, . . .",5. Proposed Method,[0],[0]
", n} with yi = 1 and yj = +1.",5. Proposed Method,[0],[0]
"In the spirit of regularized risk minimization (Vapnik, 1999), this problem is formalized as follows:
Definition 5 (TSR optimization problem).",5. Proposed Method,[0],[0]
"TSR solves a quadratically regularized, non-convex optimization problem with generic loss-functions h :",5. Proposed Method,[0],[0]
"[0, 1]⇥{+1, 1} !",5. Proposed Method,[0],[0]
"R:
minimize w F (w) =
2
kwk2 + nX
i=1
h(p (i) (w), yi) .",5. Proposed Method,[0],[0]
"(9)
Using the notion of p(i)(w) visually indicates that node ranking probabilities p are (non-linearly) dependent on the parameter vector w.",5. Proposed Method,[0],[0]
"As for the choice of loss-functions, we examine the following:
• Wilcoxon-Mann-Whitney (WMW) loss (Yan et al., 2003).",5. Proposed Method,[0],[0]
"WMW maximizes the area under the ROC curve:
h(p, y) =
nX
j=1
1[y = +1^yj = 1] ⇣ 1 + exp p pjb ⌘ 1 .
",5. Proposed Method,[0],[0]
"• Smooth hinge-loss variant A smooth variant of the classical support vector machine hinge-loss with two additional parameters: a decision boundary b 2 R and a scaling parameter a 2 R:
h(p, y) =
8 ><
>: 1 2 y(ap b) if y(ap b)  0, 1 2 (1 y(ap b))2 if 0 < y(ap b)  1, 0 if 1 < y(ap b).
",5. Proposed Method,[0],[0]
"In this work, we focus on smooth, differentiable lossfunctions only, ensuring fast convergence to local optima via gradient-based methods, i.e., fast second-order methods (BFGS).",5. Proposed Method,[0],[0]
"A pivotal point is hence, to assess the gradient w.r.t.",5. Proposed Method,[0],[0]
"w.
Gradient Computation The remaining of this section is dedicated to the derivation of the gradient:
",5. Proposed Method,[0],[0]
"@F (w) @w = @ kwk2 @2w + Pn i @h(p(i)(w),yi) @w ,
where the loss-function h can be further split into @h(p(i)(w),yi)
@w = @h(p(i)(w),yi) @p(i)(w) @p(i)(w) @w .",5. Proposed Method,[0],[0]
"Since we con-
strained ourselves to differentiable loss-function h(p, y), the partial derivative w.r.t.",5. Proposed Method,[0],[0]
p can be calculated rather straightforward.,5. Proposed Method,[0],[0]
"More complicated is the evaluation of
@p(i)
",5. Proposed Method,[0],[0]
@w = @ @w p(i)k,5. Proposed Method,[0],[0]
⇡(i) =,5. Proposed Method,[0],[0]
✓ @p(i)k @w ⇡ (i) p(i)k @⇡ (i) @w ◆ ⇡ (i) 2 .,5. Proposed Method,[0],[0]
"(10)
The derivative of the i-th component of ⇡ is given by:
@⇡(i)
@w =
⇣ @deg⇤(vi) @w vol(V ) @vol(V )@w deg⇤(vi) ⌘ vol(V ) 2,
where @deg ⇤(vi) @w = P e2E vi2e @ae @w = P e2E vi2e @fw( e) @w and @vol(V )",5. Proposed Method,[0],[0]
@w = 2 P e2E @ae @w = 2 P e2E @fw( e) @w .,5. Proposed Method,[0],[0]
As fw is said to be differentiable the only part of Eq.,5. Proposed Method,[0],[0]
"(10) that remains is the Jacobian @pk/@w.
Theorem 3.",5. Proposed Method,[0],[0]
"The derivative @pk/@w for k 1 is given by:
@pk",5. Proposed Method,[0],[0]
@w = ✓ k 1P l=0 plQ k 1 l ◆ @Q @w .,5. Proposed Method,[0],[0]
"(11)
(the proof is given in Appendix A).",5. Proposed Method,[0],[0]
"The derivative of Q, defined in Eq.",5. Proposed Method,[0],[0]
"(7), is given by
@Quv @w =
8 <
:
@auv",5. Proposed Method,[0],[0]
@w P x aux auv P x @aux,5. Proposed Method,[0],[0]
"@w
( P x aux) 2",5. Proposed Method,[0],[0]
"if (u, v) 2 E, 0 otherwise.
",5. Proposed Method,[0],[0]
"This completes the computation of the gradient and enables the application of gradient-based methods, i.e., BFGS to find a (locally) optimal estimate ŵ.",5. Proposed Method,[0],[0]
"By using this estimate, TSR weights the whole graph, with which a short random walk is performed to obtain the final ranking p.
Robustness of TSR against Attacks By using the negative label information, our TSR, in principle, monitors “trust leak” through random walk, and adjusts the edge weights so that the leak is minimized.",5. Proposed Method,[0],[0]
"As a result, the weights tend to be lower on the attacking edges (to reduce the propagation), and higher on the Sybil edges (to boost the stationary distribution).",5. Proposed Method,[0],[0]
"Thus, we can expect that our TSR, which is an advanced integrated method, is more robust against attacks than the SybilRank and the two-step Integro approach.",5. Proposed Method,[0],[0]
"To assess the robustness of the proposed method and the baseline methods, we generate artificially network topology and edge and node attributes in order to have full control of the underlying ground truth.",6. Empirical Evaluation on Synthetic Data,[0],[0]
"We separately create two graphs, the honest and the Sybil graph.",6. Empirical Evaluation on Synthetic Data,[0],[0]
Both use the generation method proposed by Holme & Kim (2002) for scale free networks.,6. Empirical Evaluation on Synthetic Data,[0],[0]
Node features are generated randomly and correlated through dependency injection.,6. Empirical Evaluation on Synthetic Data,[0],[0]
"The edge features function u,v simply stacks node features of the two adjacent nodes xu",6. Empirical Evaluation on Synthetic Data,[0],[0]
and,6. Empirical Evaluation on Synthetic Data,[0],[0]
xv (see Appendix B for more details).,6. Empirical Evaluation on Synthetic Data,[0],[0]
"Connections between Sybil and honest graphs are established according to a random attacking strategy that iteratively adds attacking edges randomly, i.e., equally distributed on the set of all possible attacking edges VH ⇥",6. Empirical Evaluation on Synthetic Data,[0],[0]
"VS
or a adversarial attacking strategy that solves Problem (6) for optimal attacks.",6. Empirical Evaluation on Synthetic Data,[0],[0]
This strategy only chooses an honest node to be attacked next and the corresponding Sybil node is chosen randomly (equally distributed on the set of all Sybil nodes VS).,6. Empirical Evaluation on Synthetic Data,[0],[0]
"We test our method, TSR, using the proposed loss functions and compare against the stateof-the-art methods SybilRank and Integro.",6. Empirical Evaluation on Synthetic Data,[0],[0]
"As Integro depends on a preceding victim prediction, we simulated one that achieves highest possible rankings (ROC-AUC close to 1.0).2
Random Attacking Strategy We generated a sample network (|VH | = 200 and |VS | = 30) and select 15 honest nodes and 8 Sybil nodes randomly, which will be used as labeled examples for our TSR.",6. Empirical Evaluation on Synthetic Data,[0],[0]
The labeled honest nodes are also used as the set VT of trusted seeding nodes for the random walks in all methods.,6. Empirical Evaluation on Synthetic Data,[0],[0]
We evaluate the performance in terms of ROC-AUC-values for the computed ranking.,6. Empirical Evaluation on Synthetic Data,[0],[0]
This procedure was repeated 20 times for varying number of attacking edges (10-200 edges).,6. Empirical Evaluation on Synthetic Data,[0],[0]
Figure 1 shows ROC-AUC curves for all methods under the random attacking setting.,6. Empirical Evaluation on Synthetic Data,[0],[0]
"We can obsreve that our TSR, regardless of the choice of loss function, performs superior to the other methods.",6. Empirical Evaluation on Synthetic Data,[0],[0]
Integro’s accuracy deteriorates fast but still has an edge over SybilRank up to the point where the ROCAUC-value reaches 0.5.,6. Empirical Evaluation on Synthetic Data,[0],[0]
"After that SybilRank and Integro essentially perform similar.
",6. Empirical Evaluation on Synthetic Data,[0],[0]
"Adversarial Attacking Strategy For the adversarial setting, we ran the same benchmarks but this time attacking edges were added according to the adversarial attacking strategy.",6. Empirical Evaluation on Synthetic Data,[0],[0]
"Due to the much more aggressive setting, we varied the number of attacking edges from 1-40 and repeated this procedure 20 times to report averaged ROCAUC accuracies.",6. Empirical Evaluation on Synthetic Data,[0],[0]
The results are depicted in Figure 2.,6. Empirical Evaluation on Synthetic Data,[0],[0]
All choices of loss functions outperform SybilRank and Integro clearly.,6. Empirical Evaluation on Synthetic Data,[0],[0]
The results confirm our considerations that SybilRank’s performance drops fast and steep as soon as a certain amount of attacking edges is established.,6. Empirical Evaluation on Synthetic Data,[0],[0]
"Integro behaves more robust than SybilRank, but, ultimately, must resign after a few more attacking edges.",6. Empirical Evaluation on Synthetic Data,[0],[0]
"Again, our TSR is significantly more robust against adversarial attacks and can withstand higher number of attacking edges until its performance finally deteriorates.",6. Empirical Evaluation on Synthetic Data,[0],[0]
We also evaluated our method on a sample of the Facebook graph Leskovec & Mcauley collected from survey participants using the Facebook app.,7. Empirical Evaluation on Real-world Data,[0],[0]
"The dataset includes the topology (|V | = 4039 users and |E| = 88234 friend-
2SybiRank, Integro, and TSR rely on different information, and therefore, the fairness of comparison is not trivial.",7. Empirical Evaluation on Real-world Data,[0],[0]
"We discuss this issue in Appendix C.
ships) as well as node features for every node (see Table 1 for summary), Figure 3).",7. Empirical Evaluation on Real-world Data,[0],[0]
"Node features are comprised of obfuscated categorical features of users profiles including education, work, hometown, language, last name, etc.",7. Empirical Evaluation on Real-world Data,[0],[0]
"As with most of real world social graphs, the data exhibits strong multi-cluster structure, as seen in Figure 3 and Figure 4.",7. Empirical Evaluation on Real-world Data,[0],[0]
"These clusters pose additional challenges to the application of random walk-based methods as the trust propagation between two loosely inter-connected clusters is low (Cao et al., 2012; Boshmaf et al., 2016).",7. Empirical Evaluation on Real-world Data,[0],[0]
"Hence, trust seeds should be distributed among all clusters.",7. Empirical Evaluation on Real-world Data,[0],[0]
"Following SybilRank and Integro (Cao et al., 2012), we employ the Louvian clustering method (Blondel et al., 2008) first.
",7. Empirical Evaluation on Real-world Data,[0],[0]
"As common, the Sybil graph needs to be generated.",7. Empirical Evaluation on Real-world Data,[0],[0]
"For this purpose, a (small) subgraph was copied and declared as Sybil.",7. Empirical Evaluation on Real-world Data,[0],[0]
The attacking edges were created to link the honest and the Sybil graph following one of the attacking strategies (random or adversarial).,7. Empirical Evaluation on Real-world Data,[0],[0]
It was made sure that no Sybil node attacked one of the direct neighbors of its origin which is reasonable for most social graphs.,7. Empirical Evaluation on Real-world Data,[0],[0]
"Edge features
for TSR are as follows: the number of shared features (in total), the number of shared friends, and the number of shared features within specific categories.",7. Empirical Evaluation on Real-world Data,[0],[0]
"The other experimental setup is the same as the previous section.
",7. Empirical Evaluation on Real-world Data,[0],[0]
Random Attacks,7. Empirical Evaluation on Real-world Data,[0],[0]
The trusted nodes |VT,7. Empirical Evaluation on Real-world Data,[0],[0]
| = 50 were randomly distributed among all clusters and a small subset of Sybils |VD| = 30 was chosen as known Sybil nodes.,7. Empirical Evaluation on Real-world Data,[0],[0]
Attacking edges EA were established following the random attacking strategy ranging from |EA| = 1 to |EA| = 1400.,7. Empirical Evaluation on Real-world Data,[0],[0]
Experiments were repeated 10 times.,7. Empirical Evaluation on Real-world Data,[0],[0]
"Integro was run with
sarial attacking scenario on the Facebook graph.
",7. Empirical Evaluation on Real-world Data,[0],[0]
"two levels of accuracy in simulated victim detection, i.e., perfect (AUROC = 1) and almost perfect (AUROC = 0.9).",7. Empirical Evaluation on Real-world Data,[0],[0]
Figure 5 shows the AUROC-values.,7. Empirical Evaluation on Real-world Data,[0],[0]
The detection performance of SybilRank is the lowest and drops soon as attacking edges increase.,7. Empirical Evaluation on Real-world Data,[0],[0]
"Integro with the perfect victim detection outperforms the other methods, but with just a slight reduction in the victim detection accuracy (AUROC = 0.9), its performance drops significantly.",7. Empirical Evaluation on Real-world Data,[0],[0]
All versions of TSR perform almost on par with perfect version of Integro in the lower range of attacking edges (1—800).,7. Empirical Evaluation on Real-world Data,[0],[0]
"In the higher range (800—1400), the hinge loss drop fast to end up with a performance similar to Integro with the almost perfect victim detection.",7. Empirical Evaluation on Real-world Data,[0],[0]
"However, the variant that uses the WMW-loss does not show this performance drop and stays close to the upper-bound of Integro.
",7. Empirical Evaluation on Real-world Data,[0],[0]
Adversarial Attacks The number of adversarial attack edges ranged from |EA| = 1 to |EA| = 45.,7. Empirical Evaluation on Real-world Data,[0],[0]
"Figure 6 shows
the recorded average AUROC-values.",7. Empirical Evaluation on Real-world Data,[0],[0]
"Again, SybilRank’s performance drops the fastest and steepest and Integro is insignificantly better in this adversarial scenario.",7. Empirical Evaluation on Real-world Data,[0],[0]
Both variants of TSR performs better than the baselines.,7. Empirical Evaluation on Real-world Data,[0],[0]
"However, the WMW-loss variant performs only slightly better than SybilRank and Integro, while the hinge-loss variant keeps good performance even for a large number of attacking edges.",7. Empirical Evaluation on Real-world Data,[0],[0]
"As our future work, we will investigate which loss function should be chosen, depending on data and assumed attacker’s strategy.",7. Empirical Evaluation on Real-world Data,[0],[0]
"Overall, whereas SybilRank’s and Integro’s performance drops to an average AUROC-value below 0.5 at |EA| = 30, the hinge-loss variant of TSR still achieves an average value over 0.9 at the same amount of attacking edges.",7. Empirical Evaluation on Real-world Data,[0],[0]
"In this paper, we studied the problem of Sybil detection.",8. Conclusion & Outlook,[0],[0]
We first refined the security guarantees of random walk approaches towards more realistic assumptions.,8. Conclusion & Outlook,[0],[0]
"Then, we formalized and coined the adversarial setting and introduced optimal strategies for attackers.",8. Conclusion & Outlook,[0],[0]
"Further, we proposed a new method, transductive Sybil ranking (TSR), that leverages prior information, network topology as well as node and edge features.",8. Conclusion & Outlook,[0],[0]
"Unlike Integro, it is fused in a single optimization framework and can be solved efficiently by using gradient-based optimizer.",8. Conclusion & Outlook,[0],[0]
"In our empirical evaluation, we showed the advantages of our method and investigated the susceptibility of our method and baseline competitors to adversarial attacks.",8. Conclusion & Outlook,[0],[0]
"Further research will focus on the application of our method to real-world, large-scale OSNs.",8. Conclusion & Outlook,[0],[0]
"JH was supported by MathPlan GmbH and innoCampus, TU-Berlin.",9. Acknowledgments,[0],[0]
"SN, AB and KRM were supported by the German Ministry for Education and Research as Berlin Big Data Center BBDC, funding mark 01IS14013A. KRM thanks for the Institute for Information & Communications Technology Promotion (IITP) grant funded by the Korea government (No.2017-0-00451).",9. Acknowledgments,[0],[0]
NG was supported by BMBF ALICE II grant 01IB15001B.,9. Acknowledgments,[0],[0]
Sybil detection is a crucial task to protect online social networks (OSNs) against intruders who try to manipulate automatic services provided by OSNs to their customers.,abstractText,[0],[0]
"In this paper, we first discuss the robustness of graph-based Sybil detectors SybilRank and Integro and refine theoretically their security guarantees towards more realistic assumptions.",abstractText,[0],[0]
"After that, we formally introduce adversarial settings for the graph-based Sybil detection problem and derive a corresponding optimal attacking strategy by exploitation of trust leaks.",abstractText,[0],[0]
"Based on our analysis, we propose transductive Sybil ranking (TSR), a robust extension to SybilRank and Integro that directly minimizes trust leaks.",abstractText,[0],[0]
Our empirical evaluation shows significant advantages of TSR over stateof-the-art competitors on a variety of attacking scenarios on artificially generated data and realworld datasets.,abstractText,[0],[0]
Minimizing Trust Leaks for Robust Sybil Detection,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1379–1388, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics
Deep inference on a large-scale knowledge base (KB) needs a mass of formulas, but it is almost impossible to create all formulas manually. Data-driven methods have been proposed to mine formulas from KBs automatically, where random sampling and approximate calculation are common techniques to handle big data. Among a series of methods, Random Walk is believed to be suitable for knowledge graph data. However, a pure random walk without goals still has a poor efficiency of mining useful formulas, and even introduces lots of noise which may mislead inference. Although several heuristic rules have been proposed to direct random walks, they do not work well due to the diversity of formulas. To this end, we propose a novel goaldirected inference formula mining algorithm, which directs random walks by the specific inference target at each step. The algorithm is more inclined to visit benefic structures to infer the target, so it can increase efficiency of random walks and avoid noise simultaneously. The experiments on both WordNet and Freebase prove that our approach is has a high efficiency and performs best on the task.",text,[0],[0]
"Recently, various knowledge bases (KBs), such as Freebase (Bollacker et al., 2008), WordNet (Miller, 1995), Yago (Hoffart et al., 2013), have been built, and researchers begin to explore how to make use of structural information to promote performances of several inference-based NLP applications, such as
text entailment, knowledge base completion, question and answering.",1 Introduction,[0],[0]
"Creating useful formulas is one of the most important steps in inference, and an accurate and high coverage formula set will bring a great promotion for an inference system.",1 Introduction,[0],[0]
"For example, Nationality(x, y) ∧ Nationality(z, y) ∧ Language(z, w)⇒ Language(x, w) is a high-quality formula, which means people with the same nationality probably speak the same language.",1 Introduction,[0],[0]
"However, it is a challenge to create formulas for open-domain KBs, where there are a great variety of relation types and it is impossible to construct a complete formula set by hand.
",1 Introduction,[0],[0]
"Several data-driven methods, such as Inductive Logic Programming (ILP) (Muggleton and De Raedt, 1994) and Markov Logic Network (MLN) (Richardson and Domingos, 2006), have been proposed to mine formulas automatically from KB data, which transform frequent sub-structures of KBs, e.g., paths or loops, into formulas.",1 Introduction,[0],[0]
"Figure 1.a shows a sub-graph extracted from Freebase, and the formula mentioned above about Language can be generated from the loop in Figure 1.d.",1 Introduction,[0],[0]
"However, the running time of these traditional probabilistic inference methods is unbearable over large-scale KBs.",1 Introduction,[0],[0]
"For example, MLN needs grounding for each candidate formula, i.e., it needs to enumerate all paths.",1 Introduction,[0],[0]
"Therefore, the computation complexity of MLN increases exponentially with the scale of a KB.
",1 Introduction,[0],[0]
"In order to handle large-scale KBs, the random walk is usually employed to replace enumerating all possible sub-structures.",1 Introduction,[0],[0]
"However, random walk is inefficient to find useful structures due to its completely randomized mechanism.",1 Introduction,[0],[0]
"For example in Fig-
1379
ure 1.b, the target path (yellow one) has a small probability to be visited, the reason is that the algorithm may select all the neighboring entity to transfer with an equal probability.",1 Introduction,[0],[0]
"This phenomenon is very common in KBs, e.g., each entity in Freebase has more than 30 neighbors in average, so there will be about 810,000 paths with length 4, and only several are useful.",1 Introduction,[0],[0]
"There have been two types of methods proposed to improve the efficiency of random walks, but they still meet serious problems, respectively.",1 Introduction,[0],[0]
1),1 Introduction,[0],[0]
Increasing rounds of random walks.,1 Introduction,[0],[0]
"More rounds of random walks will find more structures, but it will simultaneously introduce more noise and thus generate more false formulas.",1 Introduction,[0],[0]
"For example, the loop in Figure 1.c exists in Freebase, but it produces a false formula, Gender(x, y) ∧ Gender(z, y) ∧ Language(z, w)⇒ Language(x, w), which means people with the same gender speak the same language.",1 Introduction,[0],[0]
"This kind of structures frequently occur in KBs even the formulas are mined with a high confidence, because there are a lot of sparse structures in KBs which will lead to inaccurate confidence.",1 Introduction,[0],[0]
"According to our statistics, more than 90 percent of high-confidence formulas produced by random walk are noise.",1 Introduction,[0],[0]
2),1 Introduction,[0],[0]
Employing heuristic rules to direct random walks.,1 Introduction,[0],[0]
"This method directs random walks to find useful structures by rewriting the state transition probability matrix, but the artificial heuristic rules may only apply to a little part of formulas.",1 Introduction,[0],[0]
"For example, PRA (Lao and Cohen, 2010; Lao et al., 2011) assumes the more narrow distributions of elements in a formula are, the higher score the formula will obtain.",1 Introduction,[0],[0]
"However, formulas with high scores in PRA are not always true.",1 Introduction,[0],[0]
"For example, the formula in Figure 1.c has a high score in PRA, but it is not true.",1 Introduction,[0],[0]
"Oppositely, formulas with low scores in PRA are not always useless.",1 Introduction,[0],[0]
"For example, the formula, Father(x, y) ∧ Father(y, z) ⇒ Grandfather(x, t), has a low score when x and y both have several sons, but it obviously is the most effective to infer Grandfather.",1 Introduction,[0],[0]
"According to our investigations, the situations are common in KBs.",1 Introduction,[0],[0]
"In this paper, we propose a Goal-directed Random Walk algorithm to resolve the above problems.",1 Introduction,[0],[0]
The algorithm employs the specific inference target as the direction at each step in the random walk process.,1 Introduction,[0],[0]
"In more detail, to achieve such a goaldirected mechanism, at each step of random walk, the algorithm dynamically estimates the potentials for each neighbor by using the ultimate goal, and assigns higher probabilities to the neighbors with higher potentials.",1 Introduction,[0],[0]
"Therefore, the algorithm is more inclined to visit structures which are beneficial to infer
the target and avoid transferring to noise structures.",1 Introduction,[0],[0]
"For example in Figure 1, when the inference target is what language a person speaks, the algorithm is more inclined to walk along Nationality edge than Gender, because Nationality has greater potential than Gender to infer Language.",1 Introduction,[0],[0]
We build a real potential function based on low-rank distributional representations.,1 Introduction,[0],[0]
The reason of replacing symbols by distributional representations is that the distributional representations have less parameters and latent semantic relationship in them can contribute to estimate potentials more precisely.,1 Introduction,[0],[0]
"In summary, the contributions of this paper are as follows.",1 Introduction,[0],[0]
•,1 Introduction,[0],[0]
"Compared with the basic random walk, our approach direct random walks by the inference target, which increases efficiency of mining useful formulas and has a great capability of resisting noise.",1 Introduction,[0],[0]
•,1 Introduction,[0],[0]
"Compared with the heuristic methods, our approach can learn the strategy of random walk automatically and dynamically adjust the strategy for different inference targets, while the heuristic methods need to write heuristic rules by hand and follow the same rule all the time.",1 Introduction,[0],[0]
"• The experiments on link prediction task prove that our approach has a high efficiency on mining formulas and has a good performance on both WN18 and FB15K datasets.
",1 Introduction,[0],[0]
"The rest of this paper is structured as follows, Section 2 introduces the basic random walk for mining formulas.",1 Introduction,[0],[0]
Section 3 describes our approach in detail.,1 Introduction,[0],[0]
The experimental results and related discussions are shown in Section 4.,1 Introduction,[0],[0]
"Section 5 introduces some related works, and finally, Section 6 concludes this paper.",1 Introduction,[0],[0]
"Mining frequent patterns from source data is a problem that has a long history, and for different specific tasks, there are different types of source data and different definitions of pattern.",2.1 Frequent Pattern Mining,[0],[0]
"Mining formulas is more like frequent subgraph mining, which employs paths or loops as frequent patterns and mines them from a KB.",2.1 Frequent Pattern Mining,[0],[0]
"For each relation type R, the algorithm enumerates paths from entity H to entity T for each triplet R(H,T ).",2.1 Frequent Pattern Mining,[0],[0]
These paths are normalized to formulas by replacing entities to variables.,2.1 Frequent Pattern Mining,[0],[0]
"For example, the loop in Figure 1.d, National-
ity(Bob, America)",2.1 Frequent Pattern Mining,[0],[0]
"∧ Nationality(Stewart, America)",2.1 Frequent Pattern Mining,[0],[0]
"∧ Language(Bob, English) ⇒",2.1 Frequent Pattern Mining,[0],[0]
"Language(Stewart, English), can be normalized to the formula, Nationality(x, y) ∧ Nationality(z, y) ∧ Language(z, w) ⇒",2.1 Frequent Pattern Mining,[0],[0]
"Language(x, w).",2.1 Frequent Pattern Mining,[0],[0]
"Support and confidence are employed to estimate a formula, where the support value of a formula f : X ⇒ Y , noted as Sf , is defined as the proportion of paths in the KB which contains the body X , and the confidence value of X ⇒ Y , noted as Cf , is defined as the proportion of the paths that contains X which also meets X ⇒ Y .",2.1 Frequent Pattern Mining,[0],[0]
"Cf is calculated as follows,
Cf = Nf NX
(1)
",2.1 Frequent Pattern Mining,[0],[0]
whereNf is the total number of instantiated formula f and NX is the total number of instantiated X .,2.1 Frequent Pattern Mining,[0],[0]
Enumerating paths is a time consuming process and does not apply to large-scale KBs.,2.2 Random Walk on Knowledge Graph,[0],[0]
"Therefore, random walk on the graph is proposed to collect frequent paths instead of enumerating.",2.2 Random Walk on Knowledge Graph,[0],[0]
Random walk randomly chooses a neighbor to jump unlike enumerating which needs to search all neighbors.,2.2 Random Walk on Knowledge Graph,[0],[0]
"To estimate a formula f , the algorithm employs f ’s occurrence number during random walks N
′ f to approxi-
mate the total number Nf in Equation (1), and similarly employs N
′ X to approximate NX .",2.2 Random Walk on Knowledge Graph,[0],[0]
"Therefore,
f ’s confidence Cf can be approximatively estimated by N
′ f and N ′ X , noted as C ′ f .
",2.2 Random Walk on Knowledge Graph,[0],[0]
"Random walk maintains a state transition probability matrix P , and Pij means the probability of jumping from entity i to entity j. To make the confidence C
′",2.2 Random Walk on Knowledge Graph,[0],[0]
"f as close to the true confidence Cf as pos-
sible, the algorithm sets P as follows,
Pij = { 1/di, j ∈ Adj(i) 0, j /∈ Adj(i)",2.2 Random Walk on Knowledge Graph,[0],[0]
"(2)
where di is the out-degree of the entity i, Adj(i) is the set of adjacent entities of i, and ∑N j=1 Pij = 1.",2.2 Random Walk on Knowledge Graph,[0],[0]
Such a transition matrix means the algorithm may jump to all the neighboring entities with an equal probability.,2.2 Random Walk on Knowledge Graph,[0],[0]
"Such a random walk is independent from the inference target, so we call this type of random walk as a goalless random walk.",2.2 Random Walk on Knowledge Graph,[0],[0]
The goalless mechanism causes the inefficiency of mining useful structures.,2.2 Random Walk on Knowledge Graph,[0],[0]
"When we want to mine paths for R(H,T ), the algorithm cannot arrive at T from H
in the majority of rounds.",2.2 Random Walk on Knowledge Graph,[0],[0]
"Even though the algorithm recalls several paths for R(H,T ), some of them may generate noisy formulas for inferring R(H,T ).
",2.2 Random Walk on Knowledge Graph,[0],[0]
"To solve the above problem, several methods direct random walks by statically modifying P .",2.2 Random Walk on Knowledge Graph,[0],[0]
"For example, PRA sets Prij = P (j|i;r) |Ri| , P (j|i; r) = r(i,j) r(i,∗) , where P (j|i; r) is the probability of reaching node j from node i under the specific relation r, r(i, ∗) is the number of edges from i under r, and Ri is the number of relation types from i. Such a transition matrix implies the more narrow distributions of elements in a formula are, the higher score the formula will obtain, which can be viewed as the heuristic rule of PRA.",2.2 Random Walk on Knowledge Graph,[0],[0]
"We propose to use the inference target, ρ = R(H,T ), to direct random walks.",3.1 Goal-Directed Random Walk,[0],[0]
"When predicting ρ, our approach always directs random walks to find useful structures which may generate formulas to infer ρ.",3.1 Goal-Directed Random Walk,[0],[0]
"For different ρ, random walks are directed by modifying the transition matrix P in different ways.",3.1 Goal-Directed Random Walk,[0],[0]
"Our approach dynamically calculates Prij when jumping from entity i to entity j under relation r as follows,
",3.1 Goal-Directed Random Walk,[0],[0]
"Prij =    Φ(r(i, j), ρ)∑ k∈Adj(i) Φ(r(i, k), ρ) , j ∈ Adj(i)
0, j /∈ Adj(i)",3.1 Goal-Directed Random Walk,[0],[0]
"(3)
where Φ(r(i, j), ρ) is the r(i, j)’s potential which measures the potential contribution to infer ρ after walking to j.
Intuitively, if r(i, j) exits in a path from H to T and this path can generate a benefic formula to infer R(H,T ), the probability of jumping from i to j should larger and thus Φ(r(i, j), ρ) also should be larger.",3.1 Goal-Directed Random Walk,[0],[0]
"Reversely, if we cannot arrive at T within the maximal steps after jumping to j, or if the path produces a noisy formula leading to a wrong inference, Pij and Φ(r(i, j), ρ) should both be smaller.
",3.1 Goal-Directed Random Walk,[0],[0]
"To explicitly build a bridge between the potential Φ and the inference goal ρ, we maximize the likelihood of paths which can infer ρ.",3.1 Goal-Directed Random Walk,[0],[0]
"First, we recursively define the likelihood of a path from H to t
as PpHt = PpHs ·",3.1 Goal-Directed Random Walk,[0],[0]
"Prst , where Prst is defined in Equation (3).",3.1 Goal-Directed Random Walk,[0],[0]
"We then classify a path pHt into three separate categories: a) t = T and pHt can produce a benefic formula to infer R(H,T ); b) t 6=",3.1 Goal-Directed Random Walk,[0],[0]
T ; c) t = T but pHt may generate a noisy formula which misleads inference.,3.1 Goal-Directed Random Walk,[0],[0]
"Finally, we define the likelihood function as follows,
maxPP = ∏
pHt∈P P apHt(1− PpHt) b+c (4)
where P is all paths found in the process of performing random walks for R(H,T ), and t may be equal to T or not.",3.1 Goal-Directed Random Walk,[0],[0]
"a, b, c are three 0-1 variables corresponding to the above categories a), b), c).",3.1 Goal-Directed Random Walk,[0],[0]
"Only one in a, b, c can be 1 when PHt belongs to the corresponding category.",3.1 Goal-Directed Random Walk,[0],[0]
We then transform maximizing PP to minimizing Lrw = − logPP and employ SGD to train it.,3.1 Goal-Directed Random Walk,[0],[0]
"In practice, there is not a clear-cut boundary between a) and c), so we divide the loss into two parts:",3.1 Goal-Directed Random Walk,[0],[0]
Lrw = Ltrw + λL inf rw .,3.1 Goal-Directed Random Walk,[0],[0]
Ltrw is the loss of that t 6=,3.1 Goal-Directed Random Walk,[0],[0]
"T , and Linfrw is the loss of that pHT generates a noisy formula leading to a wrong inference.",3.1 Goal-Directed Random Walk,[0],[0]
λ is a super parameter to balance the two losses.,3.1 Goal-Directed Random Walk,[0],[0]
Ltrw and Linfrw have the same expression but are optimized in different stages.,3.1 Goal-Directed Random Walk,[0],[0]
"Ltrw can be optimized during random walks, while Linfrw should be optimized in the inference stage.",3.1 Goal-Directed Random Walk,[0],[0]
"We rewrite Lrw for a specific path p as follows,
Lrw(p) = −y logPp − (1− y) log (1− Pp) (5)
where y is the label of the path p and y = 1 if p is beneficial to infer ρ.",3.1 Goal-Directed Random Walk,[0],[0]
"To obtain the best Φ, we compute gradients of Lrw as follows,
∇Lrw(p) =",3.1 Goal-Directed Random Walk,[0],[0]
"(∇Lrw(r12),∇Lrw(r23), ...)
∇Lrw(rij)",3.1 Goal-Directed Random Walk,[0],[0]
"= ( ∂Lrw(rij) ∂Φrij , ∂Lrw(rij) ∂Φrik1 , ∂Lrw(rij) ∂Φrik2 , ...)
∂Lrw(rij)
",3.1 Goal-Directed Random Walk,[0],[0]
∂Φrij =,3.1 Goal-Directed Random Walk,[0],[0]
(Pp − y) · (1− Prij ),3.1 Goal-Directed Random Walk,[0],[0]
"Φrij · (1− Pp)
∂Lrw(rij) ∂Φrik",3.1 Goal-Directed Random Walk,[0],[0]
=,3.1 Goal-Directed Random Walk,[0],[0]
− (Pp − y) ·,3.1 Goal-Directed Random Walk,[0],[0]
"Prij
Φrij · (1− Pp) (6)
where ∇Lrw(rij) is the component of ∇Lrw(p) at rij .",3.1 Goal-Directed Random Walk,[0],[0]
"Φ(r(i,",3.1 Goal-Directed Random Walk,[0],[0]
"j), ρ) and Φ(r(i, k), ρ) are the potentials for all triplets r(i, j) ∈ p and r(i, k) /∈",3.1 Goal-Directed Random Walk,[0],[0]
"p, and rij is short for r(i, j).",3.1 Goal-Directed Random Walk,[0],[0]
"After iteratively updating Φrij and Φrik by the gradient of L t rw, the random walks can
be directed to find more paths fromH to T , and consequently it increases efficiency of the random walk.",3.1 Goal-Directed Random Walk,[0],[0]
"After updating Φrij and Φrik by the gradient ofL inf rw , random walk is more likely to find high-quality paths but not noise.",3.1 Goal-Directed Random Walk,[0],[0]
"Therefore, the goal-directed random walk increases efficiency of mining benefic formulas and has a great capability of resisting noise.",3.1 Goal-Directed Random Walk,[0],[0]
"The potential Φ(r(i, j), ρ) measures an implicit relationship between two triplets in the KB, so the total number of parameters is the square of the KB size.",3.2 Distributional Potential Function,[0],[0]
It is hard to precisely estimate all Φ because of the sparsity of training data.,3.2 Distributional Potential Function,[0],[0]
"To reduce the number of parameters, we represent each entity or relation in the KB as a low-rank numeric vector which is called embeddings (Bordes et al., 2013), and then we build a potential function Ψ on embeddings as Φ(r(i, j), ρ) = Ψ(Er(i,j), ER(H,T )), where Er(i,j) and ER(H,T ) are the embeddings of triplets.",3.2 Distributional Potential Function,[0],[0]
"In practice, we set Er(i,j) =",3.2 Distributional Potential Function,[0],[0]
"[Er, Ej ] and ER(H,T ) =",3.2 Distributional Potential Function,[0],[0]
"[ER, ET ] because Ei is the same for all triplets r(i, ∗), where [] is a concatenation operator.
",3.2 Distributional Potential Function,[0],[0]
"In the view of the neural network, our goaldirected mechanism is analogous to the attention mechanism.",3.2 Distributional Potential Function,[0],[0]
"At each step, the algorithm estimates attentions for each neighboring edges by Ψ. Therefore, there are several existing expressions of Ψ, e.g., the dot product (Sukhbaatar et al., 2015) and the single-layer perceptron (Bahdanau et al., 2015).",3.2 Distributional Potential Function,[0],[0]
"We will not compare different forms of Ψ, the detail comparison has been presented in the work (Luong et al., 2015).",3.2 Distributional Potential Function,[0],[0]
"We directly employ the simplest dot product for Ψ as follows,
Ψ(Er(i,j), ER(H,T ))",3.2 Distributional Potential Function,[0],[0]
"= σ(Er(i,j) · ER(H,T )) (7)
where σ is a nonlinear function and we set it as an exponential function.",3.2 Distributional Potential Function,[0],[0]
Ψ has no parameters beside KB embeddings which are updated during the training period.,3.2 Distributional Potential Function,[0],[0]
"To handle the dependence between goal-directed random walk and subsequent inference, we combine them into an integrated model and optimize them together.",3.3 Integrated Inference Model,[0],[0]
"To predict ρ = R(H,T ), the integrated model first collects formulas for R(H,T ), and then
Algorithm 1:",3.3 Integrated Inference Model,[0],[0]
"Train Integrated Inference Model
Input: KB, Ξ Output: Ψ, W , F 1: For ρ = R(H,T ) ∈ Ξ 2: Repeat ρ-directed Random Walk from H to t 3: Update Ψ by Ltrw 4: If t = T , then F = F ∩ fp 5: Calculate Linf and L inf rw by ρ 6: Update W by Linf 7: Update Ψ by Linfrw 8: Remove f ∈ F with little wf 9: Output Ψ, W , F
merges estimations of different formulas as features into a score function χ,
χ(ρ) =",3.3 Integrated Inference Model,[0],[0]
"∑
f∈Fρ δ(f) (8)
where Fρ is the formula set obtained by random walks for ρ, and δ(f) is an estimation of formula f .",3.3 Integrated Inference Model,[0],[0]
"The original frequent pattern mining algorithm employs formulas’ confidence as δ(f) directly, but it does not produce good results (Galárraga et al., 2013).",3.3 Integrated Inference Model,[0],[0]
"There are two ways to solve the problem: one is selecting another more suitable measure of f as δ(f) (Tan et al., 2002); the other is attaching a weight to each formula and learning weights with supervision, e.g., MLN (Richardson and Domingos, 2006) .",3.3 Integrated Inference Model,[0],[0]
We employ the latter method and set δ(f) =,3.3 Integrated Inference Model,[0],[0]
wf ·nf .,3.3 Integrated Inference Model,[0],[0]
"Finally, we employ a logistic regression classifier to predict R(H,T ), and the posterior probability of R(H,T ) is shown as follows,
P (ρ = y|χ) =",3.3 Integrated Inference Model,[0],[0]
"F(χ)y(1−F(χ))1−y
F(χ) = 1 1 + e−χ
(9)
where y is a 0-1 label of ρ.",3.3 Integrated Inference Model,[0],[0]
"Similar to Ltrw in Equation (5), we treat the negative logarithm of P (ρ = y|χ) as the loss of inference, Linf = − logP (ρ = y|χ), and turn to minimize it.",3.3 Integrated Inference Model,[0],[0]
"Moreover, the loss Linfrw of the above goal-directed random walk is influenced by the result of predicting R(H,T ), so Φrij and Φrik will be also updated.",3.3 Integrated Inference Model,[0],[0]
"Algorithm 1 shows the main process of training, where Ξ is the triplet set for training, Ψ is the potential function in Equation (7), F is the formula set, fp is
a formula generated from the path p, and H,T, t are entities in the KB.",3.3 Integrated Inference Model,[0],[0]
"To predict ρ = R(H,T ), the algorithm first performs multi rounds of random walks, and each random walk can find a path pHt (at line 2).",3.3 Integrated Inference Model,[0],[0]
"Then the algorithm decides to update Ψ by Ltrw based on whether t is T (at line 3), and adds the formula pf into the formula set when t = T (at line 4).",3.3 Integrated Inference Model,[0],[0]
"After random walks, the inference model predicts ρ, and computes Linf and L inf rw according to the prediction result (at line 5).",3.3 Integrated Inference Model,[0],[0]
"FinallyW and Ψ are updated by Linf and L inf rw (at line 6-7), respectively.",3.3 Integrated Inference Model,[0],[0]
"After training by all triplets in Ξ, the algorithm removes formulas with low weights from F (at line 8) and outputs the model (at line 9).",3.3 Integrated Inference Model,[0],[0]
"When we infer a new triplet by this model, the process is similar to Algorithm 1.",3.3 Integrated Inference Model,[0],[0]
We first compare our approach with several state-ofart methods on link prediction task to explore our approach’s overall ability of inference.,4 Experiments,[0],[0]
"Subsequently, we evaluate formulas mined by different random walk methods to explore whether the goal-directed mechanism can increase efficiency of mining useful structures.",4 Experiments,[0],[0]
"Finally, we dive deep into the formulas generated by our approach to analyze the characters of our approach.",4 Experiments,[0],[0]
"We conduct experiments on both WN18 and FB15K datasets which are subsets sampled from WordNet (Miller, 1995) and Freebase (Bollacker et al., 2008), respectively, and Table 1 shows the statistics of them.",4.1 Datasets and Evaluation Setup,[0],[0]
"For the link prediction task, we predict the missing h or t for a triplet r(h, t) in test set.",4.1 Datasets and Evaluation Setup,[0],[0]
"The detail evaluation method is that t in r(h, t) is replaced by all entities in the KB and methods need to rank the right answer at the top of the list, and so does h in r(h, t).",4.1 Datasets and Evaluation Setup,[0],[0]
"We report the mean of those true answer ranks and the Hits@10 under both ’raw’ and ’filter’ as TransE (Bordes et al., 2013) does, where Hits@10 is the proportion of correct entities ranked in the top 10.
sults on relation form of government in FB15K.",4.1 Datasets and Evaluation Setup,[0],[0]
We employ two types of baselines.,4.2 Baselines,[0],[0]
"One type is based on random walks including: a) the basic random walk algorithm whose state transition probability matrix is shown in Equation (2); b) PRA in (Lao et al., 2011) which is a typical heuristic random walk algorithm.",4.2 Baselines,[0],[0]
"The other type is based on KB embeddings including TransE (Bordes et al., 2013), Rescal (Nickel et al., 2011), TransH (Wang et al., 2014b), TransR",4.2 Baselines,[0],[0]
"(Lin et al., 2015b).",4.2 Baselines,[0],[0]
"These embedding-based methods have no explicit formulas, so we will not evaluate their performances on mining formulas.",4.2 Baselines,[0],[0]
We implement three random walk methods under a unified framework.,4.3 Settings,[0],[0]
"To predict r(h, ∗) quickly, we first select Top-K candidate instances, t1→K , by TransE as (Wei et al., 2015), and then the algorithm infers each r(h, ti) and ranks them by inference results.",4.3 Settings,[0],[0]
"We adjust parameters for our approach with the validate dataset, and the optimal configurations are set as follows.",4.3 Settings,[0],[0]
"The rounds of random walk is 10, learning rate is 0.0001, training epoch is 100, the size of candidate set is 500 for WN18 and 100 for FB15K, the embeddings have 50 dimensionalities for WN18 and 100 dimensionalities for FB15K, and the embeddings are initialized by TransE. For some relations, random walk truly finds no practicable formulas, so we employ TransE to improve per-
formance for these relations.",4.3 Settings,[0],[0]
"For embedding-based methods, we use reported results directly since the evaluation datasets are identical.",4.3 Settings,[0],[0]
"We show the results of link prediction for our approach and all baselines in Table 2 (* means the mean of ranks for random walk methods are evaluated in the Top-K subset), and we can obtain the following observations:
1)",4.4 Results on Link Prediction,[0],[0]
"Our approach achieves good performances on both WN18 and FB15K. On the FB15K, our approach outperforms all baselines.",4.4 Results on Link Prediction,[0],[0]
It indicates that our approach is effective for inference.,4.4 Results on Link Prediction,[0],[0]
"On the WN18, three random walk methods have similar performances.",4.4 Results on Link Prediction,[0],[0]
"The reason is that most entities in WN18 only have a small number of neighbors, so RW and PRA can also find useful structures in a few rounds.
2) For FB15K, the performances of RW and PRA are both poor and even worse than a part of embedding-based methods, but the performance of our approach is still the best.",4.4 Results on Link Prediction,[0],[0]
"The reason is that there are too many relation types in FB15K, so goalless random walks introduce lots of noise.",4.4 Results on Link Prediction,[0],[0]
"Oppositely, our approach has a great capability of resisting noise for the goal-directed mechanism.
3) RW and PRA have similar performances on both datasets, which indicates the heuristic rule of PRA does not apply to all relations and formulas.",4.4 Results on Link Prediction,[0],[0]
"To further explore whether the goal-directed mechanism can increase efficiency of mining paths, we compare the three random walk methods by the number of paths mined.",4.5 Paths Recall by Random Walks,[0],[0]
"For each triplet R(H,T )
in the training set, we perform 10 rounds of random walks fromH and record the number of times which arrive at T, noted as Arr@10.",4.5 Paths Recall by Random Walks,[0],[0]
We respectively select one relation type from WN18 and FB15K and show the comparison result in Figure 2.,4.5 Paths Recall by Random Walks,[0],[0]
"We can obtain the following observations:
1) With the increase of training epochs, Arr@10 of the goal-directed random walk first increases and then stays around a high value on both WN18 and FB15K, but the Arr@10 of RW and PRA always stay the same.",4.5 Paths Recall by Random Walks,[0],[0]
"This phenomenon indicates that the goal-directed random walk is a learnable model and can be trained to find more useful structures with epochs increasing, but RW and PRA are not.
2) RW and PRA always have similar Arr@10, which means PRA has not found more formulas.",4.5 Paths Recall by Random Walks,[0],[0]
This indicates that the heuristic rule of PRA is not always be beneficial to mining more structures for all relations.,4.5 Paths Recall by Random Walks,[0],[0]
"In Table 3, we show a small number of formulas mined by our approach from FB15K, and the formulas represent different types.",4.6 Example Formulas,[0],[0]
"Some formulas contain clear logic, e.g, Formula 1 means that if the writer x contributes a story to the film y and y is adapted from the book z, x is the writer of the book z.",4.6 Example Formulas,[0],[0]
"Some formulas have a high probability of being satisfied, e.g., Formula 3 means the wedding place probably is also the burial place for some people, and Formula 7 means the parent of the person x died of the disease and thus the person x has a high risk of suffering from the disease.",4.6 Example Formulas,[0],[0]
"Some formulas depend on synonyms, e.g., story by and works written have the similar meaning in Formula 2.",4.6 Example Formulas,[0],[0]
"However, there are still useless formulas, e.g, Formula 8 is useless be-
cause the body of the formula is same as the head.",4.6 Example Formulas,[0],[0]
"Such useless formula can be removed by a superrule, which is that the head of a formula cannot occur in its body.",4.6 Example Formulas,[0],[0]
"Our work has two aspects, which are related to mining formula automatically and inference on KBs, respectively.
",5 Related Work,[0],[0]
"Inductive Logic Programming (ILP) (Muggleton and De Raedt, 1994) and Association Rule Mining (ARM) (Agrawal et al., 1993) are both early works on mining formulas.",5 Related Work,[0],[0]
"FOIT (Quinlan, 1990) and SHERLOCK (Schoenmackers et al., 2010) are typical ILP systems, but the former one usually need a lot of negative facts and the latter one focuses on mining formulas from text.",5 Related Work,[0],[0]
"AMIE (Galárraga et al., 2013) is based on ARM and proposes a new measure for formulas instead of the confidence.",5 Related Work,[0],[0]
"Several structure learning algorithms (Kok and Domingos, 2005; Kok and Domingos, 2009; Kok and Domingos, 2010) based on Markov Logic Network (MLN) (Richardson and Domingos, 2006) can also learn first order logic formulas automatically, but they are too slow to run on large KBs.",5 Related Work,[0],[0]
"ProPPR (Wang et al., 2013; Wang et al., 2014a) performs structure learning by depth first searching on the knowledge graph, which is still not efficient enough to handle webscale KBs.",5 Related Work,[0],[0]
"PRA (Lao and Cohen, 2010; Lao et al., 2011) is a method based on random walks and employs heuristic rules to direct random walks.",5 Related Work,[0],[0]
"PRA is closely related to our approach, but unlike it, our approach dynamically calculates state transition prob-
abilities.",5 Related Work,[0],[0]
"Another method based on random walks (Wei et al., 2015) merges embedding similarities of candidates into the random walk as a priori, while our approach employs KB embeddings to calculate potentials for neighbors.
",5 Related Work,[0],[0]
"The majority of mining formula methods can perform inference on KBs, and besides them, a dozen methods based KB embeddings can also achieve the inference goal, and the typical ones of them are TransE (Bordes et al., 2013), Rescal (Nickel et al., 2011), TransH (Wang et al., 2014b), TransR",5 Related Work,[0],[0]
"(Lin et al., 2015b).",5 Related Work,[0],[0]
These embedding-based methods take advantage of the implicit relationship between elements of the KB and perform inference by calculating similarities.,5 Related Work,[0],[0]
"There are also methods which combine inference formulas and KB embeddings, such as PTransE (Lin et al., 2015a) and ProPPR+MF (Wang and Cohen, 2016).",5 Related Work,[0],[0]
"In this paper, we introduce a goal-directed random walk algorithm to increase efficiency of mining useful formulas and decrease noise simultaneously.",6 Conclusion and Future Works,[0],[0]
The approach employs the inference target as the direction at each steps in the random walk process and is more inclined to visit structures helpful to inference.,6 Conclusion and Future Works,[0],[0]
"In empirical studies, we show our approach achieves good performances on link prediction task over large-scale KBs.",6 Conclusion and Future Works,[0],[0]
"In the future, we are interested in exploring mining formulas directly in the distributional spaces which may resolve the sparsity of formulas.",6 Conclusion and Future Works,[0],[0]
"This work was supported by the Natural Science Foundation of China (No. 61533018), the National Basic Research Program of China (No. 2014CB340503) and the National Natural Science Foundation of China (No. 61272332).",7 Acknowledgments,[0],[0]
And this work was also supported by Google through focused research awards program.,7 Acknowledgments,[0],[0]
"Deep inference on a large-scale knowledge base (KB) needs a mass of formulas, but it is almost impossible to create all formulas manually.",abstractText,[0],[0]
"Data-driven methods have been proposed to mine formulas from KBs automatically, where random sampling and approximate calculation are common techniques to handle big data.",abstractText,[0],[0]
"Among a series of methods, Random Walk is believed to be suitable for knowledge graph data.",abstractText,[0],[0]
"However, a pure random walk without goals still has a poor efficiency of mining useful formulas, and even introduces lots of noise which may mislead inference.",abstractText,[0],[0]
"Although several heuristic rules have been proposed to direct random walks, they do not work well due to the diversity of formulas.",abstractText,[0],[0]
"To this end, we propose a novel goaldirected inference formula mining algorithm, which directs random walks by the specific inference target at each step.",abstractText,[0],[0]
"The algorithm is more inclined to visit benefic structures to infer the target, so it can increase efficiency of random walks and avoid noise simultaneously.",abstractText,[0],[0]
The experiments on both WordNet and Freebase prove that our approach is has a high efficiency and performs best on the task.,abstractText,[0],[0]
Mining Inference Formulas by Goal-Directed Random Walks,title,[0],[0]
"Proceedings of the SIGDIAL 2015 Conference, pages 22–31, Prague, Czech Republic, 2-4 September 2015. c©2015 Association for Computational Linguistics",text,[0],[0]
Physically situated dialogue differs from traditional human-computer dialogue in that interactions will make use of reference to a dialogue agent’s surroundings.,1 Introduction,[0],[0]
"Tasks may fail due to dependencies on specific environment configurations, such as when a robot’s path to a goal is blocked.",1 Introduction,[0],[0]
"People will often help; in navigation dialogues they tend to ask proactive, task-related questions instead of simply signaling communication failure (Skantze, 2005).",1 Introduction,[0],[0]
They supplement the agent’s representation of the environment and allow it to complete tasks.,1 Introduction,[0],[0]
The current study establishes an empirical basis for grounding in physically situated contexts.,1 Introduction,[1.0],['The current study establishes an empirical basis for grounding in physically situated contexts.']
"We had people provide recovery strategies for a robot in various situations.
",1 Introduction,[1.0000000776005897],['We had people provide recovery strategies for a robot in various situations.']
"The focus of this work is on recovery from situated grounding problems, a type of miscommunication that occurs when an agent fails to uniquely map a person’s instructions to its surroundings (Marge and Rudnicky, 2013).",1 Introduction,[0],[0]
"A referential ambiguity is where an instruction resolves to more than one possibility (e.g., “Search the room on the left” when there are multiple rooms on the agent’s left); an impossible-to-execute problem
fails to resolve to any action (e.g., same instruction but there are no rooms on the agent’s left).",1 Introduction,[0.9999999265426187],"['A referential ambiguity is where an instruction resolves to more than one possibility (e.g., “Search the room on the left” when there are multiple rooms on the agent’s left); an impossible-to-execute problem fails to resolve to any action (e.g., same instruction but there are no rooms on the agent’s left).']"
"A common strategy evidenced in human-human corpora is for people to ask questions to recover from situated grounding problems (Tenbrink et al., 2010).
",1 Introduction,[0.9999999919012124],"['A common strategy evidenced in human-human corpora is for people to ask questions to recover from situated grounding problems (Tenbrink et al., 2010).']"
"Dialogue divides into two levels: that of managing the actual dialogue—determining who has the floor, that an utterance was recognized, etc.—and",1 Introduction,[0],[0]
"the dialogue that serves the main joint activities that dialogue partners are carrying out, like a human-robot team exploring a new area (Bangerter and Clark, 2003).",1 Introduction,[0],[0]
"Most approaches to grounding in dialogue systems are managing the dialogue itself, making use of spoken language input as an indicator of understanding (e.g., (Bohus, 2007; Skantze, 2007)).",1 Introduction,[0],[0]
Situated grounding problems are associated with the main joint activities; to resolve them we believe that the recovery model must be extended to include planning and environment information.,1 Introduction,[0],[0]
"Flexible recovery strategies make this possible by enabling dialogue partners to coordinate their joint activities and accomplish tasks.
",1 Introduction,[0],[0]
We cast the problem space as one where the agent aims to select the most efficient recovery strategy that would resolve a user’s intended referent.,1 Introduction,[1.0],['We cast the problem space as one where the agent aims to select the most efficient recovery strategy that would resolve a user’s intended referent.']
We expect that this efficiency is tied to the cognitive load it takes to produce clarifications.,1 Introduction,[0],[0]
Viethen and Dale (2006) suggest a similar prediction in their study comparing human and automatically generated referring expressions of objects and their properties.,1 Introduction,[1.0],['Viethen and Dale (2006) suggest a similar prediction in their study comparing human and automatically generated referring expressions of objects and their properties.']
"We sought to answer the following questions in this work: • How good are people at detecting situated
grounding problems?",1 Introduction,[1.0000000556550421],['We sought to answer the following questions in this work: • How good are people at detecting situated grounding problems?']
• How do people organize recovery strategies?,1 Introduction,[0],[0]
"• When resolving ambiguity, which properties do
people use to differentiate referents?",1 Introduction,[0],[0]
"• When resolving impossible-to-execute instruc-
tions, do people use active or passive ways to get the conversation back on track?
",1 Introduction,[0],[0]
"22
We determined the most common recovery strategies for referential ambiguity and impossible-toexecute problems.",1 Introduction,[0],[0]
Several patterns emerged that suggest ways that people expect agents to recover.,1 Introduction,[0],[0]
Ultimately we intend for dialogue systems to use such strategies in physically situated contexts.,1 Introduction,[0],[0]
Researchers have long observed miscommunication and recovery in human-human dialogue corpora.,2 Related Work,[0],[0]
"The HCRC MapTask had a direction giverdirection follower pair navigate two dimensional schematics with slightly different maps (Anderson et al., 1991).",2 Related Work,[0],[0]
Carletta (1992) proposed several recovery strategies following an analysis of this corpus.,2 Related Work,[0],[0]
"The SCARE corpus collected human-human dialogues in a similar scenario where the direction follower was situated in a three-dimensional virtual environment (Stoia et al., 2008).
",2 Related Work,[0],[0]
"The current study follows up an initial proposal set of recovery strategies for physically situated domains (Marge and Rudnicky, 2011).",2 Related Work,[0],[0]
Others have also developed recovery strategies for situated dialogue.,2 Related Work,[0],[0]
Kruijff et al. (2006) developed a framework for a robot mapping an environment that employed conversational strategies as part of the grounding process.,2 Related Work,[0],[0]
"A similar study focused on resolving misunderstandings in the humanrobot domain using the Wizard-of-Oz methodology (Koulouri and Lauria, 2009).",2 Related Work,[0],[0]
"A body of work on referring expression generation uses object attributes to generate descriptions of referents (e.g., (Guhe and Bard, 2008; Garoufi and Koller, 2014)).",2 Related Work,[0],[0]
"Viethen and Dale (2006) compared human-authored referring expressions of objects to existing natural language generation algorithms and found them to have very different content.
",2 Related Work,[0],[0]
Crowdsourcing has been shown to provide useful dialogue data:,2 Related Work,[0],[0]
Manuvinakurike and DeVault (2015) used the technique to collect gameplaying conversations.,2 Related Work,[0],[0]
"Wang et al. (2012) and Mitchell et al. (2014) have used crowdsourced data for training, while others have used it in real time systems (Lasecki et al., 2013; Huang et al., 2014).",2 Related Work,[0],[0]
"In this study, participants came up with phrases that a search-and-rescue robot should say in response to an operator’s command.",3 Method,[0],[0]
"The participant’s task was to view scenes in a virtual envi-
ronment then formulate the robot’s response to an operator’s request.",3 Method,[0],[0]
"Participants listened to an operator’s verbal command then typed in a response.
",3 Method,[0],[0]
"Scenes displayed one of three situations: referential ambiguity (more than one possible action), impossible-to-execute (zero possible actions), and executable (one possible action).",3 Method,[0],[0]
The instructions showed some example problems.,3 Method,[0],[0]
All situations involved one operator and one robot.,3 Method,[0],[0]
"After instructions and a practice trial, participants viewed scenes in one of 10 different environments (see Figure 1).",3.1 Experiment Design,[0],[0]
"They would first watch a flyover video of the robot’s environment, then view a screen showing labels for all possible referable objects in the scene.",3.1 Experiment Design,[0],[0]
The participant would then watch the robot enter the first scene.,3.1 Experiment Design,[1.0],['The participant would then watch the robot enter the first scene.']
"The practice trial and instructions did not provide any examples of questions.
",3.1 Experiment Design,[0],[0]
The robot would stop and a spoken instruction from the operator would be heard.,3.1 Experiment Design,[0],[0]
The participant was free to replay the instruction multiple times.,3.1 Experiment Design,[0],[0]
They would then enter a response (say an acknowledgment or a question).,3.1 Experiment Design,[0],[0]
"Upon completion of the trial, the robot would move to a different scene, where the process was repeated.
",3.1 Experiment Design,[0],[0]
Only self-contained questions that would allow the operator to answer without follow-up were allowed.,3.1 Experiment Design,[0],[0]
Thus generic questions like “which one?” would not allow the operator to give the robot enough useful information to proceed.,3.1 Experiment Design,[0],[0]
"In the instructions, we suggested that participants include some detail about the environment in their ques-
tions.",3.1 Experiment Design,[0],[0]
Participants used a web form1 to view situations and provide responses.,3.1 Experiment Design,[0],[0]
"We recorded demographic information (gender, age, native language, native country) and time on task.",3.1 Experiment Design,[0],[0]
"The instructions had several attention checks (Paolacci et al., 2010) to ensure that participants were focusing on the task.
",3.1 Experiment Design,[0],[0]
We created fifty trials across ten environments.,3.1 Experiment Design,[0],[0]
Each environment had five trials that represented waypoints the robot was to reach.,3.1 Experiment Design,[0],[0]
Participants viewed five different environments (totaling twenty-five trials).,3.1 Experiment Design,[0],[0]
Each command from the remote operator to the robot was a route instruction in the robot navigation domain.,3.1 Experiment Design,[0],[0]
Trials were assembled in two groups and participants were assigned randomly to one (see Table 1).,3.1 Experiment Design,[1.0],['Trials were assembled in two groups and participants were assigned randomly to one (see Table 1).']
Trial order was randomized according to a Latin Square.,3.1 Experiment Design,[0],[0]
"Scenes were of a 3D virtual environment at eye level, with the camera one to two meters behind the robot.",3.1.1 Scenes and Environments,[0],[0]
"Camera angle issues with environment objects caused this variation.
",3.1.1 Scenes and Environments,[0],[0]
Participants understood that the fictional operator was not co-located with the robot.,3.1.1 Scenes and Environments,[0],[0]
The USARSim robot simulation toolkit and the UnrealEd game map editor were used to create the environment.,3.1.1 Scenes and Environments,[0],[0]
"Cepstral’s SwiftTalker was used for the operator voice.
",3.1.1 Scenes and Environments,[0],[0]
"Of the fifty scenes, twenty-five (50%) had referential ambiguities, fifteen (30%) were impossible-to-execute, and ten (20%) were executable controls.",3.1.1 Scenes and Environments,[0],[0]
"The selection was weighted to referential ambiguity, as these were expected to produce greater variety in recovery strategies.",3.1.1 Scenes and Environments,[0],[0]
"We randomly assigned each of fifty trials a stimulus type according to this distribution, then divided the list into ten environments.",3.1.1 Scenes and Environments,[0],[0]
"The environments featured objects and doorways appropriate to the trial type, as well as waypoints.
1See http://goo.gl/forms/ZGpK3L1nPh for an example.
",3.1.1 Scenes and Environments,[0],[0]
"Referential Ambiguity We arranged the sources of information participants could use to describe referents, to enable analysis of the relationship between context and recovery strategies.",3.1.1 Scenes and Environments,[0],[0]
"The sources of information (i.e., “situated dimensions”) were: (1) intrinsic properties (either color or size), (2) history (objects that the robot already encountered), (3) egocentric proximity of the robot to candidate referents around it (the robot’s perspective is always taken), and (4) object proximity (proximity of candidate referents to other objects).",3.1.1 Scenes and Environments,[0],[0]
"Table 2 provides additional details.
",3.1.1 Scenes and Environments,[0],[0]
Scenes with referential ambiguity had up to four sources of information available.,3.1.1 Scenes and Environments,[0],[0]
"Information sources were evenly distributed across five trial types: one that included all four sources, and four that included all but one source of information (e.g., one division excluded using history information but did allow proximity, spatial, and object properties, one excluded proximity, etc.).
",3.1.1 Scenes and Environments,[0],[0]
Impossible-to-Execute The impossible-to-execute trials divided into two broad types.,3.1.1 Scenes and Environments,[1.0],['Impossible-to-Execute The impossible-to-execute trials divided into two broad types.']
Nine of the fifteen scenes were impossible because the operator’s command did not match to any referent in the environment.,3.1.1 Scenes and Environments,[0],[0]
"The other six scenes were impossible because a path to get to the matching referent was not possible.
",3.1.1 Scenes and Environments,[0.9999999504145428],['The other six scenes were impossible because a path to get to the matching referent was not possible.']
Executable Ten scenes were executable for the study and served as controls.,3.1.1 Scenes and Environments,[0],[0]
"The operator’s command mentioned existing, unambiguous referents.",3.1.1 Scenes and Environments,[0],[0]
Participants were aware of the robot’s capabilities before the start of the experiment.,3.1.2 Robot Capabilities,[0],[0]
The instructions said that the robot knew the locations of all objects in the environment and whether doors were closed or open.,3.1.2 Robot Capabilities,[0],[0]
"The robot also knew the color and size of objects in the environment (intrinsic properties), where objects were relative to the robot itself and to other objects (proximity), when objects were right, left, in front, and behind it (spatial terms), the room and hallway locations of objects (location), and the places it has been (history, the robot kept track of which objects it had visited).",3.1.2 Robot Capabilities,[1.0],"['The robot also knew the color and size of objects in the environment (intrinsic properties), where objects were relative to the robot itself and to other objects (proximity), when objects were right, left, in front, and behind it (spatial terms), the room and hallway locations of objects (location), and the places it has been (history, the robot kept track of which objects it had visited).']"
The robot could not pass through closed doors.,3.1.2 Robot Capabilities,[1.0],['The robot could not pass through closed doors.']
"We made five hypotheses about the organization and content of participant responses to situated grounding problems:
• Hypothesis 1: Participants will have more difficulty detecting impossible-to-execute scenes than ambiguous ones.",3.2 Hypotheses,[0],[0]
"Determining a robot’s tasks to be impossible requires good situation awareness (Nielsen et al., 2007) (i.e., an understanding of surroundings with respect to correctly completing tasks).",3.2 Hypotheses,[0],[0]
"Detecting referential ambiguity requires understanding the operator’s command and visually inspecting the space (Spivey et al., 2002); detecting impossible commands also requires recalling the robot’s capabilities and noticing obstacles.",3.2 Hypotheses,[0],[0]
"Previous research has noted that remote teleoperators have trouble establishing good situation awareness of a robot’s surroundings (Casper and Murphy, 2003; Burke et al., 2004).",3.2 Hypotheses,[0],[0]
"Moreover, obstacles near a robot can be difficult to detect with a restricted view as in the current study (Alfano and Michel, 1990; Arthur, 2000).",3.2 Hypotheses,[0],[0]
•,3.2 Hypotheses,[0],[0]
"Hypotheses 2a and 2b: Responses will more commonly be single, self-contained questions instead of a scene description followed by a question (2a for scenes with referential ambiguity, 2b for scenes that were impossible-toexecute).",3.2 Hypotheses,[0],[0]
"This should reflect the principle of least effort (Clark, 1996), and follow from Carletta’s (1992) observations in a similar dataset.",3.2 Hypotheses,[0],[0]
•,3.2 Hypotheses,[0],[0]
Hypothesis 3: Responses will use the situated dimensions that require the least cognitive effort when disambiguating referents.,3.2 Hypotheses,[0],[0]
Viethen and Dale (2006) suggest that minimizing cognitive load for the speaker or listener would produce more human-like referring expressions.,3.2 Hypotheses,[0],[0]
"We predict that responses will mention visually salient features of the scene, such as color or size of referents, more than history or object proximity.",3.2 Hypotheses,[0],[0]
"Desimone and Duncan (1995) found that color and shape draw more attention than other
properties in visual search tasks when they are highly distinguishable.",3.2 Hypotheses,[0],[0]
•,3.2 Hypotheses,[0],[0]
"Hypothesis 4: In cases of referential ambiguity where two candidate referents are present, responses will confirm one referent in the form of a yes-no question more than presenting a list.",3.2 Hypotheses,[0],[0]
"Results from an analysis of task-oriented dialogue suggests that people are efficient when asking clarification questions (Rieser and Moore, 2005).",3.2 Hypotheses,[0],[0]
"Additionally, Clark’s least effort principle (Clark, 1996) suggests that clarifying one referent using a yes-no confirmation would require less effort than presenting a list in two ways: producing a shorter question and constraining the range of responses to expect.",3.2 Hypotheses,[1.0],"['Additionally, Clark’s least effort principle (Clark, 1996) suggests that clarifying one referent using a yes-no confirmation would require less effort than presenting a list in two ways: producing a shorter question and constraining the range of responses to expect.']"
•,3.2 Hypotheses,[0],[0]
"Hypothesis 5: For impossible-to-execute instructions, responses will most commonly be ways for the robot to proactively work with the operator’s instruction, in an effort to get the conversation back on track.",3.2 Hypotheses,[0],[0]
"The other possible technique, to simply declare that the problem is not possible, will be less common.",3.2 Hypotheses,[0],[0]
This is because participants will believe such a strategy will not align with the task goal of having the robot say something that will allow it to proceed with the task.,3.2 Hypotheses,[0],[0]
"Skantze found that in human-human navigation dialogues, people would prefer to look for alternative ways to proceed rather than simply express nonunderstanding (Skantze, 2005).",3.2 Hypotheses,[0],[0]
"The key independent variable in this study was the stimulus type that the participant viewed (i.e., referential ambiguity, impossible-to-execute, or executable).",3.3 Measures,[0],[0]
"Dependent variables were observational measurements, presented below.",3.3 Measures,[0],[0]
"We report Fleiss’ kappa score for inter-annotator agreement
between three native English speaking annotators on a subset of the data.
",3.3 Measures,[0],[0]
Correctness (κ = 0.77):,3.3 Measures,[0],[0]
"Whether participants correctly determined the situation as ambiguous, impossible, or executable.",3.3 Measures,[0],[0]
Annotators labeled correctness based on the content of participant responses.,3.3 Measures,[0],[0]
This measure assessed participant accuracy for detecting situated grounding problems.,3.3 Measures,[0],[0]
"Either correct or incorrect.
",3.3 Measures,[0],[0]
"Sentence type (κ = 0.82): Either declarative, interrogative, imperative, or exclamatory (Cowan, 2008).
",3.3 Measures,[0],[0]
Question type (κ = 0.92): Sentences that needed an answer from the operator.,3.3 Measures,[0],[0]
"The three types were yes-no questions, alternative questions (which presented a list of options and includes wh- questions that used sources from Table 2), and generic wh- questions (Cowan, 2008).
",3.3 Measures,[0],[0]
Situated dimensions in response (κ = 0.75):,3.3 Measures,[0],[0]
The capability (or capabilities) that the participant mentioned when providing a response.,3.3 Measures,[0],[0]
"The types were intrinsic (color or size), object proximity, egocentric proximity, and history.
",3.3 Measures,[0],[0]
"Projected belief (impossible-to-execute trials only, κ = 0.80):",3.3 Measures,[0],[0]
"The participant’s belief about the next task, given the current operator instruction (projected onto the robot).",3.3 Measures,[0],[0]
"The types were unknown (response indicates participant is unsure what to do next), ask for more (ask for more details), propose alternative (propose alternative object), ask for help (ask operator to physically manipulate environment), and off topic.",3.3 Measures,[1.0],"['The types were unknown (response indicates participant is unsure what to do next), ask for more (ask for more details), propose alternative (propose alternative object), ask for help (ask operator to physically manipulate environment), and off topic.']"
We recruited 30 participants.,3.4 Participation,[0],[0]
"All participants completed the web form through the Amazon Mechanical Turk (MTurk) web portal2, all were located in the United States and had a task approval rate ≥95%.",3.4 Participation,[0],[0]
The group included 29 self-reported native English speakers born in the United States; 1 self-reported as a native Bangla speaker born in Bangladesh.,3.4 Participation,[0],[0]
The gender distribution was 15 male to 15 female.,3.4 Participation,[0],[0]
"Participants ranged in age from 22 to 52 (mean: 33 years, std. dev.: 7.7).",3.4 Participation,[0],[0]
They were paid between $1 and $2 for their participation.,3.4 Participation,[0],[0]
"We
2https://www.mturk.com
collected a total of 750 responses.",3.4 Participation,[0],[0]
We analyzed the measures by tabulating frequencies for each possible value.,4 Results,[0],[0]
Table 3 presents some example responses.,4 Results,[0],[0]
"In general, participants were good at detecting situated grounding problems.",4.1 Correctness,[0],[0]
"Out of 750 responses, 667 (89%) implied the correct scene type.",4.1 Correctness,[0],[0]
"We analyzed correctness across actual stimulus types (ambiguous, impossible-to-execute, executable) using a mixed-effects analysis of variance model3, with participant included as a random effect and trial group as a fixed effect.
",4.1 Correctness,[0],[0]
Hypothesis 1 predicted that participants will do better detecting scenes with referential ambiguity than those that were impossible-to-execute; the results support this hypothesis.,4.1 Correctness,[0],[0]
"Actual stimulus type had a significant main effect on correctness (F[2, 58] = 12.3, p < 0.001); trial group did not (F[1, 28] = 0.1, p = 0.72).",4.1 Correctness,[0],[0]
Participants had significantly worse performance detecting impossible-to-execute scenes compared to ambiguous ones (p< 0.001; Tukey HSD test).,4.1 Correctness,[0],[0]
"In fact, they were four times worse; of the impossible-toexecute scenes, participants failed to detect that 22% (50/225) of them were impossible, compared to 5% (17/375) of scenes with referential ambiguity.",4.1 Correctness,[1.0],"['In fact, they were four times worse; of the impossible-toexecute scenes, participants failed to detect that 22% (50/225) of them were impossible, compared to 5% (17/375) of scenes with referential ambiguity.']"
"Of the 150 instructions that were executable, participants failed to detect 11% (16/150) of them as such.",4.1 Correctness,[0],[0]
"We analyzed the 358 responses where participants correctly detected referential ambiguity.
",4.2 Referential Ambiguity,[0],[0]
"3This approach computed standard least squares regression using reduced maximum likelihood (Harville, 1977).
",4.2 Referential Ambiguity,[0],[0]
"Hypothesis 2a predicted that participants would more commonly ask single, self-contained questions instead of describing the scene and asking a question.",4.2 Referential Ambiguity,[0],[0]
We assessed this by counting sentence types within a response.,4.2 Referential Ambiguity,[0],[0]
Responses that had both a declarative sentence and an interrogative would fit this case.,4.2 Referential Ambiguity,[0],[0]
The results confirmed this hypothesis.,4.2 Referential Ambiguity,[0],[0]
"Only 4.5% (16/358) of possible responses had a declarative and an interrogative.
",4.2 Referential Ambiguity,[0],[0]
Hypothesis 3 predicted that participants would use the situated dimensions that require the least cognitive effort when disambiguating referents.,4.2 Referential Ambiguity,[0],[0]
"More specifically, the most common mentions will be those that are visually apparent (intrinsic properties like color and size), while those that require more processing would have fewer mentions (history and to a lesser extent object proximity and egocentric proximity).",4.2 Referential Ambiguity,[0],[0]
"We measured this by tabulating mentions of situated dimensions in all 358 correct participant responses, summarized in Figure 2.",4.2 Referential Ambiguity,[0],[0]
Multiple dimensions could occur in a single response.,4.2 Referential Ambiguity,[0],[0]
The results support this hypothesis.,4.2 Referential Ambiguity,[0],[0]
"By far, across all ambiguous scenarios, the most mentioned dimension was an intrinsic property.",4.2 Referential Ambiguity,[0],[0]
"More than half of all situated dimensions used were intrinsic (59%, 242/410 total mentions).",4.2 Referential Ambiguity,[0],[0]
"This was followed by the dimensions that we hypothesize require more cognitive effort: egocentric proximity had 30% (125/410) of mentions, object proximity 9.5% (39/410), and history 1% (4/410).",4.2 Referential Ambiguity,[0.9557707953304608],"['Of the intrinsic dimensions mentioned, most were only color (61%, 148/242), followed by size (33%, 81/242), and using both (5%, 13/242).']"
"Of the intrinsic dimensions mentioned, most were only color (61%, 148/242), followed by size (33%, 81/242), and using both (5%, 13/242).
",4.2 Referential Ambiguity,[0],[0]
Hypothesis 4 predicted that participants would ask yes-no confirmation questions in favor of presenting lists when disambiguating a referent with exactly two candidates.,4.2 Referential Ambiguity,[0],[0]
"The results suggest that the opposite is true; people strongly preferred to
list options, even when a confirmation question about one would have been sufficient.",4.2 Referential Ambiguity,[0],[0]
"Of the 285 responses that were correctly detected as ambiguous and were for scenes of exactly two possible referents, 74% (212/285) presented a list of options.",4.2 Referential Ambiguity,[0],[0]
Only 14% (39/285) asked yes-no confirmation questions.,4.2 Referential Ambiguity,[0],[0]
The remaining 34 questions (12%) were generic wh-questions.,4.2 Referential Ambiguity,[0],[0]
These results held in scenes where three options were present.,4.2 Referential Ambiguity,[0],[0]
"Overall 72% (259/358) presented a list of options, while 16% (58/358) asked generic wh-questions and 11% (41/358) asked yes-no confirmations.",4.2 Referential Ambiguity,[0],[0]
"We analyzed the 175 responses where participants correctly identified impossible-to-execute situations.
",4.3 Impossible-to-Execute,[0],[0]
Hypothesis 2b predicted that participants would more often only ask a question than also describe the scene.,4.3 Impossible-to-Execute,[0],[0]
Results confirmed this hypothesis.,4.3 Impossible-to-Execute,[0],[0]
"42% (73/175) of responses simply asked a question, while 22% (39/175) used only a declarative.",4.3 Impossible-to-Execute,[0],[0]
"More than a third included a declarative as well (36%, 63/175).",4.3 Impossible-to-Execute,[0],[0]
"The general organization to these was to declare the problem then ask a question about it (89%, 56/63).
",4.3 Impossible-to-Execute,[0],[0]
"Hypothesis 5 predicted that responses for impossible-to-execute instructions will more commonly be proactive and make suggestions, instead of simply declaring that an action was not possible.",4.3 Impossible-to-Execute,[0],[0]
"Table 4 summarizes the results, which confirmed this hypothesis.",4.3 Impossible-to-Execute,[0],[0]
The most common belief that participants had for the robot was to have it propose an alternative referent to the impossible one specified by the operator.,4.3 Impossible-to-Execute,[0],[0]
The next-most common was to have the robot simply express uncertainty about what to do next.,4.3 Impossible-to-Execute,[0],[0]
"Though this belief occurred in about a third of responses, the remaining responses were all proactive ways for the robot to get the conversation back on track (i.e., propose alternative, ask for more, and ask for help).",4.3 Impossible-to-Execute,[0],[0]
"The results largely support the hypotheses, with the exception of Hypothesis 4.",5 Discussion,[0],[0]
"They also provide information about how people expect robots to recover from situated grounding problems.
",5 Discussion,[0],[0]
"Correctness Participants had the most trouble detecting impossible-to-execute scenes, supporting Hypothesis 1.",5 Discussion,[0],[0]
"An error analysis of the 50 responses for this condition had participants responding as if the impossible scenes were possible (62%, 31/50).",5 Discussion,[1.0],"['An error analysis of the 50 responses for this condition had participants responding as if the impossible scenes were possible (62%, 31/50).']"
"The lack of good situation awareness was a factor, which agrees with previous findings in the human-robot interaction literature (Casper and Murphy, 2003; Burke et al., 2004).",5 Discussion,[0],[0]
We found that participants had trouble with a specific scene where they confused the front and back of the robot (9 of the 31 impossibleexecutable responses were for this scene).,5 Discussion,[0],[0]
"Note that all scenes showed the robot entering the room with the same perspective, facing forward.
",5 Discussion,[0],[0]
"Referential Ambiguity Results for Hypothesis 2a showed that participants overwhelmingly asked only a single, self-contained question as opposed to first stating that there was an ambiguity.",5 Discussion,[0],[0]
"Participants also preferred to present a list of options, despite the number of possible candidates.",5 Discussion,[0.9638345811083074],"['A list offers several benefits: it grounds awareness of surroundings, presents a fixed set of options to the user, and constrains the range of linguistic responses.']"
"This contradicted Hypothesis 4. Rieser and Moore (2005) found that in task-oriented human-human dialogues, clarification requests aim to be as efficient as possible; they are mostly partially formed.",5 Discussion,[0],[0]
The results in our study were not of real-time dialogue; we isolated specific parts of what participants believed to be human-computer dialogue.,5 Discussion,[0],[0]
"Moreover, Rieser and Moore were observing clarifications at Bangerter and Clark’s (2003) dialogue management level; we were observing them in service of the joint activity of navigating the robot.",5 Discussion,[0],[0]
"We believe that this difference resulted in participants using caution by disambiguating with lists.
",5 Discussion,[0],[0]
"These results suggest that dialogue systems should present detection of referential ambiguity implicitly, and as a list.",5 Discussion,[0],[0]
"Generic wh- questions (e.g., “which one?” without presenting a followon list) are less desirable because they don’t constrain what the user can say, and don’t provide any indication of what the dialogue system can understand.",5 Discussion,[0],[0]
"A list offers several benefits: it grounds awareness of surroundings, presents a fixed set of options to the user, and constrains the range of
linguistic responses.",5 Discussion,[0],[0]
"This could also extend to general ambiguity, as in when there are a list of matches to a query, but that is outside the scope of this work.",5 Discussion,[0],[0]
"Lists may be less useful as they grow in size; in our study they could not grow beyond three candidates.
",5 Discussion,[0],[0]
The data also supported Hypothesis 3.,5 Discussion,[0],[0]
Participants generally preferred to use situated dimensions that required less effort to describe.,5 Discussion,[0],[0]
"Intrinsic dimensions (color and size) had the greatest count, followed by egocentric proximity, object proximity, and finally using history.",5 Discussion,[0],[0]
"We attribute these results to the salient nature of intrinsic properties compared to ones that must be computed (i.e., egocentric and object proximity require spatial processing, while history requires thinking about previous exchanges).",5 Discussion,[0],[0]
This also speaks to a similar claim by Viethen and Dale (2006).,5 Discussion,[0],[0]
"Responses included color more than any other property, suggesting that an object’s color draws more visual attention than its size.",5 Discussion,[0],[0]
"Bright colors and big shapes stand out most in visual search tasks; we had more of the former than the latter (Desimone and Duncan, 1995).
",5 Discussion,[0],[0]
"For an ambiguous scene, participants appear to traverse a salience hierarchy (Hirst et al., 1994) whereby they select the most visually salient feature that also uniquely teases apart candidates.",5 Discussion,[0],[0]
"While the salience hierarchy varies depending on the current context of a referent, we anticipate such a hierarchy can be defined computationally.",5 Discussion,[0],[0]
"Others have proposed similar processes for referring expression generation (Van Der Sluis, 2005; Guhe and Bard, 2008).",5 Discussion,[0],[0]
One way to rank salience on the hierarchy could be predicted mental load; we speculate that this is a reason why history was barely mentioned to disambiguate.,5 Discussion,[0],[0]
"Another would be to model visual attention, which could explain why color was so dominant.
",5 Discussion,[0],[0]
"Note that only a few dimensions were “competing” at any given time, and their presence in the scenes was equal (save for history, which had slightly fewer due to task design constraints).",5 Discussion,[0],[0]
"Egocentric proximity, which uses spatial language to orient candidate referents relative to the robot, had a moderate presence.",5 Discussion,[0],[0]
"When intrinsic properties were unavailable in the scene, responses most often used this property.",5 Discussion,[0],[0]
"We found that sometimes participants would derive this property even if it wasn’t made prototypical in the scene (e.g., referring to a table as “left” when it was in front and
off to the left side of the robot).",5 Discussion,[0],[0]
This suggests that using egocentric proximity to disambiguate makes a good fallback strategy when nothing else works.,5 Discussion,[0],[0]
"Another situated dimension emerged from the responses, disambiguation by location (e.g., “Do you mean the box in this room or the other one?”).",5 Discussion,[0],[0]
"Though not frequent, it provides another useful technique to disambiguate when visually salient properties are not available.
",5 Discussion,[0],[0]
"Our findings differ from those of Carlson and Hill (2009) who found that salience is not as prominent as spatial relationships between a target (in the current study, this would be the robot) and other objects.",5 Discussion,[0],[0]
Our study did not direct participants to formulate spatial descriptions; they were free to compose responses.,5 Discussion,[0],[0]
"In addition, our work directly compares intrinsic properties for objects of the same broad type (e.g., disambiguation of a doors of different colors).",5 Discussion,[0],[0]
"Our findings suggest the opposite of Moratz et al. (2003), who found that when pointing out an object, describing its position may be better than describing its attributes in human-robot interactions.",5 Discussion,[0],[0]
"Their study only had one object type (cube) and did not vary color, size, or proximity to nearby objects.",5 Discussion,[0],[0]
"As a result, participants described objects using spatial terms.",5 Discussion,[0],[0]
"In our study, we explored variation of several attributes to determine participants’ preferences.
",5 Discussion,[0],[0]
Impossible-to-Execute Results supported Hypothesis 2b.,5 Discussion,[0],[0]
Most responses had a single sentence type.,5 Discussion,[0],[0]
"Although unanticipated, a useful strategy emerged: describe the problem that makes the scene impossible, then propose an alternative referent.",5 Discussion,[0],[0]
This type of strategy helped support Hypothesis 5.,5 Discussion,[0],[0]
"Responses for impossible scenes largely had the participant proactively presenting a way to move the task forward, similar to what Skantze (2005) observed in human-human dialogues.",5 Discussion,[0],[0]
This suggests that participants believed the robot should ask directed questions to recover.,5 Discussion,[0],[0]
These questions often took the form of posing alternative options.,5 Discussion,[0],[0]
We used the Amazon Mechanical Turk web portal to gather responses in this study.,5.1 Limitations,[0],[0]
"As such we could not control the participant environment when taking the study, but we did include attention checks.",5.1 Limitations,[0],[0]
"Participants did not interact with a
dialogue system.",5.1 Limitations,[0],[0]
Instead we isolated parts of the interaction that were instances of where the robot would have to say something in response to an instruction.,5.1 Limitations,[0],[0]
We asked participants to provide what they think the robot should say; there was no ongoing interaction.,5.1 Limitations,[1.0],['We asked participants to provide what they think the robot should say; there was no ongoing interaction.']
"However, we maintained continuity by presenting videos of the robot navigating through the environment as participants completed the task.",5.1 Limitations,[0],[0]
"The robot was represented in a virtual environment, which prevents us from understanding if there are any influencing factors that may impact results if the robot were in physical form or co-present with the participant.",5.1 Limitations,[0],[0]
Recovery strategies allow situated agents like robots to recover from misunderstandings by using the human dialogue partner.,6 Conclusions,[0],[0]
We conducted a study that collected recovery strategies for physically situated dialogue with the goal of establishing an empirical basis for grounding in physically situated contexts.,6 Conclusions,[1.0],['We conducted a study that collected recovery strategies for physically situated dialogue with the goal of establishing an empirical basis for grounding in physically situated contexts.']
"We crowdsourced 750 written strategies across 30 participants and analyzed their situated properties and how they were organized.
",6 Conclusions,[0],[0]
We found that participants’ recovery strategies minimize cognitive effort and indicate a desire to successfully complete the task.,6 Conclusions,[0],[0]
"For disambiguation, there was a preference for strategies that use visually salient properties over ones that require additional mental processing, like spatial reasoning or memory recall.",6 Conclusions,[1.0],"['For disambiguation, there was a preference for strategies that use visually salient properties over ones that require additional mental processing, like spatial reasoning or memory recall.']"
"For impossible-to-execute scenes, responses more often presented alternative referents than just noting non-understanding.",6 Conclusions,[1.0],"['For impossible-to-execute scenes, responses more often presented alternative referents than just noting non-understanding.']"
"We should note that some differences between our findings and those of others may in part rest on differences in task and environment, though intrinsic variables such as mental effort will likely persist over different situations.
",6 Conclusions,[0],[0]
"In future work, we intend to use these data to model salience ranking in similar contexts.",6 Conclusions,[1.0],"['In future work, we intend to use these data to model salience ranking in similar contexts.']"
We will further assess the hypothesis that participants’ preferences in this study will enhance performance in a spoken dialogue system that deploys similar strategies.,6 Conclusions,[1.0],['We will further assess the hypothesis that participants’ preferences in this study will enhance performance in a spoken dialogue system that deploys similar strategies.']
The authors thank Prasanna Kumar Muthukumar and Juneki Hong for helping to annotate recovery strategies.,Acknowledgments,[0],[0]
"We also thank Taylor Cassidy, Arthur William Evans, and the anonymous reviewers for their valuable comments.",Acknowledgments,[0],[0]
We describe an empirical study that crowdsourced human-authored recovery strategies for various problems encountered in physically situated dialogue.,abstractText,[0],[0]
The purpose was to investigate the strategies that people use in response to requests that are referentially ambiguous or impossible to execute.,abstractText,[0],[0]
"Results suggest a general preference for including specific kinds of visual information when disambiguating referents, and for volunteering alternative plans when the original instruction was not possible to carry out.",abstractText,[0],[0]
Miscommunication Recovery in Physically Situated Dialogue,title,[0],[0]
