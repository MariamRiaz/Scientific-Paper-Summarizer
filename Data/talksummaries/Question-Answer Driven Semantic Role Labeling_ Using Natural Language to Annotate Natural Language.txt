0	41	Semantic role labeling (SRL) is the widely studied challenge of recovering predicate-argument structure for natural language words, typically verbs.
3	21	However, this intuition is difficult to formalize and fundamental aspects of the task vary across efforts, for example FrameNet (Baker et al., 1998) models a large set of interpretable thematic roles (AGENT, PATIENT, etc.)
4	47	while PropBank (Palmer et al., 2005) uses a small set of verb-specific roles (ARG0, ARG1, etc.).
6	21	In this paper, we introduce a new questionanswer driven SRL task formulation (QA-SRL), which uses question-answer pairs to label verbal predicate-argument structure.
11	27	Moreover, the formulation does not depend on any pre-defined inventory of semantic roles or frames, or build on any existing gram- 643 mar formalisms.
13	23	The annotations also, perhaps surprisingly, capture other implicit arguments that cannot be read directly off of the syntax, as was required for previous SRL approaches.
18	47	Given a sentence and target word (the verb), we ask annotators to provide as many questionanswer pairs as possible, where the question comes from a templated space of wh-questions2 and the answer is a phrase from the original sentence.
21	63	The question generation aspect of QA-SRL is unique to our formulation, and corresponds roughly to identifying what semantic role labels are present in previous formulations of the task.
27	19	We show that the data is high quality, rivaling PropBank in many aspects including coverage, and easily gathered in nonnewswire domains.3 The baseline performance levels for question generation and answering reinforce the quality of the data and highlight the potential for future work on this task.
28	39	In summary, our contributions are: • We introduce the task of question-answer driven semantic role labeling (QA-SRL), by using question-answer pairs to specify verbal arguments and the roles they play, without predefining an inventory of frames or semantic roles.
61	36	Given a sentence s and a verbal predicate v in the sentence, annotators must produce a set of wh-questions that contain v and whose answers are phrases in s. To speed annotation and simplify downstream processing, we define a small grammar over possible questions.
62	19	The questions are constrained with a template with seven fields, q ∈ WH × AUX × SBJ × TRG × OBJ1 × PP × OBJ2, each associated with a list of possible options.
65	29	The precise form of the question template is a function of the verb v and sentence s, for two of the fields.
70	21	We annotated over 3000 sentences (nearly 8,000 verbs) in total across two domains: newswire (PropBank) and Wikipedia.
72	42	In the newswire domain, we sampled sentences from the English training data of CoNLL-2009 shared task (Hajič et al., 2009), excluding questions and sentences with fewer than 10 words.
73	27	For the Wikipedia domain, we randomly sampled sentences from the English Wikipedia, excluding questions and sentences with fewer than 10 or more than 60 words.
80	32	The average cost was $0.58 per verb ($1.57 per sentence) for newswire text and $0.45 per verb ($1.01 per sentence) on the Wikipedia domain.
88	21	For each PropBank predicate that we have annotated with our scheme, we compute the agreement between the PropBank arguments and the QA-SRL answers.
90	20	An annotated answer is judged to match the PropBank argument if either (1) the gold argument head is within the annotated answer span, or (2) the gold argument head is a preposition and at least one of its children is within the answer span.
94	36	Agreement for adjuncts is lower, because the annotated QAs often contain inferred roles, especially for why, when and where questions (See examples 4, 7 and 8 in Table 1).
108	44	After five annotators, the number of agreed QA pairs starts to asymptote.
123	25	Some roles can be subclassed with prepositions.
128	106	Predicting Question Roles Given this space of possible roles, our first step in generation is to determine which roles are present in a sentence, and select the pronouns that could be used to refer to them in the resulting questions.
130	25	We define the set of possible labels L by combining the roles inR with different pronoun values: L ={role:val | role ∈ R} val ∈{φ, someone, something, do something, doing something} For example, to support the generation of the questions Who finished something?
146	23	The average number of QA pairs per verb collected by human annotator is roughly 2.5, demonstrating significant room for improving these results.
147	25	The goal of the answer identification task is to predict an answer a given sentence s, target verb v and a question q.
174	20	The biggest challenge in annotating sentences with our scheme is choosing the questions.
178	98	Most obviously, the annotation can be used for training question-answering systems, as it directly encodes question-answer pairs.
180	46	A joint syntactic and semantic parser, such as that of Lewis et al. (2015), could be trained directly on the annotations to improve both the syntactic and semantic models, for example in domain transfer settings.
181	208	Alternatively, the annotation could be used for active learning: we envisage a scheme where parsers, when faced with ambiguous attachment decisions, can generate a human-readable question whose answer will resolve the attachment.
