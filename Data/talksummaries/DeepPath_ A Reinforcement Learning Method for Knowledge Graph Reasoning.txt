15	46	Empirically, we show that our method outperforms PRA and embedding based methods on a Freebase and a Never-Ending Language Learning (Carlson et al., 2010a) dataset.
16	66	564 Our contributions are three-fold: • We are the first to consider reinforcement learning (RL) methods for learning relational paths in knowledge graphs; • Our learning method uses a complex reward function that considers accuracy, efficiency, and path diversity simultaneously, offering better control and more flexibility in the pathfinding process; • We show that our method can scale up to large scale knowledge graphs, outperforming PRA and KG embedding methods in two tasks.
39	25	The specific task of relation reasoning is to find reliable predictive paths between entity pairs.
40	53	We formulate the path finding problem as a sequential decision making problem which can be solved with a RL agent.
43	72	Then we describe the training procedure of our RL model.
44	45	After that, we describe an efficient path-constrained search algorithm for relation reasoning with the paths found by the RL agent.
46	39	The first part is the external environment E which specifies the dynamics of the interaction between the agent and the KG.
47	42	This environment is modeled as a Markov decision process (MDP).
48	42	A tuple < S,A,P,R > is defined to represent the MDP, where S is the continuous state space, A = {a1, a2, ..., an} is the set of all available actions, P(St+1 = s′ |St = s,At = a) is the transition probability matrix, and R(s, a) is the reward function of every (s, a) pairs.
49	27	The second part of the system, the RL agent, is represented as a policy network πθ(s, a) = p(a|s; θ) which maps the state vector to a stochastic policy.
52	49	One reason is that for the path finding problem in KG, the action space can be very large due to complexity of the relation graph.
75	20	In view of this challenge, the first reward function we add to the RL model is defined as follows: rGLOBAL = { +1, if the path reaches etarget −1, otherwise the agent is given an offline positive reward +1 if it reaches the target after a sequence of actions.
78	26	The efficiency reward is defined as follows: rEFFICIENCY = 1 length(p) where path p is defined as a sequence of relations r1 → r2 → ...→ rn.
88	20	For a typical KG, the RL agent is often faced with hundreds (thousands) of possible actions.
90	27	Due to the complexity of the relation graph and the large action space, if we directly train the RL model by trial and errors, which is typical for RL algorithms, the RL model will show very poor convergence properties.
91	64	After a long-time training, the agents fails to find any valuable path.
92	74	To tackle this problem, we start our training with a supervised policy which is inspired by the imitation learning pipeline used by AlphaGo (Silver et al., 2016).
165	31	We use the same dimension as TransE, R to embed the entities.
167	86	To reason using the path formulas, we adopt a similar linear regression approach as in PRA to re-rank the paths.
168	21	However, instead of using the random walk probabilities as path features, which can be computationally expensive, we simply use binary path features obtained by the bi-directional search.
171	23	Table 2 shows the mean average precision (MAP) results on two datasets.
187	62	To interpret these paths, take the personNationality relation for example, the first reasoning path indicates that if we know facts placeOfBirth(x,y) and locationContains(z,y) then it is highly possible that person x has nationality z.
204	26	In this paper, we propose a reinforcement learning framework to improve the performance of relation reasoning in KGs.
205	50	Specifically, we train a RL agent to find reasoning paths in the knowledge base.
206	27	Unlike previous path finding models that are based on random walks, the RL model allows us to control the properties of the found paths.
207	28	These effective paths can also be used as an alternative to PRA in many path-based reasoning methods.
208	46	For two standard reasoning tasks, using the RL paths as reasoning formulas, our approach generally outperforms two classes of baselines.
209	103	For future studies, we plan to investigate the possibility of incorporating adversarial learning (Goodfellow et al., 2014) to give better rewards than the human-defined reward functions used in this work.
210	41	Instead of designing rewards according to path characteristics, a discriminative model can be trained to give rewards.
211	65	Also, to address the problematic scenario when the KG does not have enough reasoning paths, we are interested in applying our RL framework to joint reasoning with KG triples and text mentions.
