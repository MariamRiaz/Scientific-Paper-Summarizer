{"sections": [{"heading": "1. Introduction", "text": "In order to produce realistic speech, a text-to-speech (TTS) system must implicitly or explicitly impute many factors that are not given in a simple text input. Such factors include the intonation, stress, rhythm and style of the speech, and are collectively referred to as prosody.\nSpeech synthesis via text-to-speech is a challenging underdetermined problem, since the meaning expressed by an utterance is inherently underspecified by the text. For example, the simple statement \u201cThe cat sat on the mat.\u201d can be spoken many different ways. If the statement is the answer to the question \u201cWhere did the cat sit?\u201d the speaker might stress the word \u201cmat\u201d to indicate that it is the answer to the question. To express uncertainty in their knowledge, the speaker may decide to intone the response with a rising pitch. The question, \u201cWould you like an apple or an orange?\u201d\n1Google, Inc.. Correspondence to: RJ Skerry-Ryan <rjryan@google.com>.\nSound demos are available at https://google. github.io/tacotron/publications/end_to_ end_prosody_transfer.\ncan also be spoken in multiple ways, indicating information about the set of objects that exist. If there are only two possible options, the intonation of the final option (\u201corange\u201d) will have a declining pitch. If there are a variety of options of which apple and orange are just two examples, both options are typically intoned with a rising pitch. The intonation of these sentences carries meaning about the environment or context of the question which is unspecified by the text, and in general, there are any number of such nuances present in speech that convey information beyond the textual content.\nIn order to avoid the challenging problem of schematizing and labeling prosody, we seek methods of modeling prosody that do not require explicit annotations, and present an architecture for learning a latent prosody representation by extracting it from the ground truth speech audio. Accordingly, we use a \u201csubtractive\u201d definition of prosody:\nDefinition. Prosody is the variation in speech signals that remains after accounting for variation due to phonetics, speaker identity, and channel effects (i.e. the recording environment).\nThis view of prosody is compatible with interpretations of prosody from previous works (Wagner & Watson, 2010; Ladd, 2008).\nOne natural problem that arises from this formulation is sampling \u2013 that is, the challenge of generating diverse and interesting prosody and output speech even for identical phonetics, speaker identities and channel effects. In this paper, we tackle the more basic problem of constructing a space that represents prosody. We propose one possible construction of a prosody latent space, and show that we capture meaningful variation in speech by demonstrating transfer in this space (i.e., using a latent representation to make one utterance sound like another): this roughly corresponds to a \u201csay it like this\u201d task.\nThe recently proposed Tacotron speech synthesis system (Wang et al., 2017a) computes its output directly from graphemes or phonemes, and its prosody model is implicit, learned from the statistics of the training data alone. It learns, for example, that an English sentence ending in a question mark likely has a rising pitch if the question has a\nar X\niv :1\n80 3.\n09 04\n7v 1\n[ cs\n.C L\n] 2\n4 M\nar 2\n01 8\nyes-or-no answer. In this work, we augment Tacotron with explicit prosody controls. We accomplish this by learning an encoder architecture that computes a low-dimensional embedding from a speech signal, where the embedding provides information not provided by the text and speaker identity. Through careful experiments, we demonstrate that this prosody embedding can be used to reproduce the desired prosody using Tacotron.\nThe immediate implication of this acoustic encoder architecture and prosody latent space is that we can control the behavior of a TTS system using a different voice than the one used in training. The resulting embedding is fixedlength and often smaller than the transcript, so it can be easily stored alongside the text for use in a production system. The longer-term implications are that we can build models that predict prosody embeddings from non-acoustic context, such as prosody labels or conversation state.\nOur main contribution is an encoder architecture that extracts a fixed-length learned representation of prosody from acoustic input; we demonstrate that this encoder allows us to transfer prosody between utterances in an almost speaker-independent fashion. To evaluate performance in this prosody transfer task, we propose a number of quantitative and qualitative metrics. Additionally, we strongly encourage the reader to listen to the audio samples on our demo page."}, {"heading": "2. Related Work", "text": "Prosody and speaking style modeling have been studied since the era of HMM-based TTS research. For example, (Eyben et al., 2012) proposes a system that first clusters the training set, and then performs HMM-based cluster-adaptive training. (Nose et al., 2007) proposes estimating the transformation matrix for a set of predefined style vectors.\nNumerous works have explored annotation schemes for diagramming and automatic labeling of prosody: ToBi (Silverman et al., 1992), AuToBI (Rosenberg, 2010), Tilt (Taylor, 1998), INTSINT (Hirst, 2001), SLAM (Obin et al., 2014) all describe methods for the annotation and automatic extraction of labels or annotations that correlate with prosodic phenomena. The challenges of annotation often require domain experts,however, and inter-rater annotations can differ substantially (Wightman, 2002).\nFew works propose the use of acoustic reference signals to control the prosody of a text-to-speech model. (Tesser et al., 2013) proposes the use of \u201csignal driven\u201d features to predict symbolic prosody representations, using AuToBI labels to improve HMM-based synthesis. (Coile et al., 1994) propose \u201cprosody transplantation\u201d via a system called PROTRAN for recording a low-bit-rate \u201cenriched phonetic transcription\u201d that can be used in conjunction with desired text to\nreproduce the prosody of an original recording. Note that the same product needs described in (Coile et al., 1994) motivate the development of this paper.\nProsody transfer is related to the task of voice conversion (also called style transfer in the audio context). To perform voice conversion, a model must synthesize an utterance, given only the acoustic signal of that utterance in a different speaker\u2019s voice (Wu et al., 2013; Nakashika et al., 2016; Kinnunen et al., 2017; van den Oord et al., 2017; Chorowski et al., 2017). An approach similar to ours can be found in (Wang et al., 2018), where a more complicated autoencoder is used to learn some elements of style in an unsupervised fashion."}, {"heading": "3. Model Architecture", "text": "Our model is based on Tacotron (Wang et al., 2017a), a recently proposed state-of-the-art end-to-end speech synthesis model that predicts mel spectrograms directly from grapheme or phoneme sequences. The predicted mel spectrograms can either be synthesized directly to the timedomain via a WaveNet vocoder (Shen et al., 2017), or by first learning a linear spectrogram prediction network, and then applying Griffin-Lim spectrogram inversion (Griffin & Lim, 1984).\nIn this work, we use the original encoder and decoder architecture from (Wang et al., 2017a), not the simplified architecture proposed by (Shen et al., 2017). Additionally, we exclusively use phoneme inputs produced by a text normalization front-end and lexicon, as we are specifically interested in addressing prosody, not the model\u2019s ability to learn pronunciation from graphemes. Finally, instead of the Bahdanau attention used in (Wang et al., 2017a), we use the GMM attention of (Graves, 2013) which we find improves generalization to long utterances.\nThe audio samples included on our demo page were produced with a WaveNet vocoder (Shen et al., 2017); however, the original linear-spectrogram prediction network followed by Griffin-Lim spectrogram inversion from (Wang et al., 2017a) works equally well for prosody transfer. In practice, we find the choice of neural vocoder only impacts audio fidelity and has no impact on the system\u2019s resulting prosody."}, {"heading": "3.1. Multi-speaker Tacotron", "text": "Tacotron as proposed in (Wang et al., 2017a) does not include explicit modeling of speaker identity; however, due to the flexibility of all neural sequence-to-sequence models, learning multi-speaker models via conditioning on speaker identity is straightforward. We follow a scheme similar to (Ar\u0131k et al., 2017) to model multiple speakers.\nThe Tacotron architecture conditions an auto-regressive de-\ncoder on an LT \u00d7 dT -dimensional representation of the phoneme or grapheme sequence produced by a transcript encoder architecture, where LT is the length of the encoded transcript representation (typically equal to the length of the input transcript) and dT is the embedding dimension produced by the transcript encoder. For each speaker in the dataset, a RdS embedding vector is initialized with Glorot (Glorot & Bengio, 2010) initialization. For each example, the dS-dimensional speaker embedding corresponding to the true speaker of the example is broadcast-concatenated with the LT \u00d7 dT -dimensional transcript encoder representation to form a (dT +dS)-dimensional sequence of encoder embeddings that the decoder will attend to. No additional changes or loss metrics are necessary."}, {"heading": "3.2. Reference Encoder", "text": "We extend the Tacotron architecture by adding a \u201creference encoder\u201d module that takes a length-LR and dRdimensional reference signal as input, and computes a dP - dimensional embedding from it. We think of this fixeddimensional embedding as the \u201cprosody space\u201d \u2013 our goal is that sampling from this space will yield diverse and plausible output speech, and that we can manipulate elements of\nthis space to control the output meaningfully.\nAs with the speaker embedding, this prosody embedding is combined with the LT \u00d7 dT text encoder representation via a broadcast-concatenation. In combination with the speaker embeddings described in Section 3.1, the encoder embeddings form a LT \u00d7(dT +dS+dP ) embedding matrix, where the speaker and prosody embeddings are fixed across all timesteps. Figure 1 illustrates this structure.\nDuring training, the reference acoustic signal is simply the target audio sequence being modeled. No explicit supervision signal is used to train the reference encoder; it is learned using Tacotron\u2019s reconstruction error as its only loss. In training, one can think of the combined system as an RNN encoder-decoder (Cho et al., 2014b) with phonetic and speaker information as conditioning input. For a sufficiently high-capacity embedding, this representation could simply learn to copy the input to the output during training. Therefore, as with an autoencoder, care must be taken to choose an architecture that sufficiently bottlenecks the prosody embedding such that it is forced to learn a compact representation.\nDuring inference, we can use the prosody reference encoder\nto encode any utterance: we are not constrained to match either the text input or the provided speaker embedding. In particular, this enables the possibility of prosody transfer \u2013 using an utterance by a different speaker, or different text to control the output. We study prosody transfer in detail in Section 4.\nFor the reference encoder architecture (Figure 2), we use a simple 6-layer convolutional network. Each layer is composed of 3\u00d7 3 filters with 2\u00d7 2 stride, SAME padding and ReLU activation. Batch normalization (Ioffe & Szegedy, 2015) is applied to every layer. The number of filters in each layer doubles at half the rate of downsampling: 32, 32, 64, 64, 128, 128.\nThe LR \u00d7 dR reference signal is downsampled by this architecture 64 times in both dimensions. The ddR/64e feature dimensions and 128 channels of the final convolution layer are unrolled as the inner dimension of the resulting dLR/64e\u00d7128ddR/64ematrix. To compress the dLR/64elength sequence produced by the CNN layers down to a single fixed-length vector, we use a recurrent neural network with a single 128-width Gated Recurrent Unit (GRU) (Cho et al., 2014a) layer. We take the final 128-dimensional output of the GRU as the pooled summarization of the sequence.\nTo compute the final dP -dimensional embedding from the 128-dimensional output of the GRU, we apply a fullyconnected layer to project the output to the desired dimensionality, followed by an activation function (e.g. softmax, tanh). The choice of activation function can constrain the information contained in the embedding and make learning easier by controlling its magnitude. After some exploration,\nwe found that a dP of 128 and a tanh activation perform well in practice."}, {"heading": "3.3. Reference signal feature representation", "text": "The choice of LR \u00d7 dR feature representation used as the input to the reference encoder architecture naturally impacts the aspects of prosody we can expect to learn. For example, a pitch track representation will not allow us to model prominence in some languages since it does not contain energy information. Similarly an MFCC representation may be somewhat pitch-invariant (depending on the number of coefficients retained), preventing us from modeling intonation. In this work, we decided to use the same perceptually-relevant summarization of the spectrum that (Wang et al., 2017a) does: the mel-warped spectrum (Stevens et al., 1937).\nThis choice of representation enables an interpretation of the resulting architecture as an RNN encoder-decoder (Cho et al., 2014b) conditioned on text and speaker identity. All it must model via its bottleneck representation is the unexplained variation in the signal, i.e. the prosody and recording environment. We illustrate this interpretation in Figure 3.\nWe explored more compact representations, such as pitch track and intensity, instead of mel spectrograms, which also produced successful results, but we focus on mel spectrograms in this paper."}, {"heading": "3.4. Variable-Length Prosody Embeddings", "text": "The use of a fixed-length prosody embedding poses an obvious scaling bottleneck, preventing the extension of this approach to longer utterances. An alternate implementation of the reference encoder in Section 3.2 uses the output of the\nGRU at every time step rather than just the final output. As with the fixed-length encoder, each GRU output is passed through a fully connected layer to transform it to the desired dimensionality. This can be interpreted as a low-bitrate representation of prosody similar to the proposal of Enhanced Phonetic Transcriptions in (Coile et al., 1994). To condition the Tacotron decoder on this sequence, we introduce a second attention head with an attention-aggregator module as proposed in (Wang et al., 2017b).\nIn our experiments, variable-length prosody embeddings are able to generalize to very long utterances; however, compared to fixed-length embeddings, variable-length embeddings are not as robust to text and speaker perturbations likely because they encode a stronger timing signal. Therefore, this paper focuses on fixed-length embeddings."}, {"heading": "4. Experiments and Results", "text": ""}, {"heading": "4.1. Datasets and training", "text": "We use the following datasets:\nSingle-speaker dataset: A single speaker high-quality English dataset of audiobook recordings by Catherine Byers (the speaker from the 2013 Blizzard Challenge). This dataset consists of 147 hours of recordings of 49 books, read in an animated and emotive storytelling style.\nMulti-speaker dataset: A proprietary high-quality English speech dataset consisting of 296 hours across 44 speakers (5 with Australian accents, 6 with British accents, 1 with an Indian accent, 2 with Singaporean accents, and 30 with United States accents).\nWe train our models for at least 200k steps with a minibatch size of 256 using the Adam optimizer (Kingma & Ba, 2015). We start with a learning rate of 1\u00d7 10\u22123 and decay it to 5\u00d7 10\u22124, 3\u00d7 10\u22124, 1\u00d7 10\u22124, and 5\u00d7 10\u22125 at step 50k, 100k, 150k, and 200k respectively. For baselines, we train models without the reference encoder architecture (Section 3)."}, {"heading": "4.2. Evaluation metrics", "text": "There are no generally-accepted metrics for prosody transfer. To measure performance, we adapt a number of metrics from general audio processing, each of which reflects an acoustic correlate of prosody. For all comparisons of predicted signals to target signals, we extend the shorter signal to the length of the longer signal using a domain-appropriate padding (e.g. 0 for a time-domain waveform, \u221213.8 for a log magnitude spectrogram with a 1\u00d7 10\u22126 stabilizing offset). All pitch and voicing metrics are computed using the\noutput of the YIN (De Cheveigne\u0301 & Kawahara, 2002) pitch tracking algorithm.\nMel Cepstral Distortion (MCDK) (Kubichek, 1993):\nMCDK = 1\nT T\u22121\u2211 t=0 \u221a\u221a\u221a\u221a K\u2211 k=1 ( ct,k \u2212 c\u2032t,k )2 Where ct,k,c\u2032t,k are the k-th mel frequency cepstral coefficient (MFCC) of the t-th frame from the reference and predicted audio. We sum the squared differences over the first K MFCCs, skipping ct,0 (overall energy).\nGross Pitch Error (GPE) (Nakatani et al., 2008): GPE = \u2211\nt 1 [|pt \u2212 p\u2032t| > 0.2pt]1[vt]1[v\u2032t]\u2211 t 1[vt]1[v \u2032 t]\nWhere pt,p\u2032t are the pitch signals from the reference and predicted audio, vt,v\u2032t are the voicing decisions from the reference and predicted audio, and 1 is the indicator function. The GPE measures the percentage of voiced frames that deviate in pitch by more than 20% compared to the reference.\nVoicing Decision Error (VDE) (Nakatani et al., 2008):\nVDE = \u2211T\u22121\nt=0 1[vt 6= v\u2032t] T\nWhere vt,v\u2032t are the voicing decisions for the reference and predicted audio, T is the total number of frames, and 1 is the indicator function."}, {"heading": "F0 Frame Error (FFE) (Chu & Alwan, 2009):\u2211T\u22121", "text": "t=0 1 [|pt \u2212 p\u2032t| > 0.2pt]1[vt]1[v\u2032t] + 1[vt 6= v\u2032t]\nT\nFFE measures the percentage of frames that either contain a 20% pitch error (according to GPE) or a voicing decision error (according to VDE).\nIn addition to these metrics, we propose a subjective (i.e., human) test structured as an AXY discrimination test that we refer to as an \u201canchored prosody side-by-side\u201d. A human rater is presented with three stimuli: a reference speech sample (A), and two competing samples (X and Y) to evaluate. The rater is asked to rate whether the prosody of X or Y is closer to that of the reference on a 7-point scale. The scale ranges from \u201cX is much closer\u201d to \u201cBoth are about the same distance\u201d to \u201cY is much closer\u201d, and can naturally be mapped on the integers from\u22123 to 3. Prior to collecting any ratings, we provide the raters with 4 examples of prosodic attributes to evaluate (intonation, stress, speaking rate, and\npauses), and explicitly instruct the raters to ignore audio quality or pronunciation differences. A screenshot of this user interface is included in Figure 7. For each triplet (A, X, Y) evaluated, we collect 4 independent ratings. No rater is used for more than 6 items in a single evaluation. To analyze the data from these subjective tests, we average the scores and compute 95% confidence intervals."}, {"heading": "4.3. Same-text Prosody Transfer", "text": "We first demonstrate that our model is capable of prosody transfer when the text is unchanged from that of the reference utterance."}, {"heading": "4.3.1. SPECTROGRAMS AND PITCH TRACKS", "text": "Figure 4 shows three spectrograms (reference, baseline model, prosody embedding model) for the same utterance. Note that the spectrogram from the model conditioned on a reference embedding bears a much stronger resemblance to the reference signal than that generated by an unconditioned model. In particular, notice that the spectrogram from the baseline model, which does not use a reference signal, exhibits noticeably different rhythm \u2013 for example, there is a long pause between the two halves of the utterance, and the utterance lasts much longer. By contrast, the output with a prosody embedding has the same length and pause characteristics as the reference audio; it also has recognizably similar harmonic and onset structure.\nFigure 5 shows the pitch tracks for the same triplet of utterances. We can see that the prosody embedding model closely follows the pitch contours of the reference, whereas the unconditioned model does something else entirely."}, {"heading": "4.3.2. QUANTITATIVE AND SUBJECTIVE EVALUATIONS", "text": "We evaluated synthesis of single- and multi-speaker models using two types of reference utterance. \u201cSame speaker\u201d indicates a reference utterance from the same speaker as the target, while \u201cunseen speaker\u201d refers to a reference utterance from a speaker unseen in training. For the multi-speaker model, we also tested synthesis with a speaker seen in training but different from the target speaker (\u201cseen speaker\u201d).\nWe present our findings in Table 1. The results show that augmenting Tacotron with a reference encoder allows it to match the reference prosody substantially more accurately. This is true for all baseline/model pairs in Table 1, and is independent of whether the reference speaker matches the target speaker. The objective metrics MCD13 and FFE also support this conclusion, both resulting in substantially lower values for the reference encoder model than for the baseline model.\nNote that when the target and reference speakers are different (i.e., when the reference in Table 1 is either \u201cseen speaker\u201d or \u201cunseen speaker\u201d), we must be careful to demonstrate that prosody transfer has been achieved. If the bottleneck allows too much information to flow through the reference encoder, for example, the overall model could simply copy the reference to the output. In this instance, listening to even a small number of outputs suffices to verify that the output speaker matches the target speaker, and that we have in fact achieved prosody transfer across speakers. However, further experiments, explored in Section 4.5, provide some surprising results."}, {"heading": "4.4. Templated Prosody Transfer", "text": "In addition to same-text prosody transfer, we also explore the robustness of our proposed model to changes in the synthesized text. Since the prosody embeddings we learn\ncapture prosodic features with some fine time detail, it isn\u2019t clear what it would mean to transfer these prosodic features to a radically different utterance. As expected, we find that drastic changes to the sentence or phrase structure result in undesirable prosody transfer. This use case may be more suited to models that capture less granular features of prosody such as emotion or style. (Wang et al., 2018), for example, applies a similar approach to learning representations of global style.\nNonetheless, we include a number of examples on our demo page demonstrating that text transformations can be performed without compromising intelligibility or desired prosody. This can be highly useful in building templated dialogue systems capable of synthesizing a template with a desired prosody."}, {"heading": "4.5. Preservation of Speaker Identity", "text": "In Table 1, the results of our \u201canchored prosody side-by-side\u201d subjective evaluation show that reference-based synthesis matches the reference audio significantly better than the baseline model. However, the evaluation does not assess whether the target speaker identity was preserved by the synthesis. This is not accidental: pitch, pacing, and other prosodic characteristics factor into speaker identity, and thus it is difficult to prescribe exactly which aspects of the\ntarget speaker\u2019s identity should be preserved during prosody transfer.\nThe audio samples we include on our demo page show that our model preserves many important aspects of speaker identity during prosody transfer. We include a grid of audio examples representative of typical performance of this system, with reference clips from 6 speakers with distinct accents. Each utterance is synthesized 6 times, each with a different target speaker. Notably, the prosody of each clip matches that of the reference, while the distinct accents and vocal tract properties of each speaker are preserved.\nHowever, listening to samples of a male voice controlling a female voice (and vice-versa) reveals that our prosody representation encodes pitch in an absolute manner. When controlled by a male reference signal, female target speakers sound as if they\u2019re imitating a person with a deeper voice. Similarly, when controlled by a female reference signal, male speakers sound as if they\u2019re imitating a person with a higher voice. This suggests that the prosody and speaker representations are somewhat entangled.\nTo quantify this entanglement, we designed a simple speaker identification model that takes varying types of acoustic input, and produces predictions of speaker identity from a\nuniverse of speakers known at training time. The architecture uses the same strided convolutions and GRU-based aggregation as the reference encoder architecture from Section 3.2, and is independently trained on ground truth mel spectrograms using the same 44-speaker dataset used to train our multi-speaker model. The architecture achieves over 99% accuracy on both the held-out ground truth and synthesized audio from our baseline 44-speaker model.\nWe then tested our prosody-enhanced Tacotron using this model. To do so, we first constructed pairs of all target speakers and reference utterances in the test set. We then used our prosody-enhanced Tacotron to generate mel spectrograms for these pairs, and fed the output into the speaker identification model. The speaker identification model identified the spectrograms as originating from the reference speaker in 61% of test set examples, and the target speaker only 21% of the time (ideally, the target speaker would be at 100%). We refer the reader to the audio samples to understand how surprising this is \u2013 the audio samples sound substantially more like the target speaker in every sample we\u2019ve listened to.\nSince our model seems to transfer prosody in a pitchabsolute manner, we ran a further experiment where we trained the speaker identification model on 13 Melfrequency cepstral coefficients (MFCCs) which contain less pitch content. In this case, the speaker identification model identified the utterances as originating from the reference speaker 41% of the time, and the target speaker 32% of the time, suggesting that, indeed, speaker-dependent pitch content is transferred from the reference to the output."}, {"heading": "4.6. Bottleneck Size and Shape", "text": "The dimensionality and activation used for the bottleneck substantially affect the information flow from the prosody\nreference encoder to the output. In this experiment, we use our single speaker as both the reference signal and target (we are essentially trying to conditionally autoencode the mel spectrograms given text). We plot the MCD13 and FFE metrics while varying the bottleneck size and activation in Figure 6, and include a series of audio samples on our demo page. We can conclude that increasing the bottleneck size allows for significantly more information flow from the reference to the output, allowing for better reproduction of the reference. More interestingly, using a softmax activation leads to a degradation of metrics in comparison to tanh: this is probably due to the exponential suppression of the non-maximal components in the softmax.\nThe quantitative metrics are in agreement with the audio samples provided on our demo page: larger bottlenecks with the tanh activation improve audio similarity, and the outputs are more faithful to the reference prosody. A potential tradeoff is that a narrower bottleneck would likely better preserve the speaker identity of the target speaker."}, {"heading": "5. Discussion and Future Work", "text": "In this work, we have demonstrated prosody transfer via an end-to-end learned representation of prosody directly from acoustic signals. While our system successfully transfers prosody from one speaker to another, it does so in a pitchabsolute manner. Future work should focus on encoding prosody in a pitch-relative manner so that speaker identity is more completely preserved during transfer.\nA substantial open question is how to disentangle the textual information implicit in the reference signal from the prosodic information. In Section 4.4, we showed that this is possible to some extent, especially when the transcripts are relatively close. But, more generally, this amounts to transferring or controlling prosody using utterances with different corresponding text transcripts. As noted earlier, this is a somewhat ill-defined task, and a more careful formalization of this problem is needed to make real progress.\nWe also defined objective and subjective metrics for evaluating prosody transfer, and evaluated our architecture on these benchmarks. Solidifying metrics that quantify all desired aspects of prosody transfer (e.g., prosodic similarity and the degree to which prosodic, textual, and speaker information are disentangled) is an important step in the long-term progression of end-to-end prosody work.\nFinally, given our construction of a prosody space, we would like to be able to sample from this space (i.e., generate prosody instead of transferring it). One could, for example, attempt to learn a prior distribution over the prosody space."}, {"heading": "Acknowledgements", "text": "The authors thank Aren Jansen and the Machine Hearing, Google Brain and Google TTS teams for their helpful discussions and feedback."}, {"heading": "A. Subjective Evaluation Template", "text": ""}], "year": 2018, "references": [{"title": "Deep voice 2: Multi-speaker neural text-to-speech", "authors": ["Ar\u0131k", "Sercan O", "Diamos", "Gregory", "Gibiansky", "Andrew", "Miller", "John", "Peng", "Kainan", "Ping", "Wei", "Raiman", "Jonathan", "Zhou", "Yanqi"], "venue": "arXiv preprint arXiv:1705.08947,", "year": 2017}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "authors": ["Cho", "Kyunghyun", "B van Merrienboer", "Gulcehre", "Caglar", "F Bougares", "H Schwenk", "Bengio", "Yoshua"], "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "authors": ["Cho", "Kyunghyun", "Van Merri\u00ebnboer", "Bart", "Gulcehre", "Caglar", "Bahdanau", "Dzmitry", "Bougares", "Fethi", "Schwenk", "Holger", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1406.1078,", "year": 2014}, {"title": "On using backpropagation for speech texture generation and voice conversion", "authors": ["Chorowski", "Jan", "Weiss", "Ron J", "Saurous", "Rif A", "Bengio", "Samy"], "venue": "arXiv preprint arXiv:1712.08363,", "year": 2017}, {"title": "Reducing f0 frame error of f0 tracking algorithms under noisy conditions with an unvoiced/voiced classification frontend", "authors": ["Chu", "Wei", "Alwan", "Abeer"], "venue": "In Acoustics, Speech and Signal Processing,", "year": 2009}, {"title": "PROTRAN: a prosody transplantation tool for text-to-speech applications", "authors": ["Coile", "Bert Van", "Tichelen", "Luc Van", "Vorstermans", "Annemie", "J.W. Jang", "M. Staessen"], "venue": "In The 3rd International Conference on Spoken Language Processing,", "year": 1994}, {"title": "Yin, a fundamental frequency estimator for speech and music", "authors": ["De Cheveign\u00e9", "Alain", "Kawahara", "Hideki"], "venue": "The Journal of the Acoustical Society of America,", "year": 1917}, {"title": "Unsupervised clustering of emotion and voice styles for expressive tts", "authors": ["Eyben", "Florian", "Buchholz", "Sabine", "Braunschweiler", "Norbert"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "year": 2012}, {"title": "Generating sequences with recurrent neural networks", "authors": ["Graves", "Alex"], "venue": "arXiv preprint arXiv:1308.0850,", "year": 2013}, {"title": "Signal estimation from modified short-time fourier transform", "authors": ["Griffin", "Daniel", "Lim", "Jae"], "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing,", "year": 1984}, {"title": "Automatic analysis of prosody for multilingual speech corpora", "authors": ["Hirst", "Daniel"], "venue": "Improvements in speech synthesis,", "year": 2001}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "authors": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "In International conference on machine learning,", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "authors": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "International Conference for Learning Representations,", "year": 2015}, {"title": "Non-parallel voice conversion using i-vector plda: Towards unifying speaker verification and transformation", "authors": ["Kinnunen", "Tomi", "Juvela", "Lauri", "Alku", "Paavo", "Yamagishi", "Junichi"], "year": 2017}, {"title": "Mel-cepstral distance measure for objective speech quality assessment", "authors": ["R. Kubichek"], "venue": "In Communications, Computers and Signal Processing,", "year": 1993}, {"title": "Intonational phonology", "authors": ["Ladd", "D Robert"], "year": 2008}, {"title": "Non-parallel training in voice conversion using an adaptive restricted boltzmann machine", "authors": ["Nakashika", "Toru", "Takiguchi", "Tetsuya", "Minami", "Yasuhiro"], "venue": "IEEE/ACM Trans. Audio, Speech and Lang. Proc.,", "year": 2045}, {"title": "A style control technique for hmmbased expressive speech synthesis", "authors": ["Nose", "Takashi", "Yamagishi", "Junichi", "Masuko", "Kobayashi", "Takao"], "venue": "IEICE TRANSACTIONS on Information and Systems,", "year": 2007}, {"title": "SLAM: Automatic stylization and labelling of speech melody", "authors": ["Obin", "Nicolas", "Beliao", "Julie", "Veaux", "Christophe", "Lacheret", "Anne"], "venue": "In Speech Prosody,", "year": 2014}, {"title": "AuToBI-a tool for automatic ToBI annotation", "authors": ["Rosenberg", "Andrew"], "venue": "In Interspeech, pp. 146\u2013149,", "year": 2010}, {"title": "ToBI: A standard for labeling english prosody", "authors": ["Silverman", "Kim", "Beckman", "Mary", "Pitrelli", "John", "Ostendorf", "Mori", "Wightman", "Colin", "Price", "Patti", "Pierrehumbert", "Janet", "Hirschberg", "Julia"], "venue": "In Second International Conference on Spoken Language Processing,", "year": 1992}, {"title": "A scale for the measurement of the psychological magnitude pitch", "authors": ["Stevens", "Stanley Smith", "Volkmann", "John", "Newman", "Edwin B"], "venue": "The Journal of the Acoustical Society of America,", "year": 1937}, {"title": "The tilt intonation model", "authors": ["Taylor", "Paul"], "year": 1998}, {"title": "Experiments with signal-driven symbolic prosody for statistical parametric speech synthesis", "authors": ["Tesser", "Fabio", "Sommavilla", "Giacomo", "Paci", "Giulio", "Cosi", "Piero"], "venue": "In Eighth ISCA Workshop on Speech Synthesis,", "year": 2013}, {"title": "Neural discrete representation learning", "authors": ["van den Oord", "Aaron", "Vinyals", "Oriol"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2017}, {"title": "Experimental and theoretical advances in prosody: A review", "authors": ["Wagner", "Michael", "Watson", "Duane G"], "venue": "Language and cognitive processes,", "year": 2010}, {"title": "Uncovering latent style factors for expressive speech synthesis", "authors": ["Wang", "Yuxuan", "RJ Skerry-Ryan", "Xiao", "Ying", "Stanton", "Daisy", "Shor", "Joel", "Battenberg", "Eric", "Clark", "Rob", "Saurous", "Rif A"], "venue": "ML4Audio Workshop,", "year": 2017}, {"title": "Style tokens: Unsupervised style modeling, control, and transfer in end-to-end speech synthesis", "authors": ["Wang", "Yuxuan", "Stanton", "Daisy", "Zhang", "Yu", "RJ Skerry-Ryan", "Battenberg", "Eric", "Shor", "Joel", "Xiao", "Ying", "Ren", "Fei", "Jia", "Ye", "Saurous", "Rif A"], "year": 2018}, {"title": "ToBI or not ToBI", "authors": ["Wightman", "Colin W"], "venue": "In Speech Prosody 2002, International Conference,", "year": 2002}, {"title": "Conditional restricted boltzmann machine for voice conversion", "authors": ["Wu", "Zhizheng", "Chng", "Eng Siong", "Li", "Haizhou"], "venue": "In ChinaSIP,", "year": 2013}], "id": "SP:e2314d7d9f116a8ed89b5924ce467fa8b737e468", "authors": [{"name": "RJ Skerry-Ryan", "affiliations": []}, {"name": "Eric Battenberg", "affiliations": []}, {"name": "Ying Xiao", "affiliations": []}, {"name": "Yuxuan Wang", "affiliations": []}, {"name": "Daisy Stanton", "affiliations": []}, {"name": "Joel Shor", "affiliations": []}, {"name": "Ron J. Weiss", "affiliations": []}, {"name": "Rob Clark", "affiliations": []}, {"name": "Rif A. Saurous", "affiliations": []}], "abstractText": "We present an extension to the Tacotron speech synthesis architecture that learns a latent embedding space of prosody, derived from a reference acoustic representation containing the desired prosody. We show that conditioning Tacotron on this learned embedding space results in synthesized audio that matches the prosody of the reference signal with fine time detail even when the reference and synthesis speakers are different. Additionally, we show that a reference prosody embedding can be used to synthesize text that is different from that of the reference utterance. We define several quantitative and subjective metrics for evaluating prosody transfer, and report results with accompanying audio samples from single-speaker and 44-speaker Tacotron models on a prosody transfer task.", "title": "Towards End-to-End Prosody Transfer  for Expressive Speech Synthesis with Tacotron"}