0,1,label2,summary_sentences
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2094–2099, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
"Neural network translation models, which learn mappings over real-valued vector representations in high-dimensional space, have recently achieved large gains in translation accuracy (Hu et al., 2014; Devlin et al., 2014; Sundermeyer et al., 2014; Auli et al., 2013; Schwenk, 2012; Sutskever et al., 2014; Bahdanau et al., 2015).
",1 Introduction,[0],[0]
"Notably, Devlin et al. (2014) proposed a neural network joint model (NNJM), which augments the n-gram neural network language model (NNLM) with an m-word source context window, as shown in Figure 1a.",1 Introduction,[0],[0]
"While this model is effective, the computation cost of using it in a large-vocabulary SMT task is quite expensive, as probabilities need to be normalized over the entire vocabulary.",1 Introduction,[0],[0]
"To solve this problem, Devlin et al. (2014) presented a technique to train the NNJM to be selfnormalized and avoided the expensive normalization cost during decoding.",1 Introduction,[0],[0]
"However, they also
note that this self-normalization technique sacrifices neural network accuracy, and the training process for the self-normalized neural network is very slow, as with standard maximum likelihood estimation (MLE).
",1 Introduction,[0],[0]
"To remedy the problem of long training times in the context of NNLMs, Vaswani et al. (2013) used a method called noise contrastive estimation (NCE).",1 Introduction,[0],[0]
"Compared with MLE, NCE does not require repeated summations over the whole vocabulary and performs nonlinear logistic regression to discriminate between the observed data and artificially generated noise.
",1 Introduction,[0],[0]
"This paper proposes an alternative framework of binarized NNJMs (BNNJM), which are similar to the NNJM, but use the current target word not as the output, but as the input of the neural network, estimating whether the target word under examination is correct or not, as shown in Figure 1b.",1 Introduction,[0],[0]
"Because the BNNJM uses the current target word as input, the information about the current target word can be combined with the context word information and processed in the hidden layers.
",1 Introduction,[0],[0]
"The BNNJM learns a simple binary classifier, given the context and target words, therefore it can be trained by MLE very efficiently.",1 Introduction,[0],[0]
"“Incorrect” target words for the BNNJM can be generated in the same way as NCE generates noise
2094
for the NNJM.",1 Introduction,[0],[0]
We present a novel noise distribution based on translation probabilities to train the NNJM and the BNNJM efficiently.,1 Introduction,[0],[0]
Let T = t|T |1 be a translation of S = s |S| 1 .,2 Neural Network Joint Model,[0],[0]
"The NNJM (Devlin et al., 2014) defines the following probability,
P (T |S) = ∏|T | i=1",2 Neural Network Joint Model,[0],[0]
"P ( ti|sai+(m−1)/2ai−(m−1)/2, t i−1 i−n+1 ) (1) where target word ti is affiliated with source word sai .",2 Neural Network Joint Model,[0],[0]
Affiliation ai is derived from the word alignments using heuristics1.,2 Neural Network Joint Model,[0],[0]
"To estimate these probabilities, the NNJM uses m source context words and n− 1 target history words as input to a neural network and performs estimation of unnormalized probabilities p (ti|C) before normalizing over all words in the target vocabulary V ,
P (ti|C) = p(ti|C)Z(C) Z (C) = ∑ ti′∈V p (ti′|C) (2)
where C stands for source and target context words as in Equation 1.
",2 Neural Network Joint Model,[0],[0]
"The NNJM can be trained on a word-aligned parallel corpus using standard MLE, but the cost of normalizing over the entire vocabulary to calculate the denominator in Equation 2 is quite large.",2 Neural Network Joint Model,[0],[0]
"Devlin et al. (2014)’s self-normalization technique can avoid normalization cost during decoding, but not during training.
",2 Neural Network Joint Model,[0],[0]
"NCE can be used to train NNLM-style models (Vaswani et al., 2013) to reduce training times.",2 Neural Network Joint Model,[0],[0]
NCE creates a noise distribution q,2 Neural Network Joint Model,[0],[0]
"(ti), selects k noise samples ti1, ..., tik for each ti and introduces a random variable v which is 1 for training examples and 0 for noise samples,
P (v = 1, ti|C)",2 Neural Network Joint Model,[0],[0]
"= 11+k · p(ti|C)Z(C) P (v = 0, ti|C) =",2 Neural Network Joint Model,[0],[0]
"k1+k · q (ti) .
",2 Neural Network Joint Model,[0],[0]
"NCE trains the model to distinguish training data from noise by maximize the conditional likelihood,
L = log P (v = 1|C, ti) + k∑
j=1
log P (v = 0|C, tik).
",2 Neural Network Joint Model,[0],[0]
"The normalization cost can be avoided by using p (ti|C) as an approximation of P (ti|C).2
1If ti aligns to exactly one source word, ai is the index of this source word; If ti aligns to multiple source words, ai is the index of the aligned word in the middle; If ti is unaligned, they inherit its affiliation from the closest aligned word.
",2 Neural Network Joint Model,[0],[0]
"2The theoretical properties of self-normalization techniques, including NCE and Devlin et al. (2014)’s method, are investigated by Andreas and Klein (2015).",2 Neural Network Joint Model,[0],[0]
"In this paper, we propose a new framework of the binarized NNJM (BNNJM), which is similar to the NNJM but learns not to predict the next word given the context, but solves a binary classification problem by adding a variable v ∈ {0, 1} that stands for whether the current target word ti is correctly/wrongly produced in terms of source context words sai+(m−1)/2ai−(m−1)/2 and target history words ti−1i−n+1 ,
P ( v|sai+(m−1)/2ai−(m−1)/2, t i−1 i−n+1, ti ) .
",3 Binarized NNJM,[0],[0]
"The BNNJM is learned by a feedforward neural network with m + n inputs{ s ai+(m−1)/2 ai−(m−1)/2, t i−1 i−n+1, ti } and two outputs for v = 1/0.",3 Binarized NNJM,[0],[0]
"Because the BNNJM uses the current target word as input, the information about the current target word can be combined with the context word information and processed in the hidden layers.",3 Binarized NNJM,[0],[0]
"Thus, the hidden layers can be used to learn the difference between correct target words and noise in the BNNJM, while in the NNJM the hidden layers just contain information about context words and only the output layer can be used to discriminate between the training data and noise, giving the BNNJM more power to learn this classification problem.
",3 Binarized NNJM,[0],[0]
"We can use the BNNJM probability in translation as an approximation for the NNJM as below,
P ( ti|sai+(m−1)/2ai−(m−1)/2, t i−1 i−n+1 )",3 Binarized NNJM,[0],[0]
"≈ P ( v = 1|sai+(m−1)/2ai−(m−1)/2, t i−1 i−n+1, ti ) .
",3 Binarized NNJM,[0],[0]
"As a binary classifier, the gradient for a single example in the BNNJM can be calculated efficiently by MLE without it being necessary to calculate the softmax over the full vocabulary.",3 Binarized NNJM,[0],[0]
"On the other hand, we need to create “positive” and “negative” examples for the classifier.",3 Binarized NNJM,[0],[0]
"Positive examples can be extracted directly from the word-aligned parallel corpus as〈 s ai+(m−1)/2 ai−(m−1)/2, t i−1 i−n+1, ti 〉 ; Negative examples can be generated for each positive example in the same way that NCE generates noise data as〈 s ai+(m−1)/2 ai−(m−1)/2, t i−1 i−n+1, ti ′ 〉 , where ti′ ∈ V \ {ti}.",3 Binarized NNJM,[0],[0]
"Vaswani et al. (2013) adopted the unigram probability distribution (UPD) to sample noise for train-
ing NNLMs with NCE,
q (ti′) = occur(ti ′)∑ ti ′′∈V occur(ti′′)
where occur (ti′) stands for how many times ti′ occurs in the training corpus.",4.1 Unigram Noise,[0],[0]
"In this paper, we propose a noise distribution specialized for translation models, such as the NNJM or BNNJM.
",4.2 Translation Model Noise,[0],[0]
Figure 2 gives a Chinese-to-English parallel sentence pair with word alignments to demonstrate the intuition behind our method.,4.2 Translation Model Noise,[0],[0]
"Focusing on sai=“安排”, this is translated into ti =“arrange”.",4.2 Translation Model Noise,[0],[0]
"For this positive example, UPD is allowed to sample any arbitrary noise, such as ti′ = “banana”.",4.2 Translation Model Noise,[0],[0]
"However, in this case, noise ti′ = “banana” is not useful for model training, as constraints on possible translations given by the phrase table ensure that “安排” will never be translated into “banana”.",4.2 Translation Model Noise,[0],[0]
"On the other hand, noise ti′ = “arranges” and “arrangement” are both possible translations of “安排” and therefore useful training data, that we would like our model to penalize.
",4.2 Translation Model Noise,[0],[0]
"Based on this intuition, we propose the use of another noise distribution that only uses ti′ that are possible translations of sai , i.e., ti ′ ∈",4.2 Translation Model Noise,[0],[0]
"U (sai) \ {ti}, where U (sai) contains all target words aligned to sai in the parallel corpus.
",4.2 Translation Model Noise,[0],[0]
"Because U (sai) may be quite large and contain many wrong translations caused by wrong alignments, “banana” may actually be included in U (“安排”).",4.2 Translation Model Noise,[0],[0]
"To mitigate the effect of uncommon examples, we use a translation probability distribution (TPD) to sample noise ti′ from U (sai)",4.2 Translation Model Noise,[0],[0]
\,4.2 Translation Model Noise,[0],[0]
"{ti} as follows,
q (ti′|sai) = align(sai ,ti′)∑
ti ′′∈U(sai )
align(sai ,ti′′)
where align (sai , ti ′) is how many times ti′ is aligned to sai in the parallel corpus.",4.2 Translation Model Noise,[0],[0]
"Note that ti could be unaligned, in which case we assume that it is aligned to a special null word.",4.2 Translation Model Noise,[0],[0]
Noise for unaligned words is sampled according to the TPD of the null word.,4.2 Translation Model Noise,[0],[0]
"If several target/source words are aligned to one source/target word, we
choose to combine these target/source words as a new target/source word.3",4.2 Translation Model Noise,[0],[0]
"We evaluated the effectiveness of the proposed approach for Chinese-to-English (CE), Japanese-toEnglish (JE) and French-to-English (FE) translation tasks.",5.1 Setting,[0],[0]
"The datasets officially provided for the patent machine translation task at NTCIR-9 (Goto et al., 2011) were used for the CE and JE tasks.",5.1 Setting,[0],[0]
The development and test sets were both provided for the CE task while only the test set was provided for the JE task.,5.1 Setting,[0],[0]
"Therefore, we used the sentences from the NTCIR-8 JE test set as the development set.",5.1 Setting,[0],[0]
"Word segmentation was done by BaseSeg (Zhao et al., 2006) for Chinese and Mecab4 for Japanese.",5.1 Setting,[0],[0]
"For the FE language pair, we used standard data for the WMT 2014 translation task.",5.1 Setting,[0],[0]
"The training sets for CE, JE and FE tasks contain 1M, 3M and 2M sentence pairs, respectively.
",5.1 Setting,[0],[0]
"For each translation task, a recent version of Moses HPB decoder (Koehn et al., 2007) with the training scripts was used as the baseline (Base).",5.1 Setting,[0],[0]
"We used the default parameters for Moses, and a 5-gram language model was trained on the target side of the training corpus using the IRSTLM Toolkit5 with improved Kneser-Ney smoothing.",5.1 Setting,[0],[0]
"Feature weights were tuned by MERT (Och, 2003).
",5.1 Setting,[0],[0]
"The word-aligned training set was used to learn the NNJM and the BNNJM.6 For both NNJM and BNNJM, we set m = 7 and n = 5.",5.1 Setting,[0],[0]
The NNJM was trained by NCE using UPD and TPD as noise distributions.,5.1 Setting,[0],[0]
"The BNNJM was trained by standard MLE using UPD and TPD to generate negative examples.
",5.1 Setting,[0],[0]
The number of noise samples for NCE was set to be 100.,5.1 Setting,[0],[0]
"For the BNNJM, we used only one negative example for each positive example in each training epoch, as the BNNJM needs to calculate
3The processing for multiple alignments helps sample more useful negative examples for TPD, and had little effect on the translation performance when UPD was used as the noise distribution for the NNJM and the BNNJM in our preliminary experiments.
",5.1 Setting,[0],[0]
"4http://sourceforge.net/projects/mecab/files/ 5http://hlt.fbk.eu/en/irstlm 6Both the NNJM and the BNNJM had one hidden layer, 100 hidden nodes, input embedding dimension 50, output embedding dimension 50.",5.1 Setting,[0],[0]
A small set of training data was used as validation data.,5.1 Setting,[0],[0]
"The training process was stopped when validation likelihood stopped increasing.
",5.1 Setting,[0],[0]
the whole neural network (not just the output layer like the NNJM) for each noise sample and thus noise computation is more expensive.,5.1 Setting,[0],[0]
"However, for different epochs, we resampled the negative example for each positive example, so the BNNJM can make use of different negative examples.",5.1 Setting,[0],[0]
"Table 1 shows how many epochs these two models needed and the training time for each epoch on a 10-core 3.47GHz Xeon X5690 machine.7 Translation results are shown in Table 2.
",5.2 Results and Discussion,[0],[0]
"We can see that using TPD instead of UPD as a noise distribution for the NNJM trained by NCE can speed up the training process significantly, with a small improvement in performance.",5.2 Results and Discussion,[0],[0]
"But for the BNNJM, using different noise distributions affects translation performance significantly.",5.2 Results and Discussion,[0],[0]
"The BNNJM with UPD does not improve over the baseline system, likely due to the small number of noise samples used in training the BNNJM, while the BNNJM with TPD achieves good performance, even better than the NNJM with TPD on the Chinese-to-English and French-to-English translation tasks.
",5.2 Results and Discussion,[0],[0]
"From Table 2, the NNJM does not improve translation performance significantly on the FE task.",5.2 Results and Discussion,[0],[0]
"Note that the baseline BLEU for the FE
7The decoding time for the NNJM and the BNNJM were similar, since the NNJM trained by NCE uses p (ti|C) as an approximation of P (ti|C) without normalization and the BNNJM only needs to be normalized over two output neurons.
task is lower than CE and JE tasks, indicating that learning is harder for the FE task than CE and JE tasks.",5.2 Results and Discussion,[0],[0]
"The validation perplexities of the NNJM with UPD for CE, JE and FE tasks are 4.03, 3.49 and 8.37.",5.2 Results and Discussion,[0],[0]
"Despite these difficult learning circumstances and lack of large gains for the NNJM, the BNNJM improves translations significantly for the FE task, suggesting that the BNNJM is more robust to difficult translation tasks that are hard for the NNJM.
",5.2 Results and Discussion,[0],[0]
Table 3 gives Chinese-to-English translation examples to demonstrate how the BNNJM (with TPD) helps to improve translations over the NNJM (with TPD).,5.2 Results and Discussion,[0],[0]
"In this case, the BNNJM helps to translate the phrase “该 移动 持续 到” better.",5.2 Results and Discussion,[0],[0]
Table 4 gives translation scores for these two translations calculated by the NNJM and the BNNJM.,5.2 Results and Discussion,[0],[0]
"Context words are used for predictions but not shown in the table.
",5.2 Results and Discussion,[0],[0]
"As can be seen, the BNNJM prefers T2 while the NNJM prefers T1.",5.2 Results and Discussion,[0],[0]
"Among these predictions, the NNJM and the BNNJM predict the translation for “到” most differently.",5.2 Results and Discussion,[0],[0]
"The NNJM clearly predicts that in this case “到” should be translated into “to” more than “until”, likely because this example rarely occurs in the training corpus.",5.2 Results and Discussion,[0],[0]
"However, the BNNJM prefers “until” more than “to”, which
demonstrates the BNNJM’s robustness to less frequent examples.",5.2 Results and Discussion,[0],[0]
"Finally, we examine the translation results to explore why the BNNJM with TPD did not outperform the NNJM with TPD for the JE translation task, as it did for the other translation tasks.",5.3 Analysis for JE Translation Results,[0],[0]
"We found that using the BNNJM instead of the NNJM on the JE task did improve translation quality significantly for infrequent words, but not for frequent words.
",5.3 Analysis for JE Translation Results,[0],[0]
"First, we describe how we estimate translation quality for infrequent words.",5.3 Analysis for JE Translation Results,[0],[0]
"Suppose we have a test set S, a reference set R and a translation set T with I sentences, Si (1 ≤",5.3 Analysis for JE Translation Results,[0],[0]
i ≤,5.3 Analysis for JE Translation Results,[0],[0]
"I) , Ri (1 ≤",5.3 Analysis for JE Translation Results,[0],[0]
i ≤,5.3 Analysis for JE Translation Results,[0],[0]
"I) , Ti (1 ≤",5.3 Analysis for JE Translation Results,[0],[0]
i ≤,5.3 Analysis for JE Translation Results,[0],[0]
I),5.3 Analysis for JE Translation Results,[0],[0]
"Ti contains J individual words, Wij ∈Words (Ti) To (Wij) is how many times Wij occurs in Ti and Ro (Wij) is how many times Wij occurs in Ri.
",5.3 Analysis for JE Translation Results,[0],[0]
"The general 1-gram translation accuracy (Papineni et al., 2002) is calculated as,
Pg =
I∑ i=1 J∑ j=1 min(To(Wij),Ro(Wij))
I∑ i=1 J∑ j=1 To(Wij)
",5.3 Analysis for JE Translation Results,[0],[0]
"This general 1-gram translation accuracy does not distinguish word frequency.
",5.3 Analysis for JE Translation Results,[0],[0]
"We use a modified 1-gram translation accuracy that weights infrequent words more heavily,
Pc =
I∑ i=1 J∑ j=1 min(To(Wij),Ro(Wij))· 1 Occur(Wij)
I∑ i=1 J∑ j=1 To(Wij)
where Occur (Wij) is how many times Wij occurs in the whole reference set.",5.3 Analysis for JE Translation Results,[0],[0]
"Note Pc will not be 1 even in the case of completely accurate translations, but it can approximately reflect infrequent word translation accuracy, since correct frequent word translations contribute less to Pc.
Table 5 shows Pg and Pc for different translation tasks.",5.3 Analysis for JE Translation Results,[0],[0]
"It can be seen that the BNNJM improves infrequent word translation quality similarly for all translation tasks, but improves general translation quality less for the JE task than the other translation tasks.",5.3 Analysis for JE Translation Results,[0],[0]
"We conjecture that the reason why the BNNJM is less useful for frequent word translations on the JE task is the fact that the JE parallel corpus has less accurate function word alignments than other language pairs, as the
grammatical features of Japanese and English are quite different.8 Wrong function word alignments will make noise sampling less effective and therefore lower the BNNJM performance for function word translations.",5.3 Analysis for JE Translation Results,[0],[0]
"Although wrong word alignments will also make noise sampling less effective for the NNJM, the BNNJM only uses one noise sample for each positive example, so wrong word alignments affect the BNNJM more than the NNJM.",5.3 Analysis for JE Translation Results,[0],[0]
Xu et al. (2011) proposed a method to use binary classifiers to learn NNLMs.,6 Related Work,[0],[0]
"But they also used the current target word in the output, similarly to NCE.",6 Related Work,[0],[0]
"The BNNJM uses the current target word as input, so the information about the current target word can be combined with the context word information and processed in hidden layers.
",6 Related Work,[0],[0]
Mauser et al. (2009) presented discriminative lexicon models to predict target words.,6 Related Work,[0],[0]
"They train a separate classifier for each target word, as these lexicon models use discrete representations of words and different classifiers do not share features.",6 Related Work,[0],[0]
"In contrast, the BNNJM uses real-valued vector representations of words and shares features, so we train one classifier and can use the similarity information between words.",6 Related Work,[0],[0]
"This paper proposes an alternative to the NNJM, the BNNJM, which learns a binary classifier that takes both the context and target words as input and combines all useful information in the hidden layers.",7 Conclusion,[0],[0]
We also present a novel noise distribution based on translation probabilities to train the BNNJM efficiently.,7 Conclusion,[0],[0]
"With the improved noise sampling method, the BNNJM can achieve comparable performance with the NNJM and even improve the translation results over the NNJM on Chineseto-English and French-to-English translations.
",7 Conclusion,[0],[0]
8Infrequent words are usually content words and frequent words are usually function words.,7 Conclusion,[0],[0]
"The neural network joint model (NNJM), which augments the neural network language model (NNLM) with an m-word source context window, has achieved large gains in machine translation accuracy, but also has problems with high normalization cost when using large vocabularies.",abstractText,[0],[0]
"Training the NNJM with noise-contrastive estimation (NCE), instead of standard maximum likelihood estimation (MLE), can reduce computation cost.",abstractText,[0],[0]
"In this paper, we propose an alternative to NCE, the binarized NNJM (BNNJM), which learns a binary classifier that takes both the context and target words as input, and can be efficiently trained using MLE.",abstractText,[0],[0]
We compare the BNNJM and NNJM trained by NCE on various translation tasks.,abstractText,[0],[0]
A Binarized Neural Network Joint Model for Machine Translation,title,[0],[0]
"We are interested in time series settings where we observe data {Yt ∈ Y : t = 1, . . .",1. Introduction - Problem Statement,[0],[0]
", L}.",1. Introduction - Problem Statement,[0],[0]
We consider problems where the observations are explained by a latent structure which assigns objects to features and this feature allocation changes over time.,1. Introduction - Problem Statement,[0],[0]
"For instance, consider the topics covered by a number of newspapers over time; some topics “die” while new ones are “born”.",1. Introduction - Problem Statement,[1.0],"['For instance, consider the topics covered by a number of newspapers over time; some topics “die” while new ones are “born”.']"
"The topic coverage of each paper is its latent feature allocation which could be modelled with an Indian buffet process (Griffiths & Ghahramani, 2011, IBP).",1. Introduction - Problem Statement,[1.0],"['The topic coverage of each paper is its latent feature allocation which could be modelled with an Indian buffet process (Griffiths & Ghahramani, 2011, IBP).']"
"While static feature allocation models are well studied, these are not able to handle the time series nature of many datasets.",1. Introduction - Problem Statement,[1.0],"['While static feature allocation models are well studied, these are not able to handle the time series nature of many datasets.']"
"We propose a process that extends the IBP by allowing the feature allocation to evolve over the covariate as a result of “birth” and “death” of features.
1University of Oxford, Oxford, UK 2Stanford University, California, USA 3University of Cambridge, Cambridge, UK 4Uber AI Labs, SF, California, USA.",1. Introduction - Problem Statement,[0],[0]
"Correspondence to: Konstantina Palla <konstantina.palla@gmail.com>.
",1. Introduction - Problem Statement,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction - Problem Statement,[0],[0]
Copyright 2017 by the author(s).,1. Introduction - Problem Statement,[0],[0]
"We target problems where the data depends on a covariate, such as time or space, and is explained by a latent structure, in particular a (multi-membership) clustering of the data points.",2. Related Work,[0],[0]
The observations are result of the underlying partitioning and its evolution over the covariate.,2. Related Work,[0],[0]
Typical models fall in two main categories: clustering and feature allocation.,2. Related Work,[0],[0]
"The former allow each data point to belong to one and only one class (cluster), while the latter let each data point belong to multiple groups (features).",2. Related Work,[0],[0]
"Bayesian nonparametric approaches are primarily based on the Chinese restaurant process (CRP, Aldous, 1983) or the Indian buffet process (IBP, Griffiths & Ghahramani, 2005) corresponding to the two categories.",2. Related Work,[0],[0]
"In particular, a sample from a CRP is an assignment of data points to disjoint classes (a clustering), while a sample from an IBP is an allocation of the data points to (possibly) overlapping classes (a feature allocation).",2. Related Work,[0],[0]
"Dependent nonparametric processes extend distributions over partitions to distributions over collections of partitions indexed by locations in some covariate space, such as R+ (e.g. continuous time), Z (e.g. discrete time), or Rd (e.g. geographical location).",2. Related Work,[0],[0]
"Teh et al. (2013) define such a process based on the duality between Kingman’s coalescent (Kingman, 1982) and the Dirichlet diffusion tree (Neal, 2003).",2. Related Work,[0],[0]
In the resulting “Fragmentation-Coagulation” process (FCP) a partitioning of the data points evolves over the covariate undergoing fragmentation and coagulation events while maintaining CRP marginals.,2. Related Work,[0],[0]
"More recently, Palla et al. (2013) derived a dependent partition-valued process (DPVP) on an arbitrary covariate space which, like the FCP, is exchangeable and has CRP distributed marginals.",2. Related Work,[0],[0]
"In the setting of feature allocations, Williamson et al. (2010) propose a nonparametric process, the dependent IBP (dIBP), with IBP distributed marginals and in which the feature allocations are coupled over the covariate space using a Gaussian process (GP, Rasmussen & Williams, 2006).",2. Related Work,[0],[0]
"In a similar vein, Van Gael et al. (iFHMM, 2009) define the Markov Indian Buffet process (mIBP), a probability distribution over a potentially infinite number of binary Markov chains evolving in discrete time.",2. Related Work,[0],[0]
"They use the mIBP to extend the factorial hidden Markov model (FHMM, Ghahramani & Jordan, 1997) to the infinite FHMM (iFHMM).
",2. Related Work,[0],[0]
"In this paper, we address the problem of dependence for
binary latent feature models.",2. Related Work,[0],[0]
"We propose a process that extends the IBP by allowing features to be “born” and “die” at times learnt by the model, while maintaining the essential mathematical properties of the IBP.",2. Related Work,[0],[0]
The process is a Markov Jump process (MJP) where the events are the birth or the death of a feature.,2. Related Work,[0],[0]
The idea is closely related to the FCP where the events are either a fragmentation of a cluster or a coagulation of two clusters.,2. Related Work,[0],[0]
"The partitions at each location in the FCP are marginally a sample from a Chinese restaurant process, while the feature allocations in the BDFP are marginally samples from an IBP.",2. Related Work,[0],[0]
"Compared to the dIBP, both processes model feature allocations evolving over the covariate.",2. Related Work,[0],[0]
"However, while in the dIBP the assignment of data points to a feature might change over the covariate, in our process, it remains the same until the feature dies.",2. Related Work,[0],[0]
"In the case of the iFHMM, the authors model the dependence of a feature allocation on a discrete time variable as opposed to our process where continuous covariate space is assumed.",2. Related Work,[0],[0]
"Moreover, in the iFHMM, the marginal distribution of a feature allocation is analogous but not equal to an IBP.",2. Related Work,[0],[0]
We call the proposed process the birth-death feature allocation process (BDFP).,2. Related Work,[0],[0]
"The BDFP is exchangeable, projective, stationary and reversible, and its equilibrium distribution is given by the Indian buffet process.",2. Related Work,[0],[0]
Consider a dataset with N data points indexed by integers,3. Feature Allocations and the Indian Buffet Process,[0],[0]
"[N ] := {1, 2, . . .",3. Feature Allocations and the Indian Buffet Process,[0],[0]
", N} (allowing N → ∞).",3. Feature Allocations and the Indian Buffet Process,[0],[0]
Each datapoint n is associated with a binary vector Zn of length K that defines its feature allocation; Znk = 1 if datapoint n has feature k and Znk = 0 otherwise.,3. Feature Allocations and the Indian Buffet Process,[1.0],['Each datapoint n is associated with a binary vector Zn of length K that defines its feature allocation; Znk = 1 if datapoint n has feature k and Znk = 0 otherwise.']
The potential total number of features K may be infinite.,3. Feature Allocations and the Indian Buffet Process,[0],[0]
The binary matrix Z[N ] =,3. Feature Allocations and the Indian Buffet Process,[0],[0]
"[ZT1 ,Z T 2 , . . .",3. Feature Allocations and the Indian Buffet Process,[0],[0]
",Z T N ]",3. Feature Allocations and the Indian Buffet Process,[0],[0]
"T specifies a random feature allocation of [N ], while ZN denotes the space of all feature allocations of [N ], i.e. Z[N ] ∈ ZN .",3. Feature Allocations and the Indian Buffet Process,[0],[0]
"We define mk as the number of datapoints that possess feature k, K+ = ∑2N−1 h=1",3. Feature Allocations and the Indian Buffet Process,[0],[0]
Kh as the number of features for which mk > 0,3. Feature Allocations and the Indian Buffet Process,[0],[0]
"and Kh as the multiplicity of feature h, that is the number of times the same binary column h appears in Z[N ].",3. Feature Allocations and the Indian Buffet Process,[0],[0]
"Under the IBP (Griffiths & Ghahramani, 2011), the probability of a matrix Z[N ] is
g([Z[N ]];α) = αK+∏H h=1Kh!",3. Feature Allocations and the Indian Buffet Process,[0],[0]
exp(−αHN ) K+∏ k=1 (N −mk)!(mk,3. Feature Allocations and the Indian Buffet Process,[0],[0]
"− 1) N !
(1)
where α > 0 is the concentration parameter, HN =∑N j=1 1 j is the N th harmonic number and H ≤ 2
N",3. Feature Allocations and the Indian Buffet Process,[0],[0]
"− 1 is the number of distinct nonzero features in the allocation.
",3. Feature Allocations and the Indian Buffet Process,[0],[0]
"Thibaux & Jordan (2007) showed one can construct the Indian buffet process from a Beta-Bernoulli process using the
following two stage sampling process for n = 1, . . .",3. Feature Allocations and the Indian Buffet Process,[0.9926183906109829],"['Thibaux & Jordan (2007) showed one can construct the Indian buffet process from a Beta-Bernoulli process using the following two stage sampling process for n = 1, .']"
", N :
B|c, µ0 ∼BP(c, µ0) Zn|B ∼ BeP(B) (2) where B = ∑∞",3. Feature Allocations and the Indian Buffet Process,[0],[0]
k=1 ωkδθk and Z = ∑∞,3. Feature Allocations and the Indian Buffet Process,[0],[0]
k=1 fkδθk .,3. Feature Allocations and the Indian Buffet Process,[0],[0]
"First a draw B is sampled from the Beta process BP(cµ0) (Hjort, 1990) with µ0 as the base distribution.",3. Feature Allocations and the Indian Buffet Process,[0],[0]
"B is a set of pairs (ωk, θk) sampled from a Poisson process on the product space",3. Feature Allocations and the Indian Buffet Process,[0],[0]
"[0, 1] × Θ with Lévy intensity ν(dω,dθ) = cω−1(1 − ω)c−1dωµ0(dθ).",3. Feature Allocations and the Indian Buffet Process,[0],[0]
"Then, B is used as the atomic hazard measure for a Bernoulli process BeP(B).",3. Feature Allocations and the Indian Buffet Process,[0],[0]
"Each Zn is a draw from the Bernoulli process and constitutes a collection of atoms of unit mass on Θ. Then, Zn is a binary vector containing the {fk}∞k=1 values resulting from tossing a countably infinite sequence of (conditionally independent) coins with success probabilities ωk, i.e. fk|ωk ∼ Bernoulli(ωk).",3. Feature Allocations and the Indian Buffet Process,[0],[0]
"This construction allows the use of de Finetti’s theorem (de Finetti, 1931) that lets the joint distribution of the rows to be written as
P (Z1, . . .",3. Feature Allocations and the Indian Buffet Process,[0.9950782930741754],"['This construction allows the use of de Finetti’s theorem (de Finetti, 1931) that lets the joint distribution of the rows to be written as P (Z1, .']"
",ZN ) = ∫ [ N∏ n=1 P (Zn|B) ] dP",3. Feature Allocations and the Indian Buffet Process,[0],[0]
"(B) (3)
where B is the random measure that renders the variables Zn conditionally independent.",3. Feature Allocations and the Indian Buffet Process,[0],[0]
"Equation (3) shows the exchangeability of the rows of Zn, since they can be described as a mixture of Bernoulli processes.",3. Feature Allocations and the Indian Buffet Process,[1.0],"['Equation (3) shows the exchangeability of the rows of Zn, since they can be described as a mixture of Bernoulli processes.']"
We consider a continuous-time Markov process (Z(t))t≥0 in which each Z(t) is a random feature allocation taking values in the discrete space ZN .,4. Birth-Death Process for Feature Allocation,[0],[0]
"The state space is countably infinite; it is determined by all the possible feature allocations defined by N datapoints and K features, where K → ∞. The Markov process (Z(t)) evolves over time jumping to different states (feature allocations).",4. Birth-Death Process for Feature Allocation,[0],[0]
"Let {t1, . . .",4. Birth-Death Process for Feature Allocation,[0],[0]
", tJ ∈ R : J ∈ N} denote the times when the chain jumps such that tj = inf{τ ≥ tj−1 : Z(τ) 6= Z(tj−1)} and Z(tj) ∈ ZN .",4. Birth-Death Process for Feature Allocation,[0],[0]
These jumps are a result of a birth or a death of a feature.,4. Birth-Death Process for Feature Allocation,[0],[0]
"The process (Z(t)) can only jump to neighbouring states, i.e. if the chain is currently at state Z(tj) = s, then at time tj+1 it transitions to Z(tj+1) = s′ where a new feature is created or an existing feature is deleted after a birth or a death event respectively.",4. Birth-Death Process for Feature Allocation,[1.0],"['The process (Z(t)) can only jump to neighbouring states, i.e. if the chain is currently at state Z(tj) = s, then at time tj+1 it transitions to Z(tj+1) = s′ where a new feature is created or an existing feature is deleted after a birth or a death event respectively.']"
Let ZsN ⊂ ZN be the discrete space of neighboring states to state s.,4. Birth-Death Process for Feature Allocation,[0],[0]
The process is time homogeneous with transition probabilities P(Z(t + y) = s′|Z(y) = s) = P(Z(t) = s′|Z(0),4. Birth-Death Process for Feature Allocation,[0],[0]
"= s) = pss′(t) for all t, y, where s, s′ ∈ ZN .",4. Birth-Death Process for Feature Allocation,[0],[0]
"At time tj+1 the process jumps to the next state Z(tj+1) = s′ with rate determined by the current state Z(t) = s and the corresponding event, i.e birth or death.",4. Birth-Death Process for Feature Allocation,[0],[0]
"More specifically,
• Birth: Suppose s ∈ ZN is a feature allocation with Ks nonzero features and s′ ∈ ZsN is another feature
allocation that differs from s in having one additional feature of size |a| so that K ′s =",4. Birth-Death Process for Feature Allocation,[0],[0]
Ks + 1.,4. Birth-Death Process for Feature Allocation,[0],[0]
"We choose the transition rate from s to s′ as
qss′ =",4. Birth-Death Process for Feature Allocation,[0],[0]
"R (|a| − 1)!(N − |a|)!
N !",4. Birth-Death Process for Feature Allocation,[0],[0]
"(4)
where R > 0 is a parameter governing the birth rate.",4. Birth-Death Process for Feature Allocation,[1.000000022271673],['(4) where R > 0 is a parameter governing the birth rate.']
The new feature a is a binary column of length N .,4. Birth-Death Process for Feature Allocation,[0],[0]
"There are ( N |a| ) binary formulations for this fea-
ture and 2N",4. Birth-Death Process for Feature Allocation,[0],[0]
− 1 = ∑N n=1 ( N n ),4. Birth-Death Process for Feature Allocation,[0],[0]
"for all possible feature births and thus, the total birth rate from s is∑N n=1 ( N n )",4. Birth-Death Process for Feature Allocation,[0],[0]
R (n−1)!(N−n)!N !,4. Birth-Death Process for Feature Allocation,[0],[0]
= R ∑N n=1 1 n =,4. Birth-Death Process for Feature Allocation,[0],[0]
"R ·HN
where HN = ∑N n=1 1/n is the N -th harmonic num-
ber and n = |a| .",4. Birth-Death Process for Feature Allocation,[0],[0]
•,4. Birth-Death Process for Feature Allocation,[0],[0]
"Death: The rate of transitioning from s′ to s is
qs′s =",4. Birth-Death Process for Feature Allocation,[0],[0]
"Rr
α (5)
",4. Birth-Death Process for Feature Allocation,[0],[0]
where D = Ra is a parameter governing the death rate and r is the multiplicity of the feature in s′ that dies.,4. Birth-Death Process for Feature Allocation,[0],[0]
The multiplicity r is the combinatorial factor that accounts for all the possible ways of obtaining the same equivalence class as defined in Griffiths & Ghahramani (2011) .,4. Birth-Death Process for Feature Allocation,[0],[0]
"There are Ks′ features (including repetitions of the same feature) in s′ that might “die”, thus the total death rate from s′ is RKs′α .
",4. Birth-Death Process for Feature Allocation,[0],[0]
"The total rate of transition out of state s ∈ ZN is the sum of the total birth and death rates, qs = RHN + RKsα =
R ( HN + Ks α ) .",4. Birth-Death Process for Feature Allocation,[0],[0]
"We call (Z(t))t>=0 a birth-death feature allocation process with birth rate R and death rate Rα and write BDFP(α,R).
",4. Birth-Death Process for Feature Allocation,[0],[0]
Theorem 1.,4. Birth-Death Process for Feature Allocation,[0],[0]
The Markov process (Z(t))t≥0 is irreducible and has stationary distribution IBP(α).,4. Birth-Death Process for Feature Allocation,[1.0],['The Markov process (Z(t))t≥0 is irreducible and has stationary distribution IBP(α).']
"Furthermore, it is reversible.
",4. Birth-Death Process for Feature Allocation,[0],[0]
Proof.,4. Birth-Death Process for Feature Allocation,[0],[0]
A continuous time Markov chain is irreducible if it is possible to eventually get from every state to every other state with positive probability.,4. Birth-Death Process for Feature Allocation,[1.0],['A continuous time Markov chain is irreducible if it is possible to eventually get from every state to every other state with positive probability.']
"It is reversible if detailed balance holds, i.e. there is a probability distribution π on ZN such that πsqss′ = πs′qs′s for all s, s′ ∈ ZN .",4. Birth-Death Process for Feature Allocation,[1.0],"['It is reversible if detailed balance holds, i.e. there is a probability distribution π on ZN such that πsqss′ = πs′qs′s for all s, s′ ∈ ZN .']"
Then π is also the invariant (equilibrium) distribution of the Markov chain.,4. Birth-Death Process for Feature Allocation,[1.0],['Then π is also the invariant (equilibrium) distribution of the Markov chain.']
"The chain in BDFP is irreducible, because for any T > 0 and any two distinct feature allocations γ, ρ ∈ ZN , there is a positive probability that if it starts at γ ∈ ZN , it will end at ρ ∈ ZN .",4. Birth-Death Process for Feature Allocation,[0],[0]
Reversibility and the equilibrium distribution can be demonstrated by detailed balance.,4. Birth-Death Process for Feature Allocation,[1.0],['Reversibility and the equilibrium distribution can be demonstrated by detailed balance.']
"Suppose γ, ρ are feature allocations such that γ, ρ ∈ ZN and ρ differs from γ in that it has one additional feature a of size |a|.",4. Birth-Death Process for Feature Allocation,[1.0],"['Suppose γ, ρ are feature allocations such that γ, ρ ∈ ZN and ρ differs from γ in that it has one additional feature a of size |a|.']"
"The number of (nonzero) features in ρ isKρ = Kγ+1.
",4. Birth-Death Process for Feature Allocation,[1.0000000686127544],['The number of (nonzero) features in ρ isKρ = Kγ+1.']
"Then,
g(γ;α)qγρ = αKγ
Π",4. Birth-Death Process for Feature Allocation,[0],[0]
Hγ h=1Kh!,4. Birth-Death Process for Feature Allocation,[0],[0]
exp (−αHN ) Kγ∏ k=1 (N −mk)!(mk,4. Birth-Death Process for Feature Allocation,[0],[0]
− 1)!,4. Birth-Death Process for Feature Allocation,[0],[0]
N !,4. Birth-Death Process for Feature Allocation,[0],[0]
R (|a| − 1)!(N − |a|)!,4. Birth-Death Process for Feature Allocation,[0],[0]
"N !
",4. Birth-Death Process for Feature Allocation,[0],[0]
"mKγ+1=|α| = αKγ+1
αΠ",4. Birth-Death Process for Feature Allocation,[0],[0]
Hγ h=1Kh!,4. Birth-Death Process for Feature Allocation,[0],[0]
exp (−αHN ) Kγ+1∏ k=1 (N −mk)!(mk,4. Birth-Death Process for Feature Allocation,[0],[0]
− 1)!,4. Birth-Death Process for Feature Allocation,[0],[0]
N !,4. Birth-Death Process for Feature Allocation,[0],[0]
"R
= αKρ
rαΠ",4. Birth-Death Process for Feature Allocation,[0],[0]
Hγ h=1Kh!,4. Birth-Death Process for Feature Allocation,[0],[0]
exp (−αHN ) Kρ∏,4. Birth-Death Process for Feature Allocation,[0],[0]
k=1,4. Birth-Death Process for Feature Allocation,[0],[0]
(N −mk)!(mk,4. Birth-Death Process for Feature Allocation,[0],[0]
− 1)!,4. Birth-Death Process for Feature Allocation,[0],[0]
N !,4. Birth-Death Process for Feature Allocation,[0],[0]
R α,4. Birth-Death Process for Feature Allocation,[0],[0]
"ra
rα=Kα= αKρ
Π",4. Birth-Death Process for Feature Allocation,[0],[0]
Hρ h=1Kh!,4. Birth-Death Process for Feature Allocation,[0],[0]
exp (−αHN ) Kρ∏,4. Birth-Death Process for Feature Allocation,[0],[0]
k=1,4. Birth-Death Process for Feature Allocation,[0],[0]
(N −mk)!(mk,4. Birth-Death Process for Feature Allocation,[0],[0]
− 1)!,4. Birth-Death Process for Feature Allocation,[0],[0]
N !,4. Birth-Death Process for Feature Allocation,[0],[0]
R α,4. Birth-Death Process for Feature Allocation,[0],[0]
"ra
= g(ρ;α)qργ (6)
where g(γ;α) is the probability of a feature allocation γ under the IBP as defined in Equation (1), qγρ is the transition rate from state γ to state ρ,",4. Birth-Death Process for Feature Allocation,[0],[0]
"Hγ , Hρ are the number of distinct features in states γ and ρ respectively and ra is the multiplicity (the times the feature is present at the current feature allocation) of feature a that dies.",4. Birth-Death Process for Feature Allocation,[0],[0]
"Detailed balance holds, and as such the process is reversible and the equilibrium distribution is IBP[N ](α).
",4. Birth-Death Process for Feature Allocation,[0.9999999797303959],"['Detailed balance holds, and as such the process is reversible and the equilibrium distribution is IBP[N ](α).']"
Assume that (z(t)) is a realization of the BDFP (Z(t)) over the finite interval,4. Birth-Death Process for Feature Allocation,[0],[0]
"[0, T ], T > 0",4. Birth-Death Process for Feature Allocation,[0],[0]
and we write (z(t))0≤t≤T .,4. Birth-Death Process for Feature Allocation,[0],[0]
"With probability one the sample path (z(t))0≤t≤T will only contain a finite number of jump events, each of which is either a birth or a death event.",4. Birth-Death Process for Feature Allocation,[0],[0]
"We write B and Q to denote the set of the features created or turned off by birth or death events respectively.
",4. Birth-Death Process for Feature Allocation,[0.9999998897124914],['We write B and Q to denote the set of the features created or turned off by birth or death events respectively.']
Proposition 1.,4. Birth-Death Process for Feature Allocation,[0],[0]
"Writing q(t) = qz(t) to denote the total transition rate out of state z(t), the probability of a realization (z(t)) under the law of the BDFP is:
R|B|+|Q| αA−|B|−|Q|∏A∗−|B∗| h=1",4. Birth-Death Process for Feature Allocation,[0.9990847090395398],"['Writing q(t) = qz(t) to denote the total transition rate out of state z(t), the probability of a realization (z(t)) under the law of the BDFP is: R|B|+|Q| αA−|B|−|Q|∏A∗−|B∗| h=1 Kh!']"
"Kh!
exp (−αHN ) exp ( − ∫ T
0
q(t)dt )",4. Birth-Death Process for Feature Allocation,[0],[0]
× . .,4. Birth-Death Process for Feature Allocation,[0],[0]
".
",4. Birth-Death Process for Feature Allocation,[0],[0]
∏ b∈B∪{z(t=0)} (|b| − 1)!(N − |b|)!,4. Birth-Death Process for Feature Allocation,[0],[0]
N !,4. Birth-Death Process for Feature Allocation,[0],[0]
∏,4. Birth-Death Process for Feature Allocation,[0],[0]
"d∈D rd
(7)
where A = K0 + |B| = KT + |Q|, A∗ = H0 + |B∗| = HT + |Q∗|. B∗, Q∗ are the sets of features with zero multiplicity at their creation time or with multiplicity of one at their death time respectively, and {z(t)} denotes the set of features at time t.",4. Birth-Death Process for Feature Allocation,[0],[0]
The BDFP process can be constructed using a nonhomogenous Poisson process Π. Consider the Lévy measure ν(dωdxdtbdtω) on a product space,4.1. Dependent Beta Process Construction,[0],[0]
"[0, 1] ⊗ X ⊗ R ⊗",4.1. Dependent Beta Process Construction,[0],[0]
"[0,∞).",4.1. Dependent Beta Process Construction,[0],[0]
"A sample corresponds to set of points Π = {ωk, xk, tkb , tkω}k where the range of k is countably infinite.",4.1. Dependent Beta Process Construction,[0],[0]
Each atom corresponds to a feature and is associated with a weight ωk ∈,4.1. Dependent Beta Process Construction,[0],[0]
"[0, 1], a location xk, a birth time tkb ∈ R and a life-span tkω ∈",4.1. Dependent Beta Process Construction,[0],[0]
"[0,∞) (Figure 1).",4.1. Dependent Beta Process Construction,[0],[0]
"The Lévy measure is of the form ν(dωdxdtbdtω) = ρ(dω)µ(dxdtbdtω) and
corresponds to a Beta process on the combined space",4.1. Dependent Beta Process Construction,[0],[0]
Θ = X⊗ R⊗,4.1. Dependent Beta Process Construction,[0],[0]
"[0,∞) with ρ(dω)",4.1. Dependent Beta Process Construction,[0],[0]
= αω−1(1− ω)α−1 and base measure µ(dθ) = µ(dxdtbdtw).,4.1. Dependent Beta Process Construction,[0],[0]
"Setting g(dtb) = dtb and β(dtω) = D exp
−Dtω dtω , the base measure is µ(dθ) = µ0(dx)g(dtb)β(dtω) = µ0(dx)dtbD exp
−Dtω dtω , where D is the death rate.",4.1. Dependent Beta Process Construction,[0],[0]
"The constant measure g(dtb) over the real line R is infinite but σ-finite, that is the total measure g(R)",4.1. Dependent Beta Process Construction,[0],[0]
"= ∞, but there is a measurable partition (Ek) of R with each g(Ek)",4.1. Dependent Beta Process Construction,[0],[0]
< ∞. Since ν(dωdθ) integrates to infinity but satisfies ∫,4.1. Dependent Beta Process Construction,[0],[0]
"[0,1] ∫ Θ
(1 ∧ |ω|)ν(dωdθ) < ∞, a countably infinite number of i.i.d. random points {(ωk,θk)}∞k=1 are obtained from the Poisson process and ∑∞",4.1. Dependent Beta Process Construction,[0],[0]
k=1 ωk is finite with probability one.,4.1. Dependent Beta Process Construction,[0],[0]
"A Beta process is a completely random measure (Kingman, 1967) and, as such, a sample can be expressed as B = ∑∞",4.1. Dependent Beta Process Construction,[0],[0]
"k=1 ωkδθk |α, µ ∼ BP(αµ), where the atoms θk = {xk, tkb , tkω} ∈ Θ and weights",4.1. Dependent Beta Process Construction,[0],[0]
ωk ∈,4.1. Dependent Beta Process Construction,[0],[0]
"[0, 1].
Having drawn a sample B we can construct the feature allocations over an index space R as follows:
B = ∞∑",4.1. Dependent Beta Process Construction,[0],[0]
"k=1 ωkδθk |α, µ ∼ BP(αµ)
",4.1. Dependent Beta Process Construction,[0],[0]
"Sn: = ∞∑ k=1 bnkδθ |B ∼ BeP(ωk) Znk(t) = SnkI(tkb < t < tkb + tkω) (8)
with bnk|ωk ∼ Bernoulli(ωk) and n = 1, . . .",4.1. Dependent Beta Process Construction,[0],[0]
", N .",4.1. Dependent Beta Process Construction,[0],[0]
"The binary matrix S of dimension N ×K, is a feature potential matrix.",4.1. Dependent Beta Process Construction,[0],[0]
Each binary element Snk indicates whether object n possesses feature fk.,4.1. Dependent Beta Process Construction,[0],[0]
S is a global variable and doesn’t depend on time t.,4.1. Dependent Beta Process Construction,[0],[0]
"At any time t, the feature allocation matrix Z(t) is a deterministic function of the current features present at t, that is {fk : tkb < t < tkb + tkw, k = 1, . . .",4.1. Dependent Beta Process Construction,[0],[0]
",∞}
and the feature potential matrix S, i.e. Znk(t) = 1 iff Snk = 1 and tkb < t",4.1. Dependent Beta Process Construction,[0],[0]
<,4.1. Dependent Beta Process Construction,[0],[0]
"t k b + t k ω .
",4.1. Dependent Beta Process Construction,[0],[0]
"The resulting feature allocation process (zn(t))T is equivalent to the following: every time a new feature fk is created, each object n joins with probability ωk, i.e. znk(tkb )|ωk ∼ Bernoulli(ωk).",4.1. Dependent Beta Process Construction,[0],[0]
"If znk(tkb ) = 1, object n will possess feature fk until tkb + t k ω .",4.1. Dependent Beta Process Construction,[0],[0]
Repeat this process for all objects.,4.1. Dependent Beta Process Construction,[0],[0]
Proposition 2.,4.1. Dependent Beta Process Construction,[0],[0]
"The BDFP is exchangeable and the Beta process BP(αµ) on X⊗R⊗[0,∞) describes its underlying mixing measure.
",4.1. Dependent Beta Process Construction,[0],[0]
Proof.,4.1. Dependent Beta Process Construction,[0],[0]
"Consider a sequence of variables (zn(t))T with n = 1, 2, . . .",4.1. Dependent Beta Process Construction,[0],[0]
", N such that each (zn(t))T is the feature allocation evolution of object n over the index space T. These variables are not independent since each (zn(t))T depends on the Z|[n−1](t) =",4.1. Dependent Beta Process Construction,[0],[0]
"(z1:(n−1)(t))T. However, given a sample from the B ∼ BP(αµ) described in Section 4.1, each variable (zn(t))T becomes conditionally independent and the following holds
P ((z1(t))T, (z2(t))T, . . .",4.1. Dependent Beta Process Construction,[0],[0]
", (zN (t))T) = ∫ N∏ n=1 P ((zn(t))T|B)φ(dB)
(9)
where φ = BP(αµ).
",4.1. Dependent Beta Process Construction,[0],[0]
Equation (9) is the de Finetti representation of the BDFP and as such the BDFP is exchangeable and the BP on Θ = X ⊗ R ⊗,4.1. Dependent Beta Process Construction,[0],[0]
"[0,∞) is its underlying mixing measure.",4.1. Dependent Beta Process Construction,[0],[0]
"Restricting our focus on each index t, the overall Beta process BP(αµ) on X⊗ R⊗ [0,∞) results in a set of dependent random measures over X, oneBt for each t ∈ T, such that each Bt is marginally a Beta process.",4.1. Dependent Beta Process Construction,[0],[0]
"Consider a fixed time point t ∈ T and the space [0, 1] ⊗ X (the red vertical plane in Figure 1).",4.1. Dependent Beta Process Construction,[0],[0]
"The point process on this plane (where blue lines intersect the plane) corresponds to features alive at time t, i.e. t ∈",4.1. Dependent Beta Process Construction,[0],[0]
"[tb, tb + tω].",4.1. Dependent Beta Process Construction,[0],[0]
"The Lévy measure on this plane, is calculated by projecting the overall Lévy measure onto the plane,
νt(dωdx) = ∫",4.1. Dependent Beta Process Construction,[0],[0]
"∞ 0 ∫ t t−tω ν(dωdxdtbdtω)
= αω−1(1− ω)α−1µ0(dx) D
(10)
where νt is a measure over [0, 1] ⊗ X for a specific t ∈ T. More specifically, it is the Lévy measure of a Beta process on X with ρ(dω) = αω−1(1 − ω)α−1 and base measure µt(dx)",4.1. Dependent Beta Process Construction,[0],[0]
"=
µ0(dx) D .",4.1. Dependent Beta Process Construction,[0],[0]
"Thus we have that marginally Bt|α, µt ∼ BP(αµt), ∀t ∈ T. (11)
",4.1. Dependent Beta Process Construction,[0],[0]
The restricted and projected measure at any index t ∈ T defines a Beta process.,4.1. Dependent Beta Process Construction,[0],[0]
"Two draws, Bt and Bs, with t, s ∈ T, will be dependent with the amount of dependence decreasing as |s− t| increases.",4.1. Dependent Beta Process Construction,[0],[0]
Proposition 3.,4.1. Dependent Beta Process Construction,[0],[0]
"The dependent Beta process construction presented has IBP marginals at any t.
Proof.",4.1. Dependent Beta Process Construction,[0],[0]
"At any t ∈ T, Bt|α, µt ∼ BP(αµt).",4.1. Dependent Beta Process Construction,[0],[0]
"It is straightforward to see that, marginally, the feature allocation matrix Zt obtained using the generative process in Equation (8) is equivalent to Zt|Bt ∼ BeP(Bt) and therefore Zt ∼ IBP(α), ∀t ∈ T. Corollary 1.",4.1. Dependent Beta Process Construction,[0],[0]
"At any t ∈ T, the feature allocation matrix Zt can be generated by the following generative model as K →∞:
ωk|α ∼ Beta ( R
K
) , Znk|ωk ∼ Bernoulli(ωk) (12)
for k = 1, . . .",4.1. Dependent Beta Process Construction,[0],[0]
",K and n = 1, . . .",4.1. Dependent Beta Process Construction,[0],[0]
", N
The proof of the corollary in included in the supplementary material.",4.1. Dependent Beta Process Construction,[0],[0]
"Note that the above is true only marginally, i.e. at time t ∈ T and it doesn’t generste dependence structure between Zt’s.
",4.1. Dependent Beta Process Construction,[0],[0]
"We underline the dependence of Zs and Zt when |s− t| → 0, ∀s, t ∈ T.",4.1. Dependent Beta Process Construction,[0],[0]
"The closer s, t are, the more the atoms (features)",4.1. Dependent Beta Process Construction,[0],[0]
Bs and Bt share.,4.1. Dependent Beta Process Construction,[0],[0]
"If we independently sampled Zs|Bs ∼ BeP(Bs) and Zt|Bt ∼ BeP(Bt) then Zs, Zt would be dependent, but not equal, even as |s − t| → 0.",4.1. Dependent Beta Process Construction,[0],[0]
"However, in the BDFP the presence of the same features results in the same (not just similar) allocation as |s−t| → 0.",4.1. Dependent Beta Process Construction,[0],[0]
"In both cases, the marginal distribution of the feature allocation matrix at any t ∈ T is Zt|Bt ∼ BeP(Bt) and Zt|α ∼ IBP(α).",4.1. Dependent Beta Process Construction,[0],[0]
"The BDFP results in a continuous evolution of the Z(t) over T: formally Zt d→ Zs as t→ s.
This construction of the BDFP resembles the spatial normalised Gamma process (SNΓP) by (Rao & Teh, 2009).",4.1. Dependent Beta Process Construction,[0],[0]
"The main difference lies in the marginal distribution; the SNΓP admits DP marginals as opposed to the Beta process marginals of the dependent Beta process as shown in Equation (11).
",4.1. Dependent Beta Process Construction,[0],[0]
Proposition 4.,4.1. Dependent Beta Process Construction,[0],[0]
"The feature allocation process described by Equation (8) with B ∼ BP(αµ), has the same birth and death rates as the BDF process.",4.1. Dependent Beta Process Construction,[0],[0]
"For the BDFP, the inference simplifies considerably if we consider a finite approximation which gives the countably infinite model in the limit.",5. Finite Model,[0],[0]
Consider the space S =,5. Finite Model,[0],[0]
"[0, 1] ⊗ X ⊗",5. Finite Model,[0],[0]
"[0, T ] ⊗",5. Finite Model,[0],[0]
"[0,∞), where we restrict the space of tb to be [0, T ] instead of the whole real line R. This accounts for typical applications of the model where we observe data at distinct times over a finite time range.",5. Finite Model,[0],[0]
"Consider the Lévy measure ν(dωdxdtbdtω) on the space S. Then, under the dependent Beta process representation (see section 4.1), the expected number of atoms present in S is
∫ S ν(dωdxdtbdtω) =∫ 1
0 ρ(dω) ∫",5. Finite Model,[0],[0]
X µ0(dx) ∫ T 0,5. Finite Model,[0],[0]
g(dtb) ∫∞ 0 β(dtω),5. Finite Model,[0],[0]
"= KT , where
K → ∞ since ∫ 1
0 ρ(dω) =",5. Finite Model,[0],[0]
"∞. By considering finite K
we allow inference on a finite model which approximates the infinite case with increasing fidelity as K →∞.
The process is depicted in Figure 2 and the infinite case can be derived as the limit K →∞ of the following:
• Consider a time range",5. Finite Model,[0],[0]
"[0, T ] and a set of features F , such that |F| ∼ Poisson(KT ).",5. Finite Model,[0],[0]
"Assign to each feature fk ∈ F , k = 1, . . .",5. Finite Model,[0],[0]
|F|,5. Finite Model,[0],[0]
"a weight ω, such that ωk",5. Finite Model,[0],[0]
"∼ Beta ( R K , 1 ) and Ω =",5. Finite Model,[0],[0]
"[ω1, ω2 . . .",5. Finite Model,[0],[0]
ω|F|].,5. Finite Model,[0],[0]
"• Associate each feature fk ∈ F , k = 1, . . .",5. Finite Model,[0],[0]
|F|,5. Finite Model,[0],[0]
"with a birth time tkb uniformly sampled in [0, T ]; t k b ∼
U(0, T ) and tb",5. Finite Model,[0],[0]
=,5. Finite Model,[0],[0]
[t1b . . .,5. Finite Model,[0],[0]
t |F|,5. Finite Model,[0],[0]
b,5. Finite Model,[0],[0]
"].
",5. Finite Model,[0],[0]
"• For each fk ∈ F , sample its life span tkw ∼ Exponential(D), where D is the death rate.",5. Finite Model,[0],[0]
Define the time of death tkd as t k d = t k b,5. Finite Model,[0],[0]
+,5. Finite Model,[0],[0]
t k w,5. Finite Model,[0],[0]
and tw,5. Finite Model,[0],[0]
"=
[t1w . . .",5. Finite Model,[0],[0]
t |F|,5. Finite Model,[0],[0]
"w ].
",5. Finite Model,[0],[0]
We call the sequence of the above steps Beta Event Process (BEP).,5. Finite Model,[0],[0]
"Putting everything together, generate a sample B = {F ,Ω, tb, tw} ∼ BEP(α,R,K, T ) as follows:
|F| ∼ Poisson(KT )",5. Finite Model,[0],[0]
"ωk ∼ Beta ( R
K , 1
) , tkb ∼ U(0, T ), tkω ∼ Exponential(D)
(13)
for k = 1, . . .",5. Finite Model,[0],[0]
", |F|.",5. Finite Model,[0],[0]
"Having drawn a sample B from the BEP, we can construct the feature allocations over time as follows
Snk|ωk ∼ Bernoulli(ωk) Znk(t) =",5. Finite Model,[0],[0]
"SnkI(tkb < t < tkb + tkω) (14)
",5. Finite Model,[0],[0]
"where n = 1, .",5. Finite Model,[0],[0]
. .,5. Finite Model,[0],[0]
", N .",5. Finite Model,[0],[0]
The feature potential matrix (as defined in section 4.1) has now N × |F| dimensions.,5. Finite Model,[0],[0]
"Moreover, each Z(t) for t ∈ T is a matrix of dimensions N × F (t) and F (t) ≤ |F|.",5. Finite Model,[0],[0]
"Figure 3(a) show the graphical model for the BEP.
",5. Finite Model,[0],[0]
Proposition 5.,5. Finite Model,[0],[0]
"In the finite model, the expected number of features present at any t ∈ T is E[Nf ]",5. Finite Model,[0],[0]
= KD and for D = Rα we have E[Nf ],5. Finite Model,[0],[0]
"= Kα R .
Hyperpriors.",5. Finite Model,[0],[0]
"We put gamma priors on α and R.
Likelihood models.",5. Finite Model,[0],[0]
"We consider two different likelihood models: linear-Gaussian for real data and logistic for binary network data.
",5. Finite Model,[0],[0]
"For the linear-Gaussian likelihood model, consider a sequence of observations {Yt ∈ Y : t = 1, . . .",5. Finite Model,[0],[0]
", L} generated as
Yt = ZtA + t (15)
where Yt is a N ×M observation matrix at each time t = 1, . . .",5. Finite Model,[0],[0]
", L, A is a factor loading matrix of dimension |F|",5. Finite Model,[0],[0]
"× M shared across time and t ∼ N ( 0, σ2 ) is Gaussian white noise.",5. Finite Model,[0],[0]
"We choose a Gaussian prior over A, i.e Afm ∼ N (0, 1).
",5. Finite Model,[0],[0]
"In the case of dynamic binary network data we extend the latent feature relational model (LFRM) proposed by (Miller et al., 2009).",5. Finite Model,[0],[0]
"Let Yt be the N × N binary matrix that contains links, i.e. ytij = Yt(i, j) = 1 iff we observe a link from entity i to entity j at time t. We assume that the matrices Yt are symmetric and ignore diagonal elements (self-links).",5. Finite Model,[0],[0]
The probability of a link from one entity to another is determined by the combined effect of all pairwise feature interactions.,5. Finite Model,[0],[0]
Let Wt be a |F| × |F|,5. Finite Model,[0],[0]
"real-valued weight matrix where Wt(k, k′) is the weight that affects the probability of there being a link from entity i to entity j if entity i has feature k on, i.e. Ztik = Zt(i, k) = 1 and entity j has feature k′ on, i.e. Ztjk′ = Zt(j, k′)",5. Finite Model,[0],[0]
= 1.,5. Finite Model,[0],[0]
"The links are independent conditioned on Zt and Wt, and only the features that are on for the entities i and j at time t influence the probability of a link between those entities at that time (see Figure 3(b)).",5. Finite Model,[0],[0]
"Formally,
P (ytij = 1|Zt,Wt) = σ",5. Finite Model,[0],[0]
"(∑
kl
ZtikZtjlWtkl + s ) (16)
for k, l = 1, .",5. Finite Model,[0],[0]
. .,5. Finite Model,[0],[0]
", |F|, where s is a bias term and σ(x) =",5. Finite Model,[0],[0]
(1 + e−x)−1 is the sigmoid function.,5. Finite Model,[0],[0]
"For completeness, we assume the priors wt(k, l) ∼ N ( µw, σ 2 w ) and s ∼
N ( µs, σ 2 s ) .",5. Finite Model,[0],[0]
"As with many other Bayesian models, exact inference is intractable so we employ Markov Chain Monte Carlo (MCMC) for posterior inference over the latent variables of the finite model.",5.1. Inference,[0],[0]
A detailed description is provided in the supplementary material.,5.1. Inference,[0],[0]
We experimentally evaluate the BEP model on real-world genomics and social network data.,6. Experiments,[0],[0]
"To evaluate the model fit, we compared the BEP model to independent models at each time point.",6. Experiments,[0],[0]
"Here we used a subset of the gene expression data from Piechota et al. (2010), including N = 500 genes in D = 4 different conditions (exposure to different drugs) over L = 24 time intervals.",6.1. Circadian Rhythm Dataset,[0],[0]
The measurements indicate how active a gene is at different times.,6.1. Circadian Rhythm Dataset,[0],[0]
"We created 7 train-test splits holding out 20% of the data, and ran 700 MCMC iterations.",6.1. Circadian Rhythm Dataset,[0],[0]
We see that in terms of predictive performance the BEP outperforms independent IBP models (Table 1).,6.1. Circadian Rhythm Dataset,[0],[0]
The genes belonging to each factor show enrichment for different known biological pathways (Figure 4).,6.1. Circadian Rhythm Dataset,[0],[0]
"Of particular note are the tryptophan metabolism genes enriched in factor 2, given tryptophan’s suspected effects on drowsiness;
the vasopressin regulated water reabsorption, given this hormone’s known circadian regulation (Earnest & Sladek, 1986; Yamaguchi et al., 2013); and the regulation of insulin producing beta cells, another hormone with circadian variation (Shi et al., 2013).",6.1. Circadian Rhythm Dataset,[0],[0]
"For this experiment we used ChIP-seq (chromatin immunoprecipitation sequencing) data downloaded from the ENCODE project (Consortium, 2007), representing histone modifications and transcription factor binding in human neural crest cell lines (see (Park, 2009) for a nice review).
",6.2. ChIP-seq Epigenetic Marks,[0],[0]
The observations involve counts associated with N = 14 (human) cell lines and D = 10 proteins.,6.2. ChIP-seq Epigenetic Marks,[0],[0]
"The counts indicate what proteins, with what chemical modifications, are bound to DNA along the genome.",6.2. ChIP-seq Epigenetic Marks,[0],[0]
"The measurements are stored in N × D matrix of counts Yt: for each cell line, how many reads for each of the 10 proteins mapped to bin t (100 base pair (bp) region of the genome).",6.2. ChIP-seq Epigenetic Marks,[0],[0]
"t = 1, . . .",6.2. ChIP-seq Epigenetic Marks,[0],[0]
", 500 bins were considered at the start of chromosome 1 (50K bp in total).",6.2. ChIP-seq Epigenetic Marks,[0],[0]
In Figure 5(a) each subfigure corresponds to one of the 10 proteins and in each subfigure the counts for the N = 14 cell lines are plotted over the genome section of length 50Kbp.,6.2. ChIP-seq Epigenetic Marks,[0],[0]
"Before inference, the raw counts were square-root transformed (a standard variance stabilizing transform for Poisson data) to make the Gaussian likelihood appropriate.",6.2. ChIP-seq Epigenetic Marks,[0],[0]
"We ran 7 different held-out tests, holding out a different 20% of the data each time.",6.2. ChIP-seq Epigenetic Marks,[0],[0]
"Results, using 700 MCMC iterations, are presented in Table 2.",6.2. ChIP-seq Epigenetic Marks,[0],[0]
"The
BEP outperforms the independent IBP model in both test likelihood and error with a statistically significant difference.",6.2. ChIP-seq Epigenetic Marks,[0],[0]
"The independent IBP appears to have better results in train error and likelihood, again suggesting overfitting.",6.2. ChIP-seq Epigenetic Marks,[0],[0]
"Comparing the plots of the true measurements to the learnt ones by the BEP and independent IBP model in Figure 5 we see that both models successfully reproduce the data but the BEP reconstructions provide a cleaned up picture of the meaningful signal.
",6.2. ChIP-seq Epigenetic Marks,[0],[0]
The features found by the model in the different genome locations correspond to different states associated with the specific genome location.,6.2. ChIP-seq Epigenetic Marks,[0],[0]
"Genes and regulatory DNA elements such as enhancers, silencers and insulators are embedded in genomes.",6.2. ChIP-seq Epigenetic Marks,[0],[0]
"These genomic elements on the DNA have footprints for the transacting proteins involved in transcription, either for the positioning or regulation of the transcriptional machinery.",6.2. ChIP-seq Epigenetic Marks,[0],[0]
"For instance, promoters are regions of DNA which recruit proteins required to initiate transcription of a particular gene and located near the transcription start sites.",6.2. ChIP-seq Epigenetic Marks,[0],[0]
Enhancers are regions of DNA that can be bound by proteins which activate transcription of a distal gene.,6.2. ChIP-seq Epigenetic Marks,[0],[0]
"So a cell line, at specific genome location (recall that here each location corresponds to 100 base pairs), will have underlying feature membership (some promoters and some enhancer for example) that determines whether particular protein are found there using ChIP-seq.
Genomic annotations, from ChromHMM (Ernst et al., 2011), are shown in Figure 8 in the supplementary document for the region we model.",6.2. ChIP-seq Epigenetic Marks,[0],[0]
Different levels of the marks in these different regions are much easier to see in the reconstructed signal using BEP in Figure 5(b).,6.2. ChIP-seq Epigenetic Marks,[0],[0]
"In van de Bunt et al. (1999), 32 university freshman students in a given discipline at a Dutch university were surveyed at seven time points about who in their class they considered as friends.",7. van de Bunt’s Dataset,[0],[0]
"Initially, i.e. t1, most of the students were unknown to each other.",7. van de Bunt’s Dataset,[0],[0]
"The first four time points are three weeks apart, whereas the last three time points are six weeks apart as showin in Figure 11 in the supplementary matrial.",7. van de Bunt’s Dataset,[0],[0]
We symmetrise the matrix by assuming friendship if either individual reported it.,7. van de Bunt’s Dataset,[0],[0]
"We test the performance of BEP using the sigmoid likelihood model as in Equation
(16) by holding out 10% of all links across all time points.",7. van de Bunt’s Dataset,[0],[0]
We ran each model for 1000 MCMC iterations.,7. van de Bunt’s Dataset,[0],[0]
The results are shown in Table 3.,7. van de Bunt’s Dataset,[0],[0]
The independent network LFR models outperform BEP in the train setting and the test error while BEP outperforms in the test likelihood.,7. van de Bunt’s Dataset,[0],[0]
"However, here the results are comparable.",7. van de Bunt’s Dataset,[0],[0]
"Looking at Figure 6, both models provide the same picture of the allocation.",7. van de Bunt’s Dataset,[0],[0]
It is possible the stationary assumption hurts the BEP: in the VDB dataset the number of links almost exclusively increases over time.,7. van de Bunt’s Dataset,[0],[0]
"Many modern machine learning and statistics tasks involve multidimensional data positioned along some linear covariate: we have shown functional genomics data where the covariate is position in the genome, and network data where links change over time.",8. Discussion,[0],[0]
"To model such data we need priors that utilize the dependencies through time, while handling high dimensionality.",8. Discussion,[0],[0]
The BDFP is an expressive new Bayesian non-parametric prior that fulfills these criteria.,8. Discussion,[0],[0]
"It outputs time-evolving feature allocations, which can then be effectively used to model high-dimensional time-series data.",8. Discussion,[0],[0]
"Since the number of latent features is unbounded, like other Bayesian non-parametric methods, the model can adapt its complexity to the data.",8. Discussion,[0],[0]
"While the combinatorial BDFP may seem like a complex object to handle computationally, our theoretical results showing that the de Finetti measure underlying the BDFP is a specific beta process, which can be well approximated by a finite K model, the BEP.",8. Discussion,[0],[0]
"Our experimental results, compared to independent feature allocations, provides evidence that effectively modeling dependency in the feature allocation through the birth-death mechanism is appropriate for a wide range of statistical applications.",8. Discussion,[0],[0]
"Moreover, the BEP provides an interpretable structure using parameters not found, to the best of our knowledge, in existing models, i.e. birth and death rate of features.",8. Discussion,[0],[0]
"We are interested in scaling inference under the BEP to larger datasets, for example using (stochastic) variational inference methods that have been successful for the IBP (Doshi et al., 2009).
",8. Discussion,[0],[0]
Acknowledgements Konstantina’s research leading to these results has received funding from the European Research Council under the European Union’s Seventh Framework Programme (FP7/2007-2013) ERC grant agreement no. 617411.,8. Discussion,[0],[0]
"We propose a Bayesian nonparametric prior over feature allocations for sequential data, the birthdeath feature allocation process (BDFP).",abstractText,[0],[0]
The BDFP models the evolution of the feature allocation of a set of N objects across a covariate (e.g. time) by creating and deleting features.,abstractText,[0],[0]
"A BDFP is exchangeable, projective, stationary and reversible, and its equilibrium distribution is given by the Indian buffet process (IBP).",abstractText,[0],[0]
We show that the Beta process on an extended space is the de Finetti mixing distribution underlying the BDFP.,abstractText,[0],[0]
"Finally, we present the finite approximation of the BDFP, the Beta Event Process (BEP), that permits simplified inference.",abstractText,[0],[0]
The utility of the BDFP as a prior is demonstrated on real world dynamic genomics and social network data.,abstractText,[0],[0]
A Birth-Death Process for Feature Allocation,title,[0],[0]
