35	7	In our experiments, both MBR and GBR show performance superior to other stateof-the-art elimination and variational algorithms.
36	21	A graphical model (GM)M = (G,F) associates a collection of n discrete random variables x = [xi : i ∈ V] ∈ XV =∏ i∈V Xi with the following joint probability distribution: Pr(x) = 1 Z ∏ α∈E fα(xα), Z = ∑ x ∏ α∈E fα(xα), where Xi = {1, 2, · · · di}, xα = [xi : i ∈ α], F = {fα}α∈E is a set of non-negative functions called factors, and Z is the normalizing constant called the partition function that is computationally intractable.
62	6	Inspired by tensor renormalization groups (TRG, Levin & Nave, 2007, see also references therein) in the physics literature, MBR utilizes low-rank approximations to the mini-buckets instead of simply applying max (or min) operations as in MBE.
65	6	Then for ` = 1, · · · ,mi − 1, mini-bucket B`i is “renormalized” through replacing vertex i by its replicate i` and then introducing local factors r`i , ri` for error compensation, i.e., B̃`i ← {fα\{i}∪{i`}|fα ∈ B`i} ∪ {r`i , ri`}.
66	14	Here, r`i , ri` are local “compensating/renormalizing factors”, chosen to approximate the factor fB`i = ∏ fα∈B`i fα well, where MBE approximates it using (2) and (3).
72	7	Here, one can check that mi∏ `=1 fB`i\{i}(xB`i\{i}) = ∑ xi fBmii (xBmii ) mi−1∏ `=1 f̃B`i (xB`i ), ≈ ∑ xi mi∏ `=1 fB`i (xB`i ), from (4) and MBR indeed approximates BE.
75	5	2: F ← F† 3: for i in o do 4: Bi ← {fα|fα ∈ F , i ∈ α} 5: Divide Bi into mi subgroups {B`i} mi `=1 such that |V(B`i )| ≤ ibound+ 1 for ` = 1, · · · ,mi.
80	4	The optimization (4) is related to the rank-1 approximation on fB`i , which can be solved efficiently via (truncated) singular value decomposition (SVD).
85	15	Due to the above observations, the complexity of (4) is NSVD(M) that denotes the complexity of SVD for matrix M. Therefore, the overall complexity becomes O (NSV D(M) · T ) = O ( NSVD(M) · |E| ·max α∈E |α| ) , where NSVD(M) = O(dibound+2) in general, but typically much faster in the existing SVD solver.
89	5	In particular, GBR directly minimizes the error in the partition function from each renormalization, aiming for improved accuracy compared to MBR.
102	8	(12) This is repeated until the MBR process terminates, as formally described in Algorithm 3.
107	6	Specifically, for each t-th renormalization, i.e., fromM(t) toM(t+1), we consider change of the following factor fi induced from global-bucketF (t) to variable xi in a “skewed” manner as follows: fi(x (1) i , x (2) i ) := ∑ xV(t)\{i} ∏ fα∈FB` i fα(x (1) i ,xα\{i}) · ∏ fα∈F(t)\FB` i fα(x (2) i ,xα\{i}), where x(1)i , x (2) i are the newly introduced “split variables” that are associated with the same vertex i, but allowed to have different values for our purpose.
121	3	In this section, we provide a formal description of GBR.
122	3	First, consider the sequence of GMs M(1), · · · ,M(T+1) from interpreting MBR as GM renormalizations.
124	6	GBR modifies this sequence iteratively by replacing intermediate choice of compensation r(t) by another choice s(t)(x) = s`i(x) = si`(x) in reversed order, approximately solving (14) until all compensating factors are updated.
125	6	Then, GBR outputs partition function Z(T+1) forM(T+1) as an approximation of the original partition function Z.
131	5	Since fi resembles the partition function in a way that it is also a summation over GM with small change in a set of factors, i.e., excluding local factors r`i , ri` , we expect fi to be approximated well by a summation gi over xV̂(T+1)\{i,i`} in M̂(T+1): gi(xi` , xi) := ∑ xV̂(T+1)\{i,i`} ∏ fα∈F̂(T+1)\{r`i ,ri`} fα(xα), which can be computed in O(|E|dibound+2) complexity via appying BE in M̂(T+1) with elimination order õ \ {i, i`} as in (13).
137	4	The formal description of the GBR scheme is provided in Algorithm 4.
141	8	4: for i` = nmn−1, · · · , n1, · · · , 1m1−1, · · · , 11 do 5: Generate s`i , s ` i by solving min s`i ,si` ∑ x (1) i ,x (2) i ( gi(x (1) i , x (2) i )− g̃i(x (1) i , x (2) i ) )2 , 6: where gi, g̃i is defined as follows: gi(xi` , xi) = ∑ xV\{i,i`} ∏ fα∈F\{r`i ,ri`} fα(xα), g̃i(xi` , xi) = s ` i(xi`) ∑ x`i si`(x ` i)gi(x ` i , xi), with its computation done by BE with elimination order of õ \ {i, i`}.
142	9	7: Update GMM by F ← F \ {r`i , ri`} ∪ {s`i , si`}.
143	8	8: end for 9: Get Z = ∑ x ∏ fα∈F fα(xα) via BE with elimination order õ.
152	5	We first consider the most popular binary pairwise GMs, called Ising models (Onsager, 1944): p(x) = 1Z exp (∑ i∈V φixi + ∑ (i,j)∈E φijxixj ) , where xi ∈ {−1, 1}.
160	3	Somewhat surprisingly, GBR outperforms GBP and even MBR is not worse than GBP although GBP (using the same order of memory) is more expensive to run due to its iterative nature.
175	9	Results for MF were omitted since MF was not able to run on these instances by its construction.
179	8	Typically, MBR and GBR are nearly as good as GBP.
182	9	First, we emphasize that using the min-fill heuristics for choosing the appropriate elimination order can improve the performance of MBR and GBR (see the supplementary material).
188	25	GBR calibrates MBR via minimization of renormalization error for the partition function explicitly.
189	43	A similar optimization was considered in the so-called second-order renormalization groups (Xie et al., 2009; 2012).
190	127	Hence, there is scope to explore potential variants of GBR.
191	152	Finally, another direction to generalize MBR is to consider larger sizes of buckets to renormalize, e.g., see (Evenbly & Vidal, 2015; Hauru et al., 2018).
