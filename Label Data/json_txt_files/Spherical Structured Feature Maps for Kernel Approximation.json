{"sections": [{"heading": "1. Introduction", "text": "Kernel methods such as Gaussian processes (GPs) (Rasmussen, 2006; Srinivas et al., 2009; Snoek et al., 2012) and support vector machines (SVMs) (Chang & Lin, 2011; Fan et al., 2008) have been successfully used in many statistical modeling and machine learning tasks. Despite of strong expressive power, kernel methods usually cannot scale up to the large scale datasets with L samples due to the need of manipulatingL\u00d7LGram matrix. Recently, random feature maps (Rahimi et al., 2007; Rahimi & Recht, 2009; Sutherland & Schneider, 2015) have demonstrated their effectiveness on kernel approximation to scale up kernel methods. Roughly speaking, a shift invariant kernel K(x, z) = K(x \u2212 z) : Rd \u2192 C can be approximated by K(x, z) \u2248 \u03a8(x)T\u03a8(z), where \u03a8 is the explicit mapped feature constructed as \u03a8(x) = f(WTx)/ \u221a N , where f(\u00b7)\n1Department of Computer Science, City University of Hong Kong, Tat Chee Avenue, Hong Kong . Correspondence to: Yueming Lyu <LV Yueming@outlook.com>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\ndenotes the nonlinear function, W \u2208 Rd\u00d7N is constructed by N i.i.d samples drawn from a distribution defined by K. Therefore, the training and inference of kernel methods can be greatly accelerated by working directly on the primal space of \u03a8(\u00b7). For example, Gaussian Processes (GPs) have O(L3) computation and O(L2) storage complexity. By using feature maps, it reduces to O(N2L + N3) computation and O(NL + N2) storage complexity. All these elegant properties make random feature maps promising for large scale kernel methods. Thus, many kernel methods (Le & Wilson, 2015; Cutajar et al., 2016; Oliva et al., 2016) have been proposed to deal with large scale statistical learning by directly working on feature maps.\nGenerally, two aspects of random feature maps are mostly concerned by literature for scaling up kernel methods. One is the approximation accuracy of feature maps while the other is the computational cost of feature maps construction. To achieve better approximation accuracy, (Yang, 2014; Avron et al., 2016) employ QMC (Dick et al., 2013) sampling instead of standard Monte Carlo sampling to construct feature maps. By mapping QMC points on [0, 1]d through the inverse cumulative distribution function, they construct more effective feature maps. To reduce time complexity, (Le et al., 2013) propose Fastfood to construct feature maps. Benefiting from the special structured matrix multiplication, it reduces time complexity of feature maps construction from O(Nd) to O(N log d). However, it achieves computational efficiency at the expense of increasing the variance of approximation. Recently, (Feng et al., 2015) employ the property of circulant matrix to accelerate feature maps construction of Gaussian kernel without increasing the variance. (Choromanski & Sindhwani, 2016) generalize the Fastfood and circulant feature maps to P model and particularly discuss the structured matrix with low-displacement rank. Despite of the success of P model, it still cannot achieve better approximation accuracy compared with feature maps obtained with fully Gaussian matrix.\nTo achieve better approximation accuracy and loglinear time complexity, we propose Spherical Structured Feature (SSF) maps to approximate shift and rotation invariant kernels as well as bth-order arc-cosine kernels (Cho & Saul, 2009). Specifically, We construct SSF maps based on the point set on d \u2212 1 dimensional sphere Sd\u22121, where the\npoints are columns of a particular structured matrix produced by a discrete Fourier matrix. The points on Sd\u22121 for SSF maps construction can be generated by optimizing the discrete Riesz s-energy. According to (Brauchart et al., 2014), optimizing the discrete Riesz s-energy (for s in some ranges) can generate QMC designs on Sd\u22121, which usually can achieve smaller approximation error compared with fully random methods. Moreover, Because of special structure of the point set, SSF maps construction can achieve loglinear time complexity via Fast Fourier Transform (FFT).\nOur contributions are summarized as follows:\n\u2022 We propose Spherical Structured Feature (SSF) maps to approximate shift and rotation invariant kernels as well as bth-order arc-cosine kernels (Cho & Saul, 2009). We prove that the inner product of SSF maps are unbiased estimates for above kernels if asymptotically uniformly distributed point set on d \u2212 1 dimensional sphere Sd\u22121 is given.\n\u2022 We propose an efficient coordinate decent method to find a local optimum of the discrete Riesz senergy (Brauchart & Grabner, 2015), thereby approximately generating asymptotically uniformly distributed points on Sd\u22121.\n\u2022 We can construct SSF maps with linear space complexity and loglinear time complexity. Empirically, SSF maps achieve superior performance compared with other methods."}, {"heading": "2. Background and Preliminaries", "text": "We provide a brief review of random feature maps and the discrete Riesz s-energy in this section as preliminaries."}, {"heading": "2.1. Random Feature Maps", "text": "Random feature maps can be viewed as equal weight approximation of multidimensional integrals. One earlier work (Rahimi et al., 2007) approximates the shift invariant kernels based on the Bochner\u2019s Theorem.\nTheorem 2.1 Bochner\u2019s Theorem ((Rudin, 2011)) : A continuous shift invariant scaled kernel function K(x, z) = K(x \u2212 z) : Rd \u2192 C is positive definite if and only if it is the Fourier Transform of a unique finite probability measure p on Rd.\nK(x, z) = \u222b Rd e \u2212i(x\u2212z)Twp(w)dw (1)\nFor a real valued kernel K(x, z), p(w) = p(\u2212w) \u2265 0 can ensure the imaginary parts of the integral vanish. According to the Bochner\u2019s theorem, there is a one-to-one corre-\nspondence between the kernel functions K(x, z) and probability densities p(w) defined on Rd.\nShift and rotation invariant kernels are shift invariant kernels with the rotation invariant property, i.e. K(x, z) = K(Rx, Rz), given any rotation R \u2208 SO(d), where SO(d) denotes rotation groups. The Gaussian kernel K(x, z) = e\u2212\u2016x\u2212z\u2016 2 2/2\u03c3 2\nis a member of this family. From Bochner\u2019s theorem, the corresponding probability density is also Gaussian. For a general Gaussian RBF kernel K(x, z) = e\u2212(x\u2212z)\nT\u03a3(x\u2212z)/2, it can be transformed into rotation invariant form by using y = \u03a31/2x in the original domain.\nbth-order arc-cosine kernels are rotation invariant kernels. As discussed in (Cho & Saul, 2009), bth-order arccosine kernels have the following form:\nKb(x, z) = 1 \u03c0 \u2016x\u2016 b 2 \u2016z\u2016 b 2 Jb(\u03b8) (2) where \u03b8 = cos\u22121 (\nxT z \u2016x\u20162\u2016z\u20162 ) bth-order arc-cosine kernels have trivial dependence on the norm of x and z. The dependence on the angle is defined by function Jb(\u03b8). bth-order arc-cosine kernels are rotation invariant kernels but not shift invariant kernels in general. For example, the zero-order (3) and first-order (4) arc-cosine kernel are not shift invariant kernels.\nK0(x, z) = 1\u2212 \u03b8\u03c0 (3)\nK1(x, z) = 1 \u03c0 \u2016x\u20162 \u2016z\u20162 (sin \u03b8 + (\u03c0 \u2212 \u03b8) cos \u03b8) (4)\nThe bth-order arc-cosine kernel Kb(x, z) can be reformulated via the integral representation:\nKb(x, z) = 2 \u222b Rd s(w Tx)s(wT z)(wTx) b (wT z) b p(w)dw\n(5) where s(\u00b7) is a step function (i.e. s(x) = 1 if x > 0 and 0 otherwise) and the density p is standard Gaussian.\nFeature maps: Both Monte Carlo and Quasi-Monte Carlo approximation (Dick et al., 2013) are equal weight approximation to integrals. Based on equal weight approximation, the feature maps can be constructed as:\nK(x, z) \u2248 1N N\u2211 i=1 f ( wTi x ) f ( wTi x ) = \u03a8(x)T\u03a8(z)\n(6) where wi, i \u2208 1, ..., N are samples constructed by Monte Carlo or Quasi-Monte Carlo methods. f(\u00b7) is a nonlinear function depending on the kernel. \u03a8(\u00b7) is the explicit finite dimensional feature map. For Gaussian kernel with bandwidth \u03c3, the associated nonlinear function is a complex exponential function f(x) = eix/\u03c3 . For a zero-order arccosine kernel in (3) and first-order arc-cosine kernel in (4), the associated nonlinear functions are step function f(x) = s(x) and ReLU activation function f(x) = max(0, x) respectively."}, {"heading": "2.2. Discrete Riesz s-energy", "text": "The discrete Riesz s-energy is related to the equal weight numerical integration and uniformly distributed point set.\nEqual weight numerical integration over a d-dimensional sphere Sd := {x \u2208 Rd+1 | \u2016x\u20162 = 1} uses equal weight summation of finite point evaluations of the integrands to approximate the integrals:\n\u222b Sd f(v)d\u03c3(v) \u2248 1 N N\u2211 i=1 f(vi) (7)\nwhere \u03c3 denotes the normalized surface area measure on Sd.\nAccording to (Brauchart & Grabner, 2015), the point set V = [v1, ...,vN ] \u2208 Sd\u00d7N is asymptotically uniformly distributed if equation (8) holds true.\nlim N\u2192\u221e\n1 N N\u2211 i=1 f(vi) = \u222b Sd f(v)d\u03c3(v) (8)\nThe discrete Riesz s-energy(Go\u0308tz, 2003; Brauchart & Grabner, 2015) is defined as equation (9):\nEs (V) :=  N\u2211 i=1 N\u2211 j=1,j 6=i 1 \u2016vi\u2212vj\u2016s2 , s 6= 0 N\u2211 i=1 N\u2211 j=1,j 6=i log 1\u2016vi\u2212vj\u20162 , s = 0\n(9)\nTheorem 2.2 ((Brauchart & Grabner, 2015)): For s > \u22122, the optimum N-point configuration of the Riesz s-energy on Sd is asymptotically uniformly distributed w.r.t the normalized surface area measure \u03c3 on Sd.\nAccording to (Brauchart et al., 2014; Brauchart & Grabner, 2015), the discrete Riesz s-energy can serve as a criterion to construct the point set V = [v1, ...,vN ] \u2208 Sd\u00d7N for QMC designs. Particularly, (Brauchart et al., 2014) have proved that maximizing the discrete Riesz s-energy with s \u2208 (\u22122, 0) can generate QMC designs for functions in Sobolev space. They also prove that QMC designs have higher convergence rate of worst-case error than fully randomly chosen points for functions in Sobolev space."}, {"heading": "3. Spherical Structured Feature Maps", "text": "In this section, we propose SSF maps to approximate shift and rotation invariant kernels as well as bth-order arccosine kernels by employing their rotation invariant property."}, {"heading": "3.1. Feature Maps for Shift and Rotation Invariant Kernels", "text": "Shift and rotation invariant kernels are highly symmetric and structured because they satisfy both shift invariant property and rotation invariant property. Rotation invariant property means that K(x, z) = K(Rx, Rz), given any rotation R \u2208 SO(d), where SO(d) denotes rotation groups. To benefit from rotation invariant property, it is reasonable to construct the feature maps by using spherical equal weight approximation in equation (7) and (8).\nThe feature maps for real valued shift and rotation invariant kernels K(x, z) can be constructed as equation (10):\n\u03a8 (x) = 1\u221a NM\n[cos ( \u03a6\u2212(t1)x Tv1 ) , sin ( \u03a6\u2212(t1)x Tv1 ) ,\n..., cos ( \u03a6\u2212(tM )x TvN ) , sin ( \u03a6\u2212(tM )x TvN ) ]T\n(10) where tj = jM+1 , V = [v1, ...,vN ] \u2208 S\nd\u22121\u00d7N denotes the point set asymptotically uniformly distributed on Sd\u22121and \u03a6\u2212(x) denotes the inverse cumulative distribution function w.r.t the nonnegative radial scale.\nTheorem 3.1: \u03a8(x)T\u03a8 (z) is an unbiased estimate of a real valued shift and rotation invariant kennel K(x, z).\nProof: From Bochner\u2019s Theorem, a shift invariant kernel K(x, z) can be written as equation (1). Let r = \u2016w\u20162and p(r) be the density function of r. Because of the rotation invariant property of K(x, z), we achieve equation (11).\nK(x, z) = \u222b R+ \u222b Sd\u22121 e \u2212ir(x\u2212z)Tvp(r)drd\u03c3(v)\n= \u222b\n[0,1] \u222b Sd\u22121 e \u2212i \u03a6\u2212(t)(x\u2212z)Tvd\u03c3(v)dt\n(11) where R+ denotes the nonnegative real values.\nFor real valued kernel K(x, z), the imaginary parts of the integral vanish. We can achieve equation (12).\nK(x, z) = \u222b\n[0,1] \u222b Sd\u22121 cos ( \u03a6\u2212(t)(x\u2212 z)Tv ) d\u03c3(v)dt\n(12) According to the property of asymptotically uniformly distributed point set V in equation (8) and the onedimensional QMC rule, we obtain equation (13).\nlim M,N\u2192\u221e\n\u03a8(x) T \u03a8 (z) =\nlim M,N\u2192\u221e\n1 MN N\u2211 i=1 M\u2211 j=1 (cos ( \u03a6\u2212(tj)x Tvi ) cos ( \u03a6\u2212(tj)z Tvi )\n+ sin ( \u03a6\u2212(tj)x Tvi ) sin ( \u03a6\u2212(tj)z Tvi ) )\n= lim M,N\u2192\u221e\n1 MN M\u2211 j=1 N\u2211 i=1 cos ( \u03a6\u2212(tj)(x\u2212 z)Tvi )\n= \u222b\n[0,1] \u222b Sd\u22121 cos ( \u03a6\u2212(t)(x\u2212 z)Tv ) d\u03c3(v)dt\n= K(x, z) (13)\nProposition 3.1: Let U = [V,\u2212V], using point set U to approximate a real valued shift and rotation invariant kernel K(x, z) by using equation (10) is equal to using point set V to approximate K(x, z):\n\u03a8(x;U) T \u03a8 (z;U) = \u03a8(x;V) T \u03a8 (z;V) (14)\nProof: Note that cosine function is an even function. Thus, we obtain equation (15).\ncos ( \u03a6\u2212(tj)(x\u2212 z)Tvi ) = cos ( \u2212\u03a6\u2212(tj)(x\u2212 z)Tvi ) (15)\nThus, we achieve equation (16).\n\u03a8(x;U) T \u03a8 (z;U)\n= 12NM N\u2211 i=1 M\u2211 j=1 cos ( \u03a6\u2212(tj)(x\u2212 z)Tvi )\n+ 12NM N\u2211 i=1 M\u2211 j=1 cos ( \u2212\u03a6\u2212(tj)(x\u2212 z)Tvi ) = 12NM N\u2211 i=1 M\u2211 j=1 2 cos ( \u03a6\u2212(tj)(x\u2212 z)Tvi ) = \u03a8(x;V) T \u03a8 (z;V)\n(16)\nProposition 3.1 shows that for a shift and rotation invariant kernel, computing N points can achieve the same approximation effect compared with using 2N points.\n3.2. Feature Maps for bth-order Arc-cosine Kernels\nIn this subsection, we discuss the feature maps for bthorder arc-cosine kernels. We discuss them separately because they are rotation invariant kernels but not shift invariant kernels in general. Moreover, they are closely related to deep neural networks (Cho & Saul, 2009), which demonstrate super performance in many areas.\nLemma 3.1: The bth-order arc-cosine kernels can be calculated as equation (17).\nKb(x, z) = Cb \u222b Sd\u22121 \u03c7 ( vTx ) \u03c7 ( vT z ) +\u03c7(\u2212vTx)\u03c7(\u2212vT z)d\u03c3(v) (17)\nwhere \u03c7(x) = max(0, sign(x)|x|b), Cb = \u222b R+ r\n2bp(r)dr. Cb is a constant that is independent of x and z. p(r) is the density function of the chi-distribution with d degrees freedom. For example, the constants associated with the zero, first and second-order arc-cosine kernels are C0 = 1, C1 = d and C2 = d(d+ 2) respectively.\nProof: From equation (5), we can achieve equation (18).\nKb(x, z) = 2 \u222b Rd s(w Tx)s(wT z)(wTx) b (wT z) b p(w)dw\n= 2 \u222b Rd \u03c7 ( wTx ) \u03c7 ( wT z ) p(w)dw\n(18)\nLet r = \u2016w\u20162. Since p is standard Gaussian, by taking rotation invariant property, we obtain equation (19).\nKb(x, z) = 2 \u222b Rd \u03c7 ( wTx ) \u03c7 ( wTz ) p(w)dw\n= 2 \u222b Sd\u22121 \u222b R+ \u03c7 ( rbvTx ) \u03c7 ( rbvT z ) p(r)d\u03c3(v)dr\n= 2 \u222b Sd\u22121 \u222b R+ r 2b\u03c7 ( vTx ) \u03c7 ( vT z ) p(r)d\u03c3(v)dr\n= 2 \u222b R+ r 2bp(r)dr \u222b Sd\u22121 \u03c7 ( vTx ) \u03c7 ( vT z ) d\u03c3(v)\n= 2Cb \u222b Sd\u22121 \u03c7 ( vTx ) \u03c7 ( vT z ) d\u03c3(v)\n(19) Since Kb(x, z) is rotation invariant, we have Kb(x, z) = Kb(\u2212x,\u2212z). Together with equation (19), we achieve equation (20).\nKb(x, z) = Cb \u222b Sd\u22121 \u03c7 ( vTx ) \u03c7 ( vT z ) +\u03c7(\u2212vTx)\u03c7(\u2212vT z)d\u03c3(v) (20)\nThe feature maps for a bth-order arc-cosine kernel Kb(x, z) can be constructed as equation (21).\n\u03a8 (x) = \u221a\nCb N [\u03c7 ( vT1 x ) , \u03c7 ( \u2212vT1 x ) , ....,\n\u03c7 ( vTNx ) , \u03c7 ( \u2212vTNx ) ]T \u2208 R2N\n(21)\nTheorem 3.2: \u03a8(x)T\u03a8 (z) is an unbiased estimate of a bth-order arc-cosine kernel Kb(x, z).\nProof: According to the Lemma 3.1 and the property of the asymptotically uniformly distributed point set V, we obtain equation (22).\nlim N\u2192\u221e\n\u03a8(x) T \u03a8 (z)\n= lim N\u2192\u221e\nCb N N\u2211 i=1 \u03c7 ( vTi x ) \u03c7 ( vTi z ) + \u03c7(\u2212vTi x)\u03c7(\u2212vTi z)\n= Cb \u222b Sd\u22121 \u03c7 ( vTx ) \u03c7 ( vT z ) + \u03c7(\u2212vTx)\u03c7(\u2212vT z)d\u03c3(v) = Kb(x, z) (22)\nFrom equation (17) and (22), we observe that the approximation is actually operated on the (d \u2212 1)-dimensional domain instead of d-dimensional domain (Cho & Saul, 2009). Generally, the approximation error of Quasi Monte Carlo methods with N points depends on the dimension of integration. A lower dimension leads to smaller approximation error, thus the feature maps in equation (21) can achieve lower approximation error.\nThe feature maps in equation (21) are closely related to the bidirectional activation neural network. Specifically, the feature maps for the first-order arc-cosine kernel are related to the bidirectional ReLU activation function (An et al., 2015) which has the distance preservation property compared with ReLU.\nFrom equation (14) and (21), we know that the feature maps actually rely on the point set U = [V,\u2212V]. The design of the point set U will be discussed in section 4."}, {"heading": "4. Design of Matrix U", "text": "We have discussed the construction of SSF maps in last section. However, one unsolved problem is how to obtain the matrix U = [V,\u2212V]. We employ the discrete Riesz s-energy as the objective function to obtain matrix U because it can generate asymptotically uniformly distributed points on Sd\u22121 (Brauchart & Grabner, 2015). Moreover, to achieve computation and storage efficiency for feature maps construction , we add a structured constraint to the matrix U. In this section, we show the structure of matrix U first and then the optimization of discrete Riesz s-energy.\nIt is worth noting that matrix U can be used not only for kernel approximation, but also for approximation of general integrals over hypersphere. Moreover, by using FFT, matrix U can accelerate the integral approximation which involves projection operations. In addition, it only needs to store the indexes with linear storage cost (i.e. O(d)) instead of to explicitly store the matrix with cost O(Nd)."}, {"heading": "4.1. Structure of Matrix U", "text": "Since U can be constructed by V, i.e. U = [V,\u2212V], we only need to define structured matrix V. To achieve loglinear time complexity of SSF maps construction, we construct V by extracting rows from a discrete Fourier matrix. The complexity analysis of SSF maps construction based on matrix V is given in section 5.\nMathematically, the construction of matrix V is shown as follows. Without loss of generality, we assume that d = 2m,N = 2n, m < n. Let F \u2208 Cn\u00d7n be a n \u00d7 n discrete Fourier matrix. Fk,j = e 2\u03c0ikj n is the (k, j)thentry of F ,\nwhere i = \u221a \u22121. Let \u039b = [k1, k2, ..., km] \u2282 {1, ..., n\u2212 1} be a subset of indexes.\nThe structured matrix V can be defined as equation (23).\nV = 1\u221a m [ ReF\u039b \u2212ImF\u039b ImF\u039b ReF\u039b ] \u2208 Rd\u00d7N (23)\nwhere F\u039b in equation (24) is the matrix constructed by m rows of F .\nF\u039b=  e 2\u03c0ik11 n \u00b7 \u00b7 \u00b7 e 2\u03c0ik1n n ... . . .\n... e 2\u03c0ikm1 n \u00b7 \u00b7 \u00b7 e 2\u03c0ikmn n  \u2208 Cm\u00d7n (24) With the V given in equation (23), it is easy to verify that \u2016vi\u20162 = 1 for i \u2208 {1, ..., n}. Thus, each column of matrix V is a point on Sd\u22121."}, {"heading": "4.2. Minimize the Discrete Riesz s-energy", "text": "With structured matrix V defined in equation (23), our goal is to select a subset of indexes \u039b that optimizes the discrete\nRiesz s-energy. Specifically, we will discuss how to minimize the Riesz 0-energy in equation (25). The other Riesz s-energy can be optimized in a similar way.\nE(U) = 2N\u2211 i=1 2N\u2211 j=1,j 6=i log 1\u2016ui\u2212uj\u2016 (25)\nwhere U = [V,\u2212V] = [u1, ...,u2N ].\nIn the following, we will discuss how to minimize equation (25) by using a coordinate decent method.\nTheorem 4.1: Let U = [V,\u2212V] with V defined in (23), the discrete Riesz 0-energy of U can be calculated as equation (26).\nE(U) = C \u2212 2n n\u22121\u2211 p=1 log ( 1\u2212 (Im 1m m\u2211 s=1 e 2\u03c0iksp n ) 2) \u22122n\nn\u22121\u2211 p=1 log ( 1\u2212 (Re 1m m\u2211 s=1 e 2\u03c0iksp n ) 2) (26)\nwhere C is a constant independent of the choice of \u039b.\nProof: Since U = [V,\u2212V] \u2208 S(d\u22121)\u00d72N, we obtain equation (27).\nE(U) = \u2212 2N\u2211 i=1 2N\u2211 j=1,j6=i log \u2016ui \u2212 uj\u2016\n= \u22122 N\u2211 i=1 log \u20162vi\u2016\n\u22122 N\u2211 i=1 N\u2211 j=1,j 6=i (log \u2016vi \u2212 vj\u2016+ log \u2016vi + vj\u2016)\n= C \u2212 2 N\u2211 i=1 N\u2211 j=1,j 6=i log (\u2016vi \u2212 vj\u2016 \u2016vi + vj\u2016)\n= C \u2212 2 N\u2211 i=1 N\u2211 j=1,j 6=i log (\u221a 2\u2212 2vTi vj \u221a 2 + 2vTi vj ) (27)\nRecall that N = 2n. By separating the summation term into two parts (each part has n\u00d7n term), we achieve equation (28).\nE(U) = C\u2212 2 2n\u2211 i=1 2n\u2211 j=1,j 6=i log\n( 2 \u221a 1\u2212 (vTi vj) 2 )\n= C \u2212 4 n\u2211 i=1 2n\u2211 j=n+1 log\n( 2 \u221a 1\u2212 (vTi vj) 2 )\n\u22124 n\u2211 i=1 n\u2211 j=1,j 6=i log\n( 2 \u221a 1\u2212 (vTi vj) 2 )\n(28)\nLet V\u00b7,1:n = [v1, ...,vn] and V\u00b7,n+1:2n = [vn+1, ...,v2n] be the matrix consisting of the first n and last n columns of V respectively. We can obtain equation (29).\nVT\u00b7,1:nV\u00b7,n+1:2n = 1 mReF\u039b T(\u2212ImF\u039b) + 1m (ImF\u039b) TReF\u039b (29)\nNote that all diagonal elements of VT\u00b7,1:nV\u00b7,n+1:2n are zero. By further separating the first summation term of equation (28) into two parts, we obtain equation (30).\nE(U) = C\u2212 4 n\u2211\ni=1 2n\u2211 j=n+i log ( 2 \u221a 1\u2212 0 ) \u22124\nn\u2211 i=1 2n\u2211 j=n+1,j 6=n+i log\n( 2 \u221a 1\u2212 (vTi vj) 2 )\n\u22124 n\u2211 i=1 n\u2211 j=1,j 6=i log\n( 2 \u221a 1\u2212 (vTi vj) 2 )\n= C \u2212 4 n\u2211 i=1 2n\u2211 j=n+1,j 6=n+i log\n( 2 \u221a 1\u2212 (vTi vj) 2 )\n\u22124 n\u2211 i=1 n\u2211 j=1,j 6=i log\n( 2 \u221a 1\u2212 (vTi vj) 2 )\n(30)\nTo be concise, let Z = [z1, , ..., zn] = 1\u221amF\u039b.\nFor 1 \u2264 j \u2264 n, j 6= i, we achieve equation (31).\n(vTi vj) 2 = (Rez\u2217i zj) 2 = ( 1 mRe m\u2211 s=1 e2\u03c0iksp/n )2\n(31)\nFor n+ 1 \u2264 j \u2264 2n, , j 6= n+ i, we attain equation (32).\n(vTi vj) 2 = (Imz\u2217i zj\u2212n) 2 = ( 1 m Im m\u2211 s=1 e2\u03c0iksp/n )2\n(32) In equation (31) and (32), p \u2261 i \u2212 j (mod n), where mod denotes the modulus operation on integers.\nNote that z\u2217i zj has at most n\u2212 1 distinct values when i 6= j (mod n) . Together with equation (30), we achieve equation (33).\nE(U) = C\u2212 4 n\u2211\ni=1 2n\u2211 j=n+1,j 6=n+i log\n( 2 \u221a 1\u2212 (vTi vj) 2 )\n\u22124 n\u2211 i=1 n\u2211 j=1,j 6=i log\n( 2 \u221a 1\u2212 (vTi vj) 2 )\n= C \u2212 4 n\u2211 i=1 2n\u2211 j=n+1,j 6=n+i log\n( 2 \u221a\n1\u2212 (Imz\u2217i zj\u2212n) 2 ) \u22124\nn\u2211 i=1 n\u2211 j=1,j 6=i log\n( 2 \u221a\n1\u2212 (Rez\u2217i zj) 2 ) = C \u2212 4n\nn\u22121\u2211 p=1 log\n( 2 \u221a 1\u2212 (Im 1m m\u2211 s=1 e2\u03c0iksp/n) 2 )\n\u22124n n\u22121\u2211 p=1 log\n( 2 \u221a 1\u2212 (Re 1m m\u2211 s=1 e2\u03c0iksp/n) 2 )\n= C \u2212 2n n\u22121\u2211 p=1 log ( 1\u2212 (Im 1m m\u2211 s=1 e2\u03c0iksp/n) 2)\n\u22122n n\u22121\u2211 p=1 log ( 1\u2212 (Re 1m m\u2211 s=1 e2\u03c0iksp/n) 2)\n(33)\nAlgorithm 1 Initialization: random sample \u039b = [k1, k2, ..., km] from {1, 2, ...n\u2212 1} without replacement. Set h\u0303 = 1TF\u039b repeat\nSet J = J(\u039b) for q = 1 to m do\nSet g = [e2\u03c0ikq/n, e2\u03c0ikq2/n..., e2\u03c0ikq(n\u22121)/n] Set h = h\u0303\u2212 g Find k\u2217q by k \u2217 q = arg max\nkq\u2208{1,...,n\u22121} J(kq) in (35)\nUpdate g = [e2\u03c0ik \u2217 q/n, e2\u03c0ik \u2217 q2/n..., e2\u03c0ik \u2217 q (n\u22121)/n]\nSet h\u0303 = h + g end for\nuntil J does not change\nFrom Theorem 4.1, we know that minimizing E(U) is equivalent to maximizing J(\u039b) which is defined in equation (34).\nJ(\u039b) = n\u22121\u2211 p=1 log ( 1\u2212 (Im 1m m\u2211 s=1 e2\u03c0iksp/n) 2)\n+ n\u22121\u2211 p=1 log ( 1\u2212 (Re 1m m\u2211 s=1 e2\u03c0iksp/n) 2) (34)\nBy keeping all the indexes in \u039b = [k1, k2, ..., km] fixed except the qth element, we can obtain equation (35).\nJ(kq) = n\u22121\u2211 p=1 log ( 1\u2212 (Im ( hp + e 2\u03c0ikqp/n ) /m) 2 )\n+ n\u22121\u2211 p=1 log ( 1\u2212 (Re ( hp + e 2\u03c0ikqp/n ) /m) 2 )\n(35) where kq \u2208 {1, 2, ...n\u2212 1}, hp = m\u2211\ns=1,s6=q e2\u03c0iksp/n.\nWith equation (35), we can maximize J(\u039b) by maximizing J(kq) with other indexes fixed each time. Let h = [h1, ..., hn\u22121] , g = [e2\u03c0ikq/n, e2\u03c0ikq2/n..., e2\u03c0ikq(n\u22121)/n]. 1 = [1, ..., 1]T \u2208 Rm is the vector of all ones. A coordinate ascent method to maximize J(\u039b) is given in Algorithm 1.\nObviously, it is a discrete optimization problem. Algorithm 1 can find a local optimum. The time complexity of the Algorithm 1 isO(Tmn2), where T denotes the number of outer iteration. Empirically, the outer iteration T is less than ten."}, {"heading": "5. Fast Feature Maps Construction", "text": "In this section, we will discuss how to construct SSF maps in loglinear time complexity and linear space complexity by using the structure property of V.\nTheorem 5.1 Assume that d = 2m,N = 2n, m < n. Let\nx = [ x1 x2 ] \u2208 R2m and z = x1 + ix2 \u2208 Cm. Given \u039b = [k1, k2, ..., km] \u2282 {1, ..., n \u2212 1} , let y \u2208 Cn with y\u039b = z. Other elements outside the index set \u039b are equal to zero. Given V defined in equation (23), equation (36) holds.\nVTx = 1\u221a m [Re(F \u2217y), Im(F \u2217y)] T (36)\nProof: Let \u2126 \u2208 Rn\u00d7n be a diagonal matrix with all diagonal elements inside the index set \u039b equal to one , the others equal to zero.\nVTx = 1\u221a m [ ReF\u039b \u2212ImF\u039b ImF\u039b ReF\u039b ]T [ x1 x2 ] = 1\u221a\nm\n[ (ReF\u039b T )x1 + (ImF\u039b T )x2\n(\u2212ImF\u039bT )x1 + (ReF\u039bT )x2 ] = 1\u221a\nm [ Re(F \u2217\u039bz) Im(F \u2217\u039bz) ] = 1\u221a\nm [ Re(F \u2217\u2126y) Im(F \u2217\u2126y) ] = 1\u221a\nm [ Re(F \u2217y) Im(F \u2217y)\n] (37)\nThus, the projection operation VTx (previously mentioned in equation (10) and (21)) can be calculated by Fast Fourier Transform algorithm (FFT) in O(n log n) time complexity. Because scaling and taking nonlinear transform can be finished in O(n), the total time complexity to construct SSF maps is O(n log n).\nAll steps to construct SSF maps are summarized as follows:\n(a) Compute x\u0303 by x\u0303 = Dx, where D \u2208 {\u22121,+1}d\u00d7d is a diagonal matrix where diagonal elements are uniformly sampled from {\u22121,+1}.\n(b) Construct y such that y\u039b = x\u03031 + ix\u03032, other elements outside the index set \u039b are equal to zero.\n(c) Compute VT x\u0303 by equation (36) via FFT.\n(d) Construct feature maps \u03a8 (x) via equation (10) or (21).\nFor each (m,n) pair , the index set \u039b only need to be computed once. It takes O(m) space to store \u039b. For shift and rotation invariant kernels, it takes O(M) space to store \u03a6\u2212(tj), j \u2208 1, ...,M and takes O(d) (d = 2m) space to store \u039b and D. For bth-order arc-cosine kernels, it only needs to store one parameter Cb and takes O(d) space to store \u039b and D. By setting M \u2264 d, the total space complexity to store the projection matrix is O(d)."}, {"heading": "6. Empirical Studies", "text": "We compare SSF maps with feature maps obtained by fully Gaussian (Cho & Saul, 2009; Rahimi et al., 2007), the Cir-\nculant (Choromanski & Sindhwani, 2016) matrices, QMC with Halton set and QMC with Sobol set (Avron et al., 2016). For Halton set and Sobol set, the implementation in MATLAB are employed in the experiments. The scrambling and shifting techniques are used for Haltonset and Sobolset. In all the experiments, we fix M = 1 (the number of one-dimensional QMC points) for SSF maps."}, {"heading": "6.1. Convergence and Speedup", "text": "First, the convergence of the logarithmic energy ( \u2212J(\u039b) in equation (34)) with (m,n) = (160, 1600) is shown in Figure 1. From Figure 1, we find that it takes less than ten iterations (i.e. T < 10) for Algorithm 1 to find a local optimum.\nSecond, the speedup results of all methods are shown in\nFigure 2. We set N = 2d for all the methods. The speedup of fully Gaussian projection is the baseline. We can observe that the speedup of QMC with Halton set is constant as the dimension d increases and is slower than the baseline. The speedup of both SSF maps and the Circulant increase fast as dimension increases, which is consistent with theoretical analysis. The speedup of Sobol set is not shown because the inbuilt Sobolset routine of MATLAB does not support dimension larger than 1,111."}, {"heading": "6.2. Approximation Accuracy", "text": "We evaluate reconstruction error of Gaussian kernel, zeroorder arc-cosine kernel and first-order arc-cosine kernel on CIFAR10 (Krizhevsky & Hinton, 2009), MNIST (LeCun & Cortes, 2010), usps and dna dataset. MNIST is a handwritten digit image dataset, which contains 70,000 samples with 784-dimensional features(pixel). For CIFAR10 with 60,000 samples, the 320-dimensional gist feature (Gong et al., 2013) are employed in the experiments. Both the relative Frobenius error (i.e. \u2016K\u0303\u2212K\u2016 F\n\u2016K\u2016F ) and the relative\nelement-wise maximum error (i.e. \u2016K\u0303\u2212K\u2016\u221e \u2016K\u2016\u221e ) are evaluated, where K and K\u0303 denote the exact and approximated Gram matrices respectively. The Frobenius norm and the elementwise maximum norm are defined as \u2016X\u2016F =\u221a\u2211\ni \u2211 j |Xij |\n2 and \u2016X\u2016\u221e = maxi,j |Xij | respectively.\nThe reconstruction error in the experiments is the mean value over 10 independent runs. The dimensions of the feature maps are set to {2\u00d7d, 3\u00d7d, 4\u00d7d, 5\u00d7d}, where d is the dimension of the data. For MNIST and CIFAR10 dataset, each run randomly select 2,000 samples to construct the Gram matrix. The mean value of the reconstruction errors with different norms on MNIST are shown in Figure 3. Results on the other datasets are similar to that of Figure 3. One can refer to the supplementary material for results on other datasets.\nFigure 3 shows that the feature maps obtained with fully Gaussian matrix, the Circulant matrix, QMC with Halton set and QMC with Sobol set have similar reconstruction error. SSF maps have the smallest approximation error among five methods. Especially for the first-order arccosine kernel, it achieves nearly one-fifth relative mean error and one-seventh relative max error of other methods. Moreover, even if M = 1, SSF maps can achieve about one-third relative mean error and half of the relative max error of other methods for Gaussian Kernel approximation."}, {"heading": "7. Conclusion", "text": "We propose Spherical Structured Feature (SSF) maps to approximate shift and rotation invariant kernels as well as bthorder arc-cosine kernels. SSF maps can achieve computation and storage efficiency as well as better approximation accuracy."}, {"heading": "Acknowledgements", "text": "We thank the anonymous reviewers for their valuable comments and suggestions."}], "year": 2017, "references": [{"title": "How can deep rectifier networks achieve linear separability and preserve distances", "authors": ["An", "Senjian", "Boussaid", "Farid", "Bennamoun", "Mohammed"], "venue": "In ICML,", "year": 2015}, {"title": "Quasi-monte carlo feature maps for shift-invariant kernels", "authors": ["Avron", "Haim", "Sindhwani", "Vikas", "Yang", "Jiyan", "Mahoney", "Michael W"], "venue": "Journal of Machine Learning Research,", "year": 2016}, {"title": "Qmc designs: optimal order quasi monte carlo integration schemes on the sphere", "authors": ["J Brauchart", "E Saff", "I Sloan", "R. Womersley"], "venue": "Mathematics of computation,", "year": 2014}, {"title": "Distributing many points on spheres: minimal energy and designs", "authors": ["Brauchart", "Johann S", "Grabner", "Peter J"], "venue": "Journal of Complexity,", "year": 2015}, {"title": "Libsvm: a library for support vector machines", "authors": ["Chang", "Chih-Chung", "Lin", "Chih-Jen"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST),", "year": 2011}, {"title": "Kernel methods for deep learning", "authors": ["Cho", "Youngmin", "Saul", "Lawrence K"], "venue": "In Advances in neural information processing systems,", "year": 2009}, {"title": "Recycling randomness with structure for sublinear time kernel expansions", "authors": ["Choromanski", "Krzysztof", "Sindhwani", "Vikas"], "year": 2016}, {"title": "Practical learning of deep gaussian processes via random fourier features", "authors": ["Cutajar", "Kurt", "Bonilla", "Edwin V", "Michiardi", "Pietro", "Filippone", "Maurizio"], "venue": "arXiv preprint arXiv:1610.04386,", "year": 2016}, {"title": "Highdimensional integration: the quasi-monte carlo way", "authors": ["Dick", "Josef", "Kuo", "Frances Y", "Sloan", "Ian H"], "venue": "Acta Numerica,", "year": 2013}, {"title": "Liblinear: A library for large linear classification", "authors": ["Fan", "Rong-En", "Chang", "Kai-Wei", "Hsieh", "Cho-Jui", "Wang", "Xiang-Rui", "Lin", "Chih-Jen"], "venue": "Journal of machine learning research,", "year": 2008}, {"title": "Random feature mapping with signed circulant matrix projection", "authors": ["Feng", "Chang", "Hu", "Qinghua", "Liao", "Shizhong"], "venue": "In IJCAI, pp", "year": 2015}, {"title": "On the riesz energy of measures", "authors": ["G\u00f6tz", "Mario"], "venue": "Journal of Approximation Theory,", "year": 2003}, {"title": "Learning multiple layers of features from tiny images", "authors": ["Krizhevsky", "Alex", "Hinton", "Geoffrey"], "year": 2009}, {"title": "Fastfoodapproximating kernel expansions in loglinear time", "authors": ["Le", "Quoc", "Sarl\u00f3s", "Tam\u00e1s", "Smola", "Alex"], "venue": "In Proceedings of the international conference on machine learning,", "year": 2013}, {"title": "Bayesian nonparametric kernel-learning", "authors": ["Oliva", "Junier B", "Dubey", "Avinava", "Poczos", "Barnabas", "Schneider", "Jeff", "Xing", "Eric P"], "venue": "In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics,", "year": 2016}, {"title": "Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning", "authors": ["Rahimi", "Ali", "Recht", "Benjamin"], "venue": "In Advances in neural information processing systems,", "year": 2009}, {"title": "Random features for large-scale kernel machines", "authors": ["Rahimi", "Ali", "Recht", "Benjamin"], "venue": "In NIPS,", "year": 2007}, {"title": "Gaussian processes for machine learning", "authors": ["Rasmussen", "Carl Edward"], "year": 2006}, {"title": "Fourier analysis on groups", "authors": ["Rudin", "Walter"], "year": 2011}, {"title": "Practical bayesian optimization of machine learning algorithms", "authors": ["Snoek", "Jasper", "Larochelle", "Hugo", "Adams", "Ryan P"], "venue": "In Advances in neural information processing systems,", "year": 2012}, {"title": "Gaussian process optimization in the bandit setting: No regret and experimental design", "authors": ["Srinivas", "Niranjan", "Krause", "Andreas", "Kakade", "Sham M", "Seeger", "Matthias"], "venue": "arXiv preprint arXiv:0912.3995,", "year": 2009}, {"title": "On the error of random fourier features", "authors": ["Sutherland", "Dougal J", "Schneider", "Jeff"], "venue": "arXiv preprint arXiv:1506.02785,", "year": 2015}, {"title": "Quasimonte carlo feature maps for shift-invariant", "authors": ["J. Yang", "Sindhwani. V. Avron H. Mahoney M"], "venue": "kernels. ICML,", "year": 2014}], "id": "SP:b6e7cce12b7b0327c164602b5a328fb209506296", "authors": [{"name": "Yueming Lyu", "affiliations": []}], "abstractText": "We propose Spherical Structured Feature (SSF) maps to approximate shift and rotation invariant kernels as well as b-order arc-cosine kernels (Cho & Saul, 2009). We construct SSF maps based on the point set on d \u2212 1 dimensional sphere Sd\u22121. We prove that the inner product of SSF maps are unbiased estimates for above kernels if asymptotically uniformly distributed point set on Sd\u22121 is given. According to (Brauchart & Grabner, 2015), optimizing the discrete Riesz s-energy can generate asymptotically uniformly distributed point set on Sd\u22121. Thus, we propose an efficient coordinate decent method to find a local optimum of the discrete Riesz s-energy for SSF maps construction. Theoretically, SSF maps construction achieves linear space complexity and loglinear time complexity. Empirically, SSF maps achieve superior performance compared with other methods.", "title": "Spherical Structured Feature Maps for Kernel Approximation"}