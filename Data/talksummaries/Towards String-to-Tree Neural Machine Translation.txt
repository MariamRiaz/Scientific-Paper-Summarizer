0	65	Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014) has recently became the state-of-the-art approach to machine translation (Bojar et al., 2016), while being much simpler than the previously dominant phrase-based statistical machine translation (SMT) approaches (Koehn, 2010).
1	21	NMT models usually do not make explicit use of syntactic information about the languages at hand.
3	61	One prominent approach to syntaxbased SMT is string-to-tree (S2T) translation (Yamada and Knight, 2001, 2002), in which a sourcelanguage string is translated into a target-language tree.
4	57	S2T approaches to SMT help to ensure the resulting translations have valid syntactic structure, while also mediating flexible reordering between the source and target languages.
5	17	The main formalism driving current S2T SMT systems is GHKM rules (Galley et al., 2004, 2006), which are synchronous transduction grammar (STSG) fragments, extracted from word-aligned sentence pairs with syntactic trees on one side.
9	20	Namely, we translate a source sentence into a linearized, lexicalized constituency tree, as demonstrated in Figure 2.
17	65	In parallel and highly related to our work, Eriguchi et al. (2017) proposed to model the target syntax in NMT in the form of dependency trees by using an RNNG-based decoder (Dyer et al., 2016), while Nadejde et al. (2017) incorporated target syntax by predicting CCG tags serialized into the target translation.
20	26	We then experiment in a low-resource scenario using the German, Russian and Czech to English training data from the News Commentary v8 corpus, following Eriguchi et al. (2017).
21	18	In all cases we parse the English sentences into constituency trees using the BLLIP parser (Charniak and Johnson, 2005).1 To enable an open vocabulary translation we used sub-word units obtained via BPE (Sennrich et al., 2016b) on both source and target.2 In each experiment we train two models.
24	34	We use the NEMATUS (Sennrich et al., 2017)3 implementation of an attention-based NMT model.4 We trained the models until there was no improvement on the development set in 10 consecutive checkpoints.
26	14	For all models we report results of the best performing single model on the dev-set (newstest2013+newstest2014 in the resource rich setting, newstest2015 in the rest, as measured by BLEU) when translating newstest2015 and newstest2016, similarly to Sennrich et al. (2016a); Eriguchi et al. (2017).
36	13	In order to better understand where or how the syntactic information improves translation quality, we perform a closer analysis of the WMT16 experiment.
38	54	While we did not perform an in-depth error-analysis, the trees seem to follow the syntax of English, and most choices seem reasonable.
42	15	We compare the amount of reordering in the bpe2bpe and bpe2tree models using a distortion score based on the alignments derived from the attention weights of the corresponding systems.
44	31	For an n-word target sentence t and source sentence s let a(i) be the position of the source word aligned to the target word in position i.
49	27	The bpe2tree model has more translations with distortion scores in bins 1- onward and significantly less translations in the least-reordering bin (0) when compared to the bpe2bpe model, indicating that the syntactic information encouraged the model to perform more reordering.5 Figure 4 tracks the distortion scores throughout the learning process, plotting the average dev-set scores for the model checkpoints saved every 30k updates.
52	27	Tying Reordering and Syntax The bpe2tree model generates translations with their constituency tree and their attention-derived alignments.
53	20	We can use this information to extract GHKM rules (Galley et al., 2004).6 We derive 6github.com/joshua-decoder/galley-ghkm hard alignments for that purpose by treating every source/target token-pair with attention score above 0.5 as an alignment.
57	29	The most common rule, VP(x0:TER x1:NP) → x1 x0, found in 184 sentences in the dev set (8.4%), is indicating that the sequence x1 x0 in German was reordered to form a verb phrase in English, in which x0 is a terminal and x1 is a noun phrase.
58	15	The extracted GHKM rules reveal very sensible German-English reordering patterns.
59	20	Relative Constructions Browsing the produced trees hints at a tendency of the syntax-aware model to favor using relative-clause structures and subordination over other syntactic constructions (i.e., “several cameras that are all priced...” vs. “several cameras, all priced...”).
60	49	To quantify this, we count the English relative pronouns (who, which, that7, whom, whose) found in the newstest2015 translations of each model and in the reference translations, as shown in Figure 5.
63	14	Figure 6 in the supplementary material presents an example of a complete attention matrix, including the syntactic brackets.
64	34	While making full sense of the attention patterns of the syntactic elements remains a challenge, one clear trend is that opening the very first bracket of the sentence consistently attends to the main verb or to structural markers (i.e. question marks, hyphens) in the source sentence, suggesting a planning-ahead behavior of the decoder.
67	26	In general, we find the alignments from the syntax-based system more sensible (i.e. in Figure 1 – the bpe2bpe alignments are off-by-1).
68	24	Qualitative Analysis and Human Evaluations The bpe2tree translations read better than their bpe2bpe counterparts, both syntactically and semantically, and we highlight some examples which demonstrate this.
70	34	We also performed a small-scale human-evaluation using mechanical turk on the first 500 sentences in the dev-set.
72	14	The results are summarized in the following table: 2bpe weakly better 100 2bpe strongly better 54 2tree weakly better 122 2tree strongly better 64 both good 26 both bad 3 disagree 131 As can be seen, in 186 cases (37.2%) the human evaluators preferred the bpe2tree translations, vs. 154 cases (30.8%) for bpe2bpe, with the rest of the cases (30%) being neutral.
74	207	While this work shows syntactic information about the target side can be beneficial for NMT, this paper only scratches the surface with what can be done on the subject.
75	52	First, better models can be proposed to alleviate the long sequence problem in the linearized approach or allow a more natural tree decoding scheme (Alvarez-Melis and Jaakkola, 2017).
