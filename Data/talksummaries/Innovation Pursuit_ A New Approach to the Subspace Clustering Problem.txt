2	19	The problem of subspace clustering is concerned with learning these low-dimensional subspaces and clustering the data points to their respective subspaces, generally without prior knowledge about the number of subspaces and their dimensions, nor the membership of the data points to these subspaces.
3	27	Subspace clustering naturally arises in many machine learning and data analysis problems, including computer vision (e.g. motion segmentation (Vidal et al., 2008), face clustering (Ho et al., 2003)), image processing (Yang et al., 2008) and system identification (Vidal et al., 2003).
4	71	Numerous approaches for subspace clustering have been studied in prior work, including statistical-based approaches (Yang et al., 2006; Rao et al., 2010), spectral clustering (Soltanolkotabi et al., 2012; Von Luxburg, 2007; Dyer et al., 2013; Elhamifar & Vidal, 2013; Heckel & Bölcskei, 2013; Liu et al., 2013; Chen & Lerman, 2009), the algebraic-geometric approach (Vidal et al., 2005) and iterative methods (Bradley & Mangasarian, 2000).
8	25	Second, the proposed method is a provable and scalable subspace clustering algorithm – the computational complexity of iPursuit only scales linearly in the number of subspaces and quadratically in their dimensions (c.f.
9	54	In contrast to the spectral-clustering-based algorithms such as (Dyer et al., 2013; Elhamifar & Vidal, 2013; Liu et al., 2013), which need to solve an M22 -dimensional optimization problem to build the similarity matrix (where M2 is the number of data points), the proposed method requires solving few M2-dimensional linear optimization problems.
10	47	This feature makes iPursuit remarkably faster than the state-of-the-art algorithms.
11	99	Third, innovation pursuit in the data span enables superior performance when the subspaces have considerable intersections in comparison to the state-of-the-art subspace clustering algorithms.
25	19	In other words, I (S2 ⊥ S1) is the complement of S1 in the subspace S1 ⊕ S2.
38	28	Moreover, it is assumed that every subspace in the set of subspaces {Si}Ni=1 has an innovation over the other subspaces, to say that, for 1 ≤ i ≤ N , the subspace Si does not completely lie in N ⊕ k=1 k 6=i Sk .
42	20	One subspace is the identified subspace and the other one is the direct sum of the other subspaces.
50	22	Define c∗ as the optimal point of the following optimization problem min ĉ ‖ĉTD‖0 subject to ĉ ∈ D and ‖ĉ‖ = 1, (1) where ‖.‖0 is the `0-norm.
59	29	Since the optimal vector should be orthogonal to the maximum number of data points, it is highly likely that c∗ 6∈ S1.
68	63	The columns of D corresponding to the nonzero elements of (c∗)TD span S2 if the following conditions are satisfied: (i) c∗ ∈ I (S2 ⊥ S1) , (ii) D2 cannot follow Data model 1 with N > 1, that is, the data points in D2 do not lie in the union of lower dimensional subspaces within S2 each with innovation w.r.t.
81	34	In order to show that the optimal point of (IP) yields correct clustering, it suffices to show that the optimal point lies in I (S2 ⊥ S1) given that condition (ii) of Lemma 1 is satisfied for D2 (or lies in I (S1 ⊥ S2) given that the condition is satisfied for D1).
84	24	Hence, assumption (4) does not lead to any loss of generality.
87	31	(5) Henceforth, “innovation subspace” refers to I (S2 ⊥ S1) whenever the two-subspace scenario is considered and (5) is satisfied.
89	49	These conditions are characterized in terms of the optimal solution to an oracle optimization problem (OP), wherein the feasible set of (IP) is replaced by the innovation subspace.
94	30	Also, assume that condition (5) and the requirement of Lemma 1 for D2 are satisfied (condition (ii) of Lemma 1).
96	23	Also, let P2 denote an orthonormal basis for I (S2 ⊥ S1) and assume that q is a unit `2-norm vector in D that is not orthogonal to I (S2 ⊥ S1).
99	47	Data distribution: The LHS of (7) is known as the permeance statistic, an efficient measure of how well the data points are distributed in a subspace (Lerman et al., 2015).
102	62	From this definition, we see that the permeance statistic is fairly small if a set of data points are aligned along a given direction (i.e. not well distribued).
105	22	The coherency of q with I (S2 ⊥ S1): For the optimal point of (IP) to lie in I (S2 ⊥ S1), the vector q should not be too coherent with S1.
106	25	This can be seen by observing that if q has a small projection on I (S2 ⊥ S1) – in which case it would be more coherent with S1 – the Euclidean norm of any feasible point of (3) lying in I (S2 ⊥ S1) will have to be large to satisfy the equality constraint in (IP).
137	25	Define U as an orthonormal basis for D. Thus, the optimization problem (3) is equivalent to min a ‖aTUTD‖1 subject to aTUTq = 1.
177	17	When the data is noisy and the singular values of D decay rapidly, it may be hard to accurately estimate r. If the dimension is incorrectly estimated, Q may contain some singular vectors corresponding to the noise component, wherefore the optimal point of (12) could end up lying close to a noise singular vector.
186	17	Specifically, we use the data point that is closest to the least dominant singular vector rather than the least dominant singular vector itself.
250	29	Choosing the vector q: Set q = the column of De closest to the last column of Q.
258	71	Assign dei to the identified subspace if ‖FT1 dei‖ ≥ ‖FT2 dei‖.
259	88	Remove the data points belonging to the identified subspace: Update De by removing the columns corresponding to the identified subspace.
260	148	End While Acknowledgment: This work was supported by NSF CAREER Award CCF-1552497 and NSF Grant CCF1320547.
