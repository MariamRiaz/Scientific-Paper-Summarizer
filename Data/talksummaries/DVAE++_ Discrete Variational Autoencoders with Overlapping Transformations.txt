0	54	Recent years have seen rapid progress in generative modeling made possible by advances in deep learning and stochastic variational inference.
1	41	The reparameterization trick (Kingma & Welling, 2014; Rezende et al., 2014) has made stochastic variational inference efficient by providing lower-variance gradient estimates.
2	6	However, reparameterization, as originally proposed, does not easily extend to semi-supervised learning, binary latent attribute models, topic modeling, variational memory addressing, hard attention models, or clustering, which require discrete latentvariables.
3	22	Continuous relaxations have been proposed for accommodating discrete variables in variational inference (Maddison et al., 2016; Jang et al., 2016; Rolfe, 2016).
4	112	The Gumbel- Softmax technique (Maddison et al., 2016; Jang et al., 2016) defines a temperature-based continuous distribution that in the zero-temperature limit converges to a discrete distribution.
5	7	However, it is limited to categorical distributions and does not scale to multivariate models such as Boltzmann machines (BM).
6	14	The approach presented in (Rolfe, 2016) can train models with BM priors but requires careful handling of the gradients during training.
7	36	We propose a new class of smoothing transformations for relaxing binary latent variables.
8	36	The method relies on two distributions with overlapping support that in the zero temperature limit converge to a Bernoulli distribution.
9	20	We present two variants of smoothing transformations using a mixture of exponential and a mixture of logistic distributions.
10	31	We demonstrate that overlapping transformations can be used to train discrete directed latent models as in (Maddison et al., 2016; Jang et al., 2016), and models with BMs in their prior as in (Rolfe, 2016).
11	53	In the case of BM priors, we show that the Kullback-Leibler (KL) contribution to the variational bound can be approximated using an analytic expression that can be optimized using automatic differentiation without requiring the special treatment of gradients in (Rolfe, 2016).
12	43	Using this analytic bound, we develop a new variational autoencoder (VAE) architecture called DVAE++, which uses a BM prior to model discontinuous latent factors such as object categories or scene configuration in images.
13	40	DVAE++ is inspired by (Rolfe, 2016) and includes continuous local latent variables to model locally smooth features in the data.
14	46	DVAE++ achieves comparable results to the state-of-the-art techniques on several datasets and captures semantically meaningful discrete aspects of the data.
15	119	We show that even when all continuous latent variables are removed, DVAE++ still attains near state-of-the-art generative likelihoods.
34	74	Let x represent observed random variables and z latent variables.
35	153	The joint distribution over these variables is defined by the generative model p(x,z) = p(z)p(x|z), where p(z) is a prior distribution and p(x|z) is a probabilistic decoder.
36	25	,x(N)}, the parameters of the model are trained by maximizing the log-likelihood: log p(X ) = N� i=1 log p(x(i)).
39	15	In the VAE, instead of the maximizing the marginal log-likelihood, a variational lower bound (ELBO) is maximized: log p(x) ≥ Eq(z|x) � log p(x|z) � − KL � q(z |x)||p(z) � .
