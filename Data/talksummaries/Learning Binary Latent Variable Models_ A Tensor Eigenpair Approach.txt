0	21	In this paper we propose a spectral method for learning the following binary latent variable model, shown in Figure 1.
2	23	The observed vector x ∈ Rm with m ≥ d features is modeled as x = W>h+ σξ, (1) whereW ∈ Rd×m is an unknown weight matrix assumed to be full rank d. Here, σ ≥ 0 is the noise level and ξ is an additive noise vector independent of h, whose m coordinates are all i.i.d.
3	11	zero mean and unit variance random variables.
5	31	The model in (1) appears, for example, in overlapping clustering (Banerjee et al., 2005; Baadel et al., 2016), in various problems in bioinformatics (Segal et al., 2002; Becker et al., 2011; Slawski et al., 2013), and in blind source separation (Van der Veen, 1997).
6	11	A special instance of model (1) is the Gaussian-Bernoulli restricted Boltzmann machine (GRBM) where the distribution Ph is further assumed to have a parametric energy-based structure (Hinton & Salakhutdinov, 2006; Cho et al., 2011; Wang et al., 2012).
15	20	For example, when Ph is a product distribution, the learning problem becomes that of independent component analysis (ICA) with binary signals (Hyvärinen et al., 2004).
16	16	In this case, several methods were derived for estimating W and under suitable non-degeneracy conditions were proven to be both computationally efficient and statistically consistent (Shalvi & Weinstein, 1993; Frieze et al., 1996; Regalia & Kofidis, 2003; Hyvärinen et al., 2004; Anandkumar et al., 2014; Jain & Oh, 2014).
17	22	Similarly, when the hidden units are mutually exclusive, namely Ph has support h ∈ {ei}di=1, the model is a Gaussian mixture (GMM) with d spherical components with linearly independent means.
26	15	We prove that our method is consistent under mild non-degeneracy conditions and achieves the parametric rate OP (n − 12 ) for any noise level σ ≥ 0.
64	11	,xn] ∈ Rm×n of n samples into a product of real and binary low-rank matrices,1 Find W ∈ Rd×m, H ∈ {0, 1}d×n s.t.
67	15	A sufficient condition for uniqueness, similar to the one posed in Slawski et al. (2013), is that H is rigid.
70	15	If there exists a positive constant p0 > 0 such that Ph(ei) ≥ p0 and Ph(ei + ej) ≥ p0, then for a sample size n > 2 log(d)/p0 the matrix H is rigid with high probability.
72	12	Let X = W>H with H ∈ {0, 1}d×n rigid and W ∈ Rd×m full rank with m ≥ d. Let W † ∈ Rm×d be the unique right pseudo-inverse of W so WW † = Id.
74	22	,v∗d} ⊆ span(X) of d non-zero vectors that satisfy the binary constraints v∗i >X ∈ {0, 1}n. The weight matrix is then W = (W †)†.
75	12	Algorithm outline We recover W † via a two step procedure.
79	10	Typically, the size of V will be much larger than d, so in the second step V is filtered by selecting all v ∈ V that satisfy v>X ∈ {0, 1}n. Before describing the two steps in more detail we first state the additional non-degeneracy conditions we pose.
102	10	Unfortunately, computing the set of all eigenpairs of a general symmetric tensor is computationally hard (Hillar & Lim, 2013).
119	22	This is because SHL constructs a candidate set of size 2d that can be computed by a suitable linear transformation of the fixed set {0, 1}d, as opposed to our candidate set which is constructed by eigenpairs of a d × d × d tensor.
126	16	To this end, in addition to the second and third order moments M andM in (9), we also consider the first order moment µ = E[x] and define the following noise corrected moments, Mσ = M − σ2Im, Mσ = M− σ2 m∑ i=1 ( µ⊗ ei ⊗ ei + ei ⊗ µ⊗ ei + ei ⊗ ei ⊗ µ ) .
131	12	Assuming m > d, the parameters σ2 and d can be consistently estimated, for example, by the methods in Kritchman & Nadler (2009).
134	18	So, after computing the plugin estimates K̂σ such that K̂>σ M̂σK̂σ = Id and Ŵσ = M̂σ(K̂σ, K̂σ, K̂σ), we compute the set Ûσ of eigenpairs of Ŵσ and for some small 0 < τ = O(n− 1 2 ) take our candidate set as V̂σ = {K̂σu/λ : (u, λ) ∈ Ûσ with λ ≥ 1−τ}.
173	10	(30) Algorithm 1 Estimate W when σ > 0 and n <∞ Input: sample matrix X ∈ Rm×n and 0 < τ 1 1: estimate number of hidden units d and noise level σ2 2: compute empirical moments µ̂, M̂ and M̂ and plugin moments M̂σ and M̂σ of (17) 3: compute K̂σ such that K̂>σ M̂σK̂σ = Id 4: construct Ŵσ = M̂σ(K̂σ, K̂σ, K̂σ) 5: compute the set Ûσ of eigenpairs of Ŵσ 6: compute the candidate set V̂σ in (20) 7: for each v̂ ∈ V̂σ compute its KS score ∆n(v̂) in (30) 8: select V̄σ ⊆ V̂σ of d vectors with smallest ∆n(v̂) 9: return the pseudo-inverse Ŵ = V̄ †σ Our estimator V̄σ ⊆ V̂σ for W † is then the set of d vectors with the smallest scores ∆n(v̂).
206	29	Figure 2 shows the error, in Frobenius norm, averaged over 50 independent realizations of X as a function of n (upper panel) and σ (lower panel) for 5 methods: (i) our spectral approach, Algorithm 1 (Spectral); (ii) Algorithm 1 followed by a single weighted least squares step (Appendix K) (Spectral+WLS); (iii) SHL, the matrix decomposition method of Slawski et al. (2013)3; (iv) ALS with a random initialization (Appendix L); and (v) an oracle estimator that is given the exact matrix H and computes W via least squares.
209	12	2, at low levels of noise our method is comparable to SHL, whereas at high levels it is far more accurate.
210	11	Finally, adding a weighted least squares step reduces the error for low noise levels, but increases the error for high noise levels.
213	39	Admixture refers to the mixing of d ≥ 2 ancestral populations that were long separated, e.g., due to geographical or cultural barriers (Pritchard et al., 2000; Alexander et al., 2009; Li et al., 2008).
218	20	The allele frequency matrix H ∈ [0, 1]d×n whose entry Hkj is the frequency of the reference allele at locus j ∈ [n] in ancestral population k ∈ [d]; and the admixture proportion matrix W ∈ [0, 1]d×m whose columns sum to 1 and its entry Wki is the proportion of individual i’s genome that was inherited from population k. A common model for X in terms of W and H is to assume that the number of alleles 2Xij ∈ {0, 1, 2} is the sum of two i.i.d.
226	11	Next, the columns of W were sampled from a symmetric Dirichlet distribution with parameter α ≥ 0.
234	47	Instead, we implemented a matrix completion algorithm derived in (Jain & Oh, 2014) for a similar setup, see Appendix J for more details.
236	24	However, as d is relatively small, we performed exhaustive search over all candidate subsets of size d and choose the one that maximized the likelihood.
