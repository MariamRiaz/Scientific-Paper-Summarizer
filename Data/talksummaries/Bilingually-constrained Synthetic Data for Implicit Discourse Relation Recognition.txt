5	15	These data are usually called as synthetic implicit data (hereafter SynData).
6	17	However, Sporleder and Lascarides (2008) argue that SynData has two drawbacks: 1) meaning shifts in some cases when removing connectives, and 2) a different word distribution with the real implicit data.
7	11	They also show that using SynData directly degrades the performance.
8	14	Recent work seeks to derive valuable information from SynData while filtering noise, via domain adaptation (Braud and Denis, 2014; Ji et al., 2015), classifying connectives (Rutherford and Xue, 2015) or multi-task learning (Lan et al., 2013; Liu et al., 2016), and shows promising results.
9	37	Different from previous work, we propose to construct bilingually-constrained synthetic implicit data (called BiSynData) for DRRimp, which can alleviate the drawbacks of SynData.
10	8	Our method is inspired by the findings that a discourse instance expressed implicitly in one language may be expressed explicitly in another.
11	20	For example, Zhou and Xue 2306 (2012) show that the connectives in Chinese omit much more frequently than those in English with about 82.0% vs. 54.5%.
13	61	As illustrated in Figure 1, a Chinese implicit instance where the connective ´ is absent, is translated into an English explicit one with the connective but.
16	11	Meanwhile, for the English explicit instance, it is very likely that removing but would not lose any information since its Chinese counterpart ´ can be omitted.
17	4	Therefore it could be used for the English DRRimp, alleviating the meaning shift problem of SynData.
24	2	Therefore, we can construct two synthetic implicit instances labeled by Connen, denoted as 〈(Arg1en, Arg2en), Connen〉 and 〈(Arg1ch, Arg2ch), Connen〉, respectively.
26	9	In our experiments, we extract our BiSynData from a combined corpus (FBIS and HongKong Law), with about 2.38 million Chinese-English sentence pairs.
27	6	We generate 30,032 synthetic English instances and the same number of Chinese instances, with 80 connectives, as our BiSynData.
28	15	Table 1 lists the top 10 most frequent connectives in our BiSynData, which are roughly consistent with the statistics of Chinese/English implicit/explicit mismatches in (Li et al., 2014a).
32	4	Overall, our constructed BiSynData covers all four main discourse relations defined in the PDTB.
34	10	We incorporate the first task to help the English DRRimp, and the second for the Chinese DRRimp.
36	5	We design a Multi-task Neural Network Model (denoted as MTN ), which incorporates a connective classification task on BiSynData (auxiliary task) to benefit DRRimp (main task).
40	5	For each task, given an instance (Arg1, Arg2), MTN simply averages embeddings of words to represent arguments, as vArg1 and vArg2 .
41	38	These two vectors are then concatenated and transformed through two non-linear hidden layers.
44	22	The idea behind MTN is borrowed from (Iyyer et al., 2015), where a deep averaging network achieves close to the state-ofthe-art performance on text classification.
46	62	In addition, the simplicity of MTN allows us to focus on measuring the quality of BiSynData.
48	45	Pre-trained word embeddings are fixed.
49	3	We find that fine-tuning word embeddings during training leads to severe overfitting in our experiments.
50	1	Following Liu et al. (2016), we alternately use two tasks to train the model, one task per epoch.
52	9	The dimension of word embedding is 100.
56	13	To avoid overfitting, we randomly drop out 20% words in each argument following Iyyer et al. (2015).
60	7	The English/Chinese Gigaword corpus (3rd edition) is used to train the English/Chinese word embeddings via word2vec (Mikolov et al., 2013), respectively.
61	85	Due to the skewed class distribution of test data (see Section 4.1), we use the macro-averaged F1 for performance evaluation.
62	8	Following Rutherford and Xue (2015), we perform a 4-way classification on the top-level discourse relations: Temporal (Temp), Comparison (Comp), Contigency (Cont) and Expansion (Expa).
