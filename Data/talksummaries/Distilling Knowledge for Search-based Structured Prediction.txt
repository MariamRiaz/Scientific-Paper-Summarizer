0	34	Search-based structured prediction models the generation of natural language structure (part-ofspeech tags, syntax tree, translations, semantic graphs, etc.)
1	17	as a search problem (Collins and Roark, 2004; Liang et al., 2006; Zhang and Clark, 2008; Huang et al., 2012; Sutskever et al., 2014; Goodman et al., 2016).
16	13	Combing the distillation from reference and exploration further improves our single model’s performance.
18	24	We conduct experiments on two typical searchbased structured prediction tasks: transition-based dependency parsing and neural machine translation.
22	35	Major contributions of this paper include: • We study the knowledge distillation in search-based structured prediction and propose to distill the knowledge of an ensemble into a single model by learning to match its distribution on both the reference states (§3.2) and exploration states encountered when using the ensemble to explore the search space (§3.3).
24	12	• We conduct experiments on two search-based structured prediction problems: transitionbased dependency parsing and neural machine translation.
28	59	Search-based structured prediction (Collins and Roark, 2004; Daumé III et al., 2005; Daumé III et al., 2009; Ross and Bagnell, 2010; Ross et al., 2011; Doppa et al., 2014; Vlachos and Clark, 2014; Chang et al., 2015) models the generation of the structure as a search problem and it can be formalized as a tuple (S,A, T (s, a),S0,ST ), in which S is a set of states, A is a set of actions, T is a function that maps S × A → S, S0 is a set of initial states, and ST is a set of terminal states.
30	27	Several natural language structured prediction problems can be modeled under the search-based framework including dependency parsing (Nivre, 2008) and neural machine translation (Liang et al., 2006; Sutskever et al., 2014).
36	12	Algorithm 1 shows the common practices in training p(a | s), which involves: first, using πR(s,y) to generate a sequence of reference states and actions on the training data (line 1 to line 11 in Algorithm 1); second, using the states and actions on the reference sequences as examples to train p(a | s) with negative log-likelihood (NLL) loss (line 12 in Algorithm 1), LNLL = ∑ s∈D ∑ a −1{a = πR} · log p(a | s) where D is a set of training data.
42	64	Besides the ambiguity problem, training and testing discrepancy is another problem that lags the search-based structured prediction performance.
47	46	Knowledge distillation (Buciluǎ et al., 2006; Ba and Caruana, 2014; Hinton et al., 2015) is a class of methods for transferring the generalization ability of the cumbersome teacher model into a small student model.
64	47	To take the advantage of the ensemble model while avoid running the models multiple times, we use the knowledge distillation technique to distill a single model from the ensemble.
68	15	In the scenario of search-based structured prediction, transferring the teacher model’s generalization ability into a student model not only includes matching the teacher model’s soft targets on the reference search sequence, but also imitating the search decisions made by the teacher model.
70	26	Input: training data: {x(n),y(n)}Nn=1; the reference policy: πR(s,y); the exploration policy: πE(s) which samples an action from the annealed ensemble q(a | s) 1 T Output: classifier p(a | s).
71	20	1 D ← ∅; 2 for n← 1...N do 3 t← 0; 4 st ← s0(x(n)); 5 while st /∈ ST do 6 if distilling from reference then 7 at ← πR(st,y(n)); 8 else 9 at ← πE(st); 10 end 11 D ← D ∪ {st}; 12 st+1 ← T (st, at); 13 t← t+ 1; 14 end 15 end 16 if distilling from reference then 17 optimize αLKD + (1− α)LNLL; 18 else 19 optimize LKD; 20 end search sequence from the ensemble and learn from the soft target on the sampled states.
109	23	For our distillation from reference, when setting α = 1.0, best performance on development set is achieved and the test LAS is 91.99.
128	13	We also compare our model with other translation models including the one trained with reinforcement learning (Ranzato et al., 2015) and that using beam search in training (Wiseman and Rush, 2016).
139	16	As mentioned in previous sections, “problematic” states which is either ambiguous or non-optimal harm structured prediciton’s performance.
141	14	To empirically testify this, we use dependency parsing as a testbed and study the ensemble’s output distribution using the dynamic oracle.
148	18	The comparison in Table 4 shows that the ensemble model significantly outperforms the baseline on ambiguous and non-optimal states.
152	43	Over our distillation from reference model, we study the effect of α in Equation 1.
153	13	We vary α from 0 to 1 by a step of 0.1 in both the transitionbased dependency parsing and neural machine translation experiments and plot the model’s performance on development sets in Figure 4.
156	51	There is only 0.2 point of difference between the best α model and the one with α equals to 1.
158	200	It also indicates that fully learning from the distillation loss outputted by the ensemble is reasonable because models configured with α = 1 generally achieves good performance.
159	14	Besides the improved performance, knowledge distillation also leads to more stable learning.
161	44	Table 5 also reveals the smaller standard derivations are achieved by our distillation methods.
162	41	As Keskar et al. (2016) pointed out that the general- ization gap is not due to overfit, but due to the network converge to sharp minimizer which generalizes worse, we attribute the more stable training from our distillation model as the distillation loss presents less sharp minimizers.
175	69	In this paper, we study knowledge distillation for search-based structured prediction and propose to distill an ensemble into a single model both from reference and exploration states.
176	62	Experiments on transition-based dependency parsing and machine translation show that our distillation method significantly improves the single model’s performance.
177	110	Comparison analysis gives empirically guarantee for our distillation method.
