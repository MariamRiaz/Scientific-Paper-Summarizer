{"sections": [{"heading": "1. Introduction", "text": "Spectral clustering (SC) is one of the most utilized methods for clustering multivariate data (Von Luxburg, 2007; Fortunato, 2010). However, because of its inherent dependence on the spectrum of some large graph, SC is computationally expensive. Let n and k be the number of nodes and clusters, respectively. Clustering a graph takes O(n3) operations if a full eigendecomposition is performed and O(kn2) if the Lanczos method is used. This has motivated a surge of research focusing in reducing its complexity, for example using matrix sketching (Fowlkes et al., 2004; Li et al., 2011; Gittens et al., 2013), coarsening (Loukas and Vandergheynst, 2018), and compressive sampling (Ramasamy and Madhow, 2015; Tremblay et al., 2016), attaining a complexity reduction by roughly a factor of n.\nYet, computation is still an issue for dynamic networks, where the edge set is a function of time. Temporal dynamics constitute an important aspect of many network datasets\n1E\u0301cole Polytechnique Fe\u0301de\u0301rale de Lausanne, Switzerland. Correspondence to: Lionel Martin<lionel.martin@epfl.ch>, Andreas Loukas <andreas.loukas@epfl.ch>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nand should be taken into account in the algorithmic design and analysis. Unfortunately, SC is poorly suited to this setting as eigendecomposition \u2013its main computational bottleneck\u2013 has to be recomputed from scratch whenever the graph is updated, or at least periodically (Ning et al., 2007). This is a missed opportunity since the clustering assignments of many real networks change slowly with time, suggesting that successive algorithmic runs wastefully repeat similar computations.\nThis paper proposes an algorithm that reuses information of past cluster assignments to expedite computation. Different from previous work on dynamic clustering, our objective is not to improve the clustering quality, e.g., by enforcing a temporal-smoothness hypothesis (Chakrabarti et al., 2006; Chi et al., 2007) or by using tensor decompositions (Gauvin et al., 2014; Tu et al., 2016). Similar to recent work by (Dhanjal et al., 2014), we focus entirely on reducing the complexity while producing assignments that are provably close to those of SC.\nOur work starts from the recent idea of sidestepping eigendecomposition by utilizing as features random vectors filtered by Chebyshev polynomials of the graph Laplacian (Tremblay et al., 2016). We notice that, in the dynamic setting, there are ample opportunities to reuse information from previous cluster assignments, both in terms of approximating the k-th eigenvalue (a necessary step of Chebyshev filter design), as well as in terms of computing the features themselves. When the consecutive graphs are appropriately similar, these ideas lead to complexity reductions.\nConcretely, we provide the following contributions:\n1. In Section 3 we refine the analysis of compressive spectral clustering (CSC) presented in (Tremblay et al., 2016). Our goal is to move from assertions about feature approximation to guarantees about the quality of the solution of CSC itself. We prove that w.h.p. the quality of the clustering assignments of CSC and SC differ by O(2k/ \u221a d), and thus d \u221d k2 filtered vectors are sufficient to obtain a good approximation. Importantly, our analysis does not make restricting assumptions about the graph structure, such as assuming a stochastic block model (Pydi and Dukkipati, 2017).\n2. In Section 4, we focus on dynamic graphs and propose dynamic CSC (dCSC), an algorithm that reuses informa-\ntion of past cluster assignments to expedite computation. We discover that the algorithm\u2019s ability to reuse features is determined by a measure of spectral similarity \u03c1 between consecutive graphs: we prove that, when pd features are reused (i.e., each new instance of the dynamic graph is clustered using pd features of the previous graph and (1 \u2212 p)d new features, where 0 < p \u2264 0.5), w.h.p. the clustering quality of dCSC approximates that of CSC up to an additive term in the order of p\u03c1.\nThe paper concludes with a proof of concept comparison against SotA approximation algorithms for Spectral Clustering (Section 5). Our experiments confirm that dCSC yields computational benefits when the graph dynamics are bounded. A case in point: we can cluster 30\u2019000 node graphs 3.9\u00d7 faster than SC and 1.5\u00d7 faster than CSC in average. Due to space constraints, certain proofs and implementation details are presented as an appendix in a supplementary document."}, {"heading": "2. Background", "text": "We start by summarizing the standard method for spectral clustering as well as the idea behind the more recent accelerated methods. Due to space constraints, our exposition is brief; the reader is encouraged to refer to the original works for a more comprehensive discussion."}, {"heading": "2.1. Spectral clustering (SC)", "text": "To determine the best node-to-cluster assignment, spectral clustering solves a k-means problem with the eigenvectors of the graph Laplacian as features (Shi and Malik, 2000; Ng et al., 2002).\nLet G = (V, E ,W) be a weighted undirected graph with n nodes V = {v1, v2, . . . , vn}, and m edges E \u2282 V \u00d7V . The graph Laplacian is defined as L = I \u2212 D\u22121/2WD\u22121/2, where D is a diagonal matrix whose entries are the degree of the nodes in the graph (i.e. the sum of the weighed edges adjacent to each node). We denote the eigendecomposition of the Laplacian by L = U\u039bU>, with the eigenvalues contained in \u039b sorted in non-decreasing order, such that 0 = \u03bb1 \u2264 \u03bb2 \u2264 . . . \u2264 \u03bbn.\nSpectral clustering consists of computing the first k eigenvectors of L arranged in matrix Uk and subsequently computing a k-means assignment of the n vectors of size k found in the rows of Uk. Formally, if \u03a6 \u2208 Rn\u00d7d is the feature matrix (here \u03a6 = Uk and d = k), and k is a positive integer denoting the number of clusters, the k-means clustering problem finds the indicator matrix X \u2208 Rn\u00d7k which satisfies\nX\u03a6 = arg min X\u2208X\n\u2016\u03a6\u2212XX>\u03a6\u2016F , (1)\nwith associated cost C\u03a6 = \u2016\u03a6 \u2212X\u03a6XT\u03a6\u03a6\u2016F . Symbol X denotes the set of all n\u00d7k indicator matrices X. These matrices indicates the cluster membership of each data point by setting\nXi,j =\n{ 1\u221a sj if data point i belongs to cluster j\n0 otherwise, (2)\nwhere sj is the size of cluster j, also equals to the number of non-zero elements in column j. Note that the cost described in eq. (1) is the square root of the more traditional definition expressed with the distances to the cluster centers (Cohen et al., 2015, Sec 2.3). We refer the reader to the work by (Boutsidis et al., 2015) and references therein for more details."}, {"heading": "2.2. Compressive spectral clustering (CSC)", "text": "To reduce the computational cost of spectral clustering, (Tremblay et al., 2016) proposed to approximate Uk using a filtering of random vectors (a similar idea was also examined by (Boutsidis et al., 2015)). The former work also introduced the benefits of compressed sampling techniques reducing the total cost down toO(k2 log2(k)+cn(log(n)+ k)), where c is the order of the polynomial approximation. Their algorithm consists of two steps:\nStep 1. Approximate features. Feature matrix Uk is approximated by the projection of a random matrix over the same subspace. In particular, let R \u2208 RN\u00d7d be a random (gaussian) matrix with centered i.i.d. entries, each having variance 1d . We can project R onto span{Uk} by multiplying each one of its columns by a projector H defined as\nH = U ( Ik 0 0 0 ) U>. (3)\nIt is then a simple consequence of the JohnshonLindenstrauss lemma that the rows\u03c8>i of matrix \u03a8 = HR can act as a replacement of the features used in spectral clustering, i.e., the rows \u03c6>i of \u03a6 = Uk.\nTheorem 2.1 (adapted from (Tremblay et al., 2016)). For every two nodes vi and vj the restricted isometry relation\n(1\u2212\u03b5)\u2016\u03c6i\u2212\u03c6j\u20162 \u2264 \u2016\u03c8i\u2212\u03c8j\u20162 \u2264 (1+\u03b5)\u2016\u03c6i\u2212\u03c6j\u20162 (4)\nholds with probability larger than 1 \u2212 n\u2212\u03b2 , as long as the dimension is d > 4+2\u03b2\u03b52/2\u2212\u03b53/3 log(n).\nWe note that, even though HR is also expensive to compute exactly, it can be easily approximated by applying the graph filter h(L) on each column of R, which entailsO(dc) sparse matrix-vector multiplications (each costing O(m)) using graph Chebychev polynomials (Shuman et al., 2011a; Hammond et al., 2011) or rational graph filters (Isufi et al., 2017; Loukas et al., 2015) (c relates to the quality of the\napproximation and is usually below 100). A more elaborate discussion on the approximation of HR can be found in the appendix.\nStep 2. Compressive k-means. The complexity is reduced further by computing the k-means step for only a subset of the nodes. The remaining cluster assignments are then inferred by solving a graph Tikhonov regularized interpolation problem involving k additional graph filtering operations, each with a cost linear in cm. To guarantee a good approximation, it is sufficient to sample O(k log(k)) nodes using variable density sampling (Puy et al., 2016). For simplicity, in the following, we present our theoretical results w.r.t. the non-compressed version of their algorithm. The proofs can be generalized using similar arguments as in (Tremblay et al., 2016)."}, {"heading": "3. The Approximation Quality of Static CSC", "text": "Before delving to the dynamic setting, we refine the analysis of compressive spectral clustering. Our objective is to move from assertions about distance preservation currently known (see Thm. 2.1) to guarantees about the quality of the solution of CSC itself. Formally, let\nX\u03a8 = arg min X\u2208X\n\u2016\u03a8\u2212XX>\u03a8\u2016F (5)\nbe the clustering assignment obtained from using k-means with \u03a8 as features (CSC assignment), and define the CSC cost C\u03a8 as\nC\u03a8 = \u2016\u03a6\u2212X\u03a8X>\u03a8\u03a6\u2016F . (6)\nThe question we ask is: how close is C\u03a8 to the cost C\u03a6 of the same problem, where the assignment has been computed using \u03a6 as features, i.e., the SC cost corresponding to (1)? Note that, as in previous work (Boutsidis et al., 2015), we express the approximation quality in terms of the difference of clustering assignment costs and not of the distance between the assignments themselves. We are not aware of any analysis that would allow us to characterize (the perhaps more intuitive goal of) how well X\u03a8 approximates X\u03a6, which is a combinatorial objective. Yet, our approach exhibits the benefit of not penalizing approximation algorithms that choose alternative assignments of the same or similar quality1.\nOur central theorem asserts that with high probability the assignments of SC and CSC have similar costs.\nTheorem 3.1. The SC cost C\u03a6 and the CSC cost C\u03a8 are related by\n1The k-means objective is a non convex objective and has multiple minima. For instance, any of the k! re-labelings of the optimal assignment are valid solutions with the same cost.\nC\u03a6 \u2264 C\u03a8 \u2264 C\u03a6 + 2 \u221a k\nd ( \u221a k + \u03b5), (7)\nwith probability at least 1\u2212 exp(\u2212\u03b52/2).\nThis result emphasizes the importance of the number of random vectors d and directly links it to the distance with the optimal assignment for the spectral features. Indeed, one can see that the difference between the two costs vanishes when d is sufficiently large. Importantly, d \u221d k2 is sufficient to guarantee a small error."}, {"heading": "3.1. The approximation quality of CSC", "text": "The first step in proving Thm. 3.1 is to establish the relation between C\u03a6 and C\u03a8. The following lemma relates the two costs by an additive error term that depends on the feature\u2019s differences \u2016\u03a8\u2212\u03a6Ik\u00d7dQ\u2016F when d \u2265 k.Since \u03a6 and \u03a8 have different sizes we introduced the multiplication by a unitary matrix Q. We will first show that any unitary Q can be picked in Lem. 3.1 and then derive the optimal Q, the one minimizing the additive term, in Thm. 3.2.\nLemma 3.1. For any unitary matrix Q \u2208 Rd\u00d7d, the SC cost C\u03a6 and the CSC cost C\u03a8 are related by\nC\u03a6 \u2264 C\u03a8 \u2264 C\u03a6 + 2\u2016\u03a8\u2212\u03a6Ik\u00d7dQ\u2016F , (8)\nwhere, the matrix I`\u00d7m of size ` \u00d7m above contains only ones on its diagonal and serves to resize matrices.\nBeing able to show that the additive term is small encompasses the result of Thm. 2.1, ensuring distance preservation. However, this statement is stronger than the previous one as our lemma is not necessarily true under distance preservation only.\nThe remaining of this section is devoted to bounding the Frobenius error \u2016\u03a8\u2212\u03a6Ik\u00d7dQ\u2016F between the features of SC and CSC. In order to prove this result, we will first express our Frobenius norm exclusively in terms of the singular values of the random matrix R and then in a second step we will study the distribution of these singular values.\nOur next result, which remarkably is an equality, reveals that the achieved error is exactly determined by how close a Gaussian matrix is to a unitary matrix.\nTheorem 3.2. There exists a d\u00d7 d unitary matrix Q, such that\n\u2016\u03a8\u2212\u03a6Ik\u00d7dQ\u2016F = \u2016\u03a3\u2212 Ik\u00d7d\u2016F , (9)\nwhere \u03a3 is the diagonal matrix holding the singular values of R\u2032 = Ik\u00d7nU>R.\nBefore presenting the proof, let us observe that R\u2032 is an i.i.d. Gaussian random matrix of size k \u00d7 d and its entries have zero mean and the same variance as that of R. We use\nthis fact in the following to control the error by appropriately selecting the number of random vectors d.\nTo bound the feature error further, we will use the following result by Vershynin, whose proof is not reproduced.\nCorollary 3.1 (adapted from Cor. 5.35 (Vershynin, 2010)). Let N be an d \u00d7 k matrix whose entries are independent standard normal random variables. Then for every \u03b5, i \u2265 0, with probability at least 1\u2212 exp(\u2212\u03b52/2) one has\n\u03c3i(N)\u2212 \u221a d \u2264 \u221a k + \u03b5, (10)\nwhere \u03c3i(N) is the ith singular value of N.\nExploiting this result, the following corollary of Thm. 3.2 reveals the relation of the feature error and the number of random vectors d.\nCorollary 3.2. There exists a d\u00d7d unitary matrix Q, such that, for every \u03b5 \u2265 0, one has\n\u2016\u03a8\u2212\u03a6Ik\u00d7dQ\u2016F \u2264 \u221a k\nd ( \u221a k + \u03b5), (11)\nwith probability at least 1\u2212 exp(\u2212\u03b52/2).\nFinally, Cor. 3.2 combined with Lem. 3.1 provide the direct proof of Thm. 3.1 that we introduced earlier.\nBefore proceeding, we would like to make some remarks about the tightness of the bound. First, guaranteeing that the feature error is small is a stronger condition than distance preservation (though necessary for a complete analysis of CSC). For this reason, the bound derived can be larger than that of Thm. 2.1. Nevertheless, we should stress it is tight: the main inequality in our analysis stems from bounding the k largest singular values of the random matrix by Vershynin\u2019s tight bound of the maximal singular value."}, {"heading": "3.2. Practical aspects", "text": "The study presented above assumes that H is defined as in eq. (3), namely that it is a projector on the subspace spanned by the first k eigenvectors of L. However, as discussed in the appendix, to be computationally efficient we choose to compute H by an application of a polynomial function h on L (Shuman et al., 2011a). More specifically, we select a polynomial that approximates the ideal low-pass response (Allen-Zhu and Li, 2016). As long as \u03bbk is known, the approximated projector h(L) can be designed to be very close to H: using the arguments of (Shuman et al., 2011b, Proposition 3) and (Laurent and Massart, 2000, Lemma 1) it is easy to prove that w.h.p. using h(L) instead of H does not add more than O(c\u2212c \u221a n) error to C\u03a8, where c is the polynomial order (the proof is omitted due to space limitations).\nMoreover, we need to estimate \u03bbk accurately (the design of h involves finding a polynomial which takes the value 1 when the input is smaller than \u03bbk and 0 otherwise). Towards this goal, we refer the readers to (Di Napoli et al., 2016; Paratte and Martin, 2016) and their respective eigencount techniques that allow to approximate the filter in O(cm log((\u03bbk+1 \u2212 \u03bbk)\u22121)) operations."}, {"heading": "4. Compressive Clustering of Dynamic Graphs", "text": "In this section, we consider the problem of spectral clustering a sequence of graphs. We focus on graphs Gt where t \u2208 {1, . . . , \u03c4}, composed of a static node set V and evolving edge sets Et. Identifying each assignment from scratch (using SC or CSC) is a computationally demanding task, as the complexity increases linearly with the number of time-steps. However, when consecutive graphs are \u201cappropriately similar\u201d, we should be able to cut on this cost by reusing information. We will utilize two alternative similarity measures:\nDefinition 4.1 (Measures of graph similarity). Two graphs Gt\u22121 and Gt are:\n\u2022 (\u03c1, k)-spectrally similar if the spaces spanned by their first k eigenvectors are almost aligned: \u2016Ht \u2212 Ht\u22121\u2016F \u2264 \u03c1.\n\u2022 \u03c1-edge similar if the edge-wise difference of their Laplacians is bounded: \u2016Lt \u2212 Lt\u22121\u2016F \u2264 \u03c1.\nBoth measures are relevant in the context of dynamic clustering. Two spectrally similar graphs might have different connectivity, but possess similar clustering assignments. On the other hand, assuming that two graphs are edge similar is a stronger condition that postulates fine-grained similarities between them. It is however more intuitive and computationally economical to ascertain (see Section 4.3)."}, {"heading": "4.1. Algorithm", "text": "We now present an accelerated method for spectral clustering an evolving network. Without loss of generality, suppose that we need to compute the assignment for Gt while knowing already that of Gt\u22121 and possessing the features \u03a8t\u22121 used to compute it.\nComponent 1. We reuse a portion of features \u03a8t\u22121 to cluster Gt. Let p be a number between 0 and 0.5, and set q = 1 \u2212 p.2 Instead of recomputing \u03a8t from scratch running a new CSC routine, we construct a feature matrix \u0398t which consists of dq new features (corresponding to Gt)\n2Although in practice p can go up to 1, the analysis only considers the case when reused features are only from Gt\u22121 (and not from earlier graphs).\nAlgorithm 1 Dynamic CSC Input: (G1,G2, . . . ,G\u03c4 ), p, d Output: (X1,X2, . . . ,X\u03c4 )\n1: Determine \u03bbk and filter h1 for G1. 2: Find X1 for G1 using CSC with \u03a81 = h1(L1)R. 3: for t from 2 to \u03c4 do 4: Select dp features from \u03a8t\u22121 and call them \u03a8 (1) t . 5: Generate d(1\u2212p) features \u03a8(2)t by filtering as many random vectors by ht\u22121(Lt). 6: Test whether \u03bbk \u2208 [\u03bbk(Lt), \u03bbk+1(Lt)] with eigencount and using \u03a8(2)t . 7: if the test fails then 8: Determine \u03bbk for Gt using eigencount. 9: Update ht and recompute \u03a8 (2) t based on ht(Lt). 10: end if 11: Find assignment Xt by applying the compressive k-\nmeans to features \u0398t = [\u03a8 (1) t ,\u03a8 (2) t ].\n12: Set \u03a8t = \u03a8 (2) t . 13: end for\nand dp randomly selected features of Gt\u22121:\n\u0398t = [Ht\u22121Rdp, HtRdq] = \u03a8t\u22121S d dp + \u03a8tS d dp (12)\nAbove, we use the sub-identity matrix Sddp = Id\u00d7dpIdp\u00d7d and its complement Sddp = Id\u00d7d \u2212 Sddp. Component 2. An important part of the complexity of CSC stems from using the eigencount algorithm to estimate \u03bbk and construct the Chebyshev polynomials (step 1 of their algorithm). To avoid recomputing \u03bbk, we start by assuming that the estimated value for \u03bbk at t \u2212 1 is a also good candidate for t and proceed to use the same polynomial in order to filter the qd new random vectors Rdq in Gt. Notice that the eigencount method requires exactly these new features to determine if \u03bbk was correctly estimated and thus to validate our assumption. If our assumption is invalid, i.e., the \u03bbk has changed from t \u2212 1 to t, then we rerun the eigencount method from scratch as in (Di Napoli et al., 2016) but providing \u03bbk as an initial estimate. The final set of features generated in the eigencount now serves as \u03a8t. When the assumption is valid, we proceed as is.\nComplexity analysis. There are two steps where the complexity is reduced with respect to CSC. First, the optimization proposed for the determination of \u03bbk avoids computing steps of dichotomy for every graph. Spectrally similar graphs generally possess close spectrum and close values for \u03bbk. One could then expect to recompute \u03bbk only intermittently, in which cases he/she would also benefit from a reduced number of iterations due to a good initialization3. If S are the total number of eigencount steps gained,\n3Though this trend has been confirmed by our numerical experiments, a formal proof remains elusive.\nthe total gain is O(Scm). Second, since we reuse random features from one graph to the next, the total number of computed random vectors will necessarily be reduced compared to the use of \u03c4 independent CSC calls. The gain here is O(cmdp) per time-step.\nAll reductions applied through compression can also benefit to our dynamic method. Indeed, we theoretically showed that reusing features from the past can replace the creation of new random vectors. Thus, sampling the combination of old and new vectors can be applied exactly as defined in CSC. Then, the result of the sub-assignment can be interpolated also as defined in (Tremblay et al., 2016)."}, {"heading": "4.2. Analysis of dynamic CSC", "text": "Similarly to the static case, our objective is to provide probabilistic guarantees about the approximation quality of the proposed method. Let\nX\u0398t = arg min X\u2208X\n\u2016\u0398t \u2212XX>\u0398t\u2016F . (13)\nbe the clustering assignment obtained from k-means with \u0398t as features, and define the dynamic CSC cost C\u0398t as\nC\u0398t = \u2016\u03a6\u2212X\u0398tX>\u0398t\u03a6\u2016F . (14)\nAs the following theorem claims, the graph evolution introduces an additional error term that is a function of the graph similarity (spectral- or edge- wise).\nTheorem 4.1. At time t, the dynamic CSC cost C\u0398t and the SC cost C\u03a6t are related by\nC\u03a6t \u2264 C\u0398t \u2264 C\u03a6t + 2 \u221a k d ( \u221a k + \u03b5) + (1 + \u03b4)p \u03b3, (15)\nwith probability at least 1\u2212 exp ( \u2212\u03b5 2\n2\n) \u2212 exp ( 2 log(n)\u2212 dp ( \u03b42\n4 \u2212 \u03b4\n3\n6\n)) ,\nwhere 0 < \u03b4 \u2264 1. Above, \u03b3 depends only on the similarity of the graphs in question. If graphs Gt\u22121 and Gt are\n\u2022 (\u03c1, k)-spectrally similar, then \u03b3 = \u03c1, \u2022 \u03c1-edge similar, then \u03b3 = ( \u221a\n2 \u03c1)/\u03b1, where \u03b1 = min{\u03bbtk, \u03bb (t\u22121) k+1 \u2212 \u03bbtk} is the Laplacian eigen-gap.\nProof. Let X\u03a6t and X\u0398t be respectively the optimal SC and dCSC clustering assignments at time t, and denote E = \u0398t \u2212\u03a6tIk\u00d7dQ. We have that,\nC\u0398t \u2264 C\u03a6t + 2\u2016\u0398t \u2212\u03a6tIk\u00d7dQ\u2016F , (16)\nfollowing the exact same steps as in the proof of Lemma 3.1. By completing the matrices containing the filtering of both graphs, we can see that the error term can be\nrewritten as\n\u2016E\u2016F = \u2016\u03a8t\u22121Sddp + \u03a8tSddp \u2212\u03a6tIk\u00d7dQ\u2016F (17)\n= \u2016(\u03a8t\u22121 \u2212\u03a8t)Sddp + \u03a8t \u2212\u03a6tIk\u00d7dQ\u2016F \u2264 \u2016(\u03a8t \u2212\u03a8t\u22121)Sddp\u2016F + \u2016\u03a8t \u2212\u03a6tIk\u00d7dQ\u2016F .\nThe rightmost term of eq. (17) corresponds to the effects of random filtering and has been studied in depth in Thm. 3.2 and Cor. 3.2. The rest of the proof is devoted to studying the leftmost term.\nWe apply the Johnson-Lindenstrauss lemma (Johnson and Lindenstrauss, 1984) on the term of interest. Setting R\u2032 = 1\u221apRId\u00d7dp, we have that\n\u2016(\u03a8t \u2212\u03a8t\u22121)Sddp\u20162F = \u2016(Ht \u2212Ht\u22121)RId\u00d7dp\u20162F\n= p n\u2211 i=1 \u2016R\u2032> (Ht \u2212Ht\u22121)>\u03b4i\u201622.\nMatrix R\u2032 = p\u22121/2RId\u00d7dp has n \u00d7 dp Gaussian i.i.d. entries with zero-mean and variance 1/dp. It follows from the Johnson-Lindenstrauss lemma that \u2016(\u03a8t \u2212\u03a8t\u22121)Sddp\u20162F \u2264 p (1 + \u03b4) n\u2211 i=1 \u2016(Ht \u2212Ht\u22121)>\u03b4i\u201622\n\u2264 p (1 + \u03b4)\u2016Ht \u2212Ht\u22121\u20162F ,\nwith probability at least 1 \u2212 n\u2212\u03b2 and for dp \u2265 4+2\u03b2 \u03b42( 12\u2212 \u03b4 3 ) log(n). Coupling the two together we obtain a probability at least equal to 1\u2212exp(2 log(n)\u2212dp\u03b4 2\n2 ( 1 2\u2212 \u03b4 3 )),\nwhere \u03b4 can be set between 0 and 1. A loose bound gives 2p\u2016H(2)\u2212H(1)\u20162F with probability 1\u2212exp(2 log(n)\u2212 dp 12 ).\nThis concludes the part of the proof concerning spectrally similar graphs. The result for edge-wise similarity follows from Cor. 5.1 found in the appendix."}, {"heading": "4.3. Controlling the approximation error", "text": "Theorem 4.1 can be used to adaptively determine p at each time-step with the objective of attaining a bounded error for the term (1+\u03b4)p\u03c1. For instance, if the graph did not evolve much between the last two time-steps, more signals should be reused than otherwise. For that, one needs to be able to approximate the graph similarity, without computing the spectral basis.\nThis can be quite easily achieved for the edge similarity measure, by simply computing the Frobenius norm of the edge weight difference between the two graph Laplacian matrices. As we show next, the spectral similarity measure \u03c1 can also be estimated as follows:\n\u03c1\u0302 := \u2016HtR\u2212Ht\u22121R\u2016F (18)\nAlgorithm 2 Dynamic CSC with adaptive p Input: (G1,G2, . . . ,G\u03c4 ), d, k, \u03b4, Output: (X1,X2, . . . ,X\u03c4 )\n1: Determine \u03bbk and filter h1 for G1. 2: Find X1 for G1 using CSC with \u03a81 = h1(L1)R. 3: for t from 2 to \u03c4 do 4: Split features \u03a8t\u22121 = [\u03a8 (1) t\u22121,\u03a8 (2) t\u22121] s.t. \u03a8 (1) t\u22121 con-\ntains d2 vectors. Let R (1) be random vectors used to\ngenerate \u03a8(1)t\u22121. 5: Set ht = ht\u22121 and compute \u03a8 (1) t = ht(Lt)R\n(1). 6: Test whether \u03bbk \u2208 [\u03bbk(Lt), \u03bbk+1(Lt)] with eigencount and using \u03a8(1)t . 7: if the test fails then 8: Determine \u03bbk for Gt using eigencount. 9: Update ht and recompute \u03a8 (1) t = ht(Lt)R\n(1). 10: end if 11: Set p = min ( 1 2 , \u03b52 1+\u03b4\u2016\u03a8 (1) t \u2212\u03a8 (1) t\u22121\u2016 \u22121 F ) . 12: Select dp features from \u03a8(2)t\u22121 and call them \u03a8 (2) t . 13: Generate d( 12 \u2212 p) features \u03a8 (3) t = ht(Lt)R\n(3), where R(3) are new random vectors.\n14: Find assignment Xt by applying the compressive kmeans to features \u0398t = [\u03a8 (1) t ,\u03a8 (2) t ,\u03a8 (3) t ]. 15: Set \u03a8t = [\u03a8 (1) t ,\u03a8 (3) t ]. 16: end for\nTo motivate this, observe that \u03c1\u03032 is an unbiased estimator:\nE [ \u03c1\u03022 ] = E [ tr ( R> (Ht \u2212Ht\u22121)> (Ht \u2212Ht\u22121) R )] = tr ( (Ht \u2212Ht\u22121)E [ RR> ] (Ht \u2212Ht\u22121)>\n) = tr ( (Ht \u2212Ht\u22121) (Ht \u2212Ht\u22121)> ) = \u03c12, (19)\nwhich implies that \u03c1\u0303 approaches \u03c1 as d grows.\nWith this in place, we proceed to modify Alg. 4.1 so as to include the estimation of \u03c1 and the adaptive estimation of p. The detailed procedure is summarized in Alg. 2, focusing on the case of spectral similarity (i.e., \u03b3 = \u03c1). Since Thm. 4.1 proved an additive error of at most (1 + \u03b4)p\u03c1, we fix an upper bound on the error that we tolerate \u03b52 and \u03b4 that controls the probability of success, and set p = \u03b52(1+\u03b4)\u03c1\u0302 .\nThough the adaptive algorithm features the same complexity, it is slightly more involved than Alg. 4.1. The main difference is that \u03c1 is estimated based on features \u03a8(1)t and \u03a8(1)t\u22121 that correspond to the same random vectors R(1) \u2208 Rn\u00d7d filtered on two consecutive graphs (i.e., Gt and Gt\u22121). Features \u03a8(1)t are combined with the pd reused features \u03a8(2)t and the d(1/2\u2212p) new features \u03a8 (3) t to identify assignment Xt."}, {"heading": "5. Experiments", "text": "This section complements the theoretical results described in Section 4. All our experiments are designed using the GSPBox (Perraudin et al., 2014)."}, {"heading": "5.1. Experimental setup", "text": "As is common practice, we use the Stochastic Block Model (SBM) to evaluate the efficiency of our spectral clustering method (e.g., Go\u0308rke et al., 2013; Tremblay et al., 2016). In SBM, data are clustered in k classes and the n nodes are connected at random with edge-wise probability that depends if the two extremities belong to the same cluster (q1) or not (q2 with q2 q1). In the following, we qualify the SBM parameters in terms of the nodes\u2019 average degree \u03b4\u0304 and the ratio q2/q1 that captures the graph clusterability (Decelle et al., 2011). Following the recommendations of the former work, we set q2q1 = \u03b4\u0304\u2212 \u221a \u03b4\u0304 2(\u03b4\u0304+ \u221a \u03b4\u0304(k\u22121))\nto construct non-trivially clusterable graphs.\nWe compare the quality and complexity of our dynamic method (dCSC) against the algorithm of Tremblay et al. (CSC) and an optimized spectral clustering (Ng et al., 2002) that uses the Lanczos algorithm to compute the first k-eigenvectors (this is significantly faster than doing the entire eigendecomposition while introducing negligible error). We use relative error measures to compare the achieved clustering accuracy of CSC and dCSC with that of SC (i.e., |CA \u2212 CSC |/CSC , where CA is the cost of algorithm A and CSC the cost of SC). We considered two cost measures: the k-means cost (eq. (6)) and the normalized cut (ncut) cost. Since the obtained results were almost identical, we only report the results for ncut in the rest of this section (except for Table 1). After all, the k-means cost of the spectral features is a relaxation of the ncut cost.\nOur analysis highlights the importance of the spectral similarity between consecutive graphs. It is thus important to define how the graph changes between consecutive steps. Starting from a SBM, we perform two types of perturbations: edge redrawing and node reassignment. Edge redrawing consists of removing some edges at random from the original graph and then adding the same number following the probabilities defined by the graph model (using q1 and q2). In node reassignment, one selects nodes, removes all edges that share at least one end with the nodes previously picked, reassigns those nodes to any other class at random and reconnects these nodes with new edges using again the same probabilities q1 and q2. Both perturbations are combined in the synthetic graph that we are studying. We replicate the construction of 100 different SBM with the same parameters, then we alter each with 1% of node reassignment and 1% of edges modifications. The modified graph is used for the evaluation of all methods."}, {"heading": "5.2. When does reusing features pay off?", "text": "We first study the error-complexity trade-off achieved by the compressive clustering methods as a function of d. We set n = 15000, k = 25, \u03b4\u0304 = 60. Each point in Figure 1 corresponds to a single graph being clustered. For each of the two methods, there are 1600 points resulting from 100 repetitions when the number of features is d \u2208 [6, 200] with logarithmic increments. To comprehend the results, it is helpful to consider each of the six sextants in the figure separately. The top-middle sextant shows that when d is large enough (left side), the relative error of CSC and dCSC is close to zero. Increasing d reduces the error but increases the time required for the computation, following the elbow from right to left. The top-right and top-left sextants occur because Lloyd\u2019s algorithm (despite being rerun 100 times) sometimes fails to retrieve the optimal solution to the k-means problem: the top-left (resp. right) sextant corresponds to cases when Lloyd\u2019s algorithm produces a suboptimal assignment for SC (resp. CSC/dCSC). The bot-\ntom three sextants correspond to cases when dCSC did not have to recompute \u03bbk (step 7 of Alg. 4.1). In these cases, dCSC is up to 2\u00d7 faster than CSC. Though the frequency of this phenomenon depends on many factors, such as the size of the eigengap and the spectral similarity of consecutive graphs, we report that in our experiment dCSC could avoid recomputing \u03bbk, roughly 50% of the times.\nIn summary, reusing features produces a clear computational benefit with a reasonable loss of accuracy. Most benefit comes from \u03bbk estimation (component 2) that can be often avoided when consecutive graphs are spectrally similar, especially for well-clusterable graphs (where the gap \u03bbk+1 \u2212 \u03bbk is large). To quantify the benefit of reusing a portion of features (component 1), we compare here the execution time of CSC and dCSC, excluding the time for \u03bbk estimation. Increasing p by 0.25 saved 0.97 and 9.98 seconds respectively when n = 10\u2032000 and 50\u2032000\u2014the later corresponds to a speedup of 1.29x w.r.t. feature estimation."}, {"heading": "5.3. Comparison with state-of-the-art", "text": "To evaluate the efficiency of dCSC, we varied the number of nodes n (while fixing k = 25, d = 50, \u03b4\u0304 = 60). Figure 2 shows the results. As expected, the difference of complexity between spectral clustering using the partial eigen-decomposition and dCSC is clearly visible. Increasing p from 0.25 to 0.5 incurs a non-negligible computational benefit for larger n (14.5 seconds when n =30\u2019000, corresponding to a 12% improvement). We also report that the achieved relative error for both methods remained consistently below 0.1% and did not grow as n increased. We do not present values of n above 30\u2019000 as, for such cases, SC took too long to complete. For example, with 64Gb of RAM, SC took one hour to process the graph and return an assignment when n = 50\u2032000. For the same graph, dCSC run in 6.5 minutes and resulted in a similar k-means cost.\nTable 1 further compares our proposed method to SC, CSC and IASC, the state-of-the-art method for spectral clustering suitable for dynamic graphs (Dhanjal et al., 2014). Note that there is a long list of heuristic-based clustering algo-\nrithms optimized for speed (Dhillon et al., 2007; Karypis and Kumar, 1998), but we only consider here algorithms that provably approximate spectral clustering. We can see that dCSC achieves a significant improvement in timing when n is large enough. Note that IASC results were obtained by running the optimized and parallel implementation kindly provided by the original authors4. Our hypothesis is that the poor complexity of IASC is attributed to the fact that, in our tests (and as is frequently the case) the eigengap was not particularly large."}, {"heading": "6. Conclusion and Future Work", "text": "The major contribution of this paper has been the presentation of a clustering algorithm for dynamic graphs that achieves solution quality provably approximating that of Spectral Clustering. Numerical experiments suggest that our method is faster than previous approximation methods.\nRecent advances in spectral clustering, including this work, can provide a huge complexity gain. Nevertheless, practitioners must pay attention because these works require a proper setup. In particular, n must be large for the approximated method to make sense. Moreover, when working with dynamic networks, the spectral similarity should remain bounded for our algorithm to perform best.\nWe highlight in this paper two open directions of research. It appears clearly in the experiments that the majority of the remaining complexity lies in the estimation of H and more precisely the capability to reuse the previous determination of \u03bbk. Finally, expressing the approximation error in function of the assignment instead of ncut could produce a more insightful explaination of the impact of the various factors."}, {"heading": "Acknowledgements", "text": "We would like to acknowledge the reviewers for their valuable comments. Their suggestions helped us clarify and improve our work.\n4Available at https://github.com/charanpald/sandbox"}], "year": 2018, "references": [{"title": "Faster principal component regression via optimal polynomial approximation to sgn (x). arXiv preprint arXiv:1608.04773", "authors": ["Z. Allen-Zhu", "Y. Li"], "year": 2016}, {"title": "Spectral clustering via the power method-provably", "authors": ["C. Boutsidis", "A. Gittens", "P. Kambadur"], "venue": "In Proceedings of the 24th International Conference on Machine Learning (ICML)", "year": 2015}, {"title": "Evolutionary clustering", "authors": ["D. Chakrabarti", "R. Kumar", "A. Tomkins"], "venue": "In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining,", "year": 2006}, {"title": "Evolutionary spectral clustering by incorporating temporal smoothness", "authors": ["Y. Chi", "X. Song", "D. Zhou", "K. Hino", "B.L. Tseng"], "venue": "In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining,", "year": 2007}, {"title": "Dimensionality reduction for k-means clustering and low rank approximation", "authors": ["M.B. Cohen", "S. Elder", "C. Musco", "M. Persu"], "venue": "In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing,", "year": 2015}, {"title": "Asymptotic analysis of the stochastic block model for modular networks and its algorithmic applications", "authors": ["A. Decelle", "F. Krzakala", "C. Moore", "L. Zdeborov\u00e1"], "venue": "Physical Review E,", "year": 2011}, {"title": "Efficient eigen-updating for spectral graph clustering", "authors": ["C. Dhanjal", "R. Gaudel", "S. Cl\u00e9men\u00e7on"], "year": 2014}, {"title": "Weighted graph cuts without eigenvectors a multilevel approach", "authors": ["I.S. Dhillon", "Y. Guan", "B. Kulis"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "year": 2007}, {"title": "Efficient estimation of eigenvalue counts in an interval", "authors": ["E. Di Napoli", "E. Polizzi", "Y. Saad"], "year": 2016}, {"title": "Community detection in graphs", "authors": ["S. Fortunato"], "venue": "Physics reports,", "year": 2010}, {"title": "Spectral grouping using the nystrom method", "authors": ["C. Fowlkes", "S. Belongie", "F. Chung", "J. Malik"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "year": 2004}, {"title": "Detecting the community structure and activity patterns of temporal networks: a non-negative tensor factorization approach", "authors": ["L. Gauvin", "A. Panisson", "C. Cattuto"], "venue": "PloS one,", "year": 2014}, {"title": "Approximate spectral clustering via randomized sketching", "authors": ["A. Gittens", "P. Kambadur", "C. Boutsidis"], "venue": "Ebay/IBM Research Technical Report", "year": 2013}, {"title": "Dynamic graph clustering combining modularity and smoothness", "authors": ["R. G\u00f6rke", "P. Maillard", "A. Schumm", "C. Staudt", "D. Wagner"], "venue": "Journal of Experimental Algorithmics (JEA),", "year": 2013}, {"title": "Wavelets on graphs via spectral graph theory", "authors": ["D.K. Hammond", "P. Vandergheynst", "R. Gribonval"], "venue": "Applied and Computational Harmonic Analysis,", "year": 2011}, {"title": "Autoregressive moving average graph filtering", "authors": ["E. Isufi", "A. Loukas", "A. Simonetto", "G. Leus"], "venue": "IEEE Transactions on Signal Processing,", "year": 2017}, {"title": "Extensions of lipschitz mappings into a hilbert space", "authors": ["W.B. Johnson", "J. Lindenstrauss"], "venue": "Contemporary mathematics,", "year": 1984}, {"title": "A fast and high quality multilevel scheme for partitioning irregular graphs", "authors": ["G. Karypis", "V. Kumar"], "venue": "SIAM Journal on scientific Computing,", "year": 1998}, {"title": "Adaptive estimation of a quadratic functional by model selection", "authors": ["B. Laurent", "P. Massart"], "venue": "Annals of Statistics,", "year": 2000}, {"title": "Time and space efficient spectral clustering via column sampling", "authors": ["M. Li", "Lian", "X.-C", "J.T. Kwok", "Lu", "B.-L"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "year": 2011}, {"title": "Distributed autoregressive moving average graph filters", "authors": ["A. Loukas", "A. Simonetto", "G. Leus"], "venue": "Signal Processing Letters,", "year": 2015}, {"title": "Spectrally approximating large graphs with smaller graphs", "authors": ["A. Loukas", "P. Vandergheynst"], "venue": "In Interenational Conference on Machine Learning (ICML)", "year": 2018}, {"title": "On spectral clustering: Analysis and an algorithm. Advances in neural information processing systems, 2:849\u2013856", "authors": ["A.Y. Ng", "M.I. Jordan", "Y Weiss"], "year": 2002}, {"title": "Incremental spectral clustering with application to monitoring of evolving blog communities", "authors": ["H. Ning", "W. Xu", "Y. Chi", "Y. Gong", "T. Huang"], "venue": "In Proceedings of the 2007 SIAM International Conference on Data Mining,", "year": 2007}, {"title": "Fast eigenspace approximation using random signals. arXiv preprint arXiv:1611.00938", "authors": ["J. Paratte", "L. Martin"], "year": 2016}, {"title": "GSPBOX: A toolbox for signal processing on graphs. ArXiv e-prints", "authors": ["N. Perraudin", "J. Paratte", "D. Shuman", "L. Martin", "V. Kalofolias", "P. Vandergheynst", "D.K. Hammond"], "year": 2014}, {"title": "Random sampling of bandlimited signals on", "authors": ["G. Puy", "N. Tremblay", "R. Gribonval", "P. Vandergheynst"], "year": 2016}, {"title": "Spectral clustering via graph filtering: Consistency on the highdimensional stochastic block model", "authors": ["M.S. Pydi", "A. Dukkipati"], "venue": "arXiv preprint arXiv:1702.03522", "year": 2017}, {"title": "Compressive spectral embedding: sidestepping the svd", "authors": ["D. Ramasamy", "U. Madhow"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2015}, {"title": "Normalized cuts and image segmentation", "authors": ["J. Shi", "J. Malik"], "venue": "IEEE Transactions on pattern analysis and machine intelligence,", "year": 2000}, {"title": "Chebyshev polynomial approximation for distributed signal processing", "authors": ["D.I. Shuman", "P. Vandergheynst", "P. Frossard"], "venue": "In 2011 International Conference on Distributed Computing in Sensor Systems and Workshops (DCOSS),", "year": 2011}, {"title": "Distributed signal processing via chebyshev polynomial approximation", "authors": ["D.I. Shuman", "P. Vandergheynst", "P. Frossard"], "venue": "arXiv preprint arXiv:1111.5239", "year": 2011}, {"title": "Compressive Spectral Clustering", "authors": ["N.T. Tremblay", "G. Puy", "R. Gribonval", "P. Vandergheynst"], "venue": "In 33rd International Conference on Machine Learning,", "year": 2016}, {"title": "Detecting cluster with temporal information in sparse dynamic graph. arXiv preprint arXiv:1605.08074", "authors": ["K. Tu", "B. Ribeiro", "A. Swami", "D. Towsley"], "year": 2016}, {"title": "Introduction to the non-asymptotic analysis of random matrices. arXiv preprint arXiv:1011.3027", "authors": ["R. Vershynin"], "year": 2010}, {"title": "A tutorial on spectral clustering", "authors": ["U. Von Luxburg"], "venue": "Statistics and computing,", "year": 2007}], "id": "SP:302c4de3de2bd42eeb9b5c83a801af48ae498c48", "authors": [{"name": "Lionel Martin", "affiliations": []}, {"name": "Andreas Loukas", "affiliations": []}, {"name": "Pierre Vandergheynst", "affiliations": []}], "abstractText": "Spectral clustering is a widely studied problem, yet its complexity is prohibitive for dynamic graphs of even modest size. We claim that it is possible to reuse information of past cluster assignments to expedite computation. Our approach builds on a recent idea of sidestepping the main bottleneck of spectral clustering, i.e., computing the graph eigenvectors, by a polynomialbased randomized sketching technique. We show that the proposed algorithm achieves clustering assignments with quality approximating that of spectral clustering and that it can yield significant complexity benefits when the graph dynamics are appropriately bounded. In our experiments, our method clusters 30k node graphs 3.9\u00d7 faster in average and deviates from the correct assignment by less than 0.1%.", "title": "Fast Approximate Spectral Clustering for Dynamic Networks"}