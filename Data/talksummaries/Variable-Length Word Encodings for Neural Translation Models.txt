5	15	The exact same learning and infer- ence machinery applied to these transformed data yields improved translations.
7	58	All of them eliminate the need to replace rare words with the unknown word symbol.
9	22	It does not introduce new parameters into the model, change the model structure, affect inference, require access to a complete dictionary, or require any additional learning procedures.
10	42	Nonetheless, compared to a baseline system that replaces all rare words with an unknown word symbol, our encoding approach improves EnglishFrench news translation by up to 1.7 BLEU.
11	64	Neural machine translation describes approaches to machine translation that learn from corpora in a single integrated model that embeds words and sentences into a vector space (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014).
13	2	The architecture consists of an encoder and a decoder.
14	86	The encoder receives a source sentence x and encodes each prefix using a recurrent neural network that recursively combines embeddings xj for each word position j: −→ h j = f(xj , −→ h j−1) (1) where f is a non-linear function.
16	27	These vector representations are stacked to form hj , a representation of the 2088 whole sentence focused on position j.
17	37	The decoder predicts each target word yi sequentially according to the distribution P (yi|yi−1, ..., y1,x) = g(yi−1, si, ci) (2) where si is a hidden decoder state summarizing the prefix of the translation generated so far, ci is a summary of the entire input sequence, and g is another non-linear function.
23	5	The speed of prediction scales with the output vocabulary size, due to the denominator of Equation 2 (Jean et al., 2014).
40	45	In the case of translation, let V be the original corpus vocabulary, which can number in the millions of word types in a typical corpus.
46	60	An optimal encoding can be found using a greedy algorithm (Huffman, 1952).
47	420	to be or not to be take it or leave it to be or not it (take) (leave) s0 be s0 to be or not to be s0 be it or s0 s0 it Original Corpus Repeat-All Encoding to be s0 s1 (not) (it) or s0 s1 Repeat-Symbol Encoding to be or s0 s0 to be s1 s0 s0 s1 or s1 s1 s0 s1 (take) (leave) s0 s1 s0 s1 s2 (not)(it)(to) t0 t1 No-Repeats Encoding s0 t0 s0 t1 or s1 t0 s0 t0 s0 t1 s2 t0 s1 t1 or s2 t1 s1 t1 (take)(leave) t0 t1 (be) or t0 t1 13 encoded tokens 16 encoded tokens 20 encoded tokens Figure 1: Our three encoding schemes are applied to a two-sentence toy corpus for which each word type appears one or two times, and the total vocabulary size V is 7.
48	13	An optimal encoding tree under each scheme is shown for an encoded vocabulary sizeW of 6.
50	18	Twosymbol encodings of rare words are underlined.
51	6	We consider three different encoding schemes that are based on Huffman codes.
53	35	While a Huffman code achieves the shortest possible encoded length using a fixed vocabulary size W , symbols are often shared between both common words and rare words.
56	2	In our experiments with V ≈ 2 · 106, W = 3 · 104, and frequencies drawn from the WMT corpus, all words in V are encoded as either a single symbol or two symbols ofW .
59	70	In Figure 1, common words are represented as themselves.
60	86	Rare words are represented by two words, and the first is always a pseudo-word symbol introduced intoW of the form sX for an integer X. Repeat-Symbol.
