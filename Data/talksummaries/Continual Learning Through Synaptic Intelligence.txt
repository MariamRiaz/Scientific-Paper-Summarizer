0	54	Artificial neural networks (ANNs) have become an indispensable asset for applied machine learning, rivaling human performance in a variety of domain-specific tasks (LeCun et al., 2015).
2	30	For instance, parameters of ANNs are learned on a dataset in the training phase, and then frozen and used statically on new data in the deployment or recall phase.
3	27	To accommodate changes in the data distribution, ANNs typically have to be retrained on the entire dataset to avoid overfitting and catastrophic forgetting (Choy et al., 2006; Goodfellow et al., 2013).
4	71	On the other hand, biological neural networks exhibit continual learning in which they acquire new knowledge over a lifetime.
6	30	Somehow, our brains have evolved to learn from non-stationary data and to update internal memories or beliefs on-the-fly.
8	22	Perhaps one of the greatest gaps in the design of modern ANNs versus biological neural networks lies in the complexity of synapses.
11	34	While this complexity has been surmised to serve memory consolidation (Fusi et al., 2005; Lahiri & Ganguli, 2013; Zenke et al., 2015; Ziegler et al., 2015; Benna & Fusi, 2016), few studies have illustrated how it benefits learning in ANNs.
13	28	While simple, scalar one-dimensional synapses suffer from catastrophic forgetting, in which the network forgets previously learned tasks when trained on a novel task, this problem can be largely alleviated by synapses with a more complex three-dimensional state space.
34	44	They used a diagonal weighting proportional to the diagonal of the Fisher information metric over the old parameters on the old task.
37	25	To tackle the problem of continual learning in neural networks, we sought to build a simple structural regularizer that could be computed online and implemented locally at each synapse.
38	27	Specifically, we aim to endow each individual synapse with a local measure of “importance” in solving tasks the network has been trained on in the past.
39	62	When training on a new task we penalize changes to important parameters to avoid old memories from being overwritten.
42	19	The process of training a neural network is characterized by a trajectory θ(t) in parameter space (Fig.
43	97	The feat of successful training lies in finding learning trajectories for which the endpoint lies close to a minimum of the loss function L on all tasks.
45	18	To compute the change in loss over an entire trajectory through parameter space we have to sum over all infinitesimal changes.
46	50	This amounts to computing the path integral of the gradient vector field along the parameter trajectory from the initial point (at time t0) to the final point (at time t1): ∫ C g(θ(t))dθ = ∫ t1 t0 g(θ(t)) · θ′(t)dt.
52	47	In practice, we can approximate ωµk online as the running sum of the product of the gradient gk(t) = ∂L∂θk with the parameter update θ′k(t) = ∂θk ∂t .
62	26	To avoid large changes to important parameters, we use a modified cost function L̃µ in which we introduced a surrogate loss which approximates the summed loss functions of previous tasks Lν (ν < µ).
63	52	Specifically, we use a quadratic surrogate loss that has the same minimum as the cost function of the previous tasks and yields the same ωνk over the parameter distance ∆k.
65	37	For two tasks this is achieved exactly by the following quadratic surrogate loss L̃µ = Lµ + c ∑ k Ωµk ( θ̃k − θk )2 ︸ ︷︷ ︸ surrogate loss (4) where we have introduced the dimensionless strength parameter c, the reference weight corresponding to the parameters at the end of the previous task θ̃k = θk(tµ−1), and the per-parameter regularization strength: Ωµk = ∑ ν<µ ωνk (∆νk) 2 + ξ .
74	21	4 as a surrogate loss only holds in the case of two tasks, we will show empirically that our approach leads to good performance when learning additional tasks.
138	18	In this scenario the system exhibits catastrophic forgetting, i.e. it learns to solve the most recent task, but rapidly forgets about previous tasks (blue line, Fig.
141	35	Finally, these results were consistent across training and validation error and comparable to the results reported with EWC (Kirkpatrick et al., 2017).
146	43	Specifically, we trained a CNN (4 convolutional, followed by 2 dense layers with dropout; see Appendix for details).
156	48	Importantly, the performance of networks trained with consolidation was always better than without consolidation, except on the last task.
161	36	In summary, we found that consolidation not only protected old memories from being slowly forgotten over time, but also allowed networks to generalize better on new tasks with limited data.
169	48	While we make no claim that biological synapses behave like the intelligent synapses of our model, a wealth of experimental data in neurobiology suggests that biological synapses act in much more complex ways than the artificial scalar synapses that dominate current machine learning models.
173	21	These chemical factors are thought to encode the valence or novelty of a recent change (Redondo & Morris, 2011).
175	32	Here, we introduced one specific higher dimensional synaptic model to tackle a specific problem: catastrophic forgetting in continual learning.
177	30	In essence, in machine learning, in addition to adding depth to our networks, we may need to add intelligence to our synapses.
