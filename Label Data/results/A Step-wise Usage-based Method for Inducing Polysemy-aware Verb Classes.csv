0,1,label2,summary_sentences
Deep learning has significantly advanced our ability to address a wide range of difficult machine learning and signal processing problems.,1. Introduction,[0],[0]
"Today’s machine learning landscape is dominated by deep (neural) networks (DNs), which are compositions of a large number of simple parameterized linear and nonlinear transforms.",1. Introduction,[0],[0]
"An all-too-common story of late is that of plugging a deep network into an application as a black box, training it on copious training data,
1ECE Department, Rice University, Houston, TX, USA.",1. Introduction,[0],[0]
"Correspondence to: Randall B. <randallbalestriero@gmail.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"and then significantly improving performance over classical approaches.
",1. Introduction,[0],[0]
"Despite this empirical progress, the precise mechanisms by which deep learning works so well remain relatively poorly understood, adding an air of mystery to the entire field.",1. Introduction,[0],[0]
"Ongoing attempts to build a rigorous mathematical framework fall roughly into five camps: (i) probing and measuring DNs to visualize their inner workings (Zeiler & Fergus, 2014); (ii) analyzing their properties such as expressive power (Cohen et al., 2016), loss surface geometry (Lu & Kawaguchi, 2017; Soudry & Hoffer, 2017), nuisance management (Soatto & Chiuso, 2016), sparsification (Papyan et al., 2017), and generalization abilities; (iii) new mathematical frameworks that share some (but not all) common features with DNs (Bruna & Mallat, 2013); (iv) probabilistic generative models from which specific DNs can be derived (Arora et al., 2013; Patel et al., 2016); and (v) information theoretic bounds (Tishby & Zaslavsky, 2015).
",1. Introduction,[0],[0]
"In this paper, we build a rigorous bridge between DNs and approximation theory via spline functions and operators.",1. Introduction,[0],[0]
"We prove that a large class of DNs — including convolutional neural networks (CNNs) (LeCun, 1998), residual networks (ResNets) (He et al., 2016; Targ et al., 2016), skip connection networks (Srivastava et al., 2015), fully connected networks (Pal & Mitra, 1992), recurrent neural networks (RNNs) (Graves, 2013), and beyond — can be written as spline operators.",1. Introduction,[0],[0]
"In particular, when these networks employ current standard-practice piecewise-affine, convex nonlinearities (e.g., ReLU, max-pooling, etc.)",1. Introduction,[0],[0]
"they can be written as the composition of max-affine spline operators (MASOs) (Magnani & Boyd, 2009; Hannah & Dunson, 2013).",1. Introduction,[0],[0]
"We focus on such nonlinearities here but note that our framework applies also to non-piecewise-affine nonlinearities through a standard approximation argument.
",1. Introduction,[0],[0]
The max-affine spline connection provides a powerful portal through which to view and analyze the inner workings of a DN using tools from approximation theory and functional analysis.,1. Introduction,[0],[0]
"Here is a summary of our key contributions:
[C1] We prove that a large class of DNs can be written as a composition of MASOs, from which it follows immediately that, conditioned on the input signal, the output of a DN is a simple affine transformation of the input.",1. Introduction,[0],[0]
"We illustrate in Section 4 by deriving a closed-form expression for the
input/output mapping of a CNN.
",1. Introduction,[0],[0]
"[C2] The affine mapping formula enables us to interpret a MASO DN as constructing a set of signal-dependent, classspecific templates against which the signal is compared via a simple inner product.",1. Introduction,[0],[0]
"In Section 5 we relate DNs directly to the classical theory of optimal classification via matched filters and provide insights into the effects of data memorization (Zhang et al., 2016).
",1. Introduction,[0],[0]
[C3] We propose a simple penalty term that can be added to the cost function of any DN learning algorithm to force the templates to be orthogonal to each other.,1. Introduction,[0],[0]
"In Section 6, we show that this leads to significantly improved classification performance and reduced overfitting on standard test data sets like CIFAR100 with no change to the DN architecture.
",1. Introduction,[0],[0]
"[C4] The partition of the input space induced by a MASO links DNs to the theory of vector quantization (VQ) and K-means clustering, which opens up a new geometric avenue to study how DNs cluster and organize signals in a hierarchical fashion.",1. Introduction,[0],[0]
"Section 7 studies the properties of the MASO partition.
",1. Introduction,[0],[0]
"[C5] Leveraging the fact that a DN considers two signals to be similar if they lie in the same MASO partition region, we develop a new signal distance in Section 7.3 that measures the difference between their partition encodings.",1. Introduction,[0],[0]
"The distance is easily computed via backpropagation.
",1. Introduction,[0],[0]
A number of appendices in the Supplementary Material (SM) contain the mathematical setup and proofs.,1. Introduction,[0],[0]
"A significantly extended account of these events with numerous new results is available in (Balestriero & Baraniuk, 2018).",1. Introduction,[0],[0]
A deep network (DN) is an operator fΘ :,2. Background on Deep Networks,[0],[0]
RD → RC that maps an input signal1 x ∈ RD to an output prediction ŷ ∈ RC as fΘ :,2. Background on Deep Networks,[0],[0]
RD → RC .,2. Background on Deep Networks,[0],[0]
"All current DNs can be written as a composition of L intermediate mappings called layers
fΘ(x) =",2. Background on Deep Networks,[0],[0]
( f (L) θ(L),2. Background on Deep Networks,[0],[0]
◦ · · · ◦,2. Background on Deep Networks,[0],[0]
f (1) θ(1) ),2. Background on Deep Networks,[0],[0]
"(x), (1)
where Θ = { θ(1), . . .",2. Background on Deep Networks,[0],[0]
", θ(L) } is the collection of the network’s parameters from each layer.",2. Background on Deep Networks,[0],[0]
"This composition of mappings is nonlinear and non-commutative, in general.
",2. Background on Deep Networks,[0],[0]
A DN layer at level ` is an operator f (`) θ(`) that takes as input the vector-valued signal z(`−1)(x) ∈ RD(`−1) and produces the vector-valued output z(`)(x) ∈ RD(`) .,2. Background on Deep Networks,[0],[0]
We will assume that x and z(`) are column vectors.,2. Background on Deep Networks,[0],[0]
We initialize with z(0)(x) =,2. Background on Deep Networks,[0],[0]
"x and denote z(L)(x) =: z for convenience.
1",2. Background on Deep Networks,[0],[0]
"For concreteness, we focus here on processing K-channel images x, such as color digital photographs.",2. Background on Deep Networks,[0],[0]
"But our analysis and techniques apply to signals of any index-dimensionality, including speech and audio signals, video signals, etc.
",2. Background on Deep Networks,[0],[0]
"The signals z(`)(x) are typically called feature maps; it is easy to see that
z(`)(x) =",2. Background on Deep Networks,[0],[0]
"( f (`)
θ(`) ◦ · · · ◦ f (1) θ(1)
) (x), ` ∈ {1, . . .",2. Background on Deep Networks,[0],[0]
", L}.",2. Background on Deep Networks,[0],[0]
"(2)
We briefly overview the basic DN operators and layers we consider in this paper; more details and additional layers are provided in (Goodfellow et al., 2016) and (Balestriero & Baraniuk, 2018).",2. Background on Deep Networks,[0],[0]
"A fully connected operator performs an arbitrary affine transformation by multiplying its input by the dense matrix W (`) ∈ RD(`)×D(`−1) and adding the arbitrary bias vector b(`)W ∈ RD (`) , as in f (`)W ( z(`−1)(x) )",2. Background on Deep Networks,[0],[0]
:= W (`)z(`−1)(x) + b (`) W .,2. Background on Deep Networks,[0],[0]
"A convolution operator reduces the number of parameters in the affine transformation by replacing the unconstrained W (`) with a multichannel convolution matrix, as in f (`)C ( z(`−1)(x) )",2. Background on Deep Networks,[0],[0]
:= C(`)z(`−1)(x) +,2. Background on Deep Networks,[0],[0]
"b (`) C .
",2. Background on Deep Networks,[0],[0]
"An activation operator applies a scalar nonlinear activation function σ independently to each entry of its input, as in[ f (`) σ",2. Background on Deep Networks,[0],[0]
( z(`−1)(x) ),2. Background on Deep Networks,[0],[0]
],2. Background on Deep Networks,[0],[0]
k := σ,2. Background on Deep Networks,[0],[0]
"( [z(`−1)(x)]k ) , k = 1, . . .",2. Background on Deep Networks,[0],[0]
", D(`).",2. Background on Deep Networks,[0],[0]
"Nonlinearities are crucial to DNs, since otherwise the entire network would collapse to a single global affine transform.",2. Background on Deep Networks,[0],[0]
Three popular activation functions are the rectified linear unit (ReLU) σReLU(u),2. Background on Deep Networks,[0],[0]
":= max(u, 0), the leaky ReLU σLReLU(u)",2. Background on Deep Networks,[0],[0]
":= max(ηu, u), η > 0, and the absolute value σabs(u) := |u|.",2. Background on Deep Networks,[0],[0]
These three functions are both piecewise affine and convex.,2. Background on Deep Networks,[0],[0]
Other popular activation functions include the sigmoid σsig(u) := 11+e−u and hyperbolic tangent σtanh(u) := 2σsig(2u)−1.,2. Background on Deep Networks,[0],[0]
"These two functions are neither piecewise affine nor convex.
",2. Background on Deep Networks,[0],[0]
"A pooling operator subsamples its input to reduce its dimensionality according to a sub-sampling policy ρ applied over a collection of input indices {Rk}K (`)
k=1 (typically a small patch), e.g., max pooling[ f (`) ρ ( z(`−1)(x) )",2. Background on Deep Networks,[0],[0]
],2. Background on Deep Networks,[0],[0]
"k
:= max d∈R(`)k
[ z(`−1)(x) ]",2. Background on Deep Networks,[0],[0]
"d , k =
1, . . .",2. Background on Deep Networks,[0],[0]
", D(`).",2. Background on Deep Networks,[0],[0]
"See (Balestriero & Baraniuk, 2018) for the definitions of average pooling, channel pooling, skip connections, and recurrent layers.
",2. Background on Deep Networks,[0],[0]
Definition 1.,2. Background on Deep Networks,[0],[0]
"A DN layer f (`) θ(`)
comprises a single nonlinear DN operator (non-affine to be precise) composed with any preceding affine operators lying between it and the preceding nonlinear operator.
",2. Background on Deep Networks,[0],[0]
"This definition yields a single, unique layer decomposition for any DN, and the complete DN is then the composition of its layers per (1).",2. Background on Deep Networks,[0],[0]
"For example, in a standard CNN, there are two different layers types: i) convolution-activation and ii) max-pooling.
",2. Background on Deep Networks,[0],[0]
We form the prediction ŷ by feeding fΘ(x) through a final nonlinearity g : RD(L) → RD(L) as in ŷ = g(fΘ(x)).,2. Background on Deep Networks,[0],[0]
"In classification, g is typically the softmax nonlinearity, which arises naturally from posing the classification inference as a
multinomial logistic regression problem (Bishop, 1995).",2. Background on Deep Networks,[0],[0]
"In regression, typically no g is applied.
",2. Background on Deep Networks,[0],[0]
"We learn the DN parameters Θ for a particular prediction task in a supervised setting using a labeled data set D = (xn,yn) N n=1, a loss function, and a learning policy to update the parameters Θ in the predictor fΘ(x).",2. Background on Deep Networks,[0],[0]
"For classification problems, the loss function is typically the negative cross-entropy LCE(x,y) (Bishop, 1995).",2. Background on Deep Networks,[0],[0]
"For regression problems, the loss function is typically is the squared error.",2. Background on Deep Networks,[0],[0]
"Since the layer-by-layer operations in a DN are differentiable almost everywhere with respect to their parameters and inputs, we can use some flavor of first-order optimization such as gradient descent to optimize the parameters Θ with respect to the loss function.",2. Background on Deep Networks,[0],[0]
"Moreover, the gradients for all internal parameters can be computed efficiently by backpropagation (Hecht-Nielsen, 1992), which follows from the chain rule of calculus.",2. Background on Deep Networks,[0],[0]
"Approximation theory is the study of how and how well functions can best be approximated using simpler functions (Powell, 1981).",3. Background on Spline Operators,[0],[0]
"A classical example of a simpler function is a spline s : RD → R (Schmidhuber, 1994).",3. Background on Spline Operators,[0],[0]
"For concreteness, we will work exclusively with affine splines in this paper (aka “linear splines”), but our ideas generalize naturally to higher-order splines.
",3. Background on Spline Operators,[0],[0]
Multivariate Affine Splines.,3. Background on Spline Operators,[0],[0]
"Consider a partition of a domain RD into a set of regions Ω = {ω1, . . .",3. Background on Spline Operators,[0],[0]
", ωR}",3. Background on Spline Operators,[0],[0]
"and a set of local mappings Φ = {φ1, . . .",3. Background on Spline Operators,[0],[0]
", φR} that map each region in the partition to R via φr(x) := 〈[α]r,·,x〉+ [β]r for x ∈ ωr.2",3. Background on Spline Operators,[0],[0]
"The parameters are: α ∈ RR×D, a matrix of hyperplane “slopes,” and β ∈ RR, a vector of hyperplane “offsets” or “biases”.",3. Background on Spline Operators,[0],[0]
We will use the terms offset and bias interchangeably in the sequel.,3. Background on Spline Operators,[0],[0]
"The notation [α]r,· denotes the column vector formed from the rth row of α.
",3. Background on Spline Operators,[0],[0]
"With this setup, the multivariate affine spline is defined as
s[α, β,Ω](x) = R∑ r=1 (〈[α]r,·,x〉+ [β]r)1(x ∈ ωr)
",3. Background on Spline Operators,[0],[0]
"=: 〈α[x],x〉+ β[x], (3)
where 1(x ∈ ωr) is the indicator function.",3. Background on Spline Operators,[0],[0]
The second line of (3) introduces the streamlined notation α[x] =,3. Background on Spline Operators,[0],[0]
"[α]r,· when x ∈ ωr; the definition for β[x] is similar.",3. Background on Spline Operators,[0],[0]
Such a spline is piecewise affine and hence piecewise convex.,3. Background on Spline Operators,[0],[0]
"However, in general, it is neither globally affine nor globally convex unless R = 1, a case we denote as a degenerate spline, since it corresponds simply to an affine mapping.
",3. Background on Spline Operators,[0],[0]
"2 To make the connection between splines and DNs more immediately obvious, here x is interpreted as a point in RD , which plays the rôle of the space of signals in the other sections.
",3. Background on Spline Operators,[0],[0]
Max-Affine Spline Functions.,3. Background on Spline Operators,[0],[0]
"A major complication of function approximation with splines in general is the need to jointly optimize both the spline parameters α, β and the input domain partition Ω (the “knots” for a 1D spline) (Bennett & Botkin, 1985).",3. Background on Spline Operators,[0],[0]
"However, if a multivariate affine spline is constrained to be globally convex, then it can always be rewritten as a max-affine spline (Magnani & Boyd, 2009; Hannah & Dunson, 2013)
s[α, β,Ω](x) = max r=1,...,R
〈[α]r,·,x〉+ [β]r .",3. Background on Spline Operators,[0],[0]
"(4)
An extremely useful feature of such a spline is that it is completely determined by its parameters α and β without needing to specify the partition Ω.",3. Background on Spline Operators,[0],[0]
"As such, we denote a max-affine spline simply as s[α, β].",3. Background on Spline Operators,[0],[0]
"Changes in the parameters α, β of a max-affine spline automatically induce changes in the partition Ω, meaning that they are adaptive partitioning splines (Magnani & Boyd, 2009).
",3. Background on Spline Operators,[0],[0]
Max-Affine Spline Operators.,3. Background on Spline Operators,[0],[0]
"A natural extension of an affine spline function is an affine spline operator (ASO) S[A,B,ΩS ] that produces a multivariate output.",3. Background on Spline Operators,[0],[0]
It is obtained simply by concatenating K affine spline functions from (3).,3. Background on Spline Operators,[0],[0]
"The details and a more general development are provided in the SM and (Balestriero & Baraniuk, 2018).
",3. Background on Spline Operators,[0],[0]
"We are particularly interested in the max-affine spline operator (MASO) S[A,B] :",3. Background on Spline Operators,[0],[0]
RD → RK formed by concatenating K independent max-affine spline functions from (4).,3. Background on Spline Operators,[0],[0]
"A MASO with slope parameters A ∈ RK×R×D and offset parameters B ∈ RK×R is defined as
S[A,B](x) =  maxr=1,...,R〈[A]1,r,·,x〉+ [B]1,r... maxr=1,...,R〈[A]K,r,·,x〉+ [B]K,r  =: A[x]x",3. Background on Spline Operators,[0],[0]
+B[x].,3. Background on Spline Operators,[0],[0]
"(5)
The second line of (5) introduces the streamlined notation in terms of the signal-dependent matrix A[x] and signal-dependent vector B[x], where [A[x]]k,· := [A]k,rk(x),· and [B[x]]k := [B]k,rk(x) with rk(x) = arg maxr〈[A]k,r,·,x〉+ [B]k,r.
Max-affine spline functions and operators are always piecewise affine and globally convex (and hence also continuous) with respect to each output dimension.",3. Background on Spline Operators,[0],[0]
"Conversely, any piecewise affine and globally convex function/operator can be written as a max-affine spline.",3. Background on Spline Operators,[0],[0]
"Moverover, using standard approximation arguments, it is easy to show that a MASO can approximate arbitrarily closely any (nonlinear) operator that is convex in each output dimension.",3. Background on Spline Operators,[0],[0]
"While a MASO is appropriate only for approximating convex functions/operators, we now show that virtually all of
today’s DNs can be written as a composition of MASOs, one for each layer.",4. DNs are Compositions of Spline Operators,[0],[0]
"Such a composition is, in general, nonconvex and hence can approximate a much larger class of functions/operators.",4. DNs are Compositions of Spline Operators,[0],[0]
"Interestingly, under certain broad conditions, the composition remains a piecewise affine spline operator, which enables a variety of insights into DNs.",4. DNs are Compositions of Spline Operators,[0],[0]
"We now state our main theoretical results, which are proved in the SM and elaborated in (Balestriero & Baraniuk, 2018).
",4.1. DN Operators are MASOs,[0],[0]
Proposition 1.,4.1. DN Operators are MASOs,[0],[0]
An arbitrary fully connected operator f (`)W is an affine mapping and hence a degenerate MASO S,4.1. DN Operators are MASOs,[0],[0]
"[ A
(`) W , B (`) W ] , with R = 1, [A(`)W ]k,1,· = [ W (`) ]",4.1. DN Operators are MASOs,[0],[0]
"k,· and
[B (`) W ]k,1 =
[ b
(`) W ] k , leading to W (`)z(`−1)(x) + b(`)W",4.1. DN Operators are MASOs,[0],[0]
"=
A (`)",4.1. DN Operators are MASOs,[0],[0]
W,4.1. DN Operators are MASOs,[0],[0]
[x]z (`−1)(x) +B (`) W,4.1. DN Operators are MASOs,[0],[0]
[x].,4.1. DN Operators are MASOs,[0],[0]
"The same is true of a convolution operator with W (`), b(`)W replaced by C (`), b (`) C .",4.1. DN Operators are MASOs,[0],[0]
Proposition 2.,4.1. DN Operators are MASOs,[0],[0]
"Any activation operator f (`)σ using a piecewise affine and convex activation function is a MASO S [ A (`) σ ,B (`) σ ] with R = 2, [ B (`) σ ]",4.1. DN Operators are MASOs,[0],[0]
"k,1 = [ B (`) σ ] k,2 =
0 ∀k, and for ReLU [ A (`) σ ]",4.1. DN Operators are MASOs,[0],[0]
"k,1,· = 0, [ A (`) σ ] k,2,· =
ek ∀k; for leaky ReLU [ A (`) σ ]",4.1. DN Operators are MASOs,[0],[0]
"k,1,· = νek, [ A (`) σ ] k,2,· =
ek ∀k, ν > 0; and for absolute value [ A (`) σ ]",4.1. DN Operators are MASOs,[0],[0]
"k,1,·
= −ek,[ A (`) σ ]",4.1. DN Operators are MASOs,[0],[0]
"k,2,· = ek ∀k, where ek represents the kth canonical basis element of RD(`) .
",4.1. DN Operators are MASOs,[0],[0]
Proposition 3.,4.1. DN Operators are MASOs,[0],[0]
"Any pooling operator f (`)ρ that is piecewise affine and convex is a MASO S [ A (`) ρ ,B (`) ρ ]",4.1. DN Operators are MASOs,[0],[0]
.3,4.1. DN Operators are MASOs,[0],[0]
"Max-
pooling has R = #Rk (typically a constant over all output dimensions k), [ A (`) ρ ] k,·,·
= {ei, i ∈ Rk}, and[ B (`) ρ ] k,r = 0 ∀k, r. Average-pooling is a degenerate
MASO with R = 1, [ A (`) ρ ] k,1,· = 1#(Rk) ∑ i∈Rk ei, and[
B (`) ρ ] k,1 = 0 ∀k.
",4.1. DN Operators are MASOs,[0],[0]
Proposition 4.,4.1. DN Operators are MASOs,[0],[0]
"A DN layer constructed from an arbitrary composition of fully connected/convolution operators followed by one activation or pooling operator is a MASO S[A(`), B(`)] such that
f (`)(z(`−1)(x))",4.1. DN Operators are MASOs,[0],[0]
= A(`)[x]z(`−1)(x) +B(`)[x].,4.1. DN Operators are MASOs,[0],[0]
"(6)
Consequently, a large class of DNs boil down to a composition of MASOs.",4.1. DN Operators are MASOs,[0],[0]
"We prove the following in the SM and in (Balestriero & Baraniuk, 2018) for CNNs, ResNets, skip connection nets, fully connected nets, and RNNs.
3",4.1. DN Operators are MASOs,[0],[0]
"This result is agnostic to the pooling type (spatial or channel).
",4.1. DN Operators are MASOs,[0],[0]
Theorem 1.,4.1. DN Operators are MASOs,[0],[0]
"A DN constructed from an arbitrary composition of fully connected/convolution, activation, and pooling operators of the types in Propositions 1–3 is a composition of MASOs that is equivalent to a global affine spline operator.
",4.1. DN Operators are MASOs,[0],[0]
"Note carefully that, while the layers of each of the DNs stated in Theorem 1 are MASOs, the composition of several layers is not necessarily a MASO.",4.1. DN Operators are MASOs,[0],[0]
"Indeed, a composition of MASOs remains a MASO if and only if all of its component operators (except the first) are non-decreasing with respect to each of their output dimensions (Boyd & Vandenberghe, 2004).",4.1. DN Operators are MASOs,[0],[0]
"Interestingly, ReLU and max-pooling are both nondecreasing, while leaky ReLU is strictly increasing.",4.1. DN Operators are MASOs,[0],[0]
"The culprits causing non-convexity of the composition of layers are negative entries in the fully connected or convolution operators, which destroy the required non-increasing property.",4.1. DN Operators are MASOs,[0],[0]
"A DN where these culprits are thwarted is an interesting special case, because it is convex with respect to its input (Amos et al., 2016) and multiconvex (Xu & Yin, 2013) with respect to its parameters.
",4.1. DN Operators are MASOs,[0],[0]
Theorem 2.,4.1. DN Operators are MASOs,[0],[0]
"A DN whose layers ` = 2, . . .",4.1. DN Operators are MASOs,[0],[0]
", L consist of an arbitrary composition of fully connected and convolution operators with nonnegative weights, i.e., W (`)k,j ≥ 0, C (`) k,j ≥ 0; non-decreasing, piecewise-affine, and convex activation operators; and non-decreasing, piecewise-affine, and convex pooling operators is globally a MASO and thus also globally convex with respect to each of its output dimensions.
",4.1. DN Operators are MASOs,[0],[0]
"The above results pertain to DNs using convex, affine operators.",4.1. DN Operators are MASOs,[0],[0]
"Other popular non-convex DN operators (e.g., the sigmoid and arctan activation functions) can be approximated arbitrarily closely by an affine spline operator but not by a MASO.
DNs are Signal-Dependent Affine Transformations.",4.1. DN Operators are MASOs,[0],[0]
"A common theme of the above results is that, for DNs constructed from fully connected/convolution, activation, and pooling operators from Propositions 1–3, the operator/layer outputs z(`)(x) are always a signal-dependent affine function of the input x (recall (5)).",4.1. DN Operators are MASOs,[0],[0]
The particular affine mapping applied to x depends on which partition of the spline it falls in RD.,4.1. DN Operators are MASOs,[0],[0]
"More on this in Section 7 below.
",4.1. DN Operators are MASOs,[0],[0]
DN Learning and MASO Parameters.,4.1. DN Operators are MASOs,[0],[0]
"Given labeled training data (xn,yn)Nn=1, learning in a DN that meets the conditions of Theorem 1 (i.e., optimizing its parameters Θ) is equivalent to optimally approximating the mapping from input x to output ŷ =",4.1. DN Operators are MASOs,[0],[0]
g ( z(L)(x) ),4.1. DN Operators are MASOs,[0],[0]
"using an appropriate cost function (e.g., cross-entropy for classification or squared error for regression) by learning the parameters θ(`) of the layers.",4.1. DN Operators are MASOs,[0],[0]
"In general the overall optimization problem is nonconvex (it is actually piecewise multi-convex in general (Rister, 2016)).
",4.1. DN Operators are MASOs,[0],[0]
"z (L) CNN(x) = W (L)
( 1∏
`=L−1
A(`)ρ",4.1. DN Operators are MASOs,[0],[0]
[x]A (`) σ,4.1. DN Operators are MASOs,[0],[0]
[x]C (`) ) ︸,4.1. DN Operators are MASOs,[0],[0]
"︷︷ ︸
ACNN[x]
x + W (L) L−1∑",4.1. DN Operators are MASOs,[0],[0]
`=1  `+1∏ j=L−1 A(j)ρ [x]A (j) σ,4.1. DN Operators are MASOs,[0],[0]
[x]C (j) (A(`)ρ [x]A(`)σ [x]b(`)C )︸ ︷︷ ︸,4.1. DN Operators are MASOs,[0],[0]
"BCNN[x] +b (L) W
(7)",4.1. DN Operators are MASOs,[0],[0]
"Combining Propositions 1–3 and Theorem 1 and substituting (5) into (2), we can write an explicit formula for the output of any layer z(`)(x) of a DN in terms of the input x for a variety of different architectures.",4.2. Application: DN Affine Mapping Formula,[0],[0]
"The formula for a standard CNN (using ReLU activation and max-pooling) is given in (7) above; we derive this formula and analogous formulas for ResNets and RNNs in (Balestriero & Baraniuk, 2018).",4.2. Application: DN Affine Mapping Formula,[0],[0]
"In (7), A(`)σ [x] are the signal-dependent matrices corresponding to the ReLU activations,",4.2. Application: DN Affine Mapping Formula,[0],[0]
A(`)ρ,4.2. Application: DN Affine Mapping Formula,[0],[0]
"[x] are the signal-dependent matrices corresponding to maxpooling, and the biases b(L)W , b (`) C arise directly from the fully connected and convolution operators.",4.2. Application: DN Affine Mapping Formula,[0],[0]
The absence of B (`) σ,4.2. Application: DN Affine Mapping Formula,[0],[0]
"[x], B (`) ρ",4.2. Application: DN Affine Mapping Formula,[0],[0]
"[x] is due to the absence of bias in the ReLU (recall (2)) and max-pooling operators (recall (3)).
",4.2. Application: DN Affine Mapping Formula,[0],[0]
"Inspection of (7) reveals the exact form of the signaldependent, piecewise affine mapping linking x to z(L)CNN(x).",4.2. Application: DN Affine Mapping Formula,[0],[0]
"Moreover, this formula can be collapsed into
z (L) CNN(x) = W (L) ( ACNN[x]x +BCNN[x] )",4.2. Application: DN Affine Mapping Formula,[0],[0]
"+ b (L) W (8)
from which we can recognize
z",4.2. Application: DN Affine Mapping Formula,[0],[0]
(L−1) CNN (x) = ACNN[x]x,4.2. Application: DN Affine Mapping Formula,[0],[0]
"+BCNN[x] (9)
as an explicit, signal-dependent, affine formula for the featurization process that aims to convert x into a set of (hopefully) linearly separable features that are then input to the linear classifier in layer ` = L with parameters W (L) and b
(L) W .",4.2. Application: DN Affine Mapping Formula,[0],[0]
"Of course, the final prediction ŷ is formed by running z (L) CNN(x) through a softmax nonlinearity g, but this merely rescales its entries to create a probability distribution.",4.2. Application: DN Affine Mapping Formula,[0],[0]
We now dig deeper into (8) in order to bridge DNs and classical optimal classification theory.,5. DNs are Template Matching Machines,[0],[0]
"While we focus on CNNs and classification for concreteness, our analysis holds for any DN meeting the conditions of Theorem 1.",5. DNs are Template Matching Machines,[0],[0]
An alternate interpretation of (8) is that z(L)CNN(x) is the output of a bank of linear matched filters (plus a set of biases).,5.1. Template Matching,[0],[0]
"That is, the cth element of z(L)(x) equals the inner product between the signal x and the matched filter for the cth class, which is contained in the cth row of the matrix
W (L)A[x].",5.1. Template Matching,[0],[0]
"The bias W (L)B[x] + b(L)W can be used to account for the fact that some classes might be more likely than others (i.e., the prior probability over the classes).",5.1. Template Matching,[0],[0]
"It is well-known that a matched filterbank is the optimal classifier for deterministic signals in additive white Gaussian noise (Rabiner & Gold, 1975).",5.1. Template Matching,[0],[0]
"Given an input x, the class decision is simply the index of the largest element of z(L)(x).4
Yet another interpretation of (8) is that z(L)(x) is computed not in a single matched filter calculation but hierarchically as the signal propagates through the DN layers.",5.1. Template Matching,[0],[0]
Abstracting (5) to write the per-layer maximization process as z(`)(x) = maxr(`),5.1. Template Matching,[0],[0]
A (`) r(`) z(`−1)(x),5.1. Template Matching,[0],[0]
"+B (`) r(`) and cascading, we obtain a formula for the end-to-end DN mapping
z(L)(x) =",5.1. Template Matching,[0],[0]
"W (L) max r(L−1)
",5.1. Template Matching,[0],[0]
"( A
(L−1) r(L−1)
max r(2)
",5.1. Template Matching,[0],[0]
"( A (2)
r(2) . . .
",5.1. Template Matching,[0],[0]
"max r(1)
",5.1. Template Matching,[0],[0]
"( A (1)
r(1) x + B (1) r(1)
) + B (2)
r(2)
) · · ·+ B(L−1)
r(L−1)
) + b
(L) W .
",5.1. Template Matching,[0],[0]
"(10)
",5.1. Template Matching,[0],[0]
"This formula elucidates that a DN performs a hierarchical, greedy template matching on its input, a computationally efficient yet sub-optimal template matching technique.",5.1. Template Matching,[0],[0]
Such a procedure is globally optimal when the DN is globally convex.,5.1. Template Matching,[0],[0]
Corollary 1.,5.1. Template Matching,[0],[0]
"For a DN abiding by the requirements of Theorem 2, the computation (10) collapses to the following globally optimal template matching
z(L−1)(x)",5.1. Template Matching,[0],[0]
"= W (L) max r(L−1),r(2),...,r(1)
( A
(L−1) r(L−1)
",5.1. Template Matching,[0],[0]
"( A (2)
r(2) . . .",5.1. Template Matching,[0],[0]
"(
A (1)
r(1) x + B
(1) r(1)
) +",5.1. Template Matching,[0],[0]
"B (2)
r(2)
) · · ·+ B(L−1
r(L−1)
)",5.1. Template Matching,[0],[0]
"+ b
(L) W .
(11)",5.1. Template Matching,[0],[0]
"Since the complete DN mapping (up to the final softmax) can be expressed as in (8), given a signal x, we can compute the signal-dependent template for class c via A[x]c =",5.2. Template Visualization Examples,[0],[0]
"d[z(L)(x)]c dx , which can be efficiently computed via backpropogation (Hecht-Nielsen, 1992).5",5.2. Template Visualization Examples,[0],[0]
"Once the template A[x]c has been computed, the bias term b[x]c can be computed via b[x]c = z(L)(x)c − 〈A[x]c,·,x〉.",5.2. Template Visualization Examples,[0],[0]
"Figure 1 plots
4Again, since the softmax merely rescales the entries of z(L)(x) into a probability distribution, it does not affect the location of its largest element.
",5.2. Template Visualization Examples,[0],[0]
"5In fact, we can use the same backpropagation procedure used for computing the gradient with respect to a fully connected or
various signal-dependent templates for two CNNs trained on the MNIST and CIFAR10 datasets.",5.2. Template Visualization Examples,[0],[0]
"Under the matched filterbank interpretation of a DN developed in Section 5.1, the optimal template for an image x of class c is a scaled version of x itself.",5.3. Collinear Templates and Data Set Memorization,[0],[0]
But what are the optimal templates for the other (incorrect) classes?,5.3. Collinear Templates and Data Set Memorization,[0],[0]
"In an idealized setting, we can answer this question.
",5.3. Collinear Templates and Data Set Memorization,[0],[0]
Proposition 5.,5.3. Collinear Templates and Data Set Memorization,[0],[0]
Consider an idealized DN consisting of a composition of MASOs that has sufficient approximation power to span arbitrary MASO matrices A[xn] from (9) for any input xn from the training set.,5.3. Collinear Templates and Data Set Memorization,[0],[0]
"Train the DN to classify among C classes using the training data D = (xn, yn)Nn=1 with normalized inputs ‖xn‖2 = 1 ∀n and the cross-entropy loss LCE(yn, fΘ(xn)) with the addition of the regularization constraint that ∑ c ‖A[xn]c,·‖2",5.3. Collinear Templates and Data Set Memorization,[0],[0]
< α with α > 0.,5.3. Collinear Templates and Data Set Memorization,[0],[0]
"At the global minimum of this constrained optimization problem, the rows of A?[xn] (the optimal templates) have the form:
",5.3. Collinear Templates and Data Set Memorization,[0],[0]
"[A?[xn]]c,· =  + √ (C−1)α C xn, c = yn − √
α C(C−1) xn, c 6=",5.3. Collinear Templates and Data Set Memorization,[0],[0]
"yn
(12)
",5.3. Collinear Templates and Data Set Memorization,[0],[0]
"In short, the idealized CNN in the proposition will memorize a set of collinear templates whose bimodal outputs force
convolution weight but instead with the input x.",5.3. Collinear Templates and Data Set Memorization,[0],[0]
"This procedure is becoming increasingly popular in the study of adversarial examples (Szegedy et al., 2013).
",5.3. Collinear Templates and Data Set Memorization,[0],[0]
the softmax output to a Dirac delta function (aka 1-hot representation) that peaks at the correct class.,5.3. Collinear Templates and Data Set Memorization,[0],[0]
Figure 2 confirms this bimodal behavior on the MNIST and CIFAR10 datasets.,5.3. Collinear Templates and Data Set Memorization,[0],[0]
"While a DN’s signal-dependent matched filterbank (8) is optimized for classifying signals immersed in additive white Gaussian noise, such a statistical model is overly simplistic for most machine learning problems of interest.",6. New DNs with Orthogonal Templates,[0],[0]
"In practice, errors will arise not just from random noise but also from nuisance variations in the inputs such as arbitrary rotations, positions, and modalities of the objects of interest.",6. New DNs with Orthogonal Templates,[0],[0]
The effects of these nuisances are only poorly approximated as Gaussian random errors.,6. New DNs with Orthogonal Templates,[0],[0]
"Limited work has been done on filterbanks for classification in nonGaussian noise; one promising direction involves using not matched but rather orthogonal templates (Eldar & Oppenheim, 2001).
",6. New DNs with Orthogonal Templates,[0],[0]
"For a MASO DN’s templates to be orthogonal for all inputs, it is necessary that the rows of the matrix W (L) in the final linear classifier layer be orthogonal.",6. New DNs with Orthogonal Templates,[0],[0]
"This weak constraint on the DN still enables the earlier layers to create a high-performance, class-agnostic, featurized representation (recall the discussion just below (9)).",6. New DNs with Orthogonal Templates,[0],[0]
"To create orthogonal templates during learning, we simply add to the standard (potentially regularized) cross-entropy loss function LCE a term that penalizes non-zero off-diagonal entries in the matrix W (L)(W (L))T leading to the new loss with the additional penalty
LCE + λ ∑ c1 6=c2 ∣∣∣〈[W",6. New DNs with Orthogonal Templates,[0],[0]
"(L)] c1,· , [ W (L) ]",6. New DNs with Orthogonal Templates,[0],[0]
"c2,· 〉∣∣∣2 .",6. New DNs with Orthogonal Templates,[0],[0]
"(13) The parameter λ controls the tradeoff between cross-entropy
minimization and orthogonality preservation.",6. New DNs with Orthogonal Templates,[0],[0]
"Conveniently, when minimizing (13) via backpropagation, the orthogonal rows of W (L) induce orthogonal backpropagation updates for the various classes.
",6. New DNs with Orthogonal Templates,[0],[0]
We now empirically demonstrate that orthogonal templates lead to significantly improved classification performance.,6. New DNs with Orthogonal Templates,[0],[0]
"We conducted a range of experiments with three different conventional DN architectures – smallCNN, largeCNN, and ResNet4-4 – trained on three different datasets – SVHN, CIFAR10, and CIFAR100.",6. New DNs with Orthogonal Templates,[0],[0]
"Each DN employed bias units, ReLU activations, and max-pooling as well as batchnormalization prior each ReLU.",6. New DNs with Orthogonal Templates,[0],[0]
"The full experimental details are given in the (Balestriero & Baraniuk, 2018).",6. New DNs with Orthogonal Templates,[0],[0]
"For learning, we used the Adam optimizer with an exponential learning rate decay.",6. New DNs with Orthogonal Templates,[0],[0]
All inputs were centered to zero mean and scaled to a maximum value of one.,6. New DNs with Orthogonal Templates,[0],[0]
"No further preprocessing was performed, such as ZCA whitening (Nam et al., 2014).",6. New DNs with Orthogonal Templates,[0],[0]
We assessed how the classification performance of a given DN would change as we varied the orthogonality penalty λ in (13).,6. New DNs with Orthogonal Templates,[0],[0]
"For each configuration of DN architecture, training dataset, learning rate, and penalty λ, we averaged over 15 runs to estimate the average performance and standard deviation.
",6. New DNs with Orthogonal Templates,[0],[0]
We report here on only the CIFAR100 with largeCNN experiments.,6. New DNs with Orthogonal Templates,[0],[0]
"(See (Balestriero & Baraniuk, 2018) for detailed results for all three datasets and the other architectures.",6. New DNs with Orthogonal Templates,[0],[0]
The trends for all three datasets are similar and are independent of the learning rate.),6. New DNs with Orthogonal Templates,[0],[0]
The results for CIFAR100 in Figure 3 indicate that the benefits of the orthogonality penalty emerge distinctly as soon as λ > 0.,6. New DNs with Orthogonal Templates,[0],[0]
"In addition to improved final accuracy and generalization performance, we see that template orthogonality reduces the temptation of the DN to overfit.",6. New DNs with Orthogonal Templates,[0],[0]
"(This is is especially visible in the examples in (Balestriero & Baraniuk, 2018).)",6. New DNs with Orthogonal Templates,[0],[0]
One explanation is that the orthogonal weights W (L) positively impact not only the prediction but also the backpropagation via orthogonal gradient updates with respect to each output dimension’s partial derivatives.,6. New DNs with Orthogonal Templates,[0],[0]
"Like any spline, it is the interplay between the (affine) spline mappings and the input space partition that work the magic in a MASO DN.",7. DN’s Intrinsic Multiscale Partition,[0],[0]
Recall from Section 3 that a MASO has the attractive property that it implicitly partitions its input space as a function of its slope and offset parameters.,7. DN’s Intrinsic Multiscale Partition,[0],[0]
The induced partition Ω opens up a new geometric avenue to study how a DN clusters and organizes signals in a hierarchical fashion.,7. DN’s Intrinsic Multiscale Partition,[0],[0]
"A DN operator at level ` directly influences the partitioning of its input space RD(`−1) and indirectly influences the partitioning of the overall signal space RD.
",7.1. Effect of the DN Operators on the Partition,[0],[0]
A ReLU activation operator splits each of its input dimensions into two half-planes depending on the sign of the input in each dimension.,7.1. Effect of the DN Operators on the Partition,[0],[0]
"This partitions RD(`−1) into a combinatorially large number (up to 2D (`)
) of regions.",7.1. Effect of the DN Operators on the Partition,[0],[0]
Following a fully connected or convolution operator with a ReLU simply rotates the partition in RD(`−1) .,7.1. Effect of the DN Operators on the Partition,[0],[0]
"A max-pooling operator also partitions RD(`−1) into a combinatorially large number (up to #RD (`)
) of regions, where #R is the size of the pooling region.
",7.1. Effect of the DN Operators on the Partition,[0],[0]
This per-MASO partitioning of each layer’s input space constructs an overall partitioning of the input signal space RD.,7.1. Effect of the DN Operators on the Partition,[0],[0]
"As each MASO is applied, it subdivides the input space RD into finer and finer partitions.",7.1. Effect of the DN Operators on the Partition,[0],[0]
"The final partition corresponds to the intersection of all of the intermediate partitions, and hence we can encode the input in terms of the ordered collection of per-layer partition regions into which it falls.",7.1. Effect of the DN Operators on the Partition,[0],[0]
This overall process can be interpreted as a hierarchical vector quantization (VQ) of the training input signals xn.,7.1. Effect of the DN Operators on the Partition,[0],[0]
"There are thus many potential connections between DNs and optimal quantization, information theory, and clustering that we leave for future research.",7.1. Effect of the DN Operators on the Partition,[0],[0]
"See (Balestriero & Baraniuk, 2018) for some early results.",7.1. Effect of the DN Operators on the Partition,[0],[0]
Unfortunately there is no simple formula for the partition of the signal space.,7.2. Inferring a DN Layer’s Intrinsic Partition,[0],[0]
"However, once can obtain the set of inputs signals xn that fall into the same partition region at each layer of a DN.",7.2. Inferring a DN Layer’s Intrinsic Partition,[0],[0]
"At layer `, denote the index of the region selected by the input x (recall (6)) by[ t(`)(x) ]",7.2. Inferring a DN Layer’s Intrinsic Partition,[0],[0]
"k
= arg max r
〈",7.2. Inferring a DN Layer’s Intrinsic Partition,[0],[0]
"[A(`)]k,r,·, z (`−1)(x) 〉 +",7.2. Inferring a DN Layer’s Intrinsic Partition,[0],[0]
"[B(`)]k,r.
(14)
Thus, [t(`)]k ∈ {1, . . .",7.2. Inferring a DN Layer’s Intrinsic Partition,[0],[0]
", R(`)}, with R(`) the number of partition regions in the layer’s input space.",7.2. Inferring a DN Layer’s Intrinsic Partition,[0],[0]
"Encoding the partition as an ordered collection of integers designating the activate hyperplane parameters from (4), we can now visualize which inputs fall into the same or nearby partitions.
",7.2. Inferring a DN Layer’s Intrinsic Partition,[0],[0]
"Due to the very large number of possible regions (up to 2D (`) for a ReLU at layer `) and the limited amount of training data, in general, many partitions will be empty or contain only a single training data point.",7.2. Inferring a DN Layer’s Intrinsic Partition,[0],[0]
"To validate the utility of the hierarchical intrinsic clustering induced by a DN, we define a new distance function between the signals x1 and x2 that quantifies the similarity of their position encodings t`(x1) and t`(x2) at layer ` via
d ( t(`)(x1), t (`)(x2) )
",7.3. A New Image Distance based on the DN Partition,[0],[0]
"= 1− ∑D(`) k=1 1 ( [t(`)(x1)]k = [t (`)(x2)]k )
D(`) .",7.3. A New Image Distance based on the DN Partition,[0],[0]
"(15)
For a ReLU MASO, this corresponds simply to counting how many entries of the layer inputs for x1 and x2 are positive or negative at the same positions.",7.3. A New Image Distance based on the DN Partition,[0],[0]
"For a max-pooling
MASO, this corresponds to counting how many argmax positions are the same in each patch for x1 and x2.
",7.3. A New Image Distance based on the DN Partition,[0],[0]
Figure 4 provides a visualization of the nearest neighbors of a test image under this partition-based distance measure.,7.3. A New Image Distance based on the DN Partition,[0],[0]
"Visual inspection of the figures highlights that, as we progress through the layers of the DN, similar images become closer in the new distance but further in Euclidean distance.",7.3. A New Image Distance based on the DN Partition,[0],[0]
We have used the theory of splines to build a rigorous bridge between deep networks (DNs) and approximation theory.,8. Conclusions,[0],[0]
"Our key finding is that, conditioned on the input signal, the output of a DN can be written as a simple affine transformation of the input.",8. Conclusions,[0],[0]
"This links DNs directly to the classical theory of optimal classification via matched filters and provides insights into the positive effects of data memorization.
",8. Conclusions,[0],[0]
"There are many avenues for future work, including a more in-depth analysis of the hierarchical MASO partitioning, particularly from the viewpoint of vector quantization and K-means clustering, which are unsupervised learning techniques, and information theory.",8. Conclusions,[0],[0]
The spline viewpoint also could inspire the creation of new DN layers that have certain attractive partitioning or approximation capabilities.,8. Conclusions,[0],[0]
"We have begun exploring some of these directions in (Balestriero & Baraniuk, 2018).6
6 This work was partially supported by ARO grant W911NF-151-0316, AFOSR grant FA9550-14-1-0088, ONR grants N0001417-1-2551 and N00014-18-12571, DARPA grant G001534-7500, and a DOD Vannevar Bush Faculty Fellowship (NSSEFF) grant N00014-18-1-2047.",8. Conclusions,[0],[0]
We build a rigorous bridge between deep networks (DNs) and approximation theory via spline functions and operators.,abstractText,[0],[0]
"Our key result is that a large class of DNs can be written as a composition of max-affine spline operators (MASOs), which provide a powerful portal through which to view and analyze their inner workings.",abstractText,[0],[0]
"For instance, conditioned on the input signal, the output of a MASO DN can be written as a simple affine transformation of the input.",abstractText,[0],[0]
"This implies that a DN constructs a set of signal-dependent, class-specific templates against which the signal is compared via a simple inner product; we explore the links to the classical theory of optimal classification via matched filters and the effects of data memorization.",abstractText,[0],[0]
"Going further, we propose a simple penalty term that can be added to the cost function of any DN learning algorithm to force the templates to be orthogonal with each other; this leads to significantly improved classification performance and reduced overfitting with no change to the DN architecture.",abstractText,[0],[0]
The spline partition of the input signal space opens up a new geometric avenue to study how DNs organize signals in a hierarchical fashion.,abstractText,[0],[0]
"As an application, we develop and validate a new distance metric for signals that quantifies the difference between their partition encodings.",abstractText,[0],[0]
A Spline Theory of Deep Networks,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2263–2270, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics",text,[0],[0]
"As a fundamental task in natural language processing (NLP), discourse parsing entails the discovery of the latent relational structure in multi-sentence level analysis.",1 Introduction,[0],[0]
"It is also central to many practical tasks such as question answering (Liakata et al., 2013; Jansen et al., 2014), machine translation (Meyer and Popescu-Belis, 2012; Meyer and Webber, 2013) and automatic summarization (Murray et al., 2006;
∗Corresponding author.",1 Introduction,[0],[0]
"This paper was partially supported by Cai Yuanpei Program (CSC No. 201304490199 and No. 201304490171), National Natural Science Foundation of China (No. 61170114, No. 61672343 and No. 61272248), National Basic Research Program of China (No. 2013CB329401), Major Basic Research Program of Shanghai Science and Technology Committee (No. 15JC1400103), Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04), and Key Project of National Society Science Foundation of China (No. 15-ZDA041).
",1 Introduction,[0],[0]
"Yoshida et al., 2014).",1 Introduction,[0],[0]
"Discourse parsing is also the shared task of CoNLL 2015 and 2016 (Xue et al., 2015; Xue et al., 2016), and many previous works previous on this task (Qin et al., 2016b; Li et al., 2016; Chen et al., 2015; Wang and Lan, 2016).",1 Introduction,[0],[0]
"In a discourse parser, implicit relation recognition has been the bottleneck due to lack of explicit connectives (like “because” or “and”) that can be strong indicators for the senses between adjacent clauses (Qin et al., 2016b; Pitler et al., 2009; Lin et al., 2014).",1 Introduction,[0],[0]
"This work therefore focuses on implicit relation recognition that infers the senses of the discourse relations within adjacent sentence pairs.
",1 Introduction,[0],[0]
"Most previous works on PDTB implicit relation recognition only focus on one-versus-others binary classification problems of the top level four classes (Pitler et al., 2009; Zhou et al., 2010; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014; Braud and Denis, 2015).",1 Introduction,[0],[0]
"Traditional classification methods directly rely on feature engineering, based on bag-of-words, production rules, and some linguistically-informed features (Zhou et al., 2010; Rutherford and Xue, 2014).",1 Introduction,[0],[0]
"However, discourse relations root in semantics, which may be hard to recover from surface level feature, thus these methods did not report satisfactory performance.",1 Introduction,[0],[0]
"Recently, neural network (NN) models have shown competitive or even better results than traditional linear models with handcrafted sparse features (Wang et al., 2016b; Zhang et al., 2016a; Jia and Zhao, 2014).",1 Introduction,[0],[0]
"They have been proved to be effective for many tasks (Qin et al., 2016a; Wang et al., 2016a; Zhang et al., 2016b; Wang et al., 2015; Wang et al., 2014; Cai and
2263
Zhao, 2016), also including discourse parsing.",1 Introduction,[0],[0]
Ji and Eisenstein (2015) adopt recursive neural network and incorporate with entity-augmented distributed semantics.,1 Introduction,[0],[0]
Zhang et al. (2015) explore a shallow convolutional neural network and achieve competitive performance.,1 Introduction,[0],[0]
"Although simple neural network has been shown effective, the result has not been quite satisfactory which suggests that there is still space for improving.
",1 Introduction,[0],[0]
"The concerned task could be straightforwardly formalized as a sentence-pair classification problem, which needs inferring senses solely based on the two arguments without cues of connectives.",1 Introduction,[0],[0]
Two problems should be carefully handled in this task: how to model sentences and how to capture the interactions between the two arguments.,1 Introduction,[0],[0]
"The former could be addressed by Convolutional Neural Network (CNN) which has been proved effective for sentence modeling (Kalchbrenner et al., 2014; Kim, 2014), while the latter is the key problem, which might need deep semantic analysis for the interaction of two arguments.",1 Introduction,[0],[0]
"To solve the latter problem, we propose collaborative gated neural network (CGNN) which is partially inspired by Highway Network whose gate mechanism achieves success (Srivastava et al., 2015).",1 Introduction,[0],[0]
"Our method will be evaluated on the benchmark dataset against state-of-the-art methods.
",1 Introduction,[0],[0]
"The rest of the paper is organized as follows: Section 2 briefly describes our model, introducing the stacking architecture of CNN and CGNN, Section 3 shows the experiments and analysis, and Section 4 concludes this paper.",1 Introduction,[0],[0]
"The architecture of the model, as shown in Figure 1, is straightforward.",2 Method,[0],[0]
It can be divided into three parts: 1) CNN for modeling arguments; 2) CGNN unit for feature transformation; 3) a conventional softmax layer for the final classification.,2 Method,[0],[0]
"CNN is used to obtain the vector representations for the sentences, CGNN further captures and transforms the features for the final classification.",2 Method,[0],[0]
"As CNN has been broadly adopted for modeling sentences, we will explain it in brevity.",2.1 Convolutional Neural Network,[0],[0]
"For two arguments, typical sentence modeling process
will be applied: sentence embedding (including embeddings for words and part-of-speech (POS) tags) through projection layer, convolution operations (with multiple groups of filters) through the convolution layer, obtaining the sentence representation through one-max-pooling.",2.1 Convolutional Neural Network,[0],[0]
"The two arguments will get their sentence vectors independently without any interfering, and the convolution operation will be the same by sharing parameters.",2.1 Convolutional Neural Network,[0],[0]
The final argument-pair representation will be the vector v which is concatenated from two sentence vectors and this vector will be used as the input of the CGNN unit.,2.1 Convolutional Neural Network,[0],[0]
"For implicit sense classification, the key is how to effectively capture the interactions between the two arguments.",2.2 Collaborative Gated Neural Network,[0],[0]
"The interactions could be word pairs, phrase pairs or even the latent meaning of the two full arguments.",2.2 Collaborative Gated Neural Network,[0],[0]
Pitler et al. (2009) has shown that word pair features are helpful.,2.2 Collaborative Gated Neural Network,[0],[0]
"To model these interactions, we have to make a full use of the sentence vectors obtained from CNN.",2.2 Collaborative Gated Neural Network,[0],[0]
"However, common neural hidden layers might be insufficient to deal with the challenge.",2.2 Collaborative Gated Neural Network,[0],[0]
"We need to seek more powerful neural models, i.e., gated neural network.
",2.2 Collaborative Gated Neural Network,[0],[0]
"In recent years, gated mechanism has gained popularity in neural models.",2.2 Collaborative Gated Neural Network,[0],[0]
"Although it is first introduced in the cells of recurrent neural networks, like Long-Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (GRU) (Chung et al., 2014), traditional feed-forward neural models such as the Highway Network could also benefit from it (Srivastava et al., 2015).",2.2 Collaborative Gated Neural Network,[0],[0]
"The existing studies show that the gated mechanism in highway network serves not only a means for easier training, but also a tool to route information in a trained network.
",2.2 Collaborative Gated Neural Network,[0],[0]
"Motivated by the idea of highway network, we propose a collaborative gated neural network (CGNN) for this task.",2.2 Collaborative Gated Neural Network,[0],[0]
"The architecture of CGNN is illustrated in Figure 1, and it contains a sequence of transformations.",2.2 Collaborative Gated Neural Network,[0],[0]
"First, the inner-cell ĉ is obtained through linear transformation and non-linear activation on the input v, and this process is exactly the operation of an ordinary neural layer.
",2.2 Collaborative Gated Neural Network,[0],[0]
"ĉ = tanh(Wc · v + bc)
",2.2 Collaborative Gated Neural Network,[0],[0]
"Meanwhile, the two gates gi and go are calculated independently because they are only influenced by
the original input through different parameters:
gi = σ(W i · v + bi) go = σ(W o · v + bo)
where the σ denotes sigmoid function which guarantees the values in the gates are in [0,1].",2.2 Collaborative Gated Neural Network,[0],[0]
"Two gated operations are applied sequentially, where a gated operation indicates the element-wise multiplication of an inner-cell and a gate.",2.2 Collaborative Gated Neural Network,[0],[0]
"Between the two gated operations, a non-linear activation operation is applied.",2.2 Collaborative Gated Neural Network,[0],[0]
"The procedure could be formulated as follows:
c = ĉ gi h = tanh(c) go
where denotes element-wise multiplication, c is the second inner-cell and h is the output of CGNN unit.
",2.2 Collaborative Gated Neural Network,[0],[0]
"Although the two gates are generated independently, they will work collaboratively because they control the information flow of the inner-cells sequentially which resembles logical AND operation in a probabilistic version.",2.2 Collaborative Gated Neural Network,[0],[0]
"In fact, the transformations after ĉ will concern only element-wise operations which might give finer controls for each dimension, and the information can only flow on the dimensions where both gates are “open”.",2.2 Collaborative Gated Neural Network,[0],[0]
"This procedure will help select the most crucial features.
",2.2 Collaborative Gated Neural Network,[0],[0]
The gates in this model are mainly used for routing information from sentence-pairs vectors.,2.2 Collaborative Gated Neural Network,[0],[0]
"When there is only one gate in our network, the model works similar to the highway network (Srivastava et al., 2015).",2.2 Collaborative Gated Neural Network,[0],[0]
"After the transformation of the CGNN unit, the transformed vector h will be sent to a conventional softmax for classification.
",2.3 Output and Training,[0],[0]
"The training object J will be the cross-entropy error E with L2 regularization:
E(ŷ, y) =",2.3 Output and Training,[0],[0]
"− l∑
j
yj × log(Pr(ŷj))
",2.3 Output and Training,[0],[0]
J(θ),2.3 Output and Training,[0],[0]
"= 1
m
m∑
k
E(ŷ(k), y(k))",2.3 Output and Training,[0],[0]
+,2.3 Output and Training,[0],[0]
"λ
2 ‖θ‖2
where yj is the gold label and ŷj is the predicted one.",2.3 Output and Training,[0],[0]
"We adopt the diagonal variant of AdaGrad (Duchi et al., 2011) for the optimization process.",2.3 Output and Training,[0],[0]
"As for the benchmark dataset, Penn Discourse Treebank (PDTB) (Prasad et al., 2008) corpus1 is used for evaluation.",3.1 Setting,[0],[0]
"In the PDTB, each discourse relation is annotated between two argument spans.
",3.1 Setting,[0],[0]
"To be consistent with the setups of prior works, we formulate the implicit relation classification task as four one-versus-other binary classification problems only using the four top level classes: COMPARISON (COMP.), CONTINGENCY (CONT.), EXPANSION (EXP.) and TEMPORAL (TEMP.).",3.1 Setting,[0],[0]
"While different works include different relations of varying specificities, all of them include these four core relations (Pitler et al., 2009).",3.1 Setting,[0],[0]
"Following dataset splitting convention of the previous works, we use sections 2-20 for training, sections 21-22 for testing and sections 0-1 for development set.",3.1 Setting,[0],[0]
"The proposed model is possible to be extended for multi-class classification of discourse parsing, but for the comparisons with most of previous works, we will follow them and focus on the binary classification problems.
",3.1 Setting,[0],[0]
"For other hyper-parameters of the model and training process, we fix the lengths of both the input arguments to be 80, and apply truncating or zero-padding when necessary.",3.1 Setting,[0],[0]
"The dimensions for word embeddings and POS embeddings are respectively 300 and 50, and the embedding layer adopts a dropout of 0.2.",3.1 Setting,[0],[0]
"The word embeddings are initialized with pre-trained word vectors using word2vec 2 (Mikolov et al., 2013) and other parameters are randomly initialized including POS embeddings.",3.1 Setting,[0],[0]
"We
1http://www.seas.upenn.edu/˜pdtb/ 2http://www.code.google.com/p/word2vec
set the starting learning rate to 0.001.",3.1 Setting,[0],[0]
"For CNN model, we utilize three groups of filters with window widths of (2, 2, 2) and their filter numbers are all set to 1024.",3.1 Setting,[0],[0]
The hyper-parameters are the same for all models and we do not tune them individually.,3.1 Setting,[0],[0]
"For transformation of sentence vectors, a simple Multilayer Perceptron (MLP) layer could be a straightforward choice, while more complex neural modules, such as LSTM and highway network, could also be considered.",3.2 Model Analysis,[0],[0]
Our model utilizes a CGNN unit with refined gated mechanism for the transformation.,3.2 Model Analysis,[0],[0]
Will the proposed CGNN really bring about further performance improvement?,3.2 Model Analysis,[0],[0]
"We now answer this question empirically.
",3.2 Model Analysis,[0],[0]
"As shown in Table 1, CNN model usually performs well on its own.",3.2 Model Analysis,[0],[0]
"Utilizing an MLP layer or a Highway layer could improve the accuracies on CONTINGENCY, EXPANSION, TEMPORARY except for COMPARISON.",3.2 Model Analysis,[0],[0]
"Though the primary motivation of Highway is to ease gradient-based training of highly deep networks through utilizing gated units, it works merely as an ordinary MLP in the proposed model, which explains the reason that it performs like MLP.",3.2 Model Analysis,[0],[0]
"Despite one of four classes, COMPARISON, not receiving performance improvement, introducing a non-linear transformation layer lets the classification benefit as a whole.",3.2 Model Analysis,[0],[0]
"“CNN+LSTM” denotes the method of using LSTM to read the convolution sequence (without pooling operation), and it even does not perform better than MLP.
",3.2 Model Analysis,[0],[0]
The CGNN achieves the best performance on all classes including COMPARISON.,3.2 Model Analysis,[0],[0]
It gains 3.97% imrovement on average F1 score using CNN only model.,3.2 Model Analysis,[0],[0]
"We assume that CGNN is well-suited to work with CNN, adaptively transforming and combining local features detected by the individual filters.",3.2 Model Analysis,[0],[0]
We show the main results in Tables 2 and 3.,3.3 Results,[0],[0]
"The metrics include precision (P), recall (R), accuracy (Acc) and F1 score.",3.3 Results,[0],[0]
"Since not all of these metrics are reported in previous work, the comparisons are correspondingly in Table 2 and 3.",3.3 Results,[0],[0]
"Some previous work merges Entrel with Expansion, which is also explored in our study and noted as EXP.+.
",3.3 Results,[0],[0]
We compare with best-performed or competitive models including both traditional linear methods and recent neural methods.,3.3 Results,[0],[0]
"For traditional methods: Pitler et al. (2009) use several linguistically informed features, including polarity tags, Levin verb classes, length of verb phrases, modality, context, and lexical features; Zhou et al. (2010) improve the performance through predicting connective words as features; Park and Cardie (2012) propose a locallyoptimal feature set and further identify factors for feature extraction that can have a major impact performance, including stemming and lexicon look-up; Biran and McKeown (2013) collect word pairs from arguments of explicit examples to help the learning; Rutherford and Xue (2014) employ Brown cluster pair and coreference patterns for performance enhancement.",3.3 Results,[0],[0]
"Several neural methods have also been included for comparison: Zhang et al. (2015) propose a simplified neural network which has only
three different pooling operations (max, min, average); Ji and Eisenstein (2015) compute distributed semantics representation by composition up the syntactic parse tree through recursive neural network; Braud and Denis (2015) consider shallow lexical features and word embeddings.",3.3 Results,[0],[0]
Chen et al. (2016) replace the original words by word embeddings to overcome the data sparsity problem and they also utilize gated relevance network to capture the semantic interaction between word pairs.,3.3 Results,[0],[0]
"The gated network is different from ours but also works well.
",3.3 Results,[0],[0]
"Our model achieves F-measure improvements of 1.85% on COMPARISON, 1.56% on CONTINGENCY, 1.27% on EXPANSION, 0.94% on EXPANSION+, 4.89% on TEMPORAL, against the state-ofthe-art of each class.",3.3 Results,[0],[0]
We improve by 4.73% on average F1 score when not including ENTREL in EXPANSION as reported in Table 2 and 3.19% on average F1 score otherwise as reported in Table 3.,3.3 Results,[0],[0]
The results show that our model achieves the best performance and especially makes the most remarkable progress on TEMPORAL.,3.3 Results,[0],[0]
"In this paper, we propose a stacking gated neural architecture for implicit discourse relation classification.",4 Conclusion,[0],[0]
Our model includes convolution and collaborative gated neural network.,4 Conclusion,[0],[0]
The analysis and experiments show that CNN performs well on its own and combining CGNN provides further gains.,4 Conclusion,[0],[0]
Our evaluation on PTDB shows that the proposed model outperforms previous state-of-the-art systems.,4 Conclusion,[0],[0]
Discourse parsing is considered as one of the most challenging natural language processing (NLP) tasks.,abstractText,[0],[0]
Implicit discourse relation classification is the bottleneck for discourse parsing.,abstractText,[0],[0]
"Without the guide of explicit discourse connectives, the relation of sentence pairs are very hard to be inferred.",abstractText,[0],[0]
This paper proposes a stacking neural network model to solve the classification problem in which a convolutional neural network (CNN) is utilized for sentence modeling and a collaborative gated neural network (CGNN) is proposed for feature transformation.,abstractText,[0],[0]
Our evaluation and comparisons show that the proposed model outperforms previous state-of-the-art systems.,abstractText,[0],[0]
A Stacking Gated Neural Architecture for Implicit Discourse Relation Classification,title,[0],[0]
"Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1030–1040, Baltimore, Maryland, USA, June 23-25 2014. c©2014 Association for Computational Linguistics",text,[0],[0]
A verb plays a primary role in conveying the meaning of a sentence.,1 Introduction,[0],[0]
"Capturing the sense of a verb is essential for natural language processing (NLP), and thus lexical resources for verbs play an important role in NLP.
",1 Introduction,[0],[0]
Verb classes are one such lexical resource.,1 Introduction,[0],[0]
"Manually-crafted verb classes have been developed, such as Levin’s classes (Levin, 1993) and their extension, VerbNet (Kipper-Schuler, 2005), in which verbs are organized into classes on the basis of their syntactic and semantic behavior.",1 Introduction,[0],[0]
"Such verb classes have been used in many NLP applications that need to consider semantics in particular, such as word sense disambiguation (Dang, 2004), semantic parsing (Swier and Stevenson, 2005; Shi and Mihalcea, 2005) and discourse parsing (Subba and Di Eugenio, 2009).
",1 Introduction,[0],[0]
"There have also been many attempts to automatically acquire verb classes with the goal of ei-
ther adding frequency information to an existing resource or of inducing similar verb classes for other languages.",1 Introduction,[0],[0]
"Most of these approaches assume that all target verbs are monosemous (Stevenson and Joanis, 2003; Schulte im Walde, 2006; Joanis et al., 2008; Li and Brew, 2008; Sun et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Parisien and Stevenson, 2010; Parisien and Stevenson, 2011; Falk et al., 2012; Lippincott et al., 2012; Reichart and Korhonen, 2013; Sun et al., 2013).",1 Introduction,[0],[0]
"This monosemous assumption, however, is not realistic because many frequent verbs actually have multiple senses.",1 Introduction,[0],[0]
"Moreover, to the best of our knowledge, none of the following approaches attempt to quantitatively evaluate soft clusterings of verb classes induced by polysemy-aware unsupervised approaches (Korhonen et al., 2003; Lapata and Brew, 2004; Li and Brew, 2007; Schulte im Walde et al., 2008).
",1 Introduction,[0],[0]
"In this paper, we propose an unsupervised method for inducing verb classes that is aware of verb polysemy.",1 Introduction,[0],[0]
Our method consists of two clustering steps: verb-specific semantic frames are first induced by clustering verb uses in a corpus and then verb classes are induced by clustering these frames.,1 Introduction,[0],[0]
"By taking this step-wise approach, we can not only induce verb classes with frequency information from a massive amount of verb uses in a scalable manner, but also deal with verb polysemy.
",1 Introduction,[0],[0]
"Our novel contributions are summarized as follows:
• induce both semantic frames and verb classes from a massive amount of verb uses by a scalable method,
• explicitly deal with verb polysemy, • discover effective features for each of the
clustering steps, and
• quantitatively evaluate a soft clustering of verbs.
1030",1 Introduction,[0],[0]
"As stated in Section 1, most of the previous studies on verb clustering assume that verbs are monosemous.",2 Related Work,[0],[0]
"A typical method in these studies is to represent each verb as a single data point and apply classification (e.g., Joanis et al. (2008)) or clustering (e.g., Sun and Korhonen (2009)) to these data points.",2 Related Work,[0],[0]
"As a representation for a data point, distributions of subcategorization frames are often used, and other semantic features (e.g., selectional preferences) are sometimes added to improve the performance.
",2 Related Work,[0],[0]
"Among these studies on monosemous verb clustering (i.e., predominant class induction), there have been several Bayesian methods.",2 Related Work,[0],[0]
Vlachos et al. (2009) proposed a Dirichlet process mixture model (DPMM; Neal (2000)) to cluster verbs based on subcategorization frame distributions.,2 Related Work,[0],[0]
"They evaluated their result with a gold-standard test set, where a single class is assigned to a verb.",2 Related Work,[0],[0]
Parisien and Stevenson (2010) proposed a hierarchical Dirichlet process (HDP; Teh et al. (2006)) model to jointly learn argument structures (subcategorization frames) and verb classes by using syntactic features.,2 Related Work,[0],[0]
Parisien and Stevenson (2011) extended their model by adding semantic features.,2 Related Work,[0],[0]
They tried to account for verb learning by children and did not evaluate the resultant verb classes.,2 Related Work,[0],[0]
"Modi et al. (2012) extended the model of Titov and Klementiev (2012), which is an unsupervised model for inducing semantic roles, to jointly induce semantic roles and frames across verbs using the Chinese Restaurant Process (Aldous, 1985).",2 Related Work,[0],[0]
"All of the above methods considered verbs to be monosemous and did not deal with verb polysemy.
",2 Related Work,[0],[0]
"Our approach also uses Bayesian methods, but is designed to capture verb polysemy.
",2 Related Work,[0],[0]
"We summarize a few studies that consider polysemy of verbs in the rest of this section.
",2 Related Work,[0],[0]
Miyao and Tsujii (2009) proposed a supervised method that can handle verb polysemy.,2 Related Work,[0],[0]
"Their method represents a verb’s syntactic and semantic features, and learns a log-linear model from the SemLink corpus (Loper et al., 2007).",2 Related Work,[0],[0]
"Boleda et al. (2007) also proposed a supervised method for Catalan adjectives considering the polysemy of adjectives.
",2 Related Work,[0],[0]
"The most closely related work to our polysemyaware task of unsupervised verb class induction is the work of Korhonen et al. (2003), who used distributions of subcategorization frames to cluster verbs.",2 Related Work,[0],[0]
They adopted the Nearest Neighbor (NN) and Information Bottleneck (IB) methods for clustering.,2 Related Work,[0],[0]
"In particular, they tried to consider verb polysemy by using the IB method, which is a soft clustering method (Tishby et al., 1999).",2 Related Work,[0],[0]
"However, the verb itself is still represented as a single data point.",2 Related Work,[0],[0]
"After performing soft clustering, they noted that most verbs fell into a single class, and they decided to assign a single class to each verb by hardening the clustering.",2 Related Work,[0],[0]
They considered multiple classes only in the gold-standard data used for their evaluations.,2 Related Work,[0],[0]
"We also evaluate our induced verb classes on this gold-standard data, which was created on the basis of Levin’s classes (Levin, 1993).
",2 Related Work,[0],[0]
Lapata and Brew (2004) and Li and Brew (2007) proposed probabilistic models for calculating prior probabilities of verb classes for a verb.,2 Related Work,[0],[0]
"These models are approximated to condition not
on verbs but on subcategorization frames.",2 Related Work,[0],[0]
"As mentioned in Li and Brew (2007), it is desirable to extend the model to depend on verbs to further improve accuracy.",2 Related Work,[0],[0]
"They conducted several evaluations including predominant class induction and token-level verb sense disambiguation, but did not evaluate multiple classes output by their models.",2 Related Work,[0],[0]
Schulte im Walde et al. (2008) also applied probabilistic soft clustering to verbs by incorporating subcategorization frames and selectional preferences based on WordNet.,2 Related Work,[0],[0]
This model is based on the Expectation-Maximization algorithm and the Minimum Description Length principle.,2 Related Work,[0],[0]
"Since they focused on the incorporation of selectional preferences, they did not evaluate verb classes but evaluated only selectional preferences using a language model-based measure.
",2 Related Work,[0],[0]
"Materna proposed LDA-frames, which are defined across verbs and can be considered to be a kind of verb class (Materna, 2012; Materna, 2013).",2 Related Work,[0],[0]
LDA-frames are probabilistic semantic frames automatically induced from a raw corpus.,2 Related Work,[0],[0]
"He used a model based on latent Dirichlet allocation (LDA; Blei et al. (2003)) and the Dirichlet process to cluster verb instances of a triple (subject, verb, object) to produce semantic frames and roles.",2 Related Work,[0],[0]
Both of these are represented as a probabilistic distribution of words across verbs.,2 Related Work,[0],[0]
"He applied this method to the BNC and acquired 1,200 frames and 400 roles (Materna, 2012).",2 Related Work,[0],[0]
"He did not evaluate the resulting frames as verb classes.
",2 Related Work,[0],[0]
"In sum, there have been no studies that quantitatively evaluate polysemous verb classes automatically induced by unsupervised methods.",2 Related Work,[0],[0]
Our objective is to automatically learn semantic frames and verb classes from a massive amount of verb uses following usage-based approaches.,3.1 Overview,[0],[0]
"Although Bayesian approaches are a possible solution to simultaneously induce frames and verb classes from a corpus as used in previous studies, it has prohibitive computational cost.",3.1 Overview,[0],[0]
"For instance, Parisien and Stevenson applied HDP only to a small-scale child speech corpus that contains 170K verb uses to jointly induce subcategorization frames and verb classes (Parisien and Stevenson, 2010; Parisien and Stevenson, 2011).",3.1 Overview,[1.0],"['For instance, Parisien and Stevenson applied HDP only to a small-scale child speech corpus that contains 170K verb uses to jointly induce subcategorization frames and verb classes (Parisien and Stevenson, 2010; Parisien and Stevenson, 2011).']"
"Materna applied an LDA-based method to the BNC, which contains 1.4M verb uses, to induce seman-
tic frames across verbs that can be considered to be verb classes (Materna, 2012; Materna, 2013).",3.1 Overview,[0],[0]
"However, it would take three months for this experiment using this 100 million word corpus.1 Although it is best to use the largest possible corpus for this kind of knowledge acquisition tasks (Sasano et al., 2009), it is infeasible to scale to giga-word corpora using such joint models.
",3.1 Overview,[0.9999999944648424],"['However, it would take three months for this experiment using this 100 million word corpus.1 Although it is best to use the largest possible corpus for this kind of knowledge acquisition tasks (Sasano et al., 2009), it is infeasible to scale to giga-word corpora using such joint models.']"
"In this paper, we propose a two-step approach for inducing semantic frames and verb classes.",3.1 Overview,[0],[0]
"First, we make multiple data points for each verb to deal with verb polysemy (cf. polysemy-aware previous studies still represented a verb as one data point (Korhonen et al., 2003; Miyao and Tsujii, 2009)).",3.1 Overview,[0],[0]
"To do that, we induce verb-specific semantic frames by clustering verb uses.",3.1 Overview,[1.0],"['To do that, we induce verb-specific semantic frames by clustering verb uses.']"
"Then, we induce verb classes by clustering these verbspecific semantic frames across verbs.",3.1 Overview,[1.0],"['Then, we induce verb classes by clustering these verbspecific semantic frames across verbs.']"
"An interesting point here is that we can use exactly the same method for these two clustering steps.
",3.1 Overview,[0.9999999782201158],['An interesting point here is that we can use exactly the same method for these two clustering steps.']
"Our procedure to automatically induce verb classes from verb uses is summarized as follows:
1.",3.1 Overview,[0],[0]
"induce verb-specific semantic frames by clustering predicate-argument structures for each verb extracted from automatic parses as shown in the lower part of Figure 1, and
2.",3.1 Overview,[0],[0]
"induce verb classes by clustering the induced semantic frames across verbs as shown in the upper part of Figure 1.
",3.1 Overview,[0],[0]
Each of these two steps is described in the following sections in detail.,3.1 Overview,[0],[0]
We induce verb-specific semantic frames from verb uses based on the method of Kawahara et al. (2014).,3.2 Inducing Verb-specific Semantic Frames,[0],[0]
"Our semantic frames consist of case slots, each of which consists of word instances that can be filled.",3.2 Inducing Verb-specific Semantic Frames,[0],[0]
"The procedure for inducing these semantic frames is as follows:
1.",3.2 Inducing Verb-specific Semantic Frames,[0],[0]
"apply dependency parsing to a raw corpus and extract predicate-argument structures for each verb from the automatic parses,
2.",3.2 Inducing Verb-specific Semantic Frames,[0],[0]
"merge the predicate-argument structures that have presumably the same meaning based on the assumption of one sense per collocation (Yarowsky, 1993) to get a set of initial frames, and
1In our replication experiment, it took a week to perform 70 iterations using Materna’s code and an Intel Xeon E5-2680 (2.7GHz) CPU.",3.2 Inducing Verb-specific Semantic Frames,[0],[0]
"To reach 1,000 iterations, which are reported to be optimum, it would take three months.
3.",3.2 Inducing Verb-specific Semantic Frames,[0],[0]
"apply clustering to the initial frames based on the Chinese Restaurant Process (Aldous, 1985) to produce verb-specific semantic frames.
",3.2 Inducing Verb-specific Semantic Frames,[0],[0]
These three steps are briefly described below.,3.2 Inducing Verb-specific Semantic Frames,[0],[0]
We apply dependency parsing to a large raw corpus.,3.2.1 Extracting Predicate-argument Structures from a Raw Corpus,[0],[0]
"We use the Stanford parser with Stanford dependencies (de Marneffe et al., 2006).2 Collapsed dependencies are adopted to directly extract prepositional phrases.
",3.2.1 Extracting Predicate-argument Structures from a Raw Corpus,[0],[0]
"Then, we extract predicate-argument structures from the dependency parses.",3.2.1 Extracting Predicate-argument Structures from a Raw Corpus,[0],[0]
"Dependents that have the following dependency relations to a verb are extracted as arguments:
nsubj, xsubj, dobj, iobj, ccomp, xcomp, prep ∗
In this process, the verb and arguments are lemmatized, and only the head of an argument is preserved for compound nouns.
",3.2.1 Extracting Predicate-argument Structures from a Raw Corpus,[0],[0]
Predicate-argument structures are collected for each verb and the subsequent processes are applied to the predicate-argument structures of each verb.,3.2.1 Extracting Predicate-argument Structures from a Raw Corpus,[0],[0]
"To make the computation feasible, we merge the predicate-argument structures that have the same or similar meaning to get initial frames.",3.2.2 Constructing Initial Frames from Predicate-argument Structures,[0],[0]
These initial frames are the input of the subsequent clustering process.,3.2.2 Constructing Initial Frames from Predicate-argument Structures,[0],[0]
"For this merge, we assume one sense per collocation (Yarowsky, 1993) for predicateargument structures.
",3.2.2 Constructing Initial Frames from Predicate-argument Structures,[0],[0]
"For each predicate-argument structure of a verb, we couple the verb and an argument to make a unit for sense disambiguation.",3.2.2 Constructing Initial Frames from Predicate-argument Structures,[0],[0]
"We select an argument in the following order by considering the degree of effect on the verb sense:3
dobj, ccomp, nsubj, prep ∗, iobj.",3.2.2 Constructing Initial Frames from Predicate-argument Structures,[1.000000003330164],"['We select an argument in the following order by considering the degree of effect on the verb sense:3 dobj, ccomp, nsubj, prep ∗, iobj.']"
"Then, the predicate-argument structures that have the same verb and argument pair (slot and word, e.g., “dobj:effect”) are merged into an initial frame.",3.2.2 Constructing Initial Frames from Predicate-argument Structures,[1.0],"['Then, the predicate-argument structures that have the same verb and argument pair (slot and word, e.g., “dobj:effect”) are merged into an initial frame.']"
"After this process, we discard minor initial frames that occur fewer than 10 times.
",3.2.2 Constructing Initial Frames from Predicate-argument Structures,[0],[0]
"2http://nlp.stanford.edu/software/lex-parser.shtml 3If a predicate-argument structure has multiple preposi-
tional phrases, one of them is randomly selected.",3.2.2 Constructing Initial Frames from Predicate-argument Structures,[0],[0]
"We cluster initial frames for each verb to produce semantic frames using the Chinese Restaurant Process (Aldous, 1985), regarding each initial frame as an instance.
",3.2.3 Clustering Method,[1.0000000576976278],"['We cluster initial frames for each verb to produce semantic frames using the Chinese Restaurant Process (Aldous, 1985), regarding each initial frame as an instance.']"
"We calculate the posterior probability of a cluster cj given an initial frame fi as follows:
P (cj |fi) ∝",3.2.3 Clustering Method,[0],[0]
"{ n(cj) N+α · P (fi|cj) cj ̸= new
α N+α · P (fi|cj) cj",3.2.3 Clustering Method,[0],[0]
"= new,
(1)
where N is the number of initial frames for the target verb and n(cj) is the current number of initial frames assigned to the cluster cj .",3.2.3 Clustering Method,[0],[0]
α is a hyperparameter that determines how likely it is for a new cluster to be created.,3.2.3 Clustering Method,[0],[0]
"In this equation, the first term is the Dirichlet process prior and the second term is the likelihood of fi.
P (fi|cj) is defined based on the DirichletMultinomial distribution as follows:
P (fi|cj) = ∏ w∈V",3.2.3 Clustering Method,[0],[0]
"P (w|cj)count(fi,w), (2)
where V is the vocabulary in all case slots cooccurring with the verb and count(fi, w) is the number of w in the initial frame fi.",3.2.3 Clustering Method,[0],[0]
"The original method in Kawahara et al. (2014) defined w as pairs of slots and words, e.g., “nsubj:child” and “dobj:bird,” but does not consider slot-only features, e.g., “nsubj” and “dobj,” which ignore lexical information.",3.2.3 Clustering Method,[0],[0]
"Here we experiment with both representations and compare the results.
",3.2.3 Clustering Method,[0],[0]
"P (w|cj) is defined as follows:
P (w|cj) =",3.2.3 Clustering Method,[0],[0]
"count(cj , w) + β∑ t∈V",3.2.3 Clustering Method,[0],[0]
"count(cj , t) + |V",3.2.3 Clustering Method,[0],[0]
"| · β , (3)
where count(cj , w) is the current number of w in the cluster cj , and β is a hyper-parameter of Dirichlet distribution.",3.2.3 Clustering Method,[0],[0]
"For a new cluster, this probability is uniform (1/|V |).
",3.2.3 Clustering Method,[0],[0]
"We regard each output cluster as a semantic frame, by merging the initial frames in a cluster into a semantic frame.",3.2.3 Clustering Method,[0],[0]
"In this way, semantic frames for each verb are acquired.
",3.2.3 Clustering Method,[0],[0]
We use Gibbs sampling to realize this clustering.,3.2.3 Clustering Method,[0],[0]
"To induce verb classes across verbs, we apply clustering to the induced verb-specific semantic
frames.",3.3 Inducing Verb Classes from Semantic Frames,[0],[0]
We can use exactly the same clustering method as described in Section 3.2.3 by using semantic frames for multiple verbs as an input instead of initial frames for a single verb.,3.3 Inducing Verb Classes from Semantic Frames,[0],[0]
"This is because an initial frame has the same structure as a semantic frame, which is produced by merging initial frames.",3.3 Inducing Verb Classes from Semantic Frames,[0],[0]
"We regard each output cluster as a verb class this time.
",3.3 Inducing Verb Classes from Semantic Frames,[0],[0]
"For the features, w, in equation (2), we try the two representations again: slot-only features and slot-word pair features.",3.3 Inducing Verb Classes from Semantic Frames,[0],[0]
The representation using only slots corresponds to the consideration of only syntactic argument patterns.,3.3 Inducing Verb Classes from Semantic Frames,[0],[0]
The other representation using the slot-word pairs means that semantic similarity based on word overlap is naturally considered by looking at lexical information.,3.3 Inducing Verb Classes from Semantic Frames,[0],[0]
We will compare in our experiments four possible combinations: two feature representations for each of the two clustering steps.,3.3 Inducing Verb Classes from Semantic Frames,[0],[0]
We first describe our experimental settings and define evaluation metrics to evaluate induced soft clusterings of verb classes.,4 Experiments and Evaluations,[0],[0]
"Then, we conduct type-level multi-class evaluations, type-level single-class evaluations and token-level multiclass evaluations.",4 Experiments and Evaluations,[0],[0]
These two levels of evaluations are performed by considering the work of Reichart et al. (2010) on clustering evaluation.,4 Experiments and Evaluations,[0],[0]
"Finally, we discuss the results of our full experiments.",4 Experiments and Evaluations,[0],[0]
"We use two kinds of large-scale corpora: a web corpus and the English Gigaword corpus.
To prepare a web corpus, we extracted sentences from crawled web pages that are judged to be written in English based on the encoding information.",4.1 Experimental Settings,[0],[0]
"Then, we selected sentences that consist of at most 40 words, and removed duplicated sentences.",4.1 Experimental Settings,[0],[0]
"From this process, we obtained a corpus of one billion sentences, totaling approximately 20 billion words.",4.1 Experimental Settings,[0],[0]
"We focused on verbs whose frequency in the web corpus was more than 1,000.",4.1 Experimental Settings,[1.0],"['We focused on verbs whose frequency in the web corpus was more than 1,000.']"
"There were 19,649 verbs, including phrasal verbs, and separating passive and active constructions.",4.1 Experimental Settings,[1.0],"['There were 19,649 verbs, including phrasal verbs, and separating passive and active constructions.']"
"We extracted 2,032,774,982 predicate-argument structures.
",4.1 Experimental Settings,[0],[0]
We also used the English Gigaword corpus (LDC2011T07; English Gigaword Fifth Edition).,4.1 Experimental Settings,[0],[0]
"This corpus consists of approximately 180 million sentences, which totaling four billion words.
",4.1 Experimental Settings,[0],[0]
"There were 7,356 verbs after applying the same frequency threshold as the web corpus.",4.1 Experimental Settings,[0],[0]
"We extracted 423,778,278 predicate-argument structures from this corpus.
",4.1 Experimental Settings,[0],[0]
We set the hyper-parameters α in (1) and β in (3) to 1.0.,4.1 Experimental Settings,[0],[0]
The cluster assignments for all the components were initialized randomly.,4.1 Experimental Settings,[0],[0]
We took 100 samples for each input frame and selected the cluster assignment that has the highest probability.,4.1 Experimental Settings,[0],[0]
"To measure the precision and recall of a clustering, modified purity and inverse purity (also called collocation or weighted class accuracy) are commonly used in previous studies on verb clustering (e.g., Sun and Korhonen (2009)).",4.2 Evaluation Metrics,[0],[0]
"However, since these measures are only applicable to a hard clustering, it is necessary to extend them to be applicable to a soft clustering, because in our task a verb can belong to multiple clusters or classes.4",4.2 Evaluation Metrics,[0],[0]
We propose a normalized version of modified purity and inverse purity.,4.2 Evaluation Metrics,[0],[0]
"This kind of normalization for soft clusterings was performed for other evaluation metrics as in Springorum et al. (2013).
",4.2 Evaluation Metrics,[0],[0]
"To measure the precision of a clustering, a normalized version of modified purity is defined as follows.",4.2 Evaluation Metrics,[0],[0]
Suppose K is the set of automatically induced clusters and G is the set of gold classes.,4.2 Evaluation Metrics,[0],[0]
Let Ki be the verb vector of the i-th cluster and Gj be the verb vector of the j-th gold class.,4.2 Evaluation Metrics,[0],[0]
"Each component of these vectors is a normalized frequency, which equals a cluster/class attribute probability given a verb.",4.2 Evaluation Metrics,[0],[0]
"Where there is no frequency information available for class distribution, such as the gold-standard data described in Section 4.3, we use a uniform distribution across the verb’s classes.",4.2 Evaluation Metrics,[0],[0]
The core idea of purity is that each cluster Ki is associated with its most prevalent gold class.,4.2 Evaluation Metrics,[0],[0]
"In addition, to penalize clusters that consist of only one verb, such singleton clusters in K are considered as errors, as is usual with modified purity.",4.2 Evaluation Metrics,[0],[0]
"The normalized modified purity (nmPU) can then be written as follows:
nmPU = 1 N ∑ i s.t. |Ki|>1",4.2 Evaluation Metrics,[0],[0]
"max j δKi(Ki ∩ Gj), (4)
δKi(Ki ∩ Gj) = ∑
v∈Ki∩Gj civ, (5)
",4.2 Evaluation Metrics,[0],[0]
4Korhonen et al. (2003) evaluated hard clusterings based on a gold standard with multiple classes per verb.,4.2 Evaluation Metrics,[0],[0]
"They reported only precision measures including modified purity, and avoided extending the evaluation metrics for soft clusterings.
where N denotes the total number of verbs, |Ki| denotes the number of positive components in Ki, and civ denotes the v-th component of Ki.",4.2 Evaluation Metrics,[0],[0]
"δKi(Ki ∩ Gj) means the total mass of the set of verbs in Ki ∩Gj , given by summing up the values in Ki.",4.2 Evaluation Metrics,[0],[0]
"In case of evaluating a hard clustering, this is equal to |Ki ∩",4.2 Evaluation Metrics,[0],[0]
"Gj | because all the values of civ are equal to 1.
",4.2 Evaluation Metrics,[0],[0]
"As usual, the following normalized inverse purity (niPU) is used to measure the recall of a clustering:
niPU = 1 N ∑",4.2 Evaluation Metrics,[0],[0]
j max i δGj (Ki ∩ Gj).,4.2 Evaluation Metrics,[0],[0]
"(6)
Finally, we use the harmonic mean (F1) of nmPU and niPU as a single measure of clustering quality.",4.2 Evaluation Metrics,[0],[0]
"We first evaluate our induced verb classes on the test set created by Korhonen et al. (2003) (Table 1 of their paper) which was created by considering verb polysemy on the basis of Levin’s classes and the LCS database (Dorr, 1997).",4.3 Type-level Multi-class Evaluations,[0],[0]
"It consists of 62 classes and 110 verbs, out of which 35 verbs are monosemous and 75 verbs are polysemous.",4.3 Type-level Multi-class Evaluations,[0],[0]
The average number of verb classes per verb is 2.24.,4.3 Type-level Multi-class Evaluations,[0],[0]
"An excerpt from this data is shown in Table 1.
",4.3 Type-level Multi-class Evaluations,[0],[0]
"As our baselines, we adopt two previously proposed methods.",4.3 Type-level Multi-class Evaluations,[0],[0]
We first implemented a soft clustering method for verb class induction proposed by Korhonen et al. (2003).,4.3 Type-level Multi-class Evaluations,[1.0],['We first implemented a soft clustering method for verb class induction proposed by Korhonen et al. (2003).']
They used the information bottleneck (IB) method for assigning probabilities of classes to each verb.,4.3 Type-level Multi-class Evaluations,[0],[0]
"Note that Korhonen et al. (2003) actually hardened the clusterings and left
the evaluations of soft clusterings for their future work.",4.3 Type-level Multi-class Evaluations,[0],[0]
"For input data, we employ VALEX (Korhonen et al., 2006), which is a publicly-available large-scale subcategorization lexicon.5 By following the method of Korhonen et al. (2003), prepositional phrases (pp) are parameterized for two frequent subcategorization frames (NP and NP PP), and the unfiltered raw frequencies of subcategorization frames are used as features to represent a verb.",4.3 Type-level Multi-class Evaluations,[0],[0]
"It is necessary to specify the number of clusters, k, for the IB method beforehand, and we adopt 35 and 42 clusters according to their reported high accuracies.",4.3 Type-level Multi-class Evaluations,[0],[0]
"To output multiple classes for each verb, we set a threshold, t, for class attribute probabilities.",4.3 Type-level Multi-class Evaluations,[0],[0]
"That is, classes that have a higher class attribute probability than the threshold are output for each verb.",4.3 Type-level Multi-class Evaluations,[0],[0]
"We report the results of the following threshold values: 0.01, 0.02, 0.05 and 0.10.
",4.3 Type-level Multi-class Evaluations,[0],[0]
"The other baseline is LDA-frames (Materna, 2012).",4.3 Type-level Multi-class Evaluations,[0],[0]
"We use the induced LDA-frames that are
5http://ilexir.co.uk/applications/valex/
available on the web site.6 This frame data was induced from the BNC and consists of 1,200 frames and 400 semantic roles.",4.3 Type-level Multi-class Evaluations,[0],[0]
"Again, we set a threshold for frame attribute probabilities.
",4.3 Type-level Multi-class Evaluations,[0],[0]
We report results using our methods with four feature combinations (slot-only (S) and slot-word pair (SW) features each used for both the framegeneration and verb-class clustering steps) for both the Gigaword and web corpora.,4.3 Type-level Multi-class Evaluations,[0],[0]
"Table 2 lists evaluation results for the baseline methods and our methods.7 The results of the IB baseline and our methods are obtained by averaging five runs.
",4.3 Type-level Multi-class Evaluations,[0.9999999275091919],['Table 2 lists evaluation results for the baseline methods and our methods.7 The results of the IB baseline and our methods are obtained by averaging five runs.']
We can see that “web/SW-S” achieved the best performance and obtained a higher F1 than the baselines by more than nine points.,4.3 Type-level Multi-class Evaluations,[0],[0]
“Web/SWS” uses the combination of slot-word pair features for clustering verb-specific frames and slotonly features for clustering across verbs.,4.3 Type-level Multi-class Evaluations,[1.0],['“Web/SWS” uses the combination of slot-word pair features for clustering verb-specific frames and slotonly features for clustering across verbs.']
"Interestingly, this result indicates that slot distributions are more effective than lexical information in slotword pairs for inducing verb classes similar to the gold standard.",4.3 Type-level Multi-class Evaluations,[0],[0]
"This result is consistent with expectations, given a gold standard based on Levin’s verb classes, which are organized according to the syntactic behavior of verbs.",4.3 Type-level Multi-class Evaluations,[0],[0]
"The use of slot-word pairs for verb class induction generally merged too many frames into each class, apparently due to accidental word overlaps across verbs.
",4.3 Type-level Multi-class Evaluations,[0],[0]
The verb classes induced from the web corpus achieved a higher F1 than those from the Gigaword corpus.,4.3 Type-level Multi-class Evaluations,[0],[0]
This can be attributed to the larger size of the web corpus.,4.3 Type-level Multi-class Evaluations,[0],[0]
"The employment of this kind of huge corpus is enabled by our scalable method.
",4.3 Type-level Multi-class Evaluations,[0],[0]
"6http://nlp.fi.muni.cz/projekty/lda-frames/ 7Although we do not think that the classes with very small attribute probabilities are meaningful, the F1 scores for lower thresholds than 0.01 converged to about 66 in the case of LDA-frames.",4.3 Type-level Multi-class Evaluations,[0],[0]
"Since we focus on the handling of verb polysemy, predominant class induction for each verb is not our main objective.",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[1.0],"['Since we focus on the handling of verb polysemy, predominant class induction for each verb is not our main objective.']"
"However, we wish to compare our method with previous work on the induction of a predominant (monosemous) class for each verb.
",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"To output a single class for each verb by using our proposed method, we skip the induction of verb-specific semantic frames and instead create a single frame for each verb by merging all predicate-argument structures of the verb.",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"Then, we apply clustering to these frames across verbs.",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"For clustering features, we again compare two representations: slot-only features (S) and slot-word pair features (SW).
",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"We evaluate the single-class output for each verb based on the predominant gold-standard classes, which are defined for each verb in the test set of Korhonen et al. (2003).",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
This data contains 110 verbs and 33 classes.,4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"We evaluate these single-class outputs in the same manner as Korhonen et al. (2003), using the gold standard with multiple classes, which we also use for our multi-class evaluations.
",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0.9999999141786287],"['We evaluate these single-class outputs in the same manner as Korhonen et al. (2003), using the gold standard with multiple classes, which we also use for our multi-class evaluations.']"
"As we did with the multi-class evaluations, we adopt modified purity (mPU), inverse purity (iPU) and their harmonic mean (F1) as the metrics for the evaluation with predominant classes.",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"It is not necessary to normalize these metrics when we treat verbs as monosemous, and evaluate against the predominant sense.",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"When we evaluate against the multiple classes in the gold standard, we do normalize the inverse purity.
",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"For baselines, we once more adopt the Nearest Neighbor (NN) and Information Bottleneck (IB) methods proposed by Korhonen et al. (2003), and LDA-frames proposed by Materna (2012).",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"The
clusterings with the NN and IB methods are obtained by using the VALEX subcategorization lexicon.",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"To harden the clusterings of the IB method and the LDA-frames, the class with the highest probability is selected for each verb.",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
This hardening process is exactly the same as Korhonen et al. (2003).,4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"Note that our results of the NN and IB methods are different from those reported in their paper since the data source is different.8
Table 3 lists accuracies of baseline methods and our methods.",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
Our proposed method using the web corpus achieved comparable performance with the baseline methods on the predominant class evaluation and outperformed them on the multiple class evaluation.,4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"More sophisticated methods for predominant class induction, such as the method of Sun and Korhonen (2009) using selectional preferences, could produce better single-class outputs, but have difficulty in producing polysemy-aware verb classes.
",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"From the result, we can see that the induced verb classes based on slot-only features did not achieve a higher F1 than those based on slot-word pair features in many cases.",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
This result is different from that of multi-class evaluations in Section 4.3.,4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"We speculate that slot distributions are not so different among verbs when all uses of a verb are merged into one frame, and thus their discrimination power is lower than that in the intermediate construction of semantic frames.",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"We conduct token-level multi-class evaluations using 119 verbs, which appear 100 or more times in sections 02-21 of the SemLink WSJ corpus.",4.5 Token-level Multi-class Evaluations,[0],[0]
"These 119 verbs cover 102 VerbNet classes, and 48 of them are polysemous in the sense of being in more than one VerbNet class.",4.5 Token-level Multi-class Evaluations,[0],[0]
Each instance of these 119 verbs in this corpus belongs to one of 102 VerbNet classes.,4.5 Token-level Multi-class Evaluations,[0],[0]
We first add these instances to the instances from a raw corpus and apply the twostep clustering to these merged instances.,4.5 Token-level Multi-class Evaluations,[0],[0]
"Then, we compare the induced verb classes of the SemLink instances with their gold-standard VerbNet classes.",4.5 Token-level Multi-class Evaluations,[0],[0]
"We report the values of modified purity (mPU), inverse purity (iPU) and their harmonic mean (F1).",4.5 Token-level Multi-class Evaluations,[1.0],"['We report the values of modified purity (mPU), inverse purity (iPU) and their harmonic mean (F1).']"
"It is not necessary to normalize these metrics because the clustering of these instances is hard.
",4.5 Token-level Multi-class Evaluations,[0],[0]
"8Korhonen et al. (2003) reported that the highest modified purity was 49% against predominant classes and 60% against multiple classes.
",4.5 Token-level Multi-class Evaluations,[0],[0]
"For clustering features, we compare two feature combinations: “S-S” and “SW-S,” which achieved high performance in the type-level multiclass evaluations (Section 4.3).",4.5 Token-level Multi-class Evaluations,[1.0],"['For clustering features, we compare two feature combinations: “S-S” and “SW-S,” which achieved high performance in the type-level multiclass evaluations (Section 4.3).']"
The results of these methods are obtained by averaging five runs.,4.5 Token-level Multi-class Evaluations,[1.0],['The results of these methods are obtained by averaging five runs.']
"For a baseline, we use verb-specific semantic frames without clustering across verbs (“S-NIL” and “SW-NIL”), where these frames are considered to be verb classes but not shared across verbs.",4.5 Token-level Multi-class Evaluations,[1.0],"['For a baseline, we use verb-specific semantic frames without clustering across verbs (“S-NIL” and “SW-NIL”), where these frames are considered to be verb classes but not shared across verbs.']"
Table 4 lists accuracies of these methods for the two corpora.,4.5 Token-level Multi-class Evaluations,[0],[0]
"We can see that “SW-S” achieved a higher F1 than “S-S” and the baselines without verb class induction (“S-NIL” and “SW-NIL”).
",4.5 Token-level Multi-class Evaluations,[0.9999999886346828],['We can see that “SW-S” achieved a higher F1 than “S-S” and the baselines without verb class induction (“S-NIL” and “SW-NIL”).']
Modi et al. (2012) induced semantic frames across verbs using the monosemous assumption and reported an F1 of 44.7% (77.9% PU and 31.4% iPU) for the assignment of FrameNet frames to the FrameNet corpus.,4.5 Token-level Multi-class Evaluations,[1.0],['Modi et al. (2012) induced semantic frames across verbs using the monosemous assumption and reported an F1 of 44.7% (77.9% PU and 31.4% iPU) for the assignment of FrameNet frames to the FrameNet corpus.']
We also conducted the above evaluation against FrameNet frames for 75 verbs.9,4.5 Token-level Multi-class Evaluations,[0],[0]
"We achieved an F1 of 62.79% (66.97% mPU and 59.09% iPU) for “web/SW-S,” and an F1 of 60.06% (65.58% mPU and 55.39% iPU) for “Gigaword/SW-S.” It is difficult to directly compare these results with Modi et al. (2012), but our induced verb classes seem to have higher F1 accuracy.",4.5 Token-level Multi-class Evaluations,[0],[0]
"We finally induce verb classes from the semantic frames of 1,667 verbs, which appear at least once in sections 02-21 of the WSJ corpus.",4.6 Full Experiments and Discussions,[0],[0]
"Based on the best results in the above evaluations, we induced semantic frames using slot-word pair features, and then induced verb classes using slotonly features.",4.6 Full Experiments and Discussions,[1.0],"['Based on the best results in the above evaluations, we induced semantic frames using slot-word pair features, and then induced verb classes using slotonly features.']"
"We ended with 38,481 semantic frames and 699 verb classes from the Gigaword
9Since FrameNet frames are not assigned to all verbs of SemLink, the number of verbs is different from the evaluations against VerbNet classes.
corpus, and 61,903 semantic frames and 840 verb classes from the web corpus.",4.6 Full Experiments and Discussions,[0],[0]
"It took two days to induce verb classes from the Gigaword corpus and three days from the web corpus.
",4.6 Full Experiments and Discussions,[1.0000000298100213],['It took two days to induce verb classes from the Gigaword corpus and three days from the web corpus.']
Examples of verb classes and semantic frames induced from the web corpus are shown in Table 5 and Table 6.,4.6 Full Experiments and Discussions,[0],[0]
"While there are many classes with consistent meanings, such as “Class 4” and “Class 16,” some classes have mixed meanings.",4.6 Full Experiments and Discussions,[0],[0]
"For instance, “Class 2” consists of the semantic frames “need:2” and “say:2.”",4.6 Full Experiments and Discussions,[0],[0]
"These frames were merged due to the high syntactic similarity of constituting slot distributions, which are comprised of a subject and a sentential complement.",4.6 Full Experiments and Discussions,[0],[0]
"To improve the quality of verb classes, it is necessary to develop a clustering model that can consider syntactic and lexical similarity in a balanced way.",4.6 Full Experiments and Discussions,[1.0],"['To improve the quality of verb classes, it is necessary to develop a clustering model that can consider syntactic and lexical similarity in a balanced way.']"
We presented a step-wise unsupervised method for inducing verb classes from instances in gigaword corpora.,5 Conclusion,[0],[0]
This method first clusters predicateargument structures to induce verb-specific semantic frames and then clusters these semantic frames across verbs to induce verb classes.,5 Conclusion,[0],[0]
"Both clustering steps are performed with exactly the same method, which is based on the Chinese Restaurant Process.",5 Conclusion,[0],[0]
"The resulting semantic frames and verb classes are open to the public and also can be searched via our web interface.10
10http://nlp.ist.i.kyoto-u.ac.jp/member/kawahara/cf/crp.en/
From the results, we can see that the combination of the slot-word pair features for clustering verb-specific frames and the slot-only features for clustering across verbs is the most effective and outperforms the baselines by approximately 10 points.",5 Conclusion,[1.000000063377641],"['The resulting semantic frames and verb classes are open to the public and also can be searched via our web interface.10 From the results, we can see that the combination of the slot-word pair features for clustering verb-specific frames and the slot-only features for clustering across verbs is the most effective and outperforms the baselines by approximately 10 points.']"
"This indicates that slot distributions are more effective than lexical information in slotword pairs for the induction of verb classes, when Levin-style classes are used for evaluation.",5 Conclusion,[0],[0]
"This is consistent with Levin’s principle of organizing verb classes according to the syntactic behavior of verbs.
",5 Conclusion,[0],[0]
"As applications of the resulting semantic frames and verb classes, we plan to integrate them into syntactic parsing, semantic role labeling and verb sense disambiguation.",5 Conclusion,[1.0],"['As applications of the resulting semantic frames and verb classes, we plan to integrate them into syntactic parsing, semantic role labeling and verb sense disambiguation.']"
"For instance, Kawahara and Kurohashi (2006) improved accuracy of dependency parsing based on Japanese semantic frames automatically induced from a raw corpus.",5 Conclusion,[1.0],"['For instance, Kawahara and Kurohashi (2006) improved accuracy of dependency parsing based on Japanese semantic frames automatically induced from a raw corpus.']"
"It is also valuable and promising to apply the induced verb classes to NLP applications as used in metaphor identification (Shutova et al., 2010) and argumentative zoning (Guo et al., 2011).",5 Conclusion,[1.0],"['It is also valuable and promising to apply the induced verb classes to NLP applications as used in metaphor identification (Shutova et al., 2010) and argumentative zoning (Guo et al., 2011).']"
This work was supported by Kyoto University John Mung Program and JST CREST.,Acknowledgments,[0],[0]
"We also gratefully acknowledge the support of the National Science Foundation Grant NSF-IIS-1116782, A Bayesian Approach to Dynamic Lexical Resources for Flexible Language Processing.",Acknowledgments,[0],[0]
"Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.",Acknowledgments,[0],[0]
We present an unsupervised method for inducing verb classes from verb uses in gigaword corpora.,abstractText,[0],[0]
Our method consists of two clustering steps: verb-specific semantic frames are first induced by clustering verb uses in a corpus and then verb classes are induced by clustering these frames.,abstractText,[0],[0]
"By taking this step-wise approach, we can not only generate verb classes based on a massive amount of verb uses in a scalable manner, but also deal with verb polysemy, which is bypassed by most of the previous studies on verb clustering.",abstractText,[0],[0]
"In our experiments, we acquire semantic frames and verb classes from two giga-word corpora, the larger comprising 20 billion words.",abstractText,[0],[0]
The effectiveness of our approach is verified through quantitative evaluations based on polysemy-aware gold-standard data.,abstractText,[0],[0]
A Step-wise Usage-based Method for Inducing Polysemy-aware Verb Classes,title,[0],[0]
