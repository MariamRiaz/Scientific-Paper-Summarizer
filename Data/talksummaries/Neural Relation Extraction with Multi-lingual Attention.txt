29	1	It indicates that our framework can take full advantages of sentences in different languages and better capture sophisticated patterns expressing relations.
52	1	The sentence encoder can also be implemented with GRU (Cho et al., 2014) or LSTM (Hochreiter and Schmidhuber, 1997).
53	1	In experiments, we find CNN can achieve a better trade-off between computational efficiency and performance effectiveness.
57	2	We introduce the two components in detail as follows.
58	1	The sentence encoder aims to transform a sentence x into its distributed representation x via CNN.
61	1	Following (Zeng et al., 2014), we transform each input word into the concatenation of two kinds of representations: (1) a word embedding which captures syntactic and semantic meanings of the word, and (2) a position embedding which specifies the position information of this word with respect to two target entities.
63	1	(da and db are the dimensions of word embeddings and position embeddings respectively)
65	1	First, the convolutional layer extracts local features by sliding a window of length l over the sentence and perform a convolution within each sliding window.
72	1	To address the wrong-labelling issue in distant supervision, we follow the idea of sentence-level attention (Lin et al., 2016) and set mono-lingual attention for MNRE.
79	1	The key idea of cross-lingual attention is to emphasize those sentences which have strong consistency among different languages.
82	1	Suppose j indicates a language and k is a another language (k ̸= j).
88	1	Those vectors with j = k are mono-lingual attention vectors, and those with j ̸= k are cross-lingual attention vectors.
91	1	Here we simply define Rk as composed by rk in Eq.
93	1	Note that, in the training phase, the vectors {Sjk} are constructed using Eq.
94	2	(3) and (6) using the labelled relation.
96	1	Here we introduce the learning and optimization details of our MNRE framework.
98	1	To solve the optimization problem, we adopt mini-batch stochastic gradient descent (SGD) to minimize the objective function.
145	1	From the table, we can see that: (1) For the relation Contains of which the number of English training instances is only 1/7 of Chinese ones, CNN-En gets much worse performance as compared to CNN-Zh due to the lack of training data.
148	1	The reason is perhaps that, CNN-Zh of the relation is not sufficiently trained because there are only 210 Chinese training instances for this relation.
157	1	(2) MNRE(CNN)-R has similar performance as compared to MNRE(CNN) when the recall is low.
158	2	However, it has a sharp decline when the recall reaches 0.25.
165	2	Hence, the word-level multi-lingual attention, which may discover implicit alignments between words in multiple languages, will further improve multi-lingual relation extraction.
166	11	We will explore the effectiveness of word-level multilingual attention for relation extraction as our fu- ture work.
167	143	(2) MNRE can be flexibly implemented in the scenario of multiple languages, and this paper focuses on two languages of English and Chinese.
168	144	In future, we will extendMNRE to more languages and explore its significance.
