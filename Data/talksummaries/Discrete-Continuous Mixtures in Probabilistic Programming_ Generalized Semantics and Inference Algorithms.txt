22	4	In this paper, we carry out all these two tasks and implement the extended semantics as well as the new algorithms in a widely used PPL, Bayesian Logic (BLOG) (Milch et al., 2005a).
23	1	Measure-Theoretical Bayesian Nets (MTBNs) Measure theory can be applied to handle discrete-continuous mixtures or even more abstract measures.
25	1	We then show how MTBNs can provide a more general semantic foundation for PPLs.
26	2	More concretely, MTBNs support (1) random variables with infinitely (even uncountably) many parents, (2) random variables valued in arbitrary measure spaces (with RN as one case) distributed according to any measure (including discrete, continuous and mixed), (3) establishment of conditional independencies implied by an infinite graph, and (4) open-universe semantics in terms of the possible worlds in the vocabulary of the model.
29	1	Incorporating MTBNs into an existing PPL We incorporate MTBNs into BLOG with simple modifications and then define the generalized BLOG language, measuretheoretic BLOG, which formally supports arbitrary distributions, including discrete-continuous mixtures.
30	1	We prove that every generalized BLOG model corresponds to a unique MTBN.
35	1	In Section 3, we formally define measure-theoretic Bayesian nets and study their theoretical properties.
36	1	Section 4 describes the LLW and LPF inference algorithms for MTBNs with discrete-continuous mixtures and establishes their correctness.
63	2	A well-founded digraph (V,E) is one with no countably infinite ancestor chain v0 ← v1 ← v2 ← .
67	2	By definition, MTBNs allow us to define very general and abstract models with the following two major benefits: 1.
103	2	LLW is consistent: (2) converges almost surely to E[f(X)|X1:M ].
112	2	An approximation scheme for a measurable space Y consists of a measurable spaceA and measurable approximation functions αi : Y → A for i = 1, 2, .
123	1	, x(K) from the prior and evaluates: ∑K i=1 P (αn(Y )|X = x(i))f(x(i))∑K i=1 P (αn(Y )|X = x(i)) (3) Using Lemma 4.3, G.12, and G.13, we can show IRLW is consistent.
128	1	With In = ∏ i=1...M (αn(xi)− 2−n, αn(xi)] a 2−n-cube around x1:M we have lim n→∞ P (X1:M ∈ In|XM+1:N = xM+1:N ) w 2−dn = 1.
133	2	We now consider inference in a special class of highdimensional models known as state-space models, and show how LLW can be adapted to avoid the curse of dimensionality when used with such models.
137	2	Particles are propagated forward through the transition model P (Xt|Xt−1) and resampled at each time step t according to the weight of each particle, which is defined by the likelihood of observation Yt.
139	1	,K do x (k) t ← sample from transition compute (d(k), w(k)) by Eq.
177	2	1 is an example of origin function.
179	4	A mixture distribution is specified as: Mix({c1(ē1)→ w1(ē′), .
180	1	, ck(ēk)→ wk(ē′)}), where ci are arbitrary distributions, and wi’s are arbitrary real valued functions that sum to 1 for every possible assignment to their arguments: ∀ē′ ∑ i wi(ē ′) = 1.
181	1	Note that in our implementation of measure-theoretical BLOG, we only allow a Mix distribution to express a mixture of densities and masses for simplifying the system design, although it still possible to express the same semantics without Mix.
187	2	, U0k 〉, U0j = {cj : cj is a distinct τi constant inM} • Ui+1 = 〈U i+11 , .
191	1	• for each function f(x̄) and tuple ū from UM of the type of x̄, a function application variable Vf [ū] with the measurable space XVf [ū] = Xτf , where Xτf is the measurable space corresponding to τf , the return type of f .
194	1	That is, origin functions give correct inverse maps.
210	2	The implementation is based on BLOG’s C++ compiler (Wu et al., 2016).
233	2	We take the mean of the samples from all the particles as the predicted aircraft location.
235	7	BLOG code for the Aircraft-Tracking example true locations while the naive PF converges to the incorrect results.
236	29	We presented a new formalization, measure-theoretic Bayesian networks, for generalizing the semantics of PPLs to include random variables with mixtures of discrete and continuous distributions.
237	170	We developed provably correct inference algorithms for such random variables and incorporated MTBNs into a widely used PPL, BLOG.
238	170	We believe that together with the foundational inference algorithms, our proposed rigorous framework will facilitate the development of powerful techniques for probabilistic reasoning in practical applications from a much wider range of scientific areas.
