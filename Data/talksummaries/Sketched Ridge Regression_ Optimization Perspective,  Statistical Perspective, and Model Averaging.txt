1	23	The simplest and most thoroughly studied regression model is least squares regression (LSR).
2	29	, yn] T ∈ Rn, the LSR problem minw ‖Xw − y‖22 can be solved in O(nd2) time using the QR decomposition or in O(ndt) time using accelerated gradient descent algorithms.
3	71	Here, t is the number of iterations, which depends on the initialization, the condition number of X, and the stopping criterion.
4	125	This paper considers the n d problem, where there is much redundancy in X. Matrix sketching, as used within Randomized Linear Algebra (RLA) (Mahoney, 2011; Woodruff, 2014), works by reducing the size of X without losing too much information; this operation can be modeled as taking actual rows or linear combinations of the rows of X with a sketching matrix S to form the sketch STX.
5	63	Here S ∈ Rn×s satisfies d < s n so that STX generically has the same rank but much fewer rows as X. Sketching has been used to speed up LSR (Drineas et al., 2006; 2011; Clarkson & Woodruff, 2013; Meng & Mahoney, 2013; Nelson & Nguyên, 2013) by solving the sketched LSR problem minw ‖STXw − STy‖22 instead of the original LSR problem.
6	30	Solving sketched LSR costs either O(sd2 + Ts) time using the QR decomposition or O(sdt + Ts) time using accelerated gradient descent algorithms, where t is as defined previously1 and Ts is the time cost of sketching.
7	23	For example, Ts = O(nd log s) when S is the subsampled randomized Hadamard transform (Drineas et al., 2011), and Ts = O(nd) when S is a CountSketch matrix (Clarkson & Woodruff, 2013).
8	72	There has been much work in RLA on analyzing the quality of sketched LSR with different sketching methods and different objectives; see the reviews (Mahoney, 2011; Woodruff, 2014) and the references therein.
9	25	The concept of sketched LSR originated in the theoretical computer science literature, e.g., (Drineas et al., 2006; 2011), where the behavior of sketched LSR was studied from an optimization perspective.
11	14	This line of work established that if s = O(d/ + poly(d)), then the objective function value ‖Xw̃ − y ∥∥2 2 is at most times worse than ‖Xw?−y ∥∥2 2 .
12	21	These works also bounded ‖w̃−w?‖22 in terms of the difference in the objective function values and the condition number of XTX.
16	40	The optimization perspective is relevant when the data can be taken as deterministic values.
31	62	With sketch size s = Õ(d/ ), the objective satisfies f(Wc) ≤ (1 + )f(W?).
32	11	• Hessian sketch does not achieve relative error in the objective value.
36	86	We then study classical and Hessian sketches from the statistical perspective, by modeling Y = XW0 + Ξ as the sum of a true linear model and random noise, decomposing the risk R(W) = E‖XW − XW0‖2F into bias and variance terms, and bounding these terms.
37	17	We draw the following conclusions (see Theorems 4, 5, 6): • The bias of the classical sketch can be nearly as small as that of the optimal solution.
39	39	Therefore over-regularization, i.e., large γ, should be used to supress the variance.
40	23	(As γ increases, the bias increases, and the variance decreases.)
46	29	This is an empirical demonstration of the fact that the near-optimal properties of sketching from the optimization perspective are much less relevant in a statistical setting than its suboptimal statistical properties.
47	87	We propose to use model averaging, which averages the solutions of g sketched MRR problems, to attain lower optimization and statistical errors.
48	16	Without ambiguity, we denote classical and Hessian sketches with model averaging by Wc and Wh, respectively.
55	23	Different from bagging, our model averaging approach is not limited to uniform sampling.
58	18	Assume the sketch size is s = Õ( √ nd).
61	27	This observation is in accordance with the observation that bagging reduces variance.
63	11	Assuming that the data have been shuffled randomly, each machine contains a sketch constructed by uniformly sampled rows from the dataset without replacement.
64	13	In this setting, the model averaging procedure will communicate the g local models only once to return the final estimate; this process has very low communication complexity and latency, and it suggests two further applications of classical sketch with model averaging: • Model Averaging for Machine Learning.
65	12	If a lowprecision solution is acceptable, the averaged solution can be used in lieu of distributed numerical optimization algorithms requiring multiple rounds of communication.
73	22	Lu et al. (2013) has considered a different application of sketching to ridge regression: they assume d n, reduce the number of features in X using sketching, and conduct statistical analysis.
87	46	Section 2 defines our notation and introduces the sketching schemes we consider.
89	9	Section 4 conducts experiments to verify our theories and demonstrates the usefulness of model averaging.
