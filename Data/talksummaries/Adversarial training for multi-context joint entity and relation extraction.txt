0	37	Many neural network methods have recently been exploited in various natural language processing (NLP) tasks, such as parsing (Zhang et al., 2017), POS tagging (Lample et al., 2016), relation extraction (dos Santos et al., 2015), translation (Bahdanau et al., 2015), and joint tasks (Miwa and Bansal, 2016).
1	39	However, Szegedy et al. (2014) observed that intentional small scale perturbations (i.e., adversarial examples) to the input of such models may lead to incorrect decisions (with high confidence).
2	49	Goodfellow et al. (2015) proposed adversarial training (AT) (for image recognition) as a regularization method which uses a mixture of clean and adversarial examples to enhance the robustness of the model.
4	52	We start from a baseline joint model that performs the tasks of named entity recognition (NER) and relation extraction at once.
5	17	Previously proposed models (summarized in Section 2) exhibit several issues that the neural network-based baseline approach (detailed in Section 3.1) overcomes: (i) our model uses automatically extracted features without the need of external parsers nor manually extracted features (see Gupta et al. (2016); Miwa and Bansal (2016); Li et al. (2017)), (ii) all entities and the corresponding relations within the sentence are extracted at once, instead of examining one pair of entities at a time (see Adel and Schütze (2017)), and (iii) we model relation extraction in a multi-label setting, allowing multiple relations per entity (see Katiyar and Cardie (2017); Bekoulis et al. (2018a)).
6	17	The core contribution of the paper is the use of AT as an extension in the training procedure for the joint extraction task (Section 3.2).
24	8	The baseline model, described in detail in Bekoulis et al. (2018b), is illustrated in Fig.
25	16	It aims to detect (i) the type and the boundaries of the entities and (ii) the relations between them.
28	8	The character embeddings are fed to a bidirectional LSTM (BiLSTM) to obtain the character-based representation of the word.
36	15	For instance, the BIO encoding scheme imposes several constraints in the NER task (e.g., the B-PER and ILOC tags cannot be sequential).
40	29	The entity tags are later fed into the relation extraction layer as label embeddings (see Fig.
45	12	1, “Smith” could be involved not only in a Lives in relation with the token “California” (head) but also in other relations simultaneously (e.g.,Works for, Born In with some corresponding tokens).
49	8	During training, we minimize the cross-entropy loss Lrel as: n∑ i=0 m∑ j=0 − logP(yi,j , ri,j | wi; θ) (1) where m is the number of associated heads (and thus relations) per word wi.
51	20	The final objective for the joint task is computed as LJOINT(w; θ) = LNER + Lrel where θ is a set of parameters.
52	11	In the case of multi-token entities, only the last token of the entity can serve as head of another token, to eliminate redundant relations.
57	18	We generate an adversarial example by adding the worst-case perturbation ηadv to the original embedding w that maximizes the loss function: ηadv = argmax ‖η‖≤ǫ LJOINT(w + η; θ̂) (2) where θ̂ is a copy of the current model parameters.
61	23	We evaluate our models on four datasets, using the code as available from our github codebase.1 Specifically, we follow the 5-fold crossvalidation defined by Miwa and Bansal (2016) for the ACE04 (Doddington et al., 2004) dataset.
63	9	We also evaluate our models on the NER task similar to Miwa and Sasaki (2014) in the same dataset using 10-fold cross validation.
64	23	For the Dutch Real Estate Classifieds, DREC (Bekoulis et al., 2017) dataset, we use train-test splits as in Bekoulis et al. (2018a).
74	17	Table 1 shows our experimental results.
77	31	1 with the CRF layer and the sigmoid loss, (ii) baseline EC: the proposed model with the softmax layer for EC, (iii) baseline (EC) + AT: the baseline regularized using AT.
78	12	The final three columns present the F1 results for the two subtasks and their average performance.
88	16	Moreover, compared to the model of Gupta et al. (2016) that relies on complex features, the baseline model performs within a margin of 1% in terms of overall F1 score.
93	10	2 show the effectiveness of the adversarial training on top of the baseline model.
94	24	In all of the experiments, AT improves the predictive performance of the baseline model in the joint setting.
95	44	2, the performance of the models using AT is closer to maximum even from the early training epochs.
97	20	For CoNLL04, we note an improvement in the overall F1 of 0.4% for the EC and 0.8% for the NER tasks, respectively.
106	16	Joint entity recognition and relation extraction as a multi-head selection problem.
111	19	Ian Goodfellow, Jonathon Shlens, and Christian Szegedy.
125	64	Going out on a limb: Joint extraction of entity mentions and relations without dependency trees.
