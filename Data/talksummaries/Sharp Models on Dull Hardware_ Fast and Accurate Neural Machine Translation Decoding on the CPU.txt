1	52	One of the biggest challenges is the high training and decoding costs of these neural machine translation (NMT) system, which is often at least an order of magnitude higher than a phrase-based system trained on the same data.
3	29	Wu et al. (2016) was able to reach CPU decoding speeds of 100 words/sec for a deep model, but used 44 CPU cores to do so.
4	32	There has been recent work in speeding up decoding by reducing the search space (Kim and Rush, 2016), but little in computational improvements.
10	14	These speedups do not affect decoding results, so they can be applied universally.
15	23	The network architecture is very similar to Bahdanau et al. (2014), and specific details of layer size/depth are provided in subsequent sections.
20	40	We run word alignment (Brown et al., 1993) on the training and keep the top 20 context-free translations for each source word in the test sentence.
21	16	• The Intel MKL library is used for matrix multiplication, as it is the fastest floating point matrix multiplication library for CPUs.
24	82	Since each sentence is decoded separately, we can only batch over the hypotheses in the beam as well as the input vectors on the source side.
25	25	This section describes a number of speedups that can be made to a CPU-based attentional sequenceto-sequence beam decoder.
33	36	A reference implementation of 16-bit multiplication in C++/SSE2 is provided in the supplementary material, with a thorough description of lowlevel details.2 A comparison between our 16-bit integer implementation and Intel MKL’s 32-bit floating point multiplication is given in Figure 1.
34	46	We can see that 16-bit multiplication is 2x-3x faster than 32- bit multiplication for batch sizes between 2 and 8, which is the typical range of the beam size b.
36	27	In the first hidden layer on the source and target sides, xi corresponds to word embeddings.
37	37	Since this is a closed set of values that are fixed after training, the vectors V xi can be pre-computed (Devlin et al., 2014) for each word in the vocabulary and stored in a lookup table.
38	69	This can only be applied to the first hidden layer.
39	17	Pre-computation does increase the memory cost of the model, since we must store r × 3 floats per word instead of e. However, if we only compute the k most frequently words (e.g., k = 8, 000), this reduces the pre-computation memory by 90% but still results in 95%+ token coverage due to the Zipfian distribution of language.
40	91	The attention context computation in the GRU can be re-factored as follows: Uci = U( ∑ j αijsj) = ∑ j αij(Usj) Crucially, the hidden vector representation sj is only dependent on the source sentence, while aij is dependent on the target hypothesis.
46	32	Therefore, if two partial hypotheses in the beam only differ by the last emitted word, their hi−1 vectors will be identical.
53	36	Since these speedups are all mathematical identities excluding quantization noise, all outputs achieve 36.2 BLEU and are 99.9%+ identical.
54	27	The largest improvement is from 16-bit matrix multiplication, but all speedups contribute a significant amount.
55	40	Overall, we are able to achieve a 4.4x speedup over a fast baseline decoder.
56	56	Although the absolute speed is impressive, the model only uses one target layer and is several BLEU behind the SOTA, so the next goal is to maximize model accuracy while still achieving speeds greater than some target, such as 100 words/sec.
58	18	Several past works have noted that convolutional neural networks (CNNs) are significantly less expensive than RNNs, and replaced the source and/or target side with a CNN-based architecture (Gehring et al., 2016; Kalchbrenner et al., 2016).
70	18	The same pattern can be used for more FC layers, and the FC layers can be a different size than the bottom or top hidden layers.
71	58	The top hidden layer can be an RNN or an FC layer.
76	27	Model (S1) and (S2) are one and two layer baselines.
77	28	Model (S4), which uses 7 intermediate FC layers, has similar decoding cost to (S2) while doubling the improvement over (S1) to 1.2 BLEU.
80	63	We can see that the 2-model ensemble improves results by 0.9 BLEU, but the 3-model ensemble has little additional improvment.
82	48	All together, we were able to achieve a BLEU score of 38.3 while decoding at 100 words/sec on a single CPU core.
83	23	As a point of comparison, Wu et al. (2016) achieves similar BLEU scores on this test set (37.9 to 38.9) and reports a CPU decoding speed of ~100 words/sec (0.2226 sents/sec), but parallelizes this decoding across 44 CPU cores.
84	52	System (S7), which is our re-implementation of Wu et al. (2016), decodes at 28 words/sec on one CPU core, using all of the speedups described in Section 4.
