37	12	Section 7 concludes the paper.
65	3	This section will introduce our proposed deep keyphrase generation method in detail.
66	25	First, the task of keyphrase generation is defined, followed by an overview of how we apply the RNN Encoder-Decoder model.
67	26	Details of the framework as well as the copying mechanism will be introduced in Sections 3.3 and 3.4.
69	5	Both the source text x(i) and keyphrase p(i,j) are sequences of words: x(i) = x (i) 1 , x (i) 2 , .
71	5	Each data sample contains one source text sequence and multiple target phrase sequences.
72	3	To apply the RNN Encoder-Decoder model, the data need to be converted into text-keyphrase pairs that contain only one source sequence and one target sequence.
73	7	We adopt a simple way, which splits the data sample (x(i),p(i)) into Mi pairs: (x(i),p(i,1)), (x(i),p(i,2)), .
76	13	The basic idea of our keyphrase generation model is to compress the content of source text into a hidden representation with an encoder and to generate corresponding keyphrases with the decoder, based on the representation .
77	2	Both the encoder and decoder are implemented with recurrent neural networks (RNN).
78	4	The encoder RNN converts the variable-length input sequence x = (x1, x2, ..., xT ) into a set of hidden representation h = (h1, h2, .
79	4	, hT ), by iterating the following equations along time t: ht = f (xt,ht−1) (1) where f is a non-linear function.
84	4	A bidirectional gated recurrent unit (GRU) is applied as our encoder to replace the simple recurrent neural network.
86	10	As a result, the above non-linear function f is replaced by the GRU function (see in (Cho et al., 2014)).
93	19	Therefore, the RNN is not able to recall any keyphrase that contains out-ofvocabulary words.
94	80	Actually, important phrases can also be identified by positional and syntactic information in their contexts, even though their exact meanings are not known.
95	10	The copying mechanism (Gu et al., 2016) is one feasible solution that enables RNN to predict out-of-vocabulary words by selecting appropriate words from the source text.
96	13	By incorporating the copying mechanism, the probability of predicting each new word yt consists of two parts.
97	55	The first term is the probability of generating the term (see Equation 3) and the second one is the probability of copying it from the source text: p(yt|y1,...,t−1,x) = pg(yt|y1,...,t−1,x) + pc(yt|y1,...,t−1,x) (5) Similar to attention mechanism, the copying mechanism weights the importance of each word in source text with a measure of positional attention.
98	36	But unlike the generative RNN which predicts the next word from all the words in vocabulary, the copying part pc(yt|y1,...,t−1,x) only considers the words in source text.
99	11	Consequently, on the one hand, the RNN with copying mechanism is able to predict the words that are out of vocabulary but in the source text; on the other hand, the model would potentially give preference to the appearing words, which caters to the fact that most keyphrases tend to appear in the source text.
100	3	pc(yt|y1,...,t−1,x) = 1 Z ∑ j:xj=yt exp(ψc(xj)), y ∈ χ ψc(xj) = σ(h T j Wc)st (6) where χ is the set of all of the unique words in the source text x, σ is a non-linear function and Wc ∈ R is a learned parameter matrix.
101	89	Z is the sum of all the scores and is used for normalization.
102	10	Please see (Gu et al., 2016) for more details.
104	8	Then, we introduce our evaluation metrics and baselines.
105	10	There are several publicly-available datasets for evaluating keyphrase generation.
106	3	The largest one came from Krapivin et al. (2008), which contains 2,304 scientific publications.
107	17	However, this amount of data is unable to train a robust recurrent neural network model.
108	7	In fact, there are millions of scientific papers available online, each of which contains the keyphrases that were assigned by their authors.
110	32	(Han et al., 2013; Rui et al., 2016).
