9	38	The second limitation of MLE is that training is based on predicting the next token given the input and preceding ground-truth output tokens, while at test time the model predicts conditioned on the input and the so-far generated output sequence.
10	61	Given the exponentially large output space of natural language sentences, it is not obvious that the learned RNNs generalize well beyond the relatively sparse distribution of ground-truth sequences used during MLE optimization.
52	15	(3) The input is encoded by gθ and used to initialize the state sequence, and fθ is a non-linear function that updates the state given the previous state ht−1, the last output token yt−1, and possibly the input x.
53	16	The state update function can take different forms, the ones including gating mechanisms such as LSTMs (Hochreiter and Schmidhuber, 1997) and GRUs (Chung et al., 2014) are particularly effective to model long sequences.
57	33	Given a ground-truth target sequence y∗, maximum likelihood estimation (MLE) of the network parameters θ amounts to minimizing the loss `MLE(y ∗, x) = − ln pθ(y∗|x) (4) = − T∑ t=1 ln pθ(y ∗ t |h∗t ).
58	49	(5) The loss can equivalently be expressed as the KLdivergence between a Dirac centered on the target output (with δa(x) = 1 at x = a and 0 otherwise) and the model distribution, either at the sequencelevel or at the token-level: `MLE(y ∗, x) = DKL ( δy∗ ||pθ(y|x) ) (6) = T∑ t=1 DKL ( δy∗t ||pθ(yt|h ∗ t ) ) .
61	22	The reward augmented maximum likelihood approach of Norouzi et al. (2016) consists in replacing the sequence-level Dirac δy∗ in Eq.
62	130	(6) with a distribution r(y|y∗) ∝ exp r(y, y∗)/τ, (8) where r(y, y∗) is a “reward” function that measures the quality of sequence y w.r.t.
63	34	y∗, e.g. metrics used for evaluation of natural language processing tasks can be used, such as BLEU (Papineni et al., 2002) or CIDER (Vedantam et al., 2015).
64	57	The temperature parameter τ controls the concentration of the distribution around y∗.
68	18	(9) is intractable due to the exponentially large output space, and replaced with a Monte-Carlo approximation: Er[− ln pθ(y|x)] ≈ − L∑ l=1 ln pθ(y l|x).
69	15	Norouzi et al. (2016) show that when using the Hamming or edit distance as a reward, we can sample directly from r(y|y∗) using a stratified sampling approach.
71	16	, T} from a prior distribution on d. (ii) Uniformly select d positions in the sequence to be modified.
74	39	For a reward based on BLEU or CIDER , we cannot directly sample from r(y|y∗) since the normalizing constant, or “partition function”, of the distribution is intractable to compute.
76	31	We first sample L sequences yl from a tractable proposal distribution q(y|y∗).
82	72	In the stratified sampling method for Hamming and edit distance rewards, instead of drawing from the large vocabulary V , containing typically in the order of 104 words or more, we can restrict ourselves to a smaller subset Vsub more adapted to our task.
114	23	(18) To speed up training, and since we already forward the ground truth sequence in the RNN to evaluate the MLE part of `αSeq(y ∗, x), we propose to use the same hidden states h∗t to compute both the MLE and the sequence-level smoothed loss.
116	64	We provide the pseudo-code for training in Algorithm 1.
117	86	In this section, we compare sequence prediction models trained with maximum likelihood (MLE) with our token and sequence-level loss smoothing on two different tasks: image captioning and machine translation.
118	68	We use the MS-COCO datatset (Lin et al., 2014), which consists of 82k training images each annotated with five captions.
124	32	The MS-COCO vocabulary consists of 9,800 words that occur at least 5 times in the training set.
126	26	Restricted vocabulary sampling In this section, we evaluate the impact of the vocabulary subset from which we sample the modified sentences for sequence-level smoothing.
130	42	From the results in Table 1 we note that for the inattentive models, sampling from Vrefs or Vbatch has a better performance than sampling from the full vocabulary on all metrics.
132	37	This improvement is most notable using the CIDER reward that scores candidate sequences w.r.t.
134	18	With an attentive decoder, no matter the reward, re-sampling sentences with words from Vref rather than the full vocabulary V is better for both reward functions, and all metrics.
137	29	We provide detailed timing results in Appendix B.3.
138	47	Overall For reference, we include in Table 1 baseline results obtained using MLE, and our implementation of MLE with entropy regularization (MLE+γH) (Pereyra et al., 2017), as well as the RAML approach of Norouzi et al. (2016) which corresponds to sequence-level smoothing based on the Hamming reward and sampling replacements from the full vocabulary (Seq, Hamming, V) We observe that entropy smoothing is not able to improve performance much over MLE for the model without attention, and even deteriorates for the attention model.
141	77	For sequence-level smoothing, choosing a taskrelevant reward with importance sampling yielded better results than plain Hamming distance.
143	22	In Figure 1 we showcase captions obtained with MLE and our three variants of smoothing i.e. token-level (Tok), sequencelevel (Seq) and the combination (Tok-Seq).
149	52	The three best results on the server (Rennie et al., 2017; Yao et al., 2017; Anderson et al., 2017) are trained in two stages where they first train using MLE, before switching to policy gradient methods based on CIDEr.
