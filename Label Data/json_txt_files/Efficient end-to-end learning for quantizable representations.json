{"sections": [{"heading": "1. Introduction", "text": "Learning the representations that respect the pairwise relationships is one of the most important problems in machine learning and pattern recognition with vast applications. To this end, deep metric learning methods (Hadsell et al., 2006; Weinberger et al., 2006; Schroff et al., 2015; Song et al., 2016; Sohn, 2016; Song et al., 2017; Bell & Bala, 2015; Sener et al., 2016) aim to learn an embedding representation space such that similar data are close to each other\n1Department of Computer Science and Engineering, Seoul National University, Seoul, Korea. Correspondence to: Hyun Oh Song <hyunoh@snu.ac.kr>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nand vice versa for dissimilar data. Some of these methods have shown significant advances in various applications in retrieval (Sohn, 2016; Schroff et al., 2015), clustering (Song et al., 2017), domain adaptation (Sener et al., 2016), video understanding (Wang & Gupta, 2015), etc.\nDespite recent advances in deep metric learning methods, deploying the learned embedding representation in large scale applications poses great challenges in terms of the inference efficiency and scalability. To address this, practitioners in large scale retrieval and recommendation systems often resort to a separate post-processing step where the learned embedding representation is run through quantization pipelines such as sketches, hashing, and vector quantization in order to significantly reduce the number of data to compare during the inference while trading off the accuracy.\nIn this regard, we propose a novel end-to-end learning method for quantizable representations jointly optimizing for the quality of the network embedding representation and the performance of the corresponding binary hash code. In contrast to some of the recent methods (Cao et al., 2016; Liu & Lu, 2017), our proposed method avoids ever having to cluster the entire dataset, offers the modularity to accommodate any existing deep embedding learning techniques (Schroff et al., 2015; Sohn, 2016), and is efficiently trained in a mini-batch stochastic gradient descent setting. We show that the discrete optimization problem of finding the optimal binary hash codes given the embedding representations can be computed efficiently and exactly by solving an equivalent minimum cost flow problem. The proposed method alternates between finding the optimal hash codes of the given embedding representations in the mini-batch and adjusting the embedding representations indexed at the activated hash code dimensions via deep metric learning methods.\nOur end-to-end learning method outperforms state-of-theart deep metric learning approaches (Schroff et al., 2015; Sohn, 2016) in retrieval and clustering tasks on the Cifar-100 (Krizhevsky et al., 2009) and the ImageNet (Russakovsky et al., 2015) datasets while providing up to several orders of magnitude speedup during inference. Our method utilizes efficient off-the-shelf implementations from OR-Tools (Google optimization tools for combinatorial optimization problems) (OR-tools, 2018), the deep metric learning library implementation in Tensorflow (Abadi et al., 2015),\nar X\niv :1\n80 5.\n05 80\n9v 3\n[ cs\n.L G\n] 1\n2 Ju\nn 20\n18\nand is efficient to train. The state of the art deep metric learning approaches (Schroff et al., 2015; Sohn, 2016) use the class labels during training (for the hard negative mining procedure) and since our method utilizes the approaches as a component, we focus on the settings where the class labels are available during training."}, {"heading": "2. Related works", "text": "Learning the embedding representation via neural networks dates back to over two decades ago. Starting with Siamese networks (Bromley et al., 1994; Hadsell et al., 2006), the task is to learn embedding representation of the data using neural networks so that similar examples are close to each other and dissimilar examples are farther apart in the embedding space. Despite the recent successes and near human performance reported in retrieval and verification tasks (Schroff et al., 2015), most of the related literature on learning efficient representation via deep learning focus on learning the binary hamming codes for finding nearest neighbors with linear search over entire dataset per each query. Directly optimizing for the embedding representations in deep networks for quantization codes and constructing the hash tables for significant search reduction in the number of data is much less studied.\n(Xia et al., 2014) first precompute the hash code based on the labels and trains the embedding to be similar to the hash code. (Zhao et al., 2015) apply element-wise sigmoid on the embedding and minimizes the triplet loss. (Norouzi et al., 2012) optimize the upper bound on the triplet loss defined on hamming code vectors. (Liong et al., 2015) minimize the difference between the original and the signed version of the embedding with orthogonality regularizers in a network. (Li et al., 2017) employ discrete cyclic coordinate descent (Shen et al., 2015) on a discrete sub-problem optimizing one hash bit at a time but the algorithm has neither the convergence guarantees nor the bound on the number of iterations. All of these methods focus on learning the binary codes for hamming distance ranking and perform an exhaustive linear search over the entire dataset which is not likely to be suitable for large scale problems.\n(Cao et al., 2016) minimize the difference between the similarity label and the cosine distance of network embedding. (Liu & Lu, 2017) define a distance between a quantized data and continuous embedding, and back-propagates the metric loss error only with respect to the continuous embedding. Both of these methods require repeatedly running k-means clustering on the entire dataset while training the network at the same time. This is unlikely to be practical for large scale problems because of the prohibitive computational complexity and having to store the cluster centroids for all classes in the memory as the number of classes becomes extremely large (Prabhu & Varma, 2014; Choromanska et al., 2013).\nIn this paper, we propose an efficient end-to-end learning method for quantizable representations jointly optimizing the quality of the embedding representation and the performance of the corresponding hash codes in a scalable mini-batch stochastic gradient descent setting in a deep network and demonstrate state of the art search accuracy and quantitative search efficiency on multiple datasets."}, {"heading": "3. Problem formulation", "text": "Consider a hash function r(x) that maps an input data x \u2208 X onto a d dimensional binary compound hash code h \u2208 {0, 1}d with the constraint that k out of d total bits needs to be set. We parameterize the mapping as\nr(x) = argmin h\u2208{0,1}d\n\u2212f(x;\u03b8)\u1d40h\nsubject to ||h||1 = k, (1)\nwhere f(\u00b7,\u03b8) : X \u2192 Rd is a transformation (i.e. neural network) differentiable with respect to the parameter \u03b8 and takes the input x and emits the d dimensional embedding vector. Given the hash function r(\u00b7), we define a hash table H which is composed of d buckets with each bucket indexed by a compound hash code h. Then, given a query xq , union of all the items in the buckets indexed by k active bits in r(xq) is retrieved as the candidates of the approximate nearest items of xq . Finally, this is followed by a reranking operation where the retrieved items are ranked according to the distances computed using the original embedding representation f(\u00b7;\u03b8).\nNote, in quantization based hashing (Wang et al., 2016; Cao et al., 2016), a set of prototypes or cluster centroids are first computed via dictionary learning or other clustering (i.e. k-means) algorithms. Then, the function f(x;\u03b8) is represented by the indices of k-nearest prototypes or centroids. Concretely, if we replace f(x;\u03b8) in Equation (1) with the negative distances of the input item x with respect to all d prototypes or centroids, [\u2212||x\u2212 c1||2, . . . ,\u2212||x\u2212 cd||2]\u1d40, then the corresponding hash function r(x) can be used to build the hash table.\nIn contrast to most of the recent methods that learn a hamming ranking in a neural network and perform exhaustive linear search over the entire dataset (Xia et al., 2014; Zhao et al., 2015; Norouzi et al., 2012; Li et al., 2017), quantization based methods, have guaranteed search inference speed up by only considering a subset of k out d buckets and thus avoid exhaustive linear search. We explicitly maintain the sparsity constraint on the hash code in Equation (1) throughout our optimization without continuous relaxations to inherit the efficiency aspect of quantization based hashing and this is one of the key attributes of the algorithm.\nAlthough quantization based hashing is known to show high search accuracy and search efficiency (Wang et al., 2016),\nrunning the quantization procedure on the entire dataset to compute the cluster centroids is computationally very costly and requires storing all of the cluster centroids in the memory. Our desiderata are to formulate an efficient end-to-end learning method for quantizable representations which (1) guarantees the search efficiency by avoiding linear search over the entire data, (2) can be efficiently trained in a minibatch stochastic gradient descent setting and avoid having to quantize the entire dataset or having to store the cluster centroids for all classes in the memory, and (3) offers the modularity to accommodate existing embedding representation learning methods which are known to show the state of the art performance on retrieval and clustering tasks."}, {"heading": "4. Methods", "text": "We formalize our proposed method in Section 4.1 and discuss the subproblems in Section 4.2 and in Section 4.4."}, {"heading": "4.1. End-to-end learning for quantizable representations", "text": "Finding the optimal set of embedding representations and the corresponding hash codes is a chicken and egg problem. Embedding representations are required to infer which k activation dimensions to set in the corresponding binary hash code, but the binary hash codes are needed to adjust the embedding representations indexed at the activated bits so that similar items get hashed to the same buckets and vice versa. We formalize this notion in Equation (2) below.\nminimize \u03b8\nh1,...,hn `metric({f(xi;\u03b8)}ni=1;h1, . . . ,hn)\ufe38 \ufe37\ufe37 \ufe38 embedding representation quality +\n\u03b3  n\u2211 i \u2212f(xi;\u03b8)\u1d40hi + n\u2211 i \u2211 j:yj 6=yi h\u1d40iPhj  \ufe38 \ufe37\ufe37 \ufe38\nhash code performance\nsubject to hi \u2208 {0, 1}d, ||hi||1 = k, \u2200i, (2)\nwhere the matrix P encodes the pairwise cost for the hash code similarity between each negative pair and \u03b3 is the trade-off hyperparameter balancing the loss contribution between the embedding representation quality given the hash codes and the performance of the hash code with respect to the embedding representations. We solve this optimization problem via alternating minimization through iterating over solving for k-sparse binary hash codes h1, . . . ,hn and updating the parameters of the deep network \u03b8 for the continuous embedding representations per each mini-batch. Following subsections discuss these two steps in detail."}, {"heading": "4.2. Learning the compound hash code", "text": "Given a set of continuous embedding representations {f(xi;\u03b8)}ni=1, we seek to solve the following subproblem in Equation (3) where the task is to (unary) select k as large elements of the each embedding vector as possible, while (pairwise) selecting as orthogonal elements as pos-\nsible across different classes. The unary term mimics the hash function r(x) in Equation (1) and the pairwise term has the added benefit that it also provides robustness to the optimization especially during the early stages of the training when the embedding representation is not very accurate.\nminimize h1,...,hn n\u2211 i \u2212f(xi;\u03b8)\u1d40hi + n\u2211 i \u2211 j:yj 6=yi\nh\u1d40iPhj\ufe38 \ufe37\ufe37 \ufe38 := g(h1,...,n;\u03b8)\nsubject to hi \u2208 {0, 1}d, ||hi||1 = k, \u2200i, (3)\nHowever, solving for the optimal solution of the problem in Equation (3) is NP-hard in general even for the simple case where k = 1 and d > 2 (Boykov et al., 2001). Thus, we construct a upper bound function g\u0304(h1,...,n;\u03b8) to the objective function g(h1,...,n;\u03b8) which we argue that it can be exactly optimized by establishing the connection to a network flow algorithm. The upper bound function is a slightly reparameterized discrete objective where we optimize the hash codes over the average embedding vectors per each class instead. We first rewrite1 the objective function by indexing over each class and then over each data per class and derive the upper bound function as shown below.\ng(h1,...,n;\u03b8) = nc\u2211 i \u2211 k:yk=i \u2212f(xk;\u03b8)\u1d40hk + nc\u2211 i \u2211 k:yk=i, l:yl 6=i h\u1d40kPhl\n\u2264 nc\u2211 i \u2211 k:yk=i \u2212c\u1d40ihk + nc\u2211 i \u2211 k:yk=i, l:yl 6=i h\u1d40kPhl\n+ maximize h1,...,hn\nhi\u2208{0,1}d,||hi||1=k\nnc\u2211 i=1 \u2211 k:yk=i (ci \u2212 f(xk;\u03b8))\u1d40hk\n\ufe38 \ufe37\ufe37 \ufe38 :=M(\u03b8)\n= g\u0304(h1,...,n;\u03b8) (4)\nwhere nc denotes the number of classes in the mini-batch, m = |{k : yk = i}|, and ci = 1m \u2211 k:yk=i\nf(xk;\u03b8). Here, w.l.o.g we assume each class has m number of data in the mini-batch (i.e. Npairs (Sohn, 2016) mini-batch construction). The last term in upper bound, denoted as M(\u03b8), is constant with respect to the hash codes and is non-negative. Note, from the bound in Equation (4), the gap between the minimum value of g and the minimum value of g\u0304 is bounded above by M(\u03b8). Furthermore, since this value corresponds to the maximum deviation of an embedding vector from its class mean of the embedding, the bound gap decreases over iterations as we update the network parameter \u03b8 to attract similar pairs of data and vice versa for dissimilar pairs in the other embedding subproblem (more details in Section 4.4).\nMoreover, minimizing the upper bound over each hash codes {hi}ni=1 is equivalent to minimizing a reparameter-\n1We also omit the dependence of the index i for each hk and hl to avoid the notation clutter.\nization g\u0302(z1,...,nc ;\u03b8) over {zi} nc i=1 defined below because for a given class label i, each hk shares the same ci vector.\nminimize h1,...,hn\nhi\u2208{0,1}d,||hi||1=k\nnc\u2211 i \u2211 k:yk=i \u2212c\u1d40ihk + nc\u2211 i \u2211 k:yk=i, l:yl 6=i h\u1d40kPhl\n= minimize z1,...,znc\nzi\u2208{0,1}d,||zi||1=k\nm  nc\u2211 i \u2212c\u1d40i zi + nc\u2211 i \u2211 j 6=i z\u1d40iP \u2032zj  \ufe38 \ufe37\ufe37 \ufe38\n:= g\u0302(z1,...,nc ;\u03b8)\n,\nwhere P \u2032 = mP . Therefore, we formulate the following optimization problem below whose objective upper bounds the original objective in Equation (3) over all feasible hash codes {hi}ni=1.\nminimize z1,...,znc nc\u2211 i \u2212c\u1d40i zi + \u2211 i,j 6=i z\u1d40iPzj\nsubject to zi \u2208 {0, 1}d, ||zi||1 = k, \u2200i (5)\nIn the upper bound problem above, we consider the case where the pairwise cost matrix P is a diagonal matrix of non-negative values 2. Theorem 1 in the next subsection proves that finding the optimal solution of Equation (5) is equivalent to finding the minimum cost flow solution of the flow network G\u2032 illustrated in Figure 2 which can be solved efficiently and exactly in polynomial time. In practice, we use the efficient implementations from OR-Tools (Google Optimization Tools for combinatorial optimization problems) (OR-tools, 2018) to solve the minimum cost flow problem per each mini-batch."}, {"heading": "4.3. Equivalence of problem 5 to minimum cost flow", "text": "Theorem 1. The optimization problem in Equation (5) can be solved by finding the minimum cost flow solution on the flow network G\u2019.\nProof. Suppose we construct a complete bipartite graph G = (A \u222a B,E) and create a directed graph G\u2032 = (A \u222a B \u222a {s, t}, E\u2032) from G by adding source s and sink t and directing all edges E in G from A to B. We also add edges from s to each vertex ap \u2208 A. For each vertex bq \u2208 B, we add nc number of edges to t. Edges incident to s have capacity u(s, ap) = k and cost v(s, ap) = 0. The edges between ap \u2208 A and bq \u2208 B have capacity u(ap, bq) = 1 and cost v(ap, bq) = \u2212cp[q]. Each edge r \u2208 {0, . . . , nc \u2212 1} from bq \u2208 B to t has capacity u((bq, t)r) = 1 and cost v((bq, t)r) = 2\u03bbqr. Figure 2 illustrates the flow network G\u2032. The amount of flow to be sent from s to t is nck.\nThen we define the flow {fz(e)}e\u2208E\u2032 , indexed both by (1) a given configuration of z1, . . . , znc where each zi \u2208\n2Note that we absorb the scaler factor m from the definition of P \u2032 and redefine P = diag(\u03bb1, . . . , \u03bbd).\n{0, 1}d, ||zi||1 = k, \u2200i, and by (2) the edges of G\u2032, below:\n(i) fz(s, ap) = k, (ii) fz(ap, bq) = zp[q],\n(iii) fz((bq, t)r) =\n{ 1 for r < \u2211nc p=1 zp[q]\n0 otherwise (6)\nWe first show the flow fz defined above is feasible for G\u2032. The capacity constraints are satisfied by construction in Equation (6), so we only need to check the flow conservation conditions. First, the amount of input flow at s is nck and the output flow from s is \u2211 ap\u2208A fz(s, ap) = \u2211 ap\u2208A k = nck which is equal. The amount of input flow to each vertex ap \u2208 A is given as k and the output flow is\u2211\nbq\u2208B fz(ap, bq) = \u2211d q zp[q] = ||zp||1 = k.\nLet us denote the amount of input flow at a vertex bq \u2208 B as yq = \u2211nc p zp[q]. The output\nflow from the vertex bq is \u2211nc\u22121\nr=0 fz((bq, t)r) =\u2211yq\u22121 r=0 fz((bq, t)r) + \u2211nc\u22121 r=yq\nfz((bq, t)r) = yq from Equation (6) (iii). The last condition to check is that the amount of input flow at t is equal to the output flow at s.\u2211\nbq\u2208B \u2211nc\u22121 r=0 fz((bq, t)r) = \u2211d q=1 yq = \u2211 q,p zp[q] = nck. This shows the construction of the flow {fz(e)}e\u2208E\u2032 in Equation (6) is valid in G\u2032.\nNow denote {fo(e)}e\u2208E\u2032 as the minimum cost flow solution of the flow network G\u2032 which minimizes the total cost \u2211 e\u2208E\u2032 v(e)fo(e). Denote the optimal flow from a vertex ap \u2208 A to a vertex bq \u2208 B as z\u2032p[q] := fo(ap, bq). By the optimality of the flow {fo(e)}e\u2208E\u2032 , we have that \u2211 e\u2208E\u2032 v(e)fo(e) \u2264 \u2211 e\u2208E\u2032 v(e)fz(e). By\nLemma 1, the lhs of the inequality is equal to\u2211 p\u2212c\u1d40pz\u2032p + \u2211 p1 6=p2 z \u2032\u1d40 p1Pz \u2032 p2 . Also, by Lemma 2,\nthe rhs is equal to \u2211 p\u2212c\u1d40pzp + \u2211 p1 6=p2 z \u1d40 p1Pzp2 . Finally, we have that \u2211 p\u2212c\u1d40pz\u2032p + \u2211 p1 6=p2 z \u2032\u1d40 p1Pz \u2032 p2 \u2264\u2211\np\u2212c\u1d40pzp + \u2211 p1 6=p2 z \u1d40 p1Pzp2 ,\u2200{zp}. Thus, we have proved that finding the minimum cost flow solution on the flow network G\u2032 and translating the flows between each vertices between A and B as {z\u2032p}, we can find the optimal solution to the optimization problem in Equation (5).\nLemma 1. For the minimum cost flow {fo(e)}e\u2208E\u2032 of the network G\u2032, we have that the total cost is\u2211\ne\u2208E\u2032 v(e)fo(e) = \u2211 p\u2212c\u1d40pz\u2032p + \u2211 p1 6=p2 z \u2032\u1d40 p1Pz \u2032 p2 .\nProof. The total minimum cost \u2211\ne\u2208E\u2032 v(e)fo(e) is broken down as\n\u2211 e\u2208E\u2032 v(e)fo(e) = \u2211 ap\u2208A\nv(s, ap)fo(s, ap)\ufe38 \ufe37\ufe37 \ufe38 flow from s to A +\n\u2211 ap\u2208A \u2211 bq\u2208B\nv(ap, bq)fo(ap, bq)\ufe38 \ufe37\ufe37 \ufe38 flow from A to B\n+ \u2211 bq\u2208B nc\u22121\u2211 r=0\nv((bq, t)r)fo((bq, t)r)\ufe38 \ufe37\ufe37 \ufe38 flow from B to t\nDenote the amount of input flow at a vertex bq given the minimum cost flow {fo(e)}e\u2208E\u2032 as y\u2032q = \u2211 p fo(ap, bq) =\u2211nc\np z \u2032 p[q]. From the cost definition at the edges between bq and t, v((bq, t)r) = 2\u03bbqr, and by the optimality of the minimum cost flow, we have that fo((bq, t)r) = 1 \u2200r < y\u2032q and fo((bq, t)r) = 0 \u2200r \u2265 y\u2032q . Thus, the total cost is\u2211 e\u2208E\u2032 v(e)fo(e) = 0 + nc\u2211 p d\u2211 q \u2212cp[q]z\u2032p[q] + \u2211 bq\u2208B y\u2032q\u22121\u2211 r=0 2\u03bbqr\n= \u2211 p \u2212c\u1d40pz\u2032p + \u2211 q \u03bbqy \u2032 q(y \u2032 q \u2212 1)\n= \u2211 p \u2212c\u1d40pz\u2032p + \u2211 q \u03bbqy \u2032 q 2 \u2212 \u2211 p \u2211 q \u03bbqz \u2032 p[q]\n= \u2211 p \u2212c\u1d40pz\u2032p + \u2211 p z\u2032p \u1d40 P \u2211 p z\u2032p \u2212 \u2211 p z\u2032\u1d40p Pz \u2032 p\n= \u2211 p \u2212c\u1d40pz\u2032p + \u2211\np1 6=p2\nz\u2032\u1d40p1Pz \u2032 p2 (7)\nLemma 2. For the {fz(e)}e\u2208E\u2032 defined as Equation (6) of the network G\u2032, we have that the total cost is\u2211\ne\u2208E\u2032 v(e)fz(e) = \u2211 p\u2212c\u1d40pzp + \u2211 p1 6=p2 z \u1d40 p1Pzp2 .\nProof. The proof is similar to Lemma 1 except that we use the definition of the flow {fz(e)}e\u2208E\u2032 in Equation (6) (iii) to reduce the cost of the flow from B to t to \u2211y\u2032q\u22121 r=0 2\u03bbqr.\nTime complexity For \u03bbq nc, note that the worst case time complexity of finding the minimum cost flow (MCF) solution in the network G\u2032\nis O ( (nc + d) 2 ncd log (nc + d) ) (Goldberg & Tarjan,\n1990). In practice, however, it has been shown that implementation heuristics such as price updates, price refinement, push-look-ahead, (Goldberg, 1997) and set-relabel (Bu\u0308nnagel et al., 1998) methods drastically improve the reallife performance. Also, we emphasize again that we solve the minimum cost flow problem only within the mini-batch not on the entire dataset. We benchmarked the wall clock running time of the method at varying sizes of nc and d and observed approximately linear time complexity in nc and d. Figure 1 shows the benchmark wall clock run time results."}, {"heading": "4.4. Learning the embedding", "text": "As the hash codes become more and more sparse, it becomes increasingly likely for hamming distances defined on binary codes (Norouzi et al., 2012; Zhao et al., 2015) to become zero regardless of whether the input pair of data is similar or dissimilar. This phenomenon can be problematic when trained in a deep network because the back-propagation gradient would become zero and thus the embedding representations would not be updated at all. In this regard, we propose a distance function based on gated residual as shown in Equation (8). This parameterization outputs zero distance only if the embedding representations of the two input data are identical at all the hash code activations. Concretely, given a pair of embedding vectors f(xi;\u03b8), f(xj ;\u03b8) and the corresponding binary k-sparse hash codes hi,hj ,\nwe define the following distance function d hashij between the embedding vectors\nd hashij = || (hi \u2228 hj) (f(xi;\u03b8)\u2212 f(xj ;\u03b8)) ||1, (8)\nwhere \u2228 denotes the logical or operation of the two binary hash codes and denotes the element-wise multiplication. Then, using the distance function above, we can define the following subproblems using any existing deep metric learning methods (Weinberger et al., 2006; Schroff et al., 2015; Song et al., 2016; Sohn, 2016; Song et al., 2017). Given a set of binary hash codes {hi}ni=1, we seek to solve the following subproblems where the task is to optimize the embedding representations so that similar pairs of data get hashed to the same buckets and dissimilar pairs of data get hashed to different buckets. In other words, we need similar pairs of data to have similar embedding representations indexed at the activated hash code dimensions and vice versa. In terms of the hash code optimization in Equation (4), updating the network weight has the effect of tightening the bound gap M(\u03b8).\nEquation (9) and Equation (10) show the subproblems defined on the distance function above using Triplet (Schroff et al., 2015) and Npairs (Sohn, 2016) method respectively. We optimize these embedding subproblems by updating the network parameter \u03b8 via stochastic gradient descent using the subgradients \u2202`metric(\u03b8;h1,...,n)\u2202\u03b8 given the hash codes per each mini-batch.\nminimize \u03b8\n1 |T | \u2211\n(i,j,k)\u2208T [d hashij + \u03b1\u2212 d hashik ]+\ufe38 \ufe37\ufe37 \ufe38 `triplet(\u03b8; h1,...,n)\nsubject to ||f(x;\u03b8)||2 = 1, (9)\nwhere T = {(xi,x+i ,x \u2212 i )}i is the set of triplets (Schroff\net al., 2015), and the embedding vectors are normalized onto unit hypersphere ||f(x;\u03b8)||2 = 1, \u2200x \u2208 X . We also apply the semi-hard negative mining procedure (Schroff et al., 2015) where hard negatives farther than the distance between the anchor and positives are mined within the mini-batch. In practice, since our method can be applied to any deep metric learning methods, we use existing deep metric learning implementations available in tf.contrib.losses.metric learning. Similarly, we could also employ npairs (Sohn, 2016) method,\nminimize \u03b8 \u22121 |P| \u2211 (i,j)\u2208P log exp ( \u2212d hashij ) exp ( \u2212d hashij ) + \u2211 k:yk 6=yi exp ( \u2212d hashik ) \ufe38 \ufe37\ufe37 \ufe38\n`npairs(\u03b8; h1,...,n)\n+ \u03bb\nm \u2211 i ||f(xi;\u03b8)||22, (10)\nwhere the npairs mini-batch B is constructed with positive pairs (x,x+) which are negative with respect to all\nother pairs. B = {(x1,x+1 ), . . . , (xn,x+n ))} and P denotes the set of all positive pairs within the mini-batch. We use the existing implementation of npairs loss in Tensorflow as well. Note that even though the distance d hashij is defined after masking the embeddings with the union binary vector (hi \u2228 hj), it\u2019s important to normalize or regularize the embedding representation before the masking operations for the optimization stability due to the sparse nature of the hash codes.\nAlgorithm 1 Learning algorithm input \u03b8embb (pretrained metric learning base model); \u03b8d \u2208 Rd initialize \u03b8f = [\u03b8b,\u03b8d] 1: for t = 1, . . . , MAXITER do 2: Sample a minibatch {xj} 3: Update the flow networkG\u2032 by recomputing the cost vectors\nfor all classes in the minibatch ci =\n1 m \u2211 k:yk=i f(xk;\u03b8f )\n4: Compute the hash codes {hi} minimizing Equation (5) via finding the minimum cost flow on G\u2032 5: Update the network parameter given the hash codes \u03b8f \u2190 \u03b8f \u2212 \u03b7(t)\u2202`metric(\u03b8f ; h1,...,nc)/\u2202\u03b8f 6: Update stepsize \u03b7(t) \u2190 ADAM rule (Kingma & Ba, 2014) 7: end for\noutput \u03b8f (final estimate);"}, {"heading": "4.5. Query efficiency analysis", "text": "In this subsection, we examine the expectation and the variance of the query time speed up over linear search. Recall the properties of the compound hash code defined in Section 3, h \u2208 {0, 1}d and ||h||1 = k. Given n such hash codes, we have that Pr(h\u1d40i hj = 0) = ( d\u2212k k ) / ( d k\n) assuming the hash code uniformly distributes the items throughout different buckets. For a given hash code hq, the number of retrieved data is Nq = \u2211 i 6=q 1(h \u1d40 i hq 6= 0). Then, the expected number of retrieved data is E[Nq] = (n\u22121) ( 1\u2212 ( d\u2212k k ) / ( d k )) . Thus, in contrast to linear search, the expected speedup factor (SUF) under perfectly uniform distribution of the hash code is\nE[SUF] = ( 1\u2212 ( d\u2212k k )( d k ) )\u22121 (11) In the case where d k, the speedup factor approaches(\nd k2 ) . Similarly, we have that the variance is V [Nq] =\n(n\u2212 1) ( 1\u2212 ( d\u2212k k ) / ( d k ))( d\u2212k k ) / ( d k ) ."}, {"heading": "5. Implementation details", "text": "Network architecture In our experiments, we used the NIN (Lin et al., 2013) architecture (denote the parameters as \u03b8b) with leaky relu (Xu et al., 2015) with \u03b1 = 5.5 as activation function and trained Triplet embedding network with semi-hard negative mining (Schroff et al., 2015) and\nNpairs network (Sohn, 2016) from scratch as the base model. We snapshot the network weights (\u03b8embb ) of the learned base model. Then we replace the last layer in (\u03b8embb ) with a randomly initialized d dimensional fully connected projection layer (\u03b8d) and finetune the hash network (denote the parameters as \u03b8f = [\u03b8b,\u03b8d]). Algorithm 1 summarizes the training procedure in detail.\nHash table construction and query We use the learned hash network \u03b8f and apply Equation (1) to convert a hash data xi into the hash code h(xi;\u03b8f ) and use the base embedding network \u03b8embb to convert the data into the embedding representation f(xi;\u03b8embb ). Then, the embedding representation is hashed to buckets corresponding to the k set bits in the hash code. We use the similar procedure and convert a query data xq into the hash code h(xq;\u03b8f ) and into the embedding representation f(xq;\u03b8embb ). Once we retrieve the union of all bucket items indexed at the k set bits in the hash code, we apply a reranking procedure (Wang et al., 2016) based on the euclidean distance in the embedding representation space.\nEvaluation metrics We report our accuracy results using precision@k (Pr@k) and normalized mutual information (NMI) (Manning et al., 2008) metrics. Precision@k is computed based on the reranked ordering (described above) of the retrieved items from the hash table. We evaluate NMI, when the code sparsity is set to k = 1, treating each bucket as individual clusters. In this setting, NMI becomes perfect, if each bucket has perfect class purity (pathologically putting one item per each bucket is prevented by construction since d n). We report the speedup results by comparing the number of retrieved items versus the total number of data (exhaustive linear search) and denote this metric as SUF. As the hash code becomes uniformly distributed, SUF metric approaches the theoretical expected speedup in Equation (11). Figure 3 shows that the measured SUF of our method closely follows the theoretical upper bound in contrast to other methods."}, {"heading": "6. Experiments", "text": "We report our results on Cifar-100 (Krizhevsky et al., 2009) and ImageNet (Russakovsky et al., 2015) datasets and compare the accuracy against several baseline methods. First baseline methods are the state of the art deep metric learning models (Schroff et al., 2015; Sohn, 2016) performing an exhaustive linear search over the whole dataset given a query data. Another baselines are the Binarization transform (Agrawal et al., 2014; Zhai et al., 2017) methods where the dimensions of the hash code corresponding to the top k dimensions of the embedding representation are set. We also perform vector quantization (Wang et al., 2016) on the learned embedding representation from the deep metric learning methods above on the entire dataset and compute the hash code based on the indices of the k nearest centroids.\n\u2018Triplet\u2019 and \u2018Npairs\u2019 denotes the deep metric learning base models performing an exhaust linear search per each query. \u2018Th\u2019 denotes the binarization transform baseline, \u2018VQ\u2019 denotes the vector quantization baseline."}, {"heading": "6.1. Cifar-100", "text": "Cifar-100 (Krizhevsky et al., 2009) dataset has 100 classes. Each class has 500 images for train and 100 images for test. Given a query image from test, we experiment the search performance both when the hash table is constructed from train and from test. We subtract the per-pixel mean of training images across all the images and augmented the dataset by zero-padding 4 pixels on each side, randomly cropping 32\u00d7 32, and applying random horizontal flipping. The batch size is set to 128. The metric learning base model is trained for 175k iterations, and learning rate decays to 0.1 of previous learning rate after 100k iterations. We finetune the base model for 70k iterations and decayed the learning rate to 0.1 of previous learning rate after 40k iterations. Table 1 show results using the triplet network with d=256 and Table 2 show results using the npairs network with d= 64. The results show that our method not only outperforms search accuracies of the state of the art deep metric learning base models but also provides up to 98\u00d7 speed up over exhaustive search."}, {"heading": "6.2. ImageNet", "text": "ImageNet ILSVRC-2012 (Russakovsky et al., 2015) dataset has 1, 000 classes and comes with train (1, 281, 167 images) and val set (50, 000 images). We use the first nine splits of train set to train our model, the last split of train set for validation, and use validation dataset to test the query performance. We use the images downsampled to 32\u00d7 32 from (Chrabaszcz et al., 2017). Preprocessing step is identical with cifar-100 and we used the pixel mean provided in the dataset. The batch size for the metric learning base model is set to 512 and is trained for 450k iterations, and learning rate decays to 0.3 of previous learning rate after each 200k iterations. When we finetune npairs base model for d=512, we set the batch size to 1024 and total iterations to 35k with decaying the learning rate to 0.3 of previous learning rate after each 15k iterations. When we finetune the triplet base model for d=256, we set the batch size to 512 and total iterations to 70k with decaying the learning rate to 0.3 of previous learning rate after each 30k iterations. Our results in Table 3 and Table 4 show that our method outperforms the state of the art deep metric learning base models in search accuracy while providing up to 478\u00d7 speed up over exhaustive linear search. Table 5 compares the NMI metric and shows that the hash table constructed from our representation yields buckets with significantly better class purity on both datasets and on both methods."}, {"heading": "7. Conclusion", "text": "We have presented a novel end-to-end optimization algorithm for jointly learning a quantizable embedding representation and the sparse binary hash code which then can be used to construct a hash table for efficient inference. We also show an interesting connection between finding the optimal sparse binary hash code and solving a minimum cost flow problem. Our experiments show that the proposed algorithm not only achieves the state of the art search accuracy outperforming the previous state of the art deep metric learning approaches (Schroff et al., 2015; Sohn, 2016) but also provides up to 98\u00d7 and 478\u00d7 search speedup on Cifar-100 and ImageNet datasets respectively."}, {"heading": "Acknowledgements", "text": "We would like to thank Zhen Li at Google Research for helpful discussions and anonymous reviewers for their constructive comments. This work was partially supported by Kakao, Kakao Brain and Basic Science Research Program through the National Research Foundation of Korea (NRF) (2017R1E1A1A01077431). Hyun Oh Song is the corresponding author."}], "year": 2018, "references": [{"title": "Analyzing the performance of multilayer neural networks for object recognition", "authors": ["P. Agrawal", "R. Girshick", "J. Malik"], "venue": "In ECCV,", "year": 2014}, {"title": "Learning visual similarity for product design with convolutional neural networks", "authors": ["S. Bell", "K. Bala"], "venue": "In SIGGRAPH,", "year": 2015}, {"title": "Fast approximate energy minimization via graph cuts", "authors": ["Y. Boykov", "O. Veksler", "R. Zabih"], "venue": "IEEE Transactions on pattern analysis and machine intelligence,", "year": 2001}, {"title": "Signature verification using a \u201dsiamese\u201d time delay neural network", "authors": ["J. Bromley", "I. Guyon", "Y. Lecun", "E. Sackinger", "R. Shah"], "venue": "In NIPS,", "year": 1994}, {"title": "Efficient implementation of the goldberg\u2013tarjan minimum-cost flow algorithm", "authors": ["U. B\u00fcnnagel", "B. Korte", "J. Vygen"], "venue": "Optimization Methods and Software,", "year": 1998}, {"title": "Deep quantization network for efficient image retrieval", "authors": ["Y. Cao", "M. Long", "J. Wang", "H. Zhu", "Q. Wen"], "venue": "In AAAI,", "year": 2016}, {"title": "Extreme multi class classification", "authors": ["A. Choromanska", "A. Agarwal", "J. Langford"], "venue": "In NIPS,", "year": 2013}, {"title": "A downsampled variant of imagenet as an alternative to the cifar datasets", "authors": ["P. Chrabaszcz", "I. Loshchilov", "F. Hutter"], "venue": "arXiv preprint arXiv:1707.08819,", "year": 2017}, {"title": "An efficient implementation of a scaling minimum-cost flow algorithm", "authors": ["A.V. Goldberg"], "venue": "Journal of algorithms,", "year": 1997}, {"title": "Finding minimum-cost circulations by successive approximation", "authors": ["A.V. Goldberg", "R.E. Tarjan"], "venue": "Mathematics of Operations Research,", "year": 1990}, {"title": "Dimensionality reduction by learning an invariant mapping", "authors": ["R. Hadsell", "S. Chopra", "Y. Lecun"], "venue": "In CVPR,", "year": 2006}, {"title": "Adam: A method for stochastic optimization", "authors": ["D.P. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "year": 2014}, {"title": "Cifar-100 (canadian institute for advanced research)", "authors": ["A. Krizhevsky", "V. Nair", "G. Hinton"], "year": 2009}, {"title": "Deep hashing for compact binary codes learning", "authors": ["V.E. Liong", "J. Lu", "G. Wang", "P. Moulin", "J. Zhou"], "venue": "In CVPR,", "year": 2015}, {"title": "Learning deep representations with diode loss for quantization-based similarity search", "authors": ["S. Liu", "H. Lu"], "venue": "In IJCNN,", "year": 2017}, {"title": "Introduction to Information Retrieval", "authors": ["C.D. Manning", "P. Raghavan", "H. Schutze"], "venue": "Cambridge university press,", "year": 2008}, {"title": "Hamming distance metric learning", "authors": ["M. Norouzi", "D.J. Fleet", "R.R. Salakhutdinov"], "venue": "In NIPS,", "year": 2012}, {"title": "Fastxml: A fast, accurate and stable tree-classifier for extreme multi-label learning", "authors": ["Y. Prabhu", "M. Varma"], "venue": "In SIGKDD,", "year": 2014}, {"title": "Facenet: A unified embedding for face recognition and clustering", "authors": ["F. Schroff", "D. Kalenichenko", "J. Philbin"], "venue": "In CVPR,", "year": 2015}, {"title": "Learning transferrable representations for unsupervised domain adaptation", "authors": ["O. Sener", "H.O. Song", "A. Saxena", "S. Savarese"], "venue": "In NIPS,", "year": 2016}, {"title": "Supervised discrete hashing", "authors": ["F. Shen", "C. Shen", "W. Liu", "H.T. Shen"], "venue": "In CVPR,", "year": 2015}, {"title": "Improved deep metric learning with multi-class n-pair loss objective", "authors": ["K. Sohn"], "venue": "In NIPS,", "year": 2016}, {"title": "Deep metric learning via lifted structured feature embedding", "authors": ["H.O. Song", "Y. Xiang", "S. Jegelka", "S. Savarese"], "year": 2016}, {"title": "Deep metric learning via facility location", "authors": ["H.O. Song", "S. Jegelka", "V. Rathod", "K. Murphy"], "year": 2017}, {"title": "A survey on learning to hash", "authors": ["J. Wang", "T. Zhang", "J. Song", "N. Sebe", "H.T. Shen"], "venue": "arXiv preprint arXiv:1606.00185,", "year": 2016}, {"title": "Unsupervised learning of visual representations using videos", "authors": ["X. Wang", "A. Gupta"], "venue": "In ICCV,", "year": 2015}, {"title": "Distance metric learning for large margin nearest neighbor classification", "authors": ["K.Q. Weinberger", "J. Blitzer", "L.K. Saul"], "venue": "In NIPS,", "year": 2006}, {"title": "Supervised hashing for image retrieval via image representation learning", "authors": ["R. Xia", "Y. Pan", "H. Lai", "C. Liu", "S. Yan"], "venue": "In AAAI,", "year": 2014}, {"title": "Empirical evaluation of rectified activations in convolutional network", "authors": ["B. Xu", "N. Wang", "T. Chen", "M. Li"], "venue": "arXiv preprint arXiv:1505.00853,", "year": 2015}, {"title": "Visual discovery at pinterest", "authors": ["A. Zhai", "D. Kislyuk", "Y. Jing", "M. Feng", "E. Tzeng", "J. Donahue", "Y.L. Du", "T. Darrell"], "venue": "In Proceedings of the 26th International Conference on World Wide Web Companion,", "year": 2017}, {"title": "Deep semantic ranking based hashing for multi-label image retrieval", "authors": ["F. Zhao", "Y. Huang", "L. Wang", "T. Tan"], "venue": "In CVPR,", "year": 2015}], "id": "SP:a0f7e0582ad5709a1f84304ebe006b03b2f65c16", "authors": [{"name": "Yeonwoo Jeong", "affiliations": []}, {"name": "Hyun Oh Song", "affiliations": []}, {"name": "Hyun Oh", "affiliations": []}], "abstractText": "Embedding representation learning via neural networks is at the core foundation of modern similarity based search. While much effort has been put in developing algorithms for learning binary hamming code representations for search efficiency, this still requires a linear scan of the entire dataset per each query and trades off the search accuracy through binarization. To this end, we consider the problem of directly learning a quantizable embedding representation and the sparse binary hash code end-to-end which can be used to construct an efficient hash table not only providing significant search reduction in the number of data but also achieving the state of the art search accuracy outperforming previous state of the art deep metric learning methods. We also show that finding the optimal sparse binary hash code in a mini-batch can be computed exactly in polynomial time by solving a minimum cost flow problem. Our results on Cifar-100 and on ImageNet datasets show the state of the art search accuracy in precision@k and NMI metrics while providing up to 98\u00d7 and 478\u00d7 search speedup respectively over exhaustive linear search. The source code is available at https://github.com/maestrojeong/Deep-HashTable-ICML18.", "title": "Efficient end-to-end learning for quantizable representations"}