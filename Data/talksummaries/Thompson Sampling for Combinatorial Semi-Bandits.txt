0	34	Multi-armed bandit (MAB) (Berry & Fristedt, 1985; Sutton & Barto, 1998) is a classical online learning model typically described as a game between a learning agent (player) and the environment withm arms.
1	37	In each step, the environment generates an outcome, and the player uses a policy (or an algorithm), which takes the feedback from the previous steps as input, to select an arm to pull.
2	10	After pulling an arm, the player receives a reward based on the pulled arm and the environment outcome.
3	12	In this paper, we consider stochastic MAB problem, which means the environment outcome is drawn from an unknown distribution (Lai & Robbins, 1985), not generated by an adversary (Auer et al., 2002b).
9	22	In CMAB, the environment contains m base arms, but the player needs to pull a set of base arms S in each time slot, where S is called a super arm (or an action).
11	24	In this paper, we consider the semi-bandit setting, where the feedback includes the outcomes of all base arms in the played super arm, and the reward is a function of S and the observed outcomes of arms in S. CMAB has found applications in many areas such as wireless networking, social networks, online advertising, etc.
13	14	An alternative approach different from UCB is the Thompson sampling (TS) approach, which is introduced much earlier by Thompson (1933), but the theoretical analysis of the TS policy comes much later — Kaufmann et al. (2012) and Agrawal & Goyal (2012) give the first regret bound for the TS policy, which essentially matches the UCB policy theoretically.
16	16	The unknown distribution of environment outcomes is parameterized, with an assumed prior distribution.
17	8	TS updates the prior distribution in each step with two phases: first it uses the prior distribution to sample a parameter, which is used to determine the action to play in the current step; second it uses the feedback obtained in the current step to update the prior distribution to posterior distribution according to the Bayes’ rule.
18	34	To avoid confusion on these two kinds of random variables, in the rest of this paper, we use the word “sample” to denote the variable in the first phase, i.e. the random variable coming from the prior distribution.
19	25	The word “observation” represents the feedback random variable, which follows the unknown environment distribution.
20	8	In this paper, we study the application of the Thompson sampling approach to CMAB.
21	12	The reason that we are interested in this approach is that it has good performance in experiments.
28	7	We consider a general CMAB case similar with (Chen et al., 2016b), i.e. we assume that (a) the problem instance satisfies a Lipschitz continuity assumption to handle non-linear reward functions, and (b) the player has access to an exact oracle for the offline optimization problem.
37	31	We show that CTS achieves O( ∑ i∈[m] log T/∆i,min) + O((2/ε)2k ∗ ) distribution-dependent regret bound for some small ε, where ∆i,min is the minimum gap between the optimal expected reward and any non-optimal expected reward containing arm i, and k∗ is the size of the optimal solution.
38	19	This is the first distribution-dependent regret bound for general CMAB using TS-based policy, and the result matches the theoretical performance of the UCB-based solution CUCB in (Chen et al., 2016b).
39	16	When considering CMAB with linear reward functions, the other complexity factors in the leading log T term also matches the regret lower bound for linear CMAB in (Kveton et al., 2015a).
40	13	For the exponential constant term, we show an example that it is unavoidable for Thompson sampling.
41	34	Comparing to the UCB-based solution in (Chen et al., 2016b), the advantages of CTS is that: a) we do not need to assume that the expected reward is monotone to the mean outcomes of the base arms; b) it has better behaviour in experiments.
42	13	CTS also suffers from some disadvantages.
43	35	For example, CTS policy can not adapt an approximation oracle as in (Chen et al., 2016b) (the regret becomes to approximate regret as well).
45	23	To show this, we provide a counter example for origin MAB problem, which cause an approximate regret of Θ(T ) when using TS policy.
46	24	Another disadvantage is that we need to assume that all the outcomes of all base arms are mutually independent.
48	12	Only when the distributions are independent, we can use a simple method to update those prior distributions; otherwise the update method will be much more complicated for both the implmentation and the analysis.
49	9	This assumption is still reasonable, since many real applications satisfy this assumption.
50	20	However, when applying on some further combinatorial structures, we do not need such an assumption, such as in matroid bandit.
51	7	Matroid bandit is a special class of CMAB (Kveton et al., 2014), in which the base arms are the elements in the ground set and the super arms are the independent sets of a matroid.
52	8	The reward function is the sum of all outcomes of the base arms in the super arm.
53	10	We show that the regret of CTS is upper bounded by O( ∑ i 6∈S∗ log T/∆i) +O(m/ε 4) for some small ε, where S∗ is an optimal solution and ∆i is the minimum positive gap between the mean outcome of any arms in S∗ and the mean outcome of arm i.
56	17	We further conduct empirical simulations, and show that CTS performs much better than the CUCB algorithm of (Chen et al., 2016b) and C-KL-UCB algorithm based on KL-UCB of (Garivier & Cappé, 2011) on both matroid and non-matroid CMAB problem instances.
