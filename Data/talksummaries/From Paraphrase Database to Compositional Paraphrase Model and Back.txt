1	19	It is useful for a variety of NLP tasks like question answering (Rinaldi et al., 2003; Fader et al., 2013), semantic parsing (Berant and Liang, 2014), textual entailment (Bosma and CallisonBurch, 2007), and machine translation (Marton et al., 2009).
2	17	One component of many such systems is a paraphrase table containing pairs of text snippets, usually automatically generated, that have the same meaning.
4	20	The PPDB is a massive resource, containing 220 million paraphrase pairs.
29	22	In summary, we make the following contributions: Provide new PARAGRAM word vectors, learned using PPDB, that achieve state-of-the-art performance on the SimLex-999 lexical similarity task (Hill et al., 2014b) and lead to improved performance in sentiment analysis.
30	18	Provide ways to use PPDB to embed phrases.
57	37	We created two novel datasets: (1) AnnotatedPPDB, a subset of phrase pairs from PPDB which are annotated according to how strongly they represent a paraphrase relationship, and (2) MLParaphrase, a re-annotation of the bigram similarity dataset from Mitchell and Lapata (2010), again annotated for strength of paraphrase relationship.
58	41	Our motivation for creating Annotated-PPDB was to establish a way to evaluate compositional paraphrase models on short phrases.
94	33	For instance, television set and television programme were the highest rated phrases in the NN section (based on average annotator score).
95	75	Similarly, one of the highest ranked JN pairs was older man and elderly woman.
97	21	Therefore, we had the data re-annotated by two authors of this paper who are native English speakers.8 The bigrams were labeled on a scale from 1- 5 where 5 denotes phrases that are equivalent in a large number of contexts, 3 indicates the phrases are roughly equivalent in a narrow set of contexts, and 1 means the phrases are not at all equivalent in any context.
101	30	In fact, when evaluating our NN annotations against those from the original ML data (column 4), we find ρ to be 0.38, well below the average human correlation of 0.49 (final column) reported by Mitchell and Lapata and also surpassed by pointwise multiplication (Mitchell and Lapata, 2010).
105	20	We use a recursive neural network (RNN) similar to that used by Socher et al. (2014).
117	25	The intuition for this objective is that we want the two phrases to be more similar to each other (g(x1) · g(x2)) than either is to their respective negative examples t1 and t2, by a margin of at least δ.
123	16	We also tried a strategy in which we selected the least similar phrase that would trigger an update (i.e., g(ti) ·g(xi) > g(x1) ·g(x2)−δ), but we found the simpler strategy above to work better and used it for all experiments reported below.
135	21	This objective function is: min Ww 1 |X| ( ∑ 〈x1,x2〉∈X max(0, δ −W (x1)w ·W (x2)w +W (x1)w ·W (t1)w ) + max(0, δ −W (x1)w ·W (x2)w + W (x2)w ·W (t2)w ) ) + λWw ‖Wwinitial −Ww‖2 (2) It is like Eq.
148	17	The vectors were trained on English Wikipedia (tokenized and lowercased, yielding 1.8B tokens).9 We used a window size of 5 and a minimum count cut-off of 60, producing vectors for approximately 270K word types.
151	21	For training, we extracted word pairs from the lexical XL section of PPDB.
154	31	There are a total of 548,085 pairs.
188	21	We see improvements over the baselines when using PARAGRAM vectors, even exceeding the performance of higherdimensional skip-gram vectors.
190	17	We start with the simplest case of bigrams, and then proceed to short phrases.
191	75	For all tasks, we again train on appropriate data from PPDB and test on various evaluation datasets, including our two novel datasets (Annotated-PPDB and ML-Paraphrase).
194	29	We fixed the initial learning rates to 0.5 for the word embeddings and 0.05 for the composition parameters, and the margin to 1.
195	16	Then we did a coarse grid search over a parameter space for λWw , λW , and mini-batch size.
218	42	We also compare to several results from prior work.
228	71	We also outperform the strong baseline of adding 1000-dimensional skip-gram embeddings, a model with 40 times the number of parameters, on our MLParaphrase dataset.
229	55	This baseline had correlations of 0.45, 0.43, and 0.47 on the JN, NN, and VN partitions, with an average of 0.45—below the average ρ of the RNN (0.52) and even the {PARAGRAM, +} model (0.46).
232	20	In this section we show that by training a model based on filtered phrase pairs in PPDB, we can actually distinguish between quality paraphrases and poor paraphrases in PPDB better than the original heuristic scoring scheme from Ganitkevitch et al. (2013).
250	19	We again consider addition of 1000-dimensional skip-gram embeddings as a baseline, and they continue to perform strongly (ρ = 0.37).
255	126	Clearly, initialization is important when optimizing non-convex objectives like ours, but it is noteworthy that our best results came from first improving the word vectors and then learning the composition model, rather than jointly learning both from scratch.
260	19	The RNN performs better (has lower average absolute error) for less similar pairs.
