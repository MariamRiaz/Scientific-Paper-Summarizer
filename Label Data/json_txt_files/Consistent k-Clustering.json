{"sections": [{"heading": "1. Introduction", "text": "Competitive analysis of online algorithms has been an area of spirited research with beautiful results over the past two decades. At its heart, this area is about decision making under uncertainty about the future\u2014the input is revealed in an online manner, and at every point in time the algorithm must make an irrevocable choice. A standard example is that of caching algorithms\u2014at every time step the algorithm must make a choice about which elements to keep in the cache, and which elements to evict (Fiat et al., 1991). The generalization of caching to metric spaces is encapsu-\n*Equal contribution 1Google, Zurich, Switzerland 2Google, New York, New York, USA. Correspondence to: Silvio Lattanzi <silviol@google.com>, Sergei Vassilvitskii <sergeiv@google.com>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nlated in the k-server problem, which has been the subject of intense study (Bansal et al., 2015; Manasse et al., 1990).\nThe key metric in online algorithms is the competitive ratio. It measures the quality of the solution obtained by an online algorithm versus an offline optimum, which has the luxury of seeing the whole input before making any decisions. In situations where the competitive ratio is relatively small, for example, the list update problem (Sleator & Tarjan, 1985), this is a great measure by which we can compare different algorithms. However, in some scenarios strong lower bounds on the competitive ratio imply that any algorithm that makes irrevocable choices will necessarily perform poorly when compared to an offline optimum.\nOnline clustering is one such example. In this setting points x1, x2, . . . arrive one at a time, and must be instantly given one of k cluster labels. As is typical, the goal is to have the highest quality clustering (under some pre-specified objective function, like k-CENTER or k-MEDIAN) at every point in time. As Liberty et al. (2016) showed, not only do online clustering algorithms have an unbounded competitive ratio, but one must use bi-criteria approximations to have any hope of a constant approximate solution.\nAnother approach to evade strong lower bounds is to make additional assumptions about the input to the problem. For example, one may assume that the input comes in a random (or partially random) order. This assumption has been a fruitful avenue when studying online problems in different contexts, as the classic secretary problem (Ferguson, 1989; Kesselheim et al., 2015; Kleinberg, 2005) or matching (Karp et al., 1990; Mahdian & Yan, 2011). Another alternative is to assume some additional structure on the distribution that points are coming from (Feldman et al., 2009). A big downside of both of these assumptions is that they are hard to test and validate in practice, which is why we take a different approach in this work."}, {"heading": "1.1. Consistency", "text": "While the irrevocability of past choices makes sense from a theoretical standpoint, for some practical problems this requirement is unrealistically draconian. For example, consider a load balancer, which, when faced with requests arriving online, assigns them to different machines. Better cache performance dictates that similar requests should be\nassigned to the same machine, thus the load balancer is essentially performing online clustering. However, fundamentally, nothing is preventing the load balancer from reassigning some of the past jobs to other machines. In this situation, a re-clustering\u2014a reassignment of jobs to machines to increase performance\u2014is not an impossible operation.\nAnother common example of a costly, but not prohibitive recomputation comes from standard applications of unsupervised clustering: feature engineering for large scale machine learned systems. In this setting a feature vector x, is augmented with the id of a cluster it falls in, x\u2032, and the full vector (x, x\u2032) is given as input to the learner. This is mainly done to introduce expressiveness and non-linearity to simple systems. In this situation, changing the clustering would entail changing the set of features passed to the learner, and retraining the whole system; thus one certainly does not want to do it at every time step, but it can be done if the gains are worthwhile.\nFrom a theoretical perspective, the ability to correct for past mistakes offers the ability for much better solutions. In particular for clustering problems, it avoids the lower bounds introduced by Liberty et al. (2016). As we will show, the option to recluster dramatically improves the quality of the solution, even if it is taken rarely. More formally, we will introduce a parameter \u03b2 which controls the number of times the solution changes. Setting \u03b2 = 0 is equivalent to online algorithms, whereas a large value of \u03b2 is equivalent to recomputing the answer from scratch at every time step."}, {"heading": "1.2. Our Contributions", "text": "In this paper we focus on exploring the trade-off between the approximation ratio of clustering algorithms, and the number of times we must recompute the results.\nWe begin by formally defining the notion of (\u03b1, \u03b2)consistent clustering in Section 3. Then we prove a lower bound, showing that any constant competitive algorithm must change its cluster centers at least \u2126(k log n) times (Section 3.1). Then we show that a known algorithm by Charikar et al. (2004) achieves this bound for the kCENTER problem, and we develop a new algorithm for other clustering objectives, and show that it requires at most O(k2 log4 n) reclusterings, an exponential improvement over the naive solution (Section 5). Finally, we show that the proposed algorithms perform well on real world datasets (Section 7)."}, {"heading": "1.3. Related Work", "text": "There are two avenues for related work that we build on in this paper. The first is clustering algorithms, particularly the online clustering variants. In their seminal work Charikar et al. (2004) gave algorithms for the k-CENTER\nproblem. The case of k-MEDIAN and k-MEANS proved more complex. For the former, Meyerson (2001) gave an O(log n) competitive ration for closely related online facility location problem. This result was further improved by Fotakis (2008) and Anagnostopoulos et al. (2004). The latter was recently studied by Liberty et al. (2016) who gave bicriteria approximations and showed that these are necessary in an online setting. For the soft partition version of the k-clustering problem, an Expectation Maximization algorithm was suggested by Liang & Klein (2009).\nThe second, closely related area, is that of streaming algorithms. The literature of clustering in the streaming model is very rich, we highlight the most relevant results. The first paper to study clustering problem is by Charikar et al. (2004) studying the k-CENTER problem. Guha et al. (2000) give the first single pass constant approximation algorithm to the k-MEDIAN variant. Subsequently their result has been is improved by Charikar et al. (2003). Finally, the best algorithm for the closely related variant of facility location is due to Czumaj et al. (2013), who gave a (1 + )- approximation for the problem."}, {"heading": "2. Preliminaries", "text": "Let X be a set of n points, and d : X \u00d7 X \u2192 R a distance function. We assume that d is symmetric and that (X, d) form a metric space, that is d(x, x) = 0 for any x \u2208 X; d(x, y) = d(y, x) \u2265 0 for any x, y \u2208 X; and, for any x, y, z \u2208 X , d(x, y) \u2264 d(x, z) + d(z, x). Finally, by scaling d, let minx,y d(x, y) = 1 and denote by \u2206 the maximum pairwise distance, maxx,y d(x, y). We will assume that \u2206 is bounded by a polynomial in n, therefore log \u2206 = O(log n).\nConsider a set of k points c = {c1, c2, . . . , ck} \u2286 X,which we will refer to as centers. For each ci, let Ci \u2286 X be the set of points in X closer to ci than to any other center c \u2208 C. 1 Formally, Ci = {x \u2208 X | d(x, ci) \u2264 minc\u2208c d(x, c)}.\nGiven a p > 0, in the rest of the paper we refer to the cost of a point x with to respect to a set of centers as: costp(x, c) = minci d(x, ci)\np. And cost of a cluster Ci as: costp(X,Ci) = \u2211 x\u2208Ci d(x, ci) p.\nNow we are ready to define our problem. For any p > 0 we can define the cost of clustering of pointsX with respect to the centers c \u2286 X as: costp(X, c) = \u2211 x\u2208X costp(x, c) =\u2211k\ni=1 \u2211 x\u2208Ci d(x, ci) p.\nThe k-clustering family of problems asks to find the set of centers c that minimize costp for a specific p. When p = 1, cost1(X, c) is precisely the k-MEDIAN clustering\n1 For clarity of the exposition we will assume that all of the pairwise distances are unique. The results still hold when ties are broken lexicographically.\nobjective. Setting p = 2 is equivalent to the k-MEDOIDS problem2. Finally, with p =\u221e, we recover the k-CENTER problem, which asks to minimize the maximum distance of any point to its nearest cluster center.\nObserve that although d(\u00b7, \u00b7) satisfies the triangle inequality, when raised to p-th power we need to relax the condition. In particular we have that for any x, y, z \u2208 X: d(x, y)p \u2264 2p\u22121(d(x, z)p + d(z, y)p).\nWhen p is clear from the context, we will refer to costp(X, c) as the cost of the clustering and denote it cost(X, c). We will us optp(X) to denote the optimum cost for the metric space (X, d). We will use c\u2217 = {c\u22171, c\u22172, . . . , c\u2217k} to denote the optimal solution.\nThe k clustering problem is NP-hard to solve exactly, thus we consider approximate solutions. We say that a clustering generated from a set of centers c is \u03b1-approximate if costp(X, c) \u2264 \u03b1 \u00b7optp(X). The best known approximation factors are 2 for the k-CENTER problem (Gonzalez, 1985), 1 + \u221a\n3 + for the k-MEDIAN problem (Li & Svensson, 2016), and 9 + for the k-MEDOIDS problem (Kanungo et al., 2004)."}, {"heading": "3. Consistency", "text": "As noted in the introduction, in many online clustering applications the choices made by the online algorithm are not irrevocable, but simply expensive to change. Moreover, by allowing a small number of full recomputations, we can circumvent the stringent lower bounds on competitive ratio for online clustering.\nTo this end, our goal in this work is to better understand the trade-off between the approximation ratio of online clustering algorithms, and the number of times the representative centers change.\nWe focus on a dynamic setting where the points arrive sequentially. Let xt denote the point that arrives at time t, and denote by Xt the set of points that has arrived from the beginning. Thus X0 = \u2205, and Xi+1 = Xi \u222a {xi+1} = {x1, x2, . . . , xi+1}.\nFor any two sets of centers c, c\u2032 let |c\u2212c\u2032| denote the number of elements present in c, but not in c\u2032: |c \u2212 c\u2032| = |c \\ (c \u2229 c\u2032)|. Observe that when c and c\u2032 have the same cardinality, |c\u2212 c\u2032| = |c\u2032 \u2212 c|. Definition 3.1. Given a sequence of sets of centers, c0, c1, . . . , ct and a positive monotone non-decreasing function \u03b2 : Z \u2192 R, we say that the sequence is \u03b2-consistent if for all T , \u2211T t=1 |ct \u2212 ct\u22121| \u2264 \u03b2(T ).\nIn other words, a sequence is \u03b2-consistent, if at time T at\n2In the Euclidean space if the centers do not need to be part of the input, setting p = 2 recovers the k-MEANS problem.\nmost \u03b2(T ) centers have changed between successive sets.\nDefinition 3.2. Given a sequence of points x1, x2, . . . , xT , and a parameter p, a sequence of centers c1, c2, . . . , cT is (\u03b1, \u03b2)-consistent if: (i) Approximation. At every time t, the centers ct form an \u03b1 approximate solution to the optimum solution at that time: costp(Xt, ct) \u2264 \u03b1 \u00b7 optp(Xt) for all t \u2264 T . (ii) Consistency. The sets of centers form a \u03b2-consistent sequence."}, {"heading": "3.1. A lower bound", "text": "Before we look for (\u03b1, \u03b2) consistent algorithms it is useful to understand what values are possible. We show that it is impossible to get a constant approximation and achieve consistency of o(log n) for any of the k clustering problems. Later, in Section 6 we will give a non-constructive result that shows that there is always a sequence of clusterings that is simultaneously constant-approximate and O(k log2 n) consistent.3\nLemma 3.3. There exists a sequence of points such that for any constant \u03b1 > 0, any algorithm that returns an \u03b1-approximate solution while processing n points must be \u2126(k log n)-consistent.\nProof. For ease of exposition, assume that p = 1, and consider points lying in (k \u2212 1)-dimensional Euclidean space, Rk\u22121. We begin by adding a point x0 at the origin, and points x1, . . . , xk\u22121 in positions e1, e2, . . . , ek\u22121, where ej is the standard basis vector that is 1 in the j-th dimension, and 0 everywhere else.\nWe then proceed in phases, where in phase 1 \u2264 i < log n we add points at position (\u03b3)i \u00b7 ej for each j \u2208 [1, k \u2212 1], for some \u03b3 > 0 that we will set later. In phase log n we add the remaining n\u2212 (k\u2212 1) log n\u2212 1 points at arbitrary positions within the convex hull of already added points.\nLet Pi be the set of points at the end of phase i. Consider any algorithm that returns an\u03b1-approximate solution onPi. Let p1, p2, . . . , pk\u22121 be the points added to the input during phase i, pj = \u03b3i \u00b7 ej . Then Pi = Pi\u22121 \u222a {p1, . . . , pk\u22121}. One feasible solution choses as centers the points added in phase i as well as the origin, C = {p1, p2, . . . , pk\u22121, 0}.\nFor every point in Pi\u22121 the origin is closer than any of the other centers, therefore the total cost is: opt(Pi) \u2264 cost(Pi, C) = (k \u2212 1) \u2211i\u22121 z=1 \u03b3 z \u2264 (k \u2212 1)\u03b3 i\u22121 \u03b3\u22121 . On the other hand, consider a set of centers c\u2032 that does not include some pj = \u03b3iej . The closest point to pj is at \u03b3i\u22121ej , which is at distance \u03b3i\u22121(\u03b3 \u2212 1) away. There-\n3Note that we assume throughout the paper that the maximum distance between any two points, \u2206, is polynomial in n. Alternatively we can restate the lower bound in this section as a \u2126(k log \u2206) upper bound in section 6 as a O(k log2 \u2206).\nfore, cost(Pi, c\u2032) \u2265 cost({pj}, c\u2032) = \u03b3i\u22121(\u03b3 \u2212 1). If \u03b3 \u2265 (2 + k\u03b1) then we can bound the approximation ratio as: cost(Pi,c\n\u2032) opt(Pi) \u2265 \u03b3 i\u22121(\u03b3\u22121) (k\u22121) \u03b3i\u22121\u03b3\u22121 \u2265 \u03b3 i\u22121(\u03b3\u22121)2 (k\u22121)\u03b3i \u2265 \u03b3\u22122 k\u22121 > \u03b1\nso c\u2032 cannot be an \u03b1-approximate solution. Therefore at the end of phase 1 \u2264 i < log n, any \u03b1-approximate set of centers, must include all points added in phase i. Thus any sequence of sets of centers must be \u2126(k log n)-consistent.\nNote that considering any p > 1 only makes any omission of point pj even more costly, as compared to the optimum solution.\n4. Warm up: k-CENTER Clustering To gain some intuition about consistent clustering, we begin with the k-CENTER objective. Given a dataset X , the goal is to identify k centers c = {c1, . . . , ck} that minimize: maxx\u2208X minc\u2208c d(x, c). This problem is known to be NP-hard, but a simple 2-approximation algorithm exists in the batch setting (Gonzalez, 1985). In the streaming setting, when points arrive one at a time, the DOUBLING algorithm by Charikar et al. (2004) was the first algorithm discovered for this problem. The algorithm maintains an 8-approximation. Furthermore, it works in O(log \u2206) = O(log n) phases and the total consistency cost of each phase is k; thus we get the following lemma.\nLemma 4.1. The DOUBLING algorithm for the k-CENTER problem is (8, O(k log n))-consistent."}, {"heading": "5. Main Algorithm", "text": "In this section we present our main result, an algorithm that achieves a polylogarithmic consistency factor. More precisely, we show that for every constant p \u2265 1, it is possible to design an algorithm for the Consistent k-clustering problem under costp that is constant approximate, and O(k2 log4 n)-consistent.\nIn the remainder of the section we first present the main ideas behind our algorithm, then prove some useful technical lemmas, and finally present the full algorithm."}, {"heading": "5.1. Main ideas", "text": "Before delving into the details, we highlight the three main building blocks of our algorithm.\nThe first is the Meyerson sketch for online facility location (Meyerson, 2001). This sketch has already been used by Charikar et al. (2003) to solve the k-median problem on data streams. We show that the main ingredients of the sketch continue to work under costp objectives, and use it to generally reduce the number of points under considerations from n to k \u00b7 poly log n.\nOne caveat of this sketch is that to use it we need to have access to a good lower bound on the cost of the optimal solution at any point in time. We obtain it by running the \u0398(p) approximation algorithm described by Gupta & Tangwongsan (2008) on all available points. In this way, at any point in time we have a good approximation of the optimum solution. Then we divide the progress of our algorithm into log n phases based on this lower bound and in each phase we use a different sketch.\nFinally, while the Meyerson sketch maintains O(k log2 n) possible centers, to computer the k-clustering, we have to reduce these points into exactly k final centers. We first show that this is possible and then we prove that we do not need to recluster frequently. In fact we will do it only when either a new point is added to the Meyerson sketch\u2014 O(k log2 n) times\u2014or when the number of points assigned to one of these elements of the Meyerson sketch doubles\u2014 O(k log n) events per sketch.\nBy putting all of these ingredients together, we show that the number of times we need to fully recluster is at most O(k log3 n) per phase, or that we haveO(k2 log4 n) cluster changes in total."}, {"heading": "5.2. The Meyerson sketch", "text": "We present the Meyerson sketch and prove some useful properties. We assume to have access to a lower bound to the cost of the optimal solution L, such that Lp \u2265 \u03b2optp, for some constant 0 \u2264 \u03b2 \u2264 1. (We will remove the assumption later.) Then the algorithm works in phases, such that at any time in phase j, L \u2208 [2j\u22121, 2j). So in each phase j we can use the same lower bound Lpj = 2 j\u22121 and have Lpj \u2265 \u03b2optp 2 .\nIn each phase j we create 2 log n Meyerson sketches as described in Algorithm 1. Then we combine them in a single sketch as described in Algorithm 2.\nAlgorithm 1 Single Meyerson sketch 1: Input: A sequence of points x0, x1, x2, . . . , xn. A finite p. 2: Output: A set S that is a constant bi-criteria approximate\nsolution for the k-clustering problem. 3: S \u2190 \u2205 4: Let X be a set of points and let L be such that L \u2265 \u03b3optp(X), for some constant \u03b3 > 0 5: for x \u2208 X do 6: if S == \u2205 then 7: S \u2190 {x} 8: else 9: Let \u03b4 = d(x, S)p\n10: With probability min ( \u03b4k(1+logn) Lp , 1 ) add x to S\n11: Return S\nFor simplicity we first analyze the property of a single Meyerson sketch. In particular we give a bound on both the\nnumber of points selected by a single sketch, as well as the quality of the approximation. The Lemma generalizes the results in (Charikar et al., 2003; Meyerson, 2001) to all finite p and follows the general structure their proof so it is deferred to the extended version of the paper.\nLemma 5.1. For a constant \u03b3 \u2208 (0, 1), with probability at least 12 the set S computed by Algorithm 1 has: (i)\nsize at most 4k(1 + log n) ( 22p+1 \u03b3p + 1 )\n; (ii) costp(S) \u2264 64optp(X).\nFrom Lemma 5.1 we know that with constant probability a single Meyerson sketch is of size O(k log n) and contains a set of points that give a good solution to our problem. Thus, if we construct 2 log n single Meyerson sketches in parallel, at least one of them gives a constant approximation to the optimum at every point in time with probability at least 1 \u2212 O(n\u22121). The observation inspired the design of Algorithm 2, whose properties are formalized next.\nLemma 5.2. For a constant \u03b3 \u2208 (0, 1), with probability 1 \u2212 O(n\u22121) the set M = \u222a2 logni=1 Mi computed by Algorithm 2 has: size at most O(k log2 n) and costp(M) \u2264 64optp(X).\nProof. As mentioned above, Lemma 5.1 implies that if we construct 2 log n single Meyerson sketches in parallel, with probability 1 \u2212 O(n\u22121), at least one of them gives a constant approximation to the optimum at every point in time. Furthermore in total it contains only 4k(1 + log n) ( 22p+1 \u03b3p + 1 ) points.\nNow in Algorithm 2 we are almost building 2 log n Meyerson sketches; the only difference is that we stop adding points to a single sketch when it becomes too large. This modification does not change the probability that there exist at least one single sketch that gives a constant approximation to the optimum at every point in time and has at most 4k(1 + log n) ( 22p+1 \u03b3p + 1 ) points.\nThus with probability 1 \u2212 O(n\u22121) at least one of the sketches constructed in Lemma 5.1 gives a constant approximation to the optimum at every point in time. Merging other sketches to this sketch does not affect this property. Furthermore the number of points in each sketch is explicitly bounded by 4k(1 + log n) ( 22p+1 \u03b3p + 1 )\nso the total number of points in M is bounded by 8k log n(1 + log n) ( 22p+1 \u03b3p + 1 )\nNote that in some cases we do not need to recompute all the sketches from scratch but we need only to update them, so we can define a faster update function described in Algorithm 3.\nAlgorithm 2 ComputeMeyerson(Xt, \u03c6) 1: Input: A sequence of points Xt, a lower bound to the opti-\nmum \u03c6. 2: Output: 2 logn independent Meyerson sketches M1, . . . ,M2 logn 3: Lp = \u03c6 \u03b3\n, 4: for i \u2208 [2 logn] do: . Initialize all Meyerson sketches 5: Mi \u2190 x0 6: for x \u2208 Xt do: 7: for i \u2208 [2 logn] do: . If Mi is not too large, analyze x 8: if |Mi| \u2264 4k(1 + logn) ( 22p+1\n\u03b3p + 1\n) then:\n9: Let \u03b4 = d(x,Mi)p 10: p\u0302 = min ( \u03b4k(1+logn) Lp , 1 ) 11: Add x to Mi with probability p\u0302 12: Return M1, . . . ,M2 logn\nAlgorithm 3 UpdateMeyerson(M1, . . . ,Ms, xt, \u03c6) 1: Input: A point xt, a lower bound to the optimum \u03c6 and s\nindependent Meyerson sketches M1, . . . ,Ms. 2: Output: s independent Meyerson sketches M1, . . . ,Ms 3: Lp = \u03c6\n\u03b3 ,\n4: for i \u2208 [s] do: . If Mi is not too large, analyze xt 5: if |Mi| \u2264 4k(1 + logn)\n( 22p+1\n\u03b3p + 1\n) then:\n6: Let \u03b4 = d(xt,Mi)p 7: p\u0302 = min ( \u03b4k(1+logn) Lp , 1 ) 8: Add xt to Mi with probability p\u0302 9: Return M1, . . . ,Ms\nIn the rest of the paper we refer to a single Meyerson sketch as Mi and to their union as M ."}, {"heading": "5.3. From Meyerson to k clusters", "text": "Our next step is to show that in the Meyerson sketch there exists a subset of k centers that gives an approximately optimal solution. We follow the approach in Guha et al. (2000) and show that by weighing the points in the Meyerson sketch with the number of original data points assigned to them, and then running a weighted k-clustering algorithm to recluster them into k clusters, we can achieve a constant approximate solution.\nBefore formalizing this observation we give some additional notation. In the remainder of the section we denote the weight of a point x in the Meyerson sketch with w(x), the cost of the centers used in Meyerson sketch with costM, and the cost of the aforementioned weighted clustering instance with costL. Finally we refer to the optimal set of centers for the weighted k-clustering instance as c\u2032.\nWe begin with two technical Lemmas.\nLemma 5.3. For any constant p \u2265 1, costp(X, c\u2032) \u2264 2p\u22121 (costM + costL)\nLemma 5.4. For any constant p \u2265 1, costL \u2264\n22p\u22121 ( costM + optp ) Note that combining Lemmas 5.3 and 5.4 the following Corollary follows. Corollary 5.5. For any constant p \u2265 1, costp(c\u2032) \u2264 23p\u22121 ( costM + optp\n) We defer the proofs of lemma 5.3 and lemma 5.4 to the extended version of the paper. Those proofs are similar in spirit to those in (Bateni et al., 2014; Guha et al., 2000), but are generalized here for all p.\nThanks to Corollary 5.5 we know that by using a Meyerson sketch, M contains a good approximation for our problem. In the next subsection we show how to use this to obtain a solution for the consistency problem.\nBefore doing this we define two algorithms that allow us to construct a weighted clustering instance starting from a Meyerson sketch (Algorithm 4) and to update the weights for a weighted instance (Algorithm 5).\nAlgorithm 4CreateWeightedInstance(M1, . . . ,Ms, \u03c6,Xt) 1: Input: A sequence of points Xt, a lower bound to the opti-\nmum \u03c6 and s independent Meyerson sketches M1, . . . ,Ms. 2: Output: A weighted k-clustering instance (M,w). 3: Let M = \u222aiMi 4: Assign points in Xt to the closest point in M 5: Let w(y) to be equal to the number of points assigned to y \u2208 M 6: Return (M,w)\nAlgorithm 5 UpdateWeights(M,w, x) 1: Input: A point x, the current weights w and the Meyerson\nsketch M . 2: Output: A weighted k-clustering instance (M,w). 3: Assign x to the closest point in M 4: Let mx be the closest point to x in M 5: w(mx) = w(mx) + 1 6: Return (M,w)"}, {"heading": "5.4. The algorithm", "text": "We are now ready to formally state and prove the correctness of our main algorithm, which we present in Algorithm 6. The input of our algorithm is a sequence of points x0, x1, x2, . . . , xn. Recall, that we denote the prefix up to t as Xt, and the cost of the solution using centers c as costp(Xt, c). Finally we assume to have access to a \u03b3approximation algorithm A for the weighted k-clustering problem for any constant p-norm (we can use for example the local search algorithm described by Gupta & Tangwongsan (2008)).\nWe can now state our main theorem. Theorem 5.6. For any constant p \u2265 1, with probability 1 \u2212 O(n\u22121), Algorithm 6 returns a sequence of\nAlgorithm 6 Consistent k-clustering algorithm 1: Input: A sequence of points x0, x1, x2, . . . , xn. 2: Output: A sequence of centers c0, c1, c2, . . . , cn 3: Select the first k points as centers c0 = {x0, x1, x2, . . . , xk} 4: t\u2190 0 5: while costp(c0, Xt) = 0 do: 6: ct \u2190 c0; Output ct; t\u2190 t+ 1 7: \u03c6\u2190 0 . Initialize lower bound to the optimum M1, ...,M2 logn \u2190 ComputeMeyerson(X0, \u03c6)\n8: c\u2190 \u2205; s\u2190 2 logn 9: while t \u2264 n do:\n10: Run A on Xt to get approximated solution c\u2032 11: if costp(Xt, c\u2032) \u2265 2\u03c6 then: . New l.b. for \u03c6 12: \u03c6\u2190 costp(Xt, c\u2032), 13: M1, ...,Ms \u2190 ComputeMeyerson(Xt, \u03c6) 14: (M,w)\u2190 GetWeightedProb(M1, ...,Ms, \u03c6,Xt) 15: Solve (M,w) using algorithm A 16: Let ct be the set of centers computed by A 17: else: . Update Meyerson and recluster if needed 18: M1,M2, ..\u2190 UpdateMeyerson(M1, ..,Ms, xt, \u03c6) 19: Let M = \u222aiMi, 20: if xt \u2208M then: . xt is in Meyerson sketch 21: (M,w)\u2190 GetWeightedProb(M1, ...,Ms, \u03c6,Xt) 22: Solve (M,w) using algorithm A 23: Let ct be the set of centers computed by A 24: else: 25: (M,w)\u2190 UpdateWeights(M,w, x) 26: Let mt be the closest point to xt in M 27: if w(mt) is a power of 2 then: 28: . Weight of a point \u201cdoubled\u201d 29: Solve (M,w) using algorithm A 30: Let ct be the set of computed centers 31: else: 32: ct = ct\u22121 33: Output ct; t\u2190 t+ 1\ncenters c0, c1, c2, . . . , cn such that at any point in time t costp(ct, Xt) \u2264 \u03b1poptp(Xt) for a constant \u03b1 and the total inconsistency factor of the solution is O(k2 log4 n)\nProof. We start by bounding the inconsistency factor,\u2211n\u22121 i=1 |ci+1 \u2212 ci|.\nDuring the execution of Algorithm 6 the set of centers changes if and only if one of the three following conditions is met: (i) the cost of the clustering on Xt computed byA increases by a factor of 2, (ii) we add a new point to a Meyerson sketch, (iii) a new point is assigned to a point of the Meyerson sketch, mt, and the weight of mt is a power of 2 after this addition. Note that every time we change the centers, we fully recluster, and so increase the consistency factor by k in the worst case. Therefore to prove the theorem we need to show that one of these conditions is met at most O(k log4 n) times.\nFrom our assumptions we know that the spread of the point set is polynomial in n, which implies the same bound on the cost of the optimum solution. Therefore, the cost of the solution computed by A doubles at most O(log n) times.\nFor the same reason we update the lower bounds, \u03c6 at most O(log n) times during the execution of our algorithm. This in turn implies that we rebuild the Meyerson sketches from scratch at mostO(log n) times. Given that we runO(log n) Meyerson sketches in parallel, during the execution of the algorithm we use at most O(log2 n) Meyerson sketches. Furthermore each Meyerson sketch has at most O(k log n) centers, thus in total we can add at most O(k log3 n) points under condition (ii).\nFinally note that while a Meyerson sketch is fixed, the weight of every point in the sketch can only grow. In addition, the weight is always is bounded by n, and therefore can double at most log n times per sketch point, resulting in O(k log2 n) changes under a fixed Meyerson sketch. Therefore condition (iii) holds at most O(k log4 n) times. So overall at least one of the conditions is satisfied at most O(k log4 n) times, thus the algorithm is O(k2 log4 n)-consistent.\nTo finish our proof we need to show that at any point in time our algorithm returns with probability 1\u2212o(n\u22121) a constant approximation to the optimum. Note that by corollary 5.5 we know that for any constant p \u2265 1 the cost of a solution computed on the Meyerson sketch can be bounded by costp(c \u2032) \u2264 23p\u22121 ( costM + optp ) . From Lemma 5.2 we know that the Meyerson sketch guarantees with probability 1\u2212 o(n\u22121) that costM \u2264 16optp(X). So we have the cost of the optimal set of centers in the Meyerson sketch at any point in time is at mostO(\u03b1poptp) w.h.p. for a constant \u03b14.\nWhile we cannot compute the optimal set of centers in the Meyerson sketch, we can find an O(p) approximation for every constant p by relying on the local search algorithm of Gupta & Tangwongsan (2008). Therefore, every time we recompute the centers usingA we are sure that we obtain a constant approximation.\nFinally it remains to show that when none of the three conditions are met, and we simply add a point to the solution without recomputing the centers we retain an approximately optimal solution. By Lemma 5.3 we know that for any constant p \u2265 1, costp(c\u2032) \u2264 2p\u22121 (costM + costL) .\nMoreover, we can always bound the cost of Meyerson sketch with 16optp(X).\nIt remains to get a bound on costL. Note that the number of points assigned to any point in M did not double since the previous reclustering. Therefore, in the weighted reclustering formulation the weight of all points increased by a factor less than 2. Therefore, costL at this point is bounded by at most twice costL computed when we last reclustered. Therefore, costp(c\u2032) \u2264 24p\u22121 ( costM + optp ) and the cur-\n4We do not make an attempt to optimize the constant factors. As we show in the experimental section, in practice the algorithm gives a very good approximation.\nrent solution remains approximately optimal."}, {"heading": "6. Optimizing Consistency", "text": "How many times do we need to change the centers to obtain a good k-clustering at any point in time? In Section 5 we presented an algorithm that is O(k2 log4 n)-consistent, while in Subsection 3.1 we showed that at least \u2126(k log n) changes are needed (assuming that \u2206 is polynomial in n). We give an existential result, we show that for any input sequence there exist a solution that is constant approximate and O(k log2 n)-consistent. In interest of space we deferred the proof of the lemma to the extended version of the paper.\nTheorem 6.1. For any sequence x0, x1, . . . , xn there exists a sequence of solutions c0, c1, . . . , cn such that \u2200i, costp(Xi, ci) \u2264 \u03b1optp(Xi) for some constant \u03b1, and the\u2211 i |ci+1 \u2212 ci\u2016 = O(k log 2 n)."}, {"heading": "7. Experiments", "text": "We demonstrate the efficacy of our algorithm by tracking both the quality of the solution and the number of reclusterings needed to maintain it on a number of diverse datasets. As we will show, the theoretical guarantees that we prove in the previous section provide a loose bound on the number of reclusterings; in practice the number of times we recompute the solution grows logarithmically with time.\nData We evaluate our algorithm on three datasets from the UCI Repository (Lichman, 2013) that vary in data size and dimensionality. (i) SKINTYPE has 245, 057 points lying in 4-dimensions. (ii) SHUTTLE has 58, 000 points in 9 dimensions. (iii) COVERTYPE has 581, 012 points in 54 dimensions. For each of the datasets we try values of k in {10, 50, 100}, and observe that the qualitative results are consistent across datasets and values of k.\nAlgorithm Modifications In the development of the algorithm we made a number of decisions to obtain high probability results. The key among them was to run O(log n) copies of the Meyerson sketch, since each sketch succeeds only with constant probability. We eschew this change in the implementation, and maintain just a single sketch, at the cost of incurring a worse solution quality.\nMetrics and results The goal of this work is to give algorithms that maintain a good clustering, but only recluster judiciously, when necessary. To that end, we focus on two main metrics: number of reclusterings and solution quality.\nReclustering We plot the number of reclusterings as a function of time for the three different datasets in Figure 1. Note that the x-axis is on log-scale, and thus a straight line\nrepresents number of reclusterings that grows logarithmically with time. Qualitatively we make two observations, across all datasets, and values of k.\nFirst, the rate of reclustering (defined as the fraction of time the algorithm recomputes the solution) is approximately logn/n, which tends to 0 as the dataset size grows. Further, the rate is higher for higher values of k, a fact also suggested by our theoretical analysis.\nUnlike the SHUTTLE and COVERTYPE datasets, the SKINTYPE dataset exhibits a change in behavior, where initially the reclustering rate is relatively high, but then it sharplly drops after about O(2k) steps. This is explained by the fact that the order of the points in this data set is not random. Therefore initially the algorithm reclusters at a high rate, once all of the parts of the input space are explored, the rate of reclustering slows. When we run the algorithm on a randomly permuted instance of SKINTYPE, this phase transition in behavior disappears.\nApproximation Ratio We plot the approximation ratio of the solution (as compared to the best obtained by ten runs of k-means++ (Arthur & Vassilvitskii, 2007)) in Figure 2.\nFor the SKIN and COVERTYPE datasets, the approximation ratio stays relatively low, largely bounded by 4, after an initial period. A more careful examination of the plots shows exactly the times when the consistent algorithm allows the solution to degrade, and when it decides to recompute the solution from scratch. The latter are indicated by sharp drops in the approximation ratio, whereas the former are the relatively flat patterns.\nIt is interesting to note that the additional points sometimes worsen the approximation (as indicated by the lines sloping upwards), but sometimes actually improve the approximation. This is due to the fact that decisions made by the online algorithm balance optimality at that point in time, with the potential location of points arriving in the future. The latter is most apparent in the k = 100 experiment of the SHUTTLE dataset.\nAll of the datasets sometimes exhibit large fluctuations in the approximation ratio. This is an artifact of using a single Myerson sketch, which does not capture the structure of the points with small, but constant probability."}, {"heading": "8. Conclusions and Future Work", "text": "We introduced the notion of consistent clustering: a variant of online clustering which balances the need for maintaining an approximately optimal solution with the cost of reclustering. We proved \u2126(klog n) lower bounds, and gave algorithms for all k-clustering variants that come close to achieving this bound.\nThe notion of quantifying the worst case number of changes necessary to maintain a constant approximate solution in an online setting is interesting to study in contexts other than k-clustering. For example, one can consider online graph problems, such as online matching and online densest subgraph, or other types of clustering problems, such as hierarchical or correlation clustering."}], "year": 2017, "references": [{"title": "A simple and deterministic competitive algorithm for online facility location", "authors": ["Anagnostopoulos", "Aris", "Bent", "Russell", "Upfal", "Eli", "Hentenryck", "Pascal Van"], "venue": "Inf. Comput.,", "year": 2004}, {"title": "K-means++: The advantages of careful seeding", "authors": ["Arthur", "David", "Vassilvitskii", "Sergei"], "venue": "In Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms,", "year": 2007}, {"title": "A polylogarithmic-competitive algorithm for the k-server problem", "authors": ["Bansal", "Nikhil", "Buchbinder", "Niv", "Madry", "Aleksander", "Naor", "Joseph (Seffi"], "venue": "J. ACM,", "year": 2015}, {"title": "Distributed balanced clustering via mapping coresets", "authors": ["Bateni", "MohammadHossein", "Bhaskara", "Aditya", "Lattanzi", "Silvio", "Mirrokni", "Vahab S"], "venue": "In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems", "year": 2014}, {"title": "Better streaming algorithms for clustering problems", "authors": ["Charikar", "Moses", "O\u2019Callaghan", "Liadan", "Panigrahy", "Rina"], "venue": "In Proceedings of the 35th Annual ACM Symposium on Theory of Computing, June 9-11,", "year": 2003}, {"title": "Incremental clustering and dynamic information retrieval", "authors": ["Charikar", "Moses", "Chekuri", "Chandra", "Feder", "Tom\u00e1s", "Motwani", "Rajeev"], "venue": "SIAM J. Comput.,", "year": 2004}, {"title": "-approximation for facility location in data streams", "authors": ["Czumaj", "Artur", "Lammersen", "Christiane", "Monemizadeh", "Morteza", "Sohler", "Christian"], "venue": "In Proceedings of the Twenty-Fourth Annual ACM-SIAM Symposium on Discrete Algorithms,", "year": 2013}, {"title": "Online stochastic matching: Beating 1-1/e", "authors": ["Feldman", "Jon", "Mehta", "Aranyak", "Mirrokni", "Vahab S", "S. Muthukrishnan"], "venue": "In 50th Annual IEEE Symposium on Foundations of Computer Science,", "year": 2009}, {"title": "Who solved the secretary problem", "authors": ["Ferguson", "Thomas S"], "venue": "Statist. Sci.,", "year": 1989}, {"title": "On the competitive ratio for online facility", "authors": ["Fotakis", "Dimitris"], "venue": "location. Algorithmica,", "year": 2008}, {"title": "Clustering to minimize the maximum intercluster distance", "authors": ["Gonzalez", "Teofilo F"], "venue": "Theor. Comput. Sci.,", "year": 1985}, {"title": "Clustering data streams", "authors": ["Guha", "Sudipto", "Mishra", "Nina", "Motwani", "Rajeev", "O\u2019Callaghan", "Liadan"], "venue": "Annual Symposium on Foundations of Computer Science,", "year": 2000}, {"title": "Simpler analyses of local search algorithms for facility location", "authors": ["Gupta", "Anupam", "Tangwongsan", "Kanat"], "venue": "CoRR, abs/0809.2554,", "year": 2008}, {"title": "A local search approximation algorithm for k-means clustering", "authors": ["Kanungo", "Tapas", "Mount", "David M", "Netanyahu", "Nathan S", "Piatko", "Christine D", "Silverman", "Ruth", "Wu", "Angela Y"], "venue": "Comput. Geom.,", "year": 2004}, {"title": "An optimal algorithm for on-line bipartite matching", "authors": ["Karp", "Richard M", "Vazirani", "Umesh V", "Vijay V"], "venue": "In Proceedings of the 22nd Annual ACM Symposium on Theory of Computing, May 13-17,", "year": 1990}, {"title": "Secretary problems with non-uniform arrival order", "authors": ["Kesselheim", "Thomas", "Kleinberg", "Robert D", "Niazadeh", "Rad"], "venue": "In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing,", "year": 2015}, {"title": "A multiple-choice secretary algorithm with applications to online auctions", "authors": ["Kleinberg", "Robert D"], "venue": "In Proceedings of the Sixteenth Annual ACM-SIAM Symposium on Discrete Algorithms,", "year": 2005}, {"title": "Online em for unsupervised models", "authors": ["Liang", "Percy", "Klein", "Dan"], "venue": "In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,", "year": 2009}, {"title": "An algorithm for online k-means clustering", "authors": ["Liberty", "Edo", "Sriharsha", "Ram", "Sviridenko", "Maxim"], "venue": "In Proceedings of the Eighteenth Workshop on Algorithm Engineering and Experiments,", "year": 2016}, {"title": "Online bipartite matching with random arrivals: an approach based on strongly factor-revealing lps", "authors": ["Mahdian", "Mohammad", "Yan", "Qiqi"], "venue": "In Proceedings of the 43rd ACM Symposium on Theory of Computing,", "year": 2011}, {"title": "Competitive algorithms for server problems", "authors": ["Daniel Dominic"], "venue": "J. Algorithms,", "year": 1990}, {"title": "Online facility location", "authors": ["Meyerson", "Adam"], "venue": "Annual Symposium on Foundations of Computer Science,", "year": 2001}, {"title": "Amortized efficiency of list update and paging rules", "authors": ["Sleator", "Daniel Dominic", "Tarjan", "Robert Endre"], "venue": "Commun. ACM,", "year": 1985}], "id": "SP:834cf34ad97a0fb048c3e012402164d7c5488132", "authors": [{"name": "Silvio Lattanzi", "affiliations": []}, {"name": "Sergei Vassilvitskii", "affiliations": []}], "abstractText": "The study of online algorithms and competitive analysis provides a solid foundation for studying the quality of irrevocable decision making when the data arrives in an online manner. While in some scenarios the decisions are indeed irrevocable, there are many practical situations when changing a previous decision is not impossible, but simply expensive. In this work we formalize this notion and introduce the consistent k-clustering problem. With points arriving online, the goal is to maintain a constant approximate solution, while minimizing the number of reclusterings necessary. We prove a lower bound, showing that \u03a9(k log n) changes are necessary in the worst case for a wide range of objective functions. On the positive side, we give an algorithm that needs onlyO(k log n) changes to maintain a constant competitive solution, an exponential improvement on the naive solution of reclustering at every time step. Finally, we show experimentally that our approach performs much better than the theoretical bound, with the number of changes growing approximately as O(log n).", "title": "Consistent k-Clustering"}