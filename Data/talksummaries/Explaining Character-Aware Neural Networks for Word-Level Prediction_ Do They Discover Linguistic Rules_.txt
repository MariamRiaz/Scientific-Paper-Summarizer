7	32	Rulebased taggers are transparent models that allow us to easily trace back why the tagger made a certain decision (e.g., Brill (1994)).
12	16	Following the above rule, the word mistress will mistakingly be tagged as a plural common noun while it actually is a common noun1.
13	21	This is in stark contrast with the most recent generation of part-of-speech and morphological taggers which mainly rely on neural networks.
15	36	However, it is currently unknown which characterlevel patterns these neural network models learn and whether these patterns coincide with our linguistic knowledge.
20	17	By visualizing the contributions of each character, we observe that the model indeed uses the suffix -s to correctly predict that the word is plural.
44	19	The idea behind CD is that, in the context of character-level decomposition, we can decompose the output value of the network for a certain class into two distinct groups of contributions: (1) contributions originating from a specific character or set of characters within a word and (2) contributions originating from all the other characters within the same word.
45	53	More generally, we can decompose every output value z of every neural network component into a relevant contribution β and an irrelevant contribution γ: z = β + γ (1)
46	21	A CNN typically consist of three components: the convolution itself, an activation function and an optional max-pooling operation.
48	21	Decomposing the convolution Given a sequence of character embeddings x1, ...,xT ∈ Rd1 of length T , we can calculate the convolution of size n of a single filter over the sequence x1:T by applying the following equation to each n-length subsequence {xt+i, i = 0, .., n − 1}, denoted as xt:t+n−1: zt = n−1∑ i=0 Wi · xt+i + b, (2) with zt ∈ R and where W ∈ Rd1×n and b ∈ R are the weight matrix and bias of the convolutional filter.
50	15	When we want to calculate the contribution of a subset of characters, where S is the set of corresponding character position indexes and S ⊆ {1, ..., T}, we should decompose the output of the filter zt into three parts: zt = βt + γt + b.
51	20	(3) That is, the relevant contribution βt originating from the selected subset of characters with indexes S, the irrelevant contribution γt originating from the remaining characters in the sequence, and a bias which is deemed neutral (Murdoch et al., 2018).
56	24	To that end, we compute LfReLU (yk), the linearized contribution of yk as the average difference of partial sums over all possible permutations π1, ..., πMN of all N components yi involved: Lf (yk) = 1 MN MN∑ i=1 [f( π−1i (k)∑ l=1 yπi(l))− f( π−1i (k)−1∑ l=1 yπi(l))] (6) Consequently, we can decompose the output ct after the activation function as follows: ct =fReLU (zt) (7) =fReLU (βz,t + γz,t + b) (8) =LReLU (βz,t) + [LReLU (γz,t) + LReLU (b)] (9) =βc,t + γc,t (10) Following Murdoch et al. (2018), βc,t contains the contributions that can be directly attributed to the specific set of input indexes S. Hence, the bias b is part of γc,t.
58	21	(10) is exact in terms of the total sum, the individual attribution to relevant (βc,t) and irrelevant (γc,t) is an approximation, due to the linearization.
73	43	3 For each language, Silfverberg and Hulden (2017) selected the first non-unique 300 words from the UD test set and manually segmented each word according to the associated lemma and morphological features in the dataset.
83	20	We experiment with both a CNN and BiLSTM architecture for character-level modeling of words.
134	36	Word (verb): olivat (were), target class: Tense=Past ˆ g r a t u i t a $ BiLSTM CNN -2.6 0 2.6 (b) Example of Spanish.
136	31	quence corresponds to the set or sequence of characters with the same length within the considered word that has the highest contribution for predicting the correct label for that word.
165	18	Spanish While there is no single clear-cut rule for the Spanish gender, in general the suffix a denotes the feminine gender in adjectives.
166	16	However, there exist many nouns that are feminine but do not have the suffix a. Teschner and Russell (1984) identify d, and ión as typical endings of feminine nouns, which our models identified too as for example ad$ or ió/sió.
167	21	Swedish In Swedish, there exist four suffixes for creating a plural form: or, ar, (e)r and n. Both models identified the suffix or.
169	49	In Swedish, the suffix na only occurs together with one of the first three plural suffixes.
173	43	For example the third person singular form of the verb gustar is gusta.
174	52	Hence, this raises the question if the model will classify gusta wrongly as feminine or correctly as NA.
179	34	Next, for each word in both groups we calculated the most positively and negatively contributing character set out of all possible character sets of any length within the considered word, using the CD algorithm.
180	31	We compared the contribution scores in both groups using a Kruskal-Wallis significance test.6 While no significant (p < 0.05) difference could be found between the positive contributions of both groups (p=1.000), a borderline significant difference could be found between the negative contributions of words predicted as feminine and words predicted as not-feminine (p=0.070).
183	21	While the positive evidence is the strongest for the class feminine, the model identifies the verb stem gust as negative evidence which ultimately leads to the correct final prediction NA.
184	19	While neural network-based models are part of many NLP systems, little is understood on how they handle the input data.
188	23	We showed that these patterns generally coincide with the morphological segments as defined by linguists for three morphologically different languages, but that sometimes other linguistically plausible patterns are learned.
189	30	Finally, we showed that our CD algorithm for CNNs is able to explain why the model made a wrong or correct prediction.
190	54	By visualizing the contributions of each input unit or combinations thereof, we believe that much can be learned on how a neural network handles the input data, why it makes certain decisions, or even for debugging neural network models.
