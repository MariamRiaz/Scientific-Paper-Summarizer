0	108	“Or vedi l’anime di color cui vinse l’ira.”1 – Dante Alighieri, Divina Commedia, Inferno Online conversations have a reputation for going awry (Hinds and Mortensen, 2005; Gheitasy et al., 2015): antisocial behavior (Shepherd et al., 2015) or simple misunderstandings (Churchill and Bly, 2000; Yamashita and Ishida, 2006) hamper the efforts of even the best intentioned collaborators.
1	53	Prior computational work has focused on characterizing and detecting content exhibiting antisocial online behavior: trolling (Cheng et al., 2015, 2017), hate speech (Warner and Hirschberg, 2012; Davidson et al., 2017), harassment (Yin et al., 2009), personal attacks (Wulczyn et al., ∗ Corresponding senior author.
3	25	Our goal is crucially different: instead of identifying antisocial comments after the fact, we aim to detect warning signs indicating that a civil conversation is at risk of derailing into such undesirable behaviors.
4	54	Such warning signs could provide potentially actionable knowledge at a time when the conversation is still salvageable.
8	52	As humans, we have some intuition about which conversation is more likely to derail.2 We may note the repeated, direct questioning with which A1 opens the exchange, and that A2 replies with yet another question.
24	94	The dataset consists of conversations which begin with ostensibly civil comments, and either remain healthy or derail into personal attacks.
28	69	In this controlled setting, we find that pragmatic cues extracted from the very first exchange in a conversation (i.e., the first comment-reply pair) can indeed provide some signal of whether the conversation will subsequently go awry.
56	25	To select candidate conversations to include in our collection, we use the toxicity classifier provided by the Perspective API,5 which is trained on Wikipedia talk page comments that have been annotated by crowdworkers (Wulczyn et al., 2016).
71	47	Specifically, starting from our human-vetted collection of conversations, we pair each awry-turning conversation, with an on-track conversation, such that both took place on the same talk page.
76	20	We now describe our framework for capturing linguistic cues that might inform a conversation’s future trajectory.
88	20	We also consider markers of impolite behavior, such as the use of direct questions (“Why’s there no mention of it?’) and sentenceinitial second person pronouns (“Your sources don’t matter...”), which may serve as forcefulsounding contrasts to negative politeness markers.
92	17	For instance, in a collaborative setting, we may expect conversations that start with an invitation for working together to signal less tension between the participants than those that start with statements of dispute.
93	77	We discover types of such conversation prompts in an unsupervised fashion by extending a framework used to infer the rhetorical role of questions in (offline) political debates (Zhang et al., 2017b) to more generally extract the rhetorical functions of comments.
95	46	As such, comments which tend to trigger similar replies constitute a particular type of prompt.
101	17	We project P into the same space as UR by solving for P̂ in P = P̂SV TR as P̂ = PVRS−1.
112	69	In contrast, comments which initiate on-track conversations tend to contain gratitude (p < 0.05) and greetings (p < 0.001), both positive politeness strategies.
114	44	Negative politeness strategies are salient in on-track conversations as well, reflected by the use of hedges (p < 0.01) and opinion prompts (p < 0.05), which may serve to soften impositions or factual contentions (Hübler, 1983).
123	19	Among conversations initiated instead by the non-attacker (Figure 2C, 662 conversations), the non-attacker’s linguistic behavior in the first comment (⃝s) is less distinctive from that of initiators in the on-track setting (i.e., log-odds ratios closer to 0); markers of future derailment are (unsurprisingly) more pronounced once the eventual attacker (▽s) joins the conversation in the second comment.12 More broadly, these results reveal how different politeness strategies and rhetorical prompts deployed in the initial stages of a conversation are tied to its future trajectory.
125	39	In doing so, we demonstrate that the pragmatic devices examined above encode signals about the future trajectory of conversations, capturing some of the intuition humans are shown to have.
142	40	Majority vote across three annotators was used to determine the human labels, resulting in an accuracy of 72%.
143	118	This confirms that humans have some intuition about whether a conversation might be heading in a bad direction, which our features can partially capture.
144	73	In fact, the classifier using pragmatic features is accurate on 80% of the examples that humans also got right.
147	18	Overall, these initial results show the feasibility of reconstructing some of the human intuition about the future trajectory of an ostensibly civil conversation in order to predict whether it will eventually turn awry.
151	136	Making use of machine learning and crowdsourcing tools, we formulate a tightly-controlled setting that enables us to meaningfully compare conversations that stay on track with those that go awry.
160	39	Beyond the present binary classification task, one could explore a sequential formulation predicting whether the next turn is likely to be an attack as a discussion unfolds, capturing conversational dynamics such as sustained escalation.
161	30	Finally, our study of derailment offers only one glimpse into the space of possible conversational trajectories.
162	85	Indeed, a manual investigation of conversations whose eventual trajectories were misclassified by our models—as well as by the human annotators—suggests that interactions which initially seem prone to attacks can nonetheless maintain civility, by way of level-headed interlocutors, as well as explicit acts of reparation.
163	68	A promising line of future work could consider the complementary problem of identifying pragmatic strategies that can help bring uncivil conversations back on track.
164	57	We are grateful to the anonymous reviewers for their thoughtful comments and suggestions, and to Maria Antoniak, Valts Blukis, Liye Fu, Sam Havron, Jack Hessel, Ishaan Jhaveri, Lillian Lee, Alex Niculescu-Mizil, Alexandra Schofield, Laure Thompson, Andrew Wang, Leila Zia and the members of the Wikimedia Foundation anti-harassment program for extremely insightful (on-track) conversations and for assisting with data annotation efforts.
165	164	This work is supported in part by NSF CAREER Award IIS1750615, NSF Grant SES-1741441, a Google Faculty Award, a WMF gift and a CrowdFlower AI for Everyone Award.
