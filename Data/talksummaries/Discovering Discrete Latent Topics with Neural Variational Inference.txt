12	29	In this work we propose and evaluate a range of topic models parameterised with neural networks and trained with variational inference.
20	28	Previous neural document models, such as the neural variational document model (NVDM) (Miao et al., 2016), belief networks document model (Mnih & Gregor, 2014), neural auto-regressive document model (Larochelle & Lauly, 2012) and replicated softmax (Hinton & Salakhutdinov, 2009), have not explicitly modelled latent topics.
21	24	Through evaluations on a range of data sets we compare our models with previously proposed neural document models and traditional probabilistic topic models, demonstrating their robustness and effectiveness.
22	11	In probabilistic topic models, such as LDA (Blei et al., 2003), we use the latent variables θd and zn for the topic proportion of document d, and the topic assignment for the observed word wn, respectively.
33	19	By using a Gaussian prior distribution, we are able to employ the re-parameterisation trick (Kingma & Welling, 2014) to build an unbiased and low-variance gradient estimator for the variational distribution.
35	10	We defer discussion of the inference process until the next section.
43	14	More specifically, conditioned on a Gaussian sample x ∈ RH , the breaking proportions η ∈ RK−1 are generated by applying the sigmoid function η = sigmoid(WT2 x) where W ∈ RH×K−1.
51	16	Recurrent Neural Networks (RNN) are commonly used for modelling sequences of inputs in deep learning.
54	22	The fRNN(x) is decomposed as: hk = RNNSB(hk−1) ηk = sigmoid(hTk−1x) where hk is the output of the kth state, which we feed into the next state of the RNNSB as an input.
57	12	Here, the RNN is able to dynamically produce new logits to break the stick ad infinitum.
75	45	With the RSB construction we can model an unbounded number of topics, however in addition to the RNNSB that generates the topic proportions θ ∈ R∞ for each document, we must introduce another neural network RNNTopic to produce the topics t ∈ R∞×H dynamically, so as to avoid the need to truncate the variational inference.
76	23	For comparison, in finite neural topic models we have topic vectors t ∈ RK×H , while in unbounded neural topic models the topics t ∈ R∞×H are dynamically generated by RNNTopic and the order of the topics corresponds to the order of the states in RNNSB.
77	10	The generation of β follows: tk = RNNTopic(tk−1), βk = softmax(v · tTk ), where v ∈ RV×H represents the word vectors, tk is the kth topic generated by RNNTopic and k < ∞.
78	18	Figure 4 illustrates the neural structure of RNNTopic.
90	17	Miao et al. (2016) proposed a neural variational document model (NVDM) implemented as a variational auto-encoder (Kingma & Welling, 2014), which has a very similar neural structure to our models.
92	29	(5) In the GSM construction, if we replace the generative distribution (Equation 4) with the above distribution (Equation 5) and remove the softmax function over θ, it reduces to a variant of the NVDM model (GSM applies topic and word vectors to compute β, while NVDM directly models β).
93	14	Srivastava & Sutton (2016) interpret the above decoder as a weighted product of experts topic model, but do not model the topics explicitly.
116	83	For the recurrent stick breaking construction we use a one layer LSTM cell (256 hidden units) for constructing the recurrent neural network.
124	13	Table 1 presents the test document perplexities of the topic models on the three datasets.
125	30	Amongst the finite topic models, the Gaussian softmax construction (GSM) achieves the lowest perplexity in most cases, while all of the GSM, GSB and RSB models are significantly better than the benchmark LDA and NVLDA models.
127	168	Here we see that the recurrent neural topic model performs significantly better than the HDP topic model on perplexity.
129	27	Table 2 compares the proposed neural document models with the benchmarks.
130	72	According to our experimental results, the generalisation abilities of the GSM, GSB and RSB models are all improved by switching to an implicit topic distribution, and their performance is also significantly better than the NVDM and ProdLDA.
132	23	Interestingly, the RSB model performs better than the GSM and GSB on 20NewsGroups in both the 50 and 200 topic settings.
133	25	This is possibly due to the fact that GSM and GSB apply linear transformations W1 andW2 to generate the hidden variable θ and breaking proportions η from a Gaussian draw, while the RSB applies recurrent neural networks to produce η in a sequence which induces dependencies in η and helps escape local minima.
134	41	It is worth noting that the recurrent neural network uses more parameters than the other two models.
135	104	As mentioned in Section 3.3, GSM is a variant of NVDM that applies topic and word vectors to construct the topic distribution over words instead of directly modelling a multinomial distribution by a softmax function, which further simplifies optimisation.
136	15	If it is not necessary to model the explicit topic distribution over words, using an implicit topic distribution may lead to better generalisation.
137	13	To further demonstrate the effectiveness of the stickbreaking construction, Figure 5 presents the average probability of each topic by estimating the posterior probability q(z|d) of each document from 20NewsGroups.
138	13	Here we set the number of topics to 400, which is large enough for this dataset.
