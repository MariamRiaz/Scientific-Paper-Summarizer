0,1,label2,summary_sentences
"Given a graph G = (V,E) on n vertices with adjacency matrix A ∈ {0, 1}n×n, the problem of graph embedding is to map the vertices of G to some d-dimensional vector space S in such a way that geometry in S reflects the topology of G. For example, we may ask that vertices with high conductance in G be assigned to nearby vectors in S.",1. Introduction,[0],[0]
"This is a special case of the problem of dimensionality reduction, well-studied in machine learning and related disciplines (van der Maaten et al., 2009).",1. Introduction,[0],[0]
"When applied to graph data, each vertex in G is described by an n-dimensional binary
1Department of Statistics, University of Michigan, USA.",1. Introduction,[0],[0]
"2School of Mathematics and Physics, University of Queensland, Australia.",1. Introduction,[0],[0]
"3International Computer Science Institute, Berkeley, USA.",1. Introduction,[0],[0]
"4Department of Statistics, University of California at Berkeley, USA.",1. Introduction,[0],[0]
"5Department of Applied Mathematics and Statistics, Johns Hopkins University, USA.",1. Introduction,[0],[0]
"Correspondence to: Keith Levin <klevin@umich.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
vector, namely its corresponding column (or row) in adjacency matrix A ∈ {0, 1}n×n, and we wish to associate with each vertex v ∈ V a lower-dimensional representation, say xv ∈ S.",1. Introduction,[0],[0]
"The two most commonly-used approaches for graph embeddings are the graph Laplacian embedding and its variants (Belkin & Niyogi, 2003; Coifman & Lafon, 2006) and the adjacency spectral embedding (ASE, Sussman et al., 2012).",1. Introduction,[0],[0]
"Both of these embedding procedures produce low-dimensional representations of the vertices in a graph G, and the question of “which embedding is preferable?” is dependent on the downstream task.",1. Introduction,[0],[0]
"Indeed, one can show that neither embedding dominates the other for the purposes of vertex classification; see, for example, Section 4.3 of Tang & Priebe (to appear).",1. Introduction,[0],[0]
"In addition, the results in Section 4.3 of Tang & Priebe (to appear) suggest that ASE performs better than the Laplacian eigenmaps embedding for graphs that exhibit a core-periphery structure.",1. Introduction,[0],[0]
"Such structures are ubiquitous in real networks, such as those arising in social and biological sciences (Jeub et al., 2015; Leskovec et al., 2009).
",1. Introduction,[0],[0]
"The ASE and Laplacian embedding differ in that the latter has received far more attention, especially with respect to questions of limit objects (Hein et al., 2005) and out-ofsample extensions (Bengio et al., 2003).",1. Introduction,[0],[0]
The aim of this paper is to establish theoretical foundations for the latter of these two problems in the case of the adjacency spectral embedding.,1. Introduction,[0],[0]
"In the standard out-of-sample (OOS) extension, we are presented with training dataD",2. Background and Notation,[0],[0]
"= {z1, z2, . . .",2. Background and Notation,[0],[0]
", zn} ⊆ X , where X is the set of possible observations.",2. Background and Notation,[0],[0]
"The data D give rise to a symmetric matrix M = [K(zi, zj)] ∈",2. Background and Notation,[0],[0]
"Rn×n, where K : X × X → R≥0 is a kernel function that measures similarity between elements of X , so that K(y, z) is large if y, z ∈ X are similar, and is small otherwise.",2. Background and Notation,[0],[0]
"Suppose that we have computed an embedding of the data D. Let us denote this embedding by X ∈ Rn×d, so that the embedding of zi ∈ D is given by the i-th row of X .",2. Background and Notation,[0],[0]
"Suppose that we are given an additional observation z ∈ X , not necessarily included in D, and we wish to embed z under the same scheme as was used to produce X .",2. Background and Notation,[0],[0]
"A naı̈ve approach would be to discard the old embedding X , consider the augmented
collection D = D ∪ {z} and construct a new embedding X̃ ∈ R(n+1)×d.",2. Background and Notation,[0],[0]
"However, in many applications, it is infeasible to compute this embedding again from scratch, either because of computational constraints or because the similarities {K(zi, zj) : zi, zj ∈ D} may no longer be available after X has been computed.",2. Background and Notation,[0],[0]
"Thus, the OOS problem is to embed z using only the available embedding X which was initially learned from D and the similarities {K(zi, z)}ni=1.
",2. Background and Notation,[0],[0]
"As an example, consider the Laplacian eigenmaps embedding (Belkin & Niyogi, 2003; Belkin et al., 2006).",2. Background and Notation,[0],[0]
"Given a graph G = (V,E) with adjacency matrix A ∈ Rn×n, the d-dimensional normalized Laplacian of G is the matrix L = D−1/2AD−1/2, where D ∈ Rn×n is the diagonal degree matrix, i.e., dii = ∑ j Aij is the degree of the vertex",2. Background and Notation,[0],[0]
"i (Luxburg, 2007; Vishnoi, 2013).",2. Background and Notation,[0],[0]
"The d-dimensional normalized Laplacian eigenmaps embedding of G is given by the rows of the matrix UL ∈ Rn×d, whose columns are the d orthonormal eigenvectors corresponding to the top d eigenvalues of L, excepting the trivial eigenvalue 1.",2. Background and Notation,[0],[0]
"We note that some authors (see, for example, Chung, 1997) use I −D−1/2AD−1/2 to be the normalized graph Laplacian, but since this matrix has the same eigenspace as our L, results concerning the eigenvectors of either of these matrices are equivalent.",2. Background and Notation,[0],[0]
"Suppose that a vertex v is added to graph G, to form graph G̃ with adjacency matrix
Ã =",2. Background and Notation,[0],[0]
"[ A ~a ~aT 0 ] , (1)
where ~a ∈ {0, 1}n.",2. Background and Notation,[0],[0]
A naı̈ve approach to embedding G̃ would be to compute the top eigenvectors of the graph Laplacian of G̃ as before.,2. Background and Notation,[0],[0]
"However, the OOS extension problem requires that we only use the information available in UL and ~a to compute an embedding of the new vertex v.
Bengio et al. (2003) presented out-of-sample extensions for multidimensional scaling (MDS, Torgerson, 1952; Borg & Groenen, 2005), spectral clustering (Weiss, 1999; Ng et al., 2002), Laplacian eigenmaps (Belkin & Niyogi, 2003) and ISOMAP (Tenenbaum et al., 2000).",2. Background and Notation,[0],[0]
"These OOS extensions were based on a least-squares formulation of the embedding problem, arising from the fact that the in-sample embeddings are given by functions of the eigenvalues and eigenfunctions.",2. Background and Notation,[0],[0]
Trosset & Priebe (2008) considered a different OOS extension for MDS.,2. Background and Notation,[0],[0]
"Rather than following the approach of Bengio et al. (2003), Trosset & Priebe (2008) cast the MDS OOS extension as a simple modification of the in-sample MDS optimization problem.
",2. Background and Notation,[0],[0]
"Let {(λt, vt)}nt=1 be the eigen-pairs of the matrix M , constructed from some suitably-chosen similarity function, K, defined on pairs of observations in D ×D. In general, OOS extensions for eigenvector-based embeddings can be derived as in Bengio et al. (2003) as the solution of a least-squares
problem
min f(x)∈Rd n∑ i=1
( K(x, xi)− 1
n d∑ t=1 λtft(xi)ft(x)
)2 ,
where {xi}ni=1 are the in-sample observations, and ft(xi) =",2. Background and Notation,[0],[0]
[vt]i is ith component of vt.,2. Background and Notation,[0],[0]
Belkin et al. (2006) presented a slightly different approach that incorporates regularization in both the intrinsic geometry of the data distribution and the geometry of the similarity function K.,2. Background and Notation,[0],[0]
Their approach applies to Laplacian eigenmaps as well as to regularized least squares and SVM.,2. Background and Notation,[0],[0]
"The authors also introduced a Laplacian SVM, in which a Laplacian penalty term is added to the standard SVM objective function.",2. Background and Notation,[0],[0]
Belkin et al. (2006) showed that all of these embeddings have OOS extensions that arise as the solution of a generalized eigenvalue problem.,2. Background and Notation,[0],[0]
We refer the interested reader to Levin et al. (2015) for a practical application of this OOS extension.,2. Background and Notation,[0],[0]
"More recent approaches to OOS extension have avoided altogether the need to solve a least squares or eigenvalue problem by, instead, training a neural net to learn the embedding directly (see, for example, Quispe et al., 2016; Jansen et al., 2017).
",2. Background and Notation,[0],[0]
The only existing work to date on the ASE OOS extension of which we are aware appears in Tang et al. (2013a).,2. Background and Notation,[0],[0]
"The authors considered the OOS extension for ASE applied to latent position graphs (see, for example Hoff et al., 2002), in which each vertex is associated with an element of a vector space and edge probabilities are given by a suitably-chosen inner product.",2. Background and Notation,[0],[0]
"The authors introduced a least-squares OOS extension for embeddings of latent position graphs and proved a theorem, analogous to our Theorem 1, for the error of this extension about the true latent position.",2. Background and Notation,[0],[0]
"Theorem 1 simplifies the proof of the result due to Tang et al. (2013a) for the case of random dot product graphs (see Definition 1 below).
",2. Background and Notation,[0],[0]
"Of crucial importance in assessing OOS extensions, but largely missing from the existing literature, is an investigation of how the OOS estimate compares with the insample embedding.",2. Background and Notation,[0],[0]
"That is, for an out-of-sample observation z ∈ X , how well does its OOS embedding X̂z ∈ Rd, approximate the embedding that would be obtained by considering the full sample D = D ∪ {z}?",2. Background and Notation,[0],[0]
"In this paper, we address this question in the context of the adjacency spectral embedding.",2. Background and Notation,[0],[0]
"In particular, we show in our main results, Theorems 1 and 2, that two different approaches to the ASE OOS extension recover the in-sample embedding at a rate that is, in a certain sense, optimal (see the discussion at the end of Section 4).",2. Background and Notation,[0],[0]
We conjecture that analogous rate results can be obtained for other OOS extensions such as those presented in Bengio et al. (2003).,2. Background and Notation,[0],[0]
We pause briefly to establish notational conventions for this paper.,2.1. Notation,[0],[0]
"For a matrix B ∈ Rn1×n2 , we let σi(B) denote the i-th singular value of B, so that σ1(B) ≥ σ2(B) ≥ · · · ≥ σk(B) ≥ 0, where k = min{n1, n2}.",2.1. Notation,[0],[0]
"For positive integer n, we let [n] = {1, 2, . . .",2.1. Notation,[0],[0]
", n}.",2.1. Notation,[0],[0]
"Throughout this paper, n will index the number of vertices in a hollow graph G, the observed data, and we let",2.1. Notation,[0],[0]
"c > 0 denote a positive constant, not depending on n, whose value may change from line to line.",2.1. Notation,[0],[0]
"For an event E, we let Ec denote its complement.",2.1. Notation,[0],[0]
"We will say that event En, indexed so as to depend on n, occurs with high probability, and write En w.h.p.",2.1. Notation,[0],[0]
", if for some constant > 0, it holds for all suitably large n that Pr[Ecn] ≤ n−(1+ ).",2.1. Notation,[0],[0]
"In this paper, we will show Pr[Ec] ≤",2.1. Notation,[0],[0]
cn−2 any time we wish to show that event E occurs with high probability.,2.1. Notation,[0],[0]
"All our results involve showing that some event En occurs w.h.p., and we note that in all such cases, the Borel-Cantelli Lemma implies that with probability 1, the event Ecn occurs for at most finitely many n. That is, all our finite-sample results can be easily altered to yield corresponding asymptotic results, as well.",2.1. Notation,[0],[0]
"For a function f : Z≥0 → R≥0 and a sequence of random variables {Zn}, we will write Zn = O(f(n))",2.1. Notation,[0],[0]
"if there exists a constant C and a number n0 such that Zn ≤ Cf(n) for all n ≥ n0, and write Zn = O(f(n))",2.1. Notation,[0],[0]
a.s.,2.1. Notation,[0],[0]
if the event Zn ≤ Cf(n) occurs a.s.a.a.,2.1. Notation,[0],[0]
"For a vector x ∈ Rd, we use the unadorned norm ‖x‖ to denote the Euclidean norm of x. For a matrix M ∈ Rn×d, we use the unadorned norm ‖M‖ to denote the operator norm
‖M‖ = max x∈Rd:‖x‖=1 ‖Mx‖
and we use ‖ · ‖2→∞ to denote the matrix operator norm
‖M‖2→∞ = max x:‖x‖=1",2.1. Notation,[0],[0]
"‖Mx‖∞ = max i∈[n] ‖Mi,·‖,
which can be proven via the Cauchy-Schwarz inequality (Horn & Johnson, 2013).",2.1. Notation,[0],[0]
"This latter operator norm will be especially useful for us, in that a bound on ‖M‖2→∞ gives a uniform bound on the rows of matrix M .",2.1. Notation,[0],[0]
The remainder of this paper is structured as follows.,2.2. Roadmap,[0],[0]
"In Section 3, we present two OOS extensions of the ASE.",2.2. Roadmap,[0],[0]
"In Section 4, we prove convergence of these two OOS extensions when applied to random dot product graphs.",2.2. Roadmap,[0],[0]
"In Section 5, we explore the empirical performance of the two extensions presented in Section 3, and we conclude with a brief discussion in Section 6.",2.2. Roadmap,[0],[0]
"Given a graph G encoded by adjacency matrix A ∈ {0, 1}n×n, the adjacency spectral embedding (ASE) pro-
duces a d-dimensional embedding of the vertices ofG, given by the rows of the n-by-d matrix
X̂ = UAS 1/2 A , (2)
where UA ∈ Rn×d is a matrix with orthonormal columns given by the d eigenvectors corresponding to the top d eigenvalues of A, which we collect in the diagonal matrix SA ∈ Rd×d.",3. Out-of-sample Embedding for ASE,[0],[0]
"We note that in general, one would be better-suited to consider the matrix [ATA]1/2, so that all eigenvalues are guaranteed to be nonnegative, but we will see that in the random dot product graph, the model that is the focus of this paper, the top d eigenvalues of A are positive with high probability (see, for example, either Lemma 1 in Athreya et al. (2016) or Observation 2 in Levin et al. (2017), or refer to the technical report, (Levin et al., 2018)).
",3. Out-of-sample Embedding for ASE,[0],[0]
"The random dot product graph (RDPG, Young & Scheinerman, 2007) is an edge-independent random graph model in which the graph structure arises from the geometry of a set of latent positions, i.e., vectors associated to the vertices of the graph.",3. Out-of-sample Embedding for ASE,[0],[0]
"As such, the adjacency spectral embedding is particularly well-suited to this model.
",3. Out-of-sample Embedding for ASE,[0],[0]
Definition 1.,3. Out-of-sample Embedding for ASE,[0],[0]
(Random Dot Product Graph) Let F be a distribution on Rd such that,3. Out-of-sample Embedding for ASE,[0],[0]
xT,3. Out-of-sample Embedding for ASE,[0],[0]
y ∈,3. Out-of-sample Embedding for ASE,[0],[0]
"[0, 1] whenever x, y ∈ suppF , and let X1, X2, . . .",3. Out-of-sample Embedding for ASE,[0],[0]
", Xn be drawn i.i.d.",3. Out-of-sample Embedding for ASE,[0],[0]
from F .,3. Out-of-sample Embedding for ASE,[0],[0]
Collect these n random points in the rows of a matrix X ∈ Rn×d.,3. Out-of-sample Embedding for ASE,[0],[0]
"Suppose that (symmetric) adjacency matrix A ∈ {0, 1}n×n is distributed in such a way that
Pr[A|X] = ∏
1≤i<j≤n
(XTi Xj) Aij (1−XTi Xj)1−Aij .",3. Out-of-sample Embedding for ASE,[0],[0]
"(3)
When this is the case, we write (A,X) ∼ RDPG(F, n).",3. Out-of-sample Embedding for ASE,[0],[0]
"If G is the random graph corresponding to adjacency matrix A, we say that G is a random dot product graph with latent positions X1, X2, . . .",3. Out-of-sample Embedding for ASE,[0],[0]
", Xn, where Xi is the latent position corresponding to the i-th vertex.
",3. Out-of-sample Embedding for ASE,[0],[0]
"A number of results exist showing that the adjacency spectral embedding yields consistent estimates of the latent positions in a random dot product graph (Sussman et al., 2012; Tang et al., 2013b) and recovers community structure in the stochastic block model (Lyzinski et al., 2014).",3. Out-of-sample Embedding for ASE,[0],[0]
"We note an inherent nonidentifiability in the random dot product graph, arising from the fact that for any orthogonal matrix W ∈ Rd×d, the latent positions X ∈ Rn×d and XW ∈ Rd×d give rise to the same distribution over graphs, since XXT = (XW )(XW )T = E[A | X].",3. Out-of-sample Embedding for ASE,[0],[0]
"Owing to this nonidentifiability, we can only hope to recover the latent positions in X up to some orthogonal rotation.",3. Out-of-sample Embedding for ASE,[0],[0]
The reader may notice that the RDPG as defined has the limitation that it can only capture graphs with positive semi-definite expected value.,3. Out-of-sample Embedding for ASE,[0],[0]
"This limitation can be overcome by extending the RDPG to the generalized RDPG (Rubin-Delanchy
et al., 2017).",3. Out-of-sample Embedding for ASE,[0],[0]
"The results stated in the present work can, for the most part, be extended to this generalized model, but we restrict ourselves here to the RDPG as it appears in Definition 1 for the sake of simplicity.
",3. Out-of-sample Embedding for ASE,[0],[0]
"Suppose that, given adjacency matrix A, we compute embedding
X̂ =",3. Out-of-sample Embedding for ASE,[0],[0]
[X̂1X̂2 . . .,3. Out-of-sample Embedding for ASE,[0],[0]
"X̂n] T ,
where X̂i ∈ Rd denotes the embedding of the i-th vertex.",3. Out-of-sample Embedding for ASE,[0],[0]
Now suppose we add a vertex v with latent position w̄ ∈,3. Out-of-sample Embedding for ASE,[0],[0]
"Rd to the original graph G, obtaining an augmented graph G̃ =",3. Out-of-sample Embedding for ASE,[0],[0]
(,3. Out-of-sample Embedding for ASE,[0],[0]
"[n] ∪ {v}, E ∪ Ev), where Ev denotes the set of edges between v and the vertices of G. One would like to embed vertex v according to the same distribution as the original n vertices and obtain an estimate of w̄.",3. Out-of-sample Embedding for ASE,[0],[0]
"Let the binary vector ~a ∈ {0, 1}n encode the edges Ev incident upon vertex v, with entries ai = (~a)i ∼ Bernoulli(XTi w̄).",3. Out-of-sample Embedding for ASE,[0],[0]
The augmented graph G̃ then has the adjacency matrix as in (1).,3. Out-of-sample Embedding for ASE,[0],[0]
"As discussed earlier, the natural approach to embedding vertex v is to simply re-embed the whole matrix G̃ by computing the ASE of Ã. Suppose that we wish to avoid such a computation, for example due to resource constraints.",3. Out-of-sample Embedding for ASE,[0],[0]
The problem then becomes one of embedding the new vertex v based solely on the information present in X̂ and ~a.,3. Out-of-sample Embedding for ASE,[0],[0]
Two natural approaches to such an OOS extension suggest themselves.,3. Out-of-sample Embedding for ASE,[0],[0]
"A natural approach to OOS embedding, pursued by, for example, Bengio et al. (2003), is to embed vertex v as the least-squares solution to X̂w = ~a.",3.1. Linear Least Squares OOS Extension,[0],[0]
"That is, we embed the vertex v as the vector ŵLS solving
min w∈Rd n∑ i=1",3.1. Linear Least Squares OOS Extension,[0],[0]
"( ai − X̂Ti w )2 , (4)
where ai denotes the i-th component of the binary vector ~a encoding the edges between v and the original n vertices.",3.1. Linear Least Squares OOS Extension,[0],[0]
"We will denote the solution to the least-squares optimization in Equation (4) by ŵLS, and term this the linear least squares out-of-sample (LLS OOS) embedding.",3.1. Linear Least Squares OOS Extension,[0],[0]
"A more principled approach to OOS extension, but perhaps more involved computationally, is to consider the following maximum-likelihood formulation.",3.2. Maximum Likelihood OOS Extension,[0],[0]
"The entries of the vector ~a are distributed independently as ai ∼ Bernoulli(XTi w̄), where w̄ denotes the true latent position of OOS vertex v.",3.2. Maximum Likelihood OOS Extension,[0],[0]
"Since we do not have access to the latent positions {Xi}ni=1, we use instead their estimates {X̂i}ni=1.",3.2. Maximum Likelihood OOS Extension,[0],[0]
"This yields the following objective:
max w∈Rd n∑ i=1 ai log X̂ T",3.2. Maximum Likelihood OOS Extension,[0],[0]
i w + (1− ai) log ( 1− X̂Ti w ) .,3.2. Maximum Likelihood OOS Extension,[0],[0]
"(5)
Unfortunately, this optimization problem may fail to achieve its optimum inside the support of F .",3.2. Maximum Likelihood OOS Extension,[0],[0]
"Indeed, it may not even have a finite solution.",3.2. Maximum Likelihood OOS Extension,[0],[0]
"Thus, we will instead settle for solving the following constrained modification of Equation (5),
max w∈T̂ n∑ i=1",3.2. Maximum Likelihood OOS Extension,[0],[0]
ai log,3.2. Maximum Likelihood OOS Extension,[0],[0]
X̂,3.2. Maximum Likelihood OOS Extension,[0],[0]
T,3.2. Maximum Likelihood OOS Extension,[0],[0]
"i w + (1− ai) log ( 1− X̂Ti w ) , (6)
",3.2. Maximum Likelihood OOS Extension,[0],[0]
where T̂ = {w ∈,3.2. Maximum Likelihood OOS Extension,[0],[0]
Rd : ≤ X̂Ti w ≤ 1,3.2. Maximum Likelihood OOS Extension,[0],[0]
"− , i ∈",3.2. Maximum Likelihood OOS Extension,[0],[0]
"[n]}, and > 0 is a small constant.",3.2. Maximum Likelihood OOS Extension,[0],[0]
"We note that this is based only on the edges incident on the OOS vertex rather than on the full data Ã, and uses the spectral estimates {X̂i}ni=1 rather than the true latent positions {Xi}ni=1.",3.2. Maximum Likelihood OOS Extension,[0],[0]
"Despite both of these facts, we will term the extension given by Equation (6) as the maximum-likelihood out-of-sample (ML OOS) extension, and we will let ŵML denote its solution.",3.2. Maximum Likelihood OOS Extension,[0],[0]
"Our main results show that both the linear least-squares and maximum-likelihood OOS extensions in Equations (4) and (6) recover the true latent position w̄ of v. Further, both OOS extensions converge to w̄ at the same asymptotic rate (i.e., up to a constant) as we would have obtained, had we computed the ASE of Ã in (1) directly.",4. Main Results,[0],[0]
"This rate is given by Lemma 2.5 from Lyzinski et al. (2014), which we state here in a slightly adapted form.",4. Main Results,[0],[0]
"The lemma states, in essence, that the ASE recovers the latent positions with error of order n−1/2 log n, uniformly over the n vertices.",4. Main Results,[0],[0]
"We remind the reader that ‖M‖2→∞ denotes the 2-to-∞ operator norm,",4. Main Results,[0],[0]
"‖M‖2→∞ = maxx:‖x‖=1 ‖Mx‖∞. Lemma 1 (Adapted from Lyzinski et al. (2014), Lemma 2.5).",4. Main Results,[0],[0]
Let X =,4. Main Results,[0],[0]
"[X1, X2, . . .",4. Main Results,[0],[0]
", Xn]T ∈ Rn×d be the matrix of latent positions of an RDPG, and let X̂ ∈ Rn×d denote the matrix of estimated latent positions yielded by ASE as in (2).",4. Main Results,[0],[0]
"Then with probability at least 1− cn−2, there exists orthogonal matrix W ∈ Rd×d such that
‖X̂",4. Main Results,[0],[0]
"−XW‖2→∞ ≤ c log n
n1/2 .
",4. Main Results,[0],[0]
"That is, it holds with high probability that for all i ∈",4. Main Results,[0],[0]
"[n],
‖X̂i",4. Main Results,[0],[0]
−WTXi‖ ≤,4. Main Results,[0],[0]
"c log n
n1/2 .
",4. Main Results,[0],[0]
"In what follows, we let A ∈ {0, 1}n×n denote the random adjacency matrix of an RDPGG, and letX1, X2, . . .",4. Main Results,[0],[0]
", Xn ∈ Rd denote its latent positions, collected in matrix X =",4. Main Results,[0],[0]
"[X1, X2, . . .",4. Main Results,[0],[0]
", Xn]
T ∈ Rn×d.",4. Main Results,[0],[0]
"That is, (A,X) ∼ RDPG(F, n).",4. Main Results,[0],[0]
We use X̂ =,4. Main Results,[0],[0]
"[X̂1, X̂2, . .",4. Main Results,[0],[0]
.,4. Main Results,[0],[0]
", X̂n]T ∈ Rn×d to denote the matrix whose rows are the estimated latent positions, obtained via ASE as in (2).",4. Main Results,[0],[0]
"We let w̄ denote the true latent position of the OOS vertex v.
Theorem 1.",4. Main Results,[0],[0]
"With notation as above, let ŵLS denote the least-squares estimate of w̄, i.e., the solution to (4).",4. Main Results,[0],[0]
"Then there exists an orthogonal matrix W ∈ Rd×d such that
‖WŵLS − w̄‖ ≤",4. Main Results,[0],[0]
"cn−1/2 log n w.h.p.
",4. Main Results,[0],[0]
Proof.,4. Main Results,[0],[0]
"The proof of this result relies upon a classic result for solutions of perturbed linear systems to establish that with high probability, ‖WŵLS",4. Main Results,[0],[0]
− wLS‖ ≤,4. Main Results,[0],[0]
"cn−1/2 log n, where W ∈ Rd×d is the orthogonal matrix guaranteed by Lemma 1 andwLS is the LS estimate based on the true latent positions {Xi} rather than on the estimates {X̂i}.",4. Main Results,[0],[0]
"A basic Hoeffding inequality to show that with high probability, ‖wLS − w̄‖ ≤",4. Main Results,[0],[0]
"cn−1/2 log n, where again W ∈ Rd×d is the orthogonal matrix in Lemma 1.",4. Main Results,[0],[0]
A triangle inequality applied to ‖WŵLS− w̄‖ combined with a union bound over the two high-probability events just described yields the result.,4. Main Results,[0],[0]
"A detailed version of this proof can be found in the technical report (Levin et al., 2018).
",4. Main Results,[0],[0]
"As mentioned in Section 3, we would like to consider a maximum-likelihood OOS extension based on the likelihood ˆ̀(w) = ∑n i=1 ai log",4. Main Results,[0],[0]
X̂ T,4. Main Results,[0],[0]
i w+ (1−ai) log(1− X̂Ti w).,4. Main Results,[0],[0]
"Toward this end, we would ideally like to use the solution to the optimization problem
arg max w∈Rd
ˆ̀(w),
but to ensure a sensible solution, we instead consider
ŵML = arg max w∈T̂
ˆ̀(w), (7)
where we remind the reader that T̂ = {w ∈",4. Main Results,[0],[0]
Rd : ≤ X̂Ti w ≤ 1,4. Main Results,[0],[0]
"− , i = 1, 2, . . .",4. Main Results,[0],[0]
", n}.",4. Main Results,[0],[0]
"Theorem 2 shows that ŵML recovers the true latent position of the OOS vertex, up to rotation, with error decaying at the same rate as that obtained in Theorem 1 for the LS OOS extension.
",4. Main Results,[0],[0]
Theorem 2.,4. Main Results,[0],[0]
"With notation as above, let ŵML be the estimate defined in Equation (7), and let > 0 be such that x, y ∈ suppF implies < xT",4. Main Results,[0],[0]
y,4. Main Results,[0],[0]
< 1 − .,4. Main Results,[0],[0]
Denote the true latent position of the OOS vertex v by w̄ ∈ suppF .,4. Main Results,[0],[0]
"Then for all n suitably large, there exists an orthogonal matrix W ∈ Rd×d such that with high probability,
‖WŵML − w̄‖ ≤",4. Main Results,[0],[0]
"cn−1/2 log n w.h.p.,
and this matrix W is the same one guaranteed by Lemma 1.
",4. Main Results,[0],[0]
Proof.,4. Main Results,[0],[0]
"By a standard argument from convex optimization, alongside the definition of T̂ , one can thow that for suitably large n,
‖WŵML − w̄‖ ≤",4. Main Results,[0],[0]
"c‖∇ˆ̀(WT w̄)‖
n w.h.p.
",4. Main Results,[0],[0]
"By the triangle inequality one can then show that
‖∇ˆ̀(WT w̄)‖ ≤",4. Main Results,[0],[0]
c,4. Main Results,[0],[0]
"√ n log n w.h.p.
",4. Main Results,[0],[0]
"A detailed proof can be found in (Levin et al., 2018).
",4. Main Results,[0],[0]
Remark 1.,4. Main Results,[0],[0]
"Given our in-sample embedding X̂ and the vector of edge indicators ~a, we can think of the OOS extension as an estimate of w̄, the latent position of the OOS vertex v. Lemma 1 implies that if we took the naı̈ve approach of applying ASE to the adjacency matrix Ã in (1), our estimate would have error of order at most O(n−1/2 log n).",4. Main Results,[0],[0]
"Theorems 1 and 2 imply that the OOS estimate obtains the same asymptotic estimation error, without recomputing the embedding of Ã.
",4. Main Results,[0],[0]
"In addition to the bounds in Theorems 1 and 2, we can show that the least-squares OOS extension satisfies a stronger property, namely the following central limit theorem.
",4. Main Results,[0],[0]
Theorem 3.,4. Main Results,[0],[0]
"Let (A,X) ∼ RDPG(F, n) be a ddimensional RDPG.",4. Main Results,[0],[0]
Let w̄ ∈,4. Main Results,[0],[0]
suppF,4. Main Results,[0],[0]
"and ŵLS ∈ Rd be, respectively, the latent position and the least-squares embedding from (4) of an OOS vertex v.",4. Main Results,[0],[0]
"There exists a sequence of orthogonal d× d matrices {Vn}∞n=1 such that
√ n(V Tn ŵLS",4. Main Results,[0],[0]
"− w̄) L−→ N (0,Σw̄),
where Σw̄ ∈ Rd×d is given by
Σw̄ = ∆ −1E",4. Main Results,[0],[0]
"[ XT1 w̄(1−XT1 w̄)X1XT1 ] ∆−1, (8)
and ∆ = EX1XT1 .
",4. Main Results,[0],[0]
Proof.,4. Main Results,[0],[0]
"This theorem follows from an adaptation of Theorem 1 in (Levin et al., 2017)",4. Main Results,[0],[0]
"A detailed proof can be found in (Levin et al., 2018).
",4. Main Results,[0],[0]
"If the OOS vertex is distributed according to F , we have the following corollary by integrating w̄ with respect to F .
",4. Main Results,[0],[0]
Corollary 1.,4. Main Results,[0],[0]
"Let (A,X) ∼ RDPG(F, n) be a ddimensional RDPG, and let w̄ be distributed according to F , independent of (A,X).",4. Main Results,[0],[0]
"Then there exists a sequence of orthogonal d× d matrices {Vn}∞n=1 such that
√ n(V Tn ŵLS − w̄) L−→ ∫ N (0,Σw)dF (w),
where Σw is defined as in Equation (8) above.
",4. Main Results,[0],[0]
We conjecture that a CLT analogous to Theorem 3 holds for the ML OOS extension.,4. Main Results,[0],[0]
"In this section, we briefly explore our results through simulations.",5. Experiments,[0],[0]
"We leave a more thorough experimental examination of our results, particularly as they apply to realworld data, for future work.",5. Experiments,[0],[0]
We first give a brief exploration of how quickly the asymptotic distribution in Theorem 3 becomes a good approximation.,5. Experiments,[0],[0]
"Toward this end, let us consider a simple mixture of point masses, F = Fλ,x1,x2 = λδx1 +(1−λ)δx2 , where x1, x2 ∈ R2 and λ ∈",5. Experiments,[0],[0]
"(0, 1).",5. Experiments,[0],[0]
"This corresponds to a two-block stochastic block model (Holland et al., 1983), in which the block probability matrix is given by [
xT1 x1 x T 1 x2",5. Experiments,[0],[0]
xT1 x2 x T 2,5. Experiments,[0],[0]
"x2
] .
",5. Experiments,[0],[0]
"Corollary 1 implies that if all latent positions (including the OOS vertex) are drawn according to F , then the OOS estimate should be distributed as a mixture of normals centered at x1 and x2, with respective mixing coefficients λ and 1− λ.
",5. Experiments,[0],[0]
"To assess how well the asymptotic distribution predicted by Theorem 3 and Corollary 1 holds, we generate RDPGs with latent positions drawn i.i.d.",5. Experiments,[0],[0]
"from distribution F = Fλ,x1,x2 defined above, with
λ = 0.4, x1 = (0.2, 0.7) T , and x2 = (0.65, 0.3)T .
",5. Experiments,[0],[0]
"For each trial, we draw n+ 1 independent latent positions from F , and generate a binary adjacency matrix from these latent positions.",5. Experiments,[0],[0]
We let the (n+1)-th vertex be the OOS vertex.,5. Experiments,[0],[0]
"Retaining the subgraph induced by the first n vertices, we obtain an estimate X̂ ∈",5. Experiments,[0],[0]
"Rn×2 via ASE, from which we obtain an estimate for the OOS vertex via the LS OOS extension as defined in (4).",5. Experiments,[0],[0]
"We remind the reader that for each RDPG draw, we initially recover the latent positions
only up to a rotation.",5. Experiments,[0],[0]
"Thus, for each trial, we compute a Procrustes alignment (Gower & Dijksterhuis, 2004) of the in-sample estimates X̂ to their true latent positions.",5. Experiments,[0],[0]
"This yields a rotation matrix R, which we apply to the OOS estimate.",5. Experiments,[0],[0]
"Thus, the OOS estimates are sensibly comparable across trials.",5. Experiments,[0],[0]
"Figure 1 shows the empirical distribution of the OOS embeddings of 100 independent RDPG draws, for n = 50 (left), n = 100 (center) and n = 500 (right) in-sample vertices.",5. Experiments,[0],[0]
"Each cross is the location of the OOS estimate for a single draw from the RDPG with latent position distribution F , colored according to true latent position.",5. Experiments,[0],[0]
"OOS estimates with true latent position x1 are plotted as blue crosses, while OOS estimates with true latent position x2 are plotted as red crosses.",5. Experiments,[0],[0]
"The true latent positions x1 and x2 are plotted as solid circles, colored accordingly.",5. Experiments,[0],[0]
"The plot includes contours for the two normals centered at x1 and x2 predicted by Theorem 3 and Corollary 1, with the ellipses indicating the isoclines corresponding to one and two (generalized) standard deviations.
",5. Experiments,[0],[0]
"Examining Figure 1, we see that even with only 100 vertices, the mixture of normal distributions predicted by Theorem 3 holds quite well, with the exception of a few gross outliers from the blue cluster.",5. Experiments,[0],[0]
"With n = 500 vertices, the approximation is particularly good.",5. Experiments,[0],[0]
"Indeed, the n = 500 case appears to be slightly under-dispersed, possibly due to the Procrustes alignment.",5. Experiments,[0],[0]
It is natural to wonder whether a similarly good fit is exhibited by the ML-based OOS extension.,5. Experiments,[0],[0]
We conjectured at the end of Section 4 that a CLT similar to that in Theorem 3 would also hold for the ML-based OOS extension as defined in Equation (7).,5. Experiments,[0],[0]
"Figure 2 shows the empirical distribution of 100 independent OOS estimates, under the same experimental setup as Figure 1, but using the ML OOS extension rather than the linear least-squares extension.",5. Experiments,[0],[0]
"The plot supports our conjecture that the ML-based OOS estimates are also approximately normally distributed
about the true latent positions.
",5. Experiments,[0],[0]
Figure 1 suggests that we may be confident in applying the large-sample approximation suggested by Theorem 3 and Corollary 1.,5. Experiments,[0],[0]
"Applying this approximation allows us to investigate the trade-offs between computational cost and classification accuracy, to which we now turn our attention.",5. Experiments,[0],[0]
"The mixture distribution Fλ,x1,x2 above suggests a task in which, given an adjacency matrix A, we wish to classify the vertices according to which of two clusters or communities they belong.",5. Experiments,[0],[0]
"That is, we will view two vertices as belonging to the same community if their latent positions are the same (Holland et al., 1983, i.e., the latent positions specify an SBM,).",5. Experiments,[0],[0]
"More generally, one may view the task of recovering vertex block memberships in a stochastic block model as a clustering problem.",5. Experiments,[0],[0]
"Lyzinski et al. (2014) showed that applying ASE to such a graph, followed by k-means clustering of the estimated latent positions, correctly recovers community memberships of all the vertices (i.e., correctly assigns all vertices to their true latent positions) with high probability.
",5. Experiments,[0],[0]
"For concreteness, let us consider a still simpler mixture model, F = Fλ,p,q = λδp+ (1−λ)δq , where 0 < p",5. Experiments,[0],[0]
"< q < 1, and draw an RDPG (Ã,X) ∼ RDPG(F, n+m), taking the first n vertices to be in-sample, with induced adjacency matrix A ∈ Rn×n.",5. Experiments,[0],[0]
"That is, we draw the full matrix
Ã =",5. Experiments,[0],[0]
"[ A B BT C ] ,
where C ∈ Rm×m is the adjacency matrix of the subgraph induced by them OOS vertices andB ∈ Rn×m encodes the edges between the in-sample vertices and the OOS vertices.",5. Experiments,[0],[0]
"The latent positions p and q encode a community structure in the graph Ã, and, as alluded to above, a common task in network statistics is to recover this community structure.
",5. Experiments,[0],[0]
"Let w̄(1), w̄(2), . . .",5. Experiments,[0],[0]
", w̄(m) ∈ {p, q} denote the true latent positions of the m OOS vertices, with respective least-squares OOS estimates ŵ(1)LS , ŵ (2) LS , . . .",5. Experiments,[0],[0]
", ŵ (m) LS , each obtained from the in-sample ASE X̂ ∈",5. Experiments,[0],[0]
Rn of A.,5. Experiments,[0],[0]
"We note that one could devise a different OOS embedding procedure that makes use of the subgraph C induced by these m OOS vertices, but we leave the development of such a method to future work.",5. Experiments,[0],[0]
Corollary 1 implies that each ŵ(t)LS for t ∈,5. Experiments,[0],[0]
"[m] is marginally (approximately) distributed as
ŵ (t) LS ∼ λN",5. Experiments,[0],[0]
"(p, (n+ 1) −1σ2p) + (1−λ)N (q, (n+ 1)−1σ2q ),
where σ2p = ∆ −2",5. Experiments,[0],[0]
"(λp2(1− p2)p2 + (1− λ)pq(1− pq)q2) ,
σ2q = ∆ −2",5. Experiments,[0],[0]
"(λpq(1− pq)p2 + (1− λ)q2(1− q2)q2) ,
and ∆ =",5. Experiments,[0],[0]
"λp2 + (1− λ)q2.
",5. Experiments,[0],[0]
"Classifying the t-th OOS vertex based on ŵ(t)LS via likelihood ratio thus has (approximate) probability of error
ηn,p,q = λ(1−",5. Experiments,[0],[0]
"Φ (√
n+ 1(xn+1,p,q − p) σp ) + (1− λ)Φ (√ n+ 1(xn+1,p,q − q)
σq
) ,
where Φ denotes the cdf of the standard normal and xn,p,q is the value of x solving
λσ−1p exp{n(x− p)2/(2σ2p)} =",5. Experiments,[0],[0]
"(1− λ)σ−1q exp{n(x− q)2/(2σ2q )},
and hence our overall error rate when classifying the m OOS vertices will grow as mηn+1,p,q .
",5. Experiments,[0],[0]
"As discussed previously, the OOS extension allows us to avoid the expense of computing the ASE of the full matrix
Ã =",5. Experiments,[0],[0]
"[ A B BT C ] .
",5. Experiments,[0],[0]
"The LLS OOS extension is computationally inexpensive, requiring only the computation of the matrix-vector product S−1/2A U T A~a, with a time complexity O(d
2n) (assuming one does not precompute the product S−1/2A U T A ).",5. Experiments,[0],[0]
The eigenvalue computation required for embedding Ã is far more expensive than the LLS OOS extension.,5. Experiments,[0],[0]
"Nonetheless, if one were intent on reducing the OOS classification error ηn+1,p,q, one might consider paying the computational expense of embedding Ã to obtain estimates w̃(1), w̃(2), . . .",5. Experiments,[0],[0]
", w̃(m) of the m OOS vertices.",5. Experiments,[0],[0]
"That is, we obtain estimates for the m OOS vertices by making them insample vertices, at the expense of solving an eigenproblem on the (m + n)-by-(m + n) adjacency matrix.",5. Experiments,[0],[0]
"Of course, the entire motivation of our approach is that the in-sample matrix A may not be available.",5. Experiments,[0],[0]
"Nonetheless, a comparison against this baseline, in which all data is used to compute our embeddings, is instructive.
",5. Experiments,[0],[0]
"Theorem 1 in Athreya et al. (2016) implies that the w̃(t) estimates based on embedding the full matrix Ã are (approximately) marginally distributed as
w̃(t) ∼ λN",5. Experiments,[0],[0]
"(p, (n+m)−1σ2p)+(1−λ)N (q, (n+m)−1σ2q ),
with classification error
ηn+m,p,q = λΦ
( p− xn+m,p,q
σp ) + (1− λ)Φ ( xn+m,p,q",5. Experiments,[0],[0]
"− q
σq
) ,
where xn+m,p,q is the value of x solving
λσ−1p exp{(m+ n)(x− p)2/(2σ2p)}",5. Experiments,[0],[0]
=,5. Experiments,[0],[0]
"(1− λ)σ−1q exp{(m+ n)(x− q)2/(2σ2q )},
and it can be checked that ηn+m,q,p < ηn,q,p when m > 1.",5. Experiments,[0],[0]
"Thus, at the cost of computing the ASE of Ã, we may obtain a better estimate.",5. Experiments,[0],[0]
How much does this additional computation improve classification the OOS vertices?,5. Experiments,[0],[0]
"Figure 3 explores this question.
",5. Experiments,[0],[0]
"Figure 3 compares the error rates of the in-sample and OOS estimates as a function of m and n in the model just described, with λ = 0.4, p = 0.6 and q = 0.61.",5. Experiments,[0],[0]
"The plot depicts the ratio of the (approximate) in-sample classification error η(n+m),p,q to the (approximate) OOS classification error η(n+1),p,q , as a function of the number of OOS vertices m, for differently-sized in-sample graphs, n = 100, 1000, and 10000.",5. Experiments,[0],[0]
"We see that over several magnitudes of graph
size, the in-sample embedding does not improve appreciably over the OOS embedding except when multiple hundreds of OOS vertices are available.",5. Experiments,[0],[0]
"When hundreds or thousands of OOS vertices are available simultaneously, we see in the right-hand side of Figure 3 that the in-sample embedding classification error may improve upon the OOS classification error by a large multiplicative factor.",5. Experiments,[0],[0]
"Whether or not this improvement is worth the additional computational expense will, depend upon the available resources and desired accuracy, but this suggests that the additional expense associated with performing a second ASE computation is only worthwhile in the event that hundreds or thousands of OOS vertices are available simultaneously.",5. Experiments,[0],[0]
"This surfeit of OOS vertices is rather divorced from the typical setting of OOS extension problems, where one typically wishes to embed at most a few previously unseen observations.",5. Experiments,[0],[0]
"We have presented a theoretical investigation of two OOS extensions of the ASE, one based on a linear least squares estimate and the other based on a plug-in maximum-likelihood estimate.",6. Discussion and Conclusion,[0],[0]
"We have also proven a central limit theorem for the LLS-based extension, and simulation suggests that this CLT is a good approximation even with just a few hundred vertices.",6. Discussion and Conclusion,[0],[0]
"We conjecture that a similar CLT holds for the MLbased OOS extension, a conjecture supported by similar simulation data.",6. Discussion and Conclusion,[0],[0]
"Finally, we have given a brief illustration of how this OOS extension and the approximation it introduces might be weighed against the computational expense of recomputing a full graph embedding by examining how vertex classification error depends on the size of the set of OOS vertices.",6. Discussion and Conclusion,[0],[0]
We leave a more thorough exploration of this trade-off for future work.,6. Discussion and Conclusion,[0],[0]
"The authors would like to thank the reviewers for their helpful comments, which markedly improved the quality of this paper.",Acknowledgements,[0],[0]
Keith Levin was partially supported by NSF grant DMS-1646108.,Acknowledgements,[0],[0]
Farbod Roosta-Khorasani was partially supported by the Australian Research Council through a Discovery Early Career Researcher Award (DE180100923).,Acknowledgements,[0],[0]
Carey E. Priebe was supported by the DARPA D3M program through contract FA8750-17-2-0112.,Acknowledgements,[0],[0]
Farbod RoostaKhorasani and Michael Mahoney also gratefully acknowledge support from DARPA D3M.,Acknowledgements,[0],[0]
"Many popular dimensionality reduction procedures have out-of-sample extensions, which allow a practitioner to apply a learned embedding to observations not seen in the initial training sample.",abstractText,[0],[0]
"In this work, we consider the problem of obtaining an out-of-sample extension for the adjacency spectral embedding, a procedure for embedding the vertices of a graph into Euclidean space.",abstractText,[0],[0]
"We present two different approaches to this problem, one based on a least-squares objective and the other based on a maximum-likelihood formulation.",abstractText,[0],[0]
"We show that if the graph of interest is drawn according to a certain latent position model called a random dot product graph, then both of these out-of-sample extensions estimate the true latent position of the out-of-sample vertex with the same error rate.",abstractText,[0],[0]
"Further, we prove a central limit theorem for the least-squares-based extension, showing that the estimate is asymptotically normal about the truth in the large-graph limit.",abstractText,[0],[0]
Out-of-sample extension of graph adjacency spectral embedding,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1136–1145 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"The increasing availability of digitized historical corpora, together with newly developed tools of computational analysis, make the quantitative study of language change possible on a larger scale than ever before.",1 Introduction,[0],[0]
"Thus, many important questions may now be addressed using a variety of NLP tools that were originally developed to study synchronic similarities between words.",1 Introduction,[0],[0]
"This has catalyzed the evolution of an exciting new field
of historical distributional semantics, which has yielded findings that inform our understanding of the dynamic structure of language (Sagi et al., 2009; Wijaya and Yeniterzi, 2011; Mitra et al., 2014; Hilpert and Perek, 2015; Frermann and Lapata, 2016; Dubossarsky et al., 2016).",1 Introduction,[0],[0]
"Recent research has even proposed laws of change that predict the conditions under which the meaning of words is likely to change (Dubossarsky et al., 2015; Xu and Kemp, 2015; Hamilton et al., 2016).",1 Introduction,[0],[0]
"This is an important development, as traditional historical linguistics has generally been unable to provide predictive models of semantic change.
",1 Introduction,[0],[0]
"However, these preliminary results should be addressed with caution.",1 Introduction,[0],[0]
"To date, analyses of changes in words’ meanings have relied on the comparison of word representations at different points in time.",1 Introduction,[0],[0]
Thus any proposed change in meaning is contingent on a particular model of word representation and the method used to measure change.,1 Introduction,[0],[0]
Distributional semantic models typically count words and their co-occurrence statistics (explicit models) or predict the embedding contexts of words (implicit models).,1 Introduction,[0],[0]
"In this paper, we show that the choice of model may introduce biases into the analysis.",1 Introduction,[0],[0]
"We therefore suggest that empirical findings may be used to support laws of semantic change only after a proper control can be shown to eliminate artefactual factors as the underlying cause of the empirical observations.
",1 Introduction,[0],[0]
"Regardless of the specific representation used, a frequent method of measuring the semantic change a word has undergone (Gulordava and Baroni, 2011; Jatowt and Duh, 2014; Kim et al., 2014; Dubossarsky et al., 2015; Kulkarni et al., 2015; Hamilton et al., 2016) is to compare the word’s vector representations between two points in time using the cosine distance:
cosDist(x, y) = 1− x · y‖x‖2‖y‖2 (1)
1136
This choice naturally assumes that greater distances correspond to greater semantic changes.",1 Introduction,[0],[0]
"However, this measure introduces biases that may affect our interpretation of meaning change.
",1 Introduction,[0],[0]
"We examine various representations of word meaning, in order to identify inherent confounds when meaning change is evaluated using the cosine distance.",1 Introduction,[0],[0]
"In addition to the empirical evaluation, in Section 5 we provide an analytical account of the influence of word frequency on cosine distance scores when using these representations.
",1 Introduction,[0],[0]
"In our empirical investigation, we highlight the critical role of control conditions in the validation of experimental findings.",1 Introduction,[0],[0]
"Specifically, we argue that every observation about a change of meaning over time should be subjected to a control test.",1 Introduction,[0],[0]
"The control condition described in Section 2.1 is based on the construction of an artificially generated corpus, which resembles the historical corpus in most respects but where no change of meaning over time exists.",1 Introduction,[0],[0]
"In order to establish the validity of an observation about meaning change - and even more importantly, the validity of a lawlike generalization about meaning change - the result obtained in a genuine experimental condition should be demonstrated to be lacking (or at least significantly diminished) in the control condition.
",1 Introduction,[0],[0]
"As we show in Section 4, some recently reported laws of historical meaning change do not survive this proposed test.",1 Introduction,[0],[0]
"In other words, similar results are obtained in the genuine and control conditions.",1 Introduction,[0],[0]
"These include the correlation of meaning change with word frequency, polysemy (the number of different meanings a word has), and prototypicality (how representative a word is of its category).",1 Introduction,[0],[0]
"These factors lie at the basis of the following proposed laws of semantic change:
• The Law of Conformity, according to which frequency is negatively correlated with semantic change (Hamilton et al., 2016).
",1 Introduction,[0],[0]
"• The Law of Innovation, according to which polysemy is positively correlated with semantic change (Hamilton et al., 2016).
",1 Introduction,[0],[0]
"• The Law of Prototypicality, according to which prototypicality is negatively correlated with semantic change (Dubossarsky et al., 2015).
",1 Introduction,[0],[0]
"Our analysis shows that these laws have only residual effects, suggesting that frequency and
prototypicality may play a smaller role in semantic change than previously claimed.",1 Introduction,[0],[0]
The main artefact underlying the emergence of the first two laws in both the genuine and control conditions may be due to the SVD step used for the embedding of the PPMI word representation (see Section 2.5).,1 Introduction,[0],[0]
The historical corpus used here is Google Books 5-grams of English fiction.,2 Methods,[0],[0]
"Equally sized samples of 10 million 5-grams per year were randomly sampled for the period of 1900-1999 (Kim et al., 2014) to prevent the more prolific publication years from biasing the results, and were grouped into ten-year bins.",2 Methods,[0],[0]
"Uncommon words were removed, keeping the 100,000 most frequent words as the vocabulary for subsequent model learning.",2 Methods,[0],[0]
"All words were lowercased and stripped of punctuation.
",2 Methods,[0],[0]
"This corpus served as the genuine condition, and was used to replicate and evaluate findings from previous studies.",2 Methods,[0],[0]
"In this corpus, words are expected to change their meaning between decadal bins, as they do in a truly random sample of texts.",2 Methods,[0],[0]
"According to the distributional hypothesis (Firth, 1957), one can extract a word’s meaning from the contexts in which it appears.",2 Methods,[0],[0]
"Therefore, if words’ meanings change over time, as has been argued at least since Reisig (1839), it follows that the words’ contexts should change accordingly, and this change should be detected by our model.",2 Methods,[0],[0]
"Complementary to the genuine condition, a control condition was created where no change of meaning is expected.",2.1 Control condition setup,[0],[0]
"Therefore, any observed change in a word’s meaning in the control condition can only stem from random “noise“, while changes in meaning in the genuine condition are attributed to “real“ semantic change in addition to “noise“.",2.1 Control condition setup,[0],[0]
"Two methods were used to construct the corpus in the control condition:
Chronologically shuffled corpus (shuffle): 5- grams were randomly shuffled between decadal bins, so that each bin contained 5-grams from all the decades evenly.",2.1 Control condition setup,[0],[0]
This was chosen as a control condition for two reasons.,2.1 Control condition setup,[0],[0]
"First, this condition resembles the genuine condition in size of the vocabulary, size of the corpus, overall variance in words’ usage, and size of the decadal bins.",2.1 Control condition setup,[0],[0]
"Second and
crucially, words are not expected to show any apparent change in their meaning between decades in the control condition, because their various usage contexts are shuffled across decades.
",2.1 Control condition setup,[0],[0]
"One synchronous corpus (subsample): All 5- grams of the year 1999, which amount to 250 million 5-grams, were selected from Google Books English fiction.",2.1 Control condition setup,[0],[0]
"10 million 5-grams were randomly subsampled from this selection, and this process was repeated 30 times.",2.1 Control condition setup,[0],[0]
This is suggested as an additional control condition since the underlying assumption is always that words in the same year do not change their meaning.,2.1 Control condition setup,[0],[0]
"Again, unlike in the genuine condition, any changes that are observed based on these 30 subsamples can be attributed only to ”noise” that stems from random sampling, rather than real change in meaning.",2.1 Control condition setup,[0],[0]
Meaning change: Meaning change was evaluated as the cosine distance between vector representations of the same word in consecutive decades.,2.2 Measures of interest,[0],[0]
This was done separately for each processing stage (see Section 2.5).,2.2 Measures of interest,[0],[0]
"For the subsample condition, this was defined as the average cosine distance between the vectors in all 30 samples.
",2.2 Measures of interest,[0],[0]
Frequency: Words’ frequencies were computed separately for each decadal bin as the number of times a word appeared divided by the total number of words in that decade.,2.2 Measures of interest,[0],[0]
"For the subsample control condition, it was computed as the number of times a word appeared among the 250 million 5-grams, divided by the total number of words.",2.2 Measures of interest,[0],[0]
"To establish the adequacy of our control condition, we compared the meaning change scores (before log-transformation and standardization) between the genuine and the shuffled control conditions.",2.3 Construct validity,[0],[0]
Change scores were obtained by taking the average meaning change over all words in each decade using the representation of the final processing stage (SVD).,2.3 Construct validity,[0],[0]
"An adequate control condition will exhibit a lower degree of change compared to the genuine condition, and is expected to show a fixed rate of change across decades (see 3a).",2.3 Construct validity,[0],[0]
"Following common practice (Hamilton et al., 2016), the 10k most frequent words, as measured by their average decadal bin frequencies, were
used for the analysis of semantic change.",2.4 Statistical analysis,[0],[0]
"Change scores and frequencies were log-transformed, and all variables were subsequently standardized.
",2.4 Statistical analysis,[0],[0]
A linear mixed effects model was used to evaluate meaning change in both the genuine and shuffled control conditions.,2.4 Statistical analysis,[0],[0]
Frequency was set as a fixed effect while random intercepts were set per word.,2.4 Statistical analysis,[0],[0]
"The model attempts to account for semantic change scores using frequency, while controlling for the variability between words by assuming that each word’s behavior is strongly correlated across decades and independent across words as follows:
∆w(t)i =",2.4 Statistical analysis,[0],[0]
β0 + βffreq (t) wi + zwi,2.4 Statistical analysis,[0],[0]
"+ ε (t) wi (2)
",2.4 Statistical analysis,[0],[0]
"Here ∆w(t)i is the semantic change score of the i’th word measured between two specific consecutive decades, β0 is the model’s intercept, βf is the fixed-effect predictor coefficient for frequency, zwi ∼ N(0, σ) is a random intercept for the i’th word, and ε(t)wi is an error term associated with the i’th word.",2.4 Statistical analysis,[0],[0]
We report the predictor coefficient as well as the proportion of variance explained1 by each model.,2.4 Statistical analysis,[0],[0]
Only statistically significant results (p < .01) are reported.,2.4 Statistical analysis,[0],[0]
All statistical tests are performed in R (lme4 and MuMln packages).,2.4 Statistical analysis,[0],[0]
"We used a cascade of processing stages based on the explicit meaning representation of words (i.e., word counts, PPMI, SVD, as explained below) as commonly practiced (Baroni et al., 2014; Levy et al., 2015).",2.5 Word meaning representation,[0],[0]
"For each of these stages, we sought to evaluate the relationship between word frequency and meaning change, by computing the corresponding correlations between these two factors in the subsample control condition.
",2.5 Word meaning representation,[0.9586840581478586],"['To improve the quality of the author embeddings, we expand the set of author nodes by adding nodes that do the most to densify the author networks: for the follower network, we add additional individuals that are followed by at least a hundred authors in the original set; for the mention and retweet networks, we add all users that have been mentioned or retweeted by at least twenty authors in the original set.']"
"Counts: Co-occurrence counts were collected for all the words in the vocabulary per decade.
",2.5 Word meaning representation,[0],[0]
PPMI:,2.5 Word meaning representation,[0],[0]
Sparse square matrices of vocabulary size containing positive pointwise mutual information (PPMI) scores were constructed for each decade based on the co-occurrence counts.,2.5 Word meaning representation,[0],[0]
"We used the context distribution smoothing parameter α = 0.75, as recommended by (Levy et al., 2015), using the following procedure:
PPMIα(w, c) = max ( log ( P̂ (w, c)
P̂ (w)P̂α(c)
) , 0 ) 1R2 for mixed linear models (Nakagawa and Schielzeth,
2013)
where P̂ (w, c) denotes the probability that word c appears as a context word of w, while P̂ (w) and P̂α(c) =
#(c)α∑ C #(c)
α denote the marginal probabilities of the word and its context, respectively.
SVD:",2.5 Word meaning representation,[0],[0]
"Each PPMI matrix was approximated by a truncated singular value decomposition as described in (Levy et al., 2015).",2.5 Word meaning representation,[0],[0]
"This embedding was shown to improve results on downstream tasks (Baroni et al., 2014; Bullinaria and Levy, 2012; Turney and Pantel, 2010).",2.5 Word meaning representation,[0],[0]
"Specifically, the top 300 elements of the diagonal matrix of singular values Σ, denoted Σd, were retained to represent a new, dense embedding of the word vectors, using the truncated left hand orthonormal matrix Ud:
WSV Di = (Ud · Σd)i (3) These representations were subsequently aligned with the orthogonal Procrustes method following (Hamilton et al., 2016).
",2.5 Word meaning representation,[0],[0]
"Relation to other models: (Levy and Goldberg) have shown that the Skip-Gram with Negative Sampling (SGNS) embedding model, e.g. word2vec (Mikolov et al., 2013) - perhaps the most popular model of word meaning representation, implicitly factorizes the values of the wordcontext PMI matrix.",2.5 Word meaning representation,[0],[0]
"Hence, the optimization goal and the sources of information available to SGNS and our model are in fact very similar.",2.5 Word meaning representation,[0],[0]
We therefore hypothesize that conclusions similar to those reported below can be drawn for SGNS models.,2.5 Word meaning representation,[0],[0]
There are many factors that may confound the measurement of meaning change.,3.1 Confound of frequency,[0],[0]
"Here we focus
on frequency, and investigate the existence of an artefactual relation between frequency and meaning change.",3.1 Confound of frequency,[0],[0]
This is done by evaluating this relation in the subsample control condition.,3.1 Confound of frequency,[0],[0]
"Any changes observed in this condition must be the consequence of inherent noise, since this control condition contains random samples from the same year (and the baseline assumption is that no change can be observed within the same year).
",3.1 Confound of frequency,[0],[0]
We first plotted the change scores that use the representation based on word count vs. word frequency.,3.1 Confound of frequency,[0],[0]
"This resulted in a robust correlation (r = −0.915) between the two variables, as shown in Fig.",3.1 Confound of frequency,[0],[0]
1a (see the analytical account in Section 5).,3.1 Confound of frequency,[0],[0]
"We repeated the same procedure using the PPMI representation, which showed a much weaker correlation with frequency (r = −0.295), see Fig. 1b.
",3.1 Confound of frequency,[0],[0]
"Finally, we repeated the same procedure using the final explicit representation after SVD embedding2, see Fig. 1c.",3.1 Confound of frequency,[0],[0]
"Surprisingly, the negative correlation with frequency was reinstated (r = −0.793).",3.1 Confound of frequency,[0],[0]
"To investigate how this came about,
2Similar results were obtained for the implicit embedding (word2vec-SGNS) described in Section 2.5.
",3.1 Confound of frequency,[0],[0]
we computed the change in the PPMI vectors before and after the low-rank SVD embedding using the cosine-distance.,3.1 Confound of frequency,[0],[0]
"As apparent from Fig. 2, it turns out that the SVD procedure distorts data in an uneven manner - frequent words are distorted less than infrequent words.",3.1 Confound of frequency,[0],[0]
Thus we demonstrate that this reinstatement of correlation between frequency and change scores is merely an artefactual consequence of the truncated SVD factorization.,3.1 Confound of frequency,[0],[0]
Potential confounding factors can be addressed by comparing any experimental finding to a validated control condition.,3.2 Construct validity,[0],[0]
Here we validate the use of the shuffled condition as a proper control.,3.2 Construct validity,[0],[0]
"To this end, the average change scores of words per decade in both the genuine and shuffled conditions are compared within each processing stage.",3.2 Construct validity,[0],[0]
"In the genuine condition, words appear in different usage contexts between decades, while in the shuffled condition they do not, because the random shuffling creates a homogeneous corpus.",3.2 Construct validity,[0],[0]
"Therefore, the validity of the control condition is established if: (a) the change scores are diminished as compared to the genuine condition; (b) change scores are uniform across decades (since decades are shuffled); (c) the variance of change scores is smaller that in the genuine condition.",3.2 Construct validity,[0],[0]
As seen in Fig.,3.2 Construct validity,[0],[0]
"3a, all these requirements are met by the control condition.",3.2 Construct validity,[0],[0]
"Note that the change scores in the shuffled condition are all significantly positive, namely, meaning change allegedly exists in this control condition.",3.2 Construct validity,[0],[0]
"This supports the claim that any measurement is significantly affected by unrelated noise.
",3.2 Construct validity,[0],[0]
"Thus, we have established that the shuffled condition is a suitable control for meaning change.
",3.2 Construct validity,[0],[0]
"While validity was established for each of the processing stages, the most robust effect was seen for the PPMI representation, following by SVD and word counts.",3.2 Construct validity,[0],[0]
In Section 3.1 we used the subsample control condition to establish the confounding effect of frequency on meaning change.,3.3 Accounting for the frequency confound,[0],[0]
We now examine the extent to which this frequency confound exists in a historical corpus.,3.3 Accounting for the frequency confound,[0],[0]
"We do so by comparing the frequency confound between the genuine historical corpus and the shuffled historical corpus.
",3.3 Accounting for the frequency confound,[0],[0]
"To visualize the frequency confound in a manner comparable to the analysis presented in Section 3.1, we again plot change scores vs. frequency, ignoring the time dimension of the data.",3.3 Accounting for the frequency confound,[0],[0]
Fig.,3.3 Accounting for the frequency confound,[0],[0]
3b presents this plot for the genuine condition.,3.3 Accounting for the frequency confound,[0],[0]
"The same analysis is repeated in the shuffled condition, see Fig.",3.3 Accounting for the frequency confound,[0],[0]
"3c.
",3.3 Accounting for the frequency confound,[0],[0]
Both plots reveal a highly significant correlation between change scores and frequency.,3.3 Accounting for the frequency confound,[0],[0]
"Furthermore, the fact that the correlation coefficients are virtually identical in the genuine and shuffled conditions, with r = −0.748 and r = −0.747 respectively, suggests that they are due to artefactual factors in both conditions and not to true change of meaning over time.",3.3 Accounting for the frequency confound,[0],[0]
"In fact, this pattern of results is reminiscent of the spurious pattern we see in Fig.",3.3 Accounting for the frequency confound,[0],[0]
"1c.
",3.3 Accounting for the frequency confound,[0],[0]
"The relation between frequency and meaning change can also be represented by a linear mixed effect model, with the benefit that this model enables the addition of more explanatory variables to the data.",3.3 Accounting for the frequency confound,[0],[0]
"The regression model found frequency to have a negative influence on change scores,
with βf=-0.91 and βf=-0.75, for the genuine and shuffled conditions respectively.",3.3 Accounting for the frequency confound,[0],[0]
"Importantly, frequency accounted for 67% of the variance in the change scores in the genuine condition, and was only slightly diminished in the shuffled condition, accounting for 56% of the variance.",3.3 Accounting for the frequency confound,[0],[0]
Similar results were obtained for the PPMI representation (see Table 1).,3.3 Accounting for the frequency confound,[0],[0]
"We replicated three recent results that were affected by this frequency effect, since they all define change as the word’s cosine distance relative to itself at two time points.",4 Revisiting previous studies,[0],[0]
"These studies report laws of semantic change that measure the role of frequency in semantic change either directly (Law of Conformity), or indirectly through another linguistic variable that is dependent on frequency (Laws of Innovation and Prototypicality).",4 Revisiting previous studies,[0],[0]
"Continuing the work described in Section 3.1, we replicated the model and analysis procedure described in (Hamilton et al., 2016), where two predictors were used together to explain the change scores: frequency and polysemy.",4.1 Laws of conformity and innovation,[0],[0]
"Polysemy, which describes the number of different senses a word has, naturally differs among words, where some words are more polysemous than others (compare bank and date to wine).",4.1 Laws of conformity and innovation,[0],[0]
"Following (Hamilton et al., 2016), we defined polysemy as the words’ secondary connections patterns - the connections between each word’s co-occurring words (using the entries in the PPMI representation for that word).",4.1 Laws of conformity and innovation,[0],[0]
"The more interconnected these secondary connections are, the less polysemic a word is, and vice versa.",4.1 Laws of conformity and innovation,[0],[0]
"Polysemy scores were com-
puted using the authors’ provided code3.",4.1 Laws of conformity and innovation,[0],[0]
We then log-transformed and standardized the polysemy scores.,4.1 Laws of conformity and innovation,[0],[0]
"Next, frequency and polysemy were set as two fixed effect predictors in a linear mixed effect model, like the one described in Section 2.4.
",4.1 Laws of conformity and innovation,[0],[0]
"Thus we were able to replicate the results in the genuine condition as reported in (Hamilton et al., 2016).",4.1 Laws of conformity and innovation,[0],[0]
"Interestingly, the same pattern of results emerged, again, in the shuffled condition (see Table 1).",4.1 Laws of conformity and innovation,[0],[0]
"Importantly, the difference in effect size between conditions, as evaluated by the explained variance of frequency and polysemy together, showed a modest effect of 8% over the shuffled condition, pointing to the conclusion that the putative effects may indeed be real, but to a far lesser extent than had been claimed.",4.1 Laws of conformity and innovation,[0],[0]
"We conclude that adding polysemy to the analysis contributed very little to the model’s predictive power.
",4.1 Laws of conformity and innovation,[0],[0]
"Since the PPMI representation (the explicit representation without dimensionality reduction with SVD) seems much less affected by spurious effects correlated with frequency (see Fig. 1b), we repeated the analysis of frequency described here and in Section 3.1 while using this representation.",4.1 Laws of conformity and innovation,[0],[0]
"The results are listed in Table 1, showing a similar pattern of rather small frequency effect.",4.1 Laws of conformity and innovation,[0],[0]
Prototypicality is the degree to which a word is representative of the category of which it is a member (a robin is a more prototypical bird than a parrot).,4.2 Prototypicality,[0],[0]
"According to the proposed Law of Prototypicality, words with more prototypical meanings will show less semantic change, and vice versa.",4.2 Prototypicality,[0],[0]
"Following (Dubossarsky et al., 2015), we computed words’ prototypicality scores for each decade as the cos-distance between a word’s vec-
3https://github.com/williamleif/histwords
tor and its k-means cluster’s centroid, and extended the analysis to encompass the entire 20th century.",4.2 Prototypicality,[0],[0]
"The previous regression model assumed independence between words, and therefore assigned words to a random effect variable.",4.2 Prototypicality,[0],[0]
"However, when modeling prototypicality, this assumption is invalid as relations between words are what inherently define prototypicality.",4.2 Prototypicality,[0],[0]
"We therefore designed a model in which decades, rather than words, are the random effect variable.
",4.2 Prototypicality,[0],[0]
With this analysis the prototypicality effect seems to be substantiated in two ways.,4.2 Prototypicality,[0],[0]
"First, the addition of prototypicality explains an additional 5% of the variance.",4.2 Prototypicality,[0],[0]
"Second, the effect of prototypicality meets the more stringent requirement of being diminished in the shuffle condition (see Table 1).",4.2 Prototypicality,[0],[0]
"Nevertheless, here too the effect originally reported was found to be drastically reduced after being compared with the proper control.",4.2 Prototypicality,[0],[0]
"We show in Section 5.1 that the average cosine distance between two vectors representing the same word is equivalent to the variance of the population of vectors representing the same word in independent samples, and is therefore always positive.",5 Theoretical analysis,[0],[0]
"This is true for any word vector representation.
",5 Theoretical analysis,[0],[0]
"In Sections 5.2-5.3 we prove that the average cosines distance between two count vectors representing the same word is negatively correlated with the frequency of the word, and positively correlated with the polysemy score of the word.",5 Theoretical analysis,[0],[0]
Lemma 1.,5.1 Sampling variability and the cos distance,[0],[0]
"Assume two random variables x, y of length ‖x‖2 = ‖y‖2 = 1, distributed iid with expected value µ and covariance matrix",5.1 Sampling variability and the cos distance,[0],[0]
Σ.,5.1 Sampling variability and the cos distance,[0],[0]
"The expected value of the cosine distance between them is equal to the sum of the diagonal elements of Σ.
Proof.
E(x− y)2 =E(x− µ)2 + E(y − µ)2+ 2E(x− µ)(y",5.1 Sampling variability and the cos distance,[0],[0]
"− µ)
=2 ∑ E(xi − µi)2 = 2 ∑ V ar(xi)
E(x− y)2 =E(x2) + E(y2)− 2E(x · y) =2− 2E (
x · y ‖x‖2‖y‖2 ) =2E(cosDist(x, y))
",5.1 Sampling variability and the cos distance,[0],[0]
"It follows that E(cosDist(x, y))",5.1 Sampling variability and the cos distance,[0],[0]
"= ∑ V ar(xi) (4)
Implication:",5.1 Sampling variability and the cos distance,[0],[0]
"The average cosine distance between two samples of the same random variables is directly related to the variance of the variable, or the sampling noise.",5.1 Sampling variability and the cos distance,[0],[0]
"This variance should be measured empirically whenever cosine distance is used, since only distances that are larger than the empirical variance can be relied upon to support significant observations.",5.1 Sampling variability and the cos distance,[0],[0]
"Next, we analyze the cosine distance between 2 iid samples from a normalized multinomial random variable.",5.2 Cos distance of count vectors: frequency,[0],[0]
This distribution models the distribution of the count vector representation.,5.2 Cos distance of count vectors: frequency,[0],[0]
"Let ki, 1 ≤",5.2 Cos distance of count vectors: frequency,[0],[0]
"i ≤ m denote the number of times word i appeared in the context of word w, and let m denote the size of the dictionary not including w. Let n = ∑",5.2 Cos distance of count vectors: frequency,[0],[0]
ki denote the number of words in the count vector of w; n determines the word’s frequency score.,5.2 Cos distance of count vectors: frequency,[0],[0]
"Assume that the counts are sampled from the distribution Multinomial(n, ~p), namely
Prob(k1, · · · , km) = (
n
k1 · · · , km
) pk11 · · · pkmm
Lemma 2.",5.2 Cos distance of count vectors: frequency,[0],[0]
"The expected value of the cosine distance between two count vectors x, y sampled iid from this distribution is monotonically decreasing with n.
Proof.",5.2 Cos distance of count vectors: frequency,[0],[0]
"By definition, 1−E[cosDist(x, y)] equals
E
[ x · y
‖x‖2‖y‖2 ]",5.2 Cos distance of count vectors: frequency,[0],[0]
= ∑ i,5.2 Cos distance of count vectors: frequency,[0],[0]
"[ E xi ‖x‖2 ]2 = ∑ i E2i (5)
",5.2 Cos distance of count vectors: frequency,[0],[0]
"We compute the expected value of Ei directly:
Ei = ∑
(k1,··· ,km) ki√∑ j k 2 j
( n
k1 · · · , km
) pk11 · · · pkmm
Using Taylor expansion:
ki√∑ j k 2 j = ki n√",5.2 Cos distance of count vectors: frequency,[0],[0]
"( ∑ j kj n ) 2 −∑l 6=j kjkln2 = ki n
1√ 1−∑l 6=j kjkln2
= ki n
( 1 + ε
2 +O(ε2)
) (6)
where ε = ∑
l 6=j kjkl n2
.",5.2 Cos distance of count vectors: frequency,[0],[0]
"The expected value of the 0-order term with respect to ε in (6) equals pi, which is independent of n.",5.2 Cos distance of count vectors: frequency,[0],[0]
"We conclude the proof by focusing on the first order term with respect to ε in (6), to be denoted f1, showing that its expected value is monotonically decreasing with n. Specifically:
f1 = ∑ ~k ∑ l 6=j ki",5.2 Cos distance of count vectors: frequency,[0],[0]
n,5.2 Cos distance of count vectors: frequency,[0],[0]
kj n kl n,5.2 Cos distance of count vectors: frequency,[0],[0]
"( n k1 · · · , km ) pk11 · · · pkmm
We switch the summation order and compute each expression in the external sum, considering two cases separately:",5.2 Cos distance of count vectors: frequency,[0],[0]
"when l 6= j 6= i∑
(k1,··· ,km)
",5.2 Cos distance of count vectors: frequency,[0],[0]
ki n,5.2 Cos distance of count vectors: frequency,[0],[0]
"kj n kl n
( n
k1 · · · , km
) pk11 · · · pkmm
= n(n− 1)(n− 2)
",5.2 Cos distance of count vectors: frequency,[0],[0]
"n3 pipjpl
When l 6= j =",5.2 Cos distance of count vectors: frequency,[0],[0]
"i w.l.g, we rewrite kikj = ki(ki",5.2 Cos distance of count vectors: frequency,[0],[0]
"− 1) + ki, and the sum above becomes n(n−1)(n−2)
n3 p2i pl + n(n−1) n2 pipl.",5.2 Cos distance of count vectors: frequency,[0],[0]
"Thus
f1 = n− 1 n pi n− 2 n",5.2 Cos distance of count vectors: frequency,[0],[0]
∑,5.2 Cos distance of count vectors: frequency,[0],[0]
"l,j:l 6=j pjpl",5.2 Cos distance of count vectors: frequency,[0],[0]
+,5.2 Cos distance of count vectors: frequency,[0],[0]
"(1− pi) 
",5.2 Cos distance of count vectors: frequency,[0],[0]
"and it readily follows that f1 is monotonically increasing with n.
Since n measures the frequency score of word w, it follows from (5) that the expected value of the cosine distance between two iid samples from the distribution of the count vector of w is monotonically decreasing with the word’s frequency.",5.2 Cos distance of count vectors: frequency,[0],[0]
We start our investigation of polysemy by modeling the distribution of the parameters of the multinomial distribution from which count vectors are sampled.,5.3 Cos distance of count vectors: polysemy,[0],[0]
"A common prior distribution on the vector ~pw in m-simplex, which defines the multinomial distribution generating the context of word w, is the Dirichlet distribution f(~pw; ~αw) =",5.3 Cos distance of count vectors: polysemy,[0],[0]
"f(p1, · · · , pm;α1, · · · , αm).",5.3 Cos distance of count vectors: polysemy,[0],[0]
"~αw is a sparse vector of prior counts on all the words in the dictionary, by which the cooccurrence context of word w is modeled.",5.3 Cos distance of count vectors: polysemy,[0.9534180127037377],"['Our approach is inspired by ensemble learning, where the system prediction is the weighted combination of the outputs of several basis models.']"
"We divide the set of none-zero indices of ~αw into two subsets: i1, · · · , im0 correspond to the words which always appear in the context of w, while j1, · · · , im1 correspond to the words which appear in the context of w in one given meaning.",5.3 Cos distance of count vectors: polysemy,[0],[0]
"If w is
polysemous and has two meanings, then there is a third set of indices k1, · · · , km2 which correspond to the words appearing in the context of w in its second meaning.",5.3 Cos distance of count vectors: polysemy,[0],[0]
"If w has more then two meanings, they can be modeled with additional sets of disjoint indices.
",5.3 Cos distance of count vectors: polysemy,[0],[0]
Lemma 3.,5.3 Cos distance of count vectors: polysemy,[0],[0]
"Under certain conditions specified in the proof, given two count vectors x, y sampled iid from the above distribution of w, the expected value of the cosine distance between them increases with the number of sets of disjoint indices which represent different meanings of w.
Proof.",5.3 Cos distance of count vectors: polysemy,[0],[0]
"We will prove that when w has two meanings, the expected value of the cosine distance is larger than in the case of a single meaning.",5.3 Cos distance of count vectors: polysemy,[0],[0]
"The proof for the general case immediately follows.
",5.3 Cos distance of count vectors: polysemy,[0],[0]
"Starting from (6) while keeping only the 0-order term in ε, it follows from the derivations in the proof of Lemma 2 that the expected cosine distance between two count vector samples of w, to be denotedM , is 1−∑ p2i .",5.3 Cos distance of count vectors: polysemy,[0],[0]
"In our current model ~p is a random variable, and we shall compute the expected value of this random variable under the two conditions, when w has either one or two meanings.
",5.3 Cos distance of count vectors: polysemy,[0],[0]
"We start by observing that, given the definition of the Dirichlet distribution, it follows that
E(p2i ) =V ar(pi) + E(pi) 2 = αi(1 + αi) α0(1 + α0)
αo = ∑ αi
=⇒M = ∑ E(p2i ) =",5.3 Cos distance of count vectors: polysemy,[0],[0]
"α0 + ∑ α2i
α0(1 + α0) (7)
Considering the different sets of indices in isolation, let ϕo = ∑im0",5.3 Cos distance of count vectors: polysemy,[0],[0]
"i=i1 αi, ϕ1 = ∑jm1",5.3 Cos distance of count vectors: polysemy,[0],[0]
"i=j1 αi, and
ϕ2 = ∑km2
i=k1 αi.",5.3 Cos distance of count vectors: polysemy,[0],[0]
Let ψo,5.3 Cos distance of count vectors: polysemy,[0],[0]
"= ∑im0 i=i1
α2i , ψ1 =∑jm1 i=j1 α2i , and ψ2 = ∑km2",5.3 Cos distance of count vectors: polysemy,[0],[0]
"i=k1 α2i .
",5.3 Cos distance of count vectors: polysemy,[0],[0]
"We rewrite (7) for the two conditions:
1.",5.3 Cos distance of count vectors: polysemy,[0],[0]
"w has one meaning:
M (1) = ϕ0 + ϕ1 + ψ0 + ψ1
(ϕ0 + ϕ1)(1 + ϕ0 + ϕ1)
2.",5.3 Cos distance of count vectors: polysemy,[0],[0]
"w has two meanings:
M (2) = ϕ0 + ϕ1 + ϕ2 + ψ0 + ψ1 + ψ2
(ϕ0 + ϕ1 + ϕ2)(1 + ϕ0 + ϕ1 + ϕ2)
",5.3 Cos distance of count vectors: polysemy,[0],[0]
"With some algebraic manipulations, it can be shown that M (1) > M (2) if the following holds:
(ϕ0 + ϕ1)2ϕ2",5.3 Cos distance of count vectors: polysemy,[0],[0]
+ (ψ0 + ψ1)ϕ22,5.3 Cos distance of count vectors: polysemy,[0],[0]
(8) +2(ψ0 + ψ1)(ϕ0 + ϕ1)ϕ2,5.3 Cos distance of count vectors: polysemy,[0],[0]
+ (ψ0 + ψ1)ϕ2 +(ϕ0 + ϕ1)(ϕ22,5.3 Cos distance of count vectors: polysemy,[0],[0]
"− ψ2) > ψ2(ϕ0 + ϕ1)2
Thus when (8) holds, the average cosine distance between two samples of a certain word w gets larger as w acquires more meanings.
(8) readily holds under reasonable conditions, e.g., when the prior counts for each meaning are similar (as a set) and much bigger than the prior counts of the joint context words (i.e., ϕ0 = ψ0 = ε, ϕ1 = ϕ2, ψ1 = ψ2).",5.3 Cos distance of count vectors: polysemy,[0],[0]
In this article we have shown that some reported laws of semantic change are largely spurious results of the word representation models on which they are based.,6 Conclusions and discussion,[0],[0]
"While identifying such laws is probably within the reach of NLP analyses of massive digital corpora, we argued that a more stringent standard of proof is necessary in order to put them on a firm footing.",6 Conclusions and discussion,[0],[0]
"Specifically, it is necessary to demonstrate that any proposed law of change has to be observable in the genuine condition, but to be diminished or absent in a control condition.",6 Conclusions and discussion,[0],[0]
"We replicated previous studies claiming to establish such laws, which propose that semantic change is negatively correlated with frequency and prototypicality, and positively correlated with polysemy.",6 Conclusions and discussion,[0],[0]
"None of these laws - at least in their strong versions - survived the more stringent standard of proof, since the observed correlations were found in the control conditions.
",6 Conclusions and discussion,[0],[0]
"In our analysis, the Law of Conformity, which claims a negative correlation between word frequency and meaning change, was shown to have a much smaller effect size than previously claimed.",6 Conclusions and discussion,[0],[0]
This indicates that word frequency probably does play a role - but a small one - in semantic change.,6 Conclusions and discussion,[0],[0]
"According to the Law of Innovation, polysemy was claimed to correlate positively with meaning change.",6 Conclusions and discussion,[0],[0]
"However, our analysis showed that polysemy is highly collinear with frequency, and as such, did not demonstrate independent contribution to semantic change.",6 Conclusions and discussion,[0],[0]
"For similar reasons, the alleged role of prototypicality was diminished.
",6 Conclusions and discussion,[0],[0]
"These results may be more consonant than previous ones with the findings of historical linguis-
tics, as it is commonly assumed that the factors leading to semantic change are more diverse than purely distributional factors.",6 Conclusions and discussion,[0],[0]
"For example, sociocultural, political, and technological changes are known to impact semantic change (Bochkarev et al., 2014; Newman, 2015).",6 Conclusions and discussion,[0],[0]
"Furthermore, some regularities of semantic change have been imputed to ‘channel bias‘, inherent biases of utterance production and interpretation on the part of speakers and listeners, e.g., (Moreton, 2008).",6 Conclusions and discussion,[0],[0]
"As such, it would be surprising if word frequency, polysemy, and prototypicality were to capture too high a degree of variance.",6 Conclusions and discussion,[0],[0]
"In other words, since semantic change may result from the interaction of many factors, small effects may be a priori more credible than large ones.
",6 Conclusions and discussion,[0],[0]
The results of our empirical analysis showed that the spurious effects of frequency were much weaker for the explicit PPMI representation unaugmented by SVD dimensionality reduction.,6 Conclusions and discussion,[0],[0]
We therefore conclude that the artefactual frequency effects reported are inherent to the type of word representations upon which these analyses are based.,6 Conclusions and discussion,[0],[0]
"As the analytical proof in Section 5 demonstrates, it is count vectors that introduce an artefactual dependence on word frequency.
",6 Conclusions and discussion,[0],[0]
"Intuitively, one might expect that the average value for the cosine distance between a given word’s vector in any two samples would be 0.",6 Conclusions and discussion,[0],[0]
"However, Lemma 1 above shows that this is not the case, and the average distance is the variance of the population of vectors representing the same word.",6 Conclusions and discussion,[0],[0]
This result is independent of the specific method used to represent words as vectors.,6 Conclusions and discussion,[0],[0]
"Lemma 2 proves that the average cosine distance between two samples of the same word, when using count vector representations, is negatively correlated with the word’s frequency.",6 Conclusions and discussion,[0],[0]
"Thus, the role of frequency cannot be evaluated as an independent predictor in any model based on count vector representations.",6 Conclusions and discussion,[0],[0]
"It remains for future research to establish whether other approaches to word representation, e.g. (Blei et al., 2003; Mikolov et al., 2013), have inherent biases.
",6 Conclusions and discussion,[0],[0]
"While our findings may seem to be mainly negative, since they invalidate proposed laws of semantic change, we would like to point to the positive contribution made by articulating more stringent standards of proof and devising replicable control conditions for future research on language change based on distributional semantics representations.",6 Conclusions and discussion,[0],[0]
This article evaluates three proposed laws of semantic change.,abstractText,[0],[0]
"Our claim is that in order to validate a putative law of semantic change, the effect should be observed in the genuine condition but absent or reduced in a suitably matched control condition, in which no change can possibly have taken place.",abstractText,[0],[0]
Our analysis shows that the effects reported in recent literature must be substantially revised: (i) the proposed negative correlation between meaning change and word frequency is shown to be largely an artefact of the models of word representation used; (ii) the proposed negative correlation between meaning change and prototypicality is shown to be much weaker than what has been claimed in prior art; and (iii) the proposed positive correlation between meaning change and polysemy is largely an artefact of word frequency.,abstractText,[0],[0]
"These empirical observations are corroborated by analytical proofs that show that count representations introduce an inherent dependence on word frequency, and thus word frequency cannot be evaluated as an independent factor with these representations.",abstractText,[0],[0]
Outta Control: Laws of Semantic Change and Inherent Biases in Word Representation Models,title,[0],[0]
Words can mean different things to different people.,1 Introduction,[0],[0]
"Fortunately, these differences are rarely idiosyncratic, but are often linked to social factors, such as age (Rosenthal and McKeown, 2011), gender (Eckert and McConnell-Ginet, 2003), race (Green, 2002), geography (Trudgill, 1974), and more ineffable characteristics such as political and cultural attitudes (Fischer, 1958; Labov, 1963).",1 Introduction,[0],[0]
"In natural language processing (NLP), social media data has brought variation to the fore, spurring the development of new computational techniques for charac-
terizing variation in the lexicon (Eisenstein et al., 2010), orthography (Eisenstein, 2015), and syntax (Blodgett et al., 2016).",1 Introduction,[0],[0]
"However, aside from the focused task of spelling normalization (Sproat et al., 2001; Aw et al., 2006), there have been few attempts to make NLP systems more robust to language variation across speakers or writers.
",1 Introduction,[0],[0]
"One exception is the work of Hovy (2015), who shows that the accuracies of sentiment analysis and topic classification can be improved by the inclusion of coarse-grained author demographics such as age and gender.",1 Introduction,[0],[0]
"However, such demographic information is not directly available in most datasets, and it is not yet clear whether predicted age and gender offer any improvements.",1 Introduction,[0],[0]
"On the other end of the spectrum are attempts to create personalized language technologies, as are often employed in information retrieval (Shen et al., 2005), recommender systems (Basilico and Hofmann, 2004), and language modeling (Federico, 1996).",1 Introduction,[0],[0]
"But personalization requires annotated data for each individual user—something that may be possible in interactive settings such as information retrieval, but is not typically feasible in natural language processing.
",1 Introduction,[0],[0]
"We propose a middle ground between group-level demographic characteristics and personalization, by exploiting social network structure.",1 Introduction,[0],[0]
"The sociological theory of homophily asserts that individuals are usually similar to their friends (McPherson et al., 2001).",1 Introduction,[0],[0]
"This property has been demonstrated for language (Bryden et al., 2013) as well as for the demographic properties targeted by Hovy (2015), which are more likely to be shared by friends than by random pairs of individuals (Thelwall, 2009).",1 Introduction,[0],[0]
"Social
295
Transactions of the Association for Computational Linguistics, vol. 5, pp.",1 Introduction,[0],[0]
"295–307, 2017.",1 Introduction,[0],[0]
Action Editor: Christopher Potts.,1 Introduction,[0],[0]
"Submission batch: 10/2016; Revision batch: 12/2016; Published 8/2017.
",1 Introduction,[0],[0]
c©2017 Association for Computational Linguistics.,1 Introduction,[0],[0]
"Distributed under a CC-BY 4.0 license.
network information is available in a wide range of contexts, from social media (Huberman et al., 2008) to political speech (Thomas et al., 2006) to historical texts (Winterer, 2012).",1 Introduction,[0],[0]
"Thus, social network homophily has the potential to provide a more general way to account for linguistic variation in NLP.
",1 Introduction,[0],[0]
Figure 1 gives a schematic of the motivation for our approach.,1 Introduction,[0],[0]
"The word ‘sick’ typically has a negative sentiment, e.g., ‘I would like to believe he’s sick rather than just mean and evil.’1",1 Introduction,[0],[0]
"However, in some communities the word can have a positive sentiment, e.g., the lyric ‘this sick beat’, recently trademarked by the musician Taylor Swift.2",1 Introduction,[0],[0]
"Given labeled examples of ‘sick’ in use by individuals in a social network, we assume that the word will have a similar sentiment meaning for their near neighbors—an assumption of linguistic homophily that is the basis for this research.",1 Introduction,[0],[0]
"Note that this differs from the assumption of label homophily, which entails that neighbors in the network will hold similar opinions, and will therefore produce similar document-level labels (Tan et al., 2011; Hu et al., 2013).",1 Introduction,[0],[0]
"Linguistic homophily is a more generalizable claim, which could in principle be applied to any language processing task where author network information is available.
",1 Introduction,[0],[0]
"To scale this basic intuition to datasets with tens of thousands of unique authors, we compress the social network into vector representations of each author node, using an embedding method for large
1Charles Rangel, describing Dick Cheney 2In the case of ‘sick’, speakers like Taylor Swift may employ either the positive and negative meanings, while speakers like Charles Rangel employ only the negative meaning.",1 Introduction,[0],[0]
"In other cases, communities may maintain completely distinct semantics for a word, such as the term ‘pants’ in American and British English.",1 Introduction,[0],[0]
"Thanks to Christopher Potts for suggesting this distinction and this example.
",1 Introduction,[0],[0]
"scale networks (Tang et al., 2015b).",1 Introduction,[0],[0]
"Applying the algorithm to Figure 1, the authors within each triad would likely be closer to each other than to authors in the opposite triad.",1 Introduction,[0],[0]
"We then incorporate these embeddings into an attention-based neural network model, called SOCIAL ATTENTION, which employs multiple basis models to focus on different regions of the social network.
",1 Introduction,[1.0000000770734114],"['We then incorporate these embeddings into an attention-based neural network model, called SOCIAL ATTENTION, which employs multiple basis models to focus on different regions of the social network.']"
"We apply SOCIAL ATTENTION to Twitter sentiment classification, gathering social network metadata for Twitter users in the SemEval Twitter sentiment analysis tasks (Nakov et al., 2013).",1 Introduction,[0],[0]
"We further adopt the system to Ciao product reviews (Tang et al., 2012), training author embeddings using trust relationships between reviewers.",1 Introduction,[0],[0]
SOCIAL ATTENTION offers a 2-3% improvement over related neural and ensemble architectures in which the social information is ablated.,1 Introduction,[0],[0]
It also outperforms all prior published results on the SemEval Twitter test sets.,1 Introduction,[0],[0]
"In the SemEval Twitter sentiment analysis tasks, the goal is to classify the sentiment of each message as positive, negative, or neutral.",2 Data,[0],[0]
"Following Rosenthal et al. (2015), we train and tune our systems on the SemEval Twitter 2013 training and development datasets respectively, and evaluate on the 2013–2015 SemEval Twitter test sets.",2 Data,[0],[0]
Statistics of these datasets are presented in Table 1.,2 Data,[0],[0]
"Our training and development datasets lack some of the original Twitter messages, which may have been deleted since the datasets were constructed.",2 Data,[1.0],"['Our training and development datasets lack some of the original Twitter messages, which may have been deleted since the datasets were constructed.']"
"However, our test datasets contain all the tweets used in the SemEval evaluations, making our results comparable with prior work.
",2 Data,[0],[0]
"We construct three author social networks based on the follow, mention, and retweet relations between the 7,438 authors in the training dataset,
which we refer as FOLLOWER, MENTION and RETWEET.3 Specifically, we use the Twitter API to crawl the friends of the SemEval users (individuals that they follow) and the most recent 3,200 tweets in their timelines.4 The mention and retweet links are then extracted from the tweet text and metadata.",2 Data,[0],[0]
"We treat all social networks as undirected graphs, where two users are socially connected if there exists at least one social relation between them.",2 Data,[0],[0]
"The hypothesis of linguistic homophily is that socially connected individuals tend to use language similarly, as compared to a randomly selected pair of individuals who are not socially connected.",3 Linguistic Homophily,[0],[0]
"We now describe a pilot study that provides support for this hypothesis, focusing on the domain of sentiment analysis.",3 Linguistic Homophily,[0],[0]
"The purpose of this study is to test whether errors in sentiment analysis are assortative on the social networks defined in the previous section: that is, if two individuals (i, j) are connected in the network, then a classifier error on i suggests that errors on j are more likely.
",3 Linguistic Homophily,[0],[0]
"We test this idea using a simple lexicon-based classification approach, which we apply to the SemEval training data, focusing only on messages that are labeled as positive or negative (ignoring the neutral class), and excluding authors who contributed more than one message (a tiny minority).",3 Linguistic Homophily,[0],[0]
"Using the social media sentiment lexicons defined by Tang et al. (2014),5 we label a message as positive if it has at least as many positive words as negative words, and as negative otherwise.6",3 Linguistic Homophily,[0],[0]
"The assortativity is the fraction of dyads for which the classifier makes two correct predictions or two incorrect predictions (Newman, 2003).",3 Linguistic Homophily,[0],[0]
"This measures whether classification errors are clustered on the network.
",3 Linguistic Homophily,[0],[0]
"We compare the observed assortativity against the assortativity in a network that has been randomly
3We could not gather the authorship information of 10% of the tweets in the training data, because the tweets or user accounts had been deleted by the time we crawled the social information.
",3 Linguistic Homophily,[0],[0]
"4The Twitter API returns a maximum of 3,200 tweets.",3 Linguistic Homophily,[0],[0]
"5The lexicons include words that are assigned at least 0.99 confidence by the method of Tang et al. (2014): 1,474 positive and 1,956 negative words in total.
6Ties go to the positive class because it is more common.
",3 Linguistic Homophily,[0],[0]
rewired.7 Each rewiring epoch involves a number of random rewiring operations equal to the total number of edges in the network.,3 Linguistic Homophily,[0],[0]
"(The edges are randomly selected, so a given edge may not be rewired in each epoch.)",3 Linguistic Homophily,[0],[0]
"By counting the number of edges that occur in both the original and rewired networks, we observe that this process converges to a steady state after three or four epochs.",3 Linguistic Homophily,[0],[0]
"As shown in Figure 2, the original observed network displays more assortativity than the randomly rewired networks in nearly every case.",3 Linguistic Homophily,[0],[0]
"Thus, the Twitter social networks display more linguistic homophily than we would expect due to chance alone.
",3 Linguistic Homophily,[0],[0]
"The differences in assortativity across network types are small, indicating that none of the networks are clearly best.",3 Linguistic Homophily,[0],[0]
"The retweet network was the most difficult to rewire, with the greatest proportion of shared edges between the original and rewired networks.",3 Linguistic Homophily,[0],[0]
This may explain why the assortativities of the randomly rewired networks were closest to the observed network in this case.,3 Linguistic Homophily,[0],[0]
"In this section, we describe a neural network method that leverages social network information to improve text classification.",4 Model,[1.0],"['In this section, we describe a neural network method that leverages social network information to improve text classification.']"
"Our approach is inspired by ensemble learning, where the system prediction is the weighted combination of the outputs of several basis models.",4 Model,[0],[0]
"We encourage each basis model to focus on a local region of the social network, so that classification on socially connected individuals employs similar model combinations.
",4 Model,[0.9999999592743379],"['We encourage each basis model to focus on a local region of the social network, so that classification on socially connected individuals employs similar model combinations.']"
"Given a set of instances {xi} and authors {ai}, the goal of personalized probabilistic classification is to estimate a conditional label distribution p(y | x, a).",4 Model,[1.0],"['Given a set of instances {xi} and authors {ai}, the goal of personalized probabilistic classification is to estimate a conditional label distribution p(y | x, a).']"
"For most authors, no labeled data is available, so it is impossible to estimate this distribution directly.",4 Model,[1.0],"['For most authors, no labeled data is available, so it is impossible to estimate this distribution directly.']"
We therefore make a smoothness assumption over a social network G: individuals who are socially proximate in G should have similar classifiers.,4 Model,[0],[0]
"This idea is put into practice by modeling the conditional label distribution as a mixture over the
7Specifically, we use the double edge swap operation of the networkx package (Hagberg et al., 2008).",4 Model,[0],[0]
"This operation preserves the degree of each node in the network.
predictions of K basis classifiers,
p(y | x, a) = K∑
k=1
Pr(Za = k",4 Model,[0],[0]
"| a,G)× p(y | x, Za = k).
",4 Model,[0],[0]
"(1)
The basis classifiers p(y | x, Za = k) can be arbitrary conditional distributions; we use convolutional neural networks, as described in § 4.2.",4 Model,[0],[0]
The component weighting distribution Pr(Za = k,4 Model,[0],[0]
"| a,G) is conditioned on the social network G, and functions as an attentional mechanism, described in § 4.1.",4 Model,[0],[0]
"The basic intuition is that for a pair of authors ai and aj who are nearby in the social network G, the prediction rules should behave similarly if the attentional distributions are similar, p(z | ai, G)",4 Model,[0.9817598094928669],"['The basic intuition is that for a pair of authors ai and aj who are nearby in the social network G, the prediction rules should behave similarly if the attentional distributions are similar, p(z | ai, G) ≈ p(z | aj , G).']"
"≈ p(z | aj , G).",4 Model,[0],[0]
"If we have labeled data only for ai, some of the personalization from that data will be shared by aj .",4 Model,[1.0],"['If we have labeled data only for ai, some of the personalization from that data will be shared by aj .']"
"The overall classification approach can be viewed as a mixture of experts (Jacobs et al., 1991), leveraging the social network as side information to choose the distribution over experts for each author.",4 Model,[0],[0]
The goal of the social attention model is to assign similar basis weights to authors who are nearby in the social networkG. We operationalize social proximity by embedding each node’s social network position into a vector representation.,4.1 Social Attention Model,[0],[0]
"Specifically, we employ the LINE method (Tang et al., 2015b), which estimates D(v) dimensional node embeddings va as parameters in a probabilistic model over edges in the social network.",4.1 Social Attention Model,[0],[0]
"These embeddings are learned solely from the social networkG, without leveraging
any textual information.",4.1 Social Attention Model,[0],[0]
"The attentional weights are then computed from the embeddings using a softmax layer,
Pr(Za = k",4.1 Social Attention Model,[0],[0]
"| a,G) = exp
( φ>k va + bk )",4.1 Social Attention Model,[0],[0]
"∑K
k′ exp ( φ>k′va + bk′ ) .
",4.1 Social Attention Model,[0],[0]
"(2) This embedding method uses only singlerelational networks; in the evaluation, we will show results for Twitter networks built from networks of follow, mention, and retweet relations.",4.1 Social Attention Model,[0],[0]
"In future work, we may consider combining all of these relation types into a unified multi-relational network.",4.1 Social Attention Model,[0],[0]
"It is possible that embeddings in such a network could be estimated using techniques borrowed from multirelational knowledge networks (Bordes et al., 2014; Wang et al., 2014).",4.1 Social Attention Model,[0],[0]
"We next describe the basis models, p(y | x, Z = k).",4.2 Sentiment Classification with Convolutional Neural Networks,[0],[0]
"Because our target task is classification on microtext documents, we model this distribution using convolutional neural networks (CNNs; Lecun et al., 1989), which have been proven to perform well on sentence classification tasks (Kalchbrenner et al., 2014; Kim, 2014).",4.2 Sentiment Classification with Convolutional Neural Networks,[0],[0]
"CNNs apply layers of convolving filters to n-grams, thereby generating a vector of dense local features.",4.2 Sentiment Classification with Convolutional Neural Networks,[0],[0]
"CNNs improve upon traditional bagof-words models because of their ability to capture word ordering information.
",4.2 Sentiment Classification with Convolutional Neural Networks,[0],[0]
Let x =,4.2 Sentiment Classification with Convolutional Neural Networks,[0],[0]
"[h1,h2, · · · ,hn] be the input sentence, where hi is the D(w) dimensional word vector corresponding to the i-th word in the sentence.",4.2 Sentiment Classification with Convolutional Neural Networks,[0],[0]
"We use
one convolutional layer and one max pooling layer to generate the sentence representation of x.",4.2 Sentiment Classification with Convolutional Neural Networks,[0.9999999671960076],['We use one convolutional layer and one max pooling layer to generate the sentence representation of x.']
The convolutional layer involves filters that are applied to bigrams to produce feature maps.,4.2 Sentiment Classification with Convolutional Neural Networks,[0],[0]
"Formally, given the bigram word vectors hi,hi+1, the features generated by m filters can be computed by
ci = tanh(WLhi + WRhi+1 + b), (3)
where ci is an m dimensional vector, WL and WR are m×D(w) projection matrices, and b is the bias vector.",4.2 Sentiment Classification with Convolutional Neural Networks,[0],[0]
"The m dimensional vector representation of the sentence is given by the pooling operation
s = max i∈1,··· ,n−1 ci. (4)
To obtain the conditional label probability, we utilize a multiclass logistic regression model,
",4.2 Sentiment Classification with Convolutional Neural Networks,[0],[0]
"Pr(Y = t | x, Z = k) =",4.2 Sentiment Classification with Convolutional Neural Networks,[0],[0]
"exp(β > t sk + βt)∑T
t′=1 exp(β > t′ sk + βt′)
,
(5) where βt is an m dimensional weight vector, βt is the corresponding bias term, and sk is the m dimensional sentence representation produced by the k-th basis model.",4.2 Sentiment Classification with Convolutional Neural Networks,[0.9672714375673704],"['(4) To obtain the conditional label probability, we utilize a multiclass logistic regression model, Pr(Y = t | x, Z = k) = exp(β > t sk + βt)∑T t′=1 exp(β > t′ sk + βt′) , (5) where βt is an m dimensional weight vector, βt is the corresponding bias term, and sk is the m dimensional sentence representation produced by the k-th basis model.']"
We fix the pretrained author and word embeddings during training our social attention model.,4.3 Training,[0],[0]
"Let Θ denote the parameters that need to be learned, which include {WL,WR,b, {βt, βt}Tt=1} for every basis CNN model, and the attentional weights {φk, bk}Kk=1.",4.3 Training,[1.0],"['Let Θ denote the parameters that need to be learned, which include {WL,WR,b, {βt, βt}Tt=1} for every basis CNN model, and the attentional weights {φk, bk}Kk=1.']"
"We minimize the following logistic loss objective for each training instance:
`(Θ) =",4.3 Training,[0],[0]
"− T∑
t=1
1[Y ∗ = t] log Pr(Y = t | x, a), (6)
where Y ∗ is the ground truth class for x, and 1[·] represents an indicator function.",4.3 Training,[0.9812483417522716],"['We minimize the following logistic loss objective for each training instance: `(Θ) = − T∑ t=1 1[Y ∗ = t] log Pr(Y = t | x, a), (6) where Y ∗ is the ground truth class for x, and 1[·] represents an indicator function.']"
"We train the models for between 10 and 15 epochs using the Adam optimizer (Kingma and Ba, 2014), with early stopping on the development set.",4.3 Training,[0],[0]
"One potential problem is that after initialization, a small number of basis models may claim most of the mixture weights for all the users, while other basis
models are inactive.",4.4 Initialization,[0],[0]
This can occur because some basis models may be initialized with parameters that are globally superior.,4.4 Initialization,[0],[0]
"As a result, the “dead” basis models will receive near-zero gradient updates, and therefore can never improve.",4.4 Initialization,[0],[0]
"The true model capacity can thereby be substantially lower than the K assigned experts.
",4.4 Initialization,[0],[0]
"Ideally, dead basis models will be avoided because each basis model should focus on a unique region of the social network.",4.4 Initialization,[1.0],"['Ideally, dead basis models will be avoided because each basis model should focus on a unique region of the social network.']"
"To ensure that this happens, we pretrain the basis models using an instance weighting approach from the domain adaptation literature (Jiang and Zhai, 2007).",4.4 Initialization,[0],[0]
"For each basis model k, each author a has an instance weight αa,k.",4.4 Initialization,[0],[0]
"These instance weights are based on the author’s social network node embedding, so that socially proximate authors will have high weights for the same basis models.",4.4 Initialization,[0],[0]
"This is ensured by endowing each basis model with a random vector γk ∼ N(0, σ2I), and setting the instance weights as,
αa,k = sigmoid(γ>k va).",4.4 Initialization,[0],[0]
"(7)
The simple design results in similar instance weights for socially proximate authors.",4.4 Initialization,[0],[0]
"During pretraining, we train the k-th basis model by optimizing the following loss function for every instance:
`k = −αa,k T∑
t=1
1[Y ∗ = t] log Pr(Y = t | x, Za = k).
(8) The pretrained basis models are then assembled to-
gether and jointly trained using Equation 6.",4.4 Initialization,[0.9705589704632295],"['During pretraining, we train the k-th basis model by optimizing the following loss function for every instance: `k = −αa,k T∑ t=1 1[Y ∗ = t] log Pr(Y = t | x, Za = k).']"
Our main evaluation focuses on the 2013–2015 SemEval Twitter sentiment analysis tasks.,5 Experiments,[0],[0]
The datasets have been described in § 2.,5 Experiments,[0],[0]
"We train and tune our systems on the Train 2013 and Dev 2013 datasets respectively, and evaluate on the Test 2013– 2015 sets.",5 Experiments,[0],[0]
"In addition, we evaluate on another dataset based on Ciao product reviews (Tang et al., 2012).",5 Experiments,[0],[0]
"We utilize Twitter’s follower, mention, and retweet social networks to train user embeddings.",5.1 Social Network Expansion,[0],[0]
"By querying the Twitter API in April 2015, we were able
to identify 15,221 authors for the tweets in the SemEval datasets described above.",5.1 Social Network Expansion,[0],[0]
"We induce social networks for these individuals by crawling their friend links and timelines, as described in § 2.",5.1 Social Network Expansion,[0],[0]
"Unfortunately, these networks are relatively sparse, with a large amount of isolated author nodes.",5.1 Social Network Expansion,[1.0],"['Unfortunately, these networks are relatively sparse, with a large amount of isolated author nodes.']"
"To improve the quality of the author embeddings, we expand the set of author nodes by adding nodes that do the most to densify the author networks: for the follower network, we add additional individuals that are followed by at least a hundred authors in the original set; for the mention and retweet networks, we add all users that have been mentioned or retweeted by at least twenty authors in the original set.",5.1 Social Network Expansion,[0],[0]
The statistics of the resulting networks are presented in Table 2.,5.1 Social Network Expansion,[1.0],['The statistics of the resulting networks are presented in Table 2.']
"We employ the pretrained word embeddings used by Astudillo et al. (2015), which are trained with a corpus of 52 million tweets, and have been shown to perform very well on this task.",5.2 Experimental Settings,[0],[0]
"The embeddings are learned using the structured skip-gram model (Ling et al., 2015), and the embedding dimension is set at 600, following Astudillo et al. (2015).",5.2 Experimental Settings,[0],[0]
"We report the same evaluation metric as the SemEval challenge: the average F1 score of positive and negative classes.8
Competitive systems We consider five competitive Twitter sentiment classification methods.",5.2 Experimental Settings,[1.000000015372057],['We report the same evaluation metric as the SemEval challenge: the average F1 score of positive and negative classes.8 Competitive systems We consider five competitive Twitter sentiment classification methods.']
"Convolutional neural network (CNN) has been described in § 4.2, and is the basis model of SOCIAL ATTENTION.",5.2 Experimental Settings,[0],[0]
"Mixture of experts employs the same CNN model as an expert, but the mixture densi-
8Regarding the neutral class: systems are penalized with false positives when neutral tweets are incorrectly classified as positive or negative, and with false negatives when positive or negative tweets are incorrectly classified as neutral.",5.2 Experimental Settings,[0],[0]
"This follows the evaluation procedure of the SemEval challenge.
ties solely depend on the input values.",5.2 Experimental Settings,[0],[0]
We adopt the summation of the pretrained word embeddings as the sentence-level input to learn the gating function.9,5.2 Experimental Settings,[0],[0]
The model architecture of random attention is nearly identical to SOCIAL ATTENTION:,5.2 Experimental Settings,[0],[0]
"the only distinction is that we replace the pretrained author embeddings with random embedding vectors, drawing uniformly from the interval (−0.25, 0.25).",5.2 Experimental Settings,[0],[0]
"Concatenation concatenates the author embedding with the sentence representation obtained from CNN, and then feeds the new representation to a softmax classifier.",5.2 Experimental Settings,[0],[0]
"Finally, we include SOCIAL ATTENTION, the attention-based neural network method described in § 4.
",5.2 Experimental Settings,[1.0000000268823126],"['Finally, we include SOCIAL ATTENTION, the attention-based neural network method described in § 4.']"
"We also compare against the three top-performing systems in the SemEval 2015 Twitter sentiment analysis challenge (Rosenthal et al., 2015): WEBIS (Hagen et al., 2015), UNITN (Severyn and Moschitti, 2015), and LSISLIF (Hamdan et al., 2015).",5.2 Experimental Settings,[0],[0]
UNITN achieves the best average F1 score on Test 2013–2015 sets among all the submitted systems.,5.2 Experimental Settings,[0],[0]
"Finally, we republish results of NLSE (Astudillo et al., 2015), a non-linear subspace embedding model.
",5.2 Experimental Settings,[0],[0]
Parameter tuning We tune all the hyperparameters on the SemEval 2013 development set.,5.2 Experimental Settings,[0],[0]
"We choose the number of bigram filters for the CNN models from {50, 100, 150}.",5.2 Experimental Settings,[0],[0]
"The size of author embeddings is selected from {50, 100}.",5.2 Experimental Settings,[0],[0]
"For mixture of experts, random attention and SOCIAL ATTENTION, we compare a range of numbers of basis models, {3, 5, 10, 15}.",5.2 Experimental Settings,[0],[0]
We found that a relatively small number of basis models are usually sufficient to achieve good performance.,5.2 Experimental Settings,[0],[0]
"The number of pretraining epochs is selected from {1, 2, 3}.",5.2 Experimental Settings,[0],[0]
"During joint training, we check the performance on the development set after each epoch to perform early stopping.",5.2 Experimental Settings,[0],[0]
"Table 3 summarizes the main empirical findings, where we report results obtained from author embeddings trained on RETWEET+ network for SOCIAL ATTENTION.",5.3 Results,[0],[0]
The results of different social networks for SOCIAL ATTENTION are shown in Table 4.,5.3 Results,[0],[0]
"The best hyperparameters are: 100 bigram
9The summation of the pretrained word embeddings works better than the average of the word embeddings.
filters; 100-dimensional author embeddings; K = 5 basis models; 1 pre-training epoch.",5.3 Results,[0],[0]
"To establish the statistical significance of the results, we obtain 100 bootstrap samples for each test set, and compute the F1 score on each sample for each algorithm.",5.3 Results,[0],[0]
"A twotail paired t-test is then applied to determine if the F1 scores of two algorithms are significantly different, p < 0.05.
",5.3 Results,[0],[0]
"Mixture of experts, random attention, and CNN all achieve similar average F1 scores on the SemEval Twitter 2013–2015 test sets.",5.3 Results,[0],[0]
"Note that random attention can benefit from some of the personalized information encoded in the random author embeddings, as Twitter messages posted by the same author share the same attentional weights.",5.3 Results,[0],[0]
"However, it barely improves the results, because the majority of authors contribute a single message in the SemEval datasets.",5.3 Results,[0],[0]
"With the incorporation of author social network information, concatenation slightly improves the classification performance.",5.3 Results,[0],[0]
"Finally, SOCIAL ATTENTION gives much better results than concatena-
tion, as it is able to model the interactions between text representations and author representations.",5.3 Results,[0],[0]
"It significantly outperforms CNN on all the SemEval test sets, yielding 2.8% improvement on average F1 score.",5.3 Results,[0],[0]
"SOCIAL ATTENTION also performs substantially better than the top-performing SemEval systems and NLSE, especially on the 2014 and 2015 test sets.
",5.3 Results,[0],[0]
We now turn to a comparison of the social networks.,5.3 Results,[0],[0]
"As shown in Table 4, the RETWEET+ network is the most effective, although the differences are small: SOCIAL ATTENTION outperforms prior work regardless of which network is selected.",5.3 Results,[0],[0]
"Twitter’s “following” relation is a relatively low-cost form of social engagement, and it is less public than retweeting or mentioning another user.",5.3 Results,[0],[0]
Thus it is unsurprising that the follower network is least useful for socially-informed personalization.,5.3 Results,[0],[0]
"The RETWEET+ network has denser social connections than MENTION+, which could lead to better author embeddings.",5.3 Results,[0],[0]
We now investigate whether language variation in sentiment meaning has been captured by different basis models.,5.4 Analysis,[0],[0]
"We focus on the same sentiment words (Tang et al., 2014) that we used to test linguistic homophily in our analysis.",5.4 Analysis,[0],[0]
We are interested to discover sentiment words that are used with the opposite sentiment meanings by some authors.,5.4 Analysis,[0],[0]
"To measure the level of model-specificity for each
word w, we compute the difference between the model-specific probabilities p(y | X = w,Z = k) and the average probabilities of all basis models 1 K ∑K k=1 p(y | X = w,Z = k) for positive and negative classes.",5.4 Analysis,[1.0000000235640858],"['To measure the level of model-specificity for each word w, we compute the difference between the model-specific probabilities p(y | X = w,Z = k) and the average probabilities of all basis models 1 K ∑K k=1 p(y | X = w,Z = k) for positive and negative classes.']"
"The five words in the negative and positive lexicons with the highest scores for each model are presented in Table 5.
",5.4 Analysis,[0],[0]
"As shown in Table 5, Twitter users corresponding to basis models 1 and 4 often use some words ironically in their tweets.",5.4 Analysis,[1.0],"['As shown in Table 5, Twitter users corresponding to basis models 1 and 4 often use some words ironically in their tweets.']"
"Basis model 3 tends to assign positive sentiment polarity to swear words, and Twitter users related to basis model 5 seem to be less fond of fans of certain celebrities.",5.4 Analysis,[1.0],"['Basis model 3 tends to assign positive sentiment polarity to swear words, and Twitter users related to basis model 5 seem to be less fond of fans of certain celebrities.']"
"Finally, basis model 2 identifies Twitter users that we have described in the introduction—they often adopt general negative words like ‘ill’, ‘sick’, and ‘suck’ positively.",5.4 Analysis,[0],[0]
Examples containing some of these words are shown in Table 6.,5.4 Analysis,[0],[0]
"The labeled datasets for Twitter sentiment analysis are relatively small; to evaluate our method on a larger dataset, we utilize a product review dataset by Tang et al. (2012).",5.5 Sentiment Analysis of Product Reviews,[0],[0]
"The dataset consists of 257,682 reviews written by 10,569 users crawled from a popular product review sites, Ciao.10 The rating information in discrete five-star range is available for the reviews, which is treated as the ground truth label information for the reviews.",5.5 Sentiment Analysis of Product Reviews,[0],[0]
"Moreover, the users of this site can mark explicit “trust” relationships with each other, creating a social network.
",5.5 Sentiment Analysis of Product Reviews,[0],[0]
"To select examples from this dataset, we first removed reviews that were marked by readers as “not useful.”",5.5 Sentiment Analysis of Product Reviews,[0],[0]
"We treated reviews with more than three stars as positive, and less than three stars as negative; reviews with exactly three stars were removed.
",5.5 Sentiment Analysis of Product Reviews,[0.9518318704189208],"['To select examples from this dataset, we first removed reviews that were marked by readers as “not useful.” We treated reviews with more than three stars as positive, and less than three stars as negative; reviews with exactly three stars were removed.']"
"10http://www.ciao.co.uk
We then sampled 100,000 reviews from this set, and split them randomly into training (70%), development (10%) and test sets (20%).",5.5 Sentiment Analysis of Product Reviews,[0],[0]
The statistics of the resulting datasets are presented in Table 7.,5.5 Sentiment Analysis of Product Reviews,[0],[0]
"We utilize 145,828 trust relations between 18,999 Ciao users to train the author embeddings.",5.5 Sentiment Analysis of Product Reviews,[1.0],"['We utilize 145,828 trust relations between 18,999 Ciao users to train the author embeddings.']"
"We consider the 10,000 most frequent words in the datasets, and assign them pretrained word2vec embeddings.11",5.5 Sentiment Analysis of Product Reviews,[0],[0]
"As shown in Table 7, the datasets have highly skewed class distributions.",5.5 Sentiment Analysis of Product Reviews,[0],[0]
"Thus, we use the average F1 score of positive and negative classes as the evaluation metic.
",5.5 Sentiment Analysis of Product Reviews,[0],[0]
The evaluation results are presented in Table 8.,5.5 Sentiment Analysis of Product Reviews,[0],[0]
"The best hyperparameters are generally the same as those for Twitter sentiment analysis, except that the optimal number of basis models is 10, and the optimal number of pretraining epochs is 2.",5.5 Sentiment Analysis of Product Reviews,[0],[0]
"Mixture of experts and concatenation obtain slightly worse F1 scores than the baseline CNN system, but random attention performs significantly better.",5.5 Sentiment Analysis of Product Reviews,[0],[0]
"In contrast to the SemEval datasets, individual users often contribute multiple reviews in the Ciao datasets (the average number of reviews from an author is 10.8; Table 7).",5.5 Sentiment Analysis of Product Reviews,[0],[0]
"As an author tends to express similar opinions toward related products, random attention
11https://code.google.com/archive/p/ word2vec
is able to leverage the personalized information to improve sentiment analysis.",5.5 Sentiment Analysis of Product Reviews,[0],[0]
"Prior work has investigated the direction, obtaining positive results using speaker adaptation techniques (Al Boni et al., 2015).",5.5 Sentiment Analysis of Product Reviews,[0],[0]
"Finally, by exploiting the social network of trust relations, SOCIAL ATTENTION obtains further improvements, outperforming random attention by a small but significant margin.",5.5 Sentiment Analysis of Product Reviews,[0],[0]
"Domain adaptation and personalization Domain adaptation is a classic approach to handling the variation inherent in social media data (Eisenstein, 2013).",6 Related Work,[0],[0]
"Early approaches to supervised domain adaptation focused on adapting the classifier weights across domains, using enhanced feature spaces (Daumé III, 2007) or Bayesian priors (Chelba and Acero, 2006; Finkel and Manning, 2009).",6 Related Work,[0],[0]
"Recent work focuses on unsupervised domain adaptation, which typically works by transforming the input feature space so as to overcome domain differences (Blitzer et al., 2006).",6 Related Work,[0],[0]
"However, in many cases, the data has no natural partitioning into domains.",6 Related Work,[0],[0]
"In preliminary work, we constructed social network domains by running community detection algorithms on the author social network (Fortunato, 2010).",6 Related Work,[0],[0]
"However, these algorithms proved to be unstable on the sparse networks obtained from social media datasets, and offered minimal performance improvements.",6 Related Work,[0],[0]
"In this paper, we convert social network positions into node embeddings, and use an attentional component to smooth the classification rule across the embedding space.
",6 Related Work,[0],[0]
Personalization has been an active research topic in areas such as speech recognition and information retrieval.,6 Related Work,[0],[0]
"Standard techniques for these tasks include linear transformation of model parameters (Leggetter and Woodland, 1995) and collaborative filtering (Breese et al., 1998).",6 Related Work,[0],[0]
"These methods have recently been adapted to personalized sentiment analysis (Tang et al., 2015a; Al Boni et al., 2015).",6 Related Work,[0],[0]
Supervised personalization typically requires labeled training examples for every individual user.,6 Related Work,[0],[0]
"In contrast, by leveraging the social network structure, we can obtain personalization even when labeled data is unavailable for many authors.
",6 Related Work,[0],[0]
"Sentiment analysis with social relations Previous work on incorporating social relations into sentiment classification has relied on the label consistency assumption, where the existence of social connections between users is taken as a clue that the sentiment polarities of the users’ messages should be similar.",6 Related Work,[0],[0]
"Speriosu et al. (2011) construct a heterogeneous network with tweets, users, and n-grams as nodes.",6 Related Work,[0],[0]
"Each node is then associated with a sentiment label distribution, and these label distributions are smoothed by label propagation over the graph.",6 Related Work,[0],[0]
"Similar approaches are explored by Hu et al. (2013), who employ the graph Laplacian as a source of regularization, and by Tan et al. (2011) who take a factor graph approach.",6 Related Work,[0],[0]
A related idea is to label the sentiment of individuals in a social network towards each other: West et al. (2014) exploit the sociological theory of structural balance to improve the accuracy of dyadic sentiment labels in this setting.,6 Related Work,[0],[0]
All of these efforts are based on the intuition that individual predictions p(y) should be smooth across the network.,6 Related Work,[0],[0]
"In contrast, our work is based on the intuition that social neighbors use language similarly, so they should have a similar conditional distribution p(y | x).",6 Related Work,[0],[0]
"These intuitions are complementary: if both hold for a specific setting, then label consistency and linguistic consistency could in principle be combined to improve performance.
",6 Related Work,[0],[0]
"Social relations can also be applied to improve personalized sentiment analysis (Song et al., 2015; Wu and Huang, 2015).",6 Related Work,[0],[0]
Song et al. (2015) present a latent factor model that alleviates the data sparsity problem by decomposing the messages into words that are represented by the weighted sentiment and topic units.,6 Related Work,[0],[0]
Social relations are further incorporated into the model based on the intuition that linked individuals share similar interests with respect to the latent topics.,6 Related Work,[0],[0]
Wu and Huang (2015) build a personalized sentiment classifier for each author; socially connected users are encouraged to have similar userspecific classifier components.,6 Related Work,[0],[0]
"As discussed above, the main challenge in personalized sentiment analysis is to obtain labeled data for each individual author.",6 Related Work,[0],[0]
"Both papers employ distant supervision, using emoticons to label additional instances.",6 Related Work,[0],[0]
"However, emoticons may be unavailable for some authors or even for entire genres, such as reviews.",6 Related Work,[0],[0]
"Furthermore, the pragmatic function of emoticons is com-
plex, and in many cases emoticons do not refer to sentiment (Walther and D’Addario, 2001).",6 Related Work,[0],[0]
"Our approach does not rely on distant supervision, and assumes only that the classification decision function should be smooth across the social network.",6 Related Work,[0],[0]
"This paper presents a new method for learning to overcome language variation, leveraging the tendency of socially proximate individuals to use language similarly—the phenomenon of linguistic homophily.",7 Conclusion,[0],[0]
"By learning basis models that focus on different local regions of the social network, our method is able to capture subtle shifts in meaning across the network.",7 Conclusion,[0],[0]
"Inspired by ensemble learning, we have formulated this model by employing a social attention mechanism: the final prediction is the weighted combination of the outputs of the basis models, and each author has a unique weighting, depending on their position in the social network.",7 Conclusion,[0],[0]
"Our model achieves significant improvements over standard convolutional networks, and ablation analyses show that social network information is the critical ingredient.",7 Conclusion,[1.0],"['Our model achieves significant improvements over standard convolutional networks, and ablation analyses show that social network information is the critical ingredient.']"
"In other work, language variation has been shown to pose problems for the entire NLP stack, from part-of-speech tagging to information extraction.",7 Conclusion,[1.0],"['In other work, language variation has been shown to pose problems for the entire NLP stack, from part-of-speech tagging to information extraction.']"
A key question for future research is whether we can learn a socially-infused ensemble that is useful across multiple tasks.,7 Conclusion,[1.0],['A key question for future research is whether we can learn a socially-infused ensemble that is useful across multiple tasks.']
We thank Duen Horng “Polo” Chau for discussions about community detection and Ramon Astudillo for sharing data and helping us to reproduce the NLSE results.,8 Acknowledgments,[0],[0]
"This research was supported by the National Science Foundation under award RI1452443, by the National Institutes of Health under award number R01GM112697-01, and by the Air Force Office of Scientific Research.",8 Acknowledgments,[0],[0]
The content is solely the responsibility of the authors and does not necessarily represent the official views of these sponsors.,8 Acknowledgments,[0],[0]
"Variation in language is ubiquitous, particularly in newer forms of writing such as social media.",abstractText,[0],[0]
"Fortunately, variation is not random; it is often linked to social properties of the author.",abstractText,[0],[0]
"In this paper, we show how to exploit social networks to make sentiment analysis more robust to social language variation.",abstractText,[0],[0]
The key idea is linguistic homophily: the tendency of socially linked individuals to use language in similar ways.,abstractText,[0],[0]
"We formalize this idea in a novel attention-based neural network architecture, in which attention is divided among several basis models, depending on the author’s position in the social network.",abstractText,[0],[0]
"This has the effect of smoothing the classification function across the social network, and makes it possible to induce personalized classifiers even for authors for whom there is no labeled data or demographic metadata.",abstractText,[0],[0]
This model significantly improves the accuracies of sentiment analysis on Twitter and on review data.,abstractText,[0],[0]
Overcoming Language Variation in Sentiment Analysis with Social Attention,title,[0],[0]
