1	12	Word senses can shift over long periods of time (Wilkins, 1993; Wijaya and Yeniterzi, 2011; Hamilton et al., 2016), and written language can change rapidly in online platforms (Eisenstein et al., 2014; Goel et al., 2016).
9	50	To address question 1, we train and test on data from different time periods, to understand how performance varies with time.
22	33	We grouped documents into two types of intervals: • Seasonal: Time intervals within a year (e.g., January through March) that may be repeated across years.
31	11	We downsampled the training data within each time interval to match the number of documents in the smallest interval, so that differences in performance are not due to the size of the training data.
39	38	Jan-M ar Apr-Ju n Jul-Se p Oct-D ec Train JanMar Apr -Jun Jul-S ep Oct -De c Te st 0.948 0.912 0.913 0.910 0.916 0.949 0.914 0.909 0.916 0.912 0.952 0.910 0.916 0.914 0.918 0.945 Reviews data - music Jan-M ar Apr-Ju n Jul-Se p Oct-D ec Train JanMar Apr -Jun Jul-S ep Oct -De c Te st 0.865 0.862 0.862 0.861 0.863 0.862 0.861 0.858 0.862 0.859 0.866 0.861 0.863 0.863 0.863 0.858 Reviews data - hotels Jan-M ar Apr-Ju n Jul-Se p Oct-D ec Train JanMar Apr -Jun Jul-S ep Oct -De c Te st 0.898 0.806 0.750 0.769 0.795 0.876 0.745 0.787 0.794 0.795 0.900 0.767 0.791 0.790 0.731 0.891 News data - economy Jan-M ar Apr-Ju n Jul-Se p Oct-D ec Train JanMar Apr -Jun Jul-S ep Oct -De c Te st 0.896 0.894 0.891 0.856 0.808 0.940 0.853 0.829 0.836 0.904 0.917 0.845 0.849 0.891 0.884 0.902 Twitter data - vaccine 2006 -08 2009 -11 2012 -14 2015 -17 Train 200 6-08 200 9-11 201 2-14 201 5-17 Te st 0.823 0.828 0.825 0.859 0.799 0.843 0.830 0.858 0.800 0.819 0.833 0.869 0.790 0.813 0.835 0.880 Reviews data - hotels 2006 -08 2009 -11 2012 -14 2015 -17 Train 200 6-08 200 9-11 201 2-14 201 5-17 Te st 0.829 0.838 0.869 0.883 0.814 0.856 0.870 0.883 0.815 0.842 0.884 0.894 0.814 0.839 0.875 0.902 Reviews data - restaurants 1948 -56 1960 -68 1972 -80 1984 -92 1996 -20042008 -16 Train 194 8-56 196 0-68 197 2-80 198 4-92 199 6-20 04 200 8-16 Te st 0.659 0.567 0.518 0.544 0.525 0.532 0.551 0.800 0.529 0.477 0.474 0.495 0.545 0.506 0.678 0.635 0.573 0.523 0.515 0.473 0.565 0.866 0.594 0.569 0.435 0.404 0.490 0.618 0.848 0.684 0.435 0.416 0.480 0.606 0.674 0.819 Politics - US political data 1985 -89 1990 -94 1995 -99 2000 -04 2005 -09 2010 -14 Train 198 5-89 199 0-94 199 5-99 200 0-04 200 5-09 201 0-14 Te st 0.876 0.758 0.783 0.794 0.777 0.756 0.764 0.883 0.771 0.802 0.789 0.748 0.759 0.760 0.905 0.798 0.806 0.763 0.760 0.756 0.770 0.926 0.805 0.771 0.773 0.767 0.783 0.826 0.900 0.778 0.773 0.750 0.778 0.810 0.786 0.897 News data - economy Figure 1: Document classification performance when training and testing on different times of year (top) and different years (bottom).
49	10	Thus, characterizing the ways in which content distributions vary over time, and why this affects performance, is still an open question.
50	66	The bottom row of Figure 1 shows the test scores from training and testing on each pair of nonseasonal time intervals.
51	12	A strong pattern emerges in the political parties corpus: F1 scores can drop by as much as 40 points when testing on different time intervals.
53	22	The performance declines more when testing on time intervals that are further away in time from the training interval, suggesting that changes in party platforms shift gradually over time.
54	18	In contrast, while there was a performance drop when testing outside the training interval in the economic news corpus, the drop was not gradual.
56	47	We observe an intriguing non-seasonal pattern that is consistent in both of the review corpora from Yelp, but not in the music review corpus from Amazon (not pictured), which is that the classification performance fairly consistently increases over time.
57	38	Since we sampled the dataset so that the time intervals have the same number of reviews, this suggests something else changed over time about the way reviews are written that makes the sentiment easier to detect.
61	19	Performance diminishes when applied to different time intervals, although different corpora exhibit differ patterns in the way in which the performance diminishes.
64	12	We propose to treat this as a domain adaptation problem.
69	34	In our experiments, we use the feature augmentation approach of Daumé III (2007) to perform domain adaptation.
70	28	Each feature is duplicated to have a specific version of the feature for every domain, as well as a domain-independent version of the feature.
71	13	In each instance, the domainindependent feature and the domain-specific feature for that instance’s domain have the same feature value, while the value is zeroed out for the domain-specific features for the other domains.
78	10	If this pattern holds in other corpora, then this suggests that it does not hurt performance to apply domain adaptation across different times of year, and in some cases can lead to a small performance boost.
81	18	This requires a modification to the domain adaptation approach, because future data includes domains that did not exist in the training data, and thus we cannot learn domain-specific feature weights.
82	32	To solve this, we train in the usual way, but when testing on future data, we only include the domain-independent features.
83	22	The intuition is that the domain-independent parameters should be applicable to all domains, and so using only these features should lead to better generalizability to new domains.
85	52	For hyperparameter tuning, we used the final time interval of the training data (i.e., the penultimate interval) as the validation set.
88	44	We see that this approach leads to a small performance boost in all cases except the Twitter dataset.
89	30	This means that this simple feature augmentation approach has the potential to make classifiers more robust to future changes in data.
93	13	We also experimented with including the seasonal features when performing non-seasonal adaptation.
95	22	As above, we remove the non-seasonal features at test time; however, we retain the season-specific features in addition to the domain-independent features, as they can be reused in future years.
96	16	The results of this approach are shown in the last column of Table 3.
97	10	We find that combining seasonal and non-seasonal features together leads to an additional performance gain in most cases.
101	44	First, evaluation will be most accurate if the test data is as similar as possible to whatever future data the classifier will be applied to, and one way to achieve this is to select test data from the chronological end of the corpus, rather than randomly sampling data without regard to time.
102	28	Second, we observed that performance on future data tends to increase when hyperparameter tuning is conducted on later data; thus, we also recommend sampling validation data from the chronological end of the corpus.
