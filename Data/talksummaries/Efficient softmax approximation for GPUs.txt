0	26	This paper considers strategies to learn parametric models for language modeling with very large vocabularies.
1	28	This problem is key to natural language processing, with applications in machine translation (Schwenk et al., 2012; Sutskever et al., 2014; Vaswani et al., 2013) or automatic speech recognition (Graves et al., 2013; Hinton et al., 2012).
6	12	This potentially makes parametric models prohibitively slow to train on corpora with very large vocabulary.
56	13	, wT ∈ VT is given as P (w1, .
58	24	In particular, smoothed N-gram models (Bahl et al., 1983; Katz, 1987; Kneser & Ney, 1995) achieve good performance in practice (Mikolov et al., 2011a), especially when they are associated with cache models (Kuhn & De Mori, 1990).
62	19	In the simplest case, this probability is represented by a 2-layer neural network acting on an input xt ∈ VN , defined as the concatenation of the one-hot representation of the N previous words, wt−N+1, .
71	15	In neural language modeling, predicting the probability of the next word requires computing scores for every word in the vocabulary and to normalize them to form a probability distribution.
76	16	A simple approach (Goodman, 2001a) to reduce this computational cost is to assign each word w of the vocabulary to a unique class C(w) and to factorize the probability distribution over words as p(wt | ht) = p1(C(wt) | ht)× p2(wt | C(wt), ht), where p1 and p2 are obtained using the softmax function (Eq.
77	40	If each class contains √ k words, the computational cost is reduced from O(dk) to O(d √ k).
82	25	The bottleneck of the model described in the previous section is the matrix multiplication between the matrix representing the hidden states (of size B × d, where B denotes the batch size), and the matrix of word representations, of size d × k. For a fixed size d of the hidden layer, we denote by g(k,B) the computation time of this multiplication (using an efficient implementation such as cuBLAS), and simplify the notation wherever some parameters are fixed.
84	21	We observe that the computation time g(k) is constant for low values of k, until a certain inflection point k0 ≈ 50, and then becomes affine for values k > k0.
87	22	We observe the same behavior when measuring the timings as a function of the batch size B, i.e., it is inefficient to matrix-multiplication when one of the dimensions is small.
89	53	Similarly, clusters comprising only rare words have a low probabilty p and a shrinking batch size of p B, which also lead to iniffient matrix-multiplication.
99	23	For instance, one may define the head would only contain 20% of the vocabulary (covering for 87% on PennTree Bank).
108	25	The tail cluster will then contain the rest of the vocabulary, made of kt = k − kh words and covering for pt = 1 − ph of the overall distribution.
109	27	The computation time corresponding to the matrix multiplication of the root is equal to g(kh +1, B), while the computation time for the tail of the distribution is equal to g(kt, ptB), where B is the batch size.
110	47	We thus obtain the overall computation time C = g(kh + 1, B) + g(kt, ptB).
111	12	We can then find the size of the head cluster kh which minimizes the computation time C. We plot the value of C as a function of kh in Figure 2, for the word distribution of the Bulgarian Europarl dataset.
119	40	Like in Chen et al. (2015), we exploit this observation to further reduce the computational time of our classifier.
145	11	Moreover, we observe that using more than 5 clusters does not lead to significant gains in computational time (a couple of milliseconds at best).
146	13	In practice, we thus decide to use a small number of clusters (between 2 and 5), as it usually lead to slightly better perplexity, and we empirically determine the best speed/perplexity compromise on training data.
166	24	Our method is compared to: (1) the full softmax, (2) the hierarchical softmax with frequency binning (HSM freq) and similarity-based binning (HSM sim), (3) importance sampling (Bengio et al., 2003b; Bengio & Senécal, 2008) and (4) the differentiated softmax (Chen et al., 2015).
170	18	For the negative sampling method, we used a number of samples equal to 20% of the size of the vocabulary (Chen et al., 2015).
175	19	Comparison with the state of the art.
178	28	Our approach is the only one to approach the result of the full soft-max (below by 3 points of perplexity), while being the fastest.
187	30	We also note that for models of similar size, we achieve similar perplexity than the method introduced by Jozefowicz et al. (2016).
188	13	As far as we know, ours the first method to achieve a perplexity lower than 50 on a single GPU.
192	32	In all our experiments on GPU, our method consistently maintains a low perplexity while enjoying a speed-up going from 2× to 10× compared to the exact model.
193	13	This type of speed-up allows to deal with extremely large corpora in reasonable time and without the need of a large number of GPUs.
194	49	We believe our approach to be general enough to be applied to other parallel computing architectures and other losses, as well as to other domains where the distributions of the class are unbalanced.
