{"sections": [{"heading": "1. Introduction", "text": "Categorical distributions are fundamental to many areas of machine learning. Examples include classification (Gupta et al., 2014), language models (Bengio et al., 2006), recommendation systems (Marlin & Zemel, 2004), reinforcement learning (Sutton & Barto, 1998), and neural attention models (Bahdanau et al., 2015). They also play an important role in discrete choice models (McFadden, 1978).\nA categorical is a die with K sides, a discrete random variable that takes on one of K unordered outcomes; a categorical distribution gives the probability of each possible outcome. Categorical variables are challenging to use when there are many possible outcomes. Such large categoricals appear in common applications such as image classification\n1University of Cambridge. 2Columbia University. 3Athens University of Economics and Business.. Correspondence to: Francisco J. R. Ruiz <f.ruiz@eng.cam.ac.uk, f.ruiz@columbia.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nwith many classes, recommendation systems with many items, and language models over large vocabularies. In this paper, we develop a new method for fitting and using large categorical distributions.\nThe most common way to form a categorical is through the softmax transformation, which maps a K-vector of reals to a distribution of K outcomes. Let \u03c8 be a real-valued K-vector. The softmax transformation is\np(y = k |\u03c8) = exp {\u03c8k}\u2211 k\u2032 exp {\u03c8k\u2032} . (1)\nNote the softmax is not the only way to map real vectors to categorical distributions; for example, the multinomial probit (Albert & Chib, 1993) is an alternative. Also note that in many applications, such as in multiclass classification, the parameter \u03c8k is a function of per-sample features x. For example, a linear classifier forms a categorical over classes through a linear combination, \u03c8k = w>k x.\nWe usually fit a categorical with maximum likelihood estimation or any other closely related strategy. Given a dataset y1:N of categorical data\u2014each yn is one of K values\u2014we aim to maximize the log likelihood,\nLlog likelihood = N\u2211 n=1 log p(yn |\u03c8). (2)\nFitting this objective requires evaluating both the log probability and its gradient.\nEqs. 1 and 2 reveal the challenge to using large categoricals. Evaluating the log probability and evaluating its gradient are both O(K) operations. But this is not OK: most algorithms for fitting categoricals\u2014for example, stochastic gradient ascent\u2014require repeated evaluations of both gradients and probabilities. When K is large, these algorithms are prohibitively expensive.\nHere we develop a method for fitting large categorical distributions, including the softmax but also more generally. It is called augment and reduce (A&R). A&R rewrites the categorical distribution with an auxiliary variable \u03b5,\np(y |\u03c8) = \u222b p(y, \u03b5 |\u03c8)d\u03b5. (3)\nA&R then replaces the expensive log probability with a variational bound on the integral in Eq. 3. Using stochastic\nvariational methods (Hoffman et al., 2013), the cost to evaluate the bound (or its gradient) is far below O(K).\nBecause it relies on variational methods, A&R provides a lower bound on the marginal likelihood of the data. With this bound, we can embed A&R in a larger algorithm for fitting a categorical, e.g., a (stochastic) variational expectation maximization (VEM) algorithm (Beal, 2003). Though we focus on maximum likelihood, we can also use A&R in other algorithms that require log p(y |\u03c8) or its gradient, e.g., fully Bayesian approaches (Gelman et al., 2003) or the REINFORCE algorithm (Williams, 1992).\nWe study A&R on linear classification tasks with up to 104 classes. On simulated and real data, we find that it provides accurate estimates of the categorical probabilities and gives better performance than existing approaches.\nRelated work. There are many methods to reduce the cost of large categorical distributions, particularly under the softmax transformation. These include methods that approximate the exact computations (Gopal & Yang, 2013; Vijayanarasimhan et al., 2014), those that rely on sampling (Bengio & S\u00e9n\u00e9cal, 2003; Mikolov et al., 2013; Devlin et al., 2014; Ji et al., 2016; Botev et al., 2017), those that use approximations and distributed computing (Grave et al., 2017), double-sum formulations (Raman et al., 2017; Fagan & Iyengar, 2018), and those that avail themselves of other techniques such as noise contrastive estimation (Smith & Jason, 2005; Gutmann & Hyv\u00e4rinen, 2010) or random nearest neighbor search (Mussmann et al., 2017).\nOther methods change the model. They might replace the softmax transformation with a hierarchical or stickbreaking model (Kurzynski, 1988; Morin & Bengio, 2005; Tsoumakas et al., 2008; Beygelzimer et al., 2009; Dembczyn\u0301ski et al., 2010; Khan et al., 2012). These approaches can be successful, but the structure of the hierarchy may influence the learned probabilities. Other methods replace the softmax with a scalable spherical family of losses (Vincent et al., 2015; de Br\u00e9bisson & Vincent, 2016).\nA&R is different from all of these techniques. Unlike many of them, it provides a lower bound on the log probability rather than an approximation. The bound is useful because it can naturally be embedded in algorithms like stochastic VEM. Further, the A&R methodology applies to transformations beyond the softmax. In this paper, we study large categoricals via softmax, multinomial probit, and multinomial logistic. A&R is the first scalable approach for the two latter models. It accelerates any transformation that can be recast as an additive noise model (e.g., Gumbel, 1954; Albert & Chib, 1993).\nThe approach that most closely relates to A&R is the one-vseach (OVE) bound of Titsias (2016), which is a lower bound of the softmax. Like the other related methods, it is narrower\nthan A&R in that it does not apply to transformations beyond the softmax. We also empirically compare A&R to OVE in Section 4. A&R provides a tighter lower bound and yields better predictive performance."}, {"heading": "2. Augment and Reduce", "text": "We develop augment and reduce (A&R), a method for computing with large categorical random variables.\nThe utility perspective. A&R uses the additive noise model perspective on the categorical, which we refer to as the utility perspective. Define a mean utility \u03c8k for each possible outcome k \u2208 {1, . . . ,K}. To draw a variable y from a categorical, we draw a zero-mean noise term \u03b5k for each possible outcome and then choose the value that maximizes the realized utility \u03c8k + \u03b5k. This corresponds to the following process,\n\u03b5k \u223c \u03c6(\u00b7), k \u2208 {1, . . . ,K}, y = argmax\nk (\u03c8k + \u03b5k) .\n(4)\nNote the errors \u03b5k are drawn fresh each time we draw a variable y. We assume that the errors are independent of each other, independent of the mean utility \u03c8k, and identically distributed according to some distribution \u03c6(\u00b7).\nNow consider the model where we marginalize the errors from Eq. 4. This results in a distribution p(y |\u03c8), a categorical that transforms \u03c8 to the simplex. Depending on the distribution of the errors, this induces different transformations. For example, a standard Gumbel distribution recovers the softmax transformation; a standard Gaussian recovers the multinomial probit transformation; a standard logistic recovers the multinomial logistic transformation.\nTypically, the mean utility \u03c8k is a function of observed features x, e.g., \u03c8k = x>wk in linear models or \u03c8k = fwk(x) in non-linear settings. In both cases, wk are model parameters, relating the features to mean utilities.\nLet us focus momentarily on a linear classification problem under the softmax model. For each observation n, the mean utilities are \u03c8nk = x>nwk and the random errors \u03b5nk are Gumbel distributed. After marginalizing out the errors, the probability that observation n is in class k is given by Eq. 1, p(yn = k |xn, w) \u221d exp{x>nwk}. Fitting the classifier involves learning the weights wk that parameterize \u03c8. For example, maximum likelihood uses gradient ascent to maximize \u2211 n log p(yn |xn, w) with respect to w.\nLarge categoricals. When the number of outcomes K is large, the normalizing constant of the softmax is a computational burden; it is O(K). Consequently, it is burdensome to calculate useful quantities like log p(yn |xn, w) and its gradient \u2207w log p(yn |xn, w). As an ultimate consequence, maximum likelihood estimation is slow\u2014it needs to evalu-\nate the gradient for each n at each iteration.\nIts difficulty scaling is not unique to the softmax. Similar issues arise for the multinomial probit and multinomial logistic. With these transformations as well, evaluating likelihoods and related quantities is O(K)."}, {"heading": "2.1. Augment and reduce", "text": "We introduce A&R to relieve this burden. A&R accelerates training in models with categorical distributions and a large number of outcomes.\nRather than operating directly on the marginal p(y |\u03c8), A&R augments the model with one of the error terms and forms a joint p(y, \u03b5 |\u03c8). (We drop the subscript n to avoid cluttered notation.) This augmented model has a desirable property: its log-joint is a sum over all the possible outcomes. A&R then reduces\u2014it subsamples a subset of outcomes to construct estimates of the log-joint and its gradient. As a result, its complexity relates to the size of the subsample, not the total number of outcomes K.\nThe augmented model. Let \u03c6(\u03b5) be the distribution over the error terms, and \u03a6(\u03b5) = \u222b \u03b5 \u2212\u221e \u03c6(\u03c4)d\u03c4 the corresponding cumulative distribution function (CDF). The marginal probability of outcome k is the probability that its realized utility (\u03c8k + \u03b5k) is greater than all others,\np(y = k |\u03c8) = Pr (\u03c8k + \u03b5k \u2265 \u03c8k\u2032 + \u03b5k\u2032 \u2200k\u2032 6= k) .\nWe write this probability as an integral over the kth error \u03b5k using the CDF of the other errors, p(y = k |\u03c8) = \u222b +\u221e \u2212\u221e \u03c6(\u03b5k) (\u220f k\u2032 6=k \u222b \u03b5k+\u03c8k\u2212\u03c8k\u2032 \u2212\u221e \u03c6(\u03b5k\u2032)d\u03b5k\u2032 ) d\u03b5k\n= \u222b +\u221e \u2212\u221e \u03c6(\u03b5) (\u220f k\u2032 6=k \u03a6(\u03b5+ \u03c8k \u2212 \u03c8k\u2032) ) d\u03b5. (5)\n(We renamed the dummy variable \u03b5k as \u03b5 to avoid clutter.) Eq. 5 is the same as found by Girolami & Rogers (2006) for the multinomial probit model, although we do not assume a Gaussian density \u03c6(\u03b5). Rather, we only assume that we can evaluate both \u03c6(\u03b5) and \u03a6(\u03b5).\nWe derived Eq. 5 from the utility perspective, which encompasses many common models. We obtain the softmax by choosing a standard Gumbel distribution for \u03c6(\u03b5), in which case Eqs. 1 and 5 are equivalent. We obtain the multinomial probit by choosing a standard Gaussian distribution over the errors, and in this case the integral in Eq. 5 does not have a closed form. Similarly, we obtain the multinomial logistic by choosing a standard logistic distribution \u03c6(\u03b5). What is important is that regardless of the model, the cost to compute the marginal probability p(y = k |\u03c8) is O(K).\nWe now augment the model with the auxiliary latent variable \u03b5 to form the joint distribution p(y, \u03b5 |\u03c8),\np(y = k, \u03b5 |\u03c8) = \u03c6(\u03b5) \u220f k\u2032 6=k \u03a6(\u03b5+ \u03c8k \u2212 \u03c8k\u2032). (6)\nThis is a model that includes the kth error term from Eq. 4 but marginalizes out all the other errors. By construction, marginalizing \u03b5 from Eq. 6 recovers the original model p(y |\u03c8) in Eq. 5. Figure 1 illustrates this idea.\nRiihim\u00e4ki et al. (2013) used Eq. 6 in the nested expectation propagation for Gaussian process classification. We use it to scale learning with categorical distributions.\nThe variational bound. The augmented model in Eq. 6 involves one latent variable \u03b5. But our goal is to calculate the marginal log p(y |\u03c8) and its gradient. A&R derives a variational lower bound on log p(y |\u03c8) using the joint in Eq. 6. Define q(\u03b5) to be a variational distribution on the auxiliary variable. The bound is log p(y |\u03c8) \u2265 L, where\nL = Eq(\u03b5) [ log p(y = k, \u03b5 |\u03c8)\u2212 log q(\u03b5) ] (7)\n= Eq(\u03b5) [ log \u03c6(\u03b5) + \u2211 k\u2032 6=k log\u03a6(\u03b5+ \u03c8k \u2212 \u03c8k\u2032)\u2212 log q(\u03b5) ] .\nIn Eq. 7, L is the evidence lower bound (ELBO); it is tight when q(\u03b5) is equal to the posterior of \u03b5 given y, p(\u03b5 | y, \u03c8) (Jordan et al., 1999; Blei et al., 2017).\nThe ELBO contains a summation over the outcomes k\u2032 6= k. A&R exploits this property to reduce complexity, as we describe below. Next we show how to use the bound in a variational expectation maximization (VEM) procedure and we describe the reduce step of A&R.\nVariational expectation maximization. Consider again a linear classification task, where we have a dataset of features xn and labels yn \u2208 {1, . . . ,K} for n = 1, . . . , N . The mean utility for each observation n is \u03c8nk = w>k xn, and the goal is to learn the weights wk by maximizing the log likelihood \u2211 n log p(yn |xn, w).\nA&R replaces each term in the data log likelihood with its bound using Eq. 7. The objective becomes \u2211 n L(n). Maximizing this objective requires an iterative process with two steps. In one step, A&R optimizes the objective with respect to w. In the other step, A&R optimizes each L(n) with respect to the variational distribution. The resulting procedure takes the form of a VEM algorithm (Beal, 2003).\nThe VEM algorithm requires optimizing the ELBO with respect to w and the variational distributions.1 This is challenging for two reasons. First, the expectations in Eq. 7 might not be tractable. Second, the cost to compute the gradients of Eq. 7 is still O(K).\nSection 3 addresses these issues. To sidestep the intractable expectations, A&R forms unbiased Monte Carlo estimates of the gradient of the ELBO. To alleviate the computational complexity, A&R uses stochastic optimization, subsampling a set of outcomes k\u2032.\nReduce by subsampling. The subsampling step in the VEM procedure is one of the key ideas behind A&R. Since Eq. 7 contains a summation over the outcomes k\u2032 6= k, we can apply stochastic optimization techniques to obtain unbiased estimates of the ELBO and its gradient.\nMore specifically, consider the gradient of the ELBO in Eq. 7 with respect to w (the parameters of \u03c8). It is\n\u2207wL = \u2211 k\u2032 6=k Eq(\u03b5) [ \u2207w log\u03a6(\u03b5+ \u03c8k \u2212 \u03c8k\u2032) ] .\nA&R estimates this by first randomly sampling a subset of outcomes S \u2286 {1, . . . ,K} {k} of size |S|. A&R then uses the outcomes in S to approximate the gradient,\n\u2207\u0303wL = K \u2212 1 |S| \u2211 k\u2032\u2208S Eq(\u03b5) [ \u2207w log\u03a6(\u03b5+ \u03c8k \u2212 \u03c8k\u2032) ] .\nThis is an unbiased estimator2 of the gradient \u2207wL. Crucially, A&R only needs to iterate over |S| outcomes to obtain it, reducing the complexity to O(|S|).\nThe reduce step is also applicable to optimize the ELBO with respect to q(\u03b5). Section 3 gives further details about the stochastic VEM procedure in different settings."}, {"heading": "3. Algorithm Description", "text": "Here we provide the details to run the variational expectation maximization (VEM) algorithm for the softmax model (Sec-\n1Note that maximizing the ELBO in Eq. 7 with respect to the distribution q(\u03b5) is equivalent to minimizing the Kullback-Leibler divergence from q(\u03b5) to the posterior p(\u03b5 | y, \u03c8).\n2This is not the only way to construct an unbiased estimator. Alternatively, we can draw the outcomes k\u2032 using importance sampling, taking into account the frequency of each class. We leave this for future work.\ntion 3.1) and for more general models including the multinomial probit and multinomial logistic (Section 3.2). These models only differ in the prior over the errors \u03c6(\u03b5).\nAugment and reduce (A&R) is not limited to point-mass estimation of the parameters w. It is straightforward to extend the algorithm to perform posterior inference on w via stochastic variational inference, but for simplicity we describe maximum likelihood estimation."}, {"heading": "3.1. Augment and Reduce for Softmax", "text": "In the softmax model, the distribution over the error terms is a standard Gumbel (Gumbel, 1954),\n\u03c6softmax(\u03b5) = exp{\u2212\u03b5\u2212 e\u2212\u03b5}, \u03a6softmax(\u03b5) = exp{\u2212e\u2212\u03b5}.\nIn this model, the optimal distribution q?(\u03b5), which achieves equality in the bound, has closed-form expression:\nq?softmax(\u03b5) = Gumbel(\u03b5 ; log \u03b7 ?, 1), with \u03b7? = 1 + \u2211 k\u2032 6=k e\n\u03c8k\u2032\u2212\u03c8k . However, even though q?softmax(\u03b5) has an analytic form, its parameter \u03b7\n? is computationally expensive to obtain because it involves a summation over K \u2212 1 classes. Instead, we set\nqsoftmax(\u03b5 ; \u03b7) = Gumbel(\u03b5 ; log \u03b7, 1).\nSubstituting this choice for qsoftmax(\u03b5 ; \u03b7) into Eq. 7 gives the following evidence lower bound (ELBO):\nLsoftmax = 1\u2212 log(\u03b7)\u2212 1\n\u03b7 1 + \u2211 k\u2032 6=k e\u03c8k\u2032\u2212\u03c8k  . (8) Eq. 8 coincides with the log-concavity bound (Bouchard, 2007; Blei & Lafferty, 2007), although we have derived it from a completely different perspective. This derivation allows us to optimize \u03b7 efficiently, as we describe next.\nThe Gumbel(\u03b5 ; log \u03b7, 1) is an exponential family distribution whose natural parameter is \u03b7. This allows us to use natural gradients in the stochastic inference procedure. A&R iterates between a local step, in which we update \u03b7, and a global step, in which we update the parameters \u03c8.\nIn the local step (E step), we optimize \u03b7 by taking a step in the direction of the noisy natural gradient, yielding \u03b7new = (1 \u2212 \u03b1)\u03b7old + \u03b1\u03b7\u0303. Here, \u03b7\u0303 is an estimate of the optimal natural parameter, which we obtain using a random set of outcomes, i.e., \u03b7\u0303 = 1 + K\u22121|S| \u2211 k\u2032\u2208S e\n\u03c8k\u2032\u2212\u03c8k , where S \u2286 {1, . . . ,K} {k}. The parameter \u03b1 is the step size; it must satisfy the Robbins-Monro conditions (Robbins & Monro, 1951; Hoffman et al., 2013).\nIn the global step (M step), we take a gradient step with respect to w (the parameters of \u03c8), holding \u03b7 fixed. Similarly, we can estimate the gradient of Eq. 8 with complexity O(|S|) by leveraging stochastic optimization.\nAlgorithm 1 Softmax A&R for classification Input: data (xn, yn), minibatch sizes |B| and |S| Output: weights w = {wk}Kk=1 Initialize all weights and natural parameters for iteration t = 1, 2, . . . , do # Sample minibatches: Sample a minibatch of data, B \u2286 {1, . . . , N} for n \u2208 B do\nSample a set of labels, Sn \u2286 {1, . . . ,K} {yn} end for # Local step (E step): for n \u2208 B do\nCompute \u03b7\u0303n = 1 + K\u22121|S| \u2211 k\u2032\u2208Sn e \u03c8nk\u2032\u2212\u03c8nyn\nUpdate natural param., \u03b7n \u2190 (1\u2212\u03b1(t))\u03b7n+\u03b1(t)\u03b7\u0303n end for # Global step (M step): Set g = \u2212 N|B| K\u22121 |S| \u2211 n\u2208B 1 \u03b7n \u2211 k\u2032\u2208Sn\u2207we \u03c8nk\u2032\u2212\u03c8nyn\nGradient step on the weights, w \u2190 w + \u03c1(t)g end for\nAlgorithm 1 summarizes the procedure for a classification task. In this example, the dataset consists of N datapoints (xn, yn), where xn is a feature vector and yn \u2208 {1, . . . ,K} is the class label. Each observation is associated with its parameters \u03c8nk; e.g., \u03c8nk = x>nwk. We posit a softmax likelihood, and we wish to infer the weights via maximum likelihood using A&R. Thus, the objective function is\u2211 n L (n) softmax. (It is straightforward to obtain the maximum a posteriori solution by adding a regularizer.) At each iteration, we process a random subset of observations as well as a random subset of classes for each one.\nFinally, note that we can perform posterior inference on the parameters w (instead of maximum likelihood) using A&R. One way is to consider a variational distribution q(w) and take gradient steps with respect to the variational parameters of q(w) in the global step, using the reparameterization trick (Rezende et al., 2014; Titsias & L\u00e1zaro-Gredilla, 2014; Kingma & Welling, 2014) to approximate that gradient. In the local step, we only need to evaluate the moment generating function, estimating the optimal natural parameter as \u03b7\u0303 = 1 + K\u22121|S| \u2211 k\u2032\u2208S Eq(w) [ e\u03c8k\u2032\u2212\u03c8k ] ."}, {"heading": "3.2. Augment and Reduce for Other Models", "text": "For most models, the expectations of the ELBO in Eq. 7 are intractable, and there is no closed-form solution for the optimal variational distribution q?(\u03b5). Fortunately, we can apply A&R, using the reparameterization trick to build Monte Carlo estimates of the gradient of the ELBO with respect to the variational parameters (Rezende et al., 2014; Titsias & L\u00e1zaro-Gredilla, 2014; Kingma & Welling, 2014).\nMore in detail, consider the variational distribution q(\u03b5 ; \u03bd),\nAlgorithm 2 General A&R for classification Input: data (xn, yn), minibatch sizes |B| and |S| Output: weights w = {wk}Kk=1 Initialize all weights and local variational parameters for iteration t = 1, 2, . . . , do # Sample minibatches: Sample a minibatch of data, B \u2286 {1, . . . , N} for n \u2208 B do\nSample a set of labels, Sn \u2286 {1, . . . ,K} {yn} end for # Local step (E step): for n \u2208 B do\nSample auxiliary variable un \u223c q(rep)(un) Transform auxiliary variable, \u03b5n = T (un ; \u03bdn) Estimate the gradient \u2207\u0303\u03bdnL(n) (Eq. 9) Update variational param., \u03bdn \u2190 \u03bdn+\u03b1(t)\u2207\u0303\u03bdnL(n)\nend for # Global step (M step): Sample \u03b5n \u223c q(\u03b5n ; \u03bdn) for all n \u2208 B Set g= N|B| K\u22121 |S| \u2211 n\u2208B \u2211 k\u2032\u2208Sn \u2207wlog\u03a6(\u03b5n+\u03c8nyn\u2212\u03c8nk\u2032)\nGradient step on the weights, w \u2190 w + \u03c1(t)g end for\nparameterized by some variational parameters \u03bd. We assume that this distribution is reparameterizable, i.e., we can sample from q(\u03b5 ; \u03bd) by first sampling an auxiliary variable u \u223c q(rep)(u) and then setting \u03b5 = T (u ; \u03bd).\nIn the local step, we fit q(\u03b5 ; \u03bd) by taking a gradient step of the ELBO with respect to the variational parameters \u03bd. Since the expectations in Eq. 7 are not tractable, we obtain Monte Carlo estimates by sampling \u03b5 from the variational distribution. To sample \u03b5, we sample u \u223c q(rep)(u) and set \u03b5 = T (u ; \u03bd). To alleviate the computational complexity, we apply the reduce step, sampling a random subset S \u2286 {1, . . . ,K} {k} of outcomes. We thus form a one-sample gradient estimator as\n\u2207\u0303\u03bdL = \u2207\u03b5 log p\u0303(y, \u03b5 |\u03c8)\u2207\u03bdT (u ; \u03bd) +\u2207\u03bdH[q(\u03b5 ; \u03bd)], (9) where H[q(\u03b5 ; \u03bd)] is the entropy of the variational distribution,3 and log p\u0303(y, \u03b5 |\u03c8) is a log joint estimate,\nlog p\u0303(y, \u03b5 |\u03c8) = log \u03c6(\u03b5)+K \u2212 1 |S| \u2211 k\u2032\u2208S log\u03a6(\u03b5+\u03c8k\u2212\u03c8k\u2032).\nIn the global step, we estimate the gradient of the ELBO with respect to w. Following a similar approach, we obtain an unbiased one-sample gradient estimator as \u2207\u0303wL = K\u22121 |S| \u2211 k\u2032\u2208S \u2207w log\u03a6(\u03b5+ \u03c8k \u2212 \u03c8k\u2032).\nAlgorithm 2 summarizes the procedure to efficiently run\n3We can estimate the gradient of the entropy when it is not available analytically. Even when it is, the Monte Carlo estimator may have lower variance (Roeder et al., 2017).\nmaximum likelihood on a classification problem. We subsample observations and classes at each iteration.\nFinally, note that we can perform posterior inference on the parameters w by positing a variational distribution q(w) and taking gradient steps with respect to the variational parameters of q(w) in the global step. In this case, the reparameterization trick is needed in both the local and global step to obtain Monte Carlo estimates of the gradient.\nWe now particularize A&R for the multinomial probit and multinomial logistic models.\nA&R for multinomial probit. Consider a standard Gaussian distribution over the error terms,\n\u03c6probit(\u03b5) = 1\u221a 2\u03c0 e\u2212 1 2 \u03b5 2 , \u03a6probit(\u03b5) = \u222b \u03b5 \u2212\u221e \u03c6probit(\u03c4)d\u03c4.\nA&R chooses a Gaussian variational distribution qprobit(\u03b5 ; \u03bd) = N (\u03b5 ; \u00b5, \u03c32) and fits the variational parameters \u03bd = [\u00b5, \u03c3]>. The Gaussian is reparameterizable in terms of a standard Gaussian, i.e., q(rep)probit(u) = N (u ; 0, 1). The transformation is \u03b5 = T (u ; \u03bd) = \u00b5 + \u03c3u. Thus, the gradients in Eq. 9 are \u2207\u03bdT (u ; \u03bd) = [1, u]> and \u2207\u03bdH[qprobit(\u03b5 ; \u03bd)] = [0, 1/\u03c3]>.\nA&R for multinomial logistic. Consider now a standard logistic distribution over the errors,\n\u03c6logistic(\u03b5) = \u03c3(\u03b5)\u03c3(\u2212\u03b5), \u03a6logistic(\u03b5) = \u03c3(\u03b5),\nwhere \u03c3(\u03b5) = 11+e\u2212\u03b5 is the sigmoid function. (The logistic distribution has heavier tails than the Gaussian.) Under this model, the ELBO in Eq. 7 takes the form\nLlogistic=Eq(\u03b5) [ log\n\u03c3(\u03b5)\u03c3(\u2212\u03b5) q(\u03b5) + \u2211 k\u2032 6=k log \u03c3(\u03b5+\u03c8k\u2212\u03c8k\u2032) ] .\nNote the close resemblance between this expression and the one-vs-each (OVE) bound of Titsias (2016),\nLOVE = \u2211 k\u2032 6=k log \u03c3(\u03c8k \u2212 \u03c8k\u2032). (10)\nHowever, while the former is a bound on the multinomial logistic model, the OVE is a bound on the softmax.\nA&R sets qlogistic(\u03b5 ; \u03bd) = 1\u03b2\u03c3 ( \u03b5\u2212\u00b5 \u03b2 ) \u03c3 ( \u2212 \u03b5\u2212\u00b5\u03b2 ) , a logistic distribution. The variational parameters are \u03bd = [\u00b5, \u03b2]>. The logistic distribution is reparameterizable, with q(rep)logistic(u) = \u03c3(u)\u03c3(\u2212u) and transformation \u03b5 = T (u ; \u03bd) = \u00b5+ \u03b2u. The gradient of the entropy in Eq. 9 is \u2207\u03bdH[qlogistic(\u03b5 ; \u03bd)] = [0, 1/\u03b2]>."}, {"heading": "4. Experiments", "text": "We showcase augment and reduce (A&R) on a linear classification task. Our goal is to assess the predictive performance\nof A&R in this classification task, to assess the quality of the marginal bound of the data, and to compare its complexity4 with existing approaches.\nWe run A&R for three different models of categorical distributions (softmax, multinomial probit, and multinomial logistic).5 For the softmax model, we compare A&R against the one-vs-each (OVE) bound (Titsias, 2016). Just like A&R, OVE is a rigorous lower bound on the marginal likelihood. It can also run on a single machine,6 and it has been shown to outperform other approaches.\nFor softmax, A&R runs nearly as fast as OVE but has better predictive performance and provides a tighter bound on the marginal likelihood than OVE. On two small datasets, the A&R bound closely reaches the marginal likelihood of exact softmax maximum likelihood estimation.\nWe now describe the experimental settings. In Section 4.1, we analyze synthetic data and K = 104 classes. In Section 4.2, we analyze five real datasets.\nExperimental setup. We consider linear classification, where the mean utilities are \u03c8nk = w>k xn + w (0) k . We fit the model parameters (weights and biases) via maximum likelihood estimation, using stochastic gradient ascent. We initialize the weights and biases randomly, drawing from a Gaussian distribution with zero mean and standard deviation 0.1 (0.001 for the biases). For each experiment, we use the same initialization across all methods.\nAlgorithms 1 and 2 require setting a step size schedule for \u03c1(t). We use the adaptive step size sequence proposed by Kucukelbir et al. (2017), which combines RMSPROP (Tieleman & Hinton, 2012) and Adagrad (Duchi et al., 2011). We set the step size using the default parameters, i.e.,\n\u03c1(t) = \u03c10 \u00d7 t\u22121/2+10 \u221216 \u00d7 ( 1 + \u221a s(t) )\u22121 ,\ns(t) = 0.1(g(t))2 + 0.9s(t\u22121).\nWe set \u03c10 = 0.02 and we additionally decrease \u03c10 by a factor of 0.9 every 2000 iterations. We use the same step size sequence for OVE.\nWe set the step size \u03b1(t) in Algorithm 1 as \u03b1(t) = (1+t)\u22120.9, the default values suggested by Hoffman et al. (2013). For the step size \u03b1(t) in Algorithm 2, we set \u03b1(t) = 0.01(1 + t)\u22120.9. For the multinomial logit and multinomial probit A&R, we parameterize the variational distributions in terms of their means \u00b5 and their unconstrained scale parameter \u03b3, such that the scale parameter is log(1 + exp(\u03b3)).\n4We focus on runtime cost. A&R requires O(N) memory storage capacity due to the local variational parameters.\n5Code for A&R is available at https://github.com/ franrruiz/augment-reduce.\n6A&R is amenable to an embarrassingly parallel algorithm, but we focus on single-core procedures."}, {"heading": "4.1. Synthetic Dataset", "text": "We mimic the toy experiment of Titsias (2016) to assess how well A&R estimates the categorical probabilities. We generate a dataset with 104 classes andN = 3\u00d7105 observations, each assigned label k with probability pk \u221d p\u03032k, where each p\u0303k is randomly generated from a uniform distribution in [0, 1]. After generating the data, we have K = 9,035 effective classes (thus we use this value for K). In this simple setting, there are no observed covariates xn.\nWe estimate the probabilities pk via maximum likelihood on the biases w(0)k . We posit a softmax model, and we apply both the variational expectation maximization (VEM) in Section 3.1 and the OVE bound. For both approaches, we choose a minibatch size of |B| = 500 observations and |S| = 100 classes, and we run 5\u00d7 105 iterations.\nWe run each approach on one CPU core. On average, the wall-clock time per epoch (one epoch takes N/|B| = 600 iterations) is 0.196 minutes for softmax A&R and 0.189 minutes for OVE. A&R is slightly slower because of the local step that OVE does not require; however, the bound on the marginal log likelihood is tighter (by orders of magnitude) for A&R than for OVE (\u22122.62\u00d7106 and\u22121.40\u00d7109, respectively). The estimated probabilities are similar for both methods: the average absolute error is 3.00 \u00d7 10\u22126 for A&R and 3.65 \u00d7 10\u22126 for OVE; the difference is not statistically significant."}, {"heading": "4.2. Real Datasets", "text": "We now turn to real datasets. We consider MNIST and Bibtex (Katakis et al., 2008; Prabhu & Varma, 2014), where we can compare against the exact softmax. We also analyze Omniglot (Lake et al., 2015), EURLex-4K (Mencia & Furnkranz, 2008; Bhatia et al., 2015), and AmazonCat-13K (McAuley & Leskovec, 2013).7 Table 1 gives information about the structure of these datasets.\nWe run each method for a fixed number of iterations. We set the minibatch sizes |B| and |S| beforehand. The specific values for each dataset are also in Table 1.\nData preprocessing. For MNIST, we divide the pixel values by 255 so that the maximum value is one. For Omniglot, following other works in the literature (e.g., Burda et al., 2016), we resize the images to 28 \u00d7 28 pixels. For EURLex-4K and AmazonCat-13K, we normalize the covariates dividing by their maximum value.\nBibtex, EURLex-4K, and AmazonCat-13K are multi-class datasets, i.e., each observation may be assigned more than one label. Following Titsias (2016), we keep only the first non-zero label for each data point. See Table 1 for the resulting number of classes in each case.\nEvaluation. For the softmax, we compare A&R against the OVE bound.8 We also compare against the exact softmax on MNIST and Bibtex, where the number of classes is small. For the multinomial probit and multinomial logistic models, we also report the predictive performance of A&R.\nWe evaluate performance with test log likelihood and accuracy. The accuracy is the fraction of correctly classified instances, assuming that we assign the most likely label (i.e., the one with the highest mean utility). To compute the test log likelihood, we use Eq. 1 for the softmax and Eq. 5 for the multinomial probit and multinomial logistic models. We approximate the integral in Eq. 5 with 1,000 samples using importance sampling (we use a Gaussian distribution with mean 5 and standard deviation 5 as a proposal).\nResults. Table 2 shows the wall-clock time per epoch for each method and dataset. In general, softmax A&R is almost as fast as OVE because the extra local step can be performed efficiently without additional expensive operations. It requires to evaluate exponential functions that can be reused in the global step. Multinomial probit A&R and multinomial\n7MNIST is available at http://yann.lecun.com/ exdb/mnist. Omniglot can be found at https://github. com/brendenlake/omniglot. Bibtex, EURLex-4K, and AmazonCat-13K are available at http://manikvarma.org/ downloads/XC/XMLRepository.html.\n8We also implemented the approach of Botev et al. (2017), but we do not report the results because it did not outperform OVE in terms of test log-likelihood on four out of the five considered datasets. On the fifth dataset, softmax A&R was still superior.\nTable 1. Statistics and experimental settings of the considered datasets. Ntrain and Ntest are the number of training and test data points. The number of classes is the resulting value after the preprocessing step (see text). The minibatch sizes correspond to |B| and |S|, respectively.\ndataset\nMNIST Bibtex\nOmniglot EURLex-4K\nAmazonCat-13K\nNtrain Ntest covariates classes\n60, 000 10, 000 784 10 4, 880 2, 413 1, 836 148 25, 968 6, 492 784 1, 623 15, 539 3, 809 5, 000 896 1, 186, 239 306, 782 203, 882 2, 919\nminibatch (obs.) minibatch (classes) iterations\n500 1 35, 000 488 20 5, 000 541 50 45, 000 379 50 100, 000 1, 987 60 5, 970\nTable 2. Average time per epoch for each method and dataset. Softmax A&R (Section 3.1) is almost as fast as OVE. The A&R approaches in Section 3.2 take longer because they require some additional computations, but they are still competitive.\ndataset\nMNIST Bibtex\nOmniglot EURLex-4K\nAmazonCat-13K\nOVE (Titsias, 2016)\n0.336 s 0.181 s 4.47 s 5.54 s 2.80 h\nA&R [this paper] softmax multi. probit multi. logistic\n0.337 s 0.431 s 0.511 s 0.188 s 0.244 s 0.246 s 4.65 s 5.63 s 5.57 s 5.65 s 6.46 s 6.23 s 2.80 h 2.82 h 2.91 h\nlogistic A&R are slightly slower because of the local step, but they are still competitive.\nFor the five datasets, Figure 2 shows the evolution of the evidence lower bound (ELBO) as a function of wall-clock time for the softmax A&R (Eq. 8), compared to the OVE (Eq. 10). For easier visualization, we plot a smoothed version of the bounds after applying a moving average window of size 100. (For AmazonCat-13K, we only compute the ELBO every 50 iterations and we use a window of size 5.) Softmax A&R provides a significantly tighter bound for most datasets (except for Bibtex, where the ELBO of A&R is close to the OVE bound). For MNIST and Bibtex, we also plot the marginal likelihood obtained after running maximum likelihood estimation on the exact softmax model. The ELBO of A&R nearly achieves this value.\nFinally, Table 3 shows the predictive performance for all methods across all datasets. We report test log likelihood and accuracy. Softmax A&R outperforms OVE in both metrics on all but one dataset (except EURLex-4K). Although our goal is not to compare performance across different models, for completeness Table 3 also shows the predictive performance of multinomial probit A&R and multinomial logistic A&R. In general, softmax A&R provides the highest\ntest log likelihood, but multinomial probit A&R outperforms all other methods in EURLex-4K and AmazonCat-13K. Additionally, multinomial logistic A&R presents better predictive performance than OVE on Omniglot and Bibtex."}, {"heading": "5. Conclusion", "text": "We have introduced augment and reduce (A&R), a scalable method to fit models involving categorical distributions. A&R is general and applicable to many models, including the softmax and the multinomial probit. On classification tasks, we found that A&R outperforms state-of-the art algorithms with little extra computational cost."}, {"heading": "Acknowledgements", "text": "This work was supported by ONR N00014-15-1-2209, ONR 133691-5102004, NIH 5100481-5500001084, NSF CCF1740833, the Alfred P. Sloan Foundation, the John Simon Guggenheim Foundation, Facebook, Amazon, and IBM. Francisco J. R. Ruiz is supported by the EU Horizon 2020 programme (Marie Sk\u0142odowska-Curie Individual Fellowship, grant agreement 706760). We also thank Victor Elvira and Pablo Moreno for their comments and help."}], "year": 2018, "references": [{"title": "Bayesian analysis of binary and polychotomous response data", "authors": ["J.H. Albert", "S. Chib"], "venue": "Journal of the American Statistical Association,", "year": 1993}, {"title": "Neural machine translation by jointly learning to align and translate", "authors": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "In International Conference on Learning Representations,", "year": 2015}, {"title": "Variational algorithms for approximate Bayesian inference", "authors": ["M.J. Beal"], "venue": "PhD thesis, Gatsby Computational Neuroscience Unit,", "year": 2003}, {"title": "Quick training of probabilistic neural nets by importance sampling", "authors": ["Y. Bengio", "S\u00e9n\u00e9cal", "J.-S"], "venue": "In Artificial Intelligence and Statistics,", "year": 2003}, {"title": "Neural probabilistic language models", "authors": ["Y. Bengio", "H. Schwenk", "Sen\u00e9cal", "J.-S", "F. Morin", "Gauvain", "J.-L"], "venue": "In Innovations in Machine Learning. Springer,", "year": 2006}, {"title": "Conditional probability tree estimation analysis and algorithms", "authors": ["A. Beygelzimer", "J. Langford", "Y. Lifshits", "G.B. Sorkin", "L. Strehl"], "venue": "In Uncertainty in Artificial Intelligence,", "year": 2009}, {"title": "Sparse local embeddings for extreme multi-label classification", "authors": ["K. Bhatia", "H. Jain", "P. Kar", "M. Varma", "P. Jain"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2015}, {"title": "Variational inference: A review for statisticians", "authors": ["D. Blei", "A. Kucukelbir", "J. McAuliffe"], "venue": "Journal of American Statistical Association,", "year": 2017}, {"title": "A correlated topic model of Science", "authors": ["D.M. Blei", "J.D. Lafferty"], "venue": "The Annals of Applied Statistics,", "year": 2007}, {"title": "Complementary sum sampling for likelihood approximation in large scale classification", "authors": ["A. Botev", "B. Zheng", "D. Barber"], "venue": "In Artificial Intelligence and Statistics,", "year": 2017}, {"title": "Efficient bounds for the softmax and applications to approximate inference in hybrid models", "authors": ["G. Bouchard"], "venue": "In Advances in Neural Information Processing Systems, Workshop on Approximate Inference in Hybrid Models,", "year": 2007}, {"title": "Importance weighted autoencoders", "authors": ["Y. Burda", "R. Grosse", "R. Salakhutdinov"], "venue": "In International Conference on Learning Representations,", "year": 2016}, {"title": "An exploration of softmax alternatives belonging to the spherical loss family", "authors": ["A. de Br\u00e9bisson", "P. Vincent"], "venue": "In International Conference on Learning Representations,", "year": 2016}, {"title": "Bayes optimal multilabel classification via probabilistic classifier chains", "authors": ["K. Dembczy\u0144ski", "W. Cheng", "E. H\u00fcllermeier"], "venue": "In International Conference on Machine Learning,", "year": 2010}, {"title": "Fast and robust neural network joint models for statistical machine translation", "authors": ["J. Devlin", "R. Zbib", "Z. Huang", "T. Lamar", "R. Schwartz", "J. Makhoul"], "venue": "In Annual Meeting of the Association for Computational Linguistics", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "authors": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "year": 2011}, {"title": "Unbiased scalable softmax optimization", "authors": ["F. Fagan", "G. Iyengar"], "venue": "In arXiv:1803.08577,", "year": 2018}, {"title": "Variational Bayesian multinomial probit regression with Gaussian process priors", "authors": ["M. Girolami", "S. Rogers"], "venue": "Neural Computation,", "year": 2006}, {"title": "Distributed training of large-scale logistic models", "authors": ["S. Gopal", "Y. Yang"], "venue": "In International Conference on Machine Learning,", "year": 2013}, {"title": "Efficient softmax approximation for GPUs", "authors": ["E. Grave", "A. Joulin", "M. Ciss\u00e9", "D. Grangier", "H. J\u00e9grou"], "venue": "In arXiv:1609.04309,", "year": 2017}, {"title": "Statistical theory of extreme values and some practical applications: A series of lectures", "authors": ["E.J. Gumbel"], "venue": "U. S. Govt. Print. Office,", "year": 1954}, {"title": "Training highly multiclass classifiers", "authors": ["M.R. Gupta", "S. Bengio", "W. Jason"], "venue": "Journal of Machine Learning Research,", "year": 2014}, {"title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models", "authors": ["M. Gutmann", "A. Hyv\u00e4rinen"], "venue": "In Artificial Intelligence and Statistics,", "year": 2010}, {"title": "Stochastic variational inference", "authors": ["M.D. Hoffman", "D.M. Blei", "C. Wang", "J. Paisley"], "venue": "Journal of Machine Learning Research,", "year": 2013}, {"title": "Blackout: Speeding up recurrent neural network language models with very large vocabularies", "authors": ["S. Ji", "S.V.N. Vishwanathan", "N. Satish", "M.J. Anderson", "P. Dubey"], "venue": "In International Conference on Learning Representations,", "year": 2016}, {"title": "Multilabel text classification for automated tag suggestion", "authors": ["I. Katakis", "G. Tsoumakas", "I. Vlahavas"], "venue": "In ECML/PKDD Discovery Challenge,", "year": 2008}, {"title": "A stick-breaking likelihood for categorical data analysis with latent Gaussian models", "authors": ["M.E. Khan", "S. Mohamed", "B.M. Marlin", "K.P. Murphy"], "venue": "In Artificial Intelligence and Statistics,", "year": 2012}, {"title": "Auto-encoding variational Bayes", "authors": ["D.P. Kingma", "M. Welling"], "venue": "In International Conference on Learning Representations,", "year": 2014}, {"title": "Automatic differentiation variational inference", "authors": ["A. Kucukelbir", "D. Tran", "R. Ranganath", "A. Gelman", "D.M. Blei"], "venue": "Journal of Machine Learning Research,", "year": 2017}, {"title": "On the multistage Bayes classifier", "authors": ["M. Kurzynski"], "venue": "Pattern Recognition,", "year": 1988}, {"title": "Human-level concept learning through probabilistic program induction", "authors": ["B.M. Lake", "R. Salakhutdinov", "J.B. Tenenbaum"], "year": 2015}, {"title": "The multiple multiplicative factor model for collaborative filtering", "authors": ["B.M. Marlin", "R.S. Zemel"], "venue": "In International Conference on Machine Learning,", "year": 2004}, {"title": "Hidden factors and hidden topics: understanding rating dimensions with review text", "authors": ["J. McAuley", "J. Leskovec"], "venue": "In ACM Conference on Recommender Systems,", "year": 2013}, {"title": "Modeling the choice of residential location", "authors": ["D. McFadden"], "venue": "In Spatial Interaction Theory and Residential Location,", "year": 1978}, {"title": "Efficient pairwise multilabel classification for large-scale problems in the legal domain", "authors": ["E.L. Mencia", "J. Furnkranz"], "venue": "In ECML/PKDD,", "year": 2008}, {"title": "Distributed representations of words and phrases and their compositionality", "authors": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2013}, {"title": "Hierarchical probabilistic neural network language model", "authors": ["F. Morin", "Y. Bengio"], "venue": "In Artificial Intelligence and Statistics,", "year": 2005}, {"title": "Fast amortized inference and learning in log-linear models with randomly perturbed nearest neighbor search", "authors": ["S. Mussmann", "D. Levy", "S. Ermon"], "venue": "In Uncertainty in Artificial Intelligence,", "year": 2017}, {"title": "FastXML: Fast, accurate and stable tree-classifier for eXtreme multi-label learning", "authors": ["Y. Prabhu", "M. Varma"], "venue": "In KDD,", "year": 2014}, {"title": "DS-MLR: exploiting double separability for scaling up distributed multinomial logistic regression", "authors": ["P. Raman", "S. Matsushima", "X. Zhang", "H. Yun", "S.V.N. Vishwanathan"], "year": 2017}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "authors": ["D.J. Rezende", "S. Mohamed", "D. Wierstra"], "venue": "In International Conference on Machine Learning,", "year": 2014}, {"title": "Nested expectation propagation for Gaussian process classification with a multinomial probit likelihood", "authors": ["J. Riihim\u00e4ki", "P. Jyl\u00e4nki", "A. Vehtari"], "venue": "Journal of Machine Learning Research,", "year": 2013}, {"title": "A stochastic approximation method", "authors": ["H. Robbins", "S. Monro"], "venue": "The Annals of Mathematical Statistics,", "year": 1951}, {"title": "Sticking the landing: Simple, lower-variance gradient estimators for variational inference", "authors": ["G. Roeder", "Y. Wu", "D. Duvenaud"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2017}, {"title": "Contrastive estimation: Training log-linear models on unlabeled data", "authors": ["N.A. Smith", "E. Jason"], "venue": "In Association for Computational Linguistics,", "year": 2005}, {"title": "Reinforcement Learning: An Introduction", "authors": ["R.S. Sutton", "A.G. Barto"], "year": 1998}, {"title": "Lecture 6.5-RMSPROP: Divide the gradient by a running average of its recent magnitude", "authors": ["T. Tieleman", "G. Hinton"], "venue": "Coursera: Neural Networks for Machine Learning,", "year": 2012}, {"title": "One-vs-each approximation to softmax for scalable estimation of probabilities", "authors": ["M.K. Titsias"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2016}, {"title": "Doubly stochastic variational Bayes for non-conjugate inference", "authors": ["M.K. Titsias", "M. L\u00e1zaro-Gredilla"], "venue": "In International Conference on Machine Learning,", "year": 2014}, {"title": "Effective and efficient multilabel classification in domains with large number of labels", "authors": ["G. Tsoumakas", "I. Katakis", "I. Vlahavas"], "venue": "In ECML/PKDD Workshop on Mining Multidimensional Data,", "year": 2008}, {"title": "Deep networks with large output spaces", "authors": ["S. Vijayanarasimhan", "J. Shlens", "R. Monga", "J. Yagnik"], "venue": "In arXiv:1412.7479,", "year": 2014}, {"title": "Efficient exact gradient update for training deep networks with very large sparse targets", "authors": ["P. Vincent", "A. de Br\u00e9bisson", "X. Bouthillier"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2015}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "authors": ["R.J. Williams"], "venue": "Machine Learning,", "year": 1992}], "id": "SP:7187d0dbb481b9bb168ba5c4f829ce571fcd1785", "authors": [{"name": "Francisco J. R. Ruiz", "affiliations": []}, {"name": "Michalis K. Titsias", "affiliations": []}, {"name": "Adji B. Dieng", "affiliations": []}, {"name": "David M. Blei", "affiliations": []}], "abstractText": "Categorical distributions are ubiquitous in machine learning, e.g., in classification, language models, and recommendation systems. However, when the number of possible outcomes is very large, using categorical distributions becomes computationally expensive, as the complexity scales linearly with the number of outcomes. To address this problem, we propose augment and reduce (A&R), a method to alleviate the computational complexity. A&R uses two ideas: latent variable augmentation and stochastic variational inference. It maximizes a lower bound on the marginal likelihood of the data. Unlike existing methods which are specific to softmax, A&R is more general and is amenable to other categorical models, such as multinomial probit. On several large-scale classification problems, we show that A&R provides a tighter bound on the marginal likelihood and has better predictive performance than existing approaches.", "title": "Augment and Reduce: Stochastic Inference for Large Categorical Distributions"}