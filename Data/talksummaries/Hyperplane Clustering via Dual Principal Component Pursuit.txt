1	72	A particular case of this problem is hyperplane clustering, which arises when the data lie in a union of hyperplanes, as in, e.g., projective motion segmentation (Vidal et al., 2006), 3D point cloud analysis (Sampath & Shan, 2010) and hybrid system identification (Vidal et al., 2003; Bako, 2011).
2	95	Even though in some ways hyperplane clustering is simpler than general subspace clustering, since, e.g., the dimensions of the subspaces are equal and known a priori, modern self-expressivenessbased methods (Liu et al., 2013; Lu et al., 2012; Elhamifar & Vidal, 2013; Wang et al., 2013; You et al., 2016), in principle do not apply in this case, because they require small relative subspace dimensions d/D, where d,D are the dimensions of the subspace and ambient space, respectively.
9	22	The minimization is done via a stochastic gradient descent scheme, and searches directly for a basis of each subspace, which makes it slower to converge for hyperplanes.
13	14	Motivated by the robustness of DPCP to outliers, one could naively use it for hyperplane clustering by recovering the normal vector to a hyperplane one at a time, while treating points from other hyperplanes as outliers.
18	19	Experiments on synthetic data show that this DPCP-based algorithm significantly improves over similar sequential algorithms, which are based on RANSAC or REAPER.
29	43	,x (i) Ni ], belong to Hi, with ∑n i=1Ni = N .
33	41	Dual Principal Component Pursuit (DPCP) (Tsakiris & Vidal, 2017a) is a robust single subspace learning method.
34	29	Given unlabeled data X , which consist of inliers in a single subspace S of RD of dimension d < D, together with outliers to the subspace, DPCP computes a basis for the orthogonal complement of the inlier subspace S. The key idea of DPCP is to identify a single hyperplaneH with normal vector b that is maximal with respect to the data X .
38	75	If S is a hyperplane, i.e., dimS = D−1, and if there are at least dimS+1 = D inliers, it is straightforward to show that (1) has a unique up to scale global minimizer, the normal vector to the inlier hyperplane.
39	56	Since (1) is hard to solve, we relax it to min b ∥∥∥X>b∥∥∥ 1 s.t.
47	18	Proposition 2 (Tsakiris & Vidal, 2015a) Suppose that the inliers are sufficiently uniformly distributed (in a deterministic sense defined in Grabner et al. 1997) inside the intersection of the inlier hyperplane and the (unit) sphere, and that the outliers are sufficiently uniformly distributed on the sphere.
70	107	In that case the weighted geometric median of the two lines spanned by the normals to the hyperplanes always corresponds to one of the two normals: Proposition 3 Consider an arrangement of two hyperplanes in RD with normal vectors b1, b2 and weightsN1 ≥ N2.
73	45	The continuous problem (11) is equally favorable in recovering normal vectors as global minimizers in another extreme situation, where the arrangement consists of up to D perfectly separated (orthogonal) hyperplanes: Proposition 4 Consider n ≤ D hyperplanes in RD with orthogonal normal vectors b1, .
78	12	Even when n = 3, characterizing the global minimizers of (11) as a function of the geometry and the weights seems challenging.
79	27	Nevertheless, when the three hyperplanes are equiangular and their weights are equal, the symmetry of the configuration allows us to analytically characterize the median as a function of the angle of the arrangement.
80	13	Proposition 5 Consider three hyperplanes of RD, with normal vectors b1, b2, b3 s.t.
81	18	b > i bj = cos(θ) > 0, i 6= j, and N1 = N2 = N3.
105	14	xi,b can be viewed as an approximation to the vector integral∫ x∈Ĥi Sign(b>x)x dµĤi = c ĥi,b.
127	22	Finally, a theorem of the same flavor gives conditions under which (3) converges in a finite number of iterations to b1 or −b1; see Theorem 7 in Tsakiris & Vidal 2017b.
128	12	Späth & Watson 1987; Tsakiris & Vidal 2015a propose solving the non-convex problem (2) by means of the recursion of convex optimization problems (3), referred to as DPCP-r.
129	62	This is computationally equivalent to a recursion of linear programs, which can be solved efficiently by an optimized LP solver such as GUROBI.
131	17	To alleviate this issue, we solve (2) by standard Iteratively Reweighted Least Squares (IRLS) applied to `1 minimization problems (Candès et al., 2008; Chartrand & Yin, 2008; Daubechies et al., 2010; Lerman et al., 2015).
132	36	The resulting algorithm, referred to as DPCP-IRLS, is dramatically faster than solving DPCP-r by GUROBI: a MATLAB implementation on a standard MacBook Pro with a dual core 2.5GHz processor and a total of 4GB cache memory is able to handle 6000 points of R1000 in about one minute, while in such a regime DPCP-r seems, as of now, inapplicable.
133	18	Moreover, the performance of DPCPIRLS, investigated in §5, suggests that DPCP-IRLS converges most of the time to a global minimizer of (2); the theoretical justification of this claim is ongoing research.
149	17	The ambient dimension is set to D = 4, 9, 30, as inspired by major applications where hyperplane arrangements appear, e.g., D = 4 in 3D point cloud analysis (in homogeneous coordinates), and D = 9 in twoview geometry (Cheng et al., 2015).
158	21	First, RANSAC is the best method when D = 4 irrespectively of the number of hyperplanes, since for such a low ambient dimension the probability that D − 1 = 3 randomly selected points lie in the same hyperplane is very high.
159	18	Indeed, for D = 4 RANSAC’s accuracy ranges from 99% (n = 2) to 97% (n = 4), as opposed to (for n = 4) REAPER (56%) or even DPCP-IRLS (89%) and DPCP-r (94%).
161	19	The right column of Figure 1 plots the clustering accuracy as a function of n and of the percentage of outliers, for D = 9 and additive noise as before.
168	30	This is an important problem in robotics, where estimating the geometry of a scene is essential for successful robot navigation.
182	46	Minimization of (28) is done via Graph-Cuts (Boykov et al., 2001).
