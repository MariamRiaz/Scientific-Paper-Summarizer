2	31	Numeracy and literacy refer to the ability to comprehend, use, and attach meaning to numbers and words, respectively.
3	55	Language models exhibit literacy by being able to assign higher probabilities to sentences that are both grammatical and realistic, as in this example: ‘I eat an apple’ (grammatical and realistic) ‘An apple eats me’ (unrealistic) ‘I eats an apple’ (ungrammatical) Likewise, a numerate language model should be able to rank numerical claims based on plausibility: ’John’s height is 1.75 metres’ (realistic) ’John’s height is 999.999 metres’ (unrealistic) Existing approaches to language modelling treat numerals similarly to other words, typically using categorical distributions over a fixed vocabulary.
5	18	In that respect, existing work on language modelling does not explicitly evaluate or optimise for numeracy.
12	28	We explore different strategies for modelling numerals, such as memorisation and digit-bydigit composition, and propose a novel neural architecture based on continuous probability density functions.
19	19	This vocabulary includes special symbols (e.g. ‘UNK’) to handle out-of-vocabulary tokens, such as unseen words or numerals.
47	47	In the next subsections, we examine different strategies for modelling the branch of numerals, i.e. p(st|ct=numeral,ht).
49	43	Let d1,d2...dN be the digits of numeral s. A digit-bydigit composition strategy estimates the probability of the numeral from the probabilities of its digits: p(s)=p(d1)p(d2|d1)...p(dN |d<N) (7) The d-RNN model feeds the hidden state ht of the token-level RNN into a character-level RNN (Graves, 2013; Sutskever et al., 2011) to estimate this probability.
50	38	This strategy can accommodate an open vocabulary, i.e. it eliminates the need for an UNKnumeral symbol, as the probability is normalised one digit at a time over the much smaller vocabulary of digits (digits 0-9, decimal separator, and end-of-sequence).
51	68	Inspired by the approximate number system and the mental number line (Dehaene et al., 2003), our proposed MoG model computes the probability of numerals from a probability density function (pdf) over real numbers, using a mixture of Gaussians for the underlying pdf: q(v)= K∑ k=1 πkNk(v;µk,σ2k) πk=softmax ( BTht ) , (8) where K is the number of components, πk are mixture weights that depend on hidden state ht of the token-level RNN, Nk is the pdf of the normal distribution with mean µk ∈R and variance σ2k ∈R, andB∈RD×K is a matrix.
54	20	is the cumulative density function of q(.
59	21	We propose a combination model that can select among different strategies for modelling numerals: p(s)= ∑ ∀m∈M αmp(s|m) αm=softmax ( ATht ) , (12) where M={h-softmax, d-RNN, MoG}, and A∈RD×|M|.
60	19	Since both d-RNN and MoG are openvocabulary models, the unknown numeral token can now be removed from the vocabulary of h-softmax.
66	69	As an extreme example, in a document where all words are out of vocabulary, the best perplexity is achieved by a trivial model that predicts everything as unknown.
67	23	Ueberla (1994) proposed Adjusted Perplexity (APP; Eq.
69	87	The APP is the perplexity of an adjusted model that uniformly redistributes the probability of each out-of-vocabulary class over all different types in that class: p′(s)= { p(s) 1|OOVc| if s∈OOVc p(s) otherwise (13) where OOVc is an out-of-vocabulary class (e.g. words and numerals), and |OOVc| is the cardinality of each OOV set.
76	29	In reverse order of tolerance to extreme errors, some of the most popular are Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and Median Absolute Error (MdAE): ei = vi−v̂i RMSE = √ 1 N N∑ i=1 e2i MAE = 1N N∑ i=1 |ei| MdAE = median{|ei|} (15) The above are sensitive to the scale of the data.
85	19	We used the preprocessed ARXMLIV (Stamerjohanns et al., 2010; Stamerjohanns and Kohlhase, 2008) 2 version, where papers have been converted from LATEX into a custom XML format using the LATEXML 3 tool.
86	69	We then kept all paragraphs with at least one reference to a table and a number.
91	50	We set the vocabularies to the 1,000 and 5,000 most frequent token types for the clinical and scientific datasets, respectively.
92	137	We use gated token-character embeddings (Miyamoto and Cho, 2016) for the input of numerals and token embeddings for the input and output of words, since the scope of our paper is numeracy.
107	34	The combination model had the best overall APP results for both datasets.
108	23	Evaluations on the Number Line To factor out model specific decoding processes for finding the best next numeral, we use our models to rank a set of candidate numerals: we compose the union of in-vocabulary numbers and 100 percentile points from the training set, and we convert numbers into numerals by considering all formats up to n decimal points.
119	43	Softmax versus Hierarchical Softmax Figure 3 visualises the cosine similarities of the output token embeddings of numerals for the softmax and h-softmax models.
121	27	This is not the case for h-softmax that uses two different spaces: similarities are concentrated along the diagonal and fan out as the magnitude grows, with the exception of numbers with special meaning, e.g. years and percentile points.
127	17	The h-softmax model’s probabilities are spiked, d-RNNs are saw-tooth like and MoG’s are smooth, with the occasional spike, whenever a narrow component allows for it.
134	55	Significant Digits Figure 5 shows the distributions of the most significant digits under the d-RNN model and from data counts.
136	27	The law predicts that the first digit is 1 with higher probability (about 30%) than 9 (< 5%) and weakens towards uniformity at higher digits.
158	95	We found that modelling numerals separately from other words through a hierarchical softmax can substantially improve the perplexity of LMs, that different strategies are suitable for different contexts, and that a combination of these strategies can help improve the perplexity further.
159	33	Finally, we found that using a continuous probability density function can improve prediction accuracy of LMs for numbers by substantially reducing the mean absolute percentage metric.
160	31	Our approaches in modelling and evaluation can be used in future work in tasks such as approximate information extraction, knowledge base completion, numerical fact checking, numerical question answering, and fraud detection.
