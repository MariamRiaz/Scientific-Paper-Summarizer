0	23	The 20 Question Game (Q20 Game) is a classic game that requires deductive reasoning and creativity.
1	19	At the beginning of the game, the answerer thinks of a target object and keeps it concealed.
2	20	Then the questioner tries to figure out the target object by asking questions about it, and the answerer answers each question with a simple “Yes”, “No” or “Unknown”, honestly.
4	44	In a Q20 game system, the user is considered as the answerer while the system itself acts as the questioner which requires a good question selection strategy to win the game.
5	14	As a game with the hype read your mind, Q20 has been played since the 19th century, and was brought to screen in the 1950s by the TV show Twenty Questions.
6	40	Burgener’s program (Burgener, 2006) further popularized Q20 as an electronic game in 1988, and modern virtual assistants like Microsoft XiaoIce and Amazon Alexa also incorporate this game into their system to demonstrate their intelligence.
7	22	However, it is not easy to design the algorithm to construct a Q20 game system.
19	18	Our contributions can be summarized as follows: (1) We propose a novel RL framework to learn the optimal policy of question selection in the Q20 game without any dependencies on the existing KBs of target objects.
22	14	(3) Extensive experiments show that our RL method clearly outperforms a highly engineered baseline in the real-world Q20 games where noisy answers are common.
28	26	At each time-step t, the agent picks up the promising action (select a question) according to the policy πθ(a|st), and transits from the state st to the next state st+1 after receiving the answer (“Yes”/“No”/“Unknown”) from the user.
46	26	Therefore, the state st is a probability distribution over all the objects and st,i is the confidence that the object oi is the target object otgt at time-step t. The initial state s0 can either be a uniform distribution or initialized by the prior knowledge.
48	20	For example, the founder of Tesla Inc. and the designer of SpaceX, “Elon Musk”, is more likely to be chosen compared to a CEO of a new startup.
51	68	Given the object set O and the question set A, we collect the normalized probabilities of the answer over “Yes”, “No” and “Unknown” for each object-question pair.
52	14	And the rule of state transition is define as: st+1 = st α (2) where α depends on the answer xt to the question qat which is selected by the agent at the step t: α =  [R(1, at), .
53	45	, U(|O|, at)], xt = Unk (3) where O is the object set and for each objectquestion pair (oi, qj), R(i, j) and W (i, j) are cal- culated as follows: R(i, j) = Cyes(i, j) + δ Cyes(i, j) + Cno(i, j) + Cunk(i, j) + λ W (i, j) = Cno(i, j) + δ Cyes(i, j) + Cno(i, j) + Cunk(i, j) + λ (4) R(i, j) and W (i, j) are probabilities of answering “Yes” and “No” to question qj with respect to the object oi respectively.
63	26	In the Q20 game, however, the immediate reward rt of selecting question qat is unknown at the time-step t (t < T ) because each selected question is just answered with a simple “Yes”, “No” or “Unknown” and there is no extra information provided by user.
69	37	Therefore, it is necessary to design a better reward function to estimate a non-zero immediate reward rt, and make the long-time return Gt =∑T k=0 γ krt+k+1 more informative.
72	15	The reward function takes the state-action pair (st, at) as input and outputs the corresponding immediate reward rt+1.
78	25	Furthermore, since the target object otgt can be obtained at the end of each episode, we can use the extra information provided by otgt to estimate a better immediate reward rt.
79	38	To capture the relevance between the selected questions and otgt in an episode, we further propose a objectaware RewardNet which takes the 〈st, at, otgt〉 tuple as input and produces corresponding rt+1 as output.
96	21	4.1) (2) Does our RewardNet help in the training process?
122	33	The relevance matrix is then used for question ranking and object ranking via carefully designed formulas and engineering tricks.
127	54	2.1), we further evaluate two variants of our model: the agent with uniform distribution s0 (RL uniform) and the agent with the distribution s0 initialized by the prior knowledge on the object popularity (RL popularity).
144	26	For example, as for the target object “Donald Trump”, question (a) “Is your role the American president?” is sometimes answered with “No” or “Unknown” by real users.
163	15	This indicates that ObjectRewardNet can estimate the immediate reward more quickly with the extra information provided by the target object, which leads to the faster convergence of the agent.
165	12	We use the user simulator to play the game with the RL uniform agent and two settings are taken into account: the simulator samples the target object following the uniform object distribution (UnifSimulator), and samples following the prior object distribution based on the object popularity (PopSimulator).
167	13	As we can see that UnifSimulator achieves the win rate of 80% with only 14 questions in both settings.
193	44	Besides, our RL agent can also ask various questions and does not require the existing KB and complicated engineering tricks.
195	50	As for the future work, we plan to explore methods to use machine reading to automatically construct the state transition dynamics from corpora like Wikipedia.
196	16	In this way, we can further build an end-to-end framework for the large-scale Q20 games in the real world.
