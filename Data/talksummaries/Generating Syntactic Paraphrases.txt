3	66	Our work makes the following contributions.
4	25	We show that conditioning text generation on syntactic information permits generating distinct syntactic paraphrases for the same input.
21	38	In order to generate syntactically distinct paraphrases, we formulate the generation task as a structured prediction task conditioned on both some input I and some syntactic constraint k. In this way, the same input I can be mapped to several output Ti each satisfying a different syntactic constraint ki.
24	26	For the later, we consider two subtasks namely text expansion and text reduction.
27	11	Conversely, for text reduction, the output is a text verbalising the input text minus the text verbalising the input data.
29	3	The WEBNLG dataset (Gardent et al., 2017) associates sets of RDF triples with one or more texts verbalising these sets of triples.
30	19	We derive training corpora for syntactically constrained generation from this dataset as follows.
32	43	We use the following list of syntactic labels: subject relative, object relative, sentence coordination, VP coordination, passive voice, apposition, possessive relative, pied piping, transitive clause, prepositional object, ditransitive clause, predicative clause.
33	45	Based on the resulting, syntactically enriched, WEBNLG corpus, we then build four training corpora (T2Tsyn, TXsyn, D2Tsyn, TRsyn) using the sets of RDF triples as pivots to relate paraphrases.
34	79	For data-to-text generation (D2Tsyn), the input is a linearised and delexicalised version of the set of RDF triples representing the meaning of the output text, for text-to-text generation (T2Tsyn), the input is a text and for hybrid data-and-text-to-text generation (TXsyn and TRsyn), the input is a text and a linearised RDF triple.
35	4	For the text-to-text datasets, we additionally require that, for any corpus instance 〈k, Ti, To〉, To differs from Ti on exactly one syntactic label2.
38	5	Models and Baselines D2T5best and T2T5best For each generation task, we aim to learn a model that maximises the likelihood P (T |I; k; θ) of a text given some input I , some model parameters θ and some syntactic constraint k. We use a simple encoder-decoder model where both encoder and decoder are bidirectional LSTMs and the encoder receives as input a sequence including both the input I and the syntactic constraint k. We compare our models with the output produced by beam search when no syntactic constraint applies.
43	2	The encoder and decoder have two layers.
46	10	We assess both the linguistic/syntactic adequacy of the generated texts and the diversity of the paraphrases being generated.
47	2	Syntactic and Linguistic Adequacy (BLEU, Synt, BLEUsyn).
58	15	The human evaluation further shows that the distinct outputs generated by the ALLsyn model are indeed syntactic, not purely lexical, variants.
61	42	Interestingly, the text expansion and reduction models markedly improve on traditional T2T and D2T models both in terms of linguistic adequacy (higher BLEU score) and in terms of diversity (higher number of distinct output per meaning, lower similarity between texts generated from the same meaning).
62	12	The comparison with T2T generation is particularly striking as the training data is 3 to 5 times larger for the T2Tsyn model than for the TXsyn and the TRsyn model respectively.
66	4	Moreover, the generated sentences show close similarity with the reference sentence realising the input constraint (BLEUsyn: from 48.16 to 89.32).
70	10	Table 1 shows some example outputs illustrating the main differences between the D2T5best, T2T5best and the ALLsyn model.
71	10	As these examples show, syntactically constrained generation (ALLsyn) outputs a much larger number of paraphrases.
73	6	Thus in the example shown, ALLsyn yields 15 paraphrases each with strong syntactic differences as summarised below.
75	5	One output text is made of 2 sentences and one VP coordination, another of 3 coordinated clauses and a third of two coordinated clauses and a VP coordination.
76	4	The same input property is realised by different syntactic structures.
78	3	The same content is verbalised using varying word order and clause ordering.
79	13	Thus the ALLsyn output shows four different ways of ordering the realisation of the three properties operatinOrganization (oO), runwayLength (rL), runwayName (rN) contained in the input namely, rLoO-rN (once), oO-rN-rL (6 times), oO-rL-rN (6 times) and rN-rL-oO (once).
80	5	By constrast, the baseline models output a much smaller range of syntactic paraphrases.
83	16	One reason for this is that, contrary to the D2T5best model which has a single input (namely a set of RDF triples), this model can have several inputs for the same set of RDF triples.
