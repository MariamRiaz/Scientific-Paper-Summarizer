0	113	Modeling discourse coherence (the way parts of a text are linked into a coherent whole) is essential for summarization (Barzilay and McKeown, 2005), text planning (Hovy, 1988; Marcu, 1997) question-answering (Verberne et al., 2007), and even psychiatric diagnosis (Elvevåg et al., 2007; Bedi et al., 2015).
1	34	Various frameworks exist, each tackling aspects of coherence.
2	102	Lexical cohesion (Halliday and Hasan, 1976; Morris and Hirst, 1991) models chains of words and synonyms.
4	149	Relational models like RST (Mann and Thompson, 1988; Lascarides and Asher, 1991) define relations that hierarchically structure texts.
5	37	The entity grid model (Barzilay and Lapata, 2008) and its extensions1 capture the referential coherence of entities moving in and out of focus across a text.
6	83	Each captures only a single aspect of coherence, and all focus on scoring existing sentences, rather than on generating coherent discourse for tasks like abstractive summarization.
7	36	Here we introduce two classes of neural models for discourse coherence.
8	94	Our discriminative models induce coherence by treating human generated texts as coherent examples and texts with random sentence replacements as negative examples, feeding LSTM sentence embeddings of pairs of consecutive sentences to a classifier.
9	18	These achieve stateof-the-art (96% accuracy) on the standard domainspecific sentence-pair-ordering dataset (Barzilay and Lapata, 2008), but suffer in a larger opendomain setting due to the small semantic space that negative sampling is able to cover.
11	67	These generative models obtain the best result on a large open-domain setting, including on the difficult task of reconstructing the order of every sentence in a paragraph, and our latent variable generative model significantly improves the coherence of text generated by the model.
13	28	The discriminative model treats cliques (sets of sentences surrounding a center sentence) taken from the original articles as coherent positive examples and cliques with random replacements of the center sentence as negative examples.
25	23	Since the sentence-level semantic space in the open-domain setting is huge, the sampled instances can only cover a tiny proportion of the possible negative candidates, and therefore don’t cover the space of possible meanings.
27	83	Secondly and more importantly, discriminative models are only able to tell whether an already-given chunk of text is coherent or not.
28	152	While they can thus be used in tasks like extractive summarization for sentence re-ordering, they cannot be used for coherent text generation in tasks like dialogue generation or abstractive text summarization.
29	14	We therefore introduce three neural generative models of discourse coherence.
30	70	In a coherent context, a machine should be able to guess the next utterance given the preceding ones.
31	124	A straightforward way to do that is to train a SEQ2SEQ model to predict a sentence given its contexts (Sutskever et al., 2014).
32	38	Generating sentences based on neighboring sentences resembles skip-thought models (Kiros et al., 2015), which build an encoder-decoder model by predicting tokens in neighboring sentences.
37	164	This gives rise to the coherence model (denoted by bi) that measures the bidirectional dependency between the two consecutive sentences: L(si, si+1) = 1 Ni log pB(si|si+1) + log 1 Ni+1 pF (si+1|si) (2) We separately train two models: a forward model pF (si+1|si) that predicts the next sentence based on the previous one and a backward model pB(si|si+1) that predicts the previous sentence given the next sentence.
40	55	One problem with the described uni and bi models is that sentences with higher language model probability (e.g., sentences without rare words) also tend to have higher conditional probability given their preceding or succeeding sentences.
42	82	We thus propose eliminating the influence of the language model, which yields the following coherence score: L(si, si+1) = 1 Ni [log pB(si|si+1)− log pL(si)] + 1 Ni+1 [log pB(si+1|si)− log pL(si+1)] (3) where pL(s) is the language model probability for generating sentence s. We train an LSTM language model, which can be thought of as a SEQ2SEQ model with an empty source.
43	10	3 shows that it is of the same form as the mutual information between si+1 and si, namely log[p(si+1, si)/p(si+1)p(si)].
44	15	Generation The scoring functions in Eqs.
46	59	2 and 3 can not be directly used for generation purposes, since they requires the completion of si+1 before the score can be computed.
51	108	The generation process is thus not exposed to more global features of the discourse like topics.
53	134	(2) By predicting a sentence conditioning only on its left or right neighbor, the model lacks the ability to handle the longerterm discourse dependencies across the sentences of a text.
59	44	1b), in which an HMM-LDA model provides the SEQ2SEQ model with global information for token generation, with two components: (1) Running HMM-LDA: we first run a sentence-level HMM-LDA similar to Gruber et al. (2007).
60	17	Our implementation forces all words in a sentence to be generated from the same topic, and this topic is sampled from a distribution based on the topic from previous sentence.
61	29	Let tn denote the distribution of topics for the current sentence, where tn ∈ R1×T .
62	21	We also associate each LDA topic with a K dimensional vector, representing the semantics embedded in this topic.
