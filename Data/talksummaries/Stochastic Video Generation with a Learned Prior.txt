0	98	Learning to generate future frames of a video sequence is a challenging research problem with great relevance to reinforcement learning, planning and robotics.
2	20	The main issue is the inherent uncertainty in the dynamics of the world.
3	41	For example, when a bouncing ball hits the ground unknown effects, such surface imperfections or ball spin, ensure that its future trajectory is inherently random.
4	11	Consequently, pixel-level frame predictions of such an event degrade when a deterministic loss function is used, e.g. with the ball itself blurring to accommodate multiple possible futures.
6	20	One such approach are adversarial losses (Goodfellow et al., 2014), but training difficulties and mode collapse often mean the full distribution is not captured well.
7	71	We propose a new stochastic video generation (SVG) model that combines a deterministic frame predictor with time- dependent stochastic latent variables.
9	12	The key insight we leverage for the learned-prior model is that for the majority of the ball’s trajectory, a deterministic model suffices.
11	18	The learned prior can can be interpreted as a a predictive model of uncertainty.
13	11	However, at the instant the ball hits the ground it will predict a high variance event, causing frame samples to differ significantly.
63	14	We start by explaining how our model generates new video frames, before detailing the training procedure.
64	35	Our model has two distinct components: (i) a prediction model pθ that generates the next frame x̂t, based on previous ones in the sequence x1:t−1 and a latent variable zt and (ii) a prior distribution p(z) from which zt is sampled at at each time step .
68	16	The model is trained with the aid of a separate inference model (not used a test time).
82	27	The model is trained by optimizing the variational lower bound: Lθ,φ(x1:T ) = T∑ t=1 [ Eqφ(z1:t|x1:t) log pθ(xt|x1:t−1, z1:t) −βDKL(qφ(zt|x1:t)||p(z)) ] Given the form of pθ the likelihood term reduces to an `2 penalty between x̂t and xt.
85	14	The hyper-parameter β represents the trade-off between minimizing frame prediction error and fitting the prior.
87	35	If β is too small the inference network may learn to simply copy the target frame xt, resulting in low prediction error during training but poor performance at test time due to the mismatch between the posterior qφ(zt|x1:t) and the prior p(zt).
116	14	First, the generated frames are significantly sharper with both our models (see direct comparisons to Babaeizadeh et al. (2017) in Figure Fig.
129	13	The output of LSTMθ is passed through a tanh nonlinearity before going into the frame decoder.
141	15	In the original Moving MNIST dataset (Srivastava et al., 2015) the digits move with constant velocity and bounce off the walls in a deterministic manner.
155	18	In SVG-LP , the burden of predicting points of high uncertainty can be offloaded to the prior network.
157	18	Five hundred different video sequences were constructed, each with different test digits, but whose trajectories were synchronized.
158	25	The plot shows the mean of σψ(x1:t), i.e., the variance of the distribution over zt predicted by the learned prior over 100 time steps.
180	27	3 (bottom) shows generations from the deterministic baseline and SVG-FP.
184	54	The BAIR robot pushing dataset (Ebert et al., 2017) contains videos of a Sawyer robotic arm pushing a variety of objects around a table top.
185	62	The movements of the arm are highly stochastic, providing a good test for our model.
188	33	We compute SSIM for SVG-FP and SVG-LP by drawing 100 samples from the model for each test sequence and picking the one with the best score with respect to the ground truth.
194	42	PSNR is biased towards overly smooth (i.e. blurry) results which might explain the slightly better PSNR scores obtained by Babaeizadeh et al. (2017) for later time steps.
199	24	Our recurrent inference network estimates the latent distribution for each time step allowing easy end-to-end training.
200	53	Evaluating the model on real-world sequences, we demonstrate high quality generations that are comparable to, or better than, existing approaches.
201	81	On synthetic data where it is possible to characterize the distribution of samples, we see that is able to match complex distributions of futures.
202	18	The framework is sufficiently general that it can readily be applied to more complex datasets, given appropriate encoder and decoder modules.
