0	101	Multimodal machine translation is the problem of translating sentences paired with images into a different target language (Elliott et al., 2016).
1	26	In this setting, translation is expected to be more accurate compared to purely text-based translation, as the visual context could help resolve ambiguous multi-sense words.
2	15	Examples of real-world applications of multimodal (vision plus text) translation include translating multimedia news, web product information, and movie subtitles.
6	28	In this paper, we propose a new model called Visual Attention Grounding Neural Ma- chine Translation (VAG-NMT) to leverage visual information more effectively.
10	26	When evaluated on the benchmark Multi30K and the Ambiguous COCO datasets, our VAG-NMT model demonstrates competitive performance compared to existing state-of-the-art multimodal machine translation systems.
16	22	We have included a total of 3,600 products so far and will include more in the future.
38	38	We treat the problem of multimodal machine translation as a joint optimization of two tasks: (1) learning a robust translation model and (2) constructing a visual-language shared embedding that grounds the visual semantics with text.
41	38	For the joint embedding, we obtain the text representation using a weighted sum of hidden states from the encoder of the sequenceto-sequence model and we obtain the image representation from a pre-trained convnet.
43	19	We learn the shared space with a ranking loss and the translation model with a cross entropy loss.
53	15	After encoding the source sentence, we project both the image and text into the shared space to find a good distributed representation that can capture the semantic meaning across the two modalities.
58	27	As different words in the source sentence will have different importance, we employ a visual-language attention mechanism—inspired by the attention mechanism applied in sequenceto-sequence models (Bahdanau et al., 2014)—to emphasize words that have the stronger semantic connection with the image.
85	13	We think IKEA is a good data set to simulate real-world multimodal translation problems.
90	45	We evaluate our proposed model on three datasets: Multi30K (Elliott et al., 2016), Ambiguous COCO (Elliott et al., 2017), and our newly-collected IKEA dataset.
92	53	It consists of 31,014 images, where each image is annotated with an English caption and manual translations of image captions in German and French.
94	13	Additionally, we also evaluate our model on the Ambiguous COCO Dataset collected in the WMT2017 multimodal machine translation challenge (Elliott et al., 2017).
101	13	We evaluate the performance of all models using BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014).
108	15	LIUMCVC is the best multimodal machine translation model in WMT 2017 multimodal machine translation challenge and exploits visual information with several different methods.
114	18	We observe that our multimodal VAG-NMT model has equal or slightly better result compared to the text-only neural machine translation model on the Multi30K dataset.
116	36	We suspect this is because Multi30K does not have many cases where images can help improve translation quality, as most of the image captions are short and simple.
154	36	These examples demonstrate evidence that our attention mechanism learns to assign high weights to words that have corresponding visual semantics in the image.
162	23	In the first example, our VAG-NMT properly translates the word "racquet" to “den schläger", while the Text-Only NMT mistranslated it to “den boden" which means “ground" in English.
165	16	We consistently observe that VAG-NMT translates prepositions better than Text-Only NMT.
166	24	We think it is because the pre-trained convnet features captured the relative object position that leads to a better preposition choice.
168	29	Our VAGNMT mistranslates the verb phrase “sticking out" to “springt aus" which means “jump out" in German, while Text-Only NMT translates to “streckt aus", which is correct.
170	196	We think it is because the image vectors are pretrained on an object classification task, which does not have any human action information.
172	43	The visual attention mechanism and visual context grounding module help to integrate the visual content into the sequence-to-sequence model, which leads to better translation quality compared to the model with only text information.
173	79	We achieved stateof-the-art results on the Multi30K and Ambiguous COCO dataset.
174	14	We also proposed a new product dataset, IKEA, to simulate a real-world online product description translation challenge.
175	143	In the future, we will continue exploring different methods to ground the visual context into the translation model, such as learning a multimodal shared space across image, source language text, as well as target language text.
177	29	Acknowledge We would like to thank Daniel Boyla for providing insightful discussions to help with this research.
