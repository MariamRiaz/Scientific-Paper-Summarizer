1	95	Tasks such as speech recognition (Yuhas et al., 1989), emotion recognition, (De Silva et al., 1997), (Chen et al., 1998), (Wöllmer et al., 2013), sentiment analysis, (Morency et al., 2011) as well as speaker trait analysis and media description (Park et al., 2014a) have seen a great boost in performance with developments in multimodal research.
3	68	The goal of fusion is to combine multiple modalities to leverage the complementarity of heterogeneous data and provide more robust predictions.
4	98	In this regard, an important challenge has been on scaling up fusion to multiple modalities while maintaining reasonable model complexity.
8	28	In this paper, we propose the Low-rank Multimodal Fusion, a method leveraging low-rank weight tensors to make multimodal fusion efficient without compromising on performance.
14	15	The main contributions of our paper are as follows: • We propose the Low-rank Multimodal Fusion method for multimodal fusion that can scale linearly in the number of modalities.
47	32	In addition, in order to be able to model the interactions between any subset of modalities using one tensor, Zadeh et al. (2017) proposed a simple extension to append 1s to the unimodal representations before taking the outer product.
54	42	Each W̃k contributes to one dimension in the output vector h, i.e. hk = W̃k ⋅Z .
60	38	As a solution to the problems of tensor-based fusion, we propose Low-rank Multimodal Fusion (LMF).
64	32	The idea of LMF is to decompose the weight tensor W into M sets of modality-specific factors.
69	162	The vector sets {{w(i)m,k} M m=1}Ri=1 are called the rank R decomposition factors of the original tensor.
78	17	In this section, we will introduce an efficient procedure for computing h, exploiting the fact that tensor Z naturally decomposes into the original input {zm}Mm=1, which is parallel to the modalityspecific low-rank factors.
80	19	Using the fact that Z =⊗Mm=1 zm, we can simplify equation 5: h = ( r ∑ i=1 M ⊗ m=1 w(i)m ) ⋅Z = r ∑ i=1 ( M ⊗ m=1 w(i)m ⋅Z) = r ∑ i=1 ( M ⊗ m=1 w(i)m ⋅ M ⊗ m=1 zm) = M Λ m=1 [ r ∑ i=1 w(i)m ⋅ zm] (6) where ΛMm=1 denotes the element-wise product over a sequence of tensors: Λ3t=1 xt = x1 ○ x2 ○ x3.
83	19	In addition, different modalities are decoupled in the simplified computation of h, which allows for easy generalization of our approach to an arbitrary number of modalities.
84	15	Adding a new modality can be simply done by adding another set of modality-specific factors and extend Equation 7.
86	55	Using Equation 6, we can compute h directly from input unimodal representations and their modal-specific decomposition factors, avoiding the weight-lifting of computing the large input tensor Z and W , as well as the r linear transformation.
95	18	(2) Comparison with the State-of-the-art: We evaluate the performance of LMF and state-of-theart baselines on three different tasks and datasets.
96	26	(3) Complexity Analysis: We study the modal complexity of LMF and compare it with the TFN model.
99	24	We perform our experiments on the following multimodal datasets, CMU-MOSI (Zadeh et al., 2016a), POM (Park et al., 2014b), and IEMOCAP (Busso et al., 2008) for sentiment analysis, speaker traits recognition, and emotion recognition task, where the goal is to identify speakers emotions based on the speakers’ verbal and nonverbal behaviors.
103	23	Each video is annotated with the following speaker traits: confident, passionate, voice pleasant, dominant, credible, vivid, expertise, entertaining, reserved, trusting, relaxed, outgoing, thorough, nervous, persuasive and humorous.
118	18	We compare the performance of LMF to the following baselines and state-of-the-art models in multimodal sentiment analysis, speaker trait recognition, and emotion recognition.
126	27	Multi-attention Recurrent Network The Multiattention Recurrent Network (MARN) (Zadeh et al., 2018b) explicitly models interactions between modalities through time using a neural component called the Multi-attention Block (MAB) and storing them in the hybrid memory called the Long-short Term Hybrid Memory (LSTHM).
127	37	Multiple evaluation tasks are performed during our evaluation: multi-class classification and regression.
147	18	Note that, the number of parameters above counts not only the parameters in the multimodal fusion stage but also the parameters in the subnetworks.
154	29	On an NVIDIA Quadro K4200 GPU, LMF trains with an average frequency of 1134.82 IPS (data point inferences per second) while the TFN model trains at an average of 340.74 IPS.
157	23	We observed that as the rank increases, the training results become more and more unstable and that using a very low rank is enough to achieve fairly competent performance.
159	80	LMF scales linearly in the number of modalities.
160	21	LMF achieves competitive results across different multimodal tasks.
161	25	Furthermore, LMF demonstrates a significant decrease in computational complexity from exponential to linear time.
162	103	In practice, LMF effectively improves the training and testing efficiency compared to TFN which performs multimodal fusion with tensor representations.
163	215	Future work on similar topics could explore the applications of using low-rank tensors for attention models over tensor representations, as they can be even more memory and computationally intensive.
