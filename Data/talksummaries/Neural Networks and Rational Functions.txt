4	3	For the sake of brevity, a network with ReLU activations is simply called a ReLU network.
5	13	The main theorem here states that ReLU networks and rational functions approximate each other well in the sense that -approximating one class with the other requires a representation whose size is polynomial in ln(1 / ), rather than being polynomial in 1/ .
6	13	Let ∈ (0, 1] and nonnegative inte- ger k be given.
9	9	Consider a ReLU network f : [−1,+1]d → R with at most m nodes in each of at most k layers, where each node computes z 7→ σr(a >z + b) where the pair (a, b) (possibly distinct across nodes) satisfies ‖a‖1 + |b| ≤ 1.
11	61	Perhaps the main wrinkle is the appearance of mk when approximating neural networks by rational functions.
12	22	The following theorem shows that this dependence is tight.
15	46	Note that this statement implies the desired difficulty of approximation, since a gap in the above integral (L1) distance implies a gap in the earlier uniform distance (L∞), and furthermore an r-degree rational function necessarily has ≤ 2r + 2 total terms in its numerator and denominator.
17	18	Let a ReLU network f : [−1,+1]d → R be given as in Theorem 1.1, meaning f has at most l layers and each node computes z 7→ σr(a>z+b) where where the pair (a, b) (possibly distinct across nodes) satisfies ‖a‖1 + |b| ≤ 1.
18	27	Then there exists a rational function R of degree O(ln(l/ )2) so that replacing each σr in f with R yields a function g : [−1,+1]d → R with sup x∈[−1,+1]d |f(x)− g(x)| ≤ .
19	2	Combining Theorem 1.2 and Lemma 1.3 yields an intriguing corollary.
21	73	The hard-to-approximate function f is a rational network which has a description of size O(k2).
24	3	The first thing to stress is that Theorem 1.1 is impossible with polynomials: namely, while it is true that ReLU networks can efficiently approximate polynomials (Yarotsky, 2016; Safran & Shamir, 2016; Liang & Srikant, 2017), on the other hand polynomials require degree Ω(poly(1/ )), rather than O(poly(ln(1/ ))), to approximate a single ReLU, or equivalently the absolute value function (Petrushev & Popov, 1987, Chapter 4, Page 73).
29	37	Lastly, the implementation of division in a ReLU network requires a few steps, arguably the most interesting being a “continuous switch statement”, which computes reciprocals differently based on the magnitude of the input.
30	22	The ability to compute switch statements appears to be a fairly foundational operation available to neural networks and rational functions (Petrushev & Popov, 1987, Theorem 5.2), but is not available to polynomials (since otherwise they could approximate the ReLU).
46	18	Here is a brief description of the sorts of neural networks used in this work.
47	19	Neural networks represent computation as a directed graph, where nodes consume the outputs of their parents, apply a computation to them, and pass the resulting value onward.
48	40	In the present work, nodes take their parents’ outputs z and compute σr(a>z + b), where a is a vector, b is a scalar, and σr(x) := max{0, x}; another popular choice of nonlineary is the sigmoid x 7→ (1 + exp(−x))−1.
52	6	The degree of a rational function is the maximum of the degrees of its numerator and denominator.
54	35	The starting point is a seminal result in the theory of rational functions (Zolotarev, 1877; Newman, 1964): there exists a rational function of degree O(ln(1/ )2) which can approximate the absolute value function along [−1,+1] to accuracy > 0.
57	22	The Newman polynomials N5, N9, and N13 are depicted in Figure 3.
58	14	Typical polynomials in approximation theory, for instance the Chebyshev polynomials, have very active oscillations; in comparison, the Newman polynomials look a little funny, lying close to 0 over [−1, 0], and quickly increasing monotonically over [0, 1].
65	39	The key computation, however, is as follows.
66	14	Let R(x) denote a rational approximation to σr.
68	11	Then∣∣∣σr(a>H(x) + b)−R(a>HR(x) + b)∣∣∣ ≤ ∣∣∣σr(a>H(x) + b)− σr(a>HR(x) + b)∣∣∣︸ ︷︷ ︸ ♥ + ∣∣∣σr(a>HR(x) + b)−R(a>HR(x) + b)∣∣∣︸ ︷︷ ︸ ♣ .
70	40	For the second term ♣, if a>HR(x) + b can be shown to lie in [−1,+1] (which is another easy induction), then ♣ is just the error between R and σr on the same input.
71	67	It is now easy to find a rational function that approximates a neural network, and to then bound its size.
72	18	The first step, via Lemma 1.3, is to replace each σr with a rational functionR of low degree (this last bit using Newman polynomials).
73	26	The second step is to inductively collapse the network into a single rational function.
74	23	The reason for the dependence on the number of nodes m is that, unlike polynomials, summing rational functions involves an increase in degree: p1(x) q1(x) + p1(x) q2(x) = p1(x)q2(x) + p2(x)q1(x) q1(x)q2(x) .
75	8	The final interesting bit is to show that the dependence on ml in part 2 of Theorem 1.1 (where m is the number of nodes and l is the number of layers) is tight.
