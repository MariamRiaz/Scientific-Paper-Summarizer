33	22	Most recent NMT studies have focused on model improvement (Luong et al., 2015; Tu et al., 2016b; Gehring et al., 2017; Vaswani et al., 2017), and only a few have studied the search problem directly.
34	22	For example, Khayrallah et al. (2017) and Stahlberg et al. (2016) explored searching on lattices generated by traditional Statistical Machine Translation (SMT).
37	13	Rather than considering candidates from other model’s k-best lists, we focus on the own exploration space of a single NMT model and provide a method for more efficient searching.
51	10	We include previousstep states, because equivalent states may have different sequence lengths and thus not be in the same beam-search step.
53	8	When deciding whether to merge, we also consider a criterion on model scores: we only merge state c when its score is lower than s′.
57	9	We can further extract k-best list from this structure using another beam-search on the lattice (also with length reward when comparing partial hypotheses).
58	12	Note that this beam search process can be fast, since we reuse the model scores from previous search and no extra neural computations will be included.
60	13	Here, we consider a n-gram suffix based heuristic approximation for this problem.
61	47	We adopt an approximate equivalence function: Eq′(s1, s2) ≡ s1.suffix(n) = s2.suffix(n) ∧ |s1.length− s2.length| < r Here, suffix(n) represents the n-gram suffix of the sequence of a state, and r is the threshold for the length different of the two states.
66	20	In Algorithm 1, we can store the n-gram features of the surviving states in a hash-map and replace the for-loop checking (Line 6-10) with hashing, making the extra time-complexity O(1) for each state.
67	8	During experiments, we found the extra cost brought by feature matching is far less than the cost of original neural computation.
73	43	All the experiments were carried out on one P100 GPU.
74	17	For Zh-En, we set the vocabulary size of both sides to 30K, and for En-De, we adopted 50K BPE operations (Sennrich et al., 2016).
83	15	Figure 2 show the results of various beam sizes on the concatenation of all test sets.
84	14	Separate results are given in the supplementary material.
85	48	As shown by the speed curves, merging adds little extra cost (less than 10%) to decoding at the same beam size.
88	75	For translation quality, the results indicate that the proposed methods can yield improvements at various beam sizes for Zh-En and small beam sizes for En-De.
94	11	A possible explanation for this is that in NIST Zh-En dataset, each source sentences has four references for evaluation, which encourages the diversity brought by expanding reached search space.
96	13	The En-De dataset also has only one reference and is similar to this case.
98	9	This concerns more on modeling than searching and corresponds with previous findings on the relations between NMT searching and modeling (Tu et al., 2016a; Niehues et al., 2017; Li et al., 2018).
99	10	The potential of the proposed method might be better realized with improved NMT models.
103	8	For a beam size of 10, with influences from the local pruner and the proposed merger, the average expanding size is 7.60 for each step, and the average number of merger-pruned partial hypotheses is 0.61 per step (22.5 per sentence).
108	11	To evaluate this assumption, we calculated the similarity between the hidden layers of the merged partial hypotheses.
112	10	First, we investigated the model scores of their predictions.
119	127	Merge-based searcher could obtain an oracle score of 47.83, while ordinary beam searcher could only get 42.57.
121	19	This indicates that recombination helps to touch more output space.
124	31	The potential of recombination may be further realized by improving how the output sequences are modeled.
127	12	For example, a model-based equivalence function can be trained by using the neural features (hidden layers in RNN).
128	18	However, modelbased equivalence functions may bring extra neural computation cost and be harder to efficiently implemented.
129	36	In this work, we focus on the merging mechanism and leave the study of equivalence function for future work.
