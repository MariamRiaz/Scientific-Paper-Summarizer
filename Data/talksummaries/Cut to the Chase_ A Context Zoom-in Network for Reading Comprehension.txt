21	21	Experimental results show the usefulness of our framework for RC tasks and we outperform stateof-the-art results on this dataset.
23	8	First, the identification of relevant regions of text is computed by the Co-attention and Context Zoom layers as explained in Sections 2.1 and 2.2.
24	31	Second, the comprehension of identified regions of text and output generation is computed by Answer Generation block as explained in Section 2.3.
25	25	The words in the document, question and answer are represented using pre-trained word embeddings (Pennington et al., 2014).
26	27	These wordbased embeddings are concatenated with their corresponding char embeddings.
30	44	Let di be the vector representation for the document word i, qj be the vector for the question word j, and ld and lq be the lengths of the document and question respectively.
37	41	The Split Context operation splits the attended document vectors into sentences or fixed size chunks (useful when sentence tokenization is not available for a particular language).
45	15	The policy of the reinforcement learner is defined as π(r|u; θz) = ψr, where ψr is the probability of a text region r (agent’s action) being selected, u is the environment state as defined in Eq.
52	89	The decoder block is based on an attention mechanism (Bahdanau et al., 2015) and a copy mechanism by using a pointer network similar to (See et al., 2017).
59	35	(6) To allow decoder to copy words from the encoder sequence, we compute a soft gate (Pcopy), which helps the decoder to generate a word by sampling from the fixed vocabulary or by copying from a selected text regions (ψr).
64	7	We jointly estimate the parameters of our model coming from the Co-attention, Context Zoom, and Answer Generation layers, which are denoted as θa, θz , and θg respectively.
67	6	We therefore formulate the estimation of θz as a reinforcement learning problem via a policy gradient method.
69	28	We use mean F-score of ROUGE-1, ROUGE-2, and ROUGE-L (Lin and Hovy, 2003) as our reward function R. The objective function to maximize is the expected reward under the probability distribution of current text regions ψr, i.e., J2(θz) = Ep(r|θz)[R].
72	6	It is shown that any number will reduce the variance (Williams, 1992; Zaremba and Sutskever, 2015), here we used the mean of the mini-batch reward b as our baseline.
74	5	The NarrativeQA dataset (Kočiskỳ et al., 2017) consists of fictional stories gathered from books and movie scripts, where corresponding summaries and question-answer pairs are generated with the help of human experts and Wikipedia articles.
80	52	In both baselines we replace the span prediction layer with an answer generation layer.
83	5	We split each document into sentences using the sentence tokenizer of the NLTK toolkit (Bird and Loper, 2004).
86	7	All the weights of the model are initialized by Glorot Initialization (Glorot et al., 2011) and biases are initialized with zeros.
88	18	All the words that do not appear in Glove are initialized by sampling from a uniform random distribution between [-0.05, 0.05].
89	16	We apply dropout (Srivastava et al., 2014) between the layers with keep probability of 0.8 (i.e dropout=0.2).
90	5	The number of hidden units are set to 100.
91	40	We trained our model with the AdaDelta (Zeiler, 2012) optimizer for 50 epochs, an initial learning rate of 0.1, and a minibatch size of 32.
93	28	Table 2 shows the performance of various models on NarrativeQA.
99	23	We also experimented with various sample sizes to see the effect of intra sentence relations for an- swer generation.
100	14	The performance of the model improved dramatically with sample sizes 3 and 5 compared to the sample size of 1.
101	6	These results show that the importance of selecting multiple relevant sentences for generating an answer.
102	34	In addition, the low performance of Baseline 2 indicates that just selecting multiple sentences is not enough, they should also be related to each other.
104	41	We have proposed a new neural-based architecture which condenses an original document to facilitate fast comprehension in order to generate better well-formed answers than span based prediction models.
105	36	Our model achieved the best performance on the challenging NarrativeQA dataset.
106	50	Future work can focus for example on designing an inexpensive preprocess layer, and other strategies for improved performance on answer generation.
