1	12	As one of the most successful variational Bayesian models, Conditional Variational Auto-Encoder (CVAE) (Kingma et al., 2014) was proposed to improve upon the traditional Sequence-to-Sequence (Seq2Seq) dialogue models.
2	20	The CVAE based models incorporate stochastic latent variables into decoders in order to generate more relevant and diverse responses (Serban et al., 2017; Zhao et al., 2017; Shen et al., 2017).
3	69	However, existing CVAE based models normally rely on the unimodal distribution with a single latent variable to provide the global guidance to response generation, which is not sufficient to capture the complex semantics and high variability of responses.
4	49	As a result, the autoregressive decoders used in response generation always tend to ignore these oversimple latent variables and degrade the CVAE based model to the simple Seq2Seq model (aka.
6	86	However, in open-domain conversations, an utterance may have various responses which form complex multimodal distributions.
7	25	To overcome this problem and improve the quality of generated responses, we propose a novel model, named Variational Autoregressive Decoder (VAD) to iteratively incorporate a series of latent variables into the autoregressive decoder.
8	26	In particular, a distinct latent variable sampled from CVAE is associated with each time step of the generation, and it is used to condition the next state of the autoregressive decoder (e.g., the hidden state of a RNN).
11	25	Since the hidden states of the backward RNN contain the information of the succeeding words in the response, they can be used as the guidance for the latent variables to capture the long-term dependency on the future content.
38	19	The encoder is a bidirectional GRU that encodes the query sequence as the concatenation of the hidden states of a forward and a backward GRUs.
56	12	Owing to the autoregressive structure of VAD, the hidden state of backward RNN ←− hdt is used to condition the latent variable zt, which can be seen as a long-term guidance to the generation.
59	24	The input to GRU is the combination of the previous word’s embedding yt−1, the context vector produced by an attention model ct and the latent variable zt.
61	35	Inference Model We use the hidden states of the backward RNN running through the response sequence as an additional input to the inference model.
62	23	The backward RNN processes the sequence by, ←− hdt = ←−− GRU(yt+1, ←−− hdt+1) (5) The backward hidden state ←− hdt contains the information of succeeding tokens, and it serves as a future plan for generation.
74	20	The idea of the SBOW auxiliary objective is to sequentially predict the bag of succeeding words ybow(t+1,T ) in the response using the latent variable zt at each time step.
80	24	The loss function of our model is the sum of the losses at each time step, including the weighed sum of the ELBO loss LELBO(t) and the auxiliary lossLAUX(t) whereLELBO(t) can be further decomposed into a log-likelihood loss and the KL divergence: L = ∑ t [LELBO(t) + αLAUX(t)] = ∑ t [(LLL(t)− LKL(t)) + αLAUX(t)] (11) Here, LLL(t) denotes the log-likelihood loss when predicting yt.
88	20	For each dataset, we randomly select 6 million conversations for training, 10k for validation and 5k for testing.
89	19	For every conversation, we remove the sentences whose length is shorter than 6 words and only keep the first 40 words for sentences longer than 40.
100	15	We compare our proposed model with the following three baselines: • Seq2Seq: Sequence-to-Sequence model with attention (Sordoni et al., 2015).
104	16	• CVAE+BOW loss: CVAE model with the auxiliary bag-of-words loss (Zhao et al., 2017).
105	55	We employ three types of commonly used automatic evaluation metrics and human evaluation in our experiments: Embedding Similarity: Embedding-based metrics compute the cosine similarity between the sentence embedding of a ground-truth response and that of the generated one.
137	45	Since adding the auxiliary loss could alleviate the model collapse problem, we found that CVAE model with the BOW auxiliary loss outperforms our basic model without auxiliary loss, especially on the diversity metrics.
147	29	Case Study To empirically analyze the quality of the generated responses, we show some example responses generated by our model and two baselines (Seq2Seq and CVAE+BOW) in Table 5.
150	13	However, we found that CVAE+BOW tends to copy the given queries (the first and fourth example in Table 4) and repeatedly generate redundant tokens (the second example).
151	24	The generated responses of our model are more fluent and relevant to queries.
152	12	Also, our model generates longer responses compared to the baselines.
153	12	KL Divergence Visualization In order to demonstrate that our model is able to alleviate the model collapse problem of VAE, we visualize the KL divergence between the approximate posterior distribution qθ(z|y,x) and priori pφ(z|x) during the training process of our models and CVAE with BOW loss in Figure 3.
154	27	As we know, when variational models ignore the latent variable, the generated value y will be independent of the latent variable z which causes the KL divergence in Equation (3.1) to approach 0.
160	16	In this paper, a novel variational autoregressive decoder is proposed to improve the performance of VAE-based models for open-domain response generation.
162	21	Quantitative and qualitative experimental results show clear performance improvement of the proposed model over competitive baselines.
163	88	In future works, we will explore the use of other attributes of responses such as Part-of-Speech (POS) tags and chunking sequences as additional conditions for better response generation.
