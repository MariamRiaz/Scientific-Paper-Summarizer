5	39	This leads us to the question: “How do we define a model that captures all the essential (potentially still unknown) properties of real graphs?” An increasingly popular way to address this issue in other fields is by switching from explicit (prescribed) models to implicit ones.
8	11	However, despite their massive success when dealing with real-valued data, adapting GANs to handle discrete objects like graphs or text remains an open research problem (Goodfellow, 2016).
11	34	This means that in a typical setting one has to learn from a single graph.
12	31	Additionally, any model operating on a graph necessarily has to be permutation invariant, as graphs are isomorphic under node reordering.
13	11	In this work we introduce NetGAN – the first implicit generative model for graphs and networks that tackles all of the above challenges.
16	42	The main requirement for a graph generative model is the ability to generate realistic graphs.
17	17	In the experimental section we compare NetGAN to other established prescribed models on this task.
18	89	We observe that our proposed method consistently reproduces most known patterns inherent to real-world networks without explicitly specifying any of them in the model definition (e.g., degree distribution, as seen in Fig.
21	26	As our experiments show, our model exhibits competitive performance in this task and even achieves state-of-the-art results on some datasets.
54	15	We use the biased secondorder random walk sampling strategy described in Grover & Leskovec (2016), as it better captures both local and global graph structure.
58	20	At the same time, the discriminator learns to distinguish the synthetic random walks from the real ones that come from the training set.
62	16	An overview of our model’s complete architecture can be seen in Fig.
63	51	The generator G defines an implicit probabilistic model for generating random walks: (v1, ...,vT ) ∼ G. We model G as a sequential process based on a neural network fθ parametrized by θ.
64	38	At each step t, fθ produces two values: the probability distribution over the next node to be sampled, parametrized by the logits pt, and the current memory state of the model, denoted as mt.
70	16	The latent code z goes through two separate streams, each consisting of two fully connected layers with tanh activation, and then used to initialize (C0,h0).
71	15	A natural question might arise: ”Why use a model with memory and temporal dependencies, when the random walks are Markov processes?” (2nd order Markov for biased RWs).
98	18	Weights are regularized with an L2 penalty.
102	13	The first strategy, named VAL-CRITERION is concerned with the generalization properties of NetGAN.
135	27	On the other hand, NetGAN is able to capture all the graph properties well, although none of them are explicitly specified in its model definition.
159	12	This is especially true for the larger networks (CORA, DBLP, PUBMED), since given their size we need more random walks to cover the entire graph.
172	11	Then, instead of sampling z from the entire latent space Ω, we now sample from subregions of Ω and visualize the results.
178	14	In all four heatmaps, we see distinct patterns, e.g. higher average degree of starting nodes for the bottom right region of Fig.
186	28	Marked by (*) and (Ω) we see the community distributions for the input graph and the graph obtained by sampling on the complete latent space respectively.
187	76	5b and 5c, we see the evolution of selected community shares when following a trajectory from top to bottom, and left to right, respectively.
189	24	When evaluating different graph generative models in Sec.
190	11	3.2, we observed a major limitation of explicit models.
191	18	While the prescribed approaches excel at recovering the properties directly included in their definition, they perform significantly worse with respect to the rest.
192	31	This clearly indicates the need for implicit graph generators such as NetGAN.
193	62	Indeed, we notice that our model is able to consistently capture all the important graph characteristics (see Table 1).
194	11	Moreover, NetGAN generalizes beyond the input graph, as can be seen by its strong link prediction performance in Sec.
