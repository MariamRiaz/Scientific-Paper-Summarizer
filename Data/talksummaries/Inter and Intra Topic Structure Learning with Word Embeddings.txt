0	20	Significant research effort has been devoted to developing advanced text analysis technologies.
1	14	Probabilistic topic models such as Latent Dirichlet Allocation (LDA), are popular approaches for this task, which discover latent topics from text collections.
2	34	One preferred property of probabilistic topic models is interpretability: one can explain that a document is composed of topics and a topic is described by words.
3	20	Although widely used, most variations of standard vanilla topic models (e.g., LDA) assume topics are independent and there are no structures among them.
4	15	This limits those models’ ability to explore any hierarchical thematic structures.
9	12	In addition to topic hierarchies, we are also interested in analyzing the fine-grained thematic structure within each individual topic.
11	31	So we refer those topics as local topics.
12	30	Due to the limitation of the context of a target corpus, some local topics may be hard to interpret because of the following two effects: (1) They can mix the words which co-occur locally in the target corpus but are less semantically related in general; (2) Local topics can be dominated by specialized words, which are less interpretable without extra knowledge.
13	22	For example, we show four example topics of our experiments in Table 1, where we can see: Topic 1 is composed of the words from both the “scientific publication” and “biology” aspects; Topic 2 is a mixture of “sports” and “music”; Topics 3 and 4 are very specific topics about “singer” and “video game” respectively.
14	8	We humans are able to understand those local topics in the above way because we are equipped with the global semantics of the words, making us go beyond the local context of the target corpus.
15	8	Therefore, we are motivated to propose a model which is able to automatically analyze the fine-grained thematic structures of local topics, further improving the interpretability of topic modeling.
17	10	Learned from large corpora, word embeddings encode the semantics of words with their locations in a space, where more related words are closer to each other.
22	34	In this paper, we propose a novel deep structured topic model, named the Word Embeddings Deep Topic Model, WEDTM1, which improves the interpretability of topic models by discovering topic hierarchies (i.e., inter topic structure) and fine-grained interpretations of local topics (i.e., intra topic structure).
23	30	Specifically, the proposed model adapts a multi-layer Gamma Belief Network which generates deep representations of topics as well as documents.
49	23	Based on the PFA framework, WEDTM is a hierarchical model with two major components: one for discovering the inter topic hierarchies and the other for discovering intra topic structures (i.e., sub-topics) informed by word embeddings.
54	10	To assist clarity, we split the generative process of the model into three parts, shown as follows: Generating documents  θ (1) j ∼ Gam [ Φ(2)θ (2) j , p (2) j /(1− p (2) j ) ] , φ (1) k1 ∼ Dir (βk1) , x (1) j ∼ Pois ( Φ(1)θ (1) j ) , Inter structure  θ (T ) j ∼ Gam ( r, 1/c (T+1) j ) , · · · θ (t) j ∼ Gam ( Φ(t+1)θ (t+1) j , 1/c (t+1) j ) (t < T ), φ (t) kt ∼ Dir (η01) (t > 1), · · · Intra structure  w<s>k1 ∼ N [0, diag(1/σ <s>)] , α<s>k1 ∼ Gam(α <s> 0 /S, 1/c <s> 0 ), β<s>vk1 ∼ Gam ( α<s>k1 , e f>v w <s> k1 ) , βvk1 := ∑S s β <s> vk1 , where (t) is the index of the layer that a variable belongs to and <s> is the index of sub-topic s. To complete the model, we impose the following priors on the latent variables: rkT ∼ Gam(γ0/KT , 1/c0), γ0 ∼ Gam(a0, 1/b0), p(t)j ∼ Beta(a0, b0), c (t) j ∼ Gam(e0, 1/f0), α <s> 0 ∼ Gam(e0, 1/f0), c<s>0 ∼ Gam(e0, 1/f0), σ<s>l ∼ Gam(a0, 1/b0).
58	11	The k1-th column of Φ(1), φ(1)k1 ∈ R V + is the word distribution of topic k1, drawn from a Dirichlet (Dir) distribution.
65	8	First of all, WEDTM applies individual asymmetric Dirichlet parameters βk1 ∈ RV+ for each bottom-layer (local) topic φ (1) k1 .
67	24	For each sub-topic s, we introduce an Ldimensional sub-topic embedding: w<s>k1 ∈ R L. As β<s>vk1 is gamma distributed, its scale parameter is constructed by the dot product of the embeddings of sub-topic s and word v through the exponential function.
70	8	Therefore, if a word dominates in one or more sub-topics, it is likely that the word will still dominate in the local topic.
71	33	With this construction, a sub-topic is expected to capture one fine-grained thematic aspect of the local topic and each sub-topic can be directly interpreted with words via β<s>k1 ∈ R V + .
76	22	The large expectation means that v has a large weight in sub-topic s of k. Finally, β<s>vk1 further contributes to the local topic’s prior βvk1 , informing φ (1) vk1 of the local topic.
78	6	Consequently, in WEDTM, there are three latent variables capturing the weights between the words and local topic k1: eF >wk1 (F ∈ RL×V is the embeddings of all the words), βk1 , and φ (1) k1 , each of which is a vector over words.
80	10	eF >wk1 is the prior of βk1 , while βk1 is the prior of φ (1) k1 .
87	21	Here we focus on the sampling of the latent variables for modeling intra topic structure.
88	7	(28) in Appendix B of Zhou et al. (2016), the latent count for the bottom-layer local topics are x(1)vjk1 , which counts how many words v in document j are allocated with local topic k1.
89	13	We first sample: ( h<1>vk1 , · · · , h <S> vk1 ) ∼ Mult ( hvk1 , β<1>vk1 βvk1 , · · · , β<S>vk1 βvk1 ) , (1) where hvk1 ∼ CRT ( x (1) v·k1 , βvk1 ) (Zhou & Carin, 2015; Zhao et al., 2017a), and x(1)v·k1 := ∑ j x (1) vjk1 3.
94	25	To sample from PG, we use an accurate and efficient approximate sampler in Zhou (2016).
98	15	We report perplexity, document classification accuracy, and topic coherence scores.
106	37	(a)-(d): Relative per-heldout-word perplexity6 (the lower the better) with the varied K1 and fixed proportion (80%) of training words of each document.
