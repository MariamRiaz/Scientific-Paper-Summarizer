12	71	We demonstrate the effectiveness of our model on two problems that can be naturally framed as sentence extraction with external information.
13	28	These two problems, extractive document summarization and answer selection for machine reading comprehension, both require local and global contextual reasoning about a given document.
14	71	Extractive document summarization systems aim at creating a summary by identifying (and subsequently concatenating) the most important sentences in a document, whereas answer selection systems select the candidate sentence in a document most likely to contain the answer to a query.
15	90	For document summarization, we exploit the title and image captions which often appear with documents (specifically newswire articles) as external information.
16	32	For answer selection, we use word overlap features, such as the inverse sentence frequency (ISF, Trischler et al., 2016) and the inverse document frequency (IDF) together with the query, all formulated as external cues.
17	216	Our main contributions are three-fold: First, our model ensures that sentence extraction is done in a larger (rich) context, i.e., the full document is read first before we start labeling its sentences for extraction, and each sentence labeling is done by implicitly estimating its local and global relevance to the document and by directly attending to some external information for importance cues.
18	66	Second, while external information has been shown to be useful for summarization systems using traditional hand-crafted features (Edmundson, 1969; Kupiec et al., 1995; Mani, 2001), our model is the first to exploit such information in deep learning-based summarization.
23	18	Lastly, with the machine reading capabilities of our model, we confirm that a full document needs to be “read” to produce high quality extracts allowing a rich contextual reasoning, in contrast to previous answer selection approaches that often measure a score between each sentence in the document and the question and return the sentence with highest score in an isolated manner (Yin et al., 2016; dos Santos et al., 2016; Wang et al., 2016).
25	99	Our ensemble model combining scores from our model and word overlap scores using a logistic regression layer achieves state-ofthe-art results on the popular question answering datasets WikiQA (Yang et al., 2015) and NewsQA (Trischler et al., 2016), and it obtains comparable results to the state of the art for SQuAD (Rajpurkar et al., 2016).
26	42	We also evaluate our approach on the MSMarco dataset (Nguyen et al., 2016) and elaborate on the behavior of our machine reader in a scenario where each candidate answer sentence is contextually independent of each other.
29	14	The main components include a sentence encoder, a document encoder, and a novel sentence extractor (see Figure 1) that we describe in more detail below.
32	40	We use temporal narrow convolution by applying a kernel filter K of width h to a window of h words in sentence s to produce a new feature.
37	13	The final sentence embeddings have six dimensions.
41	22	, sn), we follow common practice and feed the sentences in reverse order (Sutskever et al., 2014; Li et al., 2015; Filippova et al., 2015).
42	25	Sentence Extractor Our sentence extractor sequentially labels each sentence in a document with 1 or 0 by implicitly estimating its relevance in the document and by directly attending to the external information for importance cues.
44	26	Our attention mechanism differs from the standard practice of attending intermediate states of the input (encoder).
51	13	We validate our model on two sentence extraction problems: extractive document summarization and answer selection for machine reading comprehension.
60	15	In this setting, our sentence extractor sequentially predicts label yi ∈ {0, 1} (where 1 means that si contains the answer) and assign score p(yi|si,D ,E , θ) quantifying si’s relevance to the query.
66	26	The ISF score αsi for the sentence si is computed as αsi =∑ w∈si∩q IDF(w), where IDF is the inverse document frequency score of word w, defined as: IDF(w) = log NNw , whereN is the total number of sentences in the training set and Nw is the number of sentences in which w appears.
73	18	This section presents our experimental setup and results assessing our model in both the extractive summarization and answer selection setups.
74	60	In the rest of the paper, we refer to our model as XNET for its ability to exploit eXternal information to improve document representation.
78	52	However, we generated deanonymized summaries and evaluated them against gold summaries to facilitate human evaluation and to make human evaluation comparable to automatic evaluation.
79	17	To train our model, we need documents annotated with sentence extraction information, i.e., each sentence in a document is labeled with 1 (summary-worthy) or 0 (not summary-worthy).
80	29	We followed Nallapati et al. (2017) and automatically extracted ground truth labels such that all positively labeled sentences from an article collectively give the highest ROUGE (Lin and Hovy, 2003) score with respect to the gold summary.
81	15	We used a modified script of Hermann et al. (2015) to extract titles and image captions, and we associated them with the corresponding articles.
83	13	The availability of image captions varies from 0 to 414 per article, with an average of 3 image captions.
85	27	All sentences, including titles and image captions, were padded with zeros to a sentence length of 100.
90	50	Comparison Systems We compared the output of our model against the standard baseline of simply selecting the first three sentences from each document as the summary.
101	25	We experimented with two types of external information: title (TITLE) and image captions (CAPTION).
102	20	In addition, we experimented with the first sentence (FS) of the document as external information.
