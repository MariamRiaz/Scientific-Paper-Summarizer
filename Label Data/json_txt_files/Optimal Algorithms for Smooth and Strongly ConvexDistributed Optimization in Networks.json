{"sections": [{"text": "\u221a \u03bal(1 +\n\u03c4\u221a \u03b3 ) ln(1/\u03b5)), where \u03bal is the condition number of the local functions and \u03b3 is the (normalized) eigengap of the gossip matrix used for communication between nodes. We then verify the efficiency of MSDA against state-of-the-art methods for two problems: least-squares regression and classification by logistic regression."}, {"heading": "1 Introduction", "text": "Given the numerous applications of distributed optimization in machine learning, many algorithms have recently emerged, that allow the minimization of objective functions f defined as the average 1 n \u2211n i=1 fi of functions fi which are respectively accessible by separate nodes in a network [1, 2, 3, 4]. These algorithms typically alternate local incremental improvement steps (such as gradient steps) with communication steps between nodes in the network, and come with a variety of convergence rates (see for example [5, 4, 6, 7]).\nTwo main regimes have been looked at: (a) centralized where communications are precisely scheduled and (b) decentralized where communications may not exhibit a precise schedule. In this paper, we consider these two regimes for objective functions which are smooth and strongly-convex and for which algorithms are linearly (exponentially) convergent. The main contribution of this paper\nar X\niv :1\n70 2.\n08 70\n4v 2\n[ m\nat h.\nis to propose new and matching upper and lower bounds of complexity for this class of distributed problems.\nThe optimal complexity bounds depend on natural quantities in optimization and network theory. Indeed, (a) for a single machine the optimal number of gradient steps to optimize a function is proportional to the square root of the condition number [8], and (b) for mean estimation, the optimal number of communication steps is proportional to the diameter of the network in centralized problems or to the square root of the eigengap of the Laplacian matrix in decentralized problems [9]. As shown in Section 3, our lower complexity bounds happen to be combinations of the two contributions above.\nThese lower complexity bounds are attained by two separate algorithms. In the centralized case, the trivial distribution of Nesterov\u2019s accelerated gradient attains this rate, while in the decentralized case, as shown in Section 4, the rate is achieved by a dual algorithm. We compare favorably our new optimal algorithms to existing work in Section 5.\nRelated work. Decentralized optimization has been extensively studied and early methods such as decentralized gradient descent [1, 10] or decentralized dual averaging [3] exhibited sublinear convergence rates. More recently, a number of methods with provable linear convergence rates were developed, including EXTRA [4, 11], augmented Lagrangians [6], and more recent approaches [7]. The most popular of such approaches is the distributed alternating direction method of multipliers (D-ADMM) [2, 12, 5] and has led to a large number of variations and extensions. In a different direction, second order methods were also investigated [13, 14]. However, to the best of our knowledge, the field still lacks a coherent theoretical understanding of the optimal convergence rates and its dependency on the characteristics of the communication network. In several related fields, complexity lower bounds were recently investigated, including the sequential optimization of a sum of functions [15, 16], distributed optimization in flat (i.e. totally connected) networks [17, 18], or distributed stochastic optimization [19]."}, {"heading": "2 Distributed optimization setting", "text": ""}, {"heading": "2.1 Optimization problem", "text": "Let G = (V, E) be a connected simple (i.e. undirected) graph of n computing units and diameter \u2206, each having access to a function fi(\u03b8) over \u03b8 \u2208 Rd. We consider minimizing the average of the local functions\nmin \u03b8\u2208Rd\nf\u0304(\u03b8) = 1\nn n\u2211 i=1 fi(\u03b8) (1)\nin a distributed setting. More specifically, we assume that:\n1. Each computing unit can compute first-order characteristics, such as the gradient of its own function or its Fenchel conjugate. By renormalization of the time axis, and without loss of generality, we assume that this computation is performed in one unit of time.\n2. Each computing unit can communicate values (i.e. vectors in Rd) to its neighbors. This communication requires a time \u03c4 (which may be smaller or greater than 1).\nThese actions may be performed asynchronously and in parallel, and each node i possesses a local version of the parameter, which we refer to as \u03b8i. Moreover, we assume that each function fi is \u03b1-strongly convex and \u03b2-smooth, and we denote by \u03bal = \u03b2\u03b1 \u2265 1 the local condition number. We also denote by \u03b1g , \u03b2g and \u03bag , respectively, the strong convexity, smoothness and condition number of the average (global) function f\u0304 . Note that we always have \u03bag \u2264 \u03bal, while the opposite inequality is, in general, not true (take for example f1(\u03b8) = 1{\u03b8 < 0}\u03b82 and f2(\u03b8) = 1{\u03b8 > 0}\u03b82 for which \u03bal = +\u221e and \u03bag = 1). However, the two quantities are close (resp. equal) when the local functions are similar (resp. equal) to one another."}, {"heading": "2.2 Decentralized communication", "text": "A large body of literature considers a decentralized approach to distributed optimization based on the gossip algorithm [9, 1, 3, 12]. In such a case, communication is represented as a matrix multiplication with a matrix W verifying the following constraints:\n1. W is an n\u00d7 n symmetric matrix,\n2. W is positive semi-definite,\n3. The kernel of W is the set of constant vectors: Ker(W ) = Span(1), where 1 = (1, ..., 1)>,\n4. W is defined on the edges of the network: Wij 6= 0 only if i = j or (i, j) \u2208 E .\nThe third condition will ensure that the gossip step converges to the average of all the vectors shared between the nodes. We will denote the matrix W as the gossip matrix, since each communication step will be represented using it. Note that a simple choice for the gossip matrix is the Laplacian matrix L = D \u2212 A, where A is the adjacency matrix of the network and D = diag (\u2211 iAij ) . However, in the presence of large degree nodes, weighted Laplacian matrices are usually a better choice, and the problem of optimizing these weights is known as the fastest distributed consensus averaging problem and is investigated by [20, 21].\nWe will denote by \u03bb1(W ) \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bbn(W ) = 0 the spectrum of the gossip matrix W , and its (normalized) eigengap the ratio \u03b3(W ) = \u03bbn\u22121(W )/\u03bb1(W ) between the second smallest and the largest eigenvalue. Equivalently, this is the inverse of the condition number of W projected on the space orthogonal to the constant vector 1. This quantity will be the main parameter describing the connectivity of the communication network in Section 3.3 and Section 4."}, {"heading": "3 Optimal convergence rates", "text": "In this section, we prove oracle complexity lower bounds for distributed optimization in two settings: strongly convex and smooth functions for centralized (i.e. master/slave) and decentralized algorithms based on a gossip matrix W .\nIn the first setting, we show that distributing accelerated gradient descent matches the optimal convergence rate, while, in the second setting, the algorithm proposed in Section 4 is shown to be optimal. Note that we will use the notation g(\u03b5) = \u2126(f(\u03b5)) for \u2203C > 0 s.t. \u2200\u03b5 > 0, g(\u03b5) \u2265 Cf(\u03b5), and will, for simplicity, omit the additive terms that do not depend on the precision \u03b5 in Corollary 1 and Corollary 2."}, {"heading": "3.1 Black-box optimization procedures", "text": "The lower bounds provided hereafter depend on a new notion of black-box optimization procedures for the problem in Eq. (1), where we consider distributed algorithms verifying the following constraints:\n1. Local memory: each node i can store past values in a (finite) internal memoryMi,t \u2282 Rd at time t \u2265 0. These values can be accessed and used at time t by the algorithm run by node i, and are updated either by local computation or by communication (defined below), that is, for all i \u2208 {1, ..., n},\nMi,t \u2282Mcompi,t \u222aM comm i,t . (2)\n2. Local computation: each node i can, at time t, compute the gradient of its local function \u2207fi(\u03b8) or its Fenchel conjugate \u2207f\u2217i (\u03b8) for a value \u03b8 \u2208 Mi,t in the node\u2019s internal memory, that is, for all i \u2208 {1, ..., n},\nMcompi,t = Span ({\u03b8,\u2207fi(\u03b8),\u2207f \u2217 i (\u03b8) : \u03b8 \u2208Mi,t\u22121}) . (3)\n3. Local communication: each node i can, at time t, share a value to all or part of its neighbors, that is, for all i \u2208 {1, ..., n},\nMcommi,t = Span ( \u22c3\n(i,j)\u2208E\nMj,t\u2212\u03c4 ) . (4)\n4. Output value: each node i must, at time t, specify one vector in its memory as local output of the algorithm, that is, for all i \u2208 {1, ..., n},\n\u03b8i,t \u2208Mi,t. (5)\nHence, a black-box procedure will return n output values\u2014one for each node of the network\u2014 and our analysis will focus on ensuring that all local output values are converging to the optimal parameter of Eq. (1). Moreover, we will say that a black-box procedure uses a gossip matrix W if the local communication is achieved by multiplication of a vector withW . For simplicity, we assume that all nodes start with the simple internal memoryMi,0 = {0}. Note that communications and local computations may be performed in parallel and asynchronously."}, {"heading": "3.2 Centralized algorithms", "text": "In this section, we show that, for any black-box optimization procedure, at least \u2126(\u221a\u03bag ln(1/\u03b5)) gradient steps and \u2126(\u2206\u221a\u03bag ln(1/\u03b5)) communication steps are necessary to achieve a precision \u03b5 > 0, where \u03bag is the global condition number and \u2206 is the diameter of the network. These lower bounds extend the communication complexity lower bounds for totally connected communication networks of [18], and are natural since at least \u2126(\u221a\u03bag ln(1/\u03b5)) steps are necessary to solve a strongly convex and smooth problem up to a fixed precision, and at least \u2206 communication steps are required to transmit a message between any given pair of nodes.\nIn order to simplify the proofs of the following theorems, and following the approach of [22], we will consider the limiting situation d\u2192 +\u221e. More specifically, we now assume that we are working in `2 = {\u03b8 = (\u03b8k)k\u2208N : \u2211 k \u03b8 2 k < +\u221e} rather than Rd.\nTheorem 1. Let G be a graph of diameter \u2206 > 0 and size n > 0, and \u03b2g \u2265 \u03b1g > 0. There exists n functions fi : `2 \u2192 R such that f\u0304 is \u03b1g strongly convex and \u03b2g smooth, and for any t \u2265 0 and any black-box procedure one has, for all i \u2208 {1, ..., n},\nf\u0304(\u03b8i,t)\u2212 f\u0304(\u03b8\u2217) \u2265 \u03b1g 2\n( 1\u2212 4\u221a\n\u03bag\n)1+ t1+\u2206\u03c4 \u2016\u03b8i,0 \u2212 \u03b8\u2217\u20162, (6)\nwhere \u03bag = \u03b2g/\u03b1g .\nThe proof of Theorem 1 relies on splitting the function used by Nesterov to prove oracle complexities for strongly convex and smooth optimization [8, 22] on two nodes at distance \u2206. One can show that most dimensions of the parameters \u03b8i,t will remain zero, and local gradient computations may only increase the number of non-zero dimensions by one. Finally, at least \u2206 communication rounds are necessary in-between every gradient computation, in order to share information between the two nodes. The detailed proof is available as supplementary material. Corollary 1. For any graph of diameter \u2206 and any black-box procedure, there exists functions fi such that the time to reach a precision \u03b5 > 0 is lower bounded by\n\u2126 ( \u221a \u03bag ( 1 + \u2206\u03c4 ) ln ( 1\n\u03b5\n)) , (7)\nThis optimal convergence rate is achieved by distributing Nesterov\u2019s accelerated gradient descent on the global function. Computing the gradient of f\u0304 is performed by sending all the local gradients \u2207fi to a single node (denoted as master node) in \u2206 communication steps (which may involve several simultaneous messages), and then returning the new parameter \u03b8t+1 to every node in the network (which requires another \u2206 communication steps). In practice, summing the gradients can be distributed by computing a spanning tree (with the root as master node), and asking for each node to perform the sum of its children\u2019s gradients before sending it to its parent. Standard methods as described by [23] can be used for performing this parallelization of gradient computations.\nThis algorithm has three limitations: first, the algorithm is not robust to machine failures, and the central role played by the master node also means that a failure of this particular machine may completely freeze the procedure. Second, and more generally, the algorithm requires precomputing a spanning tree, and is thus not suited to time-varying graphs, in which the connectivity between the nodes may change through time (e.g. in peer-to-peer networks). Finally, the algorithm requires every node to complete its gradient computation before aggregating them on the master node, and the efficiency of the algorithm thus depends on the slowest of all machines. Hence, in the presence of non-uniform latency of the local computations, or the slow down of a specific machine due to a hardware failure, the algorithm will suffer a significant drop in performance."}, {"heading": "3.3 Decentralized algorithms", "text": "The gossip algorithm [9] is a standard method for averaging values across a network when its connectivity may vary through time. This approach was shown to be robust against machine failures, non-uniform latencies and asynchronous or time-varying graphs, and a large body of literature extended this algorithm to distributed optimization [1, 3, 12, 4, 6, 7, 13].\nThe convergence analysis of decentralized algorithms usually relies on the spectrum of the gossip matrix W used for communicating values in the network, and more specifically on the ratio between the second smallest and the largest eigenvalue of W , denoted \u03b3. In this section, we show that, with respect to this quantity and \u03bal, reaching a precision \u03b5 requires at least \u2126( \u221a \u03bal ln(1/\u03b5)) gradient steps\nand \u2126 (\u221a\n\u03bal \u03b3 ln(1/\u03b5)\n) communication steps, by exhibiting a gossip matrix such that a corresponding\nlower bound exists. Theorem 2. Let \u03b1, \u03b2 > 0 and \u03b3 \u2208 (0, 1]. There exists a gossip matrix W of eigengap \u03b3(W ) = \u03b3, and \u03b1-strongly convex and \u03b2-smooth functions fi : `2 \u2192 R such that, for any t \u2265 0 and any black-box procedure using W one has, for all i \u2208 {1, ..., n},\nf\u0304(\u03b8i,t)\u2212 f\u0304(\u03b8\u2217) \u2265 3\u03b1\n2\n( 1\u2212 16\u221a\n\u03bal\n)1+ t 1+ \u03c4\n5 \u221a \u03b3 \u2016\u03b8i,0 \u2212 \u03b8\u2217\u20162, (8)\nwhere \u03bal = \u03b2/\u03b1 is the local condition number.\nThe proof of Theorem 2 relies on the same technique as that of Theorem 1, except that we now split the two functions on a subset of a linear graph. These networks have the appreciable property that \u2206 \u2248 1/\u221a\u03b3, and we can thus use a slightly extended version of Theorem 1 to derive the desired result. The complete proof is available as supplementary material. Corollary 2. For any \u03b3 > 0, there exists a gossip matrix W of eigengap \u03b3 and \u03b1-strongly convex, \u03b2-smooth functions such that, with \u03bal = \u03b2/\u03b1, for any black-box procedure using W the time to reach a precision \u03b5 > 0 is lower bounded by\n\u2126 ( \u221a \u03bal ( 1 +\n\u03c4 \u221a \u03b3\n) ln ( 1\n\u03b5\n)) . (9)\nWe will see in the next section that this lower bound is met for a novel decentralized algorithm called multi-step dual accelerated (MSDA) and based on the dual formulation of the optimization problem. Note that these results provide optimal convergence rates with respect to \u03bal and \u03b3, but do not imply that \u03b3 is the right quantity to consider on general graphs. The quantity 1/ \u221a \u03b3 may indeed be very large compared to \u2206, for example for star networks, for which \u2206 = 2 and 1/ \u221a \u03b3 = \u221a n. However, on many simple networks, the diameter \u2206 and the eigengap of the Laplacian matrix are tightly connected, and \u2206 \u2248 1/\u221a\u03b3. For example, for linear graphs, \u2206 = n\u2212 1 and 1/\u221a\u03b3 \u2248 2n/\u03c0, for totally connected networks, \u2206 = 1 and 1/ \u221a \u03b3 = 1, and for regular networks, 1/ \u221a \u03b3 \u2265 \u2206\n2 \u221a\n2 ln2 n\n[24]. Finally, note that the case of totally connected networks corresponds to a previous complexity lower bound on communications proven by [18], and is equivalent to our result for centralized algorithms with \u2206 = 1."}, {"heading": "4 Optimal decentralized algorithms", "text": "In this section, we present a simple framework for solving the optimization problem in Eq. (1) in a decentralized setting, from which we will derive several variants, including a synchronized algorithm whose convergence rate matches the lower bound in Corollary 2 . Note that the naive approach of distributing each (accelerated) gradient step by gossiping does not lead to a linear convergence rate, as the number of gossip steps has to increase with the number of iterations to ensure the linear rate is preserved. We begin with the simplest form of the algorithm, before extending it to more advanced scenarios.\nAlgorithm 1 Single-Step Dual Accelerated method Input: number of iterations T > 0, gossip matrix W \u2208 Rn\u00d7n, \u03b7 = \u03b1\u03bb1(W ) , \u00b5 = \u221a \u03bal\u2212 \u221a \u03b3\u221a \u03bal+ \u221a \u03b3 Output: \u03b8i,T , for i = 1, ..., n 1: x0 = 0, y0 = 0 2: for t = 0 to T \u2212 1 do 3: \u03b8i,t = \u2207f\u2217i (xi,t), for all i = 1, ..., n 4: yt+1 = xt \u2212 \u03b7\u0398tW 5: xt+1 = (1 + \u00b5)yt+1 \u2212 \u00b5yt 6: end for"}, {"heading": "4.1 Single-Step Dual Accelerated method", "text": "A standard approach for solving Eq. (1) (see [2, 6]) consists in rewriting the optimization problem as\nmin \u03b8\u2208Rd f\u0304(\u03b8) = min \u03b81=\u00b7\u00b7\u00b7=\u03b8n\n1\nn n\u2211 i=1 fi(\u03b8i). (10)\nFurthermore, the equality constraint \u03b81 = \u00b7 \u00b7 \u00b7 = \u03b8n is equivalent to \u0398 \u221a W = 0, where \u0398 = (\u03b81, . . . , \u03b8n) and W is a gossip matrix verifying the assumptions described in Section 2. Note that, since W is positive semi-definite, \u221a W exists and is defined as \u221a W = V >\u03a31/2V , where W = V >\u03a3V is the singular value decomposition of W . The equality \u0398 \u221a W = 0 implies that each row of \u0398 is constant (since Ker( \u221a W ) = Span(1)), and is thus equivalent to \u03b81 = \u00b7 \u00b7 \u00b7 = \u03b8n. This leads to the following primal version of the optimization problem:\nmin \u0398\u2208Rd\u00d7n : \u0398 \u221a W=0 F (\u0398), (11)\nwhere F (\u0398) = \u2211n i=1 fi(\u03b8i). Since Eq. (11) is a convex problem, it is equivalent to its dual optimization problem: max\n\u03bb\u2208Rd\u00d7n \u2212F \u2217(\u03bb\n\u221a W ), (12)\nwhere F \u2217(y) = supx\u2208Rd\u00d7n\u3008y, x\u3009 \u2212 F (x) is the Fenchel conjugate of F , and \u3008y, x\u3009 = tr(y>x) is the standard scalar product between matrices.\nThe optimization problem in Eq. (12) is unconstrained and convex, and can thus be solved using a variety of convex optimization techniques. The proposed single-step dual accelerated (SSDA) algorithm described in Alg. (1) uses Nesterov\u2019s accelerated gradient descent, and can be thought of as an accelerated version of the distributed augmented Lagrangian method of [6] for \u03c1 = 0. The algorithm is derived by noting that a gradient step of size \u03b7 > 0 for Eq. (12) is\n\u03bbt+1 = \u03bbt \u2212 \u03b7\u2207F \u2217(\u03bbt \u221a W ) \u221a W, (13)\nand the change of variable yt = \u03bbt \u221a W leads to\nyt+1 = yt \u2212 \u03b7\u2207F \u2217(yt)W. (14)\nThis equation can be interpreted as gossiping the gradients of the local conjugate functions\u2207f\u2217i (yi,t), since \u2207F \u2217(yt)ij = \u2207f\u2217j (yj,t)i.\nTheorem 3. The iterative scheme in Alg. (1) converges to \u0398 = \u03b8\u22171> where \u03b8\u2217 is the solution of Eq. (1). Furthermore, the time needed for this algorithm to reach any given precision \u03b5 > 0 is\nO ( (1 + \u03c4) \u221a \u03bal \u03b3 ln ( 1 \u03b5 )) . (15)\nThis theorem relies on proving that the condition number of the dual objective function is upper bounded by \u03bal\u03b3 , and noting that the convergence rate for accelerated gradient descent depends on the square root of the condition number (see, e.g., [22]). A detailed proof is available as supplementary material."}, {"heading": "4.2 Multi-Step Dual Accelerated method", "text": "The main problem of Alg. (1) is that it always performs the same number of gradient and gossip steps. When communication is cheap compared to local computations (\u03c4 1), it would be preferable to perform more gossip steps than gradient steps in order to propagate the local gradients further than the local neighborhoods of each node. This can be achieved by replacingW by PK(W ) in Alg. (1), where PK is a polynomial of degree at most K. If PK(W ) is itself a gossip matrix, then the analysis of the previous section can be applied and the convergence rate of the resulting algorithm depends on the eigengap of PK(W ). Maximizing this quantity for a fixed K leads to a common acceleration scheme known as Chebyshev acceleration [25, 26] and the choice\nPK(x) = 1\u2212 TK(c2(1\u2212 x))\nTK(c2) , (16)\nwhere c2 = 1+\u03b31\u2212\u03b3 and TK are the Chebyshev polynomials [25] defined as T0(x) = 1, T1(x) = x, and, for all k \u2265 1,\nTk+1(x) = 2xTk(x)\u2212 Tk\u22121(x). (17)\nFinally, verifying that this particular choice of PK(W ) is indeed a gossip matrix, and taking K = b 1\u221a\u03b3 c leads to Alg. (2) with an optimal convergence rate with respect to \u03b3 and \u03bal. Theorem 4. The iterative scheme in Alg. (2) converges to \u0398 = \u03b8\u22171> where \u03b8\u2217 is the solution of Eq. (1). Furthermore, the time needed for this algorithm to reach any given precision \u03b5 > 0 is\nO ( \u221a \u03bal ( 1 +\n\u03c4 \u221a \u03b3\n) ln ( 1\n\u03b5\n)) . (18)\nThe proof of Theorem 4 relies on standard properties of Chebyshev polynomials that imply that, for the particular choice of K = b 1\u221a\u03b3 c, we have\n1\u221a \u03b3(PK(W )) \u2264 2. Hence, Theorem 3 applied to the gossip matrix W \u2032 = PK(W ) gives the desired convergence rate. The complete proof is available as supplementary material."}, {"heading": "4.3 Discussion and further developments", "text": "We now discuss several extensions to the proposed algorithms.\nAlgorithm 2 Multi-Step Dual Accelerated method Input: number of iterations T > 0, gossip matrix W \u2208 Rn\u00d7n, c1 = 1\u2212\u221a\u03b3 1+ \u221a \u03b3 , c2 = 1+\u03b3 1\u2212\u03b3 , c3 =\n2 (1+\u03b3)\u03bb1(W )\n, K = \u230a\n1\u221a \u03b3\n\u230b , \u03b7 = \u03b1(1+c 2K 1 )\n(1+cK1 ) 2 , \u00b5 =\n(1+cK1 ) \u221a \u03bal\u22121+cK1 (1+cK1 ) \u221a \u03bal+1\u2212cK1\nOutput: \u03b8i,T , for i = 1, ..., n 1: x0 = 0, y0 = 0 2: for t = 0 to T \u2212 1 do 3: \u03b8i,t = \u2207f\u2217i (xi,t), for all i = 1, ..., n 4: yt+1 = xt \u2212 \u03b7 ACCELERATEDGOSSIP(\u0398t,W ,K) 5: xt+1 = (1 + \u00b5)yt+1 \u2212 \u00b5yt 6: end for\n7: procedure ACCELERATEDGOSSIP(x,W ,K) 8: a0 = 1, a1 = c2 9: x0 = x, x1 = c2x(I \u2212 c3W )\n10: for k = 1 to K \u2212 1 do 11: ak+1 = 2c2ak \u2212 ak\u22121 12: xk+1 = 2c2xk(I \u2212 c3W )\u2212 xk\u22121 13: end for 14: return x0 \u2212 xKaK 15: end procedure\n1. Computation of\u2207f\u2217i (xi,t): In practice, it may be hard to apply the dual algorithm when conjugate functions are hard to compute. We now provide three potential solutions to this problem: (1) warm starts may be used for the optimization problem\u2207f\u2217i (xi,t) = argmin\u03b8 fi(\u03b8)\u2212 x>i,t\u03b8 by starting from the previous iteration \u03b8i,t\u22121. This will drastically reduce the number of steps required for convergence. (2) SSDA and MSDA can be extended to composite functions of the form fi(\u03b8) = gi(Bi\u03b8) + c\u2016\u03b8\u201622 for Bi \u2208 Rmi\u00d7d and gi smooth, and for which we know how to compute the proximal operator. This allows applications in machine learning such as logistic regression. See supplementary material for details. (3) Beyond the composite case, one can also add a small (well-chosen) quadratic term to the dual, and by applying accelerated gradient descent on the corresponding primal, get an algorithm that uses primal gradient computations and achieves almost the same guarantee as SSDA and MSDA (off by a log(\u03bal/\u03b3) factor).\n2. Local vs. global condition number: MSDA and SSDA depend on the worst strong convexity of the local functions mini \u03b1i, which may be very small. A simple trick can be used to depend on the average strong convexity. Using the proxy functions gi(\u03b8) = fi(\u03b8) \u2212 (\u03b1i \u2212 \u03b1\u0304)\u2016\u03b8\u201622 instead of fi, where \u03b1\u0304 = 1n \u2211 i \u03b1i is the average strong convexity, will improve the local\ncondition number from \u03bal = maxi \u03b2imini \u03b1i to\n\u03ba\u2032l = maxi \u03b2i \u2212 \u03b1i\n\u03b1\u0304 \u2212 1. (19)\nSeveral algorithms, including EXTRA [4] and DIGing [7], have convergence rates that depend on the strong convexity of the global function \u03b1g . However, their convergence rates are not optimal, and it is still an open question to know if a rate close to O (\u221a\n\u03bag(1 + \u03c4\u221a \u03b3 ) ln(1/\u03b5)\n)\ncan be achieved with a decentralized algorithm.\n3. Asynchronous setting: Accelerated stochastic gradient descent such as SVRG [27] or SAGA [28] can be used on the dual problem in Eq. (12) instead of accelerated gradient descent, in order to obtain an asynchronous algorithm with a linear convergence rate. The details and exact convergence rate of such an approach are left as future work."}, {"heading": "5 Experiments", "text": "In this section, we compare our new algorithms, single-step dual accelerated (SSDA) descent and multi-step dual accelerated (MSDA) descent, to standard distributed optimization algorithms in two settings: least-squares regression and classification by logistic regression. Note that these experiments on simple generated datasets are made to assess the differences between existing state-of-theart algorithms and the ones provided in Section 4, and do not address the practical implementation details nor the efficiency of the compared algorithms on real-world distributed platforms. The effect of latency, machine failures or variable communication time is thus left for future work."}, {"heading": "5.1 Competitors and setup", "text": "We compare SSDA and MSDA to four state-of-the-art distributed algorithms that achieve linear convergence rates: distributed ADMM (D-ADMM) [5], EXTRA [4], a recent approach named DIGing [7], and the distributed version of accelerated gradient descent (DAGD) described in Section 3.2 and shown to be optimal among centralized algorithms. When available in the literature, we used the optimal parameters for each algorithm (see Theorem 2 by [5] for D-ADMM and Remark 3 by [4] for EXTRA). For the DIGing algorithm, the parameters provided by [7] are very conservative, and lead to a very slow convergence. We thus manually optimized the parameter for this algorithm. The experiments are simulated using a generated dataset consisting of 10, 000 samples randomly distributed to the nodes of a network of size 100. In order to assess the effect of the connectivity of the network, we ran each experiment on two networks: one 10\u00d710 grid and an Erdo\u0308s-Re\u0301nyi random network with parameter p = 6100 (i.e. of average degree 6). The quality metric used in this section is be the maximum approximation error among the nodes of the network\net = max i\u2208V\nf\u0304(\u03b8i,t)\u2212 f\u0304(\u03b8\u2217), (20)\nwhere \u03b8\u2217 is the optimal parameter of the optimization problem in Eq. (1)."}, {"heading": "5.2 Least-squares regression", "text": "The regularized least-squares regression problem consists in solving the optimization problem\nmin \u03b8\u2208Rd\n1 m \u2016y \u2212X>\u03b8\u201622 + c\u2016\u03b8\u201622, (21)\nwhere X \u2208 Rd\u00d7m is a matrix containing the m data points, and y \u2208 Rm is a vector containing the m associated values. The task is thus to minimize the empirical quadratic error between a function yi = g(Xi) of d variables and its linear regression g\u0302(Xi) = X>i \u03b8 on the original dataset (for\ni = 1, ...,m), while smoothing the resulting approximation by adding a regularizer c\u2016\u03b8\u201622. For our experiments, we fixed c = 0.1, d = 10, and sampled m = 10, 000 Gaussian random variables Xi \u223c N (0, 1) of mean 0 and variance 1. The function to regress is then yi = X>i 1+cos(X>i 1)+\u03bei where \u03bei \u223c N (0, 1/4) is an i.i.d. Gaussian noise of variance 1/4. These data points are then distributed randomly and evenly to the n = 100 nodes of the network. Note that the choice of function to regress y does not impact the Hessian of the objective function, and thus the convergence rate of the optimization algorithms.\nFigure 1 and Figure 2 show the performance of the compared algorithms on two networks: a 10\u00d710 grid graph and an Erdo\u0308s-Re\u0301nyi random graph of average degree 6. All algorithms are linearly convergent, although their convergence rates scale on several orders of magnitude. In all experiments, the centralized optimal algorithm DAGD has the best convergence rate, while MSDA has the best convergence rate among decentralized methods. When the communication time is smaller than the computation time (\u03c4 1), performing several communication rounds per gradient iteration will improve the efficiency of the algorithm and MSDA substantially outperforms SSDA."}, {"heading": "5.3 Logistic classification", "text": "The logistic classification problem consists in solving the optimization problem\nmin \u03b8\u2208Rd\n1\nm m\u2211 i=1 ln ( 1 + e\u2212yi\u00b7X > i \u03b8 ) + c\u2016\u03b8\u201622, (22)\nwhere X \u2208 Rd\u00d7m is a matrix containing m data points, and y \u2208 {\u22121, 1}m is a vector containing the m class assignments. The task is thus to classify a dataset by learning a linear classifier mapping data points Xi to their associated class yi \u2208 {\u22121, 1}. For our experiments, we fixed c = 0.1, d = 10, and sampled m = 10, 000 data points, 5, 000 for the first class and 5, 000 for the second. Each data point Xi \u223c N (yi1, 1) is a Gaussian random variable of mean yi1 and variance 1, where yi = 21{i \u2264 5, 000} \u2212 1 is the true class of Xi. These data points are then distributed randomly and evenly to the n = 100 nodes of the network.\nFigure 3 and Figure 4 show the performance of the compared algorithms for logistic classification on two networks: a 10\u00d710 grid graph and an Erdo\u0308s-Re\u0301nyi random graph of average degree 6. As for\nleast-squares regression, all algorithms are linearly convergent, and their convergence rates scale on several orders of magnitude. In this case, the centralized optimal algorithm DAGD is outperformed by MSDA, although the two convergence rates are relatively similar. Again, when the communication time is smaller than the computation time (\u03c4 1), performing several communication rounds per gradient iteration will improve the efficiency of the algorithm and MSDA substantially outperforms SSDA. Note that, in Figure 4(a), D-ADMM requires 383 iterations to reach the same error obtained after only 10 iterations of SSDA, demonstrating a substantial improvement over state-ofthe-art methods."}, {"heading": "6 Conclusion", "text": "In this paper, we derived optimal convergence rates for strongly convex and smooth distributed optimization in two settings: centralized and decentralized communications in a network. For the decentralized setting, we introduced the multi-step dual accelerated (MSDA) algorithm with a provable optimal linear convergence rate, and showed its high efficiency compared to other state-of-the-art methods, including distributed ADMM and EXTRA. The simplicity of the approach makes the algorithm extremely flexible, and allows for future extensions, including time-varying networks and an analysis for non-strongly-convex functions. Finally, extending our complexity lower bounds to time delays, variable computational speeds of local systems, or machine failures would be a notable addition to this work."}, {"heading": "A Detailed proofs", "text": "A.1 Complexity lower bounds\nProof of Theorem 1. This proof relies on splitting the function used by Nesterov to prove oracle complexities for strongly convex and smooth optimization [8, 22]. Let \u03b2 \u2265 \u03b1 > 0, G = (V, E) a graph and A \u2282 V a set of nodes of G. For all d > 0, we denote as Acd = {v \u2208 V : d(A, v) \u2265 d} the set of nodes at distance at least d from A, and let, for all i \u2208 V , fAi : `2 \u2192 R be the functions defined as:\nfAi (\u03b8) =  \u03b1 2n\u2016\u03b8\u2016 2 2 + \u03b2\u2212\u03b1 8|A| (\u03b8 >M1\u03b8 \u2212 \u03b81) if i \u2208 A \u03b1 2n\u2016\u03b8\u2016 2 2 + \u03b2\u2212\u03b1 8|Acd|\n\u03b8>M2\u03b8 if i \u2208 Acd \u03b1 2n\u2016\u03b8\u2016 2 2 otherwise\n(23)\nwhere M1 : `2 \u2192 `2 is the infinite block diagonal matrix with (\n1 \u22121 \u22121 1\n) on the diagonal, and\nM2 = ( 1 0\n0 M1\n) . First, note that, since 0 M1 + M2 4I , f\u0304A = 1n \u2211n i=1 f A i is \u03b1-strongly\nconvex and \u03b2-smooth. Then, Theorem 1 is a direct consequence of the following lemma:\nLemma 1. IfAcd 6= \u2205, then for any t \u2265 0 and any black-box procedure one has, for all i \u2208 {1, ..., n},\nf\u0304A(\u03b8i,t)\u2212 f\u0304A(\u03b8\u2217) \u2265 \u03b1\n2 (\u221a \u03bag \u2212 1 \u221a \u03bag + 1 )2(1+ t1+d\u03c4 ) \u2016\u03b8i,0 \u2212 \u03b8\u2217\u20162, (24)\nwhere \u03bag = \u03b2/\u03b1.\nProof. This lemma relies on the fact that most of the coordinates of the vectors in the memory of any node will remain equal to 0. More precisely, let ki,t = max{k \u2208 N : \u2203\u03b8 \u2208 Mi,t s.t. \u03b8k 6= 0} be the last non-zero coordinate of a vector in the memory of node i at time t. Then, under any black-box procedure, we have, for any local computation step,\nki,t+1 \u2264  ki,t + 1{ki,t \u2261 0 mod 2} if i \u2208 Aki,t + 1{ki,t \u2261 1 mod 2} if i \u2208 Acd ki,t otherwise . (25)\nIndeed, local gradients can only increase even dimensions for nodes in A and odd dimensions for nodes in Acd. The same holds for gradients of the dual functions, since these have the same block structure as their convex conjugates. Thus, in order to reach the third coordinate, algorithms must first perform one local computation in A, then d communication steps in order for a node in Acd to have a non-zero second coordinate, and finally, one local computation in Acd. Accordingly, one must perform at least k local computation steps and (k \u2212 1)d communication steps to achieve ki,t \u2265 k for at least one node i \u2208 V , and thus, for any k \u2208 N\u2217,\n\u2200t < 1 + (k \u2212 1)(1 + d\u03c4), ki,t \u2264 k \u2212 1. (26)\nThis implies in particular: \u2200i \u2208 V, ki,t \u2264 \u230a t\u2212 1\n1 + d\u03c4\n\u230b + 1 \u2264 t\n1 + d\u03c4 + 1. (27)\nFurthermore, by definition of ki,t, one has \u03b8i,k = 0 for all k > ki,t, and thus\n\u2016\u03b8i,t \u2212 \u03b8\u2217\u201622 \u2265 +\u221e\u2211\nk=ki,t+1\n\u03b8\u2217k 2. (28)\nand, since f\u0304A is \u03b1-strongly convex,\nf\u0304A(\u03b8i,t)\u2212 f\u0304A(\u03b8\u2217) \u2265 \u03b1\n2 \u2016\u03b8i,t \u2212 \u03b8\u2217\u201622. (29)\nFinally, the solution of the global problem min\u03b8 f\u0304A(\u03b8) is \u03b8\u2217k = (\u221a \u03b2\u2212 \u221a \u03b1\u221a \u03b2+ \u221a \u03b1 )k . Combining this result with Eqs. (27), (28) and (29) leads to the desired inequality.\nUsing the previous lemma with d = \u2206 the diameter of G and A = {v} one of the pair of nodes at distance \u2206 returns the desired result.\nProof of Theorem 2. Let \u03b3n = 1\u2212cos(\u03c0n ) 1+cos(\u03c0n )\nbe a decreasing sequence of positive numbers. Since \u03b32 = 1 and limn \u03b3n = 0, there exists n \u2265 2 such that \u03b3n \u2265 \u03b3 > \u03b3n+1. The cases n = 2 and n \u2265 3 are treated separately. If n \u2265 3, let G be the linear graph of size n ordered from node v1 to vn, and weighted with wi,i+1 = 1\u2212 a1{i = 1}. Then, if A = {v1, ..., vdn/32e} and d = (1\u2212 1/16)n\u2212 1, we have |Acd| \u2265 |A| and Lemma 1 implies:\nf\u0304A(\u03b8i,t)\u2212 f\u0304A(\u03b8\u2217) \u2265 n\u03b1\n2 (\u221a \u03bag \u2212 1 \u221a \u03bag + 1 )2(1+ t1+d\u03c4 ) \u2016\u03b8i,0 \u2212 \u03b8\u2217\u20162. (30)\nA simple calculation gives \u03bal = 1 + (\u03bag \u2212 1) n2|A| , and thus \u03bag \u2265 \u03bal/16. Finally, if we take Wa as the Laplacian of the weighted graph G, a simple calculation gives that, if a = 0, \u03b3(Wa) = \u03b3n and, if a = 1, the network is disconnected and \u03b3(Wa) = 0. Thus, by continuity of the eigenvalues of a matrix, there exists a value a \u2208 [0, 1] such that \u03b3(Wa) = \u03b3. Finally, by definition of n, one has \u03b3 > \u03b3n+1 \u2265 2(n+1)2 , and d \u2265 15 16 ( \u221a 2 \u03b3 \u2212 1)\u2212 1 \u2265 1 5 \u221a \u03b3 when \u03b3 \u2264 \u03b33 = 1 3 .\nFor the case n = 2, we consider the totally connected network of 3 nodes, reweight only the edge (v1, v3) by a \u2208 [0, 1], and let Wa be its Laplacian matrix. If a = 1, then the network is totally connected and \u03b3(Wa) = 1. If, on the contrary, a = 0, then the network is a linear graph and \u03b3(Wa) = \u03b33. Thus, there exists a value a \u2208 [0, 1] such that \u03b3(Wa) = \u03b3, and applying Lemma 1 with A = {v1} and d = 1 returns the desired result, since then \u03bag \u2265 2\u03bal/3 and d = 1 \u2265 1\u221a3\u03b3 .\nA.2 Convergence rates of SSDA and MSDA\nProof of Theorem 3. Each step of the algorithm can be decomposed in first computing gradients, and then communicating these gradients across all neighborhoods. Thus, one step takes a time 1 + \u03c4 . Moreover, the Hessian of the dual function F \u2217(\u03bb \u221a W ) is\n( \u221a W \u2297 Id)\u22072F \u2217(\u03bb \u221a W )( \u221a W \u2297 Id), (31)\nwhere \u2297 is the Kronecker product and Id is the identity matrix of size d. Also, note that, in Alg.(2), the current values xt and yt are always in the image of \u221a W \u2297 Id (i.e. the set of matrices x such that\nx>1 = 0). The condition number (in the image of \u221a W \u2297 Id) can thus be upper bounded by \u03bal\u03b3 , and Nesterov\u2019s acceleration requires \u221a\n\u03bal \u03b3 steps to achieve any given precision [22].\nProof of Theorem 4. First, since PK(W ) is a gossip matrix, Theorem 3 implies the convergence of Alg.(3). In order to simplify the analysis, we multiply W by 2(1+\u03b3)\u03bb1(W ) , so that the resulting gossip matrix has a spectrum in [1\u2212c\u221212 , 1+c \u22121 2 ]. Applying Theorem 6.2 in [25] with \u03b1 = 1\u2212c \u22121 2 , \u03b2 = 1 + c\u221212 and \u03b3 = 0 implies that the minimum\nmin p\u2208PK ,p(0)=0 max x\u2208[1\u2212c\u221212 ,1+c \u22121 2 ] |p(t)\u2212 1| (32)\nis attained by PK(x) = 1\u2212 TK(c2(1\u2212x))TK(c2) . Finally, Corollary 6.3 of [25] leads to\n\u03b3(PK(W )) \u2265 1\u2212 2 c\nK 1\n1+c2K1\n1 + 2 cK1\n1+c2K1\n= ( 1\u2212 cK1 1 + cK1 )2 , (33)\nwhere c1 = 1\u2212\u221a\u03b3 1+ \u221a \u03b3 , and taking K = b 1\u221a \u03b3 c implies\n1\u221a \u03b3(PK(W )) \u2264 1 + c 1\u221a \u03b3+1 1\n1\u2212 c 1\u221a \u03b3+1\n1\n\u2264 2. (34)\nThe time required to reach an \u03b5 > 0 precision using Alg.(3) is thus upper bounded by\nO ( (1 +K\u03c4) \u221a \u03bal\n\u03b3(PK(W )) ln(1/\u03b5)\n) = O (\u221a \u03bal(1 + 1\u221a \u03b3 \u03c4) ln(1/\u03b5) ) ."}, {"heading": "B Composite problems for machine learning", "text": "When the local functions are of the form\nfi(\u03b8) = gi(Bi\u03b8) + c\u2016\u03b8\u20162, (35)\nwhere Bi \u2208 Rmi\u00d7d and gi is smooth and has proximal operator which is easy to compute (and hence also g\u2217i ), an additional Lagrange multiplier \u03bd can be used to make the Fenchel conjugate of gi appear in the dual optimization problem. More specifically, from the primal problem of Eq. (12), one has, with \u03c1 > 0 an arbitrary parameter:\ninf \u0398 \u221a W=0 F (\u0398) = inf \u0398 \u221a W=0, \u2200i,xi=Bi\u03b8i\n1\nn n\u2211 i=1 gi(xi) + c\u2016\u03b8i\u201622\n= inf \u0398 sup \u03bb,\u03bd\n1\nn n\u2211 i=1 { \u03bd>i Bi\u03b8i \u2212 g\u2217i (\u03bdi) + c\u2016\u03b8i\u201622 } + \u03c1 n tr(\u03bb>\u0398 \u221a W )\n= sup \u03bd\u2208 \u220fn i=1Rmi , \u03bb\u2208Rd\u00d7n \u2212 1 n n\u2211 i=1 g\u2217i (\u03bdi)\u2212 1 4cn n\u2211 i=1 \u2016B>i \u03bdi + \u03c1\u03bb \u221a W i\u201622.\nTo maximize the dual problem, we can use (accelerated) proximal gradient, with the updates:\n\u03bdi,t+1 = inf \u03bd\u2208Rmi\ng\u2217i (\u03bd) + 1\n2\u03b7 \u2225\u2225\u03bd \u2212 \u03bdi,t + \u03b7 2c Bi(B > i \u03bdi,t + \u03c1\u03bbt \u221a W i) \u2225\u22252 2\n\u03bbt+1 = \u03bbt \u2212 \u03b7 \u03c1\n2cn n\u2211 i=1 (B>i \u03bdi,t + \u03c1\u03bbt \u221a W i) \u221a W > i .\nWe can rewrite all updates in terms of zt = \u03bbt \u221a W \u2208 Rd\u00d7n, as\n\u03bdi,t+1 = inf \u03bd\u2208Rmi\ng\u2217i (\u03bd) + 1\n2\u03b7 \u2225\u2225\u03bd \u2212 \u03bdi,t + \u03b7 2c Bi(B > i \u03bdi,t + \u03c1zi,t) \u2225\u22252 2\nzt+1 = zt \u2212 \u03b7 \u03c1\n2cn n\u2211 i=1 (B>i \u03bdi,t + \u03c1zi)W > i .\nIn order to compute the convergence rate of such an algorithm, if we assume that:\n\u2022 each gi is \u00b5-smooth,\n\u2022 the largest singular value of each Bi is less than M ,\nthen we simply need to compute the condition number of the quadratic function\nQ(\u03bd, \u03bb) = 1\n2\u00b5 n\u2211 i=1 \u2016\u03bdi\u201622 + 1 4c n\u2211 i=1 \u2016B>i \u03bdi + \u03c1\u03bb \u221a W i\u201622.\nWith the choice \u03c12 = 1\u03bbmax(W ) ( c \u00b5+M 2), it is lower bounded by ( 1+\u00b5M 2 c ) 4 \u03b3 , which is a natural upper bound on \u03bal/\u03b3. Thus this essentially leads to the same convergence rate than the non-composite case with the Nesterov and Chebyshev accelerations, i.e. \u221a \u03bal/\u03b3.\nThe bound on the conditional number may be shown through the two inequalities:\nQ(\u03bd, \u03bb) 6 1\n2\u00b5 n\u2211 i=1 \u2016\u03bdi\u20162 + 1 2c n\u2211 i=1 \u2016\u03c1\u03bb \u221a W i\u201622 + 1 2c n\u2211 i=1 \u2016B>i \u03bdi\u201622,\nQ(\u03bd, \u03bb) > 1\n2\u00b5 n\u2211 i=1 \u2016\u03bdi\u20162 + 1 1 + \u03b7 1 4c n\u2211 i=1 \u2016\u03c1\u03bb \u221a W i\u201622 \u2212 1 \u03b7 1 4c n\u2211 i=1 \u2016B>i \u03bdi\u201622,\nwith \u03b7 = M2\u00b5/c."}], "year": 2017, "references": [{"title": "Distributed subgradient methods for multi-agent optimization", "authors": ["Angelia Nedic", "Asuman Ozdaglar"], "venue": "IEEE Transactions on Automatic Control,", "year": 2009}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "authors": ["Stephen Boyd", "Neal Parikh", "Eric Chu", "Borja Peleato", "Jonathan Eckstein"], "venue": "Foundations and Trends in Machine Learning,", "year": 2011}, {"title": "Dual averaging for distributed optimization: Convergence analysis and network scaling", "authors": ["John C Duchi", "Alekh Agarwal", "Martin J Wainwright"], "venue": "IEEE Transactions on Automatic control,", "year": 2012}, {"title": "EXTRA: An exact first-order algorithm for decentralized consensus optimization", "authors": ["Wei Shi", "Qing Ling", "Gang Wu", "Wotao Yin"], "venue": "SIAM Journal on Optimization,", "year": 2015}, {"title": "On the linear convergence of the ADMM in decentralized consensus optimization", "authors": ["Wei Shi", "Qing Ling", "Kun Yuan", "Gang Wu", "Wotao Yin"], "venue": "IEEE Transactions on Signal Processing,", "year": 2014}, {"title": "Linear convergence rate of a class of distributed augmented lagrangian algorithms", "authors": ["Du\u0161an Jakoveti\u0107", "Jos\u00e9 MF Moura", "Joao Xavier"], "venue": "IEEE Transactions on Automatic Control,", "year": 2015}, {"title": "Achieving geometric convergence for distributed optimization over time-varying graphs", "authors": ["A. Nedich", "A. Olshevsky", "W. Shi"], "venue": "ArXiv e-prints,", "year": 2016}, {"title": "Introductory lectures on convex optimization : a basic course", "authors": ["Yurii Nesterov"], "venue": "Kluwer Academic Publishers,", "year": 2004}, {"title": "Randomized gossip algorithms", "authors": ["Stephen Boyd", "Arpita Ghosh", "Balaji Prabhakar", "Devavrat Shah"], "venue": "IEEE/ACM Transactions on Networking (TON),", "year": 2006}, {"title": "Fast distributed gradient methods", "authors": ["Du\u0161an Jakoveti\u0107", "Joao Xavier", "Jos\u00e9 MF Moura"], "venue": "IEEE Transactions on Automatic Control,", "year": 2014}, {"title": "DSA: Decentralized double stochastic averaging gradient algorithm", "authors": ["Aryan Mokhtari", "Alejandro Ribeiro"], "venue": "Journal of Machine Learning Research,", "year": 2016}, {"title": "Distributed alternating direction method of multipliers", "authors": ["Ermin Wei", "Asuman Ozdaglar"], "venue": "In 51st Annual Conference on Decision and Control (CDC),", "year": 2012}, {"title": "A decentralized second-order method with exact linear convergence rate for consensus optimization", "authors": ["A. Mokhtari", "W. Shi", "Q. Ling", "A. Ribeiro"], "venue": "IEEE Transactions on Signal and Information Processing over Networks,", "year": 2016}, {"title": "A distributed newton method for large scale consensus optimization", "authors": ["R. Tutunov", "H.B. Ammar", "A. Jadbabaie"], "venue": "ArXiv e-prints,", "year": 2016}, {"title": "On the iteration complexity of oblivious first-order optimization algorithms", "authors": ["Yossi Arjevani", "Ohad Shamir"], "venue": "In 33nd International Conference on Machine Learning,", "year": 2016}, {"title": "Dimension-free iteration complexity of finite sum optimization problems", "authors": ["Yossi Arjevani", "Ohad Shamir"], "venue": "In Advances in Neural Information Processing Systems", "year": 2016}, {"title": "Fundamental limits of online and distributed algorithms for statistical learning and estimation", "authors": ["Ohad Shamir"], "venue": "In Advances in Neural Information Processing Systems", "year": 2014}, {"title": "Communication complexity of distributed convex learning and optimization", "authors": ["Yossi Arjevani", "Ohad Shamir"], "venue": "In Advances in Neural Information Processing Systems", "year": 2015}, {"title": "Distributed stochastic optimization and learning", "authors": ["Ohad Shamir", "Nathan Srebro"], "venue": "In 52nd Annual Allerton Conference on Communication, Control, and Computing (Allerton),", "year": 2014}, {"title": "Fast linear iterations for distributed averaging", "authors": ["Lin Xiao", "Stephen Boyd"], "venue": "Systems & Control Letters,", "year": 2004}, {"title": "Fastest mixing markov chain on graphs with symmetries", "authors": ["Stephen Boyd", "Persi Diaconis", "Pablo Parrilo", "Lin Xiao"], "venue": "SIAM Journal on Optimization,", "year": 2009}, {"title": "Convex optimization: Algorithms and complexity", "authors": ["S\u00e9bastien Bubeck"], "venue": "Foundations and Trends in Machine Learning,", "year": 2015}, {"title": "Parallel and distributed computation : numerical methods", "authors": ["Dimitri P. Bertsekas", "John N. Tsitsiklis"], "venue": "Prentice-Hall International,", "year": 1989}, {"title": "\u03bb1, isoperimetric inequalities for graphs, and superconcentrators", "authors": ["N. Alon", "V.D. Milman"], "venue": "Journal of Combinatorial Theory, series B,", "year": 1985}, {"title": "Iterative Solution of Large Linear Systems", "authors": ["W. Auzinger"], "venue": "Lecture notes, TU Wien,", "year": 2011}, {"title": "Chebyshev acceleration of iterative refinement", "authors": ["M. Arioli", "J. Scott"], "venue": "Numerical Algorithms,", "year": 2014}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "authors": ["Rie Johnson", "Tong Zhang"], "venue": "In Advances in Neural Information Processing Systems", "year": 2013}, {"title": "SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives", "authors": ["Aaron Defazio", "Francis Bach", "Simon Lacoste-Julien"], "venue": "In Advances in Neural Information Processing Systems", "year": 2014}], "id": "SP:b9084c6c0248a72c6d91bc21228d9a359ff91b1a", "authors": [{"name": "Kevin Scaman", "affiliations": []}, {"name": "Francis Bach", "affiliations": []}, {"name": "S\u00e9bastien Bubeck", "affiliations": []}, {"name": "Yin Tat Lee", "affiliations": []}, {"name": "Laurent Massouli\u00e9", "affiliations": []}], "abstractText": "In this paper, we determine the optimal convergence rates for strongly convex and smooth distributed optimization in two settings: centralized and decentralized communications over a network. For centralized (i.e. master/slave) algorithms, we show that distributing Nesterov\u2019s accelerated gradient descent is optimal and achieves a precision \u03b5 > 0 in time O(\u03bag(1 + \u2206\u03c4) ln(1/\u03b5)), where \u03bag is the condition number of the (global) function to optimize, \u2206 is the diameter of the network, and \u03c4 (resp. 1) is the time needed to communicate values between two neighbors (resp. perform local computations). For decentralized algorithms based on gossip, we provide the first optimal algorithm, called the multi-step dual accelerated (MSDA) method, that achieves a precision \u03b5 > 0 in time O( \u221a \u03bal(1 + \u03c4 \u221a \u03b3 ) ln(1/\u03b5)), where \u03bal is the condition number of the local functions and \u03b3 is the (normalized) eigengap of the gossip matrix used for communication between nodes. We then verify the efficiency of MSDA against state-of-the-art methods for two problems: least-squares regression and classification by logistic regression.", "title": "Optimal algorithms for smooth and strongly convex distributed optimization in networks"}