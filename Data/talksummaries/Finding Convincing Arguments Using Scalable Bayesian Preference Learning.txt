2	19	Stance: “No, it is Raffles!” Argument 1: HE HAS A BOSS(RAFFLES) HE HAS TO FOLLOW HIM AND NOT GO ABOUT DOING ANYTHING ELSE...
4	18	The plan consisted of separate areas for different... Crowdsourced labels: {2 1, 1 2, 2 1} Figure 1: Example argument pair from an online debate.
6	32	Automated methods could help readers overcome this challenge by identifying highquality, persuasive arguments from both sides of a debate.
7	17	Theoretical approaches for assessing argument quality have proved difficult to apply to everyday arguments (Boudry et al., 2015).
14	30	An alternative way to judge arguments is to compare them against one another (Habernal and Gurevych, 2016).
16	28	Pairwise comparisons such as this are known to place less cognitive burden on human annotators than choosing a numerical rating and allow fine-grained sorting of items that is not possible with categorical labels (Kendall, 1948; Kingsley and Brown, 2010).
17	26	Unlike numerical ratings, pairwise comparisons are not affected by different annotators’ biases toward high, low or middling values, or an individual’s bias changing over time.
22	39	For example, when a user clicks on an argument in a list it can be interpreted as a preference for the selected argument over more highly-ranked arguments.
25	27	We model argument convincingness as a function of textual features, including word embeddings, and develop an inference method for GPPL that scales to realistic dataset sizes using stochastic variational inference (SVI) (Hoffman et al., 2013).
26	19	Using datasets provided by Habernal and Gurevych (2016), we show that our method outperforms the previous state-of-the-art for ranking arguments by convincingness and identifying the most convincing argument in a pair.
30	24	Section 2 reviews related work on argumentation, then Section 3 motivates the use of Bayesian methods by discussing their successful applications in NLP.
32	27	Section 6 presents our evaluation, comparing our method to the state-of-the art and testing with noisy data and active learning.
45	29	Given a model, M , and observed data, D, we apply Bayes’ rule to obtain a posterior distribution over M , which can be used to make predictions about unseen data or latent variables: P (M |D) = P (D|M)P (M) P (D) , (1) where P (D|M) is the likelihood of the data given M , and P (M) is the model prior.
79	72	We simplify Equation 2 by integrating out δi and δj to obtain the preference likelihood: p(i j|f(xi), f(xj)) = ∫ ∫ p(i j|f(xi), f(xj), δi, δj) N (δi; 0, 1)N (δj ; 0, 1)dδidδj = Φ (z) , (3) where z = (f(xi) − f(xj))/ √ 2, and Φ is the cumulative distribution function of the standard normal distribution.
94	28	The variational inference algorithm begins by initializing the parameters G, f̂ , C, a and b at random.
112	29	To avoid the cost of optimizing the length-scales, we can alternatively set them using a median heuristic, which has been shown to perform well in practice (Gretton et al., 2012): l̃d = 1Dmedian ({|xi,d − xj,d|, ∀i = 1, ..., N,∀j = 1, ..., N}).
171	82	Figure 6 shows the effect of varying the number of inducing points, M , on the overall runtime and accuracy of the method.
175	34	We tested GPPL with both the SVI algorithm, with M = 100 and Pn = 200, and variational inference without inducing points or stochastic updates (labeled “no SVI”) with different sizes of training dataset subsampled from UKPConvArgStrict.
185	19	We include runtimes for SVM and BiLSTM in Figures 5a and 5c to show their runtime patterns, but note that the runtimes reflect differences in implementations and system hardware.
196	28	Using a combination of features improves all methods, suggesting that embeddings and linguistic features contain complementary information.
206	51	We use UKPConvArgCrowdSample to introduce noisy data and conflicting pairwise labels to both the classification and regression tasks, to test the hypothesis that GPPL would best handle unreliable crowdsourced data.
218	74	The labels for these pairs are then added to the training set and used to re-train the model.
220	26	The result is plotted in Figure 7, showing that GPPL reaches a mean accuracy of 70% with only 100 labels, while SVM and BiLSTM do not reach the same performance given 400 labels.
223	18	If the model overfits to a small dataset, it can mis-classify some data points with high confidence so that they are not selected and corrected by uncertainty sampling.
229	59	Figure 8 shows the distribution of length-scales for each category of ling+GloVe features, averaged over the folds in UKPConvArgStrict where MLII optimization improved accuracy by 3%.
258	512	GPPL underrated short arguments with the ngrams “I think”, “why?”, and “don’t know”, which were used as part of a rhetorical question rather than to state that the author was uncertain or uninformed.
259	32	These cases may not be distinguishable by a GP given only ling + GloVe features.
267	59	Active learning experiments showed that GPPL is an effective model for cold-start situations and that the convincingness of Internet arguments can be predicted reasonably well given only a small number of samples.
269	33	Future work will evaluate our approach on other NLP tasks where reliable classifications may be difficult to obtain, such as learning to classify text from implicit user feedback (Joachims, 2002).
270	18	We also plan to investigate training the GP using absolute scores in combination with pairwise labels.
