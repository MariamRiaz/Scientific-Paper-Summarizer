5	28	Driven by this observation, our main hypothesis is that the discourse structure of NF answers provides complementary information to state-ofthe-art QA models that measure the similarity (either lexical and/or semantic) between question and answer.
6	68	We propose a novel answer reranking (AR) model that combines lexical semantics (LS) with discourse information, driven by two representations of discourse: a shallow representation centered around discourse markers and surface text information, and a deep one based on the Rhetorical Structure Theory (RST) discourse framework (Mann and Thompson, 1988).
14	30	We demonstrate good domain transfer performance between these corpora, suggesting that answer discourse structures are largely independent of domain, and thus broadly applicable to NF QA.
29	56	This framework functions in two distinct scenarios, which use the same AR model, but differ in the way candidate answers are retrieved: CQA: In this scenario, the task is defined as reranking all the user-posted answers for a particular question to boost the community-selected best answer to the top position.
30	30	This is a commonly used setup in the CQA community (Wang et al., 2009).4 Thus, for a given question, all its answers are fetched from the answer collection, and an initial ranking is constructed based on the cosine similarity between theirs and the question’s lemma vector representations, with lemmas weighted using tf.idf (Ch.
32	61	We use this setup to answer questions from a biology textbook, where each section is indexed as a standalone document, and each paragraph in a given document is considered as a candidate answer.
34	42	The candidate answers are scored using a linear interpolation of two cosine similarity scores: one between the entire parent document and question (to model global context), and a second between the answer candidate and question (for local context).6 Because the number of answer candidates is typically large (e.g., equal to the number of paragraphs in the textbook), we return the N top candidates with the highest scores.
36	67	AR analyzes the candidates using more expensive techniques to extract discourse and LS features (detailed in §4), and these features are then used in concert with a learning framework to rerank the candidates and elevate correct answers to higher positions.
38	30	We propose two separate discourse representation schemes – one shallow, centered around discourse markers, and one deep, based on RST.
56	29	It is important to note that these discourse features are more expressive than features based on discourse markers alone (Higashinaka and Isozaki, 2008; Verberne et al., 2010).
58	147	Second, these features model the intensity of the match between the text surrounding the discourse structure and the question text using both the assigned argument labels and the feature values.
60	26	In RST, the text is segmented into a sequence of non-overlapping fragments called elementary discourse units (EDUs), and binary discourse relations recursively connect neighboring units.
61	22	Most relations are hypotactic, where one of the units in the relation (the nucleus) is considered more important than the other (the satellite).
70	25	Similar to the DMM, these features take real values obtained by averaging the cosine similarity of the arguments with the question content.9 Fig.
73	78	Inspired by the work of Yih et al. (2013), we include lexical semantics in our reranking model.
112	21	The second baseline (CR + LS) trains a reranking model without discourse, using just the CR and LS features.
123	35	Lexical semantic features increase performance for all settings, but demonstrate far more utility to the open-domain YA corpus.
124	43	This disparity is likely due to the difficulty in assembling LS training data at an appropriate level for the biology corpus, contrasted with the relative abundance of large scale open-domain lexical semantic resources.
125	40	For the YA corpus, where lexical semantics showed the most benefit, simply adding LS features to the CR baseline increases baseline P@1 performance from 19.57 to 26.57, a +36% relative improvement.
126	35	Most importantly, comparing lines 5 and 9 with their respective baselines (lines 2 and 6, respectively) indicates that LS is largely orthogonal to discourse.
129	78	That this absolute performance increase is nearly identical indicates that LS features are complementary to and additive with the full discourse model.
130	33	Indeed, an analysis of the questions improved by discourse vs. LS (line 5 vs. 6) showed that the intersection of the two sets is low (approximately a third of each set).
131	53	Finally, while the discourse models perform well for HOW or manner questions, performance on Bio WHY corpus suggests that reason questions are particularly amenable to discourse analysis.
139	54	For the Bio corpus where answer candidates consist of entire paragraphs of a biology text, overall performance is dominated by inter-sentence discourse features.
159	38	This way, the DMM and DPM features jointly capture discourse structures and semantic similarity between answer segments and question.
183	41	Here we show how to model answer structures using discourse and how to integrate the two aspects into a holistic framework.
184	61	Empirically we show that modeling answer discourse structures is complementary to modeling lexical semantic similarity and that the best performance is obtained when they are tightly integrated.
185	46	We evaluate the proposed approach on multiple genres and question types and obtain benefits of up to 24% relative improvement over a strong baseline that combines information retrieval and lexical semantics.
186	102	We further demonstrate that answer discourse structures are largely independent of domain and transfer well, even between radically different datasets.
187	35	This work is open source and available at: http://nlp.sista.arizona.edu/releases/ acl2014.
