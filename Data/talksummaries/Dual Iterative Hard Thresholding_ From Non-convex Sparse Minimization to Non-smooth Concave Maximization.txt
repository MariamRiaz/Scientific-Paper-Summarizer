34	39	For a matrixA, σmax(A) (σmin(A)) denotes its largest (smallest) singular value.
35	57	The rest of this paper is organized as follows: In §2 we briefly review some relevant work.
36	140	In §3 we develop a Lagrangian duality theory for sparsityconstrained minimization problems.
37	35	The dual IHT-style algorithms along with convergence analysis are presented in §4.
57	9	We now introduce the following concept of sparse saddle point which is a restriction of the conventional saddle point to the setting of sparse optimization.
65	31	Theorem 1 shows that the conditions (a)∼(c) are sufficient and necessary to guarantee the existence of a sparse saddle point for the Lagrangian form L. This result is different from from the traditional saddle point theorem which requires the use of the Slater Constraint Qualification to guarantee the existence of saddle point.
67	47	It is easy to verify that the condition (c) in Theorem 1 is equivalent to HF̄ (P ′(w̄)) = 0, w̄min ≥ 1 λ ‖P ′(w̄)‖∞.
68	17	The following sparse mini-max theorem guarantees that the min and max in (2) can be safely switched if and only if there exists a sparse saddle point for L(w,α).
77	10	This is called the primal minimization problem and it is the min-max side of the sparse mini-max theorem.
80	13	The dual objective function D(α) is given by D(α) = 1 N N∑ i=1 −l∗i (αi)− λ 2 ‖w(α)‖2, where w(α) = Hk ( − 1Nλ ∑N i=1 αixi ) .
91	14	The sparse duality theory developed in this section suggests a natural way for finding the global minimum of the sparsity-constrained minimization problem in (1) via maximizing its dual problem in (5).
92	19	Once the dual maximizer ᾱ is estimated, the primal sparse minimizer w̄ can then be recovered from it according to the prima-dual connection w̄ = Hk ( − 1λN ∑N i=1 ᾱixi ) as given in the condition (c).
93	9	Since the dual objective function D(α) is shown to be concave, its global maximum can be estimated using any convex/concave optimization method.
95	14	Generally, D(α) is a non-smooth function since: 1) the conjugate function l∗i of an arbitrary convex loss li is generally non-smooth and 2) the term ‖w(α)‖2 is non-smooth with respect to α due to the truncation operation involved in computing w(α).
101	14	The procedure generates a sequence of prima-dual pairs (w(0), α(0)), (w(1), α(1)), .
103	11	Then in the primal update step S2, the primal variable w(t) is constructed from α(t) using a k-sparse truncation operation in (7).
129	8	(a) Parameter estimation error: The sequence {α(t)}t≥1 generated by Algorithm 1 satisfies the following estimation error inequality: ‖α(t) − ᾱ‖2 ≤ c1 ( 1 t + ln t t ) , (b) Support recovery and primal-dual gap: Assume additionally that ̄ := w̄min − 1λ‖P ′(w̄)‖∞ > 0.
134	17	Since (t)P ≤ (t) PD always holds, the convergence rates in Theorem 4 are applicable to the primal sub-optimality as well.
138	18	For SDIHT, we can establish similar non-asymptotic convergence results as summarized in the following theorem.
146	35	We first show the model estimation performance of DIHT when applied to sparse ridge regression models on synthetic datasets.
147	11	Then we evaluate the efficiency of DIHT/SDIHT on sparse `2- regularized Huber loss and Hinge loss minimization tasks using real-world datasets.
152	45	The entries of covariance Σij = { 1 i = j 0.25 i 6= j .
160	8	We consider solving the problem under the sparsity level k = k̄.
166	13	We use 50 data copies as validation set to select the parameter λ from {10−6, ..., 102} and the percentage of successful support recovery is evaluated on the other 100 data copies.
167	10	Iterative hard thresholding (IHT) (Blumensath & Davies, 2009) and hard thresholding pursuit (HTP) (Foucart, 2011) are used as the baseline primal algorithms.
169	8	We can observe from this group of curves that DIHT consistently achieves lower parameter estimation error and higher rate of successful support recovery than IHT and HTP.
174	8	We select 0.5 million samples from RCV1 dataset for model training (N d).
178	36	After that we test the time cost spend by other algorithms to make the primal loss reach P (w(t)).
183	13	It is obvious that under all tested (k, λ) configurations on both datasets, DIHT and SDIHT need much less time than the primal baseline algorithms, IHT, HTP and SVR-GHT to reach the same primal suboptimality.
187	31	It is standard to know (Hsieh et al., 2008) l∗Hinge(αi) = { yiαi if yiαi ∈ [−1, 0] +∞ otherwise .
