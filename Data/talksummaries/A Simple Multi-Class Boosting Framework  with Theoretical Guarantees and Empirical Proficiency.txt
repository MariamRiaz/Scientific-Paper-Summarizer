28	13	More recently, CD-MCBoost and CW-Boost return a K-dimensional vector of class scores, focusing each iteration on a (binary) problem of improving the margin of one class at a time (Saberian & Vasconcelos, 2011; Shen & Hao, 2011).
37	13	The goal of our work is to propose a multi-class boosting framework with a simple family of binary weak learners that guarantee training convergence and are easily interpretable.
38	46	Using REBEL (Appel et al., 2016) as the multiclass boosting method, our framework is meant to be as straightforward as possible so that it is accessible and practical to more users; outlining it in Sec.
39	17	In this section, we define our notation, introduce our boosting framework, and describe our training procedure.
41	133	Each point is comprised of d features and belongs to one ofK classes: x ∈ X ⊆ Rd, y ∈ Y ≡ {1,2, ...,K} A good classifier reduces the training error while generalizing well to potentially-unseen data.
42	42	We use REBEL (Appel et al., 2016) due to its support for binaryweak learners, its mathematical simplicity (i.e. closed-form solution to loss minimization), and its strong empirical performance.
43	56	REBEL returns a vector-valued outputH, the sum of T {weak learner f, accumulation vector a} pairs, where ft : X → {±1} and at ∈ R K : H(x) ≡ T∑ t=1 ft(x)at The hypothesized class is simply the index of the maximal entry in H: F(x) ≡ argmax y∈Y {〈H(x), δy〉} The average misclassification error ε can be expressed as: ε ≡ 1 N N∑ n=1 1(F(xn) 6=yn) (1) REBEL uses an exponential loss function to upper-bound the average training misclassification error: ε ≤ L ≡ 1 2N N∑ n=1 〈exp[yn⊙H(xn)],1〉 (2) where: yn ≡ 1−2δyn (i.e. all +1s with a −1 in the ythn index) Being a greedy, additive model, all previously-trained parameters are fixed and each iteration amounts to jointly optimizing a new weak learner f and accumulation vector a.
44	83	To this end, the loss at iteration I+1 can be expressed as: LI+1 = 1 N N∑ n=1 〈wn, exp[f(xn)yn⊙a]〉 (3) where: wn ≡ 1 2 exp[yn⊙HI(xn)] Given a weak learner f, we define true and false (i.e. correct and incorrect) multi-class weight sums (sTf and s F f ) as: sTf ≡ 1 N N∑ n=1 1[f(xn)yn<0]⊙wn, s F f ≡ 1 N N∑ n=1 1[f(xn)yn>0]⊙wn thus: sTf+s F f = 1 N N∑ n=1 wn, s T f−sFf = 1 N N∑ n=1 f(xn)wn⊙yn Using these weight sums, the loss can be simplified to: LI+1 ≡ Lf ≡ 〈sTf , exp[−a]〉+ 〈sFf , exp[a]〉 (4) In this form, it is easily shown that with the optimal accumulation vector a∗, the loss has an explicit expression: a∗= 1 2 ( ln[sTf ]− ln[sFf ] ) ∴ L∗f = 2〈 √ sTf⊙s F f ,1〉 (5) At each iteration, growing decision trees requires an exhaustive search through a pool of decision stumps (which is tractable but time-consuming), storing the binary learner that best reduces the multi-class loss in Eq.
63	17	In the following section, we specify a suitable family of weak learners with the ability to isolate single points.
65	21	Instead, our proposed family of weak learners (called localized similarities) compare points in the input space using a similarity measure.
78	25	5, calculate the base loss L1 for the homogeneous stump f1 (i.e. the one-point stump with any xi and τ ≡ ∞, classifying all points as +1).
83	17	Of the two stumps f1 and fi, store the one with smaller loss as best-so-far.
91	12	In this section, we demonstrate that the continual reduction in loss serves only to improve the decision boundaries and not to overfit the data.
92	17	We generated 2-dimensional synthetic datasets in order to better visualize and get an intuition for what the classifiers 2 “most similar” need not be exact; approximate nearestneighbors also works with negligible differences.
95	39	Our classifier achieves perfect training (left) and test classification (right), producing a visually simple wellgeneralizing contour around the points.
100	90	In this section of experiments, classifiers were trained using varying amounts of data, from 4/5 to 1/5 of the total training set.
103	19	This is more rigorously observed from the training curves; the test error does not increase after reaching its minimum (for hundreds of iterations).
106	15	All classifiers seem to degenerate gracefully, isolating rogue points and otherwise maintaining smooth boundaries.
107	47	Even the classifier trained on 30% mislabeled data (which we would consider to be unreasonably noisy) is able to maintain smooth boundaries.
109	30	Moreover, test errors approximately equal the fraction of mislabeled data, further validating the generalization of our method.
136	36	In the regime of small to medium amounts of data (within which UCI andMNIST datasets belong, i.e. 10 < N < 106 training samples), such networks cannot be too complex.
138	37	Four neural networks were implemented, each having one of the following architectures: [d−4d−K], [d−4K−K], [d−2d−d−K], [d−4K−2K−K], where d is the number of input dimensions andK is the number of output classes.
140	15	A multi-class SVM (Chang & Lin, 2011) was validated using a 5× 6 parameter sweep for C and γ.
141	37	Our method was run until the training loss fell below 1/N .
142	20	Overall, REBEL using localized similarities achieves the best results on eight of the ten datasets, decisively marking it as the method of choice for this range of data.
145	20	Why does this happen with our framework?
146	37	Firstly, we note that the largest-margin boundary between two points is the hyperplane that bisects them.
148	66	Therefore, it is not surprising that with only a pool of localized similarities, a classifier should have what it needs to place good boundaries.
149	30	Further, not all pairs need to be separated (since many neighboring points belong to the same class); hence, only a small subset of the ∼ N2 possible learners will ever need to be selected.
150	21	Secondly, we note that if some point (either an outlier or an unfortunately-placed point) continues to increase in weight until it can no-longer be ignored, it can simply be isolated and individually dealt with using a one-point localized similarity, there is no need to combine it with other “innocentbystander” points.
