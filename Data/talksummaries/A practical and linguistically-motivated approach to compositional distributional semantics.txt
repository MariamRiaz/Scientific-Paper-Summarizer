0	18	The research of the last two decades has established empirically that distributional vectors for words obtained from corpus statistics can be used to represent word meaning in a variety of tasks (Turney and Pantel, 2010).
1	33	If distributional vectors encode certain aspects of word meaning, it is natural to expect that similar aspects of sentence meaning can also receive vector representations, obtained compositionally from word vectors.
5	108	For instance, symmetric operations like vector addition are insensitive to syntactic structure, therefore meaning differences encoded in word order are lost in composition: pandas eat bamboo is identical to bamboo eats pandas.
6	48	Guevara (2010), Mitchell and Lapata (2010), Socher et al. (2011) and Zanzotto et al. (2010) generalize the simple additive model by applying structure-encoding operators to the vectors of two sister nodes before addition, thus breaking the inherent symmetry of the simple additive model.
11	23	Generally one of the components of a phrase, e.g., an adjective, acts as a function affecting the other component (e.g., a noun).
16	21	Tensor by vector multiplication formalizes function application and serves as the general composition method.
23	36	First, one estimates matrices of verb-object phrases from subject and subject-verb-object vectors; next, transitive verb tensors are estimated from verb-object matrices and object vectors.
26	37	For example, if noun meanings are encoded in vectors of 300 dimensions, adjectives become matrices of 3002 cells, and transitive verbs are represented as tensors with 3003=27, 000, 000 dimensions.
34	23	For example, verbs like eat can be used in transitive or intransitive constructions (children eat meat/children eat), or in passive (meat is eaten).
36	16	Deverbal nouns like demolition, often used without mention of who demolished what, would have to get vector representations while the corresponding verbs (demolish) would become tensors, which makes immediately related verbs and nouns incomparable.
46	49	As follows from section 1.2, it would be desirable to have a compositional distributional model that encodes function-argument relations but avoids the troublesome high-order tensor representations of the pure lexical function model, with all the practical problems that come with them.
47	19	We may still want to represent word meanings in different syntactic contexts differently, but at the same time we need to incorporate a formal connection between those representations, e.g., between the transitive and the intransitive instantiations of the verb to eat.
86	35	For instance, for transitive verbs we estimate the verbsubject combination matrix from subject and verb- subject vectors, the verb-object combination matrix from object and verb-object vectors.
89	57	Finally, the fact that we represent the predicate interaction with each of its arguments in a separate matrix allows for a natural and intuitive treatment of argument alternations.
91	129	To model passive usages, we insert the object matrix of the verb only, which will be multiplied by the syntactic subject vector, capturing the similarity between eat meat and meat is eaten.
94	32	On the other hand, if the verb occurs with more arguments than usual in testing materials, we can add a default diagonal identity matrix to its representation, signaling agnosticism about how the verb relates to the unexpected argu- ment.
97	67	We still employ a linguistically-motivated notion of semantic composition as function application and use distinct kinds of representations for different semantic types.
101	31	The first data set, created by Edward Grefenstette and Mehrnoosh Sadrzadeh and introduced in Kartsaklis et al. (2013), features 200 sentence pairs that were rated for similarity by 43 annotators.
102	76	In this data set, sentences have fixed adjective-noun-verb-adjective-noun (anvan) structure, and they were built in order to crucially require context-based verb disambiguation (e.g., young woman filed long nails is paired with both young woman smoothed long nails and young woman registered long nails).
109	18	For example, the target A man plays an acoustic guitar is matched with paraphrases such as A man plays guitar and The man plays the guitar, and foils such as The man plays no guitar and A guitar plays a man.
116	24	Following standard practice in paraphrase detection studies (e.g., Blacoe and Lapata (2012)), we use cosine similarity between sentence pairs as computed by one of our systems together with two shallow similarity cues: word overlap between the two sentences and difference in sentence length.
120	21	This set contains 561 pairs of glosses (from the WordNet and OntoNotes databases), rated by 5 judges for similarity.
121	21	Our main interest in this set stems from the fact that glosses are rarely well-formed full sentences (consider, e.g., cause something to pass or lead somewhere; coerce by violence, fill with terror).
138	19	We did not attempt to train a lf model for the larger and more varied msrvid and onwn data sets, as this would have been extremely time consuming and impractical for all the reasons we discussed in Section 1.2 above.
140	31	Since syntax guides lf and plf composition, we supplied all test sentences with categorial grammar parses.
148	22	Evidently, the separately-trained subject and object matrices of plf, being less affected by data sparseness than the 3-way tensors of lf, are better able to capture how verbs interact with their arguments.
161	34	More worryingly, the simple word overlap baseline reported in the table sports a larger difference than our best model.
178	51	For example, one could add representations for different conjunctions (and vs. or), train matrices for verb arguments other than subject and direct object, or include new types of modifiers into the model, etc.
179	96	While there is potential for local improvements, our framework, which extends and improves on existing compositional semantic vector models, has demonstrated its ability to account for full sentences in a principled and elegant way.
180	86	Our implementation of the model relies on simple and efficient training, works fast, and shows good empirical results.
