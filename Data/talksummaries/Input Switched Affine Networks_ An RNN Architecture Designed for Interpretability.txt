16	29	In this work we follow this second approach and build interpretability into our network model, while maintaining good, though not always state-of-the-art, performance for the tasks we study.
18	15	We develop and analyze a model trained on a one-step-ahead prediction task of the Text8 dataset, which is 10 million characters of Wikipedia text (Mahoney, 2011), on the Billion Word Benchmark (Chelba et al., 2013), and finally on a toy multiple parentheses counting task which we fully reverse engineer.
20	55	Linear timevarying systems are standard material in undergraduate electrical engineering text books, and are closely related to our technique.
22	19	A recent example is the switched linear dynamical system in (Linderman et al., 2016).
23	13	Focusing on language modeling, (Belanger & Kakade, 2015) defined a probabilistic linear dynamical system (LDS) as a generative language model for creating context-dependent token embeddings and then used steady-state Kalman filtering for inference over token sequences.
25	15	A critical difference between the ISAN and the LDS is that the ISAN weight matrices are input token dependent (while the biases of both models are input dependent).
37	16	In what follows, we define the ISAN architecture, demonstrate its performance on the one-step-ahead prediction task, and then analyze the model in a multitude of ways, most of which would be currently difficult or impossible to accomplish with modern nonlinear recurrent architectures.
45	38	We map from the hidden state, ht, into a logit space via an affine map.
46	40	The probabilities are computed as p (xt+1) = softmax (lt) (2) lt = Wro ht + bro, (3) where Wro and bro are the readout weights and biases, and lt is the logit vector.
55	76	Samples of generated text from this model are relatively coherent.
56	125	We show two examples, after priming with "annual reve", at inverse temperature of 1.5, and 2.0, respectively: • “annual revenue and producer of the telecommunications and former communist action and saving its new state house of replicas and many practical persons” • “annual revenue seven five three million one nine nine eight the rest of the country in the united states and south africa new”.
57	36	As a preliminary, comparative analysis, we performed PCA on the state sequence over a large set of sequences for the vanilla RNN, GRU of varying sizes, and ISAN.
61	29	This perplexity is only slightly better than that of a Naive Bayes model on the task, at 3.3 bits / char.
82	58	Taking advantage of the linearity of the hidden state dynamics for any sequence of inputs, we decompose the current latent state ht into contributions originating from different time points s in the history of the input: ht = t∑ s=0 ( t∏ s′=s+1 Wxs′ ) bxs , (4) where the empty product when s+1 > t is 1 by convention, and bx0 = h0 is the learned initial hidden state.
83	25	Using this decomposition and the fact that matrix multiplication is a linear transformation we can also write the unnormalized logit-vector, lt, as a sum of terms linear in the biases, lt = bro + t∑ s=0 κts (5) κts = Wro ( t∏ s′=s+1 Wxs′ ) bxs , (6) where κts is the contribution from time step s to the logits at time step t, and κtt = bxt .
93	33	For example, the sequence ‘_annual_revenue_’ is processed by the ISAN: Starting with an all-zero hidden state, we use equation (6) to accumulate a sequence of κt‘_′ ,κ t ‘a′ ,κ t ‘n′ ,κ t ‘n′ , .... We then used these values to understand the prediction of the network at some time t, by simple addition across the s index.
96	146	We show that without the contributions from ‘_annual’ the most likely decoding of the character after the second ‘e’ is ‘r’ (to form ‘reverse’), while the contributions from ‘_annual’ tip the balance in favor of ‘n’, decoding to ‘revenue’.
97	50	Using ISAN, we can investigate information timescales in the network.
110	11	We are free to perform a change of basis on the hidden state, and then to run the affine ISAN dynamics in that new basis.
112	21	In particular we can construct a ‘readout basis’ that explicitly divides the latent space into a subspace Pro‖ spanned by the rows of the readout matrix Wro, and its orthogonal complement Pro⊥ .
113	16	This representation explicitly divides the hidden state dynamics into a 27-dimensional ‘readout’ subspace that is accessed by the readout matrix to make predictions, and a ‘computational’ subspace comprising the remaining 216− 27 dimensions that are orthogonal to the readout matrix.
117	77	This is because the norm of the projection of bx into Pro⊥ remains strongly correlated with character frequency, while the projection into Pro‖ shows little correlation.
118	47	This indicates that the information content or ’surprise’ of a letter is encoded through the norm of the component of bx in the computational space, rather than in the readout space.
131	55	To show the possibility of complete interpretability of the ISAN we train a model on a parenthesis counting task.
132	41	Bringing together ideas from section 3.5 we re-express the transition dynamics in a new basis that fully reveals performed computations.
134	73	Briefly, a 35-unit ISAN is required to keep track of the nesting level of 2 different types of parentheses independently.
135	175	The inputs are the one-hot encoding of the different opening and closing parentheses (e.g. ‘(’, ‘)’, ‘[’, ‘]’) as well as a noise character (‘a’).
136	19	The output is the one-hot encoding of the nesting level between (0-5), one set of counts for each parenthesis type (so the complete output vector is a 12 dimensional 2-hot vector).
139	57	One change from (Collins et al., 2016) is that we exchange the cross-entropy error with an L2 error.
140	37	This leads to slightly cleaner figures, but does not qualitatively change the results.
