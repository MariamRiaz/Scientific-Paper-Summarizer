6	45	A natural approach in this setting is to first learn a surrogate function from samples, and then optimize it, hoping that the estimated optimum will be close to the true one.
7	21	A recent line of work has been devoted to this setting of optimization from samples (OPS) (Balkanski et al., 2016; 2017).
8	19	The main result of OPS is unfortunately discouraging: for maximizing a submodular function under a cardinality constraint, no algorithm can obtain a constant factor approximation guarantee given polynomially-many samples from any distribution (Balkanski et al., 2017).
9	11	Thus, optimizing over learned surrogates does not provide any meaningful guarantees with respect to the true function.
10	13	The hardness of OPS is, however, a worst-case result.
11	31	The hardness stems from the discrepancy between how the algorithm gains access to information (via samples) and how it is evaluated (globally).
13	34	In this paper, we build on this motivation and propose an alternative framework for optimizing from samples.
15	15	In general, a function class F is in α-DOPS if an α-approximation of the empirical argmax can be found with arbitrarily high probability using polynomially many samples, for any distribution D and for any f ∈ F .
16	57	Let F = {f : 2[n] → R+} be a class of set functions over n elements.
17	54	We say that F is α-distributionally optimizable from samples if there is an algorithm A that, for every distribution D over 2[n], every f ∈ F , and every , δ ∈ [0, 1], when A is given as input a sample set S = {(Si, f(Si))}Mi=1 where Si iid∼ D, with probability of at least 1− δ over S it holds that: PT ∼Dm [ f ( A(T ) ) ≥ 1 α max S∈T f(S) ] ≥ 1− (1) where T = {(Sj)}mj=1, A(T ) ∈ T is the output of the algorithm, and S is of size M ∈ poly(n,m, 1/ , 1/δ, α).
18	23	(1) relaxes the OPS objective to hold in expectation over D. This is achieved by replacing the entire combinatorial domain with a sampled subset T of size m, allowing for a distribution-agnostic notion of approximation.
21	25	In general, classic approximation results do not necessarily transfer to statistical settings (Balkanski et al., 2017).
22	13	Nonetheless, our main theoretical result establishes a tight equivalence between DOPS and PMAC learning (Balcan & Harvey, 2011), meaning that any F that is learnable is also optimizable, and vice versa.
23	75	This demonstrates an intriguing link between learning and optimizing submodular functions, which are known to be PMAC-learnable (Balcan & Harvey, 2011).
24	14	The equivalence result is constructive, and gives a general optimization algorithm which can utilize any PMAC learner as a black box for DOPS, and vice versa.
25	22	While our main focus in this paper is on submodular functions, these results hold for any family of combinatorial functions.
26	20	In practice, however, optimizing via PMAC algorithms has several drawbacks (Balcan & Harvey, 2011; Feldman & Kothari, 2014; Feldman & Vondrak, 2016).
27	17	Our second goal in this paper is hence to design an efficient and scalable DOPS algorithm for several classes of interest.
28	27	Our algorithm optimizes a loss function whose minimization provides a sufficient condition for DOPS.
30	31	In this sense, the framework we propose is one in which the algorithm “learns to optimize”.
31	70	We show how the loss can be minimized efficiently and with guarantees for several submodular function classes, including coverage functions, cut functions, and unit demand.
32	24	An additional benefit of our approach is that it provides guarantees even when the output of the algorithm is restricted to a set of sampled alternatives.
33	52	This setting is especially prevalent in cases where both sets and their values are generated by human users.
35	94	However, targeting arbitrary subsets of users is in most cases impossible, and the algorithm must choose between the sets of users sharing currently trending items.
36	11	In the last part of the paper we demonstrate the empirical utility of our approach on this task using real data from Twitter.
37	27	In this section we give a tight characterization of function classes in DOPS by showing that a class F is in DOPS if and only if it is PMAC-learnable.
49	10	A PMAC algorithm learns a surrogate function f̃ .
52	19	Intuitively, the sample complexity is exactly the number of samples that are needed so that, with high probability, f̃ obtains a good approximation on all S ∈ T .
55	106	Let f ∈ F , D be some distribution, S = {(Si, f(Si))}Mi=1 and T = {Si}mi=1 be the train and test sets, andA be an algorithm that constructs f̃ which α-PMAC learns f with sample complexity MPMAC(n, δ, , α).
57	76	By the definition of α-PMAC, we get that with probability 1− δ over S, Pr S∼D [ f̃(S) ≤ f(S) ≤ α · f̃(S) ] ≥ (1− )1/m.
