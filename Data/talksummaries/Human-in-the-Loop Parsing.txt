2	53	Even relatively large datasets, such as the Penn Treebank, are much smaller than required—as demonstrated by improvements from semi-supervised learning (Søgaard and Rishøj, 2010; Weiss et al., 2015).
3	12	We take a step towards cheap, reliable annotations by introducing human-in-the-loop parsing, where non-experts improve parsing accuracy by answering questions automatically generated from the parser’s output.
4	27	We develop the approach for CCG parsing, leveraging the link between CCG syntax and semantics to convert uncertain attachment decisions into natural language questions.
5	54	The answers are used as soft constraints when re-parsing the sentence.
7	13	Our work is most related to that of Duan et al. (2016), which automatically generates paraphrases from n-best parses and gained significant improvement by re-training from crowdsourced judgments on two out-of-domain datasets.
8	57	Choe and McClosky (2015) improve a parser by creating paraphrases of sentences, and then parsing the sentence and its paraphrase jointly.
18	14	Each QA pair corresponds to a dependency such that if the answer is correct, it indicates that the dependency is in the correct parse.
21	45	CCG parsing assigns dependencies to each argument position, even when the arguments are reordered (as with put→ pizza) or span long distances (as with eat→ I).
27	40	Each pool becomes a query, and for each unique dependency used to generate QA pairs in that pool, we add a candidate answer to the query by choosing the answer phrase that has the highest marginalized score for that dependency.
29	25	From the resulting queries, we filter out questions and answers whose marginalized scores are below a certain threshold and queries that only have one answer choice.
33	10	There was also a None of the above option for when no answer was applicable or the question was nonsensical.
34	27	We instructed annotators to only choose options that explicitly and directly answer the question, to encourage their answers to closely mirror syntax.
38	16	Dataset Statistics Table 3 shows how many sentences we asked questions for and the total number of queries annotated.
39	21	We collected annotations for the development and test set for CCGbank (Hockenmaier and Steedman, 2007) as in-domain data and the test set of the Bioinfer corpus (Pyysalo et al., 2007) as out-of-domain.
43	27	Annotators unanimously chose the same set of answers for over 40% of the queries; an absolute majority is achieved for over 90% of the queries.
44	23	Qualitative Analysis Table 2 shows example queries from the CCGbank development set.
51	27	Another common error case involved partitives and related constructions, where the correct attachment is subtle—as reflected by the annotators’ split decision in Example 7.
57	19	Common mistakes in question generation include: bad argument span in a copula question (4 questions), bad modality/negation (3 questions), and missing argument or particle (5 questions).
65	48	If q is a question, a is an answer to q, d is the dependency that produced the QA pair 〈q, a〉, and v(a) annotators chose a, we add re-parsing constraints as follows: • If v(None of the above) ≥ T+, penalize parses that agree with q’s supertag on the verb by wt • If v(a) ≤ T−, penalize parses containing d by w− • If v(a) ≥ T+, penalize parses that do not contain d by w+ where T+, T−, wt, w−, and w+ are hyperparameters.
66	15	We incorporate these penalties into the parsing model during decoding.
69	86	If a is a subspan of another answer a′ and their votes differ by at most one (See Example 7 in Table 2), it is unlikely that both a and a′ are correct.
77	19	There was much larger improvement (1.7 F1) on the subset of sentences that are changed after reparsing, as shown in Table 7.
78	91	This suggests that our method could be effective for semi-supervised learning or re-training parsers.
79	34	Overall improvements on CCGbank are modest, due to only modifying 10% of sentences.
81	60	Our method identifies attachment uncertainty for core arguments of verbs and automatically generates questions that can be answered by untrained annotators.
82	10	These annotations improve performance, particularly on out-of-domain data, demonstrating for the first time that untrained annotators can improve state-of-the-art parsers.
83	36	Sentences modified by our framework show substantial improvements in accuracy, but only 10% of sentences are changed, limiting the effect on overall accuracy.
84	23	This work is a first step towards a complete approach to human-in-the-loop parsing.
85	32	Future work will explore the possibility of asking questions about other types of parsing uncertainties, such as nominal and adjectival argument structure, and a more thorough treatment of prepositionalphrase attachment, including distinctions between arguments and adjuncts.
86	52	We hope to scale these methods to large unlabelled corpora or other languages, to provide data for re-training parsers.
