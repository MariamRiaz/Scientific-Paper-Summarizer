{"sections": [{"heading": "1. Introduction", "text": "Support vector machines (SVMs) and Boosting have been two mainstream learning approaches during the past decade. The former (Cortes & Vapnik, 1995) roots in the statistical learning theory (Vapnik, 1995) with the central idea of searching a large margin separator, i.e., maximizing the smallest distance from the instances to the classification boundary in a RKHS (reproducing kernel Hilbert space). It is noteworthy that there is also a long history of applying margin theory to explain the latter (Freund & Schapire, 1995; Schapire et al., 1998), due to its tending to be empirically resistant to over-fitting (Reyzin & Schapire, 2006; Wang et al., 2011; Zhou, 2012).\n1National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China. Correspondence to: Zhi-Hua Zhou <zhouzh@lamda.nju.edu.cn>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nRecently, the margin theory for Boosting has finally been defended (Gao & Zhou, 2013), and has disclosed that the margin distribution rather than a single margin is more crucial to the generalization performance. It suggests that there may still exist large space to further enhance for SVMs. Inspired by this recognition, (Zhang & Zhou, 2014; 2016) proposed a binary classification method to optimize margin distribution by characterizing it through the firstand second-order statistics, which achieves quite satisfactory experimental results. Later, (Zhou & Zhou, 2016) extends the idea to an approach which is able to exploit unlabeled data and handle unequal misclassification cost. A brief summary of this line of early research can be found in (Zhou, 2014).\nAlthough it has been shown that for binary classification, optimizing the margin distribution by maximizing the margin mean and minimizing the margin variance simultaneously can get superior performance, it still remains open for multi-class classification. Moreover, the margin for multiclass classification is much more complicated than that for binary class classification, which makes the resultant optimization be a difficult non-differentiable non-convex programming. In this paper, we propose mcODM (multi-class Optimal margin Distribution Machine) to solve this problem efficiently. For optimization, we relax mcODM into a series of convex quadratic programming (QP), and extend the Block Coordinate Descent (BCD) algorithm (Tseng, 2001) to solve the dual of each QP. The sub-problem of each iteration of BCD is also a QP. By exploiting its special structure, we derive a sorting algorithm to solve it which is much faster than general QP solvers. We further provide a generalization error bound based on Rademacher complexity, and further present the analysis of the relationship between generalization error and margin distribution for multi-class classification. Extensive experiments on twenty two data sets show the superiority of our method to all four versions of multi-class SVMs.\nThe rest of this paper is organized as follows. Section 2 introduces some preliminaries. Section 3 formulates the problem. Section 4 presents the proposed algorithm. Section 5 discusses some theoretical analyses. Section 6 reports on our experimental studies and empirical observations. Finally Section 7 concludes with future work."}, {"heading": "2. Preliminaries", "text": "We denote by X \u2208 Rd the instance space and Y = [k] the label set, where [k] = {1, . . . , k}. Let D be an unknown (underlying) distribution over X \u00d7 Y . A training set S = {(x1, y1), (x2, y2), . . . , (xm, ym)} \u2208 (X \u00d7 Y)m is drawn identically and independently (i.i.d.) according to distribution D. Let \u03d5 : X 7\u2192 H be a feature mapping associated to some positive definite kernel \u03ba. For multi-class classification setting, the hypothesis is defined based on k weight vectors w1, . . . ,wk \u2208 H, where each weight vector wl, l \u2208 Y defines a scoring function x 7\u2192 w\u22a4l \u03d5(x) and the label of instance x is the one resulting in the largest score, i.e., h(x) = argmaxl\u2208Y w \u22a4 l \u03d5(x). This decision function naturally leads to the following definition of the margin for a labeled instance (x, y):\n\u03b3h(x, y) = w \u22a4 y \u03d5(x)\u2212max l \u0338=y w\u22a4l \u03d5(x).\nThus h misclassifies (x, y) if and only if it produces a negative margin for this instance.\nGiven a hypothesis set H of functions mapping X to Y and the labeled training set S, our goal is to learn a function h \u2208 H such that the generalization error R(h) = E(x,y)\u223cD[1h(x)\u0338=y] is small."}, {"heading": "3. Formulation", "text": "To design optimal margin distribution machine for multiclass classification, we need to understand how to optimize the margin distribution. (Gao & Zhou, 2013) proved that, to characterize the margin distribution, it is important to consider not only the margin mean but also the margin variance. Inspired by this idea, (Zhang & Zhou, 2014; 2016) proposed the optimal margin distribution machine for binary classification, which characterizes the margin distribution according to the first- and second-order statistics, that is, maximizing the margin mean and minimizing the margin variance simultaneously. Specifically, let \u03b3\u0304 denote the margin mean, and the optimal margin distribution machine can be formulated as:\nmin w,\u03b3\u0304,\u03bei,\u03f5i \u2126(w)\u2212 \u03b7\u03b3\u0304 + \u03bb m m\u2211 i=1 (\u03be2i + \u03f5 2 i )\ns.t. \u03b3h(xi, yi) \u2265 \u03b3\u0304 \u2212 \u03bei, \u03b3h(xi, yi) \u2264 \u03b3\u0304 + \u03f5i, \u2200i,\nwhere \u2126(w) is the regularization term to penalize the model complexity, \u03b7 and \u03bb are trading-off parameters, \u03bei and \u03f5i are the deviation of the margin \u03b3h(xi, yi) to the margin mean. It\u2019s evident that \u2211m i=1(\u03be 2 i + \u03f5 2 i )/m is exactly the margin variance.\nBy scaling w which doesn\u2019t affect the final classification results, the margin mean can be fixed as 1, then the de-\nviation of the margin of (xi, yi) to the margin mean is |\u03b3h(xi, yi) \u2212 1|, and the optimal margin distribution machine can be reformulated as\nmin w,\u03bei,\u03f5i\n\u2126(w) + \u03bb\nm m\u2211 i=1 \u03be2i + \u00b5\u03f5 2 i (1\u2212 \u03b8)2\ns.t. \u03b3h(xi, yi) \u2265 1\u2212 \u03b8 \u2212 \u03bei, \u03b3h(xi, yi) \u2264 1 + \u03b8 + \u03f5i, \u2200i.\nwhere \u00b5 \u2208 (0, 1] is a parameter to trade off two different kinds of deviation (larger or less than margin mean). \u03b8 \u2208 [0, 1) is a parameter of the zero loss band, which can control the number of support vectors, i.e., the sparsity of the solution, and (1\u2212 \u03b8)2 in the denominator is to scale the second term to be a surrogate loss for 0-1 loss.\nFor multi-class classification, let the regularization term \u2126(w) = \u2211k l=1 \u2225wl\u22252H/2 and combine with the definition of margin, and we arrive at the formulation of mcODM,\nmin wl,\u03bei,\u03f5i\n1\n2 k\u2211 l=1 \u2225wl\u22252H + \u03bb m m\u2211 i=1 \u03be2i + \u00b5\u03f5 2 i (1\u2212 \u03b8)2 (1)\ns.t. w\u22a4yi\u03d5(xi)\u2212maxl \u0338=yi w\u22a4l \u03d5(xi) \u2265 1\u2212 \u03b8 \u2212 \u03bei,\nw\u22a4yi\u03d5(xi)\u2212maxl \u0338=yi w\u22a4l \u03d5(xi) \u2264 1 + \u03b8 + \u03f5i, \u2200i.\nwhere \u03bb, \u00b5 and \u03b8 are the parameters for trading-off described previously."}, {"heading": "4. Optimization", "text": "Due to the max operator in the second constraint, mcODM is a non-differentiable non-convex programming, which is quite difficult to solve directly.\nIn this section, we first relax mcODM into a series of convex quadratic programming (QP), which can be much easier to handle. Specifically, at each iteration, we recast the first constraint as k \u2212 1 linear inequality constraints:\nw\u22a4yi\u03d5(xi)\u2212w \u22a4 l \u03d5(xi) \u2265 1\u2212 \u03b8 \u2212 \u03bei, l \u0338= yi,\nand replace the second constraint with\nw\u22a4yi\u03d5(xi)\u2212Mi \u2264 1 + \u03b8 + \u03f5i,\nwhere Mi = maxl \u0338=yi w\u0304 \u22a4 l \u03d5(xi) and w\u0304l is the solution to the previous iteration. Then we can repeatedly solve the following convex QP problem until convergence:\nmin wl,\u03bei,\u03f5i\n1\n2 k\u2211 l=1 \u2225wl\u22252H + \u03bb m m\u2211 i=1 \u03be2i + \u00b5\u03f5 2 i (1\u2212 \u03b8)2 (2)\ns.t. w\u22a4yi\u03d5(xi)\u2212w \u22a4 l \u03d5(xi) \u2265 1\u2212 \u03b8 \u2212 \u03bei, \u2200l \u0338= yi,\nw\u22a4yi\u03d5(xi)\u2212Mi \u2264 1 + \u03b8 + \u03f5i, \u2200i.\nIntroduce the lagrange multipliers \u03b6li \u2265 0, l \u0338= yi for the first k \u2212 1 constraints and \u03b2i \u2265 0 for the last constraint respectively, the Lagrangian function of Eq. 2 leads to\nL(wl, \u03bei, \u03f5i, \u03b6 l i , \u03b2i)\n= 1\n2 k\u2211 l=1 \u2225wl\u22252H + \u03bb m m\u2211 i=1 \u03be2i + \u00b5\u03f5 2 i (1\u2212 \u03b8)2\n\u2212 m\u2211 i=1 \u2211 l \u0338=yi \u03b6li(w \u22a4 yi\u03d5(xi)\u2212w \u22a4 l \u03d5(xi)\u2212 1 + \u03b8 + \u03bei)\n+ m\u2211 i=1 \u03b2i(w \u22a4 yi\u03d5(xi)\u2212Mi \u2212 1\u2212 \u03b8 \u2212 \u03f5i),\nBy setting the partial derivations of variables {wl, \u03bei, \u03f5i} to zero, we have\nwl = m\u2211 i=1 \u03b4yi,l \u2211 s\u0338=yi \u03b6si \u2212 (1\u2212 \u03b4yi,l)\u03b6li \u2212 \u03b4yi,l\u03b2i \u03d5(xi), \u03bei = m(1\u2212 \u03b8)2\n2\u03bb\n\u2211 l \u0338=yi \u03b6li , \u03f5i = m(1\u2212 \u03b8)2 2\u03bb\u00b5 \u03b2i. (3)\nwhere \u03b4yi,l equals 1 when yi = l and 0 otherwise. We further simplify the expression of wl as\nwl = m\u2211 i=1 (\u03b1li \u2212 \u03b4yi,l\u03b2i)\u03d5(xi), (4)\nby defining \u03b1li \u2261 \u2212\u03b6li for \u2200l \u0338= yi and \u03b1 yi i \u2261 \u2211 s\u0338=yi\n\u03b6si and substituting Eq. 4 and Eq. 3 into the Lagrangian function, then we have the following dual problem\nmin \u03b1li,\u03b1 yi i ,\u03b2i\n1\n2 k\u2211 l=1 \u2225wl\u22252H + m(1\u2212 \u03b8)2 4\u03bb m\u2211 i=1 (\u03b1yii ) 2\n+ m(1\u2212 \u03b8)2\n4\u03bb\u00b5\nm\u2211 i=1 \u03b22i + (1\u2212 \u03b8) m\u2211 i=1 \u2211 l \u0338=yi \u03b1li\n+ (Mi + 1 + \u03b8) m\u2211 i=1 \u03b2i (5)\ns.t. k\u2211\nl=1\n\u03b1li = 0, \u2200i,\n\u03b1li \u2264 0, \u2200i,\u2200l \u0338= yi, \u03b2i \u2265 0, \u2200i.\nThe objective function in Eq. 5 involves m(k + 1) variables in total, so it is not easy to optimize with respect to all the variables simultaneously. Note that all the constraints can be partitioned into m disjoint sets, and the i-th set only involves \u03b11i , . . . , \u03b1 k i , \u03b2i, so the variables can be divided into m decoupled groups and an efficient block coordinate descent algorithm (Tseng, 2001) can be applied.\nSpecifically, we sequentially select a group of k + 1 variables \u03b11i , . . . , \u03b1 k i , \u03b2i associated with instance xi to minimize, while keeping other variables as constants, and repeat this procedure until convergence.\nAlgorithm 1 below details the kenrel mcODM.\nAlgorithm 1 Kenrel mcODM 1: Input: Data set S. 2: Initialize \u03b1\u22a4 = [\u03b111, . . . , \u03b1 k 1 , . . . , \u03b1 1 m, . . . , \u03b1 k m] and\n\u03b2\u22a4 = [\u03b21, . . . , \u03b2m] as zero vector. 3: while \u03b1 and \u03b2 not converge do 4: for i = 1, . . . ,m do 5: Mi \u2190 maxl \u0338=yi \u2211m j=1(\u03b1 l j \u2212 \u03b4yj ,l\u03b2j)\u03ba(xj ,xi). 6: end for 7: Solve Eq. 5 by block coordinate descent method. 8: end while 9: Output: \u03b1, \u03b2."}, {"heading": "4.1. Solving the sub-problem", "text": "The sub-problem in step 7 of Algorithm 1 is also a convex QP with k + 1 variables, which can be accomplished by some standard QP solvers. However, by exploiting its special structure, i.e., only a small quantity of cross terms are involved, we can derive an algorithm to solve this subproblem just by sorting, which can be much faster than general QP solvers.\nNote that all variables except \u03b11i , . . . , \u03b1 k i , \u03b2i are fixed, so we have the following sub-problem:\nmin \u03b1li,\u03b1 yi i ,\u03b2i \u2211 l \u0338=yi A 2 (\u03b1li) 2 + \u2211 l \u0338=yi Bl\u03b1 l i + D 2 (\u03b1yii ) 2 \u2212A\u03b1yii \u03b2i\n+Byi\u03b1 yi i +\nE 2 \u03b22i + F\u03b2i\ns.t. k\u2211\nl=1\n\u03b1li = 0, (6)\n\u03b1li \u2264 0, \u2200l \u0338= yi, \u03b2i \u2265 0.\nwhere A = \u03ba(xi,xi), Bl = \u2211 j \u0338=i \u03ba(xi,xj)(\u03b1 l j \u2212\n\u03b4yj ,l\u03b2j)+1\u2212 \u03b8 for \u2200l \u0338= yi, Byi = \u2211 j \u0338=i \u03ba(xi,xj)(\u03b1 yi j \u2212 \u03b4yj ,yi\u03b2j), D = A + m(1\u2212\u03b8)2 2\u03bb , E = A + m(1\u2212\u03b8)2\n2\u03bb\u00b5 and F \u2261Mi + 1 + \u03b8 \u2212Byi .\nThe KKT conditions of Eq. 6 indicate that there are scalars \u03bd, \u03c1l and \u03b7 such that\nk\u2211 l=1 \u03b1li = 0, (7) \u03b1li \u2264 0, \u2200l \u0338= yi, (8)\n\u03b2i \u2265 0, (9) \u03c1l\u03b1 l i = 0, \u03c1l \u2265 0, \u2200l \u0338= yi, (10)\nA\u03b1li +Bl \u2212 \u03bd + \u03c1l = 0, \u2200l \u0338= yi, (11) \u03b7\u03b2i = 0, \u03b7 \u2265 0, (12) \u2212A\u03b1yii + E\u03b2i + F \u2212 \u03b7 = 0, (13) D\u03b1yii \u2212A\u03b2i +Byi \u2212 \u03bd = 0. (14)\nAccording to Eq. 8, Eq. 10 and Eq. 11 are equivalent to\nA\u03b1li +Bl \u2212 \u03bd = 0, if \u03b1li < 0, \u2200l \u0338= yi, (15) Bl \u2212 \u03bd \u2264 0, if \u03b1li = 0, \u2200l \u0338= yi. (16)\nIn the same way, Eq. 12 and Eq. 13 are equivalent to\n\u2212A\u03b1yii + E\u03b2i + F = 0, if \u03b2i > 0, (17) \u2212A\u03b1yii + F \u2265 0, if \u03b2i = 0. (18)\nThus KKT conditions turn to Eq. 7 - Eq. 9 and Eq. 14 - Eq. 18. Note that\n\u03b1li \u2261 min ( 0,\n\u03bd \u2212Bl A\n) , \u2200l \u0338= yi, (19)\nsatisfies KKT conditions Eq. 8 and Eq. 15 - Eq. 16 and \u03b2i \u2261 max ( 0,\nA\u03b1yii \u2212 F E\n) , (20)\nsatisfies KKT conditions Eq. 9 and Eq. 17 - Eq. 18. By substituting Eq. 20 into Eq. 14, we obtain\nD\u03b1yii +Byi \u2212 \u03bd = max ( 0, A\nE (A\u03b1yii \u2212 F )\n) . (21)\nLet\u2019s consider the following two cases in turn.\nCase 1: A\u03b1yii \u2264 F , according to Eq. 20 and Eq. 21, we have \u03b2i = 0 and \u03b1 yi i = \u03bd\u2212Byi D . Thus, A \u03bd\u2212Byi D \u2264 F , which implies that \u03bd \u2264 Byi + DFA . Case 2: A\u03b1yii > F , according to Eq. 20 and Eq. 21, we have \u03b2i = A\u03b1 yi i \u2212F E > 0 and \u03b1 yi i = E\u03bd\u2212AF\u2212EByi DE\u2212A2 . Thus, A E\u03bd\u2212AF\u2212EByi\nDE\u2212A2 > F , which implies that \u03bd > Byi + DF A .\nThe remaining task is to find \u03bd such that Eq. 7 holds. With Eq. 7 and Eq. 19, it can be shown that\n\u03bd =\nAByi D + \u2211 l:\u03b1li<0\nBl A D + |{l|\u03b1 l i < 0}| , Case 1, (22)\n\u03bd =\nAEByi+A 2F DE\u2212A2 + \u2211 l:\u03b1li<0 Bl\nAE DE\u2212A2 + |{l|\u03b1 l i < 0}|\n, Case 2. (23)\nIn both cases, the optimal \u03bd takes the form of (P +\u2211 l:\u03b1li<0 Bl)/(Q+ |{l|\u03b1li < 0}|), where P and Q are some\nconstants. (Fan et al., 2008) showed that it can be found by sorting {Bl} for \u2200l \u0338= yi in decreasing order and then sequentially adding them into an empty set \u03a6, until\n\u03bd\u2217 = P +\n\u2211 l\u2208\u03a6 Bl\nQ+ |\u03a6| \u2265 max l \u0338\u2208\u03a6 Bl. (24)\nNote that the Hessian matrix of the objective function of Eq. 6 is positive definite, which guarantees the existence and uniqueness of the optimal solution, so only one of Eq. 22 and Eq. 23 can hold. We can first compute \u03bd\u2217 according to Eq. 24 for Case 1, and then check whether the constraint of \u03bd is satisfied. If not, we further compute \u03bd\u2217 for Case 2. Algorithm 2 summarizes the pseudo-code for solving the sub-problem.\nAlgorithm 2 Solving the sub-problem 1: Input: Parameters A, B = {B1, . . . , Bk}, D,E, F . 2: Initialize B\u0302 \u2190 B, then swap B\u03021 and B\u0302yi , and sort\nB\u0302\\{B\u03021} in decreasing order. 3: i\u2190 2, \u03bd \u2190 AByi/D. 4: while i \u2264 k and \u03bd/(i\u2212 2 +A/D) < B\u0302i do 5: \u03bd \u2190 \u03bd + B\u0302i. 6: i\u2190 i+ 1. 7: end while 8: if \u03bd \u2264 Byi +DF/A then 9: \u03b1li \u2190 min(0, (\u03bd \u2212Bl)/A), l \u0338= yi.\n10: \u03b1yii \u2190 (\u03bd \u2212Byi)/D. 11: \u03b2i \u2190 0. 12: else 13: i\u2190 2, \u03bd \u2190 (AEB\u03021 +A2F )/(DE \u2212A2). 14: while i \u2264 k and \u03bd/(i\u2212 2+AE/(DE\u2212A2)) < B\u0302i do 15: \u03bd \u2190 \u03bd + B\u0302i. 16: i\u2190 i+ 1. 17: end while 18: \u03b1li \u2190 min(0, (\u03bd \u2212Bl)/A), l \u0338= yi. 19: \u03b1yii \u2190 (E\u03bd \u2212AF \u2212 EByi)/(DE \u2212A2). 20: \u03b2i \u2190 (A\u03b1yii \u2212 F )/E. 21: end if 22: Output: \u03b11i , . . . , \u03b1ki , \u03b2i."}, {"heading": "4.2. Speedup for linear kernel", "text": "In section 4.1, the proposed method can efficiently deal with kernel mcODM. However, the computation of Mi in step 5 of Algorithm 1 and the computation of parameters B\u0304l in Algorithm 2 both involve the kernel matrix, whose inherent computational cost takes O(m2) time, so it might be computational prohibitive for large scale problems.\nWhen linear kernel is used, these problems can be alleviated. According to Eq. 4, w is spanned by the training instance so it lies in a finite dimensional space under this\ncircumstance. By storing w1, . . . ,wk explicitly, the computational cost of Mi = maxl \u0338=yi w \u22a4 l xi can be much less. Moreover, note that B\u0304l = \u2211 j \u0338=i x \u22a4 i xj(\u03b1\nl j \u2212 \u03b4yj ,l\u03b2j) =\u2211m\nj=1 x \u22a4 i xj(\u03b1\u0304 l j\u2212\u03b4yj ,l\u03b2\u0304j)\u2212x\u22a4i xi(\u03b1\u0304li\u2212\u03b4yi,l\u03b2\u0304i) = w\u22a4l xi\u2212\nA(\u03b1li \u2212 \u03b4yi,l\u03b2i), so B\u0304l can also be computed efficiently."}, {"heading": "5. Analysis", "text": "In this section, we study the statistical property of mcODM. To present the generalization bound of mcODM, we need to introduce the following loss function \u03a6,\n\u03a6(z) = 1z\u22640 + (z \u2212 1 + \u03b8)2\n(1\u2212 \u03b8)2 10<z\u22641\u2212\u03b8,\n\u03b3h,\u03b8(x, y) = w \u22a4 y \u03d5(x)\u2212max l\u2208Y {w\u22a4l \u03d5(x)\u2212 (1\u2212 \u03b8)1l=y},\nwhere 1(\u00b7) is the indicator function that returns 1 when the argument holds, and 0 otherwise. As can be seen, \u03b3h,\u03b8(x, y) is a lower bound of \u03b3h(x, y) and \u03a6(\u03b3h,\u03b8(x, y)) = \u03a6(\u03b3h(x, y)).\nTheorem 1. Let H = {(x, y) \u2208 X \u00d7 [k] 7\u2192 w\u22a4y \u03d5(x)| \u2211k l=1 \u2225wl\u22252H \u2264 \u039b2} be the hypothesis space of mcODM, where \u03d5 : X 7\u2192 H is a feature mapping induced by some positive definite kernel \u03ba. Assume that S \u2286 {x : \u03ba(x,x) \u2264 r2}, then for any \u03b4 > 0, with probability at least 1 \u2212 \u03b4, the following generalization bound holds for any h \u2208 H ,\nR(h) \u2264 1 m m\u2211 i=1 \u03a6(\u03b3h(xi, yi)) + 16r\u039b 1\u2212 \u03b8\n\u221a 2\u03c0k\nm + 3 \u221a ln 2\u03b4 2m .\nProof. Let H\u0303\u03b8 be the family of hypotheses mapping X \u00d7 Y 7\u2192 R defined by H\u0303\u03b8 = {(x, y) 7\u2192 \u03b3h,\u03b8(x, y) : h \u2208 H}, with McDiarmid inequality (McDiarmid, 1989), yields the following inequality with probability at least 1\u2212 \u03b4,\nE[\u03a6(\u03b3h,\u03b8(x, y))] \u2264 1\nm m\u2211 i=1 \u03a6(\u03b3h,\u03b8(xi, yi))\n+ 2RS(\u03a6 \u25e6 H\u0303\u03b8) + 3 \u221a ln 2\u03b4 2m ,\u2200h \u2208 H\u0303\u03b8.\nNote that \u03a6(\u03b3h,\u03b8) = \u03a6(\u03b3h), R(h) = E[1\u03b3h(x,y)\u22640] \u2264 E[1\u03b3h,\u03b8(x,y)\u22640] \u2264 E[\u03a6(\u03b3h,\u03b8(x, y))] and \u03a6(z) is 21\u2212\u03b8 - Lipschitz function, by using Talagrand\u2019s lemma (Mohri et al., 2012), we have\nR(h) \u2264 1 m m\u2211 i=1 \u03a6(\u03b3h(xi, yi)) + 4RS(H\u0303\u03b8) 1\u2212 \u03b8 + 3 \u221a ln 2\u03b4 2m .\nAccording to Theorem 7 of (Lei et al., 2015), we have RS(H\u0303\u03b8) \u2264 4r\u039b \u221a 2\u03c0k/m and proves the stated result.\nTheorem 1 shows that we can get a tighter generalization bound for smaller r\u039b and smaller \u03b8. Since \u03b3 \u2264 2r\u039b, so the former can be viewed as an upper bound of the margin. Besides, 1 \u2212 \u03b8 is the lower bound of the zero loss band of mcODM. This verifies that better margin distribution (i.e., larger margin mean and smaller margin variance) can yield better generalization performance, which is also consistent with the work of (Gao & Zhou, 2013)."}, {"heading": "6. Empirical Study", "text": "In this section, we empirically evaluate the effectiveness of our method on a broad range of data sets. We first introduce the experimental settings and compared methods in Section 6.1, and then in Section 6.2, we compare our method with four versions of multi-class SVMs, i.e., mcSVM (Weston & Watkins, 1999; Crammer & Singer, 2001; 2002), one-versus-all SVM (ovaSVM), one-versusone SVM (ovoSVM) (Ulrich, 1998) and error-correcting output code SVM (ecocSVM) (Dietterich & Bakiri, 1995). In addition, we also study the influence of the number of classes on generalization performance and margin distribution in Section 6.3. Finally, the computational cost is presented in Section 6.4."}, {"heading": "6.1. Experimental Setup", "text": "We evaluate the effectiveness of our proposed methods on twenty two data sets. Table 1 summarizes the statistics of these data sets. The data set size ranges from 150 to more than 581,012, and the dimensionality ranges from 4 to more than 62,061. Moreover, the number of class ranges from 3 to 1,000, so these data sets cover a broad range of properties. All features are normalized into the interval [0, 1]. For each data set, eighty percent of the instances are randomly selected as training data, and the rest are used as testing data. For each data set, experiments are repeated for 10 times with random data partitions, and the average accuracies as well as the standard deviations are recorded.\nmcODM is compared with four versions of multi-class SVMs, i.e., ovaSVM, ovoSVM, ecocSVM and mcSVM. These four methods can be roughly classified into two groups. The first group includes the first three methods by converting the multi-class classification problem into a set of binary classification problems. Specially, ovaSVM consists of learning k scoring functions hl : X 7\u2192 {\u22121,+1}, l \u2208 Y , each seeking to discriminate one class l \u2208 Y from all the others, as can be seen it need train k SVM models. Alternatively, ovoSVM determines the scoring functions for all the combinations of class pairs, so it need train k(k \u2212 1)/2 SVM models. Finally, ecocSVM is a generalization of the former two methods. This technique assigns to each class l \u2208 Y a code word with length c, which serves as a signature for this class. After training\nc binary SVM models h1(\u00b7), . . . , hc(\u00b7), the class predicted for each testing instance is the one whose signatures is the closest to [h1(x), . . . , hc(x)] in Hamming distance. The weakness of these methods is that they may produce unclassifiable regions and their computational costs are usually quite large in practice, which can be observed in the following experiments. On the other hand, mcSVM belongs to the second group. It directly determines all the scroing functions at once, so the time cost is usually less than the former methods. In addition, the unclassifiable regions are also resolved.\nFor all the methods, the regularization parameter \u03bb for mcODM or C for binary SVM and mcSVM is selected by 5-fold cross validation from [20, 22, . . . , 220]. For mcODM, the regularization parameters \u00b5 and \u03b8 are both selected from [0.2, 0.4, 0.6, 0.8]. For ecocSVM, the exhaustive codes strategy is employed, i.e., for each class, we construct a code of length 2k\u22121 \u2212 1 as the signature. All the selections for parameters are performed on training sets."}, {"heading": "6.2. Results", "text": "Table 2 summarizes the detailed results on twenty two data sets. As can be seen, the overall performance of our method is superior or highly competitive to the other compared methods. Specifically, mcODM performs significantly better than mcSVM/ovaSVM/ovoSVM/ecocSVM on 17/19/18/17 over 22 data sets respectively, and achieves the best accuracy on 20 data sets. In addition, as can be seen, in comparing with other four methods which don\u2019t consider margin distribution, the win/tie/loss counts show that mcODM is always better or comparable, almost never worse than it."}, {"heading": "6.3. Influence of the Number of Classes", "text": "In this section we study the influence of the number of classes on generalization performance and margin distribution, respectively."}, {"heading": "6.3.1. GENERALIZATION PERFORMANCE", "text": "Figure 1 plots the generalization performance of all the five methods on data set segment, and similar observation can be found for other data sets. As can be seen, when the number of classes is less than four, all methods perform quite well. However, as the fifth class is added, the generalization performance of other four methods decreases drastically. This might be attributable to the introduction of some noisy data, which SVM-style algorithms are very sensitive to since they optimize the minimum margin. On the other hand, our method considers the whole margin distribution, so it can be robust to noise and relatively more stable."}, {"heading": "6.3.2. MARGIN DISTRIBUTION", "text": "Figure 2 plots the frequency histogram of margin distribution produced by mcSVM, ovaSVM and mcODM on data set segment as the number of classes increases from two to seven. As can be seen, when the number of classes is less than four, all methods can achieve good margin distribution, whereas with the increase of the number of classes, the other two methods begin to produce negative margins. At the same time, the distribution of our method becomes\n\u201csharper\u201d, which prevents the instance with small margin, so our method can still perform relatively well as the number of classes increases, which is also consistent with the observation from Figure 1."}, {"heading": "6.4. Time Cost", "text": "We compare the single iteration time cost of our method with mcSVM, ovaSVM, ovoSVM on all the data sets except aloi, on which ovoSVM could not return results in 48 hours. All the experiments are performed with MATLAB 2012b on a machine with 8\u00d72.60 GHz CPUs and 32GB main memory. The average CPU time (in seconds) on each data set is shown in Figure 3. The binary SVM used in ovaSVM, ovoSVM and mcSVM are both implemented by the LIBLINEAR (Fan et al., 2008) package. It can be seen that for small data sets, the efficiency of all the methods are similar, however, for data sets with more than ten classes, e.g., sector and rcv1, mcSVM and mcODM, which learn all the scroing functions at once, are much faster than ovaSVM and ovoSVM, owing to the inefficiency of binarydecomposition as discussed in Section 6.1. Note that LIBLINEAR are very fast implementations of binary SVM and mcSVM, and this shows that our method is also computationally efficient."}, {"heading": "7. Conclusions", "text": "Recent studies disclose that for binary class classification, maximizing the minimum margin does not necessarily lead to better generalization performances, and instead, it is crucial to optimize the margin distribution. However, it remains open to the influence of margin distribution for multi-class classification. We try to answer this question in this paper. After maximizing the margin mean and minimizing the margin variance simultaneously, the resultant optimization is a difficult non-differentiable non-convex programming. We propose mcODM to solve this problem efficiently. Extensive experiments on twenty two data sets validate the superiority of our method to four versions of multi-class SVMs. In the future it will be interesting to extend mcODM to more general learning settings, i.e., multilabel learning and structured learning."}, {"heading": "Acknowledgements", "text": "This research was supported by the NSFC (61333014) and the Collaborative Innovation Center of Novel Software Technology and Industrialization. Authors want to thank reviewers for helpful comments, and thank Dr. Wei Gao for reading a draft."}], "year": 2017, "references": [{"title": "On the algorithmic implementation of multiclass kernel-based vector machines", "authors": ["K. Crammer", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "year": 2001}, {"title": "On the learnability and design of output codes for multiclass problems", "authors": ["K. Crammer", "Y. Singer"], "venue": "Machine Learning,", "year": 2002}, {"title": "Solving multiclass learning problems via error-correcting output codes", "authors": ["T.G. Dietterich", "G. Bakiri"], "venue": "Journal of Artificial Intelligence Research,", "year": 1995}, {"title": "Liblinear: A library for large linear classification", "authors": ["R.E. Fan", "K.W. Chang", "C.J. Hsieh", "X.R. Wang", "C.J. Lin"], "venue": "Journal of Machine Learning Research,", "year": 2008}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "authors": ["Y. Freund", "R.E. Schapire"], "venue": "In Proceedings of the 2nd European Conference on Computational Learning Theory,", "year": 1995}, {"title": "On the doubt about margin explanation of boosting", "authors": ["W. Gao", "Zhou", "Z.-H"], "venue": "Artificial Intelligence,", "year": 2013}, {"title": "On the method of bounded differences", "authors": ["C. McDiarmid"], "venue": "Surveys in Combinatorics,", "year": 1989}, {"title": "Foundations of Machine Learning", "authors": ["M. Mohri", "A. Rostamizadeh", "A. Talwalkar"], "year": 2012}, {"title": "How boosting the margin can also boost classifier complexity", "authors": ["L. Reyzin", "R.E. Schapire"], "venue": "In Proceedings of 23rd International Conference on Machine Learning,", "year": 2006}, {"title": "Boosting the margin: a new explanation for the effectives of voting methods", "authors": ["R.E. Schapire", "Y. Freund", "P.L. Bartlett", "W.S. Lee"], "venue": "Annuals of Statistics,", "year": 1998}, {"title": "Convergence of a block coordinate descent method for nondifferentiable minimization", "authors": ["P. Tseng"], "venue": "Journal of Optimization Theory and Applications,", "year": 2001}, {"title": "Pairwise classification and support vector machines", "authors": ["Ulrich", "H.G. Kreel"], "venue": "Advances in Kernel Methods: Support Vector Machines,", "year": 1998}, {"title": "The Nature of Statistical Learning", "authors": ["V. Vapnik"], "year": 1995}, {"title": "A refined margin analysis for boosting algorithms via equilibrium margin", "authors": ["L.W. Wang", "M. Sugiyama", "C. Yang", "Zhou", "Z.-H", "J. Feng"], "venue": "Journal of Machine Learning Research,", "year": 2011}, {"title": "Multi-class support vector machines", "authors": ["J. Weston", "C. Watkins"], "venue": "In Proceedings of the European Symposium on Artificial Neural Networks, Brussels, Belgium,", "year": 1999}, {"title": "Large margin distribution machine", "authors": ["T. Zhang", "Zhou", "Z.-H"], "venue": "In Proceedings of the 20th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,", "year": 2014}, {"title": "Optimal margin distribution machine", "authors": ["T. Zhang", "Zhou", "Z.-H"], "venue": "CoRR, abs/1604.03348,", "year": 2016}, {"title": "Large margin distribution learning with cost interval and unlabeled data", "authors": ["Zhou", "Y.-H", "Z.-H"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "year": 2016}, {"title": "Ensemble Methods: Foundations and Algorithms", "authors": ["Zhou", "Z.-H"], "year": 2012}, {"title": "Large margin distribution learning", "authors": ["Zhou", "Z.-H"], "venue": "Proceedings of the 6th IAPR International Workshop on Artificial Neural Networks in Pattern Recognition,", "year": 2014}], "id": "SP:bc5cc0640814684349e25e8e873c17dd2d0d8318", "authors": [{"name": "Teng Zhang", "affiliations": []}, {"name": "Zhi-Hua Zhou", "affiliations": []}], "abstractText": "Recent studies disclose that maximizing the minimum margin like support vector machines does not necessarily lead to better generalization performances, and instead, it is crucial to optimize the margin distribution. Although it has been shown that for binary classification, characterizing the margin distribution by the firstand second-order statistics can achieve superior performance. It still remains open for multiclass classification, and due to the complexity of margin for multi-class classification, optimizing its distribution by mean and variance can also be difficult. In this paper, we propose mcODM (multi-class Optimal margin Distribution Machine), which can solve this problem efficiently. We also give a theoretical analysis for our method, which verifies the significance of margin distribution for multi-class classification. Empirical study further shows that mcODM always outperforms all four versions of multi-class SVMs on all experimental data sets.", "title": "Multi-Class Optimal Margin Distribution Machine"}