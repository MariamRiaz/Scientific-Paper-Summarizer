0	10	Many efforts have been made to analyze various notions of algorithmic stability and prove that a broad spectrum of learning algorithms are stable in some sense.
1	10	Intuitively, a learning algorithm is said to be stable if slight perturbations in the training data result in small changes in the output of the algorithm, and these changes vanish as the data set grows bigger and bigger (Bonnans & Shapiro, 2013).
3	29	The notion of algorithmic stability has been an important tool in deriving theoretical guarantees of the generalization abilities of learning algorithms.
8	26	This notion of stability is stronger than uniform algorithmic stability of Bousquet & Elisseeff (2002) that is only concerned about the change in the loss but not the hypothesis itself.
9	8	However, as we will show, the new notion is still quite natural and holds for a variety of learning algorithms.
14	26	Building on martingale inequalities in the Banach space of the hypotheses, we define a subset of the predefined hypothesis class, whose elements will (or will have a high probability to) be output by a learning algorithm, as the algorithmic hypothesis class, and study the complexity of the algorithmic hypothesis class of argument-stable learning algorithms.
25	16	A learning algorithm A : S ∈ Zn 7→ hS ∈ H is a mapping from Zn to a hypothesis classH that we assume to be a subset of a separable Banach space (B, ‖·‖).
27	7	In other words, we assume that the feature space X is the algebraic dual of the Banach space B.
35	10	Various ways have been introduced to measure algorithmic stability.
38	22	, Zn}, the sample S with the i-th example being replaced by an independent copy of Zi.
39	11	Definition 1 (Uniform Stability).
40	8	A learning algorithm A is β(n)-uniformly stable with respect to the loss function ` if for all i ∈ {1, .
41	9	, n}, |`(hS , Z)− `(hSi , Z)| ≤ β(n) , with probability one, where β(n) ∈ R+ .
43	14	Definition 2 (Uniform Argument Stability).
45	10	The two notions of stability are closely related: Intuitively, if the loss `(h, z) is a sufficiently smooth function of h, then uniform argument stability should imply uniform stability.
46	11	To make this intuition precise, we define the notion of Lipschitz-continuous loss functions below.
71	12	The concentration result of Lemma 1 justifies the following definition of the “algorithmic hypothesis class”: since with high probability hS concentrates around its expectation EhS , what matters in the generalization performance of the algorithm is the complexity of the ball centered at EhS and not that of the entire hypothesis class H .
73	10	Definition 4 (Algorithmic Hypothesis Class).
81	12	Recall that the Banach space (X , ‖ · ‖∗) is of type p ≥ 1 if there exists a constant Cp such that for all x1, .
82	15	In the important special case when X is a Hilbert space, the space is of type 2 with constant C2 = 1.
98	23	For the notion of uniform stability, such bounds appear in Lugosi & Pawlak (1994); Bousquet & Elisseeff (2002); Wibisono et al. (2009); Hardt et al. (2015); Liu et al. (2017).
99	26	As we will show in the examples below, many of these learning algorithms even have uniform argument stability of order O(1/n).
100	10	In such cases the bound of Corollary 1 is essentially equivalent of the earlier results cited above.
101	34	The bound is dominated by the term M √ log(1/δ) 2n present by using the bounded dif- ferences inequality.
103	10	When small risk is reasonable to expect, one may use more advanced concentration inequalities with secondmoment information, at the price of replacing the generalization error by the so-called “deformed” generalization errorR(hS)− aa−1RS(hS) where a > 1.
104	13	The next theorem derives such a bound, relying on techniques developed by Bartlett et al. (2005).
105	8	This result improves essentially on earlier stability-based bounds.
107	41	Suppose that the marginal distribution of the Xi is such that ‖Xi‖∗ ≤ B with probability one, for some B > 0 and that the loss function is bounded and Lipschitz, that is, `(h, Z) ≤ M with probability one for some M > 0 and |`(h, z)− `(h′, z)| ≤ L |〈h, x〉 − 〈h′, x〉| for all z ∈ Z and h, h′ ∈ H .
108	10	If a learning algorithm is α(n)uniformly argument stable, then, with probability at least 1− 2δ, R(hS)− a a− 1 RS(hS) ≤ 8LB √ 2 log(2/δ)α(n) + (6a+ 8)M log(1/δ) 3n .
112	11	Let F be a class of functions that map X into [0,M ].
