0	22	The primary aim of natural language generators (NLGs) for task-oriented dialogue is to effectively realize system dialogue actions and their associated content parameters.
3	22	However, in many applications it is also desirable for generators to control the style of an utterance independently of its content.
8	14	The strength of this new corpus is that: (1) we can use the PERSONAGE generator to generate as much training data as we want; (2) it allows us to systematically vary a specific set of stylistic parameters and the network architectures; (3) it allows us to systematically test the ability of different models to generate outputs that faithfully realize both the style and content of the training data.3 We develop novel neural models that vary the amount of explicit stylistic supervision given to the network, and we explore, for the first time, explicit control of multiple interacting stylistic parameters.
23	40	The frequencies of longer utterances (more attribute MRs) vary across train and test, with actual distributions in Table 2, showing how the test set was designed to be challenging, while the test set in Wen et al. (2015) averages less than 2 attributes per MR (Nayak et al., 2017).
25	24	The test set consists of 278 unique MRs and we generate 5 references per personality for a test size of 1,390 utterances.
30	90	To use PERSONAGE to create training data mapping the same MR to multiple personality-based variants, we set values for all of the parameters in Table 3 using the stylistic models defined by Mairesse and Walker (2010) for the following Big Five personality traits: agreeable, disagreeable, conscientiousness, unconscientiousness, and extravert.
37	59	However, if the other aggregation operations have a high value, PERSONAGE prefers to combine simple sentences into complex ones whenever it can, e.g., the EXTRAVERT personality example in Table 1 combines all the attributes into a single sentence by repeated use of the ALL MERGE and CONJUNCTION operations.
38	87	The CONSCIENTIOUS row in Table 1 illustrates the use of the WITH-CUE aggregation operation, e.g., with a decent rating.
40	17	In PERSONAGE, the aggregation operations are defined as syntactic operations on the dictionary entry’s syntactic tree.
42	73	The pragmatic operators in the second half of Table 3 are intended to achieve particular pragmatic effects in the generated outputs: for example the use of a hedge such as sort of softens a claim and affects perceptions of friendliness and politeness (Brown and Levinson, 1987), while the exaggeration associated with emphasizers like actually, basically, really influences perceptions of extraversion and enthusiasm (Oberlander and Gill, 2004; Dewaele and Furnham, 1999).
43	16	In PERSONAGE, the pragmatic parameters are attached to the syntactic tree at insertion points defined by syntactic constraints, e.g., EMPHASIZERS are adverbs that can occur sentence initially or before a scalar adjective.
57	15	The second model adds a token of additional supervision by introducing a new dialogue act, convert, to encode personality, inspired by the use of a language token for machine translation (Johnson et al., 2017).
61	13	The most complex model introduces a context vector, as shown at the top right of Figure 3.
74	38	We thus develop new scripts to automatically evaluate the types common types of neural generation errors: deletions (failing to realize a value), repeats (repeating a value), and substitutions (mentioning an attribute with an incorrect value).
75	17	Table 5 shows ratios for the number of deletions, repeats, and substitutions for each model for the test set of 1,390 total realizations (278 unique MRs, each realized once for each of the 5 personalities).
79	39	The table shows that MODEL NOSUP makes very few semantic errors (we show later that this is at the cost of limited stylistic variation).
85	19	Table 6 shows that the training data has the highest entropy, but MODEL CONTEXT performs the best at preserving the variation seen in the training data.
88	20	To measure whether the trained models faithfully reproduce the pragmatic markers for each personality, we count each pragmatic marker in Table 3 in the output, average the counts and compute the Pearson correlation between the PERSONAGE references and the outputs for each model and personality.
92	30	The pragmatic marker distributions for PERSONAGE train in Figure 2 indicates that the CONSCIENTIOUS personality most frequently uses acknowledgement-justify (i.e., “well”, “i see”), and request confirmation (i.e., “did you say X?”), which are less complex to introduce into a realization since they often lie at the beginning or end of a sentence, allowing the simple MODEL NOSUP to learn them.9 Aggregation.
93	13	To measure the ability of each model to aggregate, we average the counts of each aggregation operation for each model and personality and compute the Pearson correlation between the output and the PERSONAGE training data.
96	22	Note that all personalities use aggregation, even thought not all personalities use pragmatic markers, and so even without a special personality token, MODEL NOSUP is able to faithfully reproduce aggregation operations.
99	29	Based on our quantitative results, we select MODEL CONTEXT as the best-performing model and conduct an evaluation to test if humans can distinguish the personalities exhibited.
100	58	We randomly select a set of 10 unique MRs from the PERSONAGE training data along with their corresponding reference texts for each personality (50 items in total), and 30 unique MRs MODEL CONTEXT outputs (150 items in total).10 We construct a HIT on Mechanical Turk, presenting a single output (either PERSONAGE or MODEL CONTEXT), and ask 5 Turkers to label the output using the Ten Item Personality Inventory (TIPI) (Gosling et al., 2003).
103	21	Table 9 presents results as aggregated counts for the number of times at least 3 out of the 5 Turkers rated the matching item for that personality higher than the reverse item (Ratio Correct), the average rating the correct item received (range between 1-7), and an average “naturalness” score for the output (also rated 1-7).
105	53	The MODEL CONTEXT outputs exhibit the same trend except for UNCONSCIENTIOUS and AGREEABLE, where the correct ratio is only 0.17 and 0.50, respectively (they also have the lowest correct ratio for the original PERSONAGE data).
110	36	We train a version of MODEL TOKEN, as before on instances with single personalities, but such that it can be used to generate output with a combination of two personalities.
111	34	The experiment uses the original training data for MODEL TOKEN, but uses an expanded test set where the MR includes two personality CONVERT tags.
113	16	Sample outputs are given in Table 10 for the DISAGREEABLE personality, which is one of the most distinct in terms of aggregation and pragmatic marker insertion, along with occurrence counts (frequency shown scaled down by 100) of the operations that it does most frequently: specifically, period aggregation and expletive pragmatic markers.
114	31	Rows 1-2 shows the counts and an exam- ple of each personality on its own.
116	22	We can see from the table that while CONSCIENTIOUS on its own realizes the content in two sentences, period aggregation is much more prevalent in the DISAGREEABLE + CONSCIENTIOUS example, with the same content being realized in 5 sentences.
