29	27	Finally, we conclude the paper in Section 6.
30	6	In this section we introduce some preliminary knowledge on tensor algebra (Kolda & Bader, 2009) and tensor product reproducing kernel Hilbert space (Signoretto et al., 2013), together with notation.
32	14	A tensor is a multi-dimensional array that generalizes matrix representation, whose dimension is called mode or way.
35	7	The outer product of M vectors x(m) ∈ RIm for m ∈ [1 : M ] is an M -th order tensor and defined elementwise by ( x(1) ⊗ · · · ⊗ x(M) ) i1,··· ,iM = x (1) i1 · · ·x(M)iM for all values of the indices.
39	23	In general, CP model is considered to be a multilinear lowrank approximation while Tucker model is regarded as a multilinear subspace approximation (Zhao et al., 2013a).
72	26	Tensor provides a natural representation for multi-way data, but there is no guarantee that it will be effective for learning.
73	74	Learning will only be successful if the regularities that underlie data can be captured by the learning model.
82	18	(9) can be viewed as a type of kernelized Tucker factorization model, where kernel matrices K(m) defined on each mode allow to capture the nonlinear relationships within each mode and the major discriminative features between the modes.
83	33	Specifically, when G is super-diagonal and the size of each mode of G is the same, i.e., J1 = · · · = JM , it reduces to the kernelized CP (KCP) factorization model in (He et al., 2017), which can be formulated as min U(1),··· ,U(M) ‖X − JK(1)U(1), · · · ,K(M)U(M)K‖2F .
86	78	(9), they have different application scenarios that are similar to CP and Tucker models, see e.g., (Cichocki et al., 2015; Wang et al., 2015; Shao et al., 2015; Cao et al., 2017).
87	16	To solve Problem (8), we pursue a discriminative and nonlinear factorization machine by coupling kernelized tensor factorization with a maximum-margin classifier.
88	14	For simplicity, we focus on the KCP model.
89	40	We formulate the primal model of KSTM as follows: min U (m) i ,K (m),W,b γ N∑ i=1 ‖Xi − JK(1)U(1)i , · · · ,K(M)U (M) i K‖2F︸ ︷︷ ︸ Ω(X ) + 〈W,W〉︸ ︷︷ ︸ P (W) +C N∑ i=1 [ 1− yi ( 〈W, X̂i〉+ b )] +︸ ︷︷ ︸ L ( y, 〈W, X̂ 〉+ b ) , (11) where γ and C are parameters to control the approximate error and prediction loss respectively, and X̂i , JU(1)i , · · · ,U (M) i K, which have the same size as Xi.
90	27	Recall that the principle of KCP factorization, our KSTM is able to capture the multi-way and nonlinear correlations within the tensor data.
91	24	On the other hand, by sharing the kernel matrices K(m) for different data samples Xi, it makes KSTM possible to characterize tensor data taking into account both common and discriminative features.
94	38	Inspired by the idea of primal optimizations of non-linear SVMs (Chapelle, 2007), the well-known kernel trick is introduced here to implicitly capture the non-linear structures.
99	32	Now we discuss the solution of Problem (14).
101	20	The objective function is non-convex, and solving for the global minimum is difficult in general.
102	27	Therefore we derive an efficient iterative algorithm to reach the local optimum, by alternatively minimizing Problem (14) for each variable while fixing the other.
103	35	For the sake of simplicity, we let ŷi , k̂Ti β + b in the following.
104	5	Update K(m) : Since there is no supervised information involving K(m), we can utilize the original CP factorization optimization technique to find a solution, by solving the following linear system of equation: K(m) N∑ i=1 ( U (m) i W (−m) i ) = N∑ i=1 ( Xi(m)V (−m) i ) , (15) where V(−m)i = Mj 6=m(K(j)U (j) i ), W (−m) i =( V (−m) i )T V (−m) i , and Xi(m) is them-mode matricization of the data Xi.
106	28	After reordering the samples such that the first Nv samples are support tensors, the first order gradient with respect to β is as follows: ∇β = 2(λK̂β + K̂I0(K̂β − y + b1)), (16) where y is the class label vector, 1 is a vector of all ones of length N , and I0 is an N ×N diagonal matrix with the first Ns diagonal entries (number of support tensors) being 1 and 0 others, i.e., I0 = [ INs 0 0 0 ] .
109	8	The mapping function is defined as: φ : R∑ r=1 M∏ m=1 ⊗u(m)r → R∑ r=1 M∏ m=1 ⊗φ ( u(m)r ) .
112	22	Formally, we can derive a dual structural preserving kernel function as follows: κ ( X̂i, X̂j ) = κ ( R∑ p=1 M∏ m=1 ⊗u(m)ip , R∑ q=1 M∏ m=1 ⊗u(m)jq ) = R∑ p=1 R∑ q=1 M∏ m=1 κ ( u (m) ip ,u (m) jq ) .
113	20	By virtue of its derivation, we see that such a kernel function can take the multi-way structure within tensor flexibility into consideration.
114	6	Different kernel functions specify different hypothesis spaces or even different knowledge embeddings of the data and thus can be viewed as capturing different notions of correlations.
115	41	Here we consider both linear and nonlinear cases as examples.
120	25	The partial gradient of U(m)i with respect to κ(X̂i, X̂j) for the linear kernel is given by ∂κ ( X̂i, X̂j ) ∂U (m) i = U (m) j ( M∏ k 6=m ∗ ( U (k)T i U (k) j )) .
121	5	(23) Algorithm 1 Learning KSTMs Input: Training data D, rank of factorization R, regularization parameters γ and λ Output: Model parameters {U(m)i }, {K (m)},β, b 1: Initialize {U(m)i }, {K (m)},β, b 2: repeat 3: for m := 1 to M do 4: Fixing {U(m)i }, update K (m) by Eq.
123	46	(20) 7: Fixing {U(m)i }, {K (m)} and b, update β by Eq.
