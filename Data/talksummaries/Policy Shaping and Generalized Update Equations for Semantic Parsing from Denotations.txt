1	20	Several problems have been handled within this framework, including question answering (Berant et al., 2013; Iyyer et al., 2017) and instructions for robots (Artzi and Zettlemoyer, 2013; Misra et al., 2015).
3	62	Given the question and a table environment, a semantic parser maps the question to an executable program, in this case a SQL query, and then executes the query on the environment to generate the answer England.
4	20	In the SpFD setting, the training data does not contain the correct programs.
5	33	Thus, the existing learning approaches for SpFD perform two steps for every training example, a search step that explores the space of programs Question: what nation scored the most points Index Name Nation Points Games Pts/game 1 Karen Andrew England 44 5 8.8 2 Daniella Waterman England 40 5 8 3 Christelle Le Duff France 33 5 6.6 4 Charlotte Barras England 30 5 6 5 Naomi Thomas Wales 25 5 5 Select Nation Where Points is Maximum Program: Answer: Environment: England Figure 1: An example of semantic parsing from denotations.
6	22	Given the table environment, map the question to an executable program that evaluates to the answer.
7	53	and finds suitable candidates, and an update step that uses these programs to update the model.
9	30	In this paper, we address two key challenges in model training for SpFD by proposing a novel learning framework, improving both the search and update steps.
10	24	The first challenge, the existence of spurious programs, lies in the search step.
11	114	More specifically, while the success of the search step relies on its ability to find programs that are semantically correct, we can only verify if the program can generate correct answers, given that no gold programs are presented.
12	16	The search step is complicated by spurious programs, which happen to evaluate to the correct answer but do not represent accurately the meaning of the natural language question.
17	14	As a result, several families of learning methods, including maximum marginal likelihood, reinforcement learning and margin based methods have been used.
20	17	To address the first challenge, we propose a policy shaping (Griffith et al., 2013) method that incorporates simple, lightweight domain knowledge, such as a small set of lexical pairs of tokens in the question and program, in the form of a critique policy (§ 3).
54	18	However, the problems that search needs to handle in SpFD is more challenging.
55	28	In addition to finding a set of high-scoring incorrect programs, the search procedure also needs to guess the correct program(s) evaluating to the gold answer zi.
60	15	Given a training example (x, t, z), the aim of the search step is to find a set K(x, t, z) of programs consisting of correct programs that evaluate to z and high-scoring incorrect programs.
62	25	For example, in Figure 2, the spurious program Select Nation Where Index is Min will evaluate to an incorrect answer if the indices of the first two rows are swapped1.
71	13	Prior work, such as ✏-greedy exploration (Guu et al., 2017), has reduced the severity of this problem by introducing random noise in the search procedure to avoid saturating the search on high-scoring spurious programs.
74	40	This approach allows incorporating prior knowledge in the exploration policy and can bias the search away from spurious programs.
75	87	Algorithm 1 Learning a semantic parser from denotation using generalized updates.
78	80	ps(y | x, t) is the policy used for exploration and search(✓, x, t, z, ps) generates candidate programs for updating parameters (see Section 3).
79	183	is the generalized update (see Section 4).
82	28	4: K = search(✓, xi, ti, zi, ps) 5: » Compute generalized gradient updates 6: ✓ = ✓ + µ (K) 7: return ✓ Policy Shaping Policy shaping is a method to introduce prior knowledge into a policy (Griffith et al., 2013).
88	10	For example, the correct program in Figure 2 contains the token Points, which is also present in the question.
89	42	We therefore, define a simple surface form similarity feature match(x, y) that computes the ratio of number of non-keyword tokens in the program y that are also present in the question x.
91	34	For example, both the first and fourth program in Figure 2 contain the token Points but only the fourth program is correct.
93	17	For example, the token most is highly likely to co-occur with a correct program containing the keyword Max.
101	95	Given the set of programs generated by the search step, one can use many objectives to update the parameters.
102	31	For example, previous work have utilized maximum marginal likelihood (Krishnamurthy et al., 2017; Guu et al., 2017), reinforcement learning (Zhong et al., 2017; Guu et al., 2017) and margin based methods (Iyyer et al., 2017).
106	15	First, it allows us to understand existing algorithms better by examining their basic properties.
109	12	In the following, we describe how the commonly used algorithms are in fact very similar – their update rules can all be viewed as special cases of the proposed generalized update equation.
