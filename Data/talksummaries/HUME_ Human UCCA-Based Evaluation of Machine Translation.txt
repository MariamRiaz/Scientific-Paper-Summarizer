0	54	Human judgement should be the ultimate test of the quality of an MT system.
2	44	First, as the quality of translation is multifaceted, it is difficult to quantify the quality of the entire sentence in a single number.
5	43	Second, a sentence-level quality score does not indicate what parts of the sentence are badly translated, and so cannot inform developers in repairing these errors.
6	34	These problems are partially addressed by measures that decompose over parts of the evaluated translation, often words or n-grams (see §2 for a brief survey of previous work).
7	35	A promising line of research decomposes metrics over semantically defined units, quantifying the similarity of the output and the reference in terms of their verb argument structure; the most notable of these measures is HMEANT (Lo and Wu, 2011).
8	66	We propose the HUME metric, a human evaluation measure that decomposes over UCCA semantic units.
9	43	UCCA (Abend and Rappoport, 2013) is an appealing candidate for semantic analysis, due to its cross-linguistic applicability, support for rapid annotation, and coverage of many fundamental semantic phenomena, such as verbal, nominal and adjectival argument structures and their inter-relations.
12	28	This also allows the re-use of the source semantic annotation for measuring the quality of different translations of the same source sentence and avoids relying on reference translations, which have been shown to bias annotators (Fomicheva and Specia, 2016).
20	25	Some manual measures ask annotators to explicitly mark errors, but this has been found to have even lower agreement than ranking (Lommel et al., 2014).
34	23	UCCA (Universal Conceptual Cognitive Annotation) (Abend and Rappoport, 2013) is a cross-linguistically applicable scheme for semantic annotation.
35	35	Formally, an UCCA structure is a directed acyclic graph (DAG), whose leaves correspond to the words of the text.
39	18	The most basic notion is the Scene, which describes a movement, an action or a state which persists in time.
41	36	For example, the sentence “After graduation, Tom moved to America” contains two Scenes, whose main relations are “graduation” and “moved”.
46	48	Second, UCCA is cross-linguistically applicable, seeking to represent what is shared between languages by building on linguistic typological theory (Dixon, 2010b; Dixon, 2010a; Dixon, 2012).
47	19	Its cross-linguistic applicability has so far been tested in annotations of English, French, German and Czech.
52	19	Other approaches represent semantic structures as bi-lexical dependencies (Sgall et al., 1986; Hajič et al., 2012; Oepen and Lønning, 2006), which are indeed anchored in the text, but are less suitable for MT evaluation as they require linguistic expertise for their annotation.
55	31	UCCA annotation is performed once for every source sentence, irrespective of the number of its translations we wish to evaluate, and requires proficiency in the source language only.
61	26	In most cases, atomic units correspond to individual words, but they may also correspond to multi-word expressions that translate as one unit.
71	52	We will use the example “man bites dog” to illustrate typical examples of why a structural node should be labelled as “Bad”: incorrect ordering (“dog bites man”), deletion (“man bites”) and insertion (“man bites biscuit dog”).
75	23	When evaluating “to America” the annotator looks at the translation and sees the word “stateside”.
76	27	This word captures the whole phrase and so we mark this non-leaf node with an atomic label.
126	39	As expected and confirmed by confusion matrices in Figure 4, there is generally little confusion between the two types of units.
127	42	This results in the Kappa for all units being considerably higher than the Kappa over the atomic units or structural units, where there is more internal confusion.
128	27	To assess HUME reliability for long sentences, we binned the sentences according to length and measured Kappa on each bin (Figure 5).
139	36	Statistical interpretation of a large number of crowd-sourced adequacy judgements for each candidate translation on a fine-grained scale of 0 to 100 results in reliable aggregate scores, that correlate very strongly with one another.
142	19	We see this as a severe practical limitation of DA.
164	29	Consider the following example: Source ... tend to be higher in saturated fats Transl.
173	30	Diese Überprüfung bescränkte sich auf ...
185	26	The same issue is noted by Lo and Wu (2014): the IAA on SRL dropped from 90% to 61% when the two aligned structures were from two different annotators.
197	81	We believe that HUME, and a future automated version of HUME, allows for a finer-grained analysis of translation quality, and will be useful in informing the development of a more semantically aware approach to MT.
