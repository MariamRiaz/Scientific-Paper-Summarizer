{"sections": [{"heading": "1. Introduction", "text": "Real-world applications of binary classification to complex decision problems have led to the design of a wide range of evaluation metrics (Choi & Cha, 2010). Prominent examples include area under the ROC curve (AUC) for imbalanced labels (Menon et al., 2013), F-measure for information retrieval (Lewis, 1995), and precision at the top (Kar et al., 2014; 2015; Jasinska et al., 2016). To this end, several algorithms have been proposed for optimizing many of these metrics, primarily focusing on large-scale learning, without a conscious emphasis on statistical consequences of choosing models and their asymptotic behavior (Kar et al., 2015; Joachims, 2005). Wide use of such complex metrics has also re-invigorated research into their theoretical properties, which can then serve as a guide to prac-\nAuthors listed in the alphabetical order 1Institute of Computing Science, Poznan University of Technology, Poland 2Department of Computer Science, University of Illinois at UrbanaChampaign, USA 3Microsoft Research, India. Correspondence to: Wojciech Kot\u0142owski <wkotlowski@cs.put.poznan.pl>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\ntice (Koyejo et al., 2014a; Narasimhan et al., 2014a; Dembczyn\u0301ski et al., 2012; Waegeman et al., 2014; Natarajan et al., 2016).\nComplex evaluation metrics for binary classification are best described as set metrics, or non-decomposable metrics \u2013 as, in general, the evaluation for a set of predictions cannot be decomposed into the average of individual instance evaluations. This is in contrast to decomposable metrics such as accuracy which are defined as the empirical average of the instance evaluations. This property is the primary source of difficulty in theoretical analysis, and interestingly has led to two distinct settings and notions of consistency. On one hand, Population Utility (PU) focuses on estimation \u2013 so a consistent PU classifier is one which correctly estimates the population optimal utility as the size of the training set (equiv. test set) increases. The PU approach has strongest roots in classical statistical analysis which often deals with asymptotically optimal estimation. On the other hand, Expected Test Utility (ETU) focuses on generalization. Thus, the consistent ETU classifier is one which optimizes the expected prediction error over test sets of a pre-defined size. The ETU approach has strongest roots in statistical machine learning which prizes generalization as the primary goal. Importantly, these distinctions are irrelevant when the metric is a linear function of the confusion matrix e.g. (weighted) accuracy and other linear metrics. To the best of our knowledge, this dichotomy was first explicitly noted by Ye et al. (2012) in the context of F-measure.1 Like in Ye et al. (2012), our goal is not to adjudicate the correctness of either approach, but instead to explore deep connections, and highlight significant differences between both approaches for a wide range of metrics.\nContributions: We present a variety of results comparing and contrasting the PU and ETU approaches for consistent classification:\n\u2022 We show that for a wide range of metrics, PU and ETU are asymptotically equivalent with respect to the size of the test set, subject to a certain p-Lipschitzness\n1Note that Ye et al. (2012) termed the two approaches Empirical Utility Maximization (EUM) and Decision Theoretic Approach (DTA), respectively. We have instead chosen the more descriptive names Population Utility (PU) and Expected Test Utility (ETU).\ncondition which is satisfied by many metrics of interest. This further implies asymptotic equivalence of the Bayes optimal classifiers (Section 3.1). Similar results were previously only known for F-measure.\n\u2022 We provide lower bounds for the difference between PU and ETU metrics for finite test sets, and for certain metrics \u2013 thereby highlighting the difference between PU and ETU consistent classifiers with small test sets (Section 3.2).\n\u2022 We analyze approximate ETU classification using low order Taylor approximations, showing that the approximation can be computed with effectively linear complexity, yet achieves low error under standard assumptions (Section 4.1).\n\u2022 We consider the effects of model mis-specification and find that ETU may be more sensitive than PU, but this may be alleviated by properly calibrating the estimated probabilities (Section 4.2).\nIn addition, we present experimental results using simulated and real data to evaluate our theoretical claims (Section 5)."}, {"heading": "2. Preliminaries and Problem Setup", "text": "We consider the binary classification problem, where the input is a feature vector x \u2208 X , and the output is a label y \u2208 {0, 1}. We assume the examples (x, y) are generated i.i.d. according to P(x, y). A classifier is a mapping h : X \u2192 {0, 1}. We let 1C denote the indicator function i.e. equal to one if C is satisfied, and zero otherwise.\nGiven a distribution P and a binary classifier h, define:\nTP(h) = P(h = 1, y = 1), TN(h) = P(h = 0, y = 0), FP(h) = P(h = 1, y = 0), FN(h) = P(h = 0, y = 1),\nwhich are entries of the so-called confusion matrix, namely true positives, true negatives, false positives and false negatives. In this paper, we are interested in optimizing performance metrics \u03a6(h,P) (we use explicit dependence on P because we will also consider the empirical version of \u03a6) that are functions of the above four quantities. However, since the entries of the confusion matrix are interdependent, it suffices to only use their three independent combinations. Following Natarajan et al. (2016), we parametrize \u03a6(h,P) = \u03a6(u(h), v(h), p) by means of:\nu(h) = TP(h), v(h) = P(h = 1), and p = P(y = 1).\nAs argued by Natarajan et al. (2016), any metric being a function of the confusion matrix can be parameterized in this way. Table 1 lists popular examples of such metrics\nwith explicit parameterization \u03a6(u, v, p). Throughout the paper we assume \u03a6(u, v, p) is bounded from above and from below.2"}, {"heading": "2.1. Formal Definitions of PU and ETU", "text": "Definition 1 (Population Utility (PU)). Given a distribution P and classifier h, the PU of h for a performance metric \u03a6 is defined as \u03a6(u(h), v(h), p). We let h\u2217PU denote any maximizer of the PU,\nh\u2217PU \u2208 argmax h \u03a6(u(h), v(h), p) .\nIn words, the PU is obtained by taking the value of metric \u03a6 evaluated at the expected confusion matrix of h over P. Thus, one can think of the PU as evaluating the classifier h on a \u201csingle test set of infinite size\u201d drawn i.i.d. from P.\nIn contrast, ETU evaluates the expected utility for a fixedsize test set. Formally, given a sample S = {(xi, yi)}ni=1 of size n, generated i.i.d. from P, we let u\u0302(h), v\u0302(h), p\u0302 denote the corresponding empirical quantities:\nu\u0302(h) = 1\nn n\u2211 i=1 h(xi)yi, v\u0302(h) = 1 n n\u2211 i=1 h(xi), p\u0302 = 1 n n\u2211 i=1 yi,\nand the empirical value of metric \u03a6 is then \u03a6(u\u0302(h), v\u0302(h), p\u0302).\nDefinition 2 (Expected Test Utility (ETU)). Let x = (x1, . . . , xn) \u2208 Xn be an arbitrary sequence of inputs. Given a distribution P and a classifier h, the ETU of h for a performance metric \u03a6 conditioned on x is defined as:3\nEy|x [ \u03a6 ( u\u0302(h), v\u0302(h), p\u0302 )] ,\n2In fact, for essentially all metrics used in practice it holds 0 \u2264 \u03a6(u, v, p) \u2264 1.\n3The conditional expectation y|x is defined up to a zeromeasure set (over x), but this does not create any problems as we always consider x being sampled from the data distribution.\nwhere the expectation over y = (y1, . . . , yn) is with respect to the conditional distribution P(y|x) i.i.d. over the examples. We let h\u2217ETU(x) denote any maximizer of the ETU,\nh\u2217ETU(x) \u2208 argmax h\nEy|x [ \u03a6 ( u\u0302(h), v\u0302(h), p\u0302 )] .\nOne can think of ETU as evaluating the classifier h on \u201cinfinitely many test sets of size n\u201d drawn i.i.d. from P. We will see (in Section 4) that the optimal predictions (in both PU and ETU approaches) can be accurately estimated using the conditional probabilities P(yi|xi). In practice, we first obtain an estimator of the conditional probability and then compute the optimal predictions on test data based on their conditional probability estimates.\nRemark 1. More generally, ETU optimizes the expected utility Ey,x [ \u03a6 ( u\u0302(h), v\u0302(h), p\u0302 )] . However, clearly, it is suf-\nficient to analyze the predictions at any given x (Natarajan et al., 2016) as in Definition 2."}, {"heading": "2.2. Well-behaved Performance Metrics", "text": "The two frameworks treat the metrics as utility measures (i.e., they are to be maximized). Further, it is reasonable to expect that \u03a6(h,P) is non-decreasing in true positive and true negative rates (and indeed, virtually all performance measures used in practice behave this way). As shown by Natarajan et al. (2016), such monotonicity in true positive and true negative rates implies another property, called TP monotonicity, which is better suited to the parameterization employed here.\nDefinition 3 (TP monotonicity). \u03a6(u, v, p) is said to be TP monotonic if for any v, p and u1 > u2, it holds that \u03a6(u1, v, p) > \u03a6(u2, v, p).\nIt is easy to verify that all measures in Table 1 are TP monotonic.\nA contribution in this work is to develop a notion of regularity for metrics, that helps establish statistical connections between the two frameworks and their optimal classifiers. We call it p-Lipschitzness, defined next.\nDefinition 4 (p-Lipschitzness). \u03a6(u, v, p) is said to be p-Lipschitz if:\n|\u03a6(u, v, p)\u2212\u03a6(u\u2032, v\u2032, p\u2032)| \u2264 Up|u\u2212u\u2032|+Vp|v\u2212v\u2032|+Pp|p\u2212p\u2032|,\nfor any feasible u, v, p, u\u2032, v\u2032, p\u2032. The Lipschitz constants Up, Vp, Pp are allowed to depend on p, in contrast to the standard Lipschitz functions.\nThe rationale behind p-Lipschitzness is that we want to control the change in value of the measure under small\nchanges in their arguments. This property turns out to be essential to show equivalence between ETU and PU approaches. On the other hand, if we simply used a standard definition of Lipschitz function (with global constants), it would not be satisfied by many interesting measures. Hence, we weaken the Lipschitz property by allowing the constant to vary as a function of p. One can also show that general linear-fractional performance metrics studied in (Koyejo et al., 2014a; Narasimhan et al., 2015; Kot\u0142owski & Dembczyn\u0301ski, 2016) satisfy p-Lipschitzness under mild conditions (Appendix A).\nProposition 1. All measures in Table 1 are p-Lipschitz.\nProof. We only give a proof for F\u03b2-measure here (See Appendix A for the rest). For ease, let us denote F\u03b2(u, v, p) by F\u03b2 and F\u03b2(u\u2032, v\u2032, p\u2032) by F \u2032\u03b2 . Let \u2206u = u\u2212u\u2032, \u2206v = v\u2212v\u2032, \u2206p = p\u2212 p\u2032. We have:\n|F\u03b2 \u2212 F \u2032\u03b2 | = (1 + \u03b22) |u(\u03b22p\u2032 + v\u2032)\u2212 u\u2032(\u03b22p+ v)|\n(\u03b22p+ v)(\u03b22p\u2032 + v\u2032)\n= (1 + \u03b22) |\u2206u(\u03b22p\u2032 + v\u2032)\u2212 u\u2032\u03b22\u2206p\u2212 u\u2032\u2206v|\n(\u03b22p+ v)(\u03b22p\u2032 + v\u2032)\n\u2264 1 + \u03b2 2\n\u03b22p+v\n( |\u2206u|+ \u03b2 2u\u2032\n\u03b22p\u2032+v\u2032 |\u2206p|+ u\n\u2032\n\u03b22p\u2032+v\u2032 |\u2206v|\n) .\nSince u\u2032 \u2264 min{p\u2032, v\u2032}, we have \u03b2 2u\u2032 \u03b22p\u2032+v\u2032 \u2264 1, u\u2032\n\u03b22p\u2032+v\u2032 \u2264 1, and thus we can choose Up = Vp = Pp = 1+\u03b2 2\n\u03b22p .\nAs for an example of a metric which is not p-Lipschitz, consider the precision defined as \u03a6(u, v, p) = uv . Indeed, if v is close to zero, choosing v\u2032 = 2v, u\u2032 = u and p\u2032 = p gives:\n\u03a6(u, v, p)\u2212 \u03a6(u\u2032, v\u2032, p\u2032) = u 2v ,\nwhich can be arbitrarily large for sufficiently small v, while the difference |v \u2212 v\u2032| = v is small. As it turns out in Section 3.2, this pathological behavior of the precision metric is responsible for a large deviation between PU and ETU, which suggests that p-Lipschitzness is in some sense necessary to establish connections."}, {"heading": "3. Equivalence of PU and ETU", "text": "Most of the existing literature on optimizing nondecomposable classification metrics focus on one of the two approaches in isolation. In this section, we show that the two approaches are in fact asymptotically equivalent, for a range of well-behaved metrics. Informally, given a distribution P and a performance metric \u03a6, our first result is that for sufficiently large n, the PU of the associated h\u2217ETU is arbitrarily close to that of h\u2217PU, and likewise, the ETU of h\u2217PU is arbitrarily close to that of h \u2217 ETU. In contrast, we also\nshow that the PU and ETU optimal classifiers may suffer differences for small samples."}, {"heading": "3.1. Asymptotic Equivalence", "text": "The intuition behind the equivalence lies in the observation that the optimal classifiers under the two approaches exhibit a very simple, similar form, under mild assumptions on the distribution (Koyejo et al., 2014b; Narasimhan et al., 2014b; Natarajan et al., 2016). Let \u03b7(x) := P(y = 1|x) denote the conditional probability of positive class as a function of x. The following lemma shows that for any fixed classifier h that thresholds \u03b7(x), and sufficiently large sample size n, its performance measured with respect to PU and ETU are close, in particular, differ by a factor that decays as fast as O\u0303(1/ \u221a n). In fact, the result holds uniformly over all such binary classifiers.\nLemma 1. Let H = {h | h = 1\u03b7(x)\u2265\u03c4 , \u03c4 \u2208 [0, 1]}, be the class of thresholded binary decision functions. Let \u03a6 be a performance metric which is p-Lipschitz. Then, with probability at least 1 \u2212 \u03b4 over a random sample S = {(xi, yi)}ni=1 of size n generated i.i.d. from P, it holds uniformly over all h \u2208 H,\u2223\u2223\u2223\u03a6(u(h),v(h), p(h))\u2212 Ey|x [\u03a6(u\u0302(h), v\u0302(h), p\u0302(h))] \u2223\u2223\u2223\n\u2264 4Lp\n\u221a 2 log(n+ 1)\nn + 3Lp \u221a log 4\u03b4 2n + Lp\u221a n ,\nwhere u\u0302(h), v\u0302(h), p\u0302(h) are empirical quantities evaluated on S, and Lp = max{Up, Vp, Pp}. Remark 2. Lemma 1 generalizes the result obtained by Ye et al. (2012) for F\u03b2-measure to arbitrary p-Lipschitz metrics. Furthermore, using more careful bounding technique, we are able to get a better dependence on the sample size n, essentially O\u0303(1/ \u221a n) (neglecting logarithmic terms). In fact, this dependence cannot be improved any further in general (See Appendix B).\nThe uniform convergence result in Lemma 1 enables the first main result of this work. In particular, the convergence holds when the optimal classifiers with respect to ETU and PU are of the thresholded form, i.e. h\u2217PU \u2208 H, and h\u2217ETU(x) \u2208 H almost surely (with respect to random sample of inputs x), where H = {h | h = 1\u03b7(x)\u2265\u03c4 , \u03c4 \u2208 [0, 1]} is the class of threshold functions on function \u03b7(x). Several recent results have shown that the optimal classifier for many popular metrics (including all metrics in Table 1) indeed has the thresholded form (Narasimhan et al., 2014a; Lewis, 1995), under a mild condition related to continuity of the distribution of \u03b7(x) (See the proof of Theorem 1 in Appendix B.2 for details):\nAssumption 1. The random variable \u03b7(x) has a density (with respect to the Lebesgue measure) on [0, 1].\nWe are now ready to state the result. Proofs omitted in the main text are supplied in the Appendix.\nTheorem 1. Let \u03a6 be a performance metric that is TP monotonic and p-Lipschitz, and P be a distribution satisfying Assumption 1. Consider the ETU optimal classifier h\u2217ETU (Definition 2) and the PU optimal classifier h \u2217 PU (Definition 1). Then, for any given and \u03b4, we can choose n large enough (in Definition 2 of ETU), such that, with probability at least 1 \u2212 \u03b4 over the random choice of the sample of inputs x, we have:\u2223\u2223\u2223\u03a6(u(h\u2217ETU(x)),v(h\u2217ETU(x)), p)\n\u2212 \u03a6 ( u(h\u2217PU), v(h \u2217 PU), p )\u2223\u2223\u2223 \u2264 . Similarly, for large enough n, with probability 1\u2212 \u03b4,\u2223\u2223\u2223Ey|x[\u03a6(u\u0302(h\u2217ETU(x)), v\u0302(h\u2217ETU(x)), p\u0302)]\n\u2212Ey|x [ \u03a6 ( u\u0302(h\u2217PU), v\u0302(h \u2217 PU), p\u0302 )] \u2223\u2223\u2223 \u2264 . Remark 3. In essence, Theorem 1 suggests that, for large sample sizes, the optimal in the sense of one approach gives an accurate estimate (or a proxy) of the optimal in the sense of the other approach. Our characterization of p-Lipschitzness is key to showing the equivalence."}, {"heading": "3.2. Finite Sample Regime", "text": "The aforementioned result is asymptotic; to elucidate the point, we now give an example where optimal classifiers corresponding to PU and ETU differ. It is important to be aware of such extremities, especially when one applies a learned model to test data of modest sample sizes. The way we argue a lower bound is by specifying a metric and a distribution, such that on a randomly obtained test set of modest size, say m, the gap in the empirical metric computed on the test data for the two optimal classifiers can be large. As one is typically primarily interested in the empirical metric on a given test set, focusing on the empirical metric ensures fairness and forbids favoring either definition.\nExample. For some constant \u03b1 > 0, consider the (adjusted) empirical precision metric defined as:\n\u03a6Prec(u\u0302(h(x)), v\u0302(h(x)), p) = u\u0302(h(x))\nv\u0302(h(x)) + \u03b1 .\nNote that \u03a6Prec \u2208 [0, 11+\u03b1 ]; Furthermore, it is p-Lipschitz, with Lipschitz constant Vp \u221d 1\u03b1 (see Definition 4). Thus, choosing very small values of \u03b1 implies very high Lipschitz constant, and in turn the metric becomes less \u201cstable\u201d. To establish the desired lower bound, we choose a small 0 <\n\u03b1 1. Let {X1,X2,X3} denote a partition of the instance space X , i.e. \u222a3i=1Xi = X and Xi \u2229 Xj = 0, for any pair (i, j). Consider the joint distribution P defined as:\nP(y = 1|x \u2208 X1) = 1 , P(y = 1|x \u2208 X3) = 0, P(y = 1|x \u2208 X2) = 1\u2212 = 1\u2212 \u221a \u03b1, (1)\nP(X1) + P(X3) = 2 , P(X2) = 1\u2212 2.\nfor some 1 2 > 0 and note that the distribution is defined to be dependent on our choice of \u03b1. The last line in the above set of equations suggests that the distribution has a small region where labels are deterministically positive or negative, but overwhelmingly positive elsewhere. Theorem 2. Let x = {x1, x2, . . . , xn} denote a set of instances drawn i.i.d. from the distribution P. Let y = {y1, y2, . . . , yn} denote their labels drawn from the same distribution. With probability at least (1\u2212 2 \u2212 n),\n\u03a6Prec(u\u0302(h \u2217 ETU(x)), v\u0302(h \u2217 ETU(x)), p\u0302) \u2212\n\u03a6Prec(u\u0302(h \u2217 PU(x)), v\u0302(h \u2217 PU(x)), p\u0302) \u2265\n1\nn(1 + \u03b1) ."}, {"heading": "4. Algorithms: Optimization and Conditional Probability Estimation", "text": "Characterization of the optimal classifier as a thresholding of the conditional probability yields simple and efficient PU consistent estimators. The idea is to first obtain an estimator for the conditional probability using training data, and then search for an optimal threshold on a separate validation set (Narasimhan et al., 2014b; Koyejo et al., 2014b). Threshold search can be efficiently implemented in linear time (assuming probabilities are pre-sorted). In contrast, although a similar thresholding characterization exists for ETU (Natarajan et al., 2016), evaluation and prediction require the computation of an expensive expectation (Definition 2). For general metrics, there is an O(n3) procedure to determine the optimal test set labeling (Jansche, 2007; Chai, 2005; Natarajan et al., 2016), and the procedure can be sped up to O(n2) in some special cases (Ye et al., 2012; Natarajan et al., 2016). Here, we consider an approximation to ETU that requires only O(n) computation, yet achieves error O(n\u22123/2) compared to exact optimization."}, {"heading": "4.1. Approximation Algorithms", "text": "Recall that ETU seeks to find the classifier of the form:\nh\u2217ETU(x) = argmax h\nEy|x [ \u03a6(u\u0302(h), v\u0302(h), p\u0302) ] .\nFollowing (Lewis, 1995; Natarajan et al., 2016) we know that when \u03a6 is TP monotonic, it suffices to sort observations in decreasing order according to \u03b7(x) and assign positive labels to top k of them, for k = 0, . . . , n. Unfortunately, for each k, we need to calculate the expected utility\nAlgorithm 1 Approximate ETU Consistent Classifier 1: Input: \u03a6 and sorted estimates of \u03b7i, i = 1, 2, . . . , n 2: Init s\u2217i = 0,\u2200i \u2208 [n], p\u0302 = 1n \u2211n i=1 yi, u\u03020 = 0\n3: Set \u03a60 = \u03a6(0, 0, p\u0302) 4: for k = 1, 2, . . . , n do 5: Set u\u0302k = (k\u22121)u\u0302k\u22121+\u03b7k k , v\u0302k = k n 6: Set \u03a6k = \u03a6(u\u0302k, v\u0302k, p\u0302) (via Lemmas 2 or 3) 7: end for 8: k\u2217 \u2190 arg maxk=0,...,n \u03a6k. 9: return s\u2217 s.t. s\u2217i \u2190 1 for i \u2208 [k\u2217].\nmeasure, which is time consuming \u2013 requiring O(n2) in general. Our goal here is to approximate this term, so that it can be computed in O(n) time, then the whole procedure can be implemented in amortized time O(n).\nFix a binary classifier h : X \u2192 {0, 1} and the input sample x = (x1, . . . , xn). Let u\u0302(h), v\u0302(h), p\u0302 denote the empirical quantities, as defined in Section 3.1. Furthermore, we define semi-empirical quantities:\nu\u0303(h) = 1\nn n\u2211 i=1 h(xi)\u03b7(xi), and p\u0303 = 1 n n\u2211 i=1 \u03b7(xi)\n(there is no need to define v\u0303(h)). Note that u\u0303(h) = Ey|x [ u\u0302(h) ] , and p\u0303 = Ey|x [p\u0302].\nZeroth-order approximation. Our first approximation is based on Taylor-expanding the measure up to the second order:\nLemma 2. If \u03a6 is twice-differentiable in (u, p) and all its second-order derivatives are bounded by constant A, then:\u2223\u2223Ey|x [\u03a6(u\u0302(h), v\u0302(h), p\u0302)]\u2212 \u03a6(u\u0303(h), v\u0302(h), p\u0303)\u2223\u2223 \u2264 A\n2n .\nWe note that the first order terms vanish in the Taylor approximation (proof in Appendix). This constitutes a simple, yet powerful method for approximating ETU utility. Algorithm 1 outlines the resulting algorithm. As shown, the classifier can be computed inO(n) time overall, assuming the data is already sorted according to \u03b7(xi) (otherwise, the procedure is dominated by sorting time O(n log n)). We note that (Lewis, 1995) proposed a similar first order approximation, albeit without any rigorous guarantee.\nSecond order approximation. Naturally, we can get a better approximation by Taylor-expanding the measure up to the third order.\nLemma 3. Assume \u03a6 is three times differentiable in (u, p) and assume all its third-order derivatives are bounded by constant B. Let \u22072uu,\u22072up,\u22072pp denote the second-order\nderivative terms evaluated at (u\u0303, p\u0303), and likewise define \u22072up,\u22072pp. We then have:\u2223\u2223Ey|x [\u03a6(u\u0302(h), v\u0302(h), p\u0302)]\u2212 \u03a6appr(h)\u2223\u2223 \u2264 B\n3n3/2 ,\nwhere:\n\u03a6appr(h) = \u03a6(u\u0303(h), v\u0302(h), p\u0303)\n+ 1\n2 (\u22072uu + 2\u22072up)su +\u22072ppsp,\nand\nsp := 1\nn2 n\u2211 i=1 \u03b7(xi)(1\u2212 \u03b7(xi)),\nsu := 1\nn2 n\u2211 i=1 h(xi)\u03b7(xi)(1\u2212 \u03b7(xi)).\nTheorem 3 (Consistency). Given n instances x = (x1, x2, . . . , xn), sort them in decreasing order of \u03b7(xi). For 0 \u2264 k \u2264 n, let s(k) denote the vector with positions corresponding to top k of the sorted instances set to 1, and 0 otherwise. (a) Suppose first order derivatives are bounded by A, let:\nh\u2217a = arg max s(k) \u03a6(u\u0303(s(k)), v\u0302(s(k)), p\u0303),\nWe have:\n\u03a6(h\u2217ETU)\u2212 \u03a6(u\u0303(h\u2217a), v\u0302(h\u2217a), p\u0303) \u2264 A\n2n .\n(b) Suppose second order derivatives are bounded by B, let:\nh\u2217b = arg max s(k) \u03a6appr(s (k)),\nwhere \u03a6appr(h) is defined in Lemma 3. We have:\n\u03a6(h\u2217ETU)\u2212 \u03a6appr(h\u2217b) \u2264 2B\n3n3/2 .\nAs before, the approximation can be computed in O(n) total time. We could also expand the function up to orders higher than the third order, and get better approximations (still with O(n) computation if the order of the expansion is independent of n) at the cost of an even more complicated approximation formula. In experiments, we find that on real datasets with test data sets of size 100 or more, even the zeroth order approximation is highly accurate."}, {"heading": "4.2. Conditional Probability Estimation and Model Misspecification", "text": "So far, we assumed that we have access to the true class conditional density \u03b7(x) = P(y = 1|x) and the resulting\nclassifier is a threshold function on \u03b7(x). In practice, one employs some probability estimation procedure and gets \u03b7\u0302(x), which we call a model.4 Then, one uses \u03b7\u0302(x) as if it were a true conditional probability \u03b7(x) to obtain PU or ETU classifiers. Note that since \u03b7(x) is unknown, and we only have access to \u03b7\u0302(x), the best we can hope for is to choose the optimal threshold on \u03b7\u0302(x) (for PU) or choose the optimal number of test set observations k to be classified as positive after sorting them according to \u03b7\u0302(x) (for ETU). Next, we investigate these finite sample effects in practical PU and ETU procedures. For this analysis, we treat \u03b7\u0302(x) as given and fixed, make no other assumptions on how it was obtained. Let H\u0302 = {h | h = 1\u03b7\u0302(x)\u2265\u03c4 , \u03c4 \u2208 [0, 1]} denote the class of binary threshold functions on \u03b7\u0302(x).\nConsider PU first, and let h\u2217 be the PU-optimal classifier from H\u0302, i.e.:\nh\u2217 = argmax h\u2208H\u0302 \u03a6(u(h), v(h), p).\nIn practice, however, one does not have access to P, and thus u(h), v(h), p cannot be computed. Instead, given \u03b7\u0302(x), one uses a validation sample S = {(xi, yi)}ni=1 to choose a threshold on \u03b7\u0302(x) (and thus, a classifier from H\u0302), by directly optimizing the empirical version of the metric on S:\nh\u0302 = argmax h\u2208H\u0302 \u03a6(u\u0302(h), v\u0302(h), p\u0302).\nWe would like to assess how close is h\u0302 to h\u2217. By following the proof of Lemma 1 (which never assumes the class H is based on thresholding \u03b7(x)), it is easy to show that with high probability,\u2223\u2223\u03a6(u(h\u0302), v(h\u0302), p)\u2212 \u03a6(u(h\u2217), v(h\u2217), p)\u2223\u2223 \u2264 O( 1\u221a\nn\n) .\nThus, if we have a sufficiently large validation sample at our disposal, we can set the threshold which maximizes the empirical version of the metric, and our performance is guaranteed to be O\u0303(1/ \u221a n) close to the performance of the \u03a6-optimal classifier from H\u0302. In other words, PU does not require to know the true distribution in order to select the best classifier in H\u0302, only a sufficiently large validation sample is required.\nIn contrast, ETU procedure is inherently based on using \u03b7\u0302(x) as a replacement for \u03b7(x) (which we do not know) to decide upon label assignments. Let x = (x1, . . . , xn) be the input sample of size n. Assume for simplicity the distribution of \u03b7(x) and \u03b7\u0302(x) are continuous on [0, 1], so that for any i 6= j, \u03b7(xi) 6= \u03b7(xj) with probability one, and\n4For instance, \u03b7\u0302(x) could be obtained from logistic regression or neural network with soft-max function on the final layer.\nsimilarly for \u03b7\u0302. Then, given x and \u03b7\u0302, the ETU procedure chooses the classifier of the form:\nh\u0302 = argmax h\u2208H\u0302\nEy\u223c\u03b7\u0302(x) [ \u03a6(u\u0302(h), v\u0302(h), p\u0302) ] .\nLikewise, the optimal ETU classifier in H\u0302 is given by:\nh\u2217 = argmax h\u2208H\u0302\nEy\u223c\u03b7(x) [ \u03a6(u\u0302(h), v\u0302(h), p\u0302) ] ,\ni.e. by definition, the optimal classifier in the restricted class H involves the expectation with respect to the true \u03b7. Let us denote \u03a6ETU = Ey\u223c\u03b7(x) [ \u03a6(u\u0302(h), v\u0302(h), p\u0302) ] , so that h\u2217 maximizes \u03a6ETU. In the supplementary material, we show that under some mild assumptions on \u03a6:\nEx [\u2223\u2223\u03a6ETU(h\u0302)\u2212 \u03a6ETU(h\u2217)\u2223\u2223] \u2264 O\u0303( 1\u221a\nn\n) + Pp|p\u2212 p\u03b7\u0302|\n+ sup h\u2208H\u0302\nUp|u(h)\u2212 u\u03b7\u0302(h)|,\nwhere p\u03b7\u0302 = E [ \u03b7\u0302(x) ] and u\u03b7\u0302(h) = E [ h(x)\u03b7\u0302(x) ] , are the quantities corresponding to p and u(h), which were calculated by replacing the conditional probability \u03b7 with its estimate \u03b7\u0302. Thus, while for the PU procedure, the difference between h\u0302 and h\u2217 diminishes as n grows, it is not the case of ETU, as there are two bias terms |p \u2212 p\u03b7\u0302| and |u(h)\u2212u\u03b7\u0302(h)|which do not depend on n. These terms correspond to using incorrect conditional probability \u03b7\u0302 while selecting the classifier, and are present even if the sample size tends to infinity. Thus, it seems crucial for the success of ETU procedure to have \u03b7\u0302 calibrated with respect to the true distribution.\nA popular choice for class probability estimation is to use logistic regression. However, if the model is misspecified, which happens often in practice, the aforementioned discussion suggests that the desired ETU solution may not be achieved. Therefore, we need to learn the class probability function more carefully. Here, we consider two variants.\nThe first is to use the Isotron algorithm. In case of the generalized linear model, i.e. P(y|x) = \u03b3(w\u2217, x) for some unknown link function \u03b3 and model w\u2217, Kalai & Sastry (2009) proposed a simple and elegant algorithm (see Appendix E) that alternatively learns w\u2217 and the link function \u03b3 (approximated by a piecewise linear function). It provably learns the model under certain assumptions on P. The model w\u2217 and link function \u03b3 are learned using training data, and at prediction time, the link function and the scores of training data (i.e., xTi w) are used to calibrate the class probabilities \u03b7(x) of test instances.\nWe also consider using a recalibrated logistic model, i.e., we first estimate the class probabilities via standard logistic regression, and recalibrate the probabilities by running one update of the \u03b3 function in Isotron algorithm (which\nessentially solves a quadratic problem known as the Pool of Adjacent Violators). At test time, we use the learnt \u03b3 and the logistic model to estimate \u03b7(x) for test instances."}, {"heading": "5. Experiments", "text": "We empirically evaluate the effectiveness and accuracy of ETU approximations introduced in Section 4.1, on synthetic as well as real datasets. We also show on several benchmark datasets that, by carefully calibrating the conditional probabilities in ETU, we can improve the classification performance."}, {"heading": "5.1. Convergence of Approximations", "text": "We consider F1 and Jaccard metrics from Table 1. We sample conditional probabilities \u03b7i for n instances from the uniform distribution. The optimal predictions (see Definition 2) are obtained using Algorithm 1 of (Natarajan et al., 2016) (which is equivalent to searching over 2n possible label vectors). Then we compute the approximate optimal predictions using the first and the second order approximations discussed in Section 4.1. For each metric, we measure the deviation between the true and the approximate optimal values with increasing sample size in Figure 1. We observe linear convergence for the first order approximation and quadratic convergence for the second order approximation. This suggests that the bounds in Theorem 3 indeed can be improved for some metrics, if not in general."}, {"heading": "5.2. Approximations on Real Data", "text": "We report results on seven multiclass and multilabel benchmark datasets: (1) LETTERS: 16000 train, 4000 test instances, (2) SCENE: 1137 train, 1093 test (3) YEAST: 1500 train, 917 test (4) WEBPAGE: 6956 train, 27824 test (5) IMAGE: 1300 train, 1010 test (6) BREAST CANCER: 463 train, 220 test instances, (7) SPAMBASE: 3071 train, 1530 test instances.5 In case of multiclass datasets, we report results (using one-vs-all classifiers) averaged over classes (as\n5See (Koyejo et al., 2014b; Ye et al., 2012) for details.\nin Natarajan et al. (2016)).\nWe compare the exact ETU optimal, computed using the algorithm of Natarajan et al. (2016), with the approximations. The results for F1 and Jaccard metrics are presented in Table 2. The results convincingly show that the approximations are highly accurate, and almost always indistinguishable from optimizing true metrics, on real datasets. Note that even the first-order approximation (in fact, this is zeroth-order, as the first order term is zero; see Section 4) achieves high accuracy, as the test set sizes are relatively large."}, {"heading": "5.3. Model Misspecification", "text": "We now study how class probability estimation (CPE) and model misspecification affects the performances of PU and ETU approaches, on the seven benchmark datasets. We compare four methods: (a) ETU with logistic regression based CPE, (b) ETU with Isotron based CPE (discussed in Section 4.1), (c) ETU with recalibrated logistic regression based CPE (discussed in Section 4.1), and (d) PU using logistic regression based CPE followed by threshold tuning on validation set (Koyejo et al., 2014b). Additional comparisons to structured SVM (Joachims, 2005) and other classifiers are available in previously published work by others (Koyejo et al., 2014b; Natarajan et al., 2016), and are omitted here.\nThe results are presented in Table 3. We observe that the logistic model (column 1) is insufficient for many of the datasets. The results improve in several cases using the estimated generalized linear model with Isotron (column 2). However, there is a confounding factor that the two algorithms are very different, and noticed improvement may not necessarily be due to better CPE. To isolate this, recalibrated logistic model results are presented in column 3. The results are in general much better than the standard logistic model, which suggests that it is indeed the case of model misspecification in these datasets. Finally, we present the results with PU algorithm in column 4. We find that the results closely match that of the recalibrated logistic model (except in the case of SCENE dataset); thus, correcting for model misspecification helps demonstrate the theorized asymptotic equivalence of PU and ETU approaches in practice."}, {"heading": "6. Conclusions and Future Work", "text": "We have presented new results which elucidate the relationship between the two notions of consistency for complex binary classification metrics. Next, we plan to explore surrogates to further improve training efficiency nondecomposable metrics. We will also extend to more complex prediction problems such as multilabel classification, where a similar dichotomy exists."}, {"heading": "Acknowledgments", "text": "W. Kot\u0142owski has been supported by the Polish National Science Centre under Grant No. 2013/11/D/ST6/03050."}], "year": 2017, "references": [{"title": "Expectation of F-measures: tractable exact computation and some empirical observations of its properties", "authors": ["Chai", "Kian Ming Adam"], "venue": "In Proceedings of the 28th Annual Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval,", "year": 2005}, {"title": "A survey of binary similarity and distance measures", "authors": ["Choi", "Seung-Seok", "Cha", "Sung-Hyuk"], "venue": "Journal of Systemics, Cybernetics and Informatics,", "year": 2010}, {"title": "On label dependence and loss minimization in multi-label classification", "authors": ["Dembczy\u0144ski", "Krzysztof", "Waegeman", "Willem", "Cheng", "Weiwei", "H\u00fcllermeier", "Eyke"], "venue": "Machine Learning,", "year": 2012}, {"title": "A maximum expected utility framework for binary sequence labeling", "authors": ["Jansche", "Martin"], "venue": "In Annual Meeting of the Association of Computational Linguistics,", "year": 2007}, {"title": "A support vector method for multivariate performance measures", "authors": ["Joachims", "Thorsten"], "venue": "In Proceedings of the 22nd Intl. Conf. on Machine Learning,", "year": 2005}, {"title": "The Isotron algorithm: High-dimensional isotonic regression", "authors": ["Kalai", "Adam", "Sastry", "Ravi"], "venue": "In Conference on Learning Theory (COLT),", "year": 2009}, {"title": "Online and stochastic gradient methods for nondecomposable loss functions", "authors": ["Kar", "Purushottam", "Narasimhan", "Harikrishna", "Jain", "Prateek"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2014}, {"title": "Surrogate functions for maximizing precision at the top", "authors": ["Kar", "Purushottam", "Narasimhan", "Harikrishna", "Jain", "Prateek"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "year": 2015}, {"title": "Consistent binary classification with generalized performance metrics", "authors": ["Koyejo", "Oluwasanmi", "Natarajan", "Nagarajan", "Ravikumar", "Pradeep K", "Dhillon", "Inderjit S"], "venue": "In Neural Information Processing Systems (NIPS),", "year": 2014}, {"title": "Consistent binary classification with generalized performance metrics", "authors": ["Koyejo", "Oluwasanmi O", "Natarajan", "Nagarajan", "Ravikumar", "Pradeep K", "Dhillon", "Inderjit S"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2014}, {"title": "Evaluating and optimizing autonomous text classification systems", "authors": ["Lewis", "David D"], "venue": "In Proceedings of the 18th Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval,", "year": 1995}, {"title": "On the statistical consistency of algorithms for binary classification under class imbalance", "authors": ["Menon", "Aditya", "Narasimhan", "Harikrishna", "Agarwal", "Shivani", "Chawla", "Sanjay"], "venue": "In Proceedings of the 30th Intl. Conf. on Machine Learning,", "year": 2013}, {"title": "Foundations of Machine Learning", "authors": ["Mohri", "Mehryar", "Rostamizadeh", "Afshin", "Talwalkar", "Ameet"], "year": 2012}, {"title": "On the statistical consistency of plug-in classifiers for non-decomposable performance measures", "authors": ["Narasimhan", "Harikrishna", "Vaish", "Rohit", "Agarwal", "Shivani"], "venue": "In Neural Information Processing Systems (NIPS),", "year": 2014}, {"title": "On the statistical consistency of plug-in classifiers for non-decomposable performance measures", "authors": ["Narasimhan", "Harikrishna", "Vaish", "Rohit", "Agarwal", "Shivani"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2014}, {"title": "Consistent multiclass algorithms for complex performance measures", "authors": ["Narasimhan", "Harikrishna", "Ramaswamy", "Harish", "Saha", "Aadirupa", "Agarwal", "Shivani"], "venue": "In Proceedings of the 32nd Intl. Conf. on Machine Learning,", "year": 2015}, {"title": "Optimal classification with multivariate losses", "authors": ["Natarajan", "Nagarajan", "Koyejo", "Oluwasanmi", "Ravikumar", "Pradeep", "Dhillon", "Inderjit"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "year": 2016}, {"title": "On the bayes-optimality of F-measure maximizers", "authors": ["Waegeman", "Willem", "Dembczynski", "Krzysztof", "Jachnik", "Arkadiusz", "Cheng", "Weiwei", "H\u00fcllermeier", "Eyke"], "venue": "Journal of Machine Learning Research,", "year": 2014}, {"title": "Optimizing F-measures: a tale of two approaches", "authors": ["Ye", "Nan", "Chai", "Kian Ming A", "Lee", "Wee Sun", "Chieu", "Hai Leong"], "venue": "In Proceedings of the Intl. Conf. on Machine Learning,", "year": 2012}], "id": "SP:23dadf29719358223588d8cea8ed35686b9b8cf3", "authors": [{"name": "Krzysztof Dembczy\u0144ski", "affiliations": []}, {"name": "Wojciech Kot\u0142owski", "affiliations": []}, {"name": "Oluwasanmi Koyejo", "affiliations": []}, {"name": "Nagarajan Natarajan", "affiliations": []}], "abstractText": "Statistical learning theory is at an inflection point enabled by recent advances in understanding and optimizing a wide range of metrics. Of particular interest are non-decomposable metrics such as the F-measure and the Jaccard measure which cannot be represented as a simple average over examples. Non-decomposability is the primary source of difficulty in theoretical analysis, and interestingly has led to two distinct settings and notions of consistency. In this manuscript we analyze both settings, from statistical and algorithmic points of view, to explore the connections and to highlight differences between them for a wide range of metrics. The analysis complements previous results on this topic, clarifies common confusions around both settings, and provides guidance to the theory and practice of binary classification with complex metrics.", "title": "Consistency Analysis for Binary Classification Revisited"}