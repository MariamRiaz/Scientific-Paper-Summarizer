1	32	In particular, UDS includes a practical implementation of Dowty’s theory of thematic proto-roles (Dowty, 1991): arguments are labeled with properties typical of Dowty’s proto-agent (AWARENESS, VOLITION ...) and proto-patient (CHANGED STATE ...).
2	62	Annotated corpora have allowed the exploration of Semantic Proto-role Labeling (SPRL) 1 as a natural language processing task (Reisinger et al., 2015; White et al., 2016; Teichert et al., 2017).
3	32	For example, consider the following sentence, in which a particular pair of predicate and argument heads have been emphasized: “The cat ate the rat.” An SPRL system must infer from the context of the sentence whether the rat had VOLITION, CHANGED-STATE, and EXISTED-AFTER the eating event (see Table 2 for more properties).
5	49	We include a thorough quantitative analysis highlighting the contrasting errors between the proposed model and previous (nonneural) state-of-the-art.
6	16	In addition, our network naturally shares a subset of parameters between attributes.
8	60	Davidson (1967) is credited for representations of meaning involving propositions composed of a fixed arity predicate, all of its core arguments arising from the natural language syntax, and a distinguished event variable.
9	10	The earlier example could thus be denoted (modulo tense) as (∃e)eat[(e, CAT, RAT)], where the variable e is a reification of the eating event.
10	46	The order of the arguments in the predication implies their role, where leaving arguments unspecified (as in “The cat eats”) can be handled either by introducing variables for unstated arguments, e.g., (∃e)(∃x)[eat(e, CAT, x)], or by creating new predicates that correspond to different arities, e.g., (∃e)eat intransitive[(e, CAT)].3 The Neo-Davidsonian approach (Castañeda, 1967; Parsons, 1995), which we follow in this work, allows for variable arity by mapping the argument positions of individual predicates to generalized semantic roles, shared across predicates,4 e.g., AGENT, PATIENT and THEME, in: (∃e)[eat(e) ∧ Agent(e, CAT) ∧ Patient(e, RAT)].
11	100	Dowty (1991) conjectured that the distinction between the role of a prototypical Agent and prototypical Patient could be decomposed into a number of semantic properties such as “Did the argument change state?”.
12	20	Here we formulate this as a Neo-Davidsonian representation employing semantic proto-role (SPR) attributes: (∃e) [eat(e) ∧ volition(e, CAT) ∧ instigation(e, CAT)... ∧ ¬volition(e, RAT) ∧ destroyed(e, RAT)... ] Dowty’s theory was empirically verified by Kako (2006), followed by pilot (Madnani et al., 2010) and large-scale (Reisinger et al., 2015) corpus annotation efforts, the latter introducing a logistic regression baseline for SPRL.
13	22	Teichert et al. (2017) refined the evaluation protocol,5 and developed a CRF (Lafferty et al., 2001) for the task, representing existing state-of-the-art.
14	28	Full details about the SPR datasets introduced by Reisinger et al. (2015) and White et al. (2016), which we use in this work, are provided in Appendix B.
17	14	Our architecture encodes the sentence using a shared, one-layer, bidirectional LSTM (Hochreiter and Schmidhuber, 1997; Graves et al., 2013).
18	65	We then obtain a continuous, vector representation hea = [he;ha], for each predicate-argument pair as the concatenation of the hidden BiLSTM states he and ha corresponding to the syntactic head of the predicate of e and argument a respectively.
19	28	These heads are obtained over gold syntactic parses using the predicate-argument detection tool, PredPatt (White et al., 2016).6 For each SPR attribute, a score is predicted by passing hea through a separate two-layer perceptron, with the weights of the first layer shared across all attributes: Score(attr,hea) = Wattr [g (Wshared [hea])] This architecture accomodates the definition of SPRL as multi-label binary classification given by Teichert et al. (2017) by treating the score as the log-odds of the attribute being present (i.e. P(attr|hea) = 1 1+exp[−Score(attr,hea)] ).
20	21	This architecture also supports SPRL as a scalar regression task where the parameters of the network are tuned to directly minimize the discrepancy between the predicted score and a reference scalar label.
22	14	Training with Auxiliary Tasks A benefit of the shared neural-Davidsonian representation is that it offers many levels at which multi-task learning may be leveraged to improve parameter estimation so as to produce semantically rich representations hea, he, and ha.
25	15	While all settings outperformed prior work in aggregate, simply initializing the BiLSTM parameters with a pretrained English-to-French machine translation encoder7 produced the best results,8 so we simplify discussion by focusing on that model.
30	13	Each model was trained for ten epochs, selecting the best-performing epoch on dev.
31	8	Prior Work in SPRL We additionally include results from prior work: “LR” is the logisticregression model introduced by Reisinger et al. (2015) and “CRF” is the CRF model (specifically SPRL⋆) from Teichert et al. (2017).
32	10	Although White et al. (2016) released additional SPR annotations, we are unaware of any benchmark results on that data; however, our multi-task results in Appendix A do use the data and we find (unsurprisingly) that concurrent training on the two SPR datasets can be helpful.
37	34	Embeddings remained fixed during training.
39	11	Ablation experiments (Appendix A) show the advantages conferred by these features.
45	25	For some properties, the absolute F1 gains are quite large: DESTROYED (+24.2), CHANGED POSSESSION (+19.2.0), CHANGED LOCATION (+10.1), STATIONARY (+26.0) and LOCATION (+35.3).
60	9	Starting with the same randomly initialized BiLSTM13, we consider two training scenarios: (1) ignoring the remaining properties or (2) including the model’s loss on other properties with a weight of λ = 0.1 in the training objective.
67	7	Our architecture naturally supports discrete or continuous label paradigms, lends itself to multi-task initialization or concurrent training, and allows for parameter sharing across properties.
68	28	We demonstrated this sharing may be useful when some properties are only sparsely annotated in the training data, which is suggestive of future work in efficiently increasing the range of annotated SPR property types.
77	36	To complete the basic experiments reported in the main text, here we include an investigation of the impact of multi-task learning for SPRL.
80	29	We also use the terminology target task and auxiliary task to differentiate the primary task(s) we are inter- ested in from those that play only a supporting role in training.
84	12	Please refer to Appendix B for details on the datasets used in this section.
