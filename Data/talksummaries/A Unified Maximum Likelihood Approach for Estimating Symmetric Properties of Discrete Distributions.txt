37	2	One of the most common distribution estimators, dating back to Fisher is maximum likelihood, that for clarity we call sequence maximum likelihood (SML) (Aldrich, 1997).
39	1	The SML estimate is exceedingly simple to derive.
41	1	The empiri- cal frequency estimator assigns to each symbol x, the fraction p̂(x) def= N x /n of times it appears in the sample xn.
44	1	While the SML plug-in estimator performs well in the limit of many samples, sophisticated techniques have recently yielded more accurate estimators for several important symmetric properties.
46	2	Motivated by databases, where each entry appears at least once, (Raskhodnikova et al., 2009) considered distributions whose non-zero probabilities are at least 1 k , 1 k def = {p 2 : p(x) 2 {0} [ [1/k, 1]} , and estimated the normalized support ˜S(p) def= S(p)/k.
47	1	It can be shown that CSML( ˜S(p), 1 k , ") = ⇥(k log 1 " ).
48	1	Yet (Valiant & Valiant, 2011a; Wu & Yang, 2015) showed that C⇤( ˜S(p), 1 k , ") = ⇥ ⇣ k log k · log2 1 " ⌘ .
49	2	Here too we consider the normalized coverage ˜S m (p) def = S m (p)/m.
50	1	(Good & Toulmin, 1956) proposed the Good Toulmin (GT) estimator that achieves CGT( ˜S m (p), , ") = m/2.
51	1	Recently, (Orlitsky et al., 2016) derived a simple estimator showing that C⇤( ˜S m (p), , ") = ⇥( m logm · log 1 " ).
52	1	(Zou et al., 2016) derived a more complex estimator with similar dependence on m but worse dependence on ".
53	1	Since elements with arbitrarily small probability can contribute to an arbitrarily high entropy, H(p) cannot be estimated over aribtrary support with finitely many samples.
55	1	It can be shown that CSML(H(p), k , ") = ⇥(k " ) (Paninski, 2003).
57	1	(Valiant & Valiant, 2011b) showed that C⇤(kp uk 1 , k , ") = O ⇣ k log k · 1 " 2 ⌘ , and (Jiao et al., 2016) showed that this bound is tight.
58	1	These results are summarized in Table 1.
63	1	Symmetric distribution properties do not depend on the symbol labels.
123	1	Comparison to the linear-programming plug-in estimator (Valiant & Valiant, 2011a).
155	1	Recall that P is a collection of distributions over Z , and f : P !
156	1	R. Given a sample Z from an unknown p 2 P , we want to estimate f(p).
158	1	We bound the performance of this approach in the following theorem.
176	1	Suppose for a property f(p), there is an estimator with sample complexity n that achieves an accuracy ±" with probability of error at most 1/3.
177	1	The standard method to boost the error probability is the median trick: (i) Obtain O(log(1/ )) independent estimates using O(n log(1/ )) independent samples.
178	1	(ii) Output the median of these estimates.
179	1	This is an "-accurate estimator of f(p) with error probability at most .
182	1	For example, estimators for symmetric properties, such as entropy typically use the profile of the sequence, and hence Z n = n. Using the median-trick, we get the following result.
226	2	The next lemma bounds the bias of the estimator.
239	2	We studied estimation of symmetric properties of discrete distributions using the principle of maximum likelihood, and proved optimality of this approach for a number of problems.
243	23	Given our results, approximations stronger than exp( "2n) would be very interesting.
244	157	In the particular case when the desired accuracy is a constant, even an exponential approximation would be sufficient for many properties.
245	155	We plan to apply the heuristics proposed by (Vontobel, 2012) for various problems we consider, and compare with the state of the art provable methods.
