0	55	The United States government does not keep systematic records of when police kill civilians, despite a clear need for this information to serve the public interest and support social scientific analysis.
1	36	Federal records rely on incomplete cooperation from local police departments, and human rights statisticians assess that they fail to document thousands of fatalities (Lum and Ball, 2015).
2	41	News articles have emerged as a valuable alternative data source.
3	133	Organizations including The Guardian, The Washington Post, Mapping Police Violence, and Fatal Encounters have started to build such databases of U.S. police killings by manually reading millions of news articles1 and extracting victim names and event details.
4	60	This approach was recently validated by a Bureau of Justice Statistics study (Banks et al., Dec. 2016) which augmented traditional policemaintained records with media reports, finding twice as many deaths compared to past government analyses.
8	24	Populating an entity-event database: From a corpus of news articles D(test) over timespan T , extract the names of persons killed by police during that same timespan (E(pred)).
41	28	We process all documents with the open source spaCy NLP package5 to segment sentences, and extract entity mentions.
43	56	To prevent overfitting, other person names are mapped to a different PERSON symbol; e.g. “TARGET was killed in an encounter with police officer PERSON.” There were initially 18,966,757 and 6,061,717 extracted mentions for the train and test periods respectively.
47	34	Since we do not have gold-standard labels to train our model, we turn to distant supervision (Craven and Kumlien, 1999; Mintz et al., 2009), which heuristically aligns facts in a knowledge base to text in a corpus to impute positive mention-level labels for supervised learning.
48	54	Previous work typically examines distant supervision in the context of binary relation extraction (Bunescu and Mooney, 2007; Riedel et al., 2010; Hoffmann et al., 2011), but we are concerned with the unary predicate “person was killed by police.” As our gold standard knowledge base (G), we use Fatal Encounters’ (FE) publicly available dataset: around 18,000 entries of victim’s name, age, gender and race as well as location, cause and date of death.
50	24	We compare two different distant supervision training paradigms (Table 3): “hard” label training (§4.2) and “soft” EM-based training (§4.3).
53	102	Recall a single entity will have one or more mentions (i.e. the same name occurs in multiple sentences in our corpus).
54	80	For a given mention i in sentence xi, our model predicts whether the person is described as having been killed by police, zi = 1, with a binary logistic model, P (zi = 1 | xi) = σ(βTfγ(xi)).
55	35	(2) We experiment with both logistic regression (§4.4) and convolutional neural networks (§4.5) for this component, which use logistic regression weights β and feature extractor parameters γ.
56	100	Then we must somehow aggregate mention-level decisions to determine entity labels ye.9 If a human reader were to observe at least one sentence that states a person was killed by police, they would infer that person was killed by police.
57	44	Therefore we aggregate an entity’s mention-level labels with a deterministic disjunction: P (ye = 1 | zM(e)) = 1 {∨i∈M(e) zi} .
61	48	Procedurally, it counts strong probabilistic predictions as evidence, but can also incorporate a large number of weaker signals as positive evidence as well.10 In order to train these classifiers, we need mention-level labels (zi) which we impute via two different distant supervision labeling methods: “hard” and “soft.”
62	50	In “hard” distant labeling, labels for mentions in the training data are heuristically imputed and directly used for training.
63	26	First, name-only: zi = 1 if ∃e ∈ G(train) : name(i) = name(e).
66	42	We manually analyze a sample of positive mentions and find 36 out of 100 name-only sentences did not express a police fatality event—for example, sentences contain commentary, or describe killings not by police.
75	33	We initialize the model by training on the “hard” distant labels (§4.2), and then learn improved parameters by alternating E- and M-steps.
76	29	The E-step requires calculating the marginal posterior probability for each zi, q(zi) := P (zi | xM(ei), yei).
90	30	We also train a convolutional neural network (CNN) classifier, which uses word embeddings and their nonlinear compositions to potentially generalize better than sparse lexical and n-gram features.
112	50	From a practitioner’s perspective, a natural first approach to this task would be to run the corpus of police fatality documents through pre-trained, “off-the-shelf” event extractor systems that could identify killing events.
147	89	Manual analysis: Manual analysis of false positives indicates misspellings or mismatches of names, police fatalities outside of the U.S., people who were shot by police but not killed, and names of police officers who were killed are com- mon false positive errors (see detailed table in the appendix).
149	99	Our model allows for the use of mentionlevel semantic parsing models; systems with explicit trigger/agent/patient representations, more like traditional event extraction systems, may be useful, as would more sophisticated neural network models, or attention models as an alternative to disjunction aggregation (Lin et al., 2016).
150	55	One goal is to use our model as part of a semi-automatic system, where people manually review a ranked list of entity suggestions.
153	32	Higher recall may be possible through cost-sensitive training (e.g. Gimpel and Smith (2010)) and using features from beyond single sentences within the document.
155	81	Improving NLP analysis of historical events would also be useful for the event extraction task itself, by delineating between recent events that require a database update, versus historical events that appear as “noise” from the perspective of the database update task.
156	48	Finally, it may also be possible to adapt our model to extract other types of social behavior events.
