6	12	However, in dealing with the change point and clustering problem we would naturally require that our solution does not assume any knowledge of the number of changes nor the actual number of clusters, as these numbers would evolve over time, so we expect new changes in the process to happen and new clusters to form as N , the number of samples, grows.
8	13	Similar setups for change point detection have been the subject of study by Harchaoui & Cappé (2007), Arlot et al. (2016) and Garreau & Arlot (2017) who use characteristic kernels for detecting changes in the distribution, while from a computational standpoint a more effective implementation has been proposed by Celisse et al. (2017).
12	11	Data segmentation using the L1-penalty was introduced by Rudin et al. (1992).
15	9	Main contribution: The generalized setting of change point detection while clustering the segments for sequences of data points does not seem to have been previously studied.
17	7	We motivate the choice of the algorithm computationally by showing that it runs in O(N2D + D4) time (where D is an upper bound on the number of change points), statistically by showing that it can be seen as an approximation of a computationally hard MAP optimization problem for which we can derive an oracle inequality that guarantees low sample complexity, consistency and adaptivity, and practically by testing the model on simulation data.
26	12	Hence we put random variables between two consecutive changes in the same segment, and we think of random variables of the same segment or different segments as belonging to the same cluster if they are the realization of the same process.
69	11	A way to bypass this issue for the change point only detection problem is via dynamic programming (Harchaoui & Cappé, 2007); this approach works in this simplified setup since there is a natural ordering for exploring the subproblems, which does not hold here.
75	18	8: ( ȳ(k) )d k=0 := ordered sequence of (ȳ[ik+1,ik+1]) d k=0 (α(k)) d k=0 := corresponding permuted (αk) d k=0 according to permutation φd.
82	14	We restrict further this collection to partitions π satisfying what we call the clustering property, which states that if I1, I2 and I are segments in some (possibly different) parts of π, then {I1, I2 ∈ [k] ȳI1 6 ȳI 6 ȳI2 ⇒ I ∈ [k].
105	6	We start by proposing a Bayesian model selection scheme, which is later inverted to arrive at an integral form of the maximum a-posteriori probability (MAP) estimator.
111	12	Later we will see that the choice lf/m will not matter in comparison to the order of approximation, nevertheless we would like it to be a bounded continuous prior satisfying some additional conditions given in Lemma 2, even though we might be chosen as an improper prior.
112	18	On the family of models M we impose a categorical distribution measure PM as prior, with a weight pm for model m. Thus, we obtain the following sampling model for the data1: Y/F ∼ N (F, σ2IN ) F/m ∼ Ld ′ m (11) m ∼ PM = Categorical((pm)m∈M).
118	7	To obtain an approximation of the MAP estimate as a solution of a criterion of the form (4) we need the result of lemma 2 stated and proved in Appendix C using a Laplace approximation type of argument.
123	7	The standard way of assessing the performance of a statistical algorithm is by comparing its performance to a reasonable oracle.
130	8	Defining r̂m = ‖y − f̂m‖22 and pen(m) = 2σ2 ln 1pm + σ 2(d′m + 1) ln N d′m we see that the criterion (15) is of the form: Crit(m) = r̂m + pen(m).
131	6	The number of models in the family M having the same values of d′m and d ′′ m grows exponentially with those dimensions.
134	37	On the other hand we want this penalty to be as small as possible this way we give more importance to the fitting term r̂m.
137	80	See Appendix D. By investigating the oracle inequality, one notices that for an optimal choice of a one has to make a trade-off between the performance of the oracle part and the bias part of the inequality.
140	8	In our case, the value of the tuning parameter can be chosen independently of the variance of the noise and we can use the value of a for which we know that our estimator f̂m̂ will perform well.
141	6	For the set of models described in 6.1 with f∗ ∈ Fm∗ the following properties hold: • Adaptation and Risk Upper bound: The following adaptive upper bound in terms of d′m∗ and d ′′ m∗ holds for a = 2: Ef∗ [‖PFm̂ Y − f ∗‖2] 6 4σ2 ( 7 + 3(d′m∗ + 1) ln N d′m∗ + 6 ( d′m∗ ln[d ′′ m∗e 13 6 ] + d′′m∗ ln[d ′ m∗e 2] + d′′m∗ ln N d′′m∗ )) .
142	32	See Appendix D. We notice that the consistency condition d′′m∗ = o(N/ lnN) is within the restriction on the models in theorem 6.1, hence there is no loss of generality of having only models with ed′m 6 N in M since for other models we cannot guarantee convergent mean square risk anyway.
145	7	In the next section, we validate these theoretical guarantees by a series of tests on simulated data to get a sense of how tight the oracle inequality is, which signals are difficult to estimate and how the algorithm behaves in practice.
159	7	In the top histogram we notice that the algorithm successfully detects the change points most of the time; in fact, the achieved accuracy was number of change points correctly detectednumber of change points detected ≈ 0.8528.
161	23	In the bottom histogram we observe that the theoretical upper bound on the average mean square error –in this case 12.1575– found in Corollary 6.1 is very conservative and most of the 300 estimates –given by ‖f̂m̂−f ∗‖2 N – are significantly smaller.
162	6	In this work, we considered a novel problem related to change point detection where we have to address the simultaneous task of segmenting and clustering the observed signal.
166	14	We finally justified the use of Algorithm 1 by simulation data that shows some useful properties of the resulting estimate and validates the theoretical guarantees.
168	22	Another possible extension relates to the use of Algorithm 1 in the non-scalar case; this was already explored for change point only detection in (Arlot et al., 2016) through the use of characteristic kernels (Sriperumbudur et al., 2011).
169	6	We believe that the same approach can be adopted here except that we cannot perform the sorting step; this can be overcome using a Kernel clustering algorithm (Filipponea et al., 2008) or a spectral version of it (Schölkopf et al., 1998) for the second stage.
170	37	Finally, the remark after Figure 3 hints to the possibility of using a combined algorithm starting with the sparse solution of Fussed LASSO and running the 2nd dynamic programming pass of our algorithm as a way to boost the performance of Fussed LASSO to get rid of false discoveries.
171	36	This would be still computationally attractive according to the comment after Theorem 4.1, since the solution of Fussed LASSO has a small number of changes.
