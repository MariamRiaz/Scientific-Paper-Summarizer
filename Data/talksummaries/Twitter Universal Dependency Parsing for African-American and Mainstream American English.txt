0	81	Language on Twitter diverges from well-edited Mainstream American English (MAE, also called Standard American English) in a number of ways, presenting significant challenges to current NLP tools.
1	83	It contains, among other phenomena, nonstandard spelling, punctuation, capitalization, and syntax, as well as Twitter-specific conventions such as hashtags, usernames, and retweet tokens (Eisenstein, 2013).
2	71	Additionally, it contains an abundance of dialectal language, includ- ing African-American English (AAE), a dialect of American English spoken by millions of individuals, which contains lexical, phonological, and syntactic features not present in MAE (Green, 2002; Stewart, 2014; Jones, 2015).
3	79	Since standard English NLP tools are typically trained on well-edited MAE text, their performance is degraded on Twitter, and even more so for AAE tweets compared to MAE tweets— gaps exist for part-of-speech tagging (Jørgensen et al., 2016), language identification, and dependency parsing (Blodgett et al., 2016; Blodgett and O’Connor, 2017).
7	31	First, we develop standards to handle Twitter-specific and AAE-specific features within Universal Dependencies 2.0 (§3), by selecting and annotating a new dataset of 500 tweets, 250 of which are in AAE.
9	37	Third, since the UD English Treebank contains substantial amounts of traditional MAE data for training, we investigate cross-domain training methods to improve Twitter AAE dependency parsing with no, or very little, in-domain labeled data, by using Twitter-specific taggers, embeddings, and a novel heuristic training data synthesis procedure.
31	73	Our dataset contains 500 tweets, with a total of 5,951 non-punctuation edges, sampled from the publicly available TwitterAAE corpus.3 Each tweet in that corpus is accompanied by a model’s demographically-aligned topic model probabilities jointly inferred from Census demographics and word likelihood by Blodgett et al. (2016), including the African-American and White topics.
45	49	We observed AAE verbal auxiliaries, e.g., fees be looking upside my head aux Now we gone get fucked up aux damnnn I done let alot of time pass by aux including habitual be (“Continually, over and over, fees are looking at me...”), future gone (“we are going to get...”), and completive done (“I did let time pass by,” emphasizing the speaker completed a time-wasting action).
54	71	We also encountered many issues general to Twitter but not AAE; these are still important to deal with since AAE tweets include more non-standard linguistic phenomena overall.
55	38	When possible, we adapted Kong et al. (2014)’s annotation conventions into the Universal Dependencies context, which are the only published conventions we know of for Twitter dependencies (for the FUDG dependency formalism).
57	46	• Hashtags, which in their tag-like usage are utterances by themselves (#tweetliketheoppositegender Oh damn .).
59	32	Both hashtag and @- mention ambiguities are handled by Owoputi et al. (2013)’s POS tagger.
60	41	• Multiple utterances, since we do not attempt sentence segmentation, and in many cases sentential utterances are not separated by explicit punctuation.
61	70	FUDG allows for multiple roots for a text, but UD does not; instead we follow UD’s convention of the parataxis relation for what they describe as “side-by-side run-on sentences.” • Emoticons and emoji, which we attach as discourse relations to the utterance root, following UD’s treatment of interjections.
66	38	Our experiments use the following two parsers.
68	46	The parser attains 80.2% LAS (labeled attachment score) on the UD English treebank with automatically generated POS tags, and was a baseline system used in the CoNLL 2017 Shared Task (Zeman et al., 2017).6 Deep Biaffine (Dozat et al., 2017; Dozat and Manning, 2016) is a graph-based parser incorporating neural attention and biaffine classifiers for arcs and labels.
73	66	Morpho-Tagger vs. ARK POS tags: The UD Treebank contains extensive fine-grained POS and morphological information, on which UDPipe’s morphological analyzer and tagging system is originally trained.
75	35	We hypothesize that higher quality, even if coarser, POS information should improve parsing.
76	56	To test this, we retrain UDPipe in two different settings.
81	35	@-mentions, emojis, emoticons, expressions, and hashtags: For each sentence in the UD Treebank we inserted at least one @-mention, emoji, emoticon, expression (Internet-specific words and abbreviations such as lol, kmsl, and xoxo), or hashtag, annotated with the correct relation, at the beginning of the sentence.
87	32	AAE preverbal markers and auxiliaries: We introduced instances of verbal constructions present in AAE that are infrequent or non-existent in the UD Treebank data.
99	38	We compare two different settings (all using the UDPipe ARK Tagger setting): Twitter-only: To explore the effect of training with Twitter data alone, for each set of 250 we trained on that set alone, along with our Twitter embeddings, and tested on the remaining 250.
102	51	Morpho-Tagger vs. ARK Tagger: As hypothesized, UDPipe’s ARK Tagger setting outperformed the Morpho-Tagger across all settings, ranging from a 2.8% LAS improvement when trained only on the UD Treebank with no pre-trained word embeddings, to 4.7% and 5.4% improvements when trained with Twitter embeddings and both Twitter embeddings and synthetic data, respectively.
103	47	The latter improvements suggest that the ARK Tagger setup is able to take better advantage of Twitterspecific lexical information from the embeddings and syntactic patterns from the synthetic data.
109	43	Perhaps surprisingly, training with even limited amounts of in-domain training data aided in parsing performance; training with just in-domain data produced an LAS comparable to that of the baseline Deep Biaffine model, and adding UD Treebank data further increased LAS by 8.1%, indicat- ing that they independently provide critical signal.
110	113	For each model in each of the cross-domain settings, we calculated the LAS on the 250 tweets drawn from highly African-American tweets and the 250 from highly White tweets (see §3 for details); we will refer to these as the AA and WH tweets, respectively.
116	88	Finally, we observed the surprising result that adding Twitter-trained embeddings and synthetic data, which contains both Twitter-specific and AAE-specific features, increases the performance gap across both UDPipe settings.
118	50	We further calculated recall for each relation type across the AA tweets and WH tweets, and the resulting performance gap, under the UDPipe Morpho-Tagger and ARK Tagger models trained with synthetic data and embeddings.
123	38	AAE contains a variety of phonological features which present themselves on Twitter through a number of lexical variations (Green, 2002; Jones, 2015), many of which are listed in §3.1, instances of which occur a total of 80 times in the AA tweets; notably, none occur in the WH tweets.
140	41	The emerging literature on fairness in algorithms suggests interesting further challenges; for example, Kleinberg et al. (2017) and CorbettDavies et al. (2017) argue that as various commonly applied notions of fairness are mutually incompatible, algorithm designers must grapple with such trade-offs.
