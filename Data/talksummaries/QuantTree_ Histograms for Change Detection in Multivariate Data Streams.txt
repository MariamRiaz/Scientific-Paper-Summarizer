0	26	Change detection, namely the problem of analyzing a data stream to detect changes in the data-generating distribution, is very relevant in machine-learning and is typically addressed in an unsupervised manner.
3	11	Needless to say, all these have to be wisely designed and combined to yield a sound test that can provide prompt detections as well as a controlled False Positive Rate (FPR), which is one of the primary concerns in change detection.
4	9	Unfortunately, when it comes to monitoring multivariate data, it is difficult to find good density models and test statistics that do not depend on φ0: this represents a severe limitation for real-world monitoring problems, where the stream distribution is unknown.
18	10	In (Boracchi et al., 2017) it is shown that histograms built on uniform-density partitions rather than regular grids provide superior detection performance.
27	15	Our experiments (Section 5) show that QuantTree enables good detection performance in high dimensional streams.
32	12	We assume that a training set TR = {xi ∈ X , i = 1, .
34	25	Histograms: we define a histogram as: h = {(Sk, π̂k)}k=1,...,K , (1) where the K subsets Sk ⊆ X form a partition of Rd, i.e.,⋃K k=1 Sk = Rd and Sj ∩ Si = ∅, for j 6= i, and each π̂k ∈ [0, 1] corresponds to the probability for data generated from φ0 to fall inside Sk.
39	14	We focus on HTs that are based on a test statistic Th defined over the histogram h, like for instance the Pearson statistic (Lehmann & Romano, 2006).
40	10	Thus, Th uniquely depends on {yk}k=1,...,K , where yk denotes the number of samples in W falling in Sk.
41	25	We detect a change in the incoming W when Th(W ) = Th(y1, .
42	12	, yK) > τ, (3) where τ ∈ R is a threshold that controls the FPR, namely the proportion of type I errors (Lehmann & Romano, 2006).
44	9	There are two important comments.
51	10	7: Draw γ ∈ {0, 1} from a Bernoulli(0.5).
52	12	8: if γ = 0 then 9: Define Sk = {x ∈ Xk [x]i ≤ z(Lk)}.
55	9	Here we describe QuantTree1, an algorithm to define histograms h through a recursive binary splitting of the input space X .
56	17	This algorithm takes as input a training set TR containing N stationary points, the number of bins K in the histogram, and the target probabilities on each bin {πk}k=1,...,K , and returns a histogram h = {(Sk, π̂k)}k=1,...,K , where each π̂k represents an estimate of the probability for a sample drawn from φ0 to fall in Sk.
61	12	The splitting point is defined by sorting zn = [xn]i, i.e., the values of the i-th component for each xn ∈ Xk (lines 5).
68	43	, B do 2: Draw from ψ0 a training set TRb of N samples.
69	10	3: Use QuantTree to compute the histogram hb with K bins and target probabilities {πk}k over TR.
70	15	4: Draw a batch Wb containing ν points from φ0.
75	20	Indexes i and parameter γ are randomly chosen to add variability to the histogram construction.
76	14	Figure 1(a) shows a tree obtained from a bivariate Gaussian training set, defined by K = 4 bins, each having probability πk = N/4.
78	25	This result follows from Theorem 1, that is proved in Section 4.
80	39	When W ∼ φ0, the distribution of Th(W ) depends only on ν, N and {πk}k. Theorem 1 implies that we can numerically compute the thresholds for any statistic Th defined on histograms, provided ν, N and {πk}, thus disregarding φ0 and the data dimension d. To this end, we synthetically generate data from a conveniently chosen distribution ψ0, and we follow the procedure outlined in Algorithm 2 to estimate the threshold τ for HT in (2) yielding a desired FPR α.
81	69	At first we generate B training sets {TRb}b=1,...,B , sampling N points from ψ0 and, for each training set, we build a histogram hb using QuantTree (lines 2-3).
82	13	Then, for each hb we generate a batch Wb of ν points drawn from ψ0, and compute the value of the statistic tb = Th(Wb) (lines 4-5).
83	9	Finally, we estimate τ (line 7) from the set TB = {t1, .
84	34	, tB} as the 1 − α quantile of the empirical distribution of Th over the generated batches, i.e. τ = min { t ∈ TB : #{v ∈ TB : v > t} ≤ αB } , (5) where #A denotes the cardinality of a set A.
91	23	In contrast, thresholds defined by Algorithm 2 hold also in case of limited sample size, since they are not based on an asymptotic result.
92	16	These two statistics will be used for our experiments in Section 5, using thresholds reported in Table 1 for different values of N , K, ν and choosing πk = 1/K, k = 1, .
