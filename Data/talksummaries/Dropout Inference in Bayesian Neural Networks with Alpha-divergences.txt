37	60	We show that our models’ uncertainty increases on adversarial images generated from the MNIST dataset, suggesting that these lie outside of the training data distribution.
39	32	We review background in Bayesian neural networks and approximate variational inference.
40	53	In the next section we discuss α-divergences.
46	35	For classification tasks we assume a softmax likelihood, p ( y|x, ω ) = Softmax (fω(x)) or a Gaussian likelihood for regression.
51	34	Given weight matrices Wi and bias vectors bi for layer i, we often place standard matrix Gaussian prior distributions over the weight matrices, p0(Wi) = N (Wi;0, I) and often assume a point estimate for the bias vectors for simplicity.
61	34	For example, dropout can be seen as an approximation to Bayesian NN inference with dropout approximating distributions, where the rows of the matrices Wi distribute according to a mixture of two Gaussians with small variances and the mean of one of the Gaussians fixed at zero.
62	105	The uncertainty in the weights induces prediction uncertainty by marginalising over the approximate posterior using Monte Carlo integration: p(y = c|x,X,Y) = ∫ p(y = c|x, ω)p(ω|X,Y)dω ≈ ∫ p(y = c|x, ω)qθ(ω)dω ≈ 1 K K∑ k=1 p(y = c|x, ω̂k) with ω̂k ∼ qθ(ω), where qθ(ω) is the Dropout distribution (Gal, 2016).
63	55	Given its popularity, we concentrate on the dropout stochastic regularisation technique throughout the rest of the paper, although any other stochastic regularisation technique could be used instead (such as multiplicative Gaussian noise (Srivastava et al., 2014) or dropConnect (Wan et al., 2013)).
64	61	Dropout VI is an example of practical approximate inference, but it also underestimates model uncertainty (Gal, 2016, Section 3.3.2).
70	79	Popular methods of approximate inference include variational inference (VI) (Jordan et al., 1999) and expectation propagation (EP) (Minka, 2001), where these two algorithms are special cases of power EP (Minka, 2004) that minimises Amari’s α-divergence (Amari, 1985) Dα[p||q] in a local way: Dα[p||q] = 1 α(1− α) ( 1− ∫ p(ω)αq(ω)1−αdω ) .
74	27	Since α = 0 is used in VI and α = 1.0 is used in EP, in later sections we will also refer to these alpha settings as the VI value, Hellinger value, and EP value, respectively.
92	45	Third and most importantly, a naive implementation of BB-α using dropout would bring in a prohibitive computational burden.
101	19	However as discussed before, dropout implicitly samples different masked weight matrices ω̂ ∼ q for different data points.
102	22	This indicates that the naive approach, when applied to dropout approximation, would gather all these samples for all M datapoints in a mini-batch (i.e. MK sets of neural network weight matrices in total), which brings prohibitive cost if the network is wide and deep.
105	16	In the following section we propose an improved version of BB-α energy to allow applications with dropout and other flexible approximation structures.
111	18	Furthermore, provided Rβ [q̃||p0] < +∞ (which holds when assuming Zq < +∞), we have Rβ [q̃||p0] → KL[q̃||p0] = KL[q||p0] as α N → 0.
113	64	Note that here we also use the fact that now q ≈ q̃.
122	25	The Hellinger value could be used to achieve a balance between reducing training error and improving predictive likelihood, which has been found to be desirable (HernándezLobato et al., 2016; Depeweg et al., 2016).
138	21	Implementing this induced loss with Keras (Chollet, 2015) is as simple as a few lines of Python.
149	36	The model is a single-layer neural network with 50 ReLU units for all datasets except for Protein and Year, which use 100 units.
172	25	The third set of experiments considers adversarial attacks on dropout-trained Bayesian neural networks.
178	131	For the dropout trained networks we perform MC dropout at test time with Ktest = 10 MC samples.
182	21	The left panel in Figure 6 demonstrates the classification accuracy on adversarial examples, which shows that the dropout networks, especially the one trained with α = 1.0, are significantly more robust to adversarial attacks compared to the deterministic NN.
183	21	For example, for η = 0.1 the adversarial samples still visually close to the original class, and the BNN trained with α = 0.0 achieves an accuracy level almost 3 times higher than the MLP and around 20% higher than the VI-trained version.
190	22	The second attack we consider is a targeted version of FGS (Goodfellow et al., 2014; Carlini & Wagner, 2016), which maximises the predictive probability of a selected class instead.
191	19	As an example, we fix class 0 as the target and apply the iterative gradient-base attack to all non-zero digits in test data.
194	32	Similarly these adversarial examples could be detected by the Bayesian neural networks’ uncertainty, by examining the predictive entropy.
195	43	By visually inspecting the generated adversarial examples in the right panel of Figure 7, it is clear that the MLP overconfidently classifies a digit 7 to class 0.
196	103	On the other hand, the dropout models are still fairly uncertain about their predictions even after 40 gradient steps.
201	37	The technique often supersedes existing approximate inference techniques (even sparse Gaussian processes), and is easy to implement.
