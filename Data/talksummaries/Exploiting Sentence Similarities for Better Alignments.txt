32	42	The labels can be one of EQUI (semantically equivalent), OPPO (opposite meaning in context), SPE1, SPE2 (the chunk from s is more specific than the one from t and vice versa), SIMI (similar meaning, but none of the previous ones) or REL (related, but none of the above)1.
37	21	Here, si and tj denote a pair of chunks that are aligned with a label l. For brevity, we will include unaligned chunks into this format using a special null chunk and label to indicate that a chunk is unaligned.
42	26	For an unaligned chunk, the corresponding similarity z is fixed to zero.
43	35	Sentence similarity: The pair of sentences is associated with a scalar score from zero to five, to be interpreted as above.
44	34	We will use r to denote the sentence similarity for an input x.
45	19	Thus, the prediction problem is the following: Given a pair of chunked sentences x = (s, t), predict the alignment y, the alignment similarities z and the sentence similarity r. Note that this problem definition integrates the canonical semantic textual similarity task (only predicting r) and its interpretable variant (predicting both y and z) into a single task.
46	46	This section describes our model for predicting alignments, alignment scores, and the sentence similarity scores for a given pair of sentences.
50	23	Then, in Section 3.2, we will see how we can directly read off the similarity scores at both chunk and sentence level from the alignment.
67	46	That is, for a pair of chunks (si, tj) that are aligned with a label l, the chunk pair similarity zi,j,l is the coefficient associated with the corresponding inference variable.
70	17	(2) But can chunk similarities directly be used to find good alignments?
88	67	In the model proposed above, by predicting the alignment, we will be able to deterministically calculate both chunk and sentence level similarities.
89	20	This is in contrast to other approaches for the STS task, which first align constituents and then extract features from alignments to predict similarities in a pipelined fashion.
90	24	The joint prediction of alignment and similarities allows us to address the primary motivation of the paper, namely using the abundant sentence level data to train the aligner and scorer.
92	29	This assumption – similar to the one made by Chang et al. (2010b) – and the associated model described above, imply that the goal of learning is to find parameters that drive the inference towards good alignments and similarities.
96	20	The sentence dataset DS that consists of pairs of sentences where each pair is labeled with a numeric similarity score between zero and five.
99	63	We define three types of loss functions corresponding to the three components of the final output (i.e., alignment, chunk similarity and sentence similarity).
101	23	We will denote ground truth similarity scores and alignments using asterisks.
103	50	The alignment loss La is a structured loss function that penalizes alignments that are far away from the ground truth.
106	26	The chunk score loss Lc is designed to penalize errors in predicted chunk level similarities.
115	95	The sentence similarity loss Ls provides feedback to the aligner by penalizing alignments that are far away from the ground truth in their similarity assessments.
119	25	Learning algorithm We have two scenarios to consider: with only alignment dataset DA, and with both DA and sentence dataset DS .
122	52	In the first scenario, we simply perform the optimization using the alignment and the chunk score losses.
155	17	As noted in Section 3.1, the parameter αl combines chunk scores into sentence scores.
156	58	To find these hyper-parameters, we used a set of 426 sentences from the from the headlines training data that had both sentence and chunk annotation.
157	68	We simplified the search by assuming that αEqui is always 1.0 and all labels other than OPPO have the same α.
166	49	We also list the performance of the baseline system (Sultan et al., 2014a) and the top ranked systems from the 2016 shared task for each dataset6.
169	40	The typed score metric is the combination of untyped alignment, untyped score and typed alignment.
172	104	Further, we see that even our base model that only depends on the alignment data offers strong alignment F1 scores.
173	29	This validates the utility of jointly modeling alignments and chunk similarities.
177	71	While we observed slight change in the unscored alignment performance, for both the headlines and the images datasets, we saw improvements in both scored precision and recall when sentence level data was used.
