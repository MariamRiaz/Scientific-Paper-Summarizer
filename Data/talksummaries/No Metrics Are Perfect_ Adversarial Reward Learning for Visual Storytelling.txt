17	21	Visual captioning is aimed at depicting the concrete content of the images, and its expression style is rather simple.
23	29	As shown in Figure 1, the same photo stream can be paired with diverse stories, different from each other.
30	51	Here we showcase an adversarial example with an average METEOR score as high as 40.2: We had a great time to have a lot of the.
32	15	Conversely, when using some other metrics (e.g. BLEU, CIDEr) to evaluate the stories, we observe an opposite behavior: many relevant and coherent stories are receiving a very low score (nearly zero).
33	22	In order to resolve the strong bias brought by the hand-coded evaluation metrics in RL training and produce more human-like stories, we propose an Adversarial REward Learning (AREL) framework for visual storytelling.
34	17	We draw our inspiration from recent progress in inverse reinforcement learning (Ho and Ermon, 2016; Finn et al., 2016; Fu et al., 2017) and propose the AREL algorithm to learn a more intelligent reward function.
35	12	Specifically, we first incorporate a Boltzmann distribution to associate reward learning with distribution approximation, then design the adversarial process with two models – a policy model and a reward model.
36	88	The policy model performs the primitive actions and produces the story sequence, while the reward model is responsible for learning the implicit reward function from human demonstrations.
37	41	The learned reward function would be employed to optimize the policy in return.
74	18	We fist feed the photo stream I = (I1, · · · , I5) into a pretrained CNN and extract their high-level image features.
86	26	Next, multiple convolutional layers with different kernel sizes are used to extract the n-grams features, which are then projected into the sentence-level representation space by pooling layers (the design here is inspired by Kim (2014)).
88	20	Therefore, we then combine the sentence representation with the visual feature of the input image through concatenation and feed them into the final fully connected decision layer.
92	39	Reward Boltzmann Distribution In order to associate story distribution with reward function, we apply EBM to define a Reward Boltzmann distribution: pθ(W ) = exp(Rθ(W )) Zθ , (4) Where W is the word sequence of the story and pθ(W ) is the approximate data distribution, and Zθ = ∑ W exp(Rθ(W )) denotes the partition function.
94	21	Adversarial Reward Learning We first introduce an empirical distribution pe(W ) = 1(W∈D) |D| to represent the empirical distribution of the training data, whereD denotes the dataset with |D| stories and 1 denotes an indicator function.
95	37	We use this empirical distribution as the “good” examples, which provides the evidence for the reward function to learn from.
96	50	In order to approximate the Reward Boltzmann distribution towards the “real” data distribution p∗(W ), we design a min-max two-player game, where the Reward Boltzmann distribution pθ aims at maximizing the its similarity with empirical distribution pe while minimizing that with the “faked” data generated from policy model πβ .
103	15	On the other hand, the objective Jθ of the reward function is to distinguish between humanannotated stories and machine-generated stories.
115	16	Evaluation Metrics In order to comprehensively evaluate our method on storytelling dataset, we adopted both the automatic metrics and human evaluation as our criterion.
125	33	We first implement a strong baseline model (XEss), which share the same architecture with our policy model but is trained with cross-entropy loss and scheduled sampling.
135	17	In order to confirm our conjecture, we utilize automatic metrics as rewards to reinforce the visual storytelling model by adopting policy gradient with baseline to train the policy model.
162	23	From Table 1, we can observe slight gains of using AREL over GAN with automatic metrics, therefore we further deploy human evaluation for a better comparison.
164	24	Therefore, we perform two different kinds of human evaluation studies on Amazon Mechanical Turk: Turing test and pairwise human evaluation.
172	31	Besides, the Turing test of our AREL model reveals that nearly half of the workers are fooled by our machine generation, indicating a preliminary success toward generating human-like stories.
179	14	Therefore, it empirically confirms that our generated stories are more relevant to the image sequences, more coherent and concrete than the other algorithms, which however is not explicitly reflected by the automatic metric evaluation.
182	21	Then connecting the sentences together, we observe that the AREL story is more coherent and describes the photo stream more accurately.
183	17	Thus, our AREL model significantly surpasses the XEss model on all the three aspects of the qualitative example.
184	33	Besides, it won the Turing test (3 out 5 AMT workers think the AREL story is created by a human).
185	36	In the appendix, we also show a negative case that fails the Turing test.
186	103	In this paper, we not only introduce a novel adversarial reward learning algorithm to generate more human-like stories given image sequences, but also empirically analyze the limitations of the automatic metrics for story evaluation.
187	120	We believe there are still lots of improvement space in the narrative paragraph generation tasks, like how to better simulate human imagination to create more vivid and diversified stories.
