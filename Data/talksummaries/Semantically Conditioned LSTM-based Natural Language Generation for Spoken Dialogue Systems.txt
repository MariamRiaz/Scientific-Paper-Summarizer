0	125	The natural language generation (NLG) component provides much of the persona of a spoken dialogue system (SDS), and it has a significant impact on a user’s impression of the system.
1	42	As noted in Stent et al. (2005), a good generator usually depends on several factors: adequacy, fluency, readability, and variation.
3	54	The most common and widely adopted today is the rule-based (or template-based) approach (Cheyer and Guzzoni, 2007; Mirkovic and Cavedon, 2011).
9	20	However, these approaches still require a handcrafted generator to define the decision space within which statistics can be used for optimisation.
57	58	The generation model proposed in this paper is based on a recurrent NN architecture (Mikolov et al., 2010) in which a 1-hot encoding wt of a token1 wt is input at each time step t conditioned on a recurrent hidden layer ht and outputs the probability distribution of the next token wt+1.
58	49	Therefore, by sampling input tokens one by one from the output distribution of the RNN until a stop sign is generated (Karpathy and Fei-Fei, 2014) or some constraint is satisfied (Zhang and Lapata, 2014), the network can produce a sequence of tokens which can be lexicalised 2 to form the required utterance.
60	50	Of the various different connectivity designs for an LSTM cell (Graves, 2013; Zaremba et al., 2014), the architecture used in this paper is illustrated in Figure 3.1 and defined by the following equations,, it = σ(Wwiwt + Whiht−1) (1) ft = σ(Wwfwt + Whfht−1) (2) ot = σ(Wwowt + Whoht−1) (3) ĉt = tanh(Wwcwt + Whcht−1) (4) ct = ft ct−1 + it ĉt (5) ht = ot tanh(ct) (6) where σ is the sigmoid function, it, ft,ot ∈ [0, 1]n are input, forget, and output gates respectively, and ĉt and ct are proposed cell value and true cell value at time t. Note that each of these vectors has a dimension equal to the hidden layer h. In order to ensure that the generated utterance represents the intended meaning, the generator is further conditioned on a control vector d, a 1-hot representation of the dialogue act (DA) type and its slot-value pairs.
61	18	Although a related work (Karpathy and Fei-Fei, 2014) has suggested that reapplying this auxiliary information to the RNN at every time step can increase performance by mitigating the vanishing gradient problem (Mikolov and Zweig, 2012; Bengio et al., 1994), we have found that such a model also omits and duplicates slot information in the surface realisation.
62	40	In Wen et al. (2015) simple heuristics are used to turn off slot feature values in the control vector d once the corresponding slot token has been generated.
67	100	Starting from the original DA 1-hot vector d0, at each time step the DA cell decides what information should be retained for future time steps and discards the others, rt = σ(Wwrwt + αWhrht−1) (7) dt = rt dt−1 (8) where rt ∈ [0, 1]d is called the reading gate, and α is a constant.
68	46	Here Wwr and Whr act like keyword and key phrase detectors that learn to associate certain patterns of generated tokens with certain slots.
69	19	Figure 3 gives an example of how these detectors work in affecting DA features inside the network.
73	20	As shown in Figure 2, skip connections are applied to the inputs of all hidden layers as well as between all hidden layers and the outputs (Graves, 2013).
74	20	This reduces the number of processing steps between the bottom of the network and the top, and therefore mitigates the vanishing gradient problem (Bengio et al., 1994) in the vertical direction.
109	35	Each categorical value was replaced by a token representing its slot, and slots that appeared multiple times in a DA were merged into one.
110	33	After processing and grouping each utterance according to its delexicalised DA, we obtained 248 distinct DAs in the restaurant domain and 164 in the hotel domain.
112	51	The system was implemented using the Theano library (Bergstra et al., 2010; Bastien et al., 2012), and trained by partitioning each of the collected corpus into a training, validation, and testing set in the ratio 3:1:1.
113	21	The frequency of each action type and slot-value pair differs quite markedly across the corpus, hence up-sampling was used to make the corpus more uniform.
121	22	We compared the single layer semantically controlled LSTM (sc-lstm) and a deep version with two hidden layers (+deep) against several baselines: the handcrafted generator (hdc), k-nearest neighbour (kNN), class-based LMs (classlm) as proposed in Oh and Rudnicky (2000), the heuristic gated RNN as described in Wen et al. (2015) and a similar LSTM variant (rnn w/ & lstm w/), and the same RNN/LSTM but without gates (rnn w/o & lstm w/o).
123	61	The kNN was implemented by computing the similarity of the test DA 1-hot vector against all of the training DA 1-hot vectors, selecting the nearest and then lexicalising to generate the final surface form.
126	43	Setting aside the difficulty of scaling to large domains, the handcrafted generator’s (hdc) use of predefined rules yields a fixed set of sentence plans, which can differ markedly from the real colloquial human responses collected from AMT, while the class LM approach suffers from inaccurate rendering of information.
130	56	We also found that by using gates, whether learned or heuristic, gave much lower slot error rates.
135	42	For each task, two systems among the four (classlm, rnn w/, sc-lstm, and +deep) were randomly selected to generate utterances from a set of newly sampled dialogues in the restaurant domain.
136	75	In order to evaluate system performance in the presence of language variation, each system generated 5 different surface realisations for each input DA and the human judges were asked to score each of them in terms of informativeness and naturalness (rating out of 3), and also asked to state a preference between the two.
137	103	Here informativeness is defined as whether the utterance contains all the information specified in the DA, and naturalness is defined as whether the utterance could plausibly have been produced by a human.
149	52	We found that the SCLSTM model achieved the best overall performance on two objective metrics across two different domains.
151	35	This work represents a line of research that tries to model the NLG problem in a unified architecture, whereby the entire model is end-to-end trainable from data.
152	18	We contend that this approach can produce more natural responses which are more similar to colloquial styles found in human conversations.
154	20	This suggests that it should be possible to further condition the generator on some dialogue features such discourse information or social cues during the conversation.
156	18	These latter aspects will be the focus of our future work in this area.
