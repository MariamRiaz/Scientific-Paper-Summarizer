6	17	We do this in two steps: First, given bilingual sentence pairs and the associated word alignments, we convert the information into uniquely defined linear sequences.
9	37	Second, we train an n-gram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998) on the resulting JTR sequences.
20	34	To address this, we resort to neu- 1401 ral networks (NNs), as they have been successfully applied to machine translation recently (Sundermeyer et al., 2014; Devlin et al., 2014).
46	22	Nevertheless, JTR models utilize linear sequences of dependencies and combine the translation of bilingual word pairs and reoderings into a single model.
49	18	An HMM approach for word-to-phrase alignments was presented in (Deng and Byrne, 2005), showing performance similar to IBM Model 4 on the task of bitext alignment.
56	20	In (Sutskever et al., 2014), a recurrent NN was used to encode a source sequence, and output a target sentence once the source sentence was fully encoded in the network.
66	67	Each JTR token is either an aligned bilingual word pair 〈 f ,e〉 or a reordering class ∆ j′ j. Unaligned words on the source and target side are processed as if they were aligned to the empty word ε .
68	42	Each word of the source and target sentences is to appear in the corresponding JTR sequence exactly once.
73	56	Similar to Feng and Cohn (2013), we classify the reordered source positions j′ and j by ∆ j′ j: ∆ j′ j =  step backward (←), j = j′−1 jump forward (y), j > j′+1 jump backward (x), j < j′−1.
76	20	At first, g K 1 is initialized by an empty sequence (line 2).
92	31	As the JTR sequence gK1 is a unique interpretation of a bilingual sentence pair and its alignment, the probability p( f J1 ,e I 1,b I 1) can be computed as: p( f J1 ,e I 1,b I 1) = p(g K 1 ).
93	43	(1) The probability of gK1 can be factorized and approximated by an n-gram model.
94	28	p(gK1 ) = K ∏ k=1 p(gk|gk−1k−n+1) (2) Within this work, we first estimate the Viterbi alignment for the bilingual training data using GIZA++ (Och and Ney, 2003).
98	19	Within the phrase-based decoder, we extend each search state such that it additionally stores the JTR model history.
101	39	On the other hand, we represent long-range reorderings between phrases by the coverage vector and limit them by reordering constraints.
117	29	Therefore, we split JTR tokens gk and use individual words as input and output units, such that the NN receives jumps, source and target words as input and outputs target words and jumps.
125	20	The resulting sequences can then be used with existing NN architectures, without further modifications to the design of the networks.
130	29	(4) It scores the JTR target word tk at position k using the current source word sk, and the history of n JTR source words.
136	18	The BRNN uses the JTR target side as well as the full JTR source side as context, and it is given by: p(tK1 |sK1 ) = K ∏ k=1 p(tk|tk−11 ,sK1 ) (5) This equation is realized by a network that uses forward and backward recurrent layers to capture the complete source sentence.
142	18	It will be referred to as the unidirectional recurrent neural network (URNN) model in the experiments.
157	42	All baselines contain phrasal and lexical smoothing models for both directions, word and phrase penalties, a distance-based reordering model, enhanced low frequency features (Chen et al., 2011), a hierarchical reordering model (HRM) (Galley and Manning, 2008), a word class LM (Wuebker et al., 2013) and an n-gram LM.
166	46	The short lists contain the 10k most frequent words, and all remaining words are clusterd into 1000 word classes.
169	17	The URNN has 2 hidden layers while the BRNN has one forward, one backward and one additional hidden layer.
212	66	This is shown when adding the in-domain KN JTR model on top of the model trained on full data, improving it by up to 0.4 BLEU.
218	39	Rescoring the KN JTR with the FFNN improves it by up to 0.3 BLEU leading to an overall improvement between 0.5 and 1.0 BLEU.
231	33	The system translates “kommen” to “come”, jumps forward to “zurück”, translates it to “back”, then jumps back to continue translating the word “später”.
235	19	We introduced a method that converts bilingual sentence pairs and their word alignments into joint translation and reordering (JTR) sequences.
245	48	The JTR models are not dependent on the phrase-based framework, and one of the longterm goals is to perform standalone decoding with the JTR models independently of phrase-based systems.
246	119	Without the limitations introduced by phrases, we believe that JTR models could perform even better.
247	21	In addition, we aim to use JTR models to obtain the alignment, which would then be used to train the JTR models in an iterative manner, achieving consistency and hoping for improved models.
