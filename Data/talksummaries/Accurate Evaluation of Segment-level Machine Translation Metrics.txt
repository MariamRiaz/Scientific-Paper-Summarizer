0	57	Automatic segment-level machine translation (MT) metrics have the potential to greatly advance MT by providing more fine-grained error analysis, increasing efficiency of system tuning methods and leveraging techniques for system hybridization.
5	19	For evaluation of metrics that operate at the system or document-level such as BLEU, inconsistency in individual human judgments can, to some degree, be overcome by aggregation of individual human assessments over the segments within a document.
8	18	We examine the accuracy of segment scores collected with our proposed method by replicating components of the WMT-13 human evaluation (Bojar et al., 2013b), with the sole aim of optimizing agreement in segment scores to provide an effective gold standard for evaluating segment-level metrics.
15	52	Performance of a segment-level metric is assessed by the degree to which it corresponds with human judgment, measured by the number of metric scores for pairs of translations that are either concordant (Con) or discordant (Dis) with those of a human assessor, which the organizers describe as “Kendall’s τ”: τ = |Con| − |Dis| |Con|+ |Dis| Pairs of translations deemed equally good by a human assessor are omitted from evaluation of segment-level metrics (Bojar et al., 2014).
18	30	Since the human assessment data is, however, a large number of separately ranked sets of five competing translations and not a single ranking of all translations, it is not possible to compute a single Kendall’s τ correlation.1 The formula used to assess the performance of a metric in the task, therefore, is not what is ordinarily understood to be a Kendall’s τ coefficient, but, in fact, equivalent to a weighted average of all Kendall’s τ for each humanranked set of five translations.
23	40	The numerator (|Con| − |Dis|) is what determines our evaluation of the relative performance of metrics, and although the formula appears to be a straightforward subtraction of counts of concordant and discordant pairs, due to the large numbers of contradictory human relative preference judgments in data sets, what this number actually represents is not immediately obvious.
24	37	If, for example, translations A and B were scored by a metric such that metric score(A) > metric score(B), one might expect an addition or subtraction of 1 depending on whether or not the metric’s scores agreed with those of a human.
25	26	Instead, however, the following is added: (max(|A > B|, |A < B|) −min(|A > B|, |A < B|))× d where: |A > B| = # human judgments where A was preferred over B |A < B| = # human judgments where B was preferred over A d = { 1 if |A < B| > |A > B| −1 if |A < B| < |A > B| For example, translations of segment 971 for Czechto-English systems uedin-heafield and uedin-wmt13 were compared by human assessors a total of 12 times: the first system was judged to be best 4 times, the second system was judged to be best 2 times, and the two systems were judged to be equal 6 times.
28	72	In the current setup, tied translation pairs are excluded from the data, meaning that the ability for evaluation metrics to evaluate similar translations is not directly evaluated, and a metric that manages to score two equal quality translations closer, does not receive credit.
29	19	A segment-level metric that can accurately predict not just disparities between translations but also similarities is likely to have high utility for MT system optimization, and is possibly the strongest motivation for developing segment-level metrics in the first place.
30	19	In WMT-13, however, 24% of all relative preference judgments were omitted on the basis of ties, broken down as follows: • Spanish-to-English: 28% • French-to-English: 26% • German-to-English: 27% • Czech-to-English: 25% • Russian-to-English: 24% • English-to-Spanish: 23% • English-to-French: 23% • English-to-German: 20% • English-to-Czech: 16% • English-to-Russian: 27% Although significance tests for evaluation of MT systems and document-level metrics have been identified (Koehn, 2004; Graham and Baldwin, 2014; Graham et al., 2014b), no such test has been proposed for segment-level metrics, and it is unfortunately common to conclude success without taking into account the fact that an increase in correlation can occur simply by chance.
32	43	However, such tests do not provide insight into whether or not a metric outperforms another, as all that’s required for rejection of the null hypothesis with such a test is a likelihood that an individual metric’s correlation with human judgment is not equal to zero.
33	24	In addition, data sets for evaluation in both document and segment-level metrics are not independent and the correlation that exists between pairs of metrics should also be taken into account by significance tests.
34	21	Many human evaluation methodologies attempt to elicit precisely the same quality judgment for individual translations from all assessors, and inevitably produce large numbers of conflicting assessments in the process, including from the same individual human judge (Callison-Burch et al., 2007; CallisonBurch et al., 2008; Callison-Burch et al., 2009).
39	34	What the law of large numbers does not tell us, however, is, for our particular case of translation quality assessment, precisely how large the sample of assessments needs to be, so that the mean of scores provides a close enough estimate to the true mean score for any translation.
50	18	MTurk was used to collect large numbers of translation assessments, in sets of 100 translations per assessment task (or “HIT” in MTurk parlance).
53	33	The assessment task was posed as a monolingual task, where assessors were asked to rate the degree to which the MT system output adequately expressed the meaning of the corresponding reference translation.
56	65	For Spanish-toEnglish, for example, a total of (280 translations + 120 translations for quality-control purposes) × 40 assessments per translation × 2 separate data collections × ∼2 to allow for filtering of low-quality assessors = ∼64k assessments were collected; after quality control filtering and removing the qualitycontrol translations, around 22k assessments were used for the actual experiment.
58	45	For each language pair, we calculate the correlation first over the raw segment scores and second over standardized scores, based on the method of Graham et al.
59	63	(2014a).2 For all language pairs, although the correlation is relatively low for single assessments, as the sample size increases, it increases, and by approximately N = 15 assessments, for all four language pairs, the correlation reaches r = 0.9.
60	73	For Spanish-to-English, for which most assessments were collected, when we increase the number of assessments to N = 40 per translation, the correlation reaches r = 0.97.
61	120	Figure 2 is a set of scatter plots for mean segment-level scores for Spanish-toEnglish rising, for varying sample sizes N .
68	40	It is important to point out, however, that moving from Kendall’s τ over relative preference judgments to Pearson’s r over absolute scores does, in fact, change the task required of metrics in one respect: previously, there was no direct evaluation of the scores generated by a metric, nor indeed did the evaluation ever directly compare translations for different source language inputs (as relative preference judgments were always relative to other translations for the same input).
77	34	Overall, when we compare correlations using the new evaluation methodology to those from the original evaluation, even though we have raised the bar by assessing the raw numeric outputs rather than translating them into preference judgments relative to other translations for the same SL input, all metrics achieve higher correlation with human judgment than reported in the original evaluation.
80	18	Figure 3 is a heat map of the Pearson’s correlation between each pair of segment-level metrics for Spanish-to-English from WMT-13, and Figure 4 shows correspondence between scores of three segment-level metrics with our human evaluation data.
84	20	Three metrics prove not to be significantly outperformed by any other metric for Spanish-to-English, and tie for best performance: METEOR (Denkowski and Lavie, 2011), NLEPOR (Han et al., 2013) and SENTBLEU-MOSES (sBLEU-moses).
88	36	After removal of quality control items, this leaves 70 distinct translations per language pair, combined into a cross-lingual test set of 630 distinct translations spanning nine language pairs.
91	130	However, since the latter two were shown to be outperformed for Spanish-to-English, all else being equal, METEOR, SENTBLEU-MOSES and NLEPOR are still a superior choice of default metric.
92	113	We presented a new evaluation methodology for segment-level metrics that overcomes the issue of low inter-annotator agreement levels in human assessments, includes evaluation of very close and equal quality translations, and provides a significance test that supports system comparison with confidence.
93	52	Our large-scale human evaluation reveals three metrics to not be significantly outperformed by any other metric in both Spanish-to- English and a combined evaluation across nine language pairs, namely: METEOR, NLEPOR and SENTBLEU-MOSES.
