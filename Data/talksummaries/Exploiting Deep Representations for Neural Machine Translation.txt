0	15	Neural machine translation (NMT) models have advanced the machine translation community in recent years (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014).
13	34	In addition, we introduce an auxiliary objective to encourage different layers to capture diverse information, which we believe would make the deep representations more meaningful.
16	30	Experimen- tal results show that exploiting deep representations consistently improves translation performance over the vanilla TRANSFORMER model across language pairs.
28	16	The decoder is also composed of a stack of L identical layers.
32	20	However, one potential problem about the vanilla Transformer, as shown in Figure 1a, is that both the encoder and decoder stack layers in sequence and only utilize the information in the top layer.
34	19	Although residual connections have been incorporated to combine layers, these connections have been “shallow” themselves, and only fuse by simple, one-step operations (Yu et al., 2018).
38	23	Then, to explicitly encourage different layers to incorporate various information, we propose one way to measure the diversity between layers and add a regularization term to our objective function to maximize the diversity across layers (Sec 3.2).
40	52	While layer aggregation strategies combine hidden states at the same position across different layers, multi-layer attention allows the model to combine information in different positions.
49	15	As shown in Figure 1c, an intuitive strategy is to linearly combine the outputs of all layers: Ĥ = L∑ l=1 WlH l, (5) where {W1, .
55	24	Aggregation begins at the shallowest, smallest scale and then iteratively merges deeper, larger scales.
97	26	Accordingly, the training speed decreases due to more efforts to train the new parameters.
98	26	Layer aggregation mechanisms only marginally decrease decoding speed, while multi-layer attention decreases decoding speed by 21% due to an additional attention process for each layer.
99	44	Layer Aggregation (Rows 2-5): Although dense connection and linear combination only marginally improve translation performance, iterative and hierarchical aggregation strategies achieve more significant improvements, which are up to +0.99 BLEU points better than the baseline model.
101	38	Multi-Layer Attention (Rows 6-7): Benefiting from the power of attention models, multi-layer attention model can also significantly outperform baseline, although it only attends to one or two additional layers.
102	15	However, increasing the number of lower layers to be attended from k = 2 to k = 3 only gains marginal improvement, at the cost of slower training and decoding speeds.
103	21	In the following experiments, we set set k = 2 for the multi-layer attention model.
104	34	Layer Diversity (Rows 8-10): The introduced diversity regularization consistently improves performance in all cases by encouraging different layers to capture diverse information.
107	26	Main Results Table 2 lists the results on both WMT17 Zh⇒En and WMT14 En⇒De translation tasks.
108	14	As seen, exploiting deep represen- tations consistently improves translation performance across model variations and language pairs, demonstrating the effectiveness and universality of the proposed approach.
110	86	We conducted extensive analysis from different perspectives to better understand our model.
112	32	Sentence Length B LE U 22 23 24 25 26 27 28 29 30 31 Length of Source Sentence (0, 10 ] (10 , 2 0] (20 , 3 0] (30 , 4 0] (40 , 5 0] (50 , ) Ours Base B LE U 25 26 27 28 29 30 Length of Source Sentence (0, 15 ] (15 , 3 0] (30 , 4 5] > 4 5 Ours Base B LE U 25 26 27 28 29 30 Length of Source Sentence (0, 15 ] (15 , 3 0] (30 , 4 5] > 4 5 Hier.+Div.
114	14	“Hier.” denotes hierarchical aggregation and “Div.” denotes diversity regularization.
122	77	We conjecture that complex long sentences may need to store duplicate information across layers, which conflicts with the diversity objective.
124	29	In this experiment, we investigated how our models affect the two components, as shown Model Applied to BLEU Encoder Decoder BASE N/A N/A 26.13 OURS X × 26.32 × X 26.41 X X 26.69 Table 3: Experimental results of applying hierarchical aggregation to different components on En⇒De validation set.
132	14	This indi- cates that cross-layer connections are necessary to avoid the gradient vanishing problem.
136	24	Let Hi = {H2i, H2i−1, Ĥ i−1} be the input representations, we calculated the exploitation of the j-th input as sj = ∑ w∈Wj |w|∑ Hj′∈H { ∑ w′∈Wj′ |w′|} , (11) where Wj is the parameter matrix associated with the input Hj .
158	28	Specifically, the hierarchical aggregation with diversity regularization achieves the best performance by incorporating more depth and sharing across layers and by encouraging layers to capture different information.
159	37	Experimental results on WMT14 English⇒German and WMT17 Chinese⇒English show that the proposed approach consistently outperforms the state-of-theart TRANSFORMER baseline by +0.54 and +0.63 BLEU points, respectively.
160	49	By visualizing the aggregation process, we find that our model indeed utilizes lower layers to effectively fuse the information across layers.
161	15	Future directions include validating our approach on other architectures such as RNN (Bahdanau et al., 2015) or CNN (Gehring et al., 2017) based NMT models, as well as combining with other advanced techniques (Shaw et al., 2018; Shen et al., 2018; Yang et al., 2018; Li et al., 2018) to further improve the performance of TRANSFORMER.
