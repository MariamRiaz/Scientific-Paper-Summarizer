8	12	In this sense, sparsification via variational inference with log uniform prior falls into the same category of non-Bayesian approaches as, for example, Lasso (Tibshirani, 1996).
9	9	Specifically, the approximate uncertainty estimates do not have the usual interpretation, and the model may exhibit overfitting.
10	104	Consequently, we study the objective from a non-Bayesian perspective, proving that the optimised objective is impervious to some of the described pathologies due to the properties of the variational formulation itself, which might explain why the algorithm can still provide good empirical results.1 Section 4 shows how mismatch between support of the approximate and the true posterior renders application of the standard Variational Inference (VI) impossible by making the Kullback-Leibler (KL) divergence undefined.
11	147	As the second main contribution, we address this issue by proving that the remedies to this problem proposed in (Gal & Ghahramani, 2016; Gal, 2016) are special cases of a broader class of limiting constructions leading to a unique objective which we name Quasi-KL (QKL) divergence.
12	246	Section 5 provides initial discussion of QKL’s properties, uses those to suggest an explanation for the empirically observed difficulty in tuning hyperparameters of the true model (e.g. Gal (2016, p. 119)), and demonstrates the potential of QKL on an illustrative example where we try to approximate a full rank Gaussian distribution with a degenerate one using QKL, only to arrive at the well known Principal Component Analysis (PCA) algorithm.
13	151	Assume we have a discriminative probabilistic model y |x,W ∼ P(y |x,W ) where (x, y) is a single inputoutput pair, and W is the set of model parameters generated from a prior distribution P(W ).
14	21	In Bayesian inference, we usually observe a set of data points (X,Y ) = {(xn, yn)}Nn=1 and aim to infer the posterior p(W |X,Y ) ∝ p(W ) ∏ n p(yn |xn,W ),2 which can be subsequently used to obtain the posterior predictive density p(Y ′ |X ′,X,Y ) = ∫ p(Y ′ |X ′,W )p(W |X,Y )dW .
15	96	If p(y |x,W ) is a complicated function ofW like a neural network, both tasks often become computationally infeasible and thus we need to turn to approximations.
16	8	Variational inference approximates the posterior distribution over a set of latent variablesW by maximising the evidence lower bound (ELBO), L(q) = E Q(W ) [log p(Y |X,W )]−KL (Q(W )‖P(W )) , with respect to (w.r.t.)
17	53	an approximate posterior Q(W ).
18	18	If Q(W ) is parametrised by ψ and the ELBO is differentiable w.r.t.
20	42	We can then approximate the density of posterior predictive distribution using q(Y ′ |X ′,X,Y ) = ∫ p(Y ′ |X ′,W )q(W )dW , usually by Monte Carlo integration.
22	8	BNN differs from a standard NN by assuming a prior over the weightsW .
23	65	One of the main advantages of BNNs over standard NNs is that the posterior predictive distribution can be used to quantify uncertainty when predicting on previously unseen data (X ′,Y ′).
