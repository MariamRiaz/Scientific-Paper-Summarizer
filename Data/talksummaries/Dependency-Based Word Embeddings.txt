17	12	Specifically, 1code.google.com/p/word2vec/ 302 following work in sparse vector-space models (Lin, 1998; Padó and Lapata, 2007; Baroni and Lenci, 2010), we experiment with syntactic contexts that are derived from automatically produced dependency parse-trees.
24	7	To the best of our knowledge, this is the first work to suggest such an analysis of discriminativelytrained word-embedding models.
28	47	The entries in the vectors are latent, and treated as parameters to be learned.
32	9	Did this pair come from the data?
39	11	The negative samples D′ can be constructed in various ways.
50	39	Linear Bag-of-Words Contexts This is the context used by word2vec and many other neural embeddings.
52	40	In our example, the contexts of discovers are Australian, scientist, star, with.2 Note that a context window of size 2 may miss some important contexts (telescope is not a context of discovers), while including some accidental ones (Australian is a context discovers).
66	10	Notice that syntactic dependencies are both more inclusive and more focused than bag-ofwords.
67	16	They capture relations to words that are far apart and thus “out-of-reach” with small window bag-of-words (e.g. the instrument of discover is telescope/prep with), and also filter out “coincidental” contexts which are within the window but not directly related to the target word (e.g. Australian is not used as the context for discovers).
70	9	We experiment with 3 training conditions: BOW5 (bag-of-words contexts with k = 5), BOW2 (same, with k = 2) and DEPS (dependency-based syntactic contexts).
71	94	We modified word2vec to support arbitrary contexts, and to output the context embeddings in addition to the word embeddings.
75	23	For DEPS, the corpus was tagged with parts-of-speech using the Stanford tagger (Toutanova et al., 2003) and parsed into labeled Stanford dependencies (de Marneffe and Manning, 2008) using an implementation of the parser described in (Goldberg and Nivre, 2012).
77	20	This resulted in a vocabulary of about 175,000 words, with over 900,000 distinct syntactic contexts.
78	14	We report results for 300 dimension embeddings, though similar trends were also observed with 600 dimensions.
83	20	In Hogwarts - the school of magic from the fictional Harry Potter series - it is evident that BOW contexts reflect the domain aspect, whereas DEPS yield a list of famous schools, capturing the semantic type of the target word.
84	43	This observation holds for Turing3 and many other nouns as well; BOW find words that associate with w, while DEPS find words that behave like w. Turney (2012) described this distinction as domain similarity versus functional similarity.
91	17	We supplement the examples in Table 1 with quantitative evaluation to show that the qualitative differences pointed out in the previous section are indeed widespread.
95	16	We then draw a recall-precision curve that describes the embedding’s affinity towards one subset (“similarity”) over another (“relatedness”).
98	22	We repeated the experiment with a different dataset (Chiarello et al., 1990) that was used by Turney (2012) to distinguish between domain and functional similarities.
101	11	Neural word embeddings are often considered opaque and uninterpretable, unlike sparse vector space representations in which each dimension corresponds to a particular known context, or LDA models where dimensions correspond to latent topics.
102	17	While this is true to a large extent, we observe that SKIPGRAM does allow a non-trivial amount of introspection.
105	16	If we keep the context embeddings, we can query the model for the contexts that are most activated by (have the highest dot product with) a given target word.
106	9	By doing so, we can see what the model learned to be a good discriminative context for the word.
109	17	Additionally, the collapsed preposition relation is very useful (e.g. for capturing the school aspect of hogwarts).
110	25	The presence of many conjunction contexts, such as superman/conj for batman and singing/conj for dancing, may explain the functional similarity observed in Section 4; conjunctions in natural language tend to enforce their conjuncts to share the same semantic types and inflections.
112	43	We presented a generalization of the SKIPGRAM embedding model in which the linear bagof-words contexts are replaced with arbitrary ones, and experimented with dependency-based contexts, showing that they produce markedly different kinds of similarities.
113	7	These results are expected, and follow similar findings in the distributional semantics literature.
114	45	We also demonstrated how the resulting embedding model can be queried for the discriminative contexts for a given word, and observed that the learning procedure seems to favor relatively local syntactic contexts, as well as conjunctions and objects of preposition.
115	84	We hope these insights will facilitate further research into improved context modeling and better, possibly task-specific, embedded representations.
116	43	Our software, allowing for experimentation with arbitrary contexts, together with the embeddings described in this paper, are available for download at the authors’ websites.
