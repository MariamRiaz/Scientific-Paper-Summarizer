20	44	Our main results on the spectrum of random feature maps will be presented in Section 3, followed by experiments on two types of classification tasks in Section 4.
24	14	We assume that the data vector xi follows a Gaussian mixture model1, i.e., xi = µa/ p p+ !i with !i ⇠ N (0,Ca/p) for some mean µa 2 Rp and covariance Ca 2 Rp⇥p of associated class Ca.
25	92	We denote the data matrix X = ⇥ x1, .
27	79	To extract random features, X is premultiplied by some random matrix W 2 Rn⇥p with i.i.d.
29	82	In this article, we focus on the Gram matrix G ⌘ 1 n ⌃T⌃ of the random features, the entry (i, j) of which is given by Gij = 1 n (Wxi) T (Wxj) = 1 n nX k=1 (wT k xi) (w T k xj) with wT k the k-th row of W. Note that all wk follow the same distribution, so that taking expectation over w ⌘ wk of the above equation one results in the average kernel matrix , with the (i, j) entry of which given by (xi,xj) = EwGij = Ew (wTxi) (wTxj).
30	11	(1) When the entries of W follow a standard Gaussian distribution, one can compute the generic form (a,b) = Ew (wTa) (wTb) by applying the integral trick from (Williams, 1997), for a large set of nonlinear functions (·) and arbitrary vector a,b of appropriate dimension.
31	6	We list the results for commonly used functions in Table 1.
32	29	Since the Gram matrix G describes the correlation of data in the feature space, it is natural to recenter G, and thus by pre- and post-multiplying a projection matrix P ⌘ IT 1T 1T1 T T .
33	4	In the case of , we get c ⌘ P P. In the recent line of works (Louart et al., 2018; Pennington & Worah, 2017), it has been shown that the large dimensional (large n, p, T ) characterization of G, in particular its eigenspectrum, is fully determined by and the ratio n/p.
40	10	Assumption 1 ensures that the information of means or covariances is neither too simple nor impossible to be extracted from the data, as investigated in (Couillet et al., 2016).
42	9	Under Assumption 1, note that for xi 2 Ca and xj 2 Cb, i 6= j, xT i xj = !
48	60	A critical aspect of the analysis where random matrix theory comes into play now consists in developing as a sum of matrices arising from the Taylor expansion and ignoring terms that give rise to a vanishing operator norm, so as to find an asymptotic equivalent matrix ̃ such that k ̃k !
58	28	On the Spectrum of Random Features Maps of High Dimensional Data almost surely, with ̃c = P̃P and ̃ ⌘ d1 ✓ ⌦+M JT p p ◆T✓ ⌦+M JT p p ◆ +d2UBU T+d0IT where we recall that P ⌘ IT 1T 1T1 T T and U ⌘ h Jp p , i , B ⌘  ttT + 2S t tT 1 with the coefficients d0, d1, d2 given in Table 2.
77	1	Remark 1 (Constant shift in feature space).
83	19	Ignoring the coefficient d0 that gives rise to a constant shift of all eigenvalues of ̃c and thus of no practical relevance, observe from Table 2 that by tuning the parameters of the quadratic and Leaky ReLU functions (LReLU(t)), one can select arbitrary positive value for the ratio d1/d2, while the other listed functions have constraints linking d1 to d2.
84	204	Following the discussions in Remark 2, the parameters &+, & of the LReLU, as well as &1, &2 of the quadratic function, essentially act to balance the weights of means and covariances in the mixture model of the data.
85	45	1 or &2 &1, more emphasis is set on the “distance” between covariance matrices while &+ & !
86	97	1 or &1 &2 stresses the differences in means.
87	62	In Figure 5, spectral clustering on four classes of Gaussian data is performed: N (µ1,C1), N (µ1,C2), N (µ2,C1) and N (µ2,C2) with the LReLU function that takes different values for &+ and & .
88	8	For a = 1, 2, µa =⇥ 0a 1; 5;0p a ⇤ and Ca = ⇣ 1 + 15(a 1)p p ⌘ Ip.
90	22	However, by taking &+ = 1, & = 0 (the ReLU function) we distinguish all four classes in the leading two eigenvectors, to which the k-means method can then be applied for final classification, as shown in Figure 6.
91	47	Of utmost importance for random feature-based spectral methods (such as kernel spectral clustering discussed above (Ng et al., 2002)) is the presence of informative eigenvectors in the spectrum of G, and thus of c. To gain a deeper understanding on the spectrum of c, one can rewrite ̃ in the more compact form, ̃ = d1⌦ T⌦+VAVT + d0IT (2) where V ⌘ h Jp p , ,⌦TM i , A ⌘ 2 4 A11 d2t d1IK d2tT d2 0 d1IK 0 0 3 5 with A11 ⌘ d1MTM + d2(ttT + 2S), that is akin to the so-called “spiked model” in the random matrix literature (Baik et al., 2005), as it equals, if d1 6= 0, the sum of some standard (noise-like) random matrix ⌦T⌦, and a low rank (here up to 2K + 1) informative matrix VAVT, that may induce some isolated eigenvalues outside the main bulk of eigenvalues in the spectrum of ̃c, as shown in Figure 1.
92	17	The eigenvectors associated to these eigenvalues often contain crucial information about the data statistics (the classes in a classification settings).
93	90	In particular, note that the matrix V contains the canonical vector ja of class Ca and we thus hope to find some isolated eigenvector of c aligned to ja that can be directly used to perform clustering.
94	5	Intuitively speaking, if the matrix A contains sufficient energy (has sufficiently large operator norm), the eigenvalues associated to the small rank matrix VAVT may jump out from the main bulk of ⌦T⌦ and becomes “isolated” as in Figure 1, referred to as the phase transition phenomenon in the random matrix literature (Baik et al., 2005).
96	87	This alignment between the isolated eigenvectors and ja is essentially measured by the amplitude of the eigenvalues of the matrix A11, or more concretely, the statistical differences of the data (namely, t, S and M).
97	117	Therefore, a good adaptation of the ratio d1/d2 ensures the (asymptotic) detectability of different classes from the spectrum of c. On the Spectrum of Random Features Maps of High Dimensional Data
99	47	We consider two different types of classification tasks: one on handwritten digits of the popular MNIST (LeCun et al., 1998) database (number 6 and 8), and the other on epileptic EEG time series data (Andrzejak et al., 2001) (set B and E).
