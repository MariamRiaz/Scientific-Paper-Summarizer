25	25	The rest of this paper is organized as follows: Section 2 introduces work related to applying neural network to document modeling and SMT.
26	14	Section 3 introduces the general framework for document modeling.
57	36	Statistical language model assigns a probability to a natural language sequence.
59	68	For sentences in one document talking about one or several specific topics, the adjacent sentences should be in a coherent order.
60	13	Therefore, the words in the next sentence are also dependent on the preceding sentences.
69	56	Because recurrent neural network has a natural advantage in processing sequential data, we investigate how to model the whole process under a unified framework of recurrent neural network.
70	18	In this section, we describe how to leverage recurrent neural network for sentence-level language modeling.
73	14	Here we adept this framework for a RNN based sentence-level language modeling, i.e. RNNSLM.
74	49	A conventional language model reads a word each time, keeps several words as history and then predict the probability distribution of the next word.
75	55	Similar to this, our sentence-level language model reads a sentence which is a bag of words representation.
79	18	As shown in Figure 1, similar to the conventional recurrent neural network, for the sentence j, our network has two input layers xsj and hsj−1.
80	58	xsj is the current sentence representation, and hsj−1 is the history information vector before sentence j.
82	13	The layers are computed as follows: hsj = f(Us · hsj−1 +Ws · xsj) (4) ysj+1 = g(Vs · hsj) (5) where Ws, Us and Vs denote the weight matrix.
85	32	As mentioned in (Mikolov, 2012), this is kind of maximum entropy feature which can be derived by a two-layer neural network.
143	18	The document with the higher sum of log probability for each adjacency sentences is regarded as the original document.
150	58	Then we train our model with the same hidden layer size and hash array size as the baseline system.
151	20	The perplexity of these two models is evaluated on held-out documents, about 370K words.
180	18	It can select the best translation “some of” for 有些, and the output of our system is: Some of the ice more than 100,000 years.
181	40	We also calculate the BLEU increase ratio of our system on document level.
183	35	The results are shown in Table 4.
184	15	From Table 4, we can find that, for all the three test data sets, our reranking system can achieve better performance for more than 70% documents.
185	39	In this paper, we propose a hierarchical recurrent neural network language model for document modeling.
186	21	We first built a RNNSLM to capture the information between sentences.
188	37	This enables the model be able to capture both in-sentence and cross-sentence information in a unified RNN.
189	18	Compared with conventional language models, our model can perceive a longer history than other language models and captures the context patterns in the previous sentences.
190	13	At sentence level, we examine our model with sentence ordering task.
191	31	At word level, we test the model perplexity.
193	38	All these experimental results show that our hierarchical recurrent neural network has a satisfying performance.
194	88	In the future, we will explore better sentence representation such as distributed sentence representation as input for our sentence-level language model to better model document coherence.
195	232	We can even update the gradient from different RNN to get a better performance.
