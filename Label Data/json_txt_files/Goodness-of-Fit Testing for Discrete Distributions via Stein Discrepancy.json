{"sections": [{"heading": "1. Introduction", "text": "Goodness-of-fit testing is a central problem in statistics, measuring how well a model distribution p(x) fits observed data {xi}ni=1 \u2286 X d, for some domain X (e.g., X \u2286 R for continuous data or X \u2286 N for discrete data). Examples of classical goodness-of-fit tests include the \u03c72 test (Pearson, 1900), the Kolmogorov-Smirnov test (Kolmogorov, 1933; Smirnov, 1948), and the Anderson-Darling test (Anderson & Darling, 1954). These tests typically assume that the model distribution p(x) is fully specified and is easy to evaluate. In modern statistical and machine learning applications, however, p(x) is often specified only up to an intractable normalization constant; examples include large-scale graphical models, latent variable models, and statistical models\n*Equal contribution 1Department of Statistics, Purdue University, West Lafayette, IN 2Department of Computer Science, The University of Texas at Austin, Austin, TX 3Department of Computer Science, Purdue University, West Lafayette, IN. Correspondence to: Jiasen Yang <jiaseny@purdue.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nfor network data. While a variety of approximate inference techniques such as pseudo-likelihood estimation, Markov chain Monte Carlo (MCMC), and variational methods have been studied to allow learning and inference in these models, it is usually hard to quantify the approximation errors involved, making it difficult to establish statistical tests with calibrated uncertainty estimates.\nRecently, a new line of research (Gorham & Mackey, 2015; Oates et al., 2017; Chwialkowski et al., 2016; Liu et al., 2016; Gorham & Mackey, 2017; Jitkrittum et al., 2017) has developed goodness-of-fit tests which work directly with un-normalized model distributions. Central to these tests is the notion of a Stein operator, originating from Stein\u2019s method (Stein, 1986) for characterizing convergence in distribution. Given a distribution p(x) on X d and a class of test functions f \u2208 F on X d, a Stein operator Ap satisfies Ex\u223cp [Apf(x)] = 0, so that when Ap is applied to any test function f , the resulting function Apf has zero-expectation under p. Additionally, the expectation under any other distribution q 6= p should be non-zero for at least some function f in F . When F is sufficiently rich, the maximum value supf\u2208F Ex\u223cq [Apf(x)] serves as a discrepancy measure, called Stein discrepancy, between distributions p and q.\nThe properties of the Stein discrepancy measure depends on two objects: the Stein operator Ap, and the set F . Different authors have studied different choices of F : Gorham & Mackey (2015) considered test functions in the W2,\u221e Sobolev space, and the resulting test statistic requires solving a linear program under certain smoothness constraints. On the other hand, Oates et al. (2017); Chwialkowski et al. (2016); Liu et al. (2016) proposed taking F to be the unit ball of a reproducing kernel Hilbert space (RKHS), which leads to test statistics that can be computed in closed form and with time quadratic in n, the number of samples. Jitkrittum et al. (2017) further proposed a linear-time adaptive test that constructs test features by optimizing test power.\nRegarding the choice of the Stein operator Ap, all the aforementioned works consider the case when X \u2286 R is a continuous domain, p(x) is a smooth density on X d, and the Stein operator is defined in terms of the score function of p, sp(x) = \u2207 log p(x) = \u2207p(x)/p(x), where \u2207 is the gradient operator. Observe that any normalization constant in p cancels out in the score function, so that if the Stein operatorAp\ndepends on p only through sp, then the discrepancy measure supf\u2208F Ex\u223cq [Apf(x)] can still be computed when p is unnormalized. However, constructing the Stein operator using the gradient becomes restrictive when one moves beyond distributions with smooth densities. For discrete distributions, even in the simple case of Bernoulli random variables, none of the aforementioned tests apply, since the probability mass function is no longer differentiable. This motivates more general constructions of tests based on Stein\u2019s method that would also be applicable to discrete domains.\nIn this work, we focus on the case where X is a finite set. The model distribution p(x) is a probability mass function (pmf), whose normalization constant is computationally intractable. We note that examples of such intractable discrete distributions abound in statistics and machine learning, including the Ising model (Ising, 1924) in physics, the (Bernoulli) restricted Boltzmann machine (RBM) (Hinton & Salakhutdinov, 2006) for dimensionality reduction, and the exponential random graph model (ERGM) (Holland & Leinhardt, 1981) in statistical network analysis.\nOur primary contribution is in establishing a kernelized Stein discrepancy measure between discrete distributions, using an appropriate choice of Stein operators for discrete spaces. Then, adopting a similar strategy as Chwialkowski et al. (2016); Liu et al. (2016), we develop a nonparametric goodness-of-fit test for discrete distributions. Notably, the proposed test also applies to discrete distributions that were previously not amenable to classical tests due to the presence of intractable normalization constants. Furthermore, we propose a general characterization of Stein operators that encompasses both discrete and continuous distributions, providing a recipe for constructing new Stein operators. For any Stein operator constructed as such, we could then define a kernelized Stein discrepancy measure to establish a valid goodness-of-fit test. Finally, we apply our proposed goodness-of-fit test to the Ising model, the Bernoulli RBM, and the ERGM, and our experiments show that the proposed test typically outperforms a two-sample test based on the maximum mean discrepancy (Gretton et al., 2012) in terms of power while maintaining control on false-positive rate.\nOutline. Section 2 introduces notation and preliminaries. We construct and characterize discrete Stein operators in Section 3, establish the kernelized discrete Stein discrepancy measure in Section 4, and describe the goodness-of-fit testing procedure in Section 5. We apply the proposed test in experiments on several statistical models in Section 7, discuss related work in Section 6, and conclude in Section 8. All omitted results and proofs can be found in the Appendix."}, {"heading": "2. Notation and Preliminaries", "text": "We primarily focus on domains X of finite cardinality |X |. A probability mass function (pmf) p supported on X d is\nsaid to be positive if p(x) > 0 for all x \u2208 X d. A symmetric function k(\u00b7, \u00b7) is a positive definite kernel on X d if the Gram matrix K = [k(xi,xj)]ni,j=1 is positive semidefinite for any n \u2208 N and {x1, . . . ,xn} \u2286 X d. The kernel is strictly positive definite if K is positive definite. By the Moore-Aronszajin theorem, every such kernel k has a unique reproducing kernel Hilbert space (RKHS) H of functions f : X d \u2192 R satisfying the reproducing property: for any f \u2208 H, f(x) = \u3008f(\u00b7), k(\u00b7,x)\u3009H (and in particular, k(x,x\u2032) = \u3008k(\u00b7,x), k(\u00b7,x\u2032)\u3009H). More generally, let Hm = H\u00d7H \u00b7 \u00b7 \u00b7 \u00d7 H denote the Hilbert space of vectorvalued functions f = {f` : f` \u2208 H}m`=1, endowed with the inner-product \u3008f ,g\u3009Hm = \u2211m `=1 \u3008f`, g`\u3009H for f = {f`}m`=1\nand g = {g`}m`=1, and norm \u2016f\u2016Hm = \u221a\u2211m `=1 \u2016f`\u20162H."}, {"heading": "3. Discrete Stein Operators", "text": "We first propose a simple Stein operator for discrete distributions, and then provide a general characterization of Stein operators for both the discrete and continuous cases. In particular, we draw upon ideas in the literature on scorematching methods (Hyva\u0308rinen, 2005; 2007; Lyu, 2009; Amari, 2016), which we elaborate on further in Section 6."}, {"heading": "3.1. Difference Stein Operator", "text": "Definition 1 (Cyclic permutation). For a set X of finite cardinality, a cyclic permutation \u00ac : X \u2192 X is a bijective function such that for some ordering x[1], x[2], . . . , x[|X |] of the elements in X , \u00acx[i] = x[(i+1) mod |X |], \u2200i = 1, 2, . . . , |X |.\nThus, starting with any element of x, repeated application of the \u00ac operator generates the set X : X = {x,\u00acx, . . . ,\u00ac(|X |\u22121)x}. In the simplest case, when X is a binary set, one can take X = {\u00b11} and define \u00acx = \u2212x.\nThe inverse permutation of \u00ac is an operator \u2a3c : X \u2192 X that satisfies \u00ac(\u2a3cx) = \u2a3c(\u00acx) = x for any x \u2208 X . Under the ordering of Definition 1, we have \u2a3cx[i] = x[(i\u22121) mod |X |]. It is easy to verify that \u2a3c is also a cyclic permutation on X . When X is a binary set, the inverse of \u00ac is itself: \u2a3c = \u00ac. Definition 2 (Partial difference operator and difference score function). Given a cyclic permutation \u00ac on X , for any vector x = (x1, . . . , xd)T \u2208 X d, write \u00acix := (x1, . . . , xi\u22121,\u00acxi, xi+1, . . . , xd)\nT. For any function f : X d \u2192 R, denote the (partial) difference operator as\n\u2206xif(x) := f(x)\u2212 f(\u00acix), i = 1, . . . , d,\nand write \u2206f(x) = (\u2206x1f(x), . . . ,\u2206xdf(x)) T. Define the (difference) score function as sp(x) := \u2206p(x)/p(x), with\n(sp(x))i = \u2206xip(x) p(x) = 1\u2212 p(\u00acix) p(x) , i = 1, . . . , d. (1)\nWe will also be interested in the difference operator defined\nwith respect to the inverse permutation \u2a3c. To avoid cluttering notation, we shall use \u2206 and sp to denote the difference operator and score function defined with respect to \u00ac, and use \u2206\u2217 to denote the difference operator with respect to \u2a3c:\n\u2206\u2217xif(x) := f(x)\u2212 f(\u2a3cix), i = 1, . . . , d.\nAs in the continuous case, the score function sp(x) can be easily computed even if p is only known up to a normalization constant: if p(x) = p\u0303(x)/Z, then sp(x) = \u2206p\u0303(x)/p\u0303(x) does not depend on Z. For an exponential family distribution p with base measure h(x), sufficient statistics \u03c6(x), and natural parameters \u03b8: p(x) = 1Z(\u03b8)h(x) exp{\u03b8\nT\u03c6(x)}, the (difference) score function is given by\n(sp(x))i = 1\u2212 h(\u00acix)\nh(x) exp{\u03b8T(\u03c6(\u00acix)\u2212 \u03c6(x))}. (2)\nIn the continuous case, it was obvious that two densities p and q are equal almost everywhere if and only if their score functions are equal almost everywhere. This still holds for the difference score function, but its proof is less trivial.\nTheorem 1. For any positive pmfs p and q on X d, we have that sp(x) = sq(x) for all x \u2208 X d if and only if p = q.\nProof sketch. Clearly, p = q implies that sp(x) = sq(x) for all x \u2208 X d. For the converse, by Eq. (1), sp(x) = sq(x) implies that p(\u00acix)/p(x) = q(\u00acix)/q(x) for all x and i. Using the fact that \u00ac is a cyclic permutation onX , we can show that all the singleton conditional distributions of p and q must match, i.e., p(xi|x\u2212i) = q(xi|x\u2212i) for all xi and i, where x\u2212i := (x1, . . . , xi\u22121, xi+1, . . . , xd) (see the Appendix for details). By Brook\u2019s lemma (Brook, 1964; see Lemma 9 in the Appendix), the joint distribution is fully specified by the collection of singleton conditional distributions, and thus we must have p(x) = q(x) for all x \u2208 X d.\nIn the literature on score functions (Hyva\u0308rinen, 2007; Lyu, 2009), such results, showing that a score function sp(x) uniquely determines a probability distribution, are called completeness results. For our purposes, such completeness results provide a basis for establishing statistical hypothesis tests to distinguish between two distributions. We first introduce the concept of a difference Stein operator.\nDefinition 3 (Difference Stein operator). Let \u00ac be a cyclic permutation on X and let \u2a3c be its inverse permutation. For any function f : X d \u2192 R and pmf p on X d, define the difference Stein operator of p as\nApf(x) := sp(x)f(x)\u2212\u2206\u2217f(x), (3)\nwhere sp(x) = \u2206p(x)/p(x) is the difference score function defined w.r.t. \u00ac, and \u2206\u2217 is the difference operator w.r.t. \u2a3c.\nWe note that any intractable normalization constant in p cancels out in evaluating the Stein operator Ap. The Stein operator satisfies an important identity:\nTheorem 2 (Difference Stein\u2019s identity). For any function f : X d \u2192 R and probability mass function p on X d,\nEx\u223cp [Apf(x)] = Ex\u223cp [sp(x)f(x)\u2212\u2206\u2217f(x)] = 0. (4)\nProof. Notice that Ex\u223cp [Apf(x)] = \u2211\nx\u2208Xd [f(x)\u2206p(x)\u2212 p(x)\u2206\u2217f(x)] .\nTo complete the proof, simply note that for each i,\u2211 x\u2208Xd f(x)\u2206xip(x) = \u2211 x\u2208Xd f(x)p(x)\u2212 \u2211 x\u2208Xd\nf(x)p(\u00acix),\u2211 x\u2208Xd p(x)\u2206\u2217xif(x) = \u2211 x\u2208Xd p(x)f(x)\u2212 \u2211 x\u2208Xd p(x)f(\u2a3cix).\nThe two equations are equal since \u00ac and \u2a3c are inverse cyclic permutations on X , with \u00aci(\u2a3cix) = \u2a3ci(\u00acix) = x.\nFinally, we can extend the definition of the difference Stein operator to vector-valued functions f : X d \u2192 Rm. In this case, \u2206f is an d\u00d7m matrix with (\u2206f)ij = \u2206xifj(x), and the Stein operator takes the form\nApf(x) = sp(x) f(x)T \u2212\u2206\u2217f(x).\nSimilar to Theorem 2, one can show that for any function f : X d \u2192 Rm and positive pmf p on X d,\nEx\u223cp [Apf(x)] = Ex\u223cp [ sp(x) f(x) T \u2212\u2206\u2217f(x) ] = 0.\nIf m = d, taking the trace on both sides yields Ep [tr (Apf(x))] = Ep [ sp(x) Tf(x)\u2212 tr (\u2206\u2217f(x)) ] = 0."}, {"heading": "3.2. Characterization of Stein Operators", "text": "Generalizing our construction in the previous section, we can further identify a broad class of Stein operators which includes the difference Stein operator as a special case.\nLet L be any operator defined on the space of functions F = {f : X d \u2192 R} that can be written in the form1\nLf(x) = \u2211\nx\u2032\u2208Xd g(x,x\u2032)f(x\u2032), \u2200f \u2208 F (5)\nfor some bivariate (possibly vector-valued) function g on X d \u00d7X d. Define a dual operator L\u2217 via\nL\u2217f(x) = \u2211\nx\u2032\u2208Xd g(x\u2032,x)f(x\u2032), \u2200f \u2208 F . (6)\n1The notion can also be extended to vector-valued functions f ; we omit this generalization here for clarity.\nIn fact, when X is a finite set, any linear operator L on F = {f : X d \u2192 R} can be written in the form of Eq. (5). In this case, the operator L\u2217 as defined in Eq. (6) is the adjoint operator of L: \u3008Lf, g\u3009 = \u3008f,L\u2217g\u3009 for all f, g \u2208 F , where \u3008\u00b7, \u00b7\u3009 is the appropriate inner-product on X d. If g(\u00b7, \u00b7) is symmetric, then L is self-adjoint, i.e., L\u2217 = L.\nUnder these definitions, we have the following result which characterizes the Stein operators on a discrete space X d. Theorem 3. Denote F = {f : X d \u2192 R}. For any positive pmf p on X d, a linear operator Tp satisfies Stein\u2019s identity\nEx\u223cp [Tpf(x)] = 0 (7)\nfor all functions f \u2208 F if and only if there exist linear operators L and L\u2217 of the forms (5) and (6), such that\nTpf(x) = Lp(x) p(x) f(x)\u2212 L\u2217f(x) (8)\nholds for all x \u2208 X d and functions f \u2208 F .\nProof. Sufficiency: Suppose the linear operators L and L\u2217 take the forms of Eqs. (5) and (6) for some function g, we show that the operator Tp defined via Eq. (8) satisfies Stein\u2019s identity of Eq. (7). We can write\nEp [Tpf(x)] = \u2211\nx\u2208Xd [f(x)Lp(x)\u2212 p(x)L\u2217f(x)]\n= \u2211\nx\u2208Xd \u2211 x\u2032\u2208Xd f(x)g(x,x\u2032)p(x\u2032)\n\u2212 \u2211\nx\u2208Xd \u2211 x\u2032\u2208Xd p(x)g(x\u2032,x)f(x\u2032) .\nThe two terms in the last line cancel out since the doublesummations are invariant under a swapping of summation indices x and x\u2032, giving Ep [Tpf(x)] = 0.\nNecessity: See the Appendix for the remaining proof.\nWe note that the sufficiency part of Theorem 3 remains valid when X is a continuous space, p is a density, F \u2286 {f : X d \u2192 R} is some family of functions for which Tpf and Lf are well-defined, and the summations in Eqs. (5) and (6) are replaced by integrations. However, the necessity part requires further conditions on the expressiveness of F .\nTheorem 3 essentially states that (for a fixed p) given any pair of adjoint operators L and L\u2217, one can construct a linear operator Tp satisfying Stein\u2019s identity; conversely, any Stein operator Tp can be expressed using a pair of adjoint operators L and L\u2217. This connection between adjoint operators and Stein operators enables us to unify different forms of Stein operators for discrete and continuous distributions (see also Ley et al. (2017) for related discussions).\nRemark 4 (Continuous case). For a continuous space X \u2286 R, consider a smooth density p onX d. TakeL = \u2207 to be the gradient operator, and let F consist of smooth functions f : X d \u2192 R for which f(x) p(x) vanishes at the boundary \u2202X . Using integration-by-parts, it can be shown that the adjoint operator of L is L\u2217 = \u2212\u2207. Then, applying Eq. (8) of Theorem 3 recovers the standard continuous Stein operator\nApf(x) = \u2207 log p(x)f(x) +\u2207f(x).\nRemark 5 (Discrete case). In Eqs. (5) and (6), define the vector-valued function g : X d \u00d7X d \u2192 Rd with\n(g(x,x\u2032))i = I{x\u2032 = x} \u2212 I{x\u2032 = \u00acix} (9)\nwhere I{\u00b7} is the indicator function. Then, we have (Lf(x))i = \u2211\nx\u2208Xd (g(x,x\u2032))if(x) = f(x)\u2212 f(\u00acix),\nwhich recovers the difference operator \u2206. Similarly, define g\u2217 by replacing \u00ac with its inverse permutation \u2a3c in Eq. (9). Notice that g(x,x\u2032) = g\u2217(x\u2032,x), and thus the adjoint of L is given by L\u2217 = \u2206\u2217. In this case, Eq. (8) boils down to the difference Stein operator defined in Eq. (3).\nNote that if X is binary, then \u00ac = \u2a3c, and L is self-adjoint. When L is self-adjoint, in addition to Stein\u2019s identity, the Stein operator defined via Eq. (8) also satisfies Tp p(x) = 0.\nGraph-based discrete Stein operators. Extending the form of Eq. (9), we can obtain a more general recipe for constructing g, which, upon applying Theorem 3, gives rise to other Stein operators on X d. Specifically, suppose we have identified a simple graph G = (X d, E) on |X |d vertices, with each vertex corresponding to a possible configuration x \u2208 X d. Then, it is natural to define g such that it respects the structure of G, in the sense that g(x,x\u2032) = 0 if x\u2032 /\u2208 Nx \u222a {x}, where Nx := {x\u2032 : (x,x\u2032) \u2208 E} is the set of neighbors of x in G. If G is undirected, one would also make g symmetric, in which case L \u2261 L\u2217 is self-adjoint.\nRevisiting the difference Stein operator in this light, notice that \u00ac defines a d-dimensional (undirected) lattice graph G on X d, in which two vertices x and x\u2032 are connected if and only if x\u2032 = \u00acix for some i \u2208 {1, . . . , d}. In this case, every vertex x has exactly d neighbors in G: Nx = {\u00ac1x, . . . ,\u00acdx}. We then set g(x,\u00acix) = \u2212ei for each i, g(x,x) = e, and g(x,x\u2032) = 0 for x\u2032 6\u2208 Nx \u222a {x}, where ei \u2208 Rd is the i-th standard basis vector, and e \u2208 Rd is the all-ones vector. This recovers the form of g in Eq. (9).\nAs another example, one could take g(x,x\u2032) = \u2212|Nx|\u22121 for x\u2032 \u2208 Nx and set g(x,x\u2032) = I[x = x\u2032] otherwise. Then, Eq. (5) becomes Lf(x) = 1|Nx| \u2211 x\u2032\u2208Nx (f(x)\u2212 f(x \u2032)) , which recovers the normalized Laplacian of G (see also Amari, 2016). Thus, by specifying an arbitrary graph structure G onX d, one could also utilize its Laplacian L to define a corresponding Stein operator T by applying Theorem 3."}, {"heading": "4. Kernelized Discrete Stein Discrepancy", "text": "We can now proceed similarly as in the continuous case (Liu et al., 2016; Chwialkowski et al., 2016) to define the discrete Stein discrepancy and its kernelized counterpart. While all results in this section hold for the general Stein operators discussed in Section 3.2, for clarity we state them for the difference Stein operator described in Section 3.1.\nDefinition 4 (Discrete Stein discrepancy). Let X be a finite set. For a family F of functions f : X d \u2192 Rd, define the discrete Stein discrepancy between two positive pmfs p, q as\nD(q \u2016 p) := sup f\u2208F Ex\u223cq [tr (Apf(x))] ,\nwhere Apf(x) = sp(x) f(x)T \u2212 \u2206\u2217f(x) is the difference Stein operator w.r.t. p. Taking F to be the unit ball in an RKHS Hd of vector-valued functions f : X d \u2192 Rd, we obtain the kernelized discrete Stein discrepancy (KDSD):\nD(q \u2016 p) = sup f\u2208Hd, \u2016f\u2016Hd\u22641 Ex\u223cq [tr (Apf(x))] . (10)\nAlthough Eq. (10) involves solving a variational problem, the next two results show that the kernelized discrete Stein discrepancy can actually be computed in closed-form. Due to space constraints, we defer their proofs to the Appendix.\nTheorem 6. The kernelized discrete Stein discrepancy as defined in Eq. (10) admits an equivalent representation:\nD(q \u2016 p)2 = Ex,x\u2032\u223cq [ \u03b4p,q(x) Tk(x,x\u2032) \u03b4p,q(x \u2032) ] , (11)\nwhere \u03b4p,q(x) := sp(x) \u2212 sq(x) is the score-difference between p and q.\nTheorem 7. Define the kernel function\n\u03bap(x,x \u2032) = sp(x) Tk(x,x\u2032) sp(x \u2032)\u2212 sp(x)T\u2206\u2217x\u2032k(x,x\u2032) \u2212\u2206\u2217xk(x,x\u2032)Tsp(x\u2032) + tr ( \u2206\u2217x,x\u2032k(x,x \u2032) ) , (12)\nthen D(q \u2016 p)2 = Ex,x\u2032\u223cq [\u03bap(x,x\u2032)] . (13)\nThe next result justifies D(q \u2016 p) as a divergence measure. Lemma 8. For a finite set X , let p and q be positive pmfs on X d. LetH be an RKHS on X d with kernel k(\u00b7, \u00b7), and let D(q \u2016 p) be defined as in Eq. (10). Assume that the Gram matrix K = [k(x,x\u2032)]x,x\u2032\u2208Xd is strictly positive definite, then D(q \u2016 p) = 0 if and only if p = q. Proof. By Theorem 6, we have\nD(q \u2016 p)2 = Ex,x\u2032\u223cq [ \u03b4p,q(x) Tk(x,x\u2032) \u03b4p,q(x \u2032) ]\n= \u2211\nx\u2208Xd \u2211 x\u2032\u2208Xd q(x)\u03b4p,q(x) Tk(x,x\u2032) \u03b4p,q(x \u2032)q(x\u2032),\nwhere \u03b4p,q(x) = sp(x) \u2212 sq(x) \u2208 Rd. Denote the `-th element of \u03b4p,q by \u03b4`p,q , and write g` := [q(x)\u03b4 ` p,q(x)]x\u2208Xd\nfor ` = 1, . . . , d. Then, D(q \u2016 p)2 = \u2211d `=1 g T ` Kg`. Since K is strictly positive-definite, D(q \u2016 p)2 = 0 if and only if g` = 0 for all `. Therefore, \u03b4p,q(x) = 0 for all x \u2208 X d. By Theorem 1, this holds if and only if p = q."}, {"heading": "5. Goodness-of-Fit Testing via KDSD", "text": "Given a (possibly un-normalized) model distribution p and i.i.d. samples {xi}ni=1 from an unknown data distribution q on X d, we would like to measure the goodness-of-fit of the model distribution p to the observed data {xi}ni=1. To this end, we perform the hypothesis test H0 : p = q vs. H1 : p 6= q using the kernelized discrete Stein discrepancy (KDSD) measure. Denote S(q \u2016 p) := D(q \u2016 p)2; we can estimate S(q \u2016 p) via a U -statistic (Hoeffding, 1948) which provides a minimum-variance unbiased estimator:\nS\u0302(q \u2016 p) = 1 n(n\u2212 1) n\u2211 i=1 n\u2211 j 6=i \u03bap(xi,xj) . (14)\nAs in the continuous case (Liu et al., 2016), the U -statistic S\u0302(q \u2016 p) is asymptotically Normal under the alternative hypothesis H1 : p 6= q, \u221a n (S\u0302(q \u2016 p)\u2212S(q \u2016 p)) D\u2192 N (0, \u03c32), where \u03c32 = Varx\u223cq(Ex\u2032\u223cq [\u03bap(x,x\u2032)]) > 0, but becomes degenerate (\u03c32 = 0) under the null hypothesis H0 : p = q (see Theorem 11 in the Appendix for a precise statement).\nSince the asymptotic distribution of S\u0302(q \u2016 p) under the null hypothesis cannot be easily calculated, we follow Liu et al. (2016) and adopt the bootstrap method for degenerate U - statistics (Arcones & Gine, 1992; Huskova & Janssen, 1993) to draw samples from the null distribution of the test statistic. Specifically, to obtain a bootstrap sample, we draw random multinomial weights w1, . . . , wn \u223c Mult(n; 1/n, . . . , 1/n), set w\u0303i = (wi \u2212 1)/n, and compute\nS\u0302\u2217(q \u2016 p) = n\u2211 i=1 n\u2211 j 6=i w\u0303iw\u0303j\u03bap(xi,xj). (15)\nUpon repeating this procedure m times, we calculate the critical value of the test by taking the (1\u2212 \u03b1)-th quantile of the bootstrapped statistics {S\u0302\u2217b}mb=1.\nThe overall goodness-of-fit testing procedure is summarized in Algorithm 1. Computing the test statistic in Eq. (14) takes O(n2) time, where n is the number of observations, and the bootstrapping procedure takes O(mn2) time, where m is the number of bootstrap samples used.\nKernel choice. A practical question that arises when performing the KDSD test is the choice of the kernel function k(\u00b7, \u00b7) on X d. For continuous spaces, the RBF kernel might be a natural choice; Gorham & Mackey (2017) also provide further recommendations. For discrete spaces, a naive choice is the \u03b4-kernel, k(x,x\u2032) = I{x = x\u2032}, which suffers from the curse of dimensionality. A more sensible choice is\nAlgorithm 1 Goodness-of-fit testing via KDSD 1: Input: Difference score function sp of p, data samples {xi}ni=1 \u223c q, kernel function k(\u00b7, \u00b7), bootstrap sample size m, significance level \u03b1.\n2: Objective: Test H0 : p = q vs. H1 : p 6= q. 3: Compute test statistic S\u0302(q \u2016 p) via Eq. (14). 4: for b = 1, . . . ,m do 5: Compute bootstrap test statistic S\u0302\u2217b via Eq. (15). 6: end for 7: Compute critical value \u03b31\u2212\u03b1 by taking the (1\u2212 \u03b1)-th\nquantile of the bootstrap test statistics {S\u0302 \u2217 b}mb=1.\n8: Output: Reject H0 if test statistic S\u0302(q \u2016 p) > \u03b31\u2212\u03b1, otherwise do not reject H0.\nthe exponentiated Hamming kernel:\nk(x,x\u2032) = exp{\u2212H(x,x\u2032)}, (16)\nwhere H(x,x\u2032) := 1d \u2211d i=1 I{xi 6= x\u2032i} is the normalized Hamming distance. Lemma 12 in the Appendix shows that Eq. (16) defines a positive definite kernel.\nWhen the inputs x and x\u2032 encode additional structure about X d, the Hamming distance may no longer be appropriate. For instance, when x \u2208 {0, 1}( d 2) represents the (flattened) adjacency matrix of an undirected and unweighted graph on d vertices, two graphs x and x\u2032 may be isomorphic yet have non-zero Hamming distance. In this case, we can resort to the literature on graph kernels (Vishwanathan et al., 2010). Section 7 gives an example of using the Weisfeiler-Lehman graph kernel of Shervashidze et al. (2011) to test whether a set of graphs {xi}ni=1 comes from a specific distribution."}, {"heading": "6. Related Work and Discussion", "text": "Stein\u2019s method. In probability theory, Stein\u2019s method has become an important tool for deriving approximations to probability distributions and characterizing convergence rates (see e.g., Barbour & Chen (2014) for an overview). Related to our characterization via adjoint operators, Ley et al. (2017) also proposed the notion of a canonical Stein operator. Recently, Bresler & Nagaraj (2017); Reinert & Ross (2017) applied Stein\u2019s method to bound the distance between two stationary distributions of irreducible Markov chains in terms of their Glauber dynamics. Notably, they also make use of a difference operator for the binary case, and it is interesting to investigate whether their analysis techniques could be adopted for goodness-of-fit testing.\nGoodness-of-fit tests. Closely related to our work is the kernelized Stein discrepancy test proposed independently by Chwialkowski et al. (2016); Liu et al. (2016) for smooth densities on continuous spaces. Our work further identifies and characterizes Stein operators for discrete domains, unifying them via Theorem 3 under a general framework for\nconstructing Stein operators from adjoint operators. Under this framework, any Stein operator can be directly used to establish a KDSD test (under completeness conditions).\nIn addition to kernel-based tests, other forms of goodness-offit tests have also been examined for discrete distributions. Some recent examples include Valiant & Valiant (2016); Mart\u0131\u0301n del Campo et al. (2017); Daskalakis et al. (2018). However, these tests are often model-specific, and typically assume that the normalization constant is easy to evaluate. In contrast, the KDSD test we propose is fully nonparametric, and applies to any un-normalized statistical model.\nScore-matching methods. Proposed by Hyva\u0308rinen (2005), score-matching methods make use of score functions to perform parameter estimation in un-normalized models. Suppose we observe data {x}ni=1 from some unknown density q(x) which we would like to approximate using a parameterized model density p(x;\u03b8). To estimate the parameters \u03b8, score-matching methods minimize the Fisher divergence:\nJ(\u03b8) = \u222b \u03be\u2208Rd q(\u03be) \u2016\u2207\u03be log p(\u03be;\u03b8)\u2212\u2207\u03be log q(\u03be)\u201622 d\u03be.\nSimilar to the continuous KSD (Liu et al., 2016), if we set k(x,x\u2032) = I{x = x\u2032}/ \u221a q(x) q(x\u2032) and apply Theorem 6, the KDSD statistic can be written as D(q \u2016 p)2 = Ex\u223cq [ \u2016sp(x)\u2212 sq(x)\u201622 ] , which takes the same form as J(\u03b8) with the continuous score function \u2207 log p(x) replaced by the difference score function sp(x).\nExtensions of score-matching to discrete data have also been considered in Hyva\u0308rinen (2007); Lyu (2009); Amari (2016), and our work draws insights from these in the design of score functions for Stein operators. In particular, Lyu (2009) examined the connections between adjoint operators and Fisher divergence, and Amari (2016) discussed score functions for data from a graphical model. However, the connections to Stein operators and kernel-based hypothesis testing have not appeared in the score-matching literature.\nTwo-sample tests. Complementing goodness-of-fit tests (or one-sample tests) are two-sample tests, where we test if two collections of samples come from the same distribution. A well-known kernel two-sample test statistic is the maximum mean discrepancy (MMD) of Gretton et al. (2012). Given i.i.d. samples {xi}ni=1 \u223c p and {yj}n \u2032\nj=1 \u223c q, one could compute a U -statistic estimate of MMD(p, q) in O(nn\u2032) time. The critical value of the test is calculated by bootstrapping on the aggregated data.\nTwo-sample tests can also be used as goodness-of-fit tests by comparing observed data with samples from the null model. For distributions with intractable normalization constants, obtaining exact samples from p could become very difficult or expensive. Further, approximate samples may introduce bias and/or correlation among the samples, violating the test assumptions, and leading to unpredictable test errors."}, {"heading": "7. Applications", "text": "We apply the proposed KDSD goodness-of-fit test to three statistical models involving discrete distributions. We describe the models and derive their difference score functions in Section 7.1, and present experiments in Section 7.2."}, {"heading": "7.1. Statistical Models", "text": "Ising model. The Ising model (Ising, 1924) is a canonical example of a Markov random field (MRF). Consider an (undirected) graph G = (V,E), where each vertex i \u2208 V is associated with a binary spin. The collection of spins form a random vector x = (x1, x2, . . . , xd), whose components xi and xj (i 6= j) interact directly only if (i, j) \u2208 E. The pmf is p\u0398(x) = 1Z(\u0398) exp{ \u2211 (i,j)\u2208E \u03b8ijxixj}, where \u03b8ij are the edge potentials and Z(\u0398) is the partition function which is prohibitive to compute when d is high. Recognizing the pmf as an exponential family distribution, we can apply Eq. (2) to obtain the difference score function: (sp(x))i = 1 \u2212 exp{\u22122xi \u2211 j\u2208Ni \u03b8ijxj}, where Ni := {j : (i, j) \u2208 E} denotes the set of vertices adjacent to node i in graph G.\nBernoulli restricted Boltzmann machine (RBM). The RBM (Hinton, 2002) is an undirected graphical model consisting of a bipartite graph between visible units v and hidden units h. In a Bernoulli RBM, both v and h are Bernoulli-distributed; X = {0, 1}. The joint pmf of an RBM with M visible units and K hidden units is given by p(h,v|\u03b8) = 1Z(\u03b8) exp{\u2212E(v,h;\u03b8)}, with energy function E(v,h;\u03b8) = \u2212(vTWh + vTb + hTc), where W \u2208 RM\u00d7K are the weights, b \u2208 RM and c \u2208 RK are the bias terms, \u03b8 := (W,b, c), and Z(\u03b8) =\u2211\nv \u2211 h exp{\u2212E(v,h;\u03b8)} is the partition function.\nMarginalizing out the hidden variables h, the pmf of v is given by p(v|\u03b8) = 1Z\u2032(\u03b8) exp{\u2212F (v;\u03b8)}, with free energy F (v;\u03b8) = \u2212vTb \u2212 \u2211K k=1 log(1 + exp{vTW\u2217k + ck}).\nHere, W\u2217k denotes the k-th column of W, and Z \u2032(\u03b8) =\u2211 v exp{\u2212F (v;\u03b8)} is another normalization constant. Thus, we can write down the (difference) score function as (sp(v;\u03b8))i = 1 \u2212 ev\u0303ibi \u220fK k=1 1+exp{vTW\u2217k+v\u0303iwik+ck} 1+exp{vTW\u2217k+ck} , where v\u0303i = \u00acvi \u2212 vi. Note that sp(v;\u03b8) is again free of normalization constants and can be easily evaluated.\nExponential random graph model (ERGM). The ERGM is a well-studied statistical model for network data (Holland & Leinhardt, 1981). In a typical ERGM, the probability of observing an adjacency matrix y \u2208 {0, 1}n\u00d7n is p(y) =\n1 Z(\u03b8,\u03c4) exp {\u2211n\u22121 k=1 \u03b8kSk(y)+\u03c4T (y) } . Here, Sk(\u00b7) counts the number of edges (k = 1) or k-stars (k \u2265 2), T (\u00b7) counts triangles, and Z(\u03b8, \u03c4) is the normalization constant.\nWe consider an ERGM distribution of undirected graphs y with three sufficient statistics: S1(y), the number of\nedges (1-stars); S2(y), the number of wedges (2-stars); and T (y), the number of triangles.2 The parameters for these sufficient statistics are \u03b81, \u03b82, and \u03c4 , respectively. The score function can be written as (sp(y))ij = 1 \u2212 exp{\u03b81\u03b41(y) + \u03b82\u03b42(y) + \u03c4\u03b43(y)}, with the change statistics given by \u03b41(y) := [S1(\u00acijy) \u2212 S1(y)] = (\u22121)yij , \u03b42(y) := [S2(\u00acijy) \u2212 S2(y)] = (\u22121)yij (|N \\ji | + |N \\i j |), and \u03b43(y) := [T (\u00acijy)\u2212T (y)] = (\u22121)yij |Ni\u2229Nj |, where Ni denotes the neighbor-set of node i, andN \\ji := Ni\\{j}."}, {"heading": "7.2. Experiments", "text": "We apply the kernelized discrete Stein discrepancy (KDSD) test to the statistical models described in Sections 7.1. In the absence of established baselines, we compare with a two-sample test based on the maximum mean discrepancy (MMD) (see Section 6). For both KDSD and MMD, we utilize the exponentiated Hamming kernel (Eq. (16)) for the Ising model and RBM, and the Weisfeiler-Lehman graph kernel (Shervashidze et al., 2011) for the ERGM.\nSetup. Denote the null model distribution by p and the alternative distribution by q. For each distribution, we draw exact i.i.d. samples by running n independent Markov chains with different random initializations, each for 105 iterations, and collecting only the last sample of each chain. For KDSD, we draw n samples from q; for MMD, we draw n samples from q and another n samples from p. Under this setup, both KDSD and MMD takes time O(mn2), where m is the number of bootstrap samples used to determine the critical threshold. We set m = 5000 for both methods throughout.\nFor each model, we choose a \u201cperturbation parameter\u201d and fix its value for the null distribution p, while drawing data samples under various values of the perturbation parameter. We also vary the sample size n to examine the performance of the test as n increases. For each value of the perturbation parameter and each sample size n, we conduct 500 independent trials. In each trial, we first randomly flip a fair coin to decide whether to set the alternative distribution q to be the same as p or with a different value of the perturbation parameter. (In the former case, the null hypothesis H0 : p = q should not be rejected, and in the latter case it should be.) Then, we draw n independent samples from q (for KDSD) or both p and q (for MMD) and perform the hypothesis test H0 : p = q vs. H1 : p 6= q under significance level \u03b1 = 0.05. We evaluate the performance of the KDSD and MMD tests in terms of their false-positive rate (FPR; Type-I error) and false-negative rate (FNR; Type-II error), and report the results across 500 independent trials.\nIsing model. We consider a periodic 10-by-10 lattice, with d = 100 random variables. We focus on the ferromagnetic\n2 Notice that the sufficient statistics are not independent: e.g., S2(y) > T (y) since every triangle contains three 2-stars.\nsetting and set \u03b8ij = 1/T , where T is the temperature of the system. For T0 \u2208 {5, 20} and various values of T \u2032, we test the hypotheses H0 : T = T0 vs. H1 : T 6= T0 using data samples drawn from the model under T = T \u2032. To draw samples from the Ising model, we apply the Metropolis algorithm: in each iteration, we propose to flip the spin of a randomly chosen variable xi, and adopt this proposal with probability min(1, exp{\u22122xi \u2211 j\u2208Ni \u03b8ijxj}).\nBernoulli RBM. We use M = 50 visible units and K = 25 hidden units. We draw the entries of the weight matrix W i.i.d. from a Normal distribution with mean zero and standard deviation 1/M, and the entries of the bias terms b and c i.i.d. from the standard Normal distribution. We corrupt the weights in W by adding i.i.d. Gaussian noise with mean zero and standard deviation \u03c3, and test the hypotheses H0 : \u03c3 = 0 (no-corruption) vs. H1 : \u03c3 6= 0 using data samples drawn under \u03c3 = \u03c3\u2032 for various values of \u03c3\u2032. To draw samples from the RBM, we perform block Gibbs sampling by exploiting the bipartite structure of the graphical model.\nERGM. We consider an ERGM distribution for undirected graphs on 20 nodes, with the dimension of each sample d = ( 20 2 ) = 190. We fix \u03b81 = \u22122 and \u03c4 = 0.01, For various values of the 2-star parameter \u03b8\u20322, we test the hypotheses H0 : \u03b82 = 0 vs. H1 : \u03b82 6= 0 using data samples drawn under \u03b82 = \u03b8\u20322. To draw MCMC samples from the ERGM, we utilize the ergm R package (Handcock et al., 2017).\nResults. In Figure 1, the top row plots the testing error rate vs. different values of the perturbation parameter in H1, for a fixed H0 and sample size; while the bottom row plots the error rate vs. sample size n for a fixed pair of H0 and H1. We observe that both KDSD and MMD maintain a false-\npositive rate (Type-I error) around or below the significance level \u03b1 = 0.05. In addition, KDSD consistently achieves lower false-negative rate (Type-II error) than MMD in most cases, indicating that KDSD, by utilizing the score function information of p, leads to a more powerful test.\nIt is interesting to note that in the ERGM example, MMD exhibits higher power than KDSD when the data samples were drawn from an ERGM distribution with \u03b8\u20322 \u2208 (0, 0.05) (roughly). We hypothesize that this may correspond to a regime in which a small change in \u03b82 causes a subtle change in the global graph structure that can be more easily detected by MMD, while the difference Stein operator of Section 3.1 may be more adapt in detecting local differences. Thus, the performance of the KDSD test could be improved by constructing Stein operators (using the characterization of Section 3.2) that exploit higher-order structure in the graph samples, and we plan to investigate this in future work."}, {"heading": "8. Conclusion", "text": "We have introduced a kernelized Stein discrepancy measure for discrete probability distributions, which enabled us to establish a nonparametric goodness-of-fit test for discrete distributions with intractable normalization constants. Furthermore, we have proposed a general characterization of Stein operators that encompasses both discrete and continuous distributions, providing a recipe for constructing new Stein operators. We have applied the proposed goodness-offit test to three statistical models involving discrete distributions, and shown that it typically outperforms a two-sample test based on the maximum mean discrepancy.\nAcknowledgements. We thank the anonymous reviewers for their helpful comments. This research is supported by NSF under contract numbers IIS-1149789, IIS-1618690, IIS-1546488, and CCF-0939370."}], "year": 2018, "references": [{"title": "Information Geometry and Its Applications. Springer, 2016", "authors": ["Amari", "S.-i"], "year": 2016}, {"title": "A test of goodness of fit", "authors": ["T.W. Anderson", "D.A. Darling"], "venue": "Journal of the American Statistical Association,", "year": 1954}, {"title": "On the bootstrap of U and V statistics", "authors": ["M.A. Arcones", "E. Gine"], "venue": "The Annals of Statistics,", "year": 1992}, {"title": "Stein\u2019s method for stationary distributions of markov chains and application to Ising models", "authors": ["G. Bresler", "D. Nagaraj"], "year": 2017}, {"title": "On the distinction between the conditional probability and the joint probability approaches in the specification of nearest-neighbour systems", "authors": ["D. Brook"], "year": 1964}, {"title": "A kernel test of goodness of fit", "authors": ["K. Chwialkowski", "H. Strathmann", "A. Gretton"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning (ICML),", "year": 2016}, {"title": "Testing Ising models", "authors": ["C. Daskalakis", "N. Dikkala", "G. Kamath"], "venue": "In Proceedings of the 29th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA),", "year": 2007}, {"title": "Measuring sample quality with Stein\u2019s method", "authors": ["J. Gorham", "L. Mackey"], "venue": "In Proceedings of the 28th International Conference on Neural Information Processing Systems (NIPS),", "year": 2015}, {"title": "Measuring sample quality with kernels", "authors": ["J. Gorham", "L.W. Mackey"], "venue": "In Proceedings of The 34th International Conference on Machine Learning (ICML),", "year": 2017}, {"title": "A kernel two-sample test", "authors": ["A. Gretton", "K.M. Borgwardt", "M.J. Rasch", "B. Sch\u00f6lkopf", "A. Smola"], "venue": "Journal of Machine Learning Research,", "year": 2012}, {"title": "ergm: Fit, Simulate and Diagnose Exponential-Family Models for Networks", "authors": ["M.S. Handcock", "D.R. Hunter", "C.T. Butts", "S.M. Goodreau", "P.N. Krivitsky", "M. Morris"], "venue": "The Statnet Project (http://www.statnet. org),", "year": 2017}, {"title": "Training products of experts by minimizing contrastive divergence", "authors": ["G.E. Hinton"], "venue": "Neural Computation,", "year": 2002}, {"title": "Reducing the dimensionality of data with neural networks", "authors": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science,", "year": 2006}, {"title": "A class of statistics with asymptotically normal distribution", "authors": ["W. Hoeffding"], "venue": "The Annals of Mathematical Statistics,", "year": 1948}, {"title": "An exponential family of probability distributions for directed graphs", "authors": ["P.W. Holland", "S. Leinhardt"], "venue": "Journal of the American Statistical Association,", "year": 1981}, {"title": "Consistency of the generalized bootstrap for degenerate U -statistics", "authors": ["M. Huskova", "P. Janssen"], "venue": "The Annals of Statistics,", "year": 1993}, {"title": "Estimation of un-normalized statistical models by score matching", "authors": ["A. Hyv\u00e4rinen"], "venue": "Journal of Machine Learning Research,", "year": 2005}, {"title": "Some extensions of score matching", "authors": ["A. Hyv\u00e4rinen"], "venue": "Computational Statistics & Data Analysis,", "year": 2007}, {"title": "Beitrag zur Theorie des Ferro- und Paramagnetismus", "authors": ["E. Ising"], "venue": "PhD thesis,", "year": 1924}, {"title": "A linear-time kernel goodness-of-fit test", "authors": ["W. Jitkrittum", "W. Xu", "Z. Szabo", "K. Fukumizu", "A. Gretton"], "venue": "In Advances in Neural Information Processing Systems", "year": 2017}, {"title": "Sulla determinazione empirica di una legge di distribuzione", "authors": ["A.N. Kolmogorov"], "venue": "Giornale dell\u2019Istituto Italiano degli Attuari,", "year": 1933}, {"title": "Stein\u2019s density approach and information inequalities", "authors": ["C. Ley", "Y. Swan"], "venue": "Electronic Communications in Probability,", "year": 2013}, {"title": "Stein\u2019s method for comparison of univariate distributions", "authors": ["C. Ley", "G. Reinert", "Y. Swan"], "venue": "Probability Surveys,", "year": 2017}, {"title": "A kernelized Stein discrepancy for goodness-of-fit tests", "authors": ["Q. Liu", "J.D. Lee", "M.I. Jordan"], "venue": "In Proceedings of the 33rd International Conference on Machine Learning (ICML),", "year": 2016}, {"title": "Interpretation and generalization of score matching", "authors": ["S. Lyu"], "venue": "In Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence (UAI),", "year": 2009}, {"title": "Exact goodness-of-fit testing for the Ising model", "authors": ["A. Mart\u0131\u0301n del Campo", "S. Cepeda", "C. Uhler"], "venue": "Scandinavian Journal of Statistics,", "year": 2017}, {"title": "Control functionals for Monte Carlo integration", "authors": ["C.J. Oates", "M. Girolami", "N. Chopin"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "year": 2017}, {"title": "On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that can be reasonably supposed to have arisen from random sampling", "authors": ["K. Pearson"], "year": 1900}, {"title": "Approximating stationary distributions of fast mixing Glauber dynamics, with applications to exponential random graphs", "authors": ["G. Reinert", "N. Ross"], "year": 2017}, {"title": "Weisfeiler-Lehman graph kernels", "authors": ["N. Shervashidze", "P. Schweitzer", "E.J. van Leeuwen", "K. Mehlhorn", "K.M. Borgwardt"], "venue": "Journal of Machine Learning Research,", "year": 2011}, {"title": "Table for estimating the goodness of fit of empirical distributions", "authors": ["N. Smirnov"], "venue": "The Annals of Mathematical Statistics,", "year": 1948}, {"title": "Approximate computation of expectations", "authors": ["C. Stein"], "venue": "Institute of Mathematical Statistics Lecture Notes\u2013Monograph Series,", "year": 1986}, {"title": "Instance optimal learning of discrete distributions", "authors": ["G. Valiant", "P. Valiant"], "venue": "In Proceedings of the 48th Annual ACM Symposium on Theory of Computing (STOC),", "year": 2016}], "id": "SP:9886ca8d90b335441020c8d0be34f79a15d800f3", "authors": [{"name": "Jiasen Yang", "affiliations": []}, {"name": "Qiang Liu", "affiliations": []}, {"name": "Vinayak Rao", "affiliations": []}, {"name": "Jennifer Neville", "affiliations": []}], "abstractText": "Recent work has combined Stein\u2019s method with reproducing kernel Hilbert space theory to develop nonparametric goodness-of-fit tests for unnormalized probability distributions. However, the currently available tests apply exclusively to distributions with smooth density functions. In this work, we introduce a kernelized Stein discrepancy measure for discrete spaces, and develop a nonparametric goodness-of-fit test for discrete distributions with intractable normalization constants. Furthermore, we propose a general characterization of Stein operators that encompasses both discrete and continuous distributions, providing a recipe for constructing new Stein operators. We apply the proposed goodness-of-fit test to three statistical models involving discrete distributions, and our experiments show that the proposed test typically outperforms a two-sample test based on the maximum mean discrepancy.", "title": "Goodness-of-Fit Testing for Discrete Distributions via Stein Discrepancy"}