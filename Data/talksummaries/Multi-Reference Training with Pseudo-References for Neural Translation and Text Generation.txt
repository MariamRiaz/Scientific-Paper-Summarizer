5	37	Multiple references are essential for evaluation due to the non-uniqueness of translation and generation unlike classification tasks.
7	13	For example, the NIST Chinese-to-English and Arabic-to-English MT evaluation datasets (2003–2008) have in total around 10,000 Chinese sentences and 10,000 Arabic sentences each with 4 different English translations.
8	60	On the other hand, for image captioning datasets, multiple references are more common not only for evaluation, but also for training, e.g., the MSCOCO (Lin et al., 2014) dataset provides 5 references per image and PASCAL50S and ABSTRACT-50S (Vedantam et al., 2015) even provide 50 references per image.
23	31	Our multiple reference training is achieved by converting a multiple reference dataset to a single reference dataset without losing any information.
25	28	We have the following methods to convert the multiple reference dataset to a single reference dataset D′ (note that the following D′sample one, D ′ uniform and D′shuffle are ordered sets): Sample One: The most straightforward way is to use a different reference in different epochs during training to explore the variances between references.
26	35	For each example, we randomly pick one of the K references in each training epoch (note that the random function will be used in each epoch).
27	19	This method is commonly used in existing image captioning literatures, such as (Karpathy and FeiFei, 2015), but never used in MT.
31	14	So, formally it is: D′shuffle = Shuffle(D ′ uniform) Sample One is supervised by different training signals in different epochs while both Uniform and Shuffle include all the references at one time.
34	21	In text generation tasks, the given multiple references are only a small portion in the whole space of potential references.
35	17	To cover a larger number of references during training, we want to generate more pseudo-references which is similar to existing ones.
46	52	Assume that we first do a pairwise reference compression on first two references, we can merge at four sharing words: Indonesia, its, opposition and foreign, and the lattice will turn to Fig.
62	15	However, “to” in the two references are very different (it is a preposition in the first reference and an infinitive in the second) and should not be merged.
73	25	Formally, given a sentence pair yi and yj , we build a semantic substitution matrix M = R|yi|×|yj |, whose cell Mu,v represents the similarity score between word yi,u and word yj,v.
77	18	The optimization goal of our LM is to minimize the ith word’s prediction error given the surrounding word’s hidden state: p(wi | −−→ hi−1 ⊕ ←−− hi+1) (1) For any new given sentences, we concatenate both forward and backward hidden states to represent each word yi,u in a sentence yi.
80	21	With the help of semantic substitution matrix Mu,v which measures pairwise word similarity, we need to find the optimal word alignment to compress references into a lattice.
83	13	Based on pairwise substitution matrix we can define an optimal pairwise sequence alignment as an optimal path from M0,0 to M|yi|,|yj |.
89	22	Following this intuition, we order reference pairs by the maximum alignment score opt(|yi|, |yj |) (i.e. the score of bottom-right cell in Fig.
94	13	With the global penalty p, the DP algorithm will not align a word pair (yi,u,yi,v) unless Mu,v ≥ p. After the pairwise references alignment, we merge those aligned words.
98	19	1(d) by merging the first two references (assuming they have the highest score) according to pairwise alignment shown in Fig.
100	23	Similar to the previous step, we find alignments according to the dynamic programming and merge to the final lattice (see Fig.
103	25	1(e), we can generate 213 pseudo-refrences in total.
106	17	For those examples generating k pseudoreferences and k > K ′, we calculate all pseudoreferences’ BLEU scores based on gold references, and only keep top K ′−k pseudo-references with highest BLEU score.
114	214	From the figure, we can see that when the sentence length grows, the number of generated references grows exponentially.
117	59	We adopt length reward (Huang et al., 2017) to find optimal sentence length.
121	60	These batch sizes are multiple of the number of references used in experiments, so it is guaranteed that all the references of one single example are in one batch for the Uniform method.
125	17	Besides the original 4 references in the training set, we generate another four dataset with 10, 20, 50 and 100 references including pseudo-references using hard word alignment and soft word alignment.
126	43	We compare the three update methods (Sample One, Uniform, Shuffle) with always using the first reference (First).
128	48	According to Table 3, Shuffle with original 4 references has +0.7 BLEU improvement and Uniform with 50 references has +1.5 BLEU improvement.
140	36	Table 4 shows that the best result is achieved with 20 references using Shuffle.
142	27	This may be because the references in image captioning dataset are much more diverse than those in machine translation dataset.
