17	18	Although this is substantially true for several popular task oriented scenarios, like flight booking (a well known dataset is ATIS – Air Travel Information Services), point of interest navigation, and calendar scheduling (for instance the dataset used in (Eric and Manning, 2017)), other conversational scenarios show different characteristics.
18	56	In this section we focus on conversational agents for the e-commerce scenario, and highlight the characteristics which we believe are relevant for entity recognition.
19	24	E-commerce chat-bots are supposed to carry on a task-oriented dialogue whose goal is helping the user to select products presented in an online shop, and, ultimately, buy them.
21	57	The main focus of the interaction is on products (i.e. users search, compare, assess information on products they are interested in).
25	17	As we will see, there is a high variance in the way online vendors assign and manage such names.
32	22	Compared to other scenarios (e.g. booking hotels and flights), it is quite frequent that user mention more than one product in the same utterance (e.g. ”Please deliver at home a salami pizza, a pepperoni pizza with onions and two mozzarella cheese sandwiches”).
39	46	Given an utterance U = {t1, t2, ..., tn} and a set of entity categories C = {c1, c2, ..., cm}, the task is to label the tokens in U that refer to entities belonging to the categories in C. As an example, using the IOB format (Inside, Outside, Beginning) (Ramshaw and Marcus, 1995), the utterance ”I would like to order a salami pizza and two mozzarella cheese sandwiches”, would be labeled as shown in Table 1.
43	43	They can be composed by a single name (e.g. pasta, carpet, parka) or by more than one token (e.g. capri sofa bed beige, red jeans skinny fit, lightweigh full frame camera, grilled pork belly tacos).
44	40	Nominal entities are typically compositional, as they do allow morphological and syntactic variations (e.g. for food names, spanish baked salmon, roasted salmon and hot smoked salmon), which makes it possible to combine tokens of one entity name with tokens of another entity name to generate new names (e.g. for food names, salmon tacos is a potential food name given the existence of salmon and tacos).
45	22	In addition to adjectival and prepositional modifiers, conjunctions are also very frequent (e.g. beef and bean burritos, black and white t-shirt).
52	34	Partly due to the need of large amounts of training data to feed neural networks, recently there has been a diffused interest on methods for automatically generate synthetic data (see (Jaderberg et al., 2014)).
64	30	This multilayer biLSTM is meant to build an internal representation of the core compositional structure of the entity names that are listed in the gazetteer, and to generalize such structure to recognize new entity names of the same category.
71	31	Adding tokens, using the pattern t1 + i + t2, we obtain other potential negative examples: | buy black and white t-shirt | black and white tshirt and sweater | buy black and white t-shirt and sweater |, and so on.
72	94	According to this procedure, we generate more negative examples than positive.
73	42	In order to avoid an unbalanced dataset, we randomly select two negative examples per positive one: a sub-sequence and an example surrounded by other words, resulting in a 1:2 proportion.
75	32	The generic word embedding is employed to capture generic language use, and it is similar to the one used in (Lample et al., 2016).
82	33	We consider seven features of an entity name: (i) the actual position of the token within an entity name; (ii) the length of the entity name under inspection; (iii) the frequency of the token in the gazetteer; (iv) the average length of the entity name containing a certain token; (v) the average position of the token in the entity name it appears in; (vi) the bigram probability with reference to the previous token in the entity name; (vii) the list of all the possible PoS associated to the token.
84	17	After classification the algorithm takes a further step to select the actual entities, by ranking the candidates according to the confidence score provided by the classifier, and by selecting the top not overlapping candidates.
85	50	As an example, the utterance “I’m looking for golden yellow shorts and dark blue shirt” contains six sub-sequences that are classified as positive by the NNg classifier (lines [1-5]): | shorts | yellow shorts | golden yellow shorts | shirt | blue shirt | dark blue shirt |, while all other sub-sequences, such as: | I’m looking | looking for a golden | shorts and dark | dark blue |, are classified as negative.
86	32	Then, positive examples are ranked according to their confidence score (lines [6]): | golden yellow shorts | yellow shorts | dark blue shirt | etc.
121	99	We also report additional metrics that try to grasp the complexity of entity name in the gazetteer: (i) the normalized type-token ratio (TTR), as a rough measure of how much lexical diversity there is for the nominal entities in a gazetteer, see (Richards, 1987); (ii) the ratio of type1 tokens, i.e. tokens that can appear in the first position of an entity name but also in other positions, and type2 tokens, i.e. tokens appearing at the end and elsewhere; (iii) the ratio of entities that contain another entity as sub-part of their name.
126	39	After splitting each gazetteer using a 64:16:20 ratio (train:dev:test), we created the aforementioned data sets, where – for each entity i (positive example) present in the train-dev splits – we added two negative examples obtained by randomly selecting one of the methodologies described in Section 4.1.
136	27	We split the templates in a 64:16:20 ratio (train:dev:test) before lexicalization: to lexicalize SUtrain we randomly choose entities that were in the train split of the gazetteers, while for SUtest we randomly choose entities than were in the test split of the gazetteers.
137	53	It should be noted that we used this procedure to better isolate the effect of entity name and their compositional nature over learning approaches, in fact: (i) we controlled for the impact of patterns on learning by using the same patterns across data sets train and test splits.
139	87	In this way we can assess the ability of the approaches to learn the structure of entity names and generalize it to new examples.
140	49	So, for example, a simple baseline that uses exact match over the train gazetteers to identify entities in the test sentences would report a F1 of 0.
146	22	The topological configuration of NNg is kept constant, as described in Section 4.
150	33	Experiments and Comparison on SU.
151	18	Table 5 reports the comparison among the rule-based baseline, the NNp baseline, and the NNg approach.
156	25	While for food results are evident (the highest length-SD, TTR, type1 and type2 token ratios and high sub-entity ratio affect the performances even if the gazetteers are big) for furniture and clothing we need to look closer at the metrics in Table 2.
