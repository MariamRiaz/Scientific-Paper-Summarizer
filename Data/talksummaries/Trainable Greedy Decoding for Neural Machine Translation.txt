23	27	Instead of manually designing a new decoding algorithm suitable for neural machine translation, we propose to learn a decoding algorithm with an arbitrary decoding objective.
35	62	Maximum Likelihood Learning We train a neural machine translation model, or equivalently estimate the parameters θg, θf and θe, by maximizing the log-probability of a reference translation Ŷ = {ŷ1, ..., ŷT } given a source sentence.
36	15	That is, we maximize the log-likelihood function: JML(θg, θf , θe) = 1 N N∑ n=1 Tn∑ t=1 log pθ(ŷnt |ŷn<t, Xn), given a training set consisting of N source-target sentence pairs.
47	62	At each time step t, beam search picks K hypotheses with the highest scores ( ∏t t′=1 p(yt|y<t, X)).
58	26	For instance, Tu et al. (2016a) showed that beam search with a very large beam, which is supposed to find translations with better logprobabilities, suffers from pathological translations of very short length, resulting in low translation quality.
65	39	The main idea behind NPAD algorithm is that a better translation with a higher log-probability may be found by injecting unstructured noise in the transition function of a recurrent network.
68	35	In this work, we propose to significantly extend NPAD by replacing the unstructured noise t with a parametric function approximator, or an agent, πφ.
69	32	This agent takes as input the previous hidden state zt−1, previously decoded word ŷt−1 and the time-dependent context vector et(X; θe) and outputs a real-valued vectorial action at ∈ Rdim(zt).
70	17	Such an agent is trained such that greedy decoding with the agent finds a translation that maximizes any predefined, arbitrary decoding objective, while the underlying neural machine translation model is pretrained and fixed.
73	19	Related Work: Soothsayer prediction function Independently from and concurrently with our work here, Li et al. (2017) proposed, just two weeks earlier, to train a neural network that predicts an arbitrary decoding objective given a source sentence and a partial hypothesis, or a prefix of translation, and to use it as an auxiliary score in beam search.
74	30	For training such a network, referred to as a Q network in their paper, they generate each training example by either running beam search or using a ground-truth translation (when appropriate) for each source sentence.
75	18	This approach allows one to use an arbitrary decoding objective, but it still re- lies heavily on the log-probability of the underlying neural translation system in actual decoding.
76	101	We expect a combination of these and our approaches may further improve decoding for neural machine translation in the future.
77	59	While all the parameters—θg, θf and θe— of the underlying neural translation model are fixed, we only update the parameters φ of the agent π.
78	202	This ensures the generality of the pretrained translation model, and allows us to train multiple trainable greedy decoding agents with different decoding objectives, maximizing the utility of a single trained translation model.
79	17	Let us denote by R our arbitrary decoding objective as a function that scores a translation generated from trainable greedy decoding.
80	186	Then, our learning objective for trainable greedy decoding is JA(φ) = EŶ=Gπ(X)X∼D [ R(Ŷ ) ] , where we used Gπ(X) as a shorthand for trainable greedy decoding with an agent π.
82	23	First, the decoding objective R may not be differentiable with respect to the agent.
85	25	Second, the agent here is a real-valued, deterministic policy with a very high-dimensional action space (1000s of dimensions), which is well known to be difficult.
89	33	Instead, following (Silver et al., 2014; Lillicrap et al., 2015), we use a parametric, differentiable approximator, called a critic Rc, for the non-differentiable objective R. We train the critic by minimizing JC(ψ) = EŶ=Gπ(X)X∼D [ Rcψ(z1:T )−R(Ŷ ) ]2 .
90	16	The critic observes the state-action sequence of the agent π via the modified hidden states (z1, .
97	54	Algorithm 1 Trainable Greedy Decoding Require: NMT θ, actor φ, critic ψ, Nc, Na, Sc, Sa, τ 1: Train θ using MLE on training set D; 2: Initialize φ and ψ; 3: Shuffle D twice into Dφ and Dψ 4: while stopping criterion is not met do 5: for t = 1 : Nc do 6: Draw a translation pair: (X,Y ) ∼ Dψ; 7: r, rc = DECODE(Sc, X, Y, 1) 8: Update ψ using∇ψ ∑ k (r c k − rk)2/(Sc + 1) 9: for t = 1 : Na do 10: Draw a translation pair: (X,Y ) ∼ Dφ; 11: r, rc = DECODE(Sa, X, Y, 0) 12: Compute wk = exp (− (rck − rk)2 /τ) 13: Compute w̃k = wk/ ∑ k wk 14: Update φ using −∑k (w̃k · ∇φrck) Function: DECODE(S,X, Y, c) 1: Ys = {}, Zs = {}, r = {}, rc = {}; 2: for k = 1 : S do 3: Sample noise ∼ N (0, σ2) for each action; 4: Greedy decoding Ŷ k = Gθ,φ(X) with ; 5: Collect hidden states zk1:T given X , Ŷ , θ, φ 6: Ys ← Ys ∪ {Y k} 7: Zs ← Zs ∪ {zk1:T } 8: if c = 1 then 9: Collect hidden states z1:T given X , Y , θ 10: Ys ← Ys ∪ {Y } 11: Zs ← Zs ∪ {z1:T } 12: for Ŷ , Z ∈ Ys, Zs do 13: Compute the critic output rc ← Rcψ(Z, Ŷ ) 14: Compute true reward r ← R(Y, Ŷ ) 15: return r, rc Unlike the original objective, this objective function is fully differentiable with respect to the agent π.
98	40	We thus use a usual stochastic gradient descent algorithm to train the agent, while simultaneously training the critic.
99	82	We do so by alternating between training the actor and critic.
100	15	Note that we maximize the return of a full episode rather than the Q value, unlike usual approaches in reinforcement learning.
101	56	Challenges The most apparent challenge for training such a deterministic actor with a large action space is that most of action configurations will lead to zero return.
103	52	This issue has however turned out to be less of a problem than in a usual reinforcement learning setting, as the state and action spaces are well structured thanks to pretraining by maximum likelihood learning.
104	25	As observed by Cho (2016), any reasonable perturbation to the hidden state of the recurrent network generates a reasonable translation which would re- ceive again a reasonable return.
105	30	Although this property of dense reward makes the problem of trainable greedy decoding more manageable, we have observed other issues during our preliminary experiment with the vanilla deterministic policy gradient.
107	49	Critic-Aware Actor Learning A major goal of the critic is not to estimate the return of a given episode, but to estimate the gradient of the return evaluated given an episode.
