2	31	They are mechanistic computational models that, at some level, do the same task people do in the course of ordinary language comprehension.
3	33	As such, they offer a way to gain insight into the operation of the human sentence processing mechanism (for a review see Hale, 2017).
7	36	Via well-known complexity metrics, the intermediate states of this procedure yield quantitative predictions about language comprehension difficulty.
8	43	Juxtaposing these predictions against data from human encephalography (EEG), we find that they reliably derive several amplitude effects including the P600, which is known to be associated with syntactic processing (e.g. Osterhout and Holcomb, 1992).
14	30	Sections 3 then reviews a previously-proposed beam search procedure for them.
20	57	Recurrent neural network grammars (henceforth: RNNGs Kuncoro et al., 2017; Dyer et al., 2016) are probabilistic models that generate trees.
22	70	In the vanilla version of RNNG, these steps follow a depth-first traversal of the developing phrase structure tree.
25	23	This stack is “neuralized” such that each stack entry corresponds to a numerical vector.
29	30	In the present paper, these stack-summaries serve as input to a multi-layer perceptron whose output is converted via softmax into a categorical distribution over three possible parser actions: open a new constituent, close off the latest constituent, or generate a word.
47	31	To address it, Stern et al. (2017) propose a word-synchronous variant of beam search.
67	38	In the domain of language, these are traditionally referred to as complexity metrics because of the way they quantify the “processing complexity” of particular sentences.
78	23	Electroencephalography (EEG) is an experimental technique that measures very small voltage fluctuations on the scalp.
79	23	For a review emphasizing its implications vis-á-vis computational models, see Murphy et al. (2018).
81	25	All participants scored significantly better than chance on a post-session 8-question comprehension quiz.
82	35	An additional ten datasets were excluded for not meeting this behavioral criterion, six due to excessive noise, and three due to experimenter error.
83	43	All participants provided written informed consent under the oversight of the University of Michigan HSBS Institutional Review Board (#HUM00081060) and were compensated $15/h.3 Data were recorded at 500 Hz from 61 active electrodes (impedences < 25 kΩ) and divided into 2129 epochs, spanning -0.3–1 s around the onset of each word in the story.
84	21	Ocular artifacts were removed using ICA, and remaining epochs with excessive noise were excluded.
85	88	The data were filtered from 0.5–40 Hz, baseline corrected against a 100 ms pre-word interval, and separated into epochs for content words and epochs for function words because of interactions between parsing variables of interest and word-class (Roark et al., 2009).
86	44	Linear regression was used per-participant, at each time-point and electrode, to identify content-word EEG amplitudes that correlate with complexity metrics derived from the RNNG+beam search combination via the complexity metrics in Table 2.
88	31	Each Target predictor was included in its own model, along with several Control predictors that are known to influence sentence processing: sentence order, word-order in sentence, log word frequency (Lund and Burgess, 1996), frequency of the previous and subsequent word, and acoustic sound power averaged over the first 50 ms of the epoch.
89	28	All predictors were mean-centered.
98	30	An example would be the length 7 string shown below (S (NP the hungry cat )NP (VP Here, vertical lines separate symbols whose vector encoding would be considered separately by RNNG−comp.
101	65	The balanced parentheses (NP and )NP are rather like instructions for some subsequent agent who might later perform the kind of syntactic composition that occurs online in RNNGs, albeit in an implicit manner.
102	44	In all cases, these language models were trained on chapters 2–12 of Alice’s Adventures in Wonderland.
107	51	During RNNG training, the first chapter was used as a development set, proceeding until the per-word perplexity over all parser actions on this set reached a minimum, 180.
111	40	Hyperparameter settings were determined by grid search in a region near the one which yielded good performance on the Penn Treebank benchmark reported on Table 1.
119	26	We compared RNNG to its degraded cousin, RNNG−comp, in three regions of interest shown in Figure 4.
126	33	These 24 tests (eight effects by three regions) motivate a Bonferroni correction of α = 0.002 = 0.05/24.
138	84	The absence of an N400 effect in this analysis could be attributable to the choice of electrodes, or perhaps the modality of the stimulus narrative, i.e. spoken versus read.
145	102	Recurrent neural net grammars indeed learn something about natural language syntax, and what they learn corresponds to indices of human language processing difficulty that are manifested in electroencephalography.
