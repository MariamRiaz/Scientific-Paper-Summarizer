0	27	Over the past few years neural models based on the encode-attend-decode (Bahdanau et al., 2014) paradigm have shown great success in various natural language generation (NLG) tasks such as machine translation (Bahdanau et al., 2014), abstractive summarization ((Rush et al., 2015),(Nallapati et al., 2016)) dialog (Li et al., 2016), etc.
1	156	One such NLG problem which has not received enough attention in the past is query based abstractive text summarization where the aim is to generate the summary of a document in the context of a query.
2	41	In general, abstractive summarization, aims to cover all the salient points of a document in a compact and coherent fashion.
3	5	On the other hand, query focused summarization highlights those points that are relevant in the context of the query.
4	4	Thus given a document on “the super bowl”, the query “How was the half-time show?”, would result in a summary that would not cover the actual game itself.
5	15	Note that there has been some work on query based extractive summarization in the past where the aim is to simply extract the most salient sentence(s) from a document and treat these as a summary.
7	2	Since, we were interested in abstractive (as opposed to extractive) summarization we created a new dataset based on debatepedia.
8	4	This dataset contains triplets of the form (query, document, summary).
10	6	Using this dataset as a testbed, we focus on a recurring problem in models based on the encode-attend-decode paradigm.
13	15	This problem has also been reported by (Chen et al., 2016) in the context of summarization and by (Sankaran et al., 2016) in the context of machine translation.
14	22	We first provide an intuitive explanation for this problem and then propose a solution for alleviating it.
16	11	Each word is produced by feeding a new context vector to the decoder at each time step by attending to different parts of the document and query.
17	32	If the decoder produces the same word or phrase repeatedly then it could mean that the context vectors fed to the decoder at these time steps are very similar.
22	51	To account for the complete history (or all previous context vectors) we also propose an extension of this idea where we pass the sequence of context vectors through a LSTM (Hochreiter and Schmidhuber, 1997) and ensure that the current state produced by the LSTM is orthogonal to the history.
23	54	At each time step, the state of the LSTM is then fed to the decoder to produce one word in the summary.
24	29	Our contributions can be summarized as follows: (i) We propose a new dataset for query based abstractive summarization and evaluate encode-attend-decode models on this dataset (ii) We study the problem of repeating phrases in NLG in the context of this dataset and propose two solutions for countering this problem.
25	22	We show that our method outperforms a vanilla encoder-decoder model with a gain of 28% (absolute) in ROUGE-L score (iii) We also demonstrate that our method clearly outperforms a recent state of the art method proposed for handling the problem of repeating phrases with a gain of 7% (absolute) in ROUGE-L scores (iv) We do a qualitative analysis of the results and show that our model indeed produces outputs with fewer repetitions.
48	5	A given topic can belong to more than one category.
50	37	The average number of queries per debate is 5 and the average number of documents per query is 4.
53	16	It also lists the set of documents and an abstractive summary associated with each query.
54	4	As is obvious from the example, the summary is an abstractive summary and not extracted directly from the document.
57	23	We used 10 fold cross validation for all our experiments.
58	53	Each fold uses 80% of the documents for training, 10% for validation and 10% for testing.
59	127	Given a query q = q1, q2, ..., qk containing k words, a document d = d1, d2, ..., dn containing n words, the task is to generate a contextual summary y = y1, y2, ..., ym containing Figure 1: Queries associated with the topic “algae biofuel” Figure 2: Documents and summaries for a given query m words.
60	128	This can be modeled as the problem of finding a y∗ that maximizes the probability p(y|q,d) which can be further decomposed as: y∗ = argmax y m∏ t=1 p(yt|y1, ..., yt−1,q,d) (1) We now describe a way of modeling p(yt|y1, ..., yt−1,q,d) using the neural encoderattention-decoder paradigm.
61	70	The proposed model contains the following components: (i) an encoder RNN for the query (ii) an encoder RNN for the document (iii) attention mechanism for the query (iv) attention mechanism for the document and (v) a decoder RNN.
63	16	It reads the query q = q1, q2, ..., qk from left to right and computes a hidden representation for each time-step as: hqi = GRUq(h q i−1, e(qi)) (2) where e(qi) ∈ Rd is the d-dimensional embedding of the query word qi.
64	59	Encoder for the document: This is similar to the query encoder and reads the document d = d1, d2, ..., dn from left to right and computes a hidden representation for each time-step as: hdi = GRUd(h d i−1, e(di)) (3) where e(di) ∈ Rd is the d-dimensional embedding of the document word di.
65	17	Attention mechanism for the query : At each time step, the decoder produces an output word by focusing on different portions of the query (document) with the help of a query (document) attention model.
