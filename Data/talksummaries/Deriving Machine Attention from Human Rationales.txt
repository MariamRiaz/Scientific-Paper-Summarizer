1	16	In addition to significant performance gains, these models are attractive, as attention is often used as a proxy for human interpretable rationales.
5	16	The notions of rationale and attention are closely related.
6	37	Both of them highlight word importance for the final prediction.
7	31	In the case of rationale, the importance is expressed as a hard selection, while attention provides a soft distribution over the words.
9	29	One obvious approach to improve low- resource performance is to directly use human rationales as a supervision for attention generation.
12	18	Instead of providing a soft distribution, human rationales only provide the binary indication about relevance.
15	30	In contrast, machine attention is always derived as a part of a specific model architecture.
16	19	To further understand this connection, we empirically compare models informed by human rationales and those by high-quality attention.
18	91	This “oracle” attention is then used to guide a model that only has access to a small subset of this training data.
19	44	Not only does this model outperform the oracle-free variant, but it also yields substantial gains over its counterpart trained with human ra- tionales — 89.98 % vs 85.22 % average accuracy on three aspects of hotel review (see Section 4 for details).
23	19	Specifically, we learn a mapping from human rationales to high-quality attention (R2A).
24	37	We hypothesize that this mapping is generalizable across tasks and thus can be transferred from resource-rich tasks.2 Figure 1 illustrates that in both tasks, attention weighs rationale words in a similar fashion: highlighting taskspecific nouns and adjectives, while downplaying functional words.
30	15	The first one is an attention-based model for the source task(s) that provides supervision for attention generation.
35	52	This attention is consequently used to supervise the training of the target classifier.
62	41	For those labeled examples, we assume access to human-annotated rationales.
71	16	Model architecture Figure 3 illustrates the architecture of our R2A model, which consists of three components.
75	22	We achieve this goal through domain adversarial training over the source data and the unlabeled target data (Section 3.2).
76	15	• Attention generation This module learns to predict the intermediate attention obtained from the first module based on the domain-invariant representation and the rationales (Section 3.3).
89	33	This module enables effective transfer—especially in the presence of significant variance between the source and target domains.
90	20	We achieve the first goal by optimizing a language modeling objective and the second goal by minimizing the Wasserstein distance between the source and target distribution.
121	16	We minimize the prediction loss, LTlbl, on the labeled target data together with the cosine distance, LTatt, between the R2A-generated attention and the attention generated by this target classifier.
178	14	The error reduction over the best baseline is 15.08% on average.
179	13	We compare the learning curve in Figure 4.
187	20	It is worth pointing out that our R2A model has never seen any labeled hotel reviews during training.
188	18	Table 6 presents the average cosine distance between the R2A-generated attention and the oracle attention over the target training set.
189	48	Compared with human rationales, the R2A-generated attention is much closer to the oracle attention.
193	20	For example, given the rationale sentence “not the cleanest rooms but bed was clean and so was bathroom”, R2A recognizes that not every token is equally important, and the attention should focus more on “clean”, “cleanest”, “rooms” and “bathroom”.
200	35	In this paper, we propose a novel approach that utilizes the connection between human rationales and machine attention to improve the performance of low-resource tasks.
201	53	Specifically, we learn a transferrable mapping from rationales to high-quality attention on resource-rich tasks.
203	16	Experimental results on both aspect and domain transfer validate that the R2A-generated attention serves as a better form of supervision.
