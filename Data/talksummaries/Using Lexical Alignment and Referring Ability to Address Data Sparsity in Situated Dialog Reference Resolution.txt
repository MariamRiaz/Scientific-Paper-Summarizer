16	16	Thus, RL varies less within a given dialog than across dialogs, and variation of RL has an inverse relationship with the length of the time two participants interact due to alignment of dialog participants’ use of language (Clark and Wilkes-Gibbs, 1986; Garrod and Anderson, 1987; Brennan, 1996).
17	32	In this paper, we present two contributions to the automatic learning of referential semantics for reference resolution in situated dialog that address these problems: Firstly, we show the benefits of adapting models of RL semantics to a specific dialog as it progresses to accommodate the dyad’s idiosyncratic use of RL.
27	25	In Figure 1, speakerA initiates the RE the one to the left and immediately expands it in an episodic manner (Clark and Wilkes-Gibbs, 1986, 4, 17).
49	42	The data used is that of Shore et al. (2018), a set of |D| = 42 task-oriented dialogs (mean duration µ = 15:25 minutes, standard deviation SD = 1:13, total 647:35) in which one participant is an instructor referring to specific pieces on a shared game board which the other participant, the manipulator, must then attempt to resolve by selecting without the aid of extra-linguistic cues (see Figures 1–2): They sit at different locations and communicate solely through an audio channel.
54	15	Each dialog d ∈ D has |R| = 20 randomlygenerated game pieces and is divided into individual game rounds d , 〈d′1 .
58	18	A sequence of tokens T was transcribed from the speech of both participants using Penn Treebank tokenization rules (Marcus et al., 1993).
60	14	The reference resolution method used as a baseline was a words-as-classifiers (WaC) regression model (cf.
61	32	In this framework, an individual logistic regression model pt(r) , σ(w T t r + bt) is trained for each token type t, predicting the probability of a given entity r being the token’s TRUE referent r̂, given the feature vector r representing shape, size, color and position (see the Supplementary Material for details).
64	14	The score of a given entity r being the referent r = r̂ of a set of RL tokens T is defined as the normalized linear combination of the tokens’ corresponding classifiers pt(r): p′(r = r̂, T ) , 1 n n∑ t∈T pt(r) (1) For training, language in each round (R, r̂, T ) is defined as a bag of words T referring to the referent r̂.
65	48	For each token t ∈ T , a training example is defined for the referent r̂ (with a target score of 1) as well as for each non-referent entity r ∈ R \ r̂ (with a target score of 0).
73	27	This is unlike Kennington et al. (2015), who only used language from (manually annotated) REs.
74	43	As argued above, REs cannot easily be identified in the type of dialog data we are addressing.
76	17	We did 42-fold cross-validation, in each fold using 40 dialogs for training as background data, one for testing and one for use as random data to compare the effects of dialog-specific data to (see Section 5 below).
82	39	Still, it has two shortcomings: Firstly, it infers a static model of referential semantics which is good across di- alogs but is suboptimal for language within dialogs due to effects of language alignment (Garrod and Anderson, 1987; Brennan, 1996; Brennan and Clark, 1996).
91	51	We fit a linear mixed model with conditions Adt, RndAdt, Wgt and scaled Tokens as linear fixed effects and game round ordinality (ROUND) as a quadratic fixed effect: Wgt denotes weighting word classifiers by RA, which will be discussed in Section 6.
97	15	Despite that RR correlates with ROUND i even for the baseline method due to dialogic lexical alignment (cf.
115	36	Since the referent r̂ is chosen at random by the game, the amount of references to an entity increases with round ordinality, and so this corresponds with Clark andWilkesGibbs (1986).
121	38	In fact, Figure 7 shows that Wgt has better MRR using only 12 randomly-chosen dialogs as background data than the Baseline does with 40, and adaptation and weighting together (Adt,Wgt) has better MRR with only 7.
123	33	In the baseline (A), the classifier for e.g. color has as much weight as e.g. rectangle although the former is not a useful signifier for the given task.
125	34	Finally, when adapting the model with interaction data (C), models for semantically-heavy words like violet better fit the dyad’s RL use, bringing rank to 1.
126	84	When both incrementally adapting semantic models with in-domain dialog data and weighting by RA, MRR for reference resolution was improved by 32.5% over the baseline (see Table 3).
127	26	We have shown that it is possible to improve reference resolution for situated dialog by incrementally adapting word semantic model parameters to a given dialog in order to accommodate idiosyncratic language use by dyad partners, and the effect of the partners’ own alignment makes this method even more beneficial over time.
128	200	Additionally, we have defined a metric of word referring ability which is derived from a word’s referential semantics in situated dialog but holds across individual dialogs despite dyadic variation in RL use.
130	32	Both of these aspects are beneficial to natural language understanding (NLU) for situated dialog due to the difficulty of acquiring data domain-appropriate data.
132	31	Riccardi and Gorin, 2000) despite that little work has been done in this regard specifically for reference resolution.
134	26	Moreover, due to the fact that dialog participants’ use of RL converges over time (Garrod and Anderson, 1987; Brennan, 1996; Brennan and Clark, 1996), the task should adapt a pre-trained reference resolution model not only for a given dialog but also to the given state of that dialog; On the other hand, Iida et al. (2010) incorporate intra-dialogic knowledge but do not adapt to inter-dialogic effects.
136	17	However, this inaccurately assumes inter-word independence, since it does not encode a word’s context: For example, the RA of not was 0.0638, which is relatively low.
137	14	While it is a poor signifier in itself, it reverses the polarity of the predicate it modifies.
138	35	For example, in it’s the baby blue K the light one not the dark one, the NP the dark one should in fact have negative RA: Entities with a low semantic score ∑ t∈〈the,dark,one〉 pt(r) should in fact be preferred over those those with a high score.
139	28	This could be addressed via structural prediction (e.g. conditional random fields or neural networks) or even higher-order n-grams, but these methods cannot be easily utilized given the typically small size of situated dialog datasets.
