{"sections": [{"text": "ar X\niv :1\n70 6.\n04 96\n4v 4\n[ cs\n.L G\n] 1\n4 Ju\nn 20\n18\ning theory for the ResNet architectures which simultaneously creates a new technique for boosting over features (in contrast to labels) and provides a new algorithm for ResNet-style architectures. Our proposed training algorithm, BoostResNet, is particularly suitable in nondifferentiable architectures. Our method only requires the relatively inexpensive sequential training of T \u201cshallow ResNets\u201d. We prove that the training error decays exponentially with the depth T if the weak module classifiers that we train perform slightly better than some weak baseline. In other words, we propose a weak learning condition and prove a boosting theory for ResNet under the weak learning condition. A generalization error bound based on margin theory is proved and suggests that ResNet could be resistant to overfitting using a network with l1 norm bounded weights."}, {"heading": "1. Introduction", "text": "Why do residual neural networks (ResNets) (He et al., 2016) and the related highway networks (Srivastava et al., 2015) work? And if we study closely why they work, can we come up with new understandings of how to train them and how to define working algorithms?\nDeep neural networks have elicited breakthrough successes in machine learning, especially in image classification and object recognition (Krizhevsky et al., 2012; Sermanet et al., 2013; Simonyan & Zisserman, 2014; Zeiler & Fergus, 2014) in recent years. As the number of layers increases, the nonlinear network becomes more powerful, deriving richer features from input data. Em-\n1Department of Computer Science, University of Maryland; 2Department of Computer Science, Princeton University; 3Microsoft Research. Correspondence to: Furong Huang <furongh@cs.umd.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\npirical studies suggest that challenging tasks in image classification (He et al., 2015; Ioffe & Szegedy, 2015; Simonyan & Zisserman, 2014; Szegedy et al., 2015) and object recognition (Girshick, 2015; Girshick et al., 2014; He et al., 2014; Long et al., 2015; Ren et al., 2015) often require \u201cdeep\u201d networks, consisting of tens or hundreds of layers. Theoretical analyses have further justified the power of deep networks (Mhaskar & Poggio, 2016) compared to shallow networks.\nHowever, deep neural networks are difficult to train despite their intrinsic representational power. Stochastic gradient descent with back-propagation (BP) (LeCun et al., 1989) and its variants are commonly used to solve the non-convex optimization problems. A major challenge that exists for training both shallow and deep networks is vanishing or exploding gradients (Bengio et al., 1994; Glorot & Bengio, 2010). Recent works have proposed normalization techniques (Glorot & Bengio, 2010; LeCun et al., 2012; Ioffe & Szegedy, 2015; Saxe et al., 2013) to effectively ease the problem and achieve convergence. In training deep networks, however, a surprising training performance degradation is observed (He & Sun, 2015; Srivastava et al., 2015; He et al., 2016): the training performance degrades rapidly with increased network depth after some saturation point. This training performance degradation is representationally surprising as one can easily construct a deep network identical to a shallow network by forcing any part of the deep network to be the same as the shallow network with the remaining layers functioning as identity maps. He et al. (He et al., 2016) presented a residual network (ResNet) learning framework to ease the training of networks that are substantially deeper than those used previously. And they explicitly reformulate the layers as learning residual functions with reference to the layer inputs by adding identity loops to the layers. It is shown in (Hardt & Ma, 2016) that identity loops ease the problem of spurious local optima in shallow networks. Srivastava et al. (Srivastava et al., 2015) introduce a novel architecture that enables the optimization of networks with virtually arbitrary depth through the use of a learned gating mechanism for regulating information flow.\nEmpirical evidence overwhelmingly shows that these deep residual networks are easier to optimize than non-residual ones. Can we develop a theoretical justification for this\nobservation? And does that justification point us towards new algorithms with better characteristics?"}, {"heading": "1.1. Summary of Results", "text": "We propose a new framework, multi-channel telescoping sum boosting (defined in Section 4), to characterize a feed forward ResNet in Section 3. We show that the top level (final) output of a ResNet can be thought of as a layer-bylayer boosting method (defined in Section 2). Traditional boosting, which ensembles \u201cestimated score functions\u201d or \u201cestimated labels\u201d from weak learners, does not work in the ResNet setting because of two reasons: (1) ResNet is a telescoping sum boosting of weak learners, not a naive (weighted) ensemble; (2) ResNet boosts over \u201crepresentations\u201d, not \u201cestimated labels\u201d. We provide the first error bound for telescoping sum boosting over features. Boosting over features and boosting over labels are different. There is no existing work that proves a boosting theory (guaranteed 0 training error) for boosting features. Moreover, the special structure of a ResNet entails more complicated analysis: telescoping sum boosting, which has never been introduced before in the existing literature.\nWe introduce a learning algorithm (BoostResNet) guaranteed to reduce error exponentially as depth increases so long as a weak learning assumption is obeyed. BoostResNet adaptively selects training samples or changes the cost function (Section 4 Theorem 4.2). In Section 4.4, we analyze the generalization error of BoostResNet and provide advice to avoid overfitting. The procedure trains each residual block sequentially, only requiring that each provides a better-than-a-weak-baseline in predicting labels.\nBoostResNet requires radically lower computational complexity for training than end-to-end back propagation (e2eBP). The number of gradient updates required by BoostResNet is much smaller than e2eBP as discussed in Section 4.3. Memorywise, BoostResNet requires only individual layers of the network to be in the graphics processing unit (GPU) while e2eBP inevitably keeps all layers in the GPU. For example, in a state-of-the-art deep ResNet, this might reduce the RAM requirements for GPU by a factor of the depth of the network. Similar improvements in computation are observed since each e2eBP step involves back propagating through the entire deep network.\nExperimentally, we compare BoostResNet with e2eBP over two types of feed-forward ResNets, multilayer perceptron residual network (MLP-ResNet) and convolutional neural network residual network (CNN-ResNet), on multiple datasets. BoostResNet shows substantial computational performance improvements and accuracy improvement under the MLP-ResNet architecture. Under CNN-ResNet, a faster convergence for BoostResNet is observed.\nOne of the hallmarks of our approach is to make an explicit distinction between the classes of the multiclass learning problem and channels that are constructed by the learning procedure. A channel here is essentially a scalar value modified by the rounds of boosting so as to implicitly minimize the multiclass error rate. Our multi-channel telescoping sum boosting learning framework is not limited to ResNet and can be extended to other, even non-differentiable, nonlinear hypothesis units, such as decision trees or tensor decompositions. Our contribution does not limit to explaining ResNet in the boosting framework, we have also developed a new boosting framework for other relevant tasks that require multi-channel telescoping sum structure."}, {"heading": "1.2. Related Works", "text": "Training deep neural networks has been an active research area in the past few years. The main optimization challenge lies in the highly non-convex nature of the loss function. There are two main ways to address this optimization problem: one is to select a loss function and network architecture that have better geometric properties (details refer to appendix A.1), and the other is to improve the network\u2019s learning procedure (details refer to appendix A.2).\nMany authors have previously looked into neural networks and boosting, each in a different way. Bengio et al. (2006) introduce single hidden layer convex neural networks, and propose a gradient boosting algorithm to learn the weights of the linear classifier. The approach has not been generalized to deep networks with more than one hidden layer. Shalev-Shwartz (2014) proposes a selfieBoost algorithm which boosts the accuracy of an entire network. Our algorithm is different as we instead construct ensembles of classifiers. Veit et al. (2016) interpret residual networks as a collection of many paths of differing length. Their empirical study shows that residual networks avoid the vanishing gradient problem by introducing short paths which can carry gradient throughout the extent of very deep networks.\nComparison with AdaNet The authors of AdaNet (Cortes et al., 2016) consider ensembles of neural layers with a boosting-style algorithm and provide a method for structural learning of neural networks by optimizing over the generalization bound, which consists of the training error and the complexity of the AdaNet architecture. AdaNet uses the traditional boosting framework where weak classifiers are being boosted. Therefore, to obtain low training error guarantee, AdaNet maps the feature vectors (hidden layer representations) to a classifier space and boosts the weak classifiers. In AdaNet, features (representations) from each lower layer have to be fed into a classifier (in other words, be transferred to score function in the label space). This is because AdaNet uses traditional boosting, which ensembles score functions or labels. As\na result, the top classifier in AdaNet has to be connected to all lower layers, making the structure bushy. Therefore AdaNet chooses its own structure during learning, and its boosting theory does not necessarily work for a ResNet structure.\nOur BoostResNet, instead, boosts features (representations) over multiple channels, and thus produces a less \u201cbushy\u201d architecture. We are able to boost features by developing this new \u201ctelescoping-sum boosting\u201d framework, one of our main contributions. We come up with the new weak learning condition for the telescoping-sum boosting framework. The algorithm is also very different from AdaNet and is explained in details in section 3 and 4.\nBoostResNet focuses on a ResNet architecture, provides a new training algorithm for ResNet, and proves a training error guarantee for deep ResNet architecture. A ResNetstyle architecture is a special case of AdaNet, so AdaNet generalization guarantee applies here and our generalization analysis is built upon their work."}, {"heading": "2. Preliminaries", "text": "A residual neural network (ResNet) is composed of stacked entities referred to as residual blocks. Each residual block consists of a neural network module and an identity loop (shortcut). Commonly used modules include MLP and CNN. Throughout this paper, we consider training and test examples generated i.i.d. from some distribution D over X \u00d7Y , where X is the input space and Y is the label space. We denote by S = ((x1, y1), (x2, y2), . . . , (xm, ym)) a training set of m examples drawn according to Dm.\nA Residual Block of ResNet ResNet consists of residual blocks. Each residual block contains a module and an identity loop. Let each module map its input x\u0303 to ft(x\u0303) where\nt denotes the level of the modules. Each module ft is a nonlinear unit with n channels, i.e., ft(\u00b7) \u2208 Rn. In multilayer perceptron residual network (MLP-ResNet), ft is a shallow MLP, for instance, ft(x\u0303) = V\u0303 \u22a4 t \u03c3(W\u0303 \u22a4 t x\u0303) where W\u0303t \u2208 Rn\u00d7k, V\u0303t \u2208 Rk\u00d7n and \u03c3 is a nonlinear operator such as sigmoidal function or relu function. Similarly, in convolutional neural network residual network (CNN-ResNet), ft(\u00b7) represents the t-th convolutional module. Then the t-th residual block outputs gt+1(x)\ngt+1(x) = ft(gt(x)) + gt(x), (1)\nwhere x is the input fed to the ResNet. See Figure 1 for an illustration of a ResNet, which consists of stacked residual blocks (each residual block contains a nonlinear module and an identity loop).\nOutput of ResNet Due to the recursive relation specified in Equation (1), the output of the T -th residual block is equal to the summation over lower module outputs, i.e., gT+1(x) = \u2211T\nt=0 ft(gt(x)), where g0(x) = 0 and f0(g0(x)) = x. For binary classification tasks, the final output of a ResNet given input x is rendered after a linear classifier w \u2208 Rn on representation gT+1(x) (In the multiclass setting, let C be the number of classes; the linear classifier W \u2208 Rn\u00d7C is a matrix instead of a vector.):\ny\u0302 = \u03c3\u0303 (F (x)) = \u03c3\u0303(w\u22a4gT+1(x)) = \u03c3\u0303 ( w\u22a4 T\u2211\nt=0\nft(gt(x))\n)\n(2)\nwhere F (x) = w\u22a4gT+1(x) and \u03c3\u0303(\u00b7) denotes a map from classifier outputs (scores) to labels. For instance \u03c3\u0303(z) = sign(z) for binary classification (\u03c3\u0303(z) = argmax\ni zi\nfor multiclass classification). The parameters of a depthT ResNet are {w, {ft(\u00b7), \u2200t \u2208 T }}. A ResNet training involves training the classifier w and the weights of modules ft(\u00b7) \u2200t \u2208 [T ] when training examples (x1, y1), (x2, y2), . . . , (xm, ym) are available.\nBoosting Boosting (Freund & Schapire, 1995) assumes the availability of a weak learning algorithm which, given labeled training examples, produces a weak classifier (a.k.a. base classifier). The goal of boosting is to improve the performance of the weak learning algorithm. The key idea behind boosting is to choose training sets for the weak classifier in such a fashion as to force it to infer something new about the data each time it is called. The weak learning algorithm will finally combine many weak classifiers into a single strong classifier whose prediction power is strong.\nFrom empirical experience, ResNet remedies the problem of training error degradation (instability of solving nonconvex optimization problem using SGD) in deeper neural networks. We are curious about whether there is a\ntheoretical justification that identity loops help in training. More importantly, we are interested in proposing a new algorithm that avoids end-to-end back-propagation (e2eBP) through the deep network and thus is immune to the instability of SGD for non-convex optimization of deep neural networks."}, {"heading": "3. ResNet in Telescoping Sum Boosting Framework", "text": "As we recall from Equation (2), ResNet indeed has a similar form as the strong classifier in boosting. The key difference is that boosting is an ensemble of estimated hypotheses whereas ResNet is an ensemble of estimated feature representations \u2211T\nt=0 ft(gt(x)). To solve this problem, we introduce an auxiliary linear classifier wt on top of each residual block to construct a hypothesis module. Formally, a hypothesis module is defined as\not(x) def = w\u22a4t gt(x) \u2208 R (3)\nin the binary classification setting. Therefore ot+1(x) = w\u22a4t+1[ft(gt(x)) + gt(x)] as gt+1(x) = ft(gt(x)) + gt(x). We emphasize that given gt(x), we only need to train ft and wt+1 to train ot+1(x). In other words, we feed the output of previous residual block (gt(x)) to the current module and train the weights of current module ft(\u00b7) and the auxiliary classifier wt+1.\nNow the input, gt+1(x), of the t + 1-th residual block is the output, ft(gt(x))+gt(x), of the t-th residual block. As a result, ot(x) = \u2211t\u22121 t\u2032=0 w \u22a4 t ft\u2032(gt\u2032(x)). In other words, the auxiliary linear classifier is common for all modules underneath. It would not be realistic to assume a common auxiliary linear classifier, as such an assumption prevents us from training the T hypothesis module sequentially. We design a weak module classifier using the idea of telescoping sum as follows.\nDefinition 3.1. A weak module classifier is defined as\nht(x) def = \u03b1t+1ot+1(x) \u2212 \u03b1tot(x) (4)\nwhere ot(x) def = w\u22a4t gt(x) is a hypothesis module, and \u03b1t is a scalar. We call it a \u201ctelescoping sum boosting\u201d framework if the weak learners are restricted to the form of the weak module classifier.\nResNet: Ensemble of Weak Module Classifiers Recall that the T -th residual block of a ResNet outputs gT+1(x), which is fed to the top/final linear classifier for the final classification. We show that an ensemble of the weak module classifiers is equivalent to a ResNet\u2019s final output. We state it formally in Lemma 3.2. For purposes of exposition,\nwe will call F (x) the output of ResNet although a \u03c3\u0303 function is applied on top of F (x), mapping the output to the label space Y . Lemma 3.2. Let the input gt(x) of the t-th module be the output of the previous module, i.e., gt+1(x) = ft(gt(x)) + gt(x). Then the summation of T weak module classifiers divided by \u03b1T+1 is identical to the output, F (x), of the depth-T ResNet,\nF (x) = w\u22a4gT+1(x) \u2261 1\n\u03b1T+1\nT\u2211\nt=0\nht(x), (5)\nwhere the weak module classifier ht(x) is defined in Equation (4).\nSee Appendix B for the proof. Overall, our proposed ensemble of weak module classifiers is a new framework that allows for sequential training of ResNet. Note that traditional boosting algorithm results do not apply here. We now analyze our telescoping sum boosting framework in Section 4. Our analysis applies to both binary and multiclass, but we will focus on the binary class for simplicity in the main text and defer the multiclass analysis to the Appendix F."}, {"heading": "4. Telescoping Sum Boosting for Binary Classification", "text": "Below, we propose a learning algorithm whose training error decays exponentially with the number of weak module classifiers T under a weak learning condition. We restrict to bounded hypothesis modules, i.e., |ot(x)| \u2264 1."}, {"heading": "4.1. Weak Learning Condition", "text": "The weak module classifier involves the difference between (scaled version of) ot+1(x) and ot(x). Let \u03b3\u0303t def = Ei\u223cDt\u22121 [yiot(xi)] > 0 be the edge of the hypothesis module ot(x), where Dt\u22121 is the weight of the examples. As the hypothesis module ot(x) is bounded by 1, we obtain |\u03b3\u0303t| \u2264 1. So \u03b3\u0303t characterizes the performance of the hypothesis module ot(x). A natural requirement would be that ot+1(x) improves slightly upon ot(x), and thus \u03b3\u0303t+1 \u2212 \u03b3\u0303t \u2265 \u03b3\u2032 > 0 could serve as a weak learning condition. However this weak learning condition is too strong: even when current hypothesis module is performing almost ideally (\u03b3\u0303t is close to 1), we still seek a hypothesis module which performs consistently better than the previous one by \u03b3\u2032. Instead, we consider a much weaker learning condition, inspired by training error analysis, as follows.\nDefinition 4.1 (\u03b3-Weak Learning Condition). A weak module classifier ht(x) = \u03b1t+1ot+1 \u2212 \u03b1tot satisfies the \u03b3weak learning condition if \u03b3\u03032t+1\u2212\u03b3\u0303 2 t\n1\u2212\u03b3\u03032t \u2265 \u03b32 > 0 and the\nAlgorithm 1 BoostResNet: telescoping sum boosting for binary-class classification Input: m labeled samples [(xi, yi)]m where yi \u2208 {\u22121,+1} and a threshold \u03b3 Output: {ft(\u00b7), \u2200t} and wT+1 \u22b2 Discard wt+1, \u2200t 6= T\n1: Initialize t\u2190 0, \u03b3\u03030 \u2190 0, \u03b10 \u2190 0, o0(x)\u2190 0 2: Initialize sample weights at round 0: D0(i)\u2190 1/m, \u2200i \u2208 [m] 3: while \u03b3t > \u03b3 do 4: ft(\u00b7), \u03b1t+1,wt+1, ot+1(x)\u2190 Algorithm 2(gt(x), Dt, ot(x), \u03b1t)\n5: Compute \u03b3t \u2190 \u221a\n\u03b3\u03032t+1\u2212\u03b3\u03032t 1\u2212\u03b3\u03032t\n\u22b2 where \u03b3\u0303t+1 \u2190 Ei\u223cDt [yiot+1(xi)]\n6: Update Dt+1(i)\u2190 Dt(i) exp(\u2212yiht(xi))m\u2211 i=1 Dt(i) exp[\u2212yiht(xi)] \u22b2 where ht(x) = \u03b1t+1ot+1(x) \u2212 \u03b1tot(x) 7: t\u2190 t+ 1 8: end while 9: T \u2190 t\u2212 1\nAlgorithm 2 BoostResNet: oracle implementation for training a ResNet block\nInput: gt(x),Dt,ot(x) and \u03b1t Output: ft(\u00b7), \u03b1t+1, wt+1 and ot+1(x)\n1: (ft, \u03b1t+1,wt+1)\u2190 arg min (f,\u03b1,v) m\u2211 i=1 Dt(i) exp ( \u2212yi\u03b1v\u22a4 [f(gt(xi)) + gt(xi)] + yi\u03b1tot(xi) ) 2: ot+1(x)\u2190 w\u22a4t+1 [ft(gt(x)) + gt(x)]\ncovariance between exp(\u2212yot+1(x)) and exp(yot(x)) is non-positive.\nThe weak learning condition is motivated by the learning theory and it is met in practice (refer to Figure 4).\nInterpretation of weak learning condition For each weak\nmodule classifier ht(x), \u03b3t def = \u221a \u03b3\u03032t+1\u2212\u03b3\u03032t 1\u2212\u03b3\u03032t characterizes the normalized improvement of the correlation between the true labels y and the hypothesis modules ot+1(x) over the correlation between the true labels y and the hypothesis modules ot(x). The condition specified in Definition 4.1 is mild as it requires the hypothesis module ot+1(x) to perform only slightly better than the previous hypothesis module ot(x). In residual network, since ot+1(x) represents a depth-(t + 1) residual network which is a deeper counterpart of the depth-t residual network ot(x), it is natural to assume that the deeper residual network improves slightly upon the shallower residual network. When \u03b3\u0303t is close to 1, \u03b3\u03032t+1 only needs to be slightly better than \u03b3\u0303 2 t as the denominator 1 \u2212 \u03b3\u03032t is small. The assumption of the covariance between exp(\u2212yot+1(x)) and exp(yot(x)) being non-positive is suggesting that the weak module classifiers should not be adversarial, which may be a reasonable assumption for ResNet."}, {"heading": "4.2. BoostResNet", "text": "We now propose a novel training algorithm for telescoping sum boosting under binary-class classification as in Algo-\nrithm 1. In particular, we introduce a training procedure for deep ResNet in Algorithm 1 & 2, BoostResNet, which only requires sequential training of shallow ResNets.\nThe training algorithm is a module-by-module procedure following a bottom-up fashion as the outputs of the t-th module gt+1(x) are fed as the training examples to the next t + 1-th module. Each of the shallow ResNet ft(gt(x)) + gt(x) is combined with an auxiliary linear classifier wt+1 to form a hypothesis module ot+1(x). The weights of the ResNet are trained on these shallow ResNets. The telescoping sum construction is the key for successful interpretation of ResNet as ensembles of weak module classifiers. The innovative introduction of the auxiliary linear classifiers (wt+1) is the key solution for successful multi-channel representation boosting with theoretical guarantees. Auxiliary linear classifiers are only used to guide training, and they are not included in the model (proved in Lemma 3.2). This is the fundamental difference between BoostResNet and AdaNet. AdaNet (Cortes et al., 2016) maps the feature vectors (hidden layer representations) to a classifier space and boosts the weak classifiers. Our framework is a multi-channel representation (or information) boosting rather than a traditional classifier boosting. Traditional boosting theory does not apply in our setting.\nTheorem 4.2. [ Training error bound ] The training error of a T -module telescoping sum boosting framework using Algorithms 1 and 2 decays exponentially with the number\nof modules T ,\nPr i\u223cS\n( \u03c3\u0303 ( \u2211\nt\nht (xi) ) 6= yi ) \u2264 e\u2212 12T\u03b32\nif \u2200t \u2208 [T ] the weak module classifier ht(x) satisfies the \u03b3-weak learning condition defined in Definition 4.1.\nThe training error of Algorithms 1 and 2 is guaranteed to decay exponentially with the ResNet depth even when each hypothesis module ot+1(x) performs slightly better than its previous hypothesis module ot(x) (i.e., \u03b3 > 0). Refer to Appendix F for the algorithm and theoretical guarantees for multiclass classification."}, {"heading": "4.3. Oracle Implementation for ResNet", "text": "In Algorithm 2, the implementation of the oracle at line 1 is equivalent to\n(ft, \u03b1t+1,wt+1) =\narg min (f,\u03b1,v)\n1 m\nm \u2211\ni=1\nexp ( \u2212yi\u03b1v \u22a4 [f(gt(xi)) + gt(xi)] ) (6)\nThe minimization problem over f corresponds to finding the weights of the t-th nonlinear module of the residual network. Auxiliary classifier wt+1 is used to help solve this minimization problem with the guidance of training labels yi. However, the final neural network model includes none of the auxiliary classifiers, and still follows a standard ResNet structure (proved in Lemma 3.2). In practice, there are various ways to implement Equation (6). For instance, Janzamin et. al. (Janzamin et al., 2015) propose a tensor decomposition technique which decomposes a tensor formed by some transformation of the features x combined with labels y and recovers the weights of a one-hidden layer neural network with guarantees. One can also use backpropagation as numerous works have shown that gradient based training are relatively stable on shallow networks with identity loops (Hardt & Ma, 2016; He et al., 2016).\nComputational & Memory Efficiency BoostResNet training is memory efficient as the training process only requires parameters of two consecutive residual blocks to be in memory. Given that the limited GPU memory being one of the main bottlenecks for computational efficiency, BoostResNet requires significantly less training time than e2eBP in deep networks as a result of reduced communication overhead and the speed-up in shallow gradient forwarding and back-propagation. Let M1 be the memory required for one module, and M2 be the memory required for one linear classifier, the memory consumption is M1 +M2 by BoostResNet and M1T +M2 by e2eBP. Let the flops needed for gradient update over one module and one linear classifier be C1 and C2 respectively, the computation cost is C1+C2 by BoostResNet and C1T + C2 by e2eBP."}, {"heading": "4.4. Generalization Error Analysis", "text": "In this section, we analyze the generalization error to understand the possibility of overfitting under Algorithm 1. The strong classifier or the ResNet is F (x) = \u2211 t ht(x)\n\u03b1T+1 . Now\nwe define the margin for example (x, y) as yF (x). For simplicity, we consider MLP-ResNet with n multiple channels and assume that the weight vector connecting a neuron at layer t with its preceding layer neurons is l1 norm bounded by \u039bt,t\u22121. Recall that there exists a linear classifier w on top, and we restrict to l1 norm bounded classifiers, i.e., \u2016w\u20161 \u2264 C0 <\u221e. The expected training examples are l\u221e norm bounded r\u221e def = ES\u223cD [ maxi\u2208[m]\u2016xi\u2016\u221e ] < \u221e. We introduce Corollary 4.3 which follows directly from Lemma 2 of (Cortes et al., 2016). Corollary 4.3. (Cortes et al., 2016) Let D be a distribution over X \u00d7 Y and S be a sample of m examples chosen independently at random according to D. With probability at least 1\u2212\u03b4, for \u03b8 > 0, the strong classifier F (x) (ResNet) satisfies that\nPr D (yF (x) \u2264 0) \u2264 Pr S (yF (x) \u2264 \u03b8)+\n4C0r\u221e \u03b8\n\u221a\nlog(2n)\n2m\nT \u2211\nt=0\n\u039bt + 2\n\u03b8\n\u221a\nlog T\nm + \u03b2(\u03b8,m, T, \u03b4) (7)\nwhere \u039bt def = \u220ft t\u2032=0 2\u039bt\u2032,t\u2032\u22121 and \u03b2(\u03b8,m, T, \u03b4)\ndef =\u221a\u2308\n4 \u03b82\nlog (\n\u03b82m log T )\u2309 log T m + log 2 \u03b4 2m .\nFrom Corollary 4.3, we obtain a generalization error bound in terms of margin bound PrS (yF (x) \u2264 \u03b8) and network complexity 4C0r\u221e\n\u03b8\n\u221a log(2n)\n2m \u2211T t=0 \u039bt + 2 \u03b8 \u221a log T m +\n\u03b2(\u03b8,m, T, \u03b4). Larger margin bound (larger \u03b8) contributes positively to generalization accuracy, and l1 norm bounded weights (smaller \u2211T t=0 \u039bt ) are beneficial to control network complexity and to avoid overfitting. The dominant term in the network complexity is\n4C0r\u221e \u03b8\n\u221a log(2n)\n2m \u2211T t=0 \u039bt which scales as least linearly\nwith the depth T . See Appendix D for the proof.\nThis corollary suggests that stronger weak module classifiers which produce higher accuracy predictions and larger edges, will yield larger margins and suffer less from overfitting. The larger the value of \u03b8, the smaller the term 4C0r\u221e\n\u03b8\n\u221a log(2n)\n2m \u2211T t=0 \u039bt + 2 \u03b8 \u221a log T m + \u03b2(\u03b8,m, T, \u03b4) is.\nWith larger edges on the training set and when \u03b3\u0303T+1 < 1, we are able to choose larger values of \u03b8 while keeping the error term zero or close to zero."}, {"heading": "5. Experiments", "text": "We compare our proposed BoostResNet algorithm with e2eBP training a ResNet on the MNIST (LeCun et al.,\n0 5 10 15 20 25 30 0\n0.2\n0.4\n0.6\n0.8 1 T ra in in g A cc u ra cy PSfrag replacements Training Acc\nNumber of Residual Blocks\nBoostResNet e2eBP\n(a) depth vs training accuracy\n0 5 10 15 20 25 30 0\n0.2\n0.4\n0.6\n0.8\n1\nT e\nA cc\nu ra\ncy\nPSfrag replacements\nTest Acc\nBoostResNet\nNumber of Residual Blocks\ne2eBP\n(b) depth vs test accuracy\nFigure 2: Comparison of BoostResNet (ours, blue) and e2eBP (baseline, red) on multilayer perceptron residual network on MNIST dataset.\n10 7\n10 8\n10 9\nNumber of Gradient Updates\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nA cc\nu ra\ncy\nBoostResNet Training BoostResNet Test e2eBP Training e2eBP Test PSfrag replacements\nBoostResNet Training\nBoostResNet Test\ne2eBP Training (a) SVHN\n10 7\n10 8\n10 9\n10 10\nNumber of Gradient Updates\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nA cc\nu ra\ncy\nBoostResNet Training BoostResNet Test e2eBP Training e2eBP Test\n(b) CIFAR-10\nFigure 3: Convergence performance comparison between e2eBP and BoostResNet on convolutional neural network residual network on the SVHN and CIFAR-10 dataset. The vertical dotted line shows when BoostResNet training stopped, and we began refining the network with standard e2eBP training.\n0 10 20 30 40 50 0\n0.1\n0.2\nPSfrag replacements \u03b3t\ndepth T (a) \u03b3t\n0 10 20 30 40 50 0.7\n0.8\n0.9\nPSfrag replacements\u03b3\u0303t\ndepth T (b) \u03b3\u0303t\nFigure 4: Visualization of edge \u03b3t and edge for each residual block \u03b3\u0303t. The x-axis represents depth, and the y-axis represents \u03b3t or \u03b3\u0303t values. The plots are for a convolutional network composed of 50 residual blocks and trained on the SVHN dataset.\n1998), street view house numbers (SVHN) (Netzer et al., 2011), and CIFAR-10 (Krizhevsky & Hinton, 2009) benchmark datasets. Two different types of architectures are tested: a ResNet where each module is a fully-connected multi-layer perceptron (MLP-ResNet) and a more common, convolutional neural network residual network (CNNResNet). In each experiment the architecture of both algorithms is identical, and they are both initialized with the same random seed. As a baseline, we also experiment with standard boosting (AdaBoost.MM (Mukherjee & Schapire, 2013)) of convolutional modules for SVHN and CIFAR-10 datasets. Our experiments are programmed in the Torch deep learning framework for Lua and executed on NVIDIA Tesla P100 GPUs. All models are trained using the Adam variant of SGD (Kingma & Ba, 2014).\nHyperparameters are selected via random search for high-\nest accuracy on a validation set. They are specified in Appendix H. In BoostResNet, the most important hyperparameters, according to our experiments, are those that govern when the algorithm stops training the current module and begins training its successor.\nMLP-ResNet on MNISTThe MNIST database (LeCun et al., 1998) of handwritten digits has a training set of 60,000 examples, and a test set of 10,000 examples. The data contains ten classes. We test the performance of BoostResNet on MLP-ResNet using MNIST dataset, and compare it with e2eBP baseline. Each residual block is composed of an MLP with a single, 1024-dimensional hidden layer. The training and test error between BoostResNet and e2eBP is in Figure 2 as a function of depth. Surprisingly, we find that training error degrades for e2eBP, although the ResNet\u2019s identity\nloop is supposed to alleviate this problem. Our proposed sequential training procedure, BoostResNet, relieves gradient instability issues, and continues to perform well as depth increases.\nCNN-ResNet on SVHN SVHN (Netzer et al., 2011) is a real-world image dataset, obtained from house numbers in Google Street View images. The dataset contains over 600,000 training images, and about 20,000 test images. We fit a 50-layer, 25-residual-block CNN-ResNet using both BoostResNet and e2eBP (figure 3a). Each residual block is composed of a CNN using 15 3 \u00d7 3 filters. We refine the result of BoostResNet by initializing the weights using the result of BoostResNet and run end-to-end back propagation (e2eBP). From figure 3a, our BoostResNet converges much faster (requires much fewer gradient updates) than e2eBP. The test accuracy of BoostResNet is comparable with e2eBP.\nCNN-ResNet on CIFAR-10 The CIFAR-10 dataset is a benchmark dataset composed of 10 classes of small images, such as animals and vehicles. It consists of 50,000 training images and 10,000 test images. We again fit a 50-layer, 25- residual-block CNN-ResNet using both BoostResNet and e2eBP (figure 3b). BoostResNet training converges to the optimal solution faster than e2eBP. Unlike in the previous two datasets, the efficiency of BoostResNet comes at a cost when training with CIFAR-10. We find that the test accuracy of the e2eBP refined BoostResNet to be slightly lower than that produced by e2eBP.\nWeak Learning Condition Check The weak learning condition (Definition 4.1) inspired by learning theory is checked in Figure 4. The required better than random guessing edge \u03b3t is depicted in Figure 4a, it is always greater than 0 and our weak learning condition is thus nonvacuous. In Figure 4b, the representations we learned using BoostResNet is increasingly better (for this classification task) as the depth increases.\nComparison of BoostResNet, e2eBP and AdaBoost Besides e2eBP, we also experiment with standard boosting (AdaBoost.MM (Mukherjee & Schapire, 2013)), as another baseline, of convolutional modules. In this experiment, each weak learner is a residual block of the ResNet,\npaired with a classification layer. We do 25 rounds of AdaBoost.MM and train each weak learner to convergence. Table 1 and table 2 exhibit a comparison of BoostResNet, e2eBP and AdaBoost performance on SVHN and CIFAR10 dataset respectively.\nOn SVHN dataset, the advantage of BoostResNet over e2eBP is obvious. Using 3 \u00d7 108 number of gradient updates, BoostResNet achieves 93.8% test accuracy whereas e2eBP obtains a test accuracy of 83%. The training and test accuracies of SVHN are listed in Table 1. BoostResNet training allows the model to train much faster than end-to-end training, and still achieves the same test accuracy when refined with e2eBP. To list the hyperparameters we use in our BoostResNet training after searching over candidate hyperparamters, we optimize learning rate to be 0.004 with a 9 \u00d7 10\u22125 learning rate decay. The gamma threshold is optimized to be 0.001 and the initial gamma value on SVHN is 0.75. On CIFAR-10 dataset, the main advantage of BoostResNet over e2eBP is the speed of training. BoostResNet refined with e2eBP obtains comparable results with e2eBP. This is because we are using a suboptimal architecture of ResNet which overfits the CIFAR-10 dataset. AdaBoost, on the other hand, is known to be resistant to overfitting. In BoostResNet training, we optimize learning rate to be 0.014 with a 3.46 \u00d7 10\u22125 learning rate decay. The gamma threshold is optimized to be 0.007 and the initial gamma value on CIFAR-10 is 0.93. We find that a standard ResNet, to its credit, is quite robust to hyperparameters, namely learning rate and learning rate decay, provided that we use an optimization procedure that automatically modulates these values."}, {"heading": "6. Conclusions and Future Works", "text": "Our proposed BoostResNet algorithm achieves exponentially decaying (with the depth T ) training error under the weak learning condition. BoostResNet is much more computationally efficient compared to end-to-end backpropagation in deep ResNet. More importantly, the memory required by BoostResNet is trivial compared to end-toend back-propagation. It is particularly beneficial given the limited GPU memory and large network depth. Our learn-\ning framework is natural for non-differentiable data. For instance, our learning framework is amenable to take weak learning oracles using tensor decomposition techniques. Tensor decomposition, a spectral learning framework with theoretical guarantees, is applied to learning one layer MLP in (Janzamin et al., 2015). We plan to extend our learning framework to non-differentiable data using general weak learning oracles."}, {"heading": "A. Related Works", "text": "A.1. Loss function and architecture selection\nIn neural network optimization, there are many commonly-used loss functions and criteria, e.g., mean squared error, negative log likelihood, margin criterion, etc. There are extensive works (Girshick, 2015; Rubinstein & Kroese, 2013; Tygert et al., 2015) on selecting or modifying loss functions to prevent empirical difficulties such as exploding/vanishing gradients or slow learning (Balduzzi et al., 2017). However, there are no rigorous principles for selecting a loss function in general. Other works consider variations of the multilayer perceptron (MLP) or convolutional neural network (CNN) by adding identity skip connections (He et al., 2016), allowing information to bypass particular layers. However, no theoretical guarantees on the training error are provided despite breakthrough empirical successes. Hardt et al. (Hardt & Ma, 2016) have shown the advantage of identity loops in linear neural networks with theoretical justifications; however the linear setting is unrealistic in practice.\nA.2. Learning algorithm design\nThere have been extensive works on improving BP (LeCun et al., 1989). For instance, momentum (Qian, 1999), Nesterov accelerated gradient (Nesterov, 1983), Adagrad (Duchi et al., 2011) and its extension Adadelta (Zeiler, 2012). Most recently, Adaptive Moment Estimation (Adam) (Kingma & Ba, 2014), a combination of momentum and Adagrad, has received substantial success in practice. All these methods are modifications of stochastic gradient descent (SGD), but our method only requires an arbitrary oracle, which does not necessarily need to be an SGD solver, that solves a relatively simple shallow neural network."}, {"heading": "B. Proof for Lemma 3.2: the strong learner is a ResNet", "text": "Proof. In our algorithm, the input of the next module is the output of the current module\ngt+1(x) = ft(gt(x)) + gt(x), (8)\nwe thus obtain that each weak learning module is\nht(x) = \u03b1t+1w \u22a4 t+1(ft(gt(x)) + gt(x))\u2212 \u03b1tw\u22a4t gt(x) (9)\n= \u03b1t+1w \u22a4 t+1gt+1(x)\u2212 \u03b1tw\u22a4t gt(x), (10)\nand similarly\nht+1 = \u03b1t+2w \u22a4 t+2gt+2(x)\u2212 \u03b1t+1w\u22a4t+1gt+1(x). (11)\nTherefore the sum over ht(x) and ht+1(x) is\nht(x) + ht+1(x) = \u03b1t+2w \u22a4 t+2gt+2(x) \u2212 \u03b1tw\u22a4t gt(x) (12)\nAnd we further see that the weighted summation over all ht(x) is a telescoping sum (note that g0(x) = 0):\nT\u2211\nt=0\nht(x) = \u03b1T+1w \u22a4 T+1gT+1(x) \u2212 \u03b10w\u22a40 g0(x) = \u03b1T+1w\u22a4T+1gT+1(x). (13)"}, {"heading": "C. Proof for Theorem 4.2: binary class telescoping sum boosting theory", "text": "Proof. We will use a 0-1 loss to measure the training error. In our analysis, the 0-1 loss is bounded by exponential loss.\nThe training error is therefore bounded by\nPr i\u223cD1\n(p(\u03b1T+1w \u22a4 T+1gT+1(xi)) 6= yi) (14)\n=\nm\u2211\ni=1\nD1(i)1{\u03c3\u0303(\u03b1T+1w\u22a4T+1gT+1(xi)) 6= yi} (15)\n=\nm\u2211\ni=1\nD1(i)1\n{ \u03c3\u0303 ( T\u2211\nt=0\nht(xi) ) 6= yi } (16)\n\u2264 m\u2211\ni=1\nD1(i) exp { \u2212yi T\u2211\nt=0\nht(xi)\n} (17)\n=\nm\u2211\ni=1\nDT+1(i)\nT\u220f\nt=0\nZt (18)\n=\nT\u220f\nt=0\nZt (19)\nwhere Zt = m\u2211 i=1 Dt(i) exp (\u2212yiht(xi)).\nWe choose \u03b1t+1 to minimize Zt.\n\u2202Zt \u2202\u03b1t+1\n= \u2212 m\u2211\ni=1\nDt(i)yiot+1 exp (\u2212yiht(xi)) (20)\n= \u2212Zt m\u2211\ni=1\nDt+1(i)yiot+1(i) = 0 (21)\nFurthermore each learning module is bounded as we see in the following analysis. We obtain\nZt =\nm\u2211\ni=1\nDt(i)e \u2212yiht(xi) (22)\n=\nm\u2211\ni=1\nDt(i)e \u2212\u03b1t+1yiot+1(xi)+\u03b1tyiot(xi) (23)\n\u2264 m\u2211\ni=1\nDt(i)e \u2212\u03b1t+1yiot+1(xi)\nm\u2211\ni=1\nDt(i)e \u03b1tyiot(xi) (24)\n= m\u2211\ni=1\nDt(i)e \u2212\u03b1t+1\n1+yiot+1(xi) 2 +\u03b1t+1 1\u2212yiot+1(xi) 2\nm\u2211\ni=1\nDt(i)e \u03b1t\n1+yiot(xi) 2 \u2212\u03b1t 1\u2212yiot(xi)\n2 (25)\n\u2264 m\u2211\ni=1\nDt(i)\n( 1 + yiot+1(xi)\n2 e\u2212\u03b1t+1 + 1\u2212 yiot+1(xi) 2\ne\u03b1t+1 ) \u00b7 m\u2211\ni=1\nDt(i)\n( 1 + yiot(xi)\n2 e\u03b1t + 1\u2212 yiot(xi) 2\ne\u2212\u03b1t )\n(26)\n=\nm\u2211\ni=1\nDt(i)\n( 1 + yiot+1(xi)\n2 e\u2212\u03b1t+1 + 1\u2212 yiot+1(xi) 2\ne\u03b1t+1 ) e\u03b1t + e\u2212\u03b1t\n2 (27)\n=\nm\u2211\ni=1\nDt(i)\n( e\u2212\u03b1t+1 + e\u03b1t+1\n2 + e\u2212\u03b1t+1 \u2212 e\u03b1t+1 2 yiot+1(xi)\n) e\u03b1t + e\u2212\u03b1t\n2 (28)\n=\n( e\u2212\u03b1t+1 + e\u03b1t+1\n2 + e\u2212\u03b1t+1 \u2212 e\u03b1t+1 2 \u03b3\u0303t\n) e\u03b1t + e\u2212\u03b1t\n2 (29)\nEquation (24) is due to the non-positive correlation between exp(\u2212yot+1(x)) and exp(yot(x)). Jensen\u2019s inequality in Equation (26) holds only when |yiot+1(xi)| \u2264 1 which is satisfied by the definition of the weak learning module.\nThe algorithm chooses \u03b1t+1 to minimize Zt. We achieve an upper bound on Zt,\n\u221a 1\u2212\u03b3\u03032t\n1\u2212\u03b3\u03032t\u22121 by minimizing the bound in\nEquation (29)\nZt|\u03b1t+1=argminZt \u2264 Zt|\u03b1t+1= 12 ln( 1+\u03b3\u0303t1\u2212\u03b3\u0303t ) (30)\n\u2264 ( e\u2212\u03b1t+1 + e\u03b1t+1\n2 + e\u2212\u03b1t+1 \u2212 e\u03b1t+1 2 \u03b3\u0303t\n) e\u03b1t + e\u2212\u03b1t\n2\n\u2223\u2223\u2223\u2223 \u03b1t+1= 1 2 ln( 1+\u03b3\u0303t 1\u2212\u03b3\u0303t ) (31)\n= \u221a 1\u2212 \u03b3\u03032t 1\u2212 \u03b3\u03032t\u22121 = \u221a 1\u2212 \u03b32t (32)\nTherefore over the T modules, the training error is upper bounded as follows\nPr i\u223cD\n(p(\u03b1T+1w \u22a4 T+1gT+1(xi))) 6= yi) \u2264\nT\u220f\nt=0\n\u221a 1\u2212 \u03b32t \u2264 T\u220f\nt=0\n\u221a 1\u2212 \u03b32 = exp ( \u22121 2 T\u03b32 ) (33)\nOverall, Algorithm 1 leads us to consistent learning of ResNet."}, {"heading": "D. Proof for Corollary 4.3: Generalization Bound", "text": "Rademacher complexity technique is powerful for measuring the complexity of H any family of functions h : X \u2192 R, based on easiness of fitting any dataset using classifiers inH (whereX is any space). Let S =< x1, . . . , xm > be a sample of m points in X . The empirical Rademacher complexity ofH with respect to S is defined to be\nRS(H) def= E\u03c3 [ sup h\u2208H 1 m m\u2211\ni=1\n\u03c3ih(xi)\n] (34)\nwhere \u03c3 is the Rademacher variable. The Rademacher complexity on m data points drawn from distribution D is defined by Rm(H) = ES\u223cD [RS(H)] . (35) Proposition D.1. (Theorem 1 (Cortes et al., 2014)) Let H be a hypothesis set admitting a decomposition H = \u222ali=1Hi for some l > 1. Hi are distinct hypothesis sets. Let S be a random sequence of m points chosen independently from X according to some distribution D. For \u03b8 > 0 and any H =\u2211Tt=0 ht, with probability at least 1\u2212 \u03b4,\nPr D (yH(x) \u2264 0) \u2264 Pr S (yH(x) \u2264 \u03b8) + 4 \u03b8\nT\u2211\nt=0\nRm(Hkt) + 2\n\u03b8\n\u221a log l\nm\n+\n\u221a\n\u2308 4 \u03b82 log\n( \u03b82m\nlog l\n) \u2309 log l\nm +\nlog 2 \u03b4\n2m (36)\nfor all ht \u2208 Hkt . Lemma D.2. Let h\u0303 = w\u0303\u22a4f\u0303 , where w\u0303 \u2208 Rn, f\u0303 \u2208 Rn. Let H\u0303 and F\u0303 be two hypothesis sets, and h\u0303 \u2208 H\u0303 , f\u0303j \u2208 F\u0303 , \u2200j \u2208 [n]. The Rademacher complexity of H\u0303 and F\u0303 with respect to m points from D are related as follows\nRm(H\u0303) = \u2016w\u0303\u20161Rm(F\u0303). (37)\nD.1. ResNet Module Hypothesis Space\nLet n be the number of channels in ResNet, i.e., the number of input or output neurons in a module ft(gt(x)). We have proved that ResNet is equivalent as\nF (x) = w\u22a4 T\u2211\nt=0\nf(gt(x)) (38)\nWe define the family of functions that each neuron ft,j , \u2200j \u2208 [n] belong to as\nFt = {x\u2192 ut\u22121,j(\u03c3 \u25e6 ft\u22121)(x) : ut\u22121,j \u2208 Rn, \u2016ut\u22121,j\u20161 \u2264 \u039bt,t\u22121, ft\u22121,i \u2208 Ft\u22121} (39)\nwhere ut\u22121,j denotes the vector of weights for connections from unit j to a lower layer t\u22121, \u03c3\u25e6 ft\u22121 denotes element-wise nonlinear transformation on ft\u22121. The output layer of each module is connected to the output layer of previous module. We consider 1-layer modules for convenience of analysis.\nTherefore in ResNet with probability at least 1\u2212 \u03b4,\nPr D (yF (x) \u2264 0) \u2264 Pr S (yF (x) \u2264 \u03b8) + 4 \u03b8\nT\u2211\nt=0\n\u2016w\u20161Rm(Ft) + 2\n\u03b8\n\u221a logT\nm\n+\n\u221a\n\u2308 4 \u03b82 log\n( \u03b82m\nlogT\n) \u2309 logT\nm +\nlog 2 \u03b4\n2m (40)\nfor all ft \u2208 Ft.\nDefine the maximum infinity norm over samples as r\u221e def = ES\u223cD [ maxi\u2208[m]\u2016xi\u2016\u221e ] and the product of l1 norm bound on weights as \u039bt def = \u220ft\nt\u2032=0 2\u039bt\u2032,t\u2032\u22121. According to lemma 2 of (Cortes et al., 2016), the empirical Rademacher complexity is bounded as a function of r\u221e, \u039bt and n:\nRm(Ft) \u2264 r\u221e\u039bt \u221a log(2n)\n2m (41)\nOverall, with probability at least 1\u2212 \u03b4,\nPr D (yF (x) \u2264 0) \u2264 Pr S (yF (x) \u2264 \u03b8) +\n4\u2016w\u20161r\u221e \u221a log(2n) 2m\n\u03b8\nT\u2211\nt=0\n\u039bt\n+ 2\n\u03b8\n\u221a logT\nm +\n\u221a\n\u2308 4 \u03b82 log\n( \u03b82m\nlogT\n) \u2309 logT\nm +\nlog 2 \u03b4\n2m (42)\nfor all ft \u2208 Ft."}, {"heading": "E. Proof for Theorem E: Margin and Generalization Bound", "text": "Theorem E.1. [ Generalization error bound ] Given algorithm 1, the fraction of training examples with margin at most \u03b8 is at most (1 + 21\u221a \u03b3\u0303T+1 \u22121 ) \u03b8 2 exp(\u2212 12\u03b32T ). And the generalization error PrD(yF (x) \u2264 0) satisfies\nPr D (yF (x) \u2264 0) \u2264 (1 + 21\n\u03b3\u0303T+1 \u2212 1)\n\u03b8 2 exp(\u22121\n2 \u03b32T )\n+ 4C0r\u221e\n\u03b8\n\u221a log(2n)\n2m\nT\u2211\nt=0\n\u039bt + 2\n\u03b8\n\u221a logT\nm + \u03b2(\u03b8,m, T, \u03b4) (43)\nwith probability at least 1\u2212 \u03b4 for \u03b2(\u03b8,m, T, \u03b4) def= \u221a\u2308\n4 \u03b82\nlog (\n\u03b82m log T )\u2309 log T m + log 2 \u03b4 2m .\nNow the proof for Theorem E is the following.\nProof. The fraction of examples in sample set S being smaller than \u03b8 is bounded\nPr S (yF (x) \u2264 \u03b8) \u2264 1 m\nm\u2211\ni=1\n1{yiF (xi) \u2264 \u03b8} (44)\n= 1\nm\nm\u2211\ni=1\n1{yi T\u2211\nt=0\nht(xi) \u2264 \u03b8\u03b1T+1} (45)\n\u2264 1 m\nm\u2211\ni=1\nexp(\u2212yi T\u2211\nt=0\nht(xi) + \u03b8\u03b1T+1) (46)\n= exp(\u03b8\u03b1T+1) 1\nm\nm\u2211\ni=1\nexp(\u2212yi T\u2211\nt=0\nht(xi)) (47)\n= exp(\u03b8\u03b1T+1)\nT\u220f\nt=0\nZt (48)\nTo bound exp(\u03b8\u03b1T+1) = \u221a (1+\u03b3\u0303T+11\u2212\u03b3\u0303T+1 ) \u03b8 , we first bound \u03b3\u0303T+1: We know that \u220fT\nt\u2032=t+1(1\u2212 \u03b32t\u2032)\u03b32t \u2264 (1\u2212 \u03b32)T\u2212t\u03b32 for all \u2200\u03b3t \u2265 \u03b32 + \u01eb if \u03b32 \u2265 1\u2212\u01eb2 . Therefore \u2200 \u03b3t \u2265 \u03b32 + \u01eb and \u03b32 \u2265 1\u2212\u01eb2\n\u03b3\u03032T+1 = (1\u2212 \u03b32T )\u03b3\u03032T + \u03b32T (49)\n=\nT\u2211\nt=1\nT\u220f\nt\u2032=t+1\n(1 \u2212 \u03b32t\u2032)\u03b32t + T\u220f\nt=1\n(1\u2212 \u03b32t )\u03b3\u030321 (50)\n\u2264 T\u2211\nt=1\n(1\u2212 \u03b32)T\u2212t\u03b32 + (1\u2212 \u03b32)T \u03b3\u030321 (51)\n=\nT\u22121\u2211\nt=0\n(1\u2212 \u03b32)t\u03b32 + (1\u2212 \u03b32)T \u03b3\u030321 (52)\n= 1\u2212 (1\u2212 \u03b32)T + (1\u2212 \u03b32)T \u03b3\u030321 (53) = 1\u2212 (1\u2212 \u03b3\u030321)(1\u2212 \u03b32)T (54)\nTherefore\nPr S (yF (x) \u2264 \u03b8) \u2264 exp(\u03b8\u03b1T+1)\nT\u220f\nt=1\nZt (55)\n= ( 1 + \u03b3\u0303T+1 1\u2212 \u03b3\u0303T+1 ) \u03b8 2\nT\u220f\nt=1\nZt (56)\n= ( 1 + \u03b3\u0303T+1 1\u2212 \u03b3\u0303T+1 ) \u03b8 2\nT\u220f\nt=1\n\u221a 1\u2212 \u03b32t (57)\n= (1 + 2\n1 \u03b3\u0303T+1\n\u2212 1) \u03b8 2 exp(\u22121 2 \u03b32T ) (58)\n\u2264 (1 + 21\u221a 1\u2212(1\u2212\u03b3\u030321)(1\u2212\u03b32)T \u2212 1) \u03b8 2 exp(\u22121 2 \u03b32T ) (59)\nAs T \u2192\u221e, PrS(yF (x) \u2264 \u03b8) \u2264 0 as exp(\u2212 12\u03b32T ) decays faster than (1 + 21\u221a 1\u2212(1\u2212\u03b3\u03032\n1 )(1\u2212\u03b32)T\n\u22121 ) \u03b8 2 ."}, {"heading": "F. Telescoping Sum Boosting for Multi-calss Classification", "text": "Recall that the weak module classifier is defined as\nht(x) = \u03b1t+1ot+1(x) \u2212 \u03b1tot(x) \u2208 RC , (60)\nwhere ot(x) \u2208 \u2206C\u22121. The weak learning condition for multi-class classification is different from the binary classification stated in the previous section, although minimal demands placed on the weak module classifier require prediction better than random on any distribution over the training set intuitively.\nWe now define the weak learning condition. It is again inspired by the slightly better than random idea, but requires a more sophisticated analysis in the multi-class setting.\nF.1. Cost Matrix\nIn order to characterize the training error, we introduce the cost matrix C \u2208 Rm\u00d7C where each row denote the cost incurred by classifying that example into one of the C categories. We will bound the training error using exponential loss, and under the exponential loss function defined as in Definition G.1, the optimal cost function used for best possible training error is therefore determined.\nLemma F.1. The optimal cost function under the exponential loss is\nCt(i, l) = { exp (st(xi, l)\u2212 st(xi, yi)) if l 6= yi \u2212 \u2211\nl\u2032 6=yi exp (st(xi, l\n\u2032)\u2212 st(xi, yi)) if l = yi (61)\nwhere st(x) = t\u2211\n\u03c4=1 h\u03c4 (x).\nF.2. Weak Learning Condition\nDefinition F.2. Let \u03b3\u0303t+1 = \u2212\nm\u2211\ni=1\n<Ct(i,:),ot+1(xi)>\nm\u2211\ni=1\n\u2211\nl 6=yi\nCt(i,l) and \u03b3\u0303t =\n\u2212 m\u2211\ni=1\n<Ct\u22121(i,:),ot(xi)>\nm\u2211\ni=1\n\u2211\nl 6=yi\nCt\u22121(i,l) . A multi-class weak module classifier\nht(x) = \u03b1t+1ot+1(x) \u2212 \u03b1tot(x) satisfies the \u03b3-weak learning condition if \u03b3\u0303 2 t+1\u2212\u03b3\u0303 2 t\n1\u2212\u03b3\u03032t \u2265 \u03b32 > 0, and Cov(< Ct(i, :\n), ot+1(xi) >,< Ct(i, :), ot+1(xi) >) \u2265 0.\nWe propose a novel learning algorithm using the optimal edge-over-random cost function for training ResNet under multiclass classification task as in Algorithm 3.\nTheorem F.3. The training error of a T -module ResNet using Algorithm 3and 4 decays exponentially with the depth of the ResNet T ,\nC \u2212 1 m\nm\u2211\ni=1\nLexp\u03b7 (sT (xi)) \u2264 (C \u2212 1)e\u2212 1 2T\u03b3 2\n(62)\nif the weak module classifier ht(x) satisfies the \u03b3-weak learning condition \u2200t \u2208 [T ].\nThe exponential loss function defined as in Definition G.1\nAlgorithm 3 BoostResNet: telescoping sum boosting for multi-class classification Input: Given (x1, y1), . . . (xm, ym) where yi \u2208 Y = {1, . . . , C} and a threshold \u03b3 Output: {ft(\u00b7),\u2200t} and WT+1 \u22b2 Discard wt+1, \u2200t 6= T\n1: Initialize t\u2190 0, \u03b3\u03030 \u2190 1, \u03b10 \u2190 0, o0 \u2190 0 \u2208 RC , s0(xi, l) = 0, \u2200i \u2208 [m], l \u2208 Y 2: Initialize cost function C0(i, l)\u2190 { 1 if l 6= yi 1\u2212 C if l = yi 3: while \u03b3t > \u03b3 do 4: ft(\u00b7), \u03b1t+1,Wt+1, ot+1(x)\u2190 Algorithm 4(gt(x),Ct, ot(x), \u03b1t) 5: Compute \u03b3t \u2190 \u221a\n\u03b3\u03032t+1\u2212\u03b3\u03032t 1\u2212\u03b3\u03032t\n\u22b2 where \u03b3\u0303t+1 \u2190 \u2212\nm\u2211\ni=1 Ct(i,:)\u00b7ot+1(xi) m\u2211\ni=1\n\u2211\nl 6=yi\nCt(i,l)\n6: Update st+1(xi, l)\u2190 st(xi, l) + ht(xi, l) \u22b2 where ht(xi, l) = \u03b1t+1ot+1(xi, l)\u2212 \u03b1tot(xi, l) 7: Update cost function Ct+1(i, l)\u2190 { est+1(xi,l)\u2212st+1(xi,yi) if l 6= yi \u2212 \u2211\nl\u2032 6=yi est+1(xi,l \u2032)\u2212st+1(xi,yi) if l = yi\n8: t\u2190 t+ 1 9: end while\n10: T \u2190 t\u2212 1\nAlgorithm 4 BoostResNet: oracle implementation for training a ResNet module (multi-class)\nInput: gt(x),st,ot(x) and \u03b1t Output: ft(\u00b7), \u03b1t+1, Wt+1 and ot+1(x)\n1: (ft, \u03b1t+1,Wt+1)\u2190 arg min (f,\u03b1,V ) m\u2211 i=1 \u2211 l 6=yi e\u03b1V \u22a4[f(gt(xi),l)\u2212f(gt(xi),yi)+gt(xi,l)\u2212gt(xi,yi)] 2: ot+1(x)\u2190W\u22a4t+1 [ft(gt(x)) + gt(x)]\nF.3. Oracle Implementation\nWe implement an oracle to minimize Zt def = m\u2211 i=1 \u2211 l 6=yi est(xi,l)\u2212st(xi,yi)eht(xi,l)\u2212ht(xi,yi) given current state st and hypothesis module ot(x). Therefore minimizing Zt is equivalent to the following.\nmin (f,\u03b1,V )\nm\u2211\ni=1\n\u2211\nl 6=yi\nest(xi,l)\u2212st(xi,yi)e\u2212\u03b1t(ot(xi,l)\u2212ot(xi,yi))e\u03b1V \u22a4[f(gt(xi),l)\u2212f(gt(xi),yi)+gt(xi,l)\u2212gt(xi,yi)] (63)\n\u2261 min (f,\u03b1,V )\nm\u2211\ni=1\n\u2211\nl 6=yi\ne\u03b1V \u22a4[f(gt(xi),l)\u2212f(gt(xi),yi)+gt(xi,l)\u2212gt(xi,yi)] (64)\n\u2261 min \u03b1,f,v\nm\u2211\ni=1\ne\u2212\u03b1v \u22a4[f(xi,yi)+gt(xi,yi)]\n\u2211\nl 6=yi\ne\u03b1v \u22a4[f(xi,l)+gt(xi,l)] (65)"}, {"heading": "G. Proof for Theorem F.3 multiclass boosting theory", "text": "Proof. To characterize the training error, we use the exponential loss function\nDefinition G.1. Define loss function for a multiclass hypothesis H(xi) on a sample (xi, yi) as\nLexp\u03b7 (H(xi), yi) = \u2211\nl 6=yi\nexp ((H(xi, l)\u2212H(xi, yi))) . (66)\nDefine the accumulated weak learner st(xi, l) = t\u2211\nt\u2032=0 ht\u2032(xi, l) and the loss Zt = m\u2211 i=1 \u2211 l 6=yi exp(st(xi, l) \u2212\nst(xi, yi)) exp(ht(xi, l)\u2212 ht(xi, yi)).\nRecall that st(xi, l) = t\u2211\nt\u2032=0\nht\u2032(xi, l) = \u03b1t+1W \u22a4 t+1gt+1(xi), the loss for a T -module multiclass ResNet is thus\nPr i\u223cD1\n(p(\u03b1T+1W \u22a4 T+1gT+1(xi)) 6= yi) \u2264\n1\nm\nm\u2211\ni=1\nLexp\u03b7 (sT (xi)) (67)\n\u2264 1 m\nm\u2211\ni=1\n\u2211\nl 6=yi\nexp (\u03b7(sT (xi, l)\u2212 sT (xi, yi))) (68)\n\u2264 1 m ZT (69) = T\u220f\nt=0\nZt Zt\u22121\n(70)\nNote that Z0 = 1 m as the initial accumulated weak learners s0(xi, l) = 0.\nThe loss fraction between module t and t\u2212 1, Zt Zt\u22121 , is related to Zt \u2212 Zt\u22121 as ZtZt\u22121 = Zt\u2212Zt\u22121 Zt\u22121 + 1.\nThe Zt is bounded\nZt =\nm\u2211\ni=1\n\u2211\nl 6=yi\nexp(st(xi, l)\u2212 st(i, yi) + ht(xi, l)\u2212 ht(xi, yi)) (71)\n\u2264 m\u2211\ni=1\n\u2211\nl 6=yi\nest(xi,l)\u2212st(xi,yi)e\u03b1t+1ot+1(xi,l)\u2212\u03b1t+1ot+1(xi,yi) m\u2211\ni=1\n\u2211\nl 6=yi\nest(xi,l)\u2212st(xi,yi)e\u2212\u03b1tot(xi,l)+\u03b1tot(xi,yi) (72)\n\u2264 m\u2211\ni=1\n\u2211\nl 6=yi\nest(xi,l)\u2212st(xi,yi) ( e\u2212\u03b1t+1 + e\u03b1t+1\n2 + e\u2212\u03b1t+1 \u2212 e\u03b1t+1 2\n(ot+1(xi, yi)\u2212 ot+1(xi, l)) )\nm\u2211\ni=1\n\u2211\nl 6=yi\nest\u22121(xi,l)\u2212st\u22121(xi,yi) ( e\u03b1t + e\u2212\u03b1t\n2\n) (73)\n=( e\u2212\u03b1t+1 + e\u03b1t+1 \u2212 2\n2 Zt\u22121 + e\u03b1t+1 \u2212 e\u2212\u03b1t+1 2\nm\u2211\ni=1\n< Ct(xi, :), ot+1(xi, :) >)\n( e\u03b1t + e\u2212\u03b1t\n2\n)\n\u2264(e \u2212\u03b1t+1 + e\u03b1t+1 \u2212 2\n2 Zt\u22121 + e\u03b1t+1 \u2212 e\u2212\u03b1t+1 2 m\u2211\ni=1\n< Ct(xi, :), U\u03b3\u0303t(xi, :) >)\n( e\u03b1t + e\u2212\u03b1t\n2\n) (74)\n=( e\u2212\u03b1t+1 + e\u03b1t+1 \u2212 2\n2 Zt\u22121 + e\u03b1t+1 \u2212 e\u2212\u03b1t+1 2\n(\u2212\u03b3\u0303t)Zt\u22121) ( e\u03b1t + e\u2212\u03b1t\n2\n) (75)\nTherefore Zt Zt\u22121 \u2264 ( e\u2212\u03b1t+1 + e\u03b1t+1 2 + e\u2212\u03b1t+1 \u2212 e\u03b1t+1 2 \u03b3\u0303t )( e\u03b1t + e\u2212\u03b1t 2 ) (76)\nThe algorithm chooses \u03b1t+1 to minimize Zt. We achieve an upper bound on Zt,\n\u221a 1\u2212\u03b3\u03032t\n1\u2212\u03b3\u0303t\u221212\nby minimizing the bound in Equation (76)\nZt|\u03b1t+1=argminZt \u2264 Zt|\u03b1t+1= 12 ln( 1+\u03b3\u0303t1\u2212\u03b3\u0303t ) (77)\n\u2264 ( e\u2212\u03b1t+1 + e\u03b1t+1\n2 + e\u2212\u03b1t+1 \u2212 e\u03b1t+1 2 \u03b3\u0303t\n) e\u03b1t + e\u2212\u03b1t\n2\n\u2223\u2223\u2223\u2223 \u03b1t+1= 1 2 ln( 1+\u03b3\u0303t 1\u2212\u03b3\u0303t ) (78)\n= \u221a 1\u2212 \u03b3\u03032t 1\u2212 \u03b3\u03032t\u22121 = \u221a 1\u2212 \u03b32t (79)\nTherefore over the T modules, the training error is upper bounded as follows\nPr i\u223cD\n(p(\u03b1T+1w \u22a4 T+1gT+1(xi))) 6= yi) \u2264\nT\u220f\nt=0\n\u221a 1\u2212 \u03b32t \u2264 T\u220f\nt=0\n\u221a 1\u2212 \u03b32 = exp ( \u22121 2 T\u03b32 ) (80)\nOverall, Algorithm 3 and 4 leads us to consistent learning of ResNet."}, {"heading": "H. Experiments", "text": "H.1. Training error degradation of e2eBP on ResNet\nWe investigate e2eBP training performance on various depth ResNet. Surprisingly, we observe a training error degradation for e2eBP although the ResNet\u2019s identity loop is supposed to alleviate this problem. Despite the presence of identity loops, the e2eBP eventually is susceptible to spurious local optima. This phenomenon is explored further in Figures 5a and 5b, which respectively show how training and test accuracies vary throughout the fitting process. Our proposed sequential training procedure, BoostResNet, relieves gradient instability issues, and continues to perform well as depth increases."}], "year": 2018, "references": [{"title": "The shattered gradients problem: If resnets are the answer, then what is the question", "authors": ["Balduzzi", "David", "Frean", "Marcus", "Leary", "Lennox", "JP Lewis", "Ma", "Kurt Wan-Duo", "McWilliams", "Brian"], "venue": "arXiv preprint arXiv:1702.08591,", "year": 2017}, {"title": "Learning long-term dependencies with gradient descent is difficult", "authors": ["Bengio", "Yoshua", "Simard", "Patrice", "Frasconi", "Paolo"], "venue": "IEEE transactions on neural networks,", "year": 1994}, {"title": "Convex neural networks", "authors": ["Bengio", "Yoshua", "Le Roux", "Nicolas", "Vincent", "Pascal", "Delalleau", "Olivier", "Marcotte", "Patrice"], "venue": "Advances in neural information processing systems,", "year": 2006}, {"title": "Deep boosting", "authors": ["Cortes", "Corinna", "Mohri", "Mehryar", "Syed", "Umar"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "year": 2014}, {"title": "Adanet: Adaptive structural learning of artificial neural networks", "authors": ["Cortes", "Corinna", "Gonzalvo", "Xavi", "Kuznetsov", "Vitaly", "Mohri", "Mehryar", "Yang", "Scott"], "venue": "arXiv preprint arXiv:1607.01097,", "year": 2016}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "authors": ["Duchi", "John", "Hazan", "Elad", "Singer", "Yoram"], "venue": "Journal of Machine Learning Research,", "year": 2011}, {"title": "A desicion-theoretic generalization of on-line learning and an application to boosting", "authors": ["Freund", "Yoav", "Schapire", "Robert E"], "venue": "In European conference on computational learning theory,", "year": 1995}, {"title": "Fast r-cnn", "authors": ["Girshick", "Ross"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "year": 2015}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "authors": ["Girshick", "Ross", "Donahue", "Jeff", "Darrell", "Trevor", "Malik", "Jitendra"], "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,", "year": 2014}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "authors": ["Glorot", "Xavier", "Bengio", "Yoshua"], "venue": "In Aistats,", "year": 2010}, {"title": "Identity matters in deep learning", "authors": ["Hardt", "Moritz", "Ma", "Tengyu"], "venue": "arXiv preprint arXiv:1611.04231,", "year": 2016}, {"title": "Convolutional neural networks at constrained time cost", "authors": ["He", "Kaiming", "Sun", "Jian"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "year": 2015}, {"title": "Spatial pyramid pooling in deep convolutional networks for visual recognition", "authors": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In European Conference on Computer Vision,", "year": 2014}, {"title": "Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification", "authors": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In Proceedings of the IEEE international conference on computer vision,", "year": 2015}, {"title": "Deep residual learning for image recognition", "authors": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "authors": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "arXiv preprint arXiv:1502.03167,", "year": 2015}, {"title": "Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods", "authors": ["Janzamin", "Majid", "Sedghi", "Hanie", "Anandkumar", "Anima"], "venue": "arXiv preprint arXiv:1506.08473,", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "authors": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "authors": ["Krizhevsky", "Alex", "Hinton", "Geoffrey"], "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "authors": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "year": 2012}, {"title": "Backpropagation applied to handwritten zip code recognition", "authors": ["LeCun", "Yann", "Boser", "Bernhard", "Denker", "John S", "Henderson", "Donnie", "Howard", "Richard E", "Hubbard", "Wayne", "Jackel", "Lawrence D"], "venue": "Neural computation,", "year": 1989}, {"title": "Gradient-based learning applied to document recognition", "authors": ["LeCun", "Yann", "Bottou", "L\u00e9on", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "year": 1998}, {"title": "Efficient backprop", "authors": ["LeCun", "Yann A", "Bottou", "L\u00e9on", "Orr", "Genevieve B", "M\u00fcller", "Klaus-Robert"], "venue": "In Neural networks: Tricks of the trade,", "year": 2012}, {"title": "Fully convolutional networks for semantic segmentation", "authors": ["Long", "Jonathan", "Shelhamer", "Evan", "Darrell", "Trevor"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "year": 2015}, {"title": "Deep vs. shallow networks: An approximation theory perspective", "authors": ["Mhaskar", "Hrushikesh N", "Poggio", "Tomaso"], "venue": "Analysis and Applications,", "year": 2016}, {"title": "A theory of multiclass boosting", "authors": ["Mukherjee", "Indraneel", "Schapire", "Robert E"], "venue": "Journal of Machine Learning Research,", "year": 2013}, {"title": "A method for unconstrained convex minimization problem with the rate of convergence o (1/k2)", "authors": ["Nesterov", "Yurii"], "venue": "In Doklady an SSSR,", "year": 1983}, {"title": "Reading digits in natural images with unsupervised feature learning", "authors": ["Netzer", "Yuval", "Wang", "Tao", "Coates", "Adam", "Bissacco", "Alessandro", "Wu", "Bo", "Ng", "Andrew Y"], "venue": "In NIPS workshop on deep learning and unsupervised feature learning,", "year": 2011}, {"title": "On the momentum term in gradient descent learning algorithms", "authors": ["Qian", "Ning"], "venue": "Neural networks,", "year": 1999}, {"title": "Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in neural information processing", "authors": ["Ren", "Shaoqing", "He", "Kaiming", "Girshick", "Ross", "Sun", "Jian"], "year": 2015}, {"title": "The crossentropy method: a unified approach to combinatorial optimization, Monte-Carlo simulation and machine learning", "authors": ["Rubinstein", "Reuven Y", "Kroese", "Dirk P"], "venue": "Springer Science & Business Media,", "year": 2013}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "authors": ["Saxe", "Andrew M", "McClelland", "James L", "Ganguli", "Surya"], "venue": "arXiv preprint arXiv:1312.6120,", "year": 2013}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "authors": ["Sermanet", "Pierre", "Eigen", "David", "Zhang", "Xiang", "Mathieu", "Micha\u00ebl", "Fergus", "Rob", "LeCun", "Yann"], "venue": "arXiv preprint arXiv:1312.6229,", "year": 2013}, {"title": "Selfieboost: A boosting algorithm for deep learning", "authors": ["Shalev-Shwartz", "Shai"], "venue": "arXiv preprint arXiv:1411.3436,", "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "authors": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "arXiv preprint arXiv:1409.1556,", "year": 2014}, {"title": "Convolutional networks and learning invariant to homogeneous multiplicative scalings", "authors": ["Tygert", "Mark", "Szlam", "Arthur", "Chintala", "Soumith", "Ranzato", "Marc\u2019Aurelio", "Tian", "Yuandong", "Zaremba", "Wojciech"], "venue": "arXiv preprint arXiv:1506.08230,", "year": 2015}, {"title": "Residual networks behave like ensembles of relatively shallow networks", "authors": ["Veit", "Andreas", "Wilber", "Michael J", "Belongie", "Serge"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2016}, {"title": "Adadelta: an adaptive learning rate method", "authors": ["Zeiler", "Matthew D"], "venue": "arXiv preprint arXiv:1212.5701,", "year": 2012}, {"title": "Visualizing and understanding convolutional networks", "authors": ["Zeiler", "Matthew D", "Fergus", "Rob"], "venue": "In European conference on computer vision,", "year": 2014}], "id": "SP:4dce09fde057de2550065d944b833e1d47bc15e9", "authors": [{"name": "Furong Huang", "affiliations": []}, {"name": "John Langford", "affiliations": []}, {"name": "Robert E. Schapire", "affiliations": []}], "abstractText": "We prove a multi-channel telescoping sum boosting theory for the ResNet architectures which simultaneously creates a new technique for boosting over features (in contrast to labels) and provides a new algorithm for ResNet-style architectures. Our proposed training algorithm, BoostResNet, is particularly suitable in nondifferentiable architectures. Our method only requires the relatively inexpensive sequential training of T \u201cshallow ResNets\u201d. We prove that the training error decays exponentially with the depth T if the weak module classifiers that we train perform slightly better than some weak baseline. In other words, we propose a weak learning condition and prove a boosting theory for ResNet under the weak learning condition. A generalization error bound based on margin theory is proved and suggests that ResNet could be resistant to overfitting using a network with l1 norm bounded weights.", "title": "Learning Deep ResNet Blocks Sequentially using Boosting Theory"}