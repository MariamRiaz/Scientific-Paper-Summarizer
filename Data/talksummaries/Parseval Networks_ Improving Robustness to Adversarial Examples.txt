0	27	Deep neural networks achieve near-human accuracy on many perception tasks (He et al., 2016; Amodei et al., 2015).
1	4	However, they lack robustness to small alterations of the inputs at test time (Szegedy et al., 2014).
2	96	Indeed when presented with a corrupted image that is barely distinguishable from a legitimate one by a human, they can predict incorrect labels, with high-confidence.
3	14	An adversary can design such so-called adversarial examples, by adding a small perturbation to a legitimate input to maximize the likelihood of an incorrect class under constraints on the magnitude of the perturbation (Szegedy et al., 2014; Goodfellow et al., 2015; Moosavi-Dezfooli et al., 2015; Pa- pernot et al., 2016a).
7	27	Second, it underlines the lack of robustness of neural networks and questions their ability to generalize in settings where the train and test distributions can be (slightly) different as is the case for the distributions of legitimate and adversarial examples.
8	116	Whereas the earliest works on adversarial examples already suggested that their existence was related to the magnitude of the hidden activations gradient with respect to their inputs (Szegedy et al., 2014), they also empirically assessed that standard regularization schemes such as weight decay or training with random noise do not solve the problem (Goodfellow et al., 2015; Fawzi et al., 2016).
9	41	The current mainstream approach to improving the robustness of deep networks is adversarial training.
10	32	It consists in generating adversarial examples on-line using the current network’s parameters (Goodfellow et al., 2015; Miyato et al., 2015; Moosavi-Dezfooli et al., 2015; Szegedy et al., 2014; Kurakin et al., 2016) and adding them to the training data.
11	43	This data augmentation method can be interpreted as a robust optimization procedure (Shaham et al., 2015).
12	61	In this paper, we introduce Parseval networks, a layerwise regularization method for reducing the network’s sensitivity to small perturbations by carefully controlling its global Lipschitz constant.
13	24	Since the network is a composition of functions represented by its layers, we achieve increased robustness by maintaining a small Lipschitz constant (e.g., 1) at every hidden layer; be it fully-connected, convolutional or residual.
14	56	In particular, a critical quantity governing the local Lipschitz constant in both fully connected and convolutional layers is the spectral norm of the weight matrix.
15	35	Our main idea is to control this norm by parameterizing the network with parseval tight frames (Kovačević & Chebira, 2008), a generalization of orthogonal matrices.
16	8	The idea that regularizing the spectral norm of each weight matrix could help in the context of robustness appeared as early as (Szegedy et al., 2014), but no experiment nor algorithm was proposed, and no clear conclusion was drawn on how to deal with convolutional layers.
17	13	Previous work, such as double backpropagation (Drucker & Le Cun, 1992) has also explored jacobian normalization as a way to improve generalization.
18	12	First, we provide a deeper analysis which applies to fully connected networks, convolutional networks, as well as Residual networks (He et al., 2016).
19	7	Second, we propose a computationally efficient algorithm and validate its effectiveness on standard benchmark datasets.
20	13	We report results on MNIST, CIFAR-10, CIFAR-100 and Street View House Numbers (SVHN), in which fully connected and wide residual networks were trained (Zagoruyko & Komodakis, 2016) with Parseval regularization.
21	13	The accuracy of Parseval networks on legitimate test examples matches the state-of-the-art, while the results show notable improvements on adversarial examples.
22	50	Besides, Parseval networks train significantly faster than their vanilla counterpart.
23	11	In the remainder of the paper, we first discuss the previous work on adversarial examples.
25	3	Then, we introduce Parseval networks and its efficient training algorithm.
26	5	Section 5 presents experimental results validating the model and providing several insights.
34	24	A multiclass classifier is a function ĝ : (x ∈ RD,W ∈ W) 7→ argmaxȳ∈Y gȳ(x,W ), where W are the parameters to be learnt, and gȳ(x,W ) is the score given to the (input, class) pair (x, ȳ) by a function g : RD × W → RY .
35	68	We take g to be a neural network, represented by a computation graph G = (N , E), which is a directed acyclic graph with a single root node, and each node n ∈ N takes values in Rd (n) out and is a function of its children in the graph, with learnable parameters W (n): n : x 7→ φ(n) ( W (n), ( n′(x) ) n′:(n,n′)∈E ) .
