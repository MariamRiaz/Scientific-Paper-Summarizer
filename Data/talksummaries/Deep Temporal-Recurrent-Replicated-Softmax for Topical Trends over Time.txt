21	18	(2) Demonstrate better generalization (logprobability and time stamp prediction), topic interpretation (coherence), evolution and characterization, compared to the state-of-the-art.
24	11	RSM (Fig 2, Left) models are a family of differentsized Restricted Boltzmann Machines (RBMs) (Gehler et al., 2006; Xing et al., 2005; Gupta et al., 2015b,c) that models word counts by sharing the same parameters with multinomial distribution over the observable i.e. it can be interpreted as a single multinomial unit (Fig 2, Middle) sampled as many times as the document size.
26	15	The proposed RNN-RSM model (Fig 2, Right) is a sequence of conditional RSMs1 such that at any time step t, the RSM’s bias parameters bv(t) and bh(t) depend on the output of a deterministic RNN with hidden layer u(t−1) in the previous time step, t−1.
27	10	Similar to RNN-RBM (BoulangerLewandowski et al., 2012), we constrain RNN hidden units (u(t)) to convey temporal information, while RSM hidden units (h(t)) to model conditional distributions.
30	61	Each h(t) ∈ {0, 1}F be a binary stochastic hidden topic vector with size F and V̂(t) = {V(t)n }N(t)n=1 be a collection of N documents at time step t. Let V (t) n be a K ×D(t)n observed binary matrix of the nth document in the collection where, D(t)n is the document size and K is the dictionary size over all the time steps.
32	15	Since biases of RSM depend on the output of RNN at previous time steps, that allows to propagate the estimated gradient at each RSM backward through time (BPTT).
34	15	2: Compute bv(t) and bh(t) using eq 1.
38	25	5: Compute gradients (eq 6) w.r.t.
42	8	v̂(t)n is a vector of v̂kn (denotes the count for the kth word in nth document).∑N(t) n=1 v̂ (t) n refers to the sum of observed vectors across documents at time step t where each document is represented as- v̂(t)n = [{v̂k,(t)n }Kk=1] and v̂k,(t)n = D (t) n∑ i=1 v k,(t) n,i (3) where vk,(t)n,i =1 if visible unit i takes on k th value.
43	22	In each RSM, a separate RBM is created for each document in the collection at time step t with D (t) n softmax units, where D (t) n is the count of words in the nth document.
44	11	Consider a document of D(t)n words, the energy of the state {V(t)n ,h(t)n } at time step, t is given by- E(V(t)n ,h (t) n ) =− F∑ j=1 K∑ k=1 h (t) n,jW k j v̂ k,(t) n − K∑ k=1 v̂k,(t)n b k v −D(t)n F∑ j=1 bh,jh (t) n,j Observe that the bias terms on hidden units are scaled up by document length to allow hidden units to stabilize when dealing with different-sized documents.
48	8	The dependence analogy follows- E(V (t) n ,h (t) n ) ∝ 1bv(t) and E(V (t) n ,h (t) n ) ∝ 1bh(t) lnP (V (t) n ) ∝ 1 E(V (t) n ,h (t) n ) ; lnP (V̂ (t) n ) ∝ lnP ({V̂τn}τ<t) Observe that the prior is seen as the deterministic hidden representation of latent topics and injected into each hidden state of RSMs, that enables the likelihood of the data to model complex temporal densities i.e. heteroscedasticity in document collections (V̂) and temporal topics (H).
55	11	The free energy F(V(t)n ) is related to normalized probability of V(t)n as P (V (t) n ) ≡ exp−F(V(t)n ) /Z(t)n and as follows- F(V(t)n ) = − K∑ k=1 v̂k,(t)n b k v − F∑ j=1 log(1+ exp(D(t)n bh,j + K∑ k=1 v̂k,(t)n W k j )) Gradient approximations w.r.t.
59	15	∂C ∂Wuh = T∑ t=1 ∂Ct ∂bh (t) u(t−1)T ∂C ∂Wuv = T∑ t=1 ∂Ct ∂bv (t) u(t−1)T ∂C ∂Wvu = T∑ t=1 ∂Ct ∂u(t) u(t)(1− u(t)) N(t)∑ n=1 v̂(t)Tn ∂C ∂bh = T∑ t=1 ∂Ct ∂bh (t) and ∂C ∂bv = T∑ t=1 ∂Ct ∂bv (t) ∂C ∂bu = T∑ t=1 ∂Ct ∂u(t) u(t)(1− u(t)) ∂C ∂Wuu = T∑ t=1 ∂Ct ∂u(t) u(t)(1− u(t))u(t−1)T (6) For the single-layer RNN-RSM, the BPTT recurrence relation for 0 ≤ t < T is given by- ∂Ct ∂u(t) = Wuu ∂Ct+1 ∂u(t+1) u(t+1)(1− u(t+1)) +Wuh ∂Ct+1 ∂bh (t+1) + Wuv ∂Ct+1 ∂bv (t+1) where u(0) being a parameter and ∂CT ∂u(T ) = 0.
60	73	See Training RNN-RSM with BPTT in Algo 1.
62	16	We combine papers for each year from the two venues to prepare the document collections over time.
69	27	We use perplexity to choose the number of topics (=30).
70	15	See Table 2 for hyperparameters.
72	15	N (t) is the number of documents in a collection (V̂(t)) at time t. Better models have lower perplexity values, suggesting less uncertainties about the documents.
80	13	Topic Detection: To extract topics from each RSM, we compute posterior P (V̂(t)|hj = 1) by activating a hidden unit and deactivating the rest in a hidden layer.
82	7	Topic Popularity: To determine topic popularity, we selected three popular topics (Sentiment Analysis, Word Vector and Dependency Parsing) in NLP research and create a set3 of key-terms (including unigrams and bigrams) for each topic.
93	20	It suggests that the higher number of new topic-terms evolved in RNN-RSM, compared to DTM.
102	23	The coherence (COH) score is computed as the arithmetic mean of the cosine similarities between all word pairs.
106	11	Table 3 shows mean-COH and median-COH scores, computed by mean and median of scores from Fig 3e and 3f, respectively.
108	9	Qualitative: Table 5 shows topics (top-10 words) with the highest and lowest coherence scores.
112	17	4github.com/earthquakesan/palmetto-py Let Q̂model = {Q(t)model}Tt=1 be a set of sets5 of topic-terms discovered by themodel (LDA, RSM, DTM and RNN-RSM) over different time steps.
114	11	We define average-SPAN for all the topic-terms appearing in the topics discovered over the years, avg-SPAN(Q̂) = 1 ||Q̂|| ∑ {k|Q(t)∈Q̂∧k∈Q(t)} Sk(Q̂) v̂k = 1 ||Q̂|| ∑ {k|Q(t)∈Q̂∧k∈Q(t)} Sdictk (Q̂) 5a set by bold and set of sets by b̂old where ||Q̂|| = |{k|Q(t) ∈ Q̂ ∧ k ∈ Q(t)}| is the count of unique topic-terms and v̂k =∑T t=1 ∑Dt j=1 v k j,t denotes the count of k th keyword.
116	26	Observe that RNNRSM captures longer SPANs for popular keywords and better word usage in NLP research.
117	152	For example: Word Embedding is one of the top keywords, appeared locally (Figure 5) in the recent years.
118	35	RNN-RSM detects it in the topics from 2010 to 2014, however DTM does not.
