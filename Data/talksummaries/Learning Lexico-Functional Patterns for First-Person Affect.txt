9	23	Large-scale sentiment dictionaries focus on compiling lexical items that bear a consistent affect all on their own (Wilson et al., 2005).
16	15	Here we are finding the internal sentiment of the speaker, or, as Rashkin et al. refer to it, the ”mental state” of the speaker.
17	45	Inspired by A&R’s framework, our work learns lexico-functional patterns (patterns involving lexical items or pairs of lexical items in specific grammatical relations that we show to capture functorargument relations in A&R’s sense), about the effects of combining particular arguments with particular verbs (event types) from first-person narratives.
18	23	Our novel observation is that learning these compositional functions is greatly simplified in the case of first-person affect.
19	54	People bear positive affect to themselves, so sentences with first-person elements, e.g. I/we/me, reduce the problem for an approach like A&R’s to learning the polarity that results from composing the verb with only one of its arguments, i.e. only Rows 1, 2 in Table 1 need to be learned for first person subjects.
28	16	To reduce noise, we restrict the blogs to those from well-known blogging sites (Ding and Riloff, 2016), and select 15,466 stories whose length ranges from 225 to 375 words.
30	24	To bootstrap, we apply AutoSlog-TS, a weakly supervised pattern learner that only requires training sets os stories labeled broadly as POSITIVE or NEGATIVE (Riloff, 1996; Riloff and Wiebe, 2003).
32	37	The left-hand side of Table 3 lists examples of AutoSlog patterns and the right-hand side illustrates a specific lexical-syntactic pattern that corresponds to each general pattern template, as instantiated in first-person stories.1 When bootstrapping a larger positive and negative story corpus, we use the whole story, not just the first person sentences.
37	15	AutoSlog-TS computes statistics on the strength of association of each pattern with each class, i.e. P(POSITIVE | p) and P(NEGATIVE | p), along with the pattern’s overall frequency.
38	38	We define three parameters for each class: θf , the frequency with which a pattern occurs, θp, the probability with which a pattern is associated with the given class and θn, the number of patterns that must occur in the text for it to be labeled.
41	23	We select θp = 0.7, θf = 10 and θn = 3 for the positive class and θp = 0.85, θf = 10 and θn = 4 for the negative class for bootstrapping.
45	15	A critical simplifying assumption of our method is that a multi-sentence story can be labelled as a whole as positive or negative, and that each of its sentences inherit this polarity.
47	23	Our training set consists of 46,255 positive and 25,069 negative sentences.
65	28	We thus experiment with a number of baseline classifiers: the default SVM classifier from Weka with unigram features (Hall et al., 2005), a version of the NRC-Canada sentiment classifier (Mohammad et al., 2013), provided to us by Qadir and Riloff (2014), and the Stanford Sentiment classifier (Socher et al., 2013).
66	35	The Stanford Sentiment classifier is a based on Recursive Neural Networks, and trained on a compositional Sentiment Treebank, which includes fine-grained sentiment labels for 215,154 phrases from 11,855 sentences from movie reviews.
69	25	Thus we also retrained it (RETRAINED STANFORD) on high precision phrases from AutoSlog extracted from our training data of positive and negative blogs.
73	36	Rows 1-3 of Table 4 show the results for the three baselines, in terms of Fscore for each class and the macro F. Stanford outperforms both NRC and SVM, but misses many cases of positive sentiment.
76	31	We therefore hypothesized that a cascading classifier, which supplements one of the baseline sentiment classifiers with the lexicofunctional patterns that AutoSlog learns might yield higher performance.
81	18	We implement cascading classifiers to test our hypothesis.
85	15	For our cascading classifiers, we combine our baseline classifiers (NRC and Stanford), with our AutoSlog classifier.
87	14	The results for the cascading experiments are in Rows 6-9 of Table 4.
88	26	Cascading NRC and AutoSlog provides the best performance, improving both the positive and negative classes, for a macro F of 0.71.
94	21	The patterns shown in Table 5 are predicted by A&R’s framework, some functions of which can be seen in Table 1.
102	18	Even many positive have uses are light verbs describing an activity such as have lunch.
103	33	Verbs from the negative class are strikingly different.
121	109	We constructed a dataset of positive and negative first-person experiencer sentences and used them to learn such patterns.
122	24	We then showed that the performance of current sentiment classifiers can be enhanced by augmenting them with these patterns.
124	48	In addition, we analyze the linguistic functions that indicate positivity and negativity for the first person experiencer, and show that they are very different.
126	25	In future work, we plan to explore the integration of these observations into sentiment resources such as the +-Effect lexicon (Choi and Wiebe, 2014).
127	31	We plan to apply these high precision first-person lexical patterns beyond blog data and with other personmarking.
