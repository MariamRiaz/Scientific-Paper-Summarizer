2	57	Consequently, online learning algorithms are well suited to dynamic and adversarial environments, where real-time learning from changing data is essential making them ubiquitous in practical applications such as servicing search advertisements.
3	38	In these settings often these algorithms interact with sensitive user data, making privacy a natural concern for these algorithms.
19	13	Online linear optimization over the sphere and prediction with expert advice are notable examples.
48	21	This section introduces the model of online (linear) learning, the distinction between full and partial feedback scenarios, and the notion of differential privacy in this model.
51	56	Subsequently, it observes the loss l t 2 Y ✓ RN and suffers a loss of hl t , x t i.
53	10	In particular, achieving a sub-linear regret (o(T )) corresponds to doing almost as good (averaging across T rounds) as the fixed decision with the least loss in hindsight.
57	19	• Prediction with Expert Advice: Here the underlying decision set is the simplex X = N = {x 2 Rn : x i 0, P n i=1 x i = 1} and the loss vectors are constrained to the unit cube Y = {l t 2 RN : kl t k1  1}.
62	15	This makes designing algorithms for this feedback model challenging.
67	49	A randomized online learning algorithm A on the action set X and the loss set Y is (", )-differentially private if for any two sequence of loss vectors L = (l 1 , .
69	27	The above definition of Differential Privacy is specific to the online learning scenario in the sense that it assumes the change of a complete loss vector.
75	44	In this section, we describe an algorithmic template (Algorithm 1) for differentially private online linear optimization, based on Follow-the-Regularized-Leader scheme.
76	9	Subsequently, we outline the noise injection scheme (Algorithm 2), based on the Tree-based Aggregation Protocol (Dwork et al., 2010), used as a subroutine by Algorithm 1 to ensure input differential privacy.
77	58	The following is our main theorem in this setting.
78	28	Algorithm 1 when run with D = LapN ( ) where = kYk1 log T " , regularization R(x), decision set X and loss vectors l 1 , .
80	10	Algorithm 1 FTRL Template for OLO input Noise distribution D, Regularization R(x) 1: Initialize an empty binary tree B to compute differentially private estimates of P t s=1 l s .
94	11	For the analysis, we define the random variable Z t to be the net amount of noise injected by the TreeBasedAggregation (Algorithm 2) on the true partial sums.
97	9	Since the outputs of Algorithm 1 are strictly determined by the prefix sum estimates produced by TreeBasedAgg, by the post-processing theorem, this certifies that the entire sequence of choices made by the algorithm (across all T rounds of play) (x t : t 2 [T ]) is "- differentially private.
98	82	We modify the standard Tree-based Aggregation protocol to make sure that the noise on each output (partial sum) is distributed identically (though not necessarily independently) across time.
100	40	Lemma 3.3 (Privacy Guarantees with Laplacian Noise).
102	13	• Distribution: 8t 2 [T ], Z t ⇠ Pdlog Te i=1 n i , where each n i is independently sampled from LapN ( ).
113	21	Let Z ⇠ D. x̂ 1 , x 1 , x̂ t , argmin x2X ⌘hx, Z + X i l i i+R(x) We have that E Z1...ZT⇠D " TX t=1 hl t , x t i # = E Z⇠D " TX t=1 hl t , x̂ t i # (1) To see the above equation note that E Zt⇠D [hlt, x̂ti] = E Z⇠D [hlt, xti] since x, x̂t have the same distribution.
117	29	Formally define the augmented series of loss functions by defining l 0 (x) = 1 ⌘ R(x) + hZ, xi where Z ⇠ D is a sample.
160	10	l t 1), it is clear that the distribution of the output of the algorithm for the first t 0 rounds (x̃ 1 , .
172	9	The following follows from Fact C.1 proved in the appendix.
181	10	Therefore we have that E[Regret| ¯F ] = E{Zt} " EA " TX t=1 ⇣ h˜l t , x̃ t i h˜l t , x⇤i ⌘# ¯F # (8) To bound the above quantity we make use of the following lemma which is a specialization of Theorem 1 in (Bubeck et al., 2012a) to the case of multi-armed bandits.
187	10	Theorem 4.5 (Bandit Linear Optimization).
191	25	In this work, we demonstrate that ensuring differential privacy leads to only a constant additive increase in the incurred regret for online linear optimization in the full feedback setting.
192	21	We also show nearly optimal bounds (in terms of T) in the bandit feedback setting.
193	12	Multiple avenues for future research arise, including extending our bandit results to other challenging partial-information models such as semi-bandit, combinatorial bandit and contextual bandits.
194	50	Another important unresolved question is whether it is possible to achieve an additive separation in ", T in the adversarial bandit setting.
