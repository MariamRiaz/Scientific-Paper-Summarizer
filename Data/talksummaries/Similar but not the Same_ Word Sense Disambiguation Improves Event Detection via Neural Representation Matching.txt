1	56	Towards this end, event detection (ED) is the task of locating event triggers (usually verbs or nouns) within a given text, and classifying them among a given set of event types.
5	9	This is due to the goal of WSD to determine the sense of a word within a particular context, given a set of possible senses that the word can take on.
7	7	For WSD, the candidate labels are the possible senses (e.g, sense ids in WordNet) that the word of interest can have, while for ED, they are the set of predetermined event types (e.g, the event subtypes in the ACE 2005 dataset1).
8	29	Consider the word “fired” in the following sentence as an example: The boss fired his secretary today.
9	22	For WSD, there are 12 possible senses for the verb “fire” in WordNet in which the correct label for the word “fired” in this case is the sense id “fire%2:41:00::” (i.e, “terminate the employment of ”).
10	16	The ED task in the ACE 2005 dataset, on the other hand, involves 33 possible event subtypes with “End-Position” as the correct event subtype/label for the word “fired” in our example.
11	26	In order to make such label predictions, both ED and WSD need to model the word itself and its context (i.e, the words “fired”, “boss”, and “secretary” in the example).
12	6	This similar modeling allows the same DL model to be adopted for both ED and WSD, facilitating the use of WSD data to improve the feature representations for ED via parameter/representation tying.
14	19	For instance, in the example above, the knowledge from WSD that the word “fired” is referring to a termination of employment would clearly help ED to identify “End-Position” as the correct event type (rather than the incorrect event type “Attack”) for “fired” in this case.
15	14	How can we exploit this intuition to improve the performance of the DL models for ED with WSD 1 https://www.ldc.upenn.edu/collaborations/past-projects/ ace data?
16	98	In this work, we propose a novel method based on representation matching to transfer the knowledge learned from the WSD data to the DL models for ED.
18	50	The two models share the network architecture, but involve different parameters that are specific to the tasks.
20	12	We demonstrate the effectiveness of the proposed method on two widely used datasets for ED.
47	20	The only difference is instead of running the convolution over the k consecutive vectors, NCNN convolutes over the k arbitrarily non-consecutive k vectors in V .
54	6	CNN+BiRNN: In this model (Feng et al., 2016), X is passed through both a CNN and a BiRNN whose results are concatenated to produce the hidden representation R for ED.
62	31	A typical method for transfer learning/multitask learning in NLP is to alternate the training process for the parameter-shared models of the related tasks (possibly with different datasets) (Guo et al., 2016; Li et al., 2015; Liu et al., 2016).
66	7	In order to learn the parameters for this model, in each iteration, (Guo et al., 2016) select one of the tasks with some probabilities, sample a mini-batch of examples in the dataset of the chosen task, and update the model parameters using the objective function specific to the chosen task.
67	14	Consequently, the model parameters for feature representation learning are updated at every iteration while only the model parameters in the output layer for the chosen task are updated at the current iteration.
72	18	In our case of WSD and ED, although there are some overlap between the semantic differentiation of the two tasks, the labels in the WSD datasets (i.e, the sense ids) tend to be more fine-grained and exhaustive than those in ED.
103	11	In this section, we compare the proposed MATCHING method with the transfer learning baseline ALT in (Guo et al., 2016) and the separate training mechanism for ED (called SEPARATE) employed in the previous work for ED (Chen et al., 2015; Nguyen and Grishman, 2015b).
105	6	We report the performance when each of the DL methods in Section 2.1 is used as the network to learn the feature representations for ED and WSD.
107	13	The first observation is that the proposed transfer learning method MATCHING is consistently better than the baseline method ALT across different deep learning models and datasets with large performance gap.
109	8	In fact, the performance of the ALT method is even worse than the traditional SEPARATE method also over different network architectures and datasets.
110	13	Consequently, training a single deep learning model on a combination of ED and WSD data (as in ALT) does not automatically enable the model to learn to exploit the similar structures of the two tasks.
115	20	Regarding the best reported performance, our best performance on ACE (i.e, 71.2% with CNN) is comparable with the recent state-of-the-art performance (i.e, Table 1).
117	21	Our current work do not employ such information to better reflect the realistic setting.
118	28	For the TAC 2015 dataset, our best performance is 60.7% with CNN+BiRNN although the performance of the other models is also very close.
119	91	This performance is better than the best performance that has been reported on the TAC 2015 (i.e, Table 2).
124	24	We present a method that improves the performance of deep learning models for ED by training two different versions of the same network architecture for ED and WSD, while encouraging the knowledge transfer between the two versions via representation matching.
125	71	The proposed method produces better results across a variety of deep learning models.
