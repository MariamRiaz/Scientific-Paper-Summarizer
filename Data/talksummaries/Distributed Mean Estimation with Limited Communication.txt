5	7	In a typical scenario of synchronized distributed learning, each client obtains a copy of a global model.
6	35	The clients then update the model independently based on their local data.
7	15	The updates (usually in the form of gradients) are then sent to a server, where they are averaged and used to update the global model.
8	28	A critical step in all of the above algorithms is to estimate the mean of a set of vectors as in Eq.
10	13	This has spurred a line of work focusing on communication cost in learning (Tsitsiklis & Luo, 1987; Balcan et al., 2012; Zhang et al., 2013; Arjevani & Shamir, 2015; Chen et al., 2016).
11	31	The communication cost can be prohibitive for modern applications, where each client can be a low-power and low-bandwidth device such as a mobile phone (Konečnỳ et al., 2016).
12	34	Given such a wide set of applications, we study the basic problem of achieving the optimal minimax rate in distributed mean estimation with limited communication.
13	31	We note that our model and results differ from previous works on mean estimation (Zhang et al., 2013; Garg et al., 2014; Braverman et al., 2016) in two ways: previous works assume that the data is generated i.i.d.
14	25	according to some distribution; we do not make any distribution assumptions on data.
16	18	Our proposed communication algorithms are simultaneous and independent, i.e., the clients independently send data to the server and they can transmit at the same time.
17	9	In any independent communication protocol, each client transmits a function of Xi (say f(Xi)), and a central server estimates the mean by some function of f(X 1 ), f(X 2 ), .
18	12	Let ⇡ be any such protocol and let Ci(⇡, Xi) be the expected number of transmitted bits by the i-th client during protocol ⇡, where throughout the paper, expectation is over the randomness in protocol ⇡.
25	26	For a protocol ⇡, the worst case error for all Xn 2 Sd is E(⇡, Sd) def= max Xn:Xi2Sd 8i E(⇡, Xn).
26	76	Let ⇧(c) denote the set of all protocols with communication cost at most c. The minimax MSE is E(⇧(c), Sd) def= min ⇡2⇧(c) E(⇡, Sd).
27	51	We first analyze the MSE E(⇡, Xn) for three algorithms, when C(⇡, Xn) = ⇥(nd), i.e., each client sends a constant number of bits per dimension.
28	53	Stochastic uniform quantization.
29	25	In Section 2.1, as a warm-up we first show that a naive stochastic binary quantization algorithm (denoted by ⇡sb) achieves an MSE of E(⇡sb, Xn) = ⇥ d n · 1 n nX i=1 ||Xi||2 2 !
30	35	, and C(⇡sb, Xn) = n · (d + ˜O(1))2, i.e., each client sends one bit per dimension.
31	12	We further show that this bound is tight.
33	17	A natural way to decease the error is to increase the number of levels of quantization.
34	6	If we use k levels of quantization, in Theorem 2, we show that the error deceases as E(⇡sk, Xn) = O d n(k 1)2 · 1 n nX i=1 ||Xi||2 2 !
35	8	(2) However, the communication cost would increase to C(⇡sk, Xn) = n · (ddlog 2 ke + ˜O(1)) bits, which can be expensive, if we would like the MSE to be o(d/n).
36	42	In order to reduce the communication cost, we propose two approaches.
37	18	Stochastic rotated quantization: We show that preprocessing the data by a random rotation reduces the mean squared error.
42	10	Instead of using dlog 2 ke bits per dimension, we show that using variable length encoding such as arithmetic coding to compress the data reduces the communication cost significantly.
44	4	, and with ⇥(nd) bits of communication i.e., constant number of bits per dimension per client.
45	5	Of the three protocols, ⇡svk has the best MSE for a given communication cost.
46	7	Note that ⇡svk uses k quantization levels but still uses O(1) bits per dimension per client for all k  pd.
47	16	Theoretically, while variable length coding has better guarantees, stochastic rotated quantization has several practical advantages: it uses fixed length coding and hence can be combined with encryption schemes for privacy preserving secure aggregation (Bonawitz et al., 2016).
48	42	It can also provide lower quantization error in some scenarios due to better constants (see Section 7 for details).
