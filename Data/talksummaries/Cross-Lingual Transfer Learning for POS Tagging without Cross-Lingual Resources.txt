1	39	Given insufficient training examples, we can improve the POS tagging performance by cross- lingual POS tagging, which exploits affluent POS tagging corpora from other source languages.
2	41	This approach usually requires linguistic knowledge or resources about the relation between the source language and the target language such as parallel corpora (Täckström et al., 2013; Duong et al., 2013; Kim et al., 2015a; Zhang et al., 2016), morphological analyses (Hana et al., 2004), dictionaries (Wisniewski et al., 2014), and gaze features (Barrett et al., 2016).
3	11	Given no linguistic resources between the source language and the target language, transfer learning methods can be utilized instead.
4	31	Transfer learning for cross-lingual cases is a type of transductive transfer learning, where the input domains of the source and the target are different (Pan and Yang, 2010) since each language has its own vocabulary space.
5	8	When the input space is the same, lower layers of hierarchical models can be shared for knowledge transfer (Collobert et al., 2011; Kim et al., 2015b; Yang et al., 2017), but that approach is not directly applicable when the input spaces differ.
6	66	Yang et al. (2017) used shared character embeddings for different languages as a cross-lingual transfer method while using different word embeddings for different languages.
8	27	In this work, we introduce a cross-lingual transfer learning model for POS tagging requiring no cross-lingual resources, where knowledge transfer is made in the BLSTM layers on top of word embeddings and character embeddings.
9	36	Inspired by Kim et al. (2016)’s multi-task slot-filling model, our model utilizes a common BLSTM for representing language-generic information, which al- 2832 lows knowledge transfer from other languages, and private BLSTMs for representing languagespecific information.
10	5	The common BLSTM is additionally encouraged to be language-agnostic with language-adversarial training (Chen et al., 2016) so that the language-general representations to be more compatible among different languages.
17	47	The outputs of the common BLSTM and the private BLSTM of the current language are summed to be used as the input to the softmax layer to predict the POS tags of given word sequences.
20	10	Language-Adversarial Training We encourage the outputs of the common BLSTM to be language-agnostic by using language-adversarial training (Chen et al., 2016) inspired by domainadversarial training (Ganin et al., 2016; Bousmalis et al., 2016).
22	6	The encoder is with three convolution filters whose sizes are 3, 4, and 5.
32	51	We adopted the bidirectional language modeling objective, where the sum of the common BLSTM and the private BLSTM is used as the input to the language modeling module.
33	14	It can be formulated as: Ll = − S∑ i=1 N∑ j=1 log (P (wj+1|fj)) + log (P (wj−1|bj)) , (3) where fj and bj represent the j-th outputs of the forward direction and the backward direction, respectively, given the output sum of the common BLSTM and the private BLSTM.
35	6	ws is used to give different weights to the source language and the target language.
36	24	Since the source language has a larger train set and we are focusing on improving the performance of the target language, ws is set to 1 when training the target language.
38	24	For the evaluation, we used the POS datasets from 14 different languages in Universal Dependencies corpus 1.4 (Nivre et al., 2016).
39	33	We used English as the source language, which is with 12,543 training sentences.2 We chose datasets with 1k to 14k training sentences.
40	16	The number of tag labels differs for each language from 15 to 18 though most of them are overlapped within the languages.
41	73	Table 1 shows the POS tagging accuracies of different transfer learning models when we limited the number of training sentences of the target languages to be the same as 1,280 for fair comparison among different languages.
43	16	Training with only the train sets in the target languages (c) showed 91.61% on average.
44	23	When bidirectional language modeling objective is used (c, l), the accuracies were significantly increased to 92.82% on average.
46	16	With transfer learning, the three cases of using only the common BLSTM (c), using only the private BLSTMs (p), and using both (c, p) were evaluated.
48	34	However, our proposed model (c, p, l + a), which utilizes both the common BLSTM with language-adversarial training and the private BLSTMs, showed the highest average score, 93.26%.
49	5	For all the Germanic languages, where the source language also belongs to, the accuracies are significantly higher than those of other transfer learning models.
59	15	In this case, just having private BLSTMs without the common BLSTM can show better performance.
64	5	Since we have four languages for each of Germanic, Slavic, and Romance language families, we evaluated the performance of those languages using the other languages in the same families as the source languages expecting that languages in the same language family are more likely to be helpful each other.
66	44	When we tried to use 1,280, 320, and 32 tag-labeled training sentences for each language in the multi-source settings, the results showed noticeably better performance than the results of using English as a single source language.
68	16	We also tried to use Wasserstein distance (Arjovsky et al., 2017) for the adversarial training in the multi-source settings, but there were no significant differences on average.4 Implementation Details All the models were optimized using ADAM (Kingma and Ba, 2015)5 with minibatch size 32 for total 100 epochs and we picked the parameters showing the best accuracy on the development set to report the score on the test set.
69	17	The dimensionalites of all the BLSTM related layers follow Plank et al. (2016)’s model.
