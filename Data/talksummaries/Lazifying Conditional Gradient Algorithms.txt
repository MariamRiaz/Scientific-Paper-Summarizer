0	74	Convex optimization is an important technique both from a theoretical and an applications perspective.
1	55	Gradient descent based methods are widely used due to their simplicity and easy applicability to many real-world problems.
2	68	We are interested in solving constraint convex optimization problems of the form min x2P f(x), (1) where f is a smooth convex function and P is a polytope, with access to f being limited to first-order information, i.e., we can obtain rf(v) and f(v) for a given v 2 P and access to P via a linear minimization oracle which returns x = argmin v2P cx for a given linear objective c. When solving Problem (1) using gradient descent approaches in order to maintain feasibility, typically a projection step is required.
3	90	This projection back into the feasible region P is potentially computationally expensive, especially for complex feasible regions in very large dimensions.
4	282	As such projection-free methods gained a lot of attention recently, in particular the Frank-Wolfe algorithm (Frank Algorithm 1 Frank-Wolfe Algorithm (Frank & Wolfe, 1956) Input: smooth convex f function with curvature C, x 1 2 P start vertex, LP P linear minimization oracle Output: x t points in P 1: for t = 1 to T 1 do 2: v t LP P (rf(x t )) 3: x t+1 (1 t )x t + t v t with t : = 2 t+2 4: end for & Wolfe, 1956) (also known as conditional gradient descent (Levitin & Polyak, 1966); see also (Jaggi, 2013) for an overview) and its online version (Hazan & Kale, 2012) due to their simplicity.
6	39	These methods eschew the projection step and rather use a linear optimization oracle to stay within the feasible region.
7	75	While convergence rates and regret bounds are often suboptimal, in many cases the gain due to only having to solve a single linear optimization problem over the feasible region in every iteration still leads to significant computational advantages (see e.g., (Hazan & Kale, 2012, Section 5)).
8	158	This led to conditional gradients algorithms being used for e.g., online optimization and more generally machine learning and the property that these algorithms naturally generate sparse distributions over the extreme points of the feasible region (sometimes also refereed to as atoms) is often helpful.
9	366	Further increasing the relevance of these methods, it was shown recently that conditional gradient methods can also achieve linear convergence (see e.g., (Garber & Hazan, 2013; Lacoste-Julien & Jaggi, 2015; Garber & Meshi, 2016)) as well as that the number of total gradient evaluations can be reduced while maintaining the optimal number of oracle calls as shown in (Lan & Zhou, 2014).
10	165	Oracle 1 Weak Separation Oracle LPsep P (c, x, ,K) Input: c 2 Rn linear objective, x 2 P point, K 1 accuracy, > 0 objective value; Output: Either (1) y 2 P vertex with c(x y) > /K, or (2) false: c(x z)  for all z 2 P .
11	146	Unfortunately, for complex feasible regions even solving the linear optimization problem might be time-consuming and as such the cost of solving the LP might be non-negligible.
12	181	This could be the case, e.g., when linear optimization over the feasible region is a hard problem or when solving largescale optimization problems or learning problems.
13	50	As such it is natural to ask the following questions: (i) Does the linear optimization oracle have to be called in every iteration?
14	188	(ii) Does one need approximately optimal solutions for convergence?
15	60	(iii) Can one reuse information across iteration?
16	79	We will answer these questions in this work, showing that (i) the LP oracle is not required to be called in every iteration, that (ii) much weaker guarantees are sufficient, and that (iii) we can reuse information.
17	115	To significantly reduce the cost of oracle calls while maintaining identical convergence rates up to small constant factors, we replace the linear optimization oracle by a (weak) separation oracle (see Oracle 1) which approximately solves a certain separation problem within a multiplicative factor and returns improving vertices (or atoms).
18	19	We stress that the weak separation oracle is significantly weaker than approximate minimization, which has been already considered in (Jaggi, 2013).
19	82	In fact, if the oracle returns an improving vertex then this vertex does not imply any guarantee in terms of solution quality with respect to the linear minimization problem.
20	29	It is this relaxation of the dual guarantees that will provide a significant speedup as we will see later.
21	89	At the same time, in case that the oracle returns false, we directly obtain a dual bound via convexity.
22	75	A (weak) separation oracle can be realized by a single call to a linear optimization oracle, however with two important differences.
23	27	It allows for caching and early termination: Previous solutions are cached, and first it is verified whether any of the cached solutions satisfy the oracleâ€™s separation condition.
24	108	The underlying linear optimization oracle has to be called, only when none of the cached solutions satisfy the condition, and the linear optimization can be stopped as soon as a satisfactory solution with respect to the separation condition is found.
26	78	We call this technique lazy optimization and we will demonstrate significant speedups in wall-clock performance (see e.g., Figure 1), while maintaining identical theoretical convergence rates.
29	59	1: if y 2 P cached with c(x y) > /K exists then 2: return y {Cache call} 3: else 4: compute y argmax x2P c(x) {LP call} 5: if c(x y) > /K then 6: return y and add y to cache 7: else 8: return false 9: end if 10: end if results demonstrating effectiveness of our approach via a significant reduction in wall-clock running time compared to their linear optimization counterparts.
31	66	The Frank-Wolfe algorithm was originally introduced in (Frank & Wolfe, 1956) (also known as conditional gradient descent (Levitin & Polyak, 1966) and has been intensely studied in particular in terms of achieving stronger convergence guarantees as well as affine-invariant versions.
34	65	This includes the linearly convergent variant via local linear optimization (Garber & Hazan, 2013) as well as the pairwise conditional gradient variant of (Garber & Meshi, 2016), which is especially efficient in terms of implementation.
43	22	A key component of the new oracle is the ability to cache and reuse old solutions, which accounts for the majority of the observed speed up.
48	57	Contribution The main technical contribution of this paper is a new approach, whereby instead of finding the optimal solution, the oracle is used only to find a good enough solution or a certificate that such a solution does not exist, both ensuring the desired convergence rate of the conditional gradient algorithms.
