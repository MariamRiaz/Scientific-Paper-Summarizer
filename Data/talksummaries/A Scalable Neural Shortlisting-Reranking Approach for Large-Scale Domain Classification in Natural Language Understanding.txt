0	90	Natural language understanding (NLU) is a core component of intelligent personal digital assistants (IPDAs) such as Amazon Alexa, Google Assistant, Apple Siri, and Microsoft Cortana (Sarikaya, 2017).
1	19	A well-established approach in current real-time systems is to classify an utterance into a domain, followed by domain-specific intent classification and slot sequence tagging (Tur and de Mori, 2011).
2	41	A domain is typically defined in terms of a specific application or a functionality such as weather, calendar and music, which narrows down the scope of NLU for a given utterance.
4	40	Traditional IPDAs cover only tens of domains that share a common schema.
7	11	Redefining the domain, intent and slot boundaries requires relabeling of the underlying data, which is very costly and time-consuming.
9	138	The difficulty of solving this problem at scale has led to stopgap solutions, such as requiring an utterance to explicitly mention a domain name and restricting the expression to be in a predefined form as in “Ask ALLRECIPES, how can I bake an apple pie?” However, such solutions lead to an unintuitive and unnatural way of conversing and create interaction friction for the end users.
11	84	There could be a number of candidate domains and even multiple overlapping recipe-related domains that could handle it.
12	10	In this paper, we propose efficient and scalable shortlisting-reranking neural models in two steps for effective large-scale domain classification in IPDAs.
38	11	(2) For each domain in the k-best list, we prepare a hypothesis per domain with additional contextual information, including domain-intent-slot semantic analysis, user preferences, and domain index of popularity and quality.
39	39	(3) A second ranker called Hypotheses Reranker (HypRank) performs a list-wise ranking of the k hypotheses to improve on the initial naive ranking and find the best hypothesis, thus domain, to handle the utterance.
40	36	Figure 1 illustrates the steps with an example utterance, “play michael jackson.” Based on character and word features, shortlister returns the kbest list in the order of CLASSIC MUSIC, POP MUSIC, and VIDEO domains.
41	9	CLASSIC MUSIC outputs PlayTune intent, but without any slots, low domain popularity, and no usage history for the user, its ranking is adjusted to be last.
43	19	In our architecture, key focus is on efficiency and scalability.
44	16	Running full domain-intent-slot semantic analysis for thousands of domains imposes a significant computational burden in terms of memory footprint, latency and number of machines, and it is impractical in real-time systems.
45	4	For the same reason, this work only uses contextual information in the reranking stage, and the utility of including it in the shortlisting stage is left for future work.
51	8	The model parameters associated with this layer are: Char embedding: ec ∈ R25 for each c ∈ C Char LSTMs: φCf , φ C b : R25 × R25 → R25 Word embedding: ew ∈ R100 for each w ∈ W Let (w1, .
54	10	1 vi = f C |wi| ⊕ b C 1 ⊕ ewi BiLSTM layer We utilize a BiLSTM to encode the word vector sequence (v1, .
55	17	The BiLSTM outputs are generated as: fWi = φ W f ( vi, f W i−1 ) ∀i = 1 .
63	10	While softmaxa tends to highlight only the groundtruth domain while suppressing all the rest, softmaxb is designed to produce a more balanced confidence score per domain independent of other domains.
68	21	In our problem context, a hypothesis is formed per domain with additional semantic and contextual information, and selecting the highest-scored hypothesis means selecting the domain represented in that hypothesis for final domain classification.
71	8	encoded cross-hypothesis information at the feature level (Robichaud et al., 2014; Crook et al., 2015; Khan et al., 2015), our approach is to let a BiLSTM layer automatically capture that information and learn appropriate representations at the model level.
74	18	NLU interpretation Each domain has three corresponding NLU models for binary domain classification, multi-class intent classification, and sequence tagging for slots.
89	6	, pk) be the sequence of k input hypothesis vectors that are sorted in decreasing order of Shortlister scores.
96	44	, k} , where σ indicates scaled exponential linear unit (SeLU) for normalized activation outputs (Klambauer et al., 2017); the outputs of all the hypotheses are generated by using the same parameter set {W1, b1,W2, b2} for consistency regardless of the hypothesis order.
101	32	We evaluated our shortlisting-reranking approach in two different settings of traditional small-scale IPDA and large-scale IPDA for comparison: Traditional IPDA For this setting, we simulated the traditional small-scale IPDA with only 20 domains that are commonly present in any IPDAs.
106	20	The dataset comprises of more than 6M utterances having strict invocation patterns.
109	24	Hypotheses Reranker We also evaluate different variations of the reranking model for comparison.
111	60	• LR: LR point-wise: A binary logistic regression model with the hypothesis vector as features (see Section 5.1).
112	207	We run it for each hypothesis made from Shortlister’s k-best list and select the highest-scoring one, hence the Traditional IPDA Large-Scale IPDA smxa smxb smxa smxb 1-best 95.58 95.56 81.38 81.49 3-best 98.45 98.43 92.53 92.81 5-best 98.81 98.77 95.77 95.93
113	29	Table 2 shows the distribution of the training, development and test sets for each setting of traditional and large-scale IPDAs.
