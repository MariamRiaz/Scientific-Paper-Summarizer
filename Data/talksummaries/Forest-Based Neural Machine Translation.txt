3	47	They can use more syntactic information (Li et al., 2017), and can conveniently incorporate prior knowledge (Zhang et al., 2017).
4	31	Because of these advantages, tree-based methods ∗ Contribution during internship at National Institute of Information and Communications Technology.
9	43	In spite of impressive performance of tree-based NMT systems, they suffer from a major drawback: they only use the 1-best parse tree to direct the translation, which potentially introduces translation mistakes due to parsing errors (Quirk and Corston-Oliver, 2006).
20	38	, xT ), the sequence-to-sequence model uses one RNN to encode the source sequence into a fixed-length context vector c and another RNN to decode this vector and generate the target sequence.
27	74	Regarding the linearization adopted for tree-tostring NMT (i.e., linearization of the source side), Sennrich and Haddow (2016) encoded the sequence of dependency labels and the sequence of words simultaneously, partially utilizing the syntax information, while Li et al. (2017) traversed the constituent tree of the source sentence and combined this with the word sequence, utilizing the syntax information completely.
28	30	Regarding the linearization used for string-totree NMT (i.e., linearization of the target side), Nadejde et al. (2017) used a CCG supertag sequence as the target sequence, while Aharoni and Goldberg (2017) applied a linearization method in a top-down manner, generating a sequence ensemble for the annotated tree in the Penn Treebank (Marcus et al., 1993).
29	21	Wu et al. (2017) used transition actions to linearize a dependency tree, and employed the sequence-to-sequence framework for NMT.
31	37	In contrast, we hope to utilize multiple trees (i.e., a forest).
34	27	Figure 1a shows a packed forest, which can be unpacked into two constituent trees (Figure 1b and Figure 1c).
48	54	Algorithm 1 Linearization of a packed forest 1: function LINEARIZEFOREST(〈V,E〉,w) 2: v ← FINDROOT(V ) 3: r← [] 4: EXPANDSEQ(v, r, 〈V,E〉,w) 5: return r 6: function FINDROOT(V ) 7: for v ∈ V do 8: if v has no parent then 9: return v 10: procedure EXPANDSEQ(v, r, 〈V,E〉,w) 11: for e ∈ E do 12: if head(e) = v then 13: if tails(e) 6= ∅ then 14: for t ∈ SORT(tails(e)) do .
53	42	In particular, a topological ordering could ignore “word sequential information” and “parentchild information.” For example, for the packed forest in Figure 1a, although “[10]→[1]→[2]→ · · · →[9]→[11]” is a valid topological ordering, the word sequential information of the words (e.g., “John” should be located ahead of the period), which is fairly crucial for translation of languages with fixed pragmatic word order such as Chinese or English, is lost.
58	43	The algorithm linearizes the packed forest from the root node (Line 2) to leaf nodes by calling the EXPANDSEQ procedure (Line 15) recursively, while preserving the word order in the sentence (Line 14).
61	27	In this way, parent-child information is preserved.
71	38	For example, “NP⊗John” (linearization result of node [2]) is followed by “ c©NNP⊗John” (linearization result of node [1], the child of node [2]).
72	29	Note that our linearization method does not output fully recoverable packed forests.
82	20	Second, the symbol sequence consists of three types of symbols: words, constituent labels, and operators ( c©, ⊗, ⊕, or ) that connect the other two types of symbols.
84	43	Formally, the input layer receives two sequences: the symbol sequence l = (l0, .
100	39	After calculating the embedding vectors in the embedding layer, the hidden vectors are calculated using Equation (5).
120	34	We compared our proposed models (i.e., Forest (SoE) and Forest (SoA)) with three types of baseline: a string-tostring model (s2s), forest-based models that do not use score sequences (Forest (No score)), and treebased models that use the 1-best parsing tree (1- best (No score, SoE, SoA)).
132	58	With this information, the NMT system succeeded to learn a better attention, paying more attention to the confident structure and less attention to the unconfident structure, which improved the translation performance.
135	26	Second, compared with the cases that only use the 1-best constituent trees, with some exceptions, using packed forests yielded statistical significantly better results for the SoE and SoA frameworks.
136	24	This shows the effectiveness of using more syntactic information.
137	30	Compared with one constituent tree, the packed forest, which contains multiple different trees, describes the syntactic structure of the sentence in different aspects, which together increase the accuracy of machine translation.
138	44	However, without using the scores, the 1-best constituent tree is preferred.
139	58	This is because without using the scores, all trees in the packed forest are treated equally, which makes it easy to import noise into the encoder.
140	108	Compared with other types of state-of-the-art systems, our systems using only the 1-best tree (1- best (SoE, SoA)) were better than the other treebased systems.
142	64	These results also support the usefulness of the scores of the edges and packed forests in NMT.
143	62	As for the efficiency, the training time of the SoA system was slightly longer than that of the SoE system, which was about twice of the s2s baseline.
144	60	The training time of the tree-based system was about 1.5 times of the baseline.
146	111	The reason for the relatively low efficiency is that the linearized sequences of packed forests were much longer than word sequences, enlarging the scale of the inputs.
