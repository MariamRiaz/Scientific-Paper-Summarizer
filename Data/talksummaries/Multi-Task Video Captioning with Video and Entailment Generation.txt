0	27	Video captioning is the task of automatically generating a natural language description of the content of a video, as shown in Fig.
4	16	It is also a step forward from static image captioning, because in addition to modeling the spatial visual features, the model also needs to learn the temporal across-frame action dynamics and the logical storyline language dynamics.
5	13	Previous work in video captioning (Venugopalan et al., 2015a; Pan et al., 2016b) has shown that recurrent neural networks (RNNs) are a good choice for modeling the temporal information in the video.
10	41	Despite these recent improvements, video captioning models still suffer from the lack of sufficient temporal and logical supervision to be able to correctly capture the action sequence and storydynamic language in videos, esp.
12	53	We address this by jointly training the task of video captioning with two related directed-generation tasks: a temporally- 1273 directed unsupervised video prediction task and a logically-directed language entailment generation task.
13	33	We model this via many-to-many multi-task learning based sequence-to-sequence models (Luong et al., 2016) that allow the sharing of parameters among the encoders and decoders across the three different tasks, with additional shareable attention mechanisms.
14	22	The unsupervised video prediction task, i.e., video-to-video generation (adapted from Srivastava et al. (2015)), shares its encoder with the video captioning task’s encoder, and helps it learn richer video representations that can predict their temporal context and action sequence.
15	24	The entailment generation task, i.e., premise-to-entailment generation (based on the image caption domain SNLI corpus (Bowman et al., 2015)), shares its decoder with the video captioning decoder, and helps it learn better video-entailing caption representations, since the caption is essentially an entailment of the video, i.e., it describes subsets of objects and events that are logically implied by or follow from the full video content).
16	27	The overall many-tomany multi-task model combines all three tasks.
48	16	The RNN is based on Long Short Term Memory (LSTM) units, which are good at memorizing long sequences due to forget-style gates (Hochreiter and Schmidhuber, 1997).
49	14	For video captioning, our input to the encoder is the video frame features1 {f1, f2, ..., fn} of length n, and the caption word sequence {w1, w2, ..., wm} of length m is generated during the decoding phase.
50	18	The distribution of the output sequence w.r.t.
51	14	the input sequence is: p(w1, ..., wm|f1, ..., fn) = m∏ t=1 p(wt|hdt ) (1) where hdt is the hidden state at the t th time step of the decoder RNN, obtained from hdt−1 and wt−1 via the standard LSTM-RNN equations.
53	31	Our attention model architecture is similar to Bahdanau et al. (2015), with a bidirectional LSTMRNN as the encoder and a unidirectional LSTMRNN as the decoder, see Fig.
61	35	The motivation is that this helps the video encoder learn rich temporal representations that are aware of their action-based context and are also robust to missing frames and varying frame lengths or motion speeds.
64	16	Given a sentence (premise), the task of entailment generation is to generate a sentence (hypothesis) which is a logical deduction or implication of the premise.
69	15	Our primary aim is to improve the video captioning model, where visual content translates to a textual form in a directed (entailed) generation way.
72	49	Here, the video captioning task shares its video encoder (parameters) with the encoder of the video prediction task (one-to-many setting) so as to learn context-aware and temporally-directed visual representations (see Sec.
108	254	See subsections below and supp for full details.
109	27	Table 1 presents our primary results on the YouTube2Text (MSVD) dataset, reporting several previous works, all our baselines and attention model ablations, and our three multi-task models, using the four automated evaluation metrics.
121	16	all previous works, demonstrating the effectiveness of multi-task learning for video captioning with video prediction, even with unsupervised signals.
124	22	Again, Table 1 shows statistically significant improvements5 in all the metrics in comparison to the best baseline model (and all previous works) under this multi-task setting.
130	14	In Table 2, we also train and evaluate our final many-to-many multi-task model on two other video captioning datasets (using their standard splits; details in supplementary).
136	37	Above, we showed that the new entailment generation task helps improve video captioning.
137	75	Next, we show that the video captioning task also inversely helps the entailment generation task.
139	17	We use only the entailment pairs subset of the SNLI corpus for this, but with a multi-reference split setup to allow automatic metric evaluation and a zero traintest premise overlap (see Sec.
148	100	5 shows video captioning generation results on the YouTube2Text dataset where our final M-to-M multi-task model is compared with our strongest attention-based baseline model for three categories of videos: (a) complex examples where the multi-task model performs better than the baseline; (b) ambiguous examples (i.e., ground truth itself confusing) where multi-task model still correctly predicts one of the possible categories (c) complex examples where both models perform poorly.
151	15	On analyzing the cases where the baseline is better than the final M-to-M multi-task model, we find that these are often scenarios where the multitask model’s caption is also correct but the baseline caption is a bit more specific, e.g., “a man is holding a gun” vs “a man is shooting a gun”.
156	27	We also show mutual multi-task improvements on the new entailment generation task.
157	21	In future work, we are applying our entailment-based multi-task paradigm to other directed language generation tasks such as image captioning and document summarization.
