{"sections": [{"heading": "1. Introduction", "text": "Change detection, namely the problem of analyzing a data stream to detect changes in the data-generating distribution, is very relevant in machine-learning and is typically addressed in an unsupervised manner. This approach is generally dictated by many practical aspects, which include the unpredictability of the change and the fact that the training set often contains only stationary data. As a matter of fact,\n1Dipartimento di Elettronica, Informazione e Bioingegneria, Politecnico di Milano, Milan, Italy. 2Institute of Intelligent Systems for Automation, National Research Council, Genova, Italy. Correspondence to: Diego Carrera <diego.carrera@polimi.it>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nmost change-detection tests in the literature (Basseville & Nikiforov, 1993; Lung-Yut-Fong et al., 2011; Ross et al., 2011; Kuncheva, 2013) consist of three major ingredients: i) a model describing the distribution of stationary data, \u03c60, that is typically learned from a training set, ii) a test statistic T used to assess the conformance of test data with the learned model, and iii) a decision rule that monitors T to detect changes in \u03c60. Needless to say, all these have to be wisely designed and combined to yield a sound test that can provide prompt detections as well as a controlled False Positive Rate (FPR), which is one of the primary concerns in change detection. Unfortunately, when it comes to monitoring multivariate data, it is difficult to find good density models and test statistics that do not depend on \u03c60: this represents a severe limitation for real-world monitoring problems, where the stream distribution is unknown. Our work presents an efficient change-detection test for multivariate data that overcomes this limitation.\nThe first change-detection tests were developed to monitor univariate data streams in the statistical process control literature (Basseville & Nikiforov, 1993). In classification problems, changes in the data stream are known as concept drift (Gama et al., 2014) and are detected by monitoring the sequence of classification errors on supervised data (Harel et al., 2014; Alippi et al., 2013; Bifet & Gavalda, 2007). Many change-detection tests are parametric, i.e., they assume that \u03c60 belongs to a known family, e.g., (Page, 1954), or are based on ad-hoc statistics that detect specific changes, e.g., the Hotelling statistic (Lehmann & Romano, 2006). Most nonparametric statistics are instead based on ranking, e.g., the Kolmogorov-Smirnov (Ross & Adams, 2012) and Lepage (Ross et al., 2011) statistics, and can be applied exclusively to univariate data.\nThere exist a few multivariate tests able to detect any distribution change (Lung-Yut-Fong et al., 2011; Justel et al., 1997). Two popular approaches consists either in reducing the data dimension by PCA (Kuncheva, 2013; Qahtan et al., 2015) or computing the likelihood with respect to a model fitted on a training set, e.g., a Gaussian mixture (Kuncheva, 2013; Alippi et al., 2016), a Gaussian process (Saatc\u0327i et al., 2010) or a kernel density estimator (Krempl, 2011). In the latter case the change-detection problem boils down to monitoring a univariate stream. Unfortunately, in these cases, T often depends on \u03c60, and detection rules be-\ncome heuristic in nature (Kuncheva, 2013; Ditzler & Polikar, 2011) preventing a proper control over the FPR. Histograms, which are perhaps the most natural candidates for describing densities, enable a different form of monitoring that is based on a comparison among distributions (Ditzler & Polikar, 2011; Boracchi et al., 2017). However, they are often implemented over regular grids and require a number of bins that grows exponentially with the data dimension. Only a few change-detection solutions (Dasu et al., 2006; Boracchi et al., 2017) adopt alternative partitioning schemes that scale well in high dimensions. In particular, kqd-trees (Dasu et al., 2006) were introduced as a variant of kd-trees (Bentley, 1975) to guarantee that all the leaves contain a minimum number of training samples and have a minimum size. In (Boracchi et al., 2017) it is shown that histograms built on uniform-density partitions rather than regular grids provide superior detection performance.\nOur main contribution is QuantTree, a recursive binary splitting scheme that defines histograms for changedetection purposes. The most prominent advantage of using QuantTree is that the distribution of any statistic defined over the resulting histograms does not depend on \u03c60. This implies that decision rules to be used in multivariate change-detection problems do not depend on the data, and can be numerically computed from synthetically generated univariate sequences. Moreover, histograms defined by QuantTree can have a pre-assigned number of bins and can be represented as a tree, thus enabling a very efficient computation of test statistics.\nQuantTree (Section 3) iteratively divides the input space by means of binary splits on a single covariate, where the cutting points are defined by the quantiles of the marginal distributions. This splitting strategy is similar to the one adopted by kd-trees (Bentley, 1975), where the split is performed w.r.t. the median value of the marginal. Such a simple construction scheme can be handled analytically, as it is possible to prove (Section 4) that the distribution of each bin probability does not depend on \u03c60. Our experiments (Section 5) show that QuantTree enables good detection performance in high dimensional streams. Moreover, when testing few samples, QuantTree guarantees a better FPR control than the Pearson goodness-of-fit test and tests based on empirical thresholds computed through bootstrap. We also show that histograms constructed with a few bins gathering the same density under \u03c60 achieve higher power than monitoring schemes based on different histograms."}, {"heading": "2. Problem Formulation", "text": "Before the change, namely in stationary conditions, data in the monitored stream x \u2208 Rd are independent and identically distributed (i.i.d.) realizations of a continuous random vector X0 having an unknown probability density function\n(pdf) \u03c60, whose support is X \u2286 Rd. We assume that a training set TR = {xi \u2208 X , i = 1, . . . , N} containing N stationary data (i.e., xi \u223c \u03c60) is provided.\nHistograms: we define a histogram as:\nh = {(Sk, \u03c0\u0302k)}k=1,...,K , (1) where the K subsets Sk \u2286 X form a partition of Rd, i.e.,\u22c3K k=1 Sk = Rd and Sj \u2229 Si = \u2205, for j 6= i, and each \u03c0\u0302k \u2208 [0, 1] corresponds to the probability for data generated from \u03c60 to fall inside Sk. Both the subsets {Sk}k and probabilities {\u03c0\u0302k}k can be adaptively defined from training data TR, and in particular \u03c0\u0302k is typically estimated as \u03c0\u0302k = Lk/N , i.e. the number of training samples LK belonging to Sk over the number of points in TR.\nBatch-wise monitoring: for the sake of simplicity, we analyze the incoming data in batches W = {x1, . . . ,x\u03bd} of \u03bd samples. We detect changes by an hypothesis test (HT) which assesses whether data in W are consistent with a reference histogram h learned from TR. In particular, this hypothesis test can be stated as follows:\nH0 :W \u223c \u03c60 vs H1 :W \u223c \u03c61 6= \u03c60 (2) where \u03c61 represents the unknown post-change distribution. We focus on HTs that are based on a test statistic Th defined over the histogram h, like for instance the Pearson statistic (Lehmann & Romano, 2006). Thus, Th uniquely depends on {yk}k=1,...,K , where yk denotes the number of samples in W falling in Sk. We detect a change in the incoming W when\nTh(W ) = Th(y1, . . . , yK) > \u03c4, (3) where \u03c4 \u2208 R is a threshold that controls the FPR, namely the proportion of type I errors (Lehmann & Romano, 2006).\nGoal: our goal is two-fold, i) learn a histogram h from TR to be used for change-detection purposes and ii) for each given test statistic Th and reference FPR value \u03b1, define a threshold \u03c4 such that\nP\u03c60(Th(W ) > \u03c4) \u2264 \u03b1, (4) where P\u03c60 denotes the probability under the null hypothesis that W contains samples generated from \u03c60.\nThere are two important comments. First, while (3) might seem an oversimplified monitoring scheme, this is enough to demonstrate that when histograms are built through QuantTree, the monitoring can be performed independently of \u03c60. As a consequence, test statistics Th can be potentially employed in sequential monitoring schemes like (Ross & Adams, 2012). Second, we focus on generalpurpose tests, which are able to detect any distribution change \u03c60 \u2192 \u03c61 as well as on histograms that can model densities in high dimensions, i.e., d 1.\nAlgorithm 1 QuantTree Input: Training set TR containing N stationary points in X ; number of bins K; target probabilities {\u03c0k}k. Output: The histogram h = {(Sk, \u03c0\u0302k)}k. 1: Set N0 = N , L0 = 0. 2: for k = 1, . . . ,K do 3: Set Nk = Nk\u22121 \u2212 Lk\u22121, Xk = X \\ \u22c3 j<k Sj , and\nLk = round(\u03c0kN). 4: Choose a random component i \u2208 {1, . . . , d}. 5: Define zn = [xn]i for each xn \u2208 Xk. 6: Sort {zn}: z(1) \u2264 z(2) \u2264 . . . z(Nk). 7: Draw \u03b3 \u2208 {0, 1} from a Bernoulli(0.5). 8: if \u03b3 = 0 then 9: Define Sk = {x \u2208 Xk [x]i \u2264 z(Lk)}.\n10: else 11: Define Sk = {x \u2208 Xk [x]i \u2265 z(Nk\u2212Lk+1)}. 12: end if 13: Set \u03c0\u0302k = Lk/N . 14: end for"}, {"heading": "3. The QuantTree Algorithm", "text": "Here we describe QuantTree1, an algorithm to define histograms h through a recursive binary splitting of the input space X . This algorithm takes as input a training set TR containing N stationary points, the number of bins K in the histogram, and the target probabilities on each bin {\u03c0k}k=1,...,K , and returns a histogram h = {(Sk, \u03c0\u0302k)}k=1,...,K , where each \u03c0\u0302k represents an estimate of the probability for a sample drawn from \u03c60 to fall in Sk.\nAlgorithm 1 presents in detail the iterative formulation of QuantTree, which constructs a new bin of h at each step k. We denote by Xk \u2286 X the subset of the input space that still has to be partitioned (i.e., Xk = X \\ \u22c3 j<k Sk) and by Nk the number of points of TR belonging to Xk. We compute (line 3) the number of training points that has to fall inside Sk as Lk = round(\u03c0kN). The subset Sk is then defined by splitting Xk along a component i \u2208 {1, . . . , d} that is randomly chosen with uniform probability (line 4). The splitting point is defined by sorting zn = [xn]i, i.e., the values of the i-th component for each xn \u2208 Xk (lines 5). We thus obtain z(1) \u2264 z(2) \u2264 \u00b7 \u00b7 \u00b7 \u2264 z(Nk) (line 6) and we define Sk by splitting Xk w.r.t. z(Lk) or z(Nk\u2212Lk+1) (lines 7-11). In both cases Sk contains Lk points among the N in X , thus the estimated probability of Sk is \u03c0\u0302k = Lk/N (line 13). This procedure is iterated until K subsets are extracted.\nQuantTree divides X in a given number of subsets, where each Sk has an estimated probability \u03c0\u0302k ' \u03c0k, and the\n1The implementation of QuantTree is available at http:// home.deib.polimi.it/boracchi/Projects\nAlgorithm 2 Numerical procedure to compute thresholds Input: Test statistic Th; arbitrarily chosen \u03c80; the number\nB of datasets and batches to compute the threshold; the number of points \u03bd in each batch; N ,K, and \u03c0\u0302k as in Algorithm 1; the desired FPR \u03b1. Output: The value \u03c4 of the threshold 1: for b = 1, . . . , B do 2: Draw from \u03c80 a training set TRb of N samples. 3: Use QuantTree to compute the histogram hb with K bins and target probabilities {\u03c0k}k over TR.\n4: Draw a batch Wb containing \u03bd points from \u03c60. 5: Compute the value tb = Th(W ). 6: end for 7: Compute the threshold \u03c4 as in (5).\nequality holds when \u03c0kN is integer. Since the probabilities \u03c0k are set a priori, in what follows we use \u03c0k in place of \u03c0\u0302k. Indexes i and parameter \u03b3 are randomly chosen to add variability to the histogram construction. Figure 1(a) shows a tree obtained from a bivariate Gaussian training set, defined by K = 4 bins, each having probability \u03c0k = N/4."}, {"heading": "3.1. Computation of Distribution-Free Test Statistics", "text": "A key feature of a histogram computed by QuantTree is that any statistic Th built over it has a distribution that is independent from \u03c60. This result follows from Theorem 1, that is proved in Section 4.\nTheorem 1. Let Th(\u00b7) be defined as in (3) over the histogram h computed by QuantTree. When W \u223c \u03c60, the distribution of Th(W ) depends only on \u03bd, N and {\u03c0k}k.\nTheorem 1 implies that we can numerically compute the thresholds for any statistic Th defined on histograms, provided \u03bd, N and {\u03c0k}, thus disregarding \u03c60 and the data dimension d. To this end, we synthetically generate data from a conveniently chosen distribution \u03c80, and we follow the procedure outlined in Algorithm 2 to estimate the threshold \u03c4 for HT in (2) yielding a desired FPR \u03b1. At first we generate B training sets {TRb}b=1,...,B , sampling N points from \u03c80 and, for each training set, we build a histogram hb using QuantTree (lines 2-3). Then, for each hb we generate a batch Wb of \u03bd points drawn from \u03c80, and compute the value of the statistic tb = Th(Wb) (lines 4-5). Finally, we estimate \u03c4 (line 7) from the set TB = {t1, . . . , tB} as the 1 \u2212 \u03b1 quantile of the empirical distribution of Th over the generated batches, i.e.\n\u03c4 = min { t \u2208 TB : #{v \u2208 TB : v > t} \u2264 \u03b1B } , (5)\nwhere #A denotes the cardinality of a set A.\nTo take full advantage of the distribution-free nature of the procedure, we set \u03c80 to a univariate uniform distribution\nU(0, 1). This allows to obtain high accuracy on the estimation of the thresholds, since we can use very large values of B with limited computational cost."}, {"heading": "3.2. Considered Statistics", "text": "We consider two meaningful examples of statistics Th that can be employed for batch-wise monitoring through histograms: the Pearson statistic and the total variation (Lehmann & Romano, 2006). The Pearson statistic is defined as\nT Ph (W ) = K\u2211 k=1 (yk \u2212 \u03bd\u03c0k)2 \u03bd\u03c0k , (6)\nwhile the total variation is defined as\nT TVh (W ) = 1\n2 K\u2211 k=1 |yk \u2212 \u03bd\u03c0k| . (7)\nIt is well known that, when {\u03c0k}k are the true probabilities of the bins {Sk}k, under the null hypothesis the statistic T Ph (W ) is asymptotically distributed as a \u03c72K\u22121. However, when the \u03c0k are estimated, the threshold obtained from the \u03c72K\u22121 distribution does not allow to properly control the FPR, and this effect is more evident when yk is small. In contrast, thresholds defined by Algorithm 2 hold also in case of limited sample size, since they are not based on an asymptotic result.\nThese two statistics will be used for our experiments in Section 5, using thresholds reported in Table 1 for different values of N , K, \u03bd and choosing \u03c0k = 1/K, k = 1, . . . ,K. These values have been computed applying the procedure described in Algorithm 2 with B = 2.5 \u00b7 106. We note that both statistics T Ph and T TVh assume only discrete values, therefore it is not always possible to set the threshold \u03c4 yielding the FPR exactly equal to \u03b1, but only to ensure that the FPR does not exceed \u03b1."}, {"heading": "3.3. Computational Remarks", "text": "We remark that since the histogram h computed by QuantTree is exclusively defined on the marginal probabilities of single components, the dimensionality of the input data d does not impact the overall computational cost. In fact, the computational cost of building a QuantTree is dominated by sorting the covariates (Algorithm 1 line 6), which is performed K times on an progressively smaller number of samples at each iteration. Therefore, the overall complexity of constructing a QuantTree is O(KN logN). In case of univariate distribution (i.e., d = 1), the complexity is reduced toO(N logN), since the partition {Sk}k can be defined through a single sorting operation.\nSince any histogram h computed by QuantTree can be represented as a tree structure, it is very efficient to identify\nthe bin where any testing point belongs to. In fact, during monitoring, at most K IF-THEN operations (that reduces to logK when d = 1) have to be performed for each input sample x. Moreover, in contrast with histograms based on regular grids, the number of binsK is here a priori defined, and does not need to grow exponentially with d."}, {"heading": "4. Theoretical Analysis", "text": "We prove Theorem 1 showing that the distribution of any test statistic Th defined over an histogram h computed by QuantTree does not depend on \u03c60. To this end, we first prove some preliminary propositions to characterize the distribution of the true probability of each bin Sk under \u03c60:\npk = P\u03c60(Sk), (8)\nwhich is also a random variable as it depends on the training data TR.\nFor the sake of simplicity, we assume that QuantTree always splits with respect to the left tail, i.e., \u03b3 = 0 in line 8 of Algorithm 1 (proofs hold when \u03b3 \u223c Bernoulli(0.5)) and, to simplify the notation, we will omit the subscript \u03c60 from P\u03c60 , thus P denotes the probability computed w.r.t. \u03c60. The following proposition will be used to derive the distributions of pk.\nProposition 1. Let x1, . . . ,xM be i.i.d. realizations of a continuous random vector X defined over D \u2286 Rd. Let us define the i-th component of x as z = [x]i, and denote with z(1) \u2264 z(2) \u2264 \u00b7 \u00b7 \u00b7 \u2264 z(M) the M sorted components of x1, . . . ,xM . For any L \u2208 {1, . . . ,M} we define the set\nQi,L := {x \u2208 D : [x]i \u2264 z(L)}. (9) Then, for each i \u2208 {1, . . . , d}, the random variable p = PX(Qi,L) is distributed as a Beta(L,M \u2212 L+ 1).\nProof. The proof consists of showing that p is an order statistic of the uniform distribution, which in turns follows a Beta distribution. For this purpose, we consider X defined over Rd and PX(Rd\\D) = 0, thus p can be expressed as\np = PX(Qi,L) = PX(x \u2208 Rd : [x]i \u2264 z(L)) = = PZ(z \u2208 R : z \u2264 z(L)), (10)\nwhere PZ denotes the marginal probability of Z = [X]i, namely the marginal of X w.r.t. the component i. We denote with FZ the cumulative distribution of Z and define U = F\u22121Z (Z) and un = F \u22121 Z (zn), n = 1, . . . ,M , where F\u22121Z (z) = inf{t \u2208 R : FZ(t) > z}. (11) The function F\u22121Z (\u00b7) is monotonically nondecreasing, thus it preserves the order and the L-th sorted value of {un} can be computed as u(L) = F \u22121 Z (z(L)). Then, (10) becomes\np = PZ(z \u2208 R : z \u2264 z(L)) = (12) = PU (u \u2208 [0, 1] : u \u2264 u(L)) = FU (u(L)) = u(L).\nSince U follows a uniform distribution over [0, 1], it follows that p is the L-th order statistic of the uniform distribution, that is a distributed as a Beta(L,M \u2212 L + 1) (Balakrishnan & Rao, 1998).\nThus, p1 in (8), namely the probability of S1 under \u03c60, is distributed as a Beta(L1, N \u2212 L1 + 1). To derive the distribution of the remaining pk, k \u2265 2, we define the conditional probability\nPS1(x \u2208 A) = P\u03c60(x \u2208 A | x /\u2208 S1), (13) where A is any Borel subset of X . Then, from the definition of conditional probability and the fact that x1, . . . ,xN are i.i.d. according to \u03c60, it can be easily proved that the N \u2212 L1 points that do not belong to S1 are i.i.d. according to PS1 . Therefore, we can apply Proposition 1 to the subset of the N \u2212 L1 points that do not fall in S1 by setting D = Rd \\ S1 and considering PS1 in place of PX. Thus, the random variable p\u03032 = PS1(S2) is distributed as Beta(L2, N2 \u2212 L2, 1), where N2 = N \u2212N1. Iterating the above procedure, we obtain that all the random variables p\u0303k, k = 1, . . . ,K, defined as2\np\u0303k = P\u22c3k\u22121 j=1 Sj (Sk), (14)\nare distributed as Beta(Lk, Nk \u2212 Lk + 1), where Nk = N \u2212 \u2211k\u22121 j=1 Nj .\nWe remark the different roles of pk and p\u0303k. While pk in 2We adopt the following conventions: an empty union of sets is the empty set, an empty sum is zero, and an empty product is 1.\n(8) the measure of the bin Sk under \u03c60, p\u0303k in (14) is the ratio between pk and the measure under \u03c60 of Xk = Rd \\\u22c3k\u22121 j Sj , namely the space that remains to be partitioned at step k. As an example, for a tree with K = 3 leaves, if we set target probabilities \u03c01 = \u03c02 = \u03c03 = 1/3, we obtain p\u03031 = 1/3, p\u03032 = 1/2 and p\u03033 = 1. To prove Theorem 1 we need to derive the distribution of pk, that are expressed in terms of p\u0303k by the following proposition.\nProposition 2. In case of histograms defined by QuantTree, the following relation holds between pk and p\u0303k:\npk = p\u0303k \u00b7 (1\u2212 k\u22121\u2211 j=1 pj) = p\u0303k k\u22121\u220f j=1 (1\u2212 p\u0303j). (15)\nProof. From the law of total probability we have that\npk = P\u03c60(x \u2208 Sk) = = P\u03c60 ( x \u2208 Sk | x /\u2208 \u222ak\u22121j=1Sj ) \u00b7 P\u03c60 ( x /\u2208 \u222ak\u22121j=1Sj ) +\n+ P\u03c60 ( x \u2208 Sk | x \u2208 \u222ak\u22121j=1Sj ) \u00b7 P\u03c60 ( x \u2208 \u222ak\u22121j=1Sj ) .\n(16) Since sets {Sk} defined by QuantTree are disjoint, it follows that Sk and \u22c3k\u22121 j=1 Sj are also disjoint, thus the second term in the sum in (16) is equal to 0. The first equality in (15) follows from the definition of p\u0303k = P\u03c60(x \u2208 Sk | x /\u2208 \u22c3k\u22121 j=1 Sj) and the fact that P\u03c60(x /\u2208 \u22c3k\u22121 j=1 Sj) =\n1 \u2212 \u2211k\u22121 j=1 pj . The second equality in (15) can be proved by induction over j.\nThe following proposition allows us to express pj as a product of independent Beta distributions.\nProposition 3. The random variables p\u0303k defined over histograms computed by QuantTree are independent.\nProof. To prove the independence of the p\u0303k, k = 1, . . . ,K, we show that p\u0303k is independent from p\u0303j , j = 1, . . . , k \u2212 1. In particular, we prove that\nP\u03c60(p\u0303k \u2264 tk | p\u0303j = tj , j = 1, . . . , k\u22121) = P\u03c60(p\u0303k \u2264 tk). (17) To this end, we follow the proof of Proposition 1, and express p\u0303k as an order statistic of the uniform distribution.\nAt iteration k, QuantTree randomly selects a dimension ik and performs a split w.r.t. the Lk-th order statistic of the ik components over the remaining Nk points (line 9 of Algorithm 1). Let L\u0303k be the position of this splitting point in {zn = [xn]ik , n = 1, . . . , N}, namely the sequence of ordered ik components of all the points in TR. The value of L\u0303k \u2208 N depends on realizations x1, . . . ,xN , and is a random variable ranging in {Lk, . . . ,Mk}, where Mk = \u2211k j=1 Lj . Obviously, at the first iteration L1 = L\u03031 but then the two may differ, as shown in Figure 1. Let us now consider the splitting point with respect to L\u0303k, i.e., z(L\u0303k). From the definition of p\u0303k we have that\np\u0303k = P\u22c3k\u22121 j=1 Sj (Sk) = P\u22c3k\u22121 j=1 Sj (z \u2264 z(L\u0303k)). (18)\nAs in the proof of Proposition 1, we denote with FZ the cdf of Z = [X]ik , and define U = F \u22121 Z (Z), that has a uniform distribution on [0, 1]. Therefore it holds that\np\u0303k = P\u22c3k\u22121 j=1 Sj (z \u2264 z(L\u0303k)) = P\u22c3k\u22121j=1 Sj (u \u2264 u(L\u0303k)) = = FU (u(L\u0303k)) = u(L\u0303k). (19)\nWe use the law of total probability w.r.t. the events {L\u0303k = a}, a \u2208 {Lk, . . . ,Mk}, to decompose the left hand side in (17) as (for simpler notation we omit the expression j = 1, . . . , k \u2212 1 in what follows): P\u03c60(p\u0303k \u2264 tk | p\u0303j = tj) = P\u03c60(u(L\u0303k) \u2264 tk | p\u0303j = tj) =\n= Mk\u2211 a=Lk P\u03c60(u(L\u0303k) \u2264 tk | L\u0303k = a, p\u0303j = tj) \u00b7 P\u03c60(L\u0303k = a)\n= Mk\u2211 a=Lk P\u03c60(u(a) \u2264 tk | p\u0303j = tj) \u00b7 P\u03c60(L\u0303k = a). (20)\nSince the distribution of u(a) does not depend on p\u0303j , we have that P\u03c60(u(a) \u2264 tk | p\u0303j = tj) = P\u03c60(u(a) \u2264 tk), therefore it follows\nP\u03c60(p\u0303k \u2264 tk | p\u0303j = tj) =\n= Mk\u2211 a=Lk P\u03c60(u(a) \u2264 tk) \u00b7 P\u03c60(L\u0303k = a)\n= Mk\u2211 a=Lk P\u03c60(u(L\u0303k) \u2264 tk | L\u0303k = a) \u00b7 P\u03c60(L\u0303k = a) =\n= P\u03c60(u(L\u0303k) \u2264 tk) = P\u03c60(p\u0303k \u2264 tk), (21) and (17) is proved.\nThe proof of Theorem 1 follows from Proposition 3.\nProof of Theorem 1. For any stationary distribution \u03c60, the random vector [y1, . . . , yK ] conditioned on p1, . . . , pK follows a Multinomial distribution with parameters (\u03bd, p1, . . . , pK) (White et al., 2009). From Proposition 3 each pk is a product of independent Beta distributions, thus depends only on {Lk} and it is independent from \u03c60.\nTherefore any statistic Th that is a function of {yk} depends only on \u03bd, and onN and {\u03c0k}which determines {Lk}."}, {"heading": "5. Experiments", "text": "We quantitatively assess the advantages of changedetection tests based on QuantTree w.r.t. other generalpurpose tests able to detect any distribution change \u03c60 \u2192 \u03c61. In particular, we show that: i) thresholds provided by Algorithm 2 can better control the FPR w.r.t. alternatives based on asymptotic results or bootstrap ii) HT based on histograms provided by QuantTree yielding a uniformdensity partition of Rd achieve higher power than other partitioning schemes."}, {"heading": "5.1. Datasets and Change Models", "text": "We employ both synthetic and real-world datasets: Synthetic datasets are generated by choosing, for each dimension d \u2208 {2, 8, 32, 64}, 250 pairs (\u03c60, \u03c61) of Gaussians, where \u03c60 has a randomly defined covariance, and \u03c61 = \u03c60(Q \u00b7 +v) is a roto-transalation of \u03c60 such that the symmetric Kullback-Leibler divergence sKL(\u03c60, \u03c61) = 1. We control sKL(\u03c60, \u03c61) by the CCM framework (Carrera & Boracchi, 2017), which guarantees all the changes to have the same magnitude. This is required when comparing detection performance in different dimensions.\nWe also employ four real-world high-dimensional sets: MiniBooNE particle identification (\u201cparticle\u201d, d = 50), Physicochemical Properties of Protein Tertiary Structure (\u201cprotein\u201d, d = 9), Sensorless Drive Diagnosis (\u201csensorless\u201d, d = 48) from the UCI Machine Learning Repository (Lichman, 2013), and Credit Card Fraud Detection (\u201ccredit\u201d, d = 29) from (Dal Pozzolo et al., 2015). We standardize these datasets and add to each component of the \u201cparticle\u201d and \u201csensorless\u201d an imperceivable amount of noise \u03b7 \u223c N(0, 0.001) to scramble the many repeated values, which harms histogram construction. For each dataset we simulate 150 changes \u03c60 \u2192 \u03c61 by randomly selecting TR and defining a random shift drawn from a normal distribution."}, {"heading": "5.2. Change Detection Methods", "text": "Four of the considered methods rely on the same histogram computed through QuantTree (Algorithm 1) to provide a uniform density partition of Rd, i.e. the target probabilities are \u03c0k = 1/K, \u2200k. These methods differ only for the threshold adopted and have been considered mainly to investigate the control over false positives.\nPearson Distribution Free / TV Distribution Free: thresholds are computed by Algorithm 2 for the Pearson T Ph (6) and the total variation T TVh statistics (7), respectively. The adopted thresholds are reported in Table 1.\nPearson Asymptotic: thresholds for T Ph are provided from the classic \u03c72 goodness-of-fit test (Lehmann & Romano, 2006), which provides an asymptotic control over the FPR.\nTV Bootstrap: thresholds for T TVh are computed empirically by bootstrapping TR.\nThree other methods built on different density models have been considered to assess the advantages \u2013 also in terms of HT power \u2013 of histograms providing uniform density.\nVoronoi: a histogram where the {Sk}k are defined as Voronoi cells around K randomly chosen centers in TR.\nHere we compute T TVh and use thresholds estimated by bootstrapping over TR.\nDensity Tree: A binary tree aiming at approximating \u03c60, where splits are defined by a maximum information-gain criterion, in a similar fashion to random density trees like (Criminisi et al., 2011). We use T TVh with thresholds empirically computed by bootstrap over TR.\nParametric: in the synthetic experiments we consider also an HT based on a parametric density model. In particular, we fit a Gaussian density on TR, compute the loglikelihood (Song et al., 2007; Kuncheva, 2013) of each incoming batchW , and detect changes by means of the t-test. Since this method exploits the true density model, it has to be considered as an ideal reference.\nAll the methods are configured and tested on the same TR and tested on the same batches W . We perform a PCA transformation, estimated from TR, to all the methods based on trees as density models. We have in fact experienced that this improves the change-detection performance, since it aligns the coordinate axes \u2013 along which splits are performed \u2013 with the principal components that become parallel to the bin boundaries."}, {"heading": "5.3. Test Design and Performance Measures", "text": "We consider a small TR configuration, where N = 4096 and \u03bd = 64, and a large TR configuration, where N = 16384 and \u03bd = 256. Both configurations have been tested with a number of bins K = 32 and K = 128, leading to 4 different combinations (N, \u03bd,K). In all our experiments, the target FPR has been set to \u03b1 = 0.05.\nWe empirically compute the FPR as the ratio of detections over 100 stationary batches W \u223c \u03c60, for each considered \u03c60. Similarly, for each change \u03c60 \u2192 \u03c61, we estimate the test power over 100 batches W \u223c \u03c61. The average FPR and power computed over the whole datasets are reported as dots in Figure 2. To illustrate the distribution of the FPR and power we report their boxplots."}, {"heading": "5.4. Results and Discussion", "text": "Figure 2 shows the FPR and the power of all the methods in the small TR configuration. Figures 2(a-b),(e-f) confirm that QuantTree effectively controls the FPR, for both the Pearson and total variation statistics, which is very important in change-detection. The peculiar QuantTree construction and Algorithm 2 provide very accurate thresholds resulting in FPR below the reference value \u03b1 = 0.05. Moreover, even if histograms defined by QuantTree feature a small number of bins, they are able to effectively monitor high-dimensional datastreams.\nThe FPRs of the total variation statistic are typically lower\nthan others: this is due to the discrete nature of the statistics, which affects both testing and quantile estimation. The same problem occurs, but to a lesser extent, in the Pearson statistic, since the expression (6) contains a square that allows this statistic to assume a larger number of distinct values. Clearly, increasing K attenuates this problem, bringing the FPR closer to \u03b1. Thresholds used in the traditional Pearson test achieve larger FPR values, as the number of training samples in each bin is too low for the asymptotic approximation to hold: in the large TR configuration, the problem attenuates (plots and tables of average values are reported in the Appendix). Since the likelihood values do not follow a Gaussian distribution, the FPR are not properly controlled in the t-test of the Parametric method either. In all these tests, smaller values of K provide a better control over FPR, since the number of samples in each bin is larger.\nConcerning the power, Figures 2(c-d) show a clear decay when d increases: this is consistent with the Detectability loss problem, which has been analytically studied in (Alippi et al., 2016) when monitoring the log-likelihood (as the Parametric). In general, all the methods on Synthetic datasets achieve satisfactory performance, and uniform histograms obtained through the QuantTree appear a better choice than Density Tree and Voronoi. There are minor differences among methods based on QuantTree which are nevertheless consistent with the FPR in Figure 2(a-b). Uniform density histograms outperforms others on real world datasets, see Figure 2(g-h), indicating that their partitioning scheme is better at detecting changes. Obviously, increasing N and \u03bd provides superior performance (see the results reported in the supplementary materials)."}, {"heading": "6. Conclusions", "text": "In this paper we have presented QuantTree, an algorithm to build histograms for change detection through a recursive binary splitting of the input space. Our theoretical analysis allows a characterization of the probability of each bin defined by QuantTree and shows that this probability is independent from the distribution \u03c60 of stationary data. This implies that statistics defined over such histograms are non parametric and thresholds can be estimated through numerical simulation on synthetically generated data. Experiments show that our thresholds (estimated using samples drawn from a univariate uniform distribution) enable a better control of the FPR than asymptotic ones or those estimated by bootstrap, which is no longer necessary when using such histograms. Ongoing work investigates how to mitigate the impact of test statistics assuming a limited number of discrete values, asymptotic results for histograms generated by QuantTree, and extensions to sequential monitoring schemes."}], "year": 2018, "references": [{"title": "Just-in-time classifiers for recurrent concepts", "authors": ["C. Alippi", "G. Boracchi", "M. Roveri"], "venue": "IEEE Transactions on Neural Networks and Learning Systems,", "year": 2013}, {"title": "Change detection in multivariate datastreams: Likelihood and detectability loss", "authors": ["C. Alippi", "G. Boracchi", "D. Carrera", "M. Roveri"], "venue": "In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI),", "year": 2016}, {"title": "Order statistics: theory & methods", "authors": ["N. Balakrishnan", "C.R. Rao"], "year": 1998}, {"title": "Detection of abrupt changes: theory and application", "authors": ["M. Basseville", "I.V. Nikiforov"], "year": 1993}, {"title": "Multidimensional binary search trees used for associative searching", "authors": ["J.L. Bentley"], "venue": "Communications of the ACM,", "year": 1975}, {"title": "Learning from time-changing data with adaptive windowing", "authors": ["A. Bifet", "R. Gavalda"], "venue": "In Proceedings of the SIAM International Conference on Data Mining,", "year": 2007}, {"title": "Uniform histograms for change detection in multivariate data", "authors": ["G. Boracchi", "C. Cervellera", "D. Macci\u00f2"], "venue": "In Proceedings of the IEEE International Joint Conference of Neural Networks (IJCNN),", "year": 2017}, {"title": "Generating high-dimensional datastreams for change detection", "authors": ["D. Carrera", "G. Boracchi"], "venue": "Big Data Research,", "year": 2017}, {"title": "Decision forests for classification, regression, density estimation, manifold learning and semi-supervised learning", "authors": ["A. Criminisi", "J. Shotton", "E. Konukoglu"], "venue": "Microsoft Research,", "year": 2011}, {"title": "Calibrating probability with undersampling for unbalanced classification", "authors": ["A. Dal Pozzolo", "O. Caelen", "R.A. Johnson", "G. Bontempi"], "venue": "In Proceedings of the IEEE Symposium Series on Computational Intelligence and Data Mining (CIDM),", "year": 2015}, {"title": "An information-theoretic approach to detecting changes in multi-dimensional data streams", "authors": ["T. Dasu", "S. Krishnan", "S. Venkatasubramanian", "K. Yi"], "venue": "In Proceedings of the Symposium on the Interface of Statistics, Computing Science, and Applications,", "year": 2006}, {"title": "Hellinger distance based drift detection for nonstationary environments", "authors": ["G. Ditzler", "R. Polikar"], "venue": "In Proceedings of the IEEE Symposium on Computational Intelligence in Dynamic and Uncertain Environments (CIDUE),", "year": 2011}, {"title": "A survey on concept drift adaptation", "authors": ["J. Gama", "I. Zliobaite", "A. Bifet", "M. Pechenizkiy", "A. Bouchachia"], "venue": "ACM Computing Surveys (CSUR),", "year": 2014}, {"title": "Concept drift detection through resampling", "authors": ["M. Harel", "S. Mannor", "R. El-Yaniv", "K. Crammer"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "year": 2014}, {"title": "A multivariate kolmogorov-smirnov test of goodness of fit", "authors": ["A. Justel", "D. Pe\u00f1a", "R. Zamar"], "venue": "Statistics & Probability Letters,", "year": 1997}, {"title": "The algorithm apt to classify in concurrence of latency and drift", "authors": ["G. Krempl"], "venue": "In Proceedings of the Intelligent Data Analysis (IDA),", "year": 2011}, {"title": "Change detection in streaming multivariate data using likelihood detectors", "authors": ["L.I. Kuncheva"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "year": 2013}, {"title": "Robust changepoint detection based on multivariate rank statistics", "authors": ["A. Lung-Yut-Fong", "C. L\u00e9vy-Leduc", "O. Capp\u00e9"], "venue": "In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "year": 2011}, {"title": "Two nonparametric control charts for detecting arbitrary distribution changes", "authors": ["G.J. Ross", "N.M. Adams"], "venue": "Journal of Quality Technology,", "year": 2012}, {"title": "Nonparametric monitoring of data streams for changes in location and scale", "authors": ["G.J. Ross", "D.K. Tasoulis", "N.M. Adams"], "year": 2011}, {"title": "Gaussian process change point models", "authors": ["Y. Saat\u00e7i", "R.D. Turner", "C.E. Rasmussen"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "year": 2010}, {"title": "Statistical change detection for multi-dimensional data", "authors": ["X. Song", "M. Wu", "C. Jermaine", "S. Ranka"], "venue": "In Proceedings of the International Conference on Knowledge Discovery and Data Mining (KDD),", "year": 2007}, {"title": "The choice of the number of bins for the m statistic", "authors": ["L.F. White", "M. Bonetti", "M. Pagano"], "venue": "Computational statistics & data analysis,", "year": 2009}], "id": "SP:85c389570bec42b542fed3b24e24d8b85a6d0a11", "authors": [{"name": "Giacomo Boracchi", "affiliations": []}, {"name": "Diego Carrera", "affiliations": []}, {"name": "Cristiano Cervellera", "affiliations": []}, {"name": "Danilo Macci\u00f2", "affiliations": []}], "abstractText": "We address the problem of detecting distribution changes in multivariate data streams by means of histograms. Histograms are very general and flexible models, which have been relatively ignored in the change-detection literature as they often require a number of bins that grows unfeasibly with the data dimension. We present QuantTree, a recursive binary splitting scheme that adaptively defines the histogram bins to ease the detection of any distribution change. Our design scheme implies that i) we can easily control the overall number of bins and ii) the bin probabilities do not depend on the distribution of stationary data. This latter is a very relevant aspect in change detection, since thresholds of tests statistics based on these histograms (e.g., the Pearson statistic or the total variation) can be numerically computed from univariate and synthetically generated data, yet guaranteeing a controlled false positive rate. Our experiments show that the proposed histograms are very effective in detecting changes in high dimensional data streams, and that the resulting thresholds can effectively control the false positive rate, even when the number of training samples is relatively small.", "title": "QuantTree: Histograms for Change Detection in Multivariate Data Streams"}