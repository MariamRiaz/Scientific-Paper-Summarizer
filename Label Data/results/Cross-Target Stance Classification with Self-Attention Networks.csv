0,1,label2,summary_sentences
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2832–2838 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"Bidirectional Long Short-Term Memory (BLSTM) based models (Graves and Schmidhuber, 2005), along with word embeddings and character embeddings, have shown competitive performance on Part-of-Speech (POS) tagging given sufficient amount of training examples (Ling et al., 2015; Lample et al., 2016; Plank et al., 2016; Yang et al., 2017).
",1 Introduction,[0],[0]
"Given insufficient training examples, we can improve the POS tagging performance by cross-
lingual POS tagging, which exploits affluent POS tagging corpora from other source languages.",1 Introduction,[0],[0]
"This approach usually requires linguistic knowledge or resources about the relation between the source language and the target language such as parallel corpora (Täckström et al., 2013; Duong et al., 2013; Kim et al., 2015a; Zhang et al., 2016), morphological analyses (Hana et al., 2004), dictionaries (Wisniewski et al., 2014), and gaze features (Barrett et al., 2016).
",1 Introduction,[0],[0]
"Given no linguistic resources between the source language and the target language, transfer learning methods can be utilized instead.",1 Introduction,[0],[0]
"Transfer learning for cross-lingual cases is a type of transductive transfer learning, where the input domains of the source and the target are different (Pan and Yang, 2010) since each language has its own vocabulary space.",1 Introduction,[0],[0]
"When the input space is the same, lower layers of hierarchical models can be shared for knowledge transfer (Collobert et al., 2011; Kim et al., 2015b; Yang et al., 2017), but that approach is not directly applicable when the input spaces differ.
",1 Introduction,[0],[0]
Yang et al. (2017) used shared character embeddings for different languages as a cross-lingual transfer method while using different word embeddings for different languages.,1 Introduction,[0],[0]
"Although the approach showed improved performance on Named Entity Recognition, it is limited to character-level representation transfer and it is not applicable for knowledge transfer between languages without overlapped alphabets.
",1 Introduction,[0],[0]
"In this work, we introduce a cross-lingual transfer learning model for POS tagging requiring no cross-lingual resources, where knowledge transfer is made in the BLSTM layers on top of word embeddings and character embeddings.",1 Introduction,[0],[0]
"Inspired by Kim et al. (2016)’s multi-task slot-filling model, our model utilizes a common BLSTM for representing language-generic information, which al-
2832
lows knowledge transfer from other languages, and private BLSTMs for representing languagespecific information.",1 Introduction,[0],[0]
"The common BLSTM is additionally encouraged to be language-agnostic with language-adversarial training (Chen et al., 2016) so that the language-general representations to be more compatible among different languages.
",1 Introduction,[0],[0]
"Evaluating on POS datasets from 14 different target languages with English as the source language in the Universal Dependencies corpus 1.4 (Nivre et al., 2016), the proposed model showed significantly better performance when the source language and the target language are in the same language family, and competitive performance when the language families are different.",1 Introduction,[0],[0]
Cross-Lingual Training Figure 1 shows the overall architecture of the proposed model.,2 Model,[0],[0]
"The baseline POS tagging model is similar to Plank et al. (2016)’s model, and it corresponds to having only word+char embeddings, common BLSTM, and Softmax Output in Figure 1.",2 Model,[0],[0]
"Given an input
word sequence, a BLSTM is used for the character sequence of each word, where the outputs of the ends of the character sequences from the forward LSTM and the backward LSTM are concatenated to the word vector of the current word to supplement the word representation.",2 Model,[0],[0]
"These serve as an input to a BLSTM, and an output layer are used for POS tag prediction.
",2 Model,[0],[0]
"For the cross-lingual transfer learning, the character embedding, the BLSTM with the character embedding (Yang et al., 2017),1 and the common BLSTM are shared for all the given languages while word embeddings and private BLSTMs have different parameters for different languages.
",2 Model,[0],[0]
The outputs of the common BLSTM and the private BLSTM of the current language are summed to be used as the input to the softmax layer to predict the POS tags of given word sequences.,2 Model,[0],[0]
"The loss function of the POS tagging can be formulate as:
Lp = − S∑
i=1",2 Model,[0],[0]
"N∑ j=1 pi,j log (p̂i,j) , (1)
where S is the number of sentences in the current minibatch,N is the number of words in the current sentence, pi,j is the label of the j-th tag of the i-th sentence in the minibatch, and p̂i,j is the predicted tag.",2 Model,[0],[0]
"In addition to this main objective, two more objectives for improving the transfer learning are described in the following subsections.
",2 Model,[0],[0]
"Language-Adversarial Training We encourage the outputs of the common BLSTM to be language-agnostic by using language-adversarial training (Chen et al., 2016) inspired by domainadversarial training (Ganin et al., 2016; Bousmalis et al., 2016).",2 Model,[0],[0]
"First, we encode a BLSTM output sequence as a single vector using a CNN/MaxPool encoder, which is implemented the same as a CNN for text classification (Kim, 2014).",2 Model,[0],[0]
"The encoder is with three convolution filters whose sizes are 3, 4, and 5.",2 Model,[0],[0]
"For each filter, we pass the BLSTM output sequence as the input sequence and obtain a single vector from the filter output by using max pooling, and then tanh activation function is used for transforming the vector.",2 Model,[0],[0]
"Then, the vector outputs of the three filters are concatenated and forwarded to the language discriminator through the gradient reversal layer.",2 Model,[0],[0]
"The discriminator is implemented
1We also tried isolated character-level modules but the overall performance was worse.
as a fully-connected neural network with a single hidden layer, whose activation function is Leaky ReLU (Maas et al., 2013), where we multiply 0.2 to negative input values as the outputs.
",2 Model,[0],[0]
"Since the gradient reversal layer is below the language classifier, the gradients minimizing language classification errors are passed back with opposed sign to the sentence encoder, which adversarially encourages the sentence encoder to be language-agnostic.",2 Model,[0],[0]
"The loss function of the language classifier is formulated as:
La = − S∑
i=1
li log l̂i, (2)
where S is the number of sentences, li is the language of the i-th sentence, and l̂i is the softmax output of the tagging.",2 Model,[0],[0]
"Note that though the language classifier is optimized to minimize the language classification error, the gradient from the language classifier is negated so that the bottom layers are trained to be language-agnostic.
",2 Model,[0],[0]
"Bidirectional Language Modeling Rei (2017) showed the effectiveness of the bidirectional language modeling objective, where each time step of the forward LSTM outputs predicts the word of the next time step, and each of the backward LSTM outputs predicts the previous word.",2 Model,[0],[0]
"For example, if the current sentence is “I am happy”, the forward LSTM predicts “am happy <eos>” and the backward LSTM predicts “<bos> I am”.",2 Model,[0],[0]
"This objective encourages the BLSTM layers and the embedding layers to learn linguistically general-purpose representations, which are also useful for specific downstream tasks (Rei, 2017).",2 Model,[0],[0]
"We adopted the bidirectional language modeling objective, where the sum of the common BLSTM and the private BLSTM is used as the input to the language modeling module.",2 Model,[0],[0]
"It can be formulated as:
Ll = − S∑
i=1",2 Model,[0],[0]
N∑ j=1 log (P (wj+1|fj)),2 Model,[0],[0]
"+
log (P (wj−1|bj)) , (3)
where fj and bj represent the j-th outputs of the forward direction and the backward direction, respectively, given the output sum of the common BLSTM and the private BLSTM.
",2 Model,[0],[0]
"All the three loss functions are added to be optimized altogether as:
L = ws",2 Model,[0],[0]
"(Lp + λLa + λLl) , (4)
where λ is gradually increased from 0 to 1 as epoch increases so that the model is stably trained with auxiliary objectives (Ganin et al., 2016).",2 Model,[0],[0]
ws is used to give different weights to the source language and the target language.,2 Model,[0],[0]
"Since the source language has a larger train set and we are focusing on improving the performance of the target language, ws is set to 1 when training the target language.",2 Model,[0],[0]
"For the source language, instead, it is set as the size of the target train set divided by the size of the source train set.",2 Model,[0],[0]
"For the evaluation, we used the POS datasets from 14 different languages in Universal Dependencies corpus 1.4 (Nivre et al., 2016).",3 Experiments,[0],[0]
"We used English as the source language, which is with 12,543 training sentences.2",3 Experiments,[0],[0]
We chose datasets with 1k to 14k training sentences.,3 Experiments,[0],[0]
"The number of tag labels differs for each language from 15 to 18 though most of them are overlapped within the languages.
",3 Experiments,[0],[0]
"Table 1 shows the POS tagging accuracies of different transfer learning models when we limited the number of training sentences of the target languages to be the same as 1,280 for fair comparison among different languages.",3 Experiments,[0],[0]
The remainder training examples of the target languages are still used for both language-adversarial training and bidirectional language modeling since the objectives do not require tag labels.,3 Experiments,[0],[0]
Training with only the train sets in the target languages (c) showed 91.61% on average.,3 Experiments,[0],[0]
"When bidirectional language modeling objective is used (c, l), the accuracies were significantly increased to 92.82% on average.",3 Experiments,[0],[0]
"Therefore, we used the bidirectional language modeling for all the transfer learning evaluations.
",3 Experiments,[0],[0]
"With transfer learning, the three cases of using only the common BLSTM (c), using only the private BLSTMs (p), and using both (c, p) were evaluated.",3 Experiments,[0],[0]
"They showed better average accuracies than target only cases, but they showed mixed results.",3 Experiments,[0],[0]
"However, our proposed model (c, p, l + a), which utilizes both the common BLSTM with language-adversarial training and the private BLSTMs, showed the highest average score, 93.26%.",3 Experiments,[0],[0]
"For all the Germanic languages, where the source language also belongs to, the accuracies are significantly higher than those of
2The accuracies of English POS tagging are 94.01 and 94.33 for models without the bidirectional language modeling and with it, respectively.
",3 Experiments,[0],[0]
other transfer learning models.,3 Experiments,[0],[0]
"For the languages belonging to Slavic, Romance, or Indo-Iranian, our model shows competitive performance with the highest average accuracies among the compared models.",3 Experiments,[0],[0]
"Since languages in the same family are more likely to be similar and compatible, it is expected that the gain from the knowledge transfer to the languages in the same family to be higher than transferring to the languages in different families, which was shown in the results.",3 Experiments,[0],[0]
"This shows that utilizing both language-general representations that are encouraged to be more language-agnostic and language-specific representations effectively helps improve the POS tagging performance with transfer learning.
",3 Experiments,[0],[0]
Table 2 shows the results when using 320 taglabeled training sentences.,3 Experiments,[0],[0]
"In this case, transfer learning methods still show better accuracies than target-only approaches on average.",3 Experiments,[0],[0]
"However, the performance gain is weakened compared to using 1,280 labeled training sentences and there are some mixed results.",3 Experiments,[0],[0]
"In several cases, just utilizing private BLSTMs without the common BLSTM showed better accuracies than utilizing the common BLSTM.
",3 Experiments,[0],[0]
"When training with only 32 tag-labeled sentences, which is an extremely low-resourced setting, transfer learning methods still showed better accuracies than target-only methods on average.",3 Experiments,[0],[0]
"However, not using the common BLSTM
in transfer learning models showed better performance than using it on average.3",3 Experiments,[0],[0]
The main reason would be that we are not given a sufficient number of labeled training sentences to train both the common BLSTM and the private BLSTMs.,3 Experiments,[0],[0]
"In this case, just having private BLSTMs without the common BLSTM can show better performance.",3 Experiments,[0],[0]
"We also evaluated the opposite cases, which use all the tag-labeled training sentences in the target languages, and they showed mixed results.",3 Experiments,[0],[0]
"For example, the accuracy of German with the target only model is 93.31% while that of the proposed model is 93.04%.",3 Experiments,[0],[0]
"This is expected since transfer learning is effective when the target train set is small.
",3 Experiments,[0],[0]
An extension of this work is utilizing multiple languages as the source languages.,3 Experiments,[0],[0]
"Since we have four languages for each of Germanic, Slavic, and Romance language families, we evaluated the performance of those languages using the other languages in the same families as the source languages expecting that languages in the same language family are more likely to be helpful each other.",3 Experiments,[0],[0]
"For the efficiency, we performed multi-task learning for multiple languages rather than differentiating the targets from sources.",3 Experiments,[0],[0]
"When we tried to use 1,280, 320, and 32 tag-labeled training sentences for each language in the multi-source settings, the results showed noticeably better per-
3The results in detail are shown in the first authors dissertation Kim (2017).
formance than the results of using English as a single source language.",3 Experiments,[0],[0]
"Considering that utilizing 1,280*3=3,840, 320*3=960, or 32*3=96 tag labels from three other languages showed better results than using 12,543 English tag labels as the source, we can see that the knowledge transfer from multiple languages can be more helpful than that from single resource-rich source language.",3 Experiments,[0],[0]
"We also tried to use Wasserstein distance (Arjovsky et al., 2017) for the adversarial training in the multi-source settings, but there were no significant differences on average.4
Implementation Details All the models were optimized using ADAM (Kingma and Ba, 2015)5 with minibatch size 32 for total 100 epochs and we picked the parameters showing the best accuracy on the development set to report the score on the test set.",3 Experiments,[0],[0]
The dimensionalites of all the BLSTM related layers follow Plank et al. (2016)’s model.,3 Experiments,[0],[0]
Each word vector is 128 dimensional and each character vector is 100 dimensional.,3 Experiments,[0],[0]
"They are randomly initialized with Xavier initialization (Glorot and Bengio, 2010).",3 Experiments,[0],[0]
"For stable training, we use gradient clipping, where the threshold is set to 5.",3 Experiments,[0],[0]
"The dimensionality of each hidden output of LSTMs is 100, and the hidden outputs of both forward LSTM and backward LSTM are concatenated, thereby the output of each BLSTM for each time step is 200.",3 Experiments,[0],[0]
"Therefore, the input to the common BLSTM and the private BLSTM is 128+200=328
4The extended work in detail are shown in Kim (2017).",3 Experiments,[0],[0]
"5learning rate=0.001, β1 = 0.9, β2 = 0.999, = 1e− 8.
dimensional.",3 Experiments,[0],[0]
"The inputs and the outputs of the BLSTMs are regularized with dropout rate 0.5 (Pham et al., 2014).",3 Experiments,[0],[0]
"For the consistent dropout usages, we let the dropout masks to be identical for all the time steps of each sentence (Gal and Ghahramani, 2016).",3 Experiments,[0],[0]
"For all the BLSTMs, forget biases are initialized with 1 (Jozefowicz et al., 2015) and the other biases are initialized with 0.",3 Experiments,[0],[0]
"Each convolution filter output for the sentence encoding is 64 dimensional, and the three filter outputs are concatenated to represent each sentence with a 192 dimensional vector.",3 Experiments,[0],[0]
We introduced a cross-lingual transfer learning model for POS tagging which uses separate BLSTMs for language-general and languagespecific representations.,4 Conclusion,[0],[0]
"Evaluating on 14 different languages, including the source language improved tagging accuracies in almost all the cases.",4 Conclusion,[0],[0]
"Specifically, our model showed noticeably better performance when the source language and the target languages belong to the same language family, and competitively performed with the highest average accuracies for target languages in different families.",4 Conclusion,[0],[0]
We thank the anonymous reviewers for their helpful comments.,Acknowledgments,[0],[0]
All the experiments in this work were conducted with machines at Ohio Supercomputer Center (1987).,Acknowledgments,[0],[0]
Training a POS tagging model with crosslingual transfer learning usually requires linguistic knowledge and resources about the relation between the source language and the target language.,abstractText,[0],[0]
"In this paper, we introduce a cross-lingual transfer learning model for POS tagging without ancillary resources such as parallel corpora.",abstractText,[0],[0]
"The proposed cross-lingual model utilizes a common BLSTM that enables knowledge transfer from other languages, and private BLSTMs for language-specific representations.",abstractText,[0],[0]
The cross-lingual model is trained with language-adversarial training and bidirectional language modeling as auxiliary objectives to better represent language-general information while not losing the information about a specific target language.,abstractText,[0],[0]
"Evaluating on POS datasets from 14 languages in the Universal Dependencies corpus, we show that the proposed transfer learning model improves the POS tagging performance of the target languages without exploiting any linguistic knowledge between the source language and the target language.",abstractText,[0],[0]
Cross-Lingual Transfer Learning for POS Tagging without Cross-Lingual Resources,title,[0],[0]
"Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 998–1008, Baltimore, Maryland, USA, June 23-25 2014. c©2014 Association for Computational Linguistics",text,[0],[0]
"Discourse structure, logical flow of sentences, and context play a large part in ordering medical events based on temporal relations within a clinical narrative.",1 Introduction,[0],[0]
"However, cross-narrative temporal relation ordering is a challenging task as it is difficult to learn temporal relations among medical events which are not part of the logically coherent discourse of a single narrative.",1 Introduction,[0],[0]
"Resolving crossnarrative temporal relationships between medical events is essential to the task of generating an event timeline from across unstructured clinical narratives such as admission notes, radiology reports, history and physical reports and discharge summaries.",1 Introduction,[0],[0]
"Such a timeline has multiple applications in clinical trial recruitment (Luo et al., 2011), medical document summarization (Bramsen et al.,
2006, Reichert et al., 2010) and clinical decision making (Demner-Fushman et al., 2009).
",1 Introduction,[0],[0]
"Given multiple temporally ordered medical event sequences generated from each clinical narrative in a patient record, how can we combine the events to create a timeline across all the narratives?",1 Introduction,[0],[0]
"The tendency to copy-paste text and summarize past information in newly generated clinical narratives leads to multiple mentions of the same medical event across narratives (Cohen et al., 2013).",1 Introduction,[0],[0]
These cross-narrative coreferences act as important anchors for reasoning with information across narratives.,1 Introduction,[0],[0]
We leverage crossnarrative coreference information along with confident cross-narrative temporal relation predictions and learn to align and temporally order medical event sequences across longitudinal clinical narratives.,1 Introduction,[0],[0]
We model the problem as a sequence alignment task and propose solving this using two approaches.,1 Introduction,[0],[0]
"First, we use weighted finite state machines to represent medical events sequences, thus enabling composition and search to obtain the most probable combined sequence of medical events.",1 Introduction,[0],[0]
"As a contrast, we adapt dynamic programming algorithms (Needleman et al., 1970, Smith and Waterman, 1981) used to produce global and local alignments for aligning sequences of medical events across narratives.",1 Introduction,[0],[0]
"We also compare the proposed methods with an Integer Linear Programming (ILP) based method for timeline construction (Do et al., 2012).",1 Introduction,[0],[0]
"The cross-narrative coreference and temporal relation scores used in both these approaches are learned from a corpus of patient narratives from The Ohio State University Wexner Medical Center.
",1 Introduction,[0],[0]
The main contribution of this paper is a general framework that allows aligning multiple event sequences using cascaded weighted finite state transducers (WFSTs) with the help of efficient composition and decoding.,1 Introduction,[0],[0]
"Moreover, we demonstrate that this method can be used for more accurate multiple sequence alignment when compared to
998
dynamic programming or other ILP-based methods proposed in literature.",1 Introduction,[0],[0]
"In the areas of summarization and text-to-text generation, there has been prior work on several ordering strategies to order pieces of information extracted from different input documents (Barzilay et al., 2002, Lapata, 2003, Bollegala et al., 2010).",2 Related Work,[0],[0]
"In this paper, we focus on temporal ordering of information, as discussed next.
",2 Related Work,[0],[0]
"Recent state-of-the art research has focused on the problem of temporal relation learning within the same document, and in many cases within the same sentence (Mani et al., 2006, Verhagen et al., 2009, Lapata and Lascarides, 2011).",2 Related Work,[0],[0]
"Chambers and Jurafsky (2009) describe a process to induce a partially ordered set of events related by a common protagonist by using an unsupervised distributional method to learn relations between events sharing coreferring arguments, followed by temporal classification to induce partial order.",2 Related Work,[0],[0]
"The task was carried out on the Timebank newswire corpus, but was limited to an intra-document setting.",2 Related Work,[0],[0]
"More recently, (Do et al., 2012) proposed an ILP-based method to combine the outputs of an event-interval and an event-event classifier for timeline construction on the ACE 2005 corpus.",2 Related Work,[0],[0]
"However, this approach is also restricted to events within documents and requires annotations for event intervals.",2 Related Work,[0],[0]
We empirically compare our methods for timeline creation from longitudinal clinical narratives to such an ILP-based approach in Section 7.,2 Related Work,[0],[0]
"While a lot of this work has been done in the news domain, there is also some recent work in rule-based algorithms (Zhou et al., 2006) and machine learning (Roberts et al., 2008) applied to temporal relations between medical events in clinical text.",2 Related Work,[0],[0]
"Clinical narratives are written in a distinct sub-language with domain specific terminology and temporal characteristics, making them markedly different from newswire text.
",2 Related Work,[0],[0]
There is limited prior work in learning relations across documents.,2 Related Work,[0],[0]
"Ji and Grishman (2008) extended the one sense per discourse idea (Yarowsky, 1995) to multiple topically related documents and propagate consistent event arguments across sentences and documents.",2 Related Work,[0],[0]
Barzilay and McKeown (2005) propose a text-to-text generation technique for synthesizing common information across documents using sentence fusion.,2 Related Work,[0],[0]
"This involves multisequence dependency tree alignment to identify phrases conveying sim-
ilar information and statistical generation to combine common phrases into a sentence.",2 Related Work,[0],[0]
"Along with syntactic features, they combine knowledge from resources like WordNet to find similar sentences.",2 Related Work,[0],[0]
"In case of clinical narratives and medical event alignment, the objective is to identify a unique sequence of temporally ordered medical events from across longitudinal clinical data.
",2 Related Work,[0],[0]
"To the best of our knowledge, there is no prior work on cross-document alignment of event sequences.",2 Related Work,[0],[0]
"Multiple sequence alignment is a problem that arises in a variety of domains including gene/protein alignments in bioinformatics (Notredame, 2002), word alignments in machine translation (Kumar and Byrne, 2003), and sentence alignments for summarization (Lacatusu et al., 2004).",2 Related Work,[0],[0]
"Dynamic programming algorithms have been popularly leveraged to produce pairwise and global genetic alignments, where edit distance based metrics are used to compute the cost of insertions, deletions and substitutions.",2 Related Work,[0],[0]
"We use dynamic programming to compute the best alignment, given the temporal and coreference information between medical events across these sequences.",2 Related Work,[0],[0]
"More importantly, we propose a cascaded WFST-based framework for crossdocument temporal ordering of medical event sequences.",2 Related Work,[0],[0]
"Composition and search operations can be used to build a single transducer that integrates these components, directly mapping from input states to desired outputs, and obtain the best alignment (Mohri et al., 2000).",2 Related Work,[0],[0]
"In natural language processing, WFSTs have seen varied applications in machine translation (Kumar and Byrne, 2003), morphology (Sproat, 2006), named entity recognition (Krstev et al., 2011) and biological sequence alignment / generation (Whelan et al., 2010) among others.",2 Related Work,[0],[0]
We demonstrate that the WFST-based approach outperforms popularly used dynamic programming algorithms for multiple sequence alignment.,2 Related Work,[0],[0]
"Medical events are temporally-associated concepts in clinical text that describe a medical condition affecting the patient’s health, or procedures performed on a patient.",3 Problem Description,[0],[0]
We represent medical events by splitting each event into a start and a stop.,3 Problem Description,[0],[0]
"When there is insufficient information to discern the start or stop of an event, it is represented as a single concept.",3 Problem Description,[0],[0]
"If only the start is known then the stop is set to +∞, whereas when only the stop is known , the start is set to the date of birth of the
patient.1 Often, for chronic ailments like hypertension, we would only associate a start with the medical event and set the stop to +∞. The start of hypertension may be associated with the temporal expression history of in the narrative.",3 Problem Description,[0],[0]
"This, when considered along with the admission date, allows us to relatively order hypertension with respect to other medical events.",3 Problem Description,[0],[0]
"A medical event occurrence like chest pain may be associated with a start and a stop, where the start may be determined by the mention of “patient was complaining of chest pain yesterday” in the narrative text.",3 Problem Description,[0],[0]
"Further, the narrative may state that “he continued to have chest pain on admission, but currently he is chest pain free”; this may be used to infer the relative stop of chest pain.",3 Problem Description,[0],[0]
"Medical events may also be instantaneous, for e.g., injected with antibiotic.",3 Problem Description,[0],[0]
Such events are represented with the start and stop as being the same.,3 Problem Description,[0],[0]
Temporal relations exist between the start and stop of events as shown in Figure 1.,3 Problem Description,[0],[0]
"Learning temporal relations before, after and simultaneous between the medical event starts and stops corresponds to learning all of Allen’s temporal relations (Allen, 1981) between the medical events.",3 Problem Description,[0],[0]
"Following our previous work (Raghavan et al., 2012c), such a representation allows us to temporally order the event starts and stops within each clinical narrative by learning to rank them in relative order of time.",3 Problem Description,[0],[0]
"The problem definition is as follows:
1Patient date of birth, admission/ discharge date are usually available in the metadata associated with a clinical narrative.
",3 Problem Description,[0],[0]
Input: Sequences of temporally ordered medical event starts and stops.,3 Problem Description,[0],[0]
"This corresponds to N1, N2, and N3 in Figure 2.",3 Problem Description,[0],[0]
Each sequence corresponds to a clinical narrative.,3 Problem Description,[0],[0]
"The total number of sequences correspond to the number of clinical narratives for a patient.
",3 Problem Description,[0],[0]
Problem:,3 Problem Description,[0],[0]
"Combine medical events across these sequences to generate a timeline i.e., a single comprehensive sequence of medical events over all clinical narratives of the patient.
",3 Problem Description,[0],[0]
Expected Output:,3 Problem Description,[0],[0]
"In the example shown in Figure 2, the output would be as follows:",3 Problem Description,[0],[0]
"Timeline (N1, N2, N3)= {cocaine usestart < hypertensionstart = hypertensionstart < admission1 < chest painstart ∼ palpitationsstart < chest painstop < heart attackstart = myocardial infarctionstart <",3 Problem Description,[0],[0]
"admission2 < infectionstart < MRSAstart < admission3 < woundsstart}.
",3 Problem Description,[0],[0]
The goal of multiple sequence alignment is to find an alignment that maximizes some overall alignment score.,3 Problem Description,[0],[0]
"Thus, in order to align event sequences, we need to compute scores corresponding to cross-narrative medical event coreference resolution and cross-narrative temporal relations.",3 Problem Description,[0],[0]
"The first approach to learning a temporal ordering of medical events across all clinical narratives is to consider all pairs of events across all narratives and learn to classify them as sharing one of Allen’s temporal relations (Allen, 1981) using a single learning model.",4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0],[0]
"Alternatively, a ranking ap-
proach, similar to the one used to generate intranarrative temporal ordering, can also be extended to the cross-narrative case.",4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0],[0]
"However, the features related to narrative structure and relative and implicit temporal expressions used for temporal ordering within a clinical narrative may not be applicable across narratives.",4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0],[0]
"For instance, a history and physical report may have sections like “past medical history”, “history of present illness”, “assessment and plan”, and a certain logical pattern to the flow of text within and across these sections.",4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0],[0]
"Further, temporal cues like “thereafter”, “subsequently”, follow from the context around an event mention.",4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0],[0]
"The absence of such features in the cross-narrative case does not allow such a model to generate accurate temporal relation predictions.
",4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0],[0]
"Thus, for use in our sequence alignment models, we learn two independent classifiers for medical event coreference and temporal relation learning across narratives.",4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0],[0]
We train a classifier to resolve cross-narrative coreferences by extracting semantic and temporal relatedness feature sets for each pair of medical concepts.,4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0],[0]
"Extracting these feature sets helps us train a classifier to predict medical event coreferences (Raghavan et al., 2012a).",4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0],[0]
"Another classifier is then trained to classify pairs of medical event starts and stops across narratives as sharing temporal relations {before, after, overlaps}.",4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0],[0]
The learned cross-narrative coreference predictions can then be used along with confident temporal relation predictions to derive a joint probability to enable cross-narrative temporal ordering.,4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0],[0]
Sequence alignment algorithms have been developed and popularly used in bioinformatics.,5 Narrative Sequence Alignment for Cross-narrative Temporal Ordering,[0],[0]
"However, multiple sequence alignment (MSA) has been shown to be NP complete (Wang and Jiang, 1994) and various heuristic algorithms have been proposed to solve this problem (Notredame, 2002).",5 Narrative Sequence Alignment for Cross-narrative Temporal Ordering,[0],[0]
"We propose a novel WFST-based representation that enables accurate decoding for MSA when compared to popularly used dynamic programming algorithms (Needleman et al., 1970, Smith and Waterman, 1981) or other state of the art methods (Do et al., 2012).
",5 Narrative Sequence Alignment for Cross-narrative Temporal Ordering,[0],[0]
"In the problem of aligning events across multiple narrative sequences, we want to align temporally ordered medical events corresponding to clinical narratives of a patient.",5 Narrative Sequence Alignment for Cross-narrative Temporal Ordering,[0],[0]
"Unlike problems in biological sequence alignment where the sym-
bols to be aligned across sequences are restricted to a fixed set, our symbol set is not fixed or certain because the symbols correspond to medical events in clinical narratives.",5 Narrative Sequence Alignment for Cross-narrative Temporal Ordering,[0],[0]
"Moreover, we cannot have fixed scores for symbol transformations since our transformations correspond to coreference and temporal relations between the medical events across sequences.",5 Narrative Sequence Alignment for Cross-narrative Temporal Ordering,[0],[0]
The computation of these scores is described next.,5 Narrative Sequence Alignment for Cross-narrative Temporal Ordering,[0],[0]
"Let us assume a, b are medical events in the first clinical narrative and have been temporally ordered so a < b.",5.1 Scoring Scheme,[0],[0]
"Similarly, x, y are medical events in the second clinical narrative such that x",5.1 Scoring Scheme,[0],[0]
< y.,5.1 Scoring Scheme,[0],[0]
"There exists a match or an alignment between a pair of medical events, across the sequences, in the following cases:
1.",5.1 Scoring Scheme,[0],[0]
"If the medical events are simultaneous and coreferring, denoted as a = x.
2.",5.1 Scoring Scheme,[0],[0]
"If the medical events are simultaneous and non-coreferring, denoted as a ∼ x.
3.",5.1 Scoring Scheme,[0],[0]
"If the a medical event from one sequence is before a medical event from another sequence, denoted as a < x.
4.",5.1 Scoring Scheme,[0],[0]
"If the a medical event from one sequence is after a medical event from another sequence, denoted as a > x.
We now illustrate how the scores for candidate aligned sequences are computed using the learned cross-narrative coreference and temporal probabilities for the following three scenarios:
• The medical events across sequences are simultaneous and corefer as illustrated in Figure 3.",5.1 Scoring Scheme,[0],[0]
"The joint score considers the probability of event temporal relations simultaneous conditioned on coreference.
",5.1 Scoring Scheme,[0],[0]
•,5.1 Scoring Scheme,[0],[0]
Some medical events across sequences are simultaneous but do not corefer as illustrated in Figure 4.,5.1 Scoring Scheme,[0],[0]
"Here, the joint score considers the joint probability of temporal relations simultaneous or before and no-coreference.
",5.1 Scoring Scheme,[0],[0]
•,5.1 Scoring Scheme,[0],[0]
The medical events across sequences are not simultaneous and do not corefer as illustrated in Figure 5.,5.1 Scoring Scheme,[0],[0]
"In this case, the joint score considers the probability of the temporal relation before and no coreference.
",5.1 Scoring Scheme,[0],[0]
"Thus, the coreference and temporal relation scores can be leveraged for aligning sequences of medical events.",5.1 Scoring Scheme,[0],[0]
"These scores are used in both the WFSTbased representation and decoding, as well as for dynamic programming.",5.1 Scoring Scheme,[0],[0]
"A weighted finite-state transducer (WFST) is an automaton in which each transition between states
is associated with an input symbol, an output symbol, and a weight (Mohri et al., 2005).",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
WFSTs can be used to efficiently represent and combine sequences of medical events based coreference and temporal relation information.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
The WFST representation gives us the ability to talk about the global joint probability derived from coreference and temporal relation scores described in Section 5.1.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
It allows us to build a weighted lattice of sequences that can be searched for the most probable sequence of medical events from across all clinical narratives of a patient.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"We use unweighted FSAs to represent the input described in Section 3, i.e. temporally ordered sequences of medical events corresponding to clinical narratives.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"This corresponds to N1 and N2 in Figure 6.
",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"Based on whether we want to align the sequences purely based on coreference scores or both coreference and temporal relation scores, the arc weights for the WFST can be determined.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
M c12 is a WFST that maps input symbols from N1 to output symbols inN2 and is weighted by the probability of coreference or no-coreference between medical events across N1 and N2.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
The representation in WFST M c+t12 shown in Figure 7 allows us to align N1 and N2 based on both coreference as well as temporal relation probabilities.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
The WFST has transitions to accommodate insertion and deletion of medical events when combining the sequences.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
Deletions correspond to the case when an event in the first sequence does not map to any event in the second sequence; similarly insertions correspond to the case where an event in the second sequence does not map to any event in the first sequence.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
The WFST composition operation allows the outputs of one WFST to be fed to the inputs of a second WFST or FSA.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"Thus, we build our final machine by composing the three sub-machines as,
D = N1 ◦M i12 ◦N2.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
(1) where i = c or i = c + t.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
This gives us a combined weighted graph by mapping the output symbols of the first medical event sequence to the input symbols of the second medical event sequence.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"The scores on the decoding graph are derived from only the coreference probabilities if i = c and both coreference and temporal relation probabilities if i = c+ t.
In the medical event sequence alignment problem, we want to align multiple sequences of medical events that correspond to multiple clinical narratives of a patient.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"Since we want to now combine
all narrative chains belonging to the same patient, the composition cascade to build the final combined sequence will be as,
Df = N1◦M i12◦N2◦M i23◦N3◦M i34...◦",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"Nn (2)
where i = c",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
or i = c + t and n is the number of medical event sequences corresponding to clinical narratives for a patient.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"During composition we retain intermediate paths like M i23 utilizing the ability to do lazy composition (Mohri and Pereira, 1998) in order to facilitate beam search through the multi-alignment.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
The best hypothesis corresponds to the highest scoring path which can be obtained using shortest path algorithms like Djikstra’s algorithm.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"The best path corresponds to the best alignment across all medical event sequences based on the joint probability of cross-narrative medical event coreferences and temporal relations across the narrative sequences.
",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"The complexity of decoding increases exponentially with the number of narrative sequences in
the composition, and exact decoding becomes infeasible.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"One solution to this problem is to do the alignment greedily pairwise, starting from the most recent medical event sequences, finding the best path, and iteratively moving on to the next sequence, and proceeding until the oldest medical event sequence.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"The disadvantage of such a method is that it does not take into account constraints between medical events across multiple event sequences and may lead to a less accurate solution.
",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
An alternative method is to use lazy composition to perform more efficient composition as it allows practical memory usage.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"We also use beam search to make for an efficient approximation to the best-path computation (Mohri et al., 2005).",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
This allows accommodating constraints from across multiple sequences and generates a more accurate best path.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"Thus, this method generates more accurate alignments when we have more than two sequences to be aligned.
",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"For instance, instance say a, b ∈ N1, x, y ∈ N2, and m,",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"n ∈ N3 are temporally medical event sequences corresponding to narratives N1, N2 and N3.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"Based on the learned pairwise temporal relations, if we have the following constraints a < x, m > x, m < a.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"Aligning N1 and N2 greedily pairwise may give us the best combined sequence as a, x, b, y ∈ N12.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"Now in aligning N12 with N3, we won’t be able to accommodate m > x",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
and m < a.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"However, performing a beam search over the composed WFST in equation 2 allows us to accommodate such constraints across multiple sequences.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"The complexity of composing two transducers is O(V1V2D1(logD2 + M2)) where each edge from the first sequence matches every edge in the second sequence and Vi is the number of states, Di is the maximum out-degree and Mi maximum multiplicity for the ith FST (Mohri et al., 2005).
",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"We also use popular dynamic programming algorithms (Needleman et al., 1970, Smith and Waterman, 1981) for sequence alignment of medical events across narratives and compare it to the WFST-based representation and decoding.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"As a contrast, we adapt two dynamic programming algorithms for sequence alignment: global alignment using the Needleman Wunsch algorithm (NW) (Needleman et al., 1970) and local alignment using the Smith-Waterman algorithm (SW) (Smith and Waterman, 1981).",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
NW allows us to align all events in one sequence with all events in another sequence.,5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
A drawback of NW is that short and highly similar sequences maybe missed because they get overweighted by the rest of the sequence.,5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
NW is suitable when the two sequences are of similar length with significant degree of similarity throughout.,5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"On the other hand, SW gives the longest sub-sequence pair that yields maximum degree of similarity between the two original sequences.",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
It does not force all events in a sequence to align with another sequence.,5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
SW is useful in aligning sequences that differ in length and have short patches of similarity.,5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"The time complexity of these methods for sequences of length m and n are O(mn).
",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
The scoring scheme described earlier is used to update the scoring matrix for dynamic programming.,5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"In order to accommodate the temporal relations before and after, we insert a null symbol after every medical event in each sequence in the scoring matrix.",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"A vertical or horizontal gap arises when cases 1, 2, 3 and 4 in Section 5.1 mentioned
above are not true.",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"If the medical events are not simultaneous, not before or not after, the medical events will not align.",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"Thus, the value of each cell in the scoring matrix is determined by computing the maximum score at each position C(i, j) as,
max{(C(i−1, j−1)+Sij), (C(i, j−1)+w), (C(i− 1, j) + w)} (3)
where, Sij = max{P (i = j), P (i < j), P (i > j)}, and w = max{(1",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"− P (i = j)), (1 − P (i < j)), (1 − P (i > j))}.",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"Here, C(i − 1, j − 1) corresponds to a match, whereas C(i, j − 1) and C(i − 1, j) correspond to a gaps in sequence one and two.
",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"In case of the SW algorithm, the negative scoring matrix cells are set to zero, thus making the positively scoring local alignments visible.",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"Backtracking starts at the highest scoring matrix cell and proceeds until a cell with score zero is encountered, yielding the highest scoring local alignment.
",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
The time and space complexity grows exponentially with the number of sequences to be aligned and finding the global optimum has been shown to be a NP-complete problem.,5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"The time complexity of aligning N sequences of length L is O(2NLN ) (Wang and Jiang, 1994).",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"Thus, for MSA using dynamic programming, we use a heuristic method where we combine pairwise alignments iteratively starting with the latest narrative and progressing towards the oldest narrative.",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
Corpus Description.,6 Experiments and Evaluation,[0],[0]
The corpus consists of a dataset of clinical narratives obtained from the [redacted] medical center.,6 Experiments and Evaluation,[0],[0]
"The corpus has a total of 2060 patients, and 100704 clinical narratives.",6 Experiments and Evaluation,[0],[0]
"We gathered a gold standard set of seven patients (80 clinical narratives overall) with manual annotation of all medical events mentioned in the narratives, coreferences, and medical event sequence information.",6 Experiments and Evaluation,[0],[0]
"The annotation agreement across annotators is high, with 89.5% agreement corresponding to inter-annotator Cohen’s kappa statistic of 0.86 (Raghavan et al., 2012b).",6 Experiments and Evaluation,[0],[0]
"The types of clinical narratives included 27 discharge summaries, 30 history and physical reports, 15 radiology reports and 8 pathology reports.",6 Experiments and Evaluation,[0],[0]
The distribution of the number of medical event sequences and unique medical events across patients is shown in Table 1.,6 Experiments and Evaluation,[0],[0]
"The annotated dataset is used to crossvalidate and train our coreference and temporal relation learning models and to evaluate our crossnarrative medical event timeline.
",6 Experiments and Evaluation,[0],[0]
Evaluation Metric.,6 Experiments and Evaluation,[0],[0]
"For each patient and each method (WFST or dynamic programming), the output timeline to evaluate is the highest scoring candidate hypothesis derived as described above.",6 Experiments and Evaluation,[0],[0]
Accuracy of the timeline is calculated as the number of transformations required to obtain the reference sequence in the annotated gold-standard from the one generated by our system.,6 Experiments and Evaluation,[0],[0]
"Transformations are measured in terms of the minimum edit distance, insertions, deletions, and substitutions of medical events.
",6 Experiments and Evaluation,[0],[0]
Experiments and Results.,6 Experiments and Evaluation,[0],[0]
"We first temporally order medical events within each clinical narrative by learning to rank them in relative order of occurence as described in our previous work (Raghavan et al., 2012c).",6 Experiments and Evaluation,[0],[0]
The overall accuracy of ranking medical events using leave-one-out cross validation is 82.1%.,6 Experiments and Evaluation,[0],[0]
"The resulting medical event sequences serve as the input to the problem of crossnarrative sequence alignment.
",6 Experiments and Evaluation,[0],[0]
The cross-narrative coreference and temporal relation pairwise classification models described in Section 4 are trained using a Maximum entropy classifier.,6 Experiments and Evaluation,[0],[0]
The coreference resolution performs with 71.5% precision and 82.3% recall.,6 Experiments and Evaluation,[0],[0]
The temporal relation classifier performs with 60.2% precision and 76.3% recall.,6 Experiments and Evaluation,[0],[0]
"The learned pairwise coreference and temporal relation probabilities are now used to derive the score for the WFST and dynamic programming approaches.
WFST representation and decoding.",6 Experiments and Evaluation,[0],[0]
We build finite-state machines using the open source OpenFST,6 Experiments and Evaluation,[0],[0]
library.2,6 Experiments and Evaluation,[0],[0]
We use a tropical semi-ring weighted using the negative log-likelihood of the computed scores.,6 Experiments and Evaluation,[0],[0]
"OpenFST provides tools that can search for the highest scoring sequences accepted by the machine, and can sample from highscoring sequences probabilistically, by treating the
2www.openfst.org
scores of each transition within the machine as a negative log probability.",6 Experiments and Evaluation,[0],[0]
The decoding process to compute the most likely combined medical event sequence can be defined as searching for the best path in the combined graph representation (Equation 2).,6 Experiments and Evaluation,[0],[0]
The best path is the one that minimizes the total weight on a path (since the arcs are negative log probabilities).,6 Experiments and Evaluation,[0],[0]
"In searching for the best path, the beam size is set to 5.",6 Experiments and Evaluation,[0],[0]
"The accuracy of the WFST-based representation and beam search across all sequences using the coreference and temporal relation scores to obtain the combined aligned sequence is 78.9%.
",6 Experiments and Evaluation,[0],[0]
Dynamic Programming.,6 Experiments and Evaluation,[0],[0]
We use the NW and SW algorithms described in Section 5.3 to produce local and global alignments respectively.,6 Experiments and Evaluation,[0],[0]
We use the scoring scheme described in Section 5.1 to update the cost matrix for dynamic programming and implement the algorithms as described in Section 5.3.,6 Experiments and Evaluation,[0],[0]
The overall accuracy of sequence alignment with both coreference and temporal relation scores using NW is 68.7% whereas SW gives an accuracy of 72.1%.,6 Experiments and Evaluation,[0],[0]
"In case of aligning just two sequences, both methods yield the same results.",6 Experiments and Evaluation,[0],[0]
"The accuracy of cross-narrative MSA for each patient, for each method, using cross validation, is shown in Table 1.",6 Experiments and Evaluation,[0],[0]
Results indicate that the WFSTbased method outperforms the dynamic programming approach for multi-sequence alignment (statistical significance p<0.05).,6 Experiments and Evaluation,[0],[0]
"Morever, the results using both coreference and temporal realtion scores for alignment outperform using only coreference scores for alignment using all approaches.",6 Experiments and Evaluation,[0],[0]
This indicates that cross-narrative temporal relations are important for accurately aligning medical event sequences across narratives.,6 Experiments and Evaluation,[0],[0]
"We propose and evaluate different approaches to multiple sequence alignment of medical events.
",7 Discussion,[0],[0]
Approaches to multi-alignment.,7 Discussion,[0],[0]
We address the problem of aligning medical event sequences using a novel WFST-based framework and empirically demonstrate that it outperforms pairwise progressive alignment using dynamic programming.,7 Discussion,[0],[0]
"This is mainly because the WFST-based allows us to consider temporal constraints from across multiple sequences when performing the alignment.
",7 Discussion,[0],[0]
"Moreover, it also outperforms the integer linear programming (ILP) method for timeline construction proposed in (Do et al., 2012).",7 Discussion,[0],[0]
We implemented the proposed method that also allows combining the output of classifiers subject to some constraints.,7 Discussion,[0],[0]
We derive intervals from event starts and stops and learn two perceptron classifiers for classifying the temporal relations between events and assigning events to intervals.,7 Discussion,[0],[0]
The classifier probabilities are then used to solve the optimization problem using the lpsolve solver.3,7 Discussion,[0],[0]
We also use intra-document coreference information to resolve coreference before performing the global optimization.,7 Discussion,[0],[0]
"We observe that in case of MSA, the optimal solution using ILP is still intractable as the number of constraints increases exponentially with the number of sequences.",7 Discussion,[0],[0]
Aligning pairwise iteratively gives us an overall average accuracy of 68.2% similar to dynamic programming.,7 Discussion,[0],[0]
"While this is comparable to the dynamic programming performance, the WFST-based method significantly outperforms this in case of multialignments for cross-narrative temporal ordering.
",7 Discussion,[0],[0]
Performance and error analysis.,7 Discussion,[0],[0]
"We perform multi-alignments over medical event sequences for a patient, where each sequence corresponds to temporally ordered medical events in a clinical narrative generated using the ranking model described in (Raghavan et al., 2012c).",7 Discussion,[0],[0]
The accuracy of intra-narrative temporal ordering is 82.1%.,7 Discussion,[0],[0]
The errors in performing this intra-narrative ordering may propagate to the cross-narrative model resulting in reduced accuracy.,7 Discussion,[0],[0]
"This may be addressed by considering n-best temporally ordered medical event sequences, generated by the ranking process, and aligning the n-best sequences using the WFST-based framework.",7 Discussion,[0],[0]
"This could be feasible as, practically, the WFST-based method for multialignment takes only a few secs to align a pair of medical event sequences with average length 40.
",7 Discussion,[0],[0]
The accuracy of alignments across multiple medical event sequences is also affected by the error induced by the coreference and temporal relation scores.,7 Discussion,[0],[0]
"Often, insufficient temporal cues leads
3http://lpsolve.sourceforge.net/5.5/
to misclassification of events incorrectly as sharing the “simultaneous” temporal relation and often as coreferring.",7 Discussion,[0],[0]
This induces errors in the score calculation and hence the alignments.,7 Discussion,[0],[0]
"Better methods to address the challenging problem of crossdocument temporal relation learning, perhaps with the help of structured data from the patient record, could improve the accuracy of alignments.
",7 Discussion,[0],[0]
"There is no clear trend with respect to the number of medical events and narratives for a patient (Table 1.), and the alignment accuracy.",7 Discussion,[0],[0]
"In future work, it would be interesting to examine any such correlation and also study the scalability of the WFST-based method for sequence alignment on longer medical event sequences and a larger dataset of patients.",7 Discussion,[0],[0]
"Further, the WFST-based method may be used to model multi-alignment tasks in other speech and language problems as well.",7 Discussion,[0],[0]
We propose a novel framework for aligning medical event sequences across clinical narratives based on coreference and temporal relation information using cascaded WFSTs.,8 Conclusion,[0],[0]
FSTs provide a convenient and flexible framework to model sequences of temporally ordered medical events and compose them into a combined graph representation.,8 Conclusion,[0],[0]
Decoding this graph allows us to jointly maximize coreference as well as temporal relation probabilities to derive a timeline of the most likely temporal ordering of medical events.,8 Conclusion,[0],[0]
This approach to aligning multiple sequences of medical events significantly outperforms other approaches such as dynamic programming.,8 Conclusion,[0],[0]
"Moreover, we demonstrate the importance of learning temporal relations for the task timeline generation from across multiple clinical narratives by empirically proving that decoding using both coreference and temporal relation scores is far more accurate than decoding with only coreference scores.",8 Conclusion,[0],[0]
The project was supported by Award Number Grant R01LM011116 from the National Library of Medicine.,Acknowledgments,[0],[0]
The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Library of Medicine or the National Institutes of Health.,Acknowledgments,[0],[0]
The authors would like to thank Yanzhang He for his input on the WFST-based model.,Acknowledgments,[0],[0]
Cross-narrative temporal ordering of medical events is essential to the task of generating a comprehensive timeline over a patient’s history.,abstractText,[0],[0]
"We address the problem of aligning multiple medical event sequences, corresponding to different clinical narratives, comparing the following approaches: (1) A novel weighted finite state transducer representation of medical event sequences that enables composition and search for decoding, and (2) Dynamic programming with iterative pairwise alignment of multiple sequences using global and local alignment algorithms.",abstractText,[0],[0]
The cross-narrative coreference and temporal relation weights used in both these approaches are learned from a corpus of clinical narratives.,abstractText,[0],[0]
We present results using both approaches and observe that the finite state transducer approach performs performs significantly better than the dynamic programming one by 6.8% for the problem of multiple-sequence alignment.,abstractText,[0],[0]
Cross-narrative temporal ordering of medical events,title,[0],[0]
Relation extraction has made great strides in newswire and Web domains.,1 Introduction,[0],[0]
"Recently, there has
∗",1 Introduction,[0],[0]
"This research was conducted when the authors were at Microsoft Research.
been increasing interest in applying relation extraction to high-value domains such as biomedicine.",1 Introduction,[0],[0]
"The advent of $1000 human genome1 heralds the dawn of precision medicine, but progress in personalized cancer treatment has been hindered by the arduous task of interpreting genomic data using prior knowledge.",1 Introduction,[0],[0]
"For example, given a tumor sequence, a molecular tumor board needs to determine which genes and mutations are important, and what drugs are available to treat them.",1 Introduction,[0],[0]
"Already the research literature has a wealth of relevant knowledge, and it is growing at an astonishing rate.",1 Introduction,[0],[0]
"PubMed2, the online repository of biomedical articles, adds two new papers per minute, or one million each year.",1 Introduction,[0],[0]
"It is thus imperative to advance relation extraction for machine reading.
",1 Introduction,[0],[0]
"In the vast literature on relation extraction, past work focused primarily on binary relations in single sentences, limiting the available information.",1 Introduction,[0],[0]
"Consider the following example: “The deletion mutation on exon-19 of EGFR gene was present in 16 patients, while the L858E point mutation on exon-21 was noted in 10.",1 Introduction,[0],[0]
All patients were treated with gefitinib and showed a partial response.”.,1 Introduction,[0],[0]
"Collectively, the two sentences convey the fact that there is a ternary interaction between the three entities in bold, which is not expressed in either sentence alone.",1 Introduction,[0],[0]
"Namely, tumors with L858E mutation in EGFR gene can be treated with gefitinib.",1 Introduction,[0],[0]
Extracting such knowledge clearly requires moving beyond binary relations and single sentences.,1 Introduction,[0],[0]
N -ary relations and cross-sentence extraction have received relatively little attention in the past.,1 Introduction,[0],[0]
"Prior
1http://www.illumina.com/systems/ hiseq-x-sequencing-system.html
2https://www.ncbi.nlm.nih.gov/pubmed
ar X
iv :1
70 8.
03 74
3v 1
[ cs
.C",1 Introduction,[0],[0]
"L
] 1
2 A
ug 2
work on n-ary relation extraction focused on single sentences (Palmer et al., 2005; McDonald et al., 2005) or entity-centric attributes that can be extracted largely independently (Chinchor, 1998; Surdeanu and Heng, 2014).",1 Introduction,[0],[0]
"Prior work on cross-sentence extraction often used coreference to gain access to arguments in a different sentence (Gerber and Chai, 2010; Yoshikawa et al., 2011), without truly modeling inter-sentential relational patterns.",1 Introduction,[0],[0]
(See Section 7 for a more detailed discussion.),1 Introduction,[0],[0]
"A notable exception is Quirk and Poon (2017), which applied distant supervision to general cross-sentence relation extraction, but was limited to binary relations.
",1 Introduction,[0],[0]
"In this paper, we explore a general framework for cross-sentence n-ary relation extraction, based on graph long short-term memory networks (graph LSTMs).",1 Introduction,[0],[0]
"By adopting the graph formulation, our framework subsumes prior approaches based on chain or tree LSTMs, and can incorporate a rich set of linguistic analyses to aid relation extraction.",1 Introduction,[0],[0]
"Relation classification takes as input the entity representations learned from the entire text, and can be easily extended for arbitrary relation arity n. This approach also facilitates joint learning with kindred relations where the supervision signal is more abundant.
",1 Introduction,[0],[0]
We conducted extensive experiments on two important domains in precision medicine.,1 Introduction,[0],[0]
"In both distant supervision and supervised learning settings, graph LSTMs that encode rich linguistic knowledge outperformed other neural network variants, as well as a well-engineered feature-based classifier.",1 Introduction,[0],[0]
Multitask learning with sub-relations led to further improvement.,1 Introduction,[0],[0]
"Syntactic analysis conferred a significant benefit to the performance of graph LSTMs, especially when syntax accuracy was high.
",1 Introduction,[0],[0]
"In the molecular tumor board domain, PubMedscale extraction using distant supervision from a
small set of known interactions produced orders of magnitude more knowledge, and cross-sentence extraction tripled the yield compared to single-sentence extraction.",1 Introduction,[0],[0]
Manual evaluation verified that the accuracy is high despite the lack of annotated examples.,1 Introduction,[0],[0]
"Let e1, · · · , em be entity mentions in text T .",2 Cross-sentence n-ary relation extraction,[0],[0]
"Relation extraction can be formulated as a classification problem of determining whether a relation R holds for e1, · · · , em in T .",2 Cross-sentence n-ary relation extraction,[0],[0]
"For example, given a cancer patient with mutation v in gene g, a molecular tumor board seeks to find if this type of cancer would respond to drug d. Literature with such knowledge has been growing rapidly; we can help the tumor board by checking if the Respond relation holds for the (d, g, v) triple.
",2 Cross-sentence n-ary relation extraction,[0],[0]
"Traditional relation extraction methods focus on binary relations where all entities occur in the same sentence (i.e., m = 2 and T is a sentence), and cannot handle the aforementioned ternary relations.",2 Cross-sentence n-ary relation extraction,[0],[0]
"Moreover, as we focus on more complex relations and n increases, it becomes increasingly rare that the related entities will be contained entirely in a single sentence.",2 Cross-sentence n-ary relation extraction,[0],[0]
"In this paper, we generalize extraction to cross-sentence, n-ary relations, where m > 2 and T can contain multiple sentences.",2 Cross-sentence n-ary relation extraction,[0],[0]
"As will be shown in our experiments section, n-ary relations are crucial for high-value domains such as biomedicine, and expanding beyond the sentence boundary enables the extraction of more knowledge.
",2 Cross-sentence n-ary relation extraction,[0],[0]
"In the standard binary-relation setting, the dominant approaches are generally defined in terms of the shortest dependency path between the two entities in question, either by deriving rich features from the path or by modeling it using deep neural
networks.",2 Cross-sentence n-ary relation extraction,[0],[0]
"Generalizing this paradigm to the n-ary setting is challenging, as there are ( n 2 ) paths.",2 Cross-sentence n-ary relation extraction,[0],[0]
"One apparent solution is inspired by Davidsonian semantics: first, identify a single trigger phrase that signifies the whole relation, then reduce the n-ary relation to n binary relations between the trigger and an argument.",2 Cross-sentence n-ary relation extraction,[0],[0]
"However, challenges remain.",2 Cross-sentence n-ary relation extraction,[0],[0]
"It is often hard to specify a single trigger, as the relation is manifested by several words, often not contiguous.",2 Cross-sentence n-ary relation extraction,[0],[0]
"Moreover, it is expensive and time-consuming to annotate training examples, especially if triggers are required, as is evident in prior annotation efforts such as GENIA (Kim et al., 2009).",2 Cross-sentence n-ary relation extraction,[0],[0]
"The realistic and widely adopted paradigm is to leverage indirect supervision, such as distant supervision (Craven and Kumlien, 1999; Mintz et al., 2009), where triggers are not available.
",2 Cross-sentence n-ary relation extraction,[0],[0]
"Additionally, lexical and syntactic patterns signifying the relation will be sparse.",2 Cross-sentence n-ary relation extraction,[0],[0]
"To handle such sparsity, traditional feature-based approaches require extensive engineering and large data.",2 Cross-sentence n-ary relation extraction,[0],[0]
"Unfortunately, this challenge becomes much more severe in crosssentence extraction when the text spans multiple sentences.
",2 Cross-sentence n-ary relation extraction,[0],[0]
"To overcome these challenges, we explore a general relation extraction framework based on graph LSTMs.",2 Cross-sentence n-ary relation extraction,[0],[0]
"By learning a continuous representation for words and entities, LSTMs can handle sparsity effectively without requiring intense feature engineering.",2 Cross-sentence n-ary relation extraction,[0],[0]
"The graph formulation subsumes prior LSTM approaches based on chains or trees, and can incorporate rich linguistic analyses.
",2 Cross-sentence n-ary relation extraction,[0],[0]
This approach also opens up opportunities for joint learning with related relations.,2 Cross-sentence n-ary relation extraction,[0],[0]
"For example, the Response relation over d, g, v also implies a binary sub-relation over drug d and mutation v, with the gene underspecified.",2 Cross-sentence n-ary relation extraction,[0],[0]
"Even with distant supervision, the supervision signal for n-ary relations will likely be sparser than their binary sub-relations.",2 Cross-sentence n-ary relation extraction,[0],[0]
Our approach makes it very easy to use multi-task learning over both the n-ary relations and their sub-relations.,2 Cross-sentence n-ary relation extraction,[0],[0]
Learning a continuous representation can be effective for dealing with lexical and syntactic sparsity.,3 Graph LSTMs,[0],[0]
"For sequential data such as text, recurrent neural networks (RNNs) are quite popular.",3 Graph LSTMs,[0],[0]
"They resemble hidden
Markov models (HMMs), except that discrete hidden states are replaced with continuous vectors, and emission and transition probabilities with neural networks.",3 Graph LSTMs,[0],[0]
"Conventional RNNs with sigmoid units suffer from gradient diffusion or explosion, making training very difficult (Bengio et al., 1994; Pascanu et al., 2013).",3 Graph LSTMs,[0],[0]
"Long short-term memory (LSTMs) (Hochreiter and Schmidhuber, 1997) combats these problems by using a series of gates (input, forget and output) to avoid amplifying or suppressing gradients during backpropagation.",3 Graph LSTMs,[0],[0]
"Consequently, LSTMs are much more effective in capturing long-distance dependencies, and have been applied to a variety of NLP tasks.",3 Graph LSTMs,[0],[0]
"However, most approaches are based on linear chains and only explicitly model the linear context, which ignores a variety of linguistic analyses, such as syntactic and discourse dependencies.
",3 Graph LSTMs,[0],[0]
"In this section, we propose a general framework that generalizes LSTMs to graphs.",3 Graph LSTMs,[0],[0]
"While there is some prior work on learning tree LSTMs (Tai et al., 2015; Miwa and Bansal, 2016), to the best of our knowledge, graph LSTMs have not been applied to any NLP task yet.",3 Graph LSTMs,[0],[0]
Figure 2 shows the architecture of this approach.,3 Graph LSTMs,[0],[0]
The input layer is the word embedding of input text.,3 Graph LSTMs,[0],[0]
Next is the graph LSTM which learns a contextual representation for each word.,3 Graph LSTMs,[0],[0]
"For the entities in question, their contextual representations are concatenated and become the input to the relation classifiers.",3 Graph LSTMs,[0],[0]
"For a multi-word entity, we simply used the average of its word representations and leave the exploration of more sophisticated aggregation approaches to future work.",3 Graph LSTMs,[0],[0]
The layers are trained jointly with backpropagation.,3 Graph LSTMs,[0],[0]
"This framework is
agnostic to the choice of classifiers.",3 Graph LSTMs,[0],[0]
"Jointly designing classifiers with graph LSTMs would be interesting future work.
",3 Graph LSTMs,[0],[0]
At the core of the graph LSTM is a document graph that captures various dependencies among the input words.,3 Graph LSTMs,[0],[0]
"By choosing what dependencies to include in the document graph, graph LSTMs naturally subsumes linear-chain or tree LSTMs.
",3 Graph LSTMs,[0],[0]
"Compared to conventional LSTMs, the graph formulation presents new challenges.",3 Graph LSTMs,[0],[0]
"Due to potential cycles in the graph, a straightforward implementation of backpropagation might require many iterations to reach a fixed point.",3 Graph LSTMs,[0],[0]
"Moreover, in the presence of a potentially large number of edge types (adjacent-word, syntactic dependency, etc.), parametrization becomes a key problem.
",3 Graph LSTMs,[0],[0]
"In the remainder of this section, we first introduce the document graph and show how to conduct backpropagation in graph LSTMs.",3 Graph LSTMs,[0],[0]
We then discuss two strategies for parametrizing the recurrent units.,3 Graph LSTMs,[0],[0]
"Finally, we show how to conduct multi-task learning with this framework.",3 Graph LSTMs,[0],[0]
"To model various dependencies from linguistic analysis at our disposal, we follow Quirk and Poon (2017) and introduce a document graph to capture intra- and inter-sentential dependencies.",3.1 Document Graph,[0],[0]
"A document graph consists of nodes that represent words and edges that represent various dependencies such as linear context (adjacent words), syntactic dependencies, and discourse relations (Lee et al., 2013; Xue et al., 2015).",3.1 Document Graph,[0],[0]
"Figure 1 shows the document graph for our running example; this instance suggests that tumors with L858E mutation in EGFR gene responds to the drug gefitinib.
",3.1 Document Graph,[0],[0]
This document graph acts as the backbone upon which a graph LSTM is constructed.,3.1 Document Graph,[0],[0]
"If it con-
tains only edges between adjacent words, we recover linear-chain LSTMs.",3.1 Document Graph,[0],[0]
"Similarly, other prior LSTM approaches can be captured in this framework by restricting edges to those in the shortest dependency path or the parse tree.",3.1 Document Graph,[0],[0]
Conventional LSTMs are essentially very deep feedforward neural networks.,3.2 Backpropagation in Graph LSTMs,[0],[0]
"For example, a left-to-right linear LSTM has one hidden vector for each word.",3.2 Backpropagation in Graph LSTMs,[0],[0]
This vector is generated by a neural network (recurrent unit) that takes as input the embedding of the given word and the hidden vector of the previous word.,3.2 Backpropagation in Graph LSTMs,[0],[0]
"In discriminative learning, these hidden vectors then serve as input for the end classifiers, from which gradients are backpropagated through the whole network.
",3.2 Backpropagation in Graph LSTMs,[0],[0]
"Generalizing such a strategy to graphs with cycles typically requires unrolling recurrence for a number of steps (Scarselli et al., 2009; Li et al., 2016; Liang et al., 2016).",3.2 Backpropagation in Graph LSTMs,[0],[0]
"Essentially, a copy of the graph is created for each step that serves as input for the next.",3.2 Backpropagation in Graph LSTMs,[0],[0]
"The result is a feed-forward neural network through time, and backpropagation is conducted accordingly.
",3.2 Backpropagation in Graph LSTMs,[0],[0]
"In principle, we could adopt the same strategy.",3.2 Backpropagation in Graph LSTMs,[0],[0]
"Effectively, gradients are backpropagated in a manner similar to loopy belief propagation (LBP).",3.2 Backpropagation in Graph LSTMs,[0],[0]
"However, this makes learning much more expensive as each update step requires multiple iterations of backpropagation.",3.2 Backpropagation in Graph LSTMs,[0],[0]
"Moreover, loopy backpropagation could suffer from the same problems encountered to in LBP, such as oscillation or failure to converge.
",3.2 Backpropagation in Graph LSTMs,[0],[0]
"We observe that dependencies such as coreference and discourse relations are generally sparse, so the backbone of a document graph consists of the linear chain and the syntactic dependency tree.",3.2 Backpropagation in Graph LSTMs,[0],[0]
"As in belief propagation, such structures can be leveraged to make backpropagation more efficient by replac-
ing synchronous updates, as in the unrolling strategy, with asynchronous updates, as in linear-chain LSTMs.",3.2 Backpropagation in Graph LSTMs,[0],[0]
"This opens up opportunities for a variety of strategies in ordering backpropagation updates.
",3.2 Backpropagation in Graph LSTMs,[0],[0]
"In this paper, we adopt a simple strategy that performed quite well in preliminary experiments, and leave further exploration to future work.",3.2 Backpropagation in Graph LSTMs,[0],[0]
"Specifically, we partition the document graph into two directed acyclic graphs (DAGs).",3.2 Backpropagation in Graph LSTMs,[0],[0]
"One DAG contains the left-to-right linear chain, as well as other forwardpointing dependencies.",3.2 Backpropagation in Graph LSTMs,[0],[0]
The other DAG covers the right-to-left linear chain and the backward-pointing dependencies.,3.2 Backpropagation in Graph LSTMs,[0],[0]
Figure 3 illustrates this strategy.,3.2 Backpropagation in Graph LSTMs,[0],[0]
"Effectively, we partition the original graph into the forward pass (left-to-right), followed by the backward pass (right-to-left), and construct the LSTMs accordingly.",3.2 Backpropagation in Graph LSTMs,[0],[0]
"When the document graph only contains linear chain edges, the graph LSTMs is exactly a bi-directional LSTMs (BiLSTMs).",3.2 Backpropagation in Graph LSTMs,[0],[0]
"A standard LSTM unit consists of an input vector (word embedding), a memory cell and an output vector (contextual representation), as well as several gates.",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"The input gate and output gate control the information flowing into and out of the cell, whereas the forget gate can optionally remove information from the recurrent connection to a precedent unit.
",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"In linear-chain LSTMs, each unit contains only one forget gate, as it has only one direct precedent (i.e., the adjacent-word edge pointing to the previous word).",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"In graph LSTMs, however, a unit may have several precedents, including connections to the same word via different edges.",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"We thus introduce a forget gate for each precedent, similar to the approach taken by Tai et al. (2015) for tree LSTMs.
",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"Encoding rich linguistic analysis introduces many distinct edge types besides word adjacency, such as syntactic dependencies, which opens up many possibilities for parametrization.",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"This was not considered in prior syntax-aware LSTM approaches (Tai et al., 2015; Miwa and Bansal, 2016).",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"In this paper, we explore two schemes that introduce more fined-grained parameters based on the edge types.
",3.3 The Basic Recurrent Propagation Unit,[0],[0]
Full Parametrization,3.3 The Basic Recurrent Propagation Unit,[0],[0]
"Our first proposal simply introduces a different set of parameters for each edge type, with computation specified below.
",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"it = σ(Wixt + ∑
j∈P (t) U
m(t,j) i hj + bi)
ot = σ(Woxt + ∑
j∈P (t) Um(t,j)o hj + bo) c̃t = tanh(Wcxt + ∑
j∈P (t) Um(t,j)c hj + bc)
ftj = σ(Wfxt",3.3 The Basic Recurrent Propagation Unit,[0],[0]
+,3.3 The Basic Recurrent Propagation Unit,[0],[0]
"U m(t,j) f hj + bf )",3.3 The Basic Recurrent Propagation Unit,[0],[0]
ct = it c̃t,3.3 The Basic Recurrent Propagation Unit,[0],[0]
"+ ∑
j∈P (t) ftj",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"cj
ht = ot tanh(ct)
",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"As in standard chain LSTMs, xt is the input word vector for node t, ht is the hidden state vector for node t, W ’s are the input weight matrices, and b’s are the bias vectors.",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"σ, tanh, and represent the sigmoid function, the hyperbolic tangent function, and the Hadamard product (pointwise multiplication), respectively.",3.3 The Basic Recurrent Propagation Unit,[0],[0]
The main differences lie in the recurrence terms.,3.3 The Basic Recurrent Propagation Unit,[0],[0]
"In graph LSTMs, a unit might have multiple predecessors (P (t)), for each of which (j) there is a forget gate ftj , and a typed weight matrix Um(t,j), where m(t, j) signifies the connection type between t and j. The input and output gates (it, ot) depend on all predecessors, whereas the forget gate (ftj) only depends on the predecessor with which the gate is associated.",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"ct and c̃t represent intermediate computation results within the memory cell, which take into account the input and forget gates, and will be combined with output gate to produce the hidden representation ht.
",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"Full parameterization is straightforward, but it requires a large number of parameters when there are many edge types.",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"For example, there are dozens of syntactic edge types, each corresponding to a Stanford dependency label.",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"As a result, in our experiments we resort to using only the coarse-grained types: word adjacency, syntactic dependency, etc.",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"Next, we will consider a more fine-grained approach by learning an edge-type embedding.
",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"Edge-Type Embedding To reduce the number of parameters and leverage potential correlation among fine-grained edge types, we learned a lowdimensional embedding of the edge types, and conducted an outer product of the predecessor’s hidden vector and the edge-type embedding to generate a “typed hidden representation”, which is a matrix.",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"The new computation is as follows:
it = σ(Wixt + ∑
j∈P (t) Ui ×T (hj ⊗ ej) + bi)
ftj = σ(Wfxt +",3.3 The Basic Recurrent Propagation Unit,[0],[0]
Uf ×T (hj ⊗ ej) + bf ),3.3 The Basic Recurrent Propagation Unit,[0],[0]
"ot = σ(Woxt + ∑
j∈P (t)",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"Uo ×T (hj ⊗ ej) + bo) c̃t = tanh(Wcxt + ∑
j∈P (t) Uc ×T (hj ⊗ ej) + bc) ct",3.3 The Basic Recurrent Propagation Unit,[0],[0]
"= it c̃t + ∑
j∈P (t) ftj cj
ht = ot tanh(ct)
",3.3 The Basic Recurrent Propagation Unit,[0],[0]
U ’s are now l ×,3.3 The Basic Recurrent Propagation Unit,[0],[0]
"l × d tensors (l is the dimension of the hidden vector and d is the dimension for edgetype embedding), and hj ⊗ ej is a tensor product that produces an l × d matrix.",3.3 The Basic Recurrent Propagation Unit,[0],[0]
×T denotes a tensor dot product defined as T ×T,3.3 The Basic Recurrent Propagation Unit,[0],[0]
"A = ∑ d(T:,:,d · A:,d), which produces an l-dimensional vector.",3.3 The Basic Recurrent Propagation Unit,[0],[0]
The edgetype embedding ej is jointly trained with the other parameters.,3.3 The Basic Recurrent Propagation Unit,[0],[0]
The main advantages of a graph formulation are its generality and flexibility.,3.4 Comparison with Prior LSTM Approaches,[0],[0]
"As seen in Section 3.1, linear-chain LSTMs are a special case when the document graph is the linear chain of adjacent words.",3.4 Comparison with Prior LSTM Approaches,[0],[0]
"Similarly, Tree LSTMs (Tai et al., 2015) are a special case when the document graph is the parse tree.
",3.4 Comparison with Prior LSTM Approaches,[0],[0]
"In graph LSTMs, the encoding of linguistic knowledge is factored from the backpropagation strategy (Section 3.2), making it much more flexible, including introducing cycles.",3.4 Comparison with Prior LSTM Approaches,[0],[0]
"For example, Miwa and Bansal (2016) conducted joint entity and binary relation extraction by stacking a LSTM for relation extraction on top of another LSTM for entity recognition.",3.4 Comparison with Prior LSTM Approaches,[0],[0]
"In graph LSTMs, the two can be combined seamlessly using a document graph comprising both the word-adjacency chain and the dependency path between the two entities.
",3.4 Comparison with Prior LSTM Approaches,[0],[0]
The document graph can also incorporate other linguistic information.,3.4 Comparison with Prior LSTM Approaches,[0],[0]
"For example, coreference and discourse parsing are intuitively relevant for cross-sentence relation extraction.",3.4 Comparison with Prior LSTM Approaches,[0],[0]
"Although existing systems have not yet been shown to improve crosssentence relation extraction (Quirk and Poon, 2017), it remains an important future direction to explore incorporating such analyses, especially after adapting them to the biomedical domains (Bell et al., 2016).",3.4 Comparison with Prior LSTM Approaches,[0],[0]
"Multi-task learning has been shown to be beneficial in training neural networks (Caruana, 1998; Collobert and Weston, 2008; Peng and Dredze, 2016).",3.5 Multi-task Learning with Sub-relations,[0],[0]
"By learning contextual entity representations, our framework makes it straightforward to conduct multi-task learning.",3.5 Multi-task Learning with Sub-relations,[0],[0]
The only change is to add a separate classifier for each related auxiliary relation.,3.5 Multi-task Learning with Sub-relations,[0],[0]
"All classifiers share the same graph LSTMs representation learner and word embeddings, and can potentially help each other by pooling their supervision signals.
",3.5 Multi-task Learning with Sub-relations,[0],[0]
"In the molecular tumor board domain, we applied this paradigm to joint learning of both the ternary relation (drug-gene-mutation) and its binary sub-relation (drug-mutation).",3.5 Multi-task Learning with Sub-relations,[0],[0]
Experiment results show that this provides significant gains in both tasks.,3.5 Multi-task Learning with Sub-relations,[0],[0]
"We implemented our methods using the Theano library (Theano Development Team, 2016).",4 Implementation Details,[0],[0]
We used logistic regression for our relation classifiers.,4 Implementation Details,[0],[0]
Hyper parameters were set based on preliminary experiments on a small development dataset.,4 Implementation Details,[0],[0]
Training was done using mini-batched stochastic gradient descent (SGD) with batch size 8.,4 Implementation Details,[0],[0]
"We used a learning rate of 0.02 and trained for at most 30 epochs, with early stopping based on development data (Caruana et al., 2001; Graves et al., 2013).",4 Implementation Details,[0],[0]
"The dimension for the hidden vectors in LSTM units was set to 150, and the dimension for the edge-type embedding was set to 3.",4 Implementation Details,[0],[0]
"The word embeddings were initialized with the publicly available 100-dimensional GloVe word vectors trained on 6 billion words from Wikipedia and web text3 (Pennington et al., 2014).",4 Implementation Details,[0],[0]
"Other model parameters were initialized with random samples drawn uniformly from the range [−1, 1].
",4 Implementation Details,[0],[0]
"In multi-task training, we alternated among all tasks, each time passing through all data for one task4, and updating the parameters accordingly.",4 Implementation Details,[0],[0]
"This was repeated for 30 epochs.
3http://nlp.stanford.edu/projects/glove/ 4However, drug-gene pairs have much more data, so we subsampled the instances down to the same size as the main n-ary relation task.",4 Implementation Details,[0],[0]
"Our main experiments focus on extracting ternary interactions over drugs, genes and mutations, which is important for molecular tumor boards.",5 Domain: Molecular Tumor Boards,[0],[0]
A druggene-mutation interaction is broadly construed as an association between the drug efficacy and the mutation in the given gene.,5 Domain: Molecular Tumor Boards,[0],[0]
There is no annotated dataset for this problem.,5 Domain: Molecular Tumor Boards,[0],[0]
"However, due to the importance of such knowledge, oncologists have been painstakingly curating known relations from reading papers.",5 Domain: Molecular Tumor Boards,[0],[0]
"Such a manual approach cannot keep up with the rapid growth of the research literature, and the coverage is generally sparse and not up to date.",5 Domain: Molecular Tumor Boards,[0],[0]
"However, the curated knowledge can be used for distant supervision.",5 Domain: Molecular Tumor Boards,[0],[0]
"We obtained biomedical literature from PubMed Central5, consisting of approximately one million fulltext articles as of 2015.",5.1 Datasets,[0],[0]
Note that only a fraction of papers contain knowledge about drug-gene-mutation interactions.,5.1 Datasets,[0],[0]
Extracting such knowledge from the vast body of biomedical papers is exactly the challenge.,5.1 Datasets,[0],[0]
"As we will see in later subsections, distant supervision enables us to generate a sizable training set from a small number of manually curated facts, and the learned model was able to extract orders of magnitude more facts.",5.1 Datasets,[0],[0]
"In future work, we will explore incorporating more known facts for distant supervision and extracting from more full-text articles.
",5.1 Datasets,[0],[0]
"We conducted tokenization, part-of-speech tagging, and syntactic parsing using SPLAT (Quirk et al., 2012), and obtained Stanford dependencies (de Marneffe et al., 2006) using Stanford CoreNLP",5.1 Datasets,[0],[0]
"(Manning et al., 2014).",5.1 Datasets,[0],[0]
"We used the entity taggers from Literome (Poon et al., 2014) to identify drug, gene and mutation mentions.
",5.1 Datasets,[0],[0]
"We used the Gene Drug Knowledge Database (GDKD) (Dienstmann et al., 2015) and the Clinical Interpretations of Variants In Cancer (CIVIC) knowledge base6 for distant supervision.",5.1 Datasets,[0],[0]
"The knowledge bases distinguish fine-grained interaction types, which we do not use in this paper.
",5.1 Datasets,[0],[0]
5http://www.ncbi.nlm.nih.gov/pmc/ 6http://civic.genome.wustl.edu,5.1 Datasets,[0],[0]
"After identifying drug, gene and mutation mentions in the text, co-occurring triples with known interactions were chosen as positive examples.",5.2 Distant Supervision,[0],[0]
"However, unlike the single-sentence setting in standard distant supervision, care must be taken in selecting the candidates.",5.2 Distant Supervision,[0],[0]
"Since the triples can reside in different sentences, an unrestricted selection of text spans would risk introducing many obviously wrong examples.",5.2 Distant Supervision,[0],[0]
"We thus followed Quirk and Poon (2017) in restricting the candidates to those occurring in a minimal span, i.e., we retain a candidate only if is no other co-occurrence of the same entities in an overlapping text span with a smaller number of consecutive sentences.",5.2 Distant Supervision,[0],[0]
"Furthermore, we avoid picking unlikely candidates where the triples are far apart in the document.",5.2 Distant Supervision,[0],[0]
"Specifically, we considered entity triples within K consecutive sentences, ignoring paragraph boundaries.",5.2 Distant Supervision,[0],[0]
K = 1 corresponds to the baseline of extraction within single sentences.,5.2 Distant Supervision,[0],[0]
"We explored K ≤ 3, which captured a large fraction of candidates without introducing many unlikely ones.
",5.2 Distant Supervision,[0],[0]
Only 59 distinct drug-gene-mutation triples from the knowledge bases were matched in the text.,5.2 Distant Supervision,[0],[0]
"Even from such a small set of unique triples, we obtained 3,462 ternary relation instances that can serve as positive examples.",5.2 Distant Supervision,[0],[0]
"For multi-task learning, we also considered drug-gene and drug-mutation sub-relations, which yielded 137,469 drug-gene and 3,192 drugmutation relation instances as positive examples.
",5.2 Distant Supervision,[0],[0]
"We generate negative examples by randomly sampling co-occurring entity triples without known interactions, subject to the same restrictions above.",5.2 Distant Supervision,[0],[0]
We sampled the same number as positive examples to obtain a balanced dataset7.,5.2 Distant Supervision,[0],[0]
"To compare the various models in our proposed framework, we conducted five-fold cross-validation, treating the positive and negative examples from distant supervision as gold annotation.",5.3 Automatic Evaluation,[0],[0]
"To avoid traintest contamination, all examples from a document were assigned to the same fold.",5.3 Automatic Evaluation,[0],[0]
"Since our datasets are balanced by construction, we simply report average test accuracy on held-out folds.",5.3 Automatic Evaluation,[0],[0]
"Obviously, the
7We will release the dataset at http://hanover.azurewebsites.net.
results could be noisy (e.g., entity triples not known to have an interaction might actually have one), but this evaluation is automatic and can quickly evaluate the impact of various design choices.
",5.3 Automatic Evaluation,[0],[0]
We evaluated two variants of graph LSTMs: “Graph LSTM-FULL” with full parametrization and “Graph LSTM-EMBED” with edge-type embedding.,5.3 Automatic Evaluation,[0],[0]
"We compared graph LSTMs with three strong baseline systems: a well-engineered feature-based classifier (Quirk and Poon, 2017), a convolutional neural network (CNN)",5.3 Automatic Evaluation,[0],[0]
"(Zeng et al., 2014; Santos et al., 2015; Wang et al., 2016), and a bi-directional LSTM (BiLSTM).",5.3 Automatic Evaluation,[0],[0]
"Following Wang et al. (2016), we used input attention for the CNN and a input window size of 5.",5.3 Automatic Evaluation,[0],[0]
Quirk and Poon (2017) only extracted binary relations.,5.3 Automatic Evaluation,[0],[0]
"We extended it to ternary relations by deriving features for each entity pair (with added annotation to signify the two entity types), and pooling the features
from all pairs.",5.3 Automatic Evaluation,[0],[0]
"For binary relation extraction, prior syntax-aware approaches are directly applicable.",5.3 Automatic Evaluation,[0],[0]
"So we also compared with a state-of-the-art tree LSTM system (Miwa and Bansal, 2016) and a BiLSTM on the shortest dependency path between the two entities (BiLSTM-Shortest-Path) (Xu et al., 2015b).
",5.3 Automatic Evaluation,[0],[0]
"Table 1 shows the results for cross-sentence, ternary relation extraction.",5.3 Automatic Evaluation,[0],[0]
"All neural-network based models outperformed the feature-based classifier, illustrating their advantage in handling sparse linguistic patterns without requiring intense feature engineering.",5.3 Automatic Evaluation,[0],[0]
"All LSTMs significantly outperformed CNN in the cross-sentence setting, verifying the importance in capturing long-distance dependencies.
",5.3 Automatic Evaluation,[0],[0]
"The two variants of graph LSTMs perform on par with each other, though Graph LSTM-FULL has a small advantage, suggesting that further exploration of parametrization schemes could be beneficial.",5.3 Automatic Evaluation,[0],[0]
"In particular, the edge-type embedding might improve by pretraining on unlabeled text with syntactic parses.
",5.3 Automatic Evaluation,[0],[0]
"Both graph variants significantly outperformed BiLSTMs (p < 0.05 by McNemar’s chi-square test), though the difference is small.",5.3 Automatic Evaluation,[0],[0]
This result is intriguing.,5.3 Automatic Evaluation,[0],[0]
"In Quirk and Poon (2017), the best system incorporated syntactic dependencies and outperformed the linear-chain variant (Base) by a large margin.",5.3 Automatic Evaluation,[0],[0]
"So why didn’t graph LSTMs make an equally substantial gain by modeling syntactic dependencies?
",5.3 Automatic Evaluation,[0],[0]
One reason is that linear-chain LSTMs can already captured some of the long-distance dependencies available in syntactic parses.,5.3 Automatic Evaluation,[0],[0]
"BiLSTMs substantially outperformed the feature-based classifier, even without explicit modeling of syntactic dependencies.",5.3 Automatic Evaluation,[0],[0]
"The gain cannot be entirely attributed to word embedding as LSTMs also outperformed CNNs.
",5.3 Automatic Evaluation,[0],[0]
Another reason is that syntactic parsing is less accurate in the biomedical domain.,5.3 Automatic Evaluation,[0],[0]
"Parse errors confuse the graph LSM learner, limiting the potential for gain.",5.3 Automatic Evaluation,[0],[0]
"In Section 6, we show supporting evidence in a domain when gold parses are available.
",5.3 Automatic Evaluation,[0],[0]
"We also reported accuracy on instances within single sentences, which exhibited a broadly similar set of trends.",5.3 Automatic Evaluation,[0],[0]
"Note that single-sentence and crosssentence accuracies are not directly comparable, as the test sets are different (one subsumes the other).
",5.3 Automatic Evaluation,[0],[0]
We conducted the same experiments on the binary sub-relation between drug-mutation pairs.,5.3 Automatic Evaluation,[0],[0]
"Table 2
shows the results, which are similar to the ternary case: Graph LSTM-FULL consistently performed the best for both single sentence and cross-sentence instances.",5.3 Automatic Evaluation,[0],[0]
"BiLSTMs on the shortest path substantially underperformed BiLSTMs or graph LSTMs, losing between 4-5 absolute points in accuracy, which could be attributed to the lower parsing quality in the biomedical domain.",5.3 Automatic Evaluation,[0],[0]
"Interestingly, the state-of-the-art tree LSTMs (Miwa and Bansal, 2016) also underperformed graph LSTMs, even though they encoded essentially the same linguistic structures (word adjacency and syntactic dependency).",5.3 Automatic Evaluation,[0],[0]
"We attributed the gain to the fact that Miwa and Bansal (2016) used separate LSTMs for the linear chain and the dependency tree, whereas graph LSTMs learned a single representation for both.
",5.3 Automatic Evaluation,[0],[0]
"To evaluate whether joint learning with subrelations can help, we conducted multi-task learning using Graph LSTM-FULL to jointly train extractors for both the ternary interaction and the drug-mutation, drug-gene sub-relations.",5.3 Automatic Evaluation,[0],[0]
Table 3 shows the results.,5.3 Automatic Evaluation,[0],[0]
Multi-task learning resulted in a significant gain for both the ternary interaction and the drug-mutation interaction.,5.3 Automatic Evaluation,[0],[0]
"Interestingly, the advantage of graph LSTMs over BiLSTMs is reduced with multi-task learning, suggesting that with more supervision signal, even linear-chain LSTMs can learn to capture long-range dependencies that are were made evident by parse features in graph LSTMs.",5.3 Automatic Evaluation,[0],[0]
"Note that there are many more instances for drug-gene interaction than others, so we only sampled a subset of comparable size.",5.3 Automatic Evaluation,[0],[0]
"Therefore, we do not evaluate the performance gain for drug-gene interaction, as in practice, one would simply learn from all available data, and the sub-sampled results are not competitive.
",5.3 Automatic Evaluation,[0],[0]
We included coreference and discourse relations in our document graph.,5.3 Automatic Evaluation,[0],[0]
"However, we didn’t observe any significant gains, similar to the observation in
Quirk and Poon (2017).",5.3 Automatic Evaluation,[0],[0]
We leave further exploration to future work.,5.3 Automatic Evaluation,[0],[0]
Our ultimate goal is to extract all knowledge from available text.,5.4 PubMed-Scale Extraction,[0],[0]
"We thus retrained our model using the best system from automatic evaluation (i.e., Graph LSTM-FULL) on all available data.",5.4 PubMed-Scale Extraction,[0],[0]
"The resulting model was then used to extract relations from all PubMed Central articles.
",5.4 PubMed-Scale Extraction,[0],[0]
Table 4 shows the number of candidates and extracted interactions.,5.4 PubMed-Scale Extraction,[0],[0]
"With as little as 59 unique druggene-mutation triples from the two databases8, we learned to extract orders of magnitude more unique interactions.",5.4 PubMed-Scale Extraction,[0],[0]
"The results also highlight the benefit of cross-sentence extraction, which yields 3 to 5 times more relations than single-sentence extraction.
",5.4 PubMed-Scale Extraction,[0],[0]
"Table 5 conducts a similar comparison on unique number of drugs, genes, and mutations.",5.4 PubMed-Scale Extraction,[0],[0]
"Again, machine reading covers far more unique entities, especially with cross-sentence extraction.",5.4 PubMed-Scale Extraction,[0],[0]
"Our automatic evaluations are useful for comparing competing approaches, but may not reflect the true classifier precision as the labels are noisy.",5.5 Manual Evaluation,[0],[0]
"Therefore, we randomly sampled extracted relation instances and asked three researchers knowledgeable in precision medicine to evaluate their correctness.",5.5 Manual Evaluation,[0],[0]
"For each instance, the annotators were presented with the provenance: sentences with the drug, gene, and mutation highlighted.",5.5 Manual Evaluation,[0],[0]
"The annotators determined in
8There are more in the databases, but these are the only ones for which we found matching instances in the text.",5.5 Manual Evaluation,[0],[0]
"In future work, we will explore various ways to increase the number, e.g., by matching underspecified drug classes to specific drugs.
",5.5 Manual Evaluation,[0],[0]
each case whether this instance implied that the given entities were related.,5.5 Manual Evaluation,[0],[0]
"Note that evaluation does not attempt to identify whether the relationships are true or replicated in follow-up papers; rather, it focuses on whether the relationships are entailed by the text.
",5.5 Manual Evaluation,[0],[0]
We focused our evaluation efforts on the crosssentence ternary-relation setting.,5.5 Manual Evaluation,[0],[0]
"We considered three probability thresholds: 0.9 for a high-precision but potentially low-recall setting, 0.5, and a random sample of all candidates.",5.5 Manual Evaluation,[0],[0]
"In each case, 150 instances were selected for a total of 450 annotations.",5.5 Manual Evaluation,[0],[0]
"A subset of 150 instances were reviewed by two annotators, and the inter-annotator agreement was 88%.
",5.5 Manual Evaluation,[0],[0]
"Table 6 shows that the classifier indeed filters out a large portion of potential candidates, with estimated instance accuracy of 64% at the threshold of 0.5, and 75% at 0.9.",5.5 Manual Evaluation,[0],[0]
"Interestingly, LSTMs are effective at screening out many entity mention errors, presumably because they include broad contextual features.",5.5 Manual Evaluation,[0],[0]
"We also conducted experiments on extracting genetic pathway interactions using the GENIA Event Extraction dataset (Kim et al., 2009).",6 Domain: Genetic Pathways,[0],[0]
"This dataset contains gold syntactic parses for the sentences, which offered a unique opportunity to investigate the impact of syntactic analysis on graph LSTMs.",6 Domain: Genetic Pathways,[0],[0]
"It also allowed us to test our framework in supervised learning.
",6 Domain: Genetic Pathways,[0],[0]
"The original shared task evaluated on complex, nested events for nine event types, many of which are unary relations (Kim et al., 2009).",6 Domain: Genetic Pathways,[0],[0]
"Following Poon et al. (2015), we focused on gene regulation and reduced it to binary-relation classification for headto-head comparison.",6 Domain: Genetic Pathways,[0],[0]
"We followed their experimental protocol by sub-sampling negative examples to be about three times of positive examples.
",6 Domain: Genetic Pathways,[0],[0]
"Since the dataset is not entirely balanced, we reported precision, recall, and F1.",6 Domain: Genetic Pathways,[0],[0]
We used our best performing graph LSTM from the previous experiments.,6 Domain: Genetic Pathways,[0],[0]
"By default, automatic parses were used in the document graphs, whereas in Graph LSTM (GOLD), gold parses were used instead.",6 Domain: Genetic Pathways,[0],[0]
Table 7 shows the results.,6 Domain: Genetic Pathways,[0],[0]
"Once again, despite the lack of intense feature engineering, linear-chain LSTMs performed on par with the feature-based classifier (Poon et al., 2015).",6 Domain: Genetic Pathways,[0],[0]
"Graph LSTMs exhibited a more commanding advantage over linear-chain LSTMs in this domain, substantially outperforming the latter (p < 0.01 by McNemar’s chi-square test).",6 Domain: Genetic Pathways,[0],[0]
"Most interestingly, graph LSTMs using gold parses significantly outperformed that using automatic parses, suggesting that encoding high-quality analysis is particularly beneficial.",6 Domain: Genetic Pathways,[0],[0]
Most work on relation extraction has been applied to binary relations of entities in a single sentence.,7 Related Work,[0],[0]
"We first review relevant work on the single-sentence bi-
nary relation extraction task, and then review related work on n-ary and cross-sentence relation extraction.
",7 Related Work,[0],[0]
"Binary relation extraction The traditional featurebased methods rely on carefully designed features to learn good models, and often integrate diverse sources of evidence such as word sequences and syntax context (Kambhatla, 2004; GuoDong et al., 2005; Boschee et al., 2005; Suchanek et al., 2006; Chan and Roth, 2010; Nguyen and Grishman, 2014).",7 Related Work,[0],[0]
"The kernel-based methods design various subsequence or tree kernels (Mooney and Bunescu, 2005; Bunescu and Mooney, 2005; Qian et al., 2008) to capture structured information.",7 Related Work,[0],[0]
"Recently, models based on neural networks have advanced the state of the art by automatically learning powerful feature representations (Xu et al., 2015a; Zhang et al., 2015; Santos et al., 2015; Xu et al., 2015b; Xu et al., 2016).
",7 Related Work,[0],[0]
"Most neural architectures resemble Figure 2, where there is a core representation learner (blue) that takes word embeddings as input and produces contextual entity representations.",7 Related Work,[0],[0]
Such representations are then taken by relation classifiers to produce the final predictions.,7 Related Work,[0],[0]
"Effectively representing sequences of words, both convolutional (Zeng et al., 2014; Wang et al., 2016; Santos et al., 2015) and RNN-based architectures (Zhang et al., 2015; Socher et al., 2012; Cai et al., 2016) have been successful.",7 Related Work,[0],[0]
Most of these have focused on modeling either the surface word sequences or the hierarchical syntactic structure.,7 Related Work,[0],[0]
"Miwa and Bansal (2016) proposed an architecture that benefits from both types of information, using a surface sequence layer, followed by a dependency-tree sequence layer.
",7 Related Work,[0],[0]
"N -ary relation extraction Early work on extracting relations between more than two arguments has been done in MUC-7, with a focus on fact/event extraction from news articles (Chinchor, 1998).",7 Related Work,[0],[0]
"Semantic role labeling in the Propbank (Palmer et al., 2005) or FrameNet (Baker et al., 1998) style are also instances of n-ary relation extraction, with extraction of events expressed in a single sentence.",7 Related Work,[0],[0]
"McDonald et al. (2005) extract n-ary relations in a biomedical domain, by first factoring the n-ary relation into pair-wise relations between all entity pairs, and then constructing maximal cliques of related entities.",7 Related Work,[0],[0]
"Recently, neural models have been applied to semantic role labeling (FitzGerald et al., 2015; Roth
and Lapata, 2016).",7 Related Work,[0],[0]
"These works learned neural representations by effectively decomposing the n-ary relation into binary relations between the predicate and each argument, by embedding the dependency path between each pair, or by combining features of the two using a feed-forward network.",7 Related Work,[0],[0]
"Although some re-ranking or joint inference models have been employed, the representations of the individual arguments do not influence each other.",7 Related Work,[0],[0]
"In contrast, we propose a neural architecture that jointly represents n entity mentions, taking into account long-distance dependencies and inter-sentential information.
",7 Related Work,[0],[0]
"Cross-sentence relation extraction Several relation extraction tasks have benefited from crosssentence extraction, including MUC fact and event extraction (Swampillai and Stevenson, 2011), record extraction from web pages (Wick et al., 2006), extraction of facts for biomedical domains (Yoshikawa et al., 2011), and extensions of semantic role labeling to cover implicit inter-sentential arguments (Gerber and Chai, 2010).",7 Related Work,[0],[0]
"These prior works have either relied on explicit co-reference annotation, or on the assumption that the whole document refers to a single coherent event, to simplify the problem and reduce the need for powerful representations of multi-sentential contexts of entity mentions.",7 Related Work,[0],[0]
"Recently, cross-sentence relation extraction models have been learned with distant supervision, and used integrated contextual evidence of diverse types without reliance on these assumptions (Quirk and Poon, 2017), but that work focused on binary relations only and explicitly engineered sparse indicator features.
",7 Related Work,[0],[0]
"Relation extraction using distant supervision Distant supervision has been applied to extraction of binary (Mintz et al., 2009; Poon et al., 2015) and n-ary (Reschke et al., 2014;",7 Related Work,[0],[0]
"Li et al., 2015) relations, traditionally using hand-engineered features.",7 Related Work,[0],[0]
"Neural architectures have recently been applied to distantly supervised extraction of binary relations (Zeng et al., 2015).",7 Related Work,[0],[0]
"Our work is the first to propose a neural architecture for n-ary relation extraction, where the representation of a tuple of entities is not decomposable into independent representations of the individual entities or entity pairs, and which integrates diverse information from multi-sentential context.",7 Related Work,[0],[0]
"To utilize training data more effectively, we show how multitask learning for component binary sub-relations can
improve performance.",7 Related Work,[0],[0]
"Our learned representation combines information sources within a single sentence in a more integrated and generalizable fashion than prior approaches, and can also improve performance on single-sentence binary relation extraction.",7 Related Work,[0],[0]
We explore a general framework for cross-sentence nary relation extraction based on graph LSTMs.,8 Conclusion,[0],[0]
The graph formulation subsumes linear-chain and tree LSTMs and makes it easy to incorporate rich linguistic analysis.,8 Conclusion,[0],[0]
"Experiments on biomedical domains showed that extraction beyond the sentence boundary produced far more knowledge, and encoding rich linguistic knowledge provided consistent gain.
",8 Conclusion,[0],[0]
"While there is much room to improve in both recall and precision, our results indicate that machine reading can already be useful in precision medicine.",8 Conclusion,[0],[0]
"In particular, automatically extracted facts (Section 5.4) can serve as candidates for manual curation.",8 Conclusion,[0],[0]
"Instead of scanning millions of articles to curate from scratch, human curators would just quickly vet thousands of extractions.",8 Conclusion,[0],[0]
The errors identified by curators offer direct supervision to the machine reading system for continuous improvement.,8 Conclusion,[0],[0]
"Therefore, the most important goal is to attain high recall and reasonable precision.",8 Conclusion,[0],[0]
"Our current models are already quite capable.
",8 Conclusion,[0],[0]
Future directions include: interactive learning with user feedback; improving discourse modeling in graph LSTMs; exploring other backpropagation strategies; joint learning with entity linking; applications to other domains.,8 Conclusion,[0],[0]
"We thank Daniel Fried and Ming-Wei Chang for useful discussions, as well as the anonymous reviewers and editor-in-chief Mark Johnson for their helpful comments.",Acknowledgements,[0],[0]
Past work in relation extraction has focused on binary relations in single sentences.,abstractText,[0],[0]
Recent NLP inroads in high-value domains have sparked interest in the more general setting of extracting n-ary relations that span multiple sentences.,abstractText,[0],[0]
"In this paper, we explore a general relation extraction framework based on graph long short-term memory networks (graph LSTMs) that can be easily extended to cross-sentence n-ary relation extraction.",abstractText,[0],[0]
"The graph formulation provides a unified way of exploring different LSTM approaches and incorporating various intra-sentential and intersentential dependencies, such as sequential, syntactic, and discourse relations.",abstractText,[0],[0]
"A robust contextual representation is learned for the entities, which serves as input to the relation classifier.",abstractText,[0],[0]
"This simplifies handling of relations with arbitrary arity, and enables multi-task learning with related relations.",abstractText,[0],[0]
"We evaluate this framework in two important precision medicine settings, demonstrating its effectiveness with both conventional supervised learning and distant supervision.",abstractText,[0],[0]
Cross-sentence extraction produced larger knowledge bases.,abstractText,[0],[0]
and multi-task learning significantly improved extraction accuracy.,abstractText,[0],[0]
A thorough analysis of various LSTM approaches yielded useful insight the impact of linguistic analysis on extraction accuracy.,abstractText,[0],[0]
Cross-Sentence N -ary Relation Extraction with Graph LSTMs,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 778–783 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
778",text,[0],[0]
"Stance classification is the task of automatically identifying users’ positions about a specific target from text (Mohammad et al., 2017).",1 Introduction,[1.0],"['Stance classification is the task of automatically identifying users’ positions about a specific target from text (Mohammad et al., 2017).']"
"Table 1 shows an example of this task, where the stance of the sentence is recognized as favorable on the target climate change is concern.",1 Introduction,[1.0],"['Table 1 shows an example of this task, where the stance of the sentence is recognized as favorable on the target climate change is concern.']"
"Traditionally, this task is approached by learning a target-specific classifier that is trained for prediction on the same target of interest (Hasan and Ng, 2013; Mohammad et al., 2016; Ebrahimi et al., 2016).",1 Introduction,[1.0],"['Traditionally, this task is approached by learning a target-specific classifier that is trained for prediction on the same target of interest (Hasan and Ng, 2013; Mohammad et al., 2016; Ebrahimi et al., 2016).']"
"This implies that a new classifier has to be built from scratch on a well-prepared set of ground-truth data whenever predictions are needed for an unseen target.
",1 Introduction,[0],[0]
"An alternative to this approach is to conduct a cross-target classification, where the classifier is adapted from different but related targets (Augenstein et al., 2016), which allows benefiting from the knowledge of existing targets.",1 Introduction,[0],[0]
"For example, in our project we are interested in online users’ stances on the approvals of particular mining projects in the country.",1 Introduction,[1.0],"['For example, in our project we are interested in online users’ stances on the approvals of particular mining projects in the country.']"
"It might be useful to start with a classifier that is adapted from a related target such as climate change is concern (presumably available and annotated), as in both cases
users could discuss the impacts from the targets to some common issues, such as the environment or communities.
",1 Introduction,[0.9999999946383274],"['It might be useful to start with a classifier that is adapted from a related target such as climate change is concern (presumably available and annotated), as in both cases users could discuss the impacts from the targets to some common issues, such as the environment or communities.']"
Cross-target stance classification is a more challenging task simply because the language models may not be compatible between different targets.,1 Introduction,[0],[0]
"However, for some targets that can be recognized as being related to the same and more general domains, it could be possible to generalize through certain aspects of the domains that reflect users’ major concerns.",1 Introduction,[1.0],"['However, for some targets that can be recognized as being related to the same and more general domains, it could be possible to generalize through certain aspects of the domains that reflect users’ major concerns.']"
"For example, from the following sentence, whose stance is against the approval of a mining project, “Environmentalists warn the $16 billion coal facility will damage the Great Barrier Reef”, it can be seen that both this sentence and the one in Table 1 mention the same aspect “reef destruction/damage”, which is closely related to the “environment” domain.
",1 Introduction,[1.000000009785634],"['For example, from the following sentence, whose stance is against the approval of a mining project, “Environmentalists warn the $16 billion coal facility will damage the Great Barrier Reef”, it can be seen that both this sentence and the one in Table 1 mention the same aspect “reef destruction/damage”, which is closely related to the “environment” domain.']"
"In this paper, we focus on cross-target stance classification and explore the limits of generalizing models between different but domain-related targets1.",1 Introduction,[0],[0]
"The basic idea is to learn a set of domainspecific aspects from a source target, and then apply them to prediction on a destination target.",1 Introduction,[0],[0]
"To this end, we propose CrossNet, a novel neural model that implements the above idea based on the self-attention mechanism.",1 Introduction,[0],[0]
"Our preliminary analysis shows that the proposed model can find useful domain-specific information from a stancebearing sentence and that the classification performance is improved in certain domains.
",1 Introduction,[0.9999999774889611],['Our preliminary analysis shows that the proposed model can find useful domain-specific information from a stancebearing sentence and that the classification performance is improved in certain domains.']
"1In this work, the source target is chosen based on common sense.",1 Introduction,[0],[0]
Exploring more sophisticated source target selection methods will be our future work.,1 Introduction,[0],[0]
"In this section, we introduce the proposed model, CrossNet, for cross-target stance classification.",2 Model,[1.0],"['In this section, we introduce the proposed model, CrossNet, for cross-target stance classification.']"
Figure 1 shows the architecture of CrossNet.,2 Model,[0],[0]
It consists of four layers from the Embedding Layer (bottom) to the Prediction Layer (top).,2 Model,[1.0],['It consists of four layers from the Embedding Layer (bottom) to the Prediction Layer (top).']
It works by taking a stance-bearing sentence and a target as input and yielding the predicted stance label as output.,2 Model,[1.0],['It works by taking a stance-bearing sentence and a target as input and yielding the predicted stance label as output.']
"In the following, we present the implementation of each layer in CrossNet.",2 Model,[0],[0]
"There are two inputs in CrossNet: a stance-bearing sentence P and a descriptive target T (e.g, climate change is concern in Table 1).",2.1 Embedding Layer,[0],[0]
"We use word embeddings (Mikolov et al., 2013) to represent each word in the input as a dense vector.",2.1 Embedding Layer,[0],[0]
"The output of this layer are two sequences of vectors P = {p1, ...,p|P |} and T = {t1, ..., t|T |}, where p, t are word vectors.",2.1 Embedding Layer,[0],[0]
"In this layer, we encode the contextual information in the input sentence and target.",2.2 Context Encoding Layer,[0],[0]
"We use a bi-directional Long Short-Term Memory Network (BiLSTM) (Hochreiter and Schmidhuber, 1997) to capture the left and right contexts of each word in the input.",2.2 Context Encoding Layer,[0],[0]
"Moreover, to account for the impact of the target on stance inference, we borrow the idea of conditional encoding (Augenstein et al., 2016) to model the dependency of the sentence on the target.",2.2 Context Encoding Layer,[1.0],"['Moreover, to account for the impact of the target on stance inference, we borrow the idea of conditional encoding (Augenstein et al., 2016) to model the dependency of the sentence on the target.']"
"Formally, we first use a BiLSTMT to encode the target:
",2.2 Context Encoding Layer,[0],[0]
[ −→ h Ti −→c,2.2 Context Encoding Layer,[0],[0]
"Ti ] = −−−−→ LSTMT (ti, −→ h Ti−1, −→c",2.2 Context Encoding Layer,[0],[0]
Ti−1),2.2 Context Encoding Layer,[0],[0]
"[ ←− h Ti ←−c Ti ] = ←−−−− LSTMT (ti, ←− h Ti+1, ←−c Ti+1)",2.2 Context Encoding Layer,[0],[0]
"(1)
where h ∈ Rh and c ∈",2.2 Context Encoding Layer,[0],[0]
Rh are the hidden state and cell state of LSTM.,2.2 Context Encoding Layer,[0],[0]
The symbol −→(←−) indicates the forward (backward) pass.,2.2 Context Encoding Layer,[0],[0]
"ti is the input word vector at time step i.
",2.2 Context Encoding Layer,[0],[0]
"Then, we learn a conditional encoding of the sentence P , by initializing BiLSTMP (a different BiLSTM) with the final states of BiLSTMT :
",2.2 Context Encoding Layer,[0],[0]
"[ −→ h P1 −→c P1 ] = −−−−→ LSTMP (p1, −→ h",2.2 Context Encoding Layer,[0],[0]
T|T,2.2 Context Encoding Layer,[0],[0]
"|, −→c T|T",2.2 Context Encoding Layer,[0],[0]
"|)
",2.2 Context Encoding Layer,[0],[0]
"[ ←− h P|P | ←−c P|P |] = ←−−−− LSTMP (p|P |, ←− h T1 , ←−c T1 )
(2)
It can be seen that the initialization is done by aligning the forward (backward) pass of the two BiLSTMs.",2.2 Context Encoding Layer,[0],[0]
"The output is a contextually-encoded sequence, HP = {hP1 , ...,hP|P |}, where h =",2.2 Context Encoding Layer,[0],[0]
[ −→ h ; ←− h ] ∈ R2h with [; ] as the vector concatenation operation.,2.2 Context Encoding Layer,[0],[0]
"In this layer, we implement the idea of discovering domain-specific aspects for cross-target stance inference.",2.3 Aspect Attention Layer,[0],[0]
"In particular, the key observation we make is that the domain aspects that reflect users’ major concerns are usually the core of understanding their stances, and could be mentioned by multiple users in a discussion.",2.3 Aspect Attention Layer,[0],[0]
"For example, we find that many users in our corpus mention the aspect “reef” to express their concerns about the impact of a mining project on the Great Barrier Reef.",2.3 Aspect Attention Layer,[0],[0]
"Based on this observation, the perception of the domain aspects can be boiled down to finding the sentence parts that not only carry the core idea of a stance-bearing sentence but also tend to be recurring in the corpus.
",2.3 Aspect Attention Layer,[0.9999999610475756],"['Based on this observation, the perception of the domain aspects can be boiled down to finding the sentence parts that not only carry the core idea of a stance-bearing sentence but also tend to be recurring in the corpus.']"
"First, to capture the recurrences of the domain aspects, a simple way is to make every input sentence be consumed by this layer (see Figure 1), so that the layer parameters are shared across the corpus for being stimulated by all appearances of the domain aspects.
",2.3 Aspect Attention Layer,[0],[0]
"Then, we utilize self-attention to signal the core parts of a stance-bearing sentence.",2.3 Aspect Attention Layer,[1.0],"['Then, we utilize self-attention to signal the core parts of a stance-bearing sentence.']"
"Self-attention is an attention mechanism for selecting specific parts of a sequence by relating its elements at different positions (Vaswani et al., 2017; Cheng et al., 2016).",2.3 Aspect Attention Layer,[0],[0]
"In our case, the self-attention process is based on the assumption that the core parts of a sentence are those that are compatible with the semantics of the entire sentence.",2.3 Aspect Attention Layer,[0],[0]
"To this end, we introduce a compatibility function to score the semantic compatibility between the encoded se-
quence HP and each of its hidden states hP :
ci = w > 2 σ(W1h P i + b1) + b2 (3)
where W1 ∈ Rd×2h, w2 ∈ Rd, b1 ∈ Rd, and b2 ∈ R are trainable parameters, and σ is the activation function.",2.3 Aspect Attention Layer,[0],[0]
Note that all the above parameters are shared by every hidden state in HP .,2.3 Aspect Attention Layer,[0],[0]
"Next, we compute the attention weight ai for each hPi based on its compatibility score via softmax operation:
ai = exp(ci)∑|P | j=1 exp(cj)
(4)
",2.3 Aspect Attention Layer,[0],[0]
"Finally, we can obtain the domain aspect encoded representation based on the attention weights:
AP = |P |∑ i=1",2.3 Aspect Attention Layer,[0],[0]
"aih P i (5)
where AP ∈ R2h is the domain aspect encoding for sentence P and also the output of this layer.",2.3 Aspect Attention Layer,[0],[0]
"We predict the stance label of the sentence based on its domain aspect encoding:
ŷ = softmax(MLP(AP ))",2.4 Prediction Layer,[0],[0]
"(6)
where we use a multilayer perceptron (MLP) to consume the domain aspect encoding AP and apply the softmax to get the predicted probability for each of the C classes, ŷ = {y1, ..., yC}.",2.4 Prediction Layer,[0],[0]
"For model training, we use multi-class crossentropy loss,
J (θ) =",2.5 Model Training,[0],[0]
− N∑ i C∑ j y,2.5 Model Training,[0],[0]
(i) j log ŷ,2.5 Model Training,[0],[0]
(i) j,2.5 Model Training,[0],[0]
"+ λ‖Θ‖ (7)
",2.5 Model Training,[0],[0]
whereN is the size of training set.,2.5 Model Training,[0],[0]
"y is the groundtruth label indicator for each class, and ŷ is the predicted probability.",2.5 Model Training,[0],[0]
λ is the coefficient for L2regularization.,2.5 Model Training,[0],[0]
Θ denotes the set of all trainable parameters in our model.,2.5 Model Training,[0],[0]
This section reports the results of quantitative and qualitative evaluations of the proposed model.,3 Experiments,[0],[0]
SemEval-2016:,3.1 Datasets,[0],[0]
the first dataset is from SemEval2016,3.1 Datasets,[0],[0]
"Task 6 on Twitter stance detection, which contains stance-bearing tweets on different targets.",3.1 Datasets,[0],[0]
"We use the following five targets for our experiments: Climate Change is Concern (CC), Feminist Movement (FM), Hillary Clinton (HC), Legalization of Abortion (LA), and Donald Trump (DT).",3.1 Datasets,[1.0],"['We use the following five targets for our experiments: Climate Change is Concern (CC), Feminist Movement (FM), Hillary Clinton (HC), Legalization of Abortion (LA), and Donald Trump (DT).']"
"The class labels are favor, against, and neither, and their distributions are shown in Table 2.",3.1 Datasets,[0],[0]
Tweets on an Australian mining project (AM): the second is our collection of tweets on a mining project in Australia obtained using Twitter API.,3.1 Datasets,[0],[0]
"It includes 220,067 tweets posted from January 2016 to June 2017 that contain the project name in the text.",3.1 Datasets,[0],[0]
"We remove all URL-only tweets and duplicate tweets, and obtain a set of 40,852 (unlabeled) tweets.",3.1 Datasets,[0],[0]
"Due to the lack of annotation, this dataset is only used for our qualitative evaluation.
",3.1 Datasets,[0],[0]
"To align with our scenario, the above targets can be categorized into three different domains: Women’s Rights (FM, LA), American Politics (HC, DT), and Environments (CC, AM).",3.1 Datasets,[1.0],"['To align with our scenario, the above targets can be categorized into three different domains: Women’s Rights (FM, LA), American Politics (HC, DT), and Environments (CC, AM).']"
We use F1-score to measure the classification performance.,3.2 Metric,[0],[0]
"Due to the imbalanced class distributions of the SemEval dataset, we compute both micro-averaged (large classes dominate) and macro-averaged (small classes dominate) F1scores (Manning et al., 2008), and use their average as the metric, i.e., F = 12(Fmicro + Fmacro).
",3.2 Metric,[0],[0]
"To evaluate the effectiveness of target adaptation, we use the metric transfer ratio (Glorot et al., 2011) to compare the cross-target and in-target performance of a model: Q = F (S,D)Fb(D,D) , where F (S,D) is the cross-target F1-score of a model trained on the source target S and tested on the destination target D, and Fb(D,D) is the in-target F1-score of a baseline model trained and tested on the same target D, which serves as the performance calibration for target adaptation.",3.2 Metric,[1.0],"['To evaluate the effectiveness of target adaptation, we use the metric transfer ratio (Glorot et al., 2011) to compare the cross-target and in-target performance of a model: Q = F (S,D)Fb(D,D) , where F (S,D) is the cross-target F1-score of a model trained on the source target S and tested on the destination target D, and Fb(D,D) is the in-target F1-score of a baseline model trained and tested on the same target D, which serves as the performance calibration for target adaptation.']"
"The word embeddings are initialized with the pretrained 200d GloVe word vectors on the 27B Twitter corpus (Pennington et al., 2014), and fixed during training.",3.3 Training setup,[0],[0]
"The model is trained (90%) and validated (10%) on a source target, and tested on a destination target.",3.3 Training setup,[0],[0]
"The following model settings are selected based on a small grid search on the validation set: the LSTM hidden size of 60, the MLP layer size of 60, and dropout 0.1.",3.3 Training setup,[0],[0]
The L2-regularization coefficient λ in the loss is 0.01.,3.3 Training setup,[0],[0]
"ADAM (Kingma and Ba, 2014) is used as the optimizer, with a learning rate of 10−3.",3.3 Training setup,[0],[0]
Stratified 10-fold cross-validation is conducted to produce averaged results.,3.3 Training setup,[0],[0]
This section reports the results of our model and two baseline approaches on cross-target stance classification.,3.4 Classification Performance,[0],[0]
BiLSTM:,3.4 Classification Performance,[0],[0]
this is a base model for our task.,3.4 Classification Performance,[0],[0]
It has two BiLSTMs for encoding the sentence and target separately.,3.4 Classification Performance,[0],[0]
"Then, the concatenation of the resulting encodings is fed into the final Prediction Layer to generate predicted stance labels.",3.4 Classification Performance,[0],[0]
"In our evaluation, this model is treated as the baseline model for deriving the in-target performance calibration Fb(D,D).",3.4 Classification Performance,[0],[0]
"MITRE (Augenstein et al., 2016):",3.4 Classification Performance,[0],[0]
"this is the
best system in SemEval-2016 Task 6.",3.4 Classification Performance,[0],[0]
It utilizes the conditional encoding to learn a targetdependent representation for the input sentence.,3.4 Classification Performance,[0],[0]
"The conditional encoding is realized in the same way as the Context Encoding Layer does in our model, namely by using the hidden states of the target-encoding BiLSTM to initialize the sentence-encoding BiLSTM.
Table 3 shows the results (in-target and crosstarget) on the two domains: Women’s Rights and American Politics.",3.4 Classification Performance,[0],[0]
"First, it is observed that MITRE outperforms BiLSTM over all target configurations, suggesting that, compared to simple concatenation, the conditional encoding of the target information could be more helpful to capture the dependency of the sentence on the target.
",3.4 Classification Performance,[0.9999999457884531],"['First, it is observed that MITRE outperforms BiLSTM over all target configurations, suggesting that, compared to simple concatenation, the conditional encoding of the target information could be more helpful to capture the dependency of the sentence on the target.']"
"Second, our model is shown to achieve better results than the two baselines in almost all cases (only slightly worse than MITRE on LA under the in-target setting, and the difference is not statistically significant), which implies that the aspect attention mechanism adopted in our model could benefit target-level generalization while it does not hurt the in-target performance.",3.4 Classification Performance,[1.0],"['Second, our model is shown to achieve better results than the two baselines in almost all cases (only slightly worse than MITRE on LA under the in-target setting, and the difference is not statistically significant), which implies that the aspect attention mechanism adopted in our model could benefit target-level generalization while it does not hurt the in-target performance.']"
"Moreover, by comparing the performance of our model under different target configurations, we see that the improvements brought by our model are more significant on the cross-target task than they are on the intarget task, with an average improvement of 6.6% (cross-target) vs. 3.0% (in-target) over MITRE in F1-score, which demonstrates a greater advantage of our model in the cross-target task.
",3.4 Classification Performance,[1.0000000448756132],"['Moreover, by comparing the performance of our model under different target configurations, we see that the improvements brought by our model are more significant on the cross-target task than they are on the intarget task, with an average improvement of 6.6% (cross-target) vs. 3.0% (in-target) over MITRE in F1-score, which demonstrates a greater advantage of our model in the cross-target task.']"
"Finally, according to the transfer ratio results, the general drop from the in-target to cross-target performance (26% averaged over all cases) could imply that while the target-independent information (i.e., the domain-specific aspects) is shown to benefit generalization, it could be important to also consider the information that is specific to the destination target for model building (which has not yet been explored in this work).",3.4 Classification Performance,[0],[0]
"To show that our model can select sentence parts that are related to domain aspects, we visualize the self-attention results on some tweet examples that are correctly classified by our model in Table 4.
",3.5 Visualization of Attention,[0.9999999240749826],"['To show that our model can select sentence parts that are related to domain aspects, we visualize the self-attention results on some tweet examples that are correctly classified by our model in Table 4.']"
We can see that the most highlighted parts in each example are relevant to the respective domain.,3.5 Visualization of Attention,[1.0],['We can see that the most highlighted parts in each example are relevant to the respective domain.']
"For example, “feminist”, “rights”, and “equality” are commonly used when talking about women’s rights, and “president” and “dreams” of-
ten appear in text about politics.",3.5 Visualization of Attention,[0.9999999489821935],"['For example, “feminist”, “rights”, and “equality” are commonly used when talking about women’s rights, and “president” and “dreams” of- ten appear in text about politics.']"
"It is also interesting to note that words that are specific to the destination target may not be captured by the model learned from the source target, such as “abortion” in sentence 1 and “trumps” in sentence 3.",3.5 Visualization of Attention,[1.0],"['It is also interesting to note that words that are specific to the destination target may not be captured by the model learned from the source target, such as “abortion” in sentence 1 and “trumps” in sentence 3.']"
"This makes sense because those words are rare in the source target corpus and thus not well noticed by the model.
",3.5 Visualization of Attention,[0],[0]
"Finally, for our project, we can see from the last two sentences that the model learned from climate change is concern is able to concentrate on words that are central to understanding the authors’ stances on the approval of the mining project, such as “reef”, “destroy”, “environmental”, and “disaster”.",3.5 Visualization of Attention,[1.0],"['Finally, for our project, we can see from the last two sentences that the model learned from climate change is concern is able to concentrate on words that are central to understanding the authors’ stances on the approval of the mining project, such as “reef”, “destroy”, “environmental”, and “disaster”.']"
"Overall, the above visualization demonstrates that our model could benefit stance inference across related targets through capturing domain-specific information.",3.5 Visualization of Attention,[1.0],"['Overall, the above visualization demonstrates that our model could benefit stance inference across related targets through capturing domain-specific information.']"
"Finally, it is also possible to show the learned domain aspects by extracting all sentence parts in a corpus that are highly attended by our model.",3.6 Learned Domain-Specific Aspects,[0],[0]
Table 5 presents a number of samples from the intersections between the sets of highly-attended words on the respective targets in the three domains.,3.6 Learned Domain-Specific Aspects,[0],[0]
"Again, we see that these highly-attended words are specific to the respective domains.",3.6 Learned Domain-Specific Aspects,[0],[0]
"We
also notice that besides the domain-aspect words, our model can find words that carry sentiments as well, such as “great”, “crazy”, and “beautiful”, which contribute to stance prediction.",3.6 Learned Domain-Specific Aspects,[0],[0]
"In this work, we study cross-target stance classification and propose a novel self-attention neural model that can extract target-independent information for model generalization.",4 Conclusion and Future Work,[0],[0]
Experimental results show that the proposed model can perceive high-level domain-specific information in a sentence and achieves superior results over a number of baselines in certain domains.,4 Conclusion and Future Work,[0],[0]
"In the future, there are several ways of extending our model.
",4 Conclusion and Future Work,[0],[0]
"First, selecting the effective source targets to generalize from is crucial for achieving satisfying results on the destination targets.",4 Conclusion and Future Work,[0],[0]
"One possibility could be to learn certain correlations between target closeness and generalization performance, which could further be used for guiding the target selection process.",4 Conclusion and Future Work,[0],[0]
"Second, our current model for identifying users’ stances on mining projects only generalizes from one source target (i.e., Climate Change is Concern).",4 Conclusion and Future Work,[0],[0]
"However, a mining project in general could affect other aspects of our society such as community and economics.",4 Conclusion and Future Work,[0],[0]
It could be useful to also consider other related sources for knowledge transfer.,4 Conclusion and Future Work,[0],[0]
"Finally, it would be interesting to evaluate our model in a multilingual scenario (Taulé et al., 2017), in order to examine its generalization ability (whether it can attend to useful domain-specific information in a new language) and multilingual scope.",4 Conclusion and Future Work,[0],[0]
We thank all anonymous reviewers for their valuable comments.,Acknowledgments,[0],[0]
We would also like to thank Keith Vander Linden for his helpful comments on drafts of this paper.,Acknowledgments,[0],[0]
"In stance classification, the target on which the stance is made defines the boundary of the task, and a classifier is usually trained for prediction on the same target.",abstractText,[0],[0]
"In this work, we explore the potential for generalizing classifiers between different targets, and propose a neural model that can apply what has been learned from a source target to a destination target.",abstractText,[0],[0]
We show that our model can find useful information shared between relevant targets which improves generalization in certain scenarios.,abstractText,[0],[0]
Cross-Target Stance Classification with Self-Attention Networks,title,[0],[0]
