0	19	Semantic role labeling (SRL) (Gildea and Jurafsky, 2002) can be informally described as the task of discovering who did what to whom.
2	52	Formally, the task includes (1) detection of predicates (e.g., makes); (2) labeling the predicates with a sense from a sense inventory (e.g., make.01); (3) identifying and assigning arguments to semantic roles (e.g., Sequa is A0, i.e., an agent / ‘doer’ for the corresponding predicate, and engines is A1, i.e., a patient / ‘an affected entity’).
13	28	We introduce a version of GCNs for modeling syntactic dependency structures and generally applicable to labeled directed graphs.
22	40	to encoding syntactic information at word level; • we propose a GCN-based SRL model and obtain state-of-the-art results on English and Chinese portions of the CoNLL-2009 dataset; • we show that bidirectional LSTMs and syntax-based GCNs have complementary modeling power.
31	97	The node representation, encoding information about its immediate neighbors, is computed as hv = ReLU 0@ X u2N (v) (W xu + b) 1A , (1) where W 2 Rm⇥m and b 2 Rm are a weight matrix and a bias, respectively; N (v) are neighbors of v; ReLU is the rectifier linear unit activation function.2 Note that v 2 N (v) (because of selfloops), so the input feature representation of v (i.e. xv) affects its induced representation hv.
32	22	As in standard convolutional networks (LeCun et al., 2001), by stacking GCN layers one can incorporate higher degree neighborhoods: h(k+1)v = ReLU 0@ X u2N (v) W (k)h(k)u + b (k) 1A where k denotes the layer number and h(1)v = xv.
36	25	Now, we introduce a generalization of GCNs appropriate for syntactic dependency trees, and in general, for directed labeled graphs.
37	25	First note that there is no reason to assume that information flows only along the syntactic dependency arcs (e.g., from makes to Sequa), so we allow it to flow in the opposite direction as well (i.e., from dependents to heads).
38	62	We use a graph G = (V, E), where the edge set contains all pairs of nodes (i.e., words) adjacent in the dependency tree.
41	43	For example, the label for (makes, Sequa) is subj, whereas the label for (Sequa, makes) is subj0, with the apostrophe indicating that the edge is in the direction opposite to the corresponding syntactic arc.
46	17	Our simplification captures the intuition that information should be propagated differently along edges depending whether this is a head-to-dependent or dependent-to-head edge (i.e., along or opposite the corresponding syntactic arc) and whether it is a self-loop.
51	18	Moreover, we rely on automatically predicted syntactic structures, and, even for English, syntactic parsers are far from being perfect, especially when used out-of-domain.
53	37	In order to address the above issues, inspired by recent literature (van den Oord et al., 2016; Dauphin et al., 2016), we calculate for each edge node pair a scalar gate of the form g(k)u,v = ⇣ h(k)u · v̂(k)dir(u,v) + b̂ (k) L(u,v) ⌘ , (3) where is the logistic sigmoid function, v̂ (k) dir(u,v) 2 Rm and b̂ (k) L(u,v) 2 R are weights and a bias for the gate.
84	44	The classifier predicts semantic roles of words given the predicate while relying on word representations provided by GCN; we concatenate hidden states of the candidate argument word and the predicate word and use them as input to a classifier (Figure 3, top).
86	25	In this way each role prediction is predicate-specific, and, at the same time, we expect to learn a good representation for roles associated with infrequent predicates.
88	16	We tested the proposed SRL model on the English and Chinese CoNLL-2009 dataset with standard splits into training, test and development sets.
95	23	Adam (Kingma and Ba, 2015) was used as an optimizer.
96	41	The hyperparameter tuning and all model selection were performed on the English development set; the chosen values are shown in Appendix.
97	16	In order to show that GCN layers are effective, we first compare our model against its version which lacks GCN layers (i.e. essentially the model of Marcheggiani et al. (2017)).
98	33	Importantly, to measure the genuine contribution of GCNs, we first tuned this syntax-agnostic model (e.g., the number of LSTM layers) to get best possible performance on the development set.6 We compare the syntax-agnostic model with 3 syntax-aware versions: one GCN layer over syntax (K = 1), one layer GCN without gates and two GCN layers (K = 2).
100	32	For both datasets, the syntax-aware model with one GCN layers (K = 1) performs the best, outperforming the LSTM version by 1.9% and 0.6% for Chinese and English, respectively.
101	44	The reasons why the improvements on Chinese are much larger are not entirely clear (e.g., both languages are relative fixed word order ones, and the syntactic parses for Chinese are considerably less accurate), this may be attributed to a higher proportion of longdistance dependencies between predicates and arguments in Chinese (see Section 3.3).
104	22	When BiLSTM layers are dropped altogether, stacking two layers (K = 2) of GCNs greatly improves the performance, resulting in a 3.8% jump in F1 for English and a 3.0% jump in F1 for Chi- nese.
105	23	Adding a 3rd layer of GCN (K = 3) further improves the performance.7 This suggests that extra GCN layers are effective but largely redundant with respect to what LSTMs already capture.
108	61	We looked closer in contribution of specific dependency relations for Chinese.
109	39	In order to assess this without retraining the model multiple times, we drop all dependencies of a given type at test time (one type at a time, only for types appearing over 300 times in the development set) and observe changes in performance.
110	65	In Figure 5, we see that the most informative dependency is COMP (complement).
112	59	The relative clause will syntactically depend on Ñ as COMP, so COMP encodes important information about predicate-argument structure.
115	54	In order to compare to previous work, in Table 3 we report test results on the English indomain (WSJ) evaluation data.
124	49	This suggests that our model is fairly robust to mistakes in syntax.
