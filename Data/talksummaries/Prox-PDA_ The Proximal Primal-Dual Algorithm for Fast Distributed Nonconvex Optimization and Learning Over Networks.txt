23	1	Further, our work reveals an interesting connection between the primal-dual based algorithm Prox-PDA and the primal-only fast distributed algorithms such as EXTRA (Shi et al., 2014).
24	1	Such new insight of the connection between primal-dual and primal-only algorithms could be of independent interest for the optimization community.
25	1	Finally, we generalize the theory for Prox-PDA based algorithms to a challenging distributed matrix factorization problem.
26	2	Define a graph G := {V, E}, where V and E are the node and edge sets; Let |V| = N and |E| = E. Each node v ∈ V represents an agent in the network, and each edge eij = (i, j) ∈ E indicates that node i and j are neighbors; see Fig.1(Left).
31	1	To proceed, let us introduce a few useful quantities related to graph G. • The incidence matrix Ã ∈ RE×N is a matrix with entires Ã(k, i) = 1 and Ã(k, j) = −1 if k = (i, j) ∈ E with j > i, and all the rest of the entries being zero.
33	1	Define the extended incidence matrix as A := Ã⊗ IM ∈ REM×Q, (3) where ⊗ denotes the Kronecker product.
41	1	In Prox-PDA, the primal iteration (7a) minimizes the augmented Lagrangian plus a proximal term β2 ‖x − x r‖2BTB .
45	1	The primal problem is decomposable over different network nodes, hence distributedly implementable.
47	1	Then by a result in (Zlobec, 2005)[Theorem 2.1], we know Algorithm 1 The Prox-PDA Algorithm 1: At iteration 0, initialize µ0 = 0 and x0 ∈ RQ.
49	1	(7b) that there exists β > 0 large enough such that the objective function of (7a) is strongly convex.
52	1	Clearly this problem is separable over the nodes, therefore it can be solved completely distributedly.
54	1	The key in the analysis is the construction of a novel potential function, which decreases at every iteration of the algorithm.
61	1	Below we provide the analysis of Prox-PDA.
63	1	Let σmin denotes the smallest non-zero eigenvalue of ATA, and we define wr := (xr+1−xr)−(xr−xr−1) for notational simplicity.
66	1	Lemma 2 Suppose Assumptions [A1] and [A2] are satisfied.
70	2	In search for an appropriate potential function, we need a new object that is decreasing in the order of β ‖wr‖2BTB .
71	1	The following lemma shows that the descent of the sum of the constraint violation and the proximal term has the desired property.
72	1	Lemma 3 Suppose Assumption [A1] is satisfied.
74	1	(10) It is interesting to observe that the new object, β/2 ( ‖Axr+1‖2 + ‖xr+1 − xr‖2BTB ) , increases in L‖xr+1 − xr‖2 and decreases in β/2‖wr‖2BTB , while the AL behaves in an opposite manner (cf.
92	1	Theorem 1 Suppose Assumption A and the conditions (13) and (14) are satisfied.
120	1	Utilizing the fact that ATA = L−, BTB = L+ and L+ + L− = 2D, we have ∇f(xr) +ATµr + 2βDxr+1 − βL+xr = 0.
124	1	Moreover, by appealing to our analysis in Section 5, it readily follows that EXTRA works for the nonconvex distributed optimization problem as well.
129	1	‖yi‖2 ≤ τ, ∀ i where X ∈ RM×K , Y ∈ RK×N ; for each i, yi ∈ RK consists of one column of Y ; Z ∈ RM×N is some known matrix; h(Y ) := ∑N i=1 hi(yi) is some convex but possibly nonsmooth penalization term; η > 0 is some given constant; for notation simplicity we have defined γ := 1/Nη.
143	1	(27) The AL function for the above problem is given by Lβ(X, Y,Ω) = N∑ i=1 ( 1 2 ‖Xiyi − zi‖2 + γ‖Xi‖2F + hi(yi) ) + 〈Ω,AX〉+ β 2 〈AX,AX〉, (28) where Ω := {Ωe} ∈ REM×K is the matrix of the dual variable, with Ωe ∈ RM×K being the dual variable for the consensus constraint on link e, i.e, Xi = Xj , e = (i, j).
145	1	In Algorithm 2 we have introduced a sequence {θri ≥ 0} which measures the size Algorithm 2 Prox-PDA for Distr.
147	1	(29d) of the local factorization error.
201	7	We set γ = 0.05, τ = 105 and β = 0.001r, and the results are averaged over 30 problem instances.
202	12	3, we compare the performance of the proposed Prox-PDAIP and the EXTRA-AO.
203	135	It can be observed that our proposed algorithm converges faster than the EXTRA-AO.
204	133	We have observed that the EXTRA-AO does have reasonably good practical performance, however it lacks formal convergence proof.
