13	12	After training, the structured classifier can be thought as a black-box.
15	11	Subsequently, we can view each prediction of the black-box classifier as an opportunity to learn how to navigate the output space more efficiently.
16	9	Thus, if the classifier sees a previously encountered situation, it could make some decisions without needless computations.
21	8	By doing so, we can achieve a reduction in inference time.
25	7	We instantiate our strategy to the task of predicting entities and relations from sentences.
51	9	(2) The dependencies between the yk’s specify the nature of the output space.
53	15	In this spirit, the problem of finding the best structure can be viewed as a combinatorial optimization problem.
57	36	One common way to solve inference is by designing efficient dynamic programming algorithms that exploit problem structure.
60	15	Since inference is essentially a combinatorial optimization problem, without loss of generality, we can represent any inference problem as an integer linear programming (ILP) instance (Schrijver, 1998).
61	10	To represent the inference task in Eq.
80	8	Directly applying the black-box solver for the large output spaces may be impractical.
89	14	Specifically, we will define the search node v as a set of pairs {(k, i)}, each element of which specifies that the variable yk is assigned the ith label.
92	11	The size of any goal node is K, the number of categorical variables.
106	7	For example, we might get a structure that mandates that the person Colin lives in a person called Ordon Village.
134	26	Note that even though heuristic function defined in this way is not always admissible, greedy search with ranking function p(v) will lead to the exact solution of Eqs.
136	33	Indeed, when Lagrangian relaxation is used for inference, the optmial dual variables are computed using subgradient optimization for each example because their value depends on the original input via the c’s.
141	17	Because the purpose of the heuristic is to help improve inference speed, we call φ(v) speedup features.
191	10	(13) If the beam Bt is selected from Ct only according to heuristic function, then ∆t is the gap between the last node in the beam and the first node outside the beam.
199	15	In this task, we are asked to label each entity, and the relation between each pair of the entities.
214	20	We use 29950 sentences from the Gigaword corpus (Graff et al., 2003) to train the speedup classifier.
215	8	The entity candidates are extracted using the Stanford Named Entity Recognizer (Manning et al., 2014).
220	17	We evaluate the learned speedup classifier in terms of both accuracy and speed.
233	15	The constraints are guaranteed to be satisfied by using the standard arc-consistency search.
238	17	With beam width b = 2, we recover the ILP model accuracy when evaluated against gold labels.
248	15	But the prediction time is lowered compared to the results from Table 1.
275	22	In this paper, we asked whether we can learn to make inference faster over the lifetime of a structured output classifier.
276	35	To address this question, we developed a search-based strategy that learns to mimic a black-box inference engine but is substantially faster.
277	64	We further extended this strategy by identifying cases where the learned search algorithm can avoid expensive input feature extraction to further improve speed without losing accuracy.
278	77	We empirically evaluated our proposed algorithms on the problem of extracting entities and relations from text.
279	95	Despite using an object-heavy JVM-based implementation of search, we showed that by exploiting regularities across the output space, we can outperform the industrial strength Gurobi integer linear program solver in terms of speed, while matching its accuracy.
