0	29	Over the last few years, we have witnessed significant progress in developing agents that can interact with increasingly complex environments (Mnih et al., 2015; Silver et al., 2016; Levine et al., 2016).
3	30	For control tasks, it is possible to work with realistic environments in robotics, but the complexity of physical hardware constraints efficient data gathering and rapid iteration.
4	17	Even for narrow domains such as grasping (Levine et al., 2016; Pinto & Gupta, 2016), the cost and effort of large-scale data collection is daunting.
5	26	To address this, we introduce World of Bits (WoB),1 a learning platform that uses the web as a rich source of opendomain environments.
6	111	In WoB, an agent receives its observations in the form of the Document Object Model (DOM) of a webpage and its rendered pixels, and accomplishes web tasks by sending mouse and keyboard actions.
7	17	The use of web as a learning platform offers three benefits: Open-domain.
8	16	By allowing agents to interact with the web, we open up the world’s supply of websites as a rich source of learning environments and application domains.
17	30	The main difficulty here is that websites are constantly changing, and yet we would like to package them into reproducible research environments for our agents.
23	26	Positive reward is given if an agent clicks on the correct answer.
27	12	To benchmark a standard approach, we evaluate the performance of convolutional neural networks that take as input the image and text from the DOM and outputs keyboard and mouse actions.
35	16	Then the agent obtains reward rt which is defined by the specific web task.
36	24	Inspired by the ATARI Learning Environment (Bellemare et al., 2013), we designed a benchmark of 100 reinforce- ment learning environments called Mini World of Bits (MiniWoB) that share many of the characteristics of live web tasks (interacting with buttons, text fields, sliders, date pickers, etc.)
39	23	Each MiniWoB environment is an HTML page that is 210 pixels high, 160 pixels wide (i.e. identical to the ATARI environment dimensions) — the top 50 pixels (in yellow background) contain the natural language task description (randomly generated) and the 160 × 160 area below is for interactions.
43	32	Since websites change over time and since we do not wish to spam websites with requests while the agent is training, we need to create an offline approximation that the agent can interact with.
44	39	To do this, when we collect human demonstrations, we use a proxy to record all HTTP requests and responses between the agent and the website.
50	12	We applied this approach to four flight booking websites (United, Alaska, AA, and JetBlue).
59	18	A worker provides a website (e.g., yelp.com) and a query template (e.g., “What is the cheapest restaurant that serves (type of food) near (geographic location)?”).
60	27	We also ask workers to generate multiple slot values for each template (e.g. “brunch” / “San Francisco”, “hamburger” / “JFK international airport”, etc.).
61	40	Next, a worker takes a query from stage 1 and uses our demonstration interface to answer it (see Figure 4).3 The interface has a “Select” button, which allows the worker to mark the DOM element of the webpage corresponding to the answer.
69	19	We gather 10 - 100 slot values per template, resulting in 13,550 total queries.
89	27	Our model (see Figure 6) first processes the image using a Convolutional Neural Network (CNN).
99	14	Policies trained with supervised learning suffer from compounding errors, so we fine tune the policies by optimizing the expected reward using a policy gradient method (Sutton et al., 1999).
127	16	Next, we evaluate our model on the four FormWoB tasks.
148	31	First, we report the test likelihood as a metric to show how well the agent models human trajectories.
150	16	We report the average rewards over the final three checkpoints.
154	14	In particular, for flight booking, the model achieves 20%–30% of human level performance on training queries, and 16% on test queries.
157	18	As we can see, the model generalizes by correctly selecting the city in dropdown and picking the correct date, aided by text matching.
158	114	The CNN identifies the “Submit” button even after some random scrolling has occurred.
159	161	The most common failure mode is if the agent falls off the demonstrators’ state distributions (e.g. triggering an error message), it is difficult to take actions to recover.
165	37	Figure 11(c) shows some example failure cases.
