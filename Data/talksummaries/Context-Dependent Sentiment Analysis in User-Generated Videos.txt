10	22	An utterance (Olson, 1977) is a unit of speech bound by breathes or pauses.
19	127	Like any other sequence classification problem (Collobert et al., 2011), sequential utterances of a video may largely be contextually correlated and, hence, influence each other’s sentiment distribution.
21	75	We treat surrounding utterances as the context of the utterance that is aimed to be classified.
22	26	For example, the MOSI dataset (Zadeh et al., 2016) contains a video, in which a girl reviews the movie ‘Green Hornet’.
23	80	At one point, she says “The Green Hornet did something similar”.
24	95	Normally, doing something similar, i.e., monotonous or repetitive might be perceived as negative.
25	97	However, the nearby utterances “It engages the audience more”, “they took a new spin on it”, “and I just loved it” indicate a positive context.
26	26	The hypothesis of the independence of tokens is quite popular in information retrieval and data mining, e.g., bag-of-words model, but it has a lot limitations (Cambria and White, 2014).
31	17	Our model preserves the sequential order of utterances and enables consecutive utterances to share information, thus providing contextual information to the utterance-level sentiment classification process.
48	12	In this work, we propose a LSTM network that takes as input the sequence of utterances in a video and extracts contextual unimodal and multimodal features by modeling the dependencies among the input utterances.
50	15	We represent the dataset as U = u1, u2, u3..., uM and each ui = ui,1, ui,2, ..., ui, Li where Li is the number of utterances in video ui.
54	9	We experimentally show that this proposed framework improves the performance of utterance-level sentiment classification over traditional frameworks.
55	17	Initially, the unimodal features are extracted from each utterance separately, i.e., we do not consider the contextual relation and dependency among the utterances.
58	30	For extracting features from the textual modality, we use a CNN (Karpathy et al., 2014).
69	18	Audio features are extracted at 30 Hz frame-rate and a sliding window of 100 ms. To compute the features, we use openSMILE (Eyben et al., 2010b), an open-source software that automatically extracts audio features such as pitch and voice intensity.
79	12	Let vid ∈ Rc×f×h×w be a video, where c = number of channels in an image (in our case c = 3, since we consider only RGB images), f = number of frames, h = height of the frames, and w = width of the frames.
80	14	Again, we consider the 3D convolutional filter filt ∈ Rfm×c×fd×fh×fw , where fm = number of feature maps, c = number of channels, fd = number of frames (in other words depth of the filter), fh = height of the filter, and fw = width of the filter.
92	10	In particular, we claim that, when classifying one utterance, other utterances can provide important contextual information.
108	14	The activations of the dense layer zi,t are used as the contextdependent features of contextual LSTM.
149	10	Multimodal Sentiment Analysis Datasets MOSI The MOSI dataset (Zadeh et al., 2016) is a dataset rich in sentimental expressions where 93 people review topics in English.
151	19	We took the average of these five annotations as the sentiment polarity and, hence, considered only two classes (positive and negative).
157	9	The utterances are labeled to be either positive, negative or neutral.
163	14	Videos by the first 8 speakers are considered in the training set.
172	28	To curate the IEMOCAP dataset, instead, subjects were provided affect-related scripts and asked to act.
173	12	As pointed out by Poria et al. (Poria et al., 2017a), acted dataset like IEMOCAP can suffer from biased labeling and incorrect acting which can further cause the poor generalizability of the models trained on the acted datasets.
176	82	Hierarchical vs Non-hierarchical Fusion Framework As expected, trained contextual unimodal features help the hierarchical fusion framework to outperform the non-hierarchical framework.
178	67	For this reason, we the rest of the analysis only leverages on the hierarchical framework.
179	30	The non-hierarchical model outperforms the baseline uni-SVM, which confirms that it is the contextsensitive learning paradigm that plays the key role in improving performance over the baseline.
180	121	Comparison of Different Network Variants It is to be noted that both sc-LSTM and bc-LSTM perform quite well on the multimodal emotion recognition and sentiment analysis datasets.
181	19	Since bc-LSTM has access to both the preceding and following information of the utterance sequence, it performs consistently better on all the datasets over sc-LSTM.
