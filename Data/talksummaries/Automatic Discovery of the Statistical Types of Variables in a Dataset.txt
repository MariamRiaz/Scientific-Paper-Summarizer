2	7	In particular, there are a growing body of work which focuses on automating the different stages of data pre-processing, including data cleaning (Hellerstein, 2008), data wrangling (Kandel et al., 2011) and data integration and fusion (Dong & Srivastava, 2013).
3	12	The outcome of data pre-processing is commonly a structured dataset, in which the objects are described by a set of attributes.
4	32	However, before being able to proceed with the predictive analytics step of the data analysis process, the data scientist often needs to identify which kind of variables (i.e., real-values, categorical, ordinal, etc.)
5	3	This labeling of the data is necessary to select the appropriate machine learning approach to explore, find patterns or make predictions on the data.
6	31	As an example, a prediction task is solved differently depending on the kind of data to be predicted—e.g., while prediction on categorical variables is usually formulated as a classification task, in the case of ordinal variables it is formulated as an ordinal regression problem (Agresti, 2010).
9	7	Although extensive work has focused on model selection (Ando, 2010; Burnham & Anderson, 2003), the likelihood model is usually assumed to be known and fixed.
10	11	As an example, a common approach is to model continuous data as Gaussian variables, and discrete data as categorical variables.
11	4	However, while extensive work has shown the advantages of capturing the statistical properties of the observed data in the likelihood model (Chu & Ghahramani, 2005a; Schmidt et al., 2009; Hilbe, 2011; Valera & Ghahramani, 2014), there still exists a lack of tools to automatically perform likelihood model selection, or equivalently to discover the most plausible statistical type of the variables in the data, directly from the data.
17	18	We derive an efficient MCMC inference algorithm to jointly infer both the low-rank representation and the weight of each likelihood model for each attribute in the observed data.
18	12	Our experimental results show that the proposed method accurately discovers the true data type of the variables in a dataset, and by doing so, it fits the data substantially better than modeling continuous data as Gaussian variables and discrete data as categorical variables.
23	16	However, distinguishing among different types of discrete and continuous variables cannot be easily solved using simple heuristics.
24	8	In the context of continuous variables, given the finite size of observed datasets, it is complicated to identify whether a variable may take values in the entire real line, or only on an interval of it, e.g., (0,∞) or (θL, θH).
29	9	As an example, while colors in M&Ms usually do not present an order, colors in a traffic light clearly do.
32	27	Second, we would need access to exact information on whether its consecutive values are equidistant or not, however, this information depends on how the data have been gathered.
33	10	For example, an attribute that collects information on “frequency of an action” will correspond to an ordinal variable if its categories belong to, e.g., {“never”, “sometimes”, “usually”, “often”}, and to a count variable if it takes values in {‘‘0 times per week”, “1 time per week”, .
41	16	In detail, we consider that each observation xdn can be explained by a K-length vector of latent variables zn = [zn1, .
51	201	(1) We place a Dirichlet prior distribution on the likelihood weights wd = [wd` ]`∈Ld , and similarly to (Salakhutdinov & Mnih, 2007), assume that both the latent feature vectors zn and the weighting vectors bdj are Gaussian distributed with zero mean and covariance matrices σ2zI and σ 2 b I, respectively.
57	20	Ordinal data, which takes values in a finite or- dered set, e.g., xdn ∈ {‘never’, ‘sometimes’, ‘often’, ‘usually’, ‘always’}.
60	11	However, solving this inference problem in an efficient way is a challenging task for several reasons.
61	52	First, we need to jointly infer all the latent variables in the model, i.e., the low-rank representation of the data (which includes the latent feature matrix Z and the corresponding weighting vectors {bd`}{`∈Ld|d=1,...,D}) and the likelihood weights {wd}Dd=1.
62	4	Second, we need to do so given a heterogeneous (and non-conjugate) observation model, which combines D different likelihood models, corresponding each of them to a mixture of likelihood functions and coupled through the latent feature matrix Z. Additionally, these likelihood functions do not only correspond to either a probability density function or a probability mass function depending on whether we are dealing with a continuous or a discrete variable, but also each mixture combines likelihood functions with different supports.
63	15	For example, while real-valued data lead to a likelihood function with the real line as support, interval data only accounts for a segment of the real line.
65	10	In order to allow for efficient inference, we exploit the key idea in (Valera & Ghahramani, 2014) to propose an alternative and equivalent model representation (shown in Figure 1b), which efficiently deal with heterogeneous likelihood functions.
67	12	(2) Note that, if we condition on the pseudo-observations the latent variable model behaves as a conjugate Gaussian model, allowing for efficient inference of the latent feature matrix Z and the weighting vectors {bd`}.
69	51	Then, given sdn, we can obtain the observation x d n as xdn = fsdn(y d nsdn + udn), (3) where udn ∼ N (0, σ2u) is a noise variable.
70	35	We gather the likelihood assignments sdn in a N ×D matrix S.
73	11	In order to obtain real-valued observations, i.e., xdn ∈ <, we need a transformation over ydn that maps from the real numbers to the real numbers, i.e., f< : < → <.
79	11	We set the rescaling parameter w = 2/max(xd) for the three continuous data types.
81	15	Hence, assuming a multinomial probit model, we can write x = fcat(y) = arg max r∈{1,...,Rd} y(r), where in this case there are as many pseudo-observations as number of categories and each pseudo-observation can be sampled as ydncat(r) ∼ N (znbdcat(r), σ2y) where bdcat(r) denotes the K-length weighting vector, which weights the influence the latent features for a categorical observation xdn taking value r. Note that, under this likelihood model, we need one pseudo-observation ydncat(r) and a weighting vector bdcat(r) for each possible value of the observation r ∈ {1, .
85	66	Then, assuming an ordered probit model, we can write xdn = ford(y d nord) =  1 if ydnord ≤ θd1 2 if θd1 < y d nord ≤ θd2 ... Rd if θdRd−1 < y d nord where again ydnord is Gaussian distributed with mean znb d ord and variance σ 2 y , and θ d r for r ∈ {1, .
