0	24	The Gamma-Poisson (GaP) model is a probabilistic matrix factorization model which was introduced in the field of text information retrieval (Canny, 2004; Buntine & Jakulin, 2006).
1	22	In this field, a corpus of text documents is typically represented by an integer-valued matrix V of size F × N , where each column vn represents a document as a so-called “bag of words”.
2	21	Given a vocabulary of F words (or in practice semantic stems), the matrix entry vfn is the number of occurrences of word f in the document n. GaP is a generative model described by a dictionary of “topics” or “patterns” W (a non-negative matrix of size F ×K) and a nonnegative “activation” or “score” matrix H (of size K×N ), as follows: H ∼ ∏ k,n Gamma(hkn|αk, βk), (1) V|H ∼ ∏ f,n Poisson (vfn|[WH]fn) , (2) where we use the shape and rate parametrization of the Gamma distribution, i.e., Gamma(x|α, β) = βα Γ(α)x α−1e−βx.
4	29	Though this generative model takes its origins in text information retrieval, it has found applications (with variants) in other areas such as image reconstruction (Cemgil, 2009), collaborative filtering (Ma et al., 2011; Gopalan et al., 2015) or audio signal processing (Virtanen et al., 2008).
5	58	, βK ]T , and treating the shape parameters αk as fixed hyperparameters, maximum joint likelihood estimation (MJLE) in GaP amounts to the minimization of CJL(W,H,β) def = − log p(V,H|W,β) (3) = DKL(V|WH) +Rα(H,β) + cst (4) where DKL(·|·) is the generalized Kullback-Leibler (KL) divergence defined by DKL(V|V̂) = ∑ f,n ( vfn log ( vfn v̂fn ) − vfn + v̂fn ) (5) and Rα(H,β) =∑ k,n [(1− αk) log(hkn) + βkhkn]−N ∑ k αk log βk.
7	57	As explained in Dikmen & Févotte (2012), MJLE can be criticized from a statistical point of view.
8	11	Indeed, the number of estimated parameters grows with the number of samples N (this is because H has as many columns as V).
9	22	To overcome this issue, they have instead proposed to consider maximum marginal likelihood estimation (MMLE), in which H is treated as a latent variable over which the joint likelihood is integrated.
11	15	(8) We emphasize that MMLE treats the dictionary W as a free deterministic variable.
13	12	For instance, Buntine & Jakulin (2006) place a Dirichlet prior on the columns of W, while Cemgil (2009) considers independent Gamma priors.
14	8	Zhou et al. (2012), Zhou & Carin (2015) set a Dirichlet prior on the columns of W and a Gamma-based non-parametric Bayesian prior on H, which allows for possible rank estimation.
18	7	This intriguing (and advantageous) behavior was left unexplained.
25	36	In Section 3, we propose two new parameterizations of the GaP model in which H has been marginalized.
27	7	Finally, a new MCEM algorithm is introduced in Section 5 and is compared to the MCEM algorithms proposed in Dikmen & Févotte (2012) on synthetic and real data.
28	47	GaP can be written as a composite model, thanks to the superposition property of the Poisson distribution (Févotte & Cemgil, 2009): hkn ∼ Gamma(αk, βk) (9) ckn|hkn ∼ ∏ f Poisson(cfkn|wfkhkn) (10) vn = ∑ k ckn (11) The vectors ckn = [c1kn, .
29	25	, cFkn]T of size F and which sum up to vn are referred to as components.
30	7	In the remainder, C will denote the F ×K ×N tensor with coefficients cfkn.
35	12	Indeed, it offers more flexibility than the Poisson distribution where the variance and the mean are equal.
80	8	As already known from Dikmen & Févotte (2012), the value of the marginal likelihood is unchanged when the scales of the columns of W and the rates β are changed accordingly.
93	15	The first term, Equation (33), captures the interaction between data V (through C) and the parameter W (through the event probabilities pfk = wfk/(‖wk‖1 +βk)).
94	28	The second term, Equation (34), only depends on the parameter W and can be interpreted as a group-regularization term.
96	18	As such, the term (34) will promote sparsity of the norms of the columns of W. When a norm ||wk||1 is set to zero for some k, the whole column wk is set to zero because of the non-negativity constraint.
97	24	This gives a formal explanation of the ability of MMLE to automatically prune columns of W, without any explicit sparsity-inducing prior at the modeling stage (recall that W is a deterministic parameter without a prior).
98	14	We now turn to the problem of optimizing Equation (8) by leveraging on the results of Section 4.
99	14	Despite obtaining a closed-form expression, the direct optimization of the marginal likelihood remains difficult.
101	46	Indeed, GaP involves observed variables V and latent variables C and H. As such, we can derive several EM algorithms based on various choices of the complete set.
102	22	More precisely, we consider three possible choices that each define a different algorithm, as follows.
103	16	The complete set is {C,H} and EM consists in the iterative minimization w.r.t W of the functional defined by QCH(W|W̃) = − ∫ C,H log p(C,H|W)p(C,H|V, W̃)dCdH, (35) where W̃ is the current estimate.
104	102	Note that V does not need to be included in the complete set because we have V = ∑ k Ck.
109	17	EM-C is a new proposal that exploits the results of Section 4.
