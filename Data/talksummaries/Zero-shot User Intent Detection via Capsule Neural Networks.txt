1	26	Products like Apple’s Siri, Amazon’s Alexa and Google Assistant are able to interpret human speech and respond them via synthesized voices.
2	35	With recent developments in deep neural networks, user intent detection models (Hu et al., 2009; Xu and Sarikaya, 2013; Zhang et al., 2016; Liu and Lane, 2016; Chen et al., 2016b) are proposed to classify user intents given their diversely expressed utterances in the natural language.
4	26	As more features and skills are being added to devices which expand their capabilities to new programs, it is common for voice assistants to encounter the scenario where no labeled utterance of an emerging user intent is available in the training data, as illustrated in Figure 1.
5	21	Current intent detection methods train classifiers in a supervised fashion and they are good at discriminating existing intents such as Get Weather and Play Music whose labeled utterances are already available.
7	21	Moreover, it’s labor-intensive and time-consuming to annotate utterances of emerging intents and retrain the whole intent detection model.
8	20	Thus, it is imperative to develop intent detection models with the zero-shot learning (ZSL) ability (Lampert et al., 2014; Socher et al., 2013; Changpinyo et al., 2016): the ability to expand classifiers and the intent detection space beyond the existing intents, of which we have labeled utterances during training, to emerging intents, of which no labeled utterances are available.
14	21	A capsule houses a vector representation of a group of neurons, and the orientation of the vector encodes properties of an object (like the shape/color of a face), while the length of the vector reflects its probability of existence (how likely a face with certain properties exists).
15	35	The capsule model learns a hierarchy of feature detectors via a routing-by-agreement mechanism: capsules for detecting low-level features (like nose/eyes) send their outputs to high-level capsules (such as faces) only when there is a strong agreement of their predictions to high-level capsules.
21	85	The ability to neglect the disagreed output of low-level semantics for certain intents during routing-by-agreement encourages the learning of generalizable semantic features that can be adapted to emerging intents.
25	19	An intent is a purpose, or a goal that underlies a user-generated utterance (Watson Assistant, 2017).
33	24	As shown in Figure 2, the cores of the proposed architectures are three types of capsules: SemanticCaps that extract interpretable semantic features from the utterance, DetectionCaps that aggregate semantic features for intent detection, and Zero-shot DetectionCaps which discriminate emerging intents.
46	21	Each mr is a 2DH−dimensional semantic vector.
47	24	Each semantic vector will have a distinguishable orientation when the objective is properly regularized (details in Equation 6), as we want each attention to be attentive to a unique semantic feature of the utterance.
51	38	To combine these features into higher-level representations, we build DetectionCaps that choose different semantic features dynamically so as to form an intent representation for each intent via an unsupervised routingby-agreement mechanism.
53	48	Wk,r ∈ R2DH×DP is the weight matrix of the DetectionCaps, pk|r is the prediction vector of the rth semantic feature of an existing intent k, andDP is the dimension of the prediction vector.
57	74	As shown in this algorithm, bkr is the initial logit representing the log prior probability that a SemanticCap r is coupled to an DetectionCap k. Algorithm 1 Dynamic routing algorithm 1: procedure DYNAMIC ROUTING(pk|r, iter) 2: for all semantic capsule r and intent capsule k: bkr ← 0.
58	42	3: for iter iterations do 4: for all SemanticCaps r: cr ← softmax(br) 5: for all DetectionCaps k: sk ← Σrckrpk|r 6: for all DetectionCaps k: vk = squash(sk) 7: for all SemanticCaps r and DetectionCaps k: bkr ← bkr + pk|r · vk 8: end for 9: Return vk 10: end procedure The squashing function squash(·) is applied on sk to get an activation vector vk for each existing intent class k: vk = ‖sk‖2 1 + ‖sk‖2 sk ‖sk‖ , (5) where the orientation of the activation vector vk represents intent properties while its norm indicates the activation probability.
65	27	As SemanticCaps are trained to extract semantic features from utterances with various existing intents, a selfattention head which has similar extraction behavior among existing and emerging intents may help transfer knowledge.
69	45	The intent labels also contain knowledge of how two intents are similar with each other.
86	23	We first compare the proposed capsulebased model INTENTCAPSNET with other text classification alternatives on the detection of existing intents: 1) TFIDF-LR/TFIDF-SVM: we use TF-IDF to represent the utterance and use logistic regression/support vector machine as classifiers.
87	26	2) CNN: a convolutional neural network (Kim, 2014) that uses convolution and pooling operations, which is popular for text classification.
110	23	On the SNIPSNLU dataset, each of the three modules has a comparable contribution to the whole model (around 2-3% improvement in F1 score).
132	82	From Table 6 we observe that the same selfattention head that extracts “play” action in the existing intent PlayMusic is also attentive to words or phrases referring to the “rate” action in an emerging intent RateABook: like rate, add the rating, and give.
134	44	Such behavior not only shows that SemanticCaps have the capacity to learn an intentindependent semantic feature extractor, which extracts generalizable semantic features that either existing or emerging intent representations are built upon, but also indicates that SemanticCaps has the ability to transfer extraction behaviors among utterances of different intents.
136	19	Beside extracting semantic features and utilizing existing routing information, we use similarities between intent embeddings to help trans- fer vote vectors from INTENTCAPSNET to INTENTCAPSNET-ZSL.
139	38	The x axis measures var(ql), the variance of the similarity distribution of each emerging intent l to all the existing intents.
141	29	In this case, 13 out of 20 emerging intents with high variances where var(ql) > 0.005 always have a decent performance (Accuracy>0.83).
143	23	In this paper, a capsule-based model, namely INTENTCAPSNET, is first introduced to harness the advantages of capsule models for text modeling in a hierarchical manner: semantic features are extracted from the utterances with selfattention, and aggregated via the dynamic routingby-agreement mechanism to obtain utterance-level intent representations.
144	66	We believe that the inductive biases subsumed in such capsule-based hierarchical learning schema have broader applicability on various text modeling tasks, besides its evidenced performance on the intent detection task we studied in this paper.
145	45	The proposed INTENTCAPSNET-ZSL model further introduces zero-shot learning ability to the capsule model via various means of knowledge transfer from existing intents for discriminating emerging intents where no labeled utterances or excessive external resources are available.
