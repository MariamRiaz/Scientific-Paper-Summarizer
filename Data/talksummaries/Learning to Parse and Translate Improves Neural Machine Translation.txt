22	9	Each hidden state hi is a concatenation of those from the forward and backward recurrent network: hi =[−→ h i; ←− h i ] , where −→ h i = −→ f enc( −→ h i−1, Vx(xi)), ←− h i = ←− f enc( ←− h i+1, Vx(xi)).
24	88	The decoder is implemented as a conditional recurrent language model which models the target sentence, or translation, as log p(y|x) = ∑ j log p(yj |y<j ,x), where y = (y1, .
26	23	The attention model first compares the current hidden state sj against each of the hidden states and assigns a scalar score: βi,j = exp(h>i Wdsj) (Luong et al., 2015).
27	35	These scores are then normalized across the hidden states to sum to 1, that is αi,j = βi,j∑ i βi,j .
29	50	A recurrent neural network grammar (RNNG, Dyer et al., 2016) is a probabilistic syntax-based language model.
33	17	That is, p(at = a|a<t) ∝ eW > a faction(h buffer t ,h stack t ,h action t ), (4) where Wa is the vector of the action a.
34	19	If the selected action is shift, the word at the beginning of the buffer is moved to the stack.
35	36	When the reduce action is selected, the top-two words in the stack are reduced to build a partial tree.
37	41	The hidden states of the buffer, stack and action sLSTM are correspondingly updated by hbuffert = StackLSTM(h buffer top , Vy(yt−1)), (5) hstackt = StackLSTM(h stack top , rt), hactiont = StackLSTM(h action top , Va(at−1)), where Vy and Va are functions returning the target word and action vectors.
39	8	This process is iterated until a complete parse tree is built.
45	11	Our main proposal in this paper is to hybridize the decoder of the neural machine translation and the RNNG.
47	22	We replace the RNNG’s buffer with the neural translation model’s decoder in two steps.
52	9	Once the buffer of the RNNG is replaced with the NMT decoder in our proposed model, the NMT decoder is also under control of the actions provided by the RNNG.2 Second, we let the next word prediction of the translation decoder as a generator of RNNG.
62	15	Later in the experiments, we show that this additional learning signal is useful for translation, even though we discard the RNNG (the stack and action sLSTMs) in the inference time.
78	12	We use the first 100K sentence pairs of length shorter than 50 for training.
101	21	It took about 15 minutes per epoch and about 20 minutes respectively for the baseline and the proposed model to train a full JP-EN parallel corpus in our implementation.4
102	12	In Table 2, we report the translation qualities of the tested models on all the four language pairs.
107	52	Removing the stack had the most adverse effect, which was found to be the case for parsing as well by Kuncoro et al. (2017).
108	30	Generated Sentences with Parsed Actions The decoder part of our proposed model consists of two components: the NMT decoder to gener- threads on Intel(R) Xeon(R) CPU E5-2680 v2 @2.80GHz) 5 Since the buffer is the decoder, it is not possible to completely remove it.
109	66	Instead we simply remove the dependency of the action distribution on it.
110	14	ate a translated sentence and the RNNG decoder to predict its parsing actions.
111	17	The proposed model can therefore output a dependency structure along with a translated sentence.
112	43	Figure 1 shows an example of JP-EN translation in the development dataset and its dependency parse tree obtained by the proposed model.
114	18	The translated sentence was generated by using beam search, which is the same setting of NMT+RNNG shown in Table 3.
116	12	The resulting dependency structure is mostly correct but contains a few errors; for example, dependency relation between “The” and “ transition” should not be “pobj”.
117	18	We propose a hybrid model, to which we refer as NMT+RNNG, that combines the decoder of an attention-based neural translation model with the RNNG.
118	114	This model learns to parse and translate simultaneously, and training it encourages both the encoder and decoder to better incorporate linguistic priors.
119	97	Our experiments confirmed its effectiveness on four language pairs ({JP, Cs, De, Ru}En).
120	41	The RNNG can in principle be trained without ground-truth parses, and this would eliminate the need of external parsers completely.
121	11	We leave the investigation into this possibility for future research.
