10	15	In this study, we investigate the impact of syntactic features on a transition-based graph parser by testing on two treebanks.
11	162	We take advantage of the recent release for the SemEval 2014 shared task on semantic dependency parsing, by Oepen et 64 al.
16	17	DeepBank Corpus Semantic dependency graphs in the DM Corpus are the result of a two-step simplification of the underspecified logical-form meaning representations, based on Minimal Recursion Semantic (MRS, (Copestake et al., 1995; Copestake et al., 2005)), derived from the manually annotated DeepBank treebank (Flickinger et al., 2012).
20	69	Predicate-Argument Structure Corpus Enju Predicate-Argument Structures (PAS Corpus) are derived from the automatic HPSG-style annotation of the Penn Treebank (Miyao and Tsujii, 2004) that was primarily used for the development of the Enju parsing system (Miyao and Tsujii, 2005).
22	31	Each dependency type is made of two elements: a coarse part-of-speech of the head predicate dependent (e.g. verb and adjective), and the argument (e.g. ARG1 and ARG2).
30	82	DM corpus is clearly heading toward more semantic analysis while the PAS corpus aims at providing a more abstract deep syntax analysis than regular surface syntax trees.
32	33	Shift-reduce transition-based parsers essentially rely on configurations formed of a stack and a buffer, with stack transitions used to move from a configuration to the next one, until reaching a final configuration.
33	51	Following Kübler et al. (2009), we define a configuration by c = (σ, β,A) where σ denotes a stack of words wi, β a buffer of words, and A a set of dependency arcs of the form (wi, r, wj), with wi the head, wj the dependent, and r a label in some set R. As shown in Figure 1, besides the usual shift and reduce transitions (lR & rR) of the arc-standard strategy, we introduced the new left and right attach (lA & rA) transitions for adding new dependencies (while keeping the dependent on the stack) and a pop0 transition to remove a word from the stack after attachment of its dependents.
35	47	It is to be noted that the pop0 action may also be used to remove words with no heads.
36	53	We base our work on the the DAG parser of Sagae and Tsujii (2008) (henceforth S&T) which we extended with the set of actions displayed above (Figure 1) to cope with partially connected planar graphs, and we gave it the ability to take advantage of an extended set of features.
46	17	We combined the previous features with different types of syntactic features (constituents and dependencies), our intuition being that syntax and semantic are interdependent, and that syntactic features should therefore help predicate-argument parsing.
49	20	Constituent Tree Fragments These consist of fragments of syntactic trees predicted by the Petrov et al. (2006) parser in a 10-way jackknife setting.
55	36	We combined the labels with a distance δ = t − h where t is the token position and h the head position (brown labels and δ in Figure 2).
56	16	In addition, we expanded these features with the part-of-speech of the head of a given token (HPOS).
68	17	All improvements from the baseline are significant with a p-value p < 0.05.
71	42	Looking at the conjunction of two classes in the DM table, it seems that dependency-based features benefit from the extra context brought by constituents features, reaching an increase of 2.21 points for BKY+BN(HPOS).
72	23	Interestingly, the maximum gain is brought by the addition of topologically different phrase-based features such as SPINES (+2.80, inherently vertical) or BKY (+2.76, often wider) to the previous best.
81	17	We consider, among other factors, the impact in terms of distance between the head and the dependent (edge length) and the labels.
101	27	When using these features, our models tend to perform better, with a gain of up to 25 points for high-dependency lengths (bins between 16-20 and 21-25).
119	30	PAS DM Overlap +2.87 +2.67 Rest +2.70 +2.74 It may argued that the improvement we noticed could stem from a potentially strong overlap between surface trees and predicate-argument structures, both in terms of edges and labels.
124	21	We evaluated the improvement of the overlap as well as for the rest.
131	18	following (Goldberg et al., 2013; Huang et al., 2012)) 4, which proves effective by reaching the state-of-the-art on PAS, outperforming Thomson et al. (2014) and second to the model of Martins and Almeida (2014).
139	17	However in that work, we mostly focused on transition based-parsing, which raises the question of the impact of our feature-set on a much more powerful and state-of-the-art model such as the TURBOSEMANTICPARSER developed by Martins and Almeida (2014).
144	78	In short, the use of syntactic features is also relevant with a strong baseline, as they provide a global view to graph-based models, establishing a new state-of-the-art on these corpora.
147	41	Actually, the conjoint use of such annotations with surface syntax dependencies bears some resemblance with predicate-argument structure parsing like we presented here.
152	23	We described the use and combination of several kinds of syntactic features to improve predicateargument parsing.
153	16	To do so, we tested our approach of injecting surface-syntax features by thoroughly evaluating their impact on one transitionbased graph parser, then validating on two more efficient parsers, over two deep syntax and semantic treebanks.
154	58	Results of the syntax-enhanced semantic parsers exhibit a constant improvement, regardless of the annotation scheme and the parser used.
155	33	The question is now to establish whether will this be verified in other semantic data sets?
156	89	From the parsing of deep syntax treebanks a la Meaning Text Theory (Ballesteros et al., 2014), to Framenet semantic parsing (Das et al., 2014) or data-driven approaches closer to ours (Flanigan et al., 2014), it is difficult to know which models will predominate from this bubbling field and what kind of semantic data sets will benefit the most from syntax.
