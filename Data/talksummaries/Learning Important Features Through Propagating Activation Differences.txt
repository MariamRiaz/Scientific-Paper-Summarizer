0	42	As neural networks become increasingly popular, their black box reputation is a barrier to adoption when interpretability is paramount.
1	32	Here, we present DeepLIFT (Deep Learning Important FeaTures), a novel algorithm to assign importance score to the inputs for a given output.
2	23	Our approach is unique in two regards.
7	15	This section provides a review of existing approaches to assign importance scores for a given task and input example.
8	30	These approaches make perturbations to individual inputs or neurons and observe the impact on later neurons in the network.
9	27	Zeiler & Fergus (Zeiler & Fergus, 2013) occluded different segments of an input image and visualized the change in the activations of later layers.
10	42	“In-silico mutagenesis” (Zhou & Troyanskaya, 2015) introduced virtual mutations at individual positions in a genomic sequence and quantified the their impact on the output.
14	136	Unlike perturbation methods, backpropagation approaches propagate an importance signal from an output neuron backwards through the layers to the input in one pass, making them efficient.
40	27	However, this strategy inherits the limitations of Guided Backpropagation caused by zero-ing out negative gradients during backpropagation.
50	25	This allows DeepLIFT to address a fundamental limitation of gradients because, as illustrated in Fig.
51	30	1, a neuron can be signaling meaningful information even in the regime where its gradient is zero.
53	16	2, where the discontinuous nature of gradients causes sudden jumps in the importance score over infinitesimal changes in the input.
63	10	When formulating the DeepLIFT rules described in Section 3.5, we assume that the reference of a neuron is its activation on the reference input.
65	12	Given the reference activations x01, x 0 2, ... of the inputs, we can calculate the reference activation y0 of the output as: y0 = f(x01, x 0 2, ...) (4) i.e. references for all neurons can be found by choosing a reference input and propagating activations through the net.
67	11	In practice, choosing a good reference would rely on domain-specific knowledge, and in some cases it may be best to compute DeepLIFT scores against multiple different references.
68	85	As a guiding principle, we can ask ourselves “what am I interested in measuring differences against?”.
71	15	5), or by averaging the results over multiple reference inputs for each sequence that are generated by shuffling each original sequence (Appendix J).
72	159	For CIFAR10 data, we found that using a blurred version of the original image as the reference highlighted outlines of key objects, while an allzeros reference highlighted hard-to-interpret pixels in the background (Appendix L).
73	48	It is important to note that gradient×input implicitly uses a reference of all-zeros (it is equivalent to a first-order Taylor approximation of gradient×∆input where ∆ is measured w.r.t.
74	30	Similary, integrated gradients (Section 2.2.3) requires the user to specify a starting point for the integral, which is conceptually similar to specifying a reference for DeepLIFT.
89	13	Let neuron y be a nonlinear transformation of its input x such that y = f(x).
97	13	By contrast, gradient×input assigns a contribution of 10+ to x and−10 to the bias term (DeepLIFT never assigns importance to bias terms).
98	21	As revealed in previous work (Lundberg & Lee, 2016), there is a connection between DeepLIFT and Shapely values.
99	41	Briefly, the Shapely values measure the average marginal effect of including an input over all possible orderings in which inputs can be included.
104	25	3, with reference values of i1 = 0 and i2 = 0.
106	36	This can obscure the fact that both inputs are relevant for the min operation.
108	39	We have h1 = (i1 − i2) > 0 and h2 = max(0, h1) = h1.
114	290	Note that gradients, gradient×input, Guided Backpropagation and integrated gradients would also assign all importance to either i1 or i2, because for any given input the gradient is zero for one of i1 or i2 (see Appendix C for a detailed calculation).
115	62	One way to address this is by treating the positive and negative contributions separately.
119	42	By considering the impact of the positive terms in the absence of negative terms, and the impact of negative terms in the absence of positive terms, we alleviate some of the issues that arise from positive and negative terms canceling each other out.
