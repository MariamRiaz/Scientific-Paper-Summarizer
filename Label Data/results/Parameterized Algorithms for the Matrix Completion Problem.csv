0,1,label2,summary_sentences
"Given a graph G = (V,E) on n vertices with adjacency matrix A ∈ {0, 1}n×n, the problem of graph embedding is to map the vertices of G to some d-dimensional vector space S in such a way that geometry in S reflects the topology of G. For example, we may ask that vertices with high conductance in G be assigned to nearby vectors in S.",1. Introduction,[0],[0]
"This is a special case of the problem of dimensionality reduction, well-studied in machine learning and related disciplines (van der Maaten et al., 2009).",1. Introduction,[0],[0]
"When applied to graph data, each vertex in G is described by an n-dimensional binary
1Department of Statistics, University of Michigan, USA.",1. Introduction,[0],[0]
"2School of Mathematics and Physics, University of Queensland, Australia.",1. Introduction,[0],[0]
"3International Computer Science Institute, Berkeley, USA.",1. Introduction,[0],[0]
"4Department of Statistics, University of California at Berkeley, USA.",1. Introduction,[0],[0]
"5Department of Applied Mathematics and Statistics, Johns Hopkins University, USA.",1. Introduction,[0],[0]
"Correspondence to: Keith Levin <klevin@umich.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
vector, namely its corresponding column (or row) in adjacency matrix A ∈ {0, 1}n×n, and we wish to associate with each vertex v ∈ V a lower-dimensional representation, say xv ∈ S.",1. Introduction,[0],[0]
"The two most commonly-used approaches for graph embeddings are the graph Laplacian embedding and its variants (Belkin & Niyogi, 2003; Coifman & Lafon, 2006) and the adjacency spectral embedding (ASE, Sussman et al., 2012).",1. Introduction,[0],[0]
"Both of these embedding procedures produce low-dimensional representations of the vertices in a graph G, and the question of “which embedding is preferable?” is dependent on the downstream task.",1. Introduction,[0],[0]
"Indeed, one can show that neither embedding dominates the other for the purposes of vertex classification; see, for example, Section 4.3 of Tang & Priebe (to appear).",1. Introduction,[0],[0]
"In addition, the results in Section 4.3 of Tang & Priebe (to appear) suggest that ASE performs better than the Laplacian eigenmaps embedding for graphs that exhibit a core-periphery structure.",1. Introduction,[0],[0]
"Such structures are ubiquitous in real networks, such as those arising in social and biological sciences (Jeub et al., 2015; Leskovec et al., 2009).
",1. Introduction,[0],[0]
"The ASE and Laplacian embedding differ in that the latter has received far more attention, especially with respect to questions of limit objects (Hein et al., 2005) and out-ofsample extensions (Bengio et al., 2003).",1. Introduction,[0],[0]
The aim of this paper is to establish theoretical foundations for the latter of these two problems in the case of the adjacency spectral embedding.,1. Introduction,[0],[0]
"In the standard out-of-sample (OOS) extension, we are presented with training dataD",2. Background and Notation,[0],[0]
"= {z1, z2, . . .",2. Background and Notation,[0],[0]
", zn} ⊆ X , where X is the set of possible observations.",2. Background and Notation,[0],[0]
"The data D give rise to a symmetric matrix M = [K(zi, zj)] ∈",2. Background and Notation,[0],[0]
"Rn×n, where K : X × X → R≥0 is a kernel function that measures similarity between elements of X , so that K(y, z) is large if y, z ∈ X are similar, and is small otherwise.",2. Background and Notation,[0],[0]
"Suppose that we have computed an embedding of the data D. Let us denote this embedding by X ∈ Rn×d, so that the embedding of zi ∈ D is given by the i-th row of X .",2. Background and Notation,[0],[0]
"Suppose that we are given an additional observation z ∈ X , not necessarily included in D, and we wish to embed z under the same scheme as was used to produce X .",2. Background and Notation,[0],[0]
"A naı̈ve approach would be to discard the old embedding X , consider the augmented
collection D = D ∪ {z} and construct a new embedding X̃ ∈ R(n+1)×d.",2. Background and Notation,[0],[0]
"However, in many applications, it is infeasible to compute this embedding again from scratch, either because of computational constraints or because the similarities {K(zi, zj) : zi, zj ∈ D} may no longer be available after X has been computed.",2. Background and Notation,[0],[0]
"Thus, the OOS problem is to embed z using only the available embedding X which was initially learned from D and the similarities {K(zi, z)}ni=1.
",2. Background and Notation,[0],[0]
"As an example, consider the Laplacian eigenmaps embedding (Belkin & Niyogi, 2003; Belkin et al., 2006).",2. Background and Notation,[0],[0]
"Given a graph G = (V,E) with adjacency matrix A ∈ Rn×n, the d-dimensional normalized Laplacian of G is the matrix L = D−1/2AD−1/2, where D ∈ Rn×n is the diagonal degree matrix, i.e., dii = ∑ j Aij is the degree of the vertex",2. Background and Notation,[0],[0]
"i (Luxburg, 2007; Vishnoi, 2013).",2. Background and Notation,[0],[0]
"The d-dimensional normalized Laplacian eigenmaps embedding of G is given by the rows of the matrix UL ∈ Rn×d, whose columns are the d orthonormal eigenvectors corresponding to the top d eigenvalues of L, excepting the trivial eigenvalue 1.",2. Background and Notation,[0],[0]
"We note that some authors (see, for example, Chung, 1997) use I −D−1/2AD−1/2 to be the normalized graph Laplacian, but since this matrix has the same eigenspace as our L, results concerning the eigenvectors of either of these matrices are equivalent.",2. Background and Notation,[0],[0]
"Suppose that a vertex v is added to graph G, to form graph G̃ with adjacency matrix
Ã =",2. Background and Notation,[0],[0]
"[ A ~a ~aT 0 ] , (1)
where ~a ∈ {0, 1}n.",2. Background and Notation,[0],[0]
A naı̈ve approach to embedding G̃ would be to compute the top eigenvectors of the graph Laplacian of G̃ as before.,2. Background and Notation,[0],[0]
"However, the OOS extension problem requires that we only use the information available in UL and ~a to compute an embedding of the new vertex v.
Bengio et al. (2003) presented out-of-sample extensions for multidimensional scaling (MDS, Torgerson, 1952; Borg & Groenen, 2005), spectral clustering (Weiss, 1999; Ng et al., 2002), Laplacian eigenmaps (Belkin & Niyogi, 2003) and ISOMAP (Tenenbaum et al., 2000).",2. Background and Notation,[0],[0]
"These OOS extensions were based on a least-squares formulation of the embedding problem, arising from the fact that the in-sample embeddings are given by functions of the eigenvalues and eigenfunctions.",2. Background and Notation,[0.9572630038533027],['DRMC and p-DRMC are in FPT when parameterized by the treewidth of the compatibility graph.']
Trosset & Priebe (2008) considered a different OOS extension for MDS.,2. Background and Notation,[0],[0]
"Rather than following the approach of Bengio et al. (2003), Trosset & Priebe (2008) cast the MDS OOS extension as a simple modification of the in-sample MDS optimization problem.
",2. Background and Notation,[0],[0]
"Let {(λt, vt)}nt=1 be the eigen-pairs of the matrix M , constructed from some suitably-chosen similarity function, K, defined on pairs of observations in D ×D. In general, OOS extensions for eigenvector-based embeddings can be derived as in Bengio et al. (2003) as the solution of a least-squares
problem
min f(x)∈Rd n∑ i=1
( K(x, xi)− 1
n d∑ t=1 λtft(xi)ft(x)
)2 ,
where {xi}ni=1 are the in-sample observations, and ft(xi) =",2. Background and Notation,[0],[0]
[vt]i is ith component of vt.,2. Background and Notation,[0],[0]
Belkin et al. (2006) presented a slightly different approach that incorporates regularization in both the intrinsic geometry of the data distribution and the geometry of the similarity function K.,2. Background and Notation,[0],[0]
Their approach applies to Laplacian eigenmaps as well as to regularized least squares and SVM.,2. Background and Notation,[0],[0]
"The authors also introduced a Laplacian SVM, in which a Laplacian penalty term is added to the standard SVM objective function.",2. Background and Notation,[0],[0]
Belkin et al. (2006) showed that all of these embeddings have OOS extensions that arise as the solution of a generalized eigenvalue problem.,2. Background and Notation,[0],[0]
We refer the interested reader to Levin et al. (2015) for a practical application of this OOS extension.,2. Background and Notation,[0],[0]
"More recent approaches to OOS extension have avoided altogether the need to solve a least squares or eigenvalue problem by, instead, training a neural net to learn the embedding directly (see, for example, Quispe et al., 2016; Jansen et al., 2017).
",2. Background and Notation,[0],[0]
The only existing work to date on the ASE OOS extension of which we are aware appears in Tang et al. (2013a).,2. Background and Notation,[0],[0]
"The authors considered the OOS extension for ASE applied to latent position graphs (see, for example Hoff et al., 2002), in which each vertex is associated with an element of a vector space and edge probabilities are given by a suitably-chosen inner product.",2. Background and Notation,[0],[0]
"The authors introduced a least-squares OOS extension for embeddings of latent position graphs and proved a theorem, analogous to our Theorem 1, for the error of this extension about the true latent position.",2. Background and Notation,[0],[0]
"Theorem 1 simplifies the proof of the result due to Tang et al. (2013a) for the case of random dot product graphs (see Definition 1 below).
",2. Background and Notation,[0],[0]
"Of crucial importance in assessing OOS extensions, but largely missing from the existing literature, is an investigation of how the OOS estimate compares with the insample embedding.",2. Background and Notation,[0],[0]
"That is, for an out-of-sample observation z ∈ X , how well does its OOS embedding X̂z ∈ Rd, approximate the embedding that would be obtained by considering the full sample D = D ∪ {z}?",2. Background and Notation,[0],[0]
"In this paper, we address this question in the context of the adjacency spectral embedding.",2. Background and Notation,[0],[0]
"In particular, we show in our main results, Theorems 1 and 2, that two different approaches to the ASE OOS extension recover the in-sample embedding at a rate that is, in a certain sense, optimal (see the discussion at the end of Section 4).",2. Background and Notation,[0],[0]
We conjecture that analogous rate results can be obtained for other OOS extensions such as those presented in Bengio et al. (2003).,2. Background and Notation,[0],[0]
We pause briefly to establish notational conventions for this paper.,2.1. Notation,[0],[0]
"For a matrix B ∈ Rn1×n2 , we let σi(B) denote the i-th singular value of B, so that σ1(B) ≥ σ2(B) ≥ · · · ≥ σk(B) ≥ 0, where k = min{n1, n2}.",2.1. Notation,[0],[0]
"For positive integer n, we let [n] = {1, 2, . . .",2.1. Notation,[0],[0]
", n}.",2.1. Notation,[0],[0]
"Throughout this paper, n will index the number of vertices in a hollow graph G, the observed data, and we let",2.1. Notation,[0],[0]
"c > 0 denote a positive constant, not depending on n, whose value may change from line to line.",2.1. Notation,[0],[0]
"For an event E, we let Ec denote its complement.",2.1. Notation,[0],[0]
"We will say that event En, indexed so as to depend on n, occurs with high probability, and write En w.h.p.",2.1. Notation,[0],[0]
", if for some constant > 0, it holds for all suitably large n that Pr[Ecn] ≤ n−(1+ ).",2.1. Notation,[0],[0]
"In this paper, we will show Pr[Ec] ≤",2.1. Notation,[0],[0]
cn−2 any time we wish to show that event E occurs with high probability.,2.1. Notation,[0],[0]
"All our results involve showing that some event En occurs w.h.p., and we note that in all such cases, the Borel-Cantelli Lemma implies that with probability 1, the event Ecn occurs for at most finitely many n. That is, all our finite-sample results can be easily altered to yield corresponding asymptotic results, as well.",2.1. Notation,[0],[0]
"For a function f : Z≥0 → R≥0 and a sequence of random variables {Zn}, we will write Zn = O(f(n))",2.1. Notation,[0],[0]
"if there exists a constant C and a number n0 such that Zn ≤ Cf(n) for all n ≥ n0, and write Zn = O(f(n))",2.1. Notation,[0],[0]
a.s.,2.1. Notation,[0],[0]
if the event Zn ≤ Cf(n) occurs a.s.a.a.,2.1. Notation,[0],[0]
"For a vector x ∈ Rd, we use the unadorned norm ‖x‖ to denote the Euclidean norm of x. For a matrix M ∈ Rn×d, we use the unadorned norm ‖M‖ to denote the operator norm
‖M‖ = max x∈Rd:‖x‖=1 ‖Mx‖
and we use ‖ · ‖2→∞ to denote the matrix operator norm
‖M‖2→∞ = max x:‖x‖=1",2.1. Notation,[0],[0]
"‖Mx‖∞ = max i∈[n] ‖Mi,·‖,
which can be proven via the Cauchy-Schwarz inequality (Horn & Johnson, 2013).",2.1. Notation,[0],[0]
"This latter operator norm will be especially useful for us, in that a bound on ‖M‖2→∞ gives a uniform bound on the rows of matrix M .",2.1. Notation,[0],[0]
The remainder of this paper is structured as follows.,2.2. Roadmap,[0],[0]
"In Section 3, we present two OOS extensions of the ASE.",2.2. Roadmap,[0],[0]
"In Section 4, we prove convergence of these two OOS extensions when applied to random dot product graphs.",2.2. Roadmap,[0],[0]
"In Section 5, we explore the empirical performance of the two extensions presented in Section 3, and we conclude with a brief discussion in Section 6.",2.2. Roadmap,[0],[0]
"Given a graph G encoded by adjacency matrix A ∈ {0, 1}n×n, the adjacency spectral embedding (ASE) pro-
duces a d-dimensional embedding of the vertices ofG, given by the rows of the n-by-d matrix
X̂ = UAS 1/2 A , (2)
where UA ∈ Rn×d is a matrix with orthonormal columns given by the d eigenvectors corresponding to the top d eigenvalues of A, which we collect in the diagonal matrix SA ∈ Rd×d.",3. Out-of-sample Embedding for ASE,[0],[0]
"We note that in general, one would be better-suited to consider the matrix [ATA]1/2, so that all eigenvalues are guaranteed to be nonnegative, but we will see that in the random dot product graph, the model that is the focus of this paper, the top d eigenvalues of A are positive with high probability (see, for example, either Lemma 1 in Athreya et al. (2016) or Observation 2 in Levin et al. (2017), or refer to the technical report, (Levin et al., 2018)).
",3. Out-of-sample Embedding for ASE,[0],[0]
"The random dot product graph (RDPG, Young & Scheinerman, 2007) is an edge-independent random graph model in which the graph structure arises from the geometry of a set of latent positions, i.e., vectors associated to the vertices of the graph.",3. Out-of-sample Embedding for ASE,[0],[0]
"As such, the adjacency spectral embedding is particularly well-suited to this model.
",3. Out-of-sample Embedding for ASE,[0],[0]
Definition 1.,3. Out-of-sample Embedding for ASE,[0],[0]
(Random Dot Product Graph) Let F be a distribution on Rd such that,3. Out-of-sample Embedding for ASE,[0],[0]
xT,3. Out-of-sample Embedding for ASE,[0],[0]
y ∈,3. Out-of-sample Embedding for ASE,[0],[0]
"[0, 1] whenever x, y ∈ suppF , and let X1, X2, . . .",3. Out-of-sample Embedding for ASE,[0],[0]
", Xn be drawn i.i.d.",3. Out-of-sample Embedding for ASE,[0],[0]
from F .,3. Out-of-sample Embedding for ASE,[0],[0]
Collect these n random points in the rows of a matrix X ∈ Rn×d.,3. Out-of-sample Embedding for ASE,[0],[0]
"Suppose that (symmetric) adjacency matrix A ∈ {0, 1}n×n is distributed in such a way that
Pr[A|X] = ∏
1≤i<j≤n
(XTi Xj) Aij (1−XTi Xj)1−Aij .",3. Out-of-sample Embedding for ASE,[0],[0]
"(3)
When this is the case, we write (A,X) ∼ RDPG(F, n).",3. Out-of-sample Embedding for ASE,[0],[0]
"If G is the random graph corresponding to adjacency matrix A, we say that G is a random dot product graph with latent positions X1, X2, . . .",3. Out-of-sample Embedding for ASE,[0],[0]
", Xn, where Xi is the latent position corresponding to the i-th vertex.
",3. Out-of-sample Embedding for ASE,[0],[0]
"A number of results exist showing that the adjacency spectral embedding yields consistent estimates of the latent positions in a random dot product graph (Sussman et al., 2012; Tang et al., 2013b) and recovers community structure in the stochastic block model (Lyzinski et al., 2014).",3. Out-of-sample Embedding for ASE,[0],[0]
"We note an inherent nonidentifiability in the random dot product graph, arising from the fact that for any orthogonal matrix W ∈ Rd×d, the latent positions X ∈ Rn×d and XW ∈ Rd×d give rise to the same distribution over graphs, since XXT = (XW )(XW )T = E[A | X].",3. Out-of-sample Embedding for ASE,[0],[0]
"Owing to this nonidentifiability, we can only hope to recover the latent positions in X up to some orthogonal rotation.",3. Out-of-sample Embedding for ASE,[0],[0]
The reader may notice that the RDPG as defined has the limitation that it can only capture graphs with positive semi-definite expected value.,3. Out-of-sample Embedding for ASE,[0],[0]
"This limitation can be overcome by extending the RDPG to the generalized RDPG (Rubin-Delanchy
et al., 2017).",3. Out-of-sample Embedding for ASE,[0],[0]
"The results stated in the present work can, for the most part, be extended to this generalized model, but we restrict ourselves here to the RDPG as it appears in Definition 1 for the sake of simplicity.
",3. Out-of-sample Embedding for ASE,[0],[0]
"Suppose that, given adjacency matrix A, we compute embedding
X̂ =",3. Out-of-sample Embedding for ASE,[0],[0]
[X̂1X̂2 . . .,3. Out-of-sample Embedding for ASE,[0],[0]
"X̂n] T ,
where X̂i ∈ Rd denotes the embedding of the i-th vertex.",3. Out-of-sample Embedding for ASE,[0],[0]
Now suppose we add a vertex v with latent position w̄ ∈,3. Out-of-sample Embedding for ASE,[0],[0]
"Rd to the original graph G, obtaining an augmented graph G̃ =",3. Out-of-sample Embedding for ASE,[0],[0]
(,3. Out-of-sample Embedding for ASE,[0],[0]
"[n] ∪ {v}, E ∪ Ev), where Ev denotes the set of edges between v and the vertices of G. One would like to embed vertex v according to the same distribution as the original n vertices and obtain an estimate of w̄.",3. Out-of-sample Embedding for ASE,[0],[0]
"Let the binary vector ~a ∈ {0, 1}n encode the edges Ev incident upon vertex v, with entries ai = (~a)i ∼ Bernoulli(XTi w̄).",3. Out-of-sample Embedding for ASE,[0],[0]
The augmented graph G̃ then has the adjacency matrix as in (1).,3. Out-of-sample Embedding for ASE,[0],[0]
"As discussed earlier, the natural approach to embedding vertex v is to simply re-embed the whole matrix G̃ by computing the ASE of Ã. Suppose that we wish to avoid such a computation, for example due to resource constraints.",3. Out-of-sample Embedding for ASE,[0],[0]
The problem then becomes one of embedding the new vertex v based solely on the information present in X̂ and ~a.,3. Out-of-sample Embedding for ASE,[0],[0]
Two natural approaches to such an OOS extension suggest themselves.,3. Out-of-sample Embedding for ASE,[0],[0]
"A natural approach to OOS embedding, pursued by, for example, Bengio et al. (2003), is to embed vertex v as the least-squares solution to X̂w = ~a.",3.1. Linear Least Squares OOS Extension,[0],[0]
"That is, we embed the vertex v as the vector ŵLS solving
min w∈Rd n∑ i=1",3.1. Linear Least Squares OOS Extension,[0],[0]
"( ai − X̂Ti w )2 , (4)
where ai denotes the i-th component of the binary vector ~a encoding the edges between v and the original n vertices.",3.1. Linear Least Squares OOS Extension,[0],[0]
"We will denote the solution to the least-squares optimization in Equation (4) by ŵLS, and term this the linear least squares out-of-sample (LLS OOS) embedding.",3.1. Linear Least Squares OOS Extension,[0],[0]
"A more principled approach to OOS extension, but perhaps more involved computationally, is to consider the following maximum-likelihood formulation.",3.2. Maximum Likelihood OOS Extension,[0],[0]
"The entries of the vector ~a are distributed independently as ai ∼ Bernoulli(XTi w̄), where w̄ denotes the true latent position of OOS vertex v.",3.2. Maximum Likelihood OOS Extension,[0],[0]
"Since we do not have access to the latent positions {Xi}ni=1, we use instead their estimates {X̂i}ni=1.",3.2. Maximum Likelihood OOS Extension,[0],[0]
"This yields the following objective:
max w∈Rd n∑ i=1 ai log X̂ T",3.2. Maximum Likelihood OOS Extension,[0],[0]
i w + (1− ai) log ( 1− X̂Ti w ) .,3.2. Maximum Likelihood OOS Extension,[0],[0]
"(5)
Unfortunately, this optimization problem may fail to achieve its optimum inside the support of F .",3.2. Maximum Likelihood OOS Extension,[0],[0]
"Indeed, it may not even have a finite solution.",3.2. Maximum Likelihood OOS Extension,[0],[0]
"Thus, we will instead settle for solving the following constrained modification of Equation (5),
max w∈T̂ n∑ i=1",3.2. Maximum Likelihood OOS Extension,[0],[0]
ai log,3.2. Maximum Likelihood OOS Extension,[0],[0]
X̂,3.2. Maximum Likelihood OOS Extension,[0],[0]
T,3.2. Maximum Likelihood OOS Extension,[0],[0]
"i w + (1− ai) log ( 1− X̂Ti w ) , (6)
",3.2. Maximum Likelihood OOS Extension,[0],[0]
where T̂ = {w ∈,3.2. Maximum Likelihood OOS Extension,[0],[0]
Rd : ≤ X̂Ti w ≤ 1,3.2. Maximum Likelihood OOS Extension,[0],[0]
"− , i ∈",3.2. Maximum Likelihood OOS Extension,[0],[0]
"[n]}, and > 0 is a small constant.",3.2. Maximum Likelihood OOS Extension,[0],[0]
"We note that this is based only on the edges incident on the OOS vertex rather than on the full data Ã, and uses the spectral estimates {X̂i}ni=1 rather than the true latent positions {Xi}ni=1.",3.2. Maximum Likelihood OOS Extension,[0],[0]
"Despite both of these facts, we will term the extension given by Equation (6) as the maximum-likelihood out-of-sample (ML OOS) extension, and we will let ŵML denote its solution.",3.2. Maximum Likelihood OOS Extension,[0],[0]
"Our main results show that both the linear least-squares and maximum-likelihood OOS extensions in Equations (4) and (6) recover the true latent position w̄ of v. Further, both OOS extensions converge to w̄ at the same asymptotic rate (i.e., up to a constant) as we would have obtained, had we computed the ASE of Ã in (1) directly.",4. Main Results,[0],[0]
"This rate is given by Lemma 2.5 from Lyzinski et al. (2014), which we state here in a slightly adapted form.",4. Main Results,[0],[0]
"The lemma states, in essence, that the ASE recovers the latent positions with error of order n−1/2 log n, uniformly over the n vertices.",4. Main Results,[0],[0]
"We remind the reader that ‖M‖2→∞ denotes the 2-to-∞ operator norm,",4. Main Results,[0],[0]
"‖M‖2→∞ = maxx:‖x‖=1 ‖Mx‖∞. Lemma 1 (Adapted from Lyzinski et al. (2014), Lemma 2.5).",4. Main Results,[0],[0]
Let X =,4. Main Results,[0],[0]
"[X1, X2, . . .",4. Main Results,[0],[0]
", Xn]T ∈ Rn×d be the matrix of latent positions of an RDPG, and let X̂ ∈ Rn×d denote the matrix of estimated latent positions yielded by ASE as in (2).",4. Main Results,[0],[0]
"Then with probability at least 1− cn−2, there exists orthogonal matrix W ∈ Rd×d such that
‖X̂",4. Main Results,[0],[0]
"−XW‖2→∞ ≤ c log n
n1/2 .
",4. Main Results,[0],[0]
"That is, it holds with high probability that for all i ∈",4. Main Results,[0],[0]
"[n],
‖X̂i",4. Main Results,[0],[0]
−WTXi‖ ≤,4. Main Results,[0],[0]
"c log n
n1/2 .
",4. Main Results,[0],[0]
"In what follows, we let A ∈ {0, 1}n×n denote the random adjacency matrix of an RDPGG, and letX1, X2, . . .",4. Main Results,[0],[0]
", Xn ∈ Rd denote its latent positions, collected in matrix X =",4. Main Results,[0],[0]
"[X1, X2, . . .",4. Main Results,[0],[0]
", Xn]
T ∈ Rn×d.",4. Main Results,[0],[0]
"That is, (A,X) ∼ RDPG(F, n).",4. Main Results,[0],[0]
We use X̂ =,4. Main Results,[0],[0]
"[X̂1, X̂2, . .",4. Main Results,[0],[0]
.,4. Main Results,[0],[0]
", X̂n]T ∈ Rn×d to denote the matrix whose rows are the estimated latent positions, obtained via ASE as in (2).",4. Main Results,[0],[0]
"We let w̄ denote the true latent position of the OOS vertex v.
Theorem 1.",4. Main Results,[0],[0]
"With notation as above, let ŵLS denote the least-squares estimate of w̄, i.e., the solution to (4).",4. Main Results,[0],[0]
"Then there exists an orthogonal matrix W ∈ Rd×d such that
‖WŵLS − w̄‖ ≤",4. Main Results,[0],[0]
"cn−1/2 log n w.h.p.
",4. Main Results,[0],[0]
Proof.,4. Main Results,[0],[0]
"The proof of this result relies upon a classic result for solutions of perturbed linear systems to establish that with high probability, ‖WŵLS",4. Main Results,[0],[0]
− wLS‖ ≤,4. Main Results,[0],[0]
"cn−1/2 log n, where W ∈ Rd×d is the orthogonal matrix guaranteed by Lemma 1 andwLS is the LS estimate based on the true latent positions {Xi} rather than on the estimates {X̂i}.",4. Main Results,[0],[0]
"A basic Hoeffding inequality to show that with high probability, ‖wLS − w̄‖ ≤",4. Main Results,[0],[0]
"cn−1/2 log n, where again W ∈ Rd×d is the orthogonal matrix in Lemma 1.",4. Main Results,[0],[0]
A triangle inequality applied to ‖WŵLS− w̄‖ combined with a union bound over the two high-probability events just described yields the result.,4. Main Results,[0],[0]
"A detailed version of this proof can be found in the technical report (Levin et al., 2018).
",4. Main Results,[0],[0]
"As mentioned in Section 3, we would like to consider a maximum-likelihood OOS extension based on the likelihood ˆ̀(w) = ∑n i=1 ai log",4. Main Results,[0],[0]
X̂ T,4. Main Results,[0],[0]
i w+ (1−ai) log(1− X̂Ti w).,4. Main Results,[0],[0]
"Toward this end, we would ideally like to use the solution to the optimization problem
arg max w∈Rd
ˆ̀(w),
but to ensure a sensible solution, we instead consider
ŵML = arg max w∈T̂
ˆ̀(w), (7)
where we remind the reader that T̂ = {w ∈",4. Main Results,[0],[0]
Rd : ≤ X̂Ti w ≤ 1,4. Main Results,[0],[0]
"− , i = 1, 2, . . .",4. Main Results,[0],[0]
", n}.",4. Main Results,[0],[0]
"Theorem 2 shows that ŵML recovers the true latent position of the OOS vertex, up to rotation, with error decaying at the same rate as that obtained in Theorem 1 for the LS OOS extension.
",4. Main Results,[0],[0]
Theorem 2.,4. Main Results,[0],[0]
"With notation as above, let ŵML be the estimate defined in Equation (7), and let > 0 be such that x, y ∈ suppF implies < xT",4. Main Results,[0],[0]
y,4. Main Results,[0],[0]
< 1 − .,4. Main Results,[0],[0]
Denote the true latent position of the OOS vertex v by w̄ ∈ suppF .,4. Main Results,[0],[0]
"Then for all n suitably large, there exists an orthogonal matrix W ∈ Rd×d such that with high probability,
‖WŵML − w̄‖ ≤",4. Main Results,[0],[0]
"cn−1/2 log n w.h.p.,
and this matrix W is the same one guaranteed by Lemma 1.
",4. Main Results,[0],[0]
Proof.,4. Main Results,[0],[0]
"By a standard argument from convex optimization, alongside the definition of T̂ , one can thow that for suitably large n,
‖WŵML − w̄‖ ≤",4. Main Results,[0],[0]
"c‖∇ˆ̀(WT w̄)‖
n w.h.p.
",4. Main Results,[0],[0]
"By the triangle inequality one can then show that
‖∇ˆ̀(WT w̄)‖ ≤",4. Main Results,[0],[0]
c,4. Main Results,[0],[0]
"√ n log n w.h.p.
",4. Main Results,[0],[0]
"A detailed proof can be found in (Levin et al., 2018).
",4. Main Results,[0],[0]
Remark 1.,4. Main Results,[0],[0]
"Given our in-sample embedding X̂ and the vector of edge indicators ~a, we can think of the OOS extension as an estimate of w̄, the latent position of the OOS vertex v. Lemma 1 implies that if we took the naı̈ve approach of applying ASE to the adjacency matrix Ã in (1), our estimate would have error of order at most O(n−1/2 log n).",4. Main Results,[0],[0]
"Theorems 1 and 2 imply that the OOS estimate obtains the same asymptotic estimation error, without recomputing the embedding of Ã.
",4. Main Results,[0],[0]
"In addition to the bounds in Theorems 1 and 2, we can show that the least-squares OOS extension satisfies a stronger property, namely the following central limit theorem.
",4. Main Results,[0],[0]
Theorem 3.,4. Main Results,[0],[0]
"Let (A,X) ∼ RDPG(F, n) be a ddimensional RDPG.",4. Main Results,[0],[0]
Let w̄ ∈,4. Main Results,[0],[0]
suppF,4. Main Results,[0],[0]
"and ŵLS ∈ Rd be, respectively, the latent position and the least-squares embedding from (4) of an OOS vertex v.",4. Main Results,[0],[0]
"There exists a sequence of orthogonal d× d matrices {Vn}∞n=1 such that
√ n(V Tn ŵLS",4. Main Results,[0],[0]
"− w̄) L−→ N (0,Σw̄),
where Σw̄ ∈ Rd×d is given by
Σw̄ = ∆ −1E",4. Main Results,[0],[0]
"[ XT1 w̄(1−XT1 w̄)X1XT1 ] ∆−1, (8)
and ∆ = EX1XT1 .
",4. Main Results,[0],[0]
Proof.,4. Main Results,[0],[0]
"This theorem follows from an adaptation of Theorem 1 in (Levin et al., 2017)",4. Main Results,[0],[0]
"A detailed proof can be found in (Levin et al., 2018).
",4. Main Results,[0],[0]
"If the OOS vertex is distributed according to F , we have the following corollary by integrating w̄ with respect to F .
",4. Main Results,[0],[0]
Corollary 1.,4. Main Results,[0],[0]
"Let (A,X) ∼ RDPG(F, n) be a ddimensional RDPG, and let w̄ be distributed according to F , independent of (A,X).",4. Main Results,[0],[0]
"Then there exists a sequence of orthogonal d× d matrices {Vn}∞n=1 such that
√ n(V Tn ŵLS − w̄) L−→ ∫ N (0,Σw)dF (w),
where Σw is defined as in Equation (8) above.
",4. Main Results,[0],[0]
We conjecture that a CLT analogous to Theorem 3 holds for the ML OOS extension.,4. Main Results,[0],[0]
"In this section, we briefly explore our results through simulations.",5. Experiments,[0],[0]
"We leave a more thorough experimental examination of our results, particularly as they apply to realworld data, for future work.",5. Experiments,[0],[0]
We first give a brief exploration of how quickly the asymptotic distribution in Theorem 3 becomes a good approximation.,5. Experiments,[0],[0]
"Toward this end, let us consider a simple mixture of point masses, F = Fλ,x1,x2 = λδx1 +(1−λ)δx2 , where x1, x2 ∈ R2 and λ ∈",5. Experiments,[0],[0]
"(0, 1).",5. Experiments,[0],[0]
"This corresponds to a two-block stochastic block model (Holland et al., 1983), in which the block probability matrix is given by [
xT1 x1 x T 1 x2",5. Experiments,[0],[0]
xT1 x2 x T 2,5. Experiments,[0],[0]
"x2
] .
",5. Experiments,[0],[0]
"Corollary 1 implies that if all latent positions (including the OOS vertex) are drawn according to F , then the OOS estimate should be distributed as a mixture of normals centered at x1 and x2, with respective mixing coefficients λ and 1− λ.
",5. Experiments,[0],[0]
"To assess how well the asymptotic distribution predicted by Theorem 3 and Corollary 1 holds, we generate RDPGs with latent positions drawn i.i.d.",5. Experiments,[0],[0]
"from distribution F = Fλ,x1,x2 defined above, with
λ = 0.4, x1 = (0.2, 0.7) T , and x2 = (0.65, 0.3)T .
",5. Experiments,[0],[0]
"For each trial, we draw n+ 1 independent latent positions from F , and generate a binary adjacency matrix from these latent positions.",5. Experiments,[0],[0]
We let the (n+1)-th vertex be the OOS vertex.,5. Experiments,[0],[0]
"Retaining the subgraph induced by the first n vertices, we obtain an estimate X̂ ∈",5. Experiments,[0],[0]
"Rn×2 via ASE, from which we obtain an estimate for the OOS vertex via the LS OOS extension as defined in (4).",5. Experiments,[0],[0]
"We remind the reader that for each RDPG draw, we initially recover the latent positions
only up to a rotation.",5. Experiments,[0],[0]
"Thus, for each trial, we compute a Procrustes alignment (Gower & Dijksterhuis, 2004) of the in-sample estimates X̂ to their true latent positions.",5. Experiments,[0],[0]
"This yields a rotation matrix R, which we apply to the OOS estimate.",5. Experiments,[0],[0]
"Thus, the OOS estimates are sensibly comparable across trials.",5. Experiments,[0],[0]
"Figure 1 shows the empirical distribution of the OOS embeddings of 100 independent RDPG draws, for n = 50 (left), n = 100 (center) and n = 500 (right) in-sample vertices.",5. Experiments,[0],[0]
"Each cross is the location of the OOS estimate for a single draw from the RDPG with latent position distribution F , colored according to true latent position.",5. Experiments,[0],[0]
"OOS estimates with true latent position x1 are plotted as blue crosses, while OOS estimates with true latent position x2 are plotted as red crosses.",5. Experiments,[0],[0]
"The true latent positions x1 and x2 are plotted as solid circles, colored accordingly.",5. Experiments,[0],[0]
"The plot includes contours for the two normals centered at x1 and x2 predicted by Theorem 3 and Corollary 1, with the ellipses indicating the isoclines corresponding to one and two (generalized) standard deviations.
",5. Experiments,[0],[0]
"Examining Figure 1, we see that even with only 100 vertices, the mixture of normal distributions predicted by Theorem 3 holds quite well, with the exception of a few gross outliers from the blue cluster.",5. Experiments,[0],[0]
"With n = 500 vertices, the approximation is particularly good.",5. Experiments,[0],[0]
"Indeed, the n = 500 case appears to be slightly under-dispersed, possibly due to the Procrustes alignment.",5. Experiments,[0],[0]
It is natural to wonder whether a similarly good fit is exhibited by the ML-based OOS extension.,5. Experiments,[0],[0]
We conjectured at the end of Section 4 that a CLT similar to that in Theorem 3 would also hold for the ML-based OOS extension as defined in Equation (7).,5. Experiments,[0],[0]
"Figure 2 shows the empirical distribution of 100 independent OOS estimates, under the same experimental setup as Figure 1, but using the ML OOS extension rather than the linear least-squares extension.",5. Experiments,[0],[0]
"The plot supports our conjecture that the ML-based OOS estimates are also approximately normally distributed
about the true latent positions.
",5. Experiments,[0],[0]
Figure 1 suggests that we may be confident in applying the large-sample approximation suggested by Theorem 3 and Corollary 1.,5. Experiments,[0],[0]
"Applying this approximation allows us to investigate the trade-offs between computational cost and classification accuracy, to which we now turn our attention.",5. Experiments,[0],[0]
"The mixture distribution Fλ,x1,x2 above suggests a task in which, given an adjacency matrix A, we wish to classify the vertices according to which of two clusters or communities they belong.",5. Experiments,[0],[0]
"That is, we will view two vertices as belonging to the same community if their latent positions are the same (Holland et al., 1983, i.e., the latent positions specify an SBM,).",5. Experiments,[0],[0]
"More generally, one may view the task of recovering vertex block memberships in a stochastic block model as a clustering problem.",5. Experiments,[0],[0]
"Lyzinski et al. (2014) showed that applying ASE to such a graph, followed by k-means clustering of the estimated latent positions, correctly recovers community memberships of all the vertices (i.e., correctly assigns all vertices to their true latent positions) with high probability.
",5. Experiments,[0],[0]
"For concreteness, let us consider a still simpler mixture model, F = Fλ,p,q = λδp+ (1−λ)δq , where 0 < p",5. Experiments,[0],[0]
"< q < 1, and draw an RDPG (Ã,X) ∼ RDPG(F, n+m), taking the first n vertices to be in-sample, with induced adjacency matrix A ∈ Rn×n.",5. Experiments,[0],[0]
"That is, we draw the full matrix
Ã =",5. Experiments,[0],[0]
"[ A B BT C ] ,
where C ∈ Rm×m is the adjacency matrix of the subgraph induced by them OOS vertices andB ∈ Rn×m encodes the edges between the in-sample vertices and the OOS vertices.",5. Experiments,[0],[0]
"The latent positions p and q encode a community structure in the graph Ã, and, as alluded to above, a common task in network statistics is to recover this community structure.
",5. Experiments,[0],[0]
"Let w̄(1), w̄(2), . . .",5. Experiments,[0],[0]
", w̄(m) ∈ {p, q} denote the true latent positions of the m OOS vertices, with respective least-squares OOS estimates ŵ(1)LS , ŵ (2) LS , . . .",5. Experiments,[0],[0]
", ŵ (m) LS , each obtained from the in-sample ASE X̂ ∈",5. Experiments,[0],[0]
Rn of A.,5. Experiments,[0],[0]
"We note that one could devise a different OOS embedding procedure that makes use of the subgraph C induced by these m OOS vertices, but we leave the development of such a method to future work.",5. Experiments,[0],[0]
Corollary 1 implies that each ŵ(t)LS for t ∈,5. Experiments,[0],[0]
"[m] is marginally (approximately) distributed as
ŵ (t) LS ∼ λN",5. Experiments,[0],[0]
"(p, (n+ 1) −1σ2p) + (1−λ)N (q, (n+ 1)−1σ2q ),
where σ2p = ∆ −2",5. Experiments,[0],[0]
"(λp2(1− p2)p2 + (1− λ)pq(1− pq)q2) ,
σ2q = ∆ −2",5. Experiments,[0],[0]
"(λpq(1− pq)p2 + (1− λ)q2(1− q2)q2) ,
and ∆ =",5. Experiments,[0],[0]
"λp2 + (1− λ)q2.
",5. Experiments,[0],[0]
"Classifying the t-th OOS vertex based on ŵ(t)LS via likelihood ratio thus has (approximate) probability of error
ηn,p,q = λ(1−",5. Experiments,[0],[0]
"Φ (√
n+ 1(xn+1,p,q − p) σp ) + (1− λ)Φ (√ n+ 1(xn+1,p,q − q)
σq
) ,
where Φ denotes the cdf of the standard normal and xn,p,q is the value of x solving
λσ−1p exp{n(x− p)2/(2σ2p)} =",5. Experiments,[0],[0]
"(1− λ)σ−1q exp{n(x− q)2/(2σ2q )},
and hence our overall error rate when classifying the m OOS vertices will grow as mηn+1,p,q .
",5. Experiments,[0],[0]
"As discussed previously, the OOS extension allows us to avoid the expense of computing the ASE of the full matrix
Ã =",5. Experiments,[0],[0]
"[ A B BT C ] .
",5. Experiments,[0],[0]
"The LLS OOS extension is computationally inexpensive, requiring only the computation of the matrix-vector product S−1/2A U T A~a, with a time complexity O(d
2n) (assuming one does not precompute the product S−1/2A U T A ).",5. Experiments,[0],[0]
The eigenvalue computation required for embedding Ã is far more expensive than the LLS OOS extension.,5. Experiments,[0],[0]
"Nonetheless, if one were intent on reducing the OOS classification error ηn+1,p,q, one might consider paying the computational expense of embedding Ã to obtain estimates w̃(1), w̃(2), . . .",5. Experiments,[0],[0]
", w̃(m) of the m OOS vertices.",5. Experiments,[0],[0]
"That is, we obtain estimates for the m OOS vertices by making them insample vertices, at the expense of solving an eigenproblem on the (m + n)-by-(m + n) adjacency matrix.",5. Experiments,[0],[0]
"Of course, the entire motivation of our approach is that the in-sample matrix A may not be available.",5. Experiments,[0],[0]
"Nonetheless, a comparison against this baseline, in which all data is used to compute our embeddings, is instructive.
",5. Experiments,[0],[0]
"Theorem 1 in Athreya et al. (2016) implies that the w̃(t) estimates based on embedding the full matrix Ã are (approximately) marginally distributed as
w̃(t) ∼ λN",5. Experiments,[0],[0]
"(p, (n+m)−1σ2p)+(1−λ)N (q, (n+m)−1σ2q ),
with classification error
ηn+m,p,q = λΦ
( p− xn+m,p,q
σp ) + (1− λ)Φ ( xn+m,p,q",5. Experiments,[0],[0]
"− q
σq
) ,
where xn+m,p,q is the value of x solving
λσ−1p exp{(m+ n)(x− p)2/(2σ2p)}",5. Experiments,[0],[0]
=,5. Experiments,[0],[0]
"(1− λ)σ−1q exp{(m+ n)(x− q)2/(2σ2q )},
and it can be checked that ηn+m,q,p < ηn,q,p when m > 1.",5. Experiments,[0],[0]
"Thus, at the cost of computing the ASE of Ã, we may obtain a better estimate.",5. Experiments,[0],[0]
How much does this additional computation improve classification the OOS vertices?,5. Experiments,[0],[0]
"Figure 3 explores this question.
",5. Experiments,[0],[0]
"Figure 3 compares the error rates of the in-sample and OOS estimates as a function of m and n in the model just described, with λ = 0.4, p = 0.6 and q = 0.61.",5. Experiments,[0],[0]
"The plot depicts the ratio of the (approximate) in-sample classification error η(n+m),p,q to the (approximate) OOS classification error η(n+1),p,q , as a function of the number of OOS vertices m, for differently-sized in-sample graphs, n = 100, 1000, and 10000.",5. Experiments,[0],[0]
"We see that over several magnitudes of graph
size, the in-sample embedding does not improve appreciably over the OOS embedding except when multiple hundreds of OOS vertices are available.",5. Experiments,[0],[0]
"When hundreds or thousands of OOS vertices are available simultaneously, we see in the right-hand side of Figure 3 that the in-sample embedding classification error may improve upon the OOS classification error by a large multiplicative factor.",5. Experiments,[0],[0]
"Whether or not this improvement is worth the additional computational expense will, depend upon the available resources and desired accuracy, but this suggests that the additional expense associated with performing a second ASE computation is only worthwhile in the event that hundreds or thousands of OOS vertices are available simultaneously.",5. Experiments,[0],[0]
"This surfeit of OOS vertices is rather divorced from the typical setting of OOS extension problems, where one typically wishes to embed at most a few previously unseen observations.",5. Experiments,[0],[0]
"We have presented a theoretical investigation of two OOS extensions of the ASE, one based on a linear least squares estimate and the other based on a plug-in maximum-likelihood estimate.",6. Discussion and Conclusion,[0],[0]
"We have also proven a central limit theorem for the LLS-based extension, and simulation suggests that this CLT is a good approximation even with just a few hundred vertices.",6. Discussion and Conclusion,[0],[0]
"We conjecture that a similar CLT holds for the MLbased OOS extension, a conjecture supported by similar simulation data.",6. Discussion and Conclusion,[0],[0]
"Finally, we have given a brief illustration of how this OOS extension and the approximation it introduces might be weighed against the computational expense of recomputing a full graph embedding by examining how vertex classification error depends on the size of the set of OOS vertices.",6. Discussion and Conclusion,[0],[0]
We leave a more thorough exploration of this trade-off for future work.,6. Discussion and Conclusion,[0],[0]
"The authors would like to thank the reviewers for their helpful comments, which markedly improved the quality of this paper.",Acknowledgements,[0],[0]
Keith Levin was partially supported by NSF grant DMS-1646108.,Acknowledgements,[0],[0]
Farbod Roosta-Khorasani was partially supported by the Australian Research Council through a Discovery Early Career Researcher Award (DE180100923).,Acknowledgements,[0],[0]
Carey E. Priebe was supported by the DARPA D3M program through contract FA8750-17-2-0112.,Acknowledgements,[0],[0]
Farbod RoostaKhorasani and Michael Mahoney also gratefully acknowledge support from DARPA D3M.,Acknowledgements,[0],[0]
"Many popular dimensionality reduction procedures have out-of-sample extensions, which allow a practitioner to apply a learned embedding to observations not seen in the initial training sample.",abstractText,[0],[0]
"In this work, we consider the problem of obtaining an out-of-sample extension for the adjacency spectral embedding, a procedure for embedding the vertices of a graph into Euclidean space.",abstractText,[0],[0]
"We present two different approaches to this problem, one based on a least-squares objective and the other based on a maximum-likelihood formulation.",abstractText,[0],[0]
"We show that if the graph of interest is drawn according to a certain latent position model called a random dot product graph, then both of these out-of-sample extensions estimate the true latent position of the out-of-sample vertex with the same error rate.",abstractText,[0],[0]
"Further, we prove a central limit theorem for the least-squares-based extension, showing that the estimate is asymptotically normal about the truth in the large-graph limit.",abstractText,[0],[0]
Out-of-sample extension of graph adjacency spectral embedding,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1136–1145 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"The increasing availability of digitized historical corpora, together with newly developed tools of computational analysis, make the quantitative study of language change possible on a larger scale than ever before.",1 Introduction,[0],[0]
"Thus, many important questions may now be addressed using a variety of NLP tools that were originally developed to study synchronic similarities between words.",1 Introduction,[0],[0]
"This has catalyzed the evolution of an exciting new field
of historical distributional semantics, which has yielded findings that inform our understanding of the dynamic structure of language (Sagi et al., 2009; Wijaya and Yeniterzi, 2011; Mitra et al., 2014; Hilpert and Perek, 2015; Frermann and Lapata, 2016; Dubossarsky et al., 2016).",1 Introduction,[0],[0]
"Recent research has even proposed laws of change that predict the conditions under which the meaning of words is likely to change (Dubossarsky et al., 2015; Xu and Kemp, 2015; Hamilton et al., 2016).",1 Introduction,[0],[0]
"This is an important development, as traditional historical linguistics has generally been unable to provide predictive models of semantic change.
",1 Introduction,[0],[0]
"However, these preliminary results should be addressed with caution.",1 Introduction,[0],[0]
"To date, analyses of changes in words’ meanings have relied on the comparison of word representations at different points in time.",1 Introduction,[0],[0]
Thus any proposed change in meaning is contingent on a particular model of word representation and the method used to measure change.,1 Introduction,[0],[0]
Distributional semantic models typically count words and their co-occurrence statistics (explicit models) or predict the embedding contexts of words (implicit models).,1 Introduction,[0],[0]
"In this paper, we show that the choice of model may introduce biases into the analysis.",1 Introduction,[0],[0]
"We therefore suggest that empirical findings may be used to support laws of semantic change only after a proper control can be shown to eliminate artefactual factors as the underlying cause of the empirical observations.
",1 Introduction,[0],[0]
"Regardless of the specific representation used, a frequent method of measuring the semantic change a word has undergone (Gulordava and Baroni, 2011; Jatowt and Duh, 2014; Kim et al., 2014; Dubossarsky et al., 2015; Kulkarni et al., 2015; Hamilton et al., 2016) is to compare the word’s vector representations between two points in time using the cosine distance:
cosDist(x, y) = 1− x · y‖x‖2‖y‖2 (1)
1136
This choice naturally assumes that greater distances correspond to greater semantic changes.",1 Introduction,[0],[0]
"However, this measure introduces biases that may affect our interpretation of meaning change.
",1 Introduction,[0],[0]
"We examine various representations of word meaning, in order to identify inherent confounds when meaning change is evaluated using the cosine distance.",1 Introduction,[0],[0]
"In addition to the empirical evaluation, in Section 5 we provide an analytical account of the influence of word frequency on cosine distance scores when using these representations.
",1 Introduction,[0],[0]
"In our empirical investigation, we highlight the critical role of control conditions in the validation of experimental findings.",1 Introduction,[0],[0]
"Specifically, we argue that every observation about a change of meaning over time should be subjected to a control test.",1 Introduction,[0],[0]
"The control condition described in Section 2.1 is based on the construction of an artificially generated corpus, which resembles the historical corpus in most respects but where no change of meaning over time exists.",1 Introduction,[0],[0]
"In order to establish the validity of an observation about meaning change - and even more importantly, the validity of a lawlike generalization about meaning change - the result obtained in a genuine experimental condition should be demonstrated to be lacking (or at least significantly diminished) in the control condition.
",1 Introduction,[0],[0]
"As we show in Section 4, some recently reported laws of historical meaning change do not survive this proposed test.",1 Introduction,[0],[0]
"In other words, similar results are obtained in the genuine and control conditions.",1 Introduction,[0],[0]
"These include the correlation of meaning change with word frequency, polysemy (the number of different meanings a word has), and prototypicality (how representative a word is of its category).",1 Introduction,[0],[0]
"These factors lie at the basis of the following proposed laws of semantic change:
• The Law of Conformity, according to which frequency is negatively correlated with semantic change (Hamilton et al., 2016).
",1 Introduction,[0],[0]
"• The Law of Innovation, according to which polysemy is positively correlated with semantic change (Hamilton et al., 2016).
",1 Introduction,[0],[0]
"• The Law of Prototypicality, according to which prototypicality is negatively correlated with semantic change (Dubossarsky et al., 2015).
",1 Introduction,[0],[0]
"Our analysis shows that these laws have only residual effects, suggesting that frequency and
prototypicality may play a smaller role in semantic change than previously claimed.",1 Introduction,[0],[0]
The main artefact underlying the emergence of the first two laws in both the genuine and control conditions may be due to the SVD step used for the embedding of the PPMI word representation (see Section 2.5).,1 Introduction,[0],[0]
The historical corpus used here is Google Books 5-grams of English fiction.,2 Methods,[0],[0]
"Equally sized samples of 10 million 5-grams per year were randomly sampled for the period of 1900-1999 (Kim et al., 2014) to prevent the more prolific publication years from biasing the results, and were grouped into ten-year bins.",2 Methods,[0],[0]
"Uncommon words were removed, keeping the 100,000 most frequent words as the vocabulary for subsequent model learning.",2 Methods,[0],[0]
"All words were lowercased and stripped of punctuation.
",2 Methods,[0],[0]
"This corpus served as the genuine condition, and was used to replicate and evaluate findings from previous studies.",2 Methods,[0],[0]
"In this corpus, words are expected to change their meaning between decadal bins, as they do in a truly random sample of texts.",2 Methods,[0],[0]
"According to the distributional hypothesis (Firth, 1957), one can extract a word’s meaning from the contexts in which it appears.",2 Methods,[0],[0]
"Therefore, if words’ meanings change over time, as has been argued at least since Reisig (1839), it follows that the words’ contexts should change accordingly, and this change should be detected by our model.",2 Methods,[0],[0]
"Complementary to the genuine condition, a control condition was created where no change of meaning is expected.",2.1 Control condition setup,[0],[0]
"Therefore, any observed change in a word’s meaning in the control condition can only stem from random “noise“, while changes in meaning in the genuine condition are attributed to “real“ semantic change in addition to “noise“.",2.1 Control condition setup,[0],[0]
"Two methods were used to construct the corpus in the control condition:
Chronologically shuffled corpus (shuffle): 5- grams were randomly shuffled between decadal bins, so that each bin contained 5-grams from all the decades evenly.",2.1 Control condition setup,[0],[0]
This was chosen as a control condition for two reasons.,2.1 Control condition setup,[0],[0]
"First, this condition resembles the genuine condition in size of the vocabulary, size of the corpus, overall variance in words’ usage, and size of the decadal bins.",2.1 Control condition setup,[0],[0]
"Second and
crucially, words are not expected to show any apparent change in their meaning between decades in the control condition, because their various usage contexts are shuffled across decades.
",2.1 Control condition setup,[0],[0]
"One synchronous corpus (subsample): All 5- grams of the year 1999, which amount to 250 million 5-grams, were selected from Google Books English fiction.",2.1 Control condition setup,[0],[0]
"10 million 5-grams were randomly subsampled from this selection, and this process was repeated 30 times.",2.1 Control condition setup,[0],[0]
This is suggested as an additional control condition since the underlying assumption is always that words in the same year do not change their meaning.,2.1 Control condition setup,[0],[0]
"Again, unlike in the genuine condition, any changes that are observed based on these 30 subsamples can be attributed only to ”noise” that stems from random sampling, rather than real change in meaning.",2.1 Control condition setup,[0],[0]
Meaning change: Meaning change was evaluated as the cosine distance between vector representations of the same word in consecutive decades.,2.2 Measures of interest,[0],[0]
This was done separately for each processing stage (see Section 2.5).,2.2 Measures of interest,[0],[0]
"For the subsample condition, this was defined as the average cosine distance between the vectors in all 30 samples.
",2.2 Measures of interest,[0],[0]
Frequency: Words’ frequencies were computed separately for each decadal bin as the number of times a word appeared divided by the total number of words in that decade.,2.2 Measures of interest,[0],[0]
"For the subsample control condition, it was computed as the number of times a word appeared among the 250 million 5-grams, divided by the total number of words.",2.2 Measures of interest,[0],[0]
"To establish the adequacy of our control condition, we compared the meaning change scores (before log-transformation and standardization) between the genuine and the shuffled control conditions.",2.3 Construct validity,[0],[0]
Change scores were obtained by taking the average meaning change over all words in each decade using the representation of the final processing stage (SVD).,2.3 Construct validity,[0],[0]
"An adequate control condition will exhibit a lower degree of change compared to the genuine condition, and is expected to show a fixed rate of change across decades (see 3a).",2.3 Construct validity,[0],[0]
"Following common practice (Hamilton et al., 2016), the 10k most frequent words, as measured by their average decadal bin frequencies, were
used for the analysis of semantic change.",2.4 Statistical analysis,[0],[0]
"Change scores and frequencies were log-transformed, and all variables were subsequently standardized.
",2.4 Statistical analysis,[0],[0]
A linear mixed effects model was used to evaluate meaning change in both the genuine and shuffled control conditions.,2.4 Statistical analysis,[0],[0]
Frequency was set as a fixed effect while random intercepts were set per word.,2.4 Statistical analysis,[0],[0]
"The model attempts to account for semantic change scores using frequency, while controlling for the variability between words by assuming that each word’s behavior is strongly correlated across decades and independent across words as follows:
∆w(t)i =",2.4 Statistical analysis,[0],[0]
β0 + βffreq (t) wi + zwi,2.4 Statistical analysis,[0],[0]
"+ ε (t) wi (2)
",2.4 Statistical analysis,[0],[0]
"Here ∆w(t)i is the semantic change score of the i’th word measured between two specific consecutive decades, β0 is the model’s intercept, βf is the fixed-effect predictor coefficient for frequency, zwi ∼ N(0, σ) is a random intercept for the i’th word, and ε(t)wi is an error term associated with the i’th word.",2.4 Statistical analysis,[0],[0]
We report the predictor coefficient as well as the proportion of variance explained1 by each model.,2.4 Statistical analysis,[0],[0]
Only statistically significant results (p < .01) are reported.,2.4 Statistical analysis,[0],[0]
All statistical tests are performed in R (lme4 and MuMln packages).,2.4 Statistical analysis,[0],[0]
"We used a cascade of processing stages based on the explicit meaning representation of words (i.e., word counts, PPMI, SVD, as explained below) as commonly practiced (Baroni et al., 2014; Levy et al., 2015).",2.5 Word meaning representation,[0],[0]
"For each of these stages, we sought to evaluate the relationship between word frequency and meaning change, by computing the corresponding correlations between these two factors in the subsample control condition.
",2.5 Word meaning representation,[0],[0]
"Counts: Co-occurrence counts were collected for all the words in the vocabulary per decade.
",2.5 Word meaning representation,[0],[0]
PPMI:,2.5 Word meaning representation,[0],[0]
Sparse square matrices of vocabulary size containing positive pointwise mutual information (PPMI) scores were constructed for each decade based on the co-occurrence counts.,2.5 Word meaning representation,[0],[0]
"We used the context distribution smoothing parameter α = 0.75, as recommended by (Levy et al., 2015), using the following procedure:
PPMIα(w, c) = max ( log ( P̂ (w, c)
P̂ (w)P̂α(c)
) , 0 ) 1R2 for mixed linear models (Nakagawa and Schielzeth,
2013)
where P̂ (w, c) denotes the probability that word c appears as a context word of w, while P̂ (w) and P̂α(c) =
#(c)α∑ C #(c)
α denote the marginal probabilities of the word and its context, respectively.
SVD:",2.5 Word meaning representation,[0],[0]
"Each PPMI matrix was approximated by a truncated singular value decomposition as described in (Levy et al., 2015).",2.5 Word meaning representation,[0],[0]
"This embedding was shown to improve results on downstream tasks (Baroni et al., 2014; Bullinaria and Levy, 2012; Turney and Pantel, 2010).",2.5 Word meaning representation,[0],[0]
"Specifically, the top 300 elements of the diagonal matrix of singular values Σ, denoted Σd, were retained to represent a new, dense embedding of the word vectors, using the truncated left hand orthonormal matrix Ud:
WSV Di = (Ud · Σd)i (3) These representations were subsequently aligned with the orthogonal Procrustes method following (Hamilton et al., 2016).
",2.5 Word meaning representation,[0],[0]
"Relation to other models: (Levy and Goldberg) have shown that the Skip-Gram with Negative Sampling (SGNS) embedding model, e.g. word2vec (Mikolov et al., 2013) - perhaps the most popular model of word meaning representation, implicitly factorizes the values of the wordcontext PMI matrix.",2.5 Word meaning representation,[0],[0]
"Hence, the optimization goal and the sources of information available to SGNS and our model are in fact very similar.",2.5 Word meaning representation,[0],[0]
We therefore hypothesize that conclusions similar to those reported below can be drawn for SGNS models.,2.5 Word meaning representation,[0],[0]
There are many factors that may confound the measurement of meaning change.,3.1 Confound of frequency,[0],[0]
"Here we focus
on frequency, and investigate the existence of an artefactual relation between frequency and meaning change.",3.1 Confound of frequency,[0],[0]
This is done by evaluating this relation in the subsample control condition.,3.1 Confound of frequency,[0],[0]
"Any changes observed in this condition must be the consequence of inherent noise, since this control condition contains random samples from the same year (and the baseline assumption is that no change can be observed within the same year).
",3.1 Confound of frequency,[0],[0]
We first plotted the change scores that use the representation based on word count vs. word frequency.,3.1 Confound of frequency,[0],[0]
"This resulted in a robust correlation (r = −0.915) between the two variables, as shown in Fig.",3.1 Confound of frequency,[0],[0]
1a (see the analytical account in Section 5).,3.1 Confound of frequency,[0],[0]
"We repeated the same procedure using the PPMI representation, which showed a much weaker correlation with frequency (r = −0.295), see Fig. 1b.
",3.1 Confound of frequency,[0],[0]
"Finally, we repeated the same procedure using the final explicit representation after SVD embedding2, see Fig. 1c.",3.1 Confound of frequency,[0],[0]
"Surprisingly, the negative correlation with frequency was reinstated (r = −0.793).",3.1 Confound of frequency,[0],[0]
"To investigate how this came about,
2Similar results were obtained for the implicit embedding (word2vec-SGNS) described in Section 2.5.
",3.1 Confound of frequency,[0],[0]
we computed the change in the PPMI vectors before and after the low-rank SVD embedding using the cosine-distance.,3.1 Confound of frequency,[0],[0]
"As apparent from Fig. 2, it turns out that the SVD procedure distorts data in an uneven manner - frequent words are distorted less than infrequent words.",3.1 Confound of frequency,[0],[0]
Thus we demonstrate that this reinstatement of correlation between frequency and change scores is merely an artefactual consequence of the truncated SVD factorization.,3.1 Confound of frequency,[0],[0]
Potential confounding factors can be addressed by comparing any experimental finding to a validated control condition.,3.2 Construct validity,[0],[0]
Here we validate the use of the shuffled condition as a proper control.,3.2 Construct validity,[0],[0]
"To this end, the average change scores of words per decade in both the genuine and shuffled conditions are compared within each processing stage.",3.2 Construct validity,[0],[0]
"In the genuine condition, words appear in different usage contexts between decades, while in the shuffled condition they do not, because the random shuffling creates a homogeneous corpus.",3.2 Construct validity,[0],[0]
"Therefore, the validity of the control condition is established if: (a) the change scores are diminished as compared to the genuine condition; (b) change scores are uniform across decades (since decades are shuffled); (c) the variance of change scores is smaller that in the genuine condition.",3.2 Construct validity,[0],[0]
As seen in Fig.,3.2 Construct validity,[0],[0]
"3a, all these requirements are met by the control condition.",3.2 Construct validity,[0],[0]
"Note that the change scores in the shuffled condition are all significantly positive, namely, meaning change allegedly exists in this control condition.",3.2 Construct validity,[0],[0]
"This supports the claim that any measurement is significantly affected by unrelated noise.
",3.2 Construct validity,[0],[0]
"Thus, we have established that the shuffled condition is a suitable control for meaning change.
",3.2 Construct validity,[0],[0]
"While validity was established for each of the processing stages, the most robust effect was seen for the PPMI representation, following by SVD and word counts.",3.2 Construct validity,[0],[0]
In Section 3.1 we used the subsample control condition to establish the confounding effect of frequency on meaning change.,3.3 Accounting for the frequency confound,[0],[0]
We now examine the extent to which this frequency confound exists in a historical corpus.,3.3 Accounting for the frequency confound,[0],[0]
"We do so by comparing the frequency confound between the genuine historical corpus and the shuffled historical corpus.
",3.3 Accounting for the frequency confound,[0],[0]
"To visualize the frequency confound in a manner comparable to the analysis presented in Section 3.1, we again plot change scores vs. frequency, ignoring the time dimension of the data.",3.3 Accounting for the frequency confound,[0],[0]
Fig.,3.3 Accounting for the frequency confound,[0],[0]
3b presents this plot for the genuine condition.,3.3 Accounting for the frequency confound,[0],[0]
"The same analysis is repeated in the shuffled condition, see Fig.",3.3 Accounting for the frequency confound,[0],[0]
"3c.
",3.3 Accounting for the frequency confound,[0],[0]
Both plots reveal a highly significant correlation between change scores and frequency.,3.3 Accounting for the frequency confound,[0],[0]
"Furthermore, the fact that the correlation coefficients are virtually identical in the genuine and shuffled conditions, with r = −0.748 and r = −0.747 respectively, suggests that they are due to artefactual factors in both conditions and not to true change of meaning over time.",3.3 Accounting for the frequency confound,[0],[0]
"In fact, this pattern of results is reminiscent of the spurious pattern we see in Fig.",3.3 Accounting for the frequency confound,[0],[0]
"1c.
",3.3 Accounting for the frequency confound,[0],[0]
"The relation between frequency and meaning change can also be represented by a linear mixed effect model, with the benefit that this model enables the addition of more explanatory variables to the data.",3.3 Accounting for the frequency confound,[0],[0]
"The regression model found frequency to have a negative influence on change scores,
with βf=-0.91 and βf=-0.75, for the genuine and shuffled conditions respectively.",3.3 Accounting for the frequency confound,[0],[0]
"Importantly, frequency accounted for 67% of the variance in the change scores in the genuine condition, and was only slightly diminished in the shuffled condition, accounting for 56% of the variance.",3.3 Accounting for the frequency confound,[0],[0]
Similar results were obtained for the PPMI representation (see Table 1).,3.3 Accounting for the frequency confound,[0],[0]
"We replicated three recent results that were affected by this frequency effect, since they all define change as the word’s cosine distance relative to itself at two time points.",4 Revisiting previous studies,[0],[0]
"These studies report laws of semantic change that measure the role of frequency in semantic change either directly (Law of Conformity), or indirectly through another linguistic variable that is dependent on frequency (Laws of Innovation and Prototypicality).",4 Revisiting previous studies,[0],[0]
"Continuing the work described in Section 3.1, we replicated the model and analysis procedure described in (Hamilton et al., 2016), where two predictors were used together to explain the change scores: frequency and polysemy.",4.1 Laws of conformity and innovation,[0],[0]
"Polysemy, which describes the number of different senses a word has, naturally differs among words, where some words are more polysemous than others (compare bank and date to wine).",4.1 Laws of conformity and innovation,[0],[0]
"Following (Hamilton et al., 2016), we defined polysemy as the words’ secondary connections patterns - the connections between each word’s co-occurring words (using the entries in the PPMI representation for that word).",4.1 Laws of conformity and innovation,[0],[0]
"The more interconnected these secondary connections are, the less polysemic a word is, and vice versa.",4.1 Laws of conformity and innovation,[0],[0]
"Polysemy scores were com-
puted using the authors’ provided code3.",4.1 Laws of conformity and innovation,[0],[0]
We then log-transformed and standardized the polysemy scores.,4.1 Laws of conformity and innovation,[0],[0]
"Next, frequency and polysemy were set as two fixed effect predictors in a linear mixed effect model, like the one described in Section 2.4.
",4.1 Laws of conformity and innovation,[0],[0]
"Thus we were able to replicate the results in the genuine condition as reported in (Hamilton et al., 2016).",4.1 Laws of conformity and innovation,[0],[0]
"Interestingly, the same pattern of results emerged, again, in the shuffled condition (see Table 1).",4.1 Laws of conformity and innovation,[0],[0]
"Importantly, the difference in effect size between conditions, as evaluated by the explained variance of frequency and polysemy together, showed a modest effect of 8% over the shuffled condition, pointing to the conclusion that the putative effects may indeed be real, but to a far lesser extent than had been claimed.",4.1 Laws of conformity and innovation,[0],[0]
"We conclude that adding polysemy to the analysis contributed very little to the model’s predictive power.
",4.1 Laws of conformity and innovation,[0],[0]
"Since the PPMI representation (the explicit representation without dimensionality reduction with SVD) seems much less affected by spurious effects correlated with frequency (see Fig. 1b), we repeated the analysis of frequency described here and in Section 3.1 while using this representation.",4.1 Laws of conformity and innovation,[0],[0]
"The results are listed in Table 1, showing a similar pattern of rather small frequency effect.",4.1 Laws of conformity and innovation,[0],[0]
Prototypicality is the degree to which a word is representative of the category of which it is a member (a robin is a more prototypical bird than a parrot).,4.2 Prototypicality,[0],[0]
"According to the proposed Law of Prototypicality, words with more prototypical meanings will show less semantic change, and vice versa.",4.2 Prototypicality,[0],[0]
"Following (Dubossarsky et al., 2015), we computed words’ prototypicality scores for each decade as the cos-distance between a word’s vec-
3https://github.com/williamleif/histwords
tor and its k-means cluster’s centroid, and extended the analysis to encompass the entire 20th century.",4.2 Prototypicality,[0],[0]
"The previous regression model assumed independence between words, and therefore assigned words to a random effect variable.",4.2 Prototypicality,[0],[0]
"However, when modeling prototypicality, this assumption is invalid as relations between words are what inherently define prototypicality.",4.2 Prototypicality,[0],[0]
"We therefore designed a model in which decades, rather than words, are the random effect variable.
",4.2 Prototypicality,[0],[0]
With this analysis the prototypicality effect seems to be substantiated in two ways.,4.2 Prototypicality,[0],[0]
"First, the addition of prototypicality explains an additional 5% of the variance.",4.2 Prototypicality,[0],[0]
"Second, the effect of prototypicality meets the more stringent requirement of being diminished in the shuffle condition (see Table 1).",4.2 Prototypicality,[0],[0]
"Nevertheless, here too the effect originally reported was found to be drastically reduced after being compared with the proper control.",4.2 Prototypicality,[0],[0]
"We show in Section 5.1 that the average cosine distance between two vectors representing the same word is equivalent to the variance of the population of vectors representing the same word in independent samples, and is therefore always positive.",5 Theoretical analysis,[0],[0]
"This is true for any word vector representation.
",5 Theoretical analysis,[0],[0]
"In Sections 5.2-5.3 we prove that the average cosines distance between two count vectors representing the same word is negatively correlated with the frequency of the word, and positively correlated with the polysemy score of the word.",5 Theoretical analysis,[0],[0]
Lemma 1.,5.1 Sampling variability and the cos distance,[0],[0]
"Assume two random variables x, y of length ‖x‖2 = ‖y‖2 = 1, distributed iid with expected value µ and covariance matrix",5.1 Sampling variability and the cos distance,[0],[0]
Σ.,5.1 Sampling variability and the cos distance,[0],[0]
"The expected value of the cosine distance between them is equal to the sum of the diagonal elements of Σ.
Proof.
E(x− y)2 =E(x− µ)2 + E(y − µ)2+ 2E(x− µ)(y",5.1 Sampling variability and the cos distance,[0],[0]
"− µ)
=2 ∑ E(xi − µi)2 = 2 ∑ V ar(xi)
E(x− y)2 =E(x2) + E(y2)− 2E(x · y) =2− 2E (
x · y ‖x‖2‖y‖2 ) =2E(cosDist(x, y))
",5.1 Sampling variability and the cos distance,[0],[0]
"It follows that E(cosDist(x, y))",5.1 Sampling variability and the cos distance,[0],[0]
"= ∑ V ar(xi) (4)
Implication:",5.1 Sampling variability and the cos distance,[0],[0]
"The average cosine distance between two samples of the same random variables is directly related to the variance of the variable, or the sampling noise.",5.1 Sampling variability and the cos distance,[0],[0]
"This variance should be measured empirically whenever cosine distance is used, since only distances that are larger than the empirical variance can be relied upon to support significant observations.",5.1 Sampling variability and the cos distance,[0],[0]
"Next, we analyze the cosine distance between 2 iid samples from a normalized multinomial random variable.",5.2 Cos distance of count vectors: frequency,[0],[0]
This distribution models the distribution of the count vector representation.,5.2 Cos distance of count vectors: frequency,[0],[0]
"Let ki, 1 ≤",5.2 Cos distance of count vectors: frequency,[0],[0]
"i ≤ m denote the number of times word i appeared in the context of word w, and let m denote the size of the dictionary not including w. Let n = ∑",5.2 Cos distance of count vectors: frequency,[0],[0]
ki denote the number of words in the count vector of w; n determines the word’s frequency score.,5.2 Cos distance of count vectors: frequency,[0],[0]
"Assume that the counts are sampled from the distribution Multinomial(n, ~p), namely
Prob(k1, · · · , km) = (
n
k1 · · · , km
) pk11 · · · pkmm
Lemma 2.",5.2 Cos distance of count vectors: frequency,[0],[0]
"The expected value of the cosine distance between two count vectors x, y sampled iid from this distribution is monotonically decreasing with n.
Proof.",5.2 Cos distance of count vectors: frequency,[0],[0]
"By definition, 1−E[cosDist(x, y)] equals
E
[ x · y
‖x‖2‖y‖2 ]",5.2 Cos distance of count vectors: frequency,[0],[0]
= ∑ i,5.2 Cos distance of count vectors: frequency,[0],[0]
"[ E xi ‖x‖2 ]2 = ∑ i E2i (5)
",5.2 Cos distance of count vectors: frequency,[0],[0]
"We compute the expected value of Ei directly:
Ei = ∑
(k1,··· ,km) ki√∑ j k 2 j
( n
k1 · · · , km
) pk11 · · · pkmm
Using Taylor expansion:
ki√∑ j k 2 j = ki n√",5.2 Cos distance of count vectors: frequency,[0],[0]
"( ∑ j kj n ) 2 −∑l 6=j kjkln2 = ki n
1√ 1−∑l 6=j kjkln2
= ki n
( 1 + ε
2 +O(ε2)
) (6)
where ε = ∑
l 6=j kjkl n2
.",5.2 Cos distance of count vectors: frequency,[0],[0]
"The expected value of the 0-order term with respect to ε in (6) equals pi, which is independent of n.",5.2 Cos distance of count vectors: frequency,[0],[0]
"We conclude the proof by focusing on the first order term with respect to ε in (6), to be denoted f1, showing that its expected value is monotonically decreasing with n. Specifically:
f1 = ∑ ~k ∑ l 6=j ki",5.2 Cos distance of count vectors: frequency,[0],[0]
n,5.2 Cos distance of count vectors: frequency,[0],[0]
kj n kl n,5.2 Cos distance of count vectors: frequency,[0],[0]
"( n k1 · · · , km ) pk11 · · · pkmm
We switch the summation order and compute each expression in the external sum, considering two cases separately:",5.2 Cos distance of count vectors: frequency,[0],[0]
"when l 6= j 6= i∑
(k1,··· ,km)
",5.2 Cos distance of count vectors: frequency,[0],[0]
ki n,5.2 Cos distance of count vectors: frequency,[0],[0]
"kj n kl n
( n
k1 · · · , km
) pk11 · · · pkmm
= n(n− 1)(n− 2)
",5.2 Cos distance of count vectors: frequency,[0],[0]
"n3 pipjpl
When l 6= j =",5.2 Cos distance of count vectors: frequency,[0],[0]
"i w.l.g, we rewrite kikj = ki(ki",5.2 Cos distance of count vectors: frequency,[0],[0]
"− 1) + ki, and the sum above becomes n(n−1)(n−2)
n3 p2i pl + n(n−1) n2 pipl.",5.2 Cos distance of count vectors: frequency,[0],[0]
"Thus
f1 = n− 1 n pi n− 2 n",5.2 Cos distance of count vectors: frequency,[0],[0]
∑,5.2 Cos distance of count vectors: frequency,[0],[0]
"l,j:l 6=j pjpl",5.2 Cos distance of count vectors: frequency,[0],[0]
+,5.2 Cos distance of count vectors: frequency,[0],[0]
"(1− pi) 
",5.2 Cos distance of count vectors: frequency,[0],[0]
"and it readily follows that f1 is monotonically increasing with n.
Since n measures the frequency score of word w, it follows from (5) that the expected value of the cosine distance between two iid samples from the distribution of the count vector of w is monotonically decreasing with the word’s frequency.",5.2 Cos distance of count vectors: frequency,[0],[0]
We start our investigation of polysemy by modeling the distribution of the parameters of the multinomial distribution from which count vectors are sampled.,5.3 Cos distance of count vectors: polysemy,[0],[0]
"A common prior distribution on the vector ~pw in m-simplex, which defines the multinomial distribution generating the context of word w, is the Dirichlet distribution f(~pw; ~αw) =",5.3 Cos distance of count vectors: polysemy,[0],[0]
"f(p1, · · · , pm;α1, · · · , αm).",5.3 Cos distance of count vectors: polysemy,[0],[0]
"~αw is a sparse vector of prior counts on all the words in the dictionary, by which the cooccurrence context of word w is modeled.",5.3 Cos distance of count vectors: polysemy,[0],[0]
"We divide the set of none-zero indices of ~αw into two subsets: i1, · · · , im0 correspond to the words which always appear in the context of w, while j1, · · · , im1 correspond to the words which appear in the context of w in one given meaning.",5.3 Cos distance of count vectors: polysemy,[0],[0]
"If w is
polysemous and has two meanings, then there is a third set of indices k1, · · · , km2 which correspond to the words appearing in the context of w in its second meaning.",5.3 Cos distance of count vectors: polysemy,[0],[0]
"If w has more then two meanings, they can be modeled with additional sets of disjoint indices.
",5.3 Cos distance of count vectors: polysemy,[0],[0]
Lemma 3.,5.3 Cos distance of count vectors: polysemy,[0],[0]
"Under certain conditions specified in the proof, given two count vectors x, y sampled iid from the above distribution of w, the expected value of the cosine distance between them increases with the number of sets of disjoint indices which represent different meanings of w.
Proof.",5.3 Cos distance of count vectors: polysemy,[0],[0]
"We will prove that when w has two meanings, the expected value of the cosine distance is larger than in the case of a single meaning.",5.3 Cos distance of count vectors: polysemy,[0],[0]
"The proof for the general case immediately follows.
",5.3 Cos distance of count vectors: polysemy,[0],[0]
"Starting from (6) while keeping only the 0-order term in ε, it follows from the derivations in the proof of Lemma 2 that the expected cosine distance between two count vector samples of w, to be denotedM , is 1−∑ p2i .",5.3 Cos distance of count vectors: polysemy,[0],[0]
"In our current model ~p is a random variable, and we shall compute the expected value of this random variable under the two conditions, when w has either one or two meanings.
",5.3 Cos distance of count vectors: polysemy,[0],[0]
"We start by observing that, given the definition of the Dirichlet distribution, it follows that
E(p2i ) =V ar(pi) + E(pi) 2 = αi(1 + αi) α0(1 + α0)
αo = ∑ αi
=⇒M = ∑ E(p2i ) =",5.3 Cos distance of count vectors: polysemy,[0],[0]
"α0 + ∑ α2i
α0(1 + α0) (7)
Considering the different sets of indices in isolation, let ϕo = ∑im0",5.3 Cos distance of count vectors: polysemy,[0],[0]
"i=i1 αi, ϕ1 = ∑jm1",5.3 Cos distance of count vectors: polysemy,[0],[0]
"i=j1 αi, and
ϕ2 = ∑km2
i=k1 αi.",5.3 Cos distance of count vectors: polysemy,[0],[0]
Let ψo,5.3 Cos distance of count vectors: polysemy,[0],[0]
"= ∑im0 i=i1
α2i , ψ1 =∑jm1 i=j1 α2i , and ψ2 = ∑km2",5.3 Cos distance of count vectors: polysemy,[0],[0]
"i=k1 α2i .
",5.3 Cos distance of count vectors: polysemy,[0],[0]
"We rewrite (7) for the two conditions:
1.",5.3 Cos distance of count vectors: polysemy,[0],[0]
"w has one meaning:
M (1) = ϕ0 + ϕ1 + ψ0 + ψ1
(ϕ0 + ϕ1)(1 + ϕ0 + ϕ1)
2.",5.3 Cos distance of count vectors: polysemy,[0],[0]
"w has two meanings:
M (2) = ϕ0 + ϕ1 + ϕ2 + ψ0 + ψ1 + ψ2
(ϕ0 + ϕ1 + ϕ2)(1 + ϕ0 + ϕ1 + ϕ2)
",5.3 Cos distance of count vectors: polysemy,[0],[0]
"With some algebraic manipulations, it can be shown that M (1) > M (2) if the following holds:
(ϕ0 + ϕ1)2ϕ2",5.3 Cos distance of count vectors: polysemy,[0],[0]
+ (ψ0 + ψ1)ϕ22,5.3 Cos distance of count vectors: polysemy,[0],[0]
(8) +2(ψ0 + ψ1)(ϕ0 + ϕ1)ϕ2,5.3 Cos distance of count vectors: polysemy,[0],[0]
+ (ψ0 + ψ1)ϕ2 +(ϕ0 + ϕ1)(ϕ22,5.3 Cos distance of count vectors: polysemy,[0],[0]
"− ψ2) > ψ2(ϕ0 + ϕ1)2
Thus when (8) holds, the average cosine distance between two samples of a certain word w gets larger as w acquires more meanings.
(8) readily holds under reasonable conditions, e.g., when the prior counts for each meaning are similar (as a set) and much bigger than the prior counts of the joint context words (i.e., ϕ0 = ψ0 = ε, ϕ1 = ϕ2, ψ1 = ψ2).",5.3 Cos distance of count vectors: polysemy,[0],[0]
In this article we have shown that some reported laws of semantic change are largely spurious results of the word representation models on which they are based.,6 Conclusions and discussion,[0],[0]
"While identifying such laws is probably within the reach of NLP analyses of massive digital corpora, we argued that a more stringent standard of proof is necessary in order to put them on a firm footing.",6 Conclusions and discussion,[0],[0]
"Specifically, it is necessary to demonstrate that any proposed law of change has to be observable in the genuine condition, but to be diminished or absent in a control condition.",6 Conclusions and discussion,[0],[0]
"We replicated previous studies claiming to establish such laws, which propose that semantic change is negatively correlated with frequency and prototypicality, and positively correlated with polysemy.",6 Conclusions and discussion,[0],[0]
"None of these laws - at least in their strong versions - survived the more stringent standard of proof, since the observed correlations were found in the control conditions.
",6 Conclusions and discussion,[0],[0]
"In our analysis, the Law of Conformity, which claims a negative correlation between word frequency and meaning change, was shown to have a much smaller effect size than previously claimed.",6 Conclusions and discussion,[0],[0]
This indicates that word frequency probably does play a role - but a small one - in semantic change.,6 Conclusions and discussion,[0],[0]
"According to the Law of Innovation, polysemy was claimed to correlate positively with meaning change.",6 Conclusions and discussion,[0],[0]
"However, our analysis showed that polysemy is highly collinear with frequency, and as such, did not demonstrate independent contribution to semantic change.",6 Conclusions and discussion,[0],[0]
"For similar reasons, the alleged role of prototypicality was diminished.
",6 Conclusions and discussion,[0],[0]
"These results may be more consonant than previous ones with the findings of historical linguis-
tics, as it is commonly assumed that the factors leading to semantic change are more diverse than purely distributional factors.",6 Conclusions and discussion,[0],[0]
"For example, sociocultural, political, and technological changes are known to impact semantic change (Bochkarev et al., 2014; Newman, 2015).",6 Conclusions and discussion,[0],[0]
"Furthermore, some regularities of semantic change have been imputed to ‘channel bias‘, inherent biases of utterance production and interpretation on the part of speakers and listeners, e.g., (Moreton, 2008).",6 Conclusions and discussion,[0],[0]
"As such, it would be surprising if word frequency, polysemy, and prototypicality were to capture too high a degree of variance.",6 Conclusions and discussion,[0],[0]
"In other words, since semantic change may result from the interaction of many factors, small effects may be a priori more credible than large ones.
",6 Conclusions and discussion,[0],[0]
The results of our empirical analysis showed that the spurious effects of frequency were much weaker for the explicit PPMI representation unaugmented by SVD dimensionality reduction.,6 Conclusions and discussion,[0.9550750697897866],['Note that the above theorem also implies that the wellknown COLORING problem is FPT parameterized by the treewidth of the complement of the input graph.']
We therefore conclude that the artefactual frequency effects reported are inherent to the type of word representations upon which these analyses are based.,6 Conclusions and discussion,[0],[0]
"As the analytical proof in Section 5 demonstrates, it is count vectors that introduce an artefactual dependence on word frequency.
",6 Conclusions and discussion,[0],[0]
"Intuitively, one might expect that the average value for the cosine distance between a given word’s vector in any two samples would be 0.",6 Conclusions and discussion,[0],[0]
"However, Lemma 1 above shows that this is not the case, and the average distance is the variance of the population of vectors representing the same word.",6 Conclusions and discussion,[0],[0]
This result is independent of the specific method used to represent words as vectors.,6 Conclusions and discussion,[0],[0]
"Lemma 2 proves that the average cosine distance between two samples of the same word, when using count vector representations, is negatively correlated with the word’s frequency.",6 Conclusions and discussion,[0],[0]
"Thus, the role of frequency cannot be evaluated as an independent predictor in any model based on count vector representations.",6 Conclusions and discussion,[0],[0]
"It remains for future research to establish whether other approaches to word representation, e.g. (Blei et al., 2003; Mikolov et al., 2013), have inherent biases.
",6 Conclusions and discussion,[0],[0]
"While our findings may seem to be mainly negative, since they invalidate proposed laws of semantic change, we would like to point to the positive contribution made by articulating more stringent standards of proof and devising replicable control conditions for future research on language change based on distributional semantics representations.",6 Conclusions and discussion,[0],[0]
This article evaluates three proposed laws of semantic change.,abstractText,[0],[0]
"Our claim is that in order to validate a putative law of semantic change, the effect should be observed in the genuine condition but absent or reduced in a suitably matched control condition, in which no change can possibly have taken place.",abstractText,[0],[0]
Our analysis shows that the effects reported in recent literature must be substantially revised: (i) the proposed negative correlation between meaning change and word frequency is shown to be largely an artefact of the models of word representation used; (ii) the proposed negative correlation between meaning change and prototypicality is shown to be much weaker than what has been claimed in prior art; and (iii) the proposed positive correlation between meaning change and polysemy is largely an artefact of word frequency.,abstractText,[0],[0]
"These empirical observations are corroborated by analytical proofs that show that count representations introduce an inherent dependence on word frequency, and thus word frequency cannot be evaluated as an independent factor with these representations.",abstractText,[0],[0]
Outta Control: Laws of Semantic Change and Inherent Biases in Word Representation Models,title,[0],[0]
Words can mean different things to different people.,1 Introduction,[0],[0]
"Fortunately, these differences are rarely idiosyncratic, but are often linked to social factors, such as age (Rosenthal and McKeown, 2011), gender (Eckert and McConnell-Ginet, 2003), race (Green, 2002), geography (Trudgill, 1974), and more ineffable characteristics such as political and cultural attitudes (Fischer, 1958; Labov, 1963).",1 Introduction,[0],[0]
"In natural language processing (NLP), social media data has brought variation to the fore, spurring the development of new computational techniques for charac-
terizing variation in the lexicon (Eisenstein et al., 2010), orthography (Eisenstein, 2015), and syntax (Blodgett et al., 2016).",1 Introduction,[0],[0]
"However, aside from the focused task of spelling normalization (Sproat et al., 2001; Aw et al., 2006), there have been few attempts to make NLP systems more robust to language variation across speakers or writers.
",1 Introduction,[0],[0]
"One exception is the work of Hovy (2015), who shows that the accuracies of sentiment analysis and topic classification can be improved by the inclusion of coarse-grained author demographics such as age and gender.",1 Introduction,[0],[0]
"However, such demographic information is not directly available in most datasets, and it is not yet clear whether predicted age and gender offer any improvements.",1 Introduction,[0],[0]
"On the other end of the spectrum are attempts to create personalized language technologies, as are often employed in information retrieval (Shen et al., 2005), recommender systems (Basilico and Hofmann, 2004), and language modeling (Federico, 1996).",1 Introduction,[0],[0]
"But personalization requires annotated data for each individual user—something that may be possible in interactive settings such as information retrieval, but is not typically feasible in natural language processing.
",1 Introduction,[0],[0]
"We propose a middle ground between group-level demographic characteristics and personalization, by exploiting social network structure.",1 Introduction,[0],[0]
"The sociological theory of homophily asserts that individuals are usually similar to their friends (McPherson et al., 2001).",1 Introduction,[0],[0]
"This property has been demonstrated for language (Bryden et al., 2013) as well as for the demographic properties targeted by Hovy (2015), which are more likely to be shared by friends than by random pairs of individuals (Thelwall, 2009).",1 Introduction,[0],[0]
"Social
295
Transactions of the Association for Computational Linguistics, vol. 5, pp.",1 Introduction,[0],[0]
"295–307, 2017.",1 Introduction,[0],[0]
Action Editor: Christopher Potts.,1 Introduction,[0],[0]
"Submission batch: 10/2016; Revision batch: 12/2016; Published 8/2017.
",1 Introduction,[0],[0]
c©2017 Association for Computational Linguistics.,1 Introduction,[0],[0]
"Distributed under a CC-BY 4.0 license.
network information is available in a wide range of contexts, from social media (Huberman et al., 2008) to political speech (Thomas et al., 2006) to historical texts (Winterer, 2012).",1 Introduction,[0],[0]
"Thus, social network homophily has the potential to provide a more general way to account for linguistic variation in NLP.
",1 Introduction,[0],[0]
Figure 1 gives a schematic of the motivation for our approach.,1 Introduction,[0],[0]
"The word ‘sick’ typically has a negative sentiment, e.g., ‘I would like to believe he’s sick rather than just mean and evil.’1",1 Introduction,[0],[0]
"However, in some communities the word can have a positive sentiment, e.g., the lyric ‘this sick beat’, recently trademarked by the musician Taylor Swift.2",1 Introduction,[0],[0]
"Given labeled examples of ‘sick’ in use by individuals in a social network, we assume that the word will have a similar sentiment meaning for their near neighbors—an assumption of linguistic homophily that is the basis for this research.",1 Introduction,[0],[0]
"Note that this differs from the assumption of label homophily, which entails that neighbors in the network will hold similar opinions, and will therefore produce similar document-level labels (Tan et al., 2011; Hu et al., 2013).",1 Introduction,[0],[0]
"Linguistic homophily is a more generalizable claim, which could in principle be applied to any language processing task where author network information is available.
",1 Introduction,[0],[0]
"To scale this basic intuition to datasets with tens of thousands of unique authors, we compress the social network into vector representations of each author node, using an embedding method for large
1Charles Rangel, describing Dick Cheney 2In the case of ‘sick’, speakers like Taylor Swift may employ either the positive and negative meanings, while speakers like Charles Rangel employ only the negative meaning.",1 Introduction,[0],[0]
"In other cases, communities may maintain completely distinct semantics for a word, such as the term ‘pants’ in American and British English.",1 Introduction,[0],[0]
"Thanks to Christopher Potts for suggesting this distinction and this example.
",1 Introduction,[0],[0]
"scale networks (Tang et al., 2015b).",1 Introduction,[0],[0]
"Applying the algorithm to Figure 1, the authors within each triad would likely be closer to each other than to authors in the opposite triad.",1 Introduction,[0],[0]
"We then incorporate these embeddings into an attention-based neural network model, called SOCIAL ATTENTION, which employs multiple basis models to focus on different regions of the social network.
",1 Introduction,[0],[0]
"We apply SOCIAL ATTENTION to Twitter sentiment classification, gathering social network metadata for Twitter users in the SemEval Twitter sentiment analysis tasks (Nakov et al., 2013).",1 Introduction,[0],[0]
"We further adopt the system to Ciao product reviews (Tang et al., 2012), training author embeddings using trust relationships between reviewers.",1 Introduction,[0],[0]
SOCIAL ATTENTION offers a 2-3% improvement over related neural and ensemble architectures in which the social information is ablated.,1 Introduction,[0],[0]
It also outperforms all prior published results on the SemEval Twitter test sets.,1 Introduction,[0],[0]
"In the SemEval Twitter sentiment analysis tasks, the goal is to classify the sentiment of each message as positive, negative, or neutral.",2 Data,[0],[0]
"Following Rosenthal et al. (2015), we train and tune our systems on the SemEval Twitter 2013 training and development datasets respectively, and evaluate on the 2013–2015 SemEval Twitter test sets.",2 Data,[0],[0]
Statistics of these datasets are presented in Table 1.,2 Data,[0],[0]
"Our training and development datasets lack some of the original Twitter messages, which may have been deleted since the datasets were constructed.",2 Data,[0],[0]
"However, our test datasets contain all the tweets used in the SemEval evaluations, making our results comparable with prior work.
",2 Data,[0],[0]
"We construct three author social networks based on the follow, mention, and retweet relations between the 7,438 authors in the training dataset,
which we refer as FOLLOWER, MENTION and RETWEET.3 Specifically, we use the Twitter API to crawl the friends of the SemEval users (individuals that they follow) and the most recent 3,200 tweets in their timelines.4 The mention and retweet links are then extracted from the tweet text and metadata.",2 Data,[0],[0]
"We treat all social networks as undirected graphs, where two users are socially connected if there exists at least one social relation between them.",2 Data,[0],[0]
"The hypothesis of linguistic homophily is that socially connected individuals tend to use language similarly, as compared to a randomly selected pair of individuals who are not socially connected.",3 Linguistic Homophily,[0],[0]
"We now describe a pilot study that provides support for this hypothesis, focusing on the domain of sentiment analysis.",3 Linguistic Homophily,[0],[0]
"The purpose of this study is to test whether errors in sentiment analysis are assortative on the social networks defined in the previous section: that is, if two individuals (i, j) are connected in the network, then a classifier error on i suggests that errors on j are more likely.
",3 Linguistic Homophily,[0],[0]
"We test this idea using a simple lexicon-based classification approach, which we apply to the SemEval training data, focusing only on messages that are labeled as positive or negative (ignoring the neutral class), and excluding authors who contributed more than one message (a tiny minority).",3 Linguistic Homophily,[0],[0]
"Using the social media sentiment lexicons defined by Tang et al. (2014),5 we label a message as positive if it has at least as many positive words as negative words, and as negative otherwise.6",3 Linguistic Homophily,[0],[0]
"The assortativity is the fraction of dyads for which the classifier makes two correct predictions or two incorrect predictions (Newman, 2003).",3 Linguistic Homophily,[0],[0]
"This measures whether classification errors are clustered on the network.
",3 Linguistic Homophily,[0],[0]
"We compare the observed assortativity against the assortativity in a network that has been randomly
3We could not gather the authorship information of 10% of the tweets in the training data, because the tweets or user accounts had been deleted by the time we crawled the social information.
",3 Linguistic Homophily,[0],[0]
"4The Twitter API returns a maximum of 3,200 tweets.",3 Linguistic Homophily,[0],[0]
"5The lexicons include words that are assigned at least 0.99 confidence by the method of Tang et al. (2014): 1,474 positive and 1,956 negative words in total.
6Ties go to the positive class because it is more common.
",3 Linguistic Homophily,[0],[0]
rewired.7 Each rewiring epoch involves a number of random rewiring operations equal to the total number of edges in the network.,3 Linguistic Homophily,[0],[0]
"(The edges are randomly selected, so a given edge may not be rewired in each epoch.)",3 Linguistic Homophily,[0],[0]
"By counting the number of edges that occur in both the original and rewired networks, we observe that this process converges to a steady state after three or four epochs.",3 Linguistic Homophily,[0],[0]
"As shown in Figure 2, the original observed network displays more assortativity than the randomly rewired networks in nearly every case.",3 Linguistic Homophily,[0],[0]
"Thus, the Twitter social networks display more linguistic homophily than we would expect due to chance alone.
",3 Linguistic Homophily,[0],[0]
"The differences in assortativity across network types are small, indicating that none of the networks are clearly best.",3 Linguistic Homophily,[0],[0]
"The retweet network was the most difficult to rewire, with the greatest proportion of shared edges between the original and rewired networks.",3 Linguistic Homophily,[0],[0]
This may explain why the assortativities of the randomly rewired networks were closest to the observed network in this case.,3 Linguistic Homophily,[0],[0]
"In this section, we describe a neural network method that leverages social network information to improve text classification.",4 Model,[0],[0]
"Our approach is inspired by ensemble learning, where the system prediction is the weighted combination of the outputs of several basis models.",4 Model,[0],[0]
"We encourage each basis model to focus on a local region of the social network, so that classification on socially connected individuals employs similar model combinations.
",4 Model,[0],[0]
"Given a set of instances {xi} and authors {ai}, the goal of personalized probabilistic classification is to estimate a conditional label distribution p(y | x, a).",4 Model,[0],[0]
"For most authors, no labeled data is available, so it is impossible to estimate this distribution directly.",4 Model,[0],[0]
We therefore make a smoothness assumption over a social network G: individuals who are socially proximate in G should have similar classifiers.,4 Model,[0],[0]
"This idea is put into practice by modeling the conditional label distribution as a mixture over the
7Specifically, we use the double edge swap operation of the networkx package (Hagberg et al., 2008).",4 Model,[0],[0]
"This operation preserves the degree of each node in the network.
predictions of K basis classifiers,
p(y | x, a) = K∑
k=1
Pr(Za = k",4 Model,[0],[0]
"| a,G)× p(y | x, Za = k).
",4 Model,[0],[0]
"(1)
The basis classifiers p(y | x, Za = k) can be arbitrary conditional distributions; we use convolutional neural networks, as described in § 4.2.",4 Model,[0],[0]
The component weighting distribution Pr(Za = k,4 Model,[0],[0]
"| a,G) is conditioned on the social network G, and functions as an attentional mechanism, described in § 4.1.",4 Model,[0],[0]
"The basic intuition is that for a pair of authors ai and aj who are nearby in the social network G, the prediction rules should behave similarly if the attentional distributions are similar, p(z | ai, G)",4 Model,[0],[0]
"≈ p(z | aj , G).",4 Model,[0],[0]
"If we have labeled data only for ai, some of the personalization from that data will be shared by aj .",4 Model,[0],[0]
"The overall classification approach can be viewed as a mixture of experts (Jacobs et al., 1991), leveraging the social network as side information to choose the distribution over experts for each author.",4 Model,[0],[0]
The goal of the social attention model is to assign similar basis weights to authors who are nearby in the social networkG. We operationalize social proximity by embedding each node’s social network position into a vector representation.,4.1 Social Attention Model,[0],[0]
"Specifically, we employ the LINE method (Tang et al., 2015b), which estimates D(v) dimensional node embeddings va as parameters in a probabilistic model over edges in the social network.",4.1 Social Attention Model,[0],[0]
"These embeddings are learned solely from the social networkG, without leveraging
any textual information.",4.1 Social Attention Model,[0],[0]
"The attentional weights are then computed from the embeddings using a softmax layer,
Pr(Za = k",4.1 Social Attention Model,[0],[0]
"| a,G) = exp
( φ>k va + bk )",4.1 Social Attention Model,[0],[0]
"∑K
k′ exp ( φ>k′va + bk′ ) .
",4.1 Social Attention Model,[0],[0]
"(2) This embedding method uses only singlerelational networks; in the evaluation, we will show results for Twitter networks built from networks of follow, mention, and retweet relations.",4.1 Social Attention Model,[0],[0]
"In future work, we may consider combining all of these relation types into a unified multi-relational network.",4.1 Social Attention Model,[0],[0]
"It is possible that embeddings in such a network could be estimated using techniques borrowed from multirelational knowledge networks (Bordes et al., 2014; Wang et al., 2014).",4.1 Social Attention Model,[0],[0]
"We next describe the basis models, p(y | x, Z = k).",4.2 Sentiment Classification with Convolutional Neural Networks,[0],[0]
"Because our target task is classification on microtext documents, we model this distribution using convolutional neural networks (CNNs; Lecun et al., 1989), which have been proven to perform well on sentence classification tasks (Kalchbrenner et al., 2014; Kim, 2014).",4.2 Sentiment Classification with Convolutional Neural Networks,[0],[0]
"CNNs apply layers of convolving filters to n-grams, thereby generating a vector of dense local features.",4.2 Sentiment Classification with Convolutional Neural Networks,[0],[0]
"CNNs improve upon traditional bagof-words models because of their ability to capture word ordering information.
",4.2 Sentiment Classification with Convolutional Neural Networks,[0],[0]
Let x =,4.2 Sentiment Classification with Convolutional Neural Networks,[0],[0]
"[h1,h2, · · · ,hn] be the input sentence, where hi is the D(w) dimensional word vector corresponding to the i-th word in the sentence.",4.2 Sentiment Classification with Convolutional Neural Networks,[0],[0]
"We use
one convolutional layer and one max pooling layer to generate the sentence representation of x.",4.2 Sentiment Classification with Convolutional Neural Networks,[0],[0]
The convolutional layer involves filters that are applied to bigrams to produce feature maps.,4.2 Sentiment Classification with Convolutional Neural Networks,[0],[0]
"Formally, given the bigram word vectors hi,hi+1, the features generated by m filters can be computed by
ci = tanh(WLhi + WRhi+1 + b), (3)
where ci is an m dimensional vector, WL and WR are m×D(w) projection matrices, and b is the bias vector.",4.2 Sentiment Classification with Convolutional Neural Networks,[0],[0]
"The m dimensional vector representation of the sentence is given by the pooling operation
s = max i∈1,··· ,n−1 ci. (4)
To obtain the conditional label probability, we utilize a multiclass logistic regression model,
",4.2 Sentiment Classification with Convolutional Neural Networks,[0],[0]
"Pr(Y = t | x, Z = k) =",4.2 Sentiment Classification with Convolutional Neural Networks,[0],[0]
"exp(β > t sk + βt)∑T
t′=1 exp(β > t′ sk + βt′)
,
(5) where βt is an m dimensional weight vector, βt is the corresponding bias term, and sk is the m dimensional sentence representation produced by the k-th basis model.",4.2 Sentiment Classification with Convolutional Neural Networks,[0],[0]
We fix the pretrained author and word embeddings during training our social attention model.,4.3 Training,[0],[0]
"Let Θ denote the parameters that need to be learned, which include {WL,WR,b, {βt, βt}Tt=1} for every basis CNN model, and the attentional weights {φk, bk}Kk=1.",4.3 Training,[0],[0]
"We minimize the following logistic loss objective for each training instance:
`(Θ) =",4.3 Training,[0],[0]
"− T∑
t=1
1[Y ∗ = t] log Pr(Y = t | x, a), (6)
where Y ∗ is the ground truth class for x, and 1[·] represents an indicator function.",4.3 Training,[0],[0]
"We train the models for between 10 and 15 epochs using the Adam optimizer (Kingma and Ba, 2014), with early stopping on the development set.",4.3 Training,[0],[0]
"One potential problem is that after initialization, a small number of basis models may claim most of the mixture weights for all the users, while other basis
models are inactive.",4.4 Initialization,[0],[0]
This can occur because some basis models may be initialized with parameters that are globally superior.,4.4 Initialization,[0],[0]
"As a result, the “dead” basis models will receive near-zero gradient updates, and therefore can never improve.",4.4 Initialization,[0],[0]
"The true model capacity can thereby be substantially lower than the K assigned experts.
",4.4 Initialization,[0],[0]
"Ideally, dead basis models will be avoided because each basis model should focus on a unique region of the social network.",4.4 Initialization,[0],[0]
"To ensure that this happens, we pretrain the basis models using an instance weighting approach from the domain adaptation literature (Jiang and Zhai, 2007).",4.4 Initialization,[0],[0]
"For each basis model k, each author a has an instance weight αa,k.",4.4 Initialization,[0],[0]
"These instance weights are based on the author’s social network node embedding, so that socially proximate authors will have high weights for the same basis models.",4.4 Initialization,[0],[0]
"This is ensured by endowing each basis model with a random vector γk ∼ N(0, σ2I), and setting the instance weights as,
αa,k = sigmoid(γ>k va).",4.4 Initialization,[0],[0]
"(7)
The simple design results in similar instance weights for socially proximate authors.",4.4 Initialization,[0],[0]
"During pretraining, we train the k-th basis model by optimizing the following loss function for every instance:
`k = −αa,k T∑
t=1
1[Y ∗ = t] log Pr(Y = t | x, Za = k).
(8) The pretrained basis models are then assembled to-
gether and jointly trained using Equation 6.",4.4 Initialization,[0],[0]
Our main evaluation focuses on the 2013–2015 SemEval Twitter sentiment analysis tasks.,5 Experiments,[0],[0]
The datasets have been described in § 2.,5 Experiments,[0],[0]
"We train and tune our systems on the Train 2013 and Dev 2013 datasets respectively, and evaluate on the Test 2013– 2015 sets.",5 Experiments,[0],[0]
"In addition, we evaluate on another dataset based on Ciao product reviews (Tang et al., 2012).",5 Experiments,[0],[0]
"We utilize Twitter’s follower, mention, and retweet social networks to train user embeddings.",5.1 Social Network Expansion,[0],[0]
"By querying the Twitter API in April 2015, we were able
to identify 15,221 authors for the tweets in the SemEval datasets described above.",5.1 Social Network Expansion,[0],[0]
"We induce social networks for these individuals by crawling their friend links and timelines, as described in § 2.",5.1 Social Network Expansion,[0],[0]
"Unfortunately, these networks are relatively sparse, with a large amount of isolated author nodes.",5.1 Social Network Expansion,[0],[0]
"To improve the quality of the author embeddings, we expand the set of author nodes by adding nodes that do the most to densify the author networks: for the follower network, we add additional individuals that are followed by at least a hundred authors in the original set; for the mention and retweet networks, we add all users that have been mentioned or retweeted by at least twenty authors in the original set.",5.1 Social Network Expansion,[0],[0]
The statistics of the resulting networks are presented in Table 2.,5.1 Social Network Expansion,[0],[0]
"We employ the pretrained word embeddings used by Astudillo et al. (2015), which are trained with a corpus of 52 million tweets, and have been shown to perform very well on this task.",5.2 Experimental Settings,[0],[0]
"The embeddings are learned using the structured skip-gram model (Ling et al., 2015), and the embedding dimension is set at 600, following Astudillo et al. (2015).",5.2 Experimental Settings,[0],[0]
"We report the same evaluation metric as the SemEval challenge: the average F1 score of positive and negative classes.8
Competitive systems We consider five competitive Twitter sentiment classification methods.",5.2 Experimental Settings,[0],[0]
"Convolutional neural network (CNN) has been described in § 4.2, and is the basis model of SOCIAL ATTENTION.",5.2 Experimental Settings,[0],[0]
"Mixture of experts employs the same CNN model as an expert, but the mixture densi-
8Regarding the neutral class: systems are penalized with false positives when neutral tweets are incorrectly classified as positive or negative, and with false negatives when positive or negative tweets are incorrectly classified as neutral.",5.2 Experimental Settings,[0],[0]
"This follows the evaluation procedure of the SemEval challenge.
ties solely depend on the input values.",5.2 Experimental Settings,[0],[0]
We adopt the summation of the pretrained word embeddings as the sentence-level input to learn the gating function.9,5.2 Experimental Settings,[0],[0]
The model architecture of random attention is nearly identical to SOCIAL ATTENTION:,5.2 Experimental Settings,[0],[0]
"the only distinction is that we replace the pretrained author embeddings with random embedding vectors, drawing uniformly from the interval (−0.25, 0.25).",5.2 Experimental Settings,[0],[0]
"Concatenation concatenates the author embedding with the sentence representation obtained from CNN, and then feeds the new representation to a softmax classifier.",5.2 Experimental Settings,[0],[0]
"Finally, we include SOCIAL ATTENTION, the attention-based neural network method described in § 4.
",5.2 Experimental Settings,[0],[0]
"We also compare against the three top-performing systems in the SemEval 2015 Twitter sentiment analysis challenge (Rosenthal et al., 2015): WEBIS (Hagen et al., 2015), UNITN (Severyn and Moschitti, 2015), and LSISLIF (Hamdan et al., 2015).",5.2 Experimental Settings,[0],[0]
UNITN achieves the best average F1 score on Test 2013–2015 sets among all the submitted systems.,5.2 Experimental Settings,[0],[0]
"Finally, we republish results of NLSE (Astudillo et al., 2015), a non-linear subspace embedding model.
",5.2 Experimental Settings,[0],[0]
Parameter tuning We tune all the hyperparameters on the SemEval 2013 development set.,5.2 Experimental Settings,[0],[0]
"We choose the number of bigram filters for the CNN models from {50, 100, 150}.",5.2 Experimental Settings,[0],[0]
"The size of author embeddings is selected from {50, 100}.",5.2 Experimental Settings,[0],[0]
"For mixture of experts, random attention and SOCIAL ATTENTION, we compare a range of numbers of basis models, {3, 5, 10, 15}.",5.2 Experimental Settings,[0],[0]
We found that a relatively small number of basis models are usually sufficient to achieve good performance.,5.2 Experimental Settings,[0],[0]
"The number of pretraining epochs is selected from {1, 2, 3}.",5.2 Experimental Settings,[0],[0]
"During joint training, we check the performance on the development set after each epoch to perform early stopping.",5.2 Experimental Settings,[0],[0]
"Table 3 summarizes the main empirical findings, where we report results obtained from author embeddings trained on RETWEET+ network for SOCIAL ATTENTION.",5.3 Results,[0],[0]
The results of different social networks for SOCIAL ATTENTION are shown in Table 4.,5.3 Results,[0],[0]
"The best hyperparameters are: 100 bigram
9The summation of the pretrained word embeddings works better than the average of the word embeddings.
filters; 100-dimensional author embeddings; K = 5 basis models; 1 pre-training epoch.",5.3 Results,[0],[0]
"To establish the statistical significance of the results, we obtain 100 bootstrap samples for each test set, and compute the F1 score on each sample for each algorithm.",5.3 Results,[0],[0]
"A twotail paired t-test is then applied to determine if the F1 scores of two algorithms are significantly different, p < 0.05.
",5.3 Results,[0],[0]
"Mixture of experts, random attention, and CNN all achieve similar average F1 scores on the SemEval Twitter 2013–2015 test sets.",5.3 Results,[0],[0]
"Note that random attention can benefit from some of the personalized information encoded in the random author embeddings, as Twitter messages posted by the same author share the same attentional weights.",5.3 Results,[0],[0]
"However, it barely improves the results, because the majority of authors contribute a single message in the SemEval datasets.",5.3 Results,[0],[0]
"With the incorporation of author social network information, concatenation slightly improves the classification performance.",5.3 Results,[0],[0]
"Finally, SOCIAL ATTENTION gives much better results than concatena-
tion, as it is able to model the interactions between text representations and author representations.",5.3 Results,[0],[0]
"It significantly outperforms CNN on all the SemEval test sets, yielding 2.8% improvement on average F1 score.",5.3 Results,[0],[0]
"SOCIAL ATTENTION also performs substantially better than the top-performing SemEval systems and NLSE, especially on the 2014 and 2015 test sets.
",5.3 Results,[0],[0]
We now turn to a comparison of the social networks.,5.3 Results,[0],[0]
"As shown in Table 4, the RETWEET+ network is the most effective, although the differences are small: SOCIAL ATTENTION outperforms prior work regardless of which network is selected.",5.3 Results,[0],[0]
"Twitter’s “following” relation is a relatively low-cost form of social engagement, and it is less public than retweeting or mentioning another user.",5.3 Results,[0],[0]
Thus it is unsurprising that the follower network is least useful for socially-informed personalization.,5.3 Results,[0],[0]
"The RETWEET+ network has denser social connections than MENTION+, which could lead to better author embeddings.",5.3 Results,[0],[0]
We now investigate whether language variation in sentiment meaning has been captured by different basis models.,5.4 Analysis,[0],[0]
"We focus on the same sentiment words (Tang et al., 2014) that we used to test linguistic homophily in our analysis.",5.4 Analysis,[0],[0]
We are interested to discover sentiment words that are used with the opposite sentiment meanings by some authors.,5.4 Analysis,[0],[0]
"To measure the level of model-specificity for each
word w, we compute the difference between the model-specific probabilities p(y | X = w,Z = k) and the average probabilities of all basis models 1 K ∑K k=1 p(y | X = w,Z = k) for positive and negative classes.",5.4 Analysis,[0],[0]
"The five words in the negative and positive lexicons with the highest scores for each model are presented in Table 5.
",5.4 Analysis,[0],[0]
"As shown in Table 5, Twitter users corresponding to basis models 1 and 4 often use some words ironically in their tweets.",5.4 Analysis,[0],[0]
"Basis model 3 tends to assign positive sentiment polarity to swear words, and Twitter users related to basis model 5 seem to be less fond of fans of certain celebrities.",5.4 Analysis,[0],[0]
"Finally, basis model 2 identifies Twitter users that we have described in the introduction—they often adopt general negative words like ‘ill’, ‘sick’, and ‘suck’ positively.",5.4 Analysis,[0],[0]
Examples containing some of these words are shown in Table 6.,5.4 Analysis,[0],[0]
"The labeled datasets for Twitter sentiment analysis are relatively small; to evaluate our method on a larger dataset, we utilize a product review dataset by Tang et al. (2012).",5.5 Sentiment Analysis of Product Reviews,[0],[0]
"The dataset consists of 257,682 reviews written by 10,569 users crawled from a popular product review sites, Ciao.10 The rating information in discrete five-star range is available for the reviews, which is treated as the ground truth label information for the reviews.",5.5 Sentiment Analysis of Product Reviews,[0],[0]
"Moreover, the users of this site can mark explicit “trust” relationships with each other, creating a social network.
",5.5 Sentiment Analysis of Product Reviews,[0],[0]
"To select examples from this dataset, we first removed reviews that were marked by readers as “not useful.”",5.5 Sentiment Analysis of Product Reviews,[0],[0]
"We treated reviews with more than three stars as positive, and less than three stars as negative; reviews with exactly three stars were removed.
",5.5 Sentiment Analysis of Product Reviews,[0],[0]
"10http://www.ciao.co.uk
We then sampled 100,000 reviews from this set, and split them randomly into training (70%), development (10%) and test sets (20%).",5.5 Sentiment Analysis of Product Reviews,[0],[0]
The statistics of the resulting datasets are presented in Table 7.,5.5 Sentiment Analysis of Product Reviews,[0],[0]
"We utilize 145,828 trust relations between 18,999 Ciao users to train the author embeddings.",5.5 Sentiment Analysis of Product Reviews,[0],[0]
"We consider the 10,000 most frequent words in the datasets, and assign them pretrained word2vec embeddings.11",5.5 Sentiment Analysis of Product Reviews,[0],[0]
"As shown in Table 7, the datasets have highly skewed class distributions.",5.5 Sentiment Analysis of Product Reviews,[0],[0]
"Thus, we use the average F1 score of positive and negative classes as the evaluation metic.
",5.5 Sentiment Analysis of Product Reviews,[0],[0]
The evaluation results are presented in Table 8.,5.5 Sentiment Analysis of Product Reviews,[0],[0]
"The best hyperparameters are generally the same as those for Twitter sentiment analysis, except that the optimal number of basis models is 10, and the optimal number of pretraining epochs is 2.",5.5 Sentiment Analysis of Product Reviews,[0],[0]
"Mixture of experts and concatenation obtain slightly worse F1 scores than the baseline CNN system, but random attention performs significantly better.",5.5 Sentiment Analysis of Product Reviews,[0],[0]
"In contrast to the SemEval datasets, individual users often contribute multiple reviews in the Ciao datasets (the average number of reviews from an author is 10.8; Table 7).",5.5 Sentiment Analysis of Product Reviews,[0],[0]
"As an author tends to express similar opinions toward related products, random attention
11https://code.google.com/archive/p/ word2vec
is able to leverage the personalized information to improve sentiment analysis.",5.5 Sentiment Analysis of Product Reviews,[0],[0]
"Prior work has investigated the direction, obtaining positive results using speaker adaptation techniques (Al Boni et al., 2015).",5.5 Sentiment Analysis of Product Reviews,[0],[0]
"Finally, by exploiting the social network of trust relations, SOCIAL ATTENTION obtains further improvements, outperforming random attention by a small but significant margin.",5.5 Sentiment Analysis of Product Reviews,[0],[0]
"Domain adaptation and personalization Domain adaptation is a classic approach to handling the variation inherent in social media data (Eisenstein, 2013).",6 Related Work,[0],[0]
"Early approaches to supervised domain adaptation focused on adapting the classifier weights across domains, using enhanced feature spaces (Daumé III, 2007) or Bayesian priors (Chelba and Acero, 2006; Finkel and Manning, 2009).",6 Related Work,[0],[0]
"Recent work focuses on unsupervised domain adaptation, which typically works by transforming the input feature space so as to overcome domain differences (Blitzer et al., 2006).",6 Related Work,[0],[0]
"However, in many cases, the data has no natural partitioning into domains.",6 Related Work,[0],[0]
"In preliminary work, we constructed social network domains by running community detection algorithms on the author social network (Fortunato, 2010).",6 Related Work,[0],[0]
"However, these algorithms proved to be unstable on the sparse networks obtained from social media datasets, and offered minimal performance improvements.",6 Related Work,[0],[0]
"In this paper, we convert social network positions into node embeddings, and use an attentional component to smooth the classification rule across the embedding space.
",6 Related Work,[0],[0]
Personalization has been an active research topic in areas such as speech recognition and information retrieval.,6 Related Work,[0],[0]
"Standard techniques for these tasks include linear transformation of model parameters (Leggetter and Woodland, 1995) and collaborative filtering (Breese et al., 1998).",6 Related Work,[0],[0]
"These methods have recently been adapted to personalized sentiment analysis (Tang et al., 2015a; Al Boni et al., 2015).",6 Related Work,[0],[0]
Supervised personalization typically requires labeled training examples for every individual user.,6 Related Work,[0],[0]
"In contrast, by leveraging the social network structure, we can obtain personalization even when labeled data is unavailable for many authors.
",6 Related Work,[0],[0]
"Sentiment analysis with social relations Previous work on incorporating social relations into sentiment classification has relied on the label consistency assumption, where the existence of social connections between users is taken as a clue that the sentiment polarities of the users’ messages should be similar.",6 Related Work,[0],[0]
"Speriosu et al. (2011) construct a heterogeneous network with tweets, users, and n-grams as nodes.",6 Related Work,[0],[0]
"Each node is then associated with a sentiment label distribution, and these label distributions are smoothed by label propagation over the graph.",6 Related Work,[0],[0]
"Similar approaches are explored by Hu et al. (2013), who employ the graph Laplacian as a source of regularization, and by Tan et al. (2011) who take a factor graph approach.",6 Related Work,[0],[0]
A related idea is to label the sentiment of individuals in a social network towards each other: West et al. (2014) exploit the sociological theory of structural balance to improve the accuracy of dyadic sentiment labels in this setting.,6 Related Work,[0],[0]
All of these efforts are based on the intuition that individual predictions p(y) should be smooth across the network.,6 Related Work,[0],[0]
"In contrast, our work is based on the intuition that social neighbors use language similarly, so they should have a similar conditional distribution p(y | x).",6 Related Work,[0],[0]
"These intuitions are complementary: if both hold for a specific setting, then label consistency and linguistic consistency could in principle be combined to improve performance.
",6 Related Work,[0],[0]
"Social relations can also be applied to improve personalized sentiment analysis (Song et al., 2015; Wu and Huang, 2015).",6 Related Work,[0],[0]
Song et al. (2015) present a latent factor model that alleviates the data sparsity problem by decomposing the messages into words that are represented by the weighted sentiment and topic units.,6 Related Work,[0],[0]
Social relations are further incorporated into the model based on the intuition that linked individuals share similar interests with respect to the latent topics.,6 Related Work,[0],[0]
Wu and Huang (2015) build a personalized sentiment classifier for each author; socially connected users are encouraged to have similar userspecific classifier components.,6 Related Work,[0],[0]
"As discussed above, the main challenge in personalized sentiment analysis is to obtain labeled data for each individual author.",6 Related Work,[0],[0]
"Both papers employ distant supervision, using emoticons to label additional instances.",6 Related Work,[0],[0]
"However, emoticons may be unavailable for some authors or even for entire genres, such as reviews.",6 Related Work,[0],[0]
"Furthermore, the pragmatic function of emoticons is com-
plex, and in many cases emoticons do not refer to sentiment (Walther and D’Addario, 2001).",6 Related Work,[0],[0]
"Our approach does not rely on distant supervision, and assumes only that the classification decision function should be smooth across the social network.",6 Related Work,[0],[0]
"This paper presents a new method for learning to overcome language variation, leveraging the tendency of socially proximate individuals to use language similarly—the phenomenon of linguistic homophily.",7 Conclusion,[0],[0]
"By learning basis models that focus on different local regions of the social network, our method is able to capture subtle shifts in meaning across the network.",7 Conclusion,[0],[0]
"Inspired by ensemble learning, we have formulated this model by employing a social attention mechanism: the final prediction is the weighted combination of the outputs of the basis models, and each author has a unique weighting, depending on their position in the social network.",7 Conclusion,[0],[0]
"Our model achieves significant improvements over standard convolutional networks, and ablation analyses show that social network information is the critical ingredient.",7 Conclusion,[0],[0]
"In other work, language variation has been shown to pose problems for the entire NLP stack, from part-of-speech tagging to information extraction.",7 Conclusion,[0],[0]
A key question for future research is whether we can learn a socially-infused ensemble that is useful across multiple tasks.,7 Conclusion,[0],[0]
We thank Duen Horng “Polo” Chau for discussions about community detection and Ramon Astudillo for sharing data and helping us to reproduce the NLSE results.,8 Acknowledgments,[0],[0]
"This research was supported by the National Science Foundation under award RI1452443, by the National Institutes of Health under award number R01GM112697-01, and by the Air Force Office of Scientific Research.",8 Acknowledgments,[0],[0]
The content is solely the responsibility of the authors and does not necessarily represent the official views of these sponsors.,8 Acknowledgments,[0],[0]
"Variation in language is ubiquitous, particularly in newer forms of writing such as social media.",abstractText,[0],[0]
"Fortunately, variation is not random; it is often linked to social properties of the author.",abstractText,[0],[0]
"In this paper, we show how to exploit social networks to make sentiment analysis more robust to social language variation.",abstractText,[0],[0]
The key idea is linguistic homophily: the tendency of socially linked individuals to use language in similar ways.,abstractText,[0],[0]
"We formalize this idea in a novel attention-based neural network architecture, in which attention is divided among several basis models, depending on the author’s position in the social network.",abstractText,[0],[0]
"This has the effect of smoothing the classification function across the social network, and makes it possible to induce personalized classifiers even for authors for whom there is no labeled data or demographic metadata.",abstractText,[0],[0]
This model significantly improves the accuracies of sentiment analysis on Twitter and on review data.,abstractText,[0],[0]
Overcoming Language Variation in Sentiment Analysis with Social Attention,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1049–1058 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"Despite the widespread use of deep neural models across a range of linguistic tasks, to what extent such models can improve information retrieval (IR) and which components a deep neural model for IR should include remain open questions.",1 Introduction,[0],[0]
"In ad-hoc IR, the goal is to produce a ranking of relevant documents given an open-domain (“ad hoc”) query and a document collection.",1 Introduction,[0],[0]
"A ranking model thus aims at evaluating the interactions between different documents and a query, assigning higher scores to documents that better match the query.",1 Introduction,[0],[0]
"Learning to rank models, like the recent IRGAN model (Wang et al., 2017), rely on handcrafted features to encode query document interactions, e.g., the relevance scores from unsupervised ranking models.",1 Introduction,[0],[0]
Neural IR models differ in that they extract interactions directly based on the queries and documents.,1 Introduction,[0],[0]
"Many early neural IR models can be categorized as seman-
tic matching models, as they embed both queries and documents into a low-dimensional space, and then assess their similarity based on such dense representations.",1 Introduction,[0],[0]
"Examples in this regard include DSSM (Huang et al., 2013) and DESM (Mitra et al., 2016).",1 Introduction,[0],[0]
"The notion of relevance is inherently asymmetric, however, making it different from well-studied semantic matching tasks such as semantic relatedness and paraphrase detection.",1 Introduction,[0],[0]
"Instead, relevance matching models such as MatchPyramid (Pang et al., 2016), DRMM (Guo et al., 2016) and the recent K-NRM (Xiong et al., 2017) resemble traditional IR retrieval measures in that they directly consider the relevance of documents’ contents with respect to the query.",1 Introduction,[0],[0]
"The DUET model (Mitra et al., 2017) is a hybrid approach that combines signals from a local model for relevance matching and a distributed model for semantic matching.",1 Introduction,[0],[0]
The two classes of models are fairly distinct.,1 Introduction,[0],[0]
"In this work, we focus on relevance matching models.
",1 Introduction,[0],[0]
"Given that relevance matching approaches mirror ideas from traditional retrieval models, the decades of research on ad-hoc IR can guide us with regard to the specific kinds of relevance signals a model ought to capture.",1 Introduction,[0],[0]
"Unigram matches are the most obvious signals to be modeled, as a counterpart to the term frequencies that appear in almost all traditional retrieval models.",1 Introduction,[0],[0]
"Beyond this, positional information, including where query terms occur and how they depend on each other, can also be exploited, as demonstrated in retrieval models that are aware of term proximity (Tao and Zhai, 2007) and term dependencies (Huston and Croft, 2014; Metzler and Croft, 2005).",1 Introduction,[0],[0]
"Query coverage is another factor that can be used to ensure that, for queries with multiple terms, top-ranked documents contain multiple query terms rather than emphasizing only one query term.",1 Introduction,[0],[0]
"For example, given the query
1049
“dog adoption requirements”, unigram matching signals correspond to the occurrences of the individual terms “dog”, “adoption”, or “requirements”.",1 Introduction,[0],[0]
"When considering positional information, text passages with “dog adoption” or “requirements for dog adoption” are highlighted, distinguishing them from text that only includes individual terms.",1 Introduction,[0],[0]
"Query coverage, meanwhile, further emphasizes that matching signals for “dog”, “adoption”, and “requirements” should all be included in a document.
",1 Introduction,[0],[0]
"Similarity signals from unigram matches are taken as input by DRMM (Guo et al., 2016) after being summarized as histograms, whereas K-NRM (Xiong et al., 2017) directly digests a query-document similarity matrix and summarizes it with multiple kernel functions.",1 Introduction,[0],[0]
"As for positional information, both the MatchPyramid (Pang et al., 2016) and local DUET (Mitra et al., 2017) models account for it by incorporating convolutional layers based on similarity matrices between queries and documents.",1 Introduction,[0],[0]
"Although this leads to more complex models, both have difficulty in significantly outperforming the DRMM model (Guo et al., 2016; Mitra et al., 2017).",1 Introduction,[0],[0]
This indicates that it is non-trivial to go beyond unigrams by utilizing positional information in deep neural IR models.,1 Introduction,[0],[0]
"Intuitively, unlike in standard sequencebased models, the interactions between a query and a document are sequential along the query axis as well as along the document axis, making the problem multi-dimensional in nature.",1 Introduction,[0],[0]
"In addition, this makes it non-trivial to combine matching signals from different parts of the documents and over different query terms.",1 Introduction,[0],[0]
"In fact, we argue that both MatchPyramid and local DUET models fail to fully account for one or more of the aforementioned factors.",1 Introduction,[0],[0]
"For example, as a pioneering work, MatchPyramid is mainly motivated by models developed in computer vision, resulting in its disregard of certain IR-specific considerations in the design of components, such as pooling sizes that ignore the query and document dimensions.",1 Introduction,[0],[0]
"Meanwhile, local DUET’s CNN filters match entire documents against individual query terms, neglecting proximity and possible dependencies among different query terms.
",1 Introduction,[0],[0]
We conjecture that a suitable combination of convolutional kernels and recurrent layers can lead to a model that better accounts for these factors.,1 Introduction,[0],[0]
"In particular, we present a novel re-ranking model
called PACRR (Position-Aware ConvolutionalRecurrent Relevance Matching).",1 Introduction,[0],[0]
Our approach first produces similarity matrices that record the semantic similarity between each query term and each individual term occurring in a document.,1 Introduction,[0],[0]
"These matrices are then fed through a series of convolutional, max-k-pooling, and recurrent layers so as to capture interactions corresponding to, for instance, bigram and trigram matches, and finally to aggregate the signals in order to produce global relevance assessments.",1 Introduction,[0],[0]
"In our model, the convolutional layers are designed to capture both unigram matching and positional information over text windows with different lengths; k-max pooling layers are along the query dimension, preserving matching signals over different query terms; the recurrent layer combines signals from different query terms to produce a query-document relevance score.",1 Introduction,[0],[0]
Organization.,1 Introduction,[0],[0]
The rest of this paper unfolds as follows.,1 Introduction,[0],[0]
Section 2 describes our approach for computing similarity matrices and the architecture of our deep learning model.,1 Introduction,[0],[0]
"The setup and results of our extensive experimental evaluation can be found in Section 3, before concluding in Section 4.",1 Introduction,[0],[0]
"We now describe our proposed PACRR approach, which consists of two main parts: a relevance matching component that converts each querydocument pair into a similarity matrix sim |q|×|d| and a deep architecture that takes a given querydocument similarity matrix as input and produces a query-document relevance score rel(q, d).",2 The PACRR Model,[0],[0]
"Note that in principle the proposed model can be trained end-to-end by backpropagating through the word embeddings, as in (Xiong et al., 2017).",2 The PACRR Model,[0],[0]
"In this work, however, we focus on highlighting the building blocks aiming at capturing positional information, and freeze the word embedding layer to achieve better efficiency.",2 The PACRR Model,[0],[0]
The pipeline is summarized in Figure 1.,2 The PACRR Model,[0],[0]
"We first encode the query-document relevance matching via query-document similarity matrices sim |q|×|d| that encodes the similarity between terms from a query q and a document d, where simij corresponds to the similarity between the i-th term from q and the j-th term from d. When using cosine similarity, we have
sim ∈",2.1 Relevance Matching,[0],[0]
"[−1, 1]|q|×|d|.",2.1 Relevance Matching,[0],[0]
"As suggested in (Hui et al., 2017), query-document similarity matrices preserve a rich signal that can be used to perform relevance matching beyond unigram matches.",2.1 Relevance Matching,[0],[0]
"In particular, n-gram matching corresponds to consecutive document terms that are highly similar to at least one of the query terms.",2.1 Relevance Matching,[0],[0]
Query coverage is reflected in the number of rows in sim that include at least one cell with high similarity.,2.1 Relevance Matching,[0],[0]
"The similarity between a query term q and document term d is calculated by taking the cosine similarity using the pre-trained1 word2vec (Mikolov et al., 2013).
",2.1 Relevance Matching,[0],[0]
The subsequent processing in PACRR’s convolutional layers requires that each query-document similarity matrix have the same dimensionality.,2.1 Relevance Matching,[0],[0]
"Given that the lengths of queries and documents vary, we first transform the raw similarity matrices sim |q|×|d| into sim lq×ld matrices with uniform lq and ld as the number of rows and columns.",2.1 Relevance Matching,[0],[0]
We unify the query dimension lq by zero padding it to the maximum query length.,2.1 Relevance Matching,[0],[0]
"With regard to the document dimension ld, we describe two strategies: firstk and kwindow.
",2.1 Relevance Matching,[0],[0]
PACRR-firstk.,2.1 Relevance Matching,[0],[0]
"Akin to (Mitra et al., 2017), the firstk distillation method simply keeps the first k columns in the matrix, which correspond to the first k terms in the document.",2.1 Relevance Matching,[0],[0]
"If k > |d|, the remaining columns are zero padded.
",2.1 Relevance Matching,[0],[0]
"1https://code.google.com/archive/p/ word2vec/
PACRR-kwindow.",2.1 Relevance Matching,[0],[0]
"As suggested in (Guo et al., 2016), relevance matching is local.",2.1 Relevance Matching,[0],[0]
Document terms that have a low query similarity relative to a document’s other terms cannot contribute substantially to the document’s relevance score.,2.1 Relevance Matching,[0],[0]
Relevance matching can be extracted in terms of pieces of text that include relevant information.,2.1 Relevance Matching,[0],[0]
"That is, one can segment documents according to relevance relative to the given query and retain only the text that is highly relevant to the given query.",2.1 Relevance Matching,[0],[0]
"Given this observation, we prune query-document similarity cells with a low similarity score.",2.1 Relevance Matching,[0],[0]
"In the case of unigrams, we simply choose the top ld terms with the highest similarity to query terms.",2.1 Relevance Matching,[0],[0]
"In the case for text snippets beyond length n, we produce a similarity matrix simnlq×ld for each querydocument pair and each n, because n consecutive terms must be co-considered later on.",2.1 Relevance Matching,[0],[0]
"For each text snippet with length n in the document, kwindow calculates the maximum similarity between each term and the query terms, and then calculates the average similarity over each n-term window.",2.1 Relevance Matching,[0],[0]
It then selects the top k = bld/nc windows by averaging similarity and discards all other terms in the document.,2.1 Relevance Matching,[0],[0]
The document dimension is zero padded if bld/nc is not a multiple of k.,2.1 Relevance Matching,[0],[0]
"When the convolutional layer later operates on a similarity matrix produced by kwindow, the model’s stride is set to n",2.1 Relevance Matching,[0],[0]
"(i.e., the sliding window moves ahead n terms at a time rather than one term at a time) since it can consider at most n consecutive terms that are
present in the original document.",2.1 Relevance Matching,[0],[0]
This variant’s output is a similarity matrix simnlq×ld for each n.,2.1 Relevance Matching,[0],[0]
"Given a query-document similarity matrix sim lq×ld as input, our deep architecture relies on convolutional layers to match every text snippet with length n in a query and in a document to produce similarity signals for different n. Subsequently, two consecutive max pooling layers extract the document’s strongest similarity cues for each n. Finally, a recurrent layer aggregates these salient signals to predict a global query-document relevance score rel(q, d).
",2.2 Deep Retrieval Model,[0],[0]
Convolutional relevance matching over local text snippets.,2.2 Deep Retrieval Model,[0],[0]
The purpose of this step is to match text snippets with different length from a query and a document given their query-document similarity matrix as input.,2.2 Deep Retrieval Model,[0],[0]
This is accomplished by applying multiple two-dimensional convolutional layers with different kernel sizes to the input similarity matrix.,2.2 Deep Retrieval Model,[0],[0]
"Each convolutional layer is responsible for a specific n; by applying its kernel on n×n windows, it produces a similarity signal for each window.",2.2 Deep Retrieval Model,[0],[0]
"When the firstk method is used, each convolutional layer receives the same similarity matrix sim lq×ld as input because firstk produces the same similarity matrix regardless of the n. When the kwindow method is used, each convolutional layer receives a similarity matrix simnlq×ld corresponding to the convolutional layer with a n × n kernel.",2.2 Deep Retrieval Model,[0],[0]
"We use lg−1 different convolutional layers with kernel sizes 2 × 2, 3 × 3, . . .",2.2 Deep Retrieval Model,[0],[0]
", lg × lg, corresponding to bi-gram, tri-gram, . . .",2.2 Deep Retrieval Model,[0],[0]
", lg-gram matching, respectively, where the length of the longest text snippet to consider is governed by a hyperparameter lg.",2.2 Deep Retrieval Model,[0],[0]
"The original similarity matrix corresponds to unigram matching, while a convolutional layer with kernel size n×n is responsible for capturing matching signals on n-term text snippets.",2.2 Deep Retrieval Model,[0],[0]
"Each convolutional layer applies nf different filters to its input, where nf is another hyperparameter.",2.2 Deep Retrieval Model,[0],[0]
"We use a stride of size (1, 1) for the firstk distillation method, meaning that the convolutional kernel advances one step at a time in both the query and document dimensions.",2.2 Deep Retrieval Model,[0],[0]
"For the kwindow distillation method, we use a stride of (1, n) to move the convolutional kernel one step at a time in the query dimension, but n steps at a time in the document dimension.",2.2 Deep Retrieval Model,[0],[0]
"This ensures that the convolutional kernel only operates over consecutive
terms that existed in the original document.",2.2 Deep Retrieval Model,[0],[0]
"Thus, we end up with lg − 1 matrices Cnlq×ld×nf , and the original similarity matrix is directly employed to handle the signals over unigrams.
",2.2 Deep Retrieval Model,[0],[0]
Two max pooling layers.,2.2 Deep Retrieval Model,[0],[0]
The purpose of this step is to capture the ns strongest similarity signals for each query term.,2.2 Deep Retrieval Model,[0],[0]
"Measuring the similarity signals separately for each query term allows the model to consider query term coverage, while capturing the ns strongest similarity signals for each query term allows the model to consider signals from different kinds of relevance matching patterns, e.g., n-gram matching and non-contiguous matching.",2.2 Deep Retrieval Model,[0],[0]
"In practice, we use a small ns to prevent the model from being biased by document length; while each similarity matrix contains the same number of document term scores, longer documents have more opportunity to contain terms that are similar to query terms.",2.2 Deep Retrieval Model,[0],[0]
"To capture the strongest ns similarity signals for each query term, we first perform max pooling over the filter dimension nf to keep only the strongest signal from the nf different filters, assuming that there only exists one particular true matching pattern in a given n × n window, which serves different purposes compared with other tasks, such as the sub-sampling in computer vision.",2.2 Deep Retrieval Model,[0],[0]
"We then perform k-max pooling (Kalchbrenner et al., 2014) over the query dimension lq to keep the strongest ns similarity signals for each query term.",2.2 Deep Retrieval Model,[0],[0]
"Both pooling steps are performed on each of the lg − 1 matrices Ci from the convolutional layer and on the original similarity matrix, which captures unigram matching, to produce the 3-dimensional tensor Plq×lg×ns .",2.2 Deep Retrieval Model,[0],[0]
"This tensor contains the ns strongest signals for each query term and for each n-gram size across all nf filters.
",2.2 Deep Retrieval Model,[0],[0]
Recurrent layer for global relevance.,2.2 Deep Retrieval Model,[0],[0]
"Finally, our model transforms the query term similarity signals in Plq×lg×ns into a single document relevance score rel(q, d).",2.2 Deep Retrieval Model,[0],[0]
"It achieves this by applying a recurrent layer toP , taking a sequence of vectors as input and learning weights to transform them into the final relevance score.",2.2 Deep Retrieval Model,[0],[0]
"More precisely, akin to (Guo et al., 2016), the IDF of each query term qi is passed through a softmax layer for normalization.",2.2 Deep Retrieval Model,[0],[0]
"Thereafter, we split up the query term dimension to produce a matrix Plg×ns for each query term qi, subsequently forming the recurrent layer’s input by flattening each matrix Plg×ns into a vector by concatenating the matrix’s rows together and appending query term qi’s normalized
IDF onto the end of the vector.",2.2 Deep Retrieval Model,[0],[0]
"This sequence of vectors for each query term qi is passed into a Long Short-Term Memory (LSTM) recurrent layer (Hochreiter and Schmidhuber, 1997) with an output dimensionality of one.",2.2 Deep Retrieval Model,[0],[0]
"That is, the LSTM’s input is a sequence of query term vectors where each vector is composed of the query term’s normalized IDF and the aforementioned salient signals for the query term along different kernel sizes.",2.2 Deep Retrieval Model,[0],[0]
"The LSTM’s output is then used as our document relevance score rel(q, d).
",2.2 Deep Retrieval Model,[0],[0]
Training objective.,2.2 Deep Retrieval Model,[0],[0]
"Our model is trained on triples consisting of a query q, relevant document d+, and non-relevant document d−, minimizing a standard pairwise max margin loss as in Eq. 1.
",2.2 Deep Retrieval Model,[0],[0]
"L(q,d+,d−;Θ)=max(0,1−rel(q,d+)+rel(q,d−))",2.2 Deep Retrieval Model,[0],[0]
(1),2.2 Deep Retrieval Model,[0],[0]
"In this section, we empirically evaluate PACRR models using manual relevance judgments from the standard TREC Web Track.",3 Evaluation,[0],[0]
"We compare them against several state-of-the-art neural IR models2, including DRMM (Guo et al., 2016), DUET (Mitra et al., 2017), MatchPyramid (Pang et al., 2016), and K-NRM (Xiong et al., 2017).",3 Evaluation,[0],[0]
The comparisons are over three task settings: reranking search results from a simple initial ranker (RERANKSIMPLE); re-ranking all runs from the TREC Web Track (RERANKALL); and examining neural IR models’ classification accuracy between document pairs (PAIRACCURACY).,3 Evaluation,[0],[0]
We rely on the widely-used 2009–2014 TREC Web Track ad-hoc task benchmarks3.,3.1 Experimental Setup,[0],[0]
The benchmarks are based on the CLUEWEB09 and CLUEWEB12 datasets as document collections.,3.1 Experimental Setup,[0],[0]
"In total, there are 300 queries and more than 100k judgments (qrels).",3.1 Experimental Setup,[0],[0]
Three years (2012–14) of query-likelihood baselines4 provided by TREC5 serve as baseline runs in the RERANKSIMPLE benchmark.,3.1 Experimental Setup,[0],[0]
"In the RERANKALL setting, the search results from runs submitted by participants from each year are also considered: there are 71 (2009), 55
2We also attempted to include IRGAN (Wang et al., 2017) model as a baseline, but failed to obtain reasonable results when training on TREC data.
",3.1 Experimental Setup,[0],[0]
"3http://trec.nist.gov/tracks.html 4Terrier (Ounis et al., 2006) version without filtering spam
documents 5https://github.com/trec-web/ trec-web-2014
(2010), 62 (2011), 48 (2012), 50 (2013), and 27 (2014) runs.",3.1 Experimental Setup,[0],[0]
"ERR@20 (Chapelle et al., 2009) and nDCG@20 (Järvelin and Kekäläinen, 2002) are employed as evaluation measures, and both are computed with the script from TREC6.
Training.",3.1 Experimental Setup,[0],[0]
"At each step, we perform Stochastic Gradient Descent (SGD) with a mini-batch of 32 triples.",3.1 Experimental Setup,[0],[0]
"For the purpose of choosing the triples, we consider all documents that are judged with a label more relevant than Rel7 as highly relevant, and put the remaining relevant documents into a relevant group.",3.1 Experimental Setup,[0],[0]
"To pick each triple, we sample a relevance group with probability proportional to the number of documents in the group within the training set, and then we randomly sample a document with the chosen label to serve as the positive document d+.",3.1 Experimental Setup,[0],[0]
"If the chosen group is the highly relevant group, we randomly sample a document from the relevant group to serve as the negative document d−. If the chosen group is the relevant group, we randomly sample a non-relevant document as d−. This sampling procedure ensures that we differentiate between highly relevant documents (i.e., those with a relevance label of HRel, Key or Nav) and relevant documents (i.e., those are labeled as Rel).",3.1 Experimental Setup,[0],[0]
"The training continues until a
6http://trec.nist.gov/data/web/12/gdeval.pl 7Judgments from TREC include junk pages (Junk), nonrelevance (NRel), relevance (Rel), high relevance (HRel), key pages (Key) and navigational pages (Nav).
",3.1 Experimental Setup,[0],[0]
given number of iterations is reached.,3.1 Experimental Setup,[0],[0]
The model is saved at every iteration.,3.1 Experimental Setup,[0],[0]
We use the model with the best ERR@20 on the validation set to make predictions.,3.1 Experimental Setup,[0],[0]
"Proceeding in a round-robin manner, we report test results on one year by exploiting the respective remaining five years (250 queries) for training.",3.1 Experimental Setup,[0],[0]
"From these 250 queries, we reserve 50 random queries as a held-out set for validation and hyper-parameter tuning, while the remaining 200 queries serve as the actual training set.
",3.1 Experimental Setup,[0],[0]
"As mentioned, model parameters and training iterations are chosen by maximizing the ERR@20 on the validation set.",3.1 Experimental Setup,[0],[0]
The selected model is then used to make predictions on the test data.,3.1 Experimental Setup,[0],[0]
An example of this training procedure is shown in Figure 2.,3.1 Experimental Setup,[0],[0]
"There are four hyper-parameters that govern the behavior of the proposed PACRR-kwindow and PACRR-firstk: the unified length of the document dimension ld, the k-max pooling size ns, the maximum n-gram size lg, and the number of filters used in convolutional layers nf .",3.1 Experimental Setup,[0],[0]
"Due to limited computational resources, we determine the range of hyper-parameters to consider based on pilot experiments and domain insights.",3.1 Experimental Setup,[0],[0]
"In particular, we evaluate ld ∈",3.1 Experimental Setup,[0],[0]
"[256, 384, 512, 640, 768], ns ∈",3.1 Experimental Setup,[0],[0]
"[1, 2, 3, 4], and lg ∈",3.1 Experimental Setup,[0],[0]
"[2, 3, 4].",3.1 Experimental Setup,[0],[0]
"Due to the limited possible matching patterns given a small kernel size (e.g., lg = 3), nf is fixed to 32.",3.1 Experimental Setup,[0],[0]
"For PACRR-firstk, we intuitively desire to retain as much information as possible from the input, and thus ld is always set to 768.
",3.1 Experimental Setup,[0],[0]
"DRMM (DRMMLCH×IDF ), DUET, MatchPyramid and K-NRM are trained under the same settings using the hyperparameters described in their respective papers.",3.1 Experimental Setup,[0],[0]
"In particular, as our focus is on the deep relevance matching model as mentioned in Section 1, we only compare against DUET’s local model, denoted as DUETL.",3.1 Experimental Setup,[0],[0]
"In addition, K-NRM is trained slightly different from the one described in (Xiong et al., 2017), namely, with a frozen word embedding layer.",3.1 Experimental Setup,[0],[0]
"This is to guarantee its fair comparison with other models, given that most of the compared models can be enhanced by co-training the embedding layers, whereas the focus here is the strength coming from the model architecture.",3.1 Experimental Setup,[0],[0]
"A fully connected middle layer with 30 neurons is added to compensate for the reduction of trainable parameters in K-NRM, mirroring the size of DRMM’s first fully connected layer.
",3.1 Experimental Setup,[0],[0]
"All models are implemented with Keras (Chollet et al., 2015) using Tensorflow as backend, and
are trained on servers with multiple CPU cores.",3.1 Experimental Setup,[0],[0]
"In particular, the training of PACRR takes 35 seconds per iteration on average, and in total at most 150 iterations are trained for each model variant.",3.1 Experimental Setup,[0],[0]
RERANKSIMPLE.,3.2 Results,[0],[0]
We first examine the proposed model by re-ranking the search results from the QL baseline on Web Track 2012–14.,3.2 Results,[0],[0]
The results are summarized in Table 1.,3.2 Results,[0],[0]
"It can be seen that DRMM can significantly improve QL on WT12 and WT14, whereas MatchPyramid fails on WT12 under ERR@20.",3.2 Results,[0],[0]
"While DUETL and K-NRM can consistently outperform QL, the two variants of PACRR are the only models that can achieve significant improvements at a 95% significance level on all years under both ERR@20 and nDCG@20.",3.2 Results,[0],[0]
"More remarkably, by solely re-ranking the search results from QL, PACRR-firstk can already rank within the top-3 participating systems on all three years as measured by both ERR and nDCG.",3.2 Results,[0],[0]
The re-ranked search results from PACRR-kwindow also ranks within the top-5 based on nDCG@20.,3.2 Results,[0],[0]
"On average, both PACRR-kwindow and PACRRfirstk achieve 60% improvements over QL.
RERANKALL.",3.2 Results,[0],[0]
"In this part, we would like to further examine the performance of the proposed models in re-ranking different sets of search results.",3.2 Results,[0],[0]
"Thus, we extend our analysis to re-rank search results from all submitted runs from six years of the TREC Web Track ad-hoc task.",3.2 Results,[0],[0]
"In particular, we only consider the judged documents from TREC, which loosely correspond to top-20 documents in each run.",3.2 Results,[0],[0]
"The tested models make predictions for individual documents, which are used to re-rank the documents within each submitted run.",3.2 Results,[0],[0]
"Given that there are about 50 runs for each year, it is no longer feasible to list the scores for each re-ranked run.",3.2 Results,[0],[0]
"Instead, we summarize the results by comparing the performance of each run before and after re-ranking, and provide statistics over each year to compare the methods under consideration in Table 2.",3.2 Results,[0],[0]
"In the top portion of Table 2, we report the relative changes in metrics before and after re-ranking in terms of percentages (“average ∆ measure score”).",3.2 Results,[0],[0]
"In the bottom portion, we report the percentage of systems whose results have increased after re-ranking.",3.2 Results,[0],[0]
"Note that these results assess two different aspects: the average ∆ measure score in Table 2 captures the degree to which a model can improve an initial run, while
the percentages of runs indicate to what extent an improvement can be achieved over runs from different systems.",3.2 Results,[0],[0]
"In other words, the former measures the strength of the models, while the latter measures the adaptability of the models.",3.2 Results,[0],[0]
Both PACRR variants improve upon existing rankings by at least 10% across different years.,3.2 Results,[0],[0]
"Remarkably, in terms of nDCG@20, at least 80% of the submitted runs are improved after re-ranking by the proposed models on individual years, and on 2010–12, all submitted runs are consistently im-
proved by PACRR-firstk.",3.2 Results,[0],[0]
"Moreover, both variants of PACRR can significantly outperform all baseline models on at least three years out of the six years in terms of average improvement.",3.2 Results,[0],[0]
"However, it is clear that none of the tested models can make consistent improvements over all submitted runs across all six years.",3.2 Results,[0],[0]
"In other words, there still exist document pairs that are predicted contradicting to the judgments from TREC.",3.2 Results,[0],[0]
"Thus, in the next part, we further investigate the performance in terms of prediction over document pairs.
PAIRACCURACY.",3.2 Results,[0],[0]
"The ranking of documents can be decomposed into rankings of document pairs as suggested in (Radinsky and Ailon, 2011).",3.2 Results,[0],[0]
"Specifically, a model’s retrieval quality can be examined by checking across a range of individual document pairs, namely, how likely a model can assign a higher score for a more relevant document.",3.2 Results,[0],[0]
"Thus, it is possible for us to compare different models over the same set of complete judgments, removing the issue of different initial runs.",3.2 Results,[0],[0]
"Moreover, although ranking is our ultimate target, a direct inspection of pairwise prediction results can indicate which kinds of document pairs a model succeeds at or fails on.",3.2 Results,[0],[0]
We first convert the graded judgments from TREC into ranked document pairs by comparing their labels.,3.2 Results,[0],[0]
Document pairs are created among documents that have different labels.,3.2 Results,[0],[0]
A prediction is counted as correct if it assigns a higher score to the document from the pair that is labeled with a higher degree of relevance.,3.2 Results,[0],[0]
"The judgments from TREC contain at most six relevance levels, and we merge and unify the original levels from the six years into four grades, namely, Nav, HRel, Rel and NRel.",3.2 Results,[0],[0]
We compute the accuracy for each pair of labels.,3.2 Results,[0],[0]
The statistics are summarized in Table 3.,3.2 Results,[0],[0]
"The volume column lists the percentage of a given label combination out of all document pairs, and the # query column provides the number of queries for which the label combination exists.",3.2 Results,[0],[0]
"In Table 3, we observe that both PACRR models always perform better than all baselines on label combinations HRel vs. NRel, Rel vs. NRel and Nav vs. NRel, which in to-
tal cover 90% of all document pairs.",3.2 Results,[0],[0]
"Meanwhile, apart from Nav-Rel, there is no significant difference when distinguishing Nav from other types.",3.2 Results,[0],[0]
K-NRM and DRMM perform better than the other two baseline models.,3.2 Results,[0],[0]
Hyper-parameters.,3.3 Discussion,[0],[0]
"As mentioned, models are selected based on the ERR@20 over validation data.",3.3 Discussion,[0],[0]
"Hence, it is sufficient to use a reasonable and representative validation dataset, rather than handpicking a specific set of parameter settings.",3.3 Discussion,[0],[0]
"However, to gain a better understanding of the influence of different hyper-parameters, we explore PACRR-kwindow’s effectiveness when several hyper-parameters are varied.",3.3 Discussion,[0],[0]
The results when re-ranking QL search results are given in Figure 3.,3.3 Discussion,[0],[0]
The results are reported based on the models with the highest validation scores after fixing certain hyper-parameters.,3.3 Discussion,[0],[0]
"For example, the ERR@20 in the leftmost figure is obtained when fixing ld to the values shown.",3.3 Discussion,[0],[0]
"The crosses in Figure 3 correspond to the models that were selected for use on the test data, based on their validation set scores.",3.3 Discussion,[0],[0]
"It can be seen that the selected models are not necessarily the best model on the test data, as evidenced by the differences between validation and test data results, but we consistently obtain scores within a reasonable margin.",3.3 Discussion,[0],[0]
"Owing to space constraints, we omit the plots for PACRR-firstk.
",3.3 Discussion,[0],[0]
Choice between kwindow and firstk approaches.,3.3 Discussion,[0],[0]
"As mentioned, both PACRR-kwindow and PACRRfirstk serve to address the variable-length chal-",3.3 Discussion,[0],[0]
"lenge for documents and queries, and to make the training feasible and more efficient.",256 384 512 640 768,[0],[0]
"In general, if both training and test documents are known to be short enough to fit in memory, then PACRR-firstk can be used directly.",256 384 512 640 768,[0],[0]
"Otherwise, PACRR-kwindow is a reasonable choice to provide comparable results.",256 384 512 640 768,[0],[0]
"Alternatively, one can regard this choice as another hyper-parameter, and make a selection based on held-out validation data.
",256 384 512 640 768,[0],[0]
Accuracy in PAIRACCURACY.,256 384 512 640 768,[0],[0]
"Beyond the observations in Section 3.2, we further examine the methods’ accuracy over binary judgments by merging the Nav, HRel and Rel labels.",256 384 512 640 768,[0],[0]
"The accuracies become 73.5%, 74.1% and 67.4% for PACRRkwindow, PACRR-firstk, and DRMM, respectively.",256 384 512 640 768,[0],[0]
"Note that the manual judgments that indicate a document as relevant or non-relevant relative to a given query contain disagreements (Carterette et al., 2008; Voorhees, 2000) and errors (Alonso and Mizzaro, 2012).",256 384 512 640 768,[0],[0]
"In particular, a 64% agreement (cf. Table 2 (b) therein) is observed over the inferred relative order among document pairs based on graded judgments from six trained judges (Carterette et al., 2008).",256 384 512 640 768,[0],[0]
"When reproducing TREC judgments, Al-Maskari et al. (Al-Maskari et al., 2008) reported a 74% agreement (cf. Table 1 therein) with the original judgments from TREC when a group of users re-judged 56 queries on the TREC-8 document collections.",256 384 512 640 768,[0],[0]
"Meanwhile, Alonso and Mizzaro (Alonso and Mizzaro, 2012) observed a 77% agreement relative to judgments from TREC when collecting judgments via crowdsourcing.",256 384 512 640 768,[0],[0]
"Therefore, the more than 73% agreement achieved by both PACRR methods is close to the aforementioned agreement levels among different human assessors.",256 384 512 640 768,[0],[0]
"However, when distinguishing Nav, HRel, and Rel, the tested models
still fall significantly short of the human judges’ agreement levels.",256 384 512 640 768,[0],[0]
"These distinctions are important for a successful ranker, especially when measuring with graded metrics such as ERR@20 and nDCG@20.",256 384 512 640 768,[0],[0]
"Hence, further research is needed for better discrimination among relevant documents with different degrees of relevance.",256 384 512 640 768,[0],[0]
"In addition, as for the distinction between Nav documents and Rel or HRel documents, we argue that since Nav actually indicates that a document mainly satisfies a navigational intent, this makes such documents qualitatively different from Rel and HRel documents.",256 384 512 640 768,[0],[0]
"Specifically, a Nav is more relevant for a user with navigational intent, whereas for other users it may in some cases be less useful than a document that directly includes highly pertinent information content.",256 384 512 640 768,[0],[0]
"Therefore, we hypothesize that further improvements can be obtained by introducing a classifier for user intents, e.g., navigational pages, before employing neural IR models.",256 384 512 640 768,[0],[0]
"In this work, we have demonstrated the importance of preserving positional information for neural IR models by incorporating domain insights into the proposed PACRR model.",4 Conclusion,[0],[0]
"In particular, PACRR captures term dependencies and proximity through multiple convolutional layers with different sizes.",4 Conclusion,[0],[0]
"Thereafter, following two max-pooling layers, it combines salient signals over different query terms with a recurrent layer.",4 Conclusion,[0],[0]
Extensive experiments show that PACRR substantially outperforms four state-of-the-art neural IR models on TREC Web Track ad-hoc datasets and can dramatically improve search results when used as a reranking model.,4 Conclusion,[0],[0]
"In order to adopt deep learning for information retrieval, models are needed that can capture all relevant information required to assess the relevance of a document to a given user query.",abstractText,[0],[0]
"While previous works have successfully captured unigram term matches, how to fully employ position-dependent information such as proximity and term dependencies has been insufficiently explored.",abstractText,[0],[0]
"In this work, we propose a novel neural IR model named PACRR aiming at better modeling position-dependent interactions between a query and a document.",abstractText,[0],[0]
Extensive experiments on six years’ TREC Web Track data confirm that the proposed model yields better results under multiple benchmarks.,abstractText,[0],[0]
PACRR: A Position-Aware Neural IR Model for Relevance Matching,title,[0],[0]
"Differential privacy (Dwork et al., 2006) has emerged as the dominant framework for protected privacy of sensitive training data when releasing learned models to untrusted third parties.",1. Introduction,[0],[0]
"This paradigm owes its popularity in part to the strong privacy model provided, and in part to the availability of general building block mechanisms such as the Laplace (Dwork et al., 2006) & exponential (McSherry & Talwar, 2007), and to composition lemmas for building up more complex mechanisms.",1. Introduction,[0],[0]
These generic mechanisms come endowed with privacy and utility bounds that hold for any appropriate application.,1. Introduction,[0],[0]
Such tools almost alleviate the burden of performing theoretical analysis in developing privacy-preserving learners.,1. Introduction,[0],[0]
"However a persis-
1School of Computing and Information Systems, University of Melbourne, Australia 2Horst Görtz Institute for IT Security and Faculty of Mathematics, Ruhr-Universität Bochum, Germany.",1. Introduction,[0],[0]
"Correspondence to: BR <brubinstein@unimelb.edu.au>, FA <francesco.alda@rub.de>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
tent requirement is the need to bound global sensitivity—a Lipschitz constant of the target, non-private function.",1. Introduction,[0],[0]
"For simple scalar statistics of the private database, sensitivity can be easily bounded (Dwork et al., 2006).",1. Introduction,[0],[0]
"However in many applications—from collaborative filtering (McSherry & Mironov, 2009) to Bayesian inference (Dimitrakakis et al., 2014; 2017; Wang et al., 2015)—the principal challenge in privatisation is completing this calculation.
",1. Introduction,[0],[0]
"In this work we develop a simple approach to approximating global sensitivity with high probability, assuming only oracle access to target function evaluations.",1. Introduction,[0],[0]
"Combined with generic mechanisms like Laplace, exponential, Gaussian or Bernstein, our sampler enables systematising of privatisation: arbitrary computer programs can be made differentially private with no additional mathematical analysis nor dynamic/static analysis, whatsoever.",1. Introduction,[0],[0]
"Our approach does not make any assumptions about the function under evaluation or underlying sampling distribution.
",1. Introduction,[0],[0]
Contributions.,1. Introduction,[0],[0]
"This paper contributes: i) SENSITIVITYSAMPLER for easily-implemented empirical estimation of global sensitivity of (potentially black-box) non-private mechanisms; ii) Empirical process theory for guaranteeing random differential privacy for any mechanism that preserves (stronger) differential privacy under bounded global sensitivity; iii) Experiments demonstrating our sampler on learners for which analytical sensitivity bounds are highly involved; and iv) Examples where sensitivity estimates beat (pessimistic) bounds, delivering pain-free random differential privacy at higher levels of accuracy, when used in concert with generic privacy-preserving mechanisms.
",1. Introduction,[0],[0]
Related Work.,1. Introduction,[0],[0]
"This paper builds on the large body of work in differential privacy (Dwork et al., 2006; Dwork & Roth, 2014), which has gained broad interest in part due the framework’s strong guarantees of data privacy when releasing aggregate statistics or models, and due to availability of many generic privatising mechanisms e.g.,: Laplace (Dwork et al., 2006), exponential (McSherry & Talwar, 2007), Gaussian (Dwork & Roth, 2014), Bernstein (Aldà & Rubinstein, 2017) and many more.",1. Introduction,[0],[0]
"While these mechanisms present a path to privatisation without need for reproving differential privacy or utility, they do have in common a need to analytically bound sensitivity— a Lipschitz-type condition on the target non-private func-
tion.",1. Introduction,[0],[0]
"Often derivations are intricate e.g., for collaborative filtering (McSherry & Mironov, 2009), SVMs (Rubinstein et al., 2012; Chaudhuri et al., 2011), model selection (Thakurta & Smith, 2013), feature selection (Kifer et al., 2012), Bayesian inference (Dimitrakakis et al., 2014; 2017; Wang et al., 2015), SGD in deep learning (Abadi et al., 2016), etc.",1. Introduction,[0],[0]
Undoubtedly the non-trivial nature of bounding sensitivity prohibits adoption by some domain experts.,1. Introduction,[0],[0]
"We address this challenge through the SENSITIVITYSAMPLER that estimates sensitivity empirically—even for privatising black-box computer programs—providing high probability privacy guarantees generically.
",1. Introduction,[0],[0]
"Several systems have been developed to ease deployment of differentially privacy, with Barthe et al. (2016) overviewing contributions from Programming Languages.",1. Introduction,[0],[0]
"Dynamic approaches track privacy budget expended at runtime, e.g., the PINQ (McSherry, 2009; McSherry & Mahajan, 2010) and Airavat (Roy et al., 2010) systems.",1. Introduction,[0],[0]
"Static checking approaches provide privacy usage forewarning, e.g.,: Fuzz (Reed & Pierce, 2010; Palamidessi & Stronati, 2012), DFuzz (Gaboardi et al., 2013).",1. Introduction,[0],[0]
"While promoting privacyby-design, such approaches impose specialised languages or limit target feasibility—challenges addressed by this work.",1. Introduction,[0],[0]
"Moreover our SENSITIVITYSAMPLER mechanism complements such systems, e.g., within broader frameworks for protecting against side-channel attacks (Haeberlen et al., 2011; Mohan et al., 2012).
",1. Introduction,[0],[0]
"Minami et al. (2016) show that special-case Gibbs sampler is ( , δ)-DP without bounded sensitivity.",1. Introduction,[0],[0]
Nissim et al. (2007) ask: Why calibrate for worst-case global sensitivity when the actual database does not witness worst-case neighbours?,1. Introduction,[0],[0]
"Their smoothed sensitivity approach privatises local sensitivity, which itself is sensitive to perturbation.",1. Introduction,[0],[0]
"While this can lead to better sensitivity estimates, our sampled sensitivity still does not require analytical bounds.",1. Introduction,[0],[0]
"A related approach is the sample-and-aggregate mechanism (Nissim et al., 2007) which avoids computation of sensitivity of the underlying target function and instead requires sensitivity of an aggregator combining the outputs of the non-private target run repeatedly on subsamples of the data.",1. Introduction,[0],[0]
"By contrast, our approach provides direct sensitivity estimates, permitting direct privatisation.
",1. Introduction,[0],[0]
Our application of empirical process theory to estimate hard-to-compute quantities resembles the work of Riondato & Upfal (2015).,1. Introduction,[0],[0]
They use VC-theory and sampling to approximate mining frequent itemsets.,1. Introduction,[0],[0]
"Here we approximate analytical computations, and to our knowledge provide a first generic mechanism that preserves random differential privacy (Hall et al., 2012)—a natural weakening of the strong guarantee of differential privacy.",1. Introduction,[0],[0]
"Hall et al. (2012) leverage empirical process theory for a specific worked example, while our setting is general sensitivity estimation.",1. Introduction,[0],[0]
We are interested in non-private mechanism f : Dn → B that maps databases in product space over domain D to responses in a normed space,2. Background,[0],[0]
"B. The terminology of “database” (DB) comes from statistical databases, and should be understood as a dataset.
",2. Background,[0],[0]
Example 1.,2. Background,[0],[0]
"For instance, in supervised learning of linear classifiers, the domain could be Euclidean vectors comprising features & labels, and responses might be parameterisations of learned classifiers such as a normal vector.
",2. Background,[0],[0]
"We aim to estimate sensitivity which is commonly used to calibrate noise in differentially-private mechanisms.
",2. Background,[0],[0]
Definition 2.,2. Background,[0],[0]
"The global sensitivity of non-private f : Dn → B is given by ∆ = supD,D′ ‖f(D)− f(D′)‖B, where the supremum is taken over all pairs of neighbouring databases D,D′ in Dn that differ in one point.",2. Background,[0],[0]
Definition 3.,2. Background,[0],[0]
Randomized mechanism M : Dn → R responding with values in arbitrary response set R preserves -differential privacy for > 0,2. Background,[0],[0]
"if for all neighbouring D,D′ ∈ Dn and measurable R ⊂ R it holds that Pr (M(D) ∈ R) ≤ exp( )Pr (M(D′) ∈ R).",2. Background,[0],[0]
If instead for 0 < δ < 1,2. Background,[0],[0]
it holds that Pr (M(D) ∈ R) ≤ exp( )Pr (M(D′) ∈ R) +,2. Background,[0],[0]
"δ then the mechanism preserves the weaker notion of ( , δ)-differential privacy.
",2. Background,[0],[0]
"In Section 4, we recall a number of key mechanisms that preserve these notions of privacy by virtue of target nonprivate function sensitivity.
",2. Background,[0],[0]
"The following definition due to Hall et al. (2012) relaxes the requirement that uniform smoothness of response distribution holds on all pairs of databases, to the requirement that uniform smoothness holds for likely database pairs.
",2. Background,[0],[0]
Definition 4.,2. Background,[0],[0]
"Randomized mechanism M : Dn → R responding with values in an arbitrary response set R preserves ( , γ)-random differential privacy, at privacy level > 0 and confidence γ ∈",2. Background,[0],[0]
"(0, 1), if Pr (∀R ⊂ R,Pr (M(D) ∈ R) ≤",2. Background,[0],[0]
e,2. Background,[0],[0]
Pr (M(D′) ∈ R)),2. Background,[0],[0]
"≥ 1 − γ, with the inner probabilities over the mechanism’s randomization, and the outer probability over neighbouring D,D′ ∈ Dn drawn from some Pn+1.",2. Background,[0],[0]
"The weaker ( , δ)-DP has analogous definition as ( , δ, γ)-RDP.
Remark 5.",2. Background,[0],[0]
"While strong -DP is ideal, utility may demand compromise.",2. Background,[0],[0]
"Precedent exists for weaker privacy, with the definition of ( , δ)-DP wherein on any databases (including likely ones) a private mechanism may leak sensitive information on low probability responses, forgiven by the additive δ relaxation.",2. Background,[0],[0]
"( , γ)-RDP offers an alternate relaxation, where on all but a small γ-proportion of unlikely database pairs, strong -DP holds—RDP plays a useful role.
",2. Background,[0],[0]
Example 6.,2. Background,[0],[0]
"Consider a database on unbounded positive reals D ∈ Rn+ representing loan default times of
a bank’s customers, and target release statistic f(D) =",2. Background,[0],[0]
n−1 ∑n i=1Di,2. Background,[0],[0]
the sample mean.,2. Background,[0],[0]
To -DP privatise scalarvalued f(D),2. Background,[0],[0]
it is natural to look to the Laplace mechanism.,2. Background,[0],[0]
"However the mechanism requires a bound on the statistic’s global sensitivity, impossible under unbounded D. Note for ∆ > 0, when neighbouring D,D′ satisfy {|f(D)",2. Background,[0],[0]
"− f(D′)| ≤ ∆} then Laplace mechanism M∆, (f(D)) enjoys -DP on that DB pair.",2. Background,[0],[0]
Therefore the probability of the latter event is bounded below by the probability of the former.,2. Background,[0],[0]
"Modelling the default times by iid exponential variables of rate λ > 0, then |f(D)− f(D′)| = |Dn −D′n|/n is distributed as Exp(nλ), and so
Pr (∀t ∈ R,Pr (M∆, (D) = t) ≤ e",2. Background,[0],[0]
"Pr (M∆, (D′) = t))",2. Background,[0],[0]
≥Pr (|f(D)− f(D′)| ≤ ∆),2. Background,[0],[0]
"= 1− e−λn∆ ≥ 1− γ ,
provided that ∆ ≥ log(1/γ)/(λn).",2. Background,[0],[0]
"While -DP fails due to unboundedness, the data is likely bounded and so the mechanism is likely strongly private: M∆, is ( , γ)-RDP.",2. Background,[0],[0]
We consider a statistician looking to apply a differentiallyprivate mechanism to an f : Dn → B whose sensitivity cannot easily be bounded analytically (cf.,3. Problem Statement,[0],[0]
"Example 6 or the case of a computer program).
",3. Problem Statement,[0],[0]
"Instead we assume that the statistician has the ability to sample from some arbitrary product space Pn+1 on Dn+1, can evaluate f arbitrarily (and in particular on the result of this sampling), and is interested in applying a privatising mechanism with the guarantee of random differential privacy (Definition 4).",3. Problem Statement,[0],[0]
Remark 7.,3. Problem Statement,[0],[0]
Natural choices for P present themselves for sampling or defining random differential privacy.,3. Problem Statement,[0],[0]
P could be taken as the underlying distribution from which a sensitive DB was drawn—in the case of sensitive training data but insensitive data source; an alternate test distribution of interest in the case of domain adaptation; or P could be uniform or an otherwise non-informative likelihood (cf.,3. Problem Statement,[0],[0]
Example 6).,3. Problem Statement,[0],[0]
"Proved in full report (Rubinstein & Aldà, 2017), the following relates RDP of similar distributions.",3. Problem Statement,[0],[0]
Proposition 8.,3. Problem Statement,[0],[0]
"Let P,Q be distributions on D with bounded KL divergence KL(P‖Q) ≤ τ .",3. Problem Statement,[0],[0]
If mechanism M on databases in Dn is RDP with confidence γ > 0,3. Problem Statement,[0],[0]
"wrt P then it is also RDP with confidence γ + √ (n+ 1)τ/2 wrt Q, with the same privacy parameters (or , δ).",3. Problem Statement,[0],[0]
When a privatising mechanism M is known to achieve differential privacy for some mapping f :,4. Sensitivity-Induced Differential Privacy,[0],[0]
"Dn → B under bounded global sensitivity, then our approach’s high-probability estimates of sensitivity will imply highprobability preservation of differential privacy.",4. Sensitivity-Induced Differential Privacy,[0],[0]
"In order to
reason about such arguments, we introduce the concept of sensitivity-induced differential privacy.",4. Sensitivity-Induced Differential Privacy,[0],[0]
Definition 9.,4. Sensitivity-Induced Differential Privacy,[0],[0]
"For arbitrary mapping f : Dn → B and randomised mechanism M∆ : B → R, we say that M∆ is sensitivity-induced -differentially private if for a neighbouring pair of databases D,D′ ∈ Dn, and ∆ ≥ 0
‖f(D)− f(D′)‖B ≤ ∆ =⇒ ∀R ⊂ R, Pr (M∆(f(D)) ∈ R)
≤ exp( ) ·",4. Sensitivity-Induced Differential Privacy,[0],[0]
"Pr (M∆(f(D′)) ∈ R)
with the qualification on R being all measurable subsets of the response set R. In the same vein, the analogous definition for ( , δ)-differential privacy can also be made.
",4. Sensitivity-Induced Differential Privacy,[0],[0]
Many generic mechanisms in use today preserve differential privacy by virtue of satisfying this condition.,4. Sensitivity-Induced Differential Privacy,[0],[0]
The following are immediate consequences of existing proofs of differential privacy.,4. Sensitivity-Induced Differential Privacy,[0],[0]
"First, when a non-private target function f aims to release Euclidean vectors responses.",4. Sensitivity-Induced Differential Privacy,[0],[0]
Corollary 10 (Laplace mechanism).,4. Sensitivity-Induced Differential Privacy,[0],[0]
"Consider database D ∈ Dn, normed space B = (Rd, ‖ · ‖1) for d ∈ N, nonprivate function f : Dn → B. The Laplace mechanism1 (Dwork et al., 2006) M∆(f(D))",4. Sensitivity-Induced Differential Privacy,[0],[0]
"∼ Lap (f(D),∆/ ), is sensitivity-induced -differentially private.",4. Sensitivity-Induced Differential Privacy,[0],[0]
Example 11.,4. Sensitivity-Induced Differential Privacy,[0],[0]
Example 6 used Corollary 10 for RDP of the Laplace mechanism on unbounded bank loan defaults.,4. Sensitivity-Induced Differential Privacy,[0],[0]
Corollary 12 (Gaussian mechanism).,4. Sensitivity-Induced Differential Privacy,[0],[0]
"Consider database D ∈ Dn, normed space B = (Rd, ‖ · ‖2) for some d ∈ N, and non-private function f : Dn → B.",4. Sensitivity-Induced Differential Privacy,[0],[0]
"The Gaussian mechanism (Dwork & Roth, 2014) M∆(f(D)) ∼ N (f(D),diag (σ)) with σ2 > 2∆2 log(1.25/δ)/ 2, is sensitivity-induced ( , δ)-differentially private.
",4. Sensitivity-Induced Differential Privacy,[0],[0]
"Second, f may aim to release elements of an arbitrary set R, where a score function s(D, ·) benchmarks quality of potential releases (placing a partial ordering onR).",4. Sensitivity-Induced Differential Privacy,[0],[0]
Corollary 13 (Exponential mechanism).,4. Sensitivity-Induced Differential Privacy,[0],[0]
"Consider database D ∈ Dn, response space R, normed space B =",4. Sensitivity-Induced Differential Privacy,[0],[0]
"( RR, ‖ · ‖∞ ) , non-private score function s : Dn × R → R, and",4. Sensitivity-Induced Differential Privacy,[0],[0]
"restriction f : Dn → B given by f(D) = s(D, ·).",4. Sensitivity-Induced Differential Privacy,[0],[0]
"The exponential mechanism (McSherry & Talwar, 2007) M∆(f(D))",4. Sensitivity-Induced Differential Privacy,[0],[0]
"∼ exp ( (f(D)) (r)/2∆), which when normalised specifies a PDF over responses r ∈ R, is sensitivity-induced -differentially private.
",4. Sensitivity-Induced Differential Privacy,[0],[0]
"Third, f could be function-valued as for learning settings, where given a training set we wish to release a model (e.g., classifier or predictive posterior) that can be subsequently evaluated on (non-sensitive) test points.",4. Sensitivity-Induced Differential Privacy,[0],[0]
Corollary 14 (Bernstein mechanism).,4. Sensitivity-Induced Differential Privacy,[0],[0]
"Consider database D ∈ Dn, query space Y =",4. Sensitivity-Induced Differential Privacy,[0],[0]
"[0, 1]` with constant dimension ` ∈ N, lattice cover of Y of size k ∈ N given by
1Lap (a, b) has unnormalised PDF exp(−‖x− a‖1/b).
",4. Sensitivity-Induced Differential Privacy,[0],[0]
"Algorithm 1 SENSITIVITYSAMPLER Input: database size n, target mapping f :",4. Sensitivity-Induced Differential Privacy,[0],[0]
"Dn → B, sample size m, order statistic index k, distribution P for i = 1",4. Sensitivity-Induced Differential Privacy,[0],[0]
"to m do
Sample D ∼ Pn+1",4. Sensitivity-Induced Differential Privacy,[0],[0]
Set Gi = ‖f (D1...n)−,4. Sensitivity-Induced Differential Privacy,[0],[0]
"f (D1...n−1,n+1)‖B
end for Sort G1, . . .",4. Sensitivity-Induced Differential Privacy,[0],[0]
", Gm as G(1) ≤ . . .",4. Sensitivity-Induced Differential Privacy,[0],[0]
"≤ G(m) return ∆̂ = G(k)
L =",4. Sensitivity-Induced Differential Privacy,[0],[0]
"({0, 1/k, . . .",4. Sensitivity-Induced Differential Privacy,[0],[0]
", 1})`, normed space B =",4. Sensitivity-Induced Differential Privacy,[0],[0]
"( RY , ‖ · ‖∞ ) , non-private function F :",4. Sensitivity-Induced Differential Privacy,[0],[0]
"Dn × Y → R, and restriction f :",4. Sensitivity-Induced Differential Privacy,[0],[0]
"Dn → B given by f(D) = F (D, ·).",4. Sensitivity-Induced Differential Privacy,[0],[0]
"The Bernstein mechanism (Aldà & Rubinstein, 2017)M∆(f(D))",4. Sensitivity-Induced Differential Privacy,[0],[0]
"∼{
Lap ( (f(D))(p),∆(k + 1)`/ )",4. Sensitivity-Induced Differential Privacy,[0],[0]
"| p ∈ L } , is sensitivity-
induced -differentially private.
",4. Sensitivity-Induced Differential Privacy,[0],[0]
"Our framework does not apply directly to the objective perturbation mechanism of Chaudhuri et al. (2011), as that mechanism does not rely directly on a notion of sensitivity of objective function, classifier, or otherwise.",4. Sensitivity-Induced Differential Privacy,[0],[0]
"However it can apply to the posterior sampler used for differentiallyprivate Bayesian inference (Mir, 2012; Dimitrakakis et al., 2014; 2017; Zhang et al., 2016): there the target function f : Dn → B returns the likelihood function p(D|·), itself mapping parameters Θ to R; using the result of f(D) and public prior ξ(θ), the mechanism samples from the posterior ξ(B|D) =",4. Sensitivity-Induced Differential Privacy,[0],[0]
∫ B p(D|θ)dξ(θ)/ ∫,4. Sensitivity-Induced Differential Privacy,[0],[0]
Θ p(D|θ)dξ(θ); differential privacy follows from a Lipschitz condition on f that would require our sensitivity sampler to sample from all database pairs—a minor modification left for future work.,4. Sensitivity-Induced Differential Privacy,[0],[0]
Algorithm 1 presents the SENSITIVITYSAMPLER in detail.,5. The Sensitivity Sampler,[0],[0]
"Consider privacy-insensitive independent sample D1, . . .",5. The Sensitivity Sampler,[0],[0]
", Dm ∼ Pn+1 of databases on n+1 records, where P is chosen to match the desired distribution in definition of random differential privacy.",5. The Sensitivity Sampler,[0],[0]
A number of natural choices are available for P (cf.,5. The Sensitivity Sampler,[0],[0]
Remark 7).,5. The Sensitivity Sampler,[0],[0]
"The main idea of SENSITIVITYSAMPLER is that for each extended-database observation of D ∼ Pn+1, we induce i.i.d.",5. The Sensitivity Sampler,[0],[0]
"observations G1, . . .",5. The Sensitivity Sampler,[0],[0]
", Gm ∈ R of the random variable
G = ‖f (D1...",5. The Sensitivity Sampler,[0],[0]
n)−,5. The Sensitivity Sampler,[0],[0]
f,5. The Sensitivity Sampler,[0],[0]
"(D1...n−1;n+1)‖B .
",5. The Sensitivity Sampler,[0],[0]
"From these observations of the sensitivity of target mapping f : Dn → B, we estimate w.h.p.",5. The Sensitivity Sampler,[0],[0]
"sensitivity that can achieve random differential privacy, for the full suite of sensitivity-induced private mechanisms discussed above.
",5. The Sensitivity Sampler,[0],[0]
"If we knew the full CDF of G, we would simply invert this CDF to determine the level of sensitivity for achieving any desired γ level of random differential privacy: higher
Algorithm 2 SAMPLE-THEN-RESPOND Input: database D; randomised mechanism M∆ : B → R; target mapping f : Dn → B, sample size m, order statistic index k, distribution P Set ∆̂ to SENSITIVITYSAMPLER (|D|, f,m, k, P ) respond M∆̂(D)
confidence would invoke higher sensitivity and therefore lower utility.",5. The Sensitivity Sampler,[0],[0]
"However as we cannot in general possess the true CDF, we resort to uniformly approximating it w.h.p.",5. The Sensitivity Sampler,[0],[0]
"using the empirical CDF induced by the sample G1, . . .",5. The Sensitivity Sampler,[0],[0]
", Gm.",5. The Sensitivity Sampler,[0],[0]
The guarantee of uniform approximation derives from empirical process theory.,5. The Sensitivity Sampler,[0],[0]
Figure 1 provides further intuition behind SENSITIVITYSAMPLER.,5. The Sensitivity Sampler,[0],[0]
"Algorithm 2 presents SAMPLE-THEN-RESPOND which composes SENSITIVITYSAMPLER with any sensitivity-induced differentially-private mechanism.
",5. The Sensitivity Sampler,[0],[0]
"Our main result Theorem 15 presents explicit expressions for parameters m, k that are sufficient to guarantee that SAMPLE-THEN-RESPOND achieves ( , δ, γ)-random differential privacy.",5. The Sensitivity Sampler,[0],[0]
"Under that result the parameter ρ, which controls the uniform approximation of the empirical CDF from G1, . . .",5. The Sensitivity Sampler,[0],[0]
", Gm sample to the true CDF, is introduced as a free parameter.",5. The Sensitivity Sampler,[0],[0]
"We demonstrate through a series of optimisations in Table 1 how ρ can be tuned to optimise either sampling effort m, utility via order statistic index k, or privacy confidence γ.",5. The Sensitivity Sampler,[0],[0]
These alternative explicit choices for ρ serve as optimal operating points for the mechanism.,5. The Sensitivity Sampler,[0],[0]
SENSITIVITYSAMPLER simplifies the application of differential privacy by obviating the challenge of bounding sensitivity.,5.1. Practicalities,[0],[0]
"As such, it is important to explore any practical issues arising in its implementation.",5.1. Practicalities,[0],[0]
"The algorithm itself involves few main stages: sampling databases, measuring sensitivity, sorting, order statistic lookup (inversion), followed by the sensitivity-induced private mechanism.
",5.1. Practicalities,[0],[0]
Sampling.,5.1. Practicalities,[0],[0]
"As discussed in Remark 7, a number of natural choices for sampling distribution P could be made.",5.1. Practicalities,[0],[0]
"Where a simulation process exists, capable of generating synthetic data approximatingD, then this could be run.",5.1. Practicalities,[0],[0]
"For example in the Bayesian setting (Dimitrakakis et al., 2014), one could use a public conditional likelihood p(·|θ), parametric family Θ, prior ξ(θ) and sample from the marginal∫
Θ p(x|θ)dξ(θ).",5.1. Practicalities,[0],[0]
"Alternatively, it may suffice to sample from the uniform distribution on D, or Gaussian restricted to EuclideanD.",5.1. Practicalities,[0],[0]
"In any of these cases, sampling is relatively straightforward and the choice should consider meaningful random differential privacy guarantees relative to P .
Sensitivity Measurement.",5.1. Practicalities,[0],[0]
"A trivial stage, given neighbouring databases, measurement could involve expanding a mathematical expression representing a target function, or a computer program such as running a deep learning or computer vision open-source package.",5.1. Practicalities,[0],[0]
"For some targets, it may be that running first on one database, covers much of the computation required for the neighbouring database in which case amortisation may improve runtime.",5.1. Practicalities,[0],[0]
"The cost of sensitivity measurement will be primarily determined by sample size m. Note that sampling and measurement can be trivially parallelised over map-reduce-like platforms.
",5.1. Practicalities,[0],[0]
"Sorting, Inversion.",5.1. Practicalities,[0],[0]
"Strictly speaking the entire sensitivity sample need not be sorted, as only one order statistic is required.",5.1. Practicalities,[0],[0]
"That said, sorting even millions of scalar measurements can be accomplished in under a second on a stock machine.",5.1. Practicalities,[0],[0]
"An alternative strategy to inversion as presented, is to take the maximum sensitivity measured so as to maximise privacy without consideration to utility.
",5.1. Practicalities,[0],[0]
Mechanism.,5.1. Practicalities,[0],[0]
"It is noteworthy that in settings where mechanism M∆ is to be run multiple times, the estimation of ∆̂ need not be redone.",5.1. Practicalities,[0],[0]
As such SENSITIVITYSAMPLER could be performed entirely in an offline amortisation stage.,5.1. Practicalities,[0],[0]
"For the i.i.d. sample of sensitivities G1, . . .",6. Analysis,[0],[0]
", Gm drawn within Algorithm 1, denote the corresponding fixed unknown CDF, and corresponding random empirical CDF, by
Φ (g) =",6. Analysis,[0],[0]
"Pr (G ≤ g) ,
Φm (g) = 1
m m∑ i=1 1",6. Analysis,[0],[0]
"[Gi ≤ g] .
",6. Analysis,[0],[0]
"In this section we use Φm (∆) to bound the likelihood of a (non-private, possibly deterministic) mapping f : Dn → R achieving sensitivity ∆.",6. Analysis,[0],[0]
"This permits bounding RDP.
Theorem 15.",6. Analysis,[0],[0]
"Consider any non-private mapping f : Dn → B, any sensitivity-induced ( , δ)-differentially private mechanismM∆ mapping B to (randomised) responses in R, any database D of n records, privacy parameters
> 0, δ ∈",6. Analysis,[0],[0]
"[0, 1], γ ∈ (0, 1), and sampling parameters size m ∈ N, order statistic index m ≥ k ∈ N, approximation confidence 0",6. Analysis,[0],[0]
<,6. Analysis,[0],[0]
"ρ < min{γ, 1/2}, distribution P on D. If
m ≥ 1 2(γ − ρ)2 log
( 1
ρ
) , (1)
k ≥ m",6. Analysis,[0],[0]
"( 1− γ + ρ+ √ log(1/ρ)/(2m) ) , (2)
then Algorithm 2 run with D,M∆, f,m, k, P , preserves ( , δ, γ)-random differential privacy.
",6. Analysis,[0],[0]
Proof.,6. Analysis,[0],[0]
"Consider any ρ′ ∈ (0, 1) to be determined later, and consider sampling G1, . . .",6. Analysis,[0],[0]
", Gm and sorting to G(1) ≤ . . .",6. Analysis,[0],[0]
≤ G(m).,6. Analysis,[0],[0]
"Provided that
1− γ + ρ+ ρ′ ≤ 1 ⇔ ρ′ ≤",6. Analysis,[0],[0]
γ,6. Analysis,[0],[0]
"− ρ , (3)
then the random sensitivity ∆̂ = G(k), where k = dm(1− γ + ρ + ρ′)e, is the smallest ∆ ≥ 0 such that Φm(∆) ≥ 1− γ + ρ+ ρ′. That is,
Φm(∆̂) ≥ 1− γ + ρ+ ρ′ .",6. Analysis,[0],[0]
"(4)
Note that if 1− γ + ρ+ ρ′ < 0 then ∆̂ can be taken as any ∆, namely zero.",6. Analysis,[0],[0]
"Define the events
A∆ = {∀R ⊂ R, Pr (M∆(f(D)) ∈ R) ≤ exp( )·",6. Analysis,[0],[0]
"Pr (M∆(f(D ′)) ∈ R) + δ}
Bρ′ = { sup ∆",6. Analysis,[0],[0]
(Φm(∆)− Φ(∆)),6. Analysis,[0],[0]
"≤ ρ′ } .
",6. Analysis,[0],[0]
"The first is the event that DP holds for a specific DB pair, when the mechanism is run with (possibly random) sensitivity parameter ∆; the second records the empirical CDF uniformly one-sided approximating the CDF to level ρ′. By the sensitivity-induced -differential privacy of M∆,
∀∆ > 0 , PrD,D′∼Pn+1 (A∆) ≥ Φ(∆) .",6. Analysis,[0],[0]
"(5)
The random D,D′ on the left-hand side induce the distribution on G on the right-hand side under which Φ(∆) = PrG (G ≤ ∆).",6. Analysis,[0],[0]
The probability on the left is the level of random differential privacy of M∆ when run on fixed ∆.,6. Analysis,[0],[0]
"By the Dvoretzky-Kiefer-Wolfowitz inequality (Massart, 1990) we have that for all ρ′ ≥ √ (log 2)/(2m),
PrG1,...,Gm (Bρ′) ≥ 1− e−2mρ ′2 .",6. Analysis,[0],[0]
"(6)
Putting inequalities (4), (5), and (6) together, provided that ρ′ ≥ √ (log 2)/(2m), yields that
Table 1.",6. Analysis,[0],[0]
"Optimal ρ operating points for budgeted resources—γ or m—minimising m, γ or k; proved in (Rubinstein & Aldà, 2017).
",6. Analysis,[0],[0]
"Budgeted Optimise ρ γ m k
γ ∈ (0, 1) m exp ( W−1 ( − γ
2 √ e ) + 12 ) •
⌈
log( 1ρ )
2(γ−ρ)2
⌉ ⌈
m
(
1− γ + ρ+
√
log( 1ρ )
2m
)⌉
m ∈ N,",6. Analysis,[0],[0]
γ k exp ( 1 2W−1 ( − 14m )),6. Analysis,[0],[0]
"≥ ρ+ √ log( 1ρ ) 2m • ⌈ m ( 1− γ + ρ+ √ log( 1ρ ) 2m
)⌉ m ∈ N γ exp",6. Analysis,[0],[0]
( 1 2W−1 ( − 14m )),6. Analysis,[0],[0]
"ρ+ √ log( 1ρ ) 2m • m
PrD,D′,G1,...,Gm ( A∆̂ )
",6. Analysis,[0],[0]
=E [ 1 [ A∆̂ ],6. Analysis,[0],[0]
∣∣Bρ′]Pr (Bρ′) + E,6. Analysis,[0],[0]
"[1 [A∆̂]∣∣Bρ′]Pr (Bρ′)
≥E",6. Analysis,[0],[0]
"[ Φ ( ∆̂ )∣∣∣Bρ′]Pr (Bρ′)
≥E",6. Analysis,[0],[0]
"[
Φm
( ∆̂ )",6. Analysis,[0],[0]
− ρ′ ∣∣∣Bρ′] (1− exp (−2mρ′2)),6. Analysis,[0],[0]
≥ (1− γ + ρ+ ρ′,6. Analysis,[0],[0]
− ρ′),6. Analysis,[0],[0]
"( 1− exp ( −2mρ′2
)) ≥(1− γ + ρ)(1− ρ) ≥1− γ + ρ− ρ =1− γ .
",6. Analysis,[0],[0]
"The last inequality follows from ρ < γ; the penultimate inequality follows from setting
ρ′",6. Analysis,[0],[0]
"≥
√ 1
2m log
( 1
ρ
) , (7)
and so the DKW condition (Massart, 1990), that ρ′ ≥√ (log 2)/(2m), is met provided that ρ ≤ 1/2.",6. Analysis,[0],[0]
"Now (1) follows from substituting (7) into (3).
",6. Analysis,[0],[0]
"Note that for sensitivity-induced -differentially private mechanisms, the theorem applies with δ = 0.
",6. Analysis,[0],[0]
Optimising Free Parameter ρ.,6. Analysis,[0],[0]
"Table 1 recommends alternative choices of free parameter ρ, derived by optimising the sampler’s performance along one axis—privacy confidence γ, sampler effort m, or order statistic index k— given a fixed budget of another.",6. Analysis,[0],[0]
"The table summarises
results with proofs found in report (Rubinstein & Aldà, 2017).",6. Analysis,[0],[0]
"The specific expressions derived involve branches of the Lambert-W function, which is the inverse relation of the function f(z) = z exp(z), and is implemented as a special function in scientific libraries as standard.",6. Analysis,[0],[0]
"While Lambert-W is in general a multi-valued relation on the analytic complex domain, all instances in our results are single-real-valued functions on the reals.",6. Analysis,[0],[0]
"The next result presents the first operating point’s corresponding rate on effort in terms of privacy, and follows from recent bounds on the secondary branch W−1 due to Chatzigeorgiou (2013).2
Corollary 16.",6. Analysis,[0],[0]
"Minimising m for given γ (cf. Table 1, row 1), yields rate for m as o ( 1 γ2 log 1 γ ) with increasing privacy confidence 1γ →∞.",6. Analysis,[0],[0]
Remark 17.,6. Analysis,[0],[0]
"Theorem 15 and Table 1 elucidate that effort, privacy and utility are in tension.",6. Analysis,[0],[0]
"Effort is naturally decreased by reducing the confidence level of RDP (ρ chosen to minimise m, or γ).",6. Analysis,[0],[0]
"By minimising order statistic index k, we select smaller Gk and therefore sensitivity estimate",6. Analysis,[0],[0]
∆̂. This in turn leads to lower generic mechanism noise and higher utility.,6. Analysis,[0],[0]
All this is achieved by sacrificing effort or privacy confidence.,6. Analysis,[0],[0]
"As usual, sacrificing or δ privacy levels also leads to utility improvement.",6. Analysis,[0],[0]
"Figures 2 and 3 visualise these operating points.
",6. Analysis,[0],[0]
"2That for all u > 0, −1",6. Analysis,[0],[0]
− √ 2u − u < W−1(−e−u−1),6. Analysis,[0],[0]
"<
",6. Analysis,[0],[0]
"−1− √ 2u− 2
3 u.
Less conservative estimates on sensitivity can lead to superior utility while also enjoying easier implementation.",6. Analysis,[0],[0]
"This hypothesis is borne out in experiments in Section 7.
",6. Analysis,[0],[0]
Proposition 18.,6. Analysis,[0],[0]
For any f : Dn → B with global sensitivity ∆ =,6. Analysis,[0],[0]
supD∼D′ ‖f(D),6. Analysis,[0],[0]
"− f(D′)‖B, SENSITIVITYSAMPLER’s random sensitivity ∆̂ ≤",6. Analysis,[0],[0]
"∆. As a result, Algorithm 2 run with any of the sensitivity-induced private mechanisms of Corollaries 10–14 achieves utility dominating that of the respective mechanisms run with ∆.",6. Analysis,[0],[0]
We now demonstrate the practical value of SENSITIVITYSAMPLER.,7. Experiments,[0],[0]
"First in Section 7.1 we illustrate how SENSITIVITYSAMPLER sensitivity quickly approaches analytical high-probability sensitivity, and how it can be significantly lower than worst-case global sensitivity in Section 7.2.",7. Experiments,[0],[0]
"Running privatising mechanisms with lower sensitivity parameters can mitigate utility loss, while maintaining (a weaker form of) differential privacy.",7. Experiments,[0],[0]
We present experimental evidence of this utility savings in Section 7.3.,7. Experiments,[0],[0]
"While application domains may find the alternate balance towards utility appealing by itself, it should be stressed that a significant advantage of SENSITIVITYSAMPLER is its ease of implementation.",7. Experiments,[0],[0]
Consider running Example 6: private release of sample mean f(D) =,7.1. Analytical RDP vs. Sampled Sensitivity,[0],[0]
n−1 ∑n i=1Di of a database D drawn i.i.d.,7.1. Analytical RDP vs. Sampled Sensitivity,[0],[0]
from Exp(1).,7.1. Analytical RDP vs. Sampled Sensitivity,[0],[0]
"Figure 4 presents, for varying probability γ: the analytical bound on sensitivity versus SENSITIVITYSAMPLER estimates for different sampling budgets averaged over 50 repeats.",7.1. Analytical RDP vs. Sampled Sensitivity,[0],[0]
"For fixed sampling budget, ∆̂ is estimated at lower limits on γ, quickly converging to exact.",7.1. Analytical RDP vs. Sampled Sensitivity,[0],[0]
Consider now the challenging goal of privately releasing an SVM classifier fit to sensitive training data.,7.2. Global Sensitivity vs. Sampled Sensitivity,[0],[0]
"In applying the Laplace mechanism to releasing the primal normal vector, Rubinstein et al. (2012) bound the vector’s sensitivity using algorithmic stability of the SVM.",7.2. Global Sensitivity vs. Sampled Sensitivity,[0],[0]
"In particular, a lengthy derivation establishes that ‖wD − wD′‖1 ≤ 4LCκ √ d/n for a statistically consistent formulation of the SVM with convex L-Lipschitz loss, d-dimensional feature mapping with supx k(x,x) ≤ κ2, and regularisation parameter C. While the original work (and others since) did not consider the practical problem of releasing unregularised bias term b, we can effectively bound this sensitivity via a short argument in full report (Rubinstein & Aldà, 2017).
",7.2. Global Sensitivity vs. Sampled Sensitivity,[0],[0]
Proposition 19.,7.2. Global Sensitivity vs. Sampled Sensitivity,[0],[0]
"For the SVM run with hinge loss, linear kernel, D =",7.2. Global Sensitivity vs. Sampled Sensitivity,[0],[0]
"[0, 1]d, the release (w, b) has L1 global sensitivity bounded by 2 + 2C",7.2. Global Sensitivity vs. Sampled Sensitivity,[0],[0]
√ d+,7.2. Global Sensitivity vs. Sampled Sensitivity,[0],[0]
"4Cd/n.
We train private SVM using the Laplace mechanism (Rubinstein et al., 2012), with global sensitivity bound of Proposition 19 or SENSITIVITYSAMPLER.",7.2. Global Sensitivity vs. Sampled Sensitivity,[0],[0]
"We synthesise a dataset of n = 1000 points, selected with equal probability of being drawn from the positive class N(0.2 · 1,diag(0.01)) or negative class N(0.8 · 1,diag(0.01)).",7.2. Global Sensitivity vs. Sampled Sensitivity,[0],[0]
The feature space’s dimension varies from d = 8 through d = 64.,7.2. Global Sensitivity vs. Sampled Sensitivity,[0],[0]
"The SVMs are run with C = 3, SENSITIVITYSAMPLER with m = 1500 & varying γ.",7.2. Global Sensitivity vs. Sampled Sensitivity,[0],[0]
Figure 5 shows very different sensitivities obtained.,7.2. Global Sensitivity vs. Sampled Sensitivity,[0],[0]
"While estimated ∆̂ hovers around 0.01 largely independent of γ, global sensitivity ∆ exceeds 20—two orders of magnitude greater.",7.2. Global Sensitivity vs. Sampled Sensitivity,[0],[0]
"These patterns are repeated as dimension increases; sensitivity increasing is to be expected since as dimensions are added, the few points in the training set become more likely to be support vectors and thus affecting sensitivity.",7.2. Global Sensitivity vs. Sampled Sensitivity,[0],[0]
Such conservative estimates could clearly lead to inferior utility.,7.2. Global Sensitivity vs. Sampled Sensitivity,[0],[0]
Support Vector Classification.,7.3. Effect on Utility,[0],[0]
"We return to the same SVM setup as in the previous section, with d = 2, now plotting utility as misclassification error (averaged over 500 repeats) vs. privacy budget .",7.3. Effect on Utility,[0],[0]
Here we set γ = 0.05 and include also the non-private SVM’s performance as a bound on utility possible.,7.3. Effect on Utility,[0],[0]
See Figure 6.,7.3. Effect on Utility,[0],[0]
At very high privacy levels both private SVMs suffer the same poor error.,7.3. Effect on Utility,[0],[0]
"But quickly with lower privacy, the misclassification error of SENSITIVITYSAMPLER drops until it reaches the nonprivate rate.",7.3. Effect on Utility,[0],[0]
Simultaneously the global sensitivity approach has a significantly higher value and suffers a much slower decline.,7.3. Effect on Utility,[0],[0]
"These results suggest that SENSITIVITYSAMPLER can achieve much better utility in addition to sensitivity.
",7.3. Effect on Utility,[0],[0]
Kernel Density Estimation.,7.3. Effect on Utility,[0],[0]
We finally consider a one dimensional (d = 1) KDE setting.,7.3. Effect on Utility,[0],[0]
"In Figure 7 we show the error (averaged over 1000 repeats) of the Bernstein mechanism (with lattice size k = 10 and Bernstein order h = 3) on 5000 points drawn from a mixture of two normal distributions N(0.5, 0.02) and N(0.75, 0.005) with weights 0.4, 0.6, respectively.",7.3. Effect on Utility,[0],[0]
"For this experimental result, we set m = 50000 and two different values for γ, as displayed in Figure 7.",7.3. Effect on Utility,[0],[0]
"Once again we observe that for high privacy levels the global sensitivity approach incurs a higher error relative to non-private, while SENSITIVITYSAMPLER provides stronger utility.",7.3. Effect on Utility,[0],[0]
"At lower privacy, both approaches converge to the approximation error of the Bernstein polynomial used.",7.3. Effect on Utility,[0],[0]
"In this paper we propose SENSITIVITYSAMPLER, an algorithm for empirical estimation of sensitivity for privatisation of black-box functions.",8. Conclusion,[0],[0]
"Our work addresses an important usability gap in differential privacy, whereby several generic privatisation mechanisms exist complete with privacy and utility guarantees, but require analyti-
cal bounds on global sensitivity (a Lipschitz condition) on the non-private target.",8. Conclusion,[0],[0]
"While this sensitivity is trivially derived for simple statistics, for state-of-the-art learners sensitivity derivations are arduous e.g., in collaborative filtering (McSherry & Mironov, 2009), SVMs (Rubinstein et al., 2012; Chaudhuri et al., 2011), model selection (Thakurta & Smith, 2013), feature selection (Kifer et al., 2012), Bayesian inference (Dimitrakakis et al., 2014; Wang et al., 2015), and deep learning (Abadi et al., 2016).
",8. Conclusion,[0],[0]
"While derivations may prevent domain experts from leveraging differential privacy, our SENSITIVITYSAMPLER promises to make privatisation simple when using existing mechanisms including Laplace (Dwork et al., 2006), Gaussian (Dwork & Roth, 2014), exponential (McSherry & Talwar, 2007) and Bernstein (Aldà & Rubinstein, 2017).",8. Conclusion,[0],[0]
"All such mechanisms guarantee differential privacy on pairs of databases for which a level ∆ of non-private function sensitivity holds, when the mechanism is run with that ∆ parameter.",8. Conclusion,[0],[0]
"For all such mechanisms we leverage results from empirical process theory to establish guarantees of random differential privacy (Hall et al., 2012) when using sampled sensitivities only.
",8. Conclusion,[0],[0]
Experiments demonstrate that real-world learners can easily be run privately without any new derivation whatsoever.,8. Conclusion,[0],[0]
"And by using a naturally-weaker form of privacy, while replacing worst-case global sensitivity bounds with estimated (actual) sensitivities, we can achieve far superior utility than existing approaches.",8. Conclusion,[0],[0]
F. Aldà and B. Rubinstein acknowledge the support of the DFG Research Training Group GRK 1817/1 and the Australian Research Council (DE160100584) respectively.,Acknowledgements,[0],[0]
"Popular approaches to differential privacy, such as the Laplace and exponential mechanisms, calibrate randomised smoothing through global sensitivity of the target non-private function.",abstractText,[0],[0]
Bounding such sensitivity is often a prohibitively complex analytic calculation.,abstractText,[0],[0]
"As an alternative, we propose a straightforward sampler for estimating sensitivity of non-private mechanisms.",abstractText,[0],[0]
"Since our sensitivity estimates hold with high probability, any mechanism that would be ( , δ)differentially private under bounded global sensitivity automatically achieves ( , δ, γ)-random differential privacy (Hall et al., 2012), without any target-specific calculations required.",abstractText,[0],[0]
"We demonstrate on worked example learners how our usable approach adopts a naturally-relaxed privacy guarantee, while achieving more accurate releases even for non-private functions that are black-box computer programs.",abstractText,[0],[0]
Pain-Free Random Differential Privacy with Sensitivity Sampling,title,[0],[0]
Chemical space is huge: it is estimated to contain over 1060 molecules.,1. Introduction,[0],[0]
"Among these, fewer than 100 million compounds can be found in public repositories or databases (Reymond et al., 2012).",1. Introduction,[0],[0]
"This discrepancy between known
*Equal contribution 1University of Cambridge, Cambridge, UK 2Invenia Labs, Cambridge, UK 3Harvard University, Cambridge, USA 4IBM Research, UK.",1. Introduction,[0],[0]
Correspondence to: José,1. Introduction,[0],[0]
"Miguel Hernández-Lobato <jmh233@cam.ac.uk>, Edward O. Pyzer-Knapp <epyzerk3@uk.ibm.com>.
Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
compounds and possible compounds indicates the potential for discoverying many new compounds with highly desirable functionality (e.g., new energy materials, pharmaceuticals, dyes, etc.).",1. Introduction,[0],[0]
"While the vast size of chemical space makes this an enormous opportunity, it also presents a significant difficulty in the identification of new relevant compounds among the many unimportant ones.",1. Introduction,[0],[0]
"This challenge is so great that any discovery process relying purely on the combination of scientific intuition with trial and error experimentation is slow, tedious and in many cases infeasible.
",1. Introduction,[0],[0]
"To accelerate the search, high-throughput approaches can be used in a combinatorial exploration of small specific areas of chemical space (Rajan, 2008).",1. Introduction,[0],[0]
"These have led to the development of high-throughput virtual screening (Pyzer-Knapp et al., 2015; Gómez-Bombarelli et al., 2016) in which large libraries of molecules are created and then analyzed using theoretical and computational techniques, typically by running a large number of parallel simulations in a computer cluster.",1. Introduction,[0],[0]
The objective is to reduce an initially very large library of molecules to a small set of promising leads for which expensive experimental evaluation is justified.,1. Introduction,[0],[0]
"However, even though these techniques only search a tiny drop in the ocean of chemical space, they can result in massive libraries whose magnitude exceeds traditional computational capabilities.",1. Introduction,[0],[0]
"As a result, at present, there is an urgent need to accelerate high-throughput screening approaches.
",1. Introduction,[0],[0]
"Bayesian optimization (BO) (Jones et al., 1998) can speed up the discovery process by using machine learning to guide the search and make improved decisions about what molecules to analyze next given the data collected so far.",1. Introduction,[0],[0]
"However, current BO methods cannot scale to the large number of parallel measurements and the massive libraries of candidate molecules currently used in high-throughput screening (Pyzer-Knapp et al., 2015).",1. Introduction,[0],[0]
"While there are BO methods that allow parallel data collection, these methods have typically been limited to tens of data points per batch (Snoek et al., 2012; Shahriari et al., 2014; Gonzlez et al., 2016).",1. Introduction,[0],[0]
"In contrast, high-throughput screening may allow the simultaneous collection of thousands of data points via large-scale parallel computation.",1. Introduction,[0],[0]
This creates a need for new scalable methods for parallel Bayesian optimization.,1. Introduction,[0],[0]
"ar X
iv :1
70 6.
01 82
5v 1
[ st
at .M
L ]
6 J
un 2
01 7
To address the above difficulty, we present here a scalable solution for parallel Bayesian optimization based on a distributed implementation of the Thompson sampling heuristic (Thompson, 1933; Chapelle & Li, 2011).",1. Introduction,[0],[0]
"We show that, for the case of small batch sizes, the proposed parallel and distributed Thompson sampling (PDTS) method performs as well as a parallel implementation of expected improvement (EI) (Snoek et al., 2012; Ginsbourger et al., 2011), the most widely used Bayesian optimization heuristic.",1. Introduction,[0],[0]
"Parallel EI selects the batch entries sequentially and so EI proposals can’t be parallelized, which limits its scalability properties.",1. Introduction,[0],[0]
PDTS generates each batch of evaluation locations by selecting the different batch entries independently and in parallel.,1. Introduction,[0],[0]
"Consequently, PDTS is highly scalable and applicable to large batch sizes.",1. Introduction,[0],[0]
"We also evaluate the performance of PDTS in several real-world high-throughput screening experiments for material and drug discovery, where parallel EI is infeasible.",1. Introduction,[0],[0]
"In these problems, PDTS outperforms other scalable baselines such as a greedy search strategy, -greedy approaches and a random search method.",1. Introduction,[0],[0]
"These results indicate that PDTS is a successful solution for largescale parallel Bayesian optimization.
",1. Introduction,[0],[0]
"Algorithm 1 Sequential Thompson sampling Input: initial data DI(1) = {(xi, yi)}i∈I(1) for t = 1 to T do
Compute current posterior p(θ|DI(t))",1. Introduction,[0],[0]
Sample θ from p(θ|DI(t)),1. Introduction,[0],[0]
"Select k ← argmaxj 6∈I(t)E[yj |xj ,θ] Collect yk by evaluating f at xk DI(t+1) ← DI(t) ∪ {(xk, yk)}
end for",1. Introduction,[0],[0]
"Let us assume we have a large library of candidate molecules M = {m1, . . .",2. BO and Thompson Sampling,[0],[0]
",m|M|}.",2. BO and Thompson Sampling,[0],[0]
"Our goal is to identify a small subset of elements {mi} ⊂ M for which the
f(mi) are as high as possible, with f being an expensiveto-evaluate objective function.",2. BO and Thompson Sampling,[0],[0]
"The objective f could be, for example, an estimate of the power-conversion efficiency of organic photovoltaics, as given by expensive quantum mechanical simulations (Scharber et al., 2006), and we may want to identify the top 1% elements inM according to this score.
",2. BO and Thompson Sampling,[0],[0]
Bayesian optimization methods can be used to identify the inputs that maximize an expensive objective function f by performing only a reduced number of function evaluations.,2. BO and Thompson Sampling,[0],[0]
"For this, BO uses a model to make predictions for the value of f at new inputs given data from previous evaluations.",2. BO and Thompson Sampling,[0],[0]
"The next point to evaluate is then chosen by maximizing an acquisition function that quantifies the benefit of evaluating the objective at a particular location.
",2. BO and Thompson Sampling,[0],[0]
"Let x1, . . .",2. BO and Thompson Sampling,[0],[0]
",x|M| be D-dimensional feature vectors for the molecules in M and let DI = {(xi, yi) :",2. BO and Thompson Sampling,[0],[0]
i ∈,2. BO and Thompson Sampling,[0],[0]
"I} be a dataset with information about past evaluations, where I is a set with the indices of the molecules already evaluated, xi is the feature vector for the i-th molecule in M and yi = f(mi) is the result of evaluating the objective function f on that molecule.",2. BO and Thompson Sampling,[0],[0]
"We assume that the evaluations of f are noise free, however, the methods described here can be applied to the case in which the objective evaluations are corrupted with additive Gaussian noise.",2. BO and Thompson Sampling,[0],[0]
"BO typically uses a probabilistic model to describe how the yi in DI are generated as a function of the corresponding features xi and some model parameters θ, that is, the model specifies p(yi|xi,θ).",2. BO and Thompson Sampling,[0],[0]
"Given the data DI and a prior distribution p(θ), the model also specifies a posterior distribution p(θ|DI) ∝ p(θ) ∏ i∈I p(yi|xi,θ).",2. BO and Thompson Sampling,[0],[0]
The predictive distribution for any mj ∈M \ {mi : i ∈,2. BO and Thompson Sampling,[0],[0]
"I} is then given by p(yj |xj ,DI) = ∫ p(yj |xj ,θ)p(θ|DI) dθ.",2. BO and Thompson Sampling,[0],[0]
"BO methods use this predictive distribution to compute an acquisition function (AF) given by
α(xj |DI) = Ep(yj |xj ,DI) [U(yj |xj ,DI)] , (1)
where U(yj |xj ,DI) is the utility of obtaining value yj when evaluating f at mj .",2. BO and Thompson Sampling,[0],[0]
Eq.,2. BO and Thompson Sampling,[0],[0]
(1) is then maximized with respect to j 6∈ I to select the next molecule mj on which to evaluate f .,2. BO and Thompson Sampling,[0],[0]
"The most common choice for the utility is the improvement: U(yj |xj ,DI) = max(0, yj",2. BO and Thompson Sampling,[0],[0]
"− y?), where y? is equal to the best yi in DI .",2. BO and Thompson Sampling,[0],[0]
"In this case, Eq. (1) is called the expected improvement (EI) (Jones et al., 1998).",2. BO and Thompson Sampling,[0],[0]
"Ideally, the AF should encourage both exploration and exploitation.",2. BO and Thompson Sampling,[0],[0]
"For this, the expected utility should increase when yj takes high values on average (to exploit), but also when there is high uncertainty about yj (to explore).",2. BO and Thompson Sampling,[0],[0]
"The EI utility function satisfies these two requirements.
",2. BO and Thompson Sampling,[0],[0]
"Thompson sampling (TS) (Thompson, 1933) can be understood as a version of the previous framework in which the utility function is defined as U(yj |xj ,DI) = yj and the expectation in (1) is taken with respect to p(yj |xj ,θ)
instead of p(yj |xj ,DI), with θ being a sample from the posterior p(θ|DI).",2. BO and Thompson Sampling,[0],[0]
"That is, when computing the AF, TS approximates the integral in p(yj |xj ,DI) =∫ p(yj |xj ,θ)p(θ|DI) dθ by Monte Carlo, using a single sample from p(θ|DI) in the approximation.",2. BO and Thompson Sampling,[0],[0]
The TS utility function enforces only exploitation because the expected utility is insensitive to any variance in yj .,2. BO and Thompson Sampling,[0],[0]
"Despite this, TS still enforces exploration because of the variance produced by the Monte Carlo approximation to p(yj |xj ,DI).",2. BO and Thompson Sampling,[0],[0]
"Under TS, the probability of evaluating the objective at a particular location matches the probability of that location being the maximizer of the objective, given the model assumptions and the data from past evaluations.",2. BO and Thompson Sampling,[0],[0]
Algorithm 1 contains the pseudocode for TS.,2. BO and Thompson Sampling,[0],[0]
The plots in the top of Figure 1 illustrate how TS works.,2. BO and Thompson Sampling,[0],[0]
The top-left plot shows several samples from a posterior distribution on f induced by p(θ|DI) since each value of the parameters θ corresponds to an associated value of f .,2. BO and Thompson Sampling,[0],[0]
Sampling from p(θ|DI) is then equivalent to selecting one of these samples for f .,2. BO and Thompson Sampling,[0],[0]
"The selected sample represents the current AF, which is optimized in the top-right plot in Figure 1 to select the next evaluation.",2. BO and Thompson Sampling,[0],[0]
"So far we have considered the sequential evaluation setting, where BO methods collect just a single data point in each iteration.",2.1. Parallel BO,[0],[0]
"However, BO can also be applied in the parallel setting, which involves choosing a batch of multiple points to evaluate next in each iteration.",2.1. Parallel BO,[0],[0]
"For example, when we run S parallel simulations in a computer cluster and each simulation performs one evaluation of f .
",2.1. Parallel BO,[0],[0]
Snoek et al. (2012) describe how to extend sequential BO methods to the parallel setting.,2.1. Parallel BO,[0],[0]
The idea is to select the first evaluation location in the batch in the same way as in the sequential setting.,2.1. Parallel BO,[0],[0]
"However, the next evaluation location is then selected while the previous one is still pending.",2.1. Parallel BO,[0],[0]
"In particular, given a setK with indexes of pending evaluation locations, we choose a new location in the batch based on the expectation of the AF under all possible outcomes of the pending evaluations according to the predictions of the model.",2.1. Parallel BO,[0],[0]
"Therefore, at any point, the next evaluation location is obtained by optimizing the AF
αparallel(xj |DI ,K) =",2.1. Parallel BO,[0],[0]
"Ep({yk}k∈K|{xk}k∈K,DI) [α(xj |DI ∪ DK)] , (2)
where DK = {(yk,xk)}k∈K and α(xj |DI ∪ DK) is given by (1).",2.1. Parallel BO,[0],[0]
Computing this expression exactly is infeasible in most cases.,2.1. Parallel BO,[0],[0]
"Snoek et al. (2012) propose a Monte Carlo approximation in which the expectation in the second line is approximated by averaging across a few samples from the predictive distribution at the pending evaluations, that is, p({yk}k∈K|{xk}k∈K,DI).",2.1. Parallel BO,[0],[0]
"These samples are referred to as fantasized data.
",2.1. Parallel BO,[0],[0]
"This approach for parallel BO has been successfully used
to collect small batches of data (about 10 elements in size), with EI as utility function and with a Gaussian process as the model for the data (Snoek et al., 2012).",2.1. Parallel BO,[0],[0]
"However, it lacks scalability to large batch sizes, failing when we need to collect thousands of simultaneous measurements.",2.1. Parallel BO,[0],[0]
The reason for this is the high computational cost of adding a new evaluation to the current batch.,2.1. Parallel BO,[0],[0]
"The corresponding cost includes: 1 sampling the fantasized data, 2 updating the posterior predictive distribution to p(yj |xj ,DI ∪ DK), which is required for evaluating α(xj |DI ∪ DK), and 3 optimizing the Monte Carlo approximation to (2).",2.1. Parallel BO,[0],[0]
Step 2 can be very expensive when the number of training points inDI is very large.,2.1. Parallel BO,[0],[0]
"This step is also considerably challenging when the model does not allow for exact inference, as it is often the case with Bayesian neural networks.",2.1. Parallel BO,[0],[0]
"Step 3 can also take a very long time when the library of candidate moleculesM is very large (e.g., when it contains millions of elements) and among all the remaining molecules we have to find one that maximizes the AF.
",2.1. Parallel BO,[0],[0]
"Despite these difficulties, the biggest disadvantage in this approach for parallel BO is that it cannot be parallelized since it is a sequential process in which (2) needs to be iteratively optimized, with each optimization step having a direct effect on the next one.",2.1. Parallel BO,[0],[0]
This prevents this method from fully exploiting the acceleration provided by multiple processors in a computer cluster.,2.1. Parallel BO,[0],[0]
The sequential nature of the algorithm is illustrated by the plot in the left of Figure 2.,2.1. Parallel BO,[0],[0]
In this plot computer node 1 is controlling the BO process and decides the batch evaluation locations.,2.1. Parallel BO,[0],[0]
"Nodes 2, . . .",2.1. Parallel BO,[0],[0]
", 5 then perform the evaluations in parallel.",2.1. Parallel BO,[0],[0]
"Note that steps 2 and 3 from the above description have been highlighted in green and magenta colors.
",2.1. Parallel BO,[0],[0]
"In the following section we describe an algorithm for batch BO which can be implemented in a fully parallel and distributed manner and which, consequently, can take full advantage of multiple processors in a computer cluster.",2.1. Parallel BO,[0],[0]
"This novel method is based on a parallel implementation of the Thompson sampling heuristic.
",2.1. Parallel BO,[0],[0]
Algorithm 2 Parallel and distributed Thompson sampling Input: initial data DI(1) =,2.1. Parallel BO,[0],[0]
"{xi, yi}i∈I(1), batch size S for t = 1 to T do
Compute current posterior p(θ|DI(t)) for s = 1 to S",2.1. Parallel BO,[0],[0]
"do
Sample θ from p(θ|DI(t))",2.1. Parallel BO,[0],[0]
"Select k(s)← argmaxj 6∈I(t)E[yj |xj ,θ] Collect yk(s) by evaluating f at xk(s)
end for DI(t+1) = DI(t) ∪ {xk(s), yk(s)}Ss=1
end for
E xe
cu te d in pa ra
lle",2.1. Parallel BO,[0],[0]
"l
in no
de s",2.1. Parallel BO,[0],[0]
We present an implementation of the parallel BO method from Section 2.1 based on the Thompson sampling (TS) heuristic.,3. Parallel and Distributed Thompson Sampling,[0],[0]
"In particular, we propose to apply to (2) the same approximation that TS applied to (1).",3. Parallel and Distributed Thompson Sampling,[0],[0]
"For this, we choose in (2) the same utility function used by TS in the sequential setting, that is, U(yj |xj ,DI ∪DK) = yj .",3. Parallel and Distributed Thompson Sampling,[0],[0]
"Then, we approximate the expectation with respect to {yk}k∈K in (2) by Monte Carlo, averaging across just one sample of {yk}k∈K drawn from p({yk}k∈K|{xk}k∈K,DI).",3. Parallel and Distributed Thompson Sampling,[0],[0]
"After that, α(xj |DI ∪ DK) in (2) is approximated in the same way as in the sequential setting by first sampling θ from p(θ|DI ∪DK) and then approximating p(yj |xj ,DI ∪DK) with p(yj |xj ,θ).",3. Parallel and Distributed Thompson Sampling,[0],[0]
"Importantly, in this process, sampling first {yk}k∈K from p({yk}k∈K|{xk}k∈K,DI) and then θ from p(θ|DI ∪ DK) is equivalent to sampling θ from just p(θ|DI).",3. Parallel and Distributed Thompson Sampling,[0],[0]
The reason for this is that updating a posterior distribution with synthetic data sampled from the model’s predictive distribution produces on average the same initial posterior distribution.,3. Parallel and Distributed Thompson Sampling,[0],[0]
"The result is that parallel TS with batch size S is the same as running sequential TS S times without updating the current posterior p(θ|DI), where each execution of sequential TS produces one of the evaluation locations in the batch.",3. Parallel and Distributed Thompson Sampling,[0],[0]
"Importantly, these executions can be done in distributed manner, with each one running in parallel in a different node.
",3. Parallel and Distributed Thompson Sampling,[0],[0]
The resulting parallel and distributed TS (PDTS) method is highly scalable and can be applied to very large batch sizes by running each execution of sequential TS on the same computer node that will then later evaluate f at the selected evaluation location.,3. Parallel and Distributed Thompson Sampling,[0],[0]
Algorithm 2 contains the pseudocode for PDTS.,3. Parallel and Distributed Thompson Sampling,[0],[0]
The parallel nature of the algorithm is illustrated by the plot in the right of Figure 2.,3. Parallel and Distributed Thompson Sampling,[0],[0]
In this plot computer node 1 is controlling the BO process.,3. Parallel and Distributed Thompson Sampling,[0],[0]
"To collect four new function evaluations in parallel, computer node 1 sends the current posterior p(θ|DI) and I to nodes 2, . . .",3. Parallel and Distributed Thompson Sampling,[0],[0]
", 5.",3. Parallel and Distributed Thompson Sampling,[0],[0]
"Each of them samples then a value for θ from the posterior and optimizes its own AF given by E[yj |xj ,θ], with j 6∈",3. Parallel and Distributed Thompson Sampling,[0],[0]
I.,3. Parallel and Distributed Thompson Sampling,[0],[0]
The objective function is evaluated at the selected input and the resulting data is sent back to node 1.,3. Parallel and Distributed Thompson Sampling,[0],[0]
"Figure 1 illustrates
how PDTS selects two parallel evaluation locations.",3. Parallel and Distributed Thompson Sampling,[0],[0]
"For this, sequential TS is run twice.
",3. Parallel and Distributed Thompson Sampling,[0],[0]
The scalability of PDTS makes it a promising method for parallel BO in high-throughput screening.,3. Parallel and Distributed Thompson Sampling,[0],[0]
"However, in this type of problem, the optimization of the AF is done over a discrete set of molecules.",3. Parallel and Distributed Thompson Sampling,[0],[0]
"Therefore, whenever we collect a batch of data in parallel with PDTS, several of the simultaneous executions of sequential TS may choose to evaluate the same molecule.",3. Parallel and Distributed Thompson Sampling,[0],[0]
A central computer node (e.g. the node controlling the BO process) maintaining a list of molecules currently selected for evaluation can be used to avoid this problem.,3. Parallel and Distributed Thompson Sampling,[0],[0]
"In this case, each sequential TS node sends to the central node a ranked list with the top S (the batch size) molecules according to its AF.",3. Parallel and Distributed Thompson Sampling,[0],[0]
"From this list, the central node then selects the highest ranked molecule that has not been selected for evaluation before.",3. Parallel and Distributed Thompson Sampling,[0],[0]
"Ginsbourger et al. (Ginsbourger et al., 2010) proposed the following framework for parallel BO: given a set of current observations DI and pending experiments {xk}Kk=1, an additional set of fantasies DK = {(xk, yk)}Kk=1 can be assumed to be the result of those pending experiments.",4. Related Work,[0],[0]
A step of Bayesian optimization can then be performed using the augmented dataset DI ∪ DK and the acquisition function α(x|DI ∪DK).,4. Related Work,[0],[0]
"Two different values are proposed for the fantasies: the constant liar, where yk = L for some constant L and all k = 1 . .",4. Related Work,[0],[0]
".K, and the Kriging believer, where yk is given by the GP",4. Related Work,[0],[0]
"predictive mean at xk.
Snoek et al. (2012) compute a Monte Carlo approximation of the expected acquisition function over potential fantasies sampled from the model’s predictive distribution.",4. Related Work,[0],[0]
"Recent methods have been proposed to modify the parallel EI procedure to recommend points jointly (Chevalier & Ginsbourger, 2013; Marmin et al., 2015; Wang et al., 2016).
",4. Related Work,[0],[0]
Azimi et al. (2010) describe a procedure called simulated matching whose goal is to propose a batch DK of points which is a good match for the set of samples that a sequential BO policy π would recommend.,4. Related Work,[0],[0]
"The authors consider a batch “good” if it contains a sample that yields, with high
probability, an objective value close to that of the best sample produced by a sequential execution of π.
",4. Related Work,[0],[0]
Several authors have proposed to extend the upper confidence bound (UCB) heuristic to the parallel setting.,4. Related Work,[0],[0]
"Since the GP predictive variance depends only on the input location of the observations, Desautels et al. (2014) propose GP-BUCP acquisition which uses the UCB acquisition with this updated variance.",4. Related Work,[0],[0]
Contal et al. (2013) introduce the Gaussian Process Upper Confidence Bound with Pure Exploration (GP-UCB-PE).,4. Related Work,[0],[0]
"Under this procedure, the first point is obtained using the standard UCB acquisition function while the remaining points are sequentially selected to be the ones yielding the highest predictive variance, while still lying in a region that contains the maximizer with high probability.
",4. Related Work,[0],[0]
Shah & Ghahramani (2015) extend the Predictive Entropy Search (PES) heuristic to the parallel setting (PPES).,4. Related Work,[0],[0]
"PPES seeks to recommend a collection of samplesDK that yields the greatest reduction in entropy for the posterior distribution of x?, the latent objective maximizer.",4. Related Work,[0],[0]
"Wu & Frazier (2016) propose the Parallel Knowledge Gradient Method which optimizes an acquisition function called the parallel knowledge gradient (q-KG), a measure of the expected incremental solution quality after q samples.
",4. Related Work,[0],[0]
An advantage of PDTS over parallel EI and other related methods is that the approximate marginalization of potential experimental outcomes adds no extra computational cost to our procedure and so PDTS is highly parallelizable.,4. Related Work,[0],[0]
"Finally, unlike other approaches, PDTS can be applied to a wide variety of models, such as GPs and Bayesian neural networks, since it only requires samples from an exact or approximate posterior distribution.",4. Related Work,[0],[0]
Neural networks are well-suited for implementing BO on molecules.,5. Bayesian Neural Networks for High-throughput Screening,[0],[0]
"They produce state-of-the-art predictions of chemical properties (Ma et al., 2015; Mayr et al., 2016; Ramsundar et al., 2015) and can be applied to large data sets by using stochastic optimization (Bousquet & Bottou, 2008).",5. Bayesian Neural Networks for High-throughput Screening,[0],[0]
Typical applications of neural networks focus on the deterministic prediction scenario.,5. Bayesian Neural Networks for High-throughput Screening,[0],[0]
"However, in large search spaces with multiple local optima (which is the case when navigating chemical space), it is desirable to use a probabilistic approach that can produce accurate estimates of uncertainty for efficient exploration and so, we use probabilistic back-propagation (PBP), a recently-developed technique for the scalable training of Bayesian neural networks (Hernández-Lobato & Adams, 2015).",5. Bayesian Neural Networks for High-throughput Screening,[0],[0]
"Note that other methods for approximate inference in Bayesian neural networks could have been chosen as well (Blundell et al., 2015; Snoek et al., 2015; Gal
& Ghahramani, 2016).",5. Bayesian Neural Networks for High-throughput Screening,[0],[0]
"We prefer PBP because it is fast and it does not require the tuning of hyper-parameters such as learning rates or regularization constants (HernándezLobato & Adams, 2015).
",5. Bayesian Neural Networks for High-throughput Screening,[0],[0]
"Given a dataset DI = {(xi, yi)}i∈I , we assume that yi = f(xi;W) +",5. Bayesian Neural Networks for High-throughput Screening,[0],[0]
"i, where f(·;W) is the output of a neural network with weights W .",5. Bayesian Neural Networks for High-throughput Screening,[0],[0]
"The network output is corrupted with additive noise variables i ∼ N (0, γ−1).",5. Bayesian Neural Networks for High-throughput Screening,[0],[0]
"The network has L layers, with Vl hidden units in layer l, and W = {Wl}Ll=1 is the collection of Vl×(Vl−1+1) synaptic weight matrices.",5. Bayesian Neural Networks for High-throughput Screening,[0],[0]
The +1 is introduced here to account for the additional per-layer biases.,5. Bayesian Neural Networks for High-throughput Screening,[0],[0]
"The activation functions for the hidden layers are rectifiers: ϕ(x) = max(x, 0).
",5. Bayesian Neural Networks for High-throughput Screening,[0],[0]
"The likelihood for the network weights W and the noise precision γ is
p({yi}i∈|I |W, {xi}i∈I , γ) =",5. Bayesian Neural Networks for High-throughput Screening,[0],[0]
"∏ i∈I N (yi|f(xi;W), γ−1) .
",5. Bayesian Neural Networks for High-throughput Screening,[0],[0]
"We specify a Gaussian prior distribution for each entry in each of the weight matrices inW:
p(W|λ) = L∏ l=1 Vl∏ k=1 Vl−1+1∏ j=1 N (wkj,l|0, λ−1) , (3)
where wkj,l is the entry in the k-th row and j-th column of Wl and λ is a precision parameter.",5. Bayesian Neural Networks for High-throughput Screening,[0],[0]
The hyper-prior for λ is gamma: p(λ),5. Bayesian Neural Networks for High-throughput Screening,[0],[0]
"= Gam(λ|αλ0 , βλ0 ) with shape αλ0 = 6 and inverse scale βλ0 = 6.",5. Bayesian Neural Networks for High-throughput Screening,[0],[0]
This relatively low value for the shape and inverse scale parameters makes this prior weakly-informative.,5. Bayesian Neural Networks for High-throughput Screening,[0],[0]
The prior for the noise precision γ is also gamma: p(γ) =,5. Bayesian Neural Networks for High-throughput Screening,[0],[0]
"Gam(γ|αγ0 , β γ 0 ).",5. Bayesian Neural Networks for High-throughput Screening,[0],[0]
"We assume that the yi have been normalized to have unit variance and, as above, we fix αγ0 = 6 and β γ 0 = 6.
",5. Bayesian Neural Networks for High-throughput Screening,[0],[0]
"The exact computation of the posterior distribution for the model parameters p(W, γ, λ|DI) is not tractable in most cases.",5. Bayesian Neural Networks for High-throughput Screening,[0],[0]
"PBP approximates the intractable posterior onW , γ and λ with the tractable approximation
q(W, γ, λ) =  L∏ l=1 Vl∏ k=1 Vl−1+1∏ j=1 N (wkj,l|mkj,l, vkj,l)  ",5. Bayesian Neural Networks for High-throughput Screening,[0],[0]
"Gam(γ |αγ , βγ)Gam(λ |αλ, βλ) , (4)
whose parameters are tuned by iteratively running an assumed density filtering (ADF) algorithm over the training data (Opper, 1998).",5. Bayesian Neural Networks for High-throughput Screening,[0],[0]
"The main operation in PBP is the update of the mean and variance parameters of q, that is, the mkj,l and vkj,l in (4), after processing each data point {(xi, yi)}.",5. Bayesian Neural Networks for High-throughput Screening,[0],[0]
"For this, PBP matches moments between the new q and the product of the old q with the corresponding likelihood factorN (yi | f(xi;W), γ−1).",5. Bayesian Neural Networks for High-throughput Screening,[0],[0]
"The matching of moments for the distributions on the weights is achieved
by using well-known Gaussian ADF updates, see equations 5.12 and 5.1 in (Minka, 2001).
",5. Bayesian Neural Networks for High-throughput Screening,[0],[0]
"To compute the ADF updates, PBP finds a Gaussian approximation to the distribution of the network output f(xi;W) when W ∼ q. This is achieved by doing a forward pass of xi through the network, with the weights W being randomly sampled from q.",5. Bayesian Neural Networks for High-throughput Screening,[0],[0]
In this forward pass the non-Gaussian distributions followed by the output of the neurons are approximated with Gaussians that have the same means and variances as the original distributions.,5. Bayesian Neural Networks for High-throughput Screening,[0],[0]
This is a Gaussian approximation by moment matching.,5. Bayesian Neural Networks for High-throughput Screening,[0],[0]
"We refer the reader to Hernández-Lobato & Adams (2015) for full details on PBP.
",5. Bayesian Neural Networks for High-throughput Screening,[0],[0]
"After several ADF iterations over the data by PBP, we can then make predictions for the unknown target variable y?",5. Bayesian Neural Networks for High-throughput Screening,[0],[0]
associated with a new feature vector x?.,5. Bayesian Neural Networks for High-throughput Screening,[0],[0]
"For this, we obtain a Gaussian approximation to f(x?;W)",5. Bayesian Neural Networks for High-throughput Screening,[0],[0]
"when W ∼ q by applying the forward pass process described above.
To implement TS, as described in Algorithm 1, we first sample the model parameters θ from the posterior p(θ|DI) and then optimize the AF given by E[yj |xj ,θ], with j 6∈ I. When the model is a Bayesian neural network trained with PBP, the corresponding operations are sampling W from q and then optimizing the AF given by f(xj ;W), with j 6∈ I.",5. Bayesian Neural Networks for High-throughput Screening,[0],[0]
"This last step requires the use of a deterministic neural network, with weight values given by the posterior sample from q, to make predictions on all the molecules that have not been evaluated yet.",5. Bayesian Neural Networks for High-throughput Screening,[0],[0]
"Then, the molecule with highest predictive value is selected for the next evaluation.",5. Bayesian Neural Networks for High-throughput Screening,[0],[0]
We first compare the performance of our parallel and distributed Thompson sampling (PDTS) algorithm with the most popular approach for parallel BO: the parallel EI method from Section 2.1.,6. Experiments with GPs and Parallel EI,[0],[0]
Existing implementations of parallel EI such as spearmint1 use a Gaussian process (GP) model for the objective function.,6. Experiments with GPs and Parallel EI,[0],[0]
"To compare with these methods, we also adopt a GP as the model in PDTS.",6. Experiments with GPs and Parallel EI,[0],[0]
Note that parallel EI cannot scale to the large batch sizes used in high-throughput screening.,6. Experiments with GPs and Parallel EI,[0],[0]
"Therefore, we consider here only parallel optimization problems with small batch sizes and synthetic objective functions.",6. Experiments with GPs and Parallel EI,[0],[0]
"Besides PDTS and parallel EI, we also analyze the performance of the sequential versions of these algorithms: TS and EI.
To implement Thompson sampling (TS) with a GP model, we approximate the non-parametric GP with a parametric approximation based on random features, as described in the supplementary material of (Hernández-Lobato et al., 2014).",6. Experiments with GPs and Parallel EI,[0],[0]
"For the experiments, we consider a cluster with 11
1https://github.com/HIPS/Spearmint
nodes: one central node for controlling the BO process and 10 additional nodes for parallel evaluations.",6. Experiments with GPs and Parallel EI,[0],[0]
We assume that all objective evaluations take a very large amount of time and that the cost of training the GPs and recomputing and optimizing the AF is negligible in comparison.,6. Experiments with GPs and Parallel EI,[0],[0]
"Thus, in practice, we perform these experiments in a sequential (non-parallel) fashion with the GP model being updated only in blocks of 10 consecutive data points at a time.
",6. Experiments with GPs and Parallel EI,[0],[0]
"As objective functions we consider the two dimensional Bohachevsky and Branin-Hoo functions and the six dimensional Hartmann function, all available in Benchfunk2.",6. Experiments with GPs and Parallel EI,[0],[0]
We also consider the optimization of functions sampled from the GP prior over the 2D unit square using a squared exponential covariance function with fixed 0.1 length scale.,6. Experiments with GPs and Parallel EI,[0],[0]
"After each objective evaluation, we compute the immediate regret (IR), which we define as the difference between the best objective value obtained so far and the minimum value of the objective function.",6. Experiments with GPs and Parallel EI,[0],[0]
"The measurement noise is zero in these experiments.
",6. Experiments with GPs and Parallel EI,[0],[0]
"Figure 3 reports mean and standard errors for the logarithm of the best IR seen so far, averaged across 50 repetitions of the experiments.",6. Experiments with GPs and Parallel EI,[0],[0]
"In the plots, the horizontal axis shows the number of function evaluations performed so far.",6. Experiments with GPs and Parallel EI,[0],[0]
"Note that in these experiments TS and EI update their GPs once per sample, while PDTS and parallel EI update only every 10 samples.",6. Experiments with GPs and Parallel EI,[0],[0]
"Figure 3 shows that EI is better than TS in most cases, although the differences between these two methods are small in the Branin-Hoo function.",6. Experiments with GPs and Parallel EI,[0],[0]
"However, EI is considerably much better than TS in Hartmann.",6. Experiments with GPs and Parallel EI,[0],[0]
The reason for this is that in Hartmann there are multiple equivalent global minima and TS tends to explore all of them.,6. Experiments with GPs and Parallel EI,[0],[0]
EI is by contrast more exploitative and focuses on evaluating the objective around only one of the minima.,6. Experiments with GPs and Parallel EI,[0],[0]
"The differences between parallel EI and PDTS are much smaller, with both obtaining very similar results.",6. Experiments with GPs and Parallel EI,[0],[0]
"The exception is again Hartmann, where parallel EI is much better than PDTS, probably because PDTS is more explorative than parallel EI.",6. Experiments with GPs and Parallel EI,[0],[0]
"Interestingly, PDTS performs better than parallel EI on the random samples from the GP prior, although parallel EI eventually catches up.
",6. Experiments with GPs and Parallel EI,[0],[0]
"These results indicate that PDTS performs in practice very similarly to parallel EI, one of the most popular methods for parallel BO.",6. Experiments with GPs and Parallel EI,[0],[0]
We describe the molecule data sets used in our experiments.,7. Experiments with Molecule Data Sets,[0],[0]
"The input features for all molecules are 512-bit Morgan circular fingerprints (Rogers & Hahn, 2010), calculated with a bond radius of 2, and derived from the canonical SMILES as implemented in the RDkit package (Landrum).
",7. Experiments with Molecule Data Sets,[0],[0]
"2https://github.com/mwhoffman/benchfunk
Harvard Clean Energy Project: The Clean Energy Project is the world’s largest materials high-throughput virtual screening effort (Hachmann et al., 2014; 2011), and has scanned more than 3.5 million molecules to find those with high power conversion efficiency (PCE) using quantum-chemical techniques, taking over 30,000 years of CPU time.",7. Experiments with Molecule Data Sets,[0],[0]
"The target value within this data set is the power conversion efficiency (PCE), which is calculated for the 2.3 million publicly released molecules, using the Scharber model (Dennler et al., 2008) and frontier orbitals calculated at the BP86 (Perdew, 1986; Becke, 1993)",7. Experiments with Molecule Data Sets,[0],[0]
"def2-SVP (Weigend & Ahlrichs, 2005) level of theory.
",7. Experiments with Molecule Data Sets,[0],[0]
Dose-Response Data Set: These data sets were obtained from the NCI-cancer database (Many authors).,7. Experiments with Molecule Data Sets,[0],[0]
"The doseresponse target value has a potential range of -100 to 100, and reports a percentage cell growth relative to a no-drug control.",7. Experiments with Molecule Data Sets,[0],[0]
"Thus, a value of +40 would correspond to a 60% growth inhibition and a value of -40 would correspond to 40% lethality.",7. Experiments with Molecule Data Sets,[0],[0]
"Molecules with a positive value for the dose-response are known as inhibitors, molecules with a score less than 0 have a cytotoxic effect.",7. Experiments with Molecule Data Sets,[0],[0]
Results against the NCI-H23 cell line were taken against a constant logconcentration of -8.00M and where multiple identical conditions were present in the data an average was used for the target variables.,7. Experiments with Molecule Data Sets,[0],[0]
"In this data set we are interested in finding molecules with smaller values of the target variable.
",7. Experiments with Molecule Data Sets,[0],[0]
"Malaria Data Set: The Malaria data set was taken from the P. falciparum whole cell screening derived by combining the GSK TCAMS data set, the Novatis-GNF Malaria Box data set and the St Jude’s Research Hospital data set, as released through the Medicines for Malaria Venture website (Spangenberg et al., 2013).",7. Experiments with Molecule Data Sets,[0],[0]
"The target variable is the EC50 value, which is defined as the concentration of the drug which gives half maximal response.",7. Experiments with Molecule Data Sets,[0],[0]
"Much like the Dose response data set, the focus here is on minimization: the lower the concentration, the stronger the drug.",7. Experiments with Molecule Data Sets,[0],[0]
We evaluate the gains produced by PDTS in experiments simulating a high throughput virtual screening setting.,7.1. Results,[0],[0]
"In these experiments, we sequentially sample molecules from
libraries of candidate molecules given by the data sets from Section 7.",7.1. Results,[0],[0]
"After each sampling step, we calculate the 1% recall, that is, the fraction of the top 1% of molecules from the original library that are found among the sampled ones.",7.1. Results,[0],[0]
"For the CEP data, we compute recall by focusing on molecules with PCE larger than 10%.",7.1. Results,[0],[0]
"In all data sets, each sampling step involves selecting a batch of molecules among those that have not been sampled so far.",7.1. Results,[0],[0]
In the Malaria and One-dose data sets we use batches of size 200.,7.1. Results,[0],[0]
"These data sets each contain about 20,000 molecules.",7.1. Results,[0],[0]
"By contrast, the CEP data set contains 2 million molecules.",7.1. Results,[0],[0]
"In this latter case, we use batches of size 500.",7.1. Results,[0],[0]
"We use Bayesian neural networks with one hidden layer and 100 hidden units.
",7.1. Results,[0],[0]
We compare the performance of PDTS with two baselines.,7.1. Results,[0],[0]
"The first one, greedy, is a sampling strategy that only considers exploitation and does not perform any exploration.",7.1. Results,[0],[0]
We implement this approach by selecting molecules according to the average of the probabilistic predictions generated by PBP.,7.1. Results,[0],[0]
"That is, the greedy approach ignores any variance in the predictions of the Bayesian neural network and generates batches by just ranking molecules according to the mean of the predictive distribution given by PBP.",7.1. Results,[0],[0]
The second baseline is a Monte Carlo approach in which the batches of molecules are selected uniformly at random.,7.1. Results,[0],[0]
"These two baselines are comparable to PDTS in that they can be easily implemented in a large scale setting in which the library of candidate molecules contains millions of elements and data is sampled using large batch sizes.
",7.1. Results,[0],[0]
"In the Malaria and One-dose data sets, we average across 50 different realizations of the experiments.",7.1. Results,[0],[0]
"This is not possible in the CEP data set, which is 100 times larger than the two other data sets.",7.1. Results,[0],[0]
"In the CEP case, we report results for a single realization of the experiment (in a second realization we obtained similar results).",7.1. Results,[0],[0]
Figure 4 shows the recall obtained by each method in the molecule data sets.,7.1. Results,[0],[0]
"PDTS significantly outperforms the Monte Carlo approach, and also offers better performance than greedy sampling.",7.1. Results,[0],[0]
"This shows the importance of building in exploration into the sampling strategy, rather than relying on purely exploitative methods.",7.1. Results,[0],[0]
"The greedy approach performs best in the
CEP data set.",7.1. Results,[0],[0]
"In this case, the greedy strategy initially finds better molecules than PDTS, but after a while PDTS overtakes, probably because a promising area of chemical space initially discovered by the greedy approach starts to become exhausted.
",7.1. Results,[0],[0]
The previous results allow us to consider the savings produced by BO.,7.1. Results,[0],[0]
"In the CEP data set, PDTS achieves about 20 times higher recall values than the Monte Carlo approach, which is comparable to the exhaustive enumeration that was used to collect the CEP data.",7.1. Results,[0],[0]
"We estimate that, with BO, the CEP virtual screening process would have taken 1,500 CPU years instead of the 30,000 that were actually used.",7.1. Results,[0],[0]
"Regarding the One-dose and Malaria data sets, PDTS can locate in both sets about 70% of the top 1% molecules by sampling approximately 6,000 molecules.",7.1. Results,[0],[0]
"By contrast, the Monte Carlo approach would require sampling 14,000 molecules.",7.1. Results,[0],[0]
This represents a significant reduction in the discovery time for new therapeutic molecules and savings in the economic costs associated with molecule synthesis and testing.,7.1. Results,[0],[0]
We can easily modify the greedy baseline from the previous section to include some amount of exploration by replacing a small fraction of the molecules in each batch with molecules chosen uniformly at random.,7.2. Comparison with -greedy Approaches,[0],[0]
"This approach is often called -greedy (Watkins, 1989), where the variable indicates the fraction of molecules that are sampled uniformly at random.",7.2. Comparison with -greedy Approaches,[0],[0]
"The disadvantage of the -greedy approach is that it requires the tuning of to the problem of interest whereas the amount of exploration is automatically set by PDTS.
",7.2. Comparison with -greedy Approaches,[0],[0]
"We compared PDTS with different versions of -greedy in the same way as above, using = 0.01, 0.025, 0.05 and 0.075.",7.2. Comparison with -greedy Approaches,[0],[0]
The experiments with the One-dose and the Malaria data sets are similar to the ones done before.,7.2. Comparison with -greedy Approaches,[0],[0]
"However,
we now sub-sample the CEP data set to be able to average across 50 different realizations of the experiment: we choose 4,000 molecules uniformly at random and then collect data in batches of size 50 across 50 different repetitions of the screening process.",7.2. Comparison with -greedy Approaches,[0],[0]
We compute the average rank obtained by each method across the 3 × 50 = 150 simulated screening experiments.,7.2. Comparison with -greedy Approaches,[0],[0]
"A ranking equal to 1 indicates that the method always obtains the highest recall at the end of the experiment, while a ranking equal to 5 indicates that the method always obtains the worst recall value.",7.2. Comparison with -greedy Approaches,[0],[0]
"Table 1 shows that the lowest average rank is obtained by PDTS, which achieves better exploration-exploitation trade-offs than the -greedy approaches.",7.2. Comparison with -greedy Approaches,[0],[0]
"We have presented a Parallel and Distributed implementation of Thompson Sampling (PDTS), a highly scalable method for parallel Bayesian optimization.",8. Conclusions,[0],[0]
PDTS can be applied when scalability limits the applicability of competing approaches.,8. Conclusions,[0],[0]
We have evaluated the performance of PDTS in experiments with both Gaussian process and probabilistic neural networks.,8. Conclusions,[0],[0]
We show that PDTS compares favorably with parallel EI in problems with small batch sizes.,8. Conclusions,[0],[0]
We also demonstrate the effectiveness of PDTS on large scale real world applications that involve searching chemical space for new molecules wit improved properties.,8. Conclusions,[0],[0]
"We show that PDTS outperforms other scalable approaches on these applications, in particular, a greedy search strategy, -greedy approaches and a random search method.",8. Conclusions,[0],[0]
J.M.H.L. acknowledges support from the Rafael del Pino Foundation.,Acknowledgements,[0],[0]
The authors thank Ryan P. Adams for useful discussions.,Acknowledgements,[0],[0]
A.A.-G. and E.O.P.-K. acknowledge the Department of Energy Program on Theory and modeling through grant DE-SC0008733.,Acknowledgements,[0],[0]
Chemical space is so large that brute force searches for new interesting molecules are infeasible.,abstractText,[0],[0]
"High-throughput virtual screening via computer cluster simulations can speed up the discovery process by collecting very large amounts of data in parallel, e.g., up to hundreds or thousands of parallel measurements.",abstractText,[0],[0]
Bayesian optimization (BO) can produce additional acceleration by sequentially identifying the most useful simulations or experiments to be performed next.,abstractText,[0],[0]
"However, current BO methods cannot scale to the large numbers of parallel measurements and the massive libraries of molecules currently used in high-throughput screening.",abstractText,[0],[0]
"Here, we propose a scalable solution based on a parallel and distributed implementation of Thompson sampling (PDTS).",abstractText,[0],[0]
"We show that, in small scale problems, PDTS performs similarly as parallel expected improvement (EI), a batch version of the most widely used BO heuristic.",abstractText,[0],[0]
"Additionally, in settings where parallel EI does not scale, PDTS outperforms other scalable baselines such as a greedy search, -greedy approaches and a random search method.",abstractText,[0],[0]
These results show that PDTS is a successful solution for large-scale parallel BO.,abstractText,[0],[0]
Parallel and Distributed Thompson Sampling for Large-scale Accelerated Exploration of Chemical Space,title,[0],[0]
"A wide range of data mining, machine learning and social network analysis problems can be modeled as graph mining tasks on large graphs.",1. Introduction,[0],[0]
The ability to analyze layers of connectivity is useful to understand the hierarchical structure of the input data and the role of nodes in different networks.,1. Introduction,[0],[0]
"A commonly used technique for this task is the k-core decomposition: a k-core of a graph is a maximal subgraph where every node has induced degree at least k. k-core decomposition has many real world applications from understanding dynamics in social networks (Bhawalkar et al., 2012) to graph visualization (Alvarez-Hamelin et al., 2005), from describing protein functions based on protein-protein networks (Altaf-Ul-Amin et al., 2006) to computing network centrality measures (Healy et al., 2006).",1. Introduction,[0],[0]
"k-core is also widely used as a sub-routine for community detection algorithms (Chester et al., 2012; Mitzenmacher et al., 2015) or for finding dense clusters in graphs (Lee et al., 2010; Mitzenmacher et al., 2015).",1. Introduction,[0],[0]
"As a graph theoretic tool k-core decomposition has been used to solve the densest subgraph
1Google Research.",1. Introduction,[0],[0]
"Correspondence to: Hossein Esfandiari <esfandiari@googol.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
problem (Lee et al., 2010; Bahmani et al., 2012; Epasto et al., 2015; Esfandiari et al., 2015).
",1. Introduction,[0],[0]
"k-core is often use a feature in machine learning systems with applications in network analysis, spam detection and biology.",1. Introduction,[0],[0]
"Furthermore, in comparison with other densitybased measure as the densest subgraph, it has the advantage to assign a score to every node in the network.",1. Introduction,[0],[0]
"Finally, the k-core decomposition induce a hierarchical clustering on the entire network.",1. Introduction,[0],[0]
"For many applications in machine learning and in data mining, it is important to be able to compute it efficiently on large graphs.
",1. Introduction,[0],[0]
"In the past decade, with increasing size of data sets available in various applications, the need for developing scalable algorithms has become more important.",1. Introduction,[0],[0]
"By definition, the process of computing k-core decomposition is sequential: in order to find the k-core, one can keep removing all nodes of degree less than k from the remaining graph until there is no such a node.",1. Introduction,[0],[0]
"As a result, computing k-cores for big graphs in distributed systems is a challenging task.",1. Introduction,[0],[0]
"In fact, while k-core decomposition has been studied extensively in the literature and many efficient decentralized and streaming heuristics have been developed for this problem (Montresor et al., 2013; Sarayuce et al., 2015), nevertheless developing a distributed or a streaming algorithm with provable guarantees for k-core decomposition problem remains an unsolved problem.",1. Introduction,[0],[0]
"One difficulty in tackling the problem is that simple non-adaptive sampling techniques used for similar problems as densest subgraph (Lee et al., 2010; Esfandiari et al., 2015; Bahmani et al., 2012; Epasto et al., 2015; Bhattacharya et al., 2016) do not work here (See Related Work for details).",1. Introduction,[0],[0]
"In this paper, we tackle this problem and present the first parallel and streaming algorithm for this problem with provable approximation guarantee.",1. Introduction,[0],[0]
"We do so by defining an approximate notion of k-core, and providing an adaptive space-efficient sketching technique that can be used to compute an approximate k-core decomposition efficiently.",1. Introduction,[0],[0]
"Roughly speaking, a 1− -approximate k-core is an induced subgraph that includes the k-core, and such that the induced degree of every node is at least (1− )k.
Our Contributions.",1. Introduction,[0],[0]
"As a foundation to all our results, we provide a powerful sketching technique to compute a 1− - approximate k-core for all k simultaneously.",1. Introduction,[0],[0]
"Our sketch is adaptive in nature and it is based on a novel iterative
edge sampling strategy.",1. Introduction,[0],[0]
"In particular, we design a sketch of size Õ(n) that can be constructed in O(log n) rounds of sampling1.
",1. Introduction,[0],[0]
We then show the first application of our sketching technique in designing a parallel algorithm for computing the k-core decomposition.,1. Introduction,[0],[0]
"More precisely, we present a MapReducebased algorithm to compute a 1 − approximate k-core decomposition of a graph in O(log n) rounds of computations, where the load of each machine is Õ(n), for any ∈ (0, 1].
",1. Introduction,[0],[0]
"Moreover, we show that one can implement our sketch for k-core decomposition in a streaming setting in one pass using only Õ(n) space.",1. Introduction,[0],[0]
"In particular, we present a one-pass streaming algorithm for 1− -approximate k-core decomposition of graphs with Õ(n) space.
",1. Introduction,[0],[0]
"Finally, we show experimentally the efficiency and accuracy of our sketching algorithm on few real world networks.
",1. Introduction,[0],[0]
Related Work.,1. Introduction,[0],[0]
The k-core decomposition problem is related to the densest subgraph problem.,1. Introduction,[0],[0]
"Streaming and turnstile algorithms for the densest subgrpah problem have been studied extensively in the past (Lee et al., 2010; Esfandiari et al., 2015; Bahmani et al., 2012; Epasto et al., 2015; Bhattacharya et al., 2016).",1. Introduction,[0],[0]
"While these problems are related, the theoretical results known for the densest subgraph problem are not directly applicable to the k-core decomposition problem.
",1. Introduction,[0],[0]
There are two types of algorithms for the densest subgraph problem in the streaming.,1. Introduction,[0],[0]
"First type of algorithms simulates the process of iteratively removing vertices with small degrees (Bahmani et al., 2012; Epasto et al., 2015; Bhattacharya et al., 2016).",1. Introduction,[0],[0]
"All of these results are based on the fact that we only need logarithmic rounds of probing to find a 1/2 approximation of the densest subgraph (Bahmani et al., 2012).",1. Introduction,[0],[0]
"However, this can not be used to 1− approximate the k-coreness numbers.
",1. Introduction,[0],[0]
"The second type of algorithms do a (non-adaptive) single pass and use uniform samplings of edges (Esfandiari et al., 2015; Mitzenmacher et al., 2015; McGregor et al., 2015).",1. Introduction,[0],[0]
"These results are based on the fact that the density of the optimum solution is proportional to the sampling rate with high probability, where the probability of failure is exponentially small.",1. Introduction,[0],[0]
There are two obstacles toward applying this approach to approximating a k-core decomposition.,1. Introduction,[0],[0]
"First, by using uniform sampling it is not possible to obtain a (1− ) approximation of the coreness number for nodes of constant degree (unless we do not sample all edges with probability one).",1. Introduction,[0],[0]
"Second, in order to achieve Õ(n) space, we can only sample Õ(1) edges per vertex.",1. Introduction,[0],[0]
"Hence, the
1In the paper, we use the notation Õ(·) to denote the fact that poly-logarithmic factors are ignored.
probability that the degree of a vertex in the sampled is not proportional to the sampling rate, is not exponentially small anymore.",1. Introduction,[0],[0]
Therefore it is not possible to union bound over exponentially many feasible solutions.,1. Introduction,[0],[0]
"To overcome this issue, we analyze the combinatorial correlation between feasible solutions and wisely pick polynomially many feasible solutions that approximate all of the feasible solutions.",1. Introduction,[0],[0]
"To the best of our knowledge this is the first work that analyzes the combinatorial correlation of different feasible solution on a graph.
",1. Introduction,[0],[0]
"In recent years the k-core decomposition problem received a lot of attention (Bhawalkar et al., 2012; Montresor et al., 2013; Aksu et al., 2014; Sarayuce et al., 2015; Zhang et al., 2017), nevertheless we do not know of any previous distributed algorithms with small bounded memory and number of rounds.",1. Introduction,[0],[0]
"A recent related paper is (Sarayuce et al., 2015) where the authors present a streaming algorithm for the kcore decomposition problem.",1. Introduction,[0],[0]
"While the authors report good empirical results for their algorithm, they do not provide a guarantee for this problem, e.g., they do not prove an upper bound on the memory complexity of this algorithm.",1. Introduction,[0],[0]
"Finally we note that Monteresor et al. (Montresor et al., 2013) provide a distributed algorithm for this problem in a vertexcentric model.",1. Introduction,[0],[0]
"Although their model is different from our, more classic, MapReduce setting and their bound on the number of rounds is linear instead we achieve a logarithmic bound.",1. Introduction,[0],[0]
"In this section, we introduce the main definitions and the computational models that we consider in the paper.",2. Preliminaries,[0],[0]
We start by defining k-core and by introducing the concept of approximate k-core.,2. Preliminaries,[0],[0]
"Then we describe the MapReduce and streaming models.
",2. Preliminaries,[0],[0]
Approximate k-core.,2. Preliminaries,[0],[0]
"Let G = (V,E) be a graph with |V",2. Preliminaries,[0],[0]
| = n nodes and |E| = m edges.,2. Preliminaries,[0],[0]
"Let H be a subgraph of G, for any node v ∈ G we denote by d(v) the degree of the node in G and for any node v ∈",2. Preliminaries,[0],[0]
H,2. Preliminaries,[0],[0]
we denote by dH(v),2. Preliminaries,[0],[0]
the degree of v in the subgraph induced by H .,2. Preliminaries,[0],[0]
A k-core is a maximal subgraph H ⊆ G such that ∀v ∈,2. Preliminaries,[0],[0]
H we have dH(v),2. Preliminaries,[0],[0]
≥ k. Note,2. Preliminaries,[0],[0]
that for any k the k-core is unique and it may be possibly disconnected.,2. Preliminaries,[0],[0]
We say that a vertex v has coreness number k if it belongs to the k-core but it does not belong to the (k + 1)-core.,2. Preliminaries,[0],[0]
"We denote the coreness number of node i in the graph G with CG(i)(we drop the subscript notation when the graph is clear from the context).
",2. Preliminaries,[0],[0]
We define the core labeling for a graph G as the labeling where every vertex v is labeled with its coreness number.,2. Preliminaries,[0],[0]
"It is wroth noting that this labeling is unique and that it defines a hierarchical decomposition of G.
In this paper we are interested in computing a good approxi-
mation of the core labeling for a graph G efficiently in the MapReduce and in the streaming model.",2. Preliminaries,[0],[0]
"For this reason, we introduce the concept of 1− approximate k-core.",2. Preliminaries,[0],[0]
We define a 1 − approximation to the k-core of G to be a subgraph H of G that contains the k-core of G and such that ∀v,2. Preliminaries,[0],[0]
∈,2. Preliminaries,[0],[0]
H we have dH(v) ≥ (1 − )k.,2. Preliminaries,[0],[0]
"In other words, a 1− approximation to the k-core of G is a subgraph of the (1− )k-core of G and supergraph of the k-core of G.",2. Preliminaries,[0],[0]
"In Figure 1 we present the 3-core for a small graph and a 2 3 -approximate 3-core.
",2. Preliminaries,[0],[0]
"Similarly, a 1− approximate core-labeling of a graph G is a labeling of the vertices in G, where each vertex is labelled with a number between its coreness number and its coreness number multiplied by 11− .
",2. Preliminaries,[0],[0]
"In the paper we often refer to the classic greedy algorithm (Matula & Beck, 1983)(also known as peeling algorithm) to compute the coreness number.",2. Preliminaries,[0],[0]
The algorithm works as follows: nodes are removed from the graph iteratively.,2. Preliminaries,[0],[0]
"In particular, in iteration i of the algorithm all nodes with degree smaller or equal to i are removed iteratively and they are assigned coreness number i.",2. Preliminaries,[0],[0]
"It is possible to show that the algorithm computes the correct coreness number of all nodes in the graph and it can be implemented in linear time.
",2. Preliminaries,[0],[0]
MapReduce model.,2. Preliminaries,[0],[0]
"Here we briefly recall the main aspect of the model by Karloff et al. (Karloff et al., 2010) of the MapReduce framework (Dean & Ghemawat, 2010).
",2. Preliminaries,[0],[0]
"In the MapReduce model, the computation happens in parallel in several rounds.",2. Preliminaries,[0],[0]
"In each round, data is analyzed on each machine in parallel and then the output of the computations are shuffled between machines.",2. Preliminaries,[0],[0]
"The model has two main restrictions, one on the total number of machines and another on the memory available on each machine.",2. Preliminaries,[0],[0]
"More specifically, given an input of size N , and a small constant > 0, in the model there are N1− machines, each with N1− memory available.",2. Preliminaries,[0],[0]
"Note that, the total amount of memory available to the entire system is O(N2−2 ).
",2. Preliminaries,[0],[0]
The efficiency of an algorithm is measured by the number of the “rounds” needed by the algorithm to terminate.,2. Preliminaries,[0],[0]
"Classes of algorithms of particular interest are the ones that run in a constant or poly-logarithmic number of rounds.
",2. Preliminaries,[0],[0]
"Streaming We also analyze the approximate core labelling problem in the streaming model (Munro & Paterson, 1980).",2. Preliminaries,[0],[0]
"In this model the input consists of an undirected graph G = (V,E) and the input is presented as a stream of edges.",2. Preliminaries,[0],[0]
"The goal of our algorithm is to obtain a good approximation of the core labelling at the end of the stream using only small memory (Õ(n)).
",2. Preliminaries,[0],[0]
3.,2. Preliminaries,[0],[0]
Sketching k-Cores In this section we present a sketch to compute an approximate core labelling that uses only O(npolylog(n)) space.,2. Preliminaries,[0],[0]
"Compared with previous sketching for similar problems (Lee et al., 2010; Esfandiari et al., 2015; Bahmani et al., 2012; Epasto et al., 2015; Bhattacharya et al., 2016)",2. Preliminaries,[0],[0]
"our sketching samples different area of the graphs with different, carefully selected, probabilities.
",2. Preliminaries,[0],[0]
The main idea behind the sketch is to sample edges more aggressively in denser areas of the graph and less aggressively in sparser areas.,2. Preliminaries,[0],[0]
"More specifically, the algorithm works as follows: we start by sampling edges with some small probability, p, so that the resulting sampled graph, H , is sparse.",2. Preliminaries,[0],[0]
We then compute the coreness numbers for the vertices in H .,2. Preliminaries,[0],[0]
The key observation is that if a vertex has logarithmic coreness number inH we can precisely estimate its coreness number in the input graph G.,2. Preliminaries,[0],[0]
Furthermore we can show that if a vertex has large enough coreness number in the input graph G it will have at least logarithmic coreness number in H .,2. Preliminaries,[0],[0]
So using this technique we can detect efficiently all nodes with sufficiently high coreness number.,2. Preliminaries,[0],[0]
"To compute the coreness numbers of the rest of the node in the graph, we first remove from the graph the nodes for which we have a good estimation and then we iterate the same approach.",2. Preliminaries,[0],[0]
In particular we double our sample probability p and sample edges again.,2. Preliminaries,[0],[0]
"Interestingly, we can show that by sampling edges adaptively, we can iteratively estimate the coreness of all nodes in the graph by analyzing only sparse subgraphs.
",2. Preliminaries,[0],[0]
We are now ready to describe our sketching algorithm in details.,2. Preliminaries,[0],[0]
We start by describing a basic subroutine that estimates a modified version of the coreness number.,2. Preliminaries,[0],[0]
We dubbed the subroutine ExclusiveCorenessLabeling.,2. Preliminaries,[0],[0]
"The subroutine takes as input a subgraph, H , and a subset of the vertices Λ ⊆ H",2. Preliminaries,[0],[0]
"and it runs a modified version of the classic peeling algorithm (Matula & Beck, 1983) to compute the coreness number.",2. Preliminaries,[0],[0]
"The main difference between ExclusiveCorenessLabeling and the peeling algorithm in (Matula & Beck, 1983) is that we do not compute labels for nodes in Λ and we do not remove them from the subgraph H .",2. Preliminaries,[0],[0]
"The pseudocode for ExclusiveCorenessLabeling is presented in Algorithm 1.
",2. Preliminaries,[0],[0]
"During the execution of the algorithm we use subroutine ExclusiveCorenessLabeling to compute a labelling for the
Algorithm 1 ExclusiveCorenessLabeling(H,Λ) 1: Input: A graph H with n vertices and a set Λ ⊆ VH .",2. Preliminaries,[0],[0]
2: Initialize Γ = VH \ Λ 3: Initialize l← 0 4: while Γ 6= ∅,2. Preliminaries,[0],[0]
do 5: while minv∈Γ(dH(v)),2. Preliminaries,[0],[0]
≤,2. Preliminaries,[0],[0]
l do 6: Let v ← argminv∈Γ(dH(v)),2. Preliminaries,[0],[0]
7: Set lv ←,2. Preliminaries,[0],[0]
l 8: Remove v from Γ 9: Remove v from H 10: end whilel←,2. Preliminaries,[0],[0]
"l + 1 11: end while
subset of the nodes in H for which we do not have already a good estimate of the coreness number.
",2. Preliminaries,[0],[0]
"Now we can formally present our algorithm, we start by sampling the graph G with p ∈",2. Preliminaries,[0],[0]
O,2. Preliminaries,[0],[0]
( logn 2n ) .,2. Preliminaries,[0],[0]
"In this way, we obtain a sparse graph H0.",2. Preliminaries,[0],[0]
Then we run ExclusiveCorenessLabeling with H = H0 and Λ = ∅ to obtain a labeling of the nodes in H0.,2. Preliminaries,[0],[0]
Let l0(i) be the label of vertex i in this labeling.,2. Preliminaries,[0],[0]
"If a vertex i has l0(i) ≥ C log n, for a specific constant C > 0, we can estimate its coreness number in G precisely.",2. Preliminaries,[0],[0]
Intuitively this is true because we are sampling the edges independently so we can use concentration results to bound its coreness number.,2. Preliminaries,[0],[0]
"Hence, in the first round of our algorithm we can compute a precise estimate of the coreness number for all nodes i with l0(i) ≥ C log n.
In the rest of the execution of our algorithm we can recurse on the remaining nodes.",2. Preliminaries,[0],[0]
"To do so, we add the nodes with a good estimate to the set Λ and we remove from G the edges in the subgraph induced by the nodes in Λ.",2. Preliminaries,[0],[0]
Then we increase the sampling probability p by 2 and sample G again.,2. Preliminaries,[0],[0]
Similarly we obtain a new subgraph H1 and we run ExclusiveCorenessLabeling with H = H1 and Λ equal to the current Λ.,2. Preliminaries,[0],[0]
"So we obtain a labeling l1 for the nodes in H1\Λ. and also in this case if a vertex i has l1(i) ≥ C log n, for a specific constant C > 0, we can estimate its coreness number in G precisely.
",2. Preliminaries,[0],[0]
We iterate this algorithm for log n steps.,2. Preliminaries,[0],[0]
In the remaining of the section we first present pseudocode of our sketching algorithm(Algorithm 2) then we show that at the end of the execution of the algorithm we have a good estimation of the coreness number for all nodes in G.,2. Preliminaries,[0],[0]
"Finally we argue that in every iteration the graphs Hi are sparse so the algorithm uses only small memory at any point in time.
",2. Preliminaries,[0],[0]
"We start by providing the pseudocode of the algorithm in Algorithm 2.
",2. Preliminaries,[0],[0]
We are now ready to prove the main properties of our sketching technique.,2. Preliminaries,[0],[0]
We start by stating two technical lemma whose proofs follow from application of concentration bounds and are deferred to the full version.,2. Preliminaries,[0],[0]
"The main goal of the lemma is to relate the degree of a vertex v in a
Algorithm 2 A sketch based algorithm to compute 1 − O( ) approximate core-labeling.
",2. Preliminaries,[0],[0]
"1: Input: A graph G with n vertices and parameter ∈ (0, 1].",2. Preliminaries,[0],[0]
"2: Initialize Λ← ∅ 3: Initialize p0 ← 12 logn 2n 4: for j = 0 to logn do 5: Let Hj be a subgraph of G with the edges sampled independently with probability pj 6: Run Exclusive Core Labeling(Hj ,Λ) and denote the label of vertex i on Hj by lj(i) 7: for i ∈",2. Preliminaries,[0],[0]
Hj do 8: if lj(i) ≥ 24 logn 2 ∨ pj = 1 then 9: //,2. Preliminaries,[0],[0]
"Node i has sufficiently high degree to estimate its
coreness number.",2. Preliminaries,[0],[0]
"10: if lj(i) ≤ 48 logn 2 then 11: Set the label of vertex i to (1− ) lj(i)
pj
12: Add i to Λ 13: else 14: Set the label of vertex i to 2(1− )n",2. Preliminaries,[0],[0]
"2j−1 15: Add i to Λ 16: end if 17: end if 18: end for 19: Remove from G the edges of G induced by Λ 20: pj+1 ← 2pj 21: end for
subgraph of G and the sampled subgraph H .
",2. Preliminaries,[0],[0]
Lemma 3.1.,2. Preliminaries,[0],[0]
"Let G be a graph and let ∈ (0, 1] and δ ∈ (0, 1) be two arbitrary numbers.",2. Preliminaries,[0],[0]
"Let f(n) be a function of n such that f(n) ≥ 6 log n δ
2 and let H be a subgraph of G that contains each edge of G independently with probability p ≥ 6 log n δ
2f(n) .",2. Preliminaries,[0],[0]
"Then for all v ∈ G the following statements holds, with probability 1 − δ3n2 : (i)",2. Preliminaries,[0],[0]
"If dG(v) ≥ f(n) we have |dH(v)− pdG(v)| ≤ dH(v), (ii)",2. Preliminaries,[0],[0]
If dG(v) < f(n),2. Preliminaries,[0],[0]
"we have dH(v) < 2pf(n).
",2. Preliminaries,[0],[0]
Lemma 3.2.,2. Preliminaries,[0],[0]
"Let G be a graph and let ∈ (0, 1] and δ ∈ (0, 1) be two arbitrary numbers.",2. Preliminaries,[0],[0]
"Let f(n) be a function of n such that f(n) ≥ 6 log n δ
2 and let H be a subgraph of G that contains each edge of G independently with probability p ≥ 6 log n δ
2f(n) .",2. Preliminaries,[0],[0]
"For all v ∈ G the following statements holds, with probability 1 − δ3n2 : (i)",2. Preliminaries,[0],[0]
If dH(v) ≥ 2pf(n),2. Preliminaries,[0],[0]
we have |dH(v),2. Preliminaries,[0],[0]
"− pdG(v)| ≤ dH(v), (ii)",2. Preliminaries,[0],[0]
If dH(v) < 2pf(n),2. Preliminaries,[0],[0]
we have dG(v) ≤ 2(1 + )f(n).,2. Preliminaries,[0],[0]
"In addition, in the first case we have dG(v) ≥ 2(1− )f(n).",2. Preliminaries,[0],[0]
"Furthermore, if the graph is directed the same claims hold for the in-degree(d−(v)) and the out-degree(d+(v)) of a node v.
In the remaining of this section we assume that Lemma 3.1 and Lemma 3.2 hold and using this assumption we prove the main properties of our algorithm.
",2. Preliminaries,[0],[0]
We start by comparing the labels computed by Algorithm 1 with the coreness number of its input graph.,2. Preliminaries,[0],[0]
"Recall that we denote the coreness number of node i in the graph G with CG(i).
",2. Preliminaries,[0],[0]
Lemma 3.3.,2. Preliminaries,[0],[0]
"Let H = (V,E) be an arbitrary graph and let Λ ⊆ V be an arbitrary set of vertices.",2. Preliminaries,[0],[0]
"Let Ĥ = (Λ, Ê) be an arbitrary graph on the set of vertices Λ, and let H ′",2. Preliminaries,[0],[0]
= H ∪ Ĥ .,2. Preliminaries,[0],[0]
"Let lv be the label computed by ExclusiveCorenessLabeling(H,Λ).",2. Preliminaries,[0],[0]
"Then for each vertex v ∈ V \ Λ we have: (i) lv ≥ CH′(v), (ii) if CH′(v) ≤ minu∈Λ",2. Preliminaries,[0],[0]
"( CH′(u) ) , we have lv = CH′(v).
",2. Preliminaries,[0],[0]
Proof.,2. Preliminaries,[0],[0]
"By definition of coreness number, if we iteratively remove all vertices with degree CH′(v)− 1 from H ′, vertex v is not removed from the graph.",2. Preliminaries,[0],[0]
Furthermore note that Algorithm 1 does not remove any vertex with degree more than CH′(v),2. Preliminaries,[0],[0]
− 1 unless CH′(v),2. Preliminaries,[0],[0]
"− 1 < l. Thus, we have lv ≥ CH′(v) as desired.
",2. Preliminaries,[0],[0]
"Note that, if we set Λ = ∅, Algorithm 1 acts as the greedy algorithm that computes the coreness numbers.",2. Preliminaries,[0],[0]
"Moreover notice that if CH′(v) ≤ minu∈Λ ( CH′(u) ) , the classic peeling algorithm does not removed any of the vertices in Λ until it does not consider nodes with degree smaller or equal than l. Therefore we have lv ≥ CH′(v) which proves the second statement of the theorem.
",2. Preliminaries,[0],[0]
"We are now ready to state the two main Lemma proving the quality of the solution computed by our sketching technique.
",2. Preliminaries,[0],[0]
Lemma 3.4.,2. Preliminaries,[0],[0]
For all 0 ≤ j ≤ log n such that pj ≤ 1 and for any node v added to Λ in round j we have with probability 1− 13n that: C(v) < 2(1 + ),2. Preliminaries,[0],[0]
"n 2j−1 .
",2. Preliminaries,[0],[0]
Furthermore for all 0 ≤ j ≤ log n such that pj < 1 we have with probability 1− 13n that: C(v) ≥ 2(1− ) n,2. Preliminaries,[0],[0]
2j .,2. Preliminaries,[0],[0]
Lemma 3.5.,2. Preliminaries,[0],[0]
"Algorithm 2 computes a 1− 2 approximate core labeling, with probability 1− 23n .
",2. Preliminaries,[0],[0]
"The proofs of the Lemma is presented in the full version.
",2. Preliminaries,[0],[0]
"Now we give a lemma that bounds the total number of edges used in sketches H0, H2, . . .",2. Preliminaries,[0],[0]
",Hρ.",2. Preliminaries,[0],[0]
Lemma 3.6.,2. Preliminaries,[0],[0]
The number of edges in ∪ρi=0Hi produced by Algorithm 2 is upper bounded by O,2. Preliminaries,[0],[0]
"( n log2 n 2 ) , with proba-
bility 1−",2. Preliminaries,[0],[0]
"1n .
",2. Preliminaries,[0],[0]
Proof.,2. Preliminaries,[0],[0]
"In the proof, we assume that the statement of Lemma 3.4 holds, and the statements of Lemma 3.1 and 3.2 hold for Hj,k and Hj,v for all choices of j and k and v.
Consider an arbitrary 0 ≤ j ≤ log n. From Lemma 3.4, we have that for any v ∈",2. Preliminaries,[0],[0]
Hj \,2. Preliminaries,[0],[0]
(∪j−1i=0 Λj) the coreness number of v is bounded by 2(1 + ) n2j−1 .,2. Preliminaries,[0],[0]
"Now consider an orientation of the edges of Hj where every edge is oriented to its endpoint of smallest core number, breaking the ties in such a way that the in-degree of every node v, d−(v) is upperbounded by C(v)2.",2. Preliminaries,[0],[0]
"Furthermore note
2Note that such an orientation exists, in fact it can be obtained
that every edge in Hj is incident to a node of coreness number at most 2(1 + ) n2j−1 , so using Lemma 3.1 we have that in-degree of every node in Hj is bounded by 2(1 + )",2. Preliminaries,[0],[0]
"2 n2j−1 p j = 48 (1+ ) 2
2 log n.",2. Preliminaries,[0],[0]
"So summing over all the in-degrees we get that the number of edges in Hj is bounded by 48 (1+ ) 2
2 n log n.",2. Preliminaries,[0],[0]
We conclude the proof by noticing that there are,2. Preliminaries,[0],[0]
at most log n different Hj,2. Preliminaries,[0],[0]
"so the total memory used is 48 (1+ ) 2 2 n log 2 n.
Putting together Lemma 3.5 and Lemma 3.6 we get the main theorem of this section.
",2. Preliminaries,[0],[0]
Theorem 3.7.,2. Preliminaries,[0],[0]
"Algorithm 2 computes a 1− 2 approximate core labeling and the total spaced used by the algorithm is O ( n log2 n 2 ) , with probability 1− 2n .",2. Preliminaries,[0],[0]
In this section we show how to compute our sketch efficiently using a MapReduce or a streaming algorithm.,4. MapReduce and Streaming Algorithms,[0],[0]
"Here, we show how to use implement the sketch introduced in Section 3 in the MapReduce model.",4.1. MapReduce algorithm,[0],[0]
"In this way we obtain an efficient MapReduce algorithm for dense graphs3.
",4.1. MapReduce algorithm,[0],[0]
Recall that the main limitation of the MapReduce model is on the number of machines and on the available memory on each machine.,4.1. MapReduce algorithm,[0],[0]
"Our algorithm runs for 2 log n rounds.4 In the first round of MapReduce, the edges are sampled in parallel with probability p0 = 12 logn 2n .",4.1. MapReduce algorithm,[0],[0]
"In this way, we obtain a graph H0 that we analyze in the second round in a single machines(note that we can do it because from Lemma 3.6 we know that for all i the number of edges inHi is bounded by O ( n log2 n 2 ) .",4.1. MapReduce algorithm,[0],[0]
"At the end of the second round,
we obtain the labeling for the nodes with high coreness number and we add them to the set Λ0.",4.1. MapReduce algorithm,[0],[0]
In the third round we send the set Λ0 to all the machined and we sample in parallel the edges in |E|\Λ0 with probability 2p0 in a round of MapReduce.,4.1. MapReduce algorithm,[0],[0]
"In this way, we obtain H1 that in the fourth round is analyzed by a single machine to obtain the labelling of few additional nodes that are added to Λ1.",4.1. MapReduce algorithm,[0],[0]
"By iterating this process for 2 log n rounds, we obtain an approximation of the coreness number for each node.",4.1. MapReduce algorithm,[0],[0]
"The pseudo-code for the MapReduce algorithm is presented in Algorithm 3.
by orienting every edges to its endpoint that is first removed by the classic peeling algorithm used to compute the coreness number.
",4.1. MapReduce algorithm,[0],[0]
"3It is important to note that we only use polylogarithmic memory for each machine so our algorithm works also in more restrictive parallel models as the massively parallel model (Andoni et al., 2014; Im et al., 2017)
4The algorithm can be implemented using logn MapReduce rounds, but for simplicity, here we present a 2 logn rounds version.
",4.1. MapReduce algorithm,[0],[0]
"Algorithm 3 A MapReduce algorithm to compute 1 − O( )- approximate core-labeling.
",4.1. MapReduce algorithm,[0],[0]
"1: Input: A graph G with n vertices and parameter ∈ (0, 1].",4.1. MapReduce algorithm,[0],[0]
"2: Initialize Λ← ∅ 3: Initialize p0 ← 12 logn 2n 4: for j = 0 to logn do 5: // First round of MapReduce 6: Send Λ to all machines 7: Let E′ be the set of edges of G that are not contained in the graph induced by Λ on G 8: Sample with probability pj in parallel using n machines
the edges in E′
9: // Second round of MapReduce 10: Send all the sampled edge to a single machine 11: Let Hj be the sampled subgraph of G 12:",4.1. MapReduce algorithm,[0],[0]
"Run Exclusive Core Labeling(Hj ,Λ) and denote the label of vertex i on Hj by lj(i) 13: for i ∈",4.1. MapReduce algorithm,[0],[0]
Hj do 14: if lj(i) ≥ 24 logn 2 ∨ pj = 1 then 15: //,4.1. MapReduce algorithm,[0],[0]
Node i has sufficiently high degree to estimate its coreness number.,4.1. MapReduce algorithm,[0],[0]
"16: if lj(i) ≤ 48 logn 2 then 17: Set the label of vertex i to (1− ) lj(i)
pj
18: Add i to Λ 19: else 20: Set the label of vertex i to 2(1− )n",4.1. MapReduce algorithm,[0],[0]
"2j−1 21: Add i to Λ 22: end if 23: end if 24: end for 25: pj+1 ← 2pj 26: end for
By Theorem 3.7 presented in the previous section we obtain the following corollary.
",4.1. MapReduce algorithm,[0],[0]
Corollary 4.1.,4.1. MapReduce algorithm,[0],[0]
"Let G = (V,E) be a graph such that |E| ∈ Ω(|V |1+γ), for some constant γ > 0.",4.1. MapReduce algorithm,[0],[0]
Then there is an algorithm that computes w.h.p.,4.1. MapReduce algorithm,[0],[0]
an approximate core-labeling of the graph in the MapReduce model using O(log n) rounds of MapReduce.,4.1. MapReduce algorithm,[0],[0]
Next we show an application of our sketch in the streaming setting.,4.2. Semi-streaming algorithms,[0],[0]
We consider the setting where edges are only added to the graph.,4.2. Semi-streaming algorithms,[0],[0]
"The main idea behind our streaming algorithm is to maintain at any point in time the sketch presented in Section 3, which requires only Õ(n) space.",4.2. Semi-streaming algorithms,[0],[0]
"In the remaining of the section we describe how we can maintain the sketching in streaming.
",4.2. Semi-streaming algorithms,[0],[0]
"When an edge is added to G, we check by sampling if it is H0.",4.2. Semi-streaming algorithms,[0],[0]
"In this case in H0, we recompute the labeling of H0 and if one of the endpoints of the edge is added to Λ0, we update the rest of the sketch to reflect this change.",4.2. Semi-streaming algorithms,[0],[0]
"Then, if both endpoints of the edge are not contained in Λ0, we check by sampling if the edge is contained in H1.",4.2. Semi-streaming algorithms,[0],[0]
"Also in this case, if it is inH1, we recompute the labeling ofH1 and
Algorithm 4 A streaming algorithm to compute 1 − O( ) approximate core-labeling.
",4.2. Semi-streaming algorithms,[0],[0]
"1: Input: Stream of edges and parameter ∈ (0, 1].",4.2. Semi-streaming algorithms,[0],[0]
"2: Initialize Λi ← ∅, ∀i 3: Initialize p0 ← 12 logn 2n 4:",4.2. Semi-streaming algorithms,[0],[0]
"Insertion of (u, v) 5: r ← random number from [0, 1] 6: for j = 0 to logn do 7: if v /∈ ∪i<jΛi or u /∈ ∪i<jΛi then 8: if r ≤",4.2. Semi-streaming algorithms,[0],[0]
"pj then 9: Add (u, v) to Hj
10: Run Exclusive Core Labeling(Hj ,Λi) and denote the label of vertex i on Hj by lj(i) 11: for i ∈",4.2. Semi-streaming algorithms,[0],[0]
"Hj do 12: if lj(i) ≥ 24 logn 2 ∨ pj = 1 then 13: if lj(i) ≤ 48 logn 2 then 14: Set the label of vertex i to (1− ) lj(i)
pj
15: Add i to Λj 16: else 17: Set the label of vertex i to 2(1− )n",4.2. Semi-streaming algorithms,[0],[0]
2j−1 18: Add i to Λj 19: end if 20: end if 21: end for 22: for j′ = j + 1 to logn do 23: Remove from Hj′ any edge induced by Λj 24: end for 25: end if 26: else 27:,4.2. Semi-streaming algorithms,[0],[0]
"Break 28: end if 29: pj+1 ← 2pj 30: end for
modify the sketch accordingly.",4.2. Semi-streaming algorithms,[0],[0]
"We continue this procedure until both endpoints of the edge are contained in Λ. Notice that, by inserting edges ∪i≤jΛis may only grow.",4.2. Semi-streaming algorithms,[0],[0]
"Hence if at some point both endpoints of an edge (u, v) are in ∪i≤jΛi, by inserting more edge u and v remain in ∪i≤jΛi.
",4.2. Semi-streaming algorithms,[0],[0]
"The pseudo-code for the streaming algorithm is presented in Algorithm 4(Note that here for simplicity we recompute the core labels after the insertion of an edge (u, v).",4.2. Semi-streaming algorithms,[0],[0]
"However, one might recurse over the neighborhood of u and v and update the core labels locally).
",4.2. Semi-streaming algorithms,[0],[0]
"By Theorem 3.7 presented in the previous section we obtain the following corollary.
",4.2. Semi-streaming algorithms,[0],[0]
Corollary 4.2.,4.2. Semi-streaming algorithms,[0],[0]
There exists a streaming algorithm that computes w.h.p.,4.2. Semi-streaming algorithms,[0],[0]
an (1 − )-approximate core-labeling of the input graph using O ( n log2 n 2 ) space.,4.2. Semi-streaming algorithms,[0],[0]
"In this section, we analyze the performances of our sketch in practice.",5. Experiments,[0],[0]
First we describe our datasets.,5. Experiments,[0],[0]
"Next we discuss the implementations of our sketch presented in Section 3 and
our MapReduce algorithm.",5. Experiments,[0],[0]
Then we study the scalability and the accuracy of our sketch.,5. Experiments,[0],[0]
"In particular, we analyze the trade-off between quality of the approximation and space used by the sketch.
",5. Experiments,[0],[0]
Datasets.,5. Experiments,[0],[0]
"We apply our sketch to eight real-world graphs available in the SNAP, Stanford Large Network Dataset Library (Leskovec & Sosič, 2016):",5. Experiments,[0],[0]
"Enron (Klimt & Yang, 2004), Epinions (Richardson et al., 2003), Slashdot (Leskovec et al., 2009), Twitter (McAuley & Leskovec, 2012), Amazon (Yang & Leskovec, 2015), Youtube (Yang & Leskovec, 2015), LiveJournal (Yang & Leskovec, 2015), Orkut (Yang & Leskovec, 2015) with respectively 36692, 75879, 82168, 81306, 334863, 1134890, 3997962 and 3072441 nodes and 183831, 508837, 948464, 1768149, 925872, 2987624, 34681189 and 117185083 edges.
",5. Experiments,[0],[0]
Implementation details.,5. Experiments,[0],[0]
"In order to have an efficient implementation of our sketch, we modify Algorithm 2 slightly.",5. Experiments,[0],[0]
"More specifically, we change line 14 to “if lj(i) ≥ T ∨pj = 1” where T is a parameter of our implementation.",5. Experiments,[0],[0]
"Furthermore, we also modify line 22 to “pj+1 ← M · pj , where M is modifiable multiplicative factor (that in Algorithm 2 is fixed to 2).",5. Experiments,[0],[0]
"We also slightly modify our MapReduce algorithm to remove iteratively in parallel all nodes with degree smaller than 3 before sending the remaining graph to a single machine.
Metrics.",5. Experiments,[0],[0]
To study the scalability of the algorithm we implement our MapReduce algorithm in distributed setting and we analyze the running time on different graphs by using a fixed number of machine.,5. Experiments,[0],[0]
"To evaluate the quality of our sketch, we consider the quality of the approximation and the space used.
",5. Experiments,[0],[0]
"For the quality of the approximation, we report the median error and the error at the 60, 70, 80 and 90 percentile of our algorithm.",5. Experiments,[0],[0]
"In interest of space, we report the errors only on nodes with coreness number at least 5, because high coreness number are harder to approximate and for almost all the nodes of coreness smaller than 5 we have errors close to 0.
",5. Experiments,[0],[0]
For space we consider the maximum size of any sample graph,5. Experiments,[0],[0]
Hi and the sum of their sizes.,5. Experiments,[0],[0]
"Note that the first quantity bounds the memory used by our distributed algorithm or a multi-pass streaming algorithm, and the second one bounds the memory used by a single pass streaming algorithm.
",5. Experiments,[0],[0]
Scalability Results.,5. Experiments,[0],[0]
In Figure 2 we present the results of our scalability experiments(In the experiment we fix T = 4 and M = 2).,5. Experiments,[0],[0]
"On the x axis we order the graphs based on their number of edges, in the y axis we show the relative running time on different graphs.",5. Experiments,[0],[0]
"Note that in the Figure
the x axis is in logscale and the y axis is in linear scale so the running time of our algorithm grows sublinearly in the number of edges in the graph proving that our algorithm is able to leverage parallelization to obtain good performance.
",5. Experiments,[0],[0]
"For comparison we also run a simple iterative algorithm to estimate the k-core number that resembles a simple adaptation of the algorithm presented in (Lee et al., 2010; Esfandiari et al., 2015; Bahmani et al., 2012; Epasto et al., 2015; Bhattacharya et al., 2016) for densest subgraph.",5. Experiments,[0],[0]
The adapted algorithm works as follows: it removes all nodes below a threshold T (initially equal to 4) from the graphs in parallel and estimate their coreness number as T .,5. Experiments,[0],[0]
"Then when no node with degree smaller than T is left, it iteratively increases T by a multiplicative factor M = 2 and recurse on the remaining graph.",5. Experiments,[0],[0]
"Interestingly we observe that this adapted algorithm is an order of magnitude slower than our distributed algorithm and so we could run it only on relatively small graphs like Amazon.5
Accuracy Results.",5. Experiments,[0],[0]
All the reported number are the average over 3 runs of our algorithm.,5. Experiments,[0],[0]
In all our experiment we either fix T = 3 and vary M or fix M to 2 and vary T .,5. Experiments,[0],[0]
"In Table 1 we present the space used in our algorithm when we vary the value of T .
",5. Experiments,[0],[0]
"5Note that the parallel version of the simple iterative algorithm is particularly slow in practice because it needs several parallel rounds to complete.
",5. Experiments,[0],[0]
There are few interesting things to note.,5. Experiments,[0],[0]
"First, the size of the maximum sampled graph is always significantly smaller than the size of input graph and in some cases it is more than one order of magnitude smaller (for example in the Twitter case).",5. Experiments,[0],[0]
"Interestingly, note that the relative size of the maximum sampled graph decrease with the size of the input graph.",5. Experiments,[0],[0]
This suggests that the sketch would be even more effective when applied to a larger graph.,5. Experiments,[0],[0]
"Also the total size of the sketch is also smaller than the size of the graph in many cases (for instance, the sketch for Twitter is always smaller than half of the size of the input graph).",5. Experiments,[0],[0]
"This implies that we can compute an approximation of the coreness number without processing most of the edges in the input graph.
",5. Experiments,[0],[0]
"In Figure 3, we report the approximation error of our algorithm.",5. Experiments,[0],[0]
First we note that as T increases the approximation error decreases as predicted by our theorems.,5. Experiments,[0],[0]
It is also interesting to note that the median error is always below 50% and with T ≥ 3 is below 25%.,5. Experiments,[0],[0]
"Observe for T ≥ 3, the error at the 90 percentile is below 50%.",5. Experiments,[0],[0]
"Overall our sketch provides a good approximation of the coreness numbers.
",5. Experiments,[0],[0]
Now we focus on the effect of M on our sketch.,5. Experiments,[0],[0]
"In Table 2, we present the space used by our algorithm as a function of M .",5. Experiments,[0],[0]
"Note that the maximum size of a single sample graph decrease with M , but the total size of the sketch increases (this is due to the increased number of sampled graphs).",5. Experiments,[0],[0]
"This suggests that we should use small M in distributed settings where we have tighter space constraint and larger M when
we want to design single pass streaming algorithms.
",5. Experiments,[0],[0]
"Finally it is interesting to note that as shown in Figure 4, the quality of the approximation is not very much influenced by the scaling factor M .",5. Experiments,[0],[0]
In this paper we introduce a new sketching technique for computing the core-labeling of a graph.,6. Conclusions and future works,[0],[0]
"In particular, we design efficient MapReduce and streaming algorithms to approximate the coreness number of all the nodes in a graph efficiently.",6. Conclusions and future works,[0],[0]
We also confirm the effectiveness of our sketch via an empirical study.,6. Conclusions and future works,[0],[0]
"The most interesting open problem in the area is to design a fully dynamic algorithm (Italiano et al., 1999) to maintain the core-labeling of a graph by using only polylog n operations per update(edge addition or deletion).",6. Conclusions and future works,[0],[0]
The k-core decomposition is a fundamental primitive in many machine learning and data mining applications.,abstractText,[0],[0]
We present the first distributed and the first streaming algorithms to compute and maintain an approximate k-core decomposition with provable guarantees.,abstractText,[0],[0]
Our algorithms achieve rigorous bounds on space complexity while bounding the number of passes or number of rounds of computation.,abstractText,[0],[0]
"We do so by presenting a new powerful sketching technique for k-core decomposition, and then by showing it can be computed efficiently in both streaming and MapReduce models.",abstractText,[0],[0]
"Finally, we confirm the effectiveness of our sketching technique empirically on a number of publicly available graphs.",abstractText,[0],[0]
Parallel and Streaming Algorithms for K-Core Decomposition,title,[0],[0]
"Many autoregressive image models factorize the joint distribution of images into per-pixel factors:
p(x1:T ) = TY
t=1
p(xt |x1:t 1) (1)
",1. Introduction,[0],[0]
"For example PixelCNN (van den Oord et al., 2016b) uses a deep convolutional network with carefully designed filter masking to preserve causal structure, so that all factors in equation 1 can be learned in parallel for a given image.",1. Introduction,[0],[0]
"However, a remaining di culty is that due to the learned causal structure, inference proceeds sequentially pixel-bypixel in raster order.
1DeepMind.",1. Introduction,[0],[0]
"Correspondence to: Scott Reed <reedscot@google.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"In the naive case, this requires a full network evaluation per pixel.",1. Introduction,[0],[0]
"Caching hidden unit activations can be used to reduce the amount of computation per pixel, as in the 1D case for WaveNet (Oord et al., 2016; Ramachandran et al., 2017).",1. Introduction,[0],[0]
"However, even with this optimization, generation is still in serial order by pixel.
",1. Introduction,[0],[0]
"Ideally we would generate multiple pixels in parallel, which could greatly accelerate sampling.",1. Introduction,[0],[0]
In the autoregressive framework this only works if the pixels are modeled as independent.,1. Introduction,[0],[0]
"Thus we need a way to judiciously break weak dependencies among pixels; for example immediately neighboring pixels should not be modeled as independent since they tend to be highly correlated.
",1. Introduction,[0],[0]
Multiscale image generation provides one such way to break weak dependencies.,1. Introduction,[0],[0]
"In particular, we can model certain groups of pixels as conditionally independent given a lower resolution image and various types of context information, such as preceding frames in a video.",1. Introduction,[0],[0]
"The basic idea is obvious, but nontrivial design problems stand between the idea and a workable implementation.
",1. Introduction,[0],[0]
"First, what is the right way to transmit global information from a low-resolution image to each generated pixel of the high-resolution image?",1. Introduction,[0],[0]
"Second, which pixels can we gen-
erate in parallel?",1. Introduction,[0],[0]
"And given that choice, how can we avoid border artifacts when merging sets of pixels that were generated in parallel, blind to one another?
",1. Introduction,[0],[0]
"In this work we show how a very substantial portion of the spatial dependencies in PixelCNN can be cut, with only modest degradation in performance.",1. Introduction,[0],[0]
"Our formulation allows sampling in O(log N) time for N pixels, instead of O(N) as in the original PixelCNN, resulting in orders of magnitude speedup in practice.",1. Introduction,[0],[0]
"In the case of video, in which we have access to high-resolution previous frames, we can even sample in O(1) time, with much better performance than comparably-fast baselines.
",1. Introduction,[0],[0]
"At a high level, the proposed approach can be viewed as a way to merge per-pixel factors in equation 1.",1. Introduction,[0],[0]
"If we merge the factors for, e.g. xi and x j, then that dependency is “cut”, so the model becomes slightly less expressive.",1. Introduction,[0],[0]
"However, we get the benefit of now being able to sample xi and x j in parallel.",1. Introduction,[0],[0]
"If we divide the N pixels into G groups of T pixels each, the joint distribution can be written as a product of the corresponding G factors:
p(x1:G1:T ) = GY
g=1
p(x(g)1:T |x",1. Introduction,[0],[0]
"(1:g 1) 1:T ) (2)
",1. Introduction,[0],[0]
"Above we assumed that each of the G groups contains exactly T pixels, but in practice the number can vary.",1. Introduction,[0],[0]
"In this work, we form pixel groups from successively higherresolution views of an image, arranged into a sub-sampling pyramid, such that G 2 O(log N).",1. Introduction,[0],[0]
In section 3 we describe this group structure implemented as a deep convolutional network.,1. Introduction,[0],[0]
In section 4 we show that the model excels in density estimation and can produce quality high-resolution samples at high speed.,1. Introduction,[0],[0]
"Deep neural autoregressive models have been applied to image generation for many years, showing promise as a tractable yet expressive density model (Larochelle & Murray, 2011; Uria et al., 2013).",2. Related work,[0],[0]
"Autoregressive LSTMs have been shown to produce state-of-the-art performance in density estimation on large-scale datasets such as ImageNet (Theis & Bethge, 2015; van den Oord et al., 2016a).
",2. Related work,[0],[0]
"Causally-structured convolutional networks such as PixelCNN (van den Oord et al., 2016b) and WaveNet (Oord et al., 2016) improved the speed and scalability of training.",2. Related work,[0],[0]
"These led to improved autoregressive models for video generation (Kalchbrenner et al., 2016b) and machine translation (Kalchbrenner et al., 2016a).
",2. Related work,[0],[0]
Non-autoregressive convolutional generator networks have been successful and widely adopted for image generation as well.,2. Related work,[0],[0]
"Instead of maximizing likelihood, Generative Ad-
versarial Networks (GANs) train a generator network to fool a discriminator network adversary (Goodfellow et al., 2014).",2. Related work,[0],[0]
"These networks have been used in a wide variety of conditional image generation schemes such as text and spatial structure to image (Mansimov et al., 2015; Reed et al., 2016b;a; Wang & Gupta, 2016).
",2. Related work,[0],[0]
The addition of multiscale structure has also been shown to be useful in adversarial networks.,2. Related work,[0],[0]
Denton et al. (2015) used a Laplacian pyramid to generate images in a coarse-to-fine manner.,2. Related work,[0],[0]
"Zhang et al. (2016) composed a low-resolution and high-resolution text-conditional GAN, yielding higher quality 256 ⇥ 256 bird and flower images.",2. Related work,[0],[0]
"Generator networks can be combined with a trained model, such as an image classifier or captioning network, to generate high-resolution images via optimization and sampling procedures (Nguyen et al., 2016).",2. Related work,[0],[0]
"Wu et al. (2017) state that it is di cult to quantify GAN performance, and propose Monte Carlo methods to approximate the loglikelihood of GANs on MNIST images.
",2. Related work,[0],[0]
Both auto-regressive and non auto-regressive deep networks have recently been applied successfully to image super-resolution.,2. Related work,[0],[0]
Shi et al. (2016) developed a sub-pixel convolutional network well-suited to this problem.,2. Related work,[0],[0]
Dahl et al. (2017) use a PixelCNN as a prior for image superresolution with a convolutional neural network.,2. Related work,[0],[0]
Johnson et al. (2016) developed a perceptual loss function useful for both style transfer and super-resolution.,2. Related work,[0],[0]
"GAN variants have also been successful in this domain (Ledig et al., 2016; Sønderby et al., 2017).
",2. Related work,[0],[0]
"Several other deep, tractable density models have recently been developed.",2. Related work,[0],[0]
"Real NVP (Dinh et al., 2016) learns a mapping from images to a simple noise distribution, which is by construction trivially invertible.",2. Related work,[0],[0]
"It is built from smaller invertible blocks called coupling layers whose Jacobian is lower-triangular, and also has a multiscale structure.",2. Related work,[0],[0]
"Inverse Autoregressive Flows (Kingma & Salimans, 2016) use autoregressive structures in the latent space to learn more flexible posteriors for variational autoencoders.",2. Related work,[0],[0]
"Autoregressive models have also been combined with VAEs as decoder models (Gulrajani et al., 2016).
",2. Related work,[0],[0]
"The original PixelRNN paper (van den Oord et al., 2016a) actually included a multiscale autoregressive version, in which PixelRNNs or PixelCNNs were trained at multiple resolutions.",2. Related work,[0],[0]
The network producing a given resolution image was conditioned on the image at the next lower resolution.,2. Related work,[0],[0]
"This work is similarly motivated by the usefulness of multiscale image structure (and the very long history of coarse-to-fine modeling).
",2. Related work,[0],[0]
"Our novel contributions in this work are (1) asymptotically and empirically faster inference by modeling conditional independence structure, (2) scaling to much higher reso-
lution, (3) evaluating the model on a diverse set of challenging benchmarks including class-, text- and structureconditional image generation and video generation.",2. Related work,[0],[0]
The main design principle that we follow in building the model is a coarse-to-fine ordering of pixels.,3. Model,[0],[0]
Successively higher-resolution frames are generated conditioned on the previous resolution (See for example Figure 1).,3. Model,[0],[0]
"Pixels are grouped so as to exploit spatial locality at each resolution, which we describe in detail below.
",3. Model,[0],[0]
The training objective is to maximize log P(x; ✓).,3. Model,[0],[0]
"Since the joint distribution factorizes over pixel groups and scales, the training can be trivially parallelized.",3. Model,[0],[0]
"Figure 2 shows how we divide an image into disjoint groups of pixels, with autoregressive structure among the groups.",3.1. Network architecture,[0],[0]
The key property to notice is that no two adjacent pixels of the high-resolution image are in the same group.,3.1. Network architecture,[0],[0]
"Also, pixels can depend on other pixels below and to the right, which would have been inaccessible in the standard PixelCNN.",3.1. Network architecture,[0],[0]
"Each group of pixels corresponds to a factor in the joint distribution of equation 2.
",3.1. Network architecture,[0],[0]
"Concretely, to create groups we tile the image with 2 ⇥ 2 blocks.",3.1. Network architecture,[0],[0]
"The corners of these 2⇥2 blocks form the four pixel groups at a given scale; i.e. upper-left, upper-right, lowerleft, lower-right.",3.1. Network architecture,[0],[0]
Note that some pairs of pixels both within each block and also across blocks can still be dependent.,3.1. Network architecture,[0],[0]
"These additional dependencies are important for capturing local textures and avoiding border artifacts.
",3.1. Network architecture,[0],[0]
Figure 3 shows an instantiation of one of these factors as a neural network.,3.1. Network architecture,[0],[0]
"Similar to the case of PixelCNN, at training time losses and gradients for all of the pixels within a group can be computed in parallel.",3.1. Network architecture,[0],[0]
"At test time, inference proceeds sequentially over pixel groups, in parallel within each group.",3.1. Network architecture,[0],[0]
"Also as in PixelCNN, we model the color channel dependencies - i.e. green sees red, blue sees red and green - using channel masking.
",3.1. Network architecture,[0],[0]
"In the case of type-A upscaling networks (See Figure 3A), sampling each pixel group thus requires 3 network evaluations 1.",3.1. Network architecture,[0],[0]
"In the case of type-B upscaling, the spatial feature map for predicting a group of pixels is divided into contiguous M ⇥ M patches for input to a shallow PixelCNN (See figure 3B).",3.1. Network architecture,[0],[0]
"This entails M2 very small network evaluations, for each color channel.",3.1. Network architecture,[0],[0]
"We used M = 4, and the shallow PixelCNN weights are shared across patches.
1However, one could also use a discretized mixture of logistics as output instead of a softmax as in Salimans et al. (2017), in which case only one network evaluation is needed.
",3.1. Network architecture,[0],[0]
The division into non-overlapping patches may appear to risk border artifacts when merging.,3.1. Network architecture,[0],[0]
"However, this does not occur for several reasons.",3.1. Network architecture,[0],[0]
"First, each predicted pixel is directly adjacent to several context pixels fed into the upscaling network.",3.1. Network architecture,[0],[0]
"Second, the generated patches are not directly adjacent in the 2K⇥2K output image; there is always a row or column of pixels on the border of any pair.
",3.1. Network architecture,[0],[0]
"Note that the only learnable portions of the upscaling module are (1) the ResNet encoder of context pixels, and (2) the shallow PixelCNN weights in the case of type-B upscaling.",3.1. Network architecture,[0],[0]
"The “merge” and “split” operations shown in figure 3 only marshal data and are not associated with parameters.
",3.1. Network architecture,[0],[0]
"Given the first group of pixels, the rest of the groups at a given scale can be generated autoregressively.",3.1. Network architecture,[0],[0]
"The first group of pixels can be modeled using the same approach as detailed above, recursively, down to a base resolution at which we use a standard PixelCNN.",3.1. Network architecture,[0],[0]
"At each scale, the number of evaluations is O(1), and the resolution doubles after each upscaling, so the overall complexity is O(log N) to produce images with N pixels.",3.1. Network architecture,[0],[0]
"Given some context information c, such as a text description, a segmentation, or previous video frames, we maximize the conditional likelihood log P(x|c; ✓).",3.2. Conditional image modeling,[0],[0]
Each factor in equation 2 simply adds c as an additional conditioning variable.,3.2. Conditional image modeling,[0],[0]
"The upscaling neural network corresponding to each factor takes c as an additional input.
",3.2. Conditional image modeling,[0],[0]
"For encoding text we used a character-CNN-GRU as in (Reed et al., 2016a).",3.2. Conditional image modeling,[0],[0]
For spatially structured data such as segmentation masks we used a standard convolutional network.,3.2. Conditional image modeling,[0],[0]
"For encoding previous frames in a video we used a ConvLSTM as in (Kalchbrenner et al., 2016b).",3.2. Conditional image modeling,[0],[0]
"We evaluate our model on ImageNet, Caltech-UCSD Birds (CUB), the MPII Human Pose dataset (MPII), the Microsoft Common Objects in Context dataset (MS-COCO), and the Google Robot Pushing dataset.
",4.1. Datasets,[0],[0]
"• For ImageNet (Deng et al., 2009), we trained a classconditional model using the 1000 leaf node classes.
",4.1. Datasets,[0],[0]
"• CUB (Wah et al., 2011) contains 11, 788 images across 200 bird species, with 10 captions per image.",4.1. Datasets,[0],[0]
"As conditioning information we used a 32⇥32 spatial encoding of the 15 annotated bird part locations.
",4.1. Datasets,[0],[0]
"• MPII (Andriluka et al., 2014) has around 25K images of 410 human activities, with 3 captions per image.
",4.1. Datasets,[0],[0]
"We kept only the images depicting a single person, and cropped the image centered around the person, leaving us about 14K images.",4.1. Datasets,[0],[0]
"We used a 32 ⇥ 32 encoding of the 17 annotated human part locations.
",4.1. Datasets,[0],[0]
"• MS-COCO (Lin et al., 2014) has 80K training images with 5 captions per image.",4.1. Datasets,[0],[0]
"As conditioning we used the 80-class segmentation scaled to 32 ⇥ 32.
",4.1. Datasets,[0],[0]
"• Robot Pushing (Finn et al., 2016) contains sequences of 20 frames of size 64 ⇥ 64 showing a robotic arm pushing objects in a basket.",4.1. Datasets,[0],[0]
"There are 50, 000 training sequences and a validation set with the same objects but di↵erent arm trajectories.",4.1. Datasets,[0],[0]
"One test set involves a subset of the objects seen during training and another involving novel objects, both captured on an arm and camera viewpoint not seen during training.
",4.1. Datasets,[0],[0]
"All models for ImageNet, CUB, MPII and MS-COCO were trained using RMSprop with hyperparameter ✏ = 1e 8, with batch size 128 for 200K steps.",4.1. Datasets,[0],[0]
"The learning rate was
set initially to 1e 4 and decayed to 1e 5.",4.1. Datasets,[0],[0]
"For all of the samples we show, the queries are drawn from the validation split of the corresponding data set.",4.1. Datasets,[0],[0]
"That is, the captions, key points, segmentation masks, and lowresolution images for super-resolution have not been seen by the model during training.
",4.1. Datasets,[0],[0]
"When we evaluate negative log-likelihood, we only quantize pixel values to [0, ..., 255] at the target resolution, not separately at each scale.",4.1. Datasets,[0],[0]
The lower resolution images are then created by sub-sampling this quantized image.,4.1. Datasets,[0],[0]
"In this section we show results for CUB, MPII and MSCOCO.",4.2. Text and location-conditional generation,[0],[0]
"For each dataset we trained type-B upscaling networks with 12 ResNet layers and 4 PixelCNN layers, with 128 hidden units per layer.",4.2. Text and location-conditional generation,[0],[0]
The base resolution at which we train a standard PixelCNN was set to 4 ⇥ 4.,4.2. Text and location-conditional generation,[0],[0]
"To encode the captions we padded to 201 characters, then
fed into a character-level CNN with three convolutional layers, followed by a GRU and average pooling over time.",4.2. Text and location-conditional generation,[0],[0]
"Upscaling networks to 8 ⇥ 8, 16 ⇥ 16 and 32 ⇥ 32 shared a single text encoder.",4.2. Text and location-conditional generation,[0],[0]
For higher-resolution upscaling networks we trained separate text encoders.,4.2. Text and location-conditional generation,[0],[0]
"In principle all upscalers could share an encoder, but we trained separably to save memory and time.
",4.2. Text and location-conditional generation,[0],[0]
"For CUB and MPII, we have body part keypoints for birds and humans, respectively.",4.2. Text and location-conditional generation,[0],[0]
"We encode these into a 32⇥32⇥ P binary feature map, where P is the number of parts; 17
for MPII and 15 for CUB.",4.2. Text and location-conditional generation,[0],[0]
"A 1 indicates the part is visible, and 0 indicates the part is not visible.",4.2. Text and location-conditional generation,[0],[0]
"For MS-COCO, we resize the class segmentation mask to 32 ⇥ 32 ⇥ 80.",4.2. Text and location-conditional generation,[0],[0]
"For comparing and replicating quantitative results, an important detail is the type of image resizing used.",4.2. Text and location-conditional generation,[0],[0]
"For CUB, MPII and MS-COCO we used the default TensorFlow bilinear interpolation resizing to the final image size, by calling tf.image.resize images.",4.2. Text and location-conditional generation,[0],[0]
"Using other resizing methods such as AREA resizing will still result in quality samples, but di↵erent likelihoods.
",4.2. Text and location-conditional generation,[0],[0]
"For all datasets, we then encode these spatial features using a 12-layer ResNet.",4.2. Text and location-conditional generation,[0],[0]
These features are then depthconcatenated with the text encoding and resized with bilinear interpolation to the spatial size of the image.,4.2. Text and location-conditional generation,[0],[0]
"If the target resolution for an upscaler network is higher than 32 ⇥ 32, these conditioning features are randomly cropped along with the target image to a 32⇥ 32 patch.",4.2. Text and location-conditional generation,[0],[0]
"Because the network is fully convolutional, the network can still generate the full resolution at test time, but we can massively save on memory and computation during training.
",4.2. Text and location-conditional generation,[0],[0]
Figure 4 shows examples of text- and keypoint-to-bird image synthesis.,4.2. Text and location-conditional generation,[0],[0]
Figure 5 shows examples of text- and keypoint-to-human image synthesis.,4.2. Text and location-conditional generation,[0],[0]
"Figure 6 shows examples of text- and segmentation-to-image synthesis.
",4.2. Text and location-conditional generation,[0],[0]
"Quantitatively, the Multiscale PixelCNN results are not far from those obtained using the original PixelCNN (Reed et al., 2016c), as shown in Table 1.",4.2. Text and location-conditional generation,[0],[0]
"In addition, we in-
creased the sample resolution by 8⇥.",4.2. Text and location-conditional generation,[0],[0]
"Qualitatively, the sample quality appears to be on par, but with much greater realism due to the higher resolution.
",4.2. Text and location-conditional generation,[0],[0]
"Based on reviewer feedback, we performed an additional experiment to study how much sample diversity arises from the upscaling networks, as opposed to the base PixelCNN.",4.2. Text and location-conditional generation,[0],[0]
"In the final appendix figure, we show for several 4 ⇥ 4 base images of birds, the resulting upscaled samples.",4.2. Text and location-conditional generation,[0],[0]
"Although the keypoints are fixed so the pose cannot change substantially, we observe a significant amount of variation in the textures, background, and support structure (e.g. tree branches and rocks that the bird stands on).",4.2. Text and location-conditional generation,[0],[0]
The low-res sample produced by the base PixelCNN seems to strongly a↵ect the overall colour palette in the hi-res samples.,4.2. Text and location-conditional generation,[0],[0]
In this section we present results on Robot Pushing videos.,4.3. Action-conditional video generation,[0],[0]
"All models were trained to perform future frame prediction conditioned on 2 starting frames and also on the robot arm actions and state, which are each 5-dimensional vectors.
",4.3. Action-conditional video generation,[0],[0]
"We trained two versions of the model, both versions using type-A upscaling networks (See Fig. 3).",4.3. Action-conditional video generation,[0],[0]
"The first is designed to sample in O(T ) time, for T video frames.",4.3. Action-conditional video generation,[0],[0]
"That is, the number of network evaluations per frame is constant with respect to the number of pixels.
",4.3. Action-conditional video generation,[0],[0]
"The motivation for training the O(T ) model is that previous frames in a video provide very detailed cues for predicting the next frame, so that our pixel groups could be conditionally independent even without access to a low-resolution
image.",4.3. Action-conditional video generation,[0],[0]
"Without the need to upscale from a low-resolution image, we can produce “group 1” pixels - i.e. the upper-left corner group - directly by conditioning on previous frames.",4.3. Action-conditional video generation,[0],[0]
"Then a constant number of network evaluations are needed to sample the next three pixel groups at the final scale.
",4.3. Action-conditional video generation,[0],[0]
"The second version is our multi-step upscaler used in previous experiments, conditioned on both previous frames and robot arm state and actions.",4.3. Action-conditional video generation,[0],[0]
"The complexity of sampling from this model is O(T log N), because at every time step the upscaling procedure must be run, taking O(log N) time.
",4.3. Action-conditional video generation,[0],[0]
"The models were trained for 200K steps with batch size 64, using the RMSprop optimizer with centering and ✏ = 1e 8.",4.3. Action-conditional video generation,[0],[0]
The learning rate was initialized to 1e 4 and decayed by factor 0.3 after 83K steps and after 113K steps.,4.3. Action-conditional video generation,[0],[0]
"For the O(T ) model we used a mixture of discretized logistic outputs (Salimans et al., 2017) and for the O(T log N) model we used a softmax ouptut.
",4.3. Action-conditional video generation,[0],[0]
Table 2 compares two variants of our model with the original VPN.,4.3. Action-conditional video generation,[0],[0]
Compared to the O(T ) baseline - a convolutional LSTM model without spatial dependencies - our O(T ) model performs dramatically better.,4.3. Action-conditional video generation,[0],[0]
"On the validation set, in which the model needs to generalize to novel combinations of objects and arm trajectories, the O(T log N) model does much better than our O(T ) model, although not as well as the original O(T N) model.
",4.3. Action-conditional video generation,[0],[0]
"On the testing sets, we observed that the O(T ) model performed as well as on the validation set, but the O(T log N) model showed a drop in performance.",4.3. Action-conditional video generation,[0],[0]
"However, this drop
does not occur due to the presence of novel objects (in fact this setting actually yields better results), but due to the novel arm and camera configuration used during testing 2.",4.3. Action-conditional video generation,[0],[0]
"It appears that the O(T log N) model may have overfit to the background details and camera position of the 10 training arms, but not necessarily to the actual arm and object motions.",4.3. Action-conditional video generation,[0],[0]
"It should be possible to overcome this e↵ect with better regularization and perhaps data augmentation such as mirroring and jittering frames, or simply training on data with more diverse camera positions.
",4.3. Action-conditional video generation,[0],[0]
The supplement contains example videos generated on the validation set arm trajectories from our O(T log N) model.,4.3. Action-conditional video generation,[0],[0]
We also trained 64 ! 128 and 128 !,4.3. Action-conditional video generation,[0],[0]
"256 upscalers conditioned on low-resolution and a previous high-resolution frame, so that we can produce 256 ⇥ 256 videos.",4.3. Action-conditional video generation,[0],[0]
"To compare against other image density models, we trained our Multiscale PixelCNN on ImageNet.",4.4. Class-conditional generation,[0],[0]
"We used type-B upscaling networks (Seee figure 3) with 12 ResNet (He et al., 2016) layers and 4 PixelCNN layers, with 256 hidden units per layer.",4.4. Class-conditional generation,[0],[0]
"For all PixelCNNs in the model, we used the same architecture as in (van den Oord et al., 2016b).",4.4. Class-conditional generation,[0],[0]
We generated images with a base resolution of 8 ⇥ 8 and trained four upscaling networks to produce up to 128⇥128 samples.,4.4. Class-conditional generation,[0],[0]
"At scales 64 ⇥ 64 and above, during training we randomly cropped the image to 32 ⇥ 32.",4.4. Class-conditional generation,[0],[0]
"This accelerates training but does not pose a problem at test time because
2From communication with the Robot Pushing dataset author.
",4.4. Class-conditional generation,[0],[0]
"all of the networks are fully convolutional.
",4.4. Class-conditional generation,[0],[0]
Table 3 shows the results.,4.4. Class-conditional generation,[0],[0]
"On both 32 ⇥ 32 and 64 ⇥ 64 ImageNet it achieves significantly better likelihood scores than have been reported for any non-pixel-autoregressive density models, such as ConvDRAW and Real NVP, that also allow e cient sampling.
",4.4. Class-conditional generation,[0],[0]
"Of course, performance of these approaches varies considerably depending on the implementation details, especially in the design and capacity of deep neural networks used.",4.4. Class-conditional generation,[0],[0]
"But it is notable that the very simple and direct approach developed here can surpass the state-of-the-art among fastsampling density models.
",4.4. Class-conditional generation,[0],[0]
"Interestingly, the model often produced quite realistic bird images from scratch when trained on CUB, and these samples looked more realistic than any animal image generated by our ImageNet models.",4.4. Class-conditional generation,[0],[0]
"One plausible explanation for this di↵erence is a lack of model capacity; a single network modeling the 1000 very diverse ImageNet categories can devote only very limited capacity to each one, compared to a network that only needs to model birds.",4.4. Class-conditional generation,[0],[0]
"This suggests that finding ways to increase capacity without slowing down training or sampling could be a promising direction.
",4.4. Class-conditional generation,[0],[0]
"Figure 7 shows upscaling starting from ground-truth images of size 8⇥8, 16⇥16 and 32⇥32.",4.4. Class-conditional generation,[0],[0]
"We observe the largest diversity of samples in terms of global structure when starting from 8 ⇥ 8, but less realistic results due to the more challenging nature of the problem.",4.4. Class-conditional generation,[0],[0]
Upscaling starting from 32 ⇥ 32 results in much more realistic images.,4.4. Class-conditional generation,[0],[0]
"Here the diversity is apparent in the samples (as in the data, conditioned on low-resolution) in the local details such as the dog’s fur patterns or the frog’s eye contours.",4.4. Class-conditional generation,[0],[0]
"As expected, we observe a very large speedup of our model compared to sampling from a standard PixelCNN at the same resolution (see Table 4).",4.5. Sampling time comparison,[0],[0]
"Even at 32 ⇥ 32 we observe two orders of magnitude speedup, and the speedup is greater for higher resolution.
",4.5. Sampling time comparison,[0],[0]
"Since our model only requires O(log N) network evaluations to sample, we can fit the entire computation graph for sampling into memory, for reasonable batch sizes.",4.5. Sampling time comparison,[0],[0]
"Ingraph computation in TensorFlow can further improve the speed of both image and video generation, due to reduced overhead by avoiding repeated calls to sess.run.
Since our model has a PixelCNN at the lowest resolution, it can also be accelerated by caching PixelCNN hidden unit activations, recently implemented b by Ramachandran et al. (2017).",4.5. Sampling time comparison,[0],[0]
This could allow one to use higher-resolution base PixelCNNs without sacrificing speed.,4.5. Sampling time comparison,[0],[0]
"In this paper, we developed a parallelized, multiscale version of PixelCNN.",5. Conclusions,[0],[0]
"It achieves competitive density estimation results on CUB, MPII, MS-COCO, ImageNet, and Robot Pushing videos, surpassing all other density models that admit fast sampling.",5. Conclusions,[0],[0]
"Qualitatively, it can achieve compelling results in text-to-image synthesis and video generation, as well as diverse super-resolution from very small images all the way to 512⇥ 512.",5. Conclusions,[0],[0]
Many more samples from all of our models can be found in the appendix and supplementary material.,5. Conclusions,[0],[0]
PixelCNN achieves state-of-the-art results in density estimation for natural images.,abstractText,[0],[0]
"Although training is fast, inference is costly, requiring one network evaluation per pixel; O(N) for N pixels.",abstractText,[0],[0]
"This can be sped up by caching activations, but still involves generating each pixel sequentially.",abstractText,[0],[0]
"In this work, we propose a parallelized PixelCNN that allows more e cient inference by modeling certain pixel groups as conditionally independent.",abstractText,[0],[0]
Our new PixelCNN model achieves competitive density estimation and orders of magnitude speedup O(log N) sampling instead of O(N) enabling the practical generation of 512⇥ 512 images.,abstractText,[0],[0]
"We evaluate the model on class-conditional image generation, text-toimage synthesis, and action-conditional video generation, showing that our model achieves the best results among non-pixel-autoregressive density models that allow e cient sampling.",abstractText,[0],[0]
Parallel Multiscale Autoregressive Density Estimation,title,[0],[0]
"Recent successes of deep learning go beyond achieving state-of-the-art results in research benchmarks, and push the frontiers in some of the most challenging real world applications such as speech recognition (Hinton et al., 2012), image recognition (Krizhevsky et al., 2012; Szegedy et al., 2015), and machine translation (Wu et al., 2016).",1. Introduction,[0],[0]
"The recently published WaveNet (van den Oord et al., 2016a) model achieves state-of-the-art results in speech synthesis, and significantly closes the gap with natural human speech.",1. Introduction,[0],[0]
"However, it is not well suited for real world deployment due to its prohibitive generation speed.",1. Introduction,[0],[0]
"In this paper, we present a new algorithm for distilling WaveNet into a feed-forward neural
1DeepMind Technologies, London, United Kingdom.",1. Introduction,[0],[0]
"Correspondence to: Aaron van den Oord <avdnoord@google.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
network which can synthesise equally high quality speech much more efficiently, and is deployed to millions of users.
",1. Introduction,[0],[0]
"WaveNet is one of a family of autoregressive deep generative models that have been applied with great success to data as diverse as text (Mikolov et al., 2010), images (Larochelle & Murray, 2011; Theis & Bethge, 2015; van den Oord et al., 2016c;b), video (Kalchbrenner et al., 2016), handwriting (Graves, 2013) as well as human speech and music.",1. Introduction,[0],[0]
"Modelling raw audio signals, as WaveNet does, represents a particularly extreme form of autoregression, with up to 24,000 samples predicted per second.",1. Introduction,[0],[0]
"Operating at such a high temporal resolution is not problematic during network training, where the complete sequence of input samples is already available and—thanks to the convolutional structure of the network—can be processed in parallel.",1. Introduction,[0],[0]
"When generating samples, however, each input sample must be drawn from the output distribution before it can be passed in as input at the next time step, making parallel processing impossible.
",1. Introduction,[0],[0]
"Inverse autoregressive flows (IAFs) (Kingma et al., 2016) represent a kind of dual formulation of deep autoregressive modelling, in which sampling can be performed in parallel, while the inference procedure required for likelihood estimation is sequential and slow.",1. Introduction,[0],[0]
The goal of this paper is to marry the best features of both models: the efficient training of WaveNet and the efficient sampling of IAF networks.,1. Introduction,[0],[0]
"The bridge between them is a new form of neural network distillation (Hinton et al., 2015), which we refer to as Probability Density Distillation, where a trained WaveNet model is used as a teacher for a feedforward IAF model.
",1. Introduction,[0],[0]
"The next section describes the original WaveNet model, while Sections 3 and 4 define in detail the new, parallel version of WaveNet and the distillation process used to transfer knowledge between them.",1. Introduction,[0],[0]
"Section 5 then presents experimental results showing no loss in perceived quality for parallel versus original WaveNet, and continued superiority over previous benchmarks.",1. Introduction,[0],[0]
"We also present timings for sample generation, demonstrating more than 1000× speedup relative to original WaveNet.",1. Introduction,[0],[0]
"Autoregressive networks model the joint distribution of highdimensional data as a product of conditional distributions using the probabilistic chain-rule:
p(x) = ∏ t p(xt|x<t,θ),
where xt is the t-th variable of x and θ are the parameters of the autoregressive model.",2. WaveNet,[0],[0]
"The conditional distributions are usually modelled with a neural network that receives x<t as input and outputs a distribution over possible xt.
",2. WaveNet,[0],[0]
"WaveNet (van den Oord et al., 2016a) is a convolutional autoregressive model which produces all p(xt|x<t) in one forward pass, by making use of causal—or masked— convolutions (van den Oord et al., 2016c; Germain et al., 2015).",2. WaveNet,[0],[0]
"Every causal convolutional layer can process its input in parallel, making these architectures very fast to train compared to RNNs (van den Oord et al., 2016b), which can only be updated sequentially.",2. WaveNet,[0],[0]
"At generation time, however, the waveform has to be synthesised in a sequential fashion as xt must be sampled first in order to obtain x>t. Due to this nature, real time (or faster) synthesis with a fully autoregressive system is challenging.",2. WaveNet,[0],[0]
"While sampling speed is not a significant issue for offline generation, it is essential for real-word applications.",2. WaveNet,[0],[0]
"A version of WaveNet that generates in real-time has been developed (Paine et al., 2016), but it required the use of a much smaller network, resulting in severely degraded quality.
",2. WaveNet,[0],[0]
"Raw audio data is typically very high-dimensional (e.g. 16,000 samples per second for 16kHz audio), and contains complex, hierarchical structures spanning many thousands of time steps, such as words in speech or melodies in music.",2. WaveNet,[0],[0]
Modelling such long-term dependencies with standard causal convolution layers would require a very deep network to ensure a sufficiently broad receptive field.,2. WaveNet,[0],[0]
"WaveNet avoids this constraint by using dilated causal convolutions, which allow the receptive field to grow exponentially with depth.
",2. WaveNet,[0],[0]
"WaveNet uses gated activation functions, together with a simple mechanism introduced in (van den Oord et al., 2016c) to condition on extra information such as class labels or linguistic features:
hi = σ",2. WaveNet,[0],[0]
"( Wg,i ∗ xi + V Tg,ic )",2. WaveNet,[0],[0]
"tanh ( Wf,i ∗ xi + V Tf,ic ) ,
(1) where ∗ denotes a convolution operator, and denotes an element-wise multiplication operator.",2. WaveNet,[0],[0]
σ(·) is a logistic sigmoid function.,2. WaveNet,[0],[0]
c represents extra conditioning data.,2. WaveNet,[0],[0]
i is the layer index.,2. WaveNet,[0],[0]
"f and g denote filter and gate, respectively.",2. WaveNet,[0],[0]
W and V are learnable weights.,2. WaveNet,[0],[0]
"In cases where c encodes spatial or sequential information (such as a sequence of linguistic features), the matrix products (V Tf,ic and V T g,ic) are replaced by convolutions (Vf,i ∗ c and Vg,i ∗ c).",2. WaveNet,[0],[0]
For this work we made two improvements to the basic WaveNet model to enhance its audio quality for production use.,2.1. Higher Fidelity WaveNet,[0],[0]
"Unlike previous versions of WaveNet (van den Oord et al., 2016a), where 8-bit (µ-law or PCM) audio was modelled with a 256-way categorical distribution, we increased the fidelity by modelling 16-bit audio.",2.1. Higher Fidelity WaveNet,[0],[0]
"Since training a 65,536-way categorical distribution would be prohibitively costly, we instead modelled the samples with the discretized mixture of logistics distribution introduced in (Salimans et al., 2017).",2.1. Higher Fidelity WaveNet,[0],[0]
We further improved fidelity by increasing the audio sampling rate from 16kHz to 24kHz.,2.1. Higher Fidelity WaveNet,[0],[0]
"This required a WaveNet with a wider receptive field, which we achieved by increasing the dilated convolution filter size from 2 to 3.",2.1. Higher Fidelity WaveNet,[0],[0]
An alternative strategy would be to increase the number of layers or add more dilation stages.,2.1. Higher Fidelity WaveNet,[0],[0]
"While the convolutional structure of WaveNet allows for rapid parallel training, sample generation remains inherently sequential and therefore slow, as it is for all autoregressive models which use ancestral sampling.",3. Parallel WaveNet,[0],[0]
"We therefore seek an alternative architecture that will allow for rapid, parallel generation.
",3. Parallel WaveNet,[0],[0]
"Inverse-autoregressive flows (IAFs) (Kingma et al., 2016) are stochastic generative models whose latent variables are arranged so that all elements of a high dimensional observable sample can be generated in parallel.",3. Parallel WaveNet,[0],[0]
"IAFs are a special type of normalising flow (Dinh et al., 2014; Rezende & Mohamed, 2015; Dinh et al., 2016) which model a multivariate distribution pX(x) as an explicit invertible non-linear transformation x = f(z) of a simple tractable distribution pZ(z) (such as an isotropic Gaussian distribution).",3. Parallel WaveNet,[0],[0]
"Using the change of variables formula the resulting distribution can be written as:
log pX(x) = log pZ(z)− log ∣∣∣dx dz ∣∣∣, where ∣∣dx dz
∣∣ is the determinant of the Jacobian of f .",3. Parallel WaveNet,[0],[0]
For all normalizing flows the transformation f is chosen so that it is invertible and its Jacobian determinant is easy to compute.,3. Parallel WaveNet,[0],[0]
"In the case of an IAF, the output is modelled by xt = f(z≤t).",3. Parallel WaveNet,[0],[0]
"Because of this strict dependency structure, the transformation has a triangular Jacobian matrix which makes the determinant equal to the product of the diagonal entries:
log ∣∣∣dx dz ∣∣∣ =∑ t log ∂f(z≤t) ∂zt .
",3. Parallel WaveNet,[0],[0]
"To sample from an IAF, a random sample is first drawn from z ∼ pZ(z) (we use the Logistic(0, I) distribution) which
is then transformed as follows:
xt = zt · s(z<t,θ) + µ(z<t,θ), (2)
where µ and s are outputs by the network.",3. Parallel WaveNet,[0],[0]
"Therefore, p(xt|z<t) follows a logistic distribution parameterised by µt and st.
",3. Parallel WaveNet,[0],[0]
"p(xt|z<t,θ) = L ( xt ∣∣µ(z<t,θ), s(z<t,θ)) ,
While µ(z<t,θ) and s(z<t,θ) can be any model, we use the same convolutional network structure as the original WaveNet (van den Oord et al., 2016a).
",3. Parallel WaveNet,[0],[0]
"Autoregressive models (or flows (Papamakarios et al., 2017)) model the data as p(xt|x<t) and IAFs as p(xt|z<t).",3. Parallel WaveNet,[0],[0]
"If these models share the same output distribution class (e.g., mixture of logistics or categorical) then mathematically they should be able to model the same multivariate distributions.",3. Parallel WaveNet,[0],[0]
"However, in practice there are some differences (see Section 3.1).",3. Parallel WaveNet,[0],[0]
"To output the correct distribution for timestep xt, the inverse autoregressive flow can implicitly infer what it would have output at previous timesteps x1, . . .",3. Parallel WaveNet,[0],[0]
", xt−1 based on the noise inputs z1, . . .",3. Parallel WaveNet,[0],[0]
", zt−1, which allows it to output all xt in parallel given zt.
",3. Parallel WaveNet,[0],[0]
"In general, normalising flows might require repeated iterations to transform uncorrelated noise into structured samples, with the output generated by the flow at each iteration passed in as input at the next (Rezende & Mohamed, 2015).",3. Parallel WaveNet,[0],[0]
"This is less crucial for IAFs, as the autoregressive latents can induce significant structure in a single pass.",3. Parallel WaveNet,[0],[0]
"Nonetheless we observed that having up to 4 flow iterations did improve the quality (the weights are not shared between the flows).
",3. Parallel WaveNet,[0],[0]
The first (bottom) network takes as input the white unconditional logistic noise: z0.,3. Parallel WaveNet,[0],[0]
"Thereafter the output of each network i is passed as input to the next network i+1 , which again transforms it.
",3. Parallel WaveNet,[0],[0]
zi = zi−1 ·,3. Parallel WaveNet,[0],[0]
"si + µi (3)
",3. Parallel WaveNet,[0],[0]
"Because we use the same ordering in all the flows, the final distribution p(xt|z<t,θ) is still logistic with location µtot and scale stot:
µtot =",3. Parallel WaveNet,[0],[0]
N∑ i µi  N∏,3. Parallel WaveNet,[0],[0]
"j>i sj  (4) stot =
N∏",3. Parallel WaveNet,[0],[0]
"i si (5)
where N is the number of flows and the dependencies on t and z are omitted for simplicity.",3. Parallel WaveNet,[0],[0]
"Although inverse-autoregressive flows (IAFs) and autoregressive models can in principle model the same distributions (Chen et al., 2016), they have different inductive biases and may vary greatly in their capacity to model certain processes.",3.1. Autoregressive Models and Inverse-autoregressive Flows,[0],[0]
"As a simple example consider the Fibonacci series (1, 1, 2, 3, 5, 8, 13, . . . ).",3.1. Autoregressive Models and Inverse-autoregressive Flows,[0],[0]
For an autoregressive model this is easy to model with a receptive field of two: f(k) =,3.1. Autoregressive Models and Inverse-autoregressive Flows,[0],[0]
f(k − 1) + f(k − 2).,3.1. Autoregressive Models and Inverse-autoregressive Flows,[0],[0]
"For an IAF, however, the receptive field needs to be at least size k to correctly model k terms, leading to a larger model that is less able to generalise.",3.1. Autoregressive Models and Inverse-autoregressive Flows,[0],[0]
"Training the parallel WaveNet model directly with maximum likelihood would be impractical, as the inference procedure required to estimate the log-likelihoods is sequential and slow1.",4. Probability Density Distillation,[0],[0]
"We therefore introduce a novel form of neural network distillation (Hinton et al., 2015) that uses an al-
1In this sense the two architectures are dual to one another: slow training and fast generation with parallel WaveNet versus fast training and slow generation with WaveNet.
",4. Probability Density Distillation,[0],[0]
ready trained WaveNet as a ‘teacher’ from which a parallel WaveNet ‘student’ can efficiently learn.,4. Probability Density Distillation,[0],[0]
"To stress the fact that we are dealing with normalised density models, we refer to this process as Probability Density Distillation (in contrast to Probability Density Estimation).",4. Probability Density Distillation,[0],[0]
"The basic idea is for the student to attempt to match the probability of its own samples under the distribution learned by the teacher.
",4. Probability Density Distillation,[0],[0]
"Given a parallel WaveNet student pS(x) and WaveNet teacher pT (x) which has been trained on a dataset of audio, we define the Probability Density Distillation loss as follows:
DKL (PS ||PT ) =",4. Probability Density Distillation,[0],[0]
"H(PS , PT )−H(PS) (6)
where DKL is the Kullback–Leibler divergence, and H(PS , PT ) is the cross-entropy between the student PS and teacher PT , and H(PS) is the entropy of the student distribution.",4. Probability Density Distillation,[0],[0]
"When the KL divergence becomes zero, the student distribution has fully recovered the teacher’s distribution.",4. Probability Density Distillation,[0],[0]
"The entropy term (which is not present in previous distillation objectives (Hinton et al., 2015)) is vital in that it prevents the student’s distribution from collapsing to the mode of the teacher (which, counter-intuitively, does not yield a good sample—see Section 4.1).",4. Probability Density Distillation,[0],[0]
"Crucially, all the operations required to estimate derivatives for this loss (sampling from pS(x), evaluating pT (x), and evaluating H(PS)) can be performed efficiently, as we will see.
",4. Probability Density Distillation,[0],[0]
"It is worth noting the parallels to Generative Adversarial Networks (GANs (Goodfellow et al., 2014)), with the student playing the role of generator, and the teacher playing the role of discriminator.",4. Probability Density Distillation,[0],[0]
"As opposed to GANs, however, the student is not attempting to fool the teacher in an adversarial manner; rather it co-operates by attempting to match the teacher’s probabilities.",4. Probability Density Distillation,[0],[0]
"Furthermore the teacher is held constant, rather than being trained in tandem with the student, and both models yield tractable normalised distributions.
",4. Probability Density Distillation,[0],[0]
"Recently (Gu et al., 2017) has presented a related idea to train feed-forward networks for neural machine translation.",4. Probability Density Distillation,[0],[0]
"Their method is based on conditioning the feedforward decoder on fertility values, which require supervision by an external alignment system.",4. Probability Density Distillation,[0],[0]
The training procedure also involves the creation of an additional dataset as well as finetuning.,4. Probability Density Distillation,[0],[0]
"During inference, their model relies on re-scoring by an auto-regressive model.
",4. Probability Density Distillation,[0],[0]
"First, observe that the entropy term H(PS) in Equation 6 can be rewritten as follows:
H(PS) = E z∼L(0,1)
",4. Probability Density Distillation,[0],[0]
"[ T∑
t=1
− ln pS(xt|z<t) ]
(7)
",4. Probability Density Distillation,[0],[0]
"= E z∼L(0,1)
",4. Probability Density Distillation,[0],[0]
"[ T∑
t=1
ln s(z<t,θ)
]",4. Probability Density Distillation,[0],[0]
"+ 2T, (8)
wherex = g(z)",4. Probability Density Distillation,[0],[0]
and zt are independent samples drawn from the logistic distribution.,4. Probability Density Distillation,[0],[0]
"The second equality in Equation 8 follows because the entropy of a logistic distribution L(µ, s) is ln s + 2.",4. Probability Density Distillation,[0],[0]
"We can therefore compute this term without having to explicitly generate x.
The cross-entropy term H(PS , PT ) however explicitly depends on x = g(z), and therefore requires sampling from the student to estimate.
",4. Probability Density Distillation,[0],[0]
"H(PS , PT ) = ∫ x pS(x) ln pT (x) (9)
",4. Probability Density Distillation,[0],[0]
= T∑ t=1 ∫ x pS(x) ln pT,4. Probability Density Distillation,[0],[0]
"(xt|x<t) (10)
=",4. Probability Density Distillation,[0],[0]
T∑ t=1 ∫,4. Probability Density Distillation,[0],[0]
x,4. Probability Density Distillation,[0],[0]
pS(x<t)pS(x≥t|x<t),4. Probability Density Distillation,[0],[0]
ln pT,4. Probability Density Distillation,[0],[0]
"(xt|x<t) (11)
",4. Probability Density Distillation,[0],[0]
= T∑ t=1,4. Probability Density Distillation,[0],[0]
"E pS(x<t) [ ∫ xt
pS(xt|x<t) ln pT",4. Probability Density Distillation,[0],[0]
"(xt|x<t) (12)∫ x>t pS(x>t|x≤t) ] (13)
",4. Probability Density Distillation,[0],[0]
= T∑ t=1,4. Probability Density Distillation,[0],[0]
"E pS(x<t) H ( pS(xt|x<t), pT (xt|x<t) ) .",4. Probability Density Distillation,[0],[0]
"(14)
= T∑ t=1 E z∼L
x=g(z)
H ( pS(xt|z<t), pT (xt|x<t) ) .",4. Probability Density Distillation,[0],[0]
"(15)
In Equation 11 we apply the chain rule to mathematicaly decompose PS(x) into conditional distributions but they are only explicitly constructed with a neural network to depend on z as in Equation 15.
",4. Probability Density Distillation,[0],[0]
"For every sample x we draw from the student pS we can compute all pT (xt|x<t) in parallel with the teacher and then evaluate H(pS(xt|z<t), pT (xt|x<t)) very efficiently by drawing multiple different samples xt from pS(xt|z<t) for each timestep.",4. Probability Density Distillation,[0],[0]
"This unbiased estimator has a much lower variance than naively evaluating the sample under the teacher with Equation 9.
",4. Probability Density Distillation,[0],[0]
"We parameterise the teacher’s output distribution pT (xt|x<t) as a mixture of logistics distribution (Salimans et al., 2017), which allows the loss term ln pT (xt|x<t) to be differentiable with respect to both xt and x<t.",4. Probability Density Distillation,[0],[0]
"A categorical distribution, on the other hand, would only be differentiable w.r.t.",4. Probability Density Distillation,[0],[0]
x<t.,4. Probability Density Distillation,[0],[0]
"In this section we make an argument against maximum a posteriori (MAP) estimation for distillation; similar arguments have been made by previous authors in a different setting (Sønderby et al., 2016).",4.1. Argument against MAP estimation,[0],[0]
The distillation loss defined in Section 4 minimises the KL divergence between the teacher and generator.,WaveNet Teacher,[0],[0]
"We could instead have minimised only the cross-entropy between the teacher and generator (the standard distillation loss term (Hinton et al., 2015)), so that the samples by the generator are as likely as possible according to the teacher.",WaveNet Teacher,[0],[0]
Doing so would give rise to MAP estimation.,WaveNet Teacher,[0],[0]
"Counterintuitively, audio samples obtained through MAP estimation do not sound as good as typical examples from the teacher: in fact they are almost completely silent, even if using conditional information such as linguistic features.",WaveNet Teacher,[0],[0]
"This effect is not due to adversarial behaviour on the part of the teacher, but rather is a fundamental property of the data distribution which the teacher has approximated.
",WaveNet Teacher,[0],[0]
"As an example consider the simple case where we have audio from a white random noise source: the distribution at every timestep is N (0, 1), regardless of the samples at previous timesteps.",WaveNet Teacher,[0],[0]
White noise has a very specific and perceptually recognizable sound: a continual hiss.,WaveNet Teacher,[0],[0]
"The MAP estimate of this data distribution, and thus of any generative model that matches it well, recovers the distribution mode, which is 0 at every timestep: i.e. complete silence.",WaveNet Teacher,[0],[0]
"More generally, any highly stochastic process is liable to have a ‘noiseless’ and therefore atypical mode.",WaveNet Teacher,[0],[0]
For the KL divergence the optimum is to recover the full teacher distribution.,WaveNet Teacher,[0],[0]
This is clearly different from any random sample from the distribution.,WaveNet Teacher,[0],[0]
"Furthermore, if one changes the representation of the data (e.g., by nonlinearly pre-processing the audio signal), then the MAP estimate changes, unlike the KL-divergence in Equation 6, which is invariant to the coordinate system.",WaveNet Teacher,[0],[0]
Training with Probability Density Distillation alone might not sufficiently constrain the student to generate high quality audio streams.,4.2. Additional loss terms,[0],[0]
"Therefore, we also introduce additional loss functions to guide the student distribution towards the desired output space.",4.2. Additional loss terms,[0],[0]
"The first additional loss we propose is the power loss, which ensures that the power in different frequency bands of the speech are on average used as much as in human speech.",POWER LOSS,[0],[0]
"The power loss helps to avoid the student from collapsing to a high-entropy WaveNet-mode, such as whispering.
",POWER LOSS,[0],[0]
"The power-loss is defined as:
||φ(g(z, c))− φ(y)||2, (16)
where (y, c) is an example with conditioning from the training set, φ(x) = |STFT(x)|2 and STFT stands for the ShortTerm Fourier Transform.",POWER LOSS,[0],[0]
"We found that φ(x) can be averaged over time before taking the Euclidean distance with little difference in effect, which means it is the average power for various frequencies that is important.",POWER LOSS,[0],[0]
"In the power loss formulation given in equation 16, one can also use a neural network instead of the STFT to conserve a perceptual property of the signal rather than total energy.",PERCEPTUAL LOSS,[0],[0]
In our case we have used a WaveNet-like classifier trained to predict the phones from raw audio.,PERCEPTUAL LOSS,[0],[0]
"Because such a classifier naturally extracts high-level features that are relevant for
recognising the phones, this loss term penalises bad pronunciations.",PERCEPTUAL LOSS,[0],[0]
"A similar principle has been used in computer vision for artistic style transfer (Gatys et al., 2015), or to get better perceptual reconstruction losses, e.g., in superresolution (Johnson et al., 2016).
",PERCEPTUAL LOSS,[0],[0]
"We have experimented with two different ways of using the perceptual loss, the feature reconstruction loss (the Euclidean distance between feature maps in the classifier) and the style loss (the Euclidean distance between the Gram matrices (Johnson et al., 2016)).",PERCEPTUAL LOSS,[0],[0]
The latter produced better results in our experiments.,PERCEPTUAL LOSS,[0],[0]
"Finally, we also introduce a contrastive distillation loss as follows:
DKL ( PS(c1)",CONTRASTIVE LOSS,[0],[0]
"∣∣∣∣∣∣PT (c1))−γDKL(PS(c1)∣∣∣∣∣∣PT c2)), (17) which minimises the KL-divergence between the teacher and student when both are conditioned on the same information c1 (e.g., linguistic features, speaker ID, . . . ), but also maximises it for different conditioning pairs c1 6= c2.",CONTRASTIVE LOSS,[0],[0]
"In order to implement this loss, we use the output of the student x = g(z, c1) and evaluate the waveform twice under the teacher: once with the same conditioning PT (x|c1) and once with a randomly sampled conditioning input: PT (x|c2).",CONTRASTIVE LOSS,[0],[0]
The weight for the contrastive term γ was set to 0.3 in our experiments.,CONTRASTIVE LOSS,[0],[0]
The contrastive loss penalises waveforms that have high likelihood regardless of the conditioning vector.,CONTRASTIVE LOSS,[0],[0]
"In all our experiments we used text-to-speech models that were conditioned on linguistic features (similar to (van den Oord et al., 2016a)), providing phonetic and duration information to the network.",5. Experiments,[0],[0]
"We also conditioned the models
on pitch information (logarithm of f0, the fundamental frequency) predicted by a different model.",5. Experiments,[0],[0]
"We never used ground-truth information (such as pitch or duration) extracted from human speech for generating audio samples and the test sentences were not present (or similar to those) in the training set.
",5. Experiments,[0],[0]
"The teacher WaveNet network was trained for 1,000,000 steps with the ADAM optimiser (Kingma & Ba, 2014) with a minibatch size of 32 audio clips, each containing 7,680 timesteps (roughly 320ms).",5. Experiments,[0],[0]
"Remarkably, a relatively short snippet of time is sufficient to train the parallel WaveNet to produce long term coherent waveforms.",5. Experiments,[0],[0]
"The learning rate was held constant at 2×10−4, and Polyak averaging (Polyak & Juditsky, 1992) was applied over the parameters.",5. Experiments,[0],[0]
"The model consists of 30 layers, grouped into 3 dilated residual block stacks of 10 layers.",5. Experiments,[0],[0]
"In every stack, the dilation rate increases by a factor of 2 in every layer, starting with rate 1 (no dilation) and reaching the maximum dilation of 512 in the last layer.",5. Experiments,[0],[0]
The filter size of causal dilated convolutions is 3.,5. Experiments,[0],[0]
The number of hidden units in the gating layers is 512 (split into two groups of 256 for the two parts of the activation function (1)).,5. Experiments,[0],[0]
"The number of hidden units in the residual connection is 512, and in the skip connection and the 1× 1 convolutions before the output layer is also 256.",5. Experiments,[0],[0]
"We used 10 mixture components for the mixture of logistics output distribution.
",5. Experiments,[0],[0]
"The student network consisted of the same WaveNet architecture layout, except with different inputs and outputs and no skip connections.",5. Experiments,[0],[0]
"The student was also trained for 1,000,000 steps with the same optimisation settings.",5. Experiments,[0],[0]
"The student typically consisted of 4 flows with 10, 10, 10, 30 layers respectively, with 64 hidden units for the residual and gating layers.",5. Experiments,[0],[0]
We have benchmarked the sampling speed of autoregressive and distilled WaveNets on an NVIDIA P100 GPU.,AUDIO GENERATION SPEED,[0],[0]
"Both models were implemented in Tensorflow (Abadi et al., 2016) and compiled with XLA.",AUDIO GENERATION SPEED,[0],[0]
"The hidden layer activations from previous timesteps in the autoregressive model were cached with circular buffers (Paine et al., 2016).",AUDIO GENERATION SPEED,[0],[0]
The resulting sampling speed with this implementation is 172 timesteps/second for a minibatch of size 1.,AUDIO GENERATION SPEED,[0],[0]
"The distilled model, which is more parallelizable, achieves over 500,000 timesteps/second with same batch size of 1, resulting in three orders of magnitude speed-up.",AUDIO GENERATION SPEED,[0],[0]
"In our first set of experiments, we looked at the quality of WaveNet distillation compared to the autoregressive WaveNet teacher and other baselines on data from a professional female speaker (van den Oord et al., 2016a).",AUDIO FIDELITY,[0],[0]
"Table
1 gives a comparison of autoregressive WaveNet, distilled WaveNet and current production systems in terms of mean opinion score (MOS).",AUDIO FIDELITY,[0],[0]
"There is no difference between MOS scores of the distilled WaveNet (4.41±0.08) and autoregressive WaveNet (4.41±0.07), and both are significantly better than the concatenative unit-selection baseline (4.19± 0.1).",AUDIO FIDELITY,[0],[0]
"It is also important to note that the difference in MOS scores of our WaveNet baseline result 4.41 compared to the previous reported result 4.21 (van den Oord et al., 2016a) is due to the improvement in audio fidelity as explained in Section 2.1: modelling a sample rate of 24kHz instead of 16kHz and bit-depth of 16-bit PCM instead of 8-bit µ-law.",AUDIO FIDELITY,[0],[0]
By conditioning on the speaker-ids we can construct a single parallel WaveNet model that is able to generate multiple speakers’ voices and their accents.,MULTI-SPEAKER GENERATION,[0],[0]
These networks require slightly more capacity than single speaker models and thus had 30 layers in each flow.,MULTI-SPEAKER GENERATION,[0],[0]
In Table 2 we show a comparison of such a distilled parallel WaveNet model with two main baselines: a parametric and a concatenative system.,MULTI-SPEAKER GENERATION,[0],[0]
"In the comparison, we use a number of English speakers from a single model (one of them, English speaker 1, is the same speaker as in Table 1) and a Japanese speaker from another model.",MULTI-SPEAKER GENERATION,[0],[0]
"For some speakers, the concatenative system gets better results than the parametric system, while for other speakers it is the opposite.",MULTI-SPEAKER GENERATION,[0],[0]
"The parallel WaveNet model, on the other hand, significantly outperforms both baselines for all the speakers.",MULTI-SPEAKER GENERATION,[0],[0]
To analyse the importance of the loss functions introduced in Section 4.2 we show how the quality of the distilled WaveNet changes with different loss functions in Table 3 (top).,ABLATION STUDIES,[0],[0]
We found that MOS scores of these models tend to be very similar to each other (and similar to the result in Table 1).,ABLATION STUDIES,[0],[0]
"Therefore, we report subjective preference scores from a paired comparison test (“A/B test”), which we found to be more reliable for noticing small (sometimes qualitative) differences.",ABLATION STUDIES,[0],[0]
"In these tests, the subjects were
asked to listen to a pair of samples and choose which they preferred, though they could choose “neutral” if they did not have any preference.
",ABLATION STUDIES,[0],[0]
"As mentioned before, the KL loss alone does not constrain the distillation process enough to obtain natural sounding speech (e.g., low-volume audio suffices for the KL), therefore we do not report preference scores with only this term.",ABLATION STUDIES,[0],[0]
The KL loss (section 4) combined with power-loss is enough to generate quite natural speech.,ABLATION STUDIES,[0],[0]
Adding the perceptual loss gives a small but noticeable improvement.,ABLATION STUDIES,[0],[0]
"Adding the contrastive loss does not improve the preference scores any further, but makes the generated speech less noisy, which is something most raters do not pay attention to, but is important for production quality speech.
",ABLATION STUDIES,[0],[0]
"As explained in Section 3, we use multiple inverseautoregressive flows in the parallel WaveNet architecture: A model with a single flow gets a MOS score of 4.21, compared to a MOS score of 4.41 for models with multiple flows.",ABLATION STUDIES,[0],[0]
"In this paper we have introduced a novel method for highfidelity speech synthesis based on WaveNet (van den Oord et al., 2016a) using Probability Density Distillation.",6. Conclusion,[0],[0]
The proposed model achieved several orders of magnitude speed-up compared to the original WaveNet with no significant difference in quality.,6. Conclusion,[0],[0]
"Moreover, we have successfully transferred
this algorithm to new languages and multiple speakers.
",6. Conclusion,[0],[0]
"As a result, we have been able to run a real-time speech synthesis system, opening the door to many exciting future developments thanks to the flexibility of deep learning models.",6. Conclusion,[0],[0]
We believe that the same method presented here can be used in many different domains to achieve similar speed improvements whilst maintaining output accuracy.,6. Conclusion,[0],[0]
"The recently-developed WaveNet architecture (van den Oord et al., 2016a) is the current state of the art in realistic speech synthesis, consistently rated as more natural sounding for many different languages than any previous system.",abstractText,[0],[0]
"However, because WaveNet relies on sequential generation of one audio sample at a time, it is poorly suited to today’s massively parallel computers, and therefore hard to deploy in a real-time production setting.",abstractText,[0],[0]
"This paper introduces Probability Density Distillation, a new method for training a parallel feed-forward network from a trained WaveNet with no significant difference in quality.",abstractText,[0],[0]
"The resulting system is capable of generating high-fidelity speech samples at more than 20 times faster than real-time, a 1000x speed up relative to the original WaveNet, and capable of serving multiple English and Japanese voices in a production setting.",abstractText,[0],[0]
Parallel WaveNet: Fast High-Fidelity Speech Synthesis,title,[0],[0]
Problem Definition and Motivation.,1. Introduction,[0],[0]
"We consider the matrix completion problem, in which we are given a matrix M (over some field that we also refer to as the domain of the matrix) with missing entries, and the goal is to complete the entries of M so that to optimize a certain measure.",1. Introduction,[0],[0]
"There is a wealth of research on this fundamental problem (Candès & Plan, 2010; Candès & Recht, 2009; Candès & Tao, 2010; Elhamifar & Vidal, 2013; Hardt et al., 2014; Fazel, 2002; Keshavan et al., 2010a;b; Recht, 2011; Saunderson et al., 2016) due to its ubiquitous applications in recommender systems, machine learning, sensing, computer vision, data science, and predictive analytics, among others.",1. Introduction,[0],[0]
"In these areas, the matrix completion problem naturally arises after
*Equal contribution 1Algorithms and Complexity Group, TU Wien, Vienna, Austria 2School of Computing, DePaul University, Chicago, USA 3Algorithms Group, University of Sheffield, Sheffield, UK.",1. Introduction,[0],[0]
"Correspondence to: Robert Ganian <rganian@gmail.com>, Iyad Kanj <ikanj@cdm.depaul.edu>, Sebastian Ordyniak <sordyniak@gmail.com>, Stefan Szeider <stefan@szeider.net>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
observing a sample from the set of entries of a low-rank matrix, and attempting to recover the missing entries with the goal of optimizing a certain measure.",1. Introduction,[0],[0]
"In this paper, we focus our study on matrix completion with respect to two measures (considered separately): (1) minimizing the rank of the completed matrix, and (2) minimizing the number of distinct rows of the completed matrix.
",1. Introduction,[0],[0]
The first problem we consider—matrix completion w.r.t.,1. Introduction,[0],[0]
"rank minimization—has been extensively studied, and is often referred to as the low-rank matrix completion problem (Candès & Plan, 2010; Candès & Recht, 2009; Candès & Tao, 2010; Hardt et al., 2014; Fazel, 2002; Keshavan et al., 2010a;b; Recht, 2011; Saunderson et al., 2016).",1. Introduction,[0],[0]
"A celebrated application of this problem lies in the recommender systems area, where it is known as the Netflix problem (net).",1. Introduction,[0],[0]
"In this user-profiling application, an entry of the input matrix represents the rating of a movie by a user, where some entries could be missing.",1. Introduction,[0],[0]
"The goal is to predict the missing entries so that the rank of the complete matrix is minimized.
",1. Introduction,[0],[0]
"The low-rank matrix completion problem is known to be NP-hard, even when the matrix is over the field GF(2) (i.e., each entry is 0 or 1), and the goal is to complete the matrix into one of rank 3 (Peeters, 1996).",1. Introduction,[0],[0]
"A significant body of work on the low-rank matrix completion problem has centered around proving that, under some feasibility assumptions, the matrix completion problem can be solved efficiently with high probability (Candès & Recht, 2009; Recht, 2011).",1. Introduction,[0],[0]
"These feasibility assumptions are: (1) low rank; (2) incoherence; and (3) randomness (Hardt et al., 2014).",1. Introduction,[0],[0]
"Hardt et al. (2014) argue that feasibility assumption (3), which states that the subset of determined entries in the matrix is selected uniformly at random and has a large (sampling) density, is very demanding.",1. Introduction,[0],[0]
"In particular, they justify that in many applications, such as the Netflix problem, it is not possible to arbitrarily choose which matrix entries are determined and which are not, as those may be dictated by outside factors.",1. Introduction,[0],[0]
The low-rank matrix completion problem also has other applications in the area of wireless sensor networks.,1. Introduction,[0],[0]
"In one such application, the goal is to reconstruct a low-dimensional geometry describing the locations of the sensors based on local distances sensed by each sensor; this problem is referred to as TRIANGULATION FROM INCOMPLETE DATA (Candès & Recht, 2009).",1. Introduction,[0],[0]
"Due to its inherent hardness, the low-rank matrix completion problem has also
been studied with respect to various notions of approximation (Candès & Recht, 2009; Candès & Tao, 2010; Frieze et al., 2004; Hardt et al., 2014; Keshavan et al., 2010a;b; Recht, 2011).
",1. Introduction,[0],[0]
The second problem we consider is the matrix completion problem w.r.t. minimizing the number of distinct rows.,1. Introduction,[0],[0]
"Although this problem has not received as much attention as low-rank-matrix completion, it certainly warrants studying.",1. Introduction,[0],[0]
"In fact, minimizing the number of distinct rows represents a special case of the SPARSE SUBSPACE CLUSTERING problem (Elhamifar & Vidal, 2013), where the goal is to complete a matrix in such a way that its rows can be partitioned into the minimum number of subspaces.",1. Introduction,[0],[0]
The problem we consider corresponds to the special case of SPARSE SUBSPACE CLUSTERING where the matrix is over GF(2) and the desired rank of each subspace is 1.,1. Introduction,[0],[0]
"Furthermore, one can see the relevance of this problem to the area of recommender systems; in this context, one seeks to complete the matrix in such a way that the profile of each user is identical to a member of a known (possibly small) group of users.
",1. Introduction,[0],[0]
"In this paper, we study the two aforementioned problems through the lens of parameterized complexity (Downey & Fellows, 2013).",1. Introduction,[0],[0]
"In this paradigm, one measures the complexity of problems not only in terms of their input size n but also by a certain parameter k ∈ N, and seeks—among other things—fixed-parameter algorithms, i.e., algorithms that run in time f(k)·nO(1) for some function f .",1. Introduction,[0],[0]
Problems admitting such algorithms are said to be fixed-parameter tractable (or contained in the parameterized complexity class FPT).,1. Introduction,[0],[0]
"The motivation is that the parameter of choice—usually describing some structural properties of the instance—can be small in some instances of interest, even when the input size is large.",1. Introduction,[0],[0]
"Therefore, by confining the combinatorial explosion to this parameter, one can obtain efficient algorithms for problem instances with a small parameter value for NP-hard problems.",1. Introduction,[0],[0]
"Problems that are not (or unlikely to be) fixedparameter tractable can still be solvable in polynomial-time for every fixed parameter value, i.e., they can be solved in time nf(k) for some function f .",1. Introduction,[0],[0]
Problems of this kind are contained in the parameterized complexity class XP.,1. Introduction,[0],[0]
"We also consider randomized versions of FPT and XP, denoted by FPTR and XPR, containing all problems that can be solved by a randomized algorithm with a run-time of f(k)nO(1) and O(nf(k)), respectively, with a constant one-sided error-probability.",1. Introduction,[0],[0]
"Finally, problems that remain NP-hard for some fixed value of the parameter are hard for the parameterized complexity class paraNP.",1. Introduction,[0],[0]
"We refer to the respective textbooks for a detailed introduction to parameterized complexity (Downey & Fellows, 2013; Cygan et al., 2015).",1. Introduction,[0],[0]
"Parameterized Complexity is a rapidly growing field with various applications in many areas of Computer Science, including Artificial Intelligence (Bäckström et al., 2015; van Bevern et al., 2016; Bessiere et al., 2008; Endriss
et al., 2015; Ganian & Ordyniak, 2018; Gaspers & Szeider, 2014; Gottlob & Szeider, 2006).
",1. Introduction,[0],[0]
Parameterizations.,1. Introduction,[0],[0]
The parameters that we consider in this paper are: The number of (matrix) rows that cover all missing entries (row); the number of columns that cover all missing entries (col); and the minimum number of rows and columns which together cover all missing entries (comb).,1. Introduction,[0],[0]
"Although we do discuss and provide results for the unbounded domain case, i.e, the case that the domain (field size) is part of the input, we focus on the case when the matrix is over a bounded domain:",1. Introduction,[0],[0]
"This case is the most relevant from a practical perspective, and most of the related works focus on this case.",1. Introduction,[0],[0]
"It is easy to see that, when stated over any bounded domain, both problems under consideration are in FPT when parameterized by the number of missing entries, since an algorithm can brute-force through all possible solutions.",1. Introduction,[0],[0]
"On the other hand, parameterizing by row (resp. col) is very interesting from a practical perspective, as rows (resp.",1. Introduction,[0],[0]
"columns) with missing entries represent the newlyadded elements (e.g., newly-added users/movies/sensors, etc.); here, the above brute-force approach naturally fails, since the number of missing entries is no longer bounded by the parameter alone.",1. Introduction,[0],[0]
"Finally, the parameterization by comb is interesting because this parameter subsumes (i.e., is smaller than) the other two parameters (i.e., row and col).",1. Introduction,[0],[0]
"In particular, any fixed-parameter algorithm w.r.t.",1. Introduction,[0],[0]
this parameter implies a fixed-parameter algorithm w.r.t.,1. Introduction,[0],[0]
"the other two parameters, but can also be efficient in cases where the number of rows or columns with missing entries is large.
Results and Techniques.",1. Introduction,[0],[0]
"We start in Section 3 by considering the BOUNDED RANK MATRIX COMPLETION problem over GF(p) (denoted p-RMC), in which the goal is to complete the missing entries in the input matrix so that the rank of the completed matrix is at most t, where t ∈ N is given as input.",1. Introduction,[0],[0]
We present a (randomized) fixed-parameter algorithm for this problem parameterized by comb.,1. Introduction,[0],[0]
"This result is obtained by applying a branch-and-bound algorithm combined with algebra techniques, allowing us to reduce the problem to a system of quadratic equations in which only few (bounded by some function of the parameter) equations contain non-linear terms.",1. Introduction,[0],[0]
We then use a result by Miura et al. (2014) (improving an earlier result by Courtois et al. (2002)) in combination with reduction techniques to show that solving such a system of equations is in FPTR parameterized by the number of equations containing nonlinear terms.,1. Introduction,[0],[0]
"In the case where the domain is unbounded, we show that RMC is in XP parameterized by either row or col and in XPR parameterized by comb.
",1. Introduction,[0],[0]
"In Section 4, we turn our attention to the BOUNDED DISTINCT ROW MATRIX COMPLETION problem over both bounded domain (p-DRMC) and unbounded domain (DRMC); here, the goal is to complete the input matrix so
that the number of distinct rows in the completed matrix is at most t. We start by showing that p-DRMC parameterized by comb is fixed-parameter tractable.",1. Introduction,[0],[0]
"We obtain this result as a special case of a more general result showing that both DRMC and p-DRMC are fixed-parameter tractable parameterized by the treewidth (Robertson & Seymour, 1986; Downey & Fellows, 2013) of the compatibility graph, i.e., the graph having one vertex for every row and an edge between two vertices if the associated rows can be made identical.",1. Introduction,[0],[0]
This result also allows us to show that DRMC is fixed-parameter tractable parameterized by row.,1. Introduction,[0],[0]
"Surprisingly, DRMC behaves very differently when parameterized by col, as we show that, for this parameterization, the problem becomes paraNP-hard.
",1. Introduction,[0],[0]
We chart our results in Table 1.,1. Introduction,[0],[0]
"Interestingly, in the unbounded domain case, both considered problems exhibit wildly different behaviors: While RMC admits XP algorithms regardless of whether we parameterize by row or col, using these two parameterizations for DRMC results in the problem being FPT and paraNP-hard, respectively.",1. Introduction,[0.955188903082856],"['For the bounded domain case, we painted a positive picture by showing that the two problems are in FPT (resp.']"
"On the other hand, in the (more studied) bounded domain case, we show that both problems are in FPT (resp.",1. Introduction,[0],[0]
FPTR) w.r.t.,1. Introduction,[0],[0]
all parameters under consideration.,1. Introduction,[0],[0]
"Finally, we prove that 2-DRMC remains NP-hard even if every column and row contains (1) a bounded number of missing entries, or (2) a bounded number of determined entries.",1. Introduction,[0],[0]
This effectively rules out FPT algorithms w.r.t.,1. Introduction,[0],[0]
the parameters: maximum number of missing/determined entries per row or column.,1. Introduction,[1.0],['the parameters: maximum number of missing/determined entries per row or column.']
"For a prime number p, let GF(p) be a field of order p; recall that each such field can be equivalently represented as the set of integers modulo",2. Preliminaries,[0],[0]
p.,2. Preliminaries,[0],[0]
For positive integers i and j >,2. Preliminaries,[0],[0]
"i, we write [i] for the set {1, 2, . . .",2. Preliminaries,[0],[0]
", i}, and i : j for the set {i, i+ 1, . . .",2. Preliminaries,[0],[0]
", j}.
",2. Preliminaries,[0],[0]
"For an m × n matrix M (i.e., a matrix with m rows and n columns), and for i ∈",2. Preliminaries,[0],[0]
[m] and j ∈,2. Preliminaries,[0],[0]
"[n], M[i, j] denotes the element in the i-th row and j-th column of M. Similarly, for a vector d, we write d[i] for the i-th coordinate of d. We write M[∗, j] for the column-vector (M[1, j],M[2, j], . . .",2. Preliminaries,[0],[0]
",M[m, j]), and M[i, ∗] for the row-
vector (M[i, 1],M[i, 2], . . .",2. Preliminaries,[0],[0]
",M[i, n]).",2. Preliminaries,[0],[0]
We will also need to refer to submatrices obtained by omitting certain rows or columns from M. We do so by using sets of indices to specify which rows and columns the matrix contains.,2. Preliminaries,[0],[0]
"For instance, the matrix M[[i], ∗] is the matrix consisting of the first i rows and all columns of M, and M[2 : m, 1 : n− 1] is the matrix obtained by omitting the first row and the last column from M.
The row-rank (resp.",2. Preliminaries,[0],[0]
"column-rank) of a matrix M is the maximum number of linearly-independent rows (resp. columns) in M. It is well known that the row-rank of a matrix is equal to its column-rank, and this number is referred to as the rank of the matrix.",2. Preliminaries,[0],[0]
"We let rk(M) and dr(M) denote the rank and the number of distinct rows of a matrix M, respectively.",2. Preliminaries,[0],[0]
"If M is a matrix over GF(p), we call GF(p)",2. Preliminaries,[0],[0]
"the domain of M.
An incomplete matrix over GF(p) is a matrix which may contain not only elements from GF(p) but also the special symbol •. An entry is a missing entry if it contains •, and is a determined entry otherwise.",2. Preliminaries,[0],[0]
"A (possibly incomplete) m×n matrix M′ is consistent with anm×nmatrix M if and only if, for each",2. Preliminaries,[0],[0]
i ∈,2. Preliminaries,[0],[0]
[m] and j ∈,2. Preliminaries,[0],[0]
"[n], either M′[i, j] = M[i, j] or M′[i, j] = •.",2. Preliminaries,[0],[0]
"We formally define the problems under consideration below.
",2.1. Problem Formulation,[0],[0]
"BOUNDED RANK MATRIX COMPLETION (p-RMC)
Input: An incomplete matrix M over GF(p) for a fixed prime number p, and an integer t. Task: Find a matrix M′ consistent with M such that rk(M′) ≤ t.
BOUNDED DISTINCT",2.1. Problem Formulation,[0],[0]
"ROW MATRIX COMPLETION (p-DRMC)
",2.1. Problem Formulation,[0],[0]
"Input: An incomplete matrix M over GF(p) for a fixed prime number p, and an integer t. Task: Find a matrix M′ consistent with M such that dr(M′) ≤",2.1. Problem Formulation,[0],[0]
"t.
Aside from the problem variants where p is a fixed prime number, we also study the case where matrix entries range over a domain that is provided as part of the input.",2.1. Problem Formulation,[0],[0]
"In particular, the problems RMC and DRMC are defined analogously to p-RMC and p-DRMC, respectively, with the sole distinction that the prime number p is provided as part of the input.",2.1. Problem Formulation,[0],[0]
"We note that 2-RMC is NP-hard even for t = 3 (Peeters, 1996), and the same holds for 2-DRMC (see Theorem 14).",2.1. Problem Formulation,[0],[0]
"Without loss of generality, we assume that the rows of the input matrix are pairwise distinct.",2.1. Problem Formulation,[0],[0]
"Treewidth (Robertson & Seymour, 1986) is one of the most prominent decompositional parameters for graphs and has found numerous applications in computer science.",2.2. Treewidth,[0],[0]
"A treedecomposition T of a graph G = (V,E) is a pair (T, χ),
where T is a tree and χ is a function that assigns each tree node t a set χ(t) ⊆ V of vertices such that the following conditions hold: (TD1) for every edge uv ∈ E(G) there is a tree node t such that u, v ∈ χ(t); and (TD2) for every vertex v ∈ V (G), the set of tree nodes t with v ∈ χ(t) forms a non-empty subtree of T .",2.2. Treewidth,[0],[0]
"The width of a tree-decomposition (T, χ) is the size of a largest bag minus 1.",2.2. Treewidth,[0],[0]
A tree-decomposition of minimum width is called optimal.,2.2. Treewidth,[0],[0]
"The treewidth of a graph G, denoted by tw(G), is the width of an optimal tree decomposition of G. We will assume that the tree T of a tree-decomposition is rooted and we will denote by Tt the subtree of T rooted at t and write χ(Tt) for the set ⋃ t′∈V (Tt) χ(t ′).",2.2. Treewidth,[1.0],"['The treewidth of a graph G, denoted by tw(G), is the width of an optimal tree decomposition of G. We will assume that the tree T of a tree-decomposition is rooted and we will denote by Tt the subtree of T rooted at t and write χ(Tt) for the set ⋃ t′∈V (Tt) χ(t ′).']"
One advantage of the parameterized complexity paradigm is that it allows us to study the complexity of a problem w.r.t.,2.3. Problem Parameterizations,[0],[0]
several parameterizations of interest/relevance.,2.3. Problem Parameterizations,[0],[0]
"To provide a concise description of the parameters under consideration, we introduce the following terminology: We say that a • entry at position [i, j] in an incomplete matrix M is covered by row i and by column j. In this paper, we study RMC and DRMC w.r.t.",2.3. Problem Parameterizations,[0],[0]
"the following parameterizations (see Figure 1 for illustration):
For instance, the aforementiond problem TRIANGULATION FROM INCOMPLETE DATA, where a small number of distance-sensors are faulty, would result in matrix completion instances where col and row are both small.
",2.3. Problem Parameterizations,[0],[0]
"We denote the parameter under consideration in brackets after the problem name (e.g., DRMC[comb]).",2.3. Problem Parameterizations,[0],[0]
"As mentioned in Section 1, both p-RMC and p-DRMC are trivially in FPT when parameterized by the number of missing entries, and hence this parameterization is not discussed further.
",2.3. Problem Parameterizations,[0],[0]
"Given an incomplete matrix M, computing the parameter values for col and row is trivial.",2.3. Problem Parameterizations,[0],[0]
"Furthermore, the parameter values satisfy comb ≤ row and comb ≤ col.",2.3. Problem Parameterizations,[0],[0]
"The parameter
value for comb can also be computed in polynomial time by reducing the problem to finding a vertex cover in a bipartite graph: Proposition 1.",2.3. Problem Parameterizations,[0],[0]
"Given an incomplete matrix M over GF(p), we can compute the parameter value for comb, along with sets R and C of total cardinality comb containing the indices of covering rows and columns, respectively, in time O((n ·m)1.5).",2.3. Problem Parameterizations,[0],[0]
"In this section we present our results for BOUNDED RANK MATRIX COMPLETION under various parameterizations.
3.1.",3. Rank Minimization,[0],[0]
"Bounded Domain: Parameterization by row
As our first result, we present an algorithm for solving p-RMC[row].",3. Rank Minimization,[0],[0]
"This will serve as a gentle introduction to the techniques used in the more complex result for pRMC[comb], and will also be used to give an XP algorithm for RMC[row].",3. Rank Minimization,[0],[0]
Theorem 2.,3. Rank Minimization,[0],[0]
"p-RMC[row] is in FPT.
",3. Rank Minimization,[0],[0]
Proof Sketch.,3. Rank Minimization,[0],[0]
"Let R be the (minimum) set of rows that cover all occurrences of • in the input matrix M. Since the existence of a solution does not change if we permute the rows of M, we permute the rows of M so that the rows in R have indices 1, . . .",3. Rank Minimization,[0],[0]
", k. We now proceed in three steps.
",3. Rank Minimization,[0],[0]
"For the first step, we will define the notion of signature: A signature S is a tuple (I,D), where I ⊆ R and D is a mapping from R \",3. Rank Minimization,[0],[0]
I to (I → GF(p)).,3. Rank Minimization,[0],[0]
"Intuitively, a signature S specifies a subset I of R which is expected to be independent in M",3. Rank Minimization,[0],[0]
"[k + 1 : m, ∗] ∪ I (i.e., adding the rows in I to M [k + 1 : m, ∗] is expected to increase the rank of M",3. Rank Minimization,[0],[0]
"[k + 1 : m, ∗] by |I|); and for each remaining row of R, S specifies how that row should depend on I .",3. Rank Minimization,[0],[0]
The latter is carried out using D: For each row in R\,3. Rank Minimization,[0],[0]
"I , D provides a set of coefficients expressing the dependency of that row on the rows in I .",3. Rank Minimization,[0],[0]
"Formally, we say that a matrix M′ that is compatible with the incomplete matrix M matches a signature (I,D) if and only if, for each row (i.e., vector) d ∈ R \",3. Rank Minimization,[0.9510687075791505],"['We say that a matrix M′ compatible with the incomplete matrix M matches a signature (IR, DR, IC , DC) if: ◦ for each row d ∈ R \\ IR, there exist coefficients adr+1, .']"
"I , there exist coefficients adk+1, . . .",3. Rank Minimization,[0],[0]
", a d m ∈ GF(p) such that
d = adk+1M[k+1, ∗]+· · ·+admM[m, ∗]+ ∑ i∈I D(d)(i)·i.",3. Rank Minimization,[0],[0]
The first step of the algorithm branches through all possible signatures,3. Rank Minimization,[0],[0]
S.,3. Rank Minimization,[0],[0]
"Clearly, the number of distinct signatures is upper-bounded by 2k · pk2 .
",3. Rank Minimization,[0],[0]
"For the second step, we fix an enumerated signature S. The algorithm will verify whether S is valid, i.e., whether there exists a matrix M′ compatible with M that matches S. To do so, the algorithm will construct a system of |R \ I| equations over vectors of size n, and then transform this into a system ΥS of |R\I| ·n equations over GF(p) (one equation for each vector coordinate).",3. Rank Minimization,[0],[0]
"For each d ∈ R\I , ΥS contains
one variable for each coefficient adk+1, . . .",3. Rank Minimization,[0],[0]
", a d m and one variable for each occurrence of • in the rows of R. For instance, the first equation in ΥS has the following form: d[1] = adk+1M[k+1, 1]+ · · ·+admM[m, 1]+ ∑ i∈I D(d)(i) ·",3. Rank Minimization,[0],[0]
"i[1], where adk+1, . . .",3. Rank Minimization,[0],[0]
", a d m are variables, and d[1] as well as each i[1] in the sum could be a variable or a fixed number.",3. Rank Minimization,[0],[0]
"Crucially, ΥS is a system of at most (k ·n) linear equations over GF(p) with at most m+ kn variables, and can be solved in time O((m+ kn)3) by Gaussian elimination.",3. Rank Minimization,[0],[0]
"Constructing the equations takes time O(m · n).
",3. Rank Minimization,[0],[0]
"During the second step, the algorithm determines whether a signature S is valid or not, and in the end, after going through all signatures, selects an arbitrary valid signature S =",3. Rank Minimization,[0.9801711040124015],"['During the second step, the algorithm determines whether a signature S is valid or not, and in the end, after going through all signatures, selects an arbitrary valid signature S = (I,D) with minimum |I|.']"
"(I,D) with minimum |I|.",3. Rank Minimization,[0],[0]
"For the final third step, the algorithm checks whether |I|+rk(M[k+1 : m, ∗])",3. Rank Minimization,[0],[0]
≤ t.,3. Rank Minimization,[0],[0]
"We note that computing rk(M[k + 1 : m, ∗]) can be carried out in time O(nm1.4) (Ibarra et al., 1982).",3. Rank Minimization,[0],[0]
"If the above inequality does not hold, the algorithm rejects; otherwise it recomputes a solution to ΥS and outputs the matrix M′ obtained from M by replacing each occurrence of • at position",3. Rank Minimization,[0],[0]
"[i, j] by the value of the variable i[j] in the solution to ΥS .",3. Rank Minimization,[0],[0]
The total running time isO((2k ·pk2) ·((m+kn)3 +nm1.4)),3. Rank Minimization,[0],[0]
"= O(2kpk2 · (m+ kn)3).
",3. Rank Minimization,[0],[0]
"Since the the transpose of M has the same rank as M, it follows immediately that p-RMC[col] is in FPT.",3. Rank Minimization,[0],[0]
Corollary 3.,3. Rank Minimization,[0],[0]
"p-RMC[col] is in FPT.
",3. Rank Minimization,[0],[0]
"As a consequence of the running time of the algorithm given in the proof of Theorem 2, we obtain: Corollary 4. RMC[row] and RMC[col] are in XP.
3.2.",3. Rank Minimization,[0],[0]
"Bounded Domain: Parameterization by comb
In this subsection, we present a randomized fixed-parameter algorithm for p-RMC[comb] with constant one-sided error probability.",3. Rank Minimization,[0],[0]
"Before we proceed to the algorithm, we need to introduce some basic terminology related to systems of equations.",3. Rank Minimization,[0],[0]
"Let Υ be a system of ` equations EQ1, EQ2,. . .",3. Rank Minimization,[0],[0]
", EQ` over GF(p); we assume that the equations are simplified as much as possible.",3. Rank Minimization,[0],[0]
"In particular, we assume that no equation contains two terms over the same set of variables such that the degree/exponent of each variable in both terms is the same.",3. Rank Minimization,[0],[0]
"Let EQi be a linear equation in Υ, and let x be a variable which occurs in EQi (with a non-zero coefficient).",3. Rank Minimization,[1.0],"['Let EQi be a linear equation in Υ, and let x be a variable which occurs in EQi (with a non-zero coefficient).']"
"Naturally, EQi can be transformed into an equivalent equation EQi,x, where x is isolated, and we use Γi,x to denote the side of EQi,x not containing x, i.e., EQi,x is of the form x = Γi,x.",3. Rank Minimization,[0],[0]
"We say that Υ′ is obtained from Υ by substitution of x in EQi if Υ′ is the system of equations obtained by:
1.",3. Rank Minimization,[0],[0]
"computing EQi,x and in particular Γi,x from EQi;
2. setting Υ′ := Υ \ {EQi}; and
3.",3. Rank Minimization,[0],[0]
"replacing x with Γi,x in every equation in Υ′.
Observe that Υ′ has sizeO(n · `), and can also be computed in timeO(n·`), where n is the number of variables occurring in Υ. Furthermore, any solution to Υ′ can be transformed into a solution to Υ in linear time, and similarly any solution to Υ can be transformed into a solution to Υ′ in linear time (i.e., Υ′ and Υ are equivalent).",3. Rank Minimization,[0],[0]
"Moreover, Υ′ contains at least one fewer variable and one fewer equation than Υ.
The following proposition is crucial for our proof, and is of independent interest.
",3. Rank Minimization,[0],[0]
Proposition 5.,3. Rank Minimization,[0],[0]
Let Υ be a system of ` quadratic equations over GF(p).,3. Rank Minimization,[0],[0]
"Then computing a solution for Υ is in FPTR parameterized by ` and p, and in XPR parameterized only by `.
Proof.",3. Rank Minimization,[0],[0]
Let n be the number of variables in Υ. We distinguish two cases.,3. Rank Minimization,[0],[0]
"If n ≥ `(` + 3)/2, then Υ can be solved in randomized time O(2`n3`(log p)2)",3. Rank Minimization,[0],[0]
"(Miura et al., 2014).",3. Rank Minimization,[0],[0]
"Otherwise, n <",3. Rank Minimization,[0],[0]
"`(` + 3)/2, and we can solve Υ by a brute-force algorithm which enumerates (all of the) at most pn < p`(`+3)/2 assignments of values to the variables in Υ. The proposition now follows by observing that the given algorithm runs in timeO(2`n3`(log p)2+p`(`+3)/2`2).
",3. Rank Minimization,[0],[0]
Theorem 6.,3. Rank Minimization,[0],[0]
"p-RMC[comb] is in FPTR.
",3. Rank Minimization,[0],[0]
Proof Sketch.,3. Rank Minimization,[0],[0]
"We begin by using Proposition 1 to compute the sets R and C containing the indices of the covering rows and columns, respectively; let |R| = r and |C| = c, and recall that the parameter value is k = r + c. Since the existence of a solution for p-RMC does not change if we permute rows and columns of M, we permute the rows of M so that the rows in R have indices 1, . . .",3. Rank Minimization,[0.9991609367592407],"['We begin by using Proposition 1 to compute the sets R and C containing the indices of the covering rows and columns, respectively; let |R| = r and |C| = c, and recall that the parameter value is k = r + c. Since the existence of a solution for p-RMC does not change if we permute rows and columns of M, we permute the rows of M so that the rows in R have indices 1, .']"
", r, and subsequently, we permute the columns of M so that the columns in C have indices 1, . . .",3. Rank Minimization,[0],[0]
", c.
Before we proceed, let us give a high-level overview of our strategy.",3. Rank Minimization,[0.9999998885214695],"[', c. Before we proceed, let us give a high-level overview of our strategy.']"
"The core idea is to branch over signatures, which will be defined in a similar way to those in Theorem 2.",3. Rank Minimization,[0],[0]
"These signatures will capture information about the dependencies among the rows in R and columns in C; one crucial difference is that for columns, we will focus only on dependencies in the submatrix M[r + 1 : m, ∗].",3. Rank Minimization,[0],[0]
"In each branch, we arrive at a system of equations that needs to be solved in order to determine whether the signatures are valid.",3. Rank Minimization,[0],[0]
"Unlike Theorem 2, here the obtained system of equations will contain non-linear (but quadratic) terms, and hence solving the system is far from being trivial.",3. Rank Minimization,[0],[0]
"Once we determine which signatures are valid, we choose one that minimizes the total rank.
",3. Rank Minimization,[0],[0]
"For the first step, let us define the notion of signature that will be used in this proof.",3. Rank Minimization,[1.0],"['For the first step, let us define the notion of signature that will be used in this proof.']"
"A signature S is a tuple
(IR, DR, IC , DC) where: 1.",3. Rank Minimization,[1.000000083983251],"['A signature S is a tuple (IR, DR, IC , DC) where: 1.']"
IR ⊆ R; 2.,3. Rank Minimization,[0],[0]
DR is a mapping from R \ IR to (IR → GF(p));,3. Rank Minimization,[0.9802409833078866],['DR is a mapping from R \\ IR to (IR → GF(p)); 3.']
3.,3. Rank Minimization,[0],[0]
IC ⊆ C; and 4.,3. Rank Minimization,[0],[0]
"DC is a mapping from C \ IC to (IC → GF(p)).
",3. Rank Minimization,[0],[0]
"We say that a matrix M′ compatible with the incomplete matrix M matches a signature (IR, DR, IC , DC) if:
◦ for each row d ∈ R \ IR, there exist coefficients adr+1, . . .",3. Rank Minimization,[0],[0]
", a d m ∈ GF(p) such that d = adr+1M′[r +
1, ∗] + · · ·+ admM′[m, ∗]",3. Rank Minimization,[0.9645122787960462],"[', a d m ∈ GF(p) such that d = adr+1M′[r + 1, ∗] + · · ·+ admM′[m, ∗] + ∑ i∈IR DR(d)(i) · i; and ◦ for each column h ∈ C \\ Ic, there exist coefficients bhc+1, .']"
"+ ∑ i∈IR DR(d)(i) · i; and
◦ for each column h ∈ C",3. Rank Minimization,[0],[0]
"\ Ic, there exist coefficients bhc+1, . . .",3. Rank Minimization,[0],[0]
", b h n ∈ GF(p)",3. Rank Minimization,[0],[0]
"such that h[r + 1 : m] =
bhc+1M ′[r",3. Rank Minimization,[0],[0]
"+ 1 : m, c] + · · ·",3. Rank Minimization,[0],[0]
"+ bhnM′[r + 1 : m,n]",3. Rank Minimization,[0],[0]
"+∑
i∈IC DC(h)(i) ·",3. Rank Minimization,[0],[0]
"i[r + 1 : m].
",3. Rank Minimization,[0],[0]
"The number of distinct signatures is upper-bounded by 2r · pr2 · 2c · pc2 ≤ 2k · pk2 , and the first step of the algorithm branches over all possible signatures S. In the second step, for each enumerated signature S, we check whether S is valid (i.e., whether there exists a matrix M′, compatible with the incomplete M, that matches S) in a similar fashion as in the proof of Theorem 2.",3. Rank Minimization,[0],[0]
"Here, this results in a system ΥS of |R",3. Rank Minimization,[0],[0]
\ IR| · n+ |C \ IC | · (m− r) equations which check the dependencies for rows in R \ IR and columns in C \ IC .,3. Rank Minimization,[0],[0]
"For instance, the first equation in ΥS for some d ∈ R \ IR has the following form: d[1] = adr+1M[r+1, 1]+· · ·+admM[m, 1]+ ∑ i∈IR DR(d)(i)·i[1], where adr+1, . . .",3. Rank Minimization,[0],[0]
", a d m are variables, DR(d)(i) is a number, and all other occurrences are either variables or numbers.",3. Rank Minimization,[0],[0]
"Similarly, the second equation in ΥS for some h ∈ C \ IC has the following form: h[r + 2] = bhc+1M[r",3. Rank Minimization,[0],[0]
"+ 2, c+ 1] + · · ·",3. Rank Minimization,[0],[0]
"+ bhnM[r + 2, n] + ∑ i∈IC DC(d)(i) ·",3. Rank Minimization,[0],[0]
"i[r + 2], where bhc+1, . . .",3. Rank Minimization,[0],[0]
", b h n are variables, DC(d)(i) is a number, and all other occurrences are either variables or numbers.
",3. Rank Minimization,[0],[0]
"Next, observe that the only equations in ΥS that may contain non-linear terms are those for d[j], where j ≤ c, and in particular ΥS contains at most k2 equations with non-linear terms (k equations for at most k vectors d inR\IR).",3. Rank Minimization,[0],[0]
"We will now use substitutions to simplify ΥS by removing all linear equations; specifically, at each step we select an arbitrary linear equation EQi containing a variable x, apply substitution of x in EQi to construct a new system of equations with one fewer equation, and simplify all equations in the new system.",3. Rank Minimization,[0],[0]
"If at any point we reach a system of equations that contains an invalid equation (e.g., 2=5), then ΥS does not have a solution, and we discard the corresponding branch.",3. Rank Minimization,[0],[0]
"Otherwise, after at most |R\IR| ·n+ |C \IC | ·(m−r) ∈ O(kn+km) substitutions, we obtain a system of at most k2 quadratic equations ΨS such that any solution to ΨS can be transformed into a solution to ΥS in time O(kn+ km).",3. Rank Minimization,[0],[0]
"We can now apply Proposition 5 to solve ΨS and mark S as a valid signature if ΨS has a solution.
",3. Rank Minimization,[0],[0]
"After all signatures have been processed, the algorithm se-
lects a valid signature S =",3. Rank Minimization,[0],[0]
"(I,D) that has the minimum value of |IR|+ |IC",3. Rank Minimization,[0],[0]
"|, checks whether |IR|+ |IC |+ rk(M[r+ 1 : m, c : 1 + n]) ≤ t, and either uses this to construct a solution (similarly to the proof of Theorem 2), or outputs “no”.",3. Rank Minimization,[0],[0]
The theorem now follows by observing that the total running time of the algorithm is obtained by combining the branching factor of branching over all signatures (O(2k ·pk2)) with the run-time of Proposition 5 for k2 many quadratic equations (O(3k2n3(log p)2 + pk4)).,3. Rank Minimization,[0],[0]
"In particular, we obtain a running time of O(3k2 · pk4 · n3).
",3. Rank Minimization,[0],[0]
"As a consequence of the running time of the algorithm given in the proof of Theorem 6, we obtain:
Corollary 7.",3. Rank Minimization,[0],[0]
RMC[comb] is in XPR.,3. Rank Minimization,[0],[0]
"Let (p,M, t) be an instance of DRMC.",4. BOUNDED DISTINCT ROW MATRIX COMPLETION,[1.0],"['Let (p,M, t) be an instance of DRMC.']"
We say that two rows of M are compatible if whenever the two rows differ at some entry then one of the rows has a • at that entry.,4. BOUNDED DISTINCT ROW MATRIX COMPLETION,[0],[0]
"The compatibility graph of M, denoted by G(M), is the undirected graph whose vertices correspond to the row indices of M and in which there is an edge between two vertices if and only if their two corresponding rows are compatible.",4. BOUNDED DISTINCT ROW MATRIX COMPLETION,[0],[0]
"See Figure 2 for an illustration.
",4. BOUNDED DISTINCT ROW MATRIX COMPLETION,[0.9999999310941148],['See Figure 2 for an illustration.']
"We start by showing that DRMC (and therefore p-DRMC) can be reduced to the CLIQUE COVER problem, which is defined as follows.
",4. BOUNDED DISTINCT ROW MATRIX COMPLETION,[0],[0]
CLIQUE,4. BOUNDED DISTINCT ROW MATRIX COMPLETION,[0],[0]
"COVER (CC)
",4. BOUNDED DISTINCT ROW MATRIX COMPLETION,[0],[0]
"Input: An undirected graph G and an integer k. Task: Find a partition of V (G) into at most k
cliques, or output that no such partition exists.
",4. BOUNDED DISTINCT ROW MATRIX COMPLETION,[0],[0]
Lemma 8.,4. BOUNDED DISTINCT ROW MATRIX COMPLETION,[0],[0]
"An instance I = (p,M, t) of DRMC has a solution if and only if the instance I ′",4. BOUNDED DISTINCT ROW MATRIX COMPLETION,[0],[0]
"= (G(M), t) of CC does.",4. BOUNDED DISTINCT ROW MATRIX COMPLETION,[0],[0]
"Moreover, a solution for I ′ can obtained in polynomial-time from a solution for I and vice versa.
",4. BOUNDED DISTINCT ROW MATRIX COMPLETION,[0],[0]
Proof Sketch.,4. BOUNDED DISTINCT ROW MATRIX COMPLETION,[0],[0]
"The lemma follows immediately from the observation that a set R of rows in M can be made identical if and only if G(M)[R] is a clique.
",4. BOUNDED DISTINCT ROW MATRIX COMPLETION,[0],[0]
Theorem 9.,4. BOUNDED DISTINCT ROW MATRIX COMPLETION,[0],[0]
"CC is in FPT when parameterized by the treewidth of the input graph.
",4. BOUNDED DISTINCT ROW MATRIX COMPLETION,[0],[0]
Proof Sketch.,4. BOUNDED DISTINCT ROW MATRIX COMPLETION,[0],[0]
"We show the theorem via a standard dynamic programming algorithm on a tree-decomposition of the input graph (Bodlaender & Koster, 2008).",4. BOUNDED DISTINCT ROW MATRIX COMPLETION,[0],[0]
"Namely after computing an optimal tree-decomposition (T, χ) of the input graph G, which can be achieved in FPT-time w.r.t.",4. BOUNDED DISTINCT ROW MATRIX COMPLETION,[0],[0]
"the treewidth of G (Kloks, 1994; Bodlaender, 1996; Bodlaender et al., 2016), we compute a set R(t) of tuples via a bottom-up dynamic programming algorithm for every t ∈ V (T ).",4. BOUNDED DISTINCT ROW MATRIX COMPLETION,[0],[0]
"In our case (P, c) ∈ R(t) if and only if P is a partition of G[χ(t)] into cliques and c is the minimum number such that G[χ(Tt)] has a partition P ′",4. BOUNDED DISTINCT ROW MATRIX COMPLETION,[0],[0]
into c cliques with P = {P ′ ∩ χ(t),4. BOUNDED DISTINCT ROW MATRIX COMPLETION,[0],[0]
| P ′ ∈ P ′ },4. BOUNDED DISTINCT ROW MATRIX COMPLETION,[0],[0]
"\ {∅}.
",4. BOUNDED DISTINCT ROW MATRIX COMPLETION,[0],[0]
Note that the above theorem also implies that the wellknown COLORING problem is FPT parameterized by the treewidth of the complement of the input graph.,4. BOUNDED DISTINCT ROW MATRIX COMPLETION,[0],[0]
"The theorem below follows immediately from Lemmas 8 and 9.
Theorem 10.",4. BOUNDED DISTINCT ROW MATRIX COMPLETION,[0.9763679703966109],['The theorem below follows immediately from Lemmas 8 and 9.']
"DRMC and p-DRMC are in FPT when parameterized by the treewidth of the compatibility graph.
",4. BOUNDED DISTINCT ROW MATRIX COMPLETION,[0],[0]
4.1.,4. BOUNDED DISTINCT ROW MATRIX COMPLETION,[0],[0]
"p-DRMC
Theorem 11.",4. BOUNDED DISTINCT ROW MATRIX COMPLETION,[0],[0]
"p-DRMC[comb] is in FPT.
",4. BOUNDED DISTINCT ROW MATRIX COMPLETION,[0],[0]
Proof.,4. BOUNDED DISTINCT ROW MATRIX COMPLETION,[0],[0]
"Let (M, t) be an instance of p-DRMC, and let k be the parameter comb.",4. BOUNDED DISTINCT ROW MATRIX COMPLETION,[0],[0]
"By Proposition 1, we can compute a set R• of rows and a set C• of columns, where |R• ∪ C•| ≤",4. BOUNDED DISTINCT ROW MATRIX COMPLETION,[0],[0]
"k, and such that every occurrence of • in M is either contained in a row or column in R• ∪ C•.",4. BOUNDED DISTINCT ROW MATRIX COMPLETION,[0],[0]
"Let R and C be the set of rows and columns of M, respectively.",4. BOUNDED DISTINCT ROW MATRIX COMPLETION,[1.0],"['Let R and C be the set of rows and columns of M, respectively.']"
Let P be the unique partition of R \,4. BOUNDED DISTINCT ROW MATRIX COMPLETION,[0],[0]
"R• such that two rows r and r′ belong to the same set in P if and only if they are identical on all columns in C \C•. Then |P | ≤ (p+ 1)k, for every P ∈ P , since two rows in P can differ on at most |C•| ≤ k entries, each having (p + 1) values to be chosen from.",4. BOUNDED DISTINCT ROW MATRIX COMPLETION,[0],[0]
"Moreover, any two rows in R \R• that are not contained in the same set in P are not compatible, which implies that they appear in different components of G(M)",4. BOUNDED DISTINCT ROW MATRIX COMPLETION,[0],[0]
\ R• and hence the set of vertices in every component of G(M),4. BOUNDED DISTINCT ROW MATRIX COMPLETION,[0],[0]
\,4. BOUNDED DISTINCT ROW MATRIX COMPLETION,[0],[0]
"R• is a subset of P , for some P ∈ P .",4. BOUNDED DISTINCT ROW MATRIX COMPLETION,[0],[0]
It is now straightforward to show that tw(G(M)),4. BOUNDED DISTINCT ROW MATRIX COMPLETION,[0],[0]
"≤ k + (p+ 1)k, and hence, tw(G(M)) is bounded by a function of the parameter k.",4. BOUNDED DISTINCT ROW MATRIX COMPLETION,[0],[0]
The theorem now follows from Theorem 10.,4. BOUNDED DISTINCT ROW MATRIX COMPLETION,[0],[0]
"The proof of the following theorem is very similar to the proof of Theorem 11, i.e., we mainly use the observation that the parameter row is also a bound on the treewidth of the compatibility graph and then apply Theorem 10.
Theorem 12.",4.2. DRMC,[0],[0]
"DRMC[row] is in FPT.
",4.2. DRMC,[0],[0]
"For the remainder of this section, we will introduce the PARTITIONING INTO TRIANGLES (PIT) problem:",4.2. DRMC,[0],[0]
"Given a graph G, decide whether there is a partition P of V (G) into triangles.",4.2. DRMC,[0],[0]
"We will often use the following easy observation.
",4.2. DRMC,[0],[0]
Observation 1.,4.2. DRMC,[0],[0]
A graph G that does not contain a clique with four vertices has a partition into triangles if and only if it has a partition into at most |V (G)|/3 cliques.,4.2. DRMC,[0],[0]
Theorem 13.,4.2. DRMC,[0],[0]
"DRMC[col] is paraNP-hard.
",4.2. DRMC,[0],[0]
Proof Sketch.,4.2. DRMC,[0],[0]
"We will reduce from the NP-complete 3-SAT2 problem (Berman et al., 2003):",4.2. DRMC,[0],[0]
"Given a propositional formula φ in conjunctive normal form such that (1) every clause of φ has exactly three distinct literals and (2) every literal occurs in exactly two clauses, decide whether φ is satisfiable.",4.2. DRMC,[0],[0]
"To make our reduction easier to follow, we will divide the reduction into two steps.",4.2. DRMC,[1.0],"['To make our reduction easier to follow, we will divide the reduction into two steps.']"
"Given an instance (formula) φ of 3-SAT-2, we will first construct an equivalent instance G of PIT with the additional property that G does not contain a clique on four vertices.",4.2. DRMC,[0],[0]
"We note that similar reductions from variants of the satisfiability problem to PIT are known (and hence our first step does not show anything new for PIT); however, our reduction is specifically designed to simplify the second step, in which we will construct an instance (M, |V (G)|/3) of DRMC such that G(M) is isomorphic to G and M has only seven columns.",4.2. DRMC,[0],[0]
"By Observation 1 and Lemma 8, this proves the theorem since (M, |V (G)|/3) has a solution if and only if φ does.
",4.2. DRMC,[0],[0]
"Let φ be an instance of 3-SAT-2 with variables x1, . . .",4.2. DRMC,[0],[0]
", xn and clauses C1, . . .",4.2. DRMC,[0],[0]
", Cm.",4.2. DRMC,[0],[0]
We first construct the instance G of PIT such that G does not contain a clique of size four.,4.2. DRMC,[1.0],['We first construct the instance G of PIT such that G does not contain a clique of size four.']
"For every variable xi of φ, let G(xi) be the graph illustrated in Figure 3, and for every clause Cj of φ, let G(Cj) be the graph illustrated in Figure 4.",4.2. DRMC,[1.0],"['For every variable xi of φ, let G(xi) be the graph illustrated in Figure 3, and for every clause Cj of φ, let G(Cj) be the graph illustrated in Figure 4.']"
Let f :,4.2. DRMC,[0],[0]
[m] ×,4.2. DRMC,[0],[0]
"[3] → {xoi , x̄oi | 1 ≤",4.2. DRMC,[0],[0]
i ≤ n,4.2. DRMC,[0],[0]
"∧ 1 ≤ o ≤ 2 } be any bijective function such that for every j and r with 1 ≤ j ≤ m and 1 ≤ r ≤ 3, it holds that: If f(j, r) = xoi",4.2. DRMC,[0],[0]
"(for some i and o), then xi is the r-th literal of Cj ; and if f(j, r)",4.2. DRMC,[0],[0]
"= x̄oi , then x̄i is the r-th literal of Cj .
",4.2. DRMC,[0],[0]
"The graph G is obtained from the disjoint union of the graphs G(x1), . . .",4.2. DRMC,[0],[0]
", G(xn), G(C1), . . .",4.2. DRMC,[0],[0]
", G(Cm)",4.2. DRMC,[0],[0]
"after applying the following modifications: (1) For every j and r with 1 ≤ j ≤ m and 1 ≤ r ≤ 3 add edges forming a triangle on the vertices l1j,r, l 2 j,r, f(j, r); and (2) for every i with 1 ≤ i ≤ 2n",4.2. DRMC,[0],[0]
"− m, add the vertices g1i , g2i and an edge between g1i and g 2 i .",4.2. DRMC,[0],[0]
Finally we add edges forming a complete bipartite graph between all vertices in { goi | 1 ≤,4.2. DRMC,[0],[0]
i ≤ 2n −m ∧ 1 ≤ o ≤ 2 } and all vertices in {hoi | 1 ≤,4.2. DRMC,[0],[0]
i ≤ n,4.2. DRMC,[0],[0]
"∧ 1 ≤ o ≤ 2 }.
",4.2. DRMC,[0],[0]
"This completes the construction of G. The following claim concludes the first step of our reduction.
",4.2. DRMC,[1.0000000196386833],['This completes the construction of G. The following claim concludes the first step of our reduction.']
Claim 1.,4.2. DRMC,[0],[0]
"φ is satisfiable if and only if G has a partition
into triangles.",4.2. DRMC,[0],[0]
"Moreover, G does not contain a clique of size four.
",4.2. DRMC,[0],[0]
"We will now proceed to the second (and final) step of our reduction, i.e., we will construct an instance (M, |V (G)|/3) of DRMC such that: G(M) is isomorphic to G and M has only seven columns.
",4.2. DRMC,[0],[0]
M contains one row R(u) for every u ∈ V (G).,4.2. DRMC,[0],[0]
The definition of R(u) for every vertex u that is part of some gadget G(xi) or G(Cj) is illustrated in Figures 3 and 4.,4.2. DRMC,[0],[0]
"Additionally, we set R(goj ) = (•, •, •, •, •, •, j) for every j and o with 1 ≤ j ≤ 2n−m and 1 ≤ o ≤ 2.",4.2. DRMC,[0],[0]
"Using an exhaustive case analysis, one can show thatG(M) is indeed isomorphic to G, which concludes the proof of the theorem.
",4.2. DRMC,[0],[0]
"We conclude this section with a hardness result showing that 2-DRMC remains NP-hard when the number of missing or known entries in each column/row is bounded.
",4.2. DRMC,[0],[0]
Theorem 14.,4.2. DRMC,[0],[0]
The restriction of 2-DRMC to instances in which each row and each column contains exactly three missing entries is NP-hard.,4.2. DRMC,[0],[0]
"The same holds for the restriction of 2-DRMC to instances in which each row and each column contains at most 4 determined entries.
",4.2. DRMC,[0],[0]
Proof Sketch.,4.2. DRMC,[0],[0]
"Consider the problem of (properly) coloring a graph on n vertices, having minimum degree n− 4 and no independent set of size 4, by n/3 colors, where n is divisible
by 3; denote this problem as (n/3)-COLORINGδ=n−4.",4.2. DRMC,[0],[0]
This problem is NP-hard via a reduction from the PARTITION INTO TRIANGLES problem on K4-free cubic graphs.,4.2. DRMC,[0],[0]
"The NP-hardness of the latter problem follows from the NPhardness of the PARTITION INTO TRIANGLES problem on planar cubic graphs (Cerioli et al., 2008), since a K4 in a cubic graph must be isolated, and hence can be removed from the start.",4.2. DRMC,[0],[0]
"Finally, using Observation 1, PARTITION INTO TRIANGLES on K4-free cubic graphs is polynomialtime reducible to (n/3)-COLORINGδ=n−4, via the simple reduction that complements the edges of the graph.
",4.2. DRMC,[0],[0]
"Now we reduce from (n/3)-COLORINGδ=n−4 to p-DRMC by mimicking a standard reduction from 3-coloring to rank minimization (Peeters, 1996).",4.2. DRMC,[0],[0]
"Given an instanceG of (n/3)COLORINGδ=n−4, we construct an n× n matrix M whose rows and columns correspond to the vertices inG, as follows.",4.2. DRMC,[0],[0]
The diagonal entries of M are all ones.,4.2. DRMC,[0],[0]
"For an entry at row i and column j, where i 6= j, M[i, j] = 0",4.2. DRMC,[0],[0]
"if ij ∈ E(G), and is • otherwise.",4.2. DRMC,[0],[0]
"Finally, we set r = n/3.",4.2. DRMC,[0],[0]
"Observe that since each vertex in G has n− 4 neighbors, the number of missing entries in any row and any column of M is 3.
",4.2. DRMC,[0.9999999691601548],"['Observe that since each vertex in G has n− 4 neighbors, the number of missing entries in any row and any column of M is 3.']"
"It is not difficult to show that G is a yes-instance of (n/3)COLORINGδ=n−4 if and only if (M, n/3) is a yes-instance of 2-DRMC.
",4.2. DRMC,[0],[0]
"The second statement in the theorem follows via a reduction from 3-COLORING on graphs of maximum degree at most 4 (Garey et al., 1976), using similar arguments.",4.2. DRMC,[0],[0]
We studied the parameterized complexity of two fundamental matrix completion problems under several parameterizations.,5. Conclusion,[0],[0]
"For the bounded domain case, we painted a positive picture by showing that the two problems are in FPT (resp.",5. Conclusion,[0],[0]
FPTR) w.r.t.,5. Conclusion,[0],[0]
all considered parameters.,5. Conclusion,[0],[0]
"For the unbounded domain case, we characterized the parameterized complexity of DRMC by showing that it is in FPT parameterized by row, and paraNP-hard parameterized by col (and hence by comb).",5. Conclusion,[1.0],"['For the unbounded domain case, we characterized the parameterized complexity of DRMC by showing that it is in FPT parameterized by row, and paraNP-hard parameterized by col (and hence by comb).']"
"For RMC, we could show its membership in XP (resp. XPR) w.r.t.",5. Conclusion,[0],[0]
all considered parameters.,5. Conclusion,[0],[0]
"Three immediate open questions ensue:
◦",5. Conclusion,[0],[0]
"Is it possible to obtain a deterministic algorithm for p-RMC and RMC parameterized by comb?
",5. Conclusion,[0],[0]
◦ Can we improve our XP (resp. XPR) results for RMC to FPT (resp.,5. Conclusion,[0],[0]
"FPTR) or show that the problems are W[1]-hard?
◦ Does a hardness result, similar to the one given in Theorem 14 for p-DRMC, hold for p-RMC?
Acknowledments.",5. Conclusion,[0.9718633702714685],"['◦ Does a hardness result, similar to the one given in Theorem 14 for p-DRMC, hold for p-RMC?']"
"Robert Ganian is also affiliated with FI MU, Czech Republic.",5. Conclusion,[1.0],"['Robert Ganian is also affiliated with FI MU, Czech Republic.']"
"We consider two matrix completion problems, in which we are given a matrix with missing entries and the task is to complete the matrix in a way that (1) minimizes the rank, or (2) minimizes the number of distinct rows.",abstractText,[0],[0]
"We study the parameterized complexity of the two aforementioned problems with respect to several parameters of interest, including the minimum number of matrix rows, columns, and rows plus columns needed to cover all missing entries.",abstractText,[0],[0]
"We obtain new algorithmic results showing that, for the bounded domain case, both problems are fixed-parameter tractable with respect to all aforementioned parameters.",abstractText,[0],[0]
We complement these results with a lower-bound result for the unbounded domain case that rules out fixed-parameter tractability w.r.t.,abstractText,[0],[0]
some of the parameters under consideration.,abstractText,[0],[0]
Parameterized Algorithms for the Matrix Completion Problem,title,[0],[0]
