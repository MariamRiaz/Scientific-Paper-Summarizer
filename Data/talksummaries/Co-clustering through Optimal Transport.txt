35	16	Last section concludes the paper and gives a couple of hints for possible future research.
37	11	Optimal transportation theory was first introduced in (Monge, 1781) to study the problem of resource allocation.
38	120	Assuming that we have a set of factories and a set of mines, the goal of optimal transportation is to move the ore from mines to factories in an optimal way, i.e., by minimizing the overall transport cost.
39	64	More formally, given two empirical probability measures1 µ̂S = 1 NS ∑NS i=1 δxSi and µ̂T = 1 NT ∑NT i=1 δxTi defined as uniformly weighted sums of Dirac with mass at locations supported on two point sets XS = {xSi ∈ Rd} NS i=1 and XT = {xTi ∈ Rd} NT i=1, the Monge-Kantorovich problem consists in finding a probabilistic coupling γ defined as a joint probability measure over XS × XT with marginals µ̂S and µ̂T that minimizes the cost of transport w.r.t.
42	15	The Wasserstein distance has been successfully used in various applications, for instance: computer vision (Rubner et al., 2000), texture analysis (Rabin et al., 2011), tomographic reconstruction (I. Abraham & Carlier, 2016), domain adaptation (Courty et al., 2014), metric learning (Cuturi & Avis, 2014) and clustering (Cuturi & Doucet, 2014; Irpino et al., 2014).
48	12	Second term E(γ) = − ∑NS ,NT i,j γi,j log(γi,j) in this equation allows to obtain smoother and more numerically stable solutions compared to the original case and converges to it at the exponential rate (Benamou et al., 2015).
49	56	Another advantage of entropic regularization is that it allows to solve optimal transportation problem efficiently using Sinkhorn-Knopp matrix scaling algorithm (Sinkhorn & Knopp, 1967).
52	51	Let us denote by X and Y two random variables taking values in the sets {xr}nr=1 and {yc}dc=1, respectively, where subscripts r and c correspond to rows (instances) and columns (features).
53	44	Similar to (Dhillon et al., 2003b), we assume that the joint probability distribution between X and Y denoted by p(X,Y ) is estimated from the data matrix A ∈ Rn×d.
57	46	, ŷm} where g and m denote the number of row and columns clusters, and discrete random variables X̂ and Ŷ represent the partitions induced byX and Y , i.e., X̂ = Cr(X) = and Ŷ = Cc(Y ).
59	15	We are now ready to present our method.
60	65	The main underlying idea of our approach is to use the optimal transportation presented above to find a probabilistic coupling of the empirical measures defined based on rows and columns of a given data matrix.
62	18	The elements of the resulting matrix γ∗λ provides us with the weights of associations between instances and features: similar instances and features correspond to higher values in γ∗λ.
64	37	Following (Benamou et al., 2015), this optimization problem can be equivalently rewritten in the following way: min γ∈Π(µ̂r,µ̂c) 〈M,γ〉F − 1 λ E(γ) = 1 λ min γ∈Π(µ̂r,µ̂c) KL(γ‖ξλ), where ξλ = e−λM is the Gibbs kernel.
66	16	The solution of the entropy regularized optimal transport can be obtained using Sinkhorn-Knopp algorithm and has the following form (Benamou et al., 2015): γ∗λ = diag(α)ξλdiag(β), (2) where α and β are the scaling coefficients of the Gibbs kernel ξλ.
67	37	In what follows, we show that under some plausible assumptions, we can interpret these two vectors as approximated rows and columns probability density functions.
71	62	From this point, one may instantly see that the solution of the optimal transport problem γ∗ has a very similar form as it also represents the joint probability distribution that approximates the original probability distribution p(x, y) given by the Gibbs measure ξλ and also factorizes into three terms.
95	11	Based on this definition, one may define the problem of the entropic Gromov-Wasserstein barycenters for similarity or distance matrices Kr and Kc as follows: min K,γr,γc ∑ i={r,c} εiΓK,Ki(γi)− λE(γi) (3) where K is the computed barycenter and γr ∈ Πµ̂,µ̂r , γc ∈ Πµ̂,µ̂c are the coupling matrices that align it with Kr and Kc, respectively.
133	17	We also note that CCOT-GW offers a great flexibility in terms of the possible data representation used at its input.
135	25	Algorithm 2 Co-clustering through Optimal Transport with Gromov-Wasserstein barycenters (CCOT-GW) Input : A - data matrix, λ - regularization parameter, εr , εc - weights for barycenter calculation Output: Cr, Cc - partition matrices for rows and columns, g,m - number of row and column clusters Kr ← pdist2(Z, Z) Kc ← pdist2(ZT, ZT) [βr,βc, γ ∗ r , γ ∗ c ]← gw barycenter(Kr, Kc, λ, εr, εc) [Lβrjumps, Cr, g]← jump detection(sort(βr)) [Lβcjumps, Cc, m]← jump detection(sort(βc))
144	16	As all the approaches we compared with are very sensitive to the initialization, we run them 50 times with random initializations and retain the best result according to the corresponding criterion.
147	15	For CCOT-GW, we use Gaussian kernels for both rows and columns with σ computed as the mean of all pairwise Euclidean distances between vectors (Kar & Jain, 2011).
149	17	Co-clustering performance We report the mean (and standard deviation) of co-clustering errors obtained in Table 2.
150	21	Based on these results, we observe that on D1, which has a clear block structure, all algorithms perform equally well, however CCOT-GW gives the best results, closely followed by CCOT and K-means.
155	27	Both approaches correctly identified the number of clusters in most cases and CCOT is slightly more accurate than CCOT-GW when the proportions of co-clusters are unbalanced.
157	11	Data and setting MOVIELENS-100K2 is a popular benchmark data set that consists of user-movie ratings, on a scale of one to five, collected from a movie recommendation service gathering 100,000 ratings from 943 users on 1682 movies.
161	12	CCOT-GW automatically detects a structure consisting of 9× 15 blocks, that corresponds to 9 user clusters and 15 movie clusters.
177	11	In the future, our work can be continued in multiple directions.
178	59	First, we would like to extend our method in order to deal with the online setting where the goal is to classify a new previously unseen observation without the need to do the co-clustering of the data set that includes it.
179	26	This can be done using a recent approach proposed in (Perrot et al., 2016) that allows to update the learned coupling matrix using the out-of-sample observations without recomputing it using all the data.
