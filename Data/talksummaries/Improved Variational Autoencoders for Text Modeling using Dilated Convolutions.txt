3	23	The generative story behind the VAE (to be described in detail in the next section) is simple: First, a continuous latent representation is sampled from a multivariate Gaussian.
6	8	The choice of decoding architecture and final output distribution, which connect the latent representation to output, depends on the kind of data being modeled.
7	39	The VAE owes its name to an accompanying variational technique (Kingma & Welling, 2013) that has been successfully used to train such models on image data (Gregor et al., 2015; Salimans et al., 2015; Yan et al., 2016).
8	18	The application of VAEs to text data has been far less successful (Bowman et al., 2015; Miao et al., 2016).
34	39	Specifically, we first generate a continuous latent vector representation z from a multivariate Gaussian prior pθ(z), and then generate the text sequence x from a conditional distribution pθ(x|z) parameterized using a neural net (often called the generation model or decoder).
39	9	Thus, the following variational lower bound is often used as an objective (Kingma & Welling, 2013): log pθ(x) = − log ∫ pθ(z)pθ(x|z)dz ≥ Eqφ(z|x)[log pθ(x|z)]− KL(qφ(z|x)||pθ(z)).
47	13	We can also view the VAE as a regularized version of the autoencoder.
52	65	When the training procedure collapses in this way, the result is an encoder that has duplicated the Gaussian prior (instead of a more interesting posterior), a decoder that completely ignores the latent variable z, and a learned model that reduces to a simpler language model.
54	34	The choice encoder and decoder depends on the type of data.
56	10	LSTMs have been used for text, but have resulted in training collapse as discussed above (Bowman et al., 2015).
57	53	Here, we propose to use a dilated CNN as the decoder instead.
59	41	When the width is very small, it behaves like a bag-ofwords model.
60	39	The architectural flexibility of dilated CNNs allows us to change the contextual capacity and conduct experiments to validate our hypothesis: decoder contextual capacity and effective use of encoding information are directly related.
61	16	We next describe the details of our decoder.
69	8	Dilation: Dilated convolution (Yu & Koltun, 2015) was introduced to greatly increase the effective receptive field size without increasing the computational cost.
71	8	Causal convolution can be seen a special case with d = 1.
73	8	In Figure 1b, we show dilation of sizes of 1 and 2 in the first and second layer, respectively.
78	12	Residual connection: We use residual connection (He et al., 2016) in the decoder ReLU 1x1, 512 ReLU 1xk, 512 conv ReLU 1x1, 1024 + conv conv to speed up convergence and enable training of deeper models.
85	10	In addition to conducting language modeling experiments, we will also conduct experiments on semi-supervised classification of text using our proposed decoder.
88	45	(3) The semi-supervised VAE fits a discriminative network q(y|x), an inference network q(z|x,y) and a generative network p(x|y, z) jointly as part of optimizing a variational lower bound similar that of basic VAE.
89	34	For labeled data (x,y), this bound is: log p(x,y) ≥Eq(z|x,y)[log p(x|y, z)] − KL(q(z|x,y)||p(z)) + log p(y) =L(x,y) + log p(y).
94	14	Samples from u can be approximated using: yi = exp((log(πi) + gi)/τ)∑c j=1 exp((log(πj) + gj)/τ) , (4) where gi follows Gumbel(0, 1).
99	37	Unsupervised clustering: In this section we adapt the same framework for unsupervised clustering.
100	13	We directly minimize the objective U(x), which is consisted of two parts: reconstruction loss and KL regularization on q(y|x).
101	22	The first part encourages the model to assign x to label y such that the reconstruction loss is low.
102	10	We find that the model can easily get stuck in two local optimum: the KL term is very small and q(y|x) is close to uniform distribution or the KL term is very large and all samples collapse to one class.
107	13	The original data sets contain millions of samples, of which we sample 100k as training and 10k as validation and test from the respective partitions.
109	47	Yahoo Answer contains 10 topics including Society & Culture, Science & Mathematics etc.
110	23	Yelp15 contains 5 level of rating, with higher rating better.
111	32	We use an LSTM as an encoder for VAE and explore LSTMs and CNNs as decoders.
