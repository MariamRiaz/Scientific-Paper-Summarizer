3	49	For example, in “Moscow traded gas and aluminium with Beijing.”, both location names were substituted in place of governments.
9	41	In order to accurately identify and ground location entities, for example, we must recognise that metonymic entities constitute false positives and should not be treated the same way as regular locations.
10	23	For example, in “London voted for the change.”, London refers to the concept of “people” and should not be classified as a location.
11	108	There are many types of metonymy (Shutova et al., 2013), however, in this paper, we primarily address metonymic location mentions with reference to GP and NER.
12	27	Contributions: (1) We investigate how to improve classification tasks by introducing a novel minimalist method called Predicate Window (PreWin), which outperforms common feature selection baselines.
44	20	Two types of entities were evaluated, organisations and locations, randomly retrieved from the British National Corpus (BNC).
51	16	Section 3 introduces our new dataset, Section 4 introduces our new feature extraction method.
56	31	We kept the sentences, which contained at least one of the places from a manually compiled list2 of countries and capitals of the world.
59	19	The distribution of the three classes in ReLocaR (literal, metonymic, mixed) is approximately  (49%, 49%, 2%) eliminating the high bias (80%, 18%, 2%) of SemEval.
90	29	We have also annotated a small subset of the CoNLL 2003 NER Shared Task data for metonymy resolution (locations only).
95	17	Through extensive experimentation and observation, we arrived at the intuition behind PreWin, our novel feature extraction method.
96	18	The classification decision of the class of the target entity is mostly informed not by the whole sentence (or paragraph), rather it is a small and focused “predicate window” pointed to by the entity’s head dependency.
105	60	PreWin then extracts up to 5 words and their dependency labels starting at the head of the entity (see the next paragraph for exceptions), going in the away (from the entity) direction.
112	19	Similarly, PreWin will also include the neighbouring compound noun as in: “Lead coffins were very rare in colonial America.”, the output will include “colonial” as a feature plus the next four words.
125	16	A common approach in lexical classification tasks is choosing the 5 to 10 words to the immediate right and left of the entity as input to a model (Mikolov et al., 2013; Mesnil et al., 2013; Baroni et al., 2014; Collobert et al., 2011).
131	54	For the SemEval test, we combined three separate models trained on the newly annotated CoNLL dataset and the training data for SemEval.
136	38	We only evaluate at the coarse level, which means literal versus nonliteral (metonymic and mixed are merged into one class).
145	27	Paragraph maximises the input sequence size, which maximises recall at the expense of including features that are either irrelevant or mislead the model, lowering precision.
147	46	PreWin can be understood as an integration of both approaches.
150	26	This does not appear to be the case in our experiments as PreWin regularly performs better than the “immediate” baseline.
151	26	Further prototypical examples of the method can be viewed in the Appendix.
154	73	Most of the time (typically 85% for the two datasets), PreWin is sufficient for an accurate classification.
177	20	We found that increasing dimension size (up to 300) did not materially improve performance.
178	53	The neural network tended to overfit, even with fewer epochs, the results were comparable to our default 50-dimensional embeddings.
182	33	Dataset / Method Literal Non-Literal
195	55	Metonymy is a frequent linguistic phenomenon (around 20% of location mentions are metonymic, see section 3.1) and could be handled by NER taggers to enable many innovative downstream NLP applications.
197	32	In order to monitor/mine text documents for geographical information only, the current NER technology does not have a solution.
198	126	We think it is incorrect for any NER tagger to label “Vancouver” as a location in “Vancouver welcomes you!”.
203	24	Previous work in MR such as most of the SemEval 2007 participants (Farkas et al., 2007; Nicolae et al., 2007; Leveling, 2007; Brun et al., 2007; Poibeau, 2007) and the more recent contributions used a selection of many of the following features/tools for classification: handmade trigger word lists, WordNet, VerbNet, FrameNet, extra features generated/learnt from parsing Wikipedia (approx 3B words) and BNC (approx 100M words), custom databases, handcrafted features, multiple (sometimes proprietary) parsers, Levin’s verb classes, 3,000 extra training instances from a corpus called MAScARA9 by Markert and Nissim (2002) and other extra resources including the SemEval Task 8 features.
206	91	The pressing new question is: “How much better the performance could have been if our method availed itself of the extra training data and resources used by previous works?” Indeed this may be the next research chapter for PreWin.
