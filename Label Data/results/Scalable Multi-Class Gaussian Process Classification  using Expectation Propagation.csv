0,1,label2,summary_sentences
"Given a dataset, similarity relationship between examples can be represented by a graph in which each example is represented by a vertex.",1. Introduction,[0],[0]
"While pairwise relationship between two vertices can be represented by an edge in a normal graph, a higher order relationship involving multiple vertices can be captured by a hyperedge, which means that all the corresponding examples are similar to one another.",1. Introduction,[0],[0]
"Hypergraphs have been used in several learning applications such as clustering of categorical data (Gibson et al., 1998), multi-label classification (Sun et al., 2008), Laplacian sparse coding (Gao et al., 2013), image classification (Yu et al., 2012), image retrieval (Huang et al., 2010), mapping users across different social networks (Tan et al., 2014) and predicting edge labels in hypernode graphs (Ricatte et al., 2014).
",1. Introduction,[0],[0]
"*Equal contribution 1University of Hong Kong, Hong Kong.",1. Introduction,[0],[0]
2This research was partially supported by the Hong Kong RGC under the grant 17200214.,1. Introduction,[0],[0]
"Correspondence to: T-H. Hubert Chan <hubert@cs.hku.hk>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"In this paper, we consider semi-supervised learning on an edge-weighted hypergraph H = (V,E,w), with a set L of labeled vertices, whose labels are given by f∗L ∈ {−1,+1}L. The task is to predict the labels of the unlabeled vertices N , with the working principle that vertices contained in a hyperedge e ∈ E are more similar to one another if the edge weight we is larger.",1. Introduction,[0],[0]
"This problem is also known as transductive inference and has been studied in (Zhou et al., 2006) and (Hein et al., 2013).
",1. Introduction,[0.9550857816095899],"['This has already been observed in regression problems (Bauer et al., 2016).']"
"However, the methods in (Zhou et al., 2006) have been criticized by (Agarwal et al., 2006), because essentially a hypergraph is converted into a normal graph.",1. Introduction,[0],[0]
"For instance, given a hyperedge e containing vertices S, either (i) a clique is added between the vertices in S, or (ii) a star is formed by adding a new vertex ve connecting every vertex in S to ve.",1. Introduction,[0],[0]
"Then, a standard convex program using a regularization potential function for normal graphs can be applied (Zhu et al., 2003).",1. Introduction,[0],[0]
"By choosing appropriate edge weights, it was shown in (Agarwal et al., 2006) that the two approaches are equivalent to the following convex program relaxation:
min Φold(f) := 1
2 ∑ e∈E we ∑
{u,v}∈(e2)
(fu − fv)2
subject to fu ∈",1. Introduction,[0],[0]
"[−1, 1], ∀u ∈ V fu = f ∗ u , ∀u ∈",1. Introduction,[0],[0]
"L.
On the other hand, it was proposed in (Hein et al., 2013) that the following regularization function is more suitable to capture hyperedge expansion:
Φnew(f) := 1
2 ∑ e∈E we · (max u∈e fu −min v∈e fv) 2.
",1. Introduction,[0],[0]
"Indeed, it was shown in (Hein et al., 2013) that their approach outperforms (Zhou et al., 2006) on several datasets from the UCI Machine Learning Repository (Lichman, 2013).
",1. Introduction,[0],[0]
Loss Function.,1. Introduction,[0],[0]
"In (Hein et al., 2013), a squared loss function was added by considering the convex program with objective function Φnew(f) + µ ‖f − f∗‖22 on f ∈",1. Introduction,[0],[0]
"[−1, 1]V , where µ > 0 is a parameter to be tuned, f∗L is given by the labeled vertices L, and for the unlabeled vertices f∗N = 0.
",1. Introduction,[0],[0]
"The loss function allows errors in the labeled vertices, and also ensures that the minimizer is unique.",1. Introduction,[0],[0]
"However, as a result, unlabeled vertices have a tendency to acquire f values close to 0.",1. Introduction,[0],[0]
"This might remove useful information as illustrated in the following example.
",1. Introduction,[0],[0]
Example.,1. Introduction,[0],[0]
"In Figure 1.1, vertices a, b ∈ L are labeled as +1 and c ∈ L is labeled as −1.",1. Introduction,[0],[0]
"Vertices x, y ∈ N are unlabeled.",1. Introduction,[0],[0]
"There are three (undirected) edges: {a, x}, {b, x} and {x, y, c}, each with unit weight.
",1. Introduction,[0],[0]
"By choosing µ = 12 for squared loss function, the unique minimizer gives fx = 15 and fy = 0.",1. Introduction,[0],[0]
"Hence, this solution gives no useful information regarding the label for vertex y.
On the other hand, if we just use the objective function Φnew(f) with the constraints fL = f∗L, then in an optimal solution, fx = 13 , but fy could be anywhere in the confidence interval",1. Introduction,[0],[0]
"[−1, 13 ].",1. Introduction,[0],[0]
"Hence, in this case, we could use the average value − 13 to predict −1 for vertex y.
Our Contributions.",1. Introduction,[0],[0]
"In this paper, we revisit the approach used in (Hein et al., 2013) and consider several extensions and simplifications.",1. Introduction,[0],[0]
"We summarize our results and give an outline of the paper as follows.
1.",1. Introduction,[0],[0]
Unified Framework for Directed Hypergraphs.,1. Introduction,[0],[0]
"Inspired also from the recent result on Laplacians for directed normal graphs (Yoshida, 2016), we introduce a semisupervised learning framework using directed hypergraphs that can capture higher order causal relationships.",1. Introduction,[0],[0]
"This notion of directed hypergraph was first introduced in (Gallo et al., 1993), who considered applications in propositional logic, analyzing dependency in relational database, and traffic analysis.",1. Introduction,[0],[0]
"On a high level, a directed hyperedge e consists of a tail set Te pointing to a head set He such that a vertex in Te labeled +1 implies that a vertex in He is more likely to be labeled +1.",1. Introduction,[0],[0]
"(Equivalently in terms of its contrapositive, a vertex in He labeled −1 implies that a vertex in Te is more likely to be labeled −1.)",1. Introduction,[0],[0]
"In Section 2, we formally define the model and the corresponding potential function Φ. An additional advantage of our potential function is that there is no need to tune any parameters.
2.",1. Introduction,[0],[0]
Confidence Interval for Unlabeled Vertices.,1. Introduction,[0],[0]
Observe that the minimizer for our convex program might not be unique.,1. Introduction,[0],[0]
"In Section 3, we introduce the concept of confidence interval for each unlabeled vertex that can be useful for predicting its label.",1. Introduction,[0],[0]
"Furthermore, we provide an algorithm to calculate the confidence interval given an optimal solution.
3.",1. Introduction,[0],[0]
Simpler Subgradient Method.,1. Introduction,[0],[0]
"Since the new potential function is not everywhere differentiable but still convex, we use the subgradient method (Shor et al., 1985) to obtain an estimated minimizer for label prediction.",1. Introduction,[0],[0]
"Inspired by the diffusion processes used for defining Laplacians in hypergraphs (Louis, 2015) and directed graphs (Yoshida, 2016), in Section 4, we define a simple Markov operator that returns a subgradient for Φ, which is used to solve the underlying convex program.",1. Introduction,[0],[0]
"We remark that our framework is very easy to understand, because it is a variation on the well-known gradient descent.
",1. Introduction,[0],[0]
"In contrast, the primal-dual approach in (Hein et al., 2013) considers the convex conjugate of the primal objective and involves complicated update operations on the primal and dual variables.",1. Introduction,[0],[0]
"The subgradient used in our approach gives the update direction, and we can actually solve exactly the same convex program with a much simpler method.",1. Introduction,[0],[0]
"Section 5, we revisit some datasets in the UCI Machine Learning Repository (Lichman, 2013), and experiments confirm that our prediction model based on confidence interval gives better accuracy than that in (Hein et al., 2013).",4. Experimental Results on Real-World Datasets. In,[0],[0]
"Our simpler subgradient method takes more iterations than the primal-dual method (Hein et al., 2013), but each iteration is much faster.",4. Experimental Results on Real-World Datasets. In,[0],[0]
"Experiments show that overall both methods have similar running times, and the subgradient method has an advantage when the number of vertices is much larger than the number of edges.
",4. Experimental Results on Real-World Datasets. In,[0],[0]
"Moreover, using the DBLP dataset (Ley, 2009), our experiments also support that using directed hypergraphs to capture causal relationships can improve the prediction accuracy.",4. Experimental Results on Real-World Datasets. In,[0],[0]
The experiments for directed hypergraphs are described in the full version.,4. Experimental Results on Real-World Datasets. In,[0],[0]
"We consider an edge-weighted directed hypergraph H = (V,E,w) with vertex set V (with n = |V |), edge set E and weight function",2. Preliminaries,[0],[0]
w : E → R+.,2. Preliminaries,[0],[0]
Each hyperedge e ∈ E consists of a tail set Te ⊆ V and a head set He ⊆ V (which are not necessarily disjoint); we use the convention that the direction is from tail to head.,2. Preliminaries,[0],[0]
"For x ∈ R, we denote [x]+ := max{x, 0}.
",2. Preliminaries,[0],[0]
"In our application, each vertex v ∈ V is supposed to have a label in {−1,+1}.",2. Preliminaries,[0],[0]
"Intuitively, the directed hypergraph attempts to capture the rule that for each edge e ∈ E, if there is a vertex in Te having label +1, then it is more likely for vertices in He to receive label +1.",2. Preliminaries,[0],[0]
"In terms of its contrapositive, if there is a vertex in He having label −1, then it is more likely for vertices in Te to receive label −1.
",2. Preliminaries,[0],[0]
"We use f ∈ RV to denote a vector, where the coordi-
nates are labeled by vertices in V .",2. Preliminaries,[0],[0]
"For U ⊆ V , we use fU ∈ RU to denote the vector restricting f to coordinates inU .",2. Preliminaries,[0],[0]
"In semi-supervised learning, we consider a setL ⊆ V of labeled vertices, which have labels f∗L ∈",2. Preliminaries,[0],[0]
"{−1,+1}L. Typically, |L| |V | and the task is to assign a label in {−1,+1} to each unlabeled vertex in N := V \ L, using information from the directed hypergraph H .
",2. Preliminaries,[0],[0]
"By relaxing labels to be in the interval [−1, 1], we consider the following regularization potential function Φ : RV → R:
Φ(f)",2. Preliminaries,[0],[0]
"= 1
2 ∑ e∈E we · ([∆e(f)]+)2,
where ∆e(f) := max(u,v)∈Te×He(fu − fv) = maxu∈Te fu −minv∈He fv .
",2. Preliminaries,[0],[0]
"In particular, there is a penalty due to edge e only if some vertex in Te receives a label larger than that of some vertex in He.",2. Preliminaries,[0],[0]
"The convexity of Φ is proved in the full version.
",2. Preliminaries,[0],[0]
Our approach is to consider the following convex program to obtain an estimated minimizer f ∈,2. Preliminaries,[0],[0]
"[−1, 1]V , which can be rounded to an integer solution for labeling all vertices.
min Φ(f) (CP1) subject to fu ∈",2. Preliminaries,[0],[0]
"[−1, 1], ∀u ∈ V
fu = f ∗ u , ∀u",2. Preliminaries,[0],[0]
"∈ L
Since the f values for the labeled vertices L are fixed in (CP1), we also view Φ : RN → R as a function on the f values of unlabeled vertices N .",2. Preliminaries,[0],[0]
"We use OPT ⊂ RV to denote the set of optimal solutions to (CP1).
",2. Preliminaries,[0],[0]
Trivial Edges.,2. Preliminaries,[0],[0]
An edge e ∈ E is trivial if there exist vertices u ∈ Te ∩ L and v ∈,2. Preliminaries,[0],[0]
He ∩ L such that f∗u = +1 and f∗v = −1.,2. Preliminaries,[0],[0]
"As trivial edges contribute constant towards the objective function Φ, we shall assume that there are no trivial edges in the convex program (CP1).
",2. Preliminaries,[0],[0]
Special Cases.,2. Preliminaries,[0],[0]
"Our directed hypergraph model can capture other graph models as follows.
1.",2. Preliminaries,[0],[0]
Undirected Hypergraphs.,2. Preliminaries,[0],[0]
"For each hyperedge e, we can set Te = He to the corresponding subset of vertices.",2. Preliminaries,[0],[0]
2.,2. Preliminaries,[0],[0]
Undirected Normal Graphs.,2. Preliminaries,[0],[0]
"For each edge e = {u, v}, we can set Te = He = e. Observe that in this case, the potential function becomes Φ(f) =∑
(u,v)∈E wuv(fu− fv)2, which is differentiable, and hence, (CP1) can be solved by standard techniques like gradient descent.
",2. Preliminaries,[0],[0]
Soft Constraints.,2. Preliminaries,[0],[0]
"In (Hein et al., 2013), each labeled vertex u ∈ L can also have some weight µu ∈ R+, which can, for instance, indicate how trustworthy the label
f∗u ∈ {−1,+1} is.",2. Preliminaries,[0],[0]
"The following relaxation is considered.
",2. Preliminaries,[0],[0]
"min Φ̂(f) := Φ(f) + 1
2 ∑ u∈L µu(fu",2. Preliminaries,[0],[0]
"− f∗u)2 (CP2)
subject to fu ∈",2. Preliminaries,[0],[0]
"[−1, 1],∀u ∈ V.
Observe that (CP2) can also be expressed in the framework of (CP1).",2. Preliminaries,[0],[0]
"We simply consider an augmented hypergraph Ĥ such that all vertices V are treated as unlabeled, and for each u ∈ L, we add a new vertex û with label f∗u and a new undirected edge {u, û} with weight µu.",2. Preliminaries,[0],[0]
"Then, it follows that the convex program (CP1) for the augmented instance for Ĥ is exactly the same as (CP2).
",2. Preliminaries,[0],[0]
Challenges Ahead.,2. Preliminaries,[0],[0]
"We next outline how we resolve the encountered challenges when we use (CP1) for semisupervised learning.
",2. Preliminaries,[0],[0]
"• Unlike the case for normal graphs, the set OPT can contain more than one optimal solution for (CP1).",2. Preliminaries,[0],[0]
"In Section 3, we prove some structural properties of the convex program, and illustrate that each u ∈ N has some confidence interval from which we can predict its label.",2. Preliminaries,[0],[0]
• The function Φ is not everywhere differentiable.,2. Preliminaries,[0],[0]
"Hence, we use the subgradient method (Shor et al., 1985).",2. Preliminaries,[0],[0]
"In Section 4, we give a method to generate a subgradient, which is inspired by the continuous diffusion processes for hypergraphs (Louis, 2015) and directed graphs (Yoshida, 2016), and our method can in fact be viewed as a discretized version.",2. Preliminaries,[0],[0]
"In general, a minimizer for (CP1) might not be unique.",3. Confidence Interval for Semi-supervised Learning,[0],[0]
"Hence, we introduce the concept of confidence interval.
",3. Confidence Interval for Semi-supervised Learning,[0],[0]
Definition 3.1 (Confidence Interval),3. Confidence Interval for Semi-supervised Learning,[0],[0]
"For each u ∈ V , we define its confidence interval to be [mu,Mu], where mu := minf∈OPT fu and Mu := maxf∈OPT fu.",3. Confidence Interval for Semi-supervised Learning,[0],[0]
"The confidence intervals induce the lower and the upper confidence vectors, ~m and ~M ∈ RV , respectively.
",3. Confidence Interval for Semi-supervised Learning,[0],[0]
"In Section 3.1, we give the proof of the following lemma, which states that the confidence vectors ~m and ~M are optimal solutions, and so are their convex combinations.
",3. Confidence Interval for Semi-supervised Learning,[0],[0]
Lemma 3.1 (Confidence Vectors Give Optimal Solutions),3. Confidence Interval for Semi-supervised Learning,[0],[0]
For any λ ∈,3. Confidence Interval for Semi-supervised Learning,[0],[0]
"[0, 1], the convex combination λ~m + (1− λ) ~M",3. Confidence Interval for Semi-supervised Learning,[0],[0]
"∈ OPT is optimal for (CP1).
",3. Confidence Interval for Semi-supervised Learning,[0],[0]
Semi-supervised Learning via Confidence Interval.,3. Confidence Interval for Semi-supervised Learning,[0],[0]
Lemma 3.1 suggests what one can do when (CP1) has more than one optimal solution.,3. Confidence Interval for Semi-supervised Learning,[0],[0]
"Specifically, in Algorithm 1, the
average vector 12 (~m + ~M) ∈ OPT can be used for label prediction.",3. Confidence Interval for Semi-supervised Learning,[0],[0]
"We show that the confidence vectors ~m and ~M can be recovered from any optimal solution f ∈ OPT, which in turn can be estimated by the subgradient method described in Section 4.",3. Confidence Interval for Semi-supervised Learning,[0],[0]
"Algorithm 1 Semi-Supervised Learning
1: Input: Directed hypergraph H = (V,E,w), labels f∗L for labeled vertices L 2: Compute (estimated) confidence vectors (~m, ~M) ∈",3. Confidence Interval for Semi-supervised Learning,[0],[0]
"RN × RN , either by Algorithm 2 or 3.",3. Confidence Interval for Semi-supervised Learning,[0],[0]
3: Compute average vector fN ← 12 (~m+ ~M).,3. Confidence Interval for Semi-supervised Learning,[0],[0]
4: Compute threshold θ ← 1|N | ∑ u∈N fu.,3. Confidence Interval for Semi-supervised Learning,[0],[0]
"5: for each u ∈ N do 6: if fu ≥ θ then 7: f̂u ← +1; 8: else 9: f̂u ← −1;
10: end if 11: end for 12: return f̂N
Fine-Tuning Parameters.",3. Confidence Interval for Semi-supervised Learning,[0],[0]
"In view of Lemma 3.1, one could further optimize the choice of λ ∈",3. Confidence Interval for Semi-supervised Learning,[0],[0]
"[0, 1] in defining fN ← λ~m+ (1−λ) ~M in Line 3.",3. Confidence Interval for Semi-supervised Learning,[0],[0]
"Similarly, one could pick the threshold θ to be the ϑ-percentile of the sorted coordinates of fN , for some choice of ϑ ∈",3. Confidence Interval for Semi-supervised Learning,[0],[0]
"[0, 1].",3. Confidence Interval for Semi-supervised Learning,[0],[0]
The parameters λ and ϑ can be tuned using standard techniques like cross-validation.,3. Confidence Interval for Semi-supervised Learning,[0],[0]
"However, to illustrate our concepts, we keep the description simple without introducing too many free parameters.",3. Confidence Interval for Semi-supervised Learning,[0],[0]
We derive some properties of the confidence vectors to prove Lemma 3.1.,3.1. Properties of Confidence Vectors,[0],[0]
"The full proofs of Lemma 3.2 and 3.3 are given in the full version.
",3.1. Properties of Confidence Vectors,[0],[0]
"Given a feasible solution f ∈ RV to (CP1), we define the following:
1. Se(f) := arg maxu∈Te fu ⊆ Te and Ie(f) := arg minv∈He fv ⊆ He.",3.1. Properties of Confidence Vectors,[0],[0]
2. f(Se),3.1. Properties of Confidence Vectors,[0],[0]
:= maxu∈Te fu and f(Ie) := minv∈He fv .,3.1. Properties of Confidence Vectors,[0],[0]
"Hence, we have ∆e(f) = f(Se)− f(Ie).",3.1. Properties of Confidence Vectors,[0],[0]
3.,3.1. Properties of Confidence Vectors,[0],[0]
"The set of active edges with respect to f is E(f) := {e ∈ E : ∆e(f) > 0}.
",3.1. Properties of Confidence Vectors,[0],[0]
"The following lemma states even though a minimizer for (CP1) might not be unique, there are still some structural properties for any optimal solution.
",3.1. Properties of Confidence Vectors,[0],[0]
Lemma 3.2 (Active Edges in an Optimal Solution) Suppose f and g are optimal solutions to (CP1).,3.1. Properties of Confidence Vectors,[0],[0]
"Then, for all e ∈ E, [∆e(f)]+ = [∆e(g)]+.",3.1. Properties of Confidence Vectors,[0],[0]
"In particular, this implies that the set of active edges E∗",3.1. Properties of Confidence Vectors,[0],[0]
":= E(f) = E(g) in any op-
timal solution is uniquely determined.",3.1. Properties of Confidence Vectors,[0],[0]
"Hence, for e ∈ E∗, we can define the corresponding ∆∗e = ∆e(f).
",3.1. Properties of Confidence Vectors,[0],[0]
"Definition 3.2 (Pinned Vertex) An unlabeled vertex u is pinned in a solution f ∈ RV if there exist active edges e and e′ ∈ E(f) such that u ∈ Se(f)∩ Ie′(f), in which case we say that the edges e and e′ pin the vertex u under f .
",3.1. Properties of Confidence Vectors,[0],[0]
Lemma 3.3 (Extending an Active Edge),3.1. Properties of Confidence Vectors,[0],[0]
Suppose edge e ∈ E(f) is active in an optimal solution f .,3.1. Properties of Confidence Vectors,[0],[0]
"If He does not contain a vertex labeled with −1, then there exist u ∈ Ie(f) and another active edge e′ ∈ E(f) such that the following holds.
",3.1. Properties of Confidence Vectors,[0],[0]
(a) The edges e and e′,3.1. Properties of Confidence Vectors,[0],[0]
"pin u under f , i.e., u ∈ Se′(f).",3.1. Properties of Confidence Vectors,[0],[0]
(b),3.1. Properties of Confidence Vectors,[0],[0]
"If g is an optimal solution, then Ie(f) ∩ Se′(f) =
Ie(g) ∩ Se′(g) and fu = gu.",3.1. Properties of Confidence Vectors,[0],[0]
vertex labeled with +1.,An analogous result holds when Te does not contain any,[0],[0]
∗(Ie),"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
":= minu∈He fu are uniquely determined by any optimal solution f .
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
Corollary 3.1 (Pinned Vertices),"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"In any optimal solution, the set of pinned vertices is uniquely determined.","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
We use L∗ to denote the set of labeled or pinned vertices in an optimal solution.,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Then, for each u ∈ L∗, its value f∗u in any optimal solution is also uniquely determined.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"From Corollary 3.1, the confidence interval for any u ∈ L∗ contains exactly one value, namely the unique value f∗u in any optimal solution.","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"The following lemma gives a characterization of an optimal solution.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
Lemma 3.4 Characterization of Optimal Solutions,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"A solution f to (CP1) is optimal iff the following conditions hold.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"(a) For each u ∈ L∗, fu = f∗u .","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"(b) For each active edge e ∈ E∗, both the maximum
maxu∈Te fu and the minimum minv∈He fv are attained by vertices in L∗. (c) For each inactive edge e /∈","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"E∗,","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
for all u ∈ Te and v ∈,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"He, fu ≤ fv .
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
Proof: We first observe that Corollary 3.1 states that the values of the vertices in L∗ are uniquely determined in any optimal solution.,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Hence, any optimal solution must satisfy the three conditions.","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"We next show that the three conditions implies that the objective value is optimal.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Once the values for vertices in L∗ are fixed, Lemma 3.3 and condition (b) implies that the contribution of all active edges E∗ are determined and are the same as any optimal solution.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Finally, condition (c) implies that edges not in E∗ do not have any contribution towards the objective function.","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Hence, any solution satisfying the three conditions must be optimal.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
Deriving Confidence Vectors.,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"To prove Lemma 3.5, we define a procedure that returns a vector ~m ∈ V R such that for any optimal f ∈ OPT, we have f ≥ ~m.","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Moreover, we shall show that ~m ∈ OPT and hence ~m is the lower confidence vector.","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
The argument for the upper confidence vector ~M is similar.,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"For the special case of undirected hypergraphs, the procedure can be simplified to Algorithm 2 in Section 3.2.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
Lemma 3.5 (Confidence Vectors are Optimal: Proof of Lemma 3.1),"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
The confidence vectors ~m and ~M defined in Definition 3.1 are optimal solutions to (CP1).,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"This implies that any of their convex combination is also optimal.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Proof: We give a procedure that returns a vector ~m such that at any moment during the procedure, the following invariant is maintained: for any f ∈ OPT, f ≥ ~m.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"The following steps correspond to maintaining the conditions in Lemma 3.4.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
(a) Initialization.,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"For v ∈ L∗, set mv := f∗v ; for v /∈ L∗, set mv := −1.","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"This satisfies the invariant, because for any f ∈ OPT and any v ∈ L∗, fv = f∗v .
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
(b) Preserving Active Edges.,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"For each v /∈ L∗, set mv ← max{mv,maxe∈E∗:v∈He f∗(Ie)}.","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Observe that Lemma 3.4(b) implies that for any optimal f ∈ OPT, any e ∈ E∗ and any v ∈","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"He, fv ≥ f∗(Ie).","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Hence, the invariant is maintained.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
(c) Preserving Inactive Edges.,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
While there is an inactive edge e /∈ E∗,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"such that u ∈ Te, v ∈","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"He and mu > mv , set mv ← mu.","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
We argue why each such update preserves the invariant.,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
Consider any optimal f ∈ OPT.,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Before this update, the invariant holds.","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Hence, we have mu ≤ fu.","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Moreover, Lemma 3.4 implies that fu ≤ fv .","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Therefore, after setting mv ← mu, we still have mv ≤ fv .
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Finally, observe that after step (b), the coordinates of ~m can take at most n distinct values.","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Moreover, after each update in step (c), one coordinate of ~m must increase strictly.","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Hence, this procedure will terminate.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"We next argue that ~m is an optimal solution by checking that it satisfies the conditions in Lemma 3.4.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
Condition (a).,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Observe that for each v ∈ L∗, mv is initialized to f∗v .","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
Afterwards the value mv could only be increased.,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"However, because the invariant holds when the procedure terminates, it must be the case that mv = f∗v at the end.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
Condition (b).,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"The procedure makes sure that at the end of
step (b), for every active edge e ∈ E∗, minv∈He mv can be attained by some vertex in L∗. Since only mv for v /∈ L∗ can be increased in step (c), it follows that in the end, the minimum can still be attained by some vertex in L∗.
Next, consider u ∈ Te, where e ∈ E∗. For any optimal solution f , Lemma 3.3 implies that fu ≤ f∗(Se).","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Hence, the invariant implies thatmu ≤ fu ≤ f∗(Se).","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Since condition (a) holds, this means that maxv∈Te mv can be attained by some vertex in L∗.
Condition (c).","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"This is clearly satisfied because of the while-termination condition.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Therefore, we have ~m ∈ OPT, as required.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
The proof for the upper confidence vector ~M is similar.,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"We omit the detailed proof and just give the corresponding procedure to return ~M .
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
(a) Initialization.,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"For v ∈ L∗, set Mv := f∗v ; for v /∈ L∗, set Mv := +1.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
(b) Preserving Active Edges.,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"For each v /∈ L∗, set Mv ← min{Mv,mine∈E∗:v∈Te f∗(Se)}.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
(c) Preserving Inactive Edges.,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
While there is an inactive edge e /∈ E∗,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"such that u ∈ Te, v ∈","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"He and Mu > Mv , set Mu ←Mv .
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"The same argument can show that for any optimal f ∈ OPT, we have f ≤ ~M .","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Moreover, we also have ~M ∈ OPT.","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"As mentioned before, the proof of Lemma 3.5 implicitly gives a procedure to compute the confidence vectors from any optimal solution.",3.2. Computing the Confidence Interval,[0],[0]
"For the special case of undirected hypergraphs, a simplified version of the procedure is given in Algorithm 2.
",3.2. Computing the Confidence Interval,[0],[0]
"Alternatively, we can try to solve the convex program (CP1), for example using Algorithm 5 in Section 4, from two initial feasible solutions to heuristically estimate the confidence vectors.",3.2. Computing the Confidence Interval,[0],[0]
"In Algorithm 3, one instance approaches an optimal solution from high f values and the other from low f values.",3.2. Computing the Confidence Interval,[0],[0]
Resolving Ties.,4. Subgradient Method via Markov Operator,[0],[0]
Observe that Φ : RN → R is differentiable at fN ∈ RN that has distinct coordinates.,4. Subgradient Method via Markov Operator,[0],[0]
"For the purpose of computing a subgradient, we assume that there is some global ordering π on V to resolve ties among coordinates with the same value.",4. Subgradient Method via Markov Operator,[0],[0]
"In particular, the vertices in L having label +1 are the highest, and those in L labeled −1 are the lowest.",4. Subgradient Method via Markov Operator,[0],[0]
"Hence, in this section, we may assume that any arg max or arg min operator over a subset of vertices
Algorithm 2 Confidence Intervals for Undirected Hypergraphs
1: Input: Undirected hypergraph H = (V,E,w), label vector f∗L and tolerance ≥ 0.",4. Subgradient Method via Markov Operator,[0],[0]
"2: Let f be a solution of (CP1), either by Algorithm 5 or by PDHG method (Hein et al., 2013) 3: For all v ∈ V , set p(v)← v, mv ← −1, Mv ← +1.",4. Subgradient Method via Markov Operator,[0],[0]
"4: Ê := {e ∈ E : ∆e(f) ≤ } 5: while ∃e1 6= e2 ∈ Ê, e1 ∩ e2 6= ∅",4. Subgradient Method via Markov Operator,[0],[0]
"do 6: Ê ← (Ê \ {e1, e2}) ∪ {e1 ∪ e2} 7: end while 8: for each e ∈ Ê do 9: x← an arbitrary vertex in e
10: for each vertex v ∈ e do 11: p(v)← p(x) 12: end for 13: end for 14: for each vertex v ∈ L do 15: mp(v) ← f∗v , Mp(v) ← f∗v 16: end for 17: for each edge e ∈ E such that ∆e(f) >",4. Subgradient Method via Markov Operator,[0],[0]
do 18: for each vertex v ∈,4. Subgradient Method via Markov Operator,[0],[0]
e,4. Subgradient Method via Markov Operator,[0],[0]
"do 19: mp(v) ← max{mp(v), f(Ie)} 20: Mp(v) ← min{Mp(v), f(Se)} 21: end for 22: end for 23: for each vertex v ∈ V do 24: mv ← mp(v), Mv ←Mp(v) 25: end for 26: return vectors (~m, ~M)
will return a unique vertex.
",4. Subgradient Method via Markov Operator,[0],[0]
"We next define a Markov operator that is inspired from the diffusion processes on hypergraphs (Louis, 2015) and directed graphs (Yoshida, 2016) in the context of defining Laplacians.",4. Subgradient Method via Markov Operator,[0],[0]
"We denote the projection operator ΠN : RV → RN that takes f ∈ RV and returns the restricted vector fN ∈ RN .
",4. Subgradient Method via Markov Operator,[0],[0]
Lemma 4.1 For f ∈,4. Subgradient Method via Markov Operator,[0],[0]
"[−1, 1]V that is feasible in (CP1), the Markov operator Mf given in Algorithm 4 returns a subgradient of Φ : RN → R at fN .
",4. Subgradient Method via Markov Operator,[0],[0]
"Proof: (Sketch) Observe that if fN ∈ RN has distinct coordinates, then Φ is differentiable at fN , and Mf gives exactly the gradient (which is the only possible subgradient in this case).",4. Subgradient Method via Markov Operator,[0],[0]
"Observe that in our subgradient method application, we could imagine that at every iteration, infinitesimal perturbation is performed on the current solution to ensure that all coordinates are distinct, and ties are resolved according to our global ordering π.
",4. Subgradient Method via Markov Operator,[0],[0]
"Algorithm 3 Estimate confidence interval 1: Input: Directed hypergraph H = (V,E,w), labels f∗L
for labeled vertices L 2: Construct feasible f (0,+)N ← +1 ∈ RN with all entries
being +1; 3: Construct feasible f (0,−)N ← −1 ∈",4. Subgradient Method via Markov Operator,[0],[0]
"RN with all entries
being −1; 4: ~M ← SGM(f (0,+)N ); 5: ~m← SGM(f (0,−)N ); 6: return the vectors (~m, ~M)
",4. Subgradient Method via Markov Operator,[0],[0]
"Algorithm 4 Markov Operator M : RV → RN
1: Input: Directed hypergraph H = (V,E,w), feasible f ∈ RV for (CP1) 2: Construct symmetric matrix A ∈ RV×V ; set A← 0.",4. Subgradient Method via Markov Operator,[0],[0]
3: for each e ∈ E such that ∆e(f) > 0,4. Subgradient Method via Markov Operator,[0],[0]
do 4: u← arg maxu∈Te fu; 5: v ← arg minv∈He fv; 6: Auv ← Auv + we; 7: (The same is done forAvu becauseA is symmetric.),4. Subgradient Method via Markov Operator,[0],[0]
"8: end for 9: Construct diagonal matrix W ∈ RN×N ; set W ← 0.
10: for each u ∈ N",4. Subgradient Method via Markov Operator,[0],[0]
do 11:,4. Subgradient Method via Markov Operator,[0],[0]
"Wuu ← ∑ v∈V Auv; 12: end for 13: return (WΠN −ΠNA)f
Hence, as the magnitude of the perturbation tends to zero, if the global ordering π is preserved, then the gradient remains the same, which implies that the gradient is also the subgradient when the perturbation reaches 0.
",4. Subgradient Method via Markov Operator,[0],[0]
"Using the Markov operator M as a subroutine to generate a subgradient, we have the following subgradient method (SGM) (Shor et al., 1985).
",4. Subgradient Method via Markov Operator,[0],[0]
"Algorithm 5 Subgradient Method SGM(f (0)N ∈ RN ) 1: Input: Directed hypergraph H = (V,E,w) with la-
bels f∗L for labeled vertices L, initial feasible solution f (0) N ∈",4. Subgradient Method via Markov Operator,[0],[0]
"RN , step size {ηt := 1 t }t≥1
2: t← 1; 3: (Throughout the algorithm, f (t)L = f ∗ L is given by the
labeled vertices.)",4. Subgradient Method via Markov Operator,[0],[0]
4: while Solution f (t)N has not “stabilized” do 5: g(t)N ← Mf (t−1) ∈ RN ; 6: f (t)N = f (t−1) N,4. Subgradient Method via Markov Operator,[0],[0]
"− ηt ·
g (t)",4. Subgradient Method via Markov Operator,[0],[0]
"N∥∥∥g(t)N ∥∥∥
2
;
7: t← t+ 1; 8: end while 9: return f (t)
",4. Subgradient Method via Markov Operator,[0],[0]
Stabilizing Condition.,4. Subgradient Method via Markov Operator,[0],[0]
"Our experiments in Section 5 suggest that it suffices to run the solver for a short time, after which a better feasible solution f does not improve the prediction accuracy.",4. Subgradient Method via Markov Operator,[0],[0]
Our experiments are run on a standard PC.,5. Experimental Results,[0],[0]
"In our graphs, each point refers to a sample mean, and the height of the vertical bar is the standard error of the mean.",5. Experimental Results,[0],[0]
"We show that our treatment of hypergraphs performs better than the previously best method in (Hein et al., 2013).
",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
Hypergraph Model.,5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"We use three datasets from the UCI Machine Learning Repository (Lichman, 2013): mushroom, covertype45 and covertype67.",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"As in (Hein et al., 2013), each dataset fits into the hypergraph learning model in the following way.",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"Each entry in the dataset corresponds to a vertex, which is labeled either +1 or −1.",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"Moreover, each entry has some categorical attributes.",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"For each attribute and each realized value for that attribute, we form a unit-weight hyperedge containing all the vertices corresponding to entries having that attribute value.",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"To summarize, below are the properties of the resulting hypergraphs.
",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"Dataset mushroom covertype45 covertype67
n = |V",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"| 8124 12240 37877 m = |E| 112 104 123 k =∑
e∈E |e| m
1523 1412 3695
Semi-supervised Learning Framework.",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"We compare our semi-supervised learning framework with that in (Hein et al., 2013), which was previously the best (compared to (Zhou et al., 2006), for instance).",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"Specifically, we compare the prediction accuracy of the following two prediction algorithms.
1.",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
Confidence Interval (CI).,5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"We use hard constraints (CP1) and confidence intervals for prediction, as described in Algorithm 1 in Section 3. 2.",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"Hein et al. We implement the method described in (Hein et al., 2013), which uses soft constraints (regularized version), plus 5-fold cross validation to determine the regularization parameter.
",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
Testing Methodology.,5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"Since we focus on prediction accuracy, using either subgradient method or PDHG (Hein et al., 2013) for solving the underlying convex programs in each algorithm produces the same results.",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"For each algorithm candidate, we try different sizes of labeled vertices L, where l = |L| ranges from 20 to 200.",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"For each size l
of labeled vertices, we randomly pick l vertices from the dataset to form the set L and treat the rest as unlabeled vertices; we re-sample if only one label (+1 or −1) appears in L. For each size",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"l, we perform 100 trials to report the average error rate together with its standard error.
Results.",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"Our experiment can recover the results reported in (Hein et al., 2013).",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"The test error for the two algorithms on the three datasets is presented in Figure 5.1, which shows that our CI method consistently has lower test error than the one in (Hein et al., 2013).",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
Different Solvers.,5.2. Comparing Running Times of Solvers,[0],[0]
"We compare the running times of the following two convex program solvers:
• Subgradient Method (SG), proposed by us.",5.2. Comparing Running Times of Solvers,[0],[0]
"Empirically, the step size ηt := 1
(t+1) min( 0.16t 105 ,1)
gives good
performance.",5.2. Comparing Running Times of Solvers,[0],[0]
"For large t, ηt grows like 1t and so the method converges; however, for small t, we would like a larger step size to speed up convergence.",5.2. Comparing Running Times of Solvers,[0],[0]
"• Primal-Dual Hybrid Gradient (PDHG), proposed in (Hein et al., 2013).",5.2. Comparing Running Times of Solvers,[0],[0]
"We choose σ = τ = 1√
1+d ,
where d is the maximum degree.
",5.2. Comparing Running Times of Solvers,[0],[0]
Theoretical Analysis.,5.2. Comparing Running Times of Solvers,[0],[0]
"Given a hypergraph with n vertices and m edges, where the average size of an edge is k, each vertex on average appears in mkn edges.",5.2. Comparing Running Times of Solvers,[0],[0]
"For SG, we use a heap-based data structure to maintain the vertices within a hyperedge.",5.2. Comparing Running Times of Solvers,[0],[0]
"Vertices attaining the maximum and the minimum value within a hyperedge can be retrieved in O(1) time, and a value update takes O(log k) time.",5.2. Comparing Running Times of Solvers,[0],[0]
"In each iteration, at most 2m vertices will have their values updated.",5.2. Comparing Running Times of Solvers,[0],[0]
"Hence, in each iteration, SG takes time 2m·mkn ·O(log k) = O(m
2k n log k).",5.2. Comparing Running Times of Solvers,[0],[0]
"In the description of PDHG in (Hein et al., 2013), each iteration takesO(mk log k) time.",5.2. Comparing Running Times of Solvers,[0],[0]
"Hence, when n m, each iteration of SG will be significantly faster, although in general, the number of iterations required by the subgradient method can be larger than that for PDHG.
",5.2. Comparing Running Times of Solvers,[0],[0]
Testing Methodology.,5.2. Comparing Running Times of Solvers,[0],[0]
"In each experiment, we consider the hypergraph from one of the above three datasets.",5.2. Comparing Running Times of Solvers,[0],[0]
"We pick l = 160 vertices at random as the labeled vertices L, and form the corresponding convex program (CP1) for the two solvers, where the initial values for unlabeled vertices are chosen independently to be uniformly at random from [−1, 1].",5.2. Comparing Running Times of Solvers,[0],[0]
"To compare the performance, we run the two solvers on the same convex program, and record each trajectory of the objective value versus the time duration.",5.2. Comparing Running Times of Solvers,[0],[0]
"According to experience, 100 seconds is good enough for either solver to reach an almost optimal solution, and we use the minimum value achieved by the two solvers after 100 seconds as an estimate for the true optimal value OPT.",5.2. Comparing Running Times of Solvers,[0],[0]
"Then, we scan each trajectory, and for each relative gap
∈ {10−i : i = 1, 2, . . .",5.2. Comparing Running Times of Solvers,[0],[0]
", 6}, we find the smallest time T ( ) after which the objective value is at most OPT away from the estimate OPT.",5.2. Comparing Running Times of Solvers,[0],[0]
Each instance of the experiment is repeated 100 times (with different sets of labeled vertices) to obtain an average of those T ( )’s and their standard error.,5.2. Comparing Running Times of Solvers,[0],[0]
"For each relative gap , we also report the test error for using a feasible solution that is OPT away from the presumed optimal value OPT.
Results.",5.2. Comparing Running Times of Solvers,[0],[0]
Both solvers have similar performance.,5.2. Comparing Running Times of Solvers,[0],[0]
"As predicted by our theoretical analysis, we see in Figure 5.2 that SG has an advantage when the number n of vertices is much larger than the number m of edges, which is the case for the the last dataset covertype67.",5.2. Comparing Running Times of Solvers,[0],[0]
"Moreover, in Figure 5.3, we see that achieving a relative gap smaller than 10−4 has almost no effect on improving the prediction accuracy.",5.2. Comparing Running Times of Solvers,[0],[0]
"Hence, we can conclude that for either solver, it takes roughly 10 to 20 seconds to produce a solution for the underlying convex program that can give good predic-
tion accuracy.",5.2. Comparing Running Times of Solvers,[0],[0]
DBLP Dataset.,5.3. Directed Hypergraph: More Powerful,[0],[0]
"We use the DBLP (Ley, 2009) dataset.",5.3. Directed Hypergraph: More Powerful,[0],[0]
Each paper is represented by a vertex.,5.3. Directed Hypergraph: More Powerful,[0],[0]
"We include papers from year 2000 to 2015 from conferences belonging to the following research areas to conduct our experiments:
• 7049 papers from machine learning (ML): NIPS, ICML • 2539 papers from theoretical computer science (TCS): STOC, FOCS • 3374 papers from database (DB): VLDB, SIGMOD
We perform the following prediction tasks: (a) ML (+1) vs TCS (-1), and (b) ML (+1) vs DB (-1).
",5.3. Directed Hypergraph: More Powerful,[0],[0]
The details of the experiment setup and the results are given in the full version.,5.3. Directed Hypergraph: More Powerful,[0.9577222917692739],['An experimental comparison with the variational approach and related methods from the literature shows that the proposed approach has benefits both in terms of the training speed and the accuracy of the predictive distribution.']
We revisit semi-supervised learning on hypergraphs.,abstractText,[0],[0]
"Same as previous approaches, our method uses a convex program whose objective function is not everywhere differentiable.",abstractText,[0],[0]
"We exploit the non-uniqueness of the optimal solutions, and consider confidence intervals which give the exact ranges that unlabeled vertices take in any optimal solution.",abstractText,[0],[0]
"Moreover, we give a much simpler approach for solving the convex program based on the subgradient method.",abstractText,[0],[0]
"Our experiments on real-world datasets confirm that our confidence interval approach on hypergraphs outperforms existing methods, and our sub-gradient method gives faster running times when the number of vertices is much larger than the number of edges.",abstractText,[0],[0]
Re-revisiting Learning on Hypergraphs:  Confidence Interval and Subgradient Method,title,[0],[0]
"The Fisher Information Metric (FIM) I(Θ) = (Iij) of a statistical parametric model p(x |Θ) of order D is defined by a D×D positive semidefinite (psd) matrix (I(Θ) 0) with coefficients Iij = Ep [ ∂l ∂Θi ∂l ∂Θj ] , where l(Θ) denotes the log-density function log p(x |Θ).",1. Fisher Information Metric,[0],[0]
"Under light regularity conditions, FIM can be rewritten equivalently as
Iij = −Ep",1. Fisher Information Metric,[0],[0]
"[ ∂2l
∂Θi∂Θj
] = 4 ∫ ∂ √ p(x |Θ) ∂Θi ∂ √ p(x |Θ) ∂Θj dx.
",1. Fisher Information Metric,[0],[0]
"As its empirical counterpart, the observed FIM (Efron & Hinkley, 1978) with respect to (wrt) a sample set Xn = {xk}nk=1 is Î(Θ |Xn) = −∇2l(Θ",1. Fisher Information Metric,[0],[0]
"|Xn), which is often evaluated at the maximum likelihood estimate Θ = Θ̂(Xn).",1. Fisher Information Metric,[0],[0]
"By the law of large numbers, Î(Θ) converges to the (expected) FIM I(Θ) as n→∞.
1King Abdullah University of Science and Technology (KAUST), Saudi Arabia 2École Polytechnique, France 3Sony Computer Science Laboratories Inc., Japan.",1. Fisher Information Metric,[0],[0]
"Correspondence to: Ke Sun <sunk@ieee.org>, Frank Nielsen <Frank.Nielsen@acm.org>.
",1. Fisher Information Metric,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Fisher Information Metric,[0],[0]
"Copyright 2017 by the author(s).
",1. Fisher Information Metric,[0],[0]
The FIM is not invariant and depends on the parameterization.,1. Fisher Information Metric,[0],[0]
We can optionally write I(Θ) as IΘ(Θ) to emphasize the coordinate system.,1. Fisher Information Metric,[0],[0]
"By definition, IΘ(Θ) = JᵀIΛ(Λ)J where J = (Jij), Jij = ∂Λi∂Θj is the Jacobian matrix.",1. Fisher Information Metric,[0],[0]
"For example, the FIM of regular natural exponential families (NEFs) l(Θ) = Θᵀt(x)",1. Fisher Information Metric,[0],[0]
− F (Θ) (loglinear models with sufficient statistics t(x)) is I(Θ),1. Fisher Information Metric,[0],[0]
"= ∇2F (Θ) 0, the Hessian of the log-normalizer function F (Θ).",1. Fisher Information Metric,[0],[0]
"Although exponential families can approximate arbitrarily any smooth density (Cobb et al., 1983), the lognormalizer function may not be available in closed-form nor computationally tractable (Montanari, 2015).
",1. Fisher Information Metric,[0],[0]
The FIM is an important concept for statistical machine learning.,1. Fisher Information Metric,[0],[0]
"It gives a Riemannian metric (Hotelling, 1929; Rao, 1945) of the learning parameter space which is unique (Čencov, 1982; Dowty, 2017).",1. Fisher Information Metric,[0],[0]
"Hence any learning is in a space that is intrinsically curved based on the FIM, regardless of the choice of the coordinate system.",1. Fisher Information Metric,[0],[0]
"It also gives a bound (Fréchet, 1943; Cramér, 1946; Nielsen, 2013) of learning efficiency saying that the variance of any unbiased learning of Θ is at least I−1(Θ)/n, where n is the i.i.d. sample size.",1. Fisher Information Metric,[0],[0]
"The FIM is applied to neural network optimization (Amari, 1997), metric learning (Lebanon, 2005), reinforcement learning (Thomas, 2014) and manifold learning (Sun & Marchand-Maillet, 2014).
",1. Fisher Information Metric,[0],[0]
However computing the FIM is expensive.,1. Fisher Information Metric,[0],[0]
"Besides the fact that learning machines have often singularities (Watanabe, 2009) (|I(Θ)| = 0, not full rank) characterized by plateaux in gradient learning, computing/estimating the FIM of a large neuron system (e.g. one with millions of parameters, Szegedy, Christian et al. 2015) is very challenging due to the finiteness of data, and the huge number D(D+1)2 of matrix coefficients to evaluate.",1. Fisher Information Metric,[0],[0]
"Furthermore, gradient descent techniques require inverting this large matrix and tuning the learning rate.
",1. Fisher Information Metric,[0],[0]
"To tackle this problem, past works mainly focus on how to approximate the FIM with a block diagonal form (Kurita, 1994; Le Roux et al., 2008; Martens, 2010; Pascanu & Bengio, 2014; Martens & Grosse, 2015) or quasi-diagonal form (Ollivier, 2013; Marceau-Caron & Ollivier, 2016).",1. Fisher Information Metric,[0],[0]
"This global approach faces increasing approximation error and increasing computational cost as the system scales up
and as complex and dynamic structures (Looks et al., 2017) emerge.
",1. Fisher Information Metric,[0],[0]
This work aims at a different local approach.,1. Fisher Information Metric,[0],[0]
"The idea is to accurately describe the information geometry (IG) in a subsystem of the large learning system, which is invariant to the scaling up and structural change of the global system, so that the local machinery, including optimization, can be discussed regardless of the other parts.
",1. Fisher Information Metric,[0],[0]
"For this purpose, a novel concept, the Relative Fisher Information Metric (RFIM), is defined.",1. Fisher Information Metric,[0],[0]
"Unlike the traditional geometric view of a high-dimensional parameter manifold, RFIMs defines multiple projected low-dimensional geometries of subsystems.",1. Fisher Information Metric,[0],[0]
This geometry is correlated to the parameters beyond the subsystem and is therefore considered dynamic.,1. Fisher Information Metric,[0],[0]
It can be used to characterize the efficiency of a local learning process.,1. Fisher Information Metric,[0],[0]
Taking this stance has potential in deep learning because a deep neural network can be decomposed into many local components such as neurons or layers.,1. Fisher Information Metric,[0],[0]
The RFIM is well suited to the compositional block structures of neural networks.,1. Fisher Information Metric,[0],[0]
"The RFIM can be used for out-of-core learning.
",1. Fisher Information Metric,[0],[0]
The paper is organized as follows.,1. Fisher Information Metric,[0],[0]
Sec. 2 reviews natural gradient within the context of Multi-Layer Perceptrons (MLPs).,1. Fisher Information Metric,[0],[0]
"Sec. 3 formally defines the RFIM, and gives a table of RFIMs of several commonly used subsystems.",1. Fisher Information Metric,[0],[0]
Sec. 4 discusses the advantages of using the RFIM as compared to the FIM. Sec. 5 gives an algorithmic framework and proof-of-concept experiments on neural network optimization.,1. Fisher Information Metric,[0],[0]
Sec. 6 presents related works on parameter diagonalization.,1. Fisher Information Metric,[0],[0]
Sec. 7 concludes this work and further hints at perspectives.,1. Fisher Information Metric,[0],[0]
"Consider a MLP x θ1−→ h1 · · ·hL−1 θL−−→ y, whose statistical model is the following conditional distribution
p(y |x,Θ) = ∑
h1,··· ,hL−1
p(h1 |x,θ1) · · · p(y |hL−1,θL).
",2. Natural Gradient: Review and Insights,[0],[0]
"The often intractable sum over h1, · · · ,hL−1 can be get rid off by deteriorating p(h1 |x,θ1), · · · , p(hL−1 |hL−2,θL−1) to Dirac’s deltas δ, and letting merely the last layer p(y |hL−1,θL) be stochastic.",2. Natural Gradient: Review and Insights,[0],[0]
"Other models such as restricted Boltzmann machines (Nair & Hinton, 2010; Montavon & Müller, 2012), deep belief networks (Hinton et al., 2006), dropout (Wager et al., 2013), and variational autoencoders (Kingma & Welling, 2014) do consider the hi’s to be stochastic.
",2. Natural Gradient: Review and Insights,[0],[0]
"The tensor metric of the neuromanifold (Amari, 1995) M, consisting of all MLPs with the same architecture but different parameter values, is locally defined by the FIM.",2. Natural Gradient: Review and Insights,[0],[0]
"Because a MLP corresponds to a con-
ditional distribution, its FIM is a function of the input x. By taking an empirical average over the input samples {xk}nk=1, the FIM of a MLP can be expressed as IΘ(Θ) = 1n",2. Natural Gradient: Review and Insights,[0],[0]
"∑n k=1Ep(y |xk,Θ) [ ∂lk ∂Θ ∂lk ∂Θᵀ ] , where lk(Θ) = log p(y |xk, Θ) denotes the conditional log-likelihood function wrt xk.
",2. Natural Gradient: Review and Insights,[0],[0]
"To understand the meaning of the Riemannian metric IΘ(Θ), it measures the intrinsic difference between two nearby neural networks around Θ ∈ M. A learning step can be regarded as a tiny displacement δΘ",2. Natural Gradient: Review and Insights,[0],[0]
onM.,2. Natural Gradient: Review and Insights,[0],[0]
"According to the FIM, the infinitesimal square distance
〈δΘ, δΘ〉IΘ(Θ) = 1
n n∑ k=1 Ep(y |xk,Θ)
[( δΘᵀ
∂lk ∂Θ )",2. Natural Gradient: Review and Insights,[0],[0]
"2] (1)
measures how much δΘ",2. Natural Gradient: Review and Insights,[0],[0]
"(with a radius constraint) is statistically along ∂l∂Θ , or equivalently how much δΘ affects intrinsically the conditional distribution p(y |x, Θ).
",2. Natural Gradient: Review and Insights,[0],[0]
Consider the negative log-likelihood function L(Θ) =,2. Natural Gradient: Review and Insights,[0],[0]
"− ∑n k=1 log p(yk |xk,Θ) wrt the observed pairs {(xk,yk)}nk=1, we try to minimize the loss while maintaining a small learning step size 〈δΘ, δΘ〉IΘ(Θ) on M. At Θt ∈ M, the target is to minimize wrt δΘ",2. Natural Gradient: Review and Insights,[0],[0]
"the Lagrange function
L(Θt + δΘ)",2. Natural Gradient: Review and Insights,[0],[0]
"+ 1
2γ 〈δΘ, δΘ〉IΘ(Θt)
",2. Natural Gradient: Review and Insights,[0],[0]
"≈ L(Θt) + δΘᵀ 5Θ L(Θt) + 1
2γ δΘᵀIΘ(Θt)δΘ,
where γ > 0 is a learning rate.",2. Natural Gradient: Review and Insights,[0],[0]
"The optimal solution of the above quadratic optimization gives a learning step
δΘt = −γI−1Θ (Θt)5Θ L(Θt).
",2. Natural Gradient: Review and Insights,[0],[0]
"In this update procedure, ∇̃ΘL(Θ) = I−1Θ (Θ)5Θ L(Θ) replaces the role of the usual gradient ∇ΘL(Θ) and is called the natural gradient (Amari, 1997).
",2. Natural Gradient: Review and Insights,[0],[0]
"Although the FIM depends on the chosen parameterization, the natural gradient is invariant to reparameterization.",2. Natural Gradient: Review and Insights,[0],[0]
Let Λ be another coordinate system and J be the Jacobian matrix of the mapping,2. Natural Gradient: Review and Insights,[0],[0]
"Θ→ Λ. Then we have
I−1Θ (Θ)5Θ L(Θ) =",2. Natural Gradient: Review and Insights,[0],[0]
"(J ᵀIΛ(Λ)J)−1 Jᵀ 5Λ L(Λ)
",2. Natural Gradient: Review and Insights,[0],[0]
"= J−1I−1Λ (Λ)5Λ L(Λ),
showing that ∇̃ΘL(Θ) and ∇̃ΛL(Λ) are the same dynamic up to coordinate transformation.",2. Natural Gradient: Review and Insights,[0],[0]
"As the learning rate γ is not infinitesimal in practice, natural gradient descent actually depends on the coordinate system (see e.g. Martens 2014).",2. Natural Gradient: Review and Insights,[0],[0]
"Other intriguing properties of natural gradient optimization lie in being free from getting trapped in plateaux of the error surface, and attaining Fisher efficiency in online learning (see Sec. 4 Amari 1998).
",2. Natural Gradient: Review and Insights,[0],[0]
"MΘ
Θ yx
Mθ1
x
x+ ∆x
θ1x
Mθ2h1
h1 + ∆h1
θ2h1
Mθ3
h2
h2 + ∆h2
θ3h2",2. Natural Gradient: Review and Insights,[0],[0]
"y
Model:
Manifold:
Computational graph:
Metric:
Θ
Θ I(Θ)
θ3 h2
θ3
h2
gy(θ3)
",2. Natural Gradient: Review and Insights,[0],[0]
"θ2 h1
θ2
h1
gh2(θ2)
θ1
θ1 gh1(θ1)
p(y |Θ,x) =",2. Natural Gradient: Review and Insights,[0],[0]
"∑ h1 ∑ h2 p(h1 |θ1,x) p(h2 |θ2,h1) p(y |θ3,h2)
",2. Natural Gradient: Review and Insights,[0],[0]
Figure 1.,2. Natural Gradient: Review and Insights,[0],[0]
(left),2. Natural Gradient: Review and Insights,[0],[0]
The traditional global geometry of a MLP; (right) information geometry of subsystems.,2. Natural Gradient: Review and Insights,[0],[0]
The gray and blue meshes show that the subsystem geometry is dynamic when the reference variable makes a tiny move.,2. Natural Gradient: Review and Insights,[0],[0]
"The square under the (sub-)system means the (R-)FIM is computed by (i) computing the FIM in the traditional way wrt all free parameters that affect the system output; (ii) choosing a sub-block that contains only the internal parameters of the (sub-)system and regarding the remaining variables as the reference.
",2. Natural Gradient: Review and Insights,[0],[0]
"For the sake of simplicity, we do not discuss singular FIMs with a subset of parameters having zero metric.",2. Natural Gradient: Review and Insights,[0],[0]
"This set of parameters forms an analytic variety (Watanabe, 2009), and technically the MLP as a statistical model is said to be non-regular (and the parameter Θ is not identifiable).",2. Natural Gradient: Review and Insights,[0],[0]
"The natural gradient has been extended (Thomas, 2014) to cope with singular FIMs having positive semi-definite matrices by taking the Moore-Penrose pseudo-inverse (that coincides with the inverse matrix for full rank matrices).
",2. Natural Gradient: Review and Insights,[0],[0]
"In the family of 2nd-order optimization methods, a fuzzy line can be drawn from the natural gradient and alternative methods such as the Hessian-free optimization (Martens, 2010).",2. Natural Gradient: Review and Insights,[0],[0]
"By definition, the FIM is a property of the parameter space which is independent or weakly dependent on the input samples.",2. Natural Gradient: Review and Insights,[0],[0]
"For example, the FIM of a MLP is independent of {yi}.",2. Natural Gradient: Review and Insights,[0],[0]
"In contrast, the Hessian (or related concepts such as the Gauss-Newton matrix, Martens 2014) is a property of the learning cost function wrt the input samples.
",2. Natural Gradient: Review and Insights,[0],[0]
"Bonnabel (Bonnabel, 2013) proposed to use the Riemannian exponential map to define a gradient descent step, thus ensuring to stay on the manifold for any chosen learning rate.",2. Natural Gradient: Review and Insights,[0],[0]
Convergence is proven for Hadamard manifolds (of negative curvatures).,2. Natural Gradient: Review and Insights,[0],[0]
"However, it is not mathematically tractable to express the exponential map of hierarchical model manifolds like the neuromanifold.",2. Natural Gradient: Review and Insights,[0],[0]
"In general, for large parametric systems, it is impossible to diagonalize or decorrelate all the parameters, so that we split instead all random variables into three parts θf , θ and h.",3. RFIM: Definition and Expressions,[0],[0]
We examine their intuitive meanings before giving the formal definition.,3. RFIM: Definition and Expressions,[0],[0]
"The reference, θf , consists of the majority of the random variables that are considered fixed (therefore allowing us to simplify the analysis).",3. RFIM: Definition and Expressions,[0],[0]
This is in analogy to the notion of a reference frame in physics.,3. RFIM: Definition and Expressions,[0],[0]
"θ is the
subsystem parameters, resembling the long-term memory adapting slowly to the observations (e.g. neural network weights).",3. RFIM: Definition and Expressions,[0],[0]
The response h is a random variable that reacts to the variations of θ.,3. RFIM: Definition and Expressions,[0],[0]
"Usually, h is the output of the subsystem that is connected to neighbour subsystems (e.g. hidden layer outputs).",3. RFIM: Definition and Expressions,[0],[0]
"Formally, a subsystem which factorizes the learning machine is characterized by the conditional distribution p(h |θ,θf ), where θ can be estimated based on h and θf .",3. RFIM: Definition and Expressions,[0],[0]
We make the following definition.,3. RFIM: Definition and Expressions,[0],[0]
Definition 1 (RFIM).,3. RFIM: Definition and Expressions,[0],[0]
"Given θf , the RFIM 1 of θ wrt h is
gh (θ |θf )",3. RFIM: Definition and Expressions,[0],[0]
"def = Ep(h | θ, θf ) [ ∂
∂θ log p(h |θ, θf )
∂
∂θᵀ log p(h |θ, θf )
] ,
or simply gh (θ), corresponding to the estimation of θ based on observations of h given θf .
",3. RFIM: Definition and Expressions,[0],[0]
"For example, consider a MLP.",3. RFIM: Definition and Expressions,[0],[0]
"If we choose θf to be the input features x, choose h to be the final output y, and choose θ to be all the network weights Θ, then the RFIM becomes the FIM: I(Θ) = gy(Θ |x).
",3. RFIM: Definition and Expressions,[0],[0]
"More generally, we can choose the response h to be other than the observables to compute the Fisher information of subsystems, especially dynamically during the learning of the global machine.",3. RFIM: Definition and Expressions,[0],[0]
"To see the meaning of the RFIM, similar to eq.",3. RFIM: Definition and Expressions,[0],[0]
"(1), the infinitesimal square distance 〈δθ, δθ〉gh(θ) =",3. RFIM: Definition and Expressions,[0],[0]
"Ep(h | θ, θf ) [( δθᵀ ∂∂θ log p(h |θ, θf )
)2] measures how much δθ impacts intrinsically the stochastic mapping θ → h which features the subsystem.",3. RFIM: Definition and Expressions,[0],[0]
"We have the following proposition following definition 1.
",3. RFIM: Definition and Expressions,[0],[0]
Proposition 2 (Relative Geometry Consistency).,3. RFIM: Definition and Expressions,[0],[0]
"If θ1 consists of a subset of θ2 so that θ2 = (θ1, θ̃1), then ∀θ̃1, Mθ1 with the metric gh(θ1 | θ̃1) has exactly the same Rie-
1We use the same term “relative FIM” (Zegers, 2015) with a different definition.
",3. RFIM: Definition and Expressions,[0],[0]
"mannian metric with the sub-manifold {θ2 ∈ Mθ2 : θ̃1 is fixed} induced by the ambient metric gh (θ2).
",3. RFIM: Definition and Expressions,[0],[0]
"When the response h is chosen, then different splits of (θ,θf ) are consistent with the same ambient geometry.
",3. RFIM: Definition and Expressions,[0],[0]
"Figure 1 shows the traditional global geometry of a learning system, where the curvature is defined by the learner’s parameter sensitivity to the external environment (x and y), as compared to the information geometry of subsystems, where the curvature is defined by the parameter sensitivity wrt hidden interface variables h.",3. RFIM: Definition and Expressions,[0],[0]
"The two-colored meshes show that the geometry structure is dynamic and varies with the reference variable θf .
",3. RFIM: Definition and Expressions,[0],[0]
"One should not confuse the RFIM with the diagonal blocks of the FIM (Kurita, 1994).",3. RFIM: Definition and Expressions,[0],[0]
Both their meanings and expressions are different.,3. RFIM: Definition and Expressions,[0],[0]
The RFIM is computed by integrating out the hidden response variables h.,3. RFIM: Definition and Expressions,[0],[0]
The FIM is always computed by integrating out the observables x and y.,3. RFIM: Definition and Expressions,[0],[0]
Hence the RFIM is a more general concept and includes the FIM as a special case.,3. RFIM: Definition and Expressions,[0],[0]
"This highlights a main difference with the backpropagated metric (Ollivier, 2013), which essentially considers parameter sensitivity wrt the final output.",3. RFIM: Definition and Expressions,[0],[0]
"Despite the fact that the FIMs of small parametric structures such as single neurons was studied (Amari, 1997), we are not looking at a small single-component system but a component embedded in a large system, targeting at improving the large system.
",3. RFIM: Definition and Expressions,[0],[0]
"In the following we provide a short table of commonly used RFIMs for future reference (the RFIMs listed are mostly straightforward from definition 1, with detailed derivations given in the supplementary material).",3. RFIM: Definition and Expressions,[0],[0]
This is meaningful since the RFIM is a new concept.,3. RFIM: Definition and Expressions,[0],[0]
We also want to demonstrate these simple closed form expressions without any approximations.,3. RFIM: Definition and Expressions,[0],[0]
We start from the RFIM of single neuron models.,3.1. RFIMs of One Neuron,[0],[0]
"Consider a stochastic neuron with input x and weights w. After a nonlinear activation function f , the output y is randomized surrounding the mean f(wᵀx̃) with a variance.",3.1. RFIMs of One Neuron,[0],[0]
"Throughout this paper x̃ = (xᵀ, 1)ᵀ denotes the augmented vector of x (homogeneous coordinates) so that wᵀx̃ contains a bias term, and a general linear transformation can be written simply asAx̃.
Using x as the reference, the RFIM of w with respect to y has a common form gy(w |x)",3.1. RFIMs of One Neuron,[0],[0]
"= νf (w,x)x̃x̃ᵀ, where νf (w,x) is a positive coefficient with large values in the linear region, or the effective learning zone of the neuron.",3.1. RFIMs of One Neuron,[0],[0]
"This agrees with early studies on single neuron FIMs (Amari, 1997; Kurita, 1994).
",3.1. RFIMs of One Neuron,[0],[0]
"If f(t) = tanh(t) is the hyperbolic tangent func-
tion, then νf (w,x) = sech2(wᵀx̃), where sech(t) = 2 exp(t)+exp(−t) is the hyperbolic secant function.",3.1. RFIMs of One Neuron,[0],[0]
"Similarly, if f(t) = sigm(t) is the sigmoid function, then νf (w,x) = sigm (w ᵀx̃)",3.1. RFIMs of One Neuron,[0],[0]
"[ 1− sigm (wᵀx̃) ] .
",3.1. RFIMs of One Neuron,[0],[0]
"If f is defined by Parametric Rectified Linear Unit (PReLU) (He et al., 2015), which includes Rectified Linear Unit (ReLU) (Nair & Hinton, 2010) as a special case, so that f(t) = t (t ≥ 0), f(t) = ιt (t < 0), 0 ≤ ι < 1, then under certain approximations (see supplementary material)
",3.1. RFIMs of One Neuron,[0],[0]
"νf (w,x) =
[ ι+ (1− ι)sigm",3.1. RFIMs of One Neuron,[0],[0]
"( 1− ι ω wᵀx̃ )]2 ,
where ω > 0 is a hyper-parameter (e.g. ω = 1).
",3.1. RFIMs of One Neuron,[0],[0]
"For the exponential linear unit (ELU) (Clevert et al., 2015), f(t) = t (t ≥ 0), f(t) = α (exp(t)− 1) (t < 0), where α > 0 is a hyper-parameter.",3.1. RFIMs of One Neuron,[0],[0]
"We get
νf (w,x) =",3.1. RFIMs of One Neuron,[0],[0]
{ 1 if wᵀx̃ ≥ 0 α2 exp (2wᵀx̃),3.1. RFIMs of One Neuron,[0],[0]
if wᵀx̃ < 0.,3.1. RFIMs of One Neuron,[0],[0]
Let D denote the dimensionality of the corresponding variable.,3.2. RFIM of One Layer,[0],[0]
"A linear layer with input x, connection weights W =",3.2. RFIM of One Layer,[0],[0]
"[ w1, · · · ,wDy ] , and stochastic output y can be represented by y ∼ G(W ᵀx̃, σ2I), where I is the identity matrix, and σ is the scale of the observation noise, and G(µ,Σ) is a multivariate Gaussian distribution with mean µ and covariance matrix Σ. We vectorize W by stacking its columns {wi}.",3.2. RFIM of One Layer,[0],[0]
"Then gy(W |x) is a tensor of size (Dx + 1)Dy× (Dx + 1)Dy , given by gy(W |x)",3.2. RFIM of One Layer,[0],[0]
"= diag [x̃x̃ᵀ, · · · , x̃x̃ᵀ], where diag(·) means the (block) diagonal matrix constructed by the given matrix entries.
",3.2. RFIM of One Layer,[0],[0]
"A nonlinear layer increments a linear layer by adding an element-wise activation function applied on W ᵀx̃, and then randomized wrt the choice of the neuron.",3.2. RFIM of One Layer,[0],[0]
"By definition 1, its RFIM is given by
gy (",3.2. RFIM of One Layer,[0],[0]
W |x) =,3.2. RFIM of One Layer,[0],[0]
"diag [ νf (w1,x)x̃x̃ ᵀ, · · · , νf (wm,x)x̃x̃ᵀ ] , (2)
where νf (wi,x) is given in Subsec.",3.2. RFIM of One Layer,[0],[0]
"3.1.
",3.2. RFIM of One Layer,[0],[0]
"A softmax layer, which often appears as the last layer of a MLP, is given by y ∈ {1, . . .",3.2. RFIM of One Layer,[0],[0]
",m}, where p(y) = ηy = exp(wyx̃)∑m i=1 exp(wix̃) .",3.2. RFIM of One Layer,[0],[0]
"Its RFIM is a dense matrix given by
gy(W )",3.2. RFIM of One Layer,[0],[0]
=  (η1 − η21)x̃x̃ᵀ · · · −η1ηmx̃x̃ᵀ −η2η1x̃x̃ᵀ · · · −η2ηmx̃x̃ᵀ ... . .,3.2. RFIM of One Layer,[0],[0]
".
...",3.2. RFIM of One Layer,[0],[0]
−ηmη1x̃x̃ᵀ · · · (ηm − η2m)x̃x̃ᵀ  .,3.2. RFIM of One Layer,[0],[0]
Notice that its i’th diagonal block (ηi − η2i ),3.2. RFIM of One Layer,[0],[0]
x̃x̃ᵀ resembles the RFIM of a single sigm neuron.,3.2. RFIM of One Layer,[0],[0]
"By eq. (2), the one-layer RFIM is a product metric (Jost, 2011) and does not consider the inter-neuron correlations, which must be obtained by looking at a larger subsystem.",3.3. RFIM of Two Layers,[0],[0]
"Consider a two-layer model with stochastic output y around the mean vector f(Cᵀh̃), where h = f (W ᵀx̃).",3.3. RFIM of Two Layers,[0],[0]
"For simplicity, we ignore inter-layer correlations between the first layer and the second layer and focus on the interneuron correlations within the first layer.",3.3. RFIM of Two Layers,[0],[0]
"To do this, both x and C are considered as references to compute the RFIM of W .",3.3. RFIM of Two Layers,[0],[0]
"By definition 1, gy(W |x,C) =",3.3. RFIM of Two Layers,[0],[0]
"[Gij ]Dh×Dh and each block has the form
Gij = Dy∑ l=1 cilcjlνf (cl,h)νf (wi,x)νf (wj ,x)x̃x̃ ᵀ.
Now that we have the one-layer and two-layer RFIMs, we can either split a given feed-forward neural network into one-layer subsystems or into two-layer subsystems.",3.3. RFIM of Two Layers,[0],[0]
"A trade-off is that using a larger subsystem entails greater analytical and computational difficulty, although it could more accurately model the global system dynamics.",3.3. RFIM of Two Layers,[0],[0]
"In the extreme case, the FIM is obtained if the whole system is considered as one single subsystem.",3.3. RFIM of Two Layers,[0],[0]
This section discusses the theoretical advantages of the RFIM over the FIM.,4. RFIM: Key Advantages,[0],[0]
"Consider wlog a MLP with Bernoulli outputs y ∈ {0, 1}m, whose mean µ is a deterministic function depending on the input x and the network parameters Θ. By Sec. 2, the FIM of the MLP can be computed as (see supplementary for proof)
I(Θ)",4. RFIM: Key Advantages,[0],[0]
= 1 n n∑ i=1,4. RFIM: Key Advantages,[0],[0]
"m∑ j=1
1 µj(xi)(1− µj(xi))",4. RFIM: Key Advantages,[0],[0]
"∂µj(xi) ∂Θ ∂µj(xi) ∂Θᵀ .
",4. RFIM: Key Advantages,[0],[0]
(3) Therefore rank(I(Θ)),4. RFIM: Key Advantages,[0],[0]
≤,4. RFIM: Key Advantages,[0],[0]
nm.,4. RFIM: Key Advantages,[0],[0]
The rank of a diagonal block of I(Θ) corresponding to one layer is even smaller.,4. RFIM: Key Advantages,[0],[0]
"In a deep neural network (e.g. Szegedy, Christian et al. 2015), if the sample size n < dim(Θ)/m, then I(Θ) is doomed to be singular.",4. RFIM: Key Advantages,[0],[0]
All methods trying to approximate the FIM suffer from this problem and therefore rely on proper regularizations.,4. RFIM: Key Advantages,[0],[0]
"If the network is decomposed into layers, the RFIM of each subsystem (layer) is given by eq.",4. RFIM: Key Advantages,[0],[0]
(2).,4. RFIM: Key Advantages,[0],[0]
Each sample can contribute maximally 1 to the rank of the neuron-RFIM and can contribute maximally Dy to the rank of the layer-RFIM.,4. RFIM: Key Advantages,[0],[0]
"It only requires maxi{dim(wi)} (the maximum layer width) observations to have a full rank RFIM, where wi is the weight vector of the i’th neuron.",4. RFIM: Key Advantages,[0],[0]
The RFIM is expected to have a much higher rank than the FIM.,4. RFIM: Key Advantages,[0],[0]
Higher rank means less singularity and more information is captured.,4. RFIM: Key Advantages,[0],[0]
"Models that can
be distinguished by the RFIM may be identical in the sense of the FIM.",4. RFIM: Key Advantages,[0],[0]
"Essentially, the RFIM integrates the internal randomness (Bengio, 2013) of the neural system by considering the output of each layer as a random variable.",4. RFIM: Key Advantages,[0],[0]
"In theory, the FIM should also consider stochastic neurons.",4. RFIM: Key Advantages,[0],[0]
"However it requires marginalizing the joint distribution of h1, h2, · · · , y. This makes the already infeasible computation even more challenging.
",4. RFIM: Key Advantages,[0],[0]
"The RFIM is not an approximation of the FIM but is an accurate metric, defining the geometry of θ wrt to its direct response h in the system, or adjacent nodes in a graphical model.",4. RFIM: Key Advantages,[0],[0]
By the example in fig.,4. RFIM: Key Advantages,[0],[0]
"1, gy(θL) of the last layer is exactly the corresponding block in I(Θ): they both characterize how θL affects the mapping hL−1",4. RFIM: Key Advantages,[0],[0]
→ y. They start to diverge from the second to last layer.,4. RFIM: Key Advantages,[0],[0]
"To compute the geometry of θL−1, the RFIM looks at how θL−1 affects the local mapping hL−2 → hL−1, which can be measured reliably regardless of the rest of the system (think of a “debugging” process to separate and measure a single component).",4. RFIM: Key Advantages,[0],[0]
"In contrast, the FIM examines how θL−1 affects the non-local mapping hL−2 → y.",4. RFIM: Key Advantages,[0],[0]
This is a difficult task because it must consider the correlation between different layers.,4. RFIM: Key Advantages,[0],[0]
"As an approximation, the block diagonalized version of the FIM ignores such correlations and therefore faces the loss of accuracy.
",4. RFIM: Key Advantages,[0],[0]
The RFIM makes it possible to maintain global system stability so that the intrinsic variations of different subsystems are balanced during learning.,4. RFIM: Key Advantages,[0],[0]
Consider a set of interconnected subsystems with internal parameters {θl} and the corresponding response variables {hl}.,4. RFIM: Key Advantages,[0],[0]
The RFIM ghl(θl) measures how much the likelihood surface of hl is curved wrt a small learning step δθl.,4. RFIM: Key Advantages,[0],[0]
"By constraining the squared Riemannian distance δθᵀl g
hl(θl)δθl having similar scales, different subsystems will present similar variations during learning.",4. RFIM: Key Advantages,[0],[0]
"Within one subsystem, the learning along sensitive parameter directions is penalized.",4. RFIM: Key Advantages,[0],[0]
"Among different subsystems, the learning of sensitive subsystems is penalized.",4. RFIM: Key Advantages,[0],[0]
"Globally, the inter-subsystem stochastic connections have similar variance, maintaining a stable reference system and achieving efficient learning.",4. RFIM: Key Advantages,[0],[0]
"This is similar to the idea of batch normalization (BN) (Ioffe & Szegedy, 2015) but has a deeper theoretical foundation.
",4. RFIM: Key Advantages,[0],[0]
"Formally, we have the following theorem.
",4. RFIM: Key Advantages,[0],[0]
Theorem 3.,4. RFIM: Key Advantages,[0],[0]
"Consider a learning system represented by a joint distribution p(x,h) of x (observables) and h (hidden variables which connect subsystems).",4. RFIM: Key Advantages,[0],[0]
"The joint FIM J (Θ) = Ep ( log p(x,h |Θ) ∂Θ",4. RFIM: Key Advantages,[0],[0]
"log p(x,h |Θ) ∂Θᵀ ) has a block diagonal form.",4. RFIM: Key Advantages,[0],[0]
"Each block isEp(gh(θ)), where θ is the parameters within a subsystem and h is its response variables to neighour subsystems.
",4. RFIM: Key Advantages,[0],[0]
"The global correspondence of the local RFIM is the joint
FIM.",4. RFIM: Key Advantages,[0],[0]
"By theorem 3, the square distance dΘᵀJ (Θ)dΘ = Ep( ∑ l dθ ᵀ l g hl(θl)dθl) measures the system variance, including both the observables x and the hidden variables h.",4. RFIM: Key Advantages,[0],[0]
An intrinsic trade-off between the RFIM and the FIM is learning system stability versus efficiency.,4. RFIM: Key Advantages,[0],[0]
"Normalizing the FIM is more efficient because it helps to achieve Fisher efficiency (Amari, 1998).",4. RFIM: Key Advantages,[0],[0]
"Normalizing the RFIM is more stable since the hidden variations are bounded, which only guarantees subsystem Fisher efficiency characterized by the Cramér-Rao lower bound of local parameters.",4. RFIM: Key Advantages,[0],[0]
The traditional non-parametric way of applying natural gradient requires re-calculating the FIM and solving a large linear system in each learning step.,5. Relative Natural Gradient Descent,[0],[0]
"Besides the huge computational cost, it has a large approximation error.",5. Relative Natural Gradient Descent,[0],[0]
"For example during online learning, a mini-batch of samples cannot faithfully reflect the “true” geometry, which has to integrate the risk of sample variations.",5. Relative Natural Gradient Descent,[0],[0]
"That is, the FIM of a mini-batch is likely to be singular or poorly conditioned.
",5. Relative Natural Gradient Descent,[0],[0]
"A recent series of efforts (Montavon & Müller, 2012; Raiko et al., 2012; Desjardins et al., 2015) are gearing towards a parametric approach to applying natural gradient, which memorizes and learns a geometry.",5. Relative Natural Gradient Descent,[0],[0]
"For example, natural neural networks (Desjardins et al., 2015) augment each layer with a redundant linear layer, and let these linear layers parametrize the geometry of the neural manifold.
",5. Relative Natural Gradient Descent,[0],[0]
"By dividing the learning system into subsystems, the RFIM potentially gives a systematical implementation of parametric natural gradient descent.",5. Relative Natural Gradient Descent,[0],[0]
"The memory complexity of storing the Riemannian metric has been reduced from O(D2) to O( ∑ iD 2 i ), where Di = dim(wi) is the size of the i’th neuron.",5. Relative Natural Gradient Descent,[0],[0]
"Consider there are M neurons in total, then the memory cost is reduced by a factor of M .",5. Relative Natural Gradient Descent,[0],[0]
"The computational complexity has been reduced from O(D%) (% ≈ 2.373, Williams 2012) to O( ∑ iD % i ).",5. Relative Natural Gradient Descent,[0],[0]
"Optimization based on RFIM is called Relative Natural Gradient Descent (RNGD).
",5. Relative Natural Gradient Descent,[0],[0]
"The good performance of batch normalization (Ioffe & Szegedy, 2015) provides an empirical support for the RFIM.",5. Relative Natural Gradient Descent,[0],[0]
"Basically, BN uses an inter-sample normalization layer to transform the layer input x to z with zero mean and unit variance and thus reduces “internal covariate shift”.",5. Relative Natural Gradient Descent,[0],[0]
"In a typical case, above this normalization layer is a linear layer given by y = W ᵀz̃.",5. Relative Natural Gradient Descent,[0],[0]
"If each dimension of z is normalized, then the diagonal blocks of the linear layer RFIM gy(W )",5. Relative Natural Gradient Descent,[0],[0]
"= diag[z̃z̃ᵀ, · · · , z̃z̃ᵀ] become a covariance matrix with identity diagonal entries (after taking an empirical average).",5. Relative Natural Gradient Descent,[0],[0]
"This gives the coordinate system W a well conditioned RFIM for efficient learning.
5.1.",5. Relative Natural Gradient Descent,[0],[0]
"RNGD with a relu MLP
",5. Relative Natural Gradient Descent,[0],[0]
This subsection builds a proof-of-concept experiment on MLP optimization.,5. Relative Natural Gradient Descent,[0],[0]
We partition the MLP into layers (one layer consists of a linear layer plus an element-wise nonlinear activation function) as the subsystems.,5. Relative Natural Gradient Descent,[0],[0]
"By eq. (2), the RFIM of layer l (l = 1, · · · , L) with input hl−1 (h0 = x) and weights {wl1, · · · ,wlml} is
diag [ νf (wl1,hl−1)h̃l−1h̃ ᵀ l−1, · · · , νf (wlml ,hl−l)h̃l−1h̃ ᵀ l−1 ] .
",5. Relative Natural Gradient Descent,[0],[0]
The subsystem stability during one learning step δw can be measured geometrically by∑L l=1 ∑ml i=1,5. Relative Natural Gradient Descent,[0],[0]
"νf (wli,hl−1)(δw ᵀ lih̃l−1)
2.",5. Relative Natural Gradient Descent,[0],[0]
"Using this term as the geometric cost (the Lagrange term) in the trust region approach in Sec. 2, we get the following RNGD method.",5. Relative Natural Gradient Descent,[0],[0]
"In a stochastic gradient descent scenario, each neuron i in layer l is updated by
wnewli ← woldli −G−1li ∂E
∂wli ,
where E is the cost function and Gli is a learned metric.",5. Relative Natural Gradient Descent,[0],[0]
"The consideration is that a mini-batch of samples do not contain enough information to compute the RFIM, which should be averaged over all training samples.",5. Relative Natural Gradient Descent,[0],[0]
"Therefore, for the i’th neuron in layer l, Gli is initialized to identity, and is updated based on
Gnewli ← (1− λ)Goldli + λνf (wli,hl−1)h̃l−1h̃ ᵀ",5. Relative Natural Gradient Descent,[0],[0]
l−1,5. Relative Natural Gradient Descent,[0],[0]
+,5. Relative Natural Gradient Descent,[0],[0]
"I,
where > 0 is a hyper-parameter to avoid singularity caused by small sample size, and the average is taken over all samples in a mini-batch, and λ is a learning rate.",5. Relative Natural Gradient Descent,[0],[0]
"In theory, λ should be gradually reduced to zero to guarantee the convergence of this geometry learning.",5. Relative Natural Gradient Descent,[0],[0]
"To avoid solving a linear system in each iteration, every T iterations we recompute and store G−1li based on the most updated Gli.",5. Relative Natural Gradient Descent,[0],[0]
"In the next T iterations, this G−1li will be used as an approximation of the inverse RFIM.",5. Relative Natural Gradient Descent,[0],[0]
"For the input layer which scales with the number of input features, and the final softmax layer, we apply instead the RFIM of the corresponding linear layer to improve the computational efficiency.
",5. Relative Natural Gradient Descent,[0],[0]
We compare different optimizers on classifying MNIST digits.,5. Relative Natural Gradient Descent,[0],[0]
"The network has shape 784-80-80-80-10, with relu activation units, a final soft-max layer, and uses the persample average cross-entropy with L2-regularization as the learning cost function.",5. Relative Natural Gradient Descent,[0],[0]
"We experiment on two different architectures: one is a plain MLP (PLAIN); the other has a batch normalization layer after each hidden layer (BNA), where a rescaling parameter is applied to ensure enough flexibility of the parametric structure (Ioffe & Szegedy, 2015).",5. Relative Natural Gradient Descent,[0],[0]
"For simplicity, the architecture, mini-batch size (50), and L2 regularization strength (10−3) are fixed to be the same for all compared methods.",5. Relative Natural Gradient Descent,[0],[0]
"The observations are consistent when these configurations vary.
",5. Relative Natural Gradient Descent,[0],[0]
Figure 2 shows the learning curves of different methods.,5. Relative Natural Gradient Descent,[0],[0]
SGD is stochastic gradient descent.,5. Relative Natural Gradient Descent,[0],[0]
"ADAM is the Adam optimizer (Kingma & Ba, 2014) with β1 = 0.9, β2 = 0.999 and = 10−8.",5. Relative Natural Gradient Descent,[0],[0]
"Our RNGD is implemented by modifying TensorFlow’s (Abadi, Martı́n",5. Relative Natural Gradient Descent,[0],[0]
"et al., 2015) SGD optimizer.",5. Relative Natural Gradient Descent,[0],[0]
"We set empirically T = 100, λ = 0.005 and ω = 1.
RNGD presents a sharper learning curve and better generalization, especially when it is combined with BN.",5. Relative Natural Gradient Descent,[0],[0]
"In this case, the final tranining error of RNGD is slightly larger than ADAM because by validation it favors a larger learning rate, which is applied on the neural network weights (based on RNGD) and BN parameters (based on SGD).",5. Relative Natural Gradient Descent,[0],[0]
"For the ReLU activation, νf (wi,x) is approximately binary, emphasizing such informative samples with wᵀi x̃ > 0, which are the ones contributing to the learning of wi with non-zero gradient values.",5. Relative Natural Gradient Descent,[0],[0]
Each output neuron has a different subset of informative samples.,5. Relative Natural Gradient Descent,[0],[0]
"RNGD normalizes x differently wrt different output neurons, so that the in-
formative samples for each output neuron are centered and decorrelated.
",5. Relative Natural Gradient Descent,[0],[0]
"In the above experiment, RNGD’s computational time per each epoch is roughly 4 ∼ 10 times more than SGD and ADAM on a modern graphic card.",5. Relative Natural Gradient Descent,[0],[0]
Therefore in terms of wall clock time RNGD does not show advantages.,5. Relative Natural Gradient Descent,[0],[0]
This can be improved by more efficient implementations with low rank approximation techniques and early stopping.,5. Relative Natural Gradient Descent,[0],[0]
Our RNGD prototype hints at a promising direction to develop scalable 2nd-order deep learning optimizers based on the RFIM.,5. Relative Natural Gradient Descent,[0],[0]
One may ponder whether we can always find a suitable parameterization that yields a diagonal FIM that is straightforward to invert.,6. Related Works on FIM Diagonalization,[0],[0]
This fundamental problem of parameter orthogonalization was first investigated by Jeffreys (1998) for decorrelating the parameters of interest from the nuisance parameters.,6. Related Works on FIM Diagonalization,[0],[0]
"Fisher diagonalization yields parameter orthogonalization (Cox & Reid, 1987), and is proved useful when estimating Θ̂ using a maximum likelihood estimator (MLE) that is asymptotically normally distributed, Θ̂n ∼ G(Θ, I−1(Θ)/n), and efficient since the variance of the estimator matches the Cramér-Rao lower bound.",6. Related Works on FIM Diagonalization,[0],[0]
"Using the chain rule, this amounts to find a suitable parameterization Ω = Ω(Θ) satisfying∑
",6. Related Works on FIM Diagonalization,[0],[0]
"i,j
E
[ ∂2l
∂Θi∂Θj ] ∂Θi",6. Related Works on FIM Diagonalization,[0],[0]
"∂Ωk ∂Θj ∂Ωl = 0, ∀k 6=",6. Related Works on FIM Diagonalization,[0],[0]
"l.
Thus in general, we end up with ( D 2 ) = D(D−1)2 (nonlinear) partial differential equations to satisfy (Huzurbazar, 1950).",6. Related Works on FIM Diagonalization,[0],[0]
"Therefore, in general there is no solution when( D 2 )",6. Related Works on FIM Diagonalization,[0],[0]
"> D, that is when D > 3.",6. Related Works on FIM Diagonalization,[0],[0]
"When D = 2, the single differential equation is usually solvable and tractable, and the solution may not be unique: For example, Huzurbazar (1950) reports two orthogonalization schemes for the location-scale families { 1σp0( x−µ σ )} that include the Gaussian family and the Cauchy family.",6. Related Works on FIM Diagonalization,[0],[0]
"Sometimes, the structure of the differential equation system yields a solution: For example, Jeffreys (1998) reported a parameter orthogonalization for Pearson’s distributions of type I which is of orderD = 4.",6. Related Works on FIM Diagonalization,[0],[0]
"Cox and Reid (1987) further investigated this topic with application to conditional inference, and provide examples (including the Weibull distribution).
",6. Related Works on FIM Diagonalization,[0],[0]
"From the viewpoint of geometry, the FIM induces a Riemannian manifold with metric tensor g(Θ) = I(Θ).",6. Related Works on FIM Diagonalization,[0],[0]
"When the FIM may be degenerate, this yields a pseudoRiemannian manifold (Thomas, 2014).",6. Related Works on FIM Diagonalization,[0],[0]
"In differential geometry, orthogonalization amounts to transforming the square length infinitesimal element gijdΘiΘj of a Riemannian geometry into an orthogonal system ω with match-
ing square length infinitesimal element ΩiidΩ2i .",6. Related Works on FIM Diagonalization,[0],[0]
"However, such a global orthogonal metric does not exist (Huzurbazar, 1950)",6. Related Works on FIM Diagonalization,[0],[0]
"when D > 3 for an arbitrary metric tensor, although interesting Riemannian parameterization structures may be derived in Riemannian 4D geometry (Grant & Vickers, 2009).
",6. Related Works on FIM Diagonalization,[0],[0]
"For NEFs, the FIM can be made block-diagonal easily by using the mixed coordinate system (Amari, 2016) (Θ1:k,Hk+1:D), where H = Ep[t(x)] = ∇F (Θ) is the moment parameter, for any k ∈ {1, ..., D − 1}, where vb:e denotes the subvector (vb, ..., ve)ᵀ of v. The geometry of NEFs is a dually flat structure (Amari, 2016) induced by the convex mgf, the potential function.",6. Related Works on FIM Diagonalization,[0],[0]
"It defines a dual affine coordinate systems ei = ∂i = ∂∂Hi and ej = ∂
j = ∂∂Θj that are orthogonal: 〈ei, ej〉 = δij , where δij = 1 iff i = j and δij = 0 otherwise.",6. Related Works on FIM Diagonalization,[0],[0]
Hence the FIM has two diagonal blocks.,6. Related Works on FIM Diagonalization,[0],[0]
"Those dual affine coordinate systems are defined up to an affine invertible transformation: Θ̃ = AΘ + b, H̃ = A−1H + c.",6. Related Works on FIM Diagonalization,[0],[0]
"In particular, for any order-2 NEF (D = 2), we can always obtain two mixed parameterizations (Θ1, H2) or (H1,Θ2).
",6. Related Works on FIM Diagonalization,[0],[0]
The RFIM contributes another line of thought in parameter diagonalization.,6. Related Works on FIM Diagonalization,[0],[0]
"We investigate the Fisher information of hidden variables, or internal interfaces in the learning machine.",6. Related Works on FIM Diagonalization,[0],[0]
"This is novel since the majority of previous works concentrate on the FIM of the observables, or the external interface of the machine.",6. Related Works on FIM Diagonalization,[0],[0]
"From a causality perspective, we factor out the main cause (parameters within the subsystem) of the response variable with a direct action-reaction relationship, and regard the remaining parameters as a reference that can be easily estimated by the empirical distribution.",6. Related Works on FIM Diagonalization,[0],[0]
"This simplification may lead to broader applications of Fisher information in machine learning.
",6. Related Works on FIM Diagonalization,[0],[0]
"The particular case of a mixed coordinate system (that is not an affine coordinate system) induces in information geometry (Amari, 2016) a dual pair of orthogonal e- and morthogonal foliations.",6. Related Works on FIM Diagonalization,[0],[0]
"Our splits in RFIMs consider general non-orthogonal foliations that provide the factorization decompositions of the whole manifold into submanifolds, that are the leaves of the foliation (see section 3.7 of Amari & Nagaoka 2000).",6. Related Works on FIM Diagonalization,[0],[0]
We investigate local structures of large learning systems using the new concept of Relative Fisher Information Metric.,7. Conclusion and Discussions,[0],[0]
The key advantage of this approach is that the local learning dynamics can be analyzed in an accurate way without approximation.,7. Conclusion and Discussions,[0],[0]
"We present a core list of such local structures in neural networks, and give their corresponding RFIMs.",7. Conclusion and Discussions,[0],[0]
"This list of recipes can be used to provide guiding principles to design new optimizers for deep learning.
",7. Conclusion and Discussions,[0],[0]
"Our work applies to mirror descent as well since natural gradient is related to mirror descent (Raskutti & Mukherjee, 2015) as follows:",7. Conclusion and Discussions,[0],[0]
"In mirror descent to minimize a cost function E(Θ), given a strictly convex distance function D(·, ·) in the first argument (playing the role of the proximity function), we express the gradient descent step as:
Θt+1 = arg min Θ
{ Θ>∇E(Θt) + 1
γ D(Θ,Θt)
} .
",7. Conclusion and Discussions,[0],[0]
"When D(Θ,Θ′) is chosen as a Bregman divergence BF (Θ,Θ
′) = F (Θ)− F (Θ′)− (Θ−Θ′)>∇F (Θ′) wrt to a convex function F , it has been proved that the mirror descent on the Θ-parameterization is equivalent (Raskutti & Mukherjee, 2015) to the natural gradient optimization on the induced Riemannian manifold with metric tensor (∇2F (Θ)) parameterized by the dual coordinate system H = ∇F (Θ).
",7. Conclusion and Discussions,[0],[0]
"In general, to perform a Riemannian gradient descent for minimizing a real-valued function f(Θ) on the manifold, one needs to choose a proper metric tensor given in matrix form G(Θ).",7. Conclusion and Discussions,[0],[0]
Thomas (2014) constructed a toy example showing that the natural gradient may diverge while the ordinary gradient (for G = I) converges.,7. Conclusion and Discussions,[0],[0]
"Recently, Thomas et al. (2016) proposed a new kind of descent method based on what they called the Energetic Natural Gradient that generalizes the natural gradient.",7. Conclusion and Discussions,[0],[0]
"The energy distance DE(p(Θ1), p(Θ2))2 = E[2dp(Θ1)(X,Y )",7. Conclusion and Discussions,[0],[0]
"− dp(Θ1)(X,X
′)",7. Conclusion and Discussions,[0],[0]
"− dp(Θ1)(Y, Y ′)] where X,X ′ ∼ p(Θ1) and Y, Y ′ ∼ p(Θ2), where dp(Θ1)(·, ·) is a distance metric over the support.",7. Conclusion and Discussions,[0],[0]
"Using a Taylor’s expansion on their energy distance, they get the Energy Information Matrix (in a way similar to recovering the FIM from a Taylor’s expansion of any f -divergence like the Kullback-Leibler divergence).",7. Conclusion and Discussions,[0],[0]
Their idea is to incorporate prior knowledge on the structure of the support (observation space) to define energy distance.,7. Conclusion and Discussions,[0],[0]
"Twisting the geometry of the support (say, Wasserstein’s optimal transport) with the geometry of the parametric distributions (Fisher-Rao geodesic distances) is indeed important (Chizat et al., 2015).",7. Conclusion and Discussions,[0],[0]
"In information geometry, invariance on the support is provided by a Markov morphism that is a probabilistic mapping of the support to itself (Čencov, 1982).",7. Conclusion and Discussions,[0],[0]
There is no neighbourhood structure on the support in IG.,7. Conclusion and Discussions,[0],[0]
Markov morphism includes deterministic transformation of a random variable by a statistic.,7. Conclusion and Discussions,[0],[0]
It is well-known that IT (Θ) IX(Θ) with equality iff.,7. Conclusion and Discussions,[0],[0]
T = T (X) is a sufficient statistic of X .,7. Conclusion and Discussions,[0],[0]
"Thus to get the same invariance for the energy distance (Thomas et al., 2016), one shall further require dp(Θ)(T (X), T (Y ))",7. Conclusion and Discussions,[0],[0]
"= dp(Θ)(X,Y ).
",7. Conclusion and Discussions,[0],[0]
We believe that RFIMs will provide a sound methodology to build further efficient systems for deep learning.,7. Conclusion and Discussions,[0],[0]
The full source codes to reproduce the experimental results are available at https://www.lix.polytechnique.,7. Conclusion and Discussions,[0],[0]
fr/˜nielsen/RFIM.,7. Conclusion and Discussions,[0],[0]
The authors would like to thank the anonymous reviewers and Yann Ollivier for the helpful comments.,Acknowledgements,[0],[0]
This work was mainly conducted when the first author was a postdoctoral researcher at École Polytechnique.,Acknowledgements,[0],[0]
Fisher information and natural gradient provided deep insights and powerful tools to artificial neural networks.,abstractText,[0],[0]
However related analysis becomes more and more difficult as the learner’s structure turns large and complex.,abstractText,[0],[0]
This paper makes a preliminary step towards a new direction.,abstractText,[0],[0]
"We extract a local component from a large neural system, and define its relative Fisher information metric that describes accurately this small component, and is invariant to the other parts of the system.",abstractText,[0],[0]
This concept is important because the geometry structure is much simplified and it can be easily applied to guide the learning of neural networks.,abstractText,[0],[0]
"We provide an analysis on a list of commonly used components, and demonstrate how to use this concept to further improve optimization.",abstractText,[0],[0]
1.,abstractText,[0],[0]
Fisher Information Metric The Fisher Information Metric (FIM) I(Θ) =,abstractText,[0],[0]
"(Iij) of a statistical parametric model p(x |Θ) of order D is defined by a D×D positive semidefinite (psd) matrix (I(Θ) 0) with coefficients Iij = Ep [ ∂l ∂Θi ∂l ∂Θj ] , where l(Θ) denotes the log-density function log p(x |Θ).",abstractText,[0],[0]
"Under light regularity conditions, FIM can be rewritten equivalently as Iij = −Ep [ ∂l ∂Θi∂Θj ]",abstractText,[0],[0]
Relative Fisher Information and Natural Gradient for Learning Large Modular Models,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 44–54, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.
Our model is evaluated both for accuracy in predicting target order, and for its impact on translation quality. We report significant performance gains over phrase reordering, and over two known preordering baselines for English-Japanese.",text,[0],[0]
"Preordering (Collins et al., 2005) aims at permuting the words of a source sentence s into a new order ś, hopefully close to a plausible target word order.",1 Introduction,[0],[0]
"Preordering is often used to bridge long distance reorderings (e.g., in Japanese- or GermanEnglish), before applying phrase-based models (Koehn et al., 2007).",1 Introduction,[0],[0]
"Preordering is often broken down into two steps: finding a suitable tree structure, and then finding a transduction function over it.",1 Introduction,[0],[0]
"A common approach is to use monolingual syntactic trees and focus on finding a transduction function of the sibling subtrees under the nodes (Lerner and Petrov, 2013; Xia and Mccord, 2004).",1 Introduction,[0],[0]
"The (direct correspondence) assumption
underlying this approach is that permuting the siblings of nodes in a source syntactic tree can produce a plausible target order.",1 Introduction,[0],[0]
"An alternative approach creates reordering rules manually and then learns the right structure for applying these rules (Katz-Brown et al., 2011).",1 Introduction,[0],[0]
"Others attempt learning the transduction structure and the transduction function in two separate, consecutive steps (DeNero and Uszkoreit, 2011).",1 Introduction,[0],[0]
"Here we address the challenge of learning both the trees and the transduction functions jointly, in one fell swoop, from word-aligned parallel corpora.
",1 Introduction,[0],[0]
Learning both trees and transductions jointly raises two questions.,1 Introduction,[0],[0]
How to obtain suitable trees for the source sentence and how to learn a distribution over random variables specifically aimed at reordering in a hierarchical model?,1 Introduction,[0],[0]
"In this work we solve both challenges by using the factorizations of permutations into Permutation Trees (PETs) (Zhang and Gildea, 2007).",1 Introduction,[0],[0]
"As we explain next, PETs can be crucial for exposing the hierarchical reordering patterns found in wordalignments.
",1 Introduction,[0],[0]
We obtain permutations in the training data by segmenting every word-aligned source-target pair into minimal phrase pairs; the resulting alignment between minimal phrases is written as a permutation (1:1 and onto) on the source side.,1 Introduction,[0],[0]
"Every permutation can be factorized into a forest of PETs (over the source sentences) which we use as a latent treebank for training a Probabilistic ContextFree Grammar (PCFG) tailor made for preordering as we explain next.
",1 Introduction,[0],[0]
Figure 1 shows two alternative PETs for the same permutation over minimal phrases.,1 Introduction,[0],[0]
"The nodes have labels (like P3142) which stand for local permutations (called prime permutation) over the child nodes; for example, the root label P3142 stands for prime permutation 〈3, 1, 4, 2〉, which says that the first child of the root becomes 3rd on the target side, the second becomes 1st, the third
44
becomes 4th and the fourth becomes 2nd.",1 Introduction,[0],[0]
"The prime permutations are non-factorizable permutations like 〈1, 2〉, 〈2, 1〉 and 〈2, 4, 1, 3〉.
",1 Introduction,[0],[0]
We think PETs are suitable for learning preordering for two reasons.,1 Introduction,[0],[0]
"Firstly, PETs specify exactly the phrase pairs defined by the permutation.",1 Introduction,[0],[0]
"Secondly, every permutation is factorizable into prime permutations only (Albert and Atkinson, 2005).",1 Introduction,[0],[0]
"Therefore, PETs expose maximal sharing between different permutations in terms of both phrases and their reordering.",1 Introduction,[0],[0]
"We expect this to be advantageous for learning hierarchical reordering.
",1 Introduction,[0],[0]
"For learning preordering, we first extract an initial PCFG from the latent treebank of PETs over the source sentences only.",1 Introduction,[0],[0]
We initialize the nonterminal set of this PCFG to the prime permutations decorating the PET nodes.,1 Introduction,[0],[0]
"Subsequently we split these coarse labels in the same way as latent variable splitting is learned for treebank parsing (Matsuzaki et al., 2005; Prescher, 2005; Petrov et al., 2006; Saluja et al., 2014).",1 Introduction,[0],[0]
"Unlike treebank parsing, however, our training treebank is latent because it consists of a whole forest of PETs per training instance (s).
",1 Introduction,[0],[0]
"Learning the splits on a latent treebank of PETs results in a Reordering PCFG which we use to parse input source sentences into split-decorated trees, i.e., the labels are the splits of prime permutations.",1 Introduction,[0],[0]
"After parsing s, we map the splits back on their initial prime permutations, and then retrieve a reordered version ś of s.",1 Introduction,[0],[0]
"In this sense, our latent splits are dedicated to reordering.
",1 Introduction,[0],[0]
We face two technical difficulties alien to work on latent PCFGs in treebank parsing.,1 Introduction,[0],[0]
"Firstly, as mentioned above, permutations may factorize into more than one PET (a forest) leading to a latent training treebank.1",1 Introduction,[0],[0]
"And secondly, after we parse a source string s, we are interested in ś, the permuted version of s, not in the best derivation/PET.",1 Introduction,[0],[0]
"Exact computation is a known NP-Complete problem (Sima’an, 2002).",1 Introduction,[0],[0]
"We solve this by a new Minimum-Bayes Risk decoding approach using Kendall reordering score as loss function, which is an efficient measure over permutations (Birch and Osborne, 2011; Isozaki et al., 2010a).
",1 Introduction,[0],[0]
"In summary, this paper contributes: • A novel latent hierarchical source reordering
model working over all derivations of PETs
1All PETs for the same permutation share the same set of prime permutations but differ only in bracketing structure (Zhang and Gildea, 2007).
•",1 Introduction,[0],[0]
"A label splitting approach based on PCFGs over minimal phrases as terminals, learned from an ambiguous treebank, where the label splits start out from prime permutations.",1 Introduction,[0],[0]
•,1 Introduction,[0],[0]
"A fast Minimum Bayes Risk decoding over
Kendall τ reordering score for selecting ś. We report results for extensive experiments on English-Japanese showing that our Reordering PCFG gives substantial improvements when used as preordering for phrase-based models, outperforming two existing baselines for this task.",1 Introduction,[0],[0]
"We aim at learning a PCFG which we will use for parsing source sentences s into synchronous trees, from which we can obtain a reordered source version ś. Since PCFGs are non-synchronous grammars, we will use the nonterminal labels to encode reordering transductions, i.e., this PCFG is implicitly an SCFG.",2 PETs and the Hidden Treebank,[0],[0]
"We can do this because s and ś are over the same alphabet.
",2 PETs and the Hidden Treebank,[0],[0]
"Here, we have access only to a word-aligned parallel corpus, not a treebank.",2 PETs and the Hidden Treebank,[0],[0]
"The following steps summarize our approach for acquiring a latent treebank and how it is used for learning a Reordering PCFG:
1.",2 PETs and the Hidden Treebank,[0],[0]
Obtain a permutation over minimal phrases from every word-alignment.,2 PETs and the Hidden Treebank,[0],[0]
2.,2 PETs and the Hidden Treebank,[0],[0]
Obtain a latent treebank of PETs by factorizing the permutations.,2 PETs and the Hidden Treebank,[0],[0]
3. Extract a PCFG from the PETs with initial nonterminals taken from the PETs.,2 PETs and the Hidden Treebank,[0],[0]
4.,2 PETs and the Hidden Treebank,[0],[0]
"Learn to split the initial nonterminals and estimate rule probabilities.
",2 PETs and the Hidden Treebank,[0],[0]
"These steps are detailed in the next section, but we will start out with an intuitive exposition of PETs, the latent treebank and the Reordering Grammar.
",2 PETs and the Hidden Treebank,[0],[0]
"Figure 1 shows examples of how PETs look like – see (Zhang and Gildea, 2007) for algorithmic details.",2 PETs and the Hidden Treebank,[0],[0]
Here we label the nodes with nonterminals which stand for prime permutations from the operators on the PETs.,2 PETs and the Hidden Treebank,[0],[0]
"For example, nonterminals P12, P21 and P3142 correspond respectively to reordering transducers 〈1, 2〉, 〈2, 1〉 and 〈3, 1, 4, 2〉.",2 PETs and the Hidden Treebank,[0],[0]
"A prime permutation on a source node µ is a transduction dictating how the children of µ are reordered at the target side, e.g., P21 inverts the child order.",2 PETs and the Hidden Treebank,[0],[0]
"We must stress that any similarity with ITG (Wu, 1997) is restricted to the fact that the straight and inverted operators of ITG are the binary case of prime permutations
in PETs (P12 and P21).",2 PETs and the Hidden Treebank,[0],[0]
"ITGs recognize only the binarizable permutations, which is a major restriction when used on the data: there are many nonbinarizable permutations in actual data (Wellington et al., 2006).",2 PETs and the Hidden Treebank,[0],[0]
"In contrast, our PETs are obtained by factorizing permutations obtained from the data, i.e., they exactly fit the range of prime permutations in the parallel corpus.",2 PETs and the Hidden Treebank,[0],[0]
"In practice we limit them to maximum arity 5.
",2 PETs and the Hidden Treebank,[0],[0]
"We can extract PCFG rules from the PETs, e.g., P21 → P12 P2413.",2 PETs and the Hidden Treebank,[0],[0]
"However, these rules are decorated with too coarse labels.",2 PETs and the Hidden Treebank,[0],[0]
"A similar problem was encountered in non-lexicalized monolingual parsing, and one solution was to lexicalize the productions (Collins, 2003) using head words.",2 PETs and the Hidden Treebank,[0],[0]
"But linguistic heads do not make sense for PETs, so we opt for the alternative approach (Matsuzaki et al., 2005), which splits the nonterminals and softly percolates the splits through the trees gradually fitting them to the training data.",2 PETs and the Hidden Treebank,[0],[0]
"Splitting has a shadow side, however, because it leads to combinatorial explosion in grammar size.
",2 PETs and the Hidden Treebank,[0],[0]
Suppose for example node P21 could split into P211 and P212 and similarly P2413 splits into P24131 and 24132.,2 PETs and the Hidden Treebank,[0],[0]
"This means that rule P21 → P12 P2413 will form eight new rules:
P211 → P121 P24131",2 PETs and the Hidden Treebank,[0],[0]
P211 → P121 P24132 P211,2 PETs and the Hidden Treebank,[0],[0]
→ P122 P24131,2 PETs and the Hidden Treebank,[0],[0]
P211 → P122 P24132 P212 → P121 P24131,2 PETs and the Hidden Treebank,[0],[0]
P212 → P121 P24132 P212 → P122 P24131,2 PETs and the Hidden Treebank,[0],[0]
"P212 → P122 P24132
Should we want to split each nonterminal into 30 subcategories, then an n-ary rule will split into 30n+1 new rules, which is prohibitively large.",2 PETs and the Hidden Treebank,[0],[0]
Here we use the “unary trick” as in Figure 2.,2 PETs and the Hidden Treebank,[0],[0]
The superscript on the nonterminals denotes the child position from left to right.,2 PETs and the Hidden Treebank,[0],[0]
"For example P2121 means that this node is a second child, and the
mother nonterminal label is P211.",2 PETs and the Hidden Treebank,[0],[0]
"For the running example rule, this gives the following rules:
P211 → P2111 P2121 P212",2 PETs and the Hidden Treebank,[0],[0]
→ P2112 P2122,2 PETs and the Hidden Treebank,[0],[0]
P2111,2 PETs and the Hidden Treebank,[0],[0]
→ P121 P2121 → P24131,2 PETs and the Hidden Treebank,[0],[0]
P2111,2 PETs and the Hidden Treebank,[0],[0]
→ P122 P2121 → P24132 P2112 → P121 P2122 → P24131,2 PETs and the Hidden Treebank,[0],[0]
P2112 → P122 P2122,2 PETs and the Hidden Treebank,[0],[0]
"→ P24132
",2 PETs and the Hidden Treebank,[0],[0]
"The unary trick leads to substantial reduction in grammar size, e.g., for arity 5 rules and 30 splits we could have had 306 = 729000000 split-rules, but with the unary trick we only have 30+302∗5 = 4530 split rules.",2 PETs and the Hidden Treebank,[0],[0]
"The unary trick was used in early lexicalized parsing work (Carroll and Rooth, 1998).2 This split PCFG constitutes a latent PCFG because the splits cannot be read of a treebank.",2 PETs and the Hidden Treebank,[0],[0]
"It must be learned from the latent treebank of PETs, as described next.",2 PETs and the Hidden Treebank,[0],[0]
"Obtaining permutations Given a source sentence s and its alignment a to a target sentence
2After applying the unary trick, we add a constraint on splitting: all nonterminals on an n-ary branching rule must be split simultaneously.
",3 Details of Latent Reordering PCFG,[0],[0]
"t in the training corpus, we segment 〈s,a, t〉 into a sequence of minimal phrases sm (maximal sequence) such that the reordering between these minimal phrases constitutes a permutation πm.",3 Details of Latent Reordering PCFG,[0],[0]
"We do not extract non-contiguous or non-minimal phrases because reordering them often involves complicated transductions which could hamper the performance of our learning algorithm.3
Unaligned words Next we describe the use of the factorization of permutations into PET forests for training a PCFG model.",3 Details of Latent Reordering PCFG,[0],[0]
But first we need to extend the PETs to allow for unaligned words.,3 Details of Latent Reordering PCFG,[0],[0]
"An unaligned word is joined with a neighboring phrase to the left or the right, depending on the source language properties (e.g., whether the language is head-initial or -final (Chomsky, 1970)).",3 Details of Latent Reordering PCFG,[0],[0]
"Our experiments use English as source language (head-initial), so the unaligned words are joined to phrases to their right.",3 Details of Latent Reordering PCFG,[0],[0]
This modifies a PET by adding a new binary branching node µ (dominating the unaligned word and the phrase it is joined to) which is labeled with a dedicated nonterminal: P01 if the unaligned word joins to the right and P10 if it joins to the left.,3 Details of Latent Reordering PCFG,[0],[0]
"We decompose the permutation πm into a forest of permutation trees PEF (πm) in O(n3), following algorithms in (Zhang et al., 2008; Zhang and Gildea, 2007) with trivial modifications.",3.1 Probability model,[0],[0]
Each PET ∆ ∈ PEF (πm) is a different bracketing (differing in binary branching structure only).,3.1 Probability model,[0],[0]
"We consider the bracketing hidden in the latent treebank, and apply unsupervised learning to induce a distribution over possible bracketings.",3.1 Probability model,[0],[0]
Our probability model starts from the joint probability of a sequence of minimal phrases sm and a permutation πm over it.,3.1 Probability model,[0],[0]
"This demands summing over all PETs ∆ in the forest PEF (πm), and for every PET also over all its label splits, which are given by the grammar derivations",3.1 Probability model,[0],[0]
"d:
P (sm, πm) = ∑
∆∈PEF (πm) ∑ d∈∆ P (d, sm) (1)
",3.1 Probability model,[0],[0]
"The probability of a derivation d is a product of probabilities of all the rules r that build it:
P (sm, πm) = ∑
∆∈PEF (πm) ∑ d∈∆ ∏ r∈d P (r) (2)
3Which differs from (Quirk and Menezes, 2006).
",3.1 Probability model,[0],[0]
"As usual, the parameters of this model are the PCFG rule probabilities which are estimated from the latent treebank using EM as explained next.",3.1 Probability model,[0],[0]
"For training the latent PCFG over the latent treebank, we resort to EM (Dempster et al., 1977) which estimates PCFG rule probabilities to maximize the likelihood of the parallel corpus instances.",3.2 Learning Splits on Latent Treebank,[0],[0]
"Computing expectations for EM is done efficiently using Inside-Outside (Lari and Young, 1990).",3.2 Learning Splits on Latent Treebank,[0],[0]
"As in other state splitting models (Matsuzaki et al., 2005), after splitting the nonterminals, we distribute the probability uniformly over the new rules, and we add to each new rule some random noise to break the symmetry.",3.2 Learning Splits on Latent Treebank,[0],[0]
"We split the non-terminals only once as in (Matsuzaki et al., 2005) (unlike (Petrov et al., 2006)).",3.2 Learning Splits on Latent Treebank,[0],[0]
For estimating the distribution for unknown words we replace all words that appear ≤ 3 times with the “UNKNOWN” token.,3.2 Learning Splits on Latent Treebank,[0],[0]
"We use CKY+ (Chappelier and Rajman, 1998) to parse a source sentence s into a forest using the learned split PCFG.",3.3 Inference,[0],[0]
"Unfortunately, computing the most-likely permutation (or alternatively ś) as in
argmax π∈Π ∑ ∆∈PEF (π) ∑ d∈∆ P (d, πm)
from a lattice of permutations Π using a PCFG is NP-complete (Sima’an, 2002).",3.3 Inference,[0],[0]
"Existing techniques, like variational decoding or MinimumBayes Risk (MBR), used for minimizing loss over trees as in (Petrov and Klein, 2007), are not directly applicable here.",3.3 Inference,[0],[0]
"Hence, we opt for minimizing the risk of making an error under a loss function over permutations using the MBR decision rule (Kumar and Byrne, 2004):
π̂ = argmin π ∑ πr Loss(π, πr)P (πr) (3)
",3.3 Inference,[0],[0]
The loss function we minimize is Kendall τ,3.3 Inference,[0],[0]
"(Birch and Osborne, 2011; Isozaki et al., 2010a) which is a ratio of wrongly ordered pairs of words (including gapped pairs) to the total number of pairs.",3.3 Inference,[0],[0]
We do Monte Carlo sampling of 10000 derivations from the chart of the s and then find the least risky permutation in terms of this loss.,3.3 Inference,[0],[0]
"We sample from the true distribution by sampling edges recursively
using their inside probabilities.",3.3 Inference,[0],[0]
"An empirical distribution over permutations P (π) is given by the relative frequency of π in the sample.
",3.3 Inference,[0],[0]
With large samples it is hard to efficiently compute expected Kendall τ loss for each sampled hypothesis.,3.3 Inference,[0],[0]
For sentence of length k and sample of size n the complexity of a naive algorithm is O(n2k2).,3.3 Inference,[0],[0]
Computing Kendall τ alone takes O(k2).,3.3 Inference,[0],[0]
"We use the fact that Kendall τ decomposes as a linear function over all skip-bigrams b that could be built for any permutation of length k:
Kendall(π, πr) = ∑ b 1− δ(π, b) k(k−1) 2 δ(πr, b) (4)
",3.3 Inference,[0],[0]
"Here δ returns 1 if permutation π contains the skip bigram b, otherwise it returns 0.",3.3 Inference,[0],[0]
"With this decomposition we can use the method from (DeNero et al., 2009) to efficiently compute the MBR hypothesis.",3.3 Inference,[0],[0]
"Combining Equations 3 and 4 we get:
π̂ = argmin π ∑ πr ∑ b 1− δ(π, b) k(k−1) 2 δ(πr, b)P (πr) (5)
",3.3 Inference,[0],[0]
"We can move the summation inside and reformulate the expected Kendall τ loss as expectation over the skip-bigrams of the permutation.
",3.3 Inference,[0],[0]
= argmin π ∑ b,3.3 Inference,[0],[0]
"(1− δ(π, b))",3.3 Inference,[0],[0]
"[∑ πr δ(πr, b)P (πr) ] (6)
= argmin π ∑ b (1− δ(π, b))EP (πr)δ(πr, b) (7)
= argmax π ∑ b δ(π, b)EP (πr)δ(πr, b) (8)
This means we need to pass through the sampled list only twice: (1) to compute expectations over skip bigrams and (2) to compute expected loss of each sampled permutation.",3.3 Inference,[0],[0]
The time complexity is O(nk2) which is quite fast in practice.,3.3 Inference,[0],[0]
We conduct experiments with three baselines:,4 Experiments,[0],[0]
• Baseline A: No preordering.,4 Experiments,[0],[0]
"• Baseline B: Rule based preordering (Isozaki
et al., 2010b), which first obtains an HPSG parse tree using Enju parser 4 and after that swaps the children by moving the syntactic head to the final position to account for different head orientation in English and Japanese.
",4 Experiments,[0],[0]
"4http://www.nactem.ac.uk/enju/
• Baseline C: LADER (Neubig et al., 2012): latent variable preordering that is based on ITG and large-margin training with latent variables.",4 Experiments,[0],[0]
"We used LADER in standard settings without any linguistic features (POS tags or syntactic trees).
",4 Experiments,[0],[0]
And we test four variants of our model:,4 Experiments,[0],[0]
• RGleft - only canonical left branching PET •,4 Experiments,[0],[0]
"RGright - only canonical right branching PET • RGITG-forest - all PETs that are binary (ITG) • RGPET-forest - all PETs.
",4 Experiments,[0],[0]
We test these models on English-Japanese NTCIR-8 Patent Translation (PATMT) Task.,4 Experiments,[0],[0]
For tuning we use all NTCIR-7 dev sets and for testing the test set from NTCIR-9 from both directions.,4 Experiments,[0],[0]
All used data was tokenized (English with Moses tokenizer and Japanese with KyTea 5) and filtered for sentences between 4 and 50 words.,4 Experiments,[0],[0]
"A subset of this data is used for training the Reordering Grammar, obtained by filtering out sentences that have prime permutations of arity > 5, and for the ITG version arity > 2.",4 Experiments,[0],[0]
Baseline C was trained on 600 sentences because training is prohibitively slow.,4 Experiments,[0],[0]
"Table 1 shows the sizes of data used.
",4 Experiments,[0],[0]
The Reordering Grammar was trained for 10 iterations of EM on train RG data.,4 Experiments,[0],[0]
We use 30 splits for binary non-terminals and 3 for non-binary.,4 Experiments,[0],[0]
Training on this dataset takes 2 days and parsing tuning and testing set without any pruning takes 11 and 18 hours respectively.,4 Experiments,[0],[0]
We test how well our model predicts gold reorderings before translation by training the alignment model using MGIZA++ 6 on the training corpus and using it to align the test corpus.,4.1 Intrinsic evaluation,[0],[0]
"Gold reorderings for the test corpus are obtained by sorting words by their average target position and (unaligned words follow their right neighboring
5http://www.phontron.com/kytea/ 6http://www.kyloo.net/software/doku.php/mgiza:overview
word).",4.1 Intrinsic evaluation,[0],[0]
"We use Kendall τ score for evaluation (note the difference with Section 3.3 where we defined it as a loss function).
",4.1 Intrinsic evaluation,[0],[0]
Table 2 shows that our models outperform all baselines on this task.,4.1 Intrinsic evaluation,[0],[0]
"The only strange result here is that rule-based preordering obtains a lower score than no preordering, which might be an artifact of the Enju parser changing the tokenization of its input, so the Kendall τ of this system might not really reflect the real quality of the preordering.",4.1 Intrinsic evaluation,[0],[0]
All other systems use the same tokenization.,4.1 Intrinsic evaluation,[0],[0]
"The reordered output of all the mentioned baselines and versions of our model are translated with phrase-based MT system (Koehn et al., 2007) (distortion limit set to 6 with distance based reordering model) that is trained on gold preordering of the training data 7 ś − t.",4.2 Extrinsic evaluation in MT,[0],[0]
"The only exception is Baseline A which is trained on original s− t.
We use a 5-gram language model trained with KenLM 8, tune 3 times with kb-mira (Cherry and Foster, 2012) to account for tuner instability and evaluated using Multeval 9 for statistical significance on 3 metrics: BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and TER (Snover et al., 2006).",4.2 Extrinsic evaluation in MT,[0],[0]
"We additionally report RIBES score (Isozaki et al., 2010a) that concentrates on word order more than other metrics.
",4.2 Extrinsic evaluation in MT,[0],[0]
Single or all PETs?,4.2 Extrinsic evaluation in MT,[0],[0]
In Table 3 we see that using all PETs during training makes a big impact on performance.,4.2 Extrinsic evaluation in MT,[0],[0]
"Only the all PETs variants
7Earlier work on preordering applies the preordering model to the training data to obtain a parallel corpus of guessed ś − t pairs, which are the word re-aligned and then used for training the back-end MT system (Khalilov and Sima’an, 2011).",4.2 Extrinsic evaluation in MT,[0],[0]
"We skip this, we take the risk of mismatch between the preordering and the back-end system, but this simplifies training and saves a good amount of training time.
8http://kheafield.com/code/kenlm/ 9https://github.com/jhclark/multeval
(RGITG-forest and RGPET-forest) significantly outperform all baselines.",4.2 Extrinsic evaluation in MT,[0],[0]
"If we are to choose a single PET per training instance, then learning RG from only left-branching PETs (the one usually chosen in other work, e.g. (Saluja et al., 2014)) performs slightly worse than the right-branching PET.",4.2 Extrinsic evaluation in MT,[0],[0]
This is possibly because English is mostly rightbranching.,4.2 Extrinsic evaluation in MT,[0],[0]
"So even though both PETs describe the same reordering, RGright captures reordering over English input better than RGleft.
",4.2 Extrinsic evaluation in MT,[0],[0]
All PETs or binary only?,4.2 Extrinsic evaluation in MT,[0],[0]
RGPET-forest performs significantly better than RGITG-forest (p < 0.05).,4.2 Extrinsic evaluation in MT,[0],[0]
"Non-ITG reordering operators are predicted rarely (in only 99 sentences of the test set), but they make a difference, because these operators often appear high in the predicted PET.",4.2 Extrinsic evaluation in MT,[0],[0]
"Furthermore, having these operators during training might allow for better fit to the data.
",4.2 Extrinsic evaluation in MT,[0],[0]
How much reordering is resolved by the Reordering Grammar?,4.2 Extrinsic evaluation in MT,[0],[0]
"Obviously, completely factorizing out the reordering from the translation process is impossible because reordering depends to a certain degree on target lexical choice.",4.2 Extrinsic evaluation in MT,[0],[0]
"To quantify the contribution of Reordering Grammar, we tested decoding with different distortion limit values in the SMT system.",4.2 Extrinsic evaluation in MT,[0],[0]
"We compare the phrase-based (PB) system with distance based cost function for reordering (Koehn et al., 2007) with and without preordering.
",4.2 Extrinsic evaluation in MT,[0],[0]
Figure 3 shows that Reordering Grammar gives substantial performance improvements at all distortion limits (both BLEU and RIBES).,4.2 Extrinsic evaluation in MT,[0],[0]
RGPET-forest is less sensitive to changes in decoder distortion limit than standard PBSMT.,4.2 Extrinsic evaluation in MT,[0],[0]
"The perfor-
mance of RGPET-forest varies only by 1.1 BLEU points while standard PBSMT by 4.3 BLEU points.",4.2 Extrinsic evaluation in MT,[0],[0]
Some local reordering in the decoder seems to help RGPET-forest but large distortion limits seem to degrade the preordering choice.,4.2 Extrinsic evaluation in MT,[0],[0]
"This shows also that the improved performance of RGPET-forest is not only a result of efficiently exploring the full space of permutations, but also a result of improved scoring of permutations.
",4.2 Extrinsic evaluation in MT,[0],[0]
Does the improvement remain for a decoder with MSD reordering model?,4.2 Extrinsic evaluation in MT,[0],[0]
"We compare the RGPET-forest preordered model against a decoder that uses the strong MSD model (Tillmann, 2004; Koehn et al., 2007).",4.2 Extrinsic evaluation in MT,[0],[0]
Table 4 shows that using Reordering Grammar as front-end to MSD reordering (full Moses) improves performance by 2.8 BLEU points.,4.2 Extrinsic evaluation in MT,[0],[0]
"The improvement is confirmed by METEOR, TER and RIBES.",4.2 Extrinsic evaluation in MT,[0],[0]
"Our preordering model and MSD are complementary – the Reordering Grammar captures long distance reordering, while MSD possibly does better local reorderings, especially reorderings conditioned on the lexical part of translation units.
",4.2 Extrinsic evaluation in MT,[0],[0]
"Interestingly, the MSD model (BLEU 29.6) improves over distance-based reordering (BLEU 27.8) by (BLEU 1.8), whereas the difference between these systems as back-ends to Reordering Grammar (respectively BLEU 32.4 and 32.0) is
far smaller (0.4 BLEU).",4.2 Extrinsic evaluation in MT,[0],[0]
This suggests that a major share of reorderings can be handled well by preordering without conditioning on target lexical choice.,4.2 Extrinsic evaluation in MT,[0],[0]
"Furthermore, this shows that RGPET-forest preordering is not very sensitive to the decoder’s reordering model.
",4.2 Extrinsic evaluation in MT,[0],[0]
Comparison to a Hierarchical model (Hiero).,4.2 Extrinsic evaluation in MT,[0],[0]
"Hierarchical preordering is not intended for a hierarchical model as Hiero (Chiang, 2005).",4.2 Extrinsic evaluation in MT,[0],[0]
"Yet, here we compare our preordering system (PB MSD+RG) to Hiero for completeness, while we should keep in mind that Hiero’s reordering model has access to much richer training data.",4.2 Extrinsic evaluation in MT,[0],[0]
"We will discuss these differences shortly.
",4.2 Extrinsic evaluation in MT,[0],[0]
"Table 4 shows that the difference in BLEU is not statistically significant, but there is more difference in METEOR and TER. RIBES, which concentrates more on reordering, prefers Reordering Grammar over Hiero.",4.2 Extrinsic evaluation in MT,[0],[0]
It is somewhat surprising that a preordering model combined with a phrase-based model succeeds to rival Hiero’s performance on English-Japanese.,4.2 Extrinsic evaluation in MT,[0],[0]
"Especially when looking at the differences between the two:
1.",4.2 Extrinsic evaluation in MT,[0],[0]
"Reordering Grammar uses only minimal phrases, while Hiero uses composite (longer) phrases which encapsulate internal reorderings, but also non-contiguous phrases.",4.2 Extrinsic evaluation in MT,[0],[0]
2.,4.2 Extrinsic evaluation in MT,[0],[0]
"Hiero conditions its reordering on the lexical target side, whereas the Reordering Grammar does not (by definition).",4.2 Extrinsic evaluation in MT,[0],[0]
3.,4.2 Extrinsic evaluation in MT,[0],[0]
"Hiero uses a range of features, e.g., a language model, while Reordering Grammar is a mere generative PCFG.",4.2 Extrinsic evaluation in MT,[0],[0]
"The advantages of Hiero can be brought to bear upon Reordering Grammar by reformulating it as a discriminative model.
",4.2 Extrinsic evaluation in MT,[0],[0]
Which structure is learned?,4.2 Extrinsic evaluation in MT,[0],[0]
"Figure 4 shows an example PET output showing how our model learns: (1) that the article “the” has no equivalent in Japanese, (2) that verbs go after their object, (3) to use postpositions instead of prepositions, and (4) to correctly group certain syntactic units, e.g. NPs and VPs.",4.2 Extrinsic evaluation in MT,[0],[0]
"The majority of work on preordering is based on syntactic parse trees, e.g., (Lerner and Petrov, 2013; Khalilov and Sima’an, 2011; Xia and Mccord, 2004).",5 Related work,[0],[0]
Here we concentrate on work that has common aspects with this work.,5 Related work,[0],[0]
"Neubig et
al (2012) trains a latent non-probabilistic discriminative model for preordering as an ITG-like grammar limited to binarizable permutations.",5 Related work,[0],[0]
Tromble and Eisner (2009) use ITG but do not train the grammar.,5 Related work,[0],[0]
They only use it to constrain the local search.,5 Related work,[0],[0]
DeNero and Uszkoreit (2011) present two separate consecutive steps for unsupervised induction of hierarchical structure (ITG) and the induction of a reordering function over it.,5 Related work,[0],[0]
"In contrast, here we learn both the structure and the reordering function simultaneously.",5 Related work,[0],[0]
"Furthermore, at test time, our inference with MBR over a measure of permutation (Kendall) allows exploiting both structure and reordering weights for inference, whereas test-time inference in (DeNero and Uszkoreit, 2011) is also a two step process – the parser forwards to the next stage the best parse.
",5 Related work,[0],[0]
Dyer and Resnik (2010) treat reordering as a latent variable and try to sum over all derivations that lead not only to the same reordering but also to the same translation.,5 Related work,[0],[0]
"In their work they consider all permutations allowed by a given syntactic tree.
",5 Related work,[0],[0]
"Saers et al (2012) induce synchronous grammar for translation by splitting the non-terminals, but unlike our approach they split generic nonterminals and not operators.",5 Related work,[0],[0]
Their most expressive grammar covers only binarizable permutations.,5 Related work,[0],[0]
The decoder that uses this model does not try to sum over many derivations that have the same yield.,5 Related work,[0],[0]
They do not make independence assumption like our “unary trick” which is probably the reason they do not split more than 8 times.,5 Related work,[0],[0]
"They do not compare their results to any other SMT system and test on a very small dataset.
",5 Related work,[0],[0]
"Saluja et al (2014) attempts inducing a refined Hiero grammar (latent synchronous CFG) from Normalized Decomposition Trees (NDT) (Zhang et al., 2008).",5 Related work,[0],[0]
"While there are similarities with
the present work, there are major differences.",5 Related work,[0],[0]
"On the similarity side, NDTs are decomposing alignments in ways similar to PETs, and both Saluja’s and our models refine the labels on the nodes of these decompositions.",5 Related work,[0],[0]
"However, there are major differences between the two:
• Our model is completely monolingual and unlexicalized (does not condition its reordering on the translation) in contrast with the Latent SCFG used in (Saluja et al., 2014), • Our Latent PCFG label splits are defined
as refinements of prime permutations, i.e., specifically designed for learning reordering, whereas (Saluja et al., 2014) aims at learning label splitting that helps predicting NDTs from source sentences, • Our model exploits all PETs and all deriva-
tions, both during training (latent treebank) and during inferences.",5 Related work,[0],[0]
"In (Saluja et al., 2014) only left branching NDT derivations are used for learning the model.",5 Related work,[0],[0]
•,5 Related work,[0],[0]
"The training data used by (Saluja et al., 2014)
is about 60 times smaller in number of words than the data used here; the test set of (Saluja et al., 2014) also consists of far shorter sentences where reordering could be less crucial.
",5 Related work,[0],[0]
"A related work with a similar intuition is presented in (Maillette de Buy Wenniger and Sima’an, 2014), where nodes of a tree structure similar to PETs are labeled with reordering patterns obtained by factorizing word alignments into Hierarchical Alignment Trees.",5 Related work,[0],[0]
These patterns are used for labeling the standard Hiero grammar.,5 Related work,[0],[0]
"Unlike this work, the labels extracted by (Maillette de Buy Wenniger and Sima’an, 2014) are clustered manually into less than a dozen labels without the possibility of fitting the labels to the training data.",5 Related work,[0],[0]
We present a generative Reordering PCFG model learned from latent treebanks over PETs obtained by factorizing permutations over minimal phrase pairs.,6 Conclusion,[0],[0]
Our Reordering PCFG handles non-ITG reordering patterns (up to 5-ary branching) and it works with all PETs that factorize a permutation (rather than a single PET).,6 Conclusion,[0],[0]
To the best of our knowledge this is the first time both extensions are shown to improve performance.,6 Conclusion,[0],[0]
"The empirical results on English-Japanese show that (1) when used for preordering, the Reordering PCFG helps particularly with relieving the phrase-based model from long range reorderings, (2) combined with a state-of-the-art phrase model, Reordering PCFG shows performance not too different from Hiero, supporting the common wisdom of factorizing long range reordering outside the decoder, (3) Reordering PCFG generates derivations that seem to coincide well with linguistically-motivated reordering patterns for English-Japanese.",6 Conclusion,[0],[0]
"There are various direction we would like to explore, the most obvious of which are integrating the learned reordering with other feature functions in a discriminative setting, and extending the model to deal with non-contiguous minimal phrases.",6 Conclusion,[0],[0]
This work is supported by STW grant nr. 12271 and NWO VICI grant nr. 277-89-002.,Acknowledgments,[0],[0]
We thank Wilker Aziz for comments on earlier version of the paper and discussions about MBR and sampling.,Acknowledgments,[0],[0]
"We present a novel approach for unsupervised induction of a Reordering Grammar using a modified form of permutation trees (Zhang and Gildea, 2007), which we apply to preordering in phrase-based machine translation.",abstractText,[0],[0]
"Unlike previous approaches, we induce in one step both the hierarchical structure and the transduction function over it from word-aligned parallel corpora.",abstractText,[0],[0]
"Furthermore, our model (1) handles non-ITG reordering patterns (up to 5-ary branching), (2) is learned from all derivations by treating not only labeling but also bracketing as latent variable, (3) is entirely unlexicalized at the level of reordering rules, and (4) requires no linguistic annotation.",abstractText,[0],[0]
"Our model is evaluated both for accuracy in predicting target order, and for its impact on translation quality.",abstractText,[0],[0]
"We report significant performance gains over phrase reordering, and over two known preordering baselines for English-Japanese.",abstractText,[0],[0]
Reordering Grammar Induction,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2401–2410 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"Deep neural models are known to be computationally expensive to train even with fast hardware (Sutskever et al., 2014; Wu et al., 2016).",1 Introduction,[0],[0]
"For example, it takes three weeks to train a deep neural machine translation system on 100 Graphics Processing Units (GPUs) (Wu et al., 2016).",1 Introduction,[0],[0]
"Furthermore, a large amount of data is usually required to train effective neural models (Goodfellow et al., 2016; Hirschberg and Manning, 2015).
",1 Introduction,[0],[0]
Bengio et al. (2009) and Kumar et al. (2010) developed training paradigms which are inspired by the learning principle that humans can learn more effectively when training starts with easier concepts and gradually proceeds with more difficult concepts.,1 Introduction,[0],[0]
"Since these approaches are motivated by
1Our code is available at scholar.harvard.edu/ hadi/RbF/
a “starting small” strategy they are called curriculum or self-paced learning.
",1 Introduction,[0],[0]
"In this paper, we present a novel training paradigm which is inspired by the broad evidence in psychology that shows human ability to retain information improves with repeated exposure and exponentially decays with delay since last exposure (Cepeda et al., 2006; Averell and Heathcote, 2011).",1 Introduction,[0],[0]
"Spaced repetition was presented in psychology (Dempster, 1989) and forms the building block of many educational devices, including flashcards, in which small pieces of information are repeatedly presented to a learner on a schedule determined by a spaced repetition algorithm.",1 Introduction,[0],[0]
"Such algorithms show that human learners can learn efficiently and effectively by increasing intervals of time between subsequent reviews of previously learned materials (Dempster, 1989; Novikoff et al., 2012).
",1 Introduction,[0],[0]
"We investigate the analogy between training neural models and findings in psychology about human memory model and develop a spaced repetition algorithm (named Repeat before Forgetting, RbF) to efficiently and effectively train neural models.",1 Introduction,[0],[0]
The core part of our algorithm is a scheduler that ensures a given neural network spends more time working on difficult training instances and less time on easier ones.,1 Introduction,[0],[0]
"Our scheduler is inspired by factors that affect human memory retention, namely, difficulty of learning materials, delay since their last review, and strength of memory.",1 Introduction,[0],[0]
The scheduler uses these factors to lengthen or shorten review intervals with respect to individual learners and training instances.,1 Introduction,[0],[0]
"We evaluate schedulers based on their scheduling accuracy, i.e., accuracy in estimating network memory retention with respect to previously-seen instances, as well as their effect on the efficiency and effectiveness of downstream neural networks.2
2 In this paper, we use the terms memory retention, recall, and learning interchangeably.
2401
The contributions of this paper are: (1) we show that memory retention in neural networks is affected by the same (known) factors that affect memory retention in humans, (2) we present a novel training paradigm for neural networks based on spaced repetition, and (3) our approach can be applied without modification to any neural network.
",1 Introduction,[0],[0]
"Our best RbF algorithm uses 34-50% of training data per epoch while producing similar results to state-of-the-art systems on three tasks, namely sentiment classification, image categorization, and arithmetic addition.3",1 Introduction,[0],[0]
"It also runs 2.9-4.8 times faster than standard training, and outperforms competing state-of-the-art baselines.",1 Introduction,[0],[0]
"Research in psychology describes the following memory model for human learning: the probability that a human recalls a previously-seen item (e.g., the Korean translation of a given English word) depends on the difficulty of the item, delay since last review of the item, and the strength of the human memory.",2 Neural and Brain Memory Models,[0],[0]
"The relation between these indicators and memory retention has the following functional form (Reddy et al., 2016; Ebbinghaus, 1913):
Pr(recall) = exp(−difficulty × delay strength ).",2 Neural and Brain Memory Models,[0],[0]
"(1)
An accurate memory model enables estimating the time by which an item might be forgotten by a learner so that a review can be scheduled for the learner before that time.
",2 Neural and Brain Memory Models,[0],[0]
We investigate the analogy between the above memory model and memory model of artificial neural networks.,2 Neural and Brain Memory Models,[0],[0]
"Our intuition is that if the probability that a network recalls an item (e.g., correctly predicts its category) depends on the same factors (difficulty of the item, delay since last review of the item, or strength of the network), then we can develop spaced repetition algorithms to efficiently and effectively train neural networks.",2 Neural and Brain Memory Models,[0],[0]
We design a set of preliminarily experiments to directly evaluate the effect of the aforementioned factors (recall indicators) on memory retention in neural networks.,2.1 Recall Indicators,[0],[0]
"For this purpose, we use a set of training instances that are partially made available to the network during training.",2.1 Recall Indicators,[0],[0]
"This scheme
3We obtained similar results on QA tasks (Weston et al., 2016) but they are excluded due to space limit.
will allow us to intrinsically examine the effect of recall indicators on memory retention in isolation from external effects such as size of training data, number of training epochs, etc.
",2.1 Recall Indicators,[0],[0]
"We first define the following concepts to ease understanding the experiments (see Figure 1):
• First and Last review points (fRev and lRev) of a training instance are the first and last epochs in which the instance is used to train the network respectively,
• Recall point (Rec) is the epoch in which network retention is computed against some training instances; network retention is the probability that a neural network recalls (i.e. correctly classifies) a previously-seen training instance, and
• Delay since last review of a training instance is the difference between the recall point and the last review point of the training instance.
",2.1 Recall Indicators,[0],[0]
"Given training data and a neural network, we uniformly at random divide the data into three disjoint sets: a base set A, a review set B, and a replacement set C that respectively contain 80%, 10%, and 10% of the data.",2.1 Recall Indicators,[0],[0]
"As depicted in Figure 1, instances of A are used for training at every epoch, while those in B and C are partially used for training.",2.1 Recall Indicators,[0],[0]
The network initially starts to train with {A ∪ C} instances.,2.1 Recall Indicators,[0],[0]
"Then, starting from the first review point, we inject the review set B and remove C, training with {A ∪ B} instances at every epoch until the last review point.",2.1 Recall Indicators,[0],[0]
The network will then continue training with {A ∪ C} instances until the recall point.,2.1 Recall Indicators,[0],[0]
"At this point, network retention is computed against set B instances, with delay defined as the number of epochs since last review point.",2.1 Recall Indicators,[0],[0]
"The intuition behind using review and replacement sets, B and C respectively, is to avoid external effects (e.g.
size of data or network generalization and learning capability) for our intrinsic evaluation purpose.
",2.1 Recall Indicators,[0],[0]
"To conduct these experiments, we identify different neural models designed for different tasks.4 For each network, we fix the recall point to either the epoch in which the network is fully trained (i.e., obtains its best performance based on standard or “rote” training in which all instances are used for training at every iteration), or partially trained (i.e., obtains half of its best performance based on rote training).",2.1 Recall Indicators,[0],[0]
We report average results across these networks for each experiment.,2.1 Recall Indicators,[0],[0]
"As aforementioned, delay since last review of a training instance is the difference between the recall point (Rec) and the last review point (lRev) of the training instance.",2.1.1 Delay since Last Review,[0],[0]
We evaluate the effect of delay on network retention (against set B instances) by keeping the recall point fixed while moving the sliding window in Figure 1.,2.1.1 Delay since Last Review,[0],[0]
Figures 2(a) and 2(b) show average network retention across networks for the fully and partially trained recall points respectively.,2.1.1 Delay since Last Review,[0],[0]
The results show an inverse relationship between network retention and delay since last review in neural networks.,2.1.1 Delay since Last Review,[0],[0]
We define difficulty of training instances by the loss values generated by a network for the instances.,2.1.2 Item Difficulty,[0],[0]
Figure 2(c) shows the difficulty of set B instances at the last review point against average network retention on these instances at recall point.,2.1.2 Item Difficulty,[0],[0]
"We normalize loss values to unit vectors (to make them com-
4See section 4, we use Addition and CIFAR10 datasets and their corresponding neural networks for these experiments.
",2.1.2 Item Difficulty,[0],[0]
parable across networks) and then average them across networks for both fully and partially trained recall points.,2.1.2 Item Difficulty,[0],[0]
"As the results show, network retention decreases as item difficulty increases.",2.1.2 Item Difficulty,[0],[0]
We define strength of a network by its performance on validation data.,2.1.3 Network Strength,[0],[0]
"To understand the effect of network strength on its retention, we use the same experimental setup as before except that we keep the delay (difference between recall point and last review point) fixed while gradually increasing the recall point; this will make the networks stronger by training them for more epochs.",2.1.3 Network Strength,[0],[0]
"Then, at every recall point, we record network retention on set B instances and network accuracy on validation data.",2.1.3 Network Strength,[0],[0]
Average results across networks for two sets of 10 consecutive recall points (before fully and partially trained recall points) are shown in Figure 2(d).,2.1.3 Network Strength,[0],[0]
"As the results show, network retention increases as memory strength increases.
",2.1.3 Network Strength,[0],[0]
"The above experiments show that memory retention in neural networks is affected by the same factors that affect memory retention in humans: (a) neural networks forget training examples after a certain period of intervening training data (b): the period of recall is shorter for more difficult examples, and (c): recall improves as networks achieve better overall performance.",2.1.3 Network Strength,[0],[0]
"We conclude that delay since last review, item difficulty (loss values of training instances), and memory strength (network performance on validation data) are key indicators that affect network retention and propose to design spaced repetition algorithms that take such indicators into account in training neural networks.",2.1.3 Network Strength,[0],[0]
"We present two spaced repetition-based algorithms: a modified version of the Leitner system developed in (Reddy et al., 2016) and our Repeat before Forgetting (RbF) model respectively.",3 Spaced Repetition,[0],[0]
"Suppose we have n queues {q0, q1, . . .",3.1 Leitner System,[0],[0]
", qn−1}.",3.1 Leitner System,[0],[0]
"The Leitner system initially places all training instances in the first queue, q0.",3.1 Leitner System,[0],[0]
"As Algorithm 1 shows, at each training iteration, the Leitner scheduler chooses some queues to train a downstream neural network.",3.1 Leitner System,[0],[0]
Only instances in the selected queues will be used for training the network.,3.1 Leitner System,[0],[0]
"During training, if an instance from qi is recalled (e.g. correctly classified) by the network, the instance will be “promoted” to qi+1, otherwise it will be “demoted” to the first queue, q0.5
The Leitner scheduler reviews instances of qi at every 2i iterations.",3.1 Leitner System,[0],[0]
"Therefore, instance in lower queues (difficult/forgotten instances) are reviewed more frequently than those in higher queues (easy/recalled ones).",3.1 Leitner System,[0],[0]
Figure 3 (bottom) provides examples of queues and their processing epochs.,3.1 Leitner System,[0],[0]
"Note that the overhead imposed on training by
5 Note that in (Reddy et al., 2016) demoted instances are moved to qi−1.",3.1 Leitner System,[0],[0]
"We observed significant improvement in Leitner system by moving such instances to q0 instead of qi−1.
the Leitner system is O(|current batch|) at every epoch for moving instances between queues.",3.1 Leitner System,[0],[0]
The challenge in developing memory models is to estimate the time by which a training instance should be reviewed before it is forgotten by the network.,3.2.1 RbF Memory Models,[0],[0]
Accurate estimation of the review time leads to efficient and effective training.,3.2.1 RbF Memory Models,[0],[0]
"However, a heuristic scheduler such as Leitner system is suboptimal as its hard review schedules (i.e. only 2iiteration delays) may lead to early or late reviews.
",3.2.1 RbF Memory Models,[0],[0]
We develop flexible schedulers that take recall indicators into account in the scheduling process.,3.2.1 RbF Memory Models,[0],[0]
Our schedulers lengthen or shorten inter-repetition intervals with respect to individual training instances.,3.2.1 RbF Memory Models,[0],[0]
"In particular, we propose using density kernel functions to estimate the latest epoch in which a given training instance can be recalled.",3.2.1 RbF Memory Models,[0],[0]
"We aim to investigate how much improvement (in terms of efficiency and effectiveness) can be achieved using more flexible schedulers that utilize the recall indicators.
",3.2.1 RbF Memory Models,[0],[0]
"We propose considering density kernels as schedulers that favor (i.e., more confidently delay) less difficult training instances in stronger networks.",3.2.1 RbF Memory Models,[0],[0]
"As a kernel we can use any non-increasing function of the following quantity:
xi = di × ti se , (2)
where di indicates the loss of network for a training instance hi ∈ H, ti indicates the number of epochs to next review of hi, and se indicates the performance of network— on validation data— at epoch e. We investigate the Gaussian, Laplace, Linear, Cosine, Quadratic, and Secant kernels as described below respectively:
fgau(x, τ) = exp(−τx2), (3) flap(x, τ) = exp(−τx), (4)
flin(x, τ) = { 1− τx x < 1τ 0",3.2.1 RbF Memory Models,[0],[0]
"otherwise , (5)
fcos(x, τ) =
{ 1 2 cos(τπx)",3.2.1 RbF Memory Models,[0],[0]
"+ 1 x < 1 τ
0 otherwise ,
(6)
fqua(x, τ) = { 1− τx2",3.2.1 RbF Memory Models,[0],[0]
x2 < 1τ 0,3.2.1 RbF Memory Models,[0],[0]
"otherwise , (7)
fsec(x, τ) = 2
exp(−τx2) + exp(τx2) , (8)
where τ is a learning parameter.",3.2.1 RbF Memory Models,[0],[0]
Figure 4 depicts these kernels with τ = 1.,3.2.1 RbF Memory Models,[0],[0]
"As we will discuss in the next section, we use these kernels to optimize delay with respect to item difficulty and network strength for each training instance.",3.2.1 RbF Memory Models,[0],[0]
"Our Repeat before Forgetting (RbF) model is a spaced repetition algorithm that takes into account the previously validated recall indicators to train neural networks, see Algorithm 2.",3.2.2 RbF Algorithm,[0],[0]
RbF divides training instances into current and delayed batches based on their delay values at each iteration.,3.2.2 RbF Algorithm,[0],[0]
Instances in the current batch are those that RbF is less confident about their recall and therefore are reviewed (used to re-train the network) at current iteration.,3.2.2 RbF Algorithm,[0],[0]
"On the other hand, instances in the delayed batch are those that are likely to be recalled by the network in the future and therefore are not reviewed at current epoch.",3.2.2 RbF Algorithm,[0],[0]
"At each iteration, the RbF scheduler estimates the optimum delay (number of epochs to next review) for each training instance in the current batch.",3.2.2 RbF Algorithm,[0],[0]
"RbF makes such item-specific estimations as follows:
Given the difficulty of a training instance di, the memory strength of the neural network at epoch e, se, and an RbF memory model f (see section 3.2.1), RbF scheduler estimates the maximum delay t̂i for the instance such that it can be recalled with a confidence greater than the given threshold η ∈",3.2.2 RbF Algorithm,[0],[0]
"(0, 1) at time e+ t̂i.",3.2.2 RbF Algorithm,[0],[0]
"As described before, di and se can be represented by the current loss of the network for the instance and the current performance of the network on validation data respectively.",3.2.2 RbF Algorithm,[0],[0]
"Therefore, the maximum delay between the current (epoch e) and next reviews of the instance can be estimated as follows:
t̂i = arg min ti
( f(xi, τ̂)− η )2 , (9)
",3.2.2 RbF Algorithm,[0],[0]
s.t 1 ≤,3.2.2 RbF Algorithm,[0],[0]
ti ≤ k,3.2.2 RbF Algorithm,[0],[0]
"− e
where τ̂ is the optimum value for the learning parameter obtained from validation data, see Equation (10).",3.2.2 RbF Algorithm,[0],[0]
"In principle, reviewing instances could be delayed for any number of epochs; in practice however, delay is bounded both below and above (e.g., by queues in the Leitner system).",3.2.2 RbF Algorithm,[0],[0]
"Thus, we assume that, at each epoch e, instances could be delayed for at least one iteration and at most k − e iterations where k is the total number of training epochs.",3.2.2 RbF Algorithm,[0],[0]
"We also note that ti is a lower bound of the maximum delay as se is expected to increase and di is expected to decrease as the network trains in next iterations.
",3.2.2 RbF Algorithm,[0],[0]
Algorithm 2 shows the outline of the proposed RbF model.,3.2.2 RbF Algorithm,[0],[0]
We estimate the optimum value of τ (line 5 of Algorithm 2) for RbF memory models using validation data.,3.2.2 RbF Algorithm,[0],[0]
"In particular, RbF uses the loss values of validation instances and strength of the network obtained at the previous epoch to estimate network retention for validation instances at the current epoch (therefore ti = 1 for every validation instance).",3.2.2 RbF Algorithm,[0],[0]
"The parameter τ for each memory model is computed as follows:
τ̂ = arg min τ
( f(xj , τ)− aj )2 ,∀hj ∈ V, aj ≥ η,
(10) where aj ∈ (0, 1) is the current accuracy of the model for the validation instance hj .",3.2.2 RbF Algorithm,[0],[0]
RbF then predicts the delay for current batch instances and reduces the delay for those in the delayed batch by one epoch.,3.2.2 RbF Algorithm,[0],[0]
The overhead of RbF is O(|H|) to compute delays and O(|V|) to compute τ̂ .,3.2.2 RbF Algorithm,[0],[0]
Note that (9) and (10) have closed form solutions.,3.2.2 RbF Algorithm,[0],[0]
"Table 1 describes the tasks, datasets, and models that we consider in our experiments.",4 Experiments,[0],[0]
It also reports the training epochs for which the models produce their best performance on validation data (based on rote training).,4 Experiments,[0],[0]
"We note that the Addition dataset is randomly generated and contains numbers with at most 4 digits.6
We consider three schedulers as baselines: a slightly modified version of the Leitner scheduler (Lit) developed in Reddy et al. (2016) for human learners (see Footnote 5), curriculum learning (CL) in which training instances are scheduled with respect to their easiness (Jiang et al., 2015), and the uniform scheduler of rote training (Rote) in which all instances are used for training at every epoch.",4 Experiments,[0],[0]
"For Lit, we experimented with different queue lengths, n = {3, 5, 7}, and set n = 5 in the experiments as this value led to the best performance of this scheduler across all datasets.
",4 Experiments,[0],[0]
Curriculum learning starts training with easy instances and gradually introduces more complex instances for training.,4 Experiments,[0],[0]
"Since easiness information is not readily available in most datasets, previous approaches have used heuristic techniques (Spitkovsky et al., 2010; Basu and Christensen, 2013) or optimization algorithms (Jiang et al., 2015, 2014) to quantify easiness of training instances.",4 Experiments,[0],[0]
These approaches consider an instance as easy if its loss is smaller than a threshold (λ).,4 Experiments,[0],[0]
"We adopt this technique as follows: at each iteration e, we divide the entire training data into easy and hard sets using iteration-specific λe and the loss values of instances, obtained from the current partially-trained network.",4 Experiments,[0],[0]
All easy instances in conjunction with αe ∈,4 Experiments,[0],[0]
"[0, 1] fraction of easiest hard instances (those with smallest loss values greater than λe) are used for training at",4 Experiments,[0],[0]
"iteration e. We set
6https://github.com/fchollet/keras/ blob/master/examples/addition_rnn.py
each λe to the average loss of training instances that are correctly classified by the current partiallytrained network.",4 Experiments,[0],[0]
"Furthermore, at each iteration e, we set αe = e/k to gradually introduce complex instances at every new iteration.7 Note that we treat all instances as easy at e = 0.
",4 Experiments,[0],[0]
Performance values reported in experiments are averaged over 10 runs of systems and the confidence parameter η is always set to 0.5 unless otherwise stated.,4 Experiments,[0],[0]
"In these experiments, we evaluate memory schedulers with respect to their accuracy in predicting network retention for delayed instances.",4.1 Evaluation of Memory Models,[0],[0]
"Since curriculum learning does not estimate delay for training instances, we only consider Leitner and RbF schedulers in these experiments.
",4.1 Evaluation of Memory Models,[0],[0]
"For this evaluation, if a scheduler predicts a delay t for a training instance h at epoch e, we evaluate network retention with respect to h at epoch e+ t. If the network recalls (correctly classifies) the instance at epoch e+ t, the scheduler has correctly predicted network retention for h, and otherwise, it has made a wrong prediction.",4.1 Evaluation of Memory Models,[0],[0]
We use this binary outcome to evaluate the accuracy of each scheduler.,4.1 Evaluation of Memory Models,[0],[0]
Note that the performance of schedulers on instances that have not been delayed is not a major concern.,4.1 Evaluation of Memory Models,[0],[0]
"Although failing to delay an item inversely affects efficiency, it makes the network stronger by providing more instances to train from.",4.1 Evaluation of Memory Models,[0],[0]
"Therefore, we consider a good scheduler as the one that accurately delays more items.
",4.1 Evaluation of Memory Models,[0],[0]
Figure 6 depicts the average accuracy of schedulers in predicting networks’ retention versus the average fraction of training instances that they delayed per epoch.,4.1 Evaluation of Memory Models,[0],[0]
"As the results show, all schedulers
7k is the total number of iterations.
",4.1 Evaluation of Memory Models,[0],[0]
delay substantial amount of instances per epoch.,4.1 Evaluation of Memory Models,[0],[0]
"In particular, Cos and Qua outperform Lit in both predicting network retention and delaying items, delaying around 50% of training instances per epoch.",4.1 Evaluation of Memory Models,[0],[0]
This is while Gau and Sec show comparable accuracy to Lit but delay more instances.,4.1 Evaluation of Memory Models,[0],[0]
"On the other hand, Lap, which has been found effective in Psychology, and Lin are less accurate in predicting network retention.",4.1 Evaluation of Memory Models,[0],[0]
This is because of the tradeoff between delaying more instances and creating stronger networks.,4.1 Evaluation of Memory Models,[0],[0]
"Since these schedulers are more flexible in delaying greater amount of instances, they might not provide networks with enough data to fully train.
",4.1 Evaluation of Memory Models,[0],[0]
"Figure 7 shows the performance of RbF schedulers with respect to the recall confidence parameter η, see Equation (9).",4.1 Evaluation of Memory Models,[0],[0]
"As the results show, schedulers have poor performance with smaller values of η.",4.1 Evaluation of Memory Models,[0],[0]
This is because smaller values of η make schedulers very flexible in delaying instances.,4.1 Evaluation of Memory Models,[0],[0]
"However, the performance of schedulers are not dramatically low even with very small ηs.",4.1 Evaluation of Memory Models,[0],[0]
"Our further analyses on the delay patterns show that although a smaller η leads to more delayed instances, the delays are significantly shorter.",4.1 Evaluation of Memory Models,[0],[0]
"Therefore, most delayed instances will be “reviewed” shortly in next epochs.",4.1 Evaluation of Memory Models,[0],[0]
"These bulk reviews make the network stronger and help it to recall most delayed instance in future iterations.
",4.1 Evaluation of Memory Models,[0],[0]
"On the other hand, greater ηs lead to more accurate schedulers at the cost of using more training data.",4.1 Evaluation of Memory Models,[0],[0]
"In fact, we found that larger ηs do not delay most training instances in the first few iterations.",4.1 Evaluation of Memory Models,[0],[0]
"However, once the network obtains a reasonably high performance, schedulers start delaying instances for longer durations.",4.1 Evaluation of Memory Models,[0],[0]
We will further study this effect in the next section.,4.1 Evaluation of Memory Models,[0],[0]
We compare RbF against Leitner and curriculum learning in terms of efficiency of training and effectiveness of trained models.,4.2 Efficiency and Effectiveness,[0],[0]
"We define effectiveness as the accuracy of a trained network on balanced test data, and efficiency as (a): fraction of instances used for training per epoch, and (b): required time for training the networks.",4.2 Efficiency and Effectiveness,[0],[0]
"For RbF schedulers, we set η to 0.5 and consider the best performing kernel Cosine with η = 0.9 based on results in Figure 7.
",4.2 Efficiency and Effectiveness,[0],[0]
The results in Table 2 show that all training paradigms have comparable effectiveness (Accuracy) to that of rote training (Rote).,4.2 Efficiency and Effectiveness,[0],[0]
Our RbF schedulers use less data per epoch (34-50% of data) and run considerably faster than Rote (2.90-4.78 times faster for η = 0.5).,4.2 Efficiency and Effectiveness,[0],[0]
"The results also show that Lit is slightly less accurate but runs 2.87 time faster than Rote; note that, as a scheduler, Lit is less accurate than RbF models, see Figures 6 and 7.
",4.2 Efficiency and Effectiveness,[0],[0]
"In addition, CL leads to comparable performance to RbF but is considerably slower than other schedulers.",4.2 Efficiency and Effectiveness,[0],[0]
This is because this scheduler has to identify easier instances and sort the harder ones to sample training data at each iteration.,4.2 Efficiency and Effectiveness,[0],[0]
"Overall, the performance of Lit, CL, Cos η = .5 and Cos η = .9 are only 2.76, 1.90, 1.88, and 0.67 absolute values lower than that of Rote respectively.",4.2 Efficiency and Effectiveness,[0],[0]
"Considering the achieved efficiency, these differences are negligible (see the overall gain in Table 2).
",4.2 Efficiency and Effectiveness,[0],[0]
Figure 8 reports detailed efficiency and effectiveness results across datasets and networks.,4.2 Efficiency and Effectiveness,[0],[0]
"For clear illustration, we report accuracy at iterations 2i ∀i in which Lit is trained on the entire data, and consider Cos η = .5",4.2 Efficiency and Effectiveness,[0],[0]
as RbF scheduler.,4.2 Efficiency and Effectiveness,[0],[0]
"In terms of efficiency (first row of Figure 8), CL starts with (small set of)
easier instances and gradually increases the amount of training data by adding slightly harder instances into its training set.",4.2 Efficiency and Effectiveness,[0],[0]
"On the other hand, Lit and RbF start big and gradually delay reviewing (easy) instances that the networks have learned.",4.2 Efficiency and Effectiveness,[0],[0]
"The difference between these two training paradigms is apparent in Figures 8(a)-8(c).
",4.2 Efficiency and Effectiveness,[0],[0]
The results also show that the efficiency of a training paradigm depends on the initial effectiveness of the downstream neural network.,4.2 Efficiency and Effectiveness,[0],[0]
"For CL to be efficient, the neural network need to initially have low performance (accuracy) so that the scheduler works on smaller set of easy instances.",4.2 Efficiency and Effectiveness,[0],[0]
"For example, in case of Addition, Figures 8(b) and 8(e), the initial network accuracy is only 35%, therefore most instances are expected to be initially treated as hard instances and don’t be used for training.",4.2 Efficiency and Effectiveness,[0],[0]
"On the other hand, CL shows a considerably lower efficiency for networks with slightly high initial accuracy, e.g. in case of IMDb or CIFAR10 where the initial network accuracy is above 56%, see Figures 8(a) and 8(d), and 8(c) and 8(f) respectively.
",4.2 Efficiency and Effectiveness,[0],[0]
"In contrast to CL, Lit and RbF are more efficient when the network has a relatively higher initial performance.",4.2 Efficiency and Effectiveness,[0],[0]
"A higher initial performance helps the
schedulers to more confidently delay “reviewing” most instances and therefore train with a much smaller set of instances.",4.2 Efficiency and Effectiveness,[0],[0]
"For example, since the initial network accuracy in IMDb or CIFAR10 is above 56%, Lit and RbF are considerably more efficient from the beginning of the training process.",4.2 Efficiency and Effectiveness,[0],[0]
"However, in case of low initial performance, Lit and RbF tend to avoid delaying instances at lower iterations which leads to poor efficiency at the beginning.",4.2 Efficiency and Effectiveness,[0],[0]
"This is the case for the Addition dataset in which instances are gradually delayed by these two schedulers even at epoch 8 when the performance of the network reaches above 65%, see Figures 8(e) and 8(b).",4.2 Efficiency and Effectiveness,[0],[0]
"However, Lit gains its true efficiency after iteration 12, see Figure 8(b), while RbF still gradually improves the efficiency.",4.2 Efficiency and Effectiveness,[0],[0]
"This might be because of the lower bound delays that RbF estimates, see Equation (9).
",4.2 Efficiency and Effectiveness,[0],[0]
"Furthermore, the effectiveness results in Figure 8 (bottom) show that all schedulers produce comparable accuracy to the Rote scheduler throughout the training process, not just at specific iterations.",4.2 Efficiency and Effectiveness,[0],[0]
"This indicates that these training paradigms can much faster achieve the same generalizability as standard training, see Figures 8(b) and 8(e).",4.2 Efficiency and Effectiveness,[0],[0]
We investigate the effect of spaced repetition on overtraining.,4.3 Robustness against Overtraining,[0],[0]
The optimal number of training epochs required to train fastText on the IMDb dataset is 8 epochs (see Table 1).,4.3 Robustness against Overtraining,[0],[0]
"In this experiment, we run fastText on IMDb for greater number of iterations to investigate the robustness of different schedulers against overtraining.",4.3 Robustness against Overtraining,[0],[0]
The results in Figure 9 show that Lit and RbF (Cos η = 0.5) are more robust against overtraining.,4.3 Robustness against Overtraining,[0],[0]
"In fact, the performance of Lit and RbF further improve at epoch 16 while CL and Rote overfit at epoch 16 (note that CL and Rote also require considerably more amount of time to reach to higher iterations).",4.3 Robustness against Overtraining,[0],[0]
We attribute the robustness of Lit and RbF to the scheduling mechanism which helps the networks to avoid retraining with easy instances.,4.3 Robustness against Overtraining,[0],[0]
"On the other hand, overtraining affects Lit and RbF at higher training iterations, compare performance of each scheduler at epochs 8 and 32.",4.3 Robustness against Overtraining,[0],[0]
This might be because these training paradigms overfit the network by paying too much training attention to very hard instances which might introduce noise to the model.,4.3 Robustness against Overtraining,[0],[0]
"Ebbinghaus (1913, 2013), and recently Murre and Dros (2015), studied the hypothesis of the exponential nature of forgetting, i.e. how information is lost over time when there is no attempt to retain it.",5 Related Work,[0],[0]
"Previous research identified three critical indicators that affect the probability of recall: repeated exposure to learning materials, elapsed time since their last review (Ebbinghaus, 1913; Wixted, 1990; Dempster, 1989), and more recently item difficulty (Reddy et al., 2016).",5 Related Work,[0],[0]
We based our investigation on these findings and validated that these indicators indeed affect memory retention in neural networks.,5 Related Work,[0],[0]
"We then developed training paradigms that utilize the above indicators to train networks.
",5 Related Work,[0],[0]
Bengio et al. (2009) and Kumar et al. (2010) also developed cognitively-motivated training paradigms which are inspired by the principle that learning can be more effective when training starts with easier concepts and gradually proceeds with more difficult ones.,5 Related Work,[0],[0]
"Our idea is motivated by the spaced repetition principle which indicates learning improves with repeated exposure and decays with delay since last exposure (Ebbinghaus, 1913; Dempster, 1989).",5 Related Work,[0],[0]
"Based on this principle, we developed schedulers that space the reviews of training instances over time for efficient and effective training of neural networks.",5 Related Work,[0],[0]
We developed a cognitively-motivated training paradigm (scheduler) that space instances over time for efficient and effective training of neural networks.,6 Conclusion and Future Work,[0],[0]
Our scheduler only uses a small fraction of training data per epoch but still effectively train neural networks.,6 Conclusion and Future Work,[0],[0]
It achieves this by estimating the time (number of epochs) by which training could be delayed for each instance.,6 Conclusion and Future Work,[0],[0]
"Our work was inspired by three recall indicators that affect memory retention in humans, namely difficulty of learning materials, delay since their last review, and memory strength of the learner, which we validated in the context of neural networks.
",6 Conclusion and Future Work,[0],[0]
There are several avenues for future work including the extent to which our RbF model and its kernels could be combined with curriculum learning or Leitner system to either predict easiness of novel training instances to inform curriculum learning or incorporate Leitner’s queueing mechanism to the RbF model.,6 Conclusion and Future Work,[0],[0]
"Other directions include extending RbF to dynamically learn the recall confidence parameter with respect to network behavior, or developing more flexible delay functions with theoretical analysis on their lower and upper bounds.",6 Conclusion and Future Work,[0],[0]
We thank Mitra Mohtarami for her constructive feedback during the development of this paper and anonymous reviewers for their thoughtful comments.,Acknowledgments,[0],[0]
This work was supported by National Institutes of Health (NIH) grant R01GM114355 from the National Institute of General Medical Sciences (NIGMS).,Acknowledgments,[0],[0]
The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.,Acknowledgments,[0],[0]
We present a novel approach for training artificial neural networks.,abstractText,[0],[0]
Our approach is inspired by broad evidence in psychology that shows human learners can learn efficiently and effectively by increasing intervals of time between subsequent reviews of previously learned materials (spaced repetition).,abstractText,[0],[0]
We investigate the analogy between training neural models and findings in psychology about human memory model and develop an efficient and effective algorithm to train neural models.,abstractText,[0],[0]
The core part of our algorithm is a cognitively-motivated scheduler according to which training instances and their “reviews” are spaced over time.,abstractText,[0],[0]
"Our algorithm uses only 34-50% of data per epoch, is 2.9-4.8 times faster than standard training, and outperforms competing state-of-the-art baselines.1",abstractText,[0],[0]
Repeat before Forgetting: Spaced Repetition for Efficient and Effective Training of Neural Networks,title,[0],[0]
"With the ever growing amount of textual data from a large variety of languages, domains, and genres, it has become standard to evaluate NLP algorithms on multiple datasets in order to ensure a consistent performance across heterogeneous setups. However, such multiple comparisons pose significant challenges to traditional statistical analysis methods in NLP and can lead to erroneous conclusions. In this paper we propose a Replicability Analysis framework for a statistically sound analysis of multiple comparisons between algorithms for NLP tasks. We discuss the theoretical advantages of this framework over the current, statistically unjustified, practice in the NLP literature, and demonstrate its empirical value across four applications: multi-domain dependency parsing, multilingual POS tagging, cross-domain sentiment classification and word similarity prediction. 1",text,[0],[0]
The field of Natural Language Processing (NLP) is going through the data revolution.,1 Introduction,[0],[0]
"With the persistent increase of the heterogeneous web, for the first time in human history, written language from multiple languages, domains, and genres is now abundant.",1 Introduction,[0],[0]
"Naturally, the expectations from NLP algorithms also grow and evaluating a new algorithm on as many languages, domains, and genres as possible is becoming a de-facto standard.
",1 Introduction,[0],[0]
"1Our code is at: https://github.com/rtmdrr/replicabilityanalysis-NLP .
",1 Introduction,[0],[0]
"For example, the phrase structure parsers of Charniak (2000) and Collins (2003) were mostly evaluated on the Wall Street Journal Penn Treebank (Marcus et al., 1993), consisting of written, edited English text of economic news.",1 Introduction,[0],[0]
"In contrast, modern dependency parsers are expected to excel on the 19 languages of the CoNLL 2006-2007 shared tasks on multilingual dependency parsing (Buchholz and Marsi, 2006; Nilsson et al., 2007), and additional challenges, such as the shared task on parsing multiple English Web domains (Petrov and McDonald, 2012), are continuously proposed.
",1 Introduction,[0],[0]
"Despite the growing number of evaluation tasks, the analysis toolbox employed by NLP researchers has remained quite stable.",1 Introduction,[0],[0]
"Indeed, in most experimental NLP papers, several algorithms are compared on a number of datasets where the performance of each algorithm is reported together with per-dataset statistical significance figures.",1 Introduction,[0],[0]
"However, with the growing number of evaluation datasets, it becomes more challenging to draw comprehensive conclusions from such comparisons.",1 Introduction,[0],[0]
"This is because although the probability of drawing an erroneous conclusion from a single comparison is small, with multiple comparisons the probability of making one or more false claims may be very high.
",1 Introduction,[0],[0]
"The goal of this paper is to provide the NLP community with a statistical analysis framework, which we term Replicability Analysis, which will allow us to draw statistically sound conclusions in evaluation setups that involve multiple comparisons.",1 Introduction,[0],[0]
"The classical goal of replicability analysis is to examine the consistency of findings across studies in order to address the basic dogma of science, that a find-
471
Transactions of the Association for Computational Linguistics, vol. 5, pp.",1 Introduction,[0],[0]
"471–486, 2017.",1 Introduction,[0],[0]
Action Editor: Brian Roark.,1 Introduction,[0],[0]
"Submission batch: 3/2017; Revision batch: 7/2017; Published 11/2017.
",1 Introduction,[0],[0]
c©2017 Association for Computational Linguistics.,1 Introduction,[0],[0]
"Distributed under a CC-BY 4.0 license.
",1 Introduction,[0],[0]
"ing is more convincingly true if it is replicated in at least one more study (Heller et al., 2014; Patil et al., 2016).",1 Introduction,[0],[0]
"We adapt this goal to NLP, where we wish to ascertain the superiority of one algorithm over another across multiple datasets, which may come from different languages, domains, and genres.",1 Introduction,[0],[0]
"Finding that one algorithm outperforms another across domains gives a sense of consistency to the results and positive evidence that the better performance is not specific to a selected setup.2
In this work we address two questions: (1) Counting: For how many datasets does a given algorithm outperform another?",1 Introduction,[0],[0]
"and (2) Identification: What are these datasets?
",1 Introduction,[0],[0]
"When comparing two algorithms on multiple datasets, NLP papers often answer informally the questions we address in this work.",1 Introduction,[0],[0]
"In some cases this is done without any statistical analysis, by simply declaring better performance of a given algorithm for datasets where its performance measure is better than that of another algorithm, and counting these datasets.",1 Introduction,[0],[0]
In other cases answers are based on the p-values from statistical tests performed for each dataset: declaring better performance for datasets with p-value below the significance level (e.g. 0.05) and counting these datasets.,1 Introduction,[0],[0]
"While it is clear that the first approach is not statistically valid, it seems that our community is not aware of the fact that the second approach, which may seem statistically sound, is not valid as well.",1 Introduction,[0],[0]
"This may lead to erroneous conclusions, which result in adopting new (and probably complicated) algorithms, while they are not better than previous (probably more simple) ones.
",1 Introduction,[0],[0]
"In this work, we demonstrate this problem and show that it becomes more severe as the number of evaluation sets grows, which seems to be the current trend in NLP.",1 Introduction,[0],[0]
"We adopt a known general statistical methodology for addressing the counting (question (1)) and identification (question (2)) problems, by choosing the tests and procedures which are valid for
2“Replicability” is sometimes referred to as “reproducibility”.",1 Introduction,[0],[0]
"In recent NLP work the term reproducibility was used when trying to get identical results on the same data (Névéol et al., 2016; Marrese-Taylor and Matsuo, 2017).",1 Introduction,[0],[0]
"In this paper, we adopt the meaning of “replicability” and its distinction from “reproducibility” from Peng (2011) and Leek and Peng (2015) and refer to replicability analysis as the effort to show that a finding is consistent over different datasets from different domains or languages, and is not idiosyncratic to a specific scenario.
situations encountered in NLP problems, and giving specific recommendations for such situations.
",1 Introduction,[0],[0]
"Particularly, we first demonstrate (Section 3) that the current prominent approach in the NLP literature, identifying the datasets for which the difference between the performance of the algorithms reaches a predefined significance level according to some statistical significance test, does not guarantee to bound the probability to make at least one erroneous claim.",1 Introduction,[0],[0]
Hence this approach is error-prone when the number of participating datasets is large.,1 Introduction,[0],[0]
We thus propose an alternative approach (Section 4).,1 Introduction,[0],[0]
"For question (1), we adopt the approach of Benjamini et al. (2009) to replicability analysis of multiple studies, based on the partial conjunction framework of Benjamini and Heller (2008).",1 Introduction,[0],[0]
This analysis comes with a guarantee that the probability of overestimating the true number of datasets with effect is upper bounded by a predefined constant.,1 Introduction,[0],[0]
"For question (2), we motivate a multiple testing procedure which guarantees that the probability of making at least one erroneous claim on the superiority of one algorithm over another is upper bounded by a predefined constant.
",1 Introduction,[0],[0]
"In Sections 5 and 6 we demonstrate how to apply the proposed frameworks to two synthetic data toy examples and four NLP applications: multidomain dependency parsing, multilingual POS tagging, cross-domain sentiment classification, and word similarity prediction with word embedding models.",1 Introduction,[0],[0]
"Our results demonstrate that the current practice in NLP for addressing our questions is error-prone, and illustrate the differences between it and the proposed statistically sound approach.
",1 Introduction,[0],[0]
"We hope that this work will encourage our community to increase the number of standard evaluation setups per task when appropriate (e.g. including additional languages and domains), possibly paving the way to hundreds of comparisons per study.",1 Introduction,[0],[0]
This is due to two main reasons.,1 Introduction,[0],[0]
"First, replicability analysis is a statistically sound framework that allows a researcher to safely draw valid conclusions with well defined statistical guarantees.",1 Introduction,[0],[0]
"Moreover, this framework provides a means of summarizing a large number of experiments with a handful of easily interpretable numbers (e.g., see Table 1).",1 Introduction,[0],[0]
"This allows researchers to report results over a large number of comparisons in a concise manner, delving into details of particular comparisons when necessary.",1 Introduction,[0],[0]
"Our work recognizes the current trend in the NLP community where, for many tasks and applications, the number of evaluation datasets constantly increases.",2 Previous Work,[0],[0]
We believe this trend is inherent to language processing technology due to the multiplicity of languages and of linguistic genres and domains.,2 Previous Work,[0],[0]
"In order to extend the reach of NLP algorithms, they have to be designed so that they can deal with many languages and with the various domains of each.",2 Previous Work,[0],[0]
"Having a sound statistical framework that can deal with multiple comparisons is hence crucial for the field.
",2 Previous Work,[0],[0]
This section is hence divided into two.,2 Previous Work,[0],[0]
"We start by discussing representative examples for multiple comparisons in NLP, focusing on evaluations across multiple languages and multiple domains.",2 Previous Work,[0],[0]
"We then discuss existing analysis frameworks for multiple comparisons, both in the NLP and in the machine learning literatures, pointing to the need for establishing new standards for our community.
",2 Previous Work,[0],[0]
"Multiple Comparisons in NLP Multiple comparisons of algorithms over datasets from different languages, domains and genres have become a de-facto standard in many areas of NLP.",2 Previous Work,[0],[0]
Here we survey a number of representative examples.,2 Previous Work,[0],[0]
"A full list of NLP tasks is beyond the scope of this paper.
",2 Previous Work,[0],[0]
"A common multilingual example is, naturally, machine translation, where it is customary to compare algorithms across a large number of sourcetarget language pairs.",2 Previous Work,[0],[0]
"This is done, for example, with the Europarl corpus consisting of 21 European languages (Koehn, 2005; Koehn and Schroeder, 2007) and with the datasets of the WMT workshop series with its multiple domains (e.g. news and biomedical in 2017), each consisting of several language pairs (7 and 14, respectively, in 2017).
",2 Previous Work,[0],[0]
Multiple dataset comparisons are also abundant in domain adaptation work.,2 Previous Work,[0],[0]
"Representative tasks include named entity recognition (Guo et al., 2009), POS tagging (Daumé III, 2007), dependency parsing (Petrov and McDonald, 2012), word sense disambiguation (Chan and Ng, 2007) and sentiment classification (Blitzer et al., 2006; Blitzer et al., 2007).
",2 Previous Work,[0],[0]
"More recently, with the emergence of crowdsourcing that makes data collection cheap and fast (Snow et al., 2008), an ever growing number of datasets is being created.",2 Previous Work,[0],[0]
"This is particularly notice-
able in lexical semantics tasks that have become central in NLP research due to the prominence of neural networks.",2 Previous Work,[0],[0]
"For example, it is customary to compare word embedding models (Mikolov et al., 2013; Pennington et al., 2014; Ó",2 Previous Work,[0],[0]
"Séaghdha and Korhonen, 2014; Levy and Goldberg, 2014; Schwartz et al., 2015) on multiple datasets where word pairs are scored according to the degree to which different semantic relations, such as similarity and association, hold between the members of the pair (Finkelstein et al., 2001a; Bruni et al., 2014; Silberer and Lapata, 2014; Hill et al., 2015).",2 Previous Work,[0],[0]
"In some works (e.g., Baroni et al. (2014))",2 Previous Work,[0],[0]
"these embedding models are compared across a large number of simple tasks.
",2 Previous Work,[0],[0]
"As discussed in Section 1, the outcomes of such comparisons are often summarized in a table that presents numerical performance values, usually accompanied by statistical significance figures and sometimes also with cross-comparison statistics such as average performance figures.",2 Previous Work,[0],[0]
"Here, we analyze the conclusions that can be drawn from this information and suggest that with the growing number of comparisons, a more intricate analysis is required.
",2 Previous Work,[0],[0]
"Existing Analysis Frameworks Machine learning work on multiple dataset comparisons dates back to Dietterich (1998) who raised the question: “given two learning algorithms and datasets from several domains, which algorithm will produce more accurate classifiers when trained on examples from new domains?”.",2 Previous Work,[0],[0]
The seminal work that proposed practical means for this problem is that of Demšar (2006).,2 Previous Work,[0],[0]
"Given performance measures for two algorithms on multiple datasets, the authors test whether there is at least one dataset on which the difference between the algorithms is statistically significant.",2 Previous Work,[0],[0]
"For this goal they propose methods such as a paired t-test, a nonparametric sign-rank test and a wins/losses/ties count, all computed across the results collected from all participating datasets.",2 Previous Work,[0],[0]
"In contrast, our goal is to count and identify the datasets for which one algorithm significantly outperforms the other, which provides more intricate information, especially when the datasets come from different sources.
",2 Previous Work,[0],[0]
"In NLP, several studies addressed the problem of measuring the statistical significance of results on a single dataset (e.g., Berg-Kirkpatrick et al. (2012); Søgaard (2013); Søgaard et al. (2014)).",2 Previous Work,[0],[0]
"Søgaard
(2013) is, to the best of our knowledge, the only work that addressed the statistical properties of evaluation with multiple datasets.",2 Previous Work,[0],[0]
"For this aim he modified the statistical tests proposed in Demšar (2006) to use a Gumbel distribution assumption on the test statistics, which he considered to suit NLP better than the original Gaussian assumption.",2 Previous Work,[0],[0]
"However, while this procedure aims to estimate the effect size across datasets, it answers neither the counting nor the identification question of Section 1.
",2 Previous Work,[0],[0]
In the next section we provide the preliminary knowledge from the field of statistics that forms the basis for the proposed framework and then proceed with its description.,2 Previous Work,[0],[0]
We start by formulating a general hypothesis testing framework for a comparison between two algorithms.,3 Preliminaries,[0],[0]
"This is a common type of hypothesis testing framework applied in NLP, its detailed formulation will help us develop our ideas.",3 Preliminaries,[0],[0]
"We wish to compare between two algorithms, A and B. Let X be a collection of datasets X = {X1, X2, . . .",3.1 Hypothesis Testing,[0],[0]
", XN}, where for all i ∈ {1, . . .",3.1 Hypothesis Testing,[0],[0]
", N}, Xi = {xi,1, . . .",3.1 Hypothesis Testing,[0],[0]
", xi,ni} .",3.1 Hypothesis Testing,[0],[0]
Each dataset Xi can be of a different language or a different domain.,3.1 Hypothesis Testing,[0],[0]
"We denote by xi,k the granular unit on which results are being measured, that, in most NLP tasks, is a word or a sequence of words.",3.1 Hypothesis Testing,[0],[0]
"The difference in performance between the two algorithms is measured using one or more of the evaluation measures in the setM = {M1, . . .",3.1 Hypothesis Testing,[0],[0]
",Mm}.3
Let us denoteMj(ALG,Xi) as the value of the measureMj when algorithmALG is applied on the dataset Xi.",3.1 Hypothesis Testing,[0],[0]
"Without loss of generality, we assume that higher values of the measure are better.",3.1 Hypothesis Testing,[0],[0]
"We define the difference in performance between two algorithms, A and B, according to the measure",3.1 Hypothesis Testing,[0],[0]
"Mj on the dataset Xi as:
δj(X i) =Mj(A,Xi)−Mj(B,Xi).
",3.1 Hypothesis Testing,[0],[0]
"3To keep the discussion concise, throughout this paper we assume that only one evaluation measure is used.",3.1 Hypothesis Testing,[0],[0]
"Our framework can be easily extended to deal with multiple measures.
",3.1 Hypothesis Testing,[0],[0]
"Finally, using this notation we formulate the following statistical hypothesis testing problem:
H0i(j) :δj(X i) ≤ 0
H1i(j) :δj(X i) > 0.
(1)
The null hypothesis, stating that there is no difference between the performance of algorithm A and algorithmB, or thatB performs better, is tested versus the alternative statement thatA is superior.",3.1 Hypothesis Testing,[0],[0]
"If the statistical test results in rejecting the null hypothesis, one concludes that A outperforms B in this setup.",3.1 Hypothesis Testing,[0],[0]
"Otherwise, there is not enough evidence in the data to make this conclusion.
",3.1 Hypothesis Testing,[0],[0]
"Rejection of the null hypothesis when it is true is termed type I error, and non-rejection of the null hypothesis when the alternative is true is termed type II error.",3.1 Hypothesis Testing,[0],[0]
"The classical approach to hypothesis testing is to find a test that guarantees that the probability of making a type I error is upper bounded by a predefined constant α, the test significance level, while achieving as low probability of type II error as possible, a.k.a “achieving as high power as possible”.
",3.1 Hypothesis Testing,[0],[0]
We next turn to the case where the difference between two algorithms is tested across multiple datasets.,3.1 Hypothesis Testing,[0],[0]
Equation 1 defines a multiple hypothesis testing problem when considering the formulation for all N datasets.,3.2 The Multiplicity Problem,[0],[0]
"If N is large, testing each hypothesis separately at the nominal significance level may result in a high number of erroneously rejected null hypotheses.",3.2 The Multiplicity Problem,[0],[0]
"In our context, when the performance of algorithm A is compared to that of algorithm B across multiple datasets, and for each dataset algorithm A is declared as superior, based on a statistical test at the nominal significance level α, the expected number of erroneous claims may grow as N grows.
",3.2 The Multiplicity Problem,[0],[0]
"For example, if a single test is performed with a significance level of α = 0.05, there is only a 5% chance of incorrectly rejecting the null hypothesis.",3.2 The Multiplicity Problem,[0],[0]
"On the other hand, for 100 tests where all null hypotheses are true, the expected number of incorrect rejections is 100 · 0.05 = 5.",3.2 The Multiplicity Problem,[0],[0]
"Denoting the total number of type I errors as V , we can see below that if the test statistics are independent then the probability of
making at least one incorrect rejection is 0.994:
P(V > 0)",3.2 The Multiplicity Problem,[0],[0]
"= 1− P(V = 0) =
1− 100∏
i=1
P(no type I error in i)",3.2 The Multiplicity Problem,[0],[0]
"=1− (1− 0.05)100.
",3.2 The Multiplicity Problem,[0],[0]
This demonstrates that the naive method of counting the datasets for which significance was reached at the nominal level is error-prone.,3.2 The Multiplicity Problem,[0],[0]
"Similar examples can be constructed for situations where some of the null hypotheses are false.
",3.2 The Multiplicity Problem,[0],[0]
"The multiple testing literature proposes various procedures for bounding the probability of making at least one type I error, as well as other, less restrictive error criteria (see a survey in Farcomeni (2007)).",3.2 The Multiplicity Problem,[0],[0]
"In this paper, we address the questions of counting and identifying the datasets for which algorithm A outperforms B, with certain statistical guarantees regarding erroneous claims.",3.2 The Multiplicity Problem,[0],[0]
"While identifying the datasets gives more information when compared to just declaring their number, we consider these two questions separately.",3.2 The Multiplicity Problem,[0],[0]
"As our experiments show, according to the statistical analysis we propose the estimated number of datasets with effect (question 1) may be higher than the number of identified datasets (question 2).",3.2 The Multiplicity Problem,[0],[0]
We next present the fundamentals of the partial conjunction framework which is at the heart of our proposed methods.,3.2 The Multiplicity Problem,[0],[0]
We start by reformulating the set of hypothesis testing problems of Equation 1 as a unified hypothesis testing problem.,3.3 Partial Conjunction Hypotheses,[0],[0]
This problem aims to identify whether algorithm A is superior to B across all datasets.,3.3 Partial Conjunction Hypotheses,[0],[0]
"The notation for the null hypothesis in this problem is HN/N0 since we test if N out of N alternative hypotheses are true:
H N/N 0",3.3 Partial Conjunction Hypotheses,[0],[0]
":
N⋃
i=1
H0i is true vs. H N/N 1 :
N⋂
i=1
H1i is true.
",3.3 Partial Conjunction Hypotheses,[0],[0]
"Requiring the rejection of the disjunction of all null hypotheses is often too restrictive for it involves observing a significant effect on all datasets, i ∈ {1, . . .",3.3 Partial Conjunction Hypotheses,[0],[0]
", N}.",3.3 Partial Conjunction Hypotheses,[0],[0]
"Instead, one can require a rejection of the global null hypothesis stating that all individual null hypotheses are true, i.e., evidence that
at least one alternative hypothesis is true.",3.3 Partial Conjunction Hypotheses,[0],[0]
"This hypothesis testing problem is formulated as follows:
H 1/N 0",3.3 Partial Conjunction Hypotheses,[0],[0]
":
N⋂
i=1
H0i is true vs. H 1/N 1 :
N⋃
i=1
H1i is true.
",3.3 Partial Conjunction Hypotheses,[0],[0]
"Obviously, rejecting the global null may not provide enough information: it only indicates that algorithm A outperforms B on at least one dataset.",3.3 Partial Conjunction Hypotheses,[0],[0]
"Hence, this claim does not give any evidence for the consistency of the results across multiple datasets.
",3.3 Partial Conjunction Hypotheses,[0],[0]
"A natural compromise between the above two formulations is to test the partial conjunction null, which states that the number of false null hypotheses is lower than u, where 1 ≤ u ≤ N is a pre-specified integer constant.",3.3 Partial Conjunction Hypotheses,[0],[0]
"The partial conjunction test contrasts this statement with the alternative statement that at least u out of the N null hypotheses are false.
",3.3 Partial Conjunction Hypotheses,[0],[0]
Definition 1 (Benjamini and Heller (2008)).,3.3 Partial Conjunction Hypotheses,[0],[0]
"Consider N ≥ 2 null hypotheses: H01, H02, . . .",3.3 Partial Conjunction Hypotheses,[0],[0]
",H0N , and let p1, . . .",3.3 Partial Conjunction Hypotheses,[0],[0]
", pN be their associated p−values.",3.3 Partial Conjunction Hypotheses,[0],[0]
"Let k be the true unknown number of false null hypotheses, then our question “Are at least u out of N null hypotheses false?” can be formulated as follows:
H u/N 0",3.3 Partial Conjunction Hypotheses,[0],[0]
:,3.3 Partial Conjunction Hypotheses,[0],[0]
"k < u vs. H u/N 1 : k ≥ u.
",3.3 Partial Conjunction Hypotheses,[0],[0]
"In our context, k is the number of datasets where algorithm A is truly better, and the partial conjunction test examines whether algorithmA outperforms algorithm B in at least u of N cases.
",3.3 Partial Conjunction Hypotheses,[0],[0]
Benjamini and Heller (2008) developed a general method for testing the above hypothesis for a given u. They also showed how to extend their method in order to answer our counting question.,3.3 Partial Conjunction Hypotheses,[0],[0]
"We next describe their framework and advocate a different, yet related method for dataset identification.",3.3 Partial Conjunction Hypotheses,[0],[0]
"Referred to as the cornerstone of science (Moonesinghe et al., 2007), replicability analysis is of predominant importance in many scientific fields including psychology (Collaboration, 2012), genomics (Heller et al., 2014), economics (Herndon et al., 2014) and medicine (Begley and Ellis, 2012), among others.",4 Replicability Analysis for NLP,[0],[0]
"Findings are usually considered as replicated if they are obtained in two or more
studies that differ from each other in some aspects (e.g. language, domain or genre in NLP).
",4 Replicability Analysis for NLP,[0],[0]
"The replicability analysis framework we employ (Benjamini and Heller, 2008; Benjamini et al., 2009) is based on partial conjunction testing.",4 Replicability Analysis for NLP,[0],[0]
"Particularly, these authors have shown that a lower bound on the number of false null hypotheses with a confidence level of 1 − α can be obtained by finding the largest u for which we can reject the partial conjunction null hypothesis Hu/N0 along with H
1/N 0 , . . .",4 Replicability Analysis for NLP,[0],[0]
",H (u−1)/N 0 at a significance levelα.",4 Replicability Analysis for NLP,[0],[0]
"Since rejecting Hu/N0 means that we see evidence in at least u out of N datasets, algorithm",4 Replicability Analysis for NLP,[0],[0]
"A is superior to B. This lower bound on k is taken as our answer to the Counting question of Section 1.
",4 Replicability Analysis for NLP,[0],[0]
"In line with the hypothesis testing framework of Section 3, the partial conjunction null, Hu/N0 , is rejected at level α if pu/N ≤ α, where pu/N is the partial conjunction p-value.",4 Replicability Analysis for NLP,[0],[0]
"Based on the known methods for testing the global null hypothesis (see, e.g., Loughin (2004)), Benjamini and Heller (2008) proposed methods for combining the p−values p1, . . .",4 Replicability Analysis for NLP,[0],[0]
", pN of H01, H02, . . .",4 Replicability Analysis for NLP,[0],[0]
",H0N in order to obtain pu/N .",4 Replicability Analysis for NLP,[0],[0]
"Below, we describe two such methods and their properties.",4 Replicability Analysis for NLP,[0],[0]
"The methods we focus on were developed by Benjamini and Heller (2008), and are based on Fisher’s and Bonferroni’s methods for testing the global null hypothesis.",4.1 The Partial Conjunction p−value,[0],[0]
"For brevity, we name them Bonferroni and Fisher.",4.1 The Partial Conjunction p−value,[0],[0]
"We choose them because they are valid in different setups that are frequently encountered in NLP (Section 6): Bonferroni for dependent datasets and both Fisher and Bonferroni for independent datasets.4
Bonferroni’s method does not make any assumptions about the dependencies between the participating datasets and it is hence applicable in NLP tasks, since in NLP it is most often hard to determine the type of dependence between the datasets.",4.1 The Partial Conjunction p−value,[0],[0]
"Fisher’s method, while assuming independence across the
4For simplicity we refer to dependent/independent datasets as those for which the test statistics are dependent/independent.",4.1 The Partial Conjunction p−value,[0],[0]
"We assume the test statistics are independent if the corresponding datasets do not have mutual samples, and one dataset is not a transformation of the other.
participating datasets, is often more powerful than Bonferroni’s method (see Loughin (2004) and Benjamini and Heller (2008) for other methods and a comparison between them).",4.1 The Partial Conjunction p−value,[0],[0]
"Our recommendation is hence to use the Bonferroni’s method when the datasets are dependent and to use the more powerful Fisher’s method when the datasets are independent.
",4.1 The Partial Conjunction p−value,[0],[0]
"Let p(i) be the i-th smallest p−value among p1, . . .",4.1 The Partial Conjunction p−value,[0],[0]
", pN .",4.1 The Partial Conjunction p−value,[0],[0]
"The partial conjunction p−values are:
p u/N Bonferroni = (N − u+ 1)p(u) (2)
p u/N Fisher = P ( χ22(N−u+1) ≥ −2",4.1 The Partial Conjunction p−value,[0],[0]
"N∑
i=u
ln p(i)
) (3)
where χ22(N−u+1) denotes a chi-squared random variable with 2(N − u+ 1) degrees of freedom.
",4.1 The Partial Conjunction p−value,[0],[0]
"To understand the reasoning behind these methods, let us consider first the above p−values for testing the global null, i.e., for the case of u = 1.",4.1 The Partial Conjunction p−value,[0],[0]
Rejecting the global null hypothesis requires evidence that at least one null hypothesis is false.,4.1 The Partial Conjunction p−value,[0],[0]
"Intuitively, we would like to see one or more small p−values.
",4.1 The Partial Conjunction p−value,[0],[0]
Both of the methods above agree with this intuition.,4.1 The Partial Conjunction p−value,[0],[0]
"Bonferroni’s method rejects the global null if p(1) ≤ α/N , i.e. if the minimum p−value is small enough, where the threshold guarantees that the significance level of the test is α for any dependency among the p−values p1, . . .",4.1 The Partial Conjunction p−value,[0],[0]
", pN .",4.1 The Partial Conjunction p−value,[0],[0]
"Fisher’s method rejects the global null for large values of −2∑Ni=1 ln p(i), or equivalently for small values of∏N i=1",4.1 The Partial Conjunction p−value,[0],[0]
pi.,4.1 The Partial Conjunction p−value,[0],[0]
"That is, while both these methods are intuitive, they are different.",4.1 The Partial Conjunction p−value,[0],[0]
Fisher’s method requires a small enough product of p−values as evidence that at least one null hypothesis is false.,4.1 The Partial Conjunction p−value,[0],[0]
"Bonferroni’s method, on the other hand, requires as evidence at least one small enough p−value.
",4.1 The Partial Conjunction p−value,[0],[0]
"For the case u = N , i.e., when the alternative states that all null hypotheses are false, both methods require that the maximal p−value is small enough for rejection of HN/N0 .",4.1 The Partial Conjunction p−value,[0],[0]
This is also intuitive because we expect that all the p−values will be small when all the null hypotheses are false.,4.1 The Partial Conjunction p−value,[0],[0]
"For other cases, where 1 < u < N , the reasoning is more complicated and is beyond the scope of this paper.
",4.1 The Partial Conjunction p−value,[0],[0]
The partial conjunction test for a specific u answers the question “Does algorithm A perform better than B on at least u datasets?”,4.1 The Partial Conjunction p−value,[0],[0]
"The next step is
the estimation of the number of datasets for which algorithm A performs better than B.",4.1 The Partial Conjunction p−value,[0],[0]
Recall that the number of datasets where algorithm A outperforms algorithm B (denoted with k in Definition 1) is the true number of false null hypotheses in our problem.,4.2 Dataset Counting (Question 1),[0],[0]
"Benjamini and Heller (2008) proposed to estimate k to be the largest u for which H u/N 0 , along with H 1/N 0 , . . .",4.2 Dataset Counting (Question 1),[0],[0]
",H (u−1)/N 0 is rejected.",4.2 Dataset Counting (Question 1),[0],[0]
"Specifically, the estimator k̂ is defined as follows:
k̂ = max{u : pu/N∗ ≤",4.2 Dataset Counting (Question 1),[0],[0]
"α}, (4)
where pu/N∗ = max{p(u−1)/N∗ , pu/N}, p1/N = p1/N∗",4.2 Dataset Counting (Question 1),[0],[0]
and α is the desired upper bound on the probability to overestimate the true k.,4.2 Dataset Counting (Question 1),[0],[0]
"It is guaranteed that P(k̂ > k) ≤ α as long as the p−value combination method used for constructing pu/N is valid for the given dependency across the test statistics.5 When k̂ is based on pu/NBonferroni it is denoted with k̂Bonferroni; when it is based on p u/N Fisher, it is denoted with k̂Fisher.",4.2 Dataset Counting (Question 1),[0],[0]
"A crucial practical consideration, when choosing between k̂Bonferroni and k̂Fisher, is the assumed dependency between the datasets.",4.2 Dataset Counting (Question 1),[0],[0]
"As discussed in Section 4.1, pu/NFisher is recommended when the participating datasets are assumed to be independent; when this assumption cannot be made, only pu/NBonferroni is appropriate.",4.2 Dataset Counting (Question 1),[0],[0]
"As the k̂ estimators are based on the respective pu/N s, the same considerations hold when choosing between them.
",4.2 Dataset Counting (Question 1),[0],[0]
"With the k̂ estimators, one can answer the counting question of Section 1, reporting that algorithm",4.2 Dataset Counting (Question 1),[0],[0]
A is better than algorithm B in at least k̂ out of N datasets with a confidence level of 1 − α.,4.2 Dataset Counting (Question 1),[0],[0]
"Regarding the identification question, a natural approach would be to declare the k̂ datasets with the smallest p−values as those for which the effect holds.",4.2 Dataset Counting (Question 1),[0],[0]
"However, with k̂Fisher this approach does not guarantee control over type I errors.",4.2 Dataset Counting (Question 1),[0],[0]
"In contrast, for k̂Bonferroni, the above approach comes with such guarantees, as described in the next section.
",4.2 Dataset Counting (Question 1),[0],[0]
5This result is a special case of Theorem 4 in Benjamini and Heller (2008).,4.2 Dataset Counting (Question 1),[0],[0]
"As demonstrated in Section 3.2, identifying the datasets with p−value below the nominal significance level and declaring them as those where algorithm A is better than B may lead to a very high number of erroneous claims.",4.3 Dataset Identification (Question 2),[0],[0]
A variety of methods exist for addressing this problem.,4.3 Dataset Identification (Question 2),[0],[0]
"A classical and very simple method for addressing this problem is named the Bonferroni’s procedure, which compensates for the increased probability of making at least one type I error by testing each individual hypothesis at a significance level of α′ = α/N , where α is the predefined bound on this probability and N is the number of hypotheses tested.6 While Bonferroni’s procedure is valid for any dependency among the p−values, the probability of detecting a true effect using this procedure is often very low, because of its strict p−value threshold.
",4.3 Dataset Identification (Question 2),[0],[0]
"Many other procedures controlling the above or other error criteria, and having less strict p−value thresholds, have been proposed.",4.3 Dataset Identification (Question 2),[0],[0]
"Below we advocate one of these methods: the Holm procedure (Holm, 1979).",4.3 Dataset Identification (Question 2),[0],[0]
This is a simple p−value based procedure that is concordant with the partial conjunction analysis when pu/NBonferroni is used in that analysis.,4.3 Dataset Identification (Question 2),[0],[0]
"Importantly for NLP applications, Holm controls the probability of making at least one type I error for any type of dependency between the participating datasets (see a demonstration in Section 6).
",4.3 Dataset Identification (Question 2),[0],[0]
"Let α be the desired upper bound on the probability that at least one false rejection occurs, let p(1) ≤",4.3 Dataset Identification (Question 2),[0],[0]
p(2) ≤ . . .,4.3 Dataset Identification (Question 2),[0],[0]
≤,4.3 Dataset Identification (Question 2),[0],[0]
p(N) be the ordered p−values and let the associated hypotheses be H(1) . . .,4.3 Dataset Identification (Question 2),[0],[0]
H(N).,4.3 Dataset Identification (Question 2),[0],[0]
"The Holm procedure for identifying the datasets with a significant effect is given below.
",4.3 Dataset Identification (Question 2),[0],[0]
Procedure Holm 1),4.3 Dataset Identification (Question 2),[0],[0]
"Let k be the minimal index such that
p(k)",4.3 Dataset Identification (Question 2),[0],[0]
> α N+1−k . 2) Reject the null hypotheses H(1) . . .,4.3 Dataset Identification (Question 2),[0],[0]
H(k−1),4.3 Dataset Identification (Question 2),[0],[0]
"and
do not reject H(k) . . .",4.3 Dataset Identification (Question 2),[0],[0]
H(N).,4.3 Dataset Identification (Question 2),[0],[0]
"If no such k exists, then reject all null hypotheses.
",4.3 Dataset Identification (Question 2),[0],[0]
The output of the Holm procedure is a rejection 6Bonferroni’s correction is based on similar considerations as pu/NBonferroni for u = 1 (Eq. 2).,4.3 Dataset Identification (Question 2),[0],[0]
"The partial conjunction framework (Sec. 4.1) extends this idea for other values of u.
list of null hypotheses; the corresponding datasets are those we return in response to the identification question of Section 1.",4.3 Dataset Identification (Question 2),[0],[0]
Note that the Holm procedure rejects a subset of hypotheses with p-value below α.,4.3 Dataset Identification (Question 2),[0],[0]
Each p-value is compared to a threshold which is smaller or equal to α and depends on the number of evaluation datasets N.,4.3 Dataset Identification (Question 2),[0],[0]
"The dependence of the thresholds on N can be intuitively explained as follows: the probability of making one or more erroneous claims may increase with N, as demonstrated in Section 3.2.",4.3 Dataset Identification (Question 2),[0],[0]
"Therefore, in order to bound this probability by a pre-specified level α, the thresholds for p-values should depend on N.
It can be shown that the Holm procedure at level α always rejects the k̂Bonferroni hypotheses with the smallest p−values, where k̂Bonferroni is the lower bound for k with a confidence level of 1 − α.",4.3 Dataset Identification (Question 2),[0],[0]
"Therefore, k̂Bonferroni corresponding to a confidence level of 1 − α is always smaller or equal to the number of datasets for which the difference between the compared algorithms is significant at level α.",4.3 Dataset Identification (Question 2),[0],[0]
"This is not surprising in view of the fact that, without making any assumptions on the dependencies among the datasets, k̂Bonferroni guarantees that the probability of making a too optimistic claim (k̂ > k) is bounded by α, when simply counting the number of datasets with p-value below α, the probability of making a too optimistic claim may be close to 1, as demonstrated in Section 5.
",4.3 Dataset Identification (Question 2),[0],[0]
Framework Summary Following Section 4.2 we answer the counting question of Section 1 by reporting either k̂Fisher (when all datasets can be assumed to be independent) or k̂Bonferroni (when such an independence assumption cannot be made).,4.3 Dataset Identification (Question 2),[0],[0]
"Based on Section 4.3 we suggest to answer the identification question of Section 1 by reporting the rejection list returned by the Holm procedure.
",4.3 Dataset Identification (Question 2),[0],[0]
Our proposed framework is based on certain assumptions regarding the experiments conducted in NLP setups.,4.3 Dataset Identification (Question 2),[0],[0]
The most prominent of these assumptions states that for dependent datasets the type of dependency cannot be determined.,4.3 Dataset Identification (Question 2),[0],[0]
"Indeed, to the best of our knowledge, the nature of the dependency between dependent test sets in NLP work has not been analyzed before.",4.3 Dataset Identification (Question 2),[0],[0]
In Section 7 we revisit our assumptions and point to alternative methods for answering our questions.,4.3 Dataset Identification (Question 2),[0],[0]
"These methods may be ap-
propriate under other assumptions that may become relevant in future.
",4.3 Dataset Identification (Question 2),[0],[0]
We next demonstrate the value of the proposed replicability analysis through toy examples with synthetic data (Section 5) as well as analysis of state-of-the-art algorithms for four major NLP applications (Section 6).,4.3 Dataset Identification (Question 2),[0],[0]
"Our point of reference is the standard, yet statistically unjustified, counting method that sets its estimator, k̂count, to the number of datasets for which the difference between the compared algorithms is significant with p−value ≤ α (i.e. k̂count = #{i : pi ≤ α}).7",4.3 Dataset Identification (Question 2),[0],[0]
"For the examples of this section we synthesize p−values to emulate a test with N = 100 hypotheses (domains), and set α to 0.05.",5 Toy Examples,[0],[0]
"We start with a simulation of a scenario where algorithmA is equivalent to B for each domain, and the datasets representing these domains are independent.",5 Toy Examples,[0],[0]
"We sample the 100 p−values from a standard uniform distribution, which is the p−value distribution under the null hypothesis, repeating the simulation 1,000 times.
",5 Toy Examples,[0],[0]
"Since all the null hypotheses are true then k, the number of false null hypotheses, is 0.",5 Toy Examples,[0],[0]
"Figure 1 presents the histogram of k̂ values from all 1,000 iterations according to k̂Bonferroni, k̂Fisher and k̂count.
",5 Toy Examples,[0],[0]
The figure clearly demonstrates that k̂count provides an overestimation of k while k̂Bonferroni and k̂Fisher do much better.,5 Toy Examples,[0],[0]
"Indeed, the histogram yields the following probability estimates: P̂ (k̂count >
7We use α in two different contexts: the significance level of an individual test and the bound on the probability to overestimate k.",5 Toy Examples,[0],[0]
"This is the standard notation in the statistical literature.
",5 Toy Examples,[0],[0]
"k) = 0.963, P̂ (k̂Bonferroni > k) = 0.001 and P̂ (k̂Fisher > k) = 0.021 (only the latter two are lower than 0.05).",5 Toy Examples,[0],[0]
"This simulation strongly supports the theoretical results of Section 4.2.
",5 Toy Examples,[0],[0]
"To consider a scenario where a dependency between the participating datasets does exist, we consider a second toy example.",5 Toy Examples,[0],[0]
"In this example we generate N = 100 p−values corresponding to 34 independent normal test statistics, and two other groups of 33 positively correlated normal test statistics with ρ = 0.2 and ρ = 0.5, respectively.",5 Toy Examples,[0],[0]
"We again assume that all null hypotheses are true and thus all the p−values are distributed uniformly, repeating the simulation 1,000 times.",5 Toy Examples,[0],[0]
"To generate positively dependent p−values, we followed the process described in Section 6.1 of Benjamini et al. (2006).
",5 Toy Examples,[0],[0]
We estimate the probability that k̂ > k,5 Toy Examples,[0],[0]
= 0,5 Toy Examples,[0],[0]
"for the three k̂ estimators based on the 1000 repetitions and get the values of: P̂ (k̂count > k) = 0.943, P̂ (k̂Bonferroni > k) = 0.046 and P̂ (k̂Fisher > k) = 0.234.",5 Toy Examples,[0],[0]
"This simulation demonstrates the importance of using Bonferroni’s method rather than Fisher’s method when the datasets are dependent, even if some of the datasets are independent.",5 Toy Examples,[0],[0]
In this section we demonstrate the potential impact of replicability analysis on the way experimental results are analyzed in NLP setups.,6 NLP Applications,[0],[0]
We explore four NLP applications: (a) two where the datasets are independent: multi-domain dependency parsing and multilingual POS tagging; and (b) two where dependency between the datasets does exist: cross-domain sentiment classification and word similarity prediction with word embedding models.,6 NLP Applications,[0],[0]
"Dependency Parsing We consider a multidomain setup, analyzing the results reported in Choi et al. (2015).",6.1 Data,[0],[0]
"The authors compared ten state-of-the-art parsers from which we pick three: (a) Mate (Bohnet, 2010)8 that performed best on the majority of datasets; (b) Redshift (Honnibal et al., 2013)9 which demonstrated comparable, still somewhat lower, performance compared to Mate;
8code.google.com/p/mate-tools.",6.1 Data,[0],[0]
"9github.com/syllog1sm/Redshift.
",6.1 Data,[0],[0]
"and (c) SpaCy (Honnibal and Johnson, 2015) that was substantially outperformed by Mate.
",6.1 Data,[0],[0]
"All parsers were trained and tested on the English portion of the OntoNotes 5 corpus (Weischedel et al., 2011; Pradhan et al., 2013), a large multigenre corpus consisting of the following 7 genres: broadcasting conversations (BC), broadcasting news (BN), news magazine (MZ), newswire (NW), pivot text (PT), telephone conversations (TC) and web text (WB).",6.1 Data,[0],[0]
"Train and test set size (in sentences) range from 6672 to 34,492 and from 280 to 2327, respectively (see Table 1 of Choi et al. (2015)).",6.1 Data,[0],[0]
"We copy the test set UAS results of Choi et al. (2015) and compute p−values using the data downloaded from http://amandastent.com/dependable/.
POS Tagging We consider a multilingual setup, analyzing the results reported in (Pinter et al., 2017).",6.1 Data,[0],[0]
"The authors compare their MIMICK model with the model of Ling et al. (2015), denoted with CHAR→TAG.",6.1 Data,[0],[0]
"Evaluation is performed on 23 of the 44 languages shared by the Polyglot word embedding dataset (Al-Rfou et al., 2013) and the universal dependencies (UD) dataset (De Marneffe et al., 2014).",6.1 Data,[0],[0]
"Pinter et al. (2017) choose their languages so that they reflect a variety of typological, and particularly morphological, properties.",6.1 Data,[0],[0]
The training/test split is the standard UD split.,6.1 Data,[0],[0]
"We copy the word level accuracy figures of Pinter et al. (2017) for the low resource training set setup, the focus setup of that paper.",6.1 Data,[0],[0]
"The authors kindly sent us their p-values.
",6.1 Data,[0],[0]
"Sentiment Classification In this task, an algorithm is trained on reviews from one domain and should classify the sentiment of reviews from another domain to the positive and negative classes.",6.1 Data,[0],[0]
For replicability analysis we explore the results of Ziser and Reichart (2017) for the cross-domain sentiment classification task of Blitzer et al. (2007).,6.1 Data,[0],[0]
"The data in this task consists of Amazon product reviews from 4 domains: books (B), DVDs (D), electronic items (E), and kitchen appliances (K), for the total of 12 domain pairs, each domain having a 2000 review test set.10 Ziser and Reichart (2017) compared the accuracy of their AE-SCL-SR model to MSDA (Chen et al., 2011), a well known domain adaptation
10http://www.cs.jhu.edu/˜mdredze/ datasets/sentiment
method, and kindly sent us the required p-values.
",6.1 Data,[0],[0]
Word Similarity We compare two state-of-the-art word embedding collections: (a) word2vec,6.1 Data,[0],[0]
"CBOW (Mikolov et al., 2013) vectors, generated by the model titled the best “predict” model in Baroni et al. (2014);11 and (b) GloVe (Pennington et al., 2014) vectors generated by a model trained on a 42B token common web crawl.12 We employed the demo of Faruqui and Dyer (2014) to perform a Spearman correlation evaluation of these vector collections on 12 English word pair datasets: WS-353 (Finkelstein et al., 2001b), WS-353-SIM (Agirre et al., 2009), WS-353-REL (Agirre et al., 2009), MC-30 (Miller and Charles, 1991), RG-65 (Rubenstein and Goodenough, 1965), Rare-Word (Luong et al., 2013), MEN (Bruni et al., 2012), MTurk-287 (Radinsky et al., 2011), MTurk-771",6.1 Data,[0],[0]
"(Halawi et al., 2012), YP-130 (Yang and Powers, ), SimLex-999 (Hill et al., 2016), and Verb-143 (Baker et al., 2014).",6.1 Data,[0],[0]
"We first calculate the p−values for each task and dataset according to the principals of p−values computation for NLP as discussed in Yeh (2000), BergKirkpatrick et al. (2012) and Søgaard et al. (2014).
",6.2 Statistical Significance Tests,[0],[0]
"For dependency parsing, we employ the aparametric paired bootstrap test (Efron and Tibshirani, 1994) that does not assume any distribution on the test statistics.",6.2 Statistical Significance Tests,[0],[0]
We chose this test because the distribution of the values for the measures commonly applied in this task is unknown.,6.2 Statistical Significance Tests,[0],[0]
"We implemented the test as in (Berg-Kirkpatrick et al., 2012) with a bootstrap size of 500 and with 105 repetitions.
",6.2 Statistical Significance Tests,[0],[0]
"For multilingual POS tagging, we employ the Wilcoxon signed-rank test (Wilcoxon, 1945) on the differences of the sentence level accuracy scores of the two compared models.",6.2 Statistical Significance Tests,[0],[0]
"This test is a nonparametric test for differences in measure, testing the null hypothesis that the difference has a symmetric distribution around zero.",6.2 Statistical Significance Tests,[0],[0]
"It is appropriate for tasks with paired continuous measures for each observation, which is the case when comparing sentence level accuracies.
",6.2 Statistical Significance Tests,[0],[0]
11http://clic.cimec.unitn.it/composes/ semantic-vectors.html.,6.2 Statistical Significance Tests,[0],[0]
"Parameters: 5-word context window, 10 negative samples, subsampling, 400 dimensions.
",6.2 Statistical Significance Tests,[0],[0]
"12http://nlp.stanford.edu/projects/glove/. 300 dimensions.
",6.2 Statistical Significance Tests,[0],[0]
"For sentiment classification we employ the McNemar test for paired nominal data (McNemar, 1947).",6.2 Statistical Significance Tests,[0],[0]
"This test is appropriate for binary classification tasks and since we compare the results of the algorithms when applied on the same datasets, we employ its paired version.",6.2 Statistical Significance Tests,[0],[0]
"Finally, for word similarity with its Spearman correlation evaluation, we choose the Steiger test (Steiger, 1980) for comparing elements in a correlation matrix.
",6.2 Statistical Significance Tests,[0],[0]
We consider the case of α = 0.05 for all four applications.,6.2 Statistical Significance Tests,[0],[0]
"For the dependent datasets experiments (sentiment classification and word similarity prediction) with their generally lower p−values (see below), we also consider the case where α = 0.01.",6.2 Statistical Significance Tests,[0],[0]
"Table 1 summarizes the replicability analysis results while Table 2 – 5 present task specific performance measures and p−values.
",6.3 Results,[0],[0]
"Independent Datasets Dependency parsing (Table 2) and multilingual POS tagging (Table 3) are our example tasks for this setup, where k̂Fisher is our recommended valid estimator for the number of cases where one algorithm outperforms another.
",6.3 Results,[0],[0]
"For dependency parsing, we compare two scenarios: (a) where in most domains the differences between the compared algorithms are quite large and the p−values are small (Mate vs. SpaCy); and (b)
where in most domains the differences between the compared algorithms are smaller and the p−values are higher (Mate vs. Redshift).",6.3 Results,[0],[0]
"Our multilingual POS tagging scenario (MIMICK vs. Char→Tag) is more similar to scenario (b) in terms of the differences between the participating algorithms.
",6.3 Results,[0],[0]
Table 1 demonstrates the k̂ estimators for the various tasks and scenarios.,6.3 Results,[0],[0]
"For dependency parsing, as expected, in scenario (a) where all the p−values are small, all estimators, even the error-prone k̂count, provide the same information.",6.3 Results,[0],[0]
"In case (b) of dependency parsing, however, k̂Fisher estimates the number of domains where Mate outperforms Redshift to be 5, while k̂count estimates this number to be 2.",6.3 Results,[0],[0]
This is a substantial difference given that the number of domains is 7.,6.3 Results,[0],[0]
"The k̂Bonferroni estimator, that is valid under arbitrary dependencies, is even more conservative than k̂count and its estimation is only 1.
",6.3 Results,[0],[0]
"Perhaps not surprisingly, the multilingual POS
tagging results are similar to case (b) of dependency parsing.",6.3 Results,[0],[0]
"Here, again, k̂count is too conservative, estimating the number of languages with effect to be 11 (out of 23) while k̂Fisher estimates this number to be 16 (an increase of 5/23 in the estimated number of languages with effect).",6.3 Results,[0],[0]
"k̂Bonferroni is again more conservative, estimating the number of languages with effect to be only 6, which is not very surprising given that it does not exploit the independence between the datasets.",6.3 Results,[0],[0]
"These two examples of case (b) demonstrate that when the differences between the algorithms are quite small, k̂Fisher may be more sensitive than the current practice in NLP for discovering the number of datasets with effect.
",6.3 Results,[0],[0]
"To complete the analysis, we would like to name the datasets with effect.",6.3 Results,[0],[0]
"As discussed in Section 4.2, while this can be straightforwardly done by naming the datasets with the k̂ smallest p−values, in general, this approach does not control the probability of identifying at least one dataset erroneously.",6.3 Results,[0],[0]
"We thus employ the Holm procedure for the identification task, noticing that the number of datasets it identifies should be equal to the value of the k̂Bonferroni estimator (Section 4.3).
",6.3 Results,[0],[0]
"Indeed, for dependency parsing in case (a), the Holm procedure identifies all seven domains as cases where Mate outperforms SpaCy, while in case (b) it identifies only the MZ domain as a case where Mate outperforms Redshift.",6.3 Results,[0],[0]
"For multilingual POS
tagging the Holm procedure identifies Tamil, Hungarian, Basque, Indonesian, Chinese and Czech as languages where MIMICK outperforms Char→Tag.",6.3 Results,[0],[0]
"This analysis demonstrates that when the performance gap between two algorithms becomes narrower, inquiring for more information (i.e. identifying the domains with effect rather than just estimating their number), may result in weaker results.13
Dependent Datasets In cross-domain sentiment classification (Table 4) and word similarity prediction (Table 5), the involved datasets manifest mutual dependence.",6.3 Results,[0],[0]
"Particularly, each sentiment setup shares its test dataset with 2 other setups, while in word similarity WS-353 is the union of WS-353REL and WS-353-SIM.",6.3 Results,[0],[0]
"As discussed in Section 4, k̂Bonferroni is the appropriate estimator of the number of cases one algorithm outperforms another.
",6.3 Results,[0],[0]
"The results in Table 1 manifest the phenomenon demonstrated by the second toy example in Section 5, which shows that when the datasets are dependent, k̂Fisher as well as the error-prone k̂count may be too optimistic regarding the number of datasets with effect.",6.3 Results,[0],[0]
"This stands in contrast to k̂Bonferroni which controls the probability to overestimate the number of such datasets.
",6.3 Results,[0],[0]
"Indeed, k̂Bonferroni is much more conservative, yielding values of 6 (α = 0.05) and 2 (α = 0.01) for sentiment, and of 6 (α = 0.05) and 4 (α = 0.01) for word similarity.",6.3 Results,[0],[0]
The differences from the conclusions that might have been drawn by k̂count are again quite substantial.,6.3 Results,[0],[0]
"The difference between k̂Bonferroni and k̂count in sentiment classification is 4, which accounts to 1/3 of the 12 test setups.",6.3 Results,[0],[0]
"Even for word similarity, the difference between the two methods, which account to 2 for both α values, represents 1/6 of the 12 test setups.",6.3 Results,[0],[0]
"The domains identified by the Holm procedure are marked in the tables.
",6.3 Results,[0],[0]
"Results Overview Our goal in this section is to demonstrate that the approach of simply looking at the number of datasets for which the difference between the performance of the algorithms reaches a predefined significance level, gives different results
13For completeness, we also performed the analysis for the independent dataset setups with α = 0.01.",6.3 Results,[0],[0]
"The results are (k̂count, k̂Bonferroni, k̂Fisher): Mate vs. SpaCy: (7,7,7); Mate vs. Redshift (1,0,2); MIMICK vs. Char→Tag: (7,5,13).",6.3 Results,[0],[0]
"The patterns are very similar to those discussed in the text.
from our suggested statistically sound analysis.",6.3 Results,[0],[0]
This approach is denoted here with k̂count and shown to be statistically not valid in Sections 3.2 and 5.,6.3 Results,[0],[0]
We observe that this happens especially in evaluation setups where the differences between the algorithms are small for most datasets.,6.3 Results,[0],[0]
"In some cases, when the datasets are independent, our analysis has the power to declare a larger number of datasets with effect than the number of individual significant test values (k̂count).",6.3 Results,[0],[0]
"In other cases, when the datasets are interdependent, k̂count is much too optimistic.
",6.3 Results,[0],[0]
Our proposed analysis changes the observations that might have been made based on the papers where the results analyzed here were originally reported.,6.3 Results,[0],[0]
"For example, for the Mate-Redshift comparison (independent evaluation sets), we show that there is evidence that the number of datasets with effect is much higher than one would assume based on counting the significant sets (5 vs. 2 out of 7 evaluation sets), giving a stronger claim regarding the superiority of Mate.",6.3 Results,[0],[0]
"In multilingual POS tagging (again, a setup of independent evaluation sets) our analysis shows evidence for 16 sets with effect compared to only 11 of the erroneous count method - a difference in 5 out of 23 evaluation sets (21.7%).",6.3 Results,[0],[0]
"Finally, in the cross-domain sentiment classification and the word similarity judgment tasks (dependent evaluation sets), the unjustified counting method may be too optimistic (e.g. 10 vs. 6 out of 12 evaluation sets, for α = 0.05 in the sentiment task), in favor of the new algorithms.",6.3 Results,[0],[0]
We proposed a statistically sound replicability analysis framework for cases where algorithms are compared across multiple datasets.,7 Discussion and Future Directions,[0],[0]
"Our main contributions are: (a) analyzing the limitations of the current practice in NLP work; and (b) proposing a framework that addresses both the estimation of the number of datasets with effect and their identification.
",7 Discussion and Future Directions,[0],[0]
The framework we propose addresses two different situations encountered in NLP: independent and dependent datasets.,7 Discussion and Future Directions,[0],[0]
"For dependent datasets, we assumed that the type of dependency cannot be determined.",7 Discussion and Future Directions,[0],[0]
One could use more powerful methods if certain assumptions on the dependency between the test statistics could be made.,7 Discussion and Future Directions,[0],[0]
"For example, one could use
the partial conjunction p-value based on Simes test for the global null hypothesis (Simes, 1986), which was proposed by Benjamini and Heller (2008) for the case where the test statistics satisfy certain positive dependency properties (see Theorem 1 in (Benjamini and Heller, 2008)).",7 Discussion and Future Directions,[0],[0]
"Using this partial conjunction p-value rather than the one based on Bonferroni, one may obtain higher values of k̂ with the same statistical guarantee.",7 Discussion and Future Directions,[0],[0]
"Similarly, for the identification question, if certain positive dependency properties hold, Holm’s procedure could be replaced by Hochberg’s or Hommel’s procedures (Hochberg, 1988; Hommel, 1988) which are more powerful.
",7 Discussion and Future Directions,[0],[0]
"An alternative, more powerful multiple testing procedure for identification of datasets with effect, is the method in Benjamini and Hochberg (1995), that controls the false discovery rate (FDR), a less strict error criterion than the one considered here.",7 Discussion and Future Directions,[0],[0]
"This method is more appropriate in cases where one may tolerate some errors as long as the proportion of errors among all the claims made is small, as expected to happen when the number of datasets grows.
",7 Discussion and Future Directions,[0],[0]
We note that the increase in the number of evaluation datasets may have positive and negative aspects.,7 Discussion and Future Directions,[0],[0]
"As noted in Section 2, we believe that multiple comparisons are integral to NLP research when aiming to develop algorithms that perform well across languages and domains.",7 Discussion and Future Directions,[0],[0]
"On the other hand, experimenting with multiple evaluation sets that reflect very similar linguistic phenomena may only complicate the comparison between alternative algorithms.
",7 Discussion and Future Directions,[0],[0]
"In fact, our analysis is useful mostly where the datasets are heterogeneous, coming from different languages or domains.",7 Discussion and Future Directions,[0],[0]
"When they are just technically different but could potentially be just combined into a one big dataset, then we believe the question of Demšar (2006), whether at least one dataset shows evidence for effect, is more appropriate.",7 Discussion and Future Directions,[0],[0]
The research of M. Bogomolov was supported by the Israel Science Foundation grant No. 1112/14.,Acknowledgement,[0],[0]
We thank Yuval Pinter for his great help with the multilingual experiments and for his useful feedback.,Acknowledgement,[0],[0]
"We also thank Ruth Heller, Marten van Schijndel, Oren Tsur, Or Zuk and the ie@technion NLP group members for their useful comments.",Acknowledgement,[0],[0]
"With the ever growing amount of textual data from a large variety of languages, domains, and genres, it has become standard to evaluate NLP algorithms on multiple datasets in order to ensure a consistent performance across heterogeneous setups.",abstractText,[0],[0]
"However, such multiple comparisons pose significant challenges to traditional statistical analysis methods in NLP and can lead to erroneous conclusions.",abstractText,[0],[0]
In this paper we propose a Replicability Analysis framework for a statistically sound analysis of multiple comparisons between algorithms for NLP tasks.,abstractText,[0],[0]
"We discuss the theoretical advantages of this framework over the current, statistically unjustified, practice in the NLP literature, and demonstrate its empirical value across four applications: multi-domain dependency parsing, multilingual POS tagging, cross-domain sentiment classification and word similarity prediction.",abstractText,[0],[0]
1,abstractText,[0],[0]
Replicability Analysis for Natural Language Processing: Testing Significance with Multiple Datasets,title,[0],[0]
Graphs are a ubiquitous structure that widely occurs in data analysis problems.,1. Introduction,[0],[0]
"Real-world graphs such as social networks, financial networks, biological networks and citation networks represent important rich information which is not seen from the individual entities alone, for example, the communities a person is in, the functional role of a molecule, and the sensitivity of the assets of an enterprise to external shocks.",1. Introduction,[0],[0]
"Therefore, representation learning of nodes in graphs aims to extract high-level features from a node as well as its neighborhood, and has proved extremely useful for many applications, such as node classification, clustering, and link prediction (Perozzi et al., 2014; Monti et al.,
1Massachusetts Institute of Technology (MIT) 2National Institute of Informatics, Tokyo.",1. Introduction,[0],[0]
Correspondence to: Keyulu Xu,1. Introduction,[0],[0]
"<keyulu@mit.edu>, Stefanie Jegelka <stefje@mit.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
2017; Grover & Leskovec, 2016; Tang et al., 2015).
",1. Introduction,[0],[0]
Recent works focus on deep learning approaches to node representation.,1. Introduction,[0],[0]
"Many of these approaches broadly follow a neighborhood aggregation (or “message passing” scheme), and those have been very promising (Kipf & Welling, 2017; Hamilton et al., 2017; Gilmer et al., 2017; Veličković",1. Introduction,[0],[0]
"et al., 2018; Kearnes et al., 2016).",1. Introduction,[0],[0]
"These models learn to iteratively aggregate the hidden features of every node in the graph with its adjacent nodes’ as its new hidden features, where an iteration is parametrized by a layer of the neural network.",1. Introduction,[0],[0]
"Theoretically, an aggregation process of k iterations makes use of the subtree structures of height k rooted at every node.",1. Introduction,[0],[0]
"Such schemes have been shown to generalize the Weisfeiler-Lehman graph isomorphism test (Weisfeiler & Lehman, 1968) enabling to simultaneously learn the topology as well as the distribution of node features in the neighborhood (Shervashidze et al., 2011; Kipf & Welling, 2017; Hamilton et al., 2017).
",1. Introduction,[0],[0]
"Yet, such aggregation schemes sometimes lead to surprises.",1. Introduction,[0],[0]
"For example, it has been observed that the best performance with one of the state-of-the-art models, Graph Convolutional Networks (GCN), is achieved with a 2-layer model.",1. Introduction,[0],[0]
"Deeper versions of the model that, in principle, have access to more information, perform worse (Kipf & Welling, 2017).",1. Introduction,[0],[0]
"A similar degradation of learning for computer vision problems is resolved by residual connections (He et al., 2016a) that greatly aid the training of deep models.",1. Introduction,[0],[0]
"But, even with residual connections, GCNs with more layers do not perform as well as the 2-layer GCN on many datasets, e.g. citation networks.
",1. Introduction,[0],[0]
"Motivated by observations like the above, in this paper, we address two questions.",1. Introduction,[0],[0]
"First, we study properties and resulting limitations of neighborhood aggregation schemes.",1. Introduction,[0],[0]
"Second, based on this analysis, we propose an architecture that, as opposed to existing models, enables adaptive, structure-aware representations.",1. Introduction,[0],[0]
"Such representations are particularly interesting for representation learning on large complex graphs with diverse subgraph structures.
",1. Introduction,[0],[0]
Model analysis.,1. Introduction,[0],[0]
"To better understand the behavior of different neighborhood aggregation schemes, we analyze the effective range of nodes that any given node’s representation draws from.",1. Introduction,[0],[0]
"We summarize this sensitivity analysis by what
we name the influence distribution of a node.",1. Introduction,[0],[0]
This effective range implicitly encodes prior assumptions on what are the “nearest neighbors” that a node should draw information from.,1. Introduction,[0],[0]
"In particular, we will see that this influence is heavily affected by the graph structure, raising the question whether “one size fits all”, in particular in graphs whose subgraphs have varying properties (such as more tree-like or more expander-like).
",1. Introduction,[0],[0]
"In particular, our more formal analysis connects influence distributions with the spread of a random walk at a given node, a well-understood phenomenon as a function of the graph structure and eigenvalues (Lovász, 1993).",1. Introduction,[0],[0]
"For instance, in some cases and applications, a 2-step random walk influence that focuses on local neighborhoods can be more informative than higher-order features where some of the information may be “washed out” via averaging.
",1. Introduction,[0],[0]
Changing locality.,1. Introduction,[0],[0]
"To illustrate the effect and importance of graph structure, recall that many real-world graphs possess locally strongly varying structure.",1. Introduction,[0],[0]
"In biological and citation networks, the majority of the nodes have few connections, whereas some nodes (hubs) are connected to many other nodes.",1. Introduction,[0],[0]
"Social and web networks usually consist of an expander-like core part and an almost-tree (bounded treewidth) part, which represent well-connected entities and the small communities respectively (Leskovec et al., 2009; Maehara et al., 2014; Tsonis et al., 2006).
",1. Introduction,[0],[0]
"Besides node features, this subgraph structure has great impact on the result of neighborhood aggregation.",1. Introduction,[0],[0]
"The speed of expansion or, equivalently, growth of the influence radius, is characterized by the random walk’s mixing time, which changes dramatically on subgraphs with different structures (Lovász, 1993).",1. Introduction,[0],[0]
"Thus, the same number of iterations (layers) can lead to influence distributions of very different locality.",1. Introduction,[0],[0]
"As an example, consider the social network in Figure 1 from GooglePlus (Leskovec & Mcauley, 2012).",1. Introduction,[0],[0]
The figure illustrates the expansions of a random walk starting at the square node.,1. Introduction,[0],[0]
The walk (a) from a node within the core rapidly includes almost the entire graph.,1. Introduction,[0],[0]
"In contrast, the walk (b) starting at a node in the tree part includes only a very small fraction of all nodes.",1. Introduction,[0],[0]
"After 5 steps, the same walk has reached the core and, suddenly, spreads quickly.",1. Introduction,[0],[0]
"Translated to graph representation models, these spreads become the influence distributions or, in other words, the averaged features yield the new feature of the walk’s starting node.",1. Introduction,[0],[0]
"This shows that in the same graph, the same number of steps can lead to very different effects.",1. Introduction,[0],[0]
"Depending on the application, wide-range or smallrange feature combinations may be more desirable.",1. Introduction,[0],[0]
"A too rapid expansion may average too broadly and thereby lose information, while in other parts of the graph, a sufficient neighborhood may be needed for stabilizing predictions.
",1. Introduction,[0],[0]
JK networks.,1. Introduction,[0],[0]
"The above observations raise the question
whether it is possible to adaptively adjust (i.e., learn) the influence radii for each node and task.",1. Introduction,[0],[0]
"To achieve this, we explore an architecture that learns to selectively exploit information from neighborhoods of differing locality.",1. Introduction,[0],[0]
"This architecture selectively combines different aggregations at the last layer, i.e., the representations “jump” to the last layer.",1. Introduction,[0],[0]
"Hence, we name the resulting networks Jumping Knowledge Networks (JK-Nets).",1. Introduction,[0],[0]
"We will see that empirically, when adaptation is an option, the networks indeed learn representations of different orders for different graph substructures.",1. Introduction,[0],[0]
"Moreover, in Section 6, we show that applying our framework to various state-of-the-art neighborhood-aggregation models consistently improves their performance.",1. Introduction,[0],[0]
"We begin by summarizing some of the most common neighborhood aggregation schemes and, along the way, introduce our notation.",2. Background and Neighborhood aggregation schemes,[0],[0]
"Let G = (V,E) be a simple graph with node features Xv ∈",2. Background and Neighborhood aggregation schemes,[0],[0]
Rdi for v ∈ V .,2. Background and Neighborhood aggregation schemes,[0],[0]
Let G̃ be the graph obtained by adding a self-loop to every v ∈ V .,2. Background and Neighborhood aggregation schemes,[0],[0]
The hidden feature of node v learned by the l-th layer of the model is denoted by h (l) v ∈,2. Background and Neighborhood aggregation schemes,[0],[0]
Rdh .,2. Background and Neighborhood aggregation schemes,[0],[0]
"Here, di is the dimension of the input features and dh is the dimension of the hidden features, which, for simplicity of exposition, we assume to be the same across layers.",2. Background and Neighborhood aggregation schemes,[0],[0]
We also use h(0)v = Xv for the node feature.,2. Background and Neighborhood aggregation schemes,[0],[0]
"The neighborhood N(v) = {u ∈ V | (v, u) ∈ E} of node v is the set of adjacent nodes of v. The analogous neighborhood Ñ(v) = {v} ∪ {u ∈ V | (v, u) ∈ E} on G̃ includes v.
A typical neighborhood aggregation scheme can generically be written as follows: for a k-layer model, the l-th layer (l = 1..k) updates h(l)v for every v ∈ V simultaneously as
h(l)v = σ",2. Background and Neighborhood aggregation schemes,[0],[0]
"( Wl · AGGREGATE ({ h(l−1)u ,∀u ∈ Ñ(v) }))",2. Background and Neighborhood aggregation schemes,[0],[0]
"(1)
where AGGREGATE is an aggregation function defined by the specific model, Wl is a trainable weight matrix on the lth layer shared by all nodes, and σ is a non-linear activation function, e.g. a ReLU.
",2. Background and Neighborhood aggregation schemes,[0],[0]
Graph Convolutional Networks (GCN).,2. Background and Neighborhood aggregation schemes,[0],[0]
"Graph Convolutional Networks (GCN) (Kipf & Welling, 2017), initially motivated by spectral graph convolutions (Hammond et al., 2011; Defferrard et al., 2016), are a specific instantiation of this framework (Gilmer et al., 2017), of the form
h(l)v = ReLU ( Wl · ∑ u∈Ñ(v) (deg(v)deg(u))−1/2 h(l−1)u ) (2)
where deg(v) is the degree of node v in G. Hamilton et al. (2017) derived a variant of GCN that also works in inductive settings (previously unseen nodes), by using a different normalization to average:
h(l)v",2. Background and Neighborhood aggregation schemes,[0],[0]
"= ReLU ( Wl · 1
d̃eg(v) ∑ u∈Ñ(v) h(l−1)u ) (3)
where d̃eg(v) is the degree of node v in G̃.
Neighborhood Aggregation with Skip Connections.",2. Background and Neighborhood aggregation schemes,[0],[0]
Instead of aggregating a node and its neighbors at the same time as in Eqn.,2. Background and Neighborhood aggregation schemes,[0],[0]
"(1), a number of recent approaches aggregate the neighbors first and then combine the resulting neighborhood representation with the node’s representation from the last iteration.",2. Background and Neighborhood aggregation schemes,[0],[0]
"More formally, each node is updated as
h (l) N(v) = σ",2. Background and Neighborhood aggregation schemes,[0],[0]
"( Wl · AGGREGATEN ( {h(l−1)u ,∀u ∈ N(v)} )) h(l)v =",2. Background and Neighborhood aggregation schemes,[0],[0]
"COMBINE ( h(l−1)v , h (l) N(v)
) where AGGREGATEN and COMBINE are defined by the specific model.",2. Background and Neighborhood aggregation schemes,[0],[0]
The COMBINE step is key to this paradigm and can be viewed as a form of a ”skip connection” between different layers.,2. Background and Neighborhood aggregation schemes,[0],[0]
"For COMBINE, GraphSAGE (Hamilton et al., 2017) uses concatenation after a feature transform.",2. Background and Neighborhood aggregation schemes,[0],[0]
"Column Networks (Pham et al., 2017) interpolate the neighborhood representation and the node’s previous representation, and Gated GNN (Li et al., 2016) uses the Gated Recurrent Unit (GRU) (Cho et al., 2014).",2. Background and Neighborhood aggregation schemes,[0],[0]
"Another wellknown variant of skip connections, residual connections, use the identity mapping to help signals propagate (He et al., 2016a;b).
",2. Background and Neighborhood aggregation schemes,[0],[0]
"These skip connections are input- but not output-unit specific: If we ”skip” a layer for h(l)v (do not aggregate) or use a certain COMBINE, all subsequent units using this representation will be using this skip implicitly.",2. Background and Neighborhood aggregation schemes,[0],[0]
It is impossible that a certain higher-up representation h(l+j)u uses the skip and another one does not.,2. Background and Neighborhood aggregation schemes,[0],[0]
"As a result, skip connections cannot adaptively adjust the neighborhood sizes of the final-layer representations independently.
",2. Background and Neighborhood aggregation schemes,[0],[0]
Neighborhood Aggregation with Directional Biases.,2. Background and Neighborhood aggregation schemes,[0],[0]
"Some recent models, rather than treating the features of
adjacent nodes equally, weigh “important” neighbors more.",2. Background and Neighborhood aggregation schemes,[0],[0]
"This paradigm can be viewed as neighborhood-aggregation with directional biases because a node will be influenced by some directions of expansion more than the others.
",2. Background and Neighborhood aggregation schemes,[0],[0]
"Graph Attention Networks (GAT) (Veličković et al., 2018) and VAIN (Hoshen, 2017) learn to select the important neighbors via an attention mechanism.",2. Background and Neighborhood aggregation schemes,[0],[0]
"The max-pooling operation in GraphSAGE (Hamilton et al., 2017) implicitly selects the important nodes.",2. Background and Neighborhood aggregation schemes,[0],[0]
"This line of work is orthogonal to ours, because it modifies the direction of expansion whereas our model operates on the locality of expansion.",2. Background and Neighborhood aggregation schemes,[0],[0]
Our model can be combined with these models to add representational power.,2. Background and Neighborhood aggregation schemes,[0],[0]
"In Section 6, we demonstrate that our framework works with not only simple neighborhood-aggregation models (GCN), but also with skip connections (GraphSAGE) and directional biases (GAT).",2. Background and Neighborhood aggregation schemes,[0],[0]
"Next, we explore some important properties of the above aggregation schemes.",3. Influence Distribution and Random Walks,[0],[0]
"Related to ideas of sensitivity analysis and influence functions in statistics (Koh & Liang, 2017) that measure the influence of a training point on parameters, we study the range of nodes whose features affect a given node’s representation.",3. Influence Distribution and Random Walks,[0],[0]
"This range gives insight into how large a neighborhood a node is drawing information from.
",3. Influence Distribution and Random Walks,[0],[0]
"We measure the sensitivity of node x to node y, or the influence of y on x, by measuring how much a change in the input feature of y affects the representation of x in the last layer.",3. Influence Distribution and Random Walks,[0],[0]
"For any node x, the influence distribution captures the relative influences of all other nodes.
",3. Influence Distribution and Random Walks,[0],[0]
Definition 3.1 (Influence score and distribution).,3. Influence Distribution and Random Walks,[0],[0]
"For a simple graph G = (V,E), let h(0)x be the input feature and h
(k) x be the learned hidden feature of node x ∈ V at the k-th (last) layer of the model.",3. Influence Distribution and Random Walks,[0],[0]
"The influence score I(x, y) of node x by any node y ∈ V is the sum of the absolute values
of the entries of the Jacobian matrix [ ∂h(k)x ∂h (0) y ] .",3. Influence Distribution and Random Walks,[0],[0]
"We define the influence distribution Ix of x ∈ V by normalizing the influence scores: Ix(y) = I(x, y)/ ∑ z I(x, z), or
Ix(y) = e T
[ ∂h (k) x
∂h (0) y
] e /(∑
z∈V eT
[ ∂h (k) x
∂h (0) z
] e )
where e is the all-ones vector.
",3. Influence Distribution and Random Walks,[0],[0]
"Later, we will see connections of influence distributions with random walks.",3. Influence Distribution and Random Walks,[0],[0]
"For completeness, we also define random walk distributions.
",3. Influence Distribution and Random Walks,[0],[0]
Definition 3.2.,3. Influence Distribution and Random Walks,[0],[0]
"Consider a random walk on G̃ starting at a node v0; if at the t-th step we are at a node vt, we move to any neighbor of vt (including vt) with equal probability.
",3. Influence Distribution and Random Walks,[0],[0]
"The t-step random walk distribution Pt of v0 is
Pt (i) =",3. Influence Distribution and Random Walks,[0],[0]
Prob (vt = i) .,3. Influence Distribution and Random Walks,[0],[0]
"(4)
Analogous definitions apply for random walks with nonuniform transition probabilities.
",3. Influence Distribution and Random Walks,[0],[0]
An important property of the random walk distribution is that it becomes more spread out as t increases and converges to the limit distribution if the graph is non-bipartite.,3. Influence Distribution and Random Walks,[0],[0]
"The rate of convergence depends on the structure of the subgraph and can be bounded by the spectral gap (or the conductance) of the random walk’s transition matrix (Lovász, 1993).",3. Influence Distribution and Random Walks,[0],[0]
The influence distribution for different aggregation models and nodes can give insights into the information captured by the respective representations.,3.1. Model Analysis,[0],[0]
The following results show that the influence distributions of common aggregation schemes are closely connected to random walk distributions.,3.1. Model Analysis,[0],[0]
"This observation hints at specific implications – strengths and weaknesses – that we will discuss.
",3.1. Model Analysis,[0],[0]
"With a randomization assumption of the ReLU activations similar to that in (Kawaguchi, 2016; Choromanska et al., 2015), we can draw connections between GCNs and random walks:
Theorem 1.",3.1. Model Analysis,[0],[0]
"Given a k-layer GCN with averaging as in Equation (3), assume that all paths in the computation graph of the model are activated with the same probability of success ρ.",3.1. Model Analysis,[0],[0]
"Then the influence distribution Ix for any node
x ∈ V is equivalent, in expectation, to the k-step random walk distribution on G̃ starting at node x.
We prove Theorem 1 in the appendix.
",3.1. Model Analysis,[0],[0]
It is straightforward to modify the proof of Theorem 1 to show a nearly equivalent result for the version of GCN in Equation (2).,3.1. Model Analysis,[0],[0]
"The only difference is that each random walk path v0p, v 1 p, ..., v k p from node x (v 0 p) to y (v k p), in-
stead of probability ρ ∏k l=1 1
d̃eg(vlp) , now has probability
ρ Q ∏k−1 l=1 1
d̃eg(vlp) · (d̃eg(x)d̃eg(y))−1/2, where Q is a nor-
malizing factor.",3.1. Model Analysis,[0],[0]
"Thus, the difference in probability is small, especially when the degree of x and y are close.
",3.1. Model Analysis,[0],[0]
"Similarly, we can show that neighborhood aggregation schemes with directional biases resemble biased random walk distributions.",3.1. Model Analysis,[0],[0]
"This follows by substituting the corresponding probabilities into the proof of Theorem 1.
",3.1. Model Analysis,[0],[0]
"Empirically, we observe that, despite somewhat simplifying assumptions, our theory is close to what happens in practice.",3.1. Model Analysis,[0],[0]
"We visualize the heat maps of the influence distributions for a node (labeled square) for trained GCNs, and compare with the random walk distributions starting at the same node.",3.1. Model Analysis,[0],[0]
Figure 2 shows example results.,3.1. Model Analysis,[0],[0]
Darker colors correspond to higher influence probabilities.,3.1. Model Analysis,[0],[0]
"To show the effect of skip connections, Figure 3 visualizes the analogous heat maps for one example—GCN with residual connections.",3.1. Model Analysis,[0],[0]
"Indeed, we observe that the influence distributions of networks with residual connections approximately correspond to lazy random walks: each step has a higher probability of staying at
the current node.",3.1. Model Analysis,[0],[0]
Local information is retained with similar probabilities for all nodes in each iteration; this cannot adapt to diverse needs of specific upper-layer nodes.,3.1. Model Analysis,[0],[0]
"Further visualizations may be found in the appendix.
",3.1. Model Analysis,[0],[0]
Fast Collapse on Expanders.,3.1. Model Analysis,[0],[0]
"To better understand the implication of Theorem 1 and the limitations of the corresponding neighborhood aggregation algorithms, we revisit the scenario of learning on a social network shown in Figure 1.",3.1. Model Analysis,[0],[0]
"Random walks starting inside an expander converge rapidly in O(log |V |) steps to an almost-uniform distribution (Hoory et al., 2006).",3.1. Model Analysis,[0],[0]
"After O(log |V |) iterations of neighborhood aggregation, by Theorem 1 the representation of every node is influenced almost equally by any other node in the expander.",3.1. Model Analysis,[0],[0]
"Thus, the node representations will be representative of the global graph and carry limited information about individual nodes.",3.1. Model Analysis,[0],[0]
"In contrast, random walks starting at the bounded tree-width (almost-tree) part converge slowly, i.e., the features retain more local information.",3.1. Model Analysis,[0],[0]
"Models that impose a fixed random walk distribution inherit these discrepancies in the speed of expansion and influence neighborhoods, which may not lead to the best representations for all nodes.",3.1. Model Analysis,[0],[0]
"The above observations raise the question whether the fixed but structure-dependent influence radius size induced by
common aggregation schemes really achieves the best representations for all nodes and tasks.",4. Jumping Knowledge Networks,[0],[0]
"Large radii may lead to too much averaging, while small radii may lead to instabilities or insufficient information aggregation.",4. Jumping Knowledge Networks,[0],[0]
"Hence, we propose two simple yet powerful architectural changes – jump connections and a subsequent selective but adaptive aggregation mechanism.
",4. Jumping Knowledge Networks,[0],[0]
"Figure 4 illustrates the main idea: as in common neighborhood aggregation networks, each layer increases the size of the influence distribution by aggregating neighborhoods from the previous layer.",4. Jumping Knowledge Networks,[0],[0]
"At the last layer, for each node, we carefully select from all of those itermediate representations (which “jump” to the last layer), potentially combining a few.",4. Jumping Knowledge Networks,[0],[0]
"If this is done independently for each node, then the model can adapt the effective neighborhood size for each node as needed, resulting in exactly the desired adaptivity.
",4. Jumping Knowledge Networks,[0],[0]
Our model permits general layer-aggregation mechanisms.,4. Jumping Knowledge Networks,[0],[0]
We explore three approaches; others are possible too.,4. Jumping Knowledge Networks,[0],[0]
"Let h (1) v , ..., h (k) v be the jumping representations of node v (from k layers) that are to be aggregated.
",4. Jumping Knowledge Networks,[0],[0]
Concatenation.,4. Jumping Knowledge Networks,[0],[0]
"A concatenation [ h (1) v , ..., h (k) v ] is the
most straightforward way to combine the layers, after which we may perform a linear transformation.",4. Jumping Knowledge Networks,[0],[0]
"If the transformation weights are shared across graph nodes, this approach is not node-adaptive.",4. Jumping Knowledge Networks,[0],[0]
"Instead, it optimizes the weights to combine the subgraph features in a way that works best for the dataset overall.",4. Jumping Knowledge Networks,[0],[0]
"One may expect concatenation to be suitable for small graphs and graphs with regular structure that require less adaptivity; also because weight-sharing helps reduce overfitting.
",4. Jumping Knowledge Networks,[0],[0]
Max-pooling.,4. Jumping Knowledge Networks,[0],[0]
"An element-wise max ( h (1) v , ..., h (k) v ) selects the most informative layer for each feature coordinate.",4. Jumping Knowledge Networks,[0],[0]
"For example, feature coordinates that represent more local properties can use the feature coordinates learned from the close neighbors and those representing global status would favor features from the higher-up layers.",4. Jumping Knowledge Networks,[0],[0]
"Max-pooling is adaptive and has the advantage that it does not introduce any additional parameters to learn.
",4. Jumping Knowledge Networks,[0],[0]
LSTM-attention.,4. Jumping Knowledge Networks,[0],[0]
"An attention mechanism identifies the most useful neighborhood ranges for each node v by computing an attention score s(l)v for each layer l (∑ l s (l) v = 1 ) , which represents the importance of the feature learned on the l-th layer for node v. The aggregated representation for node v is a weighted average of the layer features∑ l s (l) v · h(l)v .",4. Jumping Knowledge Networks,[0],[0]
"For LSTM attention, we input h(1)v , ..., h(k)v into a bi-directional LSTM (Hochreiter & Schmidhuber, 1997) and generate the forward-LSTM and backward-LSTM",4. Jumping Knowledge Networks,[0],[0]
hidden features f (l)v and b (l) v for each layer l.,4. Jumping Knowledge Networks,[0],[0]
A linear mapping of the concatenated features [f (l)v ||b(l)v ] yields the scalar importance score s(l)v .,4. Jumping Knowledge Networks,[0],[0]
"A Softmax layer applied to {s(l)v }kl=1
yields the attention of node v on its neighborhood in different ranges.",4. Jumping Knowledge Networks,[0],[0]
Finally we take the sum of [f (l)v ||b(l)v ] weighted by SoftMax({s(l)v }kl=1) to get the final layer representation.,4. Jumping Knowledge Networks,[0],[0]
Another possible implementation combines LSTM with max-pooling.,4. Jumping Knowledge Networks,[0],[0]
LSTM-attention is node adaptive because the attention scores are different for each node.,4. Jumping Knowledge Networks,[0],[0]
"We shall see that the this approach shines on large complex graphs, although it may overfit on small graphs (fewer training nodes) due to its relatively higher complexity.",4. Jumping Knowledge Networks,[0],[0]
"The key idea for the design of layer-aggregation functions is to determine the importance of a node’s subgraph features at different ranges after looking at the learned features on all layers, rather than to optimize and fix the same weights for all nodes.",4.1. JK-Net Learns to Adapt,[0],[0]
"Under the same assumption on the ReLU activation distribution as in Theorem 1, we show below that layer-wise max-pooling implicitly learns the influence locality adaptively for different nodes.",4.1. JK-Net Learns to Adapt,[0],[0]
"The proof for layerwise attention follows similarly.
",4.1. JK-Net Learns to Adapt,[0],[0]
Proposition 1.,4.1. JK-Net Learns to Adapt,[0],[0]
Assume that paths of the same length in the computation graph are activated with the same probability.,4.1. JK-Net Learns to Adapt,[0],[0]
"The influence score I(x, y) for any x, y ∈ V under a k-layer JK-Net with layer-wise max-pooling is equivalent in expectation to a mixture of 0, .., k-step random walk distributions on G̃ at y starting at x, the coefficients of which depend on the values of the layer features h(l)x .
",4.1. JK-Net Learns to Adapt,[0],[0]
We prove Proposition 1 in the appendix.,4.1. JK-Net Learns to Adapt,[0],[0]
"Contrasting this result with the influence distributions of other aggregation mechanisms, we see that JK-networks indeed differ in their node-wise adaptivity of neighborhood ranges.
",4.1. JK-Net Learns to Adapt,[0],[0]
Figure 5 illustrates how a 6-layer JK-Net with max-pooling aggregation learns to adapt to different subgraph structures on a citation network.,4.1. JK-Net Learns to Adapt,[0],[0]
"Within a tree-like structure, the influence stays in the “small community” the node belongs to.",4.1. JK-Net Learns to Adapt,[0],[0]
"In contrast, 6-layer models whose influence distributions follow random walks, e.g. GCNs, would reach out too far into irrelevant parts of the graph, and models with few layers may not be able to cover the entire “community”, as illustrated in Figure 1, and Figures 7, 8 in the appendix.",4.1. JK-Net Learns to Adapt,[0],[0]
"For
a node affiliated to a “hub”, which presumably plays the role of connecting different types of nodes, JK-Net learns to put most influence on the node itself and otherwise spreads out the influence.",4.1. JK-Net Learns to Adapt,[0],[0]
"GCNs, however, would not capture the importance of the node’s own features in such a structure because the probability at an affiliate node is small after a few random walk steps.",4.1. JK-Net Learns to Adapt,[0],[0]
"For hubs, JK-Net spreads out the influence across the neighboring nodes in a reasonable range, which makes sense because the nodes connected to the hubs are presumably as informative as the hubs’ own features.",4.1. JK-Net Learns to Adapt,[0],[0]
"For comparison, Table 6 in the appendix includes more visualizations of how models with random walk priors behave.",4.1. JK-Net Learns to Adapt,[0],[0]
"Looking at Figure 4, one may wonder whether the same inter-layer connections could be drawn between all layers.",4.2. Intermediate Layer Aggregation and Structures,[0],[0]
"The resulting architecture is approximately a graph correspondent of DenseNets, which were introduced for computer vision problems (Huang et al., 2017), if the layer-wise concatenation aggregation is applied.",4.2. Intermediate Layer Aggregation and Structures,[0],[0]
"This version, however, would require many more features to learn.",4.2. Intermediate Layer Aggregation and Structures,[0],[0]
"Viewing the DenseNet setting (images) from a graph-theoretic perspective, images correspond to regular, in fact, near-planar graphs.",4.2. Intermediate Layer Aggregation and Structures,[0],[0]
"Such graphs are far from being expanders, and do not pose the challenges of graphs with varying subgraph structures.",4.2. Intermediate Layer Aggregation and Structures,[0],[0]
"Indeed, as we shall see, models with concatenation aggregation perform well on graphs with more regular structures such as images and well-structured communities.",4.2. Intermediate Layer Aggregation and Structures,[0],[0]
"As a more general framework, JK-Net admits general layerwise aggregation models and enables better structure-aware representations on graphs with complex structures.",4.2. Intermediate Layer Aggregation and Structures,[0],[0]
"Spectral graph convolutional neural networks apply convolution on graphs by using the graph Laplacian eigenvectors as the Fourier atoms (Bruna et al., 2014; Shuman et al., 2013; Defferrard et al., 2016).",5. Other Related Work,[0],[0]
"A major drawback of the spectral methods, compared to spatial approaches like neighborhoodaggregation, is that the graph Laplacian needs to be known in advance.",5. Other Related Work,[0],[0]
"Hence, they cannot generalize to unseen graphs.",5. Other Related Work,[0],[0]
We evaluate JK-Nets on four benchmark datasets.,6. Experiments,[0],[0]
(I),6. Experiments,[0],[0]
"The task on citation networks (Citeseer, Cora) (Sen et al., 2008) is to classify academic papers into different subjects.",6. Experiments,[0],[0]
The dataset contains bag-of-words features for each document (node) and citation links (edges) between documents.,6. Experiments,[0],[0]
"(II) On Reddit (Hamilton et al., 2017), the task is to predict the community to which different Reddit posts belong.",6. Experiments,[0],[0]
Reddit is an online discussion forum where users comment in different topical communities.,6. Experiments,[0],[0]
Two posts (nodes) are connected if some user commented on both posts.,6. Experiments,[0],[0]
The dataset contains word vectors as node features.,6. Experiments,[0],[0]
"(III) For protein-protein interaction networks (PPI) (Hamilton et al., 2017), the task is to classify protein functions.",6. Experiments,[0],[0]
"PPI consists of 24 graphs, each corresponds to a human tissue.",6. Experiments,[0],[0]
"Each node has positional gene sets, motif gene sets and immunological signatures as features and gene ontology sets as labels.",6. Experiments,[0],[0]
"20 graphs are used for training, 2 graphs are used for validation and the rest for testing.",6. Experiments,[0],[0]
"Statistics of the datasets are summarized in Table 1.
",6. Experiments,[0],[0]
Settings.,6. Experiments,[0],[0]
"In the transductive setting, we are only allowed to access a subset of nodes in one graph as training data, and validate/test on others.",6. Experiments,[0],[0]
"Our experiments on Citeseer, Cora and Reddit are transductive.",6. Experiments,[0],[0]
"In the inductive setting, we use a number of full graphs as training data and use other completely unseen graphs as validation/testing data.",6. Experiments,[0],[0]
"Our experiments on PPI are inductive.
",6. Experiments,[0],[0]
"We compare against three baselines: Graph Convolutional Networks (GCN) (Kipf & Welling, 2017), GraphSAGE (Hamilton et al., 2017) and Graph Attention Networks (GAT) (Veličković et al., 2018).",6. Experiments,[0],[0]
"For experiments on Citeseer and Cora, we choose GCN as the base model since on our data split, it is outperforming GAT.",6.1. Citeseer & Cora,[0],[0]
"We construct JK-Nets by choosing MaxPooling (JKMaxPool), Concatenation (JK-Concat), or LSTM-attention (JK-LSTM) as final aggregation layer.",6.1. Citeseer & Cora,[0],[0]
"When taking the final aggregation, besides normal graph convolutional layers, we also take the first linear-transformed representation into account.",6.1. Citeseer & Cora,[0],[0]
The final prediction is done via a fully connected layer on top of the final aggregated representation.,6.1. Citeseer & Cora,[0],[0]
"We split nodes in each graph into 60%, 20% and 20% for training, validation and testing.",6.1. Citeseer & Cora,[0],[0]
"We vary the number of layers from 1
to 6 for each model and choose the best performing model with respect to the validation set.",6.1. Citeseer & Cora,[0],[0]
"Throughout the experiments, we use the Adam optimizer (Kingma & Ba, 2014) with learning rate 0.005.",6.1. Citeseer & Cora,[0],[0]
"We fix the dropout rate to be 0.5, the dimension of hidden features to be within {16, 32}, and add an L2 regularization of 0.0005 on model parameters.",6.1. Citeseer & Cora,[0],[0]
"The results are shown in Table 2.
Results.",6.1. Citeseer & Cora,[0],[0]
We observe in Table 2 that JK-Nets outperform both GCN and GAT baselines in terms of prediction accuracy.,6.1. Citeseer & Cora,[0],[0]
"Though JK-Nets perform well in general, there is no consistent winner and performance varies slightly across datasets.
",6.1. Citeseer & Cora,[0],[0]
"Taking a closer look at results on Cora, both GCN and GAT achieve their best accuracies with only 2 or 3 layers, suggesting that local information is a stronger signal for classification than global ones.",6.1. Citeseer & Cora,[0],[0]
"However, the fact that JKNets achieve the best performance with 6 layers indicates that global together with local information will help boost performance.",6.1. Citeseer & Cora,[0],[0]
This is where models like JK-Nets can be particularly beneficial.,6.1. Citeseer & Cora,[0],[0]
LSTM-attention may not be suitable for such small graphs because of its relatively high complexity.,6.1. Citeseer & Cora,[0],[0]
The Reddit data is too large to be handled well by current implementations of GCN or GAT.,6.2. Reddit,[0],[0]
"Hence, we use the more scalable GraphSAGE as the base model for JK-Net.",6.2. Reddit,[0],[0]
It has skip connections and different modes of node aggregation.,6.2. Reddit,[0],[0]
"We experiment with Mean and MaxPool node aggregators, which take mean and max-pooling of a linear transformation of representations of the sampled neighbors.",6.2. Reddit,[0],[0]
"Combining each of GraphSAGE modes with MaxPooling, Concatenation or LSTM-attention as the last aggregation layer gives 6 JK-Net variants.",6.2. Reddit,[0],[0]
"We follow exactly the same setting of GraphSAGE as in the original paper (Hamilton et al., 2017), where the model consists of 2 hidden layers, each with 128 hidden units and is trained with Adam with learning rate of 0.01 and no weight decay.",6.2. Reddit,[0],[0]
"Results are shown in Table 3.
Results.",6.2. Reddit,[0],[0]
"With MaxPool as node aggregator and Concat as layer aggregator, JK-Net achieves the best Micro-F1 score
among GarphSAGE and JK-Net variants.",6.2. Reddit,[0],[0]
Note that the original GraphSAGE already performs fairly well with a Micro-F1 of 0.95.,6.2. Reddit,[0],[0]
JK-Net reduces the error by 30%.,6.2. Reddit,[0],[0]
"The communities in the Reddit dataset were explicitly chosen from the well-behaved middle-sized communities to avoid the noisy cores and tree-like small communities (Hamilton et al., 2017).",6.2. Reddit,[0],[0]
"As a result, this graph is more regular than the original Reddit data, and hence not exhibit the problems of varying subgraph structures.",6.2. Reddit,[0],[0]
"In such a case, the added flexibility of the node-specific neighborhood choices may not be as relevant, and the stabilizing properties of concatenation instead come into play.",6.2. Reddit,[0],[0]
"We demonstrate the power of adaptive JK-Nets, e.g., JKLSTM, with experiments on the PPI data, where the subgraphs have more diverse and complex structures than those in the Reddit community detection dataset.",6.3. PPI,[0],[0]
We use both GraphSAGE and GAT as base models for JK-Net.,6.3. PPI,[0],[0]
"The implementation of GraphSAGE and GAT are quite different: GraphSAGE is sample-based, where neighbors of a node are sampled to be a fixed number, while GAT considers all neighbors.",6.3. PPI,[0],[0]
Such differences cause large gaps in terms of both scalability and performances.,6.3. PPI,[0],[0]
"Given that GraphSAGE scales to much larger graphs, it appears particularly valuable to evaluate how much JK-Net can improve upon GraphSAGE.
",6.3. PPI,[0],[0]
"For GraphSAGE we follow the setup as in the Reddit experiments, except that we use 3 layers when possible, and compare the performance after 10 and 30 epochs of training.",6.3. PPI,[0],[0]
The results are shown in Table 4.,6.3. PPI,[0],[0]
"For GAT and its JK-Net variants we stack two hidden layers with 4 attention heads computing 256 features (for a total of 1024 features), and a final prediction layer with 6 attention heads computing 121 features each.",6.3. PPI,[0],[0]
They are further averaged and input into sigmoid activations.,6.3. PPI,[0],[0]
We employ skip connections across intermediate attentional layers.,6.3. PPI,[0],[0]
These models are trained with Batch-size 2 and Adam optimizer with learning rate of 0.005.,6.3. PPI,[0],[0]
"The results are shown in Table 5.
Results.",6.3. PPI,[0],[0]
"JK-Nets with the LSTM-attention aggregators outperform the non-adaptive models GraphSAGE, GAT and JK-Nets with concatenation aggregators.",6.3. PPI,[0],[0]
"In particular, JKLSTM outperforms GraphSAGE by 0.128 in terms of micro-
F1 score after 30 epochs of training.",6.3. PPI,[0],[0]
Structure-aware node adaptive models are especially beneficial on such complex graphs with diverse structures.,6.3. PPI,[0],[0]
"Motivated by observations that reveal great differences in neighborhood information ranges for graph node embeddings, we propose a new aggregation scheme for node representation learning that can adapt neigborhood ranges to nodes individually.",7. Conclusion,[0],[0]
"This JK-network can improve representations in particular for graphs that have subgraphs of diverse local structure, and may hence not be well captured by fixed numbers of neighborhood aggregations.",7. Conclusion,[0],[0]
Interesting directions for future work include exploring other layer aggregators and studying the effect of the combination of various layer-wise and node-wise aggregators on different types of graph structures.,7. Conclusion,[0],[0]
"This research was supported by NSF CAREER award 1553284, and JST ERATO Kawarabayashi Large Graph Project, Grant Number JPMJER1201, Japan.",Acknowledgements,[0],[0]
Recent deep learning approaches for representation learning on graphs follow a neighborhood aggregation procedure.,abstractText,[0],[0]
"We analyze some important properties of these models, and propose a strategy to overcome those.",abstractText,[0],[0]
"In particular, the range of “neighboring” nodes that a node’s representation draws from strongly depends on the graph structure, analogous to the spread of a random walk.",abstractText,[0],[0]
"To adapt to local neighborhood properties and tasks, we explore an architecture – jumping knowledge (JK) networks – that flexibly leverages, for each node, different neighborhood ranges to enable better structure-aware representation.",abstractText,[0],[0]
"In a number of experiments on social, bioinformatics and citation networks, we demonstrate that our model achieves state-of-the-art performance.",abstractText,[0],[0]
"Furthermore, combining the JK framework with models like Graph Convolutional Networks, GraphSAGE and Graph Attention Networks consistently improves those models’ performance.",abstractText,[0],[0]
Representation Learning on Graphs with Jumping Knowledge Networks ,title,[0],[0]
"Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 912–921, Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics
Methods of deep neural networks (DNNs) have recently demonstrated superior performance on a number of natural language processing tasks. However, in most previous work, the models are learned based on either unsupervised objectives, which does not directly optimize the desired task, or singletask supervised objectives, which often suffer from insufficient training data. We develop a multi-task DNN for learning representations across multiple tasks, not only leveraging large amounts of cross-task data, but also benefiting from a regularization effect that leads to more general representations to help tasks in new domains. Our multi-task DNN approach combines tasks of multiple-domain classification (for query classification) and information retrieval (ranking for web search), and demonstrates significant gains over strong baselines in a comprehensive set of domain adaptation.",text,[0],[0]
"Recent advances in deep neural networks (DNNs) have demonstrated the importance of learning vector-space representations of text, e.g., words and sentences, for a number of natural language processing tasks.",1 Introduction,[0],[0]
"For example, the study reported in (Collobert et al., 2011) demonstrated significant accuracy gains in tagging, named entity recognition, and semantic role labeling when using vector space word
∗This research was conducted during the author’s internship at Microsoft Research.
representations learned from large corpora.",1 Introduction,[0],[0]
"Further, since these representations are usually in a lowdimensional vector space, they result in more compact models than those built from surface-form features.",1 Introduction,[0],[0]
"A recent successful example is the parser by (Chen and Manning, 2014), which is not only accurate but also fast.
",1 Introduction,[0],[0]
"However, existing vector-space representation learning methods are far from optimal.",1 Introduction,[0],[0]
"Most previous methods are based on unsupervised objectives such as word prediction for training (Mikolov et al., 2013c; Pennington et al., 2014).",1 Introduction,[0],[0]
"Other methods use supervised training objectives on a single task, e.g. (Socher et al., 2013), and thus are often constrained by limited amounts of training data.",1 Introduction,[0],[0]
"Motivated by the success of multi-task learning (Caruana, 1997), we propose in this paper a multi-task DNN approach for representation learning that leverages supervised data from many tasks.",1 Introduction,[0],[0]
"In addition to the benefit of having more data for training, the use of multi-task also profits from a regularization effect, i.e., reducing overfitting to a specific task, thus making the learned representations universal across tasks.
",1 Introduction,[0],[0]
"Our contributions are of two-folds: First, we propose a multi-task deep neural network for representation learning, in particular focusing on semantic classification (query classification) and semantic information retrieval (ranking for web search) tasks.",1 Introduction,[0],[0]
Our model learns to map arbitrary text queries and documents into semantic vector representations in a low dimensional latent space.,1 Introduction,[0],[0]
"While the general concept of multi-task neural nets is not new, our model is novel in that it successfully combines tasks as disparate as operations necessary for classifica-
912
tion or ranking.",1 Introduction,[0],[0]
"Second, we demonstrate strong results on query classification and web search.",1 Introduction,[0],[0]
Our multi-task representation learning consistently outperforms stateof-the-art baselines.,1 Introduction,[0],[0]
"Meanwhile, we show that our model is not only compact but it also enables agile deployment into new domains.",1 Introduction,[0],[0]
This is because the learned representations allow domain adaptation with substantially fewer in-domain labels.,1 Introduction,[0],[0]
Our multi-task model combines classification and ranking tasks.,2.1 Preliminaries,[0],[0]
"For concreteness, throughout this paper we will use query classification as the classification task and web search as the ranking task.",2.1 Preliminaries,[0],[0]
"These are important tasks in commercial search engines:
Query Classification:",2.1 Preliminaries,[0],[0]
"Given a search query Q, the model classifies in the binary fashion as to whether it belongs to one of the domains of interest.",2.1 Preliminaries,[0],[0]
"For example, if the query Q is “Denver sushi”, the classifier should decide that it belongs to the “Restaurant” domain.",2.1 Preliminaries,[0],[0]
"Accurate query classification enables a richer personalized user experience, since the search engine can tailor the interface and results.",2.1 Preliminaries,[0],[0]
"It is however challenging because queries tend to be short (Shen et al., 2006).",2.1 Preliminaries,[0],[0]
"Surface-form word features that are common in traditional document classification problems tend to be too sparse for query classification, so representation learning is a promising solution.",2.1 Preliminaries,[0],[0]
"In this study, we classify queries into four domains of interest: (“Restaurant”, “Hotel”, “Flight”, “Nightlife”).",2.1 Preliminaries,[0],[0]
Note that one query can belong to multiple domains.,2.1 Preliminaries,[0],[0]
"Therefore, a set of binary classifiers are built, one for each domain, to perform the classification.",2.1 Preliminaries,[0],[0]
We frame the problem as four binary classification tasks.,2.1 Preliminaries,[0],[0]
"Thus, for domain Ct, our goal is binary classification based on P (Ct| Q) (Ct = {0, 1} ).",2.1 Preliminaries,[0],[0]
"For each domain t, we assume supervised data (Q, yt = {0, 1} with yt as binary labels.1
Web Search:",2.1 Preliminaries,[0],[0]
"Given a search queryQ and a document list L, the model ranks documents in the order
1One could frame the problem as a a single multi-class classification task, but our formulation is more practical as it allows adding new domains without retraining existing classifiers.",2.1 Preliminaries,[0],[0]
"This will be relevant in domain adaptation (§3.3).
of relevance.",2.1 Preliminaries,[0],[0]
"For example, if the queryQ is ”Denver sushi”, model returns a list of documents that satisfies such information need.",2.1 Preliminaries,[0],[0]
"Formally, we estimate P (D1|Q), P (D2|Q), . . .",2.1 Preliminaries,[0],[0]
for each document Dn and rank according to these probabilities.,2.1 Preliminaries,[0],[0]
"We assume that supervised data exist; I.e., there is at least one relevant document Dn for each query Q.",2.1 Preliminaries,[0],[0]
"Briefly, our proposed model maps any arbitrary queries Q or documents D into fixed lowdimensional vector representations using DNNs.",2.2 The Proposed Multi-Task DNN Model,[0],[0]
These vectors can then be used to perform query classification or web search.,2.2 The Proposed Multi-Task DNN Model,[0],[0]
"In contrast to existing representation learning methods which employ either unsupervised or single-task supervised objectives, our model learns these representations using multi-task objectives.
",2.2 The Proposed Multi-Task DNN Model,[0],[0]
The architecture of our multi-task DNN model is shown in Figure 1.,2.2 The Proposed Multi-Task DNN Model,[0],[0]
"The lower layers are shared across different tasks, whereas the top layers represent task-specific outputs.",2.2 The Proposed Multi-Task DNN Model,[0],[0]
"Importantly, the input X (either a query or document), initially represented as a bag of words, is mapped to a vector (l2) of dimension 300.",2.2 The Proposed Multi-Task DNN Model,[0],[0]
This is the shared semantic representation that is trained by our multi-task objectives.,2.2 The Proposed Multi-Task DNN Model,[0],[0]
"In the following, we elaborate the model in detail:
Word Hash Layer (l1): Traditionally, each word is represented by a one-hot word vector, where the dimensionality of the vector is the vocabulary size.",2.2 The Proposed Multi-Task DNN Model,[0],[0]
"However, due to the large size of vocabulary in realworld tasks, it is very expensive to learn such kind of models.",2.2 The Proposed Multi-Task DNN Model,[0],[0]
"To alleviate this problem, we adopt the word hashing method (Huang et al., 2013).",2.2 The Proposed Multi-Task DNN Model,[0],[0]
"We map a one-hot word vector, with an extremely high dimensionality, into a limited letter-trigram space (e.g., with the dimensionality as low as 50k).",2.2 The Proposed Multi-Task DNN Model,[0],[0]
"For example, word cat is hashed as the bag of letter trigram {#-c-a, c-a-t, a-t-#}, where # is a boundary symbol.",2.2 The Proposed Multi-Task DNN Model,[0],[0]
"Word hashing complements the one-hot vector representation in two aspects: 1) out of vocabulary words can be represented by letter-trigram vectors; 2) spelling variations of the same word can be mapped to the points that are close to each other in the letter-trigram space.
",2.2 The Proposed Multi-Task DNN Model,[0],[0]
Semantic-Representation Layer (l2):,2.2 The Proposed Multi-Task DNN Model,[0],[0]
This is a shared representation learned across different tasks.,2.2 The Proposed Multi-Task DNN Model,[0],[0]
"this layer maps the letter-trigram inputs into a 300-
1
dimensional vector by
l2 = f(W1 · l1) (1)
where f(·) is the tanh nonlinear activation f(z) = 1−e−2z 1+e−2z .",2.2 The Proposed Multi-Task DNN Model,[0],[0]
"This 50k-by-300 matrix W1 is responsible for generating the cross-task semantic representation for arbitrary text inputs (e.g., Q or D).
",2.2 The Proposed Multi-Task DNN Model,[0],[0]
"Task-Specific Representation (l3): For each task, a nonlinear transformation maps the 300- dimension semantic representation l2 into the 128- dimension task-specific representation by
l3 = f(Wt2 · l2) (2)
where, t denotes different tasks (query classification or web search).
",2.2 The Proposed Multi-Task DNN Model,[0],[0]
Query Classification Output: Suppose QC1 ≡ l3 = f(Wt=C12 · l2) is the 128-dimension taskspecific representation for a query Q.,2.2 The Proposed Multi-Task DNN Model,[0],[0]
"The probability that Q belongs to class C1 is predicted by a logistic regression, with sigmoid g(z)",2.2 The Proposed Multi-Task DNN Model,[0],[0]
"= 1
",2.2 The Proposed Multi-Task DNN Model,[0],[0]
"1+e−z :
",2.2 The Proposed Multi-Task DNN Model,[0],[0]
"P (C1|Q) = g(Wt=C13 ·QC1) (3)
Web Search Output: For the web search task, both the query Q and the document D are mapped into 128-dimension task-specific representations QSq and DSd .",2.2 The Proposed Multi-Task DNN Model,[0],[0]
"Then, the relevance score is
Algorithm 1: Training a Multi-task DNN Initialize model Θ : {W1,Wt2,Wt3} randomly for iteration in 0...∞ do
1.",2.2 The Proposed Multi-Task DNN Model,[0],[0]
Pick a task t randomly 2.,2.2 The Proposed Multi-Task DNN Model,[0],[0]
"Pick sample(s) from task t
(Q, yt = {0, 1}) for query classification (Q,L) for web search
3.",2.2 The Proposed Multi-Task DNN Model,[0],[0]
Compute loss: L(Θ) L(Θ)=Eq.,2.2 The Proposed Multi-Task DNN Model,[0],[0]
5 for query classification L(Θ)=Eq.,2.2 The Proposed Multi-Task DNN Model,[0],[0]
6 for web search 4.,2.2 The Proposed Multi-Task DNN Model,[0],[0]
Compute gradient: ∇(Θ) 5.,2.2 The Proposed Multi-Task DNN Model,[0],[0]
"Update model: Θ = Θ− ∇(Θ)
end The task t is one of the query classification tasks or web search task, as shown in Figure 1.",2.2 The Proposed Multi-Task DNN Model,[0],[0]
"For query classification, each training sample includes one query and its category label.",2.2 The Proposed Multi-Task DNN Model,[0],[0]
"For web search, each training sample includes query and document list.
computed by cosine similarity as:
R(Q,D) = cos(QSq , DSd)",2.2 The Proposed Multi-Task DNN Model,[0],[0]
= QSq ·DSd ||QSq,2.2 The Proposed Multi-Task DNN Model,[0],[0]
||||DSd || (4),2.2 The Proposed Multi-Task DNN Model,[0],[0]
"In order to learn the parameters of our model, we use mini-batch-based stochastic gradient descent (SGD) as shown in Algorithm 1.",2.3 The Training Procedure,[0],[0]
"In each iteration, a task t is selected randomly, and the model is updated ac-
cording to the task-specific objective.",2.3 The Training Procedure,[0],[0]
This approximately optimizes the sum of all multi-task objectives.,2.3 The Training Procedure,[0],[0]
"For query classification of class Ct, we use the cross-entropy loss as the objective: −{yt lnP (Ct|Q)+(1−yt) ln(1−P (Ct|Q))}",2.3 The Training Procedure,[0],[0]
"(5)
where yt = {0, 1} is the label and the loss is summed over all samples in the mini-batch (1024 samples in experiments).
",2.3 The Training Procedure,[0],[0]
"The objective for web search used in this paper follows the pair-wise learning-to-rank paradigm outlined in (Burges et al., 2005).",2.3 The Training Procedure,[0],[0]
"Given a query Q, we obtain a list of documents L that includes a clicked document D+ (positive sample), and J randomlysampled non-clicked documents {D−j }j=1,.,J .",2.3 The Training Procedure,[0],[0]
"We then minimize the negative log likelihood of the clicked document (defined in Eq. 7) given queries across the training data
− log ∏
(Q,D+)
P (D+|Q) (6)
where the probability of a given document D+ is computed
P (D+|Q) =",2.3 The Training Procedure,[0],[0]
"exp(γR(Q,D +))",2.3 The Training Procedure,[0],[0]
"∑
D′∈L exp(γR(Q,D′)) (7)
",2.3 The Training Procedure,[0],[0]
"here, γ is a tuning factor determined on held-out data.",2.3 The Training Procedure,[0],[0]
"Additional training details: (1) Model parameters are initialized with uniform distribution in the range (−√6/(fanin + fanout),√6/(fanin + fanout))",2.3 The Training Procedure,[0],[0]
"(Montavon et al., 2012).",2.3 The Training Procedure,[0],[0]
"Empirically, we have not observed better performance by initialization with layer-wise pre-training.",2.3 The Training Procedure,[0],[0]
"(2) Moment methods and AdaGrad training (Duchi et al., 2011) speed up the convergence speed but gave similar results as plain SGD.",2.3 The Training Procedure,[0],[0]
The SGD learning rate is fixed at = 0.1/1024.,2.3 The Training Procedure,[0],[0]
"(3) We run Algorithm 1 for 800K iterations, taking 13 hours on an NVidia K20 GPU.",2.3 The Training Procedure,[0],[0]
"Our proposed multi-task DNN (Figure 1) can be viewed as a combination of a standard DNN for classification and a Deep Structured Semantic Model (DSSM) for ranking, shown in Figure 2.",2.4 An Alternative View of the Multi-Task Model,[0],[0]
Other ways to merge the models are possible.,2.4 An Alternative View of the Multi-Task Model,[0],[0]
"Figure 3 shows an alternative multi-task architecture, where only the query part is shared among all tasks and the DSSM
retains independent parameters for computing the document representations.",2.4 An Alternative View of the Multi-Task Model,[0],[0]
This is more similar to the original DSSM.,2.4 An Alternative View of the Multi-Task Model,[0],[0]
"We have attempted training this model using Algorithm 1, but it achieves good results on query classification at the expense of web search.",2.4 An Alternative View of the Multi-Task Model,[0],[0]
"This is likely due to unbalanced updates (i.e. parameters for queries are updated more often than that of documents), and implying that the amount of sharing is an important design choice in multi-task models.
",2.4 An Alternative View of the Multi-Task Model,[0],[0]
3,2.4 An Alternative View of the Multi-Task Model,[0],[0]
"We employ large-scale, real data sets in our evaluation.",3.1 Data Sets and Evaluation Metrics,[0],[0]
See Table 1 for statistics.,3.1 Data Sets and Evaluation Metrics,[0],[0]
The test data for query classification were sampled from one-year log files of a commercial search engine with labels (yes or no) judged by humans.,3.1 Data Sets and Evaluation Metrics,[0],[0]
"The test data for web search contains 12,071 English queries, where each query-document pair has a relevance label manually annotated on a 5-level relevance scale: bad, fair,
good, excellent and perfect.",3.1 Data Sets and Evaluation Metrics,[0],[0]
"The evaluation metric for query classification is the Area under of Receiver Operating Characteristic (ROC) curve (AUC) score (Bradley, 1997).",3.1 Data Sets and Evaluation Metrics,[0],[0]
"For web search, we employ the Normalized Discounted Cumulative Gain (NDCG) (Järvelin and Kekäläinen, 2000).",3.1 Data Sets and Evaluation Metrics,[0],[0]
"First, we evaluate whether our model can robustly improve performance, measured as accuracy across multiple tasks.
",3.2 Results on Accuracy,[0],[0]
"Table 2 summarizes the AUC scores for query classification, comparing the following classifiers: • SVM-Word: a SVM model2 with unigram, bi-
gram and trigram surface-form word features.
",3.2 Results on Accuracy,[0],[0]
• SVM-Letter: a SVM model with letter trigram features (i.e. l1 in Figure 1 as input to SVM).,3.2 Results on Accuracy,[0],[0]
• DNN:,3.2 Results on Accuracy,[0],[0]
single-task deep neural net (Figure 2).,3.2 Results on Accuracy,[0],[0]
• MT-DNN: our multi-task proposal (Figure 1).,3.2 Results on Accuracy,[0],[0]
The results show that the proposed MT-DNN performs best in all four domains.,3.2 Results on Accuracy,[0],[0]
"Further, we observe:
1.",3.2 Results on Accuracy,[0],[0]
"MT-DNN outperforms DNN, indicating the usefulness of the multi-task objective (that includes web search) over the single-task objective of query classification.
2.",3.2 Results on Accuracy,[0],[0]
"Both DNN and MT-DNN outperform SVMLetter, which initially uses the same input features (l1).",3.2 Results on Accuracy,[0],[0]
"This indicates the importance of learning a semantic representation l2 on top of these letter trigrams.
3.",3.2 Results on Accuracy,[0],[0]
"Both DNN and MT-DNN outperform a strong SVM-Word baseline, which has a large feature set that consists of 3 billion features.
",3.2 Results on Accuracy,[0],[0]
"Table 3 summarizes the NDCG results on web search, comparing the following models:",3.2 Results on Accuracy,[0],[0]
"2In this paper, we use the liblinear to build SVM classifiers and optimize the corresponding parameter C by using 5-fold cross-validation in training data.",3.2 Results on Accuracy,[0],[0]
"http://www.csie.ntu.edu.tw/ cjlin/liblinear/
• Popular baselines in the web search literature, e.g. BM25, Language Model, PLSA
• DSSM: single-task ranking model (Figure 2) • MT-DNN:",3.2 Results on Accuracy,[0],[0]
"our multi-task proposal (Figure 1)
",3.2 Results on Accuracy,[0],[0]
"Again, we observe that MT-DNN performs best.",3.2 Results on Accuracy,[0],[0]
"For example, MT-DNN achieves NDCG@1=0.334, outperforming the current state-of-the-art single-task DSSM (0.327) and the classic methods like PLSA (0.308) and BM25 (0.305).",3.2 Results on Accuracy,[0],[0]
"This is a statistically significant improvement (p < 0.05) over DSSM and other baselines.
",3.2 Results on Accuracy,[0],[0]
"To recap, our MT-DNN robustly outperforms strong baselines across all web search and query classification tasks.",3.2 Results on Accuracy,[0],[0]
"Further, due to the use of larger training data (from different domains) and the regularization effort as we discussed in Section 1, we confirm the advantage of multi-task models over than single-task ones.3",3.2 Results on Accuracy,[0],[0]
Important criteria for building practical systems are agility of deployment and small memory footprint and fast run-time.,3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"Our model satisfies both with 3We have also trained SVM using Word2Vec (Mikolov et al., 2013b; Mikolov et al., 2013a) features.",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"Unfortunately, the results are poor at 60-70 AUC, indicating the sub-optimality of unsupervised representation learning objectives for actual prediction tasks.",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"We optimized the Word2Vec features in the SVM baseline by scaling and normalizing as well, but did not observe much improvement.
high model compactness.",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
The key to the compactness is the aggressive compression from the 500kdimensional bag-of-words input to 300-dimensional semantic representation l2.,3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
This significantly reduces the memory/run-time requirements compared to systems that rely on surface-form features.,3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"The most expensive portion of the model is storage of the 50k-by-300 W1 and its matrix multiplication with l1, which is sparse: this is trivial on modern hardware.",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"Our multi-task DNN takes < 150KB in memory whereas e.g. SVM-Word takes about 200MB.
",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"Compactness is particularly important for query classification, since one may desire to add new domains after discovering new needs from the query logs of an operational system.",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"On the other hand, it is prohibitively expensive to collect labeled training data for new domains.",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"Very often, we only have very small training data or even no training data.
",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"To evaluate the models using the above criteria, we perform domain adaptation experiments on query classification using the following procedure: (1) Select one query classification task t∗.",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"Train MTDNN on the remaining tasks (including Web Search
task) to obtain a semantic representation (l2); (2) Given a fixed l2, train an SVM on the training data t∗, using varying amounts of labels; (3) Evaluate the AUC on the test data of t∗
We compare three SVM classifiers trained using different feature representations: (1) SemanticRepresentation uses the l2 features generated according to the above procedure.",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"(2) Word3gram uses unigram, bigram and trigram word features.",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
(3) Letter3gram uses letter-trigrams.,3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"Note that Word3gram and Letter3gram correspond to SVMWord and SVM-Letter respectively in Table 2.
",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
The AUC results for different amounts of t∗ training data are shown in Figure 4.,3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"In the Hotel, Flight and Restaurant domains, we observe that our semantic representation dominated the other two feature representations (Word3gram and Letter3gram) in all cases except the extremely large-data regime (more than 1 million training samples in domain t∗).",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"Given sufficient labels, SVM is able to train well on Word3gram sparse features, but for most cases Se-
manticRepresentation is recommended.4
In a further experiment, we compare the following two DNNs using the same domain adaptation procedure: (1) DNN1: DNN where W1 is randomly initialized and parameters W1,W2,Wt ∗ 3 are trained on varying amounts of data in t∗;",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"(2) DNN2: DNN where W1 is obtained from other tasks (i.e. SemanticRepresentation) and fixed, while parameters W2,Wt ∗ 3 are trained on varying amounts of data in t∗.",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
The purpose is to see whether shared semantic representation is useful even under a DNN architecture.,3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"Figure 5 show the AUC results of DNN1 vs. DNN2 (the results SVM denotes the same system as SemanticRepresentation in Figure 4, plotted here for reference).",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"We observe that when the training data is extremely large (millions of samples), one does best by training all parameters from scratch (DNN1).",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"Otherwise, one is better off using a shared semantic representation trained by multitask objectives.",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"Comparing DNN2 and SVM with SemanticRepresentation, we note that SVM works best for training data of several thousand samples; DNN2 works best in the medium data range.",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"There is a large body of work on representation learning for natural language processing, sometimes using different terminologies for similar concepts; e.g., feature generation, dimensionality reduction, and vector space models.",4 Related Work,[0],[0]
"The main motivation is similar: to abstract away from surface forms in words, sentences, or documents, in order to alleviate sparsity and approximate semantics.",4 Related Work,[0],[0]
"Traditional techniques include LSA (Deerwester et al., 1990), ESA (Gabrilovich and Markovitch, 2007), PCA (Karhunen, 1998), and non-linear kernel variants (Schölkopf et al., 1998).",4 Related Work,[0],[0]
"Recently, learningbased approaches inspired by neural networks, especially DNNs, have gained in prominence, due to their favorable performance (Huang et al., 2013; Baroni et al., 2014; Milajevs et al., 2014).
",4 Related Work,[0],[0]
"Popular methods for learning word representations include (Collobert et al., 2011; Mikolov et al., 2013c; Mnih and Kavukcuoglu, 2013; Pennington et al., 2014): all are based on unsupervised objec-
4The trends differ slightly in the Nightlife domain.",4 Related Work,[0],[0]
"We believe this may be due to data bias on test data (only 298 samples).
tives of predicting words or word frequencies from raw text.",4 Related Work,[0],[0]
"End-to-end neural network models for specific tasks (e.g. parsing) often use these word representations as initialization, which are then iteratively improved by optimizing a supervised objective (e.g. parsing accuracy).",4 Related Work,[0],[0]
"A selection of successful applications of this approach include sequence labeling (Turian et al., 2010), parsing (Chen and Manning, 2014), sentiment (Socher et al., 2013), question answering (Iyyer et al., 2014) and translation modeling (Gao et al., 2014a).
",4 Related Work,[0],[0]
"Our model takes queries and documents as input, so it learns sentence/document representations.",4 Related Work,[0],[0]
"This is currently an open research question, the challenge being how to properly model semantic compositionality of words in vector space (Huang et al., 2013; M. Baroni and Zamparelli, 2013; Socher et al., 2013).",4 Related Work,[0],[0]
"While we adopt a bag-of-words approach for practical reasons (memory and run-time), our multi-task framework is extensible to other methods for sentence/document representations, such as those based on convolutional networks (Kalchbrenner et al., 2014; Shen et al., 2014; Gao et al., 2014b), parse tree structure (Irsoy and Cardie, 2014), and run-time inference (Le and Mikolov, 2014).
",4 Related Work,[0],[0]
"The synergy between multi-task learning and neural nets is quite natural; the general idea dates back to (Caruana, 1997).",4 Related Work,[0],[0]
The main challenge is in designing the tasks and the network structure.,4 Related Work,[0],[0]
"For example, (Collobert et al., 2011) defined part-of-speech tagging, chunking, and named entity recognition as multiple tasks in a single sequence labeler; (Bordes et al., 2012) defined multiple data sources as tasks in their relation extraction system.",4 Related Work,[0],[0]
"While conceptually similar, our model is novel in that it combines tasks as disparate as classification and ranking.",4 Related Work,[0],[0]
"Further, considering that multi-task models often exhibit mixed results (i.e. gains in some tasks but degradation in others), our accuracy improvements across all tasks is a very satisfactory result.",4 Related Work,[0],[0]
"In this work, we propose a robust and practical representation learning algorithm based on multi-task objectives.",5 Conclusion,[0],[0]
"Our multi-task DNN model successfully combines tasks as disparate as classification and ranking, and the experimental results demon-
strate that the model consistently outperforms strong baselines in various query classification and web search tasks.",5 Conclusion,[0],[0]
"Meanwhile, we demonstrated compactness of the model and the utility of the learned query/document representation for domain adaptation.
",5 Conclusion,[0],[0]
Our model can be viewed as a general method for learning semantic representations beyond the word level.,5 Conclusion,[0],[0]
"Beyond query classification and web search, we believe there are many other knowledge sources (e.g. sentiment, paraphrase) that can be incorporated either as classification or ranking tasks.",5 Conclusion,[0],[0]
A comprehensive exploration will be pursued as future work.,5 Conclusion,[0],[0]
"We thank Xiaolong Li, Yelong Shen, Xinying Song, Jianshu Chen, Byungki Byun, Bin Cao and the anonymous reviewers for valuable discussions and comments.",Acknowledgments,[0],[0]
Methods of deep neural networks (DNNs) have recently demonstrated superior performance on a number of natural language processing tasks.,abstractText,[0],[0]
"However, in most previous work, the models are learned based on either unsupervised objectives, which does not directly optimize the desired task, or singletask supervised objectives, which often suffer from insufficient training data.",abstractText,[0],[0]
"We develop a multi-task DNN for learning representations across multiple tasks, not only leveraging large amounts of cross-task data, but also benefiting from a regularization effect that leads to more general representations to help tasks in new domains.",abstractText,[0],[0]
"Our multi-task DNN approach combines tasks of multiple-domain classification (for query classification) and information retrieval (ranking for web search), and demonstrates significant gains over strong baselines in a comprehensive set of domain adaptation.",abstractText,[0],[0]
Representation Learning Using Multi-Task Deep Neural Networks for Semantic Classification and Information Retrieval,title,[0],[0]
"Recently, hyperbolic embeddings have been proposed as a way to capture hierarchy information for network and natural language processing tasks (Nickel & Kiela, 2017; Chamberlain et al., 2017).",1. Introduction,[0],[0]
"This approach is an exciting way to fuse structural information (for example, from knowledge graphs or synonym hierarchies) with the continuous representations favored by modern machine learning methods.
",1. Introduction,[0],[0]
"To understand the intuition behind hyperbolic embeddings’ superior capacity, note that trees can be embedded with arbitrarily low distortion into the Poincaré disk, a twodimensional model of hyperbolic space (Sarkar, 2011).",1. Introduction,[0],[0]
"In contrast, Bourgain’s theorem (Linial et al., 1995) shows that Euclidean space cannot achieve comparably low distortion
1Department of Computer Science, Stanford University 2Department of Computer Science, Cornell University.",1. Introduction,[0],[0]
"Correspondence to: Frederic Sala <fredsala@stanford.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
for trees—even using an unbounded number of dimensions.
",1. Introduction,[0],[0]
"Many graphs, such as complex networks (Krioukov et al., 2010), the Internet (Krioukov et al., 2009), and social networks (Verbeek & Suri, 2016), are known to have tree-like or hyperbolic structure and thus befit hyperbolic embeddings.",1. Introduction,[0],[0]
"Indeed, recent works show that hyperbolic representations are suitable for many hierarchies (e.g. the question answering (Q/A) system HyperQA in Tay et al. (2018), vertex classifiers in Chamberlain et al. (2017), and link prediction (Nickel & Kiela, 2017)).",1. Introduction,[0],[0]
"However, the optimization problems underlying the embedding techniques in these works are challenging, motivating us to seek fundamental insights and to understand the subtle tradeoffs involved.
",1. Introduction,[0],[0]
"We begin by considering the case where we are given an input graph that is a tree or nearly tree-like, and our goal is to produce a low-dimensional hyperbolic embedding that preserves all distances.",1. Introduction,[0],[0]
This leads to a simple combinatorial strategy that directly places points instead of minimizing a surrogate loss function.,1. Introduction,[0],[0]
It is both fast (nearly linear time) and has formal quality guarantees.,1. Introduction,[0],[0]
"The approach proceeds in two phases: we (1) produce an embedding of a graph into a weighted tree, and (2) embed that tree into the hyperbolic disk.",1. Introduction,[0],[0]
"In particular, we consider an extension of an elegant embedding of trees into the Poincaré disk by Sarkar (2011) and work on low-distortion graph embeddings into tree metrics (Abraham et al., 2007).",1. Introduction,[0],[0]
"For trees, this approach has nearly perfect quality.",1. Introduction,[0],[0]
"On the WordNet hypernym graph reconstruction, it obtains a nearly perfect mean average precision (MAP) of 0.989 using just 2 dimensions.",1. Introduction,[0],[0]
"The best published numbers for WordNet in Nickel & Kiela (2017) range between 0.823 and 0.87 for 5 to 200 dimensions.
",1. Introduction,[0],[0]
We analyze this construction to extract fundamental tradeoffs.,1. Introduction,[0],[0]
"One tradeoff involves the embedding dimension, the properties of the graph, and the number of bits of precision used to represent components of embedded points—an important hidden cost.",1. Introduction,[0],[0]
"We show that for a fixed precision, the dimension required scales linearly with the length of the longest path.",1. Introduction,[0],[0]
"On the other hand, the dimension scales logarithmically with the maximum degree of the tree.",1. Introduction,[0],[0]
"This suggests that hyperbolic embeddings should have high quality on hierarchies like WordNet but require large dimensions or high precision on graphs with long chains.
",1. Introduction,[0],[0]
"To understand how hyperbolic embeddings perform for met-
rics that are far from tree-like, we consider a more general problem: given a matrix of distances that arise from points that are embeddable in hyperbolic space of dimension d (not necessarily from a graph), find a set of points that produces these distances.",1. Introduction,[0],[0]
"In Euclidean space, the problem is known as multidimensional scaling (MDS) and is solvable using PCA.",1. Introduction,[0],[0]
"A key step is a transformation that effectively centers the points, without knowledge of their exact coordinates.",1. Introduction,[0],[0]
"It is not obvious how to center points in hyperbolic space, which is curved.",1. Introduction,[0],[0]
"We show that in hyperbolic space, a centering operation is still possible with respect to a non-standard mean.",1. Introduction,[0],[0]
"In turn, this allows us to reduce the hyperbolic MDS problem (h-MDS) to a standard eigenvalue problem that can be solved with power methods.",1. Introduction,[0],[0]
"We also extend classical PCA perturbation analysis (Sibson, 1978; 1979).",1. Introduction,[0],[0]
"When applied to distances from graphs induced by real data, h-MDS obtains low distortion on far from tree-like graphs.",1. Introduction,[0],[0]
"However, we observe that these solutions may require high precision, which is not surprising in light of our previous analysis.
",1. Introduction,[0],[0]
"Finally, we handle increasing amounts of noise in the model, leading naturally into new SGD-based formulations.",1. Introduction,[0],[0]
"Like in traditional PCA, the underlying problem is nonconvex.",1. Introduction,[0],[0]
"In contrast to PCA, there are local minima that are not global minima—an additional challenge.",1. Introduction,[0],[0]
Our main technical result is that an SGD-based algorithm initialized with an h-MDS solution can recover the submanifold the data is on—even in some cases in which the data is perturbed by noise that can be full dimensional.,1. Introduction,[0],[0]
Our algorithm essentially provides new recovery results for convergence of Principal Geodesic Analysis (PGA) in hyperbolic space.,1. Introduction,[0],[0]
We implemented the resulting SGD-based algorithm using PyTorch.,1. Introduction,[0],[0]
"Finally, we note that all of our algorithms can handle incomplete distance information through standard techniques.",1. Introduction,[0],[0]
"We provide intuition connecting hyperbolic space and tree distances, discuss the metrics used to measure embedding fidelity, and discuss the relationship between the reconstruction and learning problems for graph embeddings.
",2. Background,[0],[0]
"Hyperbolic spaces The Poincaré disk H2 is a twodimensional model of hyperbolic geometry with points located in the interior of the unit disk, as shown in Figure 1.",2. Background,[0],[0]
"A natural generalization of H2 is the Poincaré ball Hr, with elements inside the unit ball.",2. Background,[0],[0]
"The Poincaré models offer several useful properties, chief among which is mapping conformally to Euclidean space.",2. Background,[0],[0]
"That is, angles are preserved between hyperbolic and Euclidean space.",2. Background,[0],[0]
"Distances, on the other hand, are not preserved, but are given by
dH(x, y) = acosh ( 1 + 2 ‖x− y‖2
(1− ‖x‖2)(1− ‖y‖2)
) .
",2. Background,[0],[0]
"There are some potentially unexpected consequences of this formula, and a simple example gives intuition about a key technical property that allows hyperbolic space to embed trees.",2. Background,[0],[0]
"Consider three points inside the unit disk: the origin 0, and points x and y with ‖x‖ = ‖y‖ = t for some t > 0.",2. Background,[0],[0]
"As shown on the right of Figure 1, as t → 1 (i.e., the points move towards the outside of the disk), in flat Euclidean space, the ratio dE(x,y)dE(x,0)+dE(0,y) is constant with respect to t (blue curve).",2. Background,[0],[0]
"In contrast, the ratio
dH(x,y) dH(x,0)+dH(0,y) approaches 1, or, equivalently, the distance dH(x, y) approaches dH(x, 0) +",2. Background,[0],[0]
"dH(0, y) (red and pink curves).",2. Background,[0],[0]
"That is, the shortest path between x and y is almost the same as the path through the origin.",2. Background,[0],[0]
This is analogous to the property of trees in which the shortest path between two sibling nodes is the path through their parent.,2. Background,[0],[0]
This tree-like nature of hyperbolic space is the key property exploited by embeddings.,2. Background,[0],[0]
"Moreover, this property holds for arbitrarily small angles between x and y.
Lines and geodesics There are two types of geodesics (shortest paths) in the Poincaré disk model: segments of circles that are orthogonal to the disk surface, and disk diameters (Brannan et al., 2012).",2. Background,[0],[0]
"Our algorithms and proofs make use of a simple geometric fact: isometric reflection across geodesics (preserving hyperbolic distances) is represented in this Euclidean model as a circle inversion.
",2. Background,[0],[0]
Embeddings and fidelity measures An embedding is a mapping f :,2. Background,[0],[0]
"U → V for spaces U, V with distances dU , dV .",2. Background,[0],[0]
"We measure the quality of embeddings with several fidelity measures, presented here from most local to most global.
",2. Background,[0],[0]
"Recent work (Nickel & Kiela, 2017) proposes using the mean average precision (MAP).",2. Background,[0],[0]
"For a graph G = (V,E), let a ∈ V have neighborhood Na = {b1, b2, . . .",2. Background,[0],[0]
", bdeg(a)}, where deg(a) denotes the degree of a.",2. Background,[0],[0]
"In the embedding f , consider the points closest to f(a), and define Ra,bi to be the smallest set of such points that contains bi (that is, Ra,bi is the smallest set of nearest points required to retrieve the ith neighbor of a in f ).",2. Background,[0],[0]
"Then, the MAP is defined to be
MAP(f) = 1 |V | ∑ a∈V
1
deg(a) |Na|∑ i=1",2. Background,[0],[0]
"|Na ∩Ra,bi | |Ra,bi | .
",2. Background,[0],[0]
"We have MAP(f) ≤ 1, with 1 as the best case.",2. Background,[0],[0]
"MAP is not concerned with explicit distances, but only ranks between the distances of immediate neighbors.",2. Background,[0],[0]
"It is a local metric.
",2. Background,[0],[0]
"The standard metric for graph embeddings is distortion D. For an n point embedding,
D(f)",2. Background,[0],[0]
"= 1( n 2 )  ∑ u,v∈U :u6=v |dV",2. Background,[0],[0]
"(f(u),",2. Background,[0],[0]
"f(v))− dU (u, v)| dU (u, v)  .
",2. Background,[0],[0]
"The best distortion isD(f) = 0, preserving the edge lengths exactly.",2. Background,[0],[0]
"This is a global metric, as it depends directly on the underlying distances rather than the local relationships between distances.",2. Background,[0],[0]
"A variant is the worst-case distortion Dwc, defined by
Dwc(f) =",2. Background,[0],[0]
"maxu,v∈U :u6=v dV (f(u), f(v))/dU (u, v)
minu,v∈U :u6=v dV (f(u), f(v))/dU (u, v) .
",2. Background,[0],[0]
"That is, the wost-case distortion is the ratio of the maximal expansion and the minimal contraction of distances.",2. Background,[0],[0]
Note that scaling the unit distance does not affect Dwc.,2. Background,[0],[0]
"The best worst-case distortion is Dwc(f) = 1.
",2. Background,[0],[0]
"Reconstruction and learning If we lack a full set of distances, we can either use the triangle inequality to recover the missing distances, or we can access the scaled Euclidean distances (the inside of the acosh in dH(x, y)), and apply standard matrix completion techniques (Candes & Tao, 2010).",2. Background,[0],[0]
Then we compute an embedding using any of the approaches discussed in this paper.,2. Background,[0],[0]
We quantify the error introduced by this process experimentally in Section 5.,2. Background,[0],[0]
We first focus on hyperbolic tree embeddings—a natural approach considering the tree-like behavior of hyperbolic space.,3. Combinatorial Constructions,[0],[0]
We review the embedding of Sarkar (2011).,3. Combinatorial Constructions,[0],[0]
"We then provide novel analysis on the precision, revealing fundamental limits of hyperbolic embeddings.",3. Combinatorial Constructions,[0],[0]
"In particular, we characterize the bits of precision needed for hyperbolic representations.",3. Combinatorial Constructions,[0],[0]
"We extend the construction to r dimensions, and propose to use Steiner nodes to better embed general graphs as trees, building on Abraham et al. (2007).
",3. Combinatorial Constructions,[0],[0]
Embedding trees The nature of hyperbolic space lends itself towards excellent tree embeddings.,3. Combinatorial Constructions,[0],[0]
"In fact, it is possible to embed trees into the Poincaré disk H2 with arbitrarily low distortion (Sarkar, 2011).",3. Combinatorial Constructions,[0],[0]
"Remarkably, trees cannot be embedded into Euclidean space with arbitrarily low distortion for any number of dimensions.",3. Combinatorial Constructions,[0],[0]
"These notions motivate the following two-step process for embedding hierarchies
into hyperbolic space: (1) embed the graphG = (V,E) into a tree T , and (2) embed T into the Poincaré ball",3. Combinatorial Constructions,[0],[0]
Hd.,3. Combinatorial Constructions,[0],[0]
We refer to this process as the combinatorial construction.,3. Combinatorial Constructions,[0],[0]
Note that we are not required to minimize a loss function.,3. Combinatorial Constructions,[0],[0]
"We begin by describing the second stage, where we extend an elegant construction from Sarkar (2011).",3. Combinatorial Constructions,[0],[0]
Algorithm 1 performs an embedding of trees into H2.,3.1. Sarkar’s Construction,[0],[0]
The inputs are a scaling factor τ and a node a (of degree deg(a)) from the tree with parent node b. Say a and b have already been embedded into f(a) and f(b) in H2.,3.1. Sarkar’s Construction,[0],[0]
"The algorithm places the children c1, c2, . . .",3.1. Sarkar’s Construction,[0],[0]
", cdeg(a)−1 into H2.
",3.1. Sarkar’s Construction,[0],[0]
A two-step process is used.,3.1. Sarkar’s Construction,[0],[0]
"First, f(a) and f(b) are reflected across a geodesic (using circle inversion) so that f(a) is mapped onto the origin 0 and f(b) is mapped onto some point",3.1. Sarkar’s Construction,[0],[0]
"z. Next, we place the children nodes to vectors y1, . . .",3.1. Sarkar’s Construction,[0],[0]
", yd−1 equally spaced around a circle with radius eτ−1 eτ+1 (which is a circle of radius τ in the hyperbolic metric), and maximally separated from the reflected parent node embedding z.",3.1. Sarkar’s Construction,[0],[0]
"Lastly, we reflect all of the points back across the geodesic.",3.1. Sarkar’s Construction,[0],[0]
The isometric properties of reflections imply that all children are now at hyperbolic distance exactly τ from f(a).,3.1. Sarkar’s Construction,[0],[0]
"To embed the entire tree, we place the root at the origin O and its children in a circle around it (as in Step 5 of Algorithm 1), then recursively place their children until all nodes have been placed.",3.1. Sarkar’s Construction,[0],[0]
This process runs in linear time.,3.1. Sarkar’s Construction,[0],[0]
Sarkar’s construction works by separating children sufficiently in hyperbolic space.,3.2. Analyzing Sarkar’s Construction,[0],[0]
A key technical idea is to scale all the edges by a factor τ before embedding.,3.2. Analyzing Sarkar’s Construction,[0],[0]
We can then recover the original distances by dividing by τ .,3.2. Analyzing Sarkar’s Construction,[0],[0]
This transformation exploits the fact that hyperbolic space is not scale invariant.,3.2. Analyzing Sarkar’s Construction,[0],[0]
"Sarkar’s construction always captures neighbors perfectly, but Figure 1 implies that increasing the scale preserves the distances between farther nodes better.",3.2. Analyzing Sarkar’s Construction,[0],[0]
"Indeed, if one sets τ = 1+εε ( 2 log degmaxπ/2 ) , then the worst-case distortion D of the resulting embedding is no more than
Algorithm 1 Sarkar’s Construction 1: Input: Node a with parent b, children to place c1, c2, . . .",3.2. Analyzing Sarkar’s Construction,[0],[0]
", cdeg(a)−1, partial embedding f containing an embedding for a and b, scaling factor τ
2: (0, z)← reflectf(a)→0(f(a), f(b))",3.2. Analyzing Sarkar’s Construction,[0],[0]
"3: θ ← arg(z) {angle of z from x-axis in the plane} 4: for i ∈ {1, . . .",3.2. Analyzing Sarkar’s Construction,[0],[0]
",deg(a)− 1} do 5: yi ← e τ−1 eτ+1 · ( cos ( θ + 2πideg(a) ) , sin ( θ + 2πideg(a)
))",3.2. Analyzing Sarkar’s Construction,[0],[0]
"6: (f(a), f(b), f(c1), . . .",3.2. Analyzing Sarkar’s Construction,[0],[0]
", f(cdeg(a)−1))",3.2. Analyzing Sarkar’s Construction,[0],[0]
"←
reflect0→f(a)(0, z, y1, . . .",3.2. Analyzing Sarkar’s Construction,[0],[0]
", ydeg(x)−1) 7: Output: Embedded H2 vectors f(c1), f(c2), . . .",3.2. Analyzing Sarkar’s Construction,[0],[0]
", f(cdeg(a)−1)
1 + ε.",3.2. Analyzing Sarkar’s Construction,[0],[0]
"For trees, Sarkar’s construction has arbitrarily high fidelity.",3.2. Analyzing Sarkar’s Construction,[0],[0]
"However, this comes at a cost: the scaling τ affects the bits of precision required.",3.2. Analyzing Sarkar’s Construction,[0],[0]
"In fact, we will show that the precision scales logarithmically with the degree of the tree—but linearly with the maximum path length.
",3.2. Analyzing Sarkar’s Construction,[0],[0]
How many bits of precision do we need to represent points in H2?,3.2. Analyzing Sarkar’s Construction,[0],[0]
"If x ∈ H2, then ‖x‖ < 1, so we need sufficiently many bits so that 1− ‖x‖ will not be rounded to zero.",3.2. Analyzing Sarkar’s Construction,[0],[0]
This requires roughly − log(1− ‖x‖) = log 11−‖x‖ bits.,3.2. Analyzing Sarkar’s Construction,[0],[0]
"Say we are embedding two points x, y at distance d. As described in the background, there is an isometric reflection that takes a pair of points (x, y) in H2 to (0, z) while preserving their distance, so without loss of generality we have that
d = dH(x, y) = dH(0, z) = acosh
( 1 + 2 ‖z‖2
1− ‖z‖2
) .
",3.2. Analyzing Sarkar’s Construction,[0],[0]
"Rearranging the terms, we have (cosh(d) + 1)/2 = (1 − ‖z‖2)−1 ≥ (1 − ‖z‖)−1/2.",3.2. Analyzing Sarkar’s Construction,[0],[0]
"Thus, the number of bits we want so that 1 − ‖z‖ will not be rounded to zero is log(cosh(d)+1).",3.2. Analyzing Sarkar’s Construction,[0],[0]
"Since cosh(d) = (exp(d)+exp(−d))/2, this is roughly d bits.",3.2. Analyzing Sarkar’s Construction,[0],[0]
"That is, in hyperbolic space, we need about d bits to express distances of d (rather than log d in Euclidean space).1 This result will be of use below.
",3.2. Analyzing Sarkar’s Construction,[0],[0]
Consider the largest distance in the embeddings produced by Algorithm 1.,3.2. Analyzing Sarkar’s Construction,[0],[0]
"If the longest path length in the tree is `, and each edge has length τ = 1ε ( 2 log degmax π/2 ) , the largest distance is O( `ε log degmax), and we require this number of bits for the representation.
Let us interpret this expression.",3.2. Analyzing Sarkar’s Construction,[0],[0]
"Note that degmax is inside the log term, so that a bushy tree is not penalized much in precision.",3.2. Analyzing Sarkar’s Construction,[0],[0]
"On the other hand, the longest path length ` is not, so that hyperbolic embeddings struggle with long paths.",3.2. Analyzing Sarkar’s Construction,[0],[0]
"Moreover, by selecting an explicit graph, we derive a matching lower bound, concluding that to achieve a dis-
1Although it is particularly easy to bound precision in the Poincaré model, this fact holds generally for hyperbolic space independent of model (shown in the appendix).
",3.2. Analyzing Sarkar’s Construction,[0],[0]
"tortion ε, any construction requires Ω ( ` ε log(degmax) ) bits.",3.2. Analyzing Sarkar’s Construction,[0],[0]
The argument follows from selecting a graph consisting of m(degmax+1) nodes in a tree with a single root and degmax chains each of length m (shown in the appendix).,3.2. Analyzing Sarkar’s Construction,[0],[0]
Our next contribution is a generalization of the construction from the disk H2 to the ball Hr.,3.3. Improving the Construction,[0],[0]
"Our construction follows the same line as Algorithm 1, but since we have r dimensions, the step where we place children spaced out on a circle around their parent now uses a hypersphere.
",3.3. Improving the Construction,[0],[0]
"Spacing out points on the hypersphere is a classic problem known as spherical coding (Conway & Sloane, 1999).",3.3. Improving the Construction,[0],[0]
"As we shall see, the number of children that we can place for a particular angle grows with the dimension.",3.3. Improving the Construction,[0],[0]
"Since the required scaling factor τ gets larger as the angle decreases, we can reduce τ for a particular embedding by increasing the dimension.",3.3. Improving the Construction,[0],[0]
"Note that increasing the dimension helps with bushy trees (large degmax), but has limited effect on tall trees with small degmax.",3.3. Improving the Construction,[0],[0]
"We show
Proposition 3.1.",3.3. Improving the Construction,[0],[0]
"The generalized Hr combinatorial construction has distortion at most 1 + ε and requires at most O( 1ε ` r log degmax) bits to represent a node component for r ≤ (log degmax) + 1, and O( 1ε `) bits for r > (log degmax) + 1.
To generalize to Hr, we replace Step 5 in Algorithm 1 with a node placement step based on coding theory.",3.3. Improving the Construction,[0],[0]
The children are placed at the vertices of a hypercube inscribed into the unit hypersphere (and then scaled by τ ).,3.3. Improving the Construction,[0],[0]
"Each component of a hypercube vertex has the form ±1√
r .",3.3. Improving the Construction,[0],[0]
"We index these
points using binary sequences a ∈ {0, 1}r in the following way: xa = ( (−1)a1√ r , (−1) a2 √ r , . . .",3.3. Improving the Construction,[0],[0]
", (−1) ar √ r ) .",3.3. Improving the Construction,[0],[0]
We space out the children by controlling the distances by selecting a set of binary sequences a with a prescribed minimum Hamming distance—a binary error-correcting code—and placing the children at the resulting hypercube vertices.,3.3. Improving the Construction,[0],[0]
"We provide more details, including our choice of code in the appendix.",3.3. Improving the Construction,[0],[0]
We revisit the first step of the construction: embedding graphs into trees.,3.4. Embedding into Trees,[0],[0]
"There are fundamental limits to how well graphs can be embedded into trees; in general, breaking long cycles inevitably adds distortion, as shown in Figure 2.",3.4. Embedding into Trees,[0],[0]
"We are inspired by a measure of this limit, the δ-4 points condition introduced in Abraham et al. (2007).",3.4. Embedding into Trees,[0],[0]
A graph on n nodes that satisfies the δ-4 points condition has distortion at most (1 + δ)c1 logn for some constant c1.,3.4. Embedding into Trees,[0],[0]
"This result enables our end-to-end embedding to achieve a distortion of at most D(f) ≤ (1 + δ)c1 logn(1 + ε).
",3.4. Embedding into Trees,[0],[0]
"The result in Abraham et al. (2007) builds a tree with Steiner
nodes.",3.4. Embedding into Trees,[0],[0]
These additional nodes can help control the distances in the resulting weighted tree (Figure 2).,3.4. Embedding into Trees,[0],[0]
"Note that Algorithm 1 readily extends to the case of weighted trees.
",3.4. Embedding into Trees,[0],[0]
"In summary, the key takeaways of our analysis are:
•",3.4. Embedding into Trees,[0],[0]
"There is a fundamental tension between precision and quality in hyperbolic embeddings.
",3.4. Embedding into Trees,[0],[0]
"• Hyperbolic embeddings have an exponential advantage in space compared to Euclidean embeddings for short, bushy hierarchies, but will have less of an advantage for graphs that contain long paths.
",3.4. Embedding into Trees,[0],[0]
• Choosing an appropriate scaling factor τ is critical for quality.,3.4. Embedding into Trees,[0],[0]
"Later, we will propose to learn this scale factor automatically for computing embeddings in PyTorch.
",3.4. Embedding into Trees,[0],[0]
• Steiner nodes can help improve embeddings of graphs.,3.4. Embedding into Trees,[0],[0]
"In this section, we explore a fundamental and more general question than we did in the previous section: if we are given the pairwise distances arising from a set of points in hyperbolic space, can we recover the points?",4. Hyperbolic Multidimensional Scaling,[0],[0]
This enables us to produce an embedding for a desired distance metric.,4. Hyperbolic Multidimensional Scaling,[0],[0]
The equivalent problem for Euclidean distances is solved with multidimensional scaling (MDS).,4. Hyperbolic Multidimensional Scaling,[0],[0]
The goal of this section is to analyze the hyperbolic MDS (h-MDS) problem.,4. Hyperbolic Multidimensional Scaling,[0],[0]
"We describe and overcome the additional technical challenges imposed by hyperbolic distances, and show that exact recovery is possible and interpretable.",4. Hyperbolic Multidimensional Scaling,[0],[0]
Afterwards we propose a technique for dimensionality reduction using principal geodesics analysis (PGA) that provides optimization guarantees.,4. Hyperbolic Multidimensional Scaling,[0],[0]
"In particular, this addresses the shortcomings of h-MDS when recovering points that do not exactly lie on a hyperbolic manifold.",4. Hyperbolic Multidimensional Scaling,[0],[0]
"Suppose that there is a set of hyperbolic points x1, . . .",4.1. Exact Hyperbolic MDS,[0],[0]
", xn ∈ Hr, embedded in the Poincaré ball and written X ∈ Rn×r in matrix form.",4.1. Exact Hyperbolic MDS,[0],[0]
"We observe all the pairwise distances di,j = dH(xi, xj), but do not observe X: our goal is to use the observed di,j’s to recover X (or some other set of points with the same pairwise distances di,j).
",4.1. Exact Hyperbolic MDS,[0],[0]
The MDS algorithm in the Euclidean setting makes an important centering2 assumption: the points have mean 0.,4.1. Exact Hyperbolic MDS,[0],[0]
"If an exact embedding for the distances exists, it can be recovered from a matrix factorization.",4.1. Exact Hyperbolic MDS,[0],[0]
"In other words, Euclidean MDS always recovers a centered embedding.
",4.1. Exact Hyperbolic MDS,[0],[0]
"In hyperbolic space, the same algorithm does not work, but we show that it is possible to find an embedding centered at a different mean.",4.1. Exact Hyperbolic MDS,[0],[0]
"More precisely, we introduce a new mean which we call the pseudo-Euclidean mean, that behaves like the Euclidean mean in that it enables recovery through matrix factorization.",4.1. Exact Hyperbolic MDS,[0],[0]
"Once the points are recovered in hyperbolic space, they can be recentered around a more canonical mean by translating it to the origin.
",4.1. Exact Hyperbolic MDS,[0],[0]
"Algorithm 2 is our complete algorithm, and for the remainder of this section we will describe how and why it works.",4.1. Exact Hyperbolic MDS,[0],[0]
"We first describe the hyperboloid model, an alternate but equivalent model of hyperbolic geometry in which h-MDS is simpler.",4.1. Exact Hyperbolic MDS,[0],[0]
"Of course, we can easily convert between the hyperboloid model and the Poincaré ball model.",4.1. Exact Hyperbolic MDS,[0],[0]
"Next, we show how to reduce the problem to a standard PCA problem, which recovers an embedding centered at the points’ pseudo-Euclidean mean.",4.1. Exact Hyperbolic MDS,[0],[0]
"Finally, we discuss the meaning and implications of centering and prove that the algorithm preserves submanifolds as well—that is, if there is an exact embedding in k < r dimensions centered at their canonical mean, then our algorithm will recover it.
",4.1. Exact Hyperbolic MDS,[0],[0]
The hyperboloid model Define Q to be the diagonal matrix in Rr+1 where Q00 = 1 and Qii = −1 for i > 0.,4.1. Exact Hyperbolic MDS,[0],[0]
"For a vector x ∈ Rr+1, xTQx is called the Minkowski quadratic form.",4.1. Exact Hyperbolic MDS,[0],[0]
"The hyperboloid model is defined as
Mr = { x ∈ Rr+1 ∣∣xTQx = 1 ∧ x0 > 0} , which is endowed with a distance measure dH(x, y) = acosh(xTQy).",4.1. Exact Hyperbolic MDS,[0],[0]
"For convenience, for x ∈Mr let x0 denote 0th coordinate eT0 x, and ~x ∈",4.1. Exact Hyperbolic MDS,[0],[0]
Rr denote the rest of the coordinates3.,4.1. Exact Hyperbolic MDS,[0],[0]
"With this notation, the Minkowski bilinear form can be written xTQy =",4.1. Exact Hyperbolic MDS,[0],[0]
x0y0,4.1. Exact Hyperbolic MDS,[0],[0]
"− ~xT~y.
2We say that points are centered at a particular mean if this mean is at 0.",4.1. Exact Hyperbolic MDS,[0],[0]
"The act of centering refers to applying an isometry that makes the mean of the points 0.
",4.1. Exact Hyperbolic MDS,[0],[0]
"3Since x0 = √
1 + ‖~x‖2 is just a function of ~x, we can equivalently consider just ~x as being a member of a model of hyperbolic space: This representation is sometimes known as the Gans model.
",4.1. Exact Hyperbolic MDS,[0],[0]
"A new mean Given points x1, x2, . . .",4.1. Exact Hyperbolic MDS,[0],[0]
", xn",4.1. Exact Hyperbolic MDS,[0],[0]
"∈Mr in hyperbolic space, define a variance term
Ψ(z;x1, x2, . . .",4.1. Exact Hyperbolic MDS,[0],[0]
", xn) = n∑ i=1",4.1. Exact Hyperbolic MDS,[0],[0]
"sinh2(dH(xi, z)).
",4.1. Exact Hyperbolic MDS,[0],[0]
We define a pseudo-Euclidean mean to be any local minimum of this expression.,4.1. Exact Hyperbolic MDS,[0],[0]
"Notice that this is independent of any particular model of hyperbolic space, since it is defined only through the hyperbolic distance function dH .",4.1. Exact Hyperbolic MDS,[0],[0]
Lemma 4.1.,4.1. Exact Hyperbolic MDS,[0],[0]
Define X ∈ Rn×r such that XT ei = ~xi and u ∈,4.1. Exact Hyperbolic MDS,[0],[0]
"Rn such that ui = x0,i.",4.1. Exact Hyperbolic MDS,[0],[0]
"Then
∇~zΨ(z;x1, x2, . . .",4.1. Exact Hyperbolic MDS,[0],[0]
", xn)|~z=0 = −2 n∑ i=1",4.1. Exact Hyperbolic MDS,[0],[0]
"x0,i~xi = −2XTu.
",4.1. Exact Hyperbolic MDS,[0],[0]
This means that 0 is a pseudo-Euclidean mean if and only if 0 = XTu.,4.1. Exact Hyperbolic MDS,[0],[0]
"Call some hyperbolic points x1, . . .",4.1. Exact Hyperbolic MDS,[0],[0]
", xn pseudoEuclidean centered if their average is 0 in this sense: i.e. if XTu = 0.",4.1. Exact Hyperbolic MDS,[0],[0]
"We can always center a set of points without affecting their pairwise distances by simply finding their average, and then sending it to 0 through an isometry.
",4.1. Exact Hyperbolic MDS,[0],[0]
"Recovery via matrix factorization Suppose we observe the pairwise distances dH(xi, xj) of points x1, x2, . . .",4.1. Exact Hyperbolic MDS,[0],[0]
", xn ∈Mr.",4.1. Exact Hyperbolic MDS,[0],[0]
"This gives the matrix Y such that
Yi,j = cosh (dH(xi, xj))",4.1. Exact Hyperbolic MDS,[0],[0]
"= x0,ix0,j",4.1. Exact Hyperbolic MDS,[0],[0]
− ~xiT ~xj .,4.1. Exact Hyperbolic MDS,[0],[0]
"(1)
DefiningX and u as in Lemma 4.1, then in matrix form Y = uuT−XXT .",4.1. Exact Hyperbolic MDS,[0],[0]
"Without loss of generality, suppose that the xi are centered at their pseudo-Euclidean mean, so thatXTu = 0 by Lemma 4.1.",4.1. Exact Hyperbolic MDS,[0],[0]
"This implies that u is an eigenvector of Y with positive eigenvalue, and the rest of Y ’s eigenvalues are negative.",4.1. Exact Hyperbolic MDS,[0],[0]
"Therefore an eigendecomposition of Y will find u, X̂ such that Y = uuT − X̂X̂T , i.e. it will directly recover X up to rotation.
",4.1. Exact Hyperbolic MDS,[0],[0]
"In fact, running PCA on −Y = XTX − uuT to find the n most significant non-negative eigenvectors will recover X up to rotation, and then u can be found by leveraging the fact that x0 = √ 1 + ‖~x‖2.",4.1. Exact Hyperbolic MDS,[0],[0]
"This leads to Algorithm 2, with optional post-processing steps for converting the embedding to the Poincaré ball model and for re-centering the points.
",4.1. Exact Hyperbolic MDS,[0],[0]
"A word on centering The MDS algorithm in Euclidean geometry returns points centered at their Karcher mean z, which is a point minimizing ∑ d2(z, xi) (where d is the distance metric).",4.1. Exact Hyperbolic MDS,[0],[0]
"The Karcher center is important for interpreting dimensionality reduction; we use the analogous hyperbolic Karcher mean for PGA in Section 4.2.
",4.1. Exact Hyperbolic MDS,[0],[0]
"Although Algorithm 2 returns points centered at their pseudo-Euclidean mean instead of their Karcher mean, they can be easily recentered by finding their Karcher mean and
Algorithm 2 1: Input: Distance matrix di,j and rank r 2: Compute scaled distance matrix Yi,j = cosh(di,j) 3: X → PCA(−Y, r) 4: Project X from hyperboloid model to Poincaré model: x→ x
1+ √ 1+‖x‖2
5:",4.1. Exact Hyperbolic MDS,[0],[0]
"If desired, centerX at a different mean (e.g. the Karcher mean) 6: return X
reflecting it onto the origin.",4.1. Exact Hyperbolic MDS,[0],[0]
"Furthermore, Algorithm 2 preserves the dimension of the embedding:
Lemma 4.2.",4.1. Exact Hyperbolic MDS,[0],[0]
"If a set of points lie in a dimension-k geodesic submanifold, then both their Karcher mean and their pseudo-Euclidean mean lie in the same submanifold.
",4.1. Exact Hyperbolic MDS,[0],[0]
"This implies that centering with the pseudo-Euclidean mean preserves geodesic submanifolds: If it is possible to embed distances in a dimension-k geodesic submanifold centered and rooted at a Karcher mean, then it is also possible to embed the distances in a dimension-k submanifold centered and rooted at a pseudo-Euclidean mean, and vice versa.",4.1. Exact Hyperbolic MDS,[0],[0]
"Given a high-rank embedding (resulting from h-MDS, for example), we may wish to find a lower-rank version.",4.2. Reducing Dimensionality with PGA,[0],[0]
"In Euclidean space, one can get the optimal lower rank embedding by simply discarding components.",4.2. Reducing Dimensionality with PGA,[0],[0]
"However, this may not be the case in hyperbolic space.",4.2. Reducing Dimensionality with PGA,[0],[0]
"Motivated by this, we study dimensionality reduction in hyperbolic space.
",4.2. Reducing Dimensionality with PGA,[0],[0]
"As hyperbolic space does not have a linear subspace structure like Euclidean space, we need to define what we mean by lower-dimensional.",4.2. Reducing Dimensionality with PGA,[0],[0]
"We follow Principal Geodesic Analysis (Fletcher et al., 2004), (Huckemann et al., 2010).",4.2. Reducing Dimensionality with PGA,[0],[0]
"Consider an initial embedding with points x1, . . .",4.2. Reducing Dimensionality with PGA,[0],[0]
", xn ∈ H2 and let dH :",4.2. Reducing Dimensionality with PGA,[0],[0]
H2 × H2 → R+ be the hyperbolic distance.,4.2. Reducing Dimensionality with PGA,[0],[0]
Suppose we want to map this embedding onto a one-dimensional subspace.,4.2. Reducing Dimensionality with PGA,[0],[0]
"(Note that we are considering a two-dimensional embedding and one-dimensional subspace here for simplicity, and these results immediately extend to higher dimensions.)",4.2. Reducing Dimensionality with PGA,[0],[0]
"In this case, the goal of PGA is to find a geodesic γ :",4.2. Reducing Dimensionality with PGA,[0],[0]
"[0, 1] → H2 that passes through the mean of the points and that minimizes the squared error (or variance): f(γ) = ∑n i=1 mint∈[0,1] dH(γ(t), xi) 2.
",4.2. Reducing Dimensionality with PGA,[0],[0]
This expression can be simplified significantly and reduced to a minimization in Euclidean space.,4.2. Reducing Dimensionality with PGA,[0],[0]
"First, we find the mean of the points, the point x̄ which minimizes∑n i=1",4.2. Reducing Dimensionality with PGA,[0],[0]
"dH(x̄, xi)
2.4",4.2. Reducing Dimensionality with PGA,[0],[0]
"Next, we reflect all the points xi so that their mean is 0 in the Poincaré disk model; we can
4The derivative of the hyperbolic distance has a singularity, that is, limy→x ∂x|dH(x, y)| → ∞ for any x ∈ H.",4.2. Reducing Dimensionality with PGA,[0],[0]
"This issue can
do this using a circle inversion that maps x̄ onto 0",4.2. Reducing Dimensionality with PGA,[0],[0]
"Since reflections are isometric, if γ is a line through 0 and Rγ is the reflection across γ, we have that dH(γ, x) = mint∈[0,1] dH(γ(t), x) = 1 2dH(Rlx, x).
",4.2. Reducing Dimensionality with PGA,[0],[0]
"Combining this with the Euclidean reflection formula and the hyperbolic metric produces
f(γ) = 1
4 n∑ i=1 acosh2",4.2. Reducing Dimensionality with PGA,[0],[0]
"( 1 + 8dE(γ, xi) 2 (1− ‖xi‖2)2 ) ,
in which dE is the Euclidean distance from a point to a line.",4.2. Reducing Dimensionality with PGA,[0],[0]
If we define wi = √ 8xi/(1,4.2. Reducing Dimensionality with PGA,[0],[0]
− ‖xi‖2),4.2. Reducing Dimensionality with PGA,[0],[0]
this reduces to the simplified expression f(γ),4.2. Reducing Dimensionality with PGA,[0],[0]
"= 1 4 ∑n i=1 acosh 2 ( 1 + dE(γ,wi) 2 ) .
",4.2. Reducing Dimensionality with PGA,[0],[0]
Notice that the loss function is not convex.,4.2. Reducing Dimensionality with PGA,[0],[0]
"We observe that there can be multiple local minima that are attractive and stable, in contrast to PCA.",4.2. Reducing Dimensionality with PGA,[0],[0]
Figure 3 illustrates this nonconvexity on a simple dataset in H2 with only four examples.,4.2. Reducing Dimensionality with PGA,[0],[0]
"This makes globally optimizing the objective difficult.
",4.2. Reducing Dimensionality with PGA,[0],[0]
"Nevertheless, there will always be a region Ω containing a global optimum γ∗ that is convex and admits an efficient projection, and where f is convex when restricted to Ω.",4.2. Reducing Dimensionality with PGA,[0],[0]
"Thus it is possible to build a gradient descent-based algorithm to recover lower-dimensional subspaces: for example, we built a simple optimizer in PyTorch.",4.2. Reducing Dimensionality with PGA,[0],[0]
"We also give a sufficient condition on the data for f above to be convex.
",4.2. Reducing Dimensionality with PGA,[0],[0]
Lemma 4.3.,4.2. Reducing Dimensionality with PGA,[0],[0]
"For hyperbolic PGA if for all i,
acosh2 ( 1 + dE(γ,wi) 2 ) < min ( 1, 1
3 ‖wi‖2 ) then f is locally convex at γ.
be mitigated by minimizing d2H , which does have a continuous derivative throughout H. The use of dH(x, y) is a minor instability in Nickel & Kiela (2017); Chamberlain et al. (2017)’s formulation, necessitating guarding against NANs.",4.2. Reducing Dimensionality with PGA,[0],[0]
"We discuss this further in the appendix.
",4.2. Reducing Dimensionality with PGA,[0],[0]
"As a result, if we initialize in and optimize over a region that contains γ∗ and where the condition of Lemma 4.3 holds, then gradient descent will be guaranteed to converge to γ∗.",4.2. Reducing Dimensionality with PGA,[0],[0]
"We can turn this result around and read it as a recovery result: if the noise is bounded in this regime, then we are able to provably recover the correct low-dimensional embedding.",4.2. Reducing Dimensionality with PGA,[0],[0]
We evaluate the proposed approaches and compare against existing methods.,5. Experiments,[0],[0]
"We hypothesize that for tree-like data, the combinatorial construction offers the best performance.",5. Experiments,[0],[0]
"For general data, we expect h-MDS to produce the lowest distortion, while it may have low MAP due to precision limitations.",5. Experiments,[0],[0]
We anticipate that dimension is a critical factor (outside of the combinatorial construction).,5. Experiments,[0],[0]
"In the appendix, we report on additional datasets, combinatorial construction parameters, and the effect of hyperparameters.
",5. Experiments,[0],[0]
"Datasets We consider trees, tree-like hierarchies, and graphs that are not tree-like.",5. Experiments,[0],[0]
"Trees include fully-balanced and phylogenetic trees expressing genetic heritage (Hofbauer et al., 2016), available at Sanderson et al. (1994).",5. Experiments,[0],[0]
"Nearly tree-like hierarchies include the WordNet hypernym graph (the largest connected component from Nickel & Kiela (2017)) and a graph of Ph.D. advisor-advisee relationships (De Nooy et al., 2011).",5. Experiments,[0],[0]
"Also included are datasets
that vary in their tree nearness, such as disease relationships (Goh et al., 2007) and protein interactions (Jeong et al., 2001), both available from Rossi & Ahmed (2015).",5. Experiments,[0],[0]
"We also include the general relativity and quantum cosmology (GrQC) arXiv collaboration network (Leskovec et al., 2007).
",5. Experiments,[0],[0]
Approaches Combinatorial embeddings into H2 use the ε = 0.1 precision setting; others are considered in the Appendix.,5. Experiments,[0],[0]
We performed h-MDS in floating point precision.,5. Experiments,[0],[0]
"We include results for our PyTorch implementation (PT) of an SGD-based algorithm (described later), and a warm start version (PWS) initialized with the high-dimensional combinatorial construction.",5. Experiments,[0],[0]
"We compare against classical MDS (i.e., PCA), and the optimization-based approach Nickel & Kiela (2017), which we call FB.",5. Experiments,[0],[0]
"The experiments for h-MDS, PyTorch SGD, PCA, and FB used dimensions of 2,5,10,50,100,200; we recorded the best resulting MAP and distortion.",5. Experiments,[0],[0]
"Due to the large scale, we did not replicate the best FB numbers on large graphs (i.e., Gr-QC and WordNet); we report their best published MAP numbers (their work does not report distortion).",5. Experiments,[0],[0]
These entries are marked with an asterisk.,5. Experiments,[0],[0]
"For the WordNet graph, FB uses the transitive closure; a weighted version of the graph captures the ancestor relationships.",5. Experiments,[0],[0]
"The full details are in appendix.
",5. Experiments,[0],[0]
"Quality In Table 3 (left), we report the distortion.",5. Experiments,[0],[0]
"As expected, for tree or tree-like graphs, the combinatorial construction has exceedingly low distortion.",5. Experiments,[0],[0]
"Because h-MDS is meant to recover points exactly, we hypothesized that h-MDS would offer very low distortion on these datasets.",5. Experiments,[0],[0]
"Table 3 confirms this: among h-MDS, PCA, and FB, hMDS consistently offers the lowest distortion, producing, for example, a distortion of 0.039 on the phylogenetic tree.",5. Experiments,[0],[0]
We observe that floating point h-MDS struggles with MAP.,5. Experiments,[0],[0]
"We separately confirmed that this is due to precision (by
using a high-precision solver).",5. Experiments,[0],[0]
"The optimization-based approach is bolstered by appropriate initialization from the combinatorial construction.
",5. Experiments,[0],[0]
"Table 3 (right) reports the MAP measure (we additionally include WordNet results in Table 2), which is a local measure.",5. Experiments,[0],[0]
"We confirm that the combinatorial construction performs well for tree-like hierarchies, where MAP is close to 1.",5. Experiments,[0],[0]
The construction improves on approaches such as FB that rely on optimization.,5. Experiments,[0],[0]
"On larger graphs like WordNet, our approach yields a MAP of 0.989—while their WordNet MAP result is 0.870 at 200 dimensions.",5. Experiments,[0],[0]
"This is exciting, as our approach is deterministic and linear-time.
",5. Experiments,[0],[0]
A refined understanding of hyperbolic embeddings may be used to improve the quality and runtime of extant algorithms.,5. Experiments,[0],[0]
"Indeed, we embedded WordNet entity-relationship-entity triples (Socher et al., 2013) using the combinatorial construction in 10 dimensions, accurately preserving relationship knowledge (Table 4).",5. Experiments,[0],[0]
"This suggests that hyperbolic embeddings are effective at compressing knowledge and may useful for knowledge base completion and Q/A tasks.
SGD-Based Algorithm We built an SGD-based algorithm implemented in PyTorch.",5. Experiments,[0],[0]
"The loss function is equivalent to the PGA loss, and so is continuously differentiable.
",5. Experiments,[0],[0]
"To evaluate our algorithm’s ability to deal with incomplete information, we sample the distance matrix at a ratio of nonedges to edges at 10 : 1 following Nickel & Kiela (2017).",5. Experiments,[0],[0]
"In Figure 4, we recover a good solution for the phylogenetic tree with a small fraction of the entries; for example, we sampled approximately 4% of the graph for a MAP of 0.74 and distortion of 0.6.",5. Experiments,[0],[0]
We also considered learning the scale of the embedding (details in the appendix).,5. Experiments,[0],[0]
"Finally, all of our techniques scale to graphs with millions of nodes.",5. Experiments,[0],[0]
Hyperbolic embeddings embed hierarchical information with high fidelity and few dimensions.,6. Conclusion and Future Work,[0],[0]
"We explored the limits of this approach by describing scalable, high quality algorithms.",6. Conclusion and Future Work,[0],[0]
We hope the techniques here encourage more follow-on work on the exciting techniques of Nickel & Kiela (2017); Chamberlain et al. (2017).,6. Conclusion and Future Work,[0],[0]
Thanks to Alex Ratner and Avner May for helpful discussion and to Beliz Gunel and Sen Wu for assistance with experiments.,Acknowledgements,[0],[0]
"We gratefully acknowledge the support of DARPA under No. FA87501720095 and FA87501320039, ONR under No. N000141712266, the Moore Foundation, Okawa Research Grant, American Family Insurance, Accenture, Toshiba, the Secure Internet of Things Project, Google, VMware, Qualcomm, Ericsson, Analog Devices, and members of the Stanford DAWN project: Intel, Microsoft, Teradata, and VMware.",Acknowledgements,[0],[0]
The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon.,Acknowledgements,[0],[0]
"Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of DARPA, DOE, NIH, ONR, or the U.S. Government.",Acknowledgements,[0],[0]
Hyperbolic embeddings offer excellent quality with few dimensions when embedding hierarchical data structures.,abstractText,[0],[0]
We give a combinatorial construction that embeds trees into hyperbolic space with arbitrarily low distortion without optimization.,abstractText,[0],[0]
"On WordNet, this algorithm obtains a meanaverage-precision of 0.989 with only two dimensions, outperforming existing work by 0.11 points.",abstractText,[0],[0]
We provide bounds characterizing the precisiondimensionality tradeoff inherent in any hyperbolic embedding.,abstractText,[0],[0]
"To embed general metric spaces, we propose a hyperbolic generalization of multidimensional scaling (h-MDS).",abstractText,[0],[0]
"We show how to perform exact recovery of hyperbolic points from distances, provide a perturbation analysis, and give a recovery result that enables us to reduce dimensionality.",abstractText,[0],[0]
"Finally, we extract lessons from the algorithms and theory above to design a scalable PyTorch-based implementation that can handle incomplete information.",abstractText,[0],[0]
Representation Tradeoffs for Hyperbolic Embeddings,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 613–622 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1057",text,[0],[0]
Speech recognition is one of the success stories of language technology.,1 Introduction,[0],[0]
It works remarkably well in a range of practical settings.,1 Introduction,[0],[0]
"However, this success relies on the use of very heavy supervision where the machine is fed thousands of hours of painstakingly transcribed audio speech signal.",1 Introduction,[0],[0]
Humans are able to learn to recognize and understand speech from notably weaker and noisier supervision: they manage to learn to extract structure and meaning from speech by simply being exposed to utterances situated and grounded in their daily sensory experience.,1 Introduction,[0],[0]
"Modeling and emulating this remarkable skill has been the goal of numerous studies; however in the overwhelming majority of cases researchers used severely simplified settings where either the language input or the extralinguistic sensory input, or both, are small scale and symbolically represented.",1 Introduction,[0],[0]
"Section 2 provides a brief overview of this research.
",1 Introduction,[0],[0]
More recently several lines of work have moved towards more realistic inputs while modeling or emulating language acquisition in a grounded setting.,1 Introduction,[0],[0]
"Gelderloos and Chrupała (2016) use the image captioning dataset MS COCO (Lin et al., 2014) to mimic the setting of grounded language learning: the sensory input consists of images of natural scenes, while the language input are phonetically transcribed descriptions of these scenes.",1 Introduction,[0],[0]
"The use of such moderately large and low-level data allows the authors to train a multi-layer recurrent neural network model, and to explore the nature and localization of the emerging hierarchy of linguistic representations learned in the process.",1 Introduction,[0],[0]
"Furthermore, in a series of recent studies Harwath and Glass (2015); Harwath et al. (2016); Harwath and Glass (2017) use image captioning datasets to model learning to understand spoken language from visual context with convolutional neural network models.",1 Introduction,[0],[0]
"Finally, there is a small but growing body of work dedicated to elucidating the nature of representations learned by neural networks from language data (see Section 2.2 for a brief overview).",1 Introduction,[0],[0]
"In the current work we build on these three strands of research and contribute the following advances:
• We use a multi-layer gated recurrent neural network to properly model the temporal nature of speech signal and substantially improve performance compared to the convolutional architecture from Harwath and Glass (2015); • We carry out an in-depth analysis of the representations used by different components of the trained model and correlate them to representations learned by a text-based model and to human patterns of judgment on linguistic stimuli.",1 Introduction,[0],[0]
"This analysis is especially novel for a model with speech signal as input.
",1 Introduction,[0],[0]
"The general pattern of findings in our analysis is
613
as follows:",1 Introduction,[0],[0]
"The model learns to extract from the acoustic input both form-related and semanticsrelated information, and encodes it in the activations of the hidden layers.",1 Introduction,[0],[0]
Encoding of semantic aspects tends to become richer as we go up the hierarchy of layers.,1 Introduction,[0],[0]
"Meanwhile, encoding of formrelated aspects of the language input, such as utterance length or the presence of specific words, tends to initially increase and then decay.
",1 Introduction,[0],[0]
"We release the code for our models and analyses as open source, available at https://github.com/gchrupala/visually-groundedspeech.",1 Introduction,[0],[0]
"We also release a dataset of synthetically spoken image captions based on MS COCO, available at https://doi.org/10.5281/zenodo.400926.",1 Introduction,[0],[0]
Children learn to recognize and assign meaning to words from continuous perceptual data in extremely noisy context.,2 Related work,[0],[0]
"While there have been many computational studies of human word meaning acquisition, they typically make strong simplifying assumptions about the nature of the input.",2 Related work,[0],[0]
"Often language input is given in the form of word symbols, and the context consists of a set of symbols representing possible referents (e.g. Siskind, 1996; Frank et al., 2007; Fazly et al., 2010).",2 Related work,[0],[0]
"In contrast, several studies presented models that learn from sensory rather than symbolic input, which is rich with regards to the signal itself, but very limited in scale and variation (e.g. Roy and Pentland, 2002; Yu and Ballard, 2004; Lazaridou et al., 2016).",2 Related work,[0],[0]
Chrupała et al. (2015) introduce a model that learns to predict the visual context from image captions.,2.1 Multimodal language acquisition,[0],[0]
"The model is trained on image-caption pairs from MSCOCO (Lin et al., 2014), capturing both rich visual input as well as larger scale input, but the language input still consists of word symbols.",2.1 Multimodal language acquisition,[0],[0]
"Gelderloos and Chrupała (2016) propose a similar architecture that instead takes phonemelevel transcriptions as language input, thereby incorporating the word segmentation problem into the learning task.",2.1 Multimodal language acquisition,[0],[0]
"In this work, we introduce an architecture that learns from continuous speech and images directly.
",2.1 Multimodal language acquisition,[0],[0]
This work is related to research on visual grounding of language.,2.1 Multimodal language acquisition,[0],[0]
"The field is large and growing, with most work dedicated to the ground-
ing of written text, particularly in image captioning tasks (see Bernardi et al. (2016) for an overview).",2.1 Multimodal language acquisition,[0],[0]
"However, learning to ground language to visual information is also interesting from an automatic speech recognition point of view.",2.1 Multimodal language acquisition,[0],[0]
"Potentially, ASR systems could be trained from naturally co-occurring visual context information, without the need for extensive manual annotation – a particularly promising prospect for speech recognition in low-resource languages.",2.1 Multimodal language acquisition,[0],[0]
There have been several attempts along these lines.,2.1 Multimodal language acquisition,[0],[0]
Synnaeve et al. (2014) present a method of learning to recognize spoken words in isolation from cooccurrence with image fragments.,2.1 Multimodal language acquisition,[0],[0]
"Harwath and Glass (2015) present a model that learns to map pre-segmented spoken words in sequence to aspects of the visual context, while in Harwath and Glass (2017)",2.1 Multimodal language acquisition,[0],[0]
"the model also learns to recognize words in the unsegmented signal.
",2.1 Multimodal language acquisition,[0],[0]
"Most closely related to our work is that of Harwath et al. (2016), as it presents an architecture that learns to project images and unsegmented spoken captions to the same embedding space.",2.1 Multimodal language acquisition,[0],[0]
The sentence representation is obtained by feeding the spectrogram to a convolutional network.,2.1 Multimodal language acquisition,[0],[0]
"The architecture is trained on crowd-sourced spoken captions for images from the Places dataset (Zhou et al., 2014), and evaluated on image search and caption retrieval.",2.1 Multimodal language acquisition,[0],[0]
Unfortunately this dataset is not currently available and we were thus unable to directly compare the performance of our model to Harwath et al. (2016).,2.1 Multimodal language acquisition,[0],[0]
We do compare to Harwath and Glass (2015) which was tested on a public dataset.,2.1 Multimodal language acquisition,[0],[0]
"We make different architectural choices, as our models are based on recurrent highway networks (Zilly et al., 2016).",2.1 Multimodal language acquisition,[0],[0]
"As in human cognition, speech is processed incrementally.",2.1 Multimodal language acquisition,[0],[0]
This also allows our architecture to integrate information sequentially from speech of arbitrary duration.,2.1 Multimodal language acquisition,[0],[0]
"While analysis of neural methods in NLP is often limited to evaluation of the performance on the training task, recently methods have been introduced to peek inside the black box and explore what it is that enables the model to perform the task.",2.2 Analysis of neural representations,[0],[0]
"One approach is to look at the contribution of specific parts of the input, or specific units in the model, to final representations or decisions.",2.2 Analysis of neural representations,[0],[0]
"Kádár et al. (2016) propose omission scores, a method to estimate the contribution of input tokens to the fi-
nal representation by removing them from the input and comparing the resulting representations to the ones generated by the original input.",2.2 Analysis of neural representations,[0],[0]
"In a similar approach, Li et al. (2016) study the contribution of individual input tokens as well as hidden units and word embedding dimensions by erasing them from the representation and analyzing how this affects the model.
",2.2 Analysis of neural representations,[0],[0]
Miao et al. (2016) and Tang et al. (2016) use visualization techniques for fine-grained analysis of GRU and LSTM models for ASR.,2.2 Analysis of neural representations,[0],[0]
"Visualization of input and forget gate states allows Miao et al. (2016) to make informed adaptations to gated recurrent architectures, resulting in more efficiently trainable models.",2.2 Analysis of neural representations,[0],[0]
"Tang et al. (2016) visualize qualitative differences between LSTM- and GRUbased architectures, regarding the encoding of information, as well as how it is processed through time.
",2.2 Analysis of neural representations,[0],[0]
We specifically study linguistic properties of the information encoded in the trained model.,2.2 Analysis of neural representations,[0],[0]
"Adi et al. (2016) introduce prediction tasks to analyze information encoded in sentence embeddings about word order, sentence length, and the presence of individual words.",2.2 Analysis of neural representations,[0],[0]
We use related techniques to explore encoding of aspects of form and meaning within components of our stacked architecture.,2.2 Analysis of neural representations,[0],[0]
"We use a multi-layer, gated recurrent neural network (RHN) to model the temporal nature of speech signal.",3 Models,[0],[0]
"Recurrent neural networks are designed for modeling sequential data, and gated variants (GRUs, LSTMs) are widely used with speech and text in both cognitive modeling and engineering contexts.",3 Models,[0],[0]
"RHNs are a simple generalization of GRU networks such that the transform between time points can consist of several steps.
",3 Models,[0],[0]
Our multimodal model projects spoken utterances and images to a joint semantic space.,3 Models,[0],[0]
The idea of projecting different modalities to a shared semantic space via a pair of encoders has been used in work on language and vision (among them Vendrov et al. (2015)).,3 Models,[0],[0]
"The core idea is to encourage inputs representing the same meaning in different modalities to end up nearby, while maintaining a distance from unrelated inputs.
",3 Models,[0],[0]
"The model consists of two parts: an utterance encoder, and an image encoder.",3 Models,[0],[0]
"The utterance encoder starts from MFCC speech features, while
the image encoder starts from features extracted with a VGG-16 pre-trained on ImageNet.",3 Models,[0],[0]
"Our loss function attempts to make the cosine distance between encodings of matching utterances and images greater than the distance between encodings of mismatching utterance/image pairs, by a margin:
(1)
∑
u,i
(∑
u′ max[0, α+d(u, i)−d(u′, i)]
+ ∑
i′ max[0, α+ d(u, i)− d(u, i′)]
)
where d(u, i) is the cosine distance between the encoded utterance u and encoded image i. Here (u, i) is the matching utterance-image pair, u′ ranges over utterances not describing i and i′ ranges over images not described by u.",3 Models,[0],[0]
"The image encoder enci is a simple linear projection, followed by normalization to unit L2 norm:
enci(i) = unit(Ai+ b) (2)
where unit(x) = x (xT x)0.5 and with (A, b) as learned parameters.",3 Models,[0],[0]
"The utterance encoder encu consists of a 1-dimensional convolutional layer of length s, size d and stride z, whose output feeds into a Recurrent Highway Network with k layers and L microsteps, whose output in turn goes through an attention-like lookback operator, and finally L2 normalization:
encu(u) = unit(Attn(RHNk,L(Convs,d,z(u))))",3 Models,[0],[0]
"(3)
The main function of the convolutional layer Convs,d,z is to subsample the input along the temporal dimension.",3 Models,[0],[0]
We use a 1-dimensional convolution with full border mode padding.,3 Models,[0],[0]
"The attention operator simply computes a weighted sum of the RHN activation at all timesteps:
Attn(x) = ∑
t
αtxt (4)
where the weights αt are determined by learned parameters U and W, and passed through the timewise softmax function:
αt = exp(U tanh(Wxt))∑ t′ exp(U tanh(Wxt′))
(5)
",3 Models,[0],[0]
"The main component of the utterance encoder is a recurrent network, specifically a Recurrent Highway Network (Zilly et al., 2016).",3 Models,[0],[0]
"The idea behind
RHN is to increase the depth of the transform between timesteps, or the recurrence depth.",3 Models,[0],[0]
Otherwise they are a type of gated recurrent networks.,3 Models,[0],[0]
"The transition from timestep t − 1 to t is then defined as:
rhn(xt, s (L) t−1) = s (L) t (6)
where xt stands for input at time t, and s (l) t denotes the state at time t at recurrence layer l, with L being the top layer of recurrence.",3 Models,[0],[0]
"Furthermore,
s (l) t = h (l) t t (l) t + s (l−1) t ( 1− t(l)t ) (7)
where is elementwise multiplication, and
h (l) t = tanh ( I[l = 1]WHxt +UHls (l−1) t ) (8)
t (l) t = σ",3 Models,[0],[0]
"( I[l = 1]WTxt +UTls (l−1) )
(9)
",3 Models,[0],[0]
Here I is the indicator function: input is only included in the computation for the first layer of recurrence l = 1.,3 Models,[0],[0]
"By applying the rhn function repeatedly, an RHN layer maps a sequence of inputs to a sequence of states:
(10) RHN(X, s0)
= rhn(xn, . . .",3 Models,[0],[0]
", rhn(x2, rhn(x1, s (L) 0 )))
",3 Models,[0],[0]
"Two or more RHN layers can be composed into a stack:
RHN2(RHN1(X, s1 (L) 0 ), s2 (L) 0 ), (11)
where sn (l) t stands for the state vector of layer n of the stack, at layer l of recurrence, at time t.",3 Models,[0],[0]
"In our version of the Stacked RHN architecture we use residualized layers:
RHNres(X, s0) = RHN(X, s0) +X (12)
",3 Models,[0],[0]
This formulation tends to ease optimization in multi-layer models (cf.,3 Models,[0],[0]
"He et al., 2015; Oord et al., 2016).
",3 Models,[0],[0]
"In addition to the speech model described above, we also define a comparable text model.",3 Models,[0],[0]
"As it takes a sequence of words as input, we replace the convolutional layer with a word embedding lookup table.",3 Models,[0],[0]
"We found the text model did not benefit from the use of the attention mechanism, and thus the sentence embedding is simply the L2-normalized activation vector of the topmost layer, at the last timestep.",3 Models,[0],[0]
Our main goal is to analyze the emerging representations from different components of the model and to examine the linguistic knowledge they encode.,4 Experiments,[0],[0]
"For this purpose, we employ a number of tasks that cover the spectrum from fully formbased to fully semantic.
",4 Experiments,[0],[0]
In Section 4.2 we assess the effectiveness of our architecture by evaluating it on the task of ranking images given an utterance.,4 Experiments,[0],[0]
Sections 4.3 to 4.6 present our analyses.,4 Experiments,[0],[0]
In Sections 4.3 and 4.4 we define auxiliary tasks to investigate to what extent the network encodes information about the surface form of an utterance from the speech input.,4 Experiments,[0],[0]
In Section 4.5 and 4.6 we focus on where semantic information is encoded in the model.,4 Experiments,[0],[0]
"In the analyses, we use the following features: Utterance embeddings: the weighted sum of the
unit activations on the last layer, as calculated by Equation (3).
",4 Experiments,[0],[0]
Average unit activations: hidden layer activations averaged over time and L2-normalized for each hidden layer.,4 Experiments,[0],[0]
Average input vectors: the MFCC vectors averaged over time.,4 Experiments,[0],[0]
We use this feature to examine how much information can be extracted from the input signal only.,4 Experiments,[0],[0]
For the experiments reported in the remainder of the paper we use two datasets of images with spoken captions.,4.1 Data,[0],[0]
"The Flickr8k Audio Caption Corpus was constructed by having crowdsource workers read aloud the captions in the original Flickr8K corpus (Hodosh et al., 2013).",4.1.1 Flickr8K,[0],[0]
For details of the data collection procedure refer to Harwath and Glass (2015).,4.1.1 Flickr8K,[0],[0]
"The datasets consist of 8,000 images, each image with five descriptions.",4.1.1 Flickr8K,[0],[0]
"One thousand images are held out for validation, and another one thousand for the final test set.",4.1.1 Flickr8K,[0],[0]
"We use the splits provided by (Karpathy and Fei-Fei, 2015).",4.1.1 Flickr8K,[0],[0]
"The image features come from the final fully connect layer of VGG-16 (Simonyan and Zisserman, 2014) pre-trained on Imagenet (Russakovsky et al., 2014).
",4.1.1 Flickr8K,[0],[0]
We generate the input signal as follows: we extract 12-dimensional mel-frequency cepstral coefficients (MFCC) plus log of the total energy.,4.1.1 Flickr8K,[0],[0]
"We
then compute and add first order and second order differences (deltas) for a total of 37 dimensions.",4.1.1 Flickr8K,[0],[0]
"We use 25 milisecond windows, sampled every 10 miliseconds.1",4.1.1 Flickr8K,[0],[0]
"We generated synthetic speech for the captions in the MS COCO dataset (Lin et al., 2014) via the Google Text-to-Speech API.2 The audio and the corresponding MFCC features are released as Chrupała et al. (2017)3.",4.1.2 Synthetically spoken COCO,[0],[0]
This TTS system we used produces high-quality realistic-sounding speech.,4.1.2 Synthetically spoken COCO,[0],[0]
"It is nevertheless much simpler than real human speech as it uses a single voice, and lacks tempo variation or ambient noise.",4.1.2 Synthetically spoken COCO,[0],[0]
"The data consists of over 300,000 images, each with five spoken captions.",4.1.2 Synthetically spoken COCO,[0],[0]
Five thousand images each are held out for validation and test.,4.1.2 Synthetically spoken COCO,[0],[0]
"We use the splits and image features provided by Vendrov et al. (2015).4 The image features also come from the VGG-16 network, but are averages of feature vectors for ten crops of each image.",4.1.2 Synthetically spoken COCO,[0],[0]
"For the MS COCO captions we extracted only plain MFCC and total energy features, and did not add deltas in order to keep the amount of computation manageable given the size of the dataset.",4.1.2 Synthetically spoken COCO,[0],[0]
"We evaluate our model on the task of ranking images given a spoken utterance, such that highly ranked images contain scenes described by the utterance.",4.2 Image retrieval,[0],[0]
The performance on this task on validation data is also used to choose the best variant of the model architecture and to tune the hyperparameters.,4.2 Image retrieval,[0],[0]
We compare the speech models to models trained on written sentences split into words.,4.2 Image retrieval,[0],[0]
"The best settings found for the four models were the following: Flickr8K Text RHN 300-dimensional word em-
beddings, 1 hidden layer with 1024 dimensions, 1 microstep, initial learning rate 0.001.
",4.2 Image retrieval,[0],[0]
"Flick8K Speech RHN convolutional layer with length 6, size 64, stride 2, 4 hidden layers with 1024 dimensions, 2 microsteps, atten-
1We noticed that for a number of utterances the audio signal was very long: on inspection it turned out that most of these involved failure to switch off the microphone on the part of the workers, and the audio contained ambient noise or unrelated speech.",4.2 Image retrieval,[0],[0]
"We thus trucated all audio for this dataset at 10,000 miliseconds.
2Available at https://github.com/pndurette/gTTS.",4.2 Image retrieval,[0],[0]
3Available at https://doi.org/10.5281/zenodo.400926.,4.2 Image retrieval,[0],[0]
"4See https://github.com/ivendrov/order-embedding.
tion MLP with 128 hidden units, initial learning rate 0.0002
COCO Text RHN 300-dimensional word embeddings, 1 hidden layer with 1024 dimensions, 1 microstep, initial learning rate 0.001 COCO Speech RHN convolutional layer with length 6, size 64, stride 3, 5 hidden layers with 512 dimensions, 2 microsteps, attention MLP with 512 hidden units, initial learning rate 0.0002
All models were optimized with Adam (Kingma and Ba, 2014) with early stopping: we kept the parameters for the epoch which showed the best recall@10 on validation data.
",4.2 Image retrieval,[0],[0]
Table 1 shows the results for the human speech from the Flickr8K dataset.,4.2 Image retrieval,[0],[0]
The Speech RHN model scores substantially higher than model of Harwath and Glass (2015) on the same data.,4.2 Image retrieval,[0],[0]
However the large gap between its perfomance and the scores of the text model suggests that Flickr8K is rather small for the speech task.,4.2 Image retrieval,[0],[0]
In Table 2 we present the results on the dataset of synthetic speech from MS COCO.,4.2 Image retrieval,[0],[0]
"Here the text model is still better, but the gap is much smaller than for Flickr8K. We attribute this to the much larger size of dataset, and to the less noisy and less variable synthetic speech.
",4.2 Image retrieval,[0],[0]
"While the MS COCO text model is overall better than the speech model, there are cases where it outperforms the text model.",4.2 Image retrieval,[0],[0]
"We listed the top hundred cases where the ratio of the ranks of the correct image according to the two models was the smallest, as well as another hundred cases where it was the largest.",4.2 Image retrieval,[0],[0]
"Manual inspection did not turn
up any obvious patterns for the cases of text being better than speech.",4.2 Image retrieval,[0],[0]
"For the cases where speech outperformed text, two patterns stood out: (i) sentences with spelling mistakes, (ii) unusually long sentences.",4.2 Image retrieval,[0],[0]
"For example for the sentence a yellow
and white birtd is in flight the text model misses the misspelled word birtd and returns an irrelevant image, while the speech model seems robust to some degree of variation in pronunciation and returns the target image at rank 1 (see Figure 1).",4.2 Image retrieval,[0],[0]
"In an attempt to quantify this effect we counted the number of unique words with training set frequencies below 5 in the top 100 utterances with lowest and highest rank ratio: for the utterances where text was better there were 16 such words; for utterances where speech was better there were 28, among them misspellings such as streeet, scears (for skiers), contryside, scull, birtd, devise.
",4.2 Image retrieval,[0],[0]
The distribution of utterance lengths in Figure 2 confirms pattern (ii): the set of 100 sentences where speech beats text by a large margin are longer on average and there are extremely long outliers among them.,4.2 Image retrieval,[0],[0]
"One of them is the 36-word-
long utterance depicted in Figure 3, with ranks 470 and 2 for text and speech respectively.",4.2 Image retrieval,[0],[0]
"We suspect that the speech model’s attention mechanism enables it to cherry pick key fragments of such monster utterances, while the text model lacking this mechanism may struggle.",4.2 Image retrieval,[0],[0]
"Figure 3 shows the plot of the attention weights for this utterance from the
speech model.",4.2 Image retrieval,[0],[0]
"Our first auxiliary task is to predict the length of the utterance, using the features explained at the beginning of Section 4.",4.3 Predicting utterance length,[0],[0]
"Since the length of an utterance directly corresponds to how long it takes to articulate, we also use the number of time steps5 as a feature and expect it to provide the upper bound for our task, especially for synthetic speech.",4.3 Predicting utterance length,[0],[0]
We use a Ridge Regression model for predicting utterance length using each set of features.,4.3 Predicting utterance length,[0],[0]
"The model is trained on 80% of the sentences in the validation set, and tested on the remaining 20%.",4.3 Predicting utterance length,[0],[0]
"For all features regularization penalty α = 1.0 gave the best results.
",4.3 Predicting utterance length,[0],[0]
Figure 4 shows the results for this task on human speech from Flickr8K and synthetic speech from COCO.,4.3 Predicting utterance length,[0],[0]
"With the exception of the average input vectors for Flickr8K, all features can explain a high proportion of variance in the predicted utterance length.",4.3 Predicting utterance length,[0],[0]
"The pattern observed for the two datasets is slightly different: due to the systematic conversion of words to synthetic speech in COCO, using the number of time steps for this dataset yields the highest R2.",4.3 Predicting utterance length,[0],[0]
"However, this feature is not as informative for predicting the utterance length in Flickr8K due to noise and variation in human speech, and is in fact outperformed by some of the features extracted from the model.",4.3 Predicting utterance length,[0],[0]
"Also, the input vectors from COCO are much more informative than Flickr8K due to larger quantity and simpler structure of the speech signal.",4.3 Predicting utterance length,[0],[0]
"However, in both datasets the best (non-ceiling) performance is obtained by using average unit activations from the hidden layers (layer 2 for COCO, and layers 3 and 4 for Flickr8K).",4.3 Predicting utterance length,[0],[0]
"These features outperform utterance embeddings, which are optimized according to the visual grounding objective of the model and most probably learn to ignore the superficial characteristics of the utterance that do not contribute to matching the corresponding image.
",4.3 Predicting utterance length,[0],[0]
"Note that the performance on COCO plateaus after the second layer, which might suggest that form-based knowledge is learned by lower layers.",4.3 Predicting utterance length,[0],[0]
"Since Flickr8K is much smaller in size, the stabilising happens later in layer 3.
",4.3 Predicting utterance length,[0],[0]
5This is approximately duration in milliseconds 10×stride .,4.3 Predicting utterance length,[0],[0]
Results from the previous experiment suggest that our model acquires information about higher level building blocks (words) in the continuous speech signal.,4.4 Predicting word presence,[0],[0]
Here we explore whether it can detect the presence or absence of individual words in an utterance.,4.4 Predicting word presence,[0],[0]
"We formulate detecting a word in an utterance as a binary classification task, for which we use a multi-layer perceptron with a single hidden layer of size 1024, optimized by Adam.",4.4 Predicting word presence,[0],[0]
The input to the model is a concatenation of the feature vector representing an utterance and the one representing a target word.,4.4 Predicting word presence,[0],[0]
"We again use utterance embeddings, average unit activations on each layer, and average input vectors as features, and represent each target word as a vector of MFCC features extracted from the audio signal synthetically produced for that word.
",4.4 Predicting word presence,[0],[0]
"For each utterance in the validation set, we randomly pick one positive and one negative target (i.e., one word that does and one that does not appear in the utterance) that is not a stop word.",4.4 Predicting word presence,[0],[0]
"To balance the probability of a word being positive or negative, we use each positive target as a negative target for another utterance in the validation
set.",4.4 Predicting word presence,[0],[0]
"The MLP model is trained on the positive and negative examples corresponding to 80% of the utterances in the validation set of each dataset, and evaluated on the remaining 20%.
",4.4 Predicting word presence,[0],[0]
Figure 5 shows the mean accuracy of the MLP on Flickr8K and COCO.,4.4 Predicting word presence,[0],[0]
"All results using features extracted from the model are above chance (0.5), with the average unit activations of the hidden layers yielding the best results (0.65 for Flickr8K on layer 3, and 0.79 for COCO on layer 4).",4.4 Predicting word presence,[0],[0]
These numbers show that the speech model infers reliable information about word-level blocks from the low-level audio features it receives as input.,4.4 Predicting word presence,[0],[0]
"The observed trend is similar to the previous task: average unit activations on the higher-level hidden layers are more informative for this task than the utterance embeddings, but the performance plateaus before the topmost layer.",4.4 Predicting word presence,[0],[0]
Next we explore to what extent the model’s representations correspond to those of humans.,4.5 Sentence similarity,[0],[0]
"We employ the Sentences Involving Compositional Knowledge (SICK) dataset (Marelli et al., 2014).",4.5 Sentence similarity,[0],[0]
"SICK consists of image descriptions taken from
Flickr8K and video captions from the SemEval 2012 STS MSRVideo Description data set (STS) (Agirre et al., 2012).",4.5 Sentence similarity,[0],[0]
"Captions were paired at random, as well as modified to obtain semantically similar and contrasting counterparts, and the resulting pairs were rated for semantic similarity.
",4.5 Sentence similarity,[0],[0]
"For all sentence pairs in SICK, we generate synthetic spoken sentences and feed them to the COCO Speech RHN, and calculate the cosine similarity between the averaged MFCC input vectors, the averaged hidden layer activation vectors, and the sentence embeddings.",4.5 Sentence similarity,[0],[0]
Z-score transformation was applied before calculating the cosine similarities.,4.5 Sentence similarity,[0],[0]
"We then correlate these cosine similarities with
• semantic relatedness according to human ratings • cosine similarities according to z-score transformed embeddings from COCO Text RHN • edit similarities, a measure of how similar the sentences are in form, specifically, 1−normalized Levenshtein distance over character sequences
Figure 6 shows a boxplot over 10,000 bootstrap samples for all correlations.",4.5 Sentence similarity,[0],[0]
"We observe that (i) correlation with edit similarity initially increases, then decreases; (ii) correlation with human relatedness scores and text model embeddings increases until layer 4, but decreases for hidden layer 5.",4.5 Sentence similarity,[0],[0]
The initially increasing and then decreasing correlation with edit similarity is consistent with the findings that information about form is encoded by lower layers.,4.5 Sentence similarity,[0],[0]
"The overall growing correlation with both human semantic similarity ratings and
the COCO Text RHN indicate that higher layers learn to represent semantic knowledge.",4.5 Sentence similarity,[0],[0]
We were somewhat surprised by the pattern for the correlation with human ratings and the Text model similarities which drops for layer 5.,4.5 Sentence similarity,[0],[0]
We suspect it may be caused by the model at this point in the layer hierarchy being strongly tuned to the specifics of the COCO dataset.,4.5 Sentence similarity,[0],[0]
"To test this, we checked the correlations with COCO Text embeddings on validation sentences from the COCO dataset instead of SICK.",4.5 Sentence similarity,[0],[0]
"These increased monotonically, in support of our conjecture.",4.5 Sentence similarity,[0],[0]
"Next we simulate the task of distinguishing between pairs of homonyms, i.e. words with the same acoustic form but different meaning.",4.6 Homonym disambiguation,[0],[0]
We group the words in the union of the training and validation data of the COCO dataset by their phonetic transcription.,4.6 Homonym disambiguation,[0],[0]
"We then pick pairs of words which have the same pronunciation but different spelling, for example suite/sweet.",4.6 Homonym disambiguation,[0],[0]
"We impose the following conditions: (a) both forms appear more than 20 times, (b) the two forms have different meaning (i.e. they are not simply variant spellings like theater/theatre), (c) neither form is a function word, and (d) the more frequent form constitutes less than 95% of the occurrences.",4.6 Homonym disambiguation,[0],[0]
"This
gives us 34 word pairs.",4.6 Homonym disambiguation,[0],[0]
"For each pair we generate a binary classification task by taking all the utterances where either form appears, using average input vectors, utterance embeddings, and average unit activations as features.",4.6 Homonym disambiguation,[0],[0]
"Instances for all feature sets are normalized to unit L2 norm.
",4.6 Homonym disambiguation,[0],[0]
For each task and feature set we run stratified 10-fold cross validation using Logistic Regression to predict which of the two words the utterance contains.,4.6 Homonym disambiguation,[0],[0]
"Figure 7 shows, for each pair, the relative error reduction of each feature set with respect to the majority baseline.",4.6 Homonym disambiguation,[0],[0]
"There is substantial variation across word pairs, but overall the task becomes easier as the features come from higher layers in the network.",4.6 Homonym disambiguation,[0],[0]
"Some forms can be disambiguated with very high accuracy (e.g. sale/sail, cole/coal, pairs/pears), while some others cannot be distinguished at all (peaking/peeking, great/grate, mantle/mantel).",4.6 Homonym disambiguation,[0],[0]
"We examined the sentences containing the failing forms, and found out that almost all occurrences of peaking and mantle were misspellings of peeking and mantel, which explains the impossibility of disambiguating these cases.",4.6 Homonym disambiguation,[0],[0]
We present a multi-layer recurrent highway network model of language acquisition from visually grounded speech signal.,5 Conclusion,[0],[0]
"Through detailed analysis we uncover how information in the input signal is transformed as it flows through the network: formal aspects of language such as word identities that not directly present in the input are discovered and encoded low in the layer hierarchy, while semantic information is most strongly expressed in the topmost layers.
",5 Conclusion,[0],[0]
Going forward we would like to compare the representations learned by our model to the brain activity of people listening to speech in order to determine to what extent the patterns we found correspond to localized processing in the human cortex.,5 Conclusion,[0],[0]
This will hopefully lead to a better understanding of language learning and processing by both artificial and neural networks.,5 Conclusion,[0],[0]
We would like to thank David Harwath for making the Flickr8k Audio Caption Corpus publicly available.,Acknowledgements,[0],[0]
We present a visually grounded model of speech perception which projects spoken utterances and images to a joint semantic space.,abstractText,[0],[0]
"We use a multi-layer recurrent highway network to model the temporal nature of spoken speech, and show that it learns to extract both form and meaningbased linguistic knowledge from the input signal.",abstractText,[0],[0]
"We carry out an in-depth analysis of the representations used by different components of the trained model and show that encoding of semantic aspects tends to become richer as we go up the hierarchy of layers, whereas encoding of formrelated aspects of the language input tends to initially increase and then plateau or decrease.",abstractText,[0],[0]
Representations of language in a model of visually grounded speech signal,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1499–1509, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
Representing information about real-world entities and their relations in structured knowledge base (KB) form enables numerous applications.,1 Introduction,[0],[0]
"Large, collaboratively created knowledge bases have recently become available e.g., Freebase (Bollacker et al., 2008), YAGO (Suchanek et al., 2007), and DBPedia (Auer et al., 2007), but even though they are impressively large, their coverage is far from complete.",1 Introduction,[0],[0]
"This has motivated research in automatically deriving new facts to extend a manually built knowledge base, by using information from the existing knowledge base, textual mentions of entities, and semi-structured data such as tables and web forms (Nickel et al., 2015).
",1 Introduction,[0],[0]
"In this paper we build upon the work of Riedel et al. (2013), which jointly learns continuous representations for knowledge base and textual relations.",1 Introduction,[0],[0]
"This common representation in the same vector space can serve as a kind of “universal schema” which admits joint inferences among
∗This research was conducted during the author’s internship at Microsoft Research.
KBs and text.",1 Introduction,[0],[0]
The textual relations represent the relationships between entities expressed in individual sentences (see Figure 1 for an example).,1 Introduction,[0],[0]
Riedel et al. (2013) represented each textual mention of an entity pair by the lexicalized dependency path between the two entities (see Figure 2).,1 Introduction,[0],[0]
Each such path is treated as a separate relation in a combined knowledge graph including both KB and textual relations.,1 Introduction,[0],[0]
"Following prior work in latent feature models for knowledge base completion, every textual relation receives its own continuous representation, learned from the pattern of its co-occurrences in the knowledge graph.
",1 Introduction,[0],[0]
"However, largely synonymous textual relations often share common sub-structure, and are composed of similar words and dependency arcs.",1 Introduction,[0],[0]
"For example, Table 1 shows a collection of dependency paths co-occurring with the person/organizations founded relation.
",1 Introduction,[0],[0]
"In this paper we model this sub-structure and share parameters among related dependency paths, using a unified loss function learning entity and relation representations to maximize performance on the knowledge base link prediction task.
",1 Introduction,[0],[0]
"We evaluate our approach on the FB15k-237 dataset, a knowledge base derived from the Free-
1499
base subset FB15k (Bordes et al., 2013) and filtered to remove highly redundant relations (Toutanova and Chen, 2015).",1 Introduction,[0],[0]
"The knowledge base is paired with textual mentions for all entity pairs derived from ClueWeb121 with Freebase entity mention annotations (Gabrilovich et al., 2013).
",1 Introduction,[0],[0]
"We show that using a convolutional neural network to derive continuous representations for textual relations boosts the overall performance on link prediction, with larger improvement on entity pairs that have textual mentions.",1 Introduction,[0],[0]
There has been a growing body of work on learning to predict relations between entities without requiring sentence-level annotations of textual mentions at training time.,2 Related Work,[0],[0]
"We group such related work into three groups based on whether KB, text, or both sources of information are used.",2 Related Work,[0],[0]
"Additionally, we discuss related work in the area of supervised relation extraction using continuous representations of text, even though we do not use supervision at the level of textual mentions.",2 Related Work,[0],[0]
"Nickel et al. (2015) provide a broad overview of machine learning models for knowledge graphs, including models based on observed graph features such as the path ranking algorithm (Lao et al., 2011), models based on continuous representations (latent features), and model combinations (Dong et al., 2014).",Knowledge base completion,[0],[0]
"These models predict new facts in a given knowledge base, based on information from existing entities and relations.",Knowledge base completion,[0],[0]
"From this line of work, most relevant to our study is prior work evaluating continuous representation models on the FB15k dataset.",Knowledge base completion,[0],[0]
"Yang et al. (2015) showed that a simple variant of a bilinear model DISTMULT outperformed TRANSE (Bordes et al., 2013) and more richly parameterized models on this dataset.",Knowledge base completion,[0],[0]
"We therefore build upon the best performing prior model DISTMULT from this line of work, as well as additional models E and F developed in the context of text-augmented knowledge graphs (Riedel et al., 2013), and extend them to incorporate compositional representations of textual relations.
",Knowledge base completion,[0],[0]
"1http://lemurproject.org/clueweb12/ FACC1/
Relation extraction using distant supervision
A number of works have focused on extracting new instances of relations using information from textual mentions, without sophisticated modeling of prior knowledge from the knowledge base.",Knowledge base completion,[0],[0]
"Mintz et al. (2009) demonstrated that both surface context and dependency path context were helpful for the task, but did not model the compositional sub-structure of this context.",Knowledge base completion,[0],[0]
"Other work proposed more sophisticated models that reason about sentence-level hidden variables (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) or model the noise arising from the incompleteness of knowledge bases and text collections (Ritter et al., 2013), inter alia.",Knowledge base completion,[0],[0]
"Our work focuses on representing the compositional structure of sentential context for learning joint continuous representations of text and knowledge bases.
",Knowledge base completion,[0],[0]
"Combining knowledge base and text information
A combination of knowledge base and textual information was first shown to outperform either source alone in the framework of path-ranking algorithms in a combined knowledge base and text graph (Lao et al., 2012).",Knowledge base completion,[0],[0]
"To alleviate the sparsity of textual relations arising in such a combined graph, (Gardner et al., 2013; Gardner et al., 2014) showed how to incorporate clusters or continuous representations of textual relations.",Knowledge base completion,[0],[0]
Note that these vector representations are based on the co-occurrence patterns for the textual relations and not on their compositional structure.,Knowledge base completion,[0],[0]
"Cooccurrence based textual relation representations were also learned in (Neelakantan et al., 2015).",Knowledge base completion,[0],[0]
"Wang et al. (2014a) combined knowledge base and text information by embedding knowledge base entities and the words in their names in the same vector space, but did not model the textual cooccurrences of entity pairs and the expressed textual relations.",Knowledge base completion,[0],[0]
Weston et al. (2013) combined continuous representations from a knowledge base and textual mentions for prediction of new relations.,Knowledge base completion,[0],[0]
"The two representations were trained independently of each other and using different loss functions, and were only combined at inference time.",Knowledge base completion,[0],[0]
"Additionally, the employed representations of text were non-compositional.
",Knowledge base completion,[0],[0]
"In this work we train continuous representations of knowledge base and textual relations jointly, which allows for deeper interactions between the
sources of information.",Knowledge base completion,[0],[0]
"We directly build on the universal schema approach of Riedel et al. (2013) as well as the universal schema extension of the DISTMULT model mentioned previously, to improve the representations of textual relations by capturing their compositional structure.",Knowledge base completion,[0],[0]
"Additionally, we evaluate the approach on a dataset that contains rich prior information from the training knowledge base, as well as a wealth of textual information from a large document collection.
",Knowledge base completion,[0],[0]
"Continuous representations for supervised relation extraction
In contrast to the work reviewed so far, work on sentence-level relation extraction using direct supervision has focused heavily on representing sentence context.",Knowledge base completion,[0],[0]
"Models using hand-crafted features have evolved for more than a decade, and recently, models using continuous representations have been found to achieve new state-of-the-art performance (Zeng et al., 2014; Gormley et al., 2015).",Knowledge base completion,[0],[0]
"Compared to work on representation learning for sentence-level context, such as this recent work using LSTM models on constituency or dependency trees (Tai et al., 2015), our approach using a one-hidden-layer convolutional neural network is relatively simple.",Knowledge base completion,[0],[0]
"However, even such a simple approach has been shown to be very competitive (Kim, 2014).",Knowledge base completion,[0],[0]
"We begin by introducing notation to define the task, largely following the terminology in Nickel et al. (2015).",3 Models for knowledge base completion,[0],[0]
"We assume knowledge bases are represented using RDF triples, in the form (subject, predicate, object), where the subject and object are entities and the predicate is the type of relation.",3 Models for knowledge base completion,[0],[0]
"For example, the KB fragment shown in Figure 1 is shown as a knowledge graph, where the entities are the nodes, and the relations are shown as directed labeled edges: we see three entities participating in three relation instances indicated by the edges.",3 Models for knowledge base completion,[0],[0]
"For brevity, we will denote triples by (es, r, eo), where es and eo denote the subject and object entities, respectively.
",3 Models for knowledge base completion,[0],[0]
"The task is, given a training KB consisting of entities with some relations between them, to predict new relations (links) that do not appear in the training KB.",3 Models for knowledge base completion,[0],[0]
"More specifically, we will build models that rank candidate entities for given queries (es, r, ?) or (?, r, eo), which ask about the object
1
or subject of a given relation.",3 Models for knowledge base completion,[0],[0]
"This task setting has been used in models for KB completion previously, e.g. (Dong et al., 2014; Gardner et al., 2014), even though it has not been standard in evaluations of distant supervision for relation extraction (Mintz et al., 2009; Riedel et al., 2013).",3 Models for knowledge base completion,[0],[0]
"The advantage of this evaluation setting is that it enables automatic evaluation without requiring humans to label candidate extractions, while making only a local closed world assumption for the completeness of the knowledge base — i.e., if one object eo for a certain subject / relation pair (es, r) is present in the knowledge base, it is assumed likely that all other objects (es, r, e′o) will be present.",3 Models for knowledge base completion,[0],[0]
"Such an assumption is particularly justified for nearly functional relations.
",3 Models for knowledge base completion,[0],[0]
"To incorporate textual information, we follow prior work (Lao et al., 2012; Riedel et al., 2013) and represent both textual and knowledge base relations in a single graph of “universal” relations.",3 Models for knowledge base completion,[0],[0]
"The textual relations are represented as full lexicalized dependency paths, as illustrated in Figure 2.",3 Models for knowledge base completion,[0],[0]
"An instance of the textual relation SUBJECT nsubj←−−− president prep−−→ of obj−→OBJECT connecting the entities BARACK OBAMA and UNITED STATES, is added to the knowledge graph based on this sentential occurrence.
",3 Models for knowledge base completion,[0],[0]
"To present the models for knowledge base completion based on such combined knowledge graphs, we first introduce some notation.",3 Models for knowledge base completion,[0],[0]
Let E denote the set of entities in the knowledge graph and let R denote the set of relation types.,3 Models for knowledge base completion,[0],[0]
"We denote each possible triple as T = (es, r, eo) where es, eo ∈ E , r ∈ R, and model its presence with a binary random variable yT ∈ {0, 1} which indicates whether the triple exists.",3 Models for knowledge base completion,[0],[0]
"The models we build score possible triples (es, r, eo) using continuous representations (latent features) of the three elements of the triple.",3 Models for knowledge base completion,[0],[0]
"The models use scoring function f(es, r, eo) to represent the model’s confidence in the existence of the triple.",3 Models for knowledge base completion,[0],[0]
"We present the models and then the loss function used to train
1
their parameters.",3 Models for knowledge base completion,[0],[0]
We begin with presenting the three models from prior work that this research builds upon.,3.1 Basic Models,[0],[0]
"They all learn latent continuous representations of relations and entities or entity pairs, and score possible triples based on the learned continuous representations.",3.1 Basic Models,[0],[0]
"Each of the models can be defined on a knowledge graph containing entities and KB relations only, or on a knowledge graph additionally containing textual relations.",3.1 Basic Models,[0],[0]
"We use models F and E from (Riedel et al., 2013) where they were used for a combined KB+text graph, and model DISTMULT from (Yang et al., 2015), which was originally used for a knowledge graph containing only KB relations.
",3.1 Basic Models,[0],[0]
"As shown in Figure 3, model F learns a Kdimensional latent feature vector for each candidate entity pair (es, eo), as well as a samedimensional vector for each relation r, and the scoring function is simply defined as their inner product: f(es, r, eo) = v(r)ᵀv(es, eo).",3.1 Basic Models,[0],[0]
"Therefore, different pairs sharing the same entity would not share parameters in this model.
",3.1 Basic Models,[0],[0]
"Model E does not have parameters for entity pairs, and instead has parameters for individual entities.",3.1 Basic Models,[0],[0]
"It aims to capture the compatibility be-
tween entities and the subject and object positions of relations.",3.1 Basic Models,[0],[0]
"For each relation type r, the model learns two latent feature vectors v(rs) and v(ro) of dimension K. For each entity (node) ei, the model also learns a latent feature vector of the same dimensionality.",3.1 Basic Models,[0],[0]
"The score of a candidate triple (es, r, eo) is defined as f(es, r, eo) = v(rs)ᵀv(es) + v(ro)ᵀv(eo).",3.1 Basic Models,[0],[0]
"It can be seen that when a subject entity is fixed in a query (es, r, ?), the ranking of candidate object entity fillers according to f does not depend on the subject entity but only on the relation type r.
The third model DISTMULT, is a special form of a bilinear model like RESCAL (Nickel et al., 2011), where the non-diagonal entries in the relation matrices are assumed to be zero.",3.1 Basic Models,[0],[0]
This model was proposed in Yang et al. (2015) and was shown to outperform prior work on the FB15k dataset.,3.1 Basic Models,[0],[0]
"In this model, each entity ei and each relation r is assigned a latent feature vector of dimensionK. The score of a candidate triple (es, r, eo) is defined as f(es, r, eo)",3.1 Basic Models,[0],[0]
"= v(r)ᵀ (v(es) ◦ v(eo)), where ◦ denotes the element-wise vector product.",3.1 Basic Models,[0],[0]
"In this model, entity pairs which share an entity also share parameters, and the ranking of candidate objects for queries (es, r, ?) depends on the subject entity.
",3.1 Basic Models,[0],[0]
"Denote Ne = |E|, Nr = |R|, and K = dimension of latent feature vectors, then model E has KNe +",3.1 Basic Models,[0],[0]
2KNr parameters and model DISTMULT has KNe + KNr parameters.,3.1 Basic Models,[0],[0]
"Model F has KN2e + KNr parameters, although most entity pairs will not co-occur in the knowledge base or text.
",3.1 Basic Models,[0],[0]
"In the basic models, knowledge base and textual relations are treated uniformly, and each textual relation receives its own latent representation of dimensionality K. When textual relations are added to the training knowledge graph, the total number of relations |R| grows substantially (it increases from 237 to more than 2.7 million for the dataset in this study), resulting in a substantial increase in the total number of independent parameters.
",3.1 Basic Models,[0],[0]
"Note that in all of these models queries about the arguments of knowledge base relations (es, r, ?) are answered by scoring functions looking only at the entity and KB relation representations, without using representations of textual mentions.",3.1 Basic Models,[0],[0]
The textual mention information and representations are only used at training time to improve the learned representations of KB relations and entities.,3.1 Basic Models,[0],[0]
"In the standard latent feature models discussed above, each textual relation is treated as an atomic unit receiving its own set of latent features.",3.2 CONV: Compositional Representations of Textual Relations,[0],[0]
"However, many textual relations differ only slightly in the words or dependency arcs used to express the relation.",3.2 CONV: Compositional Representations of Textual Relations,[0],[0]
"For example, Table 1 shows several textual patterns that co-occurr with the relation person/organizations founded in the training KB.",3.2 CONV: Compositional Representations of Textual Relations,[0],[0]
"While some dependency paths occur frequently, many very closely related ones have been observed only once.",3.2 CONV: Compositional Representations of Textual Relations,[0],[0]
The statistical strength of the model could be improved if similar dependency paths have a shared parameterization.,3.2 CONV: Compositional Representations of Textual Relations,[0],[0]
"We build on work using similar intuitions for other tasks and learn compositional representations of textual relations based on their internal structure, so that the derived representations are accurate for the task of predicting knowledge base relations.
",3.2 CONV: Compositional Representations of Textual Relations,[0],[0]
We use a convolutional neural network applied to the lexicalized dependency paths treated as a sequence of words and dependency arcs with direction.,3.2 CONV: Compositional Representations of Textual Relations,[0],[0]
Figure 4 depicts the neural network architecture.,3.2 CONV: Compositional Representations of Textual Relations,[0],[0]
"In the first layer, each word or directed labeled arc is mapped to a continuous representation using an embedding matrix V. In the hidden layer, every window of three elements is mapped to a hidden vector using position-specific maps W, a bias vector b, and a tanh activation function.",3.2 CONV: Compositional Representations of Textual Relations,[0],[0]
"A max-pooling operation over the sequence is applied to derive the final continuous representation for the dependency path.
",3.2 CONV: Compositional Representations of Textual Relations,[0],[0]
The CONV representation of textual relations can be used to augment any of the three basic models.,3.2 CONV: Compositional Representations of Textual Relations,[0],[0]
The difference between a basic model and its CONV-augmented variant is in the parameterization of textual mentions.,3.2 CONV: Compositional Representations of Textual Relations,[0],[0]
"The basic models learn distinct latent feature vectors of dimensionality K for all textual relation types, whereas the CONV models derive the K-dimensional latent feature vectors for textual relation types as the activation at the top layer of the convolutional network in Figure 4, given the corresponding lexicalized dependency path as input.",3.2 CONV: Compositional Representations of Textual Relations,[0],[0]
All basic and CONV-augmented models use the same training loss function.,3.3 Training loss function,[0],[0]
Our loss function is motivated by the link prediction task and the performance measures used.,3.3 Training loss function,[0],[0]
"As previously men-
tioned, the task is to predict the subject or object entity for given held-out triples (es, r, eo), i.e., to rank all entities with respect to their likelihood of filling the respective position in the triple2.",3.3 Training loss function,[0],[0]
"We would thus like the model to score correct triples (es, r, eo) higher than incorrect triples (e′, r, eo) and (es, r, e′) which differ from the correct triple by one entity.",3.3 Training loss function,[0],[0]
"Several approaches (Nickel et al., 2015) use a margin-based loss function.",3.3 Training loss function,[0],[0]
We use an approximation to the negative loglikelihood of the correct entity filler instead3.,3.3 Training loss function,[0],[0]
"We define the conditional probabilities p(eo|es, r) and p(es|r, eo) for object and subject entities given the relation and the other argument as follows:
p(eo|es, r; Θ) = e f(es,r,eo;Θ)∑
e′∈Neg(es,r,?)",3.3 Training loss function,[0],[0]
"e f(es,r,e′;Θ)
",3.3 Training loss function,[0],[0]
"Conditional probabilities for subject entities p(es|eo, r; Θ) are defined analogously.",3.3 Training loss function,[0],[0]
Here Θ denotes all the parameters of latent features.,3.3 Training loss function,[0],[0]
"The denominator is defined using a set of entities that do not fill the object position in any relation triple (es, r, ?) in the training knowledge graph.",3.3 Training loss function,[0],[0]
"Since the number of such entities is impractically large, we sample negative triples from the full set.",3.3 Training loss function,[0],[0]
"We also limit the candidate entities to ones that have types consistent with the position in the relation triple (Chang et al., 2014; Yang et al., 2015), where the types are approximated following Toutanova and Chen (2015).",3.3 Training loss function,[0],[0]
"Additionally, since the task of predicting textual relations is auxiliary to the main task, we use a weighting factor τ for the loss on predicting the arguments of textual relations (Toutanova and Chen, 2015).
",3.3 Training loss function,[0],[0]
"Denote T as a set of triples, we define the loss L(T ; Θ) as:
L(T ; Θ) =",3.3 Training loss function,[0],[0]
"− ∑
(es,r,eo)∈T log p(eo|es, r; Θ)
− ∑
(es,r,eo)∈T log p(es|eo, r; Θ)
Let TKB and Ttext represent the set of knowledge base triples and textual relation triples respectively.",3.3 Training loss function,[0],[0]
"The final training loss function is de-
2Our experimental comparison focuses on predicting object entities only, but we consider both argument types in the training loss function.
3Note that both margin-based and likelihood-based loss functions are susceptible to noise from potential selection of false negative examples.",3.3 Training loss function,[0],[0]
"An empirical comparison of training loss functions would be interesting.
fined as:
L(TKB; Θ) + τL(Ttext; Θ) + λ‖Θ‖2,
where λ is the regularization parameter, and τ is the weighing factor of the textual relations.
",3.3 Training loss function,[0],[0]
The parameters of all models are trained using a batch training algorithm.,3.3 Training loss function,[0],[0]
"The gradients of the basic models are straightforward to compute, and the gradients of the convolutional network parameters for the CONV-augmented models are also not hard to derive using back-propagation.",3.3 Training loss function,[0],[0]
"We use the FB15k-237 4 dataset, which is a subset of FB15k (Bordes et al., 2013) that excludes redundant relations and direct training links for held-out triples, with the goal of making the task more realistic (Toutanova and Chen, 2015).",Dataset and Evaluation Protocol,[0],[0]
"The FB15k dataset has been used in multiple studies on knowledge base completion (Wang et al., 2014b; Yang et al., 2015).",Dataset and Evaluation Protocol,[0],[0]
"Textual relations for
4Check the first author’s website for a release of the dataset.
1504
FB15k-237 are extracted from 200 million sentences in the ClueWeb12 corpus coupled with Freebase mention annotations (Gabrilovich et al., 2013), and include textual links of all co-occurring entities from the KB set.",Dataset and Evaluation Protocol,[0],[0]
"After pruning5, there are 2.7 million unique textual relations that are added to the knowledge graph.",Dataset and Evaluation Protocol,[0],[0]
"The set of textual relations is larger than the set used in Toutanova and Chen (2015) (25,000 versus 2.7 million), leading to improved performance.
",Dataset and Evaluation Protocol,[0],[0]
"The number of relations and triples in the training, validation and test portions of the data are given in Table 2.",Dataset and Evaluation Protocol,[0],[0]
The two rows list statistics for the KB and text portions of the data separately.,Dataset and Evaluation Protocol,[0],[0]
The 2.7 million textual relations occur in 3.9 million text triples.,Dataset and Evaluation Protocol,[0],[0]
"Almost all entities occur in textual relations (13,937 out of 14,541).",Dataset and Evaluation Protocol,[0],[0]
The numbers of triples for textual relations are shown as zero for the validation and test sets because we don’t evaluate on prediction of textual relations (all text triples are used in training).,Dataset and Evaluation Protocol,[0],[0]
"The percentage of KB triples that have textual relations for their pair of entities is 40.5% for the training, 26.6% for the validation, and 28.1% for the test set.",Dataset and Evaluation Protocol,[0],[0]
"While 26.6% of the validation set triples have textual mentions, the percentage with textual relations that have been seen in the training set is 18.4%.",Dataset and Evaluation Protocol,[0],[0]
"Having a mention increases the chance that a random entity pair has a relation from 0.1% to 5.0% — a fifty-fold increase.
",Dataset and Evaluation Protocol,[0],[0]
"Given a set of triples in a set disjoint from a training knowledge graph, we test models on predicting the object of each triple, given the subject and relation type.",Dataset and Evaluation Protocol,[0],[0]
We rank all entities in the training knowledge base in order of their likelihood of filling the argument position.,Dataset and Evaluation Protocol,[0],[0]
"We report the mean reciprocal rank (MRR) of the correct entity, as well as HITS@10 — the percentage of test triples for which the correct entity is ranked in the top 10.",Dataset and Evaluation Protocol,[0],[0]
"We use filtered measures following the protocol proposed in Bordes et al. (2013) — that is, when we rank entities for a given position, we remove all other entities that are known to be part of an existing triple in the training, validation, or test set.",Dataset and Evaluation Protocol,[0.9599226238580152],"['In our experiments we have also observed that the proposed approaches tend to place the inducing points one on top of each other, which can be seen as an inducing point pruning technique (Bauer et al., 2016).']"
"This avoids penalizing the model for ranking other correct fillers higher than the tested entity.
",Dataset and Evaluation Protocol,[0],[0]
"5The full set of 37 million textual patterns connecting the entity pairs of interest was pruned based on the count of patterns and their tri-grams, and their precision in indicating that entity pairs have KB relations.",Dataset and Evaluation Protocol,[0],[0]
"We used a value of λ = 1 for the weight of the L2 penalty for the main results in Table 3, and present some results on the impact of λ at the end of this section.",Implementation details,[0],[0]
We used batch optimization after initial experiments with AdaGrad showed inferior performance.,Implementation details,[0],[0]
"L-BFGS (Liu and Nocedal, 1989) and RProp (Riedmiller and Braun, 1993) were found to converge to similar function values, with RProp converging significantly faster.",Implementation details,[0],[0]
We thus used RProp for optimization.,Implementation details,[0],[0]
"We initialized the KB+text models from the KB-only models and also from random initial values (sampled from a Gaussian distribution), and stopped optimization when the overall MRR on the validation set decreased.",Implementation details,[0],[0]
"For each model type, we chose the better of random and KB-only initialization.",Implementation details,[0],[0]
"The word embeddings in the CONV models were initialized using the 50-dimensional vectors from Turian et al. (2010) in the main experiments, with a slight positive impact.",Implementation details,[0],[0]
"The effect of initialization is discussed at the end of the section.
",Implementation details,[0],[0]
The number of negative examples for each triple was set to 200.,Implementation details,[0],[0]
Performance improved substantially when the number of negative examples was increased and reached a plateau around 200.,Implementation details,[0],[0]
"We chose the optimal number of latent feature dimensions via a grid search to optimize MRR on the validation set, testing the values 5, 10, 15, 35, 50, 100, 200 and 500.",Implementation details,[0],[0]
"We also performed a grid search over the values of the parameter τ , testing values in the set {0.01, 0.1, 0.25, 0.5, 1}.",Implementation details,[0],[0]
"The best dimension for latent feature vectors was 10 for most KBonly models (not including model F), and 5 for the two model configurations including F. We used K = 10 for all KB+text models, as higher dimension was also not helpful for them.",Implementation details,[0],[0]
"In Table 3 we show the performance of different models and their combinations6, both when using textual mentions (KB+text), and when using only knowledge base relations (KB only).",Experimental results,[0],[0]
"In the KB+text setting, we evaluate the contribution of the CONV representations of the textual relations.",Experimental results,[0],[0]
"The upper portion of the Table shows the performance of models that have been trained using knowledge graphs including only knowledge
6Different models are combined by simply defining a combined scoring function which adds the scores from individual models.",Experimental results,[0],[0]
"Combined models are trained jointly.
base relations, and are not using any information from textual mentions.",Experimental results,[0],[0]
The lower portion of the Table shows the performance when textual relations are added to the training knowledge graph and the corresponding training loss function.,Experimental results,[0],[0]
"Note that all models predict based on the learned knowledge base relation and entity representations, and the textual relations are only used at training time when they can impact these representations.
",Experimental results,[0],[0]
"The performance of all models is shown as an overall MRR (scaled by 100) and HITS@10, as well as performance on the subset of triples that have textual mentions (column With mentions), and ones that do not (column Without mentions).",Experimental results,[0],[0]
"Around 28% of the test triples have mentions and contribute toward the measures in the With mentions column, and the other 72% of the test triples contribute to the Without mentions column.
",Experimental results,[0],[0]
"For the KB-only models, we see the performance of each individual model F, E, and DISTMULT.",Experimental results,[0],[0]
"Model F was the best performing single model from (Riedel et al., 2013), but it does not perform well when textual mentions are not used.",Experimental results,[0],[0]
"In our implementation of model F, we created entity pair parameters only for entity pairs that cooccur in the text data (Riedel et al. (2013) also trained pairwise vectors for co-occuring entities
only, but all of the training and test tuples in their study were co-occurring)7.",Experimental results,[0],[0]
"Without textual information, model F is performing essentially randomly, because entity pairs in the test sets do not occur in training set relations (by construction of the dataset).",Experimental results,[0],[0]
"Model E is able to do surprisingly well, given that it is making predictions for each object position of a relation without considering the given subject of the relation.",Experimental results,[0],[0]
DISTMULT is the best performing single model.,Experimental results,[0],[0]
"Unlike model F, it is able to share parameters among entity pairs with common subject or object entities, and, unlike model E, it captures some dependencies between the subject and object entities of a relation.",Experimental results,[0],[0]
"The combination of models E+DISTMULT improves performance, but combining model F with the other two is not helpful.
",Experimental results,[0],[0]
The lower portion of Table 3 shows results when textual relations are added to the training knowledge graph.,Experimental results,[0],[0]
The basic models treat the textual relations as atomic and learn a separate latent feature vector for each textual relation.,Experimental results,[0],[0]
"The CONV- models use the compositional representations of tex-
7Learning entity pair parameters for all entity pairs would result in 2.2 billion parameters for vectors with dimensionality 10 for our dataset.",Experimental results,[0],[0]
"This was infeasible and was also not found useful based on experiments with vectors of lower dimensionality.
",Experimental results,[0],[0]
tual relations learned using the convolutional neural network architecture shown in Figure 4.,Experimental results,[0],[0]
We show the performance of each individual model and its corresponding variant with a CONV parameterization.,Experimental results,[0],[0]
"For each model, we also show the optimal value of τ , the weight of the textual relations loss.",Experimental results,[0],[0]
"Model F is able to benefit from textual relations and its performance increases by 2.5 points in MRR, with the gain in performance being particularly large on test triples with textual mentions.",Experimental results,[0],[0]
Model F is essentially limiting its space of considered argument fillers to ones that have cooccurred with the given subject entity.,Experimental results,[0],[0]
"This gives it an advantage on test triples with textual mentions, but model F still does relatively very poorly overall when taking into account the much more numerous test triples without textual mentions.",Experimental results,[0],[0]
"The CONV parameterization performs slightly worse in MRR, but slightly better in HITS@10, compared to the atomic parameterization.",Experimental results,[0],[0]
"For model E and its CONV variant, we see that text does not help as its performance using text is the same as that when not using text and the optimal weight of the text is zero.",Experimental results,[0],[0]
"Model DISTMULT benefits from text, and its convolutional text variant CONVDISTMULT outperforms the basic model, with the gain being larger on test triples with mentions.
",Experimental results,[0],[0]
"The best model overall, as in the KB-only case, is E+DISTMULT.",Experimental results,[0],[0]
"The basic model benefits from text slightly and the model with compositional representations of textual patterns CONVE+CONV-DISTMULT, improves the performance further, by 2.4 MRR overall, and by 5 MRR on triples with textual mentions.",Experimental results,[0],[0]
It is interesting that the text and the compositional representations helped most for this combined model.,Experimental results,[0],[0]
"One hypothesis is that model E, which provides a prior over relation arguments, is needed in combination with DISTMULT to prevent the prediction of unlikely arguments based on noisy inference from textual patterns and their individual words and dependency links.",Experimental results,[0],[0]
"To gain insight into the sensitivity of the model to hyper-parameters and initialization, we report on experiments starting with the best model CONVE + CONV-DISTMULT from Table 3 and varying one parameter at a time.",Hyperparameter Sensitivity,[0],[0]
"This model has weight of the textual relations loss τ = 0.25, weight of the L2 penalty λ = 1, convolution window size of
three, and is initialized randomly for the entity and KB relation vectors, and from pre-trained embeddings for word vectors (Turian et al., 2010).",Hyperparameter Sensitivity,[0],[0]
"The overall MRR of the model is 40.4 on the validation set (test results are shown in the Table).
",Hyperparameter Sensitivity,[0],[0]
"When the weight of τ is changed to 1 (i.e., equal contribution of textual and KB relations), the overall MRR goes down to 39.6 from 40.4, indicating the usefulness of weighting the two kinds of relations non-uniformly.",Hyperparameter Sensitivity,[0],[0]
"When λ is reduced to 0.04, MRR is 40.0 and when λ is increased to 25, MRR goes down to 38.9.",Hyperparameter Sensitivity,[0],[0]
This indicates the L2 penalty hyper-parameter has a large impact on performance.,Hyperparameter Sensitivity,[0],[0]
"When we initialize the word embeddings randomly instead of using pre-trained word vectors, performance drops only slightly to 40.3.",Hyperparameter Sensitivity,[0],[0]
"If we initialize from a model trained using KBonly information, performance goes down substantially to 38.7.",Hyperparameter Sensitivity,[0],[0]
This indicates that initialization is important and there is a small gain from using pre-trained word embeddings.,Hyperparameter Sensitivity,[0],[0]
"There was a drop in performance to MRR 40.2 when using a window size of one for the convolutional architecture in Figure 4, and an increase to 40.6 when using a window size of five.",Hyperparameter Sensitivity,[0],[0]
Here we explored an alternative representation of textual relations for latent feature models that learn to represent knowledge base and textual relations in the same vector space.,5 Conclusion and Future Work,[0],[0]
"We showed that given the large degree of sharing of sub-structure in the textual relations, it was beneficial to compose their continuous representations out of the representations of their component words and dependency arc links.",5 Conclusion and Future Work,[0],[0]
"We applied a convolutional neural network model and trained it jointly with a model mapping entities and knowledge base relations to the same vector space, obtaining substantial improvements over an approach that treats the textual relations as atomic units having independent parameterization.",5 Conclusion and Future Work,[0],[0]
"We would like to thank the anonymous reviewers for their suggestions, and Jianfeng Gao, Scott Wen-tau Yih, and Wei Xu for useful discussions.",Acknowledgements,[0],[0]
"Models that learn to represent textual and knowledge base relations in the same continuous latent space are able to perform joint inferences among the two kinds of relations and obtain high accuracy on knowledge base completion (Riedel et al., 2013).",abstractText,[0],[0]
"In this paper we propose a model that captures the compositional structure of textual relations, and jointly optimizes entity, knowledge base, and textual relation representations.",abstractText,[0],[0]
The proposed model significantly improves performance over a model that does not share parameters among textual relations with common sub-structure.,abstractText,[0],[0]
Representing Text for Joint Embedding of Text and Knowledge Bases,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1257–1266 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
1257",text,[0],[0]
"The construction of large-scale Knowledge Bases (KBs) like Freebase (Bollacker et al., 2008) and Wikidata (Vrandečić and Krötzsch, 2014) has proven to be useful in many natural language processing (NLP) tasks like question-answering, web search, etc.",1 Introduction,[0],[0]
"However, these KBs are not exhaustive.",1 Introduction,[0],[0]
Relation Extraction (RE) attempts to fill this gap by extracting semantic relationships between entity pairs from plain text.,1 Introduction,[0],[0]
This task can be modeled as a simple classification problem after the entity pairs are specified.,1 Introduction,[0],[0]
"Formally, given an entity pair (e1,e2) from the KB and an entity annotated sentence (or instance), we aim to predict the
∗Research internship at Indian Institute of Science.
relation r, from a predefined relation set, that exists between e1 and e2.",1 Introduction,[0],[0]
"If no relation exists, we simply label it NA.
",1 Introduction,[0],[0]
Most supervised relation extraction methods require large labeled training data which is expensive to construct.,1 Introduction,[0],[0]
"Distant Supervision (DS) (Mintz et al., 2009) helps with the construction of this dataset automatically, under the assumption that if two entities have a relationship in a KB, then all sentences mentioning those entities express the same relation.",1 Introduction,[0],[0]
"While this approach works well in generating large amounts of training instances, the DS assumption does not hold in all cases.",1 Introduction,[0],[0]
Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) propose multi-instance based learning to relax this assumption.,1 Introduction,[0],[0]
"However, they use NLP tools to extract features, which can be noisy.
",1 Introduction,[0],[0]
"Recently, neural models have demonstrated promising performance on RE.",1 Introduction,[0],[0]
"Zeng et al. (2014, 2015) employ Convolutional Neural Networks (CNN) to learn representations of instances.",1 Introduction,[0],[0]
"For alleviating noise in distant supervised datasets, attention has been utilized by (Lin et al., 2016; Jat et al., 2018).",1 Introduction,[0],[0]
"Syntactic information from dependency parses has been used by (Mintz et al., 2009; He et al., 2018) for capturing long-range dependencies between tokens.",1 Introduction,[0],[0]
"Recently proposed Graph Convolution Networks (GCN) (Defferrard et al., 2016) have been effectively employed for encoding this information (Marcheggiani and Titov, 2017; Bastings et al., 2017).",1 Introduction,[0],[0]
"However, all the above models rely only on the noisy instances from distant supervision for RE.
",1 Introduction,[0],[0]
Relevant side information can be effective for improving RE.,1 Introduction,[0],[0]
"For instance, in the sentence, Microsoft was started by Bill Gates., the type information of Bill Gates (person) and Microsoft (organization) can be helpful in predicting the correct relation founderOfCompany.",1 Introduction,[0],[0]
"This is because every relation constrains the type of its target en-
tities.",1 Introduction,[0],[0]
"Similarly, relation phrase “was started by” extracted using Open Information Extraction (Open IE) methods can be useful, given that the aliases of relation founderOfCompany, e.g., founded, co-founded, etc., are available.",1 Introduction,[0],[0]
"KBs used for DS readily provide such information which has not been completely exploited by current models.
",1 Introduction,[0],[0]
"In this paper, we propose RESIDE, a novel distant supervised relation extraction method which utilizes additional supervision from KB through its neural network based architecture.",1 Introduction,[0],[0]
"RESIDE makes principled use of entity type and relation alias information from KBs, to impose soft constraints while predicting the relation.",1 Introduction,[0],[0]
"It uses encoded syntactic information obtained from Graph Convolution Networks (GCN), along with embedded side information, to improve neural relation extraction.",1 Introduction,[0],[0]
"Our contributions can be summarized as follows:
• We propose RESIDE, a novel neural method which utilizes additional supervision from KB in a principled manner for improving distant supervised RE.",1 Introduction,[0],[0]
•,1 Introduction,[0],[0]
"RESIDE uses Graph Convolution Networks
(GCN) for modeling syntactic information and has been shown to perform competitively even with limited side information.",1 Introduction,[0],[0]
•,1 Introduction,[0],[0]
"Through extensive experiments on benchmark
datasets, we demonstrate RESIDE’s effectiveness over state-of-the-art baselines.
RESIDE’s source code and datasets used in the paper are available at http://github.com/ malllabiisc/RESIDE.",1 Introduction,[0],[0]
Distant supervision: Relation extraction is the task of identifying the relationship between two entity mentions in a sentence.,2 Related Work,[0],[0]
"In supervised paradigm, the task is considered as a multi-class classification problem but suffers from lack of large labeled training data.",2 Related Work,[0],[0]
"To address this limitation, (Mintz et al., 2009) propose distant supervision (DS) assumption for creating large datasets, by heuristically aligning text to a given Knowledge Base (KB).",2 Related Work,[0],[0]
"As this assumption does not always hold true, some of the sentences might be wrongly labeled.",2 Related Work,[0],[0]
"To alleviate this shortcoming, Riedel et al. (2010) relax distant supervision for multi-instance single-label learning.",2 Related Work,[0],[0]
"Subsequently, for handling overlapping relations between entities (Hoffmann et al., 2011; Surdeanu et al., 2012) propose multi-instance multi-label learning paradigm.
",2 Related Work,[0],[0]
Neural Relation Extraction: The performance of the above methods strongly rely on the quality of hand engineered features.,2 Related Work,[0],[0]
"Zeng et al. (2014)
propose an end-to-end CNN based method which could automatically capture relevant lexical and sentence level features.",2 Related Work,[0],[0]
"This method is further improved through piecewise max-pooling by (Zeng et al., 2015).",2 Related Work,[0],[0]
"Lin et al. (2016); Nagarajan et al. (2017) use attention (Bahdanau et al., 2014) for learning from multiple valid sentences.",2 Related Work,[0],[0]
"We also make use of attention for learning sentence and bag representations.
",2 Related Work,[0],[0]
"Dependency tree based features have been found to be relevant for relation extraction (Mintz et al., 2009).",2 Related Work,[0],[0]
He et al. (2018) use them for getting promising results through a recursive tree-GRU based model.,2 Related Work,[0],[0]
"In RESIDE, we make use of recently proposed Graph Convolution Networks (Defferrard et al., 2016; Kipf and Welling, 2017), which have been found to be quite effective for modelling syntactic information (Marcheggiani and Titov, 2017; Nguyen and Grishman, 2018; Vashishth et al., 2018a).
",2 Related Work,[0],[0]
"Side Information in RE: Entity description from KB has been utilized for RE (Ji et al., 2017), but such information is not available for all entities.",2 Related Work,[0],[0]
Type information of entities has been used by Ling and Weld (2012); Liu et al. (2014) as features in their model.,2 Related Work,[0],[0]
Yaghoobzadeh et al. (2017) also attempt to mitigate noise in DS through their joint entity typing and relation extraction model.,2 Related Work,[0],[0]
"However, KBs like Freebase readily provide reliable type information which could be directly utilized.",2 Related Work,[0],[0]
"In our work, we make principled use of entity type and relation alias information obtained from KB.",2 Related Work,[0],[0]
"We also use unsupervised Open Information Extraction (Open IE) methods (Mausam et al., 2012; Angeli et al., 2015), which automatically discover possible relations without the need of any predefined ontology, which is used as a side information as defined in Section 5.2.",2 Related Work,[0],[0]
"In this section, we provide a brief overview of Graph Convolution Networks (GCN) for graphs with directed and labeled edges, as used in (Marcheggiani and Titov, 2017).",3 Background: Graph Convolution Networks (GCN),[0],[0]
"For a directed graph, G = (V, E), where V and E represent the set of vertices and edges respectively, an edge from node u to node v with label luv is represented as (u, v, luv).",3.1 GCN on Labeled Directed Graph,[0],[0]
"Since, informa-
tion in directed edge does not necessarily propagate along its direction, following (Marcheggiani and Titov, 2017) we define an updated edge set E ′ which includes inverse edges (v, u, l−1uv ) and",3.1 GCN on Labeled Directed Graph,[0],[0]
"selfloops (u, u,>) along with the original edge set E , where > is a special symbol to denote self-loops.",3.1 GCN on Labeled Directed Graph,[0],[0]
"For each node v in G, we have an initial representation",3.1 GCN on Labeled Directed Graph,[0],[0]
"xv ∈ Rd, ∀v ∈ V .",3.1 GCN on Labeled Directed Graph,[0],[0]
"On employing GCN, we get an updated d-dimensional hidden representation hv ∈ Rd, ∀v ∈ V , by considering only its immediate neighbors (Kipf and Welling, 2017).",3.1 GCN on Labeled Directed Graph,[0],[0]
"This can be formulated as:
hv = f  ∑ u∈N (v) (Wluvxu + bluv)  .",3.1 GCN on Labeled Directed Graph,[0],[0]
"Here, Wluv ∈ Rd×d and bluv ∈ Rd are label dependent model parameters which are trained based on the downstream task.",3.1 GCN on Labeled Directed Graph,[0],[0]
N (v) refers to the set of neighbors of v based on E ′,3.1 GCN on Labeled Directed Graph,[0],[0]
and f is any non-linear activation function.,3.1 GCN on Labeled Directed Graph,[0],[0]
"In order to capture multihop neighborhood, multiple GCN layers can be stacked.",3.1 GCN on Labeled Directed Graph,[0],[0]
"Hidden representation of node v in this case after kth GCN layer is given as:
hk+1v = f  ∑ u∈N (v) ( W kluvh k u + b k luv ) .",3.1 GCN on Labeled Directed Graph,[0],[0]
"In automatically constructed graphs, some edges might be erroneous and hence need to be discarded.",3.2 Integrating Edge Importance,[0],[0]
"Edgewise gating in GCN by (Bastings et al., 2017; Marcheggiani and Titov, 2017) allows us to alleviate this problem by subduing the noisy edges.",3.2 Integrating Edge Importance,[0],[0]
This is achieved by assigning a relevance score to each edge in the graph.,3.2 Integrating Edge Importance,[0],[0]
"At kth layer, the importance of an edge (u, v, luv) is computed as:
gkuv = σ",3.2 Integrating Edge Importance,[0],[0]
"( hku · ŵkluv + b̂ k luv ) , (1)
Here, ŵkluv ∈ R m and b̂kluv ∈ R are parameters which are trained and σ(·) is the sigmoid function.",3.2 Integrating Edge Importance,[0],[0]
"With edgewise gating, the final GCN embedding for a node v after kth layer is given as:
hk+1v = f  ∑ u∈N (v) gkuv × ( W kluvh k u + b k luv ) .",3.2 Integrating Edge Importance,[0],[0]
(2),3.2 Integrating Edge Importance,[0],[0]
"In multi-instance learning paradigm, we are given a bag of sentences (or instances) {s1, s2, ...sn} for a given entity pair, the task is to predict the relation between them.",4 RESIDE Overview,[0],[0]
"RESIDE consists of three components for learning a representation of a given bag, which is fed to a softmax classifier.",4 RESIDE Overview,[0],[0]
We briefly present the components of RESIDE below.,4 RESIDE Overview,[0],[0]
Each component will be described in detail in the subsequent sections.,4 RESIDE Overview,[0],[0]
"The overall architecture of RESIDE is shown in Figure 1.
1.",4 RESIDE Overview,[0],[0]
Syntactic Sentence Encoding: RESIDE uses a Bi-GRU over the concatenated positional and word embedding for encoding the local context of each token.,4 RESIDE Overview,[0],[0]
"For capturing long-range dependencies, GCN over dependency tree is employed and its encoding is appended to the representation of each token.",4 RESIDE Overview,[0],[0]
"Finally, attention over tokens is used to subdue irrelevant tokens and get an embedding for the entire sentence.",4 RESIDE Overview,[0],[0]
"More details in Section 5.1.
2.",4 RESIDE Overview,[0],[0]
Side Information Acquisition:,4 RESIDE Overview,[0],[0]
"In this module, we use additional supervision from KBs and utilize Open IE methods for getting relevant side information.",4 RESIDE Overview,[0],[0]
"This information is later utilized by the model as described in Section 5.2.
3.",4 RESIDE Overview,[0],[0]
Instance Set Aggregation:,4 RESIDE Overview,[0],[0]
"In this part, sentence representation from syntactic sentence encoder is concatenated with the matched relation embedding obtained from the previous step.",4 RESIDE Overview,[0],[0]
"Then, using attention over sentences, a representation for the entire bag is learned.",4 RESIDE Overview,[0],[0]
This is then concatenated with entity type embedding before feeding into the softmax classifier for relation prediction.,4 RESIDE Overview,[0],[0]
Please refer to Section 5.3 for more details.,4 RESIDE Overview,[0],[0]
"In this section, we provide the detailed description of the components of RESIDE.",5 RESIDE Details,[0],[0]
"For each sentence in the bag si with m tokens {w1, w2, ...wm}, we first represent each token by k-dimensional GloVe embedding (Pennington et al., 2014).",5.1 Syntactic Sentence Encoding,[0],[0]
"For incorporating relative position of tokens with respect to target entities, we use p-dimensional position embeddings, as used by
(Zeng et al., 2014).",5.1 Syntactic Sentence Encoding,[0],[0]
The combined token embeddings are stacked together to get the sentence representationH ∈ Rm×(k+2p).,5.1 Syntactic Sentence Encoding,[0],[0]
"Then, using Bi-GRU (Cho et al., 2014) over H, we get the new sentence representationHgru ∈ Rm×dgru , where dgru is the hidden state dimension.",5.1 Syntactic Sentence Encoding,[0],[0]
"Bi-GRUs have been found to be quite effective in encoding the context of tokens in several tasks (Sutskever et al., 2014; Graves et al., 2013).
",5.1 Syntactic Sentence Encoding,[0],[0]
"Although Bi-GRU is capable of capturing local context, it fails to capture long-range dependencies which can be captured through dependency edges.",5.1 Syntactic Sentence Encoding,[0],[0]
"Prior works (Mintz et al., 2009; He et al., 2018) have exploited features from syntactic dependency trees for improving relation extraction.",5.1 Syntactic Sentence Encoding,[0],[0]
"Motivated by their work, we employ Syntactic Graph Convolution Networks for encoding this information.",5.1 Syntactic Sentence Encoding,[0],[0]
"For a given sentence, we generate its dependency tree using Stanford CoreNLP",5.1 Syntactic Sentence Encoding,[0],[0]
"(Manning et al., 2014).",5.1 Syntactic Sentence Encoding,[0],[0]
"We then run GCN over the dependency graph and use Equation 2 for updating the embeddings, taking Hgru as the input.",5.1 Syntactic Sentence Encoding,[0],[0]
"Since dependency graph has 55 different edge labels, incorporating all of them overparameterizes the model significantly.",5.1 Syntactic Sentence Encoding,[0],[0]
"Therefore, following (Marcheggiani and Titov, 2017; Nguyen and Grishman, 2018; Vashishth et al., 2018a) we use only three edge labels based on the direction of the edge {forward (→), backward (←), selfloop (>)}.",5.1 Syntactic Sentence Encoding,[0],[0]
"We define the new edge label Luv for an edge (u, v, luv) as follows:
",5.1 Syntactic Sentence Encoding,[0],[0]
"Luv =  → if edge exists in dependency parse ← if edge is an inverse edge > if edge is a self-loop
For each token wi, GCN embedding h gcn ik+1 ∈",5.1 Syntactic Sentence Encoding,[0],[0]
"Rdgcn after kth layer is defined as:
hgcnik+1 =",5.1 Syntactic Sentence Encoding,[0],[0]
f,5.1 Syntactic Sentence Encoding,[0],[0]
"( ∑ u∈N (i) gkiu × ( W kLiuh gcn uk + bkLiu )) .
",5.1 Syntactic Sentence Encoding,[0],[0]
"Here, gkiu denotes edgewise gating as defined in Equation 1 and Liu refers to the edge label defined above.",5.1 Syntactic Sentence Encoding,[0],[0]
"We use ReLU as activation function f , throughout our experiments.",5.1 Syntactic Sentence Encoding,[0],[0]
"The syntactic graph encoding from GCN is appended to Bi-GRU output to get the final token representation, hconcati as [hgrui ;h gcn ik+1
].",5.1 Syntactic Sentence Encoding,[0],[0]
"Since, not all tokens are equally relevant for RE task, we calculate the degree of relevance of each token using attention as used in
(Jat et al., 2018).",5.1 Syntactic Sentence Encoding,[0],[0]
"For token wi in the sentence, attention weight αi is calculated as:
αi = exp(ui)∑m j=1 exp(uj) where, ui = hconcati ·",5.1 Syntactic Sentence Encoding,[0],[0]
"r.
where r is a random query vector and ui is the relevance score assigned to each token.",5.1 Syntactic Sentence Encoding,[0],[0]
Attention values {αi} are calculated by taking softmax over {ui}.,5.1 Syntactic Sentence Encoding,[0],[0]
"The representation of a sentence is given as a weighted sum of its tokens, s =∑m
j=1 αih concat i .",5.1 Syntactic Sentence Encoding,[0],[0]
"Relevant side information has been found to improve performance on several tasks (Ling and Weld, 2012; Vashishth et al., 2018b).",5.2 Side Information Acquisition,[0],[0]
"In distant supervision based relation extraction, since the entities are from a KB, knowledge about them can be utilized to improve relation extraction.",5.2 Side Information Acquisition,[0],[0]
"Moreover, several unsupervised relation extraction methods (Open IE) (Angeli et al., 2015; Mausam et al., 2012) allow extracting relation phrases between target entities without any predefined ontology and thus can be used to obtain relevant side information.",5.2 Side Information Acquisition,[0],[0]
"In RESIDE, we employ Open IE methods and additional supervision from KB for improving neural relation extraction.
",5.2 Side Information Acquisition,[0],[0]
"Relation Alias Side Information RESIDE uses Stanford Open IE (Angeli et al., 2015) for extracting relation phrases between target entities, which we denote by P .",5.2 Side Information Acquisition,[0],[0]
"As shown in Figure 2, for the sentence Matt Coffin, executive of
lowermybills, a company.., Open IE methods extract “executive of” between Matt Coffin and lowermybills.",5.2 Side Information Acquisition,[0],[0]
"Further, we extend P by including tokens at one hop distance in dependency path from target entities.",5.2 Side Information Acquisition,[0],[0]
"Such features from dependency parse have been exploited in the past by (Mintz et al., 2009; He et al., 2018).",5.2 Side Information Acquisition,[0],[0]
The degree of match between the extracted phrases in P and aliases of a relation can give important clues about the relevance of that relation for the sentence.,5.2 Side Information Acquisition,[0],[0]
"Several KBs like Wikidata provide such relation aliases, which can be readily exploited.",5.2 Side Information Acquisition,[0],[0]
"In RESIDE, we further expand the relation alias set using Paraphrase database (PPDB) (Pavlick et al., 2015).",5.2 Side Information Acquisition,[0],[0]
"We note that even for cases when aliases for relations are not available, providing only the names of relations give competitive performance.",5.2 Side Information Acquisition,[0],[0]
"We shall explore this point further in Section 7.3.
",5.2 Side Information Acquisition,[0],[0]
"For matching P with the PPDB expanded relation alias setR, we project both in a d-dimensional space using GloVe embeddings (Pennington et al., 2014).",5.2 Side Information Acquisition,[0],[0]
"Projecting phrases using word embeddings helps to further expand these sets, as semantically similar words are closer in embedding space (Mikolov et al., 2013; Pennington et al., 2014).",5.2 Side Information Acquisition,[0],[0]
"Then, for each phrase p ∈ P , we calculate its cosine distance from all relation aliases inR and take the relation corresponding to the closest relation alias as a matched relation for the sentence.",5.2 Side Information Acquisition,[0],[0]
We use a threshold on cosine distance to remove noisy aliases.,5.2 Side Information Acquisition,[0],[0]
"In RESIDE, we define a kr-dimensional embedding for each relation which we call as matched relation embedding (hrel).",5.2 Side Information Acquisition,[0],[0]
"For a given sentence, hrel is concatenated with its representa-
tion s, obtained from syntactic sentence encoder (Section 5.1) as shown in Figure 1.",5.2 Side Information Acquisition,[0],[0]
"For sentences with |P| > 1, we might get multiple matched relations.",5.2 Side Information Acquisition,[0],[0]
"In such cases, we take the average of their embeddings.",5.2 Side Information Acquisition,[0],[0]
"We hypothesize that this helps in improving the performance and find it to be true as shown in Section 7.
Entity Type Side Information Type information of target entities has been shown to give promising results on relation extraction (Ling and Weld, 2012; Yaghoobzadeh et al., 2017).",5.2 Side Information Acquisition,[0],[0]
Every relation puts some constraint on the type of entities which can be its subject and object.,5.2 Side Information Acquisition,[0],[0]
"For example, the relation person/place of birth can only occur between a person and a location.",5.2 Side Information Acquisition,[0],[0]
"Sentences in distance supervision are based on entities in KBs, where the type information is readily available.
",5.2 Side Information Acquisition,[0],[0]
"In RESIDE, we use types defined by FIGER (Ling and Weld, 2012) for entities in Freebase.",5.2 Side Information Acquisition,[0],[0]
"For each type, we define a kt-dimensional embedding which we call as entity type embedding (htype).",5.2 Side Information Acquisition,[0],[0]
"For cases when an entity has multiple types in different contexts, for instance, Paris may have types government and location, we take the average over the embeddings of each type.",5.2 Side Information Acquisition,[0],[0]
We concatenate the entity type embedding of target entities to the final bag representation before using it for relation classification.,5.2 Side Information Acquisition,[0],[0]
"To avoid over-parameterization, instead of using all fine-grained 112 entity types, we use 38 coarse types which form the first hierarchy of FIGER types.",5.2 Side Information Acquisition,[0],[0]
"For utilizing all valid sentences, following (Lin et al., 2016; Jat et al., 2018), we use attention over sentences to obtain a representation for the entire bag.",5.3 Instance Set Aggregation,[0],[0]
"Instead of directly using the sentence representation si from Section 5.1, we concatenate the embedding of each sentence with matched relation embedding hreli as obtained from Section 5.2.",5.3 Instance Set Aggregation,[0],[0]
"The attention score αi for ith sentence is formulated as:
αi = exp(ŝi · q)∑n j=1 exp(ŝj · q) where, ŝi =",5.3 Instance Set Aggregation,[0],[0]
"[si;hreli ].
here q denotes a random query vector.",5.3 Instance Set Aggregation,[0],[0]
"The bag representation B, which is the weighted sum of its sentences, is then concatenated with the entity type embeddings of the subject (htypesub ) and object
(htypeobj ) from Section 5.2 to obtain B̂.
B̂ = [B;htypesub ;h type obj ] where, B = n∑ i=1 αiŝi.
",5.3 Instance Set Aggregation,[0],[0]
"Finally, B̂ is fed to a softmax classifier to get the probability distribution over the relations.
p(y) =",5.3 Instance Set Aggregation,[0],[0]
Softmax(W · B̂ + b).,5.3 Instance Set Aggregation,[0],[0]
"In our experiments, we evaluate the models on Riedel and Google Distant Supervision (GIDS) dataset.",6.1 Datasets,[0],[0]
Statistics of the datasets is summarized in Table 1.,6.1 Datasets,[0],[0]
"Below we described each in detail1.
1.",6.1 Datasets,[0],[0]
Riedel:,6.1 Datasets,[0],[0]
"The dataset is developed by (Riedel et al., 2010) by aligning Freebase relations with New York Times (NYT) corpus, where sentences from the year 2005-2006 are used for creating the training set and from the year 2007 for the test set.",6.1 Datasets,[0],[0]
"The entity mentions are annotated using Stanford NER (Finkel et al., 2005) and are linked to Freebase.",6.1 Datasets,[0],[0]
"The dataset has been widely used for RE by (Hoffmann et al., 2011; Surdeanu et al., 2012) and more recently by (Lin et al., 2016; Feng et al.;",6.1 Datasets,[0],[0]
"He et al., 2018).
2.",6.1 Datasets,[0],[0]
GIDS:,6.1 Datasets,[0],[0]
Jat et al. (2018) created Google Distant Supervision (GIDS) dataset by extending the Google relation extraction corpus2 with additional instances for each entity pair.,6.1 Datasets,[0],[0]
"The dataset assures that the at-least-one assumption of multi-instance learning, holds.",6.1 Datasets,[0],[0]
"This makes automatic evaluation more reliable and thus removes the need for manual verification.
",6.1 Datasets,[0],[0]
1Data splits and hyperparameters are in supplementary.,6.1 Datasets,[0],[0]
"2https://research.googleblog.com/2013/04/50000-
lessons-on-how-to-read-relation.html",6.1 Datasets,[0],[0]
"For evaluating RESIDE, we compare against the following baselines:
• Mintz: Multi-class logistic regression model proposed by (Mintz et al., 2009) for distant supervision paradigm.",6.2 Baselines,[0],[0]
"• MultiR: Probabilistic graphical model for multi
instance learning by (Hoffmann et al., 2011)",6.2 Baselines,[0],[0]
"• MIMLRE: A graphical model which jointly
models multiple instances and multiple labels.",6.2 Baselines,[0],[0]
"More details in (Surdeanu et al., 2012).",6.2 Baselines,[0],[0]
• PCNN:,6.2 Baselines,[0],[0]
"A CNN based relation extraction model
by (Zeng et al., 2015) which uses piecewise max-pooling for sentence representation.",6.2 Baselines,[0],[0]
• PCNN+ATT,6.2 Baselines,[0],[0]
": A piecewise max-pooling over
CNN based model which is used by (Lin et al., 2016) to get sentence representation followed by attention over sentences.",6.2 Baselines,[0],[0]
"• BGWA: Bi-GRU based relation extraction
model with word and sentence level attention (Jat et al., 2018).",6.2 Baselines,[0],[0]
•,6.2 Baselines,[0],[0]
RESIDE:,6.2 Baselines,[0],[0]
"The method proposed in this paper,
please refer Section 5 for more details.",6.2 Baselines,[0],[0]
"Following the prior works (Lin et al., 2016; Feng et al.), we evaluate the models using held-out evaluation scheme.",6.3 Evaluation Criteria,[0],[0]
This is done by comparing the relations discovered from test articles with those in Freebase.,6.3 Evaluation Criteria,[0],[0]
We evaluate the performance of models with Precision-Recall curve and top-N precision (P@N) metric in our experiments.,6.3 Evaluation Criteria,[0],[0]
"In this section we attempt to answer the following questions:
Q1.",7 Results,[0],[0]
Is RESIDE more effective than existing approaches for distant supervised RE?,7 Results,[0],[0]
"(7.1)
Q2.",7 Results,[0],[0]
What is the effect of ablating different components on RESIDE’s performance?,7 Results,[0],[0]
"(7.2)
Q3.",7 Results,[0],[0]
How is the performance affected in the absence of relation alias information?,7 Results,[0],[0]
(7.3),7 Results,[0],[0]
"For evaluating the effectiveness of our proposed method, RESIDE, we compare it against the baselines stated in Section 6.2.",7.1 Performance Comparison,[0],[0]
We use only the neural baselines on GIDS dataset.,7.1 Performance Comparison,[0],[0]
The Precision-Recall curves on Riedel and GIDS are presented in Figure 3.,7.1 Performance Comparison,[0],[0]
"Overall, we find that RESIDE achieves higher precision over the entire recall range on both the datasets.",7.1 Performance Comparison,[0],[0]
All the non-neural baselines could not perform well as the features used by them are mostly derived from NLP tools which can be erroneous.,7.1 Performance Comparison,[0],[0]
RESIDE outperforms PCNN+ATT and BGWA which indicates that incorporating side information helps in improving the performance of the model.,7.1 Performance Comparison,[0],[0]
The higher performance of BGWA and PCNN+ATT over PCNN shows that attention helps in distant supervised RE.,7.1 Performance Comparison,[0],[0]
"Following (Lin et al., 2016; Liu et al., 2017), we also evaluate our method with different number of sentences.",7.1 Performance Comparison,[0],[0]
"Results summarized in Table 2, show the improved precision of RESIDE in all test settings, as compared to the neural baselines, which demonstrates
the efficacy of our model.",7.1 Performance Comparison,[0],[0]
"In this section, we analyze the effect of various components of RESIDE on its performance.",7.2 Ablation Results,[0],[0]
"For this, we evaluate various versions of our model with cumulatively removed components.",7.2 Ablation Results,[0],[0]
The experimental results are presented in Figure 4.,7.2 Ablation Results,[0],[0]
"We observe that on removing different components from RESIDE, the performance of the model degrades drastically.",7.2 Ablation Results,[0],[0]
The results validate that GCNs are effective at encoding syntactic information.,7.2 Ablation Results,[0],[0]
"Further, the improvement from side information shows that it is complementary to the features extracted from text, thus validating the central thesis of this paper, that inducing side information leads to improved relation extraction.",7.2 Ablation Results,[0],[0]
"In this section, we test the performance of the model in setting where relation alias information is not readily available.",7.3 Effect of Relation Alias Side Information,[0],[0]
"For this, we evaluate the performance of the model on four different settings:",7.3 Effect of Relation Alias Side Information,[0],[0]
"• None: Relation aliases are not available.
",7.3 Effect of Relation Alias Side Information,[0],[0]
• One: The name of relation is used as its alias.,7.3 Effect of Relation Alias Side Information,[0],[0]
"• One+PPDB: Relation name extended using
Paraphrase Database (PPDB).",7.3 Effect of Relation Alias Side Information,[0],[0]
•,7.3 Effect of Relation Alias Side Information,[0],[0]
"All: Relation aliases from Knowledge Base3
The overall results are summarized in Figure 5.",7.3 Effect of Relation Alias Side Information,[0],[0]
We find that the model performs best when aliases are provided by the KB itself.,7.3 Effect of Relation Alias Side Information,[0],[0]
"Overall, we find that RESIDE gives competitive performance even when very limited amount of relation alias information is available.",7.3 Effect of Relation Alias Side Information,[0],[0]
We observe that performance improves further with the availability of more alias information.,7.3 Effect of Relation Alias Side Information,[0],[0]
"In this paper, we propose RESIDE, a novel neural network based model which makes principled use of relevant side information, such as entity type and relation alias, from Knowledge Base, for improving distant supervised relation extraction.",8 Conclusion,[0],[0]
"RESIDE employs Graph Convolution Networks for
3Each relation in Riedel dataset is manually mapped to corresponding Wikidata property for getting relation aliases.",8 Conclusion,[0],[0]
"Few examples are presented in supplementary material.
encoding syntactic information of sentences and is robust to limited side information.",8 Conclusion,[0],[0]
"Through extensive experiments on benchmark datasets, we demonstrate RESIDE’s effectiveness over stateof-the-art baselines.",8 Conclusion,[0],[0]
We have made RESIDE’s source code publicly available to promote reproducible research.,8 Conclusion,[0],[0]
We thank the anonymous reviewers for their constructive comments.,Acknowledgements,[0],[0]
"This work is supported in part by the Ministry of Human Resource Development (Government of India), CAIR (DRDO) and by a gift from Google.",Acknowledgements,[0],[0]
Distantly-supervised Relation Extraction (RE) methods train an extractor by automatically aligning relation instances in a Knowledge Base (KB) with unstructured text.,abstractText,[0],[0]
"In addition to relation instances, KBs often contain other relevant side information, such as aliases of relations (e.g., founded and co-founded are aliases for the relation founderOfCompany).",abstractText,[0],[0]
RE models usually ignore such readily available side information.,abstractText,[0],[0]
"In this paper, we propose RESIDE, a distantly-supervised neural relation extraction method which utilizes additional side information from KBs for improved relation extraction.",abstractText,[0],[0]
It uses entity type and relation alias information for imposing soft constraints while predicting relations.,abstractText,[0],[0]
RESIDE employs Graph Convolution Networks (GCN) to encode syntactic information from text and improves performance even when limited side information is available.,abstractText,[0],[0]
"Through extensive experiments on benchmark datasets, we demonstrate RESIDE’s effectiveness.",abstractText,[0],[0]
We have made RESIDE’s source code available to encourage reproducible research.,abstractText,[0],[0]
RESIDE: Improving Distantly-Supervised Neural Relation Extraction using Side Information,title,[0],[0]
"The spread of data-driven decision making to civic institutions, spurred by the empirical success of machine learning and the growing availability of individual-level data, raises
This material is based upon work supported by the National Science Foundation under Grant No. 1656996.",1. Introduction,[0],[0]
Angela Zhou is supported through the National Defense Science & Engineering Graduate Fellowship Program.,1. Introduction,[0],[0]
"1Cornell University, and Cornell Tech, New York 2Cornell University, Ithaca, New York.",1. Introduction,[0],[0]
Correspondence to: Angela Zhou,1. Introduction,[0],[0]
"<az434@cornell.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
new questions about the possible harms of learning from data which is subject to historical bias.,1. Introduction,[0],[0]
"Unlike clean-cut prediction problems in other domains, datasets of individuals and their historical outcomes may reflect systematic bias due to previously prejudiced decisions (Barocas & Selbst, 2014).",1. Introduction,[0],[0]
"Recent work on fairness in machine learning proposes and analyzes competing criteria for assessing the fairness of machine learning algorithms, where some adjustments attempt to equalize accuracy metrics across groups (Corbett-Davies et al., 2017; Kleinberg et al., 2017; Hardt et al., 2016).",1. Introduction,[0],[0]
"Other work studies how historical prejudices may be reflected in training data such that algorithmic systems might replicate historical biases (Angwin et al., 2016; Lum & Isaac, 2016; Kilbertus et al., 2017).",1. Introduction,[0],[0]
We consider a model of biased data where systematic censoring affects whether or not entire observations appear in the training dataset.,1. Introduction,[0],[0]
"In such cases, the available data are not representative of the eventual realworld “test” population to which any resulting learned policy will be applied.",1. Introduction,[0],[0]
"Our paper formalizes and characterizes how systematic under- or over-representation of groups in the dataset can hamper attempts to correct for fairness, leading to residual unfairness on the target population of interest.
",1. Introduction,[0],[0]
"This important issue arises in almost all settings where fair machine learning has been studied:
(1) Data on loan default can only be collected on those loan applicants who were historically approved but is used to learn approval policies applied to all applicants.",1. Introduction,[0],[0]
"(2) Arrest data help build predictive policing models but these data are disproportionately collected on individuals in highly patrolled areas and may be subject to further prejudice at the individual level, including racial (Lum & Isaac, 2016).",1. Introduction,[0],[0]
"(3) Risk assessment and intervention tools in child welfare agencies are trained on cases which have been “screened in” by caseworkers based on external reports (Chouldechova et al., 2018).",1. Introduction,[0],[0]
"(4) Defendants may only flee and fail to appear if not detained, so any flight risk score used for setting bail can only be learned from data on defendants who were not detained.",1. Introduction,[0],[0]
"(5) Convict recidivism is impacted by sentence applied but learned risk scores are applied to all convicts.
",1. Introduction,[0],[0]
"In these applications, systematic censoring screens out ob-
servations of individuals and their outcomes from a training dataset.",1. Introduction,[0],[0]
"Such censoring may reflect historical decisions made with limited access to information, heterogeneous decision-makers, or the application of statistically discriminatory rules (Arrow, 1973).",1. Introduction,[0],[0]
"Despite the intermediate screening, domain-level restrictions may require ensuring fairness of any decision or prediction policy with respect to the original population.",1. Introduction,[0],[0]
"We formalize this mechanism as a data setting (Fig. 1) where a historical decision policy Z 2 {0, 1} specifies whether an instance will be included in the dataset.",1. Introduction,[0],[0]
"Systematic censoring may induce covariate shift on population-level estimands, such as true positive rate, as outcomes are observed in the training data only where Z = 1.",1. Introduction,[0],[0]
"Notably, predictive tools built on these censored datasets are actively being deployed: there is an opportunity to improve upon standards of practice, but the structural implications of systematically censored data ought to be accounted for (Angwin et al., 2016; Capatosto, 2017; LJAF, 2015).",1. Introduction,[0],[0]
"Our contributions are as follows:
• We characterize when systematic censoring induces residual unfairness in terms of the distributions of the conditional Bayes-optimal risk score across censored and target groups.",1. Introduction,[0],[0]
"• When benchmark data is available, we show how to use sample re-weighting techniques to estimate accuracy metrics to adjust for fairness on the target population.",1. Introduction,[0],[0]
We show how the sample weights indicate what groups remain disadvantaged by residual unfairness.,1. Introduction,[0],[0]
•,1. Introduction,[0],[0]
"We demonstrate how systematic inclusion can affect fairness adjustments on an empirical example with data from the application of the Stop, Question, and Frisk (SQF) policy of the City of New York Police Department (NYPD).
",1. Introduction,[0],[0]
"In settings where datasets can be subject to historical prejudice and decision policies ought to be truly fair on the general population, we argue it is paramount to carefully consider and account for the sampling process to ensure fairness on the true population.",1. Introduction,[0],[0]
"We consider the problem of learning a fair decision policy (classifier or threshold rule on a regressor) from a dataset
where each decision instance is characterized by observed covariates X 2 X (e.g., predictors of creditworthiness, criminality, etc), protected class A 2 [m] = {1, . . .",2. Problem Setting,[0],[0]
",m} (e.g., sex, race, etc), and label Y 2 {0, 1} (where Y = 1 is generally interpreted as the favorable label, e.g., “will pay back loan” or “will not reoffend”).
",2. Problem Setting,[0],[0]
Fig. 1 illustrates the construction of a biased training dataset in this setting.,2. Problem Setting,[0],[0]
"The indicator Z 2 {0, 1} specifies whether an instance is included in the post-censoring training data (e.g., “approved for a loan”) and another indicator T 2 {0, 1} specifying whether an instance belongs to the population to which the learned policy will be applied.",2. Problem Setting,[0],[0]
"For example, if T is the constant 1, the target population is that of all loan applicants.",2. Problem Setting,[0],[0]
"We sometimes call Z = 1 the logging policy in analogy to logged-bandit learning (Kallus, 2017; Swaminathan & Joachims, 2015), where the implementation of a (often unknown) historical policy resulted in limited bandit feedback on outcomes.",2. Problem Setting,[0],[0]
"Because a random sample from the target population is generally not available, the target population is different from the notion of a held-out “test” dataset used to evaluate predictive accuracy .
",2. Problem Setting,[0],[0]
"We consider the problem of determining a policy assigning labels Ŷ 2 {0, 1} that depends only on X,A but may be randomized (so Ŷ ?",2. Problem Setting,[0],[0]
"(Y, Z, T )",2. Problem Setting,[0],[0]
"| X,A).",2. Problem Setting,[0],[0]
"Labeled training data (X,A, Y ) is available from the Z = 1 population, so that only the conditional joint distribution of X,A, Y | Z",2. Problem Setting,[0],[0]
= 1 is characterized by this data.,2. Problem Setting,[0],[0]
"We may or may not also have unlabeled data from the T = 1 population, X,A | T = 1.
",2. Problem Setting,[0],[0]
"For concreteness, when discussing fairness criteria, we consider the specific fairness criterion of equality of opportunity or equalized odds introduced by Hardt et al. (2016).",2. Problem Setting,[0],[0]
The adjustment determines a fair policy Ŷ from a (possibly discriminatory) black-box binary predictor or score R̂ without access to the original training data.,2. Problem Setting,[0],[0]
"They identify two particular types of fairness, equal opportunity and equalized odds, which require that a fairness-adjusted policy Ŷ be independent of class A given a positive label Y = 1 or given any label Y , respectively.",2. Problem Setting,[0],[0]
"For loan approval, equality of opportunity requires that the policy treat truly creditworthy individuals the same, independent of protected class membership.",2. Problem Setting,[0],[0]
"Equalized odds prohibits abusing class membership as an unfair proxy for Y (e.g., via stereotyping or racial profiling).",2. Problem Setting,[0],[0]
"For our setting, to be explicit, we define these relative to an event: Definition 1.",2. Problem Setting,[0],[0]
"A policy Ŷ satisfies equalized odds with respect to (wrt) class variable A and event E if
Ŷ ?",2. Problem Setting,[0],[0]
"A | Y = y,E (1)
holds for y 2 {0, 1}.",2. Problem Setting,[0],[0]
"A policy satisfies equal opportunity wrt A and E if eq. (1) holds for y = 1.
",2. Problem Setting,[0],[0]
"For brevity, we will say a policy is simply fair to mean either equal opportunity or equalized odds.",2. Problem Setting,[0],[0]
"Hardt et al.
(2016) determine such a policy by a post-processing step given a score R̂ using a constrained optimization problem over group-specific thresholds (potentially randomized), enforcing the constraints in eq. (1) on the true positive rate and/or false positive rate across groups while optimizing a given classification loss.",2. Problem Setting,[0],[0]
"Specifically, define F E
a (✓) =",2. Problem Setting,[0],[0]
"Pr[R̂  ✓ | Y = 1, A = a,E] as the conditional CDF (cumulative distribution function) of the score given the event E and Y = 1.",2. Problem Setting,[0],[0]
"For a given true positive rate ⇢, the corresponding derived equal opportunity classifier at rate ⇢ is given by Ŷ = I[R̂ >",2. Problem Setting,[0],[0]
"✓
A ], where ✓ a = (F a ) 1(1 ⇢) is the threshold corresponding to group a so that it has true positive rate ⇢.",2. Problem Setting,[0],[0]
"Note that FZ=1
a
, F T=1 a are the false negative rates for a threshold classifier as evaluated on the censored and target population, respectively.",2. Problem Setting,[0],[0]
"Naturally, a policy is actually fair if it is fair on the population to which it is applied (here, T = 1).",2. Problem Setting,[0],[0]
"So, in seeking a fair policy per these definitions, we seek a policy that satisfies equal opportunity or equalized odds on the target population wrt class variable A and the event T = 1, while the approach of Hardt et al. (2016) applied directly to training data achieves fairness wrt A and the event Z = 1.",2. Problem Setting,[0],[0]
"Throughout, we assume R̂ 2 [0, 1].",2. Problem Setting,[0],[0]
Fairness and Missing Data.,3. Related Work,[0],[0]
Research on fairness and machine learning has considered some subcomponents of the overall problem we study of learning fair policies from biased datasets.,3. Related Work,[0],[0]
Hardt et al. (2016) formalize the criteria of equal opportunity and equalized odds.,3. Related Work,[0],[0]
"Lum & Isaac (2016) show that a predictive policing algorithm for drug enforcement in Oakland, trained on police records, will perpetuate disparate enforcement.",3. Related Work,[0],[0]
"Ensign et al. (2017) consider a discrete model of how beliefs of crime rates in different areas adjust after observing arrest rates, and propose implementing the Horvitz-Thompson estimator via rejection sampling of arrest observations in an “online” fashion.",3. Related Work,[0],[0]
Lakkaraju et al. (2017) identify a similar structural setting with “selective labels” where they learn a decision rule for pre-trial risk assessment from the decisions made from judges (which affect whether or not the outcome of interest will be observed).,3. Related Work,[0],[0]
"They leverage the heterogeneity of heterogeneous decision makers using different decision thresholds to identify subpopulations for comparison but do not consider the subsequent fairness of the learned policy.
",3. Related Work,[0],[0]
Sampling Adjustment and Superpopulations.,3. Related Work,[0],[0]
"Sampling adjustment and re-weighting is commonly used in the social sciences, medicine, and epidemiology for ensuring the validity of population-level inference where there is population mismatch between studies and the population of interest (Thompson, 2012; Freedman & Berk, 2008).",3. Related Work,[0],[0]
"The classic Horvitz-Thompson estimator uses the inverse probability of sampling probability weights and is unbiased for
population level estimates (Horvitz & Thompson, 1952).",3. Related Work,[0],[0]
"Much of the work on fairness in machine learning has used population-level statistics such as accuracy metrics (true positive rate, false negative rate) as metrics for identifying disparate impact.",3. Related Work,[0],[0]
"The case of sample selection bias was studied in Zadrozny (2004) for classifier evaluation, without regard for fairness impacts.",3. Related Work,[0],[0]
"We study how prejudicial biases in a dataset can lead to residual unfairness, which persists even after fairness adjustment if error parity metrics assessed from the censored dataset are used.","4. Residual Unfairness Under Disparate Benefit of the Doubt: Bias In, Bias Out",[0],[0]
"We show that the residual unfairness that remains even after adjustment will disadvantage the same group that was prejudiced against before, in the training data.","4. Residual Unfairness Under Disparate Benefit of the Doubt: Bias In, Bias Out",[0],[0]
"This proves that even after fairness adjustment, fair machine learning still has a “bias in, bias out” property.
","4. Residual Unfairness Under Disparate Benefit of the Doubt: Bias In, Bias Out",[0],[0]
An Illustrative Synthetic Example:,"4. Residual Unfairness Under Disparate Benefit of the Doubt: Bias In, Bias Out",[0],[0]
"Loan Application To illustrate the potential impact of population mismatch on fairness adjustments in a controlled setting, we consider a synthetic example for loan approval.","4. Residual Unfairness Under Disparate Benefit of the Doubt: Bias In, Bias Out",[0],[0]
"Suppose there are two classes, with half of the population of loan applicants in class 0 and the other half in class 1.","4. Residual Unfairness Under Disparate Benefit of the Doubt: Bias In, Bias Out",[0],[0]
We let T = 1 be constant as we wish to consider the impact of our policy on the whole population of loan applicants.,"4. Residual Unfairness Under Disparate Benefit of the Doubt: Bias In, Bias Out",[0],[0]
"We denote by X1 the (normalized) number of bank accounts and by X2 the (normalized) number of delinquent payments on record, including those for subprime loans.1
Suppose X is distributed as a standard bivariate normal conditioned on class, with a mean of (1, 0) among individuals
1The setting is motivated by systematic associations found in studies of the credit scores suggesting disadvantages for younger applicants and recent immigrants due to policies incorporating number of accounts (Board of Governors of the Federal Reserve System, 1997; Rice & Swesnik, 2014).
in the class A = 0 and a mean of (0, 1) among individuals in the underprivileged class A = 1.","4. Residual Unfairness Under Disparate Benefit of the Doubt: Bias In, Bias Out",[0],[0]
"Consider Y 2 {0, 1} indicating whether the individual will pay back the loan if it were approved.","4. Residual Unfairness Under Disparate Benefit of the Doubt: Bias In, Bias Out",[0],[0]
"Suppose Y is logistic in X conditioned on class with P (Y = 1 | X,A) =","4. Residual Unfairness Under Disparate Benefit of the Doubt: Bias In, Bias Out",[0],[0]
"( T
A X), where (t) =","4. Residual Unfairness Under Disparate Benefit of the Doubt: Bias In, Bias Out",[0],[0]
"1/(1 + e t) and 0 = (1, 1), 1 = (1.25, 1) so that X2 is predictive of creditworthiness in both classes but X1 is slightly more predictive in class A = 1 than in A = 0.","4. Residual Unfairness Under Disparate Benefit of the Doubt: Bias In, Bias Out",[0],[0]
Suppose the training data comes from historically approved loans where loans were approved based on X in such a way that P (Z = 1 | X) =,"4. Residual Unfairness Under Disparate Benefit of the Doubt: Bias In, Bias Out",[0],[0]
"( T
a
X).
","4. Residual Unfairness Under Disparate Benefit of the Doubt: Bias In, Bias Out",[0],[0]
"In Figs. 2a–2b we consider deriving a fair classifier for loan approval from the class-blind Bayes optimal score R̂ = P (Y = 1 | X,T = 1) =","4. Residual Unfairness Under Disparate Benefit of the Doubt: Bias In, Bias Out",[0],[0]
"P (Y = 1 | X,Z = 1), which is the same in training and target populations by construction and which we assume is given (e.g., it can be identified from the training data regardless of any covariate shift between Z = 1 and T = 1; see Sec. 4.2).","4. Residual Unfairness Under Disparate Benefit of the Doubt: Bias In, Bias Out",[0],[0]
"We simulate n = 100000 data points and censor the outcome for those with Z
i = 0.","4. Residual Unfairness Under Disparate Benefit of the Doubt: Bias In, Bias Out",[0],[0]
First we consider deriving an adjusted predictor from the Bayes-optimal classifier Ŷ = I[R̂ 0.5] by naı̈vely applying the method of Hardt et al. (2016).,"4. Residual Unfairness Under Disparate Benefit of the Doubt: Bias In, Bias Out",[0],[0]
Fig.,"4. Residual Unfairness Under Disparate Benefit of the Doubt: Bias In, Bias Out",[0],[0]
"2a shows the space of achievable FPR-TPR in the training (censored) and target (full) populations along with the optimal equalized odds and equal opportunity rates corresponding to the symmetric loss `(y, y0) =","4. Residual Unfairness Under Disparate Benefit of the Doubt: Bias In, Bias Out",[0],[0]
I[y 6= y0].,"4. Residual Unfairness Under Disparate Benefit of the Doubt: Bias In, Bias Out",[0],[0]
"As can be seen, there is a significant discrepancy between the regions in the censored vs. full population.","4. Residual Unfairness Under Disparate Benefit of the Doubt: Bias In, Bias Out",[0],[0]
"Next, we consider deriving optimal equal opportunity policies from the score","4. Residual Unfairness Under Disparate Benefit of the Doubt: Bias In, Bias Out",[0],[0]
R̂. Fig.,"4. Residual Unfairness Under Disparate Benefit of the Doubt: Bias In, Bias Out",[0],[0]
"2b shows the range of optimal policies, which is given by class-based thresholds, as we range the exchange rate between typeI and -II errors (false positives vs. false negatives).","4. Residual Unfairness Under Disparate Benefit of the Doubt: Bias In, Bias Out",[0],[0]
We also show the result from using a reweighting approach that we discuss in Sec. 5.,"4. Residual Unfairness Under Disparate Benefit of the Doubt: Bias In, Bias Out",[0],[0]
"We note that a naı̈ve application of fairness adjustment provides insufficient compensation for the unfairness toward the underprivileged class A = 1: for every threshold on class A = 0, the corresponding threshold on class A = 1 is always higher for the policy derived in the naı̈ve manner compared to that derived either using the full data or using our reweighting approach, such that the spuriously fair policy is systematically and uniformly harsher than necessary on the disadvantaged class.","4. Residual Unfairness Under Disparate Benefit of the Doubt: Bias In, Bias Out",[0],[0]
We now formalize the phenomenon illustrated in the example as residual unfairness and study why and when it arises in terms of the biases in training data due to existing prejudiced policies.,4.1. Disparate Benefit of the Doubt,[0],[0]
"For concreteness, we focus on the equality of opportunity criterion.",4.1. Disparate Benefit of the Doubt,[0],[0]
Many of our results can be easily extended to other observational fairness criteria.,4.1. Disparate Benefit of the Doubt,[0],[0]
"To quantify the extent to which the criterion is satisfied or violated, and in which direction, we define the inequity of opportunity.",4.1. Disparate Benefit of the Doubt,[0],[0]
Definition 2 (Inequity of Opportunity).,4.1. Disparate Benefit of the Doubt,[0],[0]
"The inequity of
opportunity between classes A = a and A = b wrt to event E under policy Ŷ is defined as
✏
E
a,b = P(Ŷ = 1 | E,A=a, Y=1 )",4.1. Disparate Benefit of the Doubt,[0],[0]
"P(Ŷ = 1 | E,A=b, Y=1 )
",4.1. Disparate Benefit of the Doubt,[0],[0]
"Positive values in the target population, ✏T=1 a,b > 0, indicate unfairness against group b. Zero inequity between all groups corresponds to equality of opportunity.",4.1. Disparate Benefit of the Doubt,[0],[0]
"A policy that is adjusted to be equal-opportunity or equalized-odds fair on the training data has ✏Z=1
a,b = 0.",4.1. Disparate Benefit of the Doubt,[0],[0]
"Thus, any nonzero value of ✏T=1
a,b for such a policy constitutes a residual unfairness corresponding to the additional unadjusted-for inequity introduced by going from the Z = 1 to the T = 1 population.
",4.1. Disparate Benefit of the Doubt,[0],[0]
"Intuitively, if censoring induces a spuriously higher or lower overall distribution of scores than in the true population, we might learn a higher or lower threshold from the training data.",4.1. Disparate Benefit of the Doubt,[0],[0]
"If the true distribution will have more people with comparatively lower scores, the rate of false negatives will increase in the true population.",4.1. Disparate Benefit of the Doubt,[0],[0]
"This is to be expected if the censoring decision Z = 1 has itself an associated risk or cost, such as giving a loan.",4.1. Disparate Benefit of the Doubt,[0],[0]
"Differences in the extent and effects of this censoring between groups, which is what we will define as disparate benefit of the doubt, can then give rise to non-zero residual inequity.",4.1. Disparate Benefit of the Doubt,[0],[0]
"This may occur if the censoring mechanism subjects the disadvantaged group to harsher screening than the advantaged group, so that disadvantaged screened-in individuals have higher probabilities of being positive (e.g., innocent or creditworthy) given the observables X,A.
We next derive three sufficient conditions for residual unfairness.",4.1. Disparate Benefit of the Doubt,[0],[0]
We explain how these can be interpreted in terms of the logging policy Z = 1 bestowing disparate benefit of the doubt with respect to the positive outcome Y,4.1. Disparate Benefit of the Doubt,[0],[0]
= 1 on the different groups.,4.1. Disparate Benefit of the Doubt,[0],[0]
"This disparate benefit of the doubt would be directly reflected in the distribution of risk scores in the training and target populations, which we will show necessarily leads to residual unfairness that disadvantages the same group that received comparatively lesser benefit of the doubt (or comparatively heightened suspicion), thus perpetuating historical prejudices.
",4.1. Disparate Benefit of the Doubt,[0],[0]
We first state a simple rephrasing of the residual inequity of opportunity left by a fairness adjustment.,4.1. Disparate Benefit of the Doubt,[0],[0]
"Recall that F
Z=1 a (✓), FT=1 a (✓) correspond to the false negative rate (FNR) when thresholding at ✓ on the training (censored) and test populations, respectively.",4.1. Disparate Benefit of the Doubt,[0],[0]
"Let
a (✓) = FZ=1 a
(✓) F
T=1",4.1. Disparate Benefit of the Doubt,[0],[0]
a (✓) denote the difference between true positive rates in the test population and training (censored) population.,4.1. Disparate Benefit of the Doubt,[0],[0]
Recall that Ŷ is an optimal derived equal opportunity classifier based on the training data if Ŷ = I[R̂ >,4.1. Disparate Benefit of the Doubt,[0],[0]
"✓
A ] and F
Z=1 a (✓ a ) = FZ=1 b",4.1. Disparate Benefit of the Doubt,[0],[0]
"(✓ b ) for every two groups a, b.2",4.1. Disparate Benefit of the Doubt,[0],[0]
"For short we refer to such a classifier as a derived equal oppor-
2Specifically, that the set of derived equal opportunity classi-
tunity classifier.
",4.1. Disparate Benefit of the Doubt,[0],[0]
Proposition 1.,4.1. Disparate Benefit of the Doubt,[0],[0]
Let Ŷ = I[R̂,4.1. Disparate Benefit of the Doubt,[0],[0]
>,4.1. Disparate Benefit of the Doubt,[0],[0]
✓ A ] be a derived equal opportunity classifier.,4.1. Disparate Benefit of the Doubt,[0],[0]
"Then ✏T=1
a,b
= a (✓ a ) b (✓ b ).
",4.1. Disparate Benefit of the Doubt,[0],[0]
"Next, we define first-order stochastic dominance, which we use to express our first characterization of disparate benefit of the doubt.
",4.1. Disparate Benefit of the Doubt,[0],[0]
Definition 3 (First-order stochastic dominance).,4.1. Disparate Benefit of the Doubt,[0],[0]
"Let F , G be two CDFs.",4.1. Disparate Benefit of the Doubt,[0],[0]
"We write F G whenever F (✓) G(✓) 8✓.
CDFs describe the distribution of a population of real values.",4.1. Disparate Benefit of the Doubt,[0],[0]
"The stochastic dominance F G means that the population described by F has overall smaller values than the population described by G. Specifically, F G is equivalent to saying that for each unit in the population described by F we can find a nonnegative number such that, when added to each unit, the whole population looks like that described by G (Mas-Colell et al., 1997).",4.1. Disparate Benefit of the Doubt,[0],[0]
"That is, each unit from F can be uniquely paired with a unit from G such that the former has a smaller or equal value than the latter (allowing fractional or infinitesimal “units”).",4.1. Disparate Benefit of the Doubt,[0],[0]
"Alternatively, F G holds if and only if the average of every increasing function in the F population is smaller than or equal to the corresponding average in the G population (Fishburn, 1980).",4.1. Disparate Benefit of the Doubt,[0],[0]
"This says that any rational actor with an increasing utility function would gain utility in choosing G over F and lose utility in choosing F over G (or get the same utility).
",4.1. Disparate Benefit of the Doubt,[0],[0]
Proposition 2 (Strong disparate benefit of the doubt).,4.1. Disparate Benefit of the Doubt,[0],[0]
"Suppose that
F Z=1",4.1. Disparate Benefit of the Doubt,[0],[0]
a FT=1 a and FZ=1 b ⌫ FT=1,4.1. Disparate Benefit of the Doubt,[0],[0]
"b
(2)
while not both are equalities, i.e., either FZ=1",4.1. Disparate Benefit of the Doubt,[0],[0]
a 6= FT=1,4.1. Disparate Benefit of the Doubt,[0],[0]
a or FZ=1 b 6= FT=1,4.1. Disparate Benefit of the Doubt,[0],[0]
"b
(or both).",4.1. Disparate Benefit of the Doubt,[0],[0]
"Then every derived equal opportunity classifier has nonnegative inequity of opportunity for group b relative to group a (✏T=1
a,b 0) and at least one derived equal opportunity classifier will have a strictly positive inequity of opportunity disadvantaging group b relative to group a (✏T=1
a,b
> 0).
",4.1. Disparate Benefit of the Doubt,[0],[0]
"The condition in eq. (2) requires that the distribution of scores among positive group-a members is overall smaller in the training data than in the target population, while the opposite is true for group b. Recall that scores represent the probability of having the positive, favorable label (more on this in Sec. 4.2).",4.1. Disparate Benefit of the Doubt,[0],[0]
"Thus, the condition says that positive group-a members received more benefit of the doubt when
fiers that are optimal with respect to some trade off between type-I and -II errors is exactly equal to the set of all such thresholding classifiers requires only that we assume that, in each group A = a, R̂ is not worse than random guessing and that the ROC is convex.",4.1. Disparate Benefit of the Doubt,[0],[0]
"Neither is without loss of generality as seen by only improving R̂ by conditionally (on A) negating R̂ and/or randomizing its value in nonconvex intervals between the endpoints.
being screened-in into the training data than positive groupb members.",4.1. Disparate Benefit of the Doubt,[0],[0]
"Prop. 2 shows that this will necessarily lead to group-b being further disadvantaged in the future even after correcting for equality of opportunity.
",4.1. Disparate Benefit of the Doubt,[0],[0]
"In the context of loan application, where we can think about the score as a credit score, eq. (2) means that the logging policy (i.e., historical loan approval practice) effectively dug deeper into the pile of creditworthy group-a applicants than for group-b applicants, giving the former more benefit of the doubt as to their creditworthiness based on their credit scores than it gave the latter.",4.1. Disparate Benefit of the Doubt,[0],[0]
"Seen via the equivalent utility-based interpretation of stochastic dominance, given any increasing utility function, if eq. (2) holds then the logging policy is losing utility on group-a via lax screening while gaining utility on group-b by being less lax.
",4.1. Disparate Benefit of the Doubt,[0],[0]
"The complement (or, negative) of the score can be thought of as a risk score: the probability of the unfavorable label.",4.1. Disparate Benefit of the Doubt,[0],[0]
Eq. (2) can equivalently be written as the opposite ordering on risk scores rather than positivity scores.,4.1. Disparate Benefit of the Doubt,[0],[0]
"Thus, in either a judicial bail- or sentence-setting context or in a predictive policing context, eq. (2) means that the logging policy (i.e., historical criminal justice or policing practice) was harsher on group b than on group a, screening-in lower risk scores for innocent group-b members compared to the group-b population while screening-in only higher risk scores for group-a members, giving them more benefit of the doubt as to their innocence based on observables.
",4.1. Disparate Benefit of the Doubt,[0],[0]
"If the CDFs are nowhere equal except for at 0 and 1 (where they are always equal) then a strict version of Prop. 2 shows that every derived equal opportunity classifier will be unfair.
",4.1. Disparate Benefit of the Doubt,[0],[0]
"Proposition 3 (Strong disparate benefit of the doubt, strict).",4.1. Disparate Benefit of the Doubt,[0],[0]
"Suppose eq. (2) holds and that FZ=1
a (✓) 6=",4.1. Disparate Benefit of the Doubt,[0],[0]
"F
T=1 a (✓), FZ=1 b (✓) 6=",4.1. Disparate Benefit of the Doubt,[0],[0]
"FT=1 b (✓) for all ✓ 2 (0, 1).",4.1. Disparate Benefit of the Doubt,[0],[0]
"Then every nontrivial derived equal opportunity classifier will have strictly positive inequity of opportunity disadvantaging group b relative to group a.
A nontrivial classifier is any classifier that is neither the constant Ŷ = 0 nor the constant Ŷ = 1.
",4.1. Disparate Benefit of the Doubt,[0],[0]
The conditions in Props.,4.1. Disparate Benefit of the Doubt,[0],[0]
2 and 3 are easy to interpret via stochastic dominance but may be too strong to hold in practice.,4.1. Disparate Benefit of the Doubt,[0],[0]
"In particular, suppose the decision Z = 1 itself has a benefit or risk related to whether Y = 1, as in the case of giving a loan (benefit of earning the full interest over loan term compared to risk of a default) or a police stop (benefit of curtailing crime compared to costs, including societal, of aggressive policing).",4.1. Disparate Benefit of the Doubt,[0],[0]
"Then, if the decision Z = 1 is exercised rationally, then we would expect that the distribution of scores is either overall higher (e.g., for loans) or overall lower (e.g., for police stops) in the training population regardless of group, i.e., both FZ=1
a
⌫ FT=1 a and FZ=1 b ⌫ F
T=1 b or both FZ=1 a
FT=1 a and FZ=1 b
FT=1 b .",4.1. Disparate Benefit of the Doubt,[0],[0]
"(Al-
though, prejudice in Z = 1 can be so overt and/or irrational for this not to hold.)",4.1. Disparate Benefit of the Doubt,[0],[0]
"If this is the case, then the conditions of Props.",4.1. Disparate Benefit of the Doubt,[0],[0]
"2 and 3 cannot hold and we must relax them.
",4.1. Disparate Benefit of the Doubt,[0],[0]
"The next result shows that even if the stochastic dominance holds in the same direction for both groups, if the magnitude of the dominance is overall larger in one group compared to the other for a large swath of thresholds then most derived equal opportunity classifiers will actually be unfair and disadvantage the historically disadvantaged.",4.1. Disparate Benefit of the Doubt,[0],[0]
"Proposition 4 (Weak disparate benefit of the doubt, strict).",4.1. Disparate Benefit of the Doubt,[0],[0]
"Let ✓, ✓ be such that
a (✓) >",4.1. Disparate Benefit of the Doubt,[0],[0]
"b
(✓0) 8✓, ✓0 2 (✓, ✓).",4.1. Disparate Benefit of the Doubt,[0],[0]
"(3)
Let Ŷ = I[R̂ >",4.1. Disparate Benefit of the Doubt,[0],[0]
✓ A ] be a derived equal opportunity classifier.,4.1. Disparate Benefit of the Doubt,[0],[0]
"If ✓
a
, ✓
b 2 (✓, ✓), then Ŷ induces a strictly positive inequality of opportunity disadvantaging group b relative to group a.
Note that, since A (0) =",4.1. Disparate Benefit of the Doubt,[0],[0]
"A (1) = 0, we have that eq.",4.1. Disparate Benefit of the Doubt,[0],[0]
"(3) holds for (✓, ✓) = (0, 1) if and only if the conditions of Prop. 3 hold.",4.1. Disparate Benefit of the Doubt,[0],[0]
"Therefore, for general (✓, ✓) the former can be understood as a relaxation of the latter.
",4.1. Disparate Benefit of the Doubt,[0],[0]
We can illustrate the conditions of Prop. 4 in the synthetic loan application example from the beginning of this section.,4.1. Disparate Benefit of the Doubt,[0],[0]
In Fig.,4.1. Disparate Benefit of the Doubt,[0],[0]
"3b, we plot the two
A functions and shade a large interval where eq.",4.1. Disparate Benefit of the Doubt,[0],[0]
(3) holds.,4.1. Disparate Benefit of the Doubt,[0],[0]
"In Fig. 3a, we plot the CDFs F E
A and further shade the corresponding regions of false negative rates for which both ✓0 and ✓1 lie the previously shaded interval.",4.1. Disparate Benefit of the Doubt,[0],[0]
"Taking complements, this shows that any derived equal opportunity classifier adjusted to have equal true positive rates of 0.58–0.95 on the training data will disadvantage the underprivileged class despite one’s attempt to adjust against this situation.
",4.1. Disparate Benefit of the Doubt,[0],[0]
We can slightly relax the condition in eq.,4.1. Disparate Benefit of the Doubt,[0],[0]
"(3) whenever dealing with groups that have disparate endowments of scores, as in the example above where the scores of group 0 are overall larger than those of group 1 in terms of stochastic dominance.
",4.1. Disparate Benefit of the Doubt,[0],[0]
Proposition 5 (Weak disparate benefit of the doubt on disparately endowed groups).,4.1. Disparate Benefit of the Doubt,[0],[0]
"Let ✓, ✓ be such that
a (✓) >",4.1. Disparate Benefit of the Doubt,[0],[0]
"b
(✓0) 8✓, ✓0 2 (✓, ✓) : ✓ ✓0.",4.1. Disparate Benefit of the Doubt,[0],[0]
"(4)
Suppose FZ=1 a ⌫ FZ=1 b .",4.1. Disparate Benefit of the Doubt,[0],[0]
Let Ŷ = I[R̂,4.1. Disparate Benefit of the Doubt,[0],[0]
>,4.1. Disparate Benefit of the Doubt,[0],[0]
✓ A ] be a derived equal opportunity classifier.,4.1. Disparate Benefit of the Doubt,[0],[0]
"If ✓
a
, ✓
b 2 (✓, ✓), then Ŷ induces a strictly positive inequality of opportunity disadvantaging group b relative to group a.
In the supplemental Sec.",4.1. Disparate Benefit of the Doubt,[0],[0]
B we also include an illustration of weak disparate benefit of the doubt in a real dataset of credit card applications and payment defaults.,4.1. Disparate Benefit of the Doubt,[0],[0]
"In addition to making more concrete our crediting example, this also serves to illustrate the weaker condition in eq. (4).
",4.1. Disparate Benefit of the Doubt,[0],[0]
"All of our results in this section can be equivalently stated for true negative rates instead of true positive rates, in which case the corresponding conditions such as that in eq.",4.1. Disparate Benefit of the Doubt,[0],[0]
"(2) can instead be interpreted as disparate suspicion, i.e., the disparate scrutiny of truly criminal or credit-unworthy individuals.",4.1. Disparate Benefit of the Doubt,[0],[0]
"So, whereas our notion of disparate benefit of the doubt corresponds to the phenomenon of “driving while black” (Lamberth, 1998), our notion of disparate suspicion would correspond to the phenomenon of “criming while white” (Goldfarb, 2014).",4.1. Disparate Benefit of the Doubt,[0],[0]
"If either disparate benefit of the doubt or disparate suspicion is present, a derived equalized odds classifier will in fact violate equalized odds in a way that disadvantages the same group that was disadvantaged by the disparate benefit of the doubt or suspicion.",4.1. Disparate Benefit of the Doubt,[0],[0]
In the above we interpreted the conditions in our results as disparities in the distributions of positivity or risk scores among different groups in the training and target populations.,4.2. Interpretation of Scores Under MAR,[0],[0]
"Specifically, we interpreted these scores as corresponding to the probabilities of having a positive or negative label given observables.",4.2. Interpretation of Scores Under MAR,[0],[0]
"In full generality, however, this probability might actually be different in the training and target populations, i.e., P (Y = 1 | X,A,Z = 1) 6=",4.2. Interpretation of Scores Under MAR,[0],[0]
"P (Y = 1 | X,A, T = 1).",4.2. Interpretation of Scores Under MAR,[0],[0]
"Since naturally only the training data is available at training we can consider the trainingpopulation Bayes score R̂ = P (Y = 1 | X,A,Z = 1) and interpret disparities as disparities in benefit of the doubt of positivity given this score.",4.2. Interpretation of Scores Under MAR,[0],[0]
"This is consistent with our interpretation above.
",4.2. Interpretation of Scores Under MAR,[0],[0]
"However, whenever censoring Z = 1 is itself based on observables, the data will be missing conditionally at random (MAR) and these probabilities will actually be the same so that we can interpret the scores as simply the probability of positivity given observables generally.",4.2. Interpretation of Scores Under MAR,[0],[0]
Assumption 1.,4.2. Interpretation of Scores Under MAR,[0],[0]
(MAR) Z ?,4.2. Interpretation of Scores Under MAR,[0],[0]
"Y | X,A and T ?",4.2. Interpretation of Scores Under MAR,[0],[0]
"Y | X,A.
Under MAR, it is immediate that the Bayes score satisfies R̂ = P (Y = 1 | X,A) = P (Y = 1 | X,A,Z = 1) =
P (Y = 1 | X,A, T = 1), which can be consistently estimated from training data.",4.2. Interpretation of Scores Under MAR,[0],[0]
"In fact, under MAR, the optimal (unrestricted) decision function in X,A minimizing the average over Z = 1 of any loss in Y is the same as that minimizing the average loss over T = 1.
MAR requires that the missingness is unrelated to outcome after controlling for the observables.",4.2. Interpretation of Scores Under MAR,[0],[0]
"This assumption is clearly satisfied in the common case when T = 1 is constant and only X,A (or just X) were taken into consideration for a randomized inclusion policy Z. In the examples laid out in Sec. 1, this is an appropriate assumption because the censoring mechanism does not observe outcomes Y a priori, only observable characteristics (including the protected attribute).",4.2. Interpretation of Scores Under MAR,[0],[0]
"However, violations of MAR may occur, for example in the loan case if applicants may choose an outside option, selfcensoring the observation of a default while the availability of these options is related to creditworthiness.",4.2. Interpretation of Scores Under MAR,[0],[0]
"In rare situations, we may have additional information about the target population such as an unlabeled dataset.",5. Fairness Assessment and Adjustment with Biased Data via Sample Reweighting,[0],[0]
"We next show how we can use such data to evaluate accuracy metrics on the target population, as long as data is MAR.",5. Fairness Assessment and Adjustment with Biased Data via Sample Reweighting,[0],[0]
"This will allow us to assess the residual unfairness of fairness adjusted classifiers, as we will do in our study of SQF, and to correctly adjust for fairness, as we have done in Fig. 2b.
",5. Fairness Assessment and Adjustment with Biased Data via Sample Reweighting,[0],[0]
"Let p(x, a) = P(T=1|X=x,A=a)P(Z=1|X=x,A=a) be the propensity score ratio between the target and training populations.",5. Fairness Assessment and Adjustment with Biased Data via Sample Reweighting,[0],[0]
"This ratio is a standard way to adjust for systematic covariate shift for evaluating averages in the target (Horvitz & Thompson, 1952; Bottou et al., 2012).",5. Fairness Assessment and Adjustment with Biased Data via Sample Reweighting,[0],[0]
We next state how a weighting score that’s equal to it up to proportionality in a can be used for evaluating true positive rates.,5. Fairness Assessment and Adjustment with Biased Data via Sample Reweighting,[0],[0]
Proposition 6.,5. Fairness Assessment and Adjustment with Biased Data via Sample Reweighting,[0],[0]
"Suppose p̃(x, a) = r(a)p(x, a) for some r(a) and Ŷ ?",5. Fairness Assessment and Adjustment with Biased Data via Sample Reweighting,[0],[0]
"(Y, Z, T )",5. Fairness Assessment and Adjustment with Biased Data via Sample Reweighting,[0],[0]
"| X,A.",5. Fairness Assessment and Adjustment with Biased Data via Sample Reweighting,[0],[0]
"Then, under Assumption 1, P(Ŷ = 1 | Y = y,A = a, T = 1) is equal to
E[I[Ŷ=1,Y=y,A=a]p̃(X,A)|Z=1]P ŷ2{0,1} E[I[Ŷ=ŷ,Y=y,Z=1,A=a]p̃(X,A)|Z=1]
(5)
",5. Fairness Assessment and Adjustment with Biased Data via Sample Reweighting,[0],[0]
"Given p̃(x, a), the quantity in eq. (5) involves only the distribution of the training data.",5. Fairness Assessment and Adjustment with Biased Data via Sample Reweighting,[0],[0]
"In practice, the expectations in it can be estimated using empirical averages over the training data.",5. Fairness Assessment and Adjustment with Biased Data via Sample Reweighting,[0],[0]
"Therefore, if we can compute an appropriate weighting function p̃(x, a), Prop. 6 provides a remedy to the problem of biased data: we may simply replace the condition in Def. 1, which involves an unknown distribution X,A, Y | T = 1, with the condition that eq. (5) is constant over a (for y 2 {0, 1} or for y = 1).",5. Fairness Assessment and Adjustment with Biased Data via Sample Reweighting,[0],[0]
"In particular, to apply the equality of opportunity adjustment on the target population, we need only compute the TPRs and/or FPRs of
any blackbox predictor using the adjustment of eq.",5. Fairness Assessment and Adjustment with Biased Data via Sample Reweighting,[0],[0]
"(5) and proceed with the adjustment as usual.
",5. Fairness Assessment and Adjustment with Biased Data via Sample Reweighting,[0],[0]
"Next we address when can we find an appropriate reweighting score p̃(x, a).",5. Fairness Assessment and Adjustment with Biased Data via Sample Reweighting,[0],[0]
We consider two cases.,5. Fairness Assessment and Adjustment with Biased Data via Sample Reweighting,[0],[0]
"If Z = 1 is a subpopulation of T = 1 and our data consists of iid draws from the target population population but where, naturally, only the Z = 1 units are labeled, then may simply let p̃(x, a) be the reciprocal of the conditional probability of being labeled, which may be estimated using a probabilistic classification algorithm such as logistic regression.",5. Fairness Assessment and Adjustment with Biased Data via Sample Reweighting,[0],[0]
"This case applies, for example, in the loan approval policy example if the data includes the full loan applications, whether they were approved, and whether the approved loans defaulted or were paid back.",5. Fairness Assessment and Adjustment with Biased Data via Sample Reweighting,[0],[0]
"If, however, our data consists only of the labeled examples, as in the case of arrest and SQF data, which contain only the arrests or stops made and, naturally, never any information on those not made, then this case does not apply.",5. Fairness Assessment and Adjustment with Biased Data via Sample Reweighting,[0],[0]
"But, if we have an unlabeled dataset from the target population, then we can separately estimate the distribution of X,A in the training and target distributions.",5. Fairness Assessment and Adjustment with Biased Data via Sample Reweighting,[0],[0]
"Then we may let p̃(x, a) be either the ratio of densities of X,A in T = 1 and Z = 1 or the ratio of densities of X in T = 1, A = a and Z = 1, A = a.
In supplemental Sec.",5. Fairness Assessment and Adjustment with Biased Data via Sample Reweighting,[0],[0]
"C we provide additional results characterizing residual unfairness under MAR in terms of p̃(x, a).",5. Fairness Assessment and Adjustment with Biased Data via Sample Reweighting,[0],[0]
"We next study the Stop, Question, and Frisk dataset to illustrate how residual unfairness may occur with real data.","6. Case Study: Stop, Question, and Frisk",[0],[0]
We consider learning a predictor of criminal possession of a weapon from this dataset using logistic regression and then adjusting the policy to be fair on the training population.,"6. Case Study: Stop, Question, and Frisk",[0],[0]
"The trained policy will be applied to the target population of New York City at large, where it will be shown to in fact be unfair.","6. Case Study: Stop, Question, and Frisk",[0],[0]
"This residual unfairness can be explained by disparities between the training population (SQF stops) and the target population (NYC), which is evident from the divergent geographic distributions of these two as seen in Fig. 4.","6. Case Study: Stop, Question, and Frisk",[0],[0]
"(Further details about SQF and the dataset are provided in the supplemental Sec. D.)
Table 2: Residual unfairness in SQF predictive targeting
Goel et al. (2016) considered learning predictors from this data as a way to assess the implied decision thresholds used for determining pedestrian stops related to the criteria of “reasonable suspicion.”","6. Case Study: Stop, Question, and Frisk",[0],[0]
"While the authors suggest that a logistic regression predictor of criminal possession of a weapon could be used as a secondary filter applied on those individuals targeted by officers, we instead consider training such a predictive model to guide whom to target for a stop and search and adjusting this targeting policy for fairness.
","6. Case Study: Stop, Question, and Frisk",[0],[0]
"We consider race to be the protected class A, with values Black, Black Hispanic, White, White Hispanic, and Other.3 Arrest data or SQF data only contain the arrests or stops made and, naturally, never any information on those not made.","6. Case Study: Stop, Question, and Frisk",[0],[0]
"Since biases in the demographics of stop data arise from disproportionate policing by location and potential racial biases, we define the target population with respect to demographic data about NYC precincts.","6. Case Study: Stop, Question, and Frisk",[0],[0]
"Letting X1 denote the precinct-encoding portion of the covariates and X2 denote all other covariates (which include 30 indicators of reasons for suspicion, sex, and location or stop), we set P(X1, A | T = 1) to be the same distribution as that of the population of NYC.","6. Case Study: Stop, Question, and Frisk",[0],[0]
"We further assume that, once we condition on the main sources of bias, (X1, A), the covariates X2 are not disproportionate between the training and target distribution: we set P(X2 | A,X1, T = 1) = P(X2 | A,X1, Z = 1).","6. Case Study: Stop, Question, and Frisk",[0],[0]
"That is, the main sources of systematic bias with respect to censored data are contained in X1 and A, while ancillary covariates X2 are not proxies for discrimination.","6. Case Study: Stop, Question, and Frisk",[0],[0]
"We then have that p̃(X,A) = P(A,X1|T=1)P(A,X1|Z=1) satisfies the conditions of Prop. 6","6. Case Study: Stop, Question, and Frisk",[0],[0]
"so we need only estimate P(A,X1 | Z = 1), P(A,X1 | T = 1).","6. Case Study: Stop, Question, and Frisk",[0],[0]
We estimate the former from the SQF data and the latter from the 2010 American Community Survey data by matching census blocks to precincts and using Laplace smoothing.,"6. Case Study: Stop, Question, and Frisk",[0],[0]
"We clip the ratio weights at 10.
","6. Case Study: Stop, Question, and Frisk",[0],[0]
"Using the SQF data as is, we fit a logistic regression R̂ to predict the probability of innocence, that a search would not recover a weapon from those suspected of criminal possess-
3Due to the relative size of the Asian/Pacific Islander and Native American classes included in the original SQF dataset, we combine them with the Other (U) class.
","6. Case Study: Stop, Question, and Frisk",[0],[0]
ing one based on the covariates X .,"6. Case Study: Stop, Question, and Frisk",[0],[0]
Fig. 5 shows the FNR discrepancies between training and target and the ROCs in each.,"6. Case Study: Stop, Question, and Frisk",[0],[0]
"We consider deriving both an equal opportunity classifier and an equalized odds classifier for a stop and search based on the SQF data to minimize false negatives and false positives at an exchange rate of 25:1.
","6. Case Study: Stop, Question, and Frisk",[0],[0]
"In Table 2, we report the estimated true positive and false positive rates achieved by these classifiers both in the training and in the target population.","6. Case Study: Stop, Question, and Frisk",[0],[0]
The latter is computed using Prop. 6.,"6. Case Study: Stop, Question, and Frisk",[0],[0]
FNRs quantify the percent of innocents wrongly targeted and FPRs quantify the percent of criminals undetected.,"6. Case Study: Stop, Question, and Frisk",[0],[0]
"By construction, the adjusted-for rates are equal in the training population (Z = 1).","6. Case Study: Stop, Question, and Frisk",[0],[0]
"However, in practice, residual unfairness remains in the target population even after adjustment, and both of these supposedly fair classifiers will systematically disadvantage the same groups that were previously disparately targeted.","6. Case Study: Stop, Question, and Frisk",[0],[0]
"For the equal-opportunity-adjusted classifier, whereas only 11% of white-non-Hispanic innocents are wrongly targeted, up to 20% of white-Hispanic, 16% of other, and 14–15% of black innocents are wrongly targeted and harassed.","6. Case Study: Stop, Question, and Frisk",[0],[0]
Similar disparities exist for the equalized-odds-adjusted classifier.,"6. Case Study: Stop, Question, and Frisk",[0],[0]
"The equalized odds policy, having been subject to additional constraints that require more randomization and hence less dependence on observables, induces smaller but still significant disparities and Hispanics remain particularly disproportionately burdened.
","6. Case Study: Stop, Question, and Frisk",[0],[0]
"In all cases, after fairness adjustment, non-white individuals were still unfairly disadvantaged in practice relative to white individuals, thus perpetuating the same biases that SQF is notorious for under the guise of a policy adjusted to be fair.","6. Case Study: Stop, Question, and Frisk",[0],[0]
"Our work characterizes the problem of residual unfairness, which arises when policies learned from biased datasets are adjusted for fairness but remain unfair in practice.",7. Conclusion,[0],[0]
"We study a general setting where the dataset is generated under a prejudiced historical policy, which captures the structure of many problem settings where fairness has been considered.",7. Conclusion,[0],[0]
We prove that the same prejudices will be reflected in the supposedly-fairness-adjusted policy.,7. Conclusion,[0],[0]
Recent work in fairness in machine learning has proposed adjusting for fairness by equalizing accuracy metrics across groups and has also studied how datasets affected by historical prejudices may lead to unfair decision policies.,abstractText,[0],[0]
We connect these lines of work and study the residual unfairness that arises when a fairness-adjusted predictor is not actually fair on the target population due to systematic censoring of training data by existing biased policies.,abstractText,[0],[0]
This scenario is particularly common in the same applications where fairness is a concern.,abstractText,[0],[0]
We characterize theoretically the impact of such censoring on standard fairness metrics for binary classifiers and provide criteria for when residual unfairness may or may not appear.,abstractText,[0],[0]
"We prove that, under certain conditions, fairnessadjusted classifiers will in fact induce residual unfairness that perpetuates the same injustices, against the same groups, that biased the data to begin with, thus showing that even state-of-theart fair machine learning can have a “bias in, bias out” property.",abstractText,[0],[0]
"When certain benchmark data is available, we show how sample reweighting can estimate and adjust fairness metrics while accounting for censoring.",abstractText,[0],[0]
"We use this to study the case of Stop, Question, and Frisk (SQF) and demonstrate that attempting to adjust for fairness perpetuates the same injustices that the policy is infamous for.",abstractText,[0],[0]
Residual Unfairness in Fair Machine Learning from Prejudiced Data,title,[0],[0]
"Objective: This paper develops a novel tree-based algorithm, called Bonsai, which can be trained on a laptop, or the cloud, and can then be shipped onto severely resource constrained Internet of Things (IoT) devices.
",1. Introduction,[0],[0]
"Resource constrained devices: The Arduino Uno board has an 8 bit ATmega328P microcontroller operating at 16 MHz with 2 KB SRAM and 32 KB read-only flash mem-
1Microsoft Research, Bangalore, India 2CSE Department, IIT Delhi, India.",1. Introduction,[0],[0]
"Correspondence to: <manik@microsoft.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
ory.,1. Introduction,[0],[0]
The BBC Micro:Bit has a 32 bit ARM Cortex M0 microcontroller operating at 16 MHz with 16 KB SRAM and 256 KB read-only flash.,1. Introduction,[0],[0]
Neither provides hardware support for floating point operations.,1. Introduction,[0],[0]
"Billions of such tiny IoT microcontrollers have been deployed in the world (Meunier et al., 2014).",1. Introduction,[0],[0]
"Before deployment, the OS and all application code and data are burnt onto flash, leaving only a few KB for storing the trained ML model, prediction code, feature extraction code and associated data and parameters.",1. Introduction,[0],[0]
"After deployment, the only writable memory available is the 2 KB (Uno) or 16 KB (Micro:Bit) of SRAM which might not be sufficient to hold even a single feature vector.
",1. Introduction,[0],[0]
"The Internet of Things: A number of applications have been developed for consumer, enterprise and societal IoT including predictive maintenance, intelligent healthcare, smart cities and housing, etc.",1. Introduction,[0],[0]
"The dominant paradigm for these applications, given the severe resource constraints of IoT devices, has been that the IoT device is dumb – it just senses its environment and transmits the sensor readings to the cloud where all the decision making happens.
",1. Introduction,[0],[0]
Motivating scenarios: This paper proposes an alternative paradigm where the IoT device can make predictions locally without necessarily connecting to the cloud.,1. Introduction,[0],[0]
"This enables many scenarios, beyond the pale of the traditional paradigm, where it is not possible to transmit data to the cloud due to latency, bandwidth, privacy and energy concerns.",1. Introduction,[0],[0]
"For instance, consider a microcontroller implanted in the brain which warns patients about impending seizures so that they can call for help, pull over if they are driving, etc.",1. Introduction,[0],[0]
Making predictions locally would allow the device to work everywhere irrespective of cloud connectivity.,1. Introduction,[0],[0]
"Furthermore, alerts could be raised more quickly with local predictions than if all the sensor readings had to be first transmitted to the cloud.",1. Introduction,[0],[0]
"In addition, since the energy required for executing an instruction might be much lower than the energy required to transmit a byte, making predictions locally would extend battery life significantly thereby avoiding repeated brain surgery and might also prevent brain tissue damage due to excess heat dissipation from the communicating radio.",1. Introduction,[0],[0]
"Finally, people might not be willing to transmit such sensitive data to the cloud.",1. Introduction,[0],[0]
"These characteristics are shared by many other scenarios including implants in the heart, precision agriculture on disconnected farms, smart spectacles for the visually impaired, etc.
Tree algorithms: Tree algorithms are general and can be used for classification, regression, ranking and other problems commonly found in the IoT setting.",1. Introduction,[0],[0]
"Even more importantly, they are ideally suited to IoT applications as they can achieve good prediction accuracies with prediction times and energies that are logarithmic in the number of training points.",1. Introduction,[0],[0]
"Unfortunately, they do not directly fit on tiny IoT devices as their space complexity is linear rather than logarithmic.",1. Introduction,[0],[0]
"In particular, learning shallow trees, or aggressively pruning deep trees or large ensembles, to fit in just a few KB often leads to poor prediction accuracy.
",1. Introduction,[0],[0]
Bonsai:,1. Introduction,[0],[0]
"This paper develops a novel tree learner, called Bonsai, designed specifically for severely resource constrained IoT devices based on the following contributions.",1. Introduction,[0],[0]
"First, Bonsai learns a single, shallow, sparse tree so as to reduce model size but with powerful nodes for accurate prediction.",1. Introduction,[0],[0]
"Second, both internal and leaf nodes in Bonsai make non-linear predictions.",1. Introduction,[0],[0]
Bonsai’s overall prediction for a point is the sum of the individual node predictions along the path traversed by the point.,1. Introduction,[0],[0]
Path based prediction allows Bonsai to accurately learn non-linear decision boundaries while sharing parameters along paths to further reduce model size.,1. Introduction,[0],[0]
"Third, Bonsai learns a sparse matrix which projects all data points into a low-dimensional space in which the tree is learnt.",1. Introduction,[0],[0]
This allows Bonsai to fit in a few KB of flash.,1. Introduction,[0],[0]
"Furthermore, the sparse projection is implemented in a streaming fashion thereby allowing Bonsai to tackle IoT applications where even a single feature vector might not fit in 2 KB of RAM.",1. Introduction,[0],[0]
"Fourth, rather than learning the Bonsai tree node by node in a greedy fashion, all nodes are learnt jointly, along with the sparse projection matrix, so as to optimally allocate memory budgets to each node while maximising prediction accuracy.
",1. Introduction,[0],[0]
Implementation: Another contribution is an efficient implementation of Bonsai which reduces its prediction costs on the Arduino and Micro:Bit to be even lower than that of an unoptimized linear classifier.,1. Introduction,[0],[0]
This allows Bonsai to enjoy the prediction accuracy of a non-linear classifier while paying less than linear costs.,1. Introduction,[0],[0]
"This paper does not focus on the system and implementation details due to space limitations but the interested reader is referred to the publically available source code (BonsaiCode).
",1. Introduction,[0],[0]
"Results: These contributions allow Bonsai to make predictions in milliseconds even on slow microcontrollers, fit in a few KB of flash and extend battery life beyond all other algorithms.",1. Introduction,[0],[0]
"Furthermore, it is demonstrated on multiple benchmark datasets that Bonsai’s prediction accuracies can approach those of uncompressed kNN classifiers, RBFSVMs, single hidden layer neural networks and gradient boosted decision tree ensembles whose models might take many MB of RAM.",1. Introduction,[0],[0]
"It is also demonstrated that Bonsai’s prediction accuracies for a given model size can be as much
as 30% higher than state-of-the-art methods for resourceefficient machine learning.",1. Introduction,[0],[0]
"Finally, Bonsai is shown to generalize to other resource constrained settings beyond IoT by producing significantly better search results than Bing’s L3 ranker when the model size is restricted to 300 bytes.",1. Introduction,[0],[0]
"The literature on resource-efficient machine learning is vast and specialized solutions have been developed for reducing the prediction costs of kNN algorithms (Kusner et al., 2014b; Wang et al., 2016), SVMs (Hsieh et al., 2014; Jose et al., 2013; Le et al., 2013; Li et al., 2016), deep learning (Iandola et al., 2016; Han et al., 2016; Yang et al., 2015; Denton et al., 2014; Wu et al., 2016; Rastegari et al., 2016; Hubara et al., 2016; Shankar et al., 2016; Ioannou et al., 2016a), model compression (Bucilua et al., 2006; Ba & Caruana, 2014), feature selection (Kusner et al., 2014a; Xu et al., 2013; 2012; Nan et al., 2015; Wang et al., 2015) and applications such as face detection (Viola & Jones, 2004).
",2. Related Work,[0],[0]
Resource-efficient tree classifiers are particularly germane to this paper.,2. Related Work,[0],[0]
The standard approach is to greedily grow the decision tree ensemble node by node until the prediction budget is exhausted.,2. Related Work,[0],[0]
"A popular alternative is to first learn the random forest or gradient boosted decision tree ensemble to maximize prediction accuracy and then use pruning techniques to meet the budget constraints (Duda et al., 2002; Dekel et al., 2016; Nan et al., 2016; Li, 2001; Breiman et al., 1984; Zhang & Huei-chuen, 2005; Sherali et al., 2009; Kulkarni & Sinha, 2012; Rokach & Maimon, 2014; Joly et al., 2012).",2. Related Work,[0],[0]
"Unfortunately, such techniques are fundamentally limited as they attempt to approximate complex non-linear decision boundaries using a small number of axis-aligned hyperplanes.",2. Related Work,[0],[0]
"This can lead to poor prediction accuracies as observed in Section 5.
",2. Related Work,[0],[0]
Tree models have also been developed to learn more complex decision boundaries by moving away from learning axis-aligned hyperplanes at internal nodes and constant predictors at the leaves.,2. Related Work,[0],[0]
"For instance, (Breiman, 2001; Murthy et al., 1994; Kontschieder et al., 2015) learnt more powerful branching functions at internal nodes based on oblique cuts and full hyperplanes while (Utgoff, 1989; Hsieh et al., 2014) learnt more powerful leaf node predictors based on linear classifiers, kernelized SVMs, etc.",2. Related Work,[0],[0]
"Bonsai achieves better budget utilization than such models by learning shorter trees, typically depth 4 or lower, and by sharing the parameters between leaf node predictors.
",2. Related Work,[0],[0]
"The models closest to Bonsai are Decision Jungles (Shotton et al., 2013) and LDKL (Jose et al., 2013).",2. Related Work,[0],[0]
"Bonsai improves upon LDKL by learning its tree in a lowdimensional space, learning sparse branching functions and predictors and generalizing the model to multi-class classi-
fication, ranking, etc.",2. Related Work,[0],[0]
Decision Jungles are similar to Bonsai in that they share node parameters using a DAG structure.,2. Related Work,[0],[0]
"Unfortunately, Decision Jungles need to learn deep tree ensembles with many nodes as they use weak constant classifiers as leaf node predictors.",2. Related Work,[0],[0]
"Bonsai can have lower model size and higher accuracy as it learns a single, shallow tree in a low-dimensional space with non-linear predictors.
",2. Related Work,[0],[0]
"Note that while tree based cost-sensitive feature selection methods are not directly relevant, their performance is nevertheless empirically compared to Bonsai’s in Section 5.",2. Related Work,[0],[0]
Overview:,3. The Bonsai Model for Efficient Prediction,[0],[0]
"Bonsai learns a single, shallow sparse tree whose predictions for a point x are given by
y(x) =",3. The Bonsai Model for Efficient Prediction,[0],[0]
∑ k Ik(x)W > k,3. The Bonsai Model for Efficient Prediction,[0],[0]
Zx ◦,3. The Bonsai Model for Efficient Prediction,[0],[0]
"tanh(σV>k Zx) (1)
where ◦ denotes the elementwise Hadamard product, σ is a user tunable hyper-parameter, Z is a sparse projection matrix and Bonsai’s tree is parameterized by Ik, Wk and Vk where Ik(x) is an indicator function taking the value 1 if node k lies along the path traversed by x and 0 otherwise and Wk and Vk are sparse predictors learnt at node k.",3. The Bonsai Model for Efficient Prediction,[0],[0]
"The prediction function is designed to minimize the model size, prediction time and prediction energy, while maintaining prediction accuracy, even at the expense of increased training costs.",3. The Bonsai Model for Efficient Prediction,[0],[0]
"The function is also designed to minimize the working memory required as the Uno provides only 2 KB of writeable memory for storing the feature vector, programme variables and intermediate computations.
",3. The Bonsai Model for Efficient Prediction,[0],[0]
"Streaming sparse projection: Bonsai projects each D-dimensional input feature vector x into a low D̂dimensional space using a learnt sparse projection matrix ZD̂×D. Bonsai uses fixed point arithmetic for all math computation, including Zx, when implemented on the IoT device so as to avoid floating point overheads.",3. The Bonsai Model for Efficient Prediction,[0],[0]
Note that D̂ could be as low as 5 for many binary classification applications.,3. The Bonsai Model for Efficient Prediction,[0],[0]
This has the following advantages.,3. The Bonsai Model for Efficient Prediction,[0],[0]
"First, it reduces Bonsai’s model size as all tree parameters are now learnt in the low-dimensional space.",3. The Bonsai Model for Efficient Prediction,[0],[0]
"Second, when D̂ is small, Zx could be stored directly in the microcontroller’s registers thereby reducing prediction time and energy.",3. The Bonsai Model for Efficient Prediction,[0],[0]
"Third, learning the projection matrix jointly with the tree parameters improves prediction accuracy.",3. The Bonsai Model for Efficient Prediction,[0],[0]
"Fourth, since Zx can be computed in a streaming fashion, this allows Bonsai to tackle IoT applications where even a single feature vector cannot fit in 2 KB of SRAM.",3. The Bonsai Model for Efficient Prediction,[0],[0]
This is critical since standard tree implementations are unable to handle a streaming feature vector – the entire feature vector needs to be streamed for the root node to determine whether to pass the point down to the left or right child and therefore the vector is unavailable for processing at subsequent nodes.,3. The Bonsai Model for Efficient Prediction,[0],[0]
"Some
implementations work around this limitation by simultaneously evaluating the branching function at all nodes as the vector is streamed but this increases the prediction costs from logarithmic to linear which might not be acceptable.
",3. The Bonsai Model for Efficient Prediction,[0],[0]
Branching function at internal nodes: Bonsai computes Ik by learning a sparse vector θ at each internal node such that the sign of θ>Zx determines whether point x should be branched to the node’s left or right child.,3. The Bonsai Model for Efficient Prediction,[0],[0]
Using more powerful branching functions than the axis-aligned hyperplanes in standard decision trees allows Bonsai to learn shallow trees which can fit in a few KB.,3. The Bonsai Model for Efficient Prediction,[0],[0]
"Of course, this is not a novel idea, and is insufficient in itself to allow a single, shallow decision tree to make accurate predictions.
",3. The Bonsai Model for Efficient Prediction,[0],[0]
"Node predictors: Decision trees, random forests and boosted tree ensembles are limited to making constant predictions at just the leaf nodes.",3. The Bonsai Model for Efficient Prediction,[0],[0]
This restricts their prediction accuracy when there are very few leaves.,3. The Bonsai Model for Efficient Prediction,[0],[0]
"In contrast, for a multi-class, multi-label or regression problem with L targets, Bonsai learns matrices WD̂×L and VD̂×L at both leaf and internal nodes so that each node predicts the vector W>Zx◦tanh(σV>Zx).",3. The Bonsai Model for Efficient Prediction,[0],[0]
Note that the functional form of the node predictor was chosen as it was found to work well empirically (other forms could be chosen if found to be more appropriate).,3. The Bonsai Model for Efficient Prediction,[0],[0]
"Further note that W and V will reduce to vectors for binary classification, ranking and singletarget regression.",3. The Bonsai Model for Efficient Prediction,[0],[0]
Bonsai’s overall predicted vector is given by (1) and is the sum of the individual vectors predicted by the nodes lying along the path traversed by x. This allows Bonsai to accurately learn non-linear decision boundaries using shallow trees with just a few nodes.,3. The Bonsai Model for Efficient Prediction,[0],[0]
"Furthermore, path based prediction allows parameter sharing and therefore reduces model size as compared to putting independent classifiers of at least equal complexity in the leaf nodes alone.",3. The Bonsai Model for Efficient Prediction,[0],[0]
"For instance, a depth 4 Bonsai tree with 15 internal and 16 leaf nodes stores 31 W and 31 V matrices with overall predictions being the sum of 4 terms depending on the path taken.",3. The Bonsai Model for Efficient Prediction,[0],[0]
"If parameters were not shared and each leaf node independently learnt 4 W and 4 V matrices to make predictions of at least equal complexity, then a total of 16× 4 = 64 W and 64 V matrices would need to be stored thereby exceeding the memory budget.",3. The Bonsai Model for Efficient Prediction,[0],[0]
"As an implementation detail, note that Bonsai uses the approximation tanh(x)",3. The Bonsai Model for Efficient Prediction,[0],[0]
≈ x,3. The Bonsai Model for Efficient Prediction,[0],[0]
if |x| < 1 and signum(x) otherwise in order to avoid floating point computation.,3. The Bonsai Model for Efficient Prediction,[0],[0]
Notation: Bonsai learns a balanced tree of user specified height h with 2h − 1 internal nodes and 2h leaf nodes.,4. Training Bonsai,[0],[0]
The parameters that need to be learnt include: (a) Z: the sparse projection matrix; (b) θ =,4. Training Bonsai,[0],[0]
"[θ1, . . .",4. Training Bonsai,[0],[0]
",θ2h−1]: the parameters of the branching function at each internal node; and (c) W =",4. Training Bonsai,[0],[0]
"[W1, . . .",4. Training Bonsai,[0],[0]
",W2h+1−1] and V =",4. Training Bonsai,[0],[0]
"[V1, . . .",4. Training Bonsai,[0],[0]
",V2h+1−1]:
the predictor parameters at each node.",4. Training Bonsai,[0],[0]
Let Θ =,4. Training Bonsai,[0],[0]
"[θ,W,V] denote a matrix obtained by stacking all the parameters together except for Z. Finally, it is assumed that N training points {(xi, y
¯i )Ni=1} have been provided and that bud-
get constraints BZ and BΘ on the projection matrix and tree parameters have been specified depending on the flash memory available on the IoT device.
",4. Training Bonsai,[0],[0]
"Optimization problem: Bonsai’s parameters are learnt as
min Z,Θ J (Z,Θ) = λθ 2 Tr(θ>θ) + λW 2 Tr(W>W)
+ λV 2 Tr(V>V) + λZ 2 Tr(ZZ>)
+ 1
N N∑ i=1",4. Training Bonsai,[0],[0]
"L(xi,yi,y(xi);Z,Θ)
s. t. ‖Z‖0 ≤",4. Training Bonsai,[0],[0]
"BZ, ‖Θ‖0 ≤ BΘ
(2)
where y(xi) is Bonsai’s prediction for point xi as given in (1) and L is an appropriately chosen loss function for classification, regression, ranking, etc.",4. Training Bonsai,[0],[0]
"For instance, L = max(0, 1 − yiy(xi))",4. Training Bonsai,[0],[0]
"with yi ∈ {−1,+1} for binary classification and L = maxy∈Y((yi − y)>y(xi) +",4. Training Bonsai,[0],[0]
"1 − y>i y) with Y = {y|y ∈ {0, 1}L,1>y = 1} and yi ∈ Y for multi-class classification.",4. Training Bonsai,[0],[0]
It is worth emphasizing that the optimization problem is formulated such that all parameters are learnt jointly subject to the budget constraints.,4. Training Bonsai,[0],[0]
"This leads to significantly higher prediction accuracies than if Z were first learnt independently, say using sparse PCA, and then Θ was learnt afterwards (see Section 5).
",4. Training Bonsai,[0],[0]
"Algorithm: Optimizing (2) over the space of all balanced trees of height h is a hard, non-convex problem.",4. Training Bonsai,[0],[0]
Tree growing algorithms typically optimize such problems by greedily growing the tree a node at a time starting from the root.,4. Training Bonsai,[0],[0]
"Unfortunately, this leads to a suboptimal utilization of the memory budget in Bonsai’s case as it is not clear a priori how much budget to allocate to each node.",4. Training Bonsai,[0],[0]
"For instance, it is not apparent whether the budget should be distributed equally between all nodes or whether the root node should be allocated more budget and, if so, by how much.
",4. Training Bonsai,[0],[0]
Algorithm - Joint learning of nodes: Bonsai therefore learns all node parameters jointly with the memory budget for each node being determined automatically as part of the optimization.,4. Training Bonsai,[0],[0]
The difficulty with joint learning is that a node’s ancestors need to be learnt before it can be determined which training points will reach the node.,4. Training Bonsai,[0],[0]
"Furthermore, the path traversed by a training point is a sharply discontinuous function of θ and Z thereby rendering gradient based techniques ineffective.",4. Training Bonsai,[0],[0]
"Various approaches have been proposed in the literature for tackling these difficulties (Jose et al., 2013; Kontschieder et al., 2015; Norouzi et al., 2015; Xu et al., 2013; Ioannou et al., 2016b).",4. Training Bonsai,[0],[0]
"Bonsai follows the approach of (Jose et al., 2013)
and smooths the objective function by initially allowing points to traverse multiple paths in the tree.",4. Training Bonsai,[0],[0]
"In particular, the indicator function Ik(x) is relaxed to Ik>1(x) = 1 2Ij(x) ( 1 + (−1)k−2j tanh(σIθ>j Zx) ) where j = ⌊ k 2 ⌋ is k’s parent node in a balanced tree, I1(x)",4. Training Bonsai,[0],[0]
= 1 and the parameter σI controls the fidelity of the approximation.,4. Training Bonsai,[0],[0]
"Gradients can now be computed as
∇θlIk(x) = σIIk(x)P lk(x)Zx (3) ∇ZIk(x) =",4. Training Bonsai,[0],[0]
∑ l σIIk(x)P,4. Training Bonsai,[0],[0]
"l k(x)θlx > (4)
where P lk(x) = δ l k((−1)Ck(l)",4. Training Bonsai,[0],[0]
"− tanh(σIθ>l Zx)), δlk = 1 if node l is an ancestor of node k and 0 otherwise and Ck(l) = 1 if node k is in the right subtree of node l and 0 otherwise.",4. Training Bonsai,[0],[0]
"Of course, allowing a point to traverse multiple paths increases prediction costs.",4. Training Bonsai,[0],[0]
"Some approaches therefore allow multiple paths during training but select a single path during prediction (Xu et al., 2013; Ioannou et al., 2016b).",4. Training Bonsai,[0],[0]
"At each node, a point x is greedily branched to the child node having the greatest Ik(x).",4. Training Bonsai,[0],[0]
"Unfortunately, this can lead to a drop in accuracy as the model learnt during training is different from the one used for prediction.
",4. Training Bonsai,[0],[0]
Bonsai therefore follows an alternative strategy where σI is tuned during training to ensure that points gradually start traversing at most a single path as optimization progresses.,4. Training Bonsai,[0],[0]
"In particular, σI is initialized to a small value, such as 0.01, so as to ensure that tanh values are not saturated.",4. Training Bonsai,[0],[0]
"As optimization progresses, σI is gradually increased so that tanh tends to the signum function and Ik(x) goes back to being an indicator function by the time convergence is reached.",4. Training Bonsai,[0],[0]
"This allows Bonsai to directly use the learnt model for prediction and was found to empirically lead to good results.
",4. Training Bonsai,[0],[0]
"Algorithm - Gradient descent with iterative hard thresholding: Various gradient based approaches, including those based on alternating minimization, were tried for optimizing (2).",4. Training Bonsai,[0],[0]
A gradient descent based algorithm with iterative hard thresholding (IHT) was empirically found to yield the best solutions.,4. Training Bonsai,[0],[0]
"Gradient descent was chosen over stochastic gradient descent as it removed the burden of step size tuning, led to slightly better prediction accuracies while keeping training time acceptable.",4. Training Bonsai,[0],[0]
"For instance, training times range from 2 minutes for USPS-2 to 15 minutes for MNIST-2 on a single core of a laptop with an Intel Core i7-3667U processor at 2 GHz with 8 GB of RAM.",4. Training Bonsai,[0],[0]
Stochastic gradient descent could be utilized for larger datasets or if training costs also needed to be minimized.,4. Training Bonsai,[0],[0]
"The algorithm proceeds iteratively based on the following gradient and IHT steps in each iteration.
",4. Training Bonsai,[0],[0]
Algorithm - Gradient step:,4. Training Bonsai,[0],[0]
"Given feasible Zt and Θt with a feasible allocation of the memory budget to various nodes at time step t, Bonsai applies M updates of gradient descent keeping the support of Z and Θ fixed so that
the budget allocations to nodes remain unchanged and the memory constraints are never violated.",4. Training Bonsai,[0],[0]
"The update equations at each time step are
Zt+1 =",4. Training Bonsai,[0],[0]
"Zt − ηtZ∇ZJ (Zt,Θt)|supp(Zt) (5) Θt+1",4. Training Bonsai,[0],[0]
=,4. Training Bonsai,[0],[0]
Θt − ηtΘ∇ΘJ,4. Training Bonsai,[0],[0]
"(Zt,Θt)|supp(Θt) (6)
with step sizes ηZ and ηΘ being chosen according to the Armijo rule and |supp indicating that the gradient was being computed only for the non-zero entries.",4. Training Bonsai,[0],[0]
M = 5 and M = 15 iterations were found to work well for binary and multi-class classification respectively.,4. Training Bonsai,[0],[0]
"This allows Bonsai to decrease the objective function value without changing the budget allocation of various nodes.
",4. Training Bonsai,[0],[0]
"Algorithm - IHT step: In order to improve the budget allocation, Bonsai performs a single gradient update with unrestricted support.",4. Training Bonsai,[0],[0]
"This violates the memory constraints and Bonsai therefore projects the solution onto the feasible set by retaining the parameters with the largest magnitudes
Zt+M+1 = TBZ(Z t+M − ηt+MZ",4. Training Bonsai,[0],[0]
"∇ZJ (Z t+M ,Θt+M ))",4. Training Bonsai,[0],[0]
"Θt+M+1 = TBΘ(Θ t+M − ηt+MΘ ∇ΘJ (Z t+M ,Θt+M ))
where Tk is an operator returning k of its arguments which have the largest magnitudes while setting the rest to 0.",4. Training Bonsai,[0],[0]
"This allows Bonsai to move to another feasible solution with even lower objective function value by improving the memory budget distribution across nodes.
",4. Training Bonsai,[0],[0]
Algorithm - Convergence:,4. Training Bonsai,[0],[0]
"In general, projected gradient descent based algorithms might oscillate for non-convex problems.",4. Training Bonsai,[0],[0]
"However, (Blumensath & Davies, 2008) prove that for smooth objective functions, gradient descent algorithms with IHT do indeed converge to a saddle point solution.",4. Training Bonsai,[0],[0]
"Furthermore, if the objective function satisfies the Restricted Strong Convexity (RSC) property in a local region, then projected gradient descent with IHT will converge to the local minimum in that region (Jain et al., 2014).",4. Training Bonsai,[0],[0]
"In practice, it was observed that the algorithm generally converged to a good solution soon and therefore was terminated after T = 300 iterations were reached.
",4. Training Bonsai,[0],[0]
Algorithm - Initialization & re-training: Z0 and Θ0 could be set randomly.,4. Training Bonsai,[0],[0]
Prediction accuracy gains of up to 1.5% could be observed if Bonsai was initialized by taking T steps of gradient descent without any budget constraints followed by a hard thresholding step.,4. Training Bonsai,[0],[0]
Further gains of 1.5% could be observed by taking another T steps of gradient descent with fixed support after termination.,4. Training Bonsai,[0],[0]
"This helped in fine-tuning Bonsai’s parameters once the memory budget allocation had been finalized across the tree nodes.
",4. Training Bonsai,[0],[0]
More details about the optimization can be found in the supplementary material by clicking here.,4. Training Bonsai,[0],[0]
"Datasets: Experiments were carried out on a number of publically available binary and multi-class datasets including Chars4K (Campos et al., 2009), CIFAR10 (Krizhevsky, 2009), MNIST (LeCun et al., 1998), WARD (Yang et al., 2009), USPS (Hull, 1994), Eye (Kasprowski & Ober, 2004), RTWhale (RTW), and CUReT (Varma & Zisserman, 2005).",5. Experiments,[0],[0]
"Binary versions of these datasets were downloaded from (Jose et al., 2013).",5. Experiments,[0],[0]
Bing’s L3 Ranking is a proprietary dataset where ground truth annotations specifying the relevance of query-document pairs have been provided on a scale of 0-4.,5. Experiments,[0],[0]
"Table 1 lists these datasets’ statistics.
",5. Experiments,[0],[0]
"Baseline algorithms: Bonsai was compared to stateof-the-art algorithms for resource-efficient ML spanning tree, kNN, SVM and single hidden layer neural network algorithms including Decision Jungles (Shotton et al., 2013; Pohlen), Feature Budgeted Random Forests (BudgetRF) (Nan et al., 2015),",5. Experiments,[0],[0]
"Gradient Boosted Decision Tree Ensemble Pruning (Tree Pruning) (Dekel et al., 2016), Pruned Random Forests (BudgetPrune) (Nan et al., 2016), Local Deep Kernel Learning (LDKL) (Jose et al., 2013), Neural Network Pruning (NeuralNet Pruning) (Han et al., 2016) and Stochastic Neighbor Compression (SNC) (Kusner et al., 2014b).",5. Experiments,[0],[0]
The differences between some of these algorithms and Bonsai is briefly discussed in Section 2.,5. Experiments,[0],[0]
Publically available implementations of all algorithms were used taking care to ensure that published results could be reproduced thereby verifying the code and hyper-parameter settings.,5. Experiments,[0],[0]
Note that Bonsai is not compared to deep convolutional neural networks as they have not yet been demonstrated to fit on such tiny IoT devices.,5. Experiments,[0],[0]
"In particular, convolutions are computationally expensive, drain batteries and produce intermediate results which do not fit in 2 KB RAM.",5. Experiments,[0],[0]
"Implementing them on tiny microcontrollers is still
0 50 100 10
20
30
40
50
60 Chars4K−62
Model Size (KB)
",5. Experiments,[0],[0]
"A cc
ur ac
y (%
)
",5. Experiments,[0],[0]
BonsaiOpt Bonsai NeuralNet Pruning SNC,5. Experiments,[0],[0]
"Decision Jungle BudgetPrune BudgetRF
0 50 100 70
80
90
CUReT−61
Model Size (KB)
",5. Experiments,[0],[0]
"A cc
ur ac
y (%
)
",5. Experiments,[0],[0]
BonsaiOpt Bonsai NeuralNet Pruning SNC,5. Experiments,[0],[0]
"Decision Jungle BudgetPrune BudgetRF
0 50 100 80
85
90
95
MNIST−10
Model Size (KB)
",5. Experiments,[0],[0]
"A cc
ur ac
y (%
)
",5. Experiments,[0],[0]
BonsaiOpt Bonsai NeuralNet Pruning SNC,5. Experiments,[0],[0]
"Decision Jungle BudgetPrune BudgetRF
0.2 0.4 0.6 0.8 1 42
44
46
48
50
52
L3 Ranking
Model Size (KB)
nD",5. Experiments,[0],[0]
"C
G @
1
Bonsai FastRank
0 5 10 15 50
Model Size (KB)
0 5 10 15 80
Model Size (KB)
0 5 10 15 90
Model Size (KB)
0 5 10 15 88
Model Size (KB)
0 5 10 15 66
68
70
72
74
76
78 Chars4K−2
Model Size (KB)
",5. Experiments,[0],[0]
"A cc
ur ac
y (%
)
0 5 10 15 68
70
72
74
76
CIFAR10−2
Model Size (KB)
",5. Experiments,[0],[0]
"A cc
ur ac
y (%
)
0 5 10 15 90
92
94
96
USPS−2
Model Size (KB)
",5. Experiments,[0],[0]
"A cc
ur ac
y (%
)
",5. Experiments,[0],[0]
"Legend
−
−
BonsaiOpt Bonsai GBDT",5. Experiments,[0],[0]
Tree Pruning LDKL LDKL−L1 NeuralNet Pruning SNC,5. Experiments,[0],[0]
"Decision Jungle BudgetPrune BudgetRF
Figure 2: Binary Datasets - Bonsai dominates over state-of-the-art resource-efficient ML algorithms with gains of 8.6% on RTWhale-2 and 8.2% on Eye-2 in the 0-2 KB range.",5. Experiments,[0],[0]
BonsaiOpt’s gains are even higher.,5. Experiments,[0],[0]
"Figure best viewed magnified.
",5. Experiments,[0],[0]
an open research problem.,5. Experiments,[0],[0]
"Bonsai’s performance was however compared to that of uncompressed single hidden layer neural networks without convolutions, Gradient Boosted Decision Trees (GBDT), kNN classifiers and RBF-SVMs.
",5. Experiments,[0],[0]
Hyper-parameters: The publically provided training set for each dataset was subdivided into 80% for training and 20% for validation.,5. Experiments,[0],[0]
The hyper-parameters of all algorithms were tuned on the validation set.,5. Experiments,[0],[0]
"Once the hyperparameters had been fixed, the algorithms were trained on the full training set and results were reported on the publically available test set.
",5. Experiments,[0],[0]
Evaluation: IoT applications would like to maximize their prediction accuracies using the best model that might fit within the available flash memory while minimizing their prediction times and energies.,5. Experiments,[0],[0]
Accuracies of all algorithms are therefore presented for a range of model sizes.,5. Experiments,[0],[0]
"Some
of the algorithms were implemented on the Uno and their prediction times and energies were compared to Bonsai’s.
",5. Experiments,[0],[0]
Implementation: Results are presented throughout for an unoptimized implementation of Bonsai for a fair comparison with the other methods.,5. Experiments,[0],[0]
"For instance, 4 bytes were used to store floating point numbers for all algorithms, all floating point operations were simulated in software, etc.",5. Experiments,[0],[0]
"However, results are also presented for an optimized implementation of Bonsai, called BonsaiOpt, where numbers were stored in a 1 byte fixed point format, tanh was approximated, all floating point operations were avoided, etc.
",5. Experiments,[0],[0]
"Comparison to uncompressed methods: The results in Tables 2 and 3 demonstrate that Bonsai’s prediction accuracies could compete with those of uncompressed kNN, GBDT, RBF-SVM and neural network classifiers with significantly larger model sizes.",5. Experiments,[0],[0]
"On RTWhale-2, Chars4K-62
and Chars4K-2, Bonsai’s accuracies were higher than all other methods by 4.8%, 3.2% and 1.1% while its model size was lower by 977x, 13x and 157x respectively.",5. Experiments,[0],[0]
Bonsai’s accuracies were lower by 1.0% - 5.0% on the other datasets with model size gains varying from 55x to 3996x.,5. Experiments,[0],[0]
"Note that, while BonsaiOpt’s accuracies were similar to Bonsai’s, its model sizes would be even lower.
",5. Experiments,[0],[0]
Comparison to resource-efficient ML algorithms: The results in Figures 2 and 3 demonstrate that Bonsai’s prediction accuracies dominate those of state-of-the-art resourceefficient ML algorithms for all model sizes.,5. Experiments,[0],[0]
"In fact, Bonsai could outperform all other algorithms, including tree algorithms by as much as 30.7% on Char4K-62 and 28.9% on CUReT-61 for a given model size.",5. Experiments,[0],[0]
"For binary datasets, the largest gains were observed in the 0-2 KB regime including 8.6% on RTWhale-2 and 8.2% on Eye-2.",5. Experiments,[0],[0]
"Of course, BonsaiOpt’s gains were even higher on both binary and
multi-class datasets.",5. Experiments,[0],[0]
"These results validate Bonsai’s model, showing it to be accurate and compact and demonstrate that Bonsai’s optimization algorithm yields good solutions.
L3 ranking: Bonsai was shown to generalise to other resource-constrained scenarios beyond IoT by ranking documents in response to queries on Bing.",5. Experiments,[0],[0]
"Bonsai was trained by replacing the classification gradients with rank-sensitive gradients approximating nDCG (Burges, 2010).",5. Experiments,[0],[0]
"As can be seen in Figure 1, using a 300 byte model, Bonsai could outperform Bing’s FastRank L3 ranker by 8.3%.",5. Experiments,[0],[0]
"In fact, Bonsai could achieve almost the same ranking accuracy as FastRank but with a 660x smaller model.
",5. Experiments,[0],[0]
Prediction on the Arduino Uno: Table 5 presents the prediction costs per test point for the highest accuracy models with size less than 2 KB for a few methods that were implemented on the Arduino Uno.,5. Experiments,[0],[0]
The BonsaiOpt model was a more efficient implementation of the chosen Bonsai model.,5. Experiments,[0],[0]
"The results indicate that BonsaiOpt could be significantly more accurate, faster and energy-efficient as compared to other algorithms including an unoptimized linear classifier.",5. Experiments,[0],[0]
"Transmitting the test feature vector to the cloud, whenever possible, and running uncompressed GBDT might sometimes yield higher accuracies but would also consume 47x497x more energy which might not be feasible.
Bonsai’s components: The contribution of Bonsai’s components on the Chars4K-2 dataset is presented in Table 4.",5. Experiments,[0],[0]
Modest reductions in accuracy were observed without proper initialization or re-training.,5. Experiments,[0],[0]
Learning a projection matrix independently via sparse PCA before training reduced accuracy significantly as compared to Bonsai’s joint training of the projection matrix and tree parameters.,5. Experiments,[0],[0]
Other tree and uncompressed methods also did not benefit much by training in the sparse PCA space.,5. Experiments,[0],[0]
"This paper proposed an alternative IoT paradigm, centric to the device rather than the cloud, where ML models run on tiny IoT devices without necessarily connecting to the cloud thereby engendering local decision making capabilities.",6. Conclusions,[0],[0]
"The Bonsai tree learner was developed towards this end and demonstrated to be fast, accurate, compact and energy-efficient at prediction time.",6. Conclusions,[0],[0]
"Bonsai was deployed on the Arduino Uno board as it could fit in a few KB of flash, required only 70 bytes of writable memory for binary classification and 500 bytes for a 62 class problem, handled streaming features and made predictions in milliseconds taking only milliJoules of energy.",6. Conclusions,[0],[0]
"Bonsai’s prediction accuracies could be as much as 30% higher as com-
pared to state-of-the-art resource-efficient ML algorithms for a fixed model size and could even approach and outperform those of uncompressed models taking many MB of RAM.",6. Conclusions,[0],[0]
"Bonsai achieved these gains by developing a novel model based on a single, shallow, sparse tree learnt in a low-dimensional space.",6. Conclusions,[0],[0]
Predictions made by both internal and leaf nodes and the sharing of parameters along paths allowed Bonsai to learn complex non-linear decision boundaries using a compact representation.,6. Conclusions,[0],[0]
Bonsai’s code is available from (BonsaiCode) and is part of Microsoft’s ELL machine learning compiler for IoT devices.,6. Conclusions,[0],[0]
"We are grateful to Yeshwanth Cherapanamjeri, Ofer Dekel, Chirag Gupta, Prateek Jain, Ajay Manchepalli, Nagarajan Natarajan, Praneeth Netrapalli, Bhargavi Paranjape, Suresh Parthasarathy, Vivek Seshadri, Rahul Sharma, Harsha Vardhan Simhadri, Manish Singh and Raghavendra Udupa for many helpful discussions and feedback.",Acknowledgements,[0],[0]
"This paper develops a novel tree-based algorithm, called Bonsai, for efficient prediction on IoT devices – such as those based on the Arduino Uno board having an 8 bit ATmega328P microcontroller operating at 16 MHz with no native floating point support, 2 KB RAM and 32 KB read-only flash.",abstractText,[0],[0]
"Bonsai maintains prediction accuracy while minimizing model size and prediction costs by: (a) developing a tree model which learns a single, shallow, sparse tree with powerful nodes; (b) sparsely projecting all data into a low-dimensional space in which the tree is learnt; and (c) jointly learning all tree and projection parameters.",abstractText,[0],[0]
"Experimental results on multiple benchmark datasets demonstrate that Bonsai can make predictions in milliseconds even on slow microcontrollers, can fit in KB of memory, has lower battery consumption than all other algorithms while achieving prediction accuracies that can be as much as 30% higher than stateof-the-art methods for resource-efficient machine learning.",abstractText,[0],[0]
Bonsai is also shown to generalize to other resource constrained settings beyond IoT by generating significantly better search results as compared to Bing’s L3 ranker when the model size is restricted to 300 bytes.,abstractText,[0],[0]
Bonsai’s code can be downloaded from (BonsaiCode).,abstractText,[0],[0]
Resource-efficient Machine Learning in 2 KB RAM for the Internet of Things,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 925–930 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
925",text,[0],[0]
"Natural language to code generation, a subtask of semantic parsing, is the problem of converting natural language (NL) descriptions to code (Ling et al., 2016; Yin and Neubig, 2017; Rabinovich et al., 2017).",1 Introduction,[0],[0]
"This task is challenging because it has a well-defined structured output and the input structure and output structure are in different forms.
",1 Introduction,[0],[0]
A number of neural network approaches have been proposed to solve this task.,1 Introduction,[0],[0]
"Sequential approaches (Ling et al., 2016; Jia and Liang, 2016; Locascio et al., 2016) convert the target code into a sequence of symbols and apply a sequence-tosequence model, but this approach does not ensure that the output will be syntactically correct.
",1 Introduction,[0],[0]
"1Code available at https://github.com/ sweetpeach/ReCode
Tree-based approaches (Yin and Neubig, 2017; Rabinovich et al., 2017) represent code as Abstract Syntax Trees (ASTs), which has proven effective in improving accuracy as it enforces the well-formedness of the output code.",1 Introduction,[0],[0]
"However, representing code as a tree is not a trivial task, as the number of nodes in the tree often greatly exceeds the length of the NL description.",1 Introduction,[0],[0]
"As a result, tree-based approaches are often incapable of generating correct code for phrases in the corresponding NL description that have low frequency in the training data.
",1 Introduction,[0],[0]
"In machine translation (MT) problems (Zhang et al., 2018; Gu et al., 2018; Amin Farajian et al., 2017; Li et al., 2018), hybrid methods combining retrieval of salient examples and neural models have proven successful in dealing with rare words.",1 Introduction,[0],[0]
"Following the intuition of these models, we hypothesize that our model can benefit from querying pairs of NL descriptions and AST structures from training data.
",1 Introduction,[0],[0]
"In this paper, we propose RECODE, and adaptation of Zhang et al. (2018)’s retrieval-based approach neural MT method to the code generation problem by expanding it to apply to generation of tree structures.",1 Introduction,[0],[0]
Our main contribution is to introduce the use of retrieval methods in neural code generation models.,1 Introduction,[0],[0]
We also propose a dynamic programming-based sentence-tosentence alignment method that can be applied to similar sentences to perform word substitution and enable retrieval of imperfect matches.,1 Introduction,[0],[0]
These contributions allow us to improve on previous stateof-the-art results.,1 Introduction,[0],[0]
"Given an NL description q, our purpose is to generate code (e.g. Python) represented as an AST a.",2 Syntactic Code Generation,[0],[0]
"In this work, we start with the syntactic code gen-
eration model by Yin and Neubig (2017), which uses sequences of actions to generate the AST before converting it to surface code.",2 Syntactic Code Generation,[0],[0]
"Formally, we want to find the best generated AST â given by:
â = argmax a
p(a|q)
p(a|q) = T∏ t=1 p(yt|y<t, q)
where yt is the action taken at time step t and y<t = y1...",2 Syntactic Code Generation,[0],[0]
"yt−1 and T is the number of total time steps of the whole action sequence resulting in AST a.
We have two types of actions to build an AST: APPLYRULE and GENTOKEN.",2 Syntactic Code Generation,[0],[0]
APPLYRULE(r) expands the current node in the tree by applying production rule r from the abstract syntax grammar2 to the current node.,2 Syntactic Code Generation,[0],[0]
GENTOKEN(v) populates terminal nodes with the variable v which can be generated from vocabulary or by COPYing variable names or values from the NL description.,2 Syntactic Code Generation,[0],[0]
The generation process follows a preorder traversal starting with the root node.,2 Syntactic Code Generation,[0],[0]
"Figure 1 shows an action tree for the example code: the nodes correspond to actions per time step in the construction of the AST.
",2 Syntactic Code Generation,[0],[0]
"Interested readers can reference Yin and Neubig (2017) for more detail of the neural model, which consists of a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) encoder-decoder with action embeddings, context vectors, parent feeding, and a copy mechanism using pointer networks.",2 Syntactic Code Generation,[0],[0]
"We propose RECODE, a method for retrievalbased neural syntactic code generation, using retrieved action subtrees.",3 RECODE: Retrieval-Based Neural Code Generation,[0],[0]
"Following Zhang et al. (2018)’s method for neural machine translation, these retrieved subtrees act as templates that bias the generation of output code.",3 RECODE: Retrieval-Based Neural Code Generation,[0],[0]
"Our pipeline at test time is as follows: • retrieve from the training set NL descriptions
that are most similar with our input sentence (§3.1), • extract n-gram action subtrees from these
retrieved sentences’ corresponding target ASTs (§3.2),
2https://docs.python.org/2/library/ ast.html
• alter the copying actions in these subtrees, by substituting words of the retrieved sentence with corresponding words in the input sentence (§3.3), and • at every decoding step, increase the probabil-
ity of actions that would lead to having these subtrees in the produced tree (§3.4).",3 RECODE: Retrieval-Based Neural Code Generation,[0],[0]
"For every retrieved NL description qm from training set (or retrieved sentence for short), we compute its similarity with input q, using a sentence similarity formula (Gu et al., 2016; Zhang et al., 2018):
sim(q, qm) = 1−",3.1 Retrieval of Training Instances,[0],[0]
"d(q, qm)
max(|q| ,|qm|)
where d is the edit distance.",3.1 Retrieval of Training Instances,[0],[0]
We retrieve only the top M sentences according to this metric where M is a hyperparameter.,3.1 Retrieval of Training Instances,[0],[0]
These scores will later be used to increase action probabilities accordingly.,3.1 Retrieval of Training Instances,[0],[0]
"In Zhang et al. (2018), they collect n-grams from the output side of the retrieved sentences and encourage the model to generate these n-grams.",3.2 Extracting N -gram Action Subtrees,[0],[0]
"Word n-grams are obvious candidates when generating a sequence of words as output, as in NMT.",3.2 Extracting N -gram Action Subtrees,[0],[0]
"However, in syntax-based code generation, the generation target is ASTs with no obvious linear structure.",3.2 Extracting N -gram Action Subtrees,[0],[0]
"To resolve this problem, we instead use retrieved pieces of n-gram subtrees from the target code corresponding to the retrieved NL descriptions.",3.2 Extracting N -gram Action Subtrees,[0],[0]
"Though we could select successive nodes in the AST as retrieved pieces, such as [assign; expr*(targets); expr] from Figure 1, we would miss important structural information from the rules that are used.",3.2 Extracting N -gram Action Subtrees,[0],[0]
"Thus, we choose to exploit actions in the generation model rather than AST nodes themselves to be candidates for our retrieved pieces.
",3.2 Extracting N -gram Action Subtrees,[0],[0]
"In the action tree (Figure 1), we considered only successive actions, such as subtrees where each node has one or no children, to avoid overly rigid structures or combinatorial explosion of the number of retrieved pieces the model has to consider.",3.2 Extracting N -gram Action Subtrees,[0],[0]
"For example, such an action subtree would be given by [assign → expr*(targets), expr(value) ; expr(value) → List; List → epsilon].
",3.2 Extracting N -gram Action Subtrees,[0],[0]
"As the node in the action tree holds structural information about its children, we set the subtrees
to have a fixed depth, linear in the size of the tree.",3.2 Extracting N -gram Action Subtrees,[0],[0]
"These can be considered “n-grams of actions”, emphasizing the comparison with machine translation which uses n-grams of words.",3.2 Extracting N -gram Action Subtrees,[0],[0]
n is a hyperparameter to be tuned.,3.2 Extracting N -gram Action Subtrees,[0],[0]
Using the retrieved subtree without modification is problematic if it contains at least one node corresponding to a COPY action because copied tokens from the retrieved sentence may be different from those in the input.,3.3 Word Substitution in Copy Actions,[0],[0]
"Figure 1 shows an example when the input and retrieved sentence have four common words, but the object names are different.",3.3 Word Substitution in Copy Actions,[0],[0]
"The extracted action n-gram would contain the rule that copies the second word (“lst”) of the retrieved sentence while we want to copy the first word (“params”) from the input.
",3.3 Word Substitution in Copy Actions,[0],[0]
"By computing word-based edit distance between the input description and the retrieved sentence, we implement a one-to-one sentence alignment method that infers correspondences between uncommon words.",3.3 Word Substitution in Copy Actions,[0],[0]
"For unaligned words, we alter all COPY rules in the extracted n-grams to copy tokens by their aligned counterpart, such as replace “params” with “lst”, and delete the n-gram subtree, as it is not likely to be relevant in the predicted tree.",3.3 Word Substitution in Copy Actions,[0],[0]
"Thus, in the example in Figure 1, the GENTOKEN(LST) action in t5 will not be executed.",3.3 Word Substitution in Copy Actions,[0],[0]
"N -gram subtrees from all retrieved sentences are assigned a score, based on the best similarity score
of all instances where they appeared.",3.4 Retrieval-Guided Code Generation,[0],[0]
"We normalize the scores for each input sentence by subtracting the average over the training dataset.
",3.4 Retrieval-Guided Code Generation,[0],[0]
"At decoding time, incorporate these retrievalderived scores into beam search: for a given time step, all actions that would result in one of the retrieved n-grams u to be in the prediction tree has its log probability log(p(yt | yt−11 )) increased by λ ∗ score(u) where λ is a hyperparameter, and score(u) is the maximal sim(q, qm) from which u is extracted.",3.4 Retrieval-Guided Code Generation,[0],[0]
The probability distribution is then renormalized.,3.4 Retrieval-Guided Code Generation,[0],[0]
"We evaluate RECODE with the Hearthstone (HS) (Ling et al., 2016) and Django (Oda et al., 2015) datasets, as preprocessed by Yin and Neubig (2017).",4 Datasets and Evaluation Metrics,[0],[0]
HS consists of Python classes that implement Hearthstone card descriptions while Django contains pairs of Python source code and English pseudo-code from Django web framework.,4 Datasets and Evaluation Metrics,[0],[0]
"Table 1 summarizes dataset statistics.
",4 Datasets and Evaluation Metrics,[0],[0]
"For evaluation metrics, we use accuracy of exact match and the BLEU score following Yin and Neubig (2017).",4 Datasets and Evaluation Metrics,[0],[0]
"For the neural code generation model, we use the settings explained in Yin and Neubig (2017).",5 Experiments,[0],[0]
"For the retrieval method, we tuned hyperparameters and achieved best result when we set nmax = 4 and λ = 3 for both datasets3.",5 Experiments,[0],[0]
"For HS, we set M = 3 and M = 10 for Django.
",5 Experiments,[0],[0]
"We compare our model with Yin and Neubig (2017)’s model that we call YN17 for brevity, and a sequence-to-sequence (SEQ2SEQ) model that we implemented.",5 Experiments,[0],[0]
"SEQ2SEQ is an attentionenabled encoder-decoder model (Bahdanau et al., 2015).",5 Experiments,[0],[0]
The encoder is a bidirectional LSTM and the decoder is an LSTM.,5 Experiments,[0],[0]
"Table 2 shows that RECODE outperforms the baselines in both BLEU and accuracy, providing ev-
3n-gram subtrees are collected up to nmax-gram
idence for the effectiveness of incorporating retrieval methods into tree-based approaches.
.
",5.1 Results,[0],[0]
"We ran statistical significance tests for RECODE and YN17, using bootstrap resampling with N = 10,000.",5.1 Results,[0],[0]
"For the BLEU scores of both datasets, p < 0.001.",5.1 Results,[0],[0]
"For the exact match accuracy, p < 0.001 for Django dataset, but for Hearthstone, p > 0.3, showing that the retrieval-based model is on par with YN17.",5.1 Results,[0],[0]
"It is worth noting, though, that HS consists of long and complex code, and that generating exact matches is very difficult, making exact match accuracy a less reliable metric.
",5.1 Results,[0],[0]
We also compare RECODE with Rabinovich et al. (2017)’s Abstract Syntax Networks with supervision (ASN+SUPATT) which is the state-of-the-art system for HS.,5.1 Results,[0],[0]
RECODE exceeds ASN without extra supervision though ASN+SUPATT has a slightly better result.,5.1 Results,[0],[0]
"However, ASN+SUPATT is trained with supervised attention extracted through heuristic exact word matches while our attention is unsupervised.",5.1 Results,[0],[0]
"From our observation and as mentioned in Rabinovich et al. (2017), HS contains classes with similar structure, so the code generation task could be simply matching the tree structure and filling the terminal tokens with correct variables and values.",5.2 Discussion and Analysis,[0],[0]
"However, when the code consists of complex logic, partial implementation errors occur, leading to low exact match accuracy (Yin and Neubig, 2017).",5.2 Discussion and Analysis,[0],[0]
"Analyzing our result, we find this intuition to be true not only for HS but also for Django.
",5.2 Discussion and Analysis,[0],[0]
"Examining the generated output for the Django dataset in Table 3, we can see that in the first example, our retrieval model can successfully generate the correct code when YN17 fails.",5.2 Discussion and Analysis,[0],[0]
This difference suggests that our retrieval model benefits from the action subtrees from the retrieved sentences.,5.2 Discussion and Analysis,[0],[0]
"In the second example, although our generated code does not perfectly match the reference code, it has a higher BLEU score compared
to the output of YN17 because our model can predict part of the code (timesince(d, now, reversed)) correctly.",5.2 Discussion and Analysis,[0],[0]
The third example shows where our method fails to apply the correct action as it cannot cast s to str type while YN17 can at least cast s into a type (bool).,5.2 Discussion and Analysis,[0],[0]
"Another common type of error that we found RECODE’s generated outputs is incorrect variable copying, similarly to what is discussed in Yin and Neubig (2017) and Rabinovich et al. (2017).
",5.2 Discussion and Analysis,[0],[0]
Table 4 presents a result on the HS dataset4.,5.2 Discussion and Analysis,[0],[0]
We can see that our retrieval model can handle complex code more effectively.,5.2 Discussion and Analysis,[0],[0]
"Several works on code generation focus on domain specific languages (Raza et al., 2015; Kushman and Barzilay, 2013).",6 Related Work,[0],[0]
"For general purpose code generation, some data-driven work has been
4More example of HS code is provided in the supplementary material.
done for predicting input parsers (Lei et al., 2013) or a set of relevant methods (Raghothaman et al., 2016).",6 Related Work,[0],[0]
"Some attempts using neural networks have used sequence-to-sequence models (Ling et al., 2016) or tree-based architectures (Dong and Lapata, 2016; Alvarez-Melis and Jaakkola, 2017).",6 Related Work,[0],[0]
Ling et al. (2016); Jia and Liang (2016); Locascio et al. (2016) treat semantic parsing as a sequence generation task by linearizing trees.,6 Related Work,[0],[0]
The closest work to ours are Yin and Neubig (2017) and Rabinovich et al. (2017) which represent code as an AST.,6 Related Work,[0],[0]
"Another close work is Dong and Lapata (2018), which uses a two-staged structure-aware neural architecture.",6 Related Work,[0],[0]
"They initially generate a lowlevel sketch and then fill in the missing information using the NL and the sketch.
",6 Related Work,[0],[0]
Recent works on retrieval-guided neural machine translation have been presented by Gu et al. (2018); Amin Farajian et al. (2017); Li et al. (2018); Zhang et al. (2018).,6 Related Work,[0],[0]
Gu et al. (2018) use the retrieved sentence pairs as extra inputs to the NMT model.,6 Related Work,[0],[0]
Zhang et al. (2018) employ a simpler and faster retrieval method to guide neural MT where translation pieces are n-grams from retrieved target sentences.,6 Related Work,[0],[0]
"We modify Zhang et al. (2018)’s method from textual n-grams to n-grams over subtrees to exploit the code structural similarity, and propose methods to deal with complex statements and rare words.
",6 Related Work,[0],[0]
"In addition, some previous works have used subtrees in structured prediction tasks.",6 Related Work,[0],[0]
"For example, Galley et al. (2006) used them in syntaxbased translation models.",6 Related Work,[0],[0]
"In Galley et al. (2006), subtrees of the input sentence’s parse tree are associated with corresponding words in the output sentence.",6 Related Work,[0],[0]
We proposed an action subtree retrieval method at test time on top of an AST-driven neural model for generating general-purpose code.,7 Conclusion,[0],[0]
"The predicted surface code is syntactically correct, and the retrieval component improves the performance of a previously state-of-the-art model.",7 Conclusion,[0],[0]
Our successful result suggests that our idea of retrieval-based generation can be potentially applied to other treestructured prediction tasks.,7 Conclusion,[0],[0]
"We are grateful to Lucile Callebert for insightful discussions, Aldrian Obaja Muis for helpful
input on early version writing, and anonymous reviewers for useful feedback.",Acknowledgements,[0],[0]
This material is based upon work supported by the National Science Foundation under Grant No. 1815287.,Acknowledgements,[0],[0]
"In models to generate program source code from natural language, representing this code in a tree structure has been a common approach.",abstractText,[0],[0]
"However, existing methods often fail to generate complex code correctly due to a lack of ability to memorize large and complex structures.",abstractText,[0],[0]
"We introduce RECODE, a method based on subtree retrieval that makes it possible to explicitly reference existing code examples within a neural code generation model.",abstractText,[0],[0]
"First, we retrieve sentences that are similar to input sentences using a dynamicprogramming-based sentence similarity scoring method.",abstractText,[0],[0]
"Next, we extract n-grams of action sequences that build the associated abstract syntax tree.",abstractText,[0],[0]
"Finally, we increase the probability of actions that cause the retrieved n-gram action subtree to be in the predicted code.",abstractText,[0],[0]
We show that our approach improves the performance on two code generation tasks by up to +2.6 BLEU.1,abstractText,[0],[0]
Retrieval-Based Neural Code Generation,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 152–161 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
152",text,[0],[0]
The exponentially growing online information has necessitated the development of effective automatic summarization systems.,1 Introduction,[0],[0]
"In this paper, we focus on an increasingly intriguing task, i.e., abstractive sentence summarization (Rush et al., 2015a), which generates a shorter version of a given sentence while attempting to preserve its original meaning.",1 Introduction,[0],[0]
It can be used to design or refine appealing headlines.,1 Introduction,[0],[0]
"Recently, the application of the attentional sequence-to-sequence (seq2seq) framework has attracted growing attention and achieved state-of-the-art performance on this task (Rush et al., 2015a; Chopra et al., 2016; Nallapati et al., 2016).
",1 Introduction,[0],[0]
Most previous seq2seq models purely depend on the source text to generate summaries.,1 Introduction,[0],[0]
"However, as reported in many studies (Koehn and Knowles, 2017), the performance of a seq2seq model deteriorates quickly with the increase of the length of generation.",1 Introduction,[0],[0]
Our experiments also show that seq2seq models tend to “lose control” sometimes.,1 Introduction,[0],[0]
"For example, 3% of summaries contain less than 3 words, while there are 4 summaries repeating a word for even 99 times.",1 Introduction,[0],[0]
These results largely reduce the informativeness and readability of the generated summaries.,1 Introduction,[0],[0]
"In addition, we find seq2seq models usually focus on copying source words in order, without any actual “summarization”.",1 Introduction,[0],[0]
"Therefore, we argue that, the free generation based on the source sentence is not enough for a seq2seq model.
",1 Introduction,[0],[0]
"Template based summarization (e.g., Zhou and Hovy (2004)) is a traditional approach to abstractive summarization.",1 Introduction,[0],[0]
"In general, a template is an incomplete sentence which can be filled with the input text using the manually defined rules.",1 Introduction,[0],[0]
"For instance, a concise template to conclude the stock market quotation is: [REGION] shares [open/close]",1 Introduction,[0],[0]
"[NUMBER] percent [lower/higher], e.g., “hong kong shares close #.",1 Introduction,[0],[0]
# percent lower”.,1 Introduction,[0],[0]
"Since the templates are written by humans, the produced summaries are usually fluent and informative.",1 Introduction,[0],[0]
"However, the construction of templates is extremely time-consuming and requires a plenty of domain knowledge.",1 Introduction,[0],[0]
"Moreover, it is impossible to develop all templates for summaries in various domains.
",1 Introduction,[0],[0]
"Inspired by retrieve-based conversation systems (Ji et al., 2014), we assume the golden summaries of the similar sentences can provide a reference point to guide the input sentence summarization process.",1 Introduction,[0],[0]
"We call these existing summaries soft templates since no actual rules are nee-
ded to build new summaries from them.",1 Introduction,[0],[0]
"Due to the strong rewriting ability of the seq2seq framework (Cao et al., 2017a), in this paper, we propose to combine the seq2seq and template based summarization approaches.",1 Introduction,[0],[0]
"We call our summarization system Re3Sum, which consists of three modules: Retrieve, Rerank and Rewrite.",1 Introduction,[0],[0]
We utilize a widely-used Information Retrieval (IR) platform to find out candidate soft templates from the training corpus.,1 Introduction,[0],[0]
"Then, we extend the seq2seq model to jointly learn template saliency measurement (Rerank) and final summary generation (Rewrite).",1 Introduction,[0],[0]
"Specifically, a Recurrent Neural Network (RNN) encoder is applied to convert the input sentence and each candidate template into hidden states.",1 Introduction,[0],[0]
"In Rerank, we measure the informativeness of a candidate template according to its hidden state relevance to the input sentence.",1 Introduction,[0],[0]
The candidate template with the highest predicted informativeness is regarded as the actual soft template.,1 Introduction,[0],[0]
"In Rewrite, the summary is generated according to the hidden states of both the sentence and template.
",1 Introduction,[0],[0]
"We conduct extensive experiments on the popular Gigaword dataset (Rush et al., 2015b).",1 Introduction,[0],[0]
"Experiments show that, in terms of informativeness, Re3Sum significantly outperforms the state-ofthe-art seq2seq models, and even soft templates themselves demonstrate high competitiveness.",1 Introduction,[0],[0]
"In addition, the import of high-quality external summaries improves the stability and readability of generated summaries.
",1 Introduction,[0],[0]
"The contributions of this work are summarized as follows:
• We propose to introduce soft templates as additional input to improve the readability and stability of seq2seq summarization systems.",1 Introduction,[0],[0]
Code and results can be found at http://www4.comp.polyu.,1 Introduction,[0],[0]
"edu.hk/˜cszqcao/
• We extend the seq2seq framework to conduct template reranking and template-aware summary generation simultaneously.
",1 Introduction,[0],[0]
"• We fuse the popular IR-based and seq2seqbased summarization systems, which fully utilize the supervisions from both sides.",1 Introduction,[0],[0]
"As shown in Fig. 1, our summarization system consists of three modules, i.e., Retrieve, Rerank
and Rewrite.",2 Method,[0],[0]
"Given the input sentence x, the Retrieve module filters candidate soft templates C = {ri} from the training corpus.",2 Method,[0],[0]
"For validation and test, we regard the candidate template with the highest predicted saliency (a.k.a informativeness) score as the actual soft template r. For training, we choose the one with the maximal actual saliency score in C, which speeds up convergence and shows no obvious side effect in the experiments.
",2 Method,[0],[0]
"Then, we jointly conduct reranking and rewriting through a shared encoder.",2 Method,[0],[0]
"Specifically, both the sentence x and the soft template r are converted into hidden states with a RNN encoder.",2 Method,[0],[0]
"In the Rerank module, we measure the saliency of r according to its hidden state relevance to x.",2 Method,[0],[0]
"In the Rewrite module, a RNN decoder combines the hidden states of x and r to generate a summary y. More details will be described in the rest of this section",2 Method,[0],[0]
The purpose of this module is to find out candidate templates from the training corpus.,2.1 Retrieve,[0],[0]
We assume that similar sentences should hold similar summary patterns.,2.1 Retrieve,[0],[0]
"Therefore, given a sentence x, we find out its analogies in the corpus and pick their summaries as the candidate templates.",2.1 Retrieve,[0],[0]
"Since the size of our dataset is quite large (over 3M), we leverage the widely-used Information Retrieve (IR) system Lucene1 to index and search efficiently.",2.1 Retrieve,[0],[0]
We keep the default settings of Lucene2 to build the IR system.,2.1 Retrieve,[0],[0]
"For each input sentence, we select top 30 searching results as candidate templates.",2.1 Retrieve,[0],[0]
"To conduct template-aware seq2seq generation (rewriting), it is a necessary step to encode both the source sentence x and soft template r into hidden states.",2.2 Jointly Rerank and Rewrite,[0],[0]
"Considering that the matching networks based on hidden states have demonstrated the strong ability to measure the relevance of two pieces of texts (e.g., Chen et al. (2016)), we propose to jointly conduct reranking and rewriting through a shared encoding step.",2.2 Jointly Rerank and Rewrite,[0],[0]
"Specifically, we employ a bidirectional Recurrent Neural Network (BiRNN) encoder (Cho et al., 2014) to read x and r. Take the sentence x as an example.",2.2 Jointly Rerank and Rewrite,[0],[0]
"Its hidden state of the forward RNN at timestamp i can be
1https://lucene.apache.org/ 2TextField with EnglishAnalyzer
represented by:
−→ h xi = RNN(xi, −→ h xi−1) (1)
The BiRNN consists of a forward RNN and a backward RNN.",2.2 Jointly Rerank and Rewrite,[0],[0]
"Suppose the corresponding outputs are [ −→ h x1 ; · · · ; −→ h x−1] and [ ←− h x1 ; · · · ; ←− h x−1], respectively, where the index “−1” stands for the last element.",2.2 Jointly Rerank and Rewrite,[0],[0]
"Then, the composite hidden state of a word is the concatenation of the two RNN representations, i.e., hxi =",2.2 Jointly Rerank and Rewrite,[0],[0]
[ −→ h xi ; ←− h xi ].,2.2 Jointly Rerank and Rewrite,[0],[0]
The entire representation for the source sentence is [hx1 ; · · · ;hx−1].,2.2 Jointly Rerank and Rewrite,[0],[0]
"Since a soft template r can also be regarded as a readable concise sentence, we use the same BiRNN encoder to convert it into hidden states [hr1; · · · ;hr−1].",2.2 Jointly Rerank and Rewrite,[0],[0]
"In Retrieve, the template candidates are ranked according to the text similarity between the corresponding indexed sentences and the input sentence.",2.2.1 Rerank,[0],[0]
"However, for the summarization task, we expect the soft template r resembles the actual summary y∗ as much as possible.",2.2.1 Rerank,[0],[0]
"Here we use the widely-used summarization evaluation metrics ROUGE (Lin, 2004) to measure the actual saliency s∗(r,y∗) (see Section 3.2).",2.2.1 Rerank,[0],[0]
We utilize the hidden states of x and r to predict the saliency s of the template.,2.2.1 Rerank,[0],[0]
"Specifically, we regard the output of the BiRNN as the representation of the sentence or template:
hx =",2.2.1 Rerank,[0],[0]
[ ←− h x1 ; −→ h x−1] (2) hr =,2.2.1 Rerank,[0],[0]
[ ←− h r1; −→ h r−1],2.2.1 Rerank,[0],[0]
"(3)
Next, we use Bilinear network to predict the saliency of the template for the input sentence.
",2.2.1 Rerank,[0],[0]
"s(r,x) = sigmoid(hrWshTx + bs), (4)
where Ws and bs are parameters of the Bilinear network, and we add the sigmoid activation function to make the range of s consistent with the actual saliency s∗.",2.2.1 Rerank,[0],[0]
"According to Chen et al. (2016), Bilinear outperforms multi-layer forward
neural networks in relevance measurement.",2.2.1 Rerank,[0],[0]
"As shown later, the difference of s and s∗ will provide additional supervisions for the seq2seq framework.",2.2.1 Rerank,[0],[0]
The soft template r selected by the Rerank module has already competed with the state-of-the-art method in terms of ROUGE evaluation (see Table 4).,2.2.2 Rewrite,[0],[0]
"However, r usually contains a lot of named entities that does not appear in the source (see Table 5).",2.2.2 Rewrite,[0],[0]
"Consequently, it is hard to ensure that the soft templates are faithful to the input sentences.",2.2.2 Rewrite,[0],[0]
"Therefore, we leverage the strong rewriting ability of the seq2seq model to generate more faithful and informative summaries.",2.2.2 Rewrite,[0],[0]
"Specifically, since the input of our system consists of both the sentence and soft template, we use the concatenation function3 to combine the hidden states of the sentence and template:
Hc =",2.2.2 Rewrite,[0],[0]
"[h x 1 ; · · · ;hx−1;hr1; · · · ;hr−1] (5)
",2.2.2 Rewrite,[0],[0]
"The combined hidden states are fed into the prevailing attentional RNN decoder (Bahdanau et al., 2014) to generate the decoding hidden state at the position t:
st = Att-RNN(st−1, yt−1,Hc), (6)
where yt−1 is the previous output summary word.",2.2.2 Rewrite,[0],[0]
"Finally, a softmax layer is introduced to predict the current summary word:
ot = softmax(stWo), (7)
where Wo is a parameter matrix.",2.2.2 Rewrite,[0],[0]
There are two types of costs in our system.,2.3 Learning,[0],[0]
"For Rerank, we expect the predicted saliency s(r,x) close to the actual saliency s∗(r,y∗).",2.3 Learning,[0],[0]
"Therefore,
3We also attempted complex combination approaches such as the gate network Cao et al. (2017b) but failed to achieve obvious improvement.",2.3 Learning,[0],[0]
"We assume the Rerank module has partially played the role of the gate network.
",2.3 Learning,[0],[0]
"we use the cross entropy (CE) between s and s∗ as the loss function:
JR(θ) = CE(s(r,x), s ∗(r,y∗)) (8)
= −s∗ log s− (1− s∗) log(1− s),
where θ stands for the model parameters.",2.3 Learning,[0],[0]
"For Rewrite, the learning goal is to maximize the estimated probability of the actual summary y∗.",2.3 Learning,[0],[0]
"We adopt the common negative log-likelihood (NLL) as the loss function:
JG(θ)",2.3 Learning,[0],[0]
=,2.3 Learning,[0],[0]
"− log(p(y∗|x, r))",2.3 Learning,[0],[0]
(9) =,2.3 Learning,[0],[0]
"− ∑
t log(ot[y
∗ t ])
To make full use of supervisions from both sides, we combine the above two costs as the final loss function:
J(θ) = JR(θ)",2.3 Learning,[0],[0]
+ JG(θ),2.3 Learning,[0],[0]
"(10)
We use mini-batch Stochastic Gradient Descent (SGD) to tune model parameters.",2.3 Learning,[0],[0]
The batch size is 64.,2.3 Learning,[0],[0]
"To enhance generalization, we introduce dropout (Srivastava et al., 2014) with probability p = 0.3 for the RNN layers.",2.3 Learning,[0],[0]
"The initial learning rate is 1, and it will decay by 50% if the generation loss does not decrease on the validation set.",2.3 Learning,[0],[0]
"We conduct experiments on the Annotated English Gigaword corpus, as with (Rush et al., 2015b).",3.1 Datasets,[0],[0]
This parallel corpus is produced by pairing the first sentence in the news article and its headline as the summary with heuristic rules.,3.1 Datasets,[0],[0]
"All the training, development and test datasets can be downloaded at https://github.",3.1 Datasets,[0],[0]
com/harvardnlp/sent-summary.,3.1 Datasets,[0],[0]
The statistics of the Gigaword corpus is presented in Table 1.,3.1 Datasets,[0],[0]
"We adopt ROUGE (Lin, 2004) for automatic evaluation.",3.2 Evaluation Metrics,[0],[0]
ROUGE has been the standard evaluation metric for DUC shared tasks since 2004.,3.2 Evaluation Metrics,[0],[0]
"It measures the quality of summary by computing the overlapping lexical units between the candidate summary and actual summaries, such as unigram, bi-gram and longest common subsequence (LCS).",3.2 Evaluation Metrics,[0],[0]
"Following the common practice, we report ROUGE-1 (uni-gram), ROUGE-2 (bi-gram) and ROUGE-L (LCS) F1 scores4 in the following experiments.",3.2 Evaluation Metrics,[0],[0]
"We also measure the actual saliency of a candidate template r with its combined ROUGE scores given the actual summary y∗:
s∗(r,y∗) = RG(r,y∗) +",3.2 Evaluation Metrics,[0],[0]
"RG(r,y∗), (11)
where “RG” stands for ROUGE for short.",3.2 Evaluation Metrics,[0],[0]
ROUGE mainly evaluates informativeness.,3.2 Evaluation Metrics,[0],[0]
"We also introduce a series of metrics to measure the summary quality from the following aspects: LEN DIF The absolute value of the length diffe-
rence between the generated summaries and the actual summaries.",3.2 Evaluation Metrics,[0],[0]
We use mean value ± standard deviation to illustrate this item.,3.2 Evaluation Metrics,[0],[0]
"The average value partially reflects the readability and informativeness, while the standard deviation links to stability.
",3.2 Evaluation Metrics,[0],[0]
"4We use the ROUGE evaluation option: -m -n 2 -w 1.2
LESS 3",3.2 Evaluation Metrics,[0],[0]
"The number of the generated summaries, which contains less than three tokens.",3.2 Evaluation Metrics,[0],[0]
These extremely short summaries are usually unreadable.,3.2 Evaluation Metrics,[0],[0]
COPY,3.2 Evaluation Metrics,[0],[0]
The proportion of the summary words (without stopwords) copied from the source sentence.,3.2 Evaluation Metrics,[0],[0]
A seriously large copy ratio indicates that the summarization system pays more attention to compression rather than required abstraction.,3.2 Evaluation Metrics,[0],[0]
NEW NE,3.2 Evaluation Metrics,[0],[0]
The number of the named entities that do not appear in the source sentence or actual summary.,3.2 Evaluation Metrics,[0],[0]
"Intuitively, the appearance of new named entities in the summary is likely to bring unfaithfulness.",3.2 Evaluation Metrics,[0],[0]
We use Stanford CoreNLP,3.2 Evaluation Metrics,[0],[0]
"(Manning et al., 2014) to recognize named entities.",3.2 Evaluation Metrics,[0],[0]
We use the popular seq2seq framework OpenNMT5 as the starting point.,3.3 Implementation Details,[0],[0]
"To make our model more general, we retain the default settings of OpenNMT to build the network architecture.",3.3 Implementation Details,[0],[0]
"Specifically, the dimensions of word embeddings and RNN are both 500, and the encoder and decoder structures are two-layer bidirectional Long Short Term Memory Networks (LSTMs).",3.3 Implementation Details,[0],[0]
The only difference is that we add the argument “- share embeddings” to share the word embeddings between the encoder and decoder.,3.3 Implementation Details,[0],[0]
This practice largely reduces model parameters for the monolingual task.,3.3 Implementation Details,[0],[0]
"On our computer (GPU: GTX 1080, Memory: 16G, CPU: i7-7700K), the training spends about 2 days.
",3.3 Implementation Details,[0],[0]
"During test, we use beam search of size 5 to generate summaries.",3.3 Implementation Details,[0],[0]
We add the argument “- replace unk” to replace the generated unknown words with the source word that holds the highest attention weight.,3.3 Implementation Details,[0],[0]
"Since the generated summaries are often shorter than the actual ones, we introduce an additional length penalty argument “- alpha 1” to encourage longer generation, like Wu et al. (2016).",3.3 Implementation Details,[0],[0]
"We compare our proposed model with the following state-of-the-art neural summarization systems: ABS Rush et al. (2015a) used an attentive CNN
encoder and a NNLM decoder to summarize 5https://github.com/OpenNMT/OpenNMT-py
the sentence.",3.4 Baselines,[0],[0]
ABS+,3.4 Baselines,[0],[0]
"Rush et al. (2015a) further tuned the ABS
model with additional hand-crafted features to balance between abstraction and extraction.
",3.4 Baselines,[0],[0]
"RAS-Elman As the extension of the ABS model, it used a convolutional attention-based encoder and a RNN decoder (Chopra et al., 2016).",3.4 Baselines,[0],[0]
"Featseq2seq Nallapati et al. (2016) used a complete seq2seq RNN model and added the hand-crafted features such as POS tag and NER, to enhance the encoder representation.",3.4 Baselines,[0],[0]
Luong-NMT Chopra et al. (2016) implemented the neural machine translation model of Luong et al. (2015) for summarization.,3.4 Baselines,[0],[0]
This model contained two-layer LSTMs with 500 hidden units in each layer.,3.4 Baselines,[0],[0]
OpenNMT,3.4 Baselines,[0],[0]
We also implement the standard attentional seq2seq model with OpenNMT.,3.4 Baselines,[0],[0]
All the settings are the same as our system.,3.4 Baselines,[0],[0]
It is noted that OpenNMT officially examined the Gigaword dataset.,3.4 Baselines,[0],[0]
We distinguish the official result6 and our experimental result with suffixes “O” and “I” respectively.,3.4 Baselines,[0],[0]
"FTSum Cao et al. (2017b) encoded the facts extracted from the source sentence to improve both the faithfulness and informativeness of generated summaries.
",3.4 Baselines,[0],[0]
"In addition, to evaluate the effectiveness of our joint learning framework, we develop a baseline named “PIPELINE”.",3.4 Baselines,[0],[0]
Its architecture is identical to Re3Sum.,3.4 Baselines,[0],[0]
"However, it trains the Rerank module and Rewrite module in pipeline.",3.4 Baselines,[0],[0]
Let’s first look at the final cost values (Eq. 9) on the development set.,3.5 Informativeness Evaluation,[0],[0]
"From Table 2, we can
6http://opennmt.net/Models/
see that our model achieves much lower perplexity compared against the state-of-the-art systems.",3.5 Informativeness Evaluation,[0],[0]
It is also noted that PIPELINE slightly outperforms Re3Sum.,3.5 Informativeness Evaluation,[0],[0]
"One possible reason is that Re3Sum additionally considers the cost derived from the Rerank module.
",3.5 Informativeness Evaluation,[0],[0]
The ROUGE F1 scores of different methods are then reported in Table 3.,3.5 Informativeness Evaluation,[0],[0]
"As can be seen, our model significantly outperforms most other approaches.",3.5 Informativeness Evaluation,[0],[0]
"Note that, ABS+ and Featseq2seq have utilized a series of hand-crafted features, but our model is completely data-driven.",3.5 Informativeness Evaluation,[0],[0]
"Even though, our model surpasses Featseq2seq by 22% and ABS+ by 60% on ROUGE-2.",3.5 Informativeness Evaluation,[0],[0]
"When soft templates are ignored, our model is equivalent to the standard at-
tentional seq2seq model OpenNMTI .",3.5 Informativeness Evaluation,[0],[0]
"Therefore, it is safe to conclude that soft templates have great contribute to guide the generation of summaries.
",3.5 Informativeness Evaluation,[0],[0]
We also examine the performance of directly regarding soft templates as output summaries.,3.5 Informativeness Evaluation,[0],[0]
We introduce five types of different soft templates:,3.5 Informativeness Evaluation,[0],[0]
Random An existing summary randomly selected from the training corpus.,3.5 Informativeness Evaluation,[0],[0]
First The top-ranked candidate template given by the Retrieve module.,3.5 Informativeness Evaluation,[0],[0]
"Max The template with the maximal actual
ROUGE scores among the 30 candidate templates.
",3.5 Informativeness Evaluation,[0],[0]
Optimal An existing summary in the training corpus which holds the maximal ROUGE scores.,3.5 Informativeness Evaluation,[0],[0]
Rerank The template with the maximal predicted ROUGE scores among the 30 candidate templates.,3.5 Informativeness Evaluation,[0],[0]
"It is the actual soft template we adopt.
",3.5 Informativeness Evaluation,[0],[0]
"As shown in Table 4, the performance of Random is terrible, indicating it is impossible to use one summary template to fit various actual summaries.",3.5 Informativeness Evaluation,[0],[0]
"Rerank largely outperforms First, which verifies the effectiveness of the Rerank module.",3.5 Informativeness Evaluation,[0],[0]
"However, according to Max and Rerank, we find the Rerank performance of Re3Sum is far from perfect.",3.5 Informativeness Evaluation,[0],[0]
"Likewise, comparing Max and First, we observe that the improving capacity of the Retrieve module is high.",3.5 Informativeness Evaluation,[0],[0]
Notice that Optimal greatly exceeds all the state-of-the-art approaches.,3.5 Informativeness Evaluation,[0],[0]
This finding strongly supports our practice of using existing summaries to guide the seq2seq models.,3.5 Informativeness Evaluation,[0],[0]
"We also measure the linguistic quality of generated summaries from various aspects, and the results are present in Table 5.",3.6 Linguistic Quality Evaluation,[0],[0]
"As can be seen from the rows “LEN DIF” and “LESS 3”, the performance of Re3Sum is almost the same as that of soft templates.",3.6 Linguistic Quality Evaluation,[0],[0]
The soft templates indeed well guide the summary generation.,3.6 Linguistic Quality Evaluation,[0],[0]
"Compared with
Re3Sum, the standard deviation of LEN DF is 0.7 times larger in OpenNMT, indicating that OpenNMT works quite unstably.",3.6 Linguistic Quality Evaluation,[0],[0]
"Moreover, OpenNMT generates 53 extreme short summaries, which seriously reduces readability.",3.6 Linguistic Quality Evaluation,[0],[0]
"Meanwhile, the copy ratio of actual summaries is 36%.",3.6 Linguistic Quality Evaluation,[0],[0]
"Therefore, the copy mechanism is severely overweighted in OpenNMT.",3.6 Linguistic Quality Evaluation,[0.9531000877937863],"['Thus, it is ignored in the comparison.']"
"Our model is encouraged to generate according to human-written soft templates, which relatively diminishes copying from the source sentences.",3.6 Linguistic Quality Evaluation,[0],[0]
Look at the last row “NEW NE”.,3.6 Linguistic Quality Evaluation,[0],[0]
"A number of new named entities appear in the soft templates, which makes them quite unfaithful to source sentences.",3.6 Linguistic Quality Evaluation,[0],[0]
"By contrast, this index in Re3Sum is close to the OpenNMT’s.",3.6 Linguistic Quality Evaluation,[0],[0]
It highlights the rewriting ability of our seq2seq framework.,3.6 Linguistic Quality Evaluation,[0],[0]
"In this section, we investigate how soft templates affect our model.",3.7 Effect of Templates,[0],[0]
"At the beginning, we feed different types of soft templates (refer to Table 4) into the Rewriting module of Re3Sum.",3.7 Effect of Templates,[0],[0]
"As illustrated in Table 6, the more high-quality templates are provided, the higher ROUGE scores are achieved.",3.7 Effect of Templates,[0],[0]
"It is interesting to see that,while the ROUGE-2 score of Random templates is zero, our model can still generate acceptable summaries with Random templates.",3.7 Effect of Templates,[0],[0]
It seems that Re3Sum can automatically judge whether the soft templates are trustworthy and ignore the seriously irrelevant ones.,3.7 Effect of Templates,[0],[0]
"We believe that the joint learning with the Rerank model plays a vital role here.
",3.7 Effect of Templates,[0],[0]
"Next, we manually inspect the summaries generated by different methods.",3.7 Effect of Templates,[0],[0]
"We find the outputs of Re3Sum are usually longer and more flu-
ent than the outputs of OpenNMT.",3.7 Effect of Templates,[0],[0]
Some illustrative examples are shown in Table 7.,3.7 Effect of Templates,[0],[0]
"In Example 1, there is no predicate in the source sentence.",3.7 Effect of Templates,[0],[0]
"Since OpenNMT prefers selecting source words around the predicate to form the summary, it fails on this sentence.",3.7 Effect of Templates,[0],[0]
"By contract, Re3Sum rewrites the template and produces an informative summary.",3.7 Effect of Templates,[0],[0]
"In Example 2, OpenNMT deems the starting part of the sentences are more important, while our model, guided by the template, focuses on the second part to generate the summary.
",3.7 Effect of Templates,[0],[0]
"In the end, we test the ability of our model to generate diverse summaries.",3.7 Effect of Templates,[0],[0]
"In practice, a system that can provide various candidate summaries is probably more welcome.",3.7 Effect of Templates,[0],[0]
"Specifically, two candidate templates with large text dissimilarity are manually fed into the Rewriting module.",3.7 Effect of Templates,[0],[0]
The corresponding generated summaries are shown in Table 8.,3.7 Effect of Templates,[0],[0]
"For the sake of comparison, we also present the 2-best results of OpenNMT with beam search.",3.7 Effect of Templates,[0],[0]
"As can be seen, with different templates given, our model is likely to generate dissimilar summaries.",3.7 Effect of Templates,[0],[0]
"In contrast, the 2-best results of OpenNMT is almost the same, and often a shorter summary is only a piece of the other one.",3.7 Effect of Templates,[0],[0]
"To sum up, our model demonstrates promising prospect in generation diversity.",3.7 Effect of Templates,[0],[0]
"Abstractive sentence summarization aims to produce a shorter version of a given sentence while preserving its meaning (Chopra et al., 2016).",4 Related Work,[0],[0]
"This task is similar to text simplification (Saggion, 2017) and facilitates headline design and refine.",4 Related Work,[0],[0]
"Early studies on sentence summariza-
tion include template-based methods (Zhou and Hovy, 2004), syntactic tree pruning (Knight and Marcu, 2002; Clarke and Lapata, 2008) and statistical machine translation techniques (Banko et al., 2000).",4 Related Work,[0],[0]
"Recently, the application of the attentional seq2seq framework has attracted growing attention and achieved state-of-the-art performance on this task (Rush et al., 2015a; Chopra et al., 2016; Nallapati et al., 2016).
",4 Related Work,[0],[0]
"In addition to the direct application of the general seq2seq framework, researchers attempted to integrate various properties of summarization.",4 Related Work,[0],[0]
"For example, Nallapati et al. (2016) enriched the encoder with hand-crafted features such as named entities and POS tags.",4 Related Work,[0],[0]
These features have played important roles in traditional feature based summarization systems.,4 Related Work,[0],[0]
Gu et al. (2016) found that a large proportion of the words in the summary were copied from the source text.,4 Related Work,[0],[0]
"Therefore, they proposed CopyNet which considered the copying mechanism during generation.",4 Related Work,[0],[0]
"Recently, See et al. (2017) used the coverage mechanism to discourage repetition.",4 Related Work,[0],[0]
Cao et al. (2017b) encoded facts extracted from the source sentence to enhance the summary faithfulness.,4 Related Work,[0],[0]
There were also studies to modify the loss function to fit the evaluation metrics.,4 Related Work,[0],[0]
"For instance, Ayana et al. (2016) applied the Minimum Risk Training strategy to maximize the ROUGE scores of generated sum-
maries.",4 Related Work,[0],[0]
"Paulus et al. (2017) used the reinforcement learning algorithm to optimize a mixed objective function of likelihood and ROUGE scores.
",4 Related Work,[0],[0]
Guu et al. (2017) also proposed to encode human-written sentences to improvement the performance of neural text generation.,4 Related Work,[0],[0]
"However, they handled the task of Language Modeling and randomly picked an existing sentence in the training corpus.",4 Related Work,[0],[0]
"In comparison, we develop an IR system to find proper existing summaries as soft templates.",4 Related Work,[0],[0]
"Moreover, Guu et al. (2017) used a general seq2seq framework while we extend the seq2seq framework to conduct template reranking and template-aware summary generation simultaneously.",4 Related Work,[0],[0]
This paper proposes to introduce soft templates as additional input to guide the seq2seq summarization.,5 Conclusion and Future Work,[0],[0]
We use the popular IR platform Lucene to retrieve proper existing summaries as candidate soft templates.,5 Conclusion and Future Work,[0],[0]
Then we extend the seq2seq framework to jointly conduct template reranking and template-aware summary generation.,5 Conclusion and Future Work,[0],[0]
"Experiments show that our model can generate informative, readable and stable summaries.",5 Conclusion and Future Work,[0],[0]
"In addition, our model demonstrates promising prospect in generation diversity.
",5 Conclusion and Future Work,[0],[0]
"We believe our work can be extended in vari-
ous aspects.",5 Conclusion and Future Work,[0],[0]
"On the one hand, since the candidate templates are far inferior to the optimal ones, we intend to improve the Retrieve module, e.g., by indexing both the sentence and summary fields.",5 Conclusion and Future Work,[0],[0]
"On the other hand, we plan to test our system on the other tasks such as document-level summarization and short text conversation.
",5 Conclusion and Future Work,[0],[0]
"Acknowledgments
The work described in this paper was supported by Research Grants Council of Hong Kong (PolyU 152036/17E), National Natural Science Foundation of China (61672445 and 61572049) and The Hong Kong Polytechnic University (G-YBP6, 4- BCDV).",5 Conclusion and Future Work,[0],[0]
"Most previous seq2seq summarization systems purely depend on the source text to generate summaries, which tends to work unstably.",abstractText,[0],[0]
"Inspired by the traditional template-based summarization approaches, this paper proposes to use existing summaries as soft templates to guide the seq2seq model.",abstractText,[0],[0]
"To this end, we use a popular IR platform to Retrieve proper summaries as candidate templates.",abstractText,[0],[0]
"Then, we extend the seq2seq framework to jointly conduct template Reranking and templateaware summary generation (Rewriting).",abstractText,[0],[0]
"Experiments show that, in terms of informativeness, our model significantly outperforms the state-of-the-art methods, and even soft templates themselves demonstrate high competitiveness.",abstractText,[0],[0]
"In addition, the import of high-quality external summaries improves the stability and readability of generated summaries.",abstractText,[0],[0]
"Retrieve, Rerank and Rewrite: Soft Template Based Neural Summarization",title,[0],[0]
Revealing common statistical behaviors among a group of subjects is fundamental to neuroscience and bio-medical data analysis.,1. Introduction,[0],[0]
"For example, in functional magnetic resonance imaging (fMRI) research (Bullmore et al., 1996; Smith et al., 2011; Varoquaux & Craddock, 2013), group level analyses are used for detecting brain networks from resting-state recordings (Fox et al., 2005), for detecting activities of specific regions in response to various stimuli (Haxby et al., 2001), for studying the connectivity of a specific brain region to other regions through seed based
1Electrical Engineering Dept., Technion, Israel.",1. Introduction,[0],[0]
"Correspondence to: Andrey Zhitnikov <andreyz@campus.technion.ac.il>, Rotem Mulayoff <smulayof@campus.technion.ac.il>, Tomer Michaeli <tomer.m@ee.technion.ac.il>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"analysis (Hagler et al., 2006), etc.",1. Introduction,[0],[0]
Group analyses often rely on the assumption that all subjects in the group behave according to the same statistical model.,1. Introduction,[0],[0]
"For example, to estimate the covariance (or partial covariance) matrix of several variables, a popular approach is to average the covariance matrices estimated for each of the individual subjects in the group (Power et al., 2011).",1. Introduction,[0],[0]
"This is done using either the Euclidean mean (arithmetic average) or the intrinsic (Riemannian) mean (Förstner & Moonen, 2003), (Fletcher & Joshi, 2007), which respects the geometry of the manifold of positive definite matrices (Varoquaux et al., 2010a).
",1. Introduction,[0],[0]
"Real data, however, rarely conform to this assumption.",1. Introduction,[0],[0]
"Often times, each subject in a group deviates from the common model in a different way.",1. Introduction,[0],[0]
"For example, it has been shown that estimates of connectivity patterns from fMRI scans, tend to vary significantly between subjects (Moussa et al., 2012).",1. Introduction,[0],[0]
Subject-specific deviations may even be more dominant than the common model itself.,1. Introduction,[0],[0]
"Therefore, if ignored, these deviations may severely degrade the quality of the estimate of the common model.",1. Introduction,[0],[0]
This phenomenon is illustrated in Fig. 1 in the context of nonparametric density estimation of two variables (brain regions).,1. Introduction,[0],[0]
"In this example, the deviations from the common model are additive and have a different distribution for each subject.",1. Introduction,[0],[0]
"Thus, as can be seen on the right, kernel density estimation (KDE) applied to the entire group, fails to reveal the common behavior.
",1. Introduction,[0],[0]
Approaches for accounting for subject-specific deviations often make limiting assumptions.,1. Introduction,[0],[0]
"For example, in the context of covariance estimation, (Varoquaux et al., 2010b) assumed that the precision matrices of all subjects in the group have the same sparsity pattern, and proposed a modified graph Lasso technique (Friedman et al., 2008) for simultaneously estimating those matrices.",1. Introduction,[0],[0]
"In (Marrelec et al., 2006), the authors assumed that each subject’s samples follow a Gaussian distribution with a covariance matrix that follows an inverse Wishart distribution around the group covariance.",1. Introduction,[0],[0]
"In the context of regression, a popular strategy is to use a linear mixed-effects model (Friston et al., 2005; Chen et al., 2013), which relies on a Gaussian distribution assumption for the subject specific factors.",1. Introduction,[0],[0]
"Similar lines of work include grouplevel independent component analysis (ICA) (Calhoun et al., 2001; Beckmann & Smith, 2005; Varoquaux et al., 2010c), dictionary learning (Varoquaux et al., 2011; Mensch et al., 2016), and causal structure estimation (Ramsey et al., 2010).
",1. Introduction,[0],[0]
In this paper we present non-parametric methods for estimating a common model in the presence of subject-specific noise factors.,1. Introduction,[0],[0]
"Specifically, we present a common-covariance estimation algorithm and a common probability density function (pdf) estimation method, both of which do not assume any particular form for the underlying distributions.",1. Introduction,[0],[0]
Our only assumption is that the subject-specific noise factors are additive and have diverse distributions (otherwise they could be considered part of the common model).,1. Introduction,[0],[0]
"In this setting, the Euclidean and Riemannian mean estimates do not approach the true covariance matrix as the number of subjects grows.",1. Introduction,[0],[0]
"In contrast, we prove that our estimate does converge to the true covariance under very mild assumptions.",1. Introduction,[0],[0]
We verify the advantages of our approach through extensive experiments on simulated and on real data.,1. Introduction,[0],[0]
"Let u ∈ Rd be a random vector, which represents the common source of variability across a group of subjects.",2. Problem formulation,[0],[0]
"For example, in Fig. 1, u ∈ R2 is distributed according to the ‘ground truth’ density function (top right).",2. Problem formulation,[0],[0]
"Let xj ∈ Rd be a random vector, which represents the jth subject in the group (in Fig. 1, the jth scatter plot shows realizations of xj).",2. Problem formulation,[0],[0]
"We assume the additive model
xj = u+ vj , (1)
where {vj} are random vectors that are independent of u and represent subject-specific factors.",2. Problem formulation,[0],[0]
"Generally, each vj has a different distribution (had they been distributed identically, they would have been part of the common model).
",2. Problem formulation,[0],[0]
"Given realizations of xj , for j = 1 . .",2. Problem formulation,[0],[0]
".m, our goal is to estimate statistical properties of the common component u.
In particular, we are interested in estimating either the covariance matrix Σu or the full pdf pu of u.
Obviously, the performance in those estimation tasks will generally depend on both the number of subjects m and the number of samples per subject.",2. Problem formulation,[0],[0]
"However, here, we are interested in the common situation in which the number of samples per subject suffices to obtain reasonably accurate estimates for Σxj or pxj (e.g., when the dimension d is relatively small).",2. Problem formulation,[0],[0]
Our assumption is thus that the covariances (or pdfs) of the subjects xj are known and our focus is on the problem of recovering the common covariance (or pdf) from them.,2. Problem formulation,[0],[0]
"To apply our algorithms in practice, one has to plug in estimates of the covariances (or pdfs) of the subjects (obtained using, e.g., KDE).",2. Problem formulation,[0],[0]
"Since u and vj are independent, we have from (1) that
Σxj = Σu + Σvj (2)
for every j = 1, . . .",3. Common covariance estimation,[0],[0]
",m. We would like to estimate the covariance matrix Σu of the common component, given the covariance matrices {Σxj} of the subjects.",3. Common covariance estimation,[0],[0]
"To avoid ambiguity, we define the common component Σu to be the largest one satisfying such a decomposition.",3. Common covariance estimation,[0],[0]
"In particular, this means that the smallest eigenvalue of (at least some of) the subject-specific factors {Σvj} must be arbitrarily small.",3. Common covariance estimation,[0],[0]
"Indeed, otherwise there would exist some α > 0",3. Common covariance estimation,[0],[0]
such that Σvj αI for every j so that αI would be common to all {Σvj} and not subject-specific.,3. Common covariance estimation,[0],[0]
"In other words, the common component in this case is in fact Σu + αI and the noise covariances are Σvj − αI .
",3. Common covariance estimation,[0],[0]
"Let us first informally describe the key idea underlying
our method, and then provide a formal “group consistency” result.",3. Common covariance estimation,[0],[0]
Denote the eigenvalues of Σu by λ1 ≤ λ2 ≤ . . .,3. Common covariance estimation,[0],[0]
", λd and the corresponding eigenvectors by q1, q2, . . .",3. Common covariance estimation,[0],[0]
", qd.",3. Common covariance estimation,[0],[0]
"We will begin by estimating the smallest eigenvalue, λ1, and its associated eigenvector, q1.",3. Common covariance estimation,[0],[0]
"By definition,
λ1 = min ‖q‖=1
qTΣuq. (3)
Now, observe that
qTΣuq ≤ qTΣxjq, ∀j,∀q (4)
since qTΣxjq = q TΣuq + q TΣvjq and q TΣvjq ≥ 0.",3. Common covariance estimation,[0],[0]
"Our assumption, which we formalize mathematically below, is that the subject-specific noise covariances Σvj are diverse in the sense that their bottom eigenvectors tend to point in different directions.",3. Common covariance estimation,[0],[0]
"This, together with the fact their smallest eigenvalue can be arbitrarily small, implies that as the number of subjects grows, it becomes increasingly likely that for every direction q, at least one of the values {qTΣvjq}mj=1 be small.",3. Common covariance estimation,[0],[0]
"This motivates us to estimate λ1 and q1 as
q̂1 = arg min ‖q‖=1 min j∈{1,...,m}
qTΣxjq, (5)
λ̂1 = min ‖q‖=1 min j∈{1,...,m}
qTΣxjq. (6)
That is, we minimize over the pointwise minimum of the quadratic functions of the individual subjects.",3. Common covariance estimation,[0],[0]
Figure 2 visualizes this objective for the case of 2× 2 matrices.,3. Common covariance estimation,[0],[0]
"Here, the thick blue curve corresponds to the desired objective function (3), which we cannot directly minimize (as it involves the unknown Σu).",3. Common covariance estimation,[0],[0]
"The thin curves correspond to
Algorithm 1 Common covariance estimation Input: Covariance matrices Σx1 , . . .",3. Common covariance estimation,[0],[0]
",Σxm in Rd×d.",3. Common covariance estimation,[0],[0]
Output: Common covariance estimate Σ̂u.,3. Common covariance estimation,[0],[0]
for k = 1 . . .,3. Common covariance estimation,[0],[0]
"d do
Using (14), compute q̂k and λ̂k as
q̂k = arg min q∈Sk min j∈{1,...,m}
qTΣxjq, (11)
",3. Common covariance estimation,[0],[0]
"λ̂k = min q∈Sk min j∈{1,...,m}
qTΣxjq, (12)
where
Sk = { q : ‖q‖ = 1, q ⊥ span{q̂1, . . .",3. Common covariance estimation,[0],[0]
", q̂k−1} } .
(13)
end for Construct Σ̂u from {q̂k}mk=1 and {λ̂k}mk=1 as in (10).
",3. Common covariance estimation,[0],[0]
the quadratic functions of the subjects (involving the known matrices {Σxj}).,3. Common covariance estimation,[0],[0]
"As can be seen, the pointwise minimum of the thin curves (dotted curve) is close to the thick curve when the number of subjects is large.
",3. Common covariance estimation,[0],[0]
"Next, we turn to estimate λ2 and q2.",3. Common covariance estimation,[0],[0]
"Note that
λ2 = min {q:‖q‖=1,q⊥q1}
qTΣuq
≤ min {q:‖q‖=1,q⊥q1} qTΣxjq, ∀j.",3. Common covariance estimation,[0],[0]
"(7)
Therefore, following the logic above, and replacing q1 by its estimate q̂1, we propose to calculate q̂2 and λ̂2 as
q̂2 = arg min {q:‖q‖=1,q⊥q̂1} min j∈{1,...,m}
qTΣxjq, (8)
λ̂2 = min {q:‖q‖=1,q⊥q̂1} min j∈{1,...,m}
qTΣxjq. (9)
",3. Common covariance estimation,[0],[0]
"This process can be repeated, where at the kth step, we constrain the search to the subspace orthogonal to span{q̂1, . . .",3. Common covariance estimation,[0],[0]
", q̂k−1}.",3. Common covariance estimation,[0],[0]
"The last eigenvector, q̂d, is completely determined by q̂1, . . .",3. Common covariance estimation,[0],[0]
", q̂d−1 and thus does not involve an optimization problem.",3. Common covariance estimation,[0],[0]
"The associated eigenvalue is estimated as λ̂d = minj∈{1,...,m} q̂ T d Σxj q̂d.
",3. Common covariance estimation,[0],[0]
"Having estimated all the eigenvalues and eigenvectors, we construct our estimate of Σu as
Σ̂u = d∑ k=1 λ̂kq̂kq̂",3. Common covariance estimation,[0],[0]
T k .,3. Common covariance estimation,[0],[0]
"(10)
",3. Common covariance estimation,[0],[0]
This is summarized in Alg. 1.,3. Common covariance estimation,[0],[0]
"The objective in Problems (11),(12) is the pointwise minimum of a finite set of continuous (quadratic) functions over
a compact set.",3.1. Practical implementation,[0],[0]
"Therefore, the minimum is attained at the minimum of one of those functions, each of which has a closed form.",3.1. Practical implementation,[0],[0]
"Specifically, when k = 1, we only have the constraint ‖q‖ = 1, and the minimum of the jth problem is the smallest eigenvalue of Σxj (attained by the corresponding eigenvector).",3.1. Practical implementation,[0],[0]
"When k > 1, we have an additional set of linear constraints, which can be written asQkq = 0, where Qk = ∑k−1 i=1",3.1. Practical implementation,[0],[0]
q̂iq̂ T i .,3.1. Practical implementation,[0],[0]
"In this case, the minimizer is given by the top eigenvector of (I −Qk)(cI −Σxj )(I −Qk), which we denote by vkj , where c is any constant such that cI −Σxj 0",3.1. Practical implementation,[0],[0]
"(Blau & Michaeli, 2017).",3.1. Practical implementation,[0],[0]
"Thus, in summary,
q̂k = v k j∗ , λ̂k = (v k j∗) TΣxjv k j∗ , (14)
where j∗ = arg minj∈{1,...,m}(v k j )",3.1. Practical implementation,[0],[0]
"TΣxjv k j .
",3.1. Practical implementation,[0],[0]
"In the Supplementary Material, we discuss ways to speed up the estimation on parallel platforms.",3.1. Practical implementation,[0],[0]
To analyze the behavior of Alg.,3.2. Group consistency,[0],[0]
"1 as the number of subjects m increases, one must assume something regarding the variability of the subject-specific noise covariances Σvj .",3.2. Group consistency,[0],[0]
"A rather general assumption is that they are independent draws from some distribution over PSD matrices, namely
Σvj ∼ pΣv .",3.2. Group consistency,[0],[0]
"(15)
The next theorem shows that under very mild conditions on pΣv , our estimate Σ̂u converges to Σu almost surely (a.s.).",3.2. Group consistency,[0],[0]
"We refer to this as group consistency.
",3.2. Group consistency,[0],[0]
Theorem 1 (Group consistency).,3.2. Group consistency,[0],[0]
"Assume that
P (λmax(Σv) ≤ α) = 1 (16)
for some α > 0 and that P ( qTΣvq ≤ )",3.2. Group consistency,[0],[0]
"> 0 (17)
for every > 0 and every unit-norm q. Let Σ̂ m
u denote the estimate produced by Alg. 1 using m subjects.",3.2. Group consistency,[0],[0]
"Then
P (
lim m→∞ ∥∥∥Σ̂mu −Σu∥∥∥ = 0) = 1. (18) Assumption (16) merely states that the noise factors are not arbitrarily large.",3.2. Group consistency,[0],[0]
Assumption (17) is a condition on the distribution of the smallest eigenvalue of Σv and its associated eigenvector.,3.2. Group consistency,[0],[0]
"Roughly speaking, it requires that there be a positive probability for the smallest eigenvalue to be arbitrarily small and, simultaneously, for the corresponding eigenvector to point in any direction (i.e., this eigenvector can have any distribution on the unit sphere as long as it does not vanish on a set of nonzero Lebesgue measure).",3.2. Group consistency,[0],[0]
"Recall that the condition on the smallest eigenvalue is actually
part of the definition of the common covariance estimation problem, and therefore not a limiting assumption.
",3.2. Group consistency,[0],[0]
"To prove the theorem, let us denote ψ(q) , qTΣuq, gj(q) , qTΣvjq, and hm(q) , minj∈{1,...,m} gj(q).",3.2. Group consistency,[0],[0]
Note that ψ(q) is a deterministic function (as Σu is deterministic) whereas {gj(q)} and {hm(q)} are sequences of random functions (as {Σvj} are random).,3.2. Group consistency,[0],[0]
"We will need the following lemmas (see proofs in the Supplementary).
",3.2. Group consistency,[0],[0]
Lemma 1.,3.2. Group consistency,[0],[0]
"For every q, the sequence of random variables {hm(q)} converges to zero almost surely.",3.2. Group consistency,[0],[0]
"Furthermore, for any sequence of vectors {qm}∞m=1 that converges to some vector q∗, the sequence of random variables {hm(qm)} converges to zero almost surely.
",3.2. Group consistency,[0],[0]
Lemma 2.,3.2. Group consistency,[0],[0]
"Let φ(q) be a continuous bounded function on a compact set C, which achieves a strict global minimum at q∗ ∈ C. Let {fn(q)}∞n=1 be a sequence of continuous bounded nonnegative functions on C satisfying fn(q∗)→ 0, and let wn(q) = φ(q) + fn(q).",3.2. Group consistency,[0],[0]
"Then any sequence of the form qn ∈ arg minq∈C wn(q) converges to q∗, and the sequence wn(qn) converges to φ(q ∗).
proof of Theorem 1.",3.2. Group consistency,[0],[0]
"For simplicity, we prove the theorem for d = 2.",3.2. Group consistency,[0],[0]
"The extension to higher dimensions is similar.
",3.2. Group consistency,[0],[0]
"Since problem (11) is symmetric, we can divide the unit circle into two disjoint half circles A and B such that A is closed, and restrict the search for the minimum to A. Let us first assume that λ1 6= λ2.",3.2. Group consistency,[0],[0]
"In this case, the minimum of ψ(q) over the unit circle is achieved at the points q1 and −q1.",3.2. Group consistency,[0],[0]
"Without loss of generality, we assume that q1 ∈",3.2. Group consistency,[0],[0]
A and −q1 ∈,3.2. Group consistency,[0],[0]
B.,3.2. Group consistency,[0],[0]
The objective in (11) can be written as ψ(q) + hm(q).,3.2. Group consistency,[0],[0]
"Since hm(q) is continuous for every m and hm(q1)
",3.2. Group consistency,[0],[0]
a.s.→ 0,3.2. Group consistency,[0],[0]
"(Lemma 1), the conditions of Lemma 2 hold a.s.",3.2. Group consistency,[0],[0]
"Therefore, our estimate of the bottom eigenvector, q̂m1 , converges a.s.",3.2. Group consistency,[0],[0]
"to the true eigenvector q1, namely
q̂m1 a.s.→ q1.",3.2. Group consistency,[0],[0]
"(19)
Our estimate (12) of the bottom eigenvalue, λ̂m1 , can be written as ψ(q̂m1 ) +",3.2. Group consistency,[0],[0]
hm(q̂ m 1 ).,3.2. Group consistency,[0],[0]
"Since q̂ m 1
a.s.→ q1, we have from Lemma 1 that hm(q̂ m 1 )",3.2. Group consistency,[0],[0]
"a.s.→ 0, and therefore λ̂m1 a.s.→ ψ(q1)",3.2. Group consistency,[0],[0]
"= λ1 as well.
",3.2. Group consistency,[0],[0]
"The top eigenvector is given by q2 = Rq1, where R is a 90◦ rotation matrix, and our estimate of this eigenvector is simply q̂2 = Rq̂1.",3.2. Group consistency,[0],[0]
"Therefore, (19) implies that also
q̂m2 a.s.→ q2.",3.2. Group consistency,[0],[0]
"(20)
The convergence of λ̂m2 to λ2 follows similarly by Lemma 1.
",3.2. Group consistency,[0],[0]
Let us now treat the case where λ1 = λ2.,3.2. Group consistency,[0],[0]
"In this setting, the vectors q̂m1 , q̂ m 2 do not necessarily converge.",3.2. Group consistency,[0],[0]
"However, for the matrix Σ̂ m
u to converge to Σu, it suffices that only
the eigenvalue estimates λ̂m1 , λ̂ m 2 converge to λ1, λ2 (in that case, the vectors q̂m1 , q̂ m 2 have no effect in (10)).",3.2. Group consistency,[0],[0]
"To see that the eigenvalues converge, note that the solution of (12) is bounded from below by minq∈S1 ψ(q) = λ1, because hm(q) ≥ 0.",3.2. Group consistency,[0],[0]
"Additionally, we have that
λ̂m1 = min q∈S1 min j∈{1,...,m} qTΣxjq
= λ1 + min q∈S1 hm(q) ≤",3.2. Group consistency,[0],[0]
"λ1 + hm(q̄) a.s.→ λ1, (21)
where q̄ is an arbitrary point in S1, and the convergence is due to Lemma 1.",3.2. Group consistency,[0],[0]
Therefore λ̂m1 converges to λ1.,3.2. Group consistency,[0],[0]
"Similar arguments can be invoked to show that λ̂m2 converges to λ2.
",3.2. Group consistency,[0],[0]
"Since the eigenvectors and eigenvalues converge, Σ̂ m
u converges to Σu, and the proof is complete.",3.2. Group consistency,[0],[0]
"Next, we address the problem of estimating the pdf pu of the common component, given the pdfs {pxj} of the subjects in the group.
",4. Common density function estimation,[0],[0]
"Since u and xj are statistically independent, we have that pxj (α) =",4. Common density function estimation,[0],[0]
( pu ∗ pvj ),4. Common density function estimation,[0],[0]
"(α), (22)
where ‘∗’ denotes convolution.",4. Common density function estimation,[0],[0]
"Furthermore,
ϕxj (t) = ϕu(t)ϕvj (t), (23)
where ϕz(t) =",4. Common density function estimation,[0],[0]
"E[ejt T z] denotes the characteristic function of a random vector z. We will focus on estimating ϕu(t), from which pu can be retrieved by a Fourier transform.
",4. Common density function estimation,[0],[0]
"A well known property of characteristic functions is that |ϕz(t)| ≤ 1 for every t. Therefore, we have from (23) that |ϕu(t)| ≥ |ϕxj (t)| for every j and for all t. In particular,
|ϕu(t)|",4. Common density function estimation,[0],[0]
"≥ max j∈{1,...,m} ∣∣ϕxj (t)∣∣ , ∀t. (24) Based on this observation, we propose to take the maximum among the values {|ϕxj (t)|}mj=1 as our estimate of |ϕu(t)|, for every t.",4. Common density function estimation,[0],[0]
"The idea is that if the noise characteristic functions {ϕvj (t)} are diverse, then for every t, it is likely that at least one of them attain a value close to 1 (in absolute value).",4. Common density function estimation,[0],[0]
"Namely, at least one of the values {|ϕxj (t)|} is close to |ϕu(t)|, which justifies our estimator.",4. Common density function estimation,[0],[0]
"To estimate the phase of ϕu(t), we take the phase of the characteristic function ϕxj (t) that attains the maximum.",4. Common density function estimation,[0],[0]
"That is, we construct our estimate as
k(t) = arg max j∈{1,...,m} |ϕxj (t)|,
ϕ̂u(t) = ϕxk(t)(t).",4. Common density function estimation,[0],[0]
"(25)
Algorithm 2 Common density estimation",4. Common density function estimation,[0],[0]
"Input: Density functions px1 , . . .",4. Common density function estimation,[0],[0]
", pxm .",4. Common density function estimation,[0],[0]
Output: Common density estimate p̂u. for j = 1 . .,4. Common density function estimation,[0],[0]
.m,4. Common density function estimation,[0],[0]
"do
Set ϕxj ← IDFT{pxj}.",4. Common density function estimation,[0],[0]
"for all t do
Set k as the index of the largest value in {ϕxj (t)}.",4. Common density function estimation,[0],[0]
"Set ϕ̂u(t)← ϕxk(t).
end for end for Set p̂u ← DFT{ϕ̂u}.",4. Common density function estimation,[0],[0]
"Truncate the negative values of p̂u and normalize it to have unit area.
",4. Common density function estimation,[0],[0]
"Note that our phase estimate is accurate when the pdfs {pvj} are symmetric (e.g., when {vj} are zero-mean Gaussian random vectors).",4. Common density function estimation,[0],[0]
"Indeed, in that case the phase of ϕvj is zero, so that the phase of ϕu equals the phase of ϕxj .",4. Common density function estimation,[0],[0]
"Our common pdf estimation algorithm is summarized in Alg. 2.
",4. Common density function estimation,[0],[0]
"It is interesting to note that Alg. 2 has been proposed in the Image Processing community, as a way of removing blur from several blurry images the of same scene (Delbracio & Sapiro, 2015).",4. Common density function estimation,[0],[0]
The analogy to our setting is quite natural.,4. Common density function estimation,[0],[0]
"The functions pxj in our context can be thought of as “blurry” versions of the function pu, where the “blur kernels” are the functions pvj (see (22)).",4. Common density function estimation,[0],[0]
"In this section we verify the effectiveness of our methods, first on simulated data and then on real data.",5. Experiments,[0],[0]
"In our first experiment, we study the behavior of our common covariance estimator as a function of the number of subjects and the signal to noise ratio (SNR).",5.1. Estimation of Pearson correlation coefficient,[0],[0]
"We take the common component u to be a two-dimensional random vector with covariance matrix
Σu =
( 1 0.5
0.5 1
) .",5.1. Estimation of Pearson correlation coefficient,[0],[0]
"(26)
Our goal is to estimate the Pearson correlation coefficient between u(1) and u(2) (which is ρ = 0.5 in this case) from the perturbed versions Σxj = Σu + Σvj .",5.1. Estimation of Pearson correlation coefficient,[0],[0]
This can be done by first estimating Σu and then normalizing the offdiagonal entry by the square-roots of the diagonal entries.,5.1. Estimation of Pearson correlation coefficient,[0],[0]
"We compare our estimator (Alg. 1) with naive averaging of {Σxj} using either Euclidean or Riemannian mean.
",5.1. Estimation of Pearson correlation coefficient,[0],[0]
"We generate the matrices {Σvj} as
Σvj = M jΛjM T j , (27)
",5.1. Estimation of Pearson correlation coefficient,[0],[0]
"whereM j are random rotation matrices whose angles are distributed uniformly in [0, 2π], and Λj are random diagonal matrices",5.1. Estimation of Pearson correlation coefficient,[0],[0]
"Λj = diag{βj1, β j 2} with β j 1 ∼ U [0, b] and βj2 ∼ U [b, 2b] for some b > 0.",5.1. Estimation of Pearson correlation coefficient,[0],[0]
"We draw {M j}, {β j 1}, {β j 2} independently.",5.1. Estimation of Pearson correlation coefficient,[0],[0]
"The SNR, which we define as SNR = Tr{Σu}/E{Tr{Σv}}, is 1/b in this case.
",5.1. Estimation of Pearson correlation coefficient,[0],[0]
Figures 3 and 4 visualize the mean and variance of our estimator as well as of the naive Euclidean and Riemanian mean estimators (using 200 trials per setting) as functions of the number of subjects and the SNR.,5.1. Estimation of Pearson correlation coefficient,[0],[0]
"As can be seen, while the variance of our estimator is slightly larger than the variances of the naive estimators, its bias is significantly smaller.",5.1. Estimation of Pearson correlation coefficient,[0],[0]
"Therefore, overall, it attains a substantially lower mean square error.",5.1. Estimation of Pearson correlation coefficient,[0],[0]
Figure 3 also indicates that our estimator is asymptotically (in the number of subjects) unbiased.,5.1. Estimation of Pearson correlation coefficient,[0],[0]
"The naive estimators, on the other hand, have severe biases, which do not decrease with the number of subjects.",5.1. Estimation of Pearson correlation coefficient,[0],[0]
"Figure 4 further illustrates that the performance of the naive
estimators degrades rapidly as the SNR decreases, while our estimator remains relatively accurate even at low SNRs.
",5.1. Estimation of Pearson correlation coefficient,[0],[0]
"In this example, the poor performance of the naive estimators is mainly rooted in their over-estimation of the diagonal entries of Σu.",5.1. Estimation of Pearson correlation coefficient,[0],[0]
"This happens because the contributions of the noise matrices {Σvj} are only positive on the diagonal, so that averaging does not cancel them out.",5.1. Estimation of Pearson correlation coefficient,[0],[0]
"In most practical cases, the advantage of our approach is not confined to the diagonal elements of Σu.",5.2. Clustered subject-specific noise covariances,[0],[0]
"Specifically, although our algorithm relies on the diversity of the noise covariances, it does not require their eigenvectors to be uniformly distributed on the unit sphere.",5.2. Clustered subject-specific noise covariances,[0],[0]
"Therefore, our technique can even handle cases in which the noise covariances tend to cluster around a certain matrix.",5.2. Clustered subject-specific noise covariances,[0],[0]
"As long as there exists a nonzero probability to encounter matrices away from the cluster, our algorithm is guaranteed to produce an accurate estimate as the number of subjects grows.",5.2. Clustered subject-specific noise covariances,[0],[0]
"This is in contrast to naive averaging, which typically produces estimates with severe bias in all matrix entries.
",5.2. Clustered subject-specific noise covariances,[0],[0]
"To illustrate this, we next perform a 3× 3 common covariance estimation experiment.",5.2. Clustered subject-specific noise covariances,[0],[0]
"We generate Σvj as in (27), where now we construct the unitary matrixM j assin(θ1) cos(θ2) sin(θ1) sin(θ2) cos(θ1)cos(θ1)",5.2. Clustered subject-specific noise covariances,[0],[0]
"cos(θ2) cos(θ1) sin(θ2) − sin(θ1)
− sin(θ2) cos(θ2) 0  , (28) with θ1 and θ2 being two independent random variables with a normal distribution N (1, 1) truncated to [0, π] and
[0, 2π], respectively (Chopin, 2011).
",5.2. Clustered subject-specific noise covariances,[0],[0]
"Figure 5 depicts the estimation results obtained with Alg. 1 and with naive averaging, using 1000 subjects.",5.2. Clustered subject-specific noise covariances,[0],[0]
We show results for three different common covariance matrices.,5.2. Clustered subject-specific noise covariances,[0],[0]
"These include a zero matrix (first row), an identity matrix (second row), and a random PSD matrix (third row).",5.2. Clustered subject-specific noise covariances,[0],[0]
"As can be seen, the Euclidean and Riemannian means produce inaccurate estimates in all entries of the matrix while our estimator produces accurate results.",5.2. Clustered subject-specific noise covariances,[0],[0]
This is despite the preference of the noise covariances towards specific patterns.,5.2. Clustered subject-specific noise covariances,[0],[0]
"Next, we applied our covariance estimation algorithm on the ADHD200-preprocessed dataset (Bellec et al., 2017).",5.3. FMRI data,[0],[0]
We used the Athena pipeline.,5.3. FMRI data,[0],[0]
"In particular, we used preprocessed resting state fMRI data, written into MNI space at
4mm×4mm×4mm voxel resolution.",5.3. FMRI data,[0],[0]
"We removed nuisance variance (Lund, 2001; Fox et al., 2005), applied a temporal bandpass filter (0.009 Hz < f < 0.08 Hz) (Fox et al., 2005; Biswal et al., 1995; Cordes et al., 2001) and a spatial Gaussian filter (6mm FWHM), and removed linear trend from the extracted time-courses.",5.3. FMRI data,[0],[0]
"We took the 458 control subjects from the published training set (for results on 141 subjects with ADHD, please see the Supplementary).",5.3. FMRI data,[0],[0]
"From each subject, we extracted time-courses of 39 regions of interest (ROI) of the MSDL atlas (Varoquaux et al., 2011) and estimated their covariance using the Ledoit-Wolf estimator (Ledoit & Wolf, 2004).",5.3. FMRI data,[0],[0]
This gave us a 39× 39 covariance matrix per subject.,5.3. FMRI data,[0],[0]
"We estimated the common covariance matrix using Alg. 1, using Geometric (Riemannian) mean (Varoquaux et al., 2010a), and using Euclidean mean.",5.3. FMRI data,[0],[0]
"From the estimated covariances, we calculated correlation matrices.",5.3. FMRI data,[0],[0]
"We used the nilearn and scikit-learn python packages
(Abraham et al., 2014; Pedregosa et al., 2011; Buitinck et al., 2013).",5.3. FMRI data,[0],[0]
The running time of Alg. 1 was about 10s on an 8 core Intel i7-6700 with 16GB of RAM working at 3.40GHz.,5.3. FMRI data,[0],[0]
"The results are depicted in Fig. 6.
",5.3. FMRI data,[0],[0]
"It has been shown that estimates of connectivity patterns often vary significantly between subjects (Moussa et al., 2012).",5.3. FMRI data,[0],[0]
"As can be seen in Fig. 6, our estimator detects activity within known networks despite the large variability between subjects.",5.3. FMRI data,[0],[0]
"In particular, our estimator detects stronger correlations than the Euclidean and Riemannian mean estimators within the Default Mode Network, the Right Ventral Attention network, the Left Ventral Attention network, and the Cingulate Insula (connectivity between cingulate cortex and insula) (Moussa et al., 2012).",5.3. FMRI data,[0],[0]
Zoomed versions of those networks are shown in Fig. 7.,5.3. FMRI data,[0],[0]
Note that the Euclidean mean estimator shows very low correlations within some of those regions.,5.3. FMRI data,[0],[0]
"In our last experiment, we used Alg. 2 to estimate the joint density function of arterial blood pressure (ABP) and photoplethysmogram (PPG) recordings.",5.4. Common density of PPG and ABP,[0],[0]
"We used measurements from 25 subjects in critical care taken from the MIMIC 2 dataset (Kachuee et al., 2015).",5.4. Common density of PPG and ABP,[0],[0]
"As a preprocessing step, we normalized the signals to have zero mean and unit variance.
",5.4. Common density of PPG and ABP,[0],[0]
"For each subject, we then estimated the 2D pdf of ABP and PPG using Gaussian KDE with bandwidth 0.08.",5.4. Common density of PPG and ABP,[0],[0]
"From the resulting 25 pdfs, we estimated the common pdf using Alg. 2.",5.4. Common density of PPG and ABP,[0],[0]
"As can be seen in Fig. 8, our algorithm manages to reveal delicate structures in the common pdf, which are not seen when applying KDE on all the data from all the subjects.",5.4. Common density of PPG and ABP,[0],[0]
"In the Supplementary, we show that these structures are not detected with naive KDE with any bandwidth.",5.4. Common density of PPG and ABP,[0],[0]
This illustrates again the ability of our approach to suppress subject-specific noise factors that have different distributions.,5.4. Common density of PPG and ABP,[0],[0]
"We presented algorithms for estimating the covariance and the pdf of the common component of a group of subjects, when noise has a different distribution for each subject.",6. Conclusion,[0],[0]
Our algorithms take advantage of the diversity of the subjectspecific noise distributions in order to efficiently suppress them.,6. Conclusion,[0],[0]
"In contrast to previous approaches, we did not assume any parametric model for the underlying distributions.",6. Conclusion,[0],[0]
"We proved that under rather mild assumptions, our common covariance estimate tends to the covariance of the common component as the number of subjects grows.",6. Conclusion,[0],[0]
"We presented experiments on simulated and on real data, which confirmed the advantages of our methods over alternative approaches.",6. Conclusion,[0],[0]
"In many areas of neuroscience and biological data analysis, it is desired to reveal common patterns among a group of subjects.",abstractText,[0],[0]
"Such analyses play important roles e.g., in detecting functional brain networks from fMRI scans and in identifying brain regions which show increased activity in response to certain stimuli.",abstractText,[0],[0]
"Group level techniques usually assume that all subjects in the group behave according to a single statistical model, or that deviations from the common model have simple parametric forms.",abstractText,[0],[0]
"Therefore, complex subject-specific deviations from the common model severely impair the performance of such methods.",abstractText,[0],[0]
"In this paper, we propose nonparametric algorithms for estimating the common covariance matrix and the common density function of several variables in a heterogeneous group of subjects.",abstractText,[0],[0]
"Our estimates converge to the true model as the number of subjects tends to infinity, under very mild conditions.",abstractText,[0],[0]
We illustrate the effectiveness of our methods through extensive simulations as well as on real-data from fMRI scans and from arterial blood pressure and photoplethysmogram measurements.,abstractText,[0],[0]
Revealing Common Statistical Behaviors in Heterogeneous Populations,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4295–4305 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
4295",text,[0],[0]
Neural Machine Translation (NMT) has largely replaced the complex pipeline of Phrase-Based MT with a single model that is trained end-to-end.,1 Introduction,[0],[0]
"However, NMT systems still typically rely on preand post-processing operations such as tokenization and word fragmentation through byte-pair encoding (BPE; Sennrich et al., 2016).",1 Introduction,[0],[0]
"Although these are effective, they involve hyperparameters
∗*Equal contributions
that should ideally be tuned for each language pair and corpus, an expensive step that is frequently omitted.",1 Introduction,[0],[0]
"Even when properly tuned, the representation of the corpus generated by pipelined external processing is likely to be sub-optimal.",1 Introduction,[0],[0]
"For instance, it is easy to find examples of word fragmentations, such as fling → fl + ing, that are linguistically implausible.",1 Introduction,[0],[0]
"NMT systems are generally robust to such infelicities—and can be made more robust through subword regularization (Kudo, 2018)—but their effect on performance has not been carefully studied.",1 Introduction,[0],[0]
"The problem of finding optimal segmentations becomes more complex when an NMT system must handle multiple source and target languages, as in multilingual translation or zero-shot approaches (Johnson et al., 2017).
",1 Introduction,[0],[0]
"Translating characters instead of word fragments avoids these problems, and gives the system access to all available information about source and target sequences.",1 Introduction,[0],[0]
"However, it presents significant modeling and computational challenges.",1 Introduction,[0],[0]
"Longer sequences incur linear per-layer cost and quadratic attention cost, and require information to be retained over longer temporal spans.",1 Introduction,[0],[0]
"Finer temporal granularity also creates the potential for attention jitter (Gulcehre et al., 2017).",1 Introduction,[0],[0]
"Perhaps most significantly, since the meaning of a word is not a compositional function of its characters, the system must learn to memorize many character sequences, a different task from the (mostly) compositional operations it performs at higher levels of linguistic abstraction.
",1 Introduction,[0],[0]
"In this paper, we show that a standard LSTM sequence-to-sequence model works very well for characters, and given sufficient depth, consistently outperforms identical models operating over word fragments.",1 Introduction,[0],[0]
"This result suggests that a productive line of research on character-level models is to seek architectures that approximate standard sequence-to-sequence models while being compu-
tationally cheaper.",1 Introduction,[0],[0]
One approach to this problem is temporal compression: reducing the number of state vectors required to represent input or output sequences.,1 Introduction,[0],[0]
"We evaluate various approaches for performing temporal compression, both according to a fixed schedule; and, more ambitiously, learning compression decisions with a Hierarchical Multiscale architecture (Chung et al., 2017).",1 Introduction,[0],[0]
"Following recent work by Lee et al. (2017), we focus on compressing the encoder.
",1 Introduction,[0],[0]
"Our contributions are as follows:
• The first large-scale empirical investigation of the translation quality of standard LSTM sequence-to-sequence architectures operating at the character level, demonstrating improvements in translation quality over word fragments, and quantifying the effect of corpus size and model capacity.",1 Introduction,[0],[0]
•,1 Introduction,[0],[0]
"A comparison of techniques to compress
character sequences, assessing their ability to trade translation quality for increased speed.",1 Introduction,[0],[0]
•,1 Introduction,[0],[0]
"A first attempt to learn how to compress the
source sequence during NMT training by using the Hierarchical Multiscale LSTM to dynamically shorten the source sequence as it passes through the encoder.",1 Introduction,[0],[0]
"Early work on modeling characters in NMT focused on solving the out-of-vocabulary and softmax bottleneck problems associated with wordlevel models (Ling et al., 2015; Costa-jussà and Fonollosa, 2016; Luong and Manning, 2016).",2 Related Work,[0],[0]
"These took the form of word-boundary-aware hierarchical models, with word-level models delegating to character-level models to generate representations in the encoder and words in the decoder.",2 Related Work,[0],[0]
"Our work will not assume fixed word boundaries are given in advance.
",2 Related Work,[0],[0]
"With the advent of word-fragment approaches, interest in character-level processing fell off, but has recently been reignited with the work of Lee et al. (2017).",2 Related Work,[0],[0]
"They propose a specialized character-level encoder, connected to an unmodified character-level RNN decoder.",2 Related Work,[0],[0]
"They address the modeling and efficiency challenges of long character sequences using a convolutional layer, max-pooling over time, and highway layers.",2 Related Work,[0],[0]
"We agree with their conclusion that character-level translation is effective, but revisit the question
of whether their specific encoder produces a desirable speed-quality tradeoff in the context of a much stronger baseline translation system.",2 Related Work,[0],[0]
"We draw inspiration from their pooling solution for reducing sequence length, along with similar ideas from the speech community (Chan et al., 2016), when devising fixed-schedule reduction strategies in Section 3.3.
",2 Related Work,[0],[0]
One of our primary contributions is an extensive invesigation of the efficacy of a typical LSTM-based NMT system when operating at the character-level.,2 Related Work,[0],[0]
The vast majority of existing studies compare a specialized character-level architecture to a distinct word-level one.,2 Related Work,[0],[0]
"To the best of our knowledge, only a small number of papers have explored running NMT unmodified on character sequences; these include: Luong and Manning (2016) on WMT’15 English-Czech, Wu et al. (2016) on WMT’14 English-German, and Bradbury et al. (2016) on IWSLT German-English.",2 Related Work,[0],[0]
All report scores that either trail behind or reach parity with word-level models.,2 Related Work,[0],[0]
"Only Wu et al. (2016) compare to word fragment models, which they show to outperform characters by a sizeable margin.",2 Related Work,[0],[0]
"We revisit the question of character- versus fragment-level NMT here, and reach quite different conclusions.",2 Related Work,[0],[0]
We adopt a simplified version of the LSTM architecture of Chen et al. (2018) that achieves state-ofthe-art performance on the competitive WMT14 English-French and English-German benchmarks.,3.1 Baseline Sequence-to-Sequence Model,[0],[0]
"This incorporates bidirectional LSTM (BiLSTM) layers in the encoder, concatenating the output from forward and backward directions before feeding the next layer.",3.1 Baseline Sequence-to-Sequence Model,[0],[0]
Output from the top encoder layer is projected down to the decoder dimension and used in an additive attention mechanism computed over the bottom decoder layer.,3.1 Baseline Sequence-to-Sequence Model,[0],[0]
"The decoder consists of unidirectional layers, all of which use the encoder context vectors computed from attention weights over the bottom layer.",3.1 Baseline Sequence-to-Sequence Model,[0],[0]
"For both encoder and decoder we use layer normalization (Ba et al., 2016) and residual connections beginning at the third layer.",3.1 Baseline Sequence-to-Sequence Model,[0],[0]
We do not apply a non-linearity to LSTM output.,3.1 Baseline Sequence-to-Sequence Model,[0],[0]
"We regularize with dropout applied to embeddings and to the output of each LSTM layer.
",3.1 Baseline Sequence-to-Sequence Model,[0],[0]
"In the interests of simplicity and reproducibil-
ity, we depart from Chen et al. (2018) in several ways: we do not use multi-headed attention, feed encoder context vectors to the softmax, regularize with label smoothing or weight decay, nor apply dropout to the attention mechanism.
",3.1 Baseline Sequence-to-Sequence Model,[0],[0]
"Our baseline character models and BPE models both use this architecture, differing only in whether the source and target languages are tokenized into sequences of characters or BPE word fragments.",3.1 Baseline Sequence-to-Sequence Model,[0],[0]
We describe BPE briefly below.,3.1 Baseline Sequence-to-Sequence Model,[0],[0]
"Byte-Pair Encoding (BPE) offers a simple interpolation between word- and character-level representations (Sennrich et al., 2016).",3.2 Byte-Pair Encoding,[0],[0]
It creates a vocabulary of frequent words and word fragments in an iterative greedy merging process that begins with characters and ends when a desired vocabulary size is reached.,3.2 Byte-Pair Encoding,[0],[0]
The source and target language are typically processed together in order to exploit lexical similarities.,3.2 Byte-Pair Encoding,[0],[0]
"Given a vocabulary, BPE re-tokenizes the corpus into word fragments in a greedy left-to-right fashion, selecting the longest possible vocabulary match, and backing off to characters when necessary.
",3.2 Byte-Pair Encoding,[0],[0]
"Since each BPE token consists of one or more characters, BPE-tokenized sequences will be shorter than character sequences.",3.2 Byte-Pair Encoding,[0],[0]
"Viewed as a mechanism to reduce sequence length, BPE differs from the solutions we will discuss subsequently in that it increases the vocabulary size, delegating the task of creating representations for word fragments to the embedding table.",3.2 Byte-Pair Encoding,[0],[0]
"Also, despite being data-driven, its segmentation decisions are fixed before NMT training begins.",3.2 Byte-Pair Encoding,[0],[0]
We explore using fixed stride temporal pooling within the encoder to compress the source character sequence.,3.3 Fixed stride Temporal Pooling,[0],[0]
"These solutions are characterized by pooling the contents of two or more contiguous timesteps to create a single vector that summarizes them, and will replace them to shorten the sequence in the next layer.",3.3 Fixed stride Temporal Pooling,[0],[0]
"These approaches can learn to interpret the raw character sequence in service to their translation objective, but any such interepretation must fit into the pooling schedule that was specified during network construction.",3.3 Fixed stride Temporal Pooling,[0],[0]
"We evaluate two methods in this family: a reimplementation of Lee et al. (2017), and a version of our baseline with interspersed pooling layers.
",3.3 Fixed stride Temporal Pooling,[0],[0]
"As mentioned earlier, Lee et al. (2017) propose a specialized character encoder that combines convolutional layers to accumulate local context, max-pooling layers to reduce sequence lengths, highway layers to increase network capacity, followed by bidirectional GRU layers to generate globally aware contextual source representations.",3.3 Fixed stride Temporal Pooling,[0],[0]
This strategy is particularly efficient because all reductions happen before the first recurrent layer.,3.3 Fixed stride Temporal Pooling,[0],[0]
"We re-implement their approach faithfully, with the exceptions of using LSTMs in place of GRUs,1 and modifying the batch sizes to accomodate our multi-GPU training scheme.
",3.3 Fixed stride Temporal Pooling,[0],[0]
"While pooling based approaches are typically employed in association with convolutional layers, we can also intersperse pooling layers into our high capacity baseline encoder.",3.3 Fixed stride Temporal Pooling,[0],[0]
"This means that after each BiLSTM layer, we have the option to include a fixed-stride pooling layer to compress the sequence before it is processed by the next BiLSTM layer.",3.3 Fixed stride Temporal Pooling,[0],[0]
"This is similar to the pyramidal LSTM encoders used for neural speech recognition (Chan et al., 2016).",3.3 Fixed stride Temporal Pooling,[0],[0]
"This general strategy affords considerable flexibility to the network designer, leaving the type of pooling (concatenation, max, mean), and the strides with which to pool as design decisions that can be tuned to fit the task.",3.3 Fixed stride Temporal Pooling,[0],[0]
"It is unsatisfying to compress a sequence on a fixed schedule; after all, the characters in a sentence do not each carry an identical amount of information.",3.4 Learned Temporal Compression,[0],[0]
"The goal of this section is to explore data-driven reduction methods that are optimized to the NMT system’s objective, and which learn to compress as a part of training.
",3.4 Learned Temporal Compression,[0],[0]
"Any strategy for performing temporal compression will necessarily make discrete decisions, since sentence length is discrete.",3.4 Learned Temporal Compression,[0],[0]
"Examples of such strategies include sparse attention (Raffel et al., 2017) and discrete auto-encoders (Kaiser et al., 2018).",3.4 Learned Temporal Compression,[0],[0]
"For our initial exploration, we chose the hierarchical multiscale (HM) architecture of Chung et al. (2017), which we briefly describe.",3.4 Learned Temporal Compression,[0],[0]
"The HM is a bottom-up temporal subsampling approach, with each layer selecting the timesteps that will survive to the layer above.",3.4.1 Hierarchical Multiscale LSTM,[0],[0]
"At a given timestep t and layer `, the network makes a binary decision,
1 Development experiments indicated that using LSTMs over GRUs resulted in a slight improvement.
",3.4.1 Hierarchical Multiscale LSTM,[0],[0]
"z`t , to determine whether or not it should send its output up to layer `+ 1.",3.4.1 Hierarchical Multiscale LSTM,[0],[0]
"The preactivation for this decision, z̃`t , is a function of the current node’s inputs from below and from the previous hidden state, similar to an LSTM gate.",3.4.1 Hierarchical Multiscale LSTM,[0],[0]
"However, z`t ’s activation is a binary step function in the forward pass, to enable discrete decisions, and a hard sigmoid in the backward pass, to allow gradients to flow through the decision point.2",3.4.1 Hierarchical Multiscale LSTM,[0],[0]
"The z`t decision affects both the layer above, and the next timestep of the current layer:
• z`t = 1, flow up: the node above (t, `+1) performs a normal LSTM update; the node to the right (t+1, `) performs a modified update called a flush, which ignores the LSTM internal cell at (t, `), and redirects the incoming LSTM hidden state from (t, `) to (t, `+ 1).
",3.4.1 Hierarchical Multiscale LSTM,[0],[0]
"• z`t = 0, flow right: the node above (t, `+1) simply copies the cell and hidden state values from (t−1, `+1); the node to the right (t+1, `) performs a normal LSTM update.
",3.4.1 Hierarchical Multiscale LSTM,[0],[0]
"Conceptually, when z`t = 0, the node above it becomes a placeholder and is effectively removed from the sequence for that layer.",3.4.1 Hierarchical Multiscale LSTM,[0],[0]
"Shorter upper layers save computation and facilitate the left-toright flow of information for the surviving nodes.
",3.4.1 Hierarchical Multiscale LSTM,[0],[0]
"Typically, one uses the top hidden state hLt from a stack of L RNNs to provide the representation for a timestep t. But for the HM, the top layer may be updated much less frequently than the layers below it.",3.4.1 Hierarchical Multiscale LSTM,[0],[0]
"To enable tasks that need a distinct representation for each timestep, such as language modeling, the HM employs a gated output module to mix hidden states across layers.",3.4.1 Hierarchical Multiscale LSTM,[0],[0]
"This learned module combines the states h1t , h 2 t , . .",3.4.1 Hierarchical Multiscale LSTM,[0],[0]
.,3.4.1 Hierarchical Multiscale LSTM,[0],[0]
", h L t using scaling and projection operators to produce a single output ht.",3.4.1 Hierarchical Multiscale LSTM,[0],[0]
We would like sequences to become progressively shorter as we move upward through the layers.,3.4.2 Modifying the HM for NMT,[0],[0]
"As originally specified, the HM calculates z`t independently for every t and `, including copied nodes, meaning that a “removed” timestep could reappear in a higher layer when a copied node (t, `) sets z`t = 1.",3.4.2 Modifying the HM for NMT,[0],[0]
"This is easily addressed by locking z ` t = 0 for copied nodes, creating a hierarchical structure
2This disconnect between forward and backward activations is known as a straight-through estimator (Bengio et al., 2013).
in which upper layers never increase the amount of computation.
",3.4.2 Modifying the HM for NMT,[0],[0]
"We also found that the flush component of the original architecture, which modifies the LSTM update at (t+1, `) to discard the LSTM’s internal cell, provided too much incentive to leave z`t at 0, resulting in degenerate configurations which collapsed to having very few tokens in their upper layers.",3.4.2 Modifying the HM for NMT,[0],[0]
We addressed this by removing the notion of a flush from our architecture.,3.4.2 Modifying the HM for NMT,[0],[0]
"The node to the right (t+1, `) always performs a normal LSTM update, regardless of z`t .",3.4.2 Modifying the HM for NMT,[0],[0]
"This modification is similar to one proposed independently by Kádár et al. (2018), who simplified the flush operation by removing the connection to (t, `+ 1).
",3.4.2 Modifying the HM for NMT,[0],[0]
"We found it useful to change the initial value of the bias term used in the calculation of z̃`t , which we refer to as the z-bias.",3.4.2 Modifying the HM for NMT,[0],[0]
"Setting z-bias to 1, which is the saturation point for the hard sigmoid with slope 1, improves training stability by encouraging the encoder to explore configurations where most timesteps survive through all layers, before starting to discard them.
",3.4.2 Modifying the HM for NMT,[0],[0]
"Even with these modifications, we observed degenerate behavior in some settings.",3.4.2 Modifying the HM for NMT,[0],[0]
"To discourage this, we added a compression loss component similar to that of Ke et al. (2018) to penalize z activation rates outside a specified range α1, α2:",3.4.2 Modifying the HM for NMT,[0],[0]
"Lc = ∑ l max(0, Z
l − α1T, α2T − Z l), where T is source sequence length and Z l = ∑T t=1 z",3.4.2 Modifying the HM for NMT,[0],[0]
"l t.
To incorporate the HM into our NMT encoder, we replace the lowest BiLSTM layer with unidirectional HM",3.4.2 Modifying the HM for NMT,[0],[0]
layers.3 We adapt any remaining BiLSTM layers to copy or update according to the z-values calculated by the top HM layer.,3.4.2 Modifying the HM for NMT,[0],[0]
"We adopt the corpora used by Lee et al (2017), with the exception of WMT15 Russian-English.4 To measure performance on an “easy” language pair, and to calibrate our results against recent benchmarks, we also included WMT14 EnglishFrench.",4.1 Corpora,[0],[0]
Table 1 gives details of the corpora used.,4.1 Corpora,[0],[0]
"All corpora are preprocessed using Moses tools.5
3The flush operation makes the original HM inherently left-to-right.",4.1 Corpora,[0],[0]
"Since we have dropped flushes from our current version, it should be straightforward to devise a bidirectional variant, which we leave to future work.
",4.1 Corpora,[0],[0]
4Due,4.1 Corpora,[0],[0]
to licence restrictions.,4.1 Corpora,[0],[0]
"5 Scripts and arguments:
remove-non-printing-char.perl
Dev and test corpora are tokenized, but not filtered or cleaned.",4.1 Corpora,[0],[0]
"Our character models use only the most frequent 496 characters across both source and target languages; similarly, BPE is run across both languages, with a vocabulary size of 32k.",4.1 Corpora,[0],[0]
"Except where noted below, we used 6 bidirectional layers in the encoder, and 8 unidirectional layers in the decoder.","4.2 Model sizes, training, and inference",[0],[0]
"All vector dimensions were 512.
","4.2 Model sizes, training, and inference",[0],[0]
Models were trained using sentence-level crossentropy loss.,"4.2 Model sizes, training, and inference",[0],[0]
"Batch sizes are capped at 16,384 tokens, and each batch is divided among 16 NVIDIA P100s running synchronously.
","4.2 Model sizes, training, and inference",[0],[0]
Parameters were initialized with a uniform (0.04) distribution.,"4.2 Model sizes, training, and inference",[0],[0]
"We use the Adam optimizer, with β1 = 0.9, β2 = 0.999, and = 10−6 (Kingma and Ba, 2014).","4.2 Model sizes, training, and inference",[0],[0]
Gradient norm is clipped to 5.0.,"4.2 Model sizes, training, and inference",[0],[0]
"The initial learning rate is 0.0004, and we halve it whenever dev set perplexity has not decreased for 2k batches, with at least 2k batches between successive halvings.","4.2 Model sizes, training, and inference",[0],[0]
"Training stops when dev set perplexity has not decreased for 8k batches.
","4.2 Model sizes, training, and inference",[0],[0]
"Inference uses beam search with 8 hypotheses, coverage penalty of 0.2 (Tu et al., 2016), and length normalization of 0.2 (Wu et al., 2016).","4.2 Model sizes, training, and inference",[0],[0]
"When comparing character-level and BPE models, we tuned dropout independently for each setting, greedily exploring increments of 0.1 in the range 0.1–0.5, and selecting based on dev-set BLEU.",4.3 Tuning and Evalution,[0],[0]
"This expensive strategy is crucial to obtaining valid conclusions, since optimal dropout values tend to be lower for character models.
",4.3 Tuning and Evalution,[0],[0]
Our main evaluation metric is Moses-tokenized case-sensitive BLEU score.,4.3 Tuning and Evalution,[0],[0]
We report test-set scores on the checkpoints having highest dev-set BLEU.,4.3 Tuning and Evalution,[0],[0]
"To facilitate comparison with future work
tokenize.perl clean-corpus-n.perl -ratio 9 1 100
we also report SacreBLEU scores (Post, 2018) for key results, using the Moses detokenizer.",4.3 Tuning and Evalution,[0],[0]
"We begin with experiments to compare the standard RNN architecture from Section 3.1 at the character and BPE levels, using our full-scale model with 6 bidirectional encoder layers and 8 decoder layers.",5.1 Character-level translation,[0],[0]
"The primary results of our experiments are presented in Table 2, while Table 3 positions the same results with respect to recent points from the literature.
",5.1 Character-level translation,[0],[0]
There are a number of observations we can draw from this data.,5.1 Character-level translation,[0],[0]
"First, from the EnFr results in Table 3, we are in line with GNMT (Wu et al., 2016), and within 2 BLEU points of the RNN and Transformer models investigated by Chen et al. (2018).",5.1 Character-level translation,[0],[0]
"So, while we are not working at the exact state-ofthe-art, we are definitely in a range that should be relevant to most practitioners.
",5.1 Character-level translation,[0],[0]
"Also from Table 3, we compare quite favorably with Lee et al. (2017), exceeding their reported scores by 3-6 points, which we attribute to having employed much higher model capacity, as they use a single bidirectional layer in the encoder and a two-layer decoder.",5.1 Character-level translation,[0],[0]
"We investigate the impact of model capacity in Section 5.1.1.
",5.1 Character-level translation,[0],[0]
"Finally, Table 2 clearly shows the characterlevel systems outperforming BPE for all language pairs.",5.1 Character-level translation,[0],[0]
"The dominance of character-level methods in Table 2 indicates that RNN-based NMT architectures are not only capable of translating charac-
ter sequences, but actually benefit from them.",5.1 Character-level translation,[0],[0]
"This is in direct contradiction to the few previously reported results on this matter, which can in most cases be explained by our increased model capacity.",5.1 Character-level translation,[0],[0]
"The exception is GNMT (Wu et al., 2016), which had similar depth.",5.1 Character-level translation,[0],[0]
"In this case, possible explanations for the discrepancy include our use of a fully bidirectional encoder, our translating into English instead of German, and our modelspecific tuning of dropout.",5.1 Character-level translation,[0],[0]
"Character-level NMT systems have a more difficult sequence-modeling task, as they need to infer the meaning of words from their constituent characters, where models with larger tokens instead delegate this task to the embedding table.",5.1.1 Effect of model capacity,[0],[0]
"Therefore, we hypothesize that increasing the model’s capacity by adding layers will have a greater impact on character-level models.",5.1.1 Effect of model capacity,[0],[0]
Figure 1 tests this hypothesis by measuring the impact of three model sizes on test BLEU score.,5.1.1 Effect of model capacity,[0],[0]
"For each of our four language pairs, the word-fragment model starts out ahead, and quickly loses ground as architecture size increases.",5.1.1 Effect of model capacity,[0],[0]
"For the languages with greater morphological complexity—German, Czech and Finnish—the slope of the character model’s curve is notably steeper than that of the BPE system, indicating that these systems could benefit from yet more modeling capacity.",5.1.1 Effect of model capacity,[0],[0]
"One of the most compelling arguments for working with characters (and to a lesser extent, wordfragments) is improved generalization.",5.1.2 Effect of corpus size,[0],[0]
"Through morphological generalizations, the system can better handle low-frequency and previously unseen words.",5.1.2 Effect of corpus size,[0],[0]
"It stands to reason that as the training corpus increases in size, the importance of these generalization capabilities will decrease.",5.1.2 Effect of corpus size,[0],[0]
"We test this hypothesis by holding the language pair constant, and varying the training corpus size by downsampling the full training corpus.",5.1.2 Effect of corpus size,[0],[0]
We choose EnFr because it has by far the most available data.,5.1.2 Effect of corpus size,[0],[0]
"We compare four sizes: 2M, 4M, 14M and 40M.
The results are shown in Figure 2.",5.1.2 Effect of corpus size,[0],[0]
"As expected, the gap between character and word-fragment modeling decreases as corpus size increases.",5.1.2 Effect of corpus size,[0],[0]
"From the slopes of the curves, we can infer that the advantage of character-level modeling will disappear completely as we reach 60-70M sentence pairs.",5.1.2 Effect of corpus size,[0],[0]
"However, there is reason to expect this break-even
point to be much higher for more morphologically complex languages.",5.1.2 Effect of corpus size,[0],[0]
It is also important to recall that relatively few language-pairs can assemble parallel corpora of this size.,5.1.2 Effect of corpus size,[0],[0]
The performance advantage of working with characters comes at a significant computational cost.,5.1.3 Speed,[0],[0]
"With our full-sized architecture, character models trained roughly 8x more slowly than BPE models.6 Figure 3 shows that training time grows linearly with number of layers in the model, and that character models have a much higher per-layer cost: roughly 0.38 msec/sentence versus 0.04 for BPE.",5.1.3 Speed,[0],[0]
"We did not directly measure the difference in attention cost, but it cannot be greater than the difference in total cost for the smallest number of layers.",5.1.3 Speed,[0],[0]
"Therefore, we can infer from Figure 3 that processing 5 layers in a character model incurs roughly the same time cost as attention.",5.1.3 Speed,[0],[0]
"This is surprising given the quadratic cost of attention, and indicates that efforts to speed up character models cannot focus exclusively on attention.",5.1.3 Speed,[0],[0]
"To make a qualitative comparison between word fragments (BPE) and characters for NMT, we examined 100 randomly selected sentence pairs from the DeEn test set.",5.1.4 Qualitative comparison,[0],[0]
"One author examined the sentences, using a display that showed the source7 and the reference, along with the output of BPE and character models.",5.1.4 Qualitative comparison,[0],[0]
Any differences between the two outputs were highlighted.,5.1.4 Qualitative comparison,[0],[0]
"They then assigned tags to both system outputs indicating broad error categories, such as lexical choice, word order and German compound handling.8",5.1.4 Qualitative comparison,[0],[0]
"Tags were restricted to cases where one system made a mistake that the other did not.
",5.1.4 Qualitative comparison,[0],[0]
"Of the 100 sentences, 47 were annotated as being identical or of roughly the same quality.",5.1.4 Qualitative comparison,[0],[0]
The remaining 53 exhibited a large variety of differences.,5.1.4 Qualitative comparison,[0],[0]
Table 4 summarizes the errors that were most easily characterized.,5.1.4 Qualitative comparison,[0],[0]
"BPE and character sys-
6Recall that we use batches containing 16,384 tokens— corresponding to a fixed memory budget—for both character and BPE models.",5.1.4 Qualitative comparison,[0],[0]
"Thus character models are slowed not only by having longer sentences, but also by parallelizing across fewer sentences in each batch.
",5.1.4 Qualitative comparison,[0],[0]
7The annotating author does not speak German.,5.1.4 Qualitative comparison,[0],[0]
8Our,5.1.4 Qualitative comparison,[0],[0]
"annotator also looked specifically for agreement and negation errors, as studied by Sennrich (2017) for English-toGerman character-level NMT.",5.1.4 Qualitative comparison,[0],[0]
"However, neither system exhibited these error types with sufficient frequency to draw meaningful conclusions.
tems differ most in the number of lexical choice errors, and in the extent to which they drop content.",5.1.4 Qualitative comparison,[0],[0]
"The latter is surprising, and appears to be a side-effect of a general tendency of the character models to be more faithful to the source, verging on being overly literal.",5.1.4 Qualitative comparison,[0],[0]
"An example of dropped content is shown in Table 5 (top).
",5.1.4 Qualitative comparison,[0],[0]
"Regarding lexical choice, the two systems differ not only in the number of errors, but in the nature of those errors.",5.1.4 Qualitative comparison,[0],[0]
"In particular, the BPE model had more trouble handling German compound nouns.",5.1.4 Qualitative comparison,[0],[0]
"Table 5 (bottom) shows an example which exhibits two compound errors, includ-
ing one where the character system is a strict improvement, translating Bunsenbrenner into bunsen burner instead of bullets.",5.1.4 Qualitative comparison,[0],[0]
"The second error follows another common pattern, where both systems mishandle the German compound (Chemiestunden / chemistry lessons), but the character system fails in a more useful way.
",5.1.4 Qualitative comparison,[0],[0]
We also found that both systems occasionally mistranslate proper names.,5.1.4 Qualitative comparison,[0],[0]
"Both fail by attempting to translate when they should copy over, but the BPE system’s errors are harder to understand as they involve semantic translation, rendering Britta Hermann as Sir Leon, and Esme Nussbaum as smiling walnut.9",5.1.4 Qualitative comparison,[0],[0]
"The character system’s one observed error in this category was phonetic rather than semantic, rendering Schotten as Scottland.
",5.1.4 Qualitative comparison,[0],[0]
"Interestingly, we also observed several instances where the model correctly translates the German 24-hour clock into the English 12-hour clock; for example, 19.30 becomes 7:30 p.m..",5.1.4 Qualitative comparison,[0],[0]
"This deterministic transformation is potentially in reach for both models, but we observed it only for the character system in this sample.
9",5.1.4 Qualitative comparison,[0],[0]
The BPE segmentations for these names were: _Britt a _,5.1.4 Qualitative comparison,[0],[0]
Herr mann and _Es me _,5.1.4 Qualitative comparison,[0],[0]
N uss baum,5.1.4 Qualitative comparison,[0],[0]
"At this point we have established that characterlevel NMT benefits translation quality, but incurs a large computational cost.",5.2 Compressing the Source Sequence,[0],[0]
"In this section, we evaluate the speed-quality tradeoffs of various techniques for reducing the number of state vectors required to represent the source sentence.",5.2 Compressing the Source Sequence,[0],[0]
"All experiments are conducted on our DeEn language pair, chosen for having a good balance of morphological complexity and training corpus size.",5.2 Compressing the Source Sequence,[0],[0]
Recall that BPE interpolates between word- and character-level processing by tokenizing consecutive characters into word fragments; larger BPE vocabulary sizes result in larger fragments and shorter sequences.,5.2.1 Optimizing the BPE vocabulary,[0],[0]
"If character-level models outperform BPE with a vocabulary size of 32k, then is there a smaller BPE vocabulary size that reaps the benefits of character-level processing, while still substantially reducing the sequence length?
",5.2.1 Optimizing the BPE vocabulary,[0],[0]
"To answer this question, we test a number of BPE vocabularies, as shown in Table 6.",5.2.1 Optimizing the BPE vocabulary,[0],[0]
"For each vocabulary, we measure BLEU and sequence compression rate, defined as the average size of the source sequence in characters divided by its size in word fragments (the ratio for the target sequence was similar).",5.2.1 Optimizing the BPE vocabulary,[0],[0]
"Unfortunately, even at just 1k vocabulary items, BPE has already lost a BLEU point with respect to the character model.",5.2.1 Optimizing the BPE vocabulary,[0],[0]
"When comparing these results to the other methods in this section, it is important to recall that BPE is compressing both the source and target sequence (by approximately the same amount), doubling its effective compression rate.",5.2.1 Optimizing the BPE vocabulary,[0],[0]
The goal of these experiments is to determine whether using fixed schedule compression is a feasible alternative to BPE.,5.2.2 Fixed Stride Compression,[0],[0]
"We evaluate our reimplementation of the pooling model of Lee et al. (2017) and our pooled BiLSTM encoder, both described in Section 3.3.",5.2.2 Fixed Stride Compression,[0],[0]
"For the pooled BiLSTM encoder, development experiments led us to introduce two mean-pooling layers, a stride 3 layer after the second BiLSTM, and a stride 2 layer after the third.",5.2.2 Fixed Stride Compression,[0],[0]
"Therefore, the final output of the encoder is compressed by a factor of 6.
",5.2.2 Fixed Stride Compression,[0],[0]
The results are also shown in Table 6.,5.2.2 Fixed Stride Compression,[0],[0]
"Note that for the pooled BiLSTM, different encoder layers have different lengths: 2 full length layers, followed by 1 at 13 length and 3 at 1 6 length.",5.2.2 Fixed Stride Compression,[0],[0]
"Therefore, we report the average compression across layers here and for the HM in Section 5.2.3.
",5.2.2 Fixed Stride Compression,[0],[0]
"Our implementation of Lee et al. (2017) outperforms the original results by more than 2 BLEU
points.",5.2.2 Fixed Stride Compression,[0],[0]
We suspect most of these gains result from better optimization of the model with large batch training.,5.2.2 Fixed Stride Compression,[0],[0]
"However, our attempts to scale this encoder to larger depths, and therfore to the level of performance exhibited by our other systems, did not result in any significant improvements.",5.2.2 Fixed Stride Compression,[0],[0]
"This is possibly due to difficulties with optimizing a deeper stack of diverse layers.
",5.2.2 Fixed Stride Compression,[0],[0]
"Comparing the performance of our Pooled BiLSTM model against BPE, we notice that for a comparable level of compression (BPE size of 1k), BPE out-performs the pooled model by around 0.5 BLEU points.",5.2.2 Fixed Stride Compression,[0],[0]
"At a similar level of performance (BPE size of 4k), BPE has significantly shorter sequences.",5.2.2 Fixed Stride Compression,[0],[0]
"Although fixed-stride pooling does not yet match the performance of BPE, we remain optimistic about its potential.",5.2.2 Fixed Stride Compression,[0],[0]
"The appeal of these models derives from their simplicity; they are easy to optimize, perform reasonably well, and remove the complication of BPE preprocessing.",5.2.2 Fixed Stride Compression,[0],[0]
"We experimented with using the Hierarchical Multiscale (HM; Section 3.4.1) architecture to learn compression decisions for the encoder.
",5.2.3 Hierarchical Multiscale Compression,[0],[0]
"For initial exploration, we used a scaled-down architecture consisting of 3 unidirectional HM encoder layers and 2 LSTM decoder layers, attending over the HM’s gated output module.",5.2.3 Hierarchical Multiscale Compression,[0],[0]
Comparisons to an equivalent LSTM are shown in table 7.,5.2.3 Hierarchical Multiscale Compression,[0],[0]
"The first two HM lines justify the no-flush and hierarchical modifications described in Section 3.4.1, yielding incremental gains of 27.3 (the flush variant failed to converge), and 1.2 respectively.",5.2.3 Hierarchical Multiscale Compression,[0],[0]
"Initializing z-bias to 1 and annealing the slope of the hard binarizer from 1.0 to 5.0 over 80k minibatches gave further small gains, bringing the HM to parity with the LSTM while saving approximately 35% of layer-wise computations.",5.2.3 Hierarchical Multiscale Compression,[0],[0]
"Interestingly, we found that, over a wide range of training conditions, each layer tended to reduce computa-
tion by roughly 60% relative to the layer below.10
For full-scale experiments, we stacked 5 BiLSTM layers on top of 2 or 3 HM layers, as described in section 3.4.1, using only the top HM layer (rather than the gated output module) as input to the lowest BiLSTM layer.",5.2.3 Hierarchical Multiscale Compression,[0],[0]
"To stabilize the 3- HM configuration we used a compression penalty with a weight of 2, and α1 and α2 of 0.1 and 0.9.",5.2.3 Hierarchical Multiscale Compression,[0],[0]
"Given the tendency of HM layers to reduce computation by a roughly constant proportion, we expect fewer z-gates to be open in the 3-HM configuration, but this is achieved at the cost of one extra layer relative to our standard 12-layer encoder.",5.2.3 Hierarchical Multiscale Compression,[0],[0]
"As shown in table 6, the 3-HM configuration achieves much better compression even when this is accounted for, and also gives slightly better performance than 2-HM.",5.2.3 Hierarchical Multiscale Compression,[0],[0]
"In general, HM gating results in less compression but better performance than the fixed-stride techniques.
",5.2.3 Hierarchical Multiscale Compression,[0],[0]
"Although these preliminary results are promising, it should be emphasized that the speed gains they demonstrate are conceptual, and that realizing them in practice comes with significant engineering challenges.",5.2.3 Hierarchical Multiscale Compression,[0],[0]
We have demonstrated the translation quality of standard NMT architectures operating at the character-level.,6 Conclusion,[0],[0]
"Our experiments show the surprising result that character NMT can substantially out-perform BPE tokenization for all but the largest training corpora sizes, and the less surprising result that doing so incurs a large computational cost.",6 Conclusion,[0],[0]
"To address this cost, we have explored a number of methods for source-sequence compression, including the first application of the Hierarchical Multiscale LSTM to NMT, which allows us to learn to dynamically compress the source sequence.
",6 Conclusion,[0],[0]
We intend this paper as a call to action.,6 Conclusion,[0],[0]
"Character-level translation is well worth doing, but we do not yet have the necessary techniques to benefit from this quality boost without suffering a disproportionate reduction in speed.",6 Conclusion,[0],[0]
"We hope that these results will spur others to revisit the question of character-level translation as an interesting testbed for methods that can learn to process, summarize or compress long sequences.
",6 Conclusion,[0],[0]
"10For instance, the 2nd and 3rd layer of the best configuration shown had on average 60% and 36% of z gates open, yielding the computation ratio of (1+0.6+0.36)/3 = 0.65.",6 Conclusion,[0],[0]
"Translating characters instead of words or word-fragments has the potential to simplify the processing pipeline for neural machine translation (NMT), and improve results by eliminating hyper-parameters and manual feature engineering.",abstractText,[0],[0]
"However, it results in longer sequences in which each symbol contains less information, creating both modeling and computational challenges.",abstractText,[0],[0]
"In this paper, we show that the modeling problem can be solved by standard sequence-to-sequence architectures of sufficient depth, and that deep models operating at the character level outperform identical models operating over word fragments.",abstractText,[0],[0]
This result implies that alternative architectures for handling character input are better viewed as methods for reducing computation time than as improved ways of modeling longer sequences.,abstractText,[0],[0]
"From this perspective, we evaluate several techniques for characterlevel NMT, verify that they do not match the performance of our deep character baseline model, and evaluate the performance versus computation time tradeoffs they offer.",abstractText,[0],[0]
"Within this framework, we also perform the first evaluation for NMT of conditional computation over time, in which the model learns which timesteps can be skipped, rather than having them be dictated by a fixed schedule specified before training begins.",abstractText,[0],[0]
Revisiting Character-Based Neural Machine Translation with Capacity and Compression,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4743–4751 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
4743",text,[0],[0]
"In this paper, we explore the effectiveness of methods designed to improve sentiment classification (positive vs. negative) of sentences that contain complex syntactic structures.",1 Introduction,[0],[0]
"While simple bag-of-words or lexicon-based methods (Pang and Lee, 2005; Wang and Manning, 2012; Iyyer et al., 2015) achieve good performance on this task, they are unequipped to deal with syntactic structures that affect sentiment, such as contrastive conjunctions (i.e., sentences of the form “A-but-B”) or negations.",1 Introduction,[0],[0]
"Neural models that explicitly encode word order (Kim, 2014), syntax (Socher et al., 2013; Tai et al., 2015) and semantic features (Li et al., 2017) have been proposed with the aim of improving performance on these more complicated sentences.",1 Introduction,[0],[0]
"Recently, Hu et al. (2016) incorporate logical rules into a neural model and
show that these rules increase the model’s accuracy on sentences containing contrastive conjunctions, while Peters et al. (2018a) demonstrate increased overall accuracy on sentiment analysis by initializing a model with representations from a language model trained on millions of sentences.
",1 Introduction,[0],[0]
"In this work, we carry out an in-depth study of the effectiveness of the techniques in Hu et al. (2016) and Peters et al. (2018a) for sentiment classification of complex sentences.",1 Introduction,[0],[0]
"Part of our contribution is to identify an important gap in the methodology used in Hu et al. (2016) for performance measurement, which is addressed by averaging the experiments over several executions.",1 Introduction,[0],[0]
"With the averaging in place, we obtain three key findings: (1) the improvements in Hu et al. (2016) can almost entirely be attributed to just one of their two proposed mechanisms and are also less pronounced than previously reported; (2) contextualized word embeddings (Peters et al., 2018a) incorporate the “A-but-B” rules more effectively without explicitly programming for them; and (3) an analysis using crowdsourcing reveals a bigger picture where the errors in the automated systems have a striking correlation with the inherent sentiment-ambiguity in the data.",1 Introduction,[0],[0]
Here we briefly review background from Hu et al. (2016) to provide a foundation for our reanalysis in the next section.,2 Logic Rules in Sentiment Classification,[0],[0]
We focus on a logic rule for sentences containing an “A-but-B” structure (the only rule for which Hu et al. (2016) provide experimental results).,2 Logic Rules in Sentiment Classification,[0],[0]
"Intuitively, the logic rule for such sentences is that the sentiment associated with the whole sentence should be the same as the sentiment associated with phrase “B”.1
1The rule is vacuously true if the sentence does not have this structure.
",2 Logic Rules in Sentiment Classification,[0],[0]
"More formally, let pθ(y|x) denote the probability assigned to the label y ∈ {+,−} for an input x by the baseline model using parameters θ.",2 Logic Rules in Sentiment Classification,[0],[0]
"A logic rule is (softly) encoded as a variable rθ(x, y) ∈",2 Logic Rules in Sentiment Classification,[0],[0]
"[0, 1] indicating how well labeling x with y satisfies the rule.",2 Logic Rules in Sentiment Classification,[0],[0]
"For the case of A-but-B sentences, rθ(x, y) = pθ(y|B) if x has the structure A-but-B (and 1 otherwise).",2 Logic Rules in Sentiment Classification,[0],[0]
"Next, we discuss the two techniques from Hu et al. (2016) for incorporating rules into models: projection, which directly alters a trained model, and distillation, which progressively adjusts the loss function during training.
Projection.",2 Logic Rules in Sentiment Classification,[0],[0]
"The first technique is to project a trained model into a rule-regularized subspace, in a fashion similar to Ganchev et al. (2010).",2 Logic Rules in Sentiment Classification,[0],[0]
"More precisely, a given model pθ is projected to a model qθ defined by the optimum value of q in the following optimization problem:2
min q,ξ≥0 KL(q(X,Y )||pθ(X,Y ))",2 Logic Rules in Sentiment Classification,[0],[0]
+,2 Logic Rules in Sentiment Classification,[0],[0]
C ∑,2 Logic Rules in Sentiment Classification,[0],[0]
"x∈X ξx
s.t.",2 Logic Rules in Sentiment Classification,[0],[0]
"(1− Ey←q(·|x)[rθ(x, y)]) ≤",2 Logic Rules in Sentiment Classification,[0],[0]
"ξx
Here q(X,Y ) denotes the distribution of (x, y) when x is drawn uniformly from the set X and y is drawn according to q(·|x).
",2 Logic Rules in Sentiment Classification,[0],[0]
Iterative Rule Knowledge Distillation.,2 Logic Rules in Sentiment Classification,[0],[0]
The second technique is to transfer the domain knowledge encoded in the logic rules into a neural network’s parameters.,2 Logic Rules in Sentiment Classification,[0],[0]
"Following Hinton et al. (2015), a “student” model pθ can learn from the “teacher” model qθ, by using a loss function πH(pθ, Ptrue)+ (1− π)H(pθ, qθ) during training, where Ptrue denotes the distribution implied by the ground truth, H(·, ·) denotes the cross-entropy function, and π is a hyperparameter.",2 Logic Rules in Sentiment Classification,[0],[0]
"Hu et al. (2016) computes qθ after every gradient update by projecting the current pθ, as described above.",2 Logic Rules in Sentiment Classification,[0],[0]
"Note that both mechanisms can be combined: After fully training pθ using the iterative distillation process above, the projection step can be applied one more time to obtain qθ which is then used as the trained model.
",2 Logic Rules in Sentiment Classification,[0],[0]
Dataset.,2 Logic Rules in Sentiment Classification,[0],[0]
"All of our experiments (as well as those in Hu et al. (2016)) use the SST2 dataset, a
2The formulation in Hu et al. (2016) includes another hyperparameter λ per rule, to control its relative importance; when there is only one rule, as in our case, this parameter can be absorbed into C.
binarized subset of the popular Stanford Sentiment Treebank (SST) (Socher et al., 2013).",2 Logic Rules in Sentiment Classification,[0],[0]
"The dataset includes phrase-level labels in addition to sentence-level labels (see Table 1 for detailed statistics); following Hu et al. (2016), we use both types of labels for the comparisons in Section 3.2.",2 Logic Rules in Sentiment Classification,[0],[0]
"In all other experiments, we use only sentencelevel labels, and our baseline model for all experiments is the CNN architecture from Kim (2014).",2 Logic Rules in Sentiment Classification,[0],[0]
In this section we reanalyze the effectiveness of the techniques of Hu et al. (2016) and find that most of the performance gain is due to projection and not knowledge distillation.,3 A Reanalysis,[0],[0]
The discrepancy with the original analysis can be attributed to the relatively small dataset and the resulting variance across random initializations.,3 A Reanalysis,[0],[0]
We start by analyzing the baseline CNN by Kim (2014) to point out the need for an averaged analysis.,3 A Reanalysis,[0],[0]
"We run the baseline CNN by Kim (2014) across 100 random seeds, training on sentence-level la-
bels.",3.1 Importance of Averaging,[0],[0]
"We observe a large amount of variation from run-to-run, which is unsurprising given the small dataset size.",3.1 Importance of Averaging,[0],[0]
"The inset density plot in Figure 1 shows the range of accuracies (83.47 to 87.20) along with 25, 50 and 75",3.1 Importance of Averaging,[0],[0]
"percentiles.3 The figure also shows how the variance persists even after the average converges: the accuracies of 100 models trained for 20 epochs each are plotted in gray, and their average is shown in red.
",3.1 Importance of Averaging,[0],[0]
"We conclude that, to be reproducible, only averaged accuracies should be reported in this task and dataset.",3.1 Importance of Averaging,[0],[0]
This mirrors the conclusion from a detailed analysis by Reimers and Gurevych (2017) in the context of named entity recognition.,3.1 Importance of Averaging,[0],[0]
We carry out an averaged analysis of the publicly available implementation4 of Hu et al. (2016).,3.2 Performance of Hu et al. (2016),[0],[0]
Our analysis reveals that the reported performance of their two mechanisms (projection and distillation) is in fact affected by the high variability across random seeds.,3.2 Performance of Hu et al. (2016),[0],[0]
"Our more robust averaged analysis yields a somewhat different conclusion of their effectiveness.
",3.2 Performance of Hu et al. (2016),[0],[0]
"In Figure 2, the first two columns show the reported accuracies in Hu et al. (2016) for models trained with and without distillation (corresponding to using values π = 1 and π = 0.95t in the tth epoch, respectively).",3.2 Performance of Hu et al. (2016),[0],[0]
The two rows show the results for models with and without a final projection into the rule-regularized space.,3.2 Performance of Hu et al. (2016),[0],[0]
"We keep our hyper-parameters identical to Hu et al. (2016).5
The baseline system (no-project, no-distill) is identical to the system of Kim (2014).",3.2 Performance of Hu et al. (2016),[0],[0]
"All the systems are trained on the phrase-level SST2 dataset
3We use early stopping based on validation performance for all models in the density plot.
",3.2 Performance of Hu et al. (2016),[0],[0]
"4https://github.com/ZhitingHu/logicnn/ 5In particular, C = 6 for projection.
with early stopping on the development set.",3.2 Performance of Hu et al. (2016),[0],[0]
The number inside each arrow indicates the improvement in accuracy by adding either the projection or the distillation component to the training algorithm.,3.2 Performance of Hu et al. (2016),[0],[0]
"Note that the reported figures suggest that while both components help in improving accuracy, the distillation component is much more helpful than the projection component.
",3.2 Performance of Hu et al. (2016),[0],[0]
"The next two columns, which show the results of repeating the above analysis after averaging over 100 random seeds, contradict this claim.",3.2 Performance of Hu et al. (2016),[0],[0]
"The averaged figures show lower overall accuracy increases, and, more importantly, they attribute these improvements almost entirely to the projection component rather than the distillation component.",3.2 Performance of Hu et al. (2016),[0],[0]
"To confirm this result, we repeat our averaged analysis restricted to only “A-but-B” sentences targeted by the rule (shown in the last two columns).",3.2 Performance of Hu et al. (2016),[0],[0]
"We again observe that the effect of projection is pronounced, while distillation offers little or no advantage in comparison.",3.2 Performance of Hu et al. (2016),[0],[0]
"Traditional context-independent word embeddings like word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014) are fixed vectors for every word in the vocabulary.",4 Contextualized Word Embeddings,[0],[0]
"In contrast, contextualized embeddings are dynamic representations, dependent on the current context of the word.",4 Contextualized Word Embeddings,[0],[0]
We hypothesize that contextualized word embeddings might inherently capture these logic rules due to increasing the effective context size for the CNN layer in Kim (2014).,4 Contextualized Word Embeddings,[0],[0]
"Following the recent success of ELMo (Peters et al., 2018a) in sentiment analysis, we utilize the TensorFlow Hub implementation of ELMo6 and feed these contextualized embeddings into our CNN model.",4 Contextualized Word Embeddings,[0],[0]
"We
6https://tfhub.dev/google/elmo/1
fine-tune the ELMo LSTM weights along with the CNN weights on the downstream CNN task.",4 Contextualized Word Embeddings,[0],[0]
"As in Section 3, we check performance with and without the final projection into the rule-regularized space.",4 Contextualized Word Embeddings,[0],[0]
"We present our results in Table 2.
",4 Contextualized Word Embeddings,[0],[0]
"Switching to ELMo word embeddings improves performance by 2.9 percentage points on an average, corresponding to about 53 test sentences.",4 Contextualized Word Embeddings,[0],[0]
"Of these, about 32 sentences (60% of the improvement) correspond to A-but-B and negation style sentences, which is substantial when considering that only 24.5% of test sentences include these discourse relations (Table 1).",4 Contextualized Word Embeddings,[0],[0]
"As further evidence that ELMo helps on these specific constructions, the non-ELMo baseline model (no-project, no-distill) gets 255 sentences wrong in the test corpus on average, only 89 (34.8%) of which are A-but-B style or negations.
",4 Contextualized Word Embeddings,[0],[0]
"Statistical Significance: Using a two-sided Kolmogorov-Smirnov statistic (Massey Jr, 1951) with α = 0.001 for the results in Table 2, we find that ELMo and projection each yield statistically significant improvements, but distillation does not.",4 Contextualized Word Embeddings,[0],[0]
"Also, with ELMo, projection is not significant.",4 Contextualized Word Embeddings,[0],[0]
"Specific comparisons have been added in the Appendix, in Table A3.
KL Divergence Analysis: We observe no significant gains by projecting a trained ELMo model into an A-but-B rule-regularized space, unlike the other models.",4 Contextualized Word Embeddings,[0],[0]
"We confirm that ELMo’s predictions are much closer to the A-but-B rule’s manifold than those of the other models by computing KL(qθ||pθ) where pθ and qθ are the original and projected distributions: Averaged across all A-butB sentences and 100 seeds, this gives 0.27, 0.26 and 0.13 for the Kim (2014), Hu et al. (2016) with distillation and ELMo systems respectively.
",4 Contextualized Word Embeddings,[0],[0]
"Intra-sentence Similarity: To understand the information captured by ELMo embeddings for A-but-B sentences, we measure the cosine similarity between the word vectors of every pair of words within the A-but-B sentence (Peters et al., 2018b).",4 Contextualized Word Embeddings,[0],[0]
"We compare the intra-sentence similarity for fine-tuned word2vec embeddings (baseline), ELMo embeddings without fine-tuning and finally fine-tuned ELMo embeddings in Figure 3.",4 Contextualized Word Embeddings,[0],[0]
"In the fine-tuned ELMo embeddings, we notice the words within the A and within the B part of the A-but-B sentence share the same part of the vector space.",4 Contextualized Word Embeddings,[0],[0]
"This pattern is less visible in the
ELMo embeddings without fine-tuning and absent in the word2vec embeddings.",4 Contextualized Word Embeddings,[0],[0]
This observation is indicative of ELMo’s ability to learn specific rules for A-but-B sentences in sentiment classification.,4 Contextualized Word Embeddings,[0],[0]
More intra-sentence similarity heatmaps for A-but-B sentences are in Figure A1.,4 Contextualized Word Embeddings,[0],[0]
We conduct a crowdsourced analysis that reveals that SST2 data has significant levels of ambiguity even for human labelers.,5 Crowdsourced Experiments,[0],[0]
"We discover that ELMo’s performance improvements over the baseline are robust across varying levels of ambiguity, whereas the advantage of Hu et al. (2016) is reversed in sentences of low ambiguity (restricting to A-but-B style sentences).
",5 Crowdsourced Experiments,[0],[0]
"Our crowdsourced experiment was conducted on Figure Eight.8 Nine workers scored the sentiment of each A-but-B and negation sentence in the test SST2 split as 0 (negative), 0.5 (neutral) or 1 (positive).",5 Crowdsourced Experiments,[0],[0]
(SST originally had three crowdworkers choose a sentiment rating from 1 to 25 for every phrase.),5 Crowdsourced Experiments,[0],[0]
More details regarding the crowd experiment’s parameters have been provided in,5 Crowdsourced Experiments,[0],[0]
"Appendix A.
We average the scores across all users for each sentence.",5 Crowdsourced Experiments,[0],[0]
"Sentences with a score in the range (x, 1] are marked as positive (where x ∈",5 Crowdsourced Experiments,[0],[0]
"[0.5, 1)), sentences in [0, 1 − x) marked as negative, and sentences in [1 − x, x] are marked as neutral.",5 Crowdsourced Experiments,[0],[0]
"For instance, “flat , but with a revelatory performance by michelle williams” (score=0.56) is neutral when x",5 Crowdsourced Experiments,[0],[0]
= 0.6.9,5 Crowdsourced Experiments,[0],[0]
We present statistics of our dataset10 in Table 3.,5 Crowdsourced Experiments,[0],[0]
"Inter-annotator agree-
7Trained on sentences and not phrase-level labels for a fair comparison with baseline and ELMo, unlike Section 3.2.
",5 Crowdsourced Experiments,[0],[0]
"8 https://www.figure-eight.com/ 9More examples of neutral sentences have been provided in the Appendix in Table A1, as well as a few “flipped” sentences receiving an average score opposite to their SST2 label (Table A2).
",5 Crowdsourced Experiments,[0],[0]
"10The dataset along with source code can be found in
ment was computed using Fleiss’ Kappa (κ).",5 Crowdsourced Experiments,[0],[0]
"As expected, inter-annotator agreement is higher for higher thresholds (less ambiguous sentences).",5 Crowdsourced Experiments,[0],[0]
"According to Landis and Koch (1977), κ ∈ (0.2, 0.4] corresponds to “fair agreement”, whereas κ ∈ (0.4, 0.6] corresponds to “moderate agreement”.
",5 Crowdsourced Experiments,[0],[0]
We next compute the accuracy of our model for each threshold by removing the corresponding neutral sentences.,5 Crowdsourced Experiments,[0],[0]
Higher thresholds correspond to sets of less ambiguous sentences.,5 Crowdsourced Experiments,[0],[0]
Table 3 shows that ELMo’s performance gains in Table 2 extends across all thresholds.,5 Crowdsourced Experiments,[0],[0]
In Figure 4 we compare all the models on the A-but-B sentences in this set.,5 Crowdsourced Experiments,[0],[0]
"Across all thresholds, we notice trends similar to previous sections: 1) ELMo performs the best among all models on A-but-B style sentences, and projection results in only a slight improvement; 2) models in Hu et al. (2016) (with and without distillation) benefit considerably from projection; but 3) distillation offers little improvement (with or without projection).",5 Crowdsourced Experiments,[0],[0]
"Also, as the ambiguity threshold increases, we see decreasing gains from projection on all models.",5 Crowdsourced Experiments,[0],[0]
"In fact, beyond the 0.85 threshold, projection degrades the average performance, indicating that projection is useful for more ambiguous sentences.",5 Crowdsourced Experiments,[0],[0]
We present an analysis comparing techniques for incorporating logic rules into sentiment classification systems.,6 Conclusion,[0],[0]
"Our analysis included a metastudy highlighting the issue of stochasticity in performance across runs and the inherent ambiguity in the sentiment classification task itself, which was tackled using an averaged analysis and
https://github.com/martiansideofthemoon/ logic-rules-sentiment.
a crowdsourced experiment identifying ambiguous sentences.",6 Conclusion,[0],[0]
"We present evidence that a recently proposed contextualized word embedding model (ELMo) (Peters et al., 2018a) implicitly learns logic rules for sentiment classification of complex sentences like A-but-B sentences.",6 Conclusion,[0],[0]
Future work includes a fine-grained quantitative study of ELMo word vectors for logically complex sentences along the lines of Peters et al. (2018b).,6 Conclusion,[0],[0]
"Crowd workers residing in five English-speaking countries (United States, United Kingdom, New Zealand, Australia and Canada) were hired.",A Crowdsourcing Details,[0],[0]
"Each crowd worker had a Level 2 or higher rating on Figure Eight, which corresponds to a “group of more experienced, higher accuracy contributors”.",A Crowdsourcing Details,[0],[0]
Each contributor had to pass a test questionnaire to be eligible to take part in the experiment.,A Crowdsourcing Details,[0],[0]
Test questions were also hidden throughout the task and untrusted contributions were removed from the final dataset.,A Crowdsourcing Details,[0],[0]
"For greater quality control, an upper limit of 75 judgments per contributor was enforced.",A Crowdsourcing Details,[0],[0]
Crowd workers were paid a total of $1 for 50 judgments.,A Crowdsourcing Details,[0],[0]
"An internal unpaid workforce (including the first and second author of the paper) of 7 contributors was used to speed up data collection.
",A Crowdsourcing Details,[0],[0]
"# Judgments Average Sentence Positive Negative Neutral
1 1 7 0.50 the fight scenes are fun , but it grows tedious
3 2 4 0.56 it ’s not exactly a gourmet meal but the fare is fair , even coming from the drive thru
2 3 4 0.44 propelled not by characters but by caricatures
4 2 3 0.61 not everything works , but the average is higher than in mary and most other recent comedies
Table A1:",A Crowdsourcing Details,[0],[0]
"Examples of neutral sentences for a threshold of 0.66
# Judgments Average Original Sentence Positive Negative Neutral
1 5 3 0.28 Positive de niro and mcdormand give solid performances , but their screen time is sabotaged by the story ’s inability to create interest
6 0 3 0.83 Negative son of the bride may be a good half hour too long but comes replete with a flattering sense of mystery and quietness
0 5 4 0.22 Positive wasabi is slight fare indeed , with the entire project having the feel of something tossed off quickly ( like one of hubert ’s punches ) , but it should go down smoothly enough with popcorn
Table A2: Examples of flipped sentiment sentences, for a threshold of 0.66
Model 1 vs Model 2 Significant
distill no-project distill project",A Crowdsourcing Details,[0],[0]
"Yes no-distill no-project no-distill project Yes
ELMo no-project ELMo project No
no-distill no-project distill no-project No no-distill project distill project",A Crowdsourcing Details,[0],[0]
"No
no-distill no-project ELMo no-project",A Crowdsourcing Details,[0],[0]
Yes distill no-project ELMo no-project,A Crowdsourcing Details,[0],[0]
Yes no-distill project ELMo project,A Crowdsourcing Details,[0],[0]
"Yes distill project ELMo project Yes
Table A3: Statistical significance using a two-sided Kolmogorov-Smirnov statistic (Massey Jr, 1951) with α = 0.001.
",A Crowdsourcing Details,[0],[0]
"al l en ds we ll , so rt of , bu t th e fre nz
ie d
co m ic m
om en ts ne ve r cli ck
all ends well
, sort
of ,
but the frenzied comic
moments never click 0.1
0.0
0.1
",A Crowdsourcing Details,[0],[0]
"0.2
0.3
0.4
al l en ds we ll , so rt of , bu t th e fre nz
ie d
co m ic m
om en ts ne ve r cli ck
all ends well
, sort
of ,
but the frenzied comic
moments never click 0.1
0.2
0.3
0.4
0.5
0.6
al l en ds we ll , so rt of , bu t th e fre nz
ie d
co m ic m
om en ts ne ve r cli ck
all ends well
, sort
of ,
but the frenzied comic
moments never click
0.1
0.2
0.3
0.4
0.5
m ar isa to m ei is go od ,",A Crowdsourcing Details,[0],[0]
"bu t ju st a ki ss is ju st a m es s
marisa tomei
is good
, but just
a kiss
is just
a mess 0.1
0.0
0.1
0.2
0.3
0.4
m ar isa to m ei is go od , bu t ju st a ki ss is ju st a m es s marisa tomei
is good
, but just
a kiss
is just
a mess
0.2
0.3
0.4
",A Crowdsourcing Details,[0],[0]
"0.5
0.6
m ar isa to m ei is go",A Crowdsourcing Details,[0],[0]
od,A Crowdsourcing Details,[0],[0]
", bu t ju st a ki ss is ju st a m es s
marisa tomei
is good
, but just
a kiss
is just
a mess 0.0
0.1
0.2
0.3
0.4
0.5
0.6
",A Crowdsourcing Details,[0],[0]
"th e irw in s em
er ge
un sc
at he d ,",A Crowdsourcing Details,[0],[0]
"bu t th e fic tio na l fo ot ag e is un co nv
in cin g",A Crowdsourcing Details,[0],[0]
an d cr,A Crowdsourcing Details,[0],[0]
"im in al ly ba dl y ac te d
the irwins emerge unscathed
, but the fictional footage
is unconvincing
and criminally
badly acted
0.1
0.0
0.1
0.2
0.3
th e irw in s em
er ge
un sc
at he d ,",A Crowdsourcing Details,[0],[0]
"bu t th e fic tio na l fo ot ag e is un co nv
in cin g",A Crowdsourcing Details,[0],[0]
an d cr,A Crowdsourcing Details,[0],[0]
"im in al ly ba dl y ac te d
the irwins emerge unscathed
, but the fictional footage
is unconvincing
and criminally
badly acted 0.1
0.2
0.3
0.4
0.5
0.6
th e irw in s em
er ge
un sc
at he d , bu t th e fic tio",A Crowdsourcing Details,[0],[0]
na,A Crowdsourcing Details,[0],[0]
"l fo ot ag e is un co nv
in cin g",A Crowdsourcing Details,[0],[0]
an d cr,A Crowdsourcing Details,[0],[0]
"im in al ly ba dl y ac te d
the irwins emerge unscathed
, but the fictional footage
is unconvincing
and criminally
badly acted
0.1
0.2
0.3
0.4
0.5
Figure A1:",A Crowdsourcing Details,[0],[0]
Heat map showing the cosine similarity between pairs of word vectors within a single sentence.,A Crowdsourcing Details,[0],[0]
"The leftmost column has word2vec (Mikolov et al., 2013) embeddings, fine-tuned on the downstream task (SST2).",A Crowdsourcing Details,[0],[0]
"The middle column contains the original ELMo embeddings (Peters et al., 2018a) without any fine-tuning.",A Crowdsourcing Details,[0],[0]
The representations from the three layers (token layer and two LSTM layers) have been averaged.,A Crowdsourcing Details,[0],[0]
The rightmost column contains ELMo embeddings fine-tuned on the downstream task.,A Crowdsourcing Details,[0],[0]
"For better visualization, the cosine similarity between identical words has been set equal to the minimum value in the map.",A Crowdsourcing Details,[0],[0]
We analyze the performance of different sentiment classification models on syntacticallycomplex inputs like A-but-B sentences.,abstractText,[0],[0]
"The first contribution of this analysis addresses reproducible research: to meaningfully compare different models, their accuracies must be averaged over far more random seeds than what has traditionally been reported.",abstractText,[0],[0]
"With proper averaging in place, we notice that the distillation model described in Hu et al. (2016), which incorporates explicit logic rules for sentiment classification, is ineffective.",abstractText,[0],[0]
"In contrast, using contextualized ELMo embeddings (Peters et al., 2018a) instead of logic rules yields significantly better performance.",abstractText,[0],[0]
"Additionally, we provide analysis and visualizations that demonstrate ELMo’s ability to implicitly learn logic rules.",abstractText,[0],[0]
"Finally, a crowdsourced analysis reveals how ELMo outperforms baseline models even on sentences with ambiguous sentiment labels.",abstractText,[0],[0]
Revisiting the Importance of Encoding Logic Rules in Sentiment Classification,title,[0],[0]
"Back-propagation through time (BPTT) (Werbos, 1990) is nowadays the standard approach for training recurrent neural networks (RNNs).",1. Introduction,[0],[0]
"However, the computation and memory cost of BPTT scale linearly with the number of steps which makes BPTT impractical for applications where long sequences are common (Sutskever et al., 2014; Goodfellow
*Equal contribution 1Department of Computer Science, University of Toronto 2Uber ATG Toronto 3Vector Institute 4Department of Electrical and Computer Engineering, Rice University 5Department of Neuroscience, Baylor College of Medicine 6Canadian Institute for Advanced Research.",1. Introduction,[0],[0]
"Correspondence to: Renjie Liao <rjliao@cs.toronto.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"et al., 2016).",1. Introduction,[0],[0]
"Moreover, as the number of unrolling steps increases, the numerical error accumulates which may render the algorithm useless in some applications, e.g., gradientbased hyperparameter optimization (Maclaurin et al., 2015).",1. Introduction,[0],[0]
This issue is often solved in practice by using truncated back-propagation through time (TBPTT),1. Introduction,[0],[0]
"(Williams & Peng, 1990; Sutskever, 2013) which has constant computation and memory cost, is simple to implement, and effective in some applications.",1. Introduction,[0],[0]
"However, the quality of the TBPTT approximate gradient is not well understood.",1. Introduction,[0],[0]
"A natural question to ask is, can we get better gradient approximations while still using the same computational cost as TBPTT?
",1. Introduction,[0],[0]
"Here will show that under certain conditions on the underlying model, the answer is positive.",1. Introduction,[0],[0]
"In particular, we consider a class of RNNs whose hidden state converges to a steady state.",1. Introduction,[0],[0]
"For this class of RNNs, we can bypass BPTT and compute the exact gradient using an algorithm called recurrent back-propagation (RBP) (Almeida, 1987; Pineda, 1987).",1. Introduction,[0],[0]
The key observation exploited by RBP is that the gradient of the steady state w.r.t.,1. Introduction,[0],[0]
"the learnable parameters can be directly computed using the implicit function theorem, alleviating the need to unroll the entire forward pass.",1. Introduction,[0],[0]
The main computational cost of RBP is in solving a linear system which has constant memory and computation time w.r.t.,1. Introduction,[0],[0]
the number of unrolling steps.,1. Introduction,[0],[0]
"However, due to the strong assumptions that RBP imposes, TBPTT has become the standard approach used in practice and RBP did not get much attention for many years.
",1. Introduction,[0],[0]
"In this paper, we first revisit RBP in the context of modern deep learning.",1. Introduction,[0],[0]
"We discuss the original algorithm, the assumptions it imposes and how to satisfy them for deep neural networks.",1. Introduction,[0],[0]
"Second, we notice that although the fixed point iteration method used in (Almeida, 1987; Pineda, 1987) is guaranteed to converge if the steady hidden state is achievable, in practice it can fail to do so within a reasonable amount of steps.",1. Introduction,[0],[0]
This may be caused by the fact that there are many fixed points and the algorithm is sensitive to initialization.,1. Introduction,[0],[0]
We try to overcome the instability issue by proposing two variants of RBP based on conjugate gradient on normal equations (CG-RBP) and Neumann series (Neumann-RBP).,1. Introduction,[0],[0]
We show a connection between NeumannRBP and TBPTT which sheds some new light on the approximation quality of TBPTT.,1. Introduction,[0],[0]
"In the experiments, we show
several important applications which are naturally amenable to RBP.",1. Introduction,[0],[0]
"For example, we show how RBP can be used to back propagate thorough the optimization of deep neural networks in order to tune hyperparameters.",1. Introduction,[0],[0]
"Throughout our experiments, we found that Neumann-RBP not only inherits the advantages of original RBP but also remains consistently stable across different applications.",1. Introduction,[0],[0]
"In the context of neural networks, RBP was independently discovered by Almeida (Almeida, 1987) and Pineda (Pineda, 1987) in 1987, which is why this algorithm is sometimes called the Almeida-Pineda algorithm.",2. Related Work,[0],[0]
"Back then, RBP was shown to be useful in learning content-addressable memory (CAM) models (Hopfield, 1982; 1984) and other computational neurodynamic models (Lapedes & Farber, 1986; Haykin, 1993; Chauvin & Rumelhart, 1995).",2. Related Work,[0],[0]
These models are special RNNs in a sense that their inference stage is a convergent dynamic system by design.,2. Related Work,[0],[0]
"For these systems, one can construct a Lyapunov function for the underlying dynamics which further guarantees the asymptotic stability.",2. Related Work,[0],[0]
"We refer readers to chapter 13 of (Haykin, 1993) for more details on neurodynamic models.",2. Related Work,[0],[0]
"The goal of learning in these models is to manipulate the attractors, i.e. steady states, such that they are close to the input data.",2. Related Work,[0],[0]
"Therefore, during inference stage, even if the input data is corrupted, the corresponding correct attractor or “memory“ can still be retrieved.",2. Related Work,[0],[0]
"Instead of computing gradient via BPTT, RBP provides an more efficient alternative for manipulating the attractors.
",2. Related Work,[0],[0]
"RBP was later applied to learning graph neural networks (GNNs) (Scarselli et al., 2009), which are generalizations of RNNs that handle graph-structured input data.",2. Related Work,[0],[0]
"Specifically, the inference of GNNs is essentially a propagation process which spreads information along the graph.",2. Related Work,[0],[0]
"One can force the propagation process to converge by either constructing a contraction map explicitly, or by regularizing the Jacobian of the update function.",2. Related Work,[0],[0]
"Similarly, the goal is to push the converged inference solution close to the target.",2. Related Work,[0],[0]
RBP is naturally applicable here and demonstrated to save both computation time and memory.,2. Related Work,[0],[0]
"A recent investigation (Scellier & Bengio, 2017a) shows that RBP is related to equilibrium propagation (Scellier & Bengio, 2017b) which is motivated from the perspective of biological plausibility.",2. Related Work,[0],[0]
"Another recent related work in deep learning is OptNet (Amos & Kolter, 2017) where the gradient of the optimized solution of a quadratic programming problem w.r.t.",2. Related Work,[0],[0]
"parameters is obtained by analytically differentiating the KKT system.
",2. Related Work,[0],[0]
"In the probabilistic graphical models (PGMs) literature, similar techniques to RBP have been developed as well.",2. Related Work,[0],[0]
"For example, an efficient gradient-based method to learn the hyperparameters of log-linear models is provided in (Foo et al.,
2008) where the core contribution is to use the implicit differentiation trick to compute the gradient of the optimized inference solution w.r.t.",2. Related Work,[0],[0]
the hyperparameters.,2. Related Work,[0],[0]
"A similar implicit differentiation technique is used in (Samuel & Tappen, 2009) to optimize the maximum a posterior (MAP) solution of continuous MRFs, since the MAP solution can be regarded as the steady state of the inference process.",2. Related Work,[0],[0]
"An implicit-differentiation-based optimization method for generic energy models is proposed in (Domke, 2012) where the gradient of the optimal state (steady state) of the energy w.r.t.",2. Related Work,[0],[0]
"the parameters can be efficiently computed given the fast matrix-vector product implementation (Pearlmutter, 1994).",2. Related Work,[0],[0]
"If one regards the inference algorithms from aforementioned applications as unrolled RNNs, the implicit differentiation technique is essentially equivalent to RBP.
",2. Related Work,[0],[0]
Other efforts have been made to develop alternatives to BPTT.,2. Related Work,[0],[0]
"NoBackTrack (Ollivier et al., 2015) maintains an online estimate of the gradient via the random rank-one reduction technique.",2. Related Work,[0],[0]
"ARTBP (Tallec & Ollivier, 2017) introduces a probability distribution over the truncation points in the sequence and compensates the truncated gradient based on the distribution.",2. Related Work,[0],[0]
Both approaches provide an unbiased estimation of the gradient although their variances differ.,2. Related Work,[0],[0]
"In this section, we review the original RBP algorithm and discuss its assumptions.",3. Revisiting Recurrent Back-Propagation,[0],[0]
We denote the input data and initial hidden state as x and h0.,3.1. Recurrent Back-Propagation,[0],[0]
"During inference, the hidden state at time t is computed as follows,
ht+1 = F (x,wF , ht), (1)
where F is the update function parameterized by wF .",3.1. Recurrent Back-Propagation,[0],[0]
"A typical instantiation of F is an LSTM (Hochreiter & Schmidhuber, 1997) cell function.",3.1. Recurrent Back-Propagation,[0],[0]
"This RNN formulation differs from the one commonly used in language modeling, as the input is not time-dependent.",3.1. Recurrent Back-Propagation,[0],[0]
We restrict our attention to RNNs with fixed inputs for now as it requires fewer assumptions.,3.1. Recurrent Back-Propagation,[0],[0]
"Assuming the dynamical system, (i.e., the forward pass of the RNN), reaches steady state before time step T , we have the following equation,
h∗ = F (x,wF , h ∗), (2)
where h∗ is the steady hidden state.",3.1. Recurrent Back-Propagation,[0],[0]
"We compute the predicted output y based on the steady hidden state as follows,
y = G(x,wG, h ∗), (3)
where G is the output function parameterized by wG. Typically, a loss function L = l(ȳ, y) measures the closeness
between ground truth ȳ and predicted output y. Since the input data x is fixed for all time steps, we can construct a function Ψ of wF and h as follows,
Ψ(wF , h) = h− F (x,wF , h).",3.1. Recurrent Back-Propagation,[0],[0]
"(4)
At the fixed point, we have Ψ(wF , h∗) = 0.",3.1. Recurrent Back-Propagation,[0],[0]
"Assuming some proper conditions on F , e.g., continuous differentiability, we can take the derivative w.r.t.",3.1. Recurrent Back-Propagation,[0],[0]
wF at h∗ on both sides.,3.1. Recurrent Back-Propagation,[0],[0]
"Using the total derivative and the dependence of h∗ on wF we obtain,
∂Ψ(wF , h ∗)
∂wF =
∂h∗
∂wF − dF",3.1. Recurrent Back-Propagation,[0],[0]
"(x,wF , h
∗)
dwF
= (I − JF,h∗) ∂h∗
∂wF − ∂F (x,wF , h
∗)
∂wF = 0, (5)
where JF,h∗ = ∂F (x,wF ,h ∗)",3.1. Recurrent Back-Propagation,[0],[0]
∂h is the Jacobian matrix of F evaluated at h∗ and d is the total derivative operator.,3.1. Recurrent Back-Propagation,[0],[0]
"Assuming that I − JF,h∗ is invertible, we rearrange Eq.",3.1. Recurrent Back-Propagation,[0],[0]
"(5) to get,
∂h∗ ∂wF = (I − JF,h∗)−1 ∂F (x,wF , h ∗) ∂wF .",3.1. Recurrent Back-Propagation,[0],[0]
"(6)
In fact, Equations (4- 6) are an application of the Implicit Function Theorem (Rudin, 1964), which guarantees the existence and uniqueness of an implicit function φ such that h∗ = φ(wF ) if two conditions hold: I, Ψ is continuously differentiable and II, I − JF,h∗ is invertible.",3.1. Recurrent Back-Propagation,[0],[0]
"Although we do not know the analytic expression of the function φ, we can still compute its gradient at the fixed point.
",3.1. Recurrent Back-Propagation,[0],[0]
Based on Eq.,3.1. Recurrent Back-Propagation,[0],[0]
"(6), we now turn our attention towards computing the gradient of the loss w.r.t.",3.1. Recurrent Back-Propagation,[0],[0]
the parameters of the RNN.,3.1. Recurrent Back-Propagation,[0],[0]
"By using the total derivative and the chain rule, we have
∂L ∂wG = ∂L ∂y
∂G(x,wG, h ∗)
",3.1. Recurrent Back-Propagation,[0],[0]
"∂wG (7)
∂L ∂wF = ∂L ∂y ∂y ∂h∗",3.1. Recurrent Back-Propagation,[0],[0]
(,3.1. Recurrent Back-Propagation,[0],[0]
"I − JF,h∗)−1
∂F (x,wF , h ∗)
∂wF .",3.1. Recurrent Back-Propagation,[0],[0]
"(8)
Since the gradient of the loss w.r.t. wG can be easily obtained by back-propagation, we focus our exposition on the computation of ∂L∂wF .",3.1. Recurrent Back-Propagation,[0],[0]
"The original RBP algorithm (Pineda, 1987; Almeida, 1987) introduces an auxiliary variable z such that,
z =",3.1. Recurrent Back-Propagation,[0],[0]
"( I − J>F,h∗ )−1(∂L ∂y ∂y ∂h∗ )> , (9)
where z is a column vector.",3.1. Recurrent Back-Propagation,[0],[0]
"If we managed to compute z, then we can substitute it into Eq.",3.1. Recurrent Back-Propagation,[0],[0]
(8) to get the gradient.,3.1. Recurrent Back-Propagation,[0],[0]
"Note that the Jacobian matrix JF,h∗ is nonsymmetric for
Algorithm 1 : Original RBP 1: Initialization: initial guess z0, e.g., draw uniformly
from [0, 1], i = 0, threshold 2: repeat 3: i = i+ 1
4: zi = J>F,h∗zi−1 +",3.1. Recurrent Back-Propagation,[0],[0]
( ∂L ∂y ∂y ∂h∗ )> 5: until ‖zi − zi−1‖,3.1. Recurrent Back-Propagation,[0],[0]
< 6: ∂L∂wF =,3.1. Recurrent Back-Propagation,[0],[0]
z >,3.1. Recurrent Back-Propagation,[0],[0]
"i ∂F (x,wF ,h ∗) ∂wF 7:",3.1. Recurrent Back-Propagation,[0],[0]
"Return ∂L∂wF
general RNNs which renders direct solvers of linear system impractical.",3.1. Recurrent Back-Propagation,[0],[0]
"To compute z, the original RBP algorithm uses fixed point iteration.",3.1. Recurrent Back-Propagation,[0],[0]
"In particular, we multiply ( I − J>F,h∗ ) on the left hand of both sides of Eq.",3.1. Recurrent Back-Propagation,[0],[0]
"(9) and rearrange the terms as follows,
z = J>F,h∗z +
( ∂L
∂y
∂y
∂h∗
)> .",3.1. Recurrent Back-Propagation,[0],[0]
"(10)
If we view the right hand side of the above equation as a function of z, then applying the fixed point iteration results in the Algorithm 1.",3.1. Recurrent Back-Propagation,[0],[0]
"Note that the most expensive operation in this algorithm is the matrix-vector product J>F,h∗z, which is the same operator as back-propagation.",3.1. Recurrent Back-Propagation,[0],[0]
"In this section, we discuss how to satisfy the assumptions of RBP.",3.2. Assumptions of RBP,[0],[0]
"Recall that in order to apply the implicit function theorem, Ψ(wF , h) has to satisfy two assumptions: I, Ψ is continuously differentiable.",3.2. Assumptions of RBP,[0],[0]
"II, I − JF,h∗ is invertible.",3.2. Assumptions of RBP,[0],[0]
"Condition I requires the derivative of F to be continuous, a condition satisfied by many RNNs, like LSTM and GRU (Cho et al., 2014).",3.2. Assumptions of RBP,[0],[0]
"Condition II is equivalent to requiring the determinant of I − JF,h∗ to be nonzero, i.e., det(I − JF,h∗) 6= 0.",3.2. Assumptions of RBP,[0],[0]
"One sufficient but not necessary condition to ensure this is to force F to be a contraction map, as in Scarselli et al. (2009).",3.2. Assumptions of RBP,[0],[0]
"Recall that F is a contraction map on Banach space B, i.e., a complete normed vector space, iff, ∀h1, h2 ∈ B, ‖F (h1)− F (h2)‖ ≤ µ‖h1",3.2. Assumptions of RBP,[0],[0]
− h2‖ where 0 ≤ µ < 1.,3.2. Assumptions of RBP,[0],[0]
Banach fixed point theorem guarantees the uniqueness of the fix point of the contraction map F in B. Note that here we drop the dependency of F on w for readability.,3.2. Assumptions of RBP,[0],[0]
"Based on the first order Taylor approximation, F (h) = F (h∗) + JF,h∗(h− h∗), we have,
‖F (h)− F (h∗)‖ ‖h− h∗‖ = ‖JF,h∗(h− h∗)‖",3.2. Assumptions of RBP,[0],[0]
‖h− h∗‖ .,3.2. Assumptions of RBP,[0],[0]
"(11)
Note that if we use L2 vector norm, then the induced matrix norm, a.k.a., operator norm, is,
‖JF,h∗‖ = sup { ‖JF,h∗h‖ ‖h‖ : ∀h 6= 0",3.2. Assumptions of RBP,[0],[0]
"} = σmax(JF,h∗), (12)
",3.2. Assumptions of RBP,[0],[0]
where σmax is the largest singular value.,3.2. Assumptions of RBP,[0],[0]
"Therefore, relying on the contraction map definition, we have,
‖JF,h∗‖ ≤ µ < 1, (13)
",3.2. Assumptions of RBP,[0],[0]
"Moreover, since the minimum singular value of I − JF,h∗ is 1− σmax(JF,h∗), we have
|det(I − JF,h∗)| = ∏",3.2. Assumptions of RBP,[0],[0]
i |σi(I,3.2. Assumptions of RBP,[0],[0]
"− JF,h∗)|
≥",3.2. Assumptions of RBP,[0],[0]
"[1− σmax(JF,h∗)]d > 0.",3.2. Assumptions of RBP,[0],[0]
"(14)
Thus our second condition holds following Eq.",3.2. Assumptions of RBP,[0],[0]
"(14).
",3.2. Assumptions of RBP,[0],[0]
"Scarselli et al. (2009) use L1 vector norm which results in a looser inequality since ‖JF,h∗‖2 ≤ √ d‖JF,h∗‖1.",3.2. Assumptions of RBP,[0],[0]
"They obtain an easier to compute regularization term maxi(‖JF,h∗(: , i)‖1 − η)2 where (:, i) denotes the i-th column and η ∈ (0, 1) is the desired contraction constant.",3.2. Assumptions of RBP,[0],[0]
"We note, however, that this work makes a claim that the contraction map assumption can be achieved by regularizing the local Jacobian JF,h∗ of a general neural network.",3.2. Assumptions of RBP,[0],[0]
"This is problematic because the contraction map property is a global property of F that requires regularizing every h in the spaceB, not just h∗. Nevertheless, this regularization evaluated at h∗ encourages local contraction at the fixed point h∗, which is sufficient for satisfying condition II.",3.2. Assumptions of RBP,[0],[0]
"Another way to enforce condition II to hold is directly formalizing the Lagrangian of equality constraint Ψ(wF , h∗) = 0.",3.2. Assumptions of RBP,[0],[0]
"Since all applications we considered in this paper have converged dynamic systems in practice, we leave further discussions of condition II to the appendix.",3.2. Assumptions of RBP,[0],[0]
"In this section, we present our newly proposed variants of RBP, CG-RBP and Neumann-RBP, in detail.",4. New Recurrent Back-Propagation Variants,[0],[0]
"Facing the system of linear equations like Eq. (9) in the derivation of original RBP, one would naturally think of the most common iterative solver, i.e., conjugate gradient method (Hestenes & Stiefel, 1952).",4.1. Recurrent Back-Propagation based on Conjugate Gradient,[0],[0]
"In particular, multiplying I − J>F,h∗ on both sides, we obtain the following equations,
( I − J>F,h∗ ) z",4.1. Recurrent Back-Propagation based on Conjugate Gradient,[0],[0]
"=
( ∂L
∂y
∂y
∂h∗
)> .",4.1. Recurrent Back-Propagation based on Conjugate Gradient,[0],[0]
"(15)
Unfortunately, for general RNNs, the Jacobian matrix JF,h∗ of the update function, e.g., a cell function of LSTM, is non-symmetric in general.",4.1. Recurrent Back-Propagation based on Conjugate Gradient,[0],[0]
This increases the difficulty of solving the system.,4.1. Recurrent Back-Propagation based on Conjugate Gradient,[0],[0]
"One simple yet sometimes effective way to approach this problem is to exploit the conjugate
gradient method on the normal equations (CGNE) (Golub & Van Loan, 2012).",4.1. Recurrent Back-Propagation based on Conjugate Gradient,[0],[0]
"Specifically, we multiply I − JF,h∗ on both sides of Eq.",4.1. Recurrent Back-Propagation based on Conjugate Gradient,[0],[0]
"(15) which results in,
(I − JF,h∗) ( I − J>F,h∗ ) z =",4.1. Recurrent Back-Propagation based on Conjugate Gradient,[0],[0]
"(I − JF,h∗)
( ∂L
∂y
∂y
∂h∗
)> .
",4.1. Recurrent Back-Propagation based on Conjugate Gradient,[0],[0]
"Having a symmetric matrix multiplying z on the left hand side, we can now use the conjugate gradient method.",4.1. Recurrent Back-Propagation based on Conjugate Gradient,[0],[0]
The detailed algorithm is easily obtained by instantiating the standard conjugate gradient (CG) template.,4.1. Recurrent Back-Propagation based on Conjugate Gradient,[0],[0]
"The most expensive operation used in CGNE is JF,h∗J>F,h∗z, which can be implemented by successive matrix-vector products similarly for computing the Fisher information product of the natural gradient method (Schraudolph, 2002).",4.1. Recurrent Back-Propagation based on Conjugate Gradient,[0],[0]
"Once we solve z via K-step CGNE, we obtain the final gradient by substituting the solution into Eq.",4.1. Recurrent Back-Propagation based on Conjugate Gradient,[0],[0]
(8).,4.1. Recurrent Back-Propagation based on Conjugate Gradient,[0],[0]
"Since the condition number of the current system is the square of the original one, the system may be slower to converge in practice.",4.1. Recurrent Back-Propagation based on Conjugate Gradient,[0],[0]
"Exploring more advanced and faster convergent numerical methods under this setting, like LSQR (Paige & Saunders, 1982), is left for future work.",4.1. Recurrent Back-Propagation based on Conjugate Gradient,[0],[0]
"We now develop a new RBP variant called Neumann-RBP, which uses Neumann series from functional analysis and is efficient in terms of computation and memory.",4.2. Recurrent Back-Propagation based on Neumann Series,[0],[0]
We then show its connections to BPTT and TBPTT.,4.2. Recurrent Back-Propagation based on Neumann Series,[0],[0]
"A Neumann series is a mathematical series of the form∑∞ t=0A
t where A is an operator.",4.2. Recurrent Back-Propagation based on Neumann Series,[0],[0]
"In matrix theory, it is also known as the geometric series of a matrix.",4.2. Recurrent Back-Propagation based on Neumann Series,[0],[0]
"A convergent Neumann series has the following property,
(I −A)−1 = ∞∑ k=0 Ak. (16)
",4.2. Recurrent Back-Propagation based on Neumann Series,[0],[0]
"One sufficient condition of convergence is that the spectral radius (i.e., the largest absolute eigenvalue value) ofA is less than 1.",4.2. Recurrent Back-Propagation based on Neumann Series,[0],[0]
"This convergence criterion applied to A = JF,h∗ implies condition II.",4.2. Recurrent Back-Propagation based on Neumann Series,[0],[0]
Other cases where the convergence hold is beyond the scope of this paper.,4.2. Recurrent Back-Propagation based on Neumann Series,[0],[0]
"If the Neumann series ∑∞ t=0 J t F,h∗ converges, we can use it to replace the term (I − JF,h∗)−1 in E.q.",4.2. Recurrent Back-Propagation based on Neumann Series,[0],[0]
(8).,4.2. Recurrent Back-Propagation based on Neumann Series,[0],[0]
"Furthermore, the gradient of RBP can be approximated with the K-th order truncation of Neumann series as below,
∂L ∂wF ≈ ∂L ∂y K∑ k=0 ∂y ∂h∗ JkF,h∗ ∂F (x,wF , h ∗) ∂wF .",4.2. Recurrent Back-Propagation based on Neumann Series,[0],[0]
"(17)
There is a rich body of literature on how to compute Neumann series efficiently using binary or ternary decomposition (Westreich, 1989; Vassil & Diego, 2017).",4.2. Recurrent Back-Propagation based on Neumann Series,[0],[0]
"However,
Algorithm 2 : Neumann-RBP 1: Initialization: v0 = g0 = ( ∂L ∂y ∂y ∂h∗ )> 2: for step t = 1, 2, . . .",4.2. Recurrent Back-Propagation based on Neumann Series,[0],[0]
",K do 3: vt = J>vt−1",4.2. Recurrent Back-Propagation based on Neumann Series,[0],[0]
4: gt = gt−1 + vt 5: end for 6: ∂L∂wF = (gK) > ∂F,4.2. Recurrent Back-Propagation based on Neumann Series,[0],[0]
"(x,wF ,h∗) ∂wF
7: Return ∂L∂wF
these decomposition based approaches are inapplicable in our context since we cannot compute the Jacobian matrix JF,h∗ efficiently for general neural networks.",4.2. Recurrent Back-Propagation based on Neumann Series,[0],[0]
"Fortunately, we can instead efficiently compute the matrix-vector product J>F,h∗u and JF,h∗u (u is a proper sized vector) by using reverse and forward mode auto-differentiation (Pearlmutter, 1994).",4.2. Recurrent Back-Propagation based on Neumann Series,[0],[0]
"Relying on this technique, we summarize the Neumann series based RBP algorithm in Algorithm 2.",4.2. Recurrent Back-Propagation based on Neumann Series,[0],[0]
"In practice, we can obtain further memory efficiency by performing updates within the for loops in-place (please refer to the example code in appendix), so that memory usage need not scale with the number of truncation steps.",4.2. Recurrent Back-Propagation based on Neumann Series,[0],[0]
"Moreover, since the algorithm does not rely on hidden states except the fixed point h∗, we no longer need to store the hidden states in the forward pass of the RNN.",4.2. Recurrent Back-Propagation based on Neumann Series,[0],[0]
"Besides the computational benefit, we now have the following proposition to connect Neumann-RBP to BPTT and TBPTT.
",4.2. Recurrent Back-Propagation based on Neumann Series,[0],[0]
Proposition 1.,4.2. Recurrent Back-Propagation based on Neumann Series,[0],[0]
"Assume that there exists some step K where 0 < K ≤ T such that for the convergent sequence of hidden states h0, h1, . . .",4.2. Recurrent Back-Propagation based on Neumann Series,[0],[0]
", hT of an RNN, we have h∗ = hT = hT−1 = · · · = hT−K where h∗ is the fixed point.",4.2. Recurrent Back-Propagation based on Neumann Series,[0],[0]
"If the Neumann series ∑∞ t=0 J t F,h∗ converges, then the full and K-step Neumann-RBP are equivalent to BPTT and K-step TBPTT respectively.
",4.2. Recurrent Back-Propagation based on Neumann Series,[0],[0]
"Moreover, the following proposition bounds the error of K-step Neumann-RBP.",4.2. Recurrent Back-Propagation based on Neumann Series,[0],[0]
Proposition 2.,4.2. Recurrent Back-Propagation based on Neumann Series,[0],[0]
"If the Neumann series ∑∞ t=0 J t F,h∗ converges, then the error between K-step and full Neumann series is as follows,∥∥∥∥∥ K∑ t=0 JtF,h∗",4.2. Recurrent Back-Propagation based on Neumann Series,[0],[0]
"− (I − JF,h∗)−1 ∥∥∥∥∥ ≤ ∥∥(I",4.2. Recurrent Back-Propagation based on Neumann Series,[0],[0]
"− JF,h∗)−1∥∥ ‖JF,h∗‖K+1
We leave all proofs in appendix.",4.2. Recurrent Back-Propagation based on Neumann Series,[0],[0]
"In this section, we thoroughly study all RBP variants on diverse applications.",5. Experiments,[0],[0]
Our implementation based on PyTorch is publicly available1.,5. Experiments,[0],[0]
"Note that our Neumann-RBP is very
1https://github.com/lrjconan/RBP
simple to implement using automatic differentiation and we provide a very short example program in the appendix.",5. Experiments,[0],[0]
"The classical testbed for RBP is the associative memory (Hopfield, 1982).",5.1. Associative Memory,[0],[0]
Several images or patterns are presented to the neural network which learns to store or memorize the images.,5.1. Associative Memory,[0],[0]
"After the learning process, the network is subsequently presented with a corrupted or noisy version of the original image.",5.1. Associative Memory,[0],[0]
The task is then to retrieve the corresponding original image.,5.1. Associative Memory,[0],[0]
"We consider a simplified continuous Hopfield network as described in (Haykin, 1993).",5.1. Associative Memory,[0],[0]
"Specifically, the system of nonlinear first-order differential equations is,
d dt hi(t) =",5.1. Associative Memory,[0],[0]
− hi(t),5.1. Associative Memory,[0],[0]
a + N∑ j=1 wijφ(b · hj(t)),5.1. Associative Memory,[0],[0]
"+ Ii, (18)
where subscript i denotes the index of the neuron.",5.1. Associative Memory,[0],[0]
wij is the learnable weight between a pair of neurons.,5.1. Associative Memory,[0],[0]
hi is the hidden state of the i-th neuron.,5.1. Associative Memory,[0],[0]
φ is a nonlinear activate function which is a sigmoid function in our experiments.,5.1. Associative Memory,[0],[0]
"a, b are positive constants and are set to 1 and 0.5.",5.1. Associative Memory,[0],[0]
"The set of neurons consists of three parts: observed, hidden and output neurons, of size 784, 1024 and 784 respectively.",5.1. Associative Memory,[0],[0]
"For observed neuron, the state hi is clamped to observed pixel value Ii.",5.1. Associative Memory,[0],[0]
"For hidden and output neurons, the observed pixel value Ii = 0 and their states hi are updated according to Eq.",5.1. Associative Memory,[0],[0]
(18).,5.1. Associative Memory,[0],[0]
"During inference, the output neurons return xi = φ(b · hi) and we further binarize it for visualization.",5.1. Associative Memory,[0],[0]
"An important property of continuous Hopfield networks is
that by updating the states according to Eq.",5.1. Associative Memory,[0],[0]
"(18) until convergence (which corresponds to the forward pass of RNNs), we are guaranteed to minimize the following (Lyapunov) energy function.
",5.1. Associative Memory,[0],[0]
E = N∑ i=1,5.1. Associative Memory,[0],[0]
( 1 a ∫ xi 0 φ−1(x)dx− Iixi ),5.1. Associative Memory,[0],[0]
− N∑ i=1,5.1. Associative Memory,[0],[0]
"N∑ j=1 wijxixj 2 ,
where we drop the dependency on time t for simplicity.",5.1. Associative Memory,[0],[0]
"Instead of adopting the Hebbian learning rule as in (Hopfield, 1982), we directly formulate the learning objective as minimizing ∑ i∈I ‖xi− Ii‖1 where I is the set of observed neurons.",5.1. Associative Memory,[0],[0]
"In our experiments, we train and test on 10 MNIST images.",5.1. Associative Memory,[0],[0]
"In training we feed clean data, and during testing we randomly corrupt 50% of the non-zero pixel values to zero.",5.1. Associative Memory,[0],[0]
"The number of updates for one inference pass is 50.
",5.1. Associative Memory,[0],[0]
Fig. 1 shows the training and validation curves of continuous Hopfield network with different optimization methods.,5.1. Associative Memory,[0],[0]
"Here truncation steps for TBPTT, RBP, CG-RBP and Neumann-RBP are all set to 20.",5.1. Associative Memory,[0],[0]
"From the figure, we can see that CG-RBP and Neumann-RBP match BPTT under this setting which verifies that their gradients are accurate.",5.1. Associative Memory,[0],[0]
"Nevertheless, we can see that training curve of the original RBP blows up which validates its instability issue.",5.1. Associative Memory,[0],[0]
The hidden state of Hopfield network becomes steady within 10 steps.,5.1. Associative Memory,[0],[0]
"However, we notice that if we set the truncation step to 10, original RBP exhibits behaviors which fails to converge.",5.1. Associative Memory,[0],[0]
We also show some visualizations of retrieved images of the Hopfield network under different optimization methods in Fig. 2.,5.1. Associative Memory,[0],[0]
More visual results are provided in the appendix.,5.1. Associative Memory,[0],[0]
We investigate RBPs on semi-supervised document classification with citation networks.,5.2. Semi-supervised Document Classification,[0],[0]
A node of a network represents a document associated with a bag-of-words feature.,5.2. Semi-supervised Document Classification,[0],[0]
Nodes are connected based on the citation links.,5.2. Semi-supervised Document Classification,[0],[0]
"Given a portion of nodes labeled with subject categories, e.g., science, history, the task is to predict the categories for unlabeled nodes within the same network.",5.2. Semi-supervised Document Classification,[0],[0]
"We use two citation networks from (Yang et al., 2016), i.e., Cora, Pubmed, of which the statistics are summarized in the appendix.",5.2. Semi-supervised Document Classification,[0],[0]
"We adopt graph neural networks (GNNs) (Scarselli et al., 2009) model and employ the GRU as the update function similarly as (Li et al., 2016).",5.2. Semi-supervised Document Classification,[0],[0]
"We refer to (Li et al., 2016; Liao et al., 2018) for more details.",5.2. Semi-supervised Document Classification,[0],[0]
We compare different optimization methods with the same GNN.,5.2. Semi-supervised Document Classification,[0],[0]
We also add a logistic regression model as a baseline which is applied to every node independently.,5.2. Semi-supervised Document Classification,[0],[0]
"The labeled documents are randomly split into 1%, 49% and 50% for training, validation and testing.",5.2. Semi-supervised Document Classification,[0],[0]
We run all experiments with 10 different random seeds and report the average results.,5.2. Semi-supervised Document Classification,[0],[0]
"The training, validation and difference norm curves of BPTT, TBPTT and all RBPs are shown in Fig. 3.",5.2. Semi-supervised Document Classification,[0],[0]
We can see that the hidden states of GNNs with different optimization methods become steady during inference from Fig. 3 (c).,5.2. Semi-supervised Document Classification,[0],[0]
"As shown in Fig. 3 (a) and (b), Neumann-RBP is on par with TBPTT on both datasets.",5.2. Semi-supervised Document Classification,[0],[0]
This matches our analysis in proposition 1 since the changes of successive hidden states of TBPTT and Neumann-RBP are almost zero as shown in Fig. 3 (c).,5.2. Semi-supervised Document Classification,[0],[0]
"Moreover, they outperform other variants and the baseline model.",5.2. Semi-supervised Document Classification,[0],[0]
"On the other hand, BPTT on both datasets encounter issues in learning which may be attributable to the accumulation of errors in
the many steps of unrolling.",5.2. Semi-supervised Document Classification,[0],[0]
"Note that CG-RBP sometimes performs significantly worse than Neumann-RBP, e.g., on Cora.",5.2. Semi-supervised Document Classification,[0],[0]
This may be caused by the fact that the underlying linear system of CG-RBP is ill-conditioned in some applications as the condition number is squared in CGNE.,5.2. Semi-supervised Document Classification,[0],[0]
The test accuracy of different methods are summarized in Table 1.,5.2. Semi-supervised Document Classification,[0],[0]
It generally matches the behavior in the validation curves.,5.2. Semi-supervised Document Classification,[0],[0]
"In our next experiment, we test the abilities of RBP to perform hyperparameter optimization.",5.3. Hyperparameter Optimization,[0],[0]
"In this experiment, we view the optimization process as a RNN.",5.3. Hyperparameter Optimization,[0],[0]
"When training a neural network, the model parameters, e.g., weights and bias, are regarded as the hidden states of the RNN.",5.3. Hyperparameter Optimization,[0],[0]
Hyperparameters such as learning rate and momentum are learnable parameters of this ‘meta-learning’ RNN.,5.3. Hyperparameter Optimization,[0],[0]
"Here we focus on the gradient based hyperparameter optimization rather than the gradient-free one (Snoek et al., 2012).",5.3. Hyperparameter Optimization,[0],[0]
"We adopt the same experiment setting as in (Maclaurin et al., 2015), using an initial learning rate of exp(−1) and momentum 0.5.",5.3. Hyperparameter Optimization,[0],[0]
"The optimization is on a fully connected network with 4 layers, of sizes 784, 50, 50, and 50.",5.3. Hyperparameter Optimization,[0],[0]
"For each layer, we associate one learning rate and one momentum with weight and bias respectively which results in 16 hyperparameters in total.",5.3. Hyperparameter Optimization,[0],[0]
"We use tanh non-linearities and train on 10, 000 examples on MNIST.",5.3. Hyperparameter Optimization,[0],[0]
"At each forward step of the RNN, i.e., at each optimization step, a different mini-batch of images is fed to the model.",5.3. Hyperparameter Optimization,[0],[0]
This is different from the previous setting where input data is fixed.,5.3. Hyperparameter Optimization,[0],[0]
"However, since the minibatches are assumed to be i.i.d., the sequential input data can be viewed as sampled from a stationary distribution.",5.3. Hyperparameter Optimization,[0],[0]
We can thus safely apply RBP as the steady state holds in expectation.,5.3. Hyperparameter Optimization,[0],[0]
"In terms of implementation, we just need to average the meta gradient returned by RBPs or TBPTT across multiple mini-batches at the end of one meta step.",5.3. Hyperparameter Optimization,[0],[0]
"We use Adam (Kingma & Ba, 2014) as the meta optimizer and set the learning rate to 0.05.",5.3. Hyperparameter Optimization,[0],[0]
"The initialization of the
fully connected network at each meta step is controlled to be the same.
",5.3. Hyperparameter Optimization,[0],[0]
Fig. 5 shows the meta training losses under different training and truncation steps.,5.3. Hyperparameter Optimization,[0],[0]
"For better understanding, one can consider the training step as the unrolling step of the RNN.",5.3. Hyperparameter Optimization,[0],[0]
Truncation step is the the number of steps that TBPTT and RBPs execute.,5.3. Hyperparameter Optimization,[0],[0]
"From the figure, we can see that as the number of training steps increases (e.g., from (a) to (d)), the meta loss becomes smoother.",5.3. Hyperparameter Optimization,[0],[0]
This makes sense since more steps make the training per meta step closer to convergence.,5.3. Hyperparameter Optimization,[0],[0]
Another surprising phenomenon we found is the meta loss of TBPTT becomes worse when the training step increases.,5.3. Hyperparameter Optimization,[0],[0]
"One possible explanation is that the initial meta training loss of small training steps (e.g., (a)) is still very high as you can see from the log y-axis whereas the one with large training step, e.g., (d) is much lower.",5.3. Hyperparameter Optimization,[0],[0]
The probability of using incorrect gradients to decrease the meta loss in case (a) is most likely higher than that of (d) since it is farther from convergence.,5.3. Hyperparameter Optimization,[0],[0]
"On the other hand, our Neumann-RBP performs consistently better than the original RBP and TBPTT which empirically validates that Neumann-RBP provides better estimation of the gradient in this case.",5.3. Hyperparameter Optimization,[0],[0]
The potential reason why RBP performs poorly is that the stochasticity of mini-batches worsen the instability issue.,5.3. Hyperparameter Optimization,[0],[0]
Training losses under similar settings at the last meta step are also provided in Fig. 6.,5.3. Hyperparameter Optimization,[0],[0]
"We can see that at the end of hyperparameter optimization, our Neumann-RBP generally matches the performance of BPTT and outperforms the other methods.",5.3. Hyperparameter Optimization,[0],[0]
"Fig. 4 depicts the trajectories of hidden states in 2D space via t-SNE (Maaten & Hinton, 2008).",5.3. Hyperparameter Optimization,[0],[0]
"From the figure, we can see that as the meta training goes on, TBPTT tends to oscillate whereas Neumann-RBP converges, which matches the finding in the train loss curves in Fig. 6.
",5.3. Hyperparameter Optimization,[0],[0]
"We also compare the running time and memory cost of our unoptimized Neumann-RBP implementation with the standard BPTT, i.e., using autograd of PyTorch.",5.3. Hyperparameter Optimization,[0],[0]
"With 1000 training steps, one meta step of BPTT cost 310.4s and 4061MB GPU memory in average.",5.3. Hyperparameter Optimization,[0],[0]
"We take BPTT as the reference cost and report the ratio BPTT’s cost divided by Neumann-
RBP’s in Table 2.",5.3. Hyperparameter Optimization,[0],[0]
All results are reported as the average of 10 runs.,5.3. Hyperparameter Optimization,[0],[0]
"Even without optimizing the code, the practical runtime and memory footprint advantages of Neumann-RBP over BPTT is still significant.",5.3. Hyperparameter Optimization,[0],[0]
"In this paper, we revisit the RBP algorithm and discuss its assumptions and how to satisfy them for deep learning.",6. Conclusion,[0],[0]
"Moreover, we propose two variants of RBP based on conjugate gradient on normal equations and Neumann series.",6. Conclusion,[0],[0]
Connections between Neumann-RBP and TBPTT are established which sheds some light on analyzing the approximation quality of the gradient of TBPTT.,6. Conclusion,[0],[0]
Experimental results on diverse tasks demonstrate that Neumann-RBP is a stable and efficient alternative to original RBP and is promising for several practical problems.,6. Conclusion,[0],[0]
"In the future, we would like to explore RBP on hyperparameter optimization with large scale deep neural networks.",6. Conclusion,[0],[0]
We thank Barak Pearlmutter for the enlightening discussion and anonymous ICML reviewers for valuable comments.,Acknowledgements,[0],[0]
R.L. was supported by Connaught International Scholarships.,Acknowledgements,[0],[0]
"R.L., E.F., L.Z., K.Y., X.P., R.U. and R.Z. were supported in part by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DoI/IBC) contract number D16PC00003.",Acknowledgements,[0],[0]
K.Y. and X.P. were supported in part by BRAIN Initiative grant NIH 5U01NS094368.,Acknowledgements,[0],[0]
The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon.,Acknowledgements,[0],[0]
"Disclaimer: the views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoI/IBC, or the U.S. Government.",Acknowledgements,[0],[0]
"In this paper, we revisit the recurrent backpropagation (RBP) algorithm (Almeida, 1987; Pineda, 1987), discuss the conditions under which it applies as well as how to satisfy them in deep neural networks.",abstractText,[0],[0]
We show that RBP can be unstable and propose two variants based on conjugate gradient on the normal equations (CG-RBP) and Neumann series (Neumann-RBP).,abstractText,[0],[0]
We further investigate the relationship between Neumann-RBP and back propagation through time (BPTT) and its truncated version (TBPTT).,abstractText,[0],[0]
"Our NeumannRBP has the same time complexity as TBPTT but only requires constant memory, whereas TBPTT’s memory cost scales linearly with the number of truncation steps.",abstractText,[0],[0]
"We examine all RBP variants, along with BPTT and TBPTT, in three different application domains: associative memory with continuous Hopfield networks, document classification in citation networks using graph neural networks, and hyperparameter optimization for fully connected networks.",abstractText,[0],[0]
"All experiments demonstrate that RBPs, especially the Neumann-RBP variant, are efficient and effective for optimizing convergent recurrent neural networks.",abstractText,[0],[0]
Reviving and Improving Recurrent Back-Propagation,title,[0],[0]
"A widely used machine learning technique is the transfer of a representation learned from a source task, for which labeled data is abundant, to a target task, for which labeled data is scarce.",1. Introduction,[0],[0]
This may be effective if the tasks approximately share an intermediate representation.,1. Introduction,[0],[0]
"For example:
• features learned from an image of a human face to predict age may also be useful for predicting gender
• word embeddings learned to predict word contexts may also be useful for part of speech tagging
• features learned from financial data to predict loan default may also be useful for predicting insurance fraud.
",1. Introduction,[0],[0]
"Often a representation is learned by a different organization that may have greater access to data, computational and human resources.",1. Introduction,[0],[0]
"Examples are the Google word2vec package (Mikolov et al., 2013), and downloadable pre-trained
1The Australian National University and Data61, Canberra, ACT, Australia 2Carnegie Mellon University, Pittsburgh, PA, USA.",1. Introduction,[0],[0]
"Correspondence to: Daniel McNamara <daniel.mcnamara@anu.edu.au>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"neural networks.1 Under this ‘representation-as-a-service’ model, a user may expect to access the representation itself, as well as information about its performance on the source task data on which it was trained.",1. Introduction,[0],[0]
"We aim to convert this into a guarantee of the usefulness of the representation on other tasks, which is known in advance without the effort or cost of testing the representation on the target task(s).",1. Introduction,[0],[0]
"Our analysis also covers the case where the source task is constructed from unlabeled data, as in neural network unsupervised pre-training.
",1. Introduction,[0],[0]
"We consider two approaches to transferring a representation learned from a source task to a target task, as shown in Figure 1.",1. Introduction,[0],[0]
"We may either treat the representation as fixed, or we may narrow the class of representations considered on the target task, which we refer to as fine-tuning.",1. Introduction,[0],[0]
"The fixed option may be attractive when very little labeled target task data is available and hence overfitting is a strong concern, while the advantage of fine-tuning is relatively greater hypothesis class expressiveness.
",1. Introduction,[0],[0]
"Let X,Y and Z be sets known as the input, output and feature spaces respectively.",1. Introduction,[0],[0]
"Let F be a class of representations, where f : X → Z for f ∈ F .",1. Introduction,[0],[0]
"Let G be a class of specialized classifiers, where g : Z → Y for g ∈ G.",1. Introduction,[0],[0]
"Let the hypothesis class H := {h : ∃f ∈ F, g ∈ G such that h",1. Introduction,[0],[0]
= g,1. Introduction,[0],[0]
◦ f}.,1. Introduction,[0],[0]
"Let hS , hT : X → Y be the labeling functions and PS , PT be the input distributions for source task S and target task T respectively.",1. Introduction,[0],[0]
"We consider the setting Y = {−1, 1}.",1. Introduction,[0],[0]
Let the risk of a hypothesis h on S and T be RS(h),1. Introduction,[0],[0]
:= Ex∼PS [hS(x) 6= h(x)] and RT (h) :,1. Introduction,[0],[0]
= Ex∼PT [hT (x) 6= h(x)] respectively.,1. Introduction,[0],[0]
Let R̂S(h) and R̂T (h) be the corresponding empirical (i.e. training set) risks.,1. Introduction,[0],[0]
We have mS labelled points for S and mT labelled points for T .,1. Introduction,[0],[0]
"Let dH be the VC dimension of H .
",1. Introduction,[0],[0]
The remainder of the paper is structured as follows.,1. Introduction,[0],[0]
In Section 2 we introduce related work.,1. Introduction,[0],[0]
In Sections 3 and 4 we analyze the cases where the transferred representation is fixed and fine-tuned respectively.,1. Introduction,[0],[0]
In Section 5 we apply the results and use them to motivate and test a practical approach to weight transfer in neural networks.,1. Introduction,[0],[0]
"We conclude in Section 6 and defer more involved proofs to Section 7.
1See http://code.google.com/archive/p/ word2vec, http://caffe.berkeleyvision.org/ model_zoo and http://vlfeat.org/matconvnet/ pretrained for examples.",1. Introduction,[0],[0]
"Empirical studies have shown the success of transferring representations between tasks (Donahue et al., 2014; Hoffman et al., 2014; Girshick et al., 2014; Socher et al., 2013; Bansal et al., 2014).",2. Background,[0],[0]
"Word embeddings learned on a source task have been shown (Qu et al., 2015) to perform better than unigram features on target tasks such as part of speech tagging, and comparably or better than embeddings finetuned on the target task.",2. Background,[0],[0]
"Yosinski et al. (2014) learned neural network weights using half of the ImageNet classes, and then learned the other classes with a neural network initialized with these weights, finding a benefit compared to random initialization only with target task fine-tuning.",2. Background,[0],[0]
"The transfer of representations, both with and without finetuning, is widely and successfully used.
",2. Background,[0],[0]
"Previous work on domain adaptation (Ben-David et al., 2010; Mansour et al., 2009; Germain et al., 2013) has considered learning a hypothesis h on S and re-using it on T , bounding RT (h) using RS(h) (measured with labeled source data) and some notion of similarity between PS and PT (measured with additional unlabeled target data).",2. Background,[0],[0]
"Such results motivate a joint optimization using labeled source and unlabeled target data (Ganin et al., 2016; Long et al., 2015) to learn separate mappings fS , fT : X → Z which make the induced distributions in the feature space Z similar, and a hypothesis g : Z → Y learned from the source labels which can be re-used on T .",2. Background,[0],[0]
This approach assumes the tasks become the same if their input distributions can be aligned.,2. Background,[0],[0]
We consider a relaxation where the tasks are more weakly related but some representation step can be transferred.,2. Background,[0],[0]
"We consider learning f : X → Z on S, re-using it on T , and then learning gT :",2. Background,[0],[0]
Z → Y from a small amount of labeled target data.,2. Background,[0],[0]
"Given the widespread use of ‘downloadable’ representations, where f and gT are learned separately and there is no joint optimization over source and target data, this is a realistic setting.
",2. Background,[0],[0]
Work on lifelong learning relates the past performance of a representation over many tasks to its expected future performance.,2. Background,[0],[0]
For a representation f ∈ F we construct G ◦ f,2. Background,[0],[0]
:= {g ◦ f,2. Background,[0],[0]
: g ∈ G}.,2. Background,[0],[0]
"Suppose there is a distribution over tasks, known as an environment.",2. Background,[0],[0]
"Assume several tasks from this environment have been sampled, and that for each task some hypothesis in G ◦ f has been selected and its empirical risk evaluated.",2. Background,[0],[0]
Previous work has provided bounds on the difference between the average empirical risk and the expected risk of the best hypothesis in G ◦ f for a new task drawn from the environment.,2. Background,[0],[0]
"Such bounds have been given by measuring the complexity of F and G using covering numbers (Baxter, 2000), a variant of the growth function (Galanti et al., 2016), and a distribution-dependent measure known as Gaussian complexity (Maurer et al., 2016).",2. Background,[0],[0]
"All of these bounds rely on
known past performance on a large number of tasks.2 In practice, however, representations such as neural network weights or word embeddings are often learned using only a single source task, which is the setting we consider.",2. Background,[0],[0]
"Suppose labeled source data is abundant, labeled target data is scarce, and we believe the tasks share a representation.",3. Representation Fixed by Source Task,[0],[0]
"A natural approach to leveraging the source data is to learn ĝS ◦ f̂ ∈ H on S, from which we assume we may extract f̂ ∈ F ,3 then conduct empirical risk minimization over G ◦ f̂ := {g ◦ f̂ : g ∈ G} on T yielding ĝT ◦ f̂ .",3. Representation Fixed by Source Task,[0],[0]
"Theorem 1 upper-bounds RT (ĝT ◦ f̂) using four terms: a function ω measuring a transferrability property obtained analytically from the problem setting, the empirical risk R̂S(ĝS ◦ f̂), the generalization error of a hypothesis in H learned from mS samples, and the generalization error of a hypothesis in G learned from mT samples.",3. Representation Fixed by Source Task,[0],[0]
"The value of the theorem is that if ω(R) = O(R), R̂S(ĝS ◦ f̂) is a small constant, mS mT and dH dG,4 we improve on the VC dimension-based bound for learning T from scratch by avoiding the generalization error of a hypothesis in H learned from mT samples.",3. Representation Fixed by Source Task,[0],[0]
"Furthermore, we do not settle for bounding RT (ĝT ◦ f̂) in terms of R̂T (ĝT ◦ f̂), which may be large.",3. Representation Fixed by Source Task,[0],[0]
"The theorem can be used to select S given
2Pentina & Lampert (2014) extend this analysis to stochastic hypotheses (i.e. distributions over deterministic hypotheses), where for each task we learn a posterior given a prior and training data.",3. Representation Fixed by Source Task,[0],[0]
The quality of the prior affects the learner’s performance.,3. Representation Fixed by Source Task,[0],[0]
"The study proposes using source tasks to learn a ‘hyperposterior’, a distribution over priors which is sampled to give a prior for each task.",3. Representation Fixed by Source Task,[0],[0]
Such a hyperposterior may focus the learner on a representation shared across tasks.,3. Representation Fixed by Source Task,[0],[0]
"The study gives a PAC-Bayes bound on the expected risk of using a hyperposterior to learn a new task drawn from the environment, in terms of the average empirical risk obtained using the hyperposterior to learn the source tasks.
",3. Representation Fixed by Source Task,[0],[0]
"3This is not possible with knowledge of ĝS ◦ f̂ alone, but in the case of feedforward neural networks which we focus on, f̂ is known if the weights learned on S are known.
",3. Representation Fixed by Source Task,[0],[0]
"4We have mS mT if labeled source task data is abundant while labeled target task data is scarce, and dH dG",3. Representation Fixed by Source Task,[0],[0]
"if we simplify target task learning by substantially reducing the hypothesis space to be searched.
several options.",3. Representation Fixed by Source Task,[0],[0]
"While we refer to ω in a general form, we give an example in Section 3.1 and expect that others exist.5
Theorem 1.",3. Representation Fixed by Source Task,[0],[0]
Let ω : R → R be a non-decreasing function.,3. Representation Fixed by Source Task,[0],[0]
"Suppose PS , PT , hS , hT , f̂ , G have the property that ∀ĝS ∈ G, min
g∈G RT",3. Representation Fixed by Source Task,[0],[0]
(g ◦ f̂) ≤ ω(RS(ĝS ◦ f̂)).,3. Representation Fixed by Source Task,[0],[0]
"Let ĝT :=
arg min g∈G
R̂T (g ◦ f̂).",3. Representation Fixed by Source Task,[0],[0]
"Then with probability at least 1 − δ
over pairs of training sets for tasks S and T , RT (ĝT ◦ f̂) ≤ ω(R̂S(ĝS ◦ f̂)",3. Representation Fixed by Source Task,[0],[0]
"+ 2
",3. Representation Fixed by Source Task,[0],[0]
"√ 2dH log(2emS/dH)+2 log(8/δ)
mS ) + 4 √
2dG log(2emT /dG)+2 log(8/δ) mT .
",3. Representation Fixed by Source Task,[0],[0]
Proof.,3. Representation Fixed by Source Task,[0],[0]
Let g∗T :,3. Representation Fixed by Source Task,[0],[0]
= arg min g∈G RT,3. Representation Fixed by Source Task,[0],[0]
(g ◦ f̂).,3. Representation Fixed by Source Task,[0],[0]
"With probability at least 1− δ,
RT (ĝT ◦ f̂) ≤ R̂T (ĝT ◦ f̂) + 2 √
2dG log(2emT /dG)+2 log(8/δ) mT
≤ R̂T (g∗T ◦ f̂) + 2 √ 2dG log(2emT /dG)+2 log(8/δ) mT
≤",3. Representation Fixed by Source Task,[0],[0]
RT,3. Representation Fixed by Source Task,[0],[0]
"(g∗T ◦ f̂) + 4 √ 2dG log(2emT /dG)+2 log(8/δ) mT
≤ ω(RS(ĝS ◦ f̂))",3. Representation Fixed by Source Task,[0],[0]
"+ 4 √
2dG log(2emT /dG)+2 log(8/δ) mT
≤ ω(R̂S(ĝS ◦ f̂) + 2 √
2dH log(2emS/dH)+2 log(8/δ) mS ) + 4 √
2dG log(2emT /dG)+2 log(8/δ) mT .
",3. Representation Fixed by Source Task,[0],[0]
"Using m training points and a hypothesis class of VC dimension d, with probability at least 1 − δ, for all hypotheses h simultaneously, the riskR(h) and empirical risk R̂(h)
satisfy |R(h)−R̂(h)| ≤ 2 √
2d log(2em/d)+2 log(4/δ) m",3. Representation Fixed by Source Task,[0],[0]
"(Mohri
et al., 2012).",3. Representation Fixed by Source Task,[0],[0]
ForG this yields the first and third inequalities with probability at least 1 − δ2 .,3. Representation Fixed by Source Task,[0],[0]
"For H , because ω is nondecreasing, this yields the fifth inequality with probability at least 1 − δ2 .",3. Representation Fixed by Source Task,[0],[0]
Applying the union bound achieves the desired result.,3. Representation Fixed by Source Task,[0],[0]
The second inequality is by the definition of ĝT and the fourth inequality follows from our assumption.,3. Representation Fixed by Source Task,[0],[0]
"In Theorem 2, we give an example of the property required by Theorem 1 which is specific to a particular problem setting.",3.1. Neural Network Example with Fixed Representation,[0],[0]
We consider a neural network with a single hidden layer (see Figure 2).,3.1. Neural Network Example with Fixed Representation,[0],[0]
"We propose transferring the lowerlevel weights (corresponding to f̂ ) learned on S, so that only the upper-level weights (corresponding to G) have to be learned on T .",3.1. Neural Network Example with Fixed Representation,[0],[0]
"We want to show f̂ is also useful for T .
",3.1. Neural Network Example with Fixed Representation,[0],[0]
"5We define ω by relating RS(ĝS ◦ f̂) to min g∈G RT (g ◦ f̂), since we expect this may be feasible analytically as in our example in Section 3.1.",3.1. Neural Network Example with Fixed Representation,[0],[0]
"However, because we only observe R̂S(ĝS ◦ f̂), in Theorem 1 we use this to bound RS(ĝS ◦ f̂) and then apply ω.
",3.1. Neural Network Example with Fixed Representation,[0],[0]
"To do this, we assume that some lower-level weights perform well on both tasks, which is clearly a necessary condition for the specific f̂ we are transferring to perform well on both tasks.",3.1. Neural Network Example with Fixed Representation,[0],[0]
We also assume PS and PT have the relative rotation invariance property and that the upper-level weights have fixed magnitude.,3.1. Neural Network Example with Fixed Representation,[0],[0]
"This is so that a point x for which f̂(x) contributes to the risk on T cannot be ‘hidden’ from the risk of using f̂ on S, either through low PS(x) or low magnitude upper-level weights.",3.1. Neural Network Example with Fixed Representation,[0],[0]
"Hence RS(ĝS ◦ f̂) reliably indicates the usefulness of f̂ on T .
",3.1. Neural Network Example with Fixed Representation,[0],[0]
LetX = Rn and Z = Rk.,3.1. Neural Network Example with Fixed Representation,[0],[0]
Let F be the function class such that f(x) =,3.1. Neural Network Example with Fixed Representation,[0],[0]
"[a(w1 · x), . . .",3.1. Neural Network Example with Fixed Representation,[0],[0]
", a(wk · x)], where wi ∈ Rn for 1 ≤",3.1. Neural Network Example with Fixed Representation,[0],[0]
"i ≤ k,",3.1. Neural Network Example with Fixed Representation,[0],[0]
a : R → R is an odd function6 and · is the dot product.,3.1. Neural Network Example with Fixed Representation,[0],[0]
"Let G be the function class such that g(z) = sign(v · z), where v ∈ {−1, 1}k.",3.1. Neural Network Example with Fixed Representation,[0],[0]
"Suppose ∃f ∈ F, gS , gT ∈",3.1. Neural Network Example with Fixed Representation,[0],[0]
"G such that max[RS(gS ◦f), RT (gT ◦f)] ≤ .",3.1. Neural Network Example with Fixed Representation,[0],[0]
Let f̂(x) :=,3.1. Neural Network Example with Fixed Representation,[0],[0]
"[a(ŵ1 · x), . . .",3.1. Neural Network Example with Fixed Representation,[0],[0]
", a(ŵk · x)].",3.1. Neural Network Example with Fixed Representation,[0],[0]
"Given wi and ŵi, pick nonzero constants αi and βi such that ||wi|| = ||αiŵi",3.1. Neural Network Example with Fixed Representation,[0],[0]
− βiwi|| and wi · (αiŵi − βiwi) = 0.,3.1. Neural Network Example with Fixed Representation,[0],[0]
"Let M be a 2k×nmatrix with rowsw1, α1ŵ1−β1w1, . . .",3.1. Neural Network Example with Fixed Representation,[0],[0]
", wk, αkŵk− βkwk.",3.1. Neural Network Example with Fixed Representation,[0],[0]
"Suppose M is full rank.7 Suppose ∀x, x′ such that ||Mx|| = ||Mx′||, PT (x) ≤ cPS(x′) for some c ≥ 1, which we call relative rotation invariance and implies PS and PT have the same support.",3.1. Neural Network Example with Fixed Representation,[0],[0]
"If M is an orthogonal matrix then ∀x, x′ such that ||x|| = ||x′||, PT (x) ≤ cPS(x′).8
Theorem 2.",3.1. Neural Network Example with Fixed Representation,[0],[0]
Let ω(R) := cR + (1 + c).,3.1. Neural Network Example with Fixed Representation,[0],[0]
"Then ∀ĝS ∈ G, min g∈G
RT (g ◦ f̂) ≤ ω(RS(ĝS ◦ f̂)).",3.1. Neural Network Example with Fixed Representation,[0],[0]
6i.e. a(−x),3.1. Neural Network Example with Fixed Representation,[0],[0]
= −a(x).,3.1. Neural Network Example with Fixed Representation,[0],[0]
"Examples are tanh, sign and identity.",3.1. Neural Network Example with Fixed Representation,[0],[0]
"7To see that this condition is necessary, consider the following example where M is not full rank.",3.1. Neural Network Example with Fixed Representation,[0],[0]
"Let n = 4, k = 2, hS = sign(x1) and hT = sign(x2).",3.1. Neural Network Example with Fixed Representation,[0],[0]
For f(x) =,3.1. Neural Network Example with Fixed Representation,[0],[0]
"[x1 + x2, x1 − x2], gS(z) = sign(z1 + z2) and gT",3.1. Neural Network Example with Fixed Representation,[0],[0]
"(z) = sign(z1 − z2), we have RS(gS ◦ f) = RT (gT ◦ f) = 0.",3.1. Neural Network Example with Fixed Representation,[0],[0]
On S we learn f̂(x) =,3.1. Neural Network Example with Fixed Representation,[0],[0]
"[x1 + x3, x1−x3] and ĝS(z)",3.1. Neural Network Example with Fixed Representation,[0],[0]
"= sign(z1 +z2), so thatRS(ĝS ◦ f̂) = 0",3.1. Neural Network Example with Fixed Representation,[0],[0]
"but in general min
g∈G",3.1. Neural Network Example with Fixed Representation,[0],[0]
RT (g ◦ f̂),3.1. Neural Network Example with Fixed Representation,[0],[0]
> 0,3.1. Neural Network Example with Fixed Representation,[0],[0]
"since f̂ ignores x2.
",3.1. Neural Network Example with Fixed Representation,[0],[0]
"8For example, PS and PT are spherical Gaussians.",3.1. Neural Network Example with Fixed Representation,[0],[0]
"For a zeromean multivariate Gaussian distribution this is achieved by the whitening transformation x → Λ−1/2UTx, where the columns of U and entries of the diagonal matrix Λ are the eigenvectors and eigenvalues of the distribution’s covariance matrix respectively.",3.1. Neural Network Example with Fixed Representation,[0],[0]
"Consider learning ĝS ◦ f̂ on S, and then using f̂ and RS(ĝS ◦ f̂) to find F̂ ⊆ F , as in Figure 1.",4. Representation Fine-Tuned on Target Task,[0],[0]
Let h̃g◦f be a stochastic hypothesis (i.e. a distribution over H) associated with g ◦ f,4. Representation Fine-Tuned on Target Task,[0],[0]
(e.g. g ◦ f is the mode of h̃g◦f ).,4. Representation Fine-Tuned on Target Task,[0],[0]
"We propose learning T with the hypothesis class H̃G◦F̂ := {h̃g◦f : f ∈ F̂ , g ∈ G} and the prior h̃ĝS◦f̂ .",4. Representation Fine-Tuned on Target Task,[0],[0]
"Learning T from scratch we assume that we would instead use H̃G◦F := {h̃g◦f : f ∈ F, g ∈ G} and some fixed prior h̃0 ∈ H̃G◦F .",4. Representation Fine-Tuned on Target Task,[0],[0]
"Let RT (h̃) := Ex∼PT ,h∼h̃[hT (x) 6= h(x)] and compute R̂T (h̃) on the training set distribution of T .
",4. Representation Fine-Tuned on Target Task,[0],[0]
"In Theorem 3 we show that if F̂ is ‘small enough’ so that all h̃ ∈ H̃G◦F̂ have a small KL divergence from h̃ĝS◦f̂ , we may apply a PAC-Bayes bound to the generalization error of hypotheses in H̃G◦F̂ involving four terms: a function ω measuring a transferrability property, the empirical risk R̂S(ĝS ◦ f̂), the generalization error of a hypothesis in H learned from mS points, and a weak dependence on mT .",4. Representation Fine-Tuned on Target Task,[0],[0]
"The value of the theorem is that if ω(R) = O(R), R̂S(ĝS◦f̂) is a small constant, andmS mT , we improve on the PAC-Bayes bound for H̃G◦F and h̃0.9 F̂ is useful if it is also ‘large enough’ in the sense that ∃h̃gT ◦f ∈",4. Representation Fine-Tuned on Target Task,[0],[0]
H̃G◦F̂ such that RT (h̃gT ◦f ) ≤ .,4. Representation Fine-Tuned on Target Task,[0],[0]
"Here ω quantifies how large the F̂ we search on T must be in order to be ‘large enough’, in terms of RS(ĝS ◦ f̂).",4. Representation Fine-Tuned on Target Task,[0],[0]
"While in general such an F̂ and ω may not exist, we give an example in Section 4.1.
",4. Representation Fine-Tuned on Target Task,[0],[0]
Theorem 3.,4. Representation Fine-Tuned on Target Task,[0],[0]
Let ω : R → R be non-decreasing.,4. Representation Fine-Tuned on Target Task,[0],[0]
"Suppose given f̂ ∈ F and RS(ĝS ◦ f̂) estimated from S, it is possible to construct F̂ with the property ∀h̃ ∈ H̃G◦F̂ , KL(h̃||h̃ĝS◦f̂ ) ≤ ω(RS(ĝS ◦ f̂)).",4. Representation Fine-Tuned on Target Task,[0],[0]
"Then with probability at least 1 − δ over pairs of training sets for tasks S and T , ∀h̃ ∈ H̃G◦F̂ , RT (h̃) ≤ R̂T (h̃) +√
ω(R̂S(ĝS◦",4. Representation Fine-Tuned on Target Task,[0],[0]
f̂)+2 √ 2dH log(2emS/dH ),4. Representation Fine-Tuned on Target Task,[0],[0]
"+2 log(8/δ)
mS )+log 2mT /δ
2(mT−1) .
",4. Representation Fine-Tuned on Target Task,[0],[0]
Proof.,4. Representation Fine-Tuned on Target Task,[0],[0]
"With probability at least 1− δ,
RT (h̃) ≤ R̂T (h̃) + √ KL(h̃||h̃ĝS◦f̂ )+log 2mT /δ 2(mT−1)
≤",4. Representation Fine-Tuned on Target Task,[0],[0]
"R̂T (h̃) + √
ω(RS(ĝS◦f̂))+log 2mT /δ 2(mT−1) .
",4. Representation Fine-Tuned on Target Task,[0],[0]
"The first inequality holds with probability at least 1 − δ2 (Shalev-Shwartz & Ben-David, 2014).",4. Representation Fine-Tuned on Target Task,[0],[0]
The second inequality holds by assumption.,4. Representation Fine-Tuned on Target Task,[0],[0]
"Furthermore, RS(ĝS ◦ f̂) ≤",4. Representation Fine-Tuned on Target Task,[0],[0]
"R̂S(ĝS ◦ f̂) + 2 √ 2dH log(2emS/dH)+2 log(8/δ)
mS with prob-
ability at least 1 − δ2 (Mohri et al., 2012) and ω is nondecreasing.",4. Representation Fine-Tuned on Target Task,[0],[0]
"The result follows from the union bound.
",4. Representation Fine-Tuned on Target Task,[0],[0]
9Using the restricted deterministic hypothesis class G ◦ F̂,4. Representation Fine-Tuned on Target Task,[0],[0]
":= {h : ∃f ∈ F̂ , g ∈ G such that h",4. Representation Fine-Tuned on Target Task,[0],[0]
"= g ◦ f} and a VC dimensionbased bound may not improve on H , since possibly dG◦F̂ = dH .",4. Representation Fine-Tuned on Target Task,[0],[0]
We transfer and fine-tune weights in a feedforward neural network with one hidden layer to instantiate the property required by Theorem 3.,4.1. Neural Network Example with Fine-Tuning,[0],[0]
We learn a deterministic hypothesis of this type on S and obtain k estimated lowerlevel weight vectors ŵi.,4.1. Neural Network Example with Fine-Tuning,[0],[0]
"Learning T we now consider only lower-level weights near ŵi, corresponding to F̂ .",4.1. Neural Network Example with Fine-Tuning,[0],[0]
On T we learn a stochastic hypothesis formed by taking a deterministic network and adding independent sources of spherical Gaussian noise to the lower-level weights and sign-flipping noise to the upper-level weights.,4.1. Neural Network Example with Fine-Tuning,[0],[0]
"The KL divergence between two of the stochastic hypotheses is expressed using the angles between their lower-level weights10 and a quantity computable from their upper-level weights.
",4.1. Neural Network Example with Fine-Tuning,[0],[0]
We want to prove that we can construct such an F̂ to successfully learn T .,4.1. Neural Network Example with Fine-Tuning,[0],[0]
"To do this, we assume some lower-level weights wi perform well on both S and T .",4.1. Neural Network Example with Fine-Tuning,[0],[0]
"We make F̂ ‘small enough’ by only including lower-level weights with small angles to ŵi, and ‘large enough’ by using the risk observed using ŵi on S to provide an upper bound on the angle between each pair wi and ŵi.",4.1. Neural Network Example with Fine-Tuning,[0],[0]
"Our assumptions ensure that poor ŵi cannot be ‘hidden’ from the risk on S, either through low PS density in the region of disagreement between wi and ŵi, or through low magnitude higher-level weights.",4.1. Neural Network Example with Fine-Tuning,[0],[0]
"Hence we know that searching F̂ will include wi.
",4.1. Neural Network Example with Fine-Tuning,[0],[0]
"Let X = Rn and Z = Rk, where k is odd.",4.1. Neural Network Example with Fine-Tuning,[0],[0]
Let F be the function class such that f(x) =,4.1. Neural Network Example with Fine-Tuning,[0],[0]
"[sign(w1 · x), . . .",4.1. Neural Network Example with Fine-Tuning,[0],[0]
", sign(wk · x)], where wi ∈ Rn for 1 ≤",4.1. Neural Network Example with Fine-Tuning,[0],[0]
i ≤ k.,4.1. Neural Network Example with Fine-Tuning,[0],[0]
Let G be the function class such that g(z),4.1. Neural Network Example with Fine-Tuning,[0],[0]
"= sign(v · z), where v ∈ {−1, 1}k.",4.1. Neural Network Example with Fine-Tuning,[0],[0]
"Let Bv be a distribution on {−1, 1}k such that for
v′ ∼",4.1. Neural Network Example with Fine-Tuning,[0],[0]
"Bv , Pr(v′) = k∏ j=1 p1(v ′",4.1. Neural Network Example with Fine-Tuning,[0],[0]
"j=vj)(1 − p)1(v ′ j=−vj), where p ∈",4.1. Neural Network Example with Fine-Tuning,[0],[0]
"[0.5, 1].",4.1. Neural Network Example with Fine-Tuning,[0],[0]
Let h̃g◦f := g′ ◦f ′,4.1. Neural Network Example with Fine-Tuning,[0],[0]
"such that v′, w′1, . . .",4.1. Neural Network Example with Fine-Tuning,[0],[0]
", w′k",4.1. Neural Network Example with Fine-Tuning,[0],[0]
"∼
",4.1. Neural Network Example with Fine-Tuning,[0],[0]
Bv k∏ i=1,4.1. Neural Network Example with Fine-Tuning,[0],[0]
"N (wi, σ2I).",4.1. Neural Network Example with Fine-Tuning,[0],[0]
"Suppose ∃f ∈ F, gS , gT ∈",4.1. Neural Network Example with Fine-Tuning,[0],[0]
"G such that max[RS(gS ◦ f), RT (h̃gT ◦f )] ≤ .",4.1. Neural Network Example with Fine-Tuning,[0],[0]
Let f̂(x) :=,4.1. Neural Network Example with Fine-Tuning,[0],[0]
"[sign(ŵ1 · x), . . .",4.1. Neural Network Example with Fine-Tuning,[0],[0]
", sign(ŵk · x)], θ(wi, ŵi) be the angle between wi and ŵi, and assume ∀i, ||ŵi|| = 1.",4.1. Neural Network Example with Fine-Tuning,[0],[0]
Define M as in Section 3.1.,4.1. Neural Network Example with Fine-Tuning,[0],[0]
"Let PS have the rotation invariance property ∀x, x′ such that ||Mx|| = ||Mx′||, PS(x) ≤ cPS(x′) for some c ≥ 1.
",4.1. Neural Network Example with Fine-Tuning,[0],[0]
Theorem 4.,4.1. Neural Network Example with Fine-Tuning,[0],[0]
"Given f̂ and RS(ĝS ◦ f̂) estimated from S, let θmax := π √ 2(k",4.1. Neural Network Example with Fine-Tuning,[0],[0]
− 1)c(RS(ĝS ◦ f̂) + ) and F̂ := {f ∈,4.1. Neural Network Example with Fine-Tuning,[0],[0]
"F : ∀i, ||wi|| = 1 ∧ |θ(wi, ŵi)| ≤ θmax}.",4.1. Neural Network Example with Fine-Tuning,[0],[0]
Let ω(R) := kσ2,4.1. Neural Network Example with Fine-Tuning,[0],[0]
[1−cos θmax]+k[2p−1+(1−p) k] log2 p 1−p .,4.1. Neural Network Example with Fine-Tuning,[0],[0]
"Then ∃h̃gT ◦f ∈ H̃G◦F̂ such that RT (h̃gT ◦f ) ≤ and ∀h̃ ∈ H̃G◦F̂ , KL(h̃||h̃ĝS◦f̂ ) ≤ ω(RS(ĝS ◦ f̂)).
",4.1. Neural Network Example with Fine-Tuning,[0],[0]
"10Assuming that the lower-level weight vectors are of fixed magnitude, which is no loss of model expressiveness since we use the sign activation function at the hidden layer.",4.1. Neural Network Example with Fine-Tuning,[0],[0]
"We show the utility of the risk bounds, and present a novel technique and experiments motivated by our theorems.",5. Applications,[0],[0]
"The results described yield tighter bounds on risk when transferring representations from S, compared to learning T from scratch.",5.1. Using the Risk Bounds,[0],[0]
"Examples are shown in Figure 3.11
We set δ = 0.05.",5.1. Using the Risk Bounds,[0],[0]
"For the top part, we use the example from Section 3.1 and set n = 10, k = 5.",5.1. Using the Risk Bounds,[0],[0]
"Learning T from scratch with H , we use the bound from Mohri et al. (2012) used previously.",5.1. Using the Risk Bounds,[0],[0]
"The VC dimension of a network of |E| edges using the sign activation is O(|E| log |E|) (Shalev-Shwartz & Ben-David, 2014), where in our case |E| = nk+k.",5.1. Using the Risk Bounds,[0],[0]
We use dH = |E| log |E| in the chart.,5.1. Using the Risk Bounds,[0],[0]
"Transferring a representation from S to T without fine-tuning, we consider the limit → 0, R̂S(ĝS ◦ f̂) → 0, mS → ∞, and hence ω(·) → 0 by Theorem 2.",5.1. Using the Risk Bounds,[0],[0]
"Furthermore, dG ≤",5.1. Using the Risk Bounds,[0],[0]
k,5.1. Using the Risk Bounds,[0],[0]
"since G is finite and hence dG ≤ log2 |G| (Shalev-Shwartz & Ben-David, 2014).",5.1. Using the Risk Bounds,[0],[0]
"We use the bound from Theorem 1.
",5.1. Using the Risk Bounds,[0],[0]
"For the bottom part, we use the example from Section 4.1 and set σ2 = 110 , k = 499, p = 2 3 .",5.1. Using the Risk Bounds,[0],[0]
Learning T from scratch we use the stochastic hypothesis class {h̃g◦f :,5.1. Using the Risk Bounds,[0],[0]
"f ∈ F such that ∀i||wi|| = 1, g ∈ G} and a prior h̃0 where ∀i wi = 0 and v ∈ {−1, 1}k is arbitrary.12 Hence we have the bound KL(h̃||h̃0) ≤ 10k + k3 , which becomes tight for large k.",5.1. Using the Risk Bounds,[0],[0]
"We apply the PAC-Bayes bound (ShalevShwartz & Ben-David, 2014) used previously.",5.1. Using the Risk Bounds,[0],[0]
"Transferring a representation from S and fine-tuning on T , we consider the limit → 0, R̂S(ĝS ◦ f̂) → 0, mS → ∞. We have KL(h̃||h̃ĝS◦f̂ ) ≤",5.1. Using the Risk Bounds,[0],[0]
k 3 by Theorem 4.,5.1. Using the Risk Bounds,[0],[0]
We use the bound from Theorem 3.,5.1. Using the Risk Bounds,[0],[0]
"We relax the hard constraint on F̂ from Section 4.1 by using a modified loss function, which we find performs better in practice.",5.2. Fine-Tuning through Regularization,[0],[0]
Let yi and ŷi be the label and prediction respectively for the ith training point.,5.2. Fine-Tuning through Regularization,[0],[0]
"In a fully-connected feedforward neural network with l layers of weights, let W (j) be the jth weight matrix, Ŵ (j) be its estimate from S (excluding weights for bias units in both cases), and ||·||2 be the entry-wise 2 norm.",5.2. Fine-Tuning through Regularization,[0],[0]
"A typical loss function (1) used for training is composed of the sum of training set log loss and L2 regularization on the weights.
",5.2. Fine-Tuning through Regularization,[0],[0]
"11Note that VC dimension risk bounds are known for being rather loose, while PAC-Bayesian bounds are tighter and hence yield non-trivial results in higher dimensions with fewer samples.
",5.2. Fine-Tuning through Regularization,[0],[0]
"12This class is as expressive as H̃G◦F but by setting ||wi|| = 1 the KL divergence of all hypotheses from any prior is bounded, allowing a fair comparison to H̃G◦F̂ .",5.2. Fine-Tuning through Regularization,[0],[0]
"The choice of h̃0 minimizes worst case KL divergence to a hypothesis in the class.
",5.2. Fine-Tuning through Regularization,[0],[0]
"m∑
i=1",5.2. Fine-Tuning through Regularization,[0],[0]
[−yi log ŷi,5.2. Fine-Tuning through Regularization,[0],[0]
− (1− yi) log(1− ŷi)],5.2. Fine-Tuning through Regularization,[0],[0]
"+ λ 2
l∑
j=1 (||W (j)||22)
(1)
We replace the regularization penalty with (2).13
l∑ j=1",5.2. Fine-Tuning through Regularization,[0],[0]
"[ λ1(j) 2 ||W (j) − Ŵ (j)||22 + λ2(j) 2 ||W (j)||22] (2) This penalizes estimates of W far from the representation learned on S. Since we expect the tasks to share a lowlevel representation (e.g. edge detectors for vision, word embeddings for text) but be distinct at higher levels (e.g. image components for vision, topics for text), we set λ1(·) to be a decreasing function, while λ2(·) controls standard L2 regularization.",5.2. Fine-Tuning through Regularization,[0],[0]
"The technique is novel to our knowledge, although other approaches to transferring regularization between tasks exist (Evgeniou & Pontil, 2004; Raina et al., 2006; Argyriou et al., 2008; Ghifary et al., 2014).",5.2. Fine-Tuning through Regularization,[0],[0]
We experiment on basic image and text classification tasks.14,5.3. Experiments,[0],[0]
We show that learning algorithms motivated by our theoretical results can help to overcome a scarcity of labeled target task data.,5.3. Experiments,[0],[0]
"Note that we do not replicate the conditions specified in our theorems, nor do we attempt extensive tuning to achieve state-of-the-art performance.
",5.3. Experiments,[0],[0]
"13Basing our approach on (1), we follow the convention that weights connected to bias units are excluded from the regularization penalty.",5.3. Experiments,[0],[0]
"However, the inclusion of these weights in the ||W (j) − Ŵ (j)|| term of (2) is a plausible variant.
",5.3. Experiments,[0],[0]
"14The MNIST and 20 Newgroups datasets are available at http://yann.lecun.com/exdb/mnist and http:// qwone.com/˜jason/20Newsgroups respectively.
",5.3. Experiments,[0],[0]
"We randomly partition label classes into sets S+ and S−, where |S+| =",5.3. Experiments,[0],[0]
"|S−|.15 We construct T+ by randomly picking from S+ up to γ :=
|S+∩T+| |S+| , then randomly picking
from S− such that |T+| = |T−|.",5.3. Experiments,[0],[0]
"We let S be the task of distinguishing between S+ and S− and T be that of distinguishing T+ and T−. Constructing S+ and T+ as disjunctions of classes means that the class labels are a perfect representation shared between S and T .
",5.3. Experiments,[0],[0]
"We compare the accuracy on T of four options:
• learn T from scratch (BASE)
• transfer f̂ from S, fine-tune f and train g on T using (2) (FINE-TUNE f̂ )
• transfer f̂ from S and fix, train g on T (FIX f̂ )16
• transfer ĝS ◦ f̂ from S and fix (FIX ĝS ◦ f̂ ).17
We use λ1(1) = λ2(2) = λ",5.3. Experiments,[0],[0]
":= 1,18 λ1(2) = λ2(1) = 0, mT = 500 and the sigmoid activation function.",5.3. Experiments,[0],[0]
"For MNIST we use raw pixel intensities, a 784 × 50 × 1 network andmS = 50000.",5.3. Experiments,[0],[0]
"For NEWSGROUPS we use TF-IDF weighted counts of most frequent words, a 2000 × 50 × 1 network and mS = 15000.",5.3. Experiments,[0],[0]
"We use conjugate gradient optimization with 200 iterations.
",5.3. Experiments,[0],[0]
"The results are shown in Table 1.19 When the tasks are nonidentical, FINE-TUNE f̂ is mostly the strongest but performs better on MNIST.",5.3. Experiments,[0],[0]
FIX f̂ outperforms BASE when γ ≥ 0.8 and hence the tasks are similar.,5.3. Experiments,[0],[0]
"While FIX f̂ outperforms FIX ĝS ◦ f̂ when the tasks are non-identical on MNIST, on NEWSGROUPS there is no evidence of benefit.",5.3. Experiments,[0],[0]
"When the tasks are identical, FIX ĝS ◦ f̂ is the strongest.
",5.3. Experiments,[0],[0]
"It appears that learning an MNIST digit requires a dense weight vector and so Ŵ (1) tends to encode single digits, which helps transferrability.",5.3. Experiments,[0],[0]
"However, it appears that since we may learn a newsgroup with a sparse weight vector, Ŵ (1) tends to encode disjunctions of newsgroups which somewhat reduces transferrability.",5.3. Experiments,[0],[0]
"When transferring representations does work, fine-tuning using the regularization penalty proposed in (2) improves performance.
",5.3. Experiments,[0],[0]
15For MNIST there are 10 label classes and for 20 Newgroups there are 20.,5.3. Experiments,[0],[0]
In both cases the classes are approximately balanced.,5.3. Experiments,[0],[0]
"Note that we ignore the hierarchical structure of the 20 Newsgroups classes, which likely contributes to the lower accuracies reported for all methods for this dataset relative to MNIST.
",5.3. Experiments,[0],[0]
16i.e.,5.3. Experiments,[0],[0]
logistic regression with L2 regularization and f̂ fixed.,5.3. Experiments,[0],[0]
17Used to isolate the benefit of transferring f̂ rather than ĝS ◦ f̂ .,5.3. Experiments,[0],[0]
"18We explored tuning λ to lift the performance of BASE on MNIST, but found that the results did not materially improve.",5.3. Experiments,[0],[0]
"Potentially λ1(j) and λ2(j) in (2) could be tuned with cross validation on the target task.
",5.3. Experiments,[0],[0]
"19For γ = 1, hS = hT .",5.3. Experiments,[0],[0]
"We do not consider γ < 0.5, since that is equivalent to 1−γ with the definitions of T+ and T− swapped.",5.3. Experiments,[0],[0]
We developed sufficient conditions for the successful transfer of representations both with and without fine-tuning.,6. Conclusion,[0],[0]
This is a step towards a principled explanation of the empirical success achieved by such techniques.,6. Conclusion,[0],[0]
A promising direction for future work is generalizing the neural network architectures considered (e.g. using multiple hidden layers) and relaxing the distributional assumptions required.,6. Conclusion,[0],[0]
"Furthermore, in the fine-tuning case it may be possible to upper bound the target task generalization error of hypotheses in G ◦ F̂",6. Conclusion,[0],[0]
":= {h : ∃f ∈ F̂ , g ∈ G such that h",6. Conclusion,[0],[0]
"= g ◦ f} using another measure such as the Rademacher complexity of G ◦ F̂ , eliminating the need for stochastic hypotheses.
",6. Conclusion,[0],[0]
"We proposed a novel form of regularization for neural network training motivated by our theoretical results, which penalizes divergence from source task weights and is stricter for lower-level weights.",6. Conclusion,[0],[0]
We validated this technique through applications to image and text classification.,6. Conclusion,[0],[0]
Future directions include experiments on more challenging tasks using deeper and more tailored network architectures (e.g. convolutional neural networks).,6. Conclusion,[0],[0]
We provide complete proofs of Theorems 2 and 4.,7. Additional Proofs,[0],[0]
"For brevity, we drop the explicit dependence of f , f̂ , hS and hT on x in our notation where the meaning is clear.",7. Additional Proofs,[0],[0]
Proof.,7.1. Proof of Theorem 2,[0],[0]
Let gS(z),7.1. Proof of Theorem 2,[0],[0]
":= sign(vS · z), gT (z) :",7.1. Proof of Theorem 2,[0],[0]
"= sign(vT · z), ĝS(z) := sign(v̂S · z), ĝT (z) := sign(d ∗ v̂S · z), where d := vS ∗vT ∈ {−1, 1}k and ∗ is the elementwise product.",7.1. Proof of Theorem 2,[0],[0]
"It is sufficient to showRT (ĝT ◦f̂) ≤ cRS(ĝS◦f̂)+ (1+c).
",7.1. Proof of Theorem 2,[0],[0]
"RT (ĝT ◦ f̂)
=",7.1. Proof of Theorem 2,[0],[0]
"Prx∼PT (hT d ∗ v̂S · f̂ ≤ 0)
≤",7.1. Proof of Theorem 2,[0],[0]
"Prx∼PT (hT d ∗ vS · f ≤ 0, d ∗ vS · fd ∗ v̂S · f̂ ≥ 0)",7.1. Proof of Theorem 2,[0],[0]
"+ Prx∼PT (hT d ∗ vS · f ≥ 0, d ∗ vS · fd ∗ v̂S · f̂ ≤ 0)
≤",7.1. Proof of Theorem 2,[0],[0]
Prx∼PT (hT d ∗ vS · f ≤,7.1. Proof of Theorem 2,[0],[0]
"0)+ Prx∼PT (d ∗ vS · fd ∗ v̂S · f̂ ≤ 0)
≤",7.1. Proof of Theorem 2,[0],[0]
"+ Prx∼PT (d ∗ vS · fd ∗ v̂S · f̂ ≤ 0)
≤ + cPrx∼PS (vS · fv̂S · f̂ ≤ 0)
≤",7.1. Proof of Theorem 2,[0],[0]
"+ c[Prx∼PS (hS v̂S · f̂ ≤ 0, hSvS · f ≥ 0)+ Prx∼PS (hS v̂S · f̂ ≥ 0, hSvS · f ≤ 0)]
≤ + c[Prx∼PS (hS v̂S · f̂ ≤ 0)",7.1. Proof of Theorem 2,[0],[0]
"+Prx∼PS (hSvS · f ≤ 0)]
≤ cRS(ĝS ◦ f̂) + (1 + c).
",7.1. Proof of Theorem 2,[0],[0]
The third and final inequalities are due to the shared representation assumption in the problem statement.,7.1. Proof of Theorem 2,[0],[0]
The fourth inequality holds by Lemma 1.,7.1. Proof of Theorem 2,[0],[0]
"The remaining lines apply simple rules of probability.
",7.1. Proof of Theorem 2,[0],[0]
Lemma 1.,7.1. Proof of Theorem 2,[0],[0]
"Suppose ∀x, x′ such that ||Mx|| = ||Mx′||, PT (x) ≤ cPS(x′).",7.1. Proof of Theorem 2,[0],[0]
"Let f, f̂ ∈ F , v, v̂, d ∈ {−1, 1}k.",7.1. Proof of Theorem 2,[0],[0]
"Then Prx∼PT (d∗v ·fd∗ v̂ · f̂ ≤ 0) ≤ cPrx∼PS (v · fv̂ · f̂ ≤ 0).
",7.1. Proof of Theorem 2,[0],[0]
Proof.,7.1. Proof of Theorem 2,[0],[0]
"Suppose there is an invertible map Rn → Rn yielding x′ on input x, such that ∀x, ||Mx|| = ||Mx′|| and d ∗ v · f(x)d ∗ v̂ · f̂(x) = v · f(x′)v̂ · f̂(x′).",7.1. Proof of Theorem 2,[0],[0]
Then the result follows since PT (x) ≤ cPS(x′) by assumption.,7.1. Proof of Theorem 2,[0],[0]
"Furthermore, if M is an orthogonal matrix, ||x|| = ||x′||.
",7.1. Proof of Theorem 2,[0],[0]
"Such a map is x′ := (MTM)−1MT d̃ ∗ (Mx), where d̃ :=",7.1. Proof of Theorem 2,[0],[0]
"[d1, d1, . . .",7.1. Proof of Theorem 2,[0],[0]
", dk, dk].",7.1. Proof of Theorem 2,[0],[0]
"We have ∀i, wi · x′ = diwi · x and (αiŵi−βiwi)·x′ = di(αiŵi−βiwi)·x, and hence ŵi ·x′ = diŵi · x for αi, βi 6= 0.",7.1. Proof of Theorem 2,[0],[0]
"Therefore:
d ∗ v · f(x)d ∗ v̂ · f̂(x)
= v · d ∗ f(x)v̂ · d ∗ f̂(x)
= v · f(x′)v̂ · d ∗ f̂(x)
= v · f(x′)v̂ · f̂(x′).
",7.1. Proof of Theorem 2,[0],[0]
The first equality is a property of the elementwise and dot products.,7.1. Proof of Theorem 2,[0],[0]
"For the second equality, a(wi·x′) = a(diwi·x) = dia(wi · x) since a is an odd function.",7.1. Proof of Theorem 2,[0],[0]
"Similarly, for the third equality a(ŵi · x′) = a(diŵi · x) = dia(ŵi · x).",7.1. Proof of Theorem 2,[0],[0]
Proof of ∃h̃gT ◦f ∈,7.2. Proof of Theorem 4,[0],[0]
H̃G◦F̂ such that RT (h̃gT ◦f ) ≤ .,7.2. Proof of Theorem 4,[0],[0]
Recall that wi are the weight vectors for f and ŵi are those for f̂ .,7.2. Proof of Theorem 4,[0],[0]
"Observe that for any wi such that wi · ŵi < 0, we have −wi · ŵi > 0 and −visign(−wi · x) =",7.2. Proof of Theorem 4,[0],[0]
visign(wi · x).,7.2. Proof of Theorem 4,[0],[0]
"Combining this with the assumption
min f∈F,gS ,gT∈G max[RS(gS ◦ f), RT (gT ◦",7.2. Proof of Theorem 4,[0],[0]
"f)] ≤ , we conclude ∃f ∈ F, gS , gT ∈ G such that ∀i, wi · ŵi ≥ 0 and max[RS(gS ◦ f), RT (h̃gT ◦f )] ≤ .
",7.2. Proof of Theorem 4,[0],[0]
Let gS(z),7.2. Proof of Theorem 4,[0],[0]
:= sign(vS · z) and ĝS(z) := sign(v̂S · z).,7.2. Proof of Theorem 4,[0],[0]
Let P be a rotation invariant distribution for c = 1.,7.2. Proof of Theorem 4,[0],[0]
"To prove h̃gT ◦f ∈ H̃G◦F̂ , by the definition of H̃G◦F̂ it is sufficient to show ∀i, |θ(wi, ŵi)| ≤ π",7.2. Proof of Theorem 4,[0],[0]
√ 2(k,7.2. Proof of Theorem 4,[0],[0]
"− 1)c(RS(ĝS ◦ f̂) + ).
max i |θ(wi,ŵi)|
π √ 2(k−1)
≤ Prx∼P",7.2. Proof of Theorem 4,[0],[0]
"(vS · fvS · f̂ ≤ 0)
≤ Prx∼P",7.2. Proof of Theorem 4,[0],[0]
"(vS · fv̂S · f̂ ≤ 0)
≤ cPrx∼PS (vS · fv̂S · f̂ ≤ 0)
≤ c[Prx∼PS (hSvS · f ≤ 0, hS v̂S · f̂ ≥ 0)+ Prx∼PS (hSvS · f ≥ 0, hS v̂S · f̂ ≤ 0)]
≤ c[Prx∼PS (hSvS · f ≤ 0) + Prx∼PS (hS v̂S · f̂ ≤ 0)]
≤ c",7.2. Proof of Theorem 4,[0],[0]
"[ +RS(ĝS ◦ f̂)].
",7.2. Proof of Theorem 4,[0],[0]
The first inequality holds by Lemma 2.,7.2. Proof of Theorem 4,[0],[0]
"The second inequality holds by Lemma 3, using the fact ∀i, wi · ŵi ≥ 0.",7.2. Proof of Theorem 4,[0],[0]
The third inequality uses the rotation invariance of PS .,7.2. Proof of Theorem 4,[0],[0]
The following two lines use basic laws of probability.,7.2. Proof of Theorem 4,[0],[0]
"The final inequality uses the assumption RS(gS ◦ f) ≤ .
",7.2. Proof of Theorem 4,[0],[0]
"Proof of ∀h̃ ∈ H̃G◦F̂ ,KL(h̃||h̃ĝS◦f̂ ) ≤ ω(RS(ĝS ◦ f̂)).",7.2. Proof of Theorem 4,[0],[0]
"For any h̃g◦f ∈ H̃G◦F̂ , KL(h̃g◦f ||h̃ĝS◦f̂ ) = k∑ i=1",7.2. Proof of Theorem 4,[0],[0]
"[KL(N (wi, σ2I)||N (ŵi, σ2I))]",7.2. Proof of Theorem 4,[0],[0]
"+KL(Bv||Bv̂S ).
",7.2. Proof of Theorem 4,[0],[0]
The KL divergence of a product distribution is the sum of the KL divergences of its component distributions.,7.2. Proof of Theorem 4,[0],[0]
We upper bound both terms and apply the definition of ω.,7.2. Proof of Theorem 4,[0],[0]
k∑ i=1,7.2. Proof of Theorem 4,[0],[0]
"KL(N (wi, σ2I)||N (ŵi, σ2I))",7.2. Proof of Theorem 4,[0],[0]
= 12σ2 k∑ i=1,7.2. Proof of Theorem 4,[0],[0]
||wi,7.2. Proof of Theorem 4,[0],[0]
− ŵi||2 = 12σ2 k∑ i=1,7.2. Proof of Theorem 4,[0],[0]
"(||wi||2 + ||ŵi||2 − 2||wi||||ŵi|| cos |θ(wi, ŵi)|)",7.2. Proof of Theorem 4,[0],[0]
= 1σ2 k∑ i=1,7.2. Proof of Theorem 4,[0],[0]
"(1− cos |θ(wi, ŵi)|)
≤ kσ2",7.2. Proof of Theorem 4,[0],[0]
[1− cos(π √ 2(k,7.2. Proof of Theorem 4,[0],[0]
"− 1)c(RS(ĝS ◦ f̂) + ))].
",7.2. Proof of Theorem 4,[0],[0]
The first equality uses the KL divergence of Gaussian distributions.,7.2. Proof of Theorem 4,[0],[0]
The second equality uses the law of cosines.,7.2. Proof of Theorem 4,[0],[0]
"The third equality is because ∀i, ||wi|| = ||ŵi|| = 1 by construction.",7.2. Proof of Theorem 4,[0],[0]
The inequality follows by the definition of F̂ and the fact that 1− cos |θ| is non-decreasing for |θ| ∈,7.2. Proof of Theorem 4,[0],[0]
"[0, π].
KL(Bv||Bv̂S )",7.2. Proof of Theorem 4,[0],[0]
≤ k∑ i=1,7.2. Proof of Theorem 4,[0],[0]
"( k i ) pi(1− p)k−i log2 pi(1−p)k−i (1−p)ipk−i
= k[2p− 1 + (",7.2. Proof of Theorem 4,[0],[0]
"1− p)k] log2 p 1−p .
",7.2. Proof of Theorem 4,[0],[0]
The first inequality uses the definition of Bv to express KL(Bv||Bv̂S ).,7.2. Proof of Theorem 4,[0],[0]
"The equality is a simplification.
",7.2. Proof of Theorem 4,[0],[0]
Lemma 2.,7.2. Proof of Theorem 4,[0],[0]
"Suppose k is odd, v ∈ {−1, 1}k, f, f̂ ∈ F such that ∀i, wi · ŵi ≥ 0 and P is rotation invariant with c = 1.",7.2. Proof of Theorem 4,[0],[0]
"Then max i |θ(wi,ŵi)|
π √ 2(k−1) ≤ Prx∼P (v · fv · f̂ ≤ 0).
",7.2. Proof of Theorem 4,[0],[0]
Proof.,7.2. Proof of Theorem 4,[0],[0]
"Let v−j := [v1, . . .",7.2. Proof of Theorem 4,[0],[0]
", vj−1, vj+1, . . .",7.2. Proof of Theorem 4,[0],[0]
", vk] and define f−j and f̂−j similarly.",7.2. Proof of Theorem 4,[0],[0]
"Let Pr(·) := Prx∼P (·).
",7.2. Proof of Theorem 4,[0],[0]
"Pr(v · fv · f̂ ≤ 0)
",7.2. Proof of Theorem 4,[0],[0]
"≥ Pr(v · fv · f̂ < 0)
≥ Pr(v−j ·",7.2. Proof of Theorem 4,[0],[0]
"f−j = 0)Pr(v · fv · f̂ < 0|v−j · f−j = 0)
",7.2. Proof of Theorem 4,[0],[0]
= Pr(v−j · f−j = 0) Pr(vjfjv−j · f̂−j + fj,7.2. Proof of Theorem 4,[0],[0]
"f̂j < 0|v−j · f−j = 0)
= Pr(v−j · f−j = 0)",7.2. Proof of Theorem 4,[0],[0]
"[Pr(vjfjv−j · f̂−j < −1, fj",7.2. Proof of Theorem 4,[0],[0]
"f̂j = 1|v−j · f−j = 0)+ Pr(vjfjv−j · f̂−j < 1, fj f̂j = −1|v−j · f−j = 0)]
≥ Pr(v−j · f−j = 0)",7.2. Proof of Theorem 4,[0],[0]
"[Pr(vjfjv−j · f̂−j < −1, fj",7.2. Proof of Theorem 4,[0],[0]
"f̂j = −1|v−j · f−j = 0)+ Pr(vjfjv−j · f̂−j < 1, fj f̂j = −1|v−j · f−j = 0)]
= Pr(v−j · f−j = 0)",7.2. Proof of Theorem 4,[0],[0]
"[Pr(vjfjv−j · f̂−j < −1, fj",7.2. Proof of Theorem 4,[0],[0]
"f̂j = −1|v−j · f−j = 0)+ Pr(vjfjv−j · f̂−j > −1, fj f̂j = −1|v−j · f−j = 0)]
= Pr(v−j · f−j = 0)Pr(fj f̂j = −1|v−j · f−j = 0)
= Pr(v−j · f−j = 0)Pr(fj f̂j = −1) = (k−1 k−1 2 ) ( 12 ) k−1 |θ(wj ,ŵj)| π
≥ 2 k−1√
2(k−1) ( 12 ) k−1 |θ(wj ,ŵj)| π
≥ max i |θ(wi,ŵi)|
π √ 2(k−1) .
",7.2. Proof of Theorem 4,[0],[0]
The third inequality follows since P is rotation invariant and wj · ŵj ≥ 0.,7.2. Proof of Theorem 4,[0],[0]
The third and fifth equalities use rotation invariance.,7.2. Proof of Theorem 4,[0],[0]
The final equality uses rotation invariance and the fact that k is odd.,7.2. Proof of Theorem 4,[0],[0]
The fourth inequality is a standard lower bound for the central binomial coefficient.,7.2. Proof of Theorem 4,[0],[0]
"The other lines use basic simplifications and laws of probability.
",7.2. Proof of Theorem 4,[0],[0]
Lemma 3.,7.2. Proof of Theorem 4,[0],[0]
"Suppose k is odd, v, v̂ ∈ {−1, 1}k, f, f̂ ∈ F such that ∀i, wi · ŵi ≥ 0",7.2. Proof of Theorem 4,[0],[0]
and P is rotation invariant with c = 1.,7.2. Proof of Theorem 4,[0],[0]
Then Prx∼P,7.2. Proof of Theorem 4,[0],[0]
(v · fv · f̂ ≤ 0) ≤ Prx∼P,7.2. Proof of Theorem 4,[0],[0]
(v · fv̂ · f̂ ≤ 0).,7.2. Proof of Theorem 4,[0],[0]
Proof.,7.2. Proof of Theorem 4,[0],[0]
Let Pr(·) := Prx∼P (·) and E[·] := Ex∼P,7.2. Proof of Theorem 4,[0],[0]
[·].,7.2. Proof of Theorem 4,[0],[0]
Let Pr(f̃) := Prx∼P,7.2. Proof of Theorem 4,[0],[0]
"([f1(x)f̂1(x), . . .",7.2. Proof of Theorem 4,[0],[0]
", fk(x)f̂k(x)]",7.2. Proof of Theorem 4,[0],[0]
= f̃).,7.2. Proof of Theorem 4,[0],[0]
Let d := v̂ ∗ v and ∆(x) :,7.2. Proof of Theorem 4,[0],[0]
= 1(v · f(x)v̂ · f̂(x) ≤ 0),7.2. Proof of Theorem 4,[0],[0]
− 1(v · f(x)v · f̂(x) ≤ 0).,7.2. Proof of Theorem 4,[0],[0]
"Assume v̂ 6= v (if v̂ = v then
the lemma clearly holds).",7.2. Proof of Theorem 4,[0],[0]
Let a(f̃) := k∑ i=1,7.2. Proof of Theorem 4,[0],[0]
1(f̃i = 1) and let l :=,7.2. Proof of Theorem 4,[0],[0]
min i:di=−1 i.,7.2. Proof of Theorem 4,[0],[0]
Let,7.2. Proof of Theorem 4,[0],[0]
"F̃ := {f̃ ∈ {−1, 1}k :",7.2. Proof of Theorem 4,[0],[0]
"a(f̃) > a(d ∗ f̃) ∨ (a(f̃) = a(d ∗ f̃) ∧ f̃l = 1)}.
",7.2. Proof of Theorem 4,[0],[0]
Let Φ(a) := 1 2k−1 bk/2c∑ b=0 b∑ j=da/2+b/2−k/4e ( a j )( k−a b−j ) .,7.2. Proof of Theorem 4,[0],[0]
"The
term b counts coordinates where vif̂i = sign(v · f), while j counts those where vifi = sign(v · f) and fi = f̂i.
Pr(v · fv̂ · f̂ ≤ 0)− Pr(v · fv · f̂ ≤ 0)
= E[1(v · fv̂ · f̂ ≤ 0)]− E[1(v · fv · f̂ ≤ 0)]
= E[∆]",7.2. Proof of Theorem 4,[0],[0]
= ∑̃ f∈F̃,7.2. Proof of Theorem 4,[0],[0]
Pr(f̃)E[∆|f̃ ] + Pr(d ∗,7.2. Proof of Theorem 4,[0],[0]
"f̃)E[∆|d ∗ f̃ ]
= ∑̃ f∈F̃",7.2. Proof of Theorem 4,[0],[0]
"[Pr(f̃)− Pr(d ∗ f̃)]E[∆|f̃ ]
= ∑̃ f∈F̃",7.2. Proof of Theorem 4,[0],[0]
"[Pr(f̃)− Pr(d ∗ f̃)]
",7.2. Proof of Theorem 4,[0],[0]
[Pr(v · fv · f̂ ≤ 0|d ∗ f̃)− Pr(v · fv · f̂ ≤ 0|f̃)],7.2. Proof of Theorem 4,[0],[0]
= ∑̃ f∈F̃,7.2. Proof of Theorem 4,[0],[0]
"[Pr(f̃)− Pr(d ∗ f̃)][Φ(a(d ∗ f̃))− Φ(a(f̃))]
≥ 0.
",7.2. Proof of Theorem 4,[0],[0]
The second equality uses linearity of expectation.,7.2. Proof of Theorem 4,[0],[0]
"The third equality uses the law of total expectation and the definition of F̃ .
",7.2. Proof of Theorem 4,[0],[0]
"The fourth equality holds since E[∆|d ∗ f̃ ] = ∑ f∈{−1,1}k Pr(f |d ∗",7.2. Proof of Theorem 4,[0],[0]
"f̃)E[∆|d ∗ f̃ , f ]
=",7.2. Proof of Theorem 4,[0],[0]
"− ∑
f∈{−1,1}k Pr(f |d ∗ f̃)E[∆|f̃ , f ]
= − ∑
f∈{−1,1}k Pr(f |f̃)E[∆|f̃ , f ]",7.2. Proof of Theorem 4,[0],[0]
=,7.2. Proof of Theorem 4,[0],[0]
"−E[∆|f̃ ] due to the
rotation invariance of P .
",7.2. Proof of Theorem 4,[0],[0]
"The fifth equality holds by expanding ∆, linearity of expectation, and a similar argument to the previous equality to show Pr(v · fv̂ · f̂ ≤ 0|f̃) = Pr(v · fv · f̂ ≤ 0|d ∗ f̃).
",7.2. Proof of Theorem 4,[0],[0]
"The sixth equality holds by the rotation invariance of P and the fact that k is odd.
",7.2. Proof of Theorem 4,[0],[0]
"For the final inequality, the right hand term is non-negative since a(f̃) ≥ a(d ∗ f̃) and Φ is non-increasing.",7.2. Proof of Theorem 4,[0],[0]
"The left hand term is also non-negative due to the rotation invariance assumption and the fact that ∀i, wi · ŵi ≥ 0.",7.2. Proof of Theorem 4,[0],[0]
"Daniel McNamara was a visitor at Carnegie Mellon University during the period of this research, supported by a Fulbright Postgraduate Scholarship.
",Acknowledgements,[0],[0]
"This work was supported in part by NSF grants CCF1422910, CCF-1535967, IIS-1618714, and a Microsoft Research Faculty Fellowship.
",Acknowledgements,[0],[0]
We thank the anonymous reviewers for their useful comments.,Acknowledgements,[0],[0]
A popular machine learning strategy is the transfer of a representation (i.e. a feature extraction function) learned on a source task to a target task.,abstractText,[0],[0]
Examples include the re-use of neural network weights or word embeddings.,abstractText,[0],[0]
We develop sufficient conditions for the success of this approach.,abstractText,[0],[0]
"If the representation learned from the source task is fixed, we identify conditions on how the tasks relate to obtain an upper bound on target task risk via a VC dimension-based argument.",abstractText,[0],[0]
"We then consider using the representation from the source task to construct a prior, which is fine-tuned using target task data.",abstractText,[0],[0]
We give a PAC-Bayes target task risk bound in this setting under suitable conditions.,abstractText,[0],[0]
We show examples of our bounds using feedforward neural networks.,abstractText,[0],[0]
"Our results motivate a practical approach to weight transfer, which we validate with experiments.",abstractText,[0],[0]
Risk Bounds for Transferring Representations With and Without Fine-Tuning,title,[0],[0]
"1Massachusetts Host-Microbiome Center, Brigham and Women’s Hospital, Harvard Medical School, Boston, MA, USA. Correspondence to: TE Gibson <tgibson@mit.edu>, GK Gerber <ggerber@bwh.harvard.edu>.
Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).",text,[0],[0]
"The human microbiome constitutes all the microorganisms that live in and on our bodies (The Human Microbiome Project Consortium, 2012).",1. Introduction,[0],[0]
"There is strong evidence that the microbiome plays an important role in a variety of human diseases, including: infections, arthritis, food allergy, cancer, inflammatory bowel disease, neurological diseases, and obesity/diabetes (Hall et al., 2017; Youngster et al., 2014; Stefka et al., 2014; Schwabe & Jobin, 2013; Kostic et al., 2015; Wlodarska et al., 2015).",1. Introduction,[0],[0]
"Given the microbiome’s profound role, there is now a concerted effort to design bacteriotherapies, which are cocktails of multiple bacteria working in concert to achieve specific therapeutic effects.",1. Introduction,[0],[0]
"Multiple strains are often needed in bacteriotherpies both because multiple host pathways must be targeted, and because additional bacteria may provide stability or robustness to the community as a whole.",1. Introduction,[0],[0]
An important step toward designing bacteriotherapies is mapping out microbial interactions and predicting population dynamics of this ecosystem.,1. Introduction,[0],[0]
"One approach toward this goal, and arguably the most popular, is to learn dynamical systems models from time series measurements of microbiome abundance data.",1. Introduction,[0],[0]
"That is, one takes as input time series of microbiome abundances as depicted in Figure 1A and infers a dynamical systems model of microbial interactions as in Figure 1B. These data typically consist of two separate measurements: (1) high-throughput next generation sequencing counts of a marker gene (16S rRNA) mapped back to different microbial species or other taxonomic units (often 300+), to determine relative abundances of each unit, and (2) quantitative PCR (qPCR) measurements to determine the total concentration of bacteria in the ecosystem.
",1. Introduction,[0],[0]
Inferring dynamical systems models from microbiome time series data presents several challenges.,1. Introduction,[0],[0]
"The biggest challenge arises from the fact that the data is high-dimensional, yet temporally sparse and non-uniformly sampled.",1. Introduction,[0],[0]
"With 300 or more bacterial species in the gut, the resulting differential equation models can have more than 90,000 possible interaction parameters.",1. Introduction,[0],[0]
"However, unlike other biomedical domains where almost continuous temporal sampling is feasible (e.g., electrical recordings of cardiac activity), this is not currently possible for the gut microbiome.",1. Introduction,[0],[0]
"Instead, we must rely on fecal samples (or even more invasive processes, such as colonoscopy), which means that we are quite lim-
ited in terms of the frequency and total number of samples.",1. Introduction,[0],[0]
"Further, the techniques used to obtain estimates of microbial abundance are noisy, and with multiple technologies being combined (i.e., next generation sequencing and qPCR), the resulting measurement error models are relatively complex.",1. Introduction,[0],[0]
"Finally, the microbiome exhibits nonlinear and physically nonnegative dynamics, which introduce additional inference issues.",1. Introduction,[0],[0]
We now briefly review previous work in inferring dynamical systems from microbiome time-series data.,1.1. Prior work,[0],[0]
"The authors of (Stein et al., 2013) model microbial dynamics using continuous time deterministic generalized Lotka-Volterra (gLV) equations, transform to a discrete time linear model via a log transform to enable efficient inference, and then use L2 penalized linear regression to infer model parameters.",1.1. Prior work,[0],[0]
"The transformation performed in (Stein et al., 2013) is common in the ecological literature, and provides a point of comparison to our model, so we present it in detail now.",1.1. Prior work,[0],[0]
"Deterministic gLV dynamics can be written compactly as the Ordinary Differential Equation (ODE) ẋ(t) = x(t) (r + Ax(t)), where is the element wise product for vectors, r is a vector of growth rates and A is a matrix of interaction coefficients.",1.1. Prior work,[0],[0]
"Using for element wise division, the following representation of the ODE also holds: ẋ(t) x(t) = r + Ax(t).",1.1. Prior work,[0],[0]
The left hand side of the equivalent ODE can then be integrated resulting in the following identity: ∫ t2 t1 ẋ(t) x(t),1.1. Prior work,[0],[0]
dt = log(x(t2))− log(x(t1)).,1.1. Prior work,[0],[0]
This property of the logarithm can then be used to approximate the continuous time nonlinear ODE as a discrete time linear dynamical system.,1.1. Prior work,[0],[0]
There are a variety of both theoretical and practical issues with using this approximation.,1.1. Prior work,[0],[0]
"For instance, the transformation does not readily apply for stochastic dynamics.",1.1. Prior work,[0],[0]
"Additionally, the transform essentially assumes normally distributed error, which is inherently false, since data typically consist of sequences of counts.",1.1. Prior work,[0],[0]
"Further, we often encounter measurements of zero for microbial abundance, i.e., below the limit of detection, which would lead to taking the log of zero or adding an artificial small number.
",1.1. Prior work,[0],[0]
"Other work on inferring dynamical systems models from microbiome data includes (Fisher & Mehta, 2014), which takes a similar approach to (Stein et al., 2013), but instead of L2 penalized regression, use a sparse linear regression with bootstrap aggregation approach.",1.1. Prior work,[0],[0]
No regularization is performed and sparsity is introduced into the model by adding and removing interaction coefficients one at a time with step-wise regression.,1.1. Prior work,[0],[0]
"Several inference techniques are presented in (Bucci et al., 2016), two being extensions of the model proposed in (Stein et al., 2013) and two being new Bayesian models.",1.1. Prior work,[0],[0]
"The Bayesian models in (Bucci et al., 2016) are based on ODE gradient matching, in which
Bayesian spline smoothing is first performed to filter the experimental measurements, and then a Bayesian adaptive lasso or Bayesian variable selection method is used to infer model parameters.",1.1. Prior work,[0],[0]
"These methods do incorporate nonnormally distributed measurement error models, but errors are not propagated throughout the model, i.e., smoothing and filtering are separate steps.",1.1. Prior work,[0],[0]
"Finally, in (Alshawaqfeh et al., 2017) an Extended Kalman Filter (EKF) is applied to a stochastic gLV model, which incorporates filtering directly, unlike the aforementioned references; however, noise is assumed to be normally distributed.
",1.1. Prior work,[0],[0]
"Beyond microbiome specific dynamical systems inference approaches, there is an extensive body of work on Bayesian inference of nonlinear dynamical systems, which remains an active area of research (Ionides et al., 2006; Carlin et al., 1992; Aguilar et al., 1998; Geweke & Tanizaki, 2001).",1.1. Prior work,[0],[0]
"An interesting line of recent work leverages Gaussian Processes (GP) as a means for efficient filtering for both ordinary differential equations and partial differential equations (Chkrebtii et al., 2016).",1.1. Prior work,[0],[0]
"One of the catalysts for this line of work came from (Calderhead et al., 2009), in which a GP is used to infer the latent state variables, which in turn are used to infer parameters of an ODE.",1.1. Prior work,[0],[0]
"Extending that work, (Dondelinger et al., 2013) apply a gradient matching approach (marginalizing over state derivatives) and perform joint inference on the ODE parameters and latent state variables.",1.1. Prior work,[0],[0]
"However, several subsequent papers pointed out identifiability and efficiency issues with these approaches (Barber & Wang, 2014; Macdonald et al., 2015).",1.1. Prior work,[0],[0]
"More recently, (Gorbach et al., 2017; Bauer et al., 2017) presented a variational inference approach that addresses some of these issues.",1.1. Prior work,[0],[0]
"While we do not explore GPs in this work, they are an interesting and promising direction within the broader domain of Bayesian inference for nonlinear dynamical systems.",1.1. Prior work,[0],[0]
"Dynamic Bayesian Networks (DBN) also represent a broad class of state-space models leveraged for inference of dynamical systems given time series data (Murphy, 2002).",1.1. Prior work,[0],[0]
"Our model differs from a standard DBN, in that it learns the conditional independence structure in a latent temporal space,
and clusters the nodes in the graph nonparametrically.
",1.1. Prior work,[0],[0]
"Also related to our work are models that learn clustered representations of interacting systems, both for purposes of enhancing interpretability and for increasing efficiency of inference.",1.1. Prior work,[0],[0]
"Related approaches include Stochastic Block Models (SBM), in particular (Kemp et al., 2006), which model redundant interaction structure as probabilistic linkages between individual actors that are influenced by the blocks/groups that the actors belong to.",1.1. Prior work,[0],[0]
"SBMs typically directly model observed, non-temporal data, whereas our approach models latent temporal signals; further, our approach enforces identical interaction structure on variables in the same cluster, whereas SBMs assume a probabilistic interaction structure.",1.1. Prior work,[0],[0]
"Dependent groups/clusters have also been explored in the context of Topic Models (e.g., (Mimno et al., 2007)).",1.1. Prior work,[0],[0]
"There is also an extensive literature on Dependent Dirichlet Processes (MacEachern, 2000), which can be used to capture complex interactions between clusters, and also simpler structures (e.g., hierarchies as in (Teh et al., 2006)).",1.1. Prior work,[0],[0]
In this work we present a Bayesian nonparametric model and associated efficient inference algorithm that addresses the key conceptual and practical challenges of learning microbial dynamics from time series microbe abundance data.,1.2. Contributions,[0],[0]
"Our main contributions are:
• A new type of dynamical systems model for microbial dynamics based on what we term interaction modules, or probabilistic clusters of latent variables with redundant interaction structure.",1.2. Contributions,[0],[0]
"The aggregated concentrations of microbes in a module act as consolidated inputs to other modules, with structural learning of the network of interactions among modules.
",1.2. Contributions,[0],[0]
• A fully Bayesian formulation of the stochastic dynamical systems model that propagates measurement and latent state uncertainty throughout the model.,1.2. Contributions,[0],[0]
"This integrated approach improves on the previous work described for microbiome dynamics (which assumed deterministic dynamics and separated learning of latent states and ODE parameters).
",1.2. Contributions,[0],[0]
• Introduction of a temporally varying auxiliary variable technique to enable efficient inference by relaxing the hard non-negativity constraint on states.,1.2. Contributions,[0],[0]
"Introduction of the auxiliary variable not only allows for efficient inference with respect to filtering the latent state, it also allows for collapsed Gibbs sampling for module assignments and for the structural network learning component.
",1.2. Contributions,[0],[0]
The remainder of this paper is organized as follows.,1.2. Contributions,[0],[0]
"In Sec-
tion 2 we present the complete model.",1.2. Contributions,[0],[0]
Section 3 describes our inference algorithm.,1.2. Contributions,[0],[0]
Section 4 contains experimental validation on simulated and real data.,1.2. Contributions,[0],[0]
Section 5 contains our concluding remarks.,1.2. Contributions,[0],[0]
"Before moving on, a quick comment regarding notation: random variable are written in bold as α,β,γ, a,b, c with regular parameters denoted as α, β, γ, a, b, c.",1.2. Contributions,[0],[0]
"Our model of dynamics is based on a stochastic version of the gLV equations, widely used in ecological system modeling:
dxt,i = xt,i ( ai,1 + ai,2xt,i + ∑ j 6=i bijxt,j )",2.1. Model of dynamics,[0],[0]
"dt+ dwt,i
i ∈",2.1. Model of dynamics,[0],[0]
"{1, 2, . . .",2.1. Model of dynamics,[0],[0]
", n} where xt,i ∈ R≥0 is the abundance of microbial species i at time t ∈ R,",2.1. Model of dynamics,[0],[0]
"ai,1 ∈ R is the growth rate of microbial species i and ai,2 is the “self interaction term” and",2.1. Model of dynamics,[0],[0]
"together ai,1 and ai,2 determine the carrying capacity of the environment when species i is not interacting with any other species.",2.1. Model of dynamics,[0],[0]
The coefficients bij,2.1. Model of dynamics,[0],[0]
when i 6= j are then the microbial interaction terms.,2.1. Model of dynamics,[0],[0]
"The term wt,i ∈ R represents a stochastic disturbance.",2.1. Model of dynamics,[0],[0]
"Note that, while not shown explicitly, the disturbance must be conditioned on the state to prevent negative state values.",2.1. Model of dynamics,[0],[0]
"Overloading the first subscript in x, a discrete-time approximation to the gLV dynamics above is:
x(k+1),i−xk,i ≈ xk,i ( ai,1+ai,2xk,i+ ∑ j 6=i bijxk,j ) ∆k
+ √ ∆k(wk+1,i −wk,i) (1)
",2.1. Model of dynamics,[0],[0]
where k ∈,2.1. Model of dynamics,[0],[0]
"N>0 indexes time as tk and ∆k , tk+1 − tk.
",2.1. Model of dynamics,[0],[0]
The accuracy of this approximation will depend on a sufficiently dense discretization relative to time-scales of the dynamics of interest.,2.1. Model of dynamics,[0],[0]
"Higher order integration methods are possible for Stochastic Differential Equations (SDE), but quickly become very complicated without straightforward gains in accuracy seen with ODEs.",2.1. Model of dynamics,[0],[0]
"Our experience has been that Euler methods behave well for the gLV model in real microbial ecosystems, which are inherently stable.",2.1. Model of dynamics,[0],[0]
"However, Euler integration may be sub-optimal for strongly perturbed systems (e.g., antibiotics).",2.1. Model of dynamics,[0],[0]
"We note that Euler integration is indeed an advance over the state-of-the-art, which uses gradient-matching methods that don’t perform any integration.",2.1. Model of dynamics,[0],[0]
"An interesting area for future work would be to leverage Bayesian Probabilistic Numerical Methods (Cockayne et al., 2017) to incorporate step-size adaptation directly into our model.",2.1. Model of dynamics,[0],[0]
"We incorporate a Dirichlet Process (DP)-based clustering technique (Neal, 2000; Rasmussen, 2000) to learn redundant
interaction structures among bacterial species, which we term interaction modules.",2.2. Interaction modules,[0],[0]
"In the context of our dynamical systems model, this means that only interaction coefficients between modules need to be learned, rather than interactions between each pair of microbes.",2.2. Interaction modules,[0],[0]
"Without modules, the number of possible interaction coefficients scales as O(n2), where n is the number of microbial species.",2.2. Interaction modules,[0],[0]
"Since we are using DPs, where the expected number of clusters is O(log n)",2.2. Interaction modules,[0],[0]
"(Antoniak, 1974), the expected number of interaction coefficients is O((log n)2).",2.2. Interaction modules,[0],[0]
"For purposes of interpretability, we specifically assume no interactions within each module, corresponding to the biologically important scenario of redundant functionality among sets of microbes.",2.2. Interaction modules,[0],[0]
"An example of interaction module structure is visualized in Figure 1C: while Figures 1B and 1C both contain 10 microbes, there are only 6 interactions to learn in 1C (between modules), versus 90 microbe-microbe interactions in 1B without the module structure.
",2.2. Interaction modules,[0],[0]
Figure 2 depicts our interaction module model as a generative model.,2.2. Interaction modules,[0],[0]
"Starting with the Dirichlet Process, ci ∈ Z+ represents the cluster assignment for bacterial species i.",2.2. Interaction modules,[0],[0]
"If species i and species j are in different clusters, and thus ci 6= cj , then bci,cj ∈ R is the coefficient representing the (interaction) effect that the module containing species j has on species i.",2.2. Interaction modules,[0],[0]
"If species `, different from species i, is in the same cluster as species j, then bci,cj = bci,c` by definition (i.e., species in the same cluster share interaction coefficients).",2.2. Interaction modules,[0],[0]
"Note that no interactions are assumed to occur within a cluster, as discussed.
",2.2. Interaction modules,[0],[0]
"For each element in b there is a corresponding element in z, which is an indicator variable (0 or 1) that chooses whether an interaction exists between two modules.",2.2. Interaction modules,[0],[0]
"Thus, our model automatically adapts the interaction network by structurally adding or removing edges (analogous to approaches for standard Bayesian Networks e.g., (George & McCulloch, 1993; Heckerman, 2008)), which we refer to as Edge Se-
lection (ES).",2.2. Interaction modules,[0],[0]
"This approach allows us to easily compute Bayes factors (Kass & Raftery, 1995), enabling principled determination of the evidence for or against each interaction occurring.
",2.2. Interaction modules,[0],[0]
"The terms ai,1 and ai,2 correspond to the growth rate and self interaction term for species i, respectively.",2.2. Interaction modules,[0],[0]
Note that these variables are not part of our clustering scheme and do not have indicator variables associated with them.,2.2. Interaction modules,[0],[0]
"We now discuss one of our technical contributions, which is to relax the strict non-negativity assumption on x in Equation (1) and thereby enable efficient inference while maintaining (approximate) physically realistic non-negative dynamics.",2.3. Modeling non-negative dynamics,[0],[0]
"To accomplish this, we introduce an auxiliary trajectory variable q such that qk,i ∼ Uniform[0, L), with L > 0 and much larger than any of the measured values.",2.3. Modeling non-negative dynamics,[0],[0]
Microbial abundance data y are assumed to be generated from q through some model of measurement noise y,2.3. Modeling non-negative dynamics,[0],[0]
"| q (discussed below).
",2.3. Modeling non-negative dynamics,[0],[0]
"We couple the latent trajectory x to the auxiliary variable q through a conditional distribution q | x, which we assume to be Gaussian with small variance.",2.3. Modeling non-negative dynamics,[0],[0]
"This effectively introduces a momentum term into the model of dynamics (1) (proportional to the difference between x and q), which softly constrains x to be in the range",2.3. Modeling non-negative dynamics,[0],[0]
"[0, L).",2.3. Modeling non-negative dynamics,[0],[0]
"This renders the posterior distributions for x and gLV parameters a,b Gaussians rather than their being truncated Gaussians if strict non-negativity were imposed.",2.3. Modeling non-negative dynamics,[0],[0]
"Our technique has connections to several approaches that break or relax dependencies in a model to improve inference efficiency, such as Variational Inference (Blei et al., 2017) and distributed/parallel Bayesian inference approaches (Angelino et al., 2016).
",2.3. Modeling non-negative dynamics,[0],[0]
"Our approach can also be thought of as a product of experts: one expert is a uniform distribution confining q to
the positive orthant, and the other is a normal distribution enforcing closeness to the actual trajectory x. With either interpretation, q acts as a “restoring force” that pulls the posterior of x toward the positive orthant.",2.3. Modeling non-negative dynamics,[0],[0]
"With the introduction of q, the posterior a | x is now simply a multivariate Gaussian.",2.3. Modeling non-negative dynamics,[0],[0]
"Practically, this makes efficient inference feasible, since sampling from the posterior is now easy and we can also perform closed-form marginalizations.",2.3. Modeling non-negative dynamics,[0],[0]
"Further, the measurement model is decoupled from the dynamics, allowing for efficient inference with flexible measurement noise models, such as negative binomial distributions for modeling sequencing counts (Paulson et al., 2013; Love et al., 2014).",2.3. Modeling non-negative dynamics,[0],[0]
This is explored in detail in the subsequent subsection.,2.3. Modeling non-negative dynamics,[0],[0]
"In the Appendix, we provide a detailed analysis of the issues that ensue with a naive model that directly enforces non-negativity through the dynamics.",2.3. Modeling non-negative dynamics,[0],[0]
"Our measurement model handles two experimental technologies, sequencing counts of a marker gene (16S rRNA) mapped back to different microbial species or other taxonomic units, and qPCR measurements to determine total microbial concentration in the sample.",2.4. Measurement Model,[0],[0]
"The variable yk,i denotes the number of counts (sequencing reads) associated with bacterial species i at time k and Qk is the total bacterial concentration at time",2.4. Measurement Model,[0],[0]
k.,2.4. Measurement Model,[0],[0]
Our complete sensor model combining the two measurements is illustrated in Figure 2.,2.4. Measurement Model,[0],[0]
"The counts measurements yk,i are sampled from a Negative Binomial distribution with mean and dispersion parameters defined as:
yk,i | qk ∼ NegBin(φ(qk, rk), (qk, a0, a1))
φ(qk, rk) =",2.4. Measurement Model,[0],[0]
"rk qk,i∑ i qk,i
(2)
(qk, a0, a1) = a0 qk,i/ ∑ i qk,i + a1 (3)
where rk is the total number of sequencing reads for the sample at time k (often referred to as the read depth of the sample).",2.4. Measurement Model,[0],[0]
"The form of this model follows that of (Bucci et al., 2016; Love et al., 2014); see these references for detailed discussions on the validity of, and the empirical evidence for, using this error model for next generation sequencing
counts data.
",2.4. Measurement Model,[0],[0]
"The Negative Binomial dispersion scaling parameters a0, a1 are pre-trained on raw reads, and are not learned jointly with the rest of the model.",2.4. Measurement Model,[0],[0]
"Similarly, measurement variance, σ2Qk is estimated directly from technical replicates for each measurement.",2.4. Measurement Model,[0],[0]
"For completeness, we also give our specific parameterization of the Negative Binomial Probability Density Function (PDF):
NegBin(y;φ, ) = Γ(r + y)
y! Γ(r)
( φ
r + φ
)y ( r
r + φ )",2.4. Measurement Model,[0],[0]
"r r = 1
With this parameterization of the Negative Binomial distribution, the mean is φ and the variance is φ+ φ2.",2.4. Measurement Model,[0],[0]
"To complete the model description, we describe higher-level priors not shown in Figure 2.",2.5. Additional priors not specified in Figure 2,[0],[0]
"For the three variance random variables (σ2a ,σ 2 b ,σ 2 w) Inverse-Chi-squared priors are used.",2.5. Additional priors not specified in Figure 2,[0],[0]
The concentration parameter α for the DP is given a Gamma prior.,2.5. Additional priors not specified in Figure 2,[0],[0]
"Hyperparameters were set using a technique similar to (Bucci et al., 2016), where means of distributions were empirically calibrated based on the data and variances were set to large values to produce diffuse priors.",2.5. Additional priors not specified in Figure 2,[0],[0]
"We briefly describe our Markov Chain Monte Carlo inference algorithm, which leverages efficient collapsed Gibbs sampling steps.",3. Inference,[0],[0]
"As described in Section 2.5, we use conjugate priors on many variables (e.g., the variance terms (σ2a ,σ 2 b ,σ 2 w)), which allows straight-forward Gibbs sampling.",3. Inference,[0],[0]
"The module assignments, c, are also updated by a standard Gibbs sampling approach for Dirichlet Processes (Neal, 2000).",3. Inference,[0],[0]
"For the concentration parameter α, which has a Gamma prior on α, we use the sampling method described by (Escobar & West, 1995).
",3. Inference,[0],[0]
"Our auxiliary trajectory variables q allow us to marginalize out in closed form the interaction coefficients b, and thus perform collapsed Gibbs sampling, both during sampling assignments of species to modules and when structurally learning the network of interactions between modules.",3. Inference,[0],[0]
"Collapsed Gibbs steps have been shown to improve mixing substantially for DP inference (Neal, 2000).
",3. Inference,[0],[0]
Sampling of the auxiliary variables q and latent trajectories x require Metropolis-Hastings (MH) steps.,3. Inference,[0],[0]
"Briefly, for q, the MH proposal is based on a Generalized-Linear Model approximation.",3. Inference,[0],[0]
"For x, we use a one time-step ahead proposal similar to that described in (Geweke & Tanizaki, 2001).",3. Inference,[0],[0]
"Our proposal uses the previous time point latent abundance, the gLV coefficients, and the auxiliary trajectory (which is di-
rectly coupled to the observations) to propose the next time point abundance giving the proposal the form pxk+1|xk,q,Ω, where Ω = ai,b, z, c,σw.",3. Inference,[0],[0]
"Thus, our proposal is essentially the forward pass of a Kalman filter (which we color coded in Figure 3).",3. Inference,[0],[0]
"Our proposal uses the information from the blue nodes, to propose for the green node.",3. Inference,[0],[0]
"The future state information (orange node) is not used for the proposal, for efficiency of computation (i.e., we exploit conjugacy for the forward pass).",3. Inference,[0],[0]
"The future state information comes into the target distribution, so we sample from the true posterior.",3. Inference,[0],[0]
"Note that this is different from a standard Extended Kalman Filtering approach, which linearizes around estimated mean
and covariance and can deviate substantially from the true posterior.",3. Inference,[0],[0]
In this section we present results applying our model to both simulated and real microbiome data.,4. Results,[0],[0]
"Our goal with simulated data is to illustrate the utility of our model (and specifically Module Learning) when inferring microbial dynamics from time series data with limited biological replicates and temporal resolution, which is the reality for in vivo microbiome experiments.",4. Results,[0],[0]
"Figures 4A-4C depict our
results, comparing inference both with and without interaction module learning.",4. Results,[0],[0]
"Simulated data was constructed to mimic state-of-the-art experiments for developing and testing bacteriotherapies (Bucci et al., 2016).",4. Results,[0],[0]
"In these experiments, germ-free mice (animals raised in self-contained bacteria-free environments) were inoculated with defined collections of 13 bacterial species and serial fecal samples were collected to analyze dynamics of microbial colonization over time.",4. Results,[0],[0]
"Due to costs and logistic constraints, such experiments use relatively small numbers of biological replicates (≈ 5 mice) and limited temporal sampling (e.g., 10-30 time-points per mouse).",4. Results,[0],[0]
"To simulate these experiments, we generated data with 5 biological replicates (5 different time series simulated from the same dynamics, but with different initial conditions), 11 time-points per replicate, and assumed gLV dynamics with the following module and interaction structure:
1, 5, 7 9, 11
2, 4, 6, 8 10, 12
3, 13
2
−4
3
−1 (4)
where the numbers inside the nodes represent bacterial species in the same module and the edge weights are the module interaction coefficients bci,cj in our model in Figure 2.",4. Results,[0],[0]
"Note that this graph in (4) is just another representation of the weighted adjacency matrix in Figure 4C.
With module learning (Figure 4A), our algorithm recovers the module structure as expected, almost completely correctly, and also recovers the interaction coefficients well.",4. Results,[0],[0]
"While the algorithm incorrectly places species 6 in its own cluster, it properly learns that no other species contribute to the dynamics of species 6 (i.e. elements in the row associated with species 6, other than the self interaction term, are zero).",4. Results,[0],[0]
Our algorithm also forecasts trajectories of microbial abundances quite accurately.,4. Results,[0],[0]
"Without module learning enabled (Figure 4B), the algorithm still forecasts trajectories fairly accurately (although slightly worse than with module learning), but does much worse in inferring the interaction coefficients, and indeed the actual structure of the dynamical system is not at all evident.",4. Results,[0],[0]
"The ability to forecast trajectories relatively accurately, but not recover the underlying structure of the system well, highlights the issues with identifiability of nonlinear dynamical systems models from limited data: without additional structural constraints in the model, it is too easy to overfit, because many different settings of ODE parameters can result in exactly the same trajectories.
",4. Results,[0],[0]
"To investigate this issue further, we performed additional simulations using the same setup with varying numbers of biological replicates (Figure 4D).",4. Results,[0],[0]
"Results using 20 initial
conditions were run and aggregate statistics are presented.",4. Results,[0],[0]
"For forecasting trajectories, module learning clearly helps, although performance is relatively good without module learning with 4 or more biological replicates.",4. Results,[0],[0]
"However, as can be seen, for identification of the actual ODE parameters, module learning has a much larger advantage.
",4. Results,[0],[0]
"It is worth noting that module learning also resulted in significant improvements in wall-clock runtime, by a factor of about 10.",4. Results,[0],[0]
"We did not test this empirical observation extensively, but it is consistent with theory, in that the additional time to learn module structure with our inference algorithm is (in expectation) nO(log n), whereas the time to learn interaction coefficients is reduced from O(n2) to O((log n)2).
",4. Results,[0],[0]
"We next applied our algorithm to real data from (Bucci et al., 2016), which investigated developing a bacteriotherapy for Clostridium difficile, a pathogenic bacteria that causes serious diarrhea and is the most common cause of hospital acquired infection in the U.S. Five germ-free mice were colonized with a collection of 13 commensal (beneficial) bacterial species, termed the GnotoComplex microbiota, and monitored for 28 days (Figure 5A).",4. Results,[0],[0]
"Then, mice were infected with Clostridium difficile and monitored for another 28 days.",4. Results,[0],[0]
"All mice developed diarrhea, but recovered within about a week, indicating that some combination of the 13 bacterial species protect against the pathogen (in a germ-free mouse, the infection causes death in 24-48 hours).",4. Results,[0],[0]
"Over the course of the experiment, 26 serial fecal samples per mouse were collected and interrogated via sequencing and qPCR to determine concentrations of the commensal microbes and the pathogen.",4. Results,[0],[0]
"We removed one species from our analysis, Clostridium hiranonis, because it appeared to inconsistently colonize the mice, but otherwise used all data from the original study.
",4. Results,[0],[0]
"Figure 5 shows the results of applying our model to the data from (Bucci et al., 2016).",4. Results,[0],[0]
"Our model found a median of 4 interaction modules (5,000 MCMC samples with 1,000 burnin).",4. Results,[0],[0]
"Seven microbes formed a large and consistent module, with the remaining six microbes aggregating into smaller modules.",4. Results,[0],[0]
Figure 5B shows the module structure of a representative sample from the posterior.,4. Results,[0],[0]
"The module structure identifies groups of microbes that putatively inhibit the pathogen, and does so more clearly than in the original study, which presented a dense network of microbial interactions.",4. Results,[0],[0]
"The fine structure of this dense network is indeed still recapitulated in the posterior summary of interaction coefficients (Figure 5C), but our model also has the advantage of providing a compact module structure that is much easier to interpret biologically.",4. Results,[0],[0]
"Interestingly, the strongest interaction identified by our model (which the analysis from the original study detected relatively weakly), with Clostridium scindens inhibiting the pathogen, is in fact the only
Day 1 Day 28 Day 56
C. difficileGnotoComplex",4. Results,[0],[0]
"A
13 samples 13 samples
C. scindens B. ovatusP. distasonis
A. muciniphila R. hominis C. difficile + rest
B
−2.1 −0.13
−0.05
1 2 3 4 5 6 7 8 9 10 11 12 13
Microbe Co-cluster Proportions
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Klebsiella oxytoca 13 Ruminococcus obeum 12
Bacteroides vulgatus 11 Bacteroides fragilis 10
Escherichia coli 9 Proteus mirabilis 8
Clostridium ramosum 7 Clostridium difficile 6 Roseburia hominis 5
Akkermansia muciniphila 4 Parabacteroides distasonis 3
Bacteroides ovatus 2 Clostridium scindens 1
1 2 3 4 5 6 7 8 9 10 11 12 13
Microbe Interaction Strength
-14
-13
-12
-11
-10
-9
-8
Klebsiella oxytoca 13 Ruminococcus obeum 12
Bacteroides vulgatus 11 Bacteroides fragilis 10
Escherichia coli 9 Proteus mirabilis 8
Clostridium ramosum 7 Clostridium difficile 6 Roseburia hominis 5
Akkermansia muciniphila 4 Parabacteroides distasonis 3
Bacteroides ovatus 2 Clostridium scindens 1 -
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - + - - - - - - - - - - - + + +",4. Results,[0],[0]
+ + + - + + + + + + + + + +,4. Results,[0],[0]
"+ + - - - - + + - + + + + + + + + - + + + + + + + + + + + + + - + + + + + + + + + + + + + - + + - - - - - - - - - - - - - - + + + + + + + + + + + -
gr am
/(C",4. Results,[0],[0]
"FU
da y)
log10",4. Results,[0],[0]
"C
Figure 5.",4. Results,[0],[0]
"Inference applied to in vivo experiments from (Bucci et al., 2016), illustrating the ability of interaction module learning to produce interpretable interaction structures that agree with biologically validated and plausible interactions.",4. Results,[0],[0]
(A) Experimental timeline (performed with 5 germ-free mice).,4. Results,[0],[0]
"GnotoComplex microbes, a defined collection of beneficial gut bacteria, is introduced on day one with Clostridium difficile introduced on day 28.",4. Results,[0],[0]
(B) Module structure of a representative sample from the posterior with interaction strengths shown (interaction scale is 10−9).,4. Results,[0],[0]
"(C) Co-cluster proportions illustrating the probability that two microbes appear in the same module and expected values for interaction coefficients, log10 scale with interaction signs illustrated.
biologically validated result in their study.",4. Results,[0],[0]
"Our analysis also discovered additional putative inhibitors of the pathogen, including the commensal Akkermansia munciniphila.",4. Results,[0],[0]
"This microbe lives in the mucous layer in the gut, and has been associated positively with mucosal integrity in several studies (see e.g., (Belzer et al., 2017)), and thus suggests an interesting and biologically plausible candidate for inclusion in a bacteriotherapy against the pathogen.",4. Results,[0],[0]
We have presented a Bayesian nonparametric model and associated inference algorithm for tackling key challenges in analyzing dynamics of the microbiome.,5. Conclusions,[0],[0]
"Our method introduces several innovations, including a new type of modular dynamical systems model, uncertainty propagation throughout the model, and an efficient technique for approximating physically realistic non-negative dynamics.",5. Conclusions,[0],[0]
Applications of our method to simulated data show the ability to accurately identify the underlying dynamical system even with limited data.,5. Conclusions,[0],[0]
"Application to real data highlights the ability of our model to infer compact, biologically interpretable representations that correctly find known relationships and suggest new, biologically plausible relationships.
",5. Conclusions,[0],[0]
There are several areas for future work.,5. Conclusions,[0],[0]
"Other Bayesian clustering approaches, which are more flexible than DPs, such as mixtures of finite mixtures (Miller & Harrison, 2017), would be interesting to investigate as alternate priors for interaction modules.",5. Conclusions,[0],[0]
"The gLV dynamical systems model has been widely used in microbial ecology, but has limitations
including modeling only pairwise interactions and quadratic nonlinearities.",5. Conclusions,[0],[0]
"Our inference method is quite flexible, and could readily accommodate other dynamical systems models, although nonlinearities in coefficients would cause difficulties (gLV is linear in the coefficients) in efficiency with our current algorithm.",5. Conclusions,[0],[0]
"Another interesting avenue is using other forms of approximate inference to accelerate our algorithm, including approximate parallel MCMC and Variational Bayesian techniques.",5. Conclusions,[0],[0]
"Incorporating prior biological knowledge, such as phylogenetic relationships between microbes, is another interesting area to investigate; because our model is fully Bayesian, incorporating prior knowledge is conceptually straight forward.",5. Conclusions,[0],[0]
"Designing in vivo experiments with sufficient richness to identify dynamical systems is a very important topic, and applying our model within a formal experimental design framework would thus be very interesting.",5. Conclusions,[0],[0]
"On the application side, we plan to apply our model to additional bacteriotherapy design problems, which is an active and growing area of research.",5. Conclusions,[0],[0]
"In this regard, our goal is to apply our model to upcoming human microbiome bacteriotherapy trials, which will measure the abundances of hundreds of gut commensal bacterial species per person.",5. Conclusions,[0],[0]
We thank the reviewers for their many helpful comments and suggestions.,Acknowledgements,[0],[0]
They greatly improved the final paper.,Acknowledgements,[0],[0]
"This work was supported by NIH 5T32HL007627-33, DARPA BRICS HR0011-15-C-0094 and the BWH Precision Medicine Initiative.",Acknowledgements,[0],[0]
"Microbes are everywhere, including in and on our bodies, and have been shown to play key roles in a variety of prevalent human diseases.",abstractText,[0],[0]
"Consequently, there has been intense interest in the design of bacteriotherapies or “bugs as drugs,” which are communities of bacteria administered to patients for specific therapeutic applications.",abstractText,[0],[0]
Central to the design of such therapeutics is an understanding of the causal microbial interaction network and the population dynamics of the organisms.,abstractText,[0],[0]
In this work we present a Bayesian nonparametric model and associated efficient inference algorithm that addresses the key conceptual and practical challenges of learning microbial dynamics from time series microbe abundance data.,abstractText,[0],[0]
"These challenges include high-dimensional (300+ strains of bacteria in the gut) but temporally sparse and non-uniformly sampled data; high measurement noise; and, nonlinear and physically nonnegative dynamics.",abstractText,[0],[0]
"Our contributions include a new type of dynamical systems model for microbial dynamics based on what we term interaction modules, or learned clusters of latent variables with redundant interaction structure (reducing the expected number of interaction coefficients from O(n) to O((log n))); a fully Bayesian formulation of the stochastic dynamical systems model that propagates measurement and latent state uncertainty throughout the model; and introduction of a temporally varying auxiliary variable technique to enable efficient inference by relaxing the hard non-negativity constraint on states.",abstractText,[0],[0]
"We apply our method to simulated and real data, and demonstrate the utility of our technique for system identification from limited data, and for gaining new biological insights into bacteriotherapy design.",abstractText,[0],[0]
"Massachusetts Host-Microbiome Center, Brigham and Women’s Hospital, Harvard Medical School, Boston, MA, USA.",abstractText,[0],[0]
"Correspondence to: TE Gibson <tgibson@mit.edu>, GK Gerber <ggerber@bwh.harvard.edu>.",abstractText,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",abstractText,[0],[0]
Copyright 2018 by the author(s).,abstractText,[0],[0]
Robust and Scalable Models of Microbiome Dynamics,title,[0],[0]
"The optimal allocation of resources for maximizing influence, spread of information or coverage, has gained attention in the past few years, in particular in machine learning and data mining (Domingos & Richardson, 2001; Kempe et al., 2003; Chen et al., 2009; Gomez Rodriguez & Schölkopf, 2012; Borgs et al., 2014).
",1. Introduction,[0],[0]
"In the Budget Allocation Problem, one is given a bipartite influence graph between channels S and people T , and the task is to assign a budget y(s) to each channel s in S with the goal of maximizing the expected number of influenced people I(y).",1. Introduction,[0],[0]
"Each edge (s, t) 2 E between channel s and
1Massachusetts Institute of Technology.",1. Introduction,[0],[0]
"Correspondence to: Matthew Staib <mstaib@mit.edu>, Stefanie Jegelka <stefje@mit.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"person t is weighted with a probability pst that, e.g., an advertisement on radio station s will influence person t to buy some product.",1. Introduction,[0],[0]
The budget y(s) controls how many independent attempts are made via the channel s to influence the people in T .,1. Introduction,[0],[0]
"The probability that a customer t is influenced when the advertising budget is y is
It(y) = 1 Y (s,t)2E [1 pst]y(s), (1)
and hence the expected number of influenced people is I(y) = P
t2T It(y).",1. Introduction,[0],[0]
We write I(y; p) = I(y) to make the dependence on the probabilities pst explicit.,1. Introduction,[0],[0]
"The total budget y must remain within some feasible set Y which may encode e.g. a total budget limit P
s2S y(s)  ",1. Introduction,[0],[0]
"C. We allow the budgets y to be continuous, as in (Bian et al., 2017).
",1. Introduction,[0],[0]
"Since its introduction by Alon et al. (2012), several works have extended the formulation of Budget Allocation and provided algorithms (Bian et al., 2017; Hatano et al., 2015; Maehara et al., 2015; Soma et al., 2014; Soma & Yoshida, 2015).",1. Introduction,[0],[0]
"Budget Allocation may also be viewed as influence maximization on a bipartite graph, where information spreads as in the Independent Cascade model.",1. Introduction,[0],[0]
"For integer y, Budget Allocation and Influence Maximization are NPhard.",1. Introduction,[0],[0]
"Yet, constant-factor approximations are possible, and build on the fact that the influence function is submodular in the binary case, and DR-submodular in the integer case (Soma et al., 2014; Hatano et al., 2015).",1. Introduction,[0],[0]
"If y is continuous, the problem is a concave maximization problem.
",1. Introduction,[0],[0]
The formulation of Budget Allocation assumes that the transmission probabilities are known exactly.,1. Introduction,[0],[0]
But this is rarely true in practice.,1. Introduction,[0],[0]
"Typically, the probabilities pst, and possibly the graph itself, must be inferred from observations (Gomez Rodriguez et al., 2010; Du et al., 2013; Narasimhan et al., 2015; Du et al., 2014; Netrapalli & Sanghavi, 2012).",1. Introduction,[0],[0]
In Section 4 we will see that a misspecification or point estimate of parameters pst can lead to much reduced outcomes.,1. Introduction,[0],[0]
A more realistic assumption is to know confidence intervals for the pst.,1. Introduction,[0],[0]
"Realizing this severe deficiency, recent work studied robust versions of Influence Maximization, where a budget y must be chosen that maximizes the worst-case approximation ratio over a set of possible influence functions (He & Kempe, 2016; Chen et al., 2016; Lowalekar et al., 2016).",1. Introduction,[0],[0]
"The resulting optimization problem is hard but admits bicriteria approximations.
",1. Introduction,[0],[0]
"In this work, we revisit Budget Allocation under uncertainty from the perspective of robust optimization (Bertsimas et al., 2011; Ben-Tal et al., 2009).",1. Introduction,[0],[0]
"We maximize the worst-case influence – not approximation ratio – for p in a confidence set centered around the “best guess” (e.g., posterior mean).",1. Introduction,[0],[0]
"This avoids pitfalls of the approximation ratio formulation (which can be misled to return poor worst-case budgets, as demonstrated in Appendix A), while also allowing us to formulate the problem as a max-min game:
max y2Y min p2P I(y; p), (2)
where an “adversary” can arbitrarily manipulate p within the confidence set P .",1. Introduction,[0],[0]
"With p fixed, I(y; p) is concave in y.",1. Introduction,[0],[0]
"However, the influence function I(y; p) is not convex, and not even quasiconvex, in the adversary’s variables pst.
",1. Introduction,[0],[0]
"The new, key insight we exploit in this work is that I(y; p) has the property of continuous submodularity in p – in contrast to previously exploited submodular maximization in y – and can hence be minimized by generalizing techniques from discrete submodular optimization (Bach, 2015).",1. Introduction,[0],[0]
"The techniques in (Bach, 2015), however, are restricted to box constraints, and do not directly apply to our confidence sets.",1. Introduction,[0],[0]
"In fact, general constrained submodular minimization is hard (Svitkina & Fleischer, 2011; Goel et al., 2009; Iwata & Nagano, 2009).",1. Introduction,[0],[0]
"We make the following contributions:
1.",1. Introduction,[0],[0]
"We present an algorithm with optimality bounds for Robust Budget Allocation in the nonconvex adversarial scenario (2).
2.",1. Introduction,[0],[0]
"We provide the first results for continuous submodular minimization with box constraints and one more “nice” constraint, and conditions under which the algorithm is guaranteed to return a global optimum.",1. Introduction,[0],[0]
"We begin with some background material and, along the way, discuss related work.",1.1. Background and Related Work,[0],[0]
Submodularity is perhaps best known as a property of set functions.,1.1.1. SUBMODULARITY OVER THE INTEGER LATTICE AND CONTINUOUS DOMAINS,[0],[0]
A function F : 2V !,1.1.1. SUBMODULARITY OVER THE INTEGER LATTICE AND CONTINUOUS DOMAINS,[0],[0]
"R defined on subsets S ✓ V of a ground set V is submodular if for all sets S, T ✓ V , it holds that F (S) +",1.1.1. SUBMODULARITY OVER THE INTEGER LATTICE AND CONTINUOUS DOMAINS,[0],[0]
F (T ) F (S \ T ) + F (S[T ).,1.1.1. SUBMODULARITY OVER THE INTEGER LATTICE AND CONTINUOUS DOMAINS,[0],[0]
"A similar definition extends to functions defined over a distributive lattice L, e.g. the integer lattice.",1.1.1. SUBMODULARITY OVER THE INTEGER LATTICE AND CONTINUOUS DOMAINS,[0],[0]
"Such a function f is submodular if for all x, y 2 L, it holds that
f(x) + f(y) f(x _",1.1.1. SUBMODULARITY OVER THE INTEGER LATTICE AND CONTINUOUS DOMAINS,[0],[0]
y),1.1.1. SUBMODULARITY OVER THE INTEGER LATTICE AND CONTINUOUS DOMAINS,[0],[0]
+ f(x ^ y).,1.1.1. SUBMODULARITY OVER THE INTEGER LATTICE AND CONTINUOUS DOMAINS,[0],[0]
"(3)
For the integer lattice and vectors x, y, x _ y denotes the coordinate-wise maximum and x ^",1.1.1. SUBMODULARITY OVER THE INTEGER LATTICE AND CONTINUOUS DOMAINS,[0],[0]
"y the coordinate-wise
minimum.",1.1.1. SUBMODULARITY OVER THE INTEGER LATTICE AND CONTINUOUS DOMAINS,[0],[0]
"Submodularity has also been considered on continuous domains X ⇢ Rd, where, if f is also twicedifferentiable, the property of submodularity means that all off-diagonal entries of the the Hessian are nonpositive, i.e., @f(x) @xi@xj
 0 for all i 6=",1.1.1. SUBMODULARITY OVER THE INTEGER LATTICE AND CONTINUOUS DOMAINS,[0],[0]
"j (Topkis, 1978, Theorem 3.2).",1.1.1. SUBMODULARITY OVER THE INTEGER LATTICE AND CONTINUOUS DOMAINS,[0],[0]
"These functions may be convex, concave, or neither.
",1.1.1. SUBMODULARITY OVER THE INTEGER LATTICE AND CONTINUOUS DOMAINS,[0],[0]
"Submodular functions on lattices can be minimized by a reduction to set functions, more precisely, ring families (Birkhoff, 1937).",1.1.1. SUBMODULARITY OVER THE INTEGER LATTICE AND CONTINUOUS DOMAINS,[0],[0]
"Combinatorial algorithms for submodular optimization on lattices are discussed in (Khachaturov et al., 2012).",1.1.1. SUBMODULARITY OVER THE INTEGER LATTICE AND CONTINUOUS DOMAINS,[0],[0]
"More recently, Bach (2015) extended results based on the convex Lovász extension, by building on connections to optimal transport.",1.1.1. SUBMODULARITY OVER THE INTEGER LATTICE AND CONTINUOUS DOMAINS,[0],[0]
"The subclass of L\-convex functions admits strongly polynomial time minimization (Murota, 2003; Kolmogorov & Shioura, 2009; Murota & Shioura, 2014), but does not apply in our setting.
",1.1.1. SUBMODULARITY OVER THE INTEGER LATTICE AND CONTINUOUS DOMAINS,[0],[0]
"Similarly, results for submodular maximization extend to integer lattices, e.g. (Gottschalk & Peis, 2015).",1.1.1. SUBMODULARITY OVER THE INTEGER LATTICE AND CONTINUOUS DOMAINS,[0],[0]
Stronger results are possible if the submodular function also satisfies diminishing returns: for all x  y (coordinate-wise) and,1.1.1. SUBMODULARITY OVER THE INTEGER LATTICE AND CONTINUOUS DOMAINS,[0],[0]
"i such that y+ei 2 X , it holds that f(x+ei) f(x) f(y+ei) f(y).",1.1.1. SUBMODULARITY OVER THE INTEGER LATTICE AND CONTINUOUS DOMAINS,[0],[0]
"For such DR-submodular functions, many approximation results for the set function case extend (Bian et al., 2017; Soma & Yoshida, 2015; Soma et al., 2014).",1.1.1. SUBMODULARITY OVER THE INTEGER LATTICE AND CONTINUOUS DOMAINS,[0],[0]
"In particular, Ene & Nguyen (2016) show a generic reduction to set function optimization that they apply to maximization.",1.1.1. SUBMODULARITY OVER THE INTEGER LATTICE AND CONTINUOUS DOMAINS,[0],[0]
"In fact, it also applies to minimization:
Proposition 1.1.",1.1.1. SUBMODULARITY OVER THE INTEGER LATTICE AND CONTINUOUS DOMAINS,[0],[0]
"A DR-submodular function f defined on
Qn i=1[ki] can be minimized in strongly polynomial time O(n4 log4 k · log2(n log k) ·",1.1.1. SUBMODULARITY OVER THE INTEGER LATTICE AND CONTINUOUS DOMAINS,[0],[0]
"EO + n4 log4 k · log O(1) (n log k)), where k = maxi ki and EO is the time complexity of evaluating f .",1.1.1. SUBMODULARITY OVER THE INTEGER LATTICE AND CONTINUOUS DOMAINS,[0],[0]
"Here, [ki] = {0, 1, . . .",1.1.1. SUBMODULARITY OVER THE INTEGER LATTICE AND CONTINUOUS DOMAINS,[0],[0]
", ki 1}.
",1.1.1. SUBMODULARITY OVER THE INTEGER LATTICE AND CONTINUOUS DOMAINS,[0],[0]
"In particular, the time complexity is logarithmic in k. For general lattice submodular functions, this is not possible without further assumptions.",1.1.1. SUBMODULARITY OVER THE INTEGER LATTICE AND CONTINUOUS DOMAINS,[0],[0]
"A sister problem of Budget Allocation is Influence Maximization on general graphs, where a set of seed nodes is selected to start a propagation process.",1.1.2. RELATED PROBLEMS,[0],[0]
"The influence function is still monotone submodular and amenable to the greedy algorithm (Kempe et al., 2003), but it cannot be evaluated explicitly and requires approximation (Chen et al., 2010).",1.1.2. RELATED PROBLEMS,[0],[0]
"Stochastic Coverage (Goemans & Vondrák, 2006) is a version of Set Cover where the covering sets Si ⇢ V are random.",1.1.2. RELATED PROBLEMS,[0],[0]
A variant of Budget Allocation can be written as stochastic coverage with multiplicity.,1.1.2. RELATED PROBLEMS,[0],[0]
"Stochastic Coverage has mainly been studied in the online or adaptive setting, where logarithmic approximation factors can be achieved (Golovin & Krause, 2011; Deshpande et al., 2016; Adamczyk et al., 2016).
",1.1.2. RELATED PROBLEMS,[0],[0]
"Our objective function (2) is a signomial in p, i.e., a linear combination of monomials of the form",1.1.2. RELATED PROBLEMS,[0],[0]
"Q
i x ci",1.1.2. RELATED PROBLEMS,[0],[0]
i .,1.1.2. RELATED PROBLEMS,[0],[0]
"Gen-
",1.1.2. RELATED PROBLEMS,[0],[0]
"eral signomial optimization is NP-hard (Chiang, 2005), but certain subclasses are tractable: posynomials with all nonnegative coefficients can be minimized via Geometric Programming (Boyd et al., 2007), and signomials with a single negative coefficient admit sum of squares-like relaxations (Chandrasekaran & Shah, 2016).",1.1.2. RELATED PROBLEMS,[0],[0]
"Our problem, a constrained posynomial maximization, is not in general a geometric program.",1.1.2. RELATED PROBLEMS,[0],[0]
"Some work addresses this setting via monomial approximation (Pascual & Ben-Israel, 1970; Ecker, 1980), but, to our knowledge, our algorithm is the first that solves this problem to arbitrary accuracy.",1.1.2. RELATED PROBLEMS,[0],[0]
Two prominent strategies of addressing uncertainty in parameters of optimization problems are stochastic and robust optimization.,1.1.3. ROBUST OPTIMIZATION,[0],[0]
"If the distribution of the parameters is known (stochastic optimization), formulations such as value-at-risk (VaR) and conditional value-at-risk (CVaR) (Rockafellar & Uryasev, 2000; 2002) apply.",1.1.3. ROBUST OPTIMIZATION,[0],[0]
"In contrast, robust optimization (Ben-Tal et al., 2009; Bertsimas et al., 2011) assumes that the parameters (of the cost function and constraints) can vary arbitrarily within a known confidence set U , and the aim is to optimize the worst-case setting, i.e.,
min y sup u,A,b2U {g(y;u) s.t.",1.1.3. ROBUST OPTIMIZATION,[0],[0]
Ay  b}.,1.1.3. ROBUST OPTIMIZATION,[0],[0]
"(4)
Here, we will only have uncertainty in the cost function.
",1.1.3. ROBUST OPTIMIZATION,[0],[0]
"In this paper we are principally concerned with robust maximization of the continuous influence function I(y), but mention some results for the discrete case.",1.1.3. ROBUST OPTIMIZATION,[0],[0]
"While there exist results for robust and CVaR optimization of modular (linear) functions (Nikolova, 2010; Bertsimas & Sim, 2003), submodular objectives do not in general admit such optimization (Maehara, 2015), but variants admit approximations (Zhang et al., 2014).",1.1.3. ROBUST OPTIMIZATION,[0],[0]
"The brittleness of submodular optimization under noise has been studied in (Balkanski et al., 2016; 2017; Hassidim & Singer, 2016).
",1.1.3. ROBUST OPTIMIZATION,[0],[0]
"Approximations for robust submodular and influence optimization have been studied in (Krause et al., 2008; He & Kempe, 2016; Chen et al., 2016; Lowalekar et al., 2016), where an adversary can pick among a finite set of objective functions or remove selected elements (Orlin et al., 2016).",1.1.3. ROBUST OPTIMIZATION,[0],[0]
The unknown parameters in Budget Allocation are the transmission probabilities pst or edge weights in a graph.,2. Robust and Stochastic Budget Allocation,[0],[0]
"If these are estimated from data, we may have posterior distributions or, a weaker assumption, confidence sets for the parameters.",2. Robust and Stochastic Budget Allocation,[0],[0]
"For ease of notation, we will work with the failure probabilities",2. Robust and Stochastic Budget Allocation,[0],[0]
xst,2. Robust and Stochastic Budget Allocation,[0],[0]
"= 1 pst instead of the pst
directly, and write I(y;x) instead of I(y; p).",2. Robust and Stochastic Budget Allocation,[0],[0]
"If a (posterior) distribution of the parameters is known, a simple strategy is to use expectations.",2.1. Stochastic Optimization,[0],[0]
"We place a uniform prior on xst, and observe nst independent observations drawn from Ber(xst).",2.1. Stochastic Optimization,[0],[0]
"If we observe ↵st failures and and st successes, the resulting posterior distribution on the variable Xst is Beta(1 + ↵st, 1 + st).",2.1. Stochastic Optimization,[0],[0]
"Given such a posterior, we may optimize
max y2Y I(y;E[X]), or (5)
max y2Y E[I(y;X)].",2.1. Stochastic Optimization,[0],[0]
"(6)
Proposition 2.1.",2.1. Stochastic Optimization,[0],[0]
"Problems (5) and (6) are concave maximization problems over the (convex) set Y and can be solved exactly.
",2.1. Stochastic Optimization,[0],[0]
"Concavity of (6) follows since it is an expectation over concave functions, and the problem can be solved by stochastic gradient ascent or by explicitly computing gradients.
",2.1. Stochastic Optimization,[0],[0]
Merely maximizing expectation does not explicitly account for volatility and hence risk.,2.1. Stochastic Optimization,[0],[0]
"One option is to include variance (Ben-Tal & Nemirovski, 2000; Bertsimas et al., 2011; Atamtürk & Narayanan, 2008):
min y2Y E[I(y;X)]",2.1. Stochastic Optimization,[0],[0]
"+ ""
p
Var(I(y;X)), (7)
but in our case this CVaR formulation seems difficult: Fact 2.1.",2.1. Stochastic Optimization,[0],[0]
"For y in the nonnegative orthant, the term p
Var(I(y;X)) need not be convex or concave, and need not be submodular or supermodular.
",2.1. Stochastic Optimization,[0],[0]
"This observation does not rule out a solution, but the apparent difficulties further motivate a robust formulation that, as we will see, is amenable to optimization.",2.1. Stochastic Optimization,[0],[0]
"The focus of this work is the robust version of Budget Allocation, where we allow an adversary to arbitrarily set the parameters x within an uncertainty set X .",2.2. Robust Optimization,[0],[0]
"This uncertainty set may result, for instance, from a known distribution, or simply assumed bounds.",2.2. Robust Optimization,[0],[0]
"Formally, we solve
max y2Y min x2X I(y;x), (8)
where Y ⇢ RS + is a convex set with an efficient projection oracle, and X is an uncertainty set containing an estimate x̂.",2.2. Robust Optimization,[0],[0]
"In the sequel, we use uncertainty sets X = {x 2 Box(l, u) : R(x)  B}, where R is a distance (or divergence) from the estimate x̂, and Box(l, u) is the box Q
(s,t)2E [lst, ust].",2.2. Robust Optimization,[0],[0]
"The intervals [lst, ust] can be thought of
as either confidence intervals around x̂, or, if [lst, ust] =",2.2. Robust Optimization,[0],[0]
"[0, 1], enforce that each xst is a valid probability.
",2.2. Robust Optimization,[0],[0]
"Common examples of uncertainty sets used in Robust Optimization are Ellipsoidal and D-norm uncertainty sets (Bertsimas et al., 2011).",2.2. Robust Optimization,[0],[0]
"Our algorithm in Section 3.1 applies to both.
",2.2. Robust Optimization,[0],[0]
Ellipsoidal uncertainty.,2.2. Robust Optimization,[0],[0]
"The ellipsoidal or quadratic uncertainty set is defined by
XQ( ) = {x 2 Box(0, 1) : (x x̂)T⌃ 1(x x̂)  },
where ⌃ is the covariance of the random vector X of probabilities distributed according to our Beta posteriors.",2.2. Robust Optimization,[0],[0]
"In our case, since the distributions on each xst are independent, ⌃ 1 is actually diagonal.",2.2. Robust Optimization,[0],[0]
"Writing ⌃ = diag( 2), we have
XQ( ) = n x 2 Box(0, 1) : X
(s,t)2E
Rst(xst)  o ,
where Rst(x) =",2.2. Robust Optimization,[0],[0]
"(xst x̂st)2 2st .
",2.2. Robust Optimization,[0],[0]
D-norm uncertainty.,2.2. Robust Optimization,[0],[0]
"The D-norm uncertainty set is similar to an `
1
-ball around x̂, and is defined as
XD( ) = n x",2.2. Robust Optimization,[0],[0]
": 9c 2 Box(0, 1) s.t.
xst = x̂st +",2.2. Robust Optimization,[0],[0]
"(ust x̂st)cst, X
(s,t)2E
cst  o .
",2.2. Robust Optimization,[0],[0]
"Essentially, we allow an adversary to increase x̂st up to some upper bound ust, subject to some total budget across all terms xst.",2.2. Robust Optimization,[0],[0]
"The set XD( ) can be rewritten as
XD( ) = n x 2 Box(x̂, u) : X
(s,t)2E
Rst(xst)  o ,
where Rst(xst) =",2.2. Robust Optimization,[0],[0]
(xst x̂st)/(ust x̂st) is the fraction of the interval,2.2. Robust Optimization,[0],[0]
"[x̂st, ust] we have used up in increasing xst.
",2.2. Robust Optimization,[0],[0]
The min-max formulation maxy2Y minx2X,2.2. Robust Optimization,[0],[0]
I(y;x) has several merits: the model is not tied to a specific learning algorithm for the probabilities x as long as we can choose a suitable confidence set.,2.2. Robust Optimization,[0],[0]
"Moreover, this formulation allows to fully hedge against a worst-case scenario.",2.2. Robust Optimization,[0],[0]
"As noted above, the function I(y;x) is concave as a function of y for fixed x. As a pointwise minimum of concave functions, F (y) := minx2X I(y;x) is concave.",3. Optimization Algorithm,[0],[0]
"Hence, if we can compute subgradients of F (y), we can solve our max-min-problem via the subgradient method, as outlined in Algorithm 1.
",3. Optimization Algorithm,[0],[0]
A subgradient gy 2,3. Optimization Algorithm,[0],[0]
"@F (y) at y is given by the gradient of I(y;x⇤) for the minimizing x⇤ 2 argminx2X I(y;x), i.e.,
Algorithm 1 Subgradient Ascent Input: suboptimality tolerance "" > 0, initial feasible budget y(0) 2 Y Output: ""-optimal budget y for Problem (8) repeat x(k) argminx2X I(y(k);x) g(k) ryI(y(k);x(k))",3. Optimization Algorithm,[0],[0]
L(k) I(y(k);x(k)),3. Optimization Algorithm,[0],[0]
U (k) maxy2Y I(y;x(k)),3. Optimization Algorithm,[0],[0]
"(k) (U (k) L(k))/kg(k)k2
2
y(k+1) projY(y(k)",3. Optimization Algorithm,[0],[0]
+ (k)g(k)),3. Optimization Algorithm,[0],[0]
"k k + 1
until U (k) L(k)  ""
gy = ryI(y;x⇤).",3. Optimization Algorithm,[0],[0]
"Hence, we must be able to compute x⇤ for any y.",3. Optimization Algorithm,[0],[0]
"We also obtain a duality gap: for any x0, y0 we have
min",3. Optimization Algorithm,[0],[0]
x2X I(y0;x)  max y2Y min x2X I(y;x)  max y2Y I(y;x0).,3. Optimization Algorithm,[0],[0]
"(9)
",3. Optimization Algorithm,[0],[0]
"This means we can estimate the optimal value I⇤ and use it in Polyak’s stepsize rule for the subgradient method (Polyak, 1987).
",3. Optimization Algorithm,[0],[0]
"But I(y;x) is not convex in x, and not even quasiconvex.",3. Optimization Algorithm,[0],[0]
"For example, standard methods (Wainwright & Chiang, 2004, Chapter 12) imply that f(x
1 , x 2 , x 3
) = 1 x 1 x 2 px 3 is not quasiconvex on R3 +
.",3. Optimization Algorithm,[0],[0]
"Moreover, the above-mentioned signomial optimization techniques do not apply for an exact solution either.",3. Optimization Algorithm,[0],[0]
"So, it is not immediately clear that we can solve the inner optimization problem.
",3. Optimization Algorithm,[0],[0]
"The key insight we will be using is that I(y;x) has a different beneficial property: while not convex, I(y;x) as a function of x is continuous submodular.",3. Optimization Algorithm,[0],[0]
Lemma 3.1.,3. Optimization Algorithm,[0],[0]
"Suppose we have n 1 differentiable functions fi : R! R+, for i = 1, . . .",3. Optimization Algorithm,[0],[0]
", n, either all nonincreasing or all nondecreasing.",3. Optimization Algorithm,[0],[0]
"Then, f(x) =
Qn i=1 fi(xi) is a
continuous supermodular function from Rn to R + .
",3. Optimization Algorithm,[0],[0]
Proof.,3. Optimization Algorithm,[0],[0]
"For n = 1, the resulting function is modular and therefore supermodular.",3. Optimization Algorithm,[0],[0]
"In the case n 2, we simply need to compute derivatives.",3. Optimization Algorithm,[0],[0]
"The mixed derivatives are
@f
@xi@xj = f 0i(xi)f 0 j(xj) ·
Y
k 6=i,j fk(xk).",3. Optimization Algorithm,[0],[0]
"(10)
By monotonicity, f 0i and f 0j have the same sign, so their product is nonnegative, and since each fk is nonnegative, the entire expression is nonnegative.",3. Optimization Algorithm,[0],[0]
"Hence, f(x) is continuous supermodular by Theorem 3.2 of (Topkis, 1978).
",3. Optimization Algorithm,[0],[0]
Corollary 3.1.,3. Optimization Algorithm,[0],[0]
"The influence function I(y;x) defined in Section 2 is continuous submodular in x over the nonnegative orthant, for each y 0.
Proof.",3. Optimization Algorithm,[0],[0]
"Since submodularity is preserved under summation, it suffices to show that each function It(y) is continuous submodular.",3. Optimization Algorithm,[0],[0]
"By Lemma 3.1, since fs(z) = zy(s) is nonnegative and monotone nondecreasing for y(s) 0, the product Q
(s,t)2E x y(s) st is continuous supermodular in x.
Flipping the sign and adding a constant term yields It(y), which is hence continuous submodular.
",3. Optimization Algorithm,[0],[0]
Conjecture 3.1.,3. Optimization Algorithm,[0],[0]
"Strong duality holds, i.e.
max y2Y min x2X I(y;x) =",3. Optimization Algorithm,[0],[0]
min x2X max y2Y I(y;x).,3. Optimization Algorithm,[0],[0]
"(11)
",3. Optimization Algorithm,[0],[0]
"If strong duality holds, then the duality gap maxy2Y I(y;x⇤) minx2X I(y⇤;x)",3. Optimization Algorithm,[0],[0]
in Equation (9) is zero at optimality.,3. Optimization Algorithm,[0],[0]
"If I(y;x) were quasiconvex in x, strong duality would hold by Sion’s min-max theorem, but this is not the case.",3. Optimization Algorithm,[0],[0]
"In practice, we observe that the duality gap always converges to zero.
",3. Optimization Algorithm,[0],[0]
"Bach (2015) demonstrates how to minimize a continuous submodular function H(x) subject to box constraints x 2 Box(l, u), up to an arbitrary suboptimality gap "" > 0.",3. Optimization Algorithm,[0],[0]
"The constraint set X in our Robust Budget Allocation problem, however, has box constraints with an additional constraint R(x)  ",3. Optimization Algorithm,[0],[0]
B. This case is not addressed in any previous work.,3. Optimization Algorithm,[0],[0]
"Fortunately, for a large class of functions R, there is still an efficient algorithm for continuous submodular minimization, which we present in the next section.",3. Optimization Algorithm,[0],[0]
"We next address an algorithm for minimizing a monotone continuous submodular function H(x) subject to box constraints x 2 Box(l, u) and a constraint R(x)  ",3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
"B:
minimize H(x) s.t. R(x)  ",3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
"B
x 2 Box(l, u).",3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
"(12)
If H and R were convex, the constrained problem would be equivalent to solving, with the right Lagrange multipler ⇤ 0:
minimize H(x) + ⇤R(x) s.t. x 2",3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
"Box(l, u).",3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
"(13)
Although H and R are not necessarily convex here, it turns out that a similar approach indeed applies.",3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
"The main idea of our approach bears similarity with (Nagano et al., 2011) for the set function case, but our setting with continuous functions and various uncertainty sets is more general, and requires more argumentation.",3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
"We outline our theoretical results here, and defer further implementation details and proofs to the appendix.
",3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
"Following (Bach, 2015), we discretize the problem; for a sufficiently fine discretization, we will achieve arbitrary accuracy.",3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
"Let A be an interpolation mapping that maps the
discrete set Qn i=1[ki] into Box(l, u) = Qn
i=1[li, ui] via the componentwise interpolation functions Ai : [ki]!",3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
"[li, ui].",3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
"We say Ai is -fine if Ai(xi + 1) Ai(xi)  for all xi 2 {0, 1, . . .",3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
", ki 2}, and we say the full interpolation function A is -fine if each Ai is -fine.
",3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
This mapping yields functions H :,3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
Qn i=1[ki] !,3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
"R and R :
Qn i=1[ki] !",3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
"R via H (x) = H(A(x)) and R (x) =
R(A(x)).",3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
H is lattice submodular (on the integer lattice).,3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
"This construction leads to a reduction of Problem (12) to a submodular minimization problem over the integer lattice:
minimize H (x) +",3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
"R (x) s.t. x 2
Qn i=1[ki].
(14)
Ideally, there should then exist a such that the associated minimizer x( ) yields a close to optimal solution for the constrained problem.",3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
"Theorem 3.1 below states that this is indeed the case.
",3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
"Moreover, a second benefit of submodularity is that we can find the entire solution path for Problem (14) by solving a single optimization problem.
",3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
Lemma 3.2.,3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
"Suppose H is continuous submodular, and suppose the regularizer R is strictly increasing and separable: R(x) =
Pn i=1",3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
Ri(xi).,3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
"Then we can recover a min-
imizer x( ) for the induced discrete Problem (14) for any 2 R by solving a single convex optimization problem.
",3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
The problem in question arises from a relaxation h# that extends H in each coordinate i to a function on distributions over the domain [ki].,3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
"These distributions are represented via their inverse cumulative distribution functions ⇢i, which take the coordinate xi as input, and output the probability of exceeding xi.",3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
"The function h# is an analogue of the Lovász extension of set functions to continuous submodular functions (Bach, 2015), it is convex and coincides with H on lattice points.
",3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
"Formally, this resulting single optimization problem is:
minimize h#(⇢) +",3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
"Pn
i=1",3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
"Pki 1 ji=1 aixi(⇢i(xi))
s.t. ⇢ 2",3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
Qn i=1,3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
"R ki 1 #
(15)
where Rk# refers to the set of ordered vectors z 2 Rk that satisfy z
1 z 2 · · · zk, the notation ⇢i(xi) denotes the xi-th coordinate of the vector ⇢i, and the aixi are strictly convex functions given by
aixi(t) = 1
2
t2 ·",3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
[R i (xi) R i (xi 1)].,3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
"(16)
Problem (15) can be solved by Frank-Wolfe methods (Frank & Wolfe, 1956; Dunn & Harshbarger, 1978; Lacoste-Julien, 2016; Jaggi, 2013).",3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
"This is because the greedy algorithm for computing subgradients of the Lovász
extension can be generalized, and yields a linear optimization oracle for the dual of Problem (15).",3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
"We detail the relationship between Problems (14) and (15), as well as how to implement the Frank-Wolfe methods, in Appendix C.
Let ⇢⇤ be the optimal solution for Problem (15).",3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
"For any , we obtain a rounded solution x( ) for Problem (14) by thresholding: we set x( )i = max{j | 1  j  ki 1, ⇢⇤i (j) }, or zero if ⇢⇤i (j) < for all j. Each x( 0) is the optimal solution for Problem (14) with = 0.",3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
"We use the largest parameterized solution x( ) that is still feasible, i.e. the solution x( ⇤) where ⇤ solves
min H (x( ))",3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
"s.t. 0
R (x( ))  ",3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
"B. (17)
",3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
This ⇤ can be found efficiently via binary search or a linear scan.,3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
Theorem 3.1.,3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
"Let H be continuous submodular and monotone decreasing, with `1-Lipschitz constant G, and let R be strictly increasing and separable.",3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
Assume all entries ⇢⇤i (j) of the optimal solution ⇢⇤ of Problem (15) are distinct.,3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
"Let x0 = A(x( ⇤)) be the thresholding corresponding to the optimal solution ⇤ of Problem (17), mapped back into the original continuous domain X .",3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
"Then x0 is feasible for the continuous Problem (12), and is a 2G - approximate solution:
H(x0)  2G + min",3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
"x2Box(l,u), R(x)B H(x).
",3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
"Theorem 3.1 implies an algorithm for solving Problem (12) to ""-optimality: (1) set = ""/G, (2) compute ⇢⇤ which solves Problem (15), (3) find the optimal thresholding of ⇢⇤ by determining the smallest ⇤ for which R (x( ⇤))  B, and (4) map",3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
"x( ⇤) back into continuous space via the interpolation mapping A.
Optimality Bounds.",3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
"Theorem 3.1 is proved by comparing x0 and x⇤ to the optimal solution on the discretized mesh
x⇤d 2 argmin x2",3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
"Qn i=1[ki]:R (x)B H (x).
",3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
"Beyond the theoretical guarantee of Theorem 3.1, for any problem instance and candidate solution x0, we can compute a bound on the gap between H(x0) and H (x⇤d).",3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
"The following two bounds are proved in the appendix:
1.",3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
We can generate a discrete point x( + ),3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
"satisfying
H(x0)  ",3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
[H(x0) H (x( + ))],3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
"+H (x⇤d).
",3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
2.,3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
"The Lagrangian yields the bound
H(x0)  ",3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
⇤(B R(x0)),3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
"+H (x⇤d).
",3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
Improvements.,3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
"The requirement in Theorem 3.1 that the elements of ⇢⇤ be distinct may seem somewhat restrictive, but as long as ⇢⇤ has distinct elements in the neighborhood of our particular ⇤, this bound still holds.",3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
"We see in Section 4.1.1 that in practice, ⇢⇤ almost always has distinct elements in the regime we care about, and the bounds of Remark 3.1 are very good.
",3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
"If H is DR-submodular and R is affine in each coordinate, then Problem (14) can be represented more compactly via the reduction of Ene & Nguyen (2016), and hence problem (12) can be solved more efficiently.",3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
"In particular, the influence function I(y;x) is DR-submodular in x when for each s, y(s) = 0 or y(s) 1.",3.1. Constrained Continuous Submodular Function Minimization,[0],[0]
The above algorithm directly applies to Robust Allocation with the uncertainty sets in Section 2.2.,3.2. Application to Robust Budget Allocation,[0],[0]
"The ellipsoidal uncertainty set XQ corresponds to the constraint that P
(s,t)2E Rst(xst)  with Rst(x) =",3.2. Application to Robust Budget Allocation,[0],[0]
"(xst x̂st)2 2 st , and x 2 Box(0, 1).",3.2. Application to Robust Budget Allocation,[0],[0]
"By the monotonicity of I(x, y), there is never incentive to reduce any xst below x̂st, so we can replace Box(0, 1) with Box(x̂, 1).",3.2. Application to Robust Budget Allocation,[0],[0]
"On this interval, each Rst is strictly increasing, and Theorem 3.1 applies.
",3.2. Application to Robust Budget Allocation,[0],[0]
"For D-norm sets, we have Rst(xst) =",3.2. Application to Robust Budget Allocation,[0],[0]
(xst x̂st)/(ust x̂st).,3.2. Application to Robust Budget Allocation,[0],[0]
"Since each Rst is monotone, Theorem 3.1 applies.
",3.2. Application to Robust Budget Allocation,[0],[0]
Runtime and Alternatives.,3.2. Application to Robust Budget Allocation,[0],[0]
"Since the core algorithm is Frank-Wolfe, it is straightforward to show that Problem (15) can be solved to ""-suboptimality in time",3.2. Application to Robust Budget Allocation,[0],[0]
"O("" 1n2 3↵ 1|T |2 log n 1), where ↵ is the minimum derivative of the functions Ri.",3.2. Application to Robust Budget Allocation,[0],[0]
"If ⇢⇤ has distinct elements separated by ⌘, then choosing "" = ⌘2↵ /8 results in an exact solution to (14) in time O(⌘ 2n2 4↵ 2|T |2 log n 1).
",3.2. Application to Robust Budget Allocation,[0],[0]
"Noting that H + R is submodular for all , one could instead perform binary search over , each time converting the objective into a submodular set function via Birkhoff’s theorem and solving submodular minimization e.g. via one of the recent fast methods (Chakrabarty et al., 2017; Lee et al., 2015).",3.2. Application to Robust Budget Allocation,[0],[0]
"However, we are not aware of a practical implementation of the algorithm in (Lee et al., 2015).",3.2. Application to Robust Budget Allocation,[0],[0]
"The algorithm in (Chakrabarty et al., 2017) yields a solution in expectation.",3.2. Application to Robust Budget Allocation,[0],[0]
"This approach also requires care in the precision of the search over , whereas our approach searches directly over the O(n 1) elements of ⇢⇤.",3.2. Application to Robust Budget Allocation,[0],[0]
We evaluate our Robust Budget Allocation algorithm on both synthetic test data and a real-world bidding dataset from,4. Experiments,[0],[0]
Yahoo! Webscope (yah) to demonstrate that our method yields real improvements.,4. Experiments,[0],[0]
"For all experiments, we
used Algorithm 1 as the outer loop.",4. Experiments,[0],[0]
"For the inner submodular minimization step, we implemented the pairwise Frank-Wolfe algorithm of (Lacoste-Julien & Jaggi, 2015).",4. Experiments,[0],[0]
"In all cases, the feasible set of budgets Y is {y 2 RS
+
:
P
s2S y(s)  C} where the specific budget C depends on the experiment.",4. Experiments,[0],[0]
Our code is available at git.io/vHXkO.,4. Experiments,[0],[0]
"On the synthetic data, we probe two questions: (1) how often does the distinctness condition of Theorem 3.1 hold, so that we are guaranteed an optimal solution; and (2) what is the gain of using a robust versus non-robust solution in an adversarial setting?",4.1. Synthetic,[0],[0]
"For both settings, we set |S| = 6 and |T | = 2 and discretize with = 0.001.",4.1. Synthetic,[0],[0]
"We generated true probabilties pst, created Beta posteriors, and built both Ellipsoidal uncertainty sets XQ( ) and D-norm sets XD( ).",4.1. Synthetic,[0],[0]
"Theorem 3.1 and Remark 3.1 demand that the values ⇢⇤i (j) be distinct at our chosen Lagrange multiplier ⇤ and, under this condition, guarantee optimality.",4.1.1. OPTIMALITY,[0],[0]
"We illustrate this in four examples: for Ellipsoidal or a D-norm uncertainty set, and a total influence budget C 2 {0.4, 4}.",4.1.1. OPTIMALITY,[0],[0]
"Figure 3 shows all elements of ⇢⇤ in sorted order, as well as a horizontal line indicating our Lagrange multiplier ⇤ which serves as a threshold.",4.1.1. OPTIMALITY,[0],[0]
"Despite some plateaus, the entries ⇢⇤i (j) are distinct in most regimes, in particular around ⇤, the regime that is needed for our results.",4.1.1. OPTIMALITY,[0],[0]
"Moreover, in practice (on the Yahoo data) we observe later in Figure 3 that both solutiondependent bounds from Remark 3.1 are very good, and all solutions are optimal within a very small gap.",4.1.1. OPTIMALITY,[0],[0]
"Next, we probe the effect of a robust versus non-robust solution for different uncertainty sets and budgets of the adversary.",4.1.2. ROBUSTNESS AND QUALITY,[0],[0]
"We compare our robust solution with using a point estimate for x, i.e., y
nom 2 argmaxy2Y I(y; x̂), treating estimates as ground truth, and the stochastic solution y
expect 2 argmaxy2Y E[I(y;X)] as per Section 2.1.",4.1.2. ROBUSTNESS AND QUALITY,[0],[0]
"These two optimization problems were solved via standard first-order methods using TFOCS (Becker et al., 2011).
",4.1.2. ROBUSTNESS AND QUALITY,[0],[0]
"Figure 2 demonstrates that indeed, the alternative budgets are sensitive to the adversary and the robustly-chosen budget y
robust performs better, even in cases where the other budgets achieve zero influence.",4.1.2. ROBUSTNESS AND QUALITY,[0],[0]
"When the total budget C is large, y
expect performs nearly as well as y robust , but when resources are scarce (C is small) and the actual choice seems to matter more, y
robust
performs far better.",4.1.2. ROBUSTNESS AND QUALITY,[0],[0]
"To evaluate our method on real-world data, we formulate a Budget Allocation instance on advertiser bidding data from Yahoo! Webscope (yah).",4.2. Yahoo! data,[0],[0]
This dataset logs bids on 1000 different phrases by advertising accounts.,4.2. Yahoo! data,[0],[0]
"We map the phrases to channels S and the accounts to customers T , with an edge between s and t if a corresponding bid was made.",4.2. Yahoo! data,[0],[0]
"For each pair (s, t), we draw the associated transmission probability pst uniformly from [0, 0.4].",4.2. Yahoo! data,[0],[0]
We bias these towards zero because we expect people not to be easily influenced by advertising in the real world.,4.2. Yahoo! data,[0],[0]
"We then generate an estimate p̂st and build up a posterior by gener-
ating nst samples from Ber(pst), where nst is the number of bids between s and t in the dataset.
",4.2. Yahoo! data,[0],[0]
"This transformation yields a bipartite graph with |S| = 1000, |T | = 10475, and more than 50,000 edges that we use for Budget Allocation.",4.2. Yahoo! data,[0],[0]
"In our experiments, the typical gap between the naive y
nom and robust y robust was 100- 500 expected influenced people.",4.2. Yahoo! data,[0],[0]
"We plot convergence of the outer loop in Figure 3, where we observe fast convergence of both primal influence value and the dual bound.",4.2. Yahoo! data,[0],[0]
"Given the success of first-order methods on nonconvex problems in practice, it is natural to compare these to our method for finding the worst-case vector x. On one of our Yahoo problem instances with D-norm uncertainty set, we compared our submodular minimization scheme to FrankWolfe with fixed stepsize as in (Lacoste-Julien, 2016), implementing the linear oracle using MOSEK (MOSEK ApS, 2015).",4.3. Comparison to first-order methods,[0],[0]
"Interestingly, from various initializations, FrankWolfe finds an optimal solution, as verified by comparing to the guaranteed solution of our algorithm.",4.3. Comparison to first-order methods,[0],[0]
"Note that, due to non-convexity, there are no formal guarantees for FrankWolfe to be optimal here, motivating the question of global convergence properties of Frank-Wolfe in the presence of submodularity.
",4.3. Comparison to first-order methods,[0],[0]
It is important to note that there are many cases where firstorder methods are inefficient or do not apply to our setup.,4.3. Comparison to first-order methods,[0],[0]
"These methods require either a projection oracle (PO) onto or linear optimization oracle (LO) over the feasible set X defined by `, u and R(x).",4.3. Comparison to first-order methods,[0],[0]
"The D-norm set admits a LO via linear programming, but we are not aware of any efficient LO for Ellipsoidal uncertainty, nor PO for either set, that does not require quadratic programming.",4.3. Comparison to first-order methods,[0],[0]
"Even more, our algorithm applies for nonconvex functions R(x) which induce nonconvex feasible sets X .",4.3. Comparison to first-order methods,[0],[0]
"Such nonconvex sets may not even admit a unique projection, while our algorithm achieves provable solutions.",4.3. Comparison to first-order methods,[0],[0]
"We address the issue of uncertain parameters (or, model misspecification) in Budget Allocation or Bipartite Influence Maximization (Alon et al., 2012) from a robust optimization perspective.",5. Conclusion,[0],[0]
The resulting Robust Budget Allocation is a nonconvex-concave saddle point problem.,5. Conclusion,[0],[0]
"Although the inner optimization problem is nonconvex, we show how continuous submodularity can be leveraged to solve the problem to arbitrary accuracy "", as can be verified with the proposed bounds on the duality gap.",5. Conclusion,[0],[0]
"In particular, our approach extends continuous submodular minimization methods (Bach, 2015) to more general constraint sets, introducing a mechanism to solve a new class of constrained nonconvex optimization problems.",5. Conclusion,[0],[0]
"We confirm on synthetic and real data that our method finds high-quality solutions that are robust to parameters varying arbitrarily in an uncertainty set, and scales up to graphs with over 50,000 edges.
",5. Conclusion,[0],[0]
There are many compelling directions for further study.,5. Conclusion,[0],[0]
"The uncertainty sets we use are standard in the robust optimization literature, but have not been applied to e.g. Robust Influence Maximization; it would be interesting to generalize our ideas to general graphs.",5. Conclusion,[0],[0]
"Finally, despite the inherent nonconvexity of our problem, first-order methods are often able to find a globally optimal solution.",5. Conclusion,[0],[0]
Explaining this phenomenon requires further study of the geometry of constrained monotone submodular minimization.,5. Conclusion,[0],[0]
We thank the anonymous reviewers for their helpful suggestions.,Acknowledgements,[0],[0]
We also thank MIT Supercloud and the Lincoln Laboratory Supercomputing Center for providing computational resources.,Acknowledgements,[0],[0]
"This research was conducted with Government support under and awarded by DoD, Air Force Office of Scientific Research, National Defense Science and Engineering Graduate (NDSEG) Fellowship, 32 CFR 168a, and also supported by NSF CAREER award 1553284.",Acknowledgements,[0],[0]
"The optimal allocation of resources for maximizing influence, spread of information or coverage, has gained attention in the past years, in particular in machine learning and data mining.",abstractText,[0],[0]
"But in applications, the parameters of the problem are rarely known exactly, and using wrong parameters can lead to undesirable outcomes.",abstractText,[0],[0]
"We hence revisit a continuous version of the Budget Allocation or Bipartite Influence Maximization problem introduced by Alon et al. (2012) from a robust optimization perspective, where an adversary may choose the least favorable parameters within a confidence set.",abstractText,[0],[0]
The resulting problem is a nonconvex-concave saddle point problem (or game).,abstractText,[0],[0]
"We show that this nonconvex problem can be solved exactly by leveraging connections to continuous submodular functions, and by solving a constrained submodular minimization problem.",abstractText,[0],[0]
"Although constrained submodular minimization is hard in general, here, we establish conditions under which such a problem can be solved to arbitrary precision ✏.",abstractText,[0],[0]
Robust Budget Allocation via Continuous Submodular Functions,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 607–618 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
"Translation helps identify correspondences in bilingual texts, but other asymmetric semantic relationships can improve language understanding when translations are not exactly equivalent.",1 Introduction,[0],[0]
"One such relationship is cross-lingual hypernymy – identifying that écureuil (“squirrel” in French) is a kind of rodent, or ворона (“crow” in Russian) is a kind of bird.",1 Introduction,[0],[0]
"The ability to detect hypernyms across languages serves as a building block in a range of cross-lingual tasks, including Recognizing Textual Entailment (RTE) (Negri et al., 2012,
∗ These authors contributed equally.",1 Introduction,[0],[0]
"1https://github.com/yogarshi/
bisparse-dep/
2013), constructing multilingual taxonomies (Fu et al., 2014), event coreference across multilingual news sources (Vossen et al., 2015), and evaluating Machine Translation output (Padó et al., 2009).
",1 Introduction,[0],[0]
"Building models that can robustly identify hypernymy across the spectrum of human languages is a challenging problem, that is further compounded in low resource settings.",1 Introduction,[0],[0]
"At first glance, translating words to English and then identifying hypernyms in a monolingual setting may appear to be a sufficient solution.",1 Introduction,[0],[0]
"However, this approach cannot capture many phenomena.",1 Introduction,[0],[0]
"For instance, the English words cook, leader and supervisor can all be hypernyms of the French word chef, as the French word does not have a exact translation in English covering its possible usages.",1 Introduction,[0],[0]
"However, translating chef to cook and then determining hypernymy monolingually precludes identifying leader or supervisor as a hypernyms of chef.",1 Introduction,[0],[0]
"Similarly, language-specific usage patterns can also influence hypernymy decisions.",1 Introduction,[0],[0]
"For instance, the French word chroniqueur translates to chronicler in English, but is more frequently used in French to refer to journalists (making journalist its hypernym).2
This motivates approaches that directly detect hypernymy in the cross-lingual setting by extending distributional methods for detecting monolingual hypernymy, as in our prior work (Vyas and Carpuat, 2016).",1 Introduction,[0],[0]
"State-of-the-art distributional approaches (Roller and Erk, 2016; Shwartz et al., 2017) for detecting monolingual hypernymy require syntactic analysis (eg. dependency parsing), which may not available for many languages.",1 Introduction,[0],[0]
"Additionally, limited training resources make unsupervised methods more desirable than supervised hypernymy detection approaches (Roller and Erk,
2All examples are from our dataset.
",1 Introduction,[0],[0]
"607
2016)",1 Introduction,[0],[0]
.,1 Introduction,[0],[0]
"Furthermore, monolingual distributional approaches cannot be applied directly to the crosslingual task, because the vector spaces of two languages need to be aligned using a cross-lingual resource (a bilingual dictionary, for instance).
",1 Introduction,[0],[0]
"We tackle these challenges by proposing BISPARSE-DEP - a family of robust, unsupervised approaches for identifying cross-lingual hypernymy.",1 Introduction,[0],[0]
BISPARSE-DEP uses a cross-lingual word embedding model learned from a small bilingual dictionary and a variety of monolingual syntactic context extracted from a dependency parsed corpus.,1 Introduction,[0],[0]
BISPARSE-DEP exhibits robust behavior along multiple dimensions.,1 Introduction,[0],[0]
"In the absence of a dependency treebank for a language, it can learn embeddings using a parser trained on related languages.",1 Introduction,[0],[0]
"When exposed to less monolingual data, or a lower quality bilingual dictionary, BISPARSEDEP degrades only marginally.",1 Introduction,[0],[0]
"In all these cases, it compares favorably with models that have been supplied with all necessary resources, showing promise for low-resource settings.",1 Introduction,[0],[0]
"We extensively evaluate BISPARSE-DEP on a new crowd-sourced cross-lingual dataset, with over 2900 hypernym pairs, spanning four languages from distinct families – French, Russian, Arabic and Chinese – and release the datasets for future evaluations.",1 Introduction,[0],[0]
"Cross-lingual Distributional Semantics Cross-lingual word embeddings have been shown to encode semantics across languages in tasks such as word similarity (Faruqui and Dyer, 2014) and lexicon induction (Vulić and Moens, 2015).",2 Related Work,[0],[0]
"Our works stands apart in two aspects (1) In contrast to tasks involving similarity and synonymy (symmetric relations), the focus of our work is on detecting asymmetric relations across languages, using cross-lingual embeddings.",2 Related Work,[0],[0]
"(2) Unlike most previous work, we use dependency context instead of lexical context to induce crosslingual embeddings, which allows us to abstract away from language specific word order, and (as we show) improves hypernymy detection.
",2 Related Work,[0],[0]
"More closely related is our prior work (Vyas and Carpuat, 2016) where we used lexical context based embeddings to detect cross-lingual lexical entailment.",2 Related Work,[0],[0]
"In contrast, the focus of this work is on hypernymy, a more well-defined relation than entailment.",2 Related Work,[0],[0]
"Also, we improve upon our previous approach by using dependency based embeddings (§6.1), and show that the improvements hold even when exposed to data scarce settings (§6.3).
",2 Related Work,[0],[0]
"We also do a more comprehensive evaluation on four languages paired with English, instead of just French.
",2 Related Work,[0],[0]
"Dependency Based Embeddings In monolingual settings, dependency based embeddings have been shown to outperform window based embeddings on many tasks (Bansal et al., 2014; Hill et al., 2014; Melamud et al., 2016).",2 Related Work,[0],[0]
"Roller and Erk (2016) showed that dependency embeddings can help in recovering Hearst patterns (Hearst, 1992) like “animals such as cats”, which are known to be indicative of hypernymy.",2 Related Work,[0],[0]
Shwartz et al. (2017) demonstrated that dependency based embeddings are almost always superior to window based embeddings for identifying hypernyms in English.,2 Related Work,[0],[0]
"Our work uses dependency based embeddings in a cross-lingual setting, a less explored research direction.",2 Related Work,[0],[0]
A key novelty of our work also lies in its use of syntactic transfer to derive dependency contexts.,2 Related Work,[0],[0]
"This scenario is more relevant in a cross-lingual setting, where treebanks might not be available for many languages.
",2 Related Work,[0],[0]
"3 Our Approach – BISPARSE-DEP
We propose BISPARSE-DEP, a family of approaches that uses sparse, bilingual, dependency based word embeddings to identify cross-lingual hypernymy.
",2 Related Work,[0],[0]
Figure 1 shows an overview of the end-toend pipeline of BISPARSE-DEP.,2 Related Work,[0],[0]
"The two key components of this pipeline are: (1) Dependency based contexts (§3.1), which help us generalize across languages with minimal customization by abstracting away language-specific word order.",2 Related Work,[0],[0]
We also discuss how to extract such contexts in the absence of a treebank in the language (§3.2) using a (weak) dependency parser trained on related languages.,2 Related Work,[0],[0]
"(2) Bilingual sparse coding (§3.3), which allows us to align dependency based word embeddings in a shared semantic space using a small bilingual dictionary.",2 Related Work,[0],[0]
The resulting sparse bilingual embeddings can then be used with a unsupervised entailment scorer (§3.4) to predict hypernymy for cross-lingual word pairs.,2 Related Work,[0],[0]
The context of a word can be described in multiple ways using its syntactic neighborhood in a dependency graph.,3.1 Dependency Based Context Extraction,[0],[0]
"For instance, in Figure 2, we describe the context for a target word (traveler) in the following two ways:
• FULL context (Padó and Lapata, 2007; Baroni and Lenci, 2010; Levy and Goldberg, 2014): Children and parent words, concatenated with the label and direction of the relation (eg. roamed#nsubj−1 and tired#amod are contexts for traveler).",3.1 Dependency Based Context Extraction,[0],[0]
•,3.1 Dependency Based Context Extraction,[0],[0]
"JOINT context (Chersoni et al., 2016): Par-
ent concatenated with each of its siblings (eg. roamed#desert and roamed#seeking are contexts for traveler).
",3.1 Dependency Based Context Extraction,[0],[0]
"These two contexts exploit different amounts of syntactic information – JOINT does not require labeled parses, unlike FULL.",3.1 Dependency Based Context Extraction,[0],[0]
"The JOINT context combines parent and sibling information, while FULL keeps them as distinct contexts.",3.1 Dependency Based Context Extraction,[0],[0]
"Both encode directionality into the context, either through label direction or through sibling-parent relations.
",3.1 Dependency Based Context Extraction,[0],[0]
We use word-context co-occurrences generated using these contexts in a distributional semantic model (DSM) in lieu of window based contexts to generate dependency based embeddings.,3.1 Dependency Based Context Extraction,[0],[0]
"Using dependency contexts in multilingual settings may not always be possible, as dependency treebanks are not available for many languages.",3.2 Dependency Contexts without a Treebank,[0],[0]
"To circumvent this issue, we use related languages to train a weak dependency parser.
",3.2 Dependency Contexts without a Treebank,[0],[0]
"We train a delexicalized parser using treebanks of related languages, where the word form based
features are turned off, so that the parser is trained on purely non-lexical features (e.g. POS tags).",3.2 Dependency Contexts without a Treebank,[0],[0]
"The rationale behind this is that related languages show common syntactic structure that can be transferred to the original language, with delexicalized parsing (Zeman and Resnik, 2008; McDonald et al., 2011, inter alia) being one popular approach.3",3.2 Dependency Contexts without a Treebank,[0],[0]
"Given a dependency based co-occurrence matrix described in the previous section(s), we generate BISPARSE-DEP embeddings using the framework from our prior work (Vyas and Carpuat, 2016), which we henceforth call BISPARSE.",3.3 Bilingual Sparse Coding,[0],[0]
"BISPARSE generates sparse, bilingual word embeddings using a dictionary learning objective with a sparsity inducing l1 penalty.",3.3 Bilingual Sparse Coding,[0],[0]
"We give a brief overview of this approach, the full details of which can be found in our prior work.
",3.3 Bilingual Sparse Coding,[0],[0]
"For two languages with vocabularies ve and vf , and monolingual dependency embeddings",3.3 Bilingual Sparse Coding,[0],[0]
"Xe and Xf , BISPARSE solves the following objective:
argmin Ae,De,Af ,Df
ve∑
i=1
1 2 ||AeiDeT",3.3 Bilingual Sparse Coding,[0],[0]
−Xei||22,3.3 Bilingual Sparse Coding,[0],[0]
"+λe||Aei||1
+
vf∑
j=1
1 2 ||Af jDfT −Xf j ||22 +λf ||Af j ||1
+ ∑
i,j
1 2 λxSij ||Aei −Af j ||22 (1)
s.t.",3.3 Bilingual Sparse Coding,[0],[0]
Ak > 0,3.3 Bilingual Sparse Coding,[0],[0]
"‖Dki‖22≤ 1 k ∈ {e, f}
where S is a translation matrix, and Ae and Af 3More sophisticated techniques for transferring syntactic knowledge have been proposed (Ammar et al., 2016; Rasooli and Collins, 2017), but we prioritize simplicity and show that a simple delexicalized parser is effective.
are sparse matrices which are bilingual representations in a shared semantic space.",3.3 Bilingual Sparse Coding,[0],[0]
The translation matrix S (of size ve×vf ) captures correspondences between the vocabularies (of size ve and vf ) of two languages.,3.3 Bilingual Sparse Coding,[0],[0]
"For instance, each row of S can be a one-hot vector that identifies the word in f that is most frequently aligned with the e word for that row in a large parallel corpus, thus building a one-to-many mapping between the two languages.",3.3 Bilingual Sparse Coding,[0],[0]
"A variety of scorers can be used to quantify the directional relationship between two words, given feature representations of these words (Lin, 1998; Weeds and Weir, 2003; Lenci and Benotto, 2012).",3.4 Unsupervised Entailment Scorer,[0],[0]
"Once the BISPARSE-DEP embeddings are constructed, we use BalAPinc",3.4 Unsupervised Entailment Scorer,[0],[0]
"(Kotlerman et al., 2009) to score word pairs for hypernymy.",3.4 Unsupervised Entailment Scorer,[0],[0]
"BalAPinc is based on the distributional inclusion hypothesis (Geffet and Dagan, 2005) and computes the geometric mean of 1) LIN (Lin, 1998), a symmetric score that captures similarity, and 2) APinc, an asymmetric score based on average precision.",3.4 Unsupervised Entailment Scorer,[0],[0]
There is no publicly available dataset to evaluate models of hypernymy detection across multiple languages.,4 Crowd-Sourcing Annotations,[0],[0]
"While ontologies like Open Multilingual WordNet (OMW) (Bond and Foster, 2013) and BabelNet (Navigli and Ponzetto, 2012) contain cross-lingual links, these resources are semiautomatically generated and hence contain noisy edges.",4 Crowd-Sourcing Annotations,[0],[0]
"Thus, to get reliable and high-quality test beds, we collect evaluation datasets using CrowdFlower4.",4 Crowd-Sourcing Annotations,[0],[0]
"Our datasets span four languages from distinct families - French (Fr), Russian (Ru), Arabic (Ar) and Chinese (Zh) - paired with English.
",4 Crowd-Sourcing Annotations,[0],[0]
"To begin the annotation process, we first pool candidate pairs using hypernymy edges across languages from OMW and BabelNet, along with translations from monolingual hypernymy datasets (Baroni and Lenci, 2011; Baroni et al., 2012; Kotlerman et al., 2010).",4 Crowd-Sourcing Annotations,[0],[0]
The annotation task requires annotators to be fluent in both English and the non-English language.,4.1 Annotation Setup,[0],[0]
"To ensure only fluent speakers perform the task, for each language, we provide task instructions in the non-English language itself.",4.1 Annotation Setup,[0],[0]
"Also, we restrict the task to annotators verified by CrowdFlower to have those language skills.",4.1 Annotation Setup,[0],[0]
"Finally, annotators also
4http://crowdflower.com
need to pass a quiz based on a small amount of gold standard data to gain access to the task.
",4.1 Annotation Setup,[0],[0]
"Annotators choose between three options for each word pair (pf , qe), where pf is a non-English word and qe is a English word : “pf is a kind of qe”, “qe is a part of pf” and “none of the above”.",4.1 Annotation Setup,[0],[0]
Word pairs labeled with the first option are considered as positive examples while those labeled as “none of the above” are considered as negative.5 The second option was included to filter out meronymy examples that were part of the noisy pool.,4.1 Annotation Setup,[0],[0]
"We leave it to the annotator to infer whether the relation holds between any senses of pf or qe, if either of them are polysemous.
",4.1 Annotation Setup,[0],[0]
"For every candidate hypernym pair (pf , qe), we also ask annotators to judge its reversed and translated hyponym pair (qf , pe).",4.1 Annotation Setup,[0],[0]
"For instance, if (citron, food) is a hypernym candidate, we also show annotators (aliments, lemon) which is a potential hyponym candidate (potential, because as mentioned in §1, translation need not preserve semantic relationships).",4.1 Annotation Setup,[0],[0]
"The purpose of presenting the hyponym pair, (qf , pe), is two-fold.",4.1 Annotation Setup,[0],[0]
"First, it emphasizes the directional nature of the task.",4.1 Annotation Setup,[0],[0]
"Second, it identifies hyponym pairs, which we use as negative examples.",4.1 Annotation Setup,[0],[0]
"The hyponym pairs are challenging since differentiating them from hypernyms truly requires detecting asymmetry.
",4.1 Annotation Setup,[0],[0]
"Each pair was judged by at least 5 annotators, and judgments with 80% agreement (at least 4 annotators agree) are considered for the final dataset.",4.1 Annotation Setup,[0],[0]
"This is a stricter condition than certain monolingual hypernymy datasets - for instance, EVALution (Santus et al., 2015) - where agreement by 3 annotators is deemed sufficient.",4.1 Annotation Setup,[0],[0]
"Inter-annotator agreement measured using Fleiss’ Kappa (Fleiss, 1971) was 58.1 (French), 53.7 (Russian), 53.2 (Arabic) and 55.8 (Chinese).",4.1 Annotation Setup,[0],[0]
"This indicates moderate agreement, on par with agreement obtained on related fine-grained semantic tasks (Pavlick et al., 2015).",4.1 Annotation Setup,[0],[0]
"We cannot compare with monolin-
5We collected more negative pairs than positive, but sampled so as to keep a balanced dataset for ease of evaluation.",4.1 Annotation Setup,[0],[0]
"We will release all annotated pairs along with the dataset.
gual hypernymy annotator agreement as, to the best of our knowledge, such numbers are not available for existing test sets.",4.1 Annotation Setup,[0],[0]
"Dataset statistics are shown in Table 1.
",4.1 Annotation Setup,[0],[0]
We observed that annotators were able to agree on pairs containing polysemous words where hypernymy holds for some sense.,4.1 Annotation Setup,[0],[0]
"For instance, for the French-English pair (avocat, professional), the French word avocat can either mean lawyer or avocado, but the pair was annotated as a positive example.",4.1 Annotation Setup,[0],[0]
"Hence, we leave it to the annotators to handle polysemy by choosing the most appropriate sense.",4.1 Annotation Setup,[0],[0]
To verify if the crowdsourced hyponyms are challenging negative examples we create two evaluation sets.,4.2 Two Evaluation Test Sets,[0],[0]
"Both share the (crowdsourced) positive examples, but differ in their negatives:
• HYPER-HYPO – negative examples are the crowdsourced hyponyms.",4.2 Two Evaluation Test Sets,[0],[0]
"• HYPER-COHYPO – negative examples are
cohyponyms drawn from OMW.
",4.2 Two Evaluation Test Sets,[0],[0]
Cohyponyms are words sharing a common hypernym.,4.2 Two Evaluation Test Sets,[0],[0]
"For instance, bière (“beer” in French) and vodka are cohyponyms since they share a common hypernym in alcool/alcohol.",4.2 Two Evaluation Test Sets,[0],[0]
We choose cohyponyms for the second test set because: (a) They require differentiating between similarity (a symmetric relation) and hypernymy (an asymmetric relation).,4.2 Two Evaluation Test Sets,[0],[0]
"For instance, bière and vodka are highly similar yet, they do not have a hypernymy relationship.",4.2 Two Evaluation Test Sets,[0],[0]
"(b) Cohyponyms are a popular choice of negative examples in many entailment datasets (Baroni and Lenci, 2011).",4.2 Two Evaluation Test Sets,[0],[0]
"Training BISPARSE-DEP requires a dependency parsed monolingual corpus, and a translation matrix for jointly aligning the monolingual vectors.",5.1 Data and Evaluation Setup,[0],[0]
We compute the translation matrix using word alignments derived from parallel corpora (see corpus statistics in Table ??).,5.1 Data and Evaluation Setup,[0],[0]
"While we use parallel corpora to generate the translation matrix to be comparable to baselines (§5.2), we can obtain the matrix from any bilingual dictionary.
",5.1 Data and Evaluation Setup,[0],[0]
"The monolingual corpora are parsed using Yara Parser (Rasooli and Tetreault, 2015), trained on the corresponding treebank from the Universal Dependency Treebank (McDonald et al., 2013) (UDT-v1.4).",5.1 Data and Evaluation Setup,[0],[0]
"Yara Parser was
chosen as it is fast, and competitive with stateof-the-art parsers (Choi et al., 2015).",5.1 Data and Evaluation Setup,[0],[0]
"The monolingual corpora was POS-tagged using TurboTagger (Martins et al., 2013).",5.1 Data and Evaluation Setup,[0],[0]
"We induce dependency contexts for words by first thresholding the language vocabulary to the top 50,000 nouns, verbs and adjectives.",5.1 Data and Evaluation Setup,[0],[0]
A co-occurrence matrix is computed over this vocabulary using the context types in §3.1.,5.1 Data and Evaluation Setup,[0],[0]
"Inducing Dependency Contexts The entries of the word-context co-occurrence matrix are reweighted using Positive Pointwise Mutual Information (Bullinaria and Levy, 2007).",5.1 Data and Evaluation Setup,[0],[0]
"The resulting matrix is reduced to 1000 dimensions using SVD (Golub and Kahan, 1965).6 These vectors are used as Xe,Xf in the setup from §3.3 to generate 100 dimensional sparse bilingual vectors.
",5.1 Data and Evaluation Setup,[0],[0]
"Evaluation We use accuracy as our evaluation metric, as it is easy to interpret when the classes are balanced (Turney and Mohammad, 2015).",5.1 Data and Evaluation Setup,[0],[0]
Both evaluation datasets – HYPER-HYPO and HYPER-COHYPO – are split into 1:2 dev/test splits.,5.1 Data and Evaluation Setup,[0],[0]
"BalAPinc has two tunable parameters - 1) a threshold that indicates the BalAPinc score above which all examples are labeled as positive, 2) the maximum number of features to consider for each word.",5.1 Data and Evaluation Setup,[0.950613404029519],['Figure 4 shows the negative test log-likelihood of each method as a function of the training time on the Satellite dataset (EP results are not shown since it performs equal to SEP).']
We use the tuning set to tune the two parameters as well as the various hyper-parameters associated with the models.,5.1 Data and Evaluation Setup,[0],[0]
"We compare our BISPARSE-DEP embeddings with the following approaches:
MONO-DEP (Translation baseline) For word pair (pf , qe) in test data, we translate pf to English using the most common translation in the translation matrix.",5.2 Contrastive Approaches,[0],[0]
"Hypernymy is then determined using sparse, dependency based embeddings in English.
",5.2 Contrastive Approaches,[0],[0]
"BISPARSE-LEX (Window context) Predecessor of the BISPARSE-DEP model from our previous work (Vyas and Carpuat, 2016).",5.2 Contrastive Approaches,[0],[0]
"This model induces sparse, cross-lingual embeddings using window based context.
BIVEC+ (Window context)",5.2 Contrastive Approaches,[0],[0]
Our extension of the BIVEC model of Luong et al. (2015).,5.2 Contrastive Approaches,[0],[0]
"BIVEC generates dense, cross-lingual embeddings using window based context, by substituting aligned word pairs within a window in parallel sentences.",5.2 Contrastive Approaches,[0],[0]
"By default, BIVEC only trains using parallel data,
6Chosen based on preliminary experiments with {500,1000,2000,3000} dimensional vectors for En-Fr.
and so we initialize it with monolingually trained window based embeddings to ensure fair comparison.
",5.2 Contrastive Approaches,[0],[0]
CL-DEP (Dependency context),5.2 Contrastive Approaches,[0],[0]
"The model from Vulić (2017), which induces dense, dependency based cross-lingual embeddings by translating syntactic word-context pairs using the most common translation, and jointly training a word2vecf7 model for both languages.",5.2 Contrastive Approaches,[0],[0]
Vulić (2017) showed improvements for word similarity and bilingual lexicon induction.,5.2 Contrastive Approaches,[0],[0]
"We report the first results using CL-DEP on this task.
",5.2 Contrastive Approaches,[0],[0]
"5.3 Evaluating Robustness of BISPARSE-DEP
We investigate how robust BISPARSE-DEP is when exposed to data scarce settings.",5.2 Contrastive Approaches,[0],[0]
Evaluating on a truly low resource language is complicated by the fact that obtaining an evaluation dataset for such a language is difficult.,5.2 Contrastive Approaches,[0],[0]
"Therefore, we simulate such settings for the languages in our dataset in multiple ways.
",5.2 Contrastive Approaches,[0],[0]
No Treebank,5.2 Contrastive Approaches,[0],[0]
"If a treebank is not available for a language, dependency contexts have to be induced using treebanks from other languages (§3.2), which can affect the quality of the dependencybased embeddings.",5.2 Contrastive Approaches,[0],[0]
"To simulate this, we train a delexicalized parser for the languages in our dataset.",5.2 Contrastive Approaches,[0],[0]
"We use treebanks from Slovenian, Ukrainian, Serbian, Polish, Bulgarian, Slovak and Czech (40k sentences) for training the Russian parser, and treebanks from English, Spanish, German, Portuguese, Swedish and Italian (66k sentences) for training the French parser.",5.2 Contrastive Approaches,[0],[0]
"UDT does not (yet) have languages in the same family as Arabic or Chinese, so for the sake of completeness, we train Arabic and Chinese parsers on delexicalized treebanks of the language itself.",5.2 Contrastive Approaches,[0],[0]
"Af-
7bitbucket.org/yoavgo/word2vecf/
ter delexicalized training, the Labeled Attachment Score (LAS) on the UDT test set dropped by several points for all languages – from 76.6% to 60.0% for Russian, 83.7% to 71.1% for French, from 76.3% to 62.4% for Arabic and from 80.3% to 53.3% for Chinese.",5.2 Contrastive Approaches,[0],[0]
"The monolingual corpora are then parsed with these weaker parsers, and coocurrences and dependency contexts are computed as before.
",5.2 Contrastive Approaches,[0],[0]
"Subsampling Monolingual Data To simulate low-resource behavior along another axis, we subsample the monolingual corpora used by BISPARSE-DEP to induce monolingual vectors, Xe,Xf .",5.2 Contrastive Approaches,[0],[0]
"Specifically, we learn Xe and Xf using progressively smaller corpora.
",5.2 Contrastive Approaches,[0],[0]
Quality of Bilingual Dictionary We study the impact of the quality of the bilingual dictionary used to create the translation matrix S.,5.2 Contrastive Approaches,[0],[0]
This experiment involves using increasingly smaller parallel corpora to induce the translation dictionary.,5.2 Contrastive Approaches,[0],[0]
We aim to answer the following questions – (a) Are dependency based embeddings superior to window based embeddings for identifying crosslingual hypernymy?,6 Experiments,[0],[0]
(§6.1) (b) Does directionality in the dependency context help cross-lingual hypernymy identification?,6 Experiments,[0],[0]
(§6.2) (c) Are our models robust in data scarce settings (§6.3)?,6 Experiments,[0],[0]
(d) Is the answer to (a) predicated on the choice of entailment scorer?,6 Experiments,[0],[0]
(§6.4)?,6 Experiments,[0],[0]
We compare the performance of models described in §5.2 with the BISPARSE-DEP (FULL and JOINT) models.,6.1 Dependency v/s Window Contexts,[0],[0]
"We evaluate the models on the two test splits described in §4.2 – HYPERHYPO and HYPER-COHYPO.
",6.1 Dependency v/s Window Contexts,[0],[0]
Hyper-Hypo Results Table 3a shows the results on HYPER-HYPO.,6.1 Dependency v/s Window Contexts,[0],[0]
"First, the benefit of crosslingual modeling (as opposed to translation) is evident in that almost all models (except CL-DEP on French) outperform the translation baseline.",6.1 Dependency v/s Window Contexts,[0],[0]
"Among dependency based models, BISPARSEDEP (FULL) and CL-DEP consistently outperform both window models, while BISPARSE-DEP (JOINT) outperforms them on all except Russian.",6.1 Dependency v/s Window Contexts,[0],[0]
"BISPARSE-DEP (JOINT) is the best model overall for two languages (French and Chinese), CL-DEP for one (Arabic), with no statistically significant differences between BISPARSE-DEP (JOINT) and CL-DEP for Russian.",6.1 Dependency v/s Window Contexts,[0],[0]
"This confirms that dependency context is more useful than window context for cross-lingual hypernymy detection.
",6.1 Dependency v/s Window Contexts,[0],[0]
"Hyper-Cohypo Results The trends observed on HYPER-HYPO also hold on HYPER-COHYPO i.e. dependency based models continue to outperform window based models (Table 3b).
",6.1 Dependency v/s Window Contexts,[0],[0]
"Overall, BISPARSE-DEP (FULL) performs best in this setting, followed closely by BISPARSEDEP (JOINT).",6.1 Dependency v/s Window Contexts,[0],[0]
"This suggests that the sibling information encoded in JOINT is useful to distinguish hypernyms from hyponyms (HYPER-HYPO results), while the dependency labels encoded in FULL help to distinguish hypernyms from cohyponyms.",6.1 Dependency v/s Window Contexts,[0],[0]
"Also note that all models improve significantly on the HYPER-COHYPO set, suggesting that discriminating hypernyms from cohyponyms is easier than discriminating them from hyponyms.
",6.1 Dependency v/s Window Contexts,[0],[0]
"While the BISPARSE-DEP models were generally performing better than window models on both test sets, CL-DEP was not as consistent (e.g.,
it was worse than the best window model on HYPER-COHYPO).",6.1 Dependency v/s Window Contexts,[0],[0]
"As shown by Turney and Mohammad (2015), BalAPinc is designed for sparse embeddings and is likely to perform poorly with dense embeddings.",6.1 Dependency v/s Window Contexts,[0],[0]
"This explains the relatively inconsistent performance of CL-DEP.
",6.1 Dependency v/s Window Contexts,[0],[0]
"Besides establishing the challenging nature of our crowd-sourced set, the experiments on HYPER-COHYPO and HYPER-HYPO also demonstrate the ability of the BISPARSE-DEP models to discriminate between different lexical semantic relations (viz. hypernymy and cohyponymy) in a cross-lingual setting.",6.1 Dependency v/s Window Contexts,[0],[0]
We will investigate this ability more carefully in future work.,6.1 Dependency v/s Window Contexts,[0],[0]
"The context described by the FULL and JOINT BISPARSE models encodes directional information (§3.1) either in the form of label direction (FULL), or using sibling information (JOINT).",6.2 Ablating Directionality in Context,[0],[0]
Does such directionality in the context help to capture the asymmetric relationship inherent to hypernymy?,6.2 Ablating Directionality in Context,[0],[0]
"To answer this, we evaluate a third BISPARSE-DEP model which uses UNLABELED dependency contexts.",6.2 Ablating Directionality in Context,[0],[0]
"This is similar to the FULL context, except we do not concatenate the label of the relation to the context word (parent or children).",6.2 Ablating Directionality in Context,[0],[0]
"For instance, for traveler in Fig. 2, contexts will be roamed and tired.
",6.2 Ablating Directionality in Context,[0],[0]
"Experiments on both HYPER-HYPO and HYPER-COHYPO (bottom row, Tables 3a and 3b) highlight that directional information is indeed essential - UNLABELED almost always performs worse than FULL and JOINT, and in many cases worse than even window based models.
",6.2 Ablating Directionality in Context,[0],[0]
"6.3 Evaluating Robustness of BISPARSE-DEP
No Treebank We run experiments (Table 4) for all languages with a version of BISPARSE-DEP that use the FULL context type for both English and the non-English (target) language, but the target language contexts are derived from a corpus parsed using a delexicalized parser (§5.3).
",6.2 Ablating Directionality in Context,[0],[0]
This model compares favorably on all language pairs against the best window based and the best dependency based model.,6.2 Ablating Directionality in Context,[0],[0]
"In fact, it almost consistently outperforms the best window based model by several points, and is only slightly worse than the best dependency-based model.
",6.2 Ablating Directionality in Context,[0],[0]
Further analysis revealed that the good performance of the delexicalized model is due to the relative robustness of the delexicalized parser on frequent contexts in the co-occurrence matrix.,6.2 Ablating Directionality in Context,[0],[0]
"Specifically, we found that in French and Russian, the most frequent contexts were derived from amod, nmod, nsubj and dobj edges.8",6.2 Ablating Directionality in Context,[0],[0]
"For instance, the nmod edge appears in 44% of Russian contexts and 33% of the French contexts.",6.2 Ablating Directionality in Context,[0],[0]
The delexicalized parser predicts both the label and direction of the nmod edge correctly with an F1 of 68.6 for Russian and 69.6 for French.,6.2 Ablating Directionality in Context,[0],[0]
"In contrast, a fully-trained parser achieves a F1 of 76.7 for Russian and 76.8 for French for the same edge.
",6.2 Ablating Directionality in Context,[0],[0]
"Small Monolingual Corpus In Figure 4, we use increasingly smaller monolingual corpora (10%, 20%, 40%, 60% and 80%) sampled at random to induce the monolingual vectors for BISPARSEDEP (FULL) model.",6.2 Ablating Directionality in Context,[0],[0]
"Trends (Figure 4) indicate that BISSPARSE-DEP models that use only 40% of the original data remain competitive with the BISSPARSE-LEX model that has access to the full
8Together they make up at least 70% of the contexts.
data.",6.2 Ablating Directionality in Context,[0],[0]
"Robust performance with smaller monolingual corpora is helpful since large-enough monolingual corpora are not always easily available.
",6.2 Ablating Directionality in Context,[0],[0]
Quality of Bilingual Dictionary Bilingual dictionaries derived from smaller amounts of parallel data are likely to be of lower quality than those derived from larger corpora.,6.2 Ablating Directionality in Context,[0],[0]
"Hence, to analyze the impact of dictionary quality on BISPARSE-DEP (FULL), we use increasingly smaller parallel corpora to induce bilingual dictionaries used as the score matrix S (§3.3).",6.2 Ablating Directionality in Context,[0],[0]
"We use the top 10%, 20%, 40%, 60% and 80% sentences from the parallel corpora.",6.2 Ablating Directionality in Context,[0],[0]
"The trends in Figure 4 show that even with a lower quality dictionary, BISPARSE-DEP performs better than BISPARSE-LEX.",6.2 Ablating Directionality in Context,[0],[0]
"We change the entailment scorer from BalAPinc to SLQS (Santus et al., 2014) and redo experiments from §6.1 to see if the conclusions drawn depend
on the choice of the entailment scorer.",6.4 Choice of Entailment Scorer,[0],[0]
"SLQS is based on the distributional informativeness hypothesis, which states that hypernyms are less “informative” than hyponyms, because they occur in more general contexts.",6.4 Choice of Entailment Scorer,[0],[0]
"The informativeness Eu of a word u is defined to be the median entropy of its top N dimensions, Eu = medianNk=1H(ck), where H(ci) denotes the entropy of dimension ci.",6.4 Choice of Entailment Scorer,[0],[0]
"The SLQS score for a pair (u, v) is the relative difference in entropies,
SLQS(u→ v) = 1− Eu Ev
Recent work (Shwartz et al., 2017) has found SLQS to be more successful than other metrics in monolingual hypernymy detection.
",6.4 Choice of Entailment Scorer,[0],[0]
The trends observed in these experiments are consistent with those in §6.1 – both BISPARSEDEP models still outperform window-based models.,6.4 Choice of Entailment Scorer,[0],[0]
"Also, the delexicalized version of BISPARSEDEP outperforms the window-based models, showing that the robust behavior demonstrated in §6.3 is also invariant across metrics.
",6.4 Choice of Entailment Scorer,[0],[0]
We also found that using BalAPinc led to better results than SLQS .,6.4 Choice of Entailment Scorer,[0],[0]
"For both BISPARSE-DEP models, BalAPinc wins across the board for two languages (Russian and Chinese), and wins half the time for the other two languages compared to SLQS .",6.4 Choice of Entailment Scorer,[0],[0]
We leave detailed comparison of these and other scores to future work.,6.4 Choice of Entailment Scorer,[0],[0]
"We introduced BISPARSE-DEP, a new distributional approach for identifying cross-lingual hypernymy, based on cross-lingual embeddings derived from dependency contexts.",7 Conclusion,[0],[0]
"We showed that using BISPARSE-DEP is superior for the crosslingual hypernymy detection task, when compared to standard window based models and a translation baseline.",7 Conclusion,[0],[0]
Further analysis also showed that BISPARSE-DEP is robust to various low-resource settings.,7 Conclusion,[0],[0]
"In principle, BISPARSE-DEP can be used for any language that has a bilingual dictionary with English and a “related” language with a treebank.",7 Conclusion,[0],[0]
"We also introduced crowd-sourced crosslingual hypernymy datasets for four languages for future evaluations.
",7 Conclusion,[0],[0]
"Our approach has the potential to complement existing work on creating cross-lingual ontologies such as BabelNet and the Open Multilingual Wordnet, which are noisy because they are compiled semi-automatically, and have limited language coverage.",7 Conclusion,[0],[0]
"In general, distributional approaches can help refine ontology construction for
any language where sufficient resources are available.
",7 Conclusion,[0],[0]
It remains to be seen how our approach performs for other language pairs beyond simluated low-resource settings.,7 Conclusion,[0],[0]
"We anticipate that replacing our delexicalized parser with more sophisticated transfer strategies (Rasooli and Collins, 2017; Aufrant et al., 2016) might be beneficial in such settings.",7 Conclusion,[0],[0]
"While our delexicalized parsing based approach exhibits robustness, it can benefit from more sophisticated approaches for transfer parsing (Rasooli and Collins, 2017; Aufrant et al., 2016) to improve parser performance.",7 Conclusion,[0],[0]
We aim to explore these and other directions in the future.,7 Conclusion,[0],[0]
"The authors would like to thank the members of the CLIP lab at the University of Maryland, members of the Cognitive Computation Group at the University of Pennsylvania, and the anonymous reviewers from EMNLP/CoNLL 2017 and NAACL 2018 for their constructive feedback.",Acknowledgments,[0],[0]
"YV and MC were funded in part by research awards from Amazon, Google, and the Clare Boothe Luce Foundation.",Acknowledgments,[0],[0]
SU and DR were supported by Contract HR0011-15-2-0025 with the US Defense Advanced Research Projects Agency (DARPA).,Acknowledgments,[0],[0]
Cross-lingual Hypernymy Detection involves determining if a word in one language (“fruit”) is a hypernym of a word in another language (“pomme” i.e. apple in French).,abstractText,[0],[0]
The ability to detect hypernymy cross-lingually can aid in solving cross-lingual versions of tasks such as textual entailment and event coreference.,abstractText,[0],[0]
"We propose BISPARSE-DEP, a family of unsupervised approaches for cross-lingual hypernymy detection, which learns sparse, bilingual word embeddings based on dependency contexts.",abstractText,[0],[0]
"We show that BISPARSE-DEP can significantly improve performance on this task, compared to approaches based only on lexical context.",abstractText,[0],[0]
"Our approach is also robust, showing promise for low-resource settings: our dependency-based embeddings can be learned using a parser trained on related languages, with negligible loss in performance.",abstractText,[0],[0]
"We also crowd-source a challenging dataset for this task on four languages – Russian, French, Arabic, and Chinese.",abstractText,[0],[0]
Our embeddings and datasets are publicly available.1,abstractText,[0],[0]
Robust Cross-lingual Hypernymy Detection using Dependency Context,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2137–2147 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
2137",text,[0],[0]
Relation extraction is a core task in information extraction and natural language understanding.,1 Introduction,[0],[0]
"The goal of relation extraction is to predict relations for entities in a sentence (Zelenko et al., 2003; Bunescu and Mooney, 2005; GuoDong et al., 2005).",1 Introduction,[0],[0]
"For example, given a sentence
“Barack Obama is married to Michelle Obama.”, a relation classifier aims at predicting the relation of “spouse”.",1 Introduction,[0],[0]
"In downstream applications, relation extraction is the key module for constructing knowledge graphs, and it is a vital component of many natural language processing applications such as structured search, sentiment analysis, question answering, and summarization.
",1 Introduction,[0],[0]
"A major issue encountered in the early development of relation extraction algorithms is the data sparsity issue—It is extremely expensive, and almost impossible for human annotators to go through a large corpus of millions of sentences to provide a large amount of labeled training instances.",1 Introduction,[0],[0]
"Therefore, distant supervision relation extraction (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012) becomes popular, because it uses entity pairs from knowledge bases to select a set of noisy instances from unlabeled data.",1 Introduction,[0],[0]
"In recent years, neural network approaches (Zeng et al., 2014, 2015) have been proposed to train the relation extractor under these noisy conditions.",1 Introduction,[0],[0]
"To suppress the noisy(Roth et al., 2013), recent stud-
ies (Lin et al., 2016) have proposed the use of attention mechanisms to place soft weights on a set of noisy sentences, and select samples.",1 Introduction,[0],[0]
"However, we argue that only selecting one example or based on soft attention weights are not the optimal strategy: To improve the robustness, we need a systematic solution to make use of more instances, while removing false positives and placing them in the right place.
",1 Introduction,[0],[0]
"In this paper, we investigate the possibility of using dynamic selection strategies for robust distant supervision.",1 Introduction,[0],[0]
"More specifically, we design a deep reinforcement learning agent, whose goal is to learn to choose whether to remove or remain the distantly supervised candidate instance based on the performance change of the relation classifier.",1 Introduction,[0],[0]
"Intuitively, our agent would like to remove false positives, and reconstruct a cleaned set of distantly supervised instances to maximize the reward based on the classification accuracy.",1 Introduction,[0],[0]
"Our proposed method is classifier-independent, and it can be applied to any existing distant supervision model.",1 Introduction,[0],[0]
"Empirically, we show that our method has brought consistent performance gains in various deep neural network based models, achieving strong performances on the widely used New York Times dataset (Riedel et al., 2010).",1 Introduction,[0],[0]
"Our contributions are three-fold:
• We propose a novel deep reinforcement learning framework for robust distant supervision relation extraction.
",1 Introduction,[0],[0]
"• Our method is model-independent, meaning that it could be applied to any state-of-the-art relation extractors.
",1 Introduction,[0],[0]
"• We show that our method can boost the performances of recently proposed neural relation extractors.
",1 Introduction,[0],[0]
"In Section 2, we will discuss related works on distant supervision relation extraction.",1 Introduction,[0],[0]
"Next, we will describe our robust distant supervision framework in Section 3.",1 Introduction,[0],[0]
"In Section 4, empirical evaluation results are shown.",1 Introduction,[0],[0]
"And finally, we conclude in Section 5.",1 Introduction,[0],[0]
Mintz et al. (2009) is the first study that combines dependency path and feature aggregation for distant supervision.,2 Related Work,[0],[0]
"However, this approach would
introduce a lot of false positives, as the same entity pair might have multiple relations.",2 Related Work,[0],[0]
"To alleviate this issue, Hoffmann et al. (2011) address this issue, and propose a model to jointly learn with multiple relations.",2 Related Work,[0],[0]
Surdeanu et al. (2012) further propose a multi-instance multi-label learning framework to improve the performance.,2 Related Work,[0],[0]
"Note that these early approaches do not explicitly remove noisy instances, but rather hope that the model would be able to suppress the noise.
",2 Related Work,[0],[0]
"Recently, with the advance of neural network techniques, deep learning methods (Zeng et al., 2014, 2015) are introduced, and the hope is to model noisy distant supervision process in the hidden layers.",2 Related Work,[0],[0]
"However, their approach only selects one most plausible instance per entity pair, inevitably missing out a lot of valuable training instances.",2 Related Work,[0],[0]
"Recently, Lin et al. (2016) propose an attention mechanism to select plausible instances from a set of noisy instances.",2 Related Work,[0],[0]
"However, we believe that soft attention weight assignment might not be the optimal solution, since the false positives should be completely removed and placed in the negative set.",2 Related Work,[0],[0]
"Ji et al. (2017) combine the external knowledge to rich the representation of entity pair, in which way to improve the accuracy of attention weights.",2 Related Work,[0],[0]
"Even though these above-mentioned methods can select high-quality instances, they ignore the false positive case: all the sentences of one entity pair belongs to the false positives.",2 Related Work,[0],[0]
"In this work, we take a radical approach to solve this problem—We will make use of the distantly labeled resources as much as possible, while learning a independent false-positive indicator to remove false positives, and place them in the right place.",2 Related Work,[0],[0]
"After our ACL submission, we notice that a contemporaneous study Feng et al. (2018) also adopts reinforcement learning to learn an instance selector, but their reward is calculated from the prediction probabilities.",2 Related Work,[0],[0]
"In contrast, while in our method, the reward is intuitively reflected by the performance change of the relation classifier.",2 Related Work,[0],[0]
"Our approach is also complement to most of the approaches above, and can be directly applied on top of any existing relation extraction classifiers.",2 Related Work,[0],[0]
"We introduce a performance-driven, policy-based reinforcement learning method to heuristically recognize false positive samples.",3 Reinforcement Learning for Distant Supervision,[0],[0]
"Comparing to
a prior study that has underutilized the distantlysupervised samples (Lin et al., 2016), we consider an RL agent for robust distant supervision relation extraction.",3 Reinforcement Learning for Distant Supervision,[0],[0]
"We first describe the definitions of our RL method, including the policy-based agent, external environment, and pre-training strategy.",3 Reinforcement Learning for Distant Supervision,[0],[0]
"Next, we describe the retraining strategy for our RL agent.",3 Reinforcement Learning for Distant Supervision,[0],[0]
"The goal of our agent is to determine whether to retain or remove a distantlysupervised sentence, based on the performance change of relation classifier.",3 Reinforcement Learning for Distant Supervision,[0],[0]
"Finally, we describe the noisy-suppression method, where we teach our policy-based agent to make a redistribution for a cleaner distant supervision training dataset.
",3 Reinforcement Learning for Distant Supervision,[0],[0]
Distant supervision relation extraction is to predict the relation type of entity pair under the automatically-generated training set.,3 Reinforcement Learning for Distant Supervision,[0],[0]
"However, the issue is that these distantly-supervised sentences that mention this entity pair may not express the desired relation type.",3 Reinforcement Learning for Distant Supervision,[0],[0]
"Therefore, what our RL agent should do is to determine whether the distantly-supervised sentence is a true positive instance for this relation type.",3 Reinforcement Learning for Distant Supervision,[0],[0]
"For reinforcement learning, external environment and RL agent are two necessary components, and a robust agent is trained from the dynamic interaction between these two parts (Arulkumaran et al., 2017).",3 Reinforcement Learning for Distant Supervision,[0],[0]
"First, the prerequisite of reinforcement learning is that the external environment should be modeled as a Markov decision process (MDP).",3 Reinforcement Learning for Distant Supervision,[0],[0]
"However, the traditional setting of relation extraction cannot satisfy this condition: the input sentences are independent of each other.",3 Reinforcement Learning for Distant Supervision,[0],[0]
"In other words, we cannot merely use the information of the sentence being processed as the state.",3 Reinforcement Learning for Distant Supervision,[0],[0]
"Thus, we add the information from the early states into the representation of the current state, in which way to model our task as a MDP problem (Fang et al., 2017).",3 Reinforcement Learning for Distant Supervision,[0],[0]
"The other component, RL agent, is parameterized with a policy network πθ(s, a) = p(a|s; θ).",3 Reinforcement Learning for Distant Supervision,[0],[0]
"The probability distribution of actions A = {aremove, aremain} is calculated by policy network based on state vectors.",3 Reinforcement Learning for Distant Supervision,[0],[0]
"What needs to be noted is that, Deep Q Network (DQN) (Mnih et al., 2013) is also a widelyused RL method; however, it is not suitable for our case, even if our action space is small.",3 Reinforcement Learning for Distant Supervision,[0],[0]
"First, we cannot compute the immediate reward for every operation; In contrast, the accurate reward can only be obtained after finishing processing the whole training dataset.",3 Reinforcement Learning for Distant Supervision,[0],[0]
"Second, the stochastic policy of the policy network is capable of prevent-
ing the agent from getting stuck in an intermediate state.",3 Reinforcement Learning for Distant Supervision,[0],[0]
"The following subsections detailedly introduce the definitions of the fundamental components in the proposed RL method.
",3 Reinforcement Learning for Distant Supervision,[0],[0]
States,3 Reinforcement Learning for Distant Supervision,[0],[0]
"In order to satisfy the condition of MDP, the state s includes the information from the current sentence and the sentences that have been removed in early states.",3 Reinforcement Learning for Distant Supervision,[0],[0]
The semantic and syntactic information of sentence is represented by a continuous real-valued vector.,3 Reinforcement Learning for Distant Supervision,[0],[0]
"According to some state-of-the-art supervised relation extraction approaches (Zeng et al., 2014; Nguyen and Grishman, 2015), we utilize both word embedding and position embedding to convert sentence into vector.",3 Reinforcement Learning for Distant Supervision,[0],[0]
"With this sentence vector, the current state is the concatenation of the current sentence vector and the average vector of the removed sentences in early states.",3 Reinforcement Learning for Distant Supervision,[0],[0]
"We give relatively larger weight for the vector of the current sentence, in which way to magnify the dominating influence of the current sentence information for the decision of action.
",3 Reinforcement Learning for Distant Supervision,[0],[0]
"Actions At each step, our agent is required to determine whether the instance is false positive for target relation type.",3 Reinforcement Learning for Distant Supervision,[0],[0]
Each relation type has a agent1.,3 Reinforcement Learning for Distant Supervision,[0],[0]
There are two actions for each agent: whether to remove or retain the current instance from the training set.,3 Reinforcement Learning for Distant Supervision,[0],[0]
"With the initial distantlysupervised dataset that is blended with incorrectlylabeled instances, we hope that our agent is capable of using the policy network to filter noisy instances; Under this cleaned dataset, distant supervision is then expected to achieve better performance.
",3 Reinforcement Learning for Distant Supervision,[0],[0]
"Rewards As previously mentioned, the intuition of our model is that, when the incorrectly-labeled instances are filtered, the better performance of relation classifier will achieve.",3 Reinforcement Learning for Distant Supervision,[0],[0]
"Therefore, we use the change of performance as the result-driven reward for a series of actions decided by the agent.",3 Reinforcement Learning for Distant Supervision,[0],[0]
"Compared to accuracy, we adopt the F1 score as the evaluation criterion, since accuracy might not be an indicative metric in a multi-class classification setting where the data distribution could be imbalanced.",3 Reinforcement Learning for Distant Supervision,[0],[0]
"Thus, the reward can be formulated as the
1We also tried the strategy that just builds a single agent for all relation types: a binary classifier(TP/FP) or a multiclass classifier(rela1/rela2/.../FP).",3 Reinforcement Learning for Distant Supervision,[0],[0]
"But, it has the limitation in the performance.",3 Reinforcement Learning for Distant Supervision,[0],[0]
"We found that our one-agent-for-onerelation strategy obtained better performance than the single agent strategy.
v ; their corresponding negative
part are represented asNorit andN ori v .",3 Reinforcement Learning for Distant Supervision,[0],[0]
"In each epoch i, the agent performs a series of actions to recognize the false positive samples from P orit and treat them as negative samples.",3 Reinforcement Learning for Distant Supervision,[0],[0]
"Then, a new relation classifier is trained under the new dataset {P it , N it}.",3 Reinforcement Learning for Distant Supervision,[0],[0]
"With this relation classifier, F1 score is calculated from the new validation set {P iv, N iv}, where P iv is also filtered by the current agent.",3 Reinforcement Learning for Distant Supervision,[0],[0]
"After that, the current reward is measured as the difference of F1 between the adjacent epochs.
",3 Reinforcement Learning for Distant Supervision,[0],[0]
"difference between the adjacent epochs:
Ri = α(F",3 Reinforcement Learning for Distant Supervision,[0],[0]
i 1,3 Reinforcement Learning for Distant Supervision,[0],[0]
"− F i−11 ) (1)
",3 Reinforcement Learning for Distant Supervision,[0],[0]
"As this equation shows, in step i, our agent is given a positive reward only if F1 gets improved; otherwise, the agent will receive a negative reward.",3 Reinforcement Learning for Distant Supervision,[0],[0]
"Under this setting, the value of reward is proportional to the difference of F1, and α is used to convert this difference into a rational numeric range.",3 Reinforcement Learning for Distant Supervision,[0],[0]
"Naturally, the value of the reward is in a continuous space, which is more reasonable than a binary reward (−1 and 1), because this setting can reflect the number of wrong-labeled instance that the agent has removed.",3 Reinforcement Learning for Distant Supervision,[0],[0]
"In order to avoid the randomness of F1, we use the average F1 of last five epochs to calculate the reward.
",3 Reinforcement Learning for Distant Supervision,[0],[0]
"Policy Network For each input sentence, our policy network is to determine whether it expresses the target relation type and then make removal action if it is irrelevant to the target relation type.",3 Reinforcement Learning for Distant Supervision,[0],[0]
"Thus, it is analogous to a binary relation classifier.",3 Reinforcement Learning for Distant Supervision,[0],[0]
"CNN is commonly used to construct relation classification system (Santos et al., 2015; Xu et al., 2015; Shen and Huang, 2016), so we adopt a simple CNN with window size cw and kernel size ck, to model policy network π(s; θ).",3 Reinforcement Learning for Distant Supervision,[0],[0]
"The reason why we do not choice the variants of CNN (Zeng et al., 2015; Lin et al., 2016)
that are well-designed for distant supervision is that these two models belong to bag-level models (dealing with a bag of sentences simultaneously) and deal with the multi-classification problem; We just need a model to do binary sentencelevel classification.",3 Reinforcement Learning for Distant Supervision,[0],[0]
"Naturally, the simpler network is adopted.",3 Reinforcement Learning for Distant Supervision,[0],[0]
"Unlike the goal of distant supervision relation extraction, our agent is to determine whether an annotated sentence expresses the target relation type rather than predict the relationship of entity pair, so sentences are treated independently despite belonging to the same entity pair.",3.1 Training Policy-based Agent,[0],[0]
"In distant supervision training dataset, one relation type contains several thousands or ten thousands sentences; moreover, reward R can only be calculated after processing the whole positive set of this relation type.",3.1 Training Policy-based Agent,[0],[0]
"If we randomly initialize the parameters of policy network and train this network by trial and errors, it will waste a lot of time and be inclined to poor convergence properties.",3.1 Training Policy-based Agent,[0],[0]
"In order to overcome this problem, we adopt a supervised learning procedure to pre-train our policy network, in which way to provide a general learning direction for our policy-based agent.",3.1 Training Policy-based Agent,[0],[0]
"The pre-training strategy, inspired from AlphaGo (Silver et al., 2016), is a common strategy in RL related works to accelerate the training of RL agents.",3.1.1 Pre-training Strategy,[0],[0]
"Normally, they utilize a small part of the annotated dataset to train policy networks before reinforcement learning.",3.1.1 Pre-training Strategy,[0],[0]
"For example, AlphaGo uses the collected experts moves to do a supervised learning for Go RL agent.",3.1.1 Pre-training Strategy,[0],[0]
"However, in distant supervision relation extraction task, there is not any supervised information that can be used unless let linguistic experts to do some manual annotations for part of the entity pairs.",3.1.1 Pre-training Strategy,[0],[0]
"However, this is expensive, and it is not the original intention of distant supervision.",3.1.1 Pre-training Strategy,[0],[0]
"Under this circumstance, we propose a compromised solution.",3.1.1 Pre-training Strategy,[0],[0]
"With well-aligned corpus, the true positive samples should have evident advantage in quantity compared with false positive samples in the distantly-supervised dataset.",3.1.1 Pre-training Strategy,[0],[0]
"So, for a specific relation type, we directly treat the distantly-supervised positive set as the positive set, and randomly extract part of distantly-supervised negative set as the negative set.",3.1.1 Pre-training Strategy,[0],[0]
"In order to better consider prior information during this pre-training procedure, the amount of negative samples is 10 times of the number of positive samples.",3.1.1 Pre-training Strategy,[0],[0]
"It is because, when learning with massive negative samples, the agent is more likely to develop toward a better direction.",3.1.1 Pre-training Strategy,[0],[0]
"Cross-entropy cost function is used to train this binary classifier, where the negative label corresponds to the removing action, and the positive label corresponds to the retaining action.
",3.1.1 Pre-training Strategy,[0],[0]
(2) J(θ) =,3.1.1 Pre-training Strategy,[0],[0]
"∑ i yilog[π(a = yi|si; θ)]
+ (1− yi)log[1− π(a",3.1.1 Pre-training Strategy,[0],[0]
"= yi|si; θ)]
Due to the noisy nature of the distantly-labeled instances, if we let this pre-training process overfit this noisy dataset, the predicted probabilities of most samples tend to be close to 0 or 1, which is difficult to be corrected and unnecessarily increases the training cost of reinforcement learning.",3.1.1 Pre-training Strategy,[0],[0]
"So, we stop this training process when the accuracy reaches 85% ∼ 90%.",3.1.1 Pre-training Strategy,[0],[0]
"Theoretically, our approach can be explained as increasing the entropy of the policy gradient agent, and preventing the entropy of the policy being too low, which means that the lack of exploration may be a concern.",3.1.1 Pre-training Strategy,[0],[0]
"As shown in Figure 2, in order to discover incorrectly-labeled instances without any supervised information, we introduce a policy-based RL method.",3.1.2 Retraining Agent with Rewards,[0],[0]
What our agent tries to deal with is the noisy samples from the distantly-supervised positive dataset; Here we call it as the DS positive dataset.,3.1.2 Retraining Agent with Rewards,[0],[0]
"We split it into the training positive set P orit and the validation positive set P ori v ; naturally, both of these two set are noisy.",3.1.2 Retraining Agent with Rewards,[0],[0]
"Correspondingly, the training negative set Norit and the validation negative setNoriv are constructed by randomly selected from the DS negative dataset.",3.1.2 Retraining Agent with Rewards,[0],[0]
"In every epoch, the agent removes a noisy sample set Ψi from P orit according to the stochastic policy π(a|s), and we obtain a new positive set Pt = P ori",3.1.2 Retraining Agent with Rewards,[0],[0]
t,3.1.2 Retraining Agent with Rewards,[0],[0]
− Ψi.,3.1.2 Retraining Agent with Rewards,[0],[0]
"Because Ψi is recognized as the wrong-labeled samples, we redistribute it into the negative set Nt = Norit + Ψi.",3.1.2 Retraining Agent with Rewards,[0],[0]
"Under this setting, the scale of training set is constant for each epoch.",3.1.2 Retraining Agent with Rewards,[0],[0]
"Now we utilize the cleaned data {Pt, Nt} to train a relation classifier.",3.1.2 Retraining Agent with Rewards,[0],[0]
The desirable situation is that RL agent has the capacity to increase the performance of relation classifier through relocating incorrectly-labeled false positive instances.,3.1.2 Retraining Agent with Rewards,[0],[0]
"Therefore, we use the validation set {P oriv , Noriv } to measure the performance of the current agent.",3.1.2 Retraining Agent with Rewards,[0],[0]
"First, this validation set is filtered and redistributed by the current agent as {Pv, Nv}; the F1 score of the current relation classifier is calculated from it.",3.1.2 Retraining Agent with Rewards,[0],[0]
"Finally, the difference of F1 scores between the current and previous epoch is used to calculate reward.",3.1.2 Retraining Agent with Rewards,[0],[0]
"Next, we will introduce several strategies to train a more robust RL agent.
",3.1.2 Retraining Agent with Rewards,[0],[0]
"Removing the fixed number of sentences in each epoch In every epoch, we let the RL agent to remove a fixed number of sentences or less (when the number of the removed sentences in one epoch does not reach this fixed number during training), in which way to prevent the case that the agent tries to remove more false positive instances by removing more instances.",3.1.2 Retraining Agent with Rewards,[0],[0]
"Under the restriction of fixed number, if the agent decides to remove the current state, it means the chance of removing other states decrease.",3.1.2 Retraining Agent with Rewards,[0],[0]
"Therefore, in order to obtain a better reward, the agent should try to remove a instance set that includes more negative instances.
",3.1.2 Retraining Agent with Rewards,[0],[0]
Loss function The quality of the RL agent is reflected by the quality of the removed part.,3.1.2 Retraining Agent with Rewards,[0],[0]
"After the pre-training process, the agent just possesses
Algorithm 1 Retraining agent with rewards for relation k.",3.1.2 Retraining Agent with Rewards,[0],[0]
"For a clearer expression, k is omitted in the following algorithm.",3.1.2 Retraining Agent with Rewards,[0],[0]
"Require: Positive set {P orit , P oriv }, Negative set {Norit , Noriv }, the fixed number of removal γt, γv
1: Load parameters θ from pre-trained policy network 2: Initialize s∗ as the all-zero vector with the same dimension of sj 3: for epoch",3.1.2 Retraining Agent with Rewards,[0],[0]
"i = 1→ N do 4: for sj ∈ P orit do 5: s̃j = concatenation(sj , s
∗) 6: Randomly sample aj ∼ π(a|s̃j ; θ); compute pj = π(a = 0|s̃j ; θ) 7: if aj == 0 then 8: Save tuple tj = (s̃j , pj) in T and recompute the average vector of removed sentences s∗
9: end if 10: end for 11: Rank T based on pj from high to low, obtain Trank 12: for ti in Trank[: γt] do 13: Add ti[0] into Ψi 14: end for 15: P it = P ori",3.1.2 Retraining Agent with Rewards,[0],[0]
t,3.1.2 Retraining Agent with Rewards,[0],[0]
"− Ψi, N it = Norit + Ψi, and generate the new validation set {P iv, N iv} with current
agent 16: Train the relation classifier based on {P it , N it} 17: Calculate F i1 on the new validation set {P iv, N iv}, and Save F i1,",3.1.2 Retraining Agent with Rewards,[0],[0]
"Ψi 18: R = α(F i1 − F i−1 1 ) 19: Ωi−1 = Ψi−1 −Ψi ∩Ψi−1; Ωi = Ψi −Ψi ∩Ψi−1 20:
21: Updata θ:",3.1.2 Retraining Agent with Rewards,[0],[0]
"g ∝ 5θ ∑Ωi log π(a|s; θ)R+5θ∑Ωi−1 log π(a|s; θ)(−R) 22: end for
the ability to distinguish the obvious false positive instances, which means the discrimination of the indistinguishable wrong-labeled instances are still ambiguous.",3.1.2 Retraining Agent with Rewards,[0],[0]
"Particularly, this indistinguishable part is the criterion to reflect the quality of the agent.",3.1.2 Retraining Agent with Rewards,[0],[0]
"Therefore, regardless of these easydistinguished instances, the different parts of the removed parts in different epochs are the determinant of the change of F1 scores.",3.1.2 Retraining Agent with Rewards,[0],[0]
"Therefore, we definite two sets:
Ωi−1 = Ψi−1",3.1.2 Retraining Agent with Rewards,[0],[0]
− (Ψi ∩Ψi−1) (3) Ωi =,3.1.2 Retraining Agent with Rewards,[0],[0]
"Ψi − (Ψi ∩Ψi−1) (4)
where Ψi is the removed part of epoch i. Ωi−1 and Ωi are represented with the different colors in Figure 2.",3.1.2 Retraining Agent with Rewards,[0],[0]
"If F1 score increases in the epoch i, it means the actions of the epoch i is more reasonable than that in the epoch i− 1.",3.1.2 Retraining Agent with Rewards,[0],[0]
"In other words, Ωi is more negative than Ωi−1.",3.1.2 Retraining Agent with Rewards,[0],[0]
"Thus, we assign the positive reward to Ωi and the negative reward to Ωi−1, and vice versa.",3.1.2 Retraining Agent with Rewards,[0],[0]
"In summary, the ultimate loss function
is formulated as follow:
(5) J(θ)",3.1.2 Retraining Agent with Rewards,[0],[0]
"=
Ωi∑ log π(a|s; θ)R
+ Ωi−1∑ log π(a|s; θ)(−R)",3.1.2 Retraining Agent with Rewards,[0],[0]
"Through the above reinforcement learning procedure, for each relation type, we obtain a agent as the false-positive indicator.",3.2 Redistributing Training Dataset with Policy-based Agents,[0],[0]
These agents possess the capability of recognizing incorrectly-labeled instances of the corresponding relation types.,3.2 Redistributing Training Dataset with Policy-based Agents,[0],[0]
We adopt these agents as classifiers to recognize false positive samples in the noisy distantly-supervised training dataset.,3.2 Redistributing Training Dataset with Policy-based Agents,[0],[0]
"For one entity pair, if all the sentence aligned from corpus are classified as false positive, then this entity pair is redistributed into the negative set.",3.2 Redistributing Training Dataset with Policy-based Agents,[0],[0]
"We adopt a policy-based RL method to generate a series of relation indicators and use them to re-
distribute training dataset by moving false positive samples to negative sample set.",4 Experiments,[0],[0]
"Therefore, our experiments are intended to demonstrate that our RL agents possess this capability.",4 Experiments,[0],[0]
"We evaluate the proposed method on a commonlyused dataset2, which is first presented in Riedel et al. (2010).",4.1 Datast and Evaluation Metrics,[0],[0]
This dataset is generated by aligning entity pairs from Freebase with New York Times corpus(NYT).,4.1 Datast and Evaluation Metrics,[0],[0]
"Entity mentions of NYT corpus are recognized by the Stanford named entity recognizer (Finkel et al., 2005).",4.1 Datast and Evaluation Metrics,[0],[0]
The sentences from the years 2005-2006 are used as the training corpus and sentences from 2007 are used as the testing corpus.,4.1 Datast and Evaluation Metrics,[0],[0]
There are 52 actual relations and a special relation NA which indicates there is no relation between the head and tail entities.,4.1 Datast and Evaluation Metrics,[0],[0]
"The sentences of NA are from the entity pairs that exist in the same sentence of the actual relations but do not appear in the Freebase.
",4.1 Datast and Evaluation Metrics,[0],[0]
"Similar to the previous works, we adopt the held-out evaluation to evaluate our model, which can provide an approximate measure of the classification ability without costly human evaluation.",4.1 Datast and Evaluation Metrics,[0],[0]
"Similar to the generation of the training set, the entity pairs in test set are also selected from Freebase, which will be predicted under the sentences discovered from the NYT corpus.",4.1 Datast and Evaluation Metrics,[0],[0]
The action space of our RL agent just includes two actions.,4.2.1 Policy-based Agent,[0],[0]
"Therefore, the agent can be modeled as a binary classifier.",4.2.1 Policy-based Agent,[0],[0]
We adopt a single-window CNN as this policy network.,4.2.1 Policy-based Agent,[0],[0]
The detailed hyperparameter settings are presented in Table 1.,4.2.1 Policy-based Agent,[0],[0]
"As for word embeddings, we directly use the word embedding file released by Lin et al. (2016)3, which just keeps the words that appear more than 100 times in NYT.",4.2.1 Policy-based Agent,[0],[0]
"Moreover, we have the same dimension setting of the position embedding, and the maximum length of relative distance is −30 and 30 (“-” and “+” represent the left and right side of the entities).",4.2.1 Policy-based Agent,[0],[0]
The learning rate of reinforcement learning is 2e−5.,4.2.1 Policy-based Agent,[0],[0]
"For each relation type, the fixed number γt, γv are according to the pre-trained agent.",4.2.1 Policy-based Agent,[0],[0]
"When one relation type has too many distantsupervised positive sentences (for example, /lo-
2http://iesl.cs.umass.edu/riedel/ecml/ 3https://github.com/thunlp/NRE
cation/location/contains has 75768 sentences), we sample a subset of size 7,500 sentences to train the agent.",4.2.1 Policy-based Agent,[0],[0]
"For the average vector of the removed sentences, in the pre-training process and the first state of the retraining process, it is set as all-zero vector.",4.2.1 Policy-based Agent,[0],[0]
"In order to evaluate a series of actions by agent, we use a simple CNN model, because the simple network is more sensitive to the quality of the training set.",4.2.2 Relation Classifier for Calculating Reward,[0],[0]
"The proportion between P orit and P ori v is 2:1, and they are all derived from the training set of Riedel dataset; the corresponding negative sample setsNorit andN ori v are randomly selected from the Riedel negative dataset, whose size is twice that of their corresponding positive sets.",4.2.2 Relation Classifier for Calculating Reward,[0],[0]
"In Table 2, we list the F1 scores before and after adopting the proposed RL method.",4.3 The Effectiveness of Reinforcement Learning,[0],[0]
"Even though there are 52 actual relation types in Riedel dataset, only 10 relation types have more than 1000 pos-
itive instances4.",4.3 The Effectiveness of Reinforcement Learning,[0],[0]
"Because of the randomness of deep neural network on the small-scale dataset, we just train policy-based agents for these 10 relation types.",4.3 The Effectiveness of Reinforcement Learning,[0],[0]
"First, compared with Original case, most of the Pretrain agents yield obvious improvements: It not only demonstrates the rationality of our pretraining strategy, but also verifies our hypothesis that most of the positive samples in Riedel dataset are true positive.",4.3 The Effectiveness of Reinforcement Learning,[0],[0]
"More significantly, after retraining with the proposed policy-based RL method, the F1 scores achieve further improvement, even for the case the Pretrain agents perform bad.",4.3 The Effectiveness of Reinforcement Learning,[0],[0]
"These comparable results illustrate that the proposed policy-based RL method is capable of making agents develop towards a good direction.
4The supervised relation classification task Semeval-2010 Task 8 (Hendrickx et al., 2009) annotates nearly 1,000 instances for each relation type.",4.3 The Effectiveness of Reinforcement Learning,[0],[0]
Zeng et al. (2015) and Lin et al. (2016) are both the robust models to solve wrong labeling problem of distant supervision relation extraction.,4.4 Impact of False Positive Samples,[0],[0]
"Zeng et al. (2015) combine at-least-one multi-instance learning with deep neural network to extract only one active sentence to predict the relation between entity pair; Lin et al. (2016) combine all sentences of one entity pair and assign soft attention weights to them, in which way to generate a compositive relation representation for this entity pair.",4.4 Impact of False Positive Samples,[0],[0]
"However, the false positive phenomenon also includes the case that all the sentences of one entity pair are wrong, which is because the corpus is not completely aligned with the knowledge base.",4.4 Impact of False Positive Samples,[0],[0]
This phenomenon is also common between Riedel dataset and Freebase through our manual inspection.,4.4 Impact of False Positive Samples,[0],[0]
"Obviously, there is nothing the above two methods can do in this case.
",4.4 Impact of False Positive Samples,[0],[0]
The proposed RL method is to tackle this problem.,4.4 Impact of False Positive Samples,[0],[0]
We adopt our RL agents to redistribute Riedel dataset by moving false positive samples into the negative sample set.,4.4 Impact of False Positive Samples,[0],[0]
"Then we use Zeng et al. (2015) and Lin et al. (2016) to predict relations on this cleaned dataset, and compare the performance with that on the original Riedel dataset.",4.4 Impact of False Positive Samples,[0],[0]
"As shown in Figure 3 and Figure 4, under the assistant of our RL agent, the same model can achieve obvious improvement with more reasonable training dataset.",4.4 Impact of False Positive Samples,[0],[0]
"In order to give the more intuitive comparison, we calculate the AUC value of each PR curve, which reflects the area size under these curves.",4.4 Impact of False Positive Samples,[0],[0]
These comparable results also indicate the effectiveness of our policy-based RL method.,4.4 Impact of False Positive Samples,[0],[0]
"Moreover, as can be seen from the result of t-test evaluation, all the p-values are less than 5e-02, so the improvements are significant.",4.4 Impact of False Positive Samples,[0],[0]
"Figure 5 indicates that, for different relations, the scale of the detected false positive samples is not
proportional to the original scale, which is in accordance with the actual accident situation.",4.5 Case Study,[0],[0]
"At the same time, we analyze the correlation between the false positive phenomenon and the number of sentences of entity pairs : With this the number ranging from 1 to 5, the corresponding percentages are [55.9%, 32.0%, 3.7%, 4.4%, 0.7%].",4.5 Case Study,[0],[0]
This distribution is consistent with our assumption.,4.5 Case Study,[0],[0]
"Because Freebase is, to some extent, not completely aligned with the NYT corpus, entity pairs with fewer sentences are more likely to be false positive, which is the major factor hindering the performance of the previous systems.",4.5 Case Study,[0],[0]
"In Table 4, we present some false positive examples selected by our agents.",4.5 Case Study,[0],[0]
"Taking entity pair (Sami Moubayed, Syria) as an example, it is obvious that there is not any valuable information reflecting relation /people/person/place of birth.",4.5 Case Study,[0],[0]
Both of these sentences talks about the situation analysis of Syria from the political analyst Sami Moubayed.,4.5 Case Study,[0],[0]
"We also found that, for some entity pairs, even though there are multiple sentences, all of them are identical.",4.5 Case Study,[0],[0]
This phenomenon also increases the probability of the appearance of false positive samples.,4.5 Case Study,[0],[0]
"In this work, we propose a deep reinforcement learning framework for robust distant supervision.",5 Conclusion,[0],[0]
"The intuition is that, in contrast to prior works that utilize only one instance per entity pair and use soft attention weights to select plausible distantly supervised examples, we describe a policy-based framework to systematically learn to relocate the false positive samples, and better utilize the unlabeled data.",5 Conclusion,[0.9513462183294067],"['In very large datasets batch training is infeasible, and one must use mini-batches to update q and to approximate the required gradients.']"
"More specifically, our goal is to
teach the reinforcement agent to optimize the selection/redistribution strategy that maximizes the reward of boosting the performance of relation classification.",5 Conclusion,[0],[0]
"An important aspect of our work is that our framework does not depend on a specific form of the relation classifier, meaning that it is a plug-and-play technique that could be potentially applied to any relation extraction pipeline.",5 Conclusion,[0],[0]
"In experiments, we show that our framework boosts the performance of distant supervision relation extraction of various strong deep learning baselines on the widely used New York Times - Freebase dataset.
",5 Conclusion,[0],[0]
"Acknowledge
This work was supported by National Natural Science Foundation of China (61702047), Beijing Natural Science Foundation (4174098), the Fundamental Research Funds for the Central Universities (2017RC02) and National Natural Science Foundation of China (61703234)",5 Conclusion,[0],[0]
Distant supervision has become the standard method for relation extraction.,abstractText,[0],[0]
"However, even though it is an efficient method, it does not come at no cost—The resulted distantly-supervised training samples are often very noisy.",abstractText,[0],[0]
"To combat the noise, most of the recent state-of-theart approaches focus on selecting onebest sentence or calculating soft attention weights over the set of the sentences of one specific entity pair.",abstractText,[0],[0]
"However, these methods are suboptimal, and the false positive problem is still a key stumbling bottleneck for the performance.",abstractText,[0],[0]
"We argue that those incorrectly-labeled candidate sentences must be treated with a hard decision, rather than being dealt with soft attention weights.",abstractText,[0],[0]
"To do this, our paper describes a radical solution—We explore a deep reinforcement learning strategy to generate the false-positive indicator, where we automatically recognize false positives for each relation type without any supervised information.",abstractText,[0],[0]
"Unlike the removal operation in the previous studies, we redistribute them into the negative examples.",abstractText,[0],[0]
The experimental results show that the proposed strategy significantly improves the performance of distant supervision comparing to state-of-the-art systems.,abstractText,[0],[0]
Robust Distant Supervision Relation Extraction via Deep Reinforcement Learning,title,[0],[0]
"2 for each variable satisfies n
2 . pn/plog d, where n is the sample size and d is the number of variables, the proposed robust precision matrix estimator attains the same statistical rate as the standard estimator for Gaussian graphical models. In addition, we propose a hypothesis testing procedure to assess the uncertainty of our robust estimator. We demonstrate the effectiveness of our method through extensive experiments on both synthetic data and real-world genomic data.",text,[0],[0]
"Gaussian graphical models (GGMs) have attracted increasing attention in recent years, especially in the field of high-dimensional statistical learning.",1 Introduction,[0],[0]
"In Gaussian graphical models, a d-dimensional random vector X = (X
1 , . . .",1 Introduction,[0],[0]
", Xd)> follows a multivariate normal distribution Nd(0,⌃⇤).",1 Introduction,[0],[0]
"It corresponds to the vertex set V = {1, . . .",1 Introduction,[0],[0]
", d} of an undirected graph G = (V,E), where the edge set E describes the conditional independence relationships between nodes X
1 , . . .",1 Introduction,[0],[0]
", Xd.",1 Introduction,[0],[0]
It is well-known that the graph G is encoded by the sparsity pattern of the precision matrix ⇥,1 Introduction,[0],[0]
"⇤ = ⌃
⇤ 1.",1 Introduction,[0],[0]
"More specifically, no edge connects Xi and Xj if and only if ⇥⇤ij = 0.",1 Introduction,[0],[0]
"Consequently, estimation of
1Department of Computer Science, University of Virginia, Charlottesville, Virginia, USA.",1 Introduction,[0],[0]
"Correspondence to: Quanquan Gu <qg5w@virginia.edu>.
",1 Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1 Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1 Introduction,[0],[0]
the precision matrix ⇥,1 Introduction,[0],[0]
"⇤ corresponds to parameter estimation, and specifying the non-zero set of ⇥⇤ corresponds to graphical model selection (Cox & Wermuth, 1996).
",1 Introduction,[0],[0]
"In the high-dimensional settings, where the number of variables d can exceed the number of observations n, a large body of literature has studied the problem of precision matrix estimation in Gaussian graphical models and their variants (Meinshausen & Bühlmann, 2006; Yuan & Lin, 2007; Friedman et al., 2008; Banerjee et al., 2008; Yuan, 2010; Cai et al., 2011; Wang et al., 2016; Xu & Gu, 2016; Xu et al., 2016; 2017).",1 Introduction,[0],[0]
"For instance, Meinshausen & Bühlmann (2006) developed a neighborhood pursuit approach for estimating conditional independence relationship separately for each node in the graph.",1 Introduction,[0],[0]
This method estimates the precision matrix by solving a collection of sparse regression problems using Lasso in parallel.,1 Introduction,[0],[0]
"Yuan & Lin (2007); Friedman et al. (2008); Banerjee et al. (2008) proposed a `
1 norm regularized Gaussian negative log-likelihood method, which called Graphical Lasso (GLasso), to directly estimate the precision matrix.",1 Introduction,[0],[0]
"More recently, Yuan (2010); Cai et al. (2011) proposed the graphical Dantzig selector and CLIME, respectively.",1 Introduction,[0],[0]
"Both of these methods can be solved by linear programming and have more favorable theoretical properties than GLasso.
",1 Introduction,[0],[0]
Note that most of the aforementioned methods rely on the assumption that the observations follow a Gaussian distribution.,1 Introduction,[0],[0]
"There also exists some work, such as Ravikumar et al. (2011), studied sub-Gaussian data under bounded higher order moments.",1 Introduction,[0],[0]
"However, in many real-word applications, the data can follow a heavy-tailed distribution, or may even be corrupted arbitrarily.",1 Introduction,[0],[0]
"In such cases, conventional methods yield inaccurate graph estimation even if there are only a few contaminated observations due to the lack of robustness.",1 Introduction,[0],[0]
"In order to address this issue, a large body of literature (Liu et al., 2012; Finegold & Drton, 2011; Hirose & Fujisawa, 2015; Sun & Li, 2012; Yang & Lozano, 2015; Balmand & Dalalyan, 2015; Öllerer & Croux, 2015; Loh & Tan, 2015; Chen et al., 2015; Tarr et al., 2016) has focused on providing more robust estimators for precision matrices in the past years.",1 Introduction,[0],[0]
"However, most of these estimators were established under some specific contamination models, thus they are not good at dealing with the situation when data are arbitrarily corrupted.
",1 Introduction,[0],[0]
"In this paper, we propose a robust estimator to estimate the precision matrix in high-dimensional GGMs with arbitrarily corrupted data.",1 Introduction,[0],[0]
"More specifically, we consider the situation that the corrupted data can appear in any coordinates of the observations.",1 Introduction,[0],[0]
This includes situations that some observations are outliers or data follow some specific contamination models as special cases.,1 Introduction,[0],[0]
The definition of the arbitrary corruption model will be presented in section 3.,1 Introduction,[0],[0]
"The key idea of our method is to use a robust covariance matrix estimator, which remains accurate provided a controlled number of arbitrarily corrupted coordinates.",1 Introduction,[0],[0]
"Our theory provides not only the spectral norm based estimation error of the proposed estimator, but also the model selection consistency guarantee.",1 Introduction,[0],[0]
"More importantly, we show that provided that the number of corrupted samples n
2 for each variable satisfies n
2 .",1 Introduction,[0],[0]
"pn/plog d, where n is the sample size and d is the number of variables, the proposed robust precision matrix estimator attains the same statistical rate as the standard estimator for Gaussian graphical models.",1 Introduction,[0],[0]
"Beyond point estimation, we also propose a hypothesis testing procedure to assess the uncertainty of our robust estimator with corrupted observations, and construct the confidence interval for the point estimate.",1 Introduction,[0],[0]
"Thorough experiments on both synthetic data and real-world genomic data corroborate the effectiveness of our method.
",1 Introduction,[0],[0]
"The remainder of this paper is organized as follows: In Section 2, we discuss some more related work about the robust precision matrix estimation.",1 Introduction,[0],[0]
Section 3 summarizes our proposed estimation method and testing procedure in general and also introduces some necessary backgrounds.,1 Introduction,[0],[0]
Section 4 presents our main results including estimation error bound and inference property.,1 Introduction,[0],[0]
"Section 5 provides numerical results, for our method and a number of other methods, of some simulated datasets and a real example on gene expression data.",1 Introduction,[0],[0]
"Section 6 concludes with discussion.
",1 Introduction,[0],[0]
Notation Let A = [Aij ] 2 Rd⇥d be a d ⇥ d matrix and x =,1 Introduction,[0],[0]
"[x
1 , . . .",1 Introduction,[0],[0]
", xd]> 2 Rd be a d-dimensional vector.",1 Introduction,[0],[0]
"For 0 < q < 1, we define the `
0 , `q and `1 vector norms as kxk
0
= Pd i=1",1 Introduction,[0],[0]
"1{xi 6= 0}, kxkq =
( Pd i=1 |xi|q) 1 q , kxk1 = max1id |xi|, where 1{·} represents the indicator function.",1 Introduction,[0],[0]
"We use the following notations for the matrix `q , `max, `1,1 and `F norms: kAkq = maxkxkq=1 kAxkq, kAk1,1 = maxij |Aij",1 Introduction,[0],[0]
"|, kAk1,1 = Pd
i=1",1 Introduction,[0],[0]
Pd j=1,1 Introduction,[0],[0]
"|Aij |, kAkF =",1 Introduction,[0],[0]
( P ij |Aij |2)1/2.,1 Introduction,[0],[0]
"We use A⇤j = (A1j , . . .",1 Introduction,[0],[0]
", Adj)> to denote the j-th column vector of A and A⇤\j to denote the submatrix of A with the jth column A⇤j removed.",1 Introduction,[0],[0]
"We also denote by max(A) and min
(A) the largest and smallest eigenvalues of matrix A, respectively.",1 Introduction,[0],[0]
"Furthermore, for a matrix ⇥ and sets of tuples S, S
1 , ⇥S1,S denotes the set of numbers (⇥jk)j2S1,k2S .",1 Introduction,[0],[0]
"We define the maximum degree of a graph or row cardinality as s = max
1in |{j 2 V | ⇥⇤ij 6= 0}|, where
V = {1, . . .",1 Introduction,[0],[0]
", d} is the vertex set.",1 Introduction,[0],[0]
"Finally, for a sequence of random variables Xn, we write Xn
d !",1 Introduction,[0],[0]
"X , for some random variable X , if Xn converges in distribution to X .",1 Introduction,[0],[0]
"In recent years, some attempts have been made toward the robust estimation of high-dimensional GGMs under different corruption models.",2 Related Work,[0],[0]
"For example, to deal with heavy tailed distributions, Liu et al. (2012) developed a semiparametric approach called the nonparanormal SKEPTIC.",2 Related Work,[0],[0]
Finegold & Drton (2011) proposed a penalized likelihood approach based on multivariate t-distributions.,2 Related Work,[0],[0]
They also proposed an alternative t-model which requires the use of variational EM or Markov chain Monte Carlo algorithms.,2 Related Work,[0],[0]
"Hirose & Fujisawa (2015) introduced a robust estimation procedure for sparse precision matrices based on the penalized negative -likelihood function.
",2 Related Work,[0],[0]
"In order to address outliers, Sun & Li (2012) proposed a robust estimation of GGMs via a robustified likelihood function with `
1 penalization.",2 Related Work,[0],[0]
"In particular, they first use coordinate descent to efficiently estimate the structure of the precision matrix.",2 Related Work,[0],[0]
"Then, based on the estimated structure, they re-estimate the parameters of the precision matrix using iterative proportional fitting algorithm to ensure the positive definiteness of their estimator.",2 Related Work,[0],[0]
Yet their method does not have any theoretical guarantee.,2 Related Work,[0],[0]
Yang & Lozano (2015) proposed a trimmed Graphical Lasso method.,2 Related Work,[0],[0]
"Specifically, by adding weights to different data points, they improved upon the original graphical Lasso such that it is more robust to outliers.",2 Related Work,[0],[0]
"However, they did not provide any model selection consistency guarantee.",2 Related Work,[0],[0]
Balmand & Dalalyan (2015) also studied the problem of robustly estimating the covariance matrix when data are corrupted by outliers.,2 Related Work,[0],[0]
"In particular, they proposed to use a modified scaled lasso procedure for covariance matrix estimation and provided the theoretical guarantee of their method.
",2 Related Work,[0],[0]
"Another line of related work is Öllerer & Croux (2015); Loh & Tan (2015); Chen et al. (2015); Tarr et al. (2016), which studied the problem of robust precision matrix estimation in high dimensions under the ✏-contamination model.",2 Related Work,[0],[0]
"In particular, under the cell-wise contamination model, Tarr et al. (2016) evaluated the performance of the Glasso and CLIME estimators together with a U-statistic based robust covariance estimator for sparse precision matrix estimation.",2 Related Work,[0],[0]
"Under the same contamination model, Öllerer & Croux (2015) provided an analysis for the robustness of these estimators in terms of breakdown behavior.",2 Related Work,[0],[0]
"Later on, from the point of statistical consistency, Loh & Tan (2015) established the statistical error bounds for these estimators.",2 Related Work,[0],[0]
"However, these methods (Öllerer & Croux, 2015; Loh & Tan, 2015; Tarr et al., 2016) highly
depend on the specific cell-wise contamination structure on the data matrix.",2 Related Work,[0],[0]
"Recently, inspired by Tukey’s depth estimator (Tukey, 1975) for vector estimation, Chen et al. (2015) introduced the concept of matrix depth and proposed a robust covariance matrix estimator using empirical depth function.",2 Related Work,[0],[0]
They showed that their proposed estimator can achieve minimax optimal statistical rate under Huber’s ✏-contamination model.,2 Related Work,[0],[0]
"However, it is computationally very expensive to compute the deepest depth of a matrix even in a moderate dimension, which makes such method infeasible in the high-dimensional regime.
",2 Related Work,[0],[0]
All the aforementioned methods are limited to data with heavy tails and outliers.,2 Related Work,[0],[0]
"Therefore, they are not suitable to deal with data that are arbitrarily corrupted.",2 Related Work,[0],[0]
"In this section, we first introduce the setup of our problem, then we present our proposed estimation method and hypothesis testing procedure.",3 Problem Setup and Estimation Method,[0],[0]
"Let X = (X 1 , . . .",3.1 Problem Setup,[0],[0]
", Xd)> be a d-dimensional multivariate Gaussian random vector with zero mean and covariance matrix ⌃⇤.",3.1 Problem Setup,[0],[0]
"It is associated with an undirected graph G = (V,E) with vertex set V = (1, . . .",3.1 Problem Setup,[0],[0]
", d) corresponding to random variables and edge set E = {(j, k) | j 6= k,⇥⇤jk 6= 0} describing the connections of nodes, where ⇥",3.1 Problem Setup,[0],[0]
⇤ = ⌃,3.1 Problem Setup,[0],[0]
"⇤ 1 is the precision matrix.
",3.1 Problem Setup,[0],[0]
Suppose we have n i.i.d.,3.1 Problem Setup,[0],[0]
"observations X 1 , . . .",3.1 Problem Setup,[0],[0]
",Xn, each of which is drawn from the multivariate Gaussian distribution Nd(0,⌃⇤).",3.1 Problem Setup,[0],[0]
Let X =,3.1 Problem Setup,[0],[0]
"[X1, . . .",3.1 Problem Setup,[0],[0]
",Xn]> 2 Rn⇥d be the data matrix and there may exist arbitrary corruption of the data matrix X. More specifically, for each variable/column of data matrix X, we allow at most n
2 coordinates to be arbitrarily corrupted, and we call this kind of corruption model as the arbitrary corruption model.",3.1 Problem Setup,[0],[0]
"Note that under the arbitrary corruption model, we do not require the corrupted entries lie in the same n
2 rows.",3.1 Problem Setup,[0],[0]
"Clearly, a special case of the arbitrary corruption model is the outlier model where the corruption appears in n
2 observations.",3.1 Problem Setup,[0],[0]
"Under the arbitrary corruption model, n
2 is the upper bound on the number of corruptions for each variable, and under the outlier model, n
2 is the upper bound on the number of outliers.",3.1 Problem Setup,[0],[0]
"Specifically, under the outlier model, the set of row indices {1, . . .",3.1 Problem Setup,[0],[0]
", n} of the data matrix X is divided into two disjoint subsets A and O with |A| = n
1 , |O| = n 2 , and n = n
1 + n 2 .",3.1 Problem Setup,[0],[0]
XA denotes samples drawn from the authentic distribution.,3.1 Problem Setup,[0],[0]
XO denotes samples that are outliers.,3.1 Problem Setup,[0],[0]
"In general, there is no constraint on the type of corruptions in our setting except an upper bound on the number of corruptions, i.e., n
2 .",3.1 Problem Setup,[0],[0]
"For example, these corruptions could be drawn from other distributions or even be deterministic.",3.1 Problem Setup,[0],[0]
"Before we introduce our estimation method, we first introduce the truncated inner product which was proposed by Chen et al. (2013).",3.2 Estimation Method,[0],[0]
"The truncated inner product hu,vin2 is defined as follows: given two n-dimensional vectors u,v 2 Rn, and the truncation number n
2 satisfying n 2
 n, we first compute the quantity qi = uivi, for i = 1, . . .",3.2 Estimation Method,[0],[0]
",",3.2 Estimation Method,[0],[0]
"n. Then we sort {|qi|}ni=1 and select the smallest (n n
2 ) ones.",3.2 Estimation Method,[0],[0]
"Let ⌦ be the set of selected indices with cardinality |⌦| = n n
2 , then we have the truncated inner product as hu,vin2 = P i2⌦ qi.
",3.2 Estimation Method,[0],[0]
The main idea of our estimation method is to use a robust covariance matrix estimator which can mitigate the impact of arbitrary corruptions.,3.2 Estimation Method,[0],[0]
"More specifically, given a data matrix X 2 Rn⇥d, which is arbitrarily corrupted, we obtain the robust covariance matrix estimator b⌃ through a truncation procedure that each element b⌃jk is calculated via truncated inner product hX⇤j ,X⇤kin2/n1.",3.2 Estimation Method,[0],[0]
"The motivation of this truncation procedure is that the corrupted coordinates with large magnitude may heavily affect the precision of our estimation results, and this simple truncation procedure can reduce such impact.",3.2 Estimation Method,[0],[0]
"Next, we introduce our robust estimator, which is based on the robust covariance matrix estimator and CLIME: b
⇥ = argmin ⇥2Rd⇥d k⇥k 1,1 subject to kb⌃⇥ Ik1,1  ,
(3.1)
where b⌃ is the robust covariance matrix estimator obtained through truncation, > 0 is a constraint parameter.",3.2 Estimation Method,[0],[0]
We refer to (3.1) as Robust CLIME (RCLIME).,3.2 Estimation Method,[0],[0]
Note that here we do not consider the Glasso type estimator since it requires the stringent incoherence condition on the covariance matrix to guarantee the model selection consistency.,3.2 Estimation Method,[0],[0]
Let ✓⇤i = ⇥⇤⇤i denote the i-th column of ⇥⇤.,3.2 Estimation Method,[0],[0]
"To estimate the precision matrix more efficiently, instead of solving (3.1), we can estimate each column of ⇥⇤ as follows:
b✓ = argmin ✓2Rd k✓k 1 subject to kb⌃✓ eik1  ,
(3.2)
for i = 1, . . .",3.2 Estimation Method,[0],[0]
", d, and ei 2 Rd denotes a column vector that the i-th element is 1 and others are 0.",3.2 Estimation Method,[0],[0]
"Note that the combined solution b⇥1 = [b✓1
1 , . . .",3.2 Estimation Method,[0],[0]
", b✓1d] of (3.2) is equivalent to the solution of (3.1) (Cai et al., 2011).",3.2 Estimation Method,[0],[0]
"In addition, since b ⇥
1 is not symmetric, we need the following symmetrization procedure to get our robust estimator
b ⇥ = arginf ⇥2Sd++ k⇥",3.2 Estimation Method,[0],[0]
"b⇥1k 1 , (3.3)
where",3.2 Estimation Method,[0],[0]
"Sd ++ = {A 2 Rd⇥d | A = A>,A 0} denotes all d ⇥",3.2 Estimation Method,[0],[0]
d symmetric positive definite matrices.,3.2 Estimation Method,[0],[0]
"The symmetrization procedure in (3.3) can be solved by the projected gradient descent method, and in practice, we can use
many simple symmetrization methods, such as the method provided in Cai et al. (2011).",3.2 Estimation Method,[0],[0]
"Based on the proposed robust estimator (3.1), we are interested in testing whether there is an edge between node j and node k in GGMs (Jankova et al., 2015; Neykov et al., 2015; Gu et al., 2015; Xu et al., 2016).",3.3 Hypothesis Test,[0],[0]
"More specifically, we want to develop a procedure for the hypothesis test that H
0
: ⇥",3.3 Hypothesis Test,[0],[0]
⇤ jk = 0 versus H1 : ⇥,3.3 Hypothesis Test,[0],[0]
⇤ jk 6= 0.,3.3 Hypothesis Test,[0],[0]
"Let us assume that the k-th column of the precision matrix ⇥⇤ to be the vector ✓⇤k = (↵
⇤, ⇤>)> where ↵⇤ is the j-th element of the vector ✓⇤k and
⇤ 2 Rd 1 is the remaining (d 1)-dimensional vector.",3.3 Hypothesis Test,[0],[0]
"Thus it is equivalent to test the one dimensional component H
0 : ↵⇤ = 0 versus the non-restricted alternative H
1 : ↵⇤ 6= 0.",3.3 Hypothesis Test,[0],[0]
"In this case, ⇤ are nuisance parameters.",3.3 Hypothesis Test,[0],[0]
"To this end, we first introduce the following estimation equation projected (EEP) along the direction",3.3 Hypothesis Test,[0],[0]
"bw:
bS(✓) = bw> b
⌃✓ ek , (3.4)
where b⌃ is the the robust covariance matrix estimator and b
w is the solution of the optimization problem (3.2) with i = j.",3.3 Hypothesis Test,[0],[0]
The motivation of projecting the estimation equation to a sparse direction (3.4) is to help us construct a test statistic which has a tractable limiting distribution in the high-dimensional regime.,3.3 Hypothesis Test,[0],[0]
"In high-dimensional settings, b ⌃ is not positive definite, we cannot solve the equation b ⌃
b✓ ek = 0 by taking the inverse of b⌃ directly.",3.3 Hypothesis Test,[0],[0]
"Therefore, given the sparsity assumption on ✓⇤, the estimator in (3.2) can address such ill-posed problem for solving the estimation equation b⌃b✓ ek = 0 in high-dimensional settings.",3.3 Hypothesis Test,[0],[0]
"Furthermore, projecting the estimation equation to a certain direction (3.4) makes the limiting distribution of b✓ = (b↵, b >)",3.3 Hypothesis Test,[0],[0]
> in (3.2) tractable.,3.3 Hypothesis Test,[0],[0]
"More specifically, if we choose the bw as the projection direction, then due to the fact that bw is a consistent estimator of w⇤ := ⇥⇤⇤j , the estimator b of the high-dimensional nuisance parameters in (3.2) is asymptotically ignorable along this direction.",3.3 Hypothesis Test,[0],[0]
"Therefore, we can solve the projected estimation equation bS(↵, b ) = 0 to get an debiased estimator of ↵⇤ as follows:
e↵ = b↵ b w
>",3.3 Hypothesis Test,[0],[0]
"( b ⌃
b✓ ek) b w > b ⌃⇤j , (3.5)
where b✓ = (b↵, b >)",3.3 Hypothesis Test,[0],[0]
"> is the estimator of ⇥⇤⇤k, and bw is the estimator of ⇥⇤⇤j .",3.3 Hypothesis Test,[0],[0]
"Thus we define the following test statistic built upon the debiased estimator e↵
bTn = p n 1",3.3 Hypothesis Test,[0],[0]
"(e↵ ↵⇤)/b , (3.6)
where n is the number of observations, n 2 is the upper bound on the number of corruptions, n
1 = n n 2 , and b 2 = bwjb",3.3 Hypothesis Test,[0],[0]
"✓k + bwkb✓j , where bwj , b✓j denote the j-th elements of bw and b✓ respectively.",3.3 Hypothesis Test,[0],[0]
Note that b 2 is a consistent estimator to 2 = w⇤j ✓⇤k + w ⇤,3.3 Hypothesis Test,[0],[0]
k,3.3 Hypothesis Test,[0],[0]
"✓ ⇤ j under the Gaussian
assumption of the data, where w⇤j , ✓⇤k are the j-th and kth columns of w⇤ and ✓⇤ respectively.",3.3 Hypothesis Test,[0],[0]
"We will show in the next section that the proposed debiased estimator e↵ is consistent to ↵⇤, and the test statistic bTn is asymptotically normal p n 1
(e↵ ↵⇤)/b d !",3.3 Hypothesis Test,[0],[0]
"N(0, 1) under the null hypothesis.",3.3 Hypothesis Test,[0],[0]
"Therefore, our asymptotic level-↵ test is given by
n =
(
0 (⌘ accept H 0 )",3.3 Hypothesis Test,[0],[0]
"if | bTn|  C↵, 1 (⌘ reject H
0 ) if | bTn| > C↵, (3.7)
where C↵ = 1(1 ↵/2) is the (1 ↵/2)-quantile of the standard normal distribution N(0, 1).",3.3 Hypothesis Test,[0],[0]
"Furthermore, we can construct asymptotic level-↵ confidence intervals of ↵⇤ as e↵± 1(1 ↵/2)b / p n. Note that in practice, although we have no idea about the exact upper bound on the number of corruptions, i.e., n
2 , we can use techniques such as crossvalidation to choose the best truncation number n
2
.",3.3 Hypothesis Test,[0],[0]
"In this section, we present our main results and discuss connections with some related works.",4 Main Results,[0],[0]
"We start by stating some assumptions, which are required in our analysis.",4 Main Results,[0],[0]
We impose an important eigenvalue condition on the population covariance matrix.,4 Main Results,[0],[0]
Assumption 4.1.,4 Main Results,[0],[0]
"There exist a constant  > 0 such that
0 < 1/  min (⌃ ⇤ )  ",4 Main Results,[0],[0]
max (⌃ ⇤ )   ,4 Main Results,[0],[0]
"< 1.
",4 Main Results,[0],[0]
"This assumption can exclude singular or nearly singular covariance matrices, thus guarantee the uniqueness of ⇥⇤.
",4 Main Results,[0],[0]
"In this paper, we consider the precision matrix ⇥⇤ that belongs to a class of matrices U(s), i.e., U(s) =
⌦ 2 Rd⇥d ⌦ 0, k⌦k 1  M,max 1id Pd j=1 1{⌦ij 6= 0}  s
, where ⌦ 0 means ⌦ is positive definite and s corresponds to the row cardinality.",4 Main Results,[0],[0]
Note that this sparse precision matrix class has been previously considered in Cai et al. (2011); Liu & Wang (2012); Zhao & Liu (2013).,4 Main Results,[0],[0]
"In addition, it immediately implies that k⇥⇤⇤jk1  k⇥⇤k
1  M , where ⇥⇤⇤j is the jth column vector of ⇥⇤.
",4 Main Results,[0],[0]
"Now, we are ready to provide our main results.",4 Main Results,[0],[0]
The first one characterizes the performance of our robust estimator under the arbitrary corruption model.,4 Main Results,[0],[0]
"It shows that even if the upper bound on the number of corruptions n
2 scales with p n, where n is the number of observations, our robust estimator can still recover the correct support.",4 Main Results,[0],[0]
Note that our results are derived under the arbitrary corruption model.,4 Main Results,[0],[0]
"Since the outlier model is a special case of the arbitrary corruption model, our results can directly apply to the outlier case.",4 Main Results,[0],[0]
Theorem 4.2.,4 Main Results,[0],[0]
"Under the arbitrary corruption model, suppose ⇥⇤ 2 U(s) and Assumption 4.1 is satisfied.",4 Main Results,[0],[0]
"In addition, assume the upper bound on the number of corruptions n
2 satisfies n 2  a p n for some constant a 0.",4 Main Results,[0],[0]
"If
n 4a2, and we choose the regularization parameter satisfying = CM2 p
log d/n",4 Main Results,[0],[0]
+,4 Main Results,[0],[0]
"n 2 log d/n , then, with probability at least 1 C
1 /d, the estimator b⇥ in (3.1) satisfies
k b⇥ ⇥",4 Main Results,[0],[0]
"⇤k 2  C 2 M22 ✓ s
r
log d
n",4 Main Results,[0],[0]
"+
n 2 s log d
n
◆
.",4 Main Results,[0],[0]
"(4.1)
Furthermore, if the nonzero entries of ⇥⇤ satisfy
min i 6=j,⇥⇤ij 6=0 |⇥⇤ij | C3M22
✓
r
log d
n",4 Main Results,[0],[0]
"+
n 2 log d
n
◆
,
then the Robust CLIME can correctly identify nonzero entries of ⇥⇤.",4 Main Results,[0],[0]
Remark 4.3.,4 Main Results,[0],[0]
"According to (4.1), the estimation error of our robust estimator consists of two terms.",4 Main Results,[0],[0]
"The first one O(s p
log d/n) corresponds to the estimation error without corruptions.",4 Main Results,[0],[0]
"The second extra term O(sn
2 log d/n), which is linear in n
2 , is due to the effect of arbitrary corruption.",4 Main Results,[0],[0]
"More specifically, if there is no corruption in our data, then the second term becomes zero since n
2 = 0.",4 Main Results,[0],[0]
"Therefore, the estimation error of our method reduces to O(s p
log d/n), which matches the minimax optimal rate for sparse precision matrix estimation without corruptions in terms of spectral norm (Yuan, 2010; Cai et al., 2011; Ravikumar et al., 2011).",4 Main Results,[0],[0]
"In addition, (4.1) in Theorem 4.2 indicates that our robust estimator can correctly recover the support of ⇥⇤ even if the upper bound on the number of corruptions n
2
scales with p
n/ log d, where n is the number of observations.",4 Main Results,[0],[0]
"In addition, under the outlier model, this estimation result is comparable to the result provided by Yang & Lozano (2015).",4 Main Results,[0],[0]
"In their study, they proved that the proposed estimator can successfully recover the true parameter provided that the upper bound of the number of outliers is O( p n).",4 Main Results,[0],[0]
"However, Yang & Lozano (2015) does not consider the case when the data is arbitrarily corrupted.
",4 Main Results,[0],[0]
"Furthermore, if the upper bound on the number of corruptions n
2 satisfies n 2 .",4 Main Results,[0],[0]
"pn/plog d, our robust estimator can achieve the same statistical rate as the standard estimator for Gaussian graphical models.",4 Main Results,[0],[0]
This is summarized in the following corollary.,4 Main Results,[0],[0]
Corollary 4.4.,4 Main Results,[0],[0]
"Under the same conditions of Theorem 4.2, if we further assume that the upper bound on the number of corruptions n
2 satisfies n 2 .",4 Main Results,[0],[0]
"pn/plog d, then for the robust estimator b⇥ in (3.1), we have, with probability at least 1 C/d, that
k b⇥ ⇥",4 Main Results,[0],[0]
"⇤k 2  C 1 M22s
r
log d
n .
",4 Main Results,[0],[0]
"Furthermore, if the nonzero entries of ⇥⇤ satisfy
min i 6=j,⇥⇤ij 6=0 |⇥⇤ij | C2M22
p
log d/n,
then the Robust CLIME can correctly identify the nonzero entries of ⇥⇤.
",4 Main Results,[0],[0]
Remark 4.5.,4 Main Results,[0],[0]
"Compared with Theorem 4.2, Corollary 4.4 implies that under a slightly stricter condition on the upper bound of the number of corruptions n
2
= O( p n/ p log d), our robust estimator can successfully recover the true parameter ⇥⇤ with guaranteed estimation error O(s p
log d/n).",4 Main Results,[0],[0]
"Note that this error bound exactly recover the spectral norm error bound for the case without corruptions (Yuan, 2010; Cai et al., 2011; Ravikumar et al., 2011), which demonstrates the superiority of our estimator.
",4 Main Results,[0],[0]
"Next, we present the asymptotic results of our proposed test statistics in (3.6), which verifies the effectiveness of our testing procedure.",4 Main Results,[0],[0]
"Note that we consider the case that the true observations are drawn from a Gaussian distribution.
",4 Main Results,[0],[0]
Theorem 4.6.,4 Main Results,[0],[0]
"Suppose Assumption 4.1 is satisfied andp n 1 sM24( p log d/n 1 + n 2 log d/n 1 ) 2
= o(1), where n 1 = n n 2
.",4 Main Results,[0],[0]
"If we choose regularization parameter satisfying = CM2( p
log d/n 1 +n 2 log d/n 1 ), then the test statistic in (3.6) is asymptotically normal
p n 1
(e↵ ↵⇤) b d !",4 Main Results,[0],[0]
"N(0, 1),
where b 2 = bwjb✓k + bwkb✓j , and e↵ is defined in (3.5).
",4 Main Results,[0],[0]
Remark 4.7.,4 Main Results,[0],[0]
"Theorem 4.6 provides us an efficient test for the existence of an edge in GGMs, and gives us an efficient interval estimation of ↵⇤ = ⇥⇤ij .",4 Main Results,[0],[0]
"In addition, Theorem 4.6 implies that if the upper bound on the number of corruptions n
2 satisfies n 2 .",4 Main Results,[0],[0]
"pn/plog d and the quantity M is a constant, then the assumptionp n 1",4 Main Results,[0],[0]
"sM24( p log d/n 1 + n 2 log d/n 1 ) 2
= o(1) reduces to s log d/ p n = o(1), which gives us the sparsity assumption that s =",4 Main Results,[0],[0]
O( p n log d).,4 Main Results,[0],[0]
"This requirement on sparsity matches the best-known results for edge testing in GGMs (Liu et al., 2013; Ren et al., 2015).",4 Main Results,[0],[0]
"More importantly, Theorem 4.6 suggests that even when n
2
=",4 Main Results,[0],[0]
"O( p n/ p log d) out of n observations of each variable are arbitrarily corrupted, our testing procedure is still efficient.",4 Main Results,[0],[0]
"In this section, we compare our robust estimator with some existing methods, including trimmed Graphical Lasso (tGLasso) (Yang & Lozano, 2015), t⇤-Lasso (tLasso) (Finegold & Drton, 2011), robust `
1 penalized likelihood (RLL) (Sun & Li, 2012), nonparanormal SKEPTIC (Liu et al., 2012), and pairwise based covariance estimator (spearC) (Loh & Tan, 2015) on some synthetic datasets.",5 Experiments,[0],[0]
Our comparisons focus on their performance in both graph recovery and parameter estimation.,5 Experiments,[0],[0]
The implementation of tLasso and RLL is based on the code provided by authors.,5 Experiments,[0],[0]
"The implementation of other baseline algo-
rithms is based on R package huge1.",5 Experiments,[0],[0]
We conduct some simulations to investigate the performance of our proposed hypothesis testing procedure.,5 Experiments,[0],[0]
"Furthermore, we compare our method with GLasso on a gene expression data.",5 Experiments,[0],[0]
"In our numerical simulations, we consider the following two settings: (i) n = 100, d = 100; and (ii) n = 200, d = 400.",5.1 Synthetic Data,[0],[0]
We generate the true precision matrices based on two graph structures: cluster and band.,5.1 Synthetic Data,[0],[0]
"More specifically, the precision matrices ⇥⇤ are generated by huge package, and the magnitude of correlations is the default value (0.3) in the huge generator.",5.1 Synthetic Data,[0],[0]
"In order to incorporate corruptions, we generate our observations by the following procedure.
",5.1 Synthetic Data,[0],[0]
"For the arbitrary corruption model, we first generate the n by d data matrix X from the Gaussian distribution Nd(0,⇥⇤ 1).",5.1 Synthetic Data,[0],[0]
"Then, for each column of the data matrix, we let np coordinates be arbitrarily corrupted, where we consider the corruption rate p = 0.1 for small number of corruptions and p = 0.2 for large number of corruptions.",5.1 Synthetic Data,[0],[0]
"In addition, each corrupted coordinate is generated by normal distributions N(µ, ) as follows:
MA 1 : µ = 1, = 1, MA 2 : µ = 2, = 1.",5.1 Synthetic Data,[0],[0]
"(5.1)
For the outlier model, we use the setup similar to Sun & Li (2012); Yang & Lozano (2015).",5.1 Synthetic Data,[0],[0]
"Specifically, we generate each observation from the mixture model as follows:
Xi ⇠ (1 p)Nd(0,⇥⇤ 1) +",5.1 Synthetic Data,[0],[0]
"p
2
Nd(µ,⇥ 0 1 )
",5.1 Synthetic Data,[0],[0]
"+
p
2
Nd( µ,⇥0 1) for i = 1, . . .",5.1 Synthetic Data,[0],[0]
", n,
where we consider the corruption rate p = 0.1 for small number of corruptions and p = 0.2 for large number of corruptions.",5.1 Synthetic Data,[0],[0]
"Furthermore, each outlier is generated by normal distributions Nd(µ,⇥0 1 ) as follows
MO 1 : µ = (1, . . .",5.1 Synthetic Data,[0],[0]
", 1)>, ⇥0 = Id, (5.2)
MO 2 : µ = (2, . . .",5.1 Synthetic Data,[0],[0]
", 2)>, ⇥0 = Id. (5.3)
Note that under both corruption models, we set the corruption rate p 2 {0.1, 0.2}.",5.1 Synthetic Data,[0],[0]
"In other words, we choose the number of corruptions to be 10% and 20% of all observations.",5.1 Synthetic Data,[0],[0]
"This is due to the threshold of the number of corruptions n
2
=",5.1 Synthetic Data,[0],[0]
"O( p n) suggested in our theorem.
",5.1 Synthetic Data,[0],[0]
Point Estimation: We choose tuning parameters of each method as follow.,5.1 Synthetic Data,[0],[0]
"For tGLasso, we choose n
2 /n from [0.5, 1], which is suggested by Yang & Lozano (2015).",5.1 Synthetic Data,[0],[0]
"For RLL, we choose 2 {0.005, 0.01, 0.02}, which is suggested by Sun & Li (2012).",5.1 Synthetic Data,[0],[0]
"And for Robust CLIME, we choose n
2 around 15 (±5).",5.1 Synthetic Data,[0],[0]
"Since the performance of t⇤1http://cran.r-project.org/web/packages/huge
Lasso is similar to t-Lasso, we just show the results of t⇤Lasso.",5.1 Synthetic Data,[0],[0]
"All results we reported are their best performance based on these parameters.
",5.1 Synthetic Data,[0],[0]
"First, we use receiver operating characteristic (ROC) curves to compare the overall performance of our method with others in model selection over the full paths.",5.1 Synthetic Data,[0],[0]
"For the arbitrary corruption model, the ROC curves on cluster graphs averaged over 50 simulations are shown in Figure 1.",5.1 Synthetic Data,[0],[0]
"We can observe that under the arbitrary corruption model, as the number of corruptions increase, the advantage of our approach becomes more significant.",5.1 Synthetic Data,[0],[0]
"For the outlier model, we also observe similar good performance of our method, especially for outliers with large magnitude.",5.1 Synthetic Data,[0],[0]
"Due to space limit, the ROC curves for the outlier model can be found in the longer version of this paper.",5.1 Synthetic Data,[0],[0]
"These results indicate that our method is very competitive in the graph recovery problem with arbitrary corruptions.
",5.1 Synthetic Data,[0],[0]
"Then, we evaluate the performance of our method and some existing approaches in parameter estimation.",5.1 Synthetic Data,[0],[0]
"For model settings mentioned above, we choose the corruption rate p = 0.1 for the purpose of comparisons.",5.1 Synthetic Data,[0],[0]
"We generate a dataset as the training sample, and an independent dataset from the same distribution as the test set.",5.1 Synthetic Data,[0],[0]
"We set n 2 /n = 0.9 for tGLasso, = 0.01 for RLL, and n 2
= np for Robust CLIME.",5.1 Synthetic Data,[0],[0]
We also choose the tuning parameter by grid search based on its performance on the training sample and evaluate those estimators on the test set.,5.1 Synthetic Data,[0],[0]
Here we use Spectral norm error k b⇥ ⇥,5.1 Synthetic Data,[0],[0]
"⇤k
2 and Frobenius norm error k b⇥ ⇥",5.1 Synthetic Data,[0],[0]
⇤kF to compare the performance of different methods in parameter estimation.,5.1 Synthetic Data,[0],[0]
Tables 1 and 2 summarize estimation error results in term of Spectral norm averaged over 50 simulations.,5.1 Synthetic Data,[0],[0]
These results demonstrate the advantage of our method in parameter estimation.,5.1 Synthetic Data,[0],[0]
"Other comparison results in terms of Frobenius norm error are deferred to the longer version of this paper.
",5.1 Synthetic Data,[0],[0]
Hypothesis Test: We investigate the finite sample performance of our proposed hypothesis testing procedure through some simulation studies.,5.1 Synthetic Data,[0],[0]
"We use the data generating process similar to Jankova et al. (2015); Neykov et al. (2015), and we consider the case that there are some corruptions in our data.",5.1 Synthetic Data,[0],[0]
"More specifically, for the aforementioned two settings, we consider the band graph structure with band width 1 with the corresponding precision matrix ⇥⇤ generated by R package huge.",5.1 Synthetic Data,[0],[0]
The magnitude of correlations is the default value in the huge generator.,5.1 Synthetic Data,[0],[0]
"In order to incorporate corruptions, we use the same approach described above to generate observations.",5.1 Synthetic Data,[0],[0]
"Specifically, for the arbitrary corruption model, we generate samples through model MA
2 in (5.1) with p = 0.1, 0.2.",5.1 Synthetic Data,[0],[0]
"For the outlier model, we generate samples through model MO
2
in (5.2) with p = 0.1, 0.2.
",5.1 Synthetic Data,[0],[0]
"To check the validity of the type I error of our test, we run
500 simulations.",5.1 Synthetic Data,[0],[0]
The detail of our hypothesis testing procedure is described in Section 3.3.,5.1 Synthetic Data,[0],[0]
"In the two different settings, we set n
2 = 10 and n 2 = 20 respectively, and we choose the tuning parameters by cross-validations.",5.1 Synthetic Data,[0],[0]
Table 3 summarizes the empirical type I errors of our test in different settings.,5.1 Synthetic Data,[0],[0]
"We can observe that the empirical type I
errors are close to the significance level.",5.1 Synthetic Data,[0],[0]
Figure 2 shows the Q-Q plots of our test statistic bTn in (3.6) based on 500 simulations.,5.1 Synthetic Data,[0],[0]
These plots corroborate the asymptotic normality of our test statistic.,5.1 Synthetic Data,[0],[0]
All these results demonstrate the advantage of our hypothesis testing procedure under the arbitrary corruption model.,5.1 Synthetic Data,[0],[0]
"In this subsection, we use the gene expression data of Arabidopsis thaliana, which was analyzed by Wille et al. (2004) and later on by Finegold & Drton (2011); Hirose & Fujisawa (2015), to illustrate the advantage of our method.",5.2 Gene Expression Data,[0],[0]
This data set includes n = 118 observations with 39 gene expression levels.,5.2 Gene Expression Data,[0],[0]
"For this gene expression dataset, we preprocess it through R package limma3.",5.2 Gene Expression Data,[0],[0]
Figure 6 in Appendix illustrates the histogram of some rescaled gene expression data.,5.2 Gene Expression Data,[0],[0]
"It shows that some rescaled gene expressions contain some expression levels with extreme large magnitude, which may be outliers.",5.2 Gene Expression Data,[0],[0]
"Therefore, we want to apply our method to construct a network among these genes.",5.2 Gene Expression Data,[0],[0]
"For Robust CLIME, we set n
2 = 10 and adopt 5-fold crossvalidation to choose the tuning parameter .
",5.2 Gene Expression Data,[0],[0]
The graph estimated by our method is given in Figure 3.,5.2 Gene Expression Data,[0],[0]
"The dotted arrows and the solid undirected edges correspond to the known metabolic pathway and the graph estimated by Robust CLIME, respectively.",5.2 Gene Expression Data,[0],[0]
We can see that our approach identifies a similar graph to that obtained by previous analysis of Wille et al. (2004) but with fewer ”crosstalk” edges between two pathways.,5.2 Gene Expression Data,[0],[0]
"For example, our approach finds the important connection between AACT2 and
3Available on http://bioconductor.org/packages/limma
the group MK, MPDC1, and FFPS2 in MAV pathway.",5.2 Gene Expression Data,[0],[0]
"And in MEP path way, it also identifies the connection among DXR, MCT, CMK and MECPS.",5.2 Gene Expression Data,[0],[0]
Other methods such as GLasso tends to estimate more links between two pathways in order to identify these important relationships.,5.2 Gene Expression Data,[0],[0]
These edges between two pathways provided by GLasso might be inaccurate relationships due to the lack of robustness.,5.2 Gene Expression Data,[0],[0]
The graph recovered by GLasso and the graph established by Wille et al. (2004) can be found in the longer version of this paper.,5.2 Gene Expression Data,[0],[0]
"In this paper, for the Gaussian graphical model estimation with arbitrary corruptions, we proposed a new estimator for high-dimensional precision matrices based on the robust covariance matrix estimator.",6 Conclusions and Future Work,[0],[0]
"We not only provide the estimation error bound of our robust estimator, but also propose a hypothesis testing procedure to assess the uncertainty of our robust estimator with corrupted observations, and construct the confidence interval for the point estimate.",6 Conclusions and Future Work,[0],[0]
"However, most of the robust high dimensional estimators as well as our proposed estimator are not invariant under the group action (Davies et al., 2005; Draisma et al., 2013), we will study this problem in our future work.",6 Conclusions and Future Work,[0],[0]
We would like to thank the anonymous reviewers for their helpful comments.,Acknowledgment,[0],[0]
This research was sponsored in part by the National Science Foundation IIS-1618948 and IIS1652539.,Acknowledgment,[0],[0]
The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agencies.,Acknowledgment,[0],[0]
We study the problem of estimating the highdimensional Gaussian graphical model where the data are arbitrarily corrupted.,abstractText,[0],[0]
We propose a robust estimator for the sparse precision matrix in the highdimensional regime.,abstractText,[0],[0]
"At the core of our method is a robust covariance matrix estimator, which is based on truncated inner product.",abstractText,[0],[0]
We establish the statistical guarantee of our estimator on both estimation error and model selection consistency.,abstractText,[0],[0]
"In particular, we show that provided that the number of corrupted samples n",abstractText,[0],[0]
Robust Gaussian Graphical Model Estimation with Arbitrary Corruption,title,[0],[0]
In this paper we study the guarantees of stochastic optimization algorithms for submodular maximization.,1. Introduction,[0],[0]
A function f : 2N → R is submodular if it exhibits a diminishing returns property.,1. Introduction,[0],[0]
"That is, for any S ⊆ T ⊆ N and any a /∈ T , the function respects:
fS(a) ≥ fT (a)
where fH(x) denotes the marginal contribution of an element x ∈ N to a set H ⊆ N , i.e. fH(x) = f(H ∪ x) −
*Equal contribution 1Bar Ilan University and Google 2Harvard University.",1. Introduction,[0],[0]
"Correspondence to: Avinatan Hassidim <avinatan@cs.biu.ac.il>, Yaron Singer <yaron@seas.harvard.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
f(H).",1. Introduction,[0],[0]
"Many fundamental measures such as entropy, diversity, and clustering can be modeled as submodular functions, and as a result submodular optimization is heavily studied in machine learning for well over a decade now.
",1. Introduction,[0],[0]
"It is well known that for the problem of maximizing a monotone (S ⊆ T =⇒ f(S) ≤ f(T )) submodular function under a cardinality constraint, the celebrated greedy algorithm which iteratively adds the element whose marginal contribution is maximal, obtains an approximation guarantee of 1−1/",1. Introduction,[0],[0]
"e (Nemhauser et al., 1978).",1. Introduction,[0],[0]
"This is optimal unless P=NP (Feige) or alternatively, assuming polynomiallymany function evaluations (Nemhauser & Wolsey, 1978).
",1. Introduction,[0],[0]
"In recent years, there have been various adaptations of the classic greedy algorithm to allow for scalable, distributed, and noise-resilient optimization.",1. Introduction,[0],[0]
"Most notably, the STOCHASTIC-GREEDY algorithm recently proposed by (Mirzasoleiman et al., 2015) is a linear-time algorithm which at every step takes an element, which approximates in expectation the element with the maximal marginal contribution.",1. Introduction,[0],[0]
"Mirzasoleiman et al. show that STOCHASTICGREEDY gives an approximation guarantee which is arbitrarily close to 1 − 1/e in expectation, and does very well in practice (Mirzasoleiman et al., 2015; Lucic et al., 2016).",1. Introduction,[0],[0]
"Variants of this algorithm are used in clustering (Malioutov et al., 2016), sparsification (Lindgren et al., 2016), Gaussian RBF kernels (Sharma et al., 2015), sensing (Li et al., 2016), and social data analysis (Zhuang et al., 2016).
",1. Introduction,[0],[0]
"More generally, a greedy algorithm that iteratively adds elements that are only approximately maximal in expectation, may not necessarily be due to a design decision, but rather an artifact of its application on large and noisy data sets (see e.g. (Azaria et al., 2013)).",1. Introduction,[0],[0]
"One can model this uncertainty with a probability distribution D: at every iteration, a value ξ ∼ D is being sampled, and the greedy algorithm adds an element whose marginal contribution is a ξ-approximation to the maximal marginal contribution.
",1. Introduction,[0],[0]
"In general, we refer to an algorithm that iteratively adds an element whose marginal contribution is approximately optimal in expectation as a stochastic greedy algorithm.",1. Introduction,[0],[0]
"It is easy to show that stochastic greedy algorithms give an approximation ratio of 1 − 1/eµ in expectation, where µ is the mean of the distribution modeling the uncertainty.",1. Introduction,[0],[0]
"This however is a weak guarantee as it leaves a non-negligible
likelihood that the algorithm terminates with a solution with a poor approximation guarantee.",1. Introduction,[0],[0]
"Indeed, as we later show, there are cases where stochastic greedy algorithms have desirable guarantees in expectation, but with constant probability have arbitrarily bad approximation guarantees.",1. Introduction,[0],[0]
"We prove the following results:
• Optimization under cardinality constraints.",1.1. Our results,[0],[0]
"For the problem of maximizing a monotone submodular function under cardinality constraint k, with uncertainty distributionD with expectation µ, we show that for any ε ∈",1.1. Our results,[0],[0]
"[0, 1], when k ≥ 1µε2 a stochastic greedy algorithm obtains an approximation of 1 − 1/e(1−ε)µ w.p. at least 1−e−µkε2/2.",1.1. Our results,[0],[0]
"Furthermore, we prove that this bound is optimal by showing that for any δ > 0",1.1. Our results,[0],[0]
no algorithm can obtain an approximation ratio better than 1− 1/e(1−ε)µ w.p. 1− δ.,1.1. Our results,[0],[0]
"For the special case in which the function is modular, we prove an improved bound of (1− ε)µ w.p. at least 1− e−µkε2/2;
• Optimization under matroid constraints.",1.1. Our results,[0],[0]
"To further study the difference between guarantees that occur in expectation and guarantees which appear w.h.p, we study a generalization, where the greedy algorithm is used to maximize a monotone submodular function under intersection of matroid constraints, where a distribution D with mean µ generates uncertainty.",1.1. Our results,[0],[0]
"We show that in this case, with P matroids the algorithm obtains an approximation ratio of µ/(P +1) in expectation.",1.1. Our results,[0],[0]
"However, we show that even for a single matroid no algorithm can give a finite approximate guarantee w.p. at least 1−µ−o(1), implying that in general stochastic greedy algorithms cannot obtain high probability guarantees under general matroid constraints;
• Stochastic local search.",1.1. Our results,[0],[0]
"Finally, a natural alternative to greedy is local search.",1.1. Our results,[0],[0]
"We show that even for cardinality constraints local search performs poorly, and does not give any meaningful approximation guarantees when there is probabilistic uncertainty about the quality of the elements.",1.1. Our results,[0],[0]
"We contrast this with the case where there is deterministic uncertainty about the quality of the elements, in which we get an approximation ratio of (1 + 1/µ)−1, where µ is our uncertainty.",1.1. Our results,[0],[0]
This implies that local search does not enjoy the same robustness guarantees of the greedy algorithms under uncertainty of the quality of elements.,1.1. Our results,[0],[0]
"The above results have several immediate consequences:
• Fast algorithms for submodular optimization.",1.2. Applications,[0],[0]
"Our analysis applies to the STOCHASTIC-GREEDY algorithm (Mirzasoleiman et al., 2015), thus showing its approximation guarantee holds with high probability, implying it is the optimal algorithm in terms of running time and approximation guarantee for the problem of maximizing a submodular function under a cardinality constraint; 1 The same guarantee holds for the variants studied in (Malioutov et al., 2016; Lindgren et al., 2016; Sharma et al., 2015; Li et al., 2016);
• Submodular optimization under noise.",1.2. Applications,[0],[0]
"In (Hassidim & Singer, 2017) the problem of maximization of submodular functions under a cardinality constraint is considered when given access to a noisy oracle.",1.2. Applications,[0],[0]
"Our result simplifies the analysis of one of the algorithms in this setting, and gives high probability results for the inconsistent noise model (Singla et al., 2016).",1.2. Applications,[0],[0]
"We describe the results for general monotone submodular functions in Section 2, and the improved analysis for modular functions in Section 3.",1.3. Organization of the paper,[0],[0]
"In Section 4 we consider the more general problem of maximizing a submodular function under matroid constraints, and show the inapproximabilities of stochastic local search algorithms in Section 5.",1.3. Organization of the paper,[0],[0]
"Finally, we discuss experiments in Section 6.",1.3. Organization of the paper,[0],[0]
In this section we analyze the stochastic greedy algorithm for general monotone submodular functions.,2. Submodular Functions,[0],[0]
"We first analyze the algorithm, and then show the bound is tight.",2. Submodular Functions,[0],[0]
"For a given cardinality constraint k, the standard greedy algorithm begins with the empty set as its solution and at each step {1, . . .",2.1. Upper bound,[0],[0]
", k} adds the element whose marginal contribution to the existing solution is largest.",2.1. Upper bound,[0],[0]
"In the stochastic version, the algorithm may no longer add the element whose marginal contribution is largest.",2.1. Upper bound,[0],[0]
"Rather, the algorithm adds the element whose marginal contribution is at least a factor of ξ from the maximal marginal contribution, where ξ is drawn i.i.d from some distribution D with mean µ.",2.1. Upper bound,[0],[0]
"We give a formal description below.
",2.1. Upper bound,[0],[0]
"1Formally, the algorithm in (Mirzasoleiman et al., 2015) does not assume that the expected marginal contribution of the element selected is approximately optimal, but rather that in expectation its marginal contribution approximates that of some element in the optimal solution.",2.1. Upper bound,[0],[0]
"Nevertheless our analysis still applies.
",2.1. Upper bound,[0],[0]
"Algorithm 1 STOCHASTIC-GREEDY input k
1: S ← ∅ 2:",2.1. Upper bound,[0],[0]
while |S| < k do 3: ξ ∼ D 4: S ← S ∪ arbitrary a s.t. fS(a),2.1. Upper bound,[0],[0]
"≥ ξmaxx∈N fS(x) 5: end while
output S
The following lemma shows that when the sampled mean of the distribution is close to 1, stochastic greedy algorithms obtain a near optimal performance guarantee.
",2.1. Upper bound,[0],[0]
Lemma 1.,2.1. Upper bound,[0],[0]
Let S be the set of k elements selected by a stochastic greedy algorithm s.t. in each iteration,2.1. Upper bound,[0],[0]
i ∈,2.1. Upper bound,[0],[0]
"[k] the algorithm selects an element whose marginal contribution is an ξi approximation to the marginal contribution of the element with the largest marginal contribution at that stage, and let µ̂ = 1k ∑k i=1",2.1. Upper bound,[0],[0]
ξi.,2.1. Upper bound,[0],[0]
"Then:
f(S) ≥ ( 1− 1
eµ̂
) OPT.
",2.1. Upper bound,[0],[0]
Proof.,2.1. Upper bound,[0],[0]
"Let Si = {a1, . . .",2.1. Upper bound,[0],[0]
", ai}, and let the optimal solution be O, i.e. O ∈ argmaxT :|T |≤k f(T ).",2.1. Upper bound,[0],[0]
Note that for any i,2.1. Upper bound,[0],[0]
< k,2.1. Upper bound,[0],[0]
"we have that:
f(Si+1)− f(Si) = fSi(ai+1) ≥ ξi+1",2.1. Upper bound,[0],[0]
"max
o∈O fSi(o)
≥",2.1. Upper bound,[0],[0]
ξi+1 k fSi(O) = ξi+1 k (f(O ∪ Si)− f(Si)),2.1. Upper bound,[0],[0]
≥ ξi+1 k,2.1. Upper bound,[0],[0]
"(f(O)− f(Si))
",2.1. Upper bound,[0],[0]
"Rearranging, we get:
f(Si+1) ≥ ξi+1",2.1. Upper bound,[0],[0]
"k
( f(O)− f(Si) )",2.1. Upper bound,[0],[0]
+ f(Si),2.1. Upper bound,[0],[0]
"(1)
We will show by induction that in every iteration i ∈",2.1. Upper bound,[0],[0]
"[k]:
f(Si)",2.1. Upper bound,[0],[0]
≥ 1− i∏ j=1 ( 1− ξj k ),2.1. Upper bound,[0],[0]
"f(O) The base case is when i = 1, and S0 = ∅ and by (1):
f(S1) ≥ ξi+1 k
( f(O)− f(S0) )",2.1. Upper bound,[0],[0]
= 1−,2.1. Upper bound,[0],[0]
"( 1− ξ1
k
)",2.1. Upper bound,[0],[0]
"f(O)
For a general iteration i+1, applying (1) for iteration i+1
and using the inductive hypothesis:
f(Si+1)
≥ ξi+1 k f(O) +
( 1− ξi+1
k
) f(Si)
≥ ξi+1 k f(O) +
( 1− ξi+1
k )",2.1. Upper bound,[0],[0]
1− i∏ j=1 ( 1− ξj k ),2.1. Upper bound,[0],[0]
f(O),2.1. Upper bound,[0],[0]
"=
1− i+1∏ j=1 ( 1− ξj k ) f(O) Since 1− x ≤ e−x, the above inequality implies:
f(S) = f(Sk)
= 1− k∏ j=1 ( 1− ξj k ) f(O) ≥ ( 1− e− 1k ∑k i=1 ξi )",2.1. Upper bound,[0],[0]
"f(O)
= ( 1− e−µ̂ ) f(O).
",2.1. Upper bound,[0],[0]
"We can now apply concentration bounds on the previous lemma and prove the main theorem.
Theorem 2.",2.1. Upper bound,[0],[0]
"Let f be a monotone submodular function, which is evaluated with uncertainty coming from a distribution D with mean µ. For any ε ∈ (0, 1), suppose a stochastic greedy algorithm is being used with k ≥ 2ε2µ .",2.1. Upper bound,[0],[0]
"Then, w.p. 1− e− εµ·k 2 the algorithm returns S s.t.:
f(S) ≥ ( 1− 1
e(1−ε)µ
) OPT
Proof.",2.1. Upper bound,[0],[0]
"Consider an application of the stochastic greedy algorithm with mean µ, and let ξ1, . . .",2.1. Upper bound,[0],[0]
", ξk be the approximations to the marginal contributions made by i.i.d samples from a distribution in all iterations 1, . . .",2.1. Upper bound,[0],[0]
", k.",2.1. Upper bound,[0],[0]
"Since all the values {ξi}ki=1 drawn from the distribution are bounded from above by 1, by the Chernoff bound we have:
Pr
[ 1
k k∑ i=1",2.1. Upper bound,[0],[0]
ξi,2.1. Upper bound,[0],[0]
<,2.1. Upper bound,[0],[0]
"(1− ε)µ
] < e εµ·k 2
By applying Lemma 1 we get our result.",2.1. Upper bound,[0],[0]
Claim 3.,2.2. Tight lower bound,[0],[0]
For any δ ∈,2.2. Tight lower bound,[0],[0]
"[0, 1) the competitive ratio of stochastic greedy with mean µ is at most (1−1/eµ)+o(1) with probability",2.2. Tight lower bound,[0],[0]
"at least 1− δ.
",2.2. Tight lower bound,[0],[0]
Proof.,2.2. Tight lower bound,[0],[0]
"Consider maximizing a submodular function when the oracle has probability µ of returning the element with
the largest marginal contribution, and probability 1 − µ to return a random element.",2.2. Tight lower bound,[0],[0]
We present an instance where greedy returns an approximation ratio of at most 1−1/eµ+ o(1) with probability at least 1−1/,2.2. Tight lower bound,[0],[0]
k.,2.2. Tight lower bound,[0],[0]
"We use the same bad instance regardless of µ.
The construction is as follows.",2.2. Tight lower bound,[0],[0]
"There are k special elements, m = 4k3 plain elements and n = 4(m + k)k2 dummy elements (note that the total number of elements is not n).",2.2. Tight lower bound,[0],[0]
"The value f(S) depends only on the number of special elements, plain elements and dummy elements contained in S (all special elements are identical).",2.2. Tight lower bound,[0],[0]
"Moreover, dummy elements contribute nothing to f , and hence, we can write f(S) = f(i, j), where i is the number of special elements in S, and j is the number of plain elements.
",2.2. Tight lower bound,[0],[0]
"For i ≥ 1, the value of f depends on j as follows:
f(i, j) =  kk-(k-1)jkk-j-1(k-i) 0",2.2. Tight lower bound,[0],[0]
≤ j ≤ k-1,2.2. Tight lower bound,[0],[0]
kk-(k-1)k-2((k-i)(2k-2-j)),2.2. Tight lower bound,[0],[0]
k ≤,2.2. Tight lower bound,[0],[0]
j ≤ 2k-3 kk-(k-1)k-2(3k-i-j-3) 2k-2 ≤,2.2. Tight lower bound,[0],[0]
j ≤,2.2. Tight lower bound,[0],[0]
3k-3-i kk 3k,2.2. Tight lower bound,[0],[0]
− i,2.2. Tight lower bound,[0],[0]
"− 2 ≤ j
Note that for i = 0, we have f(0, j) =",2.2. Tight lower bound,[0],[0]
"f(1, j − 1), and f(0, 0) = 0.",2.2. Tight lower bound,[0],[0]
"Also, one can verify, by case-by-case analysis that this function is indeed monotone and submodular.
",2.2. Tight lower bound,[0],[0]
"Since f(0, j) = f(1, j − 1) for every j ≥ 1, as long as the greedy algorithm did not choose any special element yet, the marginal contribution of a special elements is equal to the marginal contribution of a plain element.
",2.2. Tight lower bound,[0],[0]
Let t be the number of times in which the oracle supplied the greedy algorithm with the element with the maximal marginal contribution.,2.2. Tight lower bound,[0],[0]
"By an additive version of the Chernoff bound we have that with probability at least 1− 24 log k = 1− k4:
",2.2. Tight lower bound,[0],[0]
t < kµ+ 2 √ k log,2.2. Tight lower bound,[0],[0]
"k
We condition on this event.",2.2. Tight lower bound,[0],[0]
"Next, we argue that with probability at least 1 − 1/3k all the elements for which the algorithm selected a random element (i.e. did not take an element whose marginal contribution is a µ approximation to the largest marginal contribution) are dummy elements.",2.2. Tight lower bound,[0],[0]
Consider one of the times in which greedy was given a random element.,2.2. Tight lower bound,[0],[0]
The probability that it was not a dummy element is at most m+k4(m+k)k2−k .,2.2. Tight lower bound,[0],[0]
"Applying a union bound, the probability that in the k − t cases in which greedy was supplied with a random element it was always a dummy element is at least 1− 1/3k.",2.2. Tight lower bound,[0],[0]
"We condition on this event.
",2.2. Tight lower bound,[0],[0]
We will now argue that with probability at least 1−1/3k all the non-dummy elements selected are plain.,2.2. Tight lower bound,[0],[0]
Consider the first non dummy element chosen by the algorithm.,2.2. Tight lower bound,[0],[0]
With probability at least 1 − k/4k3 it is a plain element.,2.2. Tight lower bound,[0],[0]
"We
condition on this event.",2.2. Tight lower bound,[0],[0]
"Assuming by induction that the last i − 1 non dummy elements which were chosen by greedy were plain, the probability that i’th non dummy element is plain is 1 − k4k3−1 .",2.2. Tight lower bound,[0],[0]
"Taking a union bound over all t such elements, gives that with probability 1 − 1/3k all t nondummy elements chosen by greedy were plain.
",2.2. Tight lower bound,[0],[0]
"Combining this together with a union bound, we get that with probability at least 1 − 1/k greedy chose no special elements, at most t = µk + 4 √ k log k plain elements, and the rest are dummy elements that do not contribute to the value of the function.",2.2. Tight lower bound,[0],[0]
"This means that the value of the solution is at most kk− (k−1)tkk−t, and thus with probability at least 1 − 1/k the ratio between the value of greedy and the optimal value is at least:
kk − (k − 1)tkk−t
kk = 1−",2.2. Tight lower bound,[0],[0]
"( k − 1 k )t
≥ 1− (( 1− 1
k )",2.2. Tight lower bound,[0],[0]
k)µ+ 4 log k√k ≥,2.2. Tight lower bound,[0],[0]
1−,2.2. Tight lower bound,[0],[0]
"e−(µ+4 log k/ √ k)
",2.2. Tight lower bound,[0],[0]
"= 1− e−µ + o(1)
Choosing k = 1/δ",2.2. Tight lower bound,[0],[0]
we get our desired bound.,2.2. Tight lower bound,[0],[0]
In this section we show a tight upper bound for the special case in which the function is modular.,3. Modular Functions,[0],[0]
Recall that a function f : 2N → R is modular if for every set S ⊆ N,3. Modular Functions,[0],[0]
we have that f(S) = ∑ a∈S f(a).,3. Modular Functions,[0],[0]
Note that in this case fS(a) = f(a) for all S ⊆ N and a ∈ N .,3. Modular Functions,[0],[0]
Theorem 4.,3. Modular Functions,[0],[0]
Let S ⊆ N be the set returned when applying a stochastic greedy algorithm with mean µ ∈,3. Modular Functions,[0],[0]
"[0, 1] on a modular function f : 2N → R. Then, for any ε ∈",3. Modular Functions,[0],[0]
"[0, 1], when k ≥ 2ε2µ w.p. 1− e −µkε2/2:
f(S) ≥ (1− ε)µOPT.
",3. Modular Functions,[0],[0]
Proof.,3. Modular Functions,[0],[0]
Suppose that at every stage i ∈,3. Modular Functions,[0],[0]
[k] element ai ∈ N,3. Modular Functions,[0],[0]
is selected and its marginal contribution is at least ξi of the optimal marginal contribution at that stage.,3. Modular Functions,[0],[0]
Let O be the optimal solution and o? ∈ argmaxo∈O\S,3. Modular Functions,[0],[0]
"f(o), where S is the solution retuned by the algorithm, i.e. S = {a1, . .",3. Modular Functions,[0],[0]
.,3. Modular Functions,[0],[0]
", ak}.",3. Modular Functions,[0],[0]
"The basic idea in this proof is to observe that since o? is not in the solution and throughout the iterations of the algorithm is always a feasible candidate, this implies that every element ai in the solution S\O has value at least as large as ξif(o?).",3. Modular Functions,[0],[0]
"Intuitively, if there are enough elements in S \",3. Modular Functions,[0],[0]
"O for concentration bounds to kick in, we have that 1|S\O| ∑ j∈S\O ξj = (1 − ε)µ",3. Modular Functions,[0],[0]
and we would be done since f(S) = f(S \O)+f(O∩S),3. Modular Functions,[0],[0]
"≥ (1− ε)µf(O).
",3. Modular Functions,[0],[0]
"The problem is that S \O may not be sufficiently large, and we therefore need slightly more nuanced arguments.
",3. Modular Functions,[0],[0]
We will partition O∩S to two disjoint sets of low and high valued elements: L = {o ∈,3. Modular Functions,[0],[0]
O ∩,3. Modular Functions,[0],[0]
S : f(o) < f(o?)} and H = {o ∈ O ∩,3. Modular Functions,[0],[0]
S : f(o) ≥ f(o?)}.,3. Modular Functions,[0],[0]
"Notice that:
f(O)
= f(O \ S) + f(L) + f(H) = ∑ o∈O\S f(o) + ∑ o∈L f(o) + ∑ o∈H f(o)
≤ ∑ o∈O\S f(o?)",3. Modular Functions,[0],[0]
+,3. Modular Functions,[0],[0]
∑ o∈L f(o?),3. Modular Functions,[0],[0]
+ ∑ o∈H f(o?) +,3. Modular Functions,[0],[0]
( f(o)-f(o?) ),3. Modular Functions,[0],[0]
"= k · f(o?) +
∑ o∈H (f(o)-f(o?))
",3. Modular Functions,[0],[0]
Since o? ∈,3. Modular Functions,[0],[0]
"O \S, it is a feasible choice for the algorithm at every stage, and therefore by the definition of the stochastic greedy algorithm, for every element ai ∈ S",3. Modular Functions,[0],[0]
"we have that:
f(ai) ≥ ξimax a∈N fS(a)",3. Modular Functions,[0],[0]
"= ξi max a∈N\S f(a) ≥ ξif(o?)
",3. Modular Functions,[0],[0]
"Thus, for k ≥ 2/ε2 with probability 1− e− kµε2 2 :
f(S)
= f(S \O) + f(H) + f(L) = ∑
ai∈S\O f(ai) + ∑ ai∈L f(ai) + ∑ o∈H f(o)
≥ ∑
ai∈S\O
ξif(o ?)",3. Modular Functions,[0],[0]
+ ∑ ai∈L ξif(o ?),3. Modular Functions,[0],[0]
"+ ∑ o∈H f(o)
= f(o?)  ",3. Modular Functions,[0],[0]
∑ ai∈(S\O)∪L,3. Modular Functions,[0],[0]
"ξi +∑ o∈H f(o)
= f(o?)  ",3. Modular Functions,[0],[0]
∑ ai∈(S\O)∪L,3. Modular Functions,[0],[0]
"ξi +∑ o∈H ( f(o?) + f(o)-f(o?) )
",3. Modular Functions,[0],[0]
≥ f(o?)  ,3. Modular Functions,[0],[0]
∑ ai∈(S\O)∪L∪H ξi +∑ o∈H ( f(o)-f(o?) ),3. Modular Functions,[0],[0]
"(2)
≥ (1− ε)µ · kf(o?)",3. Modular Functions,[0],[0]
+ ∑ o∈H ( f(o)-f(o?) ),3. Modular Functions,[0],[0]
"(3)
≥ (1− ε)µ ( kf(o?)",3. Modular Functions,[0],[0]
"+
∑ o∈H ( f(o)-f(o?) ))",3. Modular Functions,[0],[0]
"≥ (1− ε)µf(O)
where inequality (2) is due to the fact that f(o?) ≥ ξif(o?)",3. Modular Functions,[0],[0]
"since ξi ≤ 1; inequality (3) is an application of the Chernoff bound for k ≥ 2/ε2µ; the last inequality is due to the upper bound we established on f(O).
",3. Modular Functions,[0],[0]
The upper bound is tight.,3. Modular Functions,[0],[0]
An obvious lower bound holds for the degenerate case where in every stage the marginal contribution of the element returned is a µ ∈,3. Modular Functions,[0],[0]
"[0, 1] approximation to the maximal marginal contribution with probability 1.",3. Modular Functions,[0],[0]
"Clearly in this case, the approximation ratio is no better than µ (consider n = 2k elements where k elements have value 1 and k elements have value µ).",3. Modular Functions,[0],[0]
In this section we consider the more general problem of maximizing a monotone submodular function under matroid constraints.,4. General Matroid Constraints,[0],[0]
"Recall that a matroid is a pair (N, I) where N is the ground set and I is a family of subsets of N called independent that respects two axioms: (1) A ∈ I, A′ ⊂",4. General Matroid Constraints,[0],[0]
A =⇒ A′ ∈,4. General Matroid Constraints,[0],[0]
"I and (2) if A,B ∈ I and |B| < |A| then ∃x ∈",4. General Matroid Constraints,[0],[0]
"A \ B, s.t. B ∪ {x} ∈ I.",4. General Matroid Constraints,[0],[0]
"The rank of the matroid is the size of the largest set in I. The cardinality constraint, is a special case of optimization under matroid constraints, where the matroid is uniform.
",4. General Matroid Constraints,[0],[0]
Submodular maximization under matroid constraints.,4. General Matroid Constraints,[0],[0]
The greedy algorithm for maximization under matroid constraints is a simple generalization of the uniform case: the algorithm iteratively identifies the element whose marginal contribution is maximal and adds it to the solution if it does not not violate the matroid constraint (i.e. if adding the element to the set keeps the set in the family of feasible sets I).,4. General Matroid Constraints,[0],[0]
This algorithm obtains an approximation ratio of 1/2.,4. General Matroid Constraints,[0],[0]
"2 More generally, for an intersection of P matroids, this algorithm obtains an approximation guarantee of 1/(1 + P ).
",4. General Matroid Constraints,[0],[0]
Stochastic greedy under intersection of matroids.,4. General Matroid Constraints,[0],[0]
"Consider an intersection of matroids, a monotone submodular function f defined over the independent sets, and an uncertainty distribution D with mean µ. The stochastic greedy algorithm begins with the solution S = ∅ and set of elements not yet considered X = N .",4. General Matroid Constraints,[0],[0]
In every iteration the algorithm maintains a solution S of elements that are in the intersection of the P matroids and a value ξ ∼ D is sampled.,4. General Matroid Constraints,[0],[0]
"The algorithm then considers an arbitrary element a ∈ X whose marginal contribution is ξmaxx∈X fS(x), and adds a to S if S ∪ a is independent in all P matroids, and discards a from X .",4. General Matroid Constraints,[0],[0]
"We first show that unlike the special case of uniform matroids, even for a single matroid, it is generally impossible to obtain high probability guarantees for maximization un-
2We note that unlike the uniform case, here greedy is not optimal.",4.1. Stochastic greedy fails with high probability,[0],[0]
"The optimal guarantee of 1 − 1/e can be obtained via an algorithm based on a continuous relaxation (Vondrák, 2008) or through local search (Filmus & Ward, 2012).
",4.1. Stochastic greedy fails with high probability,[0],[0]
"der matroid constraints, even when the function is modular and the rank of the matroid is sufficiently large.",4.1. Stochastic greedy fails with high probability,[0],[0]
Claim 5.,4.1. Stochastic greedy fails with high probability,[0],[0]
"Even for a modular function and arbitrarily large k, a stochastic greedy algorithm with mean µ cannot obtain an approximation better than 0 with probability greater than µ+ o(1), for maximization under matroid of rank k.
Proof.",4.1. Stochastic greedy fails with high probability,[0],[0]
"Consider the following example, where the ground set has two types of elements A = {a1, . . .",4.1. Stochastic greedy fails with high probability,[0],[0]
", am}, and B = {b1, . . .",4.1. Stochastic greedy fails with high probability,[0],[0]
", bk−1} where m = k2.",4.1. Stochastic greedy fails with high probability,[0],[0]
"The rank of the matroid is k, and a set is independent as long as it contains just a single ai ∈ A. Define a modular function: f(a1) = 1, but f(aj) = 0 for j 6= 1, and also f(bj) = 0 for any j ∈",4.1. Stochastic greedy fails with high probability,[0],[0]
[k−1].,4.1. Stochastic greedy fails with high probability,[0],[0]
"The distribution returns 1 w.p. p < 1/2 and 0 otherwise.
",4.1. Stochastic greedy fails with high probability,[0],[0]
"In the first iteration of the algorithm, the element a1 is correctly evaluated with probability p, and with probability 1 − p",4.1. Stochastic greedy fails with high probability,[0],[0]
"it is evaluated as having value 0, in which case we may assume that a random element is selected instead.",4.1. Stochastic greedy fails with high probability,[0],[0]
"Therefore, w.p. p the algorithm takes a1, and obtains the optimal solution.",4.1. Stochastic greedy fails with high probability,[0],[0]
"However, if this is not the case, then w.p. m−1/(m+k) = (k2−1)/(k2+k) the algorithm chooses an element from A whose value is 0.",4.1. Stochastic greedy fails with high probability,[0],[0]
"In this case, even if a1 is later correctly evaluated it could not be considered into the solution since its inclusion violates independence.",4.1. Stochastic greedy fails with high probability,[0],[0]
"Hence, while the expected value of greedy is slightly larger than p, with probability at least (1− p)− o(1) the value of the solution would be 0.",4.1. Stochastic greedy fails with high probability,[0],[0]
"Although the approximation guarantee cannot hold with high probability, we now show that in expectation stochastic greedy algorithms achieves the approximation guarantee of non-stochastic greedy when maximizing a monotone submodular functions under an intersection of P matroids.",4.2. The guarantee holds in expectation,[0],[0]
Theorem 6.,4.2. The guarantee holds in expectation,[0],[0]
"Let F denote the intersection of P ≥ 1 matroids on the ground setN , and f : 2N → R be a monotone submodular function.",4.2. The guarantee holds in expectation,[0],[0]
"The stochastic greedy algorithm returns a solution S ∈ F s.t.:
E[f(S)]",4.2. The guarantee holds in expectation,[0],[0]
"≥ µ (P + 1) OPT
.
",4.2. The guarantee holds in expectation,[0],[0]
An equivalent algorithm.,4.2. The guarantee holds in expectation,[0],[0]
"To simplify the analysis, it will be useful to consider the equivalent algorithm, which at every iteration when the existing solution is S, discards all elements x for which S ∪ x /∈",4.2. The guarantee holds in expectation,[0],[0]
F .,4.2. The guarantee holds in expectation,[0],[0]
"The following claim due to Nemhauser et al. is later employed in our analysis: Claim 7 (Prop. 2.2 in (Nemhauser et al., 1978)).",4.2. The guarantee holds in expectation,[0],[0]
If for ∀t ∈,4.2. The guarantee holds in expectation,[0],[0]
"[k] ∑t−1 i=0 σi ≤ t and pi−1 ≥ pi, with σi, pi ≥ 0",4.2. The guarantee holds in expectation,[0],[0]
"then:
k−1∑ i=0 piσi ≤ k−1∑ i=0 pi.
",4.2. The guarantee holds in expectation,[0],[0]
"Algorithm 2 STOCHASTIC-MATROID-GREEDY 1: S ← ∅, X ← N 2:",4.2. The guarantee holds in expectation,[0],[0]
while X 6= S do 3: X ← X \ {x : S ∪ {x} /∈ F} 4: ξ ∼ D. 5: S ← S ∪ arbitrary a s.t. fS(a),4.2. The guarantee holds in expectation,[0],[0]
"≥ ξmaxx∈X fS(x) 6: end while
Proof of Lemma 6.",4.2. The guarantee holds in expectation,[0],[0]
Consider the value obtained by the following procedure.,4.2. The guarantee holds in expectation,[0],[0]
"An adversary chooses some maximal independent set a1, . . .",4.2. The guarantee holds in expectation,[0],[0]
ak.,4.2. The guarantee holds in expectation,[0],[0]
"Let Si = {a1, a2, . . .",4.2. The guarantee holds in expectation,[0],[0]
", ai} with S0 = ∅, and for every i ∈",4.2. The guarantee holds in expectation,[0],[0]
"[k] let a?i be the element that maximizes the marginal contribution given Si, where the maximization is over elements a such that Si ∪ {a} is independent in all P matroids.",4.2. The guarantee holds in expectation,[0],[0]
"That is a?i is defined as:
max Si∪{a}∈F fSi(a)
",4.2. The guarantee holds in expectation,[0],[0]
"The value of the procedure is then:
k−1∑ i=0 fSi(a ?",4.2. The guarantee holds in expectation,[0],[0]
"i )
",4.2. The guarantee holds in expectation,[0],[0]
"We will bound the value obtained by the procedure against that of the optimal solution, and then argue that the value obtained by the stochastic greedy is equivalent.
",4.2. The guarantee holds in expectation,[0],[0]
Let O denote the optimal solution.,4.2. The guarantee holds in expectation,[0],[0]
"We have that:
f(O) ≤ f(O ∪ Sk) ≤ f(Sk) + ∑
x∈O\Sk
fSk(x) (4)
For a set S and a matroid Mp in the family F , we define rp(S), called the rank of S in Mp to be the cardinality of the largest subset of S which is independent in Mp, and define spp(S), called the span of S in Mp by:
spp(S) = {a ∈ N : rp(S ∪ a) = rp(S)}
If S is independent in Mp, we have that rp(spp(S))",4.2. The guarantee holds in expectation,[0],[0]
= rp(S) = |S|.,4.2. The guarantee holds in expectation,[0],[0]
"In particular, we have that rp(spp(Si))",4.2. The guarantee holds in expectation,[0],[0]
= i for every Si.,4.2. The guarantee holds in expectation,[0],[0]
"Now in each 1 ≤ p ≤ P , since O is an independent set in Mp we have:
rp(spp(Si) ∩ (O))",4.2. The guarantee holds in expectation,[0],[0]
"= |spp(Si) ∩ (O)|
which implies that |spp(Si) ∩ (O)| ≤ i.
Define Ui = ∪Pp=1spp(Si), to be the set of elements which are not part of the maximization in step i+ 1 of the procedure, and hence cannot give value at that stage.",4.2. The guarantee holds in expectation,[0],[0]
We have: |Ui∩O| = |(∪Pp=1spp(Si))∩O| ≤ P∑ p=1,4.2. The guarantee holds in expectation,[0],[0]
"|spp(Si)∩O| ≤ iP
Let Vi = (Ui \ Ui−1) ∩ O be the elements of O which are not part of the maximization in step i, but were part of the maximization in step i− 1.",4.2. The guarantee holds in expectation,[0],[0]
"If x ∈ Vi then it must be that:
fSk(x) ≤ fSi(x) ≤ fSi(a?i )
since x was not chosen in step i.",4.2. The guarantee holds in expectation,[0],[0]
"Hence, we can upper bound:
∑ x∈O\Sk fSk(x) ≤ k∑ i=1",4.2. The guarantee holds in expectation,[0],[0]
∑ x∈Vi fSi(a ?,4.2. The guarantee holds in expectation,[0],[0]
"i )
",4.2. The guarantee holds in expectation,[0],[0]
"=
k∑ i=1",4.2. The guarantee holds in expectation,[0],[0]
|Vi|fSi(a?),4.2. The guarantee holds in expectation,[0],[0]
≤ P k∑ i=1,4.2. The guarantee holds in expectation,[0],[0]
"fSi(a ?)
",4.2. The guarantee holds in expectation,[0],[0]
"where the last inequality uses ∑i t=1 |Vt| = |Ui ∩ O| ≤ Pi and the arithmetic claim proven in Claim 7 due to (Nemhauser et al., 1978).",4.2. The guarantee holds in expectation,[0],[0]
"Together with (4), we get:
f(O) ≤",4.2. The guarantee holds in expectation,[0],[0]
(P + 1) k∑ i=1,4.2. The guarantee holds in expectation,[0],[0]
fSi(a ?,4.2. The guarantee holds in expectation,[0],[0]
"i )
",4.2. The guarantee holds in expectation,[0],[0]
"Finally, note that STOCHASTIC-MATROID-GREEDY obtains a µ = Eξ∼D[ξ] approximation of the value of the procedure, in expectation.",4.2. The guarantee holds in expectation,[0],[0]
"In each stage, one can add the element chosen by the algorithm to the procedure.",4.2. The guarantee holds in expectation,[0],[0]
"Hence, at each stage STOCHASTIC-MATROID-GREEDY and the procedure have the same set of elements available, and the same a?i which maximizes the marginal contribution.",4.2. The guarantee holds in expectation,[0],[0]
In this section we consider variants of stochastic local search algorithms.,5. Inapproximability of Local Search,[0],[0]
"We show that unlike the greedy algorithm, stochastic local search algorithms can end up with arbitrarily bad approximation guarantees.
",5. Inapproximability of Local Search,[0],[0]
Local search for submodular maximization.,5. Inapproximability of Local Search,[0],[0]
"For N = {a1, . . .",5. Inapproximability of Local Search,[0],[0]
", an}, given a set of elements T ⊆ N we will use T−i to denote the set without element ai, i.e. T−i = T \ {ai}.",5. Inapproximability of Local Search,[0],[0]
A solution S is a local maximum if no single element ai in S can be exchanged for another element aj not in S whose marginal contribution to S−i is greater.,5. Inapproximability of Local Search,[0],[0]
"That is, S is a local maximum if for every ai ∈ S",5. Inapproximability of Local Search,[0],[0]
"we have that:
fS−i(ai) ≥ max x/∈S fS−i(x).
",5. Inapproximability of Local Search,[0],[0]
"It is not hard to show that for any monotone submodular function, if S is a local maximum it is a 1/2 approximation to the optimal solution.",5. Inapproximability of Local Search,[0],[0]
"A local search algorithm begins with an arbitrarily set of size k, and at every stage exchanges one of its elements with the element whose marginal contribution is maximal to the set, until it reaches a local maximum.",5. Inapproximability of Local Search,[0],[0]
"To guarantee that local
search algorithms converge in polynomial time, the convention is to seek approximate local maxima.",5. Inapproximability of Local Search,[0],[0]
A solution S is an α-approximate local maximum if no element ai in S can be exchanged for another element aj not in S whose marginal contribution to S−i is greater by a factor of α.,5. Inapproximability of Local Search,[0],[0]
"It is easy to show that an α-approximate local maximum is a (1 + 1/α)−1 approximation (Filmus & Ward, 2012).
",5. Inapproximability of Local Search,[0],[0]
Stochastic local search.,5. Inapproximability of Local Search,[0],[0]
A natural question is whether local search enjoys the same robustness guarantees as the greedy algorithm.,5. Inapproximability of Local Search,[0],[0]
We say that a solution S is a stochastic local maximum up to approximation µ if no single element in S can be exchanged for another element not in S whose expected marginal contribution is greater by a factor µ.,5. Inapproximability of Local Search,[0],[0]
"That is, S is a stochastic local maximum with mean µ if for every ai ∈ S",5. Inapproximability of Local Search,[0],[0]
"we have that:
E[fS−i(ai)] ≥ µ ·max x/∈S fS−i(x)
",5. Inapproximability of Local Search,[0],[0]
If we have uncertaintiy modeled by a distribution D ⊆,5. Inapproximability of Local Search,[0],[0]
"[0, 1], a solution is a stochastic local maximum w.r.t D if for every element ai in S we draw ξi ∼ D s.t.
fS−i(ai) ≥ ξi ·max b/∈S fS−i(b)
",5. Inapproximability of Local Search,[0],[0]
"A stochastic local search algorithm will therefore begin from an arbitrary solution S of size k, and at every iteration swap an element from the solution with an element outside the solution if S is not a stochastic local maximum w.r.t.",5. Inapproximability of Local Search,[0],[0]
"D. More specifically, the stochastic local search algorithm selects an element ai from S and replaces it with another element aj whose expected marginal contribution to S−i is at least fS−i(ai)/ξi, and repeats this process until no such elements are found.",5. Inapproximability of Local Search,[0],[0]
"This is a similar abstraction of stochastic greedy algorithms, applicable in settings when one cannot evaluate the optimal marginal contribution exactly, but approximately well in expectation.
",5. Inapproximability of Local Search,[0],[0]
Consistent and inconsistent stochasticity.,5. Inapproximability of Local Search,[0],[0]
"We consider two approaches to model the way in which the random variables ξi are assigned to an element ai in the solution:
1.",5. Inapproximability of Local Search,[0],[0]
"Consistent: For each element ai ∈ N , ξi is a random variable drawn independently from D ⊆",5. Inapproximability of Local Search,[0],[0]
"[0, 1], and fixed for the entire run of the algorithm;
2.",5. Inapproximability of Local Search,[0],[0]
Inconsistent:,5. Inapproximability of Local Search,[0],[0]
"At each step of the algorithm, for every element ai ∈ S, ξi is a random variable drawn independently from D.
Note that the solution converges, as the distribution D makes the algorithm more conservative.
",5. Inapproximability of Local Search,[0],[0]
Inapproximability of stochastic local search.,5. Inapproximability of Local Search,[0],[0]
"We show that in both consistent and inconsistent models, stochastic local search performs poorly, even for modular functions.",5. Inapproximability of Local Search,[0],[0]
"Consider a setting where there are n elements, and a modular function.",5. Inapproximability of Local Search,[0],[0]
"For every i > 1, we have f(ai) = ε/i for some negligible ε > 0",5. Inapproximability of Local Search,[0],[0]
"(e.g. ε = 2−n), but f(a1)",5. Inapproximability of Local Search,[0],[0]
= 1.,5. Inapproximability of Local Search,[0],[0]
"As for D, w.p. 0.99 it returns 1, and 0 o.w.",5. Inapproximability of Local Search,[0],[0]
Assume k = 1.,5. Inapproximability of Local Search,[0],[0]
Lemma 8.,5. Inapproximability of Local Search,[0],[0]
"The expected approximation guarantee of stochastic local search is at most 2−O(n) + ε.
",5. Inapproximability of Local Search,[0],[0]
Proof.,5. Inapproximability of Local Search,[0],[0]
"At the first iteration, local search chooses an.",5. Inapproximability of Local Search,[0],[0]
"If ξ = 0, we are done, and this is a local maxima.",5. Inapproximability of Local Search,[0],[0]
"Otherwise, local search chooses an−1.",5. Inapproximability of Local Search,[0],[0]
"At iteration i local search starts with ai, halts w.p. 0.01 (the probability that D outputs 0), and otherwise continues.",5. Inapproximability of Local Search,[0],[0]
"The probability that it will not halt for n steps and reach ai is 0.99n = 2−O(n).
",5. Inapproximability of Local Search,[0],[0]
We note that in the above proof we assumed that the local search algorithm chooses an arbitrary element at every iteration.,5. Inapproximability of Local Search,[0],[0]
If one allows the stochastic local search to randomly choose an element in every iteration a similar construction shows an inapproximability of O( lognn ) + ε.,5. Inapproximability of Local Search,[0],[0]
"We applied the algorithms to an ego-network from (Leskovec & Krevl, 2014).",6. Experiments,[0],[0]
This network has 333 nodes and 5038 edges.,6. Experiments,[0],[0]
"The submodular function we used is coverage, which models influence in social networks.",6. Experiments,[0],[0]
"In order to emphasize the implications of having results w.h.p, the graphs do not depict the average of many runs, but instead each graph is a single run of the algorithm.",6. Experiments,[0],[0]
"In greedy, we present the value of the solution at each iteration k.",6. Experiments,[0],[0]
"In local search and in random we sort elements of the solution according to marginal contributions.
",6. Experiments,[0],[0]
We start with greedy and describe the different distributions we used to model uncertainty.,6. Experiments,[0],[0]
The same distributions were used for local search.,6. Experiments,[0],[0]
"Both left panes include the greedy algorithm without uncertainty (greedy, blue line), and choosing a random set (random, black line).",6. Experiments,[0],[0]
"When running stochastic greedy, we first sample a value ξ ∈ D,
and then pick a random element out of the elements that have marginal contribution at least ξmaxa fS(a).",6. Experiments,[0],[0]
The distribution D varies between the different lines.,6. Experiments,[0],[0]
"In the leftmost pane, APX (red line) depicts a D which is the constant distribution 0.5.",6. Experiments,[0],[0]
"In Stochastic greedy with mean 0.75, D is the uniform distribution on [0.5, 1] (purple line), and in Stochastic greedy with mean 0.5, D is the uniform distribution on [0, 1].",6. Experiments,[0],[0]
"It is expected that APX will behave smoothly, as D is a degenerate distribution in this case (note that there is still randomization in which element to choose at every stage out of the eligible elements).",6. Experiments,[0],[0]
"However, we see that the h.p. result kicks in, and the APX line is similar (across many values of k) to stochastic greedy with mean 0.5.",6. Experiments,[0],[0]
"Raising the mean to 0.75 makes stochastic greedy behave almost like greedy when k gets large, so in some cases stochastic greedy makes the same choice greedy would make.
",6. Experiments,[0],[0]
"In the second pane, The purple line (exponential) depicts D as an exponential distribution with λ = 4, which gives a mean of 0.25.",6. Experiments,[0],[0]
"The red line is uniform in [0, 5], and the yellow is a Gaussian with µ = 0.25 and σ = 0.1.",6. Experiments,[0],[0]
"We see that all graphs are further away from Greedy compared to the leftmost pane, and that higher variance is generally not a good thing, although the differences are small.
",6. Experiments,[0],[0]
"The two right panes depict the same noise distributions as the two left panes, but this time we use local search (or stochastic local search) instead of greedy.",6. Experiments,[0],[0]
It is easy to see that D affects local search more than it affects greedy.,6. Experiments,[0],[0]
"The plateau is caused since we sort the final solution and then plot the elements, and since if some elements have a low value of ξi they are likely to stay in the solution even if they contribute very little, as in Lemma 8.",6. Experiments,[0],[0]
"A.H. was supported by ISF 1394/16; Y.S. was supported by NSF grant CCF-1301976, CAREER CCF-1452961, a Google Faculty Research Award, and a Facebook Faculty Gift.",7. Acknowledgements,[0],[0]
"We thank Andreas Krause for pointing the connection between our result and (Mirzasoleiman et al., 2015).",7. Acknowledgements,[0],[0]
In this paper we analyze the robustness of stochastic variants of the greedy algorithm for submodular maximization.,abstractText,[0],[0]
"Our main result shows that for maximizing a monotone submodular function under a cardinality constraint, iteratively selecting an element whose marginal contribution is approximately maximal in expectation is a sufficient condition to obtain the optimal approximation guarantee with exponentially high probability, assuming the cardinality is sufficiently large.",abstractText,[0],[0]
"One consequence of our result is that the linear-time STOCHASTIC-GREEDY algorithm recently proposed in (Mirzasoleiman et al., 2015) achieves the optimal running time while maintaining an optimal approximation guarantee.",abstractText,[0],[0]
"We also show that high probability guarantees cannot be obtained for stochastic greedy algorithms under matroid constraints, and prove an approximation guarantee which holds in expectation.",abstractText,[0],[0]
"In contrast to the guarantees of the greedy algorithm, we show that the approximation ratio of stochastic local search is arbitrarily bad, with high probability, as well as in expectation.",abstractText,[0],[0]
Robust Guarantees of Stochastic Greedy Algorithms,title,[0],[0]
Probabilistic modeling is a powerful approach to discovering hidden patterns in data.,1. Introduction,[0],[0]
We begin by expressing assumptions about the class of patterns we expect to discover; this is how we design a probability model.,1. Introduction,[0],[0]
We follow by inferring the posterior of the model; this is how we discover the specific patterns manifest in an observed data set.,1. Introduction,[0],[0]
"Advances in automated inference (Hoffman & Gelman, 2014; Mansinghka et al., 2014; Kucukelbir et al., 2017) enable easy development of new models for machine learning and artificial intelligence (Ghahramani, 2015).
",1. Introduction,[0],[0]
"In this paper, we present a recipe to robustify probabilistic models.",1. Introduction,[0],[0]
What do we mean by “robustify”?,1. Introduction,[0],[0]
Departure from a model’s assumptions can undermine its inference and prediction performance.,1. Introduction,[0],[0]
"This can arise due to corrupted
1Columbia University, New York City, USA.",1. Introduction,[0],[0]
"Correspondence to: Yixin Wang <yixin.wang@columbia.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"observations, or in general, measurements that do not belong to the process we are modeling.",1. Introduction,[0],[0]
"Robust models should perform well in spite of such mismatch with reality.
",1. Introduction,[0],[0]
Consider a movie recommendation system.,1. Introduction,[0],[0]
We gather data of people watching movies via the account they use to log in.,1. Introduction,[0],[0]
"Imagine a situation where a few observations are corrupted For example, a child logs in to her account and regularly watches popular animated films.",1. Introduction,[0],[0]
"One day, her parents use the same account to watch a horror movie.",1. Introduction,[0],[0]
"Recommendation models, like Poisson factorization (PF), struggle with this kind of corrupted data (see Section 4): it begins to recommend horror movies.
",1. Introduction,[0],[0]
What can be done to detect and mitigate this effect?,1. Introduction,[0],[0]
"One strategy is to design new models that are less sensitive to corrupted data, such as by replacing a Gaussian likelihood with a heavier-tailed t distribution (Huber, 2011; Insua & Ruggeri, 2012).",1. Introduction,[0],[0]
Most probabilistic models we use have more sophisticated structures; these template solutions for specific distributions are not readily applicable.,1. Introduction,[0],[0]
"Other classical robust techniques act mostly on distances between observations (Huber, 1973); these approaches struggle with high-dimensional data.",1. Introduction,[0],[0]
"How can we still make use of our favorite probabilistic models while making them less sensitive to the messy nature of reality?
",1. Introduction,[0],[0]
Main idea.,1. Introduction,[0],[0]
We propose reweighted probabilistic models (RPM).,1. Introduction,[0],[0]
The idea is simple.,1. Introduction,[0],[0]
"First, posit a probabilistic model.",1. Introduction,[0],[0]
Then adjust the contribution of each observation by raising each likelihood term to its own (latent) weight.,1. Introduction,[0],[0]
"Finally, infer these weights along with the latent variables of the original probability model.",1. Introduction,[0],[0]
"The posterior of this adjusted model identifies observations that match its assumptions; it downweights observations that disagree with its assumptions.
",1. Introduction,[0],[0]
"0 1 2
Figure 1 depicts this tradeoff.",1. Introduction,[0],[0]
"The dataset includes cor-
rupted measurements that undermine the original model; Bayesian data reweighting automatically trades off the low likelihood of the corrupted data near 1.5 to focus on the uncorrupted data near zero.",1. Introduction,[0],[0]
"The RPM (green curve) detects this mismatch and mitigates its effect compared to the poor fit of the original model (red curve).
",1. Introduction,[0],[0]
"Formally, consider a dataset of N independent observations y D .y1; : : : ; yN /.",1. Introduction,[0],[0]
"The likelihood factorizes as a productQN
nD1 `.yn",1. Introduction,[0],[0]
"j ˇ/, where ˇ is a set of latent variables.",1. Introduction,[0],[0]
"Posit a prior distribution pˇ .ˇ/.
Bayesian data reweighting follows three steps:
1.",1. Introduction,[0],[0]
"Define a probabilistic model pˇ .ˇ/ QN
nD1 `.yn",1. Introduction,[0],[0]
j ˇ/. 2.,1. Introduction,[0],[0]
"Raise each likelihood to a positive latent weight wn.
",1. Introduction,[0],[0]
"Then choose a prior on the weights pw.w/, where w D .w1; : : : ; wN /.",1. Introduction,[0],[0]
"This gives a reweighted probabilistic model (RPM)
p.y; ˇ; w/ D 1 Z pˇ .ˇ/pw.w/ NY nD1 `.yn j ˇ/wn ;
where Z is the normalizing factor.
3.",1. Introduction,[0],[0]
Infer the posterior of both the latent variables ˇ,1. Introduction,[0],[0]
"and the weights w, p.ˇ; w j y/.
",1. Introduction,[0],[0]
The latent weights w allow an RPM to automatically explore which observations match its assumptions and which do not.,1. Introduction,[0],[0]
"Writing out the logarithm of the RPM gives some intuition; it is equal (up to an additive constant) to
log pˇ .ˇ/",1. Introduction,[0],[0]
"C log pw.w/ C X
n
wn log `.yn",1. Introduction,[0],[0]
"j ˇ/: (1)
Posterior inference, loosely speaking, seeks to maximize the above with respect to ˇ and",1. Introduction,[0],[0]
"w. The prior on the weights pw.w/ plays a critical role: it trades off extremely low likelihood terms, caused by corrupted measurements, while encouraging the weights to be close to one.",1. Introduction,[0],[0]
"We study three options for this prior in Section 2.
",1. Introduction,[0],[0]
How does Bayesian data reweighting induce robustness?,1. Introduction,[0],[0]
"First, consider how the weights w affect Equation (1).",1. Introduction,[0],[0]
The logarithm of our priors are dominated by the log wn term: this is the price of moving wn from one towards zero.,1. Introduction,[0],[0]
"By shrinking wn, we gain an increase in wn log `.yn",1. Introduction,[0],[0]
j ˇ/ while paying a price in a log wn.,1. Introduction,[0],[0]
The gain outweighs the price we pay if log `.yn,1. Introduction,[0],[0]
j ˇ/ is very negative.,1. Introduction,[0],[0]
"Our priors are set to prefer wn to stay close to one; an RPM only shrinks wn for very unlikely (e.g., corrupted) measurements.
",1. Introduction,[0],[0]
Now consider how the latent variables ˇ,1. Introduction,[0],[0]
affect Equation (1).,1. Introduction,[0],[0]
"As the weights of unlikely measurements shrink, the likelihood term can afford to assign low mass to those corrupted measurements and focus on the rest of the dataset.
",1. Introduction,[0],[0]
"Jointly, the weights and latent variables work together to automatically identify unlikely measurements and focus on observations that match the original model’s assumptions.
",1. Introduction,[0],[0]
"Section 2 presents these intuitions in full detail, along with theoretical corroboration.",1. Introduction,[0],[0]
"In Section 3, we study four models under various forms of mismatch with reality, including missing modeling assumptions, misspecified nonlinearities, and skewed data.",1. Introduction,[0],[0]
RPMs provide better parameter inference and improved predictive accuracy across these models.,1. Introduction,[0],[0]
"Section 4 presents a recommendation system example, where we improve on predictive performance and identify atypical film enthusiasts in the Movielens 1M dataset.
",1. Introduction,[0],[0]
Related work.,1. Introduction,[0],[0]
"Jerzy Neyman elegantly motivates the main idea behind robust probabilistic modeling, a field that has attracted much research attention in the past century.
",1. Introduction,[0],[0]
Every attempt to use mathematics to study some real phenomena must begin with building a mathematical model of these phenomena.,1. Introduction,[0],[0]
"Of necessity, the model simplifies matters to a greater or lesser extent and a number of details are ignored.",1. Introduction,[0],[0]
[...],1. Introduction,[0],[0]
The solution of the mathematical problem may be correct and yet it may be in violent conflict with realities simply because the original assumptions of the mathematical model diverge essentially from the conditions of the practical problem considered.,1. Introduction,[0],[0]
"(Neyman, 1949, p.22).
",1. Introduction,[0],[0]
"Our work draws on three themes around robust modeling.
",1. Introduction,[0],[0]
"The first is a body of work on robust statistics and machine learning (Provost & Fawcett, 2001; Song et al., 2002; Yu et al., 2012; McWilliams et al., 2014; Feng et al., 2014; Shafieezadeh-Abadeh et al., 2015).",1. Introduction,[0],[0]
"These developments focus on making specific models more robust to imprecise measurements.
",1. Introduction,[0],[0]
One strategy is popular: localization.,1. Introduction,[0],[0]
"To localize a probabilistic model, allow each likelihood to depend on its own “copy” of the latent variable ˇn.",1. Introduction,[0],[0]
"This transforms the model into
p.y; ˇ; ˛/ D p˛.˛/ NY
nD1 `.yn",1. Introduction,[0],[0]
j ˇn/pˇ .ˇn,1. Introduction,[0],[0]
"j ˛/; (2)
where a top-level latent variable ˛ ties together all the ˇn variables (de Finetti, 1961; Wang & Blei, 2015).1 Localization decreases the effect of imprecise measurements.",1. Introduction,[0],[0]
"RPMs present a broader approach to mitigating mismatch, with improved performance over localization (Sections 3 and 4).
",1. Introduction,[0],[0]
"The second theme is robust Bayesian analysis, which studies sensitivity with respect to the prior (Berger et al., 1994).
",1. Introduction,[0],[0]
"1 Localization also relates to James-Stein shrinkage; Efron (2010) connects these dots.
",1. Introduction,[0],[0]
"Recent advances directly focus on sensitivity of the posterior (Minsker et al., 2014; Miller & Dunson, 2015) or the posterior predictive distribution (Kucukelbir & Blei, 2015).",1. Introduction,[0],[0]
"We draw connections to these ideas throughout this paper.
",1. Introduction,[0],[0]
The third theme is data reweighting.,1. Introduction,[0],[0]
This involves designing individual reweighting schemes for specific tasks and models.,1. Introduction,[0],[0]
Consider robust methods that toss away “outliers.”,1. Introduction,[0],[0]
"This strategy involves manually assigning binary weights to datapoints (Huber, 2011).",1. Introduction,[0],[0]
"Another example is covariate shift adaptation/importance sampling where reweighting transforms data to match another target distribution (Veach & Guibas, 1995; Sugiyama et al., 2007; Shimodaira, 2000; Wen et al., 2014).",1. Introduction,[0],[0]
"In contrast, RPMs treat weights as latent variables.",1. Introduction,[0],[0]
The weights are automatically inferred; no custom design is required.,1. Introduction,[0],[0]
"RPMs also connect to ideas around ensemble learning and boosting (Schapire & Freund, 2012).",1. Introduction,[0],[0]
"Boosting procedures reweight datapoints to build an ensemble of predictors for supervised learning, whereas RPMs apply to Bayesian models in general.",1. Introduction,[0],[0]
Reweighted probabilistic models (RPM) offer a new approach to robust modeling.,2. Reweighted Probabilistic Models,[0],[0]
The idea is to automatically identify observations that match the assumptions of the model and to base posterior inference on these observations.,2. Reweighted Probabilistic Models,[0],[0]
"An RPM scaffolds over a probabilistic model, pˇ .ˇ/ QN nD1 `.yn",2.1. Definitions,[0],[0]
j ˇ/. Raise each likelihood to a latent weight and posit a prior on the weights.,2.1. Definitions,[0],[0]
"This gives the reweighted joint density
p.y; ˇ; w/ D 1 Z pˇ .ˇ/pw.w/ NY nD1",2.1. Definitions,[0],[0]
`.yn j ˇ/wn ; (3) where Z D R pˇ .ˇ/pw.w/QNnD1 `.yn,2.1. Definitions,[0],[0]
"j ˇ/wn dy dˇ dw is the normalizing factor.
",2.1. Definitions,[0],[0]
The reweighted density integrates to one when the normalizing factor Z is finite.,2.1. Definitions,[0],[0]
This is always true when the likelihood `.,2.1. Definitions,[0],[0]
"j ˇ/ is an exponential family distribution with Lesbegue base measure (Bernardo & Smith, 2009); this is the class of models we study in this paper.2
RPMs apply to likelihoods that factorize over the observations.",2.1. Definitions,[0],[0]
(We discuss non-exchangeable models in Section 5.),2.1. Definitions,[0],[0]
Figure 2 depicts an RPM as a graphical model.,2.1. Definitions,[0],[0]
"Specific models may have additional structure, such as a separation of local and global latent variables (Hoffman et al., 2013), or fixed parameters; we omit these in this figure.
2Heavy-tailed likelihoods and Bayesian nonparametric priors may violate this condition; we leave these for future analysis.
ˇ ynpˇ
N
(a) Original probabilistic model
The reweighted model introduces a set of weights; these are latent variables, each with support wn 2 R>0.",2.1. Definitions,[0],[0]
"To gain intuition, consider how these weights affect the posterior, which is proportional to the product of the likelihood of every measurement.",2.1. Definitions,[0],[0]
A weight wn that is close to zero flattens out its corresponding likelihood `.yn,2.1. Definitions,[0],[0]
j ˇ/wn ; a weight that is larger than one makes its likelihood more peaked.,2.1. Definitions,[0],[0]
"This, in turn, enables the posterior to focus on some measurements more than others.",2.1. Definitions,[0],[0]
"The prior pw.w/ ensures that not too many likelihood terms get flattened; in this sense, it plays an important regularization role.
",2.1. Definitions,[0],[0]
"We study three options for this prior on weights: a bank of Beta distributions, a scaled Dirichlet distribution, and a bank of Gamma distributions.
",2.1. Definitions,[0],[0]
Bank of Beta priors.,2.1. Definitions,[0],[0]
"This option constrains each weight as wn 2 .0; 1/. We posit an independent prior for each weight
pw.w/ D NY
nD1",2.1. Definitions,[0],[0]
"Beta.wn I a; b/ (4)
and use the same parameters a and b for all weights.",2.1. Definitions,[0],[0]
"This is the most conservative option for the RPM; it ensures that none of the likelihoods ever becomes more peaked than it was in the original model.
",2.1. Definitions,[0],[0]
"The parameters a, b offer an expressive language to describe different attitudes towards the weights.",2.1. Definitions,[0],[0]
"For example, setting both parameters less than one makes the Beta act like a “two spikes and a slab” prior, encouraging weights to be close to zero or one, but not in between.",2.1. Definitions,[0],[0]
"As another example, setting a greater than b encourages weights to lean towards one.
",2.1. Definitions,[0],[0]
Scaled Dirichlet prior.,2.1. Definitions,[0],[0]
This option ensures the sum of the weights equals N .,2.1. Definitions,[0],[0]
"We posit a symmetric Dirichlet prior on all the weights
w D",2.1. Definitions,[0],[0]
"Nv pv.v/ D Dirichlet.a1/ (5)
where a is a scalar parameter and 1 is a .N ⇥",2.1. Definitions,[0],[0]
1/ vector of ones.,2.1. Definitions,[0],[0]
"In the original model, where all the weights are one, then the sum of the weights is N .",2.1. Definitions,[0],[0]
"The Dirichlet option maintains this balance; while certain likelihoods may become more peaked, others will flatten to compensate.
",2.1. Definitions,[0],[0]
The concentration parameter a gives an intuitive way to configure the Dirichlet.,2.1. Definitions,[0],[0]
Small values for a allow the model to easily up- or down-weight many data observations; larger values for a prefer a smoother distribution of weights.,2.1. Definitions,[0],[0]
"The Dirichlet option connects to the bootstrap approaches in Rubin et al. (1981); Kucukelbir & Blei (2015), which also preserves the sum of weights as N .
",2.1. Definitions,[0],[0]
Bank of Gamma priors.,2.1. Definitions,[0],[0]
"Here we posit an independent Gamma prior for each weight
pw.w/ D NY
nD1 Gamma.wn",2.1. Definitions,[0],[0]
"I a; b/ (6)
and use the same parameters a and b for all weights.",2.1. Definitions,[0],[0]
"We do not recommend this option, because observations can be arbitrarily up- or down-weighted.",2.1. Definitions,[0],[0]
"In this paper, we only consider Equation (6) for our theoretical analysis in Section 2.2.
",2.1. Definitions,[0],[0]
The bank of Beta and Dirichlet options perform similarly.,2.1. Definitions,[0],[0]
"We prefer the Beta option as it is more conservative, yet find the Dirichlet to be less sensitive to its parameters.",2.1. Definitions,[0],[0]
We explore these options in the empirical study (Section 3).,2.1. Definitions,[0],[0]
How can theory justify Bayesian data reweighting?,2.2. Theory and intuition,[0],[0]
Here we investigate its robustness properties.,2.2. Theory and intuition,[0],[0]
These analyses intend to confirm our intuition from Section 1.,2.2. Theory and intuition,[0],[0]
"Appendices B and C present proofs in full technical detail.
",2.2. Theory and intuition,[0],[0]
Intuition.,2.2. Theory and intuition,[0],[0]
Recall the logarithm of the RPM joint density from Equation (1).Now compute the maximum-a-posterior (MAP) estimate of the weights w.,2.2. Theory and intuition,[0],[0]
"The partial derivative is
@ log p.y; ˇ; w/ @wn D d log pw.wn/ dwn C log `.yn",2.2. Theory and intuition,[0],[0]
"j ˇ/ (7)
for all n D 1; : : : ; N .",2.2. Theory and intuition,[0],[0]
Plug the Gamma prior from Equation (6) into the partial derivative in Equation (7) and set it equal to zero.,2.2. Theory and intuition,[0],[0]
"This gives the MAP estimate of wn,
bwn D a 1 b log `.yn",2.2. Theory and intuition,[0],[0]
"j ˇ/ : (8)
The MAP estimate bwn is an increasing function of the log likelihood of yn when a > 1.",2.2. Theory and intuition,[0],[0]
"This reveals that bwn shrinks the contribution of observations that are unlikely under the log likelihood; in turn, this encourages the MAP estimate for b̌ to describe the majority of the observations.",2.2. Theory and intuition,[0],[0]
"This is how an RPM makes a probabilistic model more robust.
",2.2. Theory and intuition,[0],[0]
A similar argument holds for other exponential family priors on w with log wn as a sufficient statistic.,2.2. Theory and intuition,[0],[0]
"We formalize this intuition and generalize it in the following theorem, which establishes sufficient conditions where a RPM improves the inference of its latent variables ˇ.
Theorem 1",2.2. Theory and intuition,[0],[0]
Denote the true value of ˇ as ˇ⇤.,2.2. Theory and intuition,[0],[0]
Let the posterior mean of ˇ under the weighted and unweighted model be Ňw and Ňu respectively.,2.2. Theory and intuition,[0],[0]
"Assume mild conditions on pw , ` and the corruption level, and that j`.yn j Ňw/ `.yn",2.2. Theory and intuition,[0],[0]
j ˇ⇤/j <,2.2. Theory and intuition,[0],[0]
✏ holds 8n with high probability.,2.2. Theory and intuition,[0],[0]
"Then, there exists an N ⇤ such that for N > N",2.2. Theory and intuition,[0],[0]
"⇤, we have j Ňu ˇ⇤j ⌫2 j",2.2. Theory and intuition,[0],[0]
"Ňw ˇ⇤j, where ⌫2 denotes second order stochastic dominance.",2.2. Theory and intuition,[0],[0]
"(Details in Appendix B.)
",2.2. Theory and intuition,[0],[0]
The likelihood bounding assumption is common in robust statistics theory; it is satisfied for both likely and unlikely (corrupted) measurements.,2.2. Theory and intuition,[0],[0]
How much of an improvement does it give?,2.2. Theory and intuition,[0],[0]
We can quantify this through the influence function (IF) of Ňw .,2.2. Theory and intuition,[0],[0]
"Consider a distribution G and a statistic T .G/ to be a function of data that comes iid from G. Take a fixed distribution, e.g., the population distribution, F .",2.2. Theory and intuition,[0],[0]
"Then, IF.zI T; F / measures how much an additional observation at z affects the statistic T .F /.",2.2. Theory and intuition,[0],[0]
"Define
IF.zI T; F / D lim t!0C T .tız",2.2. Theory and intuition,[0],[0]
C .1,2.2. Theory and intuition,[0],[0]
"t /F / T .F / t
for z where this limit exists.",2.2. Theory and intuition,[0],[0]
"Roughly, the IF measures the asymptotic bias on T .F / caused by a specific observation z that does not come from F .",2.2. Theory and intuition,[0],[0]
"We consider a statistic T to be robust if its IF is a bounded function of z, i.e., if outliers can only exert a limited influence (Huber, 2011).",2.2. Theory and intuition,[0],[0]
"Here, we study the IF of the posterior mean T D Ňw under the true data generating distribution F D `.",2.2. Theory and intuition,[0],[0]
j ˇ⇤/.,2.2. Theory and intuition,[0],[0]
Say a value z has likelihood `.z,2.2. Theory and intuition,[0],[0]
j ˇ⇤/ that is nearly zero; we think of this z as corrupted.,2.2. Theory and intuition,[0],[0]
"Now consider the weight function induced by the prior pw.w/. Rewrite it as a function of the log likelihood, like w.log `.",2.2. Theory and intuition,[0],[0]
j ˇ⇤// as in Equation (8).,2.2. Theory and intuition,[0],[0]
Theorem 2,2.2. Theory and intuition,[0],[0]
If lima! 1 w.a/ D 0 and lima!,2.2. Theory and intuition,[0],[0]
"1 a w.a/ < 1, then IF.zI Ňw ; `.",2.2. Theory and intuition,[0],[0]
j ˇ⇤// ! 0,2.2. Theory and intuition,[0],[0]
as `.z,2.2. Theory and intuition,[0],[0]
j ˇ⇤/ ! 0,2.2. Theory and intuition,[0],[0]
":
This result shows that an RPM is robust in that its IF goes to zero for unlikely measurements.",2.2. Theory and intuition,[0],[0]
This is true for all three priors.,2.2. Theory and intuition,[0],[0]
(Details in Appendix C.),2.2. Theory and intuition,[0],[0]
"We now turn to inferring the posterior of an RPM, p.ˇ; w j y/.",2.3. Inference and computation,[0],[0]
"The posterior lacks an analytic closed-form expression for all but the simplest of models; even if the original model admits such a posterior for ˇ, the reweighted posterior may take a different form.
",2.3. Inference and computation,[0],[0]
"To approximate the posterior, we appeal to probabilistic programming.",2.3. Inference and computation,[0],[0]
A probabilistic programming system enables a user to write a probability model as a computer program and then compile that program into an inference executable.,2.3. Inference and computation,[0],[0]
"Automated inference is the backbone of such systems: it takes in a probability model, expressed as a program, and outputs an efficient algorithm for inference.",2.3. Inference and computation,[0],[0]
"We use automated inference in Stan, a probabilistic programming system (Carpenter et al., 2015).
",2.3. Inference and computation,[0],[0]
"In the empirical study that follows, we highlight how RPMs detect and mitigate various forms of model mismatch.",2.3. Inference and computation,[0],[0]
"As a common metric, we compare the predictive accuracy on held out data for the original, localized, and reweighted model.
",2.3. Inference and computation,[0],[0]
The posterior predictive likelihood of a new datapoint yé is poriginal.yé j y/,2.3. Inference and computation,[0],[0]
D R `.yé j ˇ/p.ˇ j y/,2.3. Inference and computation,[0],[0]
dˇ:,2.3. Inference and computation,[0],[0]
"Localization couples each observation with its own copy of the latent variable; this gives plocalized.yé j y/ D’
`.yé j ˇé/p.ˇé j",2.3. Inference and computation,[0],[0]
˛/p.˛ j y/ d˛ dˇé where ˇé is the localized latent variable for the new datapoint.,2.3. Inference and computation,[0],[0]
The prior p.ˇé j ˛/ has the same form as pˇ in Equation (2).,2.3. Inference and computation,[0],[0]
Bayesian data reweighting gives the following posterior predictive likelihood pRPM.yé j y/ D “ p.yé j ˇ; wé/pRPM.ˇ j y/p.wé/,2.3. Inference and computation,[0],[0]
"dwé dˇ;
where pRPM.ˇ j y/ is the marginal posterior, integrating out the inferred weights of the training dataset, and the prior p.wé/ has the same form as pw in Equation (3).",2.3. Inference and computation,[0],[0]
We study RPMs under four types of mismatch with reality.,3. Empirical Study,[0],[0]
This section involves simulations of realistic scenarios; the next section presents a recommendation system example using real data.,3. Empirical Study,[0],[0]
"We default to No-U-Turn sampler (NUTS) (Hoffman & Gelman, 2014) for inference in all experiments, except for Sections 3.5 and 4 where we leverage variational inference (Kucukelbir et al., 2017).",3. Empirical Study,[0],[0]
The additional computational cost of inferring the weights is unnoticeable relative to inference in the original model.,3. Empirical Study,[0],[0]
A router receives packets over a network and measures the time it waits for each packet.,3.1. Outliers: a network wait-time example,[0],[0]
Suppose we typically observe wait-times that follow a Poisson distribution with rate ˇ D 5.,3.1. Outliers: a network wait-time example,[0],[0]
We model each measurement using a Poisson likelihood `.yn,3.1. Outliers: a network wait-time example,[0],[0]
j ˇ/ D Poisson.ˇ/ and posit a Gamma prior on the rate pˇ .ˇ/,3.1. Outliers: a network wait-time example,[0],[0]
D,3.1. Outliers: a network wait-time example,[0],[0]
Gam.a D 2; b D 0:5/.,3.1. Outliers: a network wait-time example,[0],[0]
"Imagine that F % percent of the time, the network fails.",3.1. Outliers: a network wait-time example,[0],[0]
"During these failures, the wait-times come from a Poisson with much higher rate ˇ D 50.",3.1. Outliers: a network wait-time example,[0],[0]
"Thus, the data actually contains a mixture of two Poisson distributions; yet, our model only assumes one.",3.1. Outliers: a network wait-time example,[0],[0]
"(Details in Appendix D.1.)
",3.1. Outliers: a network wait-time example,[0],[0]
How do we expect an RPM to behave in this situation?,3.1. Outliers: a network wait-time example,[0],[0]
Suppose the network failed 25% of the time.,3.1. Outliers: a network wait-time example,[0],[0]
Figure 3a shows the posterior distribution on the rate ˇ.,3.1. Outliers: a network wait-time example,[0],[0]
"The original posterior is centered at 18; this is troubling, not only because the rate is wrong but also because of how confident the posterior fit is.",3.1. Outliers: a network wait-time example,[0],[0]
"Localization introduces greater uncertainty, yet still estimates a rate around 15.",3.1. Outliers: a network wait-time example,[0],[0]
The RPM correctly identifies that the majority of the observations come from ˇ D 5.,3.1. Outliers: a network wait-time example,[0],[0]
Observations from when the network failed are down-weighted.,3.1. Outliers: a network wait-time example,[0],[0]
"It gives a confident posterior centered at five.
",3.1. Outliers: a network wait-time example,[0],[0]
Figure 3b shows posterior 95% credible intervals of ˇ under failure rates up to F D 45%.,3.1. Outliers: a network wait-time example,[0],[0]
"The RPM is robust to corrupted measurements; instead it focuses on data that it can
explain within its assumptions.",3.1. Outliers: a network wait-time example,[0],[0]
"When there is no corruption, the RPM performs just as well as the original model.
",3.1. Outliers: a network wait-time example,[0],[0]
Visualizing the weights elucidates this point.,3.1. Outliers: a network wait-time example,[0],[0]
Figure 4 shows the posterior mean estimates of w for F D 25%.,3.1. Outliers: a network wait-time example,[0],[0]
"The weights are sorted into two groups, for ease of viewing.",3.1. Outliers: a network wait-time example,[0],[0]
"The weights of the corrupted observations are essentially zero; this downweighting is what allows the RPM to shift its posterior on ˇ towards five.
",3.1. Outliers: a network wait-time example,[0],[0]
"Despite this downweighting, the RPM posteriors on ˇ are not overdispersed, as in the localized case.",3.1. Outliers: a network wait-time example,[0],[0]
This is due to the interplay we described in the introduction.,3.1. Outliers: a network wait-time example,[0],[0]
"Downweighting observations should lead to a smaller effective sample size, which would increase posterior uncertainty.",3.1. Outliers: a network wait-time example,[0],[0]
"But the downweighted datapoints are corrupted observations; including them also increases posterior uncertainty.
",3.1. Outliers: a network wait-time example,[0],[0]
The RPM is insensitive to the prior on the weights; both Beta and Dirichlet options perform similarly.,3.1. Outliers: a network wait-time example,[0],[0]
"From here on, we focus on the Beta option.",3.1. Outliers: a network wait-time example,[0],[0]
We let the shape parameter a scale with the data size N such that N=a ⇡ 103; this encodes a mild attitude towards unit weights.,3.1. Outliers: a network wait-time example,[0],[0]
We now move on to other forms of mismatch with reality.,3.1. Outliers: a network wait-time example,[0],[0]
"Color blindness is unevenly hereditary: it is much higher for men than for women (Boron & Boulpaep, 2012).",3.2. Missing latent groups: predicting color blindness,[0],[0]
Suppose we are not aware of this fact.,3.2. Missing latent groups: predicting color blindness,[0],[0]
We have a dataset of both genders with each individual’s color blindness status and his/her relevant family history.,3.2. Missing latent groups: predicting color blindness,[0],[0]
No gender information is available.,3.2. Missing latent groups: predicting color blindness,[0],[0]
Consider analyzing this data using logistic regression.,3.2. Missing latent groups: predicting color blindness,[0],[0]
It can only capture one hereditary group.,3.2. Missing latent groups: predicting color blindness,[0],[0]
"Thus, logistic regression misrepresents both groups, even though men exhibit strong heredity.",3.2. Missing latent groups: predicting color blindness,[0],[0]
"In contrast, an RPM can detect and mitigate the missing group effect by focusing on the dominant hereditary trait.",3.2. Missing latent groups: predicting color blindness,[0],[0]
"Here we consider men as the dominant group.
",3.2. Missing latent groups: predicting color blindness,[0],[0]
We simulate this scenario by drawing binary indicators of color blindness yn ⇠ Bernoulli.1=1 C exp. pn// where the pn’s come from two latent groups: men exhibit a stronger dependency on family history (pn D 0:5xn) than women (pn D 0:01xn).,3.2. Missing latent groups: predicting color blindness,[0],[0]
"We simulate family history as
xn ⇠ Unif. 10; 10/. Consider a Bayesian logistic regression model without intercept.",3.2. Missing latent groups: predicting color blindness,[0],[0]
Posit a prior on the slope as pˇ .ˇ/ D N .0; 10/ and assume a Beta.0:1; 0:01/ prior on the weights.,3.2. Missing latent groups: predicting color blindness,[0],[0]
"(Details in Appendix D.2.)
",3.2. Missing latent groups: predicting color blindness,[0],[0]
Figure 5 shows the posterior 95% credible intervals of ˇ as we vary the percentage of females from F D 0% to 40%.,3.2. Missing latent groups: predicting color blindness,[0],[0]
"A horizontal line indicates the correct slope for the dominant group, ˇmen D 0:5.",3.2. Missing latent groups: predicting color blindness,[0],[0]
"As the size of the missing latent group (women) increases, the original model quickly shifts its credible interval away from 0:5.",3.2. Missing latent groups: predicting color blindness,[0],[0]
"The reweighted and localized posteriors both contain ˇmen D 0:5 for all percentages, but the localized model exhibits much higher variance in its estimates.
",3.2. Missing latent groups: predicting color blindness,[0],[0]
This analysis shows how RPMs can mitigate the effect of missing latent groups.,3.2. Missing latent groups: predicting color blindness,[0],[0]
"While the original logistic regression model would perform equally poorly on both groups, an RPM is able to automatically focus on the dominant group.
",3.2. Missing latent groups: predicting color blindness,[0],[0]
"0 1
Ep.wjy/Œwç
D en
si ty Corrupted (F D 25%)
",3.2. Missing latent groups: predicting color blindness,[0],[0]
"Clean (F D 0%)
Figure 6.",3.2. Missing latent groups: predicting color blindness,[0],[0]
Kernel density estimate of the distribution of weights across all measurements in the missing latent groups study.,3.2. Missing latent groups: predicting color blindness,[0],[0]
The percentage of females is denoted by F .,3.2. Missing latent groups: predicting color blindness,[0],[0]
"A hypothetical clean dataset receives weights that concentrate around one; the actual corrupted dataset exhibits a two-hump distribution of weights.
",3.2. Missing latent groups: predicting color blindness,[0],[0]
An RPM also functions as a diagnostic tool to detect mismatch with reality.,3.2. Missing latent groups: predicting color blindness,[0],[0]
The distribution of the inferred weights indicates the presence of datapoints that defy the assumptions of the original model.,3.2. Missing latent groups: predicting color blindness,[0],[0]
Figure 6 shows a kernel density estimate of the inferred posterior weights.,3.2. Missing latent groups: predicting color blindness,[0],[0]
A hypothetical dataset with no corrupted measurements receives weights close to one.,3.2. Missing latent groups: predicting color blindness,[0],[0]
"In contrast, the actual dataset with measurements from a missing latent group exhibit a bimodal distribution of weights.",3.2. Missing latent groups: predicting color blindness,[0],[0]
Testing for bimodality of the inferred weights is one way in which an RPM can be used to diagnose mismatch with reality.,3.2. Missing latent groups: predicting color blindness,[0],[0]
Consider a study of lung cancer risk.,3.3. Covariate dependence misspecification: a lung cancer risk study,[0],[0]
"While tobacco usage exhibits a clear connection, other factors may also contribute.",3.3. Covariate dependence misspecification: a lung cancer risk study,[0],[0]
"For instance, obesity and tobacco usage appear to interact, with evidence towards a quadratic dependence on obesity (Odegaard et al., 2010).
",3.3. Covariate dependence misspecification: a lung cancer risk study,[0],[0]
Denote tobacco usage as x1 and obesity as x2.,3.3. Covariate dependence misspecification: a lung cancer risk study,[0],[0]
We study three models of lung cancer risk dependency on these covariates.,3.3. Covariate dependence misspecification: a lung cancer risk study,[0],[0]
"We are primarily interested in understanding the effect of tobacco usage; thus we focus on ˇ1, the regression coefficient for tobacco.",3.3. Covariate dependence misspecification: a lung cancer risk study,[0],[0]
"In each model, some form of covariance misspecification discriminates the true structure from the assumed structure.
",3.3. Covariate dependence misspecification: a lung cancer risk study,[0],[0]
"For each model, we simulate a dataset of size N D 100 with random covariates x1 ⇠ N .10; 52/ and x2 ⇠ N .0; 102/ and regression coefficients ˇ0;1;2;3 ⇠ Unif. 10; 10/. Consider a Bayesian linear regression model with prior pˇ .ˇ/",3.3. Covariate dependence misspecification: a lung cancer risk study,[0],[0]
D N .0;,3.3. Covariate dependence misspecification: a lung cancer risk study,[0],[0]
10/. (Details in Appendix D.3.),3.3. Covariate dependence misspecification: a lung cancer risk study,[0],[0]
Table 1 summarizes the misspecification and shows absolute differences on the estimated ˇ1 regression coefficient.,3.3. Covariate dependence misspecification: a lung cancer risk study,[0],[0]
The RPM yields better estimates of ˇ1 in the first two models.,3.3. Covariate dependence misspecification: a lung cancer risk study,[0],[0]
These highlight how the RPM leverages datapoints useful for estimating ˇ1.,3.3. Covariate dependence misspecification: a lung cancer risk study,[0],[0]
The third model is particularly challenging because obesity is ignored in the misspecified model.,3.3. Covariate dependence misspecification: a lung cancer risk study,[0],[0]
"Here, the RPM gives similar results to the original model; this highlights that RPMs can only use available information.",3.3. Covariate dependence misspecification: a lung cancer risk study,[0],[0]
"Since the original model lacks dependence on x2, the RPM cannot compensate for this.",3.3. Covariate dependence misspecification: a lung cancer risk study,[0],[0]
Table 2 shows how RPMs also improve predictive accuracy.,3.4. Predictive likelihood results,[0],[0]
"In all the above examples, we simulate test data with and without their respective types of corruption.",3.4. Predictive likelihood results,[0],[0]
"RPMs improve prediction for both clean and corrupted data, as they focus on data that match the assumptions of the original model.",3.4. Predictive likelihood results,[0],[0]
"Finally, we show how RPMs handle skewed data.",3.5. Skewed data: cluster selection in a mixture model,[0],[0]
"The Dirichlet process mixture model (DPMM) is a versatile model for density estimation and clustering (Bishop, 2006;
Murphy, 2012).",3.5. Skewed data: cluster selection in a mixture model,[0],[0]
"While real data may indeed come from a finite mixture of clusters, there is no reason to assume each cluster is distributed as a Gaussian.",3.5. Skewed data: cluster selection in a mixture model,[0],[0]
"Inspired by the experiments in Miller & Dunson (2015), we show how a reweighted DPMM reliably recovers the correct number of components in a mixture of skewnormals dataset.
",3.5. Skewed data: cluster selection in a mixture model,[0],[0]
"A standard Gaussian mixture model (GMM) with large K and a sparse Dirichlet prior on the mixture proportions is an approximation to a DPMM (Ishwaran & James, 2012).",3.5. Skewed data: cluster selection in a mixture model,[0],[0]
We simulate three clusters from two-dimensional skewnormal distributions and fit a GMM with maximum K D 30.,3.5. Skewed data: cluster selection in a mixture model,[0],[0]
"Here we use automatic differentiation variational inference (ADVI), as NUTS struggles with inference of mixture models (Kucukelbir et al., 2017).",3.5. Skewed data: cluster selection in a mixture model,[0],[0]
"(Details in Appendix D.4.)
",3.5. Skewed data: cluster selection in a mixture model,[0],[0]
Figure 7 shows posterior mean estimates from the original GMM; it incorrectly finds six clusters.,3.5. Skewed data: cluster selection in a mixture model,[0],[0]
"In contrast, the RPM identifies the correct three clusters.",3.5. Skewed data: cluster selection in a mixture model,[0],[0]
Datapoints in the tails of each cluster get down-weighted; these are datapoints that do not match the Gaussianity assumption of the model.,3.5. Skewed data: cluster selection in a mixture model,[0],[0]
We now turn to a study of real data: a recommendation system.,4. Case Study: Poisson factorization for recommendation,[0],[0]
Consider a video streaming service; data comes as a binary matrix of users and the movies they choose to watch.,4. Case Study: Poisson factorization for recommendation,[0],[0]
How can we identify patterns from such data?,4. Case Study: Poisson factorization for recommendation,[0],[0]
"Poisson factorization (PF) offers a flexible solution (Cemgil, 2009; Gopalan et al., 2015).",4. Case Study: Poisson factorization for recommendation,[0],[0]
"The idea is to infer a K-dimensional
latent space of user preferences ✓ and movie attributes ˇ.",4. Case Study: Poisson factorization for recommendation,[0],[0]
The inner product ✓>ˇ determines the rate of a Poisson likelihood for each binary measurement; Gamma priors on ✓ and ˇ promote sparse patterns.,4. Case Study: Poisson factorization for recommendation,[0],[0]
"As a result, PF finds interpretable groupings of movies, often clustered according to popularity or genre.",4. Case Study: Poisson factorization for recommendation,[0],[0]
"(Full model in Appendix E.)
How does classical PF compare to its reweighted counterpart?",4. Case Study: Poisson factorization for recommendation,[0],[0]
"As input, we use the MovieLens 1M dataset, which contains one million movie ratings from 6 000 users on 4 000 movies.",4. Case Study: Poisson factorization for recommendation,[0],[0]
We place iid Gamma.1; 0:001/ priors on the preferences and attributes.,4. Case Study: Poisson factorization for recommendation,[0],[0]
"Here, we have the option of reweighting users or items.",4. Case Study: Poisson factorization for recommendation,[0],[0]
We focus on users and place a Beta.100; 1/,4. Case Study: Poisson factorization for recommendation,[0],[0]
prior on their weights.,4. Case Study: Poisson factorization for recommendation,[0],[0]
"For this model, we use MAP estimation.",4. Case Study: Poisson factorization for recommendation,[0],[0]
"(Localization is computationally challenging for PF; it requires a separate “copy” of ✓ for each movie, along with a separate ˇ for each user.",4. Case Study: Poisson factorization for recommendation,[0],[0]
"This dramatically increases computational cost.)
",4. Case Study: Poisson factorization for recommendation,[0],[0]
We begin by analyzing the original (clean) dataset.,4. Case Study: Poisson factorization for recommendation,[0],[0]
"Reweighting improves the average held-out log likelihood from 1:68 of the original model to 1:53 of the corre-
sponding RPM.",4. Case Study: Poisson factorization for recommendation,[0],[0]
The boxplot in Figure 8a shows the inferred weights.,4. Case Study: Poisson factorization for recommendation,[0],[0]
"The majority of users receive weight one, but a few users are down-weighted.",4. Case Study: Poisson factorization for recommendation,[0],[0]
These are film enthusiasts who appear to indiscriminately watch many movies from many genres.,4. Case Study: Poisson factorization for recommendation,[0],[0]
(Appendix F shows an example.),4. Case Study: Poisson factorization for recommendation,[0],[0]
"These users do not contribute towards identifying movies that go together; this explains why the RPM down-weights them.
",4. Case Study: Poisson factorization for recommendation,[0],[0]
Recall the example from our introduction.,4. Case Study: Poisson factorization for recommendation,[0],[0]
"A child typically watches popular animated films, but her parents occasionally use her account to watch horror films.",4. Case Study: Poisson factorization for recommendation,[0],[0]
We simulate this by corrupting a small percentage of users.,4. Case Study: Poisson factorization for recommendation,[0],[0]
"We replace a ratio R D .0:1; 0:5; 1/ of these users’ movies with randomly selected movies.
",4. Case Study: Poisson factorization for recommendation,[0],[0]
"The boxplot in Figure 8b shows the weights we infer for these corrupted users, based on how many of their movies we randomly replace.",4. Case Study: Poisson factorization for recommendation,[0],[0]
The weights decrease as we corrupt more movies.,4. Case Study: Poisson factorization for recommendation,[0],[0]
Table 3 shows how this leads to higher heldout predictive accuracy; down-weighting these corrupted users leads to better prediction.,4. Case Study: Poisson factorization for recommendation,[0],[0]
Reweighted probabilistic models (RPM) offer a systematic approach to mitigating various forms of mismatch with reality.,5. Discussion,[0],[0]
The idea is to raise each data likelihood to a weight and to infer the weights along with the hidden patterns.,5. Discussion,[0],[0]
"We demonstrate how this strategy introduces robustness and improves prediction accuracy across four types of mismatch.
RPMs also offer a way to detect mismatch with reality.",5. Discussion,[0],[0]
The distribution of the inferred weights sheds light onto datapoints that fail to match the original model’s assumptions.,5. Discussion,[0],[0]
"RPMs can thus lead to new model development and deeper insights about our data.
RPMs can also work with non-exchangeable data, such as time series.",5. Discussion,[0],[0]
"Some time series models admit exchangeable likelihood approximations (Guinness & Stein, 2013).",5. Discussion,[0],[0]
"For other models, a non-overlapping windowing approach would also work.",5. Discussion,[0],[0]
"The idea of reweighting could also extend to structured likelihoods, such as Hawkes process models.",5. Discussion,[0],[0]
"We thank Adji Dieng, Yuanjun Gao, Inchi Hu, Christian Naesseth, Rajesh Ranganath, Francisco Ruiz, and Dustin Tran for their insightful comments.",Acknowledgements,[0],[0]
"This work is supported by NSF IIS-1247664, ONR N00014-11-1-0651, DARPA PPAML FA8750-14-2-0009, DARPA SIMPLEX N6600115-C-4032, and the Alfred P. Sloan Foundation.",Acknowledgements,[0],[0]
Probabilistic models analyze data by relying on a set of assumptions.,abstractText,[0],[0]
Data that exhibit deviations from these assumptions can undermine inference and prediction quality.,abstractText,[0],[0]
Robust models offer protection against mismatch between a model’s assumptions and reality.,abstractText,[0],[0]
We propose a way to systematically detect and mitigate mismatch of a large class of probabilistic models.,abstractText,[0],[0]
The idea is to raise the likelihood of each observation to a weight and then to infer both the latent variables and the weights from data.,abstractText,[0],[0]
Inferring the weights allows a model to identify observations that match its assumptions and down-weight others.,abstractText,[0],[0]
This enables robust inference and improves predictive accuracy.,abstractText,[0],[0]
"We study four different forms of mismatch with reality, ranging from missing latent groups to structure misspecification.",abstractText,[0],[0]
A Poisson factorization analysis of the Movielens 1M dataset shows the benefits of this approach in a practical scenario.,abstractText,[0],[0]
Robust Probabilistic Modeling with Bayesian Data Reweighting,title,[0],[0]
"In machine learning and statistics, a linear model of the form y = ⟨θ∗,x⟩+ϵ is widely used to find the relationship between feature and response, which has gained overwhelming popularity for a very long time.",1. Introduction,[0],[0]
Here y ∈ R and x ∈,1. Introduction,[0],[0]
"Rp is the pair of observed response and feature/measurement vector, ϵ is a zero-mean noise, and θ∗ ∈",1. Introduction,[0],[0]
Rp is the unknown parameter to be estimated.,1. Introduction,[0],[0]
"The simplicity of linear model leads to its great interpretability and computational efficiency, which are often favored in practical applications.",1. Introduction,[0],[0]
"On theoretical side, even in high-dimensional regime where sample size is smaller than the problem dimension p, strong statistical guarantees have been established under mild assumptions for various estimators, such as Lasso (Tibshirani, 1996) and Dantzig selector (Candes & Tao, 2007).",1. Introduction,[0],[0]
"Despite its attractive merits, one main drawback of linear models is the stringent assumption of linear relationship between x and y, which may fail to hold in com-
1Department of Computer Science & Engineering, University of Minnesota-Twin Cities, Minnesota, USA.",1. Introduction,[0],[0]
"Correspondence to: Sheng Chen <shengc@cs.umn.edu>, Arindam Banerjee <banerjee@cs.umn.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
plicated scenarios.",1. Introduction,[0],[0]
"To introduce more flexibility, one option is to consider the general single-index models (SIMs) (Ichimura, 1993; Horowitz & Hardle, 1996),
E[y|x] = f∗(⟨θ∗,x⟩) , (1)
",1. Introduction,[0],[0]
where f∗ :,1. Introduction,[0],[0]
R 7→ R is an unknown univariate transfer function (a.k.a. link function).,1. Introduction,[0],[0]
"This class of models enjoys rich modeling power in the sense that it encompasses several useful models as special cases, which are briefly described below:
• One-bit Compressed Sensing: In one-bit compressed sensing (1-bit CS) (Boufounos & Baraniuk, 2008; Plan & Vershynin, 2013), the response y is restricted to be binary, i.e., y ∈ {+1,−1}, and the range of transfer function f∗ is [−1, 1].",1. Introduction,[0],[0]
"Given the measurement vector x, one can generate y from the Bernoulli model,
y + 1
2 ∼ Ber
( f∗(⟨θ∗,x⟩) + 1
2
) .",1. Introduction,[0],[0]
"(2)
In the noiseless case, f∗(z) = sign(z) and y always reflects the true sign of ⟨θ∗,x⟩, while y can be incorrect for other f∗ whose shape determines the noise level in some way.",1. Introduction,[0],[0]
•,1. Introduction,[0],[0]
Generalized Linear Models:,1. Introduction,[0],[0]
"In generalized linear models (GLMs) (McCullagh, 1984), the transfer function is assumed to be monotonically increasing and conditional distribution of y|x belongs to exponential family.",1. Introduction,[0],[0]
Different choices of f∗ give rise to different members in GLMs.,1. Introduction,[0],[0]
If f∗ is identity function f∗(z),1. Introduction,[0],[0]
"= z, one has the simple linear models, while the sigmoid function f∗(z) =",1. Introduction,[0],[0]
11+e−z results in the logistic model for binary classification.,1. Introduction,[0],[0]
"In this work, however, we have no access to exact f∗ other than knowing it is monotonic.",1. Introduction,[0],[0]
•,1. Introduction,[0],[0]
"Noise in Monotone Transfer: Instead of having the general expectation form of y as GLMs, one could directly introduce the noise inside monotone transfer f̃ to model the randomness of y (Plan et al., 2016),
y = f̃ (⟨θ∗,x⟩+ ϵ) .",1. Introduction,[0],[0]
"(3)
In this setting, the transfer function f̃ is slightly different from the f∗ in (1), which are related by f∗(z) = Eϵ[f̃(z + ϵ)|z].
",1. Introduction,[0],[0]
A key advantage of SIM is its robustness.,1. Introduction,[0],[0]
"First, allowing unknown f∗ prevents the mis-specification of transfer function, which could otherwise lead to a poor estimate of θ∗.",1. Introduction,[0],[0]
"Secondly, the model in (1) makes minimal assumption on the distribution of y, thus being able to tolerate potentially heavy-tailed noise.
",1. Introduction,[0],[0]
"In order to estimate θ∗, we are given n measurements of (x, y) ∈",1. Introduction,[0],[0]
"Rp × R, denoted by {(xi, yi)}ni=1.",1. Introduction,[0],[0]
"In this work, we focus on the n < p regime.",1. Introduction,[0],[0]
"In such high-dimensional setting, the recovery of θ∗ is quite challenging as the problem is ill-posed even when f∗ is given.",1. Introduction,[0],[0]
"Over the last decade, substantial progress has been made to address the challenge by exploiting the apriori structure of parameter θ∗, like sparsity (Tibshirani, 1996).",1. Introduction,[0],[0]
"For simple linear models or GLMs with known transfer, extensive studies have shown that sparse θ∗ can be consistently estimated under mild assumptions, with much lower sample complexity than p (Candes & Tao, 2007; Wainwright, 2009; Bickel et al., 2009; Kakade et al., 2010; Negahban et al., 2012; Yang et al., 2016).",1. Introduction,[0],[0]
"Recently the notion of structure has been suitably generalized beyond the unstructured sparsity (Bach et al., 2012), and Gaussian width (Gordon, 1985) has emerged as a useful measure to characterize the structural complexity which further determines the recovery guarantee of θ∗ (Chandrasekaran et al., 2012; Rao et al., 2012; Oymak et al., 2013; Amelunxen et al., 2014; Banerjee et al., 2014; Chatterjee et al., 2014; Vershynin, 2015; Tropp, 2015; Chen & Banerjee, 2016).
",1. Introduction,[0],[0]
"In the absence of exact f∗, though 1-bit CS and related variants were well-studied in recent years (Boufounos & Baraniuk, 2008; Jacques et al., 2013; Plan & Vershynin, 2013; Gopi et al., 2013; Zhang et al., 2014; Chen & Banerjee, 2015a; Zhu & Gu, 2015; Yi et al., 2015; Slawski & Li, 2015; Li, 2016; Slawski & Li, 2016), the exploration of general SIMs or the cases with monotone transfers is relatively limited, especially in the high-dimensional regime.",1. Introduction,[0],[0]
"Kalai & Sastry (2009) and Kakade et al. (2011) investigated the low-dimensional SIMs with monotone transfers, and they proposed perceptron-type algorithms to estimate both f∗ and θ∗, with provable guarantees on prediction error.",1. Introduction,[0],[0]
"In high dimension, general SIMs were studied by Alquier & Biau (2013) and Radchenko (2015), in which only unstructured sparsity of θ∗ is considered.",1. Introduction,[0],[0]
"The algorithm developed in (Alquier & Biau, 2013) relies on reversible jump MCMC, which could be slow.",1. Introduction,[0],[0]
"In Radchenko (2015), a path fitting algorithm is designed to recover f∗ and θ∗, but only asymptotic guarantees are provided.",1. Introduction,[0],[0]
"Ganti et al. (2015) considered the high-dimensional setting with monotone transfer, and their iterative algorithm is based on non-convex optimization, for which it is hard to establish the convergence.",1. Introduction,[0],[0]
"Besides, the prediction error bound they derived is also weak (in the sense that it
is even worse than the initialization of the algorithm).",1. Introduction,[0],[0]
"Recently Oymak & Soltanolkotabi (2016) proposed a constrained least-squares method to estimate θ∗, with recovery error characterized by Gaussian width and related quantities.",1. Introduction,[0],[0]
"Though their analysis considered the general structure of θ∗, it only holds for noiseless setting where y = f(⟨θ∗,x⟩).",1. Introduction,[0],[0]
General structure of θ∗ was also explored in Vershynin (2015) and Plan et al. (2016).,1. Introduction,[0],[0]
"Other types of statistical guarantees for high-dimensional SIMs is also available, such as support recovery of θ∗ in Neykov et al. (2016).",1. Introduction,[0],[0]
"It is worth noting that all the aforementioned statistical analyses rely on sub-Gaussian noise or the transfer function being bounded or Lipschitz, which indicates that none of the results can immediately hold for heavy-tailed noise (or without Lipschitzness and boundedness).
",1. Introduction,[0],[0]
"In this paper, we focus on the parameter estimation of θ∗ instead of the prediction of y given new x.",1. Introduction,[0],[0]
"In particular, we propose two families of generalized estimators, constrained and regularized, for model (1) under Gaussian measurement.",1. Introduction,[0],[0]
"The parameter θ∗ is assumed to possess certain lowcomplexity structure, which can be either captured by a constraint θ∗ ∈ K or a norm regularization term ∥θ∗∥.",1. Introduction,[0],[0]
"Our general approach is inspired by U -statistics and the advances in 1-bit CS, and subsumes several existing 1-bit CS algorithms as special cases.",1. Introduction,[0],[0]
"Similar to those algorithms, our estimator is simple and often admits closed-form solutions.",1. Introduction,[0],[0]
"Regarding the recovery analysis, there are two appealing aspects.",1. Introduction,[0],[0]
"First our results work for general structure, with error bound characterized by Gaussian width and some other easy-to-compute geometric measures.",1. Introduction,[0],[0]
"Instantiating our results with specific structure of θ∗ recovers previously established error bounds for 1-bit CS (Zhang et al., 2014; Chen & Banerjee, 2015a), which are sharper than those yielded by the general analysis in Plan & Vershynin (2013).",1. Introduction,[0],[0]
"Second, our analysis works with limited assumptions on the condition distribution of y.",1. Introduction,[0],[0]
"In particular, our estimator is robust to heavy-tailed noise and permit unbounded transfer functions f∗ as well as non-Lipschitz ones.",1. Introduction,[0],[0]
"At the heart of our analysis is the generic chaining method (Talagrand, 2014), an advanced tool in probability theory, which has been successfully applied to sparse recovery (Koltchinskii, 2011) and dimensionality reduction (Dirksen, 2016), etc.",1. Introduction,[0],[0]
"Another key ingredient in our proof is a Hoeffding-type concentration inequality for U -statistics (Lee, 1990) with sub-Gaussian tails, which is less known yet generalizes the popular one for bounded U -statistics (Hoeffding, 1963).",1. Introduction,[0],[0]
"Apart from 1-bit CS, we particularly investigate the model (3), for which the generalized estimator is specialized in a novel way.",1. Introduction,[0],[0]
"The resulting estimator better leverages the monotonicity of the transfer function, which is also demonstrated through experiments.",1. Introduction,[0],[0]
"For the ease of exposition, whenever we say “monotone”, it means “monotonically increasing” by default.",1. Introduction,[0],[0]
"Throughout the
paper, we will use c, C,C ′, C0, C1 and so on to denote absolute constants, which may differ from context to context.",1. Introduction,[0],[0]
"Detailed proofs are deferred to the supplementary material due to page limit.
",1. Introduction,[0],[0]
The rest of the paper is organized as follows.,1. Introduction,[0],[0]
"In Section 2, we introduce our estimators for SIMs along with their recovery guarantees.",1. Introduction,[0],[0]
We also provide a few examples in 1-bit CS for illustration.,1. Introduction,[0],[0]
"Section 3 is focused on model (3), for which we instantiate the general results in a new way.",1. Introduction,[0],[0]
Other structures of θ∗ beyond unstructured sparsity are also discussed.,1. Introduction,[0],[0]
Section 4 provides the proof of our main results and the related lemmas.,1. Introduction,[0],[0]
"In Section 5, we complement our theoretical developments with some experiment results.",1. Introduction,[0],[0]
The final section is dedicated to conclusions.,1. Introduction,[0],[0]
"For the sake of identifiability, we assume w.l.o.g.",2.1. Assumptions and Preliminaries,[0],[0]
that ∥θ∗∥2 = 1 throughout the paper.,2.1. Assumptions and Preliminaries,[0],[0]
"At the first glimpse of model (1), we may realize that it is difficult to recover θ∗ due to unknown f∗.",2.1. Assumptions and Preliminaries,[0],[0]
"In contrast, when f∗ is given, the recovery guarantees of θ∗ can be established under mild assumptions of x and y, such as boundedness or subGaussianity.",2.1. Assumptions and Preliminaries,[0],[0]
"If we know certain properties of the transfer function like the monotonicity introduced in GLMs and (3), the structure of f∗ is largely restricted, and it is tempting to expect that similar results will continue to hold.",2.1. Assumptions and Preliminaries,[0],[0]
"Unfortunately, we first have the following claim, which indicates that without other constraints on f∗ beyond strict monotonicity, θ∗ cannot be consistently estimated under general sub-Gaussian (or bounded) measurement, even in the noiseless setting of (3).
",2.1. Assumptions and Preliminaries,[0],[0]
Claim 1 Suppose that each element xi of x is sampled i.i.d.,2.1. Assumptions and Preliminaries,[0],[0]
"from Rademacher distribution, i.e., P(xi = 1)",2.1. Assumptions and Preliminaries,[0],[0]
=,2.1. Assumptions and Preliminaries,[0],[0]
P(xi = −1) = 0.5.,2.1. Assumptions and Preliminaries,[0],[0]
"Under model (3) with noise ϵ = 0, there exists a θ̄ ∈ Sp−1 together with a monotone f̄ , such that supp(θ̄) = supp(θ∗) and yi = f̄(⟨θ̄,xi⟩) for data {(xi, yi)}ni=1 with arbitrarily large sample size n, while ∥θ̄ − θ∗∥2",2.1. Assumptions and Preliminaries,[0],[0]
>,2.1. Assumptions and Preliminaries,[0],[0]
δ,2.1. Assumptions and Preliminaries,[0],[0]
"for some constant δ.
",2.1. Assumptions and Preliminaries,[0],[0]
"Now that consistent estimation of θ∗ is not possible for general sub-Gaussian measurement, it might be reasonable to focus on certain special cases.",2.1. Assumptions and Preliminaries,[0],[0]
"For this work, we assume that x is standard Gaussian N (0, I).",2.1. Assumptions and Preliminaries,[0],[0]
"For SIM (1), we additionally assume that the distribution of y depends on x only through the value of ⟨θ∗,x⟩, i.e., the distribution of y|x is fixed if ⟨θ∗,x⟩ is given (no matter what the exact x is).",2.1. Assumptions and Preliminaries,[0],[0]
"This assumption is quite minimal, and it turns out that the examples we provide in Section 1 all satisfy it (if noise ϵ is independent of x in (3)).",2.1. Assumptions and Preliminaries,[0],[0]
"The same assumption is used
in Plan et al. (2016) as well.
",2.1. Assumptions and Preliminaries,[0],[0]
"Under the assumptions above, given m i.i.d. observations (x1, y1), . . .",2.1. Assumptions and Preliminaries,[0],[0]
", (xm, ym), we define
u ((x1, y1), . . .",2.1. Assumptions and Preliminaries,[0],[0]
", (xm, ym))",2.1. Assumptions and Preliminaries,[0],[0]
=,2.1. Assumptions and Preliminaries,[0],[0]
"m∑ i=1 qi (y1, . . .",2.1. Assumptions and Preliminaries,[0],[0]
", ym) ·",2.1. Assumptions and Preliminaries,[0],[0]
"xi ,
(4) where all qi :",2.1. Assumptions and Preliminaries,[0],[0]
"Rm 7→ R are bounded functions with |qi| ≤ 1, which are chosen along with m based on the properties of the transfer function.",2.1. Assumptions and Preliminaries,[0],[0]
"In Section 2.4 and 3.1, we will see examples for their choices.",2.1. Assumptions and Preliminaries,[0],[0]
The vector u ∈,2.1. Assumptions and Preliminaries,[0],[0]
"Rp is critical due to the key observation below.
",2.1. Assumptions and Preliminaries,[0],[0]
"Lemma 1 Suppose the distribution of y in model (1) depends on x through ⟨θ∗,x⟩",2.1. Assumptions and Preliminaries,[0],[0]
"and we define accordingly
bi (z1, . . .",2.1. Assumptions and Preliminaries,[0],[0]
", zm;θ ∗) = (5)
E",2.1. Assumptions and Preliminaries,[0],[0]
"[qi (y1, . .",2.1. Assumptions and Preliminaries,[0],[0]
.,2.1. Assumptions and Preliminaries,[0],[0]
", ym) |⟨θ∗,x1⟩ = z1, . . .",2.1. Assumptions and Preliminaries,[0],[0]
", ⟨θ∗,xm⟩ = zm] .
",2.1. Assumptions and Preliminaries,[0],[0]
"With x being standard Gaussian N (0, I), u defined in (4) satisfies
E",2.1. Assumptions and Preliminaries,[0],[0]
"[u ((x1, y1), . . .",2.1. Assumptions and Preliminaries,[0],[0]
", (xm, ym))]",2.1. Assumptions and Preliminaries,[0],[0]
"= βθ∗ , (6)
where β = ∑m i=1 E[bi (g1, . . .",2.1. Assumptions and Preliminaries,[0],[0]
", gm;θ∗) · gi], and g1, . . .",2.1. Assumptions and Preliminaries,[0],[0]
", gm are i.i.d.",2.1. Assumptions and Preliminaries,[0],[0]
"standard Gaussian.
",2.1. Assumptions and Preliminaries,[0],[0]
"Note that Lemma 1 is true for all choices of qi, and the proof is given in the supplement.",2.1. Assumptions and Preliminaries,[0],[0]
"This lemma presents an insight towards the design of our estimator, that is, the direction of θ∗ can be approximated if we have a good sense about Eu.",2.1. Assumptions and Preliminaries,[0],[0]
"As we will see in the sequel, the scalar β plays a key role in the estimation error bound, which can give us clues to the choice of qi.",2.1. Assumptions and Preliminaries,[0],[0]
We can assume w.l.o.g.,2.1. Assumptions and Preliminaries,[0],[0]
that β ≥ 0,2.1. Assumptions and Preliminaries,[0],[0]
"since we can flip the sign of each qi.
",2.1. Assumptions and Preliminaries,[0],[0]
"The recovery analysis is built on the notion of Gaussian width (Gordon, 1985), which is defined for any A ⊆ Rp as w(A) = E[supv∈A⟨g,v⟩], where g is a standard Gaussian random vector.",2.1. Assumptions and Preliminaries,[0],[0]
"Roughly speaking, w(A) measures the scaled width of set A averaged over each direction.",2.1. Assumptions and Preliminaries,[0],[0]
"Inspired by Lemma 1, we define the vector û for the observed data {(xi, yi)}ni=1,
û = (n−m)!
n!
∑ 1≤i1,...,im≤n i1 ̸=...",2.2. Generalized Estimator,[0],[0]
"̸=im u ((xi1 , yi1), . . .",2.2. Generalized Estimator,[0],[0]
", (xim , yim)) ,
(7) which is an unbiased estimator of Eu, meaning that Eû = Eu = βθ∗. When m = 2, we essentially have
û = 1 n(n− 1) ∑
1≤i,j≤n i ̸=j
u ((xi, yi), (xj , yj)) (8)
In fact, û can be treated as a vector version of U -statistics with order m.",2.2. Generalized Estimator,[0],[0]
"Given û, a naive way to estimate θ∗ is to simply normalize û, i.e., θ̂ = û/∥û∥2.",2.2. Generalized Estimator,[0],[0]
"In highdimensional setting, θ∗ is often structured, but the naive estimator fails to take such information into account, which would lead to large error.",2.2. Generalized Estimator,[0],[0]
"To incorporate the prior knowledge on θ∗, we design two types of estimator, the constrained one and the regularized one.
",2.2. Generalized Estimator,[0],[0]
Constrained Estimator:,2.2. Generalized Estimator,[0],[0]
"If we assume that θ∗ belongs to some structured set K ⊆ Sp−1, then the estimation of θ∗ is carried out via the constrained optimization
θ̂ = argmin θ∈Rp
− ⟨û,θ⟩ s.t. θ ∈ K .",2.2. Generalized Estimator,[0],[0]
"(9)
Here the set K can be non-convex, as long as the optimization can be solved globally.",2.2. Generalized Estimator,[0],[0]
"Since the objective function is very simple, we can often end up with a global minimizer.",2.2. Generalized Estimator,[0],[0]
"Similar estimator has been used in Plan et al. (2016), but they only focused on specific û.
Regularized Estimator: If we assume that the structure of θ∗ can be captured by certain norm ∥ · ∥, we may alternatively use the regularized estimator to find θ∗,
θ̂ = argmin θ∈Rp
− ⟨û,θ⟩+",2.2. Generalized Estimator,[0],[0]
λ∥θ∥ s.t. ∥θ∥2 ≤ 1 .,2.2. Generalized Estimator,[0],[0]
"(10)
",2.2. Generalized Estimator,[0],[0]
"The optimization is convex, thus the global minimum is always attained.",2.2. Generalized Estimator,[0],[0]
"Previously this estimator was used in 1-bit CS scenario with L1 norm (Zhang et al., 2014).",2.2. Generalized Estimator,[0],[0]
"Regarding the constrained estimator, the recovery of θ∗ relies on the geometry of θ̂, which is described by
AK(θ∗) = cone { v ∣∣∣ v = θ̂ − θ∗, θ̂ ∈ K} ∩",2.3. Recovery Analysis,[0],[0]
"Sp−1
(11)",2.3. Recovery Analysis,[0],[0]
The set AK(θ∗) essentially contains all possible directions that error θ̂,2.3. Recovery Analysis,[0],[0]
− θ∗ could lie in.,2.3. Recovery Analysis,[0],[0]
"The following theorem characterizes the error of θ̂.
Theorem 1 Suppose that the optimization (9) can be solved to global minimum.",2.3. Recovery Analysis,[0],[0]
"Then the following error bound holds for the minimizer θ̂ with probability at least 1 − C ′′ exp ( −w2 (AK(θ∗)) ) ,
∥∥∥θ̂",2.3. Recovery Analysis,[0],[0]
− θ∗∥∥∥ 2 ≤,2.3. Recovery Analysis,[0],[0]
Cκm 3 2 β · w(AK(θ ∗)),2.3. Recovery Analysis,[0],[0]
"+ C ′√ n , (12)
where κ is the sub-Gaussian norm of a standard Gaussian random variable, and C, C ′, C ′′ are all absolute constant.
",2.3. Recovery Analysis,[0],[0]
Remark: Note that estimator is consistent as long as β ̸= 0.,2.3. Recovery Analysis,[0],[0]
"The error bound inversely depends on the scale of β,
which implies that we should construct suitable qi such that β is large according to its definition in Lemma 1.",2.3. Recovery Analysis,[0],[0]
"The choice of qi further depends on the assumed property of f∗. Though dependency on m may prevent us from using higher-order u, m is typically small in practice and can be treated as constant.
",2.3. Recovery Analysis,[0],[0]
"For regularized estimator, we can similarly establish the recovery guarantee in terms of Gaussian width.
",2.3. Recovery Analysis,[0],[0]
"Theorem 2 Define the following set for any ρ > 1, Aρ (θ∗) = cone { v ∣∣∣",2.3. Recovery Analysis,[0],[0]
"∥v + θ∗∥ ≤ ∥θ∗∥+ ∥v∥
ρ
} ∩",2.3. Recovery Analysis,[0],[0]
"Sp−1
If we set λ = ρ ∥û− βθ∗∥∗ = O(ρm3/2w(B∥·∥)/ √ n) and it satisfies λ",2.3. Recovery Analysis,[0],[0]
<,2.3. Recovery Analysis,[0],[0]
"∥û∥∗, then with probability at least 1− C ′ exp ( −w2 ( B∥·∥
)) , θ̂ in (10) satisfies∥∥∥θ̂",2.3. Recovery Analysis,[0],[0]
"− θ∗∥∥∥
2 ≤ C(1 + ρ)κm
3 2 β · Ψ(Aρ(θ∗)) ·",2.3. Recovery Analysis,[0],[0]
"w
( B∥·∥ )",2.3. Recovery Analysis,[0],[0]
"√ n ,
(13) where Ψ(Aρ(θ∗)) = supv∈Aρ(θ∗) ∥v∥ and B∥·∥ = {v | ∥v∥ ≤ 1} is the unit ball of norm ∥ · ∥.
Remark: The geometry of the regularized estimator is slightly different from the constrained one.",2.3. Recovery Analysis,[0],[0]
"Instead of having AK(θ∗), here the set Aρ(θ∗) depends on the choice of the regularization parameter λ.",2.3. Recovery Analysis,[0],[0]
"The same phenomenon also appears in the (Banerjee et al., 2014).",2.3. Recovery Analysis,[0],[0]
"The geometric measure Ψ(Aρ(θ∗)) is called restricted norm compatibility, which is non-random.",2.3. Recovery Analysis,[0],[0]
"For many interesting cases, it is easy to calculate (Negahban et al., 2012; Chen & Banerjee, 2015b).",2.3. Recovery Analysis,[0],[0]
"For 1-bit CS problem (2), the u defined in (4) can be chosen with m = 1 and qi = yi, ending up with
u ((x, y))",2.4. Application to 1-bit CS,[0],[0]
"= yx and û = 1
n n∑ i=1",2.4. Application to 1-bit CS,[0],[0]
yixi .,2.4. Application to 1-bit CS,[0],[0]
"(14)
By such choice of u, the β defined in Lemma 1 is simply β = E[f∗(g)g] with g being standard Gaussian random vector.",2.4. Application to 1-bit CS,[0],[0]
"Under reasonably mild noise, y is likely to take the sign of the linear measurement, which means that f∗(g) should be close to 1 (or -1)",2.4. Application to 1-bit CS,[0],[0]
if g is positive (or negative).,2.4. Application to 1-bit CS,[0],[0]
Thus we expect f∗(g)g to be positive most of time and β to be large.,2.4. Application to 1-bit CS,[0],[0]
"Given the choice of u, we can specialize our generalized constrained/regularized estimator to obtain previous results.",2.4. Application to 1-bit CS,[0],[0]
"If θ∗ is assumed to be s-sparse, for constrained estimator, we can choose a straightforward K = {θ | ∥θ∥0 ≤ s}∩Sp−1, which results in the k-support norm estimator (Chen & Banerjee, 2015a),
θ̂ks = argmin θ∈Rp",2.4. Application to 1-bit CS,[0],[0]
"− ⟨û,θ⟩ s.t. ∥θ∥0 ≤ s, ∥θ∥2 = 1 (15)
",2.4. Application to 1-bit CS,[0],[0]
"Though K is non-convex, the global minimizer can actually be obtained in closed form,
θ̂ksj =
{ ûj / ∥|û|↓1:s∥2 , if |ûj | is in |û| ↓",2.4. Application to 1-bit CS,[0],[0]
"1:s
0 , otherwise (16)
where |û|↓ is the absolute-value counterpart of û with entries sorted in descending order, and the subscript takes the top s entries.",2.4. Application to 1-bit CS,[0],[0]
"Similarly if the regularized estimator is instantiated with L1 norm ∥ · ∥1, we obtain the so-called passive algorithm introduced in Zhang et al. (2014),
θ̂ps = argmin θ∈Rp",2.4. Application to 1-bit CS,[0],[0]
"− ⟨û,θ⟩+",2.4. Application to 1-bit CS,[0],[0]
"λ∥θ∥1 s.t. ∥θ∥2 ≤ 1 , (17)
whose solution is given by θ̂ps = S (û, λ) /∥S",2.4. Application to 1-bit CS,[0],[0]
"(û, λ) ∥2, where S(·, ·) is the elementwise soft-thresholding operator, Si(û, λ) = max{sign(ûi)(|ûi|−λ), 0}.",2.4. Application to 1-bit CS,[0],[0]
"Based on Theorem 1 and 2, we can easily obtain the error bound for both ksupport norm estimator and passive algorithm.
",2.4. Application to 1-bit CS,[0],[0]
"Corollary 1 Assume that {(xi, yi)}ni=1 follow 1-bit CS model in (2) and û is given as (14).",2.4. Application to 1-bit CS,[0],[0]
"For any s-sparse θ∗, with high probability, θ̂ produced by both (15) and (17) (i.e., θ̂ks and θ̂ps) satisfy
∥∥∥θ̂",2.4. Application to 1-bit CS,[0],[0]
− θ∗∥∥∥ 2 ≤,2.4. Application to 1-bit CS,[0],[0]
O (√ s log p n ),2.4. Application to 1-bit CS,[0],[0]
"(18)
",2.4. Application to 1-bit CS,[0],[0]
The proof is included in the supplementary material.,2.4. Application to 1-bit CS,[0],[0]
"The above result was shown by Slawski & Li (2015) and Zhang et al. (2014), but their analyses do not consider the general structure.",2.4. Application to 1-bit CS,[0],[0]
"Compared with O( 4 √ s log p/n) yielded by the general result in Plan & Vershynin (2013), our bound is much sharper.",2.4. Application to 1-bit CS,[0],[0]
"In this section, we specifically study model (3).",3. A New Estimator for Monotone Transfer,[0],[0]
Here we further assume that f̃ is strictly increasing.,3. A New Estimator for Monotone Transfer,[0],[0]
What is worth mentioning is that the estimator we develop here can be applied to GLMs as well.,3. A New Estimator for Monotone Transfer,[0],[0]
"To avoid the confusion with u and û defined previously, we instead use new notations h and ĥ respectively in this section.",3. A New Estimator for Monotone Transfer,[0],[0]
"To motivate the design of h, it is helpful to rewrite model (3) by applying the inverse of f̃ on both sides,
f̃−1(y) = ⟨θ∗,x⟩+ ϵ .",3.1. Estimator with Second-Order ĥ,[0],[0]
"(19)
Note that the new formulation resembles the linear model except that we have no access to the value of f̃−1(y).",3.1. Estimator with Second-Order ĥ,[0],[0]
"Instead, all we know about r = [f̃−1(y1), . . .",3.1. Estimator with Second-Order ĥ,[0],[0]
", f̃−1(yn)]T ∈",3.1. Estimator with Second-Order ĥ,[0],[0]
Rn is that it preserves the ordering of y =,3.1. Estimator with Second-Order ĥ,[0],[0]
"[y1, . . .",3.1. Estimator with Second-Order ĥ,[0],[0]
", yn]T .
",3.1. Estimator with Second-Order ĥ,[0],[0]
"Put in another way, r needs to satisfy the constraint that ri > rj iff.",3.1. Estimator with Second-Order ĥ,[0],[0]
yi > yj and ri < rj iff.,3.1. Estimator with Second-Order ĥ,[0],[0]
yi < yj .,3.1. Estimator with Second-Order ĥ,[0],[0]
"To move one step further, it is equivalent to sign(yi − yj) = sign(ri−rj) = sign(⟨θ∗,xi−xj⟩+ϵi−ϵj) based on model assumption.",3.1. Estimator with Second-Order ĥ,[0],[0]
"Hence the information contained in sample {(xi, yi)}ni=1 can be interpreted from the perspective of 1- bit CS, where sign(yi − yj) reflects the perturbed sign of linear measurement ⟨θ∗,xi",3.1. Estimator with Second-Order ĥ,[0],[0]
"− xj⟩. Inspired by the u for 1-bit CS, we may choose m = 2 and define h, ĥ as
h ((x1, y1), (x2, y2))",3.1. Estimator with Second-Order ĥ,[0],[0]
"= sign(y1 − y2) · (x1 − x2) , (20)
ĥ = 1 n(n− 1) ∑
1≤i,j≤n i ̸=j
h ((xi, yi), (xj , yj)) , (21)
",3.1. Estimator with Second-Order ĥ,[0],[0]
"Given the definition of ĥ, Lemma 1 directly implies the following corollary.
",3.1. Estimator with Second-Order ĥ,[0],[0]
"Corollary 2 Suppose that (x1, y2) and (x2, y2) are generated by model (3), where x1,x2 follow Gaussian distribution N (0, I), and the noise ϵ1, ϵ2 are independent of x1,x2 and identically (but arbitrarily) distributed.",3.1. Estimator with Second-Order ĥ,[0],[0]
"Then the expectation of h ((x1, y1), (x2, y2)) satisfies
E",3.1. Estimator with Second-Order ĥ,[0],[0]
"[h ((x1, y1), (x2, y2))]",3.1. Estimator with Second-Order ĥ,[0],[0]
"= √ 2β′θ∗ , (22)
where β′ = Eg∼N (0,1) [ sign ( g + (ϵ1 − ϵ2)/ √ 2 ) · g ] .
",3.1. Estimator with Second-Order ĥ,[0],[0]
"Remark: The scalar √ 2β′ serves as the role of β in Lemma 1, and β′ is always guaranteed to be strictly positive regardless how the noise is distributed, which keeps θ∗ distinguishable all the time.",3.1. Estimator with Second-Order ĥ,[0],[0]
"To see this, let ξ = (ϵ1− ϵ2)/ √ 2.",3.1. Estimator with Second-Order ĥ,[0],[0]
"Note that ξ is symmetric, thus εξ has the same distribution as ξ, where ε is a Rademacher random variable.",3.1. Estimator with Second-Order ĥ,[0],[0]
"Therefore
β′",3.1. Estimator with Second-Order ĥ,[0],[0]
= E,3.1. Estimator with Second-Order ĥ,[0],[0]
"[sign (g + ξ) · g] = Eg,ξEε [sign (g + εξ) ·",3.1. Estimator with Second-Order ĥ,[0],[0]
"g] = EξEg [
sign (g − ξ) + sign (g + ξ) 2
· g ]
Since g(g − ξ) + g(g + ξ) = 2g2 ≥ 0, it follows that sign(g(g− ξ))+",3.1. Estimator with Second-Order ĥ,[0],[0]
sign(g(g+ ξ)) =,3.1. Estimator with Second-Order ĥ,[0],[0]
(sign(g− ξ)+ sign(g+ ξ)) ·,3.1. Estimator with Second-Order ĥ,[0],[0]
"sign(g) ≥ 0, thus (sign(g − ξ) + sign(g + ξ)) · g is always nonnegative.",3.1. Estimator with Second-Order ĥ,[0],[0]
"Find a large enough M > 0 such that P(|ξ| ≤ M) = 0.5 > 0, and we have
β′",3.1. Estimator with Second-Order ĥ,[0],[0]
= E,3.1. Estimator with Second-Order ĥ,[0],[0]
[sign (g + ξ) ·,3.1. Estimator with Second-Order ĥ,[0],[0]
"g] ≥ EξEg [|g| · I{|g| > |ξ|}]
≥ 0.5Eg [|g| · I{|g|",3.1. Estimator with Second-Order ĥ,[0],[0]
"> M}] = M
2 · P(|g| > M) > 0 .
",3.1. Estimator with Second-Order ĥ,[0],[0]
"In the ideal noiseless case, β′ achieve its maximum, β′max = E[sign(g)g] = E[|g|] = √ 2/π.",3.1. Estimator with Second-Order ĥ,[0],[0]
"In the worst case, if ϵ1 and ϵ2 are heavy-tailed and dominate g, then β′",3.1. Estimator with Second-Order ĥ,[0],[0]
≈ E [ sign ( (ϵ1 − ϵ2)/ √ 2 ) ·,3.1. Estimator with Second-Order ĥ,[0],[0]
g ],3.1. Estimator with Second-Order ĥ,[0],[0]
"≈ 0.
",3.1. Estimator with Second-Order ĥ,[0],[0]
Now we can instantiate the generalized estimator based on ĥ.,3.1. Estimator with Second-Order ĥ,[0],[0]
"For example, if θ∗ is s-sparse, we estimate it by
θ̂ = argmin θ∈Rp
− ⟨ĥ,θ⟩ s.t. ∥θ∥0 ≤ s, ∥θ∥2 = 1 (23)
which enjoys O (√ s log p/n )
error rate as shown in Corollary 1.",3.1. Estimator with Second-Order ĥ,[0],[0]
The regularized estimator can also be obtained with the same ĥ according to (17).,3.1. Estimator with Second-Order ĥ,[0],[0]
The bottleneck of computing θ̂ lies in the calculation of ĥ.,3.1. Estimator with Second-Order ĥ,[0],[0]
"A simple proposition below enables us to get ĥ in a fast manner.
",3.1. Estimator with Second-Order ĥ,[0],[0]
Proposition 1,3.1. Estimator with Second-Order ĥ,[0],[0]
"Given {(xi, yi)}ni=1, let π↓ be the permutation of {1, . . .",3.1. Estimator with Second-Order ĥ,[0],[0]
", n} such that yπ↓1",3.1. Estimator with Second-Order ĥ,[0],[0]
> yπ↓2,3.1. Estimator with Second-Order ĥ,[0],[0]
> .,3.1. Estimator with Second-Order ĥ,[0],[0]
. .,3.1. Estimator with Second-Order ĥ,[0],[0]
> yπ↓n .,3.1. Estimator with Second-Order ĥ,[0],[0]
"Then we have
ĥ = 2
n(n− 1) n∑ i=1",3.1. Estimator with Second-Order ĥ,[0],[0]
(n+ 1− 2i) · xπ↓i,3.1. Estimator with Second-Order ĥ,[0],[0]
"(24)
Remark: Based on the proposition above, ĥ can be efficiently computed in O(np+ n log n) time, i.e., O(n log n) time for sorting y and O(np) time for the weighted sum of all xi.",3.1. Estimator with Second-Order ĥ,[0],[0]
"This is a significant improvement compared with the the naive calculation using (21), which takes O(n2p) time.",3.1. Estimator with Second-Order ĥ,[0],[0]
"So far we have illustrated the Gaussian width based error bounds, viz (12) and (13), only through unstructured sparsity of θ∗.",3.2. Beyond Unstructured Sparsity,[0],[0]
"Here we provide two more examples, nonoverlapping group sparsity and fused sparsity.
",3.2. Beyond Unstructured Sparsity,[0],[0]
"Non-Overlapping Group Sparsity: Suppose the coordinates of θ∗ has been partitioned into K predefined disjoint groups G1, . . .",3.2. Beyond Unstructured Sparsity,[0],[0]
",GK ⊆ {1, 2, . . .",3.2. Beyond Unstructured Sparsity,[0],[0]
", p}, out of which only k groups are non-zero.",3.2. Beyond Unstructured Sparsity,[0],[0]
"If we use the regularized estimator with L2,1 norm ∥θ∥2,1 = ∑K i=1 ∥θGi∥2, the optimal solution can be similarly obtained as (17), with elementwise soft-thresholding replaced by the groupwise one.",3.2. Beyond Unstructured Sparsity,[0],[0]
"The related geometric measures that appears in (13) can be found in Banerjee et al. (2014), which are given by
Ψ(Aρ(θ∗)) ≤",3.2. Beyond Unstructured Sparsity,[0],[0]
"O( √ k) (25)
w",3.2. Beyond Unstructured Sparsity,[0],[0]
"( B∥·∥2,1 ) ≤",3.2. Beyond Unstructured Sparsity,[0],[0]
"O( √ logK + √ G) (26)
Fused Sparsity: θ∗ is said to be s-fused-sparse if the cardinality of the set F(θ∗) =",3.2. Beyond Unstructured Sparsity,[0],[0]
{1 ≤,3.2. Beyond Unstructured Sparsity,[0],[0]
i < p,3.2. Beyond Unstructured Sparsity,[0],[0]
"| θ∗i ̸= θ∗i+1} is smaller than s. If we resort to the constrained estimator (9) with K = {θ | |F(θ)| ≤ s, ∥θ∥2 = 1}, the associated optimization can be solved by dynamic programming (Bellman, 1961).",3.2. Beyond Unstructured Sparsity,[0],[0]
"The proposition below upper bounds the corresponding Gaussian width w(AK(θ∗)) in (12).
",3.2. Beyond Unstructured Sparsity,[0],[0]
"Proposition 2 For s-fused-sparse θ∗, the Gaussian width of set AK(θ∗) with K = {θ | |F(θ)| ≤ s, ∥θ∥2 = 1} satisfies
w(AK(θ∗))",3.2. Beyond Unstructured Sparsity,[0],[0]
"≤ O( √ s log p) (27)
",3.2. Beyond Unstructured Sparsity,[0],[0]
"The proof can be found in (Slawski & Li, 2016), and we provide a different one in supplementary material.",3.2. Beyond Unstructured Sparsity,[0],[0]
Here we first present the important technical lemmas that will be used in the proof of Theorem 1.,4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
The first one is the Hoeffding-type inequality for sub-Gaussian U -statistics.,4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
"In the literature, most of the studies are centered around bounded U -statistics, for which the celebrated concentration is established by Hoeffding (1963).",4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
Yet it is not easy to locate the counterpart for sub-Gaussian case.,4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
"Therefore we provide the following result and attach a proof in the supplementary material.
",4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
"Lemma 2 (Concentration for sub-Gaussian U -statistics) Define the U -statistic
Un,m(h) = (n−m)!
n!
∑ 1≤i1,...,im≤n i1 ̸=i2 ̸=...",4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
"̸=im h (zi1 , . . .",4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
", zim) (28)
with order m and kernel h :",4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
"Rd×m 7→ R based on n independent copies of random vector z ∈ Rd, denoted by z1, · · · , zn.",4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
"If h(·, . . .",4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
", ·) is sub-Gaussian with ∥h∥ψ2 ≤ κ, then the following inequality holds for Un,m(h) with any δ > 0,
P (|Un,m(h)− EUn,m(h)| > δ) ≤ 2 exp ( −C ⌊ n m ⌋ ·",4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
"δ 2 κ2 ) , (29) in which C is an absolute constant.
",4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
"As mentioned earlier in Section 1, generic chaining is the key tool that our analysis relies on.",4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
"Specifically we utilize Theorem 2.2.27 from (Talagrand, 2014).
",4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
Lemma 3 (Generic chaining concentration),4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
"Given metric space (T , s), if an associated stochastic process {Zt}t∈T has sub-Gaussian incremental, i.e., satisfies the condition
P (|Zu − Zv| ≥ δ) ≤ C exp ( − C ′δ2
s2(u,v)
) , ∀ u,v ∈ T ,
(30) then the following inequality holds
P (
sup u,v∈T
",4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
"|Zu − Zv| ≥ C1 (γ2(T , s) + δ · diam (T , s)) )
",4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
"≤ C2 exp ( −δ2 ) , (31)
where C,C ′, C1 and C2 are all absolute constants.
",4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
"The definition of the above γ2-functional γ2(·, ·) is complicated, and is not of great importance.",4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
"We refer interested
reader to the books, Talagrand (2005; 2014).",4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
"Loosely speaking, γ2(T , s) can be thought of as a measure for the size of set T under metric s. What really matters is the following relationship between γ2-functional and Gaussian width.",4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
"(see Theorem 2.4.1 in Talagrand (2014))
",4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
"Lemma 4 (Majorizing measures theorem) For any set T ⊆ Rp, the γ2-functional w.r.t. L2-metric and Gaussian width satisfy the following inequality with an absolute constant C0,
γ2 (T , ∥ · ∥2) ≤ C0 · w(T ) (32)
Equipped with these lemmas, we are ready to present the proof sketch of Theorem 1.",4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
"A complete proof is deferred to the supplementary material.
",4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
Proof Sketch of Theorem 1: We use the shorthand notation AK for the set AK(θ∗).,4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
"As θ̂ attains the global minimum of (9), we have
⟨θ̂ − θ∗, û⟩ ≥ 0 ⇐⇒ ⟨ θ̂ − θ∗, û
β − θ∗ + θ∗
⟩ ≥ 0
=⇒ ⟨θ̂,θ∗⟩",4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
≥ 1− ∥θ̂,4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
"− θ∗∥2 · sup v∈AK∪{0}
⟨ v, û
β − θ∗ ⟩",4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
"In order to bound the supremum above, we use the result from generic chaining.",4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
"We define the stochastic process {Zv = ⟨v, û/β − θ∗⟩}v∈AK∪{0}.",4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
"First, we need to check the process has sub-Gaussian incremental.",4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
"For simplicity, we denote u ((xi1 , yi1), . . .",4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
", (xim , yim)) by ui1,...,im .",4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
"By the definitions and properties of sub-Gaussian norm (Vershynin, 2012), it is not difficult to show that ∥⟨ui1,...,im ,v −w⟩∥ψ2 ≤",4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
"κm · ∥v −w∥2 for any v,w ∈ AK ∪ {0}.",4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
"By Lemma 2, we have
P (|Zv",4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
"− Zw| > δ) ≤ 2 exp ( −C ′ · nβ 2δ2
m3κ2 · ∥v −w∥22
) .
",4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
Therefore we can conclude that {Zv} has sub-Gaussian incremental w.r.t.,4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
"the metric s(v,w) , κm 32 ·",4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
∥v − w∥2/β √ n.,4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
"Now applying Lemma 3 to {Zv} with a bit calculation, we can obtain
P (
sup v∈AK∪{0}
|Zv| ≥ C1κm
3 2
β √ n
· ( γ2 (AK ∪ {0}, ∥ · ∥2)
+ 2δ ))",4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
"≤ C2 exp ( −δ2 ) Using γ2 (AK ∪ {0}, ∥ · ∥2) ≤",4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
C0 ·w (AK ∪ {0}) implied by Lemma 4 and taking δ = w,4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
"(AK ∪ {0}), we get
sup v∈AK∪{0}
⟨ v, û
β − θ∗
⟩ ≤",4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
"C3κm 3 2
β · w",4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
(AK) + C4√,4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
"n
with probability at least 1 − C2 exp ( −w2 (AK) ) .",4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
The inequality also uses the fact that w (AK ∪ {0}) ≤,4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
"w (AK) +
C4, which is a result of Lemma 2 in Maurer et al. (2014) (See Lemma A in supplementary material).",4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
"Lastly we turn to the quantity ∥θ̂ − θ∗∥2,
∥θ̂",4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
"− θ∗∥2 ≤ √ 2− 2⟨θ̂,θ∗⟩ ≤ 2C3κm 3 2
β · w (AK) + C4√",4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
"n .
",4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
"We finish the proof by letting C = 2C3, C ′ = C4 and C ′′ = C2.",4. Lemmas and Proof Sketch of Theorem 1,[0],[0]
"In the experiment, we focus on model (3) with sparse θ∗.",5. Experiment,[0],[0]
"The problem dimension is fixed as p = 1000, and the sparsity of θ∗ is set to 10.",5. Experiment,[0],[0]
"Essentially we generate our data (x, y) from
y = f̃ (⟨θ∗,x⟩+ ϵ) ,
where x ∼ N (0, I) and ϵ",5. Experiment,[0],[0]
"∼ N (0, σ2).",5. Experiment,[0],[0]
σ ranges from 0.3 to 1.5.,5. Experiment,[0],[0]
"We choose three monotonically increasing f̃ , f̃(z) = 1/(1 + exp(−z)) (which is bounded and Lipschitz), f̃(z) = z3 (which is unbounded and non-Lipschitz), and f̃(z) = log(1 + exp(z)) (which is unbounded but Lipschitz).",5. Experiment,[0],[0]
The sample size n varies from 200 to 1000.,5. Experiment,[0],[0]
We use the estimator (23) in Section 3.,5. Experiment,[0],[0]
"The baselines we compare with is the SILO and iSILO algorithm introduced in (Ganti et al., 2015).",5. Experiment,[0],[0]
SILO does not quite take the monotonicity in account.,5. Experiment,[0],[0]
"In fact, it is the special case of our generalized constrained estimator which uses the same choice of u as 1-bit CS.",5. Experiment,[0],[0]
"The original SILO use the constraint set {θ | ∥θ∥1 ≤ √ s, ∥θ∥2 ≤ 1}, which is computationally less efficient and statistically no better than K = {θ | ∥θ∥0 ≤ s} ∩",5. Experiment,[0],[0]
"Sp−1 (Zhang et al., 2014; Chen & Banerjee, 2015a).",5. Experiment,[0],[0]
Hence we also use K in SILO for a fair comparison.,5. Experiment,[0],[0]
iSILO relies on a specific implementation of isotonic regression which explicitly restricts the Lipschitz constant of f̃ to be one.,5. Experiment,[0],[0]
"To fit iSILO into our setting, we remove the Lipschitzness constraint and perform the standard isotonic regression.",5. Experiment,[0],[0]
"Since the convergence is not guaranteed for the iterative procedure of iSILO, the number of its iterations is fixed to 100.",5. Experiment,[0],[0]
"The best tuning parameter of iSILO is obtained by grid search.
",5. Experiment,[0],[0]
The experiment results are shown in Figure 1.,5. Experiment,[0],[0]
"Overall the iSILO algorithm works well under small noise, while our estimator has better performance when the variance of noise increases.",5. Experiment,[0],[0]
"To better demonstrate the robustness of our estimator to heavy-tailed noise, instead of Gaussian noise, we sample ϵ",5. Experiment,[0],[0]
from the Student’s t distribution with degrees of freedom equal to 3.,5. Experiment,[0],[0]
"We repeat the experiments for f̃(z) = z3, and obtain the plots in Figure 2.",5. Experiment,[0],[0]
We can see that the error of our estimator consistently decreases for all choice of σ as n increases.,5. Experiment,[0],[0]
"For SILO and iSILO, the errors are relatively large, and unable to shrink for large σ even when more data are provided.",5. Experiment,[0],[0]
"In this paper, we study the parameter estimation for the high-dimensional single-index models.",6. Conclusion,[0],[0]
"We propose two classes of robust estimators, which generalize previous works in two aspects.",6. Conclusion,[0],[0]
"First we allow the diverse structure (e.g., binary, monotone and etc.) of the transfer function, which can help us customize the estimators.",6. Conclusion,[0],[0]
"Secondly the structure of the true parameter can be general, either encoded by a constraint or a norm.",6. Conclusion,[0],[0]
"With limited assumption on the noise, we can show that the estimation error can be bounded by simple geometric measures under Gaussian
measurement, which subsumes the existing results for specific settings.",6. Conclusion,[0],[0]
The experiment results also validate our theoretical analyses.,6. Conclusion,[0],[0]
We thank Sreangsu Acharyya for helpful discussions related to the paper.,Acknowledgements,[0],[0]
"The research was supported by NSF grants IIS-1563950, IIS-1447566, IIS-1447574, IIS1422557, CCF-1451986, CNS- 1314560, IIS-0953274, IIS-1029711, NASA grant NNX12AQ39A, and gifts from Adobe, IBM, and Yahoo.",Acknowledgements,[0],[0]
"In this paper, we investigate general single-index models (SIMs) in high dimensions.",abstractText,[0],[0]
"Based on U -statistics, we propose two types of robust estimators for the recovery of model parameters, which can be viewed as generalizations of several existing algorithms for one-bit compressed sensing (1-bit CS).",abstractText,[0],[0]
"With minimal assumption on noise, the statistical guarantees are established for the generalized estimators under suitable conditions, which allow general structures of underlying parameter.",abstractText,[0],[0]
"Moreover, the proposed estimator is novelly instantiated for SIMs with monotone transfer function, and the obtained estimator can better leverage the monotonicity.",abstractText,[0],[0]
Experimental results are provided to support our theoretical analyses.,abstractText,[0],[0]
Robust Structured Estimation with Single-Index Models,title,[0],[0]
"p k).
In this paper, we solve a key open problem raised therein, presenting a new Partitioned Robust (PRO) submodular maximization algorithm that achieves the same guarantee for more general ⌧ = o(k). Our algorithm constructs partitions consisting of buckets with exponentially increasing sizes, and applies standard submodular optimization subroutines on the buckets in order to construct the robust solution. We numerically demonstrate the performance of PRO in data summarization and influence maximization, demonstrating gains over both the greedy algorithm and the algorithm of (Orlin et al., 2016).",text,[0],[0]
"Discrete optimization problems arise frequently in machine learning, and are often NP-hard even to approximate.",1. Introduction,[0],[0]
"In the case of a set function exhibiting submodularity, one can efficiently perform maximization subject to cardinality constraints with a 1 1
e
-factor approximation guarantee.",1. Introduction,[0],[0]
"Ap-
plications include influence maximization (Kempe et al., 2003), document summarization (Lin & Bilmes, 2011), sensor placement (Krause & Guestrin, 2007), and active learning (Krause & Golovin, 2012), just to name a few.
1LIONS, EPFL, Switzerland 2LTHC, EPFL, Switzerland.",1. Introduction,[0],[0]
"Correspondence to: Ilija Bogunovic <ilija.bogunovic@epfl.ch>, Slobodan Mitrović <slobodan.mitrovic@epfl.ch>, Jonathan Scarlett <jonathan.scarlett@epfl.ch>, Volkan Cevher <volkan.cevher@epfl.ch>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"In many applications of interest, one requires robustness in the solution set returned by the algorithm, in the sense that the objective value degrades as little as possible when some elements of the set are removed.",1. Introduction,[0],[0]
"For instance, (i) in influence maximization problems, a subset of the chosen users may decide not to spread the word about a product; (ii) in summarization problems, a user may choose to remove some items from the summary due to their personal preferences; (iii) in the problem of sensor placement for outbreak detection, some of the sensors might fail.
",1. Introduction,[0],[0]
"In situations where one does not have a reasonable prior distribution on the elements removed, or where one requires robustness guarantees with a high level of certainty, protecting against worst-case removals becomes important.",1. Introduction,[0],[0]
"This setting results in the robust submodular function maximization problem, in which we seek to return a set of cardinality k that is robust with respect to the worst-case removal of ⌧ elements.
",1. Introduction,[0],[0]
"The robust problem formulation was first introduced in (Krause et al., 2008), and was further studied in (Orlin et al., 2016).",1. Introduction,[0],[0]
"In fact, (Krause et al., 2008) considers a more general formulation where a constant-factor approximation guarantee is impossible in general, but shows that one can match the optimal (robust) objective value for a given set size at the cost of returning a set whose size is larger by a logarithmic factor.",1. Introduction,[0],[0]
"In contrast, (Orlin et al., 2016) designs an algorithm that obtains the first constant-factor approximation guarantee to the above problem when ⌧ = o( p k).",1. Introduction,[0],[0]
"A key difference between the two frameworks is that the algorithm complexity is exponential in ⌧ in (Krause et al., 2008), whereas the algorithm of (Orlin et al., 2016) runs in polynomial time.
",1. Introduction,[0],[0]
Contributions.,1. Introduction,[0],[0]
"In this paper, we solve a key open problem posed in (Orlin et al., 2016), namely, whether a constantfactor approximation guarantee is possible for general ⌧ = o(k), as opposed to only ⌧ = o( p k).",1. Introduction,[0],[0]
"We answer this question in the affirmative, providing a new Partitioned Robust (PRO) submodular maximization algorithm that attains a constant-factor approximation guarantee; see Table 1 for comparison of different algorithms for robust monotone submodular optimization with a cardinality constraint.
",1. Introduction,[0],[0]
"Achieving this result requires novelty both in the algorithm and its mathematical analysis: While our algorithm bears some similarity to that of (Orlin et al., 2016), it uses a novel structure in which the constructed set is arranged into partitions consisting of buckets whose sizes increase exponentially with the partition index.",1. Introduction,[0],[0]
"A key step in our analysis provides a recursive relationship between the objective values attained by buckets appearing in adjacent partitions.
",1. Introduction,[0],[0]
"In addition to the above contributions, we provide the first empirical study beyond what is demonstrated for ⌧ = 1 in (Krause et al., 2008).",1. Introduction,[0],[0]
"We demonstrate several scenarios in which our algorithm outperforms both the greedy algorithm and the algorithm of (Orlin et al., 2016).",1. Introduction,[0],[0]
"Let V be a ground set with cardinality |V | = n, and let",2. Problem Statement,[0],[0]
f : 2V !,2. Problem Statement,[0],[0]
R 0 be a set function defined on V .,2. Problem Statement,[0],[0]
"The function f is said to be submodular if for any sets X ✓ Y ✓ V and any element e 2 V \ Y , it holds that
f(X [ {e}) f(X) f(Y",2. Problem Statement,[0],[0]
[ {e}) f(Y ).,2. Problem Statement,[0],[0]
"We use the following notation to denote the marginal gain in the function value due to adding the elements of a set Y to the set X:
f(Y |X) := f(X [ Y ) f(X).",2. Problem Statement,[0],[0]
"In the case that Y is a singleton of the form {e}, we adopt the shorthand f(e|X).",2. Problem Statement,[0],[0]
"We say that f is monotone if for any sets X ✓ Y ✓ V we have f(X)  f(Y ), and normalized if f(;) = 0.",2. Problem Statement,[0],[0]
"The problem of maximizing a normalized monotone submodular function subject to a cardinality constraint, i.e.,
max S✓V,|S|k f(S), (1)
has been studied extensively.",2. Problem Statement,[0],[0]
"A celebrated result of (Nemhauser et al., 1978) shows that a simple greedy algorithm that starts with an empty set and then iteratively adds elements with highest marginal gain provides a (1 1/e)-approximation.
",2. Problem Statement,[0],[0]
"In this paper, we consider the following robust version of (1), introduced in (Krause et al., 2008):
max S✓V,|S|k min Z✓S,|Z|⌧
f(S \ Z) (2)
We refer to ⌧ as the robustness parameter, representing the size of the subset Z that is removed from the selected set S. Our goal is to find a set S such that it is robust upon the worst possible removal of ⌧ elements, i.e., after the removal, the objective value should remain as large as possible.",2. Problem Statement,[0],[0]
"For ⌧ = 0, our problem reduces to Problem (1).
",2. Problem Statement,[0],[0]
"The greedy algorithm, which is near-optimal for Problem (1) can perform arbitrarily badly for Problem (2).",2. Problem Statement,[0],[0]
"As an elementary example, let us fix ✏ 2 [0, n 1) and n 0, and consider the non-negative monotone submodular function given in Table 2.",2. Problem Statement,[0],[0]
"For k = 2, the greedy algorithm selects {s1, s2}.",2. Problem Statement,[0],[0]
"The set that maximizes mins2S f(S\s) (i.e., ⌧ = 1) is {s1, s3}.",2. Problem Statement,[0],[0]
"For this set, min
s2{s 1 ,s 2 } f({s1, s2} \ s) = n 1, while for the greedy set the robust objective value is ✏.",2. Problem Statement,[0],[0]
"As a result, the greedy algorithm can perform arbitrarily worse.
",2. Problem Statement,[0],[0]
"In our experiments on real-world data sets (see Section 5), we further explore the empirical behavior of the greedy solution in the robust setting.",2. Problem Statement,[0],[0]
"Among other things, we observe that the greedy solution tends to be less robust when the objective value largely depends on the first few elements selected by the greedy rule.
",2. Problem Statement,[0],[0]
Related work.,2. Problem Statement,[0],[0]
"(Krause et al., 2008) introduces the following generalization of (2):
max S✓V,|S|k min i2{1,··· ,n} f i (S), (3)
",2. Problem Statement,[0],[0]
where f i are normalized monotone submodular functions.,2. Problem Statement,[0],[0]
"The authors show that this problem is inapproximable in general, but propose an algorithm SATURATE which, when applied to (2), returns a set of size k(1+⇥(log(⌧k log n)))",2. Problem Statement,[0],[0]
whose robust objective is at least as good as the optimal size-k set.,2. Problem Statement,[0],[0]
"SATURATE requires a number of function evaluations that is exponential in ⌧ , making it very expensive to run even for small values.",2. Problem Statement,[0],[0]
"The work of (Powers et al., 2016) considers the same problem for different types of submodular constraints.
",2. Problem Statement,[0],[0]
"Recently, robust versions of submodular maximization have been applied to influence maximization.",2. Problem Statement,[0],[0]
"In (He & Kempe, 2016), the formulation (3) is used to optimize a worst-case approximation ratio.",2. Problem Statement,[0],[0]
"The confidence interval setting is considered in (Chen et al., 2016), where two runs of the GREEDY algorithm (one pessimistic and one optimistic) are used to optimize the same ratio.",2. Problem Statement,[0],[0]
"By leveraging connections to continuous submodular optimization, (Staib & Jegelka, 2017) studies a related continuous robust budget allocation problem.
",2. Problem Statement,[0],[0]
"(Orlin et al., 2016) considers the formulation in (2), and provides the first constant 0.387-factor approximation result, valid for ⌧ = o( p k).",2. Problem Statement,[0],[0]
"The algorithm proposed therein, which we refer to via the authors surnames as OSU, uses the greedy algorithm (henceforth referred to as GREEDY) as a sub-routine ⌧ +1 times.",2. Problem Statement,[0],[0]
"On each iteration, GREEDY is applied on the elements that are not yet selected on previous iterations, with these previously-selected elements ignored in the objective function.",2. Problem Statement,[0],[0]
"In the first ⌧ runs, each solution is of size ⌧ log k, while in the last run, the solution is of size k ⌧2 log k.",2. Problem Statement,[0],[0]
The union of all the obtained disjoint solutions leads to the final solution set.,2. Problem Statement,[0],[0]
"In this section, we provide several examples of applications where the robustness of the solution is favorable.",3. Applications,[0],[0]
"The objective functions in these applications are non-negative, monotone and submodular, and are used in our numerical experiments in Section 5.
",3. Applications,[0],[0]
Robust influence maximization.,3. Applications,[0],[0]
"The goal in the influence maximization problem is to find a set of k nodes (i.e., a targeted set) in a network that maximizes some measure of influence.",3. Applications,[0],[0]
"For example, this problem appears in viral marketing, where companies wish to spread the word of a new product by targeting the most influential individuals in a social network.",3. Applications,[0],[0]
"Due to poor incentives or dissatisfaction with the product, for instance, some of the users from the
targeted set might make the decision not to spread the word about the product.
",3. Applications,[0],[0]
"For many of the existing diffusion models used in the literature (e.g., see (Kempe et al., 2003)), given the targeted set S, the expected number of influenced nodes at the end of the diffusion process is a monotone and submodular function of S (He & Kempe, 2016).",3. Applications,[0],[0]
"For simplicity, we consider a basic model in which all of the neighbors of the users in S become influenced, as well as those in S itself.
",3. Applications,[0],[0]
"More formally, we are given a graph G = (V,E), where V stands for nodes and E are the edges.",3. Applications,[0],[0]
"For a set S, let N (S) denote all of its neighboring nodes.",3. Applications,[0],[0]
"The goal is to solve the robust dominating set problem, i.e., to find a set of nodes S of size k that maximizes
min |RS |⌧,RS✓S
|(S \R S )",3. Applications,[0],[0]
"[N (S \R S )|, (4)
where R S ✓ S represents the users that decide not to spread the word.",3. Applications,[0],[0]
"The non-robust version of this objective function has previously been considered in several different works, such as (Mirzasoleiman et al., 2015b) and (NorouziFard et al., 2016).
",3. Applications,[0],[0]
Robust personalized image summarization.,3. Applications,[0],[0]
"In the personalized image summarization problem, a user has a collection of images, and the goal is to find k images that are representative of the collection.
",3. Applications,[0],[0]
"After being presented with a solution, the user might decide to remove a certain number of images from the representative set due to various reasons (e.g., bad lighting, motion blur, etc.).",3. Applications,[0],[0]
"Hence, our goal is to find a set of images that remain good representatives of the collection even after the removal of some number of them.
",3. Applications,[0],[0]
"One popular way of finding a representative set in a massive dataset is via exemplar based clustering, i.e., by minimizing the sum of pairwise dissimilarities between the exemplars S and the elements of the data set V .",3. Applications,[0],[0]
"This problem can be posed as a submodular maximization problem subject to a cardinality constraint; cf., (Lucic et al., 2016).
",3. Applications,[0],[0]
"Here, we are interested in solving the robust summarization problem, i.e., we want to find a set of images S of size k that maximizes
min |RS |⌧,RS✓S
f({e0}) f((S \RS)",3. Applications,[0],[0]
"[ {e0}), (5)
where e0 is a reference element and f(S) = 1
|V | P
v2V mins2S d(s, v) is the k-medoid loss function, and where d(s, v) measures the dissimilarity between images s and v.
Further potential applications not covered here include robust sensor placement (Krause et al., 2008), robust protection of networks (Bogunovic & Krause, 2012), and robust feature selection (Globerson & Roweis, 2006).",3. Applications,[0],[0]
"Our algorithm, which we call the Partitioned Robust (PRO) submodular maximization algorithm, is presented in Algorithm 1.",4.1. The algorithm,[0],[0]
"As the input, we require a non-negative monotone submodular function f : 2V !",4.1. The algorithm,[0],[0]
"R 0, the ground set of elements V , and an optimization subroutine A.",4.1. The algorithm,[0],[0]
"The subroutine A(k0, V 0) takes a cardinality constraint k0 and a ground set of elements V 0.",4.1. The algorithm,[0],[0]
"Below, we describe the properties of A that are used to obtain approximation guarantees.
",4.1. The algorithm,[0],[0]
The output of the algorithm is a set S ✓ V of size k that is robust against the worst-case removal of ⌧ elements.,4.1. The algorithm,[0],[0]
"The returned set consists of two sets S0 and S1, illustrated in Figure 1.",4.1. The algorithm,[0],[0]
"S1 is obtained by running the subroutine A on V \ S0 (i.e., ignoring the elements already placed into S0), and is of size k |S0|.",4.1. The algorithm,[0],[0]
"We refer to the set S0 as the robust part of the solution set S. It consists of dlog ⌧e + 1 partitions, where every partition i 2 {0, · · · , dlog ⌧e} consists of d⌧/2ie buckets B
j , j 2 {1, · · · , d⌧/2ie}.",4.1. The algorithm,[0],[0]
"In partition i, every bucket contains 2i⌘ elements, where ⌘ 2 N+ is a parameter that is arbitrary for now; we use ⌘ = log2 k in our asymptotic theory, but our numerical studies indicate that even ⌘ = 1 works well in practice.",4.1. The algorithm,[0],[0]
"Each bucket B
j is created afresh by using the subroutine A on V \ S0,prev, where S0,prev contains all elements belonging to the previous buckets.
",4.1. The algorithm,[0],[0]
"The following proposition bounds the cardinality of S0, and is proved in the supplementary material.
",4.1. The algorithm,[0],[0]
Proposition 4.1 Fix k ⌧ and ⌘ 2 N+.,4.1. The algorithm,[0],[0]
"The size of the robust part S0 constructed in Algorithm 1 is
|S0| = dlog ⌧eX
i=0
d⌧/2ie2i⌘  3⌘⌧(log k + 2).
",4.1. The algorithm,[0],[0]
"This proposition reveals that the feasible values of ⌧ (i.e., those with |S0|  k) can be as high as O k
⌘⌧
.",4.1. The algorithm,[0],[0]
"We will
later set ⌘ = O(log2 k), thus permitting all ⌧ = o(k) up to a few logarithmic factors.",4.1. The algorithm,[0],[0]
"In contrast, we recall that the algorithm OSU proposed in (Orlin et al., 2016) adopts a simpler approach where a robust set is used consisting of ⌧ buckets of equal size ⌧ log k, thereby only permitting the scaling ⌧ = o",4.1. The algorithm,[0],[0]
"( p k).
",4.1. The algorithm,[0],[0]
"We provide the following intuition as to why PRO succeeds despite having a smaller size for S0 compared to the algorithm given in (Orlin et al., 2016).",4.1. The algorithm,[0],[0]
"First, by the design of the partitions, there always exists a bucket in partition i that at most 2i items are removed from.",4.1. The algorithm,[0],[0]
The bulk of our analysis is devoted to showing that the union of these buckets yields a sufficiently high objective value.,4.1. The algorithm,[0],[0]
"While the earlier
Algorithm 1 Partitioned Robust Submodular optimization algorithm (PRO)
Require: Set V , k, ⌧ , ⌘ 2 N+, algorithm A Ensure: Set S ✓ V such that |S|  k
1: S0, S1 ; 2: for i 0 to dlog ⌧e do 3: for j 1 to d⌧/2ie do 4: B
j A (2i⌘, (V \ S0)) 5: S0 S0",4.1. The algorithm,[0],[0]
"[Bj 6: S1 A (k |S0|, (V \ S0)) 7",4.1. The algorithm,[0],[0]
": S S0 [ S1 8: return S
buckets have a smaller size, they also have a higher objective value per item due to diminishing returns, and our analysis quantifies and balances this trade-off.",4.1. The algorithm,[0],[0]
"Similarly, our analysis quantifies the trade-off between how much the adversary can remove from the (typically large) set S1 and the robust part S0.",4.1. The algorithm,[0],[0]
PRO accepts a subroutine A as the input.,4.2. Subroutine and assumptions,[0],[0]
"We consider a class of algorithms that satisfy the -iterative property, defined below.",4.2. Subroutine and assumptions,[0],[0]
"We assume that the algorithm outputs the final set in some specific order (v1, . . .",4.2. Subroutine and assumptions,[0],[0]
", vk), and we refer to vi as the i-th output element.
",4.2. Subroutine and assumptions,[0],[0]
"Definition 4.2 Consider a normalized monotone submodular set function f on a ground set V , and an algorithm A.",4.2. Subroutine and assumptions,[0],[0]
"Given any set T ✓ V and size k, suppose that A outputs an ordered set (v1, . . .",4.2. Subroutine and assumptions,[0],[0]
", vk) when applied to T , and define A
i (T ) = {v1, . . .",4.2. Subroutine and assumptions,[0],[0]
", vi} for i  k.",4.2. Subroutine and assumptions,[0],[0]
"We say that A satisfies the -iterative property if
f(A i+1(T ))",4.2. Subroutine and assumptions,[0],[0]
f(Ai(T )),4.2. Subroutine and assumptions,[0],[0]
"1
max v2T
f(v|A i (T )).",4.2. Subroutine and assumptions,[0],[0]
"(6)
Intuitively, (6) states that in every iteration, the algorithm adds an element whose marginal gain is at least a 1/ fraction of the maximum marginal.",4.2. Subroutine and assumptions,[0],[0]
This necessarily requires that 1.,4.2. Subroutine and assumptions,[0],[0]
Examples.,4.2. Subroutine and assumptions,[0],[0]
"Besides the classic greedy algorithm, which satisfies (6) with = 1, a good candidate for our subroutine is THRESHOLDING-GREEDY (Badanidiyuru & Vondrák, 2014), which satisfies the -iterative property with = 1/(1 ✏).",4.2. Subroutine and assumptions,[0],[0]
This decreases the number of function evaluations to O(n/✏ log n/✏).,4.2. Subroutine and assumptions,[0],[0]
"STOCHASTIC-GREEDY (Mirzasoleiman et al., 2015a) is another potential subroutine candidate.",4.2. Subroutine and assumptions,[0],[0]
"While it is unclear whether this algorithm satisfies the -iterative property, it requires an even smaller number of function eval-
uations, namely, O(n log 1/✏).",4.2. Subroutine and assumptions,[0],[0]
We will see in Section 5 that PRO performs well empirically when used with this subroutine.,4.2. Subroutine and assumptions,[0],[0]
"We henceforth refer to PRO used along with its appropriate subroutine as PRO-GREEDY, PRO-THRESHOLDING-GREEDY, and so on.
Properties.",4.2. Subroutine and assumptions,[0],[0]
"The following lemma generalizes a classical property of the greedy algorithm (Nemhauser et al., 1978; Krause & Golovin, 2012) to the class of algorithms satisfying the -iterative property.",4.2. Subroutine and assumptions,[0],[0]
"Here and throughout the paper, we use OPT(k, V ) to denote the following optimal set for non-robust maximization:
OPT(k, V ) 2 argmax S✓V,|S|=k f(S),
Lemma 4.3 Consider a normalized monotone submodular function f : 2V !",4.2. Subroutine and assumptions,[0],[0]
"R 0 and an algorithm A(T ), T ✓ V , that satisfies the -iterative property in (6).",4.2. Subroutine and assumptions,[0],[0]
"Let A
l (T ) denote the set returned by the algorithm A(T ) after l iterations.",4.2. Subroutine and assumptions,[0],[0]
"Then for all k, l 2 N+
f(A l (T )) ⇣ 1 e l k ⌘ f(OPT(k, T )).",4.2. Subroutine and assumptions,[0],[0]
"(7)
We will also make use of the following property, which is implied by the -iterative property.
",4.2. Subroutine and assumptions,[0],[0]
Proposition 4.4 Consider a submodular set function f : 2V !,4.2. Subroutine and assumptions,[0],[0]
R 0 and an algorithm A that satisfies the - iterative property for some 1.,4.2. Subroutine and assumptions,[0],[0]
"Then, for any T ✓ V and element e 2 V \ A(T ), we have
f(e|A(T ))  f(A(T ))",4.2. Subroutine and assumptions,[0],[0]
k .,4.2. Subroutine and assumptions,[0],[0]
"(8)
Intuitively, (8) states that the marginal gain of any nonselected element cannot be more than times the average objective value of the selected elements.",4.2. Subroutine and assumptions,[0],[0]
"This is one of the rules used to define the -nice class of algorithms in (Mirrokni & Zadimoghaddam, 2015); however, we note that in general, neither the -nice nor -iterative classes are a subset of one another.",4.2. Subroutine and assumptions,[0],[0]
"For the robust maximization problem, we let OPT(k, V, ⌧) denote the optimal set:
OPT(k, V, ⌧) 2 argmax S✓V,|S|=k min E✓S,|E|⌧ f(S \ E).
",4.3. Main result: Approximation guarantee,[0],[0]
"Moreover, for a set S, we let E⇤ S denote the minimizer
E⇤ S 2 argmin E✓S,|E|⌧ f(S \ E).",4.3. Main result: Approximation guarantee,[0],[0]
"(9)
With these definitions, the main theoretical result of this paper is as follows.
",4.3. Main result: Approximation guarantee,[0],[0]
"Theorem 4.5 Let f be a normalized monotone submodular function, and let A be a subroutine satisfying the - iterative property.",4.3. Main result: Approximation guarantee,[0],[0]
"For a given budget k and parameters 2  ⌧  k3⌘(log k+2) and ⌘ 4(log k + 1), PRO returns a set S of size k such that
f(S",4.3. Main result: Approximation guarantee,[0],[0]
"\ E⇤ S
) ⌘ 5 3dlog ⌧e+⌘
⇣ 1 e k |S0| (k ⌧) ⌘
1 + ⌘5 3dlog ⌧e+⌘
⇣ 1 e k |S0| (k ⌧) ⌘
⇥ f(OPT(k, V, ⌧) \",4.3. Main result: Approximation guarantee,[0],[0]
"E⇤OPT(k,V,⌧)), (10) where E⇤
S and E⇤OPT(k,V,⌧) are defined as in (9).
",4.3. Main result: Approximation guarantee,[0],[0]
"In addition, if ⌧ = o ⇣ k
⌘ log k
⌘ and ⌘ log2 k, then we
have the following as k !1:
f(S",4.3. Main result: Approximation guarantee,[0],[0]
\ E⇤ S ),4.3. Main result: Approximation guarantee,[0],[0]
"✓ 1 e 1/ 2 e 1/ + o(1) ◆
⇥ f(OPT(k, V, ⌧)",4.3. Main result: Approximation guarantee,[0],[0]
\,4.3. Main result: Approximation guarantee,[0],[0]
"E⇤OPT(k,V,⌧)).",4.3. Main result: Approximation guarantee,[0],[0]
"(11) In particular, PRO-GREEDY achieves an asymptotic approximation factor of at least 0.387, and PROTHRESHOLDING-GREEDY with parameter ✏ achieves an asymptotic approximation factor of at least 0.387(1 ✏).
",4.3. Main result: Approximation guarantee,[0],[0]
"This result solves an open problem raised in (Orlin et al., 2016), namely, whether a constant-factor approximation guarantee can be obtained for ⌧ = o(k) as opposed to
only ⌧ = o p k .",4.3. Main result: Approximation guarantee,[0],[0]
"In the asymptotic limit, our constant factor of 0.387 for the greedy subroutine matches that of (Orlin et al., 2016), but our algorithm permits significantly “higher robustness” in the sense of allowing larger ⌧ values.",4.3. Main result: Approximation guarantee,[0],[0]
"To achieve this, we require novel proof techniques, which we now outline.",4.3. Main result: Approximation guarantee,[0],[0]
The proof of Theorem 4.5 is provided in the supplementary material.,4.4. High-level overview of the analysis,[0],[0]
"Here we provide a high-level overview of the main challenges.
",4.4. High-level overview of the analysis,[0],[0]
Let E denote a cardinality-⌧ subset of the returned set S that is removed.,4.4. High-level overview of the analysis,[0],[0]
"By the construction of the partitions, it is easy to verify that each partition i contains a bucket from which at most 2i items are removed.",4.4. High-level overview of the analysis,[0],[0]
"We denote these by B0, . . .",4.4. High-level overview of the analysis,[0],[0]
", Bdlog ⌧e, and write EBi := E \Bi.",4.4. High-level overview of the analysis,[0],[0]
"Moreover, we define E0 := E \ S0 and E1 := E \ S1.",4.4. High-level overview of the analysis,[0],[0]
"We establish the following lower bound on the final objective function value:
f(S \E) max ⇢ f(S0 \E0), f(S1) f(E1|(S \E)),
f
✓ dlog ⌧e[
i=0
(B i \ E Bi)
◆ .",4.4. High-level overview of the analysis,[0],[0]
"(12)
",4.4. High-level overview of the analysis,[0],[0]
"The arguments to the first and third terms are trivially seen to be subsets of S \ E, and the second term represents the utility of the set S1 subsided by the utility of the elements removed from S1.
",4.4. High-level overview of the analysis,[0],[0]
The first two terms above are easily lower bounded by convenient expressions via submodular and the -iterative property.,4.4. High-level overview of the analysis,[0],[0]
The bulk of the proof is dedicated to bounding the third term.,4.4. High-level overview of the analysis,[0],[0]
"To do this, we establish the following recursive relations with suitably-defined “small” values of ↵
j
:
f
j[
i=0
(B i \ E Bi)
!",4.4. High-level overview of the analysis,[0],[0]
"1 1
1 + 1 ↵j
!",4.4. High-level overview of the analysis,[0],[0]
"f(B
j
)
f E
Bj
j 1",4.4. High-level overview of the analysis,[0],[0]
"[
i=0
(B i \ E Bi)
!  ",4.4. High-level overview of the analysis,[0],[0]
"↵
j
f
j 1[
i=0
(B i \ E Bi)
! .
",4.4. High-level overview of the analysis,[0],[0]
"Intuitively, the first equation shows that the objective value from buckets i = 0, . . .",4.4. High-level overview of the analysis,[0],[0]
", j with removals cannot be too much smaller than the objective value in bucket j without removals, and the second equation shows that the loss in bucket j due to the removals is at most a small fraction of the objective value from buckets 0, . . .",4.4. High-level overview of the analysis,[0],[0]
",",4.4. High-level overview of the analysis,[0],[0]
j 1.,4.4. High-level overview of the analysis,[0],[0]
"The proofs of both the base case of the induction and the inductive step make use of submodularity properties and the -iterative property (cf., Definition 4.2).
",4.4. High-level overview of the analysis,[0],[0]
"Once the suitable lower bounds are obtained for the terms in (12), the analysis proceeds similarly to (Orlin et al.,
2016).",4.4. High-level overview of the analysis,[0],[0]
"Specifically, we can show that as the second term increases, the third term decreases, and accordingly lower bound their maximum by the value obtained when the two are equal.",4.4. High-level overview of the analysis,[0],[0]
"A similar balancing argument is then applied to the resulting term and the first term in (12).
",4.4. High-level overview of the analysis,[0],[0]
"The condition ⌧  k3⌘(log k+2) follows directly from Proposition 4.1; namely, it is a sufficient condition for |S0|  k, as is required by PRO.",4.4. High-level overview of the analysis,[0],[0]
"In this section, we numerically validate the performance of PRO and the claims given in the preceding sections.",5. Experiments,[0],[0]
"In particular, we compare our algorithm against the OSU algorithm proposed in (Orlin et al., 2016) on different datasets and corresponding objective functions (see Table 3).",5. Experiments,[0],[0]
"We demonstrate matching or improved performance in a broad range of settings, as well as observing that PRO can be implemented with larger values of ⌧ , corresponding to a greater robustness.",5. Experiments,[0],[0]
"Moreover, we show that for certain realworld data sets, the classic GREEDY algorithm can perform badly for the robust problem.",5. Experiments,[0],[0]
"We do not compare against SATURATE (Krause et al., 2008), due to its high computational cost for even a small ⌧ .
Setup.",5. Experiments,[0],[0]
"Given a solution set S of size k, we measure the performance in terms of the minimum objective value upon the worst-case removal of ⌧ elements, i.e. min
Z✓S,|Z|⌧ f(S\ Z).",5. Experiments,[0],[0]
"Unfortunately, for a given solution set S, finding such a set Z is an instance of the submodular minimization problem with a cardinality constraint,1 which is known to be NP-hard with polynomial approximation factors (Svitkina & Fleischer, 2011).",5. Experiments,[0],[0]
"Hence, in our experiments, we only implement the optimal “adversary” (i.e., removal of items) for small to moderate values of ⌧ and k, for which we use a fast C++ implementation of branch-and-bound.
",5. Experiments,[0],[0]
"Despite the difficulty in implementing the optimal adversary, we observed in our experiments that the greedy adversary, which iteratively removes elements to reduce the objective value as much as possible, has a similar impact on the objective compared to the optimal adversary for the data sets considered.",5. Experiments,[0],[0]
"Hence, we also provide a larger-scale experiment in the presence of a greedy adversary.",5. Experiments,[0],[0]
"Throughout, we write OA and GA to abbreviate the optimal adversary and greedy adversary, respectively.
",5. Experiments,[0],[0]
"In our experiments, the size of the robust part of the solution set (i.e., |S0|) is set to ⌧2 and ⌧ log ⌧ for OSU and PRO, respectively.",5. Experiments,[0],[0]
"That is, we set ⌘ = 1 in PRO, and similarly ignore constant and logarithmic factors in OSU, since both appear to be unnecessary in practice.",5. Experiments,[0],[0]
"We show
1This can be seen by noting that for submodular f and any Z ✓ X ✓ V , f 0(Z) = f(X \ Z) remains submodular.
both the “raw” objective values of the solutions, as well as the objective values after the removal of ⌧ elements.",5. Experiments,[0],[0]
"In all experiments, we implement GREEDY using the LAZYGREEDY implementation given in (Minoux, 1978).
",5. Experiments,[0],[0]
The objective functions shown in Table 3 are given in Section 3.,5. Experiments,[0],[0]
"For the exemplar objective function, we use d(s, v) = ks vk2, and let the reference element e0 be the zero vector.",5. Experiments,[0],[0]
"Instead of using the whole set V , we approximate the objective by considering a smaller random subset of V for improved computational efficiency.",5. Experiments,[0],[0]
"Since the objective is additively decomposable and bounded, standard concentration bounds (e.g., the Chernoff bound) ensure that the empirical mean over a random subsample can be made arbitrarily accurate.
Data sets.",5. Experiments,[0],[0]
"We consider the following datasets, along with the objective functions given in Section 3:
Results.",5. Experiments,[0],[0]
"In the first set of experiments, we compare PROGREEDY (written using the shorthand PRO-GR in the legend) against GREEDY and OSU on the EGO-FACEBOOK and EGO-TWITTER datasets.",5. Experiments,[0],[0]
"In this experiment, the dominating set selection objective in (4) is considered.",5. Experiments,[0],[0]
Figure 2 (a) and (c) show the results before and after the worst-case removal of ⌧ = 7 elements for different values of k.,5. Experiments,[0],[0]
"In Figure 2 (b) and (d), we show the objective value for fixed k = 50 and k = 100, respectively, while the robustness parameter ⌧ is varied.
",5. Experiments,[0],[0]
"GREEDY achieves the highest raw objective value, followed by PRO-GREEDY and OSU.",5. Experiments,[0],[0]
"However, after the worst-case removal, PRO-GREEDY-OA outperforms both OSU-OA and GREEDY-OA.",5. Experiments,[0],[0]
"In Figure 2 (a) and (b), GREEDY-OA performs poorly due to a high concentration of the objective value on the first few elements selected by GREEDY.",5. Experiments,[0],[0]
"While OSU requires k ⌧2, PRO only requires k ⌧ log ⌧ , and hence it can be run for larger values of ⌧ (e.g., see Figure 2 (b) and (c)).",5. Experiments,[0],[0]
"Moreover, in Figure 2 (a) and (b), we can observe that although PRO uses a smaller number of elements to build the robust part of the solution set, it has better robustness in comparison with OSU.
",5. Experiments,[0],[0]
"In the second set of experiments, we perform the same type of comparisons on the TINY10 and CM-MOLECULES datasets.",5. Experiments,[0],[0]
The exemplar based clustering in (5) is used as the objective function.,5. Experiments,[0],[0]
"In Figure 2 (e) and (h), the robustness parameter is fixed to ⌧ = 7 and ⌧ = 6, respectively, while the cardinality k is varied.",5. Experiments,[0],[0]
"In Figure 2 (f) and (h), the cardinality is fixed to k = 100 and k = 50, respectively, while the robustness parameter ⌧ is varied.
",5. Experiments,[0],[0]
"Again, GREEDY achieves the highest objective value.",5. Experiments,[0],[0]
"On the TINY10 dataset, GREEDY-OA (Figure 2 (e) and (f)) has a large gap between the raw and final objective, but it still slightly outperforms PRO-GREEDY-OA.",5. Experiments,[0],[0]
"This demonstrates that GREEDY can work well in some cases, despite failing in others.",5. Experiments,[0],[0]
We observed that it succeeds here because the objective value is relatively more uniformly spread across the selected elements.,5. Experiments,[0],[0]
"On the same dataset, PRO-GREEDY-OA outperforms OSU-OA.",5. Experiments,[0],[0]
"On our second dataset CM-MOLECULES (Figure 2 (g) and (h)), PROGREEDY-OA achieves the highest robust objective value, followed by OSU-OA and GREEDY-OA.
",5. Experiments,[0],[0]
"In our final experiment (see Figure 2 (i)), we compare the performance of PRO-GREEDY against two instances of PRO-STOCHASTIC-GREEDY with ✏ = 0.01 and ✏ = 0.08 (shortened to PRO-ST in the legend), seeking to understand to what extent using the more efficient stochastic subroutine impacts the performance.",5. Experiments,[0],[0]
We also show the performance of OSU.,5. Experiments,[0],[0]
"In this experiment, we fix k = 100 and vary ⌧ .",5. Experiments,[0],[0]
"We use the greedy adversary instead of the optimal one, since the latter becomes computationally challenging for larger values of ⌧ .
",5. Experiments,[0],[0]
"In Figure 2 (i), we observe a slight decrease in the objective value of PRO-STOCHASTIC-GREEDY due to the stochastic optimization.",5. Experiments,[0],[0]
"On the other hand, the gaps between the robust and non-robust solutions remain similar, or even shrink.",5. Experiments,[0],[0]
"Overall, we observe that at least in this example, the stochastic subroutine does not compromise the quality of the solution too significantly, despite having a lower computational complexity.",5. Experiments,[0],[0]
"We have provided a new Partitioned Robust (PRO) submodular maximization algorithm attaining a constantfactor approximation guarantee for general ⌧ = o(k), thus
resolving an open problem posed in (Orlin et al., 2016).",6. Conclusion,[0],[0]
"Our algorithm uses a novel partitioning structure with partitions consisting of buckets with exponentially decreasing size, thus providing a “robust part” of size O(⌧poly log ⌧).",6. Conclusion,[0],[0]
We have presented a variety of numerical experiments where PRO outperforms both GREEDY and OSU.,6. Conclusion,[0],[0]
"A potentially interesting direction for further research is to understand the linear regime, in which ⌧ = ↵k for some constant ↵ 2 (0, 1), and in particular, to seek a constant-factor guarantee for this regime.
",6. Conclusion,[0],[0]
Acknowledgment.,6. Conclusion,[0],[0]
"This work was supported in part by the European Commission under Grant ERC Future Proof, SNF 200021-146750 and SNF CRSII2-147633, and ‘EPFL Fellows’ (Horizon2020 665667).",6. Conclusion,[0],[0]
"We study the problem of maximizing a monotone submodular function subject to a cardinality constraint k, with the added twist that a number of items ⌧ from the returned set may be removed.",abstractText,[0],[0]
"We focus on the worst-case setting considered in (Orlin et al., 2016), in which a constant-factor approximation guarantee was given for ⌧ = o( p k).",abstractText,[0],[0]
"In this paper, we solve a key open problem raised therein, presenting a new Partitioned Robust (PRO) submodular maximization algorithm that achieves the same guarantee for more general ⌧ = o(k).",abstractText,[0],[0]
"Our algorithm constructs partitions consisting of buckets with exponentially increasing sizes, and applies standard submodular optimization subroutines on the buckets in order to construct the robust solution.",abstractText,[0],[0]
"We numerically demonstrate the performance of PRO in data summarization and influence maximization, demonstrating gains over both the greedy algorithm and the algorithm of (Orlin et al., 2016).",abstractText,[0],[0]
Robust Submodular Maximization:  A Non-Uniform Partitioning Approach,title,[0],[0]
"Proceedings of the SIGDIAL 2018 Conference, pages 264–272, Melbourne, Australia, 12-14 July 2018. c©2018 Association for Computational Linguistics
264
tant for chatbots if we want them to be believable. Typically, many questionanswer pairs are prepared by hand for achieving consistent responses; however, the creation of such pairs is costly. In this study, our goal is to collect a large number of question-answer pairs for a particular character by using role playbased question-answering in which multiple users play the roles of certain characters and respond to questions by online users. Focusing on two famous characters, we conducted a large-scale experiment to collect question-answer pairs by using real users. We evaluated the effectiveness of role play-based questionanswering and found that, by using our proposed method, the collected pairs lead to good-quality chatbots that exhibit consistent personalities.",text,[0],[0]
"Having a consistent personality is important for chatbots if we want them to be believable (Li et al., 2016; Gordon et al., 2016; Curry and Rieser, 2016; Sugiyama et al., 2017; Akama et al., 2017).",1 Introduction,[0],[0]
"Although neural networkbased methods are emerging for achieving consistent personalities, their quality is not that high (Li et al., 2016).",1 Introduction,[0],[0]
"Therefore, in many systems, question-answer pairs are prepared by hand for consistent responses (Takeuchi et al., 2007; Leuski et al., 2009; Traum et al., 2015).",1 Introduction,[0],[0]
"However, the creation of such pairs is costly.
",1 Introduction,[0],[0]
"In this study, our aim is to collect a large number of question-answer pairs for a particular character by using role play-based questionanswering (Higashinaka et al., 2013a) in which
multiple users play the roles of certain characters and respond to questions by online users.",1 Introduction,[0],[0]
The concept is shown in Figure 1.,1 Introduction,[0],[0]
The main idea is that role players collectively represent a single character and that a question is broadcast via a character to all role players.,1 Introduction,[0],[0]
"In this way, questionanswer pairs can be efficiently collected because there is less burden on people responding, and the entertaining nature of role playing makes people likelier to participate (Ments, 1999).",1 Introduction,[0],[0]
"In a smallscale experiment, Higashinaka et al. found that question-answer pairs of a character can be efficiently collected by multiple users and that users are highly motivated to provide questions and answers.
",1 Introduction,[0],[0]
There were two limitations to their work.,1 Introduction,[0],[0]
"One was that the experiment was conducted using only a small number of people, who were recruited by the authors.",1 Introduction,[0],[0]
"It was not clear if the scheme would work with real users (i.e., users who are not recruited nor paid by researchers).",1 Introduction,[0],[0]
The other limitation was that the applicability of the collected data to the creation of chatbots was not verified.,1 Introduction,[0],[0]
"In their small-scale experiment, the maximum number of question-answer pairs for a character was only about 80.",1 Introduction,[0],[0]
"This was because users were allowed to register any of their favorite characters, resulting in a small amount of data per character.",1 Introduction,[0],[0]
"It was difficult to create a chatbot with such little data.
",1 Introduction,[0],[0]
"In this paper, we tackle these limitations by using role play-based question-answering for collecting question-answer pairs from real users.",1 Introduction,[0],[0]
"Regarding the second limitation, we limited the characters to two famous ones so as to collect a large number of question-answer pairs per character and create workable chatbots.",1 Introduction,[0],[0]
We conducted a subjective evaluation of the chatbots by using human participants.,1 Introduction,[0],[0]
"Our contributions are as follows:
•",1 Introduction,[0],[0]
"We verified that role play-based question-
answering works with real users, collecting a large number of question-answer pairs per character in a short period.
",1 Introduction,[0],[0]
"• We proposed a method to create chatbots from collected question-answer pairs and
verified that it can lead to good-quality chatbots exhibiting consistent personalities.
",1 Introduction,[0],[0]
We first describe our data collection by using role play-based question-answering with real users.,1 Introduction,[0],[0]
"Then, we propose our method for creating chatbots using the collected question-answer pairs.",1 Introduction,[0],[0]
"Next, we describe the experiment we conducted to evaluate the quality of the chatbots by using human participants.",1 Introduction,[0],[0]
"After covering related work, we summarize the paper and mention future work.",1 Introduction,[0],[0]
"To collect a large number of question-answer pairs per character, we focused on two characters: a real person called Max Murai and a fictional character in a novel, Ayase Aragaki.",2 Data collection by real users,[0],[0]
They are popular characters in Japan and have a large number of fans.,2 Data collection by real users,[0],[0]
We created Web sites in their fan communities so that fans could try role play-based questionanswering.,2 Data collection by real users,[0],[0]
We first describe the two characters in more detail and then briefly go over the Web sites.,2 Data collection by real users,[0],[0]
"Finally, we present the statistics of the data and look at the results from several aspects.",2 Data collection by real users,[0],[0]
Max Murai,2.1 Characters,[0],[0]
"His real name is Tomotake Murai
(Max Murai is his stage name).",2.1 Characters,[0],[0]
"Born in 1981, Murai is a CEO of the IT company AppBank but also a YouTuber who specializes in the live coverage of TV games.",2.1 Characters,[0],[0]
"He is known to have a frank personality.
",2.1 Characters,[0],[0]
Ayase Aragaki,2.1 Characters,[0],[0]
"A fictional character in the novel
“Ore no imouto ga konnnai kawaii wakega
nai” (My Little Sister Can’t Be This Cute), which has sold more than five million copies in Japan in its series.",2.1 Characters,[0],[0]
Ayase is not a main character but plays a supporting role.,2.1 Characters,[0],[0]
Her character is often referred to as a “Yandere”.,2.1 Characters,[0],[0]
"According to Wikipedia, Yandere characters are mentally unstable, incredibly deranged, and use extreme violence or brutality as an outlet for their emotions.",2.1 Characters,[0],[0]
"On the Japanese streaming service NICONICO Douga1, each character has a channel for their fans.",2.2 Web sites,[0],[0]
The channel is limited to subscribers.,2.2 Web sites,[0],[0]
"Through the generosity of this service, we were allowed to establish our Web sites for role playbased question-answering on their channels.",2.2 Web sites,[0],[0]
"Murai has more than 10,000 subscribers; the number of subscribes for Ayase is not disclosed.
",2.2 Web sites,[0],[0]
"We opened the Web sites in March and October 2017 for Murai and Ayase, respectively.",2.2 Web sites,[0],[0]
Figures 2 and 3 show screenshots of the sites.,2.2 Web sites,[0],[0]
The appearances of the sites were adjusted to the characters.,2.2 Web sites,[0],[0]
"The users can ask the characters questions by
1 http://www.nicovideo.jp/
means of a text-field interface, and users who want to play the role of the characters can post answers.",2.2 Web sites,[0],[0]
"To stimulate interaction, the Web sites show the rankings of users by their number of posts.",2.2 Web sites,[0],[0]
"In addition, a “like” button is placed beside each answer so that when a user thinks the answer sounds very much “like” the character in question, this opinion can be reflected in the number of “likes”.",2.2 Web sites,[0],[0]
The sites were primarily for collecting one-shot questionanswer pairs.,2.2 Web sites,[0],[0]
"It was also possible for the Murai site to collect follow-up question-answer pairs, but this function was rarely utilized by users.",2.2 Web sites,[0],[0]
The statistics of the postings (at the time of submission) are listed in Table 1.,2.3 Statistics,[0],[0]
"We obtained a total of 12,959 and 15,112 question-answer pairs for Murai and Ayase, respectively.",2.3 Statistics,[0],[0]
The size of the data is quite large.,2.3 Statistics,[0],[0]
We want to emphasize that the users were not paid for their participation; they did so voluntarily.,2.3 Statistics,[0],[0]
This indicates that role play-based question-answering works well with real users.,2.3 Statistics,[0],[0]
"As seen in the table, more than 300 users participated for each character.",2.3 Statistics,[0],[0]
The questions/answers for Ayase were longer and contained more words and letters.,2.3 Statistics,[0],[0]
Table 2 shows the times when the number of question-answer pairs exceeded certain thresholds.,2.4 Efficiency,[0],[0]
We can see how fast we could collect a few thousand question-answer pairs.,2.4 Efficiency,[0],[0]
"For both characters, it took just about a couple of days to reach 2,000 question-answer pairs.",2.4 Efficiency,[0],[0]
"For Ayase, the pace was much faster than for Murai, reaching 10,000 question-answer pairs in 18 days.",2.4 Efficiency,[0],[0]
"After a cer-
tain period, the pace of the postings slowed.",2.4 Efficiency,[0],[0]
"Although role play-based question-answering is certainly entertaining, we may need to consider ways to keep users engaged in the interaction.",2.4 Efficiency,[0],[0]
Enabling more sustainable collection of questionanswer pairs is future work.,2.4 Efficiency,[0],[0]
We also evaluated the answers given by the users through subjective evaluation (see GOLD in Tables 4 and 5).,2.5 Quality of the postings,[0],[0]
"We obtained the average naturalness/character-ness scores of around 3.5– 4.0 on a five-point Likert scale, indicating that the answers collected through role play-based question-answering were good.",2.5 Quality of the postings,[0],[0]
"However, it was surprising that human users also struggled to obtain scores over 4.0, indicating that generating utterances for a particular character is difficult, even for humans.",2.5 Quality of the postings,[0],[0]
We asked users of the channels to participate in a survey to determine their user satisfaction.,2.6 Satisfaction of users,[0],[0]
"We used the same questionnaire as in (Higashinaka et al., 2013a).",2.6 Satisfaction of users,[0],[0]
"It consisted of three questions: (Q1) How do you rate the usability of the Web site?, (Q2) Would you be willing to use the Web site again?, and (Q3) Did you enjoy role playing on the Web site?",2.6 Satisfaction of users,[0],[0]
"The users answered based on a five-point Likert scale, with one being the lowest score and five the highest.",2.6 Satisfaction of users,[0],[0]
"Twenty-three and 36 participants took part in the survey for Murai and Ayase, respectively.
",2.6 Satisfaction of users,[0],[0]
Table 3 shows the results of the questionnaire averaged over all participants.,2.6 Satisfaction of users,[0],[0]
"Since these results were obtained from volunteers, they may not reflect the view of all site users.",2.6 Satisfaction of users,[0],[0]
"However, the results are encouraging: at the very least, they indicate that there are real users who feel very positively about the experience of role play-based questionanswering.",2.6 Satisfaction of users,[0],[0]
"Now that we have successfully collected a large number of question-answer pairs for our two characters, the next step is to determine if the collected pairs can be useful for creating chatbots that exhibit the personalities of the characters in question; namely, Murai and Ayase.",3 Creating chatbots from collected question-answer pairs,[0],[0]
"Since the size of the data was not large enough to train neural-generation models (Vinyals and Le, 2015), we opted for a retrieval-based approach in which relevant question-answer pairs are retrieved using an input question as a query and the answer part of the most relevant pair is returned as a chatbot’s response.",3 Creating chatbots from collected question-answer pairs,[0],[0]
"One of the methods we used is a simple application of an off-the-shelf text search engine, and the other is our proposed method, which is more sophisticated and uses neural-translation models for ranking.",3 Creating chatbots from collected question-answer pairs,[0],[0]
This method uses the text search engine LUCENE2 for retrieval.,3.1 Simple retrieval-based method,[0],[0]
Questions and answers are first indexed with LUCENE.,3.1 Simple retrieval-based method,[0],[0]
We use a built-in Japanese analyzer for morphological analysis.,3.1 Simple retrieval-based method,[0],[0]
"Given an input question, the BM25 algorithm (Walker et al., 1997) is used to search for a similar question using the content words of the input question.",3.1 Simple retrieval-based method,[0],[0]
The answers for the retrieved questions are used as the output of this method.,3.1 Simple retrieval-based method,[0],[0]
"Although simple, this method is quite competitive with other methods when there are many question-answer pairs because it is likely that we will be able to find a similar question by word matching.",3.1 Simple retrieval-based method,[0],[0]
Only using word-matching may not be sufficient.,3.2 Proposed method,[0],[0]
"Therefore, we developed a more elaborate method that re-ranks the results retrieved from LUCENE.",3.2 Proposed method,[0],[0]
"Our idea comes from cross-lingual question answering (CLQA) (Leuski et al., 2009) and recent advances in neural conversational models (Vinyals and Le, 2015).",3.2 Proposed method,[0],[0]
"We also conducted semantic and intent-level matching between ques-
2 https://lucene.apache.org/
tions so that appropriate answer candidates could be ranked higher.",3.2 Proposed method,[0],[0]
Figure 4 shows the flow of this method.,3.2 Proposed method,[0],[0]
"Given an input question Q, the method outputs answers in the following steps.",3.2 Proposed method,[0],[0]
"The details of some of the key models/modules used in the steps are described later.
1.",3.2 Proposed method,[0],[0]
"Given Q, LUCENE retrieves top-N questionanswer pairs (Q′1, A ′ 1) . . .",3.2 Proposed method,[0],[0]
"(Q ′ N , A′ N ), as de-
scribed in Section 3.1.
2.",3.2 Proposed method,[0],[0]
"The question-type estimation and extended
named entity recognition modules estimate the question types of Q and Q′ and extract extended named entities (Sekine et al., 2002) contained in A′.",3.2 Proposed method,[0],[0]
"The question-type match score is calculated by using the match of the question type and the number of extended named entities in A′ requested by Q. See Section 3.3 for details.
",3.2 Proposed method,[0],[0]
3.,3.2 Proposed method,[0],[0]
"The center-word extraction module extracts
center-words (noun phrases (NPs) that represent foci/topics) from both Q and Q′. The center-word score is 1.0 if one of the centerwords of Q is included in those of Q′; otherwise it is 0.0.
4.",3.2 Proposed method,[0],[0]
"The translation model is used to calculate the
probability that each A′ is translated from Q, that is, p(A′|Q).",3.2 Proposed method,[0],[0]
"We also calculate the probability bi-directionally, that is, p(Q|A′), which has been shown to be effective in CLQA (Leuski et al., 2009).",3.2 Proposed method,[0],[0]
The probabilities are normalized by dividing them by the number of words on the target side.,3.2 Proposed method,[0],[0]
"Since the raw probabilities are difficult to integrate with other scores, we sort the question-answer pairs by their probabilities and use their ranks
to obtain the translation scores.",3.2 Proposed method,[0],[0]
"That is, if the rank is r, its score is calculated by
1.0 − (r − 1)/max rank, (1)
where max rank is the maximum number of elements to be ranked.
5.",3.2 Proposed method,[0],[0]
"The semantic similarity model is used to cal-
culate the semantic similarity score between Q and Q′. We use Word2vec (Mikolov et al., 2013) to calculate this score.",3.2 Proposed method,[0],[0]
"First, we obtain word vectors (trained from Wikipedia) for each word in Q and Q′ and then calculate the cosine similarity between the averaged word vectors.
6.",3.2 Proposed method,[0],[0]
"The score calculation module integrates the
above scores to obtain a final score:
score(Q, (Q′, A′))
",3.2 Proposed method,[0],[0]
"= w1 ∗ search score
+ w2 ∗ qtypes match score
+ w3 ∗ center-word score
+ w4 ∗ translation score
+ w5 ∗ rev translation score
+ w6 ∗ semantic similarity score (2)
Here, search score indicates the score converted from the rank of the search results from LUCENE.",3.2 Proposed method,[0],[0]
The conversion is done using Eq.,3.2 Proposed method,[0],[0]
(1).,3.2 Proposed method,[0],[0]
rev translation score indicates the translation score derived from p(Q|A′).,3.2 Proposed method,[0],[0]
The w1 . . .,3.2 Proposed method,[0],[0]
"w6 denote the weights of the scores.
7.",3.2 Proposed method,[0],[0]
"The question-answer pairs are sorted by their
scores, and top-M answers are returned as output.",3.2 Proposed method,[0],[0]
"We describe some of the models/modules used in the above steps.
",3.3 Modules,[0],[0]
Question-type estimation and extended named entity recognition We estimated four question types for a question.,3.3 Modules,[0],[0]
One is a general question type.,3.3 Modules,[0],[0]
"We used the taxonomy described in (Higashinaka et al., 2014), which has 16 question subtypes.",3.3 Modules,[0],[0]
We trained a logistic-regression based question-type classifier that classifies a question into one of the 16 question types.,3.3 Modules,[0],[0]
The other three question types come from an extended named entity taxonomy proposed by Sekine (2002).,3.3 Modules,[0],[0]
"The taxonomy has three layers ranging from abstract
(e.g., Product, Location) to more concrete entities (e.g., Car, Spa, City).",3.3 Modules,[0],[0]
We trained a logisticregression-based classifier that classifies which of the named entity types is requested in a question.,3.3 Modules,[0],[0]
"We trained a classifier for each layer; thus, we had three classifiers.",3.3 Modules,[0],[0]
"Using our in-house data, by two-fold cross-validation, the classification accuracies are 86.0%, 84.9%, 76.9%, and 73.5% for the general question type, layer-1, layer-2, and layer-3 question types, respectively.",3.3 Modules,[0],[0]
"We also extract extended named entities from an answer candidate (A′) by using our extended named entity recognizer (Higashinaka et al., 2013b) and check whether the extended named entities corresponding to the layer-1, layer-2, and layer-3 question types of a question (Q) are included in A′.
The qtypes match score is calculated as follows: if there is a match of the general question type between Q and Q′, the score of one is obtained.",3.3 Modules,[0],[0]
"Then, the number of extended-namedentity question types covered by the answer candidate is added to this score.",3.3 Modules,[0],[0]
"Finally, this score is divided by four for normalization.
",3.3 Modules,[0],[0]
Center-word extraction We define a centerword as an NP that denotes the topic of a conversation.,3.3 Modules,[0],[0]
"To extract such NPs from an utterance, we used conditional random fields (CRFs) (Lafferty et al., 2001).",3.3 Modules,[0],[0]
"For the training and testing, we prepared about 20K sentences with centerword annotation.",3.3 Modules,[0],[0]
The sentences were those randomly sampled from our in-house open-domain conversation corpus.,3.3 Modules,[0],[0]
"The feature template uses words, part-of-speech (POS) tags, and semantic categories of current and neighboring words.",3.3 Modules,[0],[0]
"The extraction accuracy is 76% in F-measure with our in-house test set.
",3.3 Modules,[0],[0]
Translation model We trained a translation model by using a seq2seq model.,3.3 Modules,[0],[0]
We trained the model by using the OpenNMT Toolkit3 with default settings.,3.3 Modules,[0],[0]
The translation model learns to translate a question into an answer.,3.3 Modules,[0],[0]
"By using the trained model, we can obtain the generative probability of an answer given a question; namely p(A′|Q).",3.3 Modules,[0],[0]
"Since the amount of question-answer pairs was limited, we first trained a model by using our in-house question-answering data comprising 0.5 million pairs.",3.3 Modules,[0],[0]
The data were collected using crowd-sourcing.,3.3 Modules,[0],[0]
We then adapted the model to our question-answer pairs.,3.3 Modules,[0],[0]
"The model for p(Q|A′) was trained in the same manner by swapping the
3 http://opennmt.net/
source and target data.",3.3 Modules,[0],[0]
"To reflect the number of “likes” associated with the answers (see Section 2.2), we augmented the number of samples by their number of “likes”; that is, if a questionanswer pair has n “likes”, n samples of such a question-answer pair are included in the training data.",3.3 Modules,[0],[0]
"When developing our method, we noticed that, in some cases, top-N search results do not contain good candidates because of the lack of question coverage.",3.4 Extending question-answer pairs,[0],[0]
"When the top-N questions do not semantically match reasonably with the input question, the answers are likely to be inappropriate.",3.4 Extending question-answer pairs,[0],[0]
"To have a wider coverage of questions, we extended our question-answer pairs by using Twitter.",3.4 Extending question-answer pairs,[0],[0]
"Our methodology was simple: for each answer A that occurred twice or more in our questionanswer pairs, we searched for tweets that resemble A with a Levenshtein distance (normalized by the sentence length) below 0.1.",3.4 Extending question-answer pairs,[0],[0]
"Then, if the tweets had an in-reply-to relationship to other tweets, they were retrieved and coupled with A to form extended question-answer pairs.",3.4 Extending question-answer pairs,[0],[0]
"The reason we focused on an answer that occurred twice or more is mainly due to the efficiency of crawling, but such answers that occur multiple times are likely to be characteristics of the characters in question.",3.4 Extending question-answer pairs,[0],[0]
"We obtained 2,607,658 and 1,032,492 extended question-answer pairs for Murai and Ayase, respectively.",3.4 Extending question-answer pairs,[0],[0]
We conducted a subjective evaluation to determine the quality of chatbots created from our collected question-answer pairs.,4 Experiments,[0],[0]
We first describe how we prepared the data for evaluation and how we recruited participants.,4 Experiments,[0],[0]
We then describe the evaluation criteria.,4 Experiments,[0],[0]
"Next, we describe the methods for comparison, in which we compared the methods presented in the previous section with a rulebased baseline and gold data (human-generated data).",4 Experiments,[0],[0]
"Finally, we explain the results and present our analyses.",4 Experiments,[0],[0]
"To create the data for testing, we first randomly split the question-answer pairs into train, development, and test sets with the ratios of 0.8, 0.1, and 0.1, respectively.",4.1 Data,[0],[0]
The splits were made so that the same question would not be included over multiple sets.,4.1 Data,[0],[0]
"We used the train and development
sets to train the translation models.",4.1 Data,[0],[0]
"In addition, the question-answer pairs used by LUCENE for retrieval consisted only of train and development data.",4.1 Data,[0],[0]
"For each character, 50 questions were randomly sampled from the test set and used as input questions for this experiment.",4.1 Data,[0],[0]
We recruited 26 participants each for Murai and Ayase.,4.2 Procedure,[0],[0]
The participants were recruited mainly from the subscribers of the channels for the two characters.,4.2 Procedure,[0],[0]
"Before taking part in the experiment, they self-declared their levels of knowledge about the characters.",4.2 Procedure,[0],[0]
"Then, they rated the top-1 output of the five methods (shown below) for the 50 questions; they rated at maximum 250 answers (since some methods output duplicate answers, such answers were only rated once).",4.2 Procedure,[0],[0]
We compensated for their time by giving Amazon gift cards worth about 20 US dollars.,4.2 Procedure,[0],[0]
"The participants rated each output answer by their degree of agreement to the following statements on a five-point Likert scale (1: completely disagree, 5: completely agree).
",4.3 Evaluation criteria,[0],[0]
"Naturalness Not knowing who’s speaking, the
answer is appropriate to the input question.
",4.3 Evaluation criteria,[0],[0]
"Character-ness Knowing that the character in
question is speaking, the answer is appropriate to the input question.
",4.3 Evaluation criteria,[0],[0]
"The first criterion evaluates the interaction from a general point of view, while the second from the character point of view.",4.3 Evaluation criteria,[0],[0]
"Ideally, we want the character-ness to be high, but we want to maintain at least reasonable naturalness when considering the deployment of the chatbots.",4.3 Evaluation criteria,[0],[0]
"Note that an utterance can be rated low in terms of naturalness but high in character-ness, or vice-versa: for example, some general utterances, such as greetings, can never be uttered by particular characters.",4.3 Evaluation criteria,[0],[0]
We compared five methods.,4.4 Methods for comparison,[0],[0]
"A rule-based baseline written in Artificial Intelligence Markup Language (AIML) (Wallace, 2009) was used.",4.4 Methods for comparison,[0],[0]
The aim of having this baseline is to emulate when we do not have any question-answer pairs available.,4.4 Methods for comparison,[0],[0]
"Although this is a simple rule-based baseline, it is a competitive one because it uses one of the largest rule sets in Japanese.
",4.4 Methods for comparison,[0],[0]
Rule-based baseline (AIML),4.4 Methods for comparison,[0],[0]
"The typical ap-
proach to implement a chatbot is by using rules.",4.4 Methods for comparison,[0],[0]
We used the rules written in AIML created by Higashinaka et al (2015).,4.4 Methods for comparison,[0],[0]
There are roughly 300K rules.,4.4 Methods for comparison,[0],[0]
"In Japanese, sentence-end expressions are key factors to exhibit personality.",4.4 Methods for comparison,[0],[0]
"Therefore, following the method by Miyazaki et al. (2016), we created sentence-end conversion rules so that the output of this method would have the sentence-end expressions that match the characters in question.
",4.4 Methods for comparison,[0],[0]
Retrieval-based method (LUCENE),4.4 Methods for comparison,[0],[0]
"The
retrieval-based method described in Section 3.1.
",4.4 Methods for comparison,[0],[0]
Proposed method 1 (PROP WO EXDB),4.4 Methods for comparison,[0],[0]
"The
proposed method described in Section 3.2.",4.4 Methods for comparison,[0],[0]
This method does not use the extended question-answer pairs from Twitter.,4.4 Methods for comparison,[0],[0]
The weights w1 . . .,4.4 Methods for comparison,[0],[0]
w6 are all set to 1.0.,4.4 Methods for comparison,[0],[0]
"We used 10 for N for document retrieval.
",4.4 Methods for comparison,[0],[0]
Proposed method 2 (PROP),4.4 Methods for comparison,[0],[0]
"The proposed
method with extended question-answer pairs from Twitter, as described in Section 3.4.",4.4 Methods for comparison,[0],[0]
We retrieved 10 candidates from collected question-answer pairs and 10 from extended ones.,4.4 Methods for comparison,[0],[0]
The weights w1 . . .,4.4 Methods for comparison,[0],[0]
"w6 are all set to 1.0.
",4.4 Methods for comparison,[0],[0]
Upper bound (GOLD),4.4 Methods for comparison,[0],[0]
"The gold responses by
the online users to the test questions.",4.4 Methods for comparison,[0],[0]
"When multiple answers are given to a question, one is randomly selected.",4.4 Methods for comparison,[0],[0]
"Tables 4 and 5 list the results for Murai and Ayase, respectively.",4.5 Results,[0],[0]
The topmost row indicates the level of knowledge about the characters.,4.5 Results,[0],[0]
"‘All’ indicates the results of all participants, ‘High’ those who self-declared as being very knowledgeable, and ‘Low’ those who self-declared otherwise.",4.5 Results,[0],[0]
"We had 26 High and 6 Low participants for Murai, and 23 High and 3 Low participants for Ayase.
",4.5 Results,[0],[0]
"The tendencies were the same for the two characters, although the scores for Ayase were generally lower than those of Murai.",4.5 Results,[0],[0]
AIML performed the worst followed by LUCENE.,4.5 Results,[0],[0]
It was surprising that AIML’s score was low; this is probably because of the peculiarities of the input questions for the characters.,4.5 Results,[0],[0]
PROP WO EXDB and PROP performed better than AIML and LUCENE with statistical significance in many cases.,4.5 Results,[0],[0]
GOLD was always the best-performing method.,4.5 Results,[0],[0]
"PROP was significantly better than PROP WO EXDB for naturalness but not for character-ness.
",4.5 Results,[0],[0]
"These results indicate that simple text-based retrieval is not sufficient, and we need more elaborate methods.",4.5 Results,[0],[0]
The effectiveness of the extended question-answer pairs seems to be limited.,4.5 Results,[0],[0]
"It can be useful to make the interaction seem natural, but this does not necessarily improve character-ness, although we believe that having the ability to converse naturally is a requirement for chatbots.
",4.5 Results,[0],[0]
"When we focus on the results as they relate to the knowledge levels, we see large differences between High and Low.",4.5 Results,[0],[0]
"The High participants are likely to differentiate the answers more than Low
participants.",4.5 Results,[0],[0]
"For example, for Murai, there were only few cases in which there was statistical significance between the proposed methods when the knowledge level was low.",4.5 Results,[0],[0]
The tendency was the same for Ayase.,4.5 Results,[0],[0]
"This highlights the difficulty in evaluating for characters.
",4.5 Results,[0],[0]
"Tables 6 and 7 show examples of answers for Murai and Ayase, respectively.",4.5 Results,[0],[0]
"Overall, since the proposed methods achieved character-ness scores well over 3 (which is the middle point in the scale), we conclude that we can create chatbots with consistent personalities by means of role play-based question-answering.",4.5 Results,[0],[0]
"Although there have not been any studies involving role play-based question-answering for data collection, there is a large body of research for creating chatbots that show consistent personalities.
",5 Related Work,[0],[0]
"There have been several studies on characters by generating or rewriting utterances reflecting the underlying personality traits (Mairesse and Walker, 2007; Sugiyama et al., 2014; Miyazaki et al., 2016).",5 Related Work,[0],[0]
"In addition, there has been extensive research on extending neural conversational models to reflect personal profiles (Li et al., 2016).",5 Related Work,[0],[0]
"Although such neural networkbased methods show promising results, they still suffer from sparsity of data and non-informative utterances (Li et al., 2015).",5 Related Work,[0],[0]
This paper proposed increasing the source data for character building; the data can be useful for neural models.,5 Related Work,[0],[0]
Our goal for this study was to verify the effectiveness of role play-based question-answering for creating chatbots.,6 Summary and future work,[0],[0]
"Focusing on two famous char-
acters in Japan, we successfully collected a large volume of question-answer pairs for two characters by using real users.",6 Summary and future work,[0],[0]
We then created chatbots using the question-answer pairs.,6 Summary and future work,[0],[0]
"Subjective evaluation showed that although a simple textretrieval based method does not work well, our proposed method that uses translation models as well as question-type matching and center-word extraction works well, showing reasonable scores in terms of naturalness and character-ness.
",6 Summary and future work,[0],[0]
"For future work, we need to consider approaches to improve the quality of the proposed method.",6 Summary and future work,[0],[0]
"For example, we are currently using equal weights for scoring.",6 Summary and future work,[0],[0]
We believe that they can be optimized using training data.,6 Summary and future work,[0],[0]
"We also want to incorporate other pieces of information that may contribute to the ranking of answers, such as sentence embeddings (Kiros et al., 2015), discourse relations (Lin et al., 2009; Otsuka et al., 2017), and external knowledge about the characters.",6 Summary and future work,[0],[0]
"Although we used two very different characters in this paper, we want to use additional types of characters as targets for role play-based question-answering.",6 Summary and future work,[0],[0]
We also want to incorporate the chatbots into the Web sites so that the users can feel they are training up the characters.,6 Summary and future work,[0],[0]
"We thank the developers of DWANGO Co., Ltd. for creating the role play-based questionanswering Web sites.",Acknowledgments,[0],[0]
We also thank the subscribers of the Max Murai and Tukasa Fushimi channels on NICONICO Douga for their cooperation.,Acknowledgments,[0],[0]
"We thank the members of the Service Innovation Department at NTT DOCOMO, especially Yuiko Tsunomori, for helpful discussions and suggestions.",Acknowledgments,[0],[0]
Having consistent personalities is important for chatbots if we want them to be believable.,abstractText,[0],[0]
"Typically, many questionanswer pairs are prepared by hand for achieving consistent responses; however, the creation of such pairs is costly.",abstractText,[0],[0]
"In this study, our goal is to collect a large number of question-answer pairs for a particular character by using role playbased question-answering in which multiple users play the roles of certain characters and respond to questions by online users.",abstractText,[0],[0]
"Focusing on two famous characters, we conducted a large-scale experiment to collect question-answer pairs by using real users.",abstractText,[0],[0]
"We evaluated the effectiveness of role play-based questionanswering and found that, by using our proposed method, the collected pairs lead to good-quality chatbots that exhibit consistent personalities.",abstractText,[0],[0]
Role play-based question-answering by real users for building chatbots with consistent personalities,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 12–22, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics",text,[0],[0]
"Tree transducers are general and solid theoretical models that have been applied to a variety of NLP tasks, such as machine translation (Knight and Graehl, 2005), text summarization (Cohn and Lapata, 2009), question answering (Jones et al., 2012), paraphrasing and textual entailment (Wu, 2005).",1 Introduction,[0],[0]
"One strategy to obtain transducer rules is by exhaustive enumeration; however, this method is ineffective when there is a high structural language variability and we wish to have an expressive model.",1 Introduction,[0],[0]
"Another strategy is to heuristically extract rules from a corpus of tree/string pairs and word-alignments, as
GHKM algorithm does (Galley et al., 2004); however, word-alignments are difficult to estimate when the corpus is small.",1 Introduction,[0],[0]
"This would be the case, for example, of machine translation for low-resourced languages where there is often small numbers of parallel sentences, or in Question Answering (QA) tasks where the number of Knowledge Base (KB) identifiers (concepts) is much larger than QA datasets.
",1 Introduction,[0],[0]
"Our main contribution is an algorithm that formulates the rule extraction as a cost minimization problem, where the search for the best rules is guided by an ensemble of cost functions over pairs of tree fragments.",1 Introduction,[0],[0]
"In GHKM, a tree fragment and a sequence of words are extracted together if they are minimal and their word alignments do not fall outside of their respective boundaries.",1 Introduction,[0],[0]
"However, given that alignment violations are not allowed, the quality of the extracted rules degrades as the rate of misaligned words increases.",1 Introduction,[0],[0]
"In our framework, we can mimic GHKM by assigning an infinite cost to pairs of tree fragments that violate such conditions on word alignments and by adding a cost regularizer on the size of the tree fragments.",1 Introduction,[0],[0]
"Smoother cost functions, however, would permit controlled misalignments, contributing to generalization.",1 Introduction,[0],[0]
"Given the generality of these cost functions, we believe that the applicability of tree transducers will be extended.
",1 Introduction,[0],[0]
"A by-product of introducing these cost functions is that some of them may act as rule back-offs, where transducer rules are built “on-the-fly” when the transducer is at a predefined back-off state but there is no rule whose left-hand-side (lhs) matches the input subtree.",1 Introduction,[0],[0]
"These back-off states can be seen as functions that are capable of generating right-
12
hand-sides (rhs) for unseen input subtrees.",1 Introduction,[0],[0]
"Our rule extraction algorithm and back-off scheme are general, in the sense that they can be applied to any tree transformation task.",1 Introduction,[0],[0]
"However, in this paper, we extrinsically evaluate the quality of the extracted rules in a QA task, where the objective is to transform syntactic trees of questions into constituent trees that represent Sparql queries on Freebase, a large Knowledge Base.",1 Introduction,[0],[0]
"Implementing all components of a QA system at a sufficient level is out of the scope of this paper; for that reason, in order to evaluate our contribution in isolation, we use the FREE917 corpus released by Cai and Yates (2013), for which an entity and predicate lexicon is available1.",1 Introduction,[0],[0]
"We show that a tree-to-tree transducer induced using our rule extraction and back-off scheme is accurate and generalizes well, which was not previously achieved with tree transducers in semantic parsing tasks such as QA over large KBs.",1 Introduction,[0],[0]
"Tree transducers were first proposed by Rounds (1970) and Thatcher (1970), and have been greatly developed recently (Knight and Graehl, 2005).",2 Related Work,[0],[0]
"Jones et al. (2012) used tree transducers to semantically parse narrow-domain questions into Prolog queries for GeoQuery (Wong and Mooney, 2006), a small database of 700 geographical facts.",2 Related Work,[0],[0]
"Rules were exhaustively enumerated, which was possible given the small size of the database and low variability of questions.",2 Related Work,[0],[0]
"Another strategy is that of Li et al. (2013), where they used a variant of GHKM to induce tree transducers that parse into λ-SCFG.",2 Related Work,[0],[0]
"Wordto-node alignments could be reliably estimated with the IBM models (Brown et al., 1993) given, again, the small vocabulary and database size of GeoQuery.",2 Related Work,[0],[0]
"In such small-scale tasks, our rule extraction and back-off scheme offers no obvious advantage.",2 Related Work,[0],[0]
"However, when doing QA over larger and more realistic KBs (and other tasks with similar characteristics), exhaustive enumeration of rules or reliable estimations of alignments are not possible, which prevents the application of tree transducers.",2 Related Work,[0],[0]
"Thus, it is on the latter type of tasks where we focus our contribution.
",2 Related Work,[0],[0]
"A similar problem has been considered in the tree
1The entity lexicon was released by the authors of FREE917, and the predicate lexicon is ours.
mapping literature in the form of the tree-to-tree edit distance.",2 Related Work,[0],[0]
"In that formulation, three edit operations are defined, namely, deleting and inserting single nodes, and replacing the label of a node.",2 Related Work,[0],[0]
"These edit operations have a cost associated to them, and the task consists of finding the minimum edit cost and its corresponding edit script2 that transforms a source into a target tree.",2 Related Work,[0],[0]
"The problem was first solved by Tai (1979), and later Zhang and Shasha (1989) proposed a simpler and faster dynamic programming algorithm that operates in polynomial time, and that has inspired multiple variations (Bille, 2005).
",2 Related Work,[0],[0]
"However, we need edit operations that involve tree fragments (e.g., noun phrases or parts of verb phrases), rather than single nodes, when searching for the best mappings.",2 Related Work,[0],[0]
"We address this problem by searching for non-isomorphic tree mappings, in line with Eisner (2003), except that our rule extraction algorithm is guided by an ensemble of cost functions over pairs of tree fragments.",2 Related Work,[0],[0]
This algorithm is capable of extracting rules more robustly than GHKM by permitting misalignments in a controlled manner.,2 Related Work,[0],[0]
"Finding a tree mapping solves simultaneously the alignment and the rule extraction problem.
",2 Related Work,[0],[0]
"There is a wide array of tree transducers with different expressive capabilities (Knight and Graehl, 2005).",2 Related Work,[0],[0]
"We consider extended3 root-to-frontier4 linear5 transducers (Maletti et al., 2009), possibly with deleting6 operations.",2 Related Work,[0],[0]
"In this paper, we syntactically parse the natural language question and transform it into a meaning representation, similarly to Ge and Mooney (2005).",2 Related Work,[0],[0]
"But instead of using Prolog formulae or λ-SCFG, we use constituent representations of λ−DCS expressions (Liang, 2013), which is a formal language convenient to represent Sparql queries where variables are eliminated by making existential quantifications implicit (see example in Figure 1).
",2 Related Work,[0],[0]
"Another challenge is to construct transducers with sufficient rule coverage, which would require billions of lexical rules that map question phrases to database entities or relations.",2 Related Work,[0],[0]
"Even if those rules were available, estimating their rule probabilities would be difficult given the small data sets of ques-
2Sequence of edit operations.",2 Related Work,[0],[0]
3lhs may have depth larger than 1. 4Top-down transformations.,2 Related Work,[0],[0]
5lhs variables appear at most once in the rhs.,2 Related Work,[0],[0]
"6Some variables on the lhs may not appear in the rhs.
tions paired with their logical representations.",2 Related Work,[0],[0]
"We solve the problem by constructing lexical rules “onthe-fly” at the decoding stage, similarly to the candidate generation stage of entity linking systems (Ling et al., 2015).",2 Related Work,[0],[0]
Rule weights are also predicted on-thefly given rule features and model parameters similar to Cohn and Lapata (2009).,2 Related Work,[0],[0]
"Tree transducers apply to general tree transformation problems, but for illustrative purposes, we use the tree pair s and t in Figure 1 (from FREE917) as a running example.",3 Background,[0],[0]
"s is the syntactic constituent tree of the question “how many teams participate in the uefa”, whereas t is a constituent tree of an executable meaning representation in the λ−DCS formalism:
count(Team.",3 Background,[0],[0]
League.,3 Background,[0],[0]
"Uefa)
Its corresponding lambda expression is
count(λx.∃a.Team(x, a) ∧ League(a, Uefa))
which can be converted into a Sparql KB query:
SELECT COUNT(?x) WHERE { ?a Team ?x .
?",3 Background,[0],[0]
"a League Uefa . }
",3 Background,[0],[0]
"Following the terminology of Graehl and Knight (2004), we define a tree-to-tree transducer as a 5- tuple (Q,Σ,∆, qstart,R) where Q is the set of states, Σ and ∆ are the sets of symbols of the input and output languages, qstart is the initial state, and R is the set of rules.",3 Background,[0],[0]
"For convenience, define TΣ as the set of trees with symbols in Σ, TΣ(A) the set of trees with symbols in Σ ∪ A where symbols in A only appear in the leaves, X as the set of variables {x1, . . .",3 Background,[0],[0]
", xn}, and A.B for the cross-product of two sets A and B. A rule r ∈ R has the form q.ti
s→ to, where q ∈ Q is a state, ti ∈ TΣ(X ) is the left-hand-side (lhs) tree pattern (or elementary tree), to ∈ T∆(Q.X )",3 Background,[0],[0]
"the right-hand-side (rhs), and s ∈ R",3 Background,[0],[0]
"the rule score.
",3 Background,[0],[0]
Tree-to-tree transducers apply a sequence of rules to transform a source s into a target t tree.,3 Background,[0],[0]
"A root-tofrontier transducer starts at the root of the source tree and searches R for a rule whose i) tree pattern ti on the lhs matches the root of the source tree, and ii) the
state q of the rule is the initial state of the transducer.",3 Background,[0],[0]
An incipient target tree is created by copying the rhs of the rule.,3 Background,[0],[0]
"Then, the transducer recursively and independently visits the subtrees of the source tree at the lhs variable positions of the rule from their new states, and copies the results into the same variable on the target tree.
",3 Background,[0],[0]
"In Figure 1, the sequential application of rules r1 to r5 is a derivation that transforms the question s into the query t.",3 Background,[0],[0]
"For example, rule r1 consumes a tree fragment of s (e.g. “how”, “many”, “WRB”, etc.) and produces a tree fragment with terminals (“COUNT”, x1, x2) and non-terminals (“ID”) with
a specific structure.",3 Background,[0],[0]
Rules r2 and r3 only consume but do not produce symbols (other than variables).,3 Background,[0],[0]
The rhs of rules are target tree fragments that connect to each other at the frontier nodes (those with variables).,3 Background,[0],[0]
"Rules r4 and r5 are terminal rules, where r4 produces the predicate Team and rule r5 produces the entity Uefa and a disambiguating predicate League that has no lexical support on the source side, similarly to the role that bridging predicates play in Berant et al. (2013).
",3 Background,[0],[0]
"Given a corpus of source and target tree pairs, the learning stage aims to obtain rules such as r1−r5 in Figure 1 and their associated probabilities or scores.",3 Background,[0],[0]
We discuss our novel approach to rule extraction in Section 5.,3 Background,[0],[0]
"For the assignment of rule scores, we adopt the latent variable averaged structured perceptron, a discriminative procedure similar to Tsochantaridis et al. (2005) and Cohn and Lapata (2009).",3 Background,[0],[0]
"Here, we instantiate feature values f for every rule, and reward the weights w of rules that participate in a derivation (latent variable) that transforms a training source tree into a meaning representation that retrieves the correct answer.
",3 Background,[0],[0]
"At decoding stage, rule scores can be predicted as s = w · f .",3 Background,[0],[0]
"However, we cannot expect to have extracted all necessary rules at the training stage given the small training data and large-scale KB.",3 Background,[0],[0]
"For that reason, we propose in Section 4 a novel rule back-off scheme to alleviate coverage problems.",3 Background,[0],[0]
"As an illustrative example, consider the question “how many teams participate in the nba”, and the rules r1 to r5 in Figure 1.",4 Back-off rules,[0],[0]
"When the transducer attempts to transform the noun phrase (NP (DT the) (NN nba)), no rule’s lhs matches it.",4 Back-off rules,[0],[0]
"However, since the transducer is at state bridge (as specified by the rhs of r3), it should be able to produce a list of bridged entities, among which the target subtree (ID League NBA) will be hopefully included.",4 Back-off rules,[0],[0]
"Thus, the following rule should be created for the occasion:
This mechanism produces rules “on-the-fly”, allowing us to compensate low rule coverage by consuming and producing tree fragments that were not nec-
essarily observed in the training data.",4 Back-off rules,[0],[0]
"Back-off rules are produced when the transducer is at a back-off state qb ∈ Qb ⊂ Q, similarly as the back-off mechanisms in finite-state language models where we produce estimates (probabilities) of input structures (sequences) under less conditioning.",4 Back-off rules,[0],[0]
"In our scheme, a back-off state (or function) qb produces estimates that are target structures t2 ∈ T∆ with score s ∈ R, given some information of the source tree fragment t1 ∈ TΣ.",4 Back-off rules,[0],[0]
"That is, a function qb :",4 Back-off rules,[0],[0]
"TΣ → {(T∆,R), . . .}.",4 Back-off rules,[0],[0]
"In our QA application, we only use the leaves of the input subtree t1 and use lexicons or entity/predicate linkers to retrieve KB entities, KB relations or a compound of a disambiguating relation and an entity from backoff states ent, pred and bridge, respectively.",4 Back-off rules,[0],[0]
"Other back-off functions would transliterate the leaves of the input tree in machine translation, or produce synonyms/hypernyms in a paraphrasing application.
",4 Back-off rules,[0],[0]
"We associate a score s to these newly created rules, which we learn to predict using the discriminative training procedure suggested by Tsochantaridis et al. (2005), as described in Section 3.
",4 Back-off rules,[0],[0]
"Back-off rules are then constructed on-demand as qb.t1
s→ t2, and the discrete set of rules R is augmented with them.",4 Back-off rules,[0],[0]
"It remains now to recognize those back-off states when inducing tree transducer grammars, which is covered in Section 5.1.",4 Back-off rules,[0],[0]
"Given a pair of trees, our rule extraction algorithm finds a tree mapping that implicitly describes the rules that transform a source into a target tree.",5 Rule Extraction,[0],[0]
"In the search of the best mapping, we need to explore the space of edit operations, which are substitutions of source by target tree fragments.",5 Rule Extraction,[0],[0]
"We define cost functions for these edit operations, and formulate the tree mapping as a cost minimization problem.",5 Rule Extraction,[0],[0]
"Whereas our tree mapping algorithm and back-off scheme are generic and can be used in any tree transformation task, cost functions depend on the application.",5 Rule Extraction,[0],[0]
"In general, cost functions are defined over edit operations, which are pairs of source and target tree fragments, cost : TΣ(X )",5.1 Cost functions,[0],[0]
"× T∆(Q.X ) → R≥0, and they are equivalent to feature functions.",5.1 Cost functions,[0],[0]
"Some cost
functions are defined over all pairs of tree fragments.",5.1 Cost functions,[0],[0]
"For this QA application, these are:
csize(t1, t2) = |nodes(t1)|2 + |nodes(t2)|2
which acts as a tree size regularizer, returning a cost quadratic to the size of the tree fragments, thus encouraging small rules.",5.1 Cost functions,[0],[0]
"The cost function ccount assigns zero cost if (i) “how” and “many” appear in t1, and (ii) “COUNT” appears in t2.",5.1 Cost functions,[0],[0]
"If only either (i) or (ii), the cost is a positive constant.",5.1 Cost functions,[0],[0]
"Similarly, other operators (max, min, argmax, etc.) could be recognized, but this dataset did not require them.
",5.1 Cost functions,[0],[0]
Other cost functions only apply to some pairs of tree fragments.,5.1 Cost functions,[0],[0]
"These are the back-off functions described in Section 4, but instead of returning scores for every target tree fragment, they return a cost, e.g. cent :",5.1 Cost functions,[0],[0]
TΣ × T∆ → R≥0.,5.1 Cost functions,[0],[0]
"An ensemble will produce up to three different costs for every pair of tree fragments, depending on what back-off functions were triggered.",5.1 Cost functions,[0],[0]
"In the case of the entity cost function:
γent(t1, t2) = λ1 · csize(t1, t2) + λ2 · ccount(t1, t2) + λ3 · cent(t1, t2)
(1)
where λi ∈ R≥0 are scaling factors.",5.1 Cost functions,[0],[0]
"In the search of the lowest-cost mapping, the labels of the cost functions that are derived from the back-off functions (e.g. γent, γpred) are memorized for the pairs (t1, t2) for which they were defined and for which they outputted a cost.",5.1 Cost functions,[0],[0]
These labels are then used as back-off rule state names when constructing rules.,5.1 Cost functions,[0],[0]
"Intuitively, the cost of mapping a source node ns to a target node nt is equal to the cost of transforming a tree fragment TΣ(X ) rooted at node ns into a tree fragment T∆(Q.X ) rooted at node nt, plus the sum of costs of mapping the frontier nodes rooted at the variables.",5.2 Tree Mapping: Optimization Problem,[0],[0]
"In order to formalize our tree mapping, we need a more precise definition of a tree fragment where the locations of variables X are specified by paths.",5.2 Tree Mapping: Optimization Problem,[0],[0]
"The notation to specify subtrees is taken from (Graehl and Knight, 2004), and we introduce the ⊥ operator for convenience.
",5.2 Tree Mapping: Optimization Problem,[0],[0]
"A path p is a tuple, equivalent to a Gorn address, that uniquely identifies the node of a tree by specifying the sequence of child indices to the node from
the root.",5.2 Tree Mapping: Optimization Problem,[0],[0]
"In the tree s of Figure 1, the path to the VP node is (1, 0), whereas in t, the path to League is (1, 1, 0).",5.2 Tree Mapping: Optimization Problem,[0],[0]
The path p = () refers to the root of a tree.,5.2 Tree Mapping: Optimization Problem,[0],[0]
We denote by s ↓ p the subtree of tree s that is rooted at path p and that has no variables.,5.2 Tree Mapping: Optimization Problem,[0],[0]
"In Figure 1, the left-hand-side (lhs) of r5 is the subtree s ↓",5.2 Tree Mapping: Optimization Problem,[0],[0]
"(1, 0, 1, 1).",5.2 Tree Mapping: Optimization Problem,[0],[0]
"In order to introduce variables, we generalize the notion of subtree into a tree pattern s ↓ p ⊥ {p1, . . .",5.2 Tree Mapping: Optimization Problem,[0],[0]
", pn}, where n variables replace subtrees s ↓ pi at subpaths pi ∈ {p1, . . .",5.2 Tree Mapping: Optimization Problem,[0],[0]
", pn}.",5.2 Tree Mapping: Optimization Problem,[0],[0]
"For example, the lhs of r1 can be represented with the tree pattern s ↓ () ⊥ {(0, 1), (1)}, and r2 with s ↓ (1) ⊥ {(1, 0, 1)}.",5.2 Tree Mapping: Optimization Problem,[0],[0]
"Note that the order of subpaths {p1, . . .",5.2 Tree Mapping: Optimization Problem,[0],[0]
", pn} matters.",5.2 Tree Mapping: Optimization Problem,[0],[0]
"A tree pattern with no subpaths s ↓ p ⊥ {} is simply a subtree s ↓ p, such as the lhs of rules r4 and r5; a tree pattern with only one subpath equal to its path s ↓ p ⊥ {p} is a single variable, such as the rhs of rules r2 and r3.",5.2 Tree Mapping: Optimization Problem,[0],[0]
"Note that in s ↓ p ⊥ {p1, . . .",5.2 Tree Mapping: Optimization Problem,[0],[0]
", pn}, all paths pi to variables are prefixed by p, and that no variables are descendants of any other variable in the same tree pattern.",5.2 Tree Mapping: Optimization Problem,[0],[0]
"In other words, p = {p1, . . .",5.2 Tree Mapping: Optimization Problem,[0],[0]
", pn} are disjoint subpaths given p, where p denotes a list of paths.
",5.2 Tree Mapping: Optimization Problem,[0],[0]
We can now formalize the tree mapping algorithm as an optimization problem.,5.2 Tree Mapping: Optimization Problem,[0],[0]
"Let γ (s ↓ ps ⊥ p, t ↓ pt ⊥ p′) be the cost to transform a source into a target tree pattern, as defined in Equation 1.",5.2 Tree Mapping: Optimization Problem,[0],[0]
"To transform s ↓ ps into t ↓ pt, we need to find the best combination of source subtrees rooted at {p1, . . .",5.2 Tree Mapping: Optimization Problem,[0],[0]
", pn} that can be transformed at minimum cost to the best combination of target subtrees at {p′1, . . .",5.2 Tree Mapping: Optimization Problem,[0],[0]
", p′n}.",5.2 Tree Mapping: Optimization Problem,[0],[0]
"The transformation cost of a certain tree pattern s ↓ ps ⊥ {p1, . . .",5.2 Tree Mapping: Optimization Problem,[0],[0]
", pn} into t ↓ pt ⊥ {p′1, . . .",5.2 Tree Mapping: Optimization Problem,[0],[0]
", p′n} is equal to the cost of transforming the source tree pattern into the target tree pattern, plus the minimum cost to transform s ↓ pi into t ↓",5.2 Tree Mapping: Optimization Problem,[0],[0]
"p′i, for i ≥ 1.",5.2 Tree Mapping: Optimization Problem,[0],[0]
"That is:
C (s ↓ ps, t ↓ pt) =",5.2 Tree Mapping: Optimization Problem,[0],[0]
"min p,p′ {γ ( s ↓ ps ⊥ p, t ↓ pt ⊥ p′ )",5.2 Tree Mapping: Optimization Problem,[0],[0]
"+
|p|∑
i=1
C ( s ↓ pi, t ↓ p′i ) } (2)
subject to |p| = |p′|, that is, source and target tree patterns having the same number of variables.",5.2 Tree Mapping: Optimization Problem,[0],[0]
"Then, the cost of transforming the source into the target tree would be given by the expression
C (s ↓ (), t ↓ ()).",5.2 Tree Mapping: Optimization Problem,[0],[0]
"Since we are only interested in the pairs of source and target tree patterns that lead to the minimum cost, we keep track of subpaths p and p′ of tree pattern pairs that minimize the cost.",5.2 Tree Mapping: Optimization Problem,[0],[0]
This problem can be solved for small depths of tree patterns and a small number of variables by storing intermediate results in the computation of Eq. 2.,5.3.1 Overview,[0],[0]
"However, an exact implementation needs to enumerate all pairs of source and target disjoint subpaths (p and p′), which has a computational complexity that grows combinatorially with |p| (variable permutations), and exponentially with the number of descendant nodes of ps and pt (powerset of variables).
",5.3.1 Overview,[0],[0]
"Instead, we use a beam search algorithm (see Algorithm 1)7 that constructs source and target disjoint paths (p and p′) hierarchically (function GENERATEDISJOINT) in a bottom-up order, for any given path pair (ps, pt).",5.3.1 Overview,[0],[0]
"First, n-best solutions (pairs of disjoint paths) are computed for children; then those partial solutions are combined into their parent using the cross-product.",5.3.1 Overview,[0],[0]
"Solutions (with their associated cost) for every pair of paths (ps, pt) are stored in a weighted hypergraph, from which we can extract n-best derivations (sequences of rules).",5.3.1 Overview,[0],[0]
"In the pseudocode, we use a helper function, paths(s ↓ ps), which denotes the list of subtree paths in bottom-up order: from the leaves up to ps (including the latter).",5.3.1 Overview,[0],[0]
"For a certain path pair (ps, pt), there are three cases.",5.3.2 Detailed Description,[0],[0]
"The first case (line 34-35) considers a pair of empty disjoint subpaths (p,p′) =",5.3.2 Detailed Description,[0],[0]
"({}, {}), where the cost c of transforming s ↓ ps ⊥ {} into t ↓ pt ⊥ {} is evaluated and the empty disjoint subpaths are added to the priority queue P , indexed with ps.",5.3.2 Detailed Description,[0],[0]
"Such indexing is useful to retrieve the n-best pairs of disjoint subpaths accumulated at every tree node.
",5.3.2 Detailed Description,[0],[0]
The second case (line 28 to 31) evaluates the cost of transforming single-variable tree patterns: s ↓ ps ⊥ {pc} into t ↓ pt ⊥ {p′c}.,5.3.2 Detailed Description,[0],[0]
"In this case, variables substitute entire subtrees rooted at paths pc and p′c on the source and target tree patterns, respectively.",5.3.2 Detailed Description,[0],[0]
"Note that pc ranges over all node
7https://github.com/pasmargo/t2t-qa
Algorithm 1 Extraction of optimal sequence of rules to transform a source s into a target tree t. Input: Trees s and t, and ensemble of cost functions γ.",5.3.2 Detailed Description,[0],[0]
"Output: Sequence of optimal rules for s⇒∗ t.
1: let H = (V,E) be a hypergraph of solutions with V ← {} vertices and E ← {} hyperedges.",5.3.2 Detailed Description,[0],[0]
"2: for (ps, pt) ∈ paths(s)× paths(t) do 3: add vertex v = (ps, pt) to V 4: PP ← GENERATEDISJOINT(s ↓",5.3.2 Detailed Description,[0],[0]
"ps, t ↓ pt, γ) 5: for (p,p′) ∈ PP do 6: .",5.3.2 Detailed Description,[0],[0]
Get cost of tree pattern pair.,5.3.2 Detailed Description,[0],[0]
"7: c← γ (s ↓ ps ⊥ p, t ↓ pt ⊥ p′) 8: add edge (ps, pt)
c→ (p,p′) to E 9: end for
10: end for 11: return HYPERGRAPHSEARCH(H)
12: function GENERATEDISJOINT(s ↓ ps, t ↓ pt, γ) 13: P ← {} a priority queue of partial disjoint paths.",5.3.2 Detailed Description,[0],[0]
14: for every pc ∈ paths(s ↓ ps) do 15: .,5.3.2 Detailed Description,[0],[0]
Costs when variables combined from children.,5.3.2 Detailed Description,[0],[0]
16: for every pic immediate child of pc (if any) do 17: .,5.3.2 Detailed Description,[0],[0]
Retrieve n-best subpaths p and p′ from pic.,5.3.2 Detailed Description,[0],[0]
"18: C ← arg minn(p,p′){c | (pic,p,p′, c) ∈ P} 19: .",5.3.2 Detailed Description,[0],[0]
Combine subpaths with those accumulated 20: .,5.3.2 Detailed Description,[0],[0]
"from previous siblings and stored at path pc. 21: A← arg minn(p,p′){c | (pc,p,p′, c) ∈ P} 22: for (p,p′) ∈ (C ∪ (C.A)) do 23: c← γ (s ↓ ps ⊥ p, t ↓ pt ⊥ p′) 24: add (pc,p,p′, c) to priority queue P 25: end for 26: end for 27: .",5.3.2 Detailed Description,[0],[0]
Cost of tree patterns with one variable.,5.3.2 Detailed Description,[0],[0]
28: for every p′c ∈ paths(t ↓ pt) do 29: c←,5.3.2 Detailed Description,[0],[0]
"γ (s ↓ ps ⊥ {pc}, t ↓ pt ⊥ {p′c}) 30: add (pc, {pc}, {p′c}, c) to priority queue P 31: end for 32: end for 33: .",5.3.2 Detailed Description,[0],[0]
Cost of tree patterns with no variables.,5.3.2 Detailed Description,[0],[0]
"34: c← γ (s ↓ ps ⊥ {}, t ↓ pt ⊥ {}) 35: add (ps, {}, {}, c) to priority queue P 36: return arg minn(p,p′){c | (ps,p,p′, c) ∈ P} 37: end function
addresses that are descendant of ps (including ps), and similarly for p′c.",5.3.2 Detailed Description,[0],[0]
"The pairs of disjoint subpaths (p,p′) =",5.3.2 Detailed Description,[0],[0]
"({pc}, {p′c}) are added into the priority queue, indexed by pc.
",5.3.2 Detailed Description,[0],[0]
"The third case (line 16 to 26) performs the combination of subpaths hierarchically from children to parents, and incrementally across children.",5.3.2 Detailed Description,[0],[0]
"For every path pc ∈ paths(s ↓ ps), it visits its imme-
diate children pic one by one, and retrieves into C the n-best disjoint subpaths (line 18) that have already been obtained during previous iterations for pic.",5.3.2 Detailed Description,[0],[0]
"Then, we retrieve into A the n-best disjoint subpaths indexed at pc, which is a list of the best subpaths that were combined from previous immediate children (the list is empty if this is the first immediate child that we visit).",5.3.2 Detailed Description,[0],[0]
"The cross-product of disjoint subpaths in C and A, that is C.A, is then evaluated and the best combinations are stored in the priority queue indexed at pc.
",5.3.2 Detailed Description,[0],[0]
"As an example of a cross-product between two lists C and A of pairs of disjoint paths, let C = {(p1,p1′), (p2,p2′)} and A = {(p3,p3′)}.",5.3.2 Detailed Description,[0],[0]
"Then the cross-product C.A would be:
C.A = {(p1 · p3,p′1 · p′3), (p2 · p3,p′2 · p′3)}
where p1 · p3 = {(0, 1), (0, 2), (0, 3), (0, 4)} if p1 = {(0, 1), (0, 2)} and p3 = {(0, 3), (0, 4)}.",5.3.2 Detailed Description,[0],[0]
"At this stage, subpaths pi or p′i that are not disjoint are discarded, together with disjoint paths that produce tree patterns with depth larger than a certain userdefined threshold, or whose number of subpaths is larger than the number of variables allowed.
",5.3.2 Detailed Description,[0],[0]
"In line 24, the disjoint subpaths of C (in addition to their cross-product C.A) are also evaluated and added to the priority queue indexed by pc, to propagate upwards in the hierarchy of solutions the decision of not combining disjoint subpaths.
",5.3.2 Detailed Description,[0],[0]
"Finally, GENERATEDISJOINT returns the n-best pairs of disjoint subpaths of minimum cost (p, p′) that accumulated in the priority queue P for path ps.",5.3.2 Detailed Description,[0],[0]
"The n-best source and target pairs of disjoint subpaths are stored at every pair of source and target paths (ps, pt) (lines 2-10), forming a hypergraph, as in Figure 2.",5.3.3 Other Considerations,[0],[0]
"Then, with a hypergraph search (Huang and Chiang, 2007) we can retrieve at least n-best sequences of rules (derivations) that transform the source into the target tree (line 11).
",5.3.3 Other Considerations,[0],[0]
"To maintain diversity of partial disjoint subpaths, we divide P into a matrix of buckets with as many rows and columns as the number of non-variable terminals of the source and target tree patterns, trading memory for more effective search (Koehn, 2015).",5.3.3 Other Considerations,[0],[0]
"This operation is implicit in lines 24, 30 and 35.",5.3.3 Other Considerations,[0],[0]
Data,6.1 Experiment Settings,[0],[0]
The training data is a corpus of questions annotated with their logical forms that can be executed on Freebase to obtain a precise answer.,6.1 Experiment Settings,[0],[0]
"For an unseen set of questions, the task is to obtain automatically their logical forms and retrieve the correct answer.",6.1 Experiment Settings,[0],[0]
Our objective is to evaluate the generalization capabilities of a transducer induced using our rule extraction on an unseen open-domain test set.,6.1 Experiment Settings,[0],[0]
"We parsed questions from FREE917 into source constituent trees using the Stanford caseless model (Klein and Manning, 2003).",6.1 Experiment Settings,[0],[0]
Target constituent (meaning) representations were obtained by a simple heuristic conversion from the λ−DCS expressions released by Berant et al. (2013).,6.1 Experiment Settings,[0],[0]
We evaluate on the same training and testing split as in Berant et al. (2013).,6.1 Experiment Settings,[0],[0]
"Tree pairs (2.9%) for which the gold executable meaning representation did not retrieve valid results were filtered out.
",6.1 Experiment Settings,[0],[0]
Baselines We compared to two baselines.,6.1 Experiment Settings,[0],[0]
"The first one is SEMPRE (Berant et al., 2013), a stateof-the-art semantic parser that uses a target language grammar to over-generate trees, and a log-linear model to estimate the parameters that guide the decoder towards trees that generate correct answers.",6.1 Experiment Settings,[0],[0]
"For FREE917, SEMPRE uses a manually-created entity lexicon released by (Cai and Yates, 2013), but an automatically generated predicate lexicon.",6.1 Experiment Settings,[0],[0]
"In-
stead, our system and the second baseline use manually created entity and predicate lexicons, where the latter was created by selecting all words from every question that relate to the target predicate.",6.1 Experiment Settings,[0],[0]
"For example, for the question “what olympics has egypt participated in”, we created an entry that maps the discontinuous phrase “olympics participated in” to the predicate OlympicsParticipatedIn.
",6.1 Experiment Settings,[0],[0]
"The second baseline is a tree-to-tree transducer whose rules are extracted using a straightforward adaptation of the GHKM algorithm (Galley et al., 2004) for pairs of trees.",6.1 Experiment Settings,[0],[0]
"Word-to-concept alignments are extracted using three different strategies: i) ghkm-g uses the IBM models (Brown et al., 1993) as implemented in GIZA++",6.1 Experiment Settings,[0],[0]
"(Och and Ney, 2003), ii) ghkm-m maps KB concepts (target leaves) to as many source words as present in the entity/predicate lexicons, and iii) ghkm-c maps KB concepts as in ii) but only retaining the longest contiguous sequence of source words (or right-most sequence if there is a tie).",6.1 Experiment Settings,[0],[0]
Bridging predicates are assumed when a KB concept does not align (according to the lexicon) to any source word.,6.1 Experiment Settings,[0],[0]
"Finally, rule state names are set according to the mechanism described in Section 5.
",6.1 Experiment Settings,[0],[0]
"Our ent, pred and bridge cost/back-off functions assign a low cost (or high score) to source and target tree patterns with no variables whose leaves appear in either the entity or the predicate lexicons.",6.1 Experiment Settings,[0],[0]
Scaling factors λi (see Eq. 1) were subjectively tuned on 20 training examples.,6.1 Experiment Settings,[0],[0]
"When used as back-off functions, they generate as many rhs as entities or predicates can be retrieved from the lexicons by at least one of the words in the source tree pattern.",6.1 Experiment Settings,[0],[0]
Bridging predicates are dispreferred by adding an extra constant cost.,6.1 Experiment Settings,[0],[0]
"At back-off, this score function generates a variable predicate, acting as a wildcard in Sparql.
",6.1 Experiment Settings,[0],[0]
"Our system t2t For the rule extraction, we use a beam size of 10, and we output 100 derivations for every tree pair.",6.1 Experiment Settings,[0],[0]
"We do not impose any limit in the depth of lhs or rhs, or in the number of variables.",6.1 Experiment Settings,[0],[0]
"To increase the coverage of our rules, we produce deleting tree transducers by replacing fully lexicalized branches that are directly attached to the root of a lhs with a deleting variable.
",6.1 Experiment Settings,[0],[0]
"For the parameter estimation, we used 3 iterations of the latent variable averaged structured perceptron, where the number of iterations was selected on 20% of held-out training data.",6.1 Experiment Settings,[0],[0]
"To assess the equality be-
tween the gold and the decoded tree, we compare their denotations.",6.1 Experiment Settings,[0],[0]
"The features for the discriminative training were the lhs and rhs roots, the number of variables, deleting variables and leaves, the presence of entities or predicates in the rhs, the rule state and children states, and several measures of character overlaps between the leaves of the source and information associated to leaves in target tree patterns.
",6.1 Experiment Settings,[0],[0]
"For decoding, we used standard techniques (Graehl and Knight, 2004) to constrain and prune weighted regular tree grammars given a tree transducer and a source tree, and used the cube-growing algorithm to generate 10, 000 derivations, converted them to Sparql queries, and retained those that were valid (either syntactically correct or that retrieved any result).",6.1 Experiment Settings,[0],[0]
"We compute the accuracy of the system as the percentage of questions for which the 1-best output tree retrieves the correct answer, and the coverage as the percentage for which the correct answer is within the 10, 000 best derivations.",6.1 Experiment Settings,[0],[0]
"The average rule extraction time per tree pair when using beam size 1 was 0.46 seconds (median 0.35, maximum 2.94 seconds).",6.1 Experiment Settings,[0],[0]
"When using beam size 10, the average was 4.7 seconds (median 2.02, maximum 104.4 seconds), which gives us a glimpse of how the beam size influences the computational complexity for the typical tree size of FREE917 questions.",6.1 Experiment Settings,[0],[0]
Results are in Table 1.,6.2 Results,[0],[0]
"Note that although we compare our results to those obtained with SEMPRE (Berant et al., 2013), the systems cannot really be compared since Berant et al. (2013) did not have access to a manually created lexicon of predicates.",6.2 Results,[0],[0]
"When comparing the average number of entity and predicate rules that the back-off functions generate, we see that the number of predicate rules is much larger, implying a higher ambiguity.",6.2 Results,[0],[0]
"Despite this, our base system still produces promising results in terms of accuracy and coverage.
",6.2 Results,[0],[0]
We also carried out several ablation experiments to investigate what are the characteristics of the system that contribute the most to the accuracy:,6.2 Results,[0],[0]
"In no nbest, we only extract one sequence of rules that transform a source into a target tree.",6.2 Results,[0],[0]
"In no del, we do not introduce deleting variables.",6.2 Results,[0],[0]
"In beam 1, we use beam size 1 in rule extraction.",6.2 Results,[0],[0]
"In no size, no tree size regularizer cost function is used.",6.2 Results,[0],[0]
"And in
no back, no rule back-offs are used.",6.2 Results,[0],[0]
"As we see, removing the back-off rule capabilities is critical in this setting and makes the QA task unfeasible.",6.2 Results,[0],[0]
"We also studied the impact of the size of the training data in the generalization of our system, by training the system in {100, 200, . . .",6.2 Results,[0],[0]
", 600} examples.",6.2 Results,[0],[0]
"We found that the accuracy saturates at only 400 training instances, which might be advantageous in tasks where training resources are scarce.",6.2 Results,[0],[0]
"Finally, in order to estimate the upper bound in the coverage and accuracy of our approach on FREE917, we also run our pipeline t2t-e with a refined version of Cai and Yates (2013)’s entity lexicon, where 65 missing entities are added (7.8% of the total).",6.2 Results,[0],[0]
"We can observe a significant increase in the accuracy and coverage of the system, suggesting that the bottleneck may lie in the entity/predicate linking procedures.",6.2 Results,[0],[0]
One step further in the generalization of the rule extraction is to remove the necessity of explicitly providing cost functions such as word-to-word hardalignments or costs between tree fragments.,7 Future Work,[0],[0]
This would allow us to remove the bias introduced by engineered cost functions and to obtain rules that are globally optimal.,7 Future Work,[0],[0]
"In this setup, the parameters of the cost functions are to be learned with the objective to maximize the likelihood on the training data or
a downstream application performance.",7 Future Work,[0],[0]
"However, since rules (or tree mappings) would become hidden variables, this generalized rule extraction may require faster methods to enumerate plausible rules.",7 Future Work,[0],[0]
"Another extension would be to make the rule extraction more robust against parsing errors, using pairs of forests instead of pairs of trees, similarly as in Liu et al. (2009).
",7 Future Work,[0],[0]
"Regarding the QA application, there are two natural extensions that we want to address, namely to develop general and automatic entity and predicate linking mechanisms for large knowledge bases, and to test our approach in datasets that require higher levels of compositionality such as the QALD challenges (Unger et al., 2015) or those datasets produced by Wang et al. (2015).",7 Future Work,[0],[0]
"We proposed to induce tree to tree transducers using a rule extraction algorithm that uses cost functions over pairs of tree fragments (instead of wordalignments), which increases the applicability of these models.",8 Conclusion,[0],[0]
"Some cost functions may act as rule back-offs, generating new rhs given unseen lhs, thus producing transducer rules “on-the-fly”.",8 Conclusion,[0],[0]
The scores of these rules are obtained on demand using a discriminative training procedure that estimates weights for rule features.,8 Conclusion,[0],[0]
"This strategy was useful to compensate the lack of rule coverage when inducing tree transducers from small tree corpora.
",8 Conclusion,[0],[0]
"As a proof-of-concept, we tested the tree transducer induced with our algorithm on a QA task over a large KB, a domain in which tree transducers have not been effective before.",8 Conclusion,[0],[0]
"In this task, lexicon mappings were naturally introduced as cost functions and rule back-offs, without loss of generality.",8 Conclusion,[0],[0]
"Despite using a manually created lexicon of predicates, we showed a high accuracy and coverage of nonfinal rules, which are promising results.",8 Conclusion,[0],[0]
"This paper is based on results obtained from a project commissioned by the New Energy and Industrial Technology Development Organization (NEDO), and is also supported by JSPS KAKENHI Grant Number 16K16111.",Acknowledgments,[0],[0]
"We thank Yoshimasa Tsuruoka, Yo Ehara and the anonymous reviewers for their helpful comments.",Acknowledgments,[0],[0]
Tree transducers that model expressive linguistic phenomena often require wordalignments and a heuristic rule extractor to induce their grammars.,abstractText,[0],[0]
"However, when the corpus of tree/string pairs is small compared to the size of the vocabulary or the complexity of the grammar, word-alignments are unreliable.",abstractText,[0],[0]
"We propose a general rule extraction algorithm that uses cost functions over tree fragments, and formulate the extraction as a cost minimization problem.",abstractText,[0],[0]
"As a by-product, we are able to introduce back-off states at which some cost functions generate right-hand-sides of previously unseen lefthand-sides, thus creating transducer rules “on-the-fly”.",abstractText,[0],[0]
"We test the generalization power of our induced tree transducers on a QA task over a large Knowledge Base, obtaining a reasonable syntactic accuracy and effectively overcoming the typical lack of rule coverage.",abstractText,[0],[0]
Rule Extraction for Tree-to-Tree Transducers by Cost Minimization,title,[0],[0]
"This paper considers the general learning problem in which we have m observation vectors X1, . . .",1. Motivation and Overview,[0],[0]
", Xm ∈ Rn, with matching response values y1, . . .",1. Motivation and Overview,[0],[0]
", ym ∈ R.",1. Motivation and Overview,[0],[0]
Each response yi is a possibly noisy evaluation of an unknown function f :,1. Motivation and Overview,[0],[0]
"Rn → R at Xi, that is, yi = f(Xi) +",1. Motivation and Overview,[0],[0]
"ei, where ei ∈ R represents the noise or measurement error.",1. Motivation and Overview,[0],[0]
"The goal is to estimate f by some f̂ : Rn → R such that f̂(Xi) is a good fit for yi, that is, |f̂(Xi) − yi| tends to be small.",1. Motivation and Overview,[0],[0]
The estimate f̂ may then be used to predict the response value y corresponding to a newly encountered observation x ∈ Rn through the prediction ŷ = f̂(x).,1. Motivation and Overview,[0],[0]
A classical linear regression model is one simple example of the many possible techniques one might employ for constructing f̂ .,1. Motivation and Overview,[0],[0]
"The classical regression approach to this problem is to posit
1Management Science and Information Systems, Rutgers University, Piscataway, NJ, USA 2Department of Management, Bar-Ilan University, Ramat Gan, Israel 3Doctoral Program in Operations Research, Rutgers University, Piscataway, NJ, USA.",1. Motivation and Overview,[0],[0]
"Correspondence to: Jonathan Eckstein <jeckstei@business.rutgers.edu>.
",1. Motivation and Overview,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Motivation and Overview,[0],[0]
"Copyright 2017 by the author(s).
",1. Motivation and Overview,[0],[0]
"a particular functional form for f̂(x) (for example, an affine function of x) and then use an optimization procedure to estimate the parameters in this functional form.
",1. Motivation and Overview,[0],[0]
"Here, we are interested in cases in which a concise candidate functional form for f̂ is not readily apparent, and we wish to estimate f̂ by searching over a very highdimensional space of parameters.",1. Motivation and Overview,[0],[0]
"For example, Breiman (2001) proposed the method of random forests, which constructs f̂ by training regression trees on multiple random subsamples of the data, and then averaging the resulting predictors.",1. Motivation and Overview,[0],[0]
"Another proposal is the RuleFit algorithm (Friedman & Popescu, 2008), which enhances L1regularized regression by generating box-based rules to use as additional explanatory variables.",1. Motivation and Overview,[0],[0]
"Given a, b ∈ Rn with a ≤ b, the rule function r(a,b) :",1. Motivation and Overview,[0],[0]
"Rn → {0, 1} is given by
r(a,b)(x) = I ( ∧j∈{1,...,n}(aj ≤ xj ≤ bj) ) , (1)
that is r(a,b)(x) = 1 if a ≤ x ≤ b (componentwise) and r(a,b)(x) = 0 otherwise.",1. Motivation and Overview,[0],[0]
"RuleFit generates rules through a two-phase procedure: first, it determines a regression tree ensemble, and then decomposes these trees into rules and determines the regression model coefficients (including for the rules).
",1. Motivation and Overview,[0],[0]
"The approach of Dembczyński et al. (2008a) generates rules more directly (without having to rely on an initial ensemble of decision trees) within gradient boosting (Friedman, 2001) for non-regularized regression.",1. Motivation and Overview,[0],[0]
"In this scheme, a greedy procedure generates the rules within a gradient descent method runs that for a predetermined number of iterations.",1. Motivation and Overview,[0],[0]
Aho et al. (2012) extended the RuleFit method to solve more general multi-target regression problems.,1. Motivation and Overview,[0],[0]
"For the special case of single-target regression, however, their experiments suggest that random forests and RuleFit outperform several other methods, including their own extended implementation and the algorithm of Dembczyński et al. (2008a).",1. Motivation and Overview,[0],[0]
"Compared with random forests and other popular learning approaches such as kernel-based methods and neural networks, rule-based approaches have the advantage of generally being considered more accessible and easier to interpret by domain experts.",1. Motivation and Overview,[0],[0]
"Rule-based methods also have a considerable history in classification settings, as in for example Weiss & Indurkhya (1993), Cohen & Singer
(1999), and Dembczyński et al. (2008b).
",1. Motivation and Overview,[0],[0]
"Here, we propose an iterative optimization-based regression procedure called REPR (Rule-Enhanced Penalized Regression).",1. Motivation and Overview,[0],[0]
"Its output models resemble those of RuleFit, but our methodology draws more heavily on exact optimization techniques from the field of mathematical programming.",1. Motivation and Overview,[0],[0]
"While it is quite computationally intensive, its prediction performance appears promising.",1. Motivation and Overview,[0],[0]
"As in RuleFit, we start with a linear regression model (in this case, with L1-penalized coefficients to promote sparsity), and enhance it by synthesizing rules of the form (1).",1. Motivation and Overview,[0],[0]
We incrementally adjoin such rules to our (penalized) linear regression model as if they were new observation variables.,1. Motivation and Overview,[0],[0]
"Unlike RuleFit, we control the generation of new rules using the classical mathematical programming technique of column generation.",1. Motivation and Overview,[0],[0]
"Our employment of column generation roughly resembles its use in the LPBoost ensemble classification method of Demiriz et al. (2002).
",1. Motivation and Overview,[0],[0]
Column generation involves cyclical alternation between optimization of a restricted master problem (in our case a linear or convex quadratic program) and a pricing problem that finds the most promising new variables to adjoin to the formulation.,1. Motivation and Overview,[0],[0]
"In our case, the pricing problem is equivalent to an NP-hard combinatorial problem we call Rectangular Maximum Agreement (RMA), which generalizes the Maximum Mononial Agreement (MMA) problem as formulated and solved by Eckstein & Goldberg (2012).",1. Motivation and Overview,[0],[0]
"We solve the RMA problem by a similar branch-and-bound method procedure, implemented using parallel computing techniques.
",1. Motivation and Overview,[0],[0]
"To make our notation below more concise, we let X denote the matrix whose rows are X>1 , . . .",1. Motivation and Overview,[0],[0]
", X > m, and also let y = (y1, . . .",1. Motivation and Overview,[0],[0]
", ym) ∈ Rm.",1. Motivation and Overview,[0],[0]
"We may then express a problem instance by the pair (X, y).",1. Motivation and Overview,[0],[0]
"We also let xij denote the (i, j)th element of this matrix, that is, the value of variable j in observation i.",1. Motivation and Overview,[0],[0]
"Let K be a set of pairs (a, b) ∈",2. A Penalized Regression Model with Rules,[0],[0]
"Rn × Rn with a ≤ b, constituting a catalog of all the possible rules of the form (1) that we wish to be available to our regression model.",2. A Penalized Regression Model with Rules,[0],[0]
"The set K will typically be extremely large: restricting each aj and bj to values that appear as xij for some i, which is sufficient to describe all possible distinct behaviors of rules of the form (1) on the dataset X , there are still∏n j=1 `j(`j + 1)/2 ≥ 3n possible choices for (a, b), where
`j = | ⋃m i=1{xij}| is the number of distinct values for xij .
",2. A Penalized Regression Model with Rules,[0],[0]
"The predictors f̂ that our method constructs are of the form
f̂(x) =",2. A Penalized Regression Model with Rules,[0],[0]
"β0 + n∑ j=1 βjxj + ∑ k∈K γkrk(x) (2)
for some β0, β1, . . .",2. A Penalized Regression Model with Rules,[0],[0]
", βn, (γk)k∈K ∈ R. Finding an f̂ of this form is a matter of linear regression, but with the regression coefficients in a space with the potentially very high dimension of 1 +n+ |K|.",2. A Penalized Regression Model with Rules,[0],[0]
"As is now customary in regression models in which the number of explanatory variables potentially outnumbers the number of observations, we employ a LASSO-class model in which all explanatory variables except the constant term have L1 penalties.",2. A Penalized Regression Model with Rules,[0],[0]
Letting β =,2. A Penalized Regression Model with Rules,[0],[0]
"(β1, . . .",2. A Penalized Regression Model with Rules,[0],[0]
", βn) ∈ Rn and γ ∈ R|K|, let fβ0,β,γ( · ) denote the predictor function in (2).",2. A Penalized Regression Model with Rules,[0],[0]
"We then propose to estimate β0, β, γ by solving
min β0,β,γ { m∑ i=1 |fβ0,β,γ(Xi)− yi| p + C ‖β‖1 + E ‖γ‖1 } ,
(3) where p ∈ {1, 2} and C,E ≥ 0 are scalar parameters.",2. A Penalized Regression Model with Rules,[0],[0]
"For p = 2 and C = E > 0, this model is essentially the classic LASSO as originally proposed by Tibshirani (1996).
",2. A Penalized Regression Model with Rules,[0],[0]
"To put (3) into a more convenient form for our purposes, we split the regression coefficient vectors into positive and negative parts, so β = β+",2. A Penalized Regression Model with Rules,[0],[0]
"− β− and γ = γ+ − γ−, with β+, β− ∈ Rn+ and γ+, γ− ∈ R |K| + .",2. A Penalized Regression Model with Rules,[0],[0]
"Introducing one more vector of variables ∈ Rm, the model shown as (4) in Figure 1 is equivalent to (3).",2. A Penalized Regression Model with Rules,[0],[0]
"The model is constructed so that i = |fβ0,β,γ(Xi)− yi| for i = 1, . . .",2. A Penalized Regression Model with Rules,[0],[0]
",m. If p = 1, the model is a linear program, and if p = 2 it is a convex, linearly constrained quadratic program.",2. A Penalized Regression Model with Rules,[0],[0]
"In either case, there are 2m constraints (other than nonnegativity), but the number of variables is 1 +m+ 2n+ 2 |K|.
",2. A Penalized Regression Model with Rules,[0],[0]
"Because of this potentially unwieldy number of variables, we propose to solve (4) by using the classical technique of column generation, which dates back to Ford & Fulkerson (1958) and Gilmore & Gomory (1961); see for example Section 7.3 of Griva et al. (2009) for a recent textbook treatment.",2. A Penalized Regression Model with Rules,[0],[0]
"In brief, column generation cycles between solving two optimization problems, the restricted master problem and the pricing problem.",2. A Penalized Regression Model with Rules,[0],[0]
"In our case, the restricted master problem is the same as (4), but with K replaced by some (presumably far smaller) K ′",2. A Penalized Regression Model with Rules,[0],[0]
⊆ K.,2. A Penalized Regression Model with Rules,[0],[0]
We initially choose K ′ = ∅.,2. A Penalized Regression Model with Rules,[0],[0]
Solving the restricted master problem yields optimal Lagrange multipliers ν ∈ Rm+ and µ ∈ Rm+ (for the constraints other than simple nonnegativity).,2. A Penalized Regression Model with Rules,[0],[0]
"For each rule k ∈ K, these Lagrange multipliers yield respective reduced costs rc[γ+k",2. A Penalized Regression Model with Rules,[0],[0]
"], rc[γ",2. A Penalized Regression Model with Rules,[0],[0]
"− k ] for the variables γ + k ,",2. A Penalized Regression Model with Rules,[0],[0]
"γ − k that are in the master problem, but not the restricted master.",2. A Penalized Regression Model with Rules,[0],[0]
"One then solves the pricing problem, whose job is to identify the smallest of these reduced costs.",2. A Penalized Regression Model with Rules,[0],[0]
The reduced cost rc[v] of a variable v indicates the rate of change of the objective function as one increases v away from 0.,2. A Penalized Regression Model with Rules,[0],[0]
"If the smallest reduced
m∑ n∑ ∑
cost is nonnegative, then clearly all the reduced costs are nonnegative, which means that the current restricted master problem yields an optimal solution to the master problem by setting γ+k =",2. A Penalized Regression Model with Rules,[0],[0]
"γ − k = 0 for all k ∈ K\K ′, and the process terminates.",2. A Penalized Regression Model with Rules,[0],[0]
"If the smallest reduced cost is negative, we adjoin elements to K ′, including at least one corresponding to a variable γ+k or γ − k with a negative reduced cost, and we repeat the process, re-solving the expanded restricted master problem.
",2. A Penalized Regression Model with Rules,[0],[0]
"In our case, the reduced costs take the form
rc[γ+k ]",2. A Penalized Regression Model with Rules,[0],[0]
= E − m∑ i=1,2. A Penalized Regression Model with Rules,[0],[0]
rk(xi)νi,2. A Penalized Regression Model with Rules,[0],[0]
+,2. A Penalized Regression Model with Rules,[0],[0]
m∑ i=1,2. A Penalized Regression Model with Rules,[0],[0]
"rk(xi)µi
rc[γ−k ]",2. A Penalized Regression Model with Rules,[0],[0]
= E + m∑ i=1,2. A Penalized Regression Model with Rules,[0],[0]
rk(xi)νi,2. A Penalized Regression Model with Rules,[0],[0]
"− m∑ i=1 rk(xi)µi
and hence we have for each k ∈ K that
min { rc[γ+k",2. A Penalized Regression Model with Rules,[0],[0]
"], rc[γ",2. A Penalized Regression Model with Rules,[0],[0]
− k ] },2. A Penalized Regression Model with Rules,[0],[0]
= E,2. A Penalized Regression Model with Rules,[0],[0]
− ∣∣∣∣∣,2. A Penalized Regression Model with Rules,[0],[0]
m∑ i=1 rk(xi)(νi,2. A Penalized Regression Model with Rules,[0],[0]
− µi) ∣∣∣∣∣ .,2. A Penalized Regression Model with Rules,[0],[0]
"(5) Therefore, the pricing problem may be solved by maximizing the second term on the right-hand side of (5), that is, finding
z∗ = max k∈K ∣∣∣∣∣",2. A Penalized Regression Model with Rules,[0],[0]
m∑ i=1 rk(xi)(νi,2. A Penalized Regression Model with Rules,[0],[0]
− µi) ∣∣∣∣∣,2. A Penalized Regression Model with Rules,[0],[0]
", (6) and the stopping condition for the column generation procedure is z∗ ≤",2. A Penalized Regression Model with Rules,[0],[0]
E.,2. A Penalized Regression Model with Rules,[0],[0]
"This problem turns out to be equivalent to the RMA problem, whose formulation and solution we now describe.",2. A Penalized Regression Model with Rules,[0],[0]
"Suppose we have m observations and n explanatory variables, expressed using a matrix X ∈ Rm×n as above.",3.1. Formulation and Input Data,[0],[0]
"Each observation i ∈ {1, . . .",3.1. Formulation and Input Data,[0],[0]
",m} is assigned a nonegative weight wi ∈",3.1. Formulation and Input Data,[0],[0]
R+.,3.1. Formulation and Input Data,[0],[0]
"For any set S ⊆ {1, . . .",3.1. Formulation and Input Data,[0],[0]
",m},
let w(S) = ∑ i∈S wi.",3.1. Formulation and Input Data,[0],[0]
"We also assume we are given a partition of the observations into two subsets, a “positive” subset Ω+ ⊂ {1, . . .",3.1. Formulation and Input Data,[0],[0]
",m} and a “negative” subset Ω− = {1, . . .",3.1. Formulation and Input Data,[0],[0]
",m}\Ω+.
",3.1. Formulation and Input Data,[0],[0]
"Given two vectors a, b ∈ Rn, let B(a, b) denote the “box” {x ∈ Zn | a ≤ x ≤ b}.",3.1. Formulation and Input Data,[0],[0]
"Given the input data X , the coverage CvrX(a, b) of B(a, b) consists of the indices of the observations from X falling within B(a, b), that is,
CvrX(a, b)",3.1. Formulation and Input Data,[0],[0]
=,3.1. Formulation and Input Data,[0],[0]
"{i ∈ {1, . . .",3.1. Formulation and Input Data,[0],[0]
",m} | a ≤ Xi ≤ b} .
",3.1. Formulation and Input Data,[0],[0]
"The rectangular maximum agreement (RMA) problem is
max ∣∣w(Ω+ ∩",3.1. Formulation and Input Data,[0],[0]
"CvrX(a, b))− w(Ω− ∩",3.1. Formulation and Input Data,[0],[0]
"CvrX(a, b))∣∣ s.t.",3.1. Formulation and Input Data,[0],[0]
"a, b ∈ Rn, (7) with decision variables a, b ∈ Rn.",3.1. Formulation and Input Data,[0],[0]
"Essentially implicit in this formulation is the constraint that a ≤ b, since if a 6≤ b then CvrX(a, b) = ∅ and the objective value is 0.",3.1. Formulation and Input Data,[0],[0]
"The previously mentioned MMA problem is the special case of RMA in which all the observations are binary, X ∈ {0, 1}m×n.",3.1. Formulation and Input Data,[0],[0]
"Since the MMA problem is NPhard (Eckstein & Goldberg, 2012), so is RMA.
",3.1. Formulation and Input Data,[0],[0]
"If we take K to be the set of all possible boxes on Rn, the pricing problem (6) may be reduced to RMA by setting
(∀ i = 1, . . .",3.1. Formulation and Input Data,[0],[0]
",m) :",3.1. Formulation and Input Data,[0],[0]
"wi = |νi − µi| (8)
Ω+",3.1. Formulation and Input Data,[0],[0]
"= {i ∈ {1, . . .",3.1. Formulation and Input Data,[0],[0]
",m} | νi ≥ µi } , (9)
and thus Ω− = {i ∈",3.1. Formulation and Input Data,[0],[0]
"{1, . . .",3.1. Formulation and Input Data,[0],[0]
",m} | νi",3.1. Formulation and Input Data,[0],[0]
< µi }.,3.1. Formulation and Input Data,[0],[0]
Any RMA problem instance may be converted to an equivalent instance in which all the observation data are integer.,3.2. Preprocessing and Restriction to N,[0],[0]
"Essentially, for each coordinate j = 1, . . .",3.2. Preprocessing and Restriction to N,[0],[0]
", n, one may simply record the distinct values of xij and replace each xij with its ordinal position among these values.",3.2. Preprocessing and Restriction to N,[0],[0]
"Algorithm 1, with its parameter δ set to 0, performs exactly this procedure, outputing a equivalent data matrix X ∈ Nm×n and a vector ` ∈",3.2. Preprocessing and Restriction to N,[0],[0]
"Nn whose jth element is `j = | ⋃m i=1{xij}|
Algorithm 1",3.2. Preprocessing and Restriction to N,[0],[0]
Preprocessing discretization algorithm 1,3.2. Preprocessing and Restriction to N,[0],[0]
": Input: X ∈ Rm×n, δ ≥ 0 2: Output: X ∈",3.2. Preprocessing and Restriction to N,[0],[0]
"Nm×n, ` ∈",3.2. Preprocessing and Restriction to N,[0],[0]
"Nn 3: ProcessData 4: for j = 1 to n do 5: `j ← 0 6: Sort x1j , . . .",3.2. Preprocessing and Restriction to N,[0],[0]
", xmj and set (k1, . . .",3.2. Preprocessing and Restriction to N,[0],[0]
", km) such that
xk1j ≤ xk2j ≤ · · · ≤ xkmj 7: x̄k1,j ← 0 8: for i = 1 to m− 1 do 9: if xki+1j",3.2. Preprocessing and Restriction to N,[0],[0]
"− xkij > δ · (xkmj − xk1j) then
10: `j ← `j + 1 11: end if 12: x̄ki+1j",3.2. Preprocessing and Restriction to N,[0],[0]
"← `j 13: end for 14: `j ← `j + 1 15: end for 16: return (X, `)
as defined in the previous section.",3.2. Preprocessing and Restriction to N,[0],[0]
"Algorithm 1’s output values x̄ij for attribute j vary between 0 and `j − 1.
",3.2. Preprocessing and Restriction to N,[0],[0]
The number of distinct values `j of each explanatory variable j directly influences the difficulty of RMA instances.,3.2. Preprocessing and Restriction to N,[0],[0]
"To obtain easier instances, Algorithm 1 can combine its “integerization” process with some binning of nearby values.",3.2. Preprocessing and Restriction to N,[0],[0]
"Essentially, if the parameter δ is positive, the algorithm bins together consecutive values xij that are within relative tolerance δ, resulting in a smaller number of distinct values `j for each explanatory variable j.
Some datasets contain both categorical and numerical data.",3.2. Preprocessing and Restriction to N,[0],[0]
"In addition to Algorithm 1, we also convert each k-way categorical attribute into k − 1 binary attributes.
",3.2. Preprocessing and Restriction to N,[0],[0]
"Within the context of our REPR regression method, we set the RMA weight vector and data partition as in (8)-(9), integerize the data X using Algorithm 1 with some (small) parameter value δ, solve the RMA problem, and then translate the resulting boxes back to the original, pre-integerized coordinate system.",3.2. Preprocessing and Restriction to N,[0],[0]
"We perform this translation by expanding box boundaries to lie halfway between the boundaries of the clusters of points grouped by Algorithm 1, except when the lower boundary of the box has the lowest possible value or the upper boundary has the largest possible value.",3.2. Preprocessing and Restriction to N,[0],[0]
"In these cases, we expand the box boundaries to −∞ or +∞, respectively.",3.2. Preprocessing and Restriction to N,[0],[0]
"More precisely, for each observation variable j and v ∈ {0, . . .",3.2. Preprocessing and Restriction to N,[0],[0]
", `j − 1}, let xminj,v be the smallest value of xij assigned to the integer value v by Algorithm 1, and xmaxj,v be the largest.",3.2. Preprocessing and Restriction to N,[0],[0]
"If â, b̂ ∈ Nn, â ≤ b̂ describe an integerized box arising from the solution of the preprocessed RMA problem, we choose the corresponding box boundaries a, b ∈ Rn in the original coordinate system
to be given by, for j = 1, . . .",3.2. Preprocessing and Restriction to N,[0],[0]
",",3.2. Preprocessing and Restriction to N,[0],[0]
"n,
aj = { −∞, if âj = 0 1 2 (x max j,âj−1 + x min j,âj ), otherwise
bj = { +∞, if b̂j = `j − 1 1 2 (x max j,b̂j + xmin j,b̂j+1 ), otherwise.
",3.2. Preprocessing and Restriction to N,[0],[0]
"Overall, our procedure is equivalent to solving the pricing problem (6) over some set of boxes K = Kδ(X).",3.2. Preprocessing and Restriction to N,[0],[0]
"For δ = 0, the resulting set of boxesK0(X) is such that the corresponding set of rules {rk | k ∈ K0(X)} comprises every box-based rule distinguishable on the dataset X .",3.2. Preprocessing and Restriction to N,[0],[0]
"For small positive values of δ, the set of boxes Kδ(X) excludes those corresponding to rules that “cut” between very closely spaced observations.",3.2. Preprocessing and Restriction to N,[0],[0]
"In this and the following two subsections, we describe the key elements of our branch-and-bound procedure for solving the RMA problem, assuming that the data X have already been preprocessed as above.",3.3. Branch-and-Bound Subproblems,[0],[0]
"For brevity, we omit some details which will instead be covered in a forthcoming publication.",3.3. Branch-and-Bound Subproblems,[0],[0]
"For general background on branch-andbound algorithms, Morrison et al. (2016) provide a recent survey with numerous citations.
",3.3. Branch-and-Bound Subproblems,[0],[0]
"Branch-and-bound methods search a tree of subproblems, each describing some subset of the search space.",3.3. Branch-and-Bound Subproblems,[0],[0]
"In our RMA method, each subproblem P is characterized by four vectors a(P ), a(P ), b(P ), b(P",3.3. Branch-and-Bound Subproblems,[0],[0]
") ∈ Nn, and represents search space subset consisting of vector pairs (a, b) for which a(P ) ≤ a ≤ a(P ) and b(P ) ≤ b ≤ b(P ).",3.3. Branch-and-Bound Subproblems,[0],[0]
"Any valid subproblem conforms to a(P ) ≤ a(P ), b(P ) ≤ b(P ), a(P ) ≤ b(P ), and a(P ) ≤ b(P ).",3.3. Branch-and-Bound Subproblems,[0],[0]
"The root problem R of the branch-and-bound tree is R = (0, ` − 1,0, ` − 1), where where ` ∈ Nn is as output from Algorithm 1, and 0 and 1 respectively denote the vectors (0, 0, . . .",3.3. Branch-and-Bound Subproblems,[0],[0]
", 0) ∈",3.3. Branch-and-Bound Subproblems,[0],[0]
"Nn and (1, 1, . . .",3.3. Branch-and-Bound Subproblems,[0],[0]
", 1) ∈",3.3. Branch-and-Bound Subproblems,[0],[0]
Nn.,3.3. Branch-and-Bound Subproblems,[0],[0]
"In branch-and-bound methods, the bounding function provides an upper bound (when maximizing) on the best possible objective value in the region of the search space corresponding to a subproblem.",3.4. Inseparability and the Bounding Function,[0],[0]
Our bounding function is based on an extension of the notion of inseparability developed by Eckstein & Goldberg (2012).,3.4. Inseparability and the Bounding Function,[0],[0]
"Consider any subproblem P = (a, a, b, b) and two observations i and i′.",3.4. Inseparability and the Bounding Function,[0],[0]
"If xij = xi′j or aj ≤ xij , xi′j ≤ bj for each j = 1, . . .",3.4. Inseparability and the Bounding Function,[0],[0]
", n, then xi, xi′ ∈",3.4. Inseparability and the Bounding Function,[0],[0]
"Nn are inseparable with respect to a, b ∈ Nn, in the sense that any box B(a, b) with a ≤ a and b ≥ b must either cover both of xi, xi′ or neither of them.
",3.4. Inseparability and the Bounding Function,[0],[0]
"Inseparability with respect to a, b is an equivalence relation,
and we denote the equivalence classes it induces among the observation indices 1, . . .",3.4. Inseparability and the Bounding Function,[0],[0]
",m by E(a, b).",3.4. Inseparability and the Bounding Function,[0],[0]
"That is, observation indices i and i′ are in the same equivalence class of E(a, b) if xi and xi′ are inseparable with respect to a, b.
Our bounding function β(a, a, b, b) for each subproblem P = (a, a, b, b) is shown in (10) in Figure 2.",3.4. Inseparability and the Bounding Function,[0],[0]
"The reasoning behind this bound is that each possible box in the set specified by (a, a, b, b) must either cover or not cover the entirety of each C ∈ E(a, b).",3.4. Inseparability and the Bounding Function,[0],[0]
"The first argument to the “max” operation reflects the situation that every equivalence class C with a positive net weight is covered, and no classes with negative net weight are covered; this is the best possible situation if the box ends up covering a higher weight of positive observations than of negative.",3.4. Inseparability and the Bounding Function,[0],[0]
"The second “max” argument reflects the opposite situation, the best possible case in which the box covers a greater weight of negative observations than of positive ones.",3.4. Inseparability and the Bounding Function,[0],[0]
The branching scheme of a branch-and-bound algorithm divides subproblems into smaller ones in order to improve their bounds.,3.5. Branching,[0],[0]
"In our case, branching a subproblem P = (a, a, b, b) involves choosing an explanatory variable j ∈ {1, . . .",3.5. Branching,[0],[0]
", n} and a cutpoint v ∈ {aj , . . .",3.5. Branching,[0],[0]
", bj − 1} ∈ Nn.
",3.5. Branching,[0],[0]
"There are three possible cases, the first of which is when bj < aj and v ∈ {bj , . . .",3.5. Branching,[0],[0]
", aj − 1}.",3.5. Branching,[0],[0]
"In this case, our scheme creates three children based on the disjunction that either bj ≤ v",3.5. Branching,[0],[0]
"− 1 (the box lies below v), aj ≤ v ≤ bj (the box straddles v), or aj ≥ v+ 1 (the box lies above v).",3.5. Branching,[0],[0]
"The next case is that v ∈ { aj , . . .",3.5. Branching,[0],[0]
",min{aj , bj} − 1 } , in which case the box cannot lie below v",3.5. Branching,[0],[0]
and we split P into two children based on the disjunction that either aj ≤ v,3.5. Branching,[0],[0]
(the box straddles v) or aj ≥ v + 1 (the box is above v).,3.5. Branching,[0],[0]
"The third case occurs when v ∈ { max{aj , bj}, . . .",3.5. Branching,[0],[0]
", bj−1 } , in which case we split P into two children based on the disjunction that either bj ≤ v",3.5. Branching,[0],[0]
(the box is does not extend above v) or bj ≥ v+ 1 (the box extends above v).,3.5. Branching,[0],[0]
"If no v falling under one of these three cases exists for any dimension j, then the subproblem represents a single possible box, that is, a = a and b = b.",3.5. Branching,[0],[0]
"Such a subproblem is a terminal node of the branch-and-bound tree, and in this case we simply compute the RMA objective value for a = a = a and b = b = b as the subproblem bound.
",3.5. Branching,[0],[0]
"When more than one possible variable-cutpoint pair (j, v) exists, as is typically the case, our algorithm must select one.",3.5. Branching,[0],[0]
We use two related procedures for branching selection: strong branching and cutpoint caching.,3.5. Branching,[0],[0]
"In strong branching, we simply experiment with all applicable variable-cutpoint pairs (j, v), and select one that the maximizes the minimum bound of the resulting two or three children.",3.5. Branching,[0],[0]
"This is a standard technique in branch-and-bound algorithms, and involves evaluating the bounds of all the potential children of the current search node.",3.5. Branching,[0],[0]
"To make this process as efficient as possible, we have developed specialized data structures for manipulating equivalence classes, and we analyze the branching possibilities in a particular order.",3.5. Branching,[0],[0]
"In cutpoint caching, some subproblems use strong branching, while others select from a list of cutpoints that were chosen by strong branching for previously processed search nodes.",3.5. Branching,[0],[0]
The details of these procedures will be covered in a forthcoming companion publication.,3.5. Branching,[0],[0]
"The pseudocode in Algorithm 2 describes our full REPR column generation procedure for solving (4), using the RMA preprocessing and branch-and-bound methods described above to solve the pricing problem.",4. Full Algorithm and Implementation,[0],[0]
"Several points bear mentioning: first, the nonnegative scalar parameter θ allows us to incorporate a tolerance into the column generation stopping criterion, so that we terminate when all reduced costs exceed −θ instead of when all reduced costs are nonnegative.",4. Full Algorithm and Implementation,[0],[0]
This kind of tolerance is customary in column generation methods.,4. Full Algorithm and Implementation,[0],[0]
"The tolerance δ, on the other hand, controls the space of columns searched over.",4. Full Algorithm and Implementation,[0],[0]
"Furthermore, our implementation of the RMA branch-and-bound algorithm can identify any desired number t ≥ 1 of the best possible RMA solutions, as opposed to just one value of k attaining the maximum in (11).",4. Full Algorithm and Implementation,[0],[0]
"This t is also a parameter to our procedure, so at each iteration of Algorithm 2 we may adjoin up to t new rules to K ′. Adding multiple columns per iteration is a common technique in column generation methods.",4. Full Algorithm and Implementation,[0],[0]
"Finally, the algorithm has a parameter S specifying a limit on the number of column generation iterations, meaning that at the output model will contain at most St rules.
",4. Full Algorithm and Implementation,[0],[0]
"We implemented the algorithm in C++, using the GuRoBi
Algorithm 2 REPR:",4. Full Algorithm and Implementation,[0],[0]
Rule-enhanced penalized regression 1: Input: data X ∈,4. Full Algorithm and Implementation,[0],[0]
"Rm×n, y ∈ Rm, penalty parameters C,E ≥ 0, column generation tolerance θ ≥ 0, integer t ≥ 1, aggregation tolerance δ",4. Full Algorithm and Implementation,[0],[0]
"≥ 0, iteration limit S
2: Output: β0 ∈ R,",4. Full Algorithm and Implementation,[0],[0]
"β ∈ Rn,K ′ ⊂ Kδ(X), γ ∈ R|K ′| 3: REPR 4: K ′",4. Full Algorithm and Implementation,[0],[0]
←,4. Full Algorithm and Implementation,[0],[0]
"∅ 5: for s = 1, . . .",4. Full Algorithm and Implementation,[0],[0]
", S do 6: Solve the restricted master problem to obtain opti-
mal primal variables (β0, β+, β−, γ+, γ−) and dual variables (ν, µ)
7: Use the RMA branch-and-bound algorithm, with preprocessing as in Algorithm 1, to identify a t-best solution k1, . . .",4. Full Algorithm and Implementation,[0],[0]
", kt to
max k∈Kδ(X) ∣∣∣∣∣",4. Full Algorithm and Implementation,[0],[0]
m∑ i=1 rk(xi)(νi,4. Full Algorithm and Implementation,[0],[0]
− µi) ∣∣∣∣∣,4. Full Algorithm and Implementation,[0],[0]
", (11) with k1, . . .",4. Full Algorithm and Implementation,[0],[0]
", kt having respective objective values z1 ≥ z2 ≥ · · ·",4. Full Algorithm and Implementation,[0],[0]
"≥ zt
8: if z1 ≤ E + θ break 9: for each l ∈ {1, . . .",4. Full Algorithm and Implementation,[0],[0]
", t} with zl > E",4. Full Algorithm and Implementation,[0],[0]
"+ θ do
10: K ′",4. Full Algorithm and Implementation,[0],[0]
← K ′,4. Full Algorithm and Implementation,[0],[0]
"∪ {kl} 11: end for 12: end for 13: return (β0, β := β+",4. Full Algorithm and Implementation,[0],[0]
"− β−,K ′, γ := γ+ − γ−)
commercial optimizer (Gurobi Optimization, 2016) to solve the restricted master problems.",4. Full Algorithm and Implementation,[0],[0]
"We implemented the RMA algorithm using using the PEBBL C++ class library (Eckstein et al., 2015), an open-source C++ framework for parallel branch and bound.",4. Full Algorithm and Implementation,[0],[0]
"PEBBL employs MPIbased parallelism (Gropp et al., 1994).",4. Full Algorithm and Implementation,[0],[0]
"Since solving the RMA pricing problem is by far the most time-consuming part of Algorithm 2, we used true parallel computing only in that portion of the algorithm.",4. Full Algorithm and Implementation,[0],[0]
"The remainder of the algorithm, including solving the restricted master problems, was executed in serial and redundantly on all processors.",4. Full Algorithm and Implementation,[0],[0]
"For preliminary testing of REPR, we selected 8 datasets from the UCI repository (Lichman, 2013), choosing small datasets with continuous response variables.",5. Preliminary Testing of REPR,[0],[0]
"The first four columns of Table 1 summarize the number of observations m, the number of attributes n, and the maximum number of distinguishable box-based rules |K0(X)| for these data sets.
",5. Preliminary Testing of REPR,[0],[0]
"In our initial testing, we focused on the p = 2 case in which fitting errors are penalized quadratically, and set t = 1, that is, we added one model rule per REPR iteration.",5. Preliminary Testing of REPR,[0],[0]
"We set the iteration limit S to 100 and effectively set the termination
tolerance θ so that REPR terminated when z1 ≤ max { 0, E · ( |E[y]| − 0.1σ[y] )} + 0.001,
where E[y] denotes the sample mean of the response variable and σ[y] its sample standard deviation.",5. Preliminary Testing of REPR,[0],[0]
"We found this rule of thumb to work well in pracice, but it likely merits further study.",5. Preliminary Testing of REPR,[0],[0]
We also chose C = 1 and E = 1.,5. Preliminary Testing of REPR,[0],[0]
"We used δ = 0 for SERVO, YACHT, and MPG, and δ = 0.005 for the remaining datasets.
",5. Preliminary Testing of REPR,[0],[0]
"With the fixed parameters given above, we tested REPR and some competing regression procedures on ten different randomly-chosen partitions of each dataset; each partition consists of 80% training data and 20% testing data.",5. Preliminary Testing of REPR,[0],[0]
"The competing procedures are RuleFit, random forests, LASSO, and classical linear regression.",5. Preliminary Testing of REPR,[0],[0]
The penalty parameter in LASSO is the same as the value of C chosen for REPR.,5. Preliminary Testing of REPR,[0],[0]
"To implement RuleFit and random forests, we used their publicly available R packages.",5. Preliminary Testing of REPR,[0],[0]
Table 2 shows the averages of the resulting mean square errors and Table 3 shows their standard deviations.,5. Preliminary Testing of REPR,[0],[0]
"REPR has the smallest average MSE for 5 of the 8 datasets and has the second smallest average MSE on the remaining 3 datasets, coming very close to random forests on MPG.",5. Preliminary Testing of REPR,[0],[0]
"For the standard deviation of the MSE, which we take as a general measure of prediction stability, REPR has the lowest values for 6 of the 8 datasets.",5. Preliminary Testing of REPR,[0],[0]
"The box plots in Figures 3 and 4 visualize these results in more detail for HEAT and MACHINE, respectively.",5. Preliminary Testing of REPR,[0],[0]
"Figure 5 displays the average MSEs in a bar-chart format, with the MSE of REPR normalized to 1.
",5. Preliminary Testing of REPR,[0],[0]
Figures 6-9 give more detailed information for specific datasets.,5. Preliminary Testing of REPR,[0],[0]
"Figure 6 and 7 respectively show how REPR’s prediction MSEs for HEAT and CONCRETE evolve with each iteration, with each data point averaged over the 10 different REPR runs; the horizontal lines indicate the average MSE level for the competing procedures.",5. Preliminary Testing of REPR,[0],[0]
"MSE generally declines as REPR adds rules, although some diminish-
ing returns are evident for CONCRETE.",5. Preliminary Testing of REPR,[0],[0]
"Interestingly, neither of these figures shows appreciable evidence of overfitting by REPR, even when large numbers of rules are incorporated into the model.",5. Preliminary Testing of REPR,[0],[0]
"Figures 8 and 9 display testing-set predictions for specific (arbitrarily chosen) partitions of the MACHINE and CONCRETE datasets, respectively, with the observations sorted by response value.",5. Preliminary Testing of REPR,[0],[0]
"REPR seems to outperform the other methods in predicting extreme response values, although it is somewhat worse than the other methods at predicting non-extreme values for MACHINE.
",5. Preliminary Testing of REPR,[0],[0]
"The last two columns of Table 1 show, for a 16-core Xeon E5-2660 workstation, REPR’s average total run time per data partition and the average number of search node per invocation of RMA.",5. Preliminary Testing of REPR,[0],[0]
The longer runs could likely be accelerated by the application of more parallel processors.,5. Preliminary Testing of REPR,[0],[0]
"The results presented here suggest that REPR has significant potential as a regression tool, at least for small datasets.",6. Conclusions and Future Research,[0],[0]
"Clearly, it should be tested on more datasets and larger datasets.
",6. Conclusions and Future Research,[0],[0]
"Here, we have tested REPR using fixed values of most of its parameters, and we expect we should be able to improve its performance by using intelligent heuristics or crossvalidation procedures to select key parameters such as C andE. Improved preprocessing may also prove helpful: judicious normalization of the input data (X, y) should assist in finding good parameter choices, and we are also working on more sophisticated discretization technique for preprocessing the RMA solver input, as well as branch selection heuristics that are more efficient for large `j .
",6. Conclusions and Future Research,[0],[0]
It would be interesting to see how well REPR performs if the pricing problems are solved less exactly.,6. Conclusions and Future Research,[0],[0]
"For example, one could use various techniques for truncating the branchand-bound search, such as setting a limit on the number of subproblems explored or loosening the conditions for pruning unpromising subtrees.",6. Conclusions and Future Research,[0],[0]
"Or one could use, perhaps selectively, some entirely heuristic procedure to identify rules to add to the restricted master problem.
",6. Conclusions and Future Research,[0],[0]
"For problems with large numbers of observations m, it is conceivable that solving the restricted master problems could become a serial bottleneck in our current implementation strategy.",6. Conclusions and Future Research,[0],[0]
"If this phenomenon is observed in practice, it could be worth investigating parallel solution strategies for the restricted master.",6. Conclusions and Future Research,[0],[0]
We describe a procedure enhancingL1-penalized regression by adding dynamically generated rules describing multidimensional “box” sets.,abstractText,[0],[0]
Our rule-adding procedure is based on the classical column generation method for highdimensional linear programming.,abstractText,[0],[0]
The pricing problem for our column generation procedure reduces to the NP-hard rectangular maximum agreement (RMA) problem of finding a box that best discriminates between two weighted datasets.,abstractText,[0],[0]
We solve this problem exactly using a parallel branch-and-bound procedure.,abstractText,[0],[0]
"The resulting rule-enhanced regression method is computation-intensive, but has promising prediction performance.",abstractText,[0],[0]
1.,abstractText,[0],[0]
"Motivation and Overview This paper considers the general learning problem in which we have m observation vectors X1, . . .",abstractText,[0],[0]
", Xm ∈ R, with matching response values y1, . . .",abstractText,[0],[0]
", ym ∈ R.",abstractText,[0],[0]
Each response yi is a possibly noisy evaluation of an unknown function f :,abstractText,[0],[0]
"R → R at Xi, that is, yi = f(Xi) +",abstractText,[0],[0]
"ei, where ei ∈ R represents the noise or measurement error.",abstractText,[0],[0]
"The goal is to estimate f by some f̂ : R → R such that f̂(Xi) is a good fit for yi, that is, |f̂(Xi) − yi| tends to be small.",abstractText,[0],[0]
The estimate f̂ may then be used to predict the response value y corresponding to a newly encountered observation x ∈ R through the prediction ŷ = f̂(x).,abstractText,[0],[0]
A classical linear regression model is one simple example of the many possible techniques one might employ for constructing f̂ .,abstractText,[0],[0]
"The classical regression approach to this problem is to posit Management Science and Information Systems, Rutgers University, Piscataway, NJ, USA Department of Management, Bar-Ilan University, Ramat Gan, Israel Doctoral Program in Operations Research, Rutgers University, Piscataway, NJ, USA.",abstractText,[0],[0]
Correspondence to: Jonathan Eckstein <jeckstei@business.rutgers.edu>.,abstractText,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",abstractText,[0],[0]
Copyright 2017 by the author(s).,abstractText,[0],[0]
"a particular functional form for f̂(x) (for example, an affine function of x) and then use an optimization procedure to estimate the parameters in this functional form.",abstractText,[0],[0]
"Here, we are interested in cases in which a concise candidate functional form for f̂ is not readily apparent, and we wish to estimate f̂ by searching over a very highdimensional space of parameters.",abstractText,[0],[0]
"For example, Breiman (2001) proposed the method of random forests, which constructs f̂ by training regression trees on multiple random subsamples of the data, and then averaging the resulting predictors.",abstractText,[0],[0]
"Another proposal is the RuleFit algorithm (Friedman & Popescu, 2008), which enhances L1regularized regression by generating box-based rules to use as additional explanatory variables.",abstractText,[0],[0]
"Given a, b ∈ R with a ≤ b, the rule function r(a,b) : R → {0, 1} is given by r(a,b)(x) =",abstractText,[0],[0]
"I ( ∧j∈{1,...,n}(aj ≤ xj ≤ bj) ) , (1) that is r(a,b)(x) = 1 if a ≤ x ≤ b",abstractText,[0],[0]
"(componentwise) and r(a,b)(x) = 0",abstractText,[0],[0]
otherwise.,abstractText,[0],[0]
"RuleFit generates rules through a two-phase procedure: first, it determines a regression tree ensemble, and then decomposes these trees into rules and determines the regression model coefficients (including for the rules).",abstractText,[0],[0]
"The approach of Dembczyński et al. (2008a) generates rules more directly (without having to rely on an initial ensemble of decision trees) within gradient boosting (Friedman, 2001) for non-regularized regression.",abstractText,[0],[0]
"In this scheme, a greedy procedure generates the rules within a gradient descent method runs that for a predetermined number of iterations.",abstractText,[0],[0]
Aho et al. (2012) extended the RuleFit method to solve more general multi-target regression problems.,abstractText,[0],[0]
"For the special case of single-target regression, however, their experiments suggest that random forests and RuleFit outperform several other methods, including their own extended implementation and the algorithm of Dembczyński et al. (2008a).",abstractText,[0],[0]
"Compared with random forests and other popular learning approaches such as kernel-based methods and neural networks, rule-based approaches have the advantage of generally being considered more accessible and easier to interpret by domain experts.",abstractText,[0],[0]
"Rule-based methods also have a considerable history in classification settings, as in for example Weiss & Indurkhya (1993), Cohen & Singer Rule-Enhanced Penalized Regression by Column Generation using Rectangular Maximum Agreement (1999), and Dembczyński et al. (2008b).",abstractText,[0],[0]
"Here, we propose an iterative optimization-based regression procedure called REPR (Rule-Enhanced Penalized Regression).",abstractText,[0],[0]
"Its output models resemble those of RuleFit, but our methodology draws more heavily on exact optimization techniques from the field of mathematical programming.",abstractText,[0],[0]
"While it is quite computationally intensive, its prediction performance appears promising.",abstractText,[0],[0]
"As in RuleFit, we start with a linear regression model (in this case, with L1-penalized coefficients to promote sparsity), and enhance it by synthesizing rules of the form (1).",abstractText,[0],[0]
We incrementally adjoin such rules to our (penalized) linear regression model as if they were new observation variables.,abstractText,[0],[0]
"Unlike RuleFit, we control the generation of new rules using the classical mathematical programming technique of column generation.",abstractText,[0],[0]
Our employment of column generation roughly resembles its use in the LPBoost ensemble classification method of Demiriz et al. (2002).,abstractText,[0],[0]
Column generation involves cyclical alternation between optimization of a restricted master problem (in our case a linear or convex quadratic program) and a pricing problem that finds the most promising new variables to adjoin to the formulation.,abstractText,[0],[0]
"In our case, the pricing problem is equivalent to an NP-hard combinatorial problem we call Rectangular Maximum Agreement (RMA), which generalizes the Maximum Mononial Agreement (MMA) problem as formulated and solved by Eckstein & Goldberg (2012).",abstractText,[0],[0]
"We solve the RMA problem by a similar branch-and-bound method procedure, implemented using parallel computing techniques.",abstractText,[0],[0]
"To make our notation below more concise, we let X denote the matrix whose rows are X 1 , . . .",abstractText,[0],[0]
", X > m, and also let y = (y1, . . .",abstractText,[0],[0]
", ym) ∈ R.",abstractText,[0],[0]
"We may then express a problem instance by the pair (X, y).",abstractText,[0],[0]
"We also let xij denote the (i, j)th element of this matrix, that is, the value of variable j in observation i. 2.",abstractText,[0],[0]
"A Penalized Regression Model with Rules Let K be a set of pairs (a, b) ∈ R × R with a ≤ b, constituting a catalog of all the possible rules of the form (1) that we wish to be available to our regression model.",abstractText,[0],[0]
"The set K will typically be extremely large: restricting each aj and bj to values that appear as xij for some i, which is sufficient to describe all possible distinct behaviors of rules of the form (1) on the dataset X , there are still ∏n j=1 `j(`j + 1)/2 ≥ 3 possible choices for (a, b), where `j = | ⋃m i=1{xij}| is the number of distinct values for xij .",abstractText,[0],[0]
The predictors f̂ that our method constructs are of the form f̂(x) = β0 + n ∑,abstractText,[0],[0]
Rule-Enhanced Penalized Regression by Column Generation  using Rectangular Maximum Agreement,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1980–1989 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1980",text,[0],[0]
Rumors have always been a social disease.,1 Introduction,[0],[0]
"In recent years, it has become unprecedentedly convenient for the “evil-doers” to create and disseminate rumors in massive scale with low cost thanks to the popularity of social media outlets on Twitter, Facebook, etc.",1 Introduction,[0],[0]
"The worst effect of false rumors could be devastating to individual and/or society.
",1 Introduction,[0],[0]
"Research pertaining rumors spans multiple disciplines, such as philosophy and humanities (DiFonzo and Bordia, 2007; Donovan, 2007), social psychology (Allport and Postman, 1965; Jaeger et al., 1980; Rosnow and Foster, 2005), political studies (Allport and Postman, 1946; Berinsky, 2017), management science (DiFonzo et al., 1994; Kimmel, 2004) and recently computer science and artificial intelligence (Qazvinian et al., 2011; Ratkiewicz et al., 2011; Castillo et al., 2011; Hannak et al., 2014; Zhao et al., 2015; Ma et al.,
2015).",1 Introduction,[0],[0]
"Rumor is commonly defined as information that emerge and spread among people whose truth value is unverified or intentionally false (DiFonzo and Bordia, 2007; Qazvinian et al., 2011).",1 Introduction,[0],[0]
"Analysis shows that people tend to stop spreading a rumor if it is known as false (Zubiaga et al., 2016b).",1 Introduction,[0],[0]
"However, identifying such misinformation is non-trivial and needs investigative journalism to fact check the suspected claim, which is labor-intensive and time-consuming.",1 Introduction,[0],[0]
The proliferation of social media makes it worse due to the ever-increasing information load and dynamics.,1 Introduction,[0],[0]
"Therefore, it is necessary to develop automatic and assistant approaches to facilitate real-time rumor tracking and debunking.
",1 Introduction,[0],[0]
"For automating rumor detection, most of the previous studies focused on text mining from sequential microblog streams using supervised models based on feature engineering (Castillo et al., 2011; Kwon et al., 2013; Liu et al., 2015; Ma et al., 2015), and more recently deep neural models (Ma et al., 2016; Chen et al., 2017; Ruchansky et al., 2017).",1 Introduction,[0],[0]
These methods largely ignore or oversimplify the structural information associated with message propagation which however has been shown conducive to provide useful clues for identifying rumors.,1 Introduction,[0],[0]
"Kernel-based method (Wu et al., 2015; Ma et al., 2017) was thus proposed to model the structure as propagation trees in order to differentiate rumorous and non-rumorous claims by comparing their tree-based similarities.",1 Introduction,[0],[0]
"But such kind of approach cannot directly classify a tree without pairwise comparison with all other trees imposing unnecessary overhead, and it also cannot automatically learn any high-level feature representations out of the noisy surface features.
",1 Introduction,[0],[0]
"In this paper, we present a neural rumor detection approach based on recursive neural networks (RvNN) to bridge the content semantics and propagation clues.",1 Introduction,[0],[0]
"RvNN and its variants
were originally used to compose phrase or sentence representation for syntactic and semantic parsing (Socher et al., 2011, 2012).",1 Introduction,[0],[0]
"Unlike parsing, the input into our model is a propagation tree rooted from a source post rather than the parse tree of an individual sentence, and each tree node is a responsive post instead of an individual words.",1 Introduction,[0],[0]
"The content semantics of posts and the responsive relationship among them can be jointly captured via the recursive feature learning process along the tree structure.
",1 Introduction,[0],[0]
"So, why can such neural model do better for the task?",1 Introduction,[0],[0]
"Analysis has generally found that Twitter could “self-correct” some inaccurate information as users share opinions, conjectures and evidences (Zubiaga et al., 2017).",1 Introduction,[0],[0]
"To illustrate our intuition, Figure 1 exemplifies the propagation trees of two rumors in our dataset, one being false and the other being true1.",1 Introduction,[0],[0]
Structure-insensitive methods basically relying on the relative ratio of different stances in the text cannot do well when such clue is unclear like this example.,1 Introduction,[0],[0]
"However, it can be seen that when a post denies the false rumor, it tends to spark supportive or affirmative replies confirming the denial; in contrast, denial to a true rumor tends to trigger question or denial in its replies.",1 Introduction,[0],[0]
"This observation may suggest a more general hypothesis that the repliers tend to disagree with (or question) who support a false rumor or deny a true rumor, and also they tend to agree with who deny a false rumor or support a true rumor.",1 Introduction,[0],[0]
"Meanwhile, a reply, rather than directly responding to the source tweet (i.e., the root), is usually responsive to its immediate ancestor (Lukasik et al., 2016; Zubiaga et al., 2016a), suggesting obvious local characteristic of the interaction.",1 Introduction,[0],[0]
"The recursive network naturally models such structures for learning to capture the rumor indicative signals and enhance the representation by recursively aggregating the signals from different branches.
",1 Introduction,[0],[0]
"To this end, we extend the standard RvNN into two variants, i.e., a bottom-up (BU) model and a top-down (TD) model, which represent the propagation tree structure from different angles, in order to visit the nodes and combine their representations following distinct directions.",1 Introduction,[0],[0]
"The important merit of such architecture is that the node features can be selectively refined by the recursion given the connection and direction of all paths of the
1False (true) rumor means the veracity of the rumorous claim is false (true).
tree.",1 Introduction,[0],[0]
"As a result, it can be expected that the discriminative signals are better embedded into the learned representations.
",1 Introduction,[0],[0]
We evaluate our proposed approach based on two public Twitter datasets.,1 Introduction,[0],[0]
"The results show that our method outperforms strong rumor detection baselines with large margin and also demonstrate much higher effectiveness for detection at early stage of propagation, which is promising for realtime intervention and debunking.",1 Introduction,[0],[0]
"Our contributions are summarized as follows in three folds:
•",1 Introduction,[0],[0]
"This is the first study that deeply integrates both structure and content semantics based on tree-structured recursive neural networks for detecting rumors from microblog posts.
",1 Introduction,[0],[0]
"• We propose two variants of RvNN models based on bottom-up and top-down tree structures to generate better integrated representations for a claim by capturing both structural and textural properties signaling rumors.
",1 Introduction,[0],[0]
• Our experiments based on real-world Twitter datasets achieve superior improvements over state-of-the-art baselines on both rumor classification and early detection tasks.,1 Introduction,[0],[0]
We make the source codes in our experiments publicly accessible 2.,1 Introduction,[0],[0]
"Most previous automatic approaches for rumor detection (Castillo et al., 2011; Yang et al., 2012; Liu
2https://github.com/majingCUHK/Rumor_",2 Related Work,[0],[0]
"RvNN
et al., 2015) intended to learn a supervised classifier by utilizing a wide range of features crafted from post contents, user profiles and propagation patterns.",2 Related Work,[0],[0]
"Subsequent studies were then conducted to engineer new features such as those representing rumor diffusion and cascades (Friggeri et al., 2014; Hannak et al., 2014) characterized by comments with links to debunking websites.",2 Related Work,[0],[0]
Kwon et al. (2013) introduced a time-series-fitting model based on the volume of tweets over time.,2 Related Work,[0],[0]
Ma et al. (2015) extended their model with more chronological social context features.,2 Related Work,[0],[0]
"These approaches typically require heavy preprocessing and feature engineering.
",2 Related Work,[0],[0]
"Zhao et al. (2015) alleviated the engineering effort by using a set of regular expressions (such as “really?”, “not true”, etc) to find questing and denying tweets, but the approach was oversimplified and suffered from very low recall.",2 Related Work,[0],[0]
Ma et al. (2016) used recurrent neural networks (RNN) to learn automatically the representations from tweets content based on time series.,2 Related Work,[0],[0]
"Recently, they studied to mutually reinforce stance detection and rumor classification in a neural multi-task learning framework (Ma et al., 2018).",2 Related Work,[0],[0]
"However, the approaches cannot embed features reflecting how the posts are propagated and requires careful data segmentation to prepare for time sequence.
",2 Related Work,[0],[0]
Some kernel-based methods were exploited to model the propagation structure.,2 Related Work,[0],[0]
Wu et al. (2015) proposed a hybrid SVM classifier which combines a RBF kernel and a random-walk-based graph kernel to capture both flat and propagation patterns for detecting rumors on Sina Weibo.,2 Related Work,[0],[0]
Ma et al. (2017) used tree kernel to capture the similarity of propagation trees by counting their similar substructures in order to identify different types of rumors on Twitter.,2 Related Work,[0],[0]
"Compared to their studies, our model can learn the useful features via a more natural and general approach, i.e., the tree-structured neural network, to jointly generate representations from both structure and content.
",2 Related Work,[0],[0]
"RvNN has demonstrated state-of-the-art performances in a variety of tasks, e.g., images segmentation (Socher et al., 2011), phrase representation from word vectors (Socher et al., 2012), and sentiment classification in sentences (Socher et al., 2013).",2 Related Work,[0],[0]
"More recently, a deep RvNN was proposed to model the compositionality in natural language for fine-grained sentiment classification by stacking multiple recursive layers (Irsoy
and Cardie, 2014).",2 Related Work,[0],[0]
"In order to avoid gradient vanishing, some studies integrated Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) to RvNN (Zhu et al., 2015; Tai et al., 2015).",2 Related Work,[0],[0]
Mou et al. (2015) used a convolutional network over tree structures for syntactic tree parsing of natural language sentences.,2 Related Work,[0],[0]
"We define a Twitter rumor detection dataset as a set of claims C = {C1, C2, · · · , C|C|}, where each claim Ci corresponds to a source tweet ri which consists of ideally all its relevant responsive tweets in chronological order, i.e., Ci = {ri, xi1, xi2, · · · , xim}where each xi∗ is a responsive tweet of the root ri.",3 Problem Statement,[0],[0]
"Note that although the tweets are notated sequentially, there are connections among them based on their reply or repost relationships, which can form a propagation tree structure (Wu et al., 2015; Ma et al., 2017) with ri being the root node.
",3 Problem Statement,[0],[0]
"We formulate this task as a supervised classification problem, which learns a classifier f from labeled claims, that is f : Ci → Yi, where Yi takes one of the four finer-grained classes: non-rumor, false rumor, true rumor, and unverified rumor that are introduced in the literature (Ma et al., 2017; Zubiaga et al., 2016b).
",3 Problem Statement,[0],[0]
"An important issue of the tree structure is concerned about the direction of edges, which can result in two different architectures of the model: 1) a bottom-up tree; 2) a top-down tree, which are defined as follows:
• Bottom-up tree takes the similar shape as shown in Figure 1, where responsive nodes always point to their responded nodes and leaf nodes not having any response are laid out at the furthest level.",3 Problem Statement,[0],[0]
"We represent a tree as Ti = 〈Vi, Ei〉, where Vi = Ci which consists of all relevant posts as nodes, and Ei denotes a set of all directed links, where for any u, v ∈",3 Problem Statement,[0],[0]
"Vi, u ← v exists if v responses to u.",3 Problem Statement,[0],[0]
"This structure is similar to a citation network where a response mimics a reference.
",3 Problem Statement,[0],[0]
"• Top-down tree naturally conforms to the direction of information propagation, in which a link u → v means the information flows from u to v and v sees it and provides a response to u. This structure reverses bottomup tree and simulates how information cas-
cades from a source tweet, i.e., the root, to all its receivers, i.e., the decedents, which is similar as (Wu et al., 2015; Ma et al., 2017).",3 Problem Statement,[0],[0]
The core idea of our method is to strengthen the high-level representation of tree nodes by the recursion following the propagation structure over different branches in the tree.,4 RvNN-based Rumor Detection,[0],[0]
"For instance, the responsive nodes confirming or supporting a node (e.g., “I agree”, “be right”, etc) can further reinforce the stance of that node while denial or questioning responses (e.g., “disagree, “really?!)",4 RvNN-based Rumor Detection,[0],[0]
otherwise weaken its stance.,4 RvNN-based Rumor Detection,[0],[0]
"Compared to the kernelbased method using propagation tree (Wu et al., 2015; Ma et al., 2017), our method does not need pairwise comparison among large number of subtrees, and can learn much stronger representation of content following the response structure.
",4 RvNN-based Rumor Detection,[0],[0]
"In this section, we will describe our extension to the standard RvNN for modeling rumor detection based on the bottom-up and top-down architectures presented in Section 3.",4 RvNN-based Rumor Detection,[0],[0]
RvNN is a type of tree-structured neural networks.,4.1 Standard Recursive Neural Networks,[0],[0]
"The original version of RvNN utilized binarized sentence parse trees (Socher et al., 2012), in which the representation associated with each node of a parse tree is computed from its direct children.",4.1 Standard Recursive Neural Networks,[0],[0]
"The overall structure of the standard RvNN is illustrated as the right side of Figure 2, corresponding to the input parse tree at the left side.
",4.1 Standard Recursive Neural Networks,[0],[0]
"Leaf nodes are the words in an input sentence, each represented by a low-dimensional word embedding.",4.1 Standard Recursive Neural Networks,[0],[0]
"Non-leaf nodes are sentence constituents, computed by recursion based on the presentations of child nodes.",4.1 Standard Recursive Neural Networks,[0],[0]
"Let p be the feature vector of a parent node whose children are c1 and c2, the representation of the parent is computed by p = f(W ·[c1; c2]+b), where f(·) is the activation
function withW and b as parameters.",4.1 Standard Recursive Neural Networks,[0],[0]
This computation is done recursively over all tree nodes; the learned hidden vectors of the nodes can then be used for various classification tasks.,4.1 Standard Recursive Neural Networks,[0],[0]
The core idea of bottom-up model is to generate a feature vector for each subtree by recursively visiting every node from the leaves at the bottom to the root at the top.,4.2 Bottom-up RvNN,[0],[0]
"In this way, the subtrees with similar contexts, such as those subtrees having a denial parent and a set of supportive children, will be projected into the proximity in the representation space.",4.2 Bottom-up RvNN,[0],[0]
"And thus such local rumor indicative features are aggregated along different branches into some global representation of the whole tree.
",4.2 Bottom-up RvNN,[0],[0]
"For this purpose, we make a natural extension to the original RvNN.",4.2 Bottom-up RvNN,[0],[0]
"The overall structure of our proposed bottom-up model is illustrated in Figure 3(b), taking a bottom-up tree (see Figure 3(a)) as input.",4.2 Bottom-up RvNN,[0],[0]
"Different from the standard RvNN, the input of each node in the bottom-up model is a post represented as a vector of words in the vocabulary in terms of tfidf values.",4.2 Bottom-up RvNN,[0],[0]
"Here, every node has an input vector, and the number of children of nodes varies significantly3.
",4.2 Bottom-up RvNN,[0],[0]
"In rumor detection, long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) and gated recurrent units (GRU) (Cho et al., 2014) were used to learn textual representation, which adopts memory units to store information over long time steps (Ma et al., 2016).",4.2 Bottom-up RvNN,[0],[0]
"In this paper, we choose to extend GRU as hidden unit to model long-distance interactions over the tree nodes because it is more efficient due to fewer parameters.",4.2 Bottom-up RvNN,[0],[0]
"Let S(j) denote the set of direct children of the node j. The transition equations of node j in the bottom-up model are formulated as follows:
x̃j = xjE hS = ∑
s∈S(j)
hs
rj = σ",4.2 Bottom-up RvNN,[0],[0]
"(Wrx̃j + UrhS)
zj = σ",4.2 Bottom-up RvNN,[0],[0]
"(Wzx̃j + UzhS)
h̃j = tanh (Whx̃j + Uh(hS",4.2 Bottom-up RvNN,[0],[0]
rj)),4.2 Bottom-up RvNN,[0],[0]
hj =,4.2 Bottom-up RvNN,[0],[0]
"(1− zj) hS + zj h̃j
(1)
3In",4.2 Bottom-up RvNN,[0],[0]
"standard RvNN, since an input instance is the parse tree of a sentence, only leaf nodes have input vector, each node representing a word of the input sentence, and the nonleaf nodes are constituents of the sentence, and thus the number of children of a node is limited.
",4.2 Bottom-up RvNN,[0],[0]
"where xj is the original input vector of node j, E denotes the parameter matrix for transforming this input post, x̃j is the transformed representation of j, [W∗, U∗] are the weight connections inside GRU, and hj and hs refer to the hidden state of j and its s-th child.",4.2 Bottom-up RvNN,[0],[0]
"Thus hS denotes the sum of the hidden state of all the children of j assuming that all children are equally important to j. As with the standard GRU, denotes element-wise multiplication; a reset gate rj determines how to combine the current input x̃j with the memory of children, and an update gate zj defines how much memory from the children is cascaded into the current node; and h̃j denotes the candidate activation of the hidden state of the current node.",4.2 Bottom-up RvNN,[0],[0]
"Different from the standard GRU unit, the gating vectors in our variant of GRU are dependent on the states of many child units, allowing our model to incorporate representations from different children.
",4.2 Bottom-up RvNN,[0],[0]
"After recursive aggregation from bottom to up, the state of root node (i.e., source tweet) can be regard as the representation of the whole tree which is used for supervised classification.",4.2 Bottom-up RvNN,[0],[0]
"So, an output layer is connected to the root node for predicting the class of the tree using a softmax function:
ŷ = Softmax(Vh0 + b) (2)
where h0 is the learned hidden vector of root node; V and b are the weights and bias in output layer.",4.2 Bottom-up RvNN,[0],[0]
"This model is designed to leverage the structure of top-down tree to capture complex propagation patterns for classifying rumorous claims, which is shown in Figure 3(c).",4.3 Top-down RvNN,[0],[0]
"It models how the informa-
tion flows from source post to the current node.",4.3 Top-down RvNN,[0],[0]
"The idea of this top-down approach is to generate a strengthened feature vector for each post considering its propagation path, where rumor-indicative features are aggregated along the propagation history in the path.",4.3 Top-down RvNN,[0],[0]
"For example, if current post agree with its parent’s stance which denies the source post, the denial stance from the root node down to the current node on this path should be reinforced.",4.3 Top-down RvNN,[0],[0]
"Due to different branches of any non-leaf node, the top-down visit to its subtree nodes is also recursive.",4.3 Top-down RvNN,[0],[0]
"However, the nature of top-down tree lends this model different from the bottom-up one.",4.3 Top-down RvNN,[0],[0]
The representation of each node is computed by combining its own input and its parent node instead of its children nodes.,4.3 Top-down RvNN,[0],[0]
"This process proceeds recursively from the root node to its children until all leaf nodes are reached.
",4.3 Top-down RvNN,[0],[0]
Suppose that the hidden state of a non-leaf node can be passed synchronously to all its child nodes without loss.,4.3 Top-down RvNN,[0],[0]
Then the hidden state hj of a node j can be computed by combining the hidden state hP(j) of its parent node P(j) and its own input vector xj .,4.3 Top-down RvNN,[0],[0]
"Therefore, the transition equations of node j can be formulated as a standard GRU:
x̃j = xjE rj",4.3 Top-down RvNN,[0],[0]
= σ,4.3 Top-down RvNN,[0],[0]
( Wrx̃j + UrhP(j) ),4.3 Top-down RvNN,[0],[0]
zj = σ,4.3 Top-down RvNN,[0],[0]
"( Wzx̃j + UzhP(j)
) h̃j = tanh",4.3 Top-down RvNN,[0],[0]
"( Whx̃j + Uh(hP(j) rj)
)",4.3 Top-down RvNN,[0],[0]
hj = (1− zj) hP(j),4.3 Top-down RvNN,[0],[0]
"+ zj h̃j (3)
Through the top-down recursion, the learned representations are eventually embedded into the hidden vector of all the leaf nodes.",4.3 Top-down RvNN,[0],[0]
"Since the num-
ber of leaf nodes varies, the resulting vectors cannot be directly fed into a fixed-size neural layer for output.",4.3 Top-down RvNN,[0],[0]
"Therefore, we add a max-pooling layer to take the maximum value of each dimension of the vectors over all the leaf nodes.",4.3 Top-down RvNN,[0],[0]
"This can also help capture the most appealing indicative features from all the propagation paths.
",4.3 Top-down RvNN,[0],[0]
"Based on the pooling result, we finally use a softmax function in the output layer to predict the label of the tree:
ŷ = Softmax(Vh∞ + b) (4)
where h∞ is the pooling vector over all leaf nodes, V and b are parameters in the output layer.
",4.3 Top-down RvNN,[0],[0]
"Although both of the two RvNN models aim to capture the structural properties by recursively visiting all nodes, we can conjecture that the topdown model would be better.",4.3 Top-down RvNN,[0],[0]
"The hypothesis is that in the bottom-up case the final output relies on the representation of single root, and its information loss can be larger than the top-down one since in the top-down case the representations embedded into all leaf nodes along different propagation paths can be incorporated via pooling holistically.",4.3 Top-down RvNN,[0],[0]
"The model is trained to minimize the squared error between the probability distributions of the predictions and the ground truth:
L(y, ŷ) = N∑ n=1 C∑ c=1",4.4 Model Training,[0],[0]
"(yc − ŷc)2 + λ||θ||22 (5)
where yc is the ground truth and ŷc is the prediction probability of a class, N is the number of training claims, C is the number of classes, ||.||2 is the L2 regularization term over all model parameters θ, and λ is the trade-off coefficient.
",4.4 Model Training,[0],[0]
"During training, all the model parameters are updated using efficient back-propagation through structure (Goller and Kuchler, 1996; Socher et al., 2013), and the optimization is gradient-based following the Ada-grad update rule (Duchi et al., 2011) to speed up the convergence.",4.4 Model Training,[0],[0]
"We empirically initialize the model parameters with uniform distribution and set the vocabulary size as 5,000, the size of embedding and hidden units as 100.",4.4 Model Training,[0],[0]
We iterate over all the training examples in each epoch and continue until the loss value converges or the maximum epoch number is met.,4.4 Model Training,[0],[0]
"For experimental evaluation, we use two publicly available Twitter datasets released by Ma et al. (2017), namely Twitter15 and Twitter164, which respectively contains 1,381 and 1,181 propagation trees (see (Ma et al., 2017) for detailed statistics).",5.1 Datasets,[0],[0]
"In each dataset, a group of wide spread source tweets along with their propagation threads, i.e., replies and retweets, are provided in the form of tree structure.",5.1 Datasets,[0],[0]
"Each tree is annotated with one of the four class labels, i.e., non-rumor, false rumor, true rumor and unverified rumor.",5.1 Datasets,[0],[0]
We remove the retweets from the trees since they do not provide any extra information or evidence contentwise.,5.1 Datasets,[0],[0]
"We build two versions for each tree, one for the bottom-up tree and the other for the top-down tree, by flipping the edges’ direction.",5.1 Datasets,[0],[0]
"We make comprehensive comparisons between our models and some state-of-the-art baselines on rumor classification and early detection tasks.
- DTR: Zhao et al. (2015) proposed a DecisionTree-based Ranking model to identify trending rumors by searching for inquiry phrases.
- DTC: The information credibility model using a Decision-Tree Classifier (Castillo et al., 2011) based on manually engineering various statistical features of the tweets.
- RFC:",5.2 Experimental Setup,[0],[0]
"The Random Forest Classier using 3 fitting parameters as temporal properties and a set of handcrafted features on user, linguistic and structural properties (Kwon et al., 2013).
- SVM-TS: A linear SVM classifier that uses time-series to model the variation of handcrafted social context features (Ma et al., 2015).
- SVM-BOW: A naive baseline we built by representing text content using bag-of-words and using linear SVM for rumor classification.
- SVM-TK and SVM-HK: SVM classifier uses a Tree Kernel (Ma et al., 2017) and that uses a Hybrid Kernel (Wu et al., 2015), respectively, both of which model propagation structures with kernels.
- GRU-RNN: A detection model based on recurrent neural networks (Ma et al., 2016) with GRU units for learning rumor representations by modeling sequential structure of relevant posts.
4https://www.dropbox.com/s/ 7ewzdrbelpmrnxu/rumdetect2017.zip?dl=0
- BU-RvNN and TD-RvNN: Our bottom-up and top-down RvNN models, respectively.
",5.2 Experimental Setup,[0],[0]
"We implement DTC and RFC using Weka5, SVM-based models using LibSVM6 and all neural-network-based models with Theano7.",5.2 Experimental Setup,[0],[0]
We conduct 5-fold cross-validation on the datasets and use accuracy over all the four categories and F1 measure on each class to evaluate the performance of models.,5.2 Experimental Setup,[0],[0]
"As shown in Table 1, our proposed models basically yield much better performance than other methods on both datasets via the modeling of interaction structures of posts in the propagation.
",5.3 Rumor Classification Performance,[0],[0]
"It is observed that the performance of the 4 baselines in the first group based on handcrafted features is obviously poor, varying between 0.409 and 0.585 in accuracy, indicating that they fail to generalize due to the lack of capacity capturing helpful features.",5.3 Rumor Classification Performance,[0],[0]
"Among these baselines, SVMTS and RFC perform relatively better because they
5www.cs.waikato.ac.nz/ml/weka 6www.csie.ntu.edu.tw/˜cjlin/libsvm 7deeplearning.net/software/theano
use additional temporal traits, but they are still clearly worse than the models not relying on feature engineering.",5.3 Rumor Classification Performance,[0],[0]
DTR uses a set of regular expressions indicative of stances.,5.3 Rumor Classification Performance,[0],[0]
"However, only 19.6% and 22.2% tweets in the two datasets contain strings covered by these regular expressions, rendering unsatisfactory result.
",5.3 Rumor Classification Performance,[0],[0]
"Among the two kernel methods that are based on comparing propagation structures, we observe that SVM-TK is much more effective than SVMHK.",5.3 Rumor Classification Performance,[0],[0]
"There are two reasons: 1) SVM-HK was originally proposed and experimented on Sina Weibo (Wu et al., 2015), which may not be generalize well on Twitter.",5.3 Rumor Classification Performance,[0],[0]
"2) SVM-HK loosely couples two separate kernels: a RBF kernel based on handcrafted features, plus a random walk-based kernel which relies on a set of pre-defined keywords for jumping over the nodes probabilistically.",5.3 Rumor Classification Performance,[0],[0]
This under utilizes the propagation information due to such oversimplified treatment of tree structure.,5.3 Rumor Classification Performance,[0],[0]
"In contrast, SVM-TK is an integrated kernel and can fully utilize the structure by comparing the trees based on both textual and structural similarities.
",5.3 Rumor Classification Performance,[0],[0]
It appears that using bag-of-words is already a decent model evidenced as the fairly good performance of SVM-BOW which is even better than SVM-HK.,5.3 Rumor Classification Performance,[0],[0]
"This is because the features of SVMHK are handcrafted for binary classification (i.e., non-rumor vs rumor), ignoring the importance of indicative words or units that benefit finer-grained classification which can be captured more effectively by SVM-BOW.
",5.3 Rumor Classification Performance,[0],[0]
"The sequential neural model GRU-RNN performs slightly worse than SVM-TK, but much worse than our recursive models.",5.3 Rumor Classification Performance,[0],[0]
This is because it is a special case of the recursive model where each non-leaf node has only one child.,5.3 Rumor Classification Performance,[0],[0]
"It has to rely on a linear chain as input, which missed out valuable structural information.",5.3 Rumor Classification Performance,[0],[0]
"However, it does learn high-level features from the post content via hidden units of the neural model while SVM-TK cannot which can only evaluates similarities based on the overlapping words among subtrees.",5.3 Rumor Classification Performance,[0],[0]
"Our recursive models are inherently tree-structured and take advantages of representation learning following the propagation structure, thus beats SVM-TK.
",5.3 Rumor Classification Performance,[0],[0]
"In the two recursive models, TD-RvNN outperforms BU-RvNN, which indicates that the bottomup model may suffer from larger information loss than the top-down one.",5.3 Rumor Classification Performance,[0],[0]
"This verifies the hypothesis we made in Section 4.3 that the pooling layer
in the top-down model can effectively select important features embedded into the leaf nodes.
",5.3 Rumor Classification Performance,[0],[0]
"For only the non-rumor class, it seems that our method does not perform so well as some featureengineering baselines.",5.3 Rumor Classification Performance,[0],[0]
"This can be explained by the fact that these baselines are trained with additional features such as user information (e.g., profile, verification status, etc) which may contain clues for differentiating non-rumors from rumors.",5.3 Rumor Classification Performance,[0],[0]
"Also, the responses to non-rumors are usually much more diverse with little informative indication, making identification of non-rumors more difficult based on content even with the structure.",5.3 Rumor Classification Performance,[0],[0]
Detecting rumors at early state of propagation is important so that interventions can be made in a timely manner.,5.4 Early Rumor Detection Performance,[0],[0]
We compared different methods in term of different time delays measured by either tweet count received or time elapsed since the source tweet is posted.,5.4 Early Rumor Detection Performance,[0],[0]
"The performance is evaluated by the accuracy obtained when we incrementally add test data up to the check point given the targeted time delay or tweets volume.
",5.4 Early Rumor Detection Performance,[0],[0]
Figure 4 shows that the performance of our recursive models climbs more rapidly and starts to supersede the other models at the early stage.,5.4 Early Rumor Detection Performance,[0],[0]
"Although all the methods are getting to their best per-
formance in the end, TD-RvNN and BU-RvNN only need around 8 hours or about 90 tweets to achieve the comparable performance of the best baseline model, i.e., SVM-TK, which needs about 36 hours or around 300 posts, indicating superior early detection performance of our method.
",5.4 Early Rumor Detection Performance,[0],[0]
Figure 5 shows a sample tree at the early stage of propagation that has been correctly classified as a false rumor by both recursive models.,5.4 Early Rumor Detection Performance,[0],[0]
"We can see that this false rumor demonstrates typical patterns in subtrees and propagation paths indicative of the falsehood, where a set of responses supporting the parent posts that deny or question the source post are captured by our bottom-up model.",5.4 Early Rumor Detection Performance,[0],[0]
"Similarly, some patterns of propagation from the root to leaf nodes like “support→deny→support” are also seized by our top-down model.",5.4 Early Rumor Detection Performance,[0],[0]
"In comparison, sequential models may be confused because the supportive key terms such as “be right”, “yeah”, “exactly!” dominate the responses, and the SVM-TK may miss similar subtrees by just comparing the surface words.",5.4 Early Rumor Detection Performance,[0],[0]
We propose a bottom-up and a top-down treestructured model based on recursive neural networks for rumor detection on Twitter.,6 Conclusions and Future Work,[0],[0]
"The inher-
ent nature of recursive models allows them using propagation tree to guide the learning of representations from tweets content, such as embedding various indicative signals hidden in the structure, for better identifying rumors.",6 Conclusions and Future Work,[0],[0]
"Results on two public Twitter datasets show that our method improves rumor detection performance in very large margins as compared to state-of-the-art baselines.
",6 Conclusions and Future Work,[0],[0]
"In our future work, we plan to integrate other types of information such as user properties into the structured neural models to further enhance representation learning and detect rumor spreaders at the same time.",6 Conclusions and Future Work,[0],[0]
We also plan to use unsupervised models for the task by exploiting structural information.,6 Conclusions and Future Work,[0],[0]
"This work is partly supported by Innovation and Technology Fund (ITF) Project No. 6904333, and General Research Fund (GRF) Project",Acknowledgment,[0],[0]
No. 14232816 (12183516).,Acknowledgment,[0],[0]
We would like to thank anonymous reviewers for the insightful comments.,Acknowledgment,[0],[0]
Automatic rumor detection is technically very challenging.,abstractText,[0],[0]
"In this work, we try to learn discriminative features from tweets content by following their non-sequential propagation structure and generate more powerful representations for identifying different type of rumors.",abstractText,[0],[0]
"We propose two recursive neural models based on a bottom-up and a top-down tree-structured neural networks for rumor representation learning and classification, which naturally conform to the propagation layout of tweets.",abstractText,[0],[0]
Results on two public Twitter datasets demonstrate that our recursive neural models 1) achieve much better performance than state-of-the-art approaches; 2) demonstrate superior capacity on detecting rumors at very early stage.,abstractText,[0],[0]
Rumor Detection on Twitter with Tree-structured Recursive Neural Networks,title,[0],[0]
"Submodular Functions (Fujishige, 2005) are a special class of set functions, which have rich structures and a lot of links
1Tencent AI Lab 2State Key Lab of CAD&CG, Zhejiang University.",1. Introduction,[0],[0]
Correspondence to: Weizhong Zhang,1. Introduction,[0],[0]
<zhangweizhongzju@gmail.com,1. Introduction,[0],[0]
">, Bin Hong <hongbinzju@gmail.com>, Lin Ma <forest.linma@gmail.com>, Wei Liu <wl2223@columbia.edu>, Tong Zhang <tongzhang@tongzhang-ml.org>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
with convex functions.",1. Introduction,[0],[0]
"They arise naturally in many domains, such as clustering (Narasimhan & Bilmes, 2007), image segmentation (Kolmogorov & Zabin, 2004; Cevher et al., 2009), document summarization (Lin & Bilmes, 2011a), etc.",1. Introduction,[0],[0]
"Most of these applications can be finally deduced to a Submodular Function Minimization (SFM) problem:
min A⊆V F (A), (SFM)
where F (A) is a submodular function defined on a set V .",1. Introduction,[0],[0]
"The problem of SFM has been extensively studied for several decades in the literatures (Edmonds, 1970; Lovász, 1983; McCormick, 2005; Wu et al., 2016; Ene et al., 2017), in which many algorithms have been developed from the perspectives of combinatorial optimization and convex optimization.",1. Introduction,[0],[0]
"The most well-known conclusion is that SFM is solvable in strongly polynomial time (Iwata et al., 2001).",1. Introduction,[0],[0]
"Unfortunately, due to the high-degree polynomial dependence, the applications of submodular functions on the large scale problems remain challenging, such as image segmentation (Cevher et al., 2009) and speech analysis (Lin & Bilmes, 2011b), which both involve a huge number of variables.
",1. Introduction,[0],[0]
"Screening (El Ghaoui et al., 2012) is an emerging technique, which has been proved to be effective in accelerating large-scale sparse model training.",1. Introduction,[0],[0]
It is motivated by the well-known feature of sparse models that a significant portion of the coefficients in the optimal solutions of them (resp.,1. Introduction,[0],[0]
"their dual problems) are zeros, that is, the corresponding features (resp. samples) are irrelevant with the final learned models.",1. Introduction,[0],[0]
Screening methods aim to quickly identify these irrelevant features and/or samples and remove them from the datasets before or during the training process.,1. Introduction,[0],[0]
"Thus, the problem size can be reduced dramatically, leading to substantial savings in the computational cost.",1. Introduction,[0],[0]
The framework of these methods is given in Algorithm 1.,1. Introduction,[0],[0]
"Since screening methods are always independent of the training algorithms, they can be integrated with all the algorithms flexibly.",1. Introduction,[0],[0]
"In the recent few years, specific screening methods for most of the traditional sparse models have been developed, such as Lasso (Tibshirani et al., 2012; Wang et al., 2013; Wang & Ye, 2015), sparse logistic regression (Wang et al., 2014), multi-task learning (Ndiaye et al., 2015) and SVM (Ogawa et al., 2013; Zhang et al., 2017).",1. Introduction,[0],[0]
"Empirical studies indicate that the speedups they achieved can be orders of magnitudes.
",1. Introduction,[0],[0]
"Algorithm 1 Framework of screening in sparse learning 1: Estimate the dual (resp. primal) optimum of the sparse
model.",1. Introduction,[0],[0]
"2: Based on the estimation above, infer which components
of the primal (resp.",1. Introduction,[0],[0]
dual ) optimum are zeros from the KKT conditions.,1. Introduction,[0],[0]
3: Remove the features (resp. samples) corresponding to the identified components.,1. Introduction,[0],[0]
"4: Train the model on the reduced dataset.
",1. Introduction,[0],[0]
The binary attribute (each element in V must be either in or not in the optimal solution) of SFM motivates us to introduce the key idea of screening into SFM to accelerate its optimization process.,1. Introduction,[0],[0]
The most intuitive approach is to identify the elements that are guaranteed to be included or excluded in the minimizer A∗ of SFM prior to or during actually solving it.,1. Introduction,[0],[0]
"Then, by fixing the identified active elements and removing the inactive ones, we just need to solve a small-scale problem.",1. Introduction,[0],[0]
"However, we note that existing screening methods are all developed for convex models and they cannot be applied to SFM directly.",1. Introduction,[0],[0]
"The reason is that they all heavily depend on KKT conditions (see Algorithm 1), which do not exist in SFM problems.
",1. Introduction,[0],[0]
"In this paper, to improve the efficiency of SFM algorithms, we propose a novel Inactive and Active Element Screening (IAES) framework for SFM, which consists of two kinds of screening rules, i.e., Inactive Elements Screening (IES) and Active Elements Screening (AES).",1. Introduction,[0],[0]
"As we analyze above, the major challenge in developing IAES is the absence of KKT conditions.",1. Introduction,[0],[0]
"We bypass this obstacle by carefully studying the relationship between SFM and convex optimization, which can be regarded as another form of KKT conditions.",1. Introduction,[0],[0]
"We find that SFM is closely related to a particular convex primal and dual problem pair Q-P and Q-D (see Section 2), that is, the minimizer of SFM can be obtained from the positive components of the optimum of problem Q-P. Hence, the proposed IAES identifies the active and inactive elements by estimating the lower and upper bounds of the components of the optimum of problem Q-P. Thus, one of our major technical contributions is a novel framework (Section 3)—developed by carefully studying the strong convexity of the corresponding primal and dual objective functions, the structure of the base polyhedra, and the optimality conditions of the SFM problem—for deriving accurate optimum estimation of problem Q-P. We integrate IAES with the solver for problems Q-P and Q-D. As the solver goes on, and the estimation becomes more and more accurate, IAES can identify more and more elements.",1. Introduction,[0],[0]
"By fixing the active elements and removing the inactive ones, the problem size can be reduced gradually.",1. Introduction,[0],[0]
IAES is safe in the sense that it would never sacrifice any accuracy on the final output.,1. Introduction,[0],[0]
"To the best of our knowledge, IAES is the first screening
method in the domain of SFM or even combinatorial optimization.",1. Introduction,[0],[0]
"Moreover, compared with the screening methods for sparse models, an outstanding feature of IAES is that it has no theoretical limit in reducing the problem size.",1. Introduction,[0],[0]
"That is, we can finally reduce the problem size to zero, leading to substantial savings in the computational cost.",1. Introduction,[0],[0]
"The reason is that as the optimization proceeds, our estimation will be accurate enough to infer the affiliations of all the elements with the optimizer A∗.",1. Introduction,[0],[0]
"While in sparse models, screening methods can never reduce the problem size to zero since the features (resp. samples) with nonzero coefficients in the primal (resp. dual) optimum can never be removed.",1. Introduction,[0],[0]
Experiments (see Section 4) on both synthetic and real datasets demonstrate the significant speedups gained by IAES.,1. Introduction,[0],[0]
"For the convenience of presentation, we postpone the detailed proofs of theoretical results in the main text to the supplementary materials.
Notations: We consider a set V = {1, ..., p}, and denote its power set by 2V , which is composed of 2p subsets of V .",1. Introduction,[0],[0]
"|A| is the cardinality of a set A. A∪B and A∩B are the union and intersection of the sets A and B, respectively.",1. Introduction,[0],[0]
"A ⊆ B means that A is a subset of B, potentially being equal to B. Moreover, for w ∈",1. Introduction,[0],[0]
"Rp and α ∈ R, we let [w]k be the k-th component of w and {w ≥ α} (resp.",1. Introduction,[0],[0]
{w > α}) be the weak (resp.,1. Introduction,[0],[0]
"strong) α-sup-level set of w defined as {k : k ∈ V,",1. Introduction,[0],[0]
[w]k ≥ α} (resp.,1. Introduction,[0],[0]
"{k : k ∈ V, [w]k > α}).",1. Introduction,[0],[0]
"At last, for s ∈ Rp, we define a set function by s(A) = ∑ k∈A[s]k.",1. Introduction,[0],[0]
"This section is composed of two parts: a) briefly review some basics of submodular functions, SFM, and their relations with convex optimization; b) motivate our screening method IAES.
",2. Basics and Motivations,[0],[0]
"The followings are the definitions of submodular function, submodular polyhedra and base polyhedra, which play an important role in submodular analysis.",2. Basics and Motivations,[0],[0]
Definition 1.,2. Basics and Motivations,[0],[0]
"[Submodular Function (McCormick, 2005)]",2. Basics and Motivations,[0],[0]
"A set function F : 2V → R is submodular if and only if for all subsets A,B ⊆ V we have:
F (A) + F (B) ≥",2. Basics and Motivations,[0],[0]
F (A ∪B) + F,2. Basics and Motivations,[0],[0]
"(A ∩B).
",2. Basics and Motivations,[0],[0]
Definition 2.,2. Basics and Motivations,[0],[0]
"[Submodular and Base Polyhedra (Fujishige, 2005)] Let F be a submodular function such that F (∅) = 0.",2. Basics and Motivations,[0],[0]
"The submodular polyhedra P (F ) and the base polyhedra B(F ) are defined as:
P (F )={",2. Basics and Motivations,[0],[0]
"s∈ Rp : ∀A ⊆ V, s(A) ≤",2. Basics and Motivations,[0],[0]
"F (A)}, B(F )={",2. Basics and Motivations,[0],[0]
"s∈ Rp : s(V ) = F (V ),∀A ⊆ V, s(A) ≤",2. Basics and Motivations,[0],[0]
"F (A)}.
",2. Basics and Motivations,[0],[0]
"Below we give the definition of Lovász extension, which works as the bridge that connects submodular functions and convex functions.
",2. Basics and Motivations,[0],[0]
Definition 3.,2. Basics and Motivations,[0],[0]
"[Lovász Extension (Fujishige, 2005)]",2. Basics and Motivations,[0],[0]
"Given a set-function F such that F (∅) = 0, the Lovász extension f :",2. Basics and Motivations,[0],[0]
Rp → R is defined as follows: for w ∈,2. Basics and Motivations,[0],[0]
"Rp, order the components in a decreasing order [w]j1 ≥ ...",2. Basics and Motivations,[0],[0]
≥,2. Basics and Motivations,[0],[0]
"[w]jp , and define f(w) through the equation below,
f(w) = p∑",2. Basics and Motivations,[0],[0]
k=1,2. Basics and Motivations,[0],[0]
"[w]jk ( F ({j1, ..., jk})− F ({j1, ..., jk−1}) ) .
Lovász extension f(w) is convex if and only if F is submodular (see (Fujishige, 2005)).
",2. Basics and Motivations,[0],[0]
"We focus on the generic submodular function minimization problem SFM defined in Section 1 and denote its minimizer as A∗. To reveal the relationship between SFM and convex optimization and finally motivate our method, we need the following theorems.",2. Basics and Motivations,[0],[0]
Theorem 1.,2. Basics and Motivations,[0],[0]
"Let ψ1, ..., ψp be p convex functions on R, ψ∗1 , ..., ψ ∗ p be their Fenchel-conjugates (Borwein & Lewis, 2010), and f be the Lovász extension of a submodular function F .",2. Basics and Motivations,[0],[0]
Denote the subgradient of ψk(·) by ∂ψk(·).,2. Basics and Motivations,[0],[0]
"Then, the followings hold: (i)",2. Basics and Motivations,[0],[0]
"The problems below are dual of each other:
min w∈Rp f(w) + p∑ j=1 ψj([w]j), (P)
max s∈B(F )",2. Basics and Motivations,[0],[0]
− p∑ j=1 ψ∗j (−[s]j).,2. Basics and Motivations,[0],[0]
"(D)
(ii)",2. Basics and Motivations,[0],[0]
"The pair (w∗, s∗) is optimal for problems (P) and (D) if and only if{
(a): [s]∗k ∈ −∂ψk([w]∗k),∀k ∈ V, (b): w∗ ∈",2. Basics and Motivations,[0],[0]
"NB(F )(s∗),
(Opt)
where NB(F )(s∗) is the normal cone (see Chapter 2 of (Borwein & Lewis, 2010)) of B(F ) at s∗.
When ψj(·) is differentiable, we consider a sequence of set optimization problems parameterized by α ∈ R:
min A⊆V F (A) +",2. Basics and Motivations,[0],[0]
"∑ j∈A ∇ψj(α), (SFM’)
",2. Basics and Motivations,[0],[0]
where ∇ψj(·) is the gradient of ψk(·).,2. Basics and Motivations,[0],[0]
The problem SFM’ has tight connections with the convex optimization problem P (see the theorem below).,2. Basics and Motivations,[0],[0]
Theorem 2.,2. Basics and Motivations,[0],[0]
"[Submodular function minimization from the proximal problem, Proposition 8.4 in (Bach et al., 2013)]",2. Basics and Motivations,[0],[0]
"Under the same assumptions in Theorem 1, if ψj(·) is differentiable for all j ∈ V and w∗ is the unique minimizer of problem P, then for all α ∈ R, the minimal minimizer of problem SFM’ is {u > α} and the maximal minimizer is {u ≥ α}, that is, for any minimizers",2. Basics and Motivations,[0],[0]
"A∗α we have:
{w∗ > α} ⊆",2. Basics and Motivations,[0],[0]
A∗α ⊆ {w∗ ≥ α}.,2. Basics and Motivations,[0],[0]
"(1)
By choosing ψj(x) = 12x 2 and α = 0 in SFM’, combining Theorems 1 and 2, we can see that SFM can be reduced to the following primal and dual problems, one is a quadratic optimization problem and the other is equivalent to finding the minimum norm point in the base polytope B(F ):
min w∈Rp
P (w) := f(w) + 1
2 ‖w‖22, (Q-P)
max s∈B(F ) D(s) :",2. Basics and Motivations,[0],[0]
= −1 2 ‖s‖22.,2. Basics and Motivations,[0],[0]
"(Q-D)
",2. Basics and Motivations,[0],[0]
"According to (1), we can define two index sets:
E := {j ∈ V :",2. Basics and Motivations,[0],[0]
"[w]∗j > 0}, and G := {j ∈ V :",2. Basics and Motivations,[0],[0]
"[w]∗j < 0},
which imply that
(i): j ∈ E ⇒ j ∈ A∗, (R1) (ii): j ∈ G ⇒",2. Basics and Motivations,[0],[0]
j /∈,2. Basics and Motivations,[0],[0]
"A∗. (R2)
",2. Basics and Motivations,[0],[0]
"We call the j-th element active if j ∈ E and the ones in G inactive.
",2. Basics and Motivations,[0],[0]
"Suppose that we are given two subsets of E and G, by rules R1 and R2, we can see that many affiliations between A∗ and the elements of V can be deduced.",2. Basics and Motivations,[0],[0]
"Thus, we have less unknowns to solve in SFM and its size can be dramatically reduced.",2. Basics and Motivations,[0],[0]
We formalize this idea in Lemma 1.,2. Basics and Motivations,[0],[0]
Lemma 1.,2. Basics and Motivations,[0],[0]
"Given two subsets Ĝ ⊆ G and Ê ⊆ E , the followings hold:
(i): Ê ⊆ A∗, and for all j ∈ Ĝ we have j /∈",2. Basics and Motivations,[0],[0]
"A∗.
(ii):",2. Basics and Motivations,[0],[0]
"The problem SFM can be reduced to the following scaled problem:
min C⊆V/(Ê∪Ĝ)
F̂ (C) := F (Ê ∪ C)− F (Ê), (scaled-SFM)
which is also an SFM problem.
",2. Basics and Motivations,[0],[0]
"(iii): A∗ can be recovered by A∗ = Ê ∪ C∗, where C∗ is the minimizer of scaled-SFM.
",2. Basics and Motivations,[0],[0]
"Lemma 1 indicates that, if we can identify the active set Ê and inactive set Ĝ, we only need to solve a scaled problem scaled-SFM, which may have much smaller size than the original problem SFM, to exactly recover the optimal solution A∗ without sacrificing any accuracy.
",2. Basics and Motivations,[0],[0]
"However, since w∗ is unknown, we cannot directly apply rules R1 and R2 to identify the active set Ê and inactive set Ĝ.",2. Basics and Motivations,[0],[0]
"Inspired by the ideas in the gap safe screening methods ((Fercoq et al., 2015; Ndiaye et al., 2016; Shibagaki et al., 2016)) for convex problems, we can first estimate the region W that contains w∗ and then relax the rules R1 and R2 to the practicable versions.",2. Basics and Motivations,[0],[0]
"Specifically, we first denote
Ê := {j ∈ V : min w∈W",2. Basics and Motivations,[0],[0]
"[w]j > 0}, (2)
Ĝ := {j ∈ V : max w∈W",2. Basics and Motivations,[0],[0]
[w]j < 0}.,2. Basics and Motivations,[0],[0]
"(3)
It is obvious that Ê ⊆ E and Ĝ",2. Basics and Motivations,[0],[0]
"⊆ G. Hence, the rules R1 and R2 can be relaxed as follows:
(i): j ∈ Ê ⇒",2. Basics and Motivations,[0],[0]
"j ∈ A∗, (R1’) (ii): j ∈ Ĝ ⇒",2. Basics and Motivations,[0],[0]
j /∈,2. Basics and Motivations,[0],[0]
"A∗. (R2’)
",2. Basics and Motivations,[0],[0]
"In view of the rules R1’ and R2’, we sketch the development of IAES as follows: Step 1: Derive the estimationW such that w∗ ∈ W .",2. Basics and Motivations,[0],[0]
Step 2: Develop IAES via deriving the detailed screening rules R1’ and R2’.,2. Basics and Motivations,[0],[0]
"In this section, we first present the accurate optimum estimation by carefully studying the strong convexity of the functions P (w) andD(s), the optimality conditions of SFM and its relationship with the convex problem pair (see Section 3.1).",3. The Proposed Element Screening Method,[0],[0]
"Then, in Section 3.2, we develop our inactive and active element screening rules IES and AES step by step.",3. The Proposed Element Screening Method,[0],[0]
"At last, in Section 3.3, we develop the screening framework IAES by an alternating application of IES and AES.",3. The Proposed Element Screening Method,[0],[0]
"Let Ê and Ĝ be the active and inactive sets identified by the previous IAES steps (before applying IAES for the first time, they are ∅).",3.1. Optimum Estimation,[0],[0]
"From Lemma 1, we know that the problem SFM then can be reduced to the following scaled problem:
min C⊆V̂
F̂ (C)",3.1. Optimum Estimation,[0],[0]
":= F (Ê ∪ C)− F (Ê),
where V̂ = V/(Ê ∪ Ĝ).",3.1. Optimum Estimation,[0],[0]
The second term−F (Ê) at the right side of the equation above is added to make F̂ (∅) = 0.,3.1. Optimum Estimation,[0],[0]
"Thus, the corresponding problems Q-P and Q-D then become:
min ŵ∈Rp̂
P̂ (ŵ) := f̂(ŵ) + 1
2 ‖ŵ‖22, (Q-P’)
",3.1. Optimum Estimation,[0],[0]
max ŝ∈B(F̂ ),3.1. Optimum Estimation,[0],[0]
D̂(ŝ),3.1. Optimum Estimation,[0],[0]
":= −1 2 ‖ŝ‖22, (Q-D’)
where f̂(ŵ) is the Lovász extension of F̂ and p̂ = |V/(Ê ∪ Ĝ)|.",3.1. Optimum Estimation,[0],[0]
"Now, we turn to estimate the minimizer ŵ∗ of the problem Q-P’.",3.1. Optimum Estimation,[0],[0]
"The result is presented in the theorem below.
",3.1. Optimum Estimation,[0],[0]
Theorem 3.,3.1. Optimum Estimation,[0],[0]
"For any ŵ ∈ domP̂ (ŵ), ŝ ∈ B(F̂ ) and C ⊆ V̂ , we denote the dual gap as G(ŵ, ŝ) = P̂ (ŵ)",3.1. Optimum Estimation,[0],[0]
"− D̂(ŝ), and then we have
ŵ∗ ∈",3.1. Optimum Estimation,[0],[0]
"W = B ∩ Ω ∩ P,
where B = { w : ‖w − ŵ‖ ≤",3.1. Optimum Estimation,[0],[0]
"√ 2G(ŵ, ŝ) } , Ω = { w :
F̂ (V̂ ) − 2F̂ (C) ≤ ‖w‖1 ≤ ‖ŝ‖1 } , and P = { w :
〈w,1〉 = −F̂ (V̂ ) } .
",3.1. Optimum Estimation,[0],[0]
"From the theorem above, we can see that the estimation W is the intersection of three sets: the ball B, the `1-norm equipped spherical shell Ω and the planeP .",3.1. Optimum Estimation,[0],[0]
"As the optimizer goes on, the dual gapG(ŵ, ŝ) becomes smaller, and F̂ (V̂ )",3.1. Optimum Estimation,[0],[0]
"− 2F̂ (C) and ‖ŵ‖1 would converge to ‖ŵ∗‖1 (see Chapter 7 of (Bach et al., 2013)).",3.1. Optimum Estimation,[0],[0]
"Thus, the volumes of B and Ω become smaller and smaller during the optimization process, and the estimationW would be more and more accurate.",3.1. Optimum Estimation,[0],[0]
"We now turn to develop the screening rules IES and AES based on the estimation of the optimum ŵ∗.
From (2) and (3), we can see that, to develop the screening rules we need to solve two problems: minw∈W",3.2. Inactive and Active Element Screening,[0],[0]
[w]j and maxw∈W,3.2. Inactive and Active Element Screening,[0],[0]
[w]j .,3.2. Inactive and Active Element Screening,[0],[0]
"However, since W is highly non-convex and has a complex structure, it is very hard to solve these two problems efficiently.",3.2. Inactive and Active Element Screening,[0],[0]
"Hence, we rewrite the estimation W asW =",3.2. Inactive and Active Element Screening,[0],[0]
(B ∩ P) ∩,3.2. Inactive and Active Element Screening,[0],[0]
"(B ∩ Ω), and develop two different screening rules on B ∩ P and B ∩ Ω, respectively.",3.2. Inactive and Active Element Screening,[0],[0]
"Given the estimation B ∩ P , we derive the screening rules by solving the following problems
min w∈B∩P",3.2.1. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ P,[0],[0]
[w]j and max w∈B∩P,3.2.1. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ P,[0],[0]
"[w]j .
We show that both of the two problems above admit closedform solutions.",3.2.1. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ P,[0],[0]
Lemma 2.,3.2.1. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ P,[0],[0]
"Given the estimation ball B, the plane P and the active and inactive sets Ê and Ĝ, which are identified in the previous IAES steps, for all j ∈ [p̂] we denote
bj = 2 (∑ i 6=j [ŵ]i + F̂ (V̂ )− (p̂− 1)[ŵ]j ) ,
cj =",3.2.1. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ P,[0],[0]
(∑ i 6=j [ŵ]i + F̂ (V̂ ) )2,3.2.1. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ P,[0],[0]
"− (p̂− 1) ( 2G(ŵ, ŝ)− [ŵ]2j ) .
",3.2.1. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ P,[0],[0]
"Then the followings hold:
(i): min w∈B∩P",3.2.1. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ P,[0],[0]
"[w]j = [w] min j :=
−bj",3.2.1. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ P,[0],[0]
"− √ b2j − 4p̂cj
2p̂",3.2.1. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ P,[0],[0]
",
(ii): max w∈B∩P",3.2.1. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ P,[0],[0]
"[w]j = [w] max j :=
−bj + √ b2j − 4p̂cj
2p̂ .
",3.2.1. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ P,[0],[0]
We are now ready to present the active and inactive screening rules AES-1 and IES-1.,3.2.1. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ P,[0],[0]
Theorem 4.,3.2.1. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ P,[0],[0]
"Given the active and inactive sets Ê and Ĝ, which are identified in the previous IAES steps, we have
(i): The active element screening rule takes the form of
[w]minj > 0⇒ j ∈ A∗,∀j ∈ V/(Ê ∪ Ĝ).",3.2.1. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ P,[0],[0]
"(AES-1)
(ii): The inactive element screening rule takes the form of
[w]maxj < 0⇒ j /∈",3.2.1. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ P,[0],[0]
"A∗,∀j ∈ V/(Ê ∪ Ĝ).",3.2.1. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ P,[0],[0]
"(IES-1)
(iii): The active and inactive sets Ê and Ĝ can be updated by
Ê ← Ê ∪∆Ê , (4) Ĝ ← Ĝ ∪∆Ĝ, (5)
where ∆Ê and ∆Ĝ are the newly identified active and inactive sets defined as
∆Ê := {j ∈ V/(Ê ∪ Ĝ) :",3.2.1. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ P,[0],[0]
"[w]minj > 0},
∆Ĝ := {j ∈ V/(Ê ∪ Ĝ) :",3.2.1. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ P,[0],[0]
"[w]maxj < 0}.
From the theorem above, we can see that our rules AES-1 and IES-1 are safe in the sense that the detected elements are guaranteed to be included or excluded in A∗.",3.2.1. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ P,[0],[0]
"We now derive the second screening rule pair based on the estimation B ∩ Ω.
Due to the high non-convexity and complex structure of B ∩ Ω, directly solving problems minw∈B∩Ω[w]j and maxw∈B∩Ω[w]j is time consuming.",3.2.2. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ Ω,[0],[0]
"Notice that, to derive IAS and IES, we only need to judge whether the inequalities minw∈B∩Ω[w]j > 0 and maxw∈B∩Ω[w]j < 0 are satisfied or not, instead of calculating minw∈B∩Ω[w]j and maxw∈B∩Ω[w]j .",3.2.2. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ Ω,[0],[0]
"Hence, we only need to infer the hypotheses { w : w ∈ B,",3.2.2. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ Ω,[0],[0]
[w]j ≤ 0 } ∩ Ω = ∅,3.2.2. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ Ω,[0],[0]
"and{
w : w ∈ B,",3.2.2. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ Ω,[0],[0]
[w]j ≥ 0 } ∩ Ω = ∅ are true or false.,3.2.2. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ Ω,[0],[0]
"Thus, from the formulation of Ω (see Theorem 3), the problems boil down to calculating the minimum and the maximum of ‖w‖1 with { w : w ∈ B,",3.2.2. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ Ω,[0],[0]
"[w]j ≥ 0 } or{
w : w ∈ B,",3.2.2. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ Ω,[0],[0]
"[w]j ≤ 0 }
, which admit closed-form solutions.",3.2.2. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ Ω,[0],[0]
"The results are presented in the lemma below.
",3.2.2. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ Ω,[0],[0]
Lemma 3.,3.2.2. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ Ω,[0],[0]
"Given the estimation ball B and the active and inactive sets Ê and Ĝ, which are identified in the previous IAES steps, then the followings hold:
(i): ∀j ∈ p̂, if |[ŵ]j | > √
2G(ŵ, ŝ), then the element j can be identified by rule AES-1 or IES-1 to be active or inactive.",3.2.2. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ Ω,[0],[0]
"(ii): ∀j ∈ p̂, if 0 <",3.2.2. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ Ω,[0],[0]
"[ŵ]j ≤ √ 2G(ŵ, ŝ), we have
min w∈B,[w]j≤0
‖w‖1 < ‖ŵ‖1,
max w∈B,[w]j≤0
‖w‖1
= ‖ŵ‖1−2[ŵ]j+ √ 2p̂G(ŵ,ŝ), if [ŵ]j− √ 2G(ŵ,ŝ) p̂ <0,
‖ŵ‖1−[ŵ]j+ √ p̂−1 √ 2G(ŵ,ŝ)−[ŵ]2j , otherwise.
",3.2.2. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ Ω,[0],[0]
"(iii): ∀j ∈ p̂, if − √ 2G(ŵ, ŝ) ≤",3.2.2. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ Ω,[0],[0]
"[ŵ]j < 0, we have
min w∈B,[w]j≥0
‖w‖1 < ‖ŵ‖1,
max w∈B,[w]j≥0
‖w‖1
= ‖ŵ‖1+2[ŵ]j+ √ 2p̂G(ŵ,ŝ), if [ŵ]j+ √ 2G(ŵ,ŝ) p̂",3.2.2. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ Ω,[0],[0]
">0,
‖ŵ‖1+[ŵ]j+ √ p̂−1 √ 2G(ŵ,ŝ)−[ŵ]2j , otherwise.
",3.2.2. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ Ω,[0],[0]
We are now ready to present the second active and inactive screening rule pair AES-2 and IES-2.,3.2.2. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ Ω,[0],[0]
"From the lemma above, we can see that the element j with |[ŵ]j | >√
2G(ŵ, ŝ) can be screened by rules AES-1 and IES-1.",3.2.2. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ Ω,[0],[0]
"Hence, we now only need to consider the cases when |[ŵ]j | ≤",3.2.2. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ Ω,[0],[0]
"√ 2G(ŵ, ŝ).
",3.2.2. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ Ω,[0],[0]
Theorem 5.,3.2.2. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ Ω,[0],[0]
"Given a set C ⊆ V̂ and the active and inactive sets Ê and Ĝ identified in the previous IAES steps, then,
(i): The active element screening rule takes the form of{ 0 <",3.2.2. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ Ω,[0],[0]
"[ŵ]j ≤ √ 2G(ŵ, ŝ)
maxw∈B,[w]j≤0 ‖w‖1 < F̂ (V̂ )− 2F̂ (C)
⇒j ∈ A∗,∀j ∈ V/(Ê ∪ Ĝ).",3.2.2. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ Ω,[0],[0]
"(AES-2)
(ii): The inactive element screening rule takes the form of{ − √
2G(ŵ, ŝ) ≤",3.2.2. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ Ω,[0],[0]
[ŵ]j < 0,3.2.2. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ Ω,[0],[0]
"maxw∈B,[w]j≥0 ‖w‖1 < F̂ (V̂ )− 2F̂ (C)
⇒j /∈",3.2.2. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ Ω,[0],[0]
"A∗,∀j ∈ V/(Ê ∪ Ĝ).",3.2.2. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ Ω,[0],[0]
"(IES-2)
(iii): The active and inactive sets Ê and Ĝ can be updated by
Ê ← Ê ∪∆Ê , (6) Ĝ ← Ĝ ∪∆Ĝ, (7)
where ∆Ê and ∆Ĝ are the newly identified active and inactive sets defined as
∆Ê := { j ∈ V/(Ê ∪ Ĝ) :",3.2.2. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ Ω,[0],[0]
0 <,3.2.2. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ Ω,[0],[0]
"[ŵ]j ≤ √ 2G(ŵ, ŝ),
max w∈B,[w]j≤0
‖w‖1 < F̂ (V̂ )− 2F̂ (C) } ,
∆Ĝ",3.2.2. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ Ω,[0],[0]
:= { j ∈ V/(Ê ∪ Ĝ) :,3.2.2. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ Ω,[0],[0]
"− √ 2G(ŵ, ŝ) ≤",3.2.2. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ Ω,[0],[0]
"[ŵ]j < 0,
max w∈B,[w]j≥0
‖w‖1 < F̂ (V̂ )− 2F̂ (C) } .
",3.2.2. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ Ω,[0],[0]
Theorem 5 verifies the safety of AES-2 and IES-2.,3.2.2. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ Ω,[0],[0]
"To reinforce the capability of the proposed screening rules, we develop a novel framework IAES in Algorithm 2, which
applies the active element screening rules (AES-1 and AES2) and the inactive element screening rules (IES-1 and IES2) in an alternating manner during the optimization process.",3.3. The Proposed IAES Framework by An Alternating Execution of AES and IES,[0],[0]
"Specifically, we integrate our screening rules AES-1, AES-2, IES-1 and IES-2 with the optimization algorithm A for the problems Q-P’ and Q-D’.",3.3. The Proposed IAES Framework by An Alternating Execution of AES and IES,[0],[0]
"During the optimization process, we trigger the screening rules AES-1, AES-2, IES-1 and IES-2 every time when the dual gap is 1− ρ times smaller than itself in the last triggering of IAES.",3.3. The Proposed IAES Framework by An Alternating Execution of AES and IES,[0],[0]
"As the solver A goes on, the volumes of Ω and B would decrease to zeros quickly, IAES can thus identify more and more inactive and active elements.
",3.3. The Proposed IAES Framework by An Alternating Execution of AES and IES,[0],[0]
"Compared with the existing screening methods for convex sparse models, an appealing feature of IAES is that it has no theoretical limit in identifying the inactive and active elements and reducing the problem size.",3.3. The Proposed IAES Framework by An Alternating Execution of AES and IES,[0],[0]
"The reason is that, in convex sparse models, screening models can never rule out the features and samples whose corresponding coefficients in the optimal solution are nonzero.",3.3. The Proposed IAES Framework by An Alternating Execution of AES and IES,[0],[0]
"While in our case, as the optimizer A goes on, our estimation will be accurate enough for us to infer the affiliation of each element with A∗. Hence, we can finally identify all the inactive and active elements and the problem size can be reduced to zero.",3.3. The Proposed IAES Framework by An Alternating Execution of AES and IES,[0],[0]
"This nice feature can lead to significant speedups in the computation time.
",3.3. The Proposed IAES Framework by An Alternating Execution of AES and IES,[0],[0]
Remark 1.,3.3. The Proposed IAES Framework by An Alternating Execution of AES and IES,[0],[0]
The setC in Algorithm 2 is updated by choosing one of the super-level sets of ŵ with the smallest value F̂ (C).,3.3. The Proposed IAES Framework by An Alternating Execution of AES and IES,[0],[0]
It is free to obtain it.,3.3. The Proposed IAES Framework by An Alternating Execution of AES and IES,[0],[0]
"The reason is that most of the existing methods A for the problems Q-P’ and Q-D’ need to calculate f̂(ŵ) in each iteration, in which they need to calculate the value F̂ at all of the super-level sets of ŵ (see the greedy algorithm in (Bach et al., 2013) for details).
",3.3. The Proposed IAES Framework by An Alternating Execution of AES and IES,[0],[0]
Remark 2.,3.3. The Proposed IAES Framework by An Alternating Execution of AES and IES,[0],[0]
"The algorithm A can be all the methods for the problems Q-P’ and Q-D’, such as minimum-norm point algorithm (Wolfe, 1976) and conditional gradient descent (Dunn & Harshbarger, 1978).",3.3. The Proposed IAES Framework by An Alternating Execution of AES and IES,[0],[0]
"Although some algorithms only update s, in IAES, we can update w in each iteration by letting w = −s and refining it by the algorithm named pool adjacent violators (Best & Chakravarti, 1990).
",3.3. The Proposed IAES Framework by An Alternating Execution of AES and IES,[0],[0]
Remark 3.,3.3. The Proposed IAES Framework by An Alternating Execution of AES and IES,[0],[0]
"Due to Lemma 1 and the safety of AES-1, AES-2, IES-1 and IES-2, we can see that IAES would never sacrifice any accuracy.
",3.3. The Proposed IAES Framework by An Alternating Execution of AES and IES,[0],[0]
Remark 4.,3.3. The Proposed IAES Framework by An Alternating Execution of AES and IES,[0],[0]
"Although step 14 in Algorithm 2 may increase the dual gap slightly, it is worthwhile because of the reduced problem size.",3.3. The Proposed IAES Framework by An Alternating Execution of AES and IES,[0],[0]
"This is verified by the speedups gained by IAES in the experiments.
",3.3. The Proposed IAES Framework by An Alternating Execution of AES and IES,[0],[0]
Remark 5.,3.3. The Proposed IAES Framework by An Alternating Execution of AES and IES,[0],[0]
The parameter ρ in Algorithm 2 controls the frequency how often we trigger IAES.,3.3. The Proposed IAES Framework by An Alternating Execution of AES and IES,[0],[0]
"The larger value, the higher frequency to trigger IAES but more computational time consumed by IAES.",3.3. The Proposed IAES Framework by An Alternating Execution of AES and IES,[0],[0]
"In our experiment, we set ρ = 0.5 and it achieves a good performance.
",3.3. The Proposed IAES Framework by An Alternating Execution of AES and IES,[0],[0]
Algorithm 2 Inactive and Active Element Screening 1: Input: an optimization algorithm,3.3. The Proposed IAES Framework by An Alternating Execution of AES and IES,[0],[0]
"A for problems (Q-
P’) and (Q-D’), > 0, 0 <",3.3. The Proposed IAES Framework by An Alternating Execution of AES and IES,[0],[0]
ρ < 1.,3.3. The Proposed IAES Framework by An Alternating Execution of AES and IES,[0],[0]
"2: Initialize: Ê = Ĝ = ∅, C = ∅, g = ∞, choose ŝ ∈ B(F ) and ŵ =",3.3. The Proposed IAES Framework by An Alternating Execution of AES and IES,[0],[0]
"−ŝ. 3: repeat 4: Run A on problems (Q-P’) and (Q-D’) to update ŵ, ŝ and C. 5: if dual gap",3.3. The Proposed IAES Framework by An Alternating Execution of AES and IES,[0],[0]
"G(ŵ, ŝ)",3.3. The Proposed IAES Framework by An Alternating Execution of AES and IES,[0],[0]
"< ρg then 6: Run the active element screening rules AES-1 and AES-2 based on (ŵ, ŝ) and C. 7: Update the active set Ê by (4) and (6).",3.3. The Proposed IAES Framework by An Alternating Execution of AES and IES,[0],[0]
"8: Run the inactive element screening rules IES-1 and IES-2 based on (ŵ, ŝ) and C. 9: Update the inactive set Ĝ by (5) and (7).
10: if V/(Ê ∪ Ĝ) = ∅ then 11: Return: Ê .",3.3. The Proposed IAES Framework by An Alternating Execution of AES and IES,[0],[0]
"12: else 13: Update F̂ , Q-P’, Q-D’ according to Ê and Ĝ. 14: Update ŵ and ŝ by:
ŵ←",3.3. The Proposed IAES Framework by An Alternating Execution of AES and IES,[0],[0]
"[ŵ]V/(Ê∪Ĝ),
ŝ← arg max s∈B(F̂ ) 〈ŵ, s〉.
15: Update g ← G(ŵ, ŝ).",3.3. The Proposed IAES Framework by An Alternating Execution of AES and IES,[0],[0]
"16: end if 17: end if 18: until G(ŵ, ŝ) < .",3.3. The Proposed IAES Framework by An Alternating Execution of AES and IES,[0],[0]
19: Return: Ê ∪ {ŵ > 0}.,3.3. The Proposed IAES Framework by An Alternating Execution of AES and IES,[0],[0]
We evaluate IAES through numerical experiments on both synthetic and real datasets by two measurements.,4. Experiments,[0],[0]
"The first one is the rejection ratios of IAES over iterations: mi+nim∗+n∗ , where mi and ni are the numbers of the active and inactive elements identified by IAES after the i-th iteration, and m∗ and n∗ are the numbers of the active and inactive elements in A∗. We notice that in our experiments m∗ + n∗ = p, so the rejection ratio presents the problem size reduced by IAES.",4. Experiments,[0],[0]
"The second measurement is speedup, i.e., the ratio of the running times of the solver without IAES and with IAES.",4. Experiments,[0],[0]
We set the accuracy tolerance to be 10−6.,4. Experiments,[0],[0]
"Recall that, IAES can be integrated with all the solvers for the problems Q-P and Q-D. In this experiment, we use one of the most widely used algorithms minimum-norm point algorithm (MinNorm) (Wolfe, 1976) as the solver.",4. Experiments,[0],[0]
"The function F (A) varies according to the datasets, whose detailed definitions will be given in the subsequent subsections.
",4. Experiments,[0],[0]
"We write the code in Matlab and perform all the computations on a single core of Intel(R) Core(TM) i7-5930K 3.50GHz, 32GB MEM.",4. Experiments,[0],[0]
We perform experiments on a synthetic dataset named twomoons with different sample sizes (see Figure 2 for an example).,4.1. Experiments on Synthetic Datasets,[0],[0]
All the data points are sampled from two different semicircles.,4.1. Experiments on Synthetic Datasets,[0],[0]
"Specifically, each point can be represented as x = ci",4.1. Experiments on Synthetic Datasets,[0],[0]
+ γ ∗,4.1. Experiments on Synthetic Datasets,[0],[0]
"[cos(θi), sin(θi)], where i = 1, 2 stands for the two semicircles, c1 =",4.1. Experiments on Synthetic Datasets,[0],[0]
"[−0.5, 1],",4.1. Experiments on Synthetic Datasets,[0],[0]
c2 =,4.1. Experiments on Synthetic Datasets,[0],[0]
"[0.5,−1], γ is generated from a normal distribution N(2, 0.52), and θ1 and θ2 are sampled from two uniform distributions [−π2 , π 2 ] and [π2 , 3π 2 ], respectively.",4.1. Experiments on Synthetic Datasets,[0],[0]
We first sample p data points from these two semicircles with equal probability.,4.1. Experiments on Synthetic Datasets,[0],[0]
"Then, we randomly choose p0 = 16 samples and label each of them as positive if it is from the first semicircle and otherwise label it as negative.",4.1. Experiments on Synthetic Datasets,[0],[0]
"We generate five datasets by varying the sample size p in [200, 400, 600, 800, 1000].",4.1. Experiments on Synthetic Datasets,[0],[0]
"We perform semi-supervised clustering on each dataset and the objective function F (A) is defined as:
F (A) = I(fA, fV/A)− ∑ j∈A log ηj − ∑ j∈V/A log(1− ηj),
where I(fA, fV/A) is the mutual information between two Gaussian processes with a Gaussian kernel k(x, y) = exp(−α‖x− y‖2), α = 1.5, and ηj ∈ {0, 1} if j is labeled and otherwise ηj = 12 (see Chapter 6 of (Bach et al., 2013) for more details).",4.1. Experiments on Synthetic Datasets,[0],[0]
"The kernel matrix is dense with the size p× p, leading to a big computational cost when p is large.
",4.1. Experiments on Synthetic Datasets,[0],[0]
Figure 1 displays the rejection ratios of IAES on two-moons.,4.1. Experiments on Synthetic Datasets,[0],[0]
We can see that IAES can find the active and inactive elements incrementally during the optimization process.,4.1. Experiments on Synthetic Datasets,[0],[0]
"It can
finally identify almost all of the elements and reduce the problem size to nearly zero in no more than 400 iterations, which is consistent with our theoretical analysis in Section 3.3.",4.1. Experiments on Synthetic Datasets,[0],[0]
Figure 3 visualizes the screening process of IAES on two-moons when p = 400.,4.1. Experiments on Synthetic Datasets,[0],[0]
"It shows that, during the optimization process, IAES identifies the elements that are easy to be classified first and then identifies the rest.
",4.1. Experiments on Synthetic Datasets,[0],[0]
"Table 1 reports the running time of MinNorm without and with AES (AES-1 + AES-2), IES (IES-1 + IES-2) and IAES for solving the problem SFM on two-moons.",4.1. Experiments on Synthetic Datasets,[0],[0]
We can see that the speedup of IAES can be up to 10 times.,4.1. Experiments on Synthetic Datasets,[0],[0]
"In all the datasets, IAES is significantly faster than MinNorm, MinNorm with AES or IES.",4.1. Experiments on Synthetic Datasets,[0],[0]
"At last, we can see that the time costs of AES, IES and IAES are negligible.",4.1. Experiments on Synthetic Datasets,[0],[0]
"In this experiment, we evaluate the performance of IAES on an image segmentation task.",4.2. Experiments on Real Datasets,[0],[0]
"We use five images (included in the supplemental material) in (Rother et al., 2004) to evaluate IAES.",4.2. Experiments on Real Datasets,[0],[0]
"The objective function F (A) is the sum of the unary potentials for all individual pixels and the pairwise potentials of a 8-neighbor grid graph:
F (A) = u(A) + ∑
i∈A,j∈V/A
d(i, j),
where V presents all the pixels, u ∈ RV is the unary potential derived from the Gaussian Mixture model (Rother et al., 2004), and d(i, j)",4.2. Experiments on Real Datasets,[0],[0]
= exp{−‖xi,4.2. Experiments on Real Datasets,[0],[0]
"− xj‖2} (xi and xj are the values of two pixels) if i, j are neighbors, otherwise d(i, j) = 0.",4.2. Experiments on Real Datasets,[0],[0]
"Table 3 provides the statistics of the resulting image segmentation problems, including the numbers of the pixels and the edges in the 8-neighbor grid graph.
",4.2. Experiments on Real Datasets,[0],[0]
The rejection ratios in Figure 4 show that IAES can identify the active and inactive elements during the optimization process incrementally until all of them are identified.,4.2. Experiments on Real Datasets,[0],[0]
"This implies that IAES can lead to a significant speedup in the time cost.
",4.2. Experiments on Real Datasets,[0],[0]
"Table 2 reports the detailed time cost of MinNorm without and with AES, IES and IAES for solving the image segmentation problems.",4.2. Experiments on Real Datasets,[0],[0]
"We can see that IAES leads to significant speedups, which are up to 30.7 times.",4.2. Experiments on Real Datasets,[0],[0]
"In addition, we notice that the speedup gained by AES is small.",4.2. Experiments on Real Datasets,[0],[0]
"The reason is that AES is used to identify the pixels of the foreground, which is a small region in the image, and thus the problem size cannot be reduced dramatically even if all the active elements are identified.
",4.2. Experiments on Real Datasets,[0],[0]
"At last, from Table 2, we can also see that the speedup we achieve is supper-additive (speedup of AES + speedup of IES < speedup of IAES).",4.2. Experiments on Real Datasets,[0],[0]
"This can usually be expected, which comes from the super linear computational complexity of each iteration in MinNorm, leading to a super-additive saving in the computational cost.",4.2. Experiments on Real Datasets,[0],[0]
We notice that the speedup we achieve on some of the two-moon datasets is not superadditive.,4.2. Experiments on Real Datasets,[0],[0]
"The reason is that we cannot identify a lot of
elements in the early stage (Figure 1).",4.2. Experiments on Real Datasets,[0],[0]
"Thus, the early stage takes up too much time cost.",4.2. Experiments on Real Datasets,[0],[0]
"In this paper, we proposed a novel safe element screening method IAES for SFM to accelerate its optimization process by simultaneously identifying the active and inactive elements.",5. Conclusion,[0],[0]
"Our major contribution is a novel framework for accurately estimating the optimum of the corresponding primal problem of SFM developed by carefully studying the strong convexity of the primal and dual problems, the structure of the base polyhedra, and the optimality conditions of SFM.",5. Conclusion,[0],[0]
"To the best of our knowledge, IAES is the first screening method in the fields of SFM and even combinatorial optimization.",5. Conclusion,[0],[0]
The extensive experimental results demonstrate that IAES can achieve significant speedups.,5. Conclusion,[0],[0]
"Submodular functions are discrete analogs of convex functions, which have applications in various fields, including machine learning and computer vision.",abstractText,[0],[0]
"However, in large-scale applications, solving Submodular Function Minimization (SFM) problems remains challenging.",abstractText,[0],[0]
"In this paper, we make the first attempt to extend the emerging technique named screening in large-scale sparse learning to SFM for accelerating its optimization process.",abstractText,[0],[0]
"We first conduct a careful studying of the relationships between SFM and the corresponding convex proximal problems, as well as the accurate primal optimum estimation of the proximal problems.",abstractText,[0],[0]
"Relying on this study, we subsequently propose a novel safe screening method to quickly identify the elements guaranteed to be included (we refer to them as active) or excluded (inactive) in the final optimal solution of SFM during the optimization process.",abstractText,[0],[0]
"By removing the inactive elements and fixing the active ones, the problem size can be dramatically reduced, leading to great savings in the computational cost without sacrificing any accuracy.",abstractText,[0],[0]
"To the best of our knowledge, the proposed method is the first screening method in the fields of SFM and even combinatorial optimization, thus pointing out a new direction for accelerating SFM algorithms.",abstractText,[0],[0]
Experiment results on both synthetic and real datasets demonstrate the significant speedups gained by our approach.,abstractText,[0],[0]
Safe Element Screening for Submodular Function Minimization,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2805–2811 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
2805",text,[0],[0]
The hashtag #MeToo1 has been prevalent on various social media platforms as a campaign centered around sharing stories of sexual harassment in an act of solidarity with other victims and spreading awareness of a widespread and endemic issue.,1 Introduction,[0],[0]
"With vast amounts of personal stories on the internet, it is important that we make scientific use of this data to push these movements forward and enable real-world change.",1 Introduction,[0],[0]
"Manually sorting and comprehending the information shared in these stories is an arduous task, and the power of natural language processing (NLP) can serve as the missing link between online activism and real change.
",1 Introduction,[0],[0]
"We present several neural NLP models that allow us to automatically classify, aggregate, and analyze vast amounts of harassment data found on social media, becoming an effective tool for
1https://metoomvmt.org
spreading awareness, increasing understanding, and allowing faster action.",1 Introduction,[0],[0]
"This large-scale automatic categorization, summarization, and analysis of personal abuse stories can help activist groups enlighten the public and advocate for social change in a timely manner.
",1 Introduction,[0],[0]
"We present single-label and multi-label classification of diverse forms of sexual harassment present in abuse stories shared online through the forum SafeCity, a crowd-sourcing platform for personal stories of sexual harassment and abuse.",1 Introduction,[0],[0]
"Each story includes one or more tagged forms of sexual harassment, along with a description of the occurrence.",1 Introduction,[0],[0]
"For example, the description “My college was nearby.",1 Introduction,[0],[0]
This happened all the time.,1 Introduction,[0],[0]
"Guys passing comments, staring, trying to touch.",1 Introduction,[0],[0]
"Frustrating” is positive for three classes: commenting, ogling/staring, and touching/groping.
",1 Introduction,[0],[0]
We use CNN-RNN architectures (with character-level CNN embeddings and bidirectional RNNs) to classify the three forms of sexual harassment mentioned above using both singleand multi-label setups.,1 Introduction,[0],[0]
Our models achieve strong performances of 80-86% on these setups.,1 Introduction,[0],[0]
"This automatic classification of different forms of sexual harassment can help victims and authorities to partially automate and speed up the process of filling online sexual violence reporting forms (see Figure 1), which usually requires the victim to detail each form of sexual harassment that took place.",1 Introduction,[0],[0]
"The act of partially filling out the report
(by our classifier) in itself makes it more likely for the victim to file a report.",1 Introduction,[0],[0]
"A study by the Bureau of Justice found that victims who report sexual assault are more likely to seek medical treatment for injuries, which also allows for more immediate prosecution and a better chance of finding DNA evidence to convict the offender (Rennison, 2002).",1 Introduction,[0],[0]
"Further, it can also be used to fulfill the need to automatically categorize and summarize large numbers of online testimonials describing or reporting sexual harassment.
",1 Introduction,[0],[0]
"Next, in order to further utilize these stories as an important tool for harassment understanding and to help prevent similar situations from happening to others, we present interpretability analysis of our neural classification results in the forms of LIME analysis, first-derivative saliency heatmaps, activation clustering, and t-SNE embedding visualization.",1 Introduction,[0],[0]
"We show how these analysis techniques hold promise as avenues for future work and can potentially provide insightful clues towards building (1) a tool to analyze the most common circumstances around each distinct form of harassment to provide more detailed and accurate safety advice, (2) a map of unsafe areas to help others avoid dangerous spaces, and 3) an unofficial sex offender registry that marks frequentlymentioned offenders to warn potential victims.",1 Introduction,[0],[0]
"This paper seeks to provide an avenue to utilize the millions of stories shared on social media describing instances of sexual harassment, including #MeToo, #WhyILeft, and #YesAllWomen.",1 Introduction,[0],[0]
"With this task and analysis, we hope that these stories can be used to prevent future sexual harassment.",1 Introduction,[0],[0]
"Analyzing personal sexual harassment stories from online social forums is fairly unexplored, to the best of our knowledge.",2 Related Work,[0],[0]
"However, recent works in a similar vein include detecting the presence of domestic abuse stories on social media sites (Schrading et al., 2015a; Schrading, 2015; Schrading et al., 2015b).",2 Related Work,[0],[0]
"In more distantly related work, NLP has been used for various sociallydriven tasks, such as detecting the presence of cyberbullying or incivility (Ziegele et al., 2018; Founta et al., 2018; Chen et al., 2012; Zhao et al., 2016; Agrawal and Awekar, 2018; Van Hee et al., 2018), and detecting and providing aid for signs of depression or suicidal thoughts (Pestian et al., 2010; Yazdavar et al., 2017; Stepanov et al., 2017; Fitzpatrick et al., 2017).",2 Related Work,[0],[0]
"For our single-label binary classification task, the two output classes can be [commenting, noncommenting], [ogling, non-ogling], or [groping, non-groping].",3 Classification Models,[0],[0]
"For our multi-label scenario, there are a total of 8 combinations (true or false for three types of sexual harassment), including a label for none of the three classes present in the description.",3 Classification Models,[0],[0]
CNN:,3 Classification Models,[0],[0]
"For each input description, an embedding and convolutional layer are applied.",3 Classification Models,[0],[0]
"This is followed by a max-pooling layer (Collobert et al., 2011).",3 Classification Models,[0],[0]
"Filters of varying window sizes are applied to each window of word vectors, the result of which is then passed through a softmax layer to produce probabilities over the output classes.",3 Classification Models,[0],[0]
"LSTM-RNN: As CNNs are not designed to capture sequential relationships (Pascanu et al., 2014), we adopted an RNN model that consisted of word vectors fed into LSTM layer, the final state of which was fed into a fully-connected layer.",3 Classification Models,[0],[0]
The result is passed through a softmax layer to output the probability over all output classes.,3 Classification Models,[0],[0]
CNN-RNN:,3 Classification Models,[0],[0]
"As both models have strengths and weaknesses, we experimented with a hybrid architecture in which our LSTM-RNN model after the embedding layer is laid on top of our CNN model before the max-pooling (related to Zhou et al. (2015)).",3 Classification Models,[0],[0]
"For single-label models, the final fully-connected layer is fed into a softmax to give final output probabilities.",3 Classification Models,[0],[0]
"Multi-Label Classification We also present multi-label classification (Boutell et al., 2004; Tsoumakas and Katakis, 2006; Katakis et al., 2008), which allows for models to predict multiple categories simultaneously for the same input.",3 Classification Models,[0],[0]
"We further utilized CNN-based character embeddings in addition to word embeddings, and also employed bidirectional RNNs (see Figure 2).",3 Classification Models,[0],[0]
"The
outputs of the final fully-connected layer (F) are fed into a sigmoid function.",3 Classification Models,[0],[0]
"The classification for each category (C) are seen as positive (1) if the output is above threshold t and negative (0) if the output is below threshold t, a hyperparameter, giving the equation: C = 1(σ(F ) ≥ t).",3 Classification Models,[0],[0]
"SafeCity2 is, to the best of our knowledge, the largest publicly-available online forum for reporting sexual harassment.",4.1 Dataset,[0],[0]
Its motto is “pin the creeps”.,4.1 Dataset,[0],[0]
"Victims of sexual harassment share personal stories, with the objective of spreading awareness of ongoing sexual harassment and showcasing location-based trends.",4.1 Dataset,[0],[0]
"The language styles of SafeCity forums are very diverse, and therefore can potentially be used for a variety of test cases, such as emails or tweets.
",4.1 Dataset,[0],[0]
"Each of the 9,892 stories includes a description of the incident, the location, and tagged forms of harassment, with all identifying information removed.",4.1 Dataset,[0],[0]
SafeCity has explicitly given us permission to use this data.,4.1 Dataset,[0],[0]
"The dataset3 contains descriptions of text submitted by forum users, along with tags of 13 forms of sexual harassment.",4.1 Dataset,[0],[0]
"We chose the top three most dense categories—groping/touching, staring/ogling, and commenting—to use as our dataset, as the others were more sparse.",4.1 Dataset,[0],[0]
"Each description may fall into none, some, or all of the categories.",4.1 Dataset,[0],[0]
The single-label models were evaluated using accuracy.,4.2 Evaluation,[0],[0]
The multi-label models were evaluated using exact match ratio and Hamming score (calculated as the complement of Hamming loss).,4.2 Evaluation,[0],[0]
Hamming loss was used as detailed by Tsoumakas and Katakis (2006).,4.2 Evaluation,[0],[0]
"Hamming loss (y) is equal to 1 over |D| (number of multi-label samples), multiplied by the sum of the symmetric differences between the predictions (Z) and the true labels (Y), divided by the number of labels (L), giving
y = 1|D| |D|∑ i=1",4.2 Evaluation,[0],[0]
"|Yi∆Zi| |L| .
2http://safecity.in 3We release our dataset splits at https://github.",4.2 Evaluation,[0],[0]
com/swkarlekar/safecity.,4.2 Evaluation,[0],[0]
Please follow SafeCity guidelines for usage.,4.2 Evaluation,[0],[0]
"All models have vocabulary size of 10, 000, and use AdamOptimizer (Kingma and Ba, 2015) with a learning rate of 1e−4.",4.3 Training Details,[0],[0]
"All gradient norms are clipped to 2.0 (Pascanu et al., 2013; Graves, 2013).",4.3 Training Details,[0],[0]
"For each model, the hyperparameters are tuned using the development set.",4.3 Training Details,[0],[0]
CNN We use a 2-D CNN.,4.3 Training Details,[0],[0]
"Filter sizes of [3, 4, 5] are used with 128 filters per filter size.",4.3 Training Details,[0],[0]
"Batch size is set to 128, and a dropout (Srivastava et al., 2014) of 0.80 is applied.",4.3 Training Details,[0],[0]
LSTM Our LSTM has 2 layers with 60 hidden units.,4.3 Training Details,[0],[0]
Batch size is 64 with a dropout of 0.75.,4.3 Training Details,[0],[0]
CNN-LSTM Our CNN-LSTM model consists of an LSTM on top of a CNN.,4.3 Training Details,[0],[0]
"The CNN has 100 filters per filter size of [3, 4, 5].",4.3 Training Details,[0],[0]
Embedding dimensions of 300 are used.,4.3 Training Details,[0],[0]
An LSTM with 300 hidden units is used.,4.3 Training Details,[0],[0]
"For the character level embeddings, we use an additional CNN with 100 filters per filter size of [3, 4, 5].",4.3 Training Details,[0],[0]
Bidirectional RNNs of 300 units are used.,4.3 Training Details,[0],[0]
"See Table 1 for single-label results on the selected harassment categories, where CNN-RNN was the best performing model compared to several nonneural and neural baselines.",5 Results,[0],[0]
"See Table 2 for multi-label classification results, where the Hamming score for the multi-label CNN-RNN model is 82.5%, showing potential for real-world use as well as substantial future research scope.",5 Results,[0],[0]
We provide various visualization techniques to analyze our models.,6 Analysis,[0],[0]
Each of these techniques employs a different approach and offers new information or supports previous findings.,6 Analysis,[0],[0]
"We selected seed words that corresponded to class labels and found the nearest neighbors of each seed word’s vector by reducing the dimensionality of the word embeddings using t-SNE (see Table 3) (Maaten and Hinton, 2008).",6.1 Word Embedding Visualization,[0],[0]
"This form of visualization not only ensures that our model has learned appropriate word embeddings, but also demonstrates that each form of sexual harassment has a unique and distinct context.",6.1 Word Embedding Visualization,[0],[0]
"Furthermore, this shows that our model learns related words and concepts for each type of harassment.",6.1 Word Embedding Visualization,[0],[0]
"LIME analysis (Ribeiro et al., 2016), or Local Interpretable Model-Agnostic Explanation, interprets the local reasoning of a model around an instance.",6.2 LIME Analysis,[0],[0]
"Results of LIME (ξ) are found by taking the minimum of L, which is the measure of how unfaithful the interpretable model (g) is to approximating the probability that an input (x) belongs to a certain class (f ) in the locally defined area (πx) summed with complexity measures Ω, giving ξ(x) = argmin L(f, g, πx) + Ω(g).",6.2 LIME Analysis,[0],[0]
"In Figure 3 (left), the words “touch”, “man”, and the collective words “indecently till pushed away” are the most important to the local classification of “groping”.",6.2 LIME Analysis,[0],[0]
"Furthermore, the word “metro” has importance in the classification, suggesting that this may be a fre-
quent location in which groping takes place.",6.2 LIME Analysis,[0],[0]
"In Figure 3 (middle), the words with the most importance are “comments” and “staring”, indicating that ogling may coincide with commenting very frequently.",6.2 LIME Analysis,[0],[0]
"In Figure 3 (right), the words “ogling”, “sexual”, and “commenting” had the most importance, which further supports the notion that ogling and commenting often occur together.",6.2 LIME Analysis,[0],[0]
"As verified by the data, ogling and commenting together is more common than ogling alone.",6.2 LIME Analysis,[0],[0]
"Saliency heatmaps (Simonyan et al., 2014; Li et al., 2016) illustrate which words of an input have the biggest impact on the final classification by taking the gradient of the final scores outputted by the neural network (S) with respect to the embedding (E), given the true label (L), giving ∂SL(E)
∂E .",6.3 First Derivative Saliency,[0],[0]
"While LIME analysis and first derivative saliency are both used to find word-level contributions, first derivative saliency is model-dependent and gives reasoning behind classification based on the whole model, in contrast to the locally-faithful, model-agnostic LIME analysis technique.
",6.3 First Derivative Saliency,[0],[0]
"In Figure 4 (left), the word “commenting” and the words “one boy” have the most influence on the classification.",6.3 First Derivative Saliency,[0],[0]
The influence of the word “lighting” indicates poor lighting is often present in situations where sexual harassment takes place.,6.3 First Derivative Saliency,[0],[0]
"In Figure 4 (middle), the classification of “commenting” was most influenced by the word “commenting”, followed by the word “age”.",6.3 First Derivative Saliency,[0],[0]
This suggests the possibility of using descriptors of offenders as a classification tool.,6.3 First Derivative Saliency,[0],[0]
Figure 4 (right) is an incorrectly classified example.,6.3 First Derivative Saliency,[0],[0]
"We see that the word “body”, followed by “language”, had the most influence on the classification of this exam-
ple as “commenting”.",6.3 First Derivative Saliency,[0],[0]
Our model identifies synonyms and hyponyms like the word “language” in relation to the category of commenting.,6.3 First Derivative Saliency,[0],[0]
"However, the true label was “non-commenting”, as the word was not used in a context of sexual language, but rather as “vague language” and “body language”.",6.3 First Derivative Saliency,[0],[0]
"Activation clustering (Girshick et al., 2014; Aubakirova and Bansal, 2016) accesses the activation values of all n neurons and treats the activation values per input as coordinates in ndimensional space.",6.4 Activation Clustering,[0],[0]
K-means clustering was performed to group activation clusters and find common themes in these reports.,6.4 Activation Clustering,[0],[0]
Activation clustering is distinct from both LIME analysis and first derivative saliency in that it finds patterns and clusterings at a description-level.,6.4 Activation Clustering,[0],[0]
Circumstances of Harassment: One of the clusters was classified as “ogling”: {‘a group of boys was standing near us and were making weird expressions and as we moved away they started following’; ‘a group of guys lurking around the theater...’}.,6.4 Activation Clustering,[0],[0]
"Another cluster was classified as “commenting”: {’a group of men were standing who commented on every girl who passed by the’, ’a group of boys were standing there... as we started moving one of them commented on us’}",6.4 Activation Clustering,[0],[0]
"Both of these clusters contained examples describing circumstances of the harassment, following the pattern of “a group of boys/men were standing/lurking and...”",6.4 Activation Clustering,[0],[0]
It can be inferred that certain forms of sexual harassment are more likely to happen with large groups of men.,6.4 Activation Clustering,[0],[0]
"Activation clustering can identify the circumstances of harassment, helping potential victims to be better prepared.",6.4 Activation Clustering,[0],[0]
"Location and Time of Harassment: Some clusters contain examples that point to specific locations of harassment, e.g., a groping cluster: {‘i was in the bus and there was this man who purposely fell on me and touched me inappropriately’; ‘while traveling in a crowded bus most of the time men try to grind their intimate part over my body’; ‘i was in the bus when a man standing tried to put his d**k on my hand’}.",6.4 Activation Clustering,[0],[0]
"Specific locations can also be found: {‘the gurgaon sohna road is very unsafe at night especially if you are alone with no street lights’; ‘kurla station really gets scary at night once i was trying to get a train from kurla station around 10’; ‘mathura highway , not enough lights on the way during nights so is not safe for a individual to journey’}.",6.4 Activation Clustering,[0],[0]
"Notice
that the second cluster examples also contain the word “night”.",6.4 Activation Clustering,[0],[0]
"With data that contains more specific locations or times of day, activation clusters can serve as an automatic way to map out unsafe areas based on location and time of day.",6.4 Activation Clustering,[0],[0]
Identifying Offenders: Examples from another groping cluster include: {‘...her step father abused her physically for a year’; ‘one of the girl of about 6 years got raped by her own father’; ‘it happened at my house my brother harassed me and also misbehaved with me one night its been six months’}.,6.4 Activation Clustering,[0],[0]
This shows that clusters can point to common relationships or titles for offenders.,6.4 Activation Clustering,[0],[0]
This phenomenon can be presumed to happen with names of offenders as well.,6.4 Activation Clustering,[0],[0]
"If many reports have been filed around this offender, clusters will form around his/her name.",6.4 Activation Clustering,[0],[0]
"Instead of a case of “he said, she said”, activation clustering provides an avenue towards “he said, they said”, as clusters form when multiple reports have been filed around the same name.
",6.4 Activation Clustering,[0],[0]
"The main purpose of our visualization techniques is to explain what the black-box deep learning models are learning, such as locations, offenders, or times of day.",6.4 Activation Clustering,[0],[0]
"With more detailed data in the future, we may be able to uncover more nuanced circumstances behind harassment.",6.4 Activation Clustering,[0],[0]
We presented the novel task of identifying various forms of sexual harassment in personal stories.,7 Conclusion,[0],[0]
Our accurate multi-label classification models illustrate the plausibility of automatically filling out incident reports.,7 Conclusion,[0],[0]
"Using visualization techniques, we found circumstances surrounding forms of harassment and the possibility of automatically identifying safe areas and repeat offenders.",7 Conclusion,[0],[0]
"In future work, we hope to experiment with the transferability of our model to other datasets to encompass the diverse mediums through which these personal stories are shared.",7 Conclusion,[0],[0]
"Honoring the courage that these victims demonstrated in sharing their stories online, we use these descriptions not only to help summarize online testimonials and provide more detailed safety advice, but also to help others report similar occurrences to hopefully prevent future sexual harassment from occurring.",7 Conclusion,[0],[0]
We thank the anonymous reviewers for their helpful comments.,Acknowledgments,[0],[0]
"This work was supported by a Google Faculty Research Award, a Bloomberg Data Science Research Grant, an IBM Faculty Award, and NVidia GPU awards.",Acknowledgments,[0],[0]
"With the recent rise of #MeToo, an increasing number of personal stories about sexual harassment and sexual abuse have been shared online.",abstractText,[0],[0]
"In order to push forward the fight against such harassment and abuse, we present the task of automatically categorizing and analyzing various forms of sexual harassment, based on stories shared on the online forum SafeCity.",abstractText,[0],[0]
"For the labels of groping, ogling, and commenting, our single-label CNN-RNN model achieves an accuracy of 86.5%, and our multi-label model achieves a Hamming score of 82.5%.",abstractText,[0],[0]
"Furthermore, we present analysis using LIME, first-derivative saliency heatmaps, activation clustering, and embedding visualization to interpret neural model predictions and demonstrate how this helps extract features that can help automatically fill out incident reports, identify unsafe areas, avoid unsafe practices, and ‘pin the creeps’.",abstractText,[0],[0]
SafeCity: Understanding Diverse Forms of Sexual Harassment Personal Stories,title,[0],[0]
"1Departments of Statistics and Electrical Engineering and Computer Sciences, University of California, Berkeley, Berkeley, USA 2Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, Berkeley, USA. Correspondence to: Aaditya Ramdas <aramdas@eecs.berkeley.edu>, Tijana Zrnic <tijana@eecs.berkeley.edu>, Martin J. Wainwright <wainwrig@eecs.berkeley.edu>, Michael I. Jordan <jordan@eecs.berkeley.edu>.
Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).",text,[0],[0]
It is now commonplace in science and technology to make thousands or even millions of related decisions based on data analysis.,1. Introduction,[0],[0]
"As a simplified example, to discover which genes may be related to diabetes, we can formulate the decision-making problem in terms of hypotheses that take the form “gene X is not associated with diabetes,” for many different genes X, and test for which of these null hypotheses can be confidently rejected by the data.",1. Introduction,[0],[0]
"As first identified by Tukey in a seminal 1953 manuscript (1953), the central difficulty when testing a large number of null hypotheses is that several of them may appear to be false, purely by chance.",1. Introduction,[0],[0]
"Arguably, we would like the set of rejected null hypothesesR to have high precision, so that most discovered genes are indeed truly correlated with diabetes and further investigations are not fruitless.",1. Introduction,[0],[0]
"Unfortunately, separately controlling the false positive rate for each individual test actually does not provide any guarantee on the precision.",1. Introduction,[0],[0]
"This motivated the development of procedures that can provide guarantees on an error metric called the false discovery rate (FDR) (Benjamini & Hochberg, 1995), defined as:
FDR ≡ E",1. Introduction,[0],[0]
[FDP(R)],1. Introduction,[0],[0]
"= E [ |H0 ∩R| |R| ] ,
whereH0 is the unknown set of truly null hypotheses, and 0/0 ≡ 0.",1. Introduction,[0],[0]
"Here the FDP represents the ratio of falsely rejected nulls to the total number of rejected nulls, and since the set of discoveries R is data-dependent, the FDR takes an expectation over the underlying randomness.",1. Introduction,[0],[0]
"The evidence from a hypothesis test can typically be summarized in terms of a p-value, and so offline multiple testing algorithms take a set of p-values {Pi} as their input, and a target FDR level α ∈ (0, 1), and produce a rejected set R that is guaranteed to have FDR ≤ α.",1. Introduction,[0],[0]
"Of course, one also desires a high recall, or equivalently a low false negative rate, but without assumptions on many uncontrollable factors like the frequency and strength of signals, additional guarantees on the recall are impossible.
",1. Introduction,[0],[0]
"While the offline paradigm previously described is the classical setting for multiple decision-making, the corresponding online problem is emerging as a major area of its own.",1. Introduction,[0],[0]
"For example, large information technology companies run thou-
sands of A/B tests every week of the year, and decisions about whether or not to reject the corresponding null hypothesis must be made without knowing the outcomes of future tests; indeed, future null hypotheses may depend on the outcome of the current test.",1. Introduction,[0],[0]
The current standard of setting all thresholds αk to a fixed quantity such as 0.05 does not provide any control of the FDR.,1. Introduction,[0],[0]
"Hence, the following hypothetical scenario is entirely plausible: a company conducts 1000 tests in one week, each with a target false positive rate of 0.05; it happens to make 80 discoveries in total of which 50 are accidental false discoveries, ending up with an FDP of 5/8.",1. Introduction,[0],[0]
"Such uncontrolled error rates can have severe financial and social consequences.
",1. Introduction,[0],[0]
"The first method for online control of the FDR was the alpha-investing algorithm of Foster and Stine (2008), later extended to generalized alpha-investing (GAI) algorithms by Aharoni and Rosset (2014).",1. Introduction,[0],[0]
"Recently, Javanmard and Montanari (2017) proposed variants of GAI algorithms that control the FDR (as opposed to the modified FDR controlled in the original paper (Foster & Stine, 2008)), including a new algorithm called LORD.",1. Introduction,[0],[0]
"The GAI++ algorithms by Ramdas et al. (2017) improved the earlier GAI algorithms (uniformly), and the improved LORD++ (henceforth LORD) method arguably represents the current state-of-the-art in online multiple hypothesis testing.
",1. Introduction,[0],[0]
The current paper’s central contribution is the derivation and analysis of a powerful new class of online FDR algorithms called “SAFFRON” (Serial estimate of the Alpha Fraction that is Futilely Rationed On true Null hypotheses).,1. Introduction,[0],[0]
"As an instance of the GAI framework, the SAFFRON method starts off with an error budget, referred to as alphawealth, that it allocates to different tests over time, earning back some alpha-wealth whenever it makes a new discovery.",1. Introduction,[0],[0]
"However, unlike earlier work in the online setting, SAFFRON is an adaptive method, meaning that it is based on an estimate of the proportion of true nulls.",1. Introduction,[0],[0]
"In the offline setting, adaptive methods were proposed by Storey (2002; 2004), who showed that they are more powerful than the Benjamini-Hochberg (BH) procedure (1995) under independence assumptions; this advantage usually increases with the proportion of non-nulls and the signal strength.",1. Introduction,[0],[0]
"Thus, the SAFFRON method can be viewed as an online analogue of Storey’s adaptive version of the BH procedure.",1. Introduction,[0],[0]
"As shown in Figure 1, our simulations show that SAFFRON demonstrates the same types of advantages over its non-adaptive counterparts, such as LORD and alpha-investing.",1. Introduction,[0],[0]
"Furthermore, the ideas behind SAFFRON’s derivation can provide a natural template for the design and analysis of a suite of other adaptive online methods.
",1. Introduction,[0],[0]
The rest of this paper is organized as follows.,1. Introduction,[0],[0]
"In Section 2, we derive the SAFFRON algorithm from first principles, leaving the precise statement and the proof of a central tech-
nical lemma for Section 3.",1. Introduction,[0],[0]
"In Section 4, we investigate the practical choice of tuning parameters, and demonstrate the effectiveness of our recommended choice using simulations.",1. Introduction,[0],[0]
We end with a summary in Section 5.,1. Introduction,[0],[0]
"Before deriving the SAFFRON algorithm, it is useful to recap a few concepts.",2. Deriving the SAFFRON Algorithm,[0],[0]
"By definition of a p-value, if the hypothesis Hi is truly null, then the corresponding p-value is stochastically larger than the uniform distribution (“superuniformly distributed,” for short), meaning that:
If the null hypothesis Hi is true, then Pr{Pi ≤ u} ≤ u for all u ∈",2. Deriving the SAFFRON Algorithm,[0],[0]
"[0, 1].
(1)
For any online FDR procedure, let the rejected set after t steps be denoted byR(t).",2. Deriving the SAFFRON Algorithm,[0],[0]
"More precisely, this set consists of all p-values among the first t ones for which the indicator for rejection is equal to 1; i.e., Rj : = 1 {Pj ≤ αj} = 1, for all j ≤ t.",2. Deriving the SAFFRON Algorithm,[0],[0]
"While we have already defined the classical FDP and FDR in the introduction, several authors, including Foster and Stine (2008), have considered a modified FDR, defined as:
mFDR(t) :",2. Deriving the SAFFRON Algorithm,[0],[0]
= E [ |H0 ∩R(t)| ],2. Deriving the SAFFRON Algorithm,[0],[0]
E,2. Deriving the SAFFRON Algorithm,[0],[0]
[|R(t)|] .,2. Deriving the SAFFRON Algorithm,[0],[0]
"(2)
In the sequel, we provide guarantees for both mFDR and FDR.",2. Deriving the SAFFRON Algorithm,[0],[0]
Our guarantees on mFDR hold under the following weakening of (1).,2. Deriving the SAFFRON Algorithm,[0],[0]
"Define the filtration formed by the sequence of sigma-fields F t : = σ(R1, . . .",2. Deriving the SAFFRON Algorithm,[0],[0]
", Rt), and let αt : = ft(R1, . . .",2. Deriving the SAFFRON Algorithm,[0],[0]
", Rt−1), where ft is an arbitrary function of the first t−1 indicators for rejection.",2. Deriving the SAFFRON Algorithm,[0],[0]
"Then, we say that the null p-values are conditionally super-uniformly distributed if the following holds:
If the null hypothesis Ht is true, then Pr { Pt ≤",2. Deriving the SAFFRON Algorithm,[0],[0]
αt ∣∣,2. Deriving the SAFFRON Algorithm,[0],[0]
F t−1} ≤ αt. (3),2. Deriving the SAFFRON Algorithm,[0],[0]
"To understand the motivation behind the new procedure, it is necessary to expand on an perspective on existing online FDR procedures, recently suggested by Ramdas et al. (2017).",2.1. An Oracle FDP Estimate and a Naive Overestimate,[0],[0]
"We begin by defining an oracle estimate of the FDP as:
FDP∗(t) :",2.1. An Oracle FDP Estimate and a Naive Overestimate,[0],[0]
"=
∑ j≤t,j∈H0 αj
|R(t)| .
",2.1. An Oracle FDP Estimate and a Naive Overestimate,[0],[0]
"The word oracle indicates that FDP∗ cannot be calculated by the scientist, since H0 is unknown.",2.1. An Oracle FDP Estimate and a Naive Overestimate,[0],[0]
"Intuitively, the numerator ∑ j≤t,j∈H0 αj overestimates the number of false discoveries, and FDP∗(t) overestimates the FDP, as formalized in the claim below:
Proposition 1.",2.1. An Oracle FDP Estimate and a Naive Overestimate,[0],[0]
"If the null p-values are conditionally superuniformly distributed (3), then we have:
(a) E [ ∑ j≤t,j∈H0 αj ] ≥",2.1. An Oracle FDP Estimate and a Naive Overestimate,[0],[0]
"E [ |H0 ∩R(t)| ] ;
(b)",2.1. An Oracle FDP Estimate and a Naive Overestimate,[0],[0]
"If FDP∗(t) ≤ α for all t ∈ N, then mFDR(t) ≤ α for all t ∈ N.
Further, if the null p-values are independent of each other and of the non-nulls, and {αt} is a monotone function of past rejections, then:
(c) E [FDP∗(t)]",2.1. An Oracle FDP Estimate and a Naive Overestimate,[0],[0]
≥ E [FDP(t)] ≡,2.1. An Oracle FDP Estimate and a Naive Overestimate,[0],[0]
"FDR(t) for all t ∈ N;
(d)",2.1. An Oracle FDP Estimate and a Naive Overestimate,[0],[0]
"The condition FDP∗(t) ≤ α for all t ∈ N implies that FDR(t) ≤ α for all t ∈ N.
To clarify, the word monotone means that αt is a coordinatewise non-decreasing function of the vector R1, . . .",2.1. An Oracle FDP Estimate and a Naive Overestimate,[0],[0]
", Rt−1.",2.1. An Oracle FDP Estimate and a Naive Overestimate,[0],[0]
"Proposition 1 follows from the results of Ramdas et al. (2017), and we prove it in Subsection 3.1 for completeness.",2.1. An Oracle FDP Estimate and a Naive Overestimate,[0],[0]
"Even though FDP∗(t) cannot be directly calculated and used, Proposition 1 is a useful way to identify what would be ideally possible.
",2.1. An Oracle FDP Estimate and a Naive Overestimate,[0],[0]
"One natural way to convert FDP∗(t) to a truly empirical overestimate of FDP(t) is to define:
F̂DPLORD(t) : =
∑ j≤t αj
|R(t)| .
",2.1. An Oracle FDP Estimate and a Naive Overestimate,[0],[0]
"Since it is trivially true that F̂DPLORD(t) ≥ FDP∗(t), we immediately obtain that Proposition 1 also holds with FDP∗(t) replaced by F̂DPLORD(t).",2.1. An Oracle FDP Estimate and a Naive Overestimate,[0],[0]
The subscript LORD is used because Ramdas et al. (2017) point out that their variant of the LORD algorithm of Javanmard and Montanari (2017) can be derived by simply assigning αj in an online fashion to ensure that the condition F̂DPLORD(t) ≤ α is met for all times t.,2.1. An Oracle FDP Estimate and a Naive Overestimate,[0],[0]
"The main drawback of F̂DPLORD is that if the underlying (unknown) truth is such that the proportion of non-nulls (true signals) is non-negligible, then F̂DPLORD(t) is a very crude and overly conservative overestimate of FDP∗(t), and hence also of the true unknown FDP.",2.2. A Better Estimate of the Alpha-Wealth Spent on Testing Nulls,[0],[0]
"With this drawback in mind, and knowing that we would expect non-nulls to typically have smaller p-values, we propose the following novel estimator:
F̂DPSAFFRON(λ)(t) ≡ F̂DPλ(t) : = ∑ j≤t αj 1{Pj>λj} 1−λj
|R(t)| ,
where {λj}∞j=1 is a predictable sequence of user-chosen parameters in the interval (0, 1).",2.2. A Better Estimate of the Alpha-Wealth Spent on Testing Nulls,[0],[0]
"Here the term predictable means that λj is a deterministic function of the information available from time 1 to j − 1, which will be formalized later.",2.2. A Better Estimate of the Alpha-Wealth Spent on Testing Nulls,[0],[0]
"For simplicity, when λj is chosen to be a constant for all j, we will drop the subscript and just write λ, and we will consider λ = 1/2 as our default choice.",2.2. A Better Estimate of the Alpha-Wealth Spent on Testing Nulls,[0],[0]
"SAFFRON is based on the idea that the numerator of F̂DPλ is a much better estimator of the quantity ∑ j≤t,j∈H0 αj than LORD’s
naive estimate ∑ j≤t αj .
",2.2. A Better Estimate of the Alpha-Wealth Spent on Testing Nulls,[0],[0]
"So as to provide some intuition for why we expect F̂DPλ to be a fairly tight estimate of FDP∗, note that 1{Pj>λj}1−λj has a unit expectation whenever Pj is uniformly distributed (null), but would typically have a much smaller expectation whenever Pj is stochastically much smaller than uniform (non-null).",2.2. A Better Estimate of the Alpha-Wealth Spent on Testing Nulls,[0],[0]
"The following theorem shows that, even though F̂DPλ(t) is not necessarily always larger than FDP∗(t), a direct analog of Proposition 1 is nonetheless valid.",2.2. A Better Estimate of the Alpha-Wealth Spent on Testing Nulls,[0],[0]
"In order to state this claim formally, we need to slightly modify the assumption (3).",2.2. A Better Estimate of the Alpha-Wealth Spent on Testing Nulls,[0],[0]
"As before, denote by Rj : = 1 {Pj ≤ αj} the indicator for rejection, and let Cj := 1 {Pj ≤ λj} be the indicator for candidacy.",2.2. A Better Estimate of the Alpha-Wealth Spent on Testing Nulls,[0],[0]
"Accordingly, we refer to the p-values for which Cj = 1 as candidates.",2.2. A Better Estimate of the Alpha-Wealth Spent on Testing Nulls,[0],[0]
"Moreover, we let αt : = ft(R1, . . .",2.2. A Better Estimate of the Alpha-Wealth Spent on Testing Nulls,[0],[0]
", Rt−1, C1, . . .",2.2. A Better Estimate of the Alpha-Wealth Spent on Testing Nulls,[0],[0]
", Ct−1), where ft denotes an arbitrary function of the first t − 1 indicators for rejection and candidacy, and define the filtration generated from sigma-fields F t : = σ(R1, . . .",2.2. A Better Estimate of the Alpha-Wealth Spent on Testing Nulls,[0],[0]
", Rt, C1, . . .",2.2. A Better Estimate of the Alpha-Wealth Spent on Testing Nulls,[0],[0]
", Ct).",2.2. A Better Estimate of the Alpha-Wealth Spent on Testing Nulls,[0],[0]
"With respect to this filtration, we introduce a conditional superuniformity condition on the null p-values similar to (3):
If the null hypothesis Ht is true, then Pr { Pt ≤",2.2. A Better Estimate of the Alpha-Wealth Spent on Testing Nulls,[0],[0]
αt ∣∣,2.2. A Better Estimate of the Alpha-Wealth Spent on Testing Nulls,[0],[0]
"F t−1} ≤ αt, (4) which can be rephrased as:
E [ 1 {Pt > αt}
1− αt ························ ∣∣∣∣ F t−1] ≥ 1 ≥ E",2.2. A Better Estimate of the Alpha-Wealth Spent on Testing Nulls,[0],[0]
"[1 {Pt ≤ αt}αt························ ∣∣∣∣ F t−1] .
",2.2. A Better Estimate of the Alpha-Wealth Spent on Testing Nulls,[0],[0]
"Note that again marginal super-uniformity (1) implies this condition, provided that the p-values are independent.
",2.2. A Better Estimate of the Alpha-Wealth Spent on Testing Nulls,[0],[0]
Theorem 1.,2.2. A Better Estimate of the Alpha-Wealth Spent on Testing Nulls,[0],[0]
"If the null p-values are conditionally superuniformly distributed (4), then we have:
(a) E [ ∑ j≤t,j∈H0 αj 1{Pj>λj} 1−λj ]",2.2. A Better Estimate of the Alpha-Wealth Spent on Testing Nulls,[0],[0]
"≥ E [ |H0 ∩R(t)| ] ;
(b)",2.2. A Better Estimate of the Alpha-Wealth Spent on Testing Nulls,[0],[0]
The condition F̂DPλ(t),2.2. A Better Estimate of the Alpha-Wealth Spent on Testing Nulls,[0],[0]
"≤ α for all t ∈ N implies that mFDR(t) ≤ α for all t ∈ N.
Further, if the null p-values are independent of each other and of the non-nulls, and {αt} is a monotone function of the vector R1, ..., Rt−1, C1, ..., Ct−1, then we additionally have:
(c) E [ F̂DPλ(t) ]",2.2. A Better Estimate of the Alpha-Wealth Spent on Testing Nulls,[0],[0]
≥ E [FDP(t)] ≡,2.2. A Better Estimate of the Alpha-Wealth Spent on Testing Nulls,[0],[0]
"FDR(t) for all t ∈ N;
(d)",2.2. A Better Estimate of the Alpha-Wealth Spent on Testing Nulls,[0],[0]
The condition F̂DPλ(t),2.2. A Better Estimate of the Alpha-Wealth Spent on Testing Nulls,[0],[0]
"≤ α for all t ∈ N implies that FDR(t) ≤ α for all t ∈ N.
The proof of this theorem is given in Section 3.2, and is based on a “reverse super-uniformity lemma” that is discussed in the next section.",2.2. A Better Estimate of the Alpha-Wealth Spent on Testing Nulls,[0],[0]
"This lemma, though of a technical nature, may be of independent interest in deriving new algorithms.",2.2. A Better Estimate of the Alpha-Wealth Spent on Testing Nulls,[0],[0]
"The statements on mFDR control allow SAFFRON to be used in place of LORD in applications in which p-values are not independent, but are conditionally super-uniformly distributed, such as the MAB-FDR framework (based on multi-armed bandits) proposed by Yang et al. (2017).",2.2. A Better Estimate of the Alpha-Wealth Spent on Testing Nulls,[0],[0]
We now present the SAFFRON algorithm at a high level.,2.3. The SAFFRON Algorithm for Constant λ,[0],[0]
"For simplicity, we consider the constant λ setting, which performs well in experiments, though it may be a useful direction for future work to construct good heuristics for time-varying sequences {λj}∞j=1.
1.",2.3. The SAFFRON Algorithm for Constant λ,[0],[0]
"Given a target FDR level α, the user first picks a constant λ ∈ (0, 1), an initial wealth W0 < (1− λ)α, and a positive non-increasing sequence {γj}∞j=1 of summing to one.",2.3. The SAFFRON Algorithm for Constant λ,[0],[0]
"For example, given a parameter s > 1, we might pick γj ∝ j−s for some s > 1.
2.",2.3. The SAFFRON Algorithm for Constant λ,[0],[0]
"We use the term “candidates” to refer to p-values smaller than λ, since SAFFRON will never reject a p-value larger than λ.",2.3. The SAFFRON Algorithm for Constant λ,[0],[0]
"Recalling the indicator for candidacy Ct : = 1 {Pt ≤ λ}, and denoting by τj be the time of the j-th rejection (and setting τ0 = 0), define the candidates after the j-th rejection as Cj+ = Cj+(t) = ∑t−1",2.3. The SAFFRON Algorithm for Constant λ,[0],[0]
"i=τj+1 Ci.
3.",2.3. The SAFFRON Algorithm for Constant λ,[0],[0]
"SAFFRON begins by allocation α1 = min{γ1W0, λ},
and then at time t = 2, 3, . .",2.3. The SAFFRON Algorithm for Constant λ,[0],[0]
"., it allocates:
αt : = min{λ, α̃t}, where α̃t : = W0γt−C0++ ((1− λ)α−W0)γt−τ1−C1+ + ∑ j≥2 (1− λ)αγt−τj−Cj+ .
",2.3. The SAFFRON Algorithm for Constant λ,[0],[0]
"In words, SAFFRON starts off with an alpha-wealth W0 <",2.3. The SAFFRON Algorithm for Constant λ,[0],[0]
"(1−λ)α, never loses wealth when testing candidate p-values, gains wealth of (1− λ)α on every rejection except the first.",2.3. The SAFFRON Algorithm for Constant λ,[0],[0]
"If there is a significant fraction of non-nulls, and the signals are fairly strong, then SAFFRON may make more rejections than LORD.
",2.3. The SAFFRON Algorithm for Constant λ,[0],[0]
"To clarify, SAFFRON guarantees FDR control for any λ ∈ (0, 1) and any chosen sequence {γj}∞j=1, but the algorithm’s power, or ability to detect signals, varies as a function of these parameters.",2.3. The SAFFRON Algorithm for Constant λ,[0],[0]
"Given the minimal nature of our assumptions, there is no universally optimal constant or sequence: specifically, we do not make assumptions on the frequency of true signals, or on how strong they are, or on their order, all of which are factors that affect the power.",2.3. The SAFFRON Algorithm for Constant λ,[0],[0]
We discuss reasonable default choices in the experimental section.,2.3. The SAFFRON Algorithm for Constant λ,[0],[0]
"Here, we compare SAFFRON to existing procedures in the literature, emphasizing commonalities that allow us to give a unified view of seemingly disparate algorithms.
",2.4. Relationship to Other Procedures,[0],[0]
Alpha-investing.,2.4. Relationship to Other Procedures,[0],[0]
"Even though the motivation that we have presented for SAFFRON relates it to the LORD algorithm, we find it interesting that the original alpha-investing algorithm of Foster and Stine (2008) is recovered by choosing λj = αj in F̂DPλ, and attempting to ensure that F̂DPλ(t) ≤ α for all times t ∈ N.",2.4. Relationship to Other Procedures,[0],[0]
"In order to see this fact, first note that with this choice of λj , the indicator 1 {Pj > λj} simply indicates when the j-th hypothesis is not rejected.",2.4. Relationship to Other Procedures,[0],[0]
"Consequently, the numerator of F̂DPλ reads as ∑ j≤t αj 1−αj 1 {j /∈ R(t)}.",2.4. Relationship to Other Procedures,[0],[0]
"Hence, ensuring that F̂DPλ(t) ≤ α at all times t ∈ N, is equivalent to ensuring that ∑ j≤t αj 1−αj 1 {j /∈ R(t)} never exceeds α(|R(t)| ∨ 1), which, in the language of alpha-investing, is equivalent to ensuring that the algorithm’s wealth never becomes negative.1 Just as Ramdas et al. (2017) were able to reinterpret and rederive LORD in terms of a particular estimate of the FDP, the current work allows us to reinterpret and rederive alpha-investing in terms of SAFFRON’s FDP.",2.4. Relationship to Other Procedures,[0],[0]
"However, our simulations demonstrate that despite this similarity, SAFFRON with λj = 1/2 is typically a more powerful algorithm than both LORD and alpha-investing.
",2.4. Relationship to Other Procedures,[0],[0]
"1Recall that the alpha-investing algorithm starts off with an alpha-wealth of α, reduces its alpha-wealth by αj
1−αj after tests
that fail to reject, and increase the wealth by α on rejections.
",2.4. Relationship to Other Procedures,[0],[0]
Storey-BH.,2.4. Relationship to Other Procedures,[0],[0]
"In offline multiple testing, where all n pvalues are immediately available, the Benjamini-Hochberg (BH) procedure (1995) is a classical method for guaranteeing FDR control.",2.4. Relationship to Other Procedures,[0],[0]
"BH estimates the FDP of the rejection set R(s) := {i : Pi ≤ s} by F̂DPBH(s) : = n·s|R(s)| , which is a conservative estimate of the oracle FDP∗BH(s) : = |H0|·s |R(s)| (details in Supplementary Material).",2.4. Relationship to Other Procedures,[0],[0]
"For independent pvalues, Storey et al. (2002; 2004) improved the BH method, by picking a constant λ ∈ (0, 1), and calculating:
F̂DPStBH(s) : = n · s · π̂0 |R(s)| ,
where π̂0 is an estimate of the unknown proportion of nulls π0 = |H0|/n computed as:
π̂0 : = 1 + ∑n i=1",2.4. Relationship to Other Procedures,[0],[0]
"1 {Pi > λ} n(1− λ) .
",2.4. Relationship to Other Procedures,[0],[0]
"Then, this procedure, which we refer to as “Storey-BH,” calculates ŝStBH : = max{s : F̂DPStBH(s) ≤ α} and rejects the setR(ŝStBH) which satisfies the bound FDR ≤ α.",2.4. Relationship to Other Procedures,[0],[0]
"Procedures such as Storey-BH are known in the multiple testing literature as adaptive procedures, since they automatically adapt to the unknown proportion of nulls.
",2.4. Relationship to Other Procedures,[0],[0]
"Returning to the setting of online FDR, what matters is not the proportion of nulls π0, but instead a running estimate of the amount of alpha-wealth that was spent testing nulls thus far; this difference arises because, unlike the offline setting where all p-values are compared to the same level ŝ, different p-values have to pass different thresholds αi.",2.4. Relationship to Other Procedures,[0],[0]
"In light of the above discussion, it should be apparent that Storey-BH is to BH as SAFFRON is to LORD.",2.4. Relationship to Other Procedures,[0],[0]
"In the offline context, Storey-BH is called an “adaptive method” (it is adaptive to the unknown null proportion) and in this sense, SAFFRON can be seen as an adaptive online FDR method.
",2.4. Relationship to Other Procedures,[0],[0]
Accumulation tests.,2.4. Relationship to Other Procedures,[0],[0]
Note that E [2I(P > 1/2)],2.4. Relationship to Other Procedures,[0],[0]
"≥ 1 for null p-values (with equality when they are exactly uniformly distributed, simply because ∫ 1 0
2I(p > 1/2)dp",2.4. Relationship to Other Procedures,[0],[0]
= 1).,2.4. Relationship to Other Procedures,[0],[0]
One may actually use any non-decreasing function h such that∫ 1 0 h(p)dp in the formula for F̂DPλ.,2.4. Relationship to Other Procedures,[0],[0]
"Such accumulation functions were studied (Li & Barber, 2017) in the (offline) context of ordered testing, and may seamlessly be transferred to the online setting considered here, yielding mFDR control using the same proof.",2.4. Relationship to Other Procedures,[0],[0]
"In initial experiments, the use of other functions does not seem to yield any advantage, and under some additional assumptions in the offline ordered testing setting, the aforementioned authors argued that the step function (1− λ)−1I(I > λ) is asymptotically optimal for power.",2.4. Relationship to Other Procedures,[0],[0]
"In this light, SAFFRON can also be seen as an online analog of adaptive SeqStep (Lei & Fithian, 2016), which is a variant of selective SeqStep (Barber & Candès, 2015) and SeqStep (Li & Barber, 2017).",2.4. Relationship to Other Procedures,[0],[0]
"In this section, we present a lemma that is central to the proof of FDR control for SAFFRON.",3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
We then use this lemma to prove Proposition 1 and Theorem 1.,3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
Let us first recall and set up some preliminary notation.,3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
"In what follows, αt, λt are random variables in (0, 1) that always satisfy αt ≤",3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
λt.,3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
"We denote the indicator for rejection at the t-th step by Rt : = 1 {Pt ≤ αt}, and recall that since only p-values smaller than λt are candidates for rejection, we had earlier defined the indicator for candidacy as Ct : = 1 {Pt ≤ λt}.",3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
"If we denote C̄t = 1 − Ct, then it is clear that RtC̄t = 0, since Rt = 1 implies C̄t = 0 and C̄t = 1 implies Rt = 0, and it is also possible for Rt and C̄t to both equal 0.",3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
"Also let R1:t : = {R1, . . .",3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
", Rt} and C1:t : = {C1, . . .",3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
", Ct}.",3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
"As before, we consider the filtration F t : = σ(R1:t, C1:t).",3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
"In what follows, we insist that the sequences {αt}∞t=1 and {λt}∞t=1 are predictable, meaning that they are functions of the information available from time 1 to t",3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
"− 1 only; specifically, we insist that αt, λt are measurable with respect to the sigma-field F t−1.",3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
"We will also require that the {αt} sequence is monotone, meaning that αt = ft(R1:t−1, C1:t−1) for some coordinatewise nondecreasing function ft : {0, 1}2(t−1)",3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
→,3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
"[0, λt].",3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
"The proof that SAFFRON as described in Subsection 2.3 satisfies this requirement is given in the Supplementary Material.
",3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
"Recall the definition (4) of conditional super-uniformity, as well as its equivalent rephrased form in the line after definition (4).",3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
"Lemma 1 guarantees that for independent p-values, this statement holds true more generally.
",3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
Lemma 1.,3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
"Assume that the p-values P1, P2, . . .",3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
"are independent and let g : {0, 1}T → R be any coordinatewise non-decreasing function.",3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
"Then, for any index t ≤ T such that Ht ∈ H0, we have:
E [ ft(R1:t−1, C1:t−1)1 {Pt > λt}
(1− λt)g(R1:T ) ∣∣∣∣ F t−1] ≥ E [ ft(R1:t−1, C1:t−1)
g(R1:T ) ∣∣∣∣ F t−1] ≥ E [ 1 {Pt ≤ ft(R1:t−1, C1:t−1)}
g(R1:T )",3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
∣∣∣∣ F t−1] .,3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
Proof.,3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
"The second inequality is a consequence of superuniformity lemmas from past work (Ramdas et al., 2017; Javanmard & Montanari, 2017), so we only prove the first inequality.",3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
"At a high level, the proof strategy is inverted, and we will hallucinate a vector with one element being set to 1, instead of being set to 0 in the aforementioned works.
",3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
"Letting P1:T = (P1, . . .",3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
", PT ) be the original vector of p-values, we define a “hallucinated” vector of p-values P̃ t→11:T : = (P̃1, . . .",3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
", P̃T ) that equals P1:T , except that the
t-th component is set to one:
P̃i",3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
"= { 1 if i = t Pi if i 6= t.
Define hallucinated candidate and rejection indicators as C̃i = 1 { P̃i ≤ λi } and R̃i =
1 { P̃i ≤",3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
"fi(R̃1:i−1, C̃1:i−1) } respectively.",3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
"Let
R1:T = (R1, . . .",3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
", RT ) and R̃t→11:T = (R̃1, . . .",3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
", R̃T ) denote the vector of rejections using P1:T and P̃ t→11:T , respectively.",3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
"Similarly, let C1:T = (C1, . . .",3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
", CT ) and C̃t→11:T = (C̃1, . . .",3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
", C̃T ) denote the vector of candidates using P1:T and P̃ t→11:T , respectively.
",3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
"By construction, we have the following properties:
1.",3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
"R̃i = Ri and C̃i = Ci for all i < t, hence fi(R1:i−1, C1:i−1) =",3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
"fi(R̃1:i−1, C̃1:i−1) for all i ≤ t.
2.",3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
"R̃t = C̃t = 0, and hence R̃i ≤ Ri for all i ≥ t, due to monotonicity of the functions fi.
Hence, on the event {Pt > λt}, we have Rt = R̃t = 0 and Ct = C̃t = 0, and hence also R1:T = R̃t→11:T .",3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
"This allows us to conclude that:
ft(R1:t−1, C1:t−1)1 {Pt > λt} (1− λt)g(R1:T ) =
ft(R1:t−1, C1:t−1)1 {Pt > λt} (1− λt)g(R̃t→11:T ) .
",3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
"Since R̃t→11:T is independent of Pt, we may take conditional expectations to obtain:
E",3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
"[ ft(R1:t−1, C1:t−1)1 {Pt > λt}
(1− λt)g(R1:T ) ∣∣∣∣ F t−1] = E",3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
"[ ft(R1:t−1, C1:t−1)1 {Pt > λt}
(1− λt)g(R̃t→11:T )
∣∣∣∣∣",3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
"F t−1 ]
(i) ≥",3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
"E
[ ft(R1:t−1, C1:t−1)
g(R̃t→11:T )
∣∣∣∣∣",3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
"F t−1 ]
(ii) ≥ E",3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
"[ ft(R1:t−1, C1:t−1)
g(R1:T )",3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
"∣∣∣∣ F t−1] , where inequality (i) follows by taking an expectation only with respect to Pt by invoking the conditional superuniformity property (4); and inequality (ii) follows because g(R1:T ) ≥ g(R̃t→11:T ) since Ri ≥ R̃i for all i by monotonicity of the online FDR rule.",3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
"This concludes the proof of the lemma.
",3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
We now proceed to using the above lemma to prove Proposition 1 and Theorem 1.,3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma,[0],[0]
"Statement (a) is proved by noting that for any time t ∈ N, we have:
E [ |H0 ∩R(t)| ] = ∑ j≤t,j∈H0 E",3.1. Proof of Proposition 1,[0],[0]
"[1 {Pj ≤ αj}]
≤ ∑
j≤t,j∈H0 E",3.1. Proof of Proposition 1,[0],[0]
"[αj ] ,
where the inequality follows after taking iterated expectations by conditioning on F j−1, and then applying the conditional super-uniformity property (3).
",3.1. Proof of Proposition 1,[0],[0]
"If we have FDP∗(t) : = 1|R(t)| ∑
j≤t,j∈H0 αj ≤ α, as assumed
in statement (b), then it follows that:
∑ j≤t,j∈H0 E",3.1. Proof of Proposition 1,[0],[0]
"[αj ] = E  ∑ j≤t,j∈H0 αj  ≤ αE",3.1. Proof of Proposition 1,[0],[0]
"[|R(t)|] ,
using linearity of expectation and the assumption on FDP∗(t).",3.1. Proof of Proposition 1,[0],[0]
"Using part (a) and rearranging yields the inequality mFDR(t) : = E[|H0∩R(t)|]
E[|R(t)|] ≤ α, which concludes the proof of part (b).
",3.1. Proof of Proposition 1,[0],[0]
"If, in addition, the null p-values are independent of each other and of the non-nulls and the sequence {αt} is monotone, we can use the following argument to prove claims (c) and (d).",3.1. Proof of Proposition 1,[0],[0]
These claims establish that the procedure controls the FDR at any time t ∈ N.,3.1. Proof of Proposition 1,[0],[0]
"Still assuming the inequality FDP∗(t) ≤ α, we have:
FDR(t) =",3.1. Proof of Proposition 1,[0],[0]
E [ |H0 ∩R(t)| |R(t)| ],3.1. Proof of Proposition 1,[0],[0]
"=
∑ j≤t,j∈H0 E [ 1 {Pj ≤ αj} |R(t)| ························ ]
≤ ∑ j≤t,j∈H0 E [ αj |R(t)| ············ ] = E [FDP∗(t)] ≤ α,
where the first inequality follows after taking iterated expectations by conditioning on F j−1, and then applying the super-uniformity lemma (Ramdas et al., 2017), the following equality uses linearity of expectation, and the final inequality follows by the assumption on FDP∗(t).",3.1. Proof of Proposition 1,[0],[0]
This concludes the proof of both statements (c) and (d).,3.1. Proof of Proposition 1,[0],[0]
"First note that, for any time t ∈ N, we have: E [ |H0 ∩R(t)| ] = ∑ j≤t,j∈H0 E",3.2. Proof of Theorem 1,[0],[0]
"[1 {Pj ≤ αj}]
(i) ≤ ∑
j≤t,j∈H0 E",3.2. Proof of Theorem 1,[0],[0]
"[αj ]
(ii)
≤",3.2. Proof of Theorem 1,[0],[0]
"E  ∑ j≤t,j∈H0 αj 1 {Pj > λj} 1− λj  , where inequality (i) first uses the law of iterated expectations by conditioning onF j−1, and then both (i) and (ii) apply the conditional super-uniformity property (4), which concludes the proof of part (a).",3.2. Proof of Theorem 1,[0],[0]
"To prove part (b), we drop the condition j ∈ H0 from the last expectation, and use the assumption
that F̂DPλ(t) : = ∑ j≤t αj 1{Pj>λj} 1−λj |R(t)| ≤",3.2. Proof of Theorem 1,[0],[0]
"α to obtain:
E  ∑ j≤t,j∈H0 αj 1 {Pj > λj} 1− λj  ≤ αE",3.2. Proof of Theorem 1,[0],[0]
[|R(t)|] .,3.2. Proof of Theorem 1,[0],[0]
"Combining this inequality with the result of part (a), and rearranging the terms, we reach the conclusion that mFDR(t) ≤ α, as desired.",3.2. Proof of Theorem 1,[0],[0]
"Under the independence and monotonicity assumptions of parts (c, d), we have
FDR(t) = E [ |H0 ∩R(t)| |R(t)| ]",3.2. Proof of Theorem 1,[0],[0]
"=
∑ j≤t,j∈H0 E [ 1 {Pj ≤ αj} |R(t)| ························ ]
(iii) ≤ ∑ j≤t,j∈H0 E [ αj |R(t)| ············ ] (iv)
≤ ∑ j≤t,j∈H0 E",3.2. Proof of Theorem 1,[0],[0]
"[ αj1 {Pj > λj} (1− λj)|R(t)| ····························· ] ,
where inequality (iii) first uses iterated expectations by conditioning on F j−1, and then both (iii) and (iv) apply Lemma 1.",3.2. Proof of Theorem 1,[0],[0]
"Assuming that the inequality F̂DPλ(t) ≤ α holds, it follows that:∑ j≤t,j∈H0 E",3.2. Proof of Theorem 1,[0],[0]
[ αj1 {Pj > λj} (1− λj)|R(t)| ····························· ] (v) ≤,3.2. Proof of Theorem 1,[0],[0]
E,3.2. Proof of Theorem 1,[0],[0]
"[∑ j≤t αj1 {Pj > λj} (1− λj)|R(t)| ········································ ]
(vi) = E [ F̂DPλ(t) ]",3.2. Proof of Theorem 1,[0],[0]
"(vii)
≤ α,
where inequality (v) follows by linearity of expectation and summing over a larger set of indices; equality (vi) simply uses the definition of F̂DPλ(t), and inequality (vii) follows by the assumption, hence proving parts (c,d).",3.2. Proof of Theorem 1,[0],[0]
"In this section, we provide the results of some numerical experiments that compare the performance of SAFFRON with current state-of-the-art algorithms for online FDR control, namely the aforementioned LORD and alpha-investing procedures.2 For each method, we provide empirical evaluations of its power while ensuring that the FDR remains below a chosen value.",4. Numerical Simulations,[0],[0]
"We only run simulations since for real data, we would not know the ground truth and hence which discoveries are true or false.
",4. Numerical Simulations,[0],[0]
"The following two subsections separately analyze two experimental settings - one in which the p-values are computed from Gaussian observations, and another in which the pvalues under the alternative are drawn from a beta distribution.",4. Numerical Simulations,[0],[0]
"In both cases, SAFFRON outperforms the competing algorithms, with the exact level of performance depending on the choice of sequence {γj}.",4. Numerical Simulations,[0],[0]
All experiments use a target FDR of α = 0.05 and estimate the FDR and power by averaging over 200 independent trials.,4. Numerical Simulations,[0],[0]
"As previously mentioned, the constant sequence λj = 1/2 for all j was found to be particularly successful, so this is our default choice, and we drop the index for simplicity.",4. Numerical Simulations,[0],[0]
We use the simple experimental setup of testing the mean of a Gaussian distribution with T = 1000 components.,4.1. Testing with Gaussian Observations,[0],[0]
"More precisely, for each index i ∈ {1, . . .",4.1. Testing with Gaussian Observations,[0],[0]
", T}, the null hypothesis takes the form",4.1. Testing with Gaussian Observations,[0],[0]
Hi : µi = 0.,4.1. Testing with Gaussian Observations,[0],[0]
"The observations consist of independent Gaussian variates Zi ∼ N(µi, 1), which are converted into one-sided p-values using the transform Pi = Φ(−Zi), where Φ is the standard Gaussian CDF.",4.1. Testing with Gaussian Observations,[0],[0]
"The motivation for one-sided conversion lies in A/B testing, where one wishes to detect larger effects, not smaller.",4.1. Testing with Gaussian Observations,[0],[0]
"The parameter µi is chosen according to a mixture model:
µi = { 0 with probability 1− π1 F1 with probability π1,
where the random variable F1 is of the form N(µc, 1) for some constant µc.",4.1. Testing with Gaussian Observations,[0],[0]
"We ran simulations for µc ∈ {2, 3}, thus seeing how changing the distance of the alternative mean to the null mean affects the performance of SAFFRON.
",4.1. Testing with Gaussian Observations,[0],[0]
"In what follows, we compare SAFFRON’s achieved power and FDR to those of LORD and alpha-investing.",4.1. Testing with Gaussian Observations,[0],[0]
"The constant infinite sequence γj ∝ log(j∨2)je√log j , where the proportionality constant is determined so that the sequence sums to one, was shown to be asymptotically optimal for testing Gaussian means via the LORD method in the paper (Javanmard & Montanari, 2017).",4.1. Testing with Gaussian Observations,[0],[0]
"Since SAFFRON loses wealth only when
2The code for all simulations described in this section is available at: https://github.com/tijana-zrnic/SAFFRONcode
testing non-candidates whereas LORD loses wealth at every step, it is expected to behave more conservatively and not use up its wealth at the same rate, conditioned on both using the same sequence {γj}.",4.1. Testing with Gaussian Observations,[0],[0]
"For this reason, informally speaking, it can reuse this leftover wealth, hence the sequence {γj} chosen for SAFFRON is more aggressive, in the sense that more wealth is concentrated around the beginning of the sequence.",4.1. Testing with Gaussian Observations,[0],[0]
"In particular, we choose sequences of the form γj ∝ j−s, where the parameter s > 1 controls the aggressiveness of the procedure; the greater the constant s, the more wealth is concentrated around small values of j. We also consider these sequences for LORD, thus observing the difference in performance resulting from using a more aggressive sequence in the regime of a finite sequence of pvalues.",4.1. Testing with Gaussian Observations,[0],[0]
"Figures showing the power and FDR of SAFFRON and LORD by varying the aggressiveness of sequence {γj} are in the Supplementary Material.
",4.1. Testing with Gaussian Observations,[0],[0]
"In Figure 2 we consider F1 = N(2, 1), and compare the level of performance of alpha-investing, SAFFRON and LORD, the latter two using the highest performing sequence chosen among six possible sequences.",4.1. Testing with Gaussian Observations,[0],[0]
"Figure 1 demonstrates the same comparison for a similar but somewhat easier testing problem, with F1 = N(3, 1).",4.1. Testing with Gaussian Observations,[0],[0]
"Experiments indicate that increasing the fraction of non-null hypotheses allows SAFFRON to achieve a faster increase of power than LORD, thus performing considerably better than both LORD and the alpha-investing procedure in settings with a great number of non-null observations.",4.1. Testing with Gaussian Observations,[0],[0]
"In this setting we generate the p-value sequence according to the following model:
Pi ∼ { Unif[0, 1], with probability 1− π1 Beta(m,n), with probability π1,
where i ∈",4.2. Testing with Beta Alternatives,[0],[0]
"[T ] and T = 1000, as before.",4.2. Testing with Beta Alternatives,[0],[0]
"Again we compare the performance of SAFFRON, alpha-investing and LORD in terms of the achieved power with the FDR controlled under a chosen level.",4.2. Testing with Beta Alternatives,[0],[0]
"For LORD, the asymptotically optimal sequence {γj} was derived in the paper (Javanmard & Montanari, 2017) and is of the form γj ∝",4.2. Testing with Beta Alternatives,[0],[0]
( 1j log j) 1/m for m < 1 and n ≥ 1.,4.2. Testing with Beta Alternatives,[0],[0]
"As in the Gaussian case, for SAFFRON and additionally for LORD we consider the sequence γj ∝ j−s with varying s, which, unlike the aforementioned sequence, does not depend on the parameters of the distribution.",4.2. Testing with Beta Alternatives,[0],[0]
Please refer to the Supplementary Material for plots of achieved power and FDR of SAFFRON and LORD obtained by varying the sequence.,4.2. Testing with Beta Alternatives,[0],[0]
"For the particular distribution of the observed p-values we choose m = 0.5 and n = 5.
Figure 3 compares the performance of SAFFRON, LORD and alpha-investing, the first two using the highest performing sequence {γj} chosen among six considered sequences, as in the setting with Gaussian tests.",4.2. Testing with Beta Alternatives,[0],[0]
"Although simulations show SAFFRON performing similarly to LORD and alphainvesting for small fractions of non-null hypotheses, it significantly outperforms its competitors in terms of power and using up available wealth with a higher number of signals.",4.2. Testing with Beta Alternatives,[0],[0]
"This paper introduces SAFFRON, a new algorithmic framework for online mFDR and FDR control.",5. Summary,[0],[0]
We show empirically that SAFFRON is more powerful than existing algorithms.,5. Summary,[0],[0]
SAFFRON is based on a novel reverse superuniformity lemma that allows us to estimate the fraction of alpha-wealth that an algorithm spends on testing null hypotheses.,5. Summary,[0],[0]
"One may interpret SAFFRON as an adaptive version of LORD, just as Storey-BH is an adaptive version of the Benjamini-Hochberg algorithm.",5. Summary,[0],[0]
"Lastly, the derivation of SAFFRON is rather different from that of earlier generalized alpha-investing (GAI) algorithms, and as such provides a template for the derivation of new algorithms.",5. Summary,[0],[0]
"In the online false discovery rate (FDR) problem, one observes a possibly infinite sequence of pvalues P1, P2, . . .",abstractText,[0],[0]
", each testing a different null hypothesis, and an algorithm must pick a sequence of rejection thresholds α1, α2, . . .",abstractText,[0],[0]
"in an online fashion, effectively rejecting the k-th null hypothesis whenever Pk ≤ αk.",abstractText,[0],[0]
"Importantly, αk must be a function of the past, and cannot depend on Pk or any of the later unseen p-values, and must be chosen to guarantee that for any time t, the FDR up to time t is less than some pre-determined quantity α ∈",abstractText,[0],[0]
"(0, 1).",abstractText,[0],[0]
"In this work, we present a powerful new framework for online FDR control that we refer to as “SAFFRON”.",abstractText,[0],[0]
"Like older alphainvesting algorithms, SAFFRON starts off with an error budget (called alpha-wealth) that it intelligently allocates to different tests over time, earning back some alpha-wealth whenever it makes a new discovery.",abstractText,[0],[0]
"However, unlike older methods, SAFFRON’s threshold sequence is based on a novel estimate of the alpha fraction that it allocates to true null hypotheses.",abstractText,[0],[0]
"In the offline setting, algorithms that employ an estimate of the proportion of true nulls are called “adaptive”, hence SAFFRON can be seen as an online analogue of the offline Storey-BH adaptive procedure.",abstractText,[0],[0]
"Just as Storey-BH is typically more powerful than the Benjamini-Hochberg (BH) procedure under independence, we demonstrate that SAFFRON is also more powerful than its non-adaptive counterparts such as LORD.",abstractText,[0],[0]
"Departments of Statistics and Electrical Engineering and Computer Sciences, University of California, Berkeley, Berkeley, USA Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, Berkeley, USA.",abstractText,[0],[0]
"Correspondence to: Aaditya Ramdas <aramdas@eecs.berkeley.edu>, Tijana Zrnic <tijana@eecs.berkeley.edu>, Martin J. Wainwright <wainwrig@eecs.berkeley.edu>, Michael I. Jordan <jordan@eecs.berkeley.edu>.",abstractText,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",abstractText,[0],[0]
Copyright 2018 by the author(s).,abstractText,[0],[0]
SAFFRON: an Adaptive Algorithm for Online Control of the False Discovery Rate,title,[0],[0]
"Proceedings of the SIGDIAL 2017 Conference, pages 147–157, Saarbrücken, Germany, 15-17 August 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"Task-oriented Spoken Dialogue Systems (SDS) aim to assist users to achieve specific goals via speech, such as hotel booking, restaurant information and accessing bus-schedules.",1 Introduction,[0],[0]
"These systems are typically designed according to a structured ontology (or a database schema), which defines the
domain that the system can talk about.",1 Introduction,[0],[0]
The development of a robust SDS traditionally requires a substantial amount of hand-crafted rules combined with various statistical components.,1 Introduction,[0],[0]
"This includes a spoken language understanding module (Chen et al., 2016; Yang et al., 2017), a dialogue belief state tracker (Henderson et al., 2014; Perez and Liu, 2016; Mrkšić et al., 2017) to predict user intent and track the dialogue history, a dialogue policy (Young et al., 2013; Gašić and Young, 2014; Budzianowski et al., 2017) to determine the dialogue flow, and a natural language generator (Rieser and Lemon, 2009; Wen et al., 2015; Hu et al., 2017) to convert conceptual representations into system responses.
",1 Introduction,[0],[0]
"In a task-oriented SDS, teaching a system how to respond appropriately in all situations is nontrivial.",1 Introduction,[0],[0]
"Traditionally, this dialogue management component has been designed manually using flow charts.",1 Introduction,[0],[0]
"More recently, it has been formulated as a planning problem and solved using reinforcement learning (RL) to optimise a dialogue policy through interaction with users (Levin and Pieraccini, 1997; Roy et al., 2000; Williams and Young, 2007; Jurčı́ček et al., 2011).",1 Introduction,[0],[0]
"In this framework, the system learns by a trial and error process governed by a potentially delayed learning objective called the reward.",1 Introduction,[0],[0]
This reward is designed to encapsulate the desired behavioural features of the dialogue.,1 Introduction,[0],[0]
"Typically it provides a positive reward for success plus a per turn penalty to encourage short dialogues (El Asri et al., 2014; Su et al., 2015a; Vandyke et al., 2015; Su et al., 2016b).
",1 Introduction,[0],[0]
"To allow the system to be trained on-line, Bayesian sample-efficient learning algorithms have been proposed (Gašić and Young, 2014; Daubigney et al., 2014) which can learn policies from a minimal number of dialogues.",1 Introduction,[0],[0]
"However, even with such methods, the initial performance is still relatively poor, and this can impact negatively
147
on the user experience.",1 Introduction,[0],[0]
Supervised learning (SL) can also be used for dialogue action selection.,1 Introduction,[0],[0]
"In this case, the policy is trained to produce an appropriate response for any given dialogue state.",1 Introduction,[0],[0]
"Wizard-of-Oz (WoZ) methods (Kelley, 1984; Dahlbäck et al., 1993) have been widely used for collecting domain-specific training corpora.",1 Introduction,[0],[0]
"Recently an emerging line of research has focused on training neural networkbased dialogue models, mostly in text-based systems (Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2015; Wen et al., 2017; Bordes et al., 2017).",1 Introduction,[0],[0]
These systems are directly trained on past dialogues without detailed specification of the internal dialogue state.,1 Introduction,[0],[0]
"However, there are two key limitations of using SL in SDS.",1 Introduction,[0],[0]
"Firstly, the effect of selecting an action on the future course of the dialogue is not considered and this may result in sub-optimal behaviour.",1 Introduction,[0],[0]
"Secondly, there will often be a large number of dialogue states which are not covered by the training data (Henderson et al., 2008; Li et al., 2014).",1 Introduction,[0],[0]
"Moreover, there is no reason to suppose that the recorded dialogue participants are acting optimally, especially in high noise levels.",1 Introduction,[0],[0]
"These problems are exacerbated in larger domains where multi-step planning is needed.
",1 Introduction,[0],[0]
"In this paper, we propose a network-based approach to policy learning which combines the best of both SL- and RL-based dialogue management, and which capitalises on recent advances in deep RL (Mnih et al., 2015), especially off-policy algorithms (Wang et al., 2017).
",1 Introduction,[0],[0]
"The main contribution of this paper is two-fold:
1. improving the sample-efficiency of actorcritic RL: trust region actor-critic with experience replay (TRACER) and episodic natural actor-critic with experience replay (eNACER).
",1 Introduction,[0],[0]
"2. efficient utilisation of demonstration data for improved early stage policy learning.
",1 Introduction,[0],[0]
The first part focusses primarily on increasing the RL learning speed.,1 Introduction,[0],[0]
"For TRACER, trust regions are introduced to standard actor-critic to control the step size and thereby avoid catastrophic model changes.",1 Introduction,[0],[0]
"For eNACER, the natural gradient identifies steepest ascent direction in policy space to ensure fast convergence.",1 Introduction,[0],[0]
Both models exploit the off-policy learning with experience replay (ER) to improve sample-efficiency.,1 Introduction,[0],[0]
"These are compared with various state-of-the-art RL methods.
",1 Introduction,[0],[0]
The second part aims to mitigate the cold start issue by using demonstration data to pre-train an RL model.,1 Introduction,[0],[0]
"This resembles the training procedure adopted in recent game playing applications (Silver et al., 2016; Hester et al., 2017).",1 Introduction,[0],[0]
"A key feature of this framework is that a single model is trained using both SL and RL with different training objectives but without modifying the architecture.
",1 Introduction,[0],[0]
"By combining the above, we demonstrate a practical approach to learning deep RL-based dialogue policies for new domains which can achieve competitive performance without significant detrimental impact on users.",1 Introduction,[0],[0]
"RL-based approaches to dialogue management have been actively studied for some time (Levin et al., 1998; Lemon et al., 2006; Gašić and Young, 2014).",2 Related Work,[0],[0]
"Initially, systems suffered from slow training, but recent advances in data efficient methods such as Gaussian Processes (GP) have enabled systems to be trained from scratch in on-line interaction with real users (Gašić et al., 2011).",2 Related Work,[0],[0]
GP provides an estimate of the uncertainty in the underlying function and a built-in noise model.,2 Related Work,[0],[0]
"This helps to achieve highly sample-efficient exploration and robustness to recognition/understanding errors.
",2 Related Work,[0],[0]
"However, since the computation in GP scales with the number of points memorised, sparse approximation methods such as the kernel span algorithm (Engel, 2005) must be used and this limits the ability to scale to very large training sets.",2 Related Work,[0],[0]
It is therefore questionable as to whether GP can scale to support commercial wide-domain SDS.,2 Related Work,[0],[0]
"Nevertheless, GP provides a good benchmark and hence it is included in the evaluation below.
",2 Related Work,[0],[0]
"In addition to increasing the sample-efficiency of the learning algorithms, the use of reward shaping has also been investigated in (El Asri et al., 2014; Su et al., 2015b) to enrich the reward function in order to speed up dialogue policy learning.
",2 Related Work,[0],[0]
Combining SL with RL for dialogue modelling is not new.,2 Related Work,[0],[0]
"Henderson et al. (2008) proposed a hybrid SL/RL model that, in order to ensure tractability in policy optimisation, performed exploration only on the states in a dialogue corpus.",2 Related Work,[0],[0]
The policy was then defined manually on parts of the space which were not found in the corpus.,2 Related Work,[0],[0]
"A method of initialising RL models using logistic regression was also described (Rieser and Lemon, 2006).",2 Related Work,[0],[0]
"For GPRL in dialogue, rather than using a linear kernel
that imposes heuristic data pair correlation, a preoptimised Gaussian kernel learned using SL from a dialogue corpus has been proposed (Chen et al., 2015).",2 Related Work,[0],[0]
"The resulting kernel was more accurate on data correlation and achieved better performance, however, the SL corpus did not help to initialise a better policy.",2 Related Work,[0],[0]
"Better initialisation of GPRL has been studied in the context of domain adaptation by specifying a GP prior or re-using an existing model which is then pre-trained for the new domain (Gašić et al., 2013).
",2 Related Work,[0],[0]
"A number of authors have proposed training a standard neural-network policy in two stages (Fatemi et al., 2016; Su et al., 2016a; Williams et al., 2017).",2 Related Work,[0],[0]
Asadi and Williams (2016) also explored off-policy RL methods for dialogue policy learning.,2 Related Work,[0],[0]
"All these studies were conducted in simulation, using error-free text-based input.",2 Related Work,[0],[0]
"A similar approach was also used in a conversational model (Li et al., 2016).",2 Related Work,[0],[0]
"In contrast, our work introduces two new sample-efficient actor-critic methods, combines both two-stage policy learning and off-policy RL, and testing at differing noise levels.",2 Related Work,[0],[0]
The proposed framework addresses the dialogue management component in a modular SDS.,3 Neural Dialogue Management,[0],[0]
The input to the model is the belief state b that encodes a distribution over the possible user intents along with the dialogue history.,3 Neural Dialogue Management,[0],[0]
The model’s role is to select the system action a at every turn that will lead to the maximum possible cumulative reward and a successful dialogue outcome.,3 Neural Dialogue Management,[0],[0]
"The system action is mapped into a system reply at the semantic level, and this is subsequently passed to the natural language generator for output to the user.
",3 Neural Dialogue Management,[0],[0]
"The semantic reply consists of three parts: the intent of the response, (e.g. inform), which slots to talk about (e.g. area), and a value for each slot (e.g. east).",3 Neural Dialogue Management,[0],[0]
"To ensure tractability, the policy selects a from a restricted action set which identifies the intent and sometimes a slot, any remaining information required to complete the reply is extracted using heuristics from the tracked belief state.",3 Neural Dialogue Management,[0],[0]
Dialogue policy optimisation can be seen as the task of learning to select the sequence of responses (actions) at each turn which maximises the longterm objective defined by the reward function.,3.1 Training with Reinforcement Learning,[0],[0]
"This can be solved by applying either value-based
or policy-based methods.",3.1 Training with Reinforcement Learning,[0],[0]
"In both cases, the goal is to find an optimal policy π∗ that maximises the discounted total return R = ∑T−1 t=0 γ
trt(bt, at) over a dialogue with T turns where rt(bt, at) is the reward when taking action at in dialogue belief state bt at turn t and γ is the discount factor.
",3.1 Training with Reinforcement Learning,[0],[0]
The main difference between the two categories is that policy-based methods have stronger convergence characteristics than value-based methods.,3.1 Training with Reinforcement Learning,[0],[0]
"The latter often diverge when using function approximation since they optimise in value space and a slight change in value estimate can lead to a large change in policy space (Sutton et al., 2000).
",3.1 Training with Reinforcement Learning,[0],[0]
"Policy-based methods suffer from low sampleefficiency, high variance and often converge to local optima since they typically learn via Monte Carlo estimation (Williams, 1992; Schulman et al., 2016).",3.1 Training with Reinforcement Learning,[0],[0]
"However, they are preferred due to their superior convergence properties.",3.1 Training with Reinforcement Learning,[0],[0]
Hence in this paper we focus on policy-based methods but also include a value-based method as a baseline.,3.1 Training with Reinforcement Learning,[0],[0]
"In a policy-based method, the training objective is to find a parametrised policy πθ(a|b) that maximises the expected reward J(θ) over all possible dialogue trajectories given a starting state.
",3.1.1 Advantage Actor-Critic (A2C),[0],[0]
"Following the Policy Gradient Theorem (Sutton et al., 2000), the gradient of the parameters given the objective function has the form:
∇θJ(θ) =",3.1.1 Advantage Actor-Critic (A2C),[0],[0]
E,3.1.1 Advantage Actor-Critic (A2C),[0],[0]
"[∇θ log πθ(a|b)Qπθ(b, a)] .",3.1.1 Advantage Actor-Critic (A2C),[0],[0]
"(1) Since this form of gradient has a potentially high variance, a baseline function is typically introduced to reduce the variance whilst not changing the estimated gradient (Williams, 1992; Sutton and Barto, 1999).",3.1.1 Advantage Actor-Critic (A2C),[0],[0]
"A natural candidate for this
baseline is the value function V (b).",3.1.1 Advantage Actor-Critic (A2C),[0],[0]
"Equation 2 then becomes:
∇θJ(θ) =",3.1.1 Advantage Actor-Critic (A2C),[0],[0]
E,3.1.1 Advantage Actor-Critic (A2C),[0],[0]
"[∇θ log πθ(a|b)Aw(b, a)] , (2) where Aw(b, a) = Q(b, a) − V (b) is the advantage function.",3.1.1 Advantage Actor-Critic (A2C),[0],[0]
"This can be viewed as a special case of the actor-critic, where πθ is the actor and Aw(b, a) is the critic, defined by two parameter sets θ and w. To reduce the number of required parameters, temporal difference (TD) errors δw = rt",3.1.1 Advantage Actor-Critic (A2C),[0],[0]
"+ γVw(bt+1)− Vw(bt) can be used to approximate the advantage function (Schulman et al., 2016).",3.1.1 Advantage Actor-Critic (A2C),[0],[0]
The left part in Figure 1 shows the architecture and parameters of the resulting A2C policy.,3.1.1 Advantage Actor-Critic (A2C),[0],[0]
"To boost the performance of A2C policy learning, two methods are introduced:
I. Experience replay with off-policy learning for speed-up
On-policy RL methods update the model with the samples collected via the current policy.",3.1.2 The TRACER Algorithm,[0],[0]
"Sample-efficiency can be improved by utilising experience replay (ER) (Lin, 1992), where minibatches of dialogue experiences are randomly sampled from a replay pool P to train the model.",3.1.2 The TRACER Algorithm,[0],[0]
This increases learning efficiency by re-using past samples in multiple updates whilst ensuring stability by reducing the data correlation.,3.1.2 The TRACER Algorithm,[0],[0]
"Since these past experiences were collected from different policies compared to the current policy, the use of ER leads to off-policy updates.
",3.1.2 The TRACER Algorithm,[0],[0]
"When training models with RL, -greedy action selection is often used to trade-off between exploration and exploitation, whereby a random action is chosen with probability otherwise the top-ranking action is selected.",3.1.2 The TRACER Algorithm,[0],[0]
"A policy used to generate a training dialogues (episodes) is referred to as a behaviour policy µ, in contrast to the policy to be optimised which is called the target policy π.
",3.1.2 The TRACER Algorithm,[0],[0]
The basic A2C training algorithm described in §3.1.1 is on-policy since it is assumed that actions are drawn from the same policy as the target to be optimised (µ = π).,3.1.2 The TRACER Algorithm,[0],[0]
"In off-policy learning, since the current policy π is updated with the samples generated from old behaviour policies µ, an importance sampling (IS) ratio is used to rescale each sampled reward to correct for the sampling bias at time-step t: ρt = π(at|bt)/µ(at|bt) (Meuleau et al., 2000).
",3.1.2 The TRACER Algorithm,[0],[0]
"For A2C, the off-policy gradient for the parametrised value function Vw thus has the form:
∆woff = ∑T−1
t=0
( R̄t − V̂w(bt) )",3.1.2 The TRACER Algorithm,[0],[0]
∇wV̂w(bt),3.1.2 The TRACER Algorithm,[0],[0]
"tΠ i=0 ρi, (3)
where R̄t is the off-policy Monte-Carlo return (Precup et al., 2001):
R̄t = rt + γrt+1 1 Π i=1",3.1.2 The TRACER Algorithm,[0],[0]
ρt+i + · · ·+ γT−t−1rT−1 T−1 Π i=1,3.1.2 The TRACER Algorithm,[0],[0]
ρt+i.,3.1.2 The TRACER Algorithm,[0],[0]
"(4)
Likewise, the updated gradient for policy πθ is:
∆θoff = T−1∑ t=0 ρt∇θ log πθ(at|bt)δ̂w, (5)
where δ̂w = rt + γV̂w(bt+1)− V̂w(bt) is the TD error using the estimated value of V̂w.
",3.1.2 The TRACER Algorithm,[0],[0]
"Also, as the gradient correlates strongly with the sampled reward, reward rt and total return R are normalised to lie in [-1,1] to stabilise training.
",3.1.2 The TRACER Algorithm,[0],[0]
II.,3.1.2 The TRACER Algorithm,[0],[0]
"Trust region constraint for stabilisation
To ensure stability in RL, each per-step policy change is often limited by setting a small learning rate.",3.1.2 The TRACER Algorithm,[0],[0]
"However, setting the rate low enough to avoid occasional large destabilising updates is not conducive to fast learning.
",3.1.2 The TRACER Algorithm,[0],[0]
"Here, we adopt a modified Trust Region Policy Optimisation method introduced by Wang et al. (2017).",3.1.2 The TRACER Algorithm,[0],[0]
"In addition to maximising the cumulative reward J(θ), the optimisation is also subject to a Kullback-Leibler (KL) divergence limit between the updated policy θ and an average policy θa to ensure safety.",3.1.2 The TRACER Algorithm,[0],[0]
"This average policy represents a running average of past policies and constrains the updated policy to not deviate far from the average θa ← αθa + (1− α)θ with a weight α.
",3.1.2 The TRACER Algorithm,[0],[0]
"Thus, given the off-policy policy gradient ∆θoff in Equation 5, the modified policy gradient with trust region g is calculated as follows:
minimize g 1 2 ‖∆θoff",3.1.2 The TRACER Algorithm,[0],[0]
"− g‖22, subject to ∇θDKL",3.1.2 The TRACER Algorithm,[0],[0]
"[πθa(bt)‖πθ(bt)]T g ≤ ξ,
where π is the policy parametrised by θ or θa, and ξ controls the magnitude of the KL constraint.",3.1.2 The TRACER Algorithm,[0],[0]
"Since the constraint is linear, a closed form solution to this quadratic programming problem can
be derived using the KKT conditions.",3.1.2 The TRACER Algorithm,[0],[0]
Setting k = ∇θDKL,3.1.2 The TRACER Algorithm,[0],[0]
"[πθa(bt)‖πθ(bt)], we get:
g∗tr = ∆θ off −max { kT∆θoff − ξ ‖k‖22 , 0 } k. (6)
",3.1.2 The TRACER Algorithm,[0],[0]
"When this constraint is satisfied, there is no change to the gradient with respect to θ.",3.1.2 The TRACER Algorithm,[0],[0]
"Otherwise, the update is scaled down along the direction of k and the policy change rate is lowered.",3.1.2 The TRACER Algorithm,[0],[0]
"This direction is also shown to be closely related to the natural gradient (Amari, 1998; Schulman et al., 2015), which is presented in the next section.
",3.1.2 The TRACER Algorithm,[0],[0]
The above enhancements speed up and stabilise A2C. We call it the Trust Region Actor-Critic with Experience Replay (TRACER) algorithm.,3.1.2 The TRACER Algorithm,[0],[0]
"Vanilla gradient descent algorithms are not guaranteed to update the model parameters in the steepest direction due to re-parametrisation (Amari, 1998; Martens, 2014).",3.1.3 The eNACER Algorithm,[0],[0]
"A widely used solution to this problem is to use a compatible function approximation for the advantage function in Equation 2: ∇wAw(b, a) = ∇θ log πθ(a|b), where the update of w is then in the same update direction as θ (Sutton et al., 2000).",3.1.3 The eNACER Algorithm,[0],[0]
"Equation 2 can then be rewritten as:
∇θJ(θ) =",3.1.3 The eNACER Algorithm,[0],[0]
E,3.1.3 The eNACER Algorithm,[0],[0]
"[∇θ log πθ(a|b)∇θ log πθ(a|b)Tw]
= F (θ) · w,
where F (θ) is the Fisher information matrix.",3.1.3 The eNACER Algorithm,[0],[0]
This implies ∆θNG = w = F (θ)−1∇θJ(θ) and it is called the natural gradient.,3.1.3 The eNACER Algorithm,[0],[0]
"The Fisher Matrix can be viewed as a correction term which makes the natural gradient independent of the parametrisation of the policy and corresponds to steepest ascent towards the objective (Martens, 2014).",3.1.3 The eNACER Algorithm,[0],[0]
"Empirically, the natural gradient has been found to significantly speed up convergence.
",3.1.3 The eNACER Algorithm,[0],[0]
"Based on these ideas, the Natural Actor-Critic (NAC) algorithm was developed by Peters and Schaal (2006).",3.1.3 The eNACER Algorithm,[0],[0]
"In its episodic version (eNAC), the Fisher matrix does not need to be explicitly computed.",3.1.3 The eNACER Algorithm,[0],[0]
"Instead, the gradient is estimated by a least squares method given the n-th episode consisting of a set of transition tuples {(bnt , ant , rnt )}Tn−1t=0 :
Rn = [∑Tn−1 t=0 ∇θ log πθ(ait|bit; θ)T ] ·∆θNG + C, (7)
which can be solved analytically.",3.1.3 The eNACER Algorithm,[0],[0]
"C is a constant which is an estimate of the baseline V (b).
",3.1.3 The eNACER Algorithm,[0],[0]
"As in TRACER, eNAC can be enhanced by using ER and off-policy learning, thus called eNACER, whereby Rn in Equation 7 is replaced by the off-policy Monte-Carlo return R̄n0 at timestep t = 0 as in Equation 4.",3.1.3 The eNACER Algorithm,[0],[0]
"For very large models, the inversion of the Fisher matrix can become prohibitively expensive to compute.",3.1.3 The eNACER Algorithm,[0],[0]
"Instead, a truncated variant can be used to calculate the natural gradient (Schulman et al., 2015).
eNACER is structured as a feed forward network with the output π as in the right of Figure 1, updated with natural gradient ∆θNG.",3.1.3 The eNACER Algorithm,[0],[0]
"Note that by using the compatible function approximation, the value function does not need to be explicitly calculated.",3.1.3 The eNACER Algorithm,[0],[0]
This makes eNACER in practice a policygradient method.,3.1.3 The eNACER Algorithm,[0],[0]
"From the user’s perspective, performing RL from scratch will invariably result in unacceptable performance in the early learning stages.",3.2 Learning from Demonstration Data,[0],[0]
This problem can be mitigated by an off-line corpus of demonstration data to bootstrap a policy.,3.2 Learning from Demonstration Data,[0],[0]
This data may come from a WoZ collection or from interactions between users and an existing policy.,3.2 Learning from Demonstration Data,[0],[0]
"It can be used in three ways: A: Pre-train the model, B: Initialise a supervised replay buffer Psup, and C: a combination of the two.
",3.2 Learning from Demonstration Data,[0],[0]
"(A) For model pre-training, the objective is to ‘mimic’ the response behaviour from the corpus.",3.2 Learning from Demonstration Data,[0],[0]
This phase is essentially standard SL.,3.2 Learning from Demonstration Data,[0],[0]
"The input to the model is the dialogue belief state b, and the training objective for each sample is to minimise a joint cross-entropy loss L(θ) = −∑k yk log(pk) between action labels y and model predictions p, where the policy is parametrised by a set θ.
",3.2 Learning from Demonstration Data,[0],[0]
A policy trained by SL on a fixed dataset may not generalise well.,3.2 Learning from Demonstration Data,[0],[0]
"In spoken dialogues, the noise levels may vary across conditions and thus can significantly affect performance.",3.2 Learning from Demonstration Data,[0],[0]
"Moreover, a policy trained using SL does not perform any long-term planning on the conversation.",3.2 Learning from Demonstration Data,[0],[0]
"Nonetheless, supervised pre-training offers a good model starting point which can then be fine-tuned using RL.
(B) For supervised replay initialisation, the demonstration data is stored in a replay pool Psup which is kept separate from the ER pool used for RL and is never over-written.",3.2 Learning from Demonstration Data,[0],[0]
"At each RL update iteration, a small portion of the demonstration data P ′sup is sampled, and the supervised crossentropy loss L(θ) computed on this data is added
to the RL objective J(θ).",3.2 Learning from Demonstration Data,[0],[0]
"Also, an L2 regularisation loss ‖·‖22 is applied to θ to help prevent it from over-fitting on the sampled demonstration dataset.",3.2 Learning from Demonstration Data,[0],[0]
"The total loss to be minimised is thus:
Lall(θ) = −J(θ)+λ1L(θ;P ′sup)+λ2‖θ‖22, (8)
where λ’s are weights.",3.2 Learning from Demonstration Data,[0],[0]
"In this way, the RL policy is guided by the sampled demonstration data while learning to optimise the total return.
",3.2 Learning from Demonstration Data,[0],[0]
(C),3.2 Learning from Demonstration Data,[0],[0]
The learned parameters of the pre-trained model in method A above might distribute differently from the optimal RL policy and this may cause some performance drop in early stages while learning an RL policy from this model.,3.2 Learning from Demonstration Data,[0],[0]
This can be alleviated by using the composite loss proposed in method B. A comparison between the three options is included in the experimental evaluation.,3.2 Learning from Demonstration Data,[0],[0]
"Our experiments utilised the software tool-kit PyDial (Ultes et al., 2017), which provides a platform for modular SDS.",4 Experimental Results,[0],[0]
The target application is a live telephone-based SDS providing restaurant information for the Cambridge (UK) area.,4 Experimental Results,[0],[0]
The task is to learn a policy which manages the dialogue flow and delivers requested information to the user.,4 Experimental Results,[0],[0]
"The domain consists of approximately 100 venues, each with 6 slots out of which 3 can be used by the system to constrain the search (food-type, area and price-range) and 3 are system-informable properties (phone-number, address and postcode) available once a database entity has been found.
",4 Experimental Results,[0],[0]
The input for all models was the full dialogue belief state b of size 268 which includes the last system act and distributions over the user intention and the three requestable slots.,4 Experimental Results,[0],[0]
The output includes 14 restricted dialogue actions determining the system intent at the semantic level.,4 Experimental Results,[0],[0]
"Combining the dialogue belief states and heuristic rules, it is then mapped into a spoken response using a natural language generator.",4 Experimental Results,[0],[0]
Two value-based methods are shown for comparison with the policy-based models described.,4.1 Model Comparison,[0],[0]
"For both of these, the policy is implicitly determined by the action-value (Q) function which estimates the expected total return when choosing action a given belief state b at time-step t. For an optimal policy π∗, the Q-function satisfies the Bellman
equation (Bellman, 1954):
Q∗(bt, at) =",4.1 Model Comparison,[0],[0]
"Eπ∗{rt + γmaxa′ Q∗(bt+1, a′)|bt, at}.",4.1 Model Comparison,[0],[0]
(9),4.1 Model Comparison,[0],[0]
DQN is a variant of the Q-learning algorithm whereby a neural network is used to non-linearly approximate the Q-function.,4.1.1 Deep Q-Network (DQN),[0],[0]
"This suggests a sequential approximation in Equation 9 by minimising the loss:
L(wt)",4.1.1 Deep Q-Network (DQN),[0],[0]
=,4.1.1 Deep Q-Network (DQN),[0],[0]
E,4.1.1 Deep Q-Network (DQN),[0],[0]
"[ (yt −Q(bt, at;wt))2 ] , (10)
where yt = rt + γmaxa′ Q(bt+1, a′;w−t ) is the target to update the parameters w. Note that yt is evaluated by a target network w− which is updated less frequently than the network w to stabilise learning, and the expectation is over the tuples (bt, at, rt+1,bt+1) sampled from the experience replay pool described in §3.1.2.
",4.1.1 Deep Q-Network (DQN),[0],[0]
DQN often suffers from over-estimation on Qvalues as the max operator is used to select an action as well as to evaluate it.,4.1.1 Deep Q-Network (DQN),[0],[0]
"Double DQN (DDQN) (Van Hasselt et al., 2016) is thus used to de-couple the action selection and Q-value estimation to achieve better performance.",4.1.1 Deep Q-Network (DQN),[0],[0]
GPRL is a state-of-the-art value-based RL algorithm for dialogue modelling.,4.1.2 Gaussian Processes (GP) RL,[0],[0]
It is appealing since it can learn from a small number of observations by exploiting the correlations defined by a kernel function and provides an uncertainty measure of its estimates.,4.1.2 Gaussian Processes (GP) RL,[0],[0]
"In GPRL, the Q-function is modelled as a GP with zero mean and kernel:",4.1.2 Gaussian Processes (GP) RL,[0],[0]
"Q(B,A) ∼ GP(0, (k(b, a), k(b, a)).",4.1.2 Gaussian Processes (GP) RL,[0],[0]
"This Qfunction is then updated by calculating the posterior given the collected belief-action pairs (b, a) (dictionary points) and their corresponding rewards (Gašić and Young, 2014).",4.1.2 Gaussian Processes (GP) RL,[0],[0]
The implicit knowledge of the distance between data points in observation space provided by the kernel greatly speeds up learning since it enables Q-values in as yet unexplored space to be estimated.,4.1.2 Gaussian Processes (GP) RL,[0],[0]
Note that GPRL was used by Fatemi et al. (2016) to compare with deep RL but no uncertainty estimate was used to guide exploration and as a result had relatively poor performance.,4.1.2 Gaussian Processes (GP) RL,[0],[0]
Here GPRL with uncertainty estimate is used as the benchmark.,4.1.2 Gaussian Processes (GP) RL,[0],[0]
"The proposed models were first evaluated under 0% semantic error rate with an agenda-based simulator which generates user interactions at the
semantic-level (Schatzmann et al., 2006).",4.2 Reinforcement Learning from Scratch,[0],[0]
"In this case, the user intent is perfectly captured in the dialogue belief state without noise.
",4.2 Reinforcement Learning from Scratch,[0],[0]
"The total return of each dialogue was set to 1(D)− 0.05× T , where T is the dialogue length and 1(D) is the success indicator for dialogue",4.2 Reinforcement Learning from Scratch,[0],[0]
D. The maximum dialogue length was set to 20 turns and γ was 0.99.,4.2 Reinforcement Learning from Scratch,[0],[0]
"All deep RL models (A2C, TRACER, eNACER and DQN) contained two hidden layers of size 130 and 50.",4.2 Reinforcement Learning from Scratch,[0],[0]
"The Adam optimiser was used (Kingma and Ba, 2014) with an initial learning rate of 0.001.",4.2 Reinforcement Learning from Scratch,[0],[0]
"During training, an -greedy policy was used, which was initially set to 0.3 and annealed to 0.0 over 3500 training dialogues.",4.2 Reinforcement Learning from Scratch,[0],[0]
"For GP, a linear kernel was used.
",4.2 Reinforcement Learning from Scratch,[0],[0]
"The ER pool P size was 1000, and the minibatch size was 64.",4.2 Reinforcement Learning from Scratch,[0],[0]
"Once an initial 192 samples had been collected, the model was updated after every 2 dialogues.",4.2 Reinforcement Learning from Scratch,[0],[0]
"Note that for DQN, each sample was a state transition (bt, at, rt,bt+1), whereas in A2C, TRACER and eNACER, each sample comprised the whole dialogue with all its state transitions.",4.2 Reinforcement Learning from Scratch,[0],[0]
"For eNACER, the natural gradient was computed to update the model weights of size ∼ 42000.",4.2 Reinforcement Learning from Scratch,[0],[0]
"For TRACER, αwas set to 0.02, and ξ was 0.01.",4.2 Reinforcement Learning from Scratch,[0],[0]
"Since the IS ratio has a high variance and can occasionally be extremely large, it was clipped between [0.8,1.0] to maintain stable training.
",4.2 Reinforcement Learning from Scratch,[0],[0]
"Figure 2 shows the success rate learning curves of on-policy A2C, A2C with ER, TRACER, DQN with ER, GP and eNACER.",4.2 Reinforcement Learning from Scratch,[0],[0]
All were tested with 600 dialogues after every 200 training dialogues.,4.2 Reinforcement Learning from Scratch,[0],[0]
"As reported in previous studies, the benchmark
GP model learns quickly and is relatively stable.",4.2 Reinforcement Learning from Scratch,[0],[0]
eNACER provides comparable performance.,4.2 Reinforcement Learning from Scratch,[0],[0]
DQN also showed high sample-efficiency but with high instability at some points.,4.2 Reinforcement Learning from Scratch,[0],[0]
This is because an iterative improvement in value space does not guarantee an improvement in policy space.,4.2 Reinforcement Learning from Scratch,[0],[0]
"Although comparably slower to learn, the difference between on-policy A2C and A2C with ER clearly demonstrates the sample-efficiency of reusing past samples in mini-batches.",4.2 Reinforcement Learning from Scratch,[0],[0]
The enhancements incorporated into the TRACER algorithm do make this form of learning competitive although it still lags behind eNACER and GPRL.,4.2 Reinforcement Learning from Scratch,[0],[0]
"Regardless of the choice of model and learning algorithm, training a policy from scratch on-line will always result in a poor user experience until sufficient interactions have been experienced to allow acceptable behaviours to be learned.
",4.2.1 Learning from Demonstration Data,[0],[0]
"As discussed in §3.2, an off-line corpus of demonstration data can potentially mitigate this problem.",4.2.1 Learning from Demonstration Data,[0],[0]
"To test this, a corpus of 720 real user spoken dialogues in the Cambridge restaurant domain was utilised.",4.2.1 Learning from Demonstration Data,[0],[0]
"The corpus was split in a 4:1:1 ratio for training, validation and testing.",4.2.1 Learning from Demonstration Data,[0],[0]
"It contains interactions between real users recruited via the Amazon Mechanical Turk service and a wellbehaved SDS as described in Su et al. (2016b).
",4.2.1 Learning from Demonstration Data,[0],[0]
"For A2C with ER and TRACER, the three ways of exploiting demonstration data in §3.2 were explored.",4.2.1 Learning from Demonstration Data,[0],[0]
The exploration parameter was also set to 0.3 and annealed to 0.0 over 2000 training dialogues.,4.2.1 Learning from Demonstration Data,[0],[0]
"Since TRACER has similar patterns to A2C with ER, we first explored the impact of demonstration data on the A2C with ER results since it provides more headroom for identifying performance gains.
",4.2.1 Learning from Demonstration Data,[0],[0]
Figure 3a shows the different combinations of demonstration data using A2C with ER in noisefree conditions.,4.2.1 Learning from Demonstration Data,[0],[0]
The supervised pre-trained model (SL model) provides reasonable starting performance.,4.2.1 Learning from Demonstration Data,[0],[0]
The A2C ER model with supervised pretraining (A2C ER+SL model) improves on this after only 400 dialogues whilst suffering initially.,4.2.1 Learning from Demonstration Data,[0],[0]
We hypothesise that the optimised SL pre-trained parameters distributed very differently to the optimal A2C ER parameters.,4.2.1 Learning from Demonstration Data,[0],[0]
"Also, the A2C ER model with SL replay (A2C ER+SL replay) shows clearly how the use of a supervised replay buffer can accelerate learning from scratch.",4.2.1 Learning from Demonstration Data,[0],[0]
"Moreover, when SL pre-training is combined with SL replay
(A2C ER+SL model+replay), it achieved the best result.",4.2.1 Learning from Demonstration Data,[0],[0]
Note that λ1 and λ2 in Equation 8 were 10 and 0.01 respectively.,4.2.1 Learning from Demonstration Data,[0],[0]
"In each policy update, 64 demonstration data were randomly sampled from the supervised replay poolPsup, which is the same number of RL samples selected from ER for A2C learning.",4.2.1 Learning from Demonstration Data,[0],[0]
Similar patterns emerge when utilising demonstration data to improve early learning in the TRACER and eNACER algorithms as shown in Figure 3b.,4.2.1 Learning from Demonstration Data,[0],[0]
"However, in this case, eNACER is less able to exploit demonstration data since the training method is different from standard actorcritics.",4.2.1 Learning from Demonstration Data,[0],[0]
"Hence, the supervised loss L cannot be directly incorporated into the RL objective J as in Equation 8.",4.2.1 Learning from Demonstration Data,[0],[0]
One could optimise the model using L separately after every RL update.,4.2.1 Learning from Demonstration Data,[0],[0]
"However, in our experiments, this did not yield improvement.",4.2.1 Learning from Demonstration Data,[0],[0]
"Hence, only eNACER learning from a pre-trained SL model is reported here.",4.2.1 Learning from Demonstration Data,[0],[0]
"Compared to eNACER learning from scratch, eNACER from SL model started with good performance but learned more slowly.",4.2.1 Learning from Demonstration Data,[0],[0]
"Again, this may be because the optimised SL pre-trained parameters distributed very differently from the optimal eNACER parameters and led to sub-optimality.",4.2.1 Learning from Demonstration Data,[0],[0]
"Overall, these results suggest that the proposed SL+RL framework to exploit demonstration data is effective in mitigating the cold start problem and TRACER provides the best solution in terms of avoiding poor initial performance, rapid learning and competitive fully trained performance.
",4.2.1 Learning from Demonstration Data,[0],[0]
"In addition to the noise-free performance, we also investigated the impact of noise on the TRACER algorithm.",4.2.1 Learning from Demonstration Data,[0],[0]
"Figure 4 shows the results after training on 2000 dialogues via interaction with
the user simulator under different semantic error rates.",4.2.1 Learning from Demonstration Data,[0],[0]
The random policy (white bars) uniformly sampled an action from the set of size 14.,4.2.1 Learning from Demonstration Data,[0],[0]
This can be regarded as the average initial performance of any learning system.,4.2.1 Learning from Demonstration Data,[0],[0]
We can see that SL generates a robust model which can be further finetuned using RL over a wide range of error rates.,4.2.1 Learning from Demonstration Data,[0],[0]
"It should be noted, however, that the drop-off in performance at high noise levels is more rapid than might be expected, comparing to the GPRL.",4.2.1 Learning from Demonstration Data,[0],[0]
We believe that deep architectures are prone to overfitting and in consequence do not handle well the uncertainty of the user behaviour.,4.2.1 Learning from Demonstration Data,[0],[0]
We plan to investigate this issue in future work.,4.2.1 Learning from Demonstration Data,[0],[0]
"Overall, these outcomes validate the benefit of the proposed twophased approach where the system can be effectively pre-trained using corpus data and further be refined via user interactions.",4.2.1 Learning from Demonstration Data,[0],[0]
This paper has presented two compatible approaches to tackling the problem of slow learning and poor initial performance in deep reinforcement learning algorithms.,5 Conclusion,[0],[0]
"Firstly, trust region actor-critic with experience replay (TRACER) and episodic natural actor-critic with experience replay (eNACER) were presented, these have been shown to be more sample-efficient than other deep RL models and broadly competitive with GPRL.",5 Conclusion,[0],[0]
"Secondly, it has been shown that demonstration data can be utilised to mitigate poor performance in the early stages of learning.",5 Conclusion,[0],[0]
"To this end, two methods for using off-line corpus data were presented: simple pre-training using SL, and using the corpus data in a replay buffer.",5 Conclusion,[0],[0]
"These were particularly effective when used with TRACER which provided the best overall performance.
",5 Conclusion,[0],[0]
"Experimental results were also presented for mismatched environments, again TRACER demonstrated the ability to avoid poor initial performance when trained only on the demonstration corpus, yet still improve substantially with subsequent reinforcement learning.",5 Conclusion,[0],[0]
"It was noted, however, that performance still falls off rather rapidly in noise compared to GPRL as the uncertainty estimates are not handled well by neural networks architectures.
",5 Conclusion,[0],[0]
"Finally, it should be emphasised that whilst this paper has focused on the early stages of learning a new domain where GPRL provides a benchmark and is hard to beat, the potential of deep RL is its readily scalability to exploit on-line learning with large user populations as the model size is not related with experience replay buffer.",5 Conclusion,[0],[0]
"Pei-Hao Su is supported by Cambridge Trust and the Ministry of Education, Taiwan.",Acknowledgments,[0],[0]
"Paweł Budzianowski is supported by EPSRC Council and Toshiba Research Europe Ltd, Cambridge Research Laboratory.",Acknowledgments,[0],[0]
The authors would like to thank the other members of the Cambridge Dialogue Systems Group for their valuable comments.,Acknowledgments,[0],[0]
Deep reinforcement learning (RL) methods have significant potential for dialogue policy optimisation.,abstractText,[0],[0]
"However, they suffer from a poor performance in the early stages of learning.",abstractText,[0],[0]
This is especially problematic for on-line learning with real users.,abstractText,[0],[0]
Two approaches are introduced to tackle this problem.,abstractText,[0],[0]
"Firstly, to speed up the learning process, two sampleefficient neural networks algorithms: trust region actor-critic with experience replay (TRACER) and episodic natural actorcritic with experience replay (eNACER) are presented.",abstractText,[0],[0]
"For TRACER, the trust region helps to control the learning step size and avoid catastrophic model changes.",abstractText,[0],[0]
"For eNACER, the natural gradient identifies the steepest ascent direction in policy space to speed up the convergence.",abstractText,[0],[0]
Both models employ off-policy learning with experience replay to improve sampleefficiency.,abstractText,[0],[0]
"Secondly, to mitigate the cold start issue, a corpus of demonstration data is utilised to pre-train the models prior to on-line reinforcement learning.",abstractText,[0],[0]
"Combining these two approaches, we demonstrate a practical approach to learning deep RLbased dialogue policies and demonstrate their effectiveness in a task-oriented information seeking domain.",abstractText,[0],[0]
Sample-efficient Actor-Critic Reinforcement Learning with Supervised Data for Dialogue Management,title,[0],[0]
"We are interested in solving a problem of the form
min w∈Rd  P (w) def=",1. Introduction,[0],[0]
"1n ∑ i∈[n] fi(w)  , (1)",1. Introduction,[0],[0]
"where each fi, i ∈",1. Introduction,[0],[0]
"[n] def = {1, . . .",1. Introduction,[0],[0]
", n}, is convex with a Lipschitz continuous gradient.",1. Introduction,[0],[0]
"Throughout the paper, we assume that there exists an optimal solution w∗ of (1).
1Department of Industrial and Systems Engineering, Lehigh University, USA.",1. Introduction,[0],[0]
"2On leave at The University of Oxford, UK.",1. Introduction,[0],[0]
All authors were supported by NSF Grant CCF-1618717.,1. Introduction,[0],[0]
"Katya Scheinberg was partially supported by NSF Grants DMS 13-19356, CCF-1320137 and CCF1618717.",1. Introduction,[0],[0]
"Correspondence to: Lam M. Nguyen <lamnguyen.mltd@gmail.com>, Jie Liu <jie.liu.2018@gmail.com>, Katya Scheinberg <katyas@lehigh.edu>, Martin Takáč <Takac.MT@gmail.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"Problems of this type arise frequently in supervised learning applications (Hastie et al., 2009).",1. Introduction,[0],[0]
"Given a training set {(xi, yi)}ni=1 with xi ∈ R
d, yi ∈ R, the least squares regression model, for example, is written as (1) with fi(w) def = (xTi w−yi)2+ λ2 ‖w‖ 2, where ‖·‖ denotes the `2-norm.",1. Introduction,[0],[0]
The `2-regularized logistic regression for binary classification is written with fi(w) def = log(1 + exp(−yixTi w)),1. Introduction,[0],[0]
+,1. Introduction,[0],[0]
"λ2 ‖w‖ 2 (yi ∈ {−1, 1}).
",1. Introduction,[0],[0]
"In recent years, many advanced optimization methods have been developed for problem (1).",1. Introduction,[0],[0]
"While the objective function is smooth and convex, the traditional optimization methods, such as gradient descent (GD) or Newton method are often impractical for this problem, when n – the number of training samples and hence the number of fi’s – is very large.",1. Introduction,[0],[0]
"In particular, GD updates iterates as follows
wt+1 = wt − ηt∇P (wt), t = 0, 1, 2, . . .",1. Introduction,[0],[0]
".
",1. Introduction,[0],[0]
"Under strong convexity assumption on P and with appropriate choice of ηt, GD converges at a linear rate in terms of objective function values P (wt).",1. Introduction,[0],[0]
"However, when n is large, computing ∇P (wt) at each iteration can be prohibitive.
",1. Introduction,[0],[0]
"As an alternative, stochastic gradient descent (SGD)1, originating from the seminal work of Robbins and Monro in 1951 (Robbins & Monro, 1951), has become the method of choice for solving (1).",1. Introduction,[0],[0]
"At each step, SGD picks an index i ∈",1. Introduction,[0],[0]
"[n] uniformly at random, and updates the iterate as wt+1 =",1. Introduction,[0],[0]
"wt − ηt∇fi(wt), which is up-to n times cheaper than an iteration of a full gradient method.",1. Introduction,[0],[0]
"The convergence rate of SGD is slower than that of GD, in particular, it is sublinear in the strongly convex case.",1. Introduction,[0],[0]
"The tradeoff, however, is advantageous due to the tremendous per-iteration savings and the fact that low accuracy solutions are sufficient.",1. Introduction,[0],[0]
"This trade-off has been thoroughly analyzed in (Bottou, 1998).",1. Introduction,[0],[0]
"Unfortunately, in practice SGD method is often too slow and its performance is too sensitive to the variance in the sample gradients∇fi(wt).",1. Introduction,[0],[0]
"Use of mini-batches (averaging multiple sample gradients ∇fi(wt)) was used in (Shalev-Shwartz et al., 2007; Cotter et al., 2011; Takáč
1We mark here that even though stochastic gradient is referred to as SG in literature, the term stochastic gradient descent (SGD) has been widely used in many important works of large-scale learning, including SAG/SAGA, SDCA, SVRG and MISO.
",1. Introduction,[0],[0]
"Method Complexity GD O (n/ )
",1. Introduction,[0],[0]
SGD O ( 1/ 2 ),1. Introduction,[0],[0]
"SVRG O (n+ ( √ n/ ))
",1. Introduction,[0],[0]
SAGA O (n+ (n/ )),1. Introduction,[0],[0]
SARAH O ((n+ (1/ )),1. Introduction,[0],[0]
"log(1/ ))
",1. Introduction,[0],[0]
SARAH (one outer loop),1. Introduction,[0],[0]
"O
( n+ (1/ 2) )",1. Introduction,[0],[0]
"et al., 2013) to reduce the variance and improve convergence rate by constant factors.",1. Introduction,[0],[0]
"Using diminishing sequence {ηt} is used to control the variance (Shalev-Shwartz et al., 2011; Bottou et al., 2016), but the practical convergence of SGD is known to be very sensitive to the choice of this sequence, which needs to be hand-picked.
",1. Introduction,[0],[0]
"Recently, a class of more sophisticated algorithms have emerged, which use the specific finite-sum form of (1) and combine some deterministic and stochastic aspects to reduce variance of the steps.",1. Introduction,[0],[0]
"The examples of these methods are SAG/SAGA (Le Roux et al., 2012; Defazio et al., 2014), SDCA (Shalev-Shwartz & Zhang, 2013), SVRG (Johnson & Zhang, 2013; Xiao & Zhang, 2014), DIAG (Mokhtari et al., 2017), MISO (Mairal, 2013) and S2GD (Konečný & Richtárik, 2013), all of which enjoy faster convergence rate than that of SGD and use a fixed learning rate parameter η.",1. Introduction,[0],[0]
"In this paper we introduce a new method in this category, SARAH, which further improves several aspects of the existing methods.",1. Introduction,[0],[0]
In Table 1 we summarize complexity and some other properties of the existing methods and SARAH when applied to strongly convex problems.,1. Introduction,[0],[0]
"Although SVRG and SARAH have the same convergence rate, we introduce a practical variant of SARAH that outperforms SVRG in our experiments.
",1. Introduction,[0],[0]
"In addition, theoretical results for complexity of the methods or their variants when applied to general convex functions have been derived (Schmidt et al., 2016; Defazio et al., 2014; Reddi et al., 2016; Allen-Zhu & Yuan, 2016; Allen-Zhu, 2017).",1. Introduction,[0],[0]
"In Table 2 we summarize the key complexity results, noting that convergence rate is now sublinear.
",1. Introduction,[0],[0]
Our Contributions.,1. Introduction,[0],[0]
"In this paper, we propose a novel algorithm which combines some of the good properties of existing algorithms, such as SAGA and SVRG, while aiming to improve on both of these methods.",1. Introduction,[0],[0]
"In particular, our algorithm does not take steps along a stochastic gradient direction, but rather along an accumulated direction using past stochastic gradient information (as in SAGA) and occasional exact gradient information (as in SVRG).",1. Introduction,[0],[0]
"We summarize the key properties of the proposed algorithm below.
",1. Introduction,[0],[0]
"• Similarly to SVRG, SARAH’s iterations are divided into the outer loop where a full gradient is computed and the inner loop where only stochastic gradient is computed.",1. Introduction,[0],[0]
"Unlike the case of SVRG, the steps of the inner loop of SARAH are based on accumulated stochastic information.",1. Introduction,[0],[0]
"• Like SAG/SAGA and SVRG, SARAH has a sublinear rate of convergence for general convex functions, and a linear rate of convergence for strongly convex functions.",1. Introduction,[0],[0]
•,1. Introduction,[0],[0]
"SARAH uses a constant learning rate, whose size is larger than that of SVRG.",1. Introduction,[0],[0]
We analyze and discuss the optimal choice of the learning rate and the number of inner loop steps.,1. Introduction,[0],[0]
"However, unlike SAG/SAGA but similar to SVRG, SARAH does not require a storage of n past stochastic gradients.",1. Introduction,[0],[0]
"• We also prove a linear convergence rate (in the strongly convex case) for the inner loop of SARAH, the property that SVRG does not possess.",1. Introduction,[0],[0]
"We show that the variance of the steps inside the inner loop goes to zero, thus SARAH is theoretically more stable and reliable than SVRG.",1. Introduction,[0],[0]
"• We provide a practical variant of SARAH based on the convergence properties of the inner loop, where the simple stable stopping criterion for the inner loop is used (see Section 4 for more details).",1. Introduction,[0],[0]
This variant shows how SARAH can be made more stable than SVRG in practice.,1. Introduction,[0],[0]
"Now we are ready to present our SARAH (Algorithm 1).
",2. Stochastic Recursive Gradient Algorithm,[0],[0]
"The key step of the algorithm is a recursive update of the stochastic gradient estimate (SARAH update)
vt = ∇fit(wt)−∇fit(wt−1) +",2. Stochastic Recursive Gradient Algorithm,[0],[0]
"vt−1, (2)
followed by the iterate update:
wt+1 =",2. Stochastic Recursive Gradient Algorithm,[0],[0]
wt − ηvt.,2. Stochastic Recursive Gradient Algorithm,[0],[0]
"(3)
For comparison, SVRG update can be written in a similar way as
vt = ∇fit(wt)−∇fit(w0) + v0.",2. Stochastic Recursive Gradient Algorithm,[0],[0]
"(4)
Algorithm 1 SARAH Parameters: the learning rate η > 0 and the inner loop size m. Initialize: w̃0 Iterate: for s = 1, 2, . .",2. Stochastic Recursive Gradient Algorithm,[0],[0]
.,2. Stochastic Recursive Gradient Algorithm,[0],[0]
do w0 = w̃s−1 v0 = 1 n,2. Stochastic Recursive Gradient Algorithm,[0],[0]
"∑n i=1∇fi(w0)
",2. Stochastic Recursive Gradient Algorithm,[0],[0]
w1 = w0,2. Stochastic Recursive Gradient Algorithm,[0],[0]
"− ηv0 Iterate: for t = 1, . . .",2. Stochastic Recursive Gradient Algorithm,[0],[0]
",m− 1 do
Sample it uniformly at random from [n] vt = ∇fit(wt)−∇fit(wt−1) +",2. Stochastic Recursive Gradient Algorithm,[0],[0]
vt−1 wt+1 =,2. Stochastic Recursive Gradient Algorithm,[0],[0]
wt,2. Stochastic Recursive Gradient Algorithm,[0],[0]
"− ηvt
end for Set w̃s = wt with t chosen uniformly at random from {0, 1, . . .",2. Stochastic Recursive Gradient Algorithm,[0],[0]
",m}
end for
Observe that in SVRG, vt is an unbiased estimator of the gradient, while it is not true for SARAH.",2. Stochastic Recursive Gradient Algorithm,[0],[0]
"Specifically, 2
E[vt|Ft] = ∇P (wt)−∇P (wt−1)+vt−1 6= ∇P (wt), (5)
where 3 Ft = σ(w0, i1, i2, . . .",2. Stochastic Recursive Gradient Algorithm,[0],[0]
", it−1) is the σ-algebra generated by w0, i1, i2, . . .",2. Stochastic Recursive Gradient Algorithm,[0],[0]
", it−1; F0 = F1 = σ(w0).",2. Stochastic Recursive Gradient Algorithm,[0],[0]
"Hence, SARAH is different from SGD and SVRG type of methods, however, the following total expectation holds, E[vt] = E[∇P (wt)], differentiating SARAH from SAG/SAGA.
",2. Stochastic Recursive Gradient Algorithm,[0],[0]
SARAH is similar to SVRG since they both contain outer loops which require one full gradient evaluation per outer iteration followed by one full gradient descent step with a given learning rate.,2. Stochastic Recursive Gradient Algorithm,[0],[0]
"The difference lies in the inner loop, where SARAH updates the stochastic step direction vt recursively by adding and subtracting component gradients to and from the previous vt−1 (t ≥ 1) in (2).",2. Stochastic Recursive Gradient Algorithm,[0],[0]
Each inner iteration evaluates 2 stochastic gradients and hence the total work per outer iteration isO(n+m) in terms of the number of gradient evaluations.,2. Stochastic Recursive Gradient Algorithm,[0],[0]
"Note that due to its nature, without running the inner loop, i.e., m = 1, SARAH reduces to the GD algorithm.",2. Stochastic Recursive Gradient Algorithm,[0],[0]
"To proceed with the analysis of the proposed algorithm, we will make the following common assumptions.
",3. Theoretical Analysis,[0],[0]
Assumption 1 (L-smooth).,3. Theoretical Analysis,[0],[0]
"Each fi : Rd → R, i ∈",3. Theoretical Analysis,[0],[0]
"[n], is L-smooth, i.e., there exists a constant L > 0",3. Theoretical Analysis,[0],[0]
"such that
‖∇fi(w)−∇fi(w′)‖ ≤ L‖w",3. Theoretical Analysis,[0],[0]
"− w′‖, ∀w,w′ ∈",3. Theoretical Analysis,[0],[0]
Rd.,3. Theoretical Analysis,[0],[0]
2 E[·|Ft] = Eit,3. Theoretical Analysis,[0],[0]
"[·], which is expectation with respect to the random choice of index it (conditioned on w0, i1, i2, . . .",3. Theoretical Analysis,[0],[0]
", it−1).",3. Theoretical Analysis,[0],[0]
"3Ft also contains all the information of w0, . . .",3. Theoretical Analysis,[0],[0]
", wt as well as v0, . . .",3. Theoretical Analysis,[0],[0]
", vt−1.
Note that this assumption implies that P (w) = 1 n",3. Theoretical Analysis,[0],[0]
∑n i=1 fi(w) is also L-smooth.,3. Theoretical Analysis,[0],[0]
"The following strong convexity assumption will be made for the appropriate parts of the analysis, otherwise, it would be dropped.
",3. Theoretical Analysis,[0],[0]
Assumption 2a (µ-strongly convex).,3. Theoretical Analysis,[0],[0]
"The function P : Rd → R, is µ-strongly convex, i.e., there exists a constant µ > 0",3. Theoretical Analysis,[0],[0]
"such that ∀w,w′ ∈ Rd,
P (w) ≥ P (w′) +∇P (w′)T (w − w′) + µ2 ‖w − w ′‖2.
Another, stronger, assumption of µ-strong convexity for (1) will also be imposed when required in our analysis.",3. Theoretical Analysis,[0],[0]
"Note that Assumption 2b implies Assumption 2a but not vice versa.
",3. Theoretical Analysis,[0],[0]
Assumption 2b.,3. Theoretical Analysis,[0],[0]
"Each function fi : Rd → R, i ∈",3. Theoretical Analysis,[0],[0]
"[n], is strongly convex with µ > 0.
",3. Theoretical Analysis,[0],[0]
"Under Assumption 2a, let us define the (unique) optimal solution of (1) as w∗, Then strong convexity of P implies that
2µ[P (w)− P (w∗)] ≤",3. Theoretical Analysis,[0],[0]
"‖∇P (w)‖2, ∀w ∈ Rd. (6)
We note here, for future use, that for strongly convex functions of the form (1), arising in machine learning applications, the condition number is defined as κ def=",3. Theoretical Analysis,[0],[0]
L/µ.,3. Theoretical Analysis,[0],[0]
"Furthermore, we should also notice that Assumptions 2a and 2b both cover a wide range of problems, e.g. l2-regularized empirical risk minimization problems with convex losses.
",3. Theoretical Analysis,[0],[0]
"Finally, as a special case of the strong convexity of all fi’s with µ = 0, we state the general convexity assumption, which we will use for convergence analysis.
",3. Theoretical Analysis,[0],[0]
Assumption 3.,3. Theoretical Analysis,[0],[0]
"Each function fi : Rd → R, i ∈",3. Theoretical Analysis,[0],[0]
"[n], is convex, i.e.,
fi(w) ≥ fi(w′)",3. Theoretical Analysis,[0],[0]
"+∇fi(w′)T (w − w′), ∀i ∈",3. Theoretical Analysis,[0],[0]
"[n].
Again, we note that Assumption 2b implies Assumption 3, but Assumption 2a does not.",3. Theoretical Analysis,[0],[0]
"Hence in our analysis, depending on the result we aim at, we will require Assumption 3 to hold by itself, or Assumption 2a and Assumption 3 to hold together, or Assumption 2b to hold by itself.",3. Theoretical Analysis,[0],[0]
"We will always use Assumption 1.
",3. Theoretical Analysis,[0],[0]
Our iteration complexity analysis aims to bound the number of outer iterations T (or total number of stochastic gradient evaluations) which is needed to guarantee that ‖∇P (wT )‖2 ≤ .,3. Theoretical Analysis,[0],[0]
In this case we will say that wT is an -accurate solution.,3. Theoretical Analysis,[0],[0]
"However, as is common practice for stochastic gradient algorithms, we aim to obtain the bound on the number of iterations, which is required to guarantee the bound on the expected squared norm of a gradient, i.e.,
E[‖∇P (wT )‖2] ≤ .",3. Theoretical Analysis,[0],[0]
(7),3. Theoretical Analysis,[0],[0]
The most important property of the SVRG algorithm is the variance reduction of the steps.,3.1. Linearly Diminishing Step-Size in a Single Inner Loop,[0],[0]
"This property holds as the number of outer iteration grows, but it does not hold, if only the number of inner iterations increases.",3.1. Linearly Diminishing Step-Size in a Single Inner Loop,[0],[0]
"In other words, if we simply run the inner loop for many iterations (without executing additional outer loops), the variance of the steps does not reduce in the case of SVRG, while it goes to zero in the case of SARAH.",3.1. Linearly Diminishing Step-Size in a Single Inner Loop,[0],[0]
"To illustrate this effect, let us take a look at Figures 1 and 2.
",3.1. Linearly Diminishing Step-Size in a Single Inner Loop,[0],[0]
"In Figure 1, we applied one outer loop of SVRG and SARAH to a sum of 5 quadratic functions in a twodimensional space, where the optimal solution is at the origin, the black lines and black dots indicate the trajectory of each algorithm and the red point indicates the final iterate.",3.1. Linearly Diminishing Step-Size in a Single Inner Loop,[0],[0]
"Initially, both SVRG and SARAH take steps along stochastic gradient directions towards the optimal solution.",3.1. Linearly Diminishing Step-Size in a Single Inner Loop,[0],[0]
"However, later iterations of SVRG wander randomly around the origin with large deviation from it, while SARAH follows a much more stable convergent trajectory, with a final iterate falling in a small neighborhood of the optimal solution.
",3.1. Linearly Diminishing Step-Size in a Single Inner Loop,[0],[0]
"In Figure 2, the x-axis denotes the number of effective passes which is equivalent to the number of passes through all of the data in the dataset, the cost of each pass being equal to the cost of one full gradient evaluation; and y-axis represents ‖vt‖2.",3.1. Linearly Diminishing Step-Size in a Single Inner Loop,[0],[0]
"Figure 2 shows the evolution of ‖vt‖2
for SARAH, SVRG, SGD+ (SGD with decreasing learning rate) and FISTA (an accelerated version of GD (Beck & Teboulle, 2009))",3.1. Linearly Diminishing Step-Size in a Single Inner Loop,[0],[0]
"withm = 4n, where the left plot shows the trend over multiple outer iterations and the right plot shows a single outer iteration4.",3.1. Linearly Diminishing Step-Size in a Single Inner Loop,[0],[0]
"We can see that for SVRG, ‖vt‖2 decreases over the outer iterations, while it has an increasing trend or oscillating trend for each inner loop.",3.1. Linearly Diminishing Step-Size in a Single Inner Loop,[0],[0]
"In contrast, SARAH enjoys decreasing trends both in the outer and the inner loop iterations.
",3.1. Linearly Diminishing Step-Size in a Single Inner Loop,[0],[0]
We will now show that the stochastic steps computed by SARAH converge linearly in the inner loop.,3.1. Linearly Diminishing Step-Size in a Single Inner Loop,[0],[0]
We present two linear convergence results based on our two different assumptions of µ-strong convexity.,3.1. Linearly Diminishing Step-Size in a Single Inner Loop,[0],[0]
These results substantiate our conclusion that SARAH uses more stable stochastic gradient estimates than SVRG.,3.1. Linearly Diminishing Step-Size in a Single Inner Loop,[0],[0]
"The following theorem is our first result to demonstrate the linear convergence of our stochastic recursive step vt.
Theorem 1a.",3.1. Linearly Diminishing Step-Size in a Single Inner Loop,[0],[0]
"Suppose that Assumptions 1, 2a and 3 hold.",3.1. Linearly Diminishing Step-Size in a Single Inner Loop,[0],[0]
"Consider vt defined by (2) in SARAH (Algorithm 1) with η < 2/L. Then, for any t ≥ 1,
E[‖vt‖2] ≤",3.1. Linearly Diminishing Step-Size in a Single Inner Loop,[0],[0]
[ 1− ( 2 ηL − 1 ) µ2η2 ],3.1. Linearly Diminishing Step-Size in a Single Inner Loop,[0],[0]
"E[‖vt−1‖2]
≤",3.1. Linearly Diminishing Step-Size in a Single Inner Loop,[0],[0]
[ 1− ( 2 ηL − 1 ) µ2η2,3.1. Linearly Diminishing Step-Size in a Single Inner Loop,[0],[0]
"]t E[‖∇P (w0)‖2].
",3.1. Linearly Diminishing Step-Size in a Single Inner Loop,[0],[0]
"This result implies that by choosing η = O(1/L), we obtain the linear convergence of ‖vt‖2 in expectation with the rate (1− 1/κ2).",3.1. Linearly Diminishing Step-Size in a Single Inner Loop,[0],[0]
"Below we show that a better convergence rate can be obtained under a stronger convexity assumption.
",3.1. Linearly Diminishing Step-Size in a Single Inner Loop,[0],[0]
Theorem 1b.,3.1. Linearly Diminishing Step-Size in a Single Inner Loop,[0],[0]
Suppose that Assumptions 1 and 2b hold.,3.1. Linearly Diminishing Step-Size in a Single Inner Loop,[0],[0]
Consider vt defined by (2) in SARAH (Algorithm 1) with η ≤ 2/(µ+ L).,3.1. Linearly Diminishing Step-Size in a Single Inner Loop,[0],[0]
"Then the following bound holds, ∀ t ≥ 1,
E[‖vt‖2] ≤ ( 1− 2µLηµ+L )",3.1. Linearly Diminishing Step-Size in a Single Inner Loop,[0],[0]
"E[‖vt−1‖2]
≤ ( 1− 2µLηµ+L )t E[‖∇P (w0)‖2].
",3.1. Linearly Diminishing Step-Size in a Single Inner Loop,[0],[0]
"Again, by setting η = O(1/L), we derive the linear convergence with the rate of (1 − 1/κ), which is a significant improvement over the result of Theorem 1a, when the problem is severely ill-conditioned.",3.1. Linearly Diminishing Step-Size in a Single Inner Loop,[0],[0]
"In this section, we derive the general convergence rate results for Algorithm 1.",3.2. Convergence Analysis,[0],[0]
"First, we present two important Lemmas as the foundation of our theory.",3.2. Convergence Analysis,[0],[0]
"Then, we proceed to prove sublinear convergence rate of a single outer iteration when applied to general convex functions.",3.2. Convergence Analysis,[0],[0]
"In the end, we
4In the plots of Figure 2, since the data for SVRG is noisy, we smooth it by using moving average filters with spans 100 for the left plot and 10 for the right one.
",3.2. Convergence Analysis,[0],[0]
"prove that the algorithm with multiple outer iterations has linear convergence rate in the strongly convex case.
",3.2. Convergence Analysis,[0],[0]
We begin with proving two useful lemmas that do not require any convexity assumption.,3.2. Convergence Analysis,[0],[0]
The first Lemma 1 bounds the sum of expected values of ‖∇P (wt)‖2.,3.2. Convergence Analysis,[0],[0]
"The second, Lemma 2, bounds E[‖∇P (wt)− vt‖2].",3.2. Convergence Analysis,[0],[0]
Lemma 1.,3.2. Convergence Analysis,[0],[0]
Suppose that Assumption 1 holds.,3.2. Convergence Analysis,[0],[0]
Consider SARAH (Algorithm 1).,3.2. Convergence Analysis,[0],[0]
"Then, we have m∑ t=0 E[‖∇P (wt)‖2] ≤ 2 η E[P (w0)− P (w∗)]",3.2. Convergence Analysis,[0],[0]
"(8)
+ m∑ t=0 E[‖∇P (wt)− vt‖2]− (1− Lη) m∑ t=0 E[‖vt‖2].
",3.2. Convergence Analysis,[0],[0]
Lemma 2.,3.2. Convergence Analysis,[0],[0]
Suppose that Assumption 1 holds.,3.2. Convergence Analysis,[0],[0]
Consider vt defined by (2) in SARAH (Algorithm 1).,3.2. Convergence Analysis,[0],[0]
"Then for any t ≥ 1,
E[‖∇P (wt)− vt‖2] = t∑
j=1
E[‖vj − vj−1‖2]
− t∑
j=1
E[‖∇P (wj)−∇P (wj−1)‖2].
",3.2. Convergence Analysis,[0],[0]
Now we are ready to provide our main theoretical results.,3.2. Convergence Analysis,[0],[0]
"Following from Lemma 2, we can obtain the following upper bound for E[‖∇P (wt) − vt‖2] for convex functions fi, i ∈",3.2.1. GENERAL CONVEX CASE,[0],[0]
[n].,3.2.1. GENERAL CONVEX CASE,[0],[0]
Lemma 3.,3.2.1. GENERAL CONVEX CASE,[0],[0]
Suppose that Assumptions 1 and 3 hold.,3.2.1. GENERAL CONVEX CASE,[0],[0]
Consider vt defined as (2) in SARAH (Algorithm 1) with η < 2/L.,3.2.1. GENERAL CONVEX CASE,[0],[0]
"Then we have that for any t ≥ 1,
E[‖∇P (wt)− vt‖2] ≤ ηL
2− ηL
[ E[‖v0‖2]− E[‖vt‖2] ] ≤ ηL
2− ηL E[‖v0‖2].",3.2.1. GENERAL CONVEX CASE,[0],[0]
"(9)
Using the above lemmas, we can state and prove one of our core theorems as follows.",3.2.1. GENERAL CONVEX CASE,[0],[0]
Theorem 2.,3.2.1. GENERAL CONVEX CASE,[0],[0]
Suppose that Assumptions 1 and 3 hold.,3.2.1. GENERAL CONVEX CASE,[0],[0]
"Consider SARAH (Algorithm 1) with η ≤ 1/L. Then for any s ≥ 1, we have
E[‖∇P (w̃s)‖2] ≤ 2
η(m+ 1) E[P (w̃s−1)− P (w∗)]
+ ηL
2− ηL E[‖∇P (w̃s−1)‖2].",3.2.1. GENERAL CONVEX CASE,[0],[0]
"(10)
Proof.",3.2.1. GENERAL CONVEX CASE,[0],[0]
"Since v0 = ∇P (w0) implies ‖∇P (w0)−v0‖2 = 0 then by Lemma 3, we can write∑m
t=0E[‖∇P (wt)− vt‖2] ≤ mηL 2−ηLE[‖v0‖ 2].",3.2.1. GENERAL CONVEX CASE,[0],[0]
"(11)
Hence, by Lemma 1 with η ≤ 1/L, we have∑m t=0E[‖∇P (wt)‖2] ≤ 2ηE[P (w0)− P (w ∗)]",3.2.1. GENERAL CONVEX CASE,[0],[0]
"+ ∑m t=0E[‖∇P (wt)− vt‖2]
(11) ≤ 2ηE[P",3.2.1. GENERAL CONVEX CASE,[0],[0]
(w0)− P (w ∗)],3.2.1. GENERAL CONVEX CASE,[0],[0]
+ mηL2−ηLE[‖v0‖ 2].,3.2.1. GENERAL CONVEX CASE,[0],[0]
"(12)
Since we are considering one outer iteration, with s ≥ 1, then we have v0 = ∇P (w0) = ∇P (w̃s−1) (since w0 = w̃s−1), and w̃s = wt, where t is picked uniformly at random from {0, 1, . . .",3.2.1. GENERAL CONVEX CASE,[0],[0]
",m}.",3.2.1. GENERAL CONVEX CASE,[0],[0]
"Therefore, the following holds,
E[‖∇P (w̃s)‖2] = 1m+1 ∑m t=0E[‖∇P (wt)‖2]
(12) ≤ 2η(m+1)E[P",3.2.1. GENERAL CONVEX CASE,[0],[0]
"(w̃s−1)− P (w ∗)]
+ ηL2−ηLE[‖∇P (w̃s−1)‖ 2].
",3.2.1. GENERAL CONVEX CASE,[0],[0]
"Theorem 2, in the case when η ≤ 1/L implies that
E[‖∇P (w̃s)‖2] ≤ 2η(m+1)E[P (w̃s−1)− P (w ∗)]
+ ηLE[‖∇P",3.2.1. GENERAL CONVEX CASE,[0],[0]
(w̃s−1)‖2].,3.2.1. GENERAL CONVEX CASE,[0],[0]
"By choosing the learning rate η = √
2 L(m+1) (with m such that √
2 L(m+1) ≤ 1/L) we can derive the following con-
vergence result,
E[‖∇P (w̃s)‖2] ≤ √
2L m+1E[P (w̃s−1)− P (w ∗) + ‖∇P (w̃s−1)‖2].
",3.2.1. GENERAL CONVEX CASE,[0],[0]
"Clearly, this result shows a sublinear convergence rate for SARAH under general convexity assumption within a single inner loop, with increasing m, and consequently, we have the following result for complexity bound.",3.2.1. GENERAL CONVEX CASE,[0],[0]
Corollary 1.,3.2.1. GENERAL CONVEX CASE,[0],[0]
Suppose that Assumptions 1 and 3 hold.,3.2.1. GENERAL CONVEX CASE,[0],[0]
"Consider SARAH (Algorithm 1) within a single outer iteration with the learning rate η = √ 2
L(m+1) wherem ≥ 2L−1 is the total number of iterations, then ‖∇P (wt)‖2 converges sublinearly in expectation with a rate of √ 2L m+1 , and therefore, the total complexity to achieve an -accurate solution defined in (7) is O(n+ 1/ 2).
",3.2.1. GENERAL CONVEX CASE,[0],[0]
We now turn to estimating convergence of SARAH with multiple outer steps.,3.2.1. GENERAL CONVEX CASE,[0],[0]
Simply using Theorem 2 for each of the outer steps we have the following result.,3.2.1. GENERAL CONVEX CASE,[0],[0]
Theorem 3.,3.2.1. GENERAL CONVEX CASE,[0],[0]
Suppose that Assumptions 1 and 3 hold.,3.2.1. GENERAL CONVEX CASE,[0],[0]
"Consider SARAH (Algorithm 1) and define
δk = 2 η(m+1)E[P (w̃k)− P (w ∗)], k = 0, 1, . . .",3.2.1. GENERAL CONVEX CASE,[0],[0]
", s− 1,
and δ = max0≤k≤s−1 δk.",3.2.1. GENERAL CONVEX CASE,[0],[0]
"Then we have
E[‖∇P (w̃s)‖2]−∆ ≤ αs(‖∇P",3.2.1. GENERAL CONVEX CASE,[0],[0]
"(w̃0)‖2 −∆), (13) where ∆ = δ",3.2.1. GENERAL CONVEX CASE,[0],[0]
"(
1 + ηL2(1−ηL) ) , and α = ηL2−ηL .
",3.2.1. GENERAL CONVEX CASE,[0],[0]
"Based on Theorem 3, we have the following total complexity for SARAH in the general convex case.
",3.2.1. GENERAL CONVEX CASE,[0],[0]
Corollary 2.,3.2.1. GENERAL CONVEX CASE,[0],[0]
"Let us choose ∆ = /4, α = 1/2 (with η = 2/(3L)), and m = O(1/ ) in Theorem 3.",3.2.1. GENERAL CONVEX CASE,[0],[0]
"Then, the total complexity to achieve an -accuracy solution defined in (7) is O((n+ (1/ ))",3.2.1. GENERAL CONVEX CASE,[0],[0]
log(1/ )).,3.2.1. GENERAL CONVEX CASE,[0],[0]
We now turn to the discussion of the linear convergence rate of SARAH under the strong convexity assumption on P .,3.2.2. STRONGLY CONVEX CASE,[0],[0]
"From Theorem 2, for any s ≥ 1, using property (6) of the µ-strongly convex P , we have
E[‖∇P (w̃s)‖2] ≤ 2η(m+1)E[P (w̃s−1)− P (w ∗)]
+ ηL2−ηLE[‖∇P (w̃s−1)‖ 2] (6) ≤ (
1 µη(m+1) + ηL 2−ηL )",3.2.2. STRONGLY CONVEX CASE,[0],[0]
"E[‖∇P (w̃s−1)‖2],
and equivalently,
E[‖∇P (w̃s)‖2] ≤",3.2.2. STRONGLY CONVEX CASE,[0],[0]
σm E[‖∇P (w̃s−1)‖2].,3.2.2. STRONGLY CONVEX CASE,[0],[0]
"(14)
Let us define σm def = 1µη(m+1) + ηL 2−ηL .",3.2.2. STRONGLY CONVEX CASE,[0],[0]
"Then by choosing η and m such that σm < 1, and applying (14) recursively, we are able to reach the following convergence result.
",3.2.2. STRONGLY CONVEX CASE,[0],[0]
Theorem 4.,3.2.2. STRONGLY CONVEX CASE,[0],[0]
"Suppose that Assumptions 1, 2a and 3 hold.",3.2.2. STRONGLY CONVEX CASE,[0],[0]
"Consider SARAH (Algorithm 1) with the choice of η and m such that
σm def =
1
µη(m+ 1) +
ηL
2− ηL < 1.",3.2.2. STRONGLY CONVEX CASE,[0],[0]
"(15)
Then, we have
E[‖∇P (w̃s)‖2] ≤ (σm)s‖∇P (w̃0)‖2.
",3.2.2. STRONGLY CONVEX CASE,[0],[0]
Remark 1.,3.2.2. STRONGLY CONVEX CASE,[0],[0]
Theorem 4 implies that any η < 1/L will work for SARAH.,3.2.2. STRONGLY CONVEX CASE,[0],[0]
Let us compare our convergence rate to that of SVRG.,3.2.2. STRONGLY CONVEX CASE,[0],[0]
"The linear rate of SVRG, as presented in (Johnson & Zhang, 2013), is given by
αm = 1 µη(1−2Lη)m + 2ηL 1−2ηL < 1.
",3.2.2. STRONGLY CONVEX CASE,[0],[0]
"We observe that it implies that the learning rate has to satisfy η < 1/(4L), which is a tighter restriction than
η < 1/L required by SARAH.",3.2.2. STRONGLY CONVEX CASE,[0],[0]
"In addition, with the same values of m and η, the rate or convergence of (the outer iterations) of SARAH is always smaller than that of SVRG.
σm",3.2.2. STRONGLY CONVEX CASE,[0],[0]
= 1 µη(m+1) + ηL,3.2.2. STRONGLY CONVEX CASE,[0],[0]
"2−ηL = 1 µη(m+1) + 1 2/(ηL)−1
< 1µη(1−2Lη)m + 1 0.5/(ηL)−1 = αm.
Remark 2.",3.2.2. STRONGLY CONVEX CASE,[0],[0]
"To further demonstrate the better convergence properties of SARAH, let us consider following optimization problem
min 0<η<1/L σm, min 0<η<1/4L αm,
which can be interpreted as the best convergence rates for different values of m, for both SARAH and SVRG.",3.2.2. STRONGLY CONVEX CASE,[0],[0]
"After simple calculations, we plot both learning rates and the corresponding theoretical rates of convergence, as shown in Figure 3, where the right plot is a zoom-in on a part of the middle plot.",3.2.2. STRONGLY CONVEX CASE,[0],[0]
"The left plot shows that the optimal learning rate for SARAH is significantly larger than that of SVRG, while the other two plots show significant improvement upon outer iteration convergence rates for SARAH over SVRG.
",3.2.2. STRONGLY CONVEX CASE,[0],[0]
"Based on Theorem 4, we are able to derive the following total complexity for SARAH in the strongly convex case.
",3.2.2. STRONGLY CONVEX CASE,[0],[0]
Corollary 3.,3.2.2. STRONGLY CONVEX CASE,[0],[0]
"Fix ∈ (0, 1), and let us run SARAH with η = 1/(2L) and m = 4.5κ for T iterations where T = dlog(‖∇P (w̃0)‖2/ )/ log(9/7)e, then we can derive an -accuracy solution defined in (7).",3.2.2. STRONGLY CONVEX CASE,[0],[0]
"Furthermore, we can obtain the total complexity of SARAH, to achieve the -accuracy solution, as O ((n+ κ) log(1/ )) .",3.2.2. STRONGLY CONVEX CASE,[0],[0]
"While SVRG is an efficient variance-reducing stochastic gradient method, one of its main drawbacks is the sensitivity of the practical performance with respect to the choice of m.",4. A Practical Variant,[0],[0]
"It is know that m should be around O(κ),5 while it still remains unknown that what the exact best choice is.",4. A Practical Variant,[0],[0]
"In this section, we propose a practical variant of SARAH as
5 In practice, when n is large, P (w) is often considered as a regularized Empirical Loss Minimization problem with regularization parameter λ = 1
n , then κ ∼ O(n).
SARAH+ (Algorithm 2), which provides an automatic and adaptive choice of the inner loop sizem.",4. A Practical Variant,[0],[0]
"Guided by the linear convergence of the steps in the inner loop, demonstrated in Figure 2, we introduce a stopping criterion based on the values of ‖vt‖2 while upper-bounding the total number of steps by a large enough m for robustness.",4. A Practical Variant,[0],[0]
"The other modification compared to SARAH (Algorithm 1) is the more practical choice w̃s = wt, where t is the last index of the particular inner loop, instead of randomly selected intermediate index.
",4. A Practical Variant,[0],[0]
"Algorithm 2 SARAH+ Parameters: the learning rate η > 0, 0",4. A Practical Variant,[0],[0]
< γ ≤ 1 and the maximum inner loop size m. Initialize: w̃0,4. A Practical Variant,[0],[0]
"Iterate: for s = 1, 2, . . .",4. A Practical Variant,[0],[0]
do w0 = w̃s−1 v0 = 1 n,4. A Practical Variant,[0],[0]
"∑n i=1∇fi(w0)
",4. A Practical Variant,[0],[0]
"w1 = w0 − ηv0 t = 1 while ‖vt−1‖2 > γ‖v0‖2 and t < m do
Sample it uniformly at random from [n] vt = ∇fit(wt)−∇fit(wt−1) +",4. A Practical Variant,[0],[0]
vt−1 wt+1 =,4. A Practical Variant,[0],[0]
"wt − ηvt t = t+ 1
end while Set w̃s = wt
end for
Different from SARAH, SARAH+ provides a possibility of earlier termination and unnecessary careful choices of m, and it also covers the classical gradient descent when we set γ = 1 (since the while loop does not proceed).",4. A Practical Variant,[0],[0]
In Figure 4 we present the numerical performance of SARAH+ with different γs on rcv1 and news20 datasets.,4. A Practical Variant,[0],[0]
The size of the inner loop provides a trade-off between the fast sublinear convergence in the inner loop and linear convergence in the outer loop.,4. A Practical Variant,[0],[0]
"From the results, it appears that γ = 1/8 is the optimal choice.",4. A Practical Variant,[0],[0]
"With a larger γ, i.e. γ > 1/8, the iterates in the inner loop do not provide sufficient reduction, before another full gradient computation is required, while with γ < 1/8 an unnecessary number of inner steps is performed without gaining substantial progress.",4. A Practical Variant,[0],[0]
"Clearly γ is another parameter that requires tuning, however, in our experiments, the performance of SARAH+ has been very robust with respect to the choices of γ and did not vary much from one data set to another.
",4. A Practical Variant,[0],[0]
"Similarly to SVRG, ‖vt‖2 decreases in the outer iterations of SARAH+.",4. A Practical Variant,[0],[0]
"However, unlike SVRG, SARAH+ also inherits from SARAH the consistent decrease of ‖vt‖2 in expectation in the inner loops.",4. A Practical Variant,[0],[0]
"It is not possible to apply the same idea of adaptively terminating the inner loop of
SVRG based on the reduction in ‖vt‖2, as ‖vt‖2 may have side fluctuations as shown in Figure 2.",4. A Practical Variant,[0],[0]
"To support the theoretical analyses and insights, we present our empirical experiments, comparing SARAH and SARAH+ with the state-of-the-art first-order methods for `2-regularized logistic regression problems with
fi(w) = log(1 + exp(−yixTi w))",5. Numerical Experiments,[0],[0]
+,5. Numerical Experiments,[0],[0]
"λ2 ‖w‖ 2,
on datasets covtype, ijcnn1, news20 and rcv1 6.",5. Numerical Experiments,[0],[0]
"For ijcnn1 and rcv1 we use the predefined testing and training sets, while covtype and news20 do not have test data, hence we randomly split the datasets with 70% for training and 30% for testing.",5. Numerical Experiments,[0],[0]
"Some statistics of the datasets are summarized in Table 3.
",5. Numerical Experiments,[0],[0]
"The penalty parameter λ is set to 1/n as is common practice (Le Roux et al., 2012).",5. Numerical Experiments,[0],[0]
"Note that like SVRG/S2GD and SAG/SAGA, SARAH also allows an efficient sparse implementation named “lazy updates” (Konečný et al., 2016).",5. Numerical Experiments,[0],[0]
"We conduct and compare numerical results of SARAH with SVRG, SAG, SGD+ and FISTA.",5. Numerical Experiments,[0],[0]
"SVRG (Johnson & Zhang, 2013) and SAG (Le Roux et al., 2012) are classic modern stochastic methods.",5. Numerical Experiments,[0],[0]
SGD+ is SGD with decreasing learning rate η = η0/(k + 1) where k is the number of effective passes and η0 is some initial constant learning rate.,5. Numerical Experiments,[0],[0]
"FISTA (Beck & Teboulle, 2009) is the Fast Iterative Shrinkage-Thresholding Algorithm, well-known as an efficient accelerated version of the gradient descent.",5. Numerical Experiments,[0],[0]
"Even though for each method, there is a theoretical safe learning rate, we compare the results for the best learning rates in hindsight.
",5. Numerical Experiments,[0],[0]
"Figure 5 shows numerical results in terms of loss residuals
6All datasets are available at http://www.csie.ntu.",5. Numerical Experiments,[0],[0]
"edu.tw/˜cjlin/libsvmtools/datasets/.
(top) and test errors (bottom) on the four datasets, SARAH is sometimes comparable or a little worse than other methods at the beginning.",5. Numerical Experiments,[0],[0]
"However, it quickly catches up to or surpasses all other methods, demonstrating a faster rate of decrease across all experiments.",5. Numerical Experiments,[0],[0]
"We observe that on covtype and rcv1, SARAH, SVRG and SAG are comparable with some advantage of SARAH on covtype.",5. Numerical Experiments,[0],[0]
"On ijcnn1 and news20, SARAH and SVRG consistently surpass the other methods.
",5. Numerical Experiments,[0],[0]
"In particular, to validate the efficiency of our practical variant SARAH+, we provide an insight into how important the choices of m and η are for SVRG and SARAH in Table 4 and Figure 6.",5. Numerical Experiments,[0],[0]
"Table 4 presents the optimal choices of m and η for each of the algorithm, while Figure 6 shows the behaviors of SVRG and SARAH with different choices of m for covtype and ijcnn1, where m∗s denote the best choices.",5. Numerical Experiments,[0],[0]
"In Table 4, the optimal learning rates of SARAH vary less among different datasets compared to all the other methods and they approximate the theoretical upper bound for SARAH (1/L); on the contrary, for the other methods the empirical optimal rates can exceed their theoretical limits (SVRG with 1/(4L), SAG with 1/(16L), FISTA with 1/L).",5. Numerical Experiments,[0],[0]
This empirical studies suggest that it is much easier to tune and find the ideal learning rate for SARAH.,5. Numerical Experiments,[0],[0]
"As observed in Figure 6, the behaviors of both SARAH and SVRG are quite sensitive to the choices of m. With improper choices of m, the loss residuals can be increased considerably from 10−15 to 10−3 on both covtype in 40 effective passes and ijcnn1 in 17 effective passes for
SARAH/SVRG.",5. Numerical Experiments,[0],[0]
"We propose a new variance reducing stochastic recursive gradient algorithm SARAH, which combines some of the properties of well known existing algorithms, such as SAGA and SVRG.",6. Conclusion,[0],[0]
"For smooth convex functions, we show a sublinear convergence rate, while for strongly convex cases, we prove the linear convergence rate and the computational complexity as those of SVRG and SAG.",6. Conclusion,[0],[0]
"However, compared to SVRG, SARAH’s convergence rate constant is smaller and the algorithms is more stable both theoretically and numerically.",6. Conclusion,[0],[0]
"Additionally, we prove the linear convergence for inner loops of SARAH which support the claim of stability.",6. Conclusion,[0],[0]
"Based on this convergence we derive a practical version of SARAH, with a simple stopping criterion for the inner loops.",6. Conclusion,[0],[0]
The authors would like to thank the reviewers for useful suggestions which helped to improve the exposition in the paper.,Acknowledgements,[0],[0]
"In this paper, we propose a StochAstic Recursive grAdient algoritHm (SARAH), as well as its practical variant SARAH+, as a novel approach to the finite-sum minimization problems.",abstractText,[0],[0]
"Different from the vanilla SGD and other modern stochastic methods such as SVRG, S2GD, SAG and SAGA, SARAH admits a simple recursive framework for updating stochastic gradient estimates; when comparing to SAG/SAGA, SARAH does not require a storage of past gradients.",abstractText,[0],[0]
The linear convergence rate of SARAH is proven under strong convexity assumption.,abstractText,[0],[0]
"We also prove a linear convergence rate (in the strongly convex case) for an inner loop of SARAH, the property that SVRG does not possess.",abstractText,[0],[0]
Numerical experiments demonstrate the efficiency of our algorithm.,abstractText,[0],[0]
SARAH: A Novel Method for Machine Learning Problems  Using Stochastic Recursive Gradient,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1979–1989 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"“When information is cheap, attention becomes expensive.”",1 Introduction,[0],[0]
"— James Gleick
Satirical news is considered to be entertainment.",1 Introduction,[0],[0]
"However, it is not easy to recognize the satire if the satirical cues are too subtle to be unmasked and the reader lacks the contextual or cultural background.",1 Introduction,[0],[0]
"The example illustrated in Table 1 is a piece of satirical news with subtle satirical cues.
",1 Introduction,[0],[0]
"Assuming readers interpret satirical news as true news, there is not much difference between satirical news and fake news in terms of the consequence, which may hurt the credibility of the media and the trust in the society.",1 Introduction,[0],[0]
"In fact, it is reported in the Guardian that people may believe satirical news and spread them to the public re-
gardless of the ridiculous content1.",1 Introduction,[0],[0]
"It is also concluded that fake news is similar to satirical news via a thorough comparison among true news, fake news, and satirical news (Horne and Adali, 2017).",1 Introduction,[0],[0]
"This paper focuses on satirical news detection to ensure the trustworthiness of online news and prevent the spreading of potential misleading information.
",1 Introduction,[0],[0]
"Some works tackling fake news and misleading information favor to discover the truth (Xiao et al., 2016; Wan et al., 2016) through knowledge base (Dong et al., 2015) and truthfulness estimation (Ge et al., 2013).",1 Introduction,[0],[0]
These approaches may not be feasible for satirical news because there is no ground-truth in the stories.,1 Introduction,[0],[0]
"Another track of works analyze social network activities (Zhao et al., 2015) to evaluate the spreading information (Gupta et al., 2012; Castillo et al., 2011).",1 Introduction,[0],[0]
"This could be ineffective for both fake news and satirical news because once they are distributed on the social network, the damage has been done.",1 Introduction,[0],[0]
"Finally, works evaluating culture difference (PérezRosas and Mihalcea, 2014), psycholinguistic features (Ott et al., 2011), and writing styles (Feng et al., 2012) for deception detection are suitable for satirical news detection.",1 Introduction,[0],[0]
"These works consider features at document level, while we observe that satirical cues are usually located in certain para-
1https://www.theguardian.com/media/2016/nov/17/facebookfake-news-satire
1979
graphs rather than the whole document.",1 Introduction,[0],[0]
"This indicates that many document level features may be superfluous and less effective.
",1 Introduction,[0],[0]
"To understand how paragraph-level features and document-level features are varied towards detection decision when only document level labels are available, we propose a 4-level neural network in a character-word-paragraph-document hierarchy and utilize attention mechanism (Bahdanau et al., 2014) to reveal their relative difference.",1 Introduction,[0],[0]
"We apply psycholinguistic features, writing stylistic features, structural features, and readability features to understand satire.",1 Introduction,[0],[0]
"The paragraph-level features are embedded into attention mechanism for selecting highly attended paragraphs, and the document-level features are incorporated for the final classification.",1 Introduction,[0],[0]
"This is the first work that unveils satirical cues between paragraph-level and document-level through neural networks to our knowledge.
",1 Introduction,[0],[0]
"We make the following contributions in our paper:
• We propose a 4-level hierarchical network for satirical news detection.",1 Introduction,[0],[0]
"The model detects satirical news effectively and incorporates attention mechanism to reveal paragraph-level satirical cues.
",1 Introduction,[0],[0]
"• We show that paragraph-level features are more important than document-level features in terms of the psycholinguistic feature, writing stylistic feature, and structural feature, while the readability feature is more important at the document level.
",1 Introduction,[0],[0]
"• We collect satirical news (16,000+) and true news (160,000+) from various sources and conduct extensive experiments on this corpus2.",1 Introduction,[0],[0]
"We categorize related works into four categories: content-based detection for news genre, truth verification and truthfulness evaluation, deception detection, and identification of highly attended component using attention mechanism.
",2 Related Work,[0],[0]
Content-based detection for news genre.,2 Related Work,[0],[0]
"Content-based methods are considerably effective to prevent satirical news from being recognized as true news and spreading through
2Please contact the first author to obtain the data
social media.",2 Related Work,[0],[0]
"Burfoot and Baldwin (2009) introduce headline features, profanity, and slang to embody satirical news.",2 Related Work,[0],[0]
They consider absurdity as the major device in satirical news and model this feature by comparing entity combination in a given document with Google query results.,2 Related Work,[0],[0]
Rubin et al. (2016) also consider absurdity but model it through unexpected new name entities.,2 Related Work,[0],[0]
"They introduce additional features including humor, grammar, negative affect, and punctuation to empower the detection.",2 Related Work,[0],[0]
"Besides satirical news, Chen et al. (2015) aim to detect click-baits, whose content exaggerates fact.",2 Related Work,[0],[0]
Potthast et al. (2017) report a writing style analysis of hyperpartisan news.,2 Related Work,[0],[0]
"Barbieri et al. (2015) focus on multilingual tweets that advertise satirical news.
",2 Related Work,[0],[0]
It is noteworthy that satirical news used for evaluation in above works are of limited quantity (around 200 articles).,2 Related Work,[0],[0]
Diverse examples of satire may not be included as discussed by Rubin et al. (2016).,2 Related Work,[0],[0]
"This issue inspires us to collect more than 16,000 satirical news for our experiment.
",2 Related Work,[0],[0]
Truth discovery and truthfulness evaluation.,2 Related Work,[0],[0]
"Although truth extraction from inconsistent sources (Ge et al., 2013; Wan et al., 2016; Li et al., 2016) and from conflicting sources (Yin et al., 2008; Li et al., 2014b), truth inference through knowledge base (Dong et al., 2015), and discovering evolving truth (Li et al., 2015) could help identify fact and detect fake news, they cannot favor much for satirical news as the story is entirely made up and the ground-truth is hardly found.",2 Related Work,[0],[0]
"Analyzing user activities (Farajtabar et al., 2017) and interactions (Castillo et al., 2011; Mukherjee and Weikum, 2015) to evaluate the credibility may not be appropriate for satirical news as it cannot prevent the spreading.",2 Related Work,[0],[0]
"Therefore, we utilize content-based features, including psycholinguistic features, writing stylistic features, structural features, and readability features, to address satirical news detection.
",2 Related Work,[0],[0]
Deception detection.,2 Related Work,[0],[0]
"We believe satirical news and opinion spam share similar characteristics of writing fictitious and deceptive content, which can be identified via a psycholinguistic consideration (Mihalcea and Strapparava, 2009; Ott et al., 2011).",2 Related Work,[0],[0]
"Beyond that, both syntactic stylometry (Feng et al., 2012) and behavioral features (Mukherjee et al., 2013b) are effective for detecting deceptive reviews, while stylistic features are practical to deal with obfuscating and imitat-
ing writings (Afroz et al., 2012).",2 Related Work,[0],[0]
"However, deceptive content varies among paragraphs in the same document, and so does satire.",2 Related Work,[0],[0]
We focus on devising and evaluating paragraph-level features to reveal the satire in this work.,2 Related Work,[0],[0]
"We compare them with features at the document level, so we are able to tell what features are important at which level.
",2 Related Work,[0],[0]
Identification of highly attended component using attention mechanism.,2 Related Work,[0],[0]
"Attention mechanism is widely applied in machine translation (Bahdanau et al., 2014), language inference (Rocktäschel et al., 2015), and question answering (Chen et al., 2016a).",2 Related Work,[0],[0]
"In addition, Yang et al. (2016b) propose hierarchical attention network to understand both attended words and sentences for sentiment classification.",2 Related Work,[0],[0]
Chen et al. (2016b) enhance the attention with the support of user preference and product information to comprehend how user and product affect sentiment ratings.,2 Related Work,[0],[0]
"Due to the capability of attention mechanism, we employ the same strategy to show attended component for satirical news.",2 Related Work,[0],[0]
"Different from above works, we further evaluate linguistic features of highly attended paragraphs to analyze characteristics of satirical news, which has not been explored to our knowledge.",2 Related Work,[0],[0]
We first present our 4-level hierarchical neural network and explain how linguistic features can be embedded in the network to reveal the difference between paragraph level and document level.,3 The Proposed Model,[0],[0]
Then we describe the linguistic features.,3 The Proposed Model,[0],[0]
We build the model in a hierarchy of characterword-paragraph-document.,3.1 The 4-Level Hierarchical Model,[0],[0]
The general overview of the model can be viewed in Figure 1 and the notations are listed in Table 2.,3.1 The 4-Level Hierarchical Model,[0],[0]
We use convolutional neural networks (CNN) to encode word representation from characters.,3.1.1 Character-Level Encoder,[0],[0]
"CNN is effective in extracting morphological information and name entities (Ma and Hovy, 2016), both of which are common in news.",3.1.1 Character-Level Encoder,[0],[0]
Each word is presented as a sequence of n characters and each character is embedded into a low-dimension vector.,3.1.1 Character-Level Encoder,[0],[0]
The sequence of characters c is brought to the network.,3.1.1 Character-Level Encoder,[0],[0]
A convolution operation with a filter wc is applied and moved along the sequence.,3.1.1 Character-Level Encoder,[0],[0]
Max pooling is performed to select the most important feature generated by the previous operation.,3.1.1 Character-Level Encoder,[0],[0]
The word representation xc ∈,3.1.1 Character-Level Encoder,[0],[0]
Rf is generated with f filters.,3.1.1 Character-Level Encoder,[0],[0]
Assume a sequence of words of paragraph i arrives at time t.,3.1.2 Word-Level Encoder,[0],[0]
"The current word representation xi,t concatenates xci,t from character level with pretrained word embedding xei,t, as xi,t =",3.1.2 Word-Level Encoder,[0],[0]
"[x c i,t;x",3.1.2 Word-Level Encoder,[0],[0]
"e i,t].",3.1.2 Word-Level Encoder,[0],[0]
Examples are given in Figure 1.,3.1.2 Word-Level Encoder,[0],[0]
"We implement Gated Recurrent Unit (GRU) (Cho et al., 2014) rather than LSTM (Hochreiter and Schmidhuber, 1997) to encode the sequence because GRU has fewer parameters.",3.1.2 Word-Level Encoder,[0],[0]
"The GRU adopts reset gate ri,t and update gate zi,t to control the information flow between the input xi,t and the candidate
state h̃i,t.",3.1.2 Word-Level Encoder,[0],[0]
"The output hidden state hi,t is computed by manipulating previous state hi,t−1 and the candidate state h̃i,t regarding to zi,t as in Equation 4, where denotes element-wise multiplication.
",3.1.2 Word-Level Encoder,[0],[0]
"zi,t = σ(Wzxi,t + Uzhi,t−1 + bz) (1) ri,t = σ(Wrxi,t + Urhi,t−1 + br) (2)
h̃i,t = tanh(Whxi,t + ri,t (Uhhi,t−1 + bh)) (3)
hi,t = (1− zi,t) hi,t−1 + zi,t h̃i,t (4)
To learn a better representation from the past and the future, we use bidirectional-GRU (BiGRU) to read the sequence of words with forward −−→ GRU from xi,1 to xi,t, and backward ←−− GRU from xi,t to xi,1.",3.1.2 Word-Level Encoder,[0],[0]
"The final output of Bi-GRU concatenates the last state of −−→ GRU and ←−− GRU, as [ −→ h i,t; ←− h i,1], to represent the ith paragraph.",3.1.2 Word-Level Encoder,[0],[0]
"We observe that not all paragraphs have satire and some of them are functional to make the article complete, so we incorporate attention mechanism to reveal which paragraphs contribute to decision making.",3.1.3 Paragraph-Level Attention,[0],[0]
"Assuming a sequence of paragraph representations have been constructed from lower levels, another Bi-GRU is used to encode these representations to a series of new states p1:t, so the sequential orders are considered.
",3.1.3 Paragraph-Level Attention,[0],[0]
"To decide how paragraphs should be attended, we calculate satirical degree αi of paragraph i. We first convey pi into hidden states ui as in Equation 5.",3.1.3 Paragraph-Level Attention,[0],[0]
Then we product ui with a learnable satireaware vector va and feed the result into softmax function as in Equation 6.,3.1.3 Paragraph-Level Attention,[0],[0]
"The final document representation d is computed as a weighted sum of αi and pi.
ui = tanh(Wapi + ba) (5) αi =",3.1.3 Paragraph-Level Attention,[0],[0]
exp(u,3.1.3 Paragraph-Level Attention,[0],[0]
">i v
a)∑t j=0",3.1.3 Paragraph-Level Attention,[0],[0]
"exp(u > j va))
(6)
d = t∑
i=0
αipi (7)
Linguistic features are leveraged to support attending satire paragraph.",3.1.3 Paragraph-Level Attention,[0],[0]
"Besides pi, we represent paragraph i based on our linguistic feature set and transform it into a high-level feature vector lpi via
multilayer perceptron (MLP).",3.1.3 Paragraph-Level Attention,[0],[0]
"So ui in Equation 5 is updated to:
ui = tanh(Wapi + Ual p i + b a) (8)",3.1.3 Paragraph-Level Attention,[0],[0]
"Similar to the paragraph level, we represent document j based on our linguistic feature set and transform it into a high-level feature vector ldj via MLP.",3.1.4 Document-Level Classification,[0],[0]
We concatenate dj and ldj together for classification.,3.1.4 Document-Level Classification,[0],[0]
"Suppose yj ∈ (0, 1) is the label of the document j, the prediction ỹj and the loss function L over N documents are:
ỹj = sigmoid(Wddj +",3.1.4 Document-Level Classification,[0],[0]
Udldj + b d),3.1.4 Document-Level Classification,[0],[0]
"(9)
L = − 1 N N∑ j yj log ỹj +",3.1.4 Document-Level Classification,[0],[0]
"(1− yj) log(1− ỹj)
(10)",3.1.4 Document-Level Classification,[0],[0]
"Linguistic features have been successfully applied to expose differences between deceptive and genuine content, so we subsume most of the features in previous works.",3.2 Linguistic Features,[0],[0]
The idea of explaining fictitious content is extended here to reveal how satirical news differs from true news.,3.2 Linguistic Features,[0],[0]
"We divide our linguistic features into four families and compute them separately for paragraph and document.
",3.2 Linguistic Features,[0],[0]
"Psycholinguistic Features: Psychological differences are useful for our problem, because professional journalists tend to express opinion conservatively to avoid unnecessary arguments.",3.2 Linguistic Features,[0],[0]
"On the contrary, satirical news includes aggressive language for the entertainment purpose.",3.2 Linguistic Features,[0],[0]
We additionally observe true news favors clarity and accuracy while satirical news is related to emotional cognition.,3.2 Linguistic Features,[0],[0]
"To capture the above observations, we employ Linguistic Inquiry and Word Count (LIWC) (Pennebaker et al., 2007) as our psycholinguistic dictionary.",3.2 Linguistic Features,[0],[0]
"Each category of LIWC is one independent feature and valued by its frequency3.
",3.2 Linguistic Features,[0],[0]
"Writing Stylistic Features: The relative distribution of part-of-speech (POS) tags reflects informative vs. imaginative writing, which contributes to detecting deceptions (Li et al., 2014a; Mukherjee et al., 2013a).",3.2 Linguistic Features,[0],[0]
We argue that the stories covered by satirical news are based on imagination.,3.2 Linguistic Features,[0],[0]
"In addition, POS tags are hints of the underlying
3Total counts divided by total words.
humor (Reyes et al., 2012), which is common in satirical news.",3.2 Linguistic Features,[0],[0]
"So we utilize POS tags (Toutanova et al., 2003) to apprehend satire.",3.2 Linguistic Features,[0],[0]
"Each tag is regarded as one independent feature and valued by its frequency.
",3.2 Linguistic Features,[0],[0]
"Readability Features: We consider readability of genuine news would differ from satirical news because the former is written by professional journalists and tend to be clearer and more accurate, while satirical news packs numerous clauses to enrich the made-up story as introduced by Rubin et al. (2016).",3.2 Linguistic Features,[0],[0]
"Different from their work, we use readability metrics, including Flesch Reading Ease (Kincaid et al., 1975), Gunning Fog Index (Gunning, 1952), Automated Readability Index (Senter and Smith, 1967), ColemanLiau Index (Coleman and Liau, 1975), and syllable count per word, as features.
",3.2 Linguistic Features,[0],[0]
"Structural Features: To further reflect the structure of news articles, we examine the following features: word count, log word count, number of punctuations, number of digits, number of capital letters, and number of sentences.",3.2 Linguistic Features,[0],[0]
We report satirical news detection results and show high weighted word features.,4 Experiment and Evaluation,[0],[0]
"Then, we provide a thorough analysis between paragraph-level and document-level features.",4 Experiment and Evaluation,[0],[0]
"Finally, we visualize an example of satirical news article to demonstrate the effectiveness of our work.",4 Experiment and Evaluation,[0],[0]
"The satirical news is collected from 14 websites that explicitly declare they are offering satire, so the correct label can be guaranteed.",4.1 Dataset,[0],[0]
"We also notice websites that mix true news, fake news, and satirical news.",4.1 Dataset,[0],[0]
"We exclude these websites in this work because it requires experts to annotate the news articles.
",4.1 Dataset,[0],[0]
"We maintain each satire source in only one of the train/validation/test sets4 as the cross-domain
4Train: Onion, the Spoof.",4.1 Dataset,[0],[0]
"Test: SatireWorld, Beaverton, Ossurworld.",4.1 Dataset,[0],[0]
"Validation: DailyCurrent, DailyReport, EnduringVision, Gomerblog, NationalReport, SatireTribune, SatireWire, Syruptrap, and UnconfirmedSource.
setting in (Li et al., 2014a).",4.1 Dataset,[0],[0]
"Otherwise, the problem may become writing pattern recognition or news site classification.",4.1 Dataset,[0],[0]
"We also combined different sources together5 as a similar setting of leveraging multiple domains (Yang et al., 2016a).",4.1 Dataset,[0],[0]
"The true news is collected from major news outlets6 and Google News using FLORIN (Liu et al., 2015).",4.1 Dataset,[0],[0]
"The satirical news in the corpus is significantly less than true news, reflecting an impressionistic view of the reality.",4.1 Dataset,[0],[0]
"We omit headline, creation time, and author information so this work concentrates on the satire in the article body.",4.1 Dataset,[0],[0]
We realize the corpus may contain different degree of satire.,4.1 Dataset,[0],[0]
"Without the annotation, we only consider binary classification in this work and leave the degree estimation for the future.",4.1 Dataset,[0],[0]
The split and the description of the dataset can be found in Table 3.,4.1 Dataset,[0],[0]
"For SVM, we use the sklearn implementation7.",4.2 Implementation Detail,[0],[0]
We find that using linear kernel and setting “class weight” to “balanced” mostly boost the result.,4.2 Implementation Detail,[0],[0]
"We search soft-margin penalty “C” and find high results occur in range [10−1, 10−4].",4.2 Implementation Detail,[0],[0]
"We use the validation set to tune the model so selecting hyper-parameters is consistent with neural network based model.
",4.2 Implementation Detail,[0],[0]
"For neural network based models, we use the Theano package (Bastien et al., 2012) for implementation.",4.2 Implementation Detail,[0],[0]
"The lengths of words, paragraphs, and documents are fixed at 24, 128, and 16 with necessary padding or truncating.",4.2 Implementation Detail,[0],[0]
Stochastic Gradient Descent is used with initial learning rate of 0.3 and decay rate of 0.9.,4.2 Implementation Detail,[0],[0]
The training is early stopped if the F1 drops 5 times continuously.,4.2 Implementation Detail,[0],[0]
"Word embeddings are initialized with 100- dimension Glove embeddings (Pennington et al., 2014).",4.2 Implementation Detail,[0],[0]
Character embeddings are randomly initialized with 30 dimensions.,4.2 Implementation Detail,[0],[0]
"Specifically for the proposed model, the following hyper-parameters are estimated based on the validation set and used
5The combination is chosen to ensure enough training examples and balanced validation/test sets.
6CNN, DailyMail, WashingtonPost, NYTimes, TheGuardian, and Fox.
7sklearn.svm.",4.2 Implementation Detail,[0],[0]
"SVC
in the final test set.",4.2 Implementation Detail,[0],[0]
The dropout is applied with probability of 0.5.,4.2 Implementation Detail,[0],[0]
The size of the hidden states is set at 60.,4.2 Implementation Detail,[0],[0]
We use 30 filters with window size of 3 for convolution.,4.2 Implementation Detail,[0],[0]
"We report accuracy, precision, recall, and F1 on the validation set and the test set.",4.3 Performance of Satirical News Detection,[0],[0]
All metrics take satirical news as the positive class.,4.3 Performance of Satirical News Detection,[0],[0]
"Both paragraph-level and document-level linguistic features are scaled to have zero mean and unit variance, respectively.",4.3 Performance of Satirical News Detection,[0],[0]
"The compared methods include:
SVM word n-grams: Unigram and bigrams of the words as the baseline.",4.3 Performance of Satirical News Detection,[0],[0]
"We report 1,2-grams because it performs better than other n-grams.
",4.3 Performance of Satirical News Detection,[0],[0]
"SVM word n-grams + LF: 1,2-word grams plus linguistic features.",4.3 Performance of Satirical News Detection,[0],[0]
"We omit comparison with similar work (Ott et al., 2011) as their features are subsumed in ours.
",4.3 Performance of Satirical News Detection,[0],[0]
"SVM word + char n-grams: 1,2-word grams plus bigrams and trigrams of the characters.
",4.3 Performance of Satirical News Detection,[0],[0]
"SVM word + char n-grams + LF: All the proposed features are considered.
",4.3 Performance of Satirical News Detection,[0],[0]
"SVM Rubin et al. (2016): Unigram and bigrams tf-idf with satirical features as proposed in (Rubin et al., 2016).",4.3 Performance of Satirical News Detection,[0],[0]
"We compare with (Rubin et al., 2016) rather than (Burfoot and Baldwin, 2009) as the former claims a better result.
",4.3 Performance of Satirical News Detection,[0],[0]
SVM Rubin et al. (2016) +,4.3 Performance of Satirical News Detection,[0],[0]
"char tf-idf + LF: Include all possible features.
",4.3 Performance of Satirical News Detection,[0],[0]
Bi-GRU: Bi-GRU for document classification.,4.3 Performance of Satirical News Detection,[0],[0]
"The document representation is the average of the hidden state at every time-step.
",4.3 Performance of Satirical News Detection,[0],[0]
SVM Doc2Vec:,4.3 Performance of Satirical News Detection,[0],[0]
"Unsupervised method learning distributed representation for documents (Le and Mikolov, 2014).",4.3 Performance of Satirical News Detection,[0],[0]
"The implementation is based on
Gensim (Řehůřek and Sojka, 2010).",4.3 Performance of Satirical News Detection,[0],[0]
"HAN: Hierarchical Attention Network (Yang et al., 2016b) for document classification with both word-level and sentence-level attention.
",4.3 Performance of Satirical News Detection,[0],[0]
"4LHN: 4-Level Hierarchical Network without any linguistic features.
",4.3 Performance of Satirical News Detection,[0],[0]
"4LHNP: 4-Level Hierarchical Network with Paragraph-level linguistic features.
",4.3 Performance of Satirical News Detection,[0],[0]
"4LHND: 4-Level Hierarchical Network with Document-level linguistic features.
",4.3 Performance of Satirical News Detection,[0],[0]
"4LHNPD: 4-Level Hierarchical Network with both Paragraph-level and Document-level linguistic features.
",4.3 Performance of Satirical News Detection,[0],[0]
"In Table 4, the performances on the test set are generally better than on the validation set due to the cross-domain setting.",4.3 Performance of Satirical News Detection,[0],[0]
"We also explored word-level attention (Yang et al., 2016b), but it performed 2% worse than 4LHN.",4.3 Performance of Satirical News Detection,[0],[0]
The result of Doc2Vec is limited.,4.3 Performance of Satirical News Detection,[0],[0]
"We suspect the reason could be the high imbalanced dataset, as an unsupervised learning method for document representation heavily relies on the distribution of the document.",4.3 Performance of Satirical News Detection,[0],[0]
We report high weighted word-grams in Table 5 based on the SVM model as incorporating word-level attention in our neural hierarchy model reduces the detection performance.,4.4 Word Level Analysis,[0],[0]
"According
to Table 5, we conclude satirical news mimics true news by using news related words, such as “stated” and “reporter”.",4.4 Word Level Analysis,[0],[0]
"However, these words may be over used so they can be detected.",4.4 Word Level Analysis,[0],[0]
"True news may use other evidence to support the credibility, which explains “twitter”, “com”, “video”, and “pictured”.",4.4 Word Level Analysis,[0],[0]
High weight of “ : ” indicates that true news uses colon to list items for clarity.,4.4 Word Level Analysis,[0],[0]
"High weight of “ '' ” indicates that satirical news involves more conversation, which is consistent with our observation.",4.4 Word Level Analysis,[0],[0]
The final interesting note is satirical news favors “washington dc”.,4.4 Word Level Analysis,[0],[0]
"We suspect that satirical news mostly covers politic topics, or satire writers do not spend efforts on changing locations.",4.4 Word Level Analysis,[0],[0]
"We use 4LHNPD to compare paragraph-level and document-level features, as 4LHNPD leverages the two-level features into the same framework and yields the best result.
",4.5 Analysis of Weighted Linguistic Features,[0],[0]
"Because all linguistic features are leveraged into MLP with non-linear functions, it is hard to check which feature indicates satire.",4.5 Analysis of Weighted Linguistic Features,[0],[0]
"Alternatively, we define the importance of linguistic features by summing the absolute value of the weights if directly connected to the feature.",4.5 Analysis of Weighted Linguistic Features,[0],[0]
"For example, the importance I of feature k is given by Ik = 1 M ∑M m=0 |wk,m|, where w ∈ RK×M is the directly connected weight, K is the number of features, and M is the dimension of the output.",4.5 Analysis of Weighted Linguistic Features,[0],[0]
"This metric gives a general idea about how much does a feature contribute to the decision making.
",4.5 Analysis of Weighted Linguistic Features,[0],[0]
We first report the scaled importance of the four linguistic feature sets by averaging the importance of individual linguistic features.,4.5 Analysis of Weighted Linguistic Features,[0],[0]
Then we report individual important features within each set.,4.5 Analysis of Weighted Linguistic Features,[0],[0]
"According to Figure 2, the importance of paragraph-level features is greater than documentlevel features except for the readability feature set.",4.5.1 Comparing the Four Feature Sets,[0],[0]
"It is reasonable to use readability at the document level because readability features evaluate the understandability of a given text, which depends on the content and the presentation.",4.5.1 Comparing the Four Feature Sets,[0],[0]
"The structural feature set is highly weighted for selecting attended paragraph, which inspires us to focus on individual features inside the structural feature set.",4.5.1 Comparing the Four Feature Sets,[0],[0]
"Within each set, we rank features based on the importance score and report their mean and standard deviation before being scaled in Table 6.",4.5.2 Comparing Individual Features,[0],[0]
"At paragraph level, we use top three attended paragraphs for calculating.",4.5.2 Comparing Individual Features,[0],[0]
"The respective p-values of all features in the table are less than 0.01 based on the t-test, indicating satirical news is statistically significantly different from true news.
",4.5.2 Comparing Individual Features,[0],[0]
"Comparing Table 6 and Table 3, we find that the word count, capital letters, and punctuations in true news are larger than in satirical news at the document level, while at paragraph level these
features in true news are less than in satirical news.",4.5.2 Comparing Individual Features,[0],[0]
This indicates satire paragraph could be more complex locally.,4.5.2 Comparing Individual Features,[0],[0]
"It also could be referred as “sentence complexity”, that “satirical articles tend to pack a great number of clauses into a sentence for comedic effect” (Rubin et al., 2016).",4.5.2 Comparing Individual Features,[0],[0]
"Accordingly, we hypothesize top complex paragraphs could represent the entire satire document for classification, which we leave for future examination.
",4.5.2 Comparing Individual Features,[0],[0]
"In Table 6, psycholinguistic feature “Humans” is more related to emotional writing than control writing (Pennebaker et al., 2007), which indicates satirical news is emotional and unprofessional compared to true news.",4.5.2 Comparing Individual Features,[0],[0]
"The same reason also applies to “Social” and “Leisure”, where the former implies emotional and the latter implies control writing.",4.5.2 Comparing Individual Features,[0],[0]
"The “Past” and “VBN” both have higher frequencies in true news, which can be explained by the fact that true news covers what happened.",4.5.2 Comparing Individual Features,[0],[0]
"A similar reason that true news reports what happened to others explains a low “Self” and a high “VBZ” in true news.
",4.5.2 Comparing Individual Features,[0],[0]
"For writing stylistic features, it is suggested that informative writing has more nouns, adjectives, prepositions and coordinating conjunctions, while imaginative writing has more verbs, adverbs, pronouns, and pre-determiners (Rayson et al., 2001).",4.5.2 Comparing Individual Features,[0],[0]
"This explains higher frequencies of “RB” and “PRP” in satirical news, and higher frequency of “NN” and “CC” in true news.",4.5.2 Comparing Individual Features,[0],[0]
"One exception is “JJ”, adjectives, which receives the highest weight in this feature set and indicates a higher frequency
in satirical news.",4.5.2 Comparing Individual Features,[0],[0]
"We suspect adjective could also be related to emotional writing, but more experiments are required.
",4.5.2 Comparing Individual Features,[0],[0]
Readability suggests satirical news is easier to be understood.,4.5.2 Comparing Individual Features,[0],[0]
"Considering satirical news is also deceptive (as the story is not true), this is consistent with works (Frank et al., 2008; Afroz et al., 2012) showing deceptive writings are more easily comprehended than genuine writings.",4.5.2 Comparing Individual Features,[0],[0]
"Finally, true news has more digits and a higher “CD”(Cardinal number) frequency, even at the paragraph level, because they tend to be clear and accurate.",4.5.2 Comparing Individual Features,[0],[0]
"To explore the attention, we sample one example in the validation set and present it in Figure 3.",4.6 Visualization of Attended Paragraph,[0],[0]
The value at the right represents the scaled attention score.,4.6 Visualization of Attended Paragraph,[0],[0]
The high attended paragraphs are longer and have more capital letters as they are referring different entities.,4.6 Visualization of Attended Paragraph,[0],[0]
"They have more double quotes, as multiple conversations are involved.
",4.6 Visualization of Attended Paragraph,[0],[0]
"Moreover, we subjectively feel the attended paragraph with score 0.98 has a sense of humor while the paragraph with score 0.86 has a sense of sarcasm, which are common in satire.",4.6 Visualization of Attended Paragraph,[0],[0]
"The paragraph with score 1.0 presents controversial topics, which could be misleading if the reader cannot understand the satire.",4.6 Visualization of Attended Paragraph,[0],[0]
This is what we expect from the attention mechanism.,4.6 Visualization of Attended Paragraph,[0],[0]
"Based on the visualization, we also feel this work could be generalized to detect figurative languages.",4.6 Visualization of Attended Paragraph,[0],[0]
"In this paper, we proposed a 4-level hierarchical network and utilized attention mechanism to understand satire at both paragraph level and document level.",5 Conclusion,[0],[0]
"The evaluation suggests readability features support the final classification while psycholinguistic features, writing stylistic features, and structural features are beneficial at the paragraph level.",5 Conclusion,[0],[0]
"In addition, although satirical news is shorter than true news at the document level, we find satirical news generally contain paragraphs which are more complex than true news at the paragraph level.",5 Conclusion,[0],[0]
"The analysis of individual features reveals that the writing of satirical news tends to be emotional and imaginative.
",5 Conclusion,[0],[0]
"We will investigate efforts to model satire at the paragraph level following our conclusion and theoretical backgrounds, such as (Ermida, 2012).",5 Conclusion,[0],[0]
We plan to go beyond the binary classification and explore satire degree estimation.,5 Conclusion,[0],[0]
"We will generalize our approach to reveal characteristics of figurative language (Joshi et al., 2016), where different paragraphs or sentences may reflect different degrees of sarcasm, irony, and humor.",5 Conclusion,[0],[0]
The authors would like to thank the anonymous reviewers for their comments.,Acknowledgments,[0],[0]
This work was support in part by the U.S. NSF grants 1546480 and 1527364.,Acknowledgments,[0],[0]
"Satirical news is considered to be entertainment, but it is potentially deceptive and harmful.",abstractText,[0],[0]
"Despite the embedded genre in the article, not everyone can recognize the satirical cues and therefore believe the news as true news.",abstractText,[0],[0]
We observe that satirical cues are often reflected in certain paragraphs rather than the whole document.,abstractText,[0],[0]
"Existing works only consider documentlevel features to detect the satire, which could be limited.",abstractText,[0],[0]
We consider paragraphlevel linguistic features to unveil the satire by incorporating neural network and attention mechanism.,abstractText,[0],[0]
"We investigate the difference between paragraph-level features and document-level features, and analyze them on a large satirical news dataset.",abstractText,[0],[0]
The evaluation shows that the proposed model detects satirical news effectively and reveals what features are important at which level.,abstractText,[0],[0]
Satirical News Detection and Analysis using Attention Mechanism and Linguistic Features,title,[0],[0]
In many biological and physical experiments it is necessary to track the movement of many isolated particles in a video datastream.,1. Introduction,[0],[0]
"This is an essential task in biomedical research, for example, to reveal the biophysical properties of both the imaged particles (e.g., single molecules) and the biological substrate (e.g., cell membrane) that the particles are traversing.",1. Introduction,[0],[0]
"Effective particle tracking algorithms have wide
1Department of Biological Sciences; 2Departments of Statistics and Neuroscience; Grossman Center for the Statistics of Mind; Center for Theoretical Neuroscience; Columbia University.",1. Introduction,[0],[0]
"Correspondence to: Liam Paninski <liam@stat.columbia.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
applications in both fundamental and applied biology, and more generally in chemistry and physical applications.
",1. Introduction,[0],[0]
Previous scalable approaches to this task have largely involved non-Bayesian methods aiming at estimating a single “best” path of the underlying particles.,1. Introduction,[0],[0]
"However, in many applications particles have indistinguishable shapes under light microscopic resolution.",1. Introduction,[0],[0]
This leads to a fundamental non-identifiability: if two particles pass close by each other (“meet”) then it is impossible to deterministically link the pre-meeting paths with the correct post-meeting paths (see Figure 1 below for an illustration).,1. Introduction,[0],[0]
"This motivates a Bayesian approach for assigning posterior probabilities over all the possible sets of particle paths consistent with the observed data.
",1. Introduction,[0],[0]
"Formally, at each timestep we observe a noisy, blurry image recording the particles’ current positions.",1. Introduction,[0],[0]
"In the simplest case, we can cast the tracking task in a factorial hidden Markov Model (HMM) framework, where each particle evolves according to a Markov process and thus multiple HMMs (one per particle) jointly determine the observed image data.",1. Introduction,[0],[0]
"The classic HMM inference approach is the forward-backward algorithm (Rabiner, 1990), but the complexity of forward-backward scales superlinearly with the number of particles here.
",1. Introduction,[0],[0]
"In this work, we propose an amortized inference approach utilizing a specialized recurrent neural network architecture to approximate the posterior particle transition densities inferred by forward-backward.",1. Introduction,[0],[0]
"After network training, posterior inference can be performed very quickly: given a new video dataset, the network outputs the conditional particle initialization and transition densities, and then we can simply sample forward from the resulting Markov chain to draw samples from the posterior particle paths.
",1. Introduction,[0],[0]
We apply the method to simulated and real data.,1. Introduction,[0],[0]
"We show that the method robustly performs approximate Bayesian inference on the observed data, and provides more accurate results than competing methods that output just a single “best” path.",1. Introduction,[0],[0]
"Our approach is much more scalable than previously proposed Bayesian approaches, scaling linearly in the number of frames and in the number of observed pixels.",1. Introduction,[0],[0]
To set the stage we describe the simplest concrete model for particle tracking data; we will generalize this model below.,2. Model,[0],[0]
We have J indistinguishable particles: each particle j appears at some time tappearj and disappears at some later time tdisappearj .,2. Model,[0],[0]
"The particles move according to independent Gauss-Markov processes, with no interactions between particles.",2. Model,[0],[0]
On each frame t we observe a blurred noisy sum of the particles that are visible at time t.,2. Model,[0],[0]
The observation likelihood depends on the details of the experimental setup; the most common model is the Gaussian blur +,2. Model,[0],[0]
"Poisson noise model:
Y (t, x) ∼ Poisson[λ(t, x) + λ0] λ(t, x) = ∑ j G[x− sj(t)],
where Y (t, x) denotes the image data observed at pixel x at time t, λ0 is a background “dark noise” Poisson intensity, G[.] is a Gaussian point spread function (psf), sj(t) represents the location of particle j at time t, and the sum is over all particles that are alive at time",2. Model,[0],[0]
"t.
The model described above is a factorial HMM (Ghahramani & Jordan, 1996).",2. Model,[0],[0]
"However, this simple model can be generalized significantly.",2. Model,[0],[0]
There may be multiple distinguishable classes of particles that have different shapes or colors.,2. Model,[0],[0]
"In many datasets particles can interact: they might merge, collide, split, etc.",2. Model,[0],[0]
"Individual particles often move in a non-Markovian manner (e.g., switching between several different latent dynamical modes).",2. Model,[0],[0]
"There may be strong dependencies between the motion of different particles, due e.g. to substrate motion.",2. Model,[0],[0]
"Finally, the observation noise may be highly non-Poisson, with correlations and strong inhomogeneities across the field of view.",2. Model,[0],[0]
Thus it is critical to develop flexible inference approaches that do not depend on strong factorial HMM assumptions.,2. Model,[0],[0]
"The literature on particle tracking methods is vast, and dates back to early physics studies of Brownian motion in fluids; see e.g. (Manzo & Garcia-Parajo, 2015) for a review, and (Chenouard et al., 2014) for a quantitative comparison of many algorithms.",3. Related work on particle tracking,[0],[0]
"We will not attempt to review all of these methods here, but note that many algorithms split the tracking problem into a “detect” followed by a “link” step.",3. Related work on particle tracking,[0],[0]
The “detect” step outputs estimated particle locations given each image Yt.,3. Related work on particle tracking,[0],[0]
"Various nonlinear filtering, thresholding, deconvolution, and neural network approaches have been employed for this task (Chenouard et al., 2014).",3. Related work on particle tracking,[0],[0]
"Most such detection algorithms take just single frames Yt as input, and therefore they do not integrate useful information across multiple frames to perform detection; (Newby et al., 2017)
is a recent counterexample that demonstrates that better performance can be achieved if multiple frames Yt are utilized in the detection step.
",3. Related work on particle tracking,[0],[0]
"The “link” step then attempts to fuse these detected locations, to estimate the tracks that each visible particle took over the length of the observed movie.",3. Related work on particle tracking,[0],[0]
"This linkage step is solved by some matching algorithm; see e.g. (Jaqaman et al., 2008) for an influential example of this approach, and (Chenouard et al., 2014; Turner et al., 2014; Wilson et al., 2016) for discussion of some other linking methods.
",3. Related work on particle tracking,[0],[0]
"As we emphasized in the introduction, deterministic detection and linking approaches are statistically suboptimal, since they ignore the irreducible uncertainty of the tracking problem that results when two or more visibly indistinguishable particles pass closer than a fraction of a psf-width of each other.",3. Related work on particle tracking,[0],[0]
"Ignoring this uncertainty leads to non-robust results, in which tiny changes to the data can lead to discontinuous changes in the estimated particle tracks.",3. Related work on particle tracking,[0],[0]
"Moreover, it is clear that the linkage and detection should not be separated: if we know the tracks of particles at times (1 : t− 1) and (t+ 1 : T ), then we have very strong prior information about the locations of particles at time t, and ignoring this useful prior information will lead to suboptimal results.",3. Related work on particle tracking,[0],[0]
"(See e.g. (Sun et al., 2017), where similar points were made in the context of a related super-resolution application.)
",3. Related work on particle tracking,[0],[0]
"Similar points have been made in the Bayesian signal processing literature; for example, sequential Monte Carlo (particle filtering) methods have been applied to perform probabilistic inference in this setting (Smal et al., 2008).",3. Related work on particle tracking,[0],[0]
"These approaches have the advantage of a proper grounding in standard Bayesian computational methodology, but scale poorly in the number of visible particles.
",3. Related work on particle tracking,[0],[0]
"Finally, there is also a very large literature on “multi-target tracking,” e.g., tracking multiple people visible on security cameras.",3. Related work on particle tracking,[0],[0]
"In this literature the different targets are typically distinguishable (e.g., different people visible on a camera will have different faces, gaits, clothing, etc.), whereas in this paper we focus on the case that the particles to be tracked are indistinguishable.",3. Related work on particle tracking,[0],[0]
"Of course a middle ground exists in which particles have some distinguishing features but some posterior uncertainty about particle identity remains due to noisy or incomplete observations; however, to keep our presentation simple we focus exclusively on the most challenging fully-indistinguishable case here.",3. Related work on particle tracking,[0],[0]
"Our conceptual starting point is the standard filter-backwardsample-forward algorithm for sampling from the posterior distribution p(Q|Y ) of the hidden state Q = {qt} of an
HMM conditional on the observed data Y (Rabiner, 1990).",4.1. Overview,[0],[0]
"This algorithm has two steps: (1) combine the observed data Y with the prior distribution p(Q) of the hidden Markov state Q to obtain a new Markov chain p(Q|Y ), and (2) sample forward from this new Markov chain.",4.1. Overview,[0],[0]
"Once (1) is complete, we can call (2) as often as we like to generate new sample paths from p(Q|Y ).",4.1. Overview,[0],[0]
"This approach is attractive in our setting because sampling forward from a Markov chain is a fast operation once the conditional initial and transition densities (p(q1|Y ) and p(qt|Y, qt−1), respectively) are in hand, where the hidden state qt is the configuration of the locations and identities of all of the particles alive at time",4.1. Overview,[0],[0]
"t. Thus in principle we can simply run (2) repeatedly to compute probabilities of any quantity we care about (e.g., the probability that a particle is in location x at time t, or the probability that particle i in frame s should be linked with particle j in frame t).
",4.1. Overview,[0],[0]
"Unfortunately, as emphasized above, computing (1) exactly is intractable in our context; thus we need to approximate the conditional initial and transition densities.",4.1. Overview,[0],[0]
"Our strategy is to train neural networks to approximate these probabilities.
",4.1. Overview,[0],[0]
"This approach is highly flexible; given enough training data, we can handle a wide variety of non-standard data, well beyond the simplest Gaussian blur +",4.1. Overview,[0],[0]
"Poisson noise factorial HMM described above, since the learned probabilities do not lean heavily on special assumptions about e.g. the noise model or the precise details of the graphical model underlying the data1.",4.1. Overview,[0],[0]
"In turn, we can generate as much training data as we need by simulating ground truth particle tracks along with the resulting observed data videos Y .
",4.1. Overview,[0],[0]
It is convenient to split the network into three parts: the conditional transition density that governs how samples move from timestep t to t+ 1; the conditional birth density that governs the probability that a new particle appears at time t; and the conditional initial density that governs the positions of the particles at timestep 1.,4.1. Overview,[0],[0]
"We describe each of these in turn below.
",4.1. Overview,[0],[0]
"1The main assumption we make is that the posterior p(Q|Y ) can be well-approximated as Markovian, so that our resulting Markovian sampler can provide good approximations to true samples from the posterior.",4.1. Overview,[0],[0]
This assumption is reasonable in the majority of particle-tracking applications we have in mind.,4.1. Overview,[0],[0]
This network is illustrated in Figure 1.,4.2. Conditional transition density network,[0],[0]
The task of this network is to combine the observed data Y with the previous particle configuration qt−1 and to output probabilities that govern the particle configuration qt in the next time step.,4.2. Conditional transition density network,[0],[0]
"This is a nontrivial task, since the dimensionality of qt can be large and varies with time t as particles appear or disappear.",4.2. Conditional transition density network,[0],[0]
"Similarly, the observed image Yt is often large (hundreds of pixels on a side), and in principle we need to observe multiple frames before and after time t to perform optimal inference.
",4.2. Conditional transition density network,[0],[0]
"Thus, for scalability, we break the problem up into a sequence of smaller pieces and work convolutionally.",4.2. Conditional transition density network,[0],[0]
We begin by choosing a random ordering of the particles in qt−1.,4.2. Conditional transition density network,[0],[0]
"Then, for each of these particles indexed by i, we input three types of data: (1) a local patch of the observed movie data (in a spatial neighborhood around the i-th particle location sit−1, and in a temporal context of M frames before and after the current frame t; (2) a binary mask indicating the locations of the particles at time t − 1 in the same spatial neighborhood as particle i; and (3) a second binary mask indicating the locations of the particles j that have been sampled at time t prior to sampling particle i2.",4.2. Conditional transition density network,[0],[0]
"The network is then trained to output a probability map p(sit|qt−1, {qjt }j<i, Y ) indicating the likely location sit, along with an auxiliary probability that the particle disappears (and is therefore no longer present at time t).",4.2. Conditional transition density network,[0],[0]
"Once these transition probabilities are learned, we can sample forward one particle and time-step at a time, as illustrated in the sampling process video and detailed in Algorithm 1; thus at test time inference scales linearly in the number of particles and time steps in the movie.
",4.2. Conditional transition density network,[0],[0]
"Note that we have slightly diverged from the vanilla filterbackward-sample-forward algorithm, which propagates information all the way back from the final observation YT to determine the state qt.",4.2. Conditional transition density network,[0],[0]
"Instead, we exploit the fact that only a local context around time t is necessary to infer qt, and thus we restrict our attention at time t to the local context Yt−M :t+M .",4.2. Conditional transition density network,[0],[0]
(We use M = 2 throughout this paper.),4.2. Conditional transition density network,[0],[0]
"The network described above moves particles forward from timestep t− 1 to t, and decides which particles should disappear at time t. However, new particles can enter the field
2This input lets the network avoid placing two particles to explain a single observed bump in Yt; if a previously-sampled particle j already explains the bump well, then the network will prefer to put particle i elsewhere.",4.3. New birth networks and initialization,[0],[0]
"Also note that the input data Y and output probability maps don’t need to have the same number of pixels (i.e., we could attempt to resolve the particle locations at higher spatial resolution than the observed data), but we have not pursued this direction in detail.
",4.3. New birth networks and initialization,[0],[0]
"Algorithm 1 Conditional sampling network Initialize: S1 = Initializer(Y1:M+1,",4.3. New birth networks and initialization,[0],[0]
[]) # Sec.,4.3. New birth networks and initialization,[0],[0]
"4.3 # Sec. 4.2 for t = 2, 3, 4... do St =",4.3. New birth networks and initialization,[0],[0]
"[] for i in Permutation{St−1} do pi = ConditionalProbability(Yt−M :t+M , St−1, St, i) particle i disappears with prob.",4.3. New birth networks and initialization,[0],[0]
"1− ∫ pi
otherwise i′ is sampled from pi Insert i′ to St
end for Nt = NewBirth(Yt−M :t+M , St−1, St) #",4.3. New birth networks and initialization,[0],[0]
Sec.,4.3. New birth networks and initialization,[0],[0]
"4.3 Insert Nt to St St−1 = St
end for
of view at any time, and therefore we need a method for adding new particles to qt.",4.3. New birth networks and initialization,[0],[0]
"Thus after running the update described above to qt, we run a second convolutional network that takes the same inputs as above (i.e., the local context of qt−1, qt, and Y , now at each location in the image instead of just at the previously-sampled particle locations) and outputs the probability that a new particle is born at each location at time t.",4.3. New birth networks and initialization,[0],[0]
"Then we iteratively sample from this density and update qt until no further particles are added.
",4.3. New birth networks and initialization,[0],[0]
The same strategy can be used to initialize q1; the only difference is that the inputs now don’t include qt−1 or the context of Y prior to Y1.,4.3. New birth networks and initialization,[0],[0]
"To handle the temporal and spatial dependencies in this data, we chose a combination of bi-directional 2D convolution LSTM and 3D convolutional layers; see Appendix.",4.4. Network architecture and training,[0],[0]
"Overall, when the network is sampling forward, we can think of the resulting algorithm as a recurrent neural network (since the sampled output is then read back into the network to define the next state transition), with the somewhat non-standard feature that the network remains at timestep t for a random number of iterations (depending on how many particles need to be updated and how many particles are born at each timestep).
",4.4. Network architecture and training,[0],[0]
To train the network we generated simulated ground truth particle tracks qt and corresponding observed movies Y .,4.4. Network architecture and training,[0],[0]
(We will discuss the training data in more detail in the following section.),4.4. Network architecture and training,[0],[0]
"Then we formed minibatches of training data, where each data sample included the inputs to the network (the local context of Y , qt−1, and a random subset of qt) along with the true particle location sit, which served as the target output of the network.",4.4. Network architecture and training,[0],[0]
"We trained the network (using default learning rate settings in Keras) to minimize the binary cross-entropy between the target mask (zero ex-
cept at sit, or all zeros if all the particles in qt were already sampled and no further particles should be added) and the network’s output probability mask.",4.4. Network architecture and training,[0],[0]
Code is available here.,4.4. Network architecture and training,[0],[0]
We begin with a simple simulated experiment in which the particles are restricted to move in the horizontal direction only.,5.1. One-dimensional example,[0],[0]
"This makes it easier to view and understand the results, by simply plotting the horizontal positions of the (true vs. inferred) particles as a function of time.",5.1. One-dimensional example,[0],[0]
"The results are illustrated in Figure 2; the same data are shown in Figure 1 and the sampling process video.
",5.1. One-dimensional example,[0],[0]
"In this example we see the appearance and disappearance of a couple particles, and two “meeting” events in which one particle overlaps significantly with another particle.",5.1. One-dimensional example,[0],[0]
"Since in this example all the particles have identical shapes and are undergoing independent and identically distributed Brownian motions, there is no way to deterministically “link” particles before and after these meeting events; i.e., the “correct” linker here must output a probabilistic answer.
",5.1. One-dimensional example,[0],[0]
In panels 2-4 of Figure 2 we display three conditional sample paths drawn by our algorithm.,5.1. One-dimensional example,[0],[0]
Sample 0,5.1. One-dimensional example,[0],[0]
"(panel 2) recovers the ground truth accurately, and Sample 1 and 2 (panels 3 and 4) give different — but also valid — sets of tracks.",5.1. One-dimensional example,[0],[0]
"Panel 5 shows an average of 100 samples overlaid together, with the colors indicating relative probabilities of the chosen tracks.",5.1. One-dimensional example,[0],[0]
"Note that at the beginning of the trial, where the two visible particles are well-isolated, the sampler essentially outputs a deterministic estimate, with all samples assigned to the left (red) or the right (blue).",5.1. One-dimensional example,[0],[0]
"However, after the “meeting” near t = 15, the colors blend, indicating probabilistic assignment of tracks following this event, as desired.
",5.1. One-dimensional example,[0],[0]
"For comparison, we also show the output of two existing particle tracking methods, both of which output deterministic particle identities.",5.1. One-dimensional example,[0],[0]
"Our approach provides visibly more robust outputs on this example, with fewer dropped particle detections and false particle appearances or disappearances.",5.1. One-dimensional example,[0],[0]
"Next we turn to a small-scale simulated two-dimensional example; the results are illustrated in the moving particles video, Figure 3, and the 3D view video.",5.2. Two-dimensional example,[0],[0]
"As in the previous one-dimensional example, we find that our proposed approach accurately detects the particle locations and appearance/disappearance times, and successfully assigns identities probabilistically following particle meetings.",5.2. Two-dimensional example,[0],[0]
"To establish a more quantitative evaluation, we compared against two baseline methods: the popular Utrack approach (Jaqaman et al., 2008) and the method proposed in (Wilson et al., 2016), which performed well on the performance metrics established in the review / competition paper (Chenouard et al., 2014).",5.3. Large scale examples and evaluation,[0],[0]
"We generated large-scale twodimensional simulated data whose parameters matched a pair of challenging datasets in (Chenouard et al., 2014), and then computed the suite of performance metrics (measuring various facets of detection accuracy, linking quality, etc.) introduced in the same paper (averaging over 100 draws from our sampler for each dataset).",5.3. Large scale examples and evaluation,[0],[0]
"Results are shown in Table 1: we find that our proposed method outperforms the
baselines on both datasets examined, on all the performance metrics computed here.
",5.3. Large scale examples and evaluation,[0],[0]
"It is worth emphasizing that these performance metrics were designed for deterministic tracking algorithms, and therefore entirely miss one of the major advantages of our approach (the fact that it outputs not just a single “best” track estimate but instead estimates the posterior distribution over all tracks).",5.3. Large scale examples and evaluation,[0],[0]
How can we evaluate the quality of our approximation to the posterior here (and quantitatively compare between different algorithms that attempt to approximate this posterior)?,5.3. Large scale examples and evaluation,[0],[0]
One natural approach is to estimate the Kullback-Leibler divergence DKL[f(Q); p(Q|Y )] between our approximate posterior f(Q) and the true posterior p(Q|Y ) on the state space Q given the observed data Y .,5.3. Large scale examples and evaluation,[0],[0]
"Of
course, this is not quite tractable, due to the intractability of p(Q|Y ), but we can estimate DKL[f(Q); p(Q|Y )] up to a constant in f(Q) by sampling from f(Q):
DKL[f(Q); p(Q|Y )]",5.3. Large scale examples and evaluation,[0],[0]
"= Ef(Q) log f(Q)
p(Q|Y )
",5.3. Large scale examples and evaluation,[0],[0]
"= Ef(Q) log f(Q)
p(Q)p(Y |Q) + const[f(Q)]
",5.3. Large scale examples and evaluation,[0],[0]
≈ 1 N N∑ i=1,5.3. Large scale examples and evaluation,[0],[0]
log f(Qi) p(Qi)p(Y,5.3. Large scale examples and evaluation,[0],[0]
"|Qi) + const[f(Q)],
where {Qi)}i=1:N are N samples from f(Q).",5.3. Large scale examples and evaluation,[0],[0]
Here p(Qi) and p(Y |Qi) can be evaluated explicitly if the prior p(Q) is e.g. Markovian; for our approach f(Qi) can also be evaluated directly since f(Q) has an explicit Markov form.,5.3. Large scale examples and evaluation,[0],[0]
This provides us a method for scoring any Bayesian particle tracking algorithm for which we can explicitly evaluate the approximate posterior f(Q).,5.3. Large scale examples and evaluation,[0],[0]
"(We do not perform this scoring on the baselines examined here, since for any deterministic algorithm f(Q) is a delta function, leading to an infinite Kullback-Leibler score if we treat qt as a continuous random variable — i.e., the probabilistic approach trivially outperforms deterministic approaches.)",5.3. Large scale examples and evaluation,[0],[0]
"Finally, we tested the performance of our algorithm on real data.",5.4. Real data example,[0],[0]
"The data are TIR-FM imaged clathrin-coated pits in a BSC1 cell (Jaqaman et al., 2008).",5.4. Real data example,[0],[0]
"We trained the network on simulated data whose parameters (signal-to-noise ratio, particle density and speed, psf width, etc.) were coarsely matched to the real data; see the comparison video for details.",5.4. Real data example,[0],[0]
We plot three samples from our algorithm using different colors in Fig. 4 and the real data video.,5.4. Real data example,[0],[0]
"While
ground truth is unavailable in this case, by visual inspection the algorithm seems to effectively follow the particles in the video, without excessive oversegmentation of the tracks; the output here seems consistent with the behavior of the algorithm on the previous simulated datasets.",5.4. Real data example,[0],[0]
"In the introduction we emphasized the importance of the particle tracking problem; we believe that the more robust, accurate, and probabilistic tracking methods developed here will have a significant impact in a wide range of biological and physical applications.
",6.1. Related machine learning work,[0],[0]
"More generally, from a machine learning point of view, the major novelty of our work is the incorporation of neural network methods to provide a flexible and scalable approximation of Bayesian inference via efficient sampling in a large graphical model.
",6.1. Related machine learning work,[0],[0]
"Of course, interactions between Bayesian analysis and neural network methods comprise a very rich thread of research these days.",6.1. Related machine learning work,[0],[0]
"The work of (Snell & Zemel, 2017) is highly relevant: this paper describes a neural network approach to sample multiple segmentations that are consistent with an observed image, much as we use neural networks to sample multiple particle tracks that are consistent with an observed video.
",6.1. Related machine learning work,[0],[0]
"As another example, variational autoencoders (Kingma & Welling, 2013; Rezende et al., 2014) and variants thereof (Johnson et al., 2016; Gao et al., 2016; Fraccaro et al., 2017; Krishnan et al., 2017) have become very popular recently for performing inference in nonlinear HMMs.",6.1. Related machine learning work,[0],[0]
"These methods
are most effective when the latent state variable is lowdimensional.",6.1. Related machine learning work,[0],[0]
"In the particle tracking problem the latent dynamical variable is very high-d (scaling with the number of particles) and more importantly the latent dimensionality is time-varying, as particles are born, die, merge, split, enter, or leave the focal plane.",6.1. Related machine learning work,[0],[0]
"We are not aware of variational autoencoder approaches that would be easily applicable to the particle tracking problem.
",6.1. Related machine learning work,[0],[0]
"Another related thread involves amortized inference using neural networks for sequential Monte Carlo; see e.g. (Paige & Wood, 2016).",6.1. Related machine learning work,[0],[0]
"Again, it is not clear how well these methods would scale to the large-scale multiple-particle tracking problems of interest here.
",6.1. Related machine learning work,[0],[0]
"Finally, our work is an example of a broad theme in the current image processing literature: start with “ground truth” images, then simulate observed data that can be generated as some kind of corruption of this ground truth, and then use this simulated data to train a neural network that can “denoise” (or super-resolve, or deblur, or infill, etc.)",6.1. Related machine learning work,[0],[0]
this corruption.,6.1. Related machine learning work,[0],[0]
"A (highly non-exhaustive) list of recent examples includes: (Parthasarathy et al., 2017), which applies this idea to approximate Bayesian decoding of neuronal spike train data; (Yoon et al., 2017), to segmentation of threedimensional neuronal images; and (Weigert et al., 2017), to denoising of microscopy images.",6.1. Related machine learning work,[0],[0]
"At test time, as emphasized above, the inference approach proposed here is highly scalable, but the network training time is relatively slow (taking on the order of hours for the experiments presented here).",6.2. Future work,[0],[0]
This is typical of “amortized inference” approaches: we pay with relatively long training times for fast test times.,6.2. Future work,[0],[0]
"Thus our proposed approach is most valuable in settings where we have repeated experimental samples from a similar data regime (instead of training a new inference network for each new experimental dataset).
",6.2. Future work,[0],[0]
"We have not expended serious effort optimizing over network architectures here; we could likely find lighter architectures that perform similarly, which would speed up both testing and training.",6.2. Future work,[0],[0]
"Similarly, we could distill/compress the network to further speed up test times, if necessary e.g. for online experimental designs.
",6.2. Future work,[0],[0]
"Similarly, we have not yet attempted to develop automated procedures for choosing parameters for generating training data.",6.2. Future work,[0],[0]
"In practice we have found that these parameters (e.g., the amplitude, density, variance/speed of particles, plus noise levels, point-spread width, etc.) are fairly straightforward to choose, and the inference results are not highly sensitive to small misspecifications of these parameters (recall Figure 4 and the corresponding comparison video).",6.2. Future work,[0],[0]
"It would be useful to develop a simple interface that would allow experimentalists to easily generate training data, followed by generation of a network trained to perform inference on their data.
",6.2. Future work,[0],[0]
An alternative approach would be to include data parameters as extra inputs for the network.,6.2. Future work,[0],[0]
Then in principle there would be no need to train a new network for each new type of data; instead we could perhaps just train a single big network on many different data types (with the corresponding data parameters included as inputs to the network) and then when presented with a new datatype we just provide the network with the required parameters and let it perform inference.,6.2. Future work,[0],[0]
"This is an ambitious but important direction for future work3.
3Note that a slightly different philosophy is espoused in (Newby et al., 2017), who trained a single deterministic network for particle detection that can be applied to a wide range of data, but without including any parameters describing the data generation mechanism as inputs to the network.",6.2. Future work,[0],[0]
"This approach makes it easy for experimentalists to use the network (since no training or parameter estimation is required), but likely sacrifices some accuracy compared to a network that is provided information about the parameters governing the generation of the data.",6.2. Future work,[0],[0]
We hope to run more detailed comparisons of these approaches in the future.,6.2. Future work,[0],[0]
"This work was funded by Army Research Office W911NF12-1-0594 (MURI), the Simons Foundation Collaboration on the Global Brain, and by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/ Interior Business Center (DoI/IBC) contract number D16PC00008.",Acknowledgements,[0],[0]
"The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.",Acknowledgements,[0],[0]
"Many important datasets in physics, chemistry, and biology consist of noisy sequences of images of multiple moving overlapping particles.",abstractText,[0],[0]
"In many cases, the observed particles are indistinguishable, leading to unavoidable uncertainty about nearby particles’ identities.",abstractText,[0],[0]
"Exact Bayesian inference is intractable in this setting, and previous approximate Bayesian methods scale poorly.",abstractText,[0],[0]
Non-Bayesian approaches that output a single “best” estimate of the particle tracks (thus discarding important uncertainty information) are therefore dominant in practice.,abstractText,[0],[0]
Here we propose a flexible and scalable amortized approach for Bayesian inference on this task.,abstractText,[0],[0]
We introduce a novel neural network method to approximate the (intractable) filter-backward-sample-forward algorithm for Bayesian inference in this setting.,abstractText,[0],[0]
"By varying the simulated training data for the network, we can perform inference on a wide variety of data types.",abstractText,[0],[0]
"This approach is therefore highly flexible and improves on the state of the art in terms of accuracy; provides uncertainty estimates about the particle locations and identities; and has a test run-time that scales linearly as a function of the data length and number of particles, thus enabling Bayesian inference in arbitrarily large particle tracking datasets.",abstractText,[0],[0]
Scalable Approximate Bayesian Inference for Particle Tracking Data,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 321–331 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1030",text,[0],[0]
"Language modeling is a fundamental task, used for example to predict the next word or character in a text sequence given the context.",1 Introduction,[0],[0]
"Recently, recurrent neural networks (RNNs) have shown promising performance on this task (Mikolov et al., 2010; Sutskever et al., 2011).",1 Introduction,[0],[0]
"RNNs with Long Short-Term Memory (LSTM) units (Hochreiter and Schmidhuber, 1997) have emerged as a popular architecture, due to their representational power and effectiveness at capturing long-term dependencies.
",1 Introduction,[0],[0]
"RNNs are usually trained via back-propagation through time (Werbos, 1990), using stochastic op-
∗Equal contribution.",1 Introduction,[0],[0]
"†Corresponding author.
",1 Introduction,[0],[0]
"timization methods such as stochastic gradient descent (SGD) (Robbins and Monro, 1951); stochastic methods of this type are particularly important for training with large data sets.",1 Introduction,[0],[0]
"However, this approach often provides a maximum a posteriori (MAP) estimate of model parameters.",1 Introduction,[0],[0]
"The MAP solution is a single point estimate, ignoring weight uncertainty (Blundell et al., 2015; HernándezLobato and Adams, 2015).",1 Introduction,[0],[0]
"Natural language often exhibits significant variability, and hence such a point estimate may make over-confident predictions on test data.
",1 Introduction,[0],[0]
"To alleviate overfitting RNNs, good regularization is known as a key factor to successful applications.",1 Introduction,[0],[0]
"In the neural network literature, Bayesian learning has been proposed as a principled method to impose regularization and incorporate model uncertainty (MacKay, 1992; Neal, 1995), by imposing prior distributions on model parameters.",1 Introduction,[0],[0]
"Due to the intractability of posterior distributions in neural networks, Hamiltonian Monte Carlo (HMC) (Neal, 1995) has been used to provide sample-based approximations to the true posterior.",1 Introduction,[0],[0]
"Despite the elegant theoretical property of asymptotic convergence to the true posterior, HMC and other conventional Markov Chain Monte Carlo methods are not scalable to large training sets.
",1 Introduction,[0],[0]
"This paper seeks to scale up Bayesian learning of RNNs to meet the challenge of the increasing amount of “big” sequential data in natural language processing, leveraging recent advances in stochastic gradient Markov Chain Monte Carlo (SG-MCMC) algorithms (Welling and Teh, 2011; Chen et al., 2014; Ding et al., 2014; Li et al., 2016a,b).",1 Introduction,[0],[0]
"Specifically, instead of training a single network, SG-MCMC is employed to train an ensemble of networks, where each network has its parameters drawn from a shared posterior distribution.",1 Introduction,[0],[0]
"This is implemented by adding additional
321
gradient noise during training and utilizing model averaging when testing.
",1 Introduction,[0],[0]
"This simple procedure has the following salutary properties for training neural networks: (i) When training, the injected noise encourages model-parameter trajectories to better explore the parameter space.",1 Introduction,[0],[0]
This procedure was also empirically found effective in Neelakantan et al. (2016).,1 Introduction,[0],[0]
"(ii) Model averaging when testing alleviates overfitting and hence improves generalization, transferring uncertainty in the learned model parameters to subsequent prediction.",1 Introduction,[0],[0]
"(iii) In theory, both asymptotic and non-asymptotic consistency properties of SG-MCMC methods in posterior estimation have been recently established to guarantee convergence (Chen et al., 2015a; Teh et al., 2016).",1 Introduction,[0],[0]
"(iv) SG-MCMC is scalable; it shares the same level of computational cost as SGD in training, by only requiring the evaluation of gradients on a small mini-batch.",1 Introduction,[0],[0]
"To the authors’ knowledge, RNN training using SG-MCMC has not been investigated previously, and is a contribution of this paper.",1 Introduction,[0],[0]
"We also perform extensive experiments on several natural language processing tasks, demonstrating the effectiveness of SG-MCMC for RNNs, including character/word-level language modeling, image captioning and sentence classification.",1 Introduction,[0],[0]
Several scalable Bayesian learning methods have been proposed recently for neural networks.,2 Related Work,[0],[0]
"These come in two broad categories: stochastic variational inference (Graves, 2011; Blundell et al., 2015; Hernández-Lobato and Adams, 2015) and
SG-MCMC methods (Korattikara et al., 2015; Li et al., 2016a).",2 Related Work,[0],[0]
"While prior work focuses on feed-forward neural networks, there has been little if any research reported for RNNs using SGMCMC.
",2 Related Work,[0],[0]
"Dropout (Hinton et al., 2012; Srivastava et al., 2014) is a commonly used regularization method for training neural networks.",2 Related Work,[0],[0]
"Recently, several works have studied how to apply dropout to RNNs (Pachitariu and Sahani, 2013; Bayer et al., 2013; Pham et al., 2014; Zaremba et al., 2014; Bluche et al., 2015; Moon et al., 2015; Semeniuta et al., 2016; Gal and Ghahramani, 2016b).",2 Related Work,[0],[0]
"Among them, naive dropout (Zaremba et al., 2014) can impose weight uncertainty only on encoding weights (those that connect input to hidden units) and decoding weights (those that connect hidden units to output), but not the recurrent weights (those that connect consecutive hidden states).",2 Related Work,[0],[0]
"It has been concluded that noise added in the recurrent connections leads to model instabilities, hence disrupting the RNN’s ability to model sequences.
",2 Related Work,[0],[0]
"Dropout has been recently shown to be a variational approximation technique in Bayesian learning (Gal and Ghahramani, 2016a; Kingma et al., 2015).",2 Related Work,[0],[0]
"Based on this, (Gal and Ghahramani, 2016b) proposed a new variant of dropout that can be successfully applied to recurrent layers, where the same dropout masks are shared along time for encoding, decoding and recurrent weights, respectively.",2 Related Work,[0],[0]
"Alternatively, we focus on SG-MCMC, which can be viewed as the Bayesian interpretation of dropout from the perspective of posterior sampling (Li et al., 2016c); this also allows imposition of model uncertainty on recurrent layers, enhancing performance.",2 Related Work,[0],[0]
A comparison of naive dropout and SG-MCMC is illustrated in Fig. 1.,2 Related Work,[0],[0]
"Consider data D = {D1, · · · ,DN}, where Dn , (Xn,Yn), with input Xn and output Yn.",3.1 RNN as Bayesian Predictive Models,[0],[0]
"Our goal is to learn model parameters θ to best characterize the relationship from Xn to Yn, with corresponding data likelihood p(D|θ) =∏N n=1 p(Dn|θ).",3.1 RNN as Bayesian Predictive Models,[0],[0]
"In Bayesian statistics, one sets a prior on θ via distribution p(θ).",3.1 RNN as Bayesian Predictive Models,[0],[0]
The posterior p(θ|D) ∝ p(θ)p(D|θ) reflects the belief concerning the model parameter distribution after observing the data.,3.1 RNN as Bayesian Predictive Models,[0],[0]
"Given a test input X̃ (with missing output Ỹ), the uncertainty learned in training
is transferred to prediction, yielding the posterior predictive distribution:
p(Ỹ|X̃,D)= ∫
θ p(Ỹ|X̃,θ)p(θ|D)dθ .",3.1 RNN as Bayesian Predictive Models,[0],[0]
"(1)
When the input is a sequence, RNNs may be used to parameterize the input-output relationship.",3.1 RNN as Bayesian Predictive Models,[0],[0]
"Specifically, consider input sequence X = {x1, . . .",3.1 RNN as Bayesian Predictive Models,[0],[0]
",xT }, where xt is the input data vector at time t.",3.1 RNN as Bayesian Predictive Models,[0],[0]
"There is a corresponding hidden state vector ht at each time t, obtained by recursively applying the transition function ht = H(ht−1,xt) (specified in Section 3.2; see Fig. 1).",3.1 RNN as Bayesian Predictive Models,[0],[0]
"The output Y differs depending on the application: a sequence {y1, . . .",3.1 RNN as Bayesian Predictive Models,[0],[0]
",yT } in language modeling or a discrete label in sentence classification.",3.1 RNN as Bayesian Predictive Models,[0],[0]
"In RNNs the corresponding decoding function is p(y|h), described in Section 3.3.",3.1 RNN as Bayesian Predictive Models,[0],[0]
"The transition function H(·) can be implemented with a gated activation function, such as Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) or a Gated Recurrent Unit (GRU) (Cho et al., 2014).",3.2 RNN Architectures,[0],[0]
"Both the LSTM and GRU have been proposed to address the issue of learning long-term sequential dependencies.
",3.2 RNN Architectures,[0],[0]
"Long Short-Term Memory The LSTM architecture addresses the problem of learning longterm dependencies by introducing a memory cell, that is able to preserve the state over long periods of time.",3.2 RNN Architectures,[0],[0]
"Specifically, each LSTM unit has a cell containing a state ct at time t. This cell can be viewed as a memory unit.",3.2 RNN Architectures,[0],[0]
"Reading or writing the cell is controlled through sigmoid gates: input gate it, forget gate ft, and output gate ot.",3.2 RNN Architectures,[0],[0]
"The hidden units ht are updated as
it = σ(Wixt +Uiht−1 + bi) ,
ft = σ(Wfxt",3.2 RNN Architectures,[0],[0]
"+Ufht−1 + bf ) ,
ot = σ(Woxt",3.2 RNN Architectures,[0],[0]
"+Uoht−1 + bo) ,
",3.2 RNN Architectures,[0],[0]
"c̃t = tanh(Wcxt +Ucht−1 + bc) ,
ct = ft ct−1 + it c̃t , ht = ot tanh(ct) ,
where σ(·) denotes the logistic sigmoid function, and represents the element-wise matrix multiplication operator.",3.2 RNN Architectures,[0],[0]
"W{i,f,o,c} are encoding weights, and U{i,f,o,c} are recurrent weights, as shown in Fig. 1.",3.2 RNN Architectures,[0],[0]
"b{i,f,o,c} are bias terms.
",3.2 RNN Architectures,[0],[0]
"Variants Similar to the LSTM unit, the GRU also has gating units that modulate the flow of information inside the hidden unit.",3.2 RNN Architectures,[0],[0]
"It has been shown that a GRU can achieve similar performance to an LSTM in sequence modeling (Chung et al., 2014).",3.2 RNN Architectures,[0],[0]
"We specify the GRU in the Supplementary Material.
",3.2 RNN Architectures,[0],[0]
The LSTM can be extended to the bidirectional LSTM and multilayer LSTM.,3.2 RNN Architectures,[0],[0]
A bidirectional LSTM consists of two LSTMs that are run in parallel: one on the input sequence and the other on the reverse of the input sequence.,3.2 RNN Architectures,[0],[0]
"At each time step, the hidden state of the bidirectional LSTM is the concatenation of the forward and backward hidden states.",3.2 RNN Architectures,[0],[0]
"In multilayer LSTMs, the hidden state of an LSTM unit in layer ` is used as input to the LSTM unit in layer `",3.2 RNN Architectures,[0],[0]
"+ 1 at the same time step (Graves, 2013).",3.2 RNN Architectures,[0],[0]
"The proposed Bayesian framework can be applied to any RNN model; we focus on the following tasks to demonstrate the ideas.
",3.3 Applications,[0],[0]
"Language Modeling In word-level language modeling, the input to the network is a sequence of words, and the network is trained to predict the next word in the sequence with a softmax classifier.",3.3 Applications,[0],[0]
"Specifically, for a length-T sequence, denote yt = xt+1 for t = 1, . . .",3.3 Applications,[0],[0]
", T − 1.",3.3 Applications,[0],[0]
"x1 and yT are always set to a special START and END token, respectively.",3.3 Applications,[0],[0]
"At each time t, there is a decoding function p(yt|ht) = softmax(Vht) to compute the distribution over words, where V are the decoding weights (the number of rows of V corresponds to the number of words/characters).",3.3 Applications,[0],[0]
"We also extend this basic language model to consider other applications: (i) a character-level language model can be specified in a similar manner by replacing words with characters (Karpathy et al., 2016).",3.3 Applications,[0],[0]
"(ii) Image captioning can be considered as a conditional language modeling problem, in which we learn a generative language model of the caption conditioned on an image (Vinyals et al., 2015; Gan et al., 2017).
",3.3 Applications,[0],[0]
Sentence Classification Sentence classification aims to assign a semantic category label y to a whole sentence X.,3.3 Applications,[0],[0]
This is usually implemented through applying the decoding function once at the end of sequence: p(y|hT ) =,3.3 Applications,[0],[0]
"softmax(VhT ), where the final hidden state of a RNN hT is often considered as the summary of the sentence (here
the number of rows of V corresponds to the number of classes).",3.3 Applications,[0],[0]
"Typically there is no closed-form solution for the posterior p(θ|D), and traditional Markov Chain Monte Carlo (MCMC) methods (Neal, 1995) scale poorly for largeN .",4.1 The Pitfall of Stochastic Optimization,[0],[0]
"To ease the computational burden, stochastic optimization is often employed to find the MAP solution.",4.1 The Pitfall of Stochastic Optimization,[0],[0]
"This is equivalent to minimizing an objective of regularized loss function U(θ) that corresponds to a (non-convex) model of interest: θMAP = argminU(θ), U(θ) =",4.1 The Pitfall of Stochastic Optimization,[0],[0]
− log p(θ|D).,4.1 The Pitfall of Stochastic Optimization,[0],[0]
"The expectation in (1) is approximated as:
p(Ỹ|X̃,D)= p(Ỹ|X̃,θMAP) .",4.1 The Pitfall of Stochastic Optimization,[0],[0]
"(2)
Though simple and effective, this procedure largely loses the benefit of the Bayesian approach, because the uncertainty on weights is ignored.",4.1 The Pitfall of Stochastic Optimization,[0],[0]
"To more accurately approximate (1), we employ stochastic gradient (SG) MCMC (Welling and Teh, 2011).",4.1 The Pitfall of Stochastic Optimization,[0],[0]
"The negative log-posterior is
U(θ) , − log p(θ)− N∑
n=1
log p(Dn|θ).",4.2 Large-scale Bayesian Learning,[0],[0]
"(3)
In optimization,E = −∑Nn=1 log p(Dn|θ) is typically referred to as the loss function, and R ∝",4.2 Large-scale Bayesian Learning,[0],[0]
"− log p(θ) as a regularizer.
",4.2 Large-scale Bayesian Learning,[0],[0]
"For large N , stochastic approximations are often employed:
Ũt(θ),− log p(θ)− N
M
M∑
m=1
log p(Dim |θ), (4)
where Sm = {i1, · · · , iM} is a random subset of the set {1, 2, · · · , N}, with M N .",4.2 Large-scale Bayesian Learning,[0],[0]
"The gradient on this mini-batch is denoted as f̃t = ∇Ũt(θ), which is an unbiased estimate of the true gradient.",4.2 Large-scale Bayesian Learning,[0],[0]
"The evaluation of (4) is cheap even when N is large, allowing one to efficiently collect a sufficient number of samples in large-scale Bayesian learning, {θs}Ss=1, where S is the number of samples (this will be specified later).",4.2 Large-scale Bayesian Learning,[0],[0]
"These samples are used to construct a sample-based estimation to the expectation in (1):
The finite-time estimation errors of SG-MCMC methods are bounded (Chen et al., 2015a), which guarantees (5) is an unbiased estimate of (1) asymptotically under appropriate decreasing stepsizes.",4.2 Large-scale Bayesian Learning,[0],[0]
"SG-MCMC and stochastic optimization are parallel lines of work, designed for different purposes; their relationship has recently been revealed in the context of deep learning.",4.3 SG-MCMC Algorithms,[0],[0]
"The most basic SG-MCMC algorithm has been applied to Langevin dynamics, and is termed SGLD (Welling and Teh, 2011).",4.3 SG-MCMC Algorithms,[0],[0]
"To help convergence, a momentum term has been introduced in SGHMC (Chen et al., 2014), a “thermostat” has been devised in SGNHT (Ding et al., 2014; Gan et al., 2015) and preconditioners have been employed in pSGLD (Li et al., 2016a).",4.3 SG-MCMC Algorithms,[0],[0]
"These SG-MCMC algorithms often share similar characteristics with their counterpart approaches from the optimization literature such as the momentum SGD, Santa (Chen et al., 2016) and RMSprop/Adagrad (Tieleman and Hinton, 2012; Duchi et al., 2011).",4.3 SG-MCMC Algorithms,[0],[0]
"The interrelationships between SG-MCMC and optimizationbased approaches are summarized in Table 1.
",4.3 SG-MCMC Algorithms,[0],[0]
"SGLD Stochastic Gradient Langevin Dynamics (SGLD) (Welling and Teh, 2011) draws posterior samples, with updates
θt = θt−1 − ηtf̃t−1 + √ 2ηtξt , (6)
where ηt is the learning rate, and ξt ∼ N (0, Ip) is a standard Gaussian random vector.",4.3 SG-MCMC Algorithms,[0],[0]
"SGLD is the SG-MCMC analog to stochastic gradient descent (SGD), whose parameter updates are given by:
θt = θt−1 − ηtf̃t−1 .",4.3 SG-MCMC Algorithms,[0],[0]
"(7)
Algorithm 1:",4.3 SG-MCMC Algorithms,[0],[0]
"pSGLD Input: Default hyperparameter settings:
ηt = 1×10−3, λ = 10−8, β1 = 0.99.",4.3 SG-MCMC Algorithms,[0],[0]
"Initialize: v0 ← 0, θ1 ∼ N (0, I) ; for t = 1, 2, . . .",4.3 SG-MCMC Algorithms,[0],[0]
", T do
% Estimate gradient from minibatch St f̃t = ∇Ũt(θ);",4.3 SG-MCMC Algorithms,[0],[0]
"% Preconditioning vt ← β1vt−1 + (1− β1)f̃t f̃t; G−1t ← diag ( 1 ( λ1+ v 1 2 t )) ;
% Parameter update ξt ∼ N",4.3 SG-MCMC Algorithms,[0],[0]
"(0, ηtG−1t ); θt+1← θt + ηt2",4.3 SG-MCMC Algorithms,[0],[0]
"G−1t f̃t+ ξt;
end
SGD is guaranteed to converge to a local minimum under mild conditions (Bottou, 2010).",4.3 SG-MCMC Algorithms,[0],[0]
"The additional Gaussian term in SGLD helps the learning trajectory to explore the parameter space to approximate posterior samples, instead of obtaining a local minimum.
",4.3 SG-MCMC Algorithms,[0],[0]
"pSGLD Preconditioned SGLD (pSGLD) (Li et al., 2016a) was proposed recently to improve the mixing of SGLD.",4.3 SG-MCMC Algorithms,[0],[0]
"It utilizes magnitudes of recent gradients to construct a diagonal preconditioner to approximate the Fisher information matrix, and thus adjusts to the local geometry of parameter space by equalizing the gradients so that a constant stepsize is adequate for all dimensions.",4.3 SG-MCMC Algorithms,[0],[0]
"This is important for RNNs, whose parameter space often exhibits pathological curvature and saddle points (Pascanu et al., 2013), resulting in slow mixing.",4.3 SG-MCMC Algorithms,[0],[0]
"There are multiple choices of preconditioners; similar ideas in optimization include Adagrad (Duchi et al., 2011), Adam (Kingma and Ba, 2015) and RMSprop (Tieleman and Hinton, 2012).",4.3 SG-MCMC Algorithms,[0],[0]
"An efficient version of pSGLD, adopting RMSprop as the preconditioner G, is summarized in Algorithm 1, where denotes elementwise matrix division.",4.3 SG-MCMC Algorithms,[0],[0]
"When the preconditioner is fixed as the identity matrix, the method reduces to SGLD.",4.3 SG-MCMC Algorithms,[0],[0]
"To further understand SG-MCMC, we show its close connection to dropout/dropConnect (Srivastava et al., 2014; Wan et al., 2013).",4.4 Understanding SG-MCMC,[0],[0]
"These methods improve the generalization ability of deep models, by randomly adding binary/Gaussian noise to the
local units or global weights.",4.4 Understanding SG-MCMC,[0],[0]
"For neural networks with the nonlinear function q(·) and consecutive layers h1 and h2, dropout and dropConnect are denoted as:
Dropout: h2 = ξ0 q(θh1), DropConnect: h2 = q((ξ0 θ)h1),
where the injected noise ξ0 can be binary-valued with dropping rate p or its equivalent Gaussian form (Wang and Manning, 2013):
Binary noise: ξ0 ∼ Ber(p), Gaussian noise: ξ0 ∼ N (1, p
1− p).
",4.4 Understanding SG-MCMC,[0],[0]
"Note that ξ0 is defined as a vector for dropout, and a matrix for dropConnect.",4.4 Understanding SG-MCMC,[0],[0]
"By combining dropConnect and Gaussian noise from the above, we have the update rule (Li et al., 2016c):
θt+1 = ξ0 θt",4.4 Understanding SG-MCMC,[0],[0]
"− η
2 f̃t = θt −
",4.4 Understanding SG-MCMC,[0],[0]
"η 2 f̃t + ξ ′ 0 , (8)
where ξ′0 ∼ N ( 0, p(1−p)diag(θ 2 t ) )
",4.4 Understanding SG-MCMC,[0],[0]
"; (8) shows that dropout/ dropConnect and SGLD in (6) share the same form of update rule, with the distinction being that the level of injected noise is different.",4.4 Understanding SG-MCMC,[0],[0]
"In practice, the noise injected by SGLD may not be enough.",4.4 Understanding SG-MCMC,[0],[0]
A better way that we find to improve the performance is to jointly apply SGLD and dropout.,4.4 Understanding SG-MCMC,[0],[0]
"This method can be interpreted as using SGLD to sample the posterior distribution of a mixture of RNNs, with mixture probability controlled by the dropout rate.",4.4 Understanding SG-MCMC,[0],[0]
"We present results on several tasks, including character/word-level language modeling, image captioning, and sentence classification.",5 Experiments,[0],[0]
We do not perform any dataset-specific tuning other than early stopping on validation sets.,5 Experiments,[0],[0]
"When dropout is utilized, the dropout rate is set to 0.5.",5 Experiments,[0],[0]
"All experiments are implemented in Theano (Theano Development Team, 2016), using a NVIDIA GeForce GTX TITAN X GPU with 12GB memory.
",5 Experiments,[0],[0]
"The hyper-parameters for the proposed algorithm include step size, minibatch size, thinning interval, number of burn-in epochs and variance of the Gaussian priors.",5 Experiments,[0],[0]
We list the specific values used in our experiments in Table 2.,5 Experiments,[0],[0]
"The explanation of these hyperparameters, the initialization of model parameters and model specifications on each dataset are provided in the Supplementary Material.",5 Experiments,[0],[0]
We first test character-level and word-level language modeling.,5.1 Language Modeling,[0],[0]
"The setup is as follows.
",5.1 Language Modeling,[0],[0]
"• Following Karpathy et al. (2016), we test character-level language modeling on the War and Peace (WP) novel.",5.1 Language Modeling,[0],[0]
"The training/validation/test sets contain 260/32/33 batches, in which there are 100 characters.",5.1 Language Modeling,[0],[0]
"The vocabulary size is 87, and we consider a 2-hidden-layer RNN of dimension 128.",5.1 Language Modeling,[0],[0]
"• The Penn Treebank (PTB) corpus (Marcus
et al., 1993) is used for word-level language modeling.",5.1 Language Modeling,[0],[0]
"The dataset adopts the standard split (929K training words, 73K validation words, and 82K test words) and has a vocabulary of size 10K. We train LSTMs of three sizes; these are denoted the small/medium/large LSTM.",5.1 Language Modeling,[0],[0]
All LSTMs have two layers and are unrolled for 20 steps.,5.1 Language Modeling,[0],[0]
"The small, medium and large LSTM has 200, 650 and 1500 units per layer, respectively.
",5.1 Language Modeling,[0],[0]
We consider two types of training schemes on PTB corpus: (i) Successive minibatches:,5.1 Language Modeling,[0],[0]
"Following Zaremba et al. (2014), the final hidden states of the current minibatch are used as the initial hidden states of the subsequent minibatch (successive minibatches sequentially traverse the training set).",5.1 Language Modeling,[0],[0]
"(ii) Random minibatches: The initial hidden states of each minibatch are set to zero vectors, hence we can randomly sample minibatches in each update.
",5.1 Language Modeling,[0],[0]
"We study the effects of different types of architecture (LSTM/GRU/Vanilla RNN (Karpathy et al., 2016)) on the WP dataset, and effects of different learning algorithms on the PTB dataset.",5.1 Language Modeling,[0],[0]
The comparison of test cross-entropy loss on WP is shown in Table 3.,5.1 Language Modeling,[0],[0]
We observe that pSGLD consistently outperforms RMSprop.,5.1 Language Modeling,[0],[0]
Table 4 summarizes the test set performance on PTB1.,5.1 Language Modeling,[0],[0]
"It is clear
1The results reported here do not match Zaremba et al. (2014) due to the implementation details.",5.1 Language Modeling,[0],[0]
"However, we pro-
that our sampling-based method consistently outperforms the optimization counterpart, where the performance gain mainly comes from adding gradient noise and model averaging.",5.1 Language Modeling,[0],[0]
"When compared with dropout, SGLD performs better on the small LSTM model, but worse on the medium and large LSTM model.",5.1 Language Modeling,[0],[0]
"This may imply that dropout is suitable to regularizing large networks, while SGLD exhibits better regularization ability on small networks, partially due to the fact that dropout may inject a higher level of noise during training than SGLD.",5.1 Language Modeling,[0],[0]
"In order to inject a higher level of noise into SGLD, we empirically apply SGLD and dropout jointly, and found that this provided the best performace on the medium and large LSTM model.
",5.1 Language Modeling,[0],[0]
"We study three strategies to do model averaging, i.e., forward collection, backward collection and thinned collection.",5.1 Language Modeling,[0],[0]
"Given samples (θ1, · · · ,θK) and the number of samples S used for averaging, forward collection refers to using (θ1, · · · ,θS) for the evaluation of a test function, backward collection refers to using (θK−S+1, · · · ,θK), while thinned collection chooses samples from θ1 to θK with interval K/S. Fig. 2 plots the effects of these strategies, where Fig. 2(a) plots the perplexity of every single sample, Fig. 2(b) plots the perplexities using the three schemes.",5.1 Language Modeling,[0],[0]
"Only after 20
vide a fair comparison to all methods.
samples is a converged perplexity achieved in the thinned collection, while it requires 30 samples for forward collection or 60 samples for backward collection.",5.1 Language Modeling,[0],[0]
"This is unsurprising, because thinned collection provides a better way to select samples.",5.1 Language Modeling,[0],[0]
"Nevertheless, averaging of samples provides significantly lower perplexity than using single samples.",5.1 Language Modeling,[0],[0]
"Note that the overfitting problem in Fig. 2(a) is also alleviated by model averaging.
",5.1 Language Modeling,[0],[0]
"To better illustrate the benefit of model averaging, we visualize in Fig. 3 the probabilities of each word in a randomly chosen test sentence.",5.1 Language Modeling,[0],[0]
"The first 3 rows are the results predicted by 3 distinctive model samples, respectively; the bottom row is the result after averaging.",5.1 Language Modeling,[0],[0]
Their corresponding perplexities for the test sentence are also shown on the right of each row.,5.1 Language Modeling,[0],[0]
The 3 individual samples provide reasonable probabilities.,5.1 Language Modeling,[0],[0]
"For example, the consecutive words “New York”, “stock exchange” and “did not” are assigned with a higher probability.",5.1 Language Modeling,[0],[0]
"After averaging, we can see a much lower perplexity, as the samples can complement each other.",5.1 Language Modeling,[0],[0]
"For example, though the second sample can yield the lowest single-model perplexity, its prediction on word “York” is still benefited from the other two via averaging.",5.1 Language Modeling,[0],[0]
"We next consider the problem of image caption generation, which is a conditional RNN model, where image features are extracted by residual network (He et al., 2016), and then fed into the RNN to generate the caption.",5.2 Image Caption Generation,[0],[0]
"We present results on two benchmark datasets, Flickr8k (Hodosh et al., 2013) and Flickr30k (Young et al., 2014).",5.2 Image Caption Generation,[0],[0]
"These
25.55the new york stock exchange did not fall apart 22.24the new york stock exchange did not fall apart 29.83the new york stock exchange did not fall apart
21.98the new york stock exchange did not fall apart 0
0.2
0.4
0.6
0.8
Figure 3: Predictive probabilities obtained by 3 samples and their average.",5.2 Image Caption Generation,[0],[0]
Colors indicate normalized probability of each word.,5.2 Image Caption Generation,[0],[0]
"Best viewed in color.
",5.2 Image Caption Generation,[0],[0]
"a""tan""dog""is""playing""in""the""grass a""tan""dog""is""playing""with""a""red""ball""in""the""grass a""tan""dog""with""a""red""collar""is""running""in""the""grass
a""yellow""dog""runs""through""the""grass a""yellow""dog""is""running""through""the""grass a""brown""dog""is""running""through""the""grass
a""group""of""people""stand""in""front""of""a""building a""group""of""people""stand""in""front""of""a""white""building a""group""of""people""stand""in""front""of""a""large""building
a""man""and""a""woman""walking""on""a""sidewalk",5.2 Image Caption Generation,[0],[0]
"a""man""and""a""woman""stand""on""a""balcony a""man""and""a""woman""standing""on""the""ground
Figure 4: Image captioning with different samples.",5.2 Image Caption Generation,[0],[0]
"Left are the given images, right are the corresponding captions.",5.2 Image Caption Generation,[0],[0]
"The captions in each box are from the same model sample.
",5.2 Image Caption Generation,[0],[0]
"datasets contain 8,000 and 31,000 images, respectively.",5.2 Image Caption Generation,[0],[0]
Each image is annotated with 5 sentences.,5.2 Image Caption Generation,[0],[0]
"A single-layer LSTM is employed with the number of hidden units set to 512.
",5.2 Image Caption Generation,[0],[0]
"The widely used BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), ROUGEL (Lin, 2004), and CIDEr-D (Vedantam et al., 2015) metrics are used to evaluate the performance.",5.2 Image Caption Generation,[0],[0]
"All the metrics are computed by using the code released by the COCO evaluation server (Chen et al., 2015b).
",5.2 Image Caption Generation,[0],[0]
"Table 5 presents results for pSGLD/RMSprop
Table 5: Performance on Flickr8k & Flickr30k:",5.2 Image Caption Generation,[0],[0]
"BLEU’s, METEOR, CIDEr, ROUGE-L and perplexity.
",5.2 Image Caption Generation,[0],[0]
Methods B-1 B-2 B-3 B-4 METEOR CIDEr ROUGE-L Perp.,5.2 Image Caption Generation,[0],[0]
Results on Flickr8k RMSprop 0.640 0.427 0.288 0.197 0.205 0.476 0.500 16.64 RMSprop + Dropout 0.647 0.444 0.305 0.209 0.208 0.514 0.510 15.72,5.2 Image Caption Generation,[0],[0]
"RMSprop + Gal’s Dropout 0.651 0.443 0.305 0.209 0.206 0.501 0.509 14.70 pSGLD 0.669 0.463 0.321 0.224 0.214 0.535 0.522 14.29 pSGLD + Dropout 0.656 0.450 0.309 0.211 0.209 0.512 0.512 14.26 Results on Flickr30k RMSprop 0.644 0.422 0.279 0.184 0.180 0.372 0.476 17.80 RMSprop + Dropout 0.656 0.435 0.295 0.200 0.185 0.396 0.481 18.05 RMSprop + Gal’s Dropout 0.636 0.429 0.290 0.197 0.190 0.408 0.480 17.27 pSGLD 0.657 0.438 0.300 0.206 0.192 0.421 0.490 15.61 pSGLD + Dropout 0.666 0.448 0.308 0.209 0.189 0.419 0.487 17.05
with or without dropout.",5.2 Image Caption Generation,[0],[0]
"In addition to (naive) dropout, we further compare pSGLD with the Gal’s dropout, recently proposed in Gal and Ghahramani (2016b), which is shown to be applicable to recurrent layers.",5.2 Image Caption Generation,[0],[0]
"Consistent with the results in the basic language modeling, pSGLD yields improved performance compared to RMSprop.",5.2 Image Caption Generation,[0],[0]
"For example, pSGLD provides 2.7 BLEU-4 score improvement over RMSprop on the Flickr8k dataset.",5.2 Image Caption Generation,[0],[0]
"By comparing pSGLD with RMSprop with dropout, we conclude that pSGLD exhibits better regularization ability than dropout on these two datasets.
",5.2 Image Caption Generation,[0],[0]
"Apart from modeling weight uncertainty, different samples from our algorithm may capture different aspects of the input image.",5.2 Image Caption Generation,[0],[0]
"An example with two images is shown in Fig. 4, where 2 randomly chosen model samples are considered for each image.",5.2 Image Caption Generation,[0],[0]
"For each model sample, the top 3 generated captions are presented.",5.2 Image Caption Generation,[0],[0]
"We use the beam search approach (Vinyals et al., 2015) to generate captions, with a beam of size 5.",5.2 Image Caption Generation,[0],[0]
"In Fig. 4, the two samples for the first image mainly differ in the color and activity of the dog, e.g., “tan” or “yellow”, “playing” or “running”, whereas for the second image, the two samples reflect different understanding of the image content.",5.2 Image Caption Generation,[0],[0]
"We study the task of sentence classification on 5 datasets as in Kiros et al. (2015): MR (Pang and Lee, 2005), CR (Hu and Liu, 2004), SUBJ (Pang and Lee, 2004), MPQA (Wiebe et al., 2005) and TREC (Li and Roth, 2002).",5.3 Sentence Classification,[0],[0]
A single-layer bidirectional LSTM is employed with the number of hidden units set to 400.,5.3 Sentence Classification,[0],[0]
"Table 6 shows the test-
5 10 15 #Epoch
0.00
0.05
0.10
0.15
0.20
0.25
Er ro
r
Train RMSprop RMSprop + Dropout pSGLD pSGLD +",5.3 Sentence Classification,[0],[0]
"Dropout
5 10 15 #Epoch
0.10
0.12
0.14
0.16
0.18
0.20
0.22
0.24
0.26
Er ro
r
Validation
5 10 15 #Epoch
0.10
0.15
0.20
",5.3 Sentence Classification,[0],[0]
"Er ro
r
Test
Figure 5:",5.3 Sentence Classification,[0],[0]
"Learning curves on TREC dataset.
",5.3 Sentence Classification,[0],[0]
ing classification errors.,5.3 Sentence Classification,[0],[0]
"10-fold cross-validation is used for evaluation on the first 4 datasets, while TREC has a pre-defined training/test split, and we run each algorithm 10 times on TREC.",5.3 Sentence Classification,[0],[0]
"The combination of pSGLD and dropout consistently provides the lowest errors.
",5.3 Sentence Classification,[0],[0]
"In the following, we focus on the analysis of TREC.",5.3 Sentence Classification,[0],[0]
"Each sentence of TREC is a question, and the goal is to decide which topic type the question is most related to: location, human, numeric, abbreviation, entity or description.",5.3 Sentence Classification,[0],[0]
"Fig. 5 plots the learning curves of different algorithms on the training, validation and testing sets of the TREC dataset.",5.3 Sentence Classification,[0],[0]
"pSGLD and dropout have similar behavior: they explore the parameter space during learning, and thus coverge slower than RMSprop on the training dataset.",5.3 Sentence Classification,[0],[0]
"However, the learned uncertainty alleviates overfitting and results in lower errors on the validation and testing datasets.
",5.3 Sentence Classification,[0],[0]
"To further study the Bayesian nature of the proposed approach, in Fig. 6 we choose two testing sentences with high uncertainty (i.e., standard derivation in prediction) from the TREC dataset.",5.3 Sentence Classification,[0],[0]
"Interestingly, after embedding to 2d-space with tSNE (Van der Maaten and Hinton, 2008), the two
Table 6: Sentence classification errors on five benchmark datasets.
Methods MR CR SUBJ MPQA TREC RMSprop 21.86±1.19 20.20±1.35 8.13±1.19 10.60±1.28 8.14±0.63 RMSprop + Dropout 20.52±0.99 19.57±1.79 7.24±0.86 10.66±0.74 7.48±0.47 RMSprop + Gal’s Dropout 20.22±1.12 19.29±1.93 7.52±1.17 10.59±1.12 7.34±0.66 pSGLD 20.36±0.85",5.3 Sentence Classification,[0],[0]
18.72±1.28,5.3 Sentence Classification,[0],[0]
7.00±0.89 10.54±0.99 7.48±0.82 pSGLD +,5.3 Sentence Classification,[0],[0]
"Dropout 19.33±1.10 18.18±1.32 6.61±1.06 10.22±0.89 6.88±0.65
sentences correspond to points lying on the boundary of different classes.",5.3 Sentence Classification,[0],[0]
We use 20 model samples to estimate the prediction mean and standard derivation on the true type and predicted type.,5.3 Sentence Classification,[0],[0]
"The classifier yields higher probability on the wrong types, associated with higher standard derivations.",5.3 Sentence Classification,[0],[0]
"One can leverage the uncertainty information to make decisions: either manually make a human judgement when uncertainty is high, or automatically choose the one with lower standard derivations when both types exhibits similar prediction means.",5.3 Sentence Classification,[0],[0]
A more rigorous usage of the uncertainty information is left as future work.,5.3 Sentence Classification,[0],[0]
Ablation Study We investigate the effectivenss of each module in the proposed algorithm in Table 7 on two datasets: TREC and PTB.,5.4 Discussion,[0],[0]
The small network size is used on PTB.,5.4 Discussion,[0],[0]
"Let M1 denote only gradient noise, and M2 denote only model averaging.",5.4 Discussion,[0],[0]
"As can be seen, The last sample in pSGLD (M1) does not necessarily bring better results than RMSprop, but the model averaging over the samples of pSGLD indeed provide better results than model averaging of RMSprop (M2).",5.4 Discussion,[0],[0]
"This indicates that both gradient noise and model averaging are crucial for good performance in pSGLD.
",5.4 Discussion,[0],[0]
Running Time We report the training and testing time for image captioning on the Flickr30k dataset in Table 8.,5.4 Discussion,[0],[0]
"For pSGLD, the extra cost in training comes from adding gradient noise, and the extra cost in testing comes from model averaging.",5.4 Discussion,[0],[0]
"However, the cost in model averaging can be alleviated via the distillation methods: learning a single neural network that approximates the results of either a large model or an ensemble of models (Korattikara et al., 2015; Kim and Rush, 2016; Kuncoro et al., 2016).",5.4 Discussion,[0],[0]
"The idea can be incorporated with our SG-MCMC technique to achieve the same goal, which we leave for our future work.",5.4 Discussion,[0],[0]
"We propose a scalable Bayesian learning framework using SG-MCMC, to model weight uncertainty in recurrent neural networks.",6 Conclusion,[0],[0]
"The learning framework is tested on several tasks, including language models, image caption generation and sentence classification.",6 Conclusion,[0],[0]
"Our algorithm outperforms stochastic optimization algorithms, indicating the importance of learning weight uncertainty in recurrent neural networks.",6 Conclusion,[0],[0]
"Our algorithm requires little additional computational overhead in training, and multiple times of forward-passing for model averaging in testing.
",6 Conclusion,[0],[0]
"Acknowledgments This research was supported by ARO, DARPA, DOE, NGA, ONR and NSF.",6 Conclusion,[0],[0]
We acknowledge Wenlin Wang for the code on language modeling experiment.,6 Conclusion,[0],[0]
Recurrent neural networks (RNNs) have shown promising performance for language modeling.,abstractText,[0],[0]
"However, traditional training of RNNs using back-propagation through time often suffers from overfitting.",abstractText,[0],[0]
One reason for this is that stochastic optimization (used for large training sets) does not provide good estimates of model uncertainty.,abstractText,[0],[0]
This paper leverages recent advances in stochastic gradient Markov Chain Monte Carlo (also appropriate for large training sets) to learn weight uncertainty in RNNs.,abstractText,[0],[0]
"It yields a principled Bayesian learning algorithm, adding gradient noise during training (enhancing exploration of the model-parameter space) and model averaging when testing.",abstractText,[0],[0]
Extensive experiments on various RNN models and across a broad range of applications demonstrate the superiority of the proposed approach relative to stochastic optimization.,abstractText,[0],[0]
Scalable Bayesian Learning of Recurrent Neural Networks for Language Modeling,title,[0],[0]
"Our goal is to build a competitor for decision tree and rule learning algorithms in terms of accuracy, interpretability, and computational speed.",1. Introduction,[0],[0]
"Decision trees are widely used, particularly in industry, because of their interpretability.",1. Introduction,[0],[0]
Their logical IF-THEN structure allows predictions to be explained to users.,1. Introduction,[0],[0]
"However, decision tree algorithms have the serious flaw that they are constructed using greedy splitting from the top down.",1. Introduction,[0],[0]
They also use greedy pruning of nodes.,1. Introduction,[0],[0]
"They do not globally optimize any function, instead they are composed entirely of local optimization heuristics.",1. Introduction,[0],[0]
"If the algorithm makes a mistake in the splitting near the
1Massachusetts Institute of Technology, Cambridge, Massachusetts, USA 2Duke University, Durham, North Carolina, USA 3Harvard University, Cambridge, Massachusetts, USA.",1. Introduction,[0],[0]
Correspondence to: Hongyu Yang,1. Introduction,[0],[0]
"<hongyuy@mit.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"top of the tree, it is difficult to undo it, and consequently the trees become long and uninterpretable, unless they are heavily pruned, in which case accuracy suffers.",1. Introduction,[0],[0]
"In general, decision tree algorithms are computationally tractable, not particularly accurate, and less sparse and interpretable than they could be.",1. Introduction,[0],[0]
"This leaves users with no good alternative if they desire an accurate yet sparse logical classifier.
",1. Introduction,[0],[0]
"Several important ingredients provide the underpinning for our method including:
(i) A principled objective, which is the posterior distribution for the Bayesian Rule List (BRL) model (see Letham et al., 2015).",1. Introduction,[0],[0]
We optimize this objective over rule lists.,1. Introduction,[0],[0]
"Our algorithm is called Scalable Bayesian Rule Lists (SBRL).
",1. Introduction,[0],[0]
(ii) A useful statistical approximation that narrows the search space.,1. Introduction,[0],[0]
We assume that each rule in the rule list contains (“captures”) a number of observations that is bounded below.,1. Introduction,[0],[0]
"Because of this approximation, the set of conditions defining each leaf is a frequent pattern.",1. Introduction,[0],[0]
This means the antecedents within the rule list are all frequent patterns.,1. Introduction,[0],[0]
All of the possible frequent patterns can be pre-mined from the dataset using one of the standard frequent pattern mining methods.,1. Introduction,[0],[0]
"This leaves us with a much smaller optimization problem: we optimize over the set of possible pre-mined antecedents and their order to create the rule list.
",1. Introduction,[0],[0]
(iii) High performance language libraries to achieve computational efficiency.,1. Introduction,[0],[0]
Optimization over rule lists is done through repeated low level computations.,1. Introduction,[0],[0]
"At every iteration, we make a change to the rule list and need to evaluate the new rule list on the data.",1. Introduction,[0],[0]
"High performance calculations (novel to this problem) speed up this evaluation.
",1. Introduction,[0],[0]
(iv) Computational reuse.,1. Introduction,[0],[0]
"When we evaluate a rule list on the data that has been modified from a previous rule list, we need only to change the evaluation of points below the change in the rule list.",1. Introduction,[0],[0]
"Thus we can reuse the computation above the change.
",1. Introduction,[0],[0]
"(v) Analytical bounds on BRL’s posterior that are tight enough to be used in practice for screening association
Through a series of controlled experiments, we show that SBRL is over two orders of magnitude faster than the previous best code for this problem.
",1. Introduction,[0],[0]
"For example, Table 1 presents a rule list that we learned for the UCI Mushroom dataset (see Bache & Lichman, 2013).",1. Introduction,[0],[0]
This rule list is a predictive model for whether a mushroom is edible.,1. Introduction,[0],[0]
It was created in about 9 seconds on a laptop and achieves perfect out-of-sample accuracy.,1. Introduction,[0],[0]
Scalable Bayesian Rule Lists maximizes the posterior distribution of the Bayesian Rule Lists algorithm.,2. Review of Bayesian Rule Lists,[0],[0]
"Our training set is {(xi, yi)}ni=1 where the xi ∈ X encode features, and yi are labels, which in our case are binary, either 0 or 1.",2. Review of Bayesian Rule Lists,[0],[0]
"A Bayesian rule list has the following form:
if x obeys a1 then y ∼ Binom(θ1), θ1 ∼ Beta(α+ N1) else if x obeys a2 then y ∼ Binom(θ2), θ2 ∼ Beta(α+ N2) ... else if x obeys am then y ∼ Binom(θm), θm ∼ Beta(α+ Nm) else y ∼ Binom(θ0), θ0 ∼ Beta(α+ N0).
",2. Review of Bayesian Rule Lists,[0],[0]
"Here, the antecedents {aj}mj=1 are conditions on the x’s that are either true or false, for instance, if x is a patient, aj is true when x’s age is above 60 years old and x has diabetes, otherwise false.",2. Review of Bayesian Rule Lists,[0],[0]
The vector α =,2. Review of Bayesian Rule Lists,[0],[0]
"[α1, α0] has a prior parameter for each of the two labels.",2. Review of Bayesian Rule Lists,[0],[0]
"Values α1 and α0 are prior parameters, in the sense that each rule’s prediction y ∼ Binomial(θj), and θj |α ∼ Beta(α).",2. Review of Bayesian Rule Lists,[0],[0]
"The notation Nj is the vector of counts, where Nj,l is the number of observations xi that satisfy condition aj but none of the previous
conditions a1, ..., aj−1, and that have label yi = l, where l is either 1 or 0.",2. Review of Bayesian Rule Lists,[0],[0]
Nj is added to the prior parameters α from the usual derivation of the posterior for the Beta-binomial.,2. Review of Bayesian Rule Lists,[0],[0]
"The default rule is at the bottom, which makes predictions for observations that are not satisfied by any of the conditions.",2. Review of Bayesian Rule Lists,[0],[0]
"When an observation satisfies condition aj but not a1, ..., aj−1 we say that the observation is captured by rule j. Formally:
Definition 1 Rule j captures observation i, denoted Captr(i)",2. Review of Bayesian Rule Lists,[0],[0]
"= j, when j = argmin j′ such that aj′(xi) =",2. Review of Bayesian Rule Lists,[0],[0]
"True.
",2. Review of Bayesian Rule Lists,[0],[0]
"Bayesian Rule Lists is an associative classification method, in the sense that the antecedents are first mined from the database, and then the set of rules and their order are learned.",2. Review of Bayesian Rule Lists,[0],[0]
"The rule mining step is fast, and there are fast parallel implementations available.",2. Review of Bayesian Rule Lists,[0],[0]
"Any frequent pattern mining method will suffice, since the method needs only to produce those conditions with sufficiently high support in the database.",2. Review of Bayesian Rule Lists,[0],[0]
"The support of antecedent aj is denoted supp(aj), which is the number of observations that obey condition aj .",2. Review of Bayesian Rule Lists,[0],[0]
"A condition is a conjunction of expressions “feature∈values,” e.g., age∈[40,50] and color=white.",2. Review of Bayesian Rule Lists,[0],[0]
"The hard part is learning the rule list, which is what this paper focuses on.",2. Review of Bayesian Rule Lists,[0],[0]
"It is an optimization over subsets of rules and their permutations.
",2. Review of Bayesian Rule Lists,[0],[0]
"The likelihood for the model discussed above is:
Likelihood = p(y|x, d, α) ∝",2. Review of Bayesian Rule Lists,[0],[0]
"∏m j=0 Γ(Nj,0+α0)Γ(Nj,1+α1) Γ(Nj,0+Nj,1+α0+α1) ,
where d denotes the rules in the list and their order, d = (m, {aj , θj}mj=0).",2. Review of Bayesian Rule Lists,[0],[0]
"Intuitively, one can see that having more of one class and less of the other class will make the likelihood larger.",2. Review of Bayesian Rule Lists,[0],[0]
"To see this, note that if Nj,0 is large and Nj,1 is small (or vice versa) the likelihood for rule j is large.
",2. Review of Bayesian Rule Lists,[0],[0]
We next discuss the prior.,2. Review of Bayesian Rule Lists,[0],[0]
"It has three terms, one governing the number of rules m in the list, one governing the size cj of each antecedent j, and one governing the choice of antecedent condition aj of rule j given its size.",2. Review of Bayesian Rule Lists,[0],[0]
"Specifically, cj is the cardinality of antecedent aj , also written |aj",2. Review of Bayesian Rule Lists,[0],[0]
"|, the number of conjunctive clauses in antecedent aj .",2. Review of Bayesian Rule Lists,[0],[0]
"E.g, if a is ‘x1=green’ and ‘x2<50’, this has cardinality 2.",2. Review of Bayesian Rule Lists,[0],[0]
"Notation a<j includes the antecedents before j in the rule list, if there are any, e.g. a<4 = {a1, a2, a3}.",2. Review of Bayesian Rule Lists,[0],[0]
c<j includes the cardinalities of the antecedents before j in the rule list.,2. Review of Bayesian Rule Lists,[0],[0]
Notation A is the set of pre-mined antecedents.,2. Review of Bayesian Rule Lists,[0],[0]
"The prior is:
p(d|A, λ, η) = p(m|A, λ) ∏m j=1 p(cj |c<j ,A, η)p(aj |a<j , cj ,A).
",2. Review of Bayesian Rule Lists,[0],[0]
The first term is the prior for the number of rules in the list.,2. Review of Bayesian Rule Lists,[0],[0]
"Here, the number of rules m is Poisson, truncated at the
total number of pre-selected antecedents:
p(m|A, λ) =",2. Review of Bayesian Rule Lists,[0],[0]
"(λ m/m!)∑|A|
j=0(λ j/j!)
, m = 0, . . .",2. Review of Bayesian Rule Lists,[0],[0]
", |A|,
where λ is a hyper-parameter.",2. Review of Bayesian Rule Lists,[0],[0]
The second term in the prior governs the number of conditions in each rule.,2. Review of Bayesian Rule Lists,[0],[0]
"The size of rule j is cj which is Poisson, truncated to remove values for which no rules are available with that cardinality:
p(cj |c<j ,A, η) =",2. Review of Bayesian Rule Lists,[0],[0]
(η cj /cj !),2. Review of Bayesian Rule Lists,[0],[0]
"∑
k∈Rj−1(c<j,A) (ηk/k!)
, cj ∈",2. Review of Bayesian Rule Lists,[0],[0]
"Rj−1(c<j ,A),
where Rj−1(c<j ,A) is the set of cardinalities available after removing the first j−1 rules, and η is a hyperparameter.",2. Review of Bayesian Rule Lists,[0],[0]
"The third term in the prior governs the choice of antecedent, given that we have determined its size through the second term.",2. Review of Bayesian Rule Lists,[0],[0]
"We have aj selected from a uniform distribution over antecedents in A of size cj , excluding those in a<j .
p(aj |a<j , cj ,A) ∝ 1, aj ∈ Qcj",2. Review of Bayesian Rule Lists,[0],[0]
= {a ∈,2. Review of Bayesian Rule Lists,[0],[0]
"A \ {a1, a2, ..., aj−1} : |a| = cj}.
",2. Review of Bayesian Rule Lists,[0],[0]
"As usual, the posterior is the likelihood times the prior.
",2. Review of Bayesian Rule Lists,[0],[0]
"p(d|x,y,A, α, λ, η) ∝",2. Review of Bayesian Rule Lists,[0],[0]
"p(y|x, d, α)p(d|A, λ, η).
",2. Review of Bayesian Rule Lists,[0],[0]
"This is the full model, and the posterior p(d|x,y,A, α, λ, η) is what we optimize to obtain the best rule lists.
",2. Review of Bayesian Rule Lists,[0],[0]
"The hyperparameter λ is chosen by the user to be the desired size of the rule list, and η is chosen as the desired number of terms in each rule.",2. Review of Bayesian Rule Lists,[0],[0]
"The parameters α0 and α1 are usually chosen to be 1 to avoid favoring one class label over another.
",2. Review of Bayesian Rule Lists,[0],[0]
"Given the prior parameters λ, η, and α, along with the set of pre-mined rulesA, the algorithm must select which rules from A to use, along with their order.",2. Review of Bayesian Rule Lists,[0],[0]
"We use an MCMC scheme: at each time t, we choose a neighboring rule list at random by adding, removing, or swapping rules, starting with the basic algorithm of Letham et al. (2015) as a starting point.",3. Representation,[0],[0]
"However, to optimize performance we use a more efficient rule list representation that is amenable to fast computation.
",3. Representation,[0],[0]
Expressing computation as bit vectors: The vast majority of the computational time spent constructing rule sets lies in determining which rules capture which observations in a particular rule ordering.,3. Representation,[0],[0]
"The naı̈ve implementation of these operations calls for various set operations – checking whether a set contains an element, adding an element to a set, and removing an element from a set.",3. Representation,[0],[0]
"However, set operations are typically slow, and hardware does little to help with efficiency.
",3. Representation,[0],[0]
"We convert all set operations to logical operations on bit vectors, for which hardware support is readily available.",3. Representation,[0],[0]
The bit vector representation is efficient in terms of both memory and computation.,3. Representation,[0],[0]
"Before beginning the algorithm, for each rule, we compute the bit vector representing the samples for which the rule generates a true value.",3. Representation,[0],[0]
"For a one million sample data set (or more precisely up to 1,048,576 observations) each rule carries with it a 128 KB vector (since a byte consists of 8 bits), which fits comfortably in most L2 caches.
",3. Representation,[0],[0]
"Representing intermediate state as bit vectors: For each rule list we consider, we maintain similarly sized vectors for each rule in the set indicating which (unique) rule in the set captures which observation.",3. Representation,[0],[0]
"Representing the rules and rule lists this way allows us to explore the rule list state space, reusing significant computation.",3. Representation,[0],[0]
"For example, consider a rule list containing n rules.",3. Representation,[0],[0]
Imagine that we wish to delete rule k from the set.,3. Representation,[0],[0]
The naı̈ve implementation recomputes the “captures” vector for every rule in the set.,3. Representation,[0],[0]
"Our implementation updates only rules j > k, using logical operators acting upon the rule list “captures” vector for k, and the rule’s “captures” vector for each rule j > k.",3. Representation,[0],[0]
"This shortens the run time of the algorithm in practice by approximately 50%.
",3. Representation,[0],[0]
A fast algebra for computational reuse: Our use of bit vectors transforms the large number of set operations (performed in a traditional implementation) into a set of boolean operations on bit vectors.,3. Representation,[0],[0]
"We have custom implementations (discussed in the full version of this work, Yang et al., 2017) for the following: (i) Remove rule k uses boolean operations on bit vectors to redistribute the observations captured by rule k to the rules below it in the list.",3. Representation,[0],[0]
"(ii) Insert rule k shifts the rules below k down one position, determines which observations are captured by the new rule, and removes those observations from the rules below it.",3. Representation,[0],[0]
(iii) Swap consecutive rules updates only which observations were captured for the two swapped rules.,3. Representation,[0],[0]
(iv) Generalized swap subroutine updates only observations captured for all rules between the two rules to be swapped.,3. Representation,[0],[0]
"All operations use only bit vector computations.
",3. Representation,[0],[0]
"Ablation study: Having transformed expensive set operations into bit vector operations, we can now leverage both hardware vector instructions and optimized software libraries.",3. Representation,[0],[0]
"We investigated four alternative implementations, each improving efficiency from the previous one.",3. Representation,[0],[0]
(i),3. Representation,[0],[0]
"First, we have the original python implementation here for comparison.",3. Representation,[0],[0]
(ii),3. Representation,[0],[0]
"Next, we retained our python implementation but converted from set operations to bit operations.",3. Representation,[0],[0]
"(iii) Then, we used the python gmpy library to perform the bit operations.",3. Representation,[0],[0]
"(iv) Finally, we moved the implementation from Python to C, represented the bit vectors as multiprecision integers, used the GMP library, which is faster on
large data sets, and implemented the algebra for computational reuse outlined above.",3. Representation,[0],[0]
"To evaluate how each of these steps improved the computation time of the algorithm, we conducted a controlled experiment where each version of the algorithm (corresponding to the four steps above) was given the same data (the UCI adult dataset, divided into three folds), same set of rules, and same number of MCMC iterations (20,000) to run.",3. Representation,[0],[0]
"We created boxplots for the log10 of the run time over the different folds, which are shown in Figure 1.",3. Representation,[0],[0]
"The final code is over two orders of magnitude faster than the original optimized python code (that of Letham et al., 2015).",3. Representation,[0],[0]
We prove two bounds.,4. Bounds,[0],[0]
First we provide an upper bound on the number of rules in a maximum a posteriori rule list.,4. Bounds,[0],[0]
This allows us to narrow our search space to rule lists below a certain size.,4. Bounds,[0],[0]
Second we provide a constraint that eliminates certain prefixes of rule lists.,4. Bounds,[0],[0]
This prevents our algorithm from searching in regions of the space that provably do not contain the maximum a posteriori rule list.,4. Bounds,[0],[0]
"Given the number of features, the parameter λ for the size of the list, and parameters α0 and α1, we can derive an upper bound for the size of a maximum a posteriori rule list.",4.1. Upper bound on the number of rules in the list,[0],[0]
"This formalizes how the prior on the number of rules is strong enough to overwhelm the likelihood.
",4.1. Upper bound on the number of rules in the list,[0],[0]
"We are considering binary antecedents and binary features (e.g., aj is true if female), so the total number of possible
Algorithm 1",4.1. Upper bound on the number of rules in the list,[0],[0]
"Calculating bj’s Initialization: index=0, b0 = 1 for c = 0 to ⌊ P 2 ⌋ do
for j = ( P c ) downto 1",4.1. Upper bound on the number of rules in the list,[0],[0]
"do
index = index + 1 bindex = j
end for if c+ c 6= p then
for j = ( P P−c ) downto 1",4.1. Upper bound on the number of rules in the list,[0],[0]
"do
index = index + 1 bindex = j
end for end if
end for
antecedents of each size can be calculated directly.",4.1. Upper bound on the number of rules in the list,[0],[0]
"When creating the upper bound, within the proof, we hypothetically exhaust antecedents from each size category in turn, starting with the smallest sizes.",4.1. Upper bound on the number of rules in the list,[0],[0]
"We discuss this further below.
",4.1. Upper bound on the number of rules in the list,[0],[0]
Let |Qc| be the number of antecedents that remain in the pile that have c logical conditions.,4.1. Upper bound on the number of rules in the list,[0],[0]
The sequence of b’s that we define next is a lower bound for the possible sequence of |Qc|’s.,4.1. Upper bound on the number of rules in the list,[0],[0]
"In particular, b0, b1, b2, etc. represents the sequence of sizes of rules that would provide the smallest possible |Qc|’s.",4.1. Upper bound on the number of rules in the list,[0],[0]
"Intuitively, the sequence of b’s arises when we deplete the antecedents of size 1, then deplete all of size 2, etc.",4.1. Upper bound on the number of rules in the list,[0],[0]
"The number of ways to do this is given by the bindex values, computed as Algorithm 1, where P is the number of features, and b = {b0, b1, ...b2P−1} is a vector of length 2P .",4.1. Upper bound on the number of rules in the list,[0],[0]
We will use b within the theorem below.,4.1. Upper bound on the number of rules in the list,[0],[0]
"In our notation, rule list d is defined by the antecedents and the probabilities on the right side of the rules, d = (m, {al, θl}ml=1).
",4.1. Upper bound on the number of rules in the list,[0],[0]
Theorem 1,4.1. Upper bound on the number of rules in the list,[0],[0]
"The size m∗ of any MAP rule list d∗ (with parameters λ, η, and α = (α0, α1)) obeys m∗ ≤ mmax, where
mmax = min
{ 2P",4.1. Upper bound on the number of rules in the list,[0],[0]
"− 1,max { m′ ∈ Z+",4.1. Upper bound on the number of rules in the list,[0],[0]
: λ m′,4.1. Upper bound on the number of rules in the list,[0],[0]
m′!,4.1. Upper bound on the number of rules in the list,[0],[0]
"≥ Γ(N−+α0)Γ(N++α1)
Γ(N+α0+α1) m′∏ j=1 bj
}} .
",4.1. Upper bound on the number of rules in the list,[0],[0]
"With parameters α0 = 1 and α1 = 1, this reduces to:
mmax = min
{ 2P − 1,max { m′ ∈",4.1. Upper bound on the number of rules in the list,[0],[0]
Z+ :,4.1. Upper bound on the number of rules in the list,[0],[0]
λ m′ m′!,4.1. Upper bound on the number of rules in the list,[0],[0]
≥ Γ(N−+1)Γ(N++1) Γ(N+2),4.1. Upper bound on the number of rules in the list,[0],[0]
"m′∏ j=1 bj }} .
",4.1. Upper bound on the number of rules in the list,[0],[0]
"The proof is in the longer version of this paper (Yang et al., 2017).",4.1. Upper bound on the number of rules in the list,[0],[0]
Theorem 1 tends to significantly reduce the size of the space.,4.1. Upper bound on the number of rules in the list,[0],[0]
"Without this bound, it is possible that the search would consider extremely long lists, without knowing that they are provably non-optimal.",4.1. Upper bound on the number of rules in the list,[0],[0]
We next provide a bound that eliminates certain regions of the rule space from consideration.,4.2. Prefix Bound,[0],[0]
"Consider a rule list beginning with antecedents a1, .., ap.",4.2. Prefix Bound,[0],[0]
"If the best possible rule list starting with a1, .., ap cannot beat the posterior of the best rule list we have found so far, then we know any rule list starting with a1, .., ap is suboptimal.",4.2. Prefix Bound,[0],[0]
"In that case, we should stop exploring rule lists that start with a1, .., ap.",4.2. Prefix Bound,[0],[0]
"This is a type of branch and bound strategy, in that we have now eliminated (bounded) the entire set of lists starting with a1, .., ap.",4.2. Prefix Bound,[0],[0]
"We formalize this intuition below.
",4.2. Prefix Bound,[0],[0]
"Denote the rule list antecedents at iteration t by dt = (at1, a t 2, ..., a t mt , a0).",4.2. Prefix Bound,[0],[0]
"The current best posterior probability has value v∗t , that is
v∗t = max t′≤t Posterior(dt ′",4.2. Prefix Bound,[0],[0]
", {(xi, yi)}ni=1).
",4.2. Prefix Bound,[0],[0]
"Let us consider a rule list with antecedents d = (a1, a2, ...am, a0).",4.2. Prefix Bound,[0],[0]
"Let dp denote a prefix of length p of the rule list d, i.e., dp = (a1, a2, ...ap), where a1, a2, ..., ap are the same as the first p antecedents in d. We want to determine whether a rule list starting with dp could be better than the best we have seen so far.
",4.2. Prefix Bound,[0],[0]
"Define Υ(dp, {(xi, yi)}ni=1)",4.2. Prefix Bound,[0],[0]
"as follows:
Υ(dp, {(xi, yi)}ni=1)",4.2. Prefix Bound,[0],[0]
":= λmax (p,λ)/(max (p,λ))!∑|A|
j=0(λ j/j!)
",4.2. Prefix Bound,[0],[0]
"(∏p j=1 p(cj |c<j ,A, η)
1 |Qcj | ) × (∏m
j=0 Γ(Nj,0+1)Γ(Nj,1+1)
Γ(Nj,0+Nj,1+2)
)",4.2. Prefix Bound,[0],[0]
"×
Γ(1+N0−",4.2. Prefix Bound,[0],[0]
"∑p j=1Nj,0)
Γ(2+N0−",4.2. Prefix Bound,[0],[0]
"∑p j=1Nj,0)
Γ(1+N1−",4.2. Prefix Bound,[0],[0]
"∑p j=1Nj,1)
Γ(2+N1−",4.2. Prefix Bound,[0],[0]
"∑p j=1Nj,1) .
",4.2. Prefix Bound,[0],[0]
"Here, Nj,0 is the number of points captured by rule j with label 0, and Nj,1 is the number of points captured by rule j with label 1,
Nj,0 = |{i : Captr(i) = j and yi = 0}|, Nj,1 = |{i : Captr(i) = j and yi = 1}|.
",4.2. Prefix Bound,[0],[0]
"The result states that for a rule list with prefix dp, if the upper bound on the posterior, Υ(dp), is not as high as the posterior of the best rule list we have seen so far, then dp is a bad prefix, which cannot lead to a MAP solution.",4.2. Prefix Bound,[0],[0]
"It tells us we no longer need to consider rule lists starting with dp.
",4.2. Prefix Bound,[0],[0]
Theorem 2,4.2. Prefix Bound,[0],[0]
"For rule list d = {dp, ap+1, ..., am, a0}, if
Υ(dp, {(xi, yi)}ni=1)",4.2. Prefix Bound,[0],[0]
<,4.2. Prefix Bound,[0],[0]
"v∗t ,
then for α0 = 1 and α1 = 1, we have
d 6∈ argmaxd′Posterior(d′, {(xi, yi)}ni=1).",4.2. Prefix Bound,[0],[0]
"(1)
Theorem 2 is implemented in our code in the following way: for each random restart, the initial rule in the list is checked against the bound of Theorem 2.",4.2. Prefix Bound,[0],[0]
If the condition Υ(d1) <,4.2. Prefix Bound,[0],[0]
"v ∗ t holds, we throw out this initial rule and choose a new one, because that rule provably cannot be the first rule in an optimal rule list.",4.2. Prefix Bound,[0],[0]
Theorem 2 provides a substantial computational speedup in finding high quality or optimal solutions.,4.2. Prefix Bound,[0],[0]
"In some cases, it provides a full order of magnitude speedup.",4.2. Prefix Bound,[0],[0]
"The proofs are lengthy and contained in the longer version of this work (Yang et al., 2017).",4.2. Prefix Bound,[0],[0]
"We provide a comparison of algorithms along three dimensions: solution quality (AUC - area under the ROC curve), sparsity, and scalability.",5. Experiments,[0],[0]
"As baselines, we chose popular algorithms to represent the sets of uninterpretable methods and the set of “interpretable” methods.",5. Experiments,[0],[0]
"To represent uninterpretable methods, we chose logistic regression, SVM RBF, random forests (RF), and boosted decision trees (ADA).",5. Experiments,[0],[0]
"To represent the class of “interpretable” algorithms, we chose CART, C4.5, RIPPER (Cohen, 1995), CBA (Liu et al., 1998), and CMAR (Li et al., 2001).",5. Experiments,[0],[0]
"Other works (see Letham et al., 2015; Wang & Rudin, 2015a) have accuracy/interpretability comparisons to Bayesian Rule Lists and Falling Rule Lists, so our main effort here will be to study the scalability component.",5. Experiments,[0],[0]
"Implementation details are in the full version (Yang et al., 2017).
",5. Experiments,[0],[0]
"We benchmark using publicly available datasets (see Bache & Lichman, 2013) that have interpretable features: the Tic Tac Toe dataset, where the goal is to determine whether the “X” player wins; the Adult dataset, where we aim to predict whether an individual makes over $50K peryear; the Mushroom dataset, where the goal is to predict whether a mushroom is edible; the Nursery dataset, where the goal is to predict whether a child’s application to nursery school will be in either the “very recommended” or “special priority” categories; and the Telco customer churn dataset (see WatsonAnalytics, 2015), where the goal is to predict whether a customer will leave the service provider.
",5. Experiments,[0],[0]
"Evaluations of prediction quality, sparsity, and timing were done using 10-fold cross validation.",5. Experiments,[0],[0]
"The prior parameters were fixed at η = 1, and α = (1, 1).",5. Experiments,[0],[0]
"For the λ for each dataset, we first let λ be 5 and ran SBRL once with the above parameters.",5. Experiments,[0],[0]
Then we fixed λ at the length of the returned rule list for that dataset.,5. Experiments,[0],[0]
It is possible that the solution quality would increase if SBRL ran for a larger number of iterations.,5. Experiments,[0],[0]
"For the purpose of providing a controlled experiment, the number of iterations was fixed at 5,000 for each of the 20 chains of SBRL, which we ran in series on a laptop.",5. Experiments,[0],[0]
"Every time SBRL started a new rule list, we checked the initial rule in the list to see whether the upper-bound on its posterior (by Theorem 2) was greater
than the best rule list we had found so far.",5. Experiments,[0],[0]
"If not, the rule was replaced until the condition was satisfied.
",5. Experiments,[0],[0]
Tic Tac Toe: Each observation in this dataset is a tic tac toe board after the game has finished.,5. Experiments,[0],[0]
"If there are 3 X’s in a row, the label of the board is 1, otherwise 0.",5. Experiments,[0],[0]
This should not be a difficult learning problem since there are solutions with perfect accuracy on the training set that generalize to the test set.,5. Experiments,[0],[0]
"Figure 2 shows a scatter plot of AUC vs. number of leaves (sparsity), where each triangular marker represents an evaluation of one algorithm, on one fold, with one parameter setting.",5. Experiments,[0],[0]
"We tried many different parameter settings for CART (in blue), and many different parameter settings for C4.5 (in gray), none of which were able to achieve points on the efficient frontier defined by SBRL.",5. Experiments,[0],[0]
"SBRL’s run time was on average 0.759 (± .02) seconds.
",5. Experiments,[0],[0]
Adult:,5. Experiments,[0],[0]
"For the Adult dataset, results are in Figure 3, Figure 4 and Table 2.",5. Experiments,[0],[0]
"Adult contains 45,121 observations and 12 features, where each observation is an individual, and the features are census data, including demographics, income levels, and other financial information.",5. Experiments,[0],[0]
"Here, SBRL, which was untuned and forced to be sparse, performed only slightly worse than several of the uninterpretable methods.",5. Experiments,[0],[0]
Its AUC performance dominated those of the CART and C4.5 algorithms.,5. Experiments,[0],[0]
"As the scatter plot shows, even if CART were tuned on the test set, it would have performed at around the same level, perhaps slightly worse than SBRL.",5. Experiments,[0],[0]
"The timing for SBRL was competitive, at about 18 seconds, where 14 seconds were MCMC iterations.",5. Experiments,[0],[0]
"If the chains were computed in parallel rather than in series, it would speed up computation further.
",5. Experiments,[0],[0]
Mushroom:,5. Experiments,[0],[0]
Table 1 contains an SBRL rule list for Mushroom; other results are in Yang et al. (2017).,5. Experiments,[0],[0]
"SBRL attains perfect test accuracy on this dataset.
",5. Experiments,[0],[0]
Nursery: Results from the Nursery dataset are shown in Figure 5.,5. Experiments,[0],[0]
"A similar story holds as for the previous datasets: SBRL is on the optimal frontier of accuracy/sparsity without tuning and with reasonable run time.
",5. Experiments,[0],[0]
Telco:,5. Experiments,[0],[0]
"Figure 6, Figure 7 and Table 3 show the results for the Telco dataset, which contains 7043 observations and 18 features.",5. Experiments,[0],[0]
Similar observations hold for this dataset.,5. Experiments,[0],[0]
The model from one of the ten folds is provided in Table 4.,5. Experiments,[0],[0]
"We used the USCensus1990 data (see Bache & Lichman, 2013) to test the scalability of SBRL on large datasets.",6. Scalability,[0],[0]
"We used 1,000,000 observations and set SBRL’s parameter to extract ≈1000 rules as problem (A) and about 50,000 observations with 50,000 rules as problem (B).",6. Scalability,[0],[0]
The runtime comparison with CART is shown in Table 5.,6. Scalability,[0],[0]
"For problem (A), the run times are similar; for (B); SBRL is slower (2.5 hours), which is not prohibitive for important problems.",6. Scalability,[0],[0]
"One can see why CART does not perform as well in high dimensions, as it often spends less time on harder problems than on easier ones; details are in (Yang et al.,
Table 3.",6. Scalability,[0],[0]
"Run Time on Telco dataset
RUN TIME LR SVM RF ADA CART C4.5 RIPPER CBA CMAR SBRL
MEAN 0.267 7.468 3.703 7.839 0.168 0.250 37.14 8.028 1.679 5.239 MEDIAN 0.272 7.550 3.695 8.726 0.168 0.252 37.63 8.050 1.705 5.271 STD 0.009 0.207 0.183 0.111 0.008 0.017 3.202 0.400 0.161 0.149
2017).",6. Scalability,[0],[0]
"Rule lists are a type of one-sided decision tree, and any decision tree can be written as a rule list by enumerating the leaves.",7. Related Works and Discussion,[0],[0]
"Thus SBRL is a competitor for CART (Breiman et al., 1984).",7. Related Works and Discussion,[0],[0]
CART is currently still popular in industry.,7. Related Works and Discussion,[0],[0]
"CART and other decision tree methods (also decision list methods and associative classification methods) form trees from the top down using greedy splitting and greedy pruning (see, e.g., Quinlan, 1983; Clark & Niblett, 1989; Cendrowska, 1987; Rivest, 1987; Quinlan, 1993; Liu et al., 1998; Li et al., 2001; Yin & Han, 2003; Marchand & Sokolova, 2005; Vanhoof & Depaire, 2010; Rudin et al., 2013).",7. Related Works and Discussion,[0],[0]
"Since our work does not use greedy splitting and pruning, it is closer to Bayesian tree models (Dension et al., 1998; Chipman et al., 2002; 2010), which are built greedily from the top down, but then the trees change according to an MCMC scheme, which allows for more exploration of the search space.",7. Related Works and Discussion,[0],[0]
"However even with MCMC, the chains tend to center on local optima.",7. Related Works and Discussion,[0],[0]
"It may be possible to use our techniques to build trees, where one would mine rules and create a globally optimal tree.
",7. Related Works and Discussion,[0],[0]
"There are a series of works from the mid-1990’s onwards on finding optimal decision trees using dynamic programming and search techniques (e.g., Bennett & Blue, 1996; Auer et al., 1995; Dobkin et al., 1996; Boros et al., 2000; Garofalakis et al., 2000; Farhangfar et al., 2008), mainly working with fixed depth trees.",7. Related Works and Discussion,[0],[0]
"The number of trees of a
fixed depth is much larger than the number of rule lists of a fixed depth and are therefore more difficult to optimize.",7. Related Works and Discussion,[0],[0]
"Nijssen & Fromont (2010) use pre-mined rules to form trees, but in a different way than our method.",7. Related Works and Discussion,[0],[0]
"There, the user premines all possible leaves, enumerating all conditions leading to that leaf.",7. Related Works and Discussion,[0],[0]
"(By contrast, in associative classification, we mine only small conjunctions, and their ordered combination creates leaves.)",7. Related Works and Discussion,[0],[0]
"As as result, Nijssen & Fromont (2010) warn about issues related to running out of memory.
",7. Related Works and Discussion,[0],[0]
"An extension of this work (CORELS - Angelino et al., 2017) does not provide probabilistic predictions, but is able to provide a certificate of optimality to a globally optimal rule list.",7. Related Works and Discussion,[0],[0]
"This indicates that SBRL is probably also achieving optimality; however, because SBRL is probabilistic, the proof of optimality is much more difficult.",7. Related Works and Discussion,[0],[0]
"To clarify: finding the optimal solution for both methods should be approximately equally difficult, but proving optimality for SBRL is much more difficult.",7. Related Works and Discussion,[0],[0]
"However, there is a clear practical benefit to having probabilistic predictions like those of SBRL.",7. Related Works and Discussion,[0],[0]
"One can post-process CORELS to have probabilistic predictions by computing P (Y = 1|x ∈ leaf) for each leaf, but this is not the same as optimizing likelihood and obtaining these probabilities directly like SBRL.
",7. Related Works and Discussion,[0],[0]
"There are several subfields that produce disjunctive normal form (DNF) classifiers rather than rule lists, including rule learning/induction, and associative classification, which stemmed possibly from work in the 1960s (Michalski, 1969), and throughout the 1980’s and 90’s (Cendrowska, 1987; Clark & Niblett, 1989; Cohen, 1995).",7. Related Works and Discussion,[0],[0]
"The vast majority of these techniques are not probabilistic, and aim for covering the positive class without covering the negative class.",7. Related Works and Discussion,[0],[0]
"Rijnbeek & Kors (2010) aim to produce globally op-
timal DNF models.",7. Related Works and Discussion,[0],[0]
"There is recent work on probabilistic DNF’s that is similar to SBRL (Wang et al., 2016; 2017).
",7. Related Works and Discussion,[0],[0]
"Teleo-reactive programs (Nilsson, 1994) use a rule list structure and could benefit from learning this structure from data.
",7. Related Works and Discussion,[0],[0]
SBRL aims to produce interpretable models.,7. Related Works and Discussion,[0],[0]
"Interpretability has been a fundamental topic in artificial intelligence for a long time (see Rüping, 2006; Bratko, 1997; Dawes, 1979; Vellido et al., 2012; Giraud-Carrier, 1998; Holte, 1993; Shmueli, 2010; Huysmans et al., 2011; Freitas, 2014).",7. Related Works and Discussion,[0],[0]
"Because the rule lists created by our method are designed to be interpretable, one would probably not want to boost them using AdaBoost to form more complicated models.",7. Related Works and Discussion,[0],[0]
"This contrasts with, for instance, Friedman & Popescu (2008), who linearly combine pre-mined rules.
",7. Related Works and Discussion,[0],[0]
"Rule lists and their variants are currently being used for text processing (King et al., 2017), discovering treatment regimes (Zhang et al., 2015; Lakkaraju & Rudin, 2017; Wang & Rudin, 2015b), and creating medical risk assessments (Letham et al., 2015), among other applications.
",7. Related Works and Discussion,[0],[0]
Conclusion We finish by stating why/when one would want to use this particular method.,7. Related Works and Discussion,[0],[0]
"SBRL is not meant as a competitor for black box classifiers such as neural networks, support vector machines, gradient boosting or random forests.",7. Related Works and Discussion,[0],[0]
"It is useful when machine learning tools are used as a decision aid to humans, who need to understand the model in order to trust it and make data-driven decisions.",7. Related Works and Discussion,[0],[0]
"SBRL does not use a greedy splitting/pruning procedure like decision tree algorithms (CART, C4.5), which means that it more reliably computes high quality solutions, at the possible expense of additional computation time.",7. Related Works and Discussion,[0],[0]
"Many of the decision tree methods do not compute sparse or interpretable trees, as we have seen with C4.5.",7. Related Works and Discussion,[0],[0]
Our code is a strict improvement over the original Bayesian Rule Lists algorithm if one is looking for a maximum a posteriori solution.,7. Related Works and Discussion,[0],[0]
"It is faster because of careful use of low-level computations and theoretical bounds.
",7. Related Works and Discussion,[0],[0]
"Code
Code for SBRL is available at the following link: https://github.com/Hongyuy/sbrlmod Link to R package SBRL on CRAN: https: //cran.r-project.org/web/packages/sbrl/ index.html",7. Related Works and Discussion,[0],[0]
"The authors would like to acknowledge partial funding provided by NSF, Philips, Wistron, and Siemens.",Acknowledgement,[0],[0]
We present an algorithm for building probabilistic rule lists that is two orders of magnitude faster than previous work.,abstractText,[0],[0]
Rule list algorithms are competitors for decision tree algorithms.,abstractText,[0],[0]
"They are associative classifiers, in that they are built from pre-mined association rules.",abstractText,[0],[0]
"They have a logical structure that is a sequence of IF-THEN rules, identical to a decision list or one-sided decision tree.",abstractText,[0],[0]
"Instead of using greedy splitting and pruning like decision tree algorithms, we aim to fully optimize over rule lists, striking a practical balance between accuracy, interpretability, and computational speed.",abstractText,[0],[0]
"The algorithm presented here uses a mixture of theoretical bounds (tight enough to have practical implications as a screening or bounding procedure), computational reuse, and highly tuned language libraries to achieve computational efficiency.",abstractText,[0],[0]
"Currently, for many practical problems, this method achieves better accuracy and sparsity than decision trees, with practical running times.",abstractText,[0],[0]
The predictions in each leaf are probabilistic.,abstractText,[0],[0]
Scalable Bayesian Rule Lists,title,[0],[0]
"It has long been known that solutions obtained from optimization methods can demonstrate striking sensitivity to the parameters of the problem (Bertsimas et al., 2011).",1. Introduction,[0],[0]
"Robust optimization, in contrast, is a paradigm in the mathematical programming community with the aim of safeguarding the solutions from the changes in the underlying parameters.
",1. Introduction,[0],[0]
"In this paper, we consider submodular maximization, a very well studied discrete optimization problem defined over a finite set of items (e.g., images, videos, blog posts, sensors, etc).",1. Introduction,[0],[0]
"Submodularity formalizes the notion of diminishing returns, stating (informally) that selecting an item earlier results in a higher utility than selecting it later.",1. Introduction,[0],[0]
"This notion has found far-reaching applications in machine learning
1Department of Computer Science, Yale University, New Haven, Connecticut, USA 2Google Research, Zurich, Switzerland.",1. Introduction,[0],[0]
"Correspondence to: Ehsan Kazemi <ehsan.kazemi@yale.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"(Bach et al., 2013), web search and mining (Borodin et al., 2017), social network (Kempe et al., 2003), crowdsourcing (Singla et al., 2016), and user modeling (Yue & Guestrin, 2011), to name a few.",1. Introduction,[0],[0]
"However, almost all the existing methods for submodular maximization, ranging from centralized (Nemhauser et al., 1978; Feldman et al., 2017) to streaming (Badanidiyuru et al., 2014; Feldman et al., 2018), to distributed (Mirzasoleiman et al., 2013; Mirrokni & Zadimoghaddam, 2015; Barbosa et al., 2015), rely on greedy selection of elements.",1. Introduction,[0],[0]
"As a result, the returned solution of such methods are remarkably sensitive to even a single deletion from the set of items.
",1. Introduction,[0],[0]
The need for efficient deletion-robust optimization methods is wide-spread across many data-driven applications.,1. Introduction,[0],[0]
"With access to big and massive data (usually generated by millions of users), along with strong machine learning techniques, many service providers have been able to exploit these new resources in order to improve the accuracy of their data analytics.",1. Introduction,[0],[0]
"At the same time, it has been observed that many such inference tasks may leak very sensitive information about the data providers (i.e., personally identifiable information, protected health information, legal or financial data, etc).",1. Introduction,[0],[0]
"Similarly these algorithms can encode hidden biases that disproportionately and adversely impact members with certain characteristics (e.g., gender and race).
",1. Introduction,[0],[0]
"In order to reduce the effect of information extraction on privacy and fairness, one needs to be able to remove sensitive data points (e.g., geolocations) or discard sensitive data features (e.g., skin color) from the dataset without incurring too much loss in performance.",1. Introduction,[0],[0]
"For instance, Article 17 of European “General Data Protection Regulation” states obligations with respect to providing individuals with the “Right to erasure (or Right to be forgotten)”.",1. Introduction,[0],[0]
"By exercising this right, individuals may enforce the service providers to delete their personal data or put restrictions from using part of it.",1. Introduction,[0],[0]
"Similarly, Title VII of the Civil Rights Act of American anti-discrimination law prohibits employment discrimination against certain characteristics (such as color and sex).",1. Introduction,[0],[0]
"Thus, to obtain fairer machine learning algorithms, we need to reduce the bias inherent in the training examples due to the lack of certain types of information, not being representative, or reflecting historical biases.",1. Introduction,[0],[0]
"This can be done by either removing protected attributes from training data
(Zemel et al., 2013) or train them separately for different protected groups (Chayes, 2017), among other procedures.",1. Introduction,[0],[0]
"Unfortunately, sensitive features or biased data usually are not known a priori and we might be aware of their existence just after training our models (Beutel et al., 2017).",1. Introduction,[0],[0]
"Retraining a machine learning model from scratch, after removing sensitive features and biased data, is quite expensive for large datasets.",1. Introduction,[0],[0]
Deletion-robust submodular maximization can save a lot of time and computational resources in these scenarios.,1. Introduction,[0],[0]
"In this paper, we provide a computationally feasible way of rerunning the algorithms should some attributes or data points be discarded.
",1. Introduction,[0],[0]
"Most existing submodular maximization methods, often used for data extraction (Mirzasoleiman et al., 2013) and informative subset selection (Wei et al., 2015), do not provide such guarantees.",1. Introduction,[0],[0]
"In this paper, we develop the first scalable and memory-efficient algorithms for maximizing a submodular function subject to a cardinality constraint that are robust against any number of adversarial deletions.",1. Introduction,[0],[0]
"This is in sharp contrast to previous methods that could only handle a fixed number of deletions (Orlin et al., 2016; Bogunovic et al., 2017) or otherwise their memory requirement scales multiplicatively with the number of deletions (Mirzasoleiman et al., 2017).
",1. Introduction,[0],[0]
"Our contributions: For a monotone submodular function with a cardinality constraint k, we develop the following randomized algorithms that are robust against any d deletions: 1.",1. Introduction,[0],[0]
Centralized:,1. Introduction,[0],[0]
We propose ROBUST-CENTRALIZED that achieves (1/2 )-approximation guarantee (in expectation) with the memory requirement O (k + d log k/ 2).,1. Introduction,[0],[0]
"Note that the memory complexity is only a logarithmic factor (e.g., log k) away from a trivial lower bound O(k + d).",1. Introduction,[0],[0]
2.,1. Introduction,[0],[0]
"Streaming: We propose ROBUST-STREAMING that achieves (1/2 )-approximation guarantee (in expectation) with the memory requirement O k log k/ + d log2 k/ 3 .
3.",1. Introduction,[0],[0]
Distributed: We propose ROBUST-DISTRIBUTED that achieves (0.218 )-approximation,1. Introduction,[0],[0]
"guarantee (in expectation) with the memory requirement O (m(k + d log k/ 2)), where m is the number of machines.",1. Introduction,[0],[0]
"We also introduce COMPACT-DISTRIBUTED, a variant of ROBUSTDISTRIBUTED, where its memory requirement is independent of number of machines.
",1. Introduction,[0],[0]
Table 1 compares our proposed methods with previous algorithms.,1. Introduction,[0],[0]
The proofs of all the theoretical results are deferred to the Supplementary Material.,1. Introduction,[0],[0]
"Monotone submodular maximization under cardinality constraints is studied extensively in centralized, streaming and distributed scenarios.",2. Related Work,[0],[0]
"The classical result of Nemhauser et al. (1978) proves that the simple GREEDY algorithm that starts with an empty set and iteratively adds elements with
the highest marginal gain provides (1 1/e)-approximation guarantee.",2. Related Work,[0],[0]
"To scale to large datasets, several streaming algorithms with constant factor approximations have recently been proposed (Badanidiyuru et al., 2014; Kumar et al., 2015; Buchbinder et al., 2015).",2. Related Work,[0],[0]
"Also, different distributed submodular maximization algorithms have been developed lately (Mirzasoleiman et al., 2013; Mirrokni & Zadimoghaddam, 2015; Barbosa et al., 2015).
",2. Related Work,[0],[0]
"Krause et al. (2008) introduced the robust formulation of the classical cardinality constrained submodular maximization for the first time and gave a bi-criterion approximation to the problem of max|A|k mini2{1,··· ,`} fi(A), where fi is normalized monotone submodular for every i. Note that submodular maximization of function f that is robust to the deletion of d items can be modeled as a special case of this problem: max|A|k min|D|d f(A \D).",2. Related Work,[0],[0]
Krause et al. (2008) guaranteed a robust solution by returning a set whose size is k(1 + ⇥(log(dk log n)).,2. Related Work,[0],[0]
"There are two main drawbacks with this approach when applied to deletions: first, the size of final solution is logarithmically larger than k, and second, the running time is exponential in d. Orlin et al. (2016) designed a centralized algorithm that outputs a set of cardinality k in a polynomial time.",2. Related Work,[0],[0]
Their algorithm is robust to the deletion of only o( p k) elements.,2. Related Work,[0],[0]
Bogunovic et al. (2017) further improved the result of Orlin et al. (2016) to o(k) deletions.,2. Related Work,[0],[0]
The approximation guarantees for both of these algorithms are 0.387.,2. Related Work,[0],[0]
The aforementioned methods try to construct a solution without allowing to update the answer after deletion.,2. Related Work,[0],[0]
"In contrast, Mirzasoleiman et al. (2017) developed a streaming algorithm which is robust to the deletion of any number of d elements.",2. Related Work,[0],[0]
"They keep a set of size O(kd log k/ ), and after each deletion they find a feasible solution of size at most k from this set.",2. Related Work,[0],[0]
They also improved the approximation guarantee to 1/2 .,2. Related Work,[0],[0]
"The main drawback of this algorithm is the memory requirement, which is quite impractical for large values of d and k; e.g., for k = O( p n) and d = O( p n)",2. Related Work,[0],[0]
"the memory requirement is even larger than n. Independently and concurrently with our work, Mitrovic et al. (2017) presented a robust to deletion streaming algorithm.",2. Related Work,[0],[0]
"Also, there are several recent works on robust optimization of non-submodular functions (Bogunovic et al., 2018; Tzoumas et al., 2018).
",2. Related Work,[0],[0]
"Submodular maximization has been widely used in classical machine learning and data mining applications, including extracting representative elements with exemplar based clustering (Krause & Gomes, 2010), data summarization through active set selection (Herbrich et al., 2003; Seeger, 2004), feature selection (Krause & Guestrin, 2005) and document summarization (Lin & Bilmes, 2011).",2. Related Work,[0],[0]
Assume we have a set function f : 2V !,3. Problem Definition,[0],[0]
R 0.,3. Problem Definition,[0],[0]
"We define the marginal gain of an element e 2 V to the set A ✓ V
by f (e|A) = f(A",3. Problem Definition,[0],[0]
[ {e}) f(A).,3. Problem Definition,[0],[0]
"The function f is submodular if for all A ✓ B ✓ V and e 2 V \ B, we have f (e|A) f (e|B).",3. Problem Definition,[0],[0]
"A submodular function f is monotone if for every A ✓ B ✓ V , we have f(A)  f(B).
",3. Problem Definition,[0],[0]
"In many submodular optimization applications, a subset of items of the ground set V may be removed at different points in time.",3. Problem Definition,[0],[0]
"For this reason, we require to find solutions which are robust to the deletion.",3. Problem Definition,[0],[0]
"Indeed, the goal is to maximize a submodular function f over a set V of items under a cardinality constraint k, where it is robust to the deletion of any subset D ⇢ V of size |D|  d. More precisely, we are interested in solving the following problem for each possible (and unknown a priori) instance of D:
S ⇤ = argmax
S✓V \D,|S|k f(S).",3. Problem Definition,[0],[0]
"(1)
We also define OPT = f(S⇤).",3. Problem Definition,[0],[0]
The most straightforward approach to this problem is to solve Eq.,3. Problem Definition,[0],[0]
"(1) for each instance of D. Unfortunately, solving Eq. (1), for large datasets, is computationally prohibitive.",3. Problem Definition,[0],[0]
"Also, deletion of elements from the set V can happen at different stages in real time applications.",3. Problem Definition,[0],[0]
This makes the problem even harder.,3. Problem Definition,[0],[0]
"Our solution to this problem is to maintain a small set A ⇢ V, called a core-set of V, where for each set D we can efficiently find a subset B ✓ A \D that provides an acceptable approximation for Eq.",3. Problem Definition,[0],[0]
(1).,3. Problem Definition,[0],[0]
"Note that set A is constructed without knowing set D. For this reason, next we define the notion of (↵, d)-robust randomized core-set.",3. Problem Definition,[0],[0]
Definition 1.,3. Problem Definition,[0],[0]
"A random subset of A ✓ V is an (↵, d)-robust randomized core-set for a set V, if for any subset D ✓ V of size |D|  ",3. Problem Definition,[0],[0]
"d, there exists a B ✓ A \D, |B|  k such that
E[f(B)]",3. Problem Definition,[0],[0]
"↵ · max S✓V \D,|S|k f(S),
where expectation is taken over the randomization of set A.",3. Problem Definition,[0],[0]
"In this section, we present three fast and scalable randomized algorithms.",4. Robustness and Cardinality Constraint,[0],[0]
"These algorithms solve the problem of robust submodular maximization in centralized, streaming and distributed scenarios.",4. Robustness and Cardinality Constraint,[0],[0]
"Our algorithms provide, in expectation, constant factor approximation guarantees, where they
are robust to the (even adversarial) deletion of any d items from the set V. In our setting, an adversary might try to find a set of inputs for which our algorithms fail to provide good results.",4. Robustness and Cardinality Constraint,[0],[0]
"In order to make the optimization robust to the adversarial deletions, we introduce randomness in the selection process.",4. Robustness and Cardinality Constraint,[0],[0]
"We assume that the adversary does not have access to the random bits of the randomized algorithms.
",4. Robustness and Cardinality Constraint,[0],[0]
The proposed algorithms are designed based on a general idea that the elements are chosen randomly from a large enough pool of similar items.,4. Robustness and Cardinality Constraint,[0],[0]
"This idea is useful because the adversary is not aware of the random bits of the algorithms, which makes the deletion probability of elements we have chosen negligible.",4. Robustness and Cardinality Constraint,[0],[0]
"Therefore, we can bound the expected value of each selected set.
",4. Robustness and Cardinality Constraint,[0],[0]
Our solution consists of two steps.,4. Robustness and Cardinality Constraint,[0],[0]
"In the first step, we find a small core-set of elements (in comparison to the whole dataset).",4. Robustness and Cardinality Constraint,[0],[0]
"We prove that after the deletion of at most d arbitrary elements, we can still find a good approximation for the optimization problem in this small set.",4. Robustness and Cardinality Constraint,[0],[0]
"In the second step, we choose at most k elements from the core-set we have found in the first step.",4. Robustness and Cardinality Constraint,[0],[0]
We prove a constant approximation factor for our algorithm in expectation.,4. Robustness and Cardinality Constraint,[0],[0]
"This guarantees that the core-set is (↵, d)-robust randomized for a constant ↵ and arbitrary d.
In the optimization procedure, we use a thresholding idea to select elements.",4. Robustness and Cardinality Constraint,[0],[0]
"Similar ideas have been used previously for designing streaming algorithms (Badanidiyuru et al., 2014; Buchbinder et al., 2015; Chekuri et al., 2015).",4. Robustness and Cardinality Constraint,[0],[0]
"In those algorithms, when an element of the stream arrives, if this element has sufficiently large marginal value it is kept; otherwise it is discarded.",4. Robustness and Cardinality Constraint,[0],[0]
"In the robust submodular maximization, we keep a large enough pool of elements with sufficient marginal values before adding or discarding them.",4. Robustness and Cardinality Constraint,[0],[0]
We randomly pick an element when the size of pool is at least d/✏.,4. Robustness and Cardinality Constraint,[0],[0]
Thus the element picked at each step is deleted with a probability at most ✏.,4. Robustness and Cardinality Constraint,[0],[0]
"This is true because the size of deleted items is at most d. To guarantee the quality of the chosen elements after the deletion (i.e., we want the expected value of f over the set of picked elements does
not change a lot after deletion), not only they should have been picked from a large pool of elements, the elements of pool should have almost the same marginal gains.",4. Robustness and Cardinality Constraint,[0],[0]
"To explain, in more details, why we need this property consider the example in Appendix A.",4. Robustness and Cardinality Constraint,[0],[0]
"In this section we outline a centralized algorithm, called ROBUST-CORESET-CENTRALIZED, to find an (↵, d)-robust core-set.",4.1. Centralized Algorithm,[0],[0]
"We also present the ROBUST-CENTRALIZED algorithm which is able to find a good solution from the core-set.
",4.1. Centralized Algorithm,[0],[0]
Badanidiyuru et al. (2014) showed that one way to obtain a constant factor approximation to the classical submodular maximization problem is to use a thresholding idea.,4.1. Centralized Algorithm,[0],[0]
They proved that choosing elements with marginal gain at least ⌧ ⇤ = OPT2k from a stream until a maximum of k elements are chosen returns a set with an approximation factor of 1/2.,4.1. Centralized Algorithm,[0],[0]
The main problem with this primary idea is that the value of OPT is not known by the algorithm.,4.1. Centralized Algorithm,[0],[0]
"Badanidiyuru et al. (2014) pointed out that, from the submodularity of f , we have 0  OPT  k 0 where 0 is the largest value in set {f({e})|e 2 V }.",4.1. Centralized Algorithm,[0],[0]
"By dividing the range [ 0, k 0] into intervals of [⌧i, ⌧i+1) (where ⌧i+1/⌧i is close to 1) it is possible to find a good enough approximation for OPT.
",4.1. Centralized Algorithm,[0],[0]
"We should first note that due to the deletion process, the relevant maximum singleton value is not 0 anymore, and it is 00 = maxe2V \D f({e}).",4.1. Centralized Algorithm,[0],[0]
"The algorithm is unaware of set D, therefore 00 could be anywhere in the range [ d, 0] where d is the (d+ 1)-th largest value in the set {f({e})|e 2 V }.",4.1. Centralized Algorithm,[0],[0]
The lower bound of d is implied by the fact that at most d elements will be deleted.,4.1. Centralized Algorithm,[0],[0]
"So ⌧⇤ = OPT2k could fall anywhere in the range [ d/2k, 0].",4.1. Centralized Algorithm,[0],[0]
"Unlike the deletion free case, the upper and lower limits of this range do not differ only by a multiplicative factor of k, thus a naive approach makes us try arbitrarily large number of different choices to find a good estimate of ⌧⇤.",4.1. Centralized Algorithm,[0],[0]
"We resolve this issue by the following observation.
",4.1. Centralized Algorithm,[0],[0]
We reserve a set B of elements that might be valuable after the deletion process.,4.1. Centralized Algorithm,[0],[0]
"Let Vd be the (d + 1) largest singleton value elements, i.e., the top d + 1 elements in the set {f({e})|e 2 V }.",4.1. Centralized Algorithm,[0],[0]
"We preserve all elements of Vd for the next round by inserting them to B. This way, we do not have to worry about thresholds above d as all elements that might have marginal value above d to any set should be in set Vd",4.1. Centralized Algorithm,[0],[0]
"and they are added to B. Therefore, we consider all thresholds in the set T = {(1 + ✏)i| d2k  (1 + ✏)
i  d}.",4.1. Centralized Algorithm,[0],[0]
"Starting from the largest ⌧ 2 T to the smallest, we iteratively construct two sets A⌧ and B⌧ .",4.1. Centralized Algorithm,[0],[0]
"At the end of the algorithm, the set B is defined as the union of Vd and [⌧2TB⌧ .",4.1. Centralized Algorithm,[0],[0]
"We output set B, along with all sets {A⌧}⌧2T, as the core-set.
",4.1. Centralized Algorithm,[0],[0]
We initialize A⌧ to ?.,4.1. Centralized Algorithm,[0],[0]
"We let B⌧ to be the set of elements whose marginal values to the set [⌧ 0 ⌧A⌧ 0 is in the range
Algorithm 1 ROBUST-CORESET-CENTRALIZED
1: d the (d+ 1)-th largest value of {f({e})|e 2 V } 2: Vd all the d+ 1 elements with the largest values in
set {f({e})|e 2 V } 3: T = {(1 + ✏)i| d2(1+✏)k  (1 + ✏)
i  d} 4: For each ⌧ 2 T : {A⌧} ?",4.1. Centralized Algorithm,[0],[0]
and {B⌧} ?,4.1. Centralized Algorithm,[0],[0]
"5: V V \ Vd 6: for ⌧ 2 T from the highest to the lowest do 7: while |B⌧ | d/✏ for B⌧ = {e 2 V : ⌧ 
f (e|[⌧ 0 ⌧ A⌧ 0) <",4.1. Centralized Algorithm,[0],[0]
(1+ ✏)⌧} and |[⌧ 0 ⌧ A⌧ 0,4.1. Centralized Algorithm,[0],[0]
| < k,4.1. Centralized Algorithm,[0],[0]
"do
8: Randomly pick an element e from B⌧ and add it to A⌧ , i.e., A⌧ A⌧ [ {e}
9: V V \",4.1. Centralized Algorithm,[0],[0]
"(A⌧ [B⌧ ) 10: B {[B⌧} [ Vd 11: Return {A⌧}, B
[⌧, (1 + ✏)⌧).",4.1. Centralized Algorithm,[0],[0]
"We note that this is a dynamic definition and whenever we add an element to any of A⌧ sets, the related B⌧ set might change as well.",4.1. Centralized Algorithm,[0],[0]
Elements in the set B⌧ are similar to each other in terms of their marginal values.,4.1. Centralized Algorithm,[0],[0]
"Without deletions, we can choose any element from B⌧ and add it to our solution.",4.1. Centralized Algorithm,[0],[0]
"However, if B⌧ has only a few elements, the adversary can delete all of them, and we will be left with an arbitrary poor solution.",4.1. Centralized Algorithm,[0],[0]
"To make the selection process robust, we select a random element from B⌧ and add it to A⌧ only if there are at least d/✏ elements in B⌧ .",4.1. Centralized Algorithm,[0],[0]
"This way even if all the deleted elements are from the set B⌧ , the probability of each selected element being deleted is at most ✏.",4.1. Centralized Algorithm,[0],[0]
We also know that all elements added to A⌧ have similar marginal values and are interchangeable.,4.1. Centralized Algorithm,[0],[0]
We keep adding elements to A⌧ until either [⌧ 0 ⌧A⌧ 0 has k elements or the size of set B⌧ becomes smaller than d/✏.,4.1. Centralized Algorithm,[0],[0]
"At this stage, we keep both sets A⌧ and B⌧ as a part of the output core-set.",4.1. Centralized Algorithm,[0],[0]
We also remove them from the ground set V and move on to the next lower threshold.,4.1. Centralized Algorithm,[0],[0]
"The pseudo code of ROBUSTCORESET-CENTRALIZED is given in Algorithm 1.
",4.1. Centralized Algorithm,[0],[0]
The sets {A⌧} and B are the outputs (core-set) of ROBUSTCORESET-CENTRALIZED.,4.1. Centralized Algorithm,[0],[0]
"In Appendix B , we show how ROBUST-CENTRALIZED (with pseudo code given in Algorithm 2) returns a solution for submodular maximization problem after the deletion of set D.
Theorem 1.",4.1. Centralized Algorithm,[0],[0]
"For any > 0, by setting ✏ = 2 3 , ROBUSTCORESET-CENTRALIZED and ROBUST-CENTRALIZED satisfy the following properties:
• ROBUST-CENTRALIZED outputs a set S such that |S|  k and E[f(S)]",4.1. Centralized Algorithm,[0],[0]
"(1/2 ) · OPT.
• ROBUST-CORESET-CENTRALIZED outputs at most O (k + d log k/ 2) elements as the core-set.
",4.1. Centralized Algorithm,[0],[0]
•,4.1. Centralized Algorithm,[0],[0]
The query complexities of ROBUST-CORESETCENTRALIZED and ROBUST-CENTRALIZED are O ((k + log k/ )|V,4.1. Centralized Algorithm,[0],[0]
"|) and O ((k + d log k/ 2)(log k/ )).
",4.1. Centralized Algorithm,[0],[0]
"Algorithm 2 ROBUST-CENTRALIZED
1: Input: {A0 ⌧ } and B0 {A0 ⌧ and B0 contain ele-
ments of A⌧ and B (outputs of ROBUST-CORESETCENTRALIZED) after deletion.}
2: Output: Set S of cardinality at most k 3: 00 the largest value of {f({e})|e 2 {[A0⌧} [B0} 4: T0 = {(1 + ✏)i| 0 0
2(1+✏)k  (1 + ✏) i  00}
5: for ⌧ 2 T0 from the highest to the lowest do 6: S⌧ S ⌧ 02T0,⌧ 0 ⌧ A 0 ⌧ 0 7: for all e 2 B0 do 8: if f (e|S⌧ ) ⌧ and |S⌧ | < k then 9: S⌧ S⌧ [ e
10: Return argmaxS⌧ f(S⌧ )",4.1. Centralized Algorithm,[0],[0]
"In many applications, the dataset does not fit in the main memory of a single machine or even the data itself arrives as a stream.",4.2. Streaming Algorithm,[0],[0]
So it is not possible to use centralized algorithms which need random access to the whole data.,4.2. Streaming Algorithm,[0],[0]
"In this section, we present a streaming algorithm with a limited available memory.",4.2. Streaming Algorithm,[0],[0]
We first use the thresholding idea of Section 4.1 in order to find a core-set for V. Then we show that it is possible to find a good solution from this core-set when deletion happens.,4.2. Streaming Algorithm,[0],[0]
"Recall that for ROBUST-CORESETCENTRALIZED, the maximum singleton element and the thresholds are fixed while in the streaming setting, they may change as new elements arrive.",4.2. Streaming Algorithm,[0],[0]
"To apply ideas of the centralized algorithm, we should overcome the following challenges: (i) it is not possible to make several passes over the data for different thresholds (i.e., we cannot start from the largest possible marginal gain to the lowest), and (ii) the value of 0 and d are not known a priori.
",4.2. Streaming Algorithm,[0],[0]
We show that it is possible to maintain a good approximation of OPT even with a single pass over the data.,4.2. Streaming Algorithm,[0],[0]
"From now on, let 0 and d, respectively, denote the largest and the (d+ 1)-th largest singleton values in the stream of data at time step t. First, note that d  OPT and the marginal gain of all the currently received elements is at most 0.",4.2. Streaming Algorithm,[0],[0]
"Therefore, it is enough to consider thresholds in the range [ d2k , 0].",4.2. Streaming Algorithm,[0],[0]
A new threshold is instantiated when the maximum singleton element is changed.,4.2. Streaming Algorithm,[0],[0]
These new (increasing) thresholds are between the current maximum and the previous one.,4.2. Streaming Algorithm,[0],[0]
"Therefore, all the elements with marginal gains larger than the new threshold will appear after its instantiation.
",4.2. Streaming Algorithm,[0],[0]
"ROBUST-CORESET-STREAMING, for each threshold ⌧ , keeps two sets A⌧ and B⌧ = [⌧ 0 ⌧B⌧,⌧ 0 .",4.2. Streaming Algorithm,[0],[0]
All the elements with marginal gains at least ⌧ to set A⌧ are good enough to be picked by this instance of the algorithm.,4.2. Streaming Algorithm,[0],[0]
"In order to make the selected elements robust to deletions, we should put all good enough elements in different B⌧,⌧ 0 sets, with thresholds ⌧ 0 in the range [⌧, 0], based on their marginal values.",4.2. Streaming Algorithm,[0],[0]
"Whenever a set B⌧,⌧ 0 becomes large, we pick one
Algorithm 3 ROBUST-CORESET-STREAMING
1: T = {(1 + e)i|i 2 Z} 2: For each ⌧, ⌧ 0 2 T : {A⌧} ?",4.2. Streaming Algorithm,[0],[0]
"and {B⌧,⌧ 0} ?",4.2. Streaming Algorithm,[0],[0]
"3: for every arriving element et do 4: d the (d + 1)-th largest element of {f{e1}, · · · , {f{et}} 5: 0 the largest element of {f{e1}, · · · , {f{et}} 6: Tt = {(1 + ✏)i|",4.2. Streaming Algorithm,[0],[0]
"d2(1+✏)k  (1 + ✏)
i  d} 7: Delete all A⌧ and B⌧,⌧ 0 such the ⌧ or ⌧ 0 /2",4.2. Streaming Algorithm,[0],[0]
"Tt 8: for ⌧ 2 Tt do 9: if |A⌧ | < k and ⌧  f (e|A⌧ ) then
10: Add et to B⌧,⌧ 0 such that for ⌧ 0  ",4.2. Streaming Algorithm,[0],[0]
f (et|A⌧ ) <,4.2. Streaming Algorithm,[0],[0]
"⌧ 0(1 + ✏) 11: while 9⌧ 00 such that |B⌧,⌧ 00",4.2. Streaming Algorithm,[0],[0]
"| d/✏ do 12: Randomly pick an element e from B⌧,⌧ 00 and add it to A⌧ , i.e., A⌧ A⌧ [ {e} 13: For all e 2
S ⌧ 002Ti,⌧ 00 ⌧ B⌧,⌧ 00 recompute
f (e|A⌧ ) and re-place them in correct bins 14: for ⌧ 2 Tn do 15: B⌧ S ⌧ 02Tn,⌧ 0 ⌧ B⌧,⌧ 0 16: Return {A⌧}, {B⌧}
element of it randomly to add to A⌧ .",4.2. Streaming Algorithm,[0],[0]
This ensures that an element is picked from a large pool of almost similar elements.,4.2. Streaming Algorithm,[0],[0]
"Formally, all the elements with a marginal gain in the range [⌧ 0, ⌧ 0(1 + ✏)) are added to the set B⌧,⌧ 0 .",4.2. Streaming Algorithm,[0],[0]
"When the size of a B⌧,⌧ 0 is at least d/✏, we randomly pick an element from B⌧,⌧ 0 and add it to A⌧ .",4.2. Streaming Algorithm,[0],[0]
"Adding an element to A⌧ may decrease the marginal gains of elements in B⌧,⌧ 0 sets.",4.2. Streaming Algorithm,[0],[0]
"So we recompute their marginal gains and put them in the right B⌧,⌧ 00 set (they are kept if their marginal gains are at least ⌧ , otherwise they are discarded).",4.2. Streaming Algorithm,[0],[0]
"These changes may make another set large, so we keep adding elements to A⌧ while we find a large B⌧,⌧ 00 set.",4.2. Streaming Algorithm,[0],[0]
This process continues until a maximum of k elements are added to A⌧ or the stream of data ends.,4.2. Streaming Algorithm,[0],[0]
"Note that there are at most d elements with marginal gains in the range ( d, 0]; we can simply keep these elements (refer to it as set Vd).",4.2. Streaming Algorithm,[0],[0]
"For all d < ⌧  0, we have A⌧ = ?, because there is no pool of size at least d/✏ elements to pick from it.",4.2. Streaming Algorithm,[0],[0]
"Also, for B⌧,⌧ 0 sets, we do not need to cover the range ( d, 0] with too many thresholds.",4.2. Streaming Algorithm,[0],[0]
"Indeed, when d changes (it can only increase), we can update the set Vd and locate the removed elements from Vd into a correct B⌧,⌧ 0 .",4.2. Streaming Algorithm,[0],[0]
"Therefore, it is sufficient to consider only thresholds in the range[ d2k , d].",4.2. Streaming Algorithm,[0],[0]
"The pseudo code of ROBUST-CORESET-STREAMING is given in Algorithm 3.
",4.2. Streaming Algorithm,[0],[0]
"In Appendix D, we introduce another algorithm (called ROBUST-STREAMING) such that after deletion of any set D from the core-set finds a solution with an expected approximation guarantee of 1 3✏2 to the optimum solution.
",4.2. Streaming Algorithm,[0],[0]
Theorem 2.,4.2. Streaming Algorithm,[0],[0]
"For any > 0, by setting ✏ = 2 3 , ROBUSTCORESET-STREAMING and ROBUST-STREAMING satisfy the following properties:
Algorithm 4 ROBUST-DISTRIBUTED 1: for e 2 V do 2: Assign e to a machine i chosen uniformly at random; 3: Let Vi be the elements assigned to machine i 4: Run ROBUST-CORESET-CENTRALIZED (Algorithm 1)
on each machine to obtain {Ai ⌧ } and Bi
5: Run ROBUST-CENTRALIZED (Algorithm 2) on each {Ai
⌧ 0} and Bi0 to get the set Si of cardinality at most k from each machine {{Ai
⌧ 0} and Bi0 are elements of {Ai
⌧ } and Bi after deletion of set D.}
6: S argmaxSi{f(Si)} 7: T GREEDY({ S i S ⌧2Ti",4.2. Streaming Algorithm,[0],[0]
"A i ⌧ 0} S { S i B
i0}) 8: Return argmax{f(T ), f(S)}
• ROBUST-STREAMING outputs a set S such that |S|  k and E[f(S)]",4.2. Streaming Algorithm,[0],[0]
"(1/2 ) · OPT.
• ROBUST-CORESET-STREAMING makes one pass over the dataset.
",4.2. Streaming Algorithm,[0],[0]
"• ROBUST-CORESET-STREAMING outputs at most O k log k/ + d log2 k/ 3 elements as the core-set.
",4.2. Streaming Algorithm,[0],[0]
•,4.2. Streaming Algorithm,[0],[0]
The query complexities of ROBUST-CORESETSTREAMING and ROBUST-STREAMING are O |V,4.2. Streaming Algorithm,[0],[0]
| log k/ + dk log2 k/ 3 and,4.2. Streaming Algorithm,[0],[0]
O,4.2. Streaming Algorithm,[0],[0]
d log3 k/ 4 .,4.2. Streaming Algorithm,[0],[0]
"In this section, build upon ideas from (Mirzasoleiman et al., 2013; Mirrokni & Zadimoghaddam, 2015; Barbosa et al., 2015), we present a robust distributed submodular maximization algorithm, called ROBUST-DISTRIBUTED.",4.3. Distributed Algorithm,[0],[0]
"We prove that our distributed algorithm finds an (↵, d)-robust randomized core-set with a constant ↵ and any arbitrary d.
ROBUST-DISTRIBUTED is a two-round distributed algorithm within a MapReduce framework.",4.3. Distributed Algorithm,[0],[0]
It first randomly partitions dataset between m machines.,4.3. Distributed Algorithm,[0],[0]
"Each machine i runs ROBUST-CORESET-CENTRALIZED on its data and passes the result (i.e., sets {Ai
⌧ } and Bi) to a central machine.",4.3. Distributed Algorithm,[0],[0]
"Af-
ter the deletion of the set D, this single central machine runs m instances of ROBUST-CENTRALIZED on the outputs received from each machine",4.3. Distributed Algorithm,[0],[0]
i and finds solutions Si.,4.3. Distributed Algorithm,[0],[0]
"In addition, it runs the classical GREEDY on the union of sets received from all machines (i.e., union of all sets {Ai
⌧ 0} and B
i0) to find another solution T .",4.3. Distributed Algorithm,[0],[0]
The final solution is the best answer among T and sets Si.,4.3. Distributed Algorithm,[0],[0]
ROBUST-DISTRIBUTED is outlined in Algorithm 4.,4.3. Distributed Algorithm,[0],[0]
Theorem 3.,4.3. Distributed Algorithm,[0],[0]
"For any > 0, by setting ✏ = /2, ROBUSTDISTRIBUTED outputs a set S, |S|  k such that E[f(S)]",4.3. Distributed Algorithm,[0],[0]
"↵ /(↵+ ) · OPT, where ↵ = 1/3 and = 1 1/e.",4.3. Distributed Algorithm,[0],[0]
This results in an approximation factor of 0.218 .,4.3. Distributed Algorithm,[0],[0]
Corollary 1.,4.3. Distributed Algorithm,[0],[0]
Running ROBUST-CORESET-CENTRALIZED on the output of ROBUST-DISTRIBUTED produces a compact core-set of size O (k + d log k/ 2).,4.3. Distributed Algorithm,[0],[0]
"Also, ROBUSTCENTRALIZED finds a solution with (0.109 )-
approximation guarantee from this compact core-set.",4.3. Distributed Algorithm,[0],[0]
"We refer to this version of our distributed algorithm as COMPACTDISTRIBUTED.
",4.3. Distributed Algorithm,[0],[0]
The main motivation of COMPACT-DISTRIBUTED is that the memory complexity does not increase with the number of machines m (while it still provides a constant factor approximation).,4.3. Distributed Algorithm,[0],[0]
"In this section, we extensively evaluate the performance of our algorithms on several publicly available real-world datasets.",5. Experimental Results,[0],[0]
We consider algorithms that can be robust to the deletion of any number of items and return k elements after deletion.,5. Experimental Results,[0],[0]
"Note that both OSU (Orlin et al., 2016) and PRO-GREEDY (Bogunovic et al., 2017) are robust to the deletion of only o(k) items.",5. Experimental Results,[0],[0]
"For this reason, we compare our proposed methods with three other baselines: (i) ROBUST (Mirzasoleiman et al., 2017), (ii) STAR-T-GREEDY (Mitrovic et al., 2017), and (iii) the stochastic greedy algorithm (Mirzasoleiman et al., 2015) (SG), where we first obtain a solution S of size r = 6k (we set r > k to make the solution robust to deletion), and then we report GREEDY(S \D) as the final answer.
",5. Experimental Results,[0],[0]
"In our experiments, we evaluate the effect of three parameters: (i) d where an algorithm is designed to be robust to d deletions; (ii) cardinality constraint k of the final solution; and (iii) number of deleted elements r.",5. Experimental Results,[0],[0]
The objective value of all algorithms are normalized to the utility obtained from a classical greedy algorithm that knows the set of deleted items D beforehand.,5. Experimental Results,[0],[0]
"Note that we are able to guarantee the performance of our algorithms (also this is true for ROBUST (Mirzasoleiman et al., 2017) and STAR-T-GREEDY (Mitrovic et al., 2017))",5. Experimental Results,[0],[0]
"only when the number of deletions r is less than d. While the theoretical improvements of our algorithms for larger values of d is more significant (see Table 1), for a fair comparison, we used the experimental setting of Mirzasoleiman et al. (2017).",5. Experimental Results,[0],[0]
"In these experiments, we also evaluate the effect of larger number of deletions, i.e., where r d.",5. Experimental Results,[0],[0]
"We observe, even though our algorithms are not designed for such higher number of deletions, they demonstrate a gracefully robust behavior.",5. Experimental Results,[0],[0]
"In a wide range of applications, data can be represented as a kernel matrix K, which encodes the similarity between different items in the database.",5.1. Location Privacy,[0],[0]
"In order to find a representative set S of cardinality k, a common objective function is
f(S) = log det(I + ↵KS,S), (2)
where KS,S is the principal sub-matrix of K indexed by S and ↵ > 0 is a regularization parameter (Herbrich et al., 2003; Seeger, 2004; Krause & Guestrin, 2005).",5.1. Location Privacy,[0],[0]
"This function is monotone submodular.
",5.1. Location Privacy,[0],[0]
"In this section, we analyze a dataset of 10,000 geolocations.
",5.1. Location Privacy,[0],[0]
"Each data entry is longitude and latitude coordinates of Uber pickups in Manhattan, New York in April 2014 (UberDataset).",5.1. Location Privacy,[0],[0]
Our goal is to find k representative samples using the objective function described in Eq.,5.1. Location Privacy,[0],[0]
(2).,5.1. Location Privacy,[0],[0]
"The similarity of two location samples i and j is defined by a Gaussian kernel Ki,j = exp( d2i,j/h2), where the distance di,j (in meters) is calculated from the coordinates and h is set to 5000.",5.1. Location Privacy,[0],[0]
"We set d = 5, i.e., we make algorithms (theoretically) robust to deletion of at most five elements.",5.1. Location Privacy,[0],[0]
"To compare the effect of deletions on the performance of algorithms, we use two strategies to choose the deleted items: (i) classical greedy algorithm, and (ii) the stochastic greedy algorithm.
",5.1. Location Privacy,[0],[0]
"In the first experiment, we study the effect of deleting different number of items on the normalized objective values.",5.1. Location Privacy,[0],[0]
"To refer to an algorithm with a specific deletion strategy, we use the name of algorithm followed by the deletion strategy, e.g., Rob-Stream-G refers to ROBUST-STREAMING where the deleted items are picked by greedy strategy.",5.1. Location Privacy,[0],[0]
"From Fig. 1a, we observe that ROBUST-STREAMING and ROBUST-CENTRALIZED are more robust to deletion than ROBUST and SG.",5.1. Location Privacy,[0],[0]
The effect of deleting by greedy strategy on the performance of algorithms is more pronounced than SG strategy.,5.1. Location Privacy,[0],[0]
"It can be seen that, even by deleting more than d = 5 items, our algorithms maintain their performance.",5.1. Location Privacy,[0],[0]
"Also, SG (which is not designed to be robust to deletions) shows the worst performance.
",5.1. Location Privacy,[0],[0]
"Other than normalized objective values, the memory requirement of each algorithm is quite important.",5.1. Location Privacy,[0],[0]
"Indeed, we are interested in deletion-robust algorithms that do not keep many items.",5.1. Location Privacy,[0],[0]
Fig.,5.1. Location Privacy,[0],[0]
1b compares the memory complexity of algorithms.,5.1. Location Privacy,[0],[0]
We observe that ROBUST-CENTRALIZED needs to keep the least number of items.,5.1. Location Privacy,[0],[0]
"For ROBUST algorithm, the memory complexity increases super linear in k (it is O(k log k)), which makes it quite impractical for large values of k and d. Also, we observe ROBUST-STREAMING outperforms STAR-T-GREEDY in both objective function and memory requirement.",5.1. Location Privacy,[0],[0]
"To sum-up, we observe that our algorithms provide the best of two worlds: while their normalized objective values are clearly better than other baselines, they need to keep much fewer number of items.",5.1. Location Privacy,[0],[0]
One of the challenges in learning from high dimensional data is to select a subset of relevant features in a computationally feasible way.,5.2. Submodular Feature Selection,[0],[0]
"For this reason, the quality of a subset of features S can be captured by the mutual information between attributes in S and the class variable Y (Krause & Guestrin, 2005).",5.2. Submodular Feature Selection,[0],[0]
"More specifically,
I(Y ;XS) = X
y2Y
X
x2XS
p(x, y) log2
✓ p(x, y)
p(x)p(y)
◆ ,
where XS is a random variable that represents the set S of k features.",5.2. Submodular Feature Selection,[0],[0]
"The joint distribution on (Y,X1, · · · , Xk),
under the Naive Bayes assumption, is defined by p(y, x1, · · · , xk) = p(y) Q k
i=1 p(xi|y).",5.2. Submodular Feature Selection,[0],[0]
This assumption makes the computation of joint distribution tractable.,5.2. Submodular Feature Selection,[0],[0]
"In our experiments, we estimate each p(xi|y) by counting frequencies in the dataset.",5.2. Submodular Feature Selection,[0],[0]
"In the feature selection problem, the goal is to choose k features such that maximizing f(S) = I(Y ;XS).",5.2. Submodular Feature Selection,[0],[0]
"It is known that the function f(S) = I(Y ;XS), under the Naive Bayes assumption, is monotone submodular (Krause & Guestrin, 2005).
",5.2. Submodular Feature Selection,[0],[0]
"In this section and Appendix G, we use this feature selection method on two real datasets.",5.2. Submodular Feature Selection,[0],[0]
"We first show that our algorithms, after the deletion of sensitive features (i.e., features that might cause unfairness in the final classifier) provide results with near optimal quality (based on mutual information).",5.2. Submodular Feature Selection,[0],[0]
"Second, we demonstrate that classifiers that are trained on these selected features perform very well.
",5.2. Submodular Feature Selection,[0],[0]
"In the first experiment, we use the Adult Income dataset from UCI Repository (Blake & Merz, 1998).",5.2. Submodular Feature Selection,[0],[0]
"This dataset contains information about 32,561 individuals and whether income of those individuals is over 50K a year.",5.2. Submodular Feature Selection,[0],[0]
We extract 113 binary features from this dataset.,5.2. Submodular Feature Selection,[0],[0]
"The goal of the classification task is to predict the income status of 16,281 test cases.",5.2. Submodular Feature Selection,[0],[0]
"For the deletions, we remove sensitive features that might result in the unfairness, e.g., features about sex, race, nationality, marital status and relationship status.",5.2. Submodular Feature Selection,[0],[0]
Fig.,5.2. Submodular Feature Selection,[0],[0]
1c compares algorithms based on different number of deletions for k = 5 and k = 10.,5.2. Submodular Feature Selection,[0],[0]
"We observe that for both values of k, ROBUST-CENTRALIZED considerably outperforms ROBUST (Mirzasoleiman et al., 2017) and SG.",5.2. Submodular Feature Selection,[0],[0]
"Also, the performance of ROBUST is better than SG.
To further investigate the effect of deletions, we compare accuracy of different classifiers, where each is trained on the features found by our algorithms and baselines.",5.2. Submodular Feature Selection,[0],[0]
"We train two type of classifiers: (i) Naive Bayes (Zhang, 2004) and (ii) SVM (Smola & Schölkopf, 2004).",5.2. Submodular Feature Selection,[0],[0]
"From Table 2, we observe that a SVM classifier, which is trained over all features, results in an accuracy of 83.0%.",5.2. Submodular Feature Selection,[0],[0]
"If we use a greedy algorithm to find the best 5 features and train SVM classifier on those features, the accuracy will drop to 79.6% (clearly there is a trade off between the number of features and accuracy).",5.2. Submodular Feature Selection,[0],[0]
"After deleting 10 features that might result in unfairness in classification (e.g., race and sex), we again use the greedy algorithm to find the best five features (referred to as GREEDYD).",5.2. Submodular Feature Selection,[0],[0]
The accuracy in this case is 79.3%.,5.2. Submodular Feature Selection,[0],[0]
"Interestingly, we observe that the accuracies of classifiers which are trained on the features found by ROBUST-CENTRALIZED and ROBUST-STREAMING drop only by 0.2%.",5.2. Submodular Feature Selection,[0],[0]
"Also, for Naive Bayes classifier, we do not observe any decrease on the accuracy when we train on the features found by our algorithms.",5.2. Submodular Feature Selection,[0],[0]
"Finally, both Centralized (22) and Streaming (29) algorithms need to keep fewer number of items than ROBUST (39) and STAR-T-GREEDY (50).",5.2. Submodular Feature Selection,[0],[0]
"To evaluate the performance of ROBUST-DISTRIBUTED on large datasets, we consider the Census1990 dataset from UCI Repository (Blake & Merz, 1998).",5.3. Large Data Summarization,[0],[0]
"This dataset consists of 2,458,285 data points with 68 features.",5.3. Large Data Summarization,[0],[0]
We are going to find k representative samples from this large dataset.,5.3. Large Data Summarization,[0],[0]
We apply the set selection objective function described in Eq.,5.3. Large Data Summarization,[0],[0]
(2).,5.3. Large Data Summarization,[0],[0]
"The similarity between two entries x and x0 is defined by 1 kx x
0kp 68 , where kx x0k is the Euclidean distance between feature vectors of x and x0.
",5.3. Large Data Summarization,[0],[0]
We randomly split the dataset into m = 12 partitions.,5.3. Large Data Summarization,[0],[0]
"For each instance of ROBUST-CORESET-CENTRALIZED, we set d = 25 with an ✏ = 0.1.",5.3. Large Data Summarization,[0],[0]
"As a baseline, we consider a distributed version of stochastic greedy algorithm (refer to it as SG-DISTRIBUTED).",5.3. Large Data Summarization,[0],[0]
"For this algorithm, we first run stochastic greedy on each partitions to select Si = 6k items.",5.3. Large Data Summarization,[0],[0]
"After deletion of D, we report f(GREEDY([Si \D))",5.3. Large Data Summarization,[0],[0]
as the final result.,5.3. Large Data Summarization,[0],[0]
"Also, we normalize the utility of functions to the objective value of an instance of SG-DISTRIBUTED that knows the set of deleted items D in advance.",5.3. Large Data Summarization,[0],[0]
"For deletions, we propose four different strategies: D1 randomly deletes 50% of items, D2 randomly deletes 80% of items, D3 deletes all men in the dataset, and D4 deletes all women.
",5.3. Large Data Summarization,[0],[0]
"We investigate the effect of different deletion strategies for
two values of k 2 {50, 100}.",5.3. Large Data Summarization,[0],[0]
In Figs.,5.3. Large Data Summarization,[0],[0]
"2a and 2b, we observe that ROBUST-DISTRIBUTED clearly outperforms SGDISTRIBUTED in all cases.",5.3. Large Data Summarization,[0],[0]
"Furthermore, we observe that the objective value of ROBUST-DISTRIBUTED in all scenarios is even better than our reference function for normalization (normalized objective values are larger than 1).",5.3. Large Data Summarization,[0],[0]
Each machine on average stores 209.3 (for k = 50) and 348.3 (for k = 100) items.,5.3. Large Data Summarization,[0],[0]
"The standard deviations of memory complexities are 36.9 and 26.5, respectively.",5.3. Large Data Summarization,[0],[0]
"To conclude, ROBUST-DISTRIBUTED enables us to robustly summarize a dataset of size 2,458,285 with storing only ⇡4500 items.",5.3. Large Data Summarization,[0],[0]
Our experimental results confirm that this core-set is robust to the deletion of even 80% of items.,5.3. Large Data Summarization,[0],[0]
"In this paper, we considered the problem of deletion-robust submodular maximization.",6. Conclusion,[0],[0]
"We provided the first scalable and memory-efficient solutions in different optimization settings, namely, centralized, streaming, and distributed models of computation.",6. Conclusion,[0],[0]
We rigorously proved that our methods enjoy constant factor approximations with respect to the optimum algorithm that is also aware of the deleted set of elements.,6. Conclusion,[0],[0]
We showcased the effectiveness of our algorithms on real-word problems where part of data should be deleted due to privacy and fairness constraints.,6. Conclusion,[0],[0]
Amin Karbasi was supported by a DARPA Young Faculty Award (D16AP00046) and a AFOSR Young Investigator Award (FA9550-18-1-0160).,Acknowledgements,[0],[0]
Ehsan Kazemi was supported by the Swiss National Science Foundation (Early Postdoc.,Acknowledgements,[0],[0]
Mobility) under grant number 168574.,Acknowledgements,[0],[0]
Can we efficiently extract useful information from a large user-generated dataset while protecting the privacy of the users and/or ensuring fairness in representation?,abstractText,[0],[0]
We cast this problem as an instance of a deletion-robust submodular maximization where part of the data may be deleted or masked due to privacy concerns or fairness criteria.,abstractText,[0],[0]
"We propose the first memory-efficient centralized, streaming, and distributed methods with constant-factor approximation guarantees against any number of adversarial deletions.",abstractText,[0],[0]
"We extensively evaluate the performance of our algorithms on real-world applications, including (i) Uber-pick up locations with location privacy constraints; (ii) feature selection with fairness constraints for income prediction and crime rate prediction; and (iii) robust to deletion summarization of census data, consisting of 2,458,285 feature vectors.",abstractText,[0],[0]
Our experiments show that our solution is robust against even 80% of data deletion.,abstractText,[0],[0]
Scalable Deletion-Robust Submodular Maximization: Data Summarization with Privacy and Fairness Constraints,title,[0],[0]
"Multi-label learning (Gibaja & Ventura, 2015; 2014) is the problem of assigning to an object a subset of labels from a potentially very large label vocabulary (Prabhu & Varma, 2014; Jain et al., 2016; Babbar & Schölkopf, 2017).",1. Introduction,[0],[0]
"In contrast to binary or multi-class classification, in multilabel learning, each example is associated with a binary label vector (potentially very large), denoting the presence/absence (relevance/irrelevance) of each label.",1. Introduction,[0],[0]
"Multilabel learning has applications in several domains such as computer vision (Wang et al., 2016), computational adver-
*Equal contribution 1Department of Computer Science and Enginerring, IIT Kanpur, Kanpur 208016, UP, India.",1. Introduction,[0],[0]
"Correspondence to: Vikas Jain <vikasj@iitk.ac.in>, Nirbhay Modhe <nirbhaym@iitk.ac.in>, Piyush Rai <piyush@cse.iitk.ac.in>
Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
tising and recommender systems (Prabhu & Varma, 2014; Jain et al., 2016), etc.
",1. Introduction,[0],[0]
Several state-of-the-art methods for multi-label learning are based on certain structural assumptions on the binary label matrix.,1. Introduction,[0],[0]
"Some of the key structural assumptions that have been used in prior work include low-rank assumption (Yu et al., 2014), locally low-rank assumption (Bhatia et al., 2015), and low-rank plus sparse assumption (Xu et al., 2016), and clusters/topics of labels assumption (Cissé et al., 2016; Rai et al., 2015).",1. Introduction,[0],[0]
"Models based on these assumptions are broadly dubbed as embedding based methods for multi-label learning and offer two key advantages: (1) The relatedness/correlation among labels can be easily modeled/captured, and (2) the label vector for each example can be represented as a low-dimensional embedding, which faciliates developing computationally scalable models for multi-label learning.",1. Introduction,[0],[0]
"A more detailed discussion of prior work is provided in the Related Work section.
",1. Introduction,[0],[0]
"Despite the considerable recent interest and progress on the problem of multi-label learning (Yu et al., 2014; Bhatia et al., 2015; Wang et al., 2016; Cissé et al., 2016), a number of important issues still remain.",1. Introduction,[0],[0]
"One of such issues, especially for the embdding based methods, is the ambiguity regarding the zeros vs unobserved (missing) entries in the binary label vector of each example.",1. Introduction,[0],[0]
"Since, in practice, the true value (0/1) for only a small subset of all the labels can be obtained, the zeros in the label vector do not necessarily represent negative labels.",1. Introduction,[0],[0]
"A typical heuristic employed by multi-label learning algorithms is to simply treat all such the zeros in the label vector as are true negatives (Yu et al., 2014).",1. Introduction,[0],[0]
"Another heuristic is to assign different weights to the zeros and ones in the binary label matrix (Yu et al., 2017), which is inspired by matrix factorization based collaborative filtering models that learn from implicit (binary) feedback data (Hu et al., 2008).",1. Introduction,[0],[0]
"However, a more principled strategy to address this issue is highly desirable.
",1. Introduction,[0],[0]
"Another important desideratum is scalability, especially in the case of extreme multi-label learning problems (Prabhu & Varma, 2014; Jain et al., 2016; Babbar & Schölkopf, 2017), which are characterized by a massive number of labels, features, and examples.",1. Introduction,[0],[0]
"Although a number of recent multi-label learning models have been proposed that can scale to large-scale problems, these models usually require
large computational resources to truly scale to massive data sets (Babbar & Schölkopf, 2017; Bhatia et al., 2015; Jain et al., 2016).",1. Introduction,[0],[0]
"Moreover, most of the scalable multi-label learning algorithms only operate in batch setting and are usually not designed to work (Prabhu & Varma, 2014; Bhatia et al., 2015; Jain et al., 2016) in online settings with continuous stream of training examples.
",1. Introduction,[0],[0]
"In this paper, we present a scalable, generative framework for multi-label learning, that not only bring to bear the modeling flexibility of probabilistic, generative models for the multi-label learning problem (Kapoor et al., 2012; Rai et al., 2015), but is also designed to handle the abovementioned challenges in a principled way.",1. Introduction,[0],[0]
"Our framework is based on a latent factor model for the binary label matrix, and has the following distinguishing aspects: (1) It naturally handles the issue of missing vs negative labels via a principled generative model with a exposure model (Liang et al., 2016) for the label matrix; (2) It is accompanied by a simple and scalable inference procedure (both via Gibbs sampling and via fast point estimation); and (3) Inference can also be easily performed in an online fashion, enabling us to apply it on large-scale problems, even when using moderate computational resources.",1. Introduction,[0],[0]
"In the multi-label learning problem, we assume that we are givenN training examples {(x1,y1), . . .",2. The Model,[0],[0]
", (xN ,yN )}",2. The Model,[0],[0]
"with xn ∈ RD and yn ∈ {0, 1}L, n = 1, . . .",2. The Model,[0],[0]
", N .",2. The Model,[0],[0]
"We will denote X = {x1, . . .",2. The Model,[0],[0]
",xN} ∈ RN×D to be the feature matrix and Y = {y1, . . .",2. The Model,[0],[0]
",yN} ∈ {0, 1}N×L to be the label matrix.",2. The Model,[0],[0]
"Given training data {X,Y}, the goal in multi-label learning is to learn a model that can predict the label vector y∗ ∈ {0, 1}L for a new test input x∗ ∈ RD.
",2. The Model,[0],[0]
Note that an entry yn` = 0 in the label matrix Y may not necessarily mean a negative label but could simply mean that this label is missing (and its true value could be 0 or 1).,2. The Model,[0],[0]
"As we shall show, our generative model can infer the missingness of a label yn` = 0 by associating another binary latent variable ξn` (called exposure variable).",2. The Model,[0],[0]
These exposure variables will be incorporated in a latent factor model (Sec. 2.1) for the label matrix Y and are jointly learned along with the rest of the model parameters.,2. The Model,[0],[0]
We model the binary label matrix Y using a latent factor model.,2.1. An Exposure-based Latent Factor Model for the Binary Label Matrix,[0],[0]
"Specifically, we assume that each training example n = 1, . . .",2.1. An Exposure-based Latent Factor Model for the Binary Label Matrix,[0],[0]
", N is associated with a latent factor un ∈ RK and each label ` = 1, . . .",2.1. An Exposure-based Latent Factor Model for the Binary Label Matrix,[0],[0]
", L is associated with a latent factor v` ∈ RK .",2.1. An Exposure-based Latent Factor Model for the Binary Label Matrix,[0],[0]
We further condition un on the feature vector xn ∈,2.1. An Exposure-based Latent Factor Model for the Binary Label Matrix,[0],[0]
RD of example n,2.1. An Exposure-based Latent Factor Model for the Binary Label Matrix,[0],[0]
"by as-
suming that the prior distribution of un is conditioned on xn, as p(un|xn) = N (un|Wxn, λ−1u IK).",2.1. An Exposure-based Latent Factor Model for the Binary Label Matrix,[0],[0]
"Here, W =",2.1. An Exposure-based Latent Factor Model for the Binary Label Matrix,[0],[0]
"[w1, . . .",2.1. An Exposure-based Latent Factor Model for the Binary Label Matrix,[0],[0]
",wK ]
> ∈ RK×D which denotes the matrix of regression weights that map the feature vectorxn to the mean of the Gaussian prior on un.",2.1. An Exposure-based Latent Factor Model for the Binary Label Matrix,[0],[0]
"We further assume a zeromean Gaussian prior p(v`) = N (v`|0, λ−1v IK) on label latent factors v`, ` = 1, . . .",2.1. An Exposure-based Latent Factor Model for the Binary Label Matrix,[0],[0]
", L. Note that, although we do not consider it here, our model can also be easily extended to incorporate label features (if available) by conditioning Gaussian prior on v` on those label features, in the same manner we condition the prior on un on input features.
",2.1. An Exposure-based Latent Factor Model for the Binary Label Matrix,[0],[0]
"The complete generative story for each label yn` of the binary label matrix Y is given by
un|xn ∼ N (un|Wxn, λ−1u IK) (1) v` ∼ N (v`|0, λ−1v IK) (2) ξn` ∼ Bernoulli(µn`) (3)
yn` ∼
{ Bernoulli ( yn`|σ(u>n v`) ) , if ξn` = 1
δ0, if ξn` = 0 (4)
where σ(z) = 1/(1 + exp(−z))",2.1. An Exposure-based Latent Factor Model for the Binary Label Matrix,[0],[0]
denotes the logistic function.,2.1. An Exposure-based Latent Factor Model for the Binary Label Matrix,[0],[0]
"Note that we have associated a binary exposure latent variable ξn` with each label yn` such that ξn` = 0 implies that yn` is 0 because it is missing (not exposed), and ξn` = 1 implies that yn` is exposed (and could be 0 or 1 depending on the outcome of the Bernoulli draw).",2.1. An Exposure-based Latent Factor Model for the Binary Label Matrix,[0],[0]
"In Eq. 4, δ0 denotes a point-mass at zero, which means that, if ξn` = 0, then yn` is zero with probability 1.",2.1. An Exposure-based Latent Factor Model for the Binary Label Matrix,[0],[0]
"Otherwise, we draw the observed label yn` from a Bernoulli distribution as yn` ∼ Bernoulli(yn`|σ(u>n v`)).",2.1. An Exposure-based Latent Factor Model for the Binary Label Matrix,[0],[0]
"Note that, effectively, each yn` is being modeled using a mixture of two distributions - a Bernoulli with probability given by the sigmoid σ(u>n v`)) and a point-mass at 0.
",2.1. An Exposure-based Latent Factor Model for the Binary Label Matrix,[0],[0]
yn` ∼ ξn`Bern ( yn`|σ(u>n v`) ),2.1. An Exposure-based Latent Factor Model for the Binary Label Matrix,[0],[0]
"+ (1− ξn`)I[yn` = 0] (5)
Note that the latent variable ξn` decides which of the two distributions from this mixture generates yn`.",2.1. An Exposure-based Latent Factor Model for the Binary Label Matrix,[0],[0]
Figure 1 shows our model in the plate notation.,2.1. An Exposure-based Latent Factor Model for the Binary Label Matrix,[0],[0]
"Also note that if yn` = 1 then ξn` = 1 with probability 1 and therefore ξn` only needs to be inferred for entries for which yn` = 0.
",2.1. An Exposure-based Latent Factor Model for the Binary Label Matrix,[0],[0]
The generative model specified in Eq (1)-(4) has two additional parameters: W =,2.1. An Exposure-based Latent Factor Model for the Binary Label Matrix,[0],[0]
"[w1, . . .",2.1. An Exposure-based Latent Factor Model for the Binary Label Matrix,[0],[0]
",wK ]> ∈ RK×D which denotes the matrix of regression weights that map each input feature vector xn to the corresponding latent factor un ∈ RK , and a probability parameter µn` ∈ (0, 1) which denotes the probability of the label yn` being exposed (but note that yn` can be 0 or 1, depending on the outcome of Bernoulli(yn`|σ(u>n v`))).",2.1. An Exposure-based Latent Factor Model for the Binary Label Matrix,[0],[0]
"We refer to µn` as the exposure probability of label ` for example n.
We assume each regression weight vector wk to have a Gaussian prior, i.e., wk ∼ N (wk|0, λ−1w ID).",2.1. An Exposure-based Latent Factor Model for the Binary Label Matrix,[0],[0]
"Note that the spherical covariance of this prior can also be replaced by a more flexible diagonal covariance, which will give the model ability to perform feature selection.
",2.1. An Exposure-based Latent Factor Model for the Binary Label Matrix,[0],[0]
"For the exposure probability µn`, we consider two types of priors.",2.1. An Exposure-based Latent Factor Model for the Binary Label Matrix,[0],[0]
"In the first case, we simply assume µn` = µ`, ∀n, which means that the probability that a label ` is observed is the same for all the examples (i.e., the label exposure for the label ` is global, not example specific).",2.1. An Exposure-based Latent Factor Model for the Binary Label Matrix,[0],[0]
"In this case, we assume a Beta prior on µ`, i.e., µ` ∼ Beta(α1, α2).",2.1. An Exposure-based Latent Factor Model for the Binary Label Matrix,[0],[0]
"In the second case, we assume access to some contexual information (often available in applications such as recommender systems) that we may have for each example-label pair (n, `), in form of some given covariates φn` ∈ RM .",2.1. An Exposure-based Latent Factor Model for the Binary Label Matrix,[0],[0]
"Given these covariates, we model the label exposure probability as µn` = σ(β>φn`), where β ∈ RM is a vector of regression coefficients.",2.1. An Exposure-based Latent Factor Model for the Binary Label Matrix,[0],[0]
"We assume a Gaussian prior on β, i.e., β ∼ N (β|0, λ−1β IM )",2.1. An Exposure-based Latent Factor Model for the Binary Label Matrix,[0],[0]
"Although the generative model specified in Eq. 1-4 is not readily conjugate because the logistic-Bernoulli likelihood is not conjugate to the Gaussian prior on the latent factors, we can leverage data-augmentation techniques (Polson et al., 2013) to make the model locally conjugate.",3. Inference,[0],[0]
This enables us to develop a simple Gibbs sampling algorithm for doing inference in our model.,3. Inference,[0],[0]
"The conjugacy also allows us to design an online expectation maximization (EM) algorithm (Cappé & Moulines, 2009), which enables us to apply our model on large-scale problems.
",3. Inference,[0],[0]
"We handle the non-conjugate logistic-Bernoulli likelihood using the Pólya-gamma augmentation technique (Polson et al., 2013), which is based on the following identity
(exp(ψ)a
(1 + exp(ψ))b = 2−b exp (κψ) ∫ ∞ 0",3. Inference,[0],[0]
exp ( −ωψ2/2 ),3. Inference,[0],[0]
"p(ω)dω
where κ = a − b/2 and p(ω) = PG(b, 0) denotes the Pólya-gamma distribution (Polson et al., 2013).",3. Inference,[0],[0]
"This identity allows us to write any likelihood of the form
(exp(ψ)a
(1+exp(ψ))b (e.g., Bernoulli, binomial, negative-binomial) as a Gaussian distribution, when conditioned on a PG random variable ω|ψ ∼ PG(b, ψ).",3. Inference,[0],[0]
"Specifically, using PG augmentation, we can write the logistic-Bernoulli likelihood
from Eq. 4 as a Gaussian when conditioned on ωn` ∼ PG(1,u>n v`).",3. Inference,[0],[0]
"In particular, ψn` = u > n v`, conditioned on ωn`, becomes a Gaussian p(ψn`|ωn`) ∝",3. Inference,[0],[0]
"exp ( κn`ψn` − 1
2 ωn`ψ
2 n`
) (6)
where κn` = yn` − 0.5.",3. Inference,[0],[0]
This likelihood with the Gaussian priors on the latent factors un and v` results in Gaussian posteriors onun and v`.,3. Inference,[0],[0]
"When doing EM, this also leads to subproblems that are like least square regression problems.",3. Inference,[0],[0]
"Using the PG augmentation, we can derive the posterior distributions of all the latent variables in our model, and perform Gibbs sampling for doing inference in our model.",3.1. Gibbs Sampling,[0],[0]
"Due to conjugacy, the inference updates are straightforward to derive as are summarized below.
",3.1. Gibbs Sampling,[0],[0]
"Sampling ξn`: Note that if yn` = 1 then ξn` = 1 with probability one, and therefore need not be inferred.",3.1. Gibbs Sampling,[0],[0]
"For yn` = 0, we sample ξn` from the posterior
p(ξn` = 1|.) ∝",3.1. Gibbs Sampling,[0],[0]
µn`σ(−u,3.1. Gibbs Sampling,[0],[0]
>,3.1. Gibbs Sampling,[0],[0]
n v,3.1. Gibbs Sampling,[0],[0]
`) (7) p(ξn` = 0|.),3.1. Gibbs Sampling,[0],[0]
∝,3.1. Gibbs Sampling,[0],[0]
"(1− µn`)× 1 (8)
Sampling µn`: For the case when µn` = µ`, ∀n, with Beta(α1, α2) prior on each µ`, the posterior will be
p(µn`|.)",3.1. Gibbs Sampling,[0],[0]
= Beta(α1 + N∑ n=1,3.1. Gibbs Sampling,[0],[0]
"ξn`, α2 +N",3.1. Gibbs Sampling,[0],[0]
"− N∑ n=1 ξn`) (9) Note that, if we parameterize each µn` as µn` = σ(β>φn`) where φn` is the interaction feature vector for the examplelabel pair, and the regresssion weight β is assumed to have a Gaussian prior, the model is not conjugate.",3.1. Gibbs Sampling,[0],[0]
"However, using the PG augmentation allows us to easily derive a closed-form Gaussian posterior for β.
",3.1. Gibbs Sampling,[0],[0]
Sampling un:,3.1. Gibbs Sampling,[0],[0]
"Given the PG variables Ωn,: = {ωn`}L`=1 and the other latent variables, the posterior of un will be un ∼ N (un|µun ,Σun) where the covariance is given by Σun = ( ∑L `=1 ξn`ωn`v`v >",3.1. Gibbs Sampling,[0],[0]
"` + λuIK)
−1",3.1. Gibbs Sampling,[0],[0]
and the mean is given by µun = Σun( ∑L `=1 ξn`κn`v` + λuWxn).,3.1. Gibbs Sampling,[0],[0]
"Note that if a label ` is inferred as not exposed for example n, i.e., ξn` = 0, it does not contribute to the update of un.
",3.1. Gibbs Sampling,[0],[0]
"Sampling v`: Given Ω:,` = {ωn`}Nn=1 and the other latent variables, the posterior v` will be v` ∼ N (v`|µv` ,Σv`) where covariance Σv` = ( ∑N n=1 ξn`ωn`unu > n",3.1. Gibbs Sampling,[0],[0]
"+λvIK) −1
and the mean µv` = Σv`( ∑N n=1 ξn`κn`un).",3.1. Gibbs Sampling,[0],[0]
"Note that if an example n is inferred as not exposed to label `, i.e., ξn` = 0, it does not contribute to the update of v`.
",3.1. Gibbs Sampling,[0],[0]
Sampling W:,3.1. Gibbs Sampling,[0],[0]
Each row {wk}Kk=1 of the regression weights matrix W =,3.1. Gibbs Sampling,[0],[0]
"[w1, . . .",3.1. Gibbs Sampling,[0],[0]
",wK ]> ∈ RK×D will have a Gaussian posterior given by wk ∼ N",3.1. Gibbs Sampling,[0],[0]
"(wk|µwk ,Σwk) where covariance Σwk = (X >X + λwID) −1, the mean µwk = Σwk(X >U), and U =",3.1. Gibbs Sampling,[0],[0]
"[u1, . . .",3.1. Gibbs Sampling,[0],[0]
",uN ] ∈ RK×N .",3.1. Gibbs Sampling,[0],[0]
"Although the Gibbs sampler (Sec. 3.1) is easy to derive and implement in practice, sampling tends to be slow in practice and convergence may be slow.",3.2. Scalable Inference via EM and Online EM,[0],[0]
"We therefore present an online expectation maximization algorithm (Cappé & Moulines, 2009) for doing efficient inference in our model.",3.2. Scalable Inference via EM and Online EM,[0],[0]
"We first show the batch EM updates for our model parameters and then describe the online EM algorithm which can process the training data in small mini-batches of examples, and results in faster convergence in practice.",3.2. Scalable Inference via EM and Online EM,[0],[0]
"The EM algorithm for our model alternates between computing the expectations of the local latent variables, namely the Pólya-gamma variables {ωn`} and the binary exposure latent variables {ξn`} in the E step, and then using these expectations to estimate the other model parameters un, v`, W, and exposure probabilities {µn`} in the M step.
",3.2.1. THE EM ALGORITHM,[0],[0]
The E Step:,3.2.1. THE EM ALGORITHM,[0],[0]
"The E step involves computing the expectations of the latent variables {ωn`} and {ξn`}, given the current values of the other model parameters un, v`, W, and µn` estimated in the previous M step.",3.2.1. THE EM ALGORITHM,[0],[0]
"The E step update equations are given below:
• Expectations of Pólya-gamma variables {ωn`}, ∀n, ` are known to be available in closed form (Scott & Sun, 2013), and are given by
ηn` = E[ωn`|ψn`] = 1
2ψn` tanh ( ψn` 2 ) (10)
",3.2.1. THE EM ALGORITHM,[0],[0]
where ψn` = u>n v` is computed using the estimates of un and vm from the previous M step.,3.2.1. THE EM ALGORITHM,[0],[0]
"• Expectations of each of the binary exposure variables ξn`, ∀n, `, are given by
pn` = E[ξn`|ψn`] = µn`σ(−ψn`)
µn`σ(−ψn`)",3.2.1. THE EM ALGORITHM,[0],[0]
"+ (1− µn`) (11)
",3.2.1. THE EM ALGORITHM,[0],[0]
The M Step:,3.2.1. THE EM ALGORITHM,[0],[0]
"Given the expectations of the latent variables computed in the E step, the M step maximizes the following expected complete data log-likelihood plus logprior terms, which we denote as Q(U,V,W,µ), where U = {un}Nn=1, V = {v`}L`=1, W, and µ = {µn`}, ∀n, `
Q(U,V,W,µ) =",3.2.1. THE EM ALGORITHM,[0],[0]
"− 1
2 ∑ n,` pn` (κn` − ηn`u>n v`) 2 ηn`
+ ∑ n,` log Bernoulli(pn`|µn`)− λu N∑ n=1 ||un −Wxn||2 − λv L∑
`=1 ||v`||2 − λw||W||2 + ∑ n,` log Beta(µn`|α1, α2) (12)
Note that the first term in the objective function given in Eq. 12 is due to the logistic likelihood transformed into a Gaussian (using PG augmentation).",3.2.1. THE EM ALGORITHM,[0],[0]
"This term is akin to a weighted least squares objective where each label being
associated with a weight pn` = E[ξn`|ψn`].",3.2.1. THE EM ALGORITHM,[0],[0]
"Intuitively, in the first term, the contribution of each label yn` to the loglikelihood gets modulated based on its expected exposure.
",3.2.1. THE EM ALGORITHM,[0],[0]
"Maximizing Q(U,V,W,µ) w.r.t.",3.2.1. THE EM ALGORITHM,[0],[0]
"each of the model parameters U,V,W,µ, fixing the rest, yields closed-form updates for each of these.",3.2.1. THE EM ALGORITHM,[0],[0]
"The updates are as follows: • Estimating each of the latent factors {un}Nn=1 is a
weighted ridge-regression problem with solution
un = Σun ( L∑ `=1 pn`κn`v` + λuWxn ) (13)
where Σun = ( ∑L `=1 pn`ηn`v`v >",3.2.1. THE EM ALGORITHM,[0],[0]
"` + λuIK)
−1.",3.2.1. THE EM ALGORITHM,[0],[0]
"Note that the updates for {un}Nn=1 are all independent of each other and are easily parallelizable.
",3.2.1. THE EM ALGORITHM,[0],[0]
"• Estimating each of the label latent factors {v`}L`=1 is a weighted ridge-regression problem with solution
v` = Σv` ( N∑ n=1 pn`κn`un ) (14)
where Σv` = (∑N n=1 pn`ηn`unu > n",3.2.1. THE EM ALGORITHM,[0],[0]
+ λvIK )−1 .,3.2.1. THE EM ALGORITHM,[0],[0]
"Again, note that the updates for {vn}L`=1 are all independent of each other and are easily parallelizable.
",3.2.1. THE EM ALGORITHM,[0],[0]
"• Estimating the regression weight matrix W is equivalent to solving a vector-valued linear regression problem un ≈Wxn, ∀n, with the following updates
W> = (X>X + λwID) −1(X",3.2.1. THE EM ALGORITHM,[0],[0]
">U) (15)
Note that solving Eq.",3.2.1. THE EM ALGORITHM,[0],[0]
"(15) exactly requires inverting a D × D matrix which will be expensive for large D. However, the EM algorithm does not require solving for W exactly in each M step.",3.2.1. THE EM ALGORITHM,[0],[0]
"We therefore solve for W efficient using gradient based methods, such as conjugate-gradient (CG) method (Bertsekas, 1999), which allows us to also leverage the sparsity in the feature matrix X. Typically, a small number of CG iterations are sufficient in practice.
",3.2.1. THE EM ALGORITHM,[0],[0]
•,3.2.1. THE EM ALGORITHM,[0],[0]
"Given pn` from the E step, the updates for µn` for the case when µn` = µ`, ∀n, is simply the MAP solution
µ` = α1 +
∑N n=1",3.2.1. THE EM ALGORITHM,[0],[0]
"pn` − 1
α1 + α2 +N",3.2.1. THE EM ALGORITHM,[0],[0]
"− 2 (16)
",3.2.1. THE EM ALGORITHM,[0],[0]
"For the other case when each µn` in modeled as µn` = σ(β>φn`) with a Gaussian prior on β, estimating β reduces to solving a regression problem with the training data being {φn`, pn`}, ∀n, `, where φn` is the given feature vector for the input-label pair n, ` and pn` is estimated in the E step.",3.2.1. THE EM ALGORITHM,[0],[0]
"Ignoring the prior term (equivalent to `2 regularizer on β), we can estimate β iteratively using gradient-descent updates
β = β",3.2.1. THE EM ALGORITHM,[0],[0]
"− τ NL ∑ n,` (σ(β>φn`)− pn`)φn` (17)
where τ denotes the learning rate.",3.2.1. THE EM ALGORITHM,[0],[0]
The EM algorithm described in Section 3.2.1 is more efficient than the Gibbs sampler described in Section 3.1.,3.2.2. ONLINE EM,[0],[0]
"It is also highly parallelizable since the updates for {u}Nn=1 and {v`}L`=1 can be easily parallelized, and solve for W efficiently using CG updates.",3.2.2. ONLINE EM,[0],[0]
"However, it is a batch procedure and requires going over the entire training data in every iteration.",3.2.2. ONLINE EM,[0],[0]
"For large-scale multi-label learning problems, which are characterized by large N , D, and L, the batch setting may not be feasible in practice, especially when having access to moderate computational resources and storage.
",3.2.2. ONLINE EM,[0],[0]
We therefore present an efficient online version of the EM algorithm for our model which allows it to scale up to massive-sized data sets even on machines with moderate hardware.,3.2.2. ONLINE EM,[0],[0]
"As we show in our experiments, this enables us to apply our model to be run efficiently on massive data sets (e.g., one of the data sets we experiment with has more than 600k examples with about 50k features per example) even on a standard laptop with very moderate hardware.
",3.2.2. ONLINE EM,[0],[0]
The online EM algorithm works by maintaining sufficient statistics of all the model parameters and updates these sufficient statistics with every mini-batch of data.,3.2.2. ONLINE EM,[0],[0]
"For each mini-batch of training examples, the E step computes the relevant expectations associated with these observations and then uses the expectations to update the sufficient statistics of the parameters to be estimated in the M step.
",3.2.2. ONLINE EM,[0],[0]
"For example, noting that the sufficient statistics for updating the label latent factors v` = A−1b are given by A =∑N n=1 pn`ηn`unu > n",3.2.2. ONLINE EM,[0],[0]
"+λvIK and b = ∑N n=1 pn`κn`un, we can update A and b using a small mini-batch containingNb examples as {(xn,yn)}",3.2.2. ONLINE EM,[0],[0]
"Nb n=1 as follows
A(t+1)",3.2.2. ONLINE EM,[0],[0]
=,3.2.2. ONLINE EM,[0],[0]
(1− γt)A(t),3.2.2. ONLINE EM,[0],[0]
+ γtA(new) (18) b(t+1) =,3.2.2. ONLINE EM,[0],[0]
"(1− γt)b(t) + γtb(new) (19)
where A(new) = ( ∑Nb n=1 pn`ηn`unu > n",3.2.2. ONLINE EM,[0],[0]
"+ λvIK), and
b(new) = ∑Nb n=1 pn`κn`un are computing using only the current mini-batch.",3.2.2. ONLINE EM,[0],[0]
The sufficient statistics of the other model parameters can also be updated in the same manner.,3.2.2. ONLINE EM,[0],[0]
"Here γt is a decaying learning rate (or a forgetting factor), which also acts as a trade-off between the contribution from the old sufficient statistics computed thus far and the sufficient statistics contribution from the new minibatch of data.",3.2.2. ONLINE EM,[0],[0]
"We set γt = (a0 + t)− with a0 = 1 and to be close to 0.5 (Cappé & Moulines, 2009).",3.2.2. ONLINE EM,[0],[0]
"Given a new test input x∗, we first predict its latent factor u∗ ∈ RK as Wx∗ and then predict each entry of its label vector y∗ as E[y∗`|u∗,v`] = σ(u>∗ v`).",3.2.3. PREDICTION,[0],[0]
"If we are only interested in the top few labels, fast search methods such as maximum inner product search (Fraccaro et al., 2016) can be used to reduce the computational cost at test time.",3.2.3. PREDICTION,[0],[0]
"A prominent line of work on multi-label learning has been based on models that learn a low-dimensional embedding of the label vectors (Chen & Lin, 2012; Yu et al., 2014; Rai et al., 2015; Bhatia et al., 2015).",4. Related Work,[0],[0]
"Note that this amounts to assuming that the label matrix is low-rank.
",4. Related Work,[0],[0]
"Since many real-world data sets have a large number of rare labels, sometimes the low-rank assumption may not be appropriate.",4. Related Work,[0],[0]
"To address this issue, (Bhatia et al., 2015) proposed a method which assumes the label matrix to be locally low-rank.",4. Related Work,[0],[0]
One way to impose this assumption is to learn embeddings that only try to preserve distances in a small neighborhood of each example.,4. Related Work,[0],[0]
"Another approach to handle the rare labels is to assume that the label matrix is a sum of a low-rank and a sparse matrix (Xu et al., 2016).
",4. Related Work,[0],[0]
"Note that our latent factor model is equivalent to imposing a low-rank assumption on the label matrix, and is therefore similar in spirit to the label-embedding approaches.",4. Related Work,[0],[0]
"However, unlike the existing label-embedding based approaches, our generative framework has a principled mechanism to handle/infer the unobserved labels.",4. Related Work,[0],[0]
"Moreover, none of the existing label-embedding methods can work in online fashion, and scaling up these methods to largescale problems requires large computational resources.",4. Related Work,[0],[0]
"In addition, our model readily allows incorporating the label features (if available) by a simple modification to the prior on the label latent factors.
",4. Related Work,[0],[0]
"Apart from the label-embedding based multi-label learning methods, tree-based methods for multi-label learning (Agrawal et al., 2013; Prabhu & Varma, 2014; Jain et al., 2016) are also popular due to being fast at test time, especially when the number of labels is large.",4. Related Work,[0],[0]
"However, these models usually have high training costs and cannot be trained easily in an online fashion, unlike our model.",4. Related Work,[0],[0]
"On the other hand, for faster predictions at test time, our framework model can easily be adapting by replacing the Gaussian prior on the label latent factors v` by a von MisesFisher prior (Fraccaro et al., 2016), which naturally facilitates using maximum inner-product search techniques, without the requirement of any post-processing.
",4. Related Work,[0],[0]
"Among other models to address the missing labels problem in multi-label learning, recently, (Kanehira & Harada, 2016) proposed a ranking based framework for learning from positive and unlabeled data in the context of multilabel learning.",4. Related Work,[0],[0]
"Although this is similar in spirit to our model in terms of not treating the unobserved labels as zeros, the approach in (Kanehira & Harada, 2016) is fundamentally different than ours.",4. Related Work,[0],[0]
"Moreover, their setting is not amenable to online learning, nor does it leverage the lowrank structure of label matrices with a huge number of labels.",4. Related Work,[0],[0]
"Other approaches that try to handle missing labels in-
clude (Bucak et al., 2011) which uses group LASSO adaptation of a multi-label ranking objective, and (Kong et al., 2014), which learns a model using a positive and unlabeled (PU) stochastic gradient descent procedure.",4. Related Work,[0],[0]
"However, it works in batch setting, uses stacking to leverage label correlations, and does not scale to large number of labels.
",4. Related Work,[0],[0]
"One-class matrix factorization (OCMF) is also an approach (Yu et al., 2017) to solve the missing labels problem by assigning different (but fixed) weights to the ones and zeros.",4. Related Work,[0],[0]
"In contrast to this method, our generative framework can learn the weight for each label by modeling these weights as latent variables.",4. Related Work,[0],[0]
"In another recent work, (Liang et al., 2016) proposed an exposure model for recommender system problems posed as matrix factorization of implicit feedback data.",4. Related Work,[0],[0]
"Their approach of modeling the exposure similar in spirit to our framework.
",4. Related Work,[0],[0]
"Some of the early works on generative models for multilabel learning problems include models specifically designed for image annotation problems (Barnard et al., 2003; Feng et al., 2004).",4. Related Work,[0],[0]
"Other recent attempts on doing multilabel learning in more general problem settings include models such as Bayesian compressive sensing (Kapoor et al., 2012) and multi-label learning using Bayesian nonnegative matrix factorization (Rai et al., 2015).",4. Related Work,[0],[0]
"However, these models do not have a mechanism to distinguish between unobserved and negative labels, have complicated inference, and do not scale to large-scale problems.
",4. Related Work,[0],[0]
Our generative framework is also amenable for various interesting extensions.,4. Related Work,[0],[0]
"For example, it can be be extended to a mixture of latent factor models, which can handle the situation when the label matrix is not low-rank but a mixture of several low-rank matrices.",4. Related Work,[0],[0]
"Note that such an extension would be a fully generative counter-part of the model in (Bhatia et al., 2015) which learns a locally low-rank model but has to rely on an ad-hoc clustering step beforehand, which is known to be unstable in practice (Bhatia et al., 2015).",4. Related Work,[0],[0]
"Another nice aspect of our framework is that is naturally allows active learning (Kapoor et al., 2012; Vasisht et al., 2014) where we can selectively ask for most informative labels for an unannotated example.",4. Related Work,[0],[0]
"Moreover, our framework is flexible and inference in our model can be performed in a fully Bayesian manner (e.g., MCMC or variational inference) as well as fast point estimation methods such as (online)",4. Related Work,[0],[0]
"EM, that we used in this work.
",4. Related Work,[0],[0]
"To summarize, our generative framework offers a flexible way to model the label generation mechanism for real-world multi-label data sets, which most of the existing models currently lack.",4. Related Work,[0],[0]
We can model label missingness/observability rigorously under our framework and infer the model parameters easily using a simple inference procedure.,4. Related Work,[0],[0]
"Moreover, the simplicity of the inference procedure makes it easy to design scalable inference algorithms,
such as online EM for our model, which enables updating the model whenever fresh training data is available.",4. Related Work,[0],[0]
"This is in contrast to some of the other state-of-the-art multi-label learning methods, which although scalable (Bhatia et al., 2015; Prabhu & Varma, 2014; Jain et al., 2016), are not suitable to be applied in such online settings.",4. Related Work,[0],[0]
We evaluate our framework on a number of benchmark data sets and compare it with several state-of-the-art methods.,5. Experiments,[0],[0]
Our baselines include both label-embedding methods as well as tree-based methods.,5. Experiments,[0],[0]
"The statistics of data sets we use in our experiments are summarized in Table 1.
",5. Experiments,[0],[0]
"We report both quantitative results (in terms of label prediction accuracies) as well as some qualitative results, namely looking at the relationship of empirical label frequencies and label exposure.",5. Experiments,[0],[0]
"Note that the label frequency for a given label denotes how many examples had this label as 1, while label exposure µ` ∈ (0, 1) in general refers to how popular the label ` is.
",5. Experiments,[0],[0]
"In our experiments, we compare with the following stateof-the-art baselines.
",5. Experiments,[0],[0]
• LEML:,5. Experiments,[0],[0]
"This is a low-rank embedding based multilabel learning model (Yu et al., 2014).",5. Experiments,[0],[0]
LEML assumes the label matrix Y to be modeled as Y ≈ UV where U = XW,5. Experiments,[0],[0]
". LEML considers various types of loss functions such as squared loss, logistic loss, hinge loss, etc.",5. Experiments,[0],[0]
"Interestingly, note that LEML with logistic loss can be seen as a special non-probabilistic case of our model when also considering λu → ∞, and the label exposure model turned off.
",5. Experiments,[0],[0]
"• BCS: Bayesian Compressive Sensing (BCS) is a generative model (Kapoor et al., 2012) for the label vector.",5. Experiments,[0],[0]
"It assumes a compressive sensing model for the label vectors and is essentially a low-rank model.
",5. Experiments,[0],[0]
• FastXML,5. Experiments,[0],[0]
": This is a fast tree-based multi-label learning model which uses an ensemble of trees (Prabhu & Varma, 2014).
",5. Experiments,[0],[0]
• PfasterXML,5. Experiments,[0],[0]
:,5. Experiments,[0],[0]
"This is an extension of FastXML and uses propensity-weighted scores to improve performance on rare labels (Jain et al., 2016).
",5. Experiments,[0],[0]
"• PD-Sparse: This model takes a different approach as compared to label-embedding methods and uses a margin-maximizing loss for the multi-label learning problem (Yen et al., 2016).
",5. Experiments,[0],[0]
"For the baselines, the reported results are either obtained using publicly available implementations (with the recommended hyperparameter settings), or the publicly known best results.",5. Experiments,[0],[0]
"We refer to our model as GenEML (for Generative Exposure-based model for Multi-label Learning)
",5. Experiments,[0],[0]
"Hyperparameter Settings: For our model, we set the hyperparameters λu and λv to 0.001, which works well on all the data sets we experimented with.",5. Experiments,[0],[0]
We select the other two hyperparameters λw and K (number of latent factors) using cross-validation.,5. Experiments,[0],[0]
"On small-/medium-scale data, both EM and online EM perform comparably and we only report the results using online EM.",5. Experiments,[0],[0]
"On large data sets, we only use online EM.",5. Experiments,[0],[0]
"On the small and medium-scale data, we however also show a separate experiment comparing EM and online EM for our model in terms of convergence speed versus accuracy.",5. Experiments,[0],[0]
"For the conjugate gradient (CG) method used by the M step of our inference algorithm, we run 5 iterations, which was found to be sufficient.",5. Experiments,[0],[0]
"For online EM, for each data set, we use mini-batch sizes of 1024 and 4096 and report the one which gives better results.",5. Experiments,[0],[0]
"In our first experiment, we assess the benefit of using the exposure model.",5.1.1. BENEFIT OF EXPOSURE MODEL,[0],[0]
"For this, we apply our model with and without exposure on a synthetic data set.",5.1.1. BENEFIT OF EXPOSURE MODEL,[0],[0]
"For this experiment, we generate a synthetic data set with N=500, D=100, and L=20 and use varying degrees of exposure probabilities µ` ∈ {0.01, 0.05, 0.1, 0.3, 0.5, 0.9} for the different labels ` = 1, . . .",5.1.1. BENEFIT OF EXPOSURE MODEL,[0],[0]
", 20.",5.1.1. BENEFIT OF EXPOSURE MODEL,[0],[0]
"We also create a test set with 500 test examples.
",5.1.1. BENEFIT OF EXPOSURE MODEL,[0],[0]
The results are shown in Table 2.,5.1.1. BENEFIT OF EXPOSURE MODEL,[0],[0]
"As the results show, our model with exposure turned on outperforms the model when the exposure is turned off.",5.1.1. BENEFIT OF EXPOSURE MODEL,[0],[0]
"This clearly demonstrate the benefit of the exposure model when a significant fraction of labels are missing (i.e., not exposed).",5.1.1. BENEFIT OF EXPOSURE MODEL,[0],[0]
Our model also outperforms LEML which does not have a mechanism to model label exposure.,5.1.1. BENEFIT OF EXPOSURE MODEL,[0],[0]
"In our next set of experiments, in Table 3 we compare our model (with exposure on) with the other baselines, in terms of Precision@1, Precision@3, and Precision@5 scores.",5.1.2. PREDICTION ACCURACIES,[0],[0]
"As Table 3 shows, our model outperforms the other baselines in most of the cases, except for the RCV and Wikipedia data, on which our model is outperformed by LEML and/or PfasterXML.",5.1.2. PREDICTION ACCURACIES,[0],[0]
"Note, however, that these stateof-the-art baselines use batch inference methods whereas we only ran our model in the online setting on a moderate 4 core processor with 8GB RAM.",5.1.2. PREDICTION ACCURACIES,[0],[0]
"Moreover, our results may further improve with a more careful hyperparameter tuning (including selection of minibatch size).",5.1.2. PREDICTION ACCURACIES,[0],[0]
"The point of the large-scale data experiment was to mainly show that the our model can be feasibly run on such large-scale data sets, on standard machines with moderate computational resources.",5.1.2. PREDICTION ACCURACIES,[0],[0]
Most of the other existing models for multi-label learning are infeasible to run under such restrictive settings.,5.1.2. PREDICTION ACCURACIES,[0],[0]
The online version of our EM algorithm is scalable and faster than its batch counterpart.,5.1.3. BATCH EM VS ONLINE EM,[0],[0]
"Fig 2 shows that online EM converges faster and to a precision score which is very similar to the batch EM on Bibtex and Mediamill datasets.
",5.1.3. BATCH EM VS ONLINE EM,[0],[0]
"Furthermore, online inference is also more effecient, storage-wise, due the need of maintaining just the sufficient statistics as in Eq 19 for the updates of each latent factor un and v`.",5.1.3. BATCH EM VS ONLINE EM,[0],[0]
"For very large datasets, the size of the the sufficient statistics (a D ×D covariance matrix) for updating the regression weight matrix W might not be feasible to store and update.",5.1.3. BATCH EM VS ONLINE EM,[0],[0]
"Therefore, we use cheap, first-order gradient based updates for finding an approximate solution to the update equation of W in each iteration of the EM algorithm (note that we need not solve for W exactly; the EM algorithm just requires a few steps of updates for W in the M step).",5.1.3. BATCH EM VS ONLINE EM,[0],[0]
"This further reduces the memory requirement of our model, while also speeding up inference due to faster computation of gradients as compared to CG updates.",5.1.3. BATCH EM VS ONLINE EM,[0],[0]
"Finally, we do some qualitative analyses of our model’s behavior.",5.2. Qualitative Results,[0],[0]
"We investigate whether the global frequency of a label necessarily correlates to its exposure probability.
",5.2. Qualitative Results,[0],[0]
"While it may be the case for some data sets where high label frequency implies a high inferred label exposure probability (e.g., see Fig. 3 for Bibtex and Mediamill data), it need not be the case with other data sets.",5.2. Qualitative Results,[0],[0]
"For example, for Movielens data, each user-movie (example-label) pair has an some context information (user and movie features) available for it.",5.2. Qualitative Results,[0],[0]
"As we show in Fig 4, the inferred exposure probability (which depends on the context features) of the same movie (label) indeed turns out to be different for different users (examples).
",5.2. Qualitative Results,[0],[0]
"Fig. 4 shows the plot of inferred exposure probabilities µnl for two users (one female, one male) plotted against the label frequencies (movie popularities).
",5.2. Qualitative Results,[0],[0]
"As Fig. 4 shows, our model infers that, a popular movie (shown in red dot in Fig 4) has a high exposure probability for the left user (Female, 25, Healthcare/Doctor) while it has a low exposure probability for the right user (Male, 35, artist).",5.2. Qualitative Results,[0],[0]
"This example illustrates that a high label frequency does not necessarily imply a high exposure probability, which can be context (user in this case) dependent.",5.2. Qualitative Results,[0],[0]
We presented a flexible and scalable generative framework for multi-label learning.,6. Conclusion,[0],[0]
"Our framework is based on a latent
factor model for the label matrix and does not assume that the zeros in the label matrix are necessarily negative labels.",6. Conclusion,[0],[0]
"We use a set of label exposure latent variables to model this, and infer these exposure probabilities from data.",6. Conclusion,[0],[0]
"Incorporating these latent variables leads to improve multi-label classification accuracies, and also enables doing interesting qualitative analyses.",6. Conclusion,[0],[0]
Our model admits a simple inference procedure which can be implemeted using Gibbs sampling or EM.,6. Conclusion,[0],[0]
"We further develop a highly scalable online EM algorithm for performing inference in our model, which allows our model to be applied on large-scale data sets, even on standard machines with moderate hardware.",6. Conclusion,[0],[0]
The generative framework makes it easy to extend our model in many interesting ways.,6. Conclusion,[0],[0]
"For example, it can be extended to a mixture of latent factor models, which will allow handling the cases where a single low-rank model does not adequately capture the structure of the label matrix.
",6. Conclusion,[0],[0]
"Acknowledgements: PR acknowledges support from Extreme Classification research grant from Microsoft Research India, DST-SERB Early Career Research Award, Dr. Deep Singh and Daljeet Kaur Fellowship, and Research-I Foundation, IIT Kanpur.",6. Conclusion,[0],[0]
"We present a scalable, generative framework for multi-label learning with missing labels.",abstractText,[0],[0]
"Our framework consists of a latent factor model for the binary label matrix, which is coupled with an exposure model to account for label missingness (i.e., whether a zero in the label matrix is indeed a zero or denotes a missing observation).",abstractText,[0],[0]
The underlying latent factor model also assumes that the low-dimensional embeddings of each label vector are directly conditioned on the respective feature vector of that example.,abstractText,[0],[0]
"Our generative framework admits a simple inference procedure, such that the parameter estimation reduces to a sequence of simple weighted leastsquare regression problems, each of which can be solved easily, efficiently, and in parallel.",abstractText,[0],[0]
"Moreover, inference can also be performed in an online fashion using mini-batches of training examples, which makes our framework scalable for large data sets, even when using moderate computational resources.",abstractText,[0],[0]
"We report both quantitative and qualitative results for our framework on several benchmark data sets, comparing it with a number of state-of-the-art methods.",abstractText,[0],[0]
Scalable Generative Models for Multi-label Learning with Missing Labels,title,[0],[0]
"Gaussian processes (GPs) are non-parametric models that can be used to address multi-class classification problems (Rasmussen & Williams, 2006).",1. Introduction,[0],[0]
These models become more expressive as the number of data instances N grows.,1. Introduction,[0],[0]
"They are also very useful to introduce prior knowledge in the learning problem, as many properties of the model are specified by a covariance function.",1. Introduction,[0],[0]
"Moreover, GPs provide an estimate of the uncertainty in the predictions made which may be critical in some applications.",1. Introduction,[0],[0]
"Neverthe-
*Equal contribution 1Universidad Autónoma de Madrid, Madrid, Spain.",1. Introduction,[0],[0]
"Correspondence to: Carlos Villacampa-Calvo <carlos.villacampa@uam.es>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
less, in spite of these advantages, GPs scale poorly to large datasets because their training cost is O(N3), where N is the number of instances.",1. Introduction,[0],[0]
"An additional challenge is that exact inference in these models is generally intractable and one has to resort to approximate methods in practice.
",1. Introduction,[0],[0]
"Traditionally, GP classification has received more attention in the binary case than in the multi-class setting (Kuss & Rasmussen, 2005; Nickisch & Rasmussen, 2008).",1. Introduction,[0],[0]
The reason is that approximate inference is more challenging in the multi-class case where there is one latent function per class.,1. Introduction,[0],[0]
"To this one has to add more complicated likelihood factors, which often have the form of softmax functions or intractable Gaussian integrals.",1. Introduction,[0],[0]
"In spite of these difficulties, there have been several works addressing multi-class GP classification (Williams & Barber, 1998; Kim & Ghahramani, 2006; Girolami & Rogers, 2006; Chai, 2012; Riihimäki et al., 2013).",1. Introduction,[0],[0]
"Nevertheless, most of the proposed methods do not scale well with the size of the training set.
",1. Introduction,[0],[0]
In the literature there have been some efforts to scale up GPs.,1. Introduction,[0],[0]
These techniques often introduce a set of M N inducing points whose location is learnt alongside with the other model hyper-parameters.,1. Introduction,[0],[0]
"The use of inducing points in the model can be understood as an approximate GP prior with a low-rank covariance structure (Quiñonero-Candela & Rasmussen, 2005).",1. Introduction,[0],[0]
"When inducing points are considered, the training cost can be reduced to O(NM2).",1. Introduction,[0],[0]
"This allows to address datasets with several thousands of instances, but not millions.",1. Introduction,[0],[0]
"The reason is the difficulty of estimating the model hyper-parameters, which is often done by maximizing an estimate of the log-marginal-likelihood.",1. Introduction,[0],[0]
"Because such an estimate does not involve a sum across the data instances, one cannot rely on efficient methods for optimization based on stochastic gradients and mini-batches.
",1. Introduction,[0],[0]
"A notable exception is the work of (Hensman et al., 2015a) which uses variational inference to approximate the calculations.",1. Introduction,[0],[0]
Such a method allows for stochastic optimization and can address datasets with millions of instances.,1. Introduction,[0],[0]
"In this work we propose an alternative based on expectation propagation (EP) (Minka, 2001) and recent advances on binary GP classification (Hernández-Lobato & HernándezLobato, 2016).",1. Introduction,[0],[0]
The proposed approach also allows for efficient training using mini-batches.,1. Introduction,[0],[0]
"This leads to a training
cost that is O(CM3), where C is the number of classes.",1. Introduction,[0],[0]
An experimental comparison with the variational approach and related methods from the literature shows that the proposed approach has benefits both in terms of the training speed and the accuracy of the predictive distribution.,1. Introduction,[0],[0]
Here we describe multi-class Gaussian process classification and the proposed method.,2. Scalable Multi-class Classification,[0],[0]
Such a method uses the expectation propagation algorithm whose original description is modified to be more efficient both in terms of memory and computational costs.,2. Scalable Multi-class Classification,[0],[0]
"For this, we consider stochastic gradients to update the hyper-parameters and an approximate likelihood that avoids one-dimensional quadratures.",2. Scalable Multi-class Classification,[1.0],"['For this, we consider stochastic gradients to update the hyper-parameters and an approximate likelihood that avoids one-dimensional quadratures.']"
"We consider a dataset of N instances in the form of a matrix of attributes X = (x1, . . .",2.1. Multi-class Gaussian Process Classification,[0.9923289044519077],"['We consider a dataset of N instances in the form of a matrix of attributes X = (x1, .']"
",xN )T with labels y = (y1, . . .",2.1. Multi-class Gaussian Process Classification,[0],[0]
", yN )
T, where yi ∈ {1, . . .",2.1. Multi-class Gaussian Process Classification,[0],[0]
", C} and C > 2 is the total number of different classes.",2.1. Multi-class Gaussian Process Classification,[0],[0]
"The task of interest is to predict the class label of a new data instance x?.
",2.1. Multi-class Gaussian Process Classification,[0],[0]
"A typical approach in multi-class Gaussian process (GP) classification is to assume the following labeling rule for yi given xi: yi = arg maxk f
k(xi), for k = 1, . . .",2.1. Multi-class Gaussian Process Classification,[0],[0]
", C, where each fk(·) is a non-linear latent function (Kim & Ghahramani, 2006).",2.1. Multi-class Gaussian Process Classification,[0],[0]
"Define fk = (fk(x1), . . .",2.1. Multi-class Gaussian Process Classification,[0],[0]
", f k(xN ))",2.1. Multi-class Gaussian Process Classification,[0],[0]
"T ∈ RN and fi = (f1(xi), . . .",2.1. Multi-class Gaussian Process Classification,[0],[0]
", f C(xi))
",2.1. Multi-class Gaussian Process Classification,[0],[0]
T ∈ RC .,2.1. Multi-class Gaussian Process Classification,[0],[0]
"The likelihood of f = (f1, . . .",2.1. Multi-class Gaussian Process Classification,[0],[0]
", fC)T ∈ RN×C , p(y|f) = ∏N i=1 p(yi|fi), is then a product of N factors of the form:
p(yi|fi) =",2.1. Multi-class Gaussian Process Classification,[0],[0]
"∏ k 6=yi Θ ( fyi(xi)− fk(xi) ) , (1)
where Θ(·) is the Heaviside step function.",2.1. Multi-class Gaussian Process Classification,[0],[0]
This likelihood takes value one if f can explain the observed data and zero otherwise.,2.1. Multi-class Gaussian Process Classification,[1.0],['This likelihood takes value one if f can explain the observed data and zero otherwise.']
Potential classification errors can be easily introduced in (1) by considering that each fk has been contaminated with Gaussian noise with variance σ2k.,2.1. Multi-class Gaussian Process Classification,[0],[0]
"That is, fk(xi) = f̂ k(xi) +",2.1. Multi-class Gaussian Process Classification,[0],[0]
"k i , where k i ∼ N (0, σ2k).
",2.1. Multi-class Gaussian Process Classification,[0],[0]
"In multi-class GP classification a GP prior is assumed for each function fk(·) (Rasmussen & Williams, 2006).",2.1. Multi-class Gaussian Process Classification,[0],[0]
"Namely, fk ∼ GP(0, c(·, ·; ξ)), where c(·, ·; ξk) is some covariance function with hyper-parameters ξk.",2.1. Multi-class Gaussian Process Classification,[0],[0]
Often these priors are assumed to be independent.,2.1. Multi-class Gaussian Process Classification,[0],[0]
"That is, p(f) =∏C
k=1 p(f k), where each p(fk) is a multivariate Gaussian distribution.",2.1. Multi-class Gaussian Process Classification,[0],[0]
"The task of interest is to make inference about f and for that Bayes’ rule is used: p(f |y) = p(y|f)p(f)/p(y), where p(y) is a normalization constant (the marginal likelihood) which can be maximized to find good hyper-parameters ξk, for k = 1, . . .",2.1. Multi-class Gaussian Process Classification,[0],[0]
", C. However, because the likelihood in (1) is non-Gaussian, evaluating p(y)
and p(f |y) is intractable.",2.1. Multi-class Gaussian Process Classification,[0],[0]
"Thus, these computations must be approximated.",2.1. Multi-class Gaussian Process Classification,[1.0],"['Thus, these computations must be approximated.']"
"Often, one computes a Gaussian approximation to p(f |y) (Kim & Ghahramani, 2006).",2.1. Multi-class Gaussian Process Classification,[0],[0]
"This results in a non-parametric classifier with training cost O(N3), where N is the number of data instances.
",2.1. Multi-class Gaussian Process Classification,[0],[0]
To reduce the computational cost of the method described a typical approach is to consider a sparse representation for each GP.,2.1. Multi-class Gaussian Process Classification,[0],[0]
"With this goal, one can introduce C datasets of M N inducting points",2.1. Multi-class Gaussian Process Classification,[0],[0]
"Xk = (x1, . . .",2.1. Multi-class Gaussian Process Classification,[0],[0]
",xkM )T, with associated values f k",2.1. Multi-class Gaussian Process Classification,[0],[0]
"= (fk(xk1), . . .",2.1. Multi-class Gaussian Process Classification,[0],[0]
", f k(xkM ))",2.1. Multi-class Gaussian Process Classification,[0],[0]
"T for k = 1, . . .",2.1. Multi-class Gaussian Process Classification,[0],[0]
", C (Snelson & Ghahramani, 2006; NaishGuzman & Holden, 2008).",2.1. Multi-class Gaussian Process Classification,[0],[0]
"Given each X k the prior for
fk is approximated as p(fk) = ∫ p(fk|fk)p(fk|Xk)dfk ≈∫
[ ∏N
i=1",2.1. Multi-class Gaussian Process Classification,[0],[0]
p(f k,2.1. Multi-class Gaussian Process Classification,[0],[0]
"i (xi)|f
k )]",2.1. Multi-class Gaussian Process Classification,[0],[0]
"p(f k|Xk)dfk = pFITC(fk|X k ), in
which the conditional Gaussian distribution p(fk|fk) has been approximated by the factorizing distribution∏N
i=1",2.1. Multi-class Gaussian Process Classification,[0],[0]
p(f k,2.1. Multi-class Gaussian Process Classification,[0],[0]
"i (xi)|f
k ).",2.1. Multi-class Gaussian Process Classification,[0],[0]
"This approximation is known as the
full independent training conditional (FITC) (QuiñoneroCandela & Rasmussen, 2005), and it leads to a Gaussian prior pFITC(fk|X k ) with a low-rank covariance matrix.",2.1. Multi-class Gaussian Process Classification,[0],[0]
This allows for approximate inference with costO(NM2).,2.1. Multi-class Gaussian Process Classification,[0],[0]
The inducing points {Xk}Ck=1 can be regarded as hyperparameters and can be learnt by maximizing the estimate of the marginal likelihood p(y).,2.1. Multi-class Gaussian Process Classification,[0],[0]
The formulation of the previous section is limited because the estimate of the log-marginal-likelihood log p(y) cannot be expressed as a sum across the data instances.,2.2. Method Specification and Expectation Propagation,[0],[0]
"This makes infeasible the use of efficient methods based on stochastic optimization for finding the model hyper-parameters.
",2.2. Method Specification and Expectation Propagation,[0],[0]
"A recent work focusing on the binary case has shown that it is possible to obtain an estimate of log p(y) that involves a sum across the data instances if the values f k associated to the inducing points are not marginalized (HernándezLobato & Hernández-Lobato, 2016).",2.2. Method Specification and Expectation Propagation,[0],[0]
We follow that work and consider the posterior approximation p(f |y) ≈∫,2.2. Method Specification and Expectation Propagation,[0],[0]
"p(f |f)q(f)df , where f = (f1, . . .",2.2. Method Specification and Expectation Propagation,[0],[0]
", fC)T, p(f |f) =∏C k=1 p(f k|fk), we have defined p(f) = ∏C k=1 p(f k|Xk), and q is a Gaussian approximation to p(f |y).",2.2. Method Specification and Expectation Propagation,[0],[0]
This distribution q is obtained in three steps.,2.2. Method Specification and Expectation Propagation,[0],[0]
"First, we use on the exact posterior the FITC approximation:
p(f |y) = ∫ p(y|f)p(f |f)dfp(f)
p(y)",2.2. Method Specification and Expectation Propagation,[0],[0]
"≈ ∫ p(y|f)pFITC(f |f)dfp(f)
p(y) =",2.2. Method Specification and Expectation Propagation,[0],[0]
"[ ∏N i=1 φi(f)]p(f)
p(y) , (2)
where we have defined pFITC(f |f) = ∏N
i=1",2.2. Method Specification and Expectation Propagation,[0],[0]
"∏C k=1 p(f k(xi)
|fk)",2.2. Method Specification and Expectation Propagation,[0],[0]
"≈ p(f |f) = ∏C
k=1 p(f k|fk) and
φi(f) = ∫",2.2. Method Specification and Expectation Propagation,[0],[0]
[ ∏ k 6=yi Θ ( fyi(xi)− fk(xi) ),2.2. Method Specification and Expectation Propagation,[0],[0]
"]
× [ ∏C
k=1 p(f k(xi)|f k )]",2.2. Method Specification and Expectation Propagation,[0],[0]
"dfi , (3)
with p(fk(xi)|f k ) = N (fk(xi)|mki , vki ), where
mki = (k k
xiX k) T(Kk X k X k) −1f k , (4)
ski = κ k",2.2. Method Specification and Expectation Propagation,[0],[0]
xixi,2.2. Method Specification and Expectation Propagation,[0],[0]
"− (k k
xiX k) T(Kk X k X k) −1kk xiX k .",2.2. Method Specification and Expectation Propagation,[0],[0]
"(5)
",2.2. Method Specification and Expectation Propagation,[0],[0]
"In the previous expressions N (·|µ, σ2) is the p.d.f. of a Gaussian with mean µ and variance σ2.",2.2. Method Specification and Expectation Propagation,[0],[0]
"Furthermore, kk
xiX k
is a vector with the covariances between fk(xi) and f k; Kk XkXk
is a M ×M matrix with the cross covariances between fk; and, finally, κkxixi is the prior variance of f k(xi).
",2.2. Method Specification and Expectation Propagation,[0],[0]
A practical difficulty is that the integral in (3) is intractable.,2.2. Method Specification and Expectation Propagation,[0],[0]
"Although it can be evaluated using one-dimensional quadrature techniques (Hernández-Lobato et al., 2011), in this paper we follow a different approach.",2.2. Method Specification and Expectation Propagation,[0],[0]
"For that, we note that (3) is simply the probability that fyi(xi) > fk(xi) for k 6= yi, given f .",2.2. Method Specification and Expectation Propagation,[0],[0]
Let fyii = fyi(xi) and fki = fk(xi).,2.2. Method Specification and Expectation Propagation,[0],[0]
"The second step consists in approximating (3) as follows:
p( ⋂ k 6=yi f yi > fk) =p(fyi > f1|S1)×",2.2. Method Specification and Expectation Propagation,[0],[0]
"p(fyi > f2|S2)×
· · · × p(fyi > fyi−1|Syi−1)× p(f yi > fyi+1|Syi+1)× · · ·",2.2. Method Specification and Expectation Propagation,[0],[0]
"≈ ∏ k 6=yip(f yi > fk) = ∏ k 6=yi Φ(α k i ) , (6)
where Sj = ⋂
k/∈{1,...,j}∪{yi} f yi > fk, Φ(·) is the c.d.f. of a standard Gaussian and αki = (myii − m k i )/",2.2. Method Specification and Expectation Propagation,[0],[0]
"√ syii + s k i , with myii , m k i , s yi i and s k i defined in (5).",2.2. Method Specification and Expectation Propagation,[0],[0]
We have omitted in (6) the dependence on f to improve the readability.,2.2. Method Specification and Expectation Propagation,[0],[0]
The quality of this approximation is supported by the good experimental results obtained in Section 4.,2.2. Method Specification and Expectation Propagation,[0],[0]
"When (6) is replaced in (2) we get an approximate posterior distribution in which we can evaluate all the likelihood factors:
p(f |y)",2.2. Method Specification and Expectation Propagation,[0],[0]
≈ [ ∏N i=1,2.2. Method Specification and Expectation Propagation,[0],[0]
∏,2.2. Method Specification and Expectation Propagation,[0],[0]
k 6=yk φ k,2.2. Method Specification and Expectation Propagation,[0],[0]
"i (f)]p(f)
p(y) , (7)
where we have defined φki (f) =",2.2. Method Specification and Expectation Propagation,[0],[0]
"Φ(α k i ).
",2.2. Method Specification and Expectation Propagation,[0],[0]
The r.h.s.,2.2. Method Specification and Expectation Propagation,[0],[0]
of (7) is intractable due to the non-Gaussian form of the likelihood factors.,2.2. Method Specification and Expectation Propagation,[0],[0]
"The third and last step uses expectation propagation (EP) (Minka, 2001) to get a Gaussian approximation q to (7).",2.2. Method Specification and Expectation Propagation,[0],[0]
This approximation is obtained by replacing each φki with an approximate Gaussian factor,2.2. Method Specification and Expectation Propagation,[0],[0]
φ̃ k,2.2. Method Specification and Expectation Propagation,[0],[0]
"i :
φ̃ki (f) = s̃i,k exp { − 12 (",2.2. Method Specification and Expectation Propagation,[0],[0]
"f yi )TṼyii,kf yi + (f yi )Tm̃yii,k } ×
exp { − 12 (f k )",2.2. Method Specification and Expectation Propagation,[0],[0]
"TṼi,kf k + (f k )",2.2. Method Specification and Expectation Propagation,[0],[0]
"Tm̃i,k } , (8)
where Ṽyii,k = C",2.2. Method Specification and Expectation Propagation,[0],[0]
"1,yi",2.2. Method Specification and Expectation Propagation,[0],[0]
"i,k υ yi",2.2. Method Specification and Expectation Propagation,[0],[0]
"i (υ yi i ) T, m̃yii,k = C 2,yi",2.2. Method Specification and Expectation Propagation,[0],[0]
"i,k υ yi",2.2. Method Specification and Expectation Propagation,[0],[0]
"i , Ṽi,k = C1i,kυ k",2.2. Method Specification and Expectation Propagation,[0],[0]
"i (υ k i ) T, m̃i,k = C2i,kυ k",2.2. Method Specification and Expectation Propagation,[0],[0]
"i , and we have defined υ",2.2. Method Specification and Expectation Propagation,[0],[0]
k,2.2. Method Specification and Expectation Propagation,[0],[0]
i = (kk xiX k ) T(Kk X k X k ) −1.,2.2. Method Specification and Expectation Propagation,[0],[0]
"In (8) C1,yii,k , C 2,yi",2.2. Method Specification and Expectation Propagation,[0],[0]
"i,k , C 1 i,k, C 2",2.2. Method Specification and Expectation Propagation,[0],[0]
"i,k and s̃i,k are free parameters adjusted by EP.",2.2. Method Specification and Expectation Propagation,[0],[0]
"Because the precision matrices in (8) are one-rank (see the supplementary material for details), we only have to store in memory O(M) parameters for each φ̃ki .",2.2. Method Specification and Expectation Propagation,[0],[0]
"The posterior approximation q is obtained by replacing in (7) each exact factor φi,k by the corresponding approximate factor φ̃i,k.",2.2. Method Specification and Expectation Propagation,[0],[0]
"That is, q(f) = ∏N i=1",2.2. Method Specification and Expectation Propagation,[0],[0]
"∏ k 6=yi φ̃ k i (f)p(f)/Zq , where Zq is a normalization constant that approximates the marginal likelihood p(y).",2.2. Method Specification and Expectation Propagation,[0],[0]
"Because all the factors involved in the computation of q are Gaussian, and we assume independence among the latent functions of different classes in (8), q is a product of C multivariate Gaussians (on per class) on M dimensions.
",2.2. Method Specification and Expectation Propagation,[0],[0]
"In EP each φ̃ki is updated until convergence as follows: First, φki is removed from q by computing q
\i,k ∝ q/φ̃ki .",2.2. Method Specification and Expectation Propagation,[0],[0]
"Because the Gaussian family is closed under the product and division operations, q\i,k is also Gaussian with parameters given by the equations in (Roweis, 1999).",2.2. Method Specification and Expectation Propagation,[0],[0]
"Then, the Kullback-Leibler divergence between Z−1i,k φ k",2.2. Method Specification and Expectation Propagation,[0],[0]
"i q \i,k and q, i.e, KL[Z−1i,k φ k",2.2. Method Specification and Expectation Propagation,[0],[0]
"i q \i,k|q], is minimized with respect to q, where Zi,k is the normalization constant of φki q \i,k.",2.2. Method Specification and Expectation Propagation,[0.9559208075136042],"['Then, the Kullback-Leibler divergence between Z−1i,k φ k i q \\i,k and q, i.e, KL[Z−1i,k φ k i q \\i,k|q], is minimized with respect to q, where Zi,k is the normalization constant of φki q \\i,k.']"
"This is done by matching the moments of Z−1i,k φ k",2.2. Method Specification and Expectation Propagation,[0],[0]
"i q \i,k. These moments can be obtained from the derivatives of Zi,k with respect to the parameters of q\i,k",2.2. Method Specification and Expectation Propagation,[0],[0]
"(Seeger, 2006).",2.2. Method Specification and Expectation Propagation,[0],[0]
"After updating q, the new approximate factor is φ̃i,k = Zi,kq/q\i,k.",2.2. Method Specification and Expectation Propagation,[1.0],"['After updating q, the new approximate factor is φ̃i,k = Zi,kq/q\\i,k.']"
"We update all the approximate factors at the same time, and reconstruct q afterwards by computing the product of all the φ̃ki and the prior, as in (Hernández-Lobato et al., 2011).
",2.2. Method Specification and Expectation Propagation,[0],[0]
"The EP approximation to the marginal likelihood is the normalization constant of q, Zq .",2.2. Method Specification and Expectation Propagation,[0],[0]
"The log of its value is:
logZq = g(θ)− g(θprior) +",2.2. Method Specification and Expectation Propagation,[0],[0]
"∑N
i=1 ∑ k 6=yk log s̃i,k , (9)
where log s̃i,k = logZi,k + g(θ\i,k)− g(θ); θ, θ\i,k, and θprior are the natural parameters of q, q\i,k and the prior, respectively; and g(θ) is the log-normalizer of a multi-variate Gaussian distribution with natural parameters θ.
",2.2. Method Specification and Expectation Propagation,[0],[0]
"It is possible to show that if EP converges, the gradient of logZq w.r.t the parameters of each φ̃i,k is zero.",2.2. Method Specification and Expectation Propagation,[0],[0]
"Thus, the gradient of logZq w.r.t.",2.2. Method Specification and Expectation Propagation,[0],[0]
"a hyper-parameter ξkj of the k-th covariance function (including the inducing points) is:
∂ logZq ∂ξkj = (ηT − ηTprior) ∂θprior ∂ξkj + N∑ i=1",2.2. Method Specification and Expectation Propagation,[0],[0]
"∑ k 6=yi logZi,k ∂ξkj , (10)
",2.2. Method Specification and Expectation Propagation,[0],[0]
"where η and ηprior are the expected sufficient statistics under q and the prior, respectively.",2.2. Method Specification and Expectation Propagation,[0],[0]
"Importantly, only the direct dependency of logZi,k on ξkj has to be taken into account.",2.2. Method Specification and Expectation Propagation,[0],[0]
"See (Seeger, 2006).",2.2. Method Specification and Expectation Propagation,[0],[0]
"The dependency through θ\i,k, i.e., the natural parameters of q\i,k can be ignored.
",2.2. Method Specification and Expectation Propagation,[0],[0]
"After obtaining q and finding the model hyper-parameters by maximizing logZq , one can get an approximate predictive distribution for the label y? of a new instance",2.2. Method Specification and Expectation Propagation,[0.9826904387362158],"['After obtaining q and finding the model hyper-parameters by maximizing logZq , one can get an approximate predictive distribution for the label y?']"
"x?:
p(y?|x?,y) = ∫ p(y?|f?, f)q(f)dfdf? , (11)
where we have defined f? =",2.2. Method Specification and Expectation Propagation,[0],[0]
"(f1(x?), . . .",2.2. Method Specification and Expectation Propagation,[0],[0]
", fC(x?))T, and∫ p(y?|f?, f)df? has the same form as the likelihood factor in (3).",2.2. Method Specification and Expectation Propagation,[0],[0]
The resulting integral in (11) is again intractable.,2.2. Method Specification and Expectation Propagation,[0],[0]
"However, it can be approximated using a one-dimensional quadrature.",2.2. Method Specification and Expectation Propagation,[0],[0]
"See the supplementary material.
",2.2. Method Specification and Expectation Propagation,[0],[0]
"Because some simplifications occur when computing the derivatives of logZq w.r.t the inducing points, the total training time of EP is O(NM2) while the total memory cost is O(NMC) (Snelson, 2007).",2.2. Method Specification and Expectation Propagation,[0],[0]
"Traditionally, for finding the model hyper-parameters with EP one re-runs EP until convergence (using the previous solution as the starting point), after each gradient ascent update of the hyper-parameters.",2.3. Scalable Expectation Propagation,[0],[0]
"The reason for this is that (10) is only true if EP has converged (i.e., the approximate factors do not change any more).",2.3. Scalable Expectation Propagation,[1.0],"['The reason for this is that (10) is only true if EP has converged (i.e., the approximate factors do not change any more).']"
"This approach is particularly inefficient initially, when there are strong changes to the model hyper-parameters, and EP may require several iterations to converge.",2.3. Scalable Expectation Propagation,[0],[0]
"Recently, a more efficient method has been proposed in (Hernández-Lobato & HernándezLobato, 2016).",2.3. Scalable Expectation Propagation,[0],[0]
In that work the authors suggest to update both the approximate factors and the model hyperparameters at the same time.,2.3. Scalable Expectation Propagation,[0],[0]
"Because we do not wait for EP to converge, one should ideally add to (10) extra terms to get the gradient.",2.3. Scalable Expectation Propagation,[0],[0]
"These terms account for the mismatch between the moments of Z−1i,k φ k",2.3. Scalable Expectation Propagation,[0],[0]
"i q \i,k and q.",2.3. Scalable Expectation Propagation,[0],[0]
"However, according to (Hernández-Lobato & Hernández-Lobato, 2016) these extra terms can be ignored and one can simply use (10) for an inner update of the hyper-parameters.
",2.3. Scalable Expectation Propagation,[0],[0]
"Figure 1 shows, for the Vehicle dataset from UCI repository (Lichman, 2013), the estimate of the marginal likelihood logZq with respect to the training time, for 250 updates of the hyper-parameters, and M = N/5.",2.3. Scalable Expectation Propagation,[0],[0]
"We compare three
methods: (i) re-running EP until convergence each time and using (10) to update the hyper-parameters (EP-outer); (ii) updating at the same time the approximate factors φ̃ki and the hyper-parameters with (10) (EP-inner-approx); and (iii) the same approach as the previous one, but using the exact gradient for the update instead of (10) (EP-inner-exact).",2.3. Scalable Expectation Propagation,[0],[0]
All approaches successfully maximize logZq .,2.3. Scalable Expectation Propagation,[0],[0]
"However, the inner updates are more efficient as they do not wait until EP converges.",2.3. Scalable Expectation Propagation,[0],[0]
"Moreover, using the approximate gradient is faster (it is cheaper to compute), and it gives almost the same results as the exact gradient.",2.3. Scalable Expectation Propagation,[0],[0]
"The memory cost of EP can be significantly reduced by a technique called stochastic EP (SEP) (Li et al., 2015).",2.3.1. STOCHASTIC EXPECTATION PROPAGATION,[0],[0]
In SEP all the approximate factors φ̃ki are tied.,2.3.1. STOCHASTIC EXPECTATION PROPAGATION,[0],[0]
"This means that instead of storing their individual parameters, what is stored is their product, i.e., φ̃ = ∏N i=1",2.3.1. STOCHASTIC EXPECTATION PROPAGATION,[0],[0]
∏ k 6=yk φ̃ k,2.3.1. STOCHASTIC EXPECTATION PROPAGATION,[0],[0]
i .,2.3.1. STOCHASTIC EXPECTATION PROPAGATION,[0],[0]
A consequence of this is that we no longer have direct access to their individual parameters.,2.3.1. STOCHASTIC EXPECTATION PROPAGATION,[0],[0]
"This only affects the computation of the cavity distribution q\i,k which now is obtained in an approximate way q\i,k ∝ q/φ̃ 1n , where n is the total number of factors and φ̃ 1 n approximates each individual factor.",2.3.1. STOCHASTIC EXPECTATION PROPAGATION,[0],[0]
"Thus, SEP reduces the memory costs of EP by a factor of n. All the other steps are carried out as in the original EP algorithm, including the computation of logZq and its gradients.",2.3.1. STOCHASTIC EXPECTATION PROPAGATION,[0],[0]
Figure 2 shows the differences between EP and SEP on a toy example.,2.3.1. STOCHASTIC EXPECTATION PROPAGATION,[0],[0]
"When SEP is used in the proposed method, the memory cost is reduced to O(CM2).",2.3.1. STOCHASTIC EXPECTATION PROPAGATION,[0],[0]
Both the estimate of the log-marginal-likelihood in (9) and its gradient in (10) contain a sum across the data instances.,2.3.2. TRAINING USING MINI-BATCHES,[0],[0]
"This allows to write an EP algorithm that processes mini-batches of data, as in (Hernández-Lobato & Hernández-Lobato, 2016).",2.3.2. TRAINING USING MINI-BATCHES,[0],[0]
"For this, the data are split in mini-batchesMj of size S N , where N is the number of instances.",2.3.2. TRAINING USING MINI-BATCHES,[0],[0]
"Given a mini-batch Mj , we process all the approximate factors corresponding to that mini-batch, i.e., {{φ̃ki }k 6=yi : (xi, yi) ∈",2.3.2. TRAINING USING MINI-BATCHES,[0],[0]
Mj}.,2.3.2. TRAINING USING MINI-BATCHES,[0],[0]
"Then, we update the model hyper-parameters using a stochastic approximation of (10): ∂ logZq ∂ξkj ≈ (ηT − ηTprior) ∂θprior ∂ξkj + ρ ∑ i∈Mj ∑ k 6=yi logZi,k ∂ξkj , (12)
where ρ = N/|Mj |.",2.3.2. TRAINING USING MINI-BATCHES,[0],[0]
"We reconstruct q after each update of the approximate factors and each update of the hyper-
parameters.",2.3.2. TRAINING USING MINI-BATCHES,[0],[0]
"When using mini-batches of data, we update more frequently q and the hyper-parameters.",2.3.2. TRAINING USING MINI-BATCHES,[0],[0]
"The consequence is that the training cost is O(CM3), assuming a constant number of updates until convergence.",2.3.2. TRAINING USING MINI-BATCHES,[0],[0]
This training scheme can handle datasets with millions of instances.,2.3.2. TRAINING USING MINI-BATCHES,[0],[0]
"The likelihood used in (1) was first considered for multiclass Gaussian process classification in (Kim & Ghahramani, 2006).",3. Related Work,[0],[0]
"That work considers full non-parametric GP priors, which lead to a training cost that is O(CN3).",3. Related Work,[0],[0]
The consequence is that it can only address small classification problems.,3. Related Work,[0],[0]
"It is, however, straight forward to replace the non-parametric GP priors with the FITC approximate priors pFITC(fk|X k ) (Quiñonero-Candela & Rasmussen, 2005).",3. Related Work,[0],[0]
"These priors are obtained by marginalizing the latent variables f k associated to the inducing points X k , as indicated in Section 2.1.",3. Related Work,[0],[0]
This allows to address datasets with a few thousand instances.,3. Related Work,[0],[0]
"This is precisely the approach followed in (Naish-Guzman & Holden, 2008) to address binary GP classification problems.",3. Related Work,[0],[0]
We refer to such an approach as the generalized FITC approximation (GFITC).,3. Related Work,[0],[0]
"Nevertheless, such an approach cannot use stochastic optimization.",3. Related Work,[0],[0]
The reason is that the estimate of the log-marginal-likelihood (needed for hyper-parameter estimation) does not contain a sum across the instances.,3. Related Work,[0],[0]
"Thus, GFITC cannot scale well to very large datasets.",3. Related Work,[0],[0]
"Nevertheless, unlike the proposed approach, it can run expectation propagation over the exact likelihood factors in (1).",3. Related Work,[0],[0]
"In GFITC we follow the traditional approach and run EP until convergence before updating the hyper-parameters.
",3. Related Work,[0],[0]
"Multi-class GP classification for potentially huge datasets has also been considered in (Hensman et al., 2015b) using variational inference (VI).",3. Related Work,[0],[0]
"However, such an approach cannot use the likelihood in (1) since its logarithm is not well defined (note that it takes value zero for some values of fi).",3. Related Work,[0],[0]
"As an alternative, Hensman et al. (2015b) have considered the robust likelihood of (Hernández-Lobato et al., 2011):
p(yi|fi) =",3. Related Work,[0],[0]
(1− ),3. Related Work,[0],[0]
∏,3. Related Work,[0],[0]
k 6=yi Θ ( fyi(xi)− fk(xi) ),3. Related Work,[0],[0]
"+ C , (13)
where is the probability of a labeling error (in that case, yi is chosen at random from the potential class labels).",3. Related Work,[0],[0]
"In (Hensman et al., 2015b)",3. Related Work,[0],[0]
"it is suggested to set = 10−3.
",3. Related Work,[0],[0]
We now describe the VI approach in detail.,3. Related Work,[0],[0]
"Using (13) and the definitions of Section 2, we know that p(y|f) =∫ p(y|f)p(f |f)df .",3. Related Work,[0],[0]
If we take the log and use Jensen’s inequality we get the bound log p(y|f) ≥ Ep(f |f)[log p(y|f)].,3. Related Work,[0],[0]
Consider now a Gaussian approximation q to p(f |y).,3. Related Work,[0],[0]
"Then,
log p(y) = ∫ q(f)p(y|f)p(f)/q(f)df ≥",3. Related Work,[0],[0]
"Eq(f)[log p(y|f)]− KL[q(f)||p(f)] , (14)
where we have used Jensen’s inequality and KL is the Kull-
back Leibler divergence.",3. Related Work,[0],[0]
"If we use the first bound we get
log p(y) ≥ Eq(f)[Ep(f |f)[log p(y|f)]]KL[q(f)||p(f)]
≥ Eq(f)[log p(y|f)]− KL[q(f)||p(f)]",3. Related Work,[0],[0]
"≥ ∑N i=1Eq(fi)[log p(yi|fi)]− KL[q(f)||p(f)] , (15)
where q(f) = ∫ p(f |f)q(f)df and the corresponding marginal over fi = (f1(xi), . . .",3. Related Work,[0],[0]
", fC(xi))T is q(fi) =∏C k=1N (fk(xi)|m̂ki , ŝki ).",3. Related Work,[0],[0]
Note that q(fi) is Gaussian because q(f) involves a Gaussian convolution.,3. Related Work,[0],[0]
"As in the proposed approach, q(f) is assumed to be a Gaussian factorizing over the latent functions f 1 , . . .",3. Related Work,[0],[0]
", f C .",3. Related Work,[0],[0]
"However, its mean and covariance parameters, i.e., {mk}Ck=1 and {Sk}Ck=1 are found by maximizing (15).",3. Related Work,[0],[0]
"The parameters of q(fi) are:
m̂ki =",3. Related Work,[0],[0]
"(k k
xiX k ) T(Kk X k X k ) −1mk , (16)
ŝki = κ k",3. Related Work,[0],[0]
xixi,3. Related Work,[0],[0]
"− (k k
xiX k ) T(Kk X k X k ) −1kk xiX",3. Related Work,[0],[0]
"k
+ (kk xiX k )",3. Related Work,[0],[0]
T(Kk X k X k ),3. Related Work,[0],[0]
−1Sk(Kk X k X k ) −1kk xiX k .,3. Related Work,[0],[0]
"(17)
Hensman et al. (2015b) consider Markov chain Monte Carlo (MCMC) to sample the hyper-parameters.",3. Related Work,[0],[0]
Here we simply maximize (15) to find the hyper-parameters and the inducing points.,3. Related Work,[0],[0]
The reason for this is that in very large datasets MCMC is not expected to give much better results.,3. Related Work,[0],[0]
We refer to the described approach as VI.,3. Related Work,[0],[0]
The objective in (15) contains a sum across the data instances.,3. Related Work,[0],[0]
"Thus, VI also allows for stochastic optimization and it results in the same cost as the proposed approach.",3. Related Work,[0],[0]
"However, the expectations in (15) must be approximated using one-dimensional quadratures.",3. Related Work,[0],[0]
This is a drawback with respect to the proposed method which is free of any quadrature.,3. Related Work,[0],[0]
"Finally, there are some methods related to the VI approach just described.",3. Related Work,[0],[0]
"Dezfouli & Bonilla (2015) assume that q can be a mixture of Gaussians, and Chai (2012) uses a soft-max likelihood (but does not consider stochastic optimization).",3. Related Work,[0],[0]
"Both works need to introduce extra approximations.
",3. Related Work,[0],[0]
In the literature there are other research works addressing multi-class Gaussian process classification.,3. Related Work,[0],[0]
"Some examples include (Williams & Barber, 1998; Girolami & Rogers, 2006; Hernández-Lobato et al., 2011; Henao & Winther, 2012; Riihimäki et al., 2013).",3. Related Work,[0],[0]
"These works employ expectation propagation, variational inference or the Laplace approximation to approximate the computations.",3. Related Work,[0],[0]
"Nevertheless, the corresponding estimate of the logmarginal-likelihood cannot be expressed as a sum across the data instances.",3. Related Work,[0],[0]
This avoids using efficient techniques for optimization based on stochastic gradients.,3. Related Work,[0],[0]
"Thus, one cannot address very large datasets with these methods.",3. Related Work,[0],[0]
We evaluate the performance of the method proposed in Section 2.2.,4. Experiments,[0],[0]
We consider two versions of it.,4. Experiments,[0],[0]
"A first
one, using expectation propagation (EP).",4. Experiments,[0],[0]
"A second, using the memory efficient stochastic EP (SEP).",4. Experiments,[0],[0]
EP and SEP are compared with the methods described in Section 3.,4. Experiments,[1.0],['EP and SEP are compared with the methods described in Section 3.']
"Namely, GFITC and VI.",4. Experiments,[0],[0]
"All methods are codified in the R language (the source code is in the supplementary material), and they consider the same initial values for the model hyper-parameters (including the inducing points, that are chosen at random from the training instances).",4. Experiments,[0],[0]
The hyperparameters are optimized by maximizing the estimate of the marginal likelihood.,4. Experiments,[0],[0]
"A Gaussian covariance function with automatic relevance determination, an amplitude parameter and an additive noise parameter is employed.",4. Experiments,[0],[0]
"We evaluate the performance of each method on 8 datasets from the UCI repository (Lichman, 2013).",4.1. Performance on Datasets from the UCI Repository,[0],[0]
The characteristics of the datasets are displayed in Table 4.1.,4.1. Performance on Datasets from the UCI Repository,[0],[0]
"We use batch training in each method (i.e., we go through all the data to compute the gradients).",4.1. Performance on Datasets from the UCI Repository,[0],[0]
Batch training does not scale to large datasets.,4.1. Performance on Datasets from the UCI Repository,[0],[0]
"However, it is preferred on small datasets like the ones considered here.",4.1. Performance on Datasets from the UCI Repository,[0],[0]
"We use 90% of the data for training and 10% for testing, expect for Satellite which is fairly big, where we use 20% for training and 80% for testing.",4.1. Performance on Datasets from the UCI Repository,[0],[0]
"In Waveform, which is synthetic, we generate 1000 instances and split them in 30% for training and 70% for testing.",4.1. Performance on Datasets from the UCI Repository,[0],[0]
"Finally, in Vowel we consider only the points that belong to the 6 first classes.",4.1. Performance on Datasets from the UCI Repository,[0],[0]
"All methods are trained for 250 iterations using gradient ascent (GFITC and VI use lBFGS, EP and SEP use an adaptive learning rate described in the supplementary material).",4.1. Performance on Datasets from the UCI Repository,[0],[0]
"We consider three values for M , the number of inducing points.",4.1. Performance on Datasets from the UCI Repository,[0],[0]
"Namely 5%, 10% and 20% of the number of training instances.",4.1. Performance on Datasets from the UCI Repository,[0],[0]
"We report averages over 20 repetitions of the experiments.
",4.1. Performance on Datasets from the UCI Repository,[0],[0]
"Table 2 shows, for each value of M , the average negative test log-likelihood of each method with the corresponding error bars (test errors are shown in the supplementary material).",4.1. Performance on Datasets from the UCI Repository,[0],[0]
The average training time of each method is also displayed.,4.1. Performance on Datasets from the UCI Repository,[0],[0]
The best method (the lower the better) for each dataset is highlighted in bold face.,4.1. Performance on Datasets from the UCI Repository,[0],[0]
"We observe that the proposed approach, EP, obtains very similar results to those of GFITC, and sometimes it obtains the best results.",4.1. Performance on Datasets from the UCI Repository,[0],[0]
"The memory efficient version of EP, SEP, seems to provide similar results without reducing the performance.",4.1. Performance on Datasets from the UCI Repository,[0],[0]
"Regarding the computational cost, SEP is the
fastest method (between 2 and 3 times faster than GFITC).",4.1. Performance on Datasets from the UCI Repository,[0],[0]
VI is slower as a consequence of the quadratures required by this method.,4.1. Performance on Datasets from the UCI Repository,[0],[0]
"VI also gives much worse results in some datasets, e.g., Glass, Svmguide2 and Waveform.",4.1. Performance on Datasets from the UCI Repository,[0],[0]
"This is related to the optimization of Eq(fi)[log p(yi|fi)] in (15), instead of logEq(fi)[p(yi|fi)], which is closer to the data loglikelihood.",4.1. Performance on Datasets from the UCI Repository,[1.0],"['This is related to the optimization of Eq(fi)[log p(yi|fi)] in (15), instead of logEq(fi)[p(yi|fi)], which is closer to the data loglikelihood.']"
"In the EP objective in (9), ∑ k 6=yi logZi,k is probably more similar to logEq(fi)[p(yi|fi)].",4.1. Performance on Datasets from the UCI Repository,[0],[0]
This explains the much better results obtained by EP and SEP.,4.1. Performance on Datasets from the UCI Repository,[1.0],['This explains the much better results obtained by EP and SEP.']
"We generate a synthetic two dimensional problem with three classes by sampling the latent functions from the GP prior and applying the rule yi = arg maxk f
k(xi).",4.2. Analysis of Inducing Point Learning,[0],[0]
The distribution of xi is uniform in the box,4.2. Analysis of Inducing Point Learning,[0],[0]
"[−2.5, 2.5] ×",4.2. Analysis of Inducing Point Learning,[0],[0]
"[−2.5, 2.5].",4.2. Analysis of Inducing Point Learning,[0],[0]
"We consider 1000 training instances and a growing number of inducing points, i.e., M = 1 to M = 256.",4.2. Analysis of Inducing Point Learning,[0],[0]
The initial location of the inducing points is chosen at random,4.2. Analysis of Inducing Point Learning,[0],[0]
and it is the same for all the methods.,4.2. Analysis of Inducing Point Learning,[0],[0]
We are interested in the location of the inducing points after training.,4.2. Analysis of Inducing Point Learning,[0],[0]
"Thus, we set the other hyper-parameters to their true values (specified before generating the data) and we keep them fixed.",4.2. Analysis of Inducing Point Learning,[0],[0]
All methods but VI are trained using batch methods during 2000 iterations.,4.2. Analysis of Inducing Point Learning,[0],[0]
VI is trained using stochastic gradients for 2000 epochs (the batch version often gets stuck in local optima).,4.2. Analysis of Inducing Point Learning,[0],[0]
"We use ADAM with the default settings (Kingma & Ba, 2015), and 100 as the mini-batch size.
",4.2. Analysis of Inducing Point Learning,[0],[0]
Figure 3 shows the location learnt by each method for the inducing points.,4.2. Analysis of Inducing Point Learning,[0],[0]
"Blue, red and green points represent the training data, black lines are decision boundaries and black border points are the inducing points.",4.2. Analysis of Inducing Point Learning,[1.0],"['Blue, red and green points represent the training data, black lines are decision boundaries and black border points are the inducing points.']"
As we increase the number of inducing points the methods become more accurate.,4.2. Analysis of Inducing Point Learning,[0],[0]
"However, GFITC, EP and SEP identify decision boundaries that are better with a smaller number of inducing points.",4.2. Analysis of Inducing Point Learning,[0],[0]
VI fails in this task.,4.2. Analysis of Inducing Point Learning,[0],[0]
This is probably because VI updates the inducing-points with a bad estimate of q during the initial iterations.,4.2. Analysis of Inducing Point Learning,[1.0],['This is probably because VI updates the inducing-points with a bad estimate of q during the initial iterations.']
"VI uses gradient steps to update q, which is less efficient than the EP updates (free of any learning rate).",4.2. Analysis of Inducing Point Learning,[0],[0]
"GFITC, EP and SEP overlap the inducing points, which can be seen as a pruning mechanism (if two inducing points are equal, it is like having only one).",4.2. Analysis of Inducing Point Learning,[0],[0]
"This has already been observed in regression problems (Bauer et al., 2016).",4.2. Analysis of Inducing Point Learning,[0],[0]
"By contrast, VI places the inducing points near the decision boundaries.",4.2. Analysis of Inducing Point Learning,[1.0],"['By contrast, VI places the inducing points near the decision boundaries.']"
"This agrees with previous results on binary classification (Hensman et al., 2015a).",4.2. Analysis of Inducing Point Learning,[0],[0]
Figure 4 shows the negative test log-likelihood of each method as a function of the training time on the Satellite dataset (EP results are not shown since it performs equal to SEP).,4.3. Performance as a Function of the Training Time,[0],[0]
Training is done as in Section 4.1.,4.3. Performance as a Function of the Training Time,[1.0],['Training is done as in Section 4.1.']
"We consider a growing number of inducing points M = 4, 20, 100 and report averages over 100 repetitions of the experiments.",4.3. Performance as a Function of the Training Time,[0],[0]
In all methods we use batch training.,4.3. Performance as a Function of the Training Time,[0],[0]
We observe that SEP is the method with the best performance at the lowest cost.,4.3. Performance as a Function of the Training Time,[0],[0]
"Again, it is faster than GFITC because it optimizes q and the hyper-parameters at the same time, while GFITC waits until EP has converged to update the hyper-parameters.",4.3. Performance as a Function of the Training Time,[0],[0]
"VI is not very efficient for small values of M , due to the quadratures.",4.3. Performance as a Function of the Training Time,[0],[0]
"It also takes more time to get a good estimate of q, which is updated by gradient descent and is less ef-
ficient than the EP updates.",4.3. Performance as a Function of the Training Time,[0],[0]
Similar results are obtained in terms of the test error.,4.3. Performance as a Function of the Training Time,[0],[0]
See the supplementary material.,4.3. Performance as a Function of the Training Time,[0],[0]
"However, in that case VI does not overfit the training data.",4.3. Performance as a Function of the Training Time,[1.0],"['However, in that case VI does not overfit the training data.']"
"In very large datasets batch training is infeasible, and one must use mini-batches to update q and to approximate the required gradients.",4.4. Performance When Using Stochastic Gradients,[0],[0]
"We evaluate the performance of each method on the MNIST dataset (LeCun et al., 1998) with M = 200 inducing points and mini-batches with 200 instances.",4.4. Performance When Using Stochastic Gradients,[0],[0]
"This dataset has 60, 000 instances for training and 10, 000 for testing.",4.4. Performance When Using Stochastic Gradients,[0],[0]
"The learning rate of each method is set using ADAM with the default parameters (Kingma & Ba, 2015).",4.4. Performance When Using Stochastic Gradients,[0],[0]
GFITC does not allow for stochastic optimization.,4.4. Performance When Using Stochastic Gradients,[1.0],['GFITC does not allow for stochastic optimization.']
"Thus, it is ignored in the comparison.",4.4. Performance When Using Stochastic Gradients,[0],[0]
The test error and the negative test log-likelihood of each method is displayed in Figure 5 (top) as a function of the training time.,4.4. Performance When Using Stochastic Gradients,[0],[0]
In this larger dataset all methods perform similarly.,4.4. Performance When Using Stochastic Gradients,[1.0],['In this larger dataset all methods perform similarly.']
"However, EP and SEP take less time to converge than VI.",4.4. Performance When Using Stochastic Gradients,[0],[0]
SEP obtains a test error that is 2.08% and average negative test log-likelihood that is 0.0725.,4.4. Performance When Using Stochastic Gradients,[0],[0]
"The results of VI are 2.02% and 0.0686, respectively.",4.4. Performance When Using Stochastic Gradients,[0],[0]
"These results are similar to the
ones reported in (Hensman et al., 2015a) using M = 500.
",4.4. Performance When Using Stochastic Gradients,[0],[0]
A last experiment considers all flights within the USA between 01/2008 and 04/2008 (http://stat-computing.,4.4. Performance When Using Stochastic Gradients,[0],[0]
org/dataexpo/2009).,4.4. Performance When Using Stochastic Gradients,[0],[0]
The task is to classify the flights according to their delay using three classes:,4.4. Performance When Using Stochastic Gradients,[0],[0]
"On time, more than 5 minutes of delay, or more than 5 minutes before time.",4.4. Performance When Using Stochastic Gradients,[0],[0]
"We consider 8 attributes: age of the aircraft, distance covered, airtime, departure time, arrival time, day of the week, day of the month and month.",4.4. Performance When Using Stochastic Gradients,[0],[0]
"After removing all instances with missing data 2, 127, 068 instances remain, from which 10, 000 are used for testing and the rest for training.",4.4. Performance When Using Stochastic Gradients,[0],[0]
We use the same settings as on the MNIST dataset and evaluate each method.,4.4. Performance When Using Stochastic Gradients,[0],[0]
The results obtained are shown in Figure 5 (bottom).,4.4. Performance When Using Stochastic Gradients,[0],[0]
We also report the performance of a logistic regression classifier.,4.4. Performance When Using Stochastic Gradients,[0],[0]
"Again, all methods perform similarly in terms of test error.",4.4. Performance When Using Stochastic Gradients,[0],[0]
"However, EP and SEP converge faster and quickly outperform the linear model.",4.4. Performance When Using Stochastic Gradients,[0],[0]
"Importantly, the negative test log-likelihood of VI starts increasing at some point, which is again probably due to the optimization of Eq(fi)[log p(yi|fi)] in (15).",4.4. Performance When Using Stochastic Gradients,[0],[0]
The supplementary material has further evidence supporting this.,4.4. Performance When Using Stochastic Gradients,[0],[0]
"We have proposed the first method for multi-class classification with Gaussian processes, based on expectation propagation (EP), that scales well to very large datasets.",5. Conclusions,[0],[0]
"Such a method uses the FITC approximation to reduce the number of latent variables in the model from O(N) to O(M), whereM N , andN is the number of data instances.",5. Conclusions,[0],[0]
"For this,M inducing points are introduced for each latent function in the model.",5. Conclusions,[0],[0]
"Importantly, the proposed method allows for stochastic optimization as the estimate of the log-
marginal-likelihood involves a sum across the data.",5. Conclusions,[0],[0]
"Moreover, we have also considered a stochastic version of EP (SEP) to reduce the memory usage.",5. Conclusions,[0],[0]
"When mini-batches and stochastic gradients are used for training, the computational cost of the proposed approach is O(CM3), with C the number of classes.",5. Conclusions,[0],[0]
"The memory cost is O(CM2).
",5. Conclusions,[0],[0]
"We have compared the proposed method with other approaches from the literature based on variational inference (VI) (Hensman et al., 2015b), and with the model considered by Kim & Ghahramani (2006), which has been combined with FITC approximate priors (GFITC) (QuiñoneroCandela & Rasmussen, 2005).",5. Conclusions,[0],[0]
"The proposed approach outperforms GFITC in large datasets as this method does not allow for stochastic optimization, and in small datasets it produces similar results.",5. Conclusions,[0],[0]
"The proposed method, SEP, is slightly faster than VI which also allows for stochastic optimization.",5. Conclusions,[0],[0]
"In particular, VI requires one-dimensional quadratures which in small datasets are expensive.",5. Conclusions,[0],[0]
We have also observed that SEP converges faster than VI.,5. Conclusions,[0],[0]
"This is probably because the EP updates, free of any learning rate, are more efficient for finding a good posterior approximation than the gradient ascent updates employed by VI.
",5. Conclusions,[0],[0]
An important conclusion of this work is that VI sometimes gives bad predictive distributions in terms of the test loglikelihood.,5. Conclusions,[1.0],['An important conclusion of this work is that VI sometimes gives bad predictive distributions in terms of the test loglikelihood.']
The EP and SEP methods do not seem to have this problem.,5. Conclusions,[1.0],['The EP and SEP methods do not seem to have this problem.']
"Thus, if one cares about accurate predictive distributions, VI should be avoided in favor of the proposed methods.",5. Conclusions,[1.0],"['Thus, if one cares about accurate predictive distributions, VI should be avoided in favor of the proposed methods.']"
"In our experiments we have also observed that the proposed approaches tend to place the inducing points one on top of each other, which can be seen as an inducing point pruning technique (Bauer et al., 2016).",5. Conclusions,[0],[0]
"By contrast, VI tends to place them near the decision boundaries.",5. Conclusions,[1.0],"['By contrast, VI tends to place them near the decision boundaries.']"
The authors gratefully acknowledge the use of the facilities of Centro de Computación Cientı́fica (CCC) at Universidad Autónoma de Madrid.,Acknowledgements,[0],[0]
"The authors also acknowledge financial support from Spanish Plan Nacional I+D+i, Grants TIN2013-42351-P, TIN2016-76406P, TIN2015-70308-REDT and TEC2016-81900-REDT (MINECO/FEDER EU), and from Comunidad de Madrid, Grant S2013/ICE-2845.",Acknowledgements,[0],[0]
This paper describes an expectation propagation (EP) method for multi-class classification with Gaussian processes that scales well to very large datasets.,abstractText,[0],[0]
In such a method the estimate of the log-marginal-likelihood involves a sum across the data instances.,abstractText,[0],[0]
This enables efficient training using stochastic gradients and mini-batches.,abstractText,[0],[0]
"When this type of training is used, the computational cost does not depend on the number of data instances N .",abstractText,[0],[0]
"Furthermore, extra assumptions in the approximate inference process make the memory cost independent of N .",abstractText,[0],[0]
The consequence is that the proposed EP method can be used on datasets with millions of instances.,abstractText,[0],[0]
We compare empirically this method with alternative approaches that approximate the required computations using variational inference.,abstractText,[0],[0]
"The results show that it performs similar or even better than these techniques, which sometimes give significantly worse predictive distributions in terms of the test log-likelihood.",abstractText,[0],[0]
"Besides this, the training process of the proposed approach also seems to converge in a smaller number of iterations.",abstractText,[0],[0]
Scalable Multi-Class Gaussian Process Classification  using Expectation Propagation,title,[0],[0]
