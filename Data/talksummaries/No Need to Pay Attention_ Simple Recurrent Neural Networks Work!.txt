0	63	First-order factoid question answering (QA) assumes that the question can be answered by a single fact in a knowledge base (KB).
1	34	For example, “How old is Tom Hanks” is about the [age] of [Tom Hanks].
2	23	Also referred to as simple questions by Bordes et al. (2015), recent attempts that apply either complex linguistic reasoning or attention-based complex neural network architectures achieve up to 76% accuracy on benchmark sets (Golub and He, 2016; Yin et al., 2016).
3	20	While it is tempting to study QA systems that can handle more complicated questions, it is hard to reach reasonably high precision for unrestricted questions.
8	20	In particular, we assume that the answer to a first-order question is a single property of a single entity in the KB, and decompose the task into two subproblems: (a) detecting entities in the question and (b) classifying the question as one of the relation types in the KB.
43	40	In relation prediction, the question is classified into one of the 1837 classes (i.e., relation types in Freebase).
44	24	In the entity detection task, each word is classified as either entity or context (i.e., k = 2).
45	24	Given a new question, we run the two RNN models to construct the structured query.
46	14	Once every question word is classified as entity (denoted by E) or context (denoted by C), we can extract entity phrase(s) by grouping consecutive entity words.
57	61	Here is how the weights would be computed for unigram “sarah” and bigram “michelle gellar” (⇒ denotes mapping): Ientity(“sarah”)⇒ {node : ei, score : TF -IDF (“sarah”, “sarah michelle gellar”)} Ientity(“michelle gellar”)⇒ {node : ei, score : TF -IDF (“michelle gellar”, “sarah michelle gellar”)} This is performed for every n-gram (n ∈ {1, 2, 3,∞}) of every entity node in the KB.
58	19	Assuming there is an entity node, say ej , for the actress “Sarah Jessica Parker”, we would end up creating a second mapping from unigram “sarah”: Ientity(“sarah”)⇒ {node : ej , score : TF -IDF (“sarah”, “sarah jessica parker”)} In other words, “sarah” would be linked to both ei and ej , with corresponding TF -IDF weights.
59	37	Once the index Ientity is built, we can link entityText from the structured query (e.g., “michelle gellar”) to the intended entity in the KB (e.g., ei).
61	14	For each n-gram, retrieved entities are appended to the candidate set C. We continue this process with decreasing value of n (i.e., n ∈ {∞, 3, 2, 1}) Early termination happens if C is non-empty and n is less than or equal to the number of tokens in entityText.
64	40	Once we have a list of candidate entities C, we use each candidate node ecand as a starting point to reach candidate answers.
68	24	These are added to the candidate answer set A.
72	19	Indexes Ientity and Ireach are built based on this knowledge base.
79	17	For each possible setting, we trained the model on the training portion and used the validation portion to avoid over-fitting.
90	16	Even though there are 1837 relation types in Freebase, the number of relation types that we need to consider per question (on average) drops to 36.
95	31	We use the best models based on validation set accuracy and compare it to three prior approaches: a specialized network architecture that explicitly memorizes facts (Bordes et al., 2015), a network that learns how to convolve sequence of characters in the question (Golub and He, 2016), and a complex network with attention mechanisms to learn most important parts of the question (Yin et al., 2016).
96	19	Our approach outperforms the state of the art in accuracy (i.e., precision at top 1) by 11.9 points (15.6% relative).
97	34	Last three rows quantify the impact of each component via an ablation study, in which we replace either entity detection (ED) or relation prediction (RP) models with a naive baseline: (i) we assign the relation that appears most frequently in training data (i.e., bornOn), and/or (ii) we tag the entire question as an entity (and then perform the n-gram entity linking).
102	20	The reverse is true for 48% cases.4 We manually labeled a sample of 50 instances from each blame scenario.
104	21	We found 16% of the detected entities to be correct, even though it was not the same as the ground truth (e.g., either “New York” or “New York City” is correct in “what can do in new york?”); 18% are inherently ambiguous and need clarification (e.g., “where bin laden got killed?” might mean “Osama” or “Salem”).
105	23	When blame is on relation prediction, we found that the predicted relation is reasonable (albeit different than ground truth) 29% of the time (e.g., “what was nikola tesla known for” can be classified as profession or notable for).
107	15	We demonstrate this by applying our model (without any modifications) to the entertainment domain and deploying it to the Comcast X1 platform serving millions of customers every day.
108	16	Training data was generated synthetically based on an internal entertainment KB.
110	48	We can draw two important conclusions from Table 2: First of all, we find that almost all of the user-generated natural-language questions (278/295∼95%) are first-order questions, supporting the significance of first-order QA as a task.
111	28	Second, we show that even if we simply use an open-sourced deep learning toolkit (keras.io) for implementation and limit the computational resources to 2 CPU cores per thread, RNN-QA answers 75% of questions correctly with very reasonable latency.
112	18	We described a simple yet effective approach for QA, focusing primarily on first-order factual questions.
121	33	While firstorder QA might seem like a solved problem, there is clearly still room for improvement.
