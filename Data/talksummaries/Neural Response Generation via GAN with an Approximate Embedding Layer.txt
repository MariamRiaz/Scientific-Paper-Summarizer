0	38	After achieving remarkable successes in Machine Translation (Sutskever et al., 2014; Cho et al., 2014), neural networks with the encoder-decoder architectures (a.k.a sequence-to-sequence models, Seq2Seq) have been proven to be a functioning method to model short-text conversations (Vinyals and Le, 2015; Shang et al., 2015), where the corresponding task is often called Neural Response Generation.
2	88	One of the potential applications of such neural response generators is to improve the capability of existing conversational interfaces (informally also known as chatbots) by enabling them to go beyond predefined tasks and chat with human users in an open domain.
3	38	However, previous research has indicated that naı̈ve implementations of Seq2Seq based conversation models tend to suffer from the so-called “safe response” problem (Li et al., 2016a), i.e. such models tend to generate non-informative responses that can be associated to most queries, e.g. “I don’t know”, “I think so”, etc.
4	35	This is due to the fundamental nature of statistical models, which fit sufficiently observed examples better than insufficiently observed ones.
6	39	Furthermore, since a safe response can be of relevance to a large amount of diverse queries, a statistical learner will tend to minimize its empirical risk in the response generation process by capturing those safe responses if naı̈ve relevance-oriented loss metrics are employed.
7	16	Frequent occurrences of safe responses can dramatically reduce the attractiveness of a chat agent, which therefore should be avoided to the best extent possible when designing the learning algorithms.
13	16	The sampling procedure is non-differentiable, and will therefore break the back-propagation.
14	16	To the best of our knowledge, Reinforcement Learning (RL) is first introduced to address the above problem (Li et al., 2017; Yu et al., 2017), where the score predicted by a discriminator was used as the reinforcement to train the generator, yielding a hybrid model of GAN and RL.
32	24	Generally speaking, the whole framework consists of a response generator G, a discriminator D and an embedding approximation layer that connects the G and the D. We explain each of the components in detail as follows.
34	82	An approximate embedding layer is designed to guarantee that the response generation procedure is continuous and differentiable, serving as an interface for the discriminator to propagate its loss to the generator.
36	49	The judgement of the CNN can be propagated to the Seq2Seq generator through the proposed approximate embedding layer, and forces the generator to be fine-tuned to produce more attractive results.
37	15	The proposed GAN framework possesses sev- eral advantages over existing conversational response generation models.
39	22	Secondly, the discriminator enforces the generator to produce a response according to the true distribution in better granularity, such that the state of promoting safe responses is leaped out.
41	33	In our proposed encoder-decoder framework, both the encoder and the generator (i.e. the decoder) G is composed of GRU (Cho et al., 2014) units, which is designed to generate responses r = {wr,1, wr,2, · · · , wr,K} conditioned on an input query q = {wq,1, wq,2, · · · , wq,J}.
43	33	Concretely, in this model, q is firstly encoded into a vector representation qv by the GRU-based encoder as shown in Figure 1, which is actually the last hidden state of the encoder.
44	32	Then the generator estimates the probability of each word occurring in r conditioned on qv.
45	39	Hence p(r|q) can be formulated as follows: p(r|q) = K∏ t=1 p(wr,t|qv, wr,1, · · · , wr,t−1) (1) Taking the logarithm of the probabilities for effective computation, the generator is trained by optimising the Maximum Likelihood Estimation (MLE) objective defined as: 1 |D| ∑ (q,r)∈D K∑ t=1 log p(wr,t|qv, wr,1, · · · , wr,t−1) (2) Note here, we need to pre-train the generator using Equation 2 as the loss function to guarantee the generator to produce grammatical utterances.
46	34	Otherwise, the discriminator will tend to learn a rule with ease to distinguish human-produced utterances from those ungrammatical responses generated in the early stages of the training phase, which would cause the failure of the training in satisfying Nash Equilibrium (Goodfellow et al., 2014).
47	37	In order to smoothly connect the output layer of the generator to the input layer of the discriminator to yield an end-to-end differentiable GAN, one needs to solve the following critical problem.
48	46	The output of the generator is a sequence of discrete words, which is usually sampled from the distributions predicted by the decoder’s RNN units in the Softmax layer, and is non-differentiable.
53	68	Concretely, the approximation layer takes the output hi of the generator and a random noise zi as the input, and reuses the word projection layer (pre-trained in the standard generator) to estimate the probability distribution of wi.
54	47	Note that, the noise zi added to hi forms a latent feature for the word embedding approximation process to enforce the diversity of the generated responses.
56	20	CNN has been proven to be an appropriate classifier for many NLP tasks, such as sentence classification (Kim, 2014) and matching (Hu et al., 2014).
61	23	Firstly, the input of the discriminator consist of the word embedding vector sequence Vq for a given query q and the word embedding vector sequence Vr for its human-produced response r, as well as the approximate word embedding vector sequence Vr̂ produced by the approximate embedding layer for the corresponding fake response r̂.
71	45	The following tricks are utilised in the adversarial training phase to achieve better convergence.
83	38	To illustrate the performance of the proposed model, we introduce three existing approaches as baselines.
85	40	• MMI-anti: a Seq2Seq model with a Maximum Mutual Information (MMI) criterion (implemented as an anti-language model) (Li et al., 2016a) in the decoding process, which reduces the probability of generating “safe responses”.
86	67	• Adver-REGS: another adversarial strategy proposed by Li et al. (2017)2, which links the generator and the discriminator together with a reinforcement learning framework, and takes the discriminator’s output probability as the reward to train the generator.
87	57	com/jiweil/Neural-Dialogue-Generation/ tree/master/Adversarial
89	122	Note here, the goal of our model is to obtain responses not only semantically relevant to the corresponding queries, but also of good diversity and novelty.
