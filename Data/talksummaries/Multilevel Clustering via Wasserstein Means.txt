2	21	A prominent strand of modeling and algorithmic works in the past couple decades has been to discover latent multilevel structures from these hierarchically structured data.
3	68	For specific clustering tasks, one may be interested in simultaneously partitioning the data in each group (to obtain local clusters) and partitioning a collection of data groups (to obtain global clusters).
4	23	Another concrete example is the problem of clustering images (i.e., global clusters) where each image contains partions of multiple annotated regions (i.e., local clusters) (Oliva and Torralba, 2001).
8	28	In this interesting work, a Bayesian nonparametric model, namely the nested Dirichlet process (NDP) model, was introduced that enables the inference of clustering of a collection of probability distributions from which different groups of data are drawn.
12	23	Our technical approach is inspired by the role of optimal transport distances in hierarchical modeling and clustering problems.
13	49	The optimal transport distances, also known as Wasserstein distances (Villani, 2003), have been shown to be the natural distance metric for the convergence theory of latent mixing measures arising in both mixture models (Nguyen, 2013) and hierarchical models (Nguyen, 2016).
15	52	If one is to perform simultaneous K-means clustering for hierarchically grouped data, both at the global level (among groups), and local level (within each group), then this can be achieved by a joint optimization problem defined with suitable notions of Wasserstein distances inserted into the objective function.
16	29	In particular, multilevel clustering requires the optimization in the space of probability mea- sures defined in different levels of abstraction, including the space of measures of measures on the space of grouped data.
20	13	From a computational viewpoint, quite interestingly, we will be able to explicate and exploit the connection betwen our optimization and that of finding the Wasserstein barycenter (Agueh and Carlier, 2011), an interesting computational problem that have also attracted much recent interests, e.g., (Cuturi and Doucet, 2014).
26	10	Section 5 presents careful simulation studies with both synthetic and real data.
29	38	For any given subset Θ ⊂ Rd, let P(Θ) denote the space of Borel probability measures on Θ.
30	62	The Wasserstein space of order r ∈ [1,∞) of probability measures on Θ is de- fined as Pr(Θ) = { G ∈ P(Θ) : ´ ‖x‖rdG(x) < ∞ } , where ‖.‖ denotes Euclidean metric in Rd.
31	20	Addition- ally, for any k ≥ 1 the probability simplex is denoted by ∆k = { u ∈ Rk : ui ≥ 0, k∑ i=1 ui = 1 } .
32	152	Finally, let Ok(Θ) (resp., Ek(Θ)) be the set of probability measures with at most (resp., exactly) k support points in Θ. Wasserstein distances For any elements G and G′ in Pr(Θ) where r ≥ 1, the Wasserstein distance of order r between G and G′ is defined as (cf.
34	51	In words,W rr (G,G′) is the optimal cost of moving mass from G to G′, where the cost of moving unit mass is proportional to r-power of Euclidean distance in Θ.
35	14	When G and G′ are two discrete measures with finite number of atoms, fast computation of Wr(G,G ′) can be achieved (see, e.g., (Cuturi, 2013)).
37	49	By a recursion of concepts, we can speak of measures of measures, and define a suitable distance metric on this abstract space: the space of Borel measures on Pr(Θ), to be denoted by Pr(Pr(Θ)).
38	27	This is also a Polish space (that is, complete and separable metric space) as Pr(Θ) is a Polish space.
40	13	Section 3 of (Nguyen, 2016)): for any D,D′ ∈ Pr(Pr(Θ)) Wr(D,D′) := ( inf ˆ Pr(Θ)2 W rr (G,G ′)dπ(G,G′) )1/r where the infimum in the above ranges over all π ∈ Π(D,D′) such that Π(D,D′) is the set of all probability measures on Pr(Θ)×Pr(Θ) that has marginals D and D′.
41	12	In words, Wr(D,D′) corresponds to the optimal cost of moving mass from D to D′, where the cost of moving unit mass in its space of support Pr(Θ) is proportional to the r-power of the Wr distance in Pr(Θ).
45	15	, PN ∈ P2(Θ) for N ≥ 1, their Wasserstein barycenter PN,λ is such that PN,λ = arg min P∈P2(Θ) N∑ i=1 λiW 2 2 (P, Pi) (1) where λ ∈ ∆N denote weights associated with P1, .
49	14	The notion of Wasserstein barycenter has been utilized for approximate Bayesian inference (Srivastava et al., 2015).
59	100	This problem can be further thought of as a Wasserstein barycenter problem where N = 1.
60	12	In light of this observation, as noted by (Cuturi and Doucet, 2014), the algorithm for finding the Wasserstein barycenter offers an alternative for the popular Loyd’s algorithm for determing local minimum of the K-means objective.
61	35	Givenm groups of nj exchangeable data pointsXj,i where 1 ≤ j ≤ m, 1 ≤ i ≤ nj , i.e., data are presented in a two-level grouping structure, our goal is to learn about the two-level clustering structure of the data.
62	95	We want to obtain simultaneously local clusters for each data group, and global clusters among all groups.
72	37	The global clustering in the space of measures of measures on Θ can be succintly expressed by inf H∈EM (P2(Θ)) W 22 ( H, 1 m m∑ j=1 δGj ) .
74	41	MWM problem formulation We have arrived at an objective function for jointly optimizing over both local and global clusters inf Gj∈Okj (Θ), H∈EM (P2(Θ)) m∑ j=1 W 22 (Gj , P j nj ) +W 2 2 (H, 1 m m∑ j=1 δGj ).
76	59	The notable feature of MWM is that its loss function consists of two types of distances associated with the hierarchical data structure: one is distance in the space of measures, e.g., W 22 (Gj , P j nj ), and the other in space of measures of measures, e.g., W 22 (H, 1 m m∑ j=1 δGj ).
77	41	By adopting K-means optimization to both local and global clustering, the multilevel Wasserstein means problem might look formidable at the first sight.
