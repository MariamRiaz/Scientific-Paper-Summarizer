{"sections": [{"heading": "1. Introduction", "text": "Imperfect-information extensive-form games model strategic multi-step scenarios between agents with hidden information, such as auctions, security interactions (both physical and virtual), negotiations, and military situations. Typically in imperfect-information games, one wishes to find a Nash equilibrium, which is a profile of strategies in which no player can improve her outcome by unilaterally changing her strategy. A linear program can find an exact Nash equilibrium in two-player zero-sum games containing fewer than about 108 nodes (Gilpin & Sandholm, 2007). For larger games, iterative algorithms are used to converge to a Nash equilibrium. There are a number of such iterative algorithms (Heinrich et al., 2015; Nesterov, 2005; Hoda et al., 2010; Pays, 2014; Kroer et al., 2015), the most popular of which is Counterfactual Regret Minimization (CFR) (Zinkevich et al., 2007). CFR minimizes regret independently at each decision point in the game. CFR+, a variant of CFR,\n1Computer Science Department, Carnegie Mellon University, Pittsburgh, PA, USA. Correspondence to: Noam Brown <noamb@cs.cmu.edu>, Tuomas Sandholm <sandholm@cs.cmu.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nwas used to essentially solve Limit Texas Hold\u2019em, the largest imperfect-information game ever to be essentially solved (Bowling et al., 2015).\nBoth computation time and storage space are difficult challenges when solving large imperfect-information games. For example, solving Limit Texas Hold\u2019em required nearly 8 million core hours and a complex, domain-specific streaming compression algorithm to store the 262 TiB of uncompressed data in only 10.9 TiB. This data had to be repeatedly decompressed from disk into memory and then compressed back to disk in order to run CFR+ (Tammelin et al., 2015).\nIn certain situations, pruning can be applied to speed up the traversal of the game tree in iterative algorithms (Lanctot et al., 2009; Brown & Sandholm, 2015a; Brown et al., 2017). However, these past pruning techniques do not reduce the space needed to solve a game and lack theoretical guarantees for improved performance.\nIn this paper we introduce Best-Response Pruning (BRP)1, a new form of pruning for iterative algorithms such as CFR in large imperfect-information games. BRP leverages the fact that in iterative algorithms we are typically interested in performance against the opponent\u2019s average strategy over all iterations, and that the opponent\u2019s average strategy cannot change faster than a rate of 1t , where t is the number of iterations conducted so far. Thus, if part-way through a run one of our actions has done very poorly relative to other available actions against the opponent\u2019s average strategy, then after just a few more iterations the opponent\u2019s average strategy cannot change sufficiently for the poorly-performing action to now be doing well against the opponent\u2019s updated average strategy. In fact, we can bound how much an action\u2019s performance can improve over any number of iterations against the opponent\u2019s average strategy. So long as the upper bound on that performance is still not competitive with the other actions, then we can safely ignore the poorly-performing action.\nBRP provably reduces the computation time needed to solve imperfect-information games. Additionally, a primary advantage of BRP is that in addition to faster convergence,\n1Earlier versions of this paper referred to Best-Response Pruning as Total Regret-Based Pruning (Total RBP)\nit also reduces the space needed over time. Specifically, once pruning begins on a branch, BRP discards the memory allocated on that branch and does not reallocate the memory until pruning ends and the branch cannot immediately be pruned again. In Section 3.1, we prove that after enough iterations of CFR are completed, space for certain pruned branches will never need to be allocated again. Specifically, we prove that when using BRP it is asymptotically only necessary to store data for parts of the game that are reached with positive probability in a best response to a Nash equilibrium. This is extremely advantageous when solving large imperfect-information games, which are often constrained by space and in which the set of best response actions may be orders of magnitude smaller than the size of the game (Schmid et al., 2014).\nWhile BRP still requires enough memory to store the entire game in the early iterations, recent work has shown that these early iterations can be skipped in CFR, and possibly other iterative algorithms, by first solving a low-memory abstraction of the game and then using its solution to warm start CFR in the full game (Brown & Sandholm, 2016). BRP\u2019s reduction in space is also helpful to the Simultaneous Abstraction and Equilibrium Finding (SAEF) algorithm (Brown & Sandholm, 2015b), which starts CFR with a small abstraction of the game and progressively expands the abstraction while also solving the game. SAEF\u2019s space requirements increase the longer the algorithm runs, and may eventually exceed the constraints of a system. BRP can counter this increase in space by eliminating the need to store suboptimal paths of the game tree.\nBRP shares some similarities to the earlier pruning algorithm Regret-Based Pruning, which has shown empirical evidence of improving the performance of CFR. In contrast, this paper proves that CFR converges faster when using BRP, because suboptimal paths in the game tree will only need to be traversed O ( ln(T ) ) times over T iterations. We also prove that BRP uses asymptotically less space, while Regret-Based Pruning does not reduce the space needed to solve a game. Moreover, Best-Response Pruning easily generalizes to iterative algorithms beyond CFR such as Fictitious Play (Heinrich et al., 2015).\nThe magnitude of the gains in speed and space that BRP provides varies depending on the game. It is possible to construct games where BRP provides no benefit. However, if there are many suboptimal actions in the game\u2014as is frequently the case in large games\u2014BRP can speed up CFR by multiple orders of magnitude and require orders of magnitude less space. Our experiments show an order of magnitude space reduction already in medium-sized games, and a reduction factor increase with game size."}, {"heading": "2. Background", "text": "In a two-player zero-sum imperfect-information extensiveform game there are two players, P = {1, 2}. Let H be the set of all possible histories (nodes) in the game tree, represented as a sequence of actions. The actions available in a history is A(h) and the player who acts at that history is P (h) \u2208 P \u222a c, where c denotes chance. Chance plays an action a \u2208 A(h) with a fixed probability. The history h\u2032 reached after action a in h is a child of h, represented by h \u00b7 a = h\u2032, while h is the parent of h\u2032. More generally, h\u2032 is an ancestor of h (and h is a descendant of h\u2032), represented by h\u2032 @ h, if there exists a sequence of actions from h\u2032 to h. Z \u2286 H are terminal histories. For each player i \u2208 P , there is a payoff function ui : Z \u2192 < where u1 = \u2212u2. Define \u2206i = maxz\u2208Z ui(z)\u2212minz\u2208Z ui(z) and \u2206 = maxi \u2206i.\nImperfect information is represented by information sets for each player i \u2208 P by a partition Ii of h \u2208 H : P (h) = i. For any information set I \u2208 Ii, all histories h, h\u2032 \u2208 I are indistinguishable to player i, so A(h) = A(h\u2032). I(h) is the information set I where h \u2208 I . P (I) is the player i such that I \u2208 Ii. A(I) is the set of actions such that for all h \u2208 I , A(I) = A(h). |Ai| = maxI\u2208Ii |A(I)| and |A| = maxi |Ai|. Define U(I) to be the maximum payoff reachable from a history in I , and L(I) to be the minimum. That is, U(I) = maxz\u2208Z,h\u2208I:hvz uP (I)(z) and L(I) = minz\u2208Z,h\u2208I:hvz uP (I)(z). Define \u2206(I) = U(I) \u2212 L(I) to be the range of payoffs reachable from a history in I . Similarly U(I, a), L(I, a), and \u2206(I, a) are the maximum, minimum, and range of payoffs (respectively) reachable from a history in I after taking action a. Define D(I, a) to be the set of information sets reachable by player P (I) after taking action a. Formally, I \u2032 \u2208 D(I, a) if for some history h \u2208 I and h\u2032 \u2208 I \u2032, h \u00b7 a v h\u2032 and P (I) = P (I \u2032).\nA strategy \u03c3i(I) is a probability vector over A(I) for player i in information set I . The probability of a particular action a is denoted by \u03c3i(I, a). Since all histories in an information set belonging to player i are indistinguishable, the strategies in each of them must be identical. That is, for all h \u2208 I , \u03c3i(h) = \u03c3i(I) and \u03c3i(h, a) = \u03c3i(I, a). Define \u03c3i to be a probability vector for player i over all available strategies \u03a3i in the game. A strategy profile \u03c3 is a tuple of strategies, one for each player. ui(\u03c3i, \u03c3\u2212i) is the expected payoff for player i if all players play according to the strategy profile \u3008\u03c3i, \u03c3\u2212i\u3009. If a series of strategies are played over T iterations, then \u03c3\u0304Ti = \u2211 t\u2208T \u03c3 t i T . \u03c0\u03c3(h) = \u03a0h\u2032\u2192avh\u03c3P (h\u2032)(h \u2032, a) is the joint probability of reaching h if all players play according to \u03c3. \u03c0\u03c3i (h) is the contribution of player i to this probability (that is, the probability of reaching h if all players other than i, and chance, always chose actions leading to h). \u03c0\u03c3\u2212i(h) is the contribution of all players other than i, and chance. \u03c0\u03c3(h, h\u2032) is the\nprobability of reaching h\u2032 given that h has been reached, and 0 if h 6@ h\u2032. In a perfect-recall game, \u2200h, h\u2032 \u2208 I \u2208 Ii, \u03c0i(h) = \u03c0i(h\n\u2032). In this paper we focus on perfect-recall games. Therefore, for i = P (I) define \u03c0i(I) = \u03c0i(h) for h \u2208 I . Moreover, I \u2032 @ I if for some h\u2032 \u2208 I \u2032 and some h \u2208 I , h\u2032 @ h. Similarly, I \u2032 \u00b7 a @ I if h\u2032 \u00b7 a @ h. The average strategy \u03c3\u0304Ti (I) for an information set I is defined\nas \u03c3\u0304Ti (I) = \u2211 t\u2208T \u03c0 \u03c3ti i (I)\u03c3 t i(I)\u2211\nt\u2208T \u03c0 \u03c3t i (I)\n.\nA best response to \u03c3\u2212i is a strategy \u03c3\u2217i such that ui(\u03c3\n\u2217 i , \u03c3\u2212i) = max\u03c3\u2032i\u2208\u03a3i ui(\u03c3 \u2032 i, \u03c3\u2212i). A Nash equilibrium \u03c3\u2217 is a strategy profile where every player plays a best response: \u2200i, ui(\u03c3\u2217i , \u03c3\u2217\u2212i) = max\u03c3\u2032i\u2208\u03a3i ui(\u03c3 \u2032 i, \u03c3 \u2217 \u2212i). A Nash equilibrium strategy for player i as a strategy \u03c3i that is part of any Nash equilibrium. In two-player zero-sum games, if \u03c3i and \u03c3\u2212i are both Nash equilibrium strategies, then \u3008\u03c3i, \u03c3\u2212i\u3009 is a Nash equilibrium. An -equilibrium as a strategy profile \u03c3\u2217 such that \u2200i, ui(\u03c3\u2217i , \u03c3\u2217\u2212i) + \u2265 max\u03c3\u2032i\u2208\u03a3i ui(\u03c3 \u2032 i, \u03c3 \u2217 \u2212i)."}, {"heading": "2.1. Counterfactual Regret Minimization", "text": "Counterfactual Regret Minimization (CFR) is a popular algorithm for extensive-form games in which the strategy vector for each information set is determined according to a regret-minimization algorithm (Zinkevich et al., 2007). We use regret matching (RM) (Hart & Mas-Colell, 2000) as the regret-minimization algorithm, but the material presented in this paper also applies to other regret minimizing algorithms such as Hedge (Brown et al., 2017).\nThe analysis of CFR makes frequent use of counterfactual value. Informally, this is the expected utility of an information set given that player i tries to reach it. For player i at information set I given a strategy profile \u03c3, this is defined as\nv\u03c3(I) = \u2211 h\u2208I ( \u03c0\u03c3\u2212i(h) \u2211 z\u2208Z ( \u03c0\u03c3(h, z)ui(z) )) (1)\nThe counterfactual value of an action a is v\u03c3(I, a) = \u2211 h\u2208I ( \u03c0\u03c3\u2212i(h) \u2211 z\u2208Z ( \u03c0\u03c3(h \u00b7 a, z)ui(z) )) (2)\nA counterfactual best response (Moravcik et al., 2016) (CBR) is a strategy similar to a best response, except that it maximizes counterfactual value even at information sets that it does not reach due to its earlier actions. Specifically, a counterfactual best response to \u03c3\u2212i is a strategy CBR(\u03c3\u2212i) such that if CBR(\u03c3\u2212i)(I, a) > 0 then v\u3008CBR(\u03c3\u2212i),\u03c3\u2212i\u3009(I, a) = maxa\u2032 v\n\u3008CBR(\u03c3\u2212i),\u03c3\u2212i\u3009(I, a\u2032). The counterfactual best response value CBV \u03c3\u2212i(I) is similar to counterfactual value, except that player i = P (I) plays according to a CBR to \u03c3\u2212i. Formally, CBV \u03c3\u2212i(I) = v\u3008CBRi(\u03c3\u2212i),\u03c3\u2212i\u3009(I).\nLet \u03c3t be the strategy profile used on iteration t. The instantaneous regret on iteration t for action a in information set I is rt(I, a) = v\u03c3 t\n(I, a) \u2212 v\u03c3t(I) and the regret for action a in I on iteration T is RT (I, a) = \u2211 t\u2208T r\nt(I, a). Additionally, RT+(I, a) = max{RT (I, a), 0} and RT (I) = maxa{RT+(I, a)}. Regret for player i in the entire game is RTi = max\u03c3\u2032i\u2208\u03a3i \u2211 t\u2208T ( ui(\u03c3 \u2032 i, \u03c3 t \u2212i)\u2212 ui(\u03c3ti , \u03c3t\u2212i) ) .\nIn regret matching, a player picks a distribution over actions in an information set in proportion to the positive regret on those actions. Formally, on each iteration T + 1, player i selects actions a \u2208 A(I) according to probabilities\n\u03c3T+1(I, a) =  RT+(I,a)\u2211 a\u2032\u2208A(I) R T +(I,a \u2032) , if \u2211 a\u2032 R T +(I, a \u2032) > 0\n1 |A(I)| , otherwise\n(3) If a player plays according to RM on every iteration then on iteration T , RT (I) \u2264 \u2206(I) \u221a |A(I)| \u221a T .\nIf a player plays according to CFR in every iteration then RTi \u2264 \u2211 I\u2208Ii R T (I). So, as T \u2192 \u221e, R T i\nT \u2192 0. In two-player zero-sum games, if both players\u2019 average regret R T i\nT \u2264 , their average strategies \u3008\u03c3\u0304 T 1 , \u03c3\u0304 T 2 \u3009 form a 2 -\nequilibrium (Waugh et al., 2009). Thus, CFR constitutes an anytime algorithm for finding an -Nash equilibrium in zero-sum games."}, {"heading": "2.2. Prior Approaches to Pruning", "text": "This section reviews forms of pruning that allow parts of the game tree to be skipped in CFR. In vanilla CFR, the entire game tree is traversed separately for each player historyby-history. On each traversal, the regret for each action of a history\u2019s information set is updated based on the expected value for that action on that iteration, weighed by the probability of opponents taking actions to reach the history (that is, weighed by \u03c0\u03c3 t\n\u2212i(h)). However, if a history h is reached on iteration t in which \u03c0\u03c3 t\n\u2212i(h) = 0, then from (1) and (2) the strategy at h contributes nothing on iteration t to the regret of I(h) (or to the information sets above it). Moreover, any history that would be reached beyond h would also contribute nothing to its information set\u2019s regret because \u03c0\u03c3 t\n\u2212i(h \u2032) = 0 for every history h\u2032 where h @ h\u2032\nand P (h\u2032) = P (h). Thus, when traversing the game tree for player i, there is no need to traverse beyond any history h when \u03c0\u03c3 t\n\u2212i(h) = 0. The benefit of this form of pruning, which we refer to as partial pruning, varies depending on the game, but empirical results show a factor of 30 improvement in some games (Lanctot et al., 2009).\nWhile partial pruning allows one to prune paths that an opponent reaches with zero probability, Regret-Based Pruning allows one to also prune paths that the traverser reaches\nwith zero probability (Brown & Sandholm, 2015a). However, this pruning is necessarily temporary. Consider an action a \u2208 A(I) such that \u03c3t(I, a) = 0, and assume that it is known action a will not be played with positive probability until some far-future iteration t\u2032 (in RM, this would be the case ifRt(I, a) 0). Since action a is played with zero probability on iteration t, so from (1) the strategy played and reward received following action a (that is, in D(I, a)) will not contribute to the regret for any information set preceding action a on iteration t. In fact, what happens in D(I, a) has no bearing on the rest of the game tree until iteration t\u2032 is reached. So one could, in theory, \u201cprocrastinate\u201d in deciding what happened beyond action a on iteration t, t+1, ..., t\u2032\u22121 until iteration t\u2032.\nHowever, upon reaching iteration t\u2032, rather than individually making up the t\u2032 \u2212 t iterations over D(I, a), one can instead do a single iteration, playing against the average of the opponents\u2019 strategies in the t\u2032 \u2212 t iterations that were missed, and declare that strategy was played on all the t\u2032\u2212 t iterations. This accomplishes the work of the t\u2032\u2212t iterations in a single traversal. Moreover, since player i never plays action a with positive probability between iterations t and t\u2032, that means every other player can apply partial pruning on that part of the game tree for iterations t\u2032 \u2212 t, and skip it completely. This, in turn, means that player i has free rein to play whatever they want in D(I, a) without affecting the regrets of the other players. In light of that, and of the fact that player i gets to decide what is played in D(I, a) after knowing what the other players have played, player i might as well play a strategy that ensures zero regret for all information sets I \u2032 \u2208 D(I, a) in the iterations t to t\u2032. A CBR to the average of the opponent strategies on the t\u2032 \u2212 t iterations would qualify as such a zero-regret strategy.\nRegret-Based Pruning only allows a player to skip traversing D(I, a) for as long as \u03c3t(I, a) = 0. Thus, in RM, if Rt0(I, a) < 0, we can prune the game tree beyond action a from iteration t0 until iteration t1 so long as \u2211t0 t=1 v \u03c3t(I, a) + \u2211t1 t=t0+1 \u03c0\u03c3 t\n\u2212i(I)U(I, a) \u2264\u2211t1 t=1 v \u03c3t(I)."}, {"heading": "3. Best-Response Pruning", "text": "This section describes the behavior of BRP. In particular we focus on the case where BRP is applied to the most popular family of iterative algorithms, CFR. BRP begins pruning an action in an information set whenever playing perfectly beyond that action against the opponent\u2019s average strategy (that is, playing a CBR) still does worse than what has been achieved in the iterations played so far (that is,\u2211T t=1 v\n\u03c3t(I)). Pruning continues for the minimum number of iterations it could take for the opponent\u2019s average strategy to change sufficiently such that the pruning starting condition (that is, playing a CBR beyond the action against the\nopponent\u2019s average strategy does worse than what has been achieved in the iterations so far) no longer holds. When pruning ends, BRP calculates a CBR in the pruned branch against the opponent\u2019s average strategy over all iterations played so far, and sets regret in the pruned branch as if that CBR strategy were played on every iteration played in the game so far\u2014even those that were played before pruning began.\nWhile using a CBR works correctly when applying BRP to CFR, it is also sound to choose a strategy that is almost a CBR (formalized later in this section), as long as that strategy ensures \u2211 a\u2208A(I) ( RT+(I, a)\n)2 \u2264 (\u2206(I))2|A(I)|T . In practice, this means that the strategy is close to a CBR, and approaches a CBR as T \u2192\u221e. We now present the theory to show that such a near-CBR can be used. However, in practice CFR converges much faster than the theoretical bound, so the potential function is typically far lower than the theoretical bound. Thus, while choosing a near-CBR rather than an exact CBR may allow for slightly longer pruning according to the theory, it may actually result in worse performance. All of the theoretical results presented in this paper, including the improved convergence bound as well as the lower space requirements, still hold if only a CBR is used, and our experiments use a CBR. Nevertheless, clever algorithms for deciding on a near-CBR may lead to even better performance in practice.\nWe define a strategy \u03b2(\u03c3\u2212i, T ) as a T -near counterfactual best response (T -near CBR) to \u03c3\u2212i if for all I belonging to player i\n\u2211 a\u2208A(I) ( v\u3008\u03b2(\u03c3\u2212i,T ),\u03c3\u2212i\u3009(I, a)\u2212v\u3008\u03b2(\u03c3\u2212i,T ),\u03c3\u2212i\u3009(I) )2 + \u2264 x T I T 2 (4) where xTI can be any value in the range 0 \u2264 xTI \u2264( \u2206(I)\n)2|A(I)|T . If xTI = 0, then a T -near CBR is always a CBR. The set of strategies that are T -near CBRs to \u03c3\u2212i is represented as \u03a3\u03b2(\u03c3\u2212i, T ). We also define the T -near counterfactual best response value as \u03c8\u03c3\u2212i,T (I, a) = min\u03c3\u2032i\u2208\u03a3\u03b2(\u03c3\u2212i,T ) v\n\u3008\u03c3\u2032i,\u03c3\u2212i\u3009(I, a) and \u03c8\u03c3\u2212i,T (I) = min\u03c3\u2032i\u2208\u03a3\u03b2(\u03c3\u2212i,T ) v \u3008\u03c3\u2032i,\u03c3\u2212i\u3009(I).\nWhen applying BRP to CFR, an action is pruned only if it would still have negative regret had a T -near CBR against the opponent\u2019s average strategy been played on every iteration. Specifically, on iteration T of CFR with RM, if\nT ( \u03c8\u03c3\u0304 T \u2212i,T (I, a) ) \u2264 T\u2211 t=1 v\u03c3 t (I) (5)\nthen D(I, a) can be pruned for\nT \u2032 =\n\u2211T t=1 v \u03c3t(I)\u2212 \u03c8\u03c3\u0304 T \u2212i,T (I, a)\nU(I, a)\u2212 L(I) (6)\niterations. After those T \u2032 iterations are over, we calculate a T + T \u2032-near CBR in D(I, a) to the opponent\u2019s average strategy and set regret as if that T + T \u2032-near CBR had been played on every iteration. Specifically, for each t \u2264 T + T \u2032 we set2 v\u03c3 t (I, a) = \u03c8\u03c3\u0304 T+T \u2032 \u2212i ,T+T \u2032 (I, a) so that\nRT+T \u2032 (I, a) = ( T+T \u2032 )( \u03c8\u03c3\u0304 T+T \u2032 \u2212i ,T+T \u2032 (I, a) ) \u2212 T+T \u2032\u2211 t=1 v\u03c3 t (I) (7) and for every information set I \u2032 \u2208 D(I, a) we set v\u03c3 t (I \u2032, a\u2032) = \u03c8\u03c3\u0304 T+T \u2032 \u2212i ,T+T \u2032 (I \u2032, a\u2032) and v\u03c3 t (I \u2032) = \u03c8\u03c3\u0304 T+T \u2032 \u2212i ,T+T \u2032 (I \u2032) so that\nRT+T \u2032 (I \u2032, a\u2032) =( T + T \u2032 )( \u03c8\u03c3\u0304 T+T \u2032 \u2212i ,T+T \u2032 (I \u2032, a\u2032)\u2212 \u03c8\u03c3\u0304 T+T \u2032 \u2212i ,T+T \u2032 (I \u2032) ) (8)\nTheorem 1 proves that if (5) holds for some action, then the action can be pruned for T \u2032 iterations, where T \u2032 is defined in (6). The same theorem holds if one replaces the T -near counterfactual best response values with exact counterfactual best response values. The proof for Theorem 1 draws from recent work on warm starting CFR using only an average strategy profile (Brown & Sandholm, 2016). Essentially, we warm start regrets in the pruned branch using only the average strategy of the opponent and knowledge of T .\nTheorem 1. Assume T iterations of CFR with RM have been played in a two-player zero-sum game and assume T ( \u03c8\u03c3\u0304 T \u2212i,T (I, a) ) \u2264 \u2211T t=1 v \u03c3t(I) where P (I) = i. Let\nT \u2032 = b \u2211T t=1 v\n\u03c3t (I)\u2212T ( \u03c8 \u03c3\u0304T\u2212i,T (I,a) ) U(I,a)\u2212L(I) c. If both players\nplay according to CFR with RM for the next T \u2032 iterations in all information sets I \u2032\u2032 6\u2208 D(I, a) except that \u03c3(I, a) is set to zero and \u03c3(I) is renormalized, then\n(T + T \u2032) ( \u03c8\u03c3\u0304 T+T \u2032 \u2212i ,T+T \u2032 (I, a) ) \u2264 \u2211T+T \u2032 t=1 v \u03c3t(I). Moreover, if one then sets v\u03c3 t (I, a) = \u03c8\u03c3\u0304 T+T \u2032 \u2212i ,T+T \u2032 (I, a) for each t \u2264 T + T \u2032 and v\u03c3t(I \u2032, a\u2032) = \u03c8\u03c3\u0304 T+T \u2032 \u2212i ,T+T \u2032 (I \u2032, a\u2032) for each I \u2032 \u2208 D(I, a), then after T \u2032\u2032 additional iterations of CFR with RM, the bound on exploitability of \u03c3\u0304T+T \u2032+T \u2032\u2032 is no worse than having played T + T \u2032 + T \u2032\u2032 iterations of CFR with RM without BRP.\nIn practice, rather than check whether (5) is met for an action on every iteration, one could only check actions that\n2In practice, only the sums \u2211T t=1 v\n\u03c3t(I) and either\u2211T t=1 v \u03c3t(I, a) or RT (I, a) are stored.\nhave very negative regret, and do a check only once every several iterations. This would still be safe and would save some computational cost of the checks, but would lead to less pruning.\nSimilar to Regret-Based Pruning, the duration of pruning in BRP can be increased by giving up knowledge beforehand of exactly how many iterations can be skipped. From (2) and (1) we see that rT (I, a) \u2264 \u03c0\u03c3t\u2212i(I) ( U(I, a) \u2212 L(I) ) . Thus, if \u03c0\u03c3 t\n\u2212i(I) is very low, then (5) would continue to hold for more iterations than (6) guarantees. Specifically, we can prune D(I, a) from iteration t0 until iteration t1 as long as\nt0 ( \u03c8\u03c3\u0304 t0 \u2212i,t0(I, a) ) + t1\u2211 t=t0+1 \u03c0\u03c3 t \u2212i(I)U(I, a) \u2264 t1\u2211 t=1 v\u03c3 t (I)\n(9)"}, {"heading": "3.1. Best-Response Pruning Requires Less Space", "text": "A key advantage of BRP is that setting the new regrets according to (7) and (8) requires no knowledge of what the regrets were before pruning began. Thus, once pruning begins, all the regrets in D(I, a) can be discarded and the space that was allocated to storing the regret can be freed. That space need only be re-allocated once (9) ceases to hold and we cannot immediately begin pruning again (that is, (5) does not hold). Theorem 2 proves that for any information set I and action a \u2208 A(I) that is not part of a best response to a Nash equilibrium, there is an iteration TI,a such that for all T \u2265 TI,a, action a in information set I (and its descendants) can be pruned.3 Thus, once this TI,a is reached, it will never be necessary to allocate space for regret in D(I, a) again. Theorem 2. In a two-player zero-sum game, if for every opponent Nash equilibrium strategy \u03c3\u2217\u2212P (I), CBV \u03c3 \u2217 \u2212P (I)(I, a) < CBV \u03c3 \u2217 \u2212P (I)(I), then there exists a TI,a and \u03b4I,a > 0 such that after T \u2265 TI,a iterations of CFR, CBV \u03c3\u0304 T \u2212i(I, a)\u2212 \u2211T t=1 v \u03c3t (I)\nT \u2264 \u2212\u03b4I,a.\nWhile such a constant TI,a exists for any suboptimal action, BRP cannot determine whether or when TI,a is reached. Thus, it is still necessary to check whether (5) is satisfied whenever (9) no longer holds, and to recalculate how much longer D(I, a) can safely be pruned. This requires the algorithm to periodically calculate a best response (or near-best response) in D(I, a). However, this (near-)best response calculation does not require knowledge of regret in D(I, a),\n3If CFR converges to a particular Nash equilibrium, then this condition could be broadened to any information set I and action a \u2208 A(I) that is not a best response to that particular Nash equilibrium. While empirically CFR does appear to always converge to a particular Nash equilibrium, there is no known proof that it always does so.\nso it is still never necessary to store regret after iteration TI,a is reached.\nWhile it is possible to discard regrets in D(I, a) without penalty once pruning begins, regret is only half the space requirement of CFR. Every information set I also stores a sum of the strategies played \u2211T t=1 ( \u03c0\u03c3 t i (I)\u03c3 t(I) ) which is normalized once CFR ends in order to calculate \u03c3\u0304T (I). Fortunately, if action a in information set I is pruned for long enough, then the stored cumulative strategy in D(I, a) can also be discarded at the cost of a small increase in the distance of the final average strategy from a Nash equilibrium. Specifically, if \u03c0\u03c3\u0304 T\ni (I, a) \u2264 C\u221aT , where C is some constant, then setting \u03c3\u0304T (I, a) = 0 and renormalizing \u03c3\u0304T (I), and setting \u03c3\u0304T (I \u2032, a\u2032) = 0 for I \u2032 \u2208 D(I, a), can result in at most C|I|\u2206\u221a\nT higher exploitability for the whole strategy\n\u03c3\u0304T . Since CFR only guarantees that \u03c3\u0304T is a 2|I|\u2206 \u221a |A|\u221a\nT -\nNash equilibrium anyway, C|I|\u2206\u221a T\nis only a constant factor of the bound. If an action is pruned from T \u2032 to T , then\u2211T t=1 ( \u03c0\u03c3 t i (I)\u03c3 t(I, a) ) \u2264 T \u2032\nT . Thus, if an action is pruned for long enough, then eventually \u2211T t=1 ( \u03c0\u03c3 t i (I)\u03c3 t(I, a) ) \u2264 C\u221a T for any C, so \u2211T t=1 ( \u03c0\u03c3 t i (I)\u03c3 t(I, a) ) could be set to zero (as well as all descendants of I \u00b7 a), while suffering at most a constant factor increase in exploitability. As more iterations are played, this penalty will continue to decrease and eventually be negligible. The constant C can be set by the user: a higher C allows the average strategy to be discarded sooner, while a lower C reduces the potential penalty in exploitability.\nWe define IS as the set of information sets that are not guaranteed to be asymptotically pruned by Theorem 2. Specifically, I \u2208 IS if I 6\u2208 D(I \u2032, a\u2032) for some I \u2032 and a\u2032 \u2208 A(I \u2032) such that for every opponent Nash equilibrium strategy \u03c3\u2217\u2212P (I\u2032), CBV \u03c3\u2217\u2212P (I\u2032)(I \u2032, a\u2032) < CBV \u03c3 \u2217 \u2212P (I\u2032)(I \u2032). Theorem 2 implies the following.\nCorollary 1. In a two-player zero-sum game with some threshold on the average strategy C\u221a\nT for C > 0, after\na finite number of iterations CFR with BRP requires only O ( |IS ||A| ) space.\nUsing a threshold of CT rather than C\u221a T does not change the theoretical properties of the corollary, and may lead to faster convergence in some situations, but it may also result in a slower reduction in the space used by the algorithm (though the asymptotic space used is identical). In particular, if BRP can be extended to first-order methods that converge to an -Nash equilibrium in O( 1 ) iterations rather than O( 1 2 ) iterations, such as the Excessive Gap Technique (Hoda et al., 2010; Kroer et al., 2017), then a threshold of CT may be more appropriate when those algorithms are used. A threshold of C T may also be preferable when using an algorithm which\nempirically converges to an -Nash equilibrium in faster than O( 1 2 ) iterations, such as CFR+ on some games."}, {"heading": "3.2. Best-Response Pruning Converges Faster", "text": "We now prove that BRP in CFR speeds up convergence to an -Nash equilibrium. Section 3 proved that CFR with BRP converges in the same number of iterations as CFR alone. In this section, we prove that BRP allows each iteration to be traversed more quickly. Specifically, if an action a \u2208 A(I) is not a CBR to a Nash equilibrium, then D(I, a) need only be traversed O(ln(T )) times over T iterations. Intuitively, as both players converge to a Nash equilibrium, actions that are not a counterfactual best response will eventually do worse than actions that are, so those suboptimal actions will accumulate increasing amounts of negative regret. This negative regret allows the action to be safely pruned for increasingly longer periods of time.\nSpecifically, let S \u2286 H be the set of histories where h\u00b7a \u2208 S if h \u2208 S and action a is part of some CBR to some Nash equilibrium. Formally, S contains \u2205 and every history h \u00b7 a such that h \u2208 S and CBV \u03c3 \u2217 \u2212P (I)(I, a) = CBV \u03c3 \u2217 \u2212P (I)(I) for some Nash equilibrium \u03c3\u2217.\nTheorem 3. In a two-player zero-sum game, if both players choose strategies according to CFR with BRP, then conducting T iterations requires only O ( |S|T + |H| ln(T ) ) nodes to be traversed.\nThe definition of S uses properties of the Nash equilibria of the game, and an action a \u2208 A(I) not in S is only guaranteed to be pruned by BRP after some TI,a is reached, which also depends on the Nash equilibria of the game. Since CFR converges to only an -Nash equilibrium, CFR cannot determine with certainty which nodes are in S or when TI,a is reached. Nevertheless, both S and TI,a are fixed properties of the game."}, {"heading": "4. Experiments", "text": "We compare the convergence speed of BRP to Regret-Based Pruning, to only partial pruning, and to no pruning at all. We also show that BRP uses less space as as more iterations are conducted, unlike prior pruning algorithms. The experiments are conducted on Leduc Hold\u2019em (Southey et al., 2005) and Leduc-5 (Brown & Sandholm, 2015a). Leduc Hold\u2019em is a common benchmark in imperfect-information game solving because it is small enough to be solved but still strategically complex. In Leduc Hold\u2019em, there is a deck consisting of six cards: two each of Jack, Queen, and King. There are two rounds. In the first round, each player places an ante of 1 chip in the pot and receives a single private card. A round of betting then takes place with a two-bet maximum, with Player 1 going first. A public shared card is then dealt face up and another round of betting takes place.\nAgain, Player 1 goes first, and there is a two-bet maximum. If one of the players has a pair with the public card, that players wins. Otherwise, the player with the higher card wins. The bet size in the first round is 2 chips, and 4 chips in the second round. Leduc-5 is like Leduc Hold\u2019em but larger: there are 5 bet sizes to choose from. In the first round a player may bet 0.5, 1, 2, 4, or 8 chips, while in the second round a player may bet 1, 2, 4, 8, or 16 chips.\nNodes touched is a hardware and implementationindependent proxy for time which we use to measure performance of the various algorithms. Overhead costs are counted in nodes touched. CFR+ is a variant of CFR in which a floor on regret is set at zero and each iteration is weighted linearly in the average strategy (that is, iteration t is weighted by t) rather than each iteration being weighted equally. Since Regret-Based Pruning can only prune negative-regret actions, Regret-Based Pruning modifies the definition of CFR+ so that regret can be negative, but immediately jumps up to zero as soon as regret increases. BRP does not require this modification. Still, BRP also modifies the behavior of CFR+ because without pruning, CFR+ would put positive probability on an action as soon as its regret increases, while BRP waits until pruning is over. This is not, by itself, a problem. However, CFR+\u2019s linear weighting of the average strategy is only guaranteed to converge to a Nash equilibrium if pruning does not occur. While both Regret-Based Pruning and BRP do well empirically with CFR+, the convergence is noisy. This noise can be reduced by using the lowest-exploitability average strategy profile found so far, which we do in the experiments.4 BRP does not do as well empirically with the linear-averaging component of CFR+. Thus, for BRP we only measure performance using RM+ with CFR, which is the same as CFR+ but without linear averaging. CFR+ with and without linear averaging has the same theoretical performance as CFR, but CFR+ does better empirically (particularly with linear averaging).\nFigure 1 and Figure 2 show the reduction in space needed to store the average strategy and regrets for BRP\u2014for various values of the constant threshold C, where an action\u2019s probability is set to zero if it is reached with probability less than C\u221a\nT in the average strategy, as we explained in\nSection 3.1. In both games, a threshold between 0.01 and 0.1 performed well in both space and number of iterations, with the lower thresholds converging somewhat faster and the higher thresholds reducing space faster. We also tested thresholds below 0.01, but the speed of convergence was essentially the same as when using 0.01. In Leduc, all variants resulted in a quick drop-off in space to about half the initial\n4Exploitability is no harder to compute than one iteration of CFR or CFR+. Snapshots are not plotted at every iteration but only after every 10,000,000 nodes touched\u2014except for the first few snapshots.\namount. In Leduc-5, a threshold of 0.1 resulted in about a factor of 7 reduction for both CFR with RM and CFR with RM+. This space reduction factor appears to continue to increase.\nFigure 3 and Figure 4 compare the convergence rates of BRP, Regret-Based Pruning, and only partial pruning for CFR with RM, CFR with RM+, and CFR+. In Leduc, BRP and Regret-Based Pruning perform comparably when added to CFR. Regret-Based Pruning with CFR+ does significantly better, while BRP with CFR using RM+ sees no improve-\nment over BRP with CFR. In Leduc-5, which is a far larger game, BRP outperforms Regret-Based Pruning by a factor of 2 when added to CFR. BRP with CFR using RM+ also performs comparably to Regret-Based Pruning with CFR+, while retaining theoretical guarantees and not suffering from noisy convergence."}, {"heading": "5. Conclusions", "text": "We introduced BRP, a new form of pruning that provably reduces both the space needed to solve an imperfectinformation game and the time needed to reach an -Nash equilibrium. This addresses both of the major bottlenecks in solving large imperfect-information games. Experimentally, BRP reduced the space needed to solve a game by a factor of 7, with the reduction factor increasing with game size. While the early iterations may still be slow and require the same amount of space as CFR without BRP, these early iterations can be skipped by warm starting CFR with an abstraction of the game (Brown & Sandholm, 2016). This paper focused on the theory of BRP when applied to CFR, the most popular algorithm for solving imperfectinformation games. However, BRP can also be applied to Fictitious Play (Heinrich et al., 2015) and likely extends to other iterative algorithms as well (Hoda et al., 2010)."}, {"heading": "6. Acknowledgments", "text": "This material is based on work supported by the National Science Foundation under grant IIS-1617590 and the ARO under award W911NF-17-1-0082."}], "year": 2017, "references": [{"title": "Heads-up limit hold\u2019em poker is solved", "authors": ["Bowling", "Michael", "Burch", "Neil", "Johanson", "Tammelin", "Oskari"], "venue": "Science,", "year": 2015}, {"title": "Regret-based pruning in extensive-form games", "authors": ["Brown", "Noam", "Sandholm", "Tuomas"], "venue": "In Advances in Neural Information Processing Systems, pp. 1972\u20131980,", "year": 2015}, {"title": "Simultaneous abstraction and equilibrium finding in games", "authors": ["Brown", "Noam", "Sandholm", "Tuomas"], "venue": "In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI),", "year": 2015}, {"title": "Strategy-based warm starting for regret minimization in games", "authors": ["Brown", "Noam", "Sandholm", "Tuomas"], "venue": "In AAAI Conference on Artificial Intelligence (AAAI),", "year": 2016}, {"title": "Dynamic thresholding and pruning for regret minimization", "authors": ["Brown", "Noam", "Kroer", "Christian", "Sandholm", "Tuomas"], "venue": "In AAAI Conference on Artificial Intelligence (AAAI),", "year": 2017}, {"title": "Lossless abstraction of imperfect information games", "authors": ["Gilpin", "Andrew", "Sandholm", "Tuomas"], "venue": "Journal of the ACM,", "year": 2007}, {"title": "A simple adaptive procedure leading to correlated", "authors": ["Hart", "Sergiu", "Mas-Colell", "Andreu"], "venue": "equilibrium. Econometrica,", "year": 2000}, {"title": "Fictitious self-play in extensive-form games", "authors": ["Heinrich", "Johannes", "Lanctot", "Marc", "Silver", "David"], "venue": "In International Conference on Machine Learning (ICML),", "year": 2015}, {"title": "Smoothing techniques for computing Nash equilibria of sequential games", "authors": ["Hoda", "Samid", "Gilpin", "Andrew", "Pe\u00f1a", "Javier", "Sandholm", "Tuomas"], "venue": "Mathematics of Operations Research,", "year": 2010}, {"title": "Faster first-order methods for extensive-form game solving", "authors": ["Kroer", "Christian", "Waugh", "Kevin", "K\u0131l\u0131n\u00e7-Karzan", "Fatma", "Sandholm", "Tuomas"], "venue": "In Proceedings of the ACM Conference on Economics and Computation (EC),", "year": 2015}, {"title": "Theoretical and practical advances on smoothing for extensive-form games", "authors": ["Kroer", "Christian", "Waugh", "Kevin", "K\u0131l\u0131n\u00e7-Karzan", "Fatma", "Sandholm", "Tuomas"], "venue": "In Proceedings of the ACM Conference on Economics and Computation", "year": 2017}, {"title": "Monte Carlo sampling for regret minimization in extensive games", "authors": ["Lanctot", "Marc", "Waugh", "Kevin", "Zinkevich", "Martin", "Bowling", "Michael"], "venue": "In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS),", "year": 2009}, {"title": "Refining subgames in large imperfect information games", "authors": ["Moravcik", "Matej", "Schmid", "Martin", "Ha", "Karel", "Hladik", "Milan", "Gaukrodger", "Stephen"], "venue": "In AAAI Conference on Artificial Intelligence (AAAI),", "year": 2016}, {"title": "Excessive gap technique in nonsmooth convex minimization", "authors": ["Nesterov", "Yurii"], "venue": "SIAM Journal of Optimization,", "year": 2005}, {"title": "An interior point approach to large games of incomplete information", "authors": ["Pays", "Fran\u00e7ois"], "venue": "In AAAI Computer Poker Workshop,", "year": 2014}, {"title": "Bounding the support size in extensive form games with imperfect information", "authors": ["Schmid", "Martin", "Moravcik", "Matej", "Hladik", "Milan"], "venue": "In AAAI Conference on Artificial Intelligence (AAAI),", "year": 2014}, {"title": "Solving heads-up limit texas hold\u2019em", "authors": ["Tammelin", "Oskari", "Burch", "Neil", "Johanson", "Michael", "Bowling"], "venue": "In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI),", "year": 2015}, {"title": "Abstraction pathologies in extensive games", "authors": ["Waugh", "Kevin", "Schnizlein", "David", "Bowling", "Michael", "Szafron", "Duane"], "venue": "In International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS),", "year": 2009}, {"title": "Regret minimization in games with incomplete information", "authors": ["Zinkevich", "Martin", "Johanson", "Michael", "Bowling", "Michael H", "Piccione", "Carmelo"], "venue": "In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS),", "year": 2007}], "id": "SP:839b3a7ee1aff9812206118c393b6a9b385174a2", "authors": [{"name": "Noam Brown", "affiliations": []}, {"name": "Tuomas Sandholm", "affiliations": []}], "abstractText": "Iterative algorithms such as Counterfactual Regret Minimization (CFR) are the most popular way to solve large zero-sum imperfect-information games. In this paper we introduce Best-Response Pruning (BRP), an improvement to iterative algorithms such as CFR that allows poorly-performing actions to be temporarily pruned. We prove that when using CFR in zero-sum games, adding BRP will asymptotically prune any action that is not part of a best response to some Nash equilibrium. This leads to provably faster convergence and lower space requirements. Experiments show that BRP results in a factor of 7 reduction in space, and the reduction factor increases with game size.", "title": "Reduced Space and Faster Convergence in Imperfect-Information Games via Pruning"}