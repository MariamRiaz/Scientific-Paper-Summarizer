0	51	Existing semantic parsing approaches for building natural language interfaces to databases (NLIDBs) either use special-purpose intermediate meaning representations that lack the full expressivity of database query languages or require extensive feature engineering, making it difficult to deploy them in new domains.
1	45	We present a robust approach to quickly and easily learn and deploy semantic parsers from scratch, whose performance Work done partly during an internship at the Allen Institute for Artificial Intelligence.
2	15	improves over time based on user feedback, and requires very little expert intervention.
3	172	To learn these semantic parsers, we (1) adapt neural sequence models to map utterances directly to SQL thereby bypassing intermediate representations and taking full advantage of SQL’s querying capabilities, (2) immediately deploy the model online to solicit questions and user feedback on results to reduce SQL annotation efforts, and (3) use crowd workers from skilled markets to provide SQL annotations that can directly be used for model improvement, in addition to being easier and cheaper to obtain than logical meaning representations.
5	35	This type of interactive learning is related to a number of recent ideas in semantic parsing, in- 963 cluding batch learning of models that directly produce programs (e.g., regular expressions (Locascio et al., 2016)), learning from paraphrases (often gathered through crowdsourcing (Wang et al., 2015)), data augmentation (e.g. based on manually engineered semantic grammars (Jia and Liang, 2016)) and learning through direct interaction with users (e.g., where a single user teaches the model new concepts (Wang et al., 2016)).
36	19	Our neural model N is initially trained on synthetic data T generated by domain-independent schema templates (see Section 4), and is then ready to answer new user questions, n. The results R of executing the predicted SQL query q are presented to the user who provides a binary correct/incorrect feedback signal.
43	21	We use an encoder-decoder model with global attention, similar to Luong et al. (2015), where the anonymized utterance (see Section 4.2) is encoded using a bidirectional LSTM network, then decoded to directly predict SQL query tokens.
45	26	The decoder predicts a conditional probability distribution over possible values for the next SQL token given the previous tokens using a combination of the previous SQL token embedding, attention over the hidden states of the encoder network, and an attention signal from the previous time step.
69	17	PPDB contains over 220 million paraphrase pairs divided into 6 sets (small to XXXL) based on precision of the paraphrases.
84	19	ATIS has the longest utterances and queries, with an average utterance length of 11 words and an average SQL query length of 67 tokens.
100	15	On both datasets, our SQL model achieves reasonably high accuracies approaching that of the best non-SQL results.
105	21	More importantly, it can be immediately deployed for users in new domains, with a large programming community available for annotation, and thus, fits effectively into a framework for interactive learning.
107	16	However, unlike in the interactive experiments (Section 6), data augmentation using schema templates does not improve performance in the fully supervised setting.
111	18	We developed a web interface for accepting natural language questions to an academic database from users, using our model to generate a SQL query, and displaying the results after execution.
115	33	Collecting accurate user feedback on predicted queries is a key challenge in the interactive learning setting for two reasons.
117	16	Second, it can be difficult for users to determine if the presented results are in fact correct.
120	23	The first assist is type highlighting, which highlights entities identified in the utterance, for example, “paper by Michael I. Jordan (AUTHOR) in ICRA (VENUE) in 2016 (YEAR).” This assist is especially helpful because the academic database contains noisy keyword and dataset tables that were automatically extracted from the papers.
121	16	The second assist is utterance paraphrasing, which shows the user another utterance that maps to the same SQL query.
123	48	Using these assists and the predicted results, users are asked to select from five feedback options: Correct, Wrong Types, Incomplete Result, Wrong Result and Can’t Tell.
127	170	Can’t Tell indicates that the user is unsure about the feedback to provide.
128	37	In this experiment, using our developed user interface, we use Algorithm 1 to learn a semantic parser from scratch.
129	15	The experiment had three stages; in each stage, we recruited 10 new users (computer science graduate students) and asked them to issue at least 10 utterances each to the system and to provide feedback on the results.
130	29	We considered results marked as either Correct or Incomplete Result as correct queries for learning.
132	37	The crowd worker had prior experience in writing SQL queries and was hired from Upwork after completing a short SQL test.
133	30	The worker was also given access to the database to be able to execute the queries and ensure that they are correct.
135	33	The complexity of the utterances issued in each of the three phases were comparable, in that, the average length of the correct SQL query for the utterances, and the number of tables required to be queried, were similar.
136	20	Table 5 shows the percent of utterances judged by users as either Correct or Incomplete Result in each stage.
137	26	In the first stage, we do not have any labeled examples, and the model is trained using only synthetically generated data from schema templates and paraphrases (see Section 4.3).
139	28	The system’s accuracy increases and annotation effort decreases in each successive stage as additional utterances are contributed and incorrect utterances are labeled.
140	39	This result demonstrates that we can successfully build semantic parsers for new domains by using neural models to generate SQL with crowdsourced annotations driven by user feedback.
