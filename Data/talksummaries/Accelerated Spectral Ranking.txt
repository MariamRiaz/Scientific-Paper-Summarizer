0	49	The problem of rank aggregation from pairwise or multiway comparisons is a fundamental one in machine learning with applications in recommendation systems, sports, social choice etc.
2	17	These scores can further be used to produce a ranking over these items.
3	66	For example, in recommendation systems, the goal might be to learn a ranking over items by observing the choices that users make when presented with different subsets of these items; in sports, the goal might be to rank teams/individuals at the end of a tournament based on pairwise or multiway games between these individuals/teams; in social choice, the goal might be to aggregate the choices of individuals when presented with different alternatives such as candidates in an election.
4	17	In the case of pairwise comparisons, a popular model is the Bradley-Terry-Luce (BTL) model (Bradley & Terry, 1952; Luce, 1959) which posits that given a set of n items, there is a positive weight wi associated with each item i, and the probability that i is preferred over j in a pairwise comparison between i and j is wiwi+wj .
7	24	Rank aggregation under pairwise comparisons has been an active area of research, and several algorithms have been proposed that are consistent under the BTL model (Negahban et al., 2017; Rajkumar & Agarwal, 2014; Hunter, 2004; Chen & Suh, 2015; Jang et al., 2016; Guiver & Snelson, 2009; Soufiani et al., 2013).
8	23	The case of multiway comparisons has also received some attention recently (Maystre & Grossglauser, 2015; Jang et al., 2017; Chen et al., 2017).
42	39	We consider a setting where there are n items, and one observes outcomes of noisy pairwise or multiway comparisons between these items.
43	23	We will assume that the outcome of these comparisons is generated according to the multinomial logit (MNL) model, which posits that each item i ∈ [n] is associated with a (unknown) weight/score wi > 0, and the probability that item i wins a comparison is proportional to its weight wi.
48	27	The comparison data is of the following form: there are d different comparison sets S1, · · · , Sd ⊆ [n], with |Sa| = m for all a ∈ [d] and some constant m < n. For each set Sa, for a ∈ [d], one observes the outcomes of L independent m-way comparisons between items in Sa, drawn according to the MNL model.
50	18	We will denote by yla the winner of the l-th comparison amongst items of Sa, for l ∈ [L] and a ∈ [d].
54	16	In the following sections, we will present an algorithm for recovering an estimate ŵ of w, and give bounds on the error ‖ŵ−w‖TV in terms of the problem parameters under natural assumptions on the comparison data.
57	29	The key idea is to construct the random walk such that the probability of transition from node i to node j is proportional to wj .
58	65	If wj is larger than wi, then with other quantities being equal, one would expect the random walk to spend more time in node j than node i in its steady state distribution.
61	18	These algorithms construct a random walk whose stationary distribution, in expectation, is exactly w. However, this construction forces their Markov chain to have self loops with large mass, slowing down the convergence rate.
76	19	Formally, define p̂i|Sa to be the fraction of times that i won a m-way comparison amongst items in the Algorithm 1 ASR Input Markov chain P̂ according to Eq.
78	36	Let us then define a random walk where the probability of transition from node i ∈ [n] to node j ∈ [n] is given by P̂ij := 1 di ∑ a∈[d]:i,j∈Sa p̂j|Sa .
79	21	One can again verify that P̂ corresponds to a valid transition probability matrix.
85	88	The algorithm computes the stationary distribution π̂ of the Markov chain P̂ using the power method.2 It then outputs the (normalized) vector ŵ that is obtained after applying the linear transform D−1 to π̂, i.e. ŵ = D −1π̂ ‖D−1π̂‖1 .
90	42	We first begin by showing that for any given comparison data Y, both RC/LSR and our algorithm will return the same estimate upon convergence.
91	23	Given items [n] and comparison data Y = {(Sa,ya)}da=1, let π̂ be the stationary distribution of the Markov chain P̂ constructed by ASR, and let ŵLSR be the stationary distribution of the Markov chain P̂LSR.
92	37	The same result is also true for ŵ RC for the case of pairwise comparisons.
107	24	The above lemma states that the mixing time of a Markov chain X is inversely proportional to its spectral gap µ(X).
135	17	Given items [n] and comparison data Y = {(Sa,ya)}da=1, let each set Sa of cardinality m be compared L times, with outcomes ya = (y1a, · · · , yLa ) produced as per a MNL model with parameters w = (w1, .
136	31	(2)) on the comparison graph Gc([n], E) induced by the comparison data Y is strongly connected, then the ASR algorithm (Algorithm 1) converges to a unique distribution ŵ, which with probability ≥ 1 − 3n−(C2−50)/25 satisfies the following error bound3 ‖w − ŵ‖TV ≤ C κdavg µ(P) dmin √ max{m, log(n)} L , where κ = log ( davg dminwmin ) , wmin = mini∈[n] wi, davg =∑ i∈[n] widi, dmin = mini∈[n] di, µ(P) is the spectral gap of the random walk P (Eq.
154	19	In this case, the above corollary implies a sample complexity bound of O ( ξ−2 n poly(log n) ) , which is sometimes referred to as quasi-linear complexity.
167	19	For the general case of multiway comparisons we are not aware of any other sample complexity bounds.
209	32	Results on synthetic data: L1 error vs. number of iterations for our algorithm, ASR, compared with the RC algorithm (for m = 2) and the LSR algorithm (for m = 5), on data generated from the MNL/BTL model with the random and star graph topologies.
219	16	These plots verify the mixing time analysis of Section 4, and show that our algorithm converges much faster than RC and LSR, and orders of magnitude faster in the case of the star graph.
227	53	We observe that our algorithm converges rapidly to the peak log-likelihood value while RC and LSR are always slower in converging to this value.
231	35	It would be interesting to see if one can use our algorithm to give better guarantees for recovery of top-k items under MNL.
