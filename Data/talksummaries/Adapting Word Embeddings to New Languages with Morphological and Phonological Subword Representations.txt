1	23	While the training of these word vectors does not rely on explicit human supervision, their quality is highly contingent on the size and quality of the unlabeled corpora available.
3	37	Disheartening though this high dependence on resources sounds, several efforts (Adams et al., 2017; Haghighi et al., 2008; Bharadwaj et al., 2016; Mayhew et al., 2017) have shown considerable performance gains across different tasks in the low resource setting by transferring knowledge from related high-resource languages.
4	11	Most existing approaches for learning cross-lingual word embeddings (Ruder, 2017) either extend the monolingual objective function by adding a cross-lingual regularization objective which is then jointly optimized or use mapping-based approaches to align similar words across languages.
6	57	In this paper, we take a different task: focusing instead on the similarity of the surface forms, phonology, or morphology of the two transfer languages.
7	57	Specifically, inspired by Ling et al. (2015), who demonstrate the effectiveness of character-level modeling for knowledge sharing in multilingual scenarios, we propose two approaches to transfer word embeddings using different types of linguistically-inspired subword-level information.
9	14	We explore the effect of different subword units— characters, lemmas, inflectional properties, and phonemes— as each one offers a unique linguistic insight, discussed more in Section 3.
10	12	Our proposed approaches do require language specific resources, but importantly do not depend on crosslingual resources and achieve considerable performance gains over existing methods which do.
20	23	We demonstrate that training embeddings on character-based phonemic representations presents substantial performance advantages over training on orthographic characters in some transfer settings, e.g. when there are script differences across languages.
21	64	These advantages are in addition to those from morphological representations (lemmas and morphological properties).
24	28	We also release morphological analyzers for Hindi and Bengali3.
25	44	The two most popular training objectives for monolingual word embeddings are the skipgram and continuous-bag-of-words (CBOW), introduced by Mikolov et al. (2013a).
26	33	The skipgram tree/master/embeddings_released  model attempts to predict the context surrounding a word, given the word itself whereas CBOW predicts the word given its context.
31	16	Negative sampling represents the above objective function (Equation 1) using a binary logistic loss as shown below: 𝑇 ∑ 𝑖=1 ( ∑𝑤𝑐∈𝐶𝑖 𝑙(𝑠(𝑤𝑖, 𝑤𝑐)) + ∑ 𝑤𝑛∈𝑁𝑖 𝑙(−𝑠(𝑤𝑖, 𝑤𝑛))) (3) where 𝑁𝑖 are the negative words sampled randomly from vocabulary and 𝑙 is the log-sigmoid function.
33	23	Mikolov et al. (2013b)’s model fails to capture internal structure of words and does not generalize for out of vocabulary words that may share morphemes with in-vocabulary words.
39	36	Avraham and Goldberg (2017) explicitly model lemmas (stems or citation forms) and morphological properties (the sets of which are sometimes called “tags”) for training the word embeddings.
58	11	This is achieved simply by combining the corpora of both the high-resource and the low-resource language and training jointly using the skip-gram objective, discussed above.
62	19	The model attempts this by taking the learned continuous representations of the high resource subword units, referred to by x𝐻𝑖𝑆𝑊𝑈, and uses them to initialize the model for the low resource language.
89	14	Subword units are initialized with uniform samples from [ −1𝑑𝑖𝑚 , 1 𝑑𝑖𝑚] where 𝑑𝑖𝑚 = 100.
92	32	For comparison, we train multilingual embeddings using MultiCCA (Ammar et al., 2016) as our baseline.
98	57	For NER, we also compare with Bharadwaj et al. (2016) who use a neural attention model over phonological features and report the best performance for Turkish using transfer from Uzbek and Uyghur, and Mayhew et al. (2017) who use a cheap translation method to translate training data from high-resource language into the lowresource language and report best NER results for Uyghur, as part of the LORELEI program.
108	11	Monolingual experiments on all four languages: Uyghur, Turkish, Bengali and Hindi.
138	102	We get +5.8 F1 points for Turkish, +4.8 F1 for Uyghur, +0.8 F1 for Hindi and +0.7 F1 for Bengali over the existing methods.
149	11	We base our analysis on the unsequestered set since annotations for full test data are not released.
151	30	One obvious advantage of jointly training with a resource-rich corpus is that coverage of NEs increases, as validated in our case where jointly training with Turkish corpus adds 114 more NEs.
161	27	One likely reason that the combination of characterngrams and lemmas consistently show the best performance is that, together, they capture lexical similarity, which is more important to translation than the syntactic information captured by morphological inflection (“morph”).
163	86	We hypothesize that this is because the MT models were trained on a training set that did not have translation pairs from the high resource language.
164	17	As Qi et al. (2018) note, when training MT systems on a single language pair, it is less necessary for the embeddings to be coordinated across the languages.
176	63	In this paper, we explored two simple methods for cross-lingual transfer, both of which are taskindependent and use transfer learning for leveraging subword information from resource-rich languages, especially through phonological and morphological representations.
177	80	CT-Joint and CTFineTune do not require morphological analyzers, but we have found that even a morphological analyzer built in 2-3 weeks can boost performance and is a worthwhile investment of resources.
178	19	Preliminary evaluation on a separate task of MT reconfirms the utility of subword units and further research will reveal what these learned subword representations can contribute to other tasks.
