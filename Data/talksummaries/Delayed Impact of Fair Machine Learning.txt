2	29	Job advertisements allocate opportunity.
4	39	Existing scholarship on fairness in automated decisionmaking criticizes unconstrained machine learning for its potential to harm historically underrepresented or disadvantaged groups in the population (Executive Office of the President, 2016; Barocas & Selbst, 2016).
5	49	Consequently, a variety of fairness criteria have been proposed as constraints on standard learning objectives.
10	38	There are two groups in the population with features described by a summary statistic, such as a credit score, whose distribution differs between the two groups.
11	24	The bank can choose thresholds for each group at which loans are offered.
15	48	A successful lending outcome leads to profit for the bank and also to an increase in credit score for the borrower.
16	16	When thinking of one of the two groups as disadvantaged, it makes sense to ask what lending policies (choices of thresholds) lead to an expected improvement in the score distribution within that group.
18	37	One frequently proposed fairness criterion, sometimes called demographic parity, requires the bank to lend to both groups at an equal rate.
26	28	We introduce a one-step feedback model that allows us to quantify the long-term impact of classification on different groups in the population.
82	52	Lastly, we assume that the success of an individual is independent of their group given the score; that is, the score summarizes all relevant information about the success event, so there exists a function ρ : X → [0, 1] such that individuals of score x succeed with probability ρ(x).
83	20	We introduce the specific domain of credit scores as a running example in the rest of the paper.
85	33	In the setting of loans, scores x ∈ [C] represent credit scores, and the bank serves as the institution.
88	48	The expected utility to the bank is given by the expected return from a loan, which can be modeled as an affine function of ρ(x): u(x) = u+ρ(x) + u−(1 − ρ(x)), where u+ denotes the profit when loans are repaid and u− the loss when they are defaulted on.
90	82	The constant c+ > 0 denotes the gain in credit score if loans are repaid and c− < 0 is the score penalty in case of default.
91	63	We now introduce important outcome regimes, stated in terms of the change in average group score.
94	50	Under our model, MaxUtil policies can be chosen in a standard fashion which applies the same threshold τ MaxUtil for both groups, and is agnostic to the distributions πA and πB.
95	25	Hence, if we define ∆µMaxUtilj := ∆µj(τ MaxUtil) (3) we say that a policy causes relative harm to group j if ∆µj(τ j) < ∆µ MaxUtil j , and relative improvement if ∆µj(τ j) > ∆µ MaxUtil j .
102	24	To explicitly connect selection rates to decision policies, we define the rate function rπ(τ j) which returns the proportion of group j selected by the policy.
108	24	We will consider policies that maximize the institution’s total expected utility, potentially subject to a constraint: τ ∈ C ∈ [0, 1]2C which enforces some notion of “fairness”.
112	65	The demographic parity (DemParity) policy results in equal selection rates between both groups.
138	15	In the utility function (1), the contributions of each group are weighted by their population proportions gj, and thus the resulting selection rates are sensitive to these proportions.
143	24	In particular, when β = β0, DemParity causes active harm, and when β = β, DemParity causes relative harm.
213	37	Individuals were labeled as defaulted if they failed to pay a debt for at least 90 days on at least one account in the ensuing 18-24 month period; we use this data to estimate the success probability given score, ρj(x), which we allow to vary by group to match the empirical data.
216	27	We model individual penalties as a score drop of c− = −150 in the case of a default, and in increase of c+ = 75 in the case of successful repayment.
220	28	To plot the MaxUtil utility curves, the group that is not on display has selection rate fixed at βMaxUtil.
227	19	However, DemParity (dashed green line) causes a negative expected credit score change in the black group, corresponding to active harm.
228	26	For the white group, the bank utility curve has almost the same shape under the fairness criteria as it does under MaxUtil, the main difference being that fairness criteria lowers the total expected profit from this group.
231	43	We remark that in other settings where the unconstrained profit maximization is misaligned with individual outcomes (e.g., when u−u+ = −10), fairness criteria may perform more favorably for the minority group by pulling the utility curve into a shape consistent with the outcome curve.
232	24	By analyzing the resulting effects of MaxUtil, DemParity, and EqOpt on actual credit score lending data, we show the applicability of our model to real-world applications.
240	35	This is consistent with much scholarship that points to the context-sensitive nature of fairness in machine learning.
