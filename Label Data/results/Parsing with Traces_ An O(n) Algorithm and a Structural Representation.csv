0,1,label2,summary_sentences
"Deep neural networks achieve near-human accuracy on many perception tasks (He et al., 2016; Amodei et al., 2015).",1. Introduction,[0],[0]
"However, they lack robustness to small alterations of the inputs at test time (Szegedy et al., 2014).",1. Introduction,[0],[0]
"Indeed when presented with a corrupted image that is barely distinguishable from a legitimate one by a human, they can predict incorrect labels, with high-confidence.",1. Introduction,[0],[0]
"An adversary can design such so-called adversarial examples, by adding a small perturbation to a legitimate input to maximize the likelihood of an incorrect class under constraints on the magnitude of the perturbation (Szegedy et al., 2014; Goodfellow et al., 2015; Moosavi-Dezfooli et al., 2015; Pa-
1Facebook AI Research.",1. Introduction,[0],[0]
"Correspondence to: Moustapha Cisse <moustaphacisse@fb.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"pernot et al., 2016a).",1. Introduction,[0],[0]
"In practice, for a significant portion of inputs, a single step in the direction of the gradient sign is sufficient to generate an adversarial example (Goodfellow et al., 2015) that is even transferable from one network to another one trained for the same problem but with a different architecture (Liu et al., 2016; Kurakin et al., 2016).
",1. Introduction,[0],[0]
The existence of transferable adversarial examples has two undesirable corollaries.,1. Introduction,[0],[0]
"First, it creates a security threat for production systems by enabling black-box attacks (Papernot et al., 2016a).",1. Introduction,[0],[0]
"Second, it underlines the lack of robustness of neural networks and questions their ability to generalize in settings where the train and test distributions can be (slightly) different as is the case for the distributions of legitimate and adversarial examples.
",1. Introduction,[0.9520246664088203],"['One advantage of their algorithm is that by introducing a new item type it can handle some cases of the Locked-Chain we define below (specifically, when N is even), though in practise they also restrict their algorithm to ignore such cases.']"
"Whereas the earliest works on adversarial examples already suggested that their existence was related to the magnitude of the hidden activations gradient with respect to their inputs (Szegedy et al., 2014), they also empirically assessed that standard regularization schemes such as weight decay or training with random noise do not solve the problem (Goodfellow et al., 2015; Fawzi et al., 2016).",1. Introduction,[0],[0]
The current mainstream approach to improving the robustness of deep networks is adversarial training.,1. Introduction,[0],[0]
"It consists in generating adversarial examples on-line using the current network’s parameters (Goodfellow et al., 2015; Miyato et al., 2015; Moosavi-Dezfooli et al., 2015; Szegedy et al., 2014; Kurakin et al., 2016) and adding them to the training data.",1. Introduction,[0.955327761952831],['We extend and improve previous work on lexicalized constituent representations (Shen et al. 2007; Carreras et al. 2008; Hayashi and Nagata 2016) to handle traces.']
"This data augmentation method can be interpreted as a robust optimization procedure (Shaham et al., 2015).
",1. Introduction,[0],[0]
"In this paper, we introduce Parseval networks, a layerwise regularization method for reducing the network’s sensitivity to small perturbations by carefully controlling its global Lipschitz constant.",1. Introduction,[0],[0]
"Since the network is a composition of functions represented by its layers, we achieve increased robustness by maintaining a small Lipschitz constant (e.g., 1) at every hidden layer; be it fully-connected, convolutional or residual.",1. Introduction,[0],[0]
"In particular, a critical quantity governing the local Lipschitz constant in both fully connected and convolutional layers is the spectral norm of the weight matrix.",1. Introduction,[0],[0]
"Our main idea is to control this norm by parameterizing the network with parseval tight frames (Kovačević & Chebira, 2008), a generalization of orthogonal matrices.
",1. Introduction,[0],[0]
"The idea that regularizing the spectral norm of each weight
matrix could help in the context of robustness appeared as early as (Szegedy et al., 2014), but no experiment nor algorithm was proposed, and no clear conclusion was drawn on how to deal with convolutional layers.",1. Introduction,[0],[0]
"Previous work, such as double backpropagation (Drucker & Le Cun, 1992) has also explored jacobian normalization as a way to improve generalization.",1. Introduction,[0],[0]
Our contribution is twofold.,1. Introduction,[0],[0]
"First, we provide a deeper analysis which applies to fully connected networks, convolutional networks, as well as Residual networks (He et al., 2016).",1. Introduction,[0],[0]
"Second, we propose a computationally efficient algorithm and validate its effectiveness on standard benchmark datasets.",1. Introduction,[0],[0]
"We report results on MNIST, CIFAR-10, CIFAR-100 and Street View House Numbers (SVHN), in which fully connected and wide residual networks were trained (Zagoruyko & Komodakis, 2016) with Parseval regularization.",1. Introduction,[0],[0]
"The accuracy of Parseval networks on legitimate test examples matches the state-of-the-art, while the results show notable improvements on adversarial examples.",1. Introduction,[0],[0]
"Besides, Parseval networks train significantly faster than their vanilla counterpart.
",1. Introduction,[0],[0]
"In the remainder of the paper, we first discuss the previous work on adversarial examples.",1. Introduction,[0],[0]
"Next, we give formal definitions of the adversarial examples and provide an analysis of the robustness of deep neural networks.",1. Introduction,[0],[0]
"Then, we introduce Parseval networks and its efficient training algorithm.",1. Introduction,[0],[0]
Section 5 presents experimental results validating the model and providing several insights.,1. Introduction,[0],[0]
"Early papers on adversarial examples attributed the vulnerability of deep networks to high local variations (Szegedy et al., 2014; Goodfellow et al., 2015).",2. Related work,[0],[0]
"Some authors argued that this sensitivity of deep networks to small changes in their inputs is because neural networks only learn the discriminative information sufficient to obtain good accuracy rather than capturing the true concepts defining the classes (Fawzi et al., 2015; Nguyen et al., 2015).
",2. Related work,[0],[0]
"Strategies to improve the robustness of deep networks include defensive distillation (Papernot et al., 2016b), as well as various regularization procedures such as contractive networks (Gu & Rigazio, 2015).",2. Related work,[0],[0]
"However, the bulk of recent proposals relies on data augmentation (Goodfellow et al., 2015; Miyato et al., 2015; Moosavi-Dezfooli et al., 2015; Shaham et al., 2015; Szegedy et al., 2014; Kurakin et al., 2016).",2. Related work,[0],[0]
It uses adversarial examples generated online during training.,2. Related work,[0],[0]
"As we shall see in the experimental section, regularization can be complemented with data augmentation; in particular, Parseval networks with data augmentation appear more robust than either data augmentation or Parseval networks considered in isolation.",2. Related work,[0],[0]
"We consider a multiclass prediction setting, where we have Y classes in Y = {1, ..., Y }.",3. Robustness in Neural Networks,[0],[0]
"A multiclass classifier is a function ĝ : (x ∈ RD,W ∈ W) 7→ argmaxȳ∈Y",3. Robustness in Neural Networks,[0],[0]
"gȳ(x,W ), where W are the parameters to be learnt, and gȳ(x,W ) is the score given to the (input, class) pair (x, ȳ) by a function g :",3. Robustness in Neural Networks,[0],[0]
RD × W → RY .,3. Robustness in Neural Networks,[0],[0]
"We take g to be a neural network, represented by a computation graph G = (N , E), which is a directed acyclic graph with a single root node, and each node n ∈ N takes values in Rd (n) out and is a function of its children in the graph, with learnable parameters W (n):
n : x 7→ φ(n) ( W (n), ( n′(x) )",3. Robustness in Neural Networks,[0],[0]
"n′:(n,n′)∈E ) .",3. Robustness in Neural Networks,[0],[0]
"(1)
The function g we want to learn is the root of G. The training data ((xi, yi))mi=1 ∈",3. Robustness in Neural Networks,[0],[0]
"(X × Y)m is an i.i.d. sample of D, and we assume X ⊂",3. Robustness in Neural Networks,[0],[0]
RD is compact.,3. Robustness in Neural Networks,[0],[0]
"A function ` : RY × Y → R measures the loss of g on an example (x, y); in a single-label classification setting for instance, a common choice for ` is the log-loss:
` ( g(x,W ), y ) = −gy(x,W ) + log (∑ ȳ∈Y egȳ(x,W ) ) .",3. Robustness in Neural Networks,[0],[0]
"(2)
The arguments that we develop below depend only on the Lipschitz constant of the loss, with respect to the norm of interest.",3. Robustness in Neural Networks,[0],[0]
"Formally, we assume that given a p-norm of interest ‖.‖p, there is a constant λp such that
∀z, z′ ∈ RY ,∀ȳ ∈ Y, |`(z, ȳ)−`(z′, ȳ)| ≤",3. Robustness in Neural Networks,[0],[0]
"λp‖z−z′‖p .
",3. Robustness in Neural Networks,[0],[0]
"For the log-loss of (2), we have λ2 ≤ √
2 and λ∞ ≤ 2.",3. Robustness in Neural Networks,[0],[0]
"In the next subsection, we define adversarial examples and the generalization performance of the classifier.",3. Robustness in Neural Networks,[0],[0]
"Then, we make the relationship between robustness to adversarial examples and the lipschitz constant of the networks.",3. Robustness in Neural Networks,[0],[0]
"Given an input (train or test) example (x, y), an adversarial example is a perturbation of the input pattern x̃ = x",3.1. Adversarial examples,[0],[0]
"+ δx where δx is small enough so that x̃ is nearly undistinguishable from x (at least from the point of view of a human annotator), but has the network predict an incorrect label.",3.1. Adversarial examples,[0],[0]
"Given the network parameters and structure g(.,W ) and a p-norm, the adversarial example is formally defined as
x̃ = argmax x̃:‖x̃−x‖p≤
` ( g(x̃,W ), y ) , (3)
where represents the strength of the adversary.",3.1. Adversarial examples,[0],[0]
"Since the optimization problem above is non-convex, Shaham et al. (2015) propose to take the first order taylor expansion of x 7→ `(g(x,W ), y) to compute δx by solving
x̃ = argmax x̃:‖x̃−x‖p≤
( ∇x`(g(x,W ), y) )",3.1. Adversarial examples,[0],[0]
T (x̃− x) .,3.1. Adversarial examples,[0],[0]
"(4)
If p = ∞, then x̃ = x + sign(∇x`(g(x,W ), y)).",3.1. Adversarial examples,[0],[0]
This is the fast gradient sign method.,3.1. Adversarial examples,[0],[0]
"For the case p = 2, we obtain x̃ = x + ∇x`(g(x,W ), y).",3.1. Adversarial examples,[0],[0]
"A more involved method is the iterative fast gradient sign method, in which several gradient steps of (4) are performed with a smaller stepsize to obtain a local minimum of (3).",3.1. Adversarial examples,[0],[0]
"In the context of adversarial examples, there are two different generalization errors of interest:
L(W )",3.2. Generalization with adversarial examples,[0],[0]
"= E (x,y)∼D
[ `(g(x,W ), y) ] ,
Ladv(W,p, ) =",3.2. Generalization with adversarial examples,[0],[0]
"E (x,y)∼D
[ max
x̃:‖x̃−x‖p≤ `(g(x̃,W ), y)
] .
",3.2. Generalization with adversarial examples,[0],[0]
"By definition, L(W )",3.2. Generalization with adversarial examples,[0],[0]
"≤ Ladv(W,p, ) for every p and >0.",3.2. Generalization with adversarial examples,[0],[0]
"Reciprocally, denoting by λp and Λp the Lipschitz constant (with respect to ‖.‖p) of ` and g respectively, we have:
Ladv(W,p, ) ≤ L(W )",3.2. Generalization with adversarial examples,[0],[0]
+,3.2. Generalization with adversarial examples,[0],[0]
"E
(x,y)∼D
[ max
x̃:‖x̃−x‖p≤ |`(g(x̃,W ), y)− `(g(x,W ), y)| ] ≤ L(W ) +",3.2. Generalization with adversarial examples,[0],[0]
"λpΛp .
",3.2. Generalization with adversarial examples,[0],[0]
This suggests that the sensitivity to adversarial examples can be controlled by the Lipschitz constant of the network.,3.2. Generalization with adversarial examples,[0],[0]
"In the robustness framework of (Xu & Mannor, 2012), the Lipschitz constant also controls the difference between the average loss on the training set and the generalization performance.",3.2. Generalization with adversarial examples,[0],[0]
"More precisely, let us denote by Cp(X , γ) the covering number of X using γ-balls for ‖.‖p.",3.2. Generalization with adversarial examples,[0],[0]
"Using M = supx,W,y `(g(x,W ), y), Theorem 3 of (Xu & Mannor, 2012) implies that for every δ ∈ (0, 1), with probability 1− δ over the i.i.d. sample ((xi, yi)mi=1, we have:
L(W )",3.2. Generalization with adversarial examples,[0],[0]
≤ 1 m m∑ i=1,3.2. Generalization with adversarial examples,[0],[0]
"`(g(xi,W ), yi)
+ λpΛpγ",3.2. Generalization with adversarial examples,[0],[0]
"+M
√ 2Y Cp(X , γ2 )",3.2. Generalization with adversarial examples,[0],[0]
"ln(2)− 2 ln(δ)
m .
",3.2. Generalization with adversarial examples,[0],[0]
"Since covering numbers of a p-norm ball in RD increases exponentially with RD, the bound above suggests that it is critical to control the Lipschitz constant of g, for both good generalization and robustness to adversarial examples.",3.2. Generalization with adversarial examples,[0],[0]
"From the network structure we consider (1), for every node n ∈",3.3. Lipschitz constant of neural networks,[0],[0]
"N , we have (see below for the definition of Λ(n,n ′) p ):
",3.3. Lipschitz constant of neural networks,[0],[0]
"‖n(x)− n(x̃)‖p ≤ ∑
n′:(n,n′)∈E
Λ(n,n ′) p ‖n′(x)− n′(x̃)‖p ,
for any Λ(n,n ′)
p that is greater than the worst case variation of n with respect to a change in its input n′(x).",3.3. Lipschitz constant of neural networks,[0],[0]
"In particular we can take for Λ(n,n ′)",3.3. Lipschitz constant of neural networks,[0],[0]
"p any value greater than the
supremum over x0 ∈ X of the Lipschitz constant for ‖.‖p of the function (1n′′ = n′ is 1 if n′′ = n′ and 0 otherwise):
x 7→ φ(n) ( W (n), ( n′′(x0+1n ′′ = n′(x−x0)) )",3.3. Lipschitz constant of neural networks,[0],[0]
"n′′:(n,n′′)∈E ) .
",3.3. Lipschitz constant of neural networks,[0],[0]
"The Lipschitz constant of n, denoted by Λ(n)p satisfies:
Λ(n)p ≤ ∑
n′:(n,n′)∈E
Λ(n,n ′) p Λ (n′) p (5)
",3.3. Lipschitz constant of neural networks,[0],[0]
"Thus, the Lipschitz constant of the network g can grow exponentially with its depth.",3.3. Lipschitz constant of neural networks,[0],[0]
"We now give the Lipschitz constants of standard layers as a function of their parameters:
Linear layers: For layer n(x) = W (n)n′(x) where n′ is the unique child of n in the graph, the Lipschitz constant for ‖.‖p is, by definition, the matrix norm of W (n) induced by ‖.‖p, which is usually denoted ‖W (n)‖p and defined by
‖W (n)‖p = sup z:‖z‖p=1 ‖W (n)z‖p .
",3.3. Lipschitz constant of neural networks,[0],[0]
"Then Λ(n)2 = ‖W (n)‖2Λ (n′) 2 , where ‖W (n)‖2, called the spectral norm of W (n), is the maximum singular value of W (n).",3.3. Lipschitz constant of neural networks,[0],[0]
"We also have Λ(n)∞ = ‖W (n)‖∞Λ(n ′) ∞ , where
‖W (n)‖∞ = maxi ∑ j |W (n)",3.3. Lipschitz constant of neural networks,[0],[0]
ij,3.3. Lipschitz constant of neural networks,[0],[0]
| is the maximum 1-norm of the rows.,3.3. Lipschitz constant of neural networks,[0],[0]
"W (n).
",3.3. Lipschitz constant of neural networks,[0],[0]
"Convolutional layers: To simplify notation, let us consider convolutions on 1D inputs without striding, and we take the width of the convolution to be 2k + 1 for k ∈ N. To write convolutional layers in the same way as linear layers, we first define an unfolding operator U , which prepares the input z, denoted by U(z).",3.3. Lipschitz constant of neural networks,[0],[0]
"If the input has length T with din inputs channels, the unfolding operator maps z",3.3. Lipschitz constant of neural networks,[0],[0]
"For a convolution of the unfolding of z considered as a T × (2k + 1)din matrix, its j-th column is:
Uj(z) =",3.3. Lipschitz constant of neural networks,[0],[0]
"[zj−k; ...; zj+k] ,
where “;” is the concatenation along the vertical axis (each zi is seen as a column din-dimensional vector), and zi",3.3. Lipschitz constant of neural networks,[0],[0]
= 0,3.3. Lipschitz constant of neural networks,[0],[0]
if i is out of bounds (0-padding).,3.3. Lipschitz constant of neural networks,[0],[0]
"A convolutional layer with dout output channels is then defined as
n(x) =",3.3. Lipschitz constant of neural networks,[0],[0]
"W (n) ∗ n′(x) = W (n)U(n′(x)) ,
where W (n) is a dout × (2k + 1)din matrix.",3.3. Lipschitz constant of neural networks,[0],[0]
"We thus have Λ
(n) 2 ≤ ‖W‖2‖U(n′(x))‖2.",3.3. Lipschitz constant of neural networks,[0],[0]
"Since U is a linear operator that essentially repeats its input (2k + 1) times, we have ‖U(n′(x))",3.3. Lipschitz constant of neural networks,[0],[0]
− U(n′(x̃))‖22 ≤ (2k + 1)‖n′(x),3.3. Lipschitz constant of neural networks,[0],[0]
"− n′(x̃)‖22, so that Λ(n)2 ≤ √ 2k + 1‖W‖2Λ(n ′) 2 .",3.3. Lipschitz constant of neural networks,[0],[0]
"Also, ‖U(n′(x))",3.3. Lipschitz constant of neural networks,[0],[0]
"− U(n′(x̃))‖∞ = ‖n′(x) − n′(x̃)‖∞, and so for a convolutional layer, Λ(n)∞ ≤ ‖W (n)‖∞Λ(n ′) ∞ .
",3.3. Lipschitz constant of neural networks,[0],[0]
"Aggregation layers/transfer functions: Layers that perform the sum of their inputs, as in Residual Netowrks (He et al., 2016), fall in the case where the values Λ(n,n ′) p in (5) come into play.",3.3. Lipschitz constant of neural networks,[0],[0]
"For a node n that sums its inputs, we have Λ (n,n′) p = 1, and thus Λ (n) p ≤ ∑ n′:(n,n′)∈E Λ (n′) p .",3.3. Lipschitz constant of neural networks,[0],[0]
"If n is a tranfer function layer (e.g., an element-wise application of ReLU)",3.3. Lipschitz constant of neural networks,[0],[0]
"we can check that Λ(n)p ≤ Λ(n ′) p , where n′ is the input node, as soon as the Lipschitz constant of the transfer function (as a function R→ R) is ≤ 1.",3.3. Lipschitz constant of neural networks,[0],[0]
"Parseval regularization, which we introduce in this section, is a regularization scheme to make deep neural networks robust, by constraining the Lipschitz constant (5) of each hidden layer to be smaller than one, assuming the Lipschitz constant of children nodes is smaller than one.",4. Parseval networks,[0],[0]
"That way, we avoid the exponential growth of the Lipschitz constant, and a usual regularization scheme (i.e., weight decay) at the last layer then controls the overall Lipschitz constant of the network.",4. Parseval networks,[0],[0]
"To enforce these constraints in practice, Parseval networks use two ideas: maintaining orthonormal rows in linear/convolutional layers, and performing convex combinations in aggregation layers.",4. Parseval networks,[0],[0]
"Below, we first explain the rationale of these constraints and then describe our approach to efficiently enforce the constraints during training.",4. Parseval networks,[0],[0]
"Orthonormality of weight matrices: For linear layers, we need to maintain the spectral norm of the weight matrix at 1.",4.1. Parseval Regularization,[0],[0]
Computing the largest singular value of weight matrices is not practical in an SGD setting unless the rows of the matrix are kept orthogonal.,4.1. Parseval Regularization,[0],[0]
"For a weight matrix W ∈ Rdout×din with dout ≤ din, Parseval regularization maintains WTW",4.1. Parseval Regularization,[0],[0]
"≈ Idout×dout , where I refers to the identity matrix.",4.1. Parseval Regularization,[0],[0]
"W is then approximately a Parseval tight frame (Kovačević & Chebira, 2008), hence the name of Parseval networks.",4.1. Parseval Regularization,[0],[0]
"For convolutional layers, the matrix W ∈ Rdout×(2k+1)din is constrained to be a Parseval tight frame (with the notations of the previous section), and the output is rescaled by a factor (2k + 1)−1/2.",4.1. Parseval Regularization,[0],[0]
"This maintains all singular values of W to (2k+ 1)−1/2, so that Λ
(n) 2 ≤ Λ (n′) 2 where n
′ is the input node.",4.1. Parseval Regularization,[0],[0]
"More generally, keeping the rows of weight matrices orthogonal makes it possible to control both the spectral norm and the ‖.‖∞ of a weight matrix through the norm of its individual rows.",4.1. Parseval Regularization,[0],[0]
Robustness for ‖.‖∞ is achieved by rescaling the rows so that their 1-norm is smaller than 1.,4.1. Parseval Regularization,[0],[0]
"For now, we only experimented with constraints on the 2-norm of the rows, so we aim for robustness in the sense of ‖.‖2.
",4.1. Parseval Regularization,[0],[0]
Remark 1 (Orthogonality is required).,4.1. Parseval Regularization,[0],[0]
"Without orthogonality, constraints on the 2-norm of the rows of weight ma-
trices are not sufficient to control the spectral norm.",4.1. Parseval Regularization,[0],[0]
"Parseval networks are thus fundamentally different from weight normalization (Salimans & Kingma, 2016).
",4.1. Parseval Regularization,[0],[0]
Aggregation Layers:,4.1. Parseval Regularization,[0],[0]
"In parseval networks, aggregation layers do not make the sum of their inputs, but rather take a convex combination of them:
n(x) = ∑
n′:(n,n′)∈E
α(n,n ′)n′(x)
with ∑ n′:(n,n′)∈E α (n,n′) = 1 and α(n,n ′) ≥ 0.",4.1. Parseval Regularization,[0],[0]
"The parameters α(n,n ′) are learnt, but using (5), these constraint guarantee that Λ(n)p ≤ 1 as soon as the children satisfy the inequality for the same p-norm.",4.1. Parseval Regularization,[0],[0]
Orthonormality constraints: The first significant difference between Parseval networks and its vanilla counterpart is the orthogonality constraint on the weight matrices.,4.2. Parseval Training,[0],[0]
"This requirement calls for an optimization algorithm on the manifold of orthogonal matrices, namely the Stiefel manifold.",4.2. Parseval Training,[0],[0]
"Optimization on matrix manifolds is a well-studied topic (see (Absil et al., 2009) for a comprehensive survey).",4.2. Parseval Training,[0],[0]
The simplest first-order geometry approaches consist in optimizing the unconstrained function of interest by moving in the direction of steepest descent (given by the gradient of the function) while at the same time staying on the manifold.,4.2. Parseval Training,[0],[0]
"To guarantee that we remain in the manifold after every parameter update, we need to define a retraction operator.",4.2. Parseval Training,[0],[0]
"There exist several pullback operators for embedded submanifolds such as the Stiefel manifold based for example on Cayley transforms (Absil et al., 2009).",4.2. Parseval Training,[0],[0]
"However, when learning the parameters of neural networks, these methods are computationally prohibitive.",4.2. Parseval Training,[0],[0]
"To overcome this difficulty, we use an approximate operator derived from the following layer-wise regularizer of weight matrices to ensure their parseval tightness (Kovačević & Chebira, 2008):
Rβ(Wk) =",4.2. Parseval Training,[0],[0]
"β
2 ‖W>k Wk − I‖22.
Optimizing Rβ(Wk) to convergence after every gradient descent step (w.r.t the main objective) guarantees us to stay on the desired manifold but this is an expensive procedure.",4.2. Parseval Training,[0],[0]
"Moreover, it may result in parameters that are far from the ones obtained after the main gradient update.",4.2. Parseval Training,[0],[0]
"We use two approximations to make the algorithm more efficient: First, we only do one step of descent on the function Rα(Wk).",4.2. Parseval Training,[0],[0]
The gradient of this regularization term is∇WkRβ(Wk) =,4.2. Parseval Training,[0],[0]
β(WkW > k − I)Wk.,4.2. Parseval Training,[0],[0]
"Consequently, after every main update we perform the following secondary update:
Wk ← (1 + β)Wk",4.2. Parseval Training,[0],[0]
"− βWkW>k Wk.
Algorithm 1 Parseval Training Θ = {Wk,αk}Kk=1, e← 0",4.2. Parseval Training,[0],[0]
"while e ≤ E do
Sample a minibatch {(xi, yi)}Bi=1.",4.2. Parseval Training,[0],[0]
"for k ∈ {1, . . .",4.2. Parseval Training,[0],[0]
",K} do
Compute the gradient:",4.2. Parseval Training,[0],[0]
"GWk ← ∇Wk`(Θ, {(xi, yi)}), Gαk ← ∇αk`(Θ, {(xi, yi)}).",4.2. Parseval Training,[0],[0]
Update the parameters: Wk ←Wk − ·GWk αk ← αk − ·Gαk .,4.2. Parseval Training,[0],[0]
"if hidden layer then
Sample a subset S of rows of Wk.",4.2. Parseval Training,[0],[0]
Projection: WS ← (1 + β)WS,4.2. Parseval Training,[0],[0]
− βWSW>S WS .,4.2. Parseval Training,[0],[0]
αk ← argminγ∈∆K−1‖αK,4.2. Parseval Training,[0],[0]
"− γ‖22
e← e+ 1.
Optionally, instead of updating the whole matrix, one can randomly select a subset S of rows and perform the update from Eq.",4.2. Parseval Training,[0],[0]
(4.2) on the submatrix composed of rows indexed by S. This sampling based approach reduces the overall complexity to O(|S|2d).,4.2. Parseval Training,[0],[0]
"Provided the rows are carefully sampled, the procedure is an accurate Monte Carlo approximation of the regularizer loss function (Drineas et al., 2006).",4.2. Parseval Training,[0],[0]
"The optimal sampling probabilities, also called statistical leverages are approximately equal if we start from an orthogonal matrix and (approximately) stay on the manifold throughout the optimization since they are proportional to the eigenvalues of W (Mahoney et al., 2011).",4.2. Parseval Training,[0],[0]
"Therefore, we can sample a subset of columns uniformly at random when applying this projection step.
",4.2. Parseval Training,[0],[0]
"While the full update does not result in an increased overhead for convolutional layers, the picture can be very different for large fully connected layers making the sampling approach computationally more appealing for such layers.",4.2. Parseval Training,[0],[0]
We show in the experiments that the weight matrices resulting from this procedure are (quasi)-orthogonal.,4.2. Parseval Training,[0],[0]
"Also, note that quasi-orthogonalization procedures similar to the one described here have been successfully used previously in the context of learning overcomplete representations with independent component analysis (Hyvärinen & Oja, 2000).
",4.2. Parseval Training,[0],[0]
"Convexity constraints in aggregation layers: In Parseval networks, aggregation layers output a convex combination of their inputs instead of e.g., their sum as in Residual networks (He et al., 2016).",4.2. Parseval Training,[0],[0]
"For an aggregation node n of the network, let us denote by α = (α(n,n
′))n′:(n,n′)∈E the K-size vector of coefficients used for the convex combination output by the layer.",4.2. Parseval Training,[0],[0]
"To ensure that the Lipschitz constant at the node n is such that Λ(n)p ≤ 1, the constraints of 4.1 call for a euclidean projection of α onto the positive simplex after a gradient update:
α∗ = argmin γ∈∆K−1 ‖α− γ‖22 ,
where ∆K−1 = {γ ∈ RK |1>γ = 1,γ ≥ 0}.",4.2. Parseval Training,[0],[0]
"This is a well studied problem (Michelot, 1986; Pardalos & Kovoor, 1990; Duchi et al., 2008; Condat, 2016).",4.2. Parseval Training,[0],[0]
"Its solution is of the form: α∗i = max(0, αi − τ(α)), with τ : RK → R the unique function satisfying ∑ i(xi − τ(α))",4.2. Parseval Training,[0],[0]
= 1 for every x ∈ RK .,4.2. Parseval Training,[0],[0]
"Therefore, the solution essentially boils down to a soft thresholding operation.",4.2. Parseval Training,[0],[0]
If we denote α1 ≥ α2 ≥ . . .,4.2. Parseval Training,[0],[0]
"αK the sorted coefficients and k(α) = max{k ∈ (1, . . .",4.2. Parseval Training,[0],[0]
",K)|1+kαk > ∑ j≤k αj}, the optimal thresholding is given by (Duchi et al., 2008):
τ(α) =",4.2. Parseval Training,[0],[0]
"( ∑ j≤k(α) αj)− 1 k(α)
",4.2. Parseval Training,[0],[0]
"Consequently, the complexity of the projection is O(K log(K))",4.2. Parseval Training,[0],[0]
since it is only dominated by the sorting of the coefficients and is typically cheap because aggregation nodes will only have few children in practice (e.g. 2).,4.2. Parseval Training,[0],[0]
"If the number of children is large, there exist efficient linear time algorithms for finding the optimal thresholding τ(α) (Michelot, 1986; Pardalos & Kovoor, 1990; Condat, 2016).",4.2. Parseval Training,[0],[0]
"In this work, we use the method detailed above (Duchi et al., 2008) to perform the projection of the coefficient α after every gradient update step.",4.2. Parseval Training,[0],[0]
"We evaluate the effectiveness of Parseval networks on well-established image classification benchmark datasets namely MNIST, CIFAR-10, CIFAR-100 (Krizhevsky, 2009) and Street View House Numbers (SVHN) (Netzer et al.).",5. Experimental evaluation,[0],[0]
We train both fully connected networks and wide residual networks.,5. Experimental evaluation,[0],[0]
"The details of the datasets, the models, and the training routines are summarized below.",5. Experimental evaluation,[0],[0]
CIFAR.,5.1. Datasets,[0],[0]
Each of the CIFAR datasets is composed of 60K natural scene color images of size 32 × 32 split between 50K training images and 10K test images.,5.1. Datasets,[0],[0]
CIFAR-10 and CIFAR-100 have respectively 10 and 100 classes.,5.1. Datasets,[0],[0]
"For these two datasets, we adopt the following standard preprocessing and data augmentation scheme (Lin et al., 2013;",5.1. Datasets,[0],[0]
"He et al., 2016; Huang et al., 2016a; Zagoruyko & Komodakis, 2016): Each training image is first zero-padded with 4 pixels on each side.",5.1. Datasets,[0],[0]
The resulting image is randomly cropped to produce a new 32 × 32 image which is subsequently horizontally flipped with probability 0.5.,5.1. Datasets,[0],[0]
We also normalize every image with the mean and standard deviation of its channels.,5.1. Datasets,[0],[0]
"Following the same practice as (Huang et al., 2016a), we initially use 5K images from the training as a validation set.",5.1. Datasets,[0],[0]
"Next, we train de novo the best model on the full set of 50K images and report the results on the test set.",5.1. Datasets,[0],[0]
SVHN The Street View House Number dataset is a set of 32× 32 color digit images officially split into 73257 training images and 26032 test images.,5.1. Datasets,[0],[0]
"Following common practice (Zagoruyko & Komodakis, 2016; He et al., 2016; Huang et al., 2016a;b), we randomly sample 10000 images from the available extra set of about 600K images as a validation set and combine the rest of the pictures with the official training set.",5.1. Datasets,[0],[0]
We divide the pixel values by 255 as a preprocessing step and report the test set performance of the best performing model on the validation set.,5.1. Datasets,[0],[0]
ConvNet Models.,5.2. Models and Implementation details,[0],[0]
"For the CIFAR and SVHN datasets, we trained wide residual networks (Zagoruyko & Komodakis, 2016) as they perform on par with standard resnets (He et al., 2016) while being faster to train thanks to a reduced depth.",5.2. Models and Implementation details,[0],[0]
We used wide resnets of depth 28 and width 10 for both CIFAR-10 and CIFAR-100.,5.2. Models and Implementation details,[0],[0]
For SVHN we used wide resnet of depth 16 and width 4.,5.2. Models and Implementation details,[0],[0]
"For each architecture, we compare Parseval networks with the vanilla model trained with standard regularization both in the adversarial and the non-adversarial training settings.
",5.2. Models and Implementation details,[0],[0]
ConvNet Training.,5.2. Models and Implementation details,[0],[0]
We train the networks with stochastic gradient descent using a momentum of 0.9.,5.2. Models and Implementation details,[0],[0]
"On CIFAR datasets, the initial learning rate is set to 0.1 and scaled by a factor of 0.2 after epochs 60, 120 and 160, for a total number of 200 epochs.",5.2. Models and Implementation details,[0],[0]
We used mini-batches of size 128.,5.2. Models and Implementation details,[0],[0]
"For SVHN, we trained the models with mini-batches of size 128 for 160 epochs starting with a learning rate of 0.01 and decreasing it by a factor of 0.1 at epochs 80 and 120.",5.2. Models and Implementation details,[0],[0]
"For all the vanilla models, we applied by default weight decay regularization (with parameter λ = 0.0005) together with batch normalization and dropout since this combination resulted in better accuracy and increased robustness in preliminary experiments.",5.2. Models and Implementation details,[0],[0]
"The dropout rate use
is 0.3 for CIFAR and 0.4 for SVHN.",5.2. Models and Implementation details,[0],[0]
"For Parseval regularized models, we choose the value of the retraction parameter to be β = 0.0003 for CIFAR datasets and β = 0.0001 for SVHN based on the performance on the validation set.",5.2. Models and Implementation details,[0],[0]
"In all cases, We also adversarially trained each of the models on CIFAR-10 and CIFAR-100 following the guidelines in (Goodfellow et al., 2015; Shaham et al., 2015; Kurakin et al., 2016).",5.2. Models and Implementation details,[0],[0]
"In particular, we replace 50% of the examples of every minibatch by their adversarially perturbed version generated using the one-step method to avoid label leaking (Kurakin et al., 2016).",5.2. Models and Implementation details,[0],[0]
"For each mini-batch, the magnitude of the adversarial perturbation is obtained by sampling from a truncated Gaussian centered at 0 with standard deviation 2.
Fully Connected Model.",5.2. Models and Implementation details,[0],[0]
We also train feedforward networks composed of 4 fully connected hidden layers of size 2048 and a classification layer.,5.2. Models and Implementation details,[0],[0]
The input to these networks are images unrolled into a C × 1024 dimensional vector where C is the number of channels.,5.2. Models and Implementation details,[0],[0]
We used these models on MNIST and CIFAR-10 mainly to demonstrate that the proposed approach is also useful on non-convolutional networks.,5.2. Models and Implementation details,[0],[0]
We compare a Parseval networks to vanilla models with and without weight decay regularization.,5.2. Models and Implementation details,[0],[0]
"For adversarially trained models, we follow the guidelines previously described for the convolutional networks.
",5.2. Models and Implementation details,[0],[0]
Fully Connected Training.,5.2. Models and Implementation details,[0],[0]
We train the models with SGD and divide the learning rate by two every 10 epochs.,5.2. Models and Implementation details,[0],[0]
We use mini-batches of size 100 and train the model for 50 epochs.,5.2. Models and Implementation details,[0],[0]
We chose the hyperparameters on the validation set and retrain the model on the union of the training and validation sets.,5.2. Models and Implementation details,[0],[0]
"The hyperparameters are β, the size of the row subset S, the learning rate and its decrease rate.",5.2. Models and Implementation details,[0],[0]
Using a subset S of 30% of all the rows of each of weight matrix for the retraction step worked well in practice.,5.2. Models and Implementation details,[0],[0]
We first validate that Parseval training (Algorithm 1) indeed yields (near)-orthonormal weight matrices.,5.3.1. (QUASI)-ORTHOGONALITY.,[0],[0]
"To do so, we analyze the spectrum of the weight matrices of the different models by plotting the histograms of their singular values, and compare these histograms for Parseval networks to networks trained using standard SGD with and without weight decay (SGD-wd and SGD).
",5.3.1. (QUASI)-ORTHOGONALITY.,[0],[0]
The histograms representing the distribution of singular values at layers 1 and 4 for the fully connected network (using S = 30%) trained on the dataset CIFAR-10 are shown in Fig. 2 (the figures for convolutional networks are similar).,5.3.1. (QUASI)-ORTHOGONALITY.,[0],[0]
The singular values obtained with our method are tightly concentrated around 1.,5.3.1. (QUASI)-ORTHOGONALITY.,[0],[0]
"This experiment confirms that the weight matrices produced by the proposed opti-
mization procedure are (almost) orthonormal.",5.3.1. (QUASI)-ORTHOGONALITY.,[0],[0]
"The distribution of the singular values of the weight matrices obtained with SGD has a lot more variance, with nearly as many small values as large ones.",5.3.1. (QUASI)-ORTHOGONALITY.,[0],[0]
"Adding weight decay to standard SGD leads to a sparse spectrum for the weight matrices, especially in the higher layers of the network suggesting a low-rank structure.",5.3.1. (QUASI)-ORTHOGONALITY.,[0],[0]
"This observation has motivated recent work on compressing deep neural networks (Denton et al., 2014).",5.3.1. (QUASI)-ORTHOGONALITY.,[0],[0]
"We evaluate the robustness of the models to adversarial noise by generating adversarial examples from the test set, for various magnitudes of the noise vector.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"Following common practice (Kurakin et al., 2016), we use the fast gradient sign method to generate the adversarial examples (using ‖.‖∞, see Section 3.1).",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"Since these adversarial examples transfer from one network to the other, the fast gradient sign method allows to benchmark the network for reasonable settings where the opponent does not know the network.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
We report the accuracy of each model as a function of the magnitude of the noise.,5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"To make the results easier to interpret, we compute the corresponding Signal to Noise Ratio (SNR).",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"For an input x and perturbation δx, the SNR is defined as SNR(x, δx) = 20 log10 ‖x‖2 ‖δx‖2 .",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"We show some adversarial examples in Fig. 1.
Fully Connected Nets.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
Figure 3 depicts a comparison of Parseval and vanilla networks with and without adversarial training at various noise levels.,5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"On both MNIST and CIFAR-10, Parseval networks consistently outperforms weight decay regularization.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"In addition, it is as robust as
adversarial training (SGD-wd-da) on CIFAR-10.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"Combining Parseval Networks and adversarial training results in the most robust method on MNIST.
ResNets.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"Table 1 summarizes the results of our experiments with wide residual Parseval and vanilla networks on CIFAR-10, CIFAR-100 and SVHN.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"In the table, we denote Parseval(OC) the Parseval network with orthogonality constraint and without using a convex combination in aggregation layers.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
Parseval indicates the configuration where both of the orthogonality and convexity constraints are used.,5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
We first observe that Parseval networks outperform vanilla ones on all datasets on the clean examples and match the state of the art performances on CIFAR-10 (96.28%) and SVHN (98.44%).,5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"On CIFAR-100, when we use Parseval wide Resnet of depth 40 instead of 28, we achieve an accuracy of 81.76%.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"In comparison, the best performance achieved by a vanilla wide resnet (Zagoruyko & Komodakis, 2016) and a pre-activation resnet (He et al., 2016) are respectively 81.12% and 77.29%.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"Therefore, our proposal is a useful regularizer for legitimate examples.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"Also note that in most cases, Parseval networks combining both the orthogonality constraint and the convexity constraint is superior to use the orthogonality constraint solely.
",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
The results presented in the table validate our most important claim: Parseval networks significantly improve the robustness of vanilla models to adversarial examples.,5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"When no adversarial training is used, the gap in accuracy be-
tween the two methods is significant (particularly in the high noise scenario).",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"For an SNR value of 40, the best Parseval network achieves 55.41% accuracy while the best vanilla model is at 44.62%.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"When the models are adversarially trained, Parseval networks remain superior to vanilla models in most cases.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"Interestingly, adversarial training only slightly improves the robustness of Parseval networks in low noise setting (e.g. SNR values of 45-50) and sometimes even deteriorates it (e.g. on CIFAR-10).",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"In contrast, combining adversarial training and Parseval networks is an effective approach in the high noise setting.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"This result suggests that thanks to the particular form of regularizer (controlling the Lipschitz constant of the network), Parseval networks achieves robustness to adversarial examples located in the immediate vicinity of each data point.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"Therefore, adversarial training only helps for adversarial examples found further away from the legitimate patterns.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"This observation holds consistently across all our datasets.
",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"Better use of capacity Given the distribution of singular values observed in Figure 2, we want to analyze the intrinsic dimensionality of the representation learned by the different networks at every layer.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"To that end, we use the local covariance dimension (Dasgupta & Freund, 2008) which can be measured from the covariance matrix of the data.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"For each layer k of the fully connected network, we compute the activation’s empirical covariance matrix 1 n",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"∑n i=1 φk(x)φk(x)
",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
>,5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
and obtain its sorted eigenvalues σ1 ≥ · · · ≥ σd.,5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"For each method and each layer, we select the smallest integer p such that ∑p i=1",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
σi ≥ 0.99 ∑d i=1,5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
σi.,5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
This gives us the number of dimensions that we need to explain 99% of the covariance.,5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"We can also compute the same quantity for the examples of each class, by only considering in the empirical estimation of the covariance of the examples xi such that yi = c. Table 2 report these numbers for all examples and the per-class average on CIFAR-10.
Table 2 shows that the local covariance dimension of all the data is consistently higher for Parseval networks than all the other approaches at any layer of the network.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"SGDwd-da contracts all the data in very low dimensional spaces at the upper levels of the network by using only 0.4% of the total dimension (layer 3 and 4) while Parseval networks use about 81% and 56% at of the whole dimension respectively
in the same layers.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"This is intriguing given that SGD-wd-da also increases the robustness of the network, apparently not in the same way as Parseval networks.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"For the average local covariance dimension of the classes, SGD-wd-da contracts each class into the same dimensionality as it contracts all the data at the upper layers of the network.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"For Parseval, the data of each class is contracted in about 30% and 19% of the overall dimension.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"These results suggest that Parseval contracts the data of each class in a lower dimensional manifold (compared to the intrinsic dimensionality of the whole data) hence making classification easier.
faster convergence Parseval networks converge significantly faster than vanilla networks trained with batch normalization and dropout as depicted by figure 4.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"Thanks to the orthogonalization step following each gradient update, the weight matrices are well conditioned at each step during the optimization.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
We hypothesize this is the main explanation of this phenomenon.,5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"For convolutional networks (resnets), the faster convergence is not obtained at the expense of larger wall-time since the cost of the projection step is negligible compared to the total cost of the forward pass on modern GPU architecture thanks to the small size of the filters.",5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.,[0],[0]
"We introduced Parseval networks, a new approach for learning neural networks that are intrinsically robust to adversarial noise.",6. Conclusion,[0],[0]
We proposed an algorithm that allows us to optimize the model efficiently.,6. Conclusion,[0],[0]
Empirical results on three classification datasets with fully connected and wide residual networks illustrate the performance of our approach.,6. Conclusion,[0],[0]
"As a byproduct of the regularization we propose, the model trains faster and makes a better use of its capacity.",6. Conclusion,[0],[0]
Further investigation of this phenomenon is left to future work.,6. Conclusion,[0],[0]
"The authors would like to thank M.A. Ranzato, Y. Tian, A. Bordes and F. Perronnin for their valuable feedback on this work.",Acknowledgements,[0],[0]
"We introduce Parseval networks, a form of deep neural networks in which the Lipschitz constant of linear, convolutional and aggregation layers is constrained to be smaller than 1.",abstractText,[0],[0]
Parseval networks are empirically and theoretically motivated by an analysis of the robustness of the predictions made by deep neural networks when their input is subject to an adversarial perturbation.,abstractText,[0],[0]
"The most important feature of Parseval networks is to maintain weight matrices of linear and convolutional layers to be (approximately) Parseval tight frames, which are extensions of orthogonal matrices to non-square matrices.",abstractText,[0],[0]
We describe how these constraints can be maintained efficiently during SGD.,abstractText,[0],[0]
"We show that Parseval networks match the state-of-the-art in terms of accuracy on CIFAR-10/100 and Street View House Numbers (SVHN), while being more robust than their vanilla counterpart against adversarial examples.",abstractText,[0],[0]
"Incidentally, Parseval networks also tend to train faster and make a better usage of the full capacity of the networks.",abstractText,[0],[0]
Parseval Networks: Improving Robustness to Adversarial Examples,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2331–2336, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics
We recast syntactic parsing as a language modeling problem and use recent advances in neural network language modeling to achieve a new state of the art for constituency Penn Treebank parsing — 93.8 F1 on section 23, using 2-21 as training, 24 as development, plus tri-training. When trees are converted to Stanford dependencies, UAS and LAS are 95.9% and 94.1%.",text,[0],[0]
"Recent work on deep learning syntactic parsing models has achieved notably good results, e.g., Dyer et al. (2016) with 92.4 F1 on Penn Treebank constituency parsing and Vinyals et al. (2015) with 92.8 F1.",1 Introduction,[0],[0]
"In this paper we borrow from the approaches of both of these works and present a neural-net parse reranker that achieves very good results, 93.8 F1, with a comparatively simple architecture.
",1 Introduction,[0],[0]
In the remainder of this section we outline the major difference between this and previous work — viewing parsing as a language modeling problem.,1 Introduction,[0],[0]
Section 2 looks more closely at three of the most relevant previous papers.,1 Introduction,[0],[0]
"We then describe our exact model (Section 3), followed by the experimental setup and results (Sections 4 and 5).",1 Introduction,[0],[0]
"Formally, a language model (LM) is a probability distribution over strings of a language:
P (x) = P (x1, · · · , xn)
=
n∏
t=1
P (xt|x1, · · · , xt−1), (1)
where x is a sentence and t indicates a word position.",1.1 Language Modeling,[0],[0]
"The efforts in language modeling go into computing P (xt|x1, · · · , xt−1), which as described next is useful for parsing as well.",1.1 Language Modeling,[0],[0]
"A generative parsing model parses a sentence (x) into its phrasal structure (y) according to
argmax y′∈Y(x)
P (x,y′),
where Y(x) lists all possible structures of x.",1.2 Parsing as Language Modeling,[0],[0]
"If we think of a tree (x,y) as a sequence (z) (Vinyals et
2331
al., 2015) as illustrated in Figure 1, we can define a probability distribution over (x,y) as follows:
P (x,y) = P (z) = P (z1, · · · , zm)
= m∏
t=1
P (zt|z1, · · · , zt−1), (2)
which is equivalent to Equation (1).",1.2 Parsing as Language Modeling,[0],[0]
"We have reduced parsing to language modeling and can use language modeling techniques of estimating P (zt|z1, · · · , zt−1) for parsing.",1.2 Parsing as Language Modeling,[0],[0]
We look here at three neural net (NN) models closest to our research along various dimensions.,2 Previous Work,[0],[0]
"The first (Zaremba et al., 2014) gives the basic language modeling architecture that we have adopted, while the other two (Vinyals et al., 2015; Dyer et al., 2016) are parsing models that have the current best results in NN parsing.",2 Previous Work,[0],[0]
"The LSTM-LM of Zaremba et al. (2014) turns (x1, · · · , xt−1) into ht, a hidden state of an LSTM (Hochreiter and Schmidhuber, 1997; Gers et al., 2003; Graves, 2013), and uses ht to guess xt:
P (xt|x1, · · · , xt−1) = P (xt|ht) = softmax(Wht)[xt],
where W is a parameter matrix and [i] indexes ith element of a vector.",2.1 LSTM-LM,[0],[0]
"The simplicity of the model makes it easily extendable and scalable, which has inspired a character-based LSTM-LM that works well for many languages (Kim et al., 2016) and an ensemble of large LSTM-LMs for English with astonishing perplexity of 23.7 (Jozefowicz et al., 2016).",2.1 LSTM-LM,[0],[0]
"In this paper, we build a parsing model based on the LSTM-LM of Zaremba et al. (2014).",2.1 LSTM-LM,[0],[0]
"Vinyals et al. (2015) observe that a phrasal structure (y) can be expressed as a sequence and build a machine translation parser (MTP), a sequence-tosequence model, which translates x into y using a
conditional probability:
P (y|x)",2.2 MTP,[0],[0]
"= P (y1, · · · , yl|x)
= l∏
t=1
P (yt|x, y1, · · · , yt−1),
where the conditioning event (x, y1, · · · , yt−1) is modeled by an LSTM encoder and an LSTM decoder.",2.2 MTP,[0],[0]
"The encoder maps x into he, a set of vectors that represents x, and the decoder obtains a summary vector (h′t) which is concatenation of the decoder’s hidden state (hdt ) and weighted sum of word representations ( ∑n i=1 αih e i ) with an alignment vector (α).",2.2 MTP,[0],[0]
Finally the decoder predicts yt given h′t.,2.2 MTP,[0],[0]
"Inspired by MTP, our model processes sequential trees.",2.2 MTP,[0],[0]
"Recurrent Neural Network Grammars (RNNG), a generative parsing model, defines a joint distribution over a tree in terms of actions the model takes to generate the tree (Dyer et al., 2016):
P (x,y) = P (a) =
m∏
t=1
P (at|a1, · · · , at−1), (3)
where a is a sequence of actions whose output precisely matches the sequence of symbols in z, which implies Equation (3) is the same as Equation (2).",2.3 RNNG,[0],[0]
"RNNG and our model differ in how they compute the conditioning event (z1, · · · , zt−1): RNNG combines hidden states of three LSTMs that keep track of actions the model has taken, an incomplete tree the model has generated and words the model has generated whereas our model uses one LSTM’s hidden state as shown in the next section.",2.3 RNNG,[0],[0]
"Our model, the model of Zaremba et al. (2014) applied to sequential trees and we call LSTM-LM from now on, is a joint distribution over trees:
P (x,y)",3 Model,[0],[0]
"= P (z) = m∏
t=1
P (zt|z1, · · · , zt−1)
=
m∏
t=1
P (zt|ht)
= m∏
t=1
softmax(Wht)[zt],
where ht is a hidden state of an LSTM.",3 Model,[0],[0]
"Due to lack of an algorithm that searches through an exponentially large phrase-structure space, we use an n-best parser to reduce Y(x) to Y ′(x), whose size is polynomial, and use LSTM-LM to find y that satisfies
argmax y′∈Y ′(x)
P (x,y′).",3 Model,[0],[0]
(4),3 Model,[0],[0]
"The model has three LSTM layers with 1,500 units and gets trained with truncated backpropagation through time with mini-batch size 20 and step size 50.",3.1 Hyper-parameters,[0],[0]
"We initialize starting states with previous minibatch’s last hidden states (Sutskever, 2013).",3.1 Hyper-parameters,[0],[0]
"The forget gate bias is initialized to be one (Jozefowicz et al., 2015) and the rest of model parameters are sampled from U(−0.05, 0.05).",3.1 Hyper-parameters,[0],[0]
"Dropout is applied to non-recurrent connections (Pham et al., 2014) and gradients are clipped when their norm is bigger than 20 (Pascanu et al., 2013).",3.1 Hyper-parameters,[0],[0]
"The learning rate is 0.25 · 0.85max( −15, 0) where is an epoch number.",3.1 Hyper-parameters,[0],[0]
"For simplicity, we use vanilla softmax over an entire vocabulary as opposed to hierarchical softmax (Morin and Bengio, 2005) or noise contrastive estimation (Gutmann and Hyvärinen, 2012).",3.1 Hyper-parameters,[0],[0]
"We describe datasets we use for evaluation, detail training and development processes.1",4 Experiments,[0],[0]
"We use the Wall Street Journal (WSJ) of the Penn Treebank (Marcus et al., 1993) for training (2-21), development (24) and testing (23) and millions of auto-parsed “silver” trees (McClosky et al., 2006; Huang et al., 2010; Vinyals et al., 2015) for tritraining.",4.1 Data,[0],[0]
"To obtain silver trees, we parse the entire section of the New York Times (NYT) of the fifth Gigaword (Parker et al., 2011) with a product of eight Berkeley parsers (Petrov, 2010)2 and ZPar (Zhu et al., 2013) and select 24 million trees on which both parsers agree (Li et al., 2014).",4.1 Data,[0],[0]
"We do not resample trees to match the sentence length distribution of the NYT to that of the WSJ (Vinyals et
1The code and trained models used for experiments are available at github.com/cdg720/emnlp2016.
",4.1 Data,[0],[0]
2We use the reimplementation by Huang et al. (2010).,4.1 Data,[0],[0]
"al., 2015) because in preliminary experiments Charniak parser (Charniak, 2000) performed better when trained on all of 24 million trees than when trained on resampled two million trees.
",500 97.0 91.8 40.0,[0],[0]
"Given x, we produce Y ′(x), 50-best trees, with Charniak parser and find y with LSTM-LM as Dyer et al. (2016) do with their discriminative and generative models.3",500 97.0 91.8 40.0,[0],[0]
"We unk words that appear fewer than 10 times in the WSJ training (6,922 types) and drop activations with probability 0.7.",4.2.1 Supervision,[0],[0]
"At the beginning of each epoch, we shuffle the order of trees in the training data.",4.2.1 Supervision,[0],[0]
Both perplexity and F1 of LSTM-LM (G) improve and then plateau (Figure 2).,4.2.1 Supervision,[0],[0]
"Perplexity, the
3Dyer et al. (2016)’s discriminative model performs comparably to Charniak (89.8 vs. 89.7).
model’s training objective, nicely correlates with F1, what we care about.",4.2.1 Supervision,[0],[0]
Training takes 12 hours (37 epochs) on a Titan X. We also evaluate our model with varying n-best trees including optimal 51-best trees that contain gold trees (51o).,4.2.1 Supervision,[0],[0]
"As shown in Table 1, the LSTM-LM (G) is robust given sufficiently large n, i.e. 50, but does not exhibit its full capacity because of search errors in Charniak parser.",4.2.1 Supervision,[0],[0]
We address this problem in Section 5.3.,4.2.1 Supervision,[0],[0]
"We unk words that appear at most once in the training (21,755 types).",4.2.2 Semi-supervision,[0],[0]
"We drop activations with probability 0.45, smaller than 0.7, thanks to many silver trees, which help regularization.",4.2.2 Semi-supervision,[0],[0]
"We train LSTM-LM (GS) on the WSJ and a different set of 400,000 NYT trees for each epoch except for the last one during which we use the WSJ only.",4.2.2 Semi-supervision,[0],[0]
Training takes 26 epochs and 68 hours on a Titan X. LSTMLM (GS) achieves 92.5 F1 on the development.,4.2.2 Semi-supervision,[0],[0]
"As shown in Table 2, with 92.6 F1 LSTM-LM (G) outperforms an ensemble of five MTPs (Vinyals et al., 2015) and RNNG (Dyer et al., 2016), both of which are trained on the WSJ only.",5.1 Supervision,[0],[0]
"We compare LSTM-LM (GS) to two very strong semi-supervised NN parsers: an ensemble of five MTPs trained on 11 million trees of the highconfidence corpus4 (HC) (Vinyals et al., 2015); and an ensemble of six one-to-many sequence models
4The HC consists of 90,000 gold trees, from the WSJ, English Web Treebank and Question Treebank, and 11 million silver trees, whose sentence length distribution matches that of the WSJ, parsed and agreed on by Berkeley parser and ZPar.
trained on the HC and 4.5 millions of EnglishGerman translation sentence pairs (Luong et al., 2016).",5.2 Semi-supervision,[0],[0]
We also compare LSTM-LM (GS) to best performing non-NN parsers in the literature.,5.2 Semi-supervision,[0],[0]
Parsers’ parsing performance along with their training data is reported in Table 3.,5.2 Semi-supervision,[0],[0]
LSTM-LM (GS) outperforms all the other parsers with 93.1 F1.,5.2 Semi-supervision,[0],[0]
"Due to search errors – good trees are missing in 50-best trees – in Charniak (G), our supervised and semi-supervised models do not exhibit their full potentials when Charniak (G) provides Y ′(x).",5.3 Improved Semi-supervision,[0],[0]
"To mitigate the search problem, we tri-train Charniak (GS) on all of 24 million NYT trees in addition to the WSJ, to yield Y ′(x).",5.3 Improved Semi-supervision,[0],[0]
"As shown in Table 3, both LSTM-LM (G) and LSTM-LM (GS) are affected by the quality of Y ′(x).",5.3 Improved Semi-supervision,[0],[0]
"A single LSTM-LM (GS) together with Charniak (GS) reaches 93.6 and an ensemble of eight LSTM-LMs (GS) with Charniak (GS) achieves a new state of the art, 93.8 F1.",5.3 Improved Semi-supervision,[0],[0]
"When trees are converted to Stanford dependencies,5 UAS and LAS are 95.9% and 94.1%,6 more than 1% higher than those of the state of the art dependency parser (Andor et al., 2016).",5.3 Improved Semi-supervision,[0],[0]
"Why an indirect method (converting trees to dependencies) is more accurate than a direct one (dependency parsing) remains unanswered (Kong and Smith, 2014).",5.3 Improved Semi-supervision,[0],[0]
The generative parsing model we presented in this paper is very powerful.,6 Conclusion,[0],[0]
"In fact, we see that a generative parsing model, LSTM-LM, is more effective than discriminative parsing models (Dyer et al., 2016).",6 Conclusion,[0],[0]
"We suspect building large models with character embeddings would lead to further improvement as in language modeling (Kim et al., 2016; Jozefowicz et al., 2016).",6 Conclusion,[0],[0]
We also wish to develop a complete parsing model using the LSTMLM framework.,6 Conclusion,[0],[0]
"We thank the NVIDIA corporation for its donation of a Titan X GPU, Tstaff of Computer Science
5Version 3.3.0.",Acknowledgments,[0],[0]
6We use the CoNLL evaluator available through the CoNLL website: ilk.uvt.nl/conll/software/eval.pl.,Acknowledgments,[0],[0]
"Following the convention, we ignore punctuation.
at Brown University for setting up GPU machines and David McClosky for helping us train Charniak parser on millions trees.",Acknowledgments,[0],[0]
"We recast syntactic parsing as a language modeling problem and use recent advances in neural network language modeling to achieve a new state of the art for constituency Penn Treebank parsing — 93.8 F1 on section 23, using 2-21 as training, 24 as development, plus tri-training.",abstractText,[0],[0]
"When trees are converted to Stanford dependencies, UAS and LAS are 95.9% and 94.1%.",abstractText,[0],[0]
Parsing as Language Modeling,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 69–81 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
"While parsing has become a relatively mature technology for written text, parser performance on conversational speech lags behind.",1 Introduction,[0],[0]
"Speech poses challenges for parsing: transcripts may contain errors and lack punctuation; even perfect transcripts can be difficult to handle because of disfluencies (restarts, repetitions, and self-corrections), filled pauses (“um”, “uh”), interjections (“like”), parentheticals (“you know”, “I mean”), and sentence fragments.",1 Introduction,[0],[0]
"Some of these phenomena can be handled in standard grammars, but disfluencies typically require extensions of the model.",1 Introduction,[0],[0]
"Different approaches have been explored in both constituency parsing (Charniak and Johnson, 2001; Johnson and Charniak, 2004) and dependency parsing (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014).
∗Equal Contribution.
",1 Introduction,[0],[0]
"Despite these challenges, speech carries helpful extra information – beyond the words – associated with the prosodic structure of an utterance and encoded via variation in timing and intonation.",1 Introduction,[0],[0]
"Speakers pause in locations that are correlated with syntactic structure (Grosjean et al., 1979), and listeners use prosodic structure in resolving syntactic ambiguities (Price et al., 1991).",1 Introduction,[0],[0]
"Prosodic cues also signal disfluencies by marking the interruption point (Shriberg, 1994).",1 Introduction,[0],[0]
"However, most speech parsing systems in practice take little advantage of these cues.",1 Introduction,[0],[0]
"Our study focuses on this last challenge, aiming to incorporate prosodic cues in a neural parser, handling disfluencies as constituents via a neural attention mechanism.
",1 Introduction,[0],[0]
"A challenge of incorporating prosody in parsing is that multiple acoustic cues interact to signal prosodic structure, including pauses, lengthening, fundamental frequency modulation, and spectral shape.",1 Introduction,[0],[0]
"These cues also vary with the phonetic segment, emphasis, emotion and speaker, so feature extraction typically involves multiple time windows and normalization techniques.",1 Introduction,[0],[0]
"The most successful constituent parsers have mapped these features to prosodic boundary posteriors by using labeled training data (Kahn et al., 2005; Hale et al., 2006; Dreyer and Shafran, 2007).",1 Introduction,[0],[0]
The approach proposed here takes advantage of advances in neural networks to automatically learn a good feature representation without the need to explicitly represent prosodic constituents.,1 Introduction,[0.9609723224500657],"['Within the CKY framework, the key to defining our algorithm is a set of rules that specify which items are allowed to combine.']"
"To narrow the scope of this work and facilitate error analysis, our experiments use known transcripts and sentence segmentation.
",1 Introduction,[0],[0]
Our work offers the following contributions.,1 Introduction,[0],[0]
We introduce a framework for directly integrating acoustic-prosodic features with text in a neural encoder-decoder parser that does not require handannotated prosodic structure.,1 Introduction,[0],[0]
"We demonstrate improvements in constituent parsing of conversational
69
speech over a high-quality text-only parser and provide analyses showing where prosodic features help and that assessment of their utility is affected by human transcription errors.",1 Introduction,[0],[0]
Our model maps a sequence of word-level input features to a linearized parse output sequence.,2 Task and Model Description,[0],[0]
"The word-level input feature vector consists of the concatenation of (learnable) word embeddings ei and several types of acoustic-prosodic features, described in Section 2.3.",2 Task and Model Description,[0],[0]
"We assume the availability of a training treebank of conversational speech (in our case, SwitchboardNXT (Calhoun et al., 2010)) and corresponding constituent parses.",2.1 Task Setup,[0],[0]
The transcriptions are preprocessed by removing punctuation and lower-casing all text to better mimic the speech recognition setting.,2.1 Task Setup,[0],[0]
"Following Vinyals et al. (2015), the parse trees are linearized, and pre-terminals are normalized as “XX” (see Appendix A.1).",2.1 Task Setup,[0],[0]
Our attention-based encoder-decoder model is similar to the one used by Vinyals et al. (2015).,2.2 Encoder-Decoder Parser,[0],[0]
"The encoder is a deep long short-term memory recurrent neural network (LSTM-RNN) (Hochreiter and Schmidhuber, 1997) that reads in a word-level inputs,1 represented as a sequence of vectors x = (x1, · · · ,xTs), and outputs high-level features h = (h1, · · · ,hTs) where hi = LSTM(xi,hi−1).2
",2.2 Encoder-Decoder Parser,[0],[0]
"The parse decoder is also a deep LSTM-RNN that predicts the linearized parse sequence y = (y1, · · · , yTo) as follows:
P (y|x) = To∏
t=1
P (yt|h,y<t)
",2.2 Encoder-Decoder Parser,[0],[0]
"In attention-based models, the posterior distribution of the output yt at time step t is given by:
P (yt|h,y<t) =",2.2 Encoder-Decoder Parser,[0],[0]
softmax(W s[ct;dt],2.2 Encoder-Decoder Parser,[0],[0]
"+ bs),
where vector bs and matrix W s are learnable parameters; ct is referred to as a context vector that summarizes the encoder’s output h; and dt is the
1As in Vinyals et al. (2015)",2.2 Encoder-Decoder Parser,[0],[0]
"the input sequence is processed in reverse order, as shown in Figure 1.
",2.2 Encoder-Decoder Parser,[0],[0]
2For brevity we omit the LSTM equations.,2.2 Encoder-Decoder Parser,[0],[0]
"The details can be found, e.g., in Zaremba et al. (2014).
",2.2 Encoder-Decoder Parser,[0],[0]
"decoder hidden state at time step t, which captures the previous output sequence context y<t.
uit = v >",2.2 Encoder-Decoder Parser,[0],[0]
tanh(W 1hi,2.2 Encoder-Decoder Parser,[0],[0]
"+W 2dt + ba)
",2.2 Encoder-Decoder Parser,[0],[0]
"αt = softmax(ut) ct = Ts∑
i=1
αtihi
where vectors v, ba and matrices W 1, W 2 are learnable parameters; ut and αt are the attention score and attention weight vector, respectively, for decoder time step t.
The above attention mechanism is only contentbased, i.e., it is only dependent on hi, dt.",2.2 Encoder-Decoder Parser,[0],[0]
"It is not location-aware, i.e., it does not consider the “location” of the previous attention vector.",2.2 Encoder-Decoder Parser,[0],[0]
"For parsing conversational text, location awareness is beneficial since disfluent structures can have duplicate words/phrases that may “confuse” the attention mechanism.
",2.2 Encoder-Decoder Parser,[0],[0]
"In order to make the model location-aware, the attention mechanism takes into account the previous attention weight vector αt−1.",2.2 Encoder-Decoder Parser,[0],[0]
"In particular, we use the attention mechanism proposed by Chorowski et al. (2015), in which αt−1 is represented via a feature vector f t = F ∗αt−1, where F ∈ Rk×r represents k learnable convolution filters of width r. The filters are used for performing 1-D convolution over αt−1 to extract k features f ti for each time step i of the input sequence.",2.2 Encoder-Decoder Parser,[0],[0]
"The extracted features are then incorporated in the alignment score calculation as:
uit = v >",2.2 Encoder-Decoder Parser,[0],[0]
tanh(W 1hi,2.2 Encoder-Decoder Parser,[0],[0]
+W 2dt,2.2 Encoder-Decoder Parser,[0],[0]
"+W ff ti + ba)
where W f is another learnable parameter matrix.",2.2 Encoder-Decoder Parser,[0],[0]
"Finally, the decoder state dt is computed as dt = LSTM([ỹt−1; ct−1],dt−1), where ỹt−1 is the embedding vector corresponding to the previous output symbol yt−1.",2.2 Encoder-Decoder Parser,[0],[0]
"As we will see in Sec. 4.1, the location-aware attention mechanism is especially useful for handling disfluencies.",2.2 Encoder-Decoder Parser,[0],[0]
"In previous work using encoder-decoder models for parsing (Vinyals et al., 2015; Luong et al., 2016), vector xi is simply the word embedding ei of the word at position i of the input sentence.",2.3 Acoustic-Prosodic Features,[0],[0]
"For parsing conversational speech, we can incorporate acousticprosodic features.",2.3 Acoustic-Prosodic Features,[0],[0]
"Here we explore four types of features widely used in computational models of prosody: pauses, duration lengthening, fundamental frequency, and energy.",2.3 Acoustic-Prosodic Features,[0],[0]
"Since prosodic cues are
at sub- and multi-word time scales, they are integrated with the encoder-decoder using different mechanisms.
",2.3 Acoustic-Prosodic Features,[0],[0]
All features are extracted from transcriptions that are time-aligned at the word level.3,2.3 Acoustic-Prosodic Features,[0],[0]
We use time alignments associated with the corpus to be consistent with other studies.,2.3 Acoustic-Prosodic Features,[0],[0]
"In a small number of cases, the time alignment for a particular word boundary is missing.",2.3 Acoustic-Prosodic Features,[0],[0]
Some cases are due to tokenization.,2.3 Acoustic-Prosodic Features,[0],[0]
"For example, contractions, such as don’t in the original transcript, are treated as separated words for the parser (do and n’t), and the internal word boundary time is missing.",2.3 Acoustic-Prosodic Features,[0],[0]
"In such cases, these internal times are estimated.",2.3 Acoustic-Prosodic Features,[0],[0]
"In other cases, there are transcription mismatches that lead to missing time alignments, where we cannot estimate times.",2.3 Acoustic-Prosodic Features,[0],[0]
"For the roughly 1% of sentences where time alignments are missing, we simply back off to the text-based parser.
Pause.",2.3 Acoustic-Prosodic Features,[0],[0]
"The pause feature vector pi for word i is the concatenation of pre-word pause feature ppre,i and post-word pause feature ppost,i, where each subvector is a learned embedding for 6 pause categories: no pause, missing, 0 < p ≤ 0.05 s, 0.05 s < p ≤ 0.2 s, 0.2 < p ≤ 1 s, and p > 1 s (including turn boundaries).",2.3 Acoustic-Prosodic Features,[0],[0]
The bins are chosen based on the observed distribution (see Appendix A.1).,2.3 Acoustic-Prosodic Features,[0],[0]
"We did not use (real-valued) pause duration directly, for two main reasons: (1) to handle missing time alignments; and (2) duration of pause does
3The assumption of known word alignments is standard for prosodic feature extraction in many spoken language processing studies.",2.3 Acoustic-Prosodic Features,[0],[0]
"Time alignments can be obtained as a by-product of recognition or from forced alignment.
not matter beyond a threshold (e.g. p > 1 s).
",2.3 Acoustic-Prosodic Features,[0],[0]
Word duration.,2.3 Acoustic-Prosodic Features,[0],[0]
"Both word duration and wordfinal duration lengthening are strong cues to prosodic phrase boundaries (Wightman et al., 1992; Pate and Goldwater, 2013).",2.3 Acoustic-Prosodic Features,[0],[0]
"The word duration feature δi is computed as the actual word duration divided by the mean duration of the word, clipped to a maximum value of 5.",2.3 Acoustic-Prosodic Features,[0],[0]
The sample mean is used for frequent words (count ≥ 15).,2.3 Acoustic-Prosodic Features,[0],[0]
For infrequent words we estimate the mean as the sum over the sample means for the phonemes in the word’s dictionary pronunciation.,2.3 Acoustic-Prosodic Features,[0],[0]
"We refer to the manually defined prosodic feature pair of pi and δi as φi.
Fundament frequency (f0) and Energy (E) contours (f0/E).",2.3 Acoustic-Prosodic Features,[0],[0]
We use a CNN to automatically learn the mapping from the time series of f0/E features to a word-level vector.,2.3 Acoustic-Prosodic Features,[0],[0]
"The contour features are extracted from 25-ms frames with 10-ms hops using Kaldi (Povey et al., 2011).",2.3 Acoustic-Prosodic Features,[0],[0]
"Three f0 features are used: warped Normalized Cross Correlation Function (NCCF), log-pitch with Probability of Voicing (",2.3 Acoustic-Prosodic Features,[0],[0]
"POV)-weighted mean subtraction over a 1.5-second window, and the estimated derivative (delta) of the raw log pitch.",2.3 Acoustic-Prosodic Features,[0],[0]
"Three energy features are extracted from the Kaldi 40-mel-frequency filter bank features: Etotal, the log of total energy normalized by dividing by the speaker side’s max total energy; Elow, the log of total energy in the lower 20 mel-frequency bands, normalized by total energy, and Ehigh, the log of total energy in the higher 20 mel-frequency bands, normalized by total energy.",2.3 Acoustic-Prosodic Features,[0],[0]
"Multi-band energy features are used as a
simple mechanism to capture articulatory strengthening at prosodic constituent onsets (Fourgeron and Keating, 1997).
",2.3 Acoustic-Prosodic Features,[0],[0]
Figure 1 summarizes the feature learning approach.,2.3 Acoustic-Prosodic Features,[0],[0]
The f0 and E features are processed at the word level: each sequence of frames corresponding to a time-aligned word (and potentially its surrounding context) is convolved with N filters of m sizes (a total of mN filters).,2.3 Acoustic-Prosodic Features,[0],[0]
The motivation for the multiple filter sizes is to enable the computation of features that capture information on different time scales.,2.3 Acoustic-Prosodic Features,[0],[0]
"For each filter, we perform a 1-D convolution over the 6-dimensional f0/E features with a stride of 1.",2.3 Acoustic-Prosodic Features,[0],[0]
"Each filter output is max-pooled, resulting in mN -dimensional speech features si.",2.3 Acoustic-Prosodic Features,[0],[0]
"Our overall acoustic-prosodic feature vector is the concatenation of pi, δi, and si in various combinations.",2.3 Acoustic-Prosodic Features,[0],[0]
"Our core corpus is Switchboard-NXT (Calhoun et al., 2010), a subset of the Switchboard corpus (Godfrey and Holliman, 1993): 2,400 telephone conversations between strangers; 642 of these were hand-annotated with syntactic parses and further augmented with richer layers of annotation facilitated by the NITE XML toolkit (Calhoun et al., 2010).",3.1 Dataset,[0],[0]
"Our sentence segmentations and syntactic trees are based on the annotations from the Treebank set, with a few manual corrections from the NXT release.",3.1 Dataset,[0],[0]
"This core dataset consists of 100K sentences, totaling 830K tokens forming a vocabulary of 13.5K words.",3.1 Dataset,[0],[0]
"We use the time alignments available from NXT, which is based on a corrected word transcript that occasionally differs from the Treebank, leading to some missing time alignments.",3.1 Dataset,[0],[0]
"We follow the sentence boundaries defined by the parsed data available,4 and the data split (90% train; 5% dev; 5% test) defined by related work done on Switchboard (Charniak and Johnson, 2001; Kahn et al., 2005; Honnibal and Johnson, 2014).",3.1 Dataset,[0],[0]
"The standard evaluation metric for constituent parsing is the parseval metric which uses bracketing precision, recall, and F1, as in the canonical implementation of EVALB.5",3.2 Evaluation Metrics and Baselines,[0],[0]
"For written text, punc-
4Note that these sentence units can be inconsistent with other layers of Switchboard annotations, such as slash units.
",3.2 Evaluation Metrics and Baselines,[0],[0]
"5http://nlp.cs.nyu.edu/evalb/
tuation is sometimes represented as part of the sequence and impacts the final score, but for speech the punctuation is not explicitly available so it does not contribute to the score.",3.2 Evaluation Metrics and Baselines,[0],[0]
Another challenge of transcribed speech is the presence of disfluencies.,3.2 Evaluation Metrics and Baselines,[0],[0]
"Speech repairs are indicated under “EDITED” nodes in Switchboard parse trees, which include structure under these nodes that is not of interest for simple text clean-up.",3.2 Evaluation Metrics and Baselines,[0],[0]
"Therefore, some studies report flattened-edit parseval F1 scores (“flatF1”), which is parseval computed on trees where the structure under edit nodes has been eliminated so that all leaves are immediate children.",3.2 Evaluation Metrics and Baselines,[0],[0]
"We report both scores for the baseline text-only model showing that the differences are small, then use the standard parseval F1 score for most results.6
Disfluencies are particularly problematic for statistical parsers, as explained by Charniak and Johnson (2001), and some systems incorporate a separate disfluency detection stage.",3.2 Evaluation Metrics and Baselines,[0],[0]
"For this reason, and because it is useful for understanding system performance, most studies also report disfluency detection performance, which is measured in terms of the F1 score for detecting whether a word is in an edit region.",3.2 Evaluation Metrics and Baselines,[0],[0]
"Our approach does not involve a separate disfluency detection stage, but identifies disfluencies implicitly via the parse structure.",3.2 Evaluation Metrics and Baselines,[0],[0]
"Consequently, the disfluency detection results are not competitive with work that directly optimize for disfluency detection.",3.2 Evaluation Metrics and Baselines,[0],[0]
"We report disfluency detection scores primarily as a diagnostic.
",3.2 Evaluation Metrics and Baselines,[0],[0]
"Most previous work on integrating prosody and parsing has used the Switchboard corpus, but it is still difficult to compare results because of differences in constraints, objectives and the use of constituent vs. dependency structure, as discussed further in Section 6.",3.2 Evaluation Metrics and Baselines,[0],[0]
The most relevant prior studies (on constituent parsing) that we compare to are a bit old.,3.2 Evaluation Metrics and Baselines,[0],[0]
The text-only result from our neural parser represents a stronger baseline and is important for decoupling the impact of prosody vs. the parsing framework.,3.2 Evaluation Metrics and Baselines,[0],[0]
Both the encoder and decoder are 3-layer deep LSTM-RNNs with 256 hidden units in each layer.,3.3 Model Training and Inference,[0],[0]
"For the location-aware attention, the convolution operation uses 5 filters of width 40 each.",3.3 Model Training and Inference,[0],[0]
"We use 512-dimensional embedding vectors to repre-
6A variant of the “flat-F1” score is used in (Charniak and Johnson, 2001; Kahn et al., 2005), which uses a relaxed edited node precision and recall but also ignores filled pauses.
sent words and linearized parsing symbols, such as “(S”.7
A number of configurations are explored for the acoustic-prosodic features, tuning based on dev set parsing performance.",3.3 Model Training and Inference,[0],[0]
"Pause embeddings are tuned over {4, 16, 32} dimensions.",3.3 Model Training and Inference,[0],[0]
"For the CNN, we try different configurations of filter widths w ∈ {",3.3 Model Training and Inference,[0],[0]
"[10, 25, 50], [5, 10, 25, 50]} and number of filters N ∈ {16, 32, 64, 128} for each filter width.8 These filter size combinations are chosen to capture f0 and energy phenomena on various levels: w = 5, 10 for sub-word, w = 25 for word, and w = 50 for word and extended context.",3.3 Model Training and Inference,[0],[0]
Our best model uses 32-dimensional pause embeddings and N = 32 filters of widthsw =,3.3 Model Training and Inference,[0],[0]
"[5, 10, 25, 50], which corresponds to m = 4 and 128 filters.
",3.3 Model Training and Inference,[0],[0]
"For optimization we use Adam (Kingma and Ba, 2014) with a minibatch size of 64.",3.3 Model Training and Inference,[0],[0]
"The initial learning rate is 0.001 which is decayed by a factor of 0.9 whenever training loss, calculated after every 500 updates, degrades relative to the worst of its previous 3 values.",3.3 Model Training and Inference,[0],[0]
All models are trained for up to 50 epochs with early stopping.,3.3 Model Training and Inference,[0],[0]
"For regularization, dropout with 0.3 probability is applied on the output of all LSTM layers (Pham et al., 2014).
",3.3 Model Training and Inference,[0],[0]
"For inference, we use a greedy decoder to generate the linearized parse.",3.3 Model Training and Inference,[0],[0]
The output token with maximum posterior probability is chosen at every time step and fed as input in the next time step.,3.3 Model Training and Inference,[0],[0]
The decoder stops upon producing the end-of-sentence symbol.,3.3 Model Training and Inference,[0],[0]
"We use TensorFlow (Abadi et al., 2015) to implement all models.9",3.3 Model Training and Inference,[0],[0]
"7The number of layers, dimension of hidden units, dimension of embedding, and convolutional attention filter parameters of the text-only parser were explored in earlier experiments on the development set and then fixed as described.
8Note that a filter of width 10 has size 6 × 10, since the features are of dimension 6.
",4.1 Text-only Results,[0],[0]
"9Our code resources can be found in Appendix A.1.
",4.1 Text-only Results,[0],[0]
"We first show our results on the model using only text (i.e. xi = ei) to establish a strong baseline, on top of which we can add acousticprosodic features.",4.1 Text-only Results,[0],[0]
We experiment with the contentonly attention model used by Vinyals et al. (2015) and the content+location attention of Chorowski et al. (2015).,4.1 Text-only Results,[0],[0]
"For comparison with previous nonneural models, we use a high-quality latent-variable parser, the Berkeley parser (Petrov et al., 2006), retrained on our Switchboard data.",4.1 Text-only Results,[0],[0]
Table 1 compares the three text-only models.,4.1 Text-only Results,[0],[0]
"In terms of F1, the content+location attention beats the Berkeley parser by about 2.5% and content-only attention by about 4.5%.",4.1 Text-only Results,[0],[0]
"Flat-F1 scores for both encoder-decoder models is lower than their corresponding F1 scores, suggesting that the encoder-decoder models do well on predicting the internal structure of EDIT nodes while the reverse is true for the Berkeley parser.
",4.1 Text-only Results,[0],[0]
"To explain the gains of content+location attention over content-only attention, we compare their scores on fluent (without EDIT nodes) and disfluent sentences, shown in Table 1.",4.1 Text-only Results,[0],[0]
It is clear that most of the gains for content+location attention are from disfluent sentences.,4.1 Text-only Results,[0],[0]
"A possible explanation is the presence of duplicate words or phrases in disfluent sentences, which can be problematic for a contentonly attention model.",4.1 Text-only Results,[0],[0]
"Since our best model is the content+location attention model, we will henceforth refer to it as the “CL-attn” text-only model.",4.1 Text-only Results,[0],[0]
"All models using acoustic-prosodic features are extensions of this model, which provides a strong text-only baseline.",4.1 Text-only Results,[0],[0]
"We extend our CL-attn model with the three kinds of acoustic-prosodic features: pause (p), word duration (δ), and CNN mappings of fundamental frequency (f0) and energy (E) features (f0/E-CNN).
",4.2 Adding Acoustic-Prosodic Features,[0],[0]
The results of several model configurations on our dev set are presented in Table 2.,4.2 Adding Acoustic-Prosodic Features,[0],[0]
"First, we note that adding any combination of acoustic-prosodic features (individually or in sets) improves performance over the text-only baseline.",4.2 Adding Acoustic-Prosodic Features,[0],[0]
"However, certain combinations of acoustic-prosodic features are not always better than their subsets.",4.2 Adding Acoustic-Prosodic Features,[0],[0]
The text + p + δ + f0/E-CNN model that uses all three types of features has the best performance with a gain of 0.7% over the already-strong text-only baseline.,4.2 Adding Acoustic-Prosodic Features,[0],[0]
"We will henceforth refer to the text + p + δ + f0/E-CNN model as our “best model”.
",4.2 Adding Acoustic-Prosodic Features,[0],[0]
"As a robustness check, we report results of averaging 10 runs on the CL-attn text-only and the best model in Table 3.",4.2 Adding Acoustic-Prosodic Features,[0],[0]
"We performed a bootstrap test (Efron and Tibshirani, 1993) that simulates 105 random test draws on the models giving median performance on the dev set.",4.2 Adding Acoustic-Prosodic Features,[0],[0]
These median models gave a statistically significant difference between the text-only and best model (p-value < 0.02).,4.2 Adding Acoustic-Prosodic Features,[0],[0]
"Additionally, a simple t-test over the two sets of 10 results also shows statistical significance p-value < 0.03.
",4.2 Adding Acoustic-Prosodic Features,[0],[0]
Table 4 presents the results on the test set.,4.2 Adding Acoustic-Prosodic Features,[0],[0]
"Again, adding the acoustic-prosodic features improves over the text-only baseline.",4.2 Adding Acoustic-Prosodic Features,[0],[0]
"The gains are statistically significant for the best model with p-value < 0.02, again using a bootstrap test with simulated 105 random test draws on the two models.
",4.2 Adding Acoustic-Prosodic Features,[0],[0]
"Table 5 includes results from prior studies that compare systems using text alone with ones that incorporate prosody, given hand transcripts and sentence segmentation.",4.2 Adding Acoustic-Prosodic Features,[0],[0]
"It is difficult to compare systems directly, because of the many differences in the experimental set-up.",4.2 Adding Acoustic-Prosodic Features,[0],[0]
"For example, the original Charniak and Johnson (2001) result (reporting F=85.9 for parsing and F=78.2 for disfluencies) leverages punctuation in the text stream, which is not realistic for speech transcripts and not used in most other work.",4.2 Adding Acoustic-Prosodic Features,[0],[0]
"Our work benefits from more text training material than others, but others benefit from gold part-of-speech tags.",4.2 Adding Acoustic-Prosodic Features,[0],[0]
Kahn et al. (2005) use a modified sentence segmentation.,4.2 Adding Acoustic-Prosodic Features,[0],[0]
There are probably minor differences in handling of word fragments and scoring edit regions.,4.2 Adding Acoustic-Prosodic Features,[0],[0]
"Thus, this table primarily shows that our framework leads to more benefits from sentence-internal prosodic cues than others have obtained.",4.2 Adding Acoustic-Prosodic Features,[0],[0]
Effect of sentence length.,5 Analysis,[0],[0]
"Figure 2 shows performance differences between our best model and the text-only model for varying sentence lengths.
",5 Analysis,[0],[0]
"Both models do worse on longer sentences, as expected since the corresponding parse trees tend to be more complex.",5 Analysis,[0],[0]
The performance difference between our best model and the text-only model increases with sentence length.,5 Analysis,[0],[0]
"This is likely because longer sentences more often have multiple prosodic phrases and disfluencies.
",5 Analysis,[0],[0]
Effect of disfluencies.,5 Analysis,[0],[0]
"Table 6 presents parse scores on the subsets of fluent and disfluent sentences, showing that the performance gain is in the disfluent set (65% of the dev set sentences).",5 Analysis,[0],[0]
"Because sentence boundaries are given, and so many fluent sentences in spontaneous speech are short, there is less potential for benefit from prosody in the fluent set.
",5 Analysis,[0],[0]
Types of errors.,5 Analysis,[0],[0]
"We use the Berkeley Parser Analyzer (Kummerfeld et al., 2012) to compare the types of errors made by the different parsers.10 Table 7 presents the relative error reductions over the text-only baseline achieved by the text + p model and our best model for disfluent sentences.",5 Analysis,[0],[0]
The two models differ in the types of error reductions they provide.,5 Analysis,[0],[0]
"Including pause information gives largest improvements on PP attachment and Modifier at-
10This analysis omits the 1% of the sentences that did not have timing information.
tachment errors.",5 Analysis,[0],[0]
"Adding the remaining acousticprosodic features helps to correct more types of attachment errors, especially VP and NP attachment.",5 Analysis,[0],[0]
Figure 3 demonstrates one case where the pause feature helps in correcting a PP attachment error made by a text-only parser.,5 Analysis,[0],[0]
"Other interesting examples (see Appendix A.2) suggest that the learned f0/E features help reduce NP attachment errors where the audio reveals a prominent word at the constituent boundary, even though there is no pause at that word.
",5 Analysis,[0],[0]
Effect of transcription errors.,5 Analysis,[0],[0]
The results and analyses so far have assumed that we have reliable transcripts.,5 Analysis,[0],[0]
"In fact, the original transcripts contained errors, and the Treebank annotators used these without reference to audio files.",5 Analysis,[0],[0]
"Mississippi State University (MS-State) ran a clean-up project
that produced more accurate word transcripts and time alignments (Deshmukh et al., 1998).",5 Analysis,[0],[0]
"The NXT corpus provides reconciliation between Treebank and MS-State transcripts in terms of annotating missed/extra/substituted words, but parses were not re-annotated.",5 Analysis,[0],[0]
The transcript errors mean that the acoustic signal is inconsistent with the “gold” parse tree.,5 Analysis,[0],[0]
"Below are some examples of “fluent” sentences (according to the Treebank transcripts) with transcription errors, for which prosodic features “hurt” parsing.",5 Analysis,[0],[0]
Words that transcribers missed are in brackets and those inserted are underlined.,5 Analysis,[0],[0]
S1: and because <,5 Analysis,[0],[0]
uh> like if your spouse died <all of a sudden you be> all alone it ’d be nice to go someplace with people similar to you to have friends S2: uh uh <i have had>,5 Analysis,[0],[0]
"my wife ’s picked up a couple of things saying uh boy if we could refinish that ’d be a beautiful piece of furniture
Multi-syllable errors are especially problematic, leading to serious inconsistencies between the text and the acoustic signal.",5 Analysis,[0],[0]
"Further, the missed words lead to an incorrect attachment in the “gold” parse in S1 and a missing restart edit in S2.",5 Analysis,[0],[0]
"Indeed, for sentences with consecutive transcript errors, which we expect to impact the prosodic features, there is a statistically significant (p-value < 0.05) negative effect on parsing with prosody.",5 Analysis,[0],[0]
"Not included in this analysis are sentence boundary errors, which also change the “gold” parse.",5 Analysis,[0],[0]
"Thus, prosody may be more useful than results here indicate.",5 Analysis,[0],[0]
"Related work on parsing conversational speech has mainly addressed four problems: speech recognition errors, unknown sentence segmentation, disfluencies, and integrating prosodic cues.",6 Related Work,[0],[0]
"Our work addresses the last two problems, which involve studies based on hand-transcribed text and known sentence boundaries, as in much speech parsing work.",6 Related Work,[0],[0]
The related studies are thus the focus of this discussion.,6 Related Work,[0],[0]
"We describe studies using the Switchboard corpus, since it has dominated work in this area, being the largest source of treebanked English spontaneous speech.
",6 Related Work,[0],[0]
"One major challenge of parsing conversational speech is the presence of disfluencies, which are grammatical and prosodic interruptions.",6 Related Work,[0],[0]
Disfluencies include repetitions (‘I am +,6 Related Work,[0],[0]
"I am’), repairs (‘I am +",6 Related Work,[0],[0]
"we are’), and restarts (‘What I",6 Related Work,[0],[0]
+,6 Related Work,[0],[0]
"Today is the...’), where the ‘+’ corresponds to an interruption point.",6 Related Work,[0],[0]
"Repairs often involve parallel grammatical
constructions, but they can be more complex, involving hedging, clarifications, etc.",6 Related Work,[0],[0]
"Charniak and Johnson (Charniak and Johnson, 2001; Johnson and Charniak, 2004) demonstrated that disfluencies are different in character than other constituents and that parsing performance improves from combining a PCFG parser with a separate module for disfluency detection via parse rescoring.",6 Related Work,[0],[0]
Our approach does not use a separate disfluency detection module; we hypothesized that the location-sensitive attention model helps handle these differences based on analysis of the text-only results (Table 1).,6 Related Work,[0],[0]
"However, more explicit modeling of disfluency pattern match characteristics in a dependency parser (Honnibal and Johnson, 2014) leads to better disfluency detection performance (F = 84.1 vs. 76.7 for our text only model).",6 Related Work,[0],[0]
"Pattern match features also benefit a neural model for disfluency detection alone (F = 87.0) (Zayats et al., 2016), and similar gains are observed by formulating disfluency detection in a transition-based framework (F = 87.5) (Wang et al., 2017).",6 Related Work,[0],[0]
"Experiments with oracle disfluencies as features improve the CL-attn text-only parsing performance from 87.85 to 89.38 on the test set, showing that more accurate disfluency modeling is a potential area of improvement.
",6 Related Work,[0],[0]
"It is well known that prosodic features play a role in human resolution of syntactic ambiguities, with more than two decades of studies seeking to incorporate prosodic features in parsing.",6 Related Work,[0],[0]
"A series of studies looked at constituent parsing informed by the presence (or likelihood) of prosodic breaks at word boundaries (Kahn et al., 2004, 2005; Hale et al., 2006; Dreyer and Shafran, 2007).",6 Related Work,[0],[0]
"Our approach improves over performance of these systems using raw acoustic features, without the need for handlabeling prosodic breaks.",6 Related Work,[0],[0]
"The gain is in part due to the improved text-based parser, but the incremental benefit of prosody here is similar to that in these prior studies.",6 Related Work,[0],[0]
"(In prior work using acoustic feature directly (Gregory et al., 2004), prosody actually degraded performance.)",6 Related Work,[0],[0]
"Our analyses of the impact of prosody also extends prior work.
",6 Related Work,[0],[0]
"Prosody is also known to provide useful cues to sentence boundaries (Liu et al., 2006), and automatic sentence segmentation performance has been shown to have a significant impact on parsing performance (Kahn and Ostendorf, 2012).",6 Related Work,[0],[0]
"In our study, sentence boundaries are given so as to focus on the role of prosody in resolving sentenceinternal parse ambiguity, for which prior work had
obtained smaller gains.",6 Related Work,[0],[0]
"Studies have also shown that parsing lattices or confusion networks can improve ASR performance (Kahn and Ostendorf, 2012; Yoshikawa et al., 2016).",6 Related Work,[0],[0]
"Our analysis of performance degradation for the system with prosody when the gold transcript and associated parse are in error suggests that prosody may have benefits for parsers operating on alternative ASR hypotheses.
",6 Related Work,[0],[0]
The results we compare to in Section 4 are relatively old.,6 Related Work,[0],[0]
"More recent parsing results on spontaneous speech involve dependency parsers using only text (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Yoshikawa et al., 2016), with the exception of a study on unsupervised dependency parsing (Pate and Goldwater, 2013).",6 Related Work,[0],[0]
"With the recent success of transition-based neural approaches in dependency parsing, researchers have adapted transition-based ideas to constituent parsing (Zhu et al., 2013; Watanabe and Sumita, 2015; Dyer et al., 2016).",6 Related Work,[0],[0]
"These approaches have not yet been used with speech, to our knowledge, but we expect it to be straightforward to extend our prosody integration framework to these systems, both for dependency and constituency parsing.",6 Related Work,[0],[0]
We have presented a framework for directly integrating acoustic-prosodic features with text in a neural encoder-decoder parser that does not require hand-annotated prosodic structure.,7 Conclusion,[0],[0]
"On conversational sentences, we obtained strong results when including word-level acoustic-prosodic features over using only transcriptions.",7 Conclusion,[0],[0]
"The acousticprosodic features provide the largest gains when sentences are disfluent or long, and analysis of error types shows that these features are especially helpful in repairing attachment errors.",7 Conclusion,[0],[0]
"In cases where prosodic features hurt performance, we observe a statistically significant negative effect caused by imperfect human transcriptions that make the “ground truth” parse tree and the acoustic signal inconsistent, which suggests that there is more to be gained from prosody than observed in prior studies.",7 Conclusion,[0],[0]
"We thus plan to investigate aligning the Treebank and MS-State versions of Switchboard for future work.
",7 Conclusion,[0],[0]
"Here, we assumed known sentence boundaries and hand transcripts, leaving open the question of whether increased benefits from prosody can be gained by incorporating sentence segmentation in parsing and/or in parsing ASR lattices.",7 Conclusion,[0],[0]
"Most prior work using prosody in parsing has been on con-
stituent parsing, since prosodic cues tend to align with constituent boundaries.",7 Conclusion,[0],[0]
"However, it remains an open question as to whether dependency, constituency or other parsing frameworks are better suited to leveraging prosody.",7 Conclusion,[0],[0]
"Our study builds on a parser that uses reverse order text processing, since it provides a stronger text-only baseline.",7 Conclusion,[0],[0]
"However, the prosody modeling component relies only on a 1 second lookahead of the current word (for pause binning), so it could be easily incorporated in an incremental parser.",7 Conclusion,[0],[0]
We thank the anonymous reviewers for their helpful feedback.,Acknowledgement,[0],[0]
"We also thank Pranava Swaroop Madhyastha, Hao Tang, Jon Cai, Hao Cheng, and Navdeep Jaitly for their help with initial discussions and code setup.",Acknowledgement,[0],[0]
"This research was partially funded by a Google Faculty Research Award to Mohit Bansal, Karen Livescu, and Kevin Gimpel; and NSF grant no. IIS-1617176.",Acknowledgement,[0],[0]
The opinions expressed in this work are those of the authors and do not necessarily reflect the views of the funding agency.,Acknowledgement,[0],[0]
"A.1 Miscellany
Our main model code is available at https://github.com/shtoshni92/ speech_parsing.",A Appendix,[0],[0]
Most of the data preprocessing code is available at https://github. com/trangham283/seq2seq_parser/ tree/master/src/data_preps.,A Appendix,[0],[0]
"Part of our data preprocessing pipeline also uses https: //github.com/syllog1sm/swbd_tools.
",A Appendix,[0],[0]
Table 8 shows statistics of our Switchboard dataset.,A Appendix,[0],[0]
"As defined, for example, in (Charniak and Johnson, 2001; Honnibal and Johnson, 2014), the splits are: conversations sw2000 to sw3000 for training, sw4500 to sw4936 for validation (dev), and sw4000 to sw4153 for evaluation (test).",A Appendix,[0],[0]
"In addition, previous work has reserved sw4154 to sw4500 for “future use” (dev2), but we added this set to our training set.",A Appendix,[0],[0]
"That is, all of our models are trained on Switchboard conversations sw2000 to sw3000 as well as sw4154 to sw4500.
",A Appendix,[0],[0]
Figure 4 illustrates the data preprocessing step.,A Appendix,[0],[0]
"On the decoder end, we also use a post-processing step that merges the original sentence with the decoder output to obtain the standard constituent tree representation.",A Appendix,[0],[0]
"During inference, in rare cases (and virtually none as our models converge), the decoder does not generate a valid parse sequence, due to the mismatch in brackets and/or the mismatch in the number of pre-terminals and terminals, i.e., num(XX) 6= num(tokens).",A Appendix,[0],[0]
"In such cases, we simply add/remove brackets from either end of the parse, or add/remove pre-terminal symbols XX in the middle of the parse to match the number of input tokens.
",A Appendix,[0],[0]
Figure 5 shows the distribution of pause durations in our training data.,A Appendix,[0],[0]
"Our pause buckets of
0 < p ≤ 0.05 s, 0.05 s < p ≤ 0.2 s, 0.2 < p ≤ 1 s, and p > 1 s described in the main paper were based on this distribution of pause lengths.
",A Appendix,[0],[0]
"Table 9 shows the comprehensive error counts in all error categories defined in the Berkeley Parse Analyzer (Kummerfeld et al., 2012) in both the fluent and disfluent subsets.
",A Appendix,[0],[0]
"A.2 Tree Examples In figures 6, 7, and 8, we follow node correction notations as in (Kummerfeld et al., 2012).",A Appendix,[0],[0]
"In particular, missing nodes are marked in blue on the gold tree, extra nodes are marked red in the predicted tree, and yellow nodes denote crossing.",A Appendix,[0],[0]
"In conversational speech, the acoustic signal provides cues that help listeners disambiguate difficult parses.",abstractText,[0],[0]
"For automatically parsing spoken utterances, we introduce a model that integrates transcribed text and acoustic-prosodic features using a convolutional neural network over energy and pitch trajectories coupled with an attention-based recurrent neural network that accepts text and prosodic features.",abstractText,[0],[0]
"We find that different types of acoustic-prosodic features are individually helpful, and together give statistically significant improvements in parse and disfluency detection F1 scores over a strong text-only baseline.",abstractText,[0],[0]
"For this study with known sentence boundaries, error analyses show that the main benefit of acousticprosodic features is in sentences with disfluencies, attachment decisions are most improved, and transcription errors obscure gains from prosody.",abstractText,[0],[0]
Parsing Speech: A Neural Approach to Integrating Lexical and Acoustic-Prosodic Information,title,[0],[0]
"General treebank analyses are graph structured, but parsers are typically restricted to tree structures for efficiency and modeling reasons. We propose a new representation and algorithm for a class of graph structures that is flexible enough to cover almost all treebank structures, while still admitting efficient learning and inference. In particular, we consider directed, acyclic, one-endpoint-crossing graph structures, which cover most long-distance dislocation, shared argumentation, and similar tree-violating linguistic phenomena. We describe how to convert phrase structure parses, including traces, to our new representation in a reversible manner. Our dynamic program uniquely decomposes structures, is sound and complete, and covers 97.3% of the Penn English Treebank. We also implement a proofof-concept parser that recovers a range of null elements and trace types.",text,[0],[0]
"Many syntactic representations use graphs and/or discontinuous structures, such as traces in Government and Binding theory and f-structure in Lexical Functional Grammar (Chomsky 1981; Kaplan and Bresnan 1982).",1 Introduction,[0],[0]
"Sentences in the Penn Treebank (PTB, Marcus et al. 1993) have a core projective tree structure and trace edges that represent control structures, wh-movement and more.",1 Introduction,[0],[0]
"However, most parsers and the standard evaluation metric ignore these edges and all null elements.",1 Introduction,[0],[0]
"By leaving out parts of the structure, they fail to provide key relations to downstream tasks such as question answering.",1 Introduction,[0],[0]
"While there has been work on capturing
some parts of this extra structure, it has generally either been through post-processing on trees (Johnson 2002; Jijkoun 2003; Campbell 2004; Levy and Manning 2004; Gabbard et al. 2006) or has only captured a limited set of phenomena via grammar augmentation (Collins 1997; Dienes and Dubey 2003; Schmid 2006; Cai et al. 2011).
",1 Introduction,[0],[0]
We propose a new general-purpose parsing algorithm that can efficiently search over a wide range of syntactic phenomena.,1 Introduction,[0],[0]
"Our algorithm extends a non-projective tree parsing algorithm (Pitler et al. 2013; Pitler 2014) to graph structures, with improvements to avoid derivational ambiguity while maintaining an O(n4) runtime.",1 Introduction,[0],[0]
"Our algorithm also includes an optional extension to ensure parses contain a directed projective tree of non-trace edges.
",1 Introduction,[0],[0]
Our algorithm cannot apply directly to constituency parses–it requires lexicalized structures similar to dependency parses.,1 Introduction,[1.0],['Our algorithm cannot apply directly to constituency parses–it requires lexicalized structures similar to dependency parses.']
We extend and improve previous work on lexicalized constituent representations (Shen et al. 2007; Carreras et al. 2008; Hayashi and Nagata 2016) to handle traces.,1 Introduction,[0],[0]
"In this form, traces can create problematic structures such as directed cycles, but we show how careful choice of head rules can minimize such issues.
We implement a proof-of-concept parser, scoring 88.1 on trees in section 23 and 70.6 on traces.",1 Introduction,[0],[0]
"Together, our representation and algorithm cover 97.3% of sentences, far above the coverage of projective tree parsers (43.9%).",1 Introduction,[0],[0]
"This work builds on two areas: non-projective tree parsing, and parsing with null elements.
",2 Background,[0],[0]
"Non-projectivity is important in syntax for rep-
441
Transactions of the Association for Computational Linguistics, vol. 5, pp.",2 Background,[0],[0]
"441–454, 2017.",2 Background,[0],[0]
Action Editor: Marco Kuhlmann.,2 Background,[0],[0]
"Submission batch: 4/2017; Published 11/2017.
",2 Background,[0],[0]
c©2017 Association for Computational Linguistics.,2 Background,[0],[0]
"Distributed under a CC-BY 4.0 license.
",2 Background,[0],[0]
"resenting many structures, but inference over the space of all non-projective graphs is intractable.",2 Background,[0],[0]
"Fortunately, in practice almost all parses are covered by well-defined subsets of this space.",2 Background,[0],[0]
"For dependency parsing, recent work has defined algorithms for inference within various subspaces (GómezRodrı́guez and Nivre 2010; Pitler et al. 2013).",2 Background,[0],[0]
We build upon these algorithms and adapt them to constituency parsing.,2 Background,[0],[0]
"For constituency parsing, a range of formalisms have been developed that are mildlycontext sensitive, such as CCG (Steedman 2000), LFG (Kaplan and Bresnan 1982), and LTAG (Joshi and Schabes 1997).
",2 Background,[0],[0]
"Concurrently with this work, Cao et al. (2017) also proposed a graph version of Pitler et al. (2013)’s One-Endpoint Crossing (1-EC) algorithm.",2 Background,[0],[0]
"However, Cao’s algorithm does not consider the direction of edges1 and so it could produce cycles, or graphs with multiple root nodes.",2 Background,[0],[0]
"Their algorithm also has spurious ambiguity, with multiple derivations of the same parse structure permitted.",2 Background,[0],[0]
"One advantage of their algorithm is that by introducing a new item type it can handle some cases of the Locked-Chain we define below (specifically, when N is even), though in practise they also restrict their algorithm to ignore such cases.",2 Background,[0],[0]
"They also show that the class of graphs they generate corresponds to the 1-EC pagenumber-2 space, a property that applies to this work as well2.
",2 Background,[0],[0]
Parsing with Null Elements in the PTB has taken two general approaches.,2 Background,[0],[0]
"The first broadly effective system was Johnson (2002), which post-processed the output of a parser, inserting extra elements.",2 Background,[1.0],"['The first broadly effective system was Johnson (2002), which post-processed the output of a parser, inserting extra elements.']"
"This was effective for some types of structure, such as null complementizers, but had difficulty with long distance dependencies.",2 Background,[1.0],"['This was effective for some types of structure, such as null complementizers, but had difficulty with long distance dependencies.']"
The other common approach has been to thread a trace through the tree structure on the non-terminal symbols.,2 Background,[1.0],['The other common approach has been to thread a trace through the tree structure on the non-terminal symbols.']
"Collins (1997)’s third model used this approach to recover wh-traces, while Cai et al. (2011) used it to recover null pronouns, and others have used it for a range of movement types (Dienes and Dubey 2003; Schmid 2006).",2 Background,[0],[0]
"These approaches have the disadvantage that each
1 To produce directed edges, their parser treats the direction as part of the edge label.
",2 Background,[0],[0]
2,2 Background,[0],[0]
This is a topological space with two half-planes sharing a boundary.,2 Background,[0],[0]
"All edges are drawn on one of the two half-planes and each half-plane contains no crossings.
",2 Background,[0],[0]
additional trace dramatically expands the grammar.,2 Background,[0],[0]
Our representation is similar to LTAG-Spinal (Shen et al. 2007) but has the advantage that it can be converted back into the PTB representation.,2 Background,[0],[0]
Hayashi and Nagata (2016) also incorporated null elements into a spinal structure but did not include a representation of co-indexation.,2 Background,[0],[0]
"In related work, dependency parsers have been used to assist in constituency parsing, with varying degrees of representation design, but only for trees (Hall, Nivre, and Nilsson 2007; Hall and Nivre 2008; FernándezGonzález and Martins 2015; Kong et al. 2015).
",2 Background,[1.0000000105036626],"['In related work, dependency parsers have been used to assist in constituency parsing, with varying degrees of representation design, but only for trees (Hall, Nivre, and Nilsson 2007; Hall and Nivre 2008; FernándezGonzález and Martins 2015; Kong et al. 2015).']"
"Kato and Matsubara (2016) described a new approach, modifying a transition-based parser to recover null elements and traces, with strong results, but using heuristics to determine trace referents.",2 Background,[0],[0]
"Our algorithm is a dynamic program, similar at a high level to CKY (Kasami 1966;",3 Algorithm,[0],[0]
Younger 1967; Cocke 1969).,3 Algorithm,[0],[0]
The states of our dynamic program (items) represent partial parses.,3 Algorithm,[0],[0]
"Usually in CKY, items are defined as covering the n words in a sentence, starting and ending at the spaces between words.",3 Algorithm,[0],[0]
"We follow Eisner (1996), defining items as covering the n−1 spaces in a sentence, starting and ending on words, as shown in Figure 1.",3 Algorithm,[0],[0]
"This means that we process each word’s left and right dependents separately, then combine the two halves.
",3 Algorithm,[0],[0]
"We use three types of items: (1) a single edge, linking two words, (2) a continuous span, going from one word to another, representing all edges linking pairs of words within the span, (3) a span (as defined in 2) plus an additional word outside the span, enabling the inclusion of edges between that word and words in the span.
",3 Algorithm,[0],[0]
"Within the CKY framework, the key to defining our algorithm is a set of rules that specify which items are allowed to combine.",3 Algorithm,[0],[0]
"From a bottom-up perspective, a parse is built in a series of steps, which come in three types: (1) adding an edge to an item, (2) combining two items that have non-overlapping adjacent spans to produce a new item with a larger span, (3) combining three items, similarly to (2).
",3 Algorithm,[0],[0]
"Example: To build intuition for the algorithm, we will describe the derivation in Figure 1.",3 Algorithm,[1.0],"['Example: To build intuition for the algorithm, we will describe the derivation in Figure 1.']"
"Note, item sub-types (I, X, and N) are defined below, and in-
cluded here for completeness.",3 Algorithm,[0],[0]
"(1) We initialize with spans of width one, going between adjacent words, e.g. between ROOT and We. ∅",3 Algorithm,[0.9999572033018341],"['(1) We initialize with spans of width one, going between adjacent words, e.g. between ROOT and We.']"
"7→ I0,1 (2) Edges can be introduced in exactly two ways, either by linking the two ends of a span, e.g. like– running, or by linking one end of a span with a word outside the span, e.g. like–. (which in this case forms a new item that has a span and an external word).
",3 Algorithm,[0.9852032101045103],"['∅ 7→ I0,1 (2) Edges can be introduced in exactly two ways, either by linking the two ends of a span, e.g. like– running, or by linking one end of a span with a word outside the span, e.g. like–.']"
"I2,3 ∧ like–running 7→ I2,3 I3,4 ∧ like–. 7→ X3,4,2
(3) We add a second edge to one of the items.",3 Algorithm,[0],[0]
"I1,2 ∧ running–We 7→ X1,2,3 (4) Now that all the edges to We have been added, the two items either side of it are combined to form an item that covers it.",3 Algorithm,[0],[0]
"I0,1 ∧ X1,2,3 7→",3 Algorithm,[0],[0]
"N0,2,3 (5) We add an edge, creating a crossing because We is an argument of a word to the right of like.",3 Algorithm,[0],[0]
"N0,2,3 ∧ ROOT–like 7→ N0,2,3 (7) We use a ternary rule to combine three adjacent items.",3 Algorithm,[0],[0]
"In the process we create another crossing.
",3 Algorithm,[0],[0]
"N0,2,3 ∧ I2,3 ∧ X3,4,2 7→ I0,6",3 Algorithm,[0],[0]
"Notation Vertices are p, q, etc.",3.1 Algorithm definition,[0],[0]
"Continuous ranges are [pq], [pq), (pq], or (pq), where the brackets indicate inclusion, [ ], or exclusion, ( ), of each endpoint.",3.1 Algorithm definition,[0],[0]
A span [pq] and vertex o that are part of the same item are [pq.o].,3.1 Algorithm definition,[0],[0]
"Two vertices and an arrow indicate an edge, ~pq.",3.1 Algorithm definition,[0],[0]
"Two vertices without an arrow are an edge in either direction, pq. Ranges and/or vertices connected by a dash define a set of edges, e.g. the
set of edges between o and (pq) is o–(pq) (in some places we will also use this to refer to an edge from the set, rather than the whole set).",3.1 Algorithm definition,[0],[0]
"If there is a path from p to q, q is reachable from p.
Item Types As shown in Figure 1, our items start and end on words, fully covering the spaces in between.",3.1 Algorithm definition,[0],[0]
"Earlier we described three item types: an edge, a span, and a span plus an external vertex.",3.1 Algorithm definition,[0],[0]
"Here we define spans more precisely as I , and divide the span plus an external point case into five types differing in the type of edge crossing they contain: p qI , Interval A span for which there are no edges sr :",3.1 Algorithm definition,[0],[0]
r ∈ (pq) and s /∈,3.1 Algorithm definition,[0],[0]
"[pq].
o X , Exterval An interval and either op or oq, where",3.1 Algorithm definition,[0],[0]
o /∈,3.1 Algorithm definition,[0],[0]
[pq].,3.1 Algorithm definition,[0],[0]
"B, Both A span and vertex [pq.o], for which there are no edges sr : r ∈ (pq) and s /∈",3.1 Algorithm definition,[0],[0]
"[pq] ∪ o. Edges o–[pq] may be crossed by pq, p–(pq) or q–(pq), and at least one crossing of the second and third types occurs.",3.1 Algorithm definition,[0],[0]
Edges o–(pq) may not be crossed by (pq)–(pq) edges.,3.1 Algorithm definition,[0],[0]
"L, Left Same as B, but o–(pq) edges may only cross p–(pq] edges.",3.1 Algorithm definition,[0],[0]
"R, Right Symmetric with L. N , Neither An interval and a vertex [pq.o], with at least one o–(pq) edge, which can be crossed by pq, but no other [pq]–[pq] edges.
",3.1 Algorithm definition,[0],[0]
Items are further specified as described in Alg.,3.1 Algorithm definition,[0],[0]
1.,3.1 Algorithm definition,[0],[0]
"Most importantly, for each pair of o, p, and q in an item, the rules specify whether one is a parent of the other, and if they are directly linked by an edge.
",3.1 Algorithm definition,[0],[0]
"For an item H with span [ij], define covered(H) as (ij), and define visible(H) as {i, j}.",3.1 Algorithm definition,[0],[0]
"When an external vertex x is present, it is in visible(H).",3.1 Algorithm definition,[0],[0]
"Call the union of multiple such sets covered(F,G,H), and visible(F,G,H).
",3.1 Algorithm definition,[0],[0]
Deduction Rules To make the deduction rules manageable,3.1 Algorithm definition,[0],[0]
", we use templates to define some constraints explicitly, and then use code to generate the rules.",3.1 Algorithm definition,[0],[0]
"During rule generation, we automatically apply additional constraints to prevent rules that would leave a word in the middle of a span without a parent or that would form a cycle (proven possible below).",3.1 Algorithm definition,[0],[0]
Algorithm 1 presents the explicit constraints.,3.1 Algorithm definition,[0],[0]
"Once expanded, these give rules that specify all properties for each item (general type, external vertex position
Algorithm 1 Dynamic program for Lock-Free, One-Endpoint Crossing, Directed, Acyclic graph parsing.",3.1 Algorithm definition,[0],[0]
Adding Edges: Consider a span [lr] and vertex x /∈,3.1 Algorithm definition,[0],[0]
[lr].,3.1 Algorithm definition,[0],[0]
"Edges between l and r can be added to items I , N , L, R, and B (making L̂ and N̂ in those cases).",3.1 Algorithm definition,[0],[0]
"Edges between l and x can be added to items I (forming an X), R, and N .",3.1 Algorithm definition,[0],[0]
"Edges between r and x can be added to items I (forming an X), L, and N .",3.1 Algorithm definition,[0],[0]
"The l–r edge cannot be added after another edge, and N items cannot get both l–x and r–x edges.",3.1 Algorithm definition,[0],[0]
Combining Items: In the rules below the following notation is used: For this explanation items are T,3.1 Algorithm definition,[0],[0]
[lr crl clr] and T,3.1 Algorithm definition,[0],[0]
[lrx crl cxl clr cxr clx crx].,3.1 Algorithm definition,[0],[0]
T is the type of item.,3.1 Algorithm definition,[0],[0]
Multiple letters indicate any of those types are allowed.,3.1 Algorithm definition,[0],[0]
"For the next three types of notation, if an item does not have a mark, either option is valid.",3.1 Algorithm definition,[0],[0]
˙ T and T : indicate the number of edges between the external vertex and the span: one or more than one respectively.,3.1 Algorithm definition,[0],[0]
·T and T · indicate the position of the external vertex relative to the item’s span (left or right respectively).,3.1 Algorithm definition,[0],[0]
T̂ indicates for N and L that ∀p ∈,3.1 Algorithm definition,[0],[0]
(ij)∃rs : i≤r<p<s≤j.,3.1 Algorithm definition,[0],[0]
"In (11) and (12) it is optional, but true for output iff true for input.",3.1 Algorithm definition,[0],[0]
"l, r, and x: the position of the left end of the span, the right end, and the external vertex, respectively.",3.1 Algorithm definition,[0],[0]
"crl, cxl, etc: connectivity of each pair of visible vertices, from the first subscript to the second.",3.1 Algorithm definition,[0],[0]
"Using crl as an example, these can be .",3.1 Algorithm definition,[0],[0]
"(unconstrained), d ( ~rl must exist), p (l is reachable from r, but ~rl does not exist), n",3.1 Algorithm definition,[0],[0]
"(l is not reachable from r), d (= p ∨ n), n",3.1 Algorithm definition,[0],[0]
(= d ∨ p).,3.1 Algorithm definition,[0],[0]
Note:,3.1 Algorithm definition,[0],[0]
"In the generated rules every value is d, p, or n, leading to multiple rules per template below.
I[ij nd]← max   (Init) j = i+1 (1) I[i i+1 nn] I[i+1 j nn] maxk∈(i,j)   (2) I[ik nd] I[kj",3.1 Algorithm definition,[0],[0]
..],3.1 Algorithm definition,[0],[0]
(3) BLRN · [ikj nndddd] I[kj ..],3.1 Algorithm definition,[0],[0]
"maxl∈(k,j){ (4) RN ·",3.1 Algorithm definition,[0],[0]
[ikl nndddd] I[kl ..] ·LNX[ljk .d..d.] (5) BLRN · [ikl nndddd] I[kl ..] I[lj ..],3.1 Algorithm definition,[0],[0]
"maxl∈(i,k){ (6) I[il n.] ·LN [lki .d.dnn]",3.1 Algorithm definition,[0],[0]
"·N : [kjl ddd.d.]
(7) RNX· [ilk nn.ddd] I[lk ..]",3.1 Algorithm definition,[0],[0]
"·LN :: [kjl .d..d.]
B· [ijx nndddd]← maxk∈(i,j)  
(8) L̂N̂ ·",3.1 Algorithm definition,[0],[0]
[ikx nn.ddd] R·,3.1 Algorithm definition,[0],[0]
[kjx ...d.d] (9) L̂N̂ ·,3.1 Algorithm definition,[0],[0]
[ikx nn.ddd] N ·,3.1 Algorithm definition,[0],[0]
[kjx d.dd.d] (10) L̂N̂ ·,3.1 Algorithm definition,[0],[0]
[ikx nn.ddd] N ·,3.1 Algorithm definition,[0],[0]
"[kjx d.dd.d]
˙ L̂[ijx dddddd]←",3.1 Algorithm definition,[0],[0]
"maxk∈(i,j){
(11) X[ikx .d.dnn] · L̂N̂",3.1 Algorithm definition,[0],[0]
[kji .d.ddd] (12) X[ikx .d.ddd,3.1 Algorithm definition,[0],[0]
],3.1 Algorithm definition,[0],[0]
· L̂N̂,3.1 Algorithm definition,[0],[0]
"[kji .d.ddd]
L :",3.1 Algorithm definition,[0],[0]
"[ijx dddddd]← maxk∈(i,j)   (13) LN",3.1 Algorithm definition,[0],[0]
[ikx .d.ddd],3.1 Algorithm definition,[0],[0]
·N,3.1 Algorithm definition,[0],[0]
[kji dddddd] (14) LN,3.1 Algorithm definition,[0],[0]
[ikx .d.ddd],3.1 Algorithm definition,[0],[0]
·N,3.1 Algorithm definition,[0],[0]
[kji dddddd] (15) L[ikx .d.ddd] I[kj ..],3.1 Algorithm definition,[0],[0]
(16) L[ikx .d.ddd] I[kj ..],3.1 Algorithm definition,[0],[0]
(17) N,3.1 Algorithm definition,[0],[0]
[ikx dddddd] I[kj ..],3.1 Algorithm definition,[0],[0]
(18) N,3.1 Algorithm definition,[0],[0]
[ikx dddddd] I[kj ..],3.1 Algorithm definition,[0],[0]
(19) N,3.1 Algorithm definition,[0],[0]
[ikx dddddd] I[kj ..,3.1 Algorithm definition,[0],[0]
"]
(20) N",3.1 Algorithm definition,[0],[0]
"[ikx dddddd] I[kj ..]
N :",3.1 Algorithm definition,[0],[0]
"[ijx dddddd]← maxk∈(i,j)   (21) ·N [ikx dddddd] I[kj ..]",3.1 Algorithm definition,[0],[0]
(22) ·N [ikx dddddd] I[kj ..] (23) I[ik ..] N ·,3.1 Algorithm definition,[0],[0]
[kjx dddddd] (24) I[ik ..] N ·,3.1 Algorithm definition,[0],[0]
"[kjx dddddd]
˙ N",3.1 Algorithm definition,[0],[0]
"[ijx dddddd]← maxk∈(i,j)   (25) ·X[ikx .d.ddd] I[kj ..] (26) ·X[ikx .d.ddd] I[kj ..] (27) I[ik ..]",3.1 Algorithm definition,[0],[0]
X· [kjx .d.ddd] (28) I[ik ..],3.1 Algorithm definition,[0],[0]
"X· [kjx .d.ddd]
I[ij pn], ·B[ijx ddnndd], R : [ijx dddddd], and ˙ R[ijx dddddd] are symmetric with cases above.
",3.1 Algorithm definition,[0],[0]
"relative to the item spans, connectivity of every pair of vertices in each item, etc).
",3.1 Algorithm definition,[0],[0]
The final item for n vertices is an interval where the left end has a parent.,3.1 Algorithm definition,[0],[0]
For parsing we assume there is a special root word at the end of the sentence.,3.1 Algorithm definition,[0],[0]
Definition 1.,3.2 Properties,[0],[0]
"A graph is One-Endpoint Crossing if, when drawn with vertices along the edge of a halfplane and edges drawn in the open half-plane above, for any edge e, all edges that cross e share a vertex.",3.2 Properties,[0],[0]
"Let that vertex be Pt(e).
",3.2 Properties,[0],[0]
"Aside from applying to graphs, this is the same as
Pitler et al. (2013)’s 1-EC tree definition.
",3.2 Properties,[0],[0]
Definition 2.,3.2 Properties,[0],[0]
"A Locked-Chain (shown in Fig. 2) is formed by a set of consecutive vertices in order from 0 to N , where N > 3, with edges {(0, N−1), (1, N)} ∪ {(i, i+2)∀i ∈",3.2 Properties,[0],[0]
"[0, N−2]}.",3.2 Properties,[0],[0]
Definition 3.,3.2 Properties,[0],[0]
"A graph is Lock-Free if it does not contain edges that form a Locked-Chain.
",3.2 Properties,[0],[0]
"Note that in practice, most parse structures satisfy 1-EC, and the Locked-Chain structure does not occur in the PTB when using our head rules.
",3.2 Properties,[0],[0]
Theorem 1.,3.2 Properties,[0],[0]
"For the space of Lock-Free OneEndpoint Crossing graphs, the algorithm is sound, complete and gives unique decompositions.
",3.2 Properties,[0],[0]
Our proof is very similar in style and structure to Pitler et al. (2013).,3.2 Properties,[0],[0]
"The general approach is to consider the set of structures an item could represent, and divide them into cases based on properties of the internal structure.",3.2 Properties,[0],[0]
"We then show how each case can be decomposed into items, taking care to ensure all the properties that defined the case are satisfied.",3.2 Properties,[0],[0]
Uniqueness follows from having no ambiguity in how a given structure could be decomposed.,3.2 Properties,[0],[0]
"Completeness and soundness follow from the fact that our rules apply equally well in either direction, and so our top-down decomposition implies a bottom-up formation.",3.2 Properties,[0],[0]
"To give intuition for the proof, we show the derivation of one rule below.",3.2 Properties,[0],[0]
The complete proof can be found in Kummerfeld (2016).,3.2 Properties,[0],[0]
"We do not include it here due to lack of space.
",3.2 Properties,[0],[0]
"We do provide the complete set of rule templates in Algorithm 1, and in the proof of Lemma 2 we show that the case in which an item cannot be decomposed occurs if and only if the graph contains a Locked-Chain.",3.2 Properties,[0],[0]
"To empirically check our rule generation code, we checked that our parser uniquely decomposes all 1-EC parses in the PTB and is unable to decompose the rest.
",3.2 Properties,[0],[0]
"Note that by using subsets of our rules, we can restrict the space of structures we generate, giving parsing algorithms for projective DAGs, projective trees (Eisner 1996), or 1-EC trees (Pitler et al. 2013).",3.2 Properties,[0],[0]
"Versions of these spaces with undirected edges could also be easily handled with the same approach.
",3.2 Properties,[0],[0]
p qs t Derivation of rule (4) in Algorithm 1:,3.2 Properties,[0],[0]
"This rule applies to intervals with the substructure shown, and with no parent in this item for p.",3.2 Properties,[0],[0]
They have at least one p–(pq) edge (otherwise rule 1 applies).,3.2 Properties,[0],[0]
"The longest p–(pq) edge, ps, is crossed (otherwise rule 2 applies).",3.2 Properties,[0],[0]
Let C be the set of (ps)–(sq) edges (note: these cross ps).,3.2 Properties,[0],[0]
"Either all of the edges in C have a common endpoint t ∈ (sq), or if |C| = 1 let t be the endpoint in (sq) (otherwise rule 6 or 7 applies).",3.2 Properties,[0],[0]
Let D be the set of s–(tq) edges.,3.2 Properties,[0],[0]
|D| > 0,3.2 Properties,[0],[0]
"(otherwise rule 3 or 5 applies).
",3.2 Properties,[0],[0]
We will break this into three items.,3.2 Properties,[0],[0]
"First, (st)–(tq] edges would violate the 1-EC property and (st)–[ps) edges do not exist by construction.",3.2 Properties,[0],[0]
"Therefore, the middle item is an Interval [st], the left item is [ps.t], and the right item is [tq.s] (since |C| > 0 and |D| > 0).",3.2 Properties,[0],[0]
"The left item can be either
an N or R, but not an L or B because that would violate the 1-EC property for the C edges.",3.2 Properties,[0],[0]
"The right item can be an X , L, or N , but not an R or B because that would violate the 1-EC property for the D edges.",3.2 Properties,[0],[0]
"We will require edge ps to be present in the first item, and not allow pt.",3.2 Properties,[0],[0]
"To avoid a spurious ambiguity, we also prevent the first or third items from having st (which could otherwise occur in any of the three items).",3.2 Properties,[0],[0]
"Now we have broken down the original item into valid sub-items, and we have ensured that those sub-items contain all of the structure used to define the case in a unique way.
",3.2 Properties,[0],[0]
"Now we will further characterize the nature of the Lock-Free restriction to the space of graphs.
",3.2 Properties,[0],[0]
Lemma 1.,3.2 Properties,[0],[0]
No edge in a Locked-Chain in a 1-EC graph is crossed by edges that are not part of it.,3.2 Properties,[0],[0]
Proof.,3.2 Properties,[0],[0]
"First, note that: Pt((0, N−1))",3.2 Properties,[0],[0]
"= N , Pt((1, N))",3.2 Properties,[0],[0]
"= 0, and {Pt((i, i+2))",3.2 Properties,[0],[0]
= i+1,3.2 Properties,[0],[0]
∀i ∈,3.2 Properties,[0],[0]
"[0, N−2]} Call the set {(i, i+2)∀i ∈",3.2 Properties,[0],[0]
"[0, N−2]}, the chain.
Consider an edge e that crosses an edge f in a Locked-Chain.",3.2 Properties,[0],[0]
"Let ein be the end of e that is between the two ends of f , and eout be the other end.",3.2 Properties,[0],[0]
"One of e’s endpoints is at Pt(f), and Pt(e) is an endpoint of f .",3.2 Properties,[0],[0]
"There are three cases:
(i) f",3.2 Properties,[0],[0]
"= (1, N).",3.2 Properties,[0],[0]
"Here, eout = Pt(f) = 0, and ein ∈ (1, N).",3.2 Properties,[0],[0]
"For all vertices v ∈ (1, N) there is an edge g in the chain such that v is between the endpoints of g. Therefore, e will cross such an edge g. To satisfy the 1-EC property, g must share an endpoint with f , which means g is either (1, 3) or (N−2, N).",3.2 Properties,[0],[0]
"In the first case, the 1-EC property forces e = (0, 2), and in the second e = (0, N−1), both of which are part of the Locked-Chain.
(ii) f = (0, N−1), symmetrical with (i).",3.2 Properties,[0],[0]
"(iii) f = (i, i+2), for some i ∈",3.2 Properties,[0],[0]
"[0, N−2].",3.2 Properties,[0],[0]
"Here, ein = Pt(f) = i+1.",3.2 Properties,[0],[0]
"We can assume e does not cross (0, N−1) or (1, N), as those cases are covered by (i).",3.2 Properties,[0],[0]
"As in (i), e must cross another edge in the chain, and that edge must share an endpoint with f .
",3.2 Properties,[0],[0]
"This forces e to be either (i−1, i+1) or (i+1, i+3) (excluding one or both if they cross (0, N−1) or (1, N)), which are both in the Locked-Chain.
",3.2 Properties,[0],[0]
Our rules define a unique way to decompose almost any item into a set of other items.,3.2 Properties,[0],[0]
"The exception is B, which in some cases can not be divided into two items (i.e. has no valid binary division).
",3.2 Properties,[0],[0]
Lemma 2.,3.2 Properties,[0],[0]
A B[ij.x] has no valid binary division if and only if the graph has a Locked-Chain.,3.2 Properties,[0],[0]
Proof.,3.2 Properties,[0],[0]
Consider the k and l that give the longest ik and lj edges in a B with no valid binary division (at least one edge of each type must exist by definition).,3.2 Properties,[0],[0]
"No vertex in (ik) or (jl) is a valid split point, as they would all require one of the items to have two external vertices.
",3.2 Properties,[0],[0]
"Now, consider p ∈",3.2 Properties,[0],[0]
[kj].,3.2 Properties,[0],[0]
"If there is no edge l1r1, where i ≤ l1 < p < r1 ≤ j, then p would be a valid split point.",3.2 Properties,[0],[0]
"Therefore, such an edge must exist.",3.2 Properties,[0],[0]
"Consider l1, either l1 ∈ (ik) or there is an edge l2c, where i ≤ l2",3.2 Properties,[0],[0]
< l1 < c ≤ j (by the same logic as for l1r1).,3.2 Properties,[0],[0]
"Similarly, either r1 ∈ (jl) or there is an edge cr2 (it must be c to satisfy 1-EC).",3.2 Properties,[0],[0]
"We can also apply this logic to edges l2c and cr2, giving edges l3l1 and r1r3.",3.2 Properties,[0],[0]
This pattern will terminate when it reaches lu ∈ (ik) and rv ∈ (jl) with edges lulu−2 and rv−2rv.,3.2 Properties,[0],[0]
"Note that k = lu−1 and l = rv−1, to satisfy 1-EC.
",3.2 Properties,[0],[0]
"Since it is a B, there must be at least two x–(ij) edges.",3.2 Properties,[0],[0]
"To satisfy 1-EC, these end at lu−1 and rv−1.
",3.2 Properties,[0],[0]
"Let x be to the right (the left is symmetrical), and call i = 0, j = N−1, and x = N .",3.2 Properties,[0],[0]
"Comparing with the Locked-Chain definition, we have all the edges except one: 0 to N−1.",3.2 Properties,[0],[0]
"However, that edge must be present in the overall graph, as all B items start with an ij edge (see rules 3 and 5 in Algorithm 1).",3.2 Properties,[0],[0]
"Therefore, if there is no valid split point for a B, the overall graph must contain a Locked-Chain.
",3.2 Properties,[0],[0]
"Now, for a graph that contains a Locked-Chain, consider the items that contain the Locked-Chain.",3.2 Properties,[0],[0]
"Grouping them by their span [ij], there are five valid options:",3.2 Properties,[0],[0]
"[0, N−1], [1, N ], [0, N ], (i ≤ 0",3.2 Properties,[0],[0]
"∧ j > N ), and (i < 0",3.2 Properties,[0],[0]
∧ j ≥ N ).,3.2 Properties,[0],[0]
"Items of the last three types would be divided by our rules into smaller items, one of which contains the whole Locked-Chain.",3.2 Properties,[0],[0]
"The first two are Bs of the type discussed above.
",3.2 Properties,[0],[0]
"Now we will prove that our code to generate rules from the templates can guarantee a DAG is formed.
",3.2 Properties,[0],[0]
Lemma 3.,3.2 Properties,[0],[0]
"For any item H , ∀v ∈ covered(H) ∃u ∈ visible(H) : v is reachable from u. Proof.",3.2 Properties,[0],[0]
"This is true for initial items because covered(H) = ∅. To apply induction, consider adding edges and combing items.",3.2 Properties,[0],[0]
The lemma clearly remains true when adding an edge.,3.2 Properties,[0],[0]
"Consider combining items E, F , G to form H[ij.x], and assume the lemma is true for E, F , and G (the binary case is similar).",3.2 Properties,[0],[0]
"Since all vertices are reachable from visible(E,F,G), we only need to ensure that ∀v ∈",3.2 Properties,[0],[0]
"visible(E,F,G) ∃u ∈",3.2 Properties,[0],[0]
visible(H) : v is reachable from u.,3.2 Properties,[0],[0]
"The connectivity between all pairs {(u, v) | u ∈ visible(H), v ∈",3.2 Properties,[0],[0]
"visible(E,F,G)} can be inferred from the item definitions, and so this requirement can be enforced in rule generation.
",3.2 Properties,[0],[0]
Lemma 4.,3.2 Properties,[0],[0]
The final item is a directed acyclic graph.,3.2 Properties,[0],[0]
Proof.,3.2 Properties,[0],[0]
"First, consider acyclicity.",3.2 Properties,[0],[0]
Initial items do not contain any edges and so cannot contain a cycle.,3.2 Properties,[0],[0]
"For induction, there are two cases:
(i) Adding an Edge ~pq to an item H: Assume that H does not contain any cycles.",3.2 Properties,[0],[0]
"~pq will create a cycle if and only if p is reachable from q. By construction, p and q ∈ visible(H), and so the item definition contains whether p is reachable from q.
(ii)",3.2 Properties,[0],[0]
"Combining Items: Assume that in isolation, none of the items being combined contain cycles.",3.2 Properties,[0],[0]
"Therefore, a cycle in the combined item must be composed of paths in multiple items.",3.2 Properties,[0],[0]
A path in one item can only continue in another item by passing through a visible vertex.,3.2 Properties,[0],[0]
"Therefore, a cycle would have to be formed by a set of paths between visible vertices.",3.2 Properties,[0],[0]
"But the connectivity of every pair of visible vertices is specified in the item definitions.
",3.2 Properties,[0],[0]
"In both cases, rules that create a cycle can be excluded during rule generation.
",3.2 Properties,[0],[0]
"By induction, the items constructed by our algorithm do not contain cycles.",3.2 Properties,[0],[0]
"Together with Lemma 3 and the final item definition, this means the final structure is an acyclic graph with all vertices reachable from vertex n.
Next, we will show two properties that give intuition for the algorithm.",3.2 Properties,[0],[0]
"Specifically, we will prove which rules add edges that are crossed in the final derivation.
",3.2 Properties,[0],[0]
Lemma 5.,3.2 Properties,[0],[0]
An edge ij added to I[ij] is not crossed.,3.2 Properties,[0],[0]
Proof.,3.2 Properties,[0],[0]
"First, we will show three properties of any pair of items in a derivation (using [ij.x] and [kl.y]).
",3.2 Properties,[0],[0]
(1) It is impossible for either i <,3.2 Properties,[0],[0]
k,3.2 Properties,[0],[0]
< j < l or k,3.2 Properties,[0],[0]
< i,3.2 Properties,[0],[0]
<,3.2 Properties,[0],[0]
"l < j, i.e., items cannot have partially overlapping spans.",3.2 Properties,[0],[0]
"As a base case, the final item is an interval spanning all vertices, and so no other item can partially overlap with it.",3.2 Properties,[0],[0]
"Now assume it is true for an item H and consider the rules in reverse, breaking H up.",3.2 Properties,[0],[0]
"By construction, each rule divides H into items with spans that are adjacent, overlapping only at their visible vertices.",3.2 Properties,[0],[0]
"Since the new items are nested within H , they do not overlap with any items H did not overlap with.",3.2 Properties,[0],[0]
"By induction, no pair of items have partially overlapping spans.
",3.2 Properties,[0],[0]
(2) For items with nested spans (i ≤ k,3.2 Properties,[0],[0]
"< l ≤ j), y ∈",3.2 Properties,[0],[0]
[ij]∪{x}.,3.2 Properties,[0],[0]
"Following the argument for the previous case, the [ij.x] item must be decomposed into a set of items that includes [kl.y].",3.2 Properties,[0],[0]
"Now, consider how those items are combined.",3.2 Properties,[0],[0]
"The rules that start with an item with an external vertex produce an item that either has the same external vertex, or with the external vertex inside the span of the new item.",3.2 Properties,[0],[0]
"Therefore, y must either be equal to x or inside [ij].
(3) For items without nested spans, x /∈",3.2 Properties,[0],[0]
(kl).,3.2 Properties,[0],[0]
Assume x ∈,3.2 Properties,[0],[0]
(kl) for two items without nested spans.,3.2 Properties,[0],[0]
"None of the rules combine such a pair of items, or allow one to be extended so that the other is nested within it.",3.2 Properties,[0],[0]
"However, all items are eventually combined to complete the derivation.",3.2 Properties,[0],[0]
"By contradiction, x /∈ (kl).
",3.2 Properties,[0],[0]
"Together, these mean that given an interval H with span [ij], and another item G, either ∀v ∈ visible(G), v ∈",3.2 Properties,[0],[0]
"[ij] or ∀v ∈ visible(G),",3.2 Properties,[0],[0]
v /∈,3.2 Properties,[0],[0]
(ij).,3.2 Properties,[0],[0]
"Since edges are only created between visible vertices, no edge can cross edge ij.
Lemma 6.",3.2 Properties,[0.9744972399421606],"['Since edges are only created between visible vertices, no edge can cross edge ij.']"
"All edges aside from those considered in Lemma 5 are crossed.
",3.2 Properties,[0],[0]
Proof.,3.2 Properties,[0],[0]
"First, consider an edge ij added to an item [ij.x] of type B, L, R, or N. This edge is crossed by all x–(ij) edges, and in these items |x–(ij)| ≥ 1 by definition.",3.2 Properties,[1.0],"['First, consider an edge ij added to an item [ij.x] of type B, L, R, or N. This edge is crossed by all x–(ij) edges, and in these items |x–(ij)| ≥ 1 by definition.']"
"Note, by the same argument as Lemma 5, the edge is not crossed later in the derivation.
",3.2 Properties,[0],[0]
"Second, consider adding e ∈ {xi, xj}, to H , an item with [ij] or [ij.x], forming an item G[ij.x].",3.2 Properties,[0],[0]
"Note, e does not cross any edges in H .",3.2 Properties,[0],[0]
Let E(F,3.2 Properties,[0],[0]
[kl.y]) be the set of y–[kl] edges in some item F .,3.2 Properties,[0],[0]
Note that e ∈ E(G).,3.2 Properties,[0],[0]
"We will show how this set of edges is affected by the rules and what that implies for e. Consider each input item A[kl.y] in each
rule, with output item C. Every item A falls into one of four categories: (1) ∀f ∈ E(A), f is crossed by an edge in another of the rule’s input items, (2) E(A) ⊆ E(C), (3) A∧ kl 7→ C",3.2 Properties,[0.9841563144404584],"['We will show how this set of edges is affected by the rules and what that implies for e. Consider each input item A[kl.y] in each rule, with output item C. Every item A falls into one of four categories: (1) ∀f ∈ E(A), f is crossed by an edge in another of the rule’s input items, (2) E(A) ⊆ E(C), (3) A∧ kl 7→ C and there are no ky or ly edges in A, (4) A contains edge kl and there are no ky or ly edges in A.']"
"and there are no ky or ly edges in A, (4)",3.2 Properties,[0],[0]
"A contains edge kl and there are no ky or ly edges in A.
Cases 2-4 are straightforward to identify.",3.2 Properties,[0],[0]
"For an example of the first case, consider the rightmost item in rule 4.",3.2 Properties,[1.0],"['For an example of the first case, consider the rightmost item in rule 4.']"
"The relevant edges are k–(lj] (by construction, kl is not present).",3.2 Properties,[0],[0]
"Since the leftmost item is either an R or N, |l–(ik)| ≥ 1.",3.2 Properties,[0],[0]
Since i < k,3.2 Properties,[0],[0]
"< l < j, all k–(lj] edges will cross all l–[ik) edges.",3.2 Properties,[0.9558826250153175],"['Since i < k < l < j, all k–(lj] edges will cross all l–[ik) edges.']"
"Therefore applying this rule will cross all k–(lj] edges in the rightmost item.
",3.2 Properties,[0],[0]
"Initially, e is not crossed and e ∈ E(G).",3.2 Properties,[0],[0]
"For each rule application, edges in E(A) are either crossed (1 and 3), remain in the set E(C)",3.2 Properties,[0],[0]
"(2), or must already be crossed (4).",3.2 Properties,[0],[0]
Since the final item is an interval and E(Interval) =,3.2 Properties,[0],[0]
"∅, there must be a subsequent rule that is not in case 2.",3.2 Properties,[0],[0]
Therefore e will be crossed.,3.2 Properties,[0],[0]
"Our algorithm is based on Pitler et al. (2013), which had the crucial idea of One-Endpoint crossing and a complete decomposition of the tree case.",3.3 Comparison with Pitler et al. (2013),[0],[0]
"Our changes and extensions provide several benefits:
Extension to graphs: By extending to support multiple parents while preventing cycles, we substantially expand the space of generatable structures.
",3.3 Comparison with Pitler et al. (2013),[0],[0]
Uniqueness:,3.3 Comparison with Pitler et al. (2013),[0],[0]
By avoiding derivational ambiguity we reduce the search space and enable efficient summing as well as maxing.,3.3 Comparison with Pitler et al. (2013),[0],[0]
Most of the cases in which ambiguity arises in Pitler et al. (2013)’s algorithm are due to symmetry that is not explicitly broken.,3.3 Comparison with Pitler et al. (2013),[0],[0]
"For example, the rule we worked through in the previous section defined t ∈ (sq)",3.3 Comparison with Pitler et al. (2013),[0],[0]
when |C| = 1.,3.3 Comparison with Pitler et al. (2013),[0],[0]
"Picking t ∈ (ps) would also lead to a valid set of rules, but allowing either creates a spurious ambiguity.",3.3 Comparison with Pitler et al. (2013),[0],[0]
"This ambiguity is resolved by tracking whether there is only one edge to the external vertex or more than one, and requiring more than one in rules 6 and 7.",3.3 Comparison with Pitler et al. (2013),[0],[0]
"Other changes include ensuring equivalent structures cannot be represented by multiple item types and enforcing a unique split point in B items.
",3.3 Comparison with Pitler et al. (2013),[0],[0]
"More concise algorithm definition: By separating edge creation from item merging, and defining our rules via a combination of templates and code, we are able to define our algorithm more concisely.",3.3 Comparison with Pitler et al. (2013),[0],[0]
Edge labels can be added by calculating either the sum or max over edge types when adding each edge.,3.4.1 Edge Labels and Word Labels,[0],[0]
"Word labels (e.g., POS Tags) must be added to the state, specifying a label for each visible word (p, q and o).",3.4.1 Edge Labels and Word Labels,[0],[0]
This state expansion is necessary to ensure agreement when combining items.,3.4.1 Edge Labels and Word Labels,[0],[0]
"Our algorithm constrains the space of graph structures, but we also want to ensure that our parse contains a projective tree of non-trace edges.
",3.4.2 Ensuring a Structural Tree is Present,[0],[0]
"To ensure every word gets one and only one structural parent, we add booleans to the state, indicating whether p, q and o have structural parents.",3.4.2 Ensuring a Structural Tree is Present,[0],[0]
"When adding edges, a structural edge cannot be added if a word already has a structural parent.",3.4.2 Ensuring a Structural Tree is Present,[0],[0]
"When combining items, no word can receive more than one structural parent, and words that will end up in the middle of the span must have exactly one.",3.4.2 Ensuring a Structural Tree is Present,[0],[0]
"Together, these constraints ensure we have a tree.
",3.4.2 Ensuring a Structural Tree is Present,[0],[0]
"To ensure the tree is projective, we need to prevent structural edges from crossing.",3.4.2 Ensuring a Structural Tree is Present,[0],[0]
"Crossing edges are introduced in two ways, and in both we can avoid structural edges crossing by tracking whether there are structural o–[pq] edges.",3.4.2 Ensuring a Structural Tree is Present,[0],[0]
"Such edges are present if a rule adds a structural op or oq edge, or if a rule combines an item with structural o–[pq] edges and o will still be external in the item formed by the rule.
",3.4.2 Ensuring a Structural Tree is Present,[0],[0]
"For adding edges, every time we add a pq edge in the N , L, R and B items we create a crossing with all o–(pq) edges.",3.4.2 Ensuring a Structural Tree is Present,[0],[0]
"We do not create a crossing with oq or op, but our ordering of edge creation means these are not present when we add a pq edge, so tracking structural o–[pq] edges gives us the information we need to prevent two structural edges crossing.
",3.4.2 Ensuring a Structural Tree is Present,[0],[0]
"For combining items, in Lemma 6 we showed that during combinations, o–[pq] edges in each pair of items will cross.",3.4.2 Ensuring a Structural Tree is Present,[0],[0]
"As a result, knowing whether any o–[pq] edge is structural is sufficient to determine whether two structural edges will cross.",3.4.2 Ensuring a Structural Tree is Present,[0],[0]
"Consider a sentence with n tokens, and let E and S be the number of edge types and word labels in our grammar respectively.
",3.5 Complexity,[0],[0]
"Parses without word or edge labels: Rules have up to four positions, leading to complexity of O(n4).",3.5 Complexity,[0],[0]
"Note, there is also an important constant–once our templates are expanded, there are 49,292 rules.
",3.5 Complexity,[0],[0]
"With edge labels: When using a first-order model, edge labels only impact the rules for edge creation, leading to a complexity of O(n4 + En2).
",3.5 Complexity,[0],[0]
"With word labels: Since we need to track word labels in the state, we need to adjust every n by a factor of S, leading to O(S4n4 + ES2n2).",3.5 Complexity,[0],[0]
Our algorithm relies on the assumption that we can process the dependents to the left and right of a word independently and then combine the two halves.,4 Parse Representation,[0],[0]
"This means we need lexicalized structures, which the PTB does not provide.",4 Parse Representation,[0],[0]
We define a new representation in which each non-terminal symbol is associated with a specific word (the head).,4 Parse Representation,[0],[0]
"Unlike dependency parsing, we retain all the information required to reconstruct the constituency parse.
",4 Parse Representation,[0],[0]
"Our approach is related to Carreras et al. (2008) and Hayashi and Nagata (2016), with three key differences: (1) we encode non-terminals explicitly, rather than implicitly through adjunction operations, which can cause ambiguity, (2) we add representations of null elements and co-indexation, (3) we modify head rules to avoid problematic structures.
",4 Parse Representation,[0.953445032300873],"['Coarse to Fine Pruning: Rather than parsing immediately with the full model we use several passes with progressively richer structure (Goodman 1997): (1) Projective parsing without traces or spines, and simultaneously a trace classifier, (2) Non-projective parsing without spines, and simultaneously a spine classifier, (3) Full structure parsing.']"
Figure 3 shows a comparison of the PTB representation and ours.,4 Parse Representation,[0],[0]
"We add lexicalization, assigning each non-terminal to a word.",4 Parse Representation,[0],[0]
"The only other changes are visual notation, with non-terminals moved to be directly above the words to more clearly show the distinction between spines and edges.
",4 Parse Representation,[0],[0]
Spines:,4 Parse Representation,[0],[0]
"Each word is assigned a spine, shown im-
mediately above the word.",4 Parse Representation,[0],[0]
"A spine is the ordered set of non-terminals that the word is the head of, e.g. SVP for like.",4 Parse Representation,[1.0],"['A spine is the ordered set of non-terminals that the word is the head of, e.g. SVP for like.']"
"If a symbol occurs more than once in a spine, we use indices to distinguish instances.
",4 Parse Representation,[0],[0]
"Edges: An edge is a link between two words, with a label indicating the symbols it links in the child and parent spines.",4 Parse Representation,[0],[0]
"In our figures, edge labels are indicated by where edges start and end.
",4 Parse Representation,[0],[0]
"Null Elements: We include each null element in the spine of its parent, unlike Hayashi and Nagata (2016), who effectively treated null elements as words, assigning them independent spines.",4 Parse Representation,[0],[0]
"We also considered encoding null elements entirely on edges but found this led to poorer performance.
",4 Parse Representation,[0],[0]
Co-indexation:,4 Parse Representation,[0],[0]
"The treebank represents movement with index pairs on null elements and nonterminals, e.g. *1 and NP1 in Figure 3.",4 Parse Representation,[0],[0]
"We represent co-indexation with edges, one per reference, going from the null element to the non-terminal.",4 Parse Representation,[1.0],"['We represent co-indexation with edges, one per reference, going from the null element to the non-terminal.']"
There are three special cases of co-indexation: (1) It is possible for trace edges to have the same start and end points as a non-trace edge.,4 Parse Representation,[1.0],['There are three special cases of co-indexation: (1) It is possible for trace edges to have the same start and end points as a non-trace edge.']
We restrict this case to allow at most one trace edge.,4 Parse Representation,[0],[0]
This decreases edge coverage in the training set by 0.006%.,4 Parse Representation,[0],[0]
"(2) In some cases the reference non-terminal only spans a null element, e.g. the WHNP in Figure 4a.",4 Parse Representation,[0],[0]
For these we use a reversed edge to avoid creating a cycle.,4 Parse Representation,[0],[0]
"Figure 4a shows a situation where the trace edge links two positions in the same spine, which we assign with the spine during parsing.",4 Parse Representation,[0],[0]
(3) For parallel constructions the treebank coindexes arguments that fulfill the same roles (Fig. 4b).,4 Parse Representation,[0],[0]
These are distinct from the previous cases because neither index is on a null element.,4 Parse Representation,[0],[0]
"We considered two options: add edges from the repetition
to the referent (middle), or add edges from the repetition to the parent of the first occurrence (bottom).",4 Parse Representation,[0],[0]
"Option two produces fewer non-1-EC structures and explicitly represents all predicates, but only implicitly captures the original structure.",4 Parse Representation,[0],[0]
Prior work on parsing with spines has used radjunction to add additional non-terminals to spines.,4.1 Avoiding Adjunction Ambiguity,[0],[0]
"This introduces ambiguity, because edges modifying the same spine from different sides may not have a unique order of application.",4.1 Avoiding Adjunction Ambiguity,[0],[0]
We resolve this issue by using more articulated spines with the complete set of non-terminals.,4.1 Avoiding Adjunction Ambiguity,[0],[0]
"We found that 0.045% of spine instances in the development set are not observed in training, though in 70% of those cases an equivalent spine sans null elements is observed in training.",4.1 Avoiding Adjunction Ambiguity,[0],[0]
"To construct the spines, we lexicalize with head rules that consider the type of each non-terminal and its children.",4.2 Head Rules,[0],[0]
Different heads often represent more syntactic or semantic aspects of the phrase.,4.2 Head Rules,[1.0],['Different heads often represent more syntactic or semantic aspects of the phrase.']
"For trees, all head rules generate valid structures.",4.2 Head Rules,[0],[0]
"For graphs, head rules influence the creation of two problematic structures:
Cycles: These arise when the head chosen for a phrase is also an argument of another word in the phrase.",4.2 Head Rules,[0],[0]
Figure 5a shows a cycle between which and proposed.,4.2 Head Rules,[0],[0]
"We resolve this by changing the head of an SBAR to be an S rather than a Wh-noun phrase.
",4.2 Head Rules,[0],[0]
"One-Endpoint Crossing Violations: Figure 5b shows an example, with the trace from CEO to Page crossing two edges with no endpoints in common.",4.2 Head Rules,[0],[0]
We resolve this case by changing the head for VPs to be a child VP rather than an auxiliary.,4.2 Head Rules,[0],[0]
Algorithm Coverage: In Table 1 we show the impact of design decisions for our representation.,5 Results,[0],[0]
The percentages indicate how many sentences in the training set are completely recoverable by our algorithm.,5 Results,[0],[0]
"Each row shows the outcome of an addition to the previous row, starting from no traces at all, going to our representation with the head rules of Carreras et al. (2008), then changing the head rules, reversing null-null edges, and changing the target of edges in parallel constructions.",5 Results,[0],[0]
"The largest gain comes from changing the head rules, which is unsurprising since Carreras et al. (2008)’s rules were designed for trees (any set of rules form valid structures for trees).
",5 Results,[0],[0]
"Problematic Structures: Of the sentences we do not cover, 54% contain a cycle, 45% contain a 1- EC violation, and 1% contain both.",5 Results,[0],[0]
"To understand these problematic sentences, we manually inspected a random sample of twenty parses that contained a cycle and twenty parses with a 1-EC violation (these forty are 6% of all problematic parses, enough to identify the key remaining challenges).
",5 Results,[0],[0]
"For the cycles, eleven cases related to sentences containing variations of NP said interposed between two parts of a single quote.",5 Results,[0],[0]
A cycle was present because the top node of the parse was co-indexed with a null argument of said while said was an argument of the head word of the quote.,5 Results,[0],[0]
"The remaining cases were all instances of pseudo-attachment, which the treebank uses to show that non-adjacent constituents are related (Bies et al. 1995).",5 Results,[1.0],"['The remaining cases were all instances of pseudo-attachment, which the treebank uses to show that non-adjacent constituents are related (Bies et al. 1995).']"
"These cases were split between use of Expletive (5) and Interpret Constituent Here (4) traces.
",5 Results,[0],[0]
It was more difficult to determine trends for cases where the parse structure has a 1-EC violation.,5 Results,[0],[0]
"The same three cases, Expletive, Interpret Constituent Here, and NP said accounted for half of the issues.",5 Results,[0],[0]
We implemented a parser with a first-order model using our algorithm and representation.,5.1 Implementation,[0],[0]
"Code for the parser, for conversion to and from our representation, and for our metrics is available3.",5.1 Implementation,[0],[0]
"Our parser uses a linear discriminative model, with features based on McDonald et al. (2005).",5.1 Implementation,[0],[0]
"We train
3 https://github.com/jkkummerfeld/ 1ec-graph-parser
with an online primal subgradient approach (Ratliff et al. 2007) as described by Kummerfeld, BergKirkpatrick, et al. (2015), with parallel lock-free sparse updates.
",5.1 Implementation,[0],[0]
"Loss Function: We use a weighted Hamming distance for loss-augmented decoding, as it can be efficiently decomposed within our dynamic program.",5.1 Implementation,[0],[0]
Calculating the loss for incorrect spines and extra edges is easy.,5.1 Implementation,[0],[0]
"For missing edges, we add when a deduction rule joins two spans that cover an end of the edge, since if it does not exist in one of those items it is not going to be created in future.",5.1 Implementation,[0],[0]
"To avoid double counting we subtract when combining two halves that contain the two ends of a gold edge4.
",5.1 Implementation,[0],[0]
Inside–Outside Calculations:,5.1 Implementation,[0],[0]
"Assigning scores to edges is simple, as they are introduced in a single item in the derivation.",5.1 Implementation,[0],[0]
"Spines must be introduced in multiple items (left, right, and external positions) and must be assigned a score in every case to avoid ties in beams.",5.1 Implementation,[0],[0]
"We add the score every time the spine is introduced and then subtract when two items with a spine in common are combined.
",5.1 Implementation,[0],[0]
Algorithm rule pruning: Many 1-EC structures are not seen in our data.,5.1 Implementation,[0],[0]
"We keep only the rules used in gold training parses, reducing the set of 49,292 from the general algorithm to 627 (including rules for both adding arcs and combining items).",5.1 Implementation,[0],[0]
"Almost every template in Algorithm 1 generates some unnecessary rules, and no items of type B are needed.
",5.1 Implementation,[0],[0]
"4 One alternative is to count half of it on each end, removing the need for subtraction later.",5.1 Implementation,[0],[0]
"Another is to add it during the combination step.
",5.1 Implementation,[0],[0]
"The remaining rules still have high coverage of the development set, missing only 15 rules, each applied once (out of 78,692 rule applications).",5.1 Implementation,[0],[0]
"By pruning in this way, we are considering the intersection of 1-EC graphs and the true space of structures used in language.
",5.1 Implementation,[0],[0]
"Chart Pruning: To improve speed we use beams and cube pruning (Chiang 2007), discarding items based on their Viterbi inside score.",5.1 Implementation,[0],[0]
We divide each beam into sub-beams based on aspects of the state.,5.1 Implementation,[0],[0]
"This ensures diversity and enables consideration of only compatible items during binary and ternary compositions.
",5.1 Implementation,[0],[0]
"Coarse to Fine Pruning: Rather than parsing immediately with the full model we use several passes with progressively richer structure (Goodman 1997): (1) Projective parsing without traces or spines, and simultaneously a trace classifier, (2) Non-projective parsing without spines, and simultaneously a spine classifier, (3) Full structure parsing.",5.1 Implementation,[0],[0]
"Each pass prunes using parse max-marginals and classifier scores, tuned on the development set.",5.1 Implementation,[0],[0]
The third pass also prunes spines that are not consistent with any unpruned edge from the second pass.,5.1 Implementation,[0],[0]
"For the spine classifier we use a bidirectional LSTM tagger, implemented in DyNet (Neubig et al. 2017).
",5.1 Implementation,[0],[0]
Speed: Parsing took an average of 8.6 seconds per sentence for graph parsing and 0.5 seconds when the parser is restricted to trees5.,5.1 Implementation,[0],[0]
"Our algorithm is also amenable to methods such as semi-supervised and adaptive supertagging, which can improve the speed of a parser after training (Kummerfeld, Roesner, et al. 2010; Lewis and Steedman 2014).
",5.1 Implementation,[0],[0]
Tree Accuracy:,5.1 Implementation,[0],[0]
"On the standard tree-metric, we score 88.1.",5.1 Implementation,[0],[0]
"Using the same non-gold POS tags as input, Carreras et al. (2008) score 90.9, probably due to their second-order features and head rules tuned for performance6.",5.1 Implementation,[0],[0]
"Shifting to use their head rules, we score 88.9.",5.1 Implementation,[1.0],"['Shifting to use their head rules, we score 88.9.']"
"Second-order features could be added to our model through the use of forest reranking, an improvement that would be orthogonal to this paper’s contributions.
",5.1 Implementation,[1.0000000554904003],"['Second-order features could be added to our model through the use of forest reranking, an improvement that would be orthogonal to this paper’s contributions.']"
We can also evaluate on spines and edges.,5.1 Implementation,[0],[0]
"Since their system produces regular PTB trees, we con-
5 Using a single core of an Amazon EC2 m4.2xlarge instance (2.4 GHz Xeon CPU and 32 Gb of RAM).
",5.1 Implementation,[0],[0]
"6 Previous work has shown that the choice of head can significantly impact accuracy (Schwartz et al. 2012).
vert its output to our representation and compare its results with our system using their head rules.",5.1 Implementation,[0],[0]
"We see slightly lower accuracy for our system on both spines (94.0 vs. 94.3) and edges (90.4 vs. 91.1).
",5.1 Implementation,[0],[0]
Trace Accuracy: Table 2 shows results using Johnson (2002)’s trace metric.,5.1 Implementation,[1.0],['Trace Accuracy: Table 2 shows results using Johnson (2002)’s trace metric.']
"Our parser is competitive with previous work that has highly-engineered models: Johnson’s system has complex non-local features on tree fragments, and similarly Kato and Matsubara (K&M 2016) consider complete items in the stack of their transition-based parser.",5.1 Implementation,[1.0],"['Our parser is competitive with previous work that has highly-engineered models: Johnson’s system has complex non-local features on tree fragments, and similarly Kato and Matsubara (K&M 2016) consider complete items in the stack of their transition-based parser.']"
On co-indexation our results fall between Johnson and K&M.,5.1 Implementation,[0],[0]
"Converting to our representation, our parser has higher precision than K&M on trace edges (84.1 vs. 78.1) but lower recall (59.5 vs. 71.3).",5.1 Implementation,[1.0],"['Converting to our representation, our parser has higher precision than K&M on trace edges (84.1 vs. 78.1) but lower recall (59.5 vs. 71.3).']"
"One modeling challenge we observed is class imbalance: of the many places a trace could be added, only a small number are correct, and so our model tends to be conservative (as shown by the P/R tradeoff).",5.1 Implementation,[1.0],"['One modeling challenge we observed is class imbalance: of the many places a trace could be added, only a small number are correct, and so our model tends to be conservative (as shown by the P/R tradeoff).']"
We propose a representation and algorithm that cover 97.3% of graph structures in the PTB.,6 Conclusion,[1.0],['We propose a representation and algorithm that cover 97.3% of graph structures in the PTB.']
"Our algorithm is O(n4), uniquely decomposes parses, and enforces the property that parses are composed of a core tree with additional traces and null elements.",6 Conclusion,[0],[0]
A proof of concept parser shows that our algorithm can be used to parse and recover traces.,6 Conclusion,[1.0],['A proof of concept parser shows that our algorithm can be used to parse and recover traces.']
"We thank Greg Durrett for advice on parser implementation and debugging, and the action editor and anonymous reviewers for their helpful feedback.",Acknowledgments,[0],[0]
"This research was partially supported by a General Sir John Monash Fellowship and the Office of Naval
Research under MURI Grant",Acknowledgments,[0],[0]
No. N000140911081.,Acknowledgments,[0],[0]
"General treebank analyses are graph structured, but parsers are typically restricted to tree structures for efficiency and modeling reasons.",abstractText,[0],[0]
"We propose a new representation and algorithm for a class of graph structures that is flexible enough to cover almost all treebank structures, while still admitting efficient learning and inference.",abstractText,[0],[0]
"In particular, we consider directed, acyclic, one-endpoint-crossing graph structures, which cover most long-distance dislocation, shared argumentation, and similar tree-violating linguistic phenomena.",abstractText,[0],[0]
"We describe how to convert phrase structure parses, including traces, to our new representation in a reversible manner.",abstractText,[0],[0]
"Our dynamic program uniquely decomposes structures, is sound and complete, and covers 97.3% of the Penn English Treebank.",abstractText,[0],[0]
We also implement a proofof-concept parser that recovers a range of null elements and trace types.,abstractText,[0],[0]
Parsing with Traces: An O(n) Algorithm and a Structural Representation,title,[0],[0]
