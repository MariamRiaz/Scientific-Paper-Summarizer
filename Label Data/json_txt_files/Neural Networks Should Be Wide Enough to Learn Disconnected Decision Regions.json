{"sections": [{"heading": "1. Introduction", "text": "While deep learning has become state of the art in many application domains such as computer vision and natural language processing and speech recognition, the theoretical understanding of this success is steadily growing but there are still plenty of questions where there is little or no understanding. In particular, for the question how one should construct the network e.g. choice of activation function, number of layers, number of hidden units per layer etc., there is little guidance and only limited understanding on the implications of the choice e.g. \u201cThe design of hidden units is an extremely active area of research and does not yet have many definitive guiding theoretical principles.\u201d is a quote from the recent book on deep learning (Goodfellow et al., 2016, p. 191). Nevertheless there is recently progress in the understanding of these choices.\n1Department of Mathematics and Computer Science, Saarland University, Germany 2University of Tu\u0308bingen, Germany. Correspondence to: Quynh Nguyen <quynh@cs.uni-saarland.de>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nThe first important results are the universal approximation theorems (Cybenko, 1989; Hornik et al., 1989) which show that even a single hidden layer network with standard nonpolynomial activation function (Leshno et al., 1993), like the sigmoid, can approximate arbitrarily well every continuous function over a compact domain of Rd. In order to explain the success of deep learning, much of the recent effort has been spent on analyzing the representation power of neural networks from the perspective of depth (Delalleau & Bengio, 2011; Telgarsky, 2016; 2015; Eldan & Shamir, 2016; Safran & Shamir, 2017; Yarotsky, 2016; Poggio et al., 2016; Liang & Srikant, 2017; Mhaskar & Poggio, 2016). Basically, they show that there exist functions that can be computed efficiently by deep networks of linear or polynomial size but require exponential size for shallow networks. To further highlight the power of depth, (Montufar et al., 2014; Pascanu et al., 2014) show that the number of linear regions that a ReLU network can form in the input space grows exponentially with depth. Tighter bounds on the number of linear regions are later on developed by (Arora et al., 2018; Serra et al., 2018; Charisopoulos & Maragos, 2018). Another measure of expressivity so-called trajectory length is proposed by (Raghu et al., 2017). They show that the complexity of functions computed by the network along a one-dimensional curve in the input space also grows exponentially with depth.\nWhile most of previous work can only show the existence of depth efficiency (i.e. there exist certain functions that can be efficiently represented by deep networks but not effectively represented or even approximated by shallow networks) but cannot show how often this holds for all functions of interest, (Cohen et al., 2016) have taken the first step to address this problem. In particular, by studying a special type of networks called convolutional arithmetic circuits \u2013 also known as Sum-Product networks (Poon & Domingos, 2011), the authors show that besides a set of measure zero, all functions that can be realized by a deep network of polynomial size require exponential size in order to be realized, or even approximated by a shallow network. Later, (Cohen & Shashua, 2016) show that this property however no longer holds for convolutional rectifier networks, which represents so far the empirically most successful deep learning architecture in practice.\nUnlike most of previous work which focuses on the power of depth, (Lu et al., 2017; Hanin & Sellke, 2017) have recently shown that neural networks with ReLU activation function have to be wide enough in order to have the universal approximation property as depth increases. In particular, the authors show that the class of continuous functions on a compact set cannot be arbitrarily well approximated by an arbitrarily deep network if the maximum width of the network is not larger than the input dimension d. Moreover, it has been shown recently, that the loss surface of fully connected networks (Nguyen & Hein, 2017) and for convolutional neural networks (Nguyen & Hein, 2018) is well behaved, in the sense that almost all local minima are global minima, if there exists a layer which has more hidden units than the number of training points.\nIn this paper we study the question under which conditions on the network the decision regions of a neural network are connected respectively can potentially be disconnected. The decision region of a class is the subset of Rd, where the network predicts this class. A similar study has been in (Makhoul et al., 1989; 1990) for feedforward networks with threshold activation functions, where they show that the initial layer has to have width d + 1 in order that one can get disconnected decision regions. On an empirical level it has recently been argued (Fawzi et al., 2017) that the decision regions of the Caffe Network (Jia et al., 2014) on ImageNet are connected. In this paper we analyze feedforward networks with continuous activation functions as currently used in practice. We show in line with previous work that almost all networks which have a pyramidal structure up to the last hidden layer, that is the width of all hidden layers is smaller than the input dimension d, can only produce connected decision regions. We show that the result is tight by providing explicit counterexamples for the case d + 1. We conclude that a guiding principle for the construction of neural networks should be that there is a layer which is wider than the input dimension as it would be a strong assumption that the Bayes optimal classifier must have connected decision regions. Interestingly, our result holds for leaky ReLU, that is \u03c3(t) = max{t, \u03b1t} for 0 < \u03b1 < 1, whereas the result of (Hanin & Sellke, 2017) is for ReLU, that is \u03c3(t) = max{t, 0}, but \u201cthe generalization is not straightforward, even for activations of the form \u03c3(t) = max{l1(t), l2(t)}, where l1, l2 are affine functions with different slopes.\u201d We discuss also the implications of connected decision regions regarding the generation of adversarial samples, which will provide another argument in favor of larger width for neural network architectures."}, {"heading": "2. Feedforward Neural Networks", "text": "We consider in this paper feedforward neural networks for multi-class classification. Let d be the input dimension and\nm the number of classes. Let L be the number of layers where the layers are indexed from k = 0, 1, . . . , L which respectively corresponds to the input layer, 1st hidden layer, . . ., and the output layer L. Let nk be the width of layer k. For consistency, we assume that n0 = d and nL = m. Let \u03c3k : R\u2192 R be the activation function of every hidden layer 1 \u2264 k \u2264 L\u2212 1. In the following, all functions are applied componentwise. We define fk : Rd \u2192 Rnk as the feature map of layer k, which computes for every input x \u2208 Rd a feature vector at layer k defined as\nfk(x) =  x k = 0 \u03c3k ( WTk fk\u22121(x) + bk ) 1 \u2264 k \u2264 L\u2212 1\nWTL fL\u22121(x) + bL k = L\nwhere Wk \u2208 Rnk\u22121\u00d7nk is the weight matrix at layer k. Please note that the output layer is linear as it is usually done in practice. We consider in the following activation functions \u03c3 : R \u2192 R which are continuous and strictly monotonically increasing. This is true for most of proposed activation functions, but does not hold for ReLU, \u03c3(t) = max{t, 0}. On the other hand, it has been argued in the recent literature, that the following variants are to be preferred over ReLU as they deal better with the vanishing gradient problem and outperform ReLU in prediction performance (He et al., 2015; Clevert et al., 2016). This is leaky ReLU (Maas et al., 2013):\n\u03c3(t) = max{t, \u03b1t} for 0 < \u03b1 < 1,\nwhere typically \u03b1 is fixed but it has also been optimized together with the network weights (He et al., 2015) and ELU (exponential linear unit) (Clevert et al., 2016):\n\u03c3(t) = { et \u2212 1 t < 0 t t \u2265 0. .\nNote that image of the activation function \u03c3, \u03c3(R) = {\u03c3(t) | t \u2208 R}, is equal to R for leaky ReLU and (\u22121,\u221e) for the exponential linear unit."}, {"heading": "3. Connectivity of Decision Regions", "text": "In this section, we prove two results on the connectivity of the decision regions of a classifier. Both require that the activation function is continuous and strictly monotonically increasing. Our main Theorem 3.10 holds for feedforward networks of arbitrary depth and requires additionally \u03c3(R) = R, the second Theorem 3.11 holds just for one hidden layer networks but has no further requirements on the activation function. Both show that in general pyramidal feedforward neural networks where the width of all the hidden layers is smaller than or equal to the input dimension can only produce connected decision regions."}, {"heading": "3.1. Preliminary technical results", "text": "We first introduce the definitions and terminologies used in the following, before we prove or recall some simple results about continuous mappings from Rm to Rn. For a function f : U \u2192 V , where dom(f) = U \u2286 Rm and V \u2286 Rn, we denote for every subset A \u2286 U , the image f(A) as f(A) := {f(x) | x \u2208 A} = \u22c3 x\u2208A f(x). Let range(f) := f(U).\nDefinition 3.1 (Decision region) The decision region of a given class 1 \u2264 j \u2264 m, denoted by Cj , is defined as\nCj = { x \u2208 Rd \u2223\u2223 (fL)j(x) > (fL)k(x), \u2200k 6= j} . Definition 3.2 (Connected set) A subset S \u2286 Rd is called connected if for every x, y \u2208 S, there exists a continuous curve r : [0, 1]\u2192 S such that r(0) = x and r(1) = y.\nTo prove our key Lemma 3.9, the following properties of connected sets and continuous functions are useful. All proofs are moved to the appendix due to limited space.\nProposition 3.3 Let f : U \u2192 V be a continuous function. If A \u2286 U is a connected set then f(A) \u2286 V is also a connected set.\nProposition 3.4 The Minkowski sum of two connected subsets U, V \u2286 Rn, defined as U + V = {u+ v | u \u2208 U, v \u2208 V }, is a connected set.\nAs our main idea is to transfer the connectedness of a set from the output layer back to the input layer, we require the notion of pre-image and inverse mapping.\nDefinition 3.5 (Pre-Image) The pre-image of a function f : U \u2192 V is the set-valued function f\u22121 : V \u2192 U defined for every y \u2208 V as\nf\u22121(y) = {x \u2208 U | f(x) = y} .\nSimilarly, for every subset A \u2286 V , let f\u22121(A) = \u22c3 y\u2208A f\u22121(y) = {x \u2208 U | f(x) \u2208 A} .\nBy definition, it holds for every subset A \u2286 V that f(x) \u2208 A if and only if x \u2208 f\u22121(A). Moreover, for every A \u2286 V\nf\u22121(A) = f\u22121(A \u2229 range(f)) \u222a f\u22121(A \\ range(f)) = f\u22121(A \u2229 range(f)) \u222a \u2205 = f\u22121(A \u2229 range(f)).\nAs a deep feedforward network is a composition of the individual layer functions, we need the following property.\nProposition 3.6 Let f : U \u2192 V and g : V \u2192 Q be two functions. Then it holds that (g \u25e6 f)\u22121 = f\u22121 \u25e6 g\u22121.\nApart from the property of connectivity, we can also show the openness of a set when considering the pre-image of a given network. We recall the following standard result from topology (see e.g. Apostol, 1974, Theorem 4.23, p. 82).\nProposition 3.7 Let f : Rm \u2192 Rn be a continuous function. If U \u2286 Rn is an open set then f\u22121(U) is also open.\nWe now recall a standard result from calculus showing that under certain, restricted conditions the inverse of a continuous mapping exists and is as well continuous.\nProposition 3.8 Let f : R \u2192 f(R) be continuous and strictly monotonically increasing. Then the inverse mapping f\u22121 : f(R)\u2192 R exists and is continuous.\nThe following lemma is a key ingredient in the following proofs. It allows us to show that the pre-image of an open and connected set by a one hidden layer network is again open and connected. Using the fact that deep networks can be seen as a composition of such individual layers, this will later on allow us to transfer the result to deep networks.\nLemma 3.9 Let m \u2265 n and f : Rm \u2192 Rn be a function defined as f = \u03c3\u0302 \u25e6 h where \u03c3\u0302 : Rn \u2192 Rn is defined as\n\u03c3\u0302(x) = \u03c3(x1)... \u03c3(xn)  , (1) and \u03c3 : R\u2192 R is bijective, continuous and strictly monotonically increasing, h : Rm \u2192 Rn is a linear map defined as h(x) = WTx + b where W \u2208 Rm\u00d7n has full rank and b \u2208 Rn. If V \u2286 Rn is an open connected set then f\u22121(V ) \u2286 Rm is also an open connected set.\nProof: By Proposition 3.6, it holds that f\u22121(V ) = h\u22121(\u03c3\u0302\u22121(V )). As \u03c3\u0302 is a componentwise function, the inverse mapping \u03c3\u0302\u22121 is given by the inverse mappings of the components\n\u03c3\u0302\u22121 : Rn \u2192 Rn, \u03c3\u0302\u22121(x) = \u03c3 \u22121(x1)\n... \u03c3\u22121(xn)  , where under the stated assumptions the inverse mapping \u03c3\u22121 : R \u2192 R exists by Lemma 3.8 and is continuous. Since V \u2286 Rn = dom(\u03c3\u0302\u22121), \u03c3\u0302\u22121(V ) is the image of the connected set V under the continuous map \u03c3\u0302\u22121. Thus by Proposition 3.3, \u03c3\u0302\u22121(V ) is connected. Moreover, \u03c3\u0302\u22121(V ) is an open set by Proposition 3.7.\nIt holds for every y \u2208 Rn that\nh\u22121(y)\n= { \u2205 y /\u2208 range(h) W (WTW )\u22121(y \u2212 b) + ker(WT ) y \u2208 range(h),\nwhere the inverse of WTW exists as W has full rank n (note that we assume n \u2264 m). As W has full rank and m \u2265 n, it holds that range(h) = Rn and thus\nh\u22121(y) =W (WTW )\u22121(y \u2212 b) + ker(WT ), \u2200 y \u2208 Rn.\nTherefore it holds for \u03c3\u0302\u22121(V ) \u2286 Rn that\nh\u22121 ( \u03c3\u0302\u22121(V ) ) =W (WTW )\u22121 ( \u03c3\u0302\u22121(V )\u2212 b ) + ker(WT ),\nwhere the first term is the image of the connected set \u03c3\u0302\u22121(V ) under an affine mapping and thus is again connected by Proposition 3.3, the second term ker(WT ) is a linear subspace which is also connected. By Proposition 3.4, the Minkowski sum of two connected sets is connected. Thus f\u22121(V ) = h\u22121(\u03c3\u0302\u22121(V )) is a connected set. Moreover, as f\u22121(V ) is the pre-image of the open set V under the continuous function f , it must be also an open set by Proposition 3.7. Thus f\u22121(V ) is an open and connected set.\nNote that in Lemma 3.9, if m < n and W has full rank then range(h) ( Rn and the linear equation h(x) = y has a unique solution x = (WWT )\u22121W (y \u2212 b) for every y \u2208 range(h) and thus\nf\u22121(V ) = h\u22121 ( \u03c3\u22121(V ) ) = h\u22121 ( \u03c3\u22121(V ) \u2229 range(h)\n) = (WWT )\u22121W ( (\u03c3\u22121(V ) \u2229 range(h))\u2212 b ) .\nIn this case, even though \u03c3\u22121(V ) is a connected set, the intersection \u03c3\u22121(V )\u2229 range(h) can be disconnected which can imply that f\u22121(V ) is disconnected and thus the decision region becomes disconnected.\nWe illustrate this with a simple example, where m = 1 and n = 2 with \u03c3(x) = x3 and WT = ( \u22121 1 ) and b = ( 0 0 ) .\nIn this case it holds that\nf(x) = \u03c3\u0302(WTx+ b) = ( \u03c3(\u2212x) \u03c3(x) ) = ( \u2212x3 x3 ) . (2)\nFigure 1 shows that f(R) is a one-dimensional submanifold (in this case subspace) of R2 and provides an example of a set S \u2282 R2 where the pre-image f\u22121(S) is disconnected."}, {"heading": "3.2. Main results", "text": "We show in the following that the decisions regions of feedforward networks which are pyramidal and have maximal width at most the input dimension d can only produce connected decision regions. We assume for the activation functions that \u03c3(R) = R, which is fulfilled by leaky ReLU.\nTheorem 3.10 Let the width of the layers of the feedforward network network satisfy d = n0 \u2265 n1 \u2265 . . . \u2265 nL\u22121 and let \u03c3l : R \u2192 R be a continuous, strictly monotonically increasing function with \u03c3l(R) = R for every layer 1 \u2264 l \u2264 L\u22121 and all the weight matrices (Wl)L\u22121l=1 have full rank. Then every decision region Cj is an open connected subset of Rd for every 1 \u2264 j \u2264 m.\nProof: From Definition 3.1, it holds for every 1 \u2264 j \u2264 m Cj= { x \u2208 Rd \u2223\u2223 fLj(x)\u2212 fLk(x) > 0,\u2200k 6= j} where\nfLj(x)\u2212 fLk(x) = \u3008(WL):j \u2212 (WL):k, fL\u22121(x)\u3009+ (bL)j \u2212 (bL)k.\nLet us define the set Vj= { y \u2223\u2223\u2223 \u3008(WL):j \u2212 (WL):k, y\u3009>(bL)k \u2212 (bL)j ,\u2200k 6= j}\nthen it holds Cj = { x \u2208 Rd \u2223\u2223 fL\u22121(x) \u2208 Vj} = f\u22121L\u22121(Vj). If Vj is an empty set then we are done, otherwise one observes that Vj is the intersection of a finite number of open half-spaces (or the whole space), which is thus an open and connected set. Moreover, it holds Vj \u2229 \u03c3\u0302L\u22121(R) = Vj , where \u03c3\u0302L\u22121 is defined as in (1). It follows from Proposition 3.7 that Cj must be an open set as it is the pre-image of the open set Vj under the continuous mapping fL\u22121. To show that Cj is a connected set, one first observes that\nfL\u22121 = \u03c3\u0302L\u22121 \u25e6 hL\u22121 \u25e6 \u03c3\u0302L\u22122 \u25e6 hL\u22122 . . . \u25e6 \u03c3\u03021 \u25e6 h1\nwhere hk : Rnk\u22121 \u00d7 Rnk is an affine mapping between layer k \u2212 1 and layer k defined as hk(x) =WTk x+ bk for every 1 \u2264 k \u2264 L \u2212 1, x \u2208 Rnk\u22121 , and \u03c3\u0302k : Rnk \u2192 Rnk is the activation mapping of layer k defined as in (1). By Proposition 3.6 it holds that\nf\u22121L\u22121(Vj) = (h \u22121 1 \u25e6 \u03c3\u0302 \u22121 1 \u25e6 . . . \u25e6 h \u22121 L\u22121 \u25e6 \u03c3\u0302 \u22121 L\u22121)(Vj)\nSince \u03c3k : R \u2192 R is a continuous bijection by our assumption, it follows that \u03c3\u0302k : Rnk \u2192 Rnk is also a continuous bijection. Moreover, it holds that Wk has full rank and nk\u22121 \u2265 nk for every 1 \u2264 k \u2264 L \u2212 1 and Vj is a connected set. Thus one can apply Lemma 3.9 subsequently for the composed functions (\u03c3\u0302k \u25e6 hk) for every k = L \u2212 1, L \u2212 2, . . . , 1 and obtains that Cj = f\u22121L\u22121(Vj) is a connected set. Thus Cj is an open and connected set for every 1 \u2264 j \u2264 m.\nThe next theorem holds just for networks with one hidden layer but allows general activation functions which are continuous and strictly monotonically increasing, that is leaky ReLU, ELU, softplus or sigmoid activation functions. Again the decision regions are connected if the hidden layer has maximal width smaller than d+ 1.\nTheorem 3.11 Let the one hidden layer network satisfy d = n0 \u2265 n1 and let \u03c31 : R\u2192 R be a continuous, strictly monotonically increasing function and the hidden layer\u2019s weight matrix W1 has full rank. Then every decision region Cj is an open connected subset of Rd for every 1 \u2264 j \u2264 m.\nProof: We note that in the proof of Theorem 3.10 the Vj is a finite intersection of open half-spaces and thus a convex set. Moreover, \u03c3\u03021(Rn1) is an open convex set (it is just an axis-aligned open box), as \u03c31 is strictly monotonically increasing. Thus\nCj = { x \u2208 Rd \u2223\u2223 f1(x) \u2208 Vj \u2229 \u03c3\u03021(Rn1)} = f\u221211 ( Vj \u2229 \u03c3\u03021(Rn1) ) .\nAs both sets are open convex sets, the intersection Vj \u2229 \u03c3\u03021(Rn1) is again convex and open as well. Thus Vj \u2229 \u03c3\u03021(Rn1) is a connected set. The rest of the argument follows then by using Lemma 3.9, noting that by Proposition\n3.8 \u03c3\u0302\u221211 : \u03c3\u03021(Rn1)\u2192 Rn1 is a continuous mapping.\nNote that Theorem 3.10 and Theorem 3.11 make no assumption on the structure of all layers in the network. Thus they can be applied to neural networks with both fully connected layers and convolutional layers. Moreover, the results hold regardless of how the parameters of the network (Wl, bl)Ll=1 have been attained, trained or otherwise, as long as all the weight matrices of hidden layers have full rank. This is a quite weak condition in practice as the set of low rank matrices has just Lebesgue measure zero. Even if the optimal weight parameters for the data generating distribution would be low rank (we discuss such an example below), then it is very unlikely that the trained weight parameters are low rank, as one has statistical noise by the training sample, \u201coptimization noise\u201d from the usage of stochastic gradient descent (SGD) and its variants and finally in practice one often uses early stopping and thus even if the optimal solution for the training set is low rank, one will not find it.\nTheorem 3.10 covers activation functions like leaky ReLU but not sigmoid, ELU or softplus. At the moment it is unclear for us if the result might hold also for the more general class of activation functions treated in Theorem 3.11. The problem is that then in Lemma 3.9 one has to compute the pre-image of V \u2229 \u03c3\u0302(Rn). Even though both sets are connected, the intersection of connected sets need not be connected. This is avoided in Theorem 3.11 by using that the initial set Vj and \u03c3\u0302(RnL\u22121) are both convex and the intersection of convex sets is convex and thus connected.\nWe show below that the result is tight in the sense that we give an empirical example of a neural network with a single hidden layer of d+ 1 hidden units which produces disconnected regions. Note that our result complements the result of (Hanin & Sellke, 2017), where they show the universal approximation property (for ReLU) only if one considers networks of width at least d+1 for arbitrary depth. Theorem 3.10 and Theorem 3.11 indicate that this result could also hold for leaky ReLU as approximation of arbitrary functions implies approximation of arbitrary decisions regions, which clearly requires that one is able to get disconnected decision regions. Taking both results together, it seems rather obvious that as a general guiding principle for the construction of hidden layers in neural networks one should use, at least for the first hidden layer, more units than the input dimension, as it is rather unlikely that the Bayes optimal decision regions are connected. Indeed, if the true decision regions are disconnected then using a network of smaller width than d+1 might still perfectly fit the finite training data but since the learned decision regions are connected there exists a path between the true decision regions which then can be used for potential adversarial manipulation. This is discussed in the next section where we show empirical evidence for the existence of such adversarial examples."}, {"heading": "4. Illustration and Discussion", "text": "In this section we discuss with analytical examples as well as trained networks that the result is tight and the conditions of the theorem cannot be further relaxed. Moreover, we argue that connected decision regions can be problematic as they open up the possibility to generate adversarial examples."}, {"heading": "4.1. Why pyramidal structure of the network is necessary to get connected decision regions?", "text": "In Theorem 3.10, if the network does not have pyramidal structure up to the last hidden layer, i.e. the condition d1 \u2265 . . . \u2265 dL\u22121 is not fulfilled, then the statement of the theorem might not hold as the decision regions can be disconnected. We illustrate this via a counter-example below. Let us consider a non-pyramidal network 2-1-2-2 defined as\nWT3 \u03c3\u03022(W T 2 \u03c3\u03021(W T 1 x+ b1) + b2) + b3 (3)\nwhere \u03c31(t) = \u03c32(t) = max {0.5 t, t}, and W1 =[ 1 1 ] , b1 = 0,W2 = [ 1 \u22121 ] , b2 = [ 0 0 ] ,W3 =[\n2 1 3 2\n] , b3 = [ 0 1 ] . Then one can check that this\nnetwork has (see appendix for the full derivation) C1 = { x \u2208 R2 \u2223\u2223 x1 + x2 \u2212 2 > 0 and x1 + x2 + 4 < 0}, which is a disconnected set as illustrated in Figure 2."}, {"heading": "4.2. Why full rank of the weight matrices is necessary to get connected decision regions?", "text": "Similar to Section 4.1, we show that if the weight matrices of hidden layers are not full rank while the other conditions are still satisfied, then the decision regions can be disconnected. The reason is simply that low rank matrices, in particular in the first layer, reduce the effective dimension of the input. We illustrate this effect with a small analytical example and then argue that nevertheless in practice it is extremely difficult to get low rank weight matrices. Suppose one has a two-class classification problem on R2 (see Figure 3) with equal class probabilities P (red) = P (blue), and the conditional distribution is given as\np(x1, x2|blue)= 1\n2 , \u2200x1 \u2208 [\u22122,\u22121] \u222a [1, 2], x2 \u2208 [\u2212\n1 2 , 1 2 ]\np(x1, x2|red)=1, \u2200x1 \u2208 [\u22121, 1], x2 \u2208 [\u2212 1 2 , 1 2 ]. (4)\nNote that the Bayes optimal decision region for class blue is disconnected. Moreover, it is easy to verify that a one hidden layer network with leaky ReLU \u03c3(t) = max{t, \u03b1t} for 0 < \u03b1 < 1 can perfectly fit the data with\nWT1 = ( 1 0 \u22121 0 ) , b1= ( \u22121 \u22121 ) ,WT2 = ( 1 1 0 0 ) , b2= ( 0 \u22122\u03b1 ) Note that W1 has low rank. Suppose that the first output unit corresponds to the blue class and second output unit corresponds to the red class. Then it holds (f2)red(x1, x2) = \u22122\u03b1, (f2)blue(x1, x2) = max{x1 \u2212 1, \u03b1(x1 \u2212 1)}+max{\u2212(x1 + 1),\u2212\u03b1(x1 + 1)} and thus\n(f2)blue(x1, x2)=  (1\u2212 \u03b1)x1 \u2212 (1 + \u03b1) x1 \u2265 1 \u22122\u03b1 \u22121 \u2264 x1 \u2264 1 \u2212(1\u2212 \u03b1)x1 \u2212 (1 + \u03b1) x1 \u2264 \u22121\nwhich implies that (f2)blue(x1, x2) > (f2)red(x1, x2) for every x1 \u2208 (\u2212\u221e,\u22121) \u222a (1,+\u221e) and thus the decision region for class blue has two disconnected decision regions. This implies that Theorems 3.10 and 3.11 do indeed not hold if the weight matrices do not have full rank. Nevertheless in practice, it is unlikely that one will get such low rank weight matrices, which we illustrate in Figure 3 that the decision regions of the trained classifier has indeed connected decision regions. This is due to statistical noise in the training set as well as through the noise in the optimization procedure (SGD) and the common practice of early stopping in training of neural networks."}, {"heading": "4.3. Does the result hold for ReLU activation function?", "text": "As the conditions of Theorem 3.10 are not fulfilled for ReLU, one might ask whether the decision regions of a ReLU network with pyramidal structure and full rank weight matrices can be potentially disconnected. We show that this is indeed possible via the following example. Let a two hidden layer network (2-2-2-2) be defined as\nWT3 \u03c3\u03022(W T 2 \u03c3\u03021(W T 1 x+ b1) + b2) + b3 (5)\nwhere \u03c31(t) = \u03c32(t) = max {t, 0} and\nWT1 = [ 1 0 0 1 ] ,WT2 = \u221a 2 2 [ 1 1 \u22121 1 ] ,WT3 = [ \u22121 0 0 \u22123 ] ,\nand b1 = [0, 0]T , b2 = 1\u221a2 [ \u221a 2 \u2212 1,\u22123]T , b3 = [1, 0]T . Then one can derive the decision region for the first class as (see appendix for the full derivation)\nC1 = { x \u2208 R2 \u2223\u2223 x1 < 1, x2 < 1, x1 + x2 < 1} \u222a { x \u2208 R2\n\u2223\u2223 x2 > 4, 2x1 \u2212 x2 + 4 < 0} which is a disconnected set as illustrated in Figure 2.\nFinally, one notes in this example that except for the activation function, all the other conditions of Theorem 3.10 are still satisfied, that is, the network has pyramidal structure (2-2-2-2) and all the weight matrices (Wl)2l=1 have full rank by our construction. Thus the statement of Theorem 3.10, at least under current form, does not hold for ReLU."}, {"heading": "4.4. The theorems are tight: disconnected decision regions for width d+ 1", "text": "We consider a binary classification task in R2 where the data points are generated so that the blue class has disconnected components on the square [\u22124, 4] \u00d7 [\u22124, 4], see Figure 4 (a) for an illustration. We use a one hidden layer network with varying number of hidden units, two output units, leaky ReLU activation function and cross-entropy loss. We then train this network by using SGD with momentum for 1000 epochs and learning rate 0.1 and reduce the it by a factor of 2 after every 50 epochs. For all the attempts with different starting points that we have done in our experiment, the resulting weight matrices always have full rank.\nWe show the training error and the decision regions of trained network in Figure 4. The grid size in each case of Figure 4 has been manually chosen so that one can see clearly the connected/disconnected components in the decision regions. First, we observe that for two hidden units (n1 = 2), the network satisfies the condition of Theorem 3.10 and thus can only learn connected regions, which one can also clearly see in the figure, where one basically gets a linear separator. However, for three hidden units (n1 = 3), one can see that the network can produce disconnected decision regions, which shows that both our Theorems 3.10 and 3.11 are tight, in the sense that width d+ 1 is already sufficient to produce disconnected components, whereas\nthe results say that for width less than d + 1 the decision regions have to be connected. As the number of hidden units increases, we observe that the network produces more easily disconnected decision regions as expected."}, {"heading": "4.5. Relation to adversarial manipulation", "text": "We use a single image of digit 1 from the MNIST dataset to create a new artificial dataset where the underlying data generation probability measure has a similar one-dimensional structure as in (4) but now embedded in the pixel space R28\u00d728. This is achieved by using rotation as the onedimensional degree of freedom. We generate 2000 training images for each red/blue class by rotating the chosen digit 1 with angles ranging from [\u22125\u25e6, 5\u25e6] for the read class, and [\u221220\u25e6,\u221215\u25e6] \u222a [15\u25e6, 20\u25e6] for the blue class, see Figure 5.\nNote that this is a binary classification task where the dataset has just one effective degree of freedom and the Bayes optimal decision regions are disconnected. We train a one hidden layer network with 784 hidden units which is equal to the input dimension and leaky ReLU as activation function with \u03b1 = 0.1. The training error is zero and the resulting weight matrices have full rank, thus the conditions of Theorem 3.10 are satisfied and the decision region of class blue should be connected even though the Bayes optimal decision region is disconnected. This can only happen by establishing a connection around the other red class. We test this by sampling a source image from the [\u221220\u25e6,\u221215\u25e6] part of the blue class and a target image from the other part [15\u25e6, 20\u25e6]. Next, we generate an adversarial image 1 from the red class using the one step target class method (Kurakin\n1This is essentially a small perturbation of an image from the red class which is classified as blue class\net al., 2016; 2017) and consider the path between the source image to the adversarial image and subsequently from the adversarial image to the target one. For each path, we simply consider the line segment \u03bbs+ (1\u2212 \u03bb)t for \u03bb \u2208 [0, 1] between the two endpoint images s and t and sample it very densely by dividing [0, 1] into 104 equidistant parts. Figure 6 shows the complete path from the source image to the target image where the color indicates that all the intermediate images are classified as blue with high confidence (note that we turned the output of the network into probabilities by using the softmax function). Moreover, the intermediate images from Figure 6 look very much like images from the red class thus could be seen as adversarial samples for the red class. The point we want to make here is that one might think that in order to avoid adversarial manipulation the solution is to use a simple classifier of low capacity. We think that rather the opposite is true in the sense that only if the classifier is rich enough to model the true underlying data generating distribution it will be able to model the true decision boundaries. In particular, the classifier should be able to realize disconnected decision regions in order to avoid paths through the input space which connect different disconnected regions of the Bayes optimal classifier. Now one could argue that the problem of our synthetic example is that the corresponding digits obviously do not fill the whole image space, nevertheless the classifier has to do a prediction for all possible images. This could be handled by introducing a background class, but then it would be even\nmore important that the classifier can produce disconnected decision regions which naturally requires a minimal width of d+ 1 of the network.\nIn Figure 7, we show another similar experiment on MNIST dataset, but now for all the 10 image classes. We train a network with 200 hidden units, leaky ReLU and softmax cross-entropy loss to zero training error. Once again, one can see that there exists a continuous path that connects two different-looking images of digit 5 (blue class) where every image along this path is classified as blue class with high confidence. Moreover this path goes through a preconstructed adversarial image of the red class (digit 4)."}, {"heading": "5. Conclusion", "text": "We have shown that deep neural networks (with a certain class of activation functions) need to have in general width larger than the input dimension in order to learn disconnected decision regions. It remains an open problem if our current requirement \u03c3(R) = R can be removed. While our result does not resolve the question how to choose the network architecture in practice, it provides at least a guideline how to choose the width of the network. Moreover, our result and experiments show that too narrow networks produce high confidence predictions on a path connecting the true disconnected decision regions which could be used to attack these networks using adversarial manipulation."}, {"heading": "Acknowledgements", "text": "The authors would like to thank the reviewers for their helpful comments on the paper and Francesco Croce for bringing up a counter-example for the ReLU activation function."}], "year": 2018, "references": [{"title": "Understanding deep neural networks with rectified linear units", "authors": ["R. Arora", "A. Basu", "P. Mianjy", "A. Mukherjee"], "venue": "In ICLR,", "year": 2018}, {"title": "A tropical approach to neural networks with piecewise linear activations, 2018", "authors": ["V. Charisopoulos", "P. Maragos"], "year": 2018}, {"title": "Fast and accurate deep network learning by exponential linear units (elus)", "authors": ["D. Clevert", "T. Unterthiner", "S. Hochreiter"], "venue": "In ICLR,", "year": 2016}, {"title": "Convolutional rectifier networks as generalized tensor decompositions", "authors": ["N. Cohen", "A. Shashua"], "venue": "In ICML,", "year": 2016}, {"title": "On the expressive power of deep learning: A tensor analysis", "authors": ["N. Cohen", "O. Sharir", "A. Shashua"], "venue": "In COLT,", "year": 2016}, {"title": "Approximation by superpositions of a sigmoidal function", "authors": ["G. Cybenko"], "venue": "Mathematics of Control, Signals, and Systems,", "year": 1989}, {"title": "Shallow vs. deep sum-product networks", "authors": ["O. Delalleau", "Y. Bengio"], "venue": "In NIPS,", "year": 2011}, {"title": "The power of depth for feedforward neural networks", "authors": ["R. Eldan", "O. Shamir"], "venue": "In COLT,", "year": 2016}, {"title": "Classification regions of deep neural networks, 2017", "authors": ["A. Fawzi", "S.M.M. Dezfooli", "P. Frossard", "S. Soatto"], "year": 2017}, {"title": "Approximating continuous functions by relu nets of minimal width, 2017", "authors": ["B. Hanin", "M. Sellke"], "year": 2017}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "authors": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "In ICCV,", "year": 2015}, {"title": "Multilayer feedforward networks are universal approximators", "authors": ["K. Hornik", "M. Stinchcombe", "H. White"], "venue": "Neural Networks,", "year": 1989}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "authors": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Grishick", "S. Guadarrama", "T. Darrell"], "venue": "In ACM Int. Conference on Multimedia,", "year": 2014}, {"title": "Adversarial machine learning at scale", "authors": ["A. Kurakin", "I. Goodfellow", "S. Bengio"], "venue": "In ICLR,", "year": 2016}, {"title": "Adversarial examples in the physical world", "authors": ["A. Kurakin", "I. Goodfellow", "S. Bengio"], "venue": "In ICLR Workshop,", "year": 2017}, {"title": "Multilayer feedforward networks with a nonpolynomial activation function can approximate any function", "authors": ["M. Leshno", "Y. Lin", "A. Pinkus", "S. Schocken"], "venue": "Neural Networks,", "year": 1993}, {"title": "Why deep neural networks for function approximation", "authors": ["S. Liang", "R. Srikant"], "venue": "In ICLR,", "year": 2017}, {"title": "The expressive power of neural networks: A view from the width", "authors": ["Z. Lu", "H. Pu", "F. Wang", "Z. Hu", "L. Wang"], "year": 2017}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "authors": ["A.L. Maas", "A.Y. Hannun", "A.Y. Ng"], "venue": "In ICML,", "year": 2013}, {"title": "Formation of disconnected decision regions with a single hidden layer", "authors": ["J. Makhoul", "A. El-Jaroudi", "R. Schwartz"], "venue": "IJCNN, pp", "year": 1989}, {"title": "Partitioning capbabilities of two-layer neural networks", "authors": ["J. Makhoul", "A. El-Jaroudi", "R. Schwartz"], "venue": "IEEE Trans. Signal Processing,", "year": 1990}, {"title": "Deep vs. shallow networks : An approximation theory perspective, 2016", "authors": ["H. Mhaskar", "T. Poggio"], "year": 2016}, {"title": "On the number of linear regions of deep neural networks", "authors": ["G. Montufar", "R. Pascanu", "K. Cho", "Y. Bengio"], "venue": "In NIPS,", "year": 2014}, {"title": "The loss surface of deep and wide neural networks", "authors": ["Q. Nguyen", "M. Hein"], "venue": "In ICML,", "year": 2017}, {"title": "Optimization landscape and expressivity of deep cnns", "authors": ["Q. Nguyen", "M. Hein"], "venue": "In ICML,", "year": 2018}, {"title": "On the number of response regions of deep feedforward networks with piecewise linear activations", "authors": ["R. Pascanu", "G. Montufar", "Y. Bengio"], "venue": "In ICLR,", "year": 2014}, {"title": "Why and when can deep \u2013 but not shallow \u2013 networks avoid the curse of dimensionality: a review, 2016", "authors": ["T. Poggio", "H. Mhaskar", "L. Rosasco", "B. Miranda", "Q. Liao"], "year": 2016}, {"title": "Sum-product networks: A new deep architecture", "authors": ["H. Poon", "P. Domingos"], "venue": "In ICCV Workshop,", "year": 2011}, {"title": "On the expressive power of deep neural networks", "authors": ["M. Raghu", "B. Poole", "J. Kleinberg", "S. Ganguli", "J. SohlDickstein"], "year": 2017}, {"title": "Depth-width tradeoffs in approximating natural functions with neural networks", "authors": ["I. Safran", "O. Shamir"], "venue": "In ICML,", "year": 2017}, {"title": "Bounding and counting linear regions of deep neural networks, 2018", "authors": ["T. Serra", "C. Tjandraatmadja", "S. Ramalingam"], "year": 2018}, {"title": "Representation benefits of deep feedforward networks, 2015. arXiv:1509.08101v2", "authors": ["M. Telgarsky"], "year": 2015}, {"title": "Benefits of depth in neural networks", "authors": ["M. Telgarsky"], "venue": "In COLT,", "year": 2016}, {"title": "Error bounds for approximations with deep relu networks, 2016", "authors": ["D. Yarotsky"], "year": 2016}], "id": "SP:450e41e4334b852f05cd9ddff138c5d24cb0db11", "authors": [{"name": "Quynh Nguyen", "affiliations": []}, {"name": "Mahesh Chandra Mukkamala", "affiliations": []}, {"name": "Matthias Hein", "affiliations": []}], "abstractText": "In the recent literature the important role of depth in deep learning has been emphasized. In this paper we argue that sufficient width of a feedforward network is equally important by answering the simple question under which conditions the decision regions of a neural network are connected. It turns out that for a class of activation functions including leaky ReLU, neural networks having a pyramidal structure, that is no layer has more hidden units than the input dimension, produce necessarily connected decision regions. This implies that a sufficiently wide hidden layer is necessary to guarantee that the network can produce disconnected decision regions. We discuss the implications of this result for the construction of neural networks, in particular the relation to the problem of adversarial manipulation of classifiers.", "title": "Neural Networks Should Be Wide Enough to Learn Disconnected Decision Regions"}