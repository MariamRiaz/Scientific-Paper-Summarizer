1	32	Imagine a child hears the word dax for the first time while observing a white rabbit jumping around – dax might mean WHITE RABBIT, RABBIT, ANIMAL, CUTE, LOOK, etc.
4	30	One mechanism for achieving this is crosssituational learning (e.g., Siskind, 1996; Frank et al., 2007; Fazly et al., 2010; Kachergis et al., 2012).
7	44	For example, children learn that the word dog refers to all kinds of dogs, and not to a specific breed, such as Dalmatians, or to a more general category, such as animals – even though some of these choices (e.g., animals) are compatible with all the cross-situational evidence available for dog (because all dogs are also animals).
10	25	According to this bias, children prefer to associate a word to a set of objects that form a basic-level category, such as dogs or trucks, and that share a significant number of attributes.
19	27	It is important to understand how word generalization occurs when embedded in the natural process of learning a word meaning and in the context of more limited category knowledge.
20	23	We address these issues by providing a unified account of word learning and word generalization within a computational model of crosssituational learning.
25	24	In each training trial of an experiment, participants hear a novel word (such as fep) and observe one or more instances exemplifying the word (in the form of pictures for adults and toy objects for children).
26	63	The conditions vary in that the make-up of the set of training instances is representative of different levels of a taxonomy (e.g., all Dalmatians vs. various kinds of dogs vs. various kinds of animals).
30	38	For example, in one training condition, participants are shown a Dalmatian, a poodle, and a beagle in three consecutive trials, hearing the word fep to refer to each object.
33	62	1) • a basic-level match: an object of the same basic-level category as a training object (e.g., a dog, but not the same breed as one in training [which would be a subordinate match]) • a superordinate match: an object of the same superordinate category as the training objects (e.g., another kind of animal, but not one seen in training [which would be a subordinate or basic-level match]) X&T report the percentage of test objects of each type of match that are selected by participants within each training condition; see Fig.
39	26	For the 3-basic-level and 3-superordinate conditions, the adults show generalization up to categories consistent with the evidence – i.e., at the basic and superordinate levels, respectively.
41	25	In the other conditions, children’s behaviour is similar to adults, but shows somewhat less generalization to unseen types of objects (e.g., other kinds of dogs/animals than those in training).
43	26	Here we give an overview of the FAS model; the next section explains extensions to handle the novel word generalization task.
46	63	Given such input, for each word w, the model of FAS learns a probability distribution over all semantic features, Pt(.|w), which represents the word’s meaning at time t. Initially, at time t = 0, P0(.|w) is a uniform distribution.
50	35	These probabilities are incrementally accumulated for each wj–fi pair, capturing the overall strength of association of wj and fi at time t: assoct(fi, wj) = assoct−1(fi, wj) + Pt(aij |U, fi) (2) The (normalized) association scores then serve as the basis for the incremental adjustment of the meaning probabilities of all features fi for each word wj seen in the input at time t: Pt(fi|wj) = assoct(fi, wj) + γ∑ fm∈M assoct(fm, wj) + k γ (3) HereM is the group of all features that the model has observed, k is the expected number of such features, and γ is a small smoothing parameter, which determines the prior probability of observing a new feature.
64	54	Each feature group is comprised of all features at the same level of specificity in the category hierarchy, which are therefore mutually exclusive, such as DOG, CAT, and BIRD (i.e., different kinds of animals).
83	63	The unseen probability is sensitive to how many instances of features from a group have already been seen with a word wj : As the model observes more instances (tokens) of features from G with wj , the corresponding assoct score(s) increase, thereby increasing the denominator in Eqn.
84	33	Thus the tendency to generalize wj to more features in G – i.e., to accept additional features as part of the meaning of wj – will decrease as the model has more evidence of (observed) features in that group occurring with wj .
85	39	Generalization of a category to include new kinds of items is typically a function of both token and type frequency (e.g., Bybee, 1985; Croft and Cruse, 2004): a category with more diverse types is more easily extended to new cases.
86	30	While the evolving association scores capture the effect of observing more feature tokens, our model as given does not distinguish the number of different types of features seen within a group (e.g., two DOGs vs. one DOG and one CAT).
118	61	Looking at the results in Figure 3b, we can see that the child learner generally replicates the patterns of results observed in X&T’s experiment on children (cf.
121	36	In contrast, after seeing a single training example (the (a) Adult data: 1-ex.
129	46	For example, having seen 3 types of animals (“3 super.” condition), the model can readily accommodate that fep refers to another kind of animal, in contrast to the “3 basic” condition, where it has seen the same number of tokens but only a single feature type from the feature group at that level (3 dogs).
130	48	We can also clearly see the inverse impact of token frequencies on generalization: the more examples of a single subordinate type are seen, the less the model accepts that fep refers to a different kind of subordinate (the “3-subord.” vs. “1-ex.” conditions).
154	32	Recall that in our earlier experiments, each object was represented as a set of features drawn from 4 different feature groups.
155	27	We take this representation as the least fine-grained representation and use it for the category “seats”.
163	69	The model mimics child behavior found by Xu and Tenenbaum (2007): it shows a “basic-level” bias – a preference for word meanings that refer to basic-level objects (like dogs), in contrast to higher-level (animals) or lower-level (Dalmatians) categories – and does so under parameter settings that treat all levels of category the same in the model (i.e., with no built-in basic-level bias).
168	72	One shortcoming of the current model is its built-in ability to “detect” in the input that DOG and CAT features are more specific than ANIMAL features.
169	80	The next step is to consider how the model might learn these relationships from its evolving knowledge of co-occurring features.
171	43	For example, this problem underlies one way to determine the selectional preferences of a verb: extract the set of nouns that occur as objects of the verb, map them to the concept nodes in a hierarchy such as WordNet, and then determine the best overarching WordNet category for capturing the salient properties of the object nouns overall (e.g., Li and Abe, 1998; Clark and Weir, 2001).
