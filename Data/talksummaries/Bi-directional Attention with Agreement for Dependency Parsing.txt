21	5	In the following, we first present how the token embeddings are constructed.
26	8	Specifically, the token embedding is computed as Eformeformi + E poseposi + E lemmaelemmai + · · · , where ei’s are one-hot encoding vectors for the ith word, and E’s are parameters to be learned that store the continuous embeddings for corresponding feature.
35	70	In the remaining part of the paper, we refer to xi ∈ Rp as the token embedding for word at position i.
36	7	Note the subscript i is substituted by j and t for the memory and query components, respectively.
38	65	Given a sentence of length n, the parser first uses a bi-directional RNN to construct n + 1 headword embeddings, m0,m1, .
39	76	,mn ∈ Re, with m0 reserved for the ROOT symbol.
42	10	,qn ∈ Rd are constructed recursively by conditioning on all headword embeddings.
46	11	Memory Component: The proposed BiAtt-DP uses a bi-directional RNN to obtain the memory vectors.
47	7	At time step j, the current hidden state vector hlj ∈ Re/2 (or hrj ∈ Re/2) is computed as a non-linear transformation based on the current input vector xj and the previous hidden state vector hlj−1 (or h r j+1), i.e. h l j = GRU(h l j−1,xj) (or hrj = GRU(h r j+1,xj)).
48	10	Ideally, the recursive nature of the RNN allows it to capture all context information from one-side, and a bi-directional RNN can thus capture context information from both sides.
49	32	We concatenate the hidden layers of the left-to-right RNN and the right-to-left RNN for the word at position j as the memory vector mj = [ hlj ;h r j ] .
50	45	These memory vectors are expected to encode the words and their context information in the headword space.
51	86	Query Component: For each query component, we use a single-directional RNN with GRU to obtain the query vectors qj’s, which are the hidden state vectors of the RNN.
52	93	Each qt is used to query the memory component, returning association scores st,j’s between the word at position t and the head- word at position j for j ∈ {0, · · · , n}, i.e. st,j = v Tφ (Cmj + Dqt) , (1) where φ(·) is the element-wise hyperbolic tangent function, and C ∈ Rh×e, D ∈ Rh×d and v ∈ Rh are model parameters.
53	86	Then, we can obtain probabilities (aka attention weights), at,0, · · · , at,n, over all headwords in the sentence by normalizing st,j’s, using a softmax function at = softmax(st).
59	52	On the other hand, by recursively feeding both the query vector and the soft headword embedding into the RNN, the model implicitly captures high-order parsing history information, which can potentially improve the parsing accuracy (Yamada and Matsumoto, 2003; McDonald and Pereira, 2006).
60	6	However, for a graph-based dependency parser, utilizing parsing history features is computationally expensive.
68	11	For the bi-directional attention model, the underlying probability distributions alt and a r t may not agree with each other.
69	17	In order to encourage the agreement, we use the mathematically convenient metric, i.e. the squared Hellinger distance H2 ( alt||art ) , for quantifying the distance between these two distri- butions.
74	12	As we can see, the loss function (3) tries to minimize the distance between the golden alignment and the intersection of the two directional attention alignments at every time step.
77	59	Alternatively, we can treat (log alt,j + log a r t,j) as a score of the corresponding arc and then search for the MST to form a dependency parse tree, as proposed in (McDonald et al., 2005).
78	12	The MST search is achieved via the ChuLiu-Edmonds algorithm (Chu and Liu, 1965; Edmonds, 1967), which can be implemented in O(n2) for dense graphs according to Tarjan (1977).
79	59	In practice, the MST search slows down the parsing speed by 6–10%.
80	47	However, it forces the parser to produce a valid tree, and we observe a slight improvement on parsing accuracy in most cases.
81	41	After obtaining each modifier and its soft header embeddings, we use a single-layer perceptron to predict the head-modifier relation, i.e. yt = softmax ( U [ m̃lt; m̃ r t ] + W [ qlt; q r t ]) , (4) where yt,1, · · · , yt,m are the probabilities of m possible relations, and U ∈ Rm×2e and W ∈ Rm×2d are model parameters.
82	20	For the t-th word (modifier) wt in a sentence of length n, let H lt and H r t denote random variables representing the predicted headword from forward (left-to-right) and backward (right-to-left) parsing directions, respectively.
83	158	Also let Rt denote the random variable representing the dependency relation for wt.
85	5	Note that the long-span context and high-order parsing history information are injected when we model P (H lt |w1:n), P (Hrt |w1:n) and P (Rt|w1:n), as discussed in Section 2.2.
86	5	As discussed in Section 2.3, the model can be trained by encouraging attention agreement between two query components.
89	41	Errors of yt come from the arc labels, whereas there are two source of errors for at, one from the headword labels and the other back-propagated from errors of yt.
