0,1,label2,summary_sentences
"People vary widely both in their linguistic preferences when producing language and in their ability to understand specific natural-language expressions, depending on what they know about the domain, their age and cognitive capacity, and many other factors.",1 Introduction,[0],[0]
"It has long been recognized that effective NLG systems should therefore adapt to the current user, in order to generate language which works well for them.",1 Introduction,[0],[0]
"This adaptation needs to address all levels of the NLG pipeline, including discourse planning (Paris, 1988), sentence planning (Walker et al., 2007), and RE generation (Janarthanam and Lemon, 2014), and depends on many features of the user, including level of expertise and language proficiency, age, and gender.
",1 Introduction,[0],[0]
Existing techniques for adapting the output of an NLG system have shortcomings which limit their practical usefulness.,1 Introduction,[0],[0]
"Some systems need user-specific information in training (Ferreira and Paraboni, 2014) and therefore cannot generalize to unseen users.",1 Introduction,[0],[0]
"Other systems assume that each user in the training data is annotated with their group, which allows them to learn a model from the data of each group.",1 Introduction,[0],[0]
"However, hand-designed user groups
may not reflect the true variability of the data, and may therefore inhibit the system’s ability to flexibly adapt to new users.
",1 Introduction,[0],[0]
"In this paper, we present a user adaptation model for NLG systems which induces user groups from training data in which these groups were not annotated.",1 Introduction,[0],[0]
"At training time, we probabilistically assign users to groups and learn the language preferences for each group.",1 Introduction,[0],[0]
"At evaluation time, we assume that our system has a chance to interact with each new user repeatedly – e.g., in the context of a dialogue system.",1 Introduction,[0],[0]
"It will then calculate an increasingly accurate estimate of the user’s group membership based on observable behavior, and use it to generate utterances that are suitable to the user’s true group.
",1 Introduction,[0],[0]
We evaluate our model on two tasks involving the generation of referring expressions (RE).,1 Introduction,[0],[0]
"First, we predict the use of spatial relations in humanlike REs in the GRE3D domain (Viethen and Dale, 2010) using a log-linear production model in the spirit of Ferreira and Paraboni (2014).",1 Introduction,[0],[0]
"Second, we predict the comprehension of generated REs, in a synthetic dataset based on data from the GIVE Challenge domain (Striegnitz et al., 2011) with the log-linear comprehension model of Engonopoulos et al. (2013).",1 Introduction,[0],[0]
"In both cases, we show that our model discovers user groups in the training data and infers the group of unseen users with high confidence after only a few interactions during testing.",1 Introduction,[0],[0]
"In the GRE3D domain, our system outperformed a strong baseline which used demographic information for the users.",1 Introduction,[0],[0]
Differences between individual users have a substantial impact on language comprehension.,2 Related Work,[0],[0]
"Factors that play a role include level of expertise and spatial ability (Benyon and Murray, 1993); age (Häuser et al., 2017); gender (Dräger and Koller,
ar X
iv :1
80 6.
",2 Related Work,[0],[0]
"05 94
7v 1
[ cs
.C",2 Related Work,[0],[0]
"L
] 1
5 Ju
n 20
18
2012); or language proficiency (Koller et al., 2010).
",2 Related Work,[0],[0]
Individual differences are also reflected in the way people produce language.,2 Related Work,[0],[0]
"Viethen and Dale (2008) present a corpus study of human-produced REs (GRE3D3) for simple visual scenes, where they note two clearly distinguishable groups of speakers, one that always uses a spatial relation and one that never does.",2 Related Work,[0],[0]
Ferreira and Paraboni (2014) show that a model using speaker-specific information outperforms a generic model in predicting the attributes used by a speaker when producing an RE.,2 Related Work,[0],[0]
"However, their system needs to have seen the particular speaker in training, while our system can dynamically adapt to unseen users.",2 Related Work,[0],[0]
"Ferreira and Paraboni (2017) also demonstrate that splitting speakers in predefined groups and training each group separately improves the human likeness of REs compared to training individual user models.
",2 Related Work,[0],[0]
"The ability to adapt to the comprehension and production preferences of a user is especially important in the context of a dialog system, where there are multiple chances of interacting with the same user.",2 Related Work,[0],[0]
Some methods adapt to dialog system users by explicitly modeling the users’ knowledge state.,2 Related Work,[0],[0]
"An early example is Paris (1988); she selects a discourse plan for a user, depending on their level of domain knowledge ranging between novice and expert, but provides no mechanism for inferring the group to which the user belongs.",2 Related Work,[0],[0]
"Rosenblum and Moore (1993) try to infer what knowledge a user possesses during dialogue, based on the questions they ask.",2 Related Work,[0],[0]
Janarthanam and Lemon (2014) adapt to unseen users by using reinforcement learning with simulated users to make a system able to adjust to the level of the user’s knowledge.,2 Related Work,[0],[0]
"They use five predefined groups from which they generate the simulated users’ behavior, but do not assign real users to these groups.",2 Related Work,[0],[0]
"Our system makes no assumptions about the user’s knowledge and does not need to train with simulated users, or use any kind of information-seeking moves; we instead rely on the groups that are discovered in training and dynamically assign new, unseen users, based only on their observable behavior in the dialog.
",2 Related Work,[0],[0]
"Another example of a user-adapting dialog component is SPaRKy (Walker et al., 2007), a trainable sentence planner that can tailor sentence plans to individual users’ preferences.",2 Related Work,[0],[0]
"This requires training on separate data for each user; in contrast to this, we leverage the similarities between users and can take advantage of the full training data.",2 Related Work,[0],[0]
We start with a basic model of the way in which people produce and comprehend language.,3 Log-linear models for NLG in dialog,[0],[0]
"In order to generalize over production and comprehension, we will simply say that a human language user exhibits a certain behavior b among a range of possible behaviors, in response to a stimulus s.",3 Log-linear models for NLG in dialog,[0],[0]
"The behavior of a speaker is the utterance b they produce in order to achieve a communicative goal s; the behavior of a listener is the meaning b which they assign to the utterance s they hear.
",3 Log-linear models for NLG in dialog,[0],[0]
"Given this terminology, we define a basic loglinear model (Berger et al., 1996) of language use as follows:
P (b|s; ρ) = exp(ρ · φ(b, s))∑ b′",3 Log-linear models for NLG in dialog,[0],[0]
"exp(ρ · φ(b′, s))
",3 Log-linear models for NLG in dialog,[0],[0]
"(1)
where ρ is a real-valued parameter vector of length n and φ(b, s) is a vector of real-valued feature functions f1, ..., fn over behaviors and stimuli.",3 Log-linear models for NLG in dialog,[0],[0]
"The parameters can be trained by maximum-likelihood estimation from a corpus of observations (b, s).",3 Log-linear models for NLG in dialog,[0],[0]
"In addition to maximum-likelihood training it is possible to include some prior probability distribution, which expresses our belief about the probability of any parameter vector and which is generally used for regularization.",3 Log-linear models for NLG in dialog,[0],[0]
"The latter case is referred to as a posteriori training, which selects the value of ρ that maximizes the product of the parameter probability and the probability of the data.
",3 Log-linear models for NLG in dialog,[0],[0]
"In this paper, we focus on the use of such models in the context of the NLG module of a dialogue system, and more specifically on the generation of referring expressions (REs).",3 Log-linear models for NLG in dialog,[0],[0]
"Using (1) as a comprehension model, Engonopoulos et al. (2013) developed an RE generation model in which the stimulus s = (r, c) consists of an RE r and a visual context c of the GIVE Challenge (Striegnitz et al., 2011), as illustrated in Fig. 1.",3 Log-linear models for NLG in dialog,[0],[0]
The behavior is the object b in the visual scene to which the user will resolve the RE.,3 Log-linear models for NLG in dialog,[0],[0]
"Thus for instance, when we consider the RE r =“the blue button” in the context of Fig. 1, the log-linear model may assign a higher probability to the button on the right than to the one in the background.",3 Log-linear models for NLG in dialog,[0],[0]
"Engonopoulos and Koller (2014) develop an algorithm for generating the RE r which maximizes P (b∗|s; ρ), where b∗ is the intended referent in this setting.
",3 Log-linear models for NLG in dialog,[0],[0]
"Conversely, log-linear models can also be used to directly capture how a human speaker would refer to an object in a given scene.",3 Log-linear models for NLG in dialog,[0],[0]
"In this case, the stimulus s = (a, c) consists of the target object a and
the visual context c, and the behavior b is the RE.",3 Log-linear models for NLG in dialog,[0],[0]
"We follow Ferreira and Paraboni (2014) in training individual models for the different attributes which can be used in the RE (e.g., that a is a button; that it is blue; that the RE contains a binary relation such as “to the right of”), such that we can simply represent b as a binary choice b ∈ {1,−1} between whether a particular attribute should be used in the RE or not.",3 Log-linear models for NLG in dialog,[0],[0]
"We can then implement an analog of Ferreira’s model in terms of (1) by using feature functions φ(b, a, c) = b · φ′(a, c), where φ′(a, c) corresponds to their context features, which do not capture any speaker-specific information.",3 Log-linear models for NLG in dialog,[0],[0]
"As discussed above, a user-agnostic model such as (1) does not do justice to the variability of language comprehension and production across different speakers and listeners.",4 Log-linear models with user groups,[0],[0]
We will therefore extend it to a model which distinguishes different user groups.,4 Log-linear models with user groups,[0],[0]
We will not try to model why1 users behave differently.,4 Log-linear models with user groups,[0],[0]
"Instead our model sorts users into groups simply based on the way in which they respond to stimuli, in the sense of Section 3, and implements this by giving each group g its own parameter vector ρ(g).",4 Log-linear models with user groups,[0],[0]
"As a theoretical example, Group 1 might contain users who reliably comprehend REs which use colors (“the green button”), whereas Group 2 might contain users who more easily understand relational REs (“the button next to the lamp”).",4 Log-linear models with user groups,[0],[0]
"These groups are then discovered at training time.
",4 Log-linear models with user groups,[0],[0]
"When our trained NLG system starts interacting with an unseen user u, it will infer the group to which u belongs based on u’s observed responses to previous stimuli.",4 Log-linear models with user groups,[0],[0]
"Thus as the dialogue with u unfolds, the system will have an increasingly pre-
1E.g., in the sense of explicitly modeling sociolects or the difference between novice system users vs. experts.
cise estimate of the group to which u belongs, and will thus be able to generate language which is increasingly well-tailored to this particular user.",4 Log-linear models with user groups,[0],[0]
"We assume training data D = {(bi, si, ui)}i which contains stimuli si together with the behaviors bi which the users ui exhibited in response to si.",4.1 Generative story,[0],[0]
"We write D(u) = {(bu1 , su1), . . .",4.1 Generative story,[0],[0]
"(buN , suN )} for the data points for each user",4.1 Generative story,[0],[0]
"u.
",4.1 Generative story,[0],[0]
"The generative story we use is illustrated in Fig. 2; observable variables are shaded gray, unobserved variables and parameters to be set in training are shaded white and externally set hyperparameters have no circle around them.",4.1 Generative story,[0],[0]
"Arrows indicate which variables and parameters influence the probability distribution of other variables.
",4.1 Generative story,[0],[0]
"We assume that each user belongs to a group g ∈ {1, . . .",4.1 Generative story,[0],[0]
",K}, where the number K of groups is fixed beforehand based on, e.g., held out data.",4.1 Generative story,[0],[0]
"A group g is assigned to u at random from the distribution
P (g|π) = exp(πg)∑K g′=1 exp(πg′)
(2)
",4.1 Generative story,[0],[0]
"Here π ∈ RK is a vector of weights, which defines how probable each group is a-priori.
",4.1 Generative story,[0],[0]
"We replace the single parameter vector ρ of (1) with group-specific parameters vectors ρ(g), thus obtaining a potentially different log-linear model P ( b|s; ρ(g) ) for each group.",4.1 Generative story,[0],[0]
"After assigning a group, our model generates responses bu1 , . . .",4.1 Generative story,[0],[0]
", b u N at
random from P ( b|s; ρ(g) ) , based on the group specific parameter vector and the stimuli su1 , . . .",4.1 Generative story,[0],[0]
", s u N .",4.1 Generative story,[0],[0]
This accounts for the generation of the data.,4.1 Generative story,[0],[0]
"We model the parameter vectors π ∈ RK , and ρ(g) ∈",4.1 Generative story,[0],[0]
"Rn for every 1 ≤ g ≤ K as drawn from
P (D; θ) = ∏ u∈U K∑ g=1",4.1 Generative story,[0],[0]
"P (g|π) · ∏ d∈D(u) P ( bd|sd; ρ(g) ) · N (π|0, σ(π)) · K∏",4.1 Generative story,[0],[0]
g=1 N,4.1 Generative story,[0],[0]
"( ρ(g)|0, σ(ρ) )",4.1 Generative story,[0],[0]
"(3) L(θ) =
∑ u∈U log K∑",4.1 Generative story,[0],[0]
g=1,4.1 Generative story,[0],[0]
P (g|π) · ∏ d∈D(u) P ( bd|sd; ρ(g) ),4.1 Generative story,[0],[0]
"(4)
AL(θ) = ∑ u∈U K∑ g=1
P (g|D(u); θ(i−1)) ·",4.1 Generative story,[0],[0]
logP (g|π) +,4.1 Generative story,[0],[0]
"∑
d∈Du
logP ( bd|sd; ρ(g) )",4.1 Generative story,[0],[0]
"(5)
normal distributions N (0, σ(π)), and N (0, σ(ρ)), which are centered at 0 with externally given variances and no covariance between parameters.",4.1 Generative story,[0],[0]
This has the effect of making parameter choices close to zero more probable.,4.1 Generative story,[0],[0]
"Consequently, our models are unlikely to contain large weights for features that only occurred a few times or which are only helpful for a few examples.",4.1 Generative story,[0],[0]
"This should reduce the risk of overfitting the training set.
",4.1 Generative story,[0],[0]
The equation for the full probability of the data and a specific parameter setting is given in (3).,4.1 Generative story,[0],[0]
"The left bracket contains the likelihood of the data, while the right bracket contains the prior probability of the parameters.",4.1 Generative story,[0],[0]
Once we have set values θ =,4.2 Predicting user behavior,[0],[0]
"(π, ρ(1), . . .",4.2 Predicting user behavior,[0],[0]
", ρ(K))",4.2 Predicting user behavior,[0],[0]
"for all the parameters, we want to predict what behavior b a user u will exhibit in response to a stimulus s. If we encounter a completely new user u, the prior user group distribution from (2) gives the probability that this user belongs to each group.",4.2 Predicting user behavior,[0],[0]
"We combine this with the group-specific log-linear behavior models to obtain the distribution:
P (b|s; θ) = K∑ g=1 P ( b|s; ρ(g) )",4.2 Predicting user behavior,[0],[0]
"· P (g|π) (6)
",4.2 Predicting user behavior,[0],[0]
"Thus, we have a group-aware replacement for (1).",4.2 Predicting user behavior,[0],[0]
"Furthermore, in the interactive setting of a dialogue system, we may have multiple opportunities to interact with the same user u. We can then develop a more precise estimate of u’s group based on their responses to previous stimuli.",4.2 Predicting user behavior,[0],[0]
Say that we have made the previous observations D(u) =,4.2 Predicting user behavior,[0],[0]
"{〈s1, b1〉, . . .",4.2 Predicting user behavior,[0],[0]
", 〈sN , bN 〉} for user u.",4.2 Predicting user behavior,[0],[0]
"Then we can use Bayes’ theorem to calculate a posterior estimate for u’s group membership:
P ( g|D(u); θ ) ∝",4.2 Predicting user behavior,[0],[0]
P ( D(u)|ρ(g) ) ·,4.2 Predicting user behavior,[0],[0]
"P (g|π) (7)
",4.2 Predicting user behavior,[0],[0]
This posterior balances whether a group is likely in general against whether members of that group behave as u does.,4.2 Predicting user behavior,[0],[0]
"We can use Pu(g) = P ( g|D(u); θ ) as our new estimate for the group membership probabilities for u and replace (6) with: P ( b|s,D(u); θ ) =
K∑ g=1 P ( b|s; ρ(g) ) ·",4.2 Predicting user behavior,[0],[0]
"Pu(g) (8)
for the next interaction with u. An NLG system can therefore adapt to each new user over time.",4.2 Predicting user behavior,[0],[0]
"Before the first interaction with u, it has no specific information about u and models u’s behavior based on (6).",4.2 Predicting user behavior,[0],[0]
"As the system interacts with u repeatedly, it collects observationsD(u) about u’s behavior.",4.2 Predicting user behavior,[0],[0]
"This allows it to calculate an increasingly accurate posterior Pu(g) = P ( g|D(u); θ ) of u’s group membership, and thus generate utterances which are more and more suitable to u using (8).",4.2 Predicting user behavior,[0],[0]
"So far we have not discussed how to find settings for the parameters θ = π, ρ(1), . . .",5 Training,[0],[0]
", ρ(K), which define our probability model.",5 Training,[0],[0]
"The key challenge for training is the fact that we want to be able to train while treating the assignment of users to groups as unobserved.
",5 Training,[0],[0]
"We will use a maximum a posteriori estimate for θ, i.e., the setting which maximizes (3) when D is our training set.",5 Training,[0],[0]
"We will first discuss how to pick parameters to maximize only the left part of (3), i.e., the data likelihood, since this is the part that involves unobserved variables.",5 Training,[0],[0]
We will then discuss handling the parameter prior in section 5.2.,5 Training,[0],[0]
"Gradient descent based methods (Nocedal and Wright, 2006) exist for finding the parameter settings which maximize the likelihood for log-linear
models, under the conditions that all relevant variables are observed in the training data.",5.1 Expectation Maximization,[0],[0]
"If group assignments were given, gradient computations, and therefore gradient based maximization, would be straightforward for our model.",5.1 Expectation Maximization,[0],[0]
"One algorithm specifically designed to solve maximization problems with unknown variables by reducing them to the case where all variables are observed, is the expectation maximization (EM) algorithm (Neal and Hinton, 1999).",5.1 Expectation Maximization,[0],[0]
"Instead of maximizing the data likelihood from (3) directly, EM equivalently maximizes the log-likelihood, given in (4).",5.1 Expectation Maximization,[0],[0]
"It helps us deal with unobserved variables by introducing “pseudo-observations” based on the expected frequency of the unobserved variables.
",5.1 Expectation Maximization,[0],[0]
"EM is an iterative algorithm which produces a sequence of parameter settings θ(1), . . .",5.1 Expectation Maximization,[0],[0]
", θ(n).",5.1 Expectation Maximization,[0],[0]
Each will achieve a larger value for (4).,5.1 Expectation Maximization,[0],[0]
Each new setting is generated in two steps: (1) an lower bound on the log-likelhood is generate and (2) the new parameter setting is found by optimizing this lower bound.,5.1 Expectation Maximization,[0],[0]
"To find the lower bound we compute the probability for every possible value the unobserved variables could have had, based on the observed variables and the parameter setting θ(i−1) from the last iteration step.",5.1 Expectation Maximization,[0],[0]
"Then the lower bound essentially assumes that each assignment was seen with a frequency equal to these probabilities - these are the “pseudo-observations”.
",5.1 Expectation Maximization,[0],[0]
In our model the unobserved variables are the assignments of users to groups.,5.1 Expectation Maximization,[0],[0]
"The probability of seeing each user u assigned to a group, given all the data D(u) and the model parameters from the last iteration θ(i−1), is simply the posterior group membership probability P ( g|D(u); θ(i−1) ) .",5.1 Expectation Maximization,[0],[0]
The lower bound is then given by (5).,5.1 Expectation Maximization,[0],[0]
"This is the sum of the log probabilities of the data points under each group model, weighted by P ( g|D(u); θ(i−1) ) .",5.1 Expectation Maximization,[0],[0]
We can now use gradient descent techniques to optimize this lower bound.,5.1 Expectation Maximization,[0],[0]
To fully implement EM we need a way to maximize (5).,5.1.1 Maximizing the Lower Bound,[0],[0]
"This can be achieved with gradient based methods such as L-BFGS (Nocedal and Wright, 2006).",5.1.1 Maximizing the Lower Bound,[0],[0]
Here the gradient refers to the vector of all partial derivatives of the function with respect to each dimension of θ.,5.1.1 Maximizing the Lower Bound,[0],[0]
"We therefore need to calculate these partial derivatives.
",5.1.1 Maximizing the Lower Bound,[0],[0]
There are existing implementations of the gradient computations our base model such as in Engonopoulos et al. (2013).,5.1.1 Maximizing the Lower Bound,[0],[0]
"The gradients of (5)
for each of the ρ(g) is simply the gradient for the base model on each datapoint d weighted by P ( g|D(u); θ(i−1) )",5.1.1 Maximizing the Lower Bound,[0],[0]
"if d ∈ Du, i.e., the probability that the user u from which the datapoint originates belongs to group g. We can therefore compute the gradients needed for each ρ(g) by using implementations developed for the base model.
",5.1.1 Maximizing the Lower Bound,[0],[0]
"We also need gradients for the parameters in π, which are only used in our extended model.",5.1.1 Maximizing the Lower Bound,[0],[0]
"We can use the rules for computing derivatives to find, for each dimension g:
∂UL(θ) ∂πg = ∑ u∈U Pu(g)− exp (πg)∑K g′=1 exp ( πg′ )
where Pu(g) = P ( g|D(u); θ(i−1) ) .",5.1.1 Maximizing the Lower Bound,[0],[0]
Using these gradients we can use L-BFGS to maximize the lower bound and implement the EM iteration.,5.1.1 Maximizing the Lower Bound,[0],[0]
So far we have discussed maximization only for the likelihood without accounting for the prior probabilities for every parameter.,5.2 Handling the Parameter Prior,[0],[0]
"To obtain our full training objective we add the log of the right hand side of (3):
log N (π|0, σ(π)) · K∏",5.2 Handling the Parameter Prior,[0],[0]
g=1 N,5.2 Handling the Parameter Prior,[0],[0]
"( ρ(g)|0, σ(ρ) )",5.2 Handling the Parameter Prior,[0],[0]
"i.e., the parameter prior, to (4) and (5).",5.2 Handling the Parameter Prior,[0],[0]
The gradient contribution from these priors can be computed with standard techniques.,5.2 Handling the Parameter Prior,[0],[0]
"We can now implement an EM loop, which maximizes (3) as follows: we randomly pick an initial value θ(0) for all parameters.",5.3 Training Iteration,[0],[0]
Then we repeatedly compute the P ( g|D(u); θ(i−1) ) values and maximize the lower bound using L-BFGS to find θ(i).,5.3 Training Iteration,[0],[0]
This EM iteration is guaranteed to eventually converge towards a local optimum of our objective function.,5.3 Training Iteration,[0],[0]
"Once change in the objective falls below a pre-defined threshold, we keep the final θ setting.
",5.3 Training Iteration,[0],[0]
"For our implementation we make a small improvement to the approach: L-BFGS is itself an iterative algorithm and instead of running it until convergence every time we need to find a new θ(i), we only let it take a few steps.",5.3 Training Iteration,[0],[0]
"Even if we just took a single L-BFGS step in each iteration, we would still obtain a correct algorithm (Neal and
Hinton, 1999) and this has the advantage that we do not spend time trying to find a θ(i) which is a good fit for the likely poor group assignments P ( g|D(u); θ(i−1) )",5.3 Training Iteration,[0],[0]
we obtain from early parameter estimates.,5.3 Training Iteration,[0],[0]
Our model can be used in any component of a dialog system for which a prediction of the user’s behavior is needed.,6 Evaluation,[0],[0]
"In this work, we evaluate it in two NLG-related prediction tasks: RE production and RE comprehension.",6 Evaluation,[0],[0]
In both cases we evaluate the ability of our model to predict the user’s behavior given a stimulus.,6 Evaluation,[0],[0]
"We expect our user-group model to gradually improve its prediction accuracy compared to a generic baseline without user groups as it sees more observations from a given user.
",6 Evaluation,[0],[0]
In all experiments described below we set the prior variances σγ = 1.0 and σπ = 0.3 after trying out values between 0.1 and 10 on the training data of the comprehension experiment.,6 Evaluation,[0],[0]
"Task The task of RE generation can be split in two steps: attribute selection, the selection of the visual attributes to be used in the RE such as color, size, relation to other objects and surface realization, the generation of a full natural language expression.",6.1 RE production,[0],[0]
"We focus here on attribute selection: given a visual scene and a target object, we want to predict the set of attributes of the target object that a human speaker would use in order to describe it.",6.1 RE production,[0],[0]
"Here we treat attribute selection in terms of individual classification decisions on whether to use each attribute, as described in Section 3.",6.1 RE production,[0],[0]
"More specifically, we focus on predicting whether the speaker will use a spatial relation to another object (“landmark”).",6.1 RE production,[0],[0]
"Our motivation for choosing this attribute stems from the fact that previous authors (Viethen and Dale, 2008; Ferreira and Paraboni, 2014) have found substantial variation between different users with respect to their preference towards using spatial relations.
",6.1 RE production,[0],[0]
"Data We use the GRE3D3 dataset of humanproduced REs (Viethen and Dale, 2010), which contains 630 descriptions for 10 scenes collected from 63 users, each describing the same target object in each scene.",6.1 RE production,[0],[0]
35% of the descriptions in this corpus use a spatial relation.,6.1 RE production,[0],[0]
"An example of such a scene can be seen in Fig. 3.
",6.1 RE production,[0],[0]
"Models We use two baselines for comparison:
Basic: The state-of-the-art model on this task with this dataset, under the assumption that users are seen in training, is presented in Ferreira and Paraboni (2014).",6.1 RE production,[0],[0]
"They define context features such as type of relation between the target object and its landmark, number of object of the same color or size, etc., then train an SVM classifier to predict the use of each attribute.",6.1 RE production,[0],[0]
"We recast their model in terms of a log-linear model with the same features, to make it fit with the setup of Section 3.
Basic++: Ferreira and Paraboni (2014) also take speaker features into account.",6.1 RE production,[0],[0]
"We do not use speaker identity and the speaker’s attribute frequency vector, because we only evaluate on unseen users.",6.1 RE production,[0],[0]
"We do use their other speaker features (age, gender), together with Basic’s context features; this gives us a strong baseline which is aware of manually annotated user group characteristics.
",6.1 RE production,[0],[0]
"We compare these baselines to our Group model for values of K between 1 and 10, using the exact same features as Basic.",6.1 RE production,[0],[0]
"We do not use the speaker features of Basic++, because we do not want to rely on manually annotated groups.",6.1 RE production,[0],[0]
"Note that our results are not directly comparable with those of Ferreira and Paraboni (2014), because of a different training-test split: their model requires having seen speakers in training, while we explicitly want to test our model’s ability to generalize to unseen users.
",6.1 RE production,[0],[0]
"Experimental setup We evaluate using crossvalidation, splitting the folds so that all speakers we see in testing are previously unseen in training.",6.1 RE production,[0],[0]
We use 9 folds in order to have folds of the same size (each containing 70 descriptions coming from 7 speakers).,6.1 RE production,[0],[0]
At each iteration we train on 8 folds and test on the 9th.,6.1 RE production,[0],[0]
"At test time, we process each test instance iteratively: we first predict for each instance whether the user uwould use a spatial relation or not and test our prediction; we then add the
actual observation from the corpus to the set D(u) of observations for this particular user, in order to update our estimate about their group membership.
",6.1 RE production,[0],[0]
"Results Figure 4 shows the test F1-score (microaveraged over all folds) as we increase the number of groups, compared to the baselines.",6.1 RE production,[0],[0]
"For our Group models, these are averaged over all interactions with the user.",6.1 RE production,[0],[0]
"Our model gets F1-scores between 0.69 and 0.76 for all values ofK > 1, outperforming both Basic (0.22) and Basic++ (0.23).
",6.1 RE production,[0],[0]
"In order to take a closer look at our model’s behavior, we also show the accuracy of our model as it observes more instances at test time.",6.1 RE production,[0],[0]
We compare the model with K = 3 groups against the two baselines.,6.1 RE production,[0],[0]
"Figure 5 shows that the group model’s F1-score increases dramatically after the first two observations and then stays high throughout the test phase, always outperforming both baselines by at least 0.37 F1-score points after the first observation.",6.1 RE production,[0],[0]
The baseline models of course are not expected to improve with time; fluctuations are due to differences between the visual scenes.,6.1 RE production,[0],[0]
"In the same figure, we plot the evolution of the entropy of the group model’s posterior distribution over the groups (see (7)).",6.1 RE production,[0],[0]
"As expected, the model is highly uncertain at the beginning of the test phase about which group the user belongs to, then gets more and more certain as the set D(u) of observations from that user grows.",6.1 RE production,[0],[0]
Task Our next task is to predict the referent to which a user will resolve an RE in the context of a visual scene.,6.2 RE comprehension,[0],[0]
"Our model is given a stimulus s = (r, c) consisting of an instruction containing an RE r and a visual context c and outputs a probability distribution over all possible referents b.",6.2 RE comprehension,[0],[0]
"Such a model can be used by a probabilistic RE generator to select an RE which is highly likely to be correctly understood by the user or predict potential
misunderstandings (see Section 3).
",6.2 RE comprehension,[0],[0]
Data We use the GIVE-2.5 corpus for training and the GIVE-2 corpus for testing our model (the same used by Engonopoulos et al. (2013)).,6.2 RE comprehension,[0],[0]
These contain recorded observations of dialog systems giving instructions to users who play a game in a 3D environment.,6.2 RE comprehension,[0],[0]
"Each instruction contains an RE r, which is recorded in the data together with the visual context c at the time the instruction was given.",6.2 RE comprehension,[0],[0]
The object b which the user understood as the referent of the RE is inferred by the immediately subsequent action of the user.,6.2 RE comprehension,[0],[0]
"In total, we extracted 2927 observations by 403 users from GIVE-25 and 5074 observations by 563 users from GIVE-2.
",6.2 RE comprehension,[0],[0]
Experimental setup We follow the training method described in Section 3.,6.2 RE comprehension,[0],[0]
"At test time, we present the observations from each user in the order they occur in the test data; for each stimulus, we ask our models to predict the referent a which the user understood to be the referent of the RE, and compare with the recorded observation.",6.2 RE comprehension,[0],[0]
"We subsequently add the recorded observation to the dataset for the user and continue.
",6.2 RE comprehension,[0],[0]
"Models As a baseline, we use the Basic model described in Section 3, with the features of the “semantic” model of Engonopoulos et al. (2013).",6.2 RE comprehension,[0],[0]
"Those features capture information about the objects in the visual scene (e.g. salience) and some basic semantic properties of the RE (e.g. color, position).",6.2 RE comprehension,[0],[0]
"We use those features for our Group model as well, and evaluate for K between 1 and 10.
Results on GIVE data Basic had a test accuracy of 72.70%, which was almost identical with the accuracy of our best Group model for K = 6 (72.78%).",6.2 RE comprehension,[0],[0]
This indicates that our group model does not differentiate between users.,6.2 RE comprehension,[0],[0]
"Indeed, after training, the 6-group model assigns 81% prior probabil-
ity to one of the groups, and effectively gets stuck with this assignment while testing; the mean entropy of the posterior group distribution only falls from an initial 1.1 to 0.7 after 10 observations.
",6.2 RE comprehension,[0],[0]
We speculate that the reason behind this is that the features we use are not sensitive enough to capture the differences between the users in this data.,6.2 RE comprehension,[0],[0]
"Since our model relies completely on observable behavior, it also relies on the ability of the features to make relevant distinctions between users.
",6.2 RE comprehension,[0],[0]
"Results on synthetic data In order to test this hypothesis, we made a synthetic dataset based on the GIVE datasets with 1000 instances from 100 users, in the following way: for each user, we randomly selected 10 scenes from GIVE-2, and replaced the target the user selected, so that half of the users always select the target with the highest visual salience, and the other half always select the one with the lowest.",6.2 RE comprehension,[0],[0]
"Our aim was to test whether our model is capable of identifying groups when they are clearly present in the data and exhibit differences which our features are able to capture.
",6.2 RE comprehension,[0],[0]
We evaluated the same models in a 2-fold crossvalidation.,6.2 RE comprehension,[0],[0]
Figure 6 shows the prediction accuracy for Basic and the Group models for K from 1 to 10.,6.2 RE comprehension,[0],[0]
"All models for K > 1 clearly outperform the baseline model: the 2-group model gets 62.3% vs 28.6% averaged over all test examples, while adding more than two groups does not further improve the accuracy.",6.2 RE comprehension,[0],[0]
We also show in Figure 7 the evolution of the accuracy asD(u) grows: the Group model with K = 2 reaches a 64% testing accuracy after seeing two observations from the same user.,6.2 RE comprehension,[0],[0]
"In the same figure, the entropy of the posterior distribution over groups (see production experiment) falls towards zero as D(u) grows.",6.2 RE comprehension,[0],[0]
"These results show that our model is capable of correctly assigning a user to the group they belong to, once the features are adequate for distinguishing between different user behaviors.",6.2 RE comprehension,[0],[0]
"Our model was shown to be successful in discovering groups of users with respect to their behavior, within datasets which present discernible user variation.",6.3 Discussion,[0],[0]
"In particular, if all listeners are influenced in a similar way by e.g. the visual salience of an object, then the group model cannot learn different weights for the visual salience feature; if this happens for all available features, there are effectively no groups for our model to discover.
",6.3 Discussion,[0],[0]
"Once the groups have been discovered, our model can then very quickly distinguish between them at test time.",6.3 Discussion,[0],[0]
This is reflected in the steep performance improvement even after the first user observation in both the real data experiment in 6.1 and the synthetic data experiment in 6.2.,6.3 Discussion,[0],[0]
"We have presented a probabilistic model for NLG which predicts the behavior of individual users of a dialog system by dynamically assigning them to user groups, which were discovered during training2.",7 Conclusion,[0],[0]
"We showed for two separate NLG-related tasks, RE production and RE comprehension, how our model, after being trained with data that is not annotated with user groups, can quickly adapt to unseen users as it gets more observations from them in the course of a dialog and makes increasingly accurate predictions about their behavior.
",7 Conclusion,[0],[0]
"Although in this work we apply our model to tasks related to NLG, nothing hinges on this choice; it can also be applied to any other dialog-related prediction task where user variation plays a role.",7 Conclusion,[0],[0]
"In the future, we will also try to apply the basic principles of our user group approach to more sophisticated underlying models, such as neural networks.
2Our code and data is available in https://bit.ly/ 2jIu1Vm",7 Conclusion,[0],[0]
We present a model which predicts how individual users of a dialog system understand and produce utterances based on user groups.,abstractText,[0],[0]
"In contrast to previous work, these user groups are not specified beforehand, but learned in training.",abstractText,[0],[0]
"We evaluate on two referring expression (RE) generation tasks; our experiments show that our model can identify user groups and learn how to most effectively talk to them, and can dynamically assign unseen users to the correct groups as they interact with the system.",abstractText,[0],[0]
Discovering User Groups for Natural Language Generation,title,[0],[0]
"As originally defined by Pearl (1988), Bayesian networks express joint distributions over finite sets of random variables as products of conditional distributions.",1. Introduction,[0],[0]
"Probabilistic programming languages (PPLs) (Koller et al., 1997; Milch et al., 2005a; Goodman et al., 2008; Wood et al., 2014b) apply the same idea to potentially infinite sets of variables with general dependency structures.",1. Introduction,[0],[0]
"Thanks to their expressive power, PPLs have been used to solve many real-world applications, including Captcha (Le et al., 2017), seismic monitoring (Arora et al., 2013), 3D pose estimation (Kulkarni et al., 2015), generating design suggestions (Ritchie et al., 2015), concept learning (Lake et al., 2015), and cognitive science applications (Stuhlmüller & Goodman, 2014).
",1. Introduction,[0],[0]
"In practical applications, we often have to deal with a mix-
1University of California, Berkeley 2Arizona State University 3Vicarious Inc. 4Carnegie Mellon University.",1. Introduction,[0],[0]
"Correspondence to: Yi Wu <jxwuyi@gmail.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
ture of continuous and discrete random variables.",1. Introduction,[0],[0]
"Existing PPLs support both discrete and continuous random variables, but not discrete-continuous mixtures, i.e., variables whose distributions combine discrete and continuous elements.",1. Introduction,[0],[0]
"Such variables are fairly common in practical applications: sensors that have thresholded limits, e.g. thermometers, weighing scales, speedometers, pressure gauges; or a hybrid sensor that can report a either real value or an error condition.",1. Introduction,[0],[0]
"The occurrence of such variables has been noted in many other applications from a wide range of scientific domains (Kharchenko et al., 2014; Pierson & Yau, 2015; Gao et al., 2017).
",1. Introduction,[0],[0]
"Many PPLs have a restricted syntax that forces the expressed random variables to be either discrete or continuous, including WebPPL (Goodman & Stuhlmüller, 2014), Edward (Tran et al., 2016), Figaro (Pfeffer, 2009) and Stan (Carpenter et al., 2016).",1. Introduction,[0],[0]
"Even for PPLs whose syntax allows for mixtures of discrete and continuous variables, such as BLOG (Milch et al., 2005a), Church (Goodman, 2013), Venture (Mansinghka et al., 2014) and Anglican (Wood et al., 2014a), the underlying semantics of these PPLs implicitly assumes the random variables are not mixtures.",1. Introduction,[0],[0]
"Moreover, the inference algorithms associated with the semantics inherit the same assumption and can produce incorrect results when discrete-continuous mixtures are used.
",1. Introduction,[0],[0]
"Consider the following GPA example: a two-variable Bayes net Nationality → GPA where the nationality follows a binary distribution
P (Nationality = USA) = P (Nationality = India)",1. Introduction,[0],[0]
"= 0.5
and the conditional probabilities are discrete-continuous mixtures
GPA|Nationality = USA ∼0.01 · 1 {GPA = 4}+ 0.99 · Unif(0, 4),
GPA|Nationality = India ∼0.01 · 1 {GPA = 10}+ 0.99 · Unif(0, 10).
",1. Introduction,[0],[0]
This is a typical scenario in practice because many top students have perfect GPAs.,1. Introduction,[0],[0]
Now suppose we observe a student with a GPA of 4.0.,1. Introduction,[0],[0]
Where do they come from?,1. Introduction,[0],[0]
"If the student is Indian, the probability of any singleton set {g}
where 0 <",1. Introduction,[0],[0]
"g < 10 is zero, as this range has a probability density.",1. Introduction,[0],[0]
"On the other hand if the student is American, the set {4} has the probability 0.01.",1. Introduction,[0],[0]
"Thus, by Bayes theorem, P (Nationality = USA|GPA = 4) = 1, which means the student must be from the USA.
",1. Introduction,[0],[0]
"However, if we run the default Bayesian inference algorithm for this problem in PPLs, e.g., the standard importance sampling algorithm (Milch et al., 2005b), a sample that picks India receives a density weight of 0.99/10.0 = 0.099, whereas one that picks USA receives a discrete-mass weight of 0.01.",1. Introduction,[0],[0]
"Since the algorithm does not distinguish probability density and mass, it will conclude that the student is very probably from India, which is far from the truth.
",1. Introduction,[0],[0]
"We can fix the GPA example by considering a density weight infinitely smaller than a discrete-mass weight (Nitti et al., 2016; Tolpin et al., 2016).",1. Introduction,[0],[0]
"However, the situation becomes more complicated when involving more than one evidence variable, e.g., GPAs over multiple semesters for students who may study in both countries.",1. Introduction,[0],[0]
Vector-valued variables also cause problems—does a point mass in three dimensions count more or less than a point mass in two dimensions?,1. Introduction,[0],[0]
"These practical issues motivate the following two tasks:
• Inherit all the existing properties of PPL semantics and extend it to handle random variables with mixed discrete and continuous distributions;
• Design provably correct inference algorithms for the extended semantics.
",1. Introduction,[0],[0]
"In this paper, we carry out all these two tasks and implement the extended semantics as well as the new algorithms in a widely used PPL, Bayesian Logic (BLOG) (Milch et al., 2005a).",1. Introduction,[0],[0]
Measure-Theoretical Bayesian Nets (MTBNs) Measure theory can be applied to handle discrete-continuous mixtures or even more abstract measures.,1.1. Main Contributions,[0],[0]
"In this paper, we define a generalization of Bayesian networks called measure-theoretic Bayesian networks (MTBNs) and prove that every MTBN represents a unique measure on the input space.",1.1. Main Contributions,[0],[0]
"We then show how MTBNs can provide a more general semantic foundation for PPLs.
More concretely, MTBNs support (1) random variables with infinitely (even uncountably) many parents, (2) random variables valued in arbitrary measure spaces (with RN as one case) distributed according to any measure (including discrete, continuous and mixed), (3) establishment of conditional independencies implied by an infinite graph, and (4) open-universe semantics in terms of the possible worlds in the vocabulary of the model.
",1.1. Main Contributions,[0],[0]
Inference Algorithms,1.1. Main Contributions,[0],[0]
"We propose a provably correct inference algorithm, lexicographic likelihood weighting (LLW), for general MTBNs with discrete-continuous mixtures.",1.1. Main Contributions,[0],[0]
"In addition, we propose LPF, a particle-filtering variant of LLW for sequential Monte Carlo (SMC) inference on state-space models.
",1.1. Main Contributions,[0],[0]
"Incorporating MTBNs into an existing PPL We incorporate MTBNs into BLOG with simple modifications and then define the generalized BLOG language, measuretheoretic BLOG, which formally supports arbitrary distributions, including discrete-continuous mixtures.",1.1. Main Contributions,[0],[0]
We prove that every generalized BLOG model corresponds to a unique MTBN.,1.1. Main Contributions,[0],[0]
"Thus, all the desired theoretical properties of MTBNs can be carried to measure-theoretic BLOG.",1.1. Main Contributions,[0],[0]
We also implement the LLW and LPF algorithms in the backend of measure-theoretic BLOG and use three representative examples to show their effectiveness.,1.1. Main Contributions,[0],[0]
This paper is organized as follows.,1.2. Organization,[0],[0]
We first discuss related work in Section 2.,1.2. Organization,[0],[0]
"In Section 3, we formally define measure-theoretic Bayesian nets and study their theoretical properties.",1.2. Organization,[0],[0]
Section 4 describes the LLW and LPF inference algorithms for MTBNs with discrete-continuous mixtures and establishes their correctness.,1.2. Organization,[0],[0]
"In Section 5, we introduce the measure-theoretic extension of BLOG and study its theoretical foundations for defining probabilistic models.",1.2. Organization,[0],[0]
"In Section 6, we empirically validate the generalized BLOG system and the new inference algorithms on three representative examples.",1.2. Organization,[0],[0]
"The motivating GPA example has been also discussed as a special case under some other PPL systems (Tolpin et al., 2016; Nitti et al., 2016).",2. Related Work,[0],[0]
Tolpin et al. (2016) and Nitti et al. (2016) proposed different solutions specific to this example but did not address the general problems of representation and inference with random variables with mixtures of discrete and continuous distributions.,2. Related Work,[0],[0]
"In contrast, we present a general formulation with provably correct inference algorithms.
",2. Related Work,[0],[0]
"Our approach builds upon the foundations of the BLOG probabilistic programming language (Milch, 2006).",2. Related Work,[0],[0]
We use a measure theoretic formulation to generalize the syntax and semantics of BLOG to random variables that may have infinitely many parents and mixed continuous and discrete distributions.,2. Related Work,[0],[0]
"The BLP framework Kersting & De Raedt (2007) unifies logic programming with probability models, but requires each random variable to be influenced by a finite set of random variables in order to define the semantics.",2. Related Work,[0],[0]
"This amounts to requiring only finitely many ances-
tors of each random variable.",2. Related Work,[0],[0]
Choi et al. (2010) present an algorithm for carrying out lifted inference over models with purely continuous random variables.,2. Related Work,[0],[0]
"They also require parfactors to be functions over finitely many random variables, thus limiting the set of influencing variables for each node to be finite.",2. Related Work,[0],[0]
Gutmann et al. (2011a) also define densities over finite dimensional vectors.,2. Related Work,[0],[0]
"In a relatively more general formulation (Gutmann et al., 2011b) define the distribution of each random variable using a definite clause, which corresponds to the limitation that each random variable (either discrete or continuous) has finitely many parents.",2. Related Work,[0],[0]
Frameworks building on Markov networks also have similar restrictions.,2. Related Work,[0],[0]
"Wang & Domingos (2008) only consider networks of finitely many random variables, which can have either discrete or continuous distributions.",2. Related Work,[0],[0]
"Singla & Domingos (2007) extend Markov logic to infinite (non-hybrid) domains, provided that each random variable has only finitely many influencing random variables.
",2. Related Work,[0],[0]
"In contrast, our approach not only allows models with arbitrarily many random variables with mixed discrete and continuous distributions, but each random variable can also have arbitrarily many parents as long as all ancestor chains are finite (but unbounded).",2. Related Work,[0],[0]
"The presented work constitutes a rigorous framework for expressing probability models with the broadest range of cardinalities (uncountably infinite parent sets) and nature of random variables (discrete, mixed, and even arbitrary measure spaces), with clear semantics in terms of first-order possible worlds and the generalization of conditional independences on such models.
",2. Related Work,[0],[0]
"Lastly, there are also other works using measure-theoretic approaches to analyze the semantics properties of probabilistic programs but with different emphases, such as the commutativity (Staton, 2017), design choices for monad structures (Ramsey, 2016) and computing a disintegration (Shan & Ramsey, 2017).",2. Related Work,[0],[0]
"In this section, we introduce measure-theoretic Bayesian networks (MTBNs) and prove that an MTBN represents a unique measure with desired theoretical properties.",3. Measure-Theoretic Bayesian Networks,[0],[0]
We assume familiarity with measure-theoretic approaches to probability theory.,3. Measure-Theoretic Bayesian Networks,[0],[0]
Some background is included in Appx.,3. Measure-Theoretic Bayesian Networks,[0],[0]
"A.
We begin with some necessary definitions of graph theory.
",3. Measure-Theoretic Bayesian Networks,[0],[0]
Definition 3.1.,3. Measure-Theoretic Bayesian Networks,[0],[0]
"A digraph G is a pair G = (V,E) of a set of vertices V , of any cardinality, and a set of directed edges E ⊆ V × V .",3. Measure-Theoretic Bayesian Networks,[0],[0]
"The notation u→ v denotes (u, v) ∈ E, and u 7→ v denotes the existence of a path from u to v in G.
Definition 3.2.",3. Measure-Theoretic Bayesian Networks,[0],[0]
"A vertex v ∈ V is a root vertex if there are no incoming edges to it, i.e., there is no u ∈ V such that u → v. Let pa(v)",3. Measure-Theoretic Bayesian Networks,[0],[0]
"= {u ∈ V : u → v} denote the set of parents of a vertex v ∈ V , and nd(v) =",3. Measure-Theoretic Bayesian Networks,[0],[0]
"{u ∈ V : not v 7→
u} denote its set of non-descendants.",3. Measure-Theoretic Bayesian Networks,[0],[0]
Definition 3.3.,3. Measure-Theoretic Bayesian Networks,[0],[0]
"A well-founded digraph (V,E) is one with no countably infinite ancestor chain v0 ← v1 ← v2 ← . . .",3. Measure-Theoretic Bayesian Networks,[0],[0]
".
",3. Measure-Theoretic Bayesian Networks,[0],[0]
This is the natural generalization of a finite directed acyclic graph to the infinite case.,3. Measure-Theoretic Bayesian Networks,[0],[0]
"Now we are ready to give the key definition of this paper.
",3. Measure-Theoretic Bayesian Networks,[0],[0]
Definition 3.4.,3. Measure-Theoretic Bayesian Networks,[0],[0]
"A measure-theoretic Bayesian network M = (V,E, {Xv}v∈V , {Kv}v∈V ) consists of (a) a wellfounded digraph (V,E) of any cardinality, (b) an arbitrary measurable space Xv for each v ∈ V , and (c) a probability kernel Kv from ∏ u∈pa(v) Xu to Xv for each v ∈ V .
",3. Measure-Theoretic Bayesian Networks,[0],[0]
"By definition, MTBNs allow us to define very general and abstract models with the following two major benefits:
1.",3. Measure-Theoretic Bayesian Networks,[0],[0]
"We can define random variables with infinitely (even uncountably) many parents because MTBN is defined on a well-founded digraph.
2.",3. Measure-Theoretic Bayesian Networks,[0],[0]
"We can define random variables in arbitrary measure spaces (with RN as one case) distributed according to any measure (including discrete, continuous and mixed).
",3. Measure-Theoretic Bayesian Networks,[0],[0]
"Next, we related MTBN to a probability measure.",3. Measure-Theoretic Bayesian Networks,[0],[0]
"Fix an MTBN M = (V,E, {Xv}v∈V , {Kv}v∈V ).",3. Measure-Theoretic Bayesian Networks,[0],[0]
For U ⊆ V let XU = ∏ u∈U Xu be the product measurable space over variables u ∈ U .,3. Measure-Theoretic Bayesian Networks,[0],[0]
"With this notation, Kv is a kernel from Xpa(v) to Xv.",3. Measure-Theoretic Bayesian Networks,[0],[0]
Whenever W ⊆ U let πUW : XU → XW denote the projection map.,3. Measure-Theoretic Bayesian Networks,[0],[0]
Let XV be our base measurable space upon which we will consider different probability measures µ.,3. Measure-Theoretic Bayesian Networks,[0],[0]
"Let Xv for v ∈ V denote both the underlying set of Xv and the random variable given by the projection πV{v}, and XU for U ⊆ V the underlying space of XU and the random variable given by the projection πVU .
",3. Measure-Theoretic Bayesian Networks,[0],[0]
Definition 3.5.,3. Measure-Theoretic Bayesian Networks,[0],[0]
"An MTBN M represents a measure µ on XV , if for all v ∈ V :
• Xv is conditionally independent of its non-descendants Xnd(v) given its parents Xpa(v).",3. Measure-Theoretic Bayesian Networks,[0],[0]
"• Kv(Xpa(v), A) = Pµ[Xv ∈ A|Xpa(v)] holds almost surely for any A ∈ Xv, i.e., Kv is a version of the conditional distribution of Xv given its parents.
",3. Measure-Theoretic Bayesian Networks,[0],[0]
Def. 3.5 captures the generalization of the local properties of Bayes networks – conditional independence and conditional distributions defined by parent-child relationships.,3. Measure-Theoretic Bayesian Networks,[0],[0]
Here we assume the conditional probability exists and is unique.,3. Measure-Theoretic Bayesian Networks,[0],[0]
"This is a mild condition because this holds as long as the probability space is regular (Kallenberg, 2002).
",3. Measure-Theoretic Bayesian Networks,[0],[0]
"The next theorem shows that MTBNs are well-defined.
",3. Measure-Theoretic Bayesian Networks,[0],[0]
Theorem 3.6.,3. Measure-Theoretic Bayesian Networks,[0],[0]
"An MTBN M represents a unique measure µ on XV .
",3. Measure-Theoretic Bayesian Networks,[0],[0]
The proof of theorem 3.6 requires several intermediate results and is presented in Appx.,3. Measure-Theoretic Bayesian Networks,[0],[0]
B.,3. Measure-Theoretic Bayesian Networks,[0],[0]
The proof proceeds by first defining a projective family of measures.,3. Measure-Theoretic Bayesian Networks,[0],[0]
This gives a way to recursively construct our measure µ.,3. Measure-Theoretic Bayesian Networks,[0],[0]
We then define a notion of consistency such that every consistent projective family constructs a measure that M represents.,3. Measure-Theoretic Bayesian Networks,[0],[0]
"Lastly, we give an explicit characterization of the unique consistent projective family, and thus of the unique measure M represents.",3. Measure-Theoretic Bayesian Networks,[0],[0]
We introduce the lexicographic likelihood weighting (LLW) algorithm for provably correct inference on MTBNs.,4. Generalized Inference Algorithms,[0],[0]
We also present lexicographic particle filter (LPF) for statespace models by adapting LLW for the sequential Monte Carlo (SMC) framework.,4. Generalized Inference Algorithms,[0],[0]
"Suppose we have an MTBN with finitely many random variables X1, . . .",4.1. Lexicographic likelihood weighting,[0],[0]
", XN , and that, without loss of generality, we observe real-valued random variables X1, . . .",4.1. Lexicographic likelihood weighting,[0],[0]
", XM for M < N as evidence.",4.1. Lexicographic likelihood weighting,[0],[0]
"Suppose the distribution of Xi given its parents Xpa(i) is a mixture between a density fi(xi|xpa(i)) with respect to the Lebesgue measure and a discrete distribution Fi(xi|xpa(i)), i.e., for any > 0, we have P (Xi ∈",4.1. Lexicographic likelihood weighting,[0],[0]
"[xi − , xi]|Xpa(i))",4.1. Lexicographic likelihood weighting,[0],[0]
"=∑ x∈[xi− ,xi] Fi(xi|Xpa(i))",4.1. Lexicographic likelihood weighting,[0],[0]
+ ∫,4.1. Lexicographic likelihood weighting,[0],[0]
xi xi− fi(x|Xpa(i)),4.1. Lexicographic likelihood weighting,[0],[0]
dx.,4.1. Lexicographic likelihood weighting,[0],[0]
This implies that Fi(xi|xpa(i)) is nonzero for at most countably many values,4.1. Lexicographic likelihood weighting,[0],[0]
xi.,4.1. Lexicographic likelihood weighting,[0],[0]
"If Fi is nonzero for finitely many points, it can be represented by a list of those points and their values.
",4.1. Lexicographic likelihood weighting,[0],[0]
"Lexicographic Likelihood Weighting (LLW) extends the classical likelihood weighting (Milch et al., 2005b) to this setting.",4.1. Lexicographic likelihood weighting,[0],[0]
"It visits each node of the graph in topological order, sampling those variables that are not observed, and accumulating a weight for those that are observed.",4.1. Lexicographic likelihood weighting,[0],[0]
"In particular, at an evidence variable Xi we update a tuple (d,w) of the number of densities and a weight, initially (0, 1), by:
(d,w)← { (d,wFi(xi|xpa(i))) Fi(xi|xpa(i))",4.1. Lexicographic likelihood weighting,[0],[0]
"> 0, (d+ 1, wfi(xi|xpa(i)))",4.1. Lexicographic likelihood weighting,[0],[0]
"otherwise.
(1)
Finally, having K samples x(1), . .",4.1. Lexicographic likelihood weighting,[0],[0]
.,4.1. Lexicographic likelihood weighting,[0],[0]
", x(K) by this process and accordingly a tuple (d(i), w(i)) for each sample x(i), let d∗ = mini:w(i) 6=0 d
(i) and estimate E[f(X)|X1:M ] by∑ {i:d(i)=d∗} w
(i) f(x(i))∑",4.1. Lexicographic likelihood weighting,[0],[0]
{i:d(i)=d∗} w (i) .,4.1. Lexicographic likelihood weighting,[0],[0]
"(2)
The algorithm is summarised in Alg. 1",4.1. Lexicographic likelihood weighting,[0],[0]
The next theorem shows this procedure is consistent.,4.1. Lexicographic likelihood weighting,[0],[0]
Theorem 4.1.,4.1. Lexicographic likelihood weighting,[0],[0]
"LLW is consistent: (2) converges almost surely to E[f(X)|X1:M ].
",4.1. Lexicographic likelihood weighting,[0],[0]
"Algorithm 1 Lexicographic Likelihood Weighting Require: densities f , masses F , evidences E, and K.
for i = 1 . .",4.1. Lexicographic likelihood weighting,[0],[0]
.K,4.1. Lexicographic likelihood weighting,[0],[0]
"do sample all the ancestors of E from prior compute (d(i), w(i)) by Eq.",4.1. Lexicographic likelihood weighting,[0],[0]
(1) end for d?,4.1. Lexicographic likelihood weighting,[0],[0]
"← mini:w(i) 6=0 d(i)
",4.1. Lexicographic likelihood weighting,[0],[0]
"Return (∑
i:d(i)=d? w (i)f(x(i))
) /",4.1. Lexicographic likelihood weighting,[0],[0]
(∑ i:d(i)=d?,4.1. Lexicographic likelihood weighting,[0],[0]
"w (i) )
",4.1. Lexicographic likelihood weighting,[0],[0]
"In order to prove Theorem 4.1, the main technique we adopt is to use a more restricted algorithm, the Iterative Refinement Likelihood Weighting (IRLW) as a reference.",4.1. Lexicographic likelihood weighting,[0],[0]
"Suppose we want to approximate the posterior distribution of an X -valued random variable X conditional on a Yvalued random variable Y , for arbitrary measure spaces X and Y .",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"In general, there is no notion of a probability density of Y given X for weighing samples.",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"If, however, we could make a discrete approximation Yt of Y then we could weight samples by the probability P",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
[Yt = yt|X].,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"If we increase the accuracy of the approximation with the number of samples, this should converge in the limit.",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"We show this is possible, if we are careful about how we approximate:
Definition 4.2.",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
An approximation scheme for a measurable space Y consists of a measurable spaceA and measurable approximation functions αi :,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"Y → A for i = 1, 2, . . .",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
and αji :,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
A → A for i < j such that αj ◦ α,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
j,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"i = αi and y can be measurably recovered from the subsequence αt(y), αt+1(y), . . .",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"for any t > 0.
",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"When Y is a real-valued variable we will use the approximation scheme αn(y) = 2−nd2nye where dre denotes the ceiling of r, i.e., the smallest integer no smaller than it.",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
Observe in this case that P (αn(Y ) = αn(y)),4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
= P (αn(y)− 2−n < Y ≤ αn(y)) which we can compute from the CDF of Y .,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
Lemma 4.3.,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"IfX,Y are real-valued random variables with E |X| <∞, then limi→∞ E[X|αi(Y )]",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"= E[X|Y ].
",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
Proof.,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
Let Fi = σ(αi(Y )),4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
be the sigma algebra generated by αi(Y ).,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
Whenever i ≤,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
j,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
we have αi(Y ),4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
= (αj ◦αji )(Y ) and so Fi ⊆ Fj .,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"This means E[X|αi(Y )] = E[X|Fi] is a martingale, so we can use martingale convergence results.",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"In particular, since E |X| <∞
E[X|Fi]→ E[X|F∞] a.s. and in L1, where F∞ =",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
⋃,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
i Fi is the sigma-algebra generated by {αi(Y ),4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
": i ∈ N} (see Theorem 7.23 in (Kallenberg, 2002)).
",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"Y is a measurable function of the sequence (α1(Y ), . . .",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"),",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
as limi→∞ αi(Y ),4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"= Y , and so σ(Y ) ⊆ F∞. By definition
the sequence is a measurable function of Y , and so F∞ ⊆ σ(Y ), and so E[X|F∞] = E[X|Y ] giving our result.
",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"Iterative refinement likelihood weighting (IRLW) samples x(1), . . .",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
", x(K)",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
from the prior,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"and evaluates:
∑K i=1",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"P (αn(Y )|X = x(i))f(x(i))∑K
i=1",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"P (αn(Y )|X = x(i)) (3)
Using Lemma 4.3, G.12, and G.13, we can show IRLW is consistent.
",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
Theorem 4.4.,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
IRLW is consistent: (3) converges almost surely to E[f(X)|Y ].,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"Now we are ready to prove Theorem 4.1.
",4.1.2. PROOF OF THEOREM 4.1,[0],[0]
Proof of Theorem 4.1.,4.1.2. PROOF OF THEOREM 4.1,[0],[0]
We prove the theorem for evidence variables that are leaves It is straightforward to extend the proof when the evidence variables are non-leaf nodes.,4.1.2. PROOF OF THEOREM 4.1,[0],[0]
"Let x be a sample produced by the algorithm with number of densities and weight (d,w).",4.1.2. PROOF OF THEOREM 4.1,[0],[0]
With In = ∏ i=1...,4.1.2. PROOF OF THEOREM 4.1,[0],[0]
"M (αn(xi)− 2−n, αn(xi)] a 2−n-cube around x1:M we have
lim n→∞ P (X1:M ∈ In|XM+1:N = xM+1:N )",4.1.2. PROOF OF THEOREM 4.1,[0],[0]
"w 2−dn = 1.
",4.1.2. PROOF OF THEOREM 4.1,[0],[0]
"Using In as an approximation scheme by Def. 4.2, the numerator in the above limit is the weight used by IRLW.",4.1.2. PROOF OF THEOREM 4.1,[0],[0]
"But given the above limit, using w 2−dn as the weight will give the same result in the limit.",4.1.2. PROOF OF THEOREM 4.1,[0],[0]
"Then if we have K samples, in the limit of n→∞ only those samples x(i) with minimal d(i) will contribute to the estimation, and up to normalization they will contribute weight w(i) to the estimation.",4.1.2. PROOF OF THEOREM 4.1,[0],[0]
"We now consider inference in a special class of highdimensional models known as state-space models, and show how LLW can be adapted to avoid the curse of dimensionality when used with such models.",4.2. Lexicographic particle filter,[0],[0]
A state-space model (SSM) consists of latent states {Xt}0≤t≤T and the observations {Yt}0≤t≤T with a special dependency structure where pa(Yt) =,4.2. Lexicographic particle filter,[0],[0]
"Xt and pa(Xt) = Xt−1 for 0 < t ≤ T .
SMC methods (Doucet et al., 2001), also knowns as particle filters, are a widely used class of methods for inference on SSMs.",4.2. Lexicographic particle filter,[0],[0]
"Given the observed variables {Yt}0≤t≤T , the posterior distribution P (Xt|Y0:t) is approximated by a set of K particles where each particle x(k)t represents a sample of {Xi}0≤i≤t.",4.2. Lexicographic particle filter,[0],[0]
"Particles are propagated forward through the transition model P (Xt|Xt−1) and resampled at each time step t according to the weight of each particle, which is defined by the likelihood of observation Yt.
",4.2. Lexicographic particle filter,[0],[0]
"Algorithm 2 Lexicographic Particle Filter (LPF) Require: densities f , masses F , evidences Y , and K
for t = 0, . . .",4.2. Lexicographic particle filter,[0],[0]
", T do for k = 0, . . .",4.2. Lexicographic particle filter,[0],[0]
",K do x
(k) t ← sample from transition
compute (d(k), w(k)) by Eq. 4 end for d?",4.2. Lexicographic particle filter,[0],[0]
← mink:w(k) 6=0 d(k) ∀k : d(k) >,4.2. Lexicographic particle filter,[0],[0]
"d?, w(k) ← 0 Output ( w(k)f(x (k) t ) ) / (∑ k w (k) ) resample particles according to w(k)
end for
In the MTBN setting, the distribution of Yt1 given its parent Xt can be a mixture of density ft(yt|xt) and a discrete distribution Ft(yt|xt).",4.2. Lexicographic particle filter,[0],[0]
"Hence, the resampling step in a particle filter should be accordingly modified: following the idea from LLW, when computing the weight of a particle, we enumerate all the observations yt,",4.2. Lexicographic particle filter,[0],[0]
"i at time step t and again update a tuple (d,w), initially (0,1), by
(d,w)← { (d,wFt(yt,i|xt)) Ft(yt,i|xt) > 0, (d+ 1, wft(yt,i|xt))",4.2. Lexicographic particle filter,[0],[0]
otherwise.,4.2. Lexicographic particle filter,[0],[0]
"(4)
We discard all those particles with a non-minimum d value and then perform the normal resampling step.",4.2. Lexicographic particle filter,[0],[0]
"We call this algorithm lexicographical particle filter (LPF), which is summarized in Alg. 2.
",4.2. Lexicographic particle filter,[0],[0]
The following theorem guarantees the correctness of LPF.,4.2. Lexicographic particle filter,[0],[0]
"Its Proof easily follows the analysis for LLW and the classical proof of particle filtering based on importance sampling.
",4.2. Lexicographic particle filter,[0],[0]
Theorem 4.5.,4.2. Lexicographic particle filter,[0],[0]
LPF is consistent: the outputs of Alg.,4.2. Lexicographic particle filter,[0],[0]
2 converges almost surely to {E[f(Xt)|Y0:t]}0≤t≤T .,4.2. Lexicographic particle filter,[0],[0]
In Section 3 and Section 4 we provided the theoretical foundation of MTBN and general inference algorithms.,5. Generalized Probabilistic Programming Languages,[0],[0]
This section describes how to incorporate MTBN into a practical PPL.,5. Generalized Probabilistic Programming Languages,[0],[0]
"We focus on a widely used open-universe PPL, BLOG (Milch, 2006).",5. Generalized Probabilistic Programming Languages,[0],[0]
"We define the generalized BLOG language, the measure-theoretic BLOG, and prove that every well-formed measure-theoretic BLOG model corresponds to a unique MTBN.",5. Generalized Probabilistic Programming Languages,[0],[0]
"Note that our approach also applies to other PPLs2.
",5. Generalized Probabilistic Programming Languages,[0],[0]
1There can be multiple variables observed.,5. Generalized Probabilistic Programming Languages,[0],[0]
"Here the notation Yt denotes {Yt,i}i for conciseness.
",5. Generalized Probabilistic Programming Languages,[0],[0]
"2It has been shown that BLOG has equivalent semantics to other PPLs (Wu et al., 2014; McAllester et al., 2008).",5. Generalized Probabilistic Programming Languages,[0],[0]
Figure 1.,16 query Nationality(David) = USA;,[0],[0]
"A BLOG code for the GPA example.
",16 query Nationality(David) = USA;,[0],[0]
"We begin with a brief description of the core syntax of BLOG, with particular emphasis on (1) number statements, which are critical for expressing open-universe models3, and (2) new syntax for expressing MTBNs, i.e., the Mix distribution.",16 query Nationality(David) = USA;,[0],[0]
Further description of BLOG’s syntax can be found in Li & Russell (2013).,16 query Nationality(David) = USA;,[0],[0]
Fig. 1 shows a BLOG model with measure-theoretic extensions for a multi-student GPA example.,5.1. Syntax of measure-theoretic BLOG,[0],[0]
"Line 1 declares two types, Applicant and Country.",5.1. Syntax of measure-theoretic BLOG,[0],[0]
"Line 2 defines 3 distinct countries with keyword distinct, New Zealand, India and USA.",5.1. Syntax of measure-theoretic BLOG,[0],[0]
"Lines 3 to 5 define a number statement, which states that the number of US applicants follows a Poisson distribution with a higher mean than those from New Zealand or India.",5.1. Syntax of measure-theoretic BLOG,[0],[0]
"Line 6 defines an origin function, which maps the object being generated to the arguments that were used in the number statement that was responsible for generating it.",5.1. Syntax of measure-theoretic BLOG,[0],[0]
Here Nationality maps applicants to their nationalities.,5.1. Syntax of measure-theoretic BLOG,[0],[0]
Lines 7 and 13 define two random variables by keyword random.,5.1. Syntax of measure-theoretic BLOG,[0],[0]
Lines 7 to 12 state that the GPA of an applicant is distributed as a mixture of weighted discrete and continuous distributions.,5.1. Syntax of measure-theoretic BLOG,[0],[0]
"For US applicants, the range of values 0",5.1. Syntax of measure-theoretic BLOG,[0],[0]
< GPA < 4 follows a truncated Gaussian with bounds 0 and 4 (line 9).,5.1. Syntax of measure-theoretic BLOG,[0],[0]
The probability mass outside the range is attributed to the corresponding bounds: P (GPA = 0),5.1. Syntax of measure-theoretic BLOG,[0],[0]
= P,5.1. Syntax of measure-theoretic BLOG,[0],[0]
(,5.1. Syntax of measure-theoretic BLOG,[0],[0]
GPA = 4) = 10−4 (line 10).,5.1. Syntax of measure-theoretic BLOG,[0],[0]
GPA distributions for other countries are specified similarly.,5.1. Syntax of measure-theoretic BLOG,[0],[0]
Line 13 defines a random applicant David.,5.1. Syntax of measure-theoretic BLOG,[0],[0]
"Line 15 states that the David’s GPA is observed to be 4 and we query in line 16 whether David is from USA.
",5.1. Syntax of measure-theoretic BLOG,[0],[0]
Number Statement (line 3 to 5) Fig. 2 shows the syntax of a number statement for Typei.,5.1. Syntax of measure-theoretic BLOG,[0],[0]
"In this specification, gj are origin functions (discussed below); ȳj are tuples of arguments drawn from x̄ = x1, . .",5.1. Syntax of measure-theoretic BLOG,[0],[0]
.,5.1. Syntax of measure-theoretic BLOG,[0],[0]
", xk; ϕj are first-order formulas with free variables ȳj ; ēj are tuples of expressions
3The specialized syntax in BLOG to express models with infinite number of variables.
",5.1. Syntax of measure-theoretic BLOG,[0],[0]
"over a subset of x1, . . .",5.1. Syntax of measure-theoretic BLOG,[0],[0]
", xk; and cj(ēj) specify kernels κj : Π{Xτe :e∈ēj}Xe → N where τe is the type of the expression e.
#Typei(g1 = x1, . . .",5.1. Syntax of measure-theoretic BLOG,[0],[0]
", gk = xk) ∼",5.1. Syntax of measure-theoretic BLOG,[0],[0]
"if ϕ1(ȳ1) then c1(ē1)
else if ϕ2(ȳ2) then c2(ē2)
.",5.1. Syntax of measure-theoretic BLOG,[0],[0]
". .
",5.1. Syntax of measure-theoretic BLOG,[0],[0]
"else cm(ēm);
ments can be recovered using the origin functions gj , each of which is declared as:
origin Typej gj(Typei),
where Typej is the type of the argument xj in the number statement of Typei where gj was used.",5.1. Syntax of measure-theoretic BLOG,[0],[0]
"The value of the jth variable used in the number statement that generated u, an element of the universe, is given by gj(u).",5.1. Syntax of measure-theoretic BLOG,[0],[0]
"Line 6 in Fig. 1 is an example of origin function.
",5.1. Syntax of measure-theoretic BLOG,[0],[0]
Mixture Distribution (line 9 to 12),5.1. Syntax of measure-theoretic BLOG,[0],[0]
"In measure-theoretic BLOG, we introduce a new distribution, the mixture distribution (e.g., lines 9-10 in Fig. 1).",5.1. Syntax of measure-theoretic BLOG,[0],[0]
"A mixture distribution is specified as:
Mix({c1(ē1)→ w1(ē′), . . .",5.1. Syntax of measure-theoretic BLOG,[0],[0]
", ck(ēk)→ wk(ē′)}), where ci are arbitrary distributions, and wi’s are arbitrary real valued functions that sum to 1 for every possible assignment to their arguments: ∀ē′ ∑ i wi(ē
′) =",5.1. Syntax of measure-theoretic BLOG,[0],[0]
1.,5.1. Syntax of measure-theoretic BLOG,[0],[0]
"Note that in our implementation of measure-theoretical BLOG, we only allow a Mix distribution to express a mixture of densities and masses for simplifying the system design, although it still possible to express the same semantics without Mix.",5.1. Syntax of measure-theoretic BLOG,[0],[0]
In this section we present the semantics of measure-theoretic BLOG and its theoretical properties.,5.2. Semantics of measure-theoretic BLOG,[0],[0]
Every BLOG model implicitly defines a first-order vocabulary consisting of the set of functions and types mentioned in the model.,5.2. Semantics of measure-theoretic BLOG,[0],[0]
"BLOG’s semantics are based on the standard, open-universe semantics of first-order logic.",5.2. Semantics of measure-theoretic BLOG,[0],[0]
"We first define the set of all possible elements that may be generated for a BLOG model.
",5.2. Semantics of measure-theoretic BLOG,[0],[0]
Definition 5.1.,5.2. Semantics of measure-theoretic BLOG,[0],[0]
"The set of possible elements UM for a BLOG model M with types {τ1, . . .",5.2. Semantics of measure-theoretic BLOG,[0],[0]
", τk} is ⋃ j∈N{Uj}, where
• U0 = 〈U01 , . . .",5.2. Semantics of measure-theoretic BLOG,[0],[0]
", U0k 〉, U0j = {cj : cj is a distinct τi constant inM} • Ui+1",5.2. Semantics of measure-theoretic BLOG,[0],[0]
"= 〈U i+11 , . . .",5.2. Semantics of measure-theoretic BLOG,[0],[0]
", U i+1 k 〉, where U i+1m = U",5.2. Semantics of measure-theoretic BLOG,[0],[0]
"im ∪
{uν,ū,m : ν(x̄) is a number statement of type τm, ū is a tuple of elements of the type of x̄ from U i, m ∈ N}
Def.",5.2. Semantics of measure-theoretic BLOG,[0],[0]
"5.1 allows us to define the set of random variables corresponding to a BLOG model.
",5.2. Semantics of measure-theoretic BLOG,[0],[0]
Definition 5.2.,5.2. Semantics of measure-theoretic BLOG,[0],[0]
"The set of basic random variables for a BLOG modelM, BRV (M), consists of:
• for each number statement ν(x̄), a number variable",5.2. Semantics of measure-theoretic BLOG,[0],[0]
Vν,5.2. Semantics of measure-theoretic BLOG,[0],[0]
"[ū] over the standard measurable space N, where ū is of the type of x̄. • for each function f(x̄) and tuple ū from UM of the type of x̄, a function application variable",5.2. Semantics of measure-theoretic BLOG,[0],[0]
Vf [ū] with the measurable space XVf,5.2. Semantics of measure-theoretic BLOG,[0],[0]
"[ū] = Xτf , where Xτf is the measurable space corresponding to τf , the return type of f .
",5.2. Semantics of measure-theoretic BLOG,[0],[0]
"We now define the space of consistent assignments to random variables.
",5.2. Semantics of measure-theoretic BLOG,[0],[0]
Definition 5.3.,5.2. Semantics of measure-theoretic BLOG,[0],[0]
"An instantiation σ of the basic RVs defined by a BLOG modelM is consistent if and only if:
•",5.2. Semantics of measure-theoretic BLOG,[0],[0]
"For every element uν,v̄,i used in an assignment of the form",5.2. Semantics of measure-theoretic BLOG,[0],[0]
σ(Vf [ū]),5.2. Semantics of measure-theoretic BLOG,[0],[0]
= w or σ(Vν [ū]),5.2. Semantics of measure-theoretic BLOG,[0],[0]
"= m > 0, σ(Vν",5.2. Semantics of measure-theoretic BLOG,[0],[0]
[v̄]),5.2. Semantics of measure-theoretic BLOG,[0],[0]
≥,5.2. Semantics of measure-theoretic BLOG,[0],[0]
i;,5.2. Semantics of measure-theoretic BLOG,[0],[0]
"• For every fixed function symbol f with the interpretation f̃ , σ(Vf [ū]) = f̃(ū); and • For every element uν,ū=〈u1,...,um〉,i, generated by the number statement ν, with origin functions g1, . . .",5.2. Semantics of measure-theoretic BLOG,[0],[0]
", gm, for every gj ∈ {g1, . . .",5.2. Semantics of measure-theoretic BLOG,[0],[0]
", gm}, σ(Vgj [uν,ū,i]) = uj .",5.2. Semantics of measure-theoretic BLOG,[0],[0]
"That is, origin functions give correct inverse maps.
",5.2. Semantics of measure-theoretic BLOG,[0],[0]
Lemma 5.4.,5.2. Semantics of measure-theoretic BLOG,[0],[0]
"Every consistent assignment σ to the basic RVs forM defines a unique possible world in the vocabulary ofM.
The proof of Lemma 5.4 is in Appx.",5.2. Semantics of measure-theoretic BLOG,[0],[0]
"F. In the following definition, we use the notation e[ū/x̄] to denote a substitution of every occurrence of the variable xi with ui in the expression e. For any BLOG modelM, let V (M) = BRV (M); for each v ∈ V , Xv is the measurable space corresponding to v. Let E(M) consist of the following edges for every number statement or function application statement of the form s(x̄):
• The edge (Vg[w̄], Vs[ū])",5.2. Semantics of measure-theoretic BLOG,[0],[0]
"if g is a function symbol in M such that g(ȳ) appears in s(x̄), and either g(w̄) = g(ȳ)[ū/x̄] or an occurrence of g(ȳ) in s(x̄) uses quantified variables z1, . . .",5.2. Semantics of measure-theoretic BLOG,[0],[0]
", zn, ū′ is a tuple of elements of the type of z̄ and g(w̄) = g(ȳ)[ū/x̄][ū′/z̄].",5.2. Semantics of measure-theoretic BLOG,[0],[0]
"• The edge (Vν [v̄], Vs[ū]), for element uν,v̄,i ∈ ū.
Note that the first set of edges defined in E(M) above may include infinitely many parents for Vs[ū].",5.2. Semantics of measure-theoretic BLOG,[0],[0]
Let the dependency statement in the BLOG model M corresponding to a number or function variable Vs[f̄ ] be s. Let expr(s) be the set of expressions used in s.,5.2. Semantics of measure-theoretic BLOG,[0],[0]
"Each such statement then defines in a straightforward manner, a kernel Ks(ū) : Xexpr(s(ū))",5.2. Semantics of measure-theoretic BLOG,[0],[0]
→ XVs[ū].,5.2. Semantics of measure-theoretic BLOG,[0],[0]
"In order ensure consistent assignments, we include a special value null ∈",5.2. Semantics of measure-theoretic BLOG,[0],[0]
Xτ for each τ,5.2. Semantics of measure-theoretic BLOG,[0],[0]
Figure 3.,9 query hasFakeCoin;,[0],[0]
"BLOG code for the Scale example
inM, and require that Ks(ū)(σ(pa(Vs[ū])), {null}c) = 0",9 query hasFakeCoin;,[0],[0]
whenever σ violates the first condition of consistent assignments (Def. 5.3).,9 query hasFakeCoin;,[0],[0]
"In other words, all the local kernels ensure are locally consistent: variables involving an object uν,ū,i get a non-null assignment only if the assignment to its number statement represents the generation of at least i objects (σ(Vν(ū))",9 query hasFakeCoin;,[0],[0]
≥ i).,9 query hasFakeCoin;,[0],[0]
"Each kernel of the formKs(ū) can be transformed into a kernel Kpa(Vs[ū]) from its parent vertices (representing basic random variables) by composing the kernels determining the truth value of each expression e ∈ expr(v) in terms of the basic random variables, with the kernel KeVs[ū].",9 query hasFakeCoin;,[0],[0]
Let κ(M) = {Kpa(Vs[ū]) : Vs[ū] ∈ BRV (M)}.,9 query hasFakeCoin;,[0],[0]
Definition 5.5.,9 query hasFakeCoin;,[0],[0]
"The MTBN M for a BLOG model M is defined using V = V (M), E = E(M), the set of measurable spaces {Xv : v ∈ BRV (M)} and the kernels for each vertex given by κ(M).
",9 query hasFakeCoin;,[0],[0]
"By Thm. 3.6, we have the main result of this section, which provides the theoretical foundation for the generalized BLOG language:
Theorem 5.6.",9 query hasFakeCoin;,[0],[0]
"If the MTBNM for a BLOG model is a wellfounded digraph, thenM represents a unique measure µ on XBRV (M).",9 query hasFakeCoin;,[0],[0]
"We implemented the measure-theoretic extension of BLOG and evaluated our inference algorithms on three models where naive algorithms fail: (1) the GPA model (GPA); (2) the noisy scale model (Scale); and (3) a SSM, the aircraft tracking model (Aircraft-Tracking).",6. Experiment Results,[0],[0]
"The implementation is based on BLOG’s C++ compiler (Wu et al., 2016).
",6. Experiment Results,[0],[0]
GPA model: Fig. 1 presents the BLOG code for the GPA example as explained in Sec. 5.,6. Experiment Results,[0],[0]
"Since the GPA of David is exactly 4, Bayes rule implies that David must be from USA.",6. Experiment Results,[0],[0]
"We evaluate LLW and the naive LW on this model in Fig 4(a), where the naive LW converges to an incorrect posterior.
",6. Experiment Results,[0],[0]
Scale model:,6. Experiment Results,[0],[0]
"In the noisy scale example (Fig. 3), we have an even number of coins and there might be a fake coin among them (Line 4).",6. Experiment Results,[0],[0]
The fake coin will be slightly heavier than a normal coin (Line 2-3).,6. Experiment Results,[0],[0]
We divide the coins into two halves and place them onto a noisy scale.,6. Experiment Results,[0],[0]
"When there is no fake coin, the scale always balances (Line 7).
",6. Experiment Results,[0],[0]
"When there is a fake coin, the scale will noisily reflect the weight difference with standard deviation σ",6. Experiment Results,[0],[0]
(sigma in Line 6).,6. Experiment Results,[0],[0]
Now we observe that the scale is balanced (Line 8),6. Experiment Results,[0],[0]
and we would like to infer whether a fake coin exists.,6. Experiment Results,[0],[0]
We again compare LLW against the naive LW with different choices of the σ parameter in Fig. 4(b).,6. Experiment Results,[0],[0]
"Since the scale is precisely balanced, there must not be a fake coin.",6. Experiment Results,[0],[0]
"LLW always produces the correct answer but naive LW converges to different incorrect posteriors for different values of σ; as σ increases, naive LWs result approaches the true posterior.
",6. Experiment Results,[0],[0]
Aircraft-Tracking model: Fig. 5 shows a simplified BLOG model for the aircraft tracking example.,6. Experiment Results,[0],[0]
"In this state-space model, we have N = 6 radar points (Line 1) and a single aircraft to track.",6. Experiment Results,[0],[0]
Both the radars and the aircraft are considered as points on a 2D plane.,6. Experiment Results,[0],[0]
The prior of the aircraft movement is a Gaussian process (Line 3 to 6).,6. Experiment Results,[0],[0]
"Each radar r has an effective range radius(r): if the aircraft is within the range, the radar will noisily measure the distance from the aircraft to its own location (Line 13); if the aircraft is out of range, the radar will almost surely just output its radius (Line 10 to 11).",6. Experiment Results,[0],[0]
Now we observe the measurements from all the radar points for T time steps and we want to infer the location of the aircraft.,6. Experiment Results,[0],[0]
"With the measure-theoretic extension, a generalized BLOG program is more expressive for modeling truncated sensors: if a radar outputs exactly its radius, we can surely infer that the aircraft must be out of the effective range of this radar.",6. Experiment Results,[0],[0]
"However, this information cannot be captured by the original BLOG language.",6. Experiment Results,[0],[0]
"To illustrate this case, we manually generated a synthesis dataset of T = 8 time steps4 and evaluated LPF against the naive particle filter with different numbers of particles in Fig. 4(c).",6. Experiment Results,[0],[0]
We take the mean of the samples from all the particles as the predicted aircraft location.,6. Experiment Results,[0],[0]
"Since we know the ground truth, we measure the average mean square error between the true location and the prediction.",6. Experiment Results,[0],[0]
"LPF accurately predicts the
4The full BLOG programs with complete data are available at https://goo.gl/f7qLwy.",6. Experiment Results,[0],[0]
Figure 5.,18 query Y(t) for Timestep t;,[0],[0]
"BLOG code for the Aircraft-Tracking example
true locations while the naive PF converges to the incorrect results.",18 query Y(t) for Timestep t;,[0],[0]
"We presented a new formalization, measure-theoretic Bayesian networks, for generalizing the semantics of PPLs to include random variables with mixtures of discrete and continuous distributions.",7. Conclusion,[0],[0]
"We developed provably correct inference algorithms for such random variables and incorporated MTBNs into a widely used PPL, BLOG.",7. Conclusion,[0],[0]
"We believe that together with the foundational inference algorithms, our proposed rigorous framework will facilitate the development of powerful techniques for probabilistic reasoning in practical applications from a much wider range of scientific areas.",7. Conclusion,[0],[0]
"This work is supported by the DARPA PPAML program, contract FA8750-14-C-0011.",Acknowledgment,[0],[0]
"Simon S. Du is funded by NSF grant IIS1563887, AFRL grant FA8750-17-2-0212 and DARPA D17AP00001.",Acknowledgment,[0],[0]
"Despite the recent successes of probabilistic programming languages (PPLs) in AI applications, PPLs offer only limited support for random variables whose distributions combine discrete and continuous elements.",abstractText,[0],[0]
We develop the notion of measure-theoretic Bayesian networks (MTBNs) and use it to provide more general semantics for PPLs with arbitrarily many random variables defined over arbitrary measure spaces.,abstractText,[0],[0]
"We develop two new general sampling algorithms that are provably correct under the MTBN framework: the lexicographic likelihood weighting (LLW) for general MTBNs and the lexicographic particle filter (LPF), a specialized algorithm for statespace models.",abstractText,[0],[0]
"We further integrate MTBNs into a widely used PPL system, BLOG, and verify the effectiveness of the new inference algorithms through representative examples.",abstractText,[0],[0]
Discrete-Continuous Mixtures in Probabilistic Programming: Generalized Semantics and Inference Algorithms,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2315–2325, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
"Discriminative sentence modeling aims to capture sentence meanings, and classify sentences according to certain criteria (e.g., sentiment).",1 Introduction,[0],[0]
"It is related to various tasks of interest, and has attracted much attention in the NLP community (Allan et al., 2003; Su and Markert, 2008; Zhao et al., 2015).",1 Introduction,[0],[0]
"Feature engineering—for example, n-gram features (Cui et al., 2006), dependency subtree features (Nakagawa et al., 2010), or more dedicated ones (Silva et al., 2011)—can play an important role in modeling sentences.",1 Introduction,[0],[0]
"Kernel machines, e.g., SVM, are exploited in Moschitti (2006) and Reichartz et al. (2010) by specifying a certain measure of similarity between sentences, without explicit feature representation.
",1 Introduction,[0],[0]
∗These authors contribute equally to this paper.,1 Introduction,[0],[0]
"†To whom correspondence should be addressed.
",1 Introduction,[0],[0]
"Recent advances of neural networks bring new techniques in understanding natural languages, and have exhibited considerable potential.",1 Introduction,[0],[0]
"Bengio et al. (2003) and Mikolov et al. (2013) propose unsupervised approaches to learn word embeddings, mapping discrete words to real-valued vectors in a meaning space.",1 Introduction,[0],[0]
Le and Mikolov (2014) extend such approaches to learn sentences’ and paragraphs’ representations.,1 Introduction,[0],[0]
"Compared with human engineering, neural networks serve as a way of automatic feature learning (Bengio et al., 2013).
",1 Introduction,[0],[0]
Two widely used neural sentence models are convolutional neural networks (CNNs) and recursive neural networks (RNNs).,1 Introduction,[0],[0]
"CNNs can extract words’ neighboring features effectively with short propagation paths, but they do not capture inherent sentence structures (e.g., parse trees).",1 Introduction,[0],[0]
"RNNs encode, to some extent, structural information by recursive semantic composition along a parse tree.",1 Introduction,[0],[0]
"However, they may have difficulties in learning deep dependencies because of long propagation paths (Erhan et al., 2009).",1 Introduction,[0],[0]
"(CNNs/RNNs and a variant, recurrent networks, will be reviewed in Section 2.)
",1 Introduction,[0],[0]
"A curious question is whether we can combine the advantages of CNNs and RNNs, i.e., whether we can exploit sentence structures (like RNNs) effectively with short propagation paths (like CNNs).
",1 Introduction,[0],[0]
"In this paper, we propose a novel neural architecture for discriminative sentence modeling, called the Tree-Based Convolutional Neural Network (TBCNN).1 Our models can leverage different sentence parse trees, e.g., constituency trees and dependency trees.",1 Introduction,[0],[0]
"The model variants are denoted as c-TBCNN and d-TBCNN, respectively.",1 Introduction,[0],[0]
"The idea of tree-based convolution is to apply a set of subtree feature detectors, sliding over the entire
1The model of tree-based convolution was firstly proposed to process program source code in our (unpublished) previous work (Mou et al., 2014).
2315
parse tree of a sentence; then pooling aggregates these extracted feature vectors by taking the maximum value in each dimension.",1 Introduction,[0],[0]
"One merit of such architecture is that all features, along the tree, have short propagation paths to the output layer, and hence structural information can be learned effectively.
",1 Introduction,[0],[0]
"TBCNNs are evaluated on two tasks, sentiment analysis and question classification; our models have outperformed previous state-of-the-art results in both experiments.",1 Introduction,[0],[0]
"To understand how TBCNNs work, we also visualize the network by plotting the convolution process.",1 Introduction,[0],[0]
We make our code and results available on our project website.2,1 Introduction,[0],[0]
"In this section, we present the background and related work regarding two prevailing neural architectures for discriminative sentence modeling.",2 Background and Related Work,[0],[0]
"Convolutional neural networks (CNNs), early used for image processing (LeCun, 1995), turn out to be effective with natural languages as well.",2.1 Convolutional Neural Networks,[0],[0]
"Figure 1a depicts a classic convolution process on a sentence (Collobert and Weston, 2008).",2.1 Convolutional Neural Networks,[0],[0]
"A set of fixed-width-window feature detectors slide over the sentence, and output the extracted features.",2.1 Convolutional Neural Networks,[0],[0]
"Let t be the window size, and x1, · · · ,",2.1 Convolutional Neural Networks,[0],[0]
xt ∈ Rne be ne-dimensional word embeddings.,2.1 Convolutional Neural Networks,[0],[0]
"The output of convolution, evaluated at the current position, is
y = f (W ·",2.1 Convolutional Neural Networks,[0],[0]
[x1; · · · ; xt] + b) where y ∈ Rnc (nc is the number of feature detectors).,2.1 Convolutional Neural Networks,[0],[0]
"W ∈ Rnc×(t·ne) and b ∈ Rnc are parame-
2https://sites.google.com/site/tbcnnsentence/
ters; f is the activation function.",2.1 Convolutional Neural Networks,[0],[0]
Semicolons represent column vector concatenation.,2.1 Convolutional Neural Networks,[0],[0]
"After convolution, the extracted features are pooled to a fixedsize vector for classification.
",2.1 Convolutional Neural Networks,[0],[0]
Convolution can extract neighboring information effectively.,2.1 Convolutional Neural Networks,[0],[0]
"However, the features are “local”—words that are not in a same convolution window do not interact with each other, even though they may be semantically related.",2.1 Convolutional Neural Networks,[0],[0]
Blunsom et al. (2014) build deep convolutional networks so that local features can mix at high-level layers.,2.1 Convolutional Neural Networks,[0],[0]
Similar CNNs include Kim (2014) and Hu et al. (2014).,2.1 Convolutional Neural Networks,[0],[0]
"All these models are “flat,” by which we mean no structural information is used explicitly.",2.1 Convolutional Neural Networks,[0],[0]
"Recursive neural networks (RNNs), proposed in Socher et al. (2011b), utilize sentence parse trees.",2.2 Recursive Neural Networks,[0],[0]
"In the original version, RNN is built upon a binarized constituency tree.",2.2 Recursive Neural Networks,[0],[0]
"Leaf nodes correspond to words in a sentence, represented by nedimensional embeddings.",2.2 Recursive Neural Networks,[0],[0]
"Non-leaf nodes are sentence constituents, coded by child nodes recursively.",2.2 Recursive Neural Networks,[0],[0]
"Let node p be the parent of c1 and c2, vector representations denoted as p, c1, and c2.",2.2 Recursive Neural Networks,[0],[0]
"The parent’s representation is composited by
p = f(W ·",2.2 Recursive Neural Networks,[0],[0]
[c1; c2] + b) (1) where W and b are parameters.,2.2 Recursive Neural Networks,[0],[0]
"This process is done recursively along the tree; the root vector is then used for supervised classification (Figure 1b).
",2.2 Recursive Neural Networks,[0],[0]
"Dependency parse and the combinatory categorical grammar can also be exploited as RNNs’ skeletons (Hermann and Blunsom, 2013; Iyyer et al., 2014).",2.2 Recursive Neural Networks,[0],[0]
Irsoy and Cardie (2014) build deep RNNs to enhance information interaction.,2.2 Recursive Neural Networks,[0],[0]
"Im-
provements for semantic compositionality include matrix-vector interaction (Socher et al., 2012), tensor interaction (Socher et al., 2013).",2.2 Recursive Neural Networks,[0],[0]
"They are more suitable for capturing logical information in sentences, such as negation and exclamation.
",2.2 Recursive Neural Networks,[0],[0]
One potential problem of RNNs is that the long propagation paths—through which leaf nodes are connected to the output layer—may lead to information loss.,2.2 Recursive Neural Networks,[0],[0]
"Thus, RNNs bury illuminating information under a complicated neural architecture.",2.2 Recursive Neural Networks,[0],[0]
"Further, during back-propagation over a long path, gradients tend to vanish (or blow up), which makes training difficult (Erhan et al., 2009).",2.2 Recursive Neural Networks,[0],[0]
"Long short term memory (LSTM), first proposed for modeling time-series data (Hochreiter and Schmidhuber, 1997), is integrated to RNNs to alleviate this problem (Tai et al., 2015; Le and Zuidema, 2015; Zhu et al., 2015).
",2.2 Recursive Neural Networks,[0],[0]
Recurrent networks.,2.2 Recursive Neural Networks,[0],[0]
"A variant class of RNNs is the recurrent neural network (Bengio et al., 1994; Shang et al., 2015), whose architecture is a rightmost tree.",2.2 Recursive Neural Networks,[0],[0]
"In such models, meaningful tree structures are also lost, similar to CNNs.",2.2 Recursive Neural Networks,[0],[0]
This section introduces the proposed tree-based convolutional neural networks (TBCNNs).,3 Tree-based Convolution,[0],[0]
"Figure 1c depicts the convolution process on a tree.
",3 Tree-based Convolution,[0],[0]
"First, a sentence is converted to a parse tree, either a constituency or dependency tree.",3 Tree-based Convolution,[0],[0]
The corresponding model variants are denoted as c-TBCNN and d-TBCNN.,3 Tree-based Convolution,[0],[0]
"Each node in the tree is represented as a distributed, real-valued vector.
",3 Tree-based Convolution,[0],[0]
"Then, we design a set of fixed-depth subtree feature detectors, called the tree-based convolution window.",3 Tree-based Convolution,[0],[0]
"The window slides over the entire tree to extract structural information of the sentence, illustrated by a dashed triangle in Figure 1c.",3 Tree-based Convolution,[0],[0]
"Formally, let us assume we have t nodes in the convolution window, x1, · · · ,xt, each represented as an ne-dimensional vector.",3 Tree-based Convolution,[0],[0]
Let nc be the number of feature detectors.,3 Tree-based Convolution,[0],[0]
"The output of the tree-based convolution window, evaluated at the current subtree, is given by the following generic equation.
",3 Tree-based Convolution,[0],[0]
"y = f
( t∑
i=1
Wi ·xi + b )
(2)
where Wi ∈ Rnc×ne is the weight parameter associated with node xi; b ∈ Rnc is the bias term.
",3 Tree-based Convolution,[0],[0]
"Extracted features are thereafter packed into one or more fixed-size vectors by max pooling,
that is, the maximum value in each dimension is taken.",3 Tree-based Convolution,[0],[0]
"Finally, we add a fully connected hidden layer, and a softmax output layer.
",3 Tree-based Convolution,[0],[0]
"From the designed architecture (Figure 1c), we see that our TBCNN models allow short propagation paths between the output layer and any position in the tree.",3 Tree-based Convolution,[0],[0]
"Therefore structural feature learning becomes effective.
",3 Tree-based Convolution,[0],[0]
Several main technical points in tree-based convolution include: (1) How can we represent hidden nodes as vectors in constituency trees?,3 Tree-based Convolution,[0],[0]
"(2) How can we determine weights, Wi, for dependency trees, where nodes may have different numbers of children?",3 Tree-based Convolution,[0],[0]
"(3) How can we pool varying sized and shaped features to fixed-size vectors?
",3 Tree-based Convolution,[0],[0]
"In the rest of this section, we explain model variants in detail.",3 Tree-based Convolution,[0],[0]
"Particularly, Subsections 3.1 and 3.2 address the first and second problems; Subsection 3.3 deals with the third problem by introducing several pooling heuristics.",3 Tree-based Convolution,[0],[0]
Subsection 3.4 presents our training objective.,3 Tree-based Convolution,[0],[0]
"Figure 2a illustrates an example of the constituency tree, where leaf nodes are words in the sentence, and non-leaf nodes represent a grammatical constituent, e.g., a noun phrase.",3.1 c-TBCNN,[0],[0]
"Sentences are parsed by the Stanford parser;3 further, constituency trees are binarized for simplicity.
",3.1 c-TBCNN,[0],[0]
One problem of constituency trees is that nonleaf nodes do not have such vector representations as word embeddings.,3.1 c-TBCNN,[0],[0]
"Our strategy is to pretrain the constituency tree with an RNN by Equation 1 (Socher et al., 2011b).",3.1 c-TBCNN,[0],[0]
"After pretraining, vector representations of nodes are fixed.
",3.1 c-TBCNN,[0],[0]
"We now consider the tree-based convolution process in c-TBCNN with a two-layer-subtree convolution window, which operates on a parent node p and its direct children cl and cr, their vector representations denoted as p, cl, and cr.",3.1 c-TBCNN,[0],[0]
"The convolution equation, specific for c-TBCNN, is
y = f ( W (c)p ·p +W (c)l ·cl +W (c)r ·cr + b(c) ) where W (c)p , W (c) l , and W (c) r are weights associated with the parent and its child nodes.",3.1 c-TBCNN,[0],[0]
Superscript (c) indicates that the weights are for cTBCNN.,3.1 c-TBCNN,[0],[0]
"For leaf nodes, which do not have children, we set cl and cr to be 0.
3http://nlp.stanford.edu/software/lex-parser.shtml
Tree-based convolution windows can be extended to arbitrary depths straightforwardly.",3.1 c-TBCNN,[0],[0]
"The complexity is exponential to the depth of the window, but linear to the number of nodes.",3.1 c-TBCNN,[0],[0]
"Hence, tree-based convolution, compared with “flat” CNNs, does not add to computational cost, provided the same amount of information to process at a time.",3.1 c-TBCNN,[0],[0]
"In our experiments, we use convolution windows of depth 2.",3.1 c-TBCNN,[0],[0]
Dependency trees are another representation of sentence structures.,3.2 d-TBCNN,[0],[0]
The nature of dependency representation leads to d-TBCNN’s major difference from traditional convolution: there exist nodes with different numbers of child nodes.,3.2 d-TBCNN,[0],[0]
"This causes trouble if we associate weight parameters according to positions in the window, which is standard for traditional convolution, e.g., Collobert and Weston (2008) or c-TBCNN.
",3.2 d-TBCNN,[0],[0]
"To overcome the problem, we extend the notion of convolution by assigning weights according to dependency types (e.g, nsubj) rather than positions.",3.2 d-TBCNN,[0],[0]
"We believe this strategy makes much sense because dependency types (de Marneffe et al., 2006) reflect the relationship between a governing word and its child words.",3.2 d-TBCNN,[0],[0]
"To be concrete, the generic convolution formula (Equation 2) for d-TBCNN becomes
y = f ( W (d)p ·p + n∑ i=1",3.2 d-TBCNN,[0],[0]
"W (d) r[ci] ·ci + b(d) )
where W (d)p is the weight parameter for the parent p (governing word); W (d)r[ci] is the weight for child ci, who has grammatical relationship r[ci]
to its parent,",3.2 d-TBCNN,[0],[0]
p. Superscript (d) indicates the parameters are for d-TBCNN.,3.2 d-TBCNN,[0],[0]
"Note that we keep 15 most frequently occurred dependency types; others appearing rarely in the corpus are mapped to one shared weight matrix.
",3.2 d-TBCNN,[0],[0]
Both c-TBCNN and d-TBCNN have their own advantages: d-TBCNN exploits structural features more efficiently because of the compact expressiveness of dependency trees; c-TBCNN may be more effective in integrating global features due to the underneath pretrained RNN.,3.2 d-TBCNN,[0],[0]
"As different sentences may have different lengths and tree structures, the extracted features by treebased convolution also have topologies varying in size and shape.",3.3 Pooling Heuristics,[0],[0]
"Dynamic pooling (Socher et al., 2011a) is a common technique for dealing with
this problem.",3.3 Pooling Heuristics,[0],[0]
We propose several heuristics for pooling along a tree structure.,3.3 Pooling Heuristics,[0],[0]
Our generic design criteria for pooling include: (1) Nodes that are pooled to one slot should be “neighboring” from some viewpoint.,3.3 Pooling Heuristics,[0],[0]
"(2) Each slot should have similar numbers of nodes, in expectation, that are pooled to it.",3.3 Pooling Heuristics,[0],[0]
"Thus, (approximately) equal amount of information is aggregated along different parts of the tree.",3.3 Pooling Heuristics,[0],[0]
"Following the above intuition, we propose pooling heuristics as follows.
",3.3 Pooling Heuristics,[0],[0]
•,3.3 Pooling Heuristics,[0],[0]
Global pooling.,3.3 Pooling Heuristics,[0],[0]
"All features are pooled to one vector, shown in Figure 3a.",3.3 Pooling Heuristics,[0],[0]
We take the maximum value in each dimension.,3.3 Pooling Heuristics,[0],[0]
"This simple heuristic is applicable to any structure, including c-TBCNN and d-TBCNN.",3.3 Pooling Heuristics,[0],[0]
• 3-slot pooling for c-TBCNN.,3.3 Pooling Heuristics,[0],[0]
"To preserve
more information over different parts of constituency trees, we propose 3-slot pooling (Figure 3b).",3.3 Pooling Heuristics,[0],[0]
"If a tree has maximum depth d, we pool nodes of less than α · d layers to a TOP slot (α is set to 0.6); lower nodes are pooled to slots LOWER LEFT or LOWER",3.3 Pooling Heuristics,[0],[0]
RIGHT according to their relative position with respect to the root node.,3.3 Pooling Heuristics,[0],[0]
"For a constituency tree, it is not completely obvious how to pool features to more than 3 slots and comply with the aforementioned criteria at the same time.",3.3 Pooling Heuristics,[0],[0]
"Therefore, we regard 3-slot pooling for c-TBCNN is a “hard mechanism” temporarily.",3.3 Pooling Heuristics,[0],[0]
Further improvement can be addressed in future work.,3.3 Pooling Heuristics,[0],[0]
• k-slot pooling for d-TBCNN.,3.3 Pooling Heuristics,[0],[0]
"Different from
constituency trees, nodes in dependency trees are one-one corresponding to words in a sentence.",3.3 Pooling Heuristics,[0],[0]
"Thus, a total order on features (after convolution) can be defined according to their corresponding word orders.",3.3 Pooling Heuristics,[0],[0]
"For kslot pooling, we can adopt an “equal allocation” strategy, shown in Figure 3c.",3.3 Pooling Heuristics,[0],[0]
"Let i be the position of a word in a sentence (i = 1, 2, · · · , n).",3.3 Pooling Heuristics,[0],[0]
"Its extracted feature vector is pooled to the j-th slot, if
(j − 1) n",3.3 Pooling Heuristics,[0],[0]
k ≤,3.3 Pooling Heuristics,[0],[0]
"i ≤ j n k
We assess the efficacy of pooling quantitatively in Section 4.3.1.",3.3 Pooling Heuristics,[0],[0]
"As we shall see by the experimental results, complicated pooling methods do preserve more information along tree structures to some extent, but the effect is not large.",3.3 Pooling Heuristics,[0],[0]
TBCNNs are not very sensitive to pooling methods.,3.3 Pooling Heuristics,[0],[0]
"After pooling, information is packed into one or more fixed-size vectors (slots).",3.4 Training Objective,[0],[0]
"We add a hidden layer, and then a softmax layer to predict the probability of each target label in a classification task.",3.4 Training Objective,[0],[0]
"The error function of a sample is the standard cross entropy loss, i.e., J = −∑ci=1 ti log yi, where t is the ground truth (one-hot represented), y the output by softmax, and c the number of classes.",3.4 Training Objective,[0],[0]
"To regularize our model, we apply both `2 penalty and dropout (Srivastava et al., 2014).",3.4 Training Objective,[0],[0]
Training details are further presented in Section 4.1 and 4.2.,3.4 Training Objective,[0],[0]
"In this section, we evaluate our models with two tasks, sentiment analysis and question classification.",4 Experimental Results,[0],[0]
We also conduct quantitative and qualitative model analysis in Subsection 4.3.,4 Experimental Results,[0],[0]
Sentiment analysis is a widely studied task for discriminative sentence modeling.,4.1.1 The Task and Dataset,[0],[0]
"The Stanford sentiment treebank4 consists of more than 10,000 movie reviews.",4.1.1 The Task and Dataset,[0],[0]
"Two settings are considered for sentiment prediction: (1) fine-grained classification with 5 labels (strongly positive, positive, neutral, negative, and strongly negative), and (2) coarse-gained polarity classification with 2 labels (positive versus negative).",4.1.1 The Task and Dataset,[0],[0]
"Some examples are shown in
4http://nlp.stanford.edu/sentiment/
Table 1.",4.1.1 The Task and Dataset,[0],[0]
"We use the standard split for training, validating, and testing, containing 8544/1101/2210 sentences for 5-class prediction.",4.1.1 The Task and Dataset,[0],[0]
"Binary classification does not contain the neutral class.
",4.1.1 The Task and Dataset,[0],[0]
"In the dataset, phrases (sub-sentences) are also tagged with sentiment labels.",4.1.1 The Task and Dataset,[0],[0]
RNNs deal with them naturally during the recursive process.,4.1.1 The Task and Dataset,[0],[0]
"We regard sub-sentences as individual samples during training, like Blunsom et al. (2014) and Le and Mikolov (2014).",4.1.1 The Task and Dataset,[0],[0]
"The training set therefore has more than 150,000 entries in total.",4.1.1 The Task and Dataset,[0],[0]
"For validating and testing, only whole sentences (root labels) are considered in our experiments.
",4.1.1 The Task and Dataset,[0],[0]
Both c-TBCNN and d-TBCNN use the Stanford parser for data preprocessing.,4.1.1 The Task and Dataset,[0],[0]
"This subsection describes training details for dTBCNN, where hyperparameters are chosen by validation.",4.1.2 Training Details,[0],[0]
"c-TBCNN is mostly tuned synchronously (e.g., optimization algorithm, activation function) with some changes in hyperparameters.",4.1.2 Training Details,[0],[0]
"c-TBCNN’s settings can be found on our website.
",4.1.2 Training Details,[0],[0]
"In our d-TBCNN model, the number of units is 300 for convolution and 200 for the last hidden layer.",4.1.2 Training Details,[0],[0]
"Word embeddings are 300 dimensional, pretrained ourselves using word2vec (Mikolov et al., 2013) on the English Wikipedia corpus.",4.1.2 Training Details,[0],[0]
2- slot pooling is applied for d-TBCNN.,4.1.2 Training Details,[0],[0]
"(c-TBCNN uses 3-slot pooling.)
",4.1.2 Training Details,[0],[0]
"To train our model, we compute gradient by back-propagation and apply stochastic gradient descent with mini-batch 200.",4.1.2 Training Details,[0],[0]
"We use ReLU (Nair and Hinton, 2010) as the activation function .
",4.1.2 Training Details,[0],[0]
"For regularization, we add `2 penalty for weights with a coefficient of 10−5.",4.1.2 Training Details,[0],[0]
"Dropout (Srivastava et al., 2014) is further applied to both weights and embeddings.",4.1.2 Training Details,[0],[0]
"All hidden layers are dropped out by 50%, and embeddings 40%.",4.1.2 Training Details,[0],[0]
Table 2 compares our models to state-of-the-art results in the task of sentiment analysis.,4.1.3 Performance,[0],[0]
"For 5- class prediction, d-TBCNN yields 51.4% accuracy, outperforming the previous state-of-the-art result, achieved by the RNN based on long-short term memory (Tai et al., 2015).",4.1.3 Performance,[0],[0]
c-TBCNN is slightly worse.,4.1.3 Performance,[0],[0]
"It achieves 50.4% accuracy, ranking third in the state-of-the-art list (including our d-TBCNN model).
",4.1.3 Performance,[0],[0]
"Regarding 2-class prediction, we adopted a simple strategy in Irsoy and Cardie (2014),5 where the 5-class network is “transferred” directly for binary classification, with estimated target probabilities (by 5-way softmax) reinterpreted for 2 classes.",4.1.3 Performance,[0],[0]
(The neutral class is discarded as in other studies.),4.1.3 Performance,[0],[0]
"This strategy enables us to take a glance at the stability of our TBCNN models, but places itself in a difficult position.",4.1.3 Performance,[0],[0]
"Nonetheless, our d-TBCNN model achieves 87.9% accuracy, ranking forth in the list.
",4.1.3 Performance,[0],[0]
"In a more controlled comparison—with shallow architectures and the basic interaction (linearly transformed and non-linearly squashed)— TBCNNs, of both variants, consistently outperform RNNs (Socher et al., 2011b) to a large extent (50.4–51.4% versus 43.2%); they also consistently outperform “flat” CNNs by more than 10%.",4.1.3 Performance,[0],[0]
"Such results show that structures are important when modeling sentences; tree-based convolution can capture these structural information more effectively than RNNs.
",4.1.3 Performance,[0],[0]
We also observe d-TBCNN achieves higher performance than c-TBCNN.,4.1.3 Performance,[0],[0]
This suggests that compact tree expressiveness is more important than integrating global information in this task.,4.1.3 Performance,[0],[0]
We further evaluate TBCNN models on a question classification task.6,4.2 Question Classification,[0],[0]
The dataset contains 5452 annotated sentences plus 500 test samples in TREC 10.,4.2 Question Classification,[0],[0]
"We also use the standard split, like Silva et al. (2011).",4.2 Question Classification,[0],[0]
"Target labels contain 6 classes, namely abbreviation, entity, description, human, location, and numeric.",4.2 Question Classification,[0],[0]
"Some examples are also shown in Table 1.
",4.2 Question Classification,[0],[0]
"We chose this task to evaluate our models because the number of training samples is rather small, so that we can know TBCNNs’ performance when applied to datasets of different sizes.",4.2 Question Classification,[0],[0]
"To alleviate the problem of data sparseness, we set the dimensions of convolutional layer and the last hidden layer to 30 and 25, respectively.",4.2 Question Classification,[0],[0]
"We do not back-propagate gradient to embeddings in this
5Richard Socher, who first applies neural networks to this task, thinks direct transfer is fine for binary classification.",4.2 Question Classification,[0],[0]
We followed this strategy for simplicity as it is non-trivial to deal with the neutral sub-sentences in the training set if we train a separate model.,4.2 Question Classification,[0],[0]
"Our website reviews some related work and provides more discussions.
",4.2 Question Classification,[0],[0]
"6http://cogcomp.cs.illinois.edu/Data/QA/QC/
task.",4.2 Question Classification,[0],[0]
"Dropout rate for embeddings is 30%; hidden layers are dropped out by 5%.
",4.2 Question Classification,[0],[0]
Table 3 compares our models to various other methods.,4.2 Question Classification,[0],[0]
"The first entry presents the previous state-of-the-art result, achieved by traditional feature/rule engineering (Silva et al., 2011).",4.2 Question Classification,[0],[0]
Their method utilizes more than 10k features and 60 hand-coded rules.,4.2 Question Classification,[0],[0]
"On the contrary, our TBCNN models do not use a single human-engineered feature or rule.",4.2 Question Classification,[0],[0]
"Despite this, c-TBCNN achieves similar accuracy compared with feature engineering; d-TBCNN pushes the state-of-the-art result to 96%.",4.2 Question Classification,[0],[0]
"To the best of our knowledge, this is the first time that neural networks beat dedicated human engineering in this question classification task.
",4.2 Question Classification,[0],[0]
"The result also shows that both c-TBCNN and d-TBCNN reduce the error rate to a large extent, compared with other neural architectures in this task.",4.2 Question Classification,[0],[0]
"In this part, we analyze our models quantitatively and qualitatively in several aspects, shedding some light on the mechanism of TBCNNs.",4.3 Model Analysis,[0],[0]
The extracted features by tree-based convolution have topologies varying in size and shape.,4.3.1 The Effect of Pooling,[0],[0]
We propose in Section 3.3 several heuristics for pooling.,4.3.1 The Effect of Pooling,[0],[0]
"This subsection aims to provide a fair comparison among these pooling methods.
",4.3.1 The Effect of Pooling,[0],[0]
One reasonable protocol for comparison is to tune all hyperparameters for each setting and compare the highest accuracy.,4.3.1 The Effect of Pooling,[0],[0]
"This methodology, however, is too time-consuming, and depends largely on the quality of hyperparameter tuning.",4.3.1 The Effect of Pooling,[0],[0]
An alternative is to predefine a set of sensible hyperparameters and report the accuracy under the same setting.,4.3.1 The Effect of Pooling,[0],[0]
"In this experiment, we chose the latter protocol, where hidden layers are all 300- dimensional; no `2 penalty is added.",4.3.1 The Effect of Pooling,[0],[0]
Each configuration was run five times with different random initializations.,4.3.1 The Effect of Pooling,[0],[0]
"We summarize the mean and standard deviation in Table 4.
",4.3.1 The Effect of Pooling,[0],[0]
"As the results imply, complicated pooling is better than global pooling to some degree for both model variants.",4.3.1 The Effect of Pooling,[0],[0]
"But the effect is not strong; our models are not that sensitive to pooling methods, which mainly serve as a necessity for dealing with varying-structure data.",4.3.1 The Effect of Pooling,[0],[0]
"In our experiments, we apply 3-slot pooling for c-TBCNN and 2-slot pooling for d-TBCNN.
",4.3.1 The Effect of Pooling,[0],[0]
"Comparing with other studies in the literature, we also notice that pooling is very effective and efficient in information gathering.",4.3.1 The Effect of Pooling,[0],[0]
"Irsoy and Cardie (2014) report 200 epochs for training a deep RNN, which achieves 49.8% accuracy in the 5-class sentiment classification.",4.3.1 The Effect of Pooling,[0],[0]
Our TBCNNs are typically trained within 25 epochs.,4.3.1 The Effect of Pooling,[0],[0]
We analyze how sentence lengths affect our models.,4.3.2 The Effect of Sentence Lengths,[0],[0]
"Sentences are split into 7 groups by length, with granularity 5.",4.3.2 The Effect of Sentence Lengths,[0],[0]
A few too long or too short sentences are grouped together for smoothing; the numbers of sentences in each group vary from 126 to 457.,4.3.2 The Effect of Sentence Lengths,[0],[0]
Figure 4 presents accuracies versus lengths in TBCNNs.,4.3.2 The Effect of Sentence Lengths,[0],[0]
"For comparison, we also reimplemented RNN, achieving 42.7% overall accuracy, slightly worse than 43.2% reported in Socher et al. (2011b).",4.3.2 The Effect of Sentence Lengths,[0],[0]
"Thus, we think our reimplementation is fair and that the comparison is sensible.
",4.3.2 The Effect of Sentence Lengths,[0],[0]
We observe that c-TBCNN and d-TBCNN yield very similar behaviors.,4.3.2 The Effect of Sentence Lengths,[0],[0]
They consistently outperform the RNN in all scenarios.,4.3.2 The Effect of Sentence Lengths,[0],[0]
"We also notice the gap, between TBCNNs and RNN, increases when sentences contain more than 20 words.",4.3.2 The Effect of Sentence Lengths,[0],[0]
"This result confirms our theoretical analysis in Section 2—for long sentences, the propagation paths in RNNs are deep, causing RNNs’ difficulty in information processing.",4.3.2 The Effect of Sentence Lengths,[0],[0]
"By contrast, our models explore structural information more effectively with
tree-based convolution.",4.3.2 The Effect of Sentence Lengths,[0],[0]
"As information from any part of the tree can propagate to the output layer with short paths, TBCNNs are more capable for sentence modeling, especially for long sentences.",4.3.2 The Effect of Sentence Lengths,[0],[0]
Visualization is important to understanding the mechanism of neural networks.,4.3.3 Visualization,[0],[0]
"For TBCNNs, we would like to see how the extracted features (after convolution) are further processed by the max pooling layer, and ultimately related to the supervised task.
",4.3.3 Visualization,[0],[0]
"To show this, we trace back where the max pooling layer’s features come from.",4.3.3 Visualization,[0],[0]
"For each dimension, the pooling layer chooses the maximum value from the nodes that are pooled to it.",4.3.3 Visualization,[0],[0]
"Thus, we can count the fraction in which a node’s features are gathered by pooling.",4.3.3 Visualization,[0],[0]
"Intuitively, if a node’s features are more related to the task, the fraction tends to be larger, and vice versa.
",4.3.3 Visualization,[0],[0]
"Figure 5 illustrates an example processed by dTBCNN in the task of sentiment analysis.7 Here, we applied global pooling because information tracing is more sensible with one pooling slot.",4.3.3 Visualization,[0],[0]
"As shown in the figure, tree-based convolution can effectively extract information relevant to the task of interest.",4.3.3 Visualization,[0],[0]
"The 2-layer windows corresponding to “visual will impress viewers,” “the stunning dreamlike visual,” say, are discriminative to the sentence’s sentiment.",4.3.3 Visualization,[0],[0]
"Hence, large fractions (0.24 and 0.19) of their features, after convolution, are gathered by pooling.",4.3.3 Visualization,[0],[0]
"On the other hand, words like the, will, even are known as stop words (Fox, 1989).",4.3.3 Visualization,[0],[0]
"They are mostly noninformative for sentiment; hence, no (or minimal) features are gathered.",4.3.3 Visualization,[0],[0]
"Such results are consistent with human intuition.
",4.3.3 Visualization,[0],[0]
We further observe that tree-based convolution does integrate information of different words in the window.,4.3.3 Visualization,[0],[0]
"For example, the word stunning appears in two windows: (a) the window “stunning” itself, and (b) the window of “the stunning dreamlike visual,” with root node visual, stunning acting as a child.",4.3.3 Visualization,[0],[0]
"We see that Window b is more relevant to the ultimate sentiment than Window a, with fractions 0.19 versus 0.07, even though the root visual itself is neutral in sentiment.",4.3.3 Visualization,[0],[0]
"In fact,
7We only have space to present one example in the paper.",4.3.3 Visualization,[0],[0]
This example was not chosen deliberately.,4.3.3 Visualization,[0],[0]
"Similar traits can be found through out the entire gallery, available on our website.",4.3.3 Visualization,[0],[0]
"Also, we only present d-TBCNN, noticing that dependency trees are intrinsically more suitable for visualization since we know the “meaning” of every node.
",4.3.3 Visualization,[0],[0]
"Window a has a larger fraction than the sum of its children’s (the windows of “the,” “stunning,” and “dreamlike”).",4.3.3 Visualization,[0],[0]
"In this paper, we proposed a novel neural discriminative sentence model based on sentence parsing structures.",5 Conclusion,[0],[0]
"Our model can be built upon either constituency trees (denoted as c-TBCNN) or dependency trees (d-TBCNN).
",5 Conclusion,[0],[0]
Both variants have achieved high performance in sentiment analysis and question classification.,5 Conclusion,[0],[0]
"d-TBCNN is slightly better than c-TBCNN in our experiments, and has outperformed previous stateof-the-art results in both tasks.",5 Conclusion,[0],[0]
"The results show that tree-based convolution can capture sentences’ structural information effectively, which is useful for sentence modeling.",5 Conclusion,[0],[0]
This research is supported by the National Basic Research Program of China (the 973 Program) under Grant No. 2015CB352201 and the National Natural Science Foundation of China under Grant Nos. 61232015 and 91318301.,Acknowledgments,[0],[0]
This paper proposes a tree-based convolutional neural network (TBCNN) for discriminative sentence modeling.,abstractText,[0],[0]
Our model leverages either constituency trees or dependency trees of sentences.,abstractText,[0],[0]
"The tree-based convolution process extracts sentences structural features, which are then aggregated by max pooling.",abstractText,[0],[0]
"Such architecture allows short propagation paths between the output layer and underlying feature detectors, enabling effective structural feature learning and extraction.",abstractText,[0],[0]
We evaluate our models on two tasks: sentiment analysis and question classification.,abstractText,[0],[0]
"In both experiments, TBCNN outperforms previous state-of-the-art results, including existing neural networks and dedicated feature/rule engineering.",abstractText,[0],[0]
"We also make efforts to visualize the tree-based convolution process, shedding light on how our models work.",abstractText,[0],[0]
Discriminative Neural Sentence Modeling by Tree-Based Convolution,title,[0],[0]
Representation learning remains an outstanding research problem in machine learning and computer vision.,1. Introduction,[0],[0]
"Recently there is a rising interest in disentangled representations, in which each component of learned features refers to a semantically meaningful concept.",1. Introduction,[0],[0]
"In the example of video sequence modelling, an ideal disentangled representation would be able to separate time-independent concepts (e.g. the identity of the object in the scene) from dynamical information (e.g. the time-varying position and the orientation or pose of that object).",1. Introduction,[0],[0]
"Such disentangled represen-
1University of Cambridge, UK 2Disney Research, Los Angeles, CA, USA.",1. Introduction,[0],[0]
"Correspondence to: Yingzhen Li<yl494@cam.ac.uk>, Stephan Mandt <stephan.mandt@disneyresearch.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
tations would open new efficient ways of compression and style manipulation, among other applications.
",1. Introduction,[0],[0]
"Recent work has investigated disentangled representation learning for images within the framework of variational auto-encoders (VAEs) (Kingma & Welling, 2013; Rezende et al., 2014) and generative adversarial networks (GANs) (Goodfellow et al., 2014).",1. Introduction,[0],[0]
"Some of them, e.g. the β-VAE method (Higgins et al., 2016), proposed new objective functions/training techniques that encourage disentanglement.",1. Introduction,[0],[0]
"On the other hand, network architecture designs that directly enforce factored representations have also been explored by e.g. Siddharth et al. (2017); Bouchacourt et al. (2017).",1. Introduction,[0],[0]
"These two types of approaches are often mixed together, e.g. the infoGAN approach (Chen et al., 2016) partitioned the latent space and proposed adding a mutual information regularisation term to the vanilla GAN loss.",1. Introduction,[0],[0]
"Mathieu et al. (2016) also partitioned the encoding space into style and content components, and performed adversarial training to encourage the datapoints from the same class to have similar content representations, but diverse style features.
",1. Introduction,[0],[0]
Less research has been conducted for unsupervised learning of disentangled representations of sequences.,1. Introduction,[0],[0]
"For video sequence modelling, Villegas et al. (2017) and Denton & Birodkar (2017) utilised different networks to encode the content and dynamics information separately, and trained the auto-encoders with a combination of reconstruction loss and GAN loss.",1. Introduction,[0],[0]
"Structured (Johnson et al., 2016) and Factorised VAEs (Deng et al., 2017) used hierarchical priors to learn more interpretable latent variables.",1. Introduction,[0],[0]
Hsu et al. (2017) designed a structured VAE in the context of speech recognition.,1. Introduction,[0],[0]
Their VAE architecture is trained using a combination of the standard variational lower bound and a discriminative regulariser to further encourage disentanglement.,1. Introduction,[0],[0]
"More related work is discussed in Section 3.
",1. Introduction,[0],[0]
"In this paper, we propose a generative model for unsupervised structured sequence modelling, such as video or audio.",1. Introduction,[0],[0]
"We show that, in contrast to previous approaches, a disentangled representation can be achieved by a careful design of the probabilistic graphical model.",1. Introduction,[0],[0]
"In the proposed architecture, we explicitly use a latent variable to represent content, i.e., information that is invariant through the sequence, and a set of latent variables associated to each frame to represent dynamical information, such as pose and position.",1. Introduction,[0],[0]
"Com-
pared to the mentioned previous models that usually predict future frames conditioned on the observed sequences, we focus on learning the distribution of the video/audio content and dynamics to enable sequence generation without conditioning.",1. Introduction,[0],[0]
"Therefore our model can also generalise to unseen sequences, which is confirmed by our experiments.",1. Introduction,[0],[0]
"In more detail, our contributions are as follows:
• Controlled generation.",1. Introduction,[0],[0]
Our architecture allows us to approximately control for content and dynamics when generating videos.,1. Introduction,[0],[0]
"We can generate random dynamics for fixed content, and random content for fixed dynamics.",1. Introduction,[0],[0]
"This gives us a controlled way of manipulating a video/audio sequence, such as swapping the identity of moving objects or the voice of a speaker.
",1. Introduction,[0],[0]
• Efficient encoding.,1. Introduction,[0],[0]
Our representation is more data efficient than encoding a video frame by frame.,1. Introduction,[0],[0]
"By factoring out a separate variable that encodes content, our dynamical latent variables can have smaller dimensions.",1. Introduction,[0],[0]
"This may be promising when it comes to end-to-end neural video encoding methods.
",1. Introduction,[0],[0]
"• We design a new metric that allow us to verify disentanglement of the latent variables, by investigating the stability of an object classifier over time.
",1. Introduction,[0],[0]
"• We give empirical evidence, based on video data of a physics simulator, that for long sequences, a stochastic transition model generates more realistic dynamics.
",1. Introduction,[0],[0]
The paper is structured as follows.,1. Introduction,[0],[0]
Section 2 introduces the generative model and the problem setting.,1. Introduction,[0],[0]
Section 3 discusses related work.,1. Introduction,[0],[0]
Section 4 presents three experiments on video and speech data.,1. Introduction,[0],[0]
"Finally, Section 5 concludes the paper and discusses future research directions.",1. Introduction,[0],[0]
"Let x1:T = (x1,x2, ...,xT ) denote a high dimensional sequence, such as a video with T consecutive frames.",2. The model,[0],[0]
"Also, assume the data distribution of the training sequences is pdata(x1:T ).",2. The model,[0],[0]
"In this paper, we model the observed data with a latent variable model that separates the representation of time-invariant concepts (e.g. object identities) from those of time-varying concepts (e.g. pose information).
",2. The model,[0],[0]
Generative model.,2. The model,[0],[0]
"Consider the following probabilistic model, which is also visualised in Figure 1:
pθ(x1:T , z1:T ,f) = pθ(f) T∏ t=1 pθ(zt|z<t)pθ(xt|zt,f).
",2. The model,[0],[0]
(1) We use the convention that z0 = 0.,2. The model,[0],[0]
The generation of frame xt at time t depends on the corresponding latent variables zt and f .,2. The model,[0],[0]
"θ are model parameters.
",2. The model,[0],[0]
"Ideally, f will be capable of modelling global aspects of the whole sequence which are time-invariant, while zt will encode time-varying features.",2. The model,[0],[0]
"This separation may be achieved when choosing the dimensionality of zt to be small enough, thus reserving zt only for time-dependent features while compressing everything else into f .",2. The model,[0],[0]
"In the context of video encodings, zt would thus encode a “morphing transformation”, which encodes how a frame at time t is morphed into a frame at time t+ 1.
Inference models.",2. The model,[0],[0]
"We use variational inference to learn an approximate posterior over latent variables given data (Jordan et al., 1999).",2. The model,[0],[0]
"This involves an approximating distribution q. We train the generative model with the VAE algorithm (Kingma & Welling, 2013):
max θ,φ
EpD(x1:T )",2. The model,[0],[0]
"[ Eqφ [ log pθ(x1:T , z1:T ,f)
qφ(z1:T ,f |x1:T )
",2. The model,[0],[0]
]] .,2. The model,[0],[0]
"(2)
To quantify the effect of the architecture of q on the learned generative model, we test with two types of q factorisation structures as follows.
",2. The model,[0],[0]
"The first architecture constructs a factorised q distribution
qφ(z1:T ,f |x1:T ) = qφ(f |x1:T )",2. The model,[0],[0]
"T∏
t=1
qφ(zt|xt) (3)
",2. The model,[0],[0]
as the amortised variational distribution.,2. The model,[0],[0]
We refer to this as “factorised q” in the experiments section.,2. The model,[0],[0]
This factorization assumes that content features are approximately independent of motion features.,2. The model,[0],[0]
"Furthermore, note that the distribution over content features is conditioned on the entire time series, whereas the dynamical features are only conditioned on the individual frames.
",2. The model,[0],[0]
"The second encoder assumes that the variational posterior of z1:T depends on f , and the q distribution has the following architecture:
qφ(z1:T ,f |x1:T ) = qφ(f |x1:T )qφ(z1:T |f ,x1:T ), (4)
and the distribution q(z1:T |f ,x1:T ) is conditioned on the entire time series.",2. The model,[0],[0]
"It can be implemented by e.g. a bidirectional LSTM (Graves & Schmidhuber, 2005) conditioned on f , followed by an RNN taking the bi-LSTM hidden states as the inputs.",2. The model,[0],[0]
We provide a visualisation of the corresponding computation graph in the appendix.,2. The model,[0],[0]
This encoder is referred to as “full q”.,2. The model,[0],[0]
"The idea behind the structured approximation is that content may affect dynamics: in video, the shape of objects may be informative about their motion patterns, thus z1:T is conditionally dependent on f .",2. The model,[0],[0]
"The architectures of the generative model and both encoders are visualised in Figure 1.
",2. The model,[0],[0]
Unconditional generation.,2. The model,[0],[0]
"After training, one can use the generative model to synthesise video or audio sequences
by sampling the latent variables from the prior and decoding them.",2. The model,[0],[0]
"Furthermore, the proposed generative model allows generation of multiple sequences entailing the same global information (e.g. the same object in a video sequence), simply by fixing f ∼ p(f), sampling different zk1:T ∼ p(z1:T ), k = 1, ...,K, and generating the observations xkt ∼ p(xt|zkt ,f).",2. The model,[0],[0]
"Generating sequences with similar dynamics is done analogously, by fixing z1:T ∼ p(z1:T ) and sampling fk, k = 1, ...K from the prior.
",2. The model,[0],[0]
Conditional generation.,2. The model,[0],[0]
"Together with the encoder, the model also allows conditional generation of sequences.",2. The model,[0],[0]
"As an example, given a video sequence x1:T as reference, one can manipulate the latent variables and generate new sequences preserving either the object identity or the pose/movement information.",2. The model,[0],[0]
"This is done by conditioning on f ∼ q(f |x1:T ) for a given x1:T then randomising z1:T from the prior, or the other way around.
",2. The model,[0],[0]
Feature swapping.,2. The model,[0],[0]
One might also want to generate a new video sequence with the object identity and pose information encoded from different sequence.,2. The model,[0],[0]
"Given two sequences xa1:T and x b 1:T , the synthesis process first infers the latent variables fa ∼ q(f |xa1:T ) and zb1:T ∼ q(z1:T |xb1:T )1, then produces a new sequence by sampling xnewt ∼ p(xt|zbt ,fa).",2. The model,[0],[0]
"This allows us to control both the content and the dynamics of the generated sequence, which can be applied to e.g. conversion of voice of the speaker in a speech sequence.",2. The model,[0],[0]
Research on learning disentangled representation has mainly focused on two aspects: the training objective and the generative model architecture.,3. Related work,[0],[0]
"Regarding the loss function design for VAE models, Higgins et al. (2016) propose the β-VAE by scaling up the KL[q(z|x)||p(z)] term in the variational lower-bound with β > 1 to encourage learning of independent attributes (as the prior p(z) is usually factorised).",3. Related work,[0],[0]
"While the β-VAE has been shown effective in learning better representations for natural images and might be able to further improve the performance of our model, we do not
1For the full q encoder",3. Related work,[0],[0]
"it also requires f b ∼ q(f |xb1:T ).
",3. Related work,[0],[0]
"test this recipe here to demonstrate that disentanglement can be achieved by a careful model design.
",3. Related work,[0],[0]
"For sequence modelling, a number of prior publications have extended VAE to video and speech data (Fabius & van Amersfoort, 2014; Bayer & Osendorfer, 2014; Chung et al., 2015).",3. Related work,[0],[0]
"These models, although being able to generate realistic sequences, do not explicitly disentangle the representation of time-invariant and time-dependent information.",3. Related work,[0],[0]
"Thus it is inconvenient for these models to perform tasks such as controlled generation and feature swapping.
",3. Related work,[0],[0]
"For GAN-like models, both Villegas et al. (2017) and Denton & Birodkar (2017) proposed an auto-encoder architecture for next frame prediction, with two separate encoders responsible for content and pose information at each time step.",3. Related work,[0],[0]
"While in Villegas et al. (2017), the pose information is extracted from the difference between two consecutive frames xt−1 and xt, Denton & Birodkar (2017) directly encoded xt for both pose and content, and further designed a training objective to encourage learning of disentangled representations.",3. Related work,[0],[0]
"On the other hand, Vondrick et al. (2016) used a spatio-temporal convolutional architecture to disentangle a video scene’s foreground from its background.",3. Related work,[0],[0]
"Although it has successfully achieved disentanglement, we note that the time-invariant information in this model is predefined to represent the background, rather than learned from the data automatically.",3. Related work,[0],[0]
"Also this architecture is suitable for video sequences only, unlike our model which can be applied to any type of sequential data.
",3. Related work,[0],[0]
"Very recent work (Hsu et al., 2017) introduced the factorised hierarchical variational auto-encoder (FHVAE) for unsupervised learning of disentangled representation of speech data.",3. Related work,[0],[0]
"Given a speech sequence that has been partitioned into segments {xn1:T }Nn=1, FHVAE models the joint distribution of {xn1:T }Nn=1 and latent variables as follows:
p({xn1:T , zn1 , zn2 },µ2) = p(µ2) N∏
n=1
p(xn1:T , z n 1 , z n 2 |µ2),
p(xn1:T , z n 1 , z n 2 |µ2) = p(zn1 )p(zn2",3. Related work,[0],[0]
"|µ2)p(xn1:T |zn1 , zn2 ).
",3. Related work,[0],[0]
"Here the zn2 variable has a hierarchical prior p(z n 2 |µ2) = N (µ2, σ2I), p(µ2) = N (0, λI).",3. Related work,[0],[0]
"The authors showed that by having different prior structures for zn1 and z n 2 , it allows the model to encode with zn2 speech sequence-level
attributes (e.g. pitch of a speaker), and other residual information with zn1 .",3. Related work,[0],[0]
"A discriminative training objective (see discussions in Section 4.2) is added to the variational lowerbound, which has been shown to further improve the quality of the disentangled representation.",3. Related work,[0],[0]
"Our model can also benefit from the usage of hierarchical prior distributions, e.g. fn ∼ p(f |µ2),µ2 ∼ p(µ2), and we leave the investigation to future work.",3. Related work,[0],[0]
We carried out experiments both on video data (Section 4.1) as well as speech data (Section 4.2).,4. Experiments,[0],[0]
"In both setups, we find strong evidence that our model learns an approximately disentangled representation that allows for conditional generation and feature swapping.",4. Experiments,[0],[0]
We further investigated the efficiency for encoding long sequences with a stochastic transition model in Section 4.3.,4. Experiments,[0],[0]
The detailed model architectures of the networks used in each experiment are reported in the appendix.,4. Experiments,[0],[0]
"We present an initial test of the proposed VAE architecture on a dataset of video game “sprites”, i.e. animated cartoon characters whose clothing, pose, hairstyle, and skin color we can fully control.",4.1. Video sequence: Sprites,[0],[0]
"This dataset comes from an open-source video game project called Liberated Pixel Cup2, and has been also considered in Reed et al. (2015); Mathieu et al. (2016) for image processing experiments.",4.1. Video sequence: Sprites,[0],[0]
"Our experiments show that static attributes such as hair color and clothing are well preserved over time for randomly generated videos.
",4.1. Video sequence: Sprites,[0],[0]
Data and preprocessing.,4.1. Video sequence: Sprites,[0],[0]
"We downloaded and selected the online available sprite sheets3, and organised them into 4 attribute categories (skin color, tops, pants and hairstyle) and 9 action categories (walking, casting spells and slashing, each with three viewing angles).",4.1. Video sequence: Sprites,[0],[0]
"In order to avoid a combinatorial explosion problem, each of the attribute categories contains 6 possible variants (see Figure 2), therefore it leads to 64 = 1296 unique characters in total.",4.1. Video sequence: Sprites,[0],[0]
We used 1000 of them for training/validation and the rest of them for testing.,4.1. Video sequence: Sprites,[0],[0]
The resulting dataset consists of sequences with T = 8 frames of dimension 64× 64.,4.1. Video sequence: Sprites,[0],[0]
Note here we did not use the labels for training the generative model.,4.1. Video sequence: Sprites,[0],[0]
"Instead these labels on the data frames are used to train a classifier that is later deployed to produce quantitative evaluations on the VAE, see below.
",4.1. Video sequence: Sprites,[0],[0]
Qualitative analysis.,4.1. Video sequence: Sprites,[0],[0]
We start with a qualitative evaluation of our VAE architecture.,4.1. Video sequence: Sprites,[0],[0]
"Figure 3 shows both re-
2http://lpc.opengameart.org/",4.1. Video sequence: Sprites,[0],[0]
"3https://github.com/jrconway3/
Universal-LPC-spritesheet
constructed as well as generated video sequences from our model.",4.1. Video sequence: Sprites,[0],[0]
Each panel shows three video sequences with time running from left to right.,4.1. Video sequence: Sprites,[0],[0]
"Panel (a) shows parts of the original data from the test set, and (b) shows its reconstruction.
",4.1. Video sequence: Sprites,[0],[0]
The sequences visualised in panel (c) are generated using zt ∼ q(zt|xt) but f ∼ p(f).,4.1. Video sequence: Sprites,[0],[0]
"Hence, the dynamics are imposed by the encoder, but the identity is sampled from the prior.",4.1. Video sequence: Sprites,[0],[0]
"We see that panel (c) reveals the same motion patterns as (a), but has different character identities.",4.1. Video sequence: Sprites,[0],[0]
"Conversely, in panel (d) we take the identity from the encoder, but sample the dynamics from the prior.",4.1. Video sequence: Sprites,[0],[0]
"Panel (d) reveals the same characters as (a), but different motion patterns.
",4.1. Video sequence: Sprites,[0],[0]
Panels (e) and (f) focus on feature swapping.,4.1. Video sequence: Sprites,[0],[0]
"In (e), the frames are constructed by computing zt ∼ q(zt|xt) on one input sequence but f encoded on another input sequence.",4.1. Video sequence: Sprites,[0],[0]
"These panels demonstrate that the encoder and the decoder have learned a factored representation for content and pose.
",4.1. Video sequence: Sprites,[0],[0]
"Panels (g) and (h) focus on conditional generation, showing randomly generated sequences that share the same f or z1:T samples from the prior.",4.1. Video sequence: Sprites,[0],[0]
"Thus, in panel (g) we see the same character performing different actions, and in (h) different characters performing the same motion.",4.1. Video sequence: Sprites,[0],[0]
"This again illustrates that the prior model disentangles the representation.
",4.1. Video sequence: Sprites,[0],[0]
Quantitative analysis.,4.1. Video sequence: Sprites,[0],[0]
"Next we perform quantitative evaluations of the generative model, using a classifier trained on the labelled frames.",4.1. Video sequence: Sprites,[0],[0]
"Empirically, we find that the fully factorized and structured inference networks produce almost identical results here, presumably because in this dataset the object identity and pose information are truly independent.",4.1. Video sequence: Sprites,[0],[0]
"Therefore we only report results on the fully factorised q distribution case.
",4.1. Video sequence: Sprites,[0],[0]
The first evaluation task considers reconstructing the test sequences with encoded f and randomly sampled zt (in the same way as to produce panel (d) in Figure 3).,4.1. Video sequence: Sprites,[0],[0]
Then we compare the classifier outputs on both the original frames and the reconstructed frames.,4.1. Video sequence: Sprites,[0],[0]
"If the character’s identity is preserved over time, the classifier should produce identical probability vectors on the data frames and the reconstructed frames (denoted as pdata and precon respectively).
",4.1. Video sequence: Sprites,[0],[0]
We evaluate the similarity between the original and reconstructed sequences both in terms of the disagreement of the predicted class labels maxi[precon(i)] 6= maxi[pdata(i)] and the KL-divergence KL[precon||pdata].,4.1. Video sequence: Sprites,[0],[0]
We also compute the two metrics on the action predictions using reconstructed sequences with randomised f and inferred zt.,4.1. Video sequence: Sprites,[0],[0]
The results in Table 1 indicate that the learned representation is indeed factorised.,4.1. Video sequence: Sprites,[0],[0]
"For example, in the fix-f generation test, only 4% out of 296× 9 data-reconstruction frame pairs contain characters whose generated skin color differs from the rest, where in the case of hairstyle preservation the disagreement rate is only 0.06%.",4.1. Video sequence: Sprites,[0],[0]
The KL metric is also much smaller than the KL-divergence KL[prandom||pdata],4.1. Video sequence: Sprites,[0],[0]
"where prandom = (1/Nclass, ..., 1/Nclass), indicating that our result is significant.
",4.1. Video sequence: Sprites,[0],[0]
"In the second evaluation, we test whether static attributes of generated sequences, such as clothing or hair style, are preserved over time.",4.1. Video sequence: Sprites,[0],[0]
"We sample 200 video sequences from the generator, using the same f but different latent dynamics z1:T .",4.1. Video sequence: Sprites,[0],[0]
We use the trained classifier to predict both the attributes and the action classes for each of the generated frames.,4.1. Video sequence: Sprites,[0],[0]
"Results are shown in Figure 4(a), where we plot the prediction of the classifiers for each frame over time.",4.1. Video sequence: Sprites,[0],[0]
"For example, the trajectory curve in the “skin color” panel in Figure 4(a) corresponds to the skin color attribute classification results for frames x1:T of a generated video sequence.",4.1. Video sequence: Sprites,[0],[0]
"We repeat this process 5 times with different f samples,
where each f corresponds to one color.
",4.1. Video sequence: Sprites,[0],[0]
"It becomes evident that those lines with the same color are clustered together, confirming that f mainly controls the generation of time-invariant attributes.",4.1. Video sequence: Sprites,[0],[0]
"Also, most character attributes are preserved over time, e.g. for the attribute “tops”, the trajectories are mostly straight lines.",4.1. Video sequence: Sprites,[0],[0]
"However, some of the trajectories for the attributes drift away from the majority class.",4.1. Video sequence: Sprites,[0],[0]
"We conjecture that this is due of the mass-covering behaviour of (approximate) maximum likelihood training, which makes the trained model generate characters that do not exist in the dataset.",4.1. Video sequence: Sprites,[0],[0]
"Indeed the middle row of panel (c) in Figure 3 contains a character with an unseen hairstyle, showing that our model is able to generalise beyond the training set.",4.1. Video sequence: Sprites,[0],[0]
"On the other hand, the sampling process returns sequences with diverse actions as depicted in the action panel, meaning that f contains little information regarding the video dynamics.
",4.1. Video sequence: Sprites,[0],[0]
"We performed similar tests on sequence generations with shared latent dynamics z1:T but different f , shown in Figure 4(b).",4.1. Video sequence: Sprites,[0],[0]
"The experiment is repeated 5 times as well, and again trajectories with the same color encoding correspond to videos generated with the same z1:T (but different f ).",4.1. Video sequence: Sprites,[0],[0]
Here we also observe diverse trajectories for the attribute categories.,4.1. Video sequence: Sprites,[0],[0]
"In contrast, the characters’ actions are mostly the same.",4.1. Video sequence: Sprites,[0],[0]
These two test results again indicate that the model has successfully learned disentangled representations of character identities and actions.,4.1. Video sequence: Sprites,[0],[0]
"Interestingly we observe multi-modalities in the action domain for the generated sequences, e.g. the trajectories in the action panel of Figure 4(b) are jumping between different levels.",4.1. Video sequence: Sprites,[0],[0]
We also visualise in Figure 5 generated sequences of the “turning” action that is not present in the dataset.,4.1. Video sequence: Sprites,[0],[0]
It again shows that the trained model generalises to unseen cases.,4.1. Video sequence: Sprites,[0],[0]
We also experiment on audio sequence data.,4.2. Speech data: TIMIT,[0],[0]
Our disentangled representation allows us to convert speaker identities into each other while conditioning on the content of the speech.,4.2. Speech data: TIMIT,[0],[0]
"We also show that our model gives rise to speaker verification, where we outperform a recent probabilistic baseline model.
(a) Trajectory plots on the generated sequences with shared f .
(b) Trajectory plots on the generated sequences with shared z1:T .
",4.2. Speech data: TIMIT,[0],[0]
Figure 4.,4.2. Speech data: TIMIT,[0],[0]
"Classification test on the generated video sequences with shared f (top) or shared z1:T (bottom), respectively.",4.2. Speech data: TIMIT,[0],[0]
The experiment is repeated 5 times and depicted by different color coding.,4.2. Speech data: TIMIT,[0],[0]
"The x and y axes are time and the class id of the attributes, respectively.
",4.2. Speech data: TIMIT,[0],[0]
Figure 5.,4.2. Speech data: TIMIT,[0],[0]
Visualising multi-modality in action space.,4.2. Speech data: TIMIT,[0],[0]
"In this case the characters turn from left to right, and this action sequence is not observed in data.
Data and preprocessing.",4.2. Speech data: TIMIT,[0],[0]
"The TIMIT data (Garofolo et al., 1993) contains broadband 16kHz recordings of phonetically-balanced read speech.",4.2. Speech data: TIMIT,[0],[0]
A total of 6300 utterances (5.4 hours) are presented with 10 sentences from each of the 630 speakers (70% male and 30% female).,4.2. Speech data: TIMIT,[0],[0]
"We follow Hsu et al. (2017) for data pre-processing: the raw speech waveforms are first split into sub-sequences of 200ms, and then preprocessed with sparse fast Fourier transform to obtain a 200 dimensional log-magnitude spectrum, computed every 10ms.",4.2. Speech data: TIMIT,[0],[0]
"This implies T = 20 for the observation x1:T .
",4.2. Speech data: TIMIT,[0],[0]
Qualitative analysis.,4.2. Speech data: TIMIT,[0],[0]
We perform voice conversion experiments to demonstrate the disentanglement of the learned representation.,4.2. Speech data: TIMIT,[0],[0]
The goal here is to convert male voice to female voice (and vice versa) with the speech content being preserved.,4.2. Speech data: TIMIT,[0],[0]
"Assuming that f has learned the representation of speaker’s identity, the conversion can be done by first encoding two sequences xmale1:T and x female 1:T with q to obtain representations {fmale, zmale1:T } and {f female, zfemale1:T }, then construct the converted sequence by feeding f female and zmale1:T to the decoder p(xt|zt,f).",4.2. Speech data: TIMIT,[0],[0]
Figure 6 shows the reconstructed spectrogram after the swapping process of the f features.,4.2. Speech data: TIMIT,[0],[0]
"We also provide the reconstructed speech waveforms using the Griffin-Lim algorithm (Griffin & Lim, 1984) in the appendix.
",4.2. Speech data: TIMIT,[0],[0]
The experiments show that the harmonics of the converted speech sequences shifted to higher frequency in the “male to female” test and vice versa.,4.2. Speech data: TIMIT,[0],[0]
"Also the pitch (the red arrow in Figure 6 indicating the fundamental frequency, i.e. the first harmonic) of the converted sequence (b) is close to the pitch of (c), same as for the comparison between (d) and (a).",4.2. Speech data: TIMIT,[0],[0]
"By an informal listening test of the speech sequence pairs (a, d) and (b, c), we confirm that the speech content is preserved.",4.2. Speech data: TIMIT,[0],[0]
"These results show that our model is successfully applied to speech sequences for learning disentangled representations.
",4.2. Speech data: TIMIT,[0],[0]
Quantitative analysis.,4.2. Speech data: TIMIT,[0],[0]
We further follow Hsu et al. (2017) to use speaker verification for quantitative evaluation.,4.2. Speech data: TIMIT,[0],[0]
"Speaker verification is the process of verifying the claimed identity of a speaker, usually by comparing the “features” wtest of the test utterance xtest1:",4.2. Speech data: TIMIT,[0],[0]
T1 with those of the target utterance xtarget1:T2 from the claimed identity.,4.2. Speech data: TIMIT,[0],[0]
"The claimed identity is confirmed if the cosine similarity cos(wtest,wtarget) is grater than a given threshold (Dehak et al., 2009).",4.2. Speech data: TIMIT,[0],[0]
By varying ∈,4.2. Speech data: TIMIT,[0],[0]
"[0, 1], we report the verification performance in terms of equal error rate (EER), where the false rejection rate equals the false acceptance rate.
",4.2. Speech data: TIMIT,[0],[0]
The extraction of the “features” is crucial for the performance of this speaker verification system.,4.2. Speech data: TIMIT,[0],[0]
"Given a speech sequence containing N segments {x(n)1:T }Nn=1, we constructed two types of “features”, one by computing µf as the mean
of q(f (n)|x(n)1:T ) across the segments, and the other by extracting the mean µzt of q(zt|x1:T ) and averaging them across both time T and segments.",4.2. Speech data: TIMIT,[0],[0]
"In formulas,
µf = 1
N N∑ n=1 µfn , µfn = Eq(fn|xn1:T )",4.2. Speech data: TIMIT,[0],[0]
"[f n],
µz = 1
TN T∑ t=1 N∑ n=1 µznt , µznt = Eq(znt |xn1:T )",4.2. Speech data: TIMIT,[0],[0]
"[z n t ].
We also include two baseline results from Hsu et al. (2017): one used the i-vector method (Dehak et al., 2011) for feature extraction, and the other one used µ1 and µ2 (analogous to µz and µf in our case) from a trained FHVAE model on Mel-scale filter bank (FBank) features.
",4.2. Speech data: TIMIT,[0],[0]
"The test data were created from the test set of TIMIT, containing 24 unique speakers and 18,336 pairs for verification.",4.2. Speech data: TIMIT,[0],[0]
Table 2 presents the EER results of the proposed model and baselines.4,4.2. Speech data: TIMIT,[0],[0]
"It is clear that the µf feature performs significantly better than the i-vector method, indicating that the f variable has learned to represent a speaker’s identity.",4.2. Speech data: TIMIT,[0],[0]
"On the other hand, using µz as the features returns considerably worse EER rates compared to the i-vector method and µf feature.",4.2. Speech data: TIMIT,[0],[0]
"This is good, as it indicates that the z variables contain less information about the speaker’s identity, again validating the success of disentangling time-variant and time-independent information.",4.2. Speech data: TIMIT,[0],[0]
"Note that the EER results for µz get worse when using the full q encoder, and in the 64 dimensional feature case the verification performance of µf improves slightly.",4.2. Speech data: TIMIT,[0],[0]
"This also shows that for real-world data it is useful to use a structured inference network to further improve the quality of disentangled representation.
",4.2. Speech data: TIMIT,[0],[0]
Our results are competitive with (or slightly better than) the FHVAE results (α = 0) reported in Hsu et al. (2017).,4.2. Speech data: TIMIT,[0],[0]
The better results for FHVAE (α = 10) is obtained by adding a discriminative training objective (scaled by α) to the variational lower-bound.,4.2. Speech data: TIMIT,[0],[0]
"In a nutshell, the timeinvariant information in FHVAE is encoded in a latent variable zn2 ∼ p(zn2 |µ2), and the discriminative objective encourages zn2 encoded from a segment of one sequence to be close to the corresponding µ2 while far away from µ2 of other sequences.",4.2. Speech data: TIMIT,[0],[0]
"However, we do not test this idea here because (1) our goal is to demonstrate that the proposed architecture is a minimalistic framework for learning disentangled representations of sequential data; (2) this discriminative objective is specifically designed for hierarchical VAE, and in general the assumption behind it might not always be true (consider encoding two speech sequences coming from the same speaker).",4.2. Speech data: TIMIT,[0],[0]
"Similar ideas for discriminative training have been considered in e.g. Mathieu et al. (2016), but that discriminative objective can only be applied
4 Hsu et al. (2017) did not provide the EER results for α = 0 and µ1 in the 16 dimension case.
to two sequences that are known to entail different timeinvariant information (e.g. two sequences with different labels), which implicitly uses supervisions.",4.2. Speech data: TIMIT,[0],[0]
"Nevertheless, a better design for the discriminative objective without supervision can further improve the disentanglement of the learned representations, and we leave it to future work.",4.2. Speech data: TIMIT,[0],[0]
"Lastly, although not a main focus of the paper, we show that the usage of a stochastic transition model for the prior leads to more realistic dynamics of the generated sequence.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"For comparison, we consider another class of models:
p(x1:T , z,f) = p(f)p(z) T∏ t=1 p(xt|z,f).
",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"The parameters of p(xt|z,f) are defined by a neural network NN(ht,f), with ht computed by a deterministic RNN conditioned on",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"z. We experiment with two types of deterministic dynamics, with the graphical model visualised in appendix.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"The first model uses an LSTM with z as the initial state: h0 = z, ht = LSTM(ht−1).",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
In later experiments we refer this dynamics as LSTM-f as the latent variable z is forward propagated in a deterministic way.,4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"The second one deploys an LSTM conditioned on z (i.e. h0 = 0,ht = LSTM(ht−1, z)), therefore we refer it as LSTM-c. This is identical to the transition dynamics used in the FHVAE model (Hsu et al., 2017).",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"For comparison, we refer to our model as the ’stochastic’ model (Eq. 1).
",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"The LSTM models encodes temporal information in a global latent variable z. Therefore, small differences/errors in z will accumulate over time, which may result in unrealistic long-time dynamics.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"In contrast, the stochastic model (Eq. 1) keeps track of the time-varying aspects of xt in zt
for every t, making the reconstruction to be time-local and therefore much easier.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"Therefore, the stochastic model is better suited if the sequences are long and complex.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"We give empirical evidence to support this claim.
",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
Data preprocessing & hyper-parameters.,4.3. Comparing stochastic & deterministic dynamics,[0],[0]
We follow Fraccaro et al. (2017) to simulate video sequences of a ball (or a square) bouncing inside an irregular polygon using Pymunk.5,4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"The irregular shape was chosen because it induces chaotic dynamics, meaning that small deviations from the initial position and velocity of the ball will create exponentially diverging trajectories at long times.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
This makes memorizing the dynamics of a prototypical sequence challenging.,4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"We randomly sampled the initial position and velocity of the ball, but did not apply any force to the ball, except for the fully elastic collisions with the walls.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"We generated 5,000 sequences in total (1000 for test), each of them containing T = 30 frames with a resolution of 32×32.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"For the deterministic LSTMs, we fix the dimensionality of zt to 64, and set ht and the LSTM internal states to be 512 dimensions.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"The latent variable dimensionality of the stochastic dynamics is dim(zt) = 16.
",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
Qualitative & quantitative analyses.,4.3. Comparing stochastic & deterministic dynamics,[0],[0]
We consider both reconstruction and missing data imputation tasks for the learned generative models.,4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"For the latter and for T = 30, the models observe the first t < T frames of a sequence and predict the remaining T − t frames using the prior dynamics.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"We visualise in Figure 7 the ground truth, recon-
5http://www.pymunk.org/en/latest/.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"For simplicity we disabled rotation of the square when hitting the wall, by setting the inertia to infinity.
",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"structed, and predicted sequences (t = 20) from all models.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"We further consider average fraction of incorrectly reconstructed/predicted pixels as a quantitative metric, to evaluate how well the ground-truth dynamics is recovered given consecutive missing frames.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
The result is reported in Figure 8.,4.3. Comparing stochastic & deterministic dynamics,[0],[0]
The stochastic model outperforms the deterministic models both qualitatively and quantitatively.,4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"The shape of the ball is better preserved over time, and the trajectories are more physical.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"This explains the lower errors of the stochastic model, and the advantage is significant when the number of missing frames is small.
",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"Our experiments give evidence that the stochastic model is better suited to modelling long, complex sequences when compared to the deterministic dynamical models.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"We expect that a better design for the stochastic transition dynamics, e.g. by combining deep neural networks with well-studied linear dynamical systems (Krishnan et al., 2015; Fraccaro et al., 2016; Karl et al., 2016; Johnson et al., 2016; Krishnan et al., 2017; Fraccaro et al., 2017), can further enhance the quality of the learned representations.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
We presented a minimalistic generative model for learning disentangled representations of high-dimensional time series.,5. Conclusions and outlook,[0],[0]
"Our model consists of a global latent variable for content features, and a stochastic RNN with time-local latent variables for dynamical features.",5. Conclusions and outlook,[0],[0]
The model is trained using standard amortized variational inference.,5. Conclusions and outlook,[0],[0]
We carried out experiments both on video and audio data.,5. Conclusions and outlook,[0],[0]
"Our approach allows us to perform full and conditional generation, as well as feature swapping, such as voice conversion and video content manipulation.",5. Conclusions and outlook,[0],[0]
"We also showed that a stochastic transition model generally outperforms a deterministic one.
",5. Conclusions and outlook,[0],[0]
Future work may investigate whether a similar model applies to more complex video and audio sequences.,5. Conclusions and outlook,[0],[0]
"Also, disentangling may further be improved by additional crossentropy terms, or discriminative training.",5. Conclusions and outlook,[0],[0]
A promising avenue of research is to explore the usage of this architecture for neural compression.,5. Conclusions and outlook,[0],[0]
"An advantage of the model is that it separates dynamical from static features, allowing the latent space for the dynamical part to be low-dimensional.",5. Conclusions and outlook,[0],[0]
"We thank Robert Bamler, Rich Turner, Jeremy Wong and Yu Wang for discussions and feedback on the manuscript.",Acknowledgements,[0],[0]
We also thank Wei-Ning Hsu for helping reproduce the FHVAE experiments.,Acknowledgements,[0],[0]
Yingzhen Li thanks Schlumberger Foundation FFTF fellowship for supporting her PhD study.,Acknowledgements,[0],[0]
"We present a VAE architecture for encoding and generating high dimensional sequential data, such as video or audio.",abstractText,[0],[0]
"Our deep generative model learns a latent representation of the data which is split into a static and dynamic part, allowing us to approximately disentangle latent time-dependent features (dynamics) from features which are preserved over time (content).",abstractText,[0],[0]
This architecture gives us partial control over generating content and dynamics by conditioning on either one of these sets of features.,abstractText,[0],[0]
"In our experiments on artificially generated cartoon video clips and voice recordings, we show that we can convert the content of a given sequence into another one by such content swapping.",abstractText,[0],[0]
"For audio, this allows us to convert a male speaker into a female speaker and vice versa, while for video we can separately manipulate shapes and dynamics.",abstractText,[0],[0]
"Furthermore, we give empirical evidence for the hypothesis that stochastic RNNs as latent state models are more efficient at compressing and generating long sequences than deterministic ones, which may be relevant for applications in video compression.",abstractText,[0],[0]
Disentangled Sequential Autoencoder,title,[0],[0]
"Proceedings of NAACL-HLT 2013, pages 820–825, Atlanta, Georgia, 9–14 June 2013. c©2013 Association for Computational Linguistics
In this paper, we propose a multi-step stacked learning model for disfluency detection. Our method incorporates refined n-gram features step by step from different word sequences. First, we detect filler words. Second, edited words are detected using n-gram features extracted from both the original text and filler filtered text. In the third step, additional n-gram features are extracted from edit removed texts together with our newly induced in-between features to improve edited word detection. We use Max-Margin Markov Networks (M3Ns) as the classifier with the weighted hamming loss to balance precision and recall. Experiments on the Switchboard corpus show that the refined n-gram features from multiple steps and M3Ns with weighted hamming loss can significantly improve the performance. Our method for disfluency detection achieves the best reported F-score 0.841 without the use of additional resources.1",text,[0],[0]
"Detecting disfluencies in spontaneous speech can be used to clean up speech transcripts, which helps improve readability of the transcripts and make it easy for downstream language processing modules.",1 Introduction,[0],[0]
"There are two types of disfluencies: filler words including filled pauses (e.g., ‘uh’, ‘um’) and discourse markers (e.g., ‘I mean’, ‘you know’), and edited words that are repeated, discarded, or corrected by
1Our source code is available at http://code.google.com/p/disfluency-detection/downloads/list
the following words.",1 Introduction,[0],[0]
"An example is shown below that includes edited words and filler words.
",1 Introduction,[0],[0]
"I want a flight to Boston︸ ︷︷ ︸ edited uh I mean︸ ︷︷ ︸ filler to Denver
Automatic filler word detection is much more accurate than edit detection as they are often fixed phrases (e.g., “uh”, “you know”, “I mean”), hence our work focuses on edited word detection.
",1 Introduction,[0],[0]
Many models have been evaluated for this task.,1 Introduction,[0],[0]
Liu et al. (2006) used Conditional Random Fields (CRFs) for sentence boundary and edited word detection.,1 Introduction,[0],[0]
They showed that CRFs significantly outperformed Maximum Entropy models and HMMs.,1 Introduction,[0],[0]
"Johnson and Charniak (2004) proposed a TAGbased noisy channel model which showed great improvement over boosting based classifier (Charniak and Johnson, 2001).",1 Introduction,[0],[0]
Zwarts and Johnson (2011) extended this model using minimal expected F-loss oriented n-best reranking.,1 Introduction,[0],[0]
They obtained the best reported F-score of 83.8% on the Switchboard corpus.,1 Introduction,[0],[0]
"Georgila (2009) presented a post-processing method during testing based on Integer Linear Programming (ILP) to incorporate local and global constraints.
",1 Introduction,[0],[0]
"From the view of features, in addition to textual information, prosodic features extracted from speech have been incorporated to detect edited words in some previous work (Kahn et al., 2005; Zhang et al., 2006; Liu et al., 2006).",1 Introduction,[0],[0]
"Zwarts and Johnson (2011) trained an extra language model on additional corpora, and used output log probabilities of language models as features in the reranking stage.",1 Introduction,[0],[0]
"They reported that the language model gained about absolute 3% F-score for edited word detection on the Switchboard development dataset.
820
In this paper, we propose a multi-step stacked learning approach for disfluency detection.",1 Introduction,[0],[0]
"In our method, we first perform filler word detection, then edited word detection.",1 Introduction,[0],[0]
"In every step, we generate new refined n-gram features based on the processed text (remove the detected filler or edited words from the previous step), and use these in the next step.",1 Introduction,[0],[0]
"We also include a new type of features, called inbetween features, and incorporate them into the last step.",1 Introduction,[0],[0]
"For edited word detection, we use Max-Margin Markov Networks (M3Ns) with weighted hamming loss as the classifier, as it can well balance the precision and recall to achieve high performance.",1 Introduction,[0],[0]
"On the commonly used Switchboard corpus, we demonstrate that our proposed method outperforms other state-of-the-art systems for edit disfluency detection.",1 Introduction,[0],[0]
"Weighted M3Ns
We use a sequence labeling model for edit detection.",2 Balancing Precision and Recall Using,[0],[0]
"Each word is assigned one of the five labels: BE (beginning of the multi-word edited region), IE (in the edited region), EE (end of the edited region), SE (single word edited region), O (other).",2 Balancing Precision and Recall Using,[0],[0]
"For example, the previous sentence is represented as:
I/",2 Balancing Precision and Recall Using,[0],[0]
O want/O a/O flight/,2 Balancing Precision and Recall Using,[0],[0]
O to/BE Boston/EE uh/,2 Balancing Precision and Recall Using,[0],[0]
O I/,2 Balancing Precision and Recall Using,[0],[0]
"O mean/O to/O Denver/O
We use the F-score as the evaluation metrics (Zwarts and Johnson, 2011; Johnson and Charniak, 2004), which is defined as the harmonic mean of the precision and recall of the edited words:
P = #correctly predicted edited words
#predicted edited words
R = #correctly predicted edited words
#gold standard edited words
F = 2× P ×R
P + R
There are many methods to train the sequence model, such as CRFs (Lafferty et al., 2001), averaged structured perceptrons (Collins, 2002), structured SVM (Altun et al., 2003), online passive aggressive learning (Crammer et al., 2006).",2 Balancing Precision and Recall Using,[0],[0]
"Previous work has shown that minimizing F-loss is more effective than minimizing log-loss (Zwarts and Johnson, 2011), because edited words are much fewer than normal words.
",2 Balancing Precision and Recall Using,[0],[0]
"In this paper, we use Max-margin Markov Networks (Taskar et al., 2004) because our preliminary
results showed that they outperform other classifiers, and using weighted hamming loss is simple in this approach (whereas for perceptron or CRFs, the modification of the objective function is not straightforward).
",2 Balancing Precision and Recall Using,[0],[0]
"The learning task for M3Ns can be represented as follows:
min α
1 2 C∥ ∑ x,y αx,y∆f(x, y)∥22 + ∑ x,y αx,yL(x, y)
s.t. ∑
y
αx,y = 1 ∀x
αx,y ≥ 0, ∀x, y
The above shows the dual form for training M3Ns, where x is the observation of a training sample, y ∈ Y is a label.",2 Balancing Precision and Recall Using,[0],[0]
"α is the parameter needed to be optimized, C > 0 is the regularization parameter.",2 Balancing Precision and Recall Using,[0],[0]
"∆f(x, y) is the residual feature vector: f(x, ỹ)",2 Balancing Precision and Recall Using,[0],[0]
"− f(x, y), where ỹ is the true label of x. L(x, y) is the loss function.",2 Balancing Precision and Recall Using,[0],[0]
"Taskar et al. (2004) used un-weighted hamming loss, which is the number of incorrect components: L(x, y) = ∑ t δ(yt, ỹt), where δ(a, b) is the binary indicator function (it is 0 if a = b).",2 Balancing Precision and Recall Using,[0],[0]
"In our work, we use the weighted hamming loss:
L(x, y) = ∑
t
v(yt, ỹt)δ(yt, ỹt)
where v(yt, ỹt) is the weighted loss for the error when ỹt is mislabeled as yt.",2 Balancing Precision and Recall Using,[0],[0]
Such a weighted loss function allows us to balance the model’s precision and recall rates.,2 Balancing Precision and Recall Using,[0],[0]
"For example, if we assign a large value to v(O, ·E) (·E denotes SE, BE, IE, EE), then the classifier is more sensitive to false negative errors (edited word misclassified as non-edited word), thus we can improve the recall rate.",2 Balancing Precision and Recall Using,[0],[0]
"In our work, we tune the weight matrix v using the development dataset.",2 Balancing Precision and Recall Using,[0],[0]
"Rather than just using the above M3Ns with some features, in this paper we propose to use stacked learning to incorporate gradually refined n-gram features.",3 Multi-step Stacked Learning for Edit Disfluency Detection,[0],[0]
"Stacked learning is a meta-learning approach (Cohen and de Carvalho, 2005).",3 Multi-step Stacked Learning for Edit Disfluency Detection,[0],[0]
"Its idea is to use two
(or more) levels of predictors, where the outputs of the low level predictors are incorporated as features into the next level predictors.",3 Multi-step Stacked Learning for Edit Disfluency Detection,[0],[0]
It has the advantage of incorporating non-local features as well as nonlinear classifiers.,3 Multi-step Stacked Learning for Edit Disfluency Detection,[0],[0]
"In our task, we do not just use the classifier’s output (a word is an edited word or not) as a feature, rather we use such output to remove the disfluencies and extract new n-gram features for the subsequent stacked classifiers.",3 Multi-step Stacked Learning for Edit Disfluency Detection,[0],[0]
We use 10 fold cross validation to train the low level predictors.,3 Multi-step Stacked Learning for Edit Disfluency Detection,[0],[0]
The following describes the three steps in our approach.,3 Multi-step Stacked Learning for Edit Disfluency Detection,[0],[0]
"In the first step, we automatically detect filler words.",3.1 Step 1: Filler Word Detection,[0],[0]
"Since filler words often occur immediately after edited words (before the corrected words), we expect that removing them will make rough copy detection easy.",3.1 Step 1: Filler Word Detection,[0],[0]
"For example, in the previous example shown in Section 1, if “uh I mean” is removed, then the reparandum “to Boston” and repair “to Denver” will be adjacent and we can use word/POS based ngram features to detect that disfluency.",3.1 Step 1: Filler Word Detection,[0],[0]
"Otherwise, the classifier needs to skip possible filler words to find the rough copy of the reparandum.
",3.1 Step 1: Filler Word Detection,[0],[0]
"For filler word detection, similar to edited word detection, we define 5 labels: BP , IP , EP , SP , O. We use un-weighted hamming loss to learn M3Ns for this task.",3.1 Step 1: Filler Word Detection,[0],[0]
"Since for filler word detection, our performance metric is not F-measure, but just the overall accuracy in order to generate cleaned text for subsequent n-gram features, we did not use the weighted hamming hoss for this.",3.1 Step 1: Filler Word Detection,[0],[0]
The features we used are listed in Table 1.,3.1 Step 1: Filler Word Detection,[0],[0]
All n-grams are extracted from the original text.,3.1 Step 1: Filler Word Detection,[0],[0]
"In the second step, edited words are detected using M3Ns with the weighted-hamming loss.",3.2 Step 2: Edited Word Detection,[0],[0]
The features we used are listed in Table 2.,3.2 Step 2: Edited Word Detection,[0],[0]
All n-grams in the first step are also used here.,3.2 Step 2: Edited Word Detection,[0],[0]
"Besides that, word n-grams, POS n-grams and logic",3.2 Step 2: Edited Word Detection,[0],[0]
n,3.2 Step 2: Edited Word Detection,[0],[0]
-grams extracted from filler word removed text are included.,3.2 Step 2: Edited Word Detection,[0],[0]
"Feature templates I(w0, w′i) is to generate features detecting rough copies separated by filler words.",3.2 Step 2: Edited Word Detection,[0],[0]
"In this step, we use n-gram features extracted from the text after removing edit disfluencies based on
the previous step.",3.3 Step 3: Refined Edited Word Detection,[0],[0]
"According to our analysis of the errors produced by step 2, we observed that many errors occurred at the boundaries of the disfluencies, and the word bigrams after removing the edited words are unnatural.",3.3 Step 3: Refined Edited Word Detection,[0],[0]
"The following is an example:
• Ref: The new type is prettier than what their/SE they used to look like.
",3.3 Step 3: Refined Edited Word Detection,[0],[0]
"• Sys: The new type is prettier than what/BE their/EE they used to look like.
",3.3 Step 3: Refined Edited Word Detection,[0],[0]
"Using the system’s prediction, we would have bigram than they, which is odd.",3.3 Step 3: Refined Edited Word Detection,[0],[0]
"Usually, the pronoun following than is accusative case.",3.3 Step 3: Refined Edited Word Detection,[0],[0]
We expect adding n-gram features derived from the cleaned-up sentences would allow the new classifier to fix such hypothesis.,3.3 Step 3: Refined Edited Word Detection,[0],[0]
"This kind of n-gram features is similar to the language models used in (Zwarts and Johnson,
2011).",3.3 Step 3: Refined Edited Word Detection,[0],[0]
"They have the benefit of measuring the fluency of the cleaned text.
",3.3 Step 3: Refined Edited Word Detection,[0],[0]
"Another common error we noticed is caused by the ambiguities of coordinates, because the coordinates have similar patterns as rough copies.",3.3 Step 3: Refined Edited Word Detection,[0],[0]
"For example,
• Coordinates: they ca n′t decide which are the good aspects and which are the bad aspects
• Rough Copies: it/",3.3 Step 3: Refined Edited Word Detection,[0],[0]
"BE ′s/IE a/IE pleasure/IE to/EE it s good to get outside
To distinguish the rough copies and the coordinate examples shown above, we analyze the training data statistically.",3.3 Step 3: Refined Edited Word Detection,[0],[0]
We extract all the pieces lying between identical word bigrams AB . . .,3.3 Step 3: Refined Edited Word Detection,[0],[0]
AB.,3.3 Step 3: Refined Edited Word Detection,[0],[0]
The observation is that coordinates are often longer than edited sequences.,3.3 Step 3: Refined Edited Word Detection,[0],[0]
Hence we introduce the in-between features for each word.,3.3 Step 3: Refined Edited Word Detection,[0],[0]
"If a word lies between identical word bigrams, then its in-between feature is the log length of the subsequence lying between the two bigrams; otherwise, it is zero (we use log length to avoid sparsity).",3.3 Step 3: Refined Edited Word Detection,[0],[0]
We also used other patterns such as A . . .,3.3 Step 3: Refined Edited Word Detection,[0],[0]
A and ABC . . .,3.3 Step 3: Refined Edited Word Detection,[0],[0]
"ABC, but they are too noisy or infrequent and do not yield much performance gain.
",3.3 Step 3: Refined Edited Word Detection,[0],[0]
Table 3 lists the feature templates used in this last step.,3.3 Step 3: Refined Edited Word Detection,[0],[0]
"We use the Switchboard corpus in our experiment, with the same train/develop/test split as the previous work (Johnson and Charniak, 2004).",4.1 Experimental Setup,[0],[0]
"We also remove the partial words and punctuation from the training and test data for the reason to simulate the situation when speech recognizers are used and
such kind of information is not available (Johnson and Charniak, 2004).
",4.1 Experimental Setup,[0],[0]
We tuned the weight matrix for hamming loss on the development dataset using simple grid search.,4.1 Experimental Setup,[0],[0]
"The diagonal elements are fixed at 0; for false positive errors, O → ·E (non-edited word mis-labeled as edited word), their weights are fixed at 1; for false negative errors, ·E → O, we tried the weight from 1 to 3, and increased the weight 0.5 each time.",4.1 Experimental Setup,[0],[0]
The optimal weight matrix is shown in Table 4.,4.1 Experimental Setup,[0],[0]
"Note that we use five labels in the sequence labeling task; however, for edited word detection evaluation, it is only a binary task, that is, all of the words labeled with ·E will be mapped to the class of edited words.",4.1 Experimental Setup,[0],[0]
"We compare several sequence labeling models: CRFs, structured averaged perceptron (AP), M3Ns with un-weighted/weighted loss, and online passiveaggressive (PA) learning.",4.2 Results,[0],[0]
"For each model, we tuned the parameters on the development data:",4.2 Results,[0],[0]
"Gaussian prior for CRFs is 1.0, iteration number for AP is 10, iteration number and regularization penalty for PA are 10 and 1.",4.2 Results,[0],[0]
"For M3Ns, we use Structured Sequential Minimal Optimization (Taskar, 2004) for model training.",4.2 Results,[0],[0]
"Regularization penalty is C = 0.1 and iteration number is 30.
",4.2 Results,[0],[0]
Table 5 shows the results using different models and features.,4.2 Results,[0],[0]
The baseline models use only the ngrams features extracted from the original text.,4.2 Results,[0],[0]
"We can see that M3Ns with the weighted hamming loss achieve the best performance, outperforming all the other models.",4.2 Results,[0],[0]
"Regarding the features, the gradually added n-gram features have consistent improvement for all models.",4.2 Results,[0],[0]
"Using the weighted hamming loss in M3Ns, we observe a gain of 2.2% after deleting filler words, and 1.8% after deleting edited words.",4.2 Results,[0],[0]
"In our analysis, we also noticed that the in-between fea-
tures yield about 1% improvement in F-score for all models (the gain of step 3 over step 2 is because of the in-between features and the new n-gram features extracted from the text after removing previously detected edited words).",4.2 Results,[0],[0]
"We performed McNemar’s test to evaluate the significance of the difference among various methods, and found that when using the same features, weighted M3Ns significantly outperforms all the other models (p value < 0.001).",4.2 Results,[0],[0]
"There are no significant differences among CRFs, AP and PA.",4.2 Results,[0],[0]
"Using recovered n-gram features and inbetween features significantly improves all sequence labeling models (p value < 0.001).
",4.2 Results,[0],[0]
"We also list the state-of-the-art systems evaluated on the same dataset, as shown in Table 6.",4.2 Results,[0],[0]
We achieved the best F-score.,4.2 Results,[0],[0]
"The most competitive system is (Zwarts and Johnson, 2011), which uses extra resources to train language models.",4.2 Results,[0],[0]
"In this paper, we proposed multi-step stacked learning to extract n-gram features step by step.",5 Conclusion,[0],[0]
The first level removes the filler words providing new ngrams for the second level to remove edited words.,5 Conclusion,[0],[0]
"The
third level uses the n-grams from the original text and the cleaned text generated by the previous two steps for accurate edit detection.",5 Conclusion,[0],[0]
"To minimize the F-loss approximately, we modified the hamming loss in M3Ns.",5 Conclusion,[0],[0]
"Experimental results show that our method is effective, and achieved the best reported performance on the Switchboard corpus without the use of any additional resources.",5 Conclusion,[0],[0]
We thank three anonymous reviewers for their valuable comments.,Acknowledgments,[0],[0]
This work is partly supported by DARPA under Contract No. HR0011-12-C-0016 and FA8750-13-2-0041.,Acknowledgments,[0],[0]
Any opinions expressed in this material are those of the authors and do not necessarily reflect the views of DARPA.,Acknowledgments,[0],[0]
"In this paper, we propose a multi-step stacked learning model for disfluency detection.",abstractText,[0],[0]
Our method incorporates refined n-gram features step by step from different word sequences.,abstractText,[0],[0]
"First, we detect filler words.",abstractText,[0],[0]
"Second, edited words are detected using n-gram features extracted from both the original text and filler filtered text.",abstractText,[0],[0]
"In the third step, additional n-gram features are extracted from edit removed texts together with our newly induced in-between features to improve edited word detection.",abstractText,[0],[0]
We use Max-Margin Markov Networks (MNs) as the classifier with the weighted hamming loss to balance precision and recall.,abstractText,[0],[0]
Experiments on the Switchboard corpus show that the refined n-gram features from multiple steps and MNs with weighted hamming loss can significantly improve the performance.,abstractText,[0],[0]
Our method for disfluency detection achieves the best reported F-score 0.841 without the use of additional resources.1,abstractText,[0],[0]
Disfluency Detection Using Multi-step Stacked Learning,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1753–1762, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.
In this paper, we propose a novel model dubbed the Piecewise Convolutional Neural Networks (PCNNs) with multi-instance learning to address these two problems. To solve the first problem, distant supervised relation extraction is treated as a multi-instance problem in which the uncertainty of instance labels is taken into account. To address the latter problem, we avoid feature engineering and instead adopt convolutional architecture with piecewise max pooling to automatically learn relevant features. Experiments show that our method is effective and outperforms several competitive baseline methods.",text,[0],[0]
"In relation extraction, one challenge that is faced when building a machine learning system is the generation of training examples.",1 Introduction,[0],[0]
"One common technique for coping with this difficulty is distant supervision (Mintz et al., 2009) which assumes that if two entities have a relationship in a known knowledge base, then all sentences that mention these two entities will express that relationship in some way.",1 Introduction,[0],[0]
"Figure 1 shows an example of the auto-
matic labeling of data through distant supervision.",1 Introduction,[0],[0]
"In this example, Apple and Steve Jobs are two related entities in Freebase1.",1 Introduction,[0],[0]
All sentences that contain these two entities are selected as training instances.,1 Introduction,[0],[0]
The distant supervision strategy is an effective method of automatically labeling training data.,1 Introduction,[0],[0]
"However, it has two major shortcomings when used for relation extraction.
",1 Introduction,[0],[0]
"First, the distant supervision assumption is too strong and causes the wrong label problem.",1 Introduction,[0],[0]
A sentence that mentions two entities does not necessarily express their relation in a knowledge base.,1 Introduction,[0],[0]
It is possible that these two entities may simply share the same topic.,1 Introduction,[0],[0]
"For instance, the upper sentence indeed expresses the “company/founders” relation in Figure 1.",1 Introduction,[0],[0]
"The lower sentence, however, does not express this relation but is still selected as a training instance.",1 Introduction,[0],[0]
"This will hinder the performance of a model trained on such noisy data.
",1 Introduction,[0],[0]
"Second, previous methods (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011) have typically applied supervised models to elaborately designed features when obtained the labeled data through distant supervision.",1 Introduction,[0],[0]
These features are often derived from preexisting Natural Language Processing (NLP) tools.,1 Introduction,[0],[0]
"Since errors inevitably exist in NLP tools, the use of traditional features leads to error propagation or accumulation.",1 Introduction,[0],[0]
"Distant supervised relation extraction generally ad-
1http://www.freebase.com/
1753
dresses corpora from the Web, including many informal texts.",1 Introduction,[0],[0]
Figure 2 shows the sentence length distribution of a benchmark distant supervision dataset that was developed by Riedel et al. (2010).,1 Introduction,[0],[0]
Approximately half of the sentences are longer than 40 words.,1 Introduction,[0],[0]
McDonald and Nivre (2007) showed that the accuracy of syntactic parsing decreases significantly with increasing sentence length.,1 Introduction,[0],[0]
"Therefore, when using traditional features, the problem of error propagation or accumulation will not only exist, it will grow more serious.
",1 Introduction,[0],[0]
"In this paper, we propose a novel model dubbed Piecewise Convolutional Neural Networks (PCNNs) with multi-instance learning to address the two problems described above.",1 Introduction,[0],[0]
"To address the first problem, distant supervised relation extraction is treated as a multi-instance problem similar to previous studies (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012).",1 Introduction,[0],[0]
"In multi-instance problem, the training set consists of many bags, and each contains many instances.",1 Introduction,[0],[0]
"The labels of the bags are known; however, the labels of the instances in the bags are unknown.",1 Introduction,[0],[0]
We design an objective function at the bag level.,1 Introduction,[0],[0]
"In the learning process, the uncertainty of instance labels can be taken into account; this alleviates the wrong label problem.
",1 Introduction,[0],[0]
"To address the second problem, we adopt convolutional architecture to automatically learn relevant features without complicated NLP preprocessing inspired by Zeng et al. (2014).",1 Introduction,[0],[0]
"Our proposal is an extension of Zeng et al. (2014), in which a single max pooling operation is utilized to determine the most significant features.",1 Introduction,[0],[0]
"Although this operation has been shown to be effective for textual feature representation (Collobert et al., 2011; Kim, 2014), it reduces the size of the
hidden layers too rapidly and cannot capture the structural information between two entities (Graham, 2014).",1 Introduction,[0],[0]
"For example, to identify the relation between Steve Jobs and Apple in Figure 1, we need to specify the entities and extract the structural features between them.",1 Introduction,[0],[0]
Several approaches have employed manually crafted features that attempt to model such structural information.,1 Introduction,[0],[0]
These approaches usually consider both internal and external contexts.,1 Introduction,[0],[0]
A sentence is inherently divided into three segments according to the two given entities.,1 Introduction,[0],[0]
"The internal context includes the characters inside the two entities, and the external context involves the characters around the two entities (Zhang et al., 2006).",1 Introduction,[0],[0]
"Clearly, single max pooling is not sufficient to capture such structural information.",1 Introduction,[0],[0]
"To capture structural and other latent information, we divide the convolution results into three segments based on the positions of the two given entities and devise a piecewise max pooling layer instead of the single max pooling layer.",1 Introduction,[0],[0]
The piecewise max pooling procedure returns the maximum value in each segment instead of a single maximum value over the entire sentence.,1 Introduction,[0],[0]
"Thus, it is expected to exhibit superior performance compared with traditional methods.
",1 Introduction,[0],[0]
"The contributions of this paper can be summarized as follows.
",1 Introduction,[0],[0]
• We explore the feasibility of performing distant supervised relation extraction without hand-designed features.,1 Introduction,[0],[0]
"PCNNS are proposed to automatically learn features without complicated NLP preprocessing.
",1 Introduction,[0],[0]
"• To address the wrong label problem, we develop innovative solutions that incorporate multi-instance learning into the PCNNS for distant supervised relation extraction.
",1 Introduction,[0],[0]
"• In the proposed network, we devise a piecewise max pooling layer, which aims to capture structural information between two entities.",1 Introduction,[0],[0]
Relation extraction is one of the most important topics in NLP.,2 Related Work,[0],[0]
"Many approaches to relation extraction have been developed, such as bootstrapping, unsupervised relation discovery and supervised classification.",2 Related Work,[0],[0]
"Supervised approaches are the most commonly used methods for relation
extraction and yield relatively high performance (Bunescu and Mooney, 2006; Zelenko et al., 2003; Zhou et al., 2005).",2 Related Work,[0],[0]
"In the supervised paradigm, relation extraction is considered to be a multi-class classification problem and may suffer from a lack of labeled data for training.",2 Related Work,[0],[0]
"To address this problem, Mintz et al. (2009) adopted Freebase to perform distant supervision.",2 Related Work,[0],[0]
"As described in Section 1, the algorithm for training data generation is sometimes faced with the wrong label problem.",2 Related Work,[0],[0]
"To address this shortcoming, (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) developed the relaxed distant supervision assumption for multi-instance learning.",2 Related Work,[0],[0]
"The term ‘multiinstance learning was coined by (Dietterich et al., 1997) while investigating the problem of predicting drug activity.",2 Related Work,[0],[0]
"In multi-instance learning, the uncertainty of instance labels can be taken into account.",2 Related Work,[0],[0]
"The focus of multi-instance learning is to discriminate among the bags.
",2 Related Work,[0],[0]
These methods have been shown to be effective for relation extraction.,2 Related Work,[0],[0]
"However, their performance depends strongly on the quality of the designed features.",2 Related Work,[0],[0]
Most existing studies have concentrated on extracting features to identify the relations between two entities.,2 Related Work,[0],[0]
Previous methods can be generally categorized into two types: feature-based methods and kernel-based methods.,2 Related Work,[0],[0]
"In feature-based methods, a diverse set of strategies is exploited to convert classification clues (e.g., sequences, parse trees) into feature vectors (Kambhatla, 2004; Suchanek et al., 2006).",2 Related Work,[0],[0]
Feature-based methods suffer from the necessity of selecting a suitable feature set when converting structured representations into feature vectors.,2 Related Work,[0],[0]
"Kernel-based methods provide a natural alternative to exploit rich representations of input classification clues, such as syntactic parse trees.",2 Related Work,[0],[0]
Kernelbased methods enable the use of a large set of features without needing to extract them explicitly.,2 Related Work,[0],[0]
"Several kernels have been proposed, such as the convolution tree kernel (Qian et al., 2008), the subsequence kernel (Bunescu and Mooney, 2006) and the dependency tree kernel (Bunescu and Mooney, 2005).
",2 Related Work,[0],[0]
"Nevertheless, as mentioned in Section 1, it is difficult to design high-quality features using existing NLP tools.",2 Related Work,[0],[0]
"With the recent revival of interest in neural networks, many researchers have investigated the possibility of using neural networks to automatically learn features (Socher et
al., 2012; Zeng et al., 2014).",2 Related Work,[0],[0]
"Inspired by Zeng et al. (2014), we propose the use of PCNNs with multi-instance learning to automatically learn features for distant supervised relation extraction.",2 Related Work,[0],[0]
Dietterich et al. (1997) suggested that the design of multi-instance modifications for neural networks is a particularly interesting topic.,2 Related Work,[0],[0]
Zhang and Zhou (2006) successfully incorporated multiinstance learning into traditional Backpropagation (BP) and Radial Basis Function (RBF) networks and optimized these networks by minimizing a sum-of-squares error function.,2 Related Work,[0],[0]
"In contrast to their method, we define the objective function based on the cross-entropy principle.",2 Related Work,[0],[0]
Distant supervised relation extraction is formulated as multi-instance problem.,3 Methodology,[0],[0]
"In this section, we present innovative solutions that incorporate multi-instance learning into a convolutional neural network to fulfill this task.",3 Methodology,[0],[0]
PCNNs are proposed for the automatic learning of features without complicated NLP preprocessing.,3 Methodology,[0],[0]
Figure 3 shows our neural network architecture for distant supervised relation extraction.,3 Methodology,[0],[0]
It illustrates the procedure that handles one instance of a bag.,3 Methodology,[0],[0]
"This procedure includes four main parts: Vector Representation, Convolution, Piecewise Max Pooling and Softmax Output.",3 Methodology,[0],[0]
We describe these parts in detail below.,3 Methodology,[0],[0]
The inputs of our network are raw word tokens.,3.1 Vector Representation,[0],[0]
"When using neural networks, we typically transform word tokens into low-dimensional vectors.",3.1 Vector Representation,[0],[0]
"In our method, each input word token is transformed into a vector by looking up pre-trained word embeddings.",3.1 Vector Representation,[0],[0]
"Moreover, we use position features (PFs) to specify entity pairs, which are also transformed into vectors by looking up position embeddings.",3.1 Vector Representation,[0],[0]
Word embeddings are distributed representations of words that map each word in a text to a ‘k’dimensional real-valued vector.,3.1.1 Word Embeddings,[0],[0]
"They have recently been shown to capture both semantic and syntactic information about words very well, setting performance records in several word similarity tasks (Mikolov et al., 2013; Pennington et al., 2014).",3.1.1 Word Embeddings,[0],[0]
"Using word embeddings that have been trained a priori has become common practice for
enhancing many other NLP tasks (Parikh et al., 2014; Huang et al., 2014).
",3.1.1 Word Embeddings,[0],[0]
A common method of training a neural network is to randomly initialize all parameters and then optimize them using an optimization algorithm.,3.1.1 Word Embeddings,[0],[0]
"Recent research (Erhan et al., 2010) has shown that neural networks can converge to better local minima when they are initialized with word embeddings.",3.1.1 Word Embeddings,[0],[0]
Word embeddings are typically learned in an entirely unsupervised manner by exploiting the co-occurrence structure of words in unlabeled text.,3.1.1 Word Embeddings,[0],[0]
"Researchers have proposed several methods of training word embeddings (Bengio et al., 2003; Collobert et al., 2011; Mikolov et al., 2013).",3.1.1 Word Embeddings,[0],[0]
"In this paper, we use the Skip-gram model (Mikolov et al., 2013) to train word embeddings.",3.1.1 Word Embeddings,[0],[0]
"In relation extraction, we focus on assigning labels to entity pairs.",3.1.2 Position Embeddings,[0],[0]
"Similar to Zeng et al. (2014), we use PFs to specify entity pairs.",3.1.2 Position Embeddings,[0],[0]
A PF is defined as the combination of the relative distances from the current word to e1 and e2.,3.1.2 Position Embeddings,[0],[0]
"For instance, in the following example, the relative distances from son to e1 (Kojo Annan) and e2 (Kofi Annan) are 3 and -2, respectively.
...",3.1.2 Position Embeddings,[0],[0]
"hired Kojo Annan , the son of Kofi Annan , in ...3 -2 Two position embedding matrixes (PF1 and PF2) are randomly initialized.",3.1.2 Position Embeddings,[0],[0]
We then transform the relative distances into real valued vectors by looking up the position embedding matrixes.,3.1.2 Position Embeddings,[0],[0]
"In the example shown in Figure 3, it is assumed that
the size of the word embedding is dw = 4 and that the size of the position embedding is dp = 1.",3.1.2 Position Embeddings,[0],[0]
"In combined word embeddings and position embeddings, the vector representation part transforms an instance into a matrix S ∈ Rs×d, where s is the sentence length and d = dw + dp ∗ 2.",3.1.2 Position Embeddings,[0],[0]
The matrix S is subsequently fed into the convolution part.,3.1.2 Position Embeddings,[0],[0]
"In relation extraction, an input sentence that is marked as containing the target entities corresponds only to a relation type; it does not predict labels for each word.",3.2 Convolution,[0],[0]
"Thus, it might be necessary to utilize all local features and perform this prediction globally.",3.2 Convolution,[0],[0]
"When using a neural network, the convolution approach is a natural means of merging all these features (Collobert et al., 2011).
",3.2 Convolution,[0],[0]
"Convolution is an operation between a vector of weights, w, and a vector of inputs that is treated as a sequence q.",3.2 Convolution,[0],[0]
The weights matrix w is regarded as the filter for the convolution.,3.2 Convolution,[0],[0]
"In the example shown in Figure 3, we assume that the length of the filter is w (w = 3);",3.2 Convolution,[0],[0]
"thus, w ∈ Rm (m = w∗d).",3.2 Convolution,[0],[0]
"We consider S to be a sequence {q1,q2, · · · ,qs}, where qi ∈ Rd.",3.2 Convolution,[0],[0]
"In general, let qi:j refer to the concatenation of qi to qj .",3.2 Convolution,[0],[0]
"The convolution operation involves taking the dot product of w with each w-gram in the sequence q to obtain another sequence c ∈ Rs+w−1:
cj = wqj−w+1:j (1)
where the index j ranges from 1 to s+w−1.",3.2 Convolution,[0],[0]
"Outof-range input values qi, where i < 1 or i > s, are
taken to be zero.",3.2 Convolution,[0],[0]
The ability to capture different features typically requires the use of multiple filters (or feature maps) in the convolution.,3.2 Convolution,[0],[0]
"Under the assumption that we use n filters (W = {w1,w2, · · · ,wn}), the convolution operation can be expressed as follows:
cij = wiqj−w+1:j 1 ≤ i ≤ n",3.2 Convolution,[0],[0]
(2),3.2 Convolution,[0],[0]
"The convolution result is a matrix C = {c1, c2, · · · , cn} ∈ Rn×(s+w−1).",3.2 Convolution,[0],[0]
Figure 3 shows an example in which we use 3 different filters in the convolution procedure.,3.2 Convolution,[0],[0]
The size of the convolution output matrix C ∈ Rn×(s+w−1) depends on the number of tokens s in the sentence that is fed into the network.,3.3 Piecewise Max Pooling,[0],[0]
"To apply subsequent layers, the features that are extracted by the convolution layer must be combined such that they are independent of the sentence length.",3.3 Piecewise Max Pooling,[0],[0]
"In traditional Convolution Neural Networks (CNNs), max pooling operations are often applied for this purpose (Collobert et al., 2011; Zeng et al., 2014).",3.3 Piecewise Max Pooling,[0],[0]
This type of pooling scheme naturally addresses variable sentence lengths.,3.3 Piecewise Max Pooling,[0],[0]
"The idea is to capture the most significant features (with the highest values) in each feature map.
",3.3 Piecewise Max Pooling,[0],[0]
"However, despite the widespread use of single max pooling, this approach is insufficient for relation extraction.",3.3 Piecewise Max Pooling,[0],[0]
"As described in the first section, single max pooling reduces the size of the hidden layers too rapidly and is too coarse to capture finegrained features for relation extraction.",3.3 Piecewise Max Pooling,[0],[0]
"In addition, single max pooling is not sufficient to capture the structural information between two entities.",3.3 Piecewise Max Pooling,[0],[0]
"In relation extraction, an input sentence can be divided into three segments based on the two selected entities.",3.3 Piecewise Max Pooling,[0],[0]
"Therefore, we propose a piecewise max pooling procedure that returns the maximum value in each segment instead of a single maximum value.",3.3 Piecewise Max Pooling,[0],[0]
"As shown in Figure 3, the output of each convolutional filter ci is divided into three segments {ci1, ci2, ci3} by Kojo Annan and Kofi Annan.",3.3 Piecewise Max Pooling,[0],[0]
"The piecewise max pooling procedure can be expressed as follows:
pij = max(cij) 1 ≤ i ≤ n, 1 ≤ j ≤ 3 (3) For the output of each convolutional filter, we can obtain a 3-dimensional vector pi = {pi1, pi2, pi3}.",3.3 Piecewise Max Pooling,[0],[0]
"We then concatenate all vectors
p1:n and apply a non-linear function, such as the hyperbolic tangent.",3.3 Piecewise Max Pooling,[0],[0]
"Finally, the piecewise max pooling procedure outputs a vector:
g = tanh(p1:n) (4)
where g ∈ R3n.",3.3 Piecewise Max Pooling,[0],[0]
The size of g is fixed and is no longer related to the sentence length.,3.3 Piecewise Max Pooling,[0],[0]
"To compute the confidence of each relation, the feature vector g is fed into a softmax classifier.
",3.4 Softmax Output,[0],[0]
"o = W1g + b (5)
W1 ∈ Rn1×3n is the transformation matrix, and o ∈ Rn1 is the final output of the network, where n1 is equal to the number of possible relation types for the relation extraction system.
",3.4 Softmax Output,[0],[0]
"We employ dropout (Hinton et al., 2012) on the penultimate layer for regularization.",3.4 Softmax Output,[0],[0]
Dropout prevents the co-adaptation of hidden units by randomly dropping out a proportion p of the hidden units during forward computing.,3.4 Softmax Output,[0],[0]
"We first apply a “masking” operation (g◦r) on g, where r is a vector of Bernoulli random variables with probability p of being 1. Eq.(5)",3.4 Softmax Output,[0],[0]
"becomes:
o = W1(g ◦ r) + b (6)
",3.4 Softmax Output,[0],[0]
Each output can then be interpreted as the confidence score of the corresponding relation.,3.4 Softmax Output,[0],[0]
This score can be interpreted as a conditional probability by applying a softmax operation (see Section 3.5).,3.4 Softmax Output,[0],[0]
"In the test procedure, the learned weight vectors are scaled by p such that Ŵ1 = pW1 and are used (without dropout) to score unseen instances.",3.4 Softmax Output,[0],[0]
"In order to alleviate the wrong label problem, we use multi-instance learning for PCNNs.",3.5 Multi-instance Learning,[0],[0]
"The PCNNs-based relation extraction can be stated as a quintuple θ = (E,PF1,PF2,W,W1)2.",3.5 Multi-instance Learning,[0],[0]
The input to the network is a bag.,3.5 Multi-instance Learning,[0],[0]
"Suppose that there are T bags {M1, M2, · · · ,MT } and that the i-th bag contains qi instances Mi = {m1i ,m2i , · · · , mqii }.",3.5 Multi-instance Learning,[0],[0]
The objective of multi-instance learning is to predict the labels of the unseen bags.,3.5 Multi-instance Learning,[0],[0]
"In this paper, all instances in a bag are considered independently.",3.5 Multi-instance Learning,[0],[0]
"Given an input instance mji , the network with the parameter θ outputs a vector o, where the r-th component or corresponds to the score associated
2E represents the word embeddings.
",3.5 Multi-instance Learning,[0],[0]
Algorithm 1 Multi-instance learning 1: Initialize θ.,3.5 Multi-instance Learning,[0],[0]
"Partition the bags into mini-
batches of size bs. 2: Randomly choose a mini-batch, and feed the
bags into the network one by one.",3.5 Multi-instance Learning,[0],[0]
3: Find the j-th instance mji (1 ≤,3.5 Multi-instance Learning,[0],[0]
"i ≤ bs) in each
bag according to Eq. (9).",3.5 Multi-instance Learning,[0],[0]
"4: Update θ based on the gradients of mji (1 ≤
i ≤ bs) via Adadelta.",3.5 Multi-instance Learning,[0],[0]
"5: Repeat steps 2-4 until either convergence or
the maximum number of epochs is reached.
with relation r.",3.5 Multi-instance Learning,[0],[0]
"To obtain the conditional probability p(r|m, θ), we apply a softmax operation over all relation types:
p(r|mji ; θ) = eor n1∑ k=1 eok (7)
",3.5 Multi-instance Learning,[0],[0]
The objective of multi-instance learning is to discriminate bags rather than instances.,3.5 Multi-instance Learning,[0],[0]
"To do so, we must define the objective function on the bags.",3.5 Multi-instance Learning,[0],[0]
"Given all (T ) training bags (Mi, yi), we can define the objective function using cross-entropy at the bag level as follows:
J (θ) = T∑
i=1
log p(yi|mji ; θ) (8)
where j is constrained as follows:
j∗ = arg max j p(yi|mji ; θ) 1 ≤ j ≤ qi (9)
",3.5 Multi-instance Learning,[0],[0]
"Using this defined objective function, we maximize J(θ) through stochastic gradient descent over shuffled mini-batches with the Adadelta (Zeiler, 2012) update rule.",3.5 Multi-instance Learning,[0],[0]
"The entire training procedure is described in Algorithm 1.
",3.5 Multi-instance Learning,[0],[0]
"From the introduction presented above, we know that the traditional backpropagation algorithm modifies a network in accordance with all training instances, whereas backpropagation with multi-instance learning modifies a network based on bags.",3.5 Multi-instance Learning,[0],[0]
"Thus, our method captures the nature of distant supervised relation extraction, in which some training instances will inevitably be incorrectly labeled.",3.5 Multi-instance Learning,[0],[0]
"When a trained PCNN is used for prediction, a bag is positively labeled if and only if the output of the network on at least one of its instances is assigned a positive label.",3.5 Multi-instance Learning,[0],[0]
Our experiments are intended to provide evidence that supports the following hypothesis: automatically learning features using PCNNs with multiinstance learning can lead to an increase in performance.,4 Experiments,[0],[0]
"To this end, we first introduce the dataset and evaluation metrics used.",4 Experiments,[0],[0]
"Next, we test several variants via cross-validation to determine the parameters to be used in our experiments.",4 Experiments,[0],[0]
We then compare the performance of our method to those of several traditional methods.,4 Experiments,[0],[0]
"Finally, we evaluate the effects of piecewise max pooling and multiinstance learning3.",4 Experiments,[0],[0]
"We evaluate our method on a widely used dataset4 that was developed by (Riedel et al., 2010) and has also been used by (Hoffmann et al., 2011; Surdeanu et al., 2012).",4.1 Dataset and Evaluation Metrics,[0],[0]
"This dataset was generated by aligning Freebase relations with the NYT corpus, with sentences from the years 2005-2006 used as the training corpus and sentences from 2007 used as the testing corpus.
",4.1 Dataset and Evaluation Metrics,[0],[0]
"Following previous work (Mintz et al., 2009), we evaluate our method in two ways: the held-out evaluation and the manual evaluation.",4.1 Dataset and Evaluation Metrics,[0],[0]
The heldout evaluation only compares the extracted relation instances against Freebase relation data and reports the precision/recall curves of the experiments.,4.1 Dataset and Evaluation Metrics,[0],[0]
"In the manual evaluation, we manually check the newly discovered relation instances that are not in Freebase.",4.1 Dataset and Evaluation Metrics,[0],[0]
"In this paper, we use the Skip-gram model (word2vec)5 to train the word embeddings on the NYT corpus.",4.2.1 Pre-trained Word Embeddings,[0],[0]
Word2vec first constructs a vocabulary from the training text data and then learns vector representations of the words.,4.2.1 Pre-trained Word Embeddings,[0],[0]
"To obtain the embeddings of the entities, we concatenate the tokens of a entity using the ## operator when the entity has multiple word tokens.",4.2.1 Pre-trained Word Embeddings,[0],[0]
"Since a comparison of the word embeddings is beyond the scope
3With regard to the position feature, our experiments yield the same positive results described in Zeng et al. (2014).",4.2.1 Pre-trained Word Embeddings,[0],[0]
"Because the position feature is not the main contribution of this paper, we do not present the results without the position feature.
",4.2.1 Pre-trained Word Embeddings,[0],[0]
"4http://iesl.cs.umass.edu/riedel/ecml/ 5https://code.google.com/p/word2vec/
Window size Feature maps
Word dimension
Position dimension
Batch size
Adadelta parameter Dropout probability
of this paper, our experiments directly utilize 50- dimensional vectors.",4.2.1 Pre-trained Word Embeddings,[0],[0]
"In this section, we experimentally study the effects of two parameters on our models: the window size, w, and the number of feature maps, n. Following (Surdeanu et al., 2012), we tune all of the models using three-fold validation on the training set.",4.2.2 Parameter Settings,[0],[0]
"We use a grid search to determine the optimal parameters and manually specify subsets of the parameter spaces: w ∈ {1, 2, 3, · · · , 7} and n ∈ {50, 60, · · · , 300}.",4.2.2 Parameter Settings,[0],[0]
Table 1 shows all parameters used in the experiments.,4.2.2 Parameter Settings,[0],[0]
"Because the position dimension has little effect on the result, we heuristically choose dp = 5.",4.2.2 Parameter Settings,[0],[0]
The batch size is fixed to 50.,4.2.2 Parameter Settings,[0],[0]
"We use Adadelta (Zeiler, 2012) in the update procedure; it relies on two main parameters, ρ and ε, which do not significantly affect the performance (Zeiler, 2012).",4.2.2 Parameter Settings,[0],[0]
"Following (Zeiler, 2012), we choose 0.95 and 1e−6, respectively, as the values of these parameters.",4.2.2 Parameter Settings,[0],[0]
"In the dropout operation, we randomly set the hidden unit activities to zero with a probability of 0.5 during training.",4.2.2 Parameter Settings,[0],[0]
The held-out evaluation provides an approximate measure of precision without requiring costly human evaluation.,4.3.1 Held-out Evaluation,[0],[0]
Half of the Freebase relations are used for testing.,4.3.1 Held-out Evaluation,[0],[0]
"The relation instances discovered from the test articles are automatically compared with those in Freebase.
",4.3.1 Held-out Evaluation,[0],[0]
"To evaluate the proposed method, we select the following three traditional methods for comparison.",4.3.1 Held-out Evaluation,[0],[0]
"Mintz represents a traditional distantsupervision-based model that was proposed by (Mintz et al., 2009).",4.3.1 Held-out Evaluation,[0],[0]
"MultiR is a multi-instance learning method that was proposed by (Hoffmann et al., 2011).",4.3.1 Held-out Evaluation,[0],[0]
"MIML is a multi-instance multilabel model that was proposed by (Surdeanu et al., 2012).",4.3.1 Held-out Evaluation,[0],[0]
"Figure 4 shows the precision-recall curves for each method, where PCNNs+MIL denotes our method, and demonstrates that PCNNs+MIL achieves higher precision over the entire range of recall.",4.3.1 Held-out Evaluation,[0],[0]
"PCNNs+MIL enhances the recall to ap-
proximately 34% without any loss of precision.",4.3.1 Held-out Evaluation,[0],[0]
"In terms of both precision and recall, PCNNs+MIL outperforms all other evaluated approaches.",4.3.1 Held-out Evaluation,[0],[0]
"Notably, the results of the methods evaluated for comparison were obtained using manually crafted features.",4.3.1 Held-out Evaluation,[0],[0]
"By contrast, our result is obtained by automatically learning features from original words.",4.3.1 Held-out Evaluation,[0],[0]
The results demonstrate that the proposed method is an effective technique for distant supervised relation extraction.,4.3.1 Held-out Evaluation,[0],[0]
Automatically learning features via PCNNs can alleviate the error propagation that occurs in traditional feature extraction.,4.3.1 Held-out Evaluation,[0],[0]
Incorporating multi-instance learning into a convolutional neural network is an effective means of addressing the wrong label problem.,4.3.1 Held-out Evaluation,[0],[0]
It is worth emphasizing that there is a sharp decline in the held-out precision-recall curves of PCNNs+MIL at very low recall (Figure 4).,4.3.2 Manual Evaluation,[0],[0]
"A manual check of the misclassified examples that were produced with high confidence reveals that the ma-
jorities of these examples are false negatives and are actually true relation instances that were misclassified due to the incomplete nature of Freebase.
",4.3.2 Manual Evaluation,[0],[0]
"Thus, the held-out evaluation suffers from false negatives in Freebase.",4.3.2 Manual Evaluation,[0],[0]
We perform a manual evaluation to eliminate these problems.,4.3.2 Manual Evaluation,[0],[0]
"For the manual evaluation, we choose the entity pairs for which at least one participating entity is not present in Freebase as a candidate.",4.3.2 Manual Evaluation,[0],[0]
This means that there is no overlap between the held-out and manual candidates.,4.3.2 Manual Evaluation,[0],[0]
"Because the number of relation instances that are expressed in the test data is unknown, we cannot calculate the recall in this case.",4.3.2 Manual Evaluation,[0],[0]
"Instead, we calculate the precision of the top N extracted relation instances.",4.3.2 Manual Evaluation,[0],[0]
"Table 2 presents the manually evaluated precisions for the top 100, top 200, and top 500 extracted instances.",4.3.2 Manual Evaluation,[0],[0]
"The results show that PCNNs+MIL achieves the best performance; moreover, the precision is higher than in the held-out evaluation.",4.3.2 Manual Evaluation,[0],[0]
"This finding indicates that many of the false negatives that we predict are, in fact, true relational facts.",4.3.2 Manual Evaluation,[0],[0]
The sharp decline observed in the held-out precision-recall curves is therefore reasonable.,4.3.2 Manual Evaluation,[0],[0]
"In this paper, we develop a method of piecewise max pooling and incorporate multi-instance learning into convolutional neural networks for distant supervised relation extraction.",4.4 Effect of Piecewise Max Pooling and Multi-instance Learning,[0],[0]
"To demonstrate the effects of these two techniques, we empirically study the performance of systems in which these techniques are not implemented through held-out evaluations (Figure 5).",4.4 Effect of Piecewise Max Pooling and Multi-instance Learning,[0],[0]
CNNs represents convolutional neural networks to which single max pooling is applied.,4.4 Effect of Piecewise Max Pooling and Multi-instance Learning,[0],[0]
"Figure 5 shows that when piecewise max pooling is used (PCNNs), better results are produced than those achieved using CNNs.",4.4 Effect of Piecewise Max Pooling and Multi-instance Learning,[0],[0]
"Moreover, compared with CNNs+MIL, PCNNs achieve slightly higher precision when the recall is greater than 0.08.",4.4 Effect of Piecewise Max Pooling and Multi-instance Learning,[0],[0]
"Since the parameters for all the model are determined by grid search, we can observe that CNNs cannot achieve competitive results compared to PCNNs when increasing the size of the hidden layer of convolutional neural networks.",4.4 Effect of Piecewise Max Pooling and Multi-instance Learning,[0],[0]
It means that we cannot capture more useful information by simply increasing the network parameter.,4.4 Effect of Piecewise Max Pooling and Multi-instance Learning,[0],[0]
"These results demonstrate that the proposed piecewise max pooling technique is beneficial and
can effectively capture structural information for relation extraction.",4.4 Effect of Piecewise Max Pooling and Multi-instance Learning,[0],[0]
A similar phenomenon is also observed when multi-instance learning is added to the network.,4.4 Effect of Piecewise Max Pooling and Multi-instance Learning,[0],[0]
"Both CNNs+MIL and PCNNs+MIL outperform their counterparts CNNs and PCNNs, respectively, thereby demonstrating that incorporation of multi-instance learning into our neural network was successful in solving the wrong label problem.",4.4 Effect of Piecewise Max Pooling and Multi-instance Learning,[0],[0]
"As expected, PCNNs+MIL obtains the best results because the advantages of both techniques are achieved simultaneously.",4.4 Effect of Piecewise Max Pooling and Multi-instance Learning,[0],[0]
"In this paper, we exploit Piecewise Convolutional Neural Networks (PCNNs) with multi-instance learning for distant supervised relation extraction.",5 Conclusion,[0],[0]
"In our method, features are automatically learned without complicated NLP preprocessing.",5 Conclusion,[0],[0]
We also successfully devise a piecewise max pooling layer in the proposed network to capture structural information and incorporate multi-instance learning to address the wrong label problem.,5 Conclusion,[0],[0]
Experimental results show that the proposed approach offers significant improvements over comparable methods.,5 Conclusion,[0],[0]
This work was sponsored by the National Basic Research Program of China (no. 2014CB340503) and the National Natural Science Foundation of China (no. 61272332 and no. 61202329).,Acknowledgments,[0],[0]
We thank the anonymous reviewers for their insightful comments.,Acknowledgments,[0],[0]
Two problems arise when using distant supervision for relation extraction.,abstractText,[0],[0]
"First, in this method, an already existing knowledge base is heuristically aligned to texts, and the alignment results are treated as labeled data.",abstractText,[0],[0]
"However, the heuristic alignment can fail, resulting in wrong label problem.",abstractText,[0],[0]
"In addition, in previous approaches, statistical models have typically been applied to ad hoc features.",abstractText,[0],[0]
The noise that originates from the feature extraction process can cause poor performance.,abstractText,[0],[0]
"In this paper, we propose a novel model dubbed the Piecewise Convolutional Neural Networks (PCNNs) with multi-instance learning to address these two problems.",abstractText,[0],[0]
"To solve the first problem, distant supervised relation extraction is treated as a multi-instance problem in which the uncertainty of instance labels is taken into account.",abstractText,[0],[0]
"To address the latter problem, we avoid feature engineering and instead adopt convolutional architecture with piecewise max pooling to automatically learn relevant features.",abstractText,[0],[0]
Experiments show that our method is effective and outperforms several competitive baseline methods.,abstractText,[0],[0]
Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 614–620 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
614",text,[0],[0]
Low-resource languages lack manually annotated data to learn even the most basic models such as part-of-speech (POS) taggers.,1 Introduction,[0],[0]
"To compensate for the absence of direct supervision, work in crosslingual learning and distant supervision has discovered creative use for a number of alternative data sources to learn feasible models: – aligned parallel corpora to project POS annota-
tions to target languages (Yarowsky et al., 2001; Agić",1 Introduction,[0],[0]
"et al., 2015; Fang and Cohn, 2016), – noisy tag dictionaries for type-level approximation of full supervision (Li et al., 2012), – combination of projection and type constraints (Das and Petrov, 2011; Täckström",1 Introduction,[0],[0]
"et al., 2013), – rapid annotation of seed training data (Garrette and Baldridge, 2013; Garrette et al., 2013).",1 Introduction,[0],[0]
"However, only one or two compatible sources of distant supervision are typically employed.",1 Introduction,[0],[0]
In reality severely under-resourced languages may require a more pragmatic “take what you can get” viewpoint.,1 Introduction,[0],[0]
"Our results suggest that combining supervision sources is the way to go about creating viable low-resource taggers.
",1 Introduction,[0],[0]
We propose a method to strike a balance between model simplicity and the capacity to easily integrate heterogeneous learning signals.,1 Introduction,[0],[0]
"Our
system is a uniform neural model for POS tagging that learns from disparate sources of distant supervision (DSDS).",1 Introduction,[0],[0]
"We use it to combine: i) multi-source annotation projection, ii) instance selection, iii) noisy tag dictionaries, and iv) distributed word and sub-word representations.",1 Introduction,[0],[0]
"We examine how far we can get by exploiting only the wide-coverage resources that are currently readily available for more than 300 languages, which is the breadth of the parallel corpus we employ.
",1 Introduction,[0],[0]
DSDS yields a new state of the art by jointly leveraging disparate sources of distant supervision in an experiment with 25 languages.,1 Introduction,[0],[0]
"We demonstrate: i) substantial gains in carefully selecting high-quality instances in annotation projection, ii) the usefulness of lexicon features for neural tagging, and iii) the importance of word embeddings initialization for faster convergence.",1 Introduction,[0],[0]
DSDS is illustrated in Figure 1.,2 Method,[0],[0]
"The base model is a bidirectional long short-term memory network (bi-LSTM) (Graves and Schmidhuber, 2005; Hochreiter and Schmidhuber, 1997; Plank et al., 2016; Kiperwasser and Goldberg, 2016).",2 Method,[0],[0]
"Let x1:n
be a given sequence of input vectors.",2 Method,[0],[0]
"In our base model, the input sequence consists of word embeddings ~w and the two output states of a character-level bi-LSTM ~c.",2 Method,[0],[0]
"Given x1:n and a desired index i, the functionBiRNNθ(x1:n, i) (here instantiated as LSTM) reads the input sequence in forward and reverse order, respectively, and uses the concatenated (◦) output states as input for tag prediction at position i.1",2 Method,[0],[0]
"Our model differs from prior work on the type of input vectors x1:n and distant data sources, in particular, we extend the input with lexicon embeddings, all described next.
",2 Method,[0],[0]
Annotation projection.,2 Method,[0],[0]
"Ever since the seminal work of Yarowsky et al. (2001), projecting sequential labels from source to target languages has been one of the most prevalent approaches to crosslingual learning.",2 Method,[0],[0]
"Its only requirement is that parallel texts are available between the languages, and that the source side is annotated for POS.
",2 Method,[0],[0]
"We apply the approach by Agić et al. (2016), where labels are projected from multiple sources and then decoded through weighted majority voting with word alignment probabilities and source POS tagger confidences.",2 Method,[0],[0]
"We exploit their widecoverage Watchtower corpus (WTC), in contrast to the typically used Europarl data.",2 Method,[0],[0]
"Europarl covers 21 languages of the EU with 400k-2M sentence pairs, while WTC spans 300+ widely diverse languages with only 10-100k pairs, in effect sacrificing depth for breadth, and introducing a more radical domain shift.",2 Method,[0],[0]
"However, as our results show little projected data turns out to be the most beneficial, reinforcing breadth for depth.
",2 Method,[0],[0]
"While Agić et al. (2016) selected 20k projected sentences at random to train taggers, we propose a novel alternative: selection by coverage.",2 Method,[0],[0]
"We rank the target sentences by percentage of words covered by word alignment from 21 sources of Agić et al. (2016), and select the top k covered instances for training.",2 Method,[0],[0]
"In specific, we employ the mean coverage ranking of target sentences, whereby each target sentence is coupled with the arithmetic mean of the 21 individual word alignment coverages for each of the 21 source-language sentences.",2 Method,[0],[0]
"We show that this simple approach to instance selection offers substantial improvements: across all languages, we learn better taggers with significantly fewer training instances.
",2 Method,[0],[0]
"1CRF decoding did not consistently improve POS accuracy, as recently also independently found (Yang et al., 2018).
",2 Method,[0],[0]
Dictionaries.,2 Method,[0],[0]
"Dictionaries are a useful source for distant supervision (Li et al., 2012; Täckström",2 Method,[0],[0]
"et al., 2013).",2 Method,[0],[0]
"There are several ways to exploit such information: i) as type constraints during encoding (Täckström et al., 2013), ii) to guide unsupervised learning (Li et al., 2012), or iii) as additional signal at training.",2 Method,[0],[0]
"We focus on the latter and evaluate two ways to integrate lexical knowledge into neural models, while comparing to the former two: a) by representing lexicon properties as n-hot vector (e.g., if a word has two properties according to lexicon src, it results in a 2-hot vector, if the word is not present in src, a zero vector), with m the number of lexicon properties; b) by embedding the lexical features, i.e., ~esrc is a lexicon src embedded into an l-dimensional space.",2 Method,[0],[0]
"We represent ~esrc as concatenation of all embedded m properties of length l, and a zero vector otherwise.",2 Method,[0],[0]
"Tuning on the dev set, we found the second embedding approach to perform best, and simple concatenation outperformed mean vector representations.
",2 Method,[0],[0]
"We evaluate two dictionary sources, motivated by ease of accessibility to many languages: WIKTIONARY, a word type dictionary that maps tokens to one of the 12 Universal POS tags (Li et al., 2012; Petrov et al., 2012); and UNIMORPH, a morphological dictionary that provides inflectional paradigms across 350 languages (Kirov et al., 2016).",2 Method,[0],[0]
"For Wiktionary, we use the freely available dictionaries from Li et al. (2012) and Agić",2 Method,[0],[0]
et al. (2017).,2 Method,[0],[0]
"The size of the dictionaries ranges from a few thousands (e.g., Hindi and Bulgarian) to 2M (Finnish UniMorph).",2 Method,[0],[0]
"Sizes are provided in Table 1, first columns.",2 Method,[0],[0]
"UniMorph covers between 8-38 morphological properties (for English and Finnish, respectively).
",2 Method,[0],[0]
Word embeddings.,2 Method,[0],[0]
Embeddings are available for many languages.,2 Method,[0],[0]
Pre-initialization of ~w offers consistent and considerable performance improvements in our distant supervision setup (Section 4).,2 Method,[0],[0]
"We use off-the-shelf Polyglot embeddings (AlRfou et al., 2013), which performed consistently better than FastText (Bojanowski et al., 2016).",2 Method,[0],[0]
Baselines.,3 Experiments,[0],[0]
"We compare to the following weaklysupervised POS taggers: – AGIC: Multi-source annotation projection with
Bible parallel data by Agić",3 Experiments,[0],[0]
et al. (2015).,3 Experiments,[0],[0]
– DAS:,3 Experiments,[0],[0]
"The label propagation approach by Das
and Petrov (2011) over Europarl data.
– GARRETTE:",3 Experiments,[0],[0]
"The approach by Garrette and Baldridge (2013) that works with projections, dictionaries, and unlabeled target text. – LI: Wiktionary supervision (Li et al., 2012).
",3 Experiments,[0],[0]
Data.,3 Experiments,[0],[0]
Our set of 25 languages is motivated by accessibility to embeddings and dictionaries.,3 Experiments,[0],[0]
"In all experiments we work with the 12 Universal POS tags (Petrov et al., 2012).",3 Experiments,[0],[0]
"For development, we use 21 dev sets of the Universal Dependencies 2.1 (Nivre et al., 2017).",3 Experiments,[0],[0]
We employ UD test sets on additional languages as well as the test sets of Agić,3 Experiments,[0],[0]
et al. (2015) to facilitate comparisons.,3 Experiments,[0],[0]
"Their test sets are a mixture of CoNLL (Buchholz and Marsi, 2006; Nivre et al., 2007) and HamleDT test data (Zeman et al., 2014), and are more distant from the training and development data.
",3 Experiments,[0],[0]
Model and parameters.,3 Experiments,[0],[0]
We extend an off-theshelf state-of-the-art bi-LSTM tagger with lexicon information.,3 Experiments,[0],[0]
The code is available at: https:// github.com/bplank/bilstm-aux.,3 Experiments,[0],[0]
The parameter l=40 was set on dev data across all languages.,3 Experiments,[0],[0]
"Besides using 10 epochs, word dropout rate (p=.25) and 40-dimensional lexicon embeddings, we use the parameters from Plank et al. (2016).",3 Experiments,[0],[0]
"For all experiments, we average over 3 randomly seeded runs, and provide mean accuracy.",3 Experiments,[0],[0]
"For the learning curve, we average over 5 random samples with 3 runs each.",3 Experiments,[0],[0]
"Table 1 shows the tagging accuracy for individual languages, while the means over all languages are given in Figure 2.",4 Results,[0],[0]
"There are several take-aways.
",4 Results,[0],[0]
Data selection.,4 Results,[0],[0]
"The first take-away is that coverage-based instance selection yields substan-
tially better training data.",4 Results,[0],[0]
"Most prior work on annotation projection resorts to arbitrary selection; informed selection clearly helps in this noisy data setup, as shown in Figure 2 (a).",4 Results,[0],[0]
"Training on 5k instances results in a sweet spot; more data (10k) starts to decrease performance, at a cost of runtime.",4 Results,[0],[0]
Training on all WTC data (around 120k) is worse for most languages.,4 Results,[0],[0]
"From now on we consider the 5k model trained with Polyglot as our baseline (Table 1, column “5k”), obtaining a mean accuracy of 83.0 over 21 languages.
",4 Results,[0],[0]
Embeddings initialization.,4 Results,[0],[0]
"Polyglot initialization offers a large boost; on average +3.8% absolute improvement in accuracy for our 5k training scheme, as shown in Figure 2 (b).",4 Results,[0],[0]
"The big gap in low-resource setups further shows their effectiveness, with up to 10% absolute increase in accuracy when training on only 500 instances.
",4 Results,[0],[0]
Lexical information.,4 Results,[0],[0]
"The main take-away is that lexical information helps neural tagging, and embedding it proves the most helpful.",4 Results,[0],[0]
"Embedding Wiktionary tags reaches 83.7 accuracy on average, versus 83.4 for n-hot encoding, and 83.2 for type constraints.",4 Results,[0],[0]
Only on 4 out of 21 languages are type constraints better.,4 Results,[0],[0]
This is the case for only one language for n-hot encoding (French).,4 Results,[0],[0]
"The best approach is to embed both Wiktionary and Unimorph, boosting performance further to 84.0, and resulting in our final model.",4 Results,[0],[0]
"It helps the most on morphological rich languages such as Uralic.
",4 Results,[0],[0]
"On the test sets (Table 4, right) DSDS reaches 87.2 over 8 test languages intersecting Li et al. (2012) and Agić",4 Results,[0],[0]
et al. (2016).,4 Results,[0],[0]
"It reaches 86.2 over the more commonly used 8 languages of Das and Petrov (2011), compared to their 83.4.",4 Results,[0],[0]
"This shows that our novel “soft” inclusion of noisy dictionaries is superior to a hard decoding restriction, and including lexicons in neural taggers helps.",4 Results,[0],[0]
"We did not assume any gold data to further enrich the lexicons, nor fix possible tagset divergences.",4 Results,[0],[0]
Analysis.,5 Discussion,[0],[0]
The inclusion of lexicons results in higher coverage and is part of the explanation for the improvement of DSDS; see correlation in Figure 3 (a).,5 Discussion,[0],[0]
"What is more interesting is that our model benefits from the lexicon beyond its content: OOV accuracy for words not present in the lexicon overall improves, besides the expected improvement on known OOV, see Figure 3 (b).
More languages.",5 Discussion,[0],[0]
All data sources employed in our experiment are very high-coverage.,5 Discussion,[0],[0]
"However, for true low-resource languages, we cannot safely assume the availability of all disparate information sources.",5 Discussion,[0],[0]
Table 2 presents results for four additional languages where some supervision sources are missing.,5 Discussion,[0],[0]
"We observe that adding lexicon information always helps, even in cases where only 1k entries are available, and embedding it is usually the most beneficial way.",5 Discussion,[0],[0]
"For closely-related languages such as Serbian and Croatian, using resources for one aids tagging the other, and modern resources are a better fit.",5 Discussion,[0],[0]
"For example, using the Croatian WTC projections to train a model for Serbian is preferable over in-language Serbian Bible data where the OOV rate is much higher.
",5 Discussion,[0],[0]
How much gold data?,5 Discussion,[0],[0]
We assume not having access to any gold annotated data.,5 Discussion,[0],[0]
It is thus interesting to ask how much gold data is needed to reach our performance.,5 Discussion,[0],[0]
"This is a tricky question, as training within the same corpus naturally favors the same corpus data.",5 Discussion,[0],[0]
"We test both in-corpus (UD)
and out-of-corpus data (our test sets) and notice an important gap: while in-corpus only 50 sentences are sufficient, outside the corpus one would need over 200 sentences.",5 Discussion,[0],[0]
"This experiment was done for a subset of 18 languages with both in- and out-ofcorpus test data.
",5 Discussion,[0],[0]
Further comparison.,5 Discussion,[0],[0]
"In Table 1 we directly report the accuracies from the original contributions by DAS, LI, GARRETTE, and AGIC over the same test data.",5 Discussion,[0],[0]
We additionally attempted to reach the scores of LI by running their tagger over the Table 1 data setup.,5 Discussion,[0],[0]
The results are depicted in Figure 4 as mean accuracies over EM iterations until convergence.,5 Discussion,[0],[0]
"We show: i) LI peaks at 10 iterations for their test languages, and at 35 iterations for all the rest.",5 Discussion,[0],[0]
"This is in slight contrast to 50 iterations that Li et al. (2012) recommend, although selecting 50 does not dramatically hurt the scores; ii)",5 Discussion,[0],[0]
Our replication falls ∼5 points short of their 84.9 accuracy.,5 Discussion,[0],[0]
"There is a large 33-point accuracy gap between the scores of Li et al. (2012), where the dictionaries are large, and the other languages in Figure 4, with smaller dictionaries.
",5 Discussion,[0],[0]
"Compared to DAS, our tagger clearly benefits from pre-trained word embeddings, while theirs relies on label propagation through Europarl, a much cleaner corpus that lacks the coverage of the noisier WTC.",5 Discussion,[0],[0]
"Similar applies to Täckström et al. (2013), as they use 1-5M near-perfect parallel sentences.",5 Discussion,[0],[0]
"Even if we use much smaller and noisier data sources, DSDS is almost on par: 86.2 vs. 87.3 for the 8 languages from Das and Petrov (2011), and we even outperform theirs on four languages: Czech, French, Italian, and Spanish.",5 Discussion,[0],[0]
"Most successful work on low-resource POS tagging is based on projection (Yarowsky et al., 2001), tag dictionaries (Li et al., 2012), annotation of seed training data (Garrette and Baldridge, 2013) or even more recently some combination of these, e.g., via multi-task learning (Fang and
Cohn, 2016; Kann et al., 2018).",6 Related Work,[0],[0]
"Our paper contributes to this literature by leveraging a range of prior directions in a unified, neural test bed.
",6 Related Work,[0],[0]
Most prior work on neural sequence prediction follows the commonly perceived wisdom that hand-crafted features are unnecessary for deep learning methods.,6 Related Work,[0],[0]
They rely on end-to-end training without resorting to additional linguistic resources.,6 Related Work,[0],[0]
Our study shows that this is not the case.,6 Related Work,[0],[0]
"Only few prior studies investigate such sources, e.g., for MT (Sennrich and Haddow, 2016; Chen et al., 2017; Li et al., 2017; Passban et al., 2018) and Sagot and Martı́nez Alonso (2017) for POS tagging use lexicons, but only as n-hot features and without examining the cross-lingual aspect.",6 Related Work,[0],[0]
We show that our approach of distant supervision from disparate sources (DSDS) is simple yet surprisingly effective for low-resource POS tagging.,7 Conclusions,[0],[0]
"Only 5k instances of projected data paired with off-the-shelf embeddings and lexical information integrated into a neural tagger are sufficient to reach a new state of the art, and both data selection and embeddings are essential components to boost neural tagging performance.",7 Conclusions,[0],[0]
"We introduce DSDS: a cross-lingual neural part-of-speech tagger that learns from disparate sources of distant supervision, and realistically scales to hundreds of low-resource languages.",abstractText,[0],[0]
"The model exploits annotation projection, instance selection, tag dictionaries, morphological lexicons, and distributed representations, all in a uniform framework.",abstractText,[0],[0]
"The approach is simple, yet surprisingly effective, resulting in a new state of the art without access to any gold annotated data.",abstractText,[0],[0]
Distant Supervision from Disparate Sources for Low-Resource Part-of-Speech Tagging,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1393–1402 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1393",text,[0],[0]
"Search-based structured prediction models the generation of natural language structure (part-ofspeech tags, syntax tree, translations, semantic graphs, etc.)",1 Introduction,[0],[0]
"as a search problem (Collins and Roark, 2004; Liang et al., 2006; Zhang and Clark, 2008; Huang et al., 2012; Sutskever et al., 2014; Goodman et al., 2016).",1 Introduction,[0],[0]
It has drawn a lot of research attention in recent years thanks to its competitive performance on both accuracy and running time.,1 Introduction,[0],[0]
A stochastic policy that controls the whole search process is usually learned by imitating a reference policy.,1 Introduction,[0],[0]
"The imitation is usually addressed as training a classifier to predict the ref-
∗*",1 Introduction,[0],[0]
"Email corresponding.
erence policy’s search action on the encountered states when performing the reference policy.",1 Introduction,[0],[0]
Such imitation process can sometimes be problematic.,1 Introduction,[0],[0]
"One problem is the ambiguities of the reference policy, in which multiple actions lead to the optimal structure but usually, only one is chosen as training instance (Goldberg and Nivre, 2012).",1 Introduction,[0],[0]
"Another problem is the discrepancy between training and testing, in which during the test phase, the learned policy enters non-optimal states whose search action is never learned (Ross and Bagnell, 2010; Ross et al., 2011).",1 Introduction,[0],[0]
"All these problems harm the generalization ability of search-based structured prediction and lead to poor performance.
",1 Introduction,[0],[0]
Previous works tackle these problems from two directions.,1 Introduction,[0],[0]
"To overcome the ambiguities in data, techniques like ensemble are often adopted (Di-
etterich, 2000).",1 Introduction,[0],[0]
"To mitigate the discrepancy, exploration is encouraged during the training process (Ross and Bagnell, 2010; Ross et al., 2011; Goldberg and Nivre, 2012; Bengio et al., 2015; Goodman et al., 2016).",1 Introduction,[0],[0]
"In this paper, we propose to consider these two problems in an integrated knowledge distillation manner (Hinton et al., 2015).",1 Introduction,[0],[0]
We distill a single model from the ensemble of several baselines trained with different initialization by matching the ensemble’s output distribution on the reference states.,1 Introduction,[0],[0]
We also let the ensemble randomly explore the search space and learn the single model to mimic ensemble’s distribution on the encountered exploration states.,1 Introduction,[0],[0]
Combing the distillation from reference and exploration further improves our single model’s performance.,1 Introduction,[0],[0]
"The workflow of our method is shown in Figure 1.
",1 Introduction,[0],[0]
We conduct experiments on two typical searchbased structured prediction tasks: transition-based dependency parsing and neural machine translation.,1 Introduction,[0],[0]
The results of both these two experiments show the effectiveness of our knowledge distillation method by outperforming strong baselines.,1 Introduction,[0],[0]
"In the parsing experiments, an improvement of 1.32 in LAS is achieved and in the machine translation experiments, such improvement is 2.65 in BLEU.",1 Introduction,[0],[0]
"Our model also outperforms the greedy models in previous works.
",1 Introduction,[0],[0]
"Major contributions of this paper include:
• We study the knowledge distillation in search-based structured prediction and propose to distill the knowledge of an ensemble into a single model by learning to match its distribution on both the reference states (§3.2) and exploration states encountered when using the ensemble to explore the search space (§3.3).",1 Introduction,[0],[0]
"A further combination of these two methods is also proposed to improve the performance (§3.4).
",1 Introduction,[0],[0]
• We conduct experiments on two search-based structured prediction problems: transitionbased dependency parsing and neural machine translation.,1 Introduction,[0],[0]
"In both these two problems, the distilled model significantly improves over strong baselines and outperforms other greedy structured prediction (§4.2).",1 Introduction,[0],[0]
Comprehensive analysis empirically shows the feasibility of our distillation method (§4.3).,1 Introduction,[0],[0]
Structured prediction maps an input x =,2.1 Search-based Structured Prediction,[0],[0]
"(x1, x2, ..., xn) to its structural output y = (y1, y2, ..., ym), where each component of y has some internal dependencies.",2.1 Search-based Structured Prediction,[0],[0]
"Search-based structured prediction (Collins and Roark, 2004; Daumé III et al., 2005; Daumé III et al., 2009; Ross and Bagnell, 2010; Ross et al., 2011; Doppa et al., 2014; Vlachos and Clark, 2014; Chang et al., 2015) models the generation of the structure as a search problem and it can be formalized as a tuple (S,A, T (s, a),S0,ST ), in which S is a set of states, A is a set of actions, T is a function that maps S × A → S, S0 is a set of initial states, and ST is a set of terminal states.",2.1 Search-based Structured Prediction,[0],[0]
"Starting from an initial state s0 ∈ S0, the structured prediction model repeatably chooses an action at ∈ A by following a policy π(s) and applies at to st and enter a new state st+1 as st+1 ← T (st, at), until a final state sT ∈ ST is achieved.",2.1 Search-based Structured Prediction,[0],[0]
"Several natural language structured prediction problems can be modeled under the search-based framework including dependency parsing (Nivre, 2008) and neural machine translation (Liang et al., 2006; Sutskever et al., 2014).",2.1 Search-based Structured Prediction,[0],[0]
"Table 1 shows the search-based structured prediction view of these two problems.
",2.1 Search-based Structured Prediction,[0],[0]
"In the data-driven settings, π(s) controls the whole search process and is usually parameterized by a classifier p(a | s) which outputs the proba-
Algorithm 1: Generic learning algorithm for search-based structured prediction.
",2.1 Search-based Structured Prediction,[0],[0]
"Input: training data: {x(n),y(n)}Nn=1; the reference policy: πR(s,y).
",2.1 Search-based Structured Prediction,[0],[0]
Output: classifier p(a|s).,2.1 Search-based Structured Prediction,[0],[0]
"1 D ← ∅; 2 for n← 1...N do 3 t← 0; 4 st ← s0(x(n)); 5 while st /∈ ST do 6 at ← πR(st,y(n)); 7 D ← D ∪ {st}; 8 st+1 ← T (st, at); 9 t← t+ 1;
10 end 11 end 12 optimize LNLL;
bility of choosing an action a on the given state s.",2.1 Search-based Structured Prediction,[0],[0]
The commonly adopted greedy policy can be formalized as choosing the most probable action with π(s) = argmaxa,2.1 Search-based Structured Prediction,[0],[0]
p(a | s) at test stage.,2.1 Search-based Structured Prediction,[0],[0]
"To learn an optimal classifier, search-based structured prediction requires constructing a reference policy πR(s,y), which takes an input state s, gold structure y and outputs its reference action a, and training p(a | s) to imitate the reference policy.",2.1 Search-based Structured Prediction,[0],[0]
"Algorithm 1 shows the common practices in training p(a | s), which involves: first, using πR(s,y) to generate a sequence of reference states and actions on the training data (line 1 to line 11 in Algorithm 1); second, using the states and actions on the reference sequences as examples to train p(a | s) with negative log-likelihood (NLL) loss (line 12 in Algorithm 1),
LNLL = ∑ s∈D ∑ a −1{a = πR} · log p(a | s)
where D is a set of training data.",2.1 Search-based Structured Prediction,[0],[0]
"The reference policy is sometimes sub-optimal and ambiguous which means on one state, there can be more than one action that leads to the optimal prediction.",2.1 Search-based Structured Prediction,[0],[0]
"In transition-based dependency parsing, Goldberg and Nivre (2012) showed that one dependency tree can be reached by several search sequences using Nivre (2008)’s arcstandard algorithm.",2.1 Search-based Structured Prediction,[0],[0]
"In machine translation, the ambiguity problem also exists because one source language sentence usually has multiple semantically correct translations but only one reference
translation is presented.",2.1 Search-based Structured Prediction,[0],[0]
"Similar problems have also been observed in semantic parsing (Goodman et al., 2016).",2.1 Search-based Structured Prediction,[0],[0]
"According to Frénay and Verleysen (2014), the widely used NLL loss is vulnerable to ambiguous data which make it worse for searchbased structured prediction.
",2.1 Search-based Structured Prediction,[0],[0]
"Besides the ambiguity problem, training and testing discrepancy is another problem that lags the search-based structured prediction performance.",2.1 Search-based Structured Prediction,[0],[0]
"Since the training process imitates the reference policy, all the states in the training data are optimal which means they are guaranteed to reach the optimal structure.",2.1 Search-based Structured Prediction,[0],[0]
"But during the test phase, the model can predict non-optimal states whose search action is never learned.",2.1 Search-based Structured Prediction,[0],[0]
The greedy search which is prone to error propagation also worsens this problem.,2.1 Search-based Structured Prediction,[0],[0]
"A cumbersome model, which could be an ensemble of several models or a single model with larger number of parameters, usually provides better generalization ability.",2.2 Knowledge Distillation,[0],[0]
"Knowledge distillation (Buciluǎ et al., 2006; Ba and Caruana, 2014; Hinton et al., 2015) is a class of methods for transferring the generalization ability of the cumbersome teacher model into a small student model.",2.2 Knowledge Distillation,[0],[0]
"Instead of optimizing NLL loss, knowledge distillation uses the distribution q(y | x) outputted by the teacher model as “soft target” and optimizes the knowledge distillation loss,
LKD = ∑ x∈D ∑ y −q(y",2.2 Knowledge Distillation,[0],[0]
"| x) · log p(y | x).
",2.2 Knowledge Distillation,[0],[0]
"In search-based structured prediction scenario, x corresponds to the state s and y corresponds to the action a. Through optimizing the distillation loss, knowledge of the teacher model is learned by the student model p(y | x).",2.2 Knowledge Distillation,[0],[0]
"When correct label is presented, NLL loss can be combined with the distillation loss via simple interpolation as
L = αLKD + (1− α)LNLL (1)",2.2 Knowledge Distillation,[0],[0]
"As Hinton et al. (2015) pointed out, although the real objective of a machine learning algorithm is to generalize well to new data, models are usually trained to optimize the performance on training data, which bias the model to the training data.
",3.1 Ensemble,[0],[0]
"In search-based structured prediction, such biases can result from either the ambiguities in the training data or the discrepancy between training and testing.",3.1 Ensemble,[0],[0]
"It would be more problematic to train p(a | s) using the loss which is in-robust to ambiguities and only considering the optimal states.
",3.1 Ensemble,[0],[0]
The effect of ensemble on ambiguous data has been studied in Dietterich (2000).,3.1 Ensemble,[0],[0]
They empirically showed that ensemble can overcome the ambiguities in the training data.,3.1 Ensemble,[0],[0]
Daumé III et al. (2005) also use weighted ensemble of parameters from different iterations as their final structure prediction model.,3.1 Ensemble,[0],[0]
"In this paper, we consider to use ensemble technique to improve the generalization ability of our search-based structured prediction model following these works.",3.1 Ensemble,[0],[0]
"In practice, we train M search-based structured prediction models with different initialized weights and ensemble them by the average of their output distribution as q(a | s) = 1M ∑ m qm(a | s).",3.1 Ensemble,[0],[0]
"In Section 4.3.1, we empirically show that the ensemble has the ability to choose a good search action in the optimal-yetambiguous states and the non-optimal states.",3.1 Ensemble,[0],[0]
"As we can see in Section 4, ensemble indeed improves the performance of baseline models.",3.2 Distillation from Reference,[0],[0]
"However, real world deployment is usually constrained by computation and memory resources.",3.2 Distillation from Reference,[0],[0]
"Ensemble requires running the structured prediction models for multiple times, and that makes it less applicable in real-world problem.",3.2 Distillation from Reference,[0],[0]
"To take the advantage of the ensemble model while avoid running the models multiple times, we use the knowledge distillation technique to distill a single model from the ensemble.",3.2 Distillation from Reference,[0],[0]
We started from changing the NLL learning objective in Algorithm 1 into the distillation loss (Equation 1) as shown in Algorithm 2.,3.2 Distillation from Reference,[0],[0]
"Since such method learns the model on the states produced by the reference policy, we name it as distillation from reference.",3.2 Distillation from Reference,[0],[0]
Blocks connected by in dashed red lines in Figure 1 show the workflow of our distillation from reference.,3.2 Distillation from Reference,[0],[0]
"In the scenario of search-based structured prediction, transferring the teacher model’s generalization ability into a student model not only includes matching the teacher model’s soft targets on the reference search sequence, but also imitating the search decisions made by the teacher model.",3.3 Distillation from Exploration,[0],[0]
"One way to accomplish the imitation can be sampling
Algorithm 2: Knowledge distillation for search-based structured prediction.
",3.3 Distillation from Exploration,[0],[0]
"Input: training data: {x(n),y(n)}Nn=1; the reference policy: πR(s,y); the exploration policy: πE(s) which samples an action from the annealed ensemble q(a | s) 1 T
Output: classifier p(a | s).",3.3 Distillation from Exploration,[0],[0]
1 D ← ∅; 2 for n← 1...N do 3 t← 0; 4 st ← s0(x(n));,3.3 Distillation from Exploration,[0],[0]
"5 while st /∈ ST do 6 if distilling from reference then 7 at ← πR(st,y(n)); 8 else 9 at ← πE(st);
10 end 11 D ← D ∪ {st}; 12 st+1 ← T (st, at); 13 t← t+ 1; 14 end 15 end 16 if distilling from reference then 17 optimize αLKD +",3.3 Distillation from Exploration,[0],[0]
"(1− α)LNLL; 18 else 19 optimize LKD; 20 end
search sequence from the ensemble and learn from the soft target on the sampled states.",3.3 Distillation from Exploration,[0],[0]
"More concretely, we change πR(s,y) into a policy πE(s) which samples an action a from q(a | s) 1 T , where T is the temperature that controls the sharpness of the distribution (Hinton et al., 2015).",3.3 Distillation from Exploration,[0],[0]
The algorithm is shown in Algorithm 2.,3.3 Distillation from Exploration,[0],[0]
"Since such distillation generate training instances from exploration, we name it as distillation from exploration.",3.3 Distillation from Exploration,[0],[0]
"Blocks connected by in solid blue lines in Figure 1 show the workflow of our distillation from exploration.
",3.3 Distillation from Exploration,[0],[0]
"On the sampled states, reference decision from πR is usually non-trivial to achieve, which makes learning from NLL loss infeasible.",3.3 Distillation from Exploration,[0],[0]
"In Section 4, we empirically show that fully distilling from the soft target, i.e. setting α = 1 in Equation 1, achieves comparable performance with that both from distillation and NLL.",3.3 Distillation from Exploration,[0],[0]
Distillation from reference can encourage the model to predict the action made by the reference policy and distillation from exploration learns the model on arbitrary states.,3.4 Distillation from Both,[0],[0]
They transfer the generalization ability of the ensemble from different aspects.,3.4 Distillation from Both,[0],[0]
Hopefully combining them can further improve the performance.,3.4 Distillation from Both,[0],[0]
"In this paper, we combine distillation from reference and exploration with the following manner: we use πR and πE to generate a set of training states.",3.4 Distillation from Both,[0],[0]
"Then, we learn p(a | s) on the generated states.",3.4 Distillation from Both,[0],[0]
"If one state was generated by the reference policy, we minimize the interpretation of distillation and NLL loss.",3.4 Distillation from Both,[0],[0]
"Otherwise, we minimize the distillation loss only.",3.4 Distillation from Both,[0],[0]
We perform experiments on two tasks: transitionbased dependency parsing and neural machine translation.,4 Experiments,[0],[0]
"Both these two tasks are converted to search-based structured prediction as Section 2.1.
",4 Experiments,[0],[0]
"For the transition-based parsing, we use the stack-lstm parsing model proposed by Dyer et al. (2015) to parameterize the classifier.1 For the neural machine translation, we parameterize the classifier as an LSTM encoder-decoder model by following Luong et al. (2015).2 We encourage the reader of this paper to refer corresponding papers for more details.",4 Experiments,[0],[0]
"We perform experiments on Penn Treebank (PTB) dataset with standard data split (Section 2-21 for training, Section 22 for development, and Section 23 for testing).",4.1.1 Transition-based Dependency Parsing,[0],[0]
Stanford dependencies are converted from the original constituent trees using Stanford CoreNLP 3.3.03 by following Dyer et al. (2015).,4.1.1 Transition-based Dependency Parsing,[0],[0]
Automatic part-of-speech tags are assigned by 10-way jackknifing whose accuracy is 97.5%.,4.1.1 Transition-based Dependency Parsing,[0],[0]
Labeled attachment score (LAS) excluding punctuation are used in evaluation.,4.1.1 Transition-based Dependency Parsing,[0],[0]
"For the other hyper-parameters, we use the same settings as Dyer et al. (2015).",4.1.1 Transition-based Dependency Parsing,[0],[0]
"The best iteration and α is determined on the development set.
",4.1.1 Transition-based Dependency Parsing,[0],[0]
"1The code for parsing experiments is available at: https://github.com/Oneplus/twpipe.
",4.1.1 Transition-based Dependency Parsing,[0],[0]
"2We based our NMT experiments on OpenNMT (Klein et al., 2017).",4.1.1 Transition-based Dependency Parsing,[0],[0]
"The code for NMT experiments is available at: https://github.com/Oneplus/OpenNMT-py.
3stanfordnlp.github.io/CoreNLP/ history.html
Reimers and Gurevych (2017) and others have pointed out that neural network training is nondeterministic and depends on the seed for the random number generator.",4.1.1 Transition-based Dependency Parsing,[0],[0]
"To control for this effect, they suggest to report the average of M differentlyseeded runs.",4.1.1 Transition-based Dependency Parsing,[0],[0]
"In all our dependency parsing, we set n = 20.",4.1.1 Transition-based Dependency Parsing,[0],[0]
"We conduct our experiments on a small machine translation dataset, which is the Germanto-English portion of the IWSLT 2014 machine translation evaluation campaign.",4.1.2 Neural Machine Translation,[0],[0]
"The dataset contains around 153K training sentence pairs, 7K development sentence pairs, and 7K testing sentence pairs.",4.1.2 Neural Machine Translation,[0],[0]
"We use the same preprocessing as Ranzato et al. (2015), which leads to a German vocabulary of about 30K entries and an English vocabulary of 25K entries.",4.1.2 Neural Machine Translation,[0],[0]
One-layer LSTM for both encoder and decoder with 256 hidden units are used by following Wiseman and Rush (2016).,4.1.2 Neural Machine Translation,[0],[0]
"BLEU (Papineni et al., 2002) was used to evaluate the translator’s performance.4 Like in the dependency parsing experiments, we run M = 10 differentlyseeded runs and report the averaged score.
",4.1.2 Neural Machine Translation,[0],[0]
Optimizing the distillation loss in Equation 1 requires enumerating over the action space.,4.1.2 Neural Machine Translation,[0],[0]
It is expensive for machine translation since the size of the action space (vocabulary) is considerably large (25K in our experiments).,4.1.2 Neural Machine Translation,[0],[0]
"In this paper, we use the K-most probable actions (translations on target side) on one state to approximate the whole probability distribution of q(a | s) as ∑ a q(a |
s) · log p(a | s)",4.1.2 Neural Machine Translation,[0],[0]
"≈ ∑K
k q(âk",4.1.2 Neural Machine Translation,[0],[0]
"| s) · log p(âk | s), where âk is the k-th probable action.",4.1.2 Neural Machine Translation,[0],[0]
"We fix α to
4We use multi-bleu.perl to evaluate our model’s performance
1 and vary K and evaluate the distillation model’s performance.",4.1.2 Neural Machine Translation,[0],[0]
"These results are shown in Figure 2 where there is no significant difference between different Ks and in speed consideration, we set K to 1 in the following experiments.",4.1.2 Neural Machine Translation,[0],[0]
Table 2 shows our PTB experimental results.,4.2.1 Transition-based Dependency Parsing,[0],[0]
"From this result, we can see that the ensemble model outperforms the baseline model by 1.90 in LAS.",4.2.1 Transition-based Dependency Parsing,[0],[0]
"For our distillation from reference, when setting α = 1.0, best performance on development set is achieved and the test LAS is 91.99.
",4.2.1 Transition-based Dependency Parsing,[0],[0]
We tune the temperature T during exploration and the results are shown in Figure 3.,4.2.1 Transition-based Dependency Parsing,[0],[0]
Sharpen the distribution during the sampling process generally performs better on development set.,4.2.1 Transition-based Dependency Parsing,[0],[0]
"Our distillation from exploration model gets almost the same performance as that from reference, but simply combing these two sets of data outperform both models by achieving an LAS of 92.14.
",4.2.1 Transition-based Dependency Parsing,[0],[0]
We also compare our parser with the other parsers in Table 2.,4.2.1 Transition-based Dependency Parsing,[0],[0]
The second group shows the greedy transition-based parsers in previous literatures.,4.2.1 Transition-based Dependency Parsing,[0],[0]
Andor et al. (2016) presented an alternative state representation and explored both greedy and beam search decoding.,4.2.1 Transition-based Dependency Parsing,[0],[0]
"(Ballesteros et al., 2016) explores training the greedy parser with dynamic oracle.",4.2.1 Transition-based Dependency Parsing,[0],[0]
Our distillation parser outperforms all these greedy counterparts.,4.2.1 Transition-based Dependency Parsing,[0],[0]
"The third group shows
parsers trained on different techniques including decoding with beam search (Buckman et al., 2016; Andor et al., 2016), training transitionbased parser with beam search (Andor et al., 2016), graph-based parsing (Dozat and Manning, 2016), distilling a graph-based parser from the output of 20 parsers (Kuncoro et al., 2016), and converting constituent parsing results to dependencies (Kuncoro et al., 2017).",4.2.1 Transition-based Dependency Parsing,[0],[0]
Our distillation parser still outperforms its transition-based counterparts but lags the others.,4.2.1 Transition-based Dependency Parsing,[0],[0]
We attribute the gap between our parser with the other parsers to the difference in parsing algorithms.,4.2.1 Transition-based Dependency Parsing,[0],[0]
Table 3 shows the experimental results on IWSLT 2014 dataset.,4.2.2 Neural Machine Translation,[0],[0]
"Similar to the PTB parsing results, the ensemble 10 translators outperforms the baseline translator by 3.47 in BLEU score.",4.2.2 Neural Machine Translation,[0],[0]
"Distilling from the ensemble by following the reference leads to a single translator of 24.76 BLEU score.
",4.2.2 Neural Machine Translation,[0],[0]
"Like in the parsing experiments, sharpen the distribution when exploring the search space is more helpful to the model’s performance but the differences when T ≤ 0.2 is not significant as shown in Figure 3.",4.2.2 Neural Machine Translation,[0],[0]
We set T = 0.1 in our distillation from exploration experiments since it achieves the best development score.,4.2.2 Neural Machine Translation,[0],[0]
Table 3 shows the exploration result of a BLEU score of 24.64 and it slightly lags the best reference model.,4.2.2 Neural Machine Translation,[0],[0]
"Distilling from both the reference and exploration improves the single model’s performance by a large margin and achieves a BLEU score of 25.44.
",4.2.2 Neural Machine Translation,[0],[0]
"We also compare our model with other translation models including the one trained with reinforcement learning (Ranzato et al., 2015) and that using beam search in training (Wiseman and Rush, 2016).",4.2.2 Neural Machine Translation,[0],[0]
"Our distillation translator outperforms these models.
",4.2.2 Neural Machine Translation,[0],[0]
Both the parsing and machine translation experiments confirm that it’s feasible to distill a reasonable search-based structured prediction model by just exploring the search space.,4.2.2 Neural Machine Translation,[0],[0]
Combining the reference and exploration further improves the model’s performance and outperforms its greedy structured prediction counterparts.,4.2.2 Neural Machine Translation,[0],[0]
"In Section 4.2, improvements from distilling the ensemble have been witnessed in both the transition-based dependency parsing and neural machine translation experiments.",4.3 Analysis,[0],[0]
"However, questions like “Why the ensemble works better? Is it feasible to fully learn from the distillation loss without NLL?",4.3 Analysis,[0],[0]
Is learning from distillation loss stable?” are yet to be answered.,4.3 Analysis,[0],[0]
"In this section, we first study the ensemble’s behavior on “problematic” states to show its generalization ability.",4.3 Analysis,[0],[0]
"Then, we empirically study the feasibility of fully learning from the distillation loss by studying the effect of α in the distillation from reference setting.",4.3 Analysis,[0],[0]
"Finally, we show that learning from distillation loss is less sensitive to initialization and achieves a more stable model.",4.3 Analysis,[0],[0]
"As mentioned in previous sections, “problematic” states which is either ambiguous or non-optimal harm structured prediciton’s performance.",4.3.1 Ensemble on “Problematic” States,[0],[0]
"Ensemble shows to improve the performance in Section 4.2, which indicates it does better on these states.",4.3.1 Ensemble on “Problematic” States,[0],[0]
"To empirically testify this, we use dependency parsing as a testbed and study the ensemble’s output distribution using the dynamic oracle.
",4.3.1 Ensemble on “Problematic” States,[0],[0]
"The dynamic oracle (Goldberg and Nivre, 2012; Goldberg et al., 2014) can be used to efficiently determine, given any state s, which transition action leads to the best achievable parse from s; if some errors may have already made, what is the best the parser can do, going forward?",4.3.1 Ensemble on “Problematic” States,[0],[0]
"This allows us to analyze the accuracy of each parser’s individual decisions, in the “problematic” states.",4.3.1 Ensemble on “Problematic” States,[0],[0]
"In this paper, we evaluate the output distributions of the baseline and ensemble parser against the reference actions suggested by the dynamic oracle.",4.3.1 Ensemble on “Problematic” States,[0],[0]
"Since dynamic oracle yields more than one reference actions due to ambiguities and previous mistakes and the output distribution can be treated as their scoring, we evaluate them as a ranking problem.",4.3.1 Ensemble on “Problematic” States,[0],[0]
"Intuitively, when multiple reference actions exist, a good parser should push probability mass to these actions.",4.3.1 Ensemble on “Problematic” States,[0],[0]
We draw problematic states by sampling from our baseline parser.,4.3.1 Ensemble on “Problematic” States,[0],[0]
The comparison in Table 4 shows that the ensemble model significantly outperforms the baseline on ambiguous and non-optimal states.,4.3.1 Ensemble on “Problematic” States,[0],[0]
"This observation indicates the ensemble’s output distribution is more “informative”, thus generalizes well on problematic states and achieves better performance.",4.3.1 Ensemble on “Problematic” States,[0],[0]
We also observe that the distillation model perform better than both the baseline and ensemble.,4.3.1 Ensemble on “Problematic” States,[0],[0]
We attribute this to the fact that the distillation model is learned from exploration.,4.3.1 Ensemble on “Problematic” States,[0],[0]
"Over our distillation from reference model, we study the effect of α in Equation 1.",4.3.2 Effect of α,[0],[0]
We vary α from 0 to 1 by a step of 0.1 in both the transitionbased dependency parsing and neural machine translation experiments and plot the model’s performance on development sets in Figure 4.,4.3.2 Effect of α,[0],[0]
Similar trends are witnessed in both these two experiments that model that’s configured with larger α generally performs better than that with smaller α.,4.3.2 Effect of α,[0],[0]
"For the dependency parsing problem, the best development performance is achieved when we set α = 1, and for the machine translation, the best α is 0.8.",4.3.2 Effect of α,[0],[0]
There is only 0.2 point of difference between the best α model and the one with α equals to 1.,4.3.2 Effect of α,[0],[0]
Such observation indicates that when distilling from the reference policy paying more attention to the distillation loss rather than the NLL is more beneficial.,4.3.2 Effect of α,[0],[0]
It also indicates that fully learning from the distillation loss outputted by the ensemble is reasonable because models configured with α = 1 generally achieves good performance.,4.3.2 Effect of α,[0],[0]
"Besides the improved performance, knowledge distillation also leads to more stable learning.",4.3.3 Learning Stability,[0],[0]
The performance score distributions of differentlyseed runs are depicted as violin plot in Figure 5.,4.3.3 Learning Stability,[0],[0]
Table 5 also reveals the smaller standard derivations are achieved by our distillation methods.,4.3.3 Learning Stability,[0],[0]
"As Keskar et al. (2016) pointed out that the general-
ization gap is not due to overfit, but due to the network converge to sharp minimizer which generalizes worse, we attribute the more stable training from our distillation model as the distillation loss presents less sharp minimizers.",4.3.3 Learning Stability,[0],[0]
Several works have been proposed to applying knowledge distillation to NLP problems.,5 Related Work,[0],[0]
Kim and Rush (2016) presented a distillation model which focus on distilling the structured loss from a large model into a small one which works on sequencelevel.,5 Related Work,[0],[0]
"In contrast to their work, we pay more attention to action-level distillation and propose to do better action-level distillation by both from reference and exploration.
",5 Related Work,[0],[0]
Freitag et al. (2017) used an ensemble of 6- translators to generate training reference.,5 Related Work,[0],[0]
Exploration was tried in their work with beam-search.,5 Related Work,[0],[0]
"We differ their work by training the single model
to match the distribution of the ensemble.",5 Related Work,[0],[0]
"Using ensemble in exploration was also studied in reinforcement learning community (Osband et al., 2016).",5 Related Work,[0],[0]
"In addition to distilling the ensemble on the labeled training data, a line of semisupervised learning works show that it’s effective to transfer knowledge of cumbersome model into a simple one on the unlabeled data (Liang et al., 2008; Li et al., 2014).",5 Related Work,[0],[0]
"Their extensions to knowledge distillation call for further study.
",5 Related Work,[0],[0]
Kuncoro et al. (2016) proposed to compile the knowledge from an ensemble of 20 transitionbased parsers into a voting and distill the knowledge by introducing the voting results as a regularizer in learning a graph-based parser.,5 Related Work,[0],[0]
"Different from their work, we directly do the distillation on the classifier of the transition-based parser.
",5 Related Work,[0],[0]
"Besides the attempts for directly using the knowledge distillation technique, Stahlberg and Byrne (2017) propose to first build the ensemble of several machine translators into one network by unfolding and then use SVD to shrink its parameters, which can be treated as another kind of knowledge distillation.",5 Related Work,[0],[0]
"In this paper, we study knowledge distillation for search-based structured prediction and propose to distill an ensemble into a single model both from reference and exploration states.",6 Conclusion,[0],[0]
Experiments on transition-based dependency parsing and machine translation show that our distillation method significantly improves the single model’s performance.,6 Conclusion,[0],[0]
Comparison analysis gives empirically guarantee for our distillation method.,6 Conclusion,[0],[0]
We thank the anonymous reviewers for their helpful comments and suggestions.,Acknowledgments,[0],[0]
This work was supported by the National Key Basic Research Program of China via grant 2014CB340503 and the National Natural Science Foundation of China (NSFC) via grant 61632011 and 61772153.,Acknowledgments,[0],[0]
Many natural language processing tasks can be modeled into structured prediction and solved as a search problem.,abstractText,[0],[0]
"In this paper, we distill an ensemble of multiple models trained with different initialization into a single model.",abstractText,[0],[0]
"In addition to learning to match the ensemble’s probability output on the reference states, we also use the ensemble to explore the search space and learn from the encountered states in the exploration.",abstractText,[0],[0]
Experimental results on two typical search-based structured prediction tasks – transition-based dependency parsing and neural machine translation show that distillation can effectively improve the single model’s performance and the final model achieves improvements of 1.32 in LAS and 2.65 in BLEU score on these two tasks respectively over strong baselines and it outperforms the greedy structured prediction models in previous literatures.,abstractText,[0],[0]
Distilling Knowledge for Search-based Structured Prediction,title,[0],[0]
"Over the last several years, the world has witnessed the emergence of data sets of an unprecedented scale across different scientific disciplines.",1. Introduction,[0],[0]
"This development has created a need for scalable, distributed machine learning algorithms to deal with the increasing amount of data.",1. Introduction,[0],[0]
"In this paper, we consider large-scale clustering or, more specifically, the task of finding provably good seedings for kMeans in a massive data setting.
",1. Introduction,[0],[0]
Seeding — the task of finding initial cluster centers — is critical to finding good clusterings for k-Means.,1. Introduction,[0],[0]
"In fact, the seeding step of the state of the art algorithm k-means++ (Arthur & Vassilvitskii, 2007) provides the
1Department of Computer Science, ETH Zurich.",1. Introduction,[0],[0]
"Correspondence to: Olivier Bachem <olivier.bachem@inf.ethz.ch>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"theoretical guarantee on the solution quality while the subsequent refinement using Lloyd’s algorithm (Lloyd, 1982) only guarantees that the quality does not deteriorate.",1. Introduction,[0],[0]
"While the k-means++ seeding step guarantees a solution that is O(log k) competitive with the optimal solution in expectation, it also requires k inherently sequential passes through the data set.",1. Introduction,[0],[0]
"This makes it unsuitable for the massive data setting where the data set is distributed across machines and computation has to occur in parallel.
",1. Introduction,[0],[0]
"As a remedy, Bahmani et al. (2012) propose the k-means‖ algorithm which produces seedings for kMeans with a reduced number of sequential iterations.",1. Introduction,[0],[0]
"Whereas k-means++ only samples a single cluster center in each of k rounds, k-means‖ samples in expectation ` points in each of t iterations.",1. Introduction,[0],[0]
"Provided t is small enough, this makes k-means‖ suitable for a distributed setting as the number of synchronizations is reduced.
",1. Introduction,[0],[0]
Our contributions.,1. Introduction,[0],[0]
"We provide a novel analysis of k-means‖ that bounds the expected solution quality for any number of rounds t and any oversampling factor ` ≥ k, the two parameters that need to be chosen in practice.",1. Introduction,[0],[0]
Our bound on the expected quantization error includes both a “traditional” multiplicative error term based on the optimal solution as well as a scale-invariant additive error term based on the variance of the data.,1. Introduction,[0],[0]
The key insight is that this additive error term vanishes at a rate of ( k e` )t if t or ` is increased.,1. Introduction,[0],[0]
"This shows that k-means‖ provides provably good clusterings even for a small, constant number of iterations and explains the commonly observed phenomenon that k-means‖ works very well even for small t.
We further provide a hard instance on which k-means‖ provably incurs an additive error based on the variance of the data and for which an exclusively multiplicative error guarantee cannot be achieved.",1. Introduction,[0],[0]
This implies that an additive error term such as the one in our analysis is in fact necessary if less than k − 1 rounds are employed.,1. Introduction,[0],[0]
k-Means clustering.,2. Background & related work,[0],[0]
Let X denote a set of points in Rd.,2. Background & related work,[0],[0]
"The k-Means clustering problem is to find a set C of k cluster centers in Rd that minimizes the quantization error
φX (C) = ∑ x∈X d(x,C)2 = ∑",2. Background & related work,[0],[0]
x∈X min,2. Background & related work,[0],[0]
"q∈C ‖x− q‖22.
Algorithm 1 k-means++ seeding Require: weighted data set (X , w), number of clusters k
1: C ← sample single x ∈ X with probability wx∑ x′∈X wx′ 2: for i = 2, . . .",2. Background & related work,[0],[0]
", k do 3: Sample x ∈ X with probability wx d(x,C)
2∑ x′∈X wx′ d(x",2. Background & related work,[0],[0]
"′,C)2
4: C ← C ∪ {x} 5: Return C
We denote the optimal quantization error by φOPT(X ) while the variance of the data is defined as Var(X ) = φX ({µ(X )}) where µ(X ) is the mean of X .
k-means++ seeding.",2. Background & related work,[0],[0]
"Given a data set X and any set of cluster centers C ⊂ X , the D2-sampling strategy selects a new center by sampling each point x ∈ X with probability
p(x) = d(x,C)2∑
x′∈X d(x ′, C ′)2
.
",2. Background & related work,[0],[0]
"The seeding step of k-means++ (Arthur & Vassilvitskii, 2007), detailed for potentially weighted data sets in Algorithm 1, selects an initial cluster center uniformly at random and then sequentially adds k − 1 cluster centers using D2 sampling whereby C is always the set of previously sampled centers.",2. Background & related work,[0],[0]
"Arthur & Vassilvitskii (2007) show that the solution quality φk-means++ of k-means++ seeding is bounded in expectation by
E[φk-means++] ≤ 8 (log2 k + 2)φOPT(X ).
",2. Background & related work,[0],[0]
The computational complexity of k-means++ seeding is O(nkd) where n is the number of data points and d the dimensionality.,2. Background & related work,[0],[0]
"Unfortunately, the iterations in k-means++ seeding are inherently sequential and, as a result, the algorithm requires k full passes through the data.",2. Background & related work,[0],[0]
"This makes the algorithm unsuitable for the distributed setting.
",2. Background & related work,[0],[0]
k-means‖ seeding.,2. Background & related work,[0],[0]
"As a remedy, Bahmani et al. (2012) propose the algorithm k-means‖ which aims to reduce the number of sequential iterations.",2. Background & related work,[0],[0]
"The key component of k-means‖ is detailed in Algorithm 2 in what we call k-means‖ overseeding: First, a data point is sampled as the first cluster center uniformly at random.",2. Background & related work,[0],[0]
"Then, in each of t sequential rounds, each data point x ∈ X is independently sampled with probability min ( 1, ` d(x,C) 2
φX (C)
) and
added to the set of sampled centers C at the end of the round.",2. Background & related work,[0],[0]
"The parameter ` ≥ 1 is called the oversampling factor and determines the expected number of sampled points in each iteration.
",2. Background & related work,[0],[0]
"At the end of Algorithm 2, one obtains an oversampled solution with t` cluster centers in expectation.",2. Background & related work,[0],[0]
"The full k-means‖ seeding algorithm as detailed in Algorithm 3 reduces such a solution to k centers as follows: First, each of the centers in the oversampled solution is weighted by the number of data points which are closer to it than the
Algorithm 2 k-means‖ overseeding Require: data set X , # rounds t, oversampling factor `
1: C ← sample a point uniformly at random from X 2: for i = 1, 2, . . .",2. Background & related work,[0],[0]
", t do 3: C ′",2. Background & related work,[0],[0]
← ∅ 4: for x ∈ X,2. Background & related work,[0],[0]
"do 5: Add x to C ′ with probability min ( 1, ` d(x,C) 2
φX (C) ) 6: C ← C ∪ C ′ 7: Return C
Algorithm 3 k-means‖ seeding Require: data set X , # rounds t, oversampling factor `
1: B ← Result of Algorithm 2 applied to (X , t, `) 2: for c ∈ B do 3:",2. Background & related work,[0],[0]
Xc ← points x ∈ X,2. Background & related work,[0],[0]
"whose closest center in B is c (ties broken arbitrarily but consistently) 4: wc ← |Xc| 5: C ← Result of Algorithm 1 applied to (B,w) 6:",2. Background & related work,[0],[0]
"Return C
other centers.",2. Background & related work,[0],[0]
"Then, k-means++ seeding is run on the weighted oversampled solution to produce a set of k final centers.",2. Background & related work,[0],[0]
"The total computational complexity of Algorithm 3 is O(nt`d) in expectation.
",2. Background & related work,[0],[0]
"The key intuition behind k-means‖ is that, if we choose a large oversampling factor `, the number of rounds t can be small — certainly much smaller than k, preferably even constant.",2. Background & related work,[0],[0]
The step in lines 4 and 5 in Algorithm 2 can be distributed over several machines and after each round the set C can be synchronized.,2. Background & related work,[0],[0]
"Due to the low number of synchronizations (i.e., rounds), Algorithm 2 can be efficiently run in a distributed setting.1
Other related work.",2. Background & related work,[0],[0]
Celebi et al. (2013) provide an overview over different seeding methods for k-Means.,2. Background & related work,[0],[0]
D2sampling and k-means++ style algorithms have been independently studied by both Ostrovsky et al. (2006) and Arthur & Vassilvitskii (2007).,2. Background & related work,[0],[0]
"This research direction has led to polynomial time approximation schemes based on D2-sampling (Jaiswal et al., 2014; 2015), constant factor approximations based on sampling more than k centers (Ailon et al., 2009; Aggarwal et al., 2009) and the analysis of hard instances (Arthur & Vassilvitskii, 2007; Brunsch & Röglin, 2011).",2. Background & related work,[0],[0]
"Recently, algorithms to approximate k-means++ seeding based on Markov Chain Monte Carlo have been proposed by Bachem et al. (2016b;a).",2. Background & related work,[0],[0]
"Finally, k-means++ has been used to construct coresets — small data set summaries — for k-Means clustering (Lucic et al., 2016; Bachem et al., 2015; Fichtenberger et al., 2013; Ackermann et al., 2012) and Gaussian mixture models (Lucic et al., 2017).
",2. Background & related work,[0],[0]
"1A popular choice is the MLLib library of Apache Spark (Meng et al., 2016) which uses k-means‖ by default.",2. Background & related work,[0],[0]
"In this section, we provide the intuition and the main results behind our novel analysis of k-means‖ and defer the formal statements and the formal proofs to Section 4.",3. Intuition and key results,[0],[0]
Solution quality of Algorithm 2.,3.1. Solution quality of k-means‖,[0],[0]
We first consider Algorithm 2 as it largely determines the final solution quality.,3.1. Solution quality of k-means‖,[0],[0]
"Algorithm 3 with its use of k-means++ to obtain the final k cluster centers, only adds an additional O(log k) factor as shown in Theorem 1.",3.1. Solution quality of k-means‖,[0],[0]
"Our key result is Lemma 4 (see Section 4) which guarantees that, for ` ≥ k, the expected error of solutions computed by Algorithm 2 is at most
E[φX (C)] ≤ 2 ( k
e`
)t Var(X ) + 26φOPT(X ).",3.1. Solution quality of k-means‖,[0],[0]
"(1)
The first term may be regarded as a scale-invariant additive error: It is additive as it does not depend on the optimal quantization error φOPT(X ).",3.1. Solution quality of k-means‖,[0],[0]
It is scale-invariant since both the variance and the quantization error are scaled by λ2 if we scale the data setX by λ > 0.,3.1. Solution quality of k-means‖,[0],[0]
"The second term is a “traditional” multiplicative error term based on the optimal quantization error.
",3.1. Solution quality of k-means‖,[0],[0]
"Given a fixed oversampling factor `, the additive error term decreases exponentially if the number of rounds t is increased.",3.1. Solution quality of k-means‖,[0],[0]
"Similarly, for a fixed number of rounds t, it decreases polynomially at a rate O ( 1 `t )",3.1. Solution quality of k-means‖,[0],[0]
if the over sampling factor ` is increased.,3.1. Solution quality of k-means‖,[0],[0]
This result implies that even for a constant number of rounds one may obtain good clusterings by increasing the oversampling factor `.,3.1. Solution quality of k-means‖,[0],[0]
"This explains the empirical observation that often even a low number of rounds t is sufficient and that increasing ` increases the solution quality (Bahmani et al., 2012).",3.1. Solution quality of k-means‖,[0],[0]
The practical implications of this result are non-trivial: Even for the choice of t = 5 and ` = 5k one retains at most 0.0004% of the variance as an additive error.,3.1. Solution quality of k-means‖,[0],[0]
"Furthermore, state of the art uniform deviation bounds for k-Means include a similar additive error term (Bachem et al., 2017).
",3.1. Solution quality of k-means‖,[0],[0]
Comparison to previous result.,3.1. Solution quality of k-means‖,[0],[0]
Bahmani et al. (2012) show the following result: Let C be the set returned by Algorithm 2 with t rounds.,3.1. Solution quality of k-means‖,[0],[0]
For α = exp ( −(1− e−`/(2k)) ),3.1. Solution quality of k-means‖,[0],[0]
"≈ e− `2k , Corollary 3 of Bahmani et al. (2012) bounds the expected quality of C by
E[φX (C)] ≤",3.1. Solution quality of k-means‖,[0],[0]
"( 1 + α
2
)t ψ + 16
1− α φOPT(X ), (2)
where ψ denotes the quantization error of X based on the first, uniformly sampled center in k-means‖.",3.1. Solution quality of k-means‖,[0],[0]
"The key difference compared to our result is as follows: First, even as we increase `, the factor α is always non-negative.",3.1. Solution quality of k-means‖,[0],[0]
"Hence, regardless of the choice of `, the additive ψ term is reduced
by at most 12 per round.",3.1. Solution quality of k-means‖,[0],[0]
2,3.1. Solution quality of k-means‖,[0],[0]
"This means that, given the analysis in Bahmani et al. (2012), one would always obtain a constant additive error for a constant number of rounds t, even as ` is increased.
",3.1. Solution quality of k-means‖,[0],[0]
Guarantee for Algorithm 3.,3.1. Solution quality of k-means‖,[0],[0]
Our main result — Theorem 1 — bounds the expected quality of solutions produced by Algorithm 3.,3.1. Solution quality of k-means‖,[0],[0]
"As in Bahmani et al. (2012), one loses another factor ofO(ln k) compared to (1) due to Algorithm 3.",3.1. Solution quality of k-means‖,[0],[0]
Theorem 1.,3.1. Solution quality of k-means‖,[0],[0]
"Let k ∈ N, t ∈ N",3.1. Solution quality of k-means‖,[0],[0]
and ` ≥ k. Let X be a data set in Rd and C be the set returned by Algorithm 3.,3.1. Solution quality of k-means‖,[0],[0]
"Then,
E[φX (C)] ≤",3.1. Solution quality of k-means‖,[0],[0]
"O
(( k
e`
)t ln k ) Var(X )+O(ln k)φOPT(X ).",3.1. Solution quality of k-means‖,[0],[0]
We consider the case t < k−1 which captures the scenario where k-means‖ is useful in practice as for t ≥,3.2. A hard instance for k-means‖,[0],[0]
"k one may simply use k-means++ instead.
",3.2. A hard instance for k-means‖,[0],[0]
Theorem 2.,3.2. A hard instance for k-means‖,[0],[0]
"For any β > 0, k ∈ N, t < k",3.2. A hard instance for k-means‖,[0],[0]
"− 1 and ` ≥ 1, there exists a data set X of size 2(t+ 1) such that
E[φX (C)] ≥ 1
4 (4`t)
−t Var(X ),
where C is the output of Algorithm 2 or Algorithm 3 applied to X with t and `.",3.2. A hard instance for k-means‖,[0],[0]
"Furthermore,
Var(X )",3.2. A hard instance for k-means‖,[0],[0]
"> 0, φOPT(X ) = 0 and n∆2 ≤ β
where ∆ is the largest distance between any points in X .
",3.2. A hard instance for k-means‖,[0],[0]
Theorem 2 shows that there exists a data set on which k-means‖ provably incurs a non-negligible error even if the optimal quantization error is zero.,3.2. A hard instance for k-means‖,[0],[0]
This implies that k-means‖ with t < k,3.2. A hard instance for k-means‖,[0],[0]
− 1 cannot provide a multiplicative guarantee on the expected quantization error for general data sets.,3.2. A hard instance for k-means‖,[0],[0]
We thus argue that an additive error bound such as the one in Theorem 1 is required.,3.2. A hard instance for k-means‖,[0],[0]
"We note that the upper bound in (1) and the lower bound in Theorem 2 exhibit the same 1`t dependence on the oversampling factor ` for a given number of rounds t.
Furthermore, Theorem 2 implies that, for general data sets, k-means‖ cannot achieve the multiplicative error of O(log k) in expectation as claimed by Bahmani et al. (2012).3 In particular, if the optimal quantization error is
2Note that E[ψ] ≤ 2Var(X ) (Arthur & Vassilvitskii, 2007).",3.2. A hard instance for k-means‖,[0],[0]
"3To see this, let ψ = φX (c1) be the quantization error of the first sampled center in Algorithm 2 and choose β small enough such that the choice of t ∈ O(logψ) leads to t < k",3.2. A hard instance for k-means‖,[0],[0]
− 1.,3.2. A hard instance for k-means‖,[0],[0]
"For X in Theorem 2, φOPT(X ) = 0 which implies that the desired multiplicative guarantee would require E[φX (C)] = 0.",3.2. A hard instance for k-means‖,[0],[0]
"However, the non-negligible, additive error in Theorem 2 and Var(X )",3.2. A hard instance for k-means‖,[0],[0]
> 0 implies that E[φX (C)],3.2. A hard instance for k-means‖,[0],[0]
"> 0.
zero, then k-means‖ would need to return a solution with quantization error zero.",3.2. A hard instance for k-means‖,[0],[0]
"While we are guaranteed to remove a constant fraction of the error in each round, the number of required iterations may be unbounded.",3.2. A hard instance for k-means‖,[0],[0]
Proof of Theorem 1.,4. Theoretical analysis,[0],[0]
"The proof is divided into four steps: First, we relate k-means‖-style oversampling to k-means++-style D2-sampling in Lemmas 1 and 2.",4. Theoretical analysis,[0],[0]
"Second, we analyze a single iteration of Algorithm 2 in Lemma 3.",4. Theoretical analysis,[0],[0]
"Third, we bound the expected solution quality of Algorithm 2 in Lemma 4.",4. Theoretical analysis,[0],[0]
"Finally, we use this to bound the expected solution quality of Algorithm 3 in Theorem 1.
Lemma 1.",4. Theoretical analysis,[0],[0]
Let A be a finite set and let,4. Theoretical analysis,[0],[0]
"f : 2A → R be a set function that is non-negative and monotonically decreasing, i.e., f(V )",4. Theoretical analysis,[0],[0]
"≥ f(U) ≥ 0, for all V ⊆ U .
",4. Theoretical analysis,[0],[0]
"Let P be a probability distribution where, for each a ∈ A, Ea denotes an independent event that occurs with probability qa ∈",4. Theoretical analysis,[0],[0]
"[0, 1].",4. Theoretical analysis,[0],[0]
"Let C be the set of elements a ∈ A for which the event Ea occurs.
",4. Theoretical analysis,[0],[0]
Let Q be the probability distribution on A where,4. Theoretical analysis,[0],[0]
"a single a ∈ A is sampled with probability qa/ ∑ a∈A qa.
",4. Theoretical analysis,[0],[0]
"Then, with ∅ denoting the empty set, we have that
EP [f(C)] ≤ EQ[f({a})]",4. Theoretical analysis,[0],[0]
"+ e− ∑ a∈A qaf(∅).
",4. Theoretical analysis,[0],[0]
Proof.,4. Theoretical analysis,[0],[0]
"To prove the claim, we first construct a series of sub-events of the events {Ea}a∈A and then use them to recursively bound EP [f(C)].
",4. Theoretical analysis,[0],[0]
Let m ∈ N.,4. Theoretical analysis,[0],[0]
"For each a ∈ A, let ia be an independent random variable drawn uniformly at random from {1, 2, . . .",4. Theoretical analysis,[0],[0]
",m}.",4. Theoretical analysis,[0],[0]
"For each a ∈ A and i = 1, 2, . . .",4. Theoretical analysis,[0],[0]
",m, let Fai be an independent event that occurs with probability
P[Fai] = (
1− qa m
)i−1 .
",4. Theoretical analysis,[0],[0]
"For each a ∈ A and i = 1, 2, . . .",4. Theoretical analysis,[0],[0]
",m, denote by Eai the event that occurs if i = ia and both Ea and Fai occur.",4. Theoretical analysis,[0],[0]
"By design, all these events are independent and thus
P[Eai] = P[Ea]P[Fai]P[ia = i] =",4. Theoretical analysis,[0],[0]
"qa m
( 1− qa
m
)i−1 (3)
for each a ∈ A and i = 1, 2, . . .",4. Theoretical analysis,[0],[0]
",m. Furthermore, for any a, a′ ∈ A with a 6= a′ and any i, i′ ∈ {1, 2, . . .",4. Theoretical analysis,[0],[0]
",m}, the events Eai and Ea′i′ are independent.
",4. Theoretical analysis,[0],[0]
"For i = 1, 2, . . .",4. Theoretical analysis,[0],[0]
",m let Gi be the event that none of the events {Eai′}a∈A,i′≤i occur, i.e.,
Gi =",4. Theoretical analysis,[0],[0]
⋂ i′≤i ⋂ a∈A,4. Theoretical analysis,[0],[0]
"Eai′
where A denotes the complement of A. For convenience, let G0 be the event that occurs with probability one.
",4. Theoretical analysis,[0],[0]
"Let (a1, a2, . . .",4. Theoretical analysis,[0],[0]
", a|A|) be any enumeration of A.",4. Theoretical analysis,[0],[0]
"For i = 1, 2, . . .",4. Theoretical analysis,[0],[0]
",m and j = 1, 2, . . .",4. Theoretical analysis,[0],[0]
", |A|+1, define the event
Gi,j = Gi−1 ∩ ⋂
0<j′<j
Eaj′ i.
We note that by definitionGi,1 = Gi−1 andGi,|A|+1 = Gi for i = 1, 2, . . .",4. Theoretical analysis,[0],[0]
",m.
For i = 1, 2, . . .",4. Theoretical analysis,[0],[0]
",m and j = 1, 2, . . .",4. Theoretical analysis,[0],[0]
", |A|, we have E[f(C)|Gi,j ] =",4. Theoretical analysis,[0],[0]
"P [ Eaji | Gi,j ] E",4. Theoretical analysis,[0],[0]
[ f(C),4. Theoretical analysis,[0],[0]
"| Eaji ∩Gij ] + P [ Eaj′ i | Gij ] E[f(C) | Gi,j+1].
(4)
We now bound the individual terms.",4. Theoretical analysis,[0],[0]
"The event Gi,j implies that the events {Eaji′}i′<i did not occur.",4. Theoretical analysis,[0],[0]
"Furthermore, Eaji is independent of the events {Eaj′ i′}i′=1,2,...,m for j′ 6=",4. Theoretical analysis,[0],[0]
"j. Hence, we have
P",4. Theoretical analysis,[0],[0]
"[ Eaji | Gi,j ] = P [ Eaji | G0 ∩ ⋂ i′<i Eaji′ ]
= P",4. Theoretical analysis,[0],[0]
"[ Eaji ] P [ G0 ∩ ⋂ i′<iEaji′
] = P [ Eaji ] 1− P",4. Theoretical analysis,[0],[0]
"[⋃ i′<iEaji′
] = P [ Eaji ] 1− ∑ i′<i P [ Eaji′ ] ,
(5)
where the last equality follows since the events {Eaji′}i′<i are disjoint.",4. Theoretical analysis,[0],[0]
"Using (3), we observe that ∑ i′<i P [ Eaji′ ] is a sum of a finite geometric series and we have∑ i′<i P [ Eaji′ ] = ∑ i′<i qa m ( 1− qa m
)i′−1 =",4. Theoretical analysis,[0],[0]
qa m 1− ( 1− qam ),4. Theoretical analysis,[0],[0]
i−1 1−,4. Theoretical analysis,[0],[0]
"( 1− qam
)",4. Theoretical analysis,[0],[0]
"= 1− ( 1− qa
m
)",4. Theoretical analysis,[0],[0]
"i−1 .
",4. Theoretical analysis,[0],[0]
"Together with (3) and (5), this implies
P",4. Theoretical analysis,[0],[0]
"[ Eaji | Gi,j ] = qa m
( 1− qam )",4. Theoretical analysis,[0],[0]
"i−1( 1− qam
)i−1 = qam.",4. Theoretical analysis,[0],[0]
(6) The event Eaji implies that C contains aj .,4. Theoretical analysis,[0],[0]
"Hence, since f is monotonically decreasing, we have
E [ f(C) | Eaji ∩Gij ] ≤ f({aj}).
",4. Theoretical analysis,[0],[0]
"Using (4) and (6), this implies
E[f(C)|Gi,j ] ≤ qaj m f({aj})+
( 1−
qaj m
) E[f(C)",4. Theoretical analysis,[0],[0]
"| Gi,j+1].
",4. Theoretical analysis,[0],[0]
"Applying this result iteratively for j = 1, 2, . . .",4. Theoretical analysis,[0],[0]
", |A| implies E[f(C)|Gi,1] = |A|∑ j=1 qaj m ∏",4. Theoretical analysis,[0],[0]
j′<j ( 1− qaj′ m ) f({aj}),4. Theoretical analysis,[0],[0]
"+
 |A|∏ j=1 ( 1− qaj m )E[f(C) | Gi,|A|+1].",4. Theoretical analysis,[0],[0]
Note that 0 ≤ 1,4. Theoretical analysis,[0],[0]
− qam ≤ 1 for all a ∈ A and that f is non-negative.,4. Theoretical analysis,[0],[0]
"This implies that for i = 1, 2, . . .",4. Theoretical analysis,[0],[0]
",m
E[f(C)|Gi,1] ≤ ∑ a∈A qa m f({a}) + c · E [ f(C)|Gi,|A|+1 ] where
c = ∏ a∈A",4. Theoretical analysis,[0],[0]
"( 1− qa m ) .
",4. Theoretical analysis,[0],[0]
"Since Gi,1 = Gi−1 and Gi,|A|+1 = Gi, we have for i = 1, 2, . . .",4. Theoretical analysis,[0],[0]
",m
E[f(C)|Gi−1] ≤ ∑ a∈A qa m f({a}) + c ·",4. Theoretical analysis,[0],[0]
"E[f(C)|Gi].
",4. Theoretical analysis,[0],[0]
"Applying this result iteratively, we obtain
E[f(C)] ≤",4. Theoretical analysis,[0],[0]
( m∑ i=1,4. Theoretical analysis,[0],[0]
"ci−1 )∑ a∈A qa m f({a}) + cm · f(∅).
",4. Theoretical analysis,[0],[0]
"Since 0 ≤ c ≤ 1, we have m∑ i=1",4. Theoretical analysis,[0],[0]
ci−1 ≤ ∞∑ i=1,4. Theoretical analysis,[0],[0]
"ci−1 = 1 1− c .
",4. Theoretical analysis,[0],[0]
For x ∈,4. Theoretical analysis,[0],[0]
"[−1, 0]",4. Theoretical analysis,[0],[0]
"it holds that log(1 + x) ≤ x and hence
cm = ∏ a∈A ( 1− qa m )m = exp ( m ∑ a∈A log ( 1− qa m ))
≤ exp ( −m
∑ a∈A qa m
) = e− ∑ a∈A qa .
",4. Theoretical analysis,[0],[0]
"This implies that
E[f(C)]",4. Theoretical analysis,[0],[0]
≤ 1 1− c ∑ a∈A,4. Theoretical analysis,[0],[0]
qa m f({a})+e− ∑ a∈A qa ·f(∅).,4. Theoretical analysis,[0],[0]
"(7)
We show the main claim by contradiction.",4. Theoretical analysis,[0],[0]
Assume,4. Theoretical analysis,[0],[0]
"that
EP [f(C)]",4. Theoretical analysis,[0],[0]
> EQ[f({a})],4. Theoretical analysis,[0],[0]
"+ e− ∑ a∈A qaf(∅).
",4. Theoretical analysis,[0],[0]
"If EQ[f({a})] = 0, the contradiction follows directly from (7).",4. Theoretical analysis,[0],[0]
"Otherwise, EQ[f({a})]",4. Theoretical analysis,[0],[0]
"> 0 implies that there exists an > 0 such that
EP [f(C)]",4. Theoretical analysis,[0],[0]
> (1 + )EQ[f({a})],4. Theoretical analysis,[0],[0]
+ e− ∑ a∈A qaf(∅).,4. Theoretical analysis,[0],[0]
"(8)
By definition, we have
c = ∏ a∈A",4. Theoretical analysis,[0],[0]
( 1− qa m ),4. Theoretical analysis,[0],[0]
= 1− ∑ a∈A,4. Theoretical analysis,[0],[0]
qa m + o,4. Theoretical analysis,[0],[0]
"( 1 m ) .
",4. Theoretical analysis,[0],[0]
"Thus, there exists a m ∈ N sufficiently large such that
c = 1− ∑ a∈A qa m + o",4. Theoretical analysis,[0],[0]
( 1 m ) ≤,4. Theoretical analysis,[0],[0]
"1− 1 1 + ∑ a∈A qa m .
",4. Theoretical analysis,[0],[0]
"Together with (7), this implies
E[f(C)]",4. Theoretical analysis,[0],[0]
≤ 1 + ∑ a∈A qa m ∑ a∈A qa m f({a}),4. Theoretical analysis,[0],[0]
"+ e− ∑ a∈A qa · f(∅)
= (1 + )",4. Theoretical analysis,[0],[0]
EQ[f({a})],4. Theoretical analysis,[0],[0]
"+ e− ∑ a∈A qaf(∅).
which is a contradiction to (8) and thus proves the claim.
",4. Theoretical analysis,[0],[0]
"Lemma 2 extends Lemma 1 to k-means‖-style sampling probabilities of the form qa = min (1, `pa).
",4. Theoretical analysis,[0],[0]
Lemma 2.,4. Theoretical analysis,[0],[0]
Let ` ≥ 1.,4. Theoretical analysis,[0],[0]
LetA be a finite set and let,4. Theoretical analysis,[0],[0]
"f : 2A → R be a set function that is non-negative and monotonically decreasing, i.e., f(V )",4. Theoretical analysis,[0],[0]
"≥ f(U) ≥ 0, for all V ⊆ U .",4. Theoretical analysis,[0],[0]
"For each a ∈ A, let pa ≥ 0",4. Theoretical analysis,[0],[0]
"and ∑ a∈A pa ≤ 1.
",4. Theoretical analysis,[0],[0]
"Let P be the probability distribution where, for each a ∈ A, Ea denotes an independent event that occurs with probability qa = min (1, `pa).",4. Theoretical analysis,[0],[0]
"Let C be the set of elements a ∈ A for which the event Ea occurs.
",4. Theoretical analysis,[0],[0]
Let Q be the probability distribution on A where,4. Theoretical analysis,[0],[0]
"a single a ∈ A is sampled with probability pa/ ∑ a∈A pa.
",4. Theoretical analysis,[0],[0]
"Then, with ∅ denoting the empty set, we have that
EP [f(C)] ≤ 2EQ[f({a})]",4. Theoretical analysis,[0],[0]
"+ e−` ∑ a∈A paf(∅).
",4. Theoretical analysis,[0],[0]
Proof.,4. Theoretical analysis,[0],[0]
LetA1 be the set of elements a ∈,4. Theoretical analysis,[0],[0]
A such that `pa ≤ 1 and A2 the set of elements a ∈,4. Theoretical analysis,[0],[0]
A such that `pa > 1.,4. Theoretical analysis,[0],[0]
"By definition, every element in A2 is sampled almost surely, i.e., A2 ⊆ C.",4. Theoretical analysis,[0],[0]
"This implies that almost surely
f(C) = f (A2 ∪ (C ∩A1)) .",4. Theoretical analysis,[0],[0]
"(9)
If |A1|= 0, the result follows trivially since
EP [f(C)] = f(A2) = EQ[f({a})].
",4. Theoretical analysis,[0],[0]
"Similarly, if |A2|= 0, the result follows directly from Lemma 1 with qa = `pa.",4. Theoretical analysis,[0],[0]
"For the remainder of the proof, we may thus assume that both A1 and A2 are non-empty.
",4. Theoretical analysis,[0],[0]
"For a ∈ A1, let qa = `pa and define the non-negative and monotonically decreasing function
g(C) = f (A2 ∪ C) .
",4. Theoretical analysis,[0],[0]
Let p1 = ∑ a∈A1 pa and p2 = ∑ a∈A2 pa.,4. Theoretical analysis,[0],[0]
"Lemma 1 applied to A1, qa and g implies that
EP [f(C)]",4. Theoretical analysis,[0],[0]
"= E[g(C)] ≤ ∑ a∈A1 pa p1 g({a}) + e−`p1g(∅).
",4. Theoretical analysis,[0],[0]
"(10)
Let d = ( 1− e−`p2 ) e−`p1
and define α =
p2 p1 + p2 − p1 p1 + p2 d.
By design, α ≤ 1.",4. Theoretical analysis,[0],[0]
"Furthermore
`p1 ≥ log `p1.
SinceA2 is nonempty and pa ≥ 1` for all a ∈ A2, it follows that p2 ≥ 1` .",4. Theoretical analysis,[0],[0]
"This implies
e`p1 ≥ `p1 ≥ p1 p2 .
",4. Theoretical analysis,[0],[0]
"Since 0 ≤ ( 1− e−`p2 ) ≤ 1, we have
p2 ≥ p1e−`p1 ≥",4. Theoretical analysis,[0],[0]
"p1 ( 1− e−`p2 ) e−`p1 = p1d.
",4. Theoretical analysis,[0],[0]
"Hence,
α = p2 p1 + p2 − p1",4. Theoretical analysis,[0],[0]
"p1 + p2
( 1− e−`p2 ) e−`p1 ≥ 0.
",4. Theoretical analysis,[0],[0]
Since α ∈,4. Theoretical analysis,[0],[0]
"[0, 1] and g({a}) ≤ g(∅) for any a ∈ A1, we may write (10), i.e., EP [f(C)] ≤ (1− α) ∑ a∈A1 pa p1 g({a})",4. Theoretical analysis,[0],[0]
+ ( α+ e−`p1 ),4. Theoretical analysis,[0],[0]
"g(∅).
(11)
",4. Theoretical analysis,[0],[0]
"By definition, we have
1− α = 1− p2 p1 + p2 + p1 p1 + p2 d = p1 p1 + p2 (1 + d).
",4. Theoretical analysis,[0],[0]
"Since g({a}) ≤ f({a}), we thus have
(1− α) ∑ a∈A1 pa p1 g({a}) ≤ (1 + d) ∑ a∈A1 pa p1 + p2 f({a}).
",4. Theoretical analysis,[0],[0]
"(12)
Similarly, we have
α+ e−`p1 = p2 p1 + p2 − p1 p1 + p2 d+ e−`p1
= p2
p1 + p2 + d p2 p1 + p2 − d+ e−`p1
= (1 + d)",4. Theoretical analysis,[0],[0]
"p2
p1 + p2 + e−`(p1+p2).
",4. Theoretical analysis,[0],[0]
"Since g(∅) ≤ f(∅), it follows that( α+ e−`p1 ) g(∅) ≤ (1+d) p2
p1 + p2 g(∅)+e−`(p1+p2)f(∅).
",4. Theoretical analysis,[0],[0]
(13) Since g(∅) = f (A2) and thus g(∅) ≤,4. Theoretical analysis,[0],[0]
"f({a}) for all a ∈ A2, we have
p2g(∅) = ∑ a∈A2 pag(∅) ≤ ∑",4. Theoretical analysis,[0],[0]
a∈A2 paf({a}).,4. Theoretical analysis,[0],[0]
"(14)
Combining (11), (12), (13), and (14) leads to
EP [f(C)] ≤ (1 + d)EQ[f({a})]",4. Theoretical analysis,[0],[0]
"+ e−` ∑ a∈A paf(∅).
",4. Theoretical analysis,[0],[0]
"Since p1 ≥ 0, we have 1 + d = 1 + ( 1− e−`p2 ) e−`p1 ≤ 2
which proves the main claim.
",4. Theoretical analysis,[0],[0]
"Lemma 3 bounds the solution quality after each iteration of Algorithm 2 based on the solution before the iteration.
",4. Theoretical analysis,[0],[0]
Lemma 3.,4. Theoretical analysis,[0],[0]
Let k ∈,4. Theoretical analysis,[0],[0]
N,4. Theoretical analysis,[0],[0]
and ` ≥ 1.,4. Theoretical analysis,[0],[0]
Let X be a data set in Rd and denote by φOPT(X ),4. Theoretical analysis,[0],[0]
the optimal k-Means clustering cost.,4. Theoretical analysis,[0],[0]
LetC denote the set of cluster centers at the beginning of an iteration in Algorithm 2 and C ′,4. Theoretical analysis,[0],[0]
the random set added in the iteration.,4. Theoretical analysis,[0],[0]
"Then, it holds that
E[φX (C ∪ C ′)] ≤",4. Theoretical analysis,[0],[0]
"( k
e`
) φX (C) + 16φOPT(X ).
",4. Theoretical analysis,[0],[0]
Proof.,4. Theoretical analysis,[0],[0]
The proof relies on applying Lemma 2 to each cluster of the optimal solution.,4. Theoretical analysis,[0],[0]
Let OPT denote any clustering achieving the minimal cost φOPT(X ) on X .,4. Theoretical analysis,[0],[0]
We assign all the points x ∈ X to their closest cluster center in OPT with ties broken arbitrarily but consistently.,4. Theoretical analysis,[0],[0]
"For c ∈ OPT we denote by Xc the subset of X assigned to c. For each c ∈ OPT, let
C ′c = C ′ ∩",4. Theoretical analysis,[0],[0]
"Xc.
",4. Theoretical analysis,[0],[0]
"By definition, a ∈ Xc is included in C ′c with probability
qa = min
( 1,
`d(a,C)2∑ a′∈X d(a ′, C)2
) .
",4. Theoretical analysis,[0],[0]
"For each c ∈ OPT, we define the monotonically decreasing function fc : 2Xc → R≥0 to be
fc(C ′",4. Theoretical analysis,[0],[0]
"c) = φXc(C ∪ C ′c).
",4. Theoretical analysis,[0],[0]
"For each c ∈ OPT, Lemma 2 applied to Xc, C ′c and fc implies
E[fc(C ′c)] ≤2 ∑ a∈Xc d(a,C)2∑ a′∈Xc d(a ′, C)2 fc({a})
+ e",4. Theoretical analysis,[0],[0]
"−`
∑ a∈Xc d(a,C)
2∑ a′∈X d(a
′,C)2 fc(∅).
",4. Theoretical analysis,[0],[0]
"(15)
Since fc({a}) = φXc(C ∪ {a}), the first term is equivalent to sampling a single element from Xc using D2 sampling.",4. Theoretical analysis,[0],[0]
"Hence, by Lemma 3.3 of Arthur & Vassilvitskii (2007) we have for all c ∈ OPT∑
a∈Xc
d(a,C)2∑ a′∈Xc d(a ′, C)2 fc({a}).",4. Theoretical analysis,[0],[0]
≤ 8φOPT(Xc).,4. Theoretical analysis,[0],[0]
"(16)
For each c ∈ OPT, we further have
e −`
∑ a∈Xc d(a,C)
",4. Theoretical analysis,[0],[0]
"2∑ a′∈X d(a
′,C)2 fc(∅) =",4. Theoretical analysis,[0],[0]
"e−`ucucφX (C).
",4. Theoretical analysis,[0],[0]
"where
uc =
∑ a∈Xc d(a,C)
2∑ a′∈X d(a ′, C)2 = φXc(C) φX (C) .
",4. Theoretical analysis,[0],[0]
"We have that
log `uc ≤",4. Theoretical analysis,[0],[0]
"`uc − 1 ⇐⇒ `uc ≤ e`uc
e
which implies
e−`ucucφX (C) ≤ 1
e` φX (C).",4. Theoretical analysis,[0],[0]
"(17)
Combining (15), (16) and (17), we obtain
E[fc(C ′c)] ≤16φOPT(Xc)",4. Theoretical analysis,[0],[0]
"+ 1
e` φX (C).",4. Theoretical analysis,[0],[0]
"(18)
",4. Theoretical analysis,[0],[0]
"Since E[φX (C ∪ C ′)] ≤ ∑ c∈OPT E[fc(C ′c)]
and φX (OPT) = ∑ c∈OPT φXc(OPT),
we thus have E[φX (C ∪ C ′)] ≤",4. Theoretical analysis,[0],[0]
"( k
e`
) φX (C) + 16φOPT(X )
which concludes the proof.
",4. Theoretical analysis,[0],[0]
"An iterated application of Lemma 3 allows us to bound the solution quality of Algorithm 2 in Lemma 4.
",4. Theoretical analysis,[0],[0]
Lemma 4.,4. Theoretical analysis,[0],[0]
"Let k ∈ N, t ∈ N",4. Theoretical analysis,[0],[0]
and ` ≥ k. Let X be a data set in Rd and C be the random set returned by Algorithm 2.,4. Theoretical analysis,[0],[0]
"Then,
E[φX (C)] ≤ 2 ( k
e`
)t Var(X ) + 26φOPT(X ).
",4. Theoretical analysis,[0],[0]
Proof.,4. Theoretical analysis,[0],[0]
The algorithm starts with a uniformly sampled initial cluster center c1.,4. Theoretical analysis,[0],[0]
"We iteratively apply Lemma 3 for each of the t rounds to obtain
E[φX (C)] ≤",4. Theoretical analysis,[0],[0]
"( k
e`
)t E[φX ({c1})]",4. Theoretical analysis,[0],[0]
"+ 16stφOPT(X ) (19)
",4. Theoretical analysis,[0],[0]
"where
st = t∑ i=1",4. Theoretical analysis,[0],[0]
"( k e` )i−1 .
",4. Theoretical analysis,[0],[0]
"For ` ≥ k, we have 0 ≤ ke` ≤ 1/e and hence
st ≤",4. Theoretical analysis,[0],[0]
t∑ i=1,4. Theoretical analysis,[0],[0]
1,4. Theoretical analysis,[0],[0]
"ei−1 ≤ ∞∑ i=0 1 ei =
1
1− 1/e .",4. Theoretical analysis,[0],[0]
"(20)
By Lemma 3.2 of Arthur & Vassilvitskii (2007), we have that E[φX ({c1})] ≤ 2 Var(X ).",4. Theoretical analysis,[0],[0]
"Together with (19), (20) and 16/(1− 1/e) ≈ 25.31 < 26, this implies the required result.
",4. Theoretical analysis,[0],[0]
"With Lemma 4, we are further able to bound the solution quality of Algorithm 3 and prove Theorem 1.
",4. Theoretical analysis,[0],[0]
Proof of Theorem 1.,4. Theoretical analysis,[0],[0]
Let B be the set returned by Algorithm 2.,4. Theoretical analysis,[0],[0]
"For any x ∈ X , let bx denote its closest point in B with ties broken arbitrarily.",4. Theoretical analysis,[0],[0]
"By the triangle inequality and since (|a|+|b|)2 ≤ 2a2 + 2b2, for any x ∈ X
d(x,C)2 ≤ 2 d(x, bx)2 + 2 d(bx, C)2
and hence
E[φX (C)]",4. Theoretical analysis,[0],[0]
"= ∑ x∈X d(x,C)2
≤ 2 ∑ x∈X d(x, bx) 2 + 2 ∑ x∈X d(bx, C) 2
= 2φX (B) + 2 ∑ x∈B wx d(x,C) 2.
(21)
",4. Theoretical analysis,[0],[0]
"Let OPTX be the optimal k-Means clustering solution on X and OPT(B,w) the optimal solution on the weighted set (B,w).",4. Theoretical analysis,[0],[0]
"By Theorem 1.1 of Arthur & Vassilvitskii (2007),
k-means++ produces an α = 8(log2 k + 2) approximation to the optimal solution.",4. Theoretical analysis,[0],[0]
"This implies that∑
x∈B wx d(x,C) 2 ≤ α ∑ x∈B wx d",4. Theoretical analysis,[0],[0]
"( x,OPT(B,w) )",4. Theoretical analysis,[0],[0]
"2
≤ α ∑ x∈B wx d(x,OPTX ) 2
= α ∑ x∈X d(bx,OPTX ) 2.
(22)
",4. Theoretical analysis,[0],[0]
"By the triangle inequality and since (|a|+|b|)2 ≤ 2a2+2b2, it holds for any x ∈ X that
d(bx,OPTX ) 2 ≤ 2 d(x, bx)2 + 2 d(x,OPTX )2
and hence∑",4. Theoretical analysis,[0],[0]
x∈X,4. Theoretical analysis,[0],[0]
"d(bx,OPTX ) 2 ≤",4. Theoretical analysis,[0],[0]
2φX (B) + 2φOPT(X ).,4. Theoretical analysis,[0],[0]
"(23)
Combining (21), (22) and (23), we obtain
E[φX (C)] ≤ 2(1 + α)φX (B) + 2αφOPT(X ).
",4. Theoretical analysis,[0],[0]
"Finally, by Lemma 4, we have E[φX (C)]",4. Theoretical analysis,[0],[0]
"≤(32 log2 k + 68) ( k
e`
)t Var(X )
+ (432 log2 k + 916)φOPT(X ).
",4. Theoretical analysis,[0],[0]
Proof of Theorem 2.,4. Theoretical analysis,[0],[0]
"For this proof, we explicitly construct a data set: Let β′ > 0 and consider points in onedimensional Euclidean space.",4. Theoretical analysis,[0],[0]
"For i = 1, 2, . . .",4. Theoretical analysis,[0],[0]
", t, set
xi =
√ β′(4`t) 1−i",4. Theoretical analysis,[0],[0]
"− β′(4`t)−i
as well as xt+1 = √ β′(4`t) −t .
",4. Theoretical analysis,[0],[0]
"Let the data setX consist of the t+1 points {xi}t=1,2,...,t+1 as well as t + 1 points at the origin.",4. Theoretical analysis,[0],[0]
Since t < k,4. Theoretical analysis,[0],[0]
"− 1, the optimal k-Means clustering solution consists of t + 2 points placed at each of the {xi}i=1,2,...t+1 and at 0.",4. Theoretical analysis,[0],[0]
"By design, this solution has a quantization error of zero and the variance is nonzero, i.e., φOPT(X )",4. Theoretical analysis,[0],[0]
= 0 and Var(X ),4. Theoretical analysis,[0],[0]
"> 0 as claimed.
",4. Theoretical analysis,[0],[0]
Choose β′ = β2(t+1) .,4. Theoretical analysis,[0],[0]
"The maximal distance ∆ between any two points in X is bounded by ∆ = d(0, x1)2 ≤ β′.",4. Theoretical analysis,[0],[0]
"Since n = 2(t+ 1), this implies ψ ≤",4. Theoretical analysis,[0],[0]
n∆2 ≤,4. Theoretical analysis,[0],[0]
"β as claimed.
",4. Theoretical analysis,[0],[0]
"For i = 1, 2, . . .",4. Theoretical analysis,[0],[0]
", t, let Ci consist of 0 and all xj with j < i.",4. Theoretical analysis,[0],[0]
"By design, we have d(0, Ci)2 = 0 as well as d(xj , Ci)2 = 0 for j < i.",4. Theoretical analysis,[0],[0]
For j ≥,4. Theoretical analysis,[0],[0]
"i, we have d(xj , Ci)2 = d(xj , 0)2.",4. Theoretical analysis,[0],[0]
"For any i = 1, 2, . . .",4. Theoretical analysis,[0],[0]
", t+ 1, we thus have∑
j≥i
d(xj , 0) 2 = β′(4`t) 1−i .
",4. Theoretical analysis,[0],[0]
Consider a single iteration of Algorithm 2 where C = Ci.,4. Theoretical analysis,[0],[0]
"In this case, all points in Xj with j <",4. Theoretical analysis,[0],[0]
i are added to C ′ with probability zero and for j >,4. Theoretical analysis,[0],[0]
i each point xj is added to C ′,4. Theoretical analysis,[0],[0]
"with probability
min ( 1, `d(xj , 0) 2∑
j′≥i d(xj′ , 0) 2
) =",4. Theoretical analysis,[0],[0]
"`d(xj , 0) 2
β′(4`t) 1−i .
",4. Theoretical analysis,[0],[0]
"By the union bound, the probability that any of the points in ⋃ j>i{xj} are sampled is bounded by∑
j>i
`d(xj , 0) 2 β′(4`t) 1−i = 1 4t .
",4. Theoretical analysis,[0],[0]
"The point xi is not sampled with probability at most
1−min ( 1, ` d(xi, 0) 2∑
j′≥i d(xj′ , 0) 2
) = 1−min ( 1, `− 1
4t ) ≤ 1
4t .
",4. Theoretical analysis,[0],[0]
"By the union bound, a single iteration of Algorithm 2 with C = Ci hence samples exactly the set C ′ = {xi} with probability at least ( 1− 12t ) .
",4. Theoretical analysis,[0],[0]
"In Algorithm 2, the first center is sampled uniformly at random from X .",4. Theoretical analysis,[0],[0]
"Since half of the elements in X are placed at 0, with probability at least 12 , the first center is at 0 or equivalently C = C1.",4. Theoretical analysis,[0],[0]
"With probability ( 1− 12t
)t ≥ 12 , we then sample exactly the points x1, x2, . . .",4. Theoretical analysis,[0],[0]
", xt in the t subsequent iterations.",4. Theoretical analysis,[0],[0]
"Hence, with probability at least 14 , the solution produced by Algorithm 2 consists of 0 and all xi except xt+1.",4. Theoretical analysis,[0],[0]
"Since xt+1 is closest to 0, this implies
E[φX (C)]",4. Theoretical analysis,[0],[0]
"≥ 1
4 d(xt+1, 0)
2 = 1
4 β′(4`t) −t .",4. Theoretical analysis,[0],[0]
"(24)
The variance of X is bounded by a single point at 0, i.e., Var(X ) ≤ φX ({0}) =",4. Theoretical analysis,[0],[0]
"∑ j≥1 d(xj , 0) 2 = β′.
Together with (24), we have that
E[φX (C)]",4. Theoretical analysis,[0],[0]
"≥ 1
4 (4`t)
−t Var(X ).
",4. Theoretical analysis,[0],[0]
The same result extends to the output of Algorithm 3 as it always picks a subset of the output of Algorithm 2.,4. Theoretical analysis,[0],[0]
"This research was partially supported by SNSF NRP 75, ERC StG 307036, a Google Ph.D. Fellowship and an IBM Ph.D. Fellowship.",Acknowledgements,[0],[0]
This work was done in part while Andreas Krause was visiting the Simons Institute for the Theory of Computing.,Acknowledgements,[0],[0]
The k-means++ algorithm is the state of the art algorithm to solve k-Means clustering problems as the computed clusterings are O(log k) competitive in expectation.,abstractText,[0],[0]
"However, its seeding step requires k inherently sequential passes through the full data set making it hard to scale to massive data sets.",abstractText,[0],[0]
The standard remedy is to use the k-means‖ algorithm which reduces the number of sequential rounds and is thus suitable for a distributed setting.,abstractText,[0],[0]
"In this paper, we provide a novel analysis of the k-means‖ algorithm that bounds the expected solution quality for any number of rounds and oversampling factors greater than k, the two parameters one needs to choose in practice.",abstractText,[0],[0]
"In particular, we show that k-means‖ provides provably good clusterings even for a small, constant number of iterations.",abstractText,[0],[0]
This theoretical finding explains the common observation that k-means‖ performs extremely well in practice even if the number of rounds is low.,abstractText,[0],[0]
We further provide a hard instance that shows that an additive error term as encountered in our analysis is inevitable if less than k−1 rounds are employed.,abstractText,[0],[0]
Distributed and Provably Good Seedings for k-Means in Constant Rounds,title,[0],[0]
"Bayesian optimization (BO) has recently gained considerable traction due to its capability of finding the global maximum of a highly complex (e.g., non-convex, no closedform expression nor derivative), noisy black-box objective
1Ludwig-Maximilians-Universität, Munich, Germany.",1. Introduction,[0],[0]
A substantial part of this research was performed during his student exchange program at the National University of Singapore under the supervision of Bryan Kian Hsiang Low and culminated in his Bachelor’s thesis.,1. Introduction,[0],[0]
"2Department of Computer Science, National University of Singapore, Republic of Singapore.",1. Introduction,[0],[0]
"Correspondence to: Bryan Kian Hsiang Low <lowkh@comp.nus.edu.sg>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"function with a limited budget of (often costly) function evaluations, consequently witnessing its use in an increasing diversity of application domains such as robotics, environmental sensing/monitoring, automatic machine learning, among others (Brochu et al., 2010; Shahriari et al., 2016).",1. Introduction,[0],[0]
"A number of acquisition functions (e.g., probability of improvement or expected improvement (EI) over the currently found maximum (Brochu et al., 2010), entropybased (Villemonteix et al., 2009; Hennig & Schuler, 2012; Hernández-Lobato et al., 2014), and upper confidence bound (UCB) (Srinivas et al., 2010)) have been devised to perform BO: They repeatedly select an input for evaluating/querying the black-box function (i.e., until the budget is depleted) that intuitively trades off between sampling where the maximum is likely to be given the current, possibly imprecise belief of the function modeled by a Gaussian process (GP) (i.e., exploitation) vs. improving the GP belief of the function over the entire input domain (i.e., exploration) to guarantee finding the global maximum.
",1. Introduction,[0],[0]
"The rapidly growing affordability and availability of hardware resources (e.g., computer clusters, sensor networks, robot teams/swarms) have motivated the recent development of BO algorithms that can repeatedly select a batch of inputs for querying the black-box function in parallel instead.",1. Introduction,[0],[0]
"Such batch/parallel BO algorithms can be classified into two types: On one extreme, batch BO algorithms like multi-points EI (q-EI) (Chevalier & Ginsbourger, 2013), parallel predictive entropy search (PPES)",1. Introduction,[0],[0]
"(Shah & Ghahramani, 2015), and the parallel knowledge gradient method (q-KG) (Wu & Frazier, 2016) jointly optimize the batch of inputs and hence scale poorly in the batch size.",1. Introduction,[0],[0]
"On the other extreme, greedy batch BO algorithms (Azimi et al., 2010; Contal et al., 2013; Desautels et al., 2014; González et al., 2016) boost the scalability by selecting the inputs of the batch one at a time.",1. Introduction,[0],[0]
"We argue that such a highly suboptimal approach to gain scalability is an overkill: In practice, each function evaluation is often much more computationally and/or economically costly (e.g., hyperparameter tuning for deep learning, drug testing on human subjects), which justifies dedicating more time to obtain better BO performance.",1. Introduction,[0],[0]
"In this paper, we show that it is in fact possible to jointly optimize the batch of inputs and still preserve scalability in the batch size by giving practitioners the flexibility to trade off BO performance for time efficiency.
",1. Introduction,[0],[0]
"To achieve this, we first observe that, interestingly, batch BO can be perceived as a cooperative multi-agent decision making problem whereby each agent optimizes a separate input of the batch while coordinating with the other agents doing likewise.",1. Introduction,[0],[0]
"To the best of our knowledge, this has not been considered in the BO literature.",1. Introduction,[0],[0]
"In particular, if batch BO can be framed as some known class of multi-agent decision making problems, then it can be solved efficiently and scalably by the latter’s state-of-the-art solvers.",1. Introduction,[0],[0]
"The key technical challenge would therefore be to investigate how batch BO can be cast as one of such to exploit its advantage of scalability in the number of agents (hence, batch size) while at the same time theoretically guaranteeing the resulting BO performance.
",1. Introduction,[0],[0]
"To tackle the above challenge, this paper presents a novel distributed batch BO algorithm (Section 3) that, in contrast to greedy batch BO algorithms (Azimi et al., 2010; Contal et al., 2013; Desautels et al., 2014; González et al., 2016), can jointly optimize a batch of inputs and, unlike the batch BO algorithms (Chevalier & Ginsbourger, 2013; Shah & Ghahramani, 2015; Wu & Frazier, 2016), still preserve scalability in the batch size.",1. Introduction,[0],[0]
"To realize this, we generalize GP-UCB (Srinivas et al., 2010) to a new batch variant amenable to a Markov approximation, which can then be naturally formulated as a multi-agent distributed constraint optimization problem (DCOP) in order to fully exploit the efficiency of its state-of-the-art solvers for achieving linear time in the batch size.",1. Introduction,[0],[0]
Our proposed distributed batch GPUCB (DB-GP-UCB) algorithm offers practitioners the flexibility to trade off between the approximation quality and time efficiency by varying the Markov order.,1. Introduction,[0],[0]
We provide a theoretical guarantee for the convergence rate of our DBGP-UCB algorithm via bounds on its cumulative regret.,1. Introduction,[0],[0]
We empirically evaluate the cumulative regret incurred by our DB-GP-UCB algorithm and its scalability in the batch size on synthetic benchmark objective functions and a realworld optimization problem (Section 4).,1. Introduction,[0],[0]
Consider the problem of sequentially optimizing an unknown objective function f : D → R where D ⊂ Rd denotes a domain of d-dimensional input feature vectors.,"2. Problem Statement, Background, and Notations",[0],[0]
"We consider the domain to be discrete as it is known how to generalize results to a continuous, compact domain via suitable discretizations (Srinivas et al., 2010).","2. Problem Statement, Background, and Notations",[0],[0]
"In each iteration t = 1, . . .","2. Problem Statement, Background, and Notations",[0],[0]
", T , a batch Dt ⊂ D of inputs is selected for evaluating/querying f to yield a corresponding column vector yDt , (yx) >","2. Problem Statement, Background, and Notations",[0],[0]
"x∈Dt of noisy observed outputs yx , f(x)+ with i.i.d.","2. Problem Statement, Background, and Notations",[0],[0]
"Gaussian noise ∼ N (0, σ2n) and noise variance σ2n.
Regret.","2. Problem Statement, Background, and Notations",[0],[0]
"Supposing our goal is to get close to the global maximum f(x∗) as rapidly as possible where x∗ , arg maxx∈D f(x), this can be achieved by minimizing a standard batch BO objective such as the batch or full cumulative regret (Contal et al., 2013; Desautels et al., 2014):","2. Problem Statement, Background, and Notations",[0],[0]
The notion of regret intuitively refers to a loss in reward from not knowing x∗ beforehand.,"2. Problem Statement, Background, and Notations",[0],[0]
"Formally, the instantaneous regret incurred by selecting a single input x to evaluate its corresponding f is defined as rx , f(x∗)− f(x).","2. Problem Statement, Background, and Notations",[0],[0]
"Assuming a fixed cost of evaluating f for every possible batch Dt of the same size, the batch and full cumulative regrets are, respectively, defined as sums (over iteration t = 1, . .","2. Problem Statement, Background, and Notations",[0],[0]
.,"2. Problem Statement, Background, and Notations",[0],[0]
", T ) of the smallest instantaneous regret incurred by any input within every batch","2. Problem Statement, Background, and Notations",[0],[0]
"Dt, i.e., RT , ∑T t=1 minx∈Dt rx, and of the instantaneous regrets incurred by all inputs of every batch","2. Problem Statement, Background, and Notations",[0],[0]
"Dt, i.e., R′T ,∑T t=1 ∑ x∈Dt rx.","2. Problem Statement, Background, and Notations",[0],[0]
The convergence rate of a batch BO algorithm can then be assessed based on some upper bound on the average regret RT /T or R′T /T,"2. Problem Statement, Background, and Notations",[0],[0]
(Section 3) since the currently found maximum after T iterations is no further away from f(x∗) than RT /T,"2. Problem Statement, Background, and Notations",[0],[0]
or R′T /T .,"2. Problem Statement, Background, and Notations",[0],[0]
"It is desirable for a batch BO algorithm to asymptotically achieve no regret, i.e., limT→∞RT /T","2. Problem Statement, Background, and Notations",[0],[0]
=,"2. Problem Statement, Background, and Notations",[0],[0]
0 or limT→∞R′T /T,"2. Problem Statement, Background, and Notations",[0],[0]
"= 0, implying that it will eventually converge to the global maximum.
","2. Problem Statement, Background, and Notations",[0],[0]
Gaussian Processes (GPs).,"2. Problem Statement, Background, and Notations",[0],[0]
"To guarantee no regret (Section 3), the unknown objective function f is modeled as a sample of a GP.","2. Problem Statement, Background, and Notations",[0],[0]
"Let {f(x)}x∈D denote a GP, that is, every finite subset of {f(x)}x∈D follows a multivariate Gaussian distribution (Rasmussen & Williams, 2006).","2. Problem Statement, Background, and Notations",[0],[0]
"Then, the GP is fully specified by its prior mean mx , E[f(x)] and covariance kxx′ , cov[f(x), f(x′)] for all x,x′ ∈ D, which, for notational simplicity (and w.l.o.g.), are assumed to be zero, i.e., mx = 0, and bounded, i.e., kxx′ ≤ 1, respectively.","2. Problem Statement, Background, and Notations",[0],[0]
"Given a column vector yD1:t-1 , (yx) >","2. Problem Statement, Background, and Notations",[0],[0]
"x∈D1:t-1 of noisy observed outputs for some set D1:t−1 , D1 ∪ . . .","2. Problem Statement, Background, and Notations",[0],[0]
"∪ Dt−1 of inputs after t− 1 iterations, a GP model can perform probabilistic regression by providing a predictive distribution p(fDt |yD1:t-1)","2. Problem Statement, Background, and Notations",[0],[0]
"= N (µDt ,ΣDtDt) of the latent outputs fDt , (f(x))","2. Problem Statement, Background, and Notations",[0],[0]
>,"2. Problem Statement, Background, and Notations",[0],[0]
"x∈Dt for any set Dt ⊆ D of inputs selected in iteration t with the following posterior mean vector and covariance matrix:
µDt,KDtD1:t-1(KD1:t-1D1:t-1+σ 2 nI) −1yD1:t-1 , ΣDtDt,KDtDt−KDtD1:t-1(KD1:t-1D1:t-1+σ2nI)−1KD1:t-1Dt (1) where KBB′ , (kxx′)x∈B,x′∈B′ for all B,B′ ⊂ D. GP-UCB and its Greedy Batch Variants.","2. Problem Statement, Background, and Notations",[0],[0]
"Inspired by the UCB algorithm for the multi-armed bandit problem, the GP-UCB algorithm (Srinivas et al., 2010) selects, in each iteration, an input x ∈ D for evaluating/querying f that trades off between sampling close to an expected maximum (i.e., with large posterior mean µ{x}) given the current GP belief of f (i.e., exploitation) vs. that of high predictive un-
certainty (i.e., with large posterior variance Σ{x}{x}) to improve the GP belief of f over D (i.e., exploration), that is, maxx∈D µ{x} + β 1/2 t Σ 1/2
{x}{x} where the parameter βt","2. Problem Statement, Background, and Notations",[0],[0]
"> 0 is set to trade off between exploitation vs. exploration for bounding its cumulative regret.
","2. Problem Statement, Background, and Notations",[0],[0]
"Existing generalizations of GP-UCB such as GP batch UCB (GP-BUCB) (Desautels et al., 2014) and GP-UCB with pure exploration (GP-UCB-PE) (Contal et al., 2013) are greedy batch BO algorithms that select the inputs of the batch one at a time (Section 1).","2. Problem Statement, Background, and Notations",[0],[0]
"Specifically, to avoid selecting the same input multiple times within a batch (hence reducing to GP-UCB), they update the posterior variance (but not the posterior mean) after adding each input to the batch, which can be performed prior to evaluating its corresponding f since the posterior variance is independent of the observed outputs (1).","2. Problem Statement, Background, and Notations",[0],[0]
"They differ in that GPBUCB greedily adds each input to the batch using GP-UCB (without updating the posterior mean) while GP-UCB-PE selects the first input using GP-UCB and each remaining input of the batch by maximizing only the posterior variance (i.e., pure exploration).","2. Problem Statement, Background, and Notations",[0],[0]
"Similarly, a recently proposed UCB-DPP-SAMPLE algorithm (Kathuria et al., 2016) selects the first input using GP-UCB and the remaining inputs by sampling from a determinantal point process (DPP).","2. Problem Statement, Background, and Notations",[0],[0]
"Like GP-BUCB, GP-UCB-PE, and UCB-DPP-SAMPLE, we can theoretically guarantee the convergence rate of our DB-GP-UCB algorithm, which, from a theoretical point of view, signifies an advantage of GP-UCB-based batch BO algorithms over those (e.g., q-EI and PPES) inspired by other acquisition functions such as EI and PES.","2. Problem Statement, Background, and Notations",[0],[0]
"Unlike these greedy batch BO algorithms (Contal et al., 2013; Desautels et al., 2014), our DB-GP-UCB algorithm can jointly optimize the batch of inputs while still preserving scalability in batch size by casting as a DCOP to be described next.
","2. Problem Statement, Background, and Notations",[0],[0]
Distributed Constraint Optimization Problem (DCOP).,"2. Problem Statement, Background, and Notations",[0],[0]
"A DCOP can be defined as a tuple (X ,V,A, h,W) that comprises a set X of input random vectors, a set V of |X | corresponding finite domains (i.e., a separate domain for each random vector), a set A of agents, a function h : X → A assigning each input random vector to an agent responsible for optimizing it, and a setW , {wn}n=1,...,N of non-negative payoff functions such that each function wn defines a constraint over only a subset Xn ⊆ X of input random vectors and represents the joint payoff that the corresponding agents An , {h(x)|x ∈ Xn} ⊆ A achieve.","2. Problem Statement, Background, and Notations",[0],[0]
"Solving a DCOP involves finding the input values ofX that maximize the sum of all functions w1, . . .","2. Problem Statement, Background, and Notations",[0],[0]
", wn (i.e., social welfare maximization), that is, maxX ∑N n=1 wn(Xn).","2. Problem Statement, Background, and Notations",[0],[0]
"To achieve a truly decentralized solution, each agent can only optimize its local input random vector(s) based on the assignment function h but communicate with its neighboring agents: Two agents are considered neighbors if there is a function/constraint involving input random vectors that
the agents have been assigned to optimize.","2. Problem Statement, Background, and Notations",[0],[0]
"Complete and approximation algorithms exist for solving a DCOP; see (Chapman et al., 2011; Leite et al., 2014) for reviews of such algorithms.","2. Problem Statement, Background, and Notations",[0],[0]
"A straightforward generalization of GP-UCB (Srinivas et al., 2010) to jointly optimize a batch of inputs is to simply consider summing the GP-UCB acquisition function over all inputs of the batch.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"This, however, results in selecting the same input |Dt| times within a batch, hence reducing to GP-UCB, as explained earlier in Section 2.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"To resolve this issue but not suffer from the suboptimal behavior of greedy batch BO algorithms such as GP-BUCB (Desautels et al., 2014) and GP-UCB-PE (Contal et al., 2013), we propose a batch variant of GP-UCB that jointly optimizes a batch of inputs in each iteration t = 1, . . .",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
", T according to
maxDt⊂D 1 >µDt + α 1/2 t I[fD;yDt |yD1:t-1 ]1/2 (2)
where the parameter αt > 0, which performs a similar role to that of βt in GP-UCB, is set to trade off between exploitation vs. exploration for bounding its cumulative regret (Theorem 1) and the conditional mutual information1 I[fD;yDt |yD1:t-1 ] can be interpreted as the information gain on f over D (i.e., equivalent to fD , (f(x))>x∈D) by selecting the batch Dt of inputs for evaluating/querying f given the noisy observed outputs yD1:t-1 from the previous t − 1 iterations.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"So, in each iteration t, our proposed batch GP-UCB algorithm (2) selects a batch Dt ⊂ D of inputs for evaluating/querying f that trades off between sampling close to expected maxima (i.e., with a large sum of posterior means 1>µDt = ∑ x∈Dt µ{x}) given the current GP belief of f (i.e., exploitation) vs. that yielding a large information gain I[fD;yDt |yD1:t-1 ] on f over D to improve its GP belief (i.e., exploration).",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"It can be derived that I[fD;yDt |yD1:t-1 ] = 0.5 log |I+σ−2n ΣDtDt | (Appendix A), which implies that the exploration term in (2) can be maximized by spreading the batch Dt of inputs far apart to achieve large posterior variance individually and small magnitude of posterior covariance between them to encourage diversity.
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Unfortunately, our proposed batch variant of GP-UCB (2) involves evaluating prohibitively many batches of inputs (i.e., exponential in the batch size), hence scaling poorly in the batch size.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"However, we will show in this section that our batch variant of GP-UCB is, interestingly, amenable to a Markov approximation, which can then be naturally formulated as a multi-agent DCOP in order to fully exploit the
1In contrast to the BO algorithm of Contal et al. (2014) that also uses mutual information, our work here considers batch BO by exploiting the correlation information between inputs of a batch in our acquisition function in (2) to encourage diversity.
efficiency of its state-of-the-art solvers for achieving linear time in the batch size.
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
Markov Approximation.,3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"The key idea is to design the structure of a matrix ΨDtDt whose log-determinant can closely approximate that of ΨDtDt , I + σ −2 n ΣDtDt residing in the I[fD;yDt |yD1:t-1 ] term in (2) and at the same time be decomposed into a sum of log-determinant terms, each of which is defined by submatrices of ΨDtDt that all depend on only a subset of the batch.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Such a decomposition enables our resulting approximation of (2) to be formulated as a DCOP (Section 2).
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"At first glance, our proposed idea may be naively implemented by constructing a sparse block-diagonal matrix ΨDtDt using, say, the N > 1 diagonal blocks of ΨDtDt .",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Then, log |ΨDtDt | can be decomposed into a sum of logdeterminants of its diagonal blocks2, each of which depends on only a disjoint subset of the batch.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"This, however, entails an issue similar to that discussed at the beginning of this section of selecting the same |Dt|/N inputs N times within a batch due to the assumption of independence of outputs between different diagonal blocks of ΨDtDt .",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"To address this issue, we significantly relax this assumption and show that it is in fact possible to construct a more refined, dense matrix approximation ΨDtDt by exploiting a Markov assumption, which consequently correlates the outputs between all its constituent blocks and is, perhaps surprisingly, still amenable to the decomposition to achieve scalability in the batch size.
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Specifically, evenly partition the batch Dt of inputs into N ∈ {1, . . .",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
", |Dt|} disjoint subsets Dt1, . . .",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
",DtN and ΨDtDt (ΨDtDt ) into N ×N square blocks, i.e., ΨDtDt , [ΨDtnDtn′ ]n,n′=1,...,N (ΨDtDt , [ΨDtnDtn′ ]n,n′=1,...,N ).",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Our first result below derives a decomposition of the logdeterminant of any symmetric positive definite block matrix ΨDtDt into a sum of log-determinant terms, each of which is defined by a separate diagonal block of the Cholesky factor of Ψ
−1",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"DtDt :
Proposition 1.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Consider the Cholesky factorization of a symmetric positive definite Ψ
−1",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"DtDt , U >U where Cholesky factor U , [Unn′ ]n,n′=1,...,N (i.e., partitioned into N × N square blocks) is an upper triangular block matrix (i.e., Unn′ = 0 for n > n′).",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Then, log |ΨDtDt | =∑N n=1 log |(U>nnUnn)−1|.
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Its proof (Appendix B) utilizes properties of the determinant and that the determinant of an upper triangular block matrix is a product of determinants of its diagonal blocks (i.e., |U | = ∏Nn=1 |Unn|).",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Proposition 1 reveals a subtle possibility of imposing some structure on the inverse of
2The determinant of a block-diagonal matrix is a product of determinants of its diagonal blocks.
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
ΨDtDt such that each diagonal block Unn of its Cholesky factor (and hence each log |(U>nnUnn)−1| term) will depend on only a subset of the batch.,3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
The following result presents one such possibility: Proposition 2.,3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"LetB ∈ {1, . . .",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
", N−1} be given.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"If Ψ−1DtDt is B-block-banded3, then
(U>nnUnn) −1",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"= ΨDtnDtn −ΨDtnDBtnΨ −1 DBtnDBtnΨDBtnDtn
(3) for n = 1, . . .",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
", N where η , min(n + B,N), DBtn , ⋃η n′=n+1Dtn′ , ΨDtnDBtn , [ΨDtnDtn′ ]n′=n+1,...,η , ΨDBtnDBtn , [ΨDtn′Dtn′′ ]n′,n′′=n+1,...,η, and ΨDBtnDtn , Ψ > DtnDBtn .
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Its proof follows directly from a block-banded matrix result of (Asif & Moura, 2005) (i.e., Theorem 1).",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Proposition 2 indicates that if Ψ
−1 DtDt is B-block-banded (Fig. 1b), then
each log |(U>nnUnn)−1| term depends on only the subset Dtn ∪ DBtn = ⋃η n′=nDtn′ of the batch",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Dt (Fig. 1c).
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Our next result defines a structure of ΨDtDt in terms of the blocks within the B-block band of ΨDtDt to induce a B-block-banded inverse of ΨDtDt :
Proposition 3.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Let
ΨDtnDtn′,  ΨDtnDtn′ if |∆| ≤ B, ΨDtnDBtnΨ −1 DBtnDBtn
ΨDBtnDtn′ if ∆ < −B, ΨDtnDBtn′ Ψ−1DB tn′D B tn′ ΨDB tn′Dtn′ if ∆ > B;
(4) where ∆ , n−n′ for n, n′ = 1, . . .",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
", N (see Fig. 1a).",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Then, Ψ −1 DtDt is B-block-banded (see Fig. 1b).
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Its proof follows directly from a block-banded matrix result of (Asif & Moura, 2005) (i.e., Theorem 3).",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"It can be observed from (4) and Fig. 1 that (a) though Ψ
−1 DtDt is a sparse
B-block-banded matrix, ΨDtDt is a dense matrix approximation for B = 1, . . .",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
", N",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"− 1; (b) when B = N − 1 or N = 1, ΨDtDt = ΨDtDt ; and (c) the blocks within the B-block band of ΨDtDt (i.e., |n − n′| ≤ B) coincide with that of ΨDtDt while each block outside the Bblock band of ΨDtDt (i.e., |n − n′| > B) is fully specified by the blocks within the B-block band of ΨDtDt (i.e., |n − n′| ≤ B) due to its recursive series of |n − n′| − B reduced-rank approximations (Fig. 1a).",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Note, however, that the log |(U>nnUnn)−1| terms (3) for n = 1, . . .",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
", N depend on only the blocks within (and not outside) the B-block band of ΨDtDt (Fig. 1c).
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
Remark 1.,3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Proposition 3 provides an attractive principled interpretation: Let εx , σ−1n (yx − µ{x}) denote a scaled
3A block matrix P , [Pnn′ ]n,n′=1,...,N (i.e., partitioned into N ×N square blocks) is B-block-banded if any block Pnn′ outside its B-block band (i.e., |n− n′| > B) is 0.
residual incurred by the GP predictive mean (1).",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Its covariance is then cov[εx, εx′ ] = Ψ{x}{x′}.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"In the same spirit as a Gaussian Markov random process, imposing a B-th order Markov property on the residual process {εx}x∈Dt is equivalent to approximating ΨDtDt with ΨDtDt (4) whose inverse isB-block-banded.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"In other words, if |n−n′| > B, then {εx}x∈Dtn and {εx}x∈Dtn′ are conditionally independent given {εx}x∈Dt\(Dtn∪Dtn′ ).",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
This conditional independence assumption therefore becomes more relaxed with a larger batch Dt.,3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Proposition 2 demonstrates the importance of such a B-th order Markov assumption (or, equivalently, the sparsity ofB-block-banded Ψ
−1 DtDt ) to achieving
scalability in the batch size.
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
Remark 2.,3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Regarding the approximation quality of ΨDtDt (4), the following result (see Appendix C for its proof) shows that the Kullback-Leibler (KL) distance of ΨDtDt from ΨDtDt measures an intuitive notion of the approximation error of ΨDtDt being the difference in information gain when relying on our Markov approximation, which can be bounded by some quantity νt:
Proposition 4.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Let the KL distance DKL(Ψ, Ψ̃) , 0.5(tr(ΨΨ̃−1)",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"− log |ΨΨ̃−1| − |Dt|) between two symmetric positive definite |Dt| × |Dt| matrices Ψ and Ψ̃ measure the error of approximating Ψ with Ψ̃. Also, let Ĩ[fD;yDt |yD1:t-1 ] , 0.5 log |ΨDtDt | denote the approximated information gain, and C ≥ I[f{x};yDt |yD1:t-1 ] for all x ∈ D and t ∈",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
N.,3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Then, for all t ∈ N,
DKL(ΨDtDt ,ΨDtDt) = Ĩ[fD;yDt |yD1:t-1 ]− I[fD;yDt |yD1:t-1 ] ≤ (exp(2C)− 1) I[fD;yDt |yD1:t-1 ] , νt .
Proposition 4 implies that the approximated information gain Ĩ[fD;yDt |yD1:t-1 ] is never smaller than the exact information gain I[fD;yDt |yD1:t-1 ] since DKL(ΨDtDt ,ΨDtDt) ≥ 0 with equality when N = 1, in which case ΨDtDt = ΨDtDt (4).",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Thus, intuitively, our proposed Markov approximation hallucinates information into ΨDtDt to yield an optimistic estimate of the information gain (by selecting a particular batch), ultimately making our resulting algorithm overconfident in selecting a batch.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"This overconfidence is information-theoretically quantified by the approximation error DKL(ΨDtDt ,ΨDtDt) ≤ νt.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
Remark 3.,3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"The KL distanceDKL(ΨDtDt ,ΨDtDt) of ΨDtDt from ΨDtDt is also the least among all |Dt|×|Dt|matrices with a B-block-banded inverse, as proven in Appendix D.
DCOP Formulation.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"By exploiting the approximated information gain Ĩ[fD;yDt |yD1:t-1 ] (Proposition 4), Proposition 1, (3), and (4), our batch variant of GP-UCB (2) can be reformulated in an approximate sense4 to a distributed batch GP-UCB (DB-GP-UCB) algorithm5 that jointly optimizes a batch of inputs in each iteration t = 1, . . .",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
", T according to
Dt , arg max Dt⊂D N∑ n=1 wn(Dtn ∪ DBtn)
wn(Dtn ∪ DBtn) , 1>µDtn+(0.5αt log |ΨDtnDtn|DBtn |) 1/2
(5) with ΨDtnDtn|DBtn ,ΨDtnDtn−ΨDtnDBtnΨ −1 DBtnDBtn ΨDBtnDtn .
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"4Note that our acquisition function (5) uses ∑N
n=1(log | · |) 1/2 instead of ( ∑N
n=1 log | · |) 1/2 to enable the decomposition.
5Pseudocode for DB-GP-UCB is provided in Appendix E.
Note that (5) is equivalent to our batch variant of GP-UCB (2) when N = 1.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
It can also be observed that (5) is naturally formulated as a multi-agent DCOP (Section 2) whereby every agent an ∈,3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"A is responsible for optimizing a disjoint subset Dtn of the batch Dt for n = 1, . . .",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
", N and each function wn defines a constraint over only the subset Dtn ∪ DBtn = ⋃η n′=nDtn′ of the batch Dt and represents the joint payoff that the corresponding agents An , {an′}ηn′=n ⊆ A achieve.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"As a result, (5) can be efficiently and scalably solved by the state-of-the-art DCOP algorithms (Chapman et al., 2011; Leite et al., 2014).",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"For example, the time complexity of an iterative message-passing algorithm called max-sum (Farinelli et al., 2008) scales exponentially in only the largest arity maxn∈{1,...,N} |Dtn ∪ DBtn| =",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"(B+1)|Dt|/N of the functionsw1, . . .",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
", wN .",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Given a limited time budget, a practitioner can set a maximum arity of ω for any function wn, after which the number N of functions is adjusted to d(B + 1)|Dt|/ωe so that the time incurred by max-sum to solve the DCOP in (5) is O(|D|ωω3B|Dt|)6 per iteration (i.e., linear in the batch size |Dt| by assuming ω and the Markov orderB to be constants).",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"In contrast, our batch variant of GP-UCB (2) incurs exponential time in the batch size |Dt|.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
The max-sum algorithm is also amenable to a distributed implementation on a cluster of parallel machines to boost scalability further.,3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"If a solution quality guarantee is desired, then a variant of maxsum called bounded max-sum (Rogers et al., 2011) can be used7.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Finally, the Markov order B can be varied to trade off between the approximation quality of ΨDtDt (4) and the time efficiency of max-sum in solving the DCOP in (5).
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
Regret Bounds.,3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Our main result to follow derives probabilistic bounds on the cumulative regret of DB-GP-UCB:
Theorem 1.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Let δ ∈ (0, 1) be given, C1 , 4/ log(1 + σ−2n ), γT , maxD1:T⊂D I[fD;yD1:T ], αt , C1|Dt| exp(2C) log(|D|t2π2/(6δ)), and ν̄T , ∑T t=1 νt.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Then, for the batch and full cumulative regrets incurred by our DB-GP-UCB algorithm (5),
RT ≤ 2 ( T |DT |−2αTN(γT + ν̄T ) )1/2 and R′T ≤ 2 (TαTN(γT + ν̄T )) 1/2
hold with probability of at least 1− δ.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"6We assume the use of online sparse GP models (Csató &
Opper, 2002; Hensman et al., 2013; Hoang et al., 2015; 2017; Low et al., 2014b; Xu et al., 2014) that can update the GP predictive/posterior distribution (1) in constant time in each iteration.
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"7Bounded max-sum is previously used in (Rogers et al., 2011) to solve a related maximum entropy sampling problem (Shewry & Wynn, 1987) formulated as a DCOP.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"But, the largest arity of any function wn in this DCOP is still the batch size |Dt| and, unlike the focus of our work here, no attempt is made in (Rogers et al., 2011) to reduce it, thus causing max-sum and bounded max-sum to incur exponential time in |Dt|.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"In fact, our proposed Markov approximation can be applied to this problem to reduce the largest arity of any function wn in this DCOP to again (B + 1)|Dt|/N .
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Its proof (Appendix F), when compared to that of GP-UCB (Srinivas et al., 2010) and its greedy batch variants (Contal et al., 2013; Desautels et al., 2014), requires tackling the additional technical challenges associated with jointly optimizing a batch Dt of inputs in each iteration t. Note that the uncertainty sampling based initialization strategy proposed by Desautels et al. (2014) can be employed to replace the √ exp(2C) term (i.e., growing linearly in |Dt|) appearing in our regret bounds by a kernel-dependent constant factor of C ′",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"that is independent of |Dt|; values of C ′ for the most commonly-used kernels are replicated in Table 2 in Appendix G (see section 4.5 in (Desautels et al., 2014) for a more detailed discussion on this issue).
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Table 1 in Appendix G compares the bounds on RT of DB-GP-UCB (5), GP-UCB-PE, GP-BUCB, GP-UCB, and UCB-DPP-SAMPLE.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Compared to the bounds on RT of GP-UCB-PE and UCB-DPP-SAMPLE, our bound includes the additional kernel-dependent factor of C ′, which is similar to GP-BUCB.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"In fact, our regret bound is of the same form as that of GP-BUCB except that our bound incorporates a parameter N of our Markov approximation and an upper bound ν̄T on the cumulative approximation error, both of which vanish for our batch variant of GP-UCB (2):
Corollary 1.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"For our batch variant of GP-UCB (2), the cumulative regrets reduce to RT ≤ 2 ( T |DT |−2αT γT
)1/2 and R′T ≤ 2 (TαT γT ) 1/2.
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Corollary 1 follows directly from Theorem 1 and by noting that for our batch variant (2), N = 1 (since ΨDtDt then trivially reduces to ΨDtDt ) and νt",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"= 0 for t = 1, . . .",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
", T .
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Finally, the convergence rate of our DB-GP-UCB algorithm is dominated by the growth behavior of γT + ν̄T .",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"While it is well-known that the bounds on the maximum mutual information γT established for the commonly-used linear, squared exponential, and Matérn kernels in (Srinivas et al., 2010; Kathuria et al., 2016) (i.e., replicated in Table 2 in Appendix G) only grow sublinearly in T , it is not immediately clear how the upper bound ν̄T on the cumulative approximation error behaves.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
Our next result reveals that ν̄T in fact only grows sublinearly in T as well: Corollary 2.,3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"ν̄T ≤ (exp(2C)− 1)γT .
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
Corollary 2 follows directly from the definitions of νt in Proposition 4 and ν̄T and γT in Theorem 1 and applying the chain rule for mutual information.,3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Since γT grows sublinearly in T for the above-mentioned kernels (Srinivas et al., 2010) and C can be chosen to be independent of T (e.g., C , γ|Dt|−1) (Desautels et al., 2014), it follows from Corollary 2 that ν̄T grows sublinearly in T .",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"As a result, Theorem 1 guarantees sublinear cumulative regrets for the above-mentioned kernels, which implies that our DBGP-UCB algorithm (5) asymptotically achieves no regret, regardless of the degree of our proposed Markov approxi-
mation (i.e., configuration of [N,B]).",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Thus, our batch variant of GP-UCB (2) achieves no regret as well.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"This section evaluates the cumulative regret incurred by our DB-GP-UCB algorithm (5) and its scalability in the batch size empirically on two synthetic benchmark objective functions such as Branin-Hoo (Lizotte, 2008) and gSobol (González et al., 2016)",4. Experiments and Discussion,[0],[0]
"(Table 3 in Appendix H) and a real-world pH field of Broom’s Barn farm (Webster & Oliver, 2007) (Fig. 3 in Appendix H) spatially distributed over a 1200 m by 680 m region discretized into a 31 × 18 grid of sampling locations.",4. Experiments and Discussion,[0],[0]
"These objective functions and the real-world pH field are each modeled as a sample of a GP whose prior covariance is defined by the widelyused squared exponential kernel kxx′ , σ2s exp(−0.5(x− x′)>Λ−2(x− x′)) where Λ , diag[`1, . . .",4. Experiments and Discussion,[0],[0]
", `d] and σ2s are its length-scale and signal variance hyperparameters, respectively.",4. Experiments and Discussion,[0],[0]
"These hyperparameters together with the noise variance σ2n are learned using maximum likelihood estimation (Rasmussen & Williams, 2006).
",4. Experiments and Discussion,[0],[0]
"The performance of our DB-GP-UCB algorithm (5) is compared with the state-of-the-art batch BO algorithms such as GP-BUCB (Desautels et al., 2014), GP-UCB-PE (Contal et al., 2013), SM-UCB (Azimi et al., 2010), q-EI (Chevalier & Ginsbourger, 2013), and BBO-LP by plugging in GP-UCB (González et al., 2016), whose implementations8 are publicly available.",4. Experiments and Discussion,[0],[0]
"These batch BO algorithms are evaluated using a performance metric that measures the cumulative regret incurred by a tested algorithm:∑T t=1 f(x
∗)",4. Experiments and Discussion,[0],[0]
"− f(x̃t) where x̃t , arg maxxt∈D µ{xt} (1) is the recommendation of the tested algorithm after t batch evaluations.",4. Experiments and Discussion,[0],[0]
"For each experiment, 5 noisy observations are randomly selected and used for initialization.",4. Experiments and Discussion,[0],[0]
This is independently repeated 64 times and we report the resulting mean cumulative regret incurred by a tested algorithm.,4. Experiments and Discussion,[0],[0]
"All experiments are run on a Linux system with Intelr Xeonr E5-2670 at 2.6GHz with 96 GB memory.
",4. Experiments and Discussion,[0],[0]
"For our experiments, we use a fixed budget of T |DT | = 64 function evaluations and analyze the trade-off between batch size |DT | (i.e., 2, 4, 8, 16) vs. time horizon T (respectively, 32, 16, 8, 4) on the performance of the tested algorithms.",4. Experiments and Discussion,[0],[0]
"This experimental setup represents a practical scenario of costly function evaluations: On one hand, when a function evaluation is computationally costly (i.e., time-consuming), it is more desirable to evaluate f for a larger batch (e.g., |DT | = 16) of inputs in parallel in each iteration t (i.e., if hardware resources permit) to reduce the total time needed (hence smaller T ).",4. Experiments and Discussion,[0],[0]
"On the other hand,
8Details on the used implementations are given in Table 4 in Appendix I. We implemented DB-GP-UCB in MATLAB to exploit the GPML toolbox (Rasmussen & Williams, 2006).
",4. Experiments and Discussion,[0],[0]
"when a function evaluation is economically costly, one may be willing to instead invest more time (hence larger T ) to evaluate f for a smaller batch (e.g., |DT | = 2) of inputs in each iteration t in return for a higher frequency of information and consequently a more adaptive BO to achieve potentially better performance.",4. Experiments and Discussion,[0],[0]
"In some settings, both factors may be equally important, that is, moderate values of |DT | and T are desired.",4. Experiments and Discussion,[0],[0]
"To the best of our knowledge, such a form of empirical analysis does not seem to be available in the batch BO literature.
",4. Experiments and Discussion,[0],[0]
"Fig. 2 shows results9 of the cumulative regret incurred by the tested algorithms to analyze their trade-off between batch size |DT | (i.e., 2, 4, 8, 16) vs. time horizon T (respectively, 32, 16, 8, 4) using a fixed budget of T |DT | = 64 function evaluations for the Branin-Hoo function (left column), gSobol function (middle column), and real-world pH field (right column).",4. Experiments and Discussion,[0],[0]
"Our DB-GP-UCB algorithm uses the configurations of [N,B] =",4. Experiments and Discussion,[0],[0]
"[4, 2], [8, 5], [16, 10] in the experiments with batch size |DT | = 4, 8, 16, respectively; in the case of |DT | = 2, we use our batch variant of GPUCB (2) which is equivalent to DB-GP-UCB whenN = 1.",4. Experiments and Discussion,[0],[0]
"It can be observed that DB-GP-UCB achieves lower cumulative regret than GP-BUCB, GP-UCB-PE, SM-UCB, and BBO-LP in all experiments (with the only exception being the gSobol function for the smallest batch size of |DT | = 2 where BBO-LP performs slightly better) since DB-GPUCB can jointly optimize a batch of inputs while GPBUCB, GP-UCB-PE, SM-UCB, and BBO-LP are greedy batch algorithms that select the inputs of a batch one at time.",4. Experiments and Discussion,[0],[0]
"Note that as the real-world pH field is not as wellbehaved as the synthetic benchmark functions (see Fig. 3 in Appendix H), the estimate of the Lipschitz constant by BBO-LP is potentially worse, hence likely degrading its performance.",4. Experiments and Discussion,[0],[0]
"Furthermore, DB-GP-UCB can scale to a much larger batch size of 16 than the other batch BO algorithms that also jointly optimize the batch of inputs, which include q-EI, PPES (Shah & Ghahramani, 2015) and q-KG (Wu & Frazier, 2016): Results of q-EI are not available for |DT | ≥ 4 as they require a prohibitively huge computational effort to be obtained10 while PPES can only operate with a small batch size of up to 3 for the Branin-Hoo function and up to 4 for other functions, as reported in (Shah & Ghahramani, 2015), and q-KG can only operate with a small batch size of 4 for all tested functions (including the Branin-Hoo function and four others), as reported in (Wu & Frazier, 2016).",4. Experiments and Discussion,[0],[0]
"The scalability of DB-GP-UCB is attributed to our proposed Markov approximation of our
9Error bars are omitted in Fig. 2 to preserve the readability of the graphs.",4. Experiments and Discussion,[0],[0]
"A replication of the graphs in Fig. 2 including standard error bars is provided in Appendix K.
10In the experiments of González et al. (2016), q-EI can reach a batch size of up to 10 but performs much worse than GP-BUCB, which is likely due to a considerable downsampling of possible batches available for selection in each iteration.
batch variant of GP-UCB (2) (Section 3), which can then be naturally formulated as a multi-agent DCOP (5) in order to fully exploit the efficiency of one of its state-of-the-art solvers called max-sum (Farinelli et al., 2008).",4. Experiments and Discussion,[0],[0]
"In the experiments with the largest batch size of |DT | = 16, we have reduced the number of iterations in max-sum to less than 5 without waiting for convergence to preserve the efficiency of DB-GP-UCB, thus sacrificing its BO performance.",4. Experiments and Discussion,[0],[0]
"Nevertheless, DB-GP-UCB can still outperform the other tested
batch BO algorithms.
",4. Experiments and Discussion,[0],[0]
We have also investigated and analyzed the trade-off between approximation quality and time efficiency of our DPGP-UCB algorithm and reported the results in Appendix J due to lack of space.,4. Experiments and Discussion,[0],[0]
"To summarize, it can be observed from our results that the approximation quality improves near-linearly with an increasing Markov order B at the expense of higher computational cost (i.e., exponential in B).",4. Experiments and Discussion,[0],[0]
"This paper develops a novel distributed batch GP-UCB (DB-GP-UCB) algorithm for performing batch BO of highly complex, costly-to-evaluate, noisy black-box objective functions.",5. Conclusion,[0],[0]
"In contrast to greedy batch BO algorithms (Azimi et al., 2010; Contal et al., 2013; Desautels et al., 2014; González et al., 2016), our DB-GP-UCB algorithm can jointly optimize a batch of inputs and, unlike (Chevalier & Ginsbourger, 2013; Shah & Ghahramani, 2015; Wu & Frazier, 2016), still preserve scalability in the batch size.",5. Conclusion,[0],[0]
"To realize this, we generalize GP-UCB (Srinivas et al., 2010) to a new batch variant amenable to a Markov approximation, which can then be naturally formulated as a multi-agent DCOP in order to fully exploit the efficiency of its state-of-the-art solvers such as max-sum (Farinelli et al., 2008; Rogers et al., 2011) for achieving linear time in the batch size.",5. Conclusion,[0],[0]
Our proposed DB-GP-UCB algorithm offers practitioners the flexibility to trade off between the approximation quality and time efficiency by varying the Markov order.,5. Conclusion,[0],[0]
We provide a theoretical guarantee for the convergence rate of our DB-GP-UCB algorithm via bounds on its cumulative regret.,5. Conclusion,[0],[0]
"Empirical evaluation on synthetic benchmark objective functions and a real-world pH field shows that our DB-GP-UCB algorithm can achieve lower cumulative regret than the greedy batch BO algorithms such as GP-BUCB, GP-UCB-PE, SM-UCB, and BBO-LP, and scale to larger batch sizes than the other batch BO algorithms that also jointly optimize the batch of inputs, which include q-EI, PPES, and q-KG.",5. Conclusion,[0],[0]
"For future work, we plan to generalize DB-GP-UCB (a) to the nonmyopic context by appealing to existing literature on nonmyopic BO (Ling et al., 2016) and active learning (Cao et al., 2013; Hoang et al., 2014a;b; Low et al., 2008; 2009; 2011; 2014a) as well as (b) to be performed by a multi-robot team to find hotspots in environmental sensing/monitoring by seeking inspiration from existing literature on multi-robot active sensing/learning (Chen et al., 2012; 2013b; 2015; Low et al., 2012; Ouyang et al., 2014).",5. Conclusion,[0],[0]
"For applications with a huge budget of function evaluations, we like to couple DB-GP-UCB with the use of parallel/distributed sparse GP models (Chen et al., 2013a; Hoang et al., 2016; Low et al., 2015) to represent the belief of the unknown objective function efficiently.",5. Conclusion,[0],[0]
"This research is supported by Singapore Ministry of Education Academic Research Fund Tier 2, MOE2016-T2-2-156.",Acknowledgements,[0],[0]
Erik A. Daxberger would like to thank Volker Tresp for his advice throughout this research project.,Acknowledgements,[0],[0]
"This paper presents a novel distributed batch Gaussian process upper confidence bound (DB-GP-UCB) algorithm for performing batch Bayesian optimization (BO) of highly complex, costly-to-evaluate black-box objective functions.",abstractText,[0],[0]
"In contrast to existing batch BO algorithms, DBGP-UCB can jointly optimize a batch of inputs (as opposed to selecting the inputs of a batch one at a time) while still preserving scalability in the batch size.",abstractText,[0],[0]
"To realize this, we generalize GP-UCB to a new batch variant amenable to a Markov approximation, which can then be naturally formulated as a multi-agent distributed constraint optimization problem in order to fully exploit the efficiency of its state-of-the-art solvers for achieving linear time in the batch size.",abstractText,[0],[0]
Our DB-GP-UCB algorithm offers practitioners the flexibility to trade off between the approximation quality and time efficiency by varying the Markov order.,abstractText,[0],[0]
We provide a theoretical guarantee for the convergence rate of DB-GP-UCB via bounds on its cumulative regret.,abstractText,[0],[0]
Empirical evaluation on synthetic benchmark objective functions and a real-world optimization problem shows that DB-GP-UCB outperforms the stateof-the-art batch BO algorithms.,abstractText,[0],[0]
Distributed Batch Gaussian Process Optimization,title,[0],[0]
"Clustering is a fundamental problem in the analysis and understanding of data, and is used widely in different areas of science.",1. Introduction,[0],[0]
The broad goal of clustering is to divide a (typically large) dataset into groups that such that data points within a group are “similar” to one another.,1. Introduction,[0],[0]
"In most applications, there is a measure of similarity between any two objects, which typically forms a metric.",1. Introduction,[0],[0]
"The problem can be formalized in many different ways, depending on the properties desired of the obtained clustering.",1. Introduction,[0],[0]
"While a “perfect” formulation may not exist (see (Kleinberg, 2002)),
*Equal contribution 1School of Computing, University of Utah.",1. Introduction,[0],[0]
Correspondence to: Aditya Bhaskara,1. Introduction,[0],[0]
<bhaskaraaditya@gmail.com,1. Introduction,[0],[0]
">, Maheshakya Wijewardena <pmaheshakya4@gmail.com",1. Introduction,[0],[0]
">.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"many formulations have been very successful in applications, including k-means, k-median, k-center, and various notions of hierarchical clustering (see (Hastie et al., 2009; Dasgupta, 2016) and references there-in).
",1. Introduction,[0],[0]
"In this paper, we focus on k-means clustering, in which the input is a set of n points in Euclidean space.",1. Introduction,[0],[0]
"Here the goal is to partition the points into k clusters, so as to minimize the sum of squared distances from the points to the respective cluster centers (see Section 2 for a formal definition).",1. Introduction,[0],[0]
k-means is one of the most well-studied clustering variants.,1. Introduction,[0],[0]
"Lloyd’s algorithm (Lloyd, 1982), developed over 35 years ago, has been extremely successful in practice (the success has been ‘explained’ in many recent works; see (Arthur et al., 2011; Awasthi & Sheffet, 2012; Kumar & Kannan, 2010) and references there-in).",1. Introduction,[0],[0]
"Despite the success, Lloyd’s algorithm can have an arbitrarily bad approximation ratio in the worst case.",1. Introduction,[0],[0]
"To address this, constant factor approximation algorithms have been developed, which are more involved but have worst case guarantees (see (Kanungo et al., 2004) and (Ahmadian et al., 2017)).",1. Introduction,[0],[0]
"In another direction, works by (Ostrovsky et al., 2006; Jaiswal et al., 2012; Arthur & Vassilvitskii, 2007) have shown how to obtain simple bicriteria approximation algorithms for k-means.",1. Introduction,[0],[0]
"(Arthur & Vassilvitskii, 2007) also proposed a variant of Lloyd’s algorithm, termed “k-means++”, which also comes with a theoretical approximation factor guarantee of O(log k) approximation.
",1. Introduction,[0],[0]
All the algorithms above assume that data fits in a single machine.,1. Introduction,[0],[0]
"However, with the ubiquity of large data sets, there has been a lot of interest in distributed algorithms where data is spread across several machines.",1. Introduction,[0],[0]
"The goal is to use available distributed models of computation to design algorithms that can (a) work with machines having access only to their local data set, (b) use small amount of memory and only a few “rounds” of communication, and (c) have approximation guarantees for the solution they output.
",1. Introduction,[0],[0]
"For k-means and related objectives, the paradigm of iterative ‘data reduction’ has been remarkably successful.",1. Introduction,[0],[0]
"The main idea is that in each round, a machine chooses a small subset of its input, and only this subset is carried to the next round.",1. Introduction,[0],[0]
"Thus the total number of points reduces by a significant factor in every round, and this results in a small number of rounds overall.",1. Introduction,[0],[0]
"Such an algorithm can be implemented
efficiently in the MapReduce framework, introduced by (Dean & Ghemawat, 2004), and formalized by (Karloff et al., 2010)).",1. Introduction,[0],[0]
"(Ene et al., 2011) gave one of the first such implementations (for the k-median problem), and showed theoretical guarantees.",1. Introduction,[0],[0]
"This line of work has subsequently been developed in (Kumar et al., 2013; Balcan et al., 2013a; Awasthi et al., 2017).",1. Introduction,[0],[0]
"The last work also gives a summary of the known results in this space.
",1. Introduction,[0],[0]
The high level ideas used in these works are similar to those used in streaming algorithms for clustering.,1. Introduction,[0],[0]
"The literature here is very rich; one of the earliest works is that of (Charikar et al., 1997), for the k-center problem.",1. Introduction,[0],[0]
"The work of (Guha et al., 2001) introduced many ideas crucial to the distributed algorithms mentioned above.",1. Introduction,[0],[0]
"Indeed, all of these algorithms can be viewed as implicitly constructing coresets (or summaries) for the underlying clustering problem.",1. Introduction,[0],[0]
"We refer to the works of (Agarwal et al., 2004; 2012; Balcan et al., 2013b; Indyk et al., 2014) for more on this connection.
",1. Introduction,[0],[0]
Motivation for our work.,1. Introduction,[0],[0]
"While iterative data reduction is powerful, it has a key bottleneck: in order to have approximation guarantees, machines always need to store > k data points.",1. Introduction,[0],[0]
"Indeed, all the algorithms we are aware of require a memory of kn if they are to useO(1/ ) rounds of MAPREDUCE computation.1 The high level reason for this is that if a machine sees k points that are all very far from one another, it needs to keep all of them, or else we might lose all the information about one of the clusters, and this could lead to a large objective value.",1. Introduction,[0],[0]
"This is also the reason each machine needs to communicate ≥ k points to the others (such a lower bound was proved formally in (Chen et al., 2016), as we will discuss later).",1. Introduction,[0],[0]
"The natural question is thus to ask: can we partition the data across machines so that different machines work in different “regions of space”, and thus focus on finding different clusters?",1. Introduction,[0],[0]
"This would result in a smaller space requirement per machine, and lesser communication between machines.",1. Introduction,[0],[0]
"Our main result is to show that this is possible, as long as we have a rough estimate of the optimum objective value (up to an arbitrary polynomial factor).",1. Introduction,[0],[0]
"We give an algorithm based on a variant of locality sensitive hashing, and prove that this yields a bi-criteria approximation guarantee.
",1. Introduction,[0],[0]
"Locality sensitive hashing was introduced in the seminal work of (Indyk & Motwani, 1998), which gave an efficient algorithm for nearest neighbor search in high dimensional space.",1. Introduction,[0],[0]
"The idea has found several applications in machine learning and data science, ranging from the early applications of similarity search to the speeding up of neural networks (Spring & Shrivastava, 2017).",1. Introduction,[0],[0]
"(Datar et al., 2004)
1All the algorithms mentioned above can be naturally implemented in the MAPREDUCE framework.
generalized the original result of (Indyk & Motwani, 1998) to the case of `p norms, and (Andoni & Indyk, 2006) gave an improved analysis.",1. Introduction,[0],[0]
"Extensions of LSH are still an active area of research, but a discussion is beyond the scope of this paper.",1. Introduction,[0],[0]
Our contribution here is to understand the behavior of clusters of points under LSH and its variants.,1. Introduction,[0],[0]
Our focus in the paper will be on the k-means objective (defined formally in Section 2).,1.1. Our results,[0],[0]
"The data set is assumed to be a collection of points in a Euclidean space Rd for some d, and distance refers to the `2 distance.
",1.1. Our results,[0],[0]
"Our first contribution is an analysis of “product LSH” (PLSH), a hash obtained by concatenating independent copies of an LSH.",1.1. Our results,[0],[0]
"For each LSH, we consider the implementation of (Andoni & Indyk, 2006).
",1.1. Our results,[0],[0]
Informal theorem 1 (See Lemmas 1 and 2).,1.1. Our results,[0],[0]
Let C be any cluster of points with diameter σ.,1.1. Our results,[0],[0]
"Then PLSH with appropriate parameters yields the same hash for all the points in C, with probability ≥ 3/4.",1.1. Our results,[0],[0]
"Furthermore, for any two points u, v such that ‖u",1.1. Our results,[0],[0]
"− v‖ ≥ α · σ, where α ≈ log n log log n, the probability that u and v have the same hash is < 1/n2.
",1.1. Our results,[0],[0]
"Thus, PLSH has a “cluster preserving” property.",1.1. Our results,[0],[0]
"We show the above by extending the analyses of (Indyk & Motwani, 1998) and (Andoni & Indyk, 2006).",1.1. Our results,[0],[0]
"Then, we use this observation to give a simple bi-criteria approximation algorithm for k-means clustering.",1.1. Our results,[0],[0]
(A bi-criteria algorithm is one that is allowed to output a slightly larger number of centers; see Section 2.),1.1. Our results,[0],[0]
"We assume knowledge of k, as well as a very rough estimate of the objective value.",1.1. Our results,[0],[0]
"The algorithm returns a polylogarithmically larger number of clusters, while obtaining a polylogarithmic factor approximation.",1.1. Our results,[0],[0]
We refer to Theorem 2 for the statement.,1.1. Our results,[0],[0]
"As we note below, if s > k polylog(()n), then we can avoid violating the bound on the number of clusters (and obtain a “true” guarantee as opposed to a bi-criteria one).
",1.1. Our results,[0],[0]
"The algorithm can be implemented in a distributed manner, specifically in the MAPREDUCE model, with dlogs ne+ 2 rounds, using machines of memory s (when we say memory s, we mean that each machine can store at most s of the points.",1.1. Our results,[0],[0]
"This will be roughly the same as measuring s in bytes, as we see in Section 2.",1.1. Our results,[0],[0]
The formal result is stated in Theorem 3.,1.1. Our results,[0],[0]
"We highlight that the distributed algorithm works for any s ≥ ω(log n), even s k (in which case the standard reduce-and-merge framework has no non-trivial guarantees).
",1.1. Our results,[0],[0]
"Finally, we prove that for any MapReduce algorithm that uses poly(n) machines of space s, the number of rounds necessary to obtain any non-trivial approximation to k-means is at least dlogs ne.",1.1. Our results,[0],[0]
"Thus the ‘round/memory tradeoff’ we
obtain is nearly optimal.",1.1. Our results,[0],[0]
"This is based on ideas from the recent remarkable result of (Roughgarden et al., 2016).",1.1. Our results,[0],[0]
(See Theorem 4.),1.1. Our results,[0],[0]
Going beyond communication lower bounds.,"1.2. Discussion, extensions and limitations",[0],[0]
"The recent result of (Chen et al., 2016) shows lower bounds on the total amount of communication necessary for distributed clustering.","1.2. Discussion, extensions and limitations",[0],[0]
"They show that for a worst-case partition of points across machines, Ω(Mk) bits are necessary, where M is the number of machines.","1.2. Discussion, extensions and limitations",[0],[0]
"Our result in Section 4.2 implies that if points have been partitioned across machines according to PLSH hashes, we can bypass this lower bound.
","1.2. Discussion, extensions and limitations",[0],[0]
Round lower bound.,"1.2. Discussion, extensions and limitations",[0],[0]
"In light of Theorem 4, one way to interpret our algorithmic result is as saying that as far as obtaining polylogarithmic bi-criteria approximations go, clustering is essentially as easy as “aggregation” (i.e., summing a collection of n numbers – which also has the same logs n upper and lower bounds).
","1.2. Discussion, extensions and limitations",[0],[0]
Precisely k clusters.,"1.2. Discussion, extensions and limitations",[0],[0]
"Theorem 3 gives only a bi-criteria guarantee, so it is natural to ask if we can obtain any guarantee when we desire precisely k centers.","1.2. Discussion, extensions and limitations",[0],[0]
"In the case when s ≥ k log2 n, we can apply known results to obtain this.","1.2. Discussion, extensions and limitations",[0],[0]
"(Guha et al., 2001) showed (in our notation) that:
Theorem 1.","1.2. Discussion, extensions and limitations",[0],[0]
"Let U be a set of points, and let S be a set of centers with the property that ∑ u∈U d(u, S)
2 ≤ γ · OPT, where OPT is the optimum k-means objective value on U .","1.2. Discussion, extensions and limitations",[0],[0]
"Let g : U 7→ S map every u ∈ U to its closest point in S, breaking ties arbitrarily.","1.2. Discussion, extensions and limitations",[0],[0]
"Now, consider a new weighted instance I of k-means where we have points in S, with weight of v ∈ S being |g−1(v)|.","1.2. Discussion, extensions and limitations",[0],[0]
"Then, any set of centers that ρ-approximate the optimum objective for I give a (4γ+ 2ρ) approximation to the original instance (given by U ).
","1.2. Discussion, extensions and limitations",[0],[0]
"Thus, if s ≥ k log2 n, we can aggregate the output of our bi-criteria algorithm onto one machine, and solve k-means approximately.","1.2. Discussion, extensions and limitations",[0],[0]
"In essence, we are using the output of our algorithm as a coreset for k-means.","1.2. Discussion, extensions and limitations",[0],[0]
"We demonstrate this in our experimental results.
","1.2. Discussion, extensions and limitations",[0],[0]
Balanced clustering.,"1.2. Discussion, extensions and limitations",[0],[0]
A common constraint for clustering algorithms is that of the clusters being balanced.,"1.2. Discussion, extensions and limitations",[0],[0]
This is often captured by requiring an upper bound on the size of a cluster.,"1.2. Discussion, extensions and limitations",[0],[0]
"(Bateni et al., 2014) showed that balanced clustering can also be solved in a distributed setting.","1.2. Discussion, extensions and limitations",[0],[0]
"Specifically, they showed that any bi-criteria algorithm for k-means can be used to solve the balanced clustering problem, via a result analogous to 1.","1.2. Discussion, extensions and limitations",[0],[0]
"In our context, this implies that if s > k log2 n, our method also gives a distributed algorithm for balanced clustering with a k-means objective.
Limitations and lower bounds.","1.2. Discussion, extensions and limitations",[0],[0]
"There are two key limita-
tions to our result.","1.2. Discussion, extensions and limitations",[0],[0]
"First, the polylogarithmic approximation factor in the approximation ratio seems difficult to avoid (although our experiments show that the guarantees are very pessimistic).","1.2. Discussion, extensions and limitations",[0],[0]
"In our argument, it arises as a by-product of being able to detect very small clusters.","1.2. Discussion, extensions and limitations",[0],[0]
"This is in contrast with single machine algorithms (e.g., (Kanungo et al., 2004; Ahmadian et al., 2017)) and the prior work in MapReduce algorithms, (Ene et al., 2011), which give constant factor approximations.","1.2. Discussion, extensions and limitations",[0],[0]
Another restriction is that our algorithms assume a Euclidean setting for the points.,"1.2. Discussion, extensions and limitations",[0],[0]
"The algorithms of (Ene et al., 2011) and related works can handle the case of arbitrary metric spaces.","1.2. Discussion, extensions and limitations",[0],[0]
The bottleneck here is the lack of locality sensitive hashing for such spaces.,"1.2. Discussion, extensions and limitations",[0],[0]
"A very interesting open problem is to develop new methods in this case, or prove stronger lower bounds.","1.2. Discussion, extensions and limitations",[0],[0]
We now introduce some notation and definitions that will be used for the rest of the paper.,2. Notation and Preliminaries,[0],[0]
We will denote by U the set of points in the input.,2. Notation and Preliminaries,[0],[0]
We denote n = |U |.,2. Notation and Preliminaries,[0],[0]
"All of our algorithms are for the Euclidean setting, where the points in U are in Rd, and the distance between x, y ∈ U is the `2 norm ‖x− y‖2 = √∑ i(xi",2. Notation and Preliminaries,[0],[0]
"− yi)2.
",2. Notation and Preliminaries,[0],[0]
"A k-clustering of the points U is a partition C of U into subsets C1, C2, . . .",2. Notation and Preliminaries,[0],[0]
", Ck.",2. Notation and Preliminaries,[0],[0]
The centroid of a cluster Ci is the point µi := 1|Ci| ∑ u∈Ci u.,2. Notation and Preliminaries,[0],[0]
The k-means objective for the clustering C is now defined as∑ i∈[k],2. Notation and Preliminaries,[0],[0]
∑ u∈Ci ‖u− µi‖22.,2. Notation and Preliminaries,[0],[0]
"(1)
The problem of k-means clustering is to find C that minimizes the objective defined above.",2. Notation and Preliminaries,[0],[0]
The minimum objective value will be denoted by OPT(U).,2. Notation and Preliminaries,[0],[0]
"(When the U is clear from context, we simply write OPT.)",2. Notation and Preliminaries,[0],[0]
A ρ-approximation algorithm for k-means clustering is a polynomial time algorithm that outputs a clustering C′ whose objective value is at most ρ ·OPT(U).,2. Notation and Preliminaries,[0],[0]
We will be interested in ρ being a constant or polylog(n).,2. Notation and Preliminaries,[0],[0]
"A (ρ, β) bi-criteria approximation (where β ≥ 1) is an efficient algorithm that outputs a clustering C′ that has at most βk clusters and has an objective value at most ρ ·OPT(U).",2. Notation and Preliminaries,[0],[0]
"Note that the optimum still has k clusters.
",2. Notation and Preliminaries,[0],[0]
Note on the dimension d.,2. Notation and Preliminaries,[0],[0]
We assume that d = O(log n).,2. Notation and Preliminaries,[0],[0]
"This is without loss of generality, because we may assume that we have pre-processed the data by applying JohnsonLindenstrauss transform.",2. Notation and Preliminaries,[0],[0]
"As the JL transform preserves all pairwise `2 distances (Johnson & Lindenstrauss, 1984; Indyk & Motwani, 1998), clustering in the transformed space gives a (1 + ) approximation to clustering in the original one.",2. Notation and Preliminaries,[0],[0]
"Furthermore, the transform can be applied in parallel, individually to each point.",2. Notation and Preliminaries,[0],[0]
"Thus we henceforth assume that the space required to store a point is O(log n).
MapReduce model.",2. Notation and Preliminaries,[0],[0]
"To illustrate our ideas, we use the well-studied MapReduce model (Dean & Ghemawat, 2004; Karloff et al., 2010).",2. Notation and Preliminaries,[0],[0]
The details of the map and reduce operations are not important for our purpose.,2. Notation and Preliminaries,[0],[0]
We will view it as a model of computation that proceeds in levels.,2. Notation and Preliminaries,[0],[0]
"At each level, we have M machines that can perform computation on their local data (the input is distributed arbitrarily among machines in the first layer).",2. Notation and Preliminaries,[0],[0]
"Once all the machines are done with computation, they send information to machines in the next layer.",2. Notation and Preliminaries,[0],[0]
The information received by a machine acts as its local data for the next round of computation.,2. Notation and Preliminaries,[0],[0]
"We assume that each machine has a memory of s.
Constants.",2. Notation and Preliminaries,[0],[0]
"For the sake of easier exposition, we do not attempt to optimize the constants in our bounds.",2. Notation and Preliminaries,[0],[0]
The key to our distributed algorithm is a two step hashing scheme.,3. Two Step Hashing,[0],[0]
"The first step is a ‘product’ of locality sensitive hashes (LSH), and the second is a random hash that maps the tuples obtained from the product-LSH to a bin with a label in the range 1, . . .",3. Two Step Hashing,[0],[0]
", Lk, for an appropriate constant L.",3. Two Step Hashing,[0],[0]
We begin with a short discussion of LSH.,3.1. Product LSH,[0],[0]
"We follow the presentation of (Andoni & Indyk, 2006).2
Suppose we have a collection of points U in Rd.
",3.1. Product LSH,[0],[0]
Locality sensitive hashing (LSH).,3.1. Product LSH,[0],[0]
"Let t, w be parameters.",3.1. Product LSH,[0],[0]
"LSHt,w is a procedure that takes a u ∈ U , and produces a (t + 1)-tuple of integers.",3.1. Product LSH,[0],[0]
"The hash uses as parameters a matrix A of dimensions t × d, whose entries are i.i.d. N",3.1. Product LSH,[0],[0]
"(0, 1) Gaussians, and a collection of shift vectors S = {s1, . . .",3.1. Product LSH,[0],[0]
", sU}, where si is picked uniformly from [0, 4w]t.",3.1. Product LSH,[0],[0]
"The shifts are used to generate a collection of shifted grids Gti := G
t + si, where Gt is the integer grid Zt, scaled by 4w.",3.1. Product LSH,[0],[0]
"Now to compute the hash of a point u, first its projection to Rt is computed by u′ = Au.",3.1. Product LSH,[0],[0]
"Next, one searches for the smallest index",3.1. Product LSH,[0],[0]
i ∈,3.1. Product LSH,[0],[0]
"[U ] for which the ball B(u′, w) contains a point of the shifted grid Gti.",3.1. Product LSH,[0],[0]
"(Alternately one could imagine radius-w balls around the points in the shifted grids, and we look for the smallest i for which the point u′ is contained in one such ball.)",3.1. Product LSH,[0],[0]
"The hash of the point is then (i, x1, . . .",3.1. Product LSH,[0],[0]
", xt), where i is the index as above, and (x1, . . .",3.1. Product LSH,[0],[0]
", xt) are the integer coordinates corresponding to the grid point in Gti that is at distance ≤ w from u′.
(Andoni & Indyk, 2006) show that to cover all of Rt (and thus to have a well-defined hash for every point), the number of shifts that suffice is 2O(t log t).",3.1. Product LSH,[0],[0]
"Consequently, this is also
2The earlier LSH schemes of (Indyk & Motwani, 1998) and (Datar et al., 2004) can also be used; however, they give a weaker approximation factor.
",3.1. Product LSH,[0],[0]
"the time required to compute hash for a point, as we may need to go through all the shifts.",3.1. Product LSH,[0],[0]
"In our setting, we will choose t = o(log n/ log log n), and thus the time needed to hash is no(1).
",3.1. Product LSH,[0],[0]
Product LSH (PLSH).,3.1. Product LSH,[0],[0]
"Given an integer `, the product LSH PLSHt,w,` is a hashing scheme that maps a point u to a concatenation of ` independent copies of LSHt,w; it thus outputs an `(t+ 1)-tuple of integers.
",3.1. Product LSH,[0],[0]
We show the following properties of PLSH.,3.1. Product LSH,[0],[0]
"In what follows, let σ be a parameter.",3.1. Product LSH,[0],[0]
"Let
w = 8σ(log n)3/2; t = log n
(log log n)2 ; ` = 32(log log n)2.
",3.1. Product LSH,[0],[0]
"(2)
Lemma 1. Suppose C ⊆ U has diameter ≤ σ.",3.1. Product LSH,[0],[0]
"Then with probability at least 3/4, PLSHt,w,` maps all the points in C to the same hash value.",3.1. Product LSH,[0],[0]
Lemma 2.,3.1. Product LSH,[0],[0]
"Let u, v be points that are at distance≥ 4w/",3.1. Product LSH,[0],[0]
√ t (= O(log n log log n) · σ).,3.1. Product LSH,[0],[0]
"Then the probability that they have the same hash value is < 1n4 .
",3.1. Product LSH,[0],[0]
Proof of Lemma 1.,3.1. Product LSH,[0],[0]
Let C ′ = {,3.1. Product LSH,[0],[0]
Ax : x ∈ C}.,3.1. Product LSH,[0],[0]
"First, we claim that the diameter ofC ′ is at most 4σ √ t+ log n, w.h.p.",3.1. Product LSH,[0],[0]
over the choice of A.,3.1. Product LSH,[0],[0]
"This is because for any x, y ∈ C, the quantity ‖A(x− y)‖22/‖x− y‖22 is distributed as a χ2 with t degrees of freedom.",3.1. Product LSH,[0],[0]
"It is known (e.g., (Laurent & Massart, 2000), Lemma 1) that the tail of a chi-square statistic Y with t degrees of freedom satisfies: for any z > 0, Pr[Y ≥ t + 2 √ tz + 2z] ≤ e−z .",3.1. Product LSH,[0],[0]
"Setting z = 4 log n, and using 2 √ tz ≤ t+ z, we get that Pr[Y ≥ 16(t+ log n)]",3.1. Product LSH,[0],[0]
< 1/n4.,3.1. Product LSH,[0],[0]
"Thus by taking union bound over all pairs of points x, y, we have that with probability ≥ 1− 1n2 , the diameter of C
′ is ≤ 4σ √ t+ log n.
Conditioned on this, let us calculate the probability that the points in C all have the same hash value.",3.1. Product LSH,[0],[0]
"(The conditioning does not introduce any dependencies, as the above argument depended only on the choice of A, while the next step will depend on the choice of the shifts.)",3.1. Product LSH,[0],[0]
"Now, consider a ball B∗ ⊆ Rt of radius r′ := 4σ √ t+ log n that contains C ′",3.1. Product LSH,[0],[0]
"(as C ′ is of small diameter, such a ball exists).
",3.1. Product LSH,[0],[0]
"Before analyzing the probability of interest, let us understand when a shifted grid Gti contains a point that has distance ≤",3.1. Product LSH,[0],[0]
w to a given point x.,3.1. Product LSH,[0],[0]
This is equivalent to saying that (x−si) is w-close to a lattice point inGt.,3.1. Product LSH,[0],[0]
"This happens iff si is in the ball B(x,w), where the ball has been reduced modulo",3.1. Product LSH,[0],[0]
"[0, 4w]t (see Figure 1).
",3.1. Product LSH,[0],[0]
"Now, we can see how it could happen that some x ∈ B∗ is w-close to a lattice point in Gti but the entirety of B
∗ does not have this property.",3.1. Product LSH,[0],[0]
"Geometrically, the bad choices of si are shown in Figure 1 (before reducing moulo",3.1. Product LSH,[0],[0]
"[0, 4w]t).",3.1. Product LSH,[0],[0]
"Thus, we have that the probability that all points in B∗ are w-close to a lattice point in Gti conditioned on there existing
a point x ∈ B∗ that is close to a lattice point is at least
p1 := (w − r′)t
(w + r′)t =
( 1− 2r ′
w + r′
)t .
",3.1. Product LSH,[0],[0]
"Thus p1 is a lower bound on a single LSH giving the same hash value for all the points inC. Repeating this ` times, and plugging in our choice of values for r′, w, t, `, the desired claim follows.
",3.1. Product LSH,[0],[0]
"Next, we get to the proof of Lemma 2.
",3.1. Product LSH,[0],[0]
Proof.,3.1. Product LSH,[0],[0]
"Let u, v be points as in the statement of the lemma.",3.1. Product LSH,[0],[0]
We show that the probability that ‖A(u− v)‖ ≤ 2w in all the ` hashes is < 1/n4.,3.1. Product LSH,[0],[0]
"This clearly implies what we need, because if in even one LSH we haveAu",3.1. Product LSH,[0],[0]
"andAv being> 2w away, they cannot have the same PLSH.3
Now, for a randomA, the quantity ‖A(u−v)‖22/‖u−v‖22 is distributed as a χ2 distribution with t degrees of freedom (as we saw earlier), and thus using the lower tail from (Laurent & Massart, 2000), we get that for any z > 0, for such a random variable Y , we have Pr[Y ≤ t",3.1. Product LSH,[0],[0]
− 2 √ tz] < e−z .,3.1. Product LSH,[0],[0]
Thus Pr[Y ≤ (1− 1√ 2 )t] ≤ e−t/8.,3.1. Product LSH,[0],[0]
"Now, for our choice of parameters, we have 4w2/‖u",3.1. Product LSH,[0],[0]
− v‖22 ≤ t/4 <,3.1. Product LSH,[0],[0]
"(1 − 1√2 )t, and thus the probability that u and v have the same PLSH is upper bounded by e−t`/8 = 1/n4, as desired.",3.1. Product LSH,[0],[0]
We have shown that the probability that a cluster of diameter ≤ σ hashes to precisely one tuple (for appropriate parameters) is ≥ 3/4.,3.2. Number of tuples for a cluster,[0],[0]
"We now show something slightly stronger (as we will need it later).
",3.2. Number of tuples for a cluster,[0],[0]
Lemma 3.,3.2. Number of tuples for a cluster,[0],[0]
"Let C be a cluster of diameter σ, and let t, w, ` be set as in Eq. (2).",3.2. Number of tuples for a cluster,[0],[0]
"The expected number of distinct tuples
3This reasoning allows us to get a bound slightly better than (Andoni & Indyk, 2006).
for points in C (produced by PLSHt,w,`) is O(1).",3.2. Number of tuples for a cluster,[0],[0]
The PLSH maps each point u ∈ U to an `(t + 1) tuple of integers.,3.3. Second step of hashing,[0],[0]
"The second step of hashing is very simple – we simply hash each tuple independently and uniformly to an integer in [Lk], for a prescribed parameter L.",3.3. Second step of hashing,[0],[0]
We start by describing our algorithm in a single machine setting.,4. Approximation Algorithm,[0],[0]
"Then in Section 4.2, we describe how it can be implemented in parallel, with a small number of machines, and a small memory per machine.",4. Approximation Algorithm,[0],[0]
"The high level idea in our algorithm is to perform the twolevel hashing above, and choose a random subset of points from each bin.
",4.1. Main algorithm,[0],[0]
"Now, in order to choose the w parameter in the hash, we need a rough scale of the optimum.",4.1. Main algorithm,[0],[0]
"To this end, we will assume that we know a D such that the optimum objective value is in the range (D/f,D), for some f = poly(n).",4.1. Main algorithm,[0],[0]
"Note that f can be something like n2, so this is a very mild assumption.",4.1. Main algorithm,[0],[0]
"With this assumption, we have that the average contribution of a point to the objective (i.e., its squared distance to its center) is≥ D/(n · f).",4.1. Main algorithm,[0],[0]
Let us denote r0 := √ D/(n · f).,4.1. Main algorithm,[0],[0]
"Also, observe that no point can have a contribution more than D to the objective (as it is an upper bound on the sum of the contributions).",4.1. Main algorithm,[0],[0]
"Thus, inuitively, all the clusters have a radius (formally defined below) ≤",4.1. Main algorithm,[0],[0]
"√ D. Let κ = dlog(nf)e, and let ri := 2i/2r0, for 1 ≤",4.1. Main algorithm,[0],[0]
i ≤,4.1. Main algorithm,[0],[0]
κ.,4.1. Main algorithm,[0],[0]
These will be the different radius “scales” for our clusters of interest.,4.1. Main algorithm,[0],[0]
"Note that κ = O(log n), as f = poly(n).
",4.1. Main algorithm,[0],[0]
The algorithm can now be described (see Algorithm 4.1).,4.1. Main algorithm,[0],[0]
"Algorithm 1 Find-Cover
Input: set of points U , rough estimate of optimum D. Output: a subset of points S. for i = 1 . . .",4.1. Main algorithm,[0],[0]
"κ do
- Hash every point in U to a bin (range [Lk]) using the two layer hash with params t, wi, `, Lk, where wi := 8ri(log n)
3/2.",4.1. Main algorithm,[0],[0]
"Let Uj be the points hashed to bin j. - Let Gj be the group of machines assigned for bin j. For each j, assign points in Uj uniformly at random to a machine in Gj . -",4.1. Main algorithm,[0],[0]
"For each j, select a uniformly random subset of Uj of size O(1) from Gj and add them to S. (If the number of points in the group is O(1), add all of them to S.)
end for
In the remainder of this section, we analyze this algorithm.",4.1. Main algorithm,[0],[0]
"We start with a definition.
",4.1. Main algorithm,[0],[0]
Definition 1 (Cluster radius).,4.1. Main algorithm,[0],[0]
"For a cluster C with centroid µ, we define the radius to be the quantity ρ :=√
1 |C| ∑ p∈C‖p− µ‖22, i.e., the “`22 average” radius.
",4.1. Main algorithm,[0],[0]
Observation 1.,4.1. Main algorithm,[0],[0]
"In any clusterC with centroid µ and radius ρ, the number of points p in C such that ‖p− µ‖2 > 2ρ is at most |C|/4.
",4.1. Main algorithm,[0],[0]
The proof follows by an averaging argument.,4.1. Main algorithm,[0],[0]
"Now, a candidate goal is to prove that for every optimum cluster Ci (center µi, radius ρi), the algorithm chooses at least one point at a distance ≤ αρi from the center µi with high probability, for some small α.
",4.1. Main algorithm,[0],[0]
Unfortunately this statement is false in general.,4.1. Main algorithm,[0],[0]
Suppose the instance has an optimal cluster with small ρi and a small |Ci| that is really far from the rest of the points (thus it is essential to “find” that cluster).,4.1. Main algorithm,[0],[0]
"In this case, for rj that is roughly ρi (which is the scale at which we hope to find a point close to this cluster), the bin containing Ci may contain many other points that are far away; thus a random sample is unlikely to choose any point close to Ci.
",4.1. Main algorithm,[0],[0]
The fix for this problem comes from the observation that small clusters (i.e. small |Ci|) can afford to pay more per point to the objective.,4.1. Main algorithm,[0],[0]
We thus define the notion of “adjusted radius” of a cluster.,4.1. Main algorithm,[0],[0]
"First, we define θ to be the real number satisfying OPT = nθ2, i.e., the typical distance of a point to its cluster center in the optimal",4.1. Main algorithm,[0],[0]
"clustering.4 Now, we have:
Definition 2 (Adjusted radius).",4.1. Main algorithm,[0],[0]
"For a cluster C with radius ρ, we define the adjusted radius to be the quantity ρ satisfying ρ2 = ρ2 + θ2 + nθ 2
k|C| .
",4.1. Main algorithm,[0],[0]
"Our main lemma about the algorithm is the following.
",4.1. Main algorithm,[0],[0]
Lemma 4.,4.1. Main algorithm,[0],[0]
Let C be a cluster in the optimal clustering with adjusted radius ρ.,4.1. Main algorithm,[0],[0]
"With probability ≥ 1/4, Algorithm 4.1 outputs a point that is at a distance ≤ α · ρ from the center of the cluster C, where α = O(log n log log n).
",4.1. Main algorithm,[0],[0]
"This is used to show the main result of this section.
",4.1. Main algorithm,[0],[0]
Theorem 2.,4.1. Main algorithm,[0],[0]
Let S′ be the union of the sets output by O(log k) independent runs of Algorithm 4.1.,4.1. Main algorithm,[0],[0]
"For α = O(log n log log n), S′ gives an (α2, O(log n log k))",4.1. Main algorithm,[0],[0]
"bicriteria approximation for k-means, w.p. at least 9/10.
",4.1. Main algorithm,[0],[0]
Proof of Theorem 2 assuming Lemma 4.,4.1. Main algorithm,[0],[0]
"First, let us analyze the number of points output.",4.1. Main algorithm,[0],[0]
"Note that in each run of the algorithm, we output O(Lk) = O(k) points for each radius range.",4.1. Main algorithm,[0],[0]
There areO(log n) radius ranges andO(log k) independent runs.,4.1. Main algorithm,[0],[0]
"Thus we have the desired bound.
",4.1. Main algorithm,[0],[0]
"Next, consider the approximation factor.",4.1. Main algorithm,[0],[0]
"As we take S′ to be the union of O(log k) independent runs of Algorithm 4.1,
4We note that θ is used solely for analysis purposes – the algorithm is not assumed to know it.
",4.1. Main algorithm,[0],[0]
"the success probability in Lemma 4 can be boosted to 1− 1 10k , and by a union bound, we have that the conclusion of the lemma holds for all clusters, with probability > 1/10.",4.1. Main algorithm,[0],[0]
"Thus for every optimal cluster Ci of adjusted radius ρi, Algorithm 4.1 outputs at least one point at a distance≤ α·ρi, for α as desired.",4.1. Main algorithm,[0],[0]
"Thus, assigning all the points in Ci to one such point would imply that the points of Ci contribute at most |Ci|ρ2i +α2|Ci|ρ2i to the objective.5 Thus the objective value is at most∑
i
|Ci|ρ2i + α2|Ci| ( ρ2i + θ 2 + nθ2
k|Ci| ) =",4.1. Main algorithm,[0],[0]
"(1 + α2)OPT + α2 · 2nθ2 ≤ 4α2OPT.
",4.1. Main algorithm,[0],[0]
This completes the proof.,4.1. Main algorithm,[0],[0]
"We now see how to implement algorithm from Theorem 2 in a distributed setting with a small number of rounds and machines, while also using memory ≤ s per machine.",4.2. Distributed implementation,[0],[0]
"Our final result is the following.
",4.2. Distributed implementation,[0],[0]
Theorem 3.,4.2. Distributed implementation,[0],[0]
"There is a distributed algorithm that performs dlogs ne + 2 rounds of MAPREDUCE computation, and outputs a bi-criteria approximation to k-means, with the same guarantee as Theorem 2.",4.2. Distributed implementation,[0],[0]
"The number of machines needed is Õ ( n
min{k,s} · k s
) , and the space per machine is
s.
Note.",4.2. Distributed implementation,[0],[0]
"Whenever s ≥ k, the bound on the number of machines is Õ(n/s), which is essentially optimal, because we need n/s machines to hold n points, if each machine has a memory of s.
While most parts of the algorithm from Theorem 2 can be immediately parallelized, sampling from Uj (which may need to be split across machines) is the tricky part and requires some work.",4.2. Distributed implementation,[0],[0]
The proof is deferred to Section 3 of the supplement.,4.2. Distributed implementation,[0],[0]
"We now show that even in the very simple setting of points on a line, we have a tradeoff between the number of rounds and memory.",5. Lower Bound,[0],[0]
"This matches the behavior of our algorithm, up to an additive constant.
Theorem 4.",5. Lower Bound,[0],[0]
Let α be any parameter that is poly(n).,5. Lower Bound,[0],[0]
"Then, any α factor approximation algorithm for k-means with k ≥ 2 that uses poly(n) machines of memory ≤ s requires at least logs n rounds of MAPREDUCE.",5. Lower Bound,[0],[0]
"The proof is by a simple reduction from Boolean-OR,6 a
5This follows from a “center-of-mass” theorem that is standard: for a set T of points with centroid µ, and any other point µ′,∑
u∈T ‖u− µ ′‖2 = ∑ u∈T ‖u− µ‖
2 + |T |‖µ− µ′‖2.",5. Lower Bound,[0],[0]
"6The input is the set of bits x1, . . .",5. Lower Bound,[0],[0]
", xn, and the desired output
problem for which a round-memory trade-off was established in (Roughgarden et al., 2016).
",5. Lower Bound,[0],[0]
Proof.,5. Lower Bound,[0],[0]
"Suppose we have inputs x1, . . .",5. Lower Bound,[0],[0]
", xn, the inputs for Boolean OR.",5. Lower Bound,[0],[0]
"We produce an instance of clustering with k + n points, all on the line.
",5. Lower Bound,[0],[0]
"First, we place points at 1, 2, . . .",5. Lower Bound,[0],[0]
", k. Additionally, for 1 ≤ i ≤ n, if xi = 1, we add a point at k+α+1.",5. Lower Bound,[0],[0]
"If xi = 0, add a point at 1.",5. Lower Bound,[0],[0]
"Now if the OR of the xi’s is TRUE, then the optimum solution places centers at 1, 2, . . .",5. Lower Bound,[0],[0]
", k−1, k+α+1.",5. Lower Bound,[0],[0]
This results in an objective value of 1.,5. Lower Bound,[0],[0]
"If the OR is FALSE, the optimum solution is to place centers at 1, 2, . . .",5. Lower Bound,[0],[0]
", k (0 cost).",5. Lower Bound,[0],[0]
"Thus an α factor approximation should be able to distinguish between the two cases (because in the NO case, it needs to have error 0, and in the YES case, this solution will be a factor",5. Lower Bound,[0],[0]
"> α off.
",5. Lower Bound,[0],[0]
Note.,5. Lower Bound,[0],[0]
The parameter α implicitly comes up in the reduction.,5. Lower Bound,[0],[0]
The number of bits necessary to write down the points xi is n logα.,5. Lower Bound,[0],[0]
"This is why we insist on α = poly(n).
",5. Lower Bound,[0],[0]
"The lower bound above can be extended in order to rule out both (a) the case in which we have a rough estimate of the optimum (as in our algorithm), and (b) bi-criteria approximations.",5. Lower Bound,[0],[0]
"To handle (a), we can perturb the NO case so as to make the objective 1/p(nα) for a large polynomial p(·).",5. Lower Bound,[0],[0]
"In order to handle (b), i.e., to rule out an (α, β) bicriteria approximation, we need to add a multiplicity of βk for each of the points in the proof above.",5. Lower Bound,[0],[0]
This leads to a slightly weaker lower bound of logs n kβ rounds.,5. Lower Bound,[0],[0]
"The details of these steps are straightforward, so we omit them.",5. Lower Bound,[0],[0]
"We evaluate our algorithmic ideas on two synthetic and two real datasets, of varying sizes.",6. Empirical Study,[0],[0]
"In the former case, we know the ground truth clustering, the “right k”, and the optimum objective value.",6. Empirical Study,[0],[0]
We use it to demonstrate how the quality of clustering depends on the parameter ` – the number of independent hashes we concatenate.,6. Empirical Study,[0],[0]
"In all the datasets, we compare the objective value obtained by our algorithm with the one obtained by k-means++ (part of scikit-learn (Pedregosa et al., 2011)).",6. Empirical Study,[0],[0]
"This will only be possible for small enough datasets, as k-means++ is a single machine algorithm that uses Ω(nk) memory.",6. Empirical Study,[0],[0]
Both the datasets we consider are mixtures of Gaussians.,6.1. Synthetic datasets,[0],[0]
"The first has n = 105 points in R50 and k = 100, while the second has n = 106 point in R50 and k = 1000.",6.1. Synthetic datasets,[0],[0]
For i ∈,6.1. Synthetic datasets,[0],[0]
"[k], the centers are chosen uniformly from a box of length 400 in each dimension, maintaining a distance of 20 from one
is simply the OR of the bits.
another.",6.1. Synthetic datasets,[0],[0]
"To form cluster Ci, a random number of points are chosen from the Gaussian N (µi, 1).
",6.1. Synthetic datasets,[0],[0]
"For the first dataset, we produce PLSH tuples using w = 15, t = 2, and vary `.We call the set of points that hash to a given tuple a bucket of points.",6.1. Synthetic datasets,[0],[0]
"We measure the following quantities: (a) the total number of non-empty buckets, (b) the “purity” of the buckets, i.e., the number of distinct clusters at intersect a non-empty bucket (on average), and (c) the “spread” of a cluster, i.e., the number of buckets that points of a cluster go to.",6.1. Synthetic datasets,[0],[0]
The plots for these quantities as ` varies are shown in Figure 2.,6.1. Synthetic datasets,[0],[0]
"Note that it makes sense for the spread to increase as ` increases, as even a difference in one of the ` independent hashes results in unequal hashes.
",6.1. Synthetic datasets,[0],[0]
"Next, we study the objective value.",6.1. Synthetic datasets,[0],[0]
For this we choose ` = 3.,6.1. Synthetic datasets,[0],[0]
This results in 257 non-empty buckets.,6.1. Synthetic datasets,[0],[0]
"Now, from each bucket, we output j points uniformly at random to form a set S (and do this for different j).",6.1. Synthetic datasets,[0],[0]
"Even for j = 1, the objective value is 41079, which is less than a factor 2 away from the optimum, 26820.",6.1. Synthetic datasets,[0],[0]
This is significantly better than the guarantee we get from Theorem 2.,6.1. Synthetic datasets,[0],[0]
"It is also significantly better than a random subset of 257 points, for which it turns out that the objective is 5897317.
",6.1. Synthetic datasets,[0],[0]
"Intuitively, a random sample will be bad for this instance, because there are many clusters of size n/k, and no points from such clusters will be chosen.",6.1. Synthetic datasets,[0],[0]
This motivates us to measure the cluster recall of our procedure – how many clusters contribute to the 257 size set we output?,6.1. Synthetic datasets,[0],[0]
"Interestingly, all 100 of the clusters do, for the above values of the parameters.",6.1. Synthetic datasets,[0],[0]
"These results are consistent with the theoretical observations that PLSH finds small-cardinality clusters while a random sample does not.
",6.1. Synthetic datasets,[0],[0]
"Next, consider the larger synthetic dataset.",6.1. Synthetic datasets,[0],[0]
"Modulo n, k, the data is generated as before.",6.1. Synthetic datasets,[0],[0]
"Here, we produce PLSH tuples using w = 15, t = 3, and ` = 4.",6.1. Synthetic datasets,[0],[0]
"For these choices of n and k, the single-machine k-means++ runs out of memory.",6.1. Synthetic datasets,[0],[0]
"However, as we know the µi, we can estimate the optimum
objective value, which is 251208.
",6.1. Synthetic datasets,[0],[0]
"In this dataset, we illustrate the use of our algorithm to generate a coreset, as discussed in Section 1.2.",6.1. Synthetic datasets,[0],[0]
"We obtain 5722 buckets, from each of which we sample j points to obtain a coreset S. We then run k-means on S with k = 1000, thus obtaining 1000 centers.",6.1. Synthetic datasets,[0],[0]
We evaluate the k-means objective with these centers.,6.1. Synthetic datasets,[0],[0]
Results for different j are shown in Figure 3.,6.1. Synthetic datasets,[0],[0]
"Note that even with j = 10, the objective is within a 1.1 factor of the optimum.",6.1. Synthetic datasets,[0],[0]
"We show our results on two datasets, both available via the UC Irvine dataset repository.
SUSY.",6.2. Real datasets,[0],[0]
"The first dataset is SUSY (see (P. Baldi, 2014)), which contains 5M points with 18 features.",6.2. Real datasets,[0],[0]
"In order to efficiently compare with k-means++, we use a sub-sample of 100000 points.",6.2. Real datasets,[0],[0]
"In this case we use the values of t, ` as in our theoretical results.",6.2. Real datasets,[0],[0]
"We also try different values for w. We start with a guess of w = σ(log n)3/2, where σ was obtained from k-means++ with k = 10 (which is very fast).",6.2. Real datasets,[0],[0]
We then scale σ from 2−4 to 22 in order to perform the hashing in different radius ranges.,6.2. Real datasets,[0],[0]
"After hashing and finding S, we use it as a coreset and compute k-means.",6.2. Real datasets,[0],[0]
"Figure 4 shows the results, and also compares against a fully random subset of points.",6.2. Real datasets,[0],[0]
"Unlike the synthetic examples, here a random set of points is not orders of magnitude worse, but is still considerably worse than the output of our algorithm.",6.2. Real datasets,[0],[0]
We also note that our values are within a factor 1.2 of k-means++ (which is sequential and significantly slower).,6.2. Real datasets,[0],[0]
"The number of buckets per cluster when k = 600 for ` = 1, . . .",6.2. Real datasets,[0],[0]
", 6 are 0.03, 0.31, 1.21, 3.97, 7, 10.55.
",6.2. Real datasets,[0],[0]
"FMA: A Dataset For Music Analysis This dataset (see (Defferrard et al., 2017)) contains 518 features extracted from audio files available in the free music archive (FMA).",6.2. Real datasets,[0],[0]
It has 106574 points.,6.2. Real datasets,[0],[0]
We perform the same experiment we did for the SUSY dataset.,6.2. Real datasets,[0],[0]
"Figure 5 shows
the results, comparing the outputs with the output of kmeans++, as well as a random subset.",6.2. Real datasets,[0],[0]
"The number of buckets per cluster when k = 512 for ` = 1, . . .",6.2. Real datasets,[0],[0]
", 6 are 0.08, 0.68, 2.29, 5.27, 9.43, 14.09 respectively.",6.2. Real datasets,[0],[0]
"Given the importance of clustering in the analysis of large scale data, distributed algorithms for formulations such as k-means, k-median, etc. have been extensively studied.",abstractText,[0],[0]
"A successful approach here has been the “reduce and merge” paradigm, in which each machine reduces its input size to Õ(k), and this data reduction continues (possibly iteratively) until all the data fits on one machine, at which point the problem is solved locally.",abstractText,[0],[0]
"This approach has the intrinsic bottleneck that each machine must solve a problem of size ≥ k, and needs to communicate at least Ω(k) points to the other machines.",abstractText,[0],[0]
"We propose a novel data partitioning idea to overcome this bottleneck, and in effect, have different machines focus on “finding different clusters”.",abstractText,[0],[0]
"Under the assumption that we know the optimum value of the objective up to a poly(n) factor (arbitrary polynomial), we establish worst-case approximation guarantees for our method.",abstractText,[0],[0]
We see that our algorithm results in lower communication as well as a near-optimal number of ‘rounds’ of computation (in the popular MapReduce framework).,abstractText,[0],[0]
Distributed Clustering via LSH Based Data Partitioning,title,[0],[0]
Given n vectors Xn def=,1.1. Background,[0],[0]
"X 1 , X 2 . .",1.1. Background,[0],[0]
.,1.1. Background,[0],[0]
", Xn 2 Rd that reside on n clients, the goal of distributed mean estimation is to estimate the mean of the vectors:
¯X def =
1
n
nX
i=1
Xi.",1.1. Background,[0],[0]
"(1)
This basic estimation problem is used as a subroutine in several learning and optimization tasks where data is distributed across several clients.",1.1. Background,[0],[0]
"For example, in Lloyd’s algorithm (Lloyd, 1982) for k-means clustering, if data is distributed across several clients, the server needs to compute
1Google Research, New York, NY, USA 2Google Research, Seattle, WA, USA.",1.1. Background,[0],[0]
"Correspondence to: Ananda Theertha Suresh <theertha@google.com>.
",1.1. Background,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1.1. Background,[0],[0]
"Copyright 2017 by the author(s).
",1.1. Background,[0],[0]
the means of all clusters in each update step.,1.1. Background,[0],[0]
"Similarly, for PCA, if data samples are distributed across several clients, then for the power-iteration method, the server needs to average the output of all clients in each step.
",1.1. Background,[0],[0]
"Recently, algorithms involving distributed mean estimation have been used extensively in training large-scale neural networks and other statistical models (McDonald et al., 2010; Povey et al., 2014; Dean et al., 2012; McMahan et al., 2016; Alistarh et al., 2016).",1.1. Background,[0],[0]
"In a typical scenario of synchronized distributed learning, each client obtains a copy of a global model.",1.1. Background,[0],[0]
The clients then update the model independently based on their local data.,1.1. Background,[0],[0]
"The updates (usually in the form of gradients) are then sent to a server, where they are averaged and used to update the global model.",1.1. Background,[0],[0]
A critical step in all of the above algorithms is to estimate the mean of a set of vectors as in Eq.,1.1. Background,[0],[0]
"(1).
",1.1. Background,[0],[0]
One of the main bottlenecks in distributed algorithms is the communication cost.,1.1. Background,[0],[0]
"This has spurred a line of work focusing on communication cost in learning (Tsitsiklis & Luo, 1987; Balcan et al., 2012; Zhang et al., 2013; Arjevani & Shamir, 2015; Chen et al., 2016).",1.1. Background,[0],[0]
"The communication cost can be prohibitive for modern applications, where each client can be a low-power and low-bandwidth device such as a mobile phone (Konečnỳ et al., 2016).",1.1. Background,[0],[0]
"Given such a wide set of applications, we study the basic problem of achieving the optimal minimax rate in distributed mean estimation with limited communication.
",1.1. Background,[0],[0]
"We note that our model and results differ from previous works on mean estimation (Zhang et al., 2013; Garg et al., 2014; Braverman et al., 2016) in two ways: previous works assume that the data is generated i.i.d.",1.1. Background,[0],[0]
according to some distribution; we do not make any distribution assumptions on data.,1.1. Background,[0],[0]
"Secondly, the objective in prior works is to estimate the mean of the underlying statistical model; our goal is to estimate the empirical mean of the data.",1.1. Background,[0],[0]
"Our proposed communication algorithms are simultaneous and independent, i.e., the clients independently send data to the server and they can transmit at the same time.",1.2. Model,[0],[0]
"In any independent communication protocol, each client transmits a function of Xi (say f(Xi)), and a central server estimates the mean by some function of f(X
1 ), f(X 2 ), . . .",1.2. Model,[0],[0]
", f(Xn).
",1.2. Model,[0],[0]
"Let ⇡ be any such protocol and let Ci(⇡, Xi) be the expected number of transmitted bits by the i-th client during protocol ⇡, where throughout the paper, expectation is over the randomness in protocol ⇡.
",1.2. Model,[0],[0]
"The total number of bits transmitted by all clients with the protocol ⇡ is
C(⇡, Xn) def= nX
i=1
Ci(⇡, Xi).
",1.2. Model,[0],[0]
Let the estimated mean be ˆ¯X .,1.2. Model,[0],[0]
"For a protocol ⇡, the MSE of the estimate is
E(⇡, Xn) = E  ˆ¯X ¯X 2
2
.
",1.2. Model,[0],[0]
We allow the use of both private and public randomness.,1.2. Model,[0],[0]
"Private randomness refers to random values that are generated by each machine separately, and public randomness refers to a sequence of random values that are shared among all parties1.
",1.2. Model,[0],[0]
The proposed algorithms work for any Xn.,1.2. Model,[0],[0]
"To measure the minimax performance, without loss of generality, we restrict ourselves to the scenario where each Xi 2 Sd, the ball of radius 1 in Rd, i.e., X 2 Sd iff
||X|| 2  1, where ||X||
2
denotes the ` 2 norm of the vector X .",1.2. Model,[0],[0]
"For a protocol ⇡, the worst case error for all Xn 2 Sd is
E(⇡, Sd) def=",1.2. Model,[0],[0]
"max Xn:Xi2Sd 8i E(⇡, Xn).
",1.2. Model,[0],[0]
Let ⇧(c) denote the set of all protocols with communication cost at most c.,1.2. Model,[0],[0]
"The minimax MSE is
E(⇧(c), Sd) def=",1.2. Model,[0],[0]
"min ⇡2⇧(c) E(⇡, Sd).",1.2. Model,[0],[0]
"We first analyze the MSE E(⇡, Xn) for three algorithms, when C(⇡, Xn) = ⇥(nd), i.e., each client sends a constant number of bits per dimension.
",1.3.1. ALGORITHMS,[0],[0]
Stochastic uniform quantization.,1.3.1. ALGORITHMS,[0],[0]
"In Section 2.1, as a warm-up we first show that a naive stochastic binary quantization algorithm (denoted by ⇡sb) achieves an MSE of
E(⇡sb, Xn) = ⇥",1.3.1. ALGORITHMS,[0],[0]
"d
n · 1 n
nX
i=1
||Xi||2 2
!",1.3.1. ALGORITHMS,[0],[0]
",
1In the absence of public randomness, the server can communicate a random seed that can be used by clients to emulate public randomness.
and C(⇡sb, Xn) = n · (d + ˜O(1))2, i.e., each client sends one bit per dimension.",1.3.1. ALGORITHMS,[0],[0]
We further show that this bound is tight.,1.3.1. ALGORITHMS,[0],[0]
"In many practical scenarios, d is much larger than n and the above error is prohibitive (Konečnỳ et al., 2016).
",1.3.1. ALGORITHMS,[0],[0]
A natural way to decease the error is to increase the number of levels of quantization.,1.3.1. ALGORITHMS,[0],[0]
"If we use k levels of quantization, in Theorem 2, we show that the error deceases as
E(⇡sk, Xn) =",1.3.1. ALGORITHMS,[0],[0]
"O
d
n(k 1)2 · 1 n
nX
i=1
||Xi||2 2
! .",1.3.1. ALGORITHMS,[0],[0]
"(2)
However, the communication cost would increase to C(⇡sk, Xn) = n ·",1.3.1. ALGORITHMS,[0],[0]
"(ddlog
2 ke + ˜O(1)) bits, which can be expensive, if we would like the MSE to be o(d/n).
",1.3.1. ALGORITHMS,[0],[0]
"In order to reduce the communication cost, we propose two approaches.
",1.3.1. ALGORITHMS,[0],[0]
Stochastic rotated quantization: We show that preprocessing the data by a random rotation reduces the mean squared error.,1.3.1. ALGORITHMS,[0],[0]
"Specifically, in Theorem 3, we show that this new scheme (denoted by ⇡srk) achieves an MSE of
E(⇡srk, Xn) =",1.3.1. ALGORITHMS,[0],[0]
"O
log d
n(k",1.3.1. ALGORITHMS,[0],[0]
"1)2 · 1 n
nX
i=1
||Xi||2 2
! , 3
and has a communication cost of C(⇡srk, Xn) = n ·",1.3.1. ALGORITHMS,[0],[0]
"(ddlog
2 ke + ˜O(1)).",1.3.1. ALGORITHMS,[0],[0]
"Note that the new scheme achieves much smaller MSE than naive stochastic quantization for the same communication cost.
",1.3.1. ALGORITHMS,[0],[0]
Variable length coding: Our second approach uses the same quantization as ⇡sk but encodes levels via variable length coding.,1.3.1. ALGORITHMS,[0],[0]
"Instead of using dlog
2 ke bits per dimension, we show that using variable length encoding such as arithmetic coding to compress the data reduces the communication cost significantly.",1.3.1. ALGORITHMS,[0],[0]
"In particular, in Theorem 4 we show that there is a scheme (denoted by ⇡svk) such that
C(⇡svk, Xn)",1.3.1. ALGORITHMS,[0],[0]
= O(nd(1 + log(k2/d+ 1)),1.3.1. ALGORITHMS,[0],[0]
"+ ˜O(n)), (3) and E(⇡svk, Xn) = E(⇡sk, Xn).",1.3.1. ALGORITHMS,[0],[0]
"Hence, setting k = p d in Eqs.",1.3.1. ALGORITHMS,[0],[0]
"2 and 3 yields
E(⇡svk, Xn) =",1.3.1. ALGORITHMS,[0],[0]
"O 1 n · 1 n
nX
i=1
||Xi||2 2
! ,
and with ⇥(nd) bits of communication i.e., constant number of bits per dimension per client.",1.3.1. ALGORITHMS,[0],[0]
"Of the three protocols, ⇡svk has the best MSE for a given communication cost.",1.3.1. ALGORITHMS,[0],[0]
Note that ⇡svk uses k quantization levels but still uses O(1) bits per dimension per client for all k  pd.,1.3.1. ALGORITHMS,[0],[0]
"Theoretically, while variable length coding has better guarantees, stochastic rotated quantization has several practical
2We use ˜O(1) to denote O(log(dn)).",1.3.1. ALGORITHMS,[0],[0]
"3All logarithms are to base e, unless stated.
advantages: it uses fixed length coding and hence can be combined with encryption schemes for privacy preserving secure aggregation (Bonawitz et al., 2016).",1.3.1. ALGORITHMS,[0],[0]
"It can also provide lower quantization error in some scenarios due to better constants (see Section 7 for details).
",1.3.1. ALGORITHMS,[0],[0]
"Concurrent to this work, Alistarh et al. (2016) showed that stochastic quantization and Elias coding can be used to obtain communication-optimal SGD.",1.3.1. ALGORITHMS,[0],[0]
"Recently, Konečnỳ & Richtárik (2016) showed that ⇡sb can be improved further by optimizing the choice of stochastic quantization boundaries.",1.3.1. ALGORITHMS,[0],[0]
"However, their results depend on the number of bits necessary to represent a float, whereas ours do not.",1.3.1. ALGORITHMS,[0],[0]
"In the above protocols, all of the clients transmit the data.",1.3.2. MINIMAX MSE,[0],[0]
"We augment these protocols with a sampling procedure, where only a random fraction of clients transmit data.",1.3.2. MINIMAX MSE,[0],[0]
"We show that a combination of k-level quantization, variable length coding, and sampling can be used to achieve information theoretically optimal MSE for a given communication cost.",1.3.2. MINIMAX MSE,[0],[0]
"In particular, combining Corollary 1 and Theorem 5 yields our minimax result: Theorem 1.",1.3.2. MINIMAX MSE,[0],[0]
"There exists a universal constant t < 1 such that for communication cost c  ndt and n 1/t,
E(⇧(c), Sd) = ⇥",1.3.2. MINIMAX MSE,[0],[0]
"✓ min ✓ 1, d
c
◆◆ .
",1.3.2. MINIMAX MSE,[0],[0]
"This result shows that the product of communication cost and MSE scales linearly in the number of dimensions.
",1.3.2. MINIMAX MSE,[0],[0]
The rest of the paper is organized as follows.,1.3.2. MINIMAX MSE,[0],[0]
We first analyze the stochastic uniform quantization technique in Section 2.,1.3.2. MINIMAX MSE,[0],[0]
"In Section 3, we propose the stochastic rotated quantization technique, and in Section 4 we analyze arithmetic coding.",1.3.2. MINIMAX MSE,[0],[0]
"In Section 5, we combine the above algorithm with a sampling technique and state the upper bound on the minimax risk, and in Section 6 we state the matching minimax lower bounds.",1.3.2. MINIMAX MSE,[0],[0]
"Finally, in Section 7 we discuss some practical considerations and apply these algorithms on distributed power iteration and Lloyd’s algorithm.",1.3.2. MINIMAX MSE,[0],[0]
"For a vector Xi, let Xmaxi = max1jd Xi(j) and similarly let Xmini = min1jd Xi(j).",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"In the stochastic binary quantization protocol ⇡sb, for each client i, the quantized value for each coordinate j is generated independently with private randomness as
Yi(j) =
( Xmaxi w.p.
Xi(j) Xmini Xmaxi Xmini ,
Xmini otherwise.
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
Observe EYi(j) = Xi(j).,2.1. Warm-up: Stochastic binary quantization,[0],[0]
"The server estimates ¯X by
ˆ ¯X⇡sb = 1
n
nX
i=1
Yi.
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"We first bound the communication cost of the this protocol.
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
Lemma 1.,2.1. Warm-up: Stochastic binary quantization,[0],[0]
"There exists an implementation of stochastic binary quantization that uses d + ˜O(1) bits per client and hence C(⇡sb, Xn)  n · ⇣ d+ ˜O(1) ⌘ .
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
Proof.,2.1. Warm-up: Stochastic binary quantization,[0],[0]
"Instead of sending vectors Yi, clients transmit two real values Xmaxi and Xmini (to a desired error) and a bit vector Y 0i such that Y 0i",2.1. Warm-up: Stochastic binary quantization,[0],[0]
(j) = 1 if Yi = Xmaxi and 0 otherwise.,2.1. Warm-up: Stochastic binary quantization,[0],[0]
"Hence each client transmits d + 2r bits, where r is the number of bits to transmit the real value to a desired error.
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
Let B be the maximum norm of the underlying vectors.,2.1. Warm-up: Stochastic binary quantization,[0],[0]
"To bound r, observe that using r bits, one can represent a number between B and B to an error of B/2r 1.",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"Thus using 3 log
2 (dn) + 1 bits one can represent the minimum and maximum to an additive error of B/(nd)3.",2.1. Warm-up: Stochastic binary quantization,[0],[0]
This error in transmitting minimum and maximum of the vector does not affect our calculations and we ignore it for simplicity.,2.1. Warm-up: Stochastic binary quantization,[0],[0]
"We note that in practice, each dimension of Xi is often stored as a 32 bit or 64 bit float, and r should be set as either 32 or 64.",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"In this case, using an even larger r does not further reduce the error.
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"We now compute the estimation error of this protocol.
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
Lemma 2.,2.1. Warm-up: Stochastic binary quantization,[0],[0]
"For any set of vectors Xn,
E(⇡sb, Xn) = 1 n2
nX
i=1
dX
j=1
(Xmaxi Xi(j))(Xi(j) Xmini ).
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"Proof.
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"E(⇡sb, Xn) = E ˆ¯X ¯X
2
2
=
1 n2 E
nX
i=1
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"(Yi Xi)
2
2
=
1
n2
nX
i=1
E ||Yi Xi||2 2 ,
where the last equality follows by observing that Yi Xi, 8i, are independent zero mean random variables.",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"The proof follows by observing that for every i,
E ||Yi Xi||2 2 =
dX
j=1
E[(Yi(j) Xi(j))2]
=
dX
j=1
(Xmaxi Xi(j))(Xi(j) Xmini ).
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
Lemma 2 implies the following upper bound.,2.1. Warm-up: Stochastic binary quantization,[0],[0]
Lemma 3.,2.1. Warm-up: Stochastic binary quantization,[0],[0]
"For any set of vectors Xn,
E(⇡sb, Xn)  d 2n · 1 n
nX
i=1
||Xi||2 2 .
Proof.",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"The proof follows by Lemma 2 observing that 8j
(Xmaxi Xi(j))(Xi(j) Xmini )  (Xmaxi Xmini )2
4
,
and (Xmaxi Xmini )2  2 ||Xi||2
2
.",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"(4)
We also show that the above bound is tight: Lemma 4.",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"There exists a set of vectors Xn such that
E(⇡sb, Xn) d 2 2n · 1 n
nX
i=1
||Xi||2 2 .
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
Proof.,2.1. Warm-up: Stochastic binary quantization,[0],[0]
"For every i, let Xi be defined as follows.",2.1. Warm-up: Stochastic binary quantization,[0],[0]
Xi(1),2.1. Warm-up: Stochastic binary quantization,[0],[0]
"= 1/ p 2, Xi(2)",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"= 1/ p 2, and for all j > 2, Xi(j) = 0.",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"For every i, Xmaxi = 1p 2 and Xmini = 1p 2
.",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"Substituting these bounds in the conclusion of Lemma 2 (which is an equality) yields the theorem.
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"Therefore, the simple algorithm proposed in this section gives MSE ⇥(d/n).",2.1. Warm-up: Stochastic binary quantization,[0],[0]
Such an error is too large for realworld use.,2.1. Warm-up: Stochastic binary quantization,[0],[0]
"For example, in the application of neural networks (Konečnỳ et al., 2016), d can be on the order of millions, yet n can be much smaller than that.",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"In such cases, the MSE is even larger than the norm of the vector.
2.2.",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"Stochastic k-level quantization
A natural generalization of binary quantization is k-level quantization.",2.1. Warm-up: Stochastic binary quantization,[0],[0]
Let k be a positive integer larger than 2.,2.1. Warm-up: Stochastic binary quantization,[0],[0]
We propose a k-level stochastic quantization scheme ⇡sk to quantize each coordinate.,2.1. Warm-up: Stochastic binary quantization,[0],[0]
"Recall that for a vector Xi, Xmaxi = max1jd Xi(j) and Xmini = min1jd Xi(j).",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"For every integer r in the range [0, k), let
Bi(r) def = Xmini + rsi",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"k 1 ,
where si satisfies Xmini + si Xmaxi .",2.1. Warm-up: Stochastic binary quantization,[0],[0]
A natural choice for si would be Xmaxi Xmini .4,2.1. Warm-up: Stochastic binary quantization,[0],[0]
The algorithm quantizes each coordinate into one of Bi(r)s stochastically.,2.1. Warm-up: Stochastic binary quantization,[0],[0]
"In ⇡sk, for the i-th client and j-th coordinate, if Xi(j) 2 [Bi(r), Bi(r + 1)),
Yi(j) =
( Bi(r + 1) w.p.
Xi(j) Bi(r) Bi(r+1) Bi(r)
Bi(r) otherwise.
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"4We will show in Section 4, however, a higher value of si and variable length coding has better guarantees.
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"The server estimates ¯X by
ˆ ¯X⇡sk = 1
n
nX
i=1
Yi.
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"As before, the communication complexity of this protocol is bounded.",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"The proof is similar to that of Lemma 1 and hence omitted.
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
Lemma 5.,2.1. Warm-up: Stochastic binary quantization,[0],[0]
"There exists an implementation of stochastic klevel quantization that uses ddlog(k)e+ ˜O(1) bits per client and hence C(⇡sk, Xn)  n ·",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"⇣ ddlog
2
ke+ ˜O(1) ⌘ .
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
The mean squared loss can be bounded as follows.,2.1. Warm-up: Stochastic binary quantization,[0],[0]
Theorem 2.,2.1. Warm-up: Stochastic binary quantization,[0],[0]
"If Xmaxi Xmini  si  p 2 ||Xi||
2 8i, then for any Xn, the ⇡sk protocol satisfies,
E(⇡sk, Xn)  ",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"d 2n(k 1)2 · 1 n
nX
i=1
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"||Xi||2 2 .
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"Proof.
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"E(⇡sk, Xn) = E ˆ¯X ¯X
2
2
=
1 n2 E
nX
i=1
(Yi Xi)
2
2
=
1
n2
nX
i=1
E ||Yi Xi||2 2  1 n2
nX
i=1
d s2i
4(k 1)2 , (5)
where the last equality follows by observing Yi(j) Xi(j) is an independent zero mean random variable with E(Yi(j) Xi(j))2  s 2 i",2.1. Warm-up: Stochastic binary quantization,[0],[0]
4(k 1)2 .,2.1. Warm-up: Stochastic binary quantization,[0],[0]
"si  p 2 ||Xi|| 2
completes the proof.
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
We conclude this section by noting that si = Xmaxi Xmini satisfies the conditions for the above theorem by Eq. (4).,2.1. Warm-up: Stochastic binary quantization,[0],[0]
We show that the algorithm of the previous section can be significantly improved by a new protocol.,3. Stochastic rotated quantization,[0],[0]
"The motivation comes from the fact that the MSE of stochastic binary quantization and stochastic k-level quantization is O( dn (X max
i Xmini )2) (the proof of Lemma 3 and Theorem 2 with si = Xmaxi Xmini ).",3. Stochastic rotated quantization,[0],[0]
Therefore the MSE is smaller when Xmaxi and Xmaxi are close.,3. Stochastic rotated quantization,[0],[0]
"For example, when Xi is generated uniformly on the unit sphere, with
high probability, Xmaxi Xmini is O ✓q log d d ◆ (Dasgupta
& Gupta, 2003).",3. Stochastic rotated quantization,[0],[0]
"In such case, E(⇡sk, Xn) is O( log dn ) instead of O( dn ).",3. Stochastic rotated quantization,[0],[0]
"In this section, we show that even without any assumptions on the distribution of the data, we can “reduce” Xmaxi Xmini with a structured random rotation, yielding
an O( log dn ) error.",3. Stochastic rotated quantization,[0],[0]
"We call the method stochastic rotated quantization and denote it by ⇡srk.
",3. Stochastic rotated quantization,[0],[0]
"Using public randomness, all clients and the central server generate a random rotation matrix (random orthogonal matrix) R 2 Rd⇥d according to some known distribution.",3. Stochastic rotated quantization,[0],[0]
Let Zi = RXi and ¯Z = R ¯X .,3. Stochastic rotated quantization,[0],[0]
"In the stochastic rotated quantization protocol ⇡srk(R), clients quantize the vectors Zi instead of Xi and transmit them similar to ⇡srk.",3. Stochastic rotated quantization,[0],[0]
"The server estimates ¯X by
ˆ ¯X⇡srk = R 1 ˆ
¯Z, ˆ¯Z = 1
n
nX
i=1
Yi.
",3. Stochastic rotated quantization,[0],[0]
The communication cost is same as ⇡sk and is given by Lemma 5.,3. Stochastic rotated quantization,[0],[0]
"We now bound the MSE.
Lemma 6.",3. Stochastic rotated quantization,[0],[0]
"For any Xn, E(⇡srk(R), Xn) is at most
d 2n2(k 1)2 nX
i=1
ER h (Zmaxi ) 2 + Zmini 2 i ,
where Zi = RXi and for every i, let si = Zmaxi Zmini .
",3. Stochastic rotated quantization,[0],[0]
"Proof.
",3. Stochastic rotated quantization,[0],[0]
"E(⇡srk, Xn) = E⇡ ˆ¯X ¯X
2
= E⇡ R 1",3. Stochastic rotated quantization,[0],[0]
"ˆ¯Z R 1 ¯Z
2 (a) = E⇡ ˆ¯Z ¯Z 2
(b) = ERE⇡  ˆ¯Z ¯Z 2 |Zn 1
 d 4n2(k",3. Stochastic rotated quantization,[0],[0]
"1)2
nX
i=1
ER[(Zmaxi Zmini )2],
where the last inequality follows Eq. (5) and the value of si.",3. Stochastic rotated quantization,[0],[0]
"(a) follows from the fact that rotation does not change the norm of the vector, and (b) follows from the tower law of expectation.",3. Stochastic rotated quantization,[0],[0]
"The lemma follows from observing that
(Zmaxi Zmini )2  2(Zmaxi )2 + 2(Zmini )2.
To obtain strong bounds, we need to find an orthogonal matrix R that achieves low (Zmaxi )2 and (Zmini )2.",3. Stochastic rotated quantization,[0],[0]
"In addition, due to the fact that d can be huge in practice, we need a type of orthogonal matrix that permits fast matrix-vector products.",3. Stochastic rotated quantization,[0],[0]
Naive orthogonal matrices that support fast multiplication such as block-diagonal matrices often result in high values of (Zmaxi )2 and (Zmini )2.,3. Stochastic rotated quantization,[0],[0]
"Motivated by recent works of structured matrices (Ailon & Chazelle, 2006; Yu et al., 2016), we propose to use a special type of orthogonal matrix R = HD, where D is a random diagonal matrix with i.i.d.",3. Stochastic rotated quantization,[0],[0]
Rademacher entries (±1 with probability 0.5).,3. Stochastic rotated quantization,[0],[0]
"H is a Walsh-Hadamard matrix (Horadam, 2012).",3. Stochastic rotated quantization,[0],[0]
"The WalshHadamard matrix of dimension 2m for m 2 N is given by
the recursive formula,
H(21) =
 1 1
1 1 , H(2m) =  H(2m 1) H(2m 1) H(2m 1) H(2m 1) .
",3. Stochastic rotated quantization,[0],[0]
Both applying the rotation and inverse rotation take O(d log d) time and O(1) additional space (with an inplace algorithm).,3. Stochastic rotated quantization,[0],[0]
"The next lemma bounds E (Zmaxi )
2 and E Zmini 2 for this choice of R. The lemma is similar to that of Ailon & Chazelle (2006), and we give the proof in Appendix A for completeness.",3. Stochastic rotated quantization,[0],[0]
Lemma 7.,3. Stochastic rotated quantization,[0],[0]
"Let R = HD, where D is a diagonal matrix with independent Radamacher random variables.",3. Stochastic rotated quantization,[0],[0]
"For every i and every sequence Xn,
E ⇥",3. Stochastic rotated quantization,[0],[0]
(Zmini ) 2 ⇤ = E ⇥,3. Stochastic rotated quantization,[0],[0]
"(Zmaxi ) 2 ⇤  ||Xi|| 2 2 (2 log d+ 2)
d .
",3. Stochastic rotated quantization,[0],[0]
Combining the above two lemmas yields the main result.,3. Stochastic rotated quantization,[0],[0]
Theorem 3.,3. Stochastic rotated quantization,[0],[0]
"For any Xn, ⇡srk(HD) protocol satisfies,
E(⇡srk(HD), Xn)  2 log d+ 2 n(k 1)2 · 1 n
nX
i=1
||Xi||2 2 .",3. Stochastic rotated quantization,[0],[0]
"Instead of preprocessing the data via a rotation matrix as in ⇡srk, in this section we propose to use a variable length coding strategy to minimize the number of bits.
Consider the stochastic k-level quantization technique.",4. Variable length coding,[0],[0]
"A natural way of transmitting Yi is sending the bin number for each coordinate, thus the total number of bits the algorithm sends per transmitted coordinate would be ddlog
2 ke.",4. Variable length coding,[0],[0]
This naive implementation is sub-optimal.,4. Variable length coding,[0],[0]
"Instead, we propose to further encode the transmitted values using universal compression schemes (Krichevsky & Trofimov, 1981; Falahatgar et al., 2015).",4. Variable length coding,[0],[0]
"We first encode hr, the number of times each quantized value r has appeared, and then use arithmetic or Huffman coding corresponding to the distribution pr = hrd .",4. Variable length coding,[0],[0]
We denote this scheme by ⇡svk.,4. Variable length coding,[0],[0]
"Since we quantize vectors the same way in ⇡sk and ⇡svk, the MSE of ⇡svk is also given by Theorem 2.",4. Variable length coding,[0],[0]
We now bound the communication cost.,4. Variable length coding,[0],[0]
Theorem 4.,4. Variable length coding,[0],[0]
Let si = p 2 ||Xi||.,4. Variable length coding,[0],[0]
"There exists an implementation of ⇡svk such that C(⇡svk, Xn) is at most
n ✓ d ✓ 2 + log2 ✓ (k 1)2
2d +
5
4
◆◆ + k log2
(d+ k)e",4. Variable length coding,[0],[0]
"k +
˜O(1) ◆ .
",4. Variable length coding,[0],[0]
Proof.,4. Variable length coding,[0],[0]
"As in Lemma 1, ˜O(1) bits are used to transmit the si’s and Xmini .",4. Variable length coding,[0],[0]
"Recall that hr is the number of coordinates that are quantized into bin r, and r takes k possible values.",4. Variable length coding,[0],[0]
"Furthermore, P r hr = d.",4. Variable length coding,[0],[0]
"Thus the number of bits
necessary to represent the hr’s is ⇠ log
2 ✓ d+",4. Variable length coding,[0],[0]
"k 1 k 1 ◆⇡  k log 2 (d+ k)e k .
",4. Variable length coding,[0],[0]
"Once we have compressed the hr’s, we use arithmetic coding corresponding to the distribution pr = hr/d to compress and transmit bin values for each coordinate.",4. Variable length coding,[0],[0]
"The total number of bits arithmetic coding uses is (MacKay, 2003)
d k 1X
r=0
hr d log 2 d hr + 2.
",4. Variable length coding,[0],[0]
"Let pr = hr/d, a = (k 1)Xmini , b = si, and =Pk 1 r=0 1/((a+ br) 2 + ).",4. Variable length coding,[0],[0]
"Note that
X
r
pr log 2
1
pr =
X
r
pr log 2
1/(((a+ br)2 + ) )
",4. Variable length coding,[0],[0]
"pr
+
X
r
pr log 2 (((a+ br)2 + ) )
 ",4. Variable length coding,[0],[0]
"X
r
pr log 2 (((a+ br)2 + ) )
 log 2 (
X
r
pr(a+ br) 2
+ ) + log 2
,
where the first inequality follows from the positivity of KLdivergence.",4. Variable length coding,[0],[0]
"Choosing = s2i , yields  4/s2i and hence, X
r
pr log 2
1 pr  log 2 (
X
r
pr(a+br) 2 +s2i )+",4. Variable length coding,[0],[0]
"log2(4/s 2 i ).
",4. Variable length coding,[0],[0]
"Note that if Yi(j) belongs to bin r, (a + br)2 = (k 1)
2Y 2i (j).",4. Variable length coding,[0],[0]
Recall that hr is the number of coordinates quantized into bin r.,4. Variable length coding,[0],[0]
"Hence P r hr(a + br)
2 is the scaled norm-square of Yi, i.e.,
X
r
hr(a+ br) 2
=",4. Variable length coding,[0],[0]
"(k 1)2 dX
j=1
Y 2i (j)
=
dX
j=1
((Xi(j) + ↵(j))(k 1))2 ,
where the ↵(j) = Yi(j) Xi(j).",4. Variable length coding,[0],[0]
"Taking expectations on both sides and using the fact that the ↵(j) are independent zero mean random variables over a range of si/(k 1), we get
E X
r
hr(a+ br) 2 =
dX
j=1
E(Xi(j)2 + ↵(j)2)(k 1)2
 ||Xi||2 2
✓ (k 1)2 + d
2
◆ .
",4. Variable length coding,[0],[0]
"Using Jensen’s inequality yields the result.
",4. Variable length coding,[0],[0]
"Thus if k = p d + 1, the communication complexity is O(nd) and the MSE is O(1/n).",4. Variable length coding,[0],[0]
"In the above protocols, all the clients transmit and hence the communication cost scales linearly with n. Instead, we show that any of the above protocols can be combined by client sampling to obtain trade-offs between the MSE and the communication cost.",5. Communication MSE trade-off,[0],[0]
"Note that similar analysis also holds for sampling the coordinates.
",5. Communication MSE trade-off,[0],[0]
"Let ⇡ be a protocol where the mean estimate is of the form:
ˆ ¯X = R 1 1
n
nX
i=1
Yi.",5. Communication MSE trade-off,[0],[0]
"(6)
All three protocols we have discussed are of this form.",5. Communication MSE trade-off,[0],[0]
Let ⇡p be the protocol where each client participates independently with probability p.,5. Communication MSE trade-off,[0],[0]
"The server estimates ¯X by
ˆ ¯X⇡p = R 1 · 1
np
X i2S Yi,
where Yis are defined in the previous section and S is the set of clients that transmitted.",5. Communication MSE trade-off,[0],[0]
Lemma 8.,5. Communication MSE trade-off,[0],[0]
"For any set of vectors Xn and protocol ⇡ of the form Equation (6), its sampled version ⇡p satisfies
E(⇡p, Xn) = 1 p · E(⇡, Xn) + 1 p np
nX
i=1
||Xi||2 2 .
and C(⇡p, Xn) = p · C(⇡, Xn).
",5. Communication MSE trade-off,[0],[0]
Proof.,5. Communication MSE trade-off,[0],[0]
"The proof of communication cost follows from Lemma 5 and the fact that in expectation, np clients transmit.",5. Communication MSE trade-off,[0],[0]
We now bound the MSE.,5. Communication MSE trade-off,[0],[0]
Let S be the set of clients that transmit.,5. Communication MSE trade-off,[0],[0]
"The error E(⇡p, Xn) is
E  ˆ¯X ¯X 2
2
= E
2
4 1
np
X i2S R 1Yi ¯X 2
2
3
5
=E
2
4 1
np
X i2S Xi ¯X 2
2
+
1
n2p2
X i2S (R 1Yi Xi) 2
2
3
5 ,
where the last equality follows by observing that R 1Yi Xi are independent zero mean random variables and hence for any i, E[(R",5. Communication MSE trade-off,[0],[0]
1Yi Xi)T ( P i2S Xi ¯X)] = 0.,5. Communication MSE trade-off,[0],[0]
"The first term can be bounded as
E 1
np
X i2S Xi ¯X 2
2
=
1
n2
nX
i=1
E 1
p Xi i2S Xi
2
2
=
1
n2
nX
i=1
✓ p (1 p)2
p2 ||Xi||2 2 + (1 p) ||Xi||2 2
◆
= 1 p np · 1 n
nX
i=1
||Xi||2 2 .
",5. Communication MSE trade-off,[0],[0]
"Furthermore, the second term can be bounded as
E
2
4 1 n2p2
X i2S (R 1Yi Xi) 2
2
3
5
(a) = 1
n2p2
X i2S E h (R 1Yi Xi) 2 2 i
=
1
n2p2
nX
i=1
E h
(R 1Yi Xi) 2 2 i2S
i
=
1
n2p
nX
i=1
E h",5. Communication MSE trade-off,[0],[0]
"R 1Yi Xi
2 2
i
=
1 n2p E
2
4
nX
i=1
(R 1Yi Xi)
2
2
3
5 = 1 p E(⇡, Xn)
where the last equality follows from the assumption that ⇡’s mean estimate is of the form (6).",5. Communication MSE trade-off,[0],[0]
"(a) follows from the fact that R 1Yi Xi are independent zero mean random variables.
",5. Communication MSE trade-off,[0],[0]
"Combining the above lemma with Theorem 4, and choosing k = p d+ 1 results in the following.",5. Communication MSE trade-off,[0],[0]
Corollary 1.,5. Communication MSE trade-off,[0],[0]
"For every c  nd(2+log 2
(7/4)), there exists a protocol ⇡",5. Communication MSE trade-off,[0],[0]
"such that C(⇡, Sd)  c and
E(⇡, Sd) =",5. Communication MSE trade-off,[0],[0]
"O ✓ min ✓ 1, d
c
◆◆ .",5. Communication MSE trade-off,[0],[0]
The lower bound relies on the lower bounds on distributed statistical estimation due to Zhang et al. (2013).,6. Lower bounds,[0],[0]
"Lemma 9 ((Zhang et al., 2013) Proposition 2).",6. Lower bounds,[0],[0]
"There exists a set of distributions Pd supported on h 1p
d ,",6. Lower bounds,[0],[0]
"1p d
id such
that if any centralized server wishes to estimate the mean of the underlying unknown distribution, then for any independent protocol ⇡
max pd2Pd E  ✓(pd) ˆ✓⇡ 2 2
tmin ✓ 1, d C(⇡) ◆ ,
where C(⇡) is the communication cost of the protocol, ✓(pd) is the mean of pd, and t is a positive constant.",6. Lower bounds,[0],[0]
Theorem 5.,6. Lower bounds,[0],[0]
Let t be the constant in Lemma 9.,6. Lower bounds,[0],[0]
"For every c  ndt/4 and n 4/t,
E(⇧(c), Sd) t 4 min
✓ 1, d
c
◆ .
",6. Lower bounds,[0],[0]
Proof.,6. Lower bounds,[0],[0]
"Given n samples from the underlying distribution where each sample belongs to Sd, it is easy to see that
E ✓(pd) ˆ✓(pd)
2
2
 1 n ,
0 1 2 3 4 5 6 −25
−20
−15
−10
−5
Bits per dimension
lo g
(M S
E )
uniform rotation variable
Figure 1.",6. Lower bounds,[0],[0]
"Distributed mean estimation on data generated from a Gaussian distribution.
",6. Lower bounds,[0],[0]
where ˆ✓(pd) is the empirical mean of the observed samples.,6. Lower bounds,[0],[0]
Let Pd be the set of distributions in Lemma 9.,6. Lower bounds,[0],[0]
"Hence for any protocol ⇡ there exists a distribution pd such that
E ˆ✓(pd) ˆ✓⇡
2
2
(a) 1
2
E ✓(pd) ˆ✓⇡
2
2
E ✓(pd) ˆ✓(pd)
2
2
(b) t
2
min ✓ 1, d C(⇡) ◆ 1 n (c) t 4 min ✓ 1, d C(⇡) ◆ ,
(a) follows from the fact that 2(a b)2 + 2(b c)2 (a c)2.",6. Lower bounds,[0],[0]
"(b) follows from Lemma 9 and (c) follows from the fact that C(⇡, Sd)  ndt/4 and n 4/t. Corollary 1 and Theorem 5 yield Theorem 1.",6. Lower bounds,[0],[0]
We note that the above lower bound holds only for communication cost c < O(nd).,6. Lower bounds,[0],[0]
"Extending the results for larger values of c remains an open problem.
",6. Lower bounds,[0],[0]
"At a first glance it may appear that combining structured random matrix and variable length encoding may improve the result asymptotically, and therefore violates the lower bound.",6. Lower bounds,[0],[0]
"However, this is not true.
",6. Lower bounds,[0],[0]
Observe that variable length coding ⇡svk and stochastic rotated quantization ⇡srk use different aspects of the data: the variable length coding uses the fact that bins with large values of index r are less frequent.,6. Lower bounds,[0],[0]
"Hence, we can use fewer bits to encode frequent bins and thus improve communication.",6. Lower bounds,[0],[0]
In this scheme bin-width (si/(k 1)) is p 2||Xi||2/(k 1).,6. Lower bounds,[0],[0]
Rotated quantization uses the fact that rotation makes the min and max closer to each other and hence we can make bins with smaller width.,6. Lower bounds,[0],[0]
"In such a case, all the bins become more or less equally likely and hence variable length coding does not help.",6. Lower bounds,[0],[0]
"In this scheme bin-width (si/(k 1)) is (Zmaxi Zmini )/(k 1) ⇡ ||Xi||2(log d)/(kd), which is much smaller than bin-width for variable length coding.",6. Lower bounds,[0],[0]
Hence variable length coding and random rotation cannot be used simultaneously.,6. Lower bounds,[0],[0]
"Based on the theoretical analysis, the variable-length coding method provides the lowest quantization error asymptotically when using a constant number of bits.",7. Practical considerations and applications,[0],[0]
"However in practice, stochastic rotated quantization may be preferred due to (hidden) constant factors and the fact that it uses a fixed amount of bits per dimension.",7. Practical considerations and applications,[0],[0]
"For example, considering quantizing the vector [ 1, 1, 0, 0], stochastic rotated
quantization can use 1 bit per dimension and gives zero error, whereas the other two protocols do not.",7. Practical considerations and applications,[0],[0]
"To see this, observe that the naive quantization will quantize 0 to either 1 or 1 and variable length coding cannot achieve 0 error with 1 bit per dimension due to its constant factors.
",7. Practical considerations and applications,[0],[0]
"We further note that the rotated quantization is preferred when applied on “unbalanced” data, due to the fact that the rotation can correct the unbalancedness.",7. Practical considerations and applications,[0],[0]
We demonstrate this by generating a dataset where the value of the last feature dimension entry is much larger than others.,7. Practical considerations and applications,[0],[0]
We generate 1000 datapoints each with 256 dimensions.,7. Practical considerations and applications,[0],[0]
The first 255 dimensions are generated i.i.d.,7. Practical considerations and applications,[0],[0]
"from N(0, 1),",7. Practical considerations and applications,[0],[0]
"and the last dimension is generated from N(100, 1).",7. Practical considerations and applications,[0],[0]
"As shown in Figure 1, the rotated stochastic quantization has the best performance.",7. Practical considerations and applications,[0],[0]
"The improvement is especially significant for low bit rate cases.
",7. Practical considerations and applications,[0],[0]
We demonstrate two applications in the rest of this section.,7. Practical considerations and applications,[0],[0]
"The experiments are performed on the MNIST (d = 1024) and CIFAR (d = 512) datasets.
",7. Practical considerations and applications,[0],[0]
Distributed Lloyd’s algorithm.,7. Practical considerations and applications,[0],[0]
"In the distributed Lloyd’s (k-means) algorithm, each client has access to a subset of data points.",7. Practical considerations and applications,[0],[0]
"In each iteration, the server broadcasts the cluster centers to all the clients.",7. Practical considerations and applications,[0],[0]
"Each client updates the centers based on its local data, and sends the centers back to the server.",7. Practical considerations and applications,[0],[0]
The server then updates the centers by computing the weighted average of the centers sent from all clients.,7. Practical considerations and applications,[0],[0]
"In
the quantized setting, the client compresses the new centers before sending to the server.",7. Practical considerations and applications,[0],[0]
"This saves the uplink communication cost, which is often the bottleneck of distributed learning5.",7. Practical considerations and applications,[0],[0]
We set both the number of centers and number of clients to 10.,7. Practical considerations and applications,[0],[0]
"Figure 2 shows the result.
",7. Practical considerations and applications,[0],[0]
Distributed power iteration.,7. Practical considerations and applications,[0],[0]
Power iteration is a widely used method to compute the top eigenvector of a matrix.,7. Practical considerations and applications,[0],[0]
"In the distributed setting, each client has access to a subset of data.",7. Practical considerations and applications,[0],[0]
"In each iteration, the server broadcasts the current estimate of the eigenvector to all clients.",7. Practical considerations and applications,[0],[0]
"Each client then updates the eigenvector based on one power iteration on its local data, and sends the updated eigenvector back to the server.",7. Practical considerations and applications,[0],[0]
The server updates the eigenvector by computing the weighted average of the eigenvectors sent by all clients.,7. Practical considerations and applications,[0],[0]
"Similar to the above distributed Lloyd’s algorithm, in the quantized setting, the client compresses the estimated eigenvector before sending to the server.",7. Practical considerations and applications,[0],[0]
Figure 3 shows the result.,7. Practical considerations and applications,[0],[0]
"The dataset is distributed over 100 clients.
",7. Practical considerations and applications,[0],[0]
"For both of these applications, variable-length coding achieves the lowest quantization error in most of the settings.",7. Practical considerations and applications,[0],[0]
"Furthermore, for low-bit rate, stochastic rotated quantization is competitive with variable-length coding.
",7. Practical considerations and applications,[0],[0]
"5In this setting, the downlink is a broadcast, and therefore its cost can be reduced by a factor of O(n/ log n) without quantization, where n is the number of clients.",7. Practical considerations and applications,[0],[0]
"We thank Jayadev Acharya, Keith Bonawitz, Dan Holtmann-Rice, Jakub Konecny, Tengyu Ma, and Xiang Wu for helpful comments and discussions.",Acknowledgments,[0],[0]
"Motivated by the need for distributed learning and optimization algorithms with low communication cost, we study communication efficient algorithms for distributed mean estimation.",abstractText,[0],[0]
"Unlike previous works, we make no probabilistic assumptions on the data.",abstractText,[0],[0]
"We first show that for d dimensional data with n clients, a naive stochastic rounding approach yields a mean squared error (MSE) of ⇥(d/n) and uses a constant number of bits per dimension per client.",abstractText,[0],[0]
We then extend this naive algorithm in two ways: we show that applying a structured random rotation before quantization reduces the error to O((log d)/n) and a better coding strategy further reduces the error to O(1/n).,abstractText,[0],[0]
"We also show that the latter coding strategy is optimal up to a constant in the minimax sense i.e., it achieves the best MSE for a given communication cost.",abstractText,[0],[0]
We finally demonstrate the practicality of our algorithms by applying them to distributed Lloyd’s algorithm for kmeans and power iteration for PCA.,abstractText,[0],[0]
Distributed Mean Estimation with Limited Communication,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 12–21, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.
We show that a standard supervised regression model is in fact sufficient to retrieve such attributes to a reasonable degree of accuracy: When evaluated on the prediction of both categorical and numeric attributes of countries and cities, the model consistently reduces baseline error by 30%, and is not far from the upper bound. Further analysis suggests that our model is able to “objectify” distributional representations for entities, anchoring them more firmly in the external world in measurable ways.",text,[0],[0]
"Distributional models induce vector-based semantic representations of words from their contextual distributions in corpora, exploiting the observation that words with related meanings tend to occur in similar linguistic contexts (Turney and Pantel, 2010; Erk, 2012).",1 Introduction,[0],[0]
"Since the approach only requires raw text as input, it can be used to harvest word representations on a very large scale.",1 Introduction,[0],[0]
"By encoding the rich knowledge that is present in text, these representations are able to capture many aspects of word meaning.",1 Introduction,[0],[0]
"Moreover, approximating semantic similarity by graded geometric distance in a vector space is an effective strategy to address the
many linguistic phenomena that are better characterized in gradient rather than discrete terms, such as synonymy, selectional preferences, and semantic priming (Baroni and Lenci, 2010; Erk et al., 2010; Padó and Lapata, 2007, among others).
",1 Introduction,[0],[0]
"However, not all aspects of human semantic knowledge are satisfactorily captured in terms of fuzzy relations and graded similarity.",1 Introduction,[0],[0]
"In particular, our knowledge of the meaning of words denoting specific entities involves a number of “hard facts” about the referents they denote that are best formalized as attribute-value pairs, of the sort that are stored in manually-curated knowledge bases, such as FreeBase or Wikidata.1 While distributional vectors can capture the useful fact that, say, Italy is in many ways more similar to Spain than to Germany, as humans we also know (or we can easily look up) a set of objective facts about Italy, such as what is its capital, its area, its official language and GDP, that are difficult to express in the language of vector algebra and geometry.
",1 Introduction,[0],[0]
"In this paper, we explore the hypothesis that distributional vectors implicitly encode such attributes of referential entities, which we will call referential attributes.",1 Introduction,[0],[0]
We show that a simple supervised algorithm applied to vectors can retrieve them so that they can be expressed in the explicit language of structured knowledge bases.,1 Introduction,[0],[0]
"Concretely, we train a logistic regression model to predict the values of both numeric and categorical FreeBase attributes of countries and cities from their distributional vectors.",1 Introduction,[0],[0]
"This model makes predictions that are significantly better than an informed baseline, in-between the latter and an upper-bound method.",1 Introduction,[0],[0]
"Qualitative analysis of the results points both to the inherent difficulty of correctly retrieving certain classes of attributes, and to some intriguing properties of the conceptual nature of the knowledge encoded in distributional data, that bias their predictions about certain objective attributes of geographic entities.
",1 Introduction,[0],[0]
"1www.freebase.com, www.wikidata.org.
12
",1 Introduction,[0],[0]
"We see our experiment as a first step towards integrating conceptual and referential aspects of meaning in distributional semantics, as we further discuss in the conclusion.",1 Introduction,[0],[0]
Mikolov et al.’s (2013) skip-gram model is a state-of-the-art “predictive” distributional semantic model which represents each word in a space of latent dimensions optimized to predict the contexts of the word’s occurrences.,2.1 Distributional Representations,[0],[0]
"For our study, we adopt the pre-trained 1,000-dimensional skipgram model for Named Entities that is available at https://code.google.com/p/word2vec/ and was produced from a 100-billion token news corpus.",2.1 Distributional Representations,[0],[0]
We refer to this model as WORD2VEC.,2.1 Distributional Representations,[0],[0]
"As our source of referential attributes, we use FreeBase (see footnote 1), a knowledge base of structured information on a wide range of entities of different semantic types (people, geographical entities, etc.).",2.2 Referential Representations,[0],[0]
"The information in FreeBase comes from various sources, including Wikipedia and domainspecific databases, plus user content generation and correction.",2.2 Referential Representations,[0],[0]
"FreeBase currently records at least 2 attributes for over 47 million entities, and it has been used fairly extensively in NLP before (Mintz et al., 2009; Socher et al., 2013a, among others).
",2.2 Referential Representations,[0],[0]
"For each entity, FreeBase contains a list of attribute-value tuples (where values can in turn be entities, allowing a graph view of the data that we do not exploit here).",2.2 Referential Representations,[0],[0]
Table 1 shows a sample of the attributes that FreeBase records for countries.,2.2 Referential Representations,[0],[0]
"Note that some attributes are simple (e.g., date founded), while other can be called complex, in the sense that they are attributes of attributes (e.g., geolocation::latitude).",2.2 Referential Representations,[0],[0]
We use a double-colon notation to refer to complex attributes.,2.2 Referential Representations,[0],[0]
The values of all attributes can be either numeric or categorical.,2.2 Referential Representations,[0],[0]
"The numeric attributes in particular are often strongly correlated, both within attributes types across years (e.g., fertility rate in different years) and across attributes within years (e.g., absolute GDP and GDP per capita in a given year).
",2.2 Referential Representations,[0],[0]
"We built two datasets for our experiments, one for countries and one for cities, with data automatically extracted from FreeBase.2 We consider two
2Both datasets are publicly available at http:
datasets in order to check that the mapping we seek can be established not just for one, possible handpicked, type of entities; we leave it to future work to study very different kinds of entities, such as people or institutions.
",2.2 Referential Representations,[0],[0]
The Countries dataset consists of the 260 countries for which we have a distributional vector.,2.2 Referential Representations,[0],[0]
"Some countries do not exist anymore, like Yugoslavia, but, since this does not impact our method, we keep them in the dataset.",2.2 Referential Representations,[0],[0]
"The dataset records all simple attributes as well as complex attributes of at most two hops in the FreeBase graph, without manual inspection.",2.2 Referential Representations,[0],[0]
We linearly rescale all numeric attributes to [0..1] and translate all categorical attributes into a binary representation by suffixing the original value to the original attribute name.,2.2 Referential Representations,[0],[0]
"For example, the attribute member-of::organization with the value world bank results in a binary attribute member-of::organization::world bank having value 1 for all and only those countries that are members of the World Bank, 0 for the others.3 Attributes that occur less than 15 times are discarded, since they are either not consistently recorded or rare.",2.2 Referential Representations,[0],[0]
This results in a total of 707 numeric and 247 binary attributes.,2.2 Referential Representations,[0],[0]
"Finally, we partition the data into training, validation, and test set, using a 60-20-20 percent split.
",2.2 Referential Representations,[0],[0]
"We apply the same process to the Cities dataset, which consists of 1645 cities from the intersection of the distributional and FreeBase city lists.",2.2 Referential Representations,[0],[0]
"In
//www.ims.uni-stuttgart.de/forschung/ ressourcen/korpora/CityCountry.html.
3We considered treating some categorical attributes as multi-valued, but decided against it since the cases in which alternative values are mutually exclusive are rare (e.g., the same country can be containedBy multiple entities, cf. Table 1).
",2.2 Referential Representations,[0],[0]
"this case, we have 211 numeric and 106 binary attributes – the numbers are smaller because countries have a richer representation in FreeBase than cities.",2.2 Referential Representations,[0],[0]
"We do zero-shot learning of full FreeBase attributebased country/city representations, based on distributional (WORD2VEC) representations.",2.3 Attribute Prediction,[0],[0]
It is zeroshot learning in the sense of Palatucci et al. (2009):,2.3 Attribute Prediction,[0],[0]
"We split the datasets at the entity, rather than attribute level, such that at test time our system must predict the full attribute set of countries and cities that were not seen during training at all.
",2.3 Attribute Prediction,[0],[0]
We use logistic regression.,2.3 Attribute Prediction,[0],[0]
"In effect, we predict each output variable (FreeBase attribute) with an independent logistic regression model based on a constant set of input features (WORD2VEC distributional dimensions).",2.3 Attribute Prediction,[0],[0]
We call this model DIST2REF.,2.3 Attribute Prediction,[0],[0]
"DIST2REF does not take advantage of the correlations between the output attributes mentioned in Section 2.2.
",2.3 Attribute Prediction,[0],[0]
"The dependent variables are binary as well as numeric FreeBase attributes, and our model does not distinguish between them.",2.3 Attribute Prediction,[0],[0]
"For binary attributes, we interpret the value returned by the model as the probability of “success” of a binary Bernoulli trial.",2.3 Attribute Prediction,[0],[0]
"In the numeric case, we view the probability returned by the model as directly representing normalized attribute values.",2.3 Attribute Prediction,[0],[0]
"We design the model using the Countries dataset, and apply it to Cities without further tuning to test its robustness.",2.4 Experimental Setup,[0],[0]
"We optimize the parameters with gradient descent, using the Cross Entropy error function.",2.4 Experimental Setup,[0],[0]
"We considered L2 regularization to address possible overfitting, but experiments on validation set showed that the model performs best without any regularization.
",2.4 Experimental Setup,[0],[0]
"As for baselines, for binary features we predict the majority class (0 or 1), and for numeric features we predict the mean value of the feature in the training set.",2.4 Experimental Setup,[0],[0]
"These are of course strong baselines to beat.
",2.4 Experimental Setup,[0],[0]
"As an upper bound, we train a model that uses the same architecture as described above but uses as input not distributional vectors but the FreeBase attributes themselves.",2.4 Experimental Setup,[0],[0]
"In other words, this model has to learn “only” an identity mapping.",2.4 Experimental Setup,[0],[0]
"This is not trivial, though, for example due to the presence of strong correlations among attributes, in particular
the time series attributes (cf. Section 2.2).",2.4 Experimental Setup,[0],[0]
We call this model REF2REF.,2.4 Experimental Setup,[0],[0]
"Since there is no appropriate unified evaluation measure that covers both numeric and binary attributes, we evaluate them separately.",2.5 Evaluation,[0],[0]
"For binary attributes, we report the attributes’ mean accuracy.
",2.5 Evaluation,[0],[0]
"For numeric attributes, we consider attribute prediction a ranking task.",2.5 Evaluation,[0],[0]
"As an example, take the population::2011::number attribute, and imagine that we only have three countries (Germany: 80M; Spain: 36M; and Netherlands: 17M).",2.5 Evaluation,[0],[0]
"If we predict 56M for Spain’s population, it is still (correctly) predicted as the second most populous country (rank difference of 0); a prediction of 16M, however, would push Spain to third place (rank difference of 1).
",2.5 Evaluation,[0],[0]
This suggests the use of rank correlation coefficients like Spearman’s ρ.,2.5 Evaluation,[0],[0]
"However, we want to measure not only how well the model can rank the countries in the test set, but also whether these predictions are consistent with the training set (which makes evaluation both more challenging and more realistic).",2.5 Evaluation,[0],[0]
"One way of achieving this goal would be to use ρ on the union of training and test instances, but this could lead to misleadingly high correlation coefficients since this method would include the labels of the training instances in the evaluation.
",2.5 Evaluation,[0],[0]
"Consequently, we define our own evaluation measure, following a rationale similar to Frome et al.’s (2013) evaluation of a zero-shot learning scenario.",2.5 Evaluation,[0],[0]
"What we evaluate, for each attribute, is the rank of the test countries in the whole country list.",2.5 Evaluation,[0],[0]
"Note that this makes our task harder, as there are more confounders: If we only evaluated on the test set, there would be shorter lists and therefore less chances of getting bad rankings.",2.5 Evaluation,[0],[0]
"So, concretely, we first define the prediction quality of each attribute, Q(a), as the median of the rank difference between the prediction and the gold standard in a list that includes both training and test countries (we use the median to give less weight to outlier countries).",2.5 Evaluation,[0],[0]
We also normalize the rank difference to obtain a number between zero and one.,2.5 Evaluation,[0],[0]
"In a second step, we define the quality of the complete model, the normalized rank score (NRS), as the mean of all attribute quality scores, in parallel to our evaluation on binary attributes.
",2.5 Evaluation,[0],[0]
Let the set of instances I be partitioned into training instances Tr and test instances Ts.,2.5 Evaluation,[0],[0]
Let a ∈,2.5 Evaluation,[0],[0]
"A
denote an attribute.",2.5 Evaluation,[0],[0]
We write pa(i) for the predicted value of attribute a for instance i and ga(i) for the gold standard value.,2.5 Evaluation,[0],[0]
"Finally, let r(v, S) denote the rank of value v in the list resulting when ordering the set S.",2.5 Evaluation,[0],[0]
"Now we can define:
Q(a) = 1 ||I||med{|r(pa(i), I)",2.5 Evaluation,[0],[0]
"− (1)
r(ga(i), I)| − 1 | i ∈ Ts} NRS =
1 ||A|| ∑ a∈A Q(a) (2)
",2.5 Evaluation,[0],[0]
"This measure can be interpreted similarly to Mean Reciprocal Rank (Manning et al., 2008):",2.5 Evaluation,[0],[0]
"It has range [0..1], with smaller numbers indicating better ranking: 0.1, for example, means that, on average, the prediction is 10% of the ranks off (e.g., by four countries in a forty-country list).4
Note that, when evaluating each instance i, we use gold-standard values for all other instances, so that there the baseline is not hampered by ties.",2.5 Evaluation,[0],[0]
Table 2 shows the results of our experiments on the two test sets.,3 Results,[0],[0]
"For accuracy 1 is best, but for NRS 0 is best.",3 Results,[0],[0]
"Recall from Section 2.2 that we perform model selection on the Countries dataset only.
",3 Results,[0],[0]
"The baseline is relatively high, in particular for the binary attributes, many of which are positive for a small subset of entities only.",3 Results,[0],[0]
"The amount of skew differs considerably between the two datasets, though.",3 Results,[0],[0]
"For Countries, the baseline yields an accuracy of 0.86, but it achieves 0.97 on Cities.",3 Results,[0],[0]
"The increase stems from very sparse categorical City features such as containedBy, which includes all
4Subtracting 1 in Equation (1) ensures that, when the predicted and gold value of an attribute are adjacent in the ranking, their rank difference is 0, capturing the intuition of rank difference as counting the number of falsely intervening items.
levels of administrative divisions – that is, for the US, all counties appear as values and are transformed into sparse binary features (cf. Section 2.2).",3 Results,[0],[0]
"Of course, the predictions of the baseline are useless, since it always predicts the absence of any features.",3 Results,[0],[0]
"On numeric features, where the baseline predicts the mean, its performance is 0.35 NRS on both datasets.",3 Results,[0],[0]
"In other words, its average prediction is off by about one third the length of the ranked list for each attribute.
",3 Results,[0],[0]
"Recall that the upper bound model, REF2REF, uses FreeBase attributes to predict FreeBase attributes.",3 Results,[0],[0]
All it has to learn is that there is one feature in the input that corresponds ideally to the output.,3 Results,[0],[0]
"This works almost perfectly for binary attributes, with accuracy values of 0.96 (Countries) and 1.00 (Cities).",3 Results,[0],[0]
"However, its performance on numeric features (with NRS at 0.14 and 0.21, respectively) is not quite perfect.",3 Results,[0],[0]
"We attribute this to the presence of correlations (cf. Section 2.2).
",3 Results,[0],[0]
"The model whose performance we are actually interested in, DIST2REF, in which we map from distributional information to FreeBase features, performs with remarkable consistency between these two extremes.",3 Results,[0],[0]
"In fact, we see a consistent error reduction of around 30% over the baseline, with a similar distance to the upper bound.",3 Results,[0],[0]
"A significance test with bootstrap resampling (Efron and Tibshirani, 1994) showed that all pairwise comparisons (Baseline vs. DIST2REF, DIST2REF vs. REF2REF) are statistically significant at p<0.001.
",3 Results,[0],[0]
"To rule out that we misinterpret our accuracybased evaluation for the binary features in the face of a highly skewed class distribution, we also computed precision, recall, and F-Score values.",3 Results,[0],[0]
"The relative patterns match those of the accuracybased evaluation well (Countries: baseline F=0.13, DIST2REF",3 Results,[0],[0]
"F=0.51, REF2REF F=0.77) and indicate that generally precision is higher than recall.
",3 Results,[0],[0]
"We think that these are overall promising results, given that the FreeBase attributes we predict are fairly fine-grained, and we only use generic distributional information as input.",3 Results,[0],[0]
We take the overall results just presented to suggest that we are able to learn referential attributes from distributional information to a large extent.,4 Analysis,[0],[0]
"In this section we take a closer look at what kind of information we are able to learn, what is beyond the scope of our model, and what are the differences between the entity representations in WORD2VEC and the ones our model produces.",4 Analysis,[0],[0]
All the data concerns the test sets only.,4 Analysis,[0],[0]
We start with a qualitative analysis of the Countries dataset.,4.1 Attribute Groups,[0],[0]
"Due to the large number of attributes, we sort all individual attributes into attribute groups by their base name (i.e. the leftmost component of their name, cf. Section 2.2), which offers an accessible level of granularity for inspection.",4.1 Attribute Groups,[0],[0]
"We obtain 34 numeric and 40 binary attribute groups with median sizes of 8.5 and 2 attributes per group, respectively.
",4.1 Attribute Groups,[0],[0]
Table 3 shows the attribute groups for both types sorted by quality.,4.1 Attribute Groups,[0],[0]
"For each group, we report average normalized rank score (NRS) and accuracy, respectively, for both DIST2REF and the baseline.
",4.1 Attribute Groups,[0],[0]
"The analysis suggests that there are two main factors that account for the results: (1) The degree to which an attribute is contextually supported, that is, to what extent its values can be identified on the basis of the contextual information that is captured in a distributional model, and (2) general properties of the data that affect Machine Learning, most notably data sparseness, possibly also feature value distributions.
",4.1 Attribute Groups,[0],[0]
"Attributes that are contextually supported include for instance those related to socioeconomic development (see below for details); people talk (and so write) about countries being more or less developed, rich, having one or another kind of laws, and this is captured in the abstractions over textual context that distributional models perform.",4.1 Attribute Groups,[0],[0]
"As an extreme example of an attribute that is not contextually supported, consider the numeric ISO code of a country (iso numeric), whose values are arbitrary: They do not correspond to facts about the world that are reflected in the way people use lan-
guage, and so can’t be picked up by the distributional model.",4.1 Attribute Groups,[0],[0]
"For this reason, DIST2REF does worse than the baseline.
",4.1 Attribute Groups,[0],[0]
"Note that, in a sufficiently large corpus, we might indeed encounter statements like The numeric ISO code for Spain is 724.",4.1 Attribute Groups,[0],[0]
"However, since distributional models represent words as aggregated distributions of their contexts, and compute semantic similarity from these context distributions, the contexts that they use need to be generic enough to yield meaningful overlap between concepts (e.g., words).",4.1 Attribute Groups,[0],[0]
"As a result, distributional models cannot easily represent knowledge of the form “the value for property Y of word/concept X is Z”.
",4.1 Attribute Groups,[0],[0]
"Fortunately, we find that many FreeBase attributes are contextually supported to a substantial degree, even some seemingly arbitrary ones.",4.1 Attribute Groups,[0],[0]
"An example is calling codes, which we predict very well.",4.1 Attribute Groups,[0],[0]
"They turn out to be correlated with geolocations: 2X calling codes are located in Africa, 3X calling codes in Southern and Eastern Europe and 4X calling codes in Western and Northern Europe (for comparison, ISO codes are assigned in a roughly alphabetical order).
",4.1 Attribute Groups,[0],[0]
Numeric Attributes.,4.1 Attribute Groups,[0],[0]
Our best numeric attributes belong to the geolocation group (latitude and longitude).,4.1 Attribute Groups,[0],[0]
We provide a more detailed analysis of these attributes below (Section 4.2).,4.1 Attribute Groups,[0],[0]
"As mentioned above, we also excel at many attributes related to a country’s economic and social development (broadly construed), such as GNI, GDP, CO2 emissions, internet usage (each per capita), or fertility rate.",4.1 Attribute Groups,[0],[0]
"These attributes can be expected to be contextually grounded – e.g., Luxembourg will occur with contexts like “broadband” or “rich” more than India.
",4.1 Attribute Groups,[0],[0]
"Note, however, that the information contained in the vectors is surprisingly subtle: For instance, the fertility rate is a function of both general development status (lower rates in more developed countries) and of specific social factors (higher rates in countries with more support for families, such as France and Finland compared countries with less support, such as Germany or Italy).
",4.1 Attribute Groups,[0],[0]
"Around the middle of the table, we find the absolute versions of the developmental cluster above (GNI in $, real and nominal GDP).",4.1 Attribute Groups,[0],[0]
"Evidently, the absolute versions of these attributes are substantially less contextually supported than the relative versions.",4.1 Attribute Groups,[0],[0]
"This is not surprising: While India and China have high absolute GDPs because they are
large countries, and for instance Luxembourg has a much smaller one, these numbers are not indicative of the actual conditions in these countries, and therefore also not so clearly correlated with what people write about them.",4.1 Attribute Groups,[0],[0]
This provides another interesting angle on the difference between distributional and formal knowledge representation.,4.1 Attribute Groups,[0],[0]
"In a formal system, absolute GDP, relative GDP, and population stand in a fixed linear relationship and knowing any two of the three uniquely determines the third – thus, all three attributes have equal status.",4.1 Attribute Groups,[0],[0]
"In our distributional space, their status is clearly
different, determined by the conceptual relevance of the different attributes.
",4.1 Attribute Groups,[0],[0]
"Towards the end of the table, we find more attributes related to socioeconomic development, such as government percent debt and minimum wage.",4.1 Attribute Groups,[0],[0]
"While these should be contextually supported, too, the problem here is factor (2) mentioned above, namely severe data sparsity (see column f(A) in Table 3, which lists the median number of datapoints that exhibit each attribute group).",4.1 Attribute Groups,[0],[0]
"The same goes for the remaining attribute groups, for instance casualties (describing the
total number of military casualties incurred in history), date founded and date dissolved,5 or climate avg rainfall.
",4.1 Attribute Groups,[0],[0]
Binary Attributes.,4.1 Attribute Groups,[0],[0]
"The binary attributes show a similar picture, albeit somewhat less sharp.",4.1 Attribute Groups,[0],[0]
"We again find contextually unsupported groups, many of them arising from our fully automatic attribute mining from FreeBase (cf. Section 2.2).",4.1 Attribute Groups,[0],[0]
"There are many categorical attributes that store metadata about numeric attributes (such as the currency in the gdp and gni groups) as well as meta-information of FreeBase: exceptions is a specific marker of potentially inconsistent entries about Ghana, and equivalent instances is a flag concerning links between FreeBase and OpenCyc.",4.1 Attribute Groups,[0],[0]
"Fortunately, almost all contextually unsupported groups are small, with only one or two attributes, and do not have a large impact on the overall performance.",4.1 Attribute Groups,[0],[0]
"We decided not to exclude them from evaluation for robustness’ sake, since there is no automatic way to identify contextually unsupported attributes in a new dataset.
",4.1 Attribute Groups,[0],[0]
"We obtain good results on meaningful attributes that are arguably strongly contextually grounded, such as geographical and geopolitical attributes (member of: membership in international organizations; location on a continent, etc.).",4.1 Attribute Groups,[0],[0]
"However, we fare relatively badly on government-related attributes (form of government, governing officials).",4.1 Attribute Groups,[0],[0]
"While this seems surprising at first glance, the form of government attribute in FreeBase makes very fine-grained distinctions: Its values include “unitary state”, “presidential system”, “parliamentary system” and “republic”, which are not mutually exclusive, and misses obvious alternatives like “authoritarian system”.",4.1 Attribute Groups,[0],[0]
It is not surprising that distributional models cannot make such subtle distinction between presidential and parliamentary systems.,4.1 Attribute Groups,[0],[0]
The attribute governing official presents a similar case.,4.1 Attribute Groups,[0],[0]
"Other bad attributes are very domain-specific, including athletes, encoding the athletic disciplines that countries participate in (such as swimming, judo, running, etc.), and the data sparsity issue is certainly worse for the binary attributes.
",4.1 Attribute Groups,[0],[0]
"5Note that date-based attributes can be contextually supported: We do better on national anthem since, for which we have more datapoints, 97.",4.1 Attribute Groups,[0],[0]
"To analyze the difference between the distributional representations and the output of our model, we focus on geolocation, our best attribute group.
",4.2 Geolocation,[0],[0]
"It has already been shown that geometric distance in distributional space captures, to a certain extent, physical distance between locations in the real world (Louwerse and Zwaan, 2009).",4.2 Geolocation,[0],[0]
Table 4 shows that DIST2REF extracts even more precise distance information from distributional vectors.,4.2 Geolocation,[0],[0]
The table reports the correlation between real and model-predicted distances for countries and cities.,4.2 Geolocation,[0],[0]
"Ground-truth great circle distances (Kern and Bland, 1948) between items are computed using the FreeBase longitude and latitude values; for DIST2REF we use its predicted latitude and longitude values; for WORD2VEC, the cosines between the corresponding distributional vectors.
",4.2 Geolocation,[0],[0]
"We obtain highly significant correlations in all cases (p<10−14), but much higher for DIST2REF.",4.2 Geolocation,[0],[0]
"For countries, as shown in Table 4, the correlation is -0.36 for WORD2VEC (negative, because cosine is a similarity measure), 0.49 for DIST2REF.",4.2 Geolocation,[0],[0]
"For cities, WORD2VEC reaches -0.45 correlation, and DIST2REF distances are at 0.88, showing that the method can estimate city positions to a perhaps unexpectedly high degree of accuracy.6
This result suggests that we manage to objectify the information in the distributional model, anchoring the entities more firmly in the external world.",4.2 Geolocation,[0],[0]
"Indeed, distributional models are known to be subject to conceptual or cultural effects in their distance estimations.",4.2 Geolocation,[0],[0]
"For instance, in WORD2VEC German and Spanish cities are much farther away than in the physical world, while cities within Spain and within Germany are predicted to be a bit closer than they actually are.",4.2 Geolocation,[0],[0]
"Note that these effects have
6The results are confirmed when the analysis is repeated using the Spearman correlation measure: The DIST2REF coefficients are stable, whereas those of WORD2VEC go down to 0.22 (countries) and 0.40 (cities), respectively.",4.2 Geolocation,[0],[0]
"The good results for Spearman, as a rank-based measure, indicate that our success is not dominated by outliers.
",4.2 Geolocation,[0],[0]
"an actual cognitive basis: Human intuitions about objective physical distance between countries and cities are biased by cognitive, cultural and socioeconomic factors, as explored for example in Friedman et al. (2002), who report that Texans locate Canadian cities closer to the US border relative to Mexican cities, despite their proximity to the latter, and that they place Southern US cities further south than they really are.
",4.2 Geolocation,[0],[0]
"Interestingly, DIST2REF does also show some cultural effects in its geolocation errors: For example, some Pacific island states with lesser-known identities (e.g., Nauru and French Polynesia) are placed in the Indian Ocean, where we find the perhaps prototypes of beautiful islands, like Seychelles and Mauritius; also, Central American countries (such as Panama, El Salvador, and Nicaragua) move towards their “cultural center of gravity”, South America.
",4.2 Geolocation,[0],[0]
"However, this kind of cultural bias is much more prominent in the original WORD2VEC distributional representation.",4.2 Geolocation,[0],[0]
The Spain/Germany effect discussed above is not found in the DIST2REF model at all.,4.2 Geolocation,[0],[0]
"And while both DIST2REF and WORD2VEC place Mexican and Spanish cities in our test set closer to each other than they actually are, WORD2VEC does so to a much larger extent.",4.2 Geolocation,[0],[0]
"In line with our goal to extract referential attributes, thus, we are satisfied to see that DIST2REF manages to minimize this bias and distill the referential part from the distributional representations.",4.2 Geolocation,[0],[0]
"There is a large literature on exploiting corpus evidence, sometimes through distributional semantic methods, in order to construct and populate structured knowledge bases (KBs) (e.g., Buitelaar and Cimiano (2008) and references therein).",5 Related Work,[0],[0]
"This line of work, however, does not attempt to connect entity representations extracted from corpora and from KBs, as we do.",5 Related Work,[0],[0]
"Moreover, it focuses on harvesting relations between entities or between entities and a limited number of discrete attributes, rather than predicting full-fledged KB representations of specific entities, like we do.",5 Related Work,[0],[0]
Freitas and Curry (2014) and Freitas et al. (2014) embed relational graphs from KBs in a distributional semantic space to support various forms of search and reasoning about the KB.,5 Related Work,[0],[0]
"The focus is again on relations between discrete entities, and on exploiting distributional semantics to navigate among them.
",5 Related Work,[0],[0]
Socher et al. (2013a) represent WordNet and FreeBase entities with corpus-based distributional vectors.,5 Related Work,[0],[0]
They train a tensor for each relation of interest to return high scores when combined with the vectors of two entities that hold the intended relation.,5 Related Work,[0],[0]
"At test time, the system is used to classify relational tuples as true or false, as well as to predict new entities that hold a certain relationship with a target entity.",5 Related Work,[0],[0]
"This is quite close in spirit to what we do, except that, given an entity1-relation-entity2 tuple, we treat relation-entity2 as a binary attribute of entity1, and we try to induce such attributes on a larger scale (Socher et al. consider seven relations in total).",5 Related Work,[0],[0]
"Moreover, we rely on the same architecture to learn discrete features denoting relations with entities and numerical features, to induce full attribute-based descriptions of entities.
",5 Related Work,[0],[0]
"Our proposal is only distantly related to methods to embed words tokens and KB entities and relationships in a vector space, e.g., for better relation extraction (see Weston et al. (2013) and references therein).",5 Related Work,[0],[0]
"This line of work does not use distributional semantics to induce word vectors, and ignores numerical attributes.
",5 Related Work,[0],[0]
The broader goal of getting at referential information with distributional semantics is shared with Herbelot (2015).,5 Related Work,[0],[0]
"However, the specific approach is different, as she constructs vectors for individual entities (literary characters) by contextualizing generic noun vectors with distributional properties of those entities.",5 Related Work,[0],[0]
"Finally, we share our methodology with work on mapping between corpus-based word representations and other representational spaces, such as subject-generated concept properties (Johns and Jones, 2012; Hill et al., 2014; Făgărăşan",5 Related Work,[0],[0]
"et al., 2015), visual features (Frome et al., 2013; Socher et al., 2013b; Lazaridou et al., 2014) or brain signals (Mitchell et al., 2008; Murphy et al., 2012).",5 Related Work,[0],[0]
"In all these settings, the focus is entirely on predicting numerical attributes, whereas we treat both numerical and binary attributes.",5 Related Work,[0],[0]
"Rubinstein et al. (2015) use distributional vectors to predict binary conceptual attributes of common nouns, as well as a continuous score measuring saliency of such attributes.",5 Related Work,[0],[0]
Our target features are conceptually very different from those of all these studies.,5 Related Work,[0],[0]
"We have shown that a simple model can learn to predict, to a reasonable degree of accuracy, ref-
erential attributes of an entity that are typically seen in a knowledge base from the corresponding corpus-based distributional representation.",6 Discussion and Conclusion,[0],[0]
"The results suggest that, while distributional semantic vectors can be used “as-is” to capture generic word similarity, with some supervision it is also possible to extract other kinds of information from them, including structured factual statements of the sort encoded in manually-curated knowledge bases.",6 Discussion and Conclusion,[0],[0]
"This makes distributional vectors very attractive as general-purpose word meaning representations.
",6 Discussion and Conclusion,[0],[0]
"We have also shown that some of the errors in the predictions can be explained on cultural grounds, but that these effects are more pronounced in the input of our model, a standard distributional semantic model, than in its output.",6 Discussion and Conclusion,[0],[0]
"In this sense, our model manages to objectify the information that it is provided with.",6 Discussion and Conclusion,[0],[0]
"Our analyses also suggest that the main limiting factor in learning referential attributes, apart from good old data sparseness, is the degree to which they are contextually supported, that is, to what extent they are expressed with consistent and specific linguistic means in the context of their target words.",6 Discussion and Conclusion,[0],[0]
"This determines whether they are actually represented in the distributional model in the first place.
",6 Discussion and Conclusion,[0],[0]
"More generally, we see our work as a small step towards the more general goal of bridging the concept-referent gap in distributional semantics.",6 Discussion and Conclusion,[0],[0]
"A common noun such as dog denotes a concept, based on a prototype with fuzzy boundaries, susceptible of metaphorical extensions, and bearing all the other hallmarks of generic conceptual knowledge (Carlson, 2009; Murphy, 2002).",6 Discussion and Conclusion,[0],[0]
These might be adequately captured by the properties of the dog vector in distributional semantic space.,6 Discussion and Conclusion,[0],[0]
"However, when used in a specific discourse, words and more complex linguistic expressions often denote specific referents with fixed, “hard” properties, such as this dog, or Amur, when used for my neighbor’s dog at 3.31pm on May 29th 2015 in Novosibirsk, a 61cm-tall black-and-tan foxhound.",6 Discussion and Conclusion,[0],[0]
Amur is more easily characterized by a set of precise attributevalue pairs than by a vector in a generic conceptual space.,6 Discussion and Conclusion,[0],[0]
Our experiment suggests that distributional vectors encode both generic conceptual knowledge and more precise attributes of specific referents.,6 Discussion and Conclusion,[0],[0]
"Of course, while we can use FreeBase and other knowledge bases to gather training data about public-domain entities, such as countries or cities, it is still not clear where we could gather
appropriate training data to learn about the specific properties of “private-discourse” referents such as Amur.",6 Discussion and Conclusion,[0],[0]
"Moreover, it remains to be seen whether the properties of common named entities, such as countries and cities, that are in a sense “hybrid” between the conceptual and referential domains, also transfer to entities of a more specific and private kind.",6 Discussion and Conclusion,[0],[0]
"Finally, it is still not clear how to extend the current approach beyond words and phrases directly denoting an entity (Amur) to other kinds of definite descriptions (this dog).
",6 Discussion and Conclusion,[0],[0]
"Acknowledgments: This project has received funding from the European Union’s Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement No 655577 (LOVe); ERC 2011 Starting Independent Research Grant n. 283554 (COMPOSES); DFG (SFB 732, Project D10); and Spanish MINECO (grant FFI2013-41301-P).",6 Discussion and Conclusion,[0],[0]
"This paper reflects the authors’ view only, and the EU is not responsible for any use that may be made of the information it contains.",6 Discussion and Conclusion,[0],[0]
"Special thanks to Christian Scheible for help with the Machine Learning part, to the anonymous reviewers for insightful and constructive feedback, and to the FLOSS reading group for helping us shape our ideas on the topic of this paper.",6 Discussion and Conclusion,[0],[0]
"Distributional methods have proven to excel at capturing fuzzy, graded aspects of meaning (Italy is more similar to Spain than to Germany).",abstractText,[0],[0]
"In contrast, it is difficult to extract the values of more specific attributes of word referents from distributional representations, attributes of the kind typically found in structured knowledge bases (Italy has 60 million inhabitants).",abstractText,[0],[0]
"In this paper, we pursue the hypothesis that distributional vectors also implicitly encode referential attributes.",abstractText,[0],[0]
"We show that a standard supervised regression model is in fact sufficient to retrieve such attributes to a reasonable degree of accuracy: When evaluated on the prediction of both categorical and numeric attributes of countries and cities, the model consistently reduces baseline error by 30%, and is not far from the upper bound.",abstractText,[0],[0]
"Further analysis suggests that our model is able to “objectify” distributional representations for entities, anchoring them more firmly in the external world in measurable ways.",abstractText,[0],[0]
Distributional vectors encode referential attributes,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1063–1072 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1098
Abstractive summarization aims to generate a shorter version of the document covering all the salient points in a compact and coherent fashion. On the other hand, query-based summarization highlights those points that are relevant in the context of a given query. The encodeattend-decode paradigm has achieved notable success in machine translation, extractive summarization, dialog systems, etc. But it suffers from the drawback of generation of repeated phrases. In this work we propose a model for the query-based summarization task based on the encode-attend-decode paradigm with two key additions (i) a query attention model (in addition to document attention model) which learns to focus on different portions of the query at different time steps (instead of using a static representation for the query) and (ii) a new diversity based attention model which aims to alleviate the problem of repeating phrases in the summary. In order to enable the testing of this model we introduce a new query-based summarization dataset building on debatepedia. Our experiments show that with these two additions the proposed model clearly outperforms vanilla encode-attend-decode models with a gain of 28% (absolute) in ROUGE-L scores.",text,[0],[0]
"Over the past few years neural models based on the encode-attend-decode (Bahdanau et al.,
2014) paradigm have shown great success in various natural language generation (NLG) tasks such as machine translation (Bahdanau et al., 2014), abstractive summarization ((Rush et al., 2015),(Nallapati et al., 2016)) dialog (Li et al., 2016), etc.",1 Introduction,[0],[0]
One such NLG problem which has not received enough attention in the past is query based abstractive text summarization where the aim is to generate the summary of a document in the context of a query.,1 Introduction,[0],[0]
"In general, abstractive summarization, aims to cover all the salient points of a document in a compact and coherent fashion.",1 Introduction,[0],[0]
"On the other hand, query focused summarization highlights those points that are relevant in the context of the query.",1 Introduction,[0],[0]
"Thus given a document on “the super bowl”, the query “How was the half-time show?”, would result in a summary that would not cover the actual game itself.
",1 Introduction,[0],[0]
Note that there has been some work on query based extractive summarization in the past where the aim is to simply extract the most salient sentence(s) from a document and treat these as a summary.,1 Introduction,[0],[0]
There is no natural language generation involved.,1 Introduction,[0],[0]
"Since, we were interested in abstractive (as opposed to extractive) summarization we created a new dataset based on debatepedia.",1 Introduction,[0],[0]
"This dataset contains triplets of the form (query, document, summary).",1 Introduction,[0],[0]
"Further, each summary is abstractive and not extractive in the sense that the summary does not necessarily comprise of a sentence which is simply copied from the original document.
",1 Introduction,[0],[0]
"Using this dataset as a testbed, we focus on a recurring problem in models based on the encode-attend-decode paradigm.",1 Introduction,[0],[0]
"Specifically, it is observed that the summaries produced by such models contain repeated phrases.",1 Introduction,[0],[0]
"Table 1 shows a few such examples of summaries gener-
1063
ated by such a model when trained on this new dataset.",1 Introduction,[0],[0]
"This problem has also been reported by (Chen et al., 2016) in the context of summarization and by (Sankaran et al., 2016) in the context of machine translation.
",1 Introduction,[0],[0]
We first provide an intuitive explanation for this problem and then propose a solution for alleviating it.,1 Introduction,[0],[0]
A typical encode-attend-decode model first computes a vectorial representation for the document and the query and then produces a contextual summary one word at a time.,1 Introduction,[0],[0]
Each word is produced by feeding a new context vector to the decoder at each time step by attending to different parts of the document and query.,1 Introduction,[0],[0]
"If the decoder produces the same word or phrase repeatedly then it could mean that the context vectors fed to the decoder at these time steps are very similar.
",1 Introduction,[0],[0]
We propose a model which explicitly prevents this by ensuring that successive context vectors are orthogonal to each other.,1 Introduction,[0],[0]
"Specifically, we subtract out any component that the
current context vector has in the direction of the previous context vector.",1 Introduction,[0],[0]
"Notice that, we do not require the current context vector to be orthogonal to all previous context vectors but just its immediate predecessor.",1 Introduction,[0],[0]
This enables the model to attend to words repeatedly if required later in the process.,1 Introduction,[0],[0]
"To account for the complete history (or all previous context vectors) we also propose an extension of this idea where we pass the sequence of context vectors through a LSTM (Hochreiter and Schmidhuber, 1997) and ensure that the current state produced by the LSTM is orthogonal to the history.",1 Introduction,[0],[0]
"At each time step, the state of the LSTM is then fed to the decoder to produce one word in the summary.
",1 Introduction,[0],[0]
Our contributions can be summarized as follows: (i) We propose a new dataset for query based abstractive summarization and evaluate encode-attend-decode models on this dataset (ii) We study the problem of repeating phrases in NLG in the context of this dataset and propose two solutions for countering this problem.,1 Introduction,[0],[0]
We show that our method outperforms a vanilla encoder-decoder model with a gain of 28% (absolute) in ROUGE-L score (iii) We also demonstrate that our method clearly outperforms a recent state of the art method proposed for handling the problem of repeating phrases with a gain of 7% (absolute) in ROUGE-L scores (iv) We do a qualitative analysis of the results and show that our model indeed produces outputs with fewer repetitions.,1 Introduction,[0],[0]
"Summarization has been studied in the context of text ((Mani, 2001), (Das and Martins, 2007), (Nenkova and McKeown, 2012)) as well as speech ((Zhu and Penn, 2006), (Zhu et al., 2009)).",2 Related Work,[0],[0]
"A vast majority of this work has focused on extractive summarization where the idea is to construct a summary by selecting the most relevant sentences from the document ((Neto et al., 2002), (Erkan and Radev, 2004), (Filippova and Altun, 2013), (Colmenares et al., 2015), (Riedhammer et al., 2010), (Ribeiro et al., 2013)).",2 Related Work,[0],[0]
There has been some work on abstractive summarization in the context of DUC-2003 and DUC-2004 contests (Zajic et al.).,2 Related Work,[0],[0]
"We refer the reader to (Das and Martins, 2007) and (Nenkova and McKeown, 2012) for an excellent survey of
the field.",2 Related Work,[0],[0]
"Recent research in abstractive summarization has focused on data driven neural models based on the encode-attend-decode paradigm (Bahdanau et al., 2014).",2 Related Work,[0],[0]
"For example, (Rush et al., 2015), report state of the art results on the GigaWord and DUC corpus using such a model.",2 Related Work,[0],[0]
"Similarly, the work of Lopyrev (2015) uses neural networks to generate news headline from short news stories.",2 Related Work,[0],[0]
Chopra et al. (2016) extend the work of Rush et al. (2015) and report further improvements on the two datasets.,2 Related Work,[0],[0]
"Hu et al. (2015) introduced a dataset for Chinese short text summarization and evaluated a similar RNN encoder-decoder model on it.
",2 Related Work,[0],[0]
One recurring problem in encoder-decoder models for NLG is that they often repeat the same phrase/word multiple times in the summary (at the cost of both coherency and fluency).,2 Related Work,[0],[0]
Sankaran et al. (2016) study this problem in the context of MT and propose a temporal attention model which enforces the attention weights for successive time steps to be different from each other.,2 Related Work,[0],[0]
"Similarly, and more relevant to this work, Chen et al. (2016) propose a distraction based attention model which maintains a history of attention vectors and context vectors.",2 Related Work,[0],[0]
It then subtracts this history from the current attention and context vector.,2 Related Work,[0],[0]
When evaluated on our dataset their method performs poorly.,2 Related Work,[0],[0]
This could be because their method is very aggressive in dealing with the history (as explained later in the Experiments section).,2 Related Work,[0],[0]
"On the other hand, our method has a better way of handling history (by passing context vectors through an LSTM recurrent network) which gives us the flexibility to forget/retain some portions of the history and at the same time produce diverse context vectors at successive time steps.
",2 Related Work,[0],[0]
We evaluate our method in the context of query based abstractive summarization - a problem which has received almost no attention in the past due to unavailability of datasets.,2 Related Work,[0],[0]
We create a new dataset for this task and show that our method indeed produces better output by reducing the number of repeated phrases produced by encoder decoder models.,2 Related Work,[0],[0]
"As mentioned earlier, there are no existing datasets for query based abstractive summarization.",3 Dataset,[0],[0]
We create such a dataset from Debatepedia an encyclopedia of pro and con arguments and quotes on critical debate topics.,3 Dataset,[0],[0]
There are 663 debates in the corpus (we have considered only those debates which have at least one query with one document).,3 Dataset,[0],[0]
"These 663 debates belong to 53 overlapping categories such as Politics, Law, Crime, Environment, Health, Morality, Religion, etc.",3 Dataset,[0],[0]
A given topic can belong to more than one category.,3 Dataset,[0],[0]
"For example, the topic “Eye for an Eye philosophy” belongs to both “Law” as well as “Morality”.",3 Dataset,[0],[0]
The average number of queries per debate is 5 and the average number of documents per query is 4.,3 Dataset,[0],[0]
"Please refer to the dataset url1 for more details about number of debates per category.
",3 Dataset,[0],[0]
"For example, Figure 1 shows the queries associated with the topic “Algae Biofuel”.",3 Dataset,[0],[0]
It also lists the set of documents and an abstractive summary associated with each query.,3 Dataset,[0],[0]
"As is obvious from the example, the summary is an abstractive summary and not extracted directly from the document.",3 Dataset,[0],[0]
"We crawled 12695 such {query, document, summary} triples from debatepedia (these were all the triples that were available).",3 Dataset,[0],[0]
"Table 2 reports the average length of the query, summary and documents in this dataset.
",3 Dataset,[0],[0]
We used 10 fold cross validation for all our experiments.,3 Dataset,[0],[0]
"Each fold uses 80% of the documents for training, 10% for validation and 10% for testing.",3 Dataset,[0],[0]
"Given a query q = q1, q2, ..., qk containing k words, a document d = d1, d2, ..., dn containing n words, the task is to generate a contextual summary y = y1, y2, ..., ym containing
1http://www.cse.iitm.ac.in/˜miteshk/ datasets/qbas.html
Figure 1: Queries associated with the topic “algae biofuel”
Figure 2: Documents and summaries for a given query
m words.",4 Proposed model,[0],[0]
"This can be modeled as the problem of finding a y∗ that maximizes the probability p(y|q,d) which can be further decomposed as:
y∗ = argmax y
m∏
t=1
p(yt|y1, ..., yt−1,q,d) (1)
We now describe a way of modeling p(yt|y1, ..., yt−1,q,d) using the neural encoderattention-decoder paradigm.",4 Proposed model,[0],[0]
The proposed model contains the following components: (i) an encoder RNN for the query (ii) an encoder RNN for the document (iii) attention mechanism for the query (iv) attention mechanism for the document and (v) a decoder RNN.,4 Proposed model,[0],[0]
All the RNNs use a GRU cell.,4 Proposed model,[0],[0]
Encoder for the query: We use a recurrent neural network with Gated Recurrent Units (GRU) for encoding the query.,4 Proposed model,[0],[0]
"It reads the query q = q1, q2, ..., qk from left to right and computes a hidden representation for each time-step as:
hqi = GRUq(h q i−1, e(qi))",4 Proposed model,[0],[0]
"(2)
where e(qi) ∈ Rd is the d-dimensional embedding of the query word qi.",4 Proposed model,[0],[0]
Encoder for the document:,4 Proposed model,[0],[0]
"This is similar to the query encoder and reads the document d = d1, d2, ..., dn from left to right and computes a hidden representation for each time-step as:
hdi = GRUd(h d i−1, e(di)) (3)
where e(di) ∈ Rd is the d-dimensional embedding of the document word di.",4 Proposed model,[0],[0]
"Attention mechanism for the query : At each time step, the decoder produces an output word
by focusing on different portions of the query (document) with the help of a query (document) attention model.",4 Proposed model,[0],[0]
"We first describe the query attention model which assigns weights αqt,i to each word in the query at each decoder timestep using the following equations.
",4 Proposed model,[0],[0]
"aqt,i = v T q tanh(Wqst +",4 Proposed model,[0],[0]
"Uqh q i ) (4) αqt,i = exp(aqt,i)∑k j=1 exp(a q t,j) (5)
where st is the current state of the decoder at time step t (we will see an exact formula for this soon).",4 Proposed model,[0],[0]
"Wq ∈ Rl2×l1 , Uq ∈ Rl2×l2 , vq ∈ Rl2 , l1 is the size of the decoder’s hidden state, l2 is both the size of hqi and also the size of the final query representation at time step t, which is computed as:
qt = k∑
i=1
αqt,ih q i (6)
Attention mechanism for the document : We now describe the document attention model which assigns weights to each word in the document using the following equations.
",4 Proposed model,[0],[0]
"adt,i = v T d",4 Proposed model,[0],[0]
tanh(Wdst,4 Proposed model,[0],[0]
+,4 Proposed model,[0],[0]
"Udh d i + Zqt) (7) αdt,i = exp(adt,i)∑n j=1 exp(a d t,j)
where st is the current state of the decoder at time step t (we will see an exact formula for this
soon).",4 Proposed model,[0],[0]
"Wd ∈ Rl4×l1 , Ud ∈ Rl4×l4 , Z ∈",4 Proposed model,[0],[0]
"Rl4×l2 , vd ∈ Rl2 , l4 is the size of hdi and also the size of the final document representation dt which is passed to the decoder at time step t as:
dt = n∑
i=1
αdt,ih d i (8)
Note that dt now encodes the relevant information from the document as well as the query (see Equation (7)) at time step t. We refer to this as the context vector for the decoder.",4 Proposed model,[0],[0]
"Decoder: The hidden state of the decoder st at each time t is again computed using a GRU as follows:
st = GRUdec(st−1, [e(yt−1), dt−1]) (9)
where, yt−1 gives a distribution over the vocabulary words at timestep t − 1 and is computed as:
yt = softmax(Wof(Wdecst + Vdecdt)) (10)
where Wo ∈ RN×l1 , Wdec ∈ Rl1×l1 , Vdec ∈ Rl1×l4 , N is the vocabulary size, yt is the final output of the model which defines a probability distribution over the output vocabulary.",4 Proposed model,[0],[0]
"This is exactly the quantity defined in Equation (1) that we wanted to model (p(yt|y1, ..., yt−1,q,d)).",4 Proposed model,[0],[0]
"Further, note that, e(yt−1) is the d-dimensional embedding of the word which has the highest probability under the distribution yt−1.",4 Proposed model,[0],[0]
"Also [e(yt−1), dt−1] means a concatenation of the vectors e(yt−1), dt−1.",4 Proposed model,[0],[0]
"We chose f to be the identity function.
",4 Proposed model,[0],[0]
The model as described above is an instantiation of the encoder-attention-decoder idea applied to query based abstractive summarization.,4 Proposed model,[0],[0]
"As mentioned earlier (and demonstrated later through experiments), this model suffers from the problem of repeating the same phrase/word in the output.",4 Proposed model,[0],[0]
We now propose a new attention model which we refer to as diversity based attention model to address this problem.,4 Proposed model,[0],[0]
"As hypothesized earlier, if the decoder produces the same phrase/word multiple times then it is possible that the context vectors being fed to the decoder at consecutive time steps are
very similar.",4.1 Diversity based attention model,[0],[0]
"We propose four models (D1, D2, SD1, SD2) to directly address this problem.",4.1 Diversity based attention model,[0],[0]
D1:,4.1 Diversity based attention model,[0],[0]
"In this model, after computing dt as described in Equation (8), we make it orthogonal to the context vector at time t− 1:
d ′",4.1 Diversity based attention model,[0],[0]
"t = dt −
dTt",4.1 Diversity based attention model,[0],[0]
d ′,4.1 Diversity based attention model,[0],[0]
"t−1
d ′T t−1d ′",4.1 Diversity based attention model,[0],[0]
"t−1
d ′ t−1 (11)
SD1: The above model imposes a hard orthogonality constraint on the context vector(d ′ t).",4.1 Diversity based attention model,[0],[0]
We also propose a relaxed version of the above model which uses a gating parameter.,4.1 Diversity based attention model,[0],[0]
"This gating parameter decides what fraction of the previous context vector should be subtracted from the current context vector using the following equations:
γt = Wgdt−1 + bg
d ′",4.1 Diversity based attention model,[0],[0]
t = dt,4.1 Diversity based attention model,[0],[0]
"− γt
dTt d ′",4.1 Diversity based attention model,[0],[0]
"t−1
d ′T t−1d ′",4.1 Diversity based attention model,[0],[0]
"t−1
d ′",4.1 Diversity based attention model,[0],[0]
"t−1
where Wg ∈ Rl4×l4 , bg ∈ Rl4 , l4 is the dimension of dt as defined in equation (8).",4.1 Diversity based attention model,[0],[0]
D2:,4.1 Diversity based attention model,[0],[0]
The above model only ensures that the current context vector is diverse w.r.t the previous context vector.,4.1 Diversity based attention model,[0],[0]
It ignores all history before time step t − 1.,4.1 Diversity based attention model,[0],[0]
"To account for the history, we treat successive context vectors as a sequence and use
a modified LSTM cell to compute the new state at each time step.",4.1 Diversity based attention model,[0],[0]
"Specifically, we use the following set of equations to compute a diverse context at time t:
it = σ(Widt + Uiht−1 + bi)
ft = σ(Wfdt + Ufht−1 + bf )
ot = σ(Wodt + Uoht−1 + bo)
ĉt = tanh(Wcdt + Ucht−1 + bc)
ct = it ĉt + ft ct−1
cdiverset",4.1 Diversity based attention model,[0],[0]
=,4.1 Diversity based attention model,[0],[0]
ct,4.1 Diversity based attention model,[0],[0]
"− ct T ct−1 cTt−1ct−1 ct−1 (12)
ht = ot tanh(cdiverset ) d ′",4.1 Diversity based attention model,[0],[0]
t = ht,4.1 Diversity based attention model,[0],[0]
"(13)
where Wi,Wf ,Wo,Wc ∈ Rl5×l4 , Ui, Uf , Uo, Uc ∈ Rl5×l4 , dt is the l4dimensional output of Equation (8); l5 is number of hidden units in the LSTM cell.",4.1 Diversity based attention model,[0],[0]
This final d ′ t from Equation (13) is then used in Equation (9).,4.1 Diversity based attention model,[0],[0]
Note that Equation (12) ensures that state of the LSTM at time step t is orthogonal to the previous history.,4.1 Diversity based attention model,[0],[0]
Figure 3 shows a pictorial representation of the model with a diversity LSTM cell.,4.1 Diversity based attention model,[0],[0]
SD2: This model again uses a relaxed version of the orthogonality constraint used in D2.,4.1 Diversity based attention model,[0],[0]
"Specifically, we define a gating parameter gt and replace (12) above by (14) as define below:
gt = σ(Wgdt + Ught−1",4.1 Diversity based attention model,[0],[0]
"+ bo)
cdiverset = ct",4.1 Diversity based attention model,[0],[0]
"− gt ct T ct−1 cTt−1ct−1 ct−1 (14)
where Wg ∈ Rl5×l4 , Ug ∈ Rl5×l4",4.1 Diversity based attention model,[0],[0]
"We compare with two recently proposed baseline diversity methods (Chen et al., 2016) as described below.",5 Baseline Methods,[0],[0]
Note that these methods were proposed in the context of abstractive summarization (not query based abstractive summarization) and we adapt them for the task of query based abstractive summarization.,5 Baseline Methods,[0],[0]
Below we just highlight the key differences from our model in computing the context vector d ′,5 Baseline Methods,[0],[0]
t passed to the decoder.,5 Baseline Methods,[0],[0]
M1:,5 Baseline Methods,[0],[0]
"This model accumulates all the previous context vectors as ∑t−1 j=1 d ′ j and incorporates
this history while computing a diverse context vector:
d ′",5 Baseline Methods,[0],[0]
"t = tanh(Wcdt − Uc
t−1∑
j=1
d ′ j) (15)
where Wc, Uc ∈ Rl4×l4 are diagonal matrices.",5 Baseline Methods,[0],[0]
We then use this diversity driven context d ′,5 Baseline Methods,[0],[0]
t in Equation (9) and (10).,5 Baseline Methods,[0],[0]
M2:,5 Baseline Methods,[0],[0]
"In this model, in addition to computing a diverse context as described in Equation (15), the attention weights at each time step are also forced to be diverse from the attention weights at the previous time step.
",5 Baseline Methods,[0],[0]
α ′,5 Baseline Methods,[0],[0]
"t,i = v T a tanh(Was ′",5 Baseline Methods,[0],[0]
t +,5 Baseline Methods,[0],[0]
"Uadt − ba
t−1∑
j=1
α ′",5 Baseline Methods,[0],[0]
"j,i)
where Wa ∈ Rl1×l1 , Ua ∈ Rl1×l4 , ba, va ∈ Rl1 , l1 is the number of hidden units in the decoder GRU.",5 Baseline Methods,[0],[0]
"Once again, they maintain a history of attention weights and compute a diverse attention vector by subtracting the history from the current attention vector.",5 Baseline Methods,[0],[0]
We evaluate our models on the dataset described in section 3.,6 Experimental Setup,[0],[0]
Note that there are no prior baselines on query based abstractive summarization so we could only compare with different variations of the encoder decoder models as described above.,6 Experimental Setup,[0],[0]
"Further, we compare our diversity based attention models with existing models for diversity by suitably adapting them to this problem as described earlier.",6 Experimental Setup,[0],[0]
"Specifically, we compare the performance of the following models:
• Vanilla e-a-d:",6 Experimental Setup,[0],[0]
This is the vanilla encoderattention-decoder model adapted to the problem of abstractive summarization.,6 Experimental Setup,[0],[0]
It contains the following components (i) document encoder (ii) document attention model (iii) decoder.,6 Experimental Setup,[0],[0]
It does not contain an encoder or attention model for the query.,6 Experimental Setup,[0],[0]
"This helps us understand the importance of the query.
",6 Experimental Setup,[0],[0]
• Queryenc:,6 Experimental Setup,[0],[0]
This model contains the query encoder in addition to the three components used in the vanilla model above.,6 Experimental Setup,[0],[0]
"It does not contain any attention model for the query.
",6 Experimental Setup,[0],[0]
• Queryatt:,6 Experimental Setup,[0],[0]
"This model contains the query attention model in addition to all the components in Queryenc.
",6 Experimental Setup,[0],[0]
"• D1: The diversity attention model as described in Section 4.1.
",6 Experimental Setup,[0],[0]
"• D2: The LSTM based diversity attention model as described in Section 4.1.
",6 Experimental Setup,[0],[0]
"• SD1: The soft diversity attention model as described in Section 4.1
•",6 Experimental Setup,[0],[0]
"SD2: The soft LSTM based diversity attention model as described in Section 4.1
• B1: Diversity cell in Figure3 is replaced by the basic LSTM cell (i.e. cdiverset = ct instead of using Equation (12).",6 Experimental Setup,[0],[0]
"This helps us understand whether simply using an LSTM to track the history of context vectors (without imposing a diversity constraint) is sufficient.
",6 Experimental Setup,[0],[0]
"• M1: The baseline model which operates on the context vector as described in Section 5.
",6 Experimental Setup,[0],[0]
"• M2: The baseline model which operates on the attention weights in addition to the context vector as described in Section 5.
",6 Experimental Setup,[0],[0]
"We used 80% of the data for training, 10% for validation and 10% for testing.",6 Experimental Setup,[0],[0]
"We create 10 such folds and report the average Rouge-1, Rouge-2, Rouge-L scores across the 10 folds.",6 Experimental Setup,[0],[0]
The hyperparameters (batch size and GRU cell sizes) of all the models are tuned on the validation set.,6 Experimental Setup,[0],[0]
"We tried the following batch sizes : 32, 64 and the following GRU cell sizes 200, 300, 400.",6 Experimental Setup,[0],[0]
"We used Adam (Kingma and Ba, 2014) as the optimization algorithm with the initial learning rate set to 0.0004, β1 = 0.9, β2 = 0.999.",6 Experimental Setup,[0],[0]
We used pre-trained publicly available Glove word embeddings2 and fine-tuned them during training.,6 Experimental Setup,[0],[0]
"The same word embeddings are used for the query words and the document words.
",6 Experimental Setup,[0],[0]
"Table 3 summarizes the results of our experiments.
",6 Experimental Setup,[0],[0]
2http://nlp.stanford.edu/projects/glove/,6 Experimental Setup,[0],[0]
"In this section, we discuss the results of the experiments reported in Table 3. 1.",7 Discussions,[0],[0]
Effect of Query:,7 Discussions,[0],[0]
Comparing rows 1 and 2 we observe that adding an encoder for the query and allowing it to influence the outputs of the decoder indeed improves the performance.,7 Discussions,[0],[0]
This is expected as the query contains some keywords which could help in sharpening the focus of the summary.,7 Discussions,[0],[0]
2.,7 Discussions,[0],[0]
Effect of Query attention model:,7 Discussions,[0],[0]
Comparing rows 2 and 3 we observe that using an attention model to dynamically compute the query representation at each time step improves the results.,7 Discussions,[0],[0]
This suggests that the attention model indeed learns to focus on relevant portions of the query at different time steps.,7 Discussions,[0],[0]
3.,7 Discussions,[0],[0]
"Effect of Diversity models: All the diversity models introduced in the paper (rows 7, 8, 9, 10) give significant improvement over the nondiversity models.",7 Discussions,[0],[0]
"In particular, the modified LSTM based diversity model gives the best results.",7 Discussions,[0],[0]
This is indeed very encouraging and Table 4 shows some sample summaries comparing the performance of different models.,7 Discussions,[0],[0]
4.,7 Discussions,[0],[0]
Comparison with baseline diversity models: The baseline diversity model M1 performs at par with our models D1 and SD1 but not as good as D2 and SD2.,7 Discussions,[0],[0]
"However, the model M2 performs very poorly.",7 Discussions,[0],[0]
We believe that simultaneously adding a constraint on the context vectors as well as attention weights (as is indeed the case with M2) is a bit too aggressive and leads to poor performance (although this needs further investigation).,7 Discussions,[0],[0]
5.,7 Discussions,[0],[0]
Quantitative Analysis:,7 Discussions,[0],[0]
"In addition to the qualitative analysis reported in Table 4 we also did a quantitative analysis by counting the num-
ber of sentences containing repeated words generated by different models.",7 Discussions,[0],[0]
Specifically for the 1268 test instances we counted the number of sentences containing repeated words as generated by different modes.,7 Discussions,[0],[0]
Table 5 summarizes this analysis.,7 Discussions,[0],[0]
In this work we proposed a query-based summarization method.,8 Conclusion,[0],[0]
"The unique feature of
the model is a novel diversification mechanism based on successive orthogonalization.",8 Conclusion,[0],[0]
This gives us the flexibility to: (i) provide diverse context vectors at successive time steps and (ii) pay attention to words repeatedly if need be later in the summary (as opposed to existing models which aggressively delete the history).,8 Conclusion,[0],[0]
We also introduced a new data set and empirically verified we perform significantly better (gain of 28% (absolute) in ROUGE-L score) than applying a plain encode-attend-decode mechanism to this problem.,8 Conclusion,[0],[0]
We observe that adding an attention mechanism on the query string gives significant improvements.,8 Conclusion,[0],[0]
We also compare with a state of the art diversity model and outperform it by a good margin (gain of 7% (absolute) in ROUGE-L score).,8 Conclusion,[0],[0]
The diversification model proposed is general enough to apply to other NLG tasks with suitable modifications and we are currently working on extending this to dialog systems and general summarization.,8 Conclusion,[0],[0]
Abstractive summarization aims to generate a shorter version of the document covering all the salient points in a compact and coherent fashion.,abstractText,[0],[0]
"On the other hand, query-based summarization highlights those points that are relevant in the context of a given query.",abstractText,[0],[0]
"The encodeattend-decode paradigm has achieved notable success in machine translation, extractive summarization, dialog systems, etc.",abstractText,[0],[0]
But it suffers from the drawback of generation of repeated phrases.,abstractText,[0],[0]
In this work we propose a model for the query-based summarization task based on the encode-attend-decode paradigm with two key additions (i) a query attention model (in addition to document attention model) which learns to focus on different portions of the query at different time steps (instead of using a static representation for the query) and (ii) a new diversity based attention model which aims to alleviate the problem of repeating phrases in the summary.,abstractText,[0],[0]
In order to enable the testing of this model we introduce a new query-based summarization dataset building on debatepedia.,abstractText,[0],[0]
Our experiments show that with these two additions the proposed model clearly outperforms vanilla encode-attend-decode models with a gain of 28% (absolute) in ROUGE-L scores.ive summarization aims to generate a shorter version of the document covering all the salient points in a compact and coherent fashion.,abstractText,[0],[0]
"On the other hand, query-based summarization highlights those points that are relevant in the context of a given query.",abstractText,[0],[0]
"The encodeattend-decode paradigm has achieved notable success in machine translation, extractive summarization, dialog systems, etc.",abstractText,[0],[0]
But it suffers from the drawback of generation of repeated phrases.,abstractText,[0],[0]
In this work we propose a model for the query-based summarization task based on the encode-attend-decode paradigm with two key additions (i) a query attention model (in addition to document attention model) which learns to focus on different portions of the query at different time steps (instead of using a static representation for the query) and (ii) a new diversity based attention model which aims to alleviate the problem of repeating phrases in the summary.,abstractText,[0],[0]
In order to enable the testing of this model we introduce a new query-based summarization dataset building on debatepedia.,abstractText,[0],[0]
Our experiments show that with these two additions the proposed model clearly outperforms vanilla encode-attend-decode models with a gain of 28% (absolute) in ROUGE-L scores.,abstractText,[0],[0]
Diversity driven attention model for query-based abstractive summarization,title,[0],[0]
"ar X
iv :1
70 9.
01 12
1v 2
[ cs
.C L
] 2
6 Fe
learning has made it possible to train neural networks that learn to both parse a sentence and use the resulting parse to interpret the sentence, all without exposure to groundtruth parse trees at training time. Surprisingly, these models often perform better at sentence understanding tasks than models that use parse trees from conventional parsers. This paper aims to investigate what these latent tree learning models learn. We replicate two such models in a shared codebase and find that (i) only one of these models outperforms conventional tree-structured models on sentence classification, (ii) its parsing strategies are not especially consistent across random restarts, (iii) the parses it produces tend to be shallower than standard Penn Treebank (PTB) parses, and (iv) they do not resemble those of PTB or any other semantic or syntactic formalism that the authors are aware of.",text,[0],[0]
"Tree-structured recursive neural networks (TreeRNNs; Socher et al., 2011)—which build a vector representation for a sentence by incrementally computing representations for each node in its parse tree—have been proven to be effective at sentence understanding tasks like sentiment analysis (Socher et al., 2013), textual entailment (Bowman et al., 2016), and translation (Eriguchi et al., 2016).",1 Introduction,[0],[0]
"Some variants of these models (Socher et al., 2011; Bowman et al., 2016) can also be trained to produce
∗Now at eBay, Inc.
parse trees that they then consume.",1 Introduction,[0],[0]
"Recent work on latent tree learning (Yogatama et al., 2017; Maillard et al., 2017; Choi et al., 2018) has led to the development of new training methods for TreeRNNs that allow them to learn to parse without ever being given an example of a correct parse tree, thereby replacing direct syntactic supervision with indirect supervision from a downstream task like sentence classification.",1 Introduction,[0],[0]
"These models are designed to learn grammars—strategies for assigning trees to sentences—that are suited to help solve the sentence understanding task at hand, rather than ones that approximate expert-designed grammars like that of the Penn Treebank (PTB; Marcus et al., 1999).
",1 Introduction,[0],[0]
"Latent tree learning models have shown striking success at sentence understanding, reliably performing better on sentiment analysis and textual entailment than do comparable TreeRNN models which use parses assigned by conventional parsers, and setting the state of the art among sentence-encoding models for textual entailment.",1 Introduction,[0],[0]
"However, none of the
work in latent tree learning to date has included any substantial evaluation of the quality of the trees induced by these models, leaving open an important question which this paper aims to answer: Do these models owe their success to consistent, principled latent grammars?",1 Introduction,[0],[0]
"If they do, these grammars may be worthy objects of study for researchers in syntax and semantics.",1 Introduction,[0],[0]
"If they do not, understanding why the models succeed without such syntax could lead to new insights into the use of TreeRNNs and into sentence understanding more broadly.
",1 Introduction,[0],[0]
"While there is still lively debate within linguistic syntax and semantics over the precise grammars that should be used for language understanding and generation, it has long been clear that understanding any natural language sentence requires implicitly or explicitly recognizing which substrings of the sentence form meaningful units, or constituents (Chomsky, 1965; Frege, 1892; Heim and Kratzer, 1998).",1 Introduction,[0],[0]
"This is well illustrated by structurally ambiguous sentences like the one below, repeated from Sag (1991)",1 Introduction,[0],[0]
"a.o.:
1. (a) I saw the [ man [ with the telescope ] ]
→֒I saw the man who had a telescope.",1 Introduction,[0],[0]
(b) I [ saw the man ],1 Introduction,[0],[0]
"[ with the telescope ]
→֒ I used the telescope to view the man.
",1 Introduction,[0],[0]
"Under the partial constituency parse shown in 1a, with a telescope forms a constituent with man, thereby providing additional information about the individual man describes.",1 Introduction,[0],[0]
"On the other hand, in 1b, with a telescope does not form a constituent with man, but instead provides additional information about the action described by saw a man.",1 Introduction,[0],[0]
"In this way, the same string of words can be assigned two different, yet equally valid constituency structures reflecting the different interpretations for the string.",1 Introduction,[0],[0]
"Constituency can be straightforwardly expressed using unlabeled parse trees like the ones used in TreeRNNs, and expressing constituency information is generally thought to be the primary motivation for using trees in TreeRNNs.
",1 Introduction,[0],[0]
"In this paper, we reimplement the latent tree learning models of Yogatama et al. (2017) and Choi et al. (2018) in a shared codebase, train both models (and several baselines) to perform textual entailment on the SNLI and MultiNLI corpora (Bowman et al., 2015; Williams et al., 2017), and evaluate the results quantitatively and qualitatively with a focus on four
issues: the degree to which latent tree learning improves task performance, the degree to which latent tree learning models learn similar grammars across random restarts, the degree to which their grammars match PTB grammar, and the degree to which their grammars appear to follow any recognizable grammatical principles.
",1 Introduction,[0],[0]
"We confirm that both types of model succeed at producing usable sentence representations, but find that only the stronger of the two models—that of Choi et al. (2018)—outperforms either a comparable TreeRNN baseline or a simple LSTM RNN.",1 Introduction,[0],[0]
"We find that the grammar of the Choi et al. model varies dramatically across random restarts, and tends to agree with PTB grammar with roughly chance accuracy.",1 Introduction,[0],[0]
"We do find, though, that the resulting grammar has some regularities, including a preference for shallow trees, a somewhat systematic treatment of negation, and a preference to treat pairs of adjacent words at the edges of a sentence as constituents.",1 Introduction,[0],[0]
"The work discussed in this paper is closely related to work on grammar induction, in which statistical learners attempt to solve the difficult problem of reconstructing the grammar that generated a corpus of text using only that corpus and, optionally, some heuristics about the nature of the expected grammar.",2 Background,[0],[0]
"Grammar induction in NLP has been widely studied since at least the mid-1990s (Chen, 1995; Cohen et al., 2011; Hsu et al., 2012), and builds on a earlier line of more general work in machine learning surveyed in Duda et al. (1973) and Fu (1977).",2 Background,[0],[0]
"One work in this area, Naseem and Barzilay (2011), additionally provides some semantic information to the learner, though only as a source of additional guidance, rather than as a primary objective as here.",2 Background,[0],[0]
"In related work, Gormley et al. (2014) present a method for jointly training a grammar induction model and a semantic role labeling (SRL) model.",2 Background,[0],[0]
"They find that the resulting SRL model is more effective than one built on a purely unsupervised grammar induction system, but that using a conventionally trained parser instead yields better SRL performance.
",2 Background,[0],[0]
"There is also a long history of work on artificial neural network models that build latent hierarchical structures without direct supervision when solving
algorithmic problems, including Das et al. (1992), Sun et al. (1993), and more recently, Joulin and Mikolov (2015) and Grefenstette et al. (2015).
",2 Background,[0],[0]
We are only aware of four prior works on latent tree learning for sentence understanding with neural networks.,2 Background,[0],[0]
"All four jointly train two model components—a parser based on distributed representations of words and phrases, and a TreeRNN of some kind that uses those parses—but differ in the parsing strategies, TreeRNN parameterizations, and training objective used.
",2 Background,[0],[0]
Socher et al. (2011) present the first neural network model that we are aware of that use the same learned representations to both parse a sentence and—using the resulting parse in a TreeRNN— perform sentence-level classification.,2 Background,[0],[0]
They use a plain TreeRNN and a simple parser that scores pairs of adjacent words and phrases and merges the highest-scoring pair.,2 Background,[0],[0]
"They train their model on a sentiment objective, but rather than training the parsing component on that objective as well, they use a combination of an auxiliary autoencoding objective and a nonparametric scoring function to parse.",2 Background,[0],[0]
"While this work shows good results on sentiment, it does not feature any evaluation of the induced trees, either through direct analysis nor through comparison with any sentiment baseline that uses trees from a conventionally-trained parser.",2 Background,[0],[0]
"Bowman et al. (2016) introduce an efficient, batchable architecture for doing this—the Shift-reduce ParserInterpreter Neural Network (SPINN; Figure 2)— which is adapted by Yogatama et al. (2017) for latent tree learning and used in this work.
",2 Background,[0],[0]
The remaining three models all use TreeLSTMs,2 Background,[0],[0]
"(Tai et al., 2015), and all train and evaluate both components on a shared semantic objective.",2 Background,[0],[0]
"All three use the task of recognizing textual entailment on the SNLI corpus (Bowman et al., 2015) as one such objective.",2 Background,[0],[0]
"The models differ from one another primarily in the ways in which they use this task objective to train their parsing components, and in the structures of those components.
",2 Background,[0],[0]
"Yogatama et al. (2017) present a model (which we call RL-SPINN) that is identical to SPINN at test time, but uses the REINFORCE algorithm (Williams, 1992) at training time to compute gradients for the transition classification function, which produces discrete decisions and does not otherwise
receive gradients through backpropagation.",2 Background,[0],[0]
"Surprisingly, and in contrast to Gormley et al., they find that a small 100D instance of this RL-SPINN model performs somewhat better on several text classification tasks than an otherwise-identical model which is explicitly trained to parse.
",2 Background,[0],[0]
"In unpublished work, Maillard et al. (2017) present a model which explicitly computes O(N2) possible tree nodes for N words, and uses a soft gating strategy to approximately select valid combinations of these nodes that form a tree.",2 Background,[0],[0]
"This model is trainable entirely using backpropagation, and a 100D instance of the model performs slightly better than RL-SPINN on SNLI.
",2 Background,[0],[0]
"Choi et al. (2018) present a model (which we call ST-Gumbel) that uses a similar data structure and gating strategy to Maillard et al., but which uses the Straight-Through Gumbel-Softmax estimator (Jang et al., 2016).",2 Background,[0],[0]
"This allows them to use a hard categorical gating function, so that their output sentence vector is computed according to a single tree, rather than a gated combination of partial trees as in Maillard et al.. They report substantial gains in both speed and accuracy over Maillard et al. and Yogatama et al. on SNLI.
",2 Background,[0],[0]
"Several models (Naradowsky et al., 2012; Kim et al., 2017; Liu and Lapata, 2017; Munkhdalai and Yu, 2017a) have also been proposed that can induce latent dependency trees over text using mechanisms like attention, but do not propagate information up the trees as in typical compositional models.",2 Background,[0],[0]
"Other models like that of Chung et al. (2017) induce and use latent trees or tree-like structures, but constrain these structures to be of a low fixed depth.
",2 Background,[0],[0]
Other past work has also investigated the degree to which neural network models for non-syntactic tasks implicitly learn syntactic information.,2 Background,[0],[0]
Recent highlights from this work include Linzen et al. (2016) on language modeling and Belinkov et al. (2017) on translation.,2 Background,[0],[0]
"Also on translation, Neubig et al. (2012) and DeNero and Uszkoreit (2011) present methods that use aligned sentences from bilingual parallel text to learn a binary constituency parser for use in word reordering.",2 Background,[0],[0]
"These two papers do not evaluate these parsers on typical parsing metrics, but find that the parsers support word reorderings that in turn yield improvements in translation quality, suggesting that they do capture some notion
of syntactic constituency.",2 Background,[0],[0]
This paper investigates the behavior of two models: RL-SPINN and ST-Gumbel.,3 Models and Methods,[0],[0]
"Both have been shown to outperform similar models based on supervised parsing, and the two represent substantially different approaches to latent tree learning.
",3 Models and Methods,[0],[0]
SPINN Variants Three of our of our baselines and one of the two latent tree learning models are based on the SPINN architecture of Bowman et al. (2016).,3 Models and Methods,[0],[0]
"Figure 2 shows and describes the architecture.
",3 Models and Methods,[0],[0]
"In the base SPINN model, all model components are used, and the transition classifier is trained on binarized Penn Treebank-style parses from the Stanford PCFG Parser (Klein and Manning, 2003), which are included with SNLI and MultiNLI.",3 Models and Methods,[0],[0]
"These binary-branching parse trees are converted to SHIFT/REDUCE sequences for use in the model through a simple reversible transformation.
",3 Models and Methods,[0],[0]
"RL-SPINN, based on the unsupervised syntax model of Yogatama et al. (2017), is architecturally equivalent to SPINN, but its transition classifier is optimized for MultiNLI classification accuracy, rather than any parsing-related loss.",3 Models and Methods,[0],[0]
"Because this component produces discrete decisions, the REINFORCE algorithm (with the standard exponential moving average baseline) is used to supply gradients for it.",3 Models and Methods,[0],[0]
We explored several alternative baseline strategies—including parametric value networks and strategies based on greedy decoding— as well as additional strategies for increasing exploration.,3 Models and Methods,[0],[0]
We also thoroughly tuned the relevant hyperparameter values for each alternative.,3 Models and Methods,[0],[0]
"In all of these experiments, we found that a standard implementation with the exponential moving average baseline produces accuracy no worse than any readily available alternative.
",3 Models and Methods,[0],[0]
We also evaluate two other variants of SPINN as baselines.,3 Models and Methods,[0],[0]
"In SPINN-NC (for No Connection from tracking to composition), the connection from the tracking LSTM to the composition function is severed.",3 Models and Methods,[0],[0]
"This weakens the model, but makes it exactly equivalent to a plain TreeLSTM—it will produce the exact same vector that a TreeLSTM with the same composition function would have produced for the tree that the transition classifier implicitly pro-
duces.",3 Models and Methods,[0],[0]
"This model serves as a maximally comparable baseline for the ST-Gumbel model, which also performs composition using a standard TreeLSTM in forward-propagation.
",3 Models and Methods,[0],[0]
"SPINN-PI-NT (for Parsed Input, No Tracking) removes the tracking LSTM, as well as the two components that depend on it: the tracking-composition connection and the transition decision function.",3 Models and Methods,[0],[0]
"As such, it cannot produce its own parse trees and must rely on trees from the input data.",3 Models and Methods,[0],[0]
"We include this in our comparison to understand the degree to which training a parser, rather than using a higher-quality off-the-shelf parser, impacts performance on our semantic task.
",3 Models and Methods,[0],[0]
ST-Gumbel The ST-Gumbel model was developed by Choi et al. (2018) and is shown in Figure 3.,3 Models and Methods,[0],[0]
The model takes a sequence of N − 1 steps to build a tree over N words.,3 Models and Methods,[0],[0]
"At every step, every possible pair of adjacent words or phrase vectors in the partial tree is given to a TreeLSTM composition function to produce a new candidate phrase vector.",3 Models and Methods,[0],[0]
"A simple learned scoring function then selects the best of these candidates, which forms a constituent node in the tree and replaces its two children in the list of nodes that are available to compose.",3 Models and Methods,[0],[0]
"This repeats until only two nodes remain, at which point they are composed and the tree is complete.",3 Models and Methods,[0],[0]
"This exhaustive search increases the computational complexity of the model over (RL-)SPINN, but also allows the model to perform a form of easy-first parsing, making it easier for the model to explore the space of possible parsing strategies.
",3 Models and Methods,[0],[0]
"Though the scoring function yields discrete decisions, the Jang et al. Straight-Through GumbelSoftmax estimator makes it possible to nonetheless efficiently compute an approximate gradient for the full model without the need for relatively brittle policy gradient techniques like REINFORCE.
",3 Models and Methods,[0],[0]
Other Baselines We also train three baselines that do not depend on a parser.,3 Models and Methods,[0],[0]
The first is a unidirectional LSTM RNN over the embedded tokens.,3 Models and Methods,[0],[0]
"The second is a version of SPINN-PI-NT that is supplied sequences of randomly sampled legal transitions (corresponding to random parse trees), rather than the output of a parser.",3 Models and Methods,[0],[0]
"The third is also a version of SPINN-PI-NT, and receives transition sequences corresponding to maximally-
shallow, approximately-balanced parse trees based on the “full binary” trees used in Munkhdalai and Yu (2017b).
",3 Models and Methods,[0],[0]
"Data To ensure that we are able to roughly reproduce the results reported by Yogatama et al. and Choi et al., we conduct an initial set of experiments on the Stanford NLI corpus of Bowman et al. (2015).",3 Models and Methods,[0],[0]
"Our primary experiments use the newer Multi-Genre Natural Language Inference Corpus (MultiNLI; Williams et al., 2017).",3 Models and Methods,[0],[0]
"MultiNLI is a 433k-example textual entailment dataset created in the style of SNLI, but with a more diverse range
of source texts and longer, more complex sentences, which we expect will encourage the models to produce more consistent and interpretable trees than they otherwise might.",3 Models and Methods,[0],[0]
"Following Williams et al., we train on the combination of MultiNLI and SNLI in these experiments (yielding just under 1M training examples) and evaluate on MultiNLI (using the matched development and test sets).
",3 Models and Methods,[0],[0]
"We also evaluate trained models on the full Wall Street Journal section of the Penn Treebank, a seminal corpus of manually-constructed constituency parses, which introduced the parsing standard used in this work.",3 Models and Methods,[0],[0]
"Because the models under study produce and consume binary-branching constituency trees without labels (and because such trees are already included with SNLI and MultiNLI), we use the Stanford Parser’s CollapseUnaryTransformer and TreeBinarizer tools to convert these Penn Treebank Trees to this form.
",3 Models and Methods,[0],[0]
"Sentence Pair Classification Because our textual entailment task requires a model to classify pairs of sentences, but the models under study produce vectors for single sentences, we concatenate the two sentence vectors, their difference, and their elementwise product (following Mou et al., 2016), and feed the result into a 1024D ReLU layer to produce a representation for the sentence pair.",3 Models and Methods,[0],[0]
"This representation is fed into a three-way softmax classifier that selects
one of the labels entailment, neutral, and contradiction for the pair.
",3 Models and Methods,[0],[0]
Additional Details We implement all models in PyTorch 0.2.,3 Models and Methods,[0],[0]
"We closely follow the original Theano code for SPINN in our implementation, and we incorporate source code provided by Choi et al. for the core parsing data structures and sampling mechanism of the ST-Gumbel model.",3 Models and Methods,[0],[0]
"Our code, saved models, and model output are available on GitHub.1
We use GloVe vectors to represent words (standard 300D, 840B word package, without fine tuning; Pennington et al., 2014), and feed them into a singlelayer 2",3 Models and Methods,[0],[0]
× 300D bi-directional GRU RNN (based on the leaf LSTM of Choi et al.),3 Models and Methods,[0],[0]
to give the models access to local context information when making parsing decisions.,3 Models and Methods,[0],[0]
"To understand the impact of this component, we follow Choi et al. in also training each model with the leaf GRU replaced with a simpler context-insensitive input encoder that simply multiplies each GloVe vector by a matrix.",3 Models and Methods,[0],[0]
"We find that these models perform best when the temperature of the ST-Gumbel distribution is a trained parameter, rather than fixed at 1.0 as in Choi et al..
We use L2 regularization and apply dropout (Srivastava et al., 2014) to the input of the 1024D sentence pair combination layer.",3 Models and Methods,[0],[0]
"We train all models using the Adam optimizer (Kingma and Ba, 2015).",3 Models and Methods,[0],[0]
"For hyperparameters for which no obvious default value exists—the L2 and dropout parameters, the relative weighting of the gradients from REINFORCE in RL-SPINN, the starting learning rate, and the size of the tracking LSTM state in SPINN—we heuristically select ranges in which usable values can be found (focusing on MultiNLI development set performance), and then randomly sample values from those ranges.",3 Models and Methods,[0],[0]
We train each model five times using different samples from those ranges and different random initializations for model parameters.,3 Models and Methods,[0],[0]
We use early stopping based on development set performance with all models.,3 Models and Methods,[0],[0]
"Table 1 shows the accuracy of all models on two test sets: SNLI (training on SNLI only, for comparison
1 https://github.com/nyu-mll/spinn/tree/is-it-syntax-release
with prior work), and MultiNLI (training on both datasets).",4 Does latent tree learning help sentence understanding?,[0],[0]
"Each figure represents the accuracy of the best run, selected using the development set, of five runs with different random initializations and hyperparameter values.",4 Does latent tree learning help sentence understanding?,[0],[0]
"Our LSTM baseline is strikingly effective, and matches or exceeds the performance of all of our PTB grammar-based tree-structured models on both SNLI and MultiNLI.",300D SPINN 67.1 (1.0) 68.3 71.5,[0],[0]
"This contradicts the primary result of Bowman et al. (2016), and suggests that there is little value in using the correct syntactic structure for a sentence to guide neural network composition, at least in the context of the TreeLSTM composition function and the NLI task.
",300D SPINN 67.1 (1.0) 68.3 71.5,[0],[0]
"We do, however, reproduce the key result of Choi et al. on both datasets.",300D SPINN 67.1 (1.0) 68.3 71.5,[0],[0]
"Their ST-Gumbel model, which receives no syntactic information at training time, outperforms SPINN-NC, which performs composition in an identical manner but is explicitly trained to parse, and also outperforms the LSTM baseline.",300D SPINN 67.1 (1.0) 68.3 71.5,[0],[0]
"This suggests that the learned latent trees are helpful in the construction of semantic representations for sentences, whether or not they resemble conventional parse trees.
",300D SPINN 67.1 (1.0) 68.3 71.5,[0],[0]
Our results with RL-SPINN are more equivocal.,300D SPINN 67.1 (1.0) 68.3 71.5,[0],[0]
"That model matches, but does not beat, the performance of the full SPINN model, which is equivalent except that it is trained to parse.",300D SPINN 67.1 (1.0) 68.3 71.5,[0],[0]
"However, our implementation of RL-SPINN outperforms Yogatama et al.’s (lower-dimensional) implementation by a substantial margin.",300D SPINN 67.1 (1.0) 68.3 71.5,[0],[0]
"The impact of the leaf GRU is sometimes substantial, but the direction of its effect is not consistent.
",300D SPINN 67.1 (1.0) 68.3 71.5,[0],[0]
"Our results with SPINN-PI-NT are not substantially better than those with any other model, suggesting the relatively simple greedy parsing strategies used by the other models are not a major limiting factor in their performance.",300D SPINN 67.1 (1.0) 68.3 71.5,[0],[0]
"Balanced trees consistently outperform randomly sampled transitions (albeit by a small margin), yet perform worse than ST-Gumbel even though ST-Gumbel uses very shallow trees as well.",300D SPINN 67.1 (1.0) 68.3 71.5,[0],[0]
"Similarly, RL-SPINN depends on mostly left-branching binary parse trees, but is outperformed by a forward LSTM.",300D SPINN 67.1 (1.0) 68.3 71.5,[0],[0]
"Structure is important, but there are differences between the architectures of compositional models worth investigating in future work.
",300D SPINN 67.1 (1.0) 68.3 71.5,[0],[0]
"None of our latent tree learning models reach the state of the art on either task, but all are comparable in both absolute and relative performance to other published results, suggesting that we have trained reasonable examples of latent tree learning models and can draw informative conclusions by studying the behaviors of these models.",300D SPINN 67.1 (1.0) 68.3 71.5,[0],[0]
"If it were the case that a latent tree learning model outperforms its baselines by identifying some specific grammar for English that is better than the one used in PTB and the Stanford Parser, then we would expect these models to identify roughly the same grammar across random restarts and minor configuration changes, and to use that grammar to produce consistent task performance.",5 Are these models consistent?,[0],[0]
"Table 2 shows two measures of consistency for the four models that produce parses, a simple random baseline that produces parses by randomly merging pairs of adjacent words and phrases, and (trivially) the deterministic strategy used in the Balanced Trees runs.
",5 Are these models consistent?,[0],[0]
We first show the variation in accuracy on the MultiNLI development set across runs.,5 Are these models consistent?,[0],[0]
"While one outlier distorts these numbers for ST-Gumbel without the leaf GRU, these figures are roughly equivalent between the latent tree learning models and the baselines, suggesting that these models are not substantially more brittle or more hyperparameter sensitive in their task performance.",5 Are these models consistent?,[0],[0]
"The second metric shows the self F1 for each model: the unlabeled binary F1 between the parses produced by two runs of the same model for the MultiNLI development
set, averaged out over all possible pairings of different runs.",5 Are these models consistent?,[0],[0]
"This measures the degree to which the models reliably converge on the same parses, and sheds some light on the behavior of the models.",5 Are these models consistent?,[0],[0]
"The baseline models show relatively high consistency, with self F1 above 65%.",5 Are these models consistent?,[0],[0]
"ST-Gumbel is substantially less consistent, with scores below 50% but above the 32.6% random baseline.",5 Are these models consistent?,[0],[0]
"RL-SPINN appears to be highly consistent, with the runs without the leaf GRU reaching 98.5% self F1, suggesting that it reliably converges to a specific grammar.",5 Are these models consistent?,[0],[0]
"However, as we will discuss in later sections, this grammar appears to be trivial.",5 Are these models consistent?,[0],[0]
"Given that our latent tree learning models are at least somewhat consistent in what they learn, it is reasonable to ask what it is that they learn.",6 Do these models learn PTB grammar?,[0],[0]
"We investigate this first quantitatively, then, in the next section, more qualitatively.
",6 Do these models learn PTB grammar?,[0],[0]
Table 3 shows parsing performance on the Wall Street Journal sections of PTB for models trained on SNLI and MultiNLI.,6 Do these models learn PTB grammar?,[0],[0]
"The baseline models perform fairly poorly in absolute terms, as they are neither well tuned for parse quality nor trained on news text, but the latent tree learning models perform dramatically worse.",6 Do these models learn PTB grammar?,[0],[0]
"The ST-Gumbel models perform at or slightly above chance (represented by the random trees results), while the RL-SPINN models per-
form consistently below chance.",6 Do these models learn PTB grammar?,[0],[0]
"These results suggest that these models do not learn grammars that in any way resemble PTB grammar.
",6 Do these models learn PTB grammar?,[0],[0]
"To confirm this, we also show results for individual Penn Treebank nonterminal node types.",6 Do these models learn PTB grammar?,[0],[0]
"On common intermediate node types such as ADJP, NP, and PP, the latent tree learning models do not perform substantially better than chance.",6 Do these models learn PTB grammar?,[0],[0]
"It is only on a two rare types that any latent tree learning model, or the balanced tree baseline, outperforms random trees by a significant margin: INTJ (interjection, as in Oh no, he ’s...) and the even rarer LST (list marker, as in 1 .",6 Do these models learn PTB grammar?,[0],[0]
"Determine if...), both of which are generally short and sentence-initial (discussed in more detail in Section 7).
",6 Do these models learn PTB grammar?,[0],[0]
"Next, we turn to the MultiNLI development set for further investigation.",6 Do these models learn PTB grammar?,[0],[0]
Table 4 shows results on MultiNLI for a wider range of measures.,6 Do these models learn PTB grammar?,[0],[0]
"The table shows F1 measured with respect to three different references: automatically generated trivial trees for the corpus that are either strictly left-branching or strictly right-branching, and the PTB-style trees produced by the Stanford Parser for the corpus.",6 Do these models learn PTB grammar?,[0],[0]
"We see that the baseline models perform about as well on MultiNLI as on PTB, with scores above 65%, and that these models produce trees that tend toward right branching rather than left branching.
",6 Do these models learn PTB grammar?,[0],[0]
"The ST-Gumbel models perform only at or slightly above chance on the parsed sentences, and
show a similar use of both right- and left-branching structures, with only a slight preference for the more linguistically common right-branching structures.",6 Do these models learn PTB grammar?,[0],[0]
"This suggests that they learn grammars that differ quite substantially from PTB grammars, but may share some minor properties.",6 Do these models learn PTB grammar?,[0],[0]
"Our implementation of an ST-Gumbel model has F1 scores with respect to left-branching and Stanford Parser trees that are much closer to the ones Yogatama et al. report than to the ones we find for our RL-SPINN implementation.
",6 Do these models learn PTB grammar?,[0],[0]
Our RL-SPINN results are unequivocally negative.,6 Do these models learn PTB grammar?,[0],[0]
"We find that our models could be tuned to produce trees that are qualitatively similar to those in Yogatama et al.. However, in our primary experiments, we tune model hyperparameters with the sole criterion of downstream task performance, and find that the trees from these experiments yield relactively trivial trees, with F1 scores that are much lower than theirs with respect to the Stanford Parser, and much higher with respect to left branching trees (see Table 4).",6 Do these models learn PTB grammar?,[0],[0]
"All runs perform below chance on the parsed sentences, and all have F1 scores over 92% with respect to the left-branching structures, suggesting that they primarily learn to produce strictly left-branching trees.",6 Do these models learn PTB grammar?,[0],[0]
"This trivial strategy, which
makes the model roughly equivalent to a sequential RNN, is very easy to learn.",6 Do these models learn PTB grammar?,[0],[0]
"In a shift–reduce model like SPINN, the model can simply learn to perform the REDUCE operation whenever it is possible to do so, regardless of the specific words and phrases being parsed.",6 Do these models learn PTB grammar?,[0],[0]
"This can be done by setting a high bias value for this choice in the transition classifier.
",6 Do these models learn PTB grammar?,[0],[0]
The rightmost column shows another measure of what is learned: the average depth—the length of the path from the root to any given word—of the induced trees.,6 Do these models learn PTB grammar?,[0],[0]
"For the baseline models, this value is slightly above the 5.7 value for the Stanford Parser trees.",6 Do these models learn PTB grammar?,[0],[0]
"For the RL-SPINN models, this number is predictably much higher, reflecting the very deep and narrow left-branching trees that those models tend to produce.",6 Do these models learn PTB grammar?,[0],[0]
"For the ST-Gumbel model, though, this metric is informative: the models consistently produce shallow trees with depth under 5—closer to the balanced trees baseline than to SPINN.",6 Do these models learn PTB grammar?,[0],[0]
"This hints at a possible interpretation: While shallower trees may be less informative about the structure of the sentence than real PTB trees, they reduce the number of layers that a word needs to pass through to reach the final classifier, potentially making it easier to learn an effective composition function that faithfully encodes the contents of a sentence.",6 Do these models learn PTB grammar?,[0],[0]
"This interpretation
is supported by the results of Munkhdalai and Yu (2017b), who show that it is possible to do well on SNLI using a TreeLSTM (with a leaf LSTM) over arbitrarily chosen balanced trees with low depths, and our balanced trees baseline, which approximates this result.
",6 Do these models learn PTB grammar?,[0],[0]
The ST-Gumbel models tend to implement their shallow parsing strategy with a good deal of randomness.,6 Do these models learn PTB grammar?,[0],[0]
"They tend to assign near-zero probability (< 10−10) to many possible compositions, generally those that would result in unnecessarily deep trees, and relatively smooth probabilities (generally > 0.01) to the remaining options.",6 Do these models learn PTB grammar?,[0],[0]
"The trainable temperature parameter for these models generally converged slowly to a value between 1 and 20, and did not fluctuate substantially during training.",6 Do these models learn PTB grammar?,[0],[0]
"In the previous three sections, we have shown that latent tree learning models are able to perform as well or better than models that have access to linguistically principled parse trees at training or test time, but that the grammars that they learn are neither consistent across runs, nor meaningfully similar to PTB grammar.",7 Analyzing the Learned Trees,[0],[0]
"In this section, we investigate the trees produced by these learned grammars directly to identify whether they capture any recognizable syntactic or semantic phenomena.
",7 Analyzing the Learned Trees,[0],[0]
The RL-SPINN models create overwhelmingly left-branching trees.,7 Analyzing the Learned Trees,[0],[0]
"We observe few deviations from this pattern, which occur almost exclusively on sentences with fewer than seven words.",7 Analyzing the Learned Trees,[0],[0]
"Given that the self-F1 scores for these models (92.7 and 98.5, Table 2) are similar to their F1 scores with respect to strictly left-branching trees (95.0 and 99.1, Table 4), there is little room for these models to have learned any consistent behavior beside left branching.
",7 Analyzing the Learned Trees,[0],[0]
"In some preliminary tuning runs not shown above, we saw models that deviated from this pattern more often, and one that fixated on right-branching structures instead, but we find no grammatically interesting patterns in any of these deviant structures.
",7 Analyzing the Learned Trees,[0],[0]
"The ST-Gumbel models learned substantially more complex grammars, and we focus on these for the remainder of the section.",7 Analyzing the Learned Trees,[0],[0]
We discuss three model behaviors which yield linguistically implausible constituents.,7 Analyzing the Learned Trees,[0],[0]
"The first two highlight settings
where the ST-Gumbel model is consistent where it shouldn’t be, and the third highlights a setting in which it is worryingly inconsistent.",7 Analyzing the Learned Trees,[0],[0]
"The models’ treatment of these three phenomena and our observation of these models’ behavior more broadly suggest that the models do not produce trees that follow any recognizable semantic or syntactic principles.
",7 Analyzing the Learned Trees,[0],[0]
Initial and Final Two-Word Constituents The shallow trees produced by the ST-Gumbel models generally contain more constituents comprised of two words (rather than a single word combined with a phrase) than appear in the reference parses.,7 Analyzing the Learned Trees,[0],[0]
"This behavior is especially pronounced at the edges of sentences, where the models frequently treat the first two words and the last two words as constituents.",7 Analyzing the Learned Trees,[0],[0]
"Since this behavior does not correspond to any grammatical phenomenon known to these authors, it likely stems from some unknown bias within the model design.
",7 Analyzing the Learned Trees,[0],[0]
These models parse the first two words of a sentence into a constituent at rates well above both the 50% rate seen in random and balanced parses2 and the 27.7% rate seen with the SPINN models.,7 Analyzing the Learned Trees,[0],[0]
This strategy appeares in 77.4% of the model’s parses with the leaf GRU and 64.1% without.,7 Analyzing the Learned Trees,[0],[0]
"While it was consistently discovered across all of our runs of STGumbel models with the leaf GRU, it was discovered less frequently across restarts for runs without, which do not have direct access to linear position information.",7 Analyzing the Learned Trees,[0],[0]
"We observe that the models combine the final two words in each sentence at similar rates.
",7 Analyzing the Learned Trees,[0],[0]
"While merging the final two words of a sentence nearly always results in a meaningless constituent containing a period or punctuation mark, merging
2The balanced parses are right-aligned, following Munkhdalai and Yu; they parse the first two words as a constituent in about 50% of cases, but the final two words in all cases.
",7 Analyzing the Learned Trees,[0],[0]
the first two words can produce reasonable parses.,7 Analyzing the Learned Trees,[0],[0]
"This strategy is reasonable, for example, in sentences that begin with a determiner and a noun (Figure 4, top left).",7 Analyzing the Learned Trees,[0],[0]
"However, combining the first two words in sentences that start with adverbials, proper names, bare plurals, or noun phrases with multiple modifiers will generally result in meaningless constituents like Kings frequently (Figure 4, bottom).
",7 Analyzing the Learned Trees,[0],[0]
Combining the first two words of a sentence also often results in more subtly unorthodox trees—like the one in the top right of Figure 4—that combine a verb with its subject rather than its object.,7 Analyzing the Learned Trees,[0],[0]
"This contrasts with some mainstream syntactic theories (Adger 2003; Sportiche et al. 2013), which generally take the object and the verb of a sentence to form a constituent for three reasons: Taking the top right sentence in Figure 4 as an example, (i) we can replace it with a new constituent of the same type without changing the surrounding sentence structure, as in he did so, (ii) it can stand alone as an answer to a question like what did he do?, and (iii) it can be omitted in otherwise-repetitive sentences like He shot his gun, but Bill didn’t .
",7 Analyzing the Learned Trees,[0],[0]
"Negation The ST-Gumbel models also tend to learn a systematic and superficially reasonable strategy for negation: they pair any negation word (e.g., not, n’t, no, none) with the word that immediately follows it.",7 Analyzing the Learned Trees,[0],[0]
"Random parses only form these constituents in 34% of the sentences, and balanced parses only do so in 50%, but the ST-Gumbel models with the leaf GRU do so about 67% of the time and consistently across runs, while those without the leaf GRU do so less consistently, but over 90% of the time in some runs.
",7 Analyzing the Learned Trees,[0],[0]
"This strategy is effective when the negation word is meant to modify a single other word to its right, as in Figure 5, top left sub-figure, but this is frequently not the case.",7 Analyzing the Learned Trees,[0],[0]
"In Figure 5, bottom left, although the model creates the potentially reasonable constituent, not at all, it also combines not with the preposition at to form a constituent with no clear interpretation (or, at best, an incredibly bizarre one).",7 Analyzing the Learned Trees,[0],[0]
"Further, combining not with at goes contra the syntactic observation that prepositional phrases can generally move along with the following noun phrases as a constituent (as in semantically comparably sentences like, He is not sure at all.).
",7 Analyzing the Learned Trees,[0],[0]
Function Words and Modifiers,7 Analyzing the Learned Trees,[0],[0]
"Finally, the STGumbel models are not consistent in their treatment of function words, like determiners or prepositions, or in their treatment of modifiers like adverbs and adjectives.",7 Analyzing the Learned Trees,[0],[0]
This reflects quantitative results in Table 3 showing that ST-Gumbel parse trees correspond to PTB for PP and ADJP constituents at much lower rates than do SPINN-based models or models supplied with random trees.,7 Analyzing the Learned Trees,[0],[0]
"For example, the top left tree of Figure 6 (ST-Gumbel) associated the determiner the with the verb, when it should form a constituent with the noun phrase Nazi angle as in the top right tree (PTB).",7 Analyzing the Learned Trees,[0],[0]
"The resulting phrase, the Nazi angle, has a clear meaning—unlike discussed the—and it passes syntactic tests for constituency; for example, one can replace the noun phrase with the pronoun it without otherwise modifying the meaning of the sentence.
",7 Analyzing the Learned Trees,[0],[0]
"Similarly, prepositions are generally expected to form constituents with the noun phrases that follow them (Adger, 2003; Sportiche et al., 2013), as in the the bottom right tree (PTB) of Figure 6.",7 Analyzing the Learned Trees,[0],[0]
"One syntactic test that with horror forms a P-NP constituent comes from the fact that it can be a stand-alone answer to a question; for example, the question how did the students react?",7 Analyzing the Learned Trees,[0],[0]
can be answered simply with with horror.,7 Analyzing the Learned Trees,[0],[0]
"ST-Gumbel models often instead pair prepositions with the verb phrases that precede them, as in Figure 6, lower left, where this results in the constituent the students acted with, which cannot be a stand-alone answer to a question.",7 Analyzing the Learned Trees,[0],[0]
"From this perspective, constituents like discussed the and we briefly (Figure 6, top left) are also syntactically anomalous, and cannot be given coherent meanings.
",7 Analyzing the Learned Trees,[0],[0]
"The ST-Gumbel models outperform syntax-based models on MultiNLI and SNLI, and the trees that they assign to sentences do not generally resemble
those of PTB grammar.",7 Analyzing the Learned Trees,[0],[0]
"If we attempt to interpret these trees under the standard assumption that all the constituents in a sentence must be interpretable and must contribute to the meaning of the sentence, we force ourselves to interpret implausible constituents like we briefly, and reach implausible sentence-level interpretations, such as taking the sentence in Figure 6, top left, to mean that those of us who are brief discussed the Nazi angle.",7 Analyzing the Learned Trees,[0],[0]
"It is clear that these models do not use constituency in the same way as the widely accepted syntactic or semantic frameworks we cite do.
",7 Analyzing the Learned Trees,[0],[0]
"In sum, we find that RL-SPINN adopts a trivial, largely left-branching parse strategy, which is consistent across runs.",7 Analyzing the Learned Trees,[0],[0]
"ST-Gumbel, on the other hand, adopts the unexpected strategy to merge initial and final constituents at higher than average rates, and is also very inconsistent with its behavior on function words and modifiers.",7 Analyzing the Learned Trees,[0],[0]
"We weren’t able to qualitatively identify structure that matches PTB-style syntax in ST-Gumbel parses, but we do find that it utilizes a strategy for negation—merging it with the immediately following constituent—that can lead to unexpected constituents, but nevertheless, is somewhat promising.",7 Analyzing the Learned Trees,[0],[0]
The experiments and analysis presented in this paper show that the best available models for latent tree learning learn grammars that do not correspond to the structures of formal syntax and semantics in any recognizable way.,8 Conclusion,[0],[0]
"In spite of this, these models perform as well or better on sentence understanding— as measured by MultiNLI performance—as models with access to Penn Treebank-style parses.
",8 Conclusion,[0],[0]
"This result leaves us with an immediate puzzle: What do these models—especially those based on the ST-Gumbel technique—learn that allows them
to do so well?",8 Conclusion,[0],[0]
"We present some observations, but we are left without a fully satisfying explanation.",8 Conclusion,[0],[0]
"A thorough investigation of this problem will likely require a search of new architectures for sentence encoding that borrow various behaviors from the models trained in this work.
",8 Conclusion,[0],[0]
This result also opens farther-reaching questions about grammar and sentence understanding: Will the optimal grammars for sentence understanding problems like NLI—were we to explore the full space of grammars to find them—share any recognizable similarities with the structures seen in formal work on syntax and semantics?,8 Conclusion,[0],[0]
"A priori, we should expect that they should.",8 Conclusion,[0],[0]
"While it is unlikely that PTB grammar is strictly optimal for any task, the empirical motivations for many of its core constituent types—the noun phrase, the prepositional phrase, and so forth—are straightforward and compelling.",8 Conclusion,[0],[0]
"However, our best latent tree learning models do not seem able to discover these structures.
",8 Conclusion,[0],[0]
"If we accept that some form of principled constituent structure is necessary or desirable, then we are left with an engineering problem: How do we identify this structure?",8 Conclusion,[0],[0]
"Making progress in this direction will likely involve both improvements to the TreeRNN models at the heart of latent tree learning systems, to make sure that these models are able to perform composition effectively enough to be able to make full use of learned structures, and also improvements to the structure search methods that are used to explore possible grammars.",8 Conclusion,[0],[0]
"This project has benefited from financial support to SB by Google, Tencent Holdings, and Samsung Research and from a Titan X Pascal GPU donated by the NVIDIA Corporation to AD.",Acknowledgments,[0],[0]
"Jon Gauthier contributed to early discussions that motivated this work, and he, Nikita Nangia, Kelly Zhang, and Cipta Herwana provided help and advice.",Acknowledgments,[0],[0]
"Recent work on the problem of latent tree learning has made it possible to train neural networks that learn to both parse a sentence and use the resulting parse to interpret the sentence, all without exposure to groundtruth parse trees at training time.",abstractText,[0],[0]
"Surprisingly, these models often perform better at sentence understanding tasks than models that use parse trees from conventional parsers.",abstractText,[0],[0]
This paper aims to investigate what these latent tree learning models learn.,abstractText,[0],[0]
"We replicate two such models in a shared codebase and find that (i) only one of these models outperforms conventional tree-structured models on sentence classification, (ii) its parsing strategies are not especially consistent across random restarts, (iii) the parses it produces tend to be shallower than standard Penn Treebank (PTB) parses, and (iv) they do not resemble those of PTB or any other semantic or syntactic formalism that the authors are aware of.",abstractText,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1722–1732, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.
We then test the performance of our model on part-of-speech tagging, named entity recognition, sentiment analysis, semantic relation identification and semantic relatedness, controlling for embedding dimensionality. We find that multi-sense embeddings do improve performance on some tasks (part-of-speech tagging, semantic relation identification, semantic relatedness) but not on others (named entity recognition, various forms of sentiment analysis). We discuss how these differences may be caused by the different role of word sense information in each of the tasks. The results highlight the importance of testing embedding models in real applications.",text,[0],[0]
Enriching vector models of word meaning so they can represent multiple word senses per word type seems to offer the potential to improve many language understanding tasks.,1 Introduction,[0],[0]
"Most traditional embedding models associate each word
type with a single embedding (e.g., Bengio et al. (2006)).",1 Introduction,[0],[0]
Thus the embedding for homonymous words like bank (with senses including ‘sloping land’ and ‘financial institution’) is forced to represent some uneasy central tendency between the various meanings.,1 Introduction,[0],[0]
"More fine-grained embeddings that represent more natural regions in semantic space could thus improve language understanding.
",1 Introduction,[0],[0]
"Early research pointed out that embeddings could model aspects of word sense (Kintsch, 2001) and recent research has proposed a number of models that represent each word type by different senses, each sense associated with a sensespecific embedding (Kintsch, 2001; Reisinger and Mooney, 2010; Neelakantan et al., 2014; Huang et al., 2012; Chen et al., 2014; Pina and Johansson, 2014; Wu and Giles, 2015; Liu et al., 2015).",1 Introduction,[0],[0]
"Such sense-specific embeddings have shown improved performance on simple artificial tasks like matching human word similarity judgments— WS353 (Rubenstein and Goodenough, 1965) or MC30 (Huang et al., 2012).
",1 Introduction,[0],[0]
"Incorporating multisense word embeddings into general NLP tasks requires a pipelined architecture that addresses three major steps:
1.",1 Introduction,[0],[0]
"Sense-specific representation learning: learn word sense specific embeddings from a large corpus, either unsupervised or aided by external resources like WordNet.
2.",1 Introduction,[0],[0]
"Sense induction: given a text unit (a phrase, sentence, document, etc.), infer word senses for its tokens and associate them with corresponding sense-specific embeddings.
3.",1 Introduction,[0],[0]
"Representation acquisition for phrases or sentences: learn representations for text units given sense-specific embeddings and pass them to machine learning classifiers.
",1 Introduction,[0],[0]
"Most existing work on multi-sense embeddings emphasizes the first step by learning sense spe-
1722
cific embeddings, but does not explore the next two steps.",1 Introduction,[0],[0]
"These are important steps, however, since it isn’t clear how existing multi-sense embeddings can be incorporated into and benefit realworld NLU tasks.
",1 Introduction,[0],[0]
"We propose a pipelined architecture to address all three steps and apply it to a variety of NLP tasks: part-of-speech tagging, named entity recognition, sentiment analysis, semantic relation identification and semantic relatedness.",1 Introduction,[0],[0]
"We find:
• Multi-sense embeddings give improved performance in some tasks (e.g., semantic similarity for words and sentences, semantic relation identification part-of-speech tagging), but not others (e.g., sentiment analysis, named entity extraction).",1 Introduction,[0],[0]
"In our analysis we offer some suggested explanations for these differences.
",1 Introduction,[0],[0]
"• Some of the improvements for multi-sense embeddings are no longer visible when using more sophisticated neural models like LSTMs which have more flexibility in filtering away the informational chaff from the wheat.
",1 Introduction,[0],[0]
"• It is important to carefully compare against embeddings of the same dimensionality.
",1 Introduction,[0],[0]
"• When doing so, the most straightforward way to yield better performance on these tasks is just to increase embedding dimensionality.
",1 Introduction,[0],[0]
"After describing related work, we introduce the new unsupervised sense-learning model in section 3, give our sense-induction algorithm in section 4, and then in following sections evaluate its performance for word similarity, and then various NLP tasks.",1 Introduction,[0],[0]
"Neural embedding learning frameworks represent each token with a dense vector representation, optimized through predicting neighboring words or decomposing co-occurrence matrices (Bengio et al., 2006; Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2013; Mikolov et al., 2010; Pennington et al., 2014).",2 Related Work,[0],[0]
"Standard neural models represent each word with a single unique vector representation.
",2 Related Work,[0],[0]
"Recent work has begun to augment the neural paradigm to address the multi-sense problem
by associating each word with a series of sense specific embeddings.",2 Related Work,[0],[0]
"The central idea is to augment standard embedding learning models like skip-grams by disambiguating word senses based on local co-occurrence— e.g., the fruit “apple” tends to co-occur with the words “cider, tree, pear” while the homophonous IT company co-occurs with words like “iphone”, “Google” or “ipod”.
",2 Related Work,[0],[0]
"For example Reisinger and Mooney (2010) and Huang et al. (2012) propose ways to develop multiple embeddings per word type by pre-clustering the contexts of each token to create a fixed number of senses for each word, and then relabeling each word token with the clustered sense before learning embeddings.",2 Related Work,[0],[0]
Neelakantan et al. (2014) extend these models by relaxing the assumption that each word must have a fixed number of senses and using a non-parametric model setting a threshold to decide when a new sense cluster should be split off; Liu et al. (2015) learns sense/topic specific embeddings by combining neural frameworks with LDA topic models.,2 Related Work,[0],[0]
Wu and Giles (2015) disambiguate sense embeddings from Wikipedia by first clustering wiki documents.,2 Related Work,[0],[0]
"Chen et al. (2014) turn to external resources and used a predefined inventory of senses, building a distinct representation for every sense defined by the Wordnet dictionary.",2 Related Work,[0],[0]
"Other relevant work includes Qiu et al. (2014) who maintains separate representations for different part-ofspeech tags of the same word.
",2 Related Work,[0],[0]
Recent work is mostly evaluated on the relatively artificial task of matching human word similarity judgments.,2 Related Work,[0],[0]
"We propose to build on this previous literature, most specifically Huang et al. (2012) and Neelakantan et al. (2014), to develop an algorithm for learning multiple embeddings for each word type, each embedding corresponding to a distinct induced word sense.",3 Learning Sense-Specific Embeddings,[0],[0]
"Such an algorithm should have the property that a word should be associated with a new sense vector just when evidence in the context (e.g., neighboring words, document-level co-occurrence statistics) suggests that it is sufficiently different from its early senses.",3 Learning Sense-Specific Embeddings,[0],[0]
"Such a line of thinking naturally points to Chinese Restaurant Processes (CRP) (Blei et al., 2004; Teh et al., 2006) which have been applied in the related field of word sense induction.",3 Learning Sense-Specific Embeddings,[0],[0]
"In the analogy of
CRP, the current word could either sit at one of the existing tables (belonging to one of the existing senses) or choose a new table (a new sense).",3 Learning Sense-Specific Embeddings,[0],[0]
The decision is made by measuring semantic relatedness (based on local context information and global document information) and the number of customers already sitting at that table (the popularity of word senses).,3 Learning Sense-Specific Embeddings,[0],[0]
We propose such a model and show that it improves over the state of the art on a standard word similarity task.,3 Learning Sense-Specific Embeddings,[0],[0]
"We offer a brief overview of Chinese Restaurant Processes in this section; readers interested in more details can consult the original papers (Blei et al., 2004; Teh et al., 2006; Pitman, 1995).",3.1 Chinese Restaurant Processes,[0],[0]
"CRP can be viewed as a practical interpretation of Dirichlet Processes (Ferguson, 1973) for nonparametric clustering.",3.1 Chinese Restaurant Processes,[0],[0]
"In the analogy, each data point is compared to a customer in a restaurant.",3.1 Chinese Restaurant Processes,[0],[0]
"The restaurant has a series of tables t, each of which serves a dish dt.",3.1 Chinese Restaurant Processes,[0],[0]
This dish can be viewed as the index of a cluster or a topic.,3.1 Chinese Restaurant Processes,[0],[0]
"The next customer w to enter would either choose an existing table, sharing the dish (cluster) already served or choosing a new cluster based on the following probability distribution:
Pr(tw = t) ∝",3.1 Chinese Restaurant Processes,[0],[0]
"{ NtP (w|dt) if t already exists γP (w|dnew) if t is new
(1) where Nt denotes the number of customers already sitting at table t and P (w|dt) denotes the probability of assigning the current data point to cluster dt.",3.1 Chinese Restaurant Processes,[0],[0]
"γ is the hyper parameter controlling the preference for sitting at a new table.
",3.1 Chinese Restaurant Processes,[0],[0]
CRPs exhibit a useful “rich get richer” property because they take into account the popularity of different word senses.,3.1 Chinese Restaurant Processes,[0],[0]
"They are also more flexible than a simple threshold strategy for setting up new clusters, due to the robustness introduced by adopting the relative ratio of P (w|dt) and P (w|dnew).",3.1 Chinese Restaurant Processes,[0],[0]
"We describe how we incorporate CRP into a standard distributed language model1.
",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"1We omit details about training standard distributed models; see Collobert and Weston (2008) and Mikolov et al. (2013).
",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"As in the standard vector-space model, each token w is associated with a K dimensional global embedding ew.",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"Additionally, it is associated with a set of senses Zw = {z1w, z2w, ..., z|Zw|w } where |Zw| denotes the number of senses discovered for word w.",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
Each sense z is associated with a distinct sense-specific embedding ezw.,3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"When we encounter a new token w in the text, at the first stage, we maximize the probability of seeing the current token given its context as in standard language models using the global vector ew:
p(ew|eneigh) = F (ew, eneigh) (2)
F() can take different forms in different learning paradigms, e.g., F = ∏ w′∈neigh p(ew, ew′) for skip-gram or F = p(ew, g(ew)) for SENNA (Collobert and Weston, 2008) and CBOW, where g(eneigh) denotes a function that projects the concatenation of neighboring vectors to a vector with the same dimension as ew for SENNA and the bag-or-word averaging for CBOW (Mikolov et al., 2013).
",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"Unlike traditional one-word-one-vector frameworks, eneigh includes sense information in addition to the global vectors for neighbors.",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"eneigh can therefore be written as2.
",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"eneigh = {en−k, , ..., en−1, en+1, ..., en−k} (3)
",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"Next we would use CRP to decide which sense the current occurrence corresponds to, or construct a new sense if it is a new meaning that we have not encountered before.",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"Based on CRP, the probability that assigns the current occurrence to each of the discovered senses or a new sense is given by:
Pr(zw = z) ∝  ",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"Nwz P (e z w|context) if z already exists
γP (w|znew) if z is new (4)
where Nwz denotes the number of times already assigned to sense z for token w. P (ezw|context) denotes the probability that current occurrence belonging to (or generated by) sense",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"z.
The algorithm for parameter update for the one token predicting procedure is illustrated in Figure
2For models that predict succeeding words, sense labels for preceding words have already been decided.",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"For models that predict words using both left and right contexts, the labels for right-context words have not been decided yet.",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"In such cases we just use its global word vector to fill up the position.
",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"01: Input : Token sequence {wn, wneigh}.",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
02: Update parameters involved in Equ (3)(4) based on current word prediction.,3.2 Incorporating CRP into Distributed Language Models,[0],[0]
03:,3.2 Incorporating CRP into Distributed Language Models,[0],[0]
Sample sense label z from CRP.,3.2 Incorporating CRP into Distributed Language Models,[0],[0]
04: If a new sense label z is sampled: 05: - add z to Zwn 06: - ezwn = argmax p(wn|zm),3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"07: else: update parameters involved based on sampled sense label z.
Figure 1: Incorporating CRP into Neural Language Models.
1: Line 2 shows parameter updating through predicting the occurrence of current token.",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"Lines 4-6 illustrate the situation when a new word sense is detected, in which case we would add the newly detected sense z into Zwn .",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"The vector representation ezw for the newly detected sense would be obtained by maximizing the function p(ezw|context).
",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"As we can see, the model performs word-sense clustering and embedding learning jointly, each one affecting the other.",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"The prediction of the global vector of the current token (line2) is based on both the global and sense-specific embeddings of its neighbors, as will be updated through predicting the current token.",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"Similarly, once the sense label is decided (line7), the model will adjust the embeddings for neighboring words, both global word vectors and sense-specific vectors.
",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
Training We train embeddings using Gigaword5 + Wikipedia2014.,3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"The training approach is implemented using skip-grams (SG) (Mikolov et al., 2013).",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"We induced senses for the top 200,000 most frequent words (and used a unified “unknown” token for other less-frequent tokens).",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
The window size is set to 11.,3.2 Incorporating CRP into Distributed Language Models,[0],[0]
We iterate three times over the corpus.,3.2 Incorporating CRP into Distributed Language Models,[0],[0]
Next we describe how we decide sense labels for tokens in context.,4 Obtaining Word Representations for NLU tasks,[0],[0]
"The scenario is treated as a inference procedure for sense labels where all global word embeddings and sense-specific embeddings are kept fixed.
",4 Obtaining Word Representations for NLU tasks,[0],[0]
"Given a document or a sentence, we have an objective function with respect to sense labels by multiplying Eq.2 over each containing token.
",4 Obtaining Word Representations for NLU tasks,[0],[0]
"Computing the global optimum sense labeling— in which every word gets an optimal sense label— requires searching over the space of all senses for all words, which can be expensive.",4 Obtaining Word Representations for NLU tasks,[0],[0]
"We therefore chose two simplified heuristic approaches:
• Greedy Search: Assign each token the locally optimum sense label and represent the current token with the embedding associated with that sense.
",4 Obtaining Word Representations for NLU tasks,[0],[0]
• Expectation:,4 Obtaining Word Representations for NLU tasks,[0],[0]
"Compute the probability of each possible sense for the current word, and represent the word with the expectation vector:
~ew = ∑ z∈Zw p(w|z, context) ·",4 Obtaining Word Representations for NLU tasks,[0],[0]
ezw,4 Obtaining Word Representations for NLU tasks,[0],[0]
"We evaluate our embeddings by comparing with other multi-sense embeddings on the standard artificial task for matching human word similarity judgments.
",5 Word Similarity Evaluation,[0],[0]
"Early work used similarity datasets like WS353 (Finkelstein et al., 2001) or RG (Rubenstein and Goodenough, 1965), whose context-free nature makes them a poor evaluation.",5 Word Similarity Evaluation,[0],[0]
"We therefore adopt Stanford’s Contextual Word Similarities (SCWS) (Huang et al., 2012), in which human judgments are associated with pairs of words in context.",5 Word Similarity Evaluation,[0],[0]
"Thus for example “bank” in the context of “river bank” would have low relatedness with “deficit” in the context “financial deficit”.
",5 Word Similarity Evaluation,[0],[0]
We first use the Greedy or Expectation strategies to obtain word vectors for tokens given their context.,5 Word Similarity Evaluation,[0],[0]
"These vectors are then used as input to get the value of cosine similarity between two words.
",5 Word Similarity Evaluation,[0],[0]
Performances are reported in Table 1.,5 Word Similarity Evaluation,[0],[0]
"Consistent with earlier work (e.g.., Neelakantan et al. (2014)), we find that multi-sense embeddings result in better performance in the context-dependent SCWS task (SG+Greedy and SG+Expect are better than SG).",5 Word Similarity Evaluation,[0],[0]
"As expected, performance is not as high when global level information is ignored when choosing word senses (SG+Greedy) as when it is included (SG+Expect), as neighboring words don’t provide sufficient information for word sense disambiguation.
",5 Word Similarity Evaluation,[0],[0]
"To note, the proposed CRF models work a little better than earlier baselines, which gives some evidence that it is sufficiently strong to stand in for
this class of multi-sense models and serves as a promise for being extended to NLU tasks.
",5 Word Similarity Evaluation,[0],[0]
Visualization Table 2 shows examples of semantically related words given the local context.,5 Word Similarity Evaluation,[0],[0]
Word embeddings for tokens are obtained by using the inferred sense labels from the Greedy model and are then used to search for nearest neighbors in the vector space based on cosine similarity.,5 Word Similarity Evaluation,[0],[0]
"Like earlier models (e.g., Neelakantan et al. (2014))., the model can disambiguate different word senses (in examples like bank, rock and apple) based on their local context; although of course the model is also capable of dealing with polysemy—senses that are less distinct.",5 Word Similarity Evaluation,[0],[0]
"Having shown that multi-sense embeddings improve word similarity tasks, we turn to ask whether they improve real-world NLU tasks: POS tagging, NER tagging, sentiment analysis at the phrase and sentence level, semantic relationship identification and sentence-level semantic relatedness.",6 Experiments on NLP Tasks,[0],[0]
"For each task, we experimented on the following sets of embeddings, which are trained using the word2vec package on the same corpus:
• Standard one-word-one-vector embeddings from skip-gram (50d).
",6 Experiments on NLP Tasks,[0],[0]
"• Sense disambiguated embeddings from Section 3 and 4 using Greedy Search and Expectation (50d)
",6 Experiments on NLP Tasks,[0],[0]
"• The concatenation of global word embeddings and sense-specific embeddings (100d).
",6 Experiments on NLP Tasks,[0],[0]
"• Standard one-word-one-vector skip-gram embeddings with dimensionality doubled (100d) (100d is the correct corresponding
baseline since the concatenation above doubles the dimensionality of word vectors)
",6 Experiments on NLP Tasks,[0],[0]
"• Embeddings with very high dimensionality (300d).
",6 Experiments on NLP Tasks,[0],[0]
"As far as possible we try to perform an appleto-apple comparison on these tasks, and our goal is an analytic one—to investigate how well semantic information can be encoded in multi-sense embeddings and how they can improve NLU performances—rather than an attempt to create state-of-the-art results.",6 Experiments on NLP Tasks,[0],[0]
"Thus for example, in tagging tasks (e.g., NER, POS), we follow the protocols in (Collobert et al., 2011) using the concatenation of neighboring embeddings as input features rather than treating embeddings as auxiliary features which are fed into a CRF model along with other manually developed features as in Pennington et al. (2014).",6 Experiments on NLP Tasks,[0],[0]
"Or for experiments on sentiment and other tasks where sentence level embeddings are required we only employ standard recurrent or recursive models for sentence embedding rather than models with sophisticated state-of-theart methods (e.g., Tai et al. (2015; Irsoy and Cardie (2014)).
",6 Experiments on NLP Tasks,[0],[0]
"Significance testing for comparing models is done via the bootstrap test (Efron and Tibshirani, 1994).",6 Experiments on NLP Tasks,[0],[0]
"Unless otherwise noted, significant testing is performed on one-word-one-vector embedding (50d) versus multi-sense embedding using Expectation inference (50d) and one-vector embedding (100d) versus Expectation (100d).",6 Experiments on NLP Tasks,[0],[0]
"Named Entity Recognition We use the CoNLL-2003 English benchmark for training, and test on the CoNLL-2003 test data.",6.1 The Tasks,[0],[0]
"We follow the protocols in Collobert et al. (2011), using the concatenation of neighboring embeddings as input to a multi-layer neural model.",6.1 The Tasks,[0],[0]
"We employ a five-layer neural architecture, comprised of an input layer, three convolutional layers with rectifier linear activation function and a softmax output layer.",6.1 The Tasks,[0],[0]
Training is done by gradient descent with minibatches where each sentence is treated as one batch.,6.1 The Tasks,[0],[0]
"Learning rate, window size, number of hidden units of hidden layers, L2 regularizations and number of iterations are tuned on the development set.
",6.1 The Tasks,[0],[0]
"Part-of-Speech Tagging We use Sections 0–18 of the Wall Street Journal (WSJ) data for train-
ing, sections 19–21 for validation and sections 22–24 for testing.",6.1 The Tasks,[0],[0]
"Similar to NER, we trained 5- layer neural models which take the concatenation of neighboring embeddings as inputs.",6.1 The Tasks,[0],[0]
"We adopt a similar training and parameter tuning strategy as for POS tagging.
",6.1 The Tasks,[0],[0]
Sentence-level Sentiment Classification (Pang),6.1 The Tasks,[0],[0]
The sentiment dataset of Pang et al. (2002) consists of movie reviews with a sentiment label for each sentence.,6.1 The Tasks,[0],[0]
We divide the original dataset into training(8101)/dev(500)/testing(2000).,6.1 The Tasks,[0],[0]
Word embeddings are initialized using the aforementioned types of embeddings and kept fixed in the learning procedure.,6.1 The Tasks,[0],[0]
"Sentence level embeddings are achieved by using standard sequence recurrent neural models (Pearlmutter, 1989) (for details, please refer to Appendix section).",6.1 The Tasks,[0],[0]
"The ob-
tained embedding is then fed into a sigmoid classifier.",6.1 The Tasks,[0],[0]
"Convolutional matrices at the word level are randomized from [-0.1, 0.1] and learned from sequence models.",6.1 The Tasks,[0],[0]
"For training, we adopt AdaGrad with mini-batch.",6.1 The Tasks,[0],[0]
"Parameters (i.e., L2 penalty, learning rate and mini batch size) are tuned on the development set.",6.1 The Tasks,[0],[0]
"Due to space limitations, we omit details of recurrent models and training.
",6.1 The Tasks,[0],[0]
Sentiment Analysis–Stanford Treebank,6.1 The Tasks,[0],[0]
"The Stanford Sentiment Treebank (Socher et al., 2013) contains gold-standard labels for each constituent in the parse tree (phrase level), thus allowing us to investigate a sentiment task at a finer granularity than the dataset in Pang et al. (2002) where labels are only found at the top of each sentence, The sentences in the treebank were split into a training(8544)/development(1101)/testing(2210) dataset.
",6.1 The Tasks,[0],[0]
"Following Socher et al. (2013) we obtained embeddings for tree nodes by using a recursive neural network model, where the embedding for parent node is obtained in a bottom-up fashion based on its children.",6.1 The Tasks,[0],[0]
"The embeddings for each parse tree constituent are output to a softmax layer; see Socher et al. (2013).
",6.1 The Tasks,[0],[0]
We focus on the standard version of recursive neural models.,6.1 The Tasks,[0],[0]
Again we fixed word embeddings to each of the different embedding settings described above3.,6.1 The Tasks,[0],[0]
"Similarly, we adopted AdaGrad
3Note that this is different from the settings used in
with mini-batch.",6.1 The Tasks,[0],[0]
"Parameters (i.e., L2 penalty, learning rate and mini batch size) are tuned on the development set.",6.1 The Tasks,[0],[0]
"The number of iterations is treated as a variable to tune and parameters are harvested based on the best performance on the development set.
",6.1 The Tasks,[0],[0]
"Semantic Relationship Classification SemEval-2010 Task 8 (Hendrickx et al., 2009) is to find semantic relationships between pairs of nominals, e.g., in “My [apartment]e1 has a pretty large [kitchen]e2” classifying the relation between [apartment] and",6.1 The Tasks,[0],[0]
[kitchen] as component-whole.,6.1 The Tasks,[0],[0]
"The dataset contains 9 ordered relationships, so the task is formalized as a 19-class classification problem, with directed relations treated as separate labels; see Hendrickx et al. (2009) for details.
",6.1 The Tasks,[0],[0]
We follow the recursive implementations defined in Socher et al. (2012).,6.1 The Tasks,[0],[0]
"The path in the parse tree between the two nominals is retrieved, and the embedding is calculated based on recursive models and fed to a softmax classifier.",6.1 The Tasks,[0],[0]
"For pure comparison purpose, we only use embeddings as features and do not explore other combination of artificial features.",6.1 The Tasks,[0],[0]
"We adopt the same training strategy as for the sentiment task (e.g., Adagrad, minibatches, etc).
",6.1 The Tasks,[0],[0]
"(Socher et al., 2013) where word vectors were treated as parameters to optimize.
",6.1 The Tasks,[0],[0]
Sentence Semantic Relatedness,6.1 The Tasks,[0],[0]
"We use the Sentences Involving Compositional Knowledge (SICK) dataset (Marelli et al., 2014) consisting of 9927 sentence pairs, split into training(4500)/development(500)/Testing(4927).",6.1 The Tasks,[0],[0]
"Each sentence pair is associated with a gold-standard label ranging from 1 to 5, indicating how semantically related are the two sentences, from 1 (the two sentences are unrelated) to 5 (the two are very related).
",6.1 The Tasks,[0],[0]
"In our setting, the similarity between two sentences is measured based on sentence-level embeddings.",6.1 The Tasks,[0],[0]
Let s1 and s2 denote two sentences and es1 and es2 denote corresponding embeddings.,6.1 The Tasks,[0],[0]
es1 and es2 are achieved through recurrent or recursive models (as illustrated in Appendix section).,6.1 The Tasks,[0],[0]
"Again, word embeddings are obtained by simple table look up in one-word-one-vector settings and inferred using the Greedy or Expectation strategy in multi-sense settings.",6.1 The Tasks,[0],[0]
"We adopt two different recurrent models for acquiring sentencelevel embeddings, a standard recurrent model and an LSTM model (Hochreiter and Schmidhuber, 1997).
",6.1 The Tasks,[0],[0]
"The similarity score is predicted using a regression model built on the structure of a three layer convolutional model, with concatenation of es1 and es2 as input, and a regression score from 1- 5 as output.",6.1 The Tasks,[0],[0]
We adopted the same training strategy as described earlier.,6.1 The Tasks,[0],[0]
The trained model is then used to predict the relatedness score between two new sentences.,6.1 The Tasks,[0],[0]
Performance is measured using Pearson’s r between the predicted score and goldstandard labels.,6.1 The Tasks,[0],[0]
"Results for different tasks are represented in Tables 3-9.
",6.2 Discussions,[0],[0]
"At first glance it seems that multi-sense embeddings do indeed offer superior performance, since combining global vectors with sense-specific vectors introduces a consistent performance boost
for every task, when compared with the standard (50d) setting.",6.2 Discussions,[0],[0]
"But of course this is an unfair comparison; combining global vector with sensespecific vector doubles the dimensionality of vector to 100, making comparison with standard dimensionality (50d) unfair.",6.2 Discussions,[0],[0]
"When comparing with standard (100), the conclusions become more nuanced.
",6.2 Discussions,[0],[0]
"For every task, the +Expectation method has performances that often seem to be higher than the simple baseline (both for the 50d case or the 100d case).",6.2 Discussions,[0],[0]
"However, only some of these differences are significant.
",6.2 Discussions,[0],[0]
(1) Using multi-sense embeddings is significantly helpful for tasks like semantic relatedness (Tables 7-8).,6.2 Discussions,[0],[0]
"This is sensible since sentence meaning here is sensitive to the semantics of one particular word, which could vary with word sense and which would directly be reflected on the relatedness score.
",6.2 Discussions,[0],[0]
"(2) By contrast, for sentiment analysis (Tables 5-6), much of the task depends on correctly identifying a few sentiment words like “good” or “bad”, whose senses tend to have similar sentiment values, and hence for which multi-sense embeddings offer little help.",6.2 Discussions,[0],[0]
"Multi-sense embeddings might promise to help sentiment analysis for some cases, like disambiguating the word “sound” in “safe and sound” versus “movie sound”.",6.2 Discussions,[0],[0]
"But we suspect that such cases are not common, explaining the nonsignificance of the improvement.",6.2 Discussions,[0],[0]
"Furthermore, the advantages of neural models in sentiment analysis tasks presumably lie in their capability to capture local composition like negation, and it’s not clear how helpful multi-sense embeddings are for that aspect.
",6.2 Discussions,[0],[0]
"(3) Similarly, multi-sense embeddings help for POS tagging, but not for NER tagging (Table 3-4).",6.2 Discussions,[0],[0]
Word senses have long been known to be related to POS tags.,6.2 Discussions,[0],[0]
"But the largest proportion of NER tags consists of the negative not-a-NER (“O”) tag, each of which is likely correctly labelable regard-
less of whether senses are disambiguated or not (since presumably if a word is not a named entity, most of its senses are not named entities either).
(4) As we apply more sophisticated models like LSTM to semantic relatedness tasks (in Table 9), the advantages caused by multi-sense embeddings disappears.
",6.2 Discussions,[0],[0]
(5) Doubling the number of dimensions is sufficient to increase performance as much as using the complex multi-sense algorithm.,6.2 Discussions,[0],[0]
"(Of course increasing vector dimensionality (to 300) boosts performance even more, although at the significant cost of exponentially increasing time complexity.)",6.2 Discussions,[0],[0]
We do larger one-word-one-vector embeddings do so well?,6.2 Discussions,[0],[0]
"We suggest some hypotheses:
• though information about distinct senses is encoded in one-word-one-vector embeddings in a mixed and less structured way, we suspect that the compositional nature of neural models is able to separate the informational chaff from the wheat and choose what information to take up, bridging the gap between single vector and multi-sense paradigms.",6.2 Discussions,[0],[0]
"For models like LSTMs which are better at doing such a job by using gates to control information flow, the difference between two paradigms should thus be further narrowed, as indeed we found.
",6.2 Discussions,[0],[0]
•,6.2 Discussions,[0],[0]
"The pipeline model proposed in the work requires sense-label inference (i.e., step 2).",6.2 Discussions,[0],[0]
"We proposed two strategies: GREEDY and EXPECTATION, and found that GREEDY models perform worse than EXPECTATION, as we might expect4.",6.2 Discussions,[0],[0]
"But even EXPECTATION can be viewed as another form of one-wordone-vector models, just one where different senses are entangled but weighted to emphasize the important ones.",6.2 Discussions,[0],[0]
"Again, this suggests another cause for the strong relative performance of larger-dimensioned one-word-onevector models.",6.2 Discussions,[0],[0]
"In this paper, we expand ongoing research into multi-sense embeddings by first proposing a new version based on Chinese restaurant processes that achieves state of the art performance on simple
4GREEDY models work in a more aggressive way and likely make mistakes due to the non-global-optimum nature and limited context information
word similarity matching tasks.",7 Conclusion,[0],[0]
"We then introduce a pipeline system for incorporating multisense embeddings into NLP applications, and examine multiple NLP tasks to see whether and when multi-sense embeddings can introduce performance boosts.",7 Conclusion,[0],[0]
Our results suggest that simply increasing the dimensionality of baseline skip-gram embeddings is sometimes sufficient to achieve the same performance wins that come from using multi-sense embeddings.,7 Conclusion,[0],[0]
"That is, the most straightforward way to yield better performance on these tasks is just to increase embedding dimensionality.
",7 Conclusion,[0],[0]
Our results come with some caveats.,7 Conclusion,[0],[0]
"In particular, our conclusions are based on the pipelined system that we introduce, and other multi-sense embedding systems (e.g., a more advanced sense learning model or a better sense label model or a completely different pipeline system) may find stronger effects of multi-sense models.",7 Conclusion,[0],[0]
"Nonetheless we do consistently find improvements for multi-sense embeddings in some tasks (part-ofspeech tagging and semantic relation identification), suggesting the benefits of our multi-sense models and those of others.",7 Conclusion,[0],[0]
"Perhaps the most important implication of our results may be the evidence they provide for the importance of going beyond simple human-matching tasks, and testing embedding models by using them as components in real NLP applications.",7 Conclusion,[0],[0]
"In sentiment classification and sentence semantic relatedness tasks, classification models require embeddings that represent the input at a sentence or phrase level.",8 Appendix,[0],[0]
"We adopt recurrent networks (standard ones or LSTMs) and recursive networks in order to map a sequence of tokens with various length to a vector representation.
",8 Appendix,[0],[0]
"Recurrent Networks A recurrent network successively takes wordwt at step t, combines its vector representation et with the previously built hidden vector ht−1 from time t− 1, calculates the resulting current embedding ht, and passes it to the next step.",8 Appendix,[0],[0]
"The embedding ht for the current time t is thus:
ht = tanh(W · ht−1 + V · et) (5) whereW and V denote compositional matrices.",8 Appendix,[0],[0]
"If Ns denote the length of the sequence, hNs represents the whole sequence S.
Recursive Networks Standard recursive models work in a similar way by working on neighboring words by parse tree order rather than sequence order.",8 Appendix,[0],[0]
They compute the representation for each parent node based on its immediate children recursively in a bottom-up fashion until reaching the root of the tree.,8 Appendix,[0],[0]
"For a given node η in the tree and its left child ηleft (with representation eleft) and right child ηright (with representation eright), the standard recursive network calculates eη:
eη = tanh(W · eηleft + V · eηright) (6)
",8 Appendix,[0],[0]
"Long Short Term Memory (LSTM) LSTM models (Hochreiter and Schmidhuber, 1997) are defined as follows: given a sequence of inputs X = {x1, x2, ..., xnX}, an LSTM associates each timestep with an input, memory and output gate, respectively denoted as it, ft and ot.",8 Appendix,[0],[0]
"We notationally disambiguate e and h, where et denote the vector for an individual text unit (e.g., word or sentence) at time step t while ht denotes the vector computed by the LSTM model at time t by combining et and ht−1.",8 Appendix,[0],[0]
σ denotes the sigmoid function.,8 Appendix,[0],[0]
W ∈ R4K×2K .,8 Appendix,[0],[0]
"The vector representation ht for each time-step t is given by:
[ it ft ot lt ] =",8 Appendix,[0],[0]
"[ σ σ σ tanh ] W · [ ht−1 et ] (7)
ct = ft · ct−1 + it · lt (8) hst = ot · ct (9)",8 Appendix,[0],[0]
"We would like to thank Sam Bowman, Ignacio Cases, Kevin Gu, Gabor Angeli, Sida Wang, Percy Liang and other members of the Stanford NLP group, as well as anonymous reviewers for their helpful advice on various aspects of this work.",9 Acknowledgments,[0],[0]
"We gratefully acknowledge the support of the NSF via award IIS-1514268, the Defense Advanced Research Projects Agency (DARPA) Deep Exploration and Filtering of Text (DEFT) Program under Air Force Research Laboratory (AFRL) contract no.",9 Acknowledgments,[0],[0]
FA8750-13-2-0040.,9 Acknowledgments,[0],[0]
"Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of NSF, DARPA, AFRL, or the US government.",9 Acknowledgments,[0],[0]
Learning a distinct representation for each sense of an ambiguous word could lead to more powerful and fine-grained models of vector-space representations.,abstractText,[0],[0]
"Yet while ‘multi-sense’ methods have been proposed and tested on artificial wordsimilarity tasks, we don’t know if they improve real natural language understanding tasks.",abstractText,[0],[0]
"In this paper we introduce a multisense embedding model based on Chinese Restaurant Processes that achieves state of the art performance on matching human word similarity judgments, and propose a pipelined architecture for incorporating multi-sense embeddings into language un-",abstractText,[0],[0]
Do Multi-Sense Embeddings Improve Natural Language Understanding?,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 462–468 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
462",text,[0],[0]
"Neural network mappings are widely used to bridge modalities or spaces in cross-modal retrieval (Qiao et al., 2017; Wang et al., 2016; Zhang et al., 2016), zero-shot learning (Lazaridou et al., 2015b, 2014; Socher et al., 2013) in building multimodal representations (Collell et al., 2017) or in word translation (Lazaridou et al., 2015a), to name a few.",1 Introduction,[0],[0]
"Typically, a neural network is firstly trained
to predict the distributed vectors of one modality (or space) from the other.",1 Introduction,[0],[0]
"At test time, some operation such as retrieval or labeling is performed based on the nearest neighbors of the predicted (mapped) vectors.",1 Introduction,[0],[0]
"For instance, in zero-shot image classification, image features are mapped to the text space and the label of the nearest neighbor word is assigned.",1 Introduction,[0],[0]
"Thus, the success of such systems relies entirely on the ability of the map to make the predicted vectors similar to the target vectors in terms of semantic or neighborhood structure.1",1 Introduction,[0],[0]
"However, whether neural nets achieve this goal in general has not been investigated yet.",1 Introduction,[0],[0]
"In fact, recent work evidences that considerable information about the input modality propagates into the predicted modality (Collell et al., 2017; Lazaridou et al., 2015b; Frome et al., 2013).
",1 Introduction,[0],[0]
"To shed light on these questions, we first introduce the (to the best of our knowledge) first existing measure to quantify similarity between the neighborhood structures of two sets of vectors.",1 Introduction,[0],[0]
"Second, we perform extensive experiments in three benchmarks where we learn image-to-text and text-to-image neural net mappings using a rich variety of state-of-the-art text and image features and loss functions.",1 Introduction,[0],[0]
"Our results reveal that, contrary to expectation, the semantic structure of the mapped vectors consistently resembles more that of the input vectors than that of the target vectors of interest.",1 Introduction,[0],[0]
"In a second experiment, by using six concept similarity tasks we show that the semantic structure of the input vectors is preserved after mapping them with an untrained network, further evidencing that feed-forward nets naturally preserve semantic information about the input.",1 Introduction,[0],[0]
"Overall, we uncover and rise awareness of a largely
1We indistinctly use the terms semantic structure, neighborhood structure and similarity structure.",1 Introduction,[0],[0]
"They refer to all pairwise similarities of a set of N vectors, for some similarity measure (e.g., Euclidean or cosine).
",1 Introduction,[0],[0]
"ignored phenomenon relevant to a wide range of cross-modal / cross-space applications such as retrieval, zero-shot learning or image annotation.
",1 Introduction,[0],[0]
"Ultimately, this paper aims at: (1) Encouraging the development of better architectures to bridge modalities / spaces; (2) Advocating for the use of semantic-based criteria to evaluate the quality of predicted vectors such as the neighborhood-based measure proposed here, instead of purely geometric measures such as mean squared error (MSE).",1 Introduction,[0],[0]
Neural network and linear mappings are popular tools to bridge modalities in cross-modal retrieval systems.,2 Related Work and Motivation,[0],[0]
Lazaridou et al. (2015b) leverage a text-to-image linear mapping to retrieve images given text queries.,2 Related Work and Motivation,[0],[0]
Weston et al. (2011) map label and image features into a shared space with a linear mapping to perform image annotation.,2 Related Work and Motivation,[0],[0]
"Alternatively, Frome et al. (2013), Lazaridou et al. (2014) and Socher et al. (2013) perform zero-shot image classification with an image-to-text neural network mapping.",2 Related Work and Motivation,[0],[0]
"Instead of mapping to latent features, Collell et al. (2018) use a 2-layer feedforward network to map word embeddings directly to image pixels in order to visualize spatial arrangements of objects.",2 Related Work and Motivation,[0],[0]
Neural networks are also popular in other cross-space applications such as cross-lingual tasks.,2 Related Work and Motivation,[0],[0]
"Lazaridou et al. (2015a) learn a linear map from language A to language B and then translate new words by returning the nearest neighbor of the mapped vector in the B space.
",2 Related Work and Motivation,[0],[0]
"In the context of zero-shot learning, shortcomings of cross-space neural mappings have also been identified.",2 Related Work and Motivation,[0],[0]
"For instance, “hubness” (Radovanović et al., 2010) and “pollu-
tion” (Lazaridou et al., 2015a) relate to the highdimensionality of the feature spaces and to overfitting respectively.",2 Related Work and Motivation,[0],[0]
"Crucially, we do not assume that our cross-modal problem has any class labels, and we study the similarity between input and mapped vectors and between output and mapped vectors.
",2 Related Work and Motivation,[0],[0]
Recent work evidences that the predicted vectors of cross-modal neural net mappings are still largely informative about the input vectors.,2 Related Work and Motivation,[0],[0]
Lazaridou et al. (2015b) qualitatively observe that abstract textual concepts are grounded with the visual input modality.,2 Related Work and Motivation,[0],[0]
"Counterintuitively, Collell et al. (2017) find that the vectors “imagined” from a language-to-vision neural map, outperform the original visual vectors in concept similarity tasks.",2 Related Work and Motivation,[0],[0]
The paper argued that the reconstructed visual vectors become grounded with language because the map preserves topological properties of the input.,2 Related Work and Motivation,[0],[0]
"Here, we go one step further and show that the mapped vectors often resemble the input vectors more than the target vectors in semantic terms, which goes against the goal of a cross-modal map.
",2 Related Work and Motivation,[0],[0]
"Well-known theoretical work shows that networks with as few as one hidden layer are able to approximate any function (Hornik et al., 1989).",2 Related Work and Motivation,[0],[0]
"However, this result does not reveal much neither about test performance nor about the semantic structure of the mapped vectors.",2 Related Work and Motivation,[0],[0]
"Instead, the phenomenon described is more closely tied to other properties of neural networks.",2 Related Work and Motivation,[0],[0]
"In particular, continuity guarantees that topological properties of the input, such as connectedness, are preserved (Armstrong, 2013).",2 Related Work and Motivation,[0],[0]
"Furthermore, continuity in a topology induced by a metric also ensures that points that are close together are mapped close together.",2 Related Work and Motivation,[0],[0]
"As a toy example, Fig. 1 illustrates the distortion of a manifold after being mapped by a neural net.2
In a noiseless world with fully statistically dependent modalities, the vectors of one modality could be perfectly predicted from those of the other.",2 Related Work and Motivation,[0],[0]
"However, in real-world problems this is unrealistic given the noise of the features and the fact that modalities encode complementary information (Collell and Moens, 2016).",2 Related Work and Motivation,[0],[0]
"Such unpredictability combined with continuity and topology-preserving properties of neural nets propel the phenomenon identified, namely mapped vectors resembling more the input than the target vectors, in nearest neighbors terms.
2Parameters of these mappings were generated at random.",2 Related Work and Motivation,[0],[0]
"To bridge modalities X and Y , we consider two popular cross-modal mappings f :",3 Proposed Approach,[0],[0]
"X → Y .
",3 Proposed Approach,[0],[0]
"(i) Linear mapping (lin):
f(x) =W0x+ b0
with W0 ∈ Rdy×dx , b0 ∈",3 Proposed Approach,[0],[0]
"Rdy , where dx and dy are the input and output dimensions respectively.
(ii) Feed-forward neural network (nn):
f(x) =W1σ(W0x+ b0) + b1
with W1 ∈ Rdy×dh , W0 ∈",3 Proposed Approach,[0],[0]
"Rdh×dx , b0 ∈ Rdh ,",3 Proposed Approach,[0],[0]
b1 ∈,3 Proposed Approach,[0],[0]
"Rdy where dh is the number of hidden units and σ() the non-linearity (e.g., tanh or sigmoid).",3 Proposed Approach,[0],[0]
"Although single hidden layer networks are already universal approximators (Hornik et al., 1989), we explored whether deeper nets with 3 and 5 hidden layers could improve the fit (see Supplement).
",3 Proposed Approach,[0],[0]
Loss:,3 Proposed Approach,[0],[0]
Our primary choice is the MSE: 1 2‖f(x),3 Proposed Approach,[0],[0]
"− y‖
2, where y is the target vector.",3 Proposed Approach,[0],[0]
"We also tested other losses such as the cosine: 1 − cos(f(x), y) and the max-margin: max{0, γ + ‖f(x)",3 Proposed Approach,[0],[0]
"− y‖ − ‖f(x̃) − y‖}, where x̃ belongs to a different class than (x, y), and γ is the margin.",3 Proposed Approach,[0],[0]
"As in Lazaridou et al. (2015a) and Weston et al. (2011), we choose the first x̃ that violates the constraint.",3 Proposed Approach,[0],[0]
"Notice that losses that do not require class labels such as MSE are suitable for a wider, more general set of tasks than discriminative losses (e.g., cross-entropy).",3 Proposed Approach,[0],[0]
"In fact, cross-modal retrieval tasks often do not exhibit any class labels.",3 Proposed Approach,[0],[0]
"Additionally, our research question concerns the cross-space mapping problem in isolation (independently of class labels).
",3 Proposed Approach,[0],[0]
Let us denote a set of N input and output vectors by X ∈ RN×dx and Y ∈ RN×dy respectively.,3 Proposed Approach,[0],[0]
"Each input vector xi is paired to the output vector yi of the same index (i = 1, · · · , N ).",3 Proposed Approach,[0],[0]
Let us henceforth denote the mapped input vectors by f(X) ∈ RN×dy .,3 Proposed Approach,[0],[0]
"In order to explore the similarity between f(X) and X , and between f(X) and Y , we propose two ad hoc settings below.",3 Proposed Approach,[0],[0]
"To measure the similarity between the neighborhood structure of two sets of paired vectors V and
Z, we propose the mean nearest neighbor overlap measure (mNNOK(V,Z)).",3.1 Neighborhood Structure of Mapped Vectors (Experiment 1),[0],[0]
"We define the nearest neighbor overlap NNOK(vi, zi) as the number of K nearest neighbors that two paired vectors vi, zi share in their respective spaces.",3.1 Neighborhood Structure of Mapped Vectors (Experiment 1),[0],[0]
"E.g., if the 3 (= K) nearest neighbors of vcat in V are {vdog, vtiger, vlion} and those of zcat in Z are {zmouse, ztiger, zlion}, the NNO3(vcat, zcat) is 2.
",3.1 Neighborhood Structure of Mapped Vectors (Experiment 1),[0],[0]
Definition 1 Let V = {vi}Ni=1 and Z = {zi}Ni=1 be two sets of N paired vectors.,3.1 Neighborhood Structure of Mapped Vectors (Experiment 1),[0],[0]
"We define:
mNNOK(V,Z) = 1
KN N∑ i=1",3.1 Neighborhood Structure of Mapped Vectors (Experiment 1),[0],[0]
"NNOK(vi, zi) (1)
with NNOK(vi, zi) = |NNK(vi) ∩ NNK(zi)|, where NNK(vi) and NNK(zi) are the indexes of the K nearest neighbors of vi and zi, respectively.
",3.1 Neighborhood Structure of Mapped Vectors (Experiment 1),[0],[0]
"The normalizing constant K simply scales mNNOK(V,Z) between 0 and 1, making it independent of the choice of K. Thus, a mNNOK(V,Z) = 0.7 means that the vectors in V and Z share, on average, 70% of their nearest neighbors.",3.1 Neighborhood Structure of Mapped Vectors (Experiment 1),[0],[0]
"Notice that mNNO implicitly performs retrieval for some similarity measure (e.g., Euclidean or cosine), and quantifies how semantically similar two sets of paired vectors are.",3.1 Neighborhood Structure of Mapped Vectors (Experiment 1),[0],[0]
"To complement the setting above (Sect. 3.1), it is instructive to consider the limit case of an untrained network.",3.2 Mapping with Untrained Networks (Experiment 2),[0],[0]
"Concept similarity tasks provide a suitable setting to study the semantic structure of distributed representations (Pennington et al., 2014).",3.2 Mapping with Untrained Networks (Experiment 2),[0],[0]
"That is, semantically similar concepts should ideally be close together.",3.2 Mapping with Untrained Networks (Experiment 2),[0],[0]
"In particular, our interest is in comparing X with its projection f(X) through a mapping with random parameters, to understand the extent to which the mapping may disrupt or preserve the semantic structure of X .",3.2 Mapping with Untrained Networks (Experiment 2),[0],[0]
"To test the generality of our claims, we select a rich diversity of cross-modal tasks involving texts at three levels: word level (ImageNet), sentence level (IAPR TC-12), and document level (Wiki).",4.1.1 Datasets,[0],[0]
"ImageNet (Russakovsky et al., 2015).",4.1.1 Datasets,[0],[0]
"Consists of ∼14M images, covering ∼22K WordNet synsets
(or meanings).",4.1.1 Datasets,[0],[0]
"Following Collell et al. (2017), we take the most relevant word for each synset and keep only synsets with more than 50 images.",4.1.1 Datasets,[0],[0]
"This yields 9,251 different words (or instances).",4.1.1 Datasets,[0],[0]
"IAPR TC-12 (Grubinger et al., 2006).",4.1.1 Datasets,[0],[0]
Contains 20K images (18K train / 2K test) annotated with 255 labels.,4.1.1 Datasets,[0],[0]
Each image is accompanied with a short description of one to three sentences.,4.1.1 Datasets,[0],[0]
"Wikipedia (Pereira et al., 2014).",4.1.1 Datasets,[0],[0]
"Has 2,866 samples (2,173 train / 693 test).",4.1.1 Datasets,[0],[0]
Each sample is a section of a Wikipedia article paired with one image.,4.1.1 Datasets,[0],[0]
See the Supplement (Sect. 1) for details.,4.1.2 Hyperparameters and Implementation,[0],[0]
"To ensure that results are independent of the choice of image and text features, we use 5 (2 image + 3 text) features of varied dimensionality (64- d, 128-d, 300-d, 2,048-d) and two directions, textto-image (T → I) and image-to-text (I → T ).",4.1.3 Image and Text Features,[0],[0]
We make our extracted features publicly available.3 Text.,4.1.3 Image and Text Features,[0],[0]
"In ImageNet we use 300-dimensional GloVe4 (Pennington et al., 2014) and 300-d word2vec (Mikolov et al., 2013) word embeddings.",4.1.3 Image and Text Features,[0],[0]
"In IAPR TC-12 and Wiki, we employ stateof-the-art bidirectional gated recurrent unit (biGRU) features (Cho et al., 2014) that we learn with a classification task (see Sect.",4.1.3 Image and Text Features,[0],[0]
2 of Supplement).,4.1.3 Image and Text Features,[0],[0]
Image.,4.1.3 Image and Text Features,[0],[0]
"For ImageNet, we use the publicly available5 VGG-128 (Chatfield et al., 2014) and ResNet (He et al., 2015) visual features from Collell et al. (2017), where we obtained 128- dimensional VGG-128 and 2,048-d ResNet features from the last layer (before the softmax) of the forward pass of each image.",4.1.3 Image and Text Features,[0],[0]
The final representation for a word is the average feature vector (centroid) of all available images for this word.,4.1.3 Image and Text Features,[0],[0]
"In IAPR TC-12 and Wiki, features for individual images are obtained similarly from the last layer of a ResNet and a VGG-128 model.",4.1.3 Image and Text Features,[0],[0]
"We include six benchmarks, comprising three types of concept similarity: (i) Semantic similarity: SemSim (Silberer and Lapata, 2014), Simlex999 (Hill et al., 2015) and SimVerb-3500 (Gerz et al., 2016); (ii) Relatedness: MEN (Bruni et al.,
3http://liir.cs.kuleuven.be/software.html 4http://nlp.stanford.edu/projects/glove 5http://liir.cs.kuleuven.be/software.html
2014) and WordSim-353 (Finkelstein et al., 2001); (iii) Visual similarity: VisSim (Silberer and Lapata, 2014) which includes the same word pairs as SemSim, rated for visual similarity instead of semantic.",4.2.1 Datasets,[0],[0]
"All six test sets contain human ratings of similarity for word pairs, e.g., (‘cat’,‘dog’).",4.2.1 Datasets,[0],[0]
"The parameters in W0,W1 are drawn from a random uniform distribution [−1, 1] and b0, b1 are set to zero.",4.2.2 Hyperparameters and Implementation,[0],[0]
"We use a tanh activation σ().6 The output dimension dy is set to 2,048 for all embeddings.",4.2.2 Hyperparameters and Implementation,[0],[0]
Textual and visual features are the same as described in Sect.,4.2.3 Image and Text Features,[0],[0]
4.1.3 for the ImageNet dataset.,4.2.3 Image and Text Features,[0],[0]
"We compute the prediction of similarity between two vectors z1, z2 with both the cosine z1z2‖z1‖‖z2‖ and the Euclidean similarity 11+‖z1−z2‖ .",4.2.4 Similarity Predictions,[0],[0]
7,4.2.4 Similarity Predictions,[0],[0]
"As is common practice, we evaluate the predictions of similarity of the embeddings (Sect. 4.2.4) against the human similarity ratings with the Spearman correlation ρ.",4.2.5 Performance Metrics,[0],[0]
We report the average of 10 sets of randomly generated parameters.,4.2.5 Performance Metrics,[0],[0]
We test statistical significance with a two-sided Wilcoxon rank sum test adjusted with Bonferroni.,5 Results and Discussion,[0],[0]
The null hypothesis is that a compared pair is equal.,5 Results and Discussion,[0],[0]
"In Tab. 1, ∗ indicates that mNNO(X, f(X)) differs from mNNO(Y, f(X))",5 Results and Discussion,[0],[0]
"(p < 0.001) on the same mapping, embedding and direction.",5 Results and Discussion,[0],[0]
"In Tab. 2, ∗ indicates that performance of mapped and input vectors differs (p < 0.05) in the 10 runs.",5 Results and Discussion,[0],[0]
Results below are with cosine neighbors and K = 10.,5.1 Experiment 1,[0],[0]
Euclidean neighbors yield similar results and are thus left to the Supplement.,5.1 Experiment 1,[0],[0]
"Similarly, results in ImageNet with GloVe embeddings are shown below and word2vec results in the Supplement.",5.1 Experiment 1,[0],[0]
"The choice ofK = {5, 10, 30} had no visible effect on results.",5.1 Experiment 1,[0],[0]
Results with 3- and 5-layer nets did not show big differences with the results below (see Supplement).,5.1 Experiment 1,[0],[0]
"The cosine and max-margin losses
6We find that sigmoid and ReLu yield similar results.",5.1 Experiment 1,[0],[0]
"7Notice that papers generally use only cosine similarity
(Lazaridou et al., 2015b; Pennington et al., 2014).
performed slightly worse than MSE (see Supplement).",5.1 Experiment 1,[0],[0]
"Although Lazaridou et al. (2015a) and Weston et al. (2011) find that max-margin performs the best in their tasks, we do not find our result entirely surprising given that max-margin focuses on inter-class differences while we look also at intraclass neighbors (in fact, we do not require classes).
",5.1 Experiment 1,[0],[0]
"Tab. 1 shows our core finding, namely that the semantic structure of f(X) resembles more that of X than that of Y , for both lin and nn maps.
",5.1 Experiment 1,[0],[0]
Fig. 2 is particularly revealing.,5.1 Experiment 1,[0],[0]
"If we would only look at train performance (and allow train MSE to reach 0) then f(X) = Y and clearly train mNNO(f(X), Y )",5.1 Experiment 1,[0],[0]
"= 1 while mNNO(f(X), X) can only be smaller than 1.",5.1 Experiment 1,[0],[0]
"However, the interest is always on test samples, and (near-)perfect test prediction is unrealistic.",5.1 Experiment 1,[0],[0]
"Notice in fact in Fig. 2 that even if we look at train fit, MSE needs to be close to 0 for mNNO(f(X), Y ) to be
reasonably large.",5.1 Experiment 1,[0],[0]
"In all the combinations from Tab. 1, the test mNNO(f(X), Y ) never surpasses test mNNO(f(X), X) for any number of epochs, even with an oracle (not shown).",5.1 Experiment 1,[0],[0]
"Tab. 2 shows that untrained linear (flin) and neural net (fnn) mappings preserve the semantic structure of the input X , complementing thus the findings of Experiment 1.",5.2 Experiment 2,[0],[0]
"Experiment 1 concerns learning, while, by “ablating” the learning part and randomizing weights, Experiment 2 is revealing about the natural tendency of neural nets to preserve semantic information about the input, regardless of the choice of the target vectors and loss function.",5.2 Experiment 2,[0],[0]
"Overall, we uncovered a phenomenon neglected so far, namely that neural net cross-modal mappings can produce mapped vectors more akin to the input vectors than the target vectors, in terms of semantic structure.",6 Conclusions,[0],[0]
Such finding has been possible thanks to the proposed measure that explicitly quantifies similarity between the neighborhood structure of two sets of vectors.,6 Conclusions,[0],[0]
"While other measures such as mean squared error can be misleading, our measure provides a more realistic estimate of the semantic similarity between predicted and target vectors.",6 Conclusions,[0],[0]
"In fact, it is the semantic structure (or pairwise similarities) what ultimately matters in cross-modal applications.",6 Conclusions,[0],[0]
This work has been supported by the CHIST-ERA EU project MUSTER8 and by the KU Leuven grant RUN/15/005.,Acknowledgments,[0],[0]
"Feed-forward networks are widely used in cross-modal applications to bridge modalities by mapping distributed vectors of one modality to the other, or to a shared space.",abstractText,[0],[0]
"The predicted vectors are then used to perform e.g., retrieval or labeling.",abstractText,[0],[0]
"Thus, the success of the whole system relies on the ability of the mapping to make the neighborhood structure (i.e., the pairwise similarities) of the predicted vectors akin to that of the target vectors.",abstractText,[0],[0]
"However, whether this is achieved has not been investigated yet.",abstractText,[0],[0]
"Here, we propose a new similarity measure and two ad hoc experiments to shed light on this issue.",abstractText,[0],[0]
In three cross-modal benchmarks we learn a large number of language-to-vision and visionto-language neural network mappings (up to five layers) using a rich diversity of image and text features and loss functions.,abstractText,[0],[0]
"Our results reveal that, surprisingly, the neighborhood structure of the predicted vectors consistently resembles more that of the input vectors than that of the target vectors.",abstractText,[0],[0]
"In a second experiment, we further show that untrained nets do not significantly disrupt the neighborhood (i.e., semantic) structure of the input vectors.",abstractText,[0],[0]
Do Neural Network Cross-Modal Mappings Really Bridge Modalities?,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1175–1185, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
Grapheme-to-phoneme (G2P) conversion is the problem of converting a string of letters into a string of phonetic symbols.,1 Introduction,[0],[0]
"Closely related to G2P are other string transduction problems in natural language processing (NLP) such as transliteration (Sherif and Kondrak, 2007),",1 Introduction,[0],[0]
"lemmatization (Dreyer et al., 2008), and spelling error correction (Brill and Moore, 2000).",1 Introduction,[0],[0]
"The classical learning paradigm in each of these settings is to train a model on pairs of strings {(x,y)} and then to evaluate model performance on test data.",1 Introduction,[0],[0]
"While there are exceptions (e.g., (Rao et al., 2015)), most state-of-the-art modelings (e.g., (Jiampojamarn et al., 2007; Bisani and Ney, 2008; Jiampojamarn et al., 2008; Jiampojamarn et al., 2010; Novak et al., 2012)) view string transduction as a two-stage process in which string pairs (x,y) in the training data are first aligned, and then a subsequent (e.g., sequence labeling) module is learned on the aligned data.
",1 Introduction,[0],[0]
"State-of-the-art alignments in G2P are characterized by the following properties:
(i) Alignments are monotone in that the ordering of characters in input and output sequences is preserved by the alignments.",1 Introduction,[0],[0]
"Furthermore, they are many-to-many in the sense that several x sequence characters may be matched up with several y sequence characters as illustrated in Table 1.
(ii)",1 Introduction,[0],[0]
"The alignment is a latent variable and learnt in an unsupervised manner from pairs of strings in the training data.
",1 Introduction,[0],[0]
"(iii) The unsupervised alignment models are unigram alignment models insofar as the overall score that the alignment model assigns an alignment is the same for all orderings of the matched-up subsequences (context independence).
",1 Introduction,[0],[0]
"To illustrate point (iii), consider, in the field of lemmatization, the case of aligning an inflected word form with the extended infinitive in German, such as absagt (‘rejects’) with abzusagen (‘to reject’).",1 Introduction,[0],[0]
"Critically, the insertion -zu- appears in infixal position and a plausible alignment might be as in Table 2.",1 Introduction,[0],[0]
"Then, correctly aligning certain
analogous forms such as zusagt (‘accepts’) with
1175
their corresponding extended infinitive zuzusagen (‘to accept’) is beyond the scope of a unigram alignment model since this cannot distinguish the linguistically correct alignment from the following linguistically incorrect alignment
z u s a g t zu z u s a g en
precisely because it has no notion of context.",1 Introduction,[0],[0]
"In this work, we firstly address bigram alignment models in G2P.",1 Introduction,[0],[0]
"We investigate whether there are phenomena in G2P that require bigram alignment models and, more generally, whether bigram alignment models produce better alignments — with respect to a human gold standard — than unigram alignment models within the G2P setting.",1 Introduction,[0],[0]
"We do so, secondly, in a supervised setting where the model learns from gold-standard alignments.",1 Introduction,[0],[0]
"While this may seem an odd scenario at first sight, modern alignment toolkits in the related field of machine translation typically include the possibility to learn both in a supervised and unsupervised manner (Liu et al., 2010; Liu and Sun, 2015).",1 Introduction,[0],[0]
"The rationale behind supervised learning models may be that they perform better than unsupervised models, and if alignment quality has a large impact upon subsequent string translation performance, then a supervised model may be a suitable alternative.",1 Introduction,[0],[0]
"Thirdly, we investigate how alignment quality affects overall G2P performance.",1 Introduction,[0],[0]
"This allows us to address whether it is worthwhile to work on better alignment models, which bigram and supervised alignment models promise to be.",1 Introduction,[0],[0]
"To our knowledge, all three outlined aspects of alignments — bigram models, supervised learning, and systematically estimating the relationship between alignment quality and overall string transduction performance — are novel in the G2P setting and its related fields as outlined; however, see also the related work section.
",1 Introduction,[0],[0]
This work is structured as follows.,1 Introduction,[0],[0]
Section 2 presents definitions and algorithms for uni- and bigram alignment models.,1 Introduction,[0],[0]
Section 3 surveys related work.,1 Introduction,[0],[0]
Section 4 presents our data and Section 5 our experiments.,1 Introduction,[0],[0]
We conclude in Section 6.,1 Introduction,[0],[0]
We first formally define the problem of aligning two strings x and y over arbitrary alphabets in a monotone and many-to-many manner.,2 Uni- and bigram alignment models,[0],[0]
Let `x = |x|,2 Uni- and bigram alignment models,[0],[0]
"and `y = |y| denote the lengths of x and y, respectively.",2 Uni- and bigram alignment models,[0],[0]
"Let N = {0, 1, 2, . . .}, and let S ⊆
N2\{(0, 0)} be a set defining the valid match-up operations between x characters and y characters.",2 Uni- and bigram alignment models,[0],[0]
"In other words, when (s, t) ∈ S, then this means we allow matches of subsequences of x of length s and subsequences of y of length t.1
It is convenient to define a monotone many-tomany alignment of x and y as a 2×k (for k ≥ 1 arbitrary) nonnegative integer matrix Ax,y ∈ N2×k
",2 Uni- and bigram alignment models,[0],[0]
"satisfying Ax,y1k = ( `x `y ) , i.e., the two rows of Ax,y sum up to the lengths of the respective strings,2 and where each column of Ax,y lies in S. For any such alignment, we let (x1, . . .",2 Uni- and bigram alignment models,[0],[0]
",xk) be the corresponding induced segmentation of x and (y1, . . .",2 Uni- and bigram alignment models,[0],[0]
",yk) be the corresponding induced segmentation of y.
Example.",2 Uni- and bigram alignment models,[0],[0]
"For any S ⊇ {(1, 1), (1, 2), (2, 1)}, the alignment of x = phoenix and y = finIks shown in Table 1 may be represented by the ma-
trix Ax,y = (
2 2 1 1 1 1 1 1 1 2
) .",2 Uni- and bigram alignment models,[0],[0]
"The correspond-
ing induced segmentations are (ph,oe,n,i,x) and (f,i,n,I,ks).
",2 Uni- and bigram alignment models,[0],[0]
"Let AS(x,y) denote the class of all alignments of x and y.",2 Uni- and bigram alignment models,[0],[0]
"We call a function f : AS(x,y)→ R an alignment model.",2 Uni- and bigram alignment models,[0],[0]
"We call an alignment model f a unigram alignment model if f takes the form, for any Ax,y ∈ AS(x,y),
f(Ax,y) = k∑ i=1",2 Uni- and bigram alignment models,[0],[0]
"sim1(xi,yi) (1)
where sim1 is an arbitrary (real-valued) similarity function measuring similarity of two subsequences.",2 Uni- and bigram alignment models,[0],[0]
"We call an alignment model f a bigram alignment model if f takes the form
f(Ax,y) = k∑ i=1",2 Uni- and bigram alignment models,[0],[0]
"sim2 ( (xi,yi), (xi−1,yi−1) )",2 Uni- and bigram alignment models,[0],[0]
"(2)
where sim2 is an arbitrary (real-valued) similarity function measuring similarity of successive pairs of subsequences.
Example.",2 Uni- and bigram alignment models,[0],[0]
"Let sim1(u,v) be equal to |u| · |v| and let funi(Ax,y) be as in Eq. (1).",2 Uni- and bigram alignment models,[0],[0]
"Then, funi is a unigram alignment model that assigns the score
1This is sometimes denoted in the manner M -N (e.g., 3- 2, 1-0), indicating that M characters of one string may be matched up with N characters of the other string.",2 Uni- and bigram alignment models,[0],[0]
"Analogously, we could write here s-t rather than (s, t).
2Here, 1k denotes the unit vector of dimension k.",2 Uni- and bigram alignment models,[0],[0]
"given in Table 2.
Example.",1 + 1 + 0 + 1 + 1 + 1 + 2 = 7 to the alignment,[0],[0]
"Let sim2 ( (u,v), (u′,v′) )",1 + 1 + 0 + 1 + 1 + 1 + 2 = 7 to the alignment,[0],[0]
= (|u| · |v|)|v′| if |u| = |u′| − 1 or u = v and −2 otherwise.,1 + 1 + 0 + 1 + 1 + 1 + 2 = 7 to the alignment,[0],[0]
"Let fbi(Ax,y) be as in Eq.",1 + 1 + 0 + 1 + 1 + 1 + 2 = 7 to the alignment,[0],[0]
(2).,1 + 1 + 0 + 1 + 1 + 1 + 2 = 7 to the alignment,[0],[0]
"Then, fbi is a bigram alignment model assigning the score (1 · 1)0 + (1 · 1)1 + (0 · 2)1 + (1 · 1)2 + (1 · 1)1 + (1 · 1)1",1 + 1 + 0 + 1 + 1 + 1 + 2 = 7 to the alignment,[0],[0]
"− 2 = 3 to the alignment in Table 2.
",1 + 1 + 0 + 1 + 1 + 1 + 2 = 7 to the alignment,[0],[0]
"In statistical alignment modeling, the task is to find an optimal alignment (i.e., one with maximal score) given strings x and y and given the alignment model f .",1 + 1 + 0 + 1 + 1 + 1 + 2 = 7 to the alignment,[0],[0]
"When f is a unigram model, this can be solved efficiently via dynamic programming (DP).",1 + 1 + 0 + 1 + 1 + 1 + 2 = 7 to the alignment,[0],[0]
"When f is a bigram alignment model, then finding the optimal alignment can still be solved via DP, by introducing a variable Mijqw denoting the score of the best alignment of x(1 : i) and y(1 : j) that ends in the matchup of x(q : i) with y(w : j).3 The variable Mijqw satisfies a recurrence leading to a DP algorithm, shown in Algorithm 1.",1 + 1 + 0 + 1 + 1 + 1 + 2 = 7 to the alignment,[0],[0]
The actual alignment can be found by storing pointers to the maximizing steps taken.,1 + 1 + 0 + 1 + 1 + 1 + 2 = 7 to the alignment,[0],[0]
Running time of the algorithm is O(`2x`2y|S|).,1 + 1 + 0 + 1 + 1 + 1 + 2 = 7 to the alignment,[0],[0]
"Note also that the sketched algorithm is supervised insofar as it assumes that the similarity values sim2(·, ·) are known.",1 + 1 + 0 + 1 + 1 + 1 + 2 = 7 to the alignment,[0],[0]
"Typically, such alignment algorithms can be converted into unsupervised algorithms in which similarity measures sim are learnt iteratively, e.g., in an EM-like fashion (cf., e.g., Eger (2012), Eger (2013)); however, in this paper, we only investigate the supervised base version as indicated.",1 + 1 + 0 + 1 + 1 + 1 + 2 = 7 to the alignment,[0],[0]
Monotone alignments have a long tradition in NLP.,3 Related work,[0],[0]
"The classical Needleman-Wunsch algorithm (Needleman and Wunsch, 1970) computes the optimal alignment between two sequences when only single character matches, mismatches, and skips are allowed.",3 Related work,[0],[0]
"It is a special case of the unigram model (1) for which S = {(1, 0), (0, 1), (1, 1)} and sim1 takes on values from {0,−1}, depending on whether compared subsequences match or not.",3 Related work,[0],[0]
"As is well-known, this alignment specification is equivalent to the edit distance problem (Levenshtein, 1966) in which the minimal number of insertions, deletions and substitutions is sought that transforms one string
3We denote by x(a : b) the substring xaxa+1 · · ·xb of the string x1x2 · · ·xt.
into another.",3 Related work,[0],[0]
"Substring-to-substring edit operations — or equivalently, (monotone) many-tomany alignments — have appeared in the NLP context, e.g., in Deligne et al. (1995), Brill and Moore (2000), Jiampojamarn et al. (2007), Bisani and Ney (2008), Jiampojamarn et al. (2010), or, significantly earlier, in Ukkonen (1985), Véronis (1988).",3 Related work,[0],[0]
"Learning edit distance/monotone alignments in an unsupervised manner has been the topic of, e.g., Ristad and Yianilos (1998), Cotterell et al. (2014), besides the works already mentioned.",3 Related work,[0],[0]
"All of these approaches are special cases of our unigram model — i.e., they consider particular S (most prominently, S = {(1, 0), (0, 1), (1, 1)})",3 Related work,[0],[0]
"and sim1.4 Eger (2015b), Yao and Kondrak (2015), and Eger (2015a) generalize to alignments of multiple strings, but likewise only consider unigram alignment models in their experiments.
",3 Related work,[0],[0]
Probably the most closely related work to ours is Jiampojamarn and Kondrak (2010).,3 Related work,[0],[0]
"There, older and specialized alignment techniques such as ALINE (Kondrak, 2000) (as well as partly heuristic/semi-automatic alignment methods) are compared with variants of the M2M alignment algorithm, which we also survey.",3 Related work,[0],[0]
"This work does not consider supervised alignments or bigram alignments, as we do.",3 Related work,[0],[0]
"Moreover, Jiampojamarn and Kondrak (2010) also evaluate the impact of alignment quality on overall G2P system accuracy by running a few experiments, finding that better alignment quality does not always translate into better G2P accuracy, but that there is a “strong correlation” between the two.",3 Related work,[0],[0]
"We more thorougly investigate this question, using, arguably, more heterogeneous aligners, and many more experiments.",3 Related work,[0],[0]
"We also quantitatively estimate how alignment quality influences G2P system accuracy on two different languages via linear regression.
",3 Related work,[0],[0]
"Goldwater et al. (2006) study the effect of context in (unsupervised) word/sequence segmentation, which may be considered the onedimensional specialization of sequence alignment, using a Bayesian method.",3 Related work,[0],[0]
"They find that bigram models greatly outperform unigram models for their task.
",3 Related work,[0],[0]
"Of course, our study is also related to the field of machine translation and its studies on the rela-
4In Cotterell et al. (2014), context influences alignments, so that the approach goes beyond the unigram model sketched in (1) (but does not allow for many-to-many match-ups).",3 Related work,[0],[0]
"The contextual dependencies in this model are set up differently from the bigram dependencies in our paper.
",3 Related work,[0],[0]
Algorithm 1 1: procedure BIGRAM-ALIGN(x = x1 . . .,3 Related work,[0],[0]
"xn,y = y1 . . .",3 Related work,[0],[0]
"ym; S, sim2) 2:",3 Related work,[0],[0]
"Mijqw ← −∞ for all (i, j, q, w) ∈ Z4 3: M0000 ← 0 4: for i = 0 . . .",3 Related work,[0],[0]
n,3 Related work,[0],[0]
do 5: for j = 0 . .,3 Related work,[0],[0]
.m,3 Related work,[0],[0]
do 6: for q = 0 . . .,3 Related work,[0],[0]
i+ 1 do 7: for w = 0 . . .,3 Related work,[0],[0]
"j + 1 do 8: if (i, j, q, w) 6=",3 Related work,[0],[0]
"(0, 0, 0, 0) then 9: if (i− q + 1, j − w + 1) ∈ S then
10: Mijqw= max (a,b)∈S Mq−1,w−1,q−a,w−b+sim2
(( x(q:i),y(w:j) ) , ( x(q−a:q−1),y(w−b:w−1) ))
",3 Related work,[0],[0]
"tionship between alignment quality and translation performance (Ganchev et al., 2008).",3 Related work,[0],[0]
"In machine translation, the monotonicity assumption of string transduction does typically not hold, however, rendering alignment and translation techniques different and more heuristic in nature.",3 Related work,[0],[0]
"For English, we conduct experiments on the General American (GA) variant of the Combilex data set (Richmond et al., 2009).",4.1 Data,[0],[0]
This contains about 128 000 grapheme-phoneme pairs as exemplified in Table 3.,4.1 Data,[0],[0]
"Importantly, Combilex provides goldstandard alignments, which we will make use of for the supervised alignment models as well as for measuring alignment quality.",4.1 Data,[0],[0]
"For German, we ran-
domly extract 3 000 G2P string pairs from CELEX (Baayen et al., 1995).",4.1 Data,[0],[0]
"We had a native speaker manually align them so that gold standard alignments are available here, too.",4.1 Data,[0],[0]
"Both data sets contain quite complex match-ups of character subsequences such as (2,3) as in English s-oi-r-ee-s/swOA-r-P-z or (4,1) as in w-eigh-t/w-P-t but the majority of match-ups are of type (1,1), (2,1), and, to a lesser degree, (1,2) and (3,1).",4.1 Data,[0],[0]
"The M2M aligner (Jiampojamarn et al., 2007), which is based on EM maximum likelihood estimation of alignment parameters, is the classical unsupervised unigram many-to-many aligner in G2P.",4.2 Alignment toolkits/models,[0],[0]
"As has been pointed out (Kubo et al., 2011), M2M greatly overfits the data.5",4.2 Alignment toolkits/models,[0],[0]
"This means that when the M2M aligner is given the freedom to align two sequences without restrictions, it matches them up as a whole.",4.2 Alignment toolkits/models,[0],[0]
"The reason is that a (probabilistic) unigram alignment model adds log-probabilities of matched-up subsequences, which, if not appropriately corrected for, makes alignments with few match-ups a priori more likely than alignments with many matchups, when probabilities of individual match-ups are uniformly or randomly initialized (as is typically the case for EM maximum likelihood estimation in unsupervised models).",4.2 Alignment toolkits/models,[0],[0]
"To address this, M2M must artifically restrain, in our language, the set S to be {(1, 1), (1, 2), (2, 1)}.",4.2 Alignment toolkits/models,[0],[0]
"In contrast, the Mpaligner (Kubo et al., 2011) introduces a prior (or penalty) in the alignment model which favors ‘short’ matches (s, t) over ‘long’ ones.",4.2 Alignment toolkits/models,[0],[0]
"Finally, the Phonetisaurus aligner (Novak et al., 2012) modifies the M2M aligner by adding additional soft constraints.
",4.2 Alignment toolkits/models,[0],[0]
"Our own alignment model is, as indicated, supervised.",4.2 Alignment toolkits/models,[0],[0]
"We implement a unigram alignment model where we specify sim1(u,v) as
α · logp((u,v))",4.2 Alignment toolkits/models,[0],[0]
"+ β · logp((|u|, |v|))",4.2 Alignment toolkits/models,[0],[0]
"+γ · logp(u) + δ · logp(v).
",4.2 Alignment toolkits/models,[0],[0]
"Here, logp(z) denotes the log-probability — estimated from the training data — of observing the
5See also the discussion in (Goldwater et al., 2006) for the related word segmentation problem.
object z, and α, β, γ and δ are parameters.",4.2 Alignment toolkits/models,[0],[0]
"This specification says that the subsequences u and v are similar insofar as (i) u and v have been paired frequently in the training data, (ii) the length of u and the length of v have been paired frequently, (iii)/(iv) u/v by itself is likely.",4.2 Alignment toolkits/models,[0],[0]
"We refer to this unigram alignment model as uniα,β,γ,δ.",4.2 Alignment toolkits/models,[0],[0]
"We also implement a bigram alignment model where we specify sim2 ( (u,v), (u′,v′) )",4.2 Alignment toolkits/models,[0],[0]
"as
α · logp((u,v) | (u′,v′))",4.2 Alignment toolkits/models,[0],[0]
"+β · logp((|u|, |v|)",4.2 Alignment toolkits/models,[0],[0]
"| (|u′|, |v′|))",4.2 Alignment toolkits/models,[0],[0]
"+γ · logp(u|u′)+ δ · logp(v|v′).
",4.2 Alignment toolkits/models,[0],[0]
"Here, logp(z | z′) denotes the logarithm of the conditional probability of observing the object z following the object z′.",4.2 Alignment toolkits/models,[0],[0]
"We refer to this bigram alignment model as biα,β,γ,δ.",4.2 Alignment toolkits/models,[0],[0]
We use two string transduction systems for our experiments.,4.3 Transduction systems,[0],[0]
"The first one is DirecTL+ (Jiampojamarn et al., 2010), a discriminative string-tostring translation system incorporating joint ngram features.",4.3 Transduction systems,[0],[0]
DirecTL+ is an extension of the model presented in Jiampojamarn et al. (2008) which treats string transduction as a source sequence segmentation and subsequent sequence labeling task.,4.3 Transduction systems,[0],[0]
"In addition, we use Phonetisaurus (Novak et al., 2012), a weighted finite state-based joint n-gram model employing recurrent neural network language model N -best rescoring in decoding.",4.3 Transduction systems,[0],[0]
Both systems take aligned pairs of strings as input and from this construct a monotone translation model.6,4.3 Transduction systems,[0],[0]
We employ two measures of alignment quality.,4.4 Measuring alignment quality,[0],[0]
"First, we use word accuracy, defined as the fraction of correctly aligned sequence pairs in a test sample.",4.4 Measuring alignment quality,[0],[0]
This is a very strict measure that penalizes even tiny deviations from the gold standard.,4.4 Measuring alignment quality,[0],[0]
"Additionally, we measure the edit distance between the true alignment Ax,y and the predicted alignment Âx,y. To implement this, we view the two induced segmentations that constitute an alignment — e.g., (ph,oe,n,i,x) and (f,i,n,I,ks) — as strings including splitting signs.",4.4 Measuring alignment quality,[0],[0]
"Thus, we can compute the edit distance between the gold-standard segmented x
6We run both systems with parameters determined by some manual tuning, without trying to systematically optimize their individual performances, however.
string and the predicted segmentation, and analogously for the y sequence.",4.4 Measuring alignment quality,[0],[0]
"Then, we define the edit distance between Ax,y and Âx,y as the sum of these two string edit distances.",4.4 Measuring alignment quality,[0],[0]
"For a test sample, we indicate so-defined average edit distance, averaged over all pairs in the sample.",4.4 Measuring alignment quality,[0],[0]
"To measure alignment quality for the different systems, for English, we run experiments on sets of size x+5 000, where x = 1 000, 2 000, 5 000, 10 000, and 20 000.",5.1 Alignment quality,[0],[0]
"For the supervised models, we consider x as the training data and the 5 000 additional string pairs as test data.7 To quantify effects when training data is very little, we let x also range over 100 and 500 string pairs for the supervised models.",5.1 Alignment quality,[0],[0]
"For the unsupervised models, we simply take all x+5 000 string pairs as data to learn from (but evaluate performance only on the 5 000 string pairs, for comparability).
",5.1 Alignment quality,[0],[0]
"Results are shown in Tables 4, 5, and 6.",5.1 Alignment quality,[0],[0]
"We first note (Table 4) that the unsupervised models perform decently, obtaining accuracy rates of 80% and beyond under appropriate parametrizations.",5.1 Alignment quality,[0],[0]
"We also observe the M2M aligner’s deterioration in performance as we increase its degrees of freedom (allowing it to match subsequences of larger length), confirming our previous remarks.",5.1 Alignment quality,[0],[0]
The Mpaligner does not suffer from this problem as it penalizes large matches.,5.1 Alignment quality,[0],[0]
"Phonetisaurus suffers from the same problems as M2M, but to a lesser degree.",5.1 Alignment quality,[0],[0]
"Overall, we find that, under optimal parametrizations, Phonetisaurus produces best alignments, followed by Mpalign and M2M.",5.1 Alignment quality,[0],[0]
"However, peak performances of all three unsupervised aligners are close.",5.1 Alignment quality,[0],[0]
"Unsurprisingly, the supervised alignment models perform better than the unsupervised ones (Tables 5 and 6).",5.1 Alignment quality,[0],[0]
"Surprisingly, however, they do so with very little training data; fewer than 100 aligned string pairs suffice to outperform the unsupervised models under good calibrations.",5.1 Alignment quality,[0],[0]
"When there is sufficient training data, the supervised models perform splendidly, with a peak accuracy of 99.43% for the bigram alignment model that includes appropriate features (scoring lengths of aligned subsequences,
7For all our below experiments involving the supervised aligners, we set S to a (‘pessimistically’ large) value of {(a, b) | 1 ≤ a ≤ 6, 1 ≤ b ≤ 6}.",5.1 Alignment quality,[0],[0]
"Also, for the bigram models, we add special sequence boundary markers.
etc.).",5.1 Alignment quality,[0],[0]
"We also note that the bigram alignment model is almost consistently better than the unigram alignment model, with a surplus of about 1% point, depending on specific parametrizations.
",5.1 Alignment quality,[0],[0]
We performed an analogous analysis for the German data.,5.1 Alignment quality,[0],[0]
"Results are quite similar except that unigram and bigram alignment model have indistinguishable performance on the German data, indicating (the known fact) that G2P is a more complex task in English, apparently not requiring bigram alignment models.
",5.1 Alignment quality,[0],[0]
"Error analysis Concerning errors that the unigram model commits and the bigram model does not, the majority of errors (roughly 80%) involve match-ups of ed/d and d.",5.1 Alignment quality,[0],[0]
"For example, the unigram model aligns as in
t w",5.1 Alignment quality,[0],[0]
i n k le d t,5.1 Alignment quality,[0],[0]
w I N k @l,5.1 Alignment quality,[0],[0]
"d
while the gold-standard alignment is
t w i n k",5.1 Alignment quality,[0],[0]
l ed t w,5.1 Alignment quality,[0],[0]
I N k @l,5.1 Alignment quality,[0],[0]
"d
While all match-ups in both alignments are plausible, the bigram model assigns here higher probability to the correct ed/d match-up in terminal position (consistently favored in the data set), which
has a particular meaning there, namely, that of a suffix marker for past tense.8,9 In the German data, there is a single instance where the unigram and bigram alignment model disagree, namely, in the alignment of s-t-o-ff-f-l-a-sch-e/S-t-O-f-f-l-&S-@, which the unigram model falsely aligns as s-t-o-f-ff-l-a-sch-e/S-t-O-f-f-l-&-S-@; note that in the correct alignment f must follow ff, not vice versa, which depends on context information, e.g., that o/O signifies a short vowel which is followed by a double consonant, not a single consonant.
",5.1 Alignment quality,[0],[0]
"All remaining errors that the bigram alignment models commits are, for the best considered parametrization and training set size, typically due to match-up types not seen in the training data, and thus mostly concern foreign names or writings (e.g., Bh-u-tt-o/b-u-t-F, falsely aligned as B-hu-tto/b-u-t-F).",5.1 Alignment quality,[0],[0]
"A few other errors might be corrected when the feature coefficients α, β, γ, δ were optimized on a development set rather than set manually.",5.1 Alignment quality,[0],[0]
"We find no indication that our G2P data, either for English or German, would further benefit from n-gram alignment models of order n > 2.",5.1 Alignment quality,[0],[0]
"Next, we estimate the relationship between alignment quality and overall G2P performance (transcription accuracy).",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"To this end, for the English data, we use the 5 000 aligned string pairs from the previous experiment on alignment quality and feed them in — as training data — to either DirecTL+ or Phonetisaurus as outlined in Section 4.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
We then evaluate G2P performance — in terms of word accuracy (fraction of correctly transcribed strings) — on a distinct test set of size 10 000.,5.2 Alignment quality vs. overall G2P performance,[0],[0]
Figure 1 shows a plot of overall G2P accuracy vs. training set size for the aligner (ranging over the x values in the last section); and a second plot that sketches G2P accuracy as a function of corresponding alignment accuracy.,5.2 Alignment quality vs. overall G2P performance,[0],[0]
"We first note that, as the supervised aligner receives more training
8Similar cases are, e.g., alignments of the type f-ee-d-ba-ck/f-i-d-b-a-k, which the unigram model falsely aligns as f-e-ed-b-a-ck/f-i-d-b-a-k. Here, too, the unigram is unable to account for the almost exclusive terminal position of the ed/d match-up in the data.
",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"9Other errors involve ‘unusual/foreign’ spelling/pronunciation pairs such as Ph-oe-n-i-c-ia/f-@n-i-S-@ (wrongly aligned as Ph-o-en-i-c-ia/f-@-n-i-S-@ by the unigram model) or m-a-d-e-m-oi-s-e-ll-e-’s/m-a-d-@-mw@-z-E-l-0-z (m-a-d-e-m-o-i-s-e-ll-e-’s/m-a-d-@-m-w-@z-E-l-0-z), where the bigram alignment model has apparently gathered the more appropriate statistics.
data from which to align the 5 000 string pairs, the overall G2P accuracy of both DirecTL+ and Phonetisaurus increase substantially (and as a convex function of training set size).",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"Apparently, the better alignments produced by more training data for the particular supervised aligner considered directly translate into better overall G2P accuracy.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"The other plot in the figure shows that, indeed, there seems to be a linear trend coupling alignment quality with overall G2P performance.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"Table 7 pairs G2P accuracy with alignment accuracy of selected systems, all run in the x = 20 000 setting.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"While, in the table, better alignments do not necessarily imply better overall G2P performance, the two best alignments also lead to the two best overall G2P performances (although, in this case, the second best alignment is paired with the best overall G2P performance); conversely, the worst alignment quality is coupled with the worst overall G2P performance.
",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"Overall, we ran 249 experiments (including the German data) in which we trained DirecTL+ or Phonetisaurus with alignments of specific quali-
ties obtained from particularly parametrized aligners.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"In each of these cases, we obtained an alignment quality score and a subsequent overall G2P system performance.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
The English part of this data is sketched in Figure 2.,5.2 Alignment quality vs. overall G2P performance,[0],[0]
"This figure seems to corroborate the linear relationship (apparently present in Figure 1) between alignment quality and overall G2P system accuracy, particularly, when alignment quality is measured in the more finegrained metric of edit distance.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"To formally test
this, we regress overall G2P system performance (measured in word accuracy) on edit distance and other variables.10 This yielded the coefficients as given in Table 8; in each case, the goodness-offit of the linear model was quite large, with R2 values above 90% for the English data and about 84% for the German data.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"Also, the coefficients on alignment quality were highly significantly different from zero.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"The table shows that the coefficients are on the order of about −3.80% to −4.70%, meaning that, all else being equal, increasing alignment quality by 1 edit distance to the gold-standard alignment increases overall G2P by about 3.80 to 4.70%.
",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"So far, we have estimated the effects of alignment quality on overall G2P system performance for a fixed size of training data, namely, 5 000 aligned string pairs.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"To see whether this relationship changes when we vary the amount of training data, we run several more experiments.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"In these, we align training sets of sizes 100, 500,
10These include binary dummy variables for the specific systems as well as alignment consistency and its square — measured in conditional entropy H(Y |X)",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"(Pervouchine et al., 2009) — in the regression.
",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"1 000, 2 000, 10 000, 20 000, 40 000 and 60 000 via our several alignment systems.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
Then we feed the aligned data to the Phonetisaurus system (we omit DirecTL+ here because of its long run times) and compute overall G2P accuracy on a disjoint test set of size 28 000 approximately.,5.2 Alignment quality vs. overall G2P performance,[0],[0]
"This time, we only use the unsupervised aligners and the gold-standard alignments directly, omitting results for our various supervised aligners.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"Note, however, that these aligners could, in principle, imitate the gold-standard alignments with a very high degree of precision, as previously seen.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"Table 9
shows that training G2P systems from the human gold standard alignments in each case yields better overall G2P transcriptions than training them from either of the three unsupervised alignments considered here.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"However, we note that the surplus over the unsupervised alignments decreases as training set size increases.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"This may be due to the fact that the unsupervised aligners themselves create better alignments once they are boot-
strapped from larger data sets (cf. Table 4).",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"Additionally, the effect of alignment quality on overall G2P system performance may simply vanish as training set sizes become large enough because the translation modules can better accomodate ‘noisy’ data as long as its size is sufficiently large.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"Figure
3 sketches the decreasing influence of alignment system on overall G2P system performance as size of the aligned data increases.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
We have investigated the need for bigram alignment models and the benefit of supervised alignment techniques in G2P. We have also quantitatively estimated the relationship between alignment quality and overall G2P system performance.,6 Conclusion,[0],[0]
"We have found that, in English, bigram alignment models do perform better than unigram alignment models on the G2P task (we find almost no differences between unigram and bigram models for the German sample of G2P data we considered).",6 Conclusion,[0],[0]
"Moreover, we have found that supervised alignment techniques may perform considerably better than their unsupervised brethren and that few manually aligned training pairs suffice for them to do so.",6 Conclusion,[0],[0]
"Finally, we have estimated a highly significant impact of alignment quality on overall G2P transcription performance and that this relationship is linear in nature.",6 Conclusion,[0],[0]
"At a particular training size, a linear regression model has estimated that improving alignment quality by 1 edit distance toward the
gold standard alignments leads to an 3.80-4.70% increase in G2P transcription accuracy.",6 Conclusion,[0],[0]
"However, we have also found that the importance of good alignments on G2P accuracy appears to dimish as data set size increases, possibly because the translation modules can accomodate more ‘noisy’ data in this scenario.
",6 Conclusion,[0],[0]
"As a ‘policy’ implication, we recommend the use of supervised alignment techniques particularly when the size of the G2P corpus is small or when high quality alignments, as an end in themselves, are required.",6 Conclusion,[0],[0]
"In this case, constructing a few dozen or few hundred alignments in an unsupervised manner and correcting them by hand (to serve as an input for a supervised technique) may be highly beneficial.
",6 Conclusion,[0],[0]
"In future work, it may be worthwhile to study the impact of alignment techniques on overall system performance in other string transduction problems such as transliteration, lemmatization, and spelling error correction.
",6 Conclusion,[0],[0]
Our supervised uni- and bigram aligners are available via https://github.com/ SteffenEger/.,6 Conclusion,[0],[0]
I thank three anonymous reviewers and Tim vor der Brück for valuable suggestions.,Acknowledgments,[0],[0]
We investigate the need for bigram alignment models and the benefit of supervised alignment techniques in graphemeto-phoneme (G2P) conversion.,abstractText,[0],[0]
"Moreover, we quantitatively estimate the relationship between alignment quality and overall G2P system performance.",abstractText,[0],[0]
"We find that, in English, bigram alignment models do perform better than unigram alignment models on the G2P task.",abstractText,[0],[0]
"Moreover, we find that supervised alignment techniques may perform considerably better than their unsupervised brethren and that few manually aligned training pairs suffice for them to do so.",abstractText,[0],[0]
"Finally, we estimate a highly significant impact of alignment quality on overall G2P transcription performance and that this relationship is linear in nature.",abstractText,[0],[0]
Do we need bigram alignment models? On the effect of alignment quality on transduction accuracy in G2P,title,[0],[0]
"Ambiguity is one of the defining characteristics of human languages, and language understanding crucially relies on the ability to obtain unambiguous representations of linguistic content.",1 Introduction,[0],[0]
"While some ambiguities can be resolved using intra-linguistic contextual cues, the disambiguation of many linguistic constructions requires integration of world knowledge and perceptual information obtained from other modalities.
",1 Introduction,[0],[0]
"In this work, we focus on the problem of grounding language in the visual modality, and introduce a novel task for language understanding which requires resolving linguistic ambiguities by utilizing the visual context in which the linguistic content is expressed.",1 Introduction,[0],[0]
"This type of inference is frequently called for in human communication that occurs in a visual environment, and is crucial for language acquisition, when much of the linguistic content refers to the visual surroundings of the child (Snow, 1972).
",1 Introduction,[0],[0]
"Our task is also fundamental to the problem of grounding vision in language, by focusing on phenomena of linguistic ambiguity, which are prevalent in language, but typically overlooked when using language as a medium for expressing understanding of visual content.",1 Introduction,[0],[0]
"Due to such ambiguities, a superficially appropriate description of a visual scene may in fact not be sufficient for demonstrating a correct understanding of the relevant visual content.",1 Introduction,[0],[0]
"Our task addresses this issue by introducing a deep validation protocol for visual understanding, requiring not only providing a surface description of a visual activity but also demonstrating structural understanding at the levels of syntax, semantics and discourse.
",1 Introduction,[0],[0]
"To enable the systematic study of visually grounded processing of ambiguous language, we create a new corpus, LAVA (Language and Vision Ambiguities).",1 Introduction,[0],[0]
This corpus contains sentences with linguistic ambiguities that can only be resolved using external information.,1 Introduction,[0],[0]
The sentences are paired with short videos that visualize different interpretations of each sentence.,1 Introduction,[0],[0]
"Our sentences encompass a wide range of syntactic, semantic and dis-
ar X
iv :1
60 3.
08 07
9v 1
[ cs
.C",1 Introduction,[0],[0]
"V
] 2
6 M
ar 2
course ambiguities, including ambiguous prepositional and verb phrase attachments, conjunctions, logical forms, anaphora and ellipsis.",1 Introduction,[0],[0]
"Overall, the corpus contains 237 sentences, with 2 to 3 interpretations per sentence, and an average of 3.37 videos that depict visual variations of each sentence interpretation, corresponding to a total of 1679 videos.
",1 Introduction,[0],[0]
"Using this corpus, we address the problem of selecting the interpretation of an ambiguous sentence that matches the content of a given video.",1 Introduction,[0],[0]
"Our approach for tackling this task extends the sentence tracker introduced in (Siddharth et al., 2014).",1 Introduction,[0],[0]
The sentence tracker produces a score which determines if a sentence is depicted by a video.,1 Introduction,[0],[0]
This earlier work had no concept of ambiguities; it assumed that every sentence had a single interpretation.,1 Introduction,[0],[0]
"We extend this approach to represent multiple interpretations of a sentence, enabling us to pick the interpretation that is most compatible with the video.
",1 Introduction,[0],[0]
"To summarize, the contributions of this paper are threefold.",1 Introduction,[0],[0]
"First, we introduce a new task for visually grounded language understanding, in which an ambiguous sentence has to be disambiguated using a visual depiction of the sentence’s content.",1 Introduction,[0],[0]
"Second, we release a multimodal corpus of sentences coupled with videos which covers a wide range of linguistic ambiguities, and enables a systematic study of linguistic ambiguities in visual contexts.",1 Introduction,[0],[0]
"Finally, we present a computational model which disambiguates the sentences in our corpus with an accuracy of 75.36%.",1 Introduction,[0],[0]
"Previous language and vision studies focused on the development of multimodal word and sentence representations (Bruni et al., 2012; Socher et al., 2013; Silberer and Lapata, 2014; Gong et al., 2014; Lazaridou et al., 2015), as well as methods for describing images and videos in natural language (Farhadi et al., 2010; Kulkarni et al., 2011; Mitchell et al., 2012; Socher et al., 2014; Thomason et al., 2014; Karpathy and Fei-Fei, 2014; Siddharth et al., 2014; Venugopalan et al., 2015; Vinyals et al., 2015).",2 Related Work,[0],[0]
"While these studies handle important challenges in multimodal processing of language and vision, they do not provide explicit modeling of linguistic ambiguities.
",2 Related Work,[0],[0]
"Previous work relating ambiguity in language to the visual modality addressed the problem of word
sense disambiguation (Barnard et al., 2003).",2 Related Work,[0],[0]
"However, this work is limited to context independent interpretation of individual words, and does not consider structure-related ambiguities.",2 Related Work,[0],[0]
"Discourse ambiguities were previously studied in work on multimodal coreference resolution (Ramanathan et al., 2014; Kong et al., 2014).",2 Related Work,[0],[0]
"Our work expands this line of research, and addresses further discourse ambiguities in the interpretation of ellipsis.",2 Related Work,[0],[0]
"More importantly, to the best of our knowledge our study is the first to present a systematic treatment of syntactic and semantic sentence level ambiguities in the context of language and vision.
",2 Related Work,[0],[0]
"The interactions between linguistic and visual information in human sentence processing have been extensively studied in psycholinguistics and cognitive psychology (Tanenhaus et al., 1995).",2 Related Work,[0],[0]
"A considerable fraction of this work focused on the processing of ambiguous language (Spivey et al., 2002; Coco and Keller, 2015), providing evidence for the importance of visual information for linguistic ambiguity resolution by humans.",2 Related Work,[0],[0]
"Such information is also vital during language acquisition, when much of the linguistic content perceived by the child refers to their immediate visual environment (Snow, 1972).",2 Related Work,[0],[0]
"Over time, children develop mechanisms for grounded disambiguation of language, manifested among others by the usage of iconic gestures when communicating ambiguous linguistic content (Kidd and Holler, 2009).",2 Related Work,[0],[0]
Our study leverages such insights to develop a complementary framework that enables addressing the challenge of visually grounded disambiguation of language in the realm of artificial intelligence.,2 Related Work,[0],[0]
In this work we provide a concrete framework for the study of language understanding with visual context by introducing the task of grounded language disambiguation.,3 Task,[0],[0]
This task requires to choose the correct linguistic representation of a sentence given a visual context depicted in a video.,3 Task,[0],[0]
"Specifically, provided with a sentence, n candidate interpretations of that sentence and a video that depicts the content of the sentence, one needs to choose the interpretation that corresponds to the content of the video.
",3 Task,[0],[0]
"To illustrate this task, consider the example in figure 1, where we are given the sentence “Sam approached the chair with a bag” along with two different linguistic interpretations.",3 Task,[0],[0]
"In the first in-
terpretation, which corresponds to parse 1(a), Sam has the bag.",3 Task,[0],[0]
"In the second interpretation associated with parse 1(b), the bag is on the chair rather than with Sam.",3 Task,[0],[0]
"Given the visual context from figure 1(c), the task is to choose which interpretation is most appropriate for the sentence.",3 Task,[0],[0]
"To address the grounded language disambiguation task, we use a compositional approach for determining if a specific interpretation of a sentence is depicted by a video.",4 Approach Overview,[0],[0]
"In this framework, described in detail in section 6, a sentence and an accompanying interpretation encoded in first order logic, give rise to a grounded model that matches a video against the provided sentence interpretation.
",4 Approach Overview,[0],[0]
"The model is comprised of Hidden Markov Models (HMMs) which encode the semantics of words, and trackers which locate objects in video frames.",4 Approach Overview,[0],[0]
"To represent an interpretation of a sentence, word models are combined with trackers through a cross-product which respects the semantic representation of the sentence to create a single model which recognizes that interpretation.
",4 Approach Overview,[0],[0]
"Given a sentence, we construct an HMM based representation for each interpretation of that sentence.",4 Approach Overview,[0],[0]
We then detect candidate locations for objects in every frame of the video.,4 Approach Overview,[0],[0]
"Together the re-
S
NP NNP Bill
VP
VBD held
NP
DT the
NP
JJ
green
NP
NN chair CC and NN bag
(a) First interpretation
S
NP NNP Bill
VP
VBD held
NP
DT the
NP
NP
JJ
green
NN chair
CC and NN bag
(b) Second interpretation
forestation for the sentence and the candidate object locations are combined to form a model which can determine if a given interpretation is depicted by the video.",4 Approach Overview,[0],[0]
We test each interpretation and report the interpretation with highest likelihood.,4 Approach Overview,[0],[0]
"To enable a systematic study of linguistic ambiguities that are grounded in vision, we compiled a corpus with ambiguous sentences describing visual actions.",5 Corpus,[0],[0]
"The sentences are formulated such that the correct linguistic interpretation of each sentence can only be determined using external, non-linguistic, information about the depicted activity.",5 Corpus,[0],[0]
"For example, in the sentence “Bill held the green chair and bag”, the correct scope of “green” can only be determined by integrating additional information about the color of the bag.",5 Corpus,[0],[0]
"This information is provided in the accompanying videos, which visualize the possible interpretations of each sentence.",5 Corpus,[0],[0]
Figure 2 presents the syntactic parses for this example along with frames from the respective videos.,5 Corpus,[0],[0]
"Although our videos contain visual uncertainty, they are not ambiguous with respect to the linguistic interpretation they are presenting, and hence a video always corresponds to a single candidate representation of a sentence.
",5 Corpus,[0],[0]
"The corpus covers a wide range of well
known syntactic, semantic and discourse ambiguity classes.",5 Corpus,[0],[0]
"While the ambiguities are associated with various types, different sentence interpretations always represent distinct sentence meanings, and are hence encoded semantically using first order logic.",5 Corpus,[0],[0]
"For syntactic and discourse ambiguities we also provide an additional, ambiguity type specific encoding as described below.
",5 Corpus,[0],[0]
"• Syntax Syntactic ambiguities include Prepositional Phrase (PP) attachments, Verb Phrase (VP) attachments, and ambiguities in the interpretation of conjunctions.",5 Corpus,[0],[0]
"In addition to logical forms, sentences with syntactic ambiguities are also accompanied with Context Free Grammar (CFG) parses of the candidate interpretations, generated from a deterministic CFG parser.
",5 Corpus,[0],[0]
• Semantics,5 Corpus,[0],[0]
"The corpus addresses several classes of semantic quantification ambiguities, in which a syntactically unambiguous sentence may correspond to different logical forms.",5 Corpus,[0],[0]
"For each such sentence we provide the respective logical forms.
",5 Corpus,[0],[0]
• Discourse,5 Corpus,[0],[0]
"The corpus contains two types of discourse ambiguities, Pronoun Anaphora and Ellipsis, offering examples comprising two sentences.",5 Corpus,[0],[0]
"In anaphora ambiguity cases, an ambiguous pronoun in the second sentence is given its candidate antecedents in the first sentence, as well as a corresponding logical form for the meaning of the second sentence.",5 Corpus,[0],[0]
"In ellipsis cases, a part of the second sentence, which can constitute either the subject and the verb, or the verb and the object, is omitted.",5 Corpus,[0],[0]
"We provide both interpretations of the omission in the form of a single unambiguous sentence, and its logical form, which combines the meanings of the first and the second sentences.
",5 Corpus,[0],[0]
"Table 2 lists examples of the different ambiguity classes, along with the candidate interpretations of each example.
",5 Corpus,[0],[0]
The corpus is generated using Part of Speech (POS) tag sequence templates.,5 Corpus,[0],[0]
"For each template, the POS tags are replaced with lexical items from the corpus lexicon, described in table 3, using all the visually applicable assignments.",5 Corpus,[0],[0]
"This generation process yields an overall of 237 sentences,
of which 213 sentences have 2 candidate interpretations, and 24 sentences have 3 interpretations.",5 Corpus,[0],[0]
"Table 1 presents the corpus templates for each ambiguity class, along with the number of sentences generated from each template.
",5 Corpus,[0],[0]
The corpus videos are filmed in an indoor environment containing background objects and pedestrians.,5 Corpus,[0],[0]
"To account for the manner of performing actions, videos are shot twice with different actors.",5 Corpus,[0],[0]
"Whenever applicable, we also filmed the actions from two different directions (e.g. approach from the left, and approach from the right).",5 Corpus,[0],[0]
"Finally, all videos were shot with two cameras from two different view points.",5 Corpus,[0],[0]
"Taking these variations into account, the resulting video corpus contains 7.1 videos per sentence and 3.37 videos per sentence interpretation, corresponding to a total of 1679 videos.",5 Corpus,[0],[0]
"The average video length is 3.02 seconds (90.78 frames), with in an overall of 1.4 hours of footage (152434 frames).
",5 Corpus,[0],[0]
"A custom corpus is required for this task because no existing corpus, containing either videos or images, systematically covers multimodal ambiguities.",5 Corpus,[0],[0]
"Datasets such as UCF Sports (Rodriguez et al., 2008), YouTube (Liu et al., 2009), and HMDB (Kuehne et al., 2011) which come out of the activity recognition community are accompanied by action labels, not sentences, and do not control for the content of the videos aside from the principal action being performed.",5 Corpus,[0],[0]
"Datasets for image and video captioning, such as MSCOCO (Lin et al., 2014) and TACOS (Regneri et al., 2013),
aim to control for more aspects of the videos than just the main action being performed but they do not provide the range of ambiguities discussed here.",5 Corpus,[0],[0]
"The closest dataset is that of Siddharth et al. (2014) as it controls for object appearance, color, action, and direction of motion, making it more likely to be suitable for evaluating disambiguation tasks.",5 Corpus,[0],[0]
"Unfortunately, that dataset was designed to avoid ambiguities, and therefore is not suitable for evaluating the work described here.",5 Corpus,[0],[0]
"To perform the disambiguation task, we extend the sentence recognition model of Siddharth et al. (2014) which represents sentences as compositions of words.",6 Model,[0],[0]
"Given a sentence, its first order logic interpretation and a video, our model produces a score which determines if the sentence is depicted by the video.",6 Model,[0],[0]
It simultaneously tracks the participants in the events described by the sentence while recognizing the events themselves.,6 Model,[0],[0]
"This al-
lows it to be flexible in the presence of noise by integrating top-down information from the sentence with bottom-up information from object and property detectors.",6 Model,[0],[0]
"Each word in the query sentence is represented by an HMM (Baum et al., 1970), which recognizes tracks (i.e. paths of detections in a video for a specific object) that satisfy the semantics of the given word.",6 Model,[0],[0]
"In essence, this model can be described as having two layers, one in which object tracking occurs and one in which words observe tracks and filter tracks that do not satisfy the word constraints.
",6 Model,[0],[0]
"Given a sentence interpretation, we construct a sentence-specific model which recognizes if a video depicts the sentence as follows.",6 Model,[0],[0]
"Each predicate in the first order logic formula has a corresponding HMM, which can recognize if that predicate is true of a video given its arguments.",6 Model,[0],[0]
"Each variable has a corresponding tracker which attempts to physically locate the bounding box corresponding to that variable in each frame of a
video.",6 Model,[0],[0]
This creates a bipartite graph: HMMs that represent predicates are connected to trackers that represent variables.,6 Model,[0],[0]
"The trackers themselves are similar to the HMMs, in that they comprise a lattice of potential bounding boxes in every frame.",6 Model,[0],[0]
"To construct a joint model for a sentence interpretation, we take the cross product of HMMs and trackers, taking only those cross products dictated by the structure of the formula corresponding to the desired interpretation.",6 Model,[0],[0]
"Given a video, we employ an object detector to generate candidate detections in each frame, construct trackers which select one of these detections in each frame, and finally construct the overall model from HMMs and trackers.
",6 Model,[0],[0]
"Provided an interpretation and its corresponding formula composed of P predicates and V variables, along with a collection of object detections, bframedetection index, in each frame of a video of length T the model computes the score of the videosentence pair by finding the optimal detection for each participant in every frame.",6 Model,[0],[0]
"This is in essence the Viterbi algorithm (Viterbi, 1971), the MAP algorithm for HMMs, applied to finding optimal object detections jframevariable for each participant, and the optimal state kframepredicate for each predicate HMM, in every frame.",6 Model,[0],[0]
"Each detection is scored by its confidence from the object detector, f and each object track is scored by a motion coherence metric g which determines if the motion of the track agrees with the underlying optical flow.",6 Model,[0],[0]
"Each predicate,
p, is scored by the probability of observing a particular detection in a given state hp, and by the probability of transitioning between states ap.",6 Model,[0],[0]
"The structure of the formula and the fact that multiple predicates often refer to the same variables is recorded by θ, a mapping between predicates and their arguments.",6 Model,[0],[0]
"The model computes the MAP estimate as:
max j11 ,..., j T 1
...",6 Model,[0],[0]
"j1V ,..., j T V
max k11,..., k T 1
... k1P ,..., k T P
V∑ v=1 T∑ t=1",6 Model,[0],[0]
f(btjtv ),6 Model,[0],[0]
+ T∑ t=2 g(bt−1,6 Model,[0],[0]
"jt−1v , btjtv )+
P∑ p=1 T∑ t=1 hp(k t p, b t jt θ1p , btjt θ2p ) + T∑ t=2 ap(k t−1 p , k t p)
for sentences which have words that refer to at most two tracks (i.e. transitive verbs or binary predicates) but is trivially extended to arbitrary arities.",6 Model,[0],[0]
"Figure 3 provides a visual overview of the model as a cross-product of tracker models and word models.
",6 Model,[0],[0]
Our model extends the approach of Siddharth et al. (2014) in several ways.,6 Model,[0],[0]
"First, we depart from the dependency based representation used in that work, and recast the model to encode first order logic formulas.",6 Model,[0],[0]
Note that some complex first order logic formulas cannot be directly encoded in the model and require additional inference steps.,6 Model,[0],[0]
"This extension enables us to represent ambiguities in which a given sentence has multiple logical interpretations for the same syntactic parse.
",6 Model,[0],[0]
"Second, we introduce several model components which are not specific to disambiguation, but are required to encode linguistic constructions that are present in our corpus and could not be handled by the model of Siddharth et al. (2014).",6 Model,[0],[0]
"These new components are the predicate “not equal”, disjunction, and conjunction.",6 Model,[0],[0]
"The key addition among these components is support for the new predicate “not equal”, which enforces that two tracks, i.e. objects, are distinct from each other.",6 Model,[0],[0]
"For example, in the sentence “Claire and Bill moved a chair” one would want to ensure that the two movers are distinct entities.",6 Model,[0],[0]
"In earlier work, this was not required because the sentences tested in that work were designed to distinguish objects based on constraints rather than identity.",6 Model,[0],[0]
"In other words, there might have been two different people but they were distinguished in the sentence by their actions or appearance.",6 Model,[0],[0]
"To faithfully recognize that two actors are moving the chair in the earlier example, we must ensure that they are disjoint from each other.",6 Model,[0],[0]
"In order to do this we create a new HMM for this predicate, which assigns low probability to tracks that heavily overlap, forcing the model to fit two different actors in the previous example.",6 Model,[0],[0]
"By combining the new first order logic based semantic representation in lieu of a syntactic representation with a more expressive model, we can encode the sentence interpretations required to perform the disambiguation task.
",6 Model,[0],[0]
Figure 3(left) shows an example of two different interpretations of the above discussed sentence “Claire and Bill moved a chair”.,6 Model,[0],[0]
"Object trackers, which correspond to variables in the first order logic representation of the sentence interpretation, are shown in red.",6 Model,[0],[0]
"Predicates which constrain the possible bindings of the trackers, corresponding to predicates in the representation of the sentence, are shown in blue.",6 Model,[0],[0]
"Links represent the argument structure of the first order logic formula, and determine the cross products that are taken between the predicate HMMs and tracker lattices in order to form the joint model which recognizes the entire interpretation in a video.
",6 Model,[0],[0]
The resulting model provides a single unified formalism for representing all the ambiguities in table 2.,6 Model,[0],[0]
"Moreover, this approach can be tuned to different levels of specificity.",6 Model,[0],[0]
"We can create models that are specific to one interpretation of a sentence or that are generic, and accept multiple interpretations by eliding constraints that are not com-
mon between the different interpretations.",6 Model,[0],[0]
"This allows the model, like humans, to defer deciding on a particular interpretation or to infer that multiple interpretation of the sentence are plausible.",6 Model,[0],[0]
We tested the performance of the model described in the previous section on the LAVA dataset presented in section 5.,7 Experimental Results,[0],[0]
"Each video in the dataset was pre-processed with object detectors for humans, bags, chairs, and telescopes.",7 Experimental Results,[0],[0]
"We employed a mixture of CNN (Krizhevsky et al., 2012) and DPM (Felzenszwalb et al., 2010) detectors, trained on held out sections of our corpus.",7 Experimental Results,[0],[0]
"For each object class we generated proposals from both the CNN and the DPM detectors, and trained a scoring function to map both results into the same space.",7 Experimental Results,[0],[0]
The scoring function consisted of a sigmoid over the confidence of the detectors trained on the same held out portion of the training set.,7 Experimental Results,[0],[0]
"As none of the disambiguation examples discussed here rely on the specific identity of the actors, we did not detect their identity.",7 Experimental Results,[0],[0]
"Instead, any sentence which contains names was automatically converted to one which contains arbitrary “person” labels.
",7 Experimental Results,[0],[0]
The sentences in our corpus have either two or three interpretations.,7 Experimental Results,[0],[0]
"Each interpretation has one or more associated videos where the scene was shot from a different angle, carried out either by different actors, with different objects, or in different directions of motion.",7 Experimental Results,[0],[0]
"For each sentence-video pair, we performed a 1-out-of-2 or 1-out-of-3 classification task to determine which of the interpretations of the corresponding sentence best fits that video.",7 Experimental Results,[0],[0]
"Overall chance performance on our dataset is 49.04%, slightly lower than 50% due to the 1- out-of-3 classification examples.
",7 Experimental Results,[0],[0]
The model presented here achieved an accuracy of 75.36% over the entire corpus averaged across all error categories.,7 Experimental Results,[0],[0]
This demonstrates that the model is largely capable of capturing the underlying task and that similar compositional crossmodal models may do the same.,7 Experimental Results,[0],[0]
"For each of the 3 major ambiguity classes we had an accuracy of 84.26% for syntactic ambiguities, 72.28% for semantic ambiguities, and 64.44% for discourse ambiguities.
",7 Experimental Results,[0],[0]
The most significant source of model failures are poor object detections.,7 Experimental Results,[0],[0]
Objects are often rotated and presented at angles that are difficult to recognize.,7 Experimental Results,[0],[0]
"Certain object classes like the telescope
are much more difficult to recognize due to their small size and the fact that hands tend to largely occlude them.",7 Experimental Results,[0],[0]
"This accounts for the degraded performance of the semantic ambiguities relative to the syntactic ambiguities, as many more semantic ambiguities involved the telescope.",7 Experimental Results,[0],[0]
Object detector performance is similarly responsible for the lower performance of the discourse ambiguities which relied much more on the accuracy of the person detector as many sentences involve only people interacting with each other without any additional objects.,7 Experimental Results,[0],[0]
"This degrades performance by removing a helpful constraint for inference, according to which people tend to be close to the objects they are manipulating.",7 Experimental Results,[0],[0]
"In addition, these sentences introduced more visual uncertainty as they often involved three actors.
",7 Experimental Results,[0],[0]
The remaining errors are due to the event models.,7 Experimental Results,[0],[0]
"HMMs can fixate on short sequences of events which seem as if they are part of an action, but in fact are just noise or the prefix of another action.",7 Experimental Results,[0],[0]
"Ideally, one would want an event model which has a global view of the action, if an object went up from the beginning to the end of the video while a person was holding it, it’s likely that the object was being picked up.",7 Experimental Results,[0],[0]
"The event models used here cannot enforce this constraint, they merely assert that the object was moving up for some number of frames; an event which can happen due to noise in the object detectors.",7 Experimental Results,[0],[0]
Enforcing such local constraints instead of the global constraint of the motion of the object over the video makes joint tracking and event recognition tractable in the framework presented here but can lead to errors.,7 Experimental Results,[0],[0]
Finding models which strike a better balance between local information and global constraints while maintaining tractable inference remains an area of future work.,7 Experimental Results,[0],[0]
We present a novel framework for studying ambiguous utterances expressed in a visual context.,8 Conclusion,[0],[0]
"In particular, we formulate a new task for resolving structural ambiguities using visual signal.",8 Conclusion,[0],[0]
"This is a fundamental task for humans, involving complex cognitive processing, and is a key challenge for language acquisition during childhood.",8 Conclusion,[0],[0]
"We release a multimodal corpus that enables to address this task, as well as support further investigation of ambiguity related phenomena in visually grounded language processing.",8 Conclusion,[0],[0]
"Finally, we
present a unified approach for resolving ambiguous descriptions of videos, achieving good performance on our corpus.
While our current investigation focuses on structural inference, we intend to extend this line of work to learning scenarios, in which the agent has to deduce the meaning of words and sentences from structurally ambiguous input.",8 Conclusion,[0],[0]
"Furthermore, our framework can be beneficial for image and video retrieval applications in which the query is expressed in natural language.",8 Conclusion,[0],[0]
"Given an ambiguous query, our approach will enable matching and clustering the retrieved results according to the different query interpretations.",8 Conclusion,[0],[0]
"This material is based upon work supported by the Center for Brains, Minds, and Machines (CBMM), funded by NSF STC award CCF1231216.",Acknowledgments,[0],[0]
SU was also supported by ERC Advanced Grant 269627 Digital Baby.,Acknowledgments,[0],[0]
Understanding language goes hand in hand with the ability to integrate complex contextual information obtained via perception.,abstractText,[0],[0]
"In this work, we present a novel task for grounded language understanding: disambiguating a sentence given a visual scene which depicts one of the possible interpretations of that sentence.",abstractText,[0],[0]
"To this end, we introduce a new multimodal corpus containing ambiguous sentences, representing a wide range of syntactic, semantic and discourse ambiguities, coupled with videos that visualize the different interpretations for each sentence.",abstractText,[0],[0]
We address this task by extending a vision model which determines if a sentence is depicted by a video.,abstractText,[0],[0]
"We demonstrate how such a model can be adjusted to recognize different interpretations of the same underlying sentence, allowing to disambiguate sentences in a unified fashion across the different ambiguity types.",abstractText,[0],[0]
Do You See What I Mean? Visual Resolution of Linguistic Ambiguities,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1275–1284 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1275
We present a document-level neural machine translation model which takes both source and target document context into account using memory networks. We model the problem as a structured prediction problem with interdependencies among the observed and hidden variables, i.e., the source sentences and their unobserved target translations in the document. The resulting structured prediction problem is tackled with a neural translation model equipped with two memory components, one each for the source and target side, to capture the documental interdependencies. We train the model endto-end, and propose an iterative decoding algorithm based on block coordinate descent. Experimental results of English translations from French, German, and Estonian documents show that our model is effective in exploiting both source and target document context, and statistically significantly outperforms the previous work in terms of BLEU and METEOR.",text,[0],[0]
"Neural machine translation (NMT) has proven to be powerful (Sutskever et al., 2014; Bahdanau et al., 2015).",1 Introduction,[0],[0]
"It is on-par, and in some cases, even surpasses the traditional statistical MT (Luong et al., 2015) while enjoying more flexibility and significantly less manual effort for feature engineering.",1 Introduction,[0],[0]
"Despite their flexibility, most neural MT models translate sentences independently.",1 Introduction,[0],[0]
"Discourse phenomenon such as pronominal anaphora and lexical consistency, may depend on long-range dependency going farther than a
few previous sentences, are neglected in sentencebased translation (Bawden et al., 2017).
",1 Introduction,[0],[0]
There are only a handful of attempts to document-wide machine translation in statistical and neural MT camps.,1 Introduction,[0],[0]
Hardmeier and Federico (2010); Gong et al. (2011); Garcia et al. (2014) propose document translation models based on statistical MT but are restrictive in the way they incorporate the document-level information and fail to gain significant improvements.,1 Introduction,[0],[0]
"More recently, there have been a few attempts to incorporate source side context into neural MT (Jean et al., 2017; Wang et al., 2017; Bawden et al., 2017); however, these works only consider a very local context including a few previous source/target sentences, ignoring the global source and target documental contexts.",1 Introduction,[0],[0]
"The latter two report deteriorated performance when using the target-side context.
",1 Introduction,[0],[0]
"In this paper, we present a document-level machine translation model which combines sentencebased NMT (Bahdanau et al., 2015) with memory networks (Sukhbaatar et al., 2015).",1 Introduction,[0],[0]
"We capture the global source and target document context with two memory components, one each for the source and target side, and incorporate it into the sentence-based NMT by changing the decoder to condition on it as the sentence translation is generated.",1 Introduction,[0],[0]
"We conduct experiments on three language pairs: French-English, German-English and Estonian-English.",1 Introduction,[0],[0]
"The experimental results and analysis demonstrate that our model is effective in exploiting both source and target document context, and statistically significantly outperforms the previous work in terms of BLEU and METEOR.",1 Introduction,[0],[0]
"Our document NMT model is grounded on sentence-based NMT model (Bahdanau et al.,
2015) which contains an encoder to read the source sentence as well as an attentional decoder to generate the target translation.
",2.1 Neural Machine Translation (NMT),[0],[0]
Encoder It is a bidirectional RNN consisting of two RNNs running in opposite directions over the source sentence:,2.1 Neural Machine Translation (NMT),[0],[0]
"−→ hi = −−→ RNN( −→ h i−1,ES",2.1 Neural Machine Translation (NMT),[0],[0]
"[xi]), ←−",2.1 Neural Machine Translation (NMT),[0],[0]
h,2.1 Neural Machine Translation (NMT),[0],[0]
"i = ←−− RNN( ←− h i+1,ES",2.1 Neural Machine Translation (NMT),[0],[0]
"[xi])
",2.1 Neural Machine Translation (NMT),[0],[0]
"where ES [xi] is embedding of the word xi from the embedding table ES of the source language, and −→ h i and ←−",2.1 Neural Machine Translation (NMT),[0],[0]
h,2.1 Neural Machine Translation (NMT),[0],[0]
"i are the hidden states of the forward and backward RNNs which can be based on the LSTM (Hochreiter and Schmidhuber, 1997) or GRU (Cho et al., 2014) units.",2.1 Neural Machine Translation (NMT),[0],[0]
"Each word in the source sentence is then represented by the concatenation of the corresponding bidirectional hidden states, hi =",2.1 Neural Machine Translation (NMT),[0],[0]
[ −→ h i; ←− h,2.1 Neural Machine Translation (NMT),[0],[0]
"i].
",2.1 Neural Machine Translation (NMT),[0],[0]
"Decoder The generation of each word yj is conditioned on all of the previously generated words y<j via the state of the RNN decoder sj , and the source sentence via a dynamic context vector cj :
yj ∼ softmax(Wy · rj + br) rj = tanh(sj +Wrc · cj",2.1 Neural Machine Translation (NMT),[0],[0]
+,2.1 Neural Machine Translation (NMT),[0],[0]
"Wrj ·ET [yj−1]) sj = tanh(Ws · sj−1 +Wsj ·ET [yj−1] +Wsc · cj)
where ET",2.1 Neural Machine Translation (NMT),[0],[0]
"[yj ] is embedding of the word yj from the embedding table ET of the target language, and W matrices and br vector are the parameters.",2.1 Neural Machine Translation (NMT),[0],[0]
"The dynamic context vector cj is computed via cj = ∑ i αjihi, where
αj = softmax(aj) aji = v · tanh(Wae ·",2.1 Neural Machine Translation (NMT),[0],[0]
"hi +Wat · sj−1)
",2.1 Neural Machine Translation (NMT),[0],[0]
This is known as the attention mechanism which dynamically attends to relevant parts of the source necessary for generating the next target word.,2.1 Neural Machine Translation (NMT),[0],[0]
"Memory Networks (Weston et al., 2015) are a class of neural models that use external memories to perform inference based on long-range dependencies.",2.2 Memory Networks (MemNets),[0],[0]
"A memory is a collection of vectors M = {m1, ..,mK} constituting the memory cells, where each cell mk may potentially correspond to a discrete object xk.",2.2 Memory Networks (MemNets),[0],[0]
The memory is equipped with a read and optionally a write operation.,2.2 Memory Networks (MemNets),[0],[0]
"Given a query vector q, the output vector generated by reading from the memory is ∑|M | i=1",2.2 Memory Networks (MemNets),[0],[0]
"pimi, where pi represents the relevance of the query to the i-th memory cell p =
softmax(qT ·M).",2.2 Memory Networks (MemNets),[0],[0]
"For the rest of the paper, we denote the read operation by MemNet(M , q).",2.2 Memory Networks (MemNets),[0],[0]
We formulate document-wide machine translation as a structured prediction problem.,3 Document NMT as Structured Prediction,[0],[0]
"Given a set of sentences {x1, . . .",3 Document NMT as Structured Prediction,[0],[0]
",x|d|} in a source document d, we are interested in generating the collection of their translations {y1, . . .",3 Document NMT as Structured Prediction,[0],[0]
",y|d|} taking into account interdependencies among them imposed by the document.",3 Document NMT as Structured Prediction,[0],[0]
We achieve this by the factor graph in Figure 1 to model the probability of the target document given the source document.,3 Document NMT as Structured Prediction,[0],[0]
"Our model has two types of factors:
• fθ(yt;xt,x−t) to capture the interdependencies between the translation yt, the corresponding source sentence xt and all the other sentences in the source document x−t, and
• gθ(yt;y−t) to capture the interdependencies between the translation yt and all the other translations in the document y−t.
",3 Document NMT as Structured Prediction,[0],[0]
"Hence, the probability of a document translation given the source document is
P (y1, . . .",3 Document NMT as Structured Prediction,[0],[0]
",y|d||x1, . . .",3 Document NMT as Structured Prediction,[0],[0]
",x|d|) ∝",3 Document NMT as Structured Prediction,[0],[0]
"exp (∑
t
fθ(yt;xt,x−t) + gθ(yt;y−t) ) .
",3 Document NMT as Structured Prediction,[0],[0]
"The factors fθ and gθ are realised by neural architectures whose parameters are collectively denoted by θ.
Training It is challenging to train the model parameters by maximising the (regularised) likelihood since computing the partition function is hard.",3 Document NMT as Structured Prediction,[0],[0]
"This is due to the enormity of factors
gθ(yt;y−t) over a large number of translation variables yt’s (i.e., the number of sentences in the document) as well as their unbounded domain (i.e., all sentences in the target language).",3 Document NMT as Structured Prediction,[0],[0]
"Thus, we resort to maximising the pseudo-likelihood (Besag, 1975) for training the parameters:
argmax θ ∏ d∈D |d|∏ t=1 Pθ(yt|xt,y−t,x−t) (1)
whereD is the set of bilingual training documents, and |d| denotes the number of (bilingual) sentences in the document d = {(xt,yt)}|d|t=1.",3 Document NMT as Structured Prediction,[0],[0]
"We directly model the document-conditioned NMT model Pθ(yt|xt,y−t,x−t) using a neural architecture which subsumes both the fθ and gθ factors (covered in the next section).
",3 Document NMT as Structured Prediction,[0],[0]
"Decoding To generate the best translation for a document according to our model, we need to solve the following optimisation problem:
arg max y1,...,y|d| |d|∏ t=1 Pθ(yt|xt,y−t,x−t)
which is hard (due to similar reasons as mentioned earlier).",3 Document NMT as Structured Prediction,[0],[0]
We hence resort to a block coordinate descent optimisation algorithm.,3 Document NMT as Structured Prediction,[0],[0]
"More specifically, we initialise the translation of each sentence using the base neural MT model P (yt|xt).",3 Document NMT as Structured Prediction,[0],[0]
"We then repeatedly visit each sentence in the document, and update its translation using our document-context dependent NMT model P (yt|xt,y−t,x−t) while the translations of other sentences are kept fixed.",3 Document NMT as Structured Prediction,[0],[0]
"We augment the sentence-level attentional NMT model by incorporating the document context (both source and target) using memory networks when generating the translation of a sentence, as shown in Figure 2.
",4 Context Dependent NMT with MemNets,[0],[0]
"Our model generates the target translation word-by-word from left to right, similar to the vanilla attentional neural translation model.",4 Context Dependent NMT with MemNets,[0],[0]
"However, it conditions the generation of a target word not only on the previously generated words and the current source sentence (as in the vanilla NMT model), but also on all the other source sentences of the document and their translations.",4 Context Dependent NMT with MemNets,[0],[0]
"That is, the
generation process is as follows:
Pθ(yt|xt,y−t,x−t)",4 Context Dependent NMT with MemNets,[0],[0]
"= |yt|∏ j=1 Pθ(yt,j |yt,<j ,xt,y−t,x−t)
(2)
where yt,j is the j-th word of the t-th target sentence, yt,<j are the previously generated words, and x−t and y−t are as introduced previously.
",4 Context Dependent NMT with MemNets,[0],[0]
"Our model represents the source and target document contexts as external memories, and attends to relevant parts of these external memories when generating the translation of a sentence.",4 Context Dependent NMT with MemNets,[0],[0]
"Let M [x−t] and M [y−t] denote external memories representing the source and target document context, respectively.",4 Context Dependent NMT with MemNets,[0],[0]
These contain memory cells corresponding to all sentences in the document except the t-th sentence (described shortly).,4 Context Dependent NMT with MemNets,[0],[0]
"Let ht and st be representations of the t-th source sentence and its current translation, from the encoder and decoder respectively.",4 Context Dependent NMT with MemNets,[0],[0]
"We make use of ht as the query to get the relevant context from the source external memory:
csrct = MemNet(M",4 Context Dependent NMT with MemNets,[0],[0]
"[x−t],ht)
Furthermore, for the t-th sentence, we get the relevant information from the target context:
ctrgt = MemNet(M",4 Context Dependent NMT with MemNets,[0],[0]
"[y−t], st +Wat · ht)
where the query consists of the representation of the translation st from the decoder endowed with that of the source sentence ht from the encoder to make the query robust to potential noises in the current translation and circumvent error propagation, and Wat projects the source representation into the hidden state space.
",4 Context Dependent NMT with MemNets,[0],[0]
"Now that we have representations of the relevant source and target document contexts, Eq. 2 can be re-written as:
Pθ(yt|xt,y−t,x−t)",4 Context Dependent NMT with MemNets,[0],[0]
"= |yt|∏ j=1 Pθ(yt,j |yt,<j ,xt, ctrgt , c src t )
(3)
More specifically, the memory contexts csrct and ctrgt are incorporated into the NMT decoder as:
• Memory-to-Context in which the memory contexts are incorporated when computing the next decoder hidden state:
st,j = tanh(Ws · st,j−1 +Wsj ·ET [yt,j ] + Wsc · ct,j +Wsm · csrct +Wst · c trg t )
",4 Context Dependent NMT with MemNets,[0],[0]
"• Memory-to-Output in which the memory contexts are incorporated in the output layer:
yt,j ∼ softmax(Wy · rt,j +Wym · csrct + Wyt · ctrgt + br)
",4 Context Dependent NMT with MemNets,[0],[0]
"where Wsm, Wst, Wym, and Wyt are the new parameter matrices.",4 Context Dependent NMT with MemNets,[0],[0]
"We use only the source, only the target, or both external memories as the additional conditioning contexts.",4 Context Dependent NMT with MemNets,[0],[0]
"Furthermore, we use either the Memory-to-Context or Memory-toOutput architectures for incorporating the document contexts.",4 Context Dependent NMT with MemNets,[0],[0]
"In the experiments, we will explore these different options to investigate the most effective combination.",4 Context Dependent NMT with MemNets,[0],[0]
"We now turn our attention to the construction of the external memories for the source and target sides of a document.
",4 Context Dependent NMT with MemNets,[0],[0]
The Source Memory We make use of a hierarchical 2-level RNN architecture to construct the external memory of the source document.,4 Context Dependent NMT with MemNets,[0],[0]
"More specifically, we pass each sentence of the document through a sentence-level bidirectional RNN to get the representation of the sentence (by concatenating the last hidden states of the forward and backward RNNs).",4 Context Dependent NMT with MemNets,[0],[0]
We then pass the sentence representations through a document-level bidirectional RNN to propagate sentences’ information across the document.,4 Context Dependent NMT with MemNets,[0],[0]
"We take the hidden states
of the document-level bidirectional RNNs as the memory cells of the source external memory.
",4 Context Dependent NMT with MemNets,[0],[0]
"The source external memory is built once for each minibatch, and does not change throughout the document translation.",4 Context Dependent NMT with MemNets,[0],[0]
"To be able to fit the computational graph of the document NMT model within GPU memory limits, we pre-train the sentence-level bidirectional RNN using the language modelling training objective.",4 Context Dependent NMT with MemNets,[0],[0]
"However, the document-level bidirectional RNN is trained together with other parameters of the document NMT model by back-propagating the document translation training objective.
",4 Context Dependent NMT with MemNets,[0],[0]
The Target Memory,4 Context Dependent NMT with MemNets,[0],[0]
The memory cells of the target external memory represent the current translations of the document.,4 Context Dependent NMT with MemNets,[0],[0]
Recall from the previous section that we use coordinate descent iteratively to update these translations.,4 Context Dependent NMT with MemNets,[0],[0]
"Let {y1, . . .",4 Context Dependent NMT with MemNets,[0],[0]
",y|d|} be the current translations, and let {s|y1|, . . .",4 Context Dependent NMT with MemNets,[0],[0]
", s|y|d||} be the last states of the decoder when these translations were generated.",4 Context Dependent NMT with MemNets,[0],[0]
We use these last decoder states as the cells of the external target memory.,4 Context Dependent NMT with MemNets,[0],[0]
"We could make use of hierarchical sentencedocument RNNs to transform the document translations into memory cells (similar to what we do for the source memory); however, it would have been computationally expensive and may have resulted in error propagation.",4 Context Dependent NMT with MemNets,[0],[0]
We will show in the experiments that our efficient target memory construction is indeed effective.,4 Context Dependent NMT with MemNets,[0],[0]
Datasets.,5 Experiments and Analysis,[0],[0]
"We conducted experiments on three language pairs: French-English, German-English and Estonian-English.",5 Experiments and Analysis,[0],[0]
Table 1 shows the statistics of the datasets used in our experiments.,5 Experiments and Analysis,[0],[0]
"The French-English dataset is based on the TED Talks corpus1 (Cettolo et al., 2012) where each talk is considered a document.",5 Experiments and Analysis,[0],[0]
"The EstonianEnglish data comes from the Europarl v7 corpus2 (Koehn, 2005).",5 Experiments and Analysis,[0],[0]
"Following Smith et al. (2013), we split the speeches based on the SPEAKER tag and treat them as documents.",5 Experiments and Analysis,[0],[0]
The FrenchEnglish and Estonian-English corpora were randomly split into train/dev/test sets.,5 Experiments and Analysis,[0],[0]
"For GermanEnglish, we use the News Commentary v9 corpus3 for training, news-dev2009 for development, 1https://wit3.fbk.eu/ 2http://www.statmt.org/europarl/ 3http://statmt.org/wmt14/news-commentary-v9-bydocument.tgz
and news-test2011 and news-test2016 as the test sets.",5 Experiments and Analysis,[0],[0]
"The news-commentary corpus has document boundaries already provided.
",5 Experiments and Analysis,[0],[0]
We pre-processed all corpora to remove very short documents and those with missing translations.,5 Experiments and Analysis,[0],[0]
"Out-of-vocabulary and rare words (frequency less than 5) are replaced by the <UNK> token, following Cohn et al. (2016).4
Evaluation Measures We use BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007) scores to measure the quality of the generated translations.",5 Experiments and Analysis,[0],[0]
"We use bootstrap resampling (Clark et al., 2011) to measure statistical significance, p < 0.05, comparing to the baselines.
",5 Experiments and Analysis,[0],[0]
"Implementation and Hyperparameters We implement our document-level neural machine translation model in C++ using the DyNet library (Neubig et al., 2017), on top of the basic sentence-level NMT implementation in mantis (Cohn et al., 2016).",5 Experiments and Analysis,[0],[0]
"For the source memory, the sentence and document-level bidirectional RNNs use LSTM and GRU units, respectively.",5 Experiments and Analysis,[0],[0]
The translation model uses GRU units for the bidirectional RNN encoder and the 2-layer RNN decoder.,5 Experiments and Analysis,[0],[0]
GRUs are used instead of LSTMs to reduce the number of parameters in the main model.,5 Experiments and Analysis,[0],[0]
"The RNN hidden dimensions and word embedding sizes are set to 512 in the translation and memory components, and the alignment dimension is set to 256 in the translation model.
",5 Experiments and Analysis,[0],[0]
Training We use a stage-wise method to train the variants of our document context NMT model.,5 Experiments and Analysis,[0],[0]
"Firstly, we pre-train the Memory-toContext/Memory-to-Output models, setting their readings from the source and target memories to
4We do not split words into subwords using BPE (Sennrich et al., 2016) as that increases sentence lengths resulting in removing long documents due to GPU memory limitations, which would heavily reduce the amount of data that we have.
",5 Experiments and Analysis,[0],[0]
the zero vector.,5 Experiments and Analysis,[0],[0]
"This effectively learns parameters associated with the underlying sentence-based NMT model, which is then used as initialisation when training all parameters in the second stage (including the ones from the first stage).",5 Experiments and Analysis,[0],[0]
"For the first stage, we make use of stochastic gradient descent (SGD)5 with initial learning rate of 0.1 and a decay factor of 0.5 after the fourth epoch for a total of ten epochs.",5 Experiments and Analysis,[0],[0]
The convergence occurs in 6-8 epochs.,5 Experiments and Analysis,[0],[0]
"For the second stage, we use SGD with an initial learning rate of 0.08 and a decay factor of 0.9 after the first epoch for a total of 15 epochs6.",5 Experiments and Analysis,[0],[0]
The best model is picked based on the dev-set perplexity.,5 Experiments and Analysis,[0],[0]
"To avoid overfitting, we employ dropout with the rate 0.2 for the single memory model.",5 Experiments and Analysis,[0],[0]
"For the dual memory model, we set dropout for Document RNN to 0.2 and for the encoder and decoder to 0.5.",5 Experiments and Analysis,[0],[0]
Mini-batching is used in both stages to speed up training.,5 Experiments and Analysis,[0],[0]
"For the largest dataset, the document NMT model takes about 4.5 hours per epoch to train on a single P100 GPU, while the sentence-level model takes about 3 hours per epoch for the same settings.
",5 Experiments and Analysis,[0],[0]
"When training the document NMT model in the second stage, we need the target memory.",5 Experiments and Analysis,[0],[0]
One option would be to use the ground truth translations for building the memory.,5 Experiments and Analysis,[0],[0]
"However, this may result in inferior training, since at the test time, the decoder iteratively updates the translation of sentences based on the noisy translations of other sentences (accessed via the target memory).",5 Experiments and Analysis,[0],[0]
"Hence, while training the document NMT model, we construct the target memory from the translations generated by the pre-trained sentence-level model7.",5 Experiments and Analysis,[0],[0]
"This effectively exposes the model to its potential test-time mistakes during the training time, resulting in more robust learned parameters.",5 Experiments and Analysis,[0],[0]
"We have three variants of our model, using: (i) only the source memory (S-NMT+src mem), (ii) only the target memory (S-NMT+trg mem), or
5In our initial experiments, we found SGD to be more effective than Adam/Adagrad; an observation also made by Bahar et al. (2017).
",5.1 Main Results,[0],[0]
"6For the document NMT model training, we did some preliminary experiments using different learning rates and used the scheme which converged to the best perplexity in the least number of epochs while for sentence-level training we follow Cohn et al. (2016).
",5.1 Main Results,[0],[0]
"7We report results for two-pass decoding, i.e., we only update the translations once using the initial translations generated from the base model.",5.1 Main Results,[0],[0]
"We tried multiple passes of decoding at test-time but it was not helpful.
",5.1 Main Results,[0],[0]
(iii) both the source and target memories (SNMT+both mems).,5.1 Main Results,[0],[0]
We compare these variants against the standard sentence-level NMT model (S-NMT).,5.1 Main Results,[0],[0]
"We also compare the source memory variants of our model to the local context-NMT models8 of Jean et al. (2017) and Wang et al. (2017), which use a few previous source sentences as context, added to the decoder hidden state (similar to our Memory-to-Context model).
",5.1 Main Results,[0],[0]
Memory-to-Context We consistently observe +1.15/+1.13 BLEU/METEOR score improvements across the three language pairs upon comparing our best model to S-NMT (see Table 2).,5.1 Main Results,[0],[0]
"Overall, our document NMT model with both memories has been the most effective variant for all of the three language pairs.
",5.1 Main Results,[0],[0]
We further experiment to train the target memory variants using gold translations instead of the generated ones for German-English.,5.1 Main Results,[0],[0]
"This led to −0.16 and −0.25 decrease9 in the BLEU scores for the target-only and both-memory variants, which confirms the intuition of constructing the target memory by exposing the model to its noises during training time.
",5.1 Main Results,[0],[0]
"Memory-to-Output From Table 2, we consistently see +.95/+1.00",5.1 Main Results,[0],[0]
"BLEU/METEOR improvements between the best variants of our model and the sentence-level baseline across the three lan-
8We implemented and trained the baseline local context models using the same hyperparameters and training procedure that we used for training our memory models.
",5.1 Main Results,[0],[0]
9Latter is statistically significant decrease w.r.t.,5.1 Main Results,[0],[0]
"the both memory model trained on generated target translations.
guage pairs.",5.1 Main Results,[0],[0]
"For French→English, all variants of document NMT model show comparable performance when using BLEU; however, when evaluated using METEOR, the dual memory model is the best.",5.1 Main Results,[0],[0]
"For German→English, the target memory variants give comparable results, whereas for Estonian→English, the dual memory variant proves to be the best.",5.1 Main Results,[0],[0]
"Overall, the Memory-toContext model variants perform better than their Memory-to-Output counterparts.",5.1 Main Results,[0],[0]
"We attribute this to the large number of parameters in the latter architecture (Table 3) and limited amount of data.
",5.1 Main Results,[0],[0]
"We further experiment with more data for train-
BLEU-1 Fr→En De→En Et→En
NC-11NC-16
Jean et al. (2017) 52.8 30.6 39.2 51.9 Wang et al. (2017) 52.6 28.2 38.3 52.3 S-NMT 51.4 28.7 36.9 50.4
+src mem 53.0 30.5 39.1 52.6 +both mems 53.5 33.1 41.3 53.2
Table 5: Unigram BLEU for our Memory-to-Context Document NMT models vs. S-NMT and Source context NMT baselines.",5.1 Main Results,[0],[0]
bold:,5.1 Main Results,[0],[0]
"Best performance.
",5.1 Main Results,[0],[0]
ing the sentence-based NMT to investigate the extent to which document context is useful in this setting.,5.1 Main Results,[0],[0]
We randomly choose an additional 300K German-English sentence pairs from WMT’14 data to train the base NMT model in stage 1.,5.1 Main Results,[0],[0]
"In stage 2, we use the same document corpus as before to train the document-level models.",5.1 Main Results,[0],[0]
"As seen from Figure 3, the document MT variants still benefit from the document context even when the base model is trained on a larger bilingual corpus.",5.1 Main Results,[0],[0]
"For the Memory-to-Context model, we see massive improvements of +0.72 and +1.44 METEOR scores for the source memory and dual memory model respectively, when compared to the baseline.",5.1 Main Results,[0],[0]
"On the other hand, for the Memory-to-Output model, the target memory model’s METEOR score increases significantly by +1.09 compared to the baseline, slightly differing from the corresponding model using the smaller corpus (+1.2).
",5.1 Main Results,[0],[0]
"Local Source Context Models Table 4 shows comparison of our Memory-to-Context model variants to local source context-NMT models (Jean et al., 2017; Wang et al., 2017).",5.1 Main Results,[0],[0]
"For French→English, our source memory model is comparable to both baselines.",5.1 Main Results,[0],[0]
"For German→English, our S-NMT+src mem model is comparable to Jean et al. (2017) but outperforms Wang et al. (2017) for one test set according to BLEU, and for both test sets according to METEOR.",5.1 Main Results,[0],[0]
"For Estonian→English, our model outperforms Jean et al. (2017).",5.1 Main Results,[0],[0]
"Our global source context model has only surface-level sentence information, and is oblivious to the individual words in the context since we do an offline training to get the sentence representations (as previously mentioned).",5.1 Main Results,[0],[0]
"However, the other two context baselines have access to that information, yet our
model’s performance is either better or quite close to those models.",5.1 Main Results,[0],[0]
We also look into the unigram BLEU scores to see how much our global source memory variants lead to improvement at the word-level.,5.1 Main Results,[0],[0]
"From Table 5, it can be seen that our model’s performance is better than the baselines for majority of the cases.",5.1 Main Results,[0],[0]
"The S-NMT+both mems model gives the best results for all three language pairs, showing that leveraging both source and target document context is indeed beneficial for improving MT performance.",5.1 Main Results,[0],[0]
Using Global/Local Target Context We first investigate whether using a local target context would have been equally sufficient in comparison to our global target memory model for the three datasets.,5.2 Analysis,[0],[0]
We condition the decoder on the previous target sentence representation (obtained from the last hidden state of the decoder) by adding it as an additional input to all decoder states (PrevTrg) similar to our Memory-to-Context model.,5.2 Analysis,[0],[0]
"From Table 6, we observe that for French→English and Estonian→English, using all sentences in the target context or just the previous target sentence gives comparable results.",5.2 Analysis,[0],[0]
"We may attribute this to these specific datasets, that is documents from TED talks or European Parliament Proceedings may depend more on the local than on the global context.",5.2 Analysis,[0],[0]
"However, for German→English (NC-11), the target memory model performs the best show-
ing that for documents with richer context (e.g. news articles) we do need the global target document context to improve MT performance.
",5.2 Analysis,[0],[0]
"Output Analysis To better understand the dual memory model, we look at the first sentence example in Table 7.",5.2 Analysis,[0],[0]
It can be seen that the source sentence has the noun “Qimonda” but the sentencelevel NMT model fails to attend to it when generating the translation.,5.2 Analysis,[0],[0]
"On the other hand, the single memory models are better in delivering some, if not all, of the underlying information in the source sentence but the dual memory model’s translation quality surpasses them.",5.2 Analysis,[0],[0]
"This is because the word “Qimonda” was being repeated in this specific document, providing a strong contextual signal to our global document context model while the local context model by Wang et al. (2017) is still unable to correctly translate the noun even when it has access to the word-level information of previous sentences.
",5.2 Analysis,[0],[0]
We resort to manual evaluation as there is no standard metric which evaluates document-level discourse information like consistency or pronominal anaphora.,5.2 Analysis,[0],[0]
"By manual inspection, we observe that our models can identify nouns in the source sentence to resolve coreferent pronouns, as shown in the second example of Table 7.",5.2 Analysis,[0],[0]
"Here the topic of the sentence is “the country under the dictatorship of Lukashenko” and our target and dual memory models are able to generate the appropriate pronoun/determiner as well as accurately translate the word ‘diktatuur’, hence producing much better translation as compared to both baselines.",5.2 Analysis,[0],[0]
"Apart from these improvements, our models are better in improving the readability of sentences by generating more context appropriate grammatical structures such as verbs and adverbs.
",5.2 Analysis,[0],[0]
"Furthermore, to validate that our model improves the consistency of translations, we look at five documents (roughly 70 sentences) from the test set of Estonian-English, each of which had a word being repeated in the gold translation.",5.2 Analysis,[0],[0]
Our model is able to resolve the consistency in 22 out of 32 cases as compared to the sentencebased model which only accurately translates 16 of those.,5.2 Analysis,[0],[0]
"Following Wang et al. (2017), we also investigate the extent to which our model can correct errors made by the baseline system.",5.2 Analysis,[0],[0]
We randomly choose five documents from the test set.,5.2 Analysis,[0],[0]
"Out of the 20 words/phrases which were incorrectly translated by the sentence-based model, our
model corrects 85% of them while also generating 10% new errors.",5.2 Analysis,[0],[0]
"Document-level Statistical MT There have been a few SMT-based attempts to document MT, but they are either restrictive or do not lead to significant improvements.",6 Related Work,[0],[0]
Hardmeier and Federico (2010) identify links among words in the source document using a word-dependency model to improve translation of anaphoric pronouns.,6 Related Work,[0],[0]
Gong et al. (2011) make use of a cache-based system to save relevant information from the previously generated translations and use that to enhance document-level translation.,6 Related Work,[0],[0]
"Garcia et al. (2014) propose a two-pass approach to improve the translations already obtained by a sentencelevel model.
",6 Related Work,[0],[0]
"Docent is an SMT-based document-level decoder (Hardmeier et al., 2012, 2013), which tries to modify the initial translation generated by the Moses decoder (Koehn et al., 2007) through stochastic local search and hill-climbing.",6 Related Work,[0],[0]
Garcia et al. (2015) make use of neural-based continuous word representations to incorporate distributional semantics into Docent.,6 Related Work,[0],[0]
"In another work, Garcia et al. (2017) incorporate new word embedding features into Docent to improve the lexical consistency of translations.",6 Related Work,[0],[0]
"The proposed methods fail to yield improvements upon automatic evaluation.
",6 Related Work,[0],[0]
"Larger Context Neural MT Jean et al. (2017)
extend the vanilla attention-based neural MT model (Bahdanau et al., 2015) by conditioning the decoder on the previous sentence via attention over its words.",6 Related Work,[0],[0]
Extending their model to consider the global source document context would be challenging due to the large size of computation graph over all the words in the source document.,6 Related Work,[0],[0]
"Wang et al. (2017) employ a 2-level hierarichal RNN to summarise three previous source sentences, which is then used as an additional input to the decoder hidden state.",6 Related Work,[0],[0]
Bawden et al. (2017) use multi-encoder NMT models to exploit context from the previous source and target sentence.,6 Related Work,[0],[0]
They highlight the importance of targetside context but report deteriorated BLEU scores when using it.,6 Related Work,[0],[0]
All these works consider a very local source/target context and completely ignore the global source and target document contexts.,6 Related Work,[0],[0]
We have proposed a document-level neural MT model that captures global source and target document context.,7 Conclusion,[0],[0]
Our model augments the vanilla sentence-based NMT model with external memories to incorporate documental interdependencies on both source and target sides.,7 Conclusion,[0],[0]
We show statistically significant improvements of the translation quality on three language pairs.,7 Conclusion,[0],[0]
"For future work, we intend to investigate models which incorporate specific discourse-level phenomena.",7 Conclusion,[0],[0]
The authors are grateful to André,Acknowledgments,[0],[0]
Martins and the anonymous reviewers for their helpful comments and corrections.,Acknowledgments,[0],[0]
"This work was supported by the Multi-modal Australian ScienceS Imaging and Visualisation Environment (MASSIVE) (www. massive.org.au), and partially supported by a Google Faculty Award to GH and the Australian Research Council through DP160102686.",Acknowledgments,[0],[0]
We present a document-level neural machine translation model which takes both source and target document context into account using memory networks.,abstractText,[0],[0]
"We model the problem as a structured prediction problem with interdependencies among the observed and hidden variables, i.e., the source sentences and their unobserved target translations in the document.",abstractText,[0],[0]
"The resulting structured prediction problem is tackled with a neural translation model equipped with two memory components, one each for the source and target side, to capture the documental interdependencies.",abstractText,[0],[0]
"We train the model endto-end, and propose an iterative decoding algorithm based on block coordinate descent.",abstractText,[0],[0]
"Experimental results of English translations from French, German, and Estonian documents show that our model is effective in exploiting both source and target document context, and statistically significantly outperforms the previous work in terms of BLEU and METEOR.",abstractText,[0],[0]
Document Context Neural Machine Translation with Memory Networks,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 414–419 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
414",text,[0],[0]
Event Detection (ED) is an important subtask of event extraction.,1 Introduction,[0],[0]
It extracts event triggers from individual sentences and further identifies the type of the corresponding events.,1 Introduction,[0],[0]
"For instance, according to the ACE-2005 annotation guideline, in the sentence “Jane and John are married”, an ED system should be able to identify the word “married” as a trigger of the event “Marry”.",1 Introduction,[0],[0]
"However, it may be difficult to identify events from isolated sentences, because the same event trigger might represent different event types in different contexts.
",1 Introduction,[0],[0]
"Existing ED methods can mainly be categorized into two classes, namely, feature-based methods (e.g., (McClosky et al., 2011; Hong et al., 2011; Li et al., 2014)) and representation-based methods (e.g., (Nguyen and Grishman, 2015; Chen et al.,
2015; Liu et al., 2016a; Chen et al., 2017)).",1 Introduction,[0],[0]
"The former mainly rely on a set of hand-designed features, while the latter employ distributed representation to capture meaningful semantic information.",1 Introduction,[0],[0]
"In general, most of these existing methods mainly exploit sentence-level contextual information.",1 Introduction,[0],[0]
"However, document-level information is also important for ED, because the sentences in the same document, although they may contain different types of events, are often correlated with respect to the theme of the document.",1 Introduction,[0],[0]
"For example, there are the following sentences in ACE-2005:
...",1 Introduction,[0],[0]
I knew it was time to leave.,1 Introduction,[0],[0]
Isn’t that a great argument for term limits? ...,1 Introduction,[0],[0]
"If we only examine the first sentence, it is hard to determine whether the trigger “leave” indicates a “Transport” event meaning that he wants to leave the current place, or an “End-Position” event indicating that he will stop working for his current organization.",1 Introduction,[0],[0]
"However, if we can capture the contextual information of this sentence, it is more confident for us to label “leave” as the trigger of an “End-Position” event.",1 Introduction,[0],[0]
"Upon such observation, there have been some feature-based studies (Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012) that construct rules to capture document-level information for improving sentence-level ED.",1 Introduction,[0],[0]
"However, they suffer from two major limitations.",1 Introduction,[0],[0]
"First, the features used therein often need to be manually designed and may involve error propagation due to natural language processing; Second, they discover inter-event information at document level by constructing inference rules, which is time-consuming and is hard to make the rule set as complete as possible.",1 Introduction,[0],[0]
"Besides, a representation-based study has been presented in (Duan et al., 2017), which employs the PV-DM model to train document embeddings and further uses it in a RNN-based event classifier.",1 Introduction,[0],[0]
"However, as being limited by the unsupervised training
process, the document-level representation cannot specifically capture event-related information.
",1 Introduction,[0],[0]
"In this paper, we propose a novel Document Embedding Enhanced Bi-RNN model, called DEEB-RNN, for ED at sentence level.",1 Introduction,[0],[0]
"This model first learns ED oriented embeddings of documents through a hierarchical and supervised attention based bidirectional RNN, which pays word-level attention to event triggers and sentence-level attention to those sentences containing events.",1 Introduction,[0],[0]
It then uses the learned document embeddings to facilitate another bidirectional RNN model to identify event triggers and their types in individual sentences.,1 Introduction,[0],[0]
This learning process is guided by a general loss function where the loss corresponding to attention at both word and sentence levels and that of event type identification are integrated.,1 Introduction,[0],[0]
"It should be mentioned that although the attention mechanism has recently been applied effectively in various tasks, including machine translation (Zhang et al., 2017), question answering (Hao et al., 2017), document summarization (Tan et al., 2017), etc., this is the first study, to the best of our knowledge, which adopts a hierarchical and supervised attention mechanism to learn ED oriented embeddings of documents.
",1 Introduction,[0],[0]
"We evaluate the developed DEEB-RNN model on the benchmark dataset, ACE-2005, and systematically investigate the impacts of different supervised attention strategies on its performance.",1 Introduction,[0],[0]
"Experimental results show that the DEEBRNN model outperforms both feature-based and
representation-based state-of-the-art methods in terms of recall and F1-measure.",1 Introduction,[0],[0]
We formalize ED as a multi-class classification problem.,2 The Proposed Model,[0],[0]
"Given a sentence, we treat every word in it as a trigger candidate, and classify each candidate to a certain event type.",2 The Proposed Model,[0],[0]
"In the ACE-2005 dataset, there are 8 event types, further being divided into 33 subtypes, and a “Not Applicable (NA)” type.",2 The Proposed Model,[0],[0]
"Without loss of generality, in this paper we regard the 33 subtypes as 33 event types.",2 The Proposed Model,[0],[0]
"Figure 1 presents the schematic diagram of the proposed DEEB-RNN model, which contains two main modules:
1.",2 The Proposed Model,[0],[0]
"The ED Oriented Document Embedding Learning (EDODEL) module, which learns the distributed representations of documents from both word and sentence levels via the well-designed hierarchical and supervised attention mechanism.
2.",2 The Proposed Model,[0],[0]
"The Document-level Enhanced Event Detector (DEED) module, which tags each trigger candidate with an event type based on the learned embedding of documents.",2 The Proposed Model,[0],[0]
"To learn the ED oriented embedding of a document, we apply the hierarchical and supervised attention network presented in Figure 1, which consists of a word-level Bi-GRU (Schuster and Paliwal, 2002) encoder with attention on event triggers
and a sentence-level Bi-GRU encoder with attention on sentences with events.",2.1 The EDODEL Module,[0],[0]
"Given a document with L sentences, DEEB-RNN learns its embedding for detecting events in all sentences.
",2.1 The EDODEL Module,[0],[0]
"Word-level embeddings Given a sentence si (i = 1, 2, ..., L) consisting of words {wit|t = 1, 2, ..., T}.",2.1 The EDODEL Module,[0],[0]
"For each word wit, we first concatenate its embedding wit and its entity type embedding1 eit (Nguyen and Grishman, 2015) as the input git of a Bi-GRU and thus obtain the bidirectional hidden state hit:
hit = [ −−−−→ GRUw(git), ←−−−− GRUw(git)].",2.1 The EDODEL Module,[0],[0]
"(1)
We then feed hit to a perceptron with no bias to get uit = tanh(Wwhit) as a hidden representation of hit and also obtain an attention weight αit = u T itcw, which should be normalized through a softmax function.",2.1 The EDODEL Module,[0],[0]
"Here, similar to that in (Yang et al., 2016), cw is a vector representing the wordlevel context of wit, which is initialized at random.",2.1 The EDODEL Module,[0],[0]
"Finally, the embedding of the sentence si can be obtained by summing up hit with their weights:
si = T∑ t=1 αithit.",2.1 The EDODEL Module,[0],[0]
"(2)
To pay more attention to trigger words than other words, we construct the gold word-level attention signals α∗i for the sentence si, as illustrated in Figure 2a.",2.1 The EDODEL Module,[0],[0]
"We can then take the square error as the general loss of the attention at word level to supervise the learning process:
Ew(α ∗,α) = L∑ i=1",2.1 The EDODEL Module,[0],[0]
T∑ t=1 (α∗it − αit)2.,2.1 The EDODEL Module,[0],[0]
"(3)
1The words in the ACE-2005 dataset are annotated with their entity types (annotated as “NA” if they are not an entity).
",2.1 The EDODEL Module,[0],[0]
"Sentence-level embeddings Given the sentence embeddings {si|i = 1, 2, ..., L}, we first get the hidden state qi via a Bi-GRU:
qi = [ −−−→ GRUs(si), ←−−− GRUs(si)].",2.1 The EDODEL Module,[0],[0]
"(4)
Then we feed qi to a perceptron with no bias to get the hidden representation ti = tanh(Wsqi) and also obtain an attention weight βi = tTi cs to be normalized via softmax.",2.1 The EDODEL Module,[0],[0]
"Similarly, cs represents the sentence-level context of si to be randomly initialized.",2.1 The EDODEL Module,[0],[0]
"We eventually obtain the document embedding d as:
d = L∑ i=1 βisi.",2.1 The EDODEL Module,[0],[0]
"(5)
We also think that the sentences containing event should obtain more attention than other ones.",2.1 The EDODEL Module,[0],[0]
"Therefore, similar to the case at word level, we construct the gold sentence-level attention signals β∗ for the document d, as illustrated in Figure 2b, and further take the square error as the general loss of the attention at sentence level to supervise the learning process:
Es(β ∗,β) = L∑ i=1",2.1 The EDODEL Module,[0],[0]
(β∗i − βi)2.,2.1 The EDODEL Module,[0],[0]
(6),2.1 The EDODEL Module,[0],[0]
"We employ another Bi-GRU encoder and a softmax output layer to model the ED task, which can handle event triggers with multiple words.",2.2 The DEED Module,[0],[0]
"Specifically, given a sentence sj (j = 1, 2, ..., L) in document d, for each of its word wjt (t = 1, 2, ..., T ), we concatenate its word embedding wjt and entity type embedding ejt with the corresponding document embedding d as the input rjt of the Bi-GRU and thus obtain the hidden state fjt:
fjt = [ −−−→ GRUe(rjt), ←−−− GRUe(rjt)].",2.2 The DEED Module,[0],[0]
"(7)
Finally, we get the probability vector ojt with K dimensions through a softmax layer for wjt, where the k-th element, o(k)jt , of ojt indicates the probability of classifying wjt to the k-th event type.",2.2 The DEED Module,[0],[0]
"The loss function, J(y,o), can thus be defined in terms of the cross-entropy error of the real event type yjt and the predicted probability o(k)jt as follows:
J(y,o) =",2.2 The DEED Module,[0],[0]
"− L∑
j=1 T∑ t=1 K∑ k=1",2.2 The DEED Module,[0],[0]
"I(yjt = k)log o (k) jt , (8)
where I(·) is the indicator function.",2.2 The DEED Module,[0],[0]
"In the DEEB-RNN model, the above two modules are jointly trained.",2.3 Joint Training of the DEEB-RNN model,[0],[0]
"For this purpose, we define the joint loss function in the training process upon the losses specified for different modules as follows:
J(θ)= ∑ ∀d∈ϕ (J(y,o)+λEw(α ∗,α)+µEs(β ∗,β)), (9) where θ denotes, as a whole, the parameters used in DEEB-RNN, ϕ is the training document set, and λ and µ are hyper-parameters for striking a balance among J(y,o), Ew(α∗,α) and Es(β∗,β).",2.3 Joint Training of the DEEB-RNN model,[0],[0]
We validate the proposed model through comparison with state-of-the-art methods on the ACE2005 dataset.,3.1 Datasets and Settings,[0],[0]
"In the experiments, the validation set has 30 documents from different genres, the test set has 40 documents and the training set contains the remaining 529 documents.",3.1 Datasets and Settings,[0],[0]
"All the data preprocessing and evaluation criteria follow those in (Ghaeini et al., 2016).
",3.1 Datasets and Settings,[0],[0]
Hyper-parameters are tuned on the validation set.,3.1 Datasets and Settings,[0],[0]
"We set the dimension of the hidden layers corresponding to GRUw, GRUs, and GRUe to 300, 200, and 300, respectively, the output size of Ww and Ws to 600 and 400, respectively, the dimension of entity type embeddings to 50, the batch size to 25, the dropout rate to 0.5.",3.1 Datasets and Settings,[0],[0]
"In addition, we utilize the pre-trained word embeddings with 300 dimensions from (Mikolov et al., 2013) for initialization.",3.1 Datasets and Settings,[0],[0]
"For entity types, their embeddings are randomly initialized.",3.1 Datasets and Settings,[0],[0]
"We train the model using Stochastic Gradient Descent (SGD) over shuffled mini-batches and using dropout (Krizhevsky et al., 2012) for regularization.",3.1 Datasets and Settings,[0],[0]
"In order to validate the proposed DEEB-RNN model through experimental comparison, we choose the following typical models as the baselines.
",3.2 Baseline Models,[0],[0]
"Sentence-level is a feature-based model proposed in (Hong et al., 2011), which regards entitytype consistency as a key feature to predict event mentions.
",3.2 Baseline Models,[0],[0]
"Joint Local is a feature-based model developed in (Li et al., 2013), which incorporates such features that explicitly capture the dependency among multiple triggers and arguments.
",3.2 Baseline Models,[0],[0]
"JRNN is a representation-based model proposed in (Nguyen et al., 2016), which exploits the inter-dependency between event triggers and argument roles via discrete structures.
",3.2 Baseline Models,[0],[0]
"Skip-CNN is a representation-based model presented in (Nguyen and Grishman, 2016), which proposes a novel convolution to exploit nonconsecutive k-grams for event detection.
",3.2 Baseline Models,[0],[0]
"ANN-S2 is a representation-based model developed in (Liu et al., 2017), which explicitly exploits argument information for event detection via supervised attention mechanisms.
",3.2 Baseline Models,[0],[0]
"Cross-event is a feature-based model proposed in (Liao and Grishman, 2010), which learns relations among event types from training corpus and futher helps predict the occurrence of events.
",3.2 Baseline Models,[0],[0]
"PSL is a feature-based model developed in (Liu et al., 2016b), which encods global information such as event-event association in the form of logic using the probabilistic soft logic model.
",3.2 Baseline Models,[0],[0]
"DLRNN is a representation-based model proposed in (Duan et al., 2017), which automatically extracts cross-sentence clues to improve sentencelevel event detection.",3.2 Baseline Models,[0],[0]
"In this section, we conduct experiments on the ACE-2005 dataset to demonstrate the effectiveness of different attention strategies.
",3.3 Impacts of Different Attention Strategies,[0],[0]
"Bi-GRU is the basic ED model, which does not employ document-level embeddings.
",3.3 Impacts of Different Attention Strategies,[0],[0]
"DEEB-RNN uses the document embeddings and computes attentions without supervision, in which hyper-parameters λ and µ are set to 0.
DEEB-RNN1/2/3 means they uses the gold attention signals as supervision information.",3.3 Impacts of Different Attention Strategies,[0],[0]
"Specifically, DEEB-RNN1 uses only the gold word-level attention signal (λ = 1 and µ = 0), DEEB-RNN2 uses only the gold sentence-level attention signal (λ = 0 and µ = 1), whilst DEEB-RNN3 employs the gold attention signals at both word and sen-
tence levels (λ = 1 and µ = 1).",3.3 Impacts of Different Attention Strategies,[0],[0]
"Table 1 compares these methods, where we can observe that the methods with document embeddings (i.e., the last four) significantly outperform the pure Bi-GRU method, which suggests that document-level information is very beneficial for ED.",3.3 Impacts of Different Attention Strategies,[0],[0]
"An interesting phenomenon is that, as compared to DEEB-RNN, DEEB-RNN2 changes the precision-recall balance.",3.3 Impacts of Different Attention Strategies,[0],[0]
This is because of the following reasons.,3.3 Impacts of Different Attention Strategies,[0],[0]
"On one hand, as compared to DEEB-RNN, DEEB-RNN2 uses the gold sentence-level attention signal, indicating that it pays special attention to the sentences containing events with event triggers.",3.3 Impacts of Different Attention Strategies,[0],[0]
"In this way, the BiRNN model for learning document embeddings will filter out the sentences containing events but without explicit event triggers.",3.3 Impacts of Different Attention Strategies,[0],[0]
That means the events detected by DEEB-RNN2 are basically the ones with explicit event triggers.,3.3 Impacts of Different Attention Strategies,[0],[0]
"Therefore, as compared to DEEB-RNN, the precision of DEEBRNN2 is improved; On the other hand, the above strategy may result in less learning of words, which are event triggers but do not appear in the training dataset.",3.3 Impacts of Different Attention Strategies,[0],[0]
"Therefore, those sentences with such event triggers cannot be detected.",3.3 Impacts of Different Attention Strategies,[0],[0]
"The recall of DEEB-RNN2 is thus lowered, as compared to DEEB-RNN.",3.3 Impacts of Different Attention Strategies,[0],[0]
"Moreover, DEEB-RNN3 shows the best performance, indicating that the gold attention signals at both word and sentence levels are useful for ED.",3.3 Impacts of Different Attention Strategies,[0],[0]
Table 2 presents the overall performance of all methods on ACE-2005.,3.4 Performance Comparison,[0],[0]
"We can see that different versions of DEEB-RNN consistently out-
perform the existing state-of-the-art methods in terms of both recall and F1-measure, while their precision is comparable to that of others.",3.4 Performance Comparison,[0],[0]
"The better performance of DEEB-RNN can be explained by the following reasons: (1) Compared with feature-based methods, including Sentencelevel, Joint Local, and representation-based methods, including JRNN, Skip-CNN and ANN-S2, our method exploits document-level information (i.e., the ED oriented document embeddings) from both word and sentence levels in a document by the supervised attention mechanism, which enhance the ability of identifying trigger words; (2) Compared with feature-based methods using document-level information, such as Cross-event, PSL, our method can automatically capture event types in documents via a end-to-end Bi-RNN based model without manually designed rules; (3) Compared with representation-based methods using document-level information, such as DLRNN, our method can learn event detection oriented embeddings of documents through the hierarchical and supervised attention based Bi-RNN network.",3.4 Performance Comparison,[0],[0]
"In this study, we proposed a hierarchical and supervised attention based and document embedding enhanced Bi-RNN method, called DEEB-RNN, for event detection.",4 Conclusions and Future Work,[0],[0]
We explored different strategies to construct gold word- and sentence-level attentions to focus on event information.,4 Conclusions and Future Work,[0],[0]
Experiments on the ACE-2005 dataset demonstrate that DEEB-RNN achieves better performance as compared to the state-of-the-art methods in terms of both recall and F1-measure.,4 Conclusions and Future Work,[0],[0]
"In this paper, we can strike a balance between sentence and document embeddings by adjusting their dimensions.",4 Conclusions and Future Work,[0],[0]
"In the future, we may improve the DEEB-RNN model to automatically determine the weights of sentence and document embeddings.",4 Conclusions and Future Work,[0],[0]
"This work is supported by National Key Research and Development Program of China under grants 2016YFB1000902 and 2017YFC0820404, and National Natural Science Foundation of China under grants 61772501, 61572473, 61572469, and 91646120.",Acknowledgments,[0],[0]
"We are grateful to Dr. Liu Kang of the Institute of Automation, Chinese Academy of Sciences for very helpful discussion on event detection.",Acknowledgments,[0],[0]
Document-level information is very important for event detection even at sentence level.,abstractText,[0],[0]
"In this paper, we propose a novel Document Embedding Enhanced Bi-RNN model, called DEEB-RNN, to detect events in sentences.",abstractText,[0],[0]
"This model first learns event detection oriented embeddings of documents through a hierarchical and supervised attention based RNN, which pays word-level attention to event triggers and sentence-level attention to those sentences containing events.",abstractText,[0],[0]
It then uses the learned document embedding to enhance another bidirectional RNN model to identify event triggers and their types in sentences.,abstractText,[0],[0]
"Through experiments on the ACE-2005 dataset, we demonstrate the effectiveness and merits of the proposed DEEB-RNN model via comparison with state-of-the-art methods.",abstractText,[0],[0]
Document Embedding Enhanced Event Detection with Hierarchical and Supervised Attention,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2020–2030 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
2020",text,[0],[0]
Recurrent neural networks have become one of the most widely used models in natural language processing (NLP).,1 Introduction,[0],[0]
"A number of variants of RNNs such as Long Short-Term Memory networks (LSTM; Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit networks (GRU; Cho et al., 2014) have been designed to model text capturing long-term dependencies in problems such as language modeling.",1 Introduction,[0],[0]
"However, document modeling, a key to many natural language
∗The first three authors made equal contributions to this paper.",1 Introduction,[0],[0]
"The work was done when the second author was visiting Edinburgh.
",1 Introduction,[0],[0]
"1Our TensorFlow code and datasets are publicly available at https://github.com/shashiongithub/ Document-Models-with-Ext-Information.
understanding tasks, is still an open challenge.",1 Introduction,[0],[0]
"Recently, some neural network architectures were proposed to capture large context for modeling text (Mikolov and Zweig, 2012; Ghosh et al., 2016; Ji et al., 2015; Wang and Cho, 2016).",1 Introduction,[0],[0]
"Lin et al. (2015) and Yang et al. (2016) proposed a hierarchical RNN network for document-level modeling as well as sentence-level modeling, at the cost of increased computational complexity.",1 Introduction,[0],[0]
"Tran et al. (2016) further proposed a contextual language model that considers information at interdocument level.
",1 Introduction,[0],[0]
"It is challenging to rely only on the document for its understanding, and as such it is not surprising that these models struggle on problems such as document summarization (Cheng and Lapata, 2016; Chen et al., 2016; Nallapati et al., 2017; See et al., 2017; Tan and Wan, 2017) and machine reading comprehension (Trischler et al., 2016; Miller et al., 2016; Weissenborn et al., 2017; Hu et al., 2017; Wang et al., 2017).",1 Introduction,[0],[0]
"In this paper, we formalize the use of external information to further guide document modeling for end goals.
",1 Introduction,[0],[0]
We present a simple yet effective document modeling framework for sentence extraction that allows machine reading with “external attention.”,1 Introduction,[0],[0]
Our model includes a neural hierarchical document encoder (or a machine reader) and a hierarchical attention-based sentence extractor.,1 Introduction,[0],[0]
Our hierarchical document encoder resembles the architectures proposed by Cheng and Lapata (2016) and Narayan et al. (2018) in that it derives the document meaning representation from its sentences and their constituent words.,1 Introduction,[0],[0]
"Our novel sentence extractor combines this document meaning representation with an attention mechanism (Bahdanau et al., 2015) over the external information to label sentences from the input document.",1 Introduction,[0],[0]
"Our model explicitly biases the extractor with external cues and
implicitly biases the encoder through training.
",1 Introduction,[0],[0]
We demonstrate the effectiveness of our model on two problems that can be naturally framed as sentence extraction with external information.,1 Introduction,[0],[0]
"These two problems, extractive document summarization and answer selection for machine reading comprehension, both require local and global contextual reasoning about a given document.",1 Introduction,[0],[0]
"Extractive document summarization systems aim at creating a summary by identifying (and subsequently concatenating) the most important sentences in a document, whereas answer selection systems select the candidate sentence in a document most likely to contain the answer to a query.",1 Introduction,[0],[0]
"For document summarization, we exploit the title and image captions which often appear with documents (specifically newswire articles) as external information.",1 Introduction,[0],[0]
"For answer selection, we use word overlap features, such as the inverse sentence frequency (ISF, Trischler et al., 2016) and the inverse document frequency (IDF) together with the query, all formulated as external cues.
",1 Introduction,[0],[0]
"Our main contributions are three-fold: First, our model ensures that sentence extraction is done in a larger (rich) context, i.e., the full document is read first before we start labeling its sentences for extraction, and each sentence labeling is done by implicitly estimating its local and global relevance to the document and by directly attending to some external information for importance cues.
",1 Introduction,[0],[0]
"Second, while external information has been shown to be useful for summarization systems using traditional hand-crafted features (Edmundson, 1969; Kupiec et al., 1995; Mani, 2001), our model is the first to exploit such information in deep learning-based summarization.",1 Introduction,[0],[0]
"We evaluate our models automatically (in terms of ROUGE scores) on the CNN news highlights dataset (Hermann et al., 2015).",1 Introduction,[0],[0]
"Experimental results show that our summarizer, informed with title and image captions, consistently outperforms summarizers that do not use this information.",1 Introduction,[0],[0]
We also conduct a human evaluation to judge which type of summary participants prefer.,1 Introduction,[0],[0]
"Our results overwhelmingly show that human subjects find our summaries more informative and complete.
",1 Introduction,[0],[0]
"Lastly, with the machine reading capabilities of our model, we confirm that a full document needs to be “read” to produce high quality extracts allowing a rich contextual reasoning, in contrast to previous answer selection approaches that often
measure a score between each sentence in the document and the question and return the sentence with highest score in an isolated manner (Yin et al., 2016; dos Santos et al., 2016; Wang et al., 2016).",1 Introduction,[0],[0]
Our model with ISF and IDF scores as external features achieves competitive results for answer selection.,1 Introduction,[0],[0]
"Our ensemble model combining scores from our model and word overlap scores using a logistic regression layer achieves state-ofthe-art results on the popular question answering datasets WikiQA (Yang et al., 2015) and NewsQA (Trischler et al., 2016), and it obtains comparable results to the state of the art for SQuAD (Rajpurkar et al., 2016).",1 Introduction,[0],[0]
"We also evaluate our approach on the MSMarco dataset (Nguyen et al., 2016) and elaborate on the behavior of our machine reader in a scenario where each candidate answer sentence is contextually independent of each other.",1 Introduction,[0],[0]
"Given a document D consisting of a sequence of n sentences (s1, s2, ..., sn) , we aim at labeling each sentence si in D with a label yi ∈ {0, 1} where yi = 1 indicates that si is extraction-worthy and 0 otherwise.",2 Document Modeling For Sentence Extraction,[0],[0]
"Our architecture resembles those previously proposed in the literature (Cheng and Lapata, 2016; Nallapati et al., 2017).",2 Document Modeling For Sentence Extraction,[0],[0]
"The main components include a sentence encoder, a document encoder, and a novel sentence extractor (see Figure 1) that we describe in more detail below.",2 Document Modeling For Sentence Extraction,[0],[0]
"The novel characteristics of our model are that each sentence is labeled by implicitly estimating its (local and global) relevance to the document and by directly attending to some external information for importance cues.
",2 Document Modeling For Sentence Extraction,[0],[0]
"Sentence Encoder A core component of our model is a convolutional sentence encoder (Kim, 2014; Kim et al., 2016) which encodes sentences into continuous representations.",2 Document Modeling For Sentence Extraction,[0],[0]
We use temporal narrow convolution by applying a kernel filter K of width h to a window of h words in sentence s to produce a new feature.,2 Document Modeling For Sentence Extraction,[0],[0]
This filter is applied to each possible window of words in s to produce a feature map f ∈ Rk−h+1 where k is the sentence length.,2 Document Modeling For Sentence Extraction,[0],[0]
We then apply max-pooling over time over the feature map f and take the maximum value as the feature corresponding to this particular filter K. We use multiple kernels of various sizes and each kernel multiple times to construct the representation of a sentence.,2 Document Modeling For Sentence Extraction,[0],[0]
"In Figure 1, ker-
nels of size 2 (red) and 4 (blue) are applied three times each.",2 Document Modeling For Sentence Extraction,[0],[0]
The max-pooling over time operation yields two feature lists fK2 and fK4 ∈ R3.,2 Document Modeling For Sentence Extraction,[0],[0]
"The final sentence embeddings have six dimensions.
",2 Document Modeling For Sentence Extraction,[0],[0]
Document Encoder,2 Document Modeling For Sentence Extraction,[0],[0]
The document encoder composes a sequence of sentences to obtain a document representation.,2 Document Modeling For Sentence Extraction,[0],[0]
"We use a recurrent neural network with LSTM cells to avoid the vanishing gradient problem when training long sequences (Hochreiter and Schmidhuber, 1997).",2 Document Modeling For Sentence Extraction,[0],[0]
"Given a document D consisting of a sequence of sentences (s1, s2, . . .",2 Document Modeling For Sentence Extraction,[0],[0]
", sn), we follow common practice and feed the sentences in reverse order (Sutskever et al., 2014; Li et al., 2015; Filippova et al., 2015).
",2 Document Modeling For Sentence Extraction,[0],[0]
Sentence Extractor,2 Document Modeling For Sentence Extraction,[0],[0]
Our sentence extractor sequentially labels each sentence in a document with 1 or 0 by implicitly estimating its relevance in the document and by directly attending to the external information for importance cues.,2 Document Modeling For Sentence Extraction,[0],[0]
"It is implemented with another RNN with LSTM cells with an attention mechanism (Bahdanau et al., 2015) and a softmax layer.",2 Document Modeling For Sentence Extraction,[0],[0]
Our attention mechanism differs from the standard practice of attending intermediate states of the input (encoder).,2 Document Modeling For Sentence Extraction,[0],[0]
"Instead, our extractor attends to a sequence of p pieces of external information E : (e1, e2, ..., ep) relevant for the task (e.g., ei is a title or an image caption for summarization) for cues.",2 Document Modeling For Sentence Extraction,[0],[0]
"At time ti, it reads sentence si and makes a binary prediction, conditioned on the document representation (obtained from the document encoder), the previously labeled sentences and the external information.",2 Document Modeling For Sentence Extraction,[0],[0]
"This way, our labeler is able to identify locally and globally important sentences within the document which correlate well with the external information.
",2 Document Modeling For Sentence Extraction,[0],[0]
"Given sentence st at time step t, it returns a probability distribution over labels as:
p(yt|st, D,E) = softmax(g(ht, h′t))",2 Document Modeling For Sentence Extraction,[0],[0]
"(1) g(ht, h ′ t) = Uo(Vhht",2 Document Modeling For Sentence Extraction,[0],[0]
+W ′ hh ′,2 Document Modeling For Sentence Extraction,[0],[0]
"t) (2)
ht = LSTM(st, ht−1)
",2 Document Modeling For Sentence Extraction,[0],[0]
h′t = p∑ i=1,2 Document Modeling For Sentence Extraction,[0],[0]
"α(t,i)ei,
where α(t,i) = exp(htei)∑",2 Document Modeling For Sentence Extraction,[0],[0]
"j exp(htej)
where g(·) is a single-layer neural network with parameters Uo, Vh and W ′h.",2 Document Modeling For Sentence Extraction,[0],[0]
"ht is an intermedi-
ate RNN state at time step t. The dynamic context vector h′t is essentially the weighted sum of the external information (e1, e2, . .",2 Document Modeling For Sentence Extraction,[0],[0]
.,2 Document Modeling For Sentence Extraction,[0],[0]
", ep).",2 Document Modeling For Sentence Extraction,[0],[0]
Figure 1 summarizes our model.,2 Document Modeling For Sentence Extraction,[0],[0]
We validate our model on two sentence extraction problems: extractive document summarization and answer selection for machine reading comprehension.,3 Sentence Extraction Applications,[0],[0]
Both these tasks require local and global contextual reasoning about a given document.,3 Sentence Extraction Applications,[0],[0]
"As such, they test the ability of our model to facilitate document modeling using external information.
",3 Sentence Extraction Applications,[0],[0]
Extractive Summarization,3 Sentence Extraction Applications,[0],[0]
An extractive summarizer aims to produce a summary S by selecting m sentences from D (where m < n).,3 Sentence Extraction Applications,[0],[0]
"In this setting, our sentence extractor sequentially predicts label yi ∈ {0, 1} (where 1 means that si should be included in the summary) by assigning score p(yi|si,D ,E , θ) quantifying the relevance of si to the summary.",3 Sentence Extraction Applications,[0],[0]
"We assemble a summary S by selecting m sentences with top p(yi = 1|si,D ,E , θ) scores.
",3 Sentence Extraction Applications,[0],[0]
We formulate external information E as the sequence of the title and the image captions associated with the document.,3 Sentence Extraction Applications,[0],[0]
"We use the convolutional sentence encoder to get their sentence-level representations.
",3 Sentence Extraction Applications,[0],[0]
"Answer Selection Given a question q and a document D , the goal of the task is to select one candidate sentence si ∈ D in which the answer exists.",3 Sentence Extraction Applications,[0],[0]
"In this setting, our sentence extractor sequentially predicts label yi ∈ {0, 1} (where 1 means that si contains the answer) and assign score p(yi|si,D ,E , θ) quantifying si’s relevance to the query.",3 Sentence Extraction Applications,[0],[0]
"We return as answer the sentence si with the highest p(yi = 1|si,D ,E , θ) score.
",3 Sentence Extraction Applications,[0],[0]
We treat the question q as external information and use the convolutional sentence encoder to get its sentence-level representation.,3 Sentence Extraction Applications,[0],[0]
This simplifies Eq.,3 Sentence Extraction Applications,[0],[0]
"(1) and (2) as follow:
p(yt|st, D, q) = softmax(g(ht, q)) (3) g(ht, q) = Uo(Vhht +Wqq),
where Vh and Wq are network parameters.",3 Sentence Extraction Applications,[0],[0]
"We exploit the simplicity of our model to further assimilate external features relevant for answer selection: the inverse sentence frequency (ISF, (Trischler et al., 2016)), the inverse document frequency (IDF) and a modified version of the ISF score which we call local ISF.",3 Sentence Extraction Applications,[0],[0]
"Trischler et al. (2016) have shown that a simple ISF baseline (i.e., a sentence with the highest ISF score as an answer) correlates well with the answers.",3 Sentence Extraction Applications,[0],[0]
"The ISF score αsi for the sentence si is computed as αsi =∑
w∈si∩q IDF(w), where IDF is the inverse document frequency score of word w, defined as: IDF(w) = log NNw , whereN is the total number of sentences in the training set and Nw is the number of sentences in which w appears.",3 Sentence Extraction Applications,[0],[0]
"Note that, si ∩ q
refers to the set of words that appear both in si and in q. Local ISF is calculated in the same manner as the ISF score, only with setting the total number of sentences (N ) to the number of sentences in the article that is being analyzed.
",3 Sentence Extraction Applications,[0],[0]
"More formally, this modifies Eq.",3 Sentence Extraction Applications,[0],[0]
"(3) as follows:
p(yt|st, D, q) = softmax(g(ht, q, αt, βt, γt)),(4)
where αt, βt and γt are the ISF, IDF and local ISF scores (real values) of sentence st respectively .",3 Sentence Extraction Applications,[0],[0]
"The function g is calculated as follows:
g(ht, q, αt, βt, γt) =Uo (Vhht+
Wqq +Wisf(αt · 1)+ Widf(βt · 1) +Wlisf(γt · 1) ) ,
where Wisf , Widf and Wlisf are new parameters added to the network and 1 is a vector of 1s of size equal to the sentence embedding size.",3 Sentence Extraction Applications,[0],[0]
"In Figure 1, these external feature vectors are represented as 6-dimensional gray vectors accompanied with dashed arrows.",3 Sentence Extraction Applications,[0],[0]
This section presents our experimental setup and results assessing our model in both the extractive summarization and answer selection setups.,4 Experiments and Results,[0],[0]
"In the rest of the paper, we refer to our model as XNET for its ability to exploit eXternal information to improve document representation.",4 Experiments and Results,[0],[0]
"Summarization Dataset We evaluated our models on the CNN news highlights dataset (Hermann et al., 2015).2",4.1 Extractive Document Summarization,[0],[0]
"We used the standard splits of Hermann et al. (2015) for training, validation, and testing (90,266/1,220/1,093 documents).",4.1 Extractive Document Summarization,[0],[0]
"We followed previous studies (Cheng and Lapata, 2016; Nallapati et al., 2016, 2017; See et al., 2017; Tan and Wan, 2017) in assuming that the
2Hermann et al. (2015) have also released the DailyMail dataset, but we do not report our results on this dataset.",4.1 Extractive Document Summarization,[0],[0]
We found that the script written by Hermann et al. to crawl DailyMail articles mistakenly extracts image captions as part of the main body of the document.,4.1 Extractive Document Summarization,[0],[0]
"As image captions often do not have sentence boundaries, they blend with the sentences of the document unnoticeably.",4.1 Extractive Document Summarization,[0],[0]
"This leads to the production of erroneous summaries.
",4.1 Extractive Document Summarization,[0],[0]
“story highlights” associated with each article are gold-standard abstractive summaries.,4.1 Extractive Document Summarization,[0],[0]
We trained our network on a named-entity-anonymized version of news articles.,4.1 Extractive Document Summarization,[0],[0]
"However, we generated deanonymized summaries and evaluated them against gold summaries to facilitate human evaluation and to make human evaluation comparable to automatic evaluation.
",4.1 Extractive Document Summarization,[0],[0]
"To train our model, we need documents annotated with sentence extraction information, i.e., each sentence in a document is labeled with 1 (summary-worthy) or 0 (not summary-worthy).",4.1 Extractive Document Summarization,[0],[0]
"We followed Nallapati et al. (2017) and automatically extracted ground truth labels such that all positively labeled sentences from an article collectively give the highest ROUGE (Lin and Hovy, 2003) score with respect to the gold summary.
",4.1 Extractive Document Summarization,[0],[0]
"We used a modified script of Hermann et al. (2015) to extract titles and image captions, and we associated them with the corresponding articles.",4.1 Extractive Document Summarization,[0],[0]
All articles get associated with their titles.,4.1 Extractive Document Summarization,[0],[0]
"The availability of image captions varies from 0 to 414 per article, with an average of 3 image captions.",4.1 Extractive Document Summarization,[0],[0]
"There are 40% CNN articles with at least one image caption.
",4.1 Extractive Document Summarization,[0],[0]
"All sentences, including titles and image captions, were padded with zeros to a sentence length of 100.",4.1 Extractive Document Summarization,[0],[0]
All input documents were padded with zeros to a maximum document length of 126.,4.1 Extractive Document Summarization,[0],[0]
"For each document, we consider a maximum of 10 image captions.",4.1 Extractive Document Summarization,[0],[0]
"We experimented with various numbers (1, 3, 5, 10 and 20) of image captions on the validation set and found that our model performed best with 10 image captions.",4.1 Extractive Document Summarization,[0],[0]
"We refer the reader to the supplementary material for more implementation details to replicate our results.
",4.1 Extractive Document Summarization,[0],[0]
Comparison Systems We compared the output of our model against the standard baseline of simply selecting the first three sentences from each document as the summary.,4.1 Extractive Document Summarization,[0],[0]
"We refer to this baseline as LEAD in the rest of the paper.
",4.1 Extractive Document Summarization,[0],[0]
We also compared our system against the sentence extraction system of Cheng and Lapata (2016).,4.1 Extractive Document Summarization,[0],[0]
"We refer to this system as POINTERNET as the neural attention architecture in Cheng and Lapata (2016) resembles the one of Pointer Networks (Vinyals et al., 2015).3 It does not exploit any external information.4 Cheng and Lap-
3The architecture of POINTERNET is closely related to our model without external information.
",4.1 Extractive Document Summarization,[0],[0]
"4Adding external information to POINTERNET is an in-
ata (2016) report only on the DailyMail dataset.",4.1 Extractive Document Summarization,[0],[0]
"We used their code (https://github.com/ cheng6076/NeuralSum) to produce results on the CNN dataset.5
Automatic Evaluation To automatically assess the quality of our summaries, we used ROUGE (Lin and Hovy, 2003), a recall-oriented metric, to compare our model-generated summaries to manually-written highlights.6",4.1 Extractive Document Summarization,[0],[0]
"Previous work has reported ROUGE-1 (R1) and ROUGE-2 (R2) scores to access informativeness, and ROUGE-L (RL) to access fluency.",4.1 Extractive Document Summarization,[0],[0]
"In addition to R1, R2 and RL, we also report ROUGE-3 (R3) and ROUGE-4 (R4) capturing higher order n-grams overlap to assess informativeness and fluency simultaneously.
",4.1 Extractive Document Summarization,[0],[0]
teresting direction of research,4.1 Extractive Document Summarization,[0],[0]
but we do not pursue it here.,4.1 Extractive Document Summarization,[0],[0]
"It requires decoding with multiple types of attentions and this is not the focus of this paper.
",4.1 Extractive Document Summarization,[0],[0]
5We are unable to compare our results to the extractive system of Nallapati et al. (2017) because they report their results on the DailyMail dataset and their code is not available.,4.1 Extractive Document Summarization,[0],[0]
"The abstractive systems of Chen et al. (2016) and Tan and Wan (2017) report their results on the CNN dataset, however, their results are not comparable to ours as they report on the full-length F1 variants of ROUGE to evaluate their abstractive summaries.",4.1 Extractive Document Summarization,[0],[0]
"We report ROUGE recall scores which is more appropriate to evaluate our extractive summaries.
",4.1 Extractive Document Summarization,[0],[0]
"6We used pyrouge, a Python package, to compute all our ROUGE scores with parameters “-a -c 95 -m -n 4 -w 1.2.”
",4.1 Extractive Document Summarization,[0],[0]
We report our results on both full length (three sentences with the top scores as the summary) and fixed length (first 75 bytes and 275 bytes as the summary) summaries.,4.1 Extractive Document Summarization,[0],[0]
"For full length summaries, our decision of selecting three sentences is guided by the fact that there are 3.11 sentences on average in the gold highlights of the training set.",4.1 Extractive Document Summarization,[0],[0]
"We conduct our ablation study on the validation set with full length ROUGE scores, but we report both fixed and full length ROUGE scores for the test set.
",4.1 Extractive Document Summarization,[0],[0]
We experimented with two types of external information: title (TITLE) and image captions (CAPTION).,4.1 Extractive Document Summarization,[0],[0]
"In addition, we experimented with the first sentence (FS) of the document as external information.",4.1 Extractive Document Summarization,[0],[0]
"Note that the latter is not external information, it is a sentence in the document.",4.1 Extractive Document Summarization,[0],[0]
"However, we wanted to explore the idea that the first sentence of the document plays a crucial part in generating summaries (Rush et al., 2015; Nallapati et al., 2016).",4.1 Extractive Document Summarization,[0],[0]
"XNET with FS acts as a baseline for XNET with title and image captions.
",4.1 Extractive Document Summarization,[0],[0]
We report the performance of several variants of XNET on the validation set in Table 1.,4.1 Extractive Document Summarization,[0],[0]
We also compare them against the LEAD baseline and POINTERNET.,4.1 Extractive Document Summarization,[0],[0]
These two systems do not use any additional information.,4.1 Extractive Document Summarization,[0],[0]
"Interestingly, all the variants of XNET significantly outperform LEAD and POINTERNET.",4.1 Extractive Document Summarization,[0],[0]
"When the title (TITLE), image captions (CAPTION) and the first sentence (FS) are used separately as additional information, XNET performs best with TITLE as its external information.",4.1 Extractive Document Summarization,[0],[0]
"Our result demonstrates the importance of the title of the document in extractive summarization (Edmundson, 1969; Kupiec et al., 1995; Mani, 2001).",4.1 Extractive Document Summarization,[0],[0]
The performance with TITLE and CAPTION is better than that with FS.,4.1 Extractive Document Summarization,[0],[0]
"We also tried possible combinations of TITLE, CAPTION and FS.",4.1 Extractive Document Summarization,[0],[0]
All XNET models are superior to the ones without any external information.,4.1 Extractive Document Summarization,[0],[0]
"XNET performs best when TITLE and CAPTION are jointly used as external information (55.4%, 21.8%, 11.8%, 7.5%, and 49.2% for R1, R2, R3, R4, and RL respectively).",4.1 Extractive Document Summarization,[0],[0]
"It is better than the the LEAD baseline by 3.7 points on average and than POINTERNET by 1.8 points on average, indicating that external information is useful to identify the gist of the document.",4.1 Extractive Document Summarization,[0],[0]
"We use this model for testing purposes.
",4.1 Extractive Document Summarization,[0],[0]
Our final results on the test set are shown in Table 2.,4.1 Extractive Document Summarization,[0],[0]
"It turns out that for smaller summaries (75 bytes) LEAD and POINTERNET are superior
to XNET.",4.1 Extractive Document Summarization,[0],[0]
"This result could be because LEAD (always) and POINTERNET (often) include the first sentence in their summaries, whereas, XNET is better capable at selecting sentences from various document positions.",4.1 Extractive Document Summarization,[0],[0]
"This is not captured by smaller summaries of 75 bytes, but it becomes more evident with longer summaries (275 bytes and full length) where XNET performs best across all ROUGE scores.",4.1 Extractive Document Summarization,[0],[0]
"We note that POINTERNET outperforms LEAD for 75-byte summaries, then its performance drops behind LEAD for 275-byte summaries, but then it outperforms LEAD for full length summaries on the metrics R1, R2 and RL.",4.1 Extractive Document Summarization,[0],[0]
"It shows that POINTERNET with its attention over sentences in the document is capable of exploring more than first few sentences in the document, but it is still behind XNET which is better at identifying salient sentences in the document.",4.1 Extractive Document Summarization,[0],[0]
"XNET performs significantly better than POINTERNET by 0.8 points for 275-byte summaries and by 1.9 points for full length summaries, on average for all ROUGE scores.
",4.1 Extractive Document Summarization,[0],[0]
Human Evaluation We complement our automatic evaluation results with human evaluation.,4.1 Extractive Document Summarization,[0],[0]
"We randomly selected 20 articles from the test set.
",4.1 Extractive Document Summarization,[0],[0]
Annotators were presented with a news article and summaries from four different systems.,4.1 Extractive Document Summarization,[0],[0]
"These include the LEAD baseline, POINTERNET, XNET and the human authored highlights.",4.1 Extractive Document Summarization,[0],[0]
"We followed the guidelines in Cheng and Lapata (2016), and asked our participants to rank the summaries from best (1st) to worst (4th) in order of informativeness (does the summary capture important information in the article?) and fluency (is the summary written in well-formed English?).",4.1 Extractive Document Summarization,[0],[0]
We did not allow any ties and we only sampled articles with nonidentical summaries.,4.1 Extractive Document Summarization,[0],[0]
We assigned this task to five annotators who were proficient English speakers.,4.1 Extractive Document Summarization,[0],[0]
Each annotator was presented with all 20 articles.,4.1 Extractive Document Summarization,[0],[0]
The order of summaries to rank was randomized per article.,4.1 Extractive Document Summarization,[0],[0]
"An example of summaries our subjects ranked is provided in the supplementary material.
",4.1 Extractive Document Summarization,[0],[0]
The results of our human evaluation study are shown in Table 3.,4.1 Extractive Document Summarization,[0],[0]
"As one might imagine, HUMAN gets ranked 1st most of the time (41%).",4.1 Extractive Document Summarization,[0],[0]
"However, it is closely followed by XNET which ranked 1st 28% of the time.",4.1 Extractive Document Summarization,[0],[0]
"In comparison, POINTERNET and LEAD were mostly ranked at 3rd and 4th places.",4.1 Extractive Document Summarization,[0],[0]
We also carried out pairwise comparisons between all models in Table 3 for their statistical significance using a one-way ANOVA with post-hoc Tukey HSD tests with (p < 0.01).,4.1 Extractive Document Summarization,[0],[0]
"It showed that XNET is significantly better than LEAD and POINTERNET, and it does not differ significantly from HUMAN.",4.1 Extractive Document Summarization,[0],[0]
"On the other hand, POINTERNET does not differ significantly from LEAD and it differs significantly from both XNET and HUMAN.",4.1 Extractive Document Summarization,[0],[0]
The human evaluation results corroborates our empirical results in Table 1 and Table 2: XNET is better than LEAD and POINTERNET in producing informative and fluent summaries.,4.1 Extractive Document Summarization,[0],[0]
"Question Answering Datasets We run experiments on four datasets collected for open domain question-answering tasks: WikiQA (Yang et al., 2015), SQuAD (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2016), and MSMarco (Nguyen et al., 2016).
",4.2 Answer Selection,[0],[0]
NewsQA was especially designed to present lexical and syntactic divergence between questions and answers.,4.2 Answer Selection,[0],[0]
"It contains 119,633 questions posed by crowdworkers on 12,744 CNN articles previously collected by Hermann et al. (2015).",4.2 Answer Selection,[0],[0]
"In a similar manner, SQuAD associates 100,000+
question with a Wikipedia article’s first paragraph, for 500+ previously chosen articles.",4.2 Answer Selection,[0],[0]
WikiQA was collected by mining web-searching query logs and then associating them with the summary section of the Wikipedia article presumed to be related to the topic of the query.,4.2 Answer Selection,[0],[0]
"A similar collection procedure was followed to create MSMarco with the difference that each candidate answer is a whole paragraph from a different browsed website associated with the query.
",4.2 Answer Selection,[0],[0]
"We follow the widely used setup of leaving out unanswered questions (Trischler et al., 2016; Yang et al., 2015) and adapt the format of each dataset to our task of answer sentence selection by labeling a candidate sentence with 1 if any answer span is contained in that sentence.",4.2 Answer Selection,[0],[0]
"In the case of MSMarco, each candidate paragraph comes associated with a label, hence we treat each one as a single long sentence.",4.2 Answer Selection,[0],[0]
"Since SQuAD keeps the official test dataset hidden and MSMarco does not provide labels for its released test set, we report results on their official validation sets.",4.2 Answer Selection,[0],[0]
"For validation, we set apart 10% of each official training set.
",4.2 Answer Selection,[0],[0]
"Our dataset splits consist of 92,525, 5,165 and 5,124 samples for NewsQA; 79,032, 8,567, and 10,570 for SQuAD; 873, 122, and 237 for WikiQA; and 79,704, 9,706, and 9,650 for MSMarco, for training, validation, and testing respectively.
",4.2 Answer Selection,[0],[0]
"Comparison Systems We compared the output of our model against the ISF (Trischler et al., 2016) and LOCALISF baselines.",4.2 Answer Selection,[0],[0]
"Given an article, the sentence with the highest ISF score is selected as an answer for the ISF baseline and the sentence with the highest local ISF score for the LOCALISF baseline.",4.2 Answer Selection,[0],[0]
"We also compare our model against a neural network (PAIRCNN) that encodes (question, candidate) in an isolated manner as in previous work (Yin et al., 2016; dos Santos et al., 2016; Wang et al., 2016).",4.2 Answer Selection,[0],[0]
The architecture uses the sentence encoder explained in earlier sections to learn the question and candidate representations.,4.2 Answer Selection,[0],[0]
"The distribution over labels is given by p(yt|q) = p(yt|st, q) = softmax(g(st, q))",4.2 Answer Selection,[0],[0]
"where g(st, q) = ReLU(Wsq ·",4.2 Answer Selection,[0],[0]
[st; q] + bsq).,4.2 Answer Selection,[0],[0]
"In addition, we also compare our model against APCNN (dos Santos et al., 2016), ABCNN (Yin et al., 2016), L.D.C (Wang and Jiang, 2017), KVMemNN (Miller et al., 2016), and COMPAGGR, a state-of-the-art system by Wang et al. (2017).
",4.2 Answer Selection,[0],[0]
We experiment with several variants of our model.,4.2 Answer Selection,[0],[0]
"XNET is the vanilla version of our sen-
tence extractor conditioned only on the query q as external information (Eq.",4.2 Answer Selection,[0],[0]
(3)).,4.2 Answer Selection,[0],[0]
"XNET+ is an extension of XNET which uses ISF, IDF and local ISF scores in addition to the query q as external information (Eqn. (4)).",4.2 Answer Selection,[0],[0]
"We also experimented with a baseline XNETTOPK where we choose the top k sentences with highest ISF score, and then among them choose the one with the highest probability according to XNET.",4.2 Answer Selection,[0],[0]
"In our experiments, we set k = 5.",4.2 Answer Selection,[0],[0]
"In the end, we experimented with an ensemble network LRXNET which combines the XNET score, the COMPAGGR score and other word-overlap-based scores (tweaked and optimized for each dataset separately) for each sentence using a logistic regression classifier.",4.2 Answer Selection,[0],[0]
"It uses ISF and LocalISF scores for NewsQA, IDF and ISF scores for SQuAD, sentence length, IDF and ISF scores for WikiQA, and word overlap and ISF score for MSMarco.",4.2 Answer Selection,[0],[0]
"We refer the reader to the supplementary material for more implementation and optimization details to replicate our results.
",4.2 Answer Selection,[0],[0]
Evaluation Metrics,4.2 Answer Selection,[0],[0]
"We consider metrics that evaluate systems that return a ranked list of candidate answers: mean average precision (MAP), mean reciprocal rank (MRR), and accuracy (ACC).
",4.2 Answer Selection,[0],[0]
"Results Table 4 gives the results for the test sets of NewsQA and WikiQA, and the original validation sets of SQuAD and MSMarco.",4.2 Answer Selection,[0],[0]
"Our first observation is that XNET outperforms PAIRCNN, supporting our claim that it is beneficial to read the whole document in order to make decisions,
instead of only observing each candidate in isolation.
",4.2 Answer Selection,[0],[0]
"Secondly, we can observe that ISF is indeed a strong baseline that outperforms XNET.",4.2 Answer Selection,[0],[0]
"This means that just “reading” the document using a vanilla version of XNET is not sufficient, and help is required through a coarse filtering.",4.2 Answer Selection,[0],[0]
"Indeed, we observe that XNET+ outperforms all baselines except for COMPAGGR.",4.2 Answer Selection,[0],[0]
"Our ensemble model LRXNET can ultimately surpass COMPAGGR on majority of the datasets.
",4.2 Answer Selection,[0],[0]
This consistent behavior validates the machine reading capabilities and the improved document representation with external features of our model for answer selection.,4.2 Answer Selection,[0],[0]
"Specifically, the combination of document reading and word overlap features is required to be done in a soft manner, using a classification technique.",4.2 Answer Selection,[0],[0]
"Using it as a hard constraint, with XNETTOPK, does not achieve the best result.",4.2 Answer Selection,[0],[0]
We believe that often the ISF score is a better indicator of answer presence in the vicinity of certain candidate instead of in the candidate itself.,4.2 Answer Selection,[0],[0]
"As such, XNET+ is capable of using this feature in datasets with richer context.
",4.2 Answer Selection,[0],[0]
It is worth noting that the improvement gained by LRXNET over the state-of-the-art follows a pattern.,4.2 Answer Selection,[0],[0]
"For the SQuAD dataset, the results are comparable (less than 1%).",4.2 Answer Selection,[0],[0]
"However, the improvement for WikiQA reaches ∼3% and then the gap shrinks again for NewsQA, with an improvement of ∼1%.",4.2 Answer Selection,[0],[0]
"This could be explained by the fact that each sample of the SQuAD is a paragraph, compared to an article summary for WikiQA, and
to an entire article for NewsQA.",4.2 Answer Selection,[0],[0]
"Hence, we further strengthen our hypothesis that a richer context is needed to achieve better results, in this case expressed as document length, but as the length of the context increases the limitation of sequential models to learn from long rich sequences arises.7
Interestingly, our model lags behind COMPAGGR on the MSMarco dataset.",4.2 Answer Selection,[0],[0]
"It turns out this is due to contextual independence between candidates in the MSMarco dataset, i.e., each candidate is a stand-alone paragraph in this dataset, in contrast to contextually dependent candidate sentences from a document in the NewsQA, SQuAD and WikiQA datasets.",4.2 Answer Selection,[0],[0]
"As a result, our models (XNET+ and LRXNET) with document reading abilities perform poorly.",4.2 Answer Selection,[0],[0]
This can be observed by the fact that XNET and PAIRCNN obtain comparable results.,4.2 Answer Selection,[0],[0]
COMPAGGR performs better because comparing each candidate independently is a better strategy.,4.2 Answer Selection,[0],[0]
We describe an approach to model documents while incorporating external information that informs the representations learned for the sentences in the document.,5 Conclusion,[0],[0]
"We implement our approach through an attention mechanism of a neural network architecture for modeling documents.
",5 Conclusion,[0],[0]
"Our experiments with extractive document summarization and answer selection tasks validates our model in two ways: first, we demonstrate that external information is important to guide document modeling for natural language understanding tasks.",5 Conclusion,[0],[0]
"Our model uses image captions and the title of the document for document summarization, and the query with word overlap features for answer selection and outperforms its counterparts that do not use this information.",5 Conclusion,[0],[0]
"Second, our external attention mechanism successfully guides the learning of the document representation for the relevant end goal.",5 Conclusion,[0],[0]
"For answer selection, we show that inserting the query with word overlap features using our external attention mechanism outperforms state-of-the-art systems that naturally also have access to this information.",5 Conclusion,[0],[0]
"We thank Jianpeng Cheng for providing us with the CNN dataset and the implementation of Point-
7See the supplementary material for an example supporting our hypothesis.
",Acknowledgments,[0],[0]
erNet.,Acknowledgments,[0],[0]
We also thank the members of the Edinburgh NLP group for participating in our human evaluation experiments.,Acknowledgments,[0],[0]
"This work greatly benefitted from discussions with Jianpeng Cheng, Annie Louis, Pedro Balage, Alfonso Mendes, Sebastião Miranda, and members of the Edinburgh NLP group.",Acknowledgments,[0],[0]
"We gratefully acknowledge the support of the European Research Council (Lapata; award number 681760), the European Union under the Horizon 2020 SUMMA project (Narayan, Cohen; grant agreement 688139), and Huawei Technologies (Cohen).",Acknowledgments,[0],[0]
Document modeling is essential to a variety of natural language understanding tasks.,abstractText,[0],[0]
We propose to use external information to improve document modeling for problems that can be framed as sentence extraction.,abstractText,[0],[0]
We develop a framework composed of a hierarchical document encoder and an attention-based extractor with attention over external information.,abstractText,[0],[0]
We evaluate our model on extractive document summarization (where the external information is image captions and the title of the document) and answer selection (where the external information is a question).,abstractText,[0],[0]
"We show that our model consistently outperforms strong baselines, in terms of both informativeness and fluency (for CNN document summarization) and achieves state-of-the-art results for answer selection on WikiQA and NewsQA.1",abstractText,[0],[0]
Document Modeling with External Attention for Sentence Extraction,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1422–1432, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
"Document level sentiment classification is a fundamental task in sentiment analysis, and is crucial to understand user generated content in social networks or product reviews (Manning and Schütze, 1999; Jurafsky and Martin, 2000; Pang and Lee, 2008; Liu, 2012).",1 Introduction,[0],[0]
"The task calls for identifying the overall sentiment polarity (e.g. thumbs up or thumbs down, 1-5 stars on review sites) of a document.",1 Introduction,[0],[0]
"In literature, dominant approaches follow (Pang et al., 2002) and exploit machine learn-
∗Corresponding author.",1 Introduction,[0],[0]
"1 Codes and datasets are publicly available at
http://ir.hit.edu.cn/˜dytang.
ing algorithm to build sentiment classifier.",1 Introduction,[0],[0]
"Many of them focus on designing hand-crafted features (Qu et al., 2010; Paltoglou and Thelwall, 2010) or learning discriminate features from data, since the performance of a machine learner is heavily dependent on the choice of data representation (Bengio et al., 2015).
",1 Introduction,[0],[0]
Document level sentiment classification remains a significant challenge: how to encode the intrinsic (semantic or syntactic) relations between sentences in the semantic meaning of document.,1 Introduction,[0],[0]
This is crucial for sentiment classification because relations like “contrast” and “cause” have great influences on determining the meaning and the overall polarity of a document.,1 Introduction,[0],[0]
"However, existing studies typically fail to effectively capture such information.",1 Introduction,[0],[0]
"For example, Pang et al. (2002) and Wang and Manning (2012) represent documents with bag-of-ngrams features and build SVM classifier upon that.",1 Introduction,[0],[0]
"Although such feature-driven SVM is an extremely strong performer and hardly to be transcended, its “sparse” and “discrete” characteristics make it clumsy in taking into account of side information like relations between sentences.",1 Introduction,[0],[0]
"Recently, Le and Mikolov (2014) exploit neural networks to learn continuous document representation from data.",1 Introduction,[0],[0]
"Essentially, they use local ngram information and do not capture semantic relations between sentences.",1 Introduction,[0],[0]
"Furthermore, a person asked to do this task will naturally carry it out in a sequential, bottom-up fashion, analyze the meanings of sentences before considering semantic relations between them.",1 Introduction,[0],[0]
"This motivates us to develop an end-to-end and bottom-up algorithm to effectively model document representation.
",1 Introduction,[0],[0]
"In this paper, we introduce a neural network approach to learn continuous document representation for sentiment classification.",1 Introduction,[0],[0]
"The method is on the basis of the principle of compositionality (Frege, 1892), which states that the meaning of a longer expression (e.g. a sentence or a docu-
1422
ment) depends on the meanings of its constituents.",1 Introduction,[0],[0]
"Specifically, the approach models document representation in two steps.",1 Introduction,[0],[0]
"In the first step, it uses convolutional neural network (CNN) or long short-term memory (LSTM) to produce sentence representations from word representations.",1 Introduction,[0],[0]
"Afterwards, gated recurrent neural network is exploited to adaptively encode semantics of sentences and their inherent relations in document representations.",1 Introduction,[0],[0]
These representations are naturally used as features to classify the sentiment label of each document.,1 Introduction,[0],[0]
"The entire model is trained end-to-end with stochastic gradient descent, where the loss function is the cross-entropy error of supervised sentiment classification2.
",1 Introduction,[0],[0]
We conduct document level sentiment classification on four large-scale review datasets from IMDB3 and Yelp Dataset Challenge4.,1 Introduction,[0],[0]
"We compare to neural network models such as paragraph vector (Le and Mikolov, 2014), convolutional neural network, and baselines such as feature-based SVM (Pang et al., 2002), recommendation algorithm JMARS (Diao et al., 2014).",1 Introduction,[0],[0]
"Experimental results show that: (1) the proposed neural model shows superior performances over all baseline algorithms; (2) gated recurrent neural network dramatically outperforms standard recurrent neural
2A similar work can be found at: http: //deeplearning.net/tutorial/lstm.html
3http://www.imdb.com/ 4http://www.yelp.com/dataset_challenge
network in document modeling.",1 Introduction,[0],[0]
The main contributions of this work are as follows: • We present a neural network approach to encode relations between sentences in document representation for sentiment classification.,1 Introduction,[0],[0]
"•We report empirical results on four large-scale datasets, and show that the approach outperforms state-of-the-art methods for document level sentiment classification.",1 Introduction,[0],[0]
"•We report empirical results that traditional recurrent neural network is weak in modeling document composition, while adding neural gates dramatically improves the classification performance.",1 Introduction,[0],[0]
"We introduce the proposed neural model in this section, which computes continuous vector representations for documents of variable length.",2 The Approach,[0],[0]
These representations are further used as features to classify the sentiment label of each document.,2 The Approach,[0],[0]
"An overview of the approach is displayed in Figure 1.
",2 The Approach,[0],[0]
"Our approach models document semantics based on the principle of compositionality (Frege, 1892), which states that the meaning of a longer expression (e.g. a sentence or a document) comes from the meanings of its constituents and the rules used to combine them.",2 The Approach,[0],[0]
"Since a document consists of a list of sentences and each sentence is made up of a list of words, the approach models document representation in two stages.",2 The Approach,[0],[0]
"It first produces continuous sentence vectors from word represen-
tations with sentence composition (Section 2.1).",2 The Approach,[0],[0]
"Afterwards, sentence vectors are treated as inputs of document composition to get document representation (Section 2.2).",2 The Approach,[0],[0]
Document representations are then used as features for document level sentiment classification (Section 2.3).,2 The Approach,[0],[0]
"We first describe word vector representation, before presenting a convolutional neural network with multiple filters for sentence composition.
",2.1 Sentence Composition,[0],[0]
"Each word is represented as a low dimensional, continuous and real-valued vector, also known as word embedding (Bengio et al., 2003).",2.1 Sentence Composition,[0],[0]
"All the word vectors are stacked in a word embedding matrix Lw ∈ Rd×|V |, where d is the dimension of word vector and |V",2.1 Sentence Composition,[0],[0]
| is vocabulary size.,2.1 Sentence Composition,[0],[0]
"These word vectors can be randomly initialized from a uniform distribution (Socher et al., 2013b), or be pre-trained from text corpus with embedding learning algorithms (Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014).",2.1 Sentence Composition,[0],[0]
"We adopt the latter strategy to make better use of semantic and grammatical associations of words.
",2.1 Sentence Composition,[0],[0]
We use convolutional neural network (CNN) and long short-term memory (LSTM) to compute continuous representations of sentences with semantic composition.,2.1 Sentence Composition,[0],[0]
"CNN and LSTM are stateof-the-art semantic composition models for sentiment classification (Kim, 2014; Kalchbrenner et al., 2014; Johnson and Zhang, 2015; Li et al., 2015a).",2.1 Sentence Composition,[0],[0]
"They learn fixed-length vectors for sentences of varying length, captures words order in a sentence and does not depend on external dependency or constituency parse results.",2.1 Sentence Composition,[0],[0]
"One could also use tree-based composition method such as Recursive Neural Tensor Network (Socher et al., 2013b) or Tree-Structured LSTM (Tai et al., 2015; Zhu et al., 2015) as alternatives.
",2.1 Sentence Composition,[0],[0]
"Specifically, we try CNN with multiple convolutional filters of different widths (Tang et al., 2015) to produce sentence representation.",2.1 Sentence Composition,[0],[0]
Figure 2 displays the method.,2.1 Sentence Composition,[0],[0]
"We use multiple convolutional filters in order to capture local semantics of n-grams of various granularities, which have been proven effective for sentiment classification.",2.1 Sentence Composition,[0],[0]
"For example, a convolutional filter with a width of 2 essentially captures the semantics of bigrams in a sentence.",2.1 Sentence Composition,[0],[0]
"In this work, we use three convolutional filters whose widths are 1, 2 and 3 to encode the semantics of unigrams, bigram-
s and trigrams in a sentence.",2.1 Sentence Composition,[0],[0]
Each filter consists of a list of linear layers with shared parameters.,2.1 Sentence Composition,[0],[0]
"Formally, let us denote a sentence consisting of n words as {w1, w2, ...wi, ...wn}, let lc be the width of a convolutional filter, and let Wc, bc be the shared parameters of linear layers in the filter.",2.1 Sentence Composition,[0],[0]
Each word wi is mapped to its embedding representation ei ∈ Rd.,2.1 Sentence Composition,[0],[0]
"The input of a linear layer is the concatenation of word embeddings in a fixed-length window size lc, which is denoted as Ic =",2.1 Sentence Composition,[0],[0]
[ei; ei+1; ...; ei+lc−1] ∈ Rd·lc .,2.1 Sentence Composition,[0],[0]
"The output of a linear layer is calculated as
Oc = Wc · Ic + bc (1)
where Wc ∈ Rloc×d·lc , bc ∈ Rloc , loc is the output length of linear layer.",2.1 Sentence Composition,[0],[0]
"To capture global semantics of a sentence, we feed the outputs of linear layers to an average pooling layer, resulting in an output vector with fixed-length.",2.1 Sentence Composition,[0],[0]
"We further add hyperbolic tangent (tanh) to incorporate pointwise nonlinearity, and average the outputs of multiple filters to get sentence representation.
",2.1 Sentence Composition,[0],[0]
"We also try lstm as the sentence level semantic calculator, the performance comparison between these two variations is given in Section 3.",2.1 Sentence Composition,[0],[0]
The obtained sentence vectors are fed to a document composition component to calculate the document representation.,2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
"We present a gated recurrent neural network approach for document composition in this part.
",2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
"Given the vectors of sentences of variable length as input, document composition produces a fixed-length document vector as output.",2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
"To this end, a simple strategy is ignoring the order of sen-
tences and averaging sentence vectors as document vector.",2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
"Despite its computational efficiency, it fails to capture complex linguistic relations (e.g. “cause” and “contrast”) between sentences.",2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
"Convolutional neural network (Denil et al., 2014) is an alternative for document composition, which models local sentence relations with shared parameters of linear layers.
",2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
Standard recurrent neural network (RNN) can map vectors of sentences of variable length to a fixed-length vector by recursively transforming current sentence vector st with the output vector of the previous step ht−1.,2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
"The transition function is typically a linear layer followed by pointwise non-linearity layer such as tanh.
",2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
ht = tanh(Wr ·,2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
"[ht−1; st] + br) (2) where Wr ∈ Rlh×(lh+loc), br ∈ Rlh , lh and loc are dimensions of hidden vector and sentence vector, respectively.",2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
"Unfortunately, standard RNN suffers the problem of gradient vanishing or exploding (Bengio et al., 1994; Hochreiter and Schmidhuber, 1997), where gradients may grow or decay exponentially over long sequences.",2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
This makes it difficult to model long-distance correlations in a sequence.,2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
"To address this problem, we develop a gated recurrent neural network for document composition, which works in a sequential way and adaptively encodes sentence semantics in document representations.",2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
"The approach is analogous to the recently emerged LSTM (Graves et al., 2013; Zaremba and Sutskever, 2014; Sutskever et al., 2014; Xu et al., 2015) and gated neural network (Cho et al., 2014; Chung et al., 2015).",2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
"Specifically, the transition function of the gated RNN used in this work is calculated as follows.
",2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
it = sigmoid(Wi ·,2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
[ht−1; st] + bi) (3) ft = sigmoid(Wf ·,2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
"[ht−1; st] + bf ) (4)
gt = tanh(Wr ·",2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
"[ht−1; st] + br) (5) ht = tanh(it gt + ft ht−1) (6)
where stands for element-wise multiplication, Wi, Wf , bi, bf adaptively select and remove history vector and input vector for semantic composition.",2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
"The model can be viewed as a LSTM whose output gate is alway on, since we prefer not to discarding any part of the semantics of sentences to get a better document representation.",2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
Figure 3 (a) displays a standard sequential way where the last hidden vector is regarded as the document representation for sentiment classification.,2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
"We can make further extensions such as averaging hidden vectors as document representation, which takes considerations of a hierarchy of historical semantics with different granularities.",2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
"The method is illustrated in Figure 3 (b), which shares some characteristics with (Zhao et al., 2015).",2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
"We can go one step further to use preceding histories and following evidences in the same way, and exploit bidirectional (Graves et al., 2013) gated RNN as the calculator.",2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
The model is embedded in Figure 1.,2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
The composed document representations can be naturally regarded as features of documents for sentiment classification without feature engineering.,2.3 Sentiment Classification,[0],[0]
"Specifically, we first add a linear layer to transform document vector to real-valued vector whose length is class number C. Afterwards, we add a softmax layer to convert real values to conditional probabilities, which is calculated as follows.
",2.3 Sentiment Classification,[0],[0]
"Pi = exp(xi)∑C
i′=1 exp(xi′) (7)
We conduct experiments in a supervised learning setting, where each document in the training data is accompanied with its gold sentiment label.
",2.3 Sentiment Classification,[0],[0]
Corpus #docs #s/d #w/d |V,2.3 Sentiment Classification,[0],[0]
"| #class Class Distribution
For model training, we use the cross-entropy error between gold sentiment distribution P g(d) and predicted sentiment distribution P (d) as the loss function.
loss = − ∑ d∈T",2.3 Sentiment Classification,[0],[0]
C∑ i=1,2.3 Sentiment Classification,[0],[0]
P gi (d) · log(Pi(d)),2.3 Sentiment Classification,[0],[0]
"(8)
where T is the training data, C is the number of classes, d represents a document.",2.3 Sentiment Classification,[0],[0]
"P g(d) has a 1-of-K coding scheme, which has the same dimension as the number of classes, and only the dimension corresponding to the ground truth is 1, with all others being 0.",2.3 Sentiment Classification,[0],[0]
We take the derivative of loss function through back-propagation with respect to the whole set of parameters θ =,2.3 Sentiment Classification,[0],[0]
"[Wc; bc;Wi; bi;Wf ; bf ;Wr; br;Wsoftmax, bsoftmax], and update parameters with stochastic gradient descent.",2.3 Sentiment Classification,[0],[0]
"We set the widths of three convolutional filters as 1, 2 and 3, output length of convolutional filter as 50.",2.3 Sentiment Classification,[0],[0]
"We learn 200-dimensional word embeddings with SkipGram (Mikolov et al., 2013) on each dataset separately, randomly initialize other parameters from a uniform distribution U(−0.01, 0.01), and set learning rate as 0.03.",2.3 Sentiment Classification,[0],[0]
We conduct experiments to empirically evaluate our method by applying it to document level sentiment classification.,3 Experiment,[0],[0]
We describe experimental settings and report empirical results in this section.,3 Experiment,[0],[0]
We conduct experiments on large-scale datasets consisting of document reviews.,3.1 Experimental Setting,[0],[0]
"Specifically, we use one movie review dataset from IMDB (Diao et al., 2014) and three restaurant review datasets from Yelp Dataset Challenge in 2013, 2014 and 2015.",3.1 Experimental Setting,[0],[0]
"Human labeled review ratings are regarded as gold standard sentiment labels, so that we do not need to manually annotate sentiment labels of
documents.",3.1 Experimental Setting,[0],[0]
"We do not consider the cases that rating does not match with review texts (Zhang et al., 2014).
",3.1 Experimental Setting,[0],[0]
Statistical information of these datasets are given in Table 1.,3.1 Experimental Setting,[0],[0]
"We use the same dataset split as in (Diao et al., 2014) on IMDB dataset, and split Yelp datasets into training, development and testing sets with 80/10/10.",3.1 Experimental Setting,[0],[0]
We run tokenization and sentence splitting with Stanford CoreNLP,3.1 Experimental Setting,[0],[0]
"(Manning et al., 2014) on all these datasets.",3.1 Experimental Setting,[0],[0]
"We use accuracy (Manning and Schütze, 1999; Jurafsky and Martin, 2000) and MSE (Diao et al., 2014) as evaluation metrics, where accuracy is a standard metric to measure the overall sentiment classification performance.",3.1 Experimental Setting,[0],[0]
"We use MSE to measure the divergences between predicted sentiment labels and ground truth sentiment labels because review labels reflect sentiment strengths (e.g. one star means strong negative and five star means strong positive).
",3.1 Experimental Setting,[0],[0]
"MSE = ∑N
i (goldi − predictedi)2 N
(9)",3.1 Experimental Setting,[0],[0]
"We compare our methods (Conv-GRNN and LSTM-GRNN) with the following baseline methods for document level sentiment classification.
",3.2 Baseline Methods,[0],[0]
"(1) Majority is a heuristic baseline, which assigns the majority sentiment label in training set to each document in test set.
",3.2 Baseline Methods,[0],[0]
"(2) In SVM+Ngrams, we use bag-of-unigrams and bag-of-bigrams as features and train SVM classifier with LibLinear (Fan et al., 2008)5.
",3.2 Baseline Methods,[0],[0]
"(3) In TextFeatures, we implement sophisticated features (Kiritchenko et al., 2014) including word ngrams, character ngrams, sentiment lexicon features, cluster features, et al.
5We also try discretized regression (Pang and Lee, 2005) with fixed decision thresholds (e.g. 0.5, 1.5, 2.5, ...).",3.2 Baseline Methods,[0],[0]
"However, its performance is obviously worse than SVM classifier.
(4) In AverageSG, we learn 200-dimensional word vectors with word2vec6 (Mikolov et al., 2013), average word embeddings to get document representation, and train a SVM classifier.
(5) We learn sentiment-specific word embeddings (SSWE), and use max/min/average pooling (Tang et al., 2014) to get document representation.
",3.2 Baseline Methods,[0],[0]
"(6) We compare with a state-of-the-art recommendation algorithm JMARS (Diao et al., 2014), which utilizes user and aspects of a review with collaborative filtering and topic modeling.
",3.2 Baseline Methods,[0],[0]
"(7) We implement a convolutional neural network (CNN) baseline as it is a state-of-the-art semantic composition method for sentiment analysis (Kim, 2014; Denil et al., 2014).
",3.2 Baseline Methods,[0],[0]
"(8) We implement a state-of-the-art neural network baseline Paragraph Vector (Le and Mikolov, 2014) because its codes are not officially provided.",3.2 Baseline Methods,[0],[0]
Window size is tuned on the development set.,3.2 Baseline Methods,[0],[0]
Experimental results are given in Table 2.,3.3 Comparison to Other Methods,[0],[0]
"We evaluate each dataset with two metrics, namely accuracy (higher is better) and MSE (lower is better).",3.3 Comparison to Other Methods,[0],[0]
"The best method in each dataset and each evaluation metric is in bold.
",3.3 Comparison to Other Methods,[0],[0]
"From Table 2, we can see that majority is the worst method because it does not capture any textual semantics.",3.3 Comparison to Other Methods,[0],[0]
"SVM classifiers with unigram and bigram features (Pang et al., 2002) are extremely strong, which are almost the strongest performers
6We use Skipgram as it performs slightly better than CBOW in the experiment.",3.3 Comparison to Other Methods,[0],[0]
"We also try off-the-shell word embeddings from Glove, but its performance is slightly worse than tailored word embedding from each corpus.
among all baseline methods.",3.3 Comparison to Other Methods,[0],[0]
"Designing complex features are also effective for document level sentiment classification, however, it does not surpass the bag-of-ngram features significantly as on Twitter corpora (Kiritchenko et al., 2014).",3.3 Comparison to Other Methods,[0],[0]
"Furthermore, the aforementioned bag-of-features are discrete and sparse.",3.3 Comparison to Other Methods,[0],[0]
"For example, the feature dimension of bigrams and TextFeatures on Yelp 2015 dataset are 899K and 4.81M after we filter out low frequent features.",3.3 Comparison to Other Methods,[0],[0]
"Based on them, we try to concatenate several discourse-driven features, but the classification performances remain unchanged.
",3.3 Comparison to Other Methods,[0],[0]
AverageSG,3.3 Comparison to Other Methods,[0],[0]
is a straight forward way to compose document representation without feature engineering.,3.3 Comparison to Other Methods,[0],[0]
"Unfortunately, we can see that it does not work in this scenario, which appeals for powerful semantic composition models for document level sentiment classification.",3.3 Comparison to Other Methods,[0],[0]
"We try to make better use of the sentiment information to learn a better SSWE (Tang et al., 2014), e.g. setting a large window size.",3.3 Comparison to Other Methods,[0],[0]
"However, its performance is still worse than context-based word embedding.",3.3 Comparison to Other Methods,[0],[0]
"This stems from the fact that there are many sentiment shifters (e.g. negation or contrast words) in document level reviews, while Tang et al. (2014) learn SSWE by assigning sentiment label of a text to each phrase it contains.",3.3 Comparison to Other Methods,[0],[0]
"How to learn SSWE effectively with document level sentiment supervision remains as an interesting future work.
",3.3 Comparison to Other Methods,[0],[0]
"Since JMARS outputs real-valued outputs, we only evaluate it in terms ofMSE.",3.3 Comparison to Other Methods,[0],[0]
"We can see that sophisticated baseline methods such as JMARS, paragraph vector and convolutional NN obtain significant performance boosts over AverageSG by
capturing deeper semantics of texts.",3.3 Comparison to Other Methods,[0],[0]
"Comparing between CNN and AverageSG, we can conclude that deep semantic compositionality is crucial for understanding the semantics and the sentiment of documents.",3.3 Comparison to Other Methods,[0],[0]
"However, it is somewhat disappointing that these models do not significantly outperform discrete bag-of-ngrams and bag-of-features.",3.3 Comparison to Other Methods,[0],[0]
"The reason might lie in that semantic meanings of documents, e.g. relations between sentences, are not well captured.",3.3 Comparison to Other Methods,[0],[0]
We can see that the proposed method Conv-GRNN and LSTM-GRNN yield the best performance on all four datasets in two evaluation metrics.,3.3 Comparison to Other Methods,[0],[0]
"Compared with CNN, Conv-GRNN shows its superior power in document composition component, which encodes semantics of sentences and their relations in document representation with gated recurrent neural network.",3.3 Comparison to Other Methods,[0],[0]
We also find that LSTM (almost) consistently performs better than CNN in modeling the sentence representation.,3.3 Comparison to Other Methods,[0],[0]
"As discussed before, document composition contributes a lot to the superior performance of ConvGRNN and LSTM-GRNN.",3.4 Model Analysis,[0],[0]
"Therefore, we take Conv-GRNN as an example and compare different neural models for document composition in this part.",3.4 Model Analysis,[0],[0]
"Specifically, after obtaining sentence vectors with convolutional neural network as described in Section 2.1, we carry out experiments in following settings.
",3.4 Model Analysis,[0],[0]
(1) Average.,3.4 Model Analysis,[0],[0]
"Sentence vectors are averaged to get the document vector.
(2) Recurrent / GatedNN.",3.4 Model Analysis,[0],[0]
Sentence vectors are fed to standard (or gated) recurrent neural network in a sequential way from the beginning of the input document.,3.4 Model Analysis,[0],[0]
"The last hidden vector is regarded as document representation.
",3.4 Model Analysis,[0],[0]
(3) Recurrent Avg / GatedNN,3.4 Model Analysis,[0],[0]
Avg.,3.4 Model Analysis,[0],[0]
"We extend setting (2) by averaging hidden vectors of recurrent neural network as document vector.
(4) Bi Recurrent Avg / Bi GatedNN Avg.",3.4 Model Analysis,[0],[0]
We extend setting (3) by calculating hidden vectors from both preceding histories and following contexts.,3.4 Model Analysis,[0],[0]
"Bi-directional hidden vectors are averaged as document representation.
",3.4 Model Analysis,[0],[0]
Table 3 shows the experimental results.,3.4 Model Analysis,[0],[0]
"We can see that standard recurrent neural network (RNN) is the worst method, even worse than the simple vector average.",3.4 Model Analysis,[0],[0]
"This is because RNN suffers from the vanishing gradient problem, stating that the influence of a given input on the hidden layer decays exponentially over time on the network output.",3.4 Model Analysis,[0],[0]
"In this paper, it means that document representation encodes rare semantics of the beginning sentences.",3.4 Model Analysis,[0],[0]
This is further justified by the great improvement of Recurrent Avg over Recurrent.,3.4 Model Analysis,[0],[0]
"Bi Recurrent Avg and Recurrent Avg perform comparably, but disappointingly both of them fail to transcend Average.",3.4 Model Analysis,[0],[0]
"After adding neural gates, GatedNN obtains dramatic accuracy improvements over Recurrent and significantly outperforms previous settings.",3.4 Model Analysis,[0],[0]
"The results indicate that Gated RNN is capable of handling the vanishing gradient problem to some extend, and it is practical to adaptively model sentence semantics in document representation. GatedNN",3.4 Model Analysis,[0],[0]
Avg and Bi GatedNN,3.4 Model Analysis,[0],[0]
Avg obtains comparable performances with GatedNN.,3.4 Model Analysis,[0],[0]
"Document level sentiment classification is a fundamental problem in sentiment analysis (Pang and Lee, 2008; Liu, 2012), which aims at identifying the sentiment label of a document (Pang et al., 2002; Turney, 2002).",4 Related Work,[0],[0]
"Pang and Lee (2002; 2005)
cast this problem as a classification task, and use machine learning method in a supervised learning framework.",4 Related Work,[0],[0]
Turney (2002) introduces an unsupervised approach by using sentiment words/phrases extracted from syntactic patterns to determine the document polarity.,4 Related Work,[0],[0]
"Goldberg and Zhu (2006) place this task in a semi-supervised setting, and use unlabelled reviews with graph-based method.",4 Related Work,[0],[0]
Dominant studies in literature follow Pang et al. (2002) and work on designing effective features for building a powerful sentiment classifier.,4 Related Work,[0],[0]
"Representative features include word ngrams (Wang and Manning, 2012), text topic (Ganu et al., 2009), bag-of-opinions (Qu et al., 2010), syntactic relations (Xia and Zong, 2010), sentiment lexicon features (Kiritchenko et al., 2014).
",4 Related Work,[0],[0]
"Despite the effectiveness of feature engineering, it is labor intensive and unable to extract and organize the discriminative information from data (Bengio et al., 2015).",4 Related Work,[0],[0]
"Recently, neural network emerges as an effective way to learn continuous text representation for sentiment classification.",4 Related Work,[0],[0]
Existing studies in this direction can be divided into two groups.,4 Related Work,[0],[0]
One line of research focuses on learning continuous word embedding.,4 Related Work,[0],[0]
"Traditional embedding learning algorithms typically leverage contexts of words in a context-prediction way (Bengio et al., 2003; Mikolov et al., 2013; Baroni et al., 2014).",4 Related Work,[0],[0]
"Since these methods typically map words with similar contexts but opposite polarity (e.g. “good” and “bad”) to neighboring vectors, several studies (Maas et al., 2011; Labutov and Lipson, 2013; Tang et al., 2014) learn sentiment-specific word embeddings by taking sentiment of texts into account.",4 Related Work,[0],[0]
"Another line of research concentrates on semantic composition (Mitchell and Lapata, 2010).",4 Related Work,[0],[0]
Yessenalina and Cardie (2011) represent each word as a matrix and use iterated matrix multiplication as phrase-level composition function.,4 Related Work,[0],[0]
Socher et al. (2013b) introduce a family of recursive neural networks for sentence-level semantic composition.,4 Related Work,[0],[0]
"Recursive neural network is extended with global feedbackward (Paulus et al., 2014), feature weight tuning (Li, 2014), deep recursive layer (Irsoy and Cardie, 2014), adaptive composition functions (Dong et al., 2014), combined with Combinatory Categorial Grammar (Hermann and Blunsom, 2013), and used for opinion relation detection (Xu et al., 2014).",4 Related Work,[0],[0]
Glorot et al. (2011) use stacked denoising autoencoder.,4 Related Work,[0],[0]
"Convolutional neural networks are widely used for semantic compo-
sition (Kim, 2014; Kalchbrenner et al., 2014; Denil et al., 2014; Johnson and Zhang, 2015) by automatically capturing local and global semantics.",4 Related Work,[0],[0]
Le and Mikolov (2014) introduce Paragraph Vector to learn document representation from semantics of words.,4 Related Work,[0],[0]
"Sequential model like recurrent neural network or long short-term memory (LSTM) are also verified as strong approaches for semantic composition (Li et al., 2015a).
",4 Related Work,[0],[0]
"In this work, we represent document with convolutional-gated recurrent neural network, which adaptively encodes semantics of sentences and their relations.",4 Related Work,[0],[0]
"A recent work in (Li et al., 2015b) also investigate LSTM to model document meaning.",4 Related Work,[0],[0]
They verify the effectiveness of LSTM in text generation task.,4 Related Work,[0],[0]
We introduce neural network models (ConvGRNN and LSTM-GRNN) for document level sentiment classification.,5 Conclusion,[0],[0]
"The approach encodes semantics of sentences and their relations in document representation, and is effectively trained end-to-end with supervised sentiment classification objectives.",5 Conclusion,[0],[0]
We conduct extensive experiments on four review datasets with two evaluation metrics.,5 Conclusion,[0],[0]
Empirical results show that our approaches achieve state-of-the-art performances on all these datasets.,5 Conclusion,[0],[0]
"We also find that (1) traditional recurrent neural network is extremely weak in modeling document composition, while adding neural gates dramatically boosts the performance, (2) LSTM performs better than a multi-filtered CNN in modeling sentence representation.
",5 Conclusion,[0],[0]
We briefly discuss some future plans.,5 Conclusion,[0],[0]
How to effectively compose sentence meanings to document meaning is a central problem in natural language processing.,5 Conclusion,[0],[0]
"In this work, we develop neural models in a sequential way, and encode sentence semantics and their relations automatically without using external discourse analysis results.",5 Conclusion,[0],[0]
"From one perspective, one could carefully define a set of sentiment-sensitive discourse relations (Zhou et al., 2011), such as “contrast”, “condition”, “cause”, etc.",5 Conclusion,[0],[0]
"Afterwards, relation-specific gated RNN can be developed to explicitly model semantic composition rules for each relation (Socher et al., 2013a).",5 Conclusion,[0],[0]
"However, defining such a relation scheme is linguistic driven and time consuming, which we leave as future work.",5 Conclusion,[0],[0]
"From another perspective, one could compose document
representation over discourse tree structures rather than in a sequential way.",5 Conclusion,[0],[0]
"Accordingly, Recursive Neural Network (Socher et al., 2013b) and Structured LSTM (Tai et al., 2015; Zhu et al., 2015) can be used as composition algorithms.",5 Conclusion,[0],[0]
"However, existing discourse structure learning algorithms are difficult to scale to massive review texts on the web.",5 Conclusion,[0],[0]
How to simultaneously learn document structure and composition function is an interesting future work.,5 Conclusion,[0],[0]
The authors give great thanks to Yaming Sun and Jiwei Li for the fruitful discussions.,Acknowledgments,[0],[0]
We also would like to thank three anonymous reviewers for their valuable comments and suggestions.,Acknowledgments,[0],[0]
"This work was supported by the National High Technology Development 863 Program of China (No. 2015AA015407), National Natural Science Foundation of China (No. 61133012 and No. 61273321).",Acknowledgments,[0],[0]
Duyu Tang is supported by Baidu Fellowship and IBM Ph.D. Fellowship.,Acknowledgments,[0],[0]
Document level sentiment classification remains a challenge: encoding the intrinsic relations between sentences in the semantic meaning of a document.,abstractText,[0],[0]
"To address this, we introduce a neural network model to learn vector-based document representation in a unified, bottom-up fashion.",abstractText,[0],[0]
The model first learns sentence representation with convolutional neural network or long short-term memory.,abstractText,[0],[0]
"Afterwards, semantics of sentences and their relations are adaptively encoded in document representation with gated recurrent neural network.",abstractText,[0],[0]
We conduct document level sentiment classification on four large-scale review datasets from IMDB and Yelp Dataset Challenge.,abstractText,[0],[0]
Experimental results show that: (1) our neural model shows superior performances over several state-of-the-art algorithms; (2) gated recurrent neural network dramatically outperforms standard recurrent neural network in document modeling for sentiment classification.1,abstractText,[0],[0]
Document Modeling with Gated Recurrent Neural Network for Sentiment Classification,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2044–2054 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"Document-level sentiment classification is one of the pragmatical sentiment analysis tasks (Pang and Lee, 2007; Liu, 2010).",1 Introduction,[0],[0]
"There are many Web sites having platforms for users to input reviews over products or services, such as TripAdvisor, Yelp, Amazon, etc.",1 Introduction,[0],[0]
Most of reviews are very comprehensive and thus long documents.,1 Introduction,[0],[0]
Analyzing these documents to predict ratings of products or services is an important complementary way for better customer relationship management.,1 Introduction,[0],[0]
"Recently, neural network based approaches have been developed and become state-of-the-arts for longdocument sentiment classification (Tang et al., 2015a,b; Yang et al., 2016).",1 Introduction,[0],[0]
"However, predicting an overall score for each long document is not enough, because the document can mention dif-
ferent aspects of the corresponding product or service.",1 Introduction,[0],[0]
"For example, in Figure 1, there could be different aspects for a review of hotel.",1 Introduction,[0],[0]
These aspects help customer service better understand what are the major pros and cons of the product or service.,1 Introduction,[0],[0]
"Compared to the overall rating, users are less motivated to give aspect ratings.",1 Introduction,[0],[0]
"Therefore, it is more practically useful to perform document-level multi-aspect sentiment classification task, predicting different ratings for each aspect rather than an overall rating.
",1 Introduction,[0],[0]
"One straightforward approach for documentlevel multi-aspect sentiment classification is multi-task learning (Caruana, 1997).",1 Introduction,[0],[0]
"For neural networks, we can simply treat each aspect (e.g., rating from one to five) as a classification task, and let different tasks use softmax classifier to extract task-specific representations at the top layer while share the input and hidden layers to mutually enhance the prediction results (Collobert et al., 2011; Luong et al., 2016).",1 Introduction,[0],[0]
"However, such approach ignores the fact that the aspects themselves have semantic meanings.",1 Introduction,[0],[0]
"For example, as human beings, if we were asked to evaluate the aspect rating of a document, we simply read the review, and find aspect-related keywords, and see around comments.",1 Introduction,[0],[0]
"Then, we aggregate all the related snippets to make a decision.
",1 Introduction,[0],[0]
"In this paper, we propose a novel approach to treat document-level multi-aspect sentiment clas-
2044
sification as a machine comprehension (Kumar et al., 2016; Sordoni et al., 2016) problem.",1 Introduction,[0],[0]
"To mimic human’s evaluation of aspect classification, we create a list of keywords for each aspect.",1 Introduction,[0],[0]
"For example, when we work on the Room aspect, we generate some keywords such as “room,” “bed,” “view,” etc.",1 Introduction,[0],[0]
Then we can ask pseudo questions: “How is the room?”,1 Introduction,[0],[0]
“How is the bed?”,1 Introduction,[0],[0]
“How is the view?” and provide an answer “Rating 5.”,1 Introduction,[0],[0]
"In this case, we can train a machine comprehension model to automatically attend corresponding text snippets in the review document to predict the aspect rating.",1 Introduction,[0],[0]
"Specifically, we introduce a hierarchical and iterative attention model to construct aspect-specific representations.",1 Introduction,[0],[0]
We use a hierarchical architecture to build up different representations at both word and sentence levels interacting with aspect questions.,1 Introduction,[0],[0]
"At each level, the model consists of input encoders and iterative attention modules.",1 Introduction,[0],[0]
The input encoder learns memories1 of documents and questions with Bi-directional LSTM (Bi-LSTM) model and non-linear mapping respectively.,1 Introduction,[0],[0]
"The iterative attention module takes into memories as input and attends them sequentially with a multiple hop mechanism, performing effective interactions between documents and aspect questions.
",1 Introduction,[0],[0]
"To evaluate the effectiveness of the proposed model, we conduct extensive experiments on the TripAdvisor and BeerAdvocate datasets and the results show that our model outperforms typical baselines.",1 Introduction,[0],[0]
"We also analyze the effects of num-
1Following the work (Weston et al., 2015; Sukhbaatar et al., 2015), we refer the memory to a set vectors which are stacked together and could be attended.
",1 Introduction,[0],[0]
bers of the hop and aspect words on performances.,1 Introduction,[0],[0]
"Moreover, a case study for attention results is performed at both word and sentence levels.
",1 Introduction,[0],[0]
The contributions of this paper are two-fold.,1 Introduction,[0],[0]
"First, we study the document-level multi-aspect sentiment classification as a machine comprehension problem and introduce a hierarchical iterative attention model for it.",1 Introduction,[0],[0]
"Second, we demonstrate the effectiveness of proposed model on two datasets, showing that our model outperforms classical baselines.",1 Introduction,[0],[0]
The code and data for this paper are available at https://github.com/ HKUST-KnowComp/DMSCMC.,1 Introduction,[0],[0]
"In this section, we introduce our proposed method.",2 Method,[0],[0]
We first briefly introduce the problem we work on.,2.1 Problem Definition and Hierarchical Framework,[0],[0]
"Given a piece of review, our task is to predict the ratings of different aspects.",2.1 Problem Definition and Hierarchical Framework,[0],[0]
"For example, in Figure 1, we predict the ratings of Cleanliness, Room, and Value.",2.1 Problem Definition and Hierarchical Framework,[0],[0]
"To achieve this, we assume that there are existing reviews with aspect ratings for machines to learn.",2.1 Problem Definition and Hierarchical Framework,[0],[0]
"Formally, we denote the review document as d containing a set of Td sentences {s1, s2, . . .",2.1 Problem Definition and Hierarchical Framework,[0],[0]
sTd}.,2.1 Problem Definition and Hierarchical Framework,[0],[0]
"For the t-th sentence st, we use a set of words { w1, w2, . . .",2.1 Problem Definition and Hierarchical Framework,[0],[0]
"w|st| } to represent it, and use wi, wwi and w p",2.1 Problem Definition and Hierarchical Framework,[0],[0]
"i as the one-hot encoding, word embedding, and phrase embedding for wi respectively.",2.1 Problem Definition and Hierarchical Framework,[0],[0]
"The phrase embedding encodes the semantics of phrases where the current word wi is the center (e.g., hidden vectors learned by Bi-LSTM shown in Section 2.2).",2.1 Problem Definition and Hierarchical Framework,[0],[0]
"For each qk of K aspects
{q1, q2, . . .",2.1 Problem Definition and Hierarchical Framework,[0],[0]
", qK}, we use Nk aspect-related keywords, { qk1 , qk2 . . .",2.1 Problem Definition and Hierarchical Framework,[0],[0]
"qkNk } , to represent it.",2.1 Problem Definition and Hierarchical Framework,[0],[0]
"Similarly, we use qki , q w ki
as the one-hot encoding and word embedding for qki respectively.
",2.1 Problem Definition and Hierarchical Framework,[0],[0]
"There are several sophisticated methods for choosing aspect keywords (e.g., topic model).",2.1 Problem Definition and Hierarchical Framework,[0],[0]
"Here, we consider a simple way where five seeds were first manually selected for each aspect and then more words were obtained based on their cosine similarities with seeds2
As shown in Figure 2 (left), our framework follows the idea of multi-task learning, which learns different aspects simultaneously.",2.1 Problem Definition and Hierarchical Framework,[0],[0]
"In this case, all these tasks share the representations of words and architecture of semantic model for the final classifiers.",2.1 Problem Definition and Hierarchical Framework,[0],[0]
"Different from straightforward neural network based multi-task learning (Collobert et al., 2011), for each document d and an aspect qk, our model uses both the content of d and all the related keywords { qk1 , qk2 . . .",2.1 Problem Definition and Hierarchical Framework,[0],[0]
qkNk } as input.,2.1 Problem Definition and Hierarchical Framework,[0],[0]
"Since the keywords can cover most of the semantic meanings of the aspect, and we do not know which document mentions which semantic meaning, we build an attention model to automatically decide it (introduced in Section 2.3).",2.1 Problem Definition and Hierarchical Framework,[0],[0]
"Assuming that the keywords have been decided, we use a hierarchical attention model to select useful information from the review documents.",2.1 Problem Definition and Hierarchical Framework,[0],[0]
"As shown in Figure 2 (right), the hierarchical attention of keywords is applied to both sentence level (to select meaningful words) and document level (to select meaningful sentence).",2.1 Problem Definition and Hierarchical Framework,[0],[0]
"Thus, our model builds aspectspecific representations in a bottom-up manner.
",2.1 Problem Definition and Hierarchical Framework,[0],[0]
"Specifically, we obtain sentence representations { sk1, sk2, . . .",2.1 Problem Definition and Hierarchical Framework,[0],[0]
skT } using the input encoder (Section 2.2) and iterative attention module (Section 2.3) at the word level.,2.1 Problem Definition and Hierarchical Framework,[0],[0]
Then we take sentence representations and k-th aspect as input and apply the sentence-level input encoder and attention model to generate the document representation dk for final classification.,2.1 Problem Definition and Hierarchical Framework,[0],[0]
"As shown in Figure 2 (right), the attention model is applied twice at different levels of the representation.",2.1 Problem Definition and Hierarchical Framework,[0],[0]
The input module builds memory vectors for the iterative attention module and is performed both at word and sentence levels.,2.2 Input Encoder,[0],[0]
"For a document, it con-
2For example, the words “value,” “price,” “worth,” “cost,” and “$” are selected as seeds for aspect Price.",2.2 Input Encoder,[0],[0]
"The information for seeds can be found in our released resource.
",2.2 Input Encoder,[0],[0]
verts word sequence into word level memory Mdw and sentence sequence into sentence level memory Mds respectively.,2.2 Input Encoder,[0],[0]
"For an aspect question qk, it takes a set of aspect-specific words {qki}1≤i≤Nk as input and derives word level memory Mqw and sentence level memory Mqs.
To construct Mdw, we obtain word embeddings{ ww1 , ww2 , . . .",2.2 Input Encoder,[0],[0]
ww|st| } from an embedding matrix EA applied to all words shown in the corpus.,2.2 Input Encoder,[0],[0]
"Then, LSTM (Hochreiter and Schmidhuber, 1997) model is used as the encoder to produce hidden vectors of words based on the word embeddings.",2.2 Input Encoder,[0],[0]
"At each step, LSTM takes input wwt and derives a new hidden vector by ht = LSTM(wwt , ht−1).",2.2 Input Encoder,[0],[0]
"To preserve the subsequent context information for words, another LSTM is ran over word sequence in a reverse order simultaneously.",2.2 Input Encoder,[0],[0]
"Then the forward hidden vector −→ h t and backward hidden
vector ←− h t are concatenated as phrase embedding wpt .",2.2 Input Encoder,[0],[0]
We stack these phrase embeddings together as word level memory Mdw.,2.2 Input Encoder,[0],[0]
"Similarly, we feed sentence representations into another Bi-LSTM to derive the sentence level memory Mds .",2.2 Input Encoder,[0],[0]
"Note that, the sentence representations are obtained using the iterative attention module which is described as Eq. (5) in Section 2.3.
",2.2 Input Encoder,[0],[0]
"Since we have question keywords as input, to allow the interactions between questions and documents, we also build question memory in following way.",2.2 Input Encoder,[0],[0]
"We obtain Qk = { qwki }
1≤i≤Nk by looking up an embedding matrix 3 EB applied to all question keywords.",2.2 Input Encoder,[0],[0]
"Then a non-linear mapping is applied to obtain the question memory at word level:
Mqkw = tanh(QkW q w), (1)
where Wqw is the parameter matrix to adapt qk at word level.",2.2 Input Encoder,[0],[0]
"Similarly, we use another mapping to obtain the sentence level memory:
Mqks = tanh(QkW q s), (2)
where Wqs is the parameter matrix to adapt qk at sentence level.",2.2 Input Encoder,[0],[0]
"The iterative attention module (IAM) attends and reads memories of questions and documents alternatively with a multi-hop mechanism, deriving
3EA and EB are initialized by the same pre-trained embeddings but are different embedding matrices with different updates.
aspect-specific sentence and document representations.",2.3 Iterative Attention Module,[0],[0]
"As we discussed in the introduction, the set of selected question keywords may not best characterize the aspect for different documents.",2.3 Iterative Attention Module,[0],[0]
"Thus, the IAM module introduces a backward attention to use document information (word or sentence) to select useful keywords of each aspect as the document-specific question to build attention model.
",2.3 Iterative Attention Module,[0],[0]
The illustration of IAM is shown in Figure 3.,2.3 Iterative Attention Module,[0],[0]
"To obtain sentence representations, it takes Mdw and Mqw as the input and performs m iterations (hops).",2.3 Iterative Attention Module,[0],[0]
"For each iteration, IAM conducts four operations: (1) attends the question memory by the selective vector p and summarizes question memory vectors into a single vector q̂; (2) updates the selective vector by the previous one and q̂; (3) attends document (content) memory based on the updated selective vector and summarizes memory vectors in to a single vector ĉ; (4) updates the selective vector by the previous one and ĉ.
We unify operations (1) and (3) by an attention function x̂ = A(p, M), where M could be Mdw or Mqw which corresponds x̂ = ĉ or x̂ = q̂.",2.3 Iterative Attention Module,[0],[0]
"The attention function A is decomposed as:
H = tanh(MWa (1p))",2.3 Iterative Attention Module,[0],[0]
"a = softmax(HvTa )
",2.3 Iterative Attention Module,[0],[0]
x̂,2.3 Iterative Attention Module,[0],[0]
"= ∑ aiMi,
(3)
where 1 is a vector with all elements are 1, which copies the selective vector to meet the dimension requirement.",2.3 Iterative Attention Module,[0],[0]
"The Wa and va are parameters, a is attention weights for memory vectors, and Mi
means i-th row in M. Operations (2) and (4) are formulated as an update function p2i−{l} = U(x̂, p2i−{l}−1), where i is the hop index, l can be 0 or 1 which corresponds to x̂ = ĉ or x̂ = q̂ respectively.",2.3 Iterative Attention Module,[0],[0]
We initialize p0 by a zero vector.,2.3 Iterative Attention Module,[0],[0]
"The update function U can be a recurrent neural network (Xiong et al., 2017) or other heuristic weighting functions.",2.3 Iterative Attention Module,[0],[0]
"In this paper, we introduce a simple strategy:
p2i−{l} = x̂, (4)
which ignores the previous selective vector but succeeds to obtain comparable results with other more complicated function in the initial experiments.
",2.3 Iterative Attention Module,[0],[0]
"Multi-hop mechanism attends different memory locations in different hops (Sukhbaatar et al., 2015), capturing different interactions between documents and questions.",2.3 Iterative Attention Module,[0],[0]
"In order to preserve the information of various kinds of interactions, we concatenate all ĉ’s in each hop as the final representations of sentences:
s = [ĉ1; ĉ2; · · · ĉm].",2.3 Iterative Attention Module,[0],[0]
"(5) After obtaining sentence representations, we feed them into the sentence-level input encoder, deriving the memories Mds and Mqs.",2.3 Iterative Attention Module,[0],[0]
"Then, the aspect-specific document representation dk is obtained by the sentence-level IAM in a similar way.",2.3 Iterative Attention Module,[0],[0]
"For each aspect, we obtain aspect-specific document representations {dk}1≤k≤K .",2.4 Objective Function,[0],[0]
"All these representations are fed into classifiers, each of which includes a softmax layer.",2.4 Objective Function,[0],[0]
"The softmax layer outputs the probability distribution over |Y| categories for the distributed representation, which is defined as:
p′(d, k) = softmax(Wclassk dk), (6)
where Wclassk is the parameter matrix.",2.4 Objective Function,[0],[0]
"We define the cross-entropy objective function between gold sentiment distribution p(d, k) and predicted sentiment distribution p′(d, k) as the classification loss function:
− ∑ d∈D K∑ k=1",2.4 Objective Function,[0],[0]
|Y|∑ i=1,2.4 Objective Function,[0],[0]
"p(d, k)log(p′(d, k)), (7)
where p(d, k) is a one-hot vector, which has the same dimension as the number of classes, and only the dimension associated with the ground truth label is one, with others being zeros.",2.4 Objective Function,[0],[0]
"In this section, we show experimental results to demonstrate our proposed algorithm.",3 Experiment,[0],[0]
"We conduct our experiments on TripAdvisor (Wang et al., 2010) and BeerAdvocate (McAuley et al., 2012; Lei et al., 2016) datasets, which contain seven aspects (value, room, location, cleanliness, check in/front desk, service, and business service) and four aspects (feel, look, smell, and taste) respectively.",3.1 Datasets,[0],[0]
"We follow the processing step (Lei et al., 2016) by choosing the reviews with different aspect ratings and the new datasets are described in Table 1.",3.1 Datasets,[0],[0]
"We tokenize the datasets by Stanford corenlp4 and randomly split them into training, development, and testing sets with 80/10/10%.",3.1 Datasets,[0],[0]
"To demonstrate the effectiveness of the proposed method, we compare our model with following baselines:
Majority uses the majority sentiment label in development sets as the predicted label.
",3.2 Baseline Methods,[0],[0]
"SVM uses unigram and bigram as text features and uses Liblinear (Fan et al., 2008) for learning.
",3.2 Baseline Methods,[0],[0]
"SLDA refers to supervised latent Dirichlet allocation (Blei and Mcauliffe, 2010) which is a statistical model of labeled documents.
",3.2 Baseline Methods,[0],[0]
"NBoW is a neural bag-of-words model averaging embeddings of all words in a document and feeds the resulted embeddings into SVM classifier.
",3.2 Baseline Methods,[0],[0]
DAN is a deep averaging network model which consists of several fully connected layers with averaged word embeddings as input.,3.2 Baseline Methods,[0],[0]
"One novel word dropout strategy is employed to boost model performances (Iyyer et al., 2015).
",3.2 Baseline Methods,[0],[0]
"CNN continuously performs a convolution operation over a sentence to extract words neighboring features, then gets a fixed-sized representation by a pooling layer (Kim, 2014).
",3.2 Baseline Methods,[0],[0]
"4http://nlp.stanford.edu/software/corenlp.shtml
LSTM is one variant of recurrent neural network and has been proved to be one of state-ofthe-art models for document-level sentiment classification (Tang et al., 2015a).",3.2 Baseline Methods,[0],[0]
"We use LSTM to refer Bi-LSTM which captures both forward and backward semantic information.
",3.2 Baseline Methods,[0],[0]
"HAN means the hierarchical attention network which is proposed in (Yang et al., 2016) for document classification.",3.2 Baseline Methods,[0],[0]
"Note that, the original HAN depends GRU as the encoder.",3.2 Baseline Methods,[0],[0]
"In our experiments, LSTM-based HAN obtains slightly better results.",3.2 Baseline Methods,[0],[0]
"Thus, we report the results of HAN with LSTM as the encoder.
",3.2 Baseline Methods,[0],[0]
"We extend DAN, CNN, LSTM with the hierarchical architecture and multi-task framework, the corresponding models are MHDAN, MHCNN and MHLSTM respectively.",3.2 Baseline Methods,[0],[0]
"Besides, MHAN is also evaluated as one baseline, which is HAN with the multi-task learning.",3.2 Baseline Methods,[0],[0]
"We implement all neural models using Theano (Theano Development Team, 2016).",3.3 Implementation Details,[0],[0]
The model parameters are tuned based on the development sets.,3.3 Implementation Details,[0],[0]
"We learn 200-dimensional word embeddings with Skip-gram model (Mikolov et al., 2013) on in-domain corpus, which follows (Tang et al., 2015a).",3.3 Implementation Details,[0],[0]
The pre-trained word embeddings are used to initialize the embedding matrices EA and EB .,3.3 Implementation Details,[0],[0]
The dimensions of all hidden vectors are set to 200.,3.3 Implementation Details,[0],[0]
"For TripAdvisor dataset, the hop numbers of word-level and sentence-level iterative attention modules are set to 4 and 2 respectively.",3.3 Implementation Details,[0],[0]
"For BeerAdvocate dataset, the hop numbers are set to 6 and 2.",3.3 Implementation Details,[0],[0]
The number of selected keywords Nk = N is set to 20.,3.3 Implementation Details,[0],[0]
"To avoid model over-fitting, we use dropout and regularization as follows: (1) the regularization parameter is set to 1e-5; (2) the dropout rate is set to 0.3, which is applied to both sentence and document vectors.",3.3 Implementation Details,[0],[0]
"All parameters are trained by ADADELTA (Zeiler, 2012) without needing to set the initial learning rate.",3.3 Implementation Details,[0],[0]
"To ensure fair comparisons, we make baselines have same settings as the proposed model, such as word embeddings, dimensions of hidden vectors and optimization details and so on.",3.3 Implementation Details,[0],[0]
"We use accuracy and mean squared error (MSE) as the evaluation metrics and the results are shown in Table 2.
",3.4 Results and Analyses,[0],[0]
"Compared to SVM and SLDA, NBoW achieves higher accuracy by 3% in both datasets, which shows that embedding features are more effective than traditional ngram features on these two datasets.",3.4 Results and Analyses,[0],[0]
All neural network models outperform NBoW.,3.4 Results and Analyses,[0],[0]
"It shows the advantages of neural networks in the document sentiment classification.
",3.4 Results and Analyses,[0],[0]
"From the results of neural networks, we can observe that DAN performs worse than LSTM and CNN, and LSTM achieves slightly higher results than CNN.",3.4 Results and Analyses,[0],[0]
"It can be explained that the simple composition method averaging embeddings of words in a document but ignoring word order, may not be as effective as other flexible composition models, such as LSTM and CNN, on aspect classification.",3.4 Results and Analyses,[0],[0]
"Additionally, we observe that the multi-task learning and hierarchical architecture are beneficial for neural networks.",3.4 Results and Analyses,[0],[0]
"Among all baselines, MHAN and MHLSTM achieve comparable results and outperform others.
",3.4 Results and Analyses,[0],[0]
"Compared with MHAN and MHLSTM, our method achieves improvements of 1.5% (3% relative improvement) and 1.0% (2.5% relative improvement) on TripAdvisor and BeerAdvocate re-
spectively, which shows that the incorporation of iterative attention mechanism helps the deep neural network based model build up more discriminative aspect-aware representation.",3.4 Results and Analyses,[0],[0]
Note that BeerAdvocate is relatively more difficult since the predicted ratings are from 1 to 10 while TripAdvisor is 1 to 5.,3.4 Results and Analyses,[0],[0]
"Moreover, t-test is conducted by randomly splitting datasets into train/dev/test sets and random initialization.",3.4 Results and Analyses,[0],[0]
The results on test sets are described in Table 3 which show performance of our model is stable.,3.4 Results and Analyses,[0],[0]
"In this section, we sample two sentences from TripAdvisor to show the visualization of attention results for case study.",3.5 Case Study for Attention Results,[0],[0]
Both word-level and sentence-level attention visualizations are shown in Figure 4.,3.5 Case Study for Attention Results,[0],[0]
"We normalize the word weight by the sentence weight to make sure that only important words in a document are highlighted.
",3.5 Case Study for Attention Results,[0],[0]
"From the top figures in (a) and (b), we observe that our model assigns different attention weights for each aspect.",3.5 Case Study for Attention Results,[0],[0]
"For example, in the first sentence, the words comfortable and bed are assigned higher
weights in the aspect Room, and the word clean are highlighted by the aspect Cleaniness.",3.5 Case Study for Attention Results,[0],[0]
"In the second sentence, the word internet is assigned a high attention value for Business.",3.5 Case Study for Attention Results,[0],[0]
"Moreover, the bottom figures in (a) and (b) show that (1) word weights of different hops are various; (2) attention values in higher hop are more reasonable.",3.5 Case Study for Attention Results,[0],[0]
"Specifically, in the first sentence, the weight of word clean is higher than the word comfortable in first hop, while comfortable surpasses clean in higher hops.",3.5 Case Study for Attention Results,[0],[0]
"In the second sentence, we observe that the value of word internet increases with the number of hop.",3.5 Case Study for Attention Results,[0],[0]
"Thus, we can see that the more sensible weights are obtained for words through the proposed iterative attention mechanism.",3.5 Case Study for Attention Results,[0],[0]
"Similarly, the figures (c) and (d) show that the conclusion from words is also suitable for sentences.",3.5 Case Study for Attention Results,[0],[0]
"For the first sentence, the sentence weight regarding the aspect Room is lower than Cleanliness in the first hop, but surpasses Cleanliness in the second hop.",3.5 Case Study for Attention Results,[0],[0]
"For the second sentence, the weight for Business becomes higher in the second hop.",3.5 Case Study for Attention Results,[0],[0]
"In this experiment, we investigate the effects of hop number m and size of aspect keywords N on performances.",3.6 Effects of Hop and Aspect Keywords,[0],[0]
"All the experiments are conducted
on the development set.",3.6 Effects of Hop and Aspect Keywords,[0],[0]
"Due to lack of space, we only present the results of TripAdvisor and the results of BeerAdvocate have a similar behavior as TripAdvisor.
",3.6 Effects of Hop and Aspect Keywords,[0],[0]
"For the hop number, we vary m from 1 to 7 and the results are shown in Figure 5 (left).",3.6 Effects of Hop and Aspect Keywords,[0],[0]
"We can see that: (1) at the word level, the performance increases when m ≤ 4, but shows no improvement after m > 4; (2) at the sentence level, model performs best when m = 2.",3.6 Effects of Hop and Aspect Keywords,[0],[0]
"Moreover, we can see that the hop number of word level leads to larger variation than the hop number of sentence level.
",3.6 Effects of Hop and Aspect Keywords,[0],[0]
"For the size of aspect keywords, we vary N from 0 to 35, incremented by 5.",3.6 Effects of Hop and Aspect Keywords,[0],[0]
"Note that, we set a learnable vector to represent question memory when N = 0.",3.6 Effects of Hop and Aspect Keywords,[0],[0]
The results are shown in Figure 5 (right).,3.6 Effects of Hop and Aspect Keywords,[0],[0]
"We observe that the performance increases when N ≤ 20, and has no improvement after N > 20.",3.6 Effects of Hop and Aspect Keywords,[0],[0]
This indicates that a small number of keywords can help the proposed model achieve competitive results.,3.6 Effects of Hop and Aspect Keywords,[0],[0]
Multi-Aspect Sentiment Classification.,4 Related Work,[0],[0]
Multiaspect sentiment classification has been studied extensively in literature.,4 Related Work,[0],[0]
"Lu et al. (2011) used support vector regression model based on hand-
crafted features to predict aspect ratings.",4 Related Work,[0],[0]
"To handle the correlation between aspects, McAuley et al. (2012) added a dependency term in final multi-class SVM objective.",4 Related Work,[0],[0]
"There were also some heuristic based methods and sophisticated topic models where multi-aspect sentiment classification is solved as a subproblem (Titov and McDonald, 2008; Wang et al., 2010; Diao et al., 2014; Pappas and Popescu-Belis, 2014).",4 Related Work,[0],[0]
"However, these approaches often rely on strict assumptions about words and sentences, for example, using the word syntax to determine if a word is an aspect or a sentiment word, or relating a sentence with an specific aspect.",4 Related Work,[0],[0]
"Another related problem is called aspect-based sentiment classification (Pontiki et al., 2014, 2016; Poria et al., 2016), which first extracts aspect expressions from sentences (Poria et al., 2014; Balahur and Montoyo, 2008; Chen et al., 2014, 2013), and then determines their sentiments.",4 Related Work,[0],[0]
"With the developments of neural networks and word embeddings in NLP, neural network based models have shown the state-of-the-art results with less feature engineering work.",4 Related Work,[0],[0]
Tang et al. (2016) employed a deep memory network for aspect-based sentiment classification given the aspect location and Lakkaraju et al. (2014) employed recurrent neural networks and its variants for the task of extraction of aspectsentiment pair.,4 Related Work,[0],[0]
"However, these tasks are sentencelevel.",4 Related Work,[0],[0]
Another related research field is documentlevel sentiment classification because we can treat single aspect sentiment classification as an individual document classification task.,4 Related Work,[0],[0]
"This line of research includes (Tang et al., 2015b; Chen et al., 2016; Tang et al., 2016; Yang et al., 2016) which are based on neural networks in a hierarchical structure.",4 Related Work,[0],[0]
"However, they did not work on multiple aspects.
",4 Related Work,[0],[0]
Machine Comprehension.,4 Related Work,[0],[0]
"Recently, neural network based machine comprehension (or reading) has been studied extensively in NLP, with the releases of large-scale evaluation datasets (Hermann et al., 2015; Hill et al., 2016; Rajpurkar et al., 2016).",4 Related Work,[0],[0]
"Most of the related studies focus on attention mechanism (Bahdanau et al., 2014) which is firstly proposed in machine translating and aims to solve the long-distance dependency between words.",4 Related Work,[0],[0]
"Hermann et al. (2015) used BiLSTM to encode document and query, and proposed Attentive Reader and Impatient Reader.",4 Related Work,[0],[0]
"The first one attends document based on the query representation, and the second one attends document by the representation of each token in query with an incremental manner.",4 Related Work,[0],[0]
"Memory Networks (Weston et al., 2015; Sukhbaatar et al., 2015) attend and reason document representation in a multihop fashion, enriching interactions between documents and questions.",4 Related Work,[0],[0]
"Dynamic Memory Network (Kumar et al., 2016) updates memories of documents by re-running GRU models based on derived attention weights.",4 Related Work,[0],[0]
"Meanwhile, the query representation is refined by another GRU model.",4 Related Work,[0],[0]
"Gated-Attention Reader (Dhingra et al., 2016) proposes a novel attention mechanism, which is based on multiplicative interactions between the query embeddings and the intermediate states of a recurrent neural network document reader.",4 Related Work,[0],[0]
"BiDirectional Attention Model (Xiong et al., 2017; Seo et al., 2017) fuses co-dependent representations of queries and documents in order to focus on relevant parts of both.",4 Related Work,[0],[0]
"Iterative Attention model (Sordoni et al., 2016) attends question and document sequentially, which is related to our model.",4 Related Work,[0],[0]
"Different from Iterative Attention model, our model focuses on the document-level multiaspect sentiment classification, which is proposed
in a hierarchical architecture and has different procedures in the iterative attention module.",4 Related Work,[0],[0]
Another related research problem is visual question answering which uses an image as question context rather than a set of keywords as question.,4 Related Work,[0],[0]
"Neural network based visual question answering (Lu et al., 2016; Xiong et al., 2016) is similar as the proposed models in text comprehension.",4 Related Work,[0],[0]
"In this paper, we model the document-level multiaspect sentiment classification as a text comprehension problem and propose a novel hierarchical iterative attention model in which documents and pseudo aspect-questions are interleaved at both word and sentence-level to learn aspect-aware document representation in a unified model.",5 Conclusion,[0],[0]
Extensive experiments show that our model outperforms the other neural models with multi-task framework and hierarchical architecture.,5 Conclusion,[0],[0]
This paper is partially supported by the National Natural Science Foundation of China (NSFC Grant Nos. 61472006 and 91646202) as well as the National Basic Research Program (973 Program No. 2014CB340405).,6 Acknowledgments,[0],[0]
"This work was also supported by NVIDIA Corporation with the donation of the Titan X GPU, Hong Kong CERG Project 26206717, China 973 Fundamental R&D Program (No.2014CB340304), and the LORELEI Contract HR0011-15-2-0025 with DARPA.",6 Acknowledgments,[0],[0]
The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government.,6 Acknowledgments,[0],[0]
We also thank the anonymous reviewers for their valuable comments and suggestions that help improve the quality of this manuscript.,6 Acknowledgments,[0],[0]
Document-level multi-aspect sentiment classification is an important task for customer relation management.,abstractText,[0],[0]
"In this paper, we model the task as a machine comprehension problem where pseudo questionanswer pairs are constructed by a small number of aspect-related keywords and aspect ratings.",abstractText,[0],[0]
A hierarchical iterative attention model is introduced to build aspectspecific representations by frequent and repeated interactions between documents and aspect questions.,abstractText,[0],[0]
"We adopt a hierarchical architecture to represent both word level and sentence level information, and use the attention operations for aspect questions and documents alternatively with the multiple hop mechanism.",abstractText,[0],[0]
Experimental results on the TripAdvisor and BeerAdvocate datasets show that our model outperforms classical baselines.,abstractText,[0],[0]
Document-Level Multi-Aspect Sentiment Classification as Machine Comprehension,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2911–2916 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"A key assumption made by classic supervised text classification (or learning) is that classes appeared in the test data must have appeared in training, called the closed-world assumption (Fei and Liu, 2016; Chen and Liu, 2016).",1 Introduction,[0],[0]
"Although this assumption holds in many applications, it is violated in many others, especially in dynamic or open environments.",1 Introduction,[0],[0]
"For example, in social media, a classifier built with past topics or classes may not be effective in classifying future data because new topics appear constantly in social media (Fei et al., 2016).",1 Introduction,[0],[0]
"This is clearly true in other domains too, e.g., self-driving cars, where new objects may appear in the scene all the time.
",1 Introduction,[0],[0]
"Ideally, in the text domain, the classifier should classify incoming documents to the right existing classes used in training and also detect those documents that don’t belong to any of the existing classes.",1 Introduction,[0],[0]
"This problem is called open world classification or open classification (Fei and Liu, 2016).",1 Introduction,[0],[0]
"Such a classifier is aware what it does and does
not know.",1 Introduction,[0],[0]
"This paper proposes a novel technique to solve this problem.
",1 Introduction,[0],[0]
Problem Definition:,1 Introduction,[0],[0]
"Given the training data D = {(x1, y1), (x2, y2), . . .",1 Introduction,[0],[0]
", (xn, yn)}, where xi is the i-th document, and yi ∈ {l1, l2, . . .",1 Introduction,[0],[0]
", lm} = Y is xi’s class label, we want to build a model f(x) that can classify each test instance x to one of them training or seen classes inY or reject it to indicate that it does not belong to any of them training or seen classes, i.e., unseen.",1 Introduction,[0],[0]
"In other words, we want to build a (m + 1)-class classifier f(x) with the classes C = {l1, l2, . . .",1 Introduction,[0],[0]
", lm, rejection}.
",1 Introduction,[0],[0]
There are some prior approaches for open classification.,1 Introduction,[0],[0]
"One-class SVM (Schölkopf et al., 2001; Tax and Duin, 2004) is the earliest approach.",1 Introduction,[0],[0]
"However, as no negative training data is used, oneclass classifiers work poorly.",1 Introduction,[0],[0]
"Fei and Liu (2016) proposed a Center-Based Similarity (CBS) space learning method (Fei and Liu, 2015).",1 Introduction,[0],[0]
This method first computes a center for each class and transforms each document to a vector of similarities to the center.,1 Introduction,[0],[0]
A binary classifier is then built using the transformed data for each class.,1 Introduction,[0],[0]
The decision surface is like a “ball” encircling each class.,1 Introduction,[0],[0]
Everything outside the ball is considered not belonging to the class.,1 Introduction,[0],[0]
Our proposed method outperforms this method greatly.,1 Introduction,[0],[0]
"Fei et al. (2016) further added the capability of incrementally or cumulatively learning new classes, which connects this work to lifelong learning (Chen and Liu, 2016) because without the ability to identify novel or new things and learn them, a system will never be able to learn by itself continually.
",1 Introduction,[0],[0]
"In computer vision, Scheirer et al. (2013) studied the problem of recognizing unseen images that the system was not trained for by reducing open space risk.",1 Introduction,[0],[0]
The basic idea is that a classifier should not cover too much open space where there are few or no training data.,1 Introduction,[0],[0]
"They proposed to reduce the half-space of a binary SVM classifier
2911
with a positive region bounded by two parallel hyperplanes.",1 Introduction,[0],[0]
Similar works were also done in a probability setting by Scheirer et al. (2014) and Jain et al. (2014).,1 Introduction,[0],[0]
"Both approaches use probability threshold, but choosing thresholds need prior knowledge, which is a weakness of the methods.",1 Introduction,[0],[0]
Dalvi et al. (2013) proposed a multi-class semisupervised method based on the EM algorithm.,1 Introduction,[0],[0]
"It has been shown that these methods are poorer than the method in (Fei and Liu, 2016).
",1 Introduction,[0],[0]
"The work closest to ours is that in (Bendale and Boult, 2016), which leverages an algorithm called OpenMax to add the rejection capability by utilizing the logits that are trained via closed-world softmax function.",1 Introduction,[0],[0]
"One weak assumption of OpenMax is that examples with equally likely logits are more likely from the unseen or rejection class, which can be examples that are hard to classify.",1 Introduction,[0],[0]
Another weakness is that it requires validation examples from the unseen/rejection class to tune the hyperparameters.,1 Introduction,[0],[0]
"Our method doesn’t make these weak assumptions and performs markedly better.
",1 Introduction,[0],[0]
"Our proposed method, called DOC (Deep Open Classification), uses deep learning (Goodfellow et al., 2016; Kim, 2014).",1 Introduction,[0],[0]
"Unlike traditional classifiers, DOC builds a multi-class classifier with a 1-vs-rest final layer of sigmoids rather than softmax to reduce the open space risk.",1 Introduction,[0],[0]
It reduces the open space risk further for rejection by tightening the decision boundaries of sigmoid functions with Gaussian fitting.,1 Introduction,[0],[0]
Experimental results show that DOC dramatically outperforms state-of-the-art existing approaches from both text classification and image classification domains.,1 Introduction,[0],[0]
"DOC uses CNN (Collobert et al., 2011; Kim, 2014) as its base and augments it with a 1-vsrest final sigmoid layer and Gaussian fitting for
classification.",2 The Proposed DOC Architecture,[0],[0]
"Note: other existing deep models like RNN (Williams and Zipser, 1989; Schuster and Paliwal, 1997) and LSTM (Hochreiter and Schmidhuber, 1997; Gers et al., 2002) can also be adopted as the base.",2 The Proposed DOC Architecture,[0],[0]
"Similar to RNN, CNN also works on embedded sequential data (using 1D convolution on text instead of 2D convolution on images).",2 The Proposed DOC Architecture,[0],[0]
"We choose CNN because OpenMax uses CNN and CNN performs well on text (Kim, 2014), which enables a fairer comparison with OpenMax.",2 The Proposed DOC Architecture,[0],[0]
"The proposed DOC system (given in Fig. 1) is a variant of the CNN architecture (Collobert et al., 2011) for text classification (Kim, 2014)1.",2.1 CNN and Feed Forward Layers of DOC,[0],[0]
The first layer embeds words in document x into dense vectors.,2.1 CNN and Feed Forward Layers of DOC,[0],[0]
The second layer performs convolution over dense vectors using different filters of varied sizes (see Sec. 3.4).,2.1 CNN and Feed Forward Layers of DOC,[0],[0]
"Next, the max-over-time pooling layer selects the maximum values from the results of the convolution layer to form a kdimension feature vector h.",2.1 CNN and Feed Forward Layers of DOC,[0],[0]
"Then we reduce h to a m-dimension vector d = d1:m (m is the number of training/seen classes) via 2 fully connected layers and one intermediate ReLU activation layer:
d = W ′(ReLU(Wh + b))",2.1 CNN and Feed Forward Layers of DOC,[0],[0]
"+ b′, (1)
where W ∈",2.1 CNN and Feed Forward Layers of DOC,[0],[0]
"Rr×k, b ∈",2.1 CNN and Feed Forward Layers of DOC,[0],[0]
"Rr, W ′ ∈ Rm×r, and b′ ∈",2.1 CNN and Feed Forward Layers of DOC,[0],[0]
Rm are trainable weights; r is the output dimension of the first fully connected layer.,2.1 CNN and Feed Forward Layers of DOC,[0],[0]
"The output layer of DOC is a 1-vs-rest layer applied to d1:m, which allows rejection.",2.1 CNN and Feed Forward Layers of DOC,[0],[0]
We describe it next.,2.1 CNN and Feed Forward Layers of DOC,[0],[0]
"Traditional multi-class classifiers (Goodfellow et al., 2016; Bendale and Boult, 2016) typically use softmax as the final output layer, which does not have the rejection capability since the probability of prediction for each class is normalized across all training/seen classes.",2.2 1-vs-Rest Layer of DOC,[0],[0]
"Instead, we build a 1-vs-rest layer containing m sigmoid functions for m seen classes.",2.2 1-vs-Rest Layer of DOC,[0],[0]
"For the i-th sigmoid function corresponding to class li, DOC takes all examples with y = li as positive examples and all the rest examples y 6= li as negative examples.
",2.2 1-vs-Rest Layer of DOC,[0],[0]
"The model is trained with the objective of summation of all log loss of the m sigmoid functions
1https://github.com/alexander-rakhlin/ CNN-for-Sentence-Classification-in-Keras
on the training data D.
Loss = m∑
i=1",2.2 1-vs-Rest Layer of DOC,[0],[0]
"n∑ j=1 −I(yj = li) log p(yj = li)
",2.2 1-vs-Rest Layer of DOC,[0],[0]
"−I(yj 6= li) log(1− p(yj = li)), (2)
where I is the indicator function and p(yj = li) = Sigmoid(dj,i) is the probability output from ith sigmoid function on the jth document’s ithdimension of d.
During testing, we reinterpret the prediction of m sigmoid functions to allow rejection, as shown in Eq. 3.",2.2 1-vs-Rest Layer of DOC,[0],[0]
"For the i-th sigmoid function, we check if the predicted probability Sigmoid(di) is less than a threshold ti belonging to class li.",2.2 1-vs-Rest Layer of DOC,[0],[0]
"If all predicted probabilities are less than their corresponding thresholds for an example, the example is rejected; otherwise, its predicted class is the one with the highest probability.",2.2 1-vs-Rest Layer of DOC,[0],[0]
"Formally, we have
ŷ = {
reject, if Sigmoid(di) <",2.2 1-vs-Rest Layer of DOC,[0],[0]
"ti,∀li ∈ Y; arg maxli∈Y Sigmoid(di), otherwise.
",2.2 1-vs-Rest Layer of DOC,[0],[0]
"(3)
Note that although multi-label classification (Huang et al., 2013; Zhang and Zhou, 2006; Tsoumakas and Katakis, 2006) may also leverage multiple sigmoid functions, Eq. 3 forbids multiple predicted labels for the same example, which is allowed in multi-label classification.",2.2 1-vs-Rest Layer of DOC,[0],[0]
"DOC is also related to multi-task learning (Huang et al., 2013; Caruana, 1998), where each label li is related to a 1-vs-rest binary classification task with shared representations from CNN and fully connected layers.",2.2 1-vs-Rest Layer of DOC,[0],[0]
"However, Eq. 3 performs classification and rejection based on the outputs of these binary classification tasks.
",2.2 1-vs-Rest Layer of DOC,[0],[0]
Comparison with OpenMax:,2.2 1-vs-Rest Layer of DOC,[0],[0]
OpenMax builds on the traditional closed-world multi-class classifier (softmax layer).,2.2 1-vs-Rest Layer of DOC,[0],[0]
"It reduces the open space for each seen class, which is weak for rejecting unseen classes.",2.2 1-vs-Rest Layer of DOC,[0],[0]
"DOC’s 1-vs-rest sigmoid layer provides a reasonable representation of all other classes (the rest of seen classes and unseen classes), and enables the 1 class forms a good boundary.",2.2 1-vs-Rest Layer of DOC,[0],[0]
Sec. 3.5 shows that this basic DOC is already much better than OpenMax.,2.2 1-vs-Rest Layer of DOC,[0],[0]
"Below, we improve DOC further by tightening the decision boundaries more.",2.2 1-vs-Rest Layer of DOC,[0],[0]
"Sigmoid function usually uses the default probability threshold of ti = 0.5 for classification of
each class i.",2.3 Reducing Open Space Risk Further,[0],[0]
But this threshold does not consider potential open space risks from unseen (rejection) class data.,2.3 Reducing Open Space Risk Further,[0],[0]
We can improve the boundary by increasing ti.,2.3 Reducing Open Space Risk Further,[0],[0]
We use Fig. 2 to illustrate.,2.3 Reducing Open Space Risk Further,[0],[0]
The x-axis represents di and y-axis is the predicted probability p(y = li|di).,2.3 Reducing Open Space Risk Further,[0],[0]
"The sigmoid function tries to push positive examples (belonging to the i-th class) and negative examples (belonging to the other seen classes) away from the y-axis via a high gain around di = 0, which serves as the default decision boundary for di with ti = 0.5.",2.3 Reducing Open Space Risk Further,[0],[0]
"As demonstrated by those 3 circles on the right-hand side of the y-axis, during testing, unseen class examples (circles) can easily fill in the gap between the y-axis and those dense positive (+) examples, which may reduce the recall of rejection and the precision of the i-th seen class prediction.",2.3 Reducing Open Space Risk Further,[0],[0]
"Obviously, a better decision boundary is at di = T , where the decision boundary more closely “wrap” those dense positive examples with the probability threshold ti 0.5 .
",2.3 Reducing Open Space Risk Further,[0],[0]
"To obtain a better ti for each seen class i-th, we use the idea of outlier detection in statistics:
1.",2.3 Reducing Open Space Risk Further,[0],[0]
"Assume the predicted probabilities p(y = li|xj , yj = li) of all training data of each class i follow one half of the Gaussian distribution (with mean µi = 1), e.g., the three positive points in Fig. 2 projected to the y-axis (we don’t need di).",2.3 Reducing Open Space Risk Further,[0],[0]
"We then artificially create the other half of the Gaussian distributed points (≥ 1): for each existing point p(y = li|xj , yj = li), we create a mirror point 1 + (1 − p(y = li|xj , yj = li) (not a probability) mirrored on the mean of 1.
2.",2.3 Reducing Open Space Risk Further,[0],[0]
"Estimate the standard deviation σi using both the existing points and the created points.
3.",2.3 Reducing Open Space Risk Further,[0],[0]
"In statistics, if a value/point is a certain number (α) of standard deviations away from the mean, it is considered an outlier.",2.3 Reducing Open Space Risk Further,[0],[0]
"We thus set the probability threshold ti = max(0.5, 1 − ασi).",2.3 Reducing Open Space Risk Further,[0],[0]
"The commonly used number for α is 3, which also works well in our experiments.
",2.3 Reducing Open Space Risk Further,[0],[0]
"Note that due to Gaussian fitting, different class li can have a different classification threshold ti.",2.3 Reducing Open Space Risk Further,[0],[0]
"We perform evaluation using two publicly available datasets, which are exactly the same datasets used in (Fei and Liu, 2016).
",3.1 Datasets,[0],[0]
"(1) 20 Newsgroups2 (Rennie, 2008): The 20 newsgroups data set contains 20 non-overlapping classes.",3.1 Datasets,[0],[0]
"Each class has about 1000 documents.
",3.1 Datasets,[0],[0]
"(2) 50-class reviews (Chen and Liu, 2014):",3.1 Datasets,[0],[0]
The dataset has Amazon reviews of 50 classes of products.,3.1 Datasets,[0],[0]
Each class has 1000 reviews.,3.1 Datasets,[0],[0]
"Although product reviews are used, we do not do sentiment classification.",3.1 Datasets,[0],[0]
We still perform topic-based classification.,3.1 Datasets,[0],[0]
"That is, given a review, the system decides what class of product the review is about.
",3.1 Datasets,[0],[0]
"For every dataset, we keep a 20000 frequent word vocabulary.",3.1 Datasets,[0],[0]
Each document is fixed to 2000- word length (cutting or padding when necessary).,3.1 Datasets,[0],[0]
"For a fair comparison, we use exactly the same settings as in (Fei and Liu, 2016).",3.2 Test Settings and Evaluation Metrics,[0],[0]
"For each class in each dataset, we randomly sampled 60% of documents for training, 10% for validation and 30% for testing.",3.2 Test Settings and Evaluation Metrics,[0],[0]
"Fei and Liu (2016) did not use a validation set, but the test data is the same 30%.",3.2 Test Settings and Evaluation Metrics,[0],[0]
We use the validation set to avoid overfitting.,3.2 Test Settings and Evaluation Metrics,[0],[0]
"For openworld evaluation, we hold out some classes (as unseen) in training and mix them back during testing.",3.2 Test Settings and Evaluation Metrics,[0],[0]
"We vary the number of training classes and use 25%, 50%, 75%, or 100% classes for training and all classes for testing.",3.2 Test Settings and Evaluation Metrics,[0],[0]
Here using 100% classes for training is the same as the traditional closedworld classification.,3.2 Test Settings and Evaluation Metrics,[0],[0]
"Taking 20 newsgroups as an example, for 25% classes, we use 5 classes (we randomly choose 5 classes from 20 classes for 10 times and average the results, as in (Fei and Liu, 2016)) for training and all 20 classes for testing (15 classes are unseen in training).",3.2 Test Settings and Evaluation Metrics,[0],[0]
"We use macro F1-score over 5 + 1 classes (1 for rejection) for
2http://qwone.com/˜jason/20Newsgroups/
evaluation.",3.2 Test Settings and Evaluation Metrics,[0],[0]
Please note that examples from unseen classes are dropped in the validation set.,3.2 Test Settings and Evaluation Metrics,[0],[0]
"We compare DOC with two state-of-the-art methods published in 2016 and one DOC variant.
cbsSVM:",3.3 Baselines,[0],[0]
"This is the latest method published in NLP (Fei and Liu, 2016).",3.3 Baselines,[0],[0]
It uses SVM to build 1-vs-rest CBS classifiers for multiclass text classification with rejection option.,3.3 Baselines,[0],[0]
"The results of this system are taken from (Fei and Liu, 2016).
",3.3 Baselines,[0],[0]
OpenMax:,3.3 Baselines,[0],[0]
"This is the latest method from computer vision (Bendale and Boult, 2016).",3.3 Baselines,[0],[0]
"Since it is a CNN-based method for image classification, we adapt it for text classification by using CNN with a softmax output layer, and adopt the OpenMax layer3 for open text classification.",3.3 Baselines,[0],[0]
"When all classes are seen (100%), the result from softmax is reported since OpenMax layer always performs rejection.",3.3 Baselines,[0],[0]
"We use default hyperparameter values of OpenMax (Weibull tail size is set to 20).
DOC(t = 0.5):",3.3 Baselines,[0],[0]
This is the basic DOC (t = 0.5).,3.3 Baselines,[0],[0]
"Gaussian fitting isn’t used to choose each ti.
",3.3 Baselines,[0],[0]
"Note that (Fei and Liu, 2016) compared with several other baselines.",3.3 Baselines,[0],[0]
We don’t compare with them as it was shown that cbsSVM was superior.,3.3 Baselines,[0],[0]
We use word vectors pre-trained from Google News4 (3 million words and 300 dimensions).,3.4 Hyperparameter Setting,[0],[0]
"For the CNN layers, 3 filter sizes are used [3, 4, 5].",3.4 Hyperparameter Setting,[0],[0]
"For each filter size, 150 filters are applied.",3.4 Hyperparameter Setting,[0],[0]
"The dimension r of the first fully connected layer is 250.
",3.4 Hyperparameter Setting,[0],[0]
"3https://github.com/abhijitbendale/ OSDN
4https://code.google.com/archive/p/ word2vec/",3.4 Hyperparameter Setting,[0],[0]
"The results of 20 newsgroups and 50-class reviews are given in Tables 1 and 2, respectively.",3.5 Result Analysis,[0],[0]
"From the tables, we can make the following observations:
1.",3.5 Result Analysis,[0],[0]
"DOC is markedly better than OpenMax and cbsSVM in macro-F1 scores for both datasets in the 25%, 50%, and 75% settings.",3.5 Result Analysis,[0],[0]
"For the 25% and 50% settings (most test examples are from unseen classes), DOC is dramatically better.",3.5 Result Analysis,[0],[0]
"Even for 100% of traditional closed-world classification, it is consistently better too.",3.5 Result Analysis,[0],[0]
"DOC(t = 0.5) is better too.
",3.5 Result Analysis,[0],[0]
2.,3.5 Result Analysis,[0],[0]
"For the 25% and 50% settings, DOC is also markedly better than DOC(t = 0.5), which shows that Gaussian fitting finds a better probability threshold than t = 0.5 when many unseen classes are present.",3.5 Result Analysis,[0],[0]
"In the 75% setting (most test examples are from seen classes), DOC(t = 0.5) is slightly better for 20 newsgroups but worse for 50-class reviews.",3.5 Result Analysis,[0],[0]
"DOC sacrifices some recall of seen class examples for better precision, while t = 0.5 sacrifices the precision of seen classes for better recall.",3.5 Result Analysis,[0],[0]
DOC(t = 0.5) is also worse than cbsSVM for 25% setting for 50-class reviews.,3.5 Result Analysis,[0],[0]
"It is thus not as robust as DOC.
3.",3.5 Result Analysis,[0],[0]
"For the 25% and 50% settings, cbsSVM is also markedly better than OpenMax.",3.5 Result Analysis,[0],[0]
"This paper proposed a novel deep learning based method, called DOC, for open text classification.",4 Conclusion,[0],[0]
"Using the same text datasets and experiment settings, we showed that DOC performs dramatically better than the state-of-the-art methods from both the text and image classification domains.",4 Conclusion,[0],[0]
"We also believe that DOC is applicable to images.
",4 Conclusion,[0],[0]
"In our future work, we plan to improve the cumulative or incremental learning method in (Fei et al., 2016) to learn new classes without training on all past and new classes of data from scratch.",4 Conclusion,[0],[0]
"This will enable the system to learn by self to achieve continual or lifelong learning (Chen and Liu, 2016).",4 Conclusion,[0],[0]
"We also plan to improve model performance during testing (Shu et al., 2017).",4 Conclusion,[0],[0]
This work was supported in part by grants from National Science Foundation (NSF) under grant no.,Acknowledgments,[0],[0]
IIS-1407927 and IIS-1650900.,Acknowledgments,[0],[0]
Traditional supervised learning makes the closed-world assumption that the classes appeared in the test data must have appeared in training.,abstractText,[0],[0]
This also applies to text learning or text classification.,abstractText,[0],[0]
"As learning is used increasingly in dynamic open environments where some new/test documents may not belong to any of the training classes, identifying these novel documents during classification presents an important problem.",abstractText,[0],[0]
This problem is called openworld classification or open classification.,abstractText,[0],[0]
This paper proposes a novel deep learning based approach.,abstractText,[0],[0]
It outperforms existing state-of-the-art techniques dramatically.,abstractText,[0],[0]
DOC: Deep Open Classification of Text Documents,title,[0],[0]
Supervised learning has been successful in many application fields.,1. Introduction,[0],[0]
"The vast majority of supervised learning research falls into the Empirical Risk Minimization (ERM) framework (Vapnik, 1998) that assumes a test distribution to be the same as a training distribution.",1. Introduction,[0],[0]
"However, such an assumption can be easily contradicted in real-world applications due to sample selection bias or non-stationarity of the environment (Quionero-Candela et al., 2009).",1. Introduction,[0],[0]
"Once the distribution shift occurs, the performance of the traditional machine learning techniques can be significantly degraded.",1. Introduction,[0],[0]
"This makes the traditional techniques unreliable for practitioners to use in the real world.
",1. Introduction,[0],[0]
"1University of Tokyo, Japan 2RIKEN, Tokyo, Japan.",1. Introduction,[0],[0]
Correspondence to:,1. Introduction,[0],[0]
Weihua Hu,1. Introduction,[0],[0]
<,1. Introduction,[0],[0]
"weihua916@gmail.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
Distributionally Robust Supervised Learning (DRSL) is a promising paradigm to tackle this problem by obtaining prediction functions explicitly robust to distribution shift.,1. Introduction,[0],[0]
"More specifically, DRSL considers a minimax game between a learner and an adversary: the adversary first shifts the test distribution from the training distribution within a pre-specified uncertainty set so as to maximize the expected loss on the test distribution.",1. Introduction,[0],[0]
"The learner then minimizes the adversarial expected loss.
",1. Introduction,[0],[0]
"DRSL with f -divergences (Bagnell, 2005; Ben-Tal et al., 2013; Duchi et al., 2016; Namkoong & Duchi, 2016; 2017) is particularly well-studied and lets the uncertainty set for test distributions be an f -divergence ball from a training distribution (see Section 2 for the detail).",1. Introduction,[0],[0]
This DRSL has been mainly studied under the assumption that the same continuous loss is used for training and testing.,1. Introduction,[0],[0]
"This is not the case in the classification scenario, in which we care about the 0-1 loss (i.e., the mis-classification rate) at test time, while at training time, we use a surrogate loss for optimization tractability.
",1. Introduction,[0],[0]
"In this paper, we revisit DRSL with f -divergences, providing novel insight for the classification scenario.",1. Introduction,[0],[0]
"In particular, we prove rather surprising results (Theorems 1–3), showing that when the DRSL is applied to classification, the obtained classifier ends up being optimal for the training distribution.",1. Introduction,[0],[0]
This is too pessimistic for DRSL given that DRSL is explicitly formulated for a distribution shift scenario and is naturally expected to give a classifier different from the one that exactly fits the given training distribution.,1. Introduction,[0],[0]
"Such pessimism comes from two sources: the particular losses used in classification and the over-flexibility of the uncertainty set used by DRSL with f -divergences.
",1. Introduction,[0],[0]
"Motivated by our analysis, we propose simple DRSL that overcomes the pessimism of the previous DRSL by incorporating structural assumptions on distribution shift (Section 4).",1. Introduction,[0],[0]
We establish convergence properties of our proposed DRSL (Theorem 4) and derive efficient optimization algorithms (Section 5).,1. Introduction,[0],[0]
"Finally, we demonstrate the effectiveness of our DRSL through experiments (Section 6).",1. Introduction,[0],[0]
"All the appendices of this paper are provided in the supplementary material.
",1. Introduction,[0],[0]
"Related work: Besides DRSL with f -divergences, different DRSL considers different classes of uncertainty sets for test distributions.",1. Introduction,[0],[0]
"DRSL by Globerson & Roweis
(2006) considered the uncertainty of features deletion, while DRSL by Liu & Ziebart (2014) considered the uncertainty of unknown properties of the conditional label distribution.",1. Introduction,[0],[0]
"DRSL by Esfahani & Kuhn (2015), Blanchet et al. (2016) and Sinha et al. (2017) lets the uncertainty set of test distributions be a Wasserstein ball from the training distribution.",1. Introduction,[0],[0]
"DRSL with the Wasserstein distance can make classifiers robust to adversarial examples (Sinha et al., 2017), while DRSL with f -divergences can make classifiers robust against adversarial reweighting of data points as shown in Section 2.",1. Introduction,[0],[0]
"Recently, in the context of fair machine learning, Hashimoto et al. (2018) applied DRSL with f - divergences in an attempt to achieve fairness without demographic information.",1. Introduction,[0],[0]
"In this section, we first review the ordinary ERM framework.",2. Review of ERM and DRSL,[0],[0]
"Then, we explain a general formulation of DRSL and review DRSL with f -divergences.
",2. Review of ERM and DRSL,[0],[0]
"Suppose training samples, {(x1, y1), . . .",2. Review of ERM and DRSL,[0],[0]
", (xN , yN )} ≡ D, are drawn i.i.d.",2. Review of ERM and DRSL,[0],[0]
"from an unknown training distribution over X × Y with density p(x, y), where X ⊂ Rd and Y is an output domain.",2. Review of ERM and DRSL,[0],[0]
"Let gθ be a prediction function with parameter θ, mapping x ∈ X into a real scaler or vector, and let ℓ(ŷ, y) be a loss between y and real-valued prediction ŷ.
ERM: The objective of the risk minimization (RM) is
min θ Ep(x,y)[ℓ(gθ(x), y)]︸ ︷︷ ︸ ≡ R(θ) , (1)
where R(θ) is called the risk.",2. Review of ERM and DRSL,[0],[0]
"In ERM, we approximate the expectation in Eq.",2. Review of ERM and DRSL,[0],[0]
"(1) by training data D:
min θ
1
N
N∑
i=1
ℓ(gθ(xi), yi)
︸ ︷︷ ︸ ≡ R̂(θ)
, (2)
where R̂(θ) is called the empirical risk.",2. Review of ERM and DRSL,[0],[0]
"To prevent overfitting, we can add regularization term Ω(θ) to Eq.",2. Review of ERM and DRSL,[0],[0]
"(2) and minimize R̂(θ)+λΩ(θ), where λ ≥ 0 is a trade-off hyperparameter.
",2. Review of ERM and DRSL,[0],[0]
"General formulation of DRSL: ERM implicitly assumes the test distribution to be the same as the training distribution, which does not hold in most real-world applications.",2. Review of ERM and DRSL,[0],[0]
"DRSL is explicitly formulated for a distribution shift scenario, where test density q(x, y) is different from training density p(x, y).",2. Review of ERM and DRSL,[0],[0]
Let Qp be an uncertainty set for test distributions.,2. Review of ERM and DRSL,[0],[0]
"In DRSL, the learning objective is
min θ sup",2. Review of ERM and DRSL,[0],[0]
"q∈Qp Eq(x,y)[ℓ(gθ(x), y)].",2. Review of ERM and DRSL,[0],[0]
"(3)
We see that Eq. (3) minimizes the risk w.r.t.",2. Review of ERM and DRSL,[0],[0]
"the worst-case test distribution within the uncertainty set Qp.
DRSL with f -divergences: Let q ≪ p denote that q is absolutely continuous w.r.t.",2. Review of ERM and DRSL,[0],[0]
"p, i.e., p(x, y) = 0 implies q(x, y) = 0.",2. Review of ERM and DRSL,[0],[0]
"Bagnell (2005) and Ben-Tal et al. (2013) considered the particular uncertainty set
Qp = {q ≪ p | Df [q∥p] ≤ δ}, (4)
where Df [·∥·] is an f -divergence defined as Df [q∥p] ≡",2. Review of ERM and DRSL,[0],[0]
"Ep [f (q/p)], and f(·) is convex with f(1) = 0.",2. Review of ERM and DRSL,[0],[0]
"The f - divergence (Ciszar, 1967) measures a discrepancy between probability distributions.",2. Review of ERM and DRSL,[0],[0]
"When f(x) = x log x, we have the well-known Kullback-Leibler divergence as an instance of it.",2. Review of ERM and DRSL,[0],[0]
Hyper-parameter δ > 0,2. Review of ERM and DRSL,[0],[0]
in Eq.,2. Review of ERM and DRSL,[0],[0]
(4) controls the degree of the distribution shift.,2. Review of ERM and DRSL,[0],[0]
"Define r(x, y) ≡ q(x, y)/p(x, y).",2. Review of ERM and DRSL,[0],[0]
"Through some calculations, the objective of DRSL with f - divergences can be rewritten as
min θ sup r∈Uf Ep(x,y)[r(x, y)ℓ(gθ(x), y)] ︸ ︷︷ ︸
≡ Radv(θ)
, (5)
",2. Review of ERM and DRSL,[0],[0]
"Uf ≡ {r(x, y) |",2. Review of ERM and DRSL,[0],[0]
"Ep(x,y)",2. Review of ERM and DRSL,[0],[0]
"[f (r(x, y))]",2. Review of ERM and DRSL,[0],[0]
"≤ δ,
Ep(x,y)[r(x, y)]",2. Review of ERM and DRSL,[0],[0]
"= 1,
r(x, y) ≥ 0, ∀(x, y) ∈ X × Y}.",2. Review of ERM and DRSL,[0],[0]
"(6)
We call Radv(θ) the adversarial risk and call the minimization problem of Eq.",2. Review of ERM and DRSL,[0],[0]
(5) the adversarial risk minimization (ARM).,2. Review of ERM and DRSL,[0],[0]
"In ARM, the density ratio, r(x, y), can be considered as the weight put by the adversary on the loss of data (x, y).",2. Review of ERM and DRSL,[0],[0]
"Then, Eq. (5) can be regarded as a minimax game between the learner (corresponding to minθ) and the adversary (corresponding to supr∈Uf ): the adversary first reweights the losses using r(·, ·) so as to maximize the expected loss; the learner then minimizes the reweighted expected loss, i.e., adversarial risk Radv(θ).",2. Review of ERM and DRSL,[0],[0]
"For notational convenience, we denote ℓ(gθ(xi), yi) by ℓi(θ).",2. Review of ERM and DRSL,[0],[0]
"Also, let r ≡ (r1, . . .",2. Review of ERM and DRSL,[0],[0]
", rN ) be a vector of density ratios evaluated at training data points, i.e., ri ≡ r(xi, yi) for 1 ≤ i ≤",2. Review of ERM and DRSL,[0],[0]
N .,2. Review of ERM and DRSL,[0],[0]
"Equations (5) and (6) can be empirically approximated as1
min θ sup r∈Ûf
1 N
N∑
i=1
riℓi(θ)
",2. Review of ERM and DRSL,[0],[0]
"︸ ︷︷ ︸ ≡ R̂adv(θ)
, (7)
Ûf = { r ∣∣∣∣∣",2. Review of ERM and DRSL,[0],[0]
"1 N N∑
i=1
f (ri) ≤ δ, 1 N
N∑
i=1
ri = 1, r ≥ 0 } , (8)
1The formulation in Eqs.",2. Review of ERM and DRSL,[0],[0]
"(7) and (8) is similar to Duchi et al. (2016), Namkoong & Duchi (2016) and Namkoong & Duchi (2017) except that they decay δ",2. Review of ERM and DRSL,[0],[0]
linearly w.r.t.,2. Review of ERM and DRSL,[0],[0]
the number of training data N .,2. Review of ERM and DRSL,[0],[0]
"Different from us, they assume δ = 0",2. Review of ERM and DRSL,[0],[0]
in Eq.,2. Review of ERM and DRSL,[0],[0]
"(4) (thus, their objective is the ordinary risk) and try to be robust to apparent distribution fluctuations due to the finiteness of training samples.",2. Review of ERM and DRSL,[0],[0]
"On the other hand, we consider using the same δ > 0",2. Review of ERM and DRSL,[0],[0]
for both Eqs.,2. Review of ERM and DRSL,[0],[0]
"(4) and (8) and try to be robust to the actual distribution change between training and test stages.
where the inequality constraint for a vector is applied in an element-wise fashion.",2. Review of ERM and DRSL,[0],[0]
We call R̂adv(θ) the adversarial empirical risk and call the minimization problem of Eq.,2. Review of ERM and DRSL,[0],[0]
(7) the adversarial empirical risk minimization (AERM).,2. Review of ERM and DRSL,[0],[0]
"In AERM, the adversary (corresponding to supr∈Ûf ) reweights data losses through r to maximize the empirical loss in Eq.",2. Review of ERM and DRSL,[0],[0]
(7).,2. Review of ERM and DRSL,[0],[0]
"To prevent overfitting, we can add regularization term Ω(θ) to Eq.",2. Review of ERM and DRSL,[0],[0]
(7).,2. Review of ERM and DRSL,[0],[0]
"At first glance, DRSL with f -divergences (which we call ARM and AERM in this paper) is reasonable to give a distributionally robust classifier in the sense that it explicitly minimizes the loss for the shifted worst-case test distribution.",3. Analysis of DRSL with f -divergences in classification,[0],[0]
"However, we show rather surprising results, suggesting that the DRSL, when applied to classification, still ends up giving a classifier optimal for a training distribution.",3. Analysis of DRSL with f -divergences in classification,[0],[0]
This is too pessimistic for DRSL because it ends up behaving similarly to ordinary ERM-based supervised classification that does not explicitly consider distribution shift.,3. Analysis of DRSL with f -divergences in classification,[0],[0]
"To make a long story short, our results hold because of the particular losses used in classification (especially, the 0-1 loss at test time) and the overly flexible uncertainty sets used by ARM and AERM.",3. Analysis of DRSL with f -divergences in classification,[0],[0]
"We will detail these points after we state our main results.
",3. Analysis of DRSL with f -divergences in classification,[0],[0]
Classification setting: Let us first briefly review classification settings to set up notations.,3. Analysis of DRSL with f -divergences in classification,[0],[0]
"In binary classification, we have gθ(·) : x )",3. Analysis of DRSL with f -divergences in classification,[0],[0]
"→ ŷ ∈ R, Y = {+1,−1} and ℓ(·, ·) :",3. Analysis of DRSL with f -divergences in classification,[0],[0]
R × Y → R≥0.,3. Analysis of DRSL with f -divergences in classification,[0],[0]
"In K-class classification for K ≥ 2, we have gθ(·) : x )",3. Analysis of DRSL with f -divergences in classification,[0],[0]
"→ ŷ ∈ RK , Y = {1, 2, . . .",3. Analysis of DRSL with f -divergences in classification,[0],[0]
",K} and ℓ(·, ·) : RK × Y → R≥0.",3. Analysis of DRSL with f -divergences in classification,[0],[0]
The goal of classification is to learn the prediction function that minimizes the mis-classification rate on the test distribution.,3. Analysis of DRSL with f -divergences in classification,[0],[0]
"The misclassification rate corresponds to the use of the 0-1 loss, i.e., ℓ(ŷ, y) ≡ 1{sign(ŷ)",3. Analysis of DRSL with f -divergences in classification,[0],[0]
"̸= y} for binary classification, and ℓ(ŷ, y) ≡",3. Analysis of DRSL with f -divergences in classification,[0],[0]
"1{argmaxkŷk ̸= y} for multi-class classification, where 1{·} is the indicator function and ŷk is the k-th element of ŷ ∈ RK .",3. Analysis of DRSL with f -divergences in classification,[0],[0]
"However, since the 0-1 loss is non-convex and non-continuous, learning with it is difficult in practice.",3. Analysis of DRSL with f -divergences in classification,[0],[0]
"Therefore, at training time, we instead use surrogate losses that are easy to optimize, such as the logistic loss and the cross-entropy loss.
",3. Analysis of DRSL with f -divergences in classification,[0],[0]
"In the following, we state our main results, analyzing ARM and AERM in the classification scenario by considering the use of the 0-1 loss and a surrogate loss.
",3. Analysis of DRSL with f -divergences in classification,[0],[0]
"The 0-1 loss case: Theorem 1 establishes the non-trivial relationship between the adversarial risk and the ordinary risk when the 0-1 loss is used.
",3. Analysis of DRSL with f -divergences in classification,[0],[0]
Theorem 1.,3. Analysis of DRSL with f -divergences in classification,[0],[0]
"Let ℓ(ŷ, y) be the 0-1 loss.",3. Analysis of DRSL with f -divergences in classification,[0],[0]
"Then, there is a monotonic relationship between Radv(θ) and R(θ) in the sense that for any pair of parameters θ1 and θ2, the followings hold.",3. Analysis of DRSL with f -divergences in classification,[0],[0]
Radv(θ1),"If Radv(θ1) < 1, then",[0],[0]
< Radv(θ2)⇐⇒ R(θ1),"If Radv(θ1) < 1, then",[0],[0]
< R(θ2).,"If Radv(θ1) < 1, then",[0],[0]
(9),"If Radv(θ1) < 1, then",[0],[0]
R(θ1) ≤,"If Radv(θ1) = 1, then",[0],[0]
R(θ2) =⇒ Radv(θ2),"If Radv(θ1) = 1, then",[0],[0]
"= 1. (10)
The same monotonic relationship also holds between their empirical approximations: R̂adv(θ) and R̂(θ).","If Radv(θ1) = 1, then",[0],[0]
See Appendix A for the proof.,"If Radv(θ1) = 1, then",[0],[0]
"Theorem 1 shows a surprising result that when the 0-1 loss is used, R(θ) and Radv(θ) are essentially equivalent objective functions in the sense that the minimization of one objective function results in the minimization of another objective function.","If Radv(θ1) = 1, then",[0],[0]
This readily implies that R(θ) and Radv(θ) have exactly the same set of global minima in the regime of Radv(θ) < 1.,"If Radv(θ1) = 1, then",[0],[0]
"An immediate practical implication is that if we select hyper-parameters such as λ for regularization according to the adversarial risk with the 0-1 loss, we will end up choosing hyper-parameters that attain the minimum misclassification rate on the training distribution.","If Radv(θ1) = 1, then",[0],[0]
"The surrogate loss case: We now turn our focus on the training stage of classification, where we use a surrogate loss instead of the 0-1 loss.","If Radv(θ1) = 1, then",[0],[0]
"In particular, for binary classification, we consider a class of classification calibrated losses (Bartlett et al., 2006) that are margin-based, i.e., ℓ(ŷ, y) is a function of product yŷ. For multi-class classification, we consider a class of classification calibrated losses (Tewari & Bartlett, 2007) that are invariant to class permutation, i.e., for any class permutation π : Y → Y , ℓ(ŷπ,π(y))","If Radv(θ1) = 1, then",[0],[0]
"= ℓ(ŷ, y) holds, where ŷπk = ŷπ(k) for 1 ≤ k ≤ K.","If Radv(θ1) = 1, then",[0],[0]
"Although we only consider the sub-class of general classification-calibrated losses (Bartlett et al., 2006; Tewari & Bartlett, 2007), we note that ours still includes some of the most widely used losses: the logistic, hinge, and exponential losses for binary classification and the softmax cross entropy loss for multi-class classification.
","If Radv(θ1) = 1, then",[0],[0]
We first review Proposition 1 by Bartlett et al. (2006) and Tewari & Bartlett (2007) that justifies the use of classification-calibrated losses in ERM for classification.,"If Radv(θ1) = 1, then",[0],[0]
We then show a surprising fact in Theorem 2 that the similar property also holds for ARM using the sub-class of classification-calibrated losses.,"If Radv(θ1) = 1, then",[0],[0]
Proposition 1 (Bartlett et al. (2006); Tewari & Bartlett (2007)).,"If Radv(θ1) = 1, then",[0],[0]
"Let ℓ(ŷ, y) be a classification calibrated loss, and assume that the hypothesis class is equal to all measurable functions.","If Radv(θ1) = 1, then",[0],[0]
"Then, the risk minimization (RM) gives the Bayes optimal classifier2.","If Radv(θ1) = 1, then",[0],[0]
Theorem 2.,"If Radv(θ1) = 1, then",[0],[0]
"Let f(·) be differentiable, the hypothesis class be all measurable functions, and ℓ(ŷ, y) be a classificationcalibrated loss that is margin-based or invariant to class
2The classifier that minimizes the mis-classification rate for the training density p(x, y) (the 0-1 loss is considered), i.e., the classifier whose prediction on x is equal to argmaxy∈Y p(y|x).
permutation.","If Radv(θ1) = 1, then",[0],[0]
"Let g(adv) be any solution of ARM3 under the above setting, and define
r∗ ≡ argmax","If Radv(θ1) = 1, then",[0],[0]
"r∈Uf Ep(x,y)[r(x, y)ℓ(g(adv)(x), y)].","If Radv(θ1) = 1, then",[0],[0]
"(11)
Then, the prediction of g(adv) coincides with that of the Bayes optimal classifier almost surely over q∗(x) ≡∑
y∈Y r ∗(x, y)p(x, y).","If Radv(θ1) = 1, then",[0],[0]
"Furthermore, among the solutions of ARM, there exists g(adv) whose prediction coincides with that of the Bayes optimal classifier almost surely over p(x).
","If Radv(θ1) = 1, then",[0],[0]
See Appendix B for the proof.,"If Radv(θ1) = 1, then",[0],[0]
"Theorem 2 indicates that ARM, similarly to RM, ends up giving the optimal decision boundary for the training distribution, if the hypothesis class is all measurable functions and we have access to true density p(x, y).","If Radv(θ1) = 1, then",[0],[0]
"Even though the assumptions made are strong, Theorem 2 together with Proposition 1 highlight the non-trivial fact that when a certain surrogate loss is used, AERM and ERM demonstrate the similar asymptotic behavior in classification.
","If Radv(θ1) = 1, then",[0],[0]
"We proceed to consider a more practical scenario, where we only have a finite amount of training data and the hypothesis class is limited.","If Radv(θ1) = 1, then",[0],[0]
"In the rest of the section, we focus on a differentiable loss and a real-valued scalar output, i.e., ŷ ∈ R, which includes the scenario of binary classification.
","If Radv(θ1) = 1, then",[0],[0]
"We first define the notion of a steeper loss, which will play a central role in our result.
","If Radv(θ1) = 1, then",[0],[0]
Definition 1 (Steeper loss).,"If Radv(θ1) = 1, then",[0],[0]
"Loss function ℓsteep(ŷ, y) is said to be steeper than loss function ℓ(ŷ, y), if there exists a non-constant, non-decreasing and non-negative function h :","If Radv(θ1) = 1, then",[0],[0]
"R≥0 → R≥0 such that
∂ℓsteep(ŷ, y)
∂ŷ = h(ℓ(ŷ, y))
","If Radv(θ1) = 1, then",[0],[0]
"∂ℓ(ŷ, y)
∂ŷ .","If Radv(θ1) = 1, then",[0],[0]
"(12)
For example, following Definition 1, we can show that the exponential loss is steeper than the logistic loss.","If Radv(θ1) = 1, then",[0],[0]
"Intuitively, outlier-sensitive losses are steeper than more outlier-robust losses.","If Radv(θ1) = 1, then",[0],[0]
"Lemma 1 shows an important property of a steeper loss in a classification scenario.
","If Radv(θ1) = 1, then",[0],[0]
Lemma 1.,"If Radv(θ1) = 1, then",[0],[0]
"Let ℓ(ŷ, y) be a margin-based convex classification-calibrated loss.","If Radv(θ1) = 1, then",[0],[0]
"Then, its steeper loss defined in Eq.","If Radv(θ1) = 1, then",[0],[0]
"(12) is also convex classification-calibrated if h(ℓ(0, y))","If Radv(θ1) = 1, then",[0],[0]
"> 0.
See Appendix C for the proof.
","If Radv(θ1) = 1, then",[0],[0]
Now we are ready to state our result in Theorem 3 that considers ŷ ∈ R. Theorem 3 holds for any hypothesis class that is parametrized by θ and sub-differentiable w.r.t.,"If Radv(θ1) = 1, then",[0],[0]
"θ, e.g., linear-in-parameter models and deep neural networks.
","If Radv(θ1) = 1, then",[0],[0]
"3There can be multiple solutions that achieve the same minimum adversarial risk.
","If Radv(θ1) = 1, then",[0],[0]
Theorem 3.,"If Radv(θ1) = 1, then",[0],[0]
Let θ∗ be a stationary point of AERM in Eq.,"If Radv(θ1) = 1, then",[0],[0]
"(7) using ℓ(ŷ, y).","If Radv(θ1) = 1, then",[0],[0]
"Then, there exists a steeper loss function, ℓDRSL(ŷ, y), such that θ∗ is also a stationary point of the following ERM.
min θ
1
N
N∑
i=1
ℓDRSL(gθ(xi), yi).","If Radv(θ1) = 1, then",[0],[0]
"(13)
See Appendix D for the proof.","If Radv(θ1) = 1, then",[0],[0]
Remark 1 (Conditions for convexity).,"If Radv(θ1) = 1, then",[0],[0]
"Let ℓ(ŷ, y) be convex in ŷ, gθ(x) be a linear-in-parameter model.","If Radv(θ1) = 1, then",[0],[0]
"Then, both AERM in Eq.","If Radv(θ1) = 1, then",[0],[0]
(7) and ERM in Eq.,"If Radv(θ1) = 1, then",[0],[0]
(13) become convex in θ.,"If Radv(θ1) = 1, then",[0],[0]
This implies that the stationary point θ∗ in Theorem 3 turns out to be the global optimum for both Eqs.,"If Radv(θ1) = 1, then",[0],[0]
(7) and (13) in this usual setting.,"If Radv(θ1) = 1, then",[0],[0]
"Note that Theorem 3 holds for general real-valued scalar prediction, i.e., ŷ ∈ R; thus, the result holds for ordinary regression (using the same loss for training and testing) as well as for binary classification.","If Radv(θ1) = 1, then",[0],[0]
"However, as we discuss in the following, the implication of Theorem 3 is drastically different for the two scenarios.
","If Radv(θ1) = 1, then",[0],[0]
"Implication for classification: Theorem 3 together with Lemma 1 indicate that under a mild condition,4 AERM using a convex classification-calibrated margin-based loss reduces to Eq.","If Radv(θ1) = 1, then",[0],[0]
"(13), which is ERM using a convex classification-calibrated loss.","If Radv(θ1) = 1, then",[0],[0]
"This implies that AERM, similarly to ordinary ERM using a classification-calibrated loss, will try to give a classifier optimal for the training distribution.
","If Radv(θ1) = 1, then",[0],[0]
Why does the use of the steeper surrogate loss fail to give meaningful robust classifiers?,"If Radv(θ1) = 1, then",[0],[0]
"This is because we are dealing with classification tasks, where we care about the performance in terms of the 0-1 loss at test time.","If Radv(θ1) = 1, then",[0],[0]
"The use of the steeper surrogate loss may make a classifier distributionally robust in terms of the surrogate loss,5 but not necessarily so in terms of the 0-1 loss.","If Radv(θ1) = 1, then",[0],[0]
"Moreover, even if we obtain a classifier that minimizes the adversarial risk in terms of the 0-1 loss, the obtained classifier ends up being optimal for the training distribution (see Theorem 1).","If Radv(θ1) = 1, then",[0],[0]
"In any case, the use of the steeper loss does not in general give classifiers that are robust to change from a training distribution.
","If Radv(θ1) = 1, then",[0],[0]
"In summary, in the classification scenario, the use of the steeper loss does more harm (making a classifier sensitive
4The condition that h(ℓ(0, y))","If Radv(θ1) = 1, then",[0],[0]
> 0,"If Radv(θ1) = 1, then",[0],[0]
in Lemma 1.,"If Radv(θ1) = 1, then",[0],[0]
"Whether the condition holds or not generally depends on the uncertainty set, the model, the loss function, and training data.","If Radv(θ1) = 1, then",[0],[0]
"Nonetheless, the condition is mild in practice; especially, the condition always holds when the Kullback-Leibler divergence is used.","If Radv(θ1) = 1, then",[0],[0]
"See Appendix C for detailed discussion.
","If Radv(θ1) = 1, then",[0],[0]
5For fixed δ (non-decaying w.r.t.,"If Radv(θ1) = 1, then",[0],[0]
"N ), whether AERM is consistent with ARM or not is an open problem.","If Radv(θ1) = 1, then",[0],[0]
"Nevertheless, we empirically confirm in Section 6 that AERM achieves lower adversarial risk than other baselines in terms of the surrogate loss.
to outliers due to the use of the steeper surrogate loss) than good (making a classifier robust to change from a training distribution).
","If Radv(θ1) = 1, then",[0],[0]
"Implication for ordinary regression: For comparison, let us rethink about the classical regression scenario, in which we use the same loss, e.g., the squared loss, during training and testing.","If Radv(θ1) = 1, then",[0],[0]
"In such a case, the use of the steeper loss may indeed make regressors distributionally robust in terms of the same loss.","If Radv(θ1) = 1, then",[0],[0]
"Nonetheless, learning can be extremely sensitive to outliers due to the use of the steeper loss.","If Radv(θ1) = 1, then",[0],[0]
"Hence, when applying DRSL with f -divergences to real-world regression tasks, we need to pay extra attention to ensure that there are no outliers in datasets.","If Radv(θ1) = 1, then",[0],[0]
"In this section, motivated by our theoretical analysis in Section 3, we propose simple yet practical DRSL that overcomes the over pessimism of ARM and AERM in the classification scenario.",4. DRSL with Latent Prior Probability Change,[0],[0]
"We then analyze its convergence property and discuss the practical use of our DRSL.
",4. DRSL with Latent Prior Probability Change,[0],[0]
Theoretical motivation: What insight can we get from our theoretical analyses in Section 3?,4. DRSL with Latent Prior Probability Change,[0],[0]
"Our key insight from proving the theorems is that the adversary of ARM has too much (non-parametric) freedom to shift the test distribution, and as a result, the learner becomes overly pessimistic.",4. DRSL with Latent Prior Probability Change,[0],[0]
"In fact, the proofs of all the theorems rely on the overflexibility of the uncertainty set Uf in Eq.",4. DRSL with Latent Prior Probability Change,[0],[0]
"(6), i.e., the values of r(·, ·) are not tied together for different (x, y) within Uf (see Eqs.",4. DRSL with Latent Prior Probability Change,[0],[0]
(5) and (6)).,4. DRSL with Latent Prior Probability Change,[0],[0]
"Consequently, the adversary of ARM simply assigns larger weight r(x, y) to data (x, y) with a larger loss.",4. DRSL with Latent Prior Probability Change,[0],[0]
"This fact, combined with the fact that we use the different losses during training and testing in classification (see discussion at the end of Section 3), led to the pessimistic results of Theorems 1–3.
",4. DRSL with Latent Prior Probability Change,[0],[0]
"Our theoretical insight suggests that in order to overcome the pessimism of ARM applied to classification, it is crucial to structurally constrain r(·, ·) in Uf , or equivalently, to impose structural assumptions on the distribution shift.",4. DRSL with Latent Prior Probability Change,[0],[0]
"To this end, in this section, we propose DRSL that overcomes the limitation of the DRSL by incorporating structural assumptions on distribution shift.
",4. DRSL with Latent Prior Probability Change,[0],[0]
"Practical structural assumptions: In practice, there can be a variety of ways to impose structural assumptions on distribution shift.",4. DRSL with Latent Prior Probability Change,[0],[0]
"Here, as one possible way, we adopt the latent prior probability change assumption (Storkey & Sugiyama, 2007) because this particular class of assumptions enjoys the following two practical advantages.
1.",4. DRSL with Latent Prior Probability Change,[0],[0]
"Within the class, users of our DRSL can easily and intuitively model their assumptions on distribution shift (see the discussion at the end of this section).
",4. DRSL with Latent Prior Probability Change,[0],[0]
2.,4. DRSL with Latent Prior Probability Change,[0],[0]
"Efficient learning algorithms can be derived (see Section 5).
",4. DRSL with Latent Prior Probability Change,[0],[0]
"Let us introduce a latent variable z ∈ Z ≡ {1, . . .",4. DRSL with Latent Prior Probability Change,[0],[0]
", S}, which we call a latent category, where S is a constant.",4. DRSL with Latent Prior Probability Change,[0],[0]
"The latent prior probability change assumes
p(x, y|z)",4. DRSL with Latent Prior Probability Change,[0],[0]
"= q(x, y|z), q(z) ̸=",4. DRSL with Latent Prior Probability Change,[0],[0]
"p(z), (14)
where p and q are the training and test densities, respectively.",4. DRSL with Latent Prior Probability Change,[0],[0]
"The intuition is that we assume a two-level hierarchical data-generation process: we first sample latent category z from the prior and then sample actual data (x, y) conditioned on z.",4. DRSL with Latent Prior Probability Change,[0],[0]
"We then assume that only the prior distribution over the latent categories changes, leaving the conditional distribution intact.
",4. DRSL with Latent Prior Probability Change,[0],[0]
We assume the structural assumption in Eq.,4. DRSL with Latent Prior Probability Change,[0],[0]
"(14) to be provided externally by users of our DRSL based on their knowledge of potential distribution shift, rather than something to be inferred from data.",4. DRSL with Latent Prior Probability Change,[0],[0]
"As we will see at the end of this section, specifying Eq.",4. DRSL with Latent Prior Probability Change,[0],[0]
"(14) amounts to grouping training data points according to their latent categories, which is quite intuitive to do in practice.
",4. DRSL with Latent Prior Probability Change,[0],[0]
Objective function of our DRSL: With the latent prior probability change of Eq.,4. DRSL with Latent Prior Probability Change,[0],[0]
"(14), the uncertainty set for test distributions in our DRSL becomes
Qp = {q ≪",4. DRSL with Latent Prior Probability Change,[0],[0]
"p | Df [q(x, y, z)||p(x, y, z)]",4. DRSL with Latent Prior Probability Change,[0],[0]
"≤ δ, q(x, y|z) = p(x, y|z)}.",4. DRSL with Latent Prior Probability Change,[0],[0]
"(15)
Then, corresponding to Eq. (3), the objective of our DRSL can be written as
min θ sup w∈Wf Ep(x,y,z)",4. DRSL with Latent Prior Probability Change,[0],[0]
"[w(z)ℓ(gθ(x), y)] ︸ ︷︷ ︸
≡ Rs-adv(θ)
, (16)
",4. DRSL with Latent Prior Probability Change,[0],[0]
Wf ≡ { w(z) ∣∣∣∣∣,4. DRSL with Latent Prior Probability Change,[0],[0]
"∑
z∈Z
p(z)f (w(z)) ≤ δ,
∑
z∈Z
p(z)w(z)",4. DRSL with Latent Prior Probability Change,[0],[0]
"= 1, w(z) ≥ 0, ∀z ∈ Z } , (17)
",4. DRSL with Latent Prior Probability Change,[0],[0]
"where w(z) ≡ q(z)/p(z) = q(x, y, z)/p(x, y, z) because of q(x, y|z) = p(x, y|z).",4. DRSL with Latent Prior Probability Change,[0],[0]
We call Rs-adv(θ) the structural adversarial risk and call the minimization problem of Eq.,4. DRSL with Latent Prior Probability Change,[0],[0]
(16) the structural adversarial risk minimization (structural ARM).,4. DRSL with Latent Prior Probability Change,[0],[0]
"Similarly to ARM, structural ARM is a minimax game between the learner and the adversary.",4. DRSL with Latent Prior Probability Change,[0],[0]
"Differently from ARM, the adversary of structural ARM (corresponding to supw∈Wf ) uses w(·) to reweight data; hence, it has much less (only parametric) freedom to shift the test distribution compared to the adversary of ARM that uses non-parametric weight r(·, ·) (see Eq. (5)).",4. DRSL with Latent Prior Probability Change,[0],[0]
"Because of this limited freedom for the adversary, we can show that Theorems 1–3 do not hold for structural ARM, and we can expect to learn meaningful classifiers that are robust to structurally constrained distribution shift.
Discussion and proposal of evaluation metric for distributional robustness:",4. DRSL with Latent Prior Probability Change,[0],[0]
"Recall from Theorem 1 that when the 0-1 loss is used, the adversarial risk ends up being equivalent to the ordinary risk as an evaluation metric, which is too pessimistic as a metric for the distributional robustness of a classifier.",4. DRSL with Latent Prior Probability Change,[0],[0]
"In contrast, we can easily verify that our structural adversarial risk using the 0-1 loss does not suffer from the pessimism.",4. DRSL with Latent Prior Probability Change,[0],[0]
We argue that our structural adversarial risk can be an alternative metric in distributionally robust classification.,4. DRSL with Latent Prior Probability Change,[0],[0]
"To better understand its property, inspired by Namkoong & Duchi (2017), we decompose it as6
Rs-adv(θ) = R(θ)︸ ︷︷ ︸",4. DRSL with Latent Prior Probability Change,[0],[0]
"(a) ordinary risk
+ √ δ ·
√∑
z∈Z p(z)(Rz(θ)",4. DRSL with Latent Prior Probability Change,[0],[0]
"− R(θ))2 ︸ ︷︷ ︸ (b) sensitivity , (18)
where Rz(θ)(≡ Ep(x,y|z)[ℓ(gθ(x), y)]) is the risk of the classifier on latent category z ∈ Z .",4. DRSL with Latent Prior Probability Change,[0],[0]
We see that Rs-adv(θ) in Eq.,4. DRSL with Latent Prior Probability Change,[0],[0]
(18) contains the risk variance term (b).,4. DRSL with Latent Prior Probability Change,[0],[0]
This variance term (b) can be large when the obtained classifier performs extremely poorly on a small number of latent categories.,4. DRSL with Latent Prior Probability Change,[0],[0]
"Once a test density concentrates on those poorlyperformed latent categories, the test accuracy of the classifier can extremely deteriorate.",4. DRSL with Latent Prior Probability Change,[0],[0]
"In this sense, the classifier with large (b) is sensitive to distribution shift.",4. DRSL with Latent Prior Probability Change,[0],[0]
"In contrast, the small risk variance (b) indicates that the obtained classifier attains similar accuracy on all the latent categories.",4. DRSL with Latent Prior Probability Change,[0],[0]
"In such a case, the test accuracy of the classifier is insensitive to latent category prior change.",4. DRSL with Latent Prior Probability Change,[0],[0]
"In this sense, the classifier with small (b) is robust to distribution shift.",4. DRSL with Latent Prior Probability Change,[0],[0]
"To sum up, the additional term (b) measures the sensitivity of the classifier to the specified structural distribution shift.
",4. DRSL with Latent Prior Probability Change,[0],[0]
"On the basis of the above discussion, we see that Rs-adv(θ) in Eq.",4. DRSL with Latent Prior Probability Change,[0],[0]
"(18) simultaneously captures (a) the ordinary risk, i.e., the mis-classification rate when no distribution shift occurs, and (b) the sensitivity to distribution shift.",4. DRSL with Latent Prior Probability Change,[0],[0]
"In this sense, our structural adversarial risk is an intuitive and reasonable metric for distributional robustness of a classifier, and we will employ this metric in our experiments in Section 6.
",4. DRSL with Latent Prior Probability Change,[0],[0]
Empirical approximation: We explain how to empirically approximate the objective functions in Eqs.,4. DRSL with Latent Prior Probability Change,[0],[0]
(16) and (17) using training data D′ ≡,4. DRSL with Latent Prior Probability Change,[0],[0]
"{(x1, y1, z1), .",4. DRSL with Latent Prior Probability Change,[0],[0]
. .,4. DRSL with Latent Prior Probability Change,[0],[0]
", (xN , yN , zN )} drawn independently from p(x, y, z).
",4. DRSL with Latent Prior Probability Change,[0],[0]
Define Gs ≡,4. DRSL with Latent Prior Probability Change,[0],[0]
{,4. DRSL with Latent Prior Probability Change,[0],[0]
"i | zi = s, 1 ≤",4. DRSL with Latent Prior Probability Change,[0],[0]
"i ≤ N} for 1 ≤ s ≤ S, which is a set of data indices belonging to latent category s. In our DRSL, users are responsible for specifying the groupings of training data points, i.e., {Gs}Ss=1.",4. DRSL with Latent Prior Probability Change,[0],[0]
"By specifying these groupings, the users incorporate their structural
6This particular decomposition holds when the Pearson (PE) divergence is used and δ is not so large.",4. DRSL with Latent Prior Probability Change,[0],[0]
Refer to Appendix E for the derivation.,4. DRSL with Latent Prior Probability Change,[0],[0]
"Analogous decomposition can be also derived when other f -divergences are used.
assumptions on distribution shift into our DRSL.",4. DRSL with Latent Prior Probability Change,[0],[0]
We will discuss how this can be done in practice at the end of this section.,4. DRSL with Latent Prior Probability Change,[0],[0]
"For notational convenience, let ws ≡ w(s), 1 ≤ s ≤ S, and define w ≡",4. DRSL with Latent Prior Probability Change,[0],[0]
"(w1, . . .",4. DRSL with Latent Prior Probability Change,[0],[0]
", wS).",4. DRSL with Latent Prior Probability Change,[0],[0]
"Equations (16) and (17) can be empirically approximated as follows using D′:
min θ sup w∈Ŵf
1 N
S∑
s=1
nswsR̂s(θ)
︸ ︷︷ ︸ ≡ R̂s-adv(θ)
(19)
Ŵf = { w ∈ RS ∣∣∣∣∣ 1 N S∑
s=1
nsf (ws) ≤ δ,
1 N
S∑
s=1
nsws = 1, w ≥ 0 } , (20)
where ns is the cardinality of Gs and R̂s(θ)(≡ 1 ns ∑ i∈Gs ℓi(θ)) is the average loss of all data points in Gs.",4. DRSL with Latent Prior Probability Change,[0],[0]
We call R̂s-adv(θ) the structural adversarial empirical risk and call the minimization problem of Eq.,4. DRSL with Latent Prior Probability Change,[0],[0]
(19) the structural adversarial empirical risk minimization (structural AERM).,4. DRSL with Latent Prior Probability Change,[0],[0]
We can add regularization term Ω(θ) to Eq.,4. DRSL with Latent Prior Probability Change,[0],[0]
"(19) to prevent overfitting.
",4. DRSL with Latent Prior Probability Change,[0],[0]
Convergence rate and estimation error: We establish the convergence rate of the model parameter and the order of the estimation error for structural AERM in terms of the number of training data points N .,4. DRSL with Latent Prior Probability Change,[0],[0]
"Due to the limited space, we only present an informal statement here.",4. DRSL with Latent Prior Probability Change,[0],[0]
"The formal statement can be found in Appendix G and its proof can be found in Appendix H.
Theorem 4 (Convergence rate and estimation error, informal statement).",4. DRSL with Latent Prior Probability Change,[0],[0]
"Let θ∗ be the solution of structural ARM, and θ̂N be the solution of regularized structural AERM given training data of size N .",4. DRSL with Latent Prior Probability Change,[0],[0]
"Assume gθ(x) is linear in θ, and regularization hyper-parameter λ decreases at a rate of O(N−1/2).",4. DRSL with Latent Prior Probability Change,[0],[0]
"Under mild conditions, as N → ∞, we have ∥θ̂N − θ∗∥2 = O(N−1/4) and consequently, |Rs-adv(θ̂N )−Rs-adv(θ∗)| = O(N−1/4).
",4. DRSL with Latent Prior Probability Change,[0],[0]
Notice that the convergence rate of θ̂N to θ∗ is not the optimal parametric rate O(N−1/2).,4. DRSL with Latent Prior Probability Change,[0],[0]
This is because the inner maximization of Eq.,4. DRSL with Latent Prior Probability Change,[0],[0]
(19) converges in O(N−1/4) that slows down the entire convergence rate.,4. DRSL with Latent Prior Probability Change,[0],[0]
"Theorem 4 applies to any f -divergence where f(t) is nonlinear in t, while knowing which f -divergence is used may improve the result to the optimal parametric rate.
",4. DRSL with Latent Prior Probability Change,[0],[0]
"Discussion on groupings: In our structural ARM and AERM, users need to incorporate their structural assumptions by grouping training data points.",4. DRSL with Latent Prior Probability Change,[0],[0]
"Here, we discuss how this can be done in practice.
",4. DRSL with Latent Prior Probability Change,[0],[0]
"Most straightforwardly, a user of our DRSL may assume
class prior change (Saerens et al., 2002) or sub-category7 prior change.",4. DRSL with Latent Prior Probability Change,[0],[0]
"To incorporate such assumptions into our DRSL, the user can simply group training data by class labels or a sub-categories, respectively.
",4. DRSL with Latent Prior Probability Change,[0],[0]
"Alternatively, a user of our DRSL can group data by available meta-information of data such as time and places in which data are collected.",4. DRSL with Latent Prior Probability Change,[0],[0]
"The intuition is that data collected in the same situations (e.g., time and places) are likely to “share the same destiny” in the future distribution shift; hence, it is natural to assume that only the prior over the situations changes at test time while the conditionals remain the same.
",4. DRSL with Latent Prior Probability Change,[0],[0]
"In any case, it is crucial that the users provide structural assumptions on distribution shift so that we can overcome the pessimism of ARM and AERM for classification raised in Section 3.",4. DRSL with Latent Prior Probability Change,[0],[0]
"In this section, we derive efficient gradient-based learning algorithms for our structural AERM in Eq.",5. Efficient Learning Algorithms,[0],[0]
(19).,5. Efficient Learning Algorithms,[0],[0]
"Thanks to Danskin’s theorem (Danskin, 1966), we can obtain the gradient ∇θR̂s-adv(θ) as
∇θR̂s-adv(θ) = 1
N
S∑
s=1
nsw ∗ s∇θR̂s(θ), (21)
where w∗ = (w∗1 , . . .",5. Efficient Learning Algorithms,[0],[0]
", w∗S) is the solution of inner maximization of AERM in Eq.",5. Efficient Learning Algorithms,[0],[0]
"(19).
",5. Efficient Learning Algorithms,[0],[0]
"In the following, we show that w∗ can be obtained very efficiently for two well-known instances of f -divergences.
",5. Efficient Learning Algorithms,[0],[0]
"Kullback-Leibler (KL) divergence: For the KL divergence, f(x) = x log x, we have
w∗s = N
Z(γ) · exp ( R̂s(θ) γ ) , 1 ≤ s ≤ S, (22)
where γ is a scalar such that the first constraint of Ŵf in Eq.",5. Efficient Learning Algorithms,[0],[0]
"(20) holds with equality, and Z(γ) ≡∑S
s=1 nsexp ( R̂s(θ)/γ ) is a normalizing constant in or-
der to satisfy the second constraint of Ŵf .",5. Efficient Learning Algorithms,[0],[0]
"To compute γ, we can simply perform a binary search.
",5. Efficient Learning Algorithms,[0],[0]
"Pearson (PE) divergence: For the PE divergence, f(x) = (x − 1)2.",5. Efficient Learning Algorithms,[0],[0]
"For small δ, w ≥ 0 of Ŵf is often satisfied in practice.",5. Efficient Learning Algorithms,[0],[0]
"We drop the inequality for simplicity; then, the solution of the inner maximization of Eq.",5. Efficient Learning Algorithms,[0],[0]
"(19) becomes analytic and efficient to obtain:
w∗ =
√ Nδ
∑S s=1",5. Efficient Learning Algorithms,[0],[0]
"nsv 2 s v + 1S , (23)
",5. Efficient Learning Algorithms,[0],[0]
7A sub,5. Efficient Learning Algorithms,[0],[0]
"-category (Ristin et al., 2015) is a refined category of a class label, e.g., a “flu” label contains three sub-categories: types A, B, and C flu.
where 1S is the S-dimensional vector with all the elements equal to 1.",5. Efficient Learning Algorithms,[0],[0]
"v is the S-dimensional vector such that vs = R̂s(θ)− R̂(θ), 1 ≤ s ≤ S.
Computational complexity:",5. Efficient Learning Algorithms,[0],[0]
"The time complexity for obtaining w∗ is: O(mS) for the KL divergence and O(S) for the PE divergence, where m is the number of the binary search iterations to compute γ in Eq.",5. Efficient Learning Algorithms,[0],[0]
(22).,5. Efficient Learning Algorithms,[0],[0]
"Calculating the adversarial weights therefore adds negligible computational overheads to computing ∇ℓi(θ) and ℓi(θ) for 1 ≤ i ≤ N , which for example requires O(Nb)-time for a b-dimensional linear-in-parameter model.",5. Efficient Learning Algorithms,[0],[0]
"In this section, we experimentally analyze our DRSL (structural AERM) in classification by comparing it with ordinary ERM and DRSL with f -divergences (AERM).",6. Experiments,[0],[0]
"We empirically demonstrate (i) the undesirability of AERM in classification and (ii) the robustness of structural AERM against specified distribution shift.
",6. Experiments,[0],[0]
"Datasets: We obtained six classification datasets from the UCI repository (Blake & Merz, 1998), two of which are for multi-class classification.",6. Experiments,[0],[0]
"We also obtained MNIST (LeCun et al., 1998) and 20newsgroups (Lang, 1995).",6. Experiments,[0],[0]
"Refer to Appendix I for the details of the datasets.
",6. Experiments,[0],[0]
"Evaluation metrics: We evaluated the three methods (ordinary ERM, AERM and structural AERM) with three kinds of metrics: the ordinary risk, the adversarial risk, and the structural adversarial risk, where the 0-1 loss is used for all the metrics.8",6. Experiments,[0],[0]
"We did not explicitly report the adversarial risk in our experiments because of Theorem 1.
",6. Experiments,[0],[0]
Both the risk and structural adversarial risk are estimated using held-out test data.,6. Experiments,[0],[0]
"In particular, the structural adversarial risk can be estimated similarly to Eqs.",6. Experiments,[0],[0]
"(19) and (20), i.e., calculating the mis-classification rate on the held-out test data and structurally and adversarially reweight them.",6. Experiments,[0],[0]
See discussion of Eq.,6. Experiments,[0],[0]
"(18) for why the structural adversarial risk is a meaningful evaluation metric to measure distributional robustness of classifiers.
",6. Experiments,[0],[0]
"Experimental protocols: For our DRSL, we consider learning classifiers robust against (a) the class prior change and (b) the sub-category prior change.",6. Experiments,[0],[0]
"This corresponds to grouping training data by (a) class labels and (b) subcategory labels, respectively.",6. Experiments,[0],[0]
"In the benchmark datasets, the sub-category labels are not available.",6. Experiments,[0],[0]
"Hence, we manually created such labels as follows.",6. Experiments,[0],[0]
"First, we converted the original multi-class classification problems into classification problems with fewer classes by integrating some classes together.",6. Experiments,[0],[0]
"Then, the original class labels are regarded as the subcategories.",6. Experiments,[0],[0]
"In this way, we converted the satimage, letter and MNIST datasets into binary classification problems, and 20newsgroups into a 7-class classifica-
8To gain more insight on the methods, we also reported all the metrics in terms of the surrogate loss in Appendix K.
tion.",6. Experiments,[0],[0]
"Appendix J details how we grouped the class labels.
",6. Experiments,[0],[0]
"For all the methods, we used linear models with softmax output for the prediction function gθ(x).",6. Experiments,[0],[0]
The cross-entropy loss with ℓ2 regularization was adopted.,6. Experiments,[0],[0]
"The regularization hyper-parameter λ was selected from {1.0, 0.1, 0.01, 0.001, 0.0001} via 5-fold cross validation.
",6. Experiments,[0],[0]
We used the two f -divergences (the KL and PE divergences) and set δ = 0.5 for AERM and structural AERM.,6. Experiments,[0],[0]
The same δ and f -divergence were used for estimating the structural adversarial risk.,6. Experiments,[0],[0]
"At the end of this section, we discuss how we can choose δ in practice.",6. Experiments,[0],[0]
Results:,6. Experiments,[0],[0]
"In Table 1, we report experimental results on the classification tasks when the KL divergence is used.",6. Experiments,[0],[0]
"Refer to Appendix L for the results when the PE divergence is used, which showed similar tendency.
",6. Experiments,[0],[0]
We see from the left half of Table 1 that ordinary ERM achieved lower estimated risks as expected.,6. Experiments,[0],[0]
"On the other hand, we see from the entire Table 1 that AERM, which does not incorporate any structural assumptions on distribution shift, performed poorly in terms of both of two evaluation metrics; hence, it also performed poorly in terms of the adversarial risk (see Theorem 1).",6. Experiments,[0],[0]
This may be because AERM was excessively sensitive to outliers as implied by Theorem 3.,6. Experiments,[0],[0]
We see from the right half of Table 1 that structural AERM achieved significantly lower estimated structural adversarial risks.,6. Experiments,[0],[0]
"Although this was expected, our experiments confirmed that structural AERM indeed obtained classifiers robust against the structural distribution shift.9
9When we used the surrogate loss to evaluate the methods
Discussion: Here we provide an insight for users to determine δ in our DRSL (structural ARM and AERM).",6. Experiments,[0],[0]
We see from Eq.,6. Experiments,[0],[0]
"(18) that the structural adversarial risk can be decomposed into the sum of the ordinary risk and the sensitivity term, where δ acts as a trade-off hyper-parameter between the two terms.",6. Experiments,[0],[0]
"In practice, users of our DRSL may want to have good balance between the two terms, i.e., the learned classifier should achieve high accuracy on the training distribution while being robust to specified distribution shift.",6. Experiments,[0],[0]
"Since both terms in Eq. (18) can be estimated by cross validation, the users can adjust δ of AERM at training time to best trade-off the two terms for their purposes, e.g., increasing δ during training to decrease the sensitivity term at the expense of a slight increase of the risk term.",6. Experiments,[0],[0]
"In this paper, we theoretically analyzed DRSL with f - divergences applied to classification.",7. Conclusion,[0],[0]
"We showed that the DRSL ends up giving a classifier optimal for the training distribution, which is too pessimistic in terms of the original motivation of distributionally robust classification.",7. Conclusion,[0],[0]
"To rectify this, we presented simple DRSL that gives a robust classifier based on structural assumptions on distribution shift.",7. Conclusion,[0],[0]
We derived efficient optimization algorithms for our DRSL and empirically demonstrated its effectiveness.,7. Conclusion,[0],[0]
"(which is not the case in ordinary classification), we confirmed that the methods indeed achieved the best performance in terms of the metrics they optimized for, i.e., ERM, AERM, and structural AERM performed the best in terms of the ordinary risk, adversarial risk and structural adversarial risk, respectively.",7. Conclusion,[0],[0]
See Appendix K for the actual experimental results.,7. Conclusion,[0],[0]
We thank anonymous reviewers for their constructive feedback.,Acknowledgement,[0],[0]
WH was supported by JSPS KAKENHI 18J22289.,Acknowledgement,[0],[0]
MS was supported by CREST JPMJCR1403.,Acknowledgement,[0],[0]
Distributionally Robust Supervised Learning (DRSL) is necessary for building reliable machine learning systems.,abstractText,[0],[0]
"When machine learning is deployed in the real world, its performance can be significantly degraded because test data may follow a different distribution from training data.",abstractText,[0],[0]
DRSL with f -divergences explicitly considers the worst-case distribution shift by minimizing the adversarially reweighted training loss.,abstractText,[0],[0]
"In this paper, we analyze this DRSL, focusing on the classification scenario.",abstractText,[0],[0]
"Since the DRSL is explicitly formulated for a distribution shift scenario, we naturally expect it to give a robust classifier that can aggressively handle shifted distributions.",abstractText,[0],[0]
"However, surprisingly, we prove that the DRSL just ends up giving a classifier that exactly fits the given training distribution, which is too pessimistic.",abstractText,[0],[0]
This pessimism comes from two sources: the particular losses used in classification and the fact that the variety of distributions to which the DRSL tries to be robust is too wide.,abstractText,[0],[0]
"Motivated by our analysis, we propose simple DRSL that overcomes this pessimism and empirically demonstrate its effectiveness.",abstractText,[0],[0]
Does Distributionally Robust Supervised Learning Give Robust Classifiers?,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1077–1087 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1077",text,[0],[0]
"The application that motivates our work is the time-critical analysis of social media (Twitter) data at the sudden-onset of an event like natural or man-made disasters (Imran et al., 2015).",1 Introduction,[0],[0]
"In such events, affected people post timely and useful information of various types such as reports of injured or dead people, infrastructure damage, urgent needs (e.g., food, shelter, medical assistance) on these social networks.",1 Introduction,[0],[0]
"Humanitarian organizations believe timely access to this important information from social networks can help significantly and reduce both human loss and economic dam-
age (Varga et al., 2013; Vieweg et al., 2014; Power et al., 2013).
",1 Introduction,[0],[0]
"In this paper, we consider the basic task of classifying each incoming tweet during a crisis event (e.g., Earthquake) into one of the predefined classes of interest (e.g., relevant vs. nonrelevant) in real-time.",1 Introduction,[0],[0]
"Recently, deep neural networks (DNNs) have shown great performance in classification tasks in NLP and data mining.",1 Introduction,[0],[0]
"However the success of DNNs on a task depends heavily on the availability of a large labeled dataset, which is not a feasible option in our setting (i.e., classifying tweets at the onset of an Earthquake).",1 Introduction,[0],[0]
"On the other hand, in most cases, we can have access to a good amount of labeled and abundant unlabeled data from past similar events (e.g., Floods) and possibly some unlabeled data for the current event.",1 Introduction,[0],[0]
"In such situations, we need methods that can leverage the labeled and unlabeled data in a past event (we refer to this as a source domain), and that can adapt to a new event (we refer to this as a target domain) without requiring any labeled data in the new event.",1 Introduction,[0],[0]
"In other words, we need models that can do domain adaptation to deal with the distribution drift between the domains and semi-supervised learning to leverage the unlabeled data in both domains.
",1 Introduction,[0],[0]
"Most recent approaches to semi-supervised learning (Yang et al., 2016) and domain adaptation (Ganin et al., 2016) use the automatic feature learning capability of DNN models.",1 Introduction,[0],[0]
"In this paper, we extend these methods by proposing a novel model that performs domain adaptation and semi-supervised learning within a single unified deep learning framework.",1 Introduction,[0],[0]
"In this framework, the basic task-solving network (a convolutional neural network in our case) is put together with two other networks – one for semi-supervised learning and the other for domain adaptation.",1 Introduction,[0],[0]
"The semisupervised component learns internal representa-
tions (features) by predicting contextual nodes in a graph that encodes similarity between labeled and unlabeled training instances.",1 Introduction,[0],[0]
"The domain adaptation is achieved by training the feature extractor (or encoder) in adversary with respect to a domain discriminator, a binary classifier that tries to distinguish the domains.",1 Introduction,[0],[0]
"The overall idea is to learn high-level abstract representation that is discriminative for the main classification task, but is invariant across the domains.",1 Introduction,[0],[0]
"We propose a stochastic gradient descent (SGD) algorithm to train the components of our model simultaneously.
",1 Introduction,[0],[0]
The evaluation of our proposed model is conducted using two Twitter datasets on scenarios where there is only unlabeled data in the target domain.,1 Introduction,[0],[0]
"Our results demonstrate the following.
1.",1 Introduction,[0],[0]
"When the network combines the semisupervised component with the supervised component, depending on the amount of labeled data used, it gives 5% to 26% absolute gains in F1 compared to when it uses only the supervised component.
2.",1 Introduction,[0],[0]
"Domain adaptation with adversarial training improves over the adaptation baseline (i.e., a transfer model) by 1.8% to 4.1% absolute F1.
3.",1 Introduction,[0],[0]
"When the network combines domain adversarial training with semi-supervised learning, we get further gains ranging from 5% to 7% absolute in F1 across events.
",1 Introduction,[0],[0]
"Our source code is available on Github1 and the data is available on CrisisNLP2.
",1 Introduction,[0],[0]
The rest of the paper is organized as follows.,1 Introduction,[0],[0]
"In Section 2, we present the proposed method, i.e., domain adaptation and semi-supervised graph embedding learning.",1 Introduction,[0],[0]
"In Section 3, we present the experimental setup and baselines.",1 Introduction,[0],[0]
The results and analysis are presented in Section 4.,1 Introduction,[0],[0]
"In Section 5, we present the works relevant to this study.",1 Introduction,[0],[0]
"Finally, conclusions appear in Section 6.",1 Introduction,[0],[0]
We demonstrate our approach for domain adaptation with adversarial training and graph embedding on a tweet classification task to support crisis response efforts.,2 The Model,[0],[0]
"Let DlS = {ti, yi} Ls i=1 and DuS = {ti} Us i=1",2 The Model,[0],[0]
"be the set of labeled and unlabeled tweets for a source crisis event S (e.g., 1https://github.com/firojalam/ domain-adaptation 2http://crisisnlp.qcri.org
Nepal earthquake), where yi ∈ {1, . . .",2 The Model,[0],[0]
",K} is the class label for tweet ti, Ls and Us are the number of labeled and unlabeled tweets for the source event, respectively.",2 The Model,[0],[0]
"In addition, we have unlabeled tweets DuT = {ti} Ut i=1 for a target event T (e.g., Queensland flood) with Ut being the number of unlabeled tweets in the target domain.",2 The Model,[0],[0]
"Our ultimate goal is to train a cross-domain model p(y|t, θ) with parameters θ that can classify any tweet in the target event T without having any information about class labels in T .
",2 The Model,[0],[0]
Figure 1 shows the overall architecture of our neural model.,2 The Model,[0],[0]
The input to the network is a tweet t =,2 The Model,[0],[0]
"(w1, . . .",2 The Model,[0],[0]
", wn) containing words that come from a finite vocabulary V defined from the training set.",2 The Model,[0],[0]
The first layer of the network maps each of these words into a distributed representation Rd by looking up a shared embedding matrix E ∈ R|V |×d.,2 The Model,[0],[0]
We initialize the embedding matrix E in our network with word embeddings that are pretrained on a large crisis dataset (Subsection 2.5).,2 The Model,[0],[0]
"However, embedding matrix E can also be initialize randomly.",2 The Model,[0],[0]
"The output of the look-up layer is a matrix X ∈ Rn×d, which is passed through a number of convolution and pooling layers to learn higher-level feature representations.",2 The Model,[0],[0]
"A convolution operation applies a filter u ∈ Rk.d to a window of k vectors to produce a new feature ht as
ht = f(u.",2 The Model,[0],[0]
"Xt:t+k−1) (1)
where Xt:t+k−1 is the concatenation of k look-up vectors, and f is a nonlinear activation; we use rectified linear units or ReLU.",2 The Model,[0],[0]
"We apply this filter to each possible k-length windows in X with stride size of 1 to generate a feature map hj as:
hj =",2 The Model,[0],[0]
"[h1, . . .",2 The Model,[0],[0]
", hn+k−1] (2)
",2 The Model,[0],[0]
We repeat this process N times with N different filters to get N different feature maps.,2 The Model,[0],[0]
"We use a wide convolution (Kalchbrenner et al., 2014), which ensures that the filters reach the entire tweet, including the boundary words.",2 The Model,[0],[0]
"This is done by performing zero-padding, where out-ofrange (i.e., t<1 or t>n) vectors are assumed to be zero.",2 The Model,[0],[0]
"With wide convolution, o zero-padding size and 1 stride size, each feature map contains (n + 2o − k + 1) convoluted features.",2 The Model,[0],[0]
"After the convolution, we apply a max-pooling operation to each of the feature maps,
m = [µp(h 1), · · · , µp(hN )]",2 The Model,[0],[0]
"(3)
where µp(hj) refers to the max operation applied to each window of p features with stride size of 1 in the feature map hi.",2 The Model,[0],[0]
"Intuitively, the convolution operation composes local features into higherlevel representations in the feature maps, and maxpooling extracts the most important aspects of each feature map while reducing the output dimensionality.",2 The Model,[0],[0]
"Since each convolution-pooling operation is performed independently, the features extracted become invariant in order (i.e., where they occur in the tweet).",2 The Model,[0],[0]
"To incorporate order information between the pooled features, we include a fully-connected (dense) layer
z = f(Vm) (4)
where V is the weight matrix.",2 The Model,[0],[0]
"We choose a convolutional architecture for feature composition because it has shown impressive results on similar tasks in a supervised setting (Nguyen et al., 2017).
",2 The Model,[0],[0]
"The network at this point splits into three branches (shaded with three different colors in Figure 1) each of which serves a different purpose and contributes a separate loss to the overall loss of the model as defined below:
L(Λ,Φ,Ω,Ψ) =",2 The Model,[0],[0]
"LC(Λ,Φ) + λgLG(Λ,Ω) + λdLD(Λ,Ψ) (5)
where Λ = {U, V } are the convolutional filters and dense layer weights that are shared across the three branches.",2 The Model,[0],[0]
"The first componentLC(Λ,Φ) is a supervised classification loss based on the labeled data in the source event.",2 The Model,[0],[0]
"The second component LG(Λ,Ω) is a graph-based semi-supervised loss that utilizes both labeled and unlabeled data in the
source and target events to induce structural similarity between training instances.",2 The Model,[0],[0]
"The third component LD(Λ,Ω) is an adversary loss that again uses all available data in the source and target domains to induce domain invariance in the learned features.",2 The Model,[0],[0]
The tunable hyperparameters λg and λd control the relative strength of the components.,2 The Model,[0],[0]
"The supervised component induces label information (e.g., relevant vs. non-relevant) directly in the network through the classification loss LC(Λ,Φ), which is computed on the labeled instances in the source event, DlS .",2.1 Supervised Component,[0],[0]
"Specifically, this branch of the network, as shown at the top in Figure 1, takes the shared representations z as input and pass it through a task-specific dense layer
zc = f(Vcz) (6)
where Vc is the corresponding weight matrix.",2.1 Supervised Component,[0],[0]
The activations zc along with the activations from the semi-supervised branch zs are used for classification.,2.1 Supervised Component,[0],[0]
"More formally, the classification layer defines a Softmax
p(y = k|t, θ) = exp
( W Tk [zc; zs] )∑ k′ exp ( W Tk′",2.1 Supervised Component,[0],[0]
"[zc; zs]
) (7) where [.; .] denotes concatenation of two column vectors, Wk are the class weights, and θ = {U, V, Vc,W} defines the relevant parameters for this branch of the network with Λ = {U, V } being the shared parameters and Φ = {Vc,W} being the parameters specific to this branch.",2.1 Supervised Component,[0],[0]
"Once learned,
we use θ for prediction on test tweets.",2.1 Supervised Component,[0],[0]
"The classification loss LC(Λ,Φ) (or LC(θ)) is defined as
LC(Λ,Φ) =",2.1 Supervised Component,[0],[0]
"− 1
Ls Ls∑ i=1",2.1 Supervised Component,[0],[0]
"I(yi = k) log p(yi = k|ti,Λ,Φ) (8)
where I(.) is an indicator function that returns 1 when the argument is true, otherwise it returns 0.",2.1 Supervised Component,[0],[0]
The semi-supervised branch (shown at the middle in Figure 1) induces structural similarity between training instances (labeled or unlabeled) in the source and target events.,2.2 Semi-supervised Component,[0],[0]
"We adopt the recently proposed graph-based semi-supervised deep learning framework (Yang et al., 2016), which shows impressive gains over existing semisupervised methods on multiple datasets.",2.2 Semi-supervised Component,[0],[0]
"In this framework, a “similarity” graph G first encodes relations between training instances, which is then used by the network to learn internal representations (i.e., embeddings).",2.2 Semi-supervised Component,[0],[0]
The semi-supervised branch takes the shared representation z as input and learns internal representations by predicting a node in the graph context of the input tweet.,2.2.1 Learning Graph Embeddings,[0],[0]
"Following (Yang et al., 2016), we use negative sampling to compute the loss for predicting the context node, and we sample two types of contextual nodes: (i) one is based on the graph G to encode structural information, and (ii) the second is based on the labels in DlS to incorporate label information through this branch of the network.",2.2.1 Learning Graph Embeddings,[0],[0]
"The ratio of positive and negative samples is controlled by a random variable ρ1 ∈ (0, 1), and the proportion of the two context types is controlled by another random variable ρ2 ∈ (0, 1); see Algorithm 1 of (Yang et al., 2016) for details on the sampling procedure.
",2.2.1 Learning Graph Embeddings,[0],[0]
"Let (j, γ) is a tuple sampled from the distribution p(j, γ|i,DlS , G), where j is a context node of an input node i and γ ∈ {+1,−1} denotes whether it is a positive or a negative sample; γ = +1 if ti and tj are neighbors in the graph (for graph-based context) or they both have same labels (for label-based context), otherwise γ = −1.",2.2.1 Learning Graph Embeddings,[0],[0]
"The negative log loss for context prediction LG(Λ,Ω) can be written as
LG(Λ,Ω) =",2.2.1 Learning Graph Embeddings,[0],[0]
"− 1
Ls + Us Ls+Us∑ i=1",2.2.1 Learning Graph Embeddings,[0],[0]
"E(j,γ) log σ",2.2.1 Learning Graph Embeddings,[0],[0]
( γCTj zg(i) ),2.2.1 Learning Graph Embeddings,[0],[0]
"(9)
where zg(i) = f(Vgz(i)) defines another dense layer (marked as Dense (zg) in Figure 1) having weights Vg, and Cj is the weight vector associated with the context node tj .",2.2.1 Learning Graph Embeddings,[0],[0]
"Note that here Λ = {U, V } defines the shared parameters and Ω = {Vg, C} defines the parameters specific to the semi-supervised branch of the network.",2.2.1 Learning Graph Embeddings,[0],[0]
"Typically graphs are constructed based on a relational knowledge source, e.g., citation links in (Lu and Getoor, 2003), or distance between instances (Zhu, 2005).",2.2.2 Graph Construction,[0],[0]
"However, we do not have access to such a relational knowledge in our setting.",2.2.2 Graph Construction,[0],[0]
"On the other hand, computing distance between n(n−1)/2 pairs of instances to construct the graph is also very expensive (Muja and Lowe, 2014).",2.2.2 Graph Construction,[0],[0]
"Therefore, we choose to use k-nearest neighborbased approach as it has been successfully used in other study (Steinbach et al., 2000).
",2.2.2 Graph Construction,[0],[0]
"The nearest neighbor graph consists of n vertices and for each vertex, there is an edge set consisting of a subset of n instances, i.e., tweets in our training set.",2.2.2 Graph Construction,[0],[0]
"The edge is defined by the distance measure d(i, j) between tweets ti and tj , where the value of d represents how similar the two tweets are.",2.2.2 Graph Construction,[0],[0]
"We used k-d tree data structure (Bentley, 1975) to efficiently find the nearest instances.",2.2.2 Graph Construction,[0],[0]
"To construct the graph, we first represent each tweet by averaging the word2vec vectors of its words, and then we measure d(i, j) by computing the Euclidean distance between the vectors.",2.2.2 Graph Construction,[0],[0]
The number of nearest neighbor k was set to 10.,2.2.2 Graph Construction,[0],[0]
The reason of averaging the word vectors is that it is computationally simpler and it captures the relevant semantic information for our task in hand.,2.2.2 Graph Construction,[0],[0]
"Likewise, we choose to use Euclidean distance instead of cosine for computational efficiency.",2.2.2 Graph Construction,[0],[0]
The network described so far can learn abstract features through convolutional and dense layers that are discriminative for the classification task (relevant vs. non-relevant).,2.3 Domain Adversarial Component,[0],[0]
"The supervised branch of the network uses labels in the source event to induce label information directly, whereas the semi-supervised branch induces similarity information between labeled and unlabeled instances.",2.3 Domain Adversarial Component,[0],[0]
"However, our goal is also to make these learned features invariant across domains or events (e.g., Nepal Earthquake vs. Queensland Flood).",2.3 Domain Adversarial Component,[0],[0]
"We achieve this by domain adversarial training of
neural networks (Ganin et al., 2016).",2.3 Domain Adversarial Component,[0],[0]
"We put a domain discriminator, another branch in the network (shown at the bottom in Figure 1) that takes the shared internal representation z as input, and tries to discriminate between the domains of the input — in our case, whether the input tweet is from DS or from DT .",2.3 Domain Adversarial Component,[0],[0]
"The domain discriminator is defined by a sigmoid function:
δ̂ = p(d = 1|t,Λ,Ψ) = sigm(wTd",2.3 Domain Adversarial Component,[0],[0]
"zd) (10)
where d ∈ {0, 1} denotes the domain of the input tweet t, wd are the final layer weights of the discriminator, and zd = f(Vdz) defines the hidden layer of the discriminator with layer weights Vd.",2.3 Domain Adversarial Component,[0],[0]
"Here Λ = {U, V } defines the shared parameters, and Ψ = {Vd,wd} defines the parameters specific to the domain discriminator.",2.3 Domain Adversarial Component,[0],[0]
"We use the negative log-probability as the discrimination loss:
Ji(Λ,Ψ) = −di log δ̂ − (1− di) log ( 1− δ̂ ) (11)
",2.3 Domain Adversarial Component,[0],[0]
"We can write the overall domain adversary loss over the source and target domains as
LD(Λ,Ψ) =",2.3 Domain Adversarial Component,[0],[0]
"− 1
Ls + Us Ls+Us∑ i=1",2.3 Domain Adversarial Component,[0],[0]
"Ji(Λ,Ψ)− 1 Ut Ut∑ i=1",2.3 Domain Adversarial Component,[0],[0]
"Ji(Λ,Ψ) (12)
where Ls + Us and Ut are the number of training instances in the source and target domains, respectively.",2.3 Domain Adversarial Component,[0],[0]
"In adversarial training, we seek parameters (saddle point) such that
θ∗ = argmin Λ,Φ,Ω max Ψ L(Λ,Φ,Ω,Ψ) (13)
which involves a maximization with respect to Ψ and a minimization with respect to {Λ,Φ,Ω}.",2.3 Domain Adversarial Component,[0],[0]
"In other words, the updates of the shared parameters Λ = {U, V } for the discriminator work adversarially to the rest of the network, and vice versa.",2.3 Domain Adversarial Component,[0],[0]
"This is achieved by reversing the gradients of the discrimination loss LD(Λ,Ψ), when they are backpropagated to the shared layers (see Figure 1).",2.3 Domain Adversarial Component,[0],[0]
Algorithm 1 illustrates the training algorithm based on stochastic gradient descent (SGD).,2.4 Model Training,[0],[0]
We first initialize the model parameters.,2.4 Model Training,[0],[0]
"The word embedding matrixE is initialized with pre-trained word2vec vectors (see Subsection 2.5) and is kept fixed during training.3 Other parameters are initialized with small random numbers sampled from
3Tuning E on our task by backpropagation increased the training time immensely (3 days compared to 5 hours on a Tesla GPU) without any significant performance gain.
",2.4 Model Training,[0],[0]
"Algorithm 1: Model Training with SGD Input : data DlS , DuS , DuT ; graph G Output: learned parameters θ = {Λ,Φ} 1.",2.4 Model Training,[0],[0]
"Initialize model parameters {E,Λ,Φ,Ω,Ψ}; 2. repeat
//",2.4 Model Training,[0],[0]
"Semi-supervised for each batch sampled from p(j, γ|i,DlS , G) do a) Compute loss LG(Λ,Ω) b)",2.4 Model Training,[0],[0]
"Take a gradient step for LG(Λ,Ω); end //",2.4 Model Training,[0],[0]
"Supervised & domain adversary for each batch sampled from DlS do a) Compute LC(Λ,Φ) and LD(Λ,Ψ) b)",2.4 Model Training,[0],[0]
"Take gradient steps for LC(Λ,Φ) and LD(Λ,Ψ); end // Domain adversary for each batch sampled from DuS and DuT do
a) Compute LD(Λ,Ψ) b) Take a gradient step for LD(Λ,Ψ);
end until convergence;
a uniform distribution (Bengio and Glorot, 2010).",2.4 Model Training,[0],[0]
"We use AdaDelta (Zeiler, 2012) adaptive update to update the parameters.
",2.4 Model Training,[0],[0]
"In each iteration, we do three kinds of gradient updates to account for the three different loss components.",2.4 Model Training,[0],[0]
"First, we do an epoch over all the training instances updating the parameters for the semi-supervised loss, then we do an epoch over the labeled instances in the source domain, each time updating the parameters for the supervised and the domain adversary losses.",2.4 Model Training,[0],[0]
"Finally, we do an epoch over the unlabeled instances in the two domains to account for the domain adversary loss.
",2.4 Model Training,[0],[0]
The main challenge in adversarial training is to balance the competing components of the network.,2.4 Model Training,[0],[0]
"If one component becomes smarter than the other, its loss to the shared layer becomes useless, and the training fails to converge (Arjovsky et al., 2017).",2.4 Model Training,[0],[0]
"Equivalently, if one component becomes weaker, its loss overwhelms that of the other, causing the training to fail.",2.4 Model Training,[0],[0]
"In our experiments, we observed the domain discriminator is weaker than the rest of the network.",2.4 Model Training,[0],[0]
"This could be due to the noisy nature of tweets, which makes the job for the domain discriminator harder.",2.4 Model Training,[0],[0]
"To balance the components, we would want the error signals from the discriminator to be fairly weak, also we would want the supervised loss to have more impact than the semi-supervised loss.",2.4 Model Training,[0],[0]
"In our experiments, the weight of the domain adversary loss λd was fixed to 1e",2.4 Model Training,[0],[0]
"− 8, and the weight of the semi-supervised loss λg was fixed to 1e",2.4 Model Training,[0],[0]
− 2.,2.4 Model Training,[0],[0]
"Other sophisticated weighting schemes have been proposed recently
(Ganin et al., 2016; Arjovsky et al., 2017; Metz et al., 2016).",2.4 Model Training,[0],[0]
"It would be interesting to see how our model performs using these advanced tuning methods, which we leave as a future work.",2.4 Model Training,[0],[0]
"As mentioned, we used word embeddings that are pre-trained on a crisis dataset.",2.5 Crisis Word Embedding,[0],[0]
"To train the wordembedding model, we first pre-processed tweets collected using the AIDR system (Imran et al., 2014) during different events occurred between 2014 and 2016.",2.5 Crisis Word Embedding,[0],[0]
"In the preprocessing step, we lowercased the tweets and removed URLs, digit, time patterns, special characters, single character, username started with the @ symbol.",2.5 Crisis Word Embedding,[0],[0]
"After preprocessing, the resulting dataset contains about 364 million tweets and about 3 billion words.
",2.5 Crisis Word Embedding,[0],[0]
"There are several approaches to train word embeddings such as continuous bag-of-words (CBOW) and skip-gram models of wrod2vec (Mikolov et al., 2013), and Glove (Pennington et al., 2014).",2.5 Crisis Word Embedding,[0],[0]
"For our work, we trained the CBOW model from word2vec.",2.5 Crisis Word Embedding,[0],[0]
"While training CBOW, we filtered out words with a frequency less than or equal to 5, and we used a context window size of 5 and k = 5 negative samples.",2.5 Crisis Word Embedding,[0],[0]
The resulting embedding model contains about 2 million words with vector dimensions of 300.,2.5 Crisis Word Embedding,[0],[0]
"In this section, we describe our experimental settings – datasets used, settings of our models, compared baselines, and evaluation metrics.",3 Experimental Settings,[0],[0]
"To conduct the experiment and evaluate our system, we used two real-world Twitter datasets collected during the 2015 Nepal earthquake (NEQ) and the 2013 Queensland floods (QFL).",3.1 Datasets,[0],[0]
"These datasets are comprised of millions of tweets collected through the Twitter streaming API4 using event-specific keywords/hashtags.
",3.1 Datasets,[0],[0]
To obtain the labeled examples for our task we employed paid workers from the Crowdflower5 – a crowdsourcing platform.,3.1 Datasets,[0],[0]
The annotation consists of two classes relevant and non-relevant.,3.1 Datasets,[0],[0]
"For the annotation, we randomly sampled 11,670 and 10,033 tweets from the Nepal earthquake and the Queensland floods datasets, respectively.",3.1 Datasets,[0],[0]
"Given a
4https://dev.twitter.com/streaming/overview 5http://crowdflower.com
tweet, we asked crowdsourcing workers to assign the “relevant” label if the tweet conveys/reports information useful for crisis response such as a report of injured or dead people, some kind of infrastructure damage, urgent needs of affected people, donations requests or offers, otherwise assign the “non-relevant” label.",3.1 Datasets,[0],[0]
"We split the labeled data into 60% as training, 30% as test and 10% as development.",3.1 Datasets,[0],[0]
Table 1 shows the resulting datasets with class-wise distributions.,3.1 Datasets,[0],[0]
Data preprocessing was performed by following the same steps used to train the word2vec model (Subsection 2.5).,3.1 Datasets,[0],[0]
"In all the experiments, the classification task consists of two classes: relevant and non-relevant.",3.1 Datasets,[0],[0]
"In order to demonstrate the effectiveness of our joint learning approach, we performed a series of experiments.",3.2 Model Settings and Baselines,[0],[0]
"To understand the contribution of different network components, we performed an ablation study showing how the model performs as a semi-supervised model alone and as a domain adaptation model alone, and then we compare them with the combined model that incorporates all the components.",3.2 Model Settings and Baselines,[0],[0]
"As a baseline for the semi-supervised experiments, we used the self-training approach (Scudder, 1965).",3.2.1 Settings for Semi-supervised Learning,[0],[0]
"For this purpose, we first trained a supervised model using the CNN architecture (i.e., shared components followed by the supervised part in Figure 1).",3.2.1 Settings for Semi-supervised Learning,[0],[0]
The trained model was then used to automatically label the unlabeled data.,3.2.1 Settings for Semi-supervised Learning,[0],[0]
"Instances with a classifier confidence score ≥ 0.75 were then used to retrain a new model.
",3.2.1 Settings for Semi-supervised Learning,[0],[0]
"Next, we run experiments using our graphbased semi-supervised approach (i.e., shared components followed by the supervised and semisupervised parts in Figure 1), which exploits unlabeled data.",3.2.1 Settings for Semi-supervised Learning,[0],[0]
"For reducing the computational cost, we randomly selected 50K unlabeled instances from the same domain.",3.2.1 Settings for Semi-supervised Learning,[0],[0]
"For our semi-supervised setting, one of the main goals was to understand how much labeled data is sufficient to obtain a
reasonable result.",3.2.1 Settings for Semi-supervised Learning,[0],[0]
"Therefore, we experimented our system by incrementally adding batches of instances, such as 100, 500, 2000, 5000, and all instances from the training set.",3.2.1 Settings for Semi-supervised Learning,[0],[0]
Such an understanding can help us design the model at the onset of a crisis event with sufficient amount of labeled data.,3.2.1 Settings for Semi-supervised Learning,[0],[0]
"To demonstrate that the semi-supervised approach outperforms the supervised baseline, we run supervised experiments using the same number of labeled instances.",3.2.1 Settings for Semi-supervised Learning,[0],[0]
"In the supervised setting, only zc activations in Figure 1 are used for classification.",3.2.1 Settings for Semi-supervised Learning,[0],[0]
"To set a baseline for the domain adaptation experiments, we train a CNN model (i.e., shared components followed by the supervised part in Figure 1) on one event (source) and test it on another event (target).",3.2.2 Settings for Domain Adaptation,[0],[0]
"We call this as transfer baseline.
",3.2.2 Settings for Domain Adaptation,[0],[0]
"To assess the performance of our domain adaptation technique alone, we exclude the semisupervised component from the network.",3.2.2 Settings for Domain Adaptation,[0],[0]
"We train and evaluate models with this network configuration using different source and target domains.
",3.2.2 Settings for Domain Adaptation,[0],[0]
"Finally, we integrate all the components of the network as shown in Figure 1 and run domain adaptation experiments using different source and target domains.",3.2.2 Settings for Domain Adaptation,[0],[0]
"In all our domain adaptation experiments, we only use unlabeled instances from the target domain.",3.2.2 Settings for Domain Adaptation,[0],[0]
"In domain adaption literature, this is known as unsupervised adaptation.",3.2.2 Settings for Domain Adaptation,[0],[0]
"We use 100, 150, and 200 filters each having the window size of 2, 3, and 4, respectively, and pooling length of 2, 3, and 4, respectively.",3.2.3 Training Settings,[0],[0]
We do not tune these hyperparameters in any experimental setting since the goal was to have an end-to-end comparison with the same hyperparameter setting and understand whether our approach can outperform the baselines or not.,3.2.3 Training Settings,[0],[0]
"Furthermore, we do not filter out any vocabulary item in any settings.
",3.2.3 Training Settings,[0],[0]
"As mentioned before in Subsection 2.4, we used AdaDelta (Zeiler, 2012) to update the model parameters in each SGD step.",3.2.3 Training Settings,[0],[0]
The learning rate was set to 0.1 when optimizing on the classification loss and to 0.001 when optimizing on the semisupervised loss.,3.2.3 Training Settings,[0],[0]
The learning rate for domain adversarial training was set to 1.0.,3.2.3 Training Settings,[0],[0]
"The maximum number of epochs was set to 200, and dropout rate of 0.02 was used to avoid overfitting (Srivastava et al., 2014).",3.2.3 Training Settings,[0],[0]
"We used validation-based early stopping using the F-measure with a patience of 25,
Experiments AUC P R F1
NEPAL EARTHQUAKE
Supervised 61.22 62.42 62.31 60.89
Semi-supervised (Self-training) 61.15 61.53 61.53 61.26
Semi-supervised (Graph-based) 64.81 64.58 64.63 65.11
QUEENSLAND FLOODS
i.e., we stop training if the score does not increase for 25 consecutive epochs.",3.2.3 Training Settings,[0],[0]
"To measure the performance of the trained models using different approaches described above, we use weighted average precision, recall, F-measure, and Area Under ROC-Curve (AUC), which are standard evaluation measures in the NLP and machine learning communities.",3.2.4 Evaluation Metrics,[0],[0]
The rationale behind choosing the weighted metric is that it takes into account the class imbalance problem.,3.2.4 Evaluation Metrics,[0],[0]
"In this section, we present the experimental results and discuss our main findings.",4 Results and Discussion,[0],[0]
"In Table 2, we present the results obtained from the supervised, self-training based semi-supervised, and our graph-based semi-supervised experiments for the both datasets.",4.1 Semi-supervised Learning,[0],[0]
It can be clearly observed that the graph-based semi-supervised approach outperforms the two baselines – supervised and self-training based semi-supervised.,4.1 Semi-supervised Learning,[0],[0]
"Specifically, the graph-based approach shows 4% to 13% absolute improvements in terms of F1 scores for the Nepal and Queensland datasets, respectively.
To determine how the semi-supervised approach performs in the early hours of an event when only fewer labeled instances are available, we mimic a batch-wise (not to be confused with minibatch in SGD) learning setting.",4.1 Semi-supervised Learning,[0],[0]
"In Table 3, we present the results using different batch sizes – 100, 500, 1,000, 2,000, and all labels.
",4.1 Semi-supervised Learning,[0],[0]
"From the results, we observe that models’ performance improve as we include more labeled data
Exp.",4.1 Semi-supervised Learning,[0],[0]
100 500 1000 2000,4.1 Semi-supervised Learning,[0],[0]
"All L
NEPAL EARTHQUAKE
L 43.63 52.89 56.37 60.11 60.89
L+50kU 52.32 59.95 61.89 64.05 65.11
QUEENSLAND FLOOD
— from 43.63 to 60.89 for NEQ and from 48.97 to 80.16 for QFL in the case of labeled only (L).",4.1 Semi-supervised Learning,[0],[0]
"When we compare supervised vs. semi-supervised (L vs. L+U), we observe significant improvements in F1 scores for the semi-supervised model for all batches over the two datasets.",4.1 Semi-supervised Learning,[0],[0]
"As we include unlabeled instances with labeled instances from the same event, performance significantly improves in each experimental setting giving 5% to 26% absolute improvements over the supervised models.",4.1 Semi-supervised Learning,[0],[0]
These improvements demonstrate the effectiveness of our approach.,4.1 Semi-supervised Learning,[0],[0]
We also notice that our semi-supervised approach can perform above 90% depending on the event.,4.1 Semi-supervised Learning,[0],[0]
"Specifically, major improvements are observed from batch size 100 to 1,000, however, after that the performance improvements are comparatively minor.",4.1 Semi-supervised Learning,[0],[0]
"The results obtained using batch sizes 500 and 1,000 are reasonably in the acceptable range when labeled and unlabeled instances are combined (i.e., L+50kU for Nepal and L+∼21kU for Queensland), which is also a reasonable number of training examples to obtain at the onset of an event.",4.1 Semi-supervised Learning,[0],[0]
"In Table 4, we present domain adaptation results.",4.2 Domain Adaptation,[0],[0]
"The first block shows event-specific (i.e., train and test on the same event)",4.2 Domain Adaptation,[0],[0]
results for the supervised CNN model.,4.2 Domain Adaptation,[0],[0]
These results set the upper bound for our domain adaptation methods.,4.2 Domain Adaptation,[0],[0]
"The transfer baselines are shown in the next block, where we train a CNN model in one domain and test it on a different domain.",4.2 Domain Adaptation,[0],[0]
"Then, the third block shows the results for the domain adversarial approach without the semi-supervised loss.",4.2 Domain Adaptation,[0],[0]
These results show the importance of domain adversarial component.,4.2 Domain Adaptation,[0],[0]
"After that, the fourth block presents the performance of the model trained with graph
Source Target AUC P R F1
",4.2 Domain Adaptation,[0],[0]
"IN-DOMAIN SUPERVISED MODEL
embedding without domain adaptation to show the importance of semi-supervised learning.",4.2 Domain Adaptation,[0],[0]
"The final block present the results for the complete model that includes all the loss components.
",4.2 Domain Adaptation,[0],[0]
The results with domain adversarial training show improvements across both events – from 1.8% to 4.1% absolute gains in F1.,4.2 Domain Adaptation,[0],[0]
"These results attest that adversarial training is an effective approach to induce domain invariant features in the internal representation as shown previously by Ganin et al. (2016).
",4.2 Domain Adaptation,[0],[0]
"Finally, when we do both semi-supervised learning and unsupervised domain adaptation, we get further improvements in F1 scores ranging from 5% to 7% absolute gains.",4.2 Domain Adaptation,[0],[0]
"From these improvements, we can conclude that domain adaptation with adversarial training along with graphbased semi-supervised learning is an effective method to leverage unlabeled and labeled data from a different domain.
",4.2 Domain Adaptation,[0],[0]
"Note that for our domain adaptation methods, we only use unlabeled data from the target domain.",4.2 Domain Adaptation,[0],[0]
"Hence, we foresee future improvements of this approach by utilizing a small amount of target domain labeled data.",4.2 Domain Adaptation,[0],[0]
Two lines of research are directly related to our work: (i) semi-supervised learning and (ii) domain adaptation.,5 Related Work,[0],[0]
Several models have been proposed for semi-supervised learning.,5 Related Work,[0],[0]
"The earliest approach is self-training (Scudder, 1965), in
which a trained model is first used to label unlabeled data instances followed by the model retraining with the most confident predicted labeled instances.",5 Related Work,[0],[0]
"The co-training (Mitchell, 1999) approach assumes that features can be split into two sets and each subset is then used to train a classifier with an assumption that the two sets are conditionally independent.",5 Related Work,[0],[0]
"Then each classifier classifies the unlabeled data, and then most confident data instances are used to re-train the other classifier, this process repeats multiple times.
",5 Related Work,[0],[0]
"In the graph-based semi-supervised approach, nodes in a graph represent labeled and unlabeled instances and edge weights represent the similarity between them.",5 Related Work,[0],[0]
"The structural information encoded in the graph is then used to regularize a model (Zhu, 2005).",5 Related Work,[0],[0]
"There are two paradigms in semi-supervised learning: 1) inductive – learning a function with which predictions can be made on unobserved instances, 2) transductive – no explicit function is learned and predictions can only be made on observed instances.",5 Related Work,[0],[0]
"As mentioned before, inductive semi-supervised learning is preferable over the transductive approach since it avoids building the graph each time it needs to infer the labels for the unlabeled instances.
",5 Related Work,[0],[0]
"In our work, we use a graph-based inductive deep learning approach proposed by Yang et al. (2016) to learn features in a deep learning model by predicting contextual (i.e., neighboring) nodes in the graph.",5 Related Work,[0],[0]
"However, our approach is different from Yang et al. (2016) in several ways.",5 Related Work,[0],[0]
"First, we construct the graph by computing the distance between tweets based on word embeddings.",5 Related Work,[0],[0]
"Second, instead of using count-based features, we use a convolutional neural network (CNN) to compose high-level features from the distributed representation of the words in a tweet.",5 Related Work,[0],[0]
"Finally, for context prediction, instead of performing a random walk, we select nodes based on their similarity in the graph.",5 Related Work,[0],[0]
"Similar similarity-based graph has shown impressive results in learning sentence representations (Saha et al., 2017).
",5 Related Work,[0],[0]
"In the literature, the proposed approaches for domain adaptation include supervised, semisupervised and unsupervised.",5 Related Work,[0],[0]
"It also varies from linear kernelized approach (Blitzer et al., 2006) to non-linear deep neural network techniques (Glorot et al., 2011; Ganin et al., 2016).",5 Related Work,[0],[0]
"One direction of research is to focus on feature space distribution matching by reweighting the samples from
the source domain (Gong et al., 2013) to map source into target.",5 Related Work,[0],[0]
The overall idea is to learn a good feature representation that is invariant across domains.,5 Related Work,[0],[0]
"In the deep learning paradigm, Glorot et al. (Glorot et al., 2011) used Stacked Denoising Auto-Encoders (SDAs) for domain adaptation.",5 Related Work,[0],[0]
"SDAs learn a robust feature representation, which is artificially corrupted with small Gaussian noise.",5 Related Work,[0],[0]
"Adversarial training of neural networks has shown big impact recently, especially in areas such as computer vision, where generative unsupervised models have proved capable of synthesizing new images (Goodfellow et al., 2014; Radford et al., 2015; Makhzani et al., 2015).",5 Related Work,[0],[0]
"Ganin et al. (2016) proposed domain adversarial neural networks (DANN) to learn discriminative but at the same time domain-invariant representations, with domain adaptation as a target.",5 Related Work,[0],[0]
"We extend this work by combining with semi-supervised graph embedding for unsupervised domain adaptation.
",5 Related Work,[0],[0]
"In a recent work, Kipf and Welling (2016) present CNN applied directly on graph-structured datasets - citation networks and on a knowledge graph dataset.",5 Related Work,[0],[0]
Their study demonstrate that graph convolution network for semi-supervised classification performs better compared to other graph based approaches.,5 Related Work,[0],[0]
"In this paper, we presented a deep learning framework that performs domain adaptation with adversarial training and graph-based semi-supervised learning to leverage labeled and unlabeled data from related events.",6 Conclusions,[0],[0]
"We use a convolutional neural network to compose high-level representation from the input, which is then passed to three components that perform supervised training, semisupervised learning and domain adversarial training.",6 Conclusions,[0],[0]
"For domain adaptation, we considered a scenario, where we have only unlabeled data in the target event.",6 Conclusions,[0],[0]
"Our evaluation on two crisis-related tweet datasets demonstrates that by combining domain adversarial training with semi-supervised learning, our model gives significant improvements over their respective baselines.",6 Conclusions,[0],[0]
We have also presented results of batch-wise incremental training of the graph-based semi-supervised approach and show approximation regarding the number of labeled examples required to get an acceptable performance at the onset of an event.,6 Conclusions,[0],[0]
The success of deep neural networks (DNNs) is heavily dependent on the availability of labeled data.,abstractText,[0],[0]
"However, obtaining labeled data is a big challenge in many real-world problems.",abstractText,[0],[0]
"In such scenarios, a DNN model can leverage labeled and unlabeled data from a related domain, but it has to deal with the shift in data distributions between the source and the target domains.",abstractText,[0],[0]
"In this paper, we study the problem of classifying social media posts during a crisis event (e.g., Earthquake).",abstractText,[0],[0]
"For that, we use labeled and unlabeled data from past similar events (e.g., Flood) and unlabeled data for the current event.",abstractText,[0],[0]
We propose a novel model that performs adversarial learning based domain adaptation to deal with distribution drifts and graph based semi-supervised learning to leverage unlabeled data within a single unified deep learning framework.,abstractText,[0],[0]
Our experiments with two real-world crisis datasets collected from Twitter demonstrate significant improvements over several baselines.,abstractText,[0],[0]
Domain Adaptation with Adversarial Training and Graph Embeddings,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1077–1087 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1077",text,[0],[0]
"The application that motivates our work is the time-critical analysis of social media (Twitter) data at the sudden-onset of an event like natural or man-made disasters (Imran et al., 2015).",1 Introduction,[0],[0]
"In such events, affected people post timely and useful information of various types such as reports of injured or dead people, infrastructure damage, urgent needs (e.g., food, shelter, medical assistance) on these social networks.",1 Introduction,[0],[0]
"Humanitarian organizations believe timely access to this important information from social networks can help significantly and reduce both human loss and economic dam-
age (Varga et al., 2013; Vieweg et al., 2014; Power et al., 2013).
",1 Introduction,[0],[0]
"In this paper, we consider the basic task of classifying each incoming tweet during a crisis event (e.g., Earthquake) into one of the predefined classes of interest (e.g., relevant vs. nonrelevant) in real-time.",1 Introduction,[0],[0]
"Recently, deep neural networks (DNNs) have shown great performance in classification tasks in NLP and data mining.",1 Introduction,[0],[0]
"However the success of DNNs on a task depends heavily on the availability of a large labeled dataset, which is not a feasible option in our setting (i.e., classifying tweets at the onset of an Earthquake).",1 Introduction,[0],[0]
"On the other hand, in most cases, we can have access to a good amount of labeled and abundant unlabeled data from past similar events (e.g., Floods) and possibly some unlabeled data for the current event.",1 Introduction,[0],[0]
"In such situations, we need methods that can leverage the labeled and unlabeled data in a past event (we refer to this as a source domain), and that can adapt to a new event (we refer to this as a target domain) without requiring any labeled data in the new event.",1 Introduction,[0],[0]
"In other words, we need models that can do domain adaptation to deal with the distribution drift between the domains and semi-supervised learning to leverage the unlabeled data in both domains.
",1 Introduction,[0],[0]
"Most recent approaches to semi-supervised learning (Yang et al., 2016) and domain adaptation (Ganin et al., 2016) use the automatic feature learning capability of DNN models.",1 Introduction,[0],[0]
"In this paper, we extend these methods by proposing a novel model that performs domain adaptation and semi-supervised learning within a single unified deep learning framework.",1 Introduction,[0],[0]
"In this framework, the basic task-solving network (a convolutional neural network in our case) is put together with two other networks – one for semi-supervised learning and the other for domain adaptation.",1 Introduction,[0],[0]
"The semisupervised component learns internal representa-
tions (features) by predicting contextual nodes in a graph that encodes similarity between labeled and unlabeled training instances.",1 Introduction,[0],[0]
"The domain adaptation is achieved by training the feature extractor (or encoder) in adversary with respect to a domain discriminator, a binary classifier that tries to distinguish the domains.",1 Introduction,[0],[0]
"The overall idea is to learn high-level abstract representation that is discriminative for the main classification task, but is invariant across the domains.",1 Introduction,[0],[0]
"We propose a stochastic gradient descent (SGD) algorithm to train the components of our model simultaneously.
",1 Introduction,[0],[0]
The evaluation of our proposed model is conducted using two Twitter datasets on scenarios where there is only unlabeled data in the target domain.,1 Introduction,[0],[0]
"Our results demonstrate the following.
1.",1 Introduction,[0],[0]
"When the network combines the semisupervised component with the supervised component, depending on the amount of labeled data used, it gives 5% to 26% absolute gains in F1 compared to when it uses only the supervised component.
2.",1 Introduction,[0],[0]
"Domain adaptation with adversarial training improves over the adaptation baseline (i.e., a transfer model) by 1.8% to 4.1% absolute F1.
3.",1 Introduction,[0],[0]
"When the network combines domain adversarial training with semi-supervised learning, we get further gains ranging from 5% to 7% absolute in F1 across events.
",1 Introduction,[0],[0]
"Our source code is available on Github1 and the data is available on CrisisNLP2.
",1 Introduction,[0],[0]
The rest of the paper is organized as follows.,1 Introduction,[0],[0]
"In Section 2, we present the proposed method, i.e., domain adaptation and semi-supervised graph embedding learning.",1 Introduction,[0],[0]
"In Section 3, we present the experimental setup and baselines.",1 Introduction,[0],[0]
The results and analysis are presented in Section 4.,1 Introduction,[0],[0]
"In Section 5, we present the works relevant to this study.",1 Introduction,[0],[0]
"Finally, conclusions appear in Section 6.",1 Introduction,[0],[0]
We demonstrate our approach for domain adaptation with adversarial training and graph embedding on a tweet classification task to support crisis response efforts.,2 The Model,[0],[0]
"Let DlS = {ti, yi} Ls i=1 and DuS = {ti} Us i=1",2 The Model,[0],[0]
"be the set of labeled and unlabeled tweets for a source crisis event S (e.g., 1https://github.com/firojalam/ domain-adaptation 2http://crisisnlp.qcri.org
Nepal earthquake), where yi ∈ {1, . . .",2 The Model,[0],[0]
",K} is the class label for tweet ti, Ls and Us are the number of labeled and unlabeled tweets for the source event, respectively.",2 The Model,[0],[0]
"In addition, we have unlabeled tweets DuT = {ti} Ut i=1 for a target event T (e.g., Queensland flood) with Ut being the number of unlabeled tweets in the target domain.",2 The Model,[0],[0]
"Our ultimate goal is to train a cross-domain model p(y|t, θ) with parameters θ that can classify any tweet in the target event T without having any information about class labels in T .
",2 The Model,[0],[0]
Figure 1 shows the overall architecture of our neural model.,2 The Model,[0],[0]
The input to the network is a tweet t =,2 The Model,[0],[0]
"(w1, . . .",2 The Model,[0],[0]
", wn) containing words that come from a finite vocabulary V defined from the training set.",2 The Model,[0],[0]
The first layer of the network maps each of these words into a distributed representation Rd by looking up a shared embedding matrix E ∈ R|V |×d.,2 The Model,[0],[0]
We initialize the embedding matrix E in our network with word embeddings that are pretrained on a large crisis dataset (Subsection 2.5).,2 The Model,[0],[0]
"However, embedding matrix E can also be initialize randomly.",2 The Model,[0],[0]
"The output of the look-up layer is a matrix X ∈ Rn×d, which is passed through a number of convolution and pooling layers to learn higher-level feature representations.",2 The Model,[0],[0]
"A convolution operation applies a filter u ∈ Rk.d to a window of k vectors to produce a new feature ht as
ht = f(u.",2 The Model,[0],[0]
"Xt:t+k−1) (1)
where Xt:t+k−1 is the concatenation of k look-up vectors, and f is a nonlinear activation; we use rectified linear units or ReLU.",2 The Model,[0],[0]
"We apply this filter to each possible k-length windows in X with stride size of 1 to generate a feature map hj as:
hj =",2 The Model,[0],[0]
"[h1, . . .",2 The Model,[0],[0]
", hn+k−1] (2)
",2 The Model,[0],[0]
We repeat this process N times with N different filters to get N different feature maps.,2 The Model,[0],[0]
"We use a wide convolution (Kalchbrenner et al., 2014), which ensures that the filters reach the entire tweet, including the boundary words.",2 The Model,[0],[0]
"This is done by performing zero-padding, where out-ofrange (i.e., t<1 or t>n) vectors are assumed to be zero.",2 The Model,[0],[0]
"With wide convolution, o zero-padding size and 1 stride size, each feature map contains (n + 2o − k + 1) convoluted features.",2 The Model,[0],[0]
"After the convolution, we apply a max-pooling operation to each of the feature maps,
m = [µp(h 1), · · · , µp(hN )]",2 The Model,[0],[0]
"(3)
where µp(hj) refers to the max operation applied to each window of p features with stride size of 1 in the feature map hi.",2 The Model,[0],[0]
"Intuitively, the convolution operation composes local features into higherlevel representations in the feature maps, and maxpooling extracts the most important aspects of each feature map while reducing the output dimensionality.",2 The Model,[0],[0]
"Since each convolution-pooling operation is performed independently, the features extracted become invariant in order (i.e., where they occur in the tweet).",2 The Model,[0],[0]
"To incorporate order information between the pooled features, we include a fully-connected (dense) layer
z = f(Vm) (4)
where V is the weight matrix.",2 The Model,[0],[0]
"We choose a convolutional architecture for feature composition because it has shown impressive results on similar tasks in a supervised setting (Nguyen et al., 2017).
",2 The Model,[0],[0]
"The network at this point splits into three branches (shaded with three different colors in Figure 1) each of which serves a different purpose and contributes a separate loss to the overall loss of the model as defined below:
L(Λ,Φ,Ω,Ψ) =",2 The Model,[0],[0]
"LC(Λ,Φ) + λgLG(Λ,Ω) + λdLD(Λ,Ψ) (5)
where Λ = {U, V } are the convolutional filters and dense layer weights that are shared across the three branches.",2 The Model,[0],[0]
"The first componentLC(Λ,Φ) is a supervised classification loss based on the labeled data in the source event.",2 The Model,[0],[0]
"The second component LG(Λ,Ω) is a graph-based semi-supervised loss that utilizes both labeled and unlabeled data in the
source and target events to induce structural similarity between training instances.",2 The Model,[0],[0]
"The third component LD(Λ,Ω) is an adversary loss that again uses all available data in the source and target domains to induce domain invariance in the learned features.",2 The Model,[0],[0]
The tunable hyperparameters λg and λd control the relative strength of the components.,2 The Model,[0],[0]
"The supervised component induces label information (e.g., relevant vs. non-relevant) directly in the network through the classification loss LC(Λ,Φ), which is computed on the labeled instances in the source event, DlS .",2.1 Supervised Component,[0],[0]
"Specifically, this branch of the network, as shown at the top in Figure 1, takes the shared representations z as input and pass it through a task-specific dense layer
zc = f(Vcz) (6)
where Vc is the corresponding weight matrix.",2.1 Supervised Component,[0],[0]
The activations zc along with the activations from the semi-supervised branch zs are used for classification.,2.1 Supervised Component,[0],[0]
"More formally, the classification layer defines a Softmax
p(y = k|t, θ) = exp
( W Tk [zc; zs] )∑ k′ exp ( W Tk′",2.1 Supervised Component,[0],[0]
"[zc; zs]
) (7) where [.; .] denotes concatenation of two column vectors, Wk are the class weights, and θ = {U, V, Vc,W} defines the relevant parameters for this branch of the network with Λ = {U, V } being the shared parameters and Φ = {Vc,W} being the parameters specific to this branch.",2.1 Supervised Component,[0],[0]
"Once learned,
we use θ for prediction on test tweets.",2.1 Supervised Component,[0],[0]
"The classification loss LC(Λ,Φ) (or LC(θ)) is defined as
LC(Λ,Φ) =",2.1 Supervised Component,[0],[0]
"− 1
Ls Ls∑ i=1",2.1 Supervised Component,[0],[0]
"I(yi = k) log p(yi = k|ti,Λ,Φ) (8)
where I(.) is an indicator function that returns 1 when the argument is true, otherwise it returns 0.",2.1 Supervised Component,[0],[0]
The semi-supervised branch (shown at the middle in Figure 1) induces structural similarity between training instances (labeled or unlabeled) in the source and target events.,2.2 Semi-supervised Component,[0],[0]
"We adopt the recently proposed graph-based semi-supervised deep learning framework (Yang et al., 2016), which shows impressive gains over existing semisupervised methods on multiple datasets.",2.2 Semi-supervised Component,[0],[0]
"In this framework, a “similarity” graph G first encodes relations between training instances, which is then used by the network to learn internal representations (i.e., embeddings).",2.2 Semi-supervised Component,[0],[0]
The semi-supervised branch takes the shared representation z as input and learns internal representations by predicting a node in the graph context of the input tweet.,2.2.1 Learning Graph Embeddings,[0],[0]
"Following (Yang et al., 2016), we use negative sampling to compute the loss for predicting the context node, and we sample two types of contextual nodes: (i) one is based on the graph G to encode structural information, and (ii) the second is based on the labels in DlS to incorporate label information through this branch of the network.",2.2.1 Learning Graph Embeddings,[0],[0]
"The ratio of positive and negative samples is controlled by a random variable ρ1 ∈ (0, 1), and the proportion of the two context types is controlled by another random variable ρ2 ∈ (0, 1); see Algorithm 1 of (Yang et al., 2016) for details on the sampling procedure.
",2.2.1 Learning Graph Embeddings,[0],[0]
"Let (j, γ) is a tuple sampled from the distribution p(j, γ|i,DlS , G), where j is a context node of an input node i and γ ∈ {+1,−1} denotes whether it is a positive or a negative sample; γ = +1 if ti and tj are neighbors in the graph (for graph-based context) or they both have same labels (for label-based context), otherwise γ = −1.",2.2.1 Learning Graph Embeddings,[0],[0]
"The negative log loss for context prediction LG(Λ,Ω) can be written as
LG(Λ,Ω) =",2.2.1 Learning Graph Embeddings,[0],[0]
"− 1
Ls + Us Ls+Us∑ i=1",2.2.1 Learning Graph Embeddings,[0],[0]
"E(j,γ) log σ",2.2.1 Learning Graph Embeddings,[0],[0]
( γCTj zg(i) ),2.2.1 Learning Graph Embeddings,[0],[0]
"(9)
where zg(i) = f(Vgz(i)) defines another dense layer (marked as Dense (zg) in Figure 1) having weights Vg, and Cj is the weight vector associated with the context node tj .",2.2.1 Learning Graph Embeddings,[0],[0]
"Note that here Λ = {U, V } defines the shared parameters and Ω = {Vg, C} defines the parameters specific to the semi-supervised branch of the network.",2.2.1 Learning Graph Embeddings,[0],[0]
"Typically graphs are constructed based on a relational knowledge source, e.g., citation links in (Lu and Getoor, 2003), or distance between instances (Zhu, 2005).",2.2.2 Graph Construction,[0],[0]
"However, we do not have access to such a relational knowledge in our setting.",2.2.2 Graph Construction,[0],[0]
"On the other hand, computing distance between n(n−1)/2 pairs of instances to construct the graph is also very expensive (Muja and Lowe, 2014).",2.2.2 Graph Construction,[0],[0]
"Therefore, we choose to use k-nearest neighborbased approach as it has been successfully used in other study (Steinbach et al., 2000).
",2.2.2 Graph Construction,[0],[0]
"The nearest neighbor graph consists of n vertices and for each vertex, there is an edge set consisting of a subset of n instances, i.e., tweets in our training set.",2.2.2 Graph Construction,[0],[0]
"The edge is defined by the distance measure d(i, j) between tweets ti and tj , where the value of d represents how similar the two tweets are.",2.2.2 Graph Construction,[0],[0]
"We used k-d tree data structure (Bentley, 1975) to efficiently find the nearest instances.",2.2.2 Graph Construction,[0],[0]
"To construct the graph, we first represent each tweet by averaging the word2vec vectors of its words, and then we measure d(i, j) by computing the Euclidean distance between the vectors.",2.2.2 Graph Construction,[0],[0]
The number of nearest neighbor k was set to 10.,2.2.2 Graph Construction,[0],[0]
The reason of averaging the word vectors is that it is computationally simpler and it captures the relevant semantic information for our task in hand.,2.2.2 Graph Construction,[0],[0]
"Likewise, we choose to use Euclidean distance instead of cosine for computational efficiency.",2.2.2 Graph Construction,[0],[0]
The network described so far can learn abstract features through convolutional and dense layers that are discriminative for the classification task (relevant vs. non-relevant).,2.3 Domain Adversarial Component,[0],[0]
"The supervised branch of the network uses labels in the source event to induce label information directly, whereas the semi-supervised branch induces similarity information between labeled and unlabeled instances.",2.3 Domain Adversarial Component,[0],[0]
"However, our goal is also to make these learned features invariant across domains or events (e.g., Nepal Earthquake vs. Queensland Flood).",2.3 Domain Adversarial Component,[0],[0]
"We achieve this by domain adversarial training of
neural networks (Ganin et al., 2016).",2.3 Domain Adversarial Component,[0],[0]
"We put a domain discriminator, another branch in the network (shown at the bottom in Figure 1) that takes the shared internal representation z as input, and tries to discriminate between the domains of the input — in our case, whether the input tweet is from DS or from DT .",2.3 Domain Adversarial Component,[0],[0]
"The domain discriminator is defined by a sigmoid function:
δ̂ = p(d = 1|t,Λ,Ψ) = sigm(wTd",2.3 Domain Adversarial Component,[0],[0]
"zd) (10)
where d ∈ {0, 1} denotes the domain of the input tweet t, wd are the final layer weights of the discriminator, and zd = f(Vdz) defines the hidden layer of the discriminator with layer weights Vd.",2.3 Domain Adversarial Component,[0],[0]
"Here Λ = {U, V } defines the shared parameters, and Ψ = {Vd,wd} defines the parameters specific to the domain discriminator.",2.3 Domain Adversarial Component,[0],[0]
"We use the negative log-probability as the discrimination loss:
Ji(Λ,Ψ) = −di log δ̂ − (1− di) log ( 1− δ̂ ) (11)
",2.3 Domain Adversarial Component,[0],[0]
"We can write the overall domain adversary loss over the source and target domains as
LD(Λ,Ψ) =",2.3 Domain Adversarial Component,[0],[0]
"− 1
Ls + Us Ls+Us∑ i=1",2.3 Domain Adversarial Component,[0],[0]
"Ji(Λ,Ψ)− 1 Ut Ut∑ i=1",2.3 Domain Adversarial Component,[0],[0]
"Ji(Λ,Ψ) (12)
where Ls + Us and Ut are the number of training instances in the source and target domains, respectively.",2.3 Domain Adversarial Component,[0],[0]
"In adversarial training, we seek parameters (saddle point) such that
θ∗ = argmin Λ,Φ,Ω max Ψ L(Λ,Φ,Ω,Ψ) (13)
which involves a maximization with respect to Ψ and a minimization with respect to {Λ,Φ,Ω}.",2.3 Domain Adversarial Component,[0],[0]
"In other words, the updates of the shared parameters Λ = {U, V } for the discriminator work adversarially to the rest of the network, and vice versa.",2.3 Domain Adversarial Component,[0],[0]
"This is achieved by reversing the gradients of the discrimination loss LD(Λ,Ψ), when they are backpropagated to the shared layers (see Figure 1).",2.3 Domain Adversarial Component,[0],[0]
Algorithm 1 illustrates the training algorithm based on stochastic gradient descent (SGD).,2.4 Model Training,[0],[0]
We first initialize the model parameters.,2.4 Model Training,[0],[0]
"The word embedding matrixE is initialized with pre-trained word2vec vectors (see Subsection 2.5) and is kept fixed during training.3 Other parameters are initialized with small random numbers sampled from
3Tuning E on our task by backpropagation increased the training time immensely (3 days compared to 5 hours on a Tesla GPU) without any significant performance gain.
",2.4 Model Training,[0],[0]
"Algorithm 1: Model Training with SGD Input : data DlS , DuS , DuT ; graph G Output: learned parameters θ = {Λ,Φ} 1.",2.4 Model Training,[0],[0]
"Initialize model parameters {E,Λ,Φ,Ω,Ψ}; 2. repeat
//",2.4 Model Training,[0],[0]
"Semi-supervised for each batch sampled from p(j, γ|i,DlS , G) do a) Compute loss LG(Λ,Ω) b)",2.4 Model Training,[0],[0]
"Take a gradient step for LG(Λ,Ω); end //",2.4 Model Training,[0],[0]
"Supervised & domain adversary for each batch sampled from DlS do a) Compute LC(Λ,Φ) and LD(Λ,Ψ) b)",2.4 Model Training,[0],[0]
"Take gradient steps for LC(Λ,Φ) and LD(Λ,Ψ); end // Domain adversary for each batch sampled from DuS and DuT do
a) Compute LD(Λ,Ψ) b) Take a gradient step for LD(Λ,Ψ);
end until convergence;
a uniform distribution (Bengio and Glorot, 2010).",2.4 Model Training,[0],[0]
"We use AdaDelta (Zeiler, 2012) adaptive update to update the parameters.
",2.4 Model Training,[0],[0]
"In each iteration, we do three kinds of gradient updates to account for the three different loss components.",2.4 Model Training,[0],[0]
"First, we do an epoch over all the training instances updating the parameters for the semi-supervised loss, then we do an epoch over the labeled instances in the source domain, each time updating the parameters for the supervised and the domain adversary losses.",2.4 Model Training,[0],[0]
"Finally, we do an epoch over the unlabeled instances in the two domains to account for the domain adversary loss.
",2.4 Model Training,[0],[0]
The main challenge in adversarial training is to balance the competing components of the network.,2.4 Model Training,[0],[0]
"If one component becomes smarter than the other, its loss to the shared layer becomes useless, and the training fails to converge (Arjovsky et al., 2017).",2.4 Model Training,[0],[0]
"Equivalently, if one component becomes weaker, its loss overwhelms that of the other, causing the training to fail.",2.4 Model Training,[0],[0]
"In our experiments, we observed the domain discriminator is weaker than the rest of the network.",2.4 Model Training,[0],[0]
"This could be due to the noisy nature of tweets, which makes the job for the domain discriminator harder.",2.4 Model Training,[0],[0]
"To balance the components, we would want the error signals from the discriminator to be fairly weak, also we would want the supervised loss to have more impact than the semi-supervised loss.",2.4 Model Training,[0],[0]
"In our experiments, the weight of the domain adversary loss λd was fixed to 1e",2.4 Model Training,[0],[0]
"− 8, and the weight of the semi-supervised loss λg was fixed to 1e",2.4 Model Training,[0],[0]
− 2.,2.4 Model Training,[0],[0]
"Other sophisticated weighting schemes have been proposed recently
(Ganin et al., 2016; Arjovsky et al., 2017; Metz et al., 2016).",2.4 Model Training,[0],[0]
"It would be interesting to see how our model performs using these advanced tuning methods, which we leave as a future work.",2.4 Model Training,[0],[0]
"As mentioned, we used word embeddings that are pre-trained on a crisis dataset.",2.5 Crisis Word Embedding,[0],[0]
"To train the wordembedding model, we first pre-processed tweets collected using the AIDR system (Imran et al., 2014) during different events occurred between 2014 and 2016.",2.5 Crisis Word Embedding,[0],[0]
"In the preprocessing step, we lowercased the tweets and removed URLs, digit, time patterns, special characters, single character, username started with the @ symbol.",2.5 Crisis Word Embedding,[0],[0]
"After preprocessing, the resulting dataset contains about 364 million tweets and about 3 billion words.
",2.5 Crisis Word Embedding,[0],[0]
"There are several approaches to train word embeddings such as continuous bag-of-words (CBOW) and skip-gram models of wrod2vec (Mikolov et al., 2013), and Glove (Pennington et al., 2014).",2.5 Crisis Word Embedding,[0],[0]
"For our work, we trained the CBOW model from word2vec.",2.5 Crisis Word Embedding,[0],[0]
"While training CBOW, we filtered out words with a frequency less than or equal to 5, and we used a context window size of 5 and k = 5 negative samples.",2.5 Crisis Word Embedding,[0],[0]
The resulting embedding model contains about 2 million words with vector dimensions of 300.,2.5 Crisis Word Embedding,[0],[0]
"In this section, we describe our experimental settings – datasets used, settings of our models, compared baselines, and evaluation metrics.",3 Experimental Settings,[0],[0]
"To conduct the experiment and evaluate our system, we used two real-world Twitter datasets collected during the 2015 Nepal earthquake (NEQ) and the 2013 Queensland floods (QFL).",3.1 Datasets,[0],[0]
"These datasets are comprised of millions of tweets collected through the Twitter streaming API4 using event-specific keywords/hashtags.
",3.1 Datasets,[0],[0]
To obtain the labeled examples for our task we employed paid workers from the Crowdflower5 – a crowdsourcing platform.,3.1 Datasets,[0],[0]
The annotation consists of two classes relevant and non-relevant.,3.1 Datasets,[0],[0]
"For the annotation, we randomly sampled 11,670 and 10,033 tweets from the Nepal earthquake and the Queensland floods datasets, respectively.",3.1 Datasets,[0],[0]
"Given a
4https://dev.twitter.com/streaming/overview 5http://crowdflower.com
tweet, we asked crowdsourcing workers to assign the “relevant” label if the tweet conveys/reports information useful for crisis response such as a report of injured or dead people, some kind of infrastructure damage, urgent needs of affected people, donations requests or offers, otherwise assign the “non-relevant” label.",3.1 Datasets,[0],[0]
"We split the labeled data into 60% as training, 30% as test and 10% as development.",3.1 Datasets,[0],[0]
Table 1 shows the resulting datasets with class-wise distributions.,3.1 Datasets,[0],[0]
Data preprocessing was performed by following the same steps used to train the word2vec model (Subsection 2.5).,3.1 Datasets,[0],[0]
"In all the experiments, the classification task consists of two classes: relevant and non-relevant.",3.1 Datasets,[0],[0]
"In order to demonstrate the effectiveness of our joint learning approach, we performed a series of experiments.",3.2 Model Settings and Baselines,[0],[0]
"To understand the contribution of different network components, we performed an ablation study showing how the model performs as a semi-supervised model alone and as a domain adaptation model alone, and then we compare them with the combined model that incorporates all the components.",3.2 Model Settings and Baselines,[0],[0]
"As a baseline for the semi-supervised experiments, we used the self-training approach (Scudder, 1965).",3.2.1 Settings for Semi-supervised Learning,[0],[0]
"For this purpose, we first trained a supervised model using the CNN architecture (i.e., shared components followed by the supervised part in Figure 1).",3.2.1 Settings for Semi-supervised Learning,[0],[0]
The trained model was then used to automatically label the unlabeled data.,3.2.1 Settings for Semi-supervised Learning,[0],[0]
"Instances with a classifier confidence score ≥ 0.75 were then used to retrain a new model.
",3.2.1 Settings for Semi-supervised Learning,[0],[0]
"Next, we run experiments using our graphbased semi-supervised approach (i.e., shared components followed by the supervised and semisupervised parts in Figure 1), which exploits unlabeled data.",3.2.1 Settings for Semi-supervised Learning,[0],[0]
"For reducing the computational cost, we randomly selected 50K unlabeled instances from the same domain.",3.2.1 Settings for Semi-supervised Learning,[0],[0]
"For our semi-supervised setting, one of the main goals was to understand how much labeled data is sufficient to obtain a
reasonable result.",3.2.1 Settings for Semi-supervised Learning,[0],[0]
"Therefore, we experimented our system by incrementally adding batches of instances, such as 100, 500, 2000, 5000, and all instances from the training set.",3.2.1 Settings for Semi-supervised Learning,[0],[0]
Such an understanding can help us design the model at the onset of a crisis event with sufficient amount of labeled data.,3.2.1 Settings for Semi-supervised Learning,[0],[0]
"To demonstrate that the semi-supervised approach outperforms the supervised baseline, we run supervised experiments using the same number of labeled instances.",3.2.1 Settings for Semi-supervised Learning,[0],[0]
"In the supervised setting, only zc activations in Figure 1 are used for classification.",3.2.1 Settings for Semi-supervised Learning,[0],[0]
"To set a baseline for the domain adaptation experiments, we train a CNN model (i.e., shared components followed by the supervised part in Figure 1) on one event (source) and test it on another event (target).",3.2.2 Settings for Domain Adaptation,[0],[0]
"We call this as transfer baseline.
",3.2.2 Settings for Domain Adaptation,[0],[0]
"To assess the performance of our domain adaptation technique alone, we exclude the semisupervised component from the network.",3.2.2 Settings for Domain Adaptation,[0],[0]
"We train and evaluate models with this network configuration using different source and target domains.
",3.2.2 Settings for Domain Adaptation,[0],[0]
"Finally, we integrate all the components of the network as shown in Figure 1 and run domain adaptation experiments using different source and target domains.",3.2.2 Settings for Domain Adaptation,[0],[0]
"In all our domain adaptation experiments, we only use unlabeled instances from the target domain.",3.2.2 Settings for Domain Adaptation,[0],[0]
"In domain adaption literature, this is known as unsupervised adaptation.",3.2.2 Settings for Domain Adaptation,[0],[0]
"We use 100, 150, and 200 filters each having the window size of 2, 3, and 4, respectively, and pooling length of 2, 3, and 4, respectively.",3.2.3 Training Settings,[0],[0]
We do not tune these hyperparameters in any experimental setting since the goal was to have an end-to-end comparison with the same hyperparameter setting and understand whether our approach can outperform the baselines or not.,3.2.3 Training Settings,[0],[0]
"Furthermore, we do not filter out any vocabulary item in any settings.
",3.2.3 Training Settings,[0],[0]
"As mentioned before in Subsection 2.4, we used AdaDelta (Zeiler, 2012) to update the model parameters in each SGD step.",3.2.3 Training Settings,[0],[0]
The learning rate was set to 0.1 when optimizing on the classification loss and to 0.001 when optimizing on the semisupervised loss.,3.2.3 Training Settings,[0],[0]
The learning rate for domain adversarial training was set to 1.0.,3.2.3 Training Settings,[0],[0]
"The maximum number of epochs was set to 200, and dropout rate of 0.02 was used to avoid overfitting (Srivastava et al., 2014).",3.2.3 Training Settings,[0],[0]
"We used validation-based early stopping using the F-measure with a patience of 25,
Experiments AUC P R F1
NEPAL EARTHQUAKE
Supervised 61.22 62.42 62.31 60.89
Semi-supervised (Self-training) 61.15 61.53 61.53 61.26
Semi-supervised (Graph-based) 64.81 64.58 64.63 65.11
QUEENSLAND FLOODS
i.e., we stop training if the score does not increase for 25 consecutive epochs.",3.2.3 Training Settings,[0],[0]
"To measure the performance of the trained models using different approaches described above, we use weighted average precision, recall, F-measure, and Area Under ROC-Curve (AUC), which are standard evaluation measures in the NLP and machine learning communities.",3.2.4 Evaluation Metrics,[0],[0]
The rationale behind choosing the weighted metric is that it takes into account the class imbalance problem.,3.2.4 Evaluation Metrics,[0],[0]
"In this section, we present the experimental results and discuss our main findings.",4 Results and Discussion,[0],[0]
"In Table 2, we present the results obtained from the supervised, self-training based semi-supervised, and our graph-based semi-supervised experiments for the both datasets.",4.1 Semi-supervised Learning,[0],[0]
It can be clearly observed that the graph-based semi-supervised approach outperforms the two baselines – supervised and self-training based semi-supervised.,4.1 Semi-supervised Learning,[0],[0]
"Specifically, the graph-based approach shows 4% to 13% absolute improvements in terms of F1 scores for the Nepal and Queensland datasets, respectively.
To determine how the semi-supervised approach performs in the early hours of an event when only fewer labeled instances are available, we mimic a batch-wise (not to be confused with minibatch in SGD) learning setting.",4.1 Semi-supervised Learning,[0],[0]
"In Table 3, we present the results using different batch sizes – 100, 500, 1,000, 2,000, and all labels.
",4.1 Semi-supervised Learning,[0],[0]
"From the results, we observe that models’ performance improve as we include more labeled data
Exp.",4.1 Semi-supervised Learning,[0],[0]
100 500 1000 2000,4.1 Semi-supervised Learning,[0],[0]
"All L
NEPAL EARTHQUAKE
L 43.63 52.89 56.37 60.11 60.89
L+50kU 52.32 59.95 61.89 64.05 65.11
QUEENSLAND FLOOD
— from 43.63 to 60.89 for NEQ and from 48.97 to 80.16 for QFL in the case of labeled only (L).",4.1 Semi-supervised Learning,[0],[0]
"When we compare supervised vs. semi-supervised (L vs. L+U), we observe significant improvements in F1 scores for the semi-supervised model for all batches over the two datasets.",4.1 Semi-supervised Learning,[0],[0]
"As we include unlabeled instances with labeled instances from the same event, performance significantly improves in each experimental setting giving 5% to 26% absolute improvements over the supervised models.",4.1 Semi-supervised Learning,[0],[0]
These improvements demonstrate the effectiveness of our approach.,4.1 Semi-supervised Learning,[0],[0]
We also notice that our semi-supervised approach can perform above 90% depending on the event.,4.1 Semi-supervised Learning,[0],[0]
"Specifically, major improvements are observed from batch size 100 to 1,000, however, after that the performance improvements are comparatively minor.",4.1 Semi-supervised Learning,[0],[0]
"The results obtained using batch sizes 500 and 1,000 are reasonably in the acceptable range when labeled and unlabeled instances are combined (i.e., L+50kU for Nepal and L+∼21kU for Queensland), which is also a reasonable number of training examples to obtain at the onset of an event.",4.1 Semi-supervised Learning,[0],[0]
"In Table 4, we present domain adaptation results.",4.2 Domain Adaptation,[0],[0]
"The first block shows event-specific (i.e., train and test on the same event)",4.2 Domain Adaptation,[0],[0]
results for the supervised CNN model.,4.2 Domain Adaptation,[0],[0]
These results set the upper bound for our domain adaptation methods.,4.2 Domain Adaptation,[0],[0]
"The transfer baselines are shown in the next block, where we train a CNN model in one domain and test it on a different domain.",4.2 Domain Adaptation,[0],[0]
"Then, the third block shows the results for the domain adversarial approach without the semi-supervised loss.",4.2 Domain Adaptation,[0],[0]
These results show the importance of domain adversarial component.,4.2 Domain Adaptation,[0],[0]
"After that, the fourth block presents the performance of the model trained with graph
Source Target AUC P R F1
",4.2 Domain Adaptation,[0],[0]
"IN-DOMAIN SUPERVISED MODEL
embedding without domain adaptation to show the importance of semi-supervised learning.",4.2 Domain Adaptation,[0],[0]
"The final block present the results for the complete model that includes all the loss components.
",4.2 Domain Adaptation,[0],[0]
The results with domain adversarial training show improvements across both events – from 1.8% to 4.1% absolute gains in F1.,4.2 Domain Adaptation,[0],[0]
"These results attest that adversarial training is an effective approach to induce domain invariant features in the internal representation as shown previously by Ganin et al. (2016).
",4.2 Domain Adaptation,[0],[0]
"Finally, when we do both semi-supervised learning and unsupervised domain adaptation, we get further improvements in F1 scores ranging from 5% to 7% absolute gains.",4.2 Domain Adaptation,[0],[0]
"From these improvements, we can conclude that domain adaptation with adversarial training along with graphbased semi-supervised learning is an effective method to leverage unlabeled and labeled data from a different domain.
",4.2 Domain Adaptation,[0],[0]
"Note that for our domain adaptation methods, we only use unlabeled data from the target domain.",4.2 Domain Adaptation,[0],[0]
"Hence, we foresee future improvements of this approach by utilizing a small amount of target domain labeled data.",4.2 Domain Adaptation,[0],[0]
Two lines of research are directly related to our work: (i) semi-supervised learning and (ii) domain adaptation.,5 Related Work,[0],[0]
Several models have been proposed for semi-supervised learning.,5 Related Work,[0],[0]
"The earliest approach is self-training (Scudder, 1965), in
which a trained model is first used to label unlabeled data instances followed by the model retraining with the most confident predicted labeled instances.",5 Related Work,[0],[0]
"The co-training (Mitchell, 1999) approach assumes that features can be split into two sets and each subset is then used to train a classifier with an assumption that the two sets are conditionally independent.",5 Related Work,[0],[0]
"Then each classifier classifies the unlabeled data, and then most confident data instances are used to re-train the other classifier, this process repeats multiple times.
",5 Related Work,[0],[0]
"In the graph-based semi-supervised approach, nodes in a graph represent labeled and unlabeled instances and edge weights represent the similarity between them.",5 Related Work,[0],[0]
"The structural information encoded in the graph is then used to regularize a model (Zhu, 2005).",5 Related Work,[0],[0]
"There are two paradigms in semi-supervised learning: 1) inductive – learning a function with which predictions can be made on unobserved instances, 2) transductive – no explicit function is learned and predictions can only be made on observed instances.",5 Related Work,[0],[0]
"As mentioned before, inductive semi-supervised learning is preferable over the transductive approach since it avoids building the graph each time it needs to infer the labels for the unlabeled instances.
",5 Related Work,[0],[0]
"In our work, we use a graph-based inductive deep learning approach proposed by Yang et al. (2016) to learn features in a deep learning model by predicting contextual (i.e., neighboring) nodes in the graph.",5 Related Work,[0],[0]
"However, our approach is different from Yang et al. (2016) in several ways.",5 Related Work,[0],[0]
"First, we construct the graph by computing the distance between tweets based on word embeddings.",5 Related Work,[0],[0]
"Second, instead of using count-based features, we use a convolutional neural network (CNN) to compose high-level features from the distributed representation of the words in a tweet.",5 Related Work,[0],[0]
"Finally, for context prediction, instead of performing a random walk, we select nodes based on their similarity in the graph.",5 Related Work,[0],[0]
"Similar similarity-based graph has shown impressive results in learning sentence representations (Saha et al., 2017).
",5 Related Work,[0],[0]
"In the literature, the proposed approaches for domain adaptation include supervised, semisupervised and unsupervised.",5 Related Work,[0],[0]
"It also varies from linear kernelized approach (Blitzer et al., 2006) to non-linear deep neural network techniques (Glorot et al., 2011; Ganin et al., 2016).",5 Related Work,[0],[0]
"One direction of research is to focus on feature space distribution matching by reweighting the samples from
the source domain (Gong et al., 2013) to map source into target.",5 Related Work,[0],[0]
The overall idea is to learn a good feature representation that is invariant across domains.,5 Related Work,[0],[0]
"In the deep learning paradigm, Glorot et al. (Glorot et al., 2011) used Stacked Denoising Auto-Encoders (SDAs) for domain adaptation.",5 Related Work,[0],[0]
"SDAs learn a robust feature representation, which is artificially corrupted with small Gaussian noise.",5 Related Work,[0],[0]
"Adversarial training of neural networks has shown big impact recently, especially in areas such as computer vision, where generative unsupervised models have proved capable of synthesizing new images (Goodfellow et al., 2014; Radford et al., 2015; Makhzani et al., 2015).",5 Related Work,[0],[0]
"Ganin et al. (2016) proposed domain adversarial neural networks (DANN) to learn discriminative but at the same time domain-invariant representations, with domain adaptation as a target.",5 Related Work,[0],[0]
"We extend this work by combining with semi-supervised graph embedding for unsupervised domain adaptation.
",5 Related Work,[0],[0]
"In a recent work, Kipf and Welling (2016) present CNN applied directly on graph-structured datasets - citation networks and on a knowledge graph dataset.",5 Related Work,[0],[0]
Their study demonstrate that graph convolution network for semi-supervised classification performs better compared to other graph based approaches.,5 Related Work,[0],[0]
"In this paper, we presented a deep learning framework that performs domain adaptation with adversarial training and graph-based semi-supervised learning to leverage labeled and unlabeled data from related events.",6 Conclusions,[0],[0]
"We use a convolutional neural network to compose high-level representation from the input, which is then passed to three components that perform supervised training, semisupervised learning and domain adversarial training.",6 Conclusions,[0],[0]
"For domain adaptation, we considered a scenario, where we have only unlabeled data in the target event.",6 Conclusions,[0],[0]
"Our evaluation on two crisis-related tweet datasets demonstrates that by combining domain adversarial training with semi-supervised learning, our model gives significant improvements over their respective baselines.",6 Conclusions,[0],[0]
We have also presented results of batch-wise incremental training of the graph-based semi-supervised approach and show approximation regarding the number of labeled examples required to get an acceptable performance at the onset of an event.,6 Conclusions,[0],[0]
The success of deep neural networks (DNNs) is heavily dependent on the availability of labeled data.,abstractText,[0],[0]
"However, obtaining labeled data is a big challenge in many real-world problems.",abstractText,[0],[0]
"In such scenarios, a DNN model can leverage labeled and unlabeled data from a related domain, but it has to deal with the shift in data distributions between the source and the target domains.",abstractText,[0],[0]
"In this paper, we study the problem of classifying social media posts during a crisis event (e.g., Earthquake).",abstractText,[0],[0]
"For that, we use labeled and unlabeled data from past similar events (e.g., Flood) and unlabeled data for the current event.",abstractText,[0],[0]
We propose a novel model that performs adversarial learning based domain adaptation to deal with distribution drifts and graph based semi-supervised learning to leverage unlabeled data within a single unified deep learning framework.,abstractText,[0],[0]
Our experiments with two real-world crisis datasets collected from Twitter demonstrate significant improvements over several baselines.,abstractText,[0],[0]
Domain Adaptation with Adversarial Training and Graph Embeddings,title,[0],[0]
√ ε rather than the eigengap.,text,[0],[0]
"The Generalized Eigenvector (GenEV) problem and the Canonical Correlation Analysis (CCA) are two fundamental problems in scientific computing, machine learning, operations research, and statistics.",1 Introduction,[0],[0]
"Algorithms solving these problems are often used to extract features to compare large-scale datasets, as well as used for problems in regression (Kakade & Foster, 2007), clustering (Chaudhuri et al., 2009), classification (Karampatziakis & Mineiro, 2014), word embeddings (Dhillon et al., 2011), and many others.
",1 Introduction,[0],[0]
GenEV.,1 Introduction,[0],[0]
"Given two symmetric matrices A,B ∈ Rd×d whereB is positive definite.",1 Introduction,[0],[0]
"The GenEV problem is to find generalized eigenvectors v1, . . .",1 Introduction,[0],[0]
", vd where each vi satisfies
vi ∈ arg max v∈Rd ∣∣v>Av∣∣ s.t. { v>Bv = 1 v>Bvj = 0 ∀j ∈",1 Introduction,[0],[0]
"[i− 1]
The values λi def = v>",1 Introduction,[0],[0]
"i Avi are known as the generalized eigenvalues, and it satisfies |λ1| ≥ · · · |λd|.",1 Introduction,[0],[0]
Following the *Equal contribution .,1 Introduction,[0],[0]
Future version of this paper shall be found at http://arxiv.org/abs/1607.06017.,1 Introduction,[0],[0]
1Microsoft Research 2Princeton University.,1 Introduction,[0],[0]
"Correspondence to: Zeyuan Allen-Zhu <zeyuan@csail.mit.edu>, Yuanzhi Li <yuanzhil@cs.princeton.edu>.
",1 Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1 Introduction,[0],[0]
"Copyright 2017 by the author(s).
tradition of (Wang et al., 2016; Garber & Hazan, 2015), we
assume without loss of generality that λi ∈",1 Introduction,[0],[0]
"[−1, 1].
CCA.",1 Introduction,[0],[0]
"Given matrices X ∈ Rn×dx , Y ∈ Rn×dy and denoting by Sxx = 1nX >X , Sxy = 1nX >Y , Syy = 1nY
>Y , the CCA problem is to find canonical-correlation vectors {(φi, ψi)}ri=1",1 Introduction,[0],[0]
"where r = min{dx, dy} and each pair (φi, ψi) ∈ arg max
φ∈Rdx ,ψ∈Rdy
{ φ>Sxyψ } such that { φ>Sxxφ = 1 ∧ φ>Sxxφj = 0 ∀j ∈",1 Introduction,[0],[0]
[i− 1] ψ>Syyψ = 1 ∧ ψ>Syyψj = 0 ∀j ∈,1 Introduction,[0],[0]
"[i− 1]
The values σi def = φ>i",1 Introduction,[0],[0]
"Sxyψi ≥ 0 are known as the canonical-correlation coefficients, and
1 ≥ σ1 ≥ · · · ≥ σr ≥ 0 is always satisfied.
",1 Introduction,[0],[0]
"It is a fact that solving CCA exactly can be reduced to solving GenEV exactly, if one defines B = diag{Sxx, Syy} ∈ Rd×d andA =",1 Introduction,[0],[0]
"[[0, Sxy]; [S>xy, 0]] ∈",1 Introduction,[0],[0]
Rd×d for d def = dx+dy; see Lemma 2.3.,1 Introduction,[0],[0]
"(This reduction does not always hold if the generalized eigenvectors are computed only approximately.)
",1 Introduction,[0],[0]
"Despite the fundamental importance and the frequent necessity in applications, there are few results on obtaining provably efficient algorithms for GenEV and CCA until very recently.",1 Introduction,[0],[0]
"In the breakthrough result of Ma, Lu and Foster (Ma et al., 2015), they proposed to study algorithms to find top k generalized eigenvectors (k-GenEV) or top k canonical-correlation vectors (k-CCA).",1 Introduction,[0],[0]
They designed an alternating minimization algorithm whose running time is only linear in the input matrix sparsity and nearly-linear in k.,1 Introduction,[0],[0]
"Such algorithms are very appealing because in real-life applications, it is often only relevant to obtain top correlation vectors, as opposed to the less meaningful vectors in the directions where the datasets do not correlate.",1 Introduction,[0],[0]
"Unfortunately, the method of Ma, Lu and Foster has a running time that linearly scales with κ and 1/gap, where
• κ ≥ 1 is the condition number of matrix B in GenEV, or of matrices X>X,Y >Y in CCA; and • gap ∈",1 Introduction,[0],[0]
"[0, 1) is the eigengap λk−λk+1λk in GenEV, or σk−σk+1
σk in CCA.
",1 Introduction,[0],[0]
"These parameters are usually not constants and scale with
the problem size.
",1 Introduction,[0],[0]
"Challenge 1: Acceleration For many easier scientific computing problems, we are able to design algorithms that have accelerated dependencies on κ and 1/gap.",1 Introduction,[0],[0]
"As two concrete examples, k-PCA can be solved with a running time linearly in 1/ √ gap as opposed to 1/gap (Golub & Van Loan, 2012); computing B−1w for a vector w can be solved in time linearly in √ κ as opposed to κ, where κ is the condition number of matrix B (Shewchuk, 1994; Axelsson, 1985; Nesterov, 1983).
",1 Introduction,[0],[0]
"Therefore, can we obtain doubly-accelerated methods for k-GenEV and k-CCA, meaning that the running times linearly scale with both √ κ and 1/ √ gap?",1 Introduction,[0],[0]
"Before this paper, for the general case k > 1, the method of Ge et al. (Ge et al., 2016) made acceleration possible for parameter κ, but not for parameter 1/gap (see Table 1).
",1 Introduction,[0],[0]
"Challenge 2: Gap-Freeness Since gap can be even zero in the extreme case, can we design algorithms that do not scale with 1/gap?",1 Introduction,[0],[0]
Recall that this is possible for the easier task of k-PCA.,1 Introduction,[0],[0]
"The block Krylov method (Musco & Musco, 2015) runs in time linear in 1/ √ ε as opposed to 1/ √ gap, where ε is the approximation ratio.",1 Introduction,[0],[0]
"There is no gap-free result previously known for k-GenEV or k-CCA even for k = 1.
",1 Introduction,[0],[0]
"Challenge 3: Stochasticity For matrix-related problems, one can usually obtain stochastic running times which requires some notations to describe.
",1 Introduction,[0],[0]
"Consider a simple task of computing B−1w for some vector w, where accelerated methods solve it in time linearly in √ κ for κ being the condition number of B. If B = 1 nX >X is given in the form of a covariance matrix where X ∈ Rn×d, then (accelerated) stochastic methods compute B−1w in a time linearly in (1 + √ κ′/n) instead of √ κ, where κ′ = maxi∈[n]{‖Xi‖ 2} λmin(B) ∈",1 Introduction,[0],[0]
"[ κ, nκ ] and Xi is the i-th
row of X .",1 Introduction,[0],[0]
(See Lemma 2.6.),1 Introduction,[0],[0]
"Since 1 + √ κ′/n ≤ O( √ κ), stochastic methods are no slower than non-stochastic ones.
",1 Introduction,[0],[0]
"So, can we obtain a similar but doubly-accelerated stochastic method for k-CCA?1 Note that, if the doublyaccelerated requirement is dropped, this task is easier and indeed possible, see Ge et al. (Ge et al., 2016).",1 Introduction,[0],[0]
"However, since their stochastic method is not doubly-accelerated, in certain parameter regimes, it runs even slower than nonstochastic ones (even for k = 1, see Table 2).
",1 Introduction,[0],[0]
Remark.,1 Introduction,[0],[0]
"In general, if designed properly, for worst case running time:
• Accelerated results are usually better because they are 1 Note that a similar problem can be also asked for k-GenEV when A and B are both given in their covariance matrix forms.",1 Introduction,[0],[0]
"We refrain from doing it in this paper for notational simplicity.
",1 Introduction,[0],[0]
"no slower than non-accelerated ones in the worst-case.
• Gap-free results are better because they imply gapdependent ones.2
• Stochastic results are usually better because they are no slower than non-stochastic ones in the worst-case.",1 Introduction,[0],[0]
"We provide algorithms LazyEV and LazyCCA that are doubly-accelerated, gap-free, and stochastic.3
For the general k-GenEV problem, our LazyEV can be implemented to run in time4
Õ (knnz(B)√κ
√ gap
+ knnz(A) + k2d",1.1 Our Main Results,[0],[0]
"√ gap
) or
Õ (knnz(B)√κ√
ε + knnz(A)",1.1 Our Main Results,[0],[0]
+,1.1 Our Main Results,[0],[0]
k2d√ ε ) in the gap-dependent and gap-free cases respectively.,1.1 Our Main Results,[0],[0]
Since our running time only linearly depends on √ κ and √ gap (resp.,1.1 Our Main Results,[0],[0]
√ ε),1.1 Our Main Results,[0],[0]
", our algorithm LazyEV is doubly-accelerated.
",1.1 Our Main Results,[0],[0]
"For the general k-CCA problem, our LazyCCA can be implemented to run in time
Õ (knnz(X,Y ) · (1 +√κ′/n)+ k2d
√ gap
) or
Õ (knnz(X,Y ) · (1 +√κ′/n)+ k2d
√ ε ) in the gap-dependent and gap-free cases respectively.",1.1 Our Main Results,[0],[0]
"Here, nnz(X,Y ) = nnz(X) + nnz(Y ) and κ′ = 2 maxi{‖Xi‖2,‖Yi‖2} λmin(diag{Sxx,Syy}) where Xi or Yi is the i-th row vector of X or Y .",1.1 Our Main Results,[0],[0]
"Therefore, our algorithm LazyCCA is doublyaccelerated and stochastic.
",1.1 Our Main Results,[0],[0]
"We fully compare our results with prior work in Table 2 (for k = 1) and Table 1 (for k ≥ 1), and summarize our main contributions:
• For k > 1, we outperform all relevant prior works (see Table 1).",1.1 Our Main Results,[0],[0]
"Moreover, no known method was doublyaccelerated even in the non-stochastic setting.
",1.1 Our Main Results,[0],[0]
"• For k ≥ 1, we obtain the first gap-free running time.",1.1 Our Main Results,[0],[0]
•,1.1 Our Main Results,[0],[0]
"Even for k = 1, we outperform most of the state-of-
the-arts (see Table 2).
",1.1 Our Main Results,[0],[0]
"Note that for CCA with k > 1, previous result CCALin only outputs the subspace spanned by the top k correlation vectors but does not identify which vector gives the highest correlation and so on.",1.1 Our Main Results,[0],[0]
"Our LazyCCA provides per-vector
2If a method depends on 1/ε then one can choose ε = gap and this translates to a gap-dependent running time.
",1.1 Our Main Results,[0],[0]
"3Recalling Footnote 1, for notational simplicity, we only state our k-GenEV result in non-stochastic running time.
",1.1 Our Main Results,[0],[0]
"4Throughout the paper, we use the Õ notation to hide polylogarithmic factors with respect to κ, 1/gap, 1/ε, d, n.",1.1 Our Main Results,[0],[0]
"We use nnz(M) to denote the time needed to multiply M to a vector.
",1.1 Our Main Results,[0],[0]
"λk
∈",1.1 Our Main Results,[0],[0]
"[0, 1] and κB = λmax(B)λmin(B) > 1.
",1.1 Our Main Results,[0],[0]
"In CCA, gap = σk−σk+1 σk ∈",1.1 Our Main Results,[0],[0]
"[0, 1], κ = λmax(diag{Sxx,Syy}) λmin(diag{Sxx,Syy}) > 1, κ′ = 2maxi{‖Xi‖ 2,‖Yi‖2} λmin(diag{Sxx,Syy}) ∈",1.1 Our Main Results,[0],[0]
"[κ, 2nκ], and σk ∈",1.1 Our Main Results,[0],[0]
"[0, 1].
",1.1 Our Main Results,[0],[0]
Remark 1.,1.1 Our Main Results,[0],[0]
Stochastic methods depend on a modified condition number κ′. The reason κ′ ∈,1.1 Our Main Results,[0],[0]
"[κ, 2nκ] is in Fact 2.5.",1.1 Our Main Results,[0],[0]
Remark 2.,1.1 Our Main Results,[0],[0]
"All non-stochastic CCA methods in this table have been outperformed because 1 + √ κ′/n ≤ O(κ).
",1.1 Our Main Results,[0],[0]
Remark 3.,1.1 Our Main Results,[0],[0]
Doubly-stochastic methods are not necessarily interesting.,1.1 Our Main Results,[0],[0]
"We discuss them in Section 1.2.
",1.1 Our Main Results,[0],[0]
guarantees on all the top k correlation vectors.,1.1 Our Main Results,[0],[0]
"Recall that when considering acceleration, there are two parameters κ and 1/gap.",1.2 Our Side Results on Doubly-Stochastic Methods,[0],[0]
"One can also design stochastic methods with respect to both parameters κ and 1/gap, meaning that
with a running time proportional to 1 + √ κ′/nc √ gap
instead of 1+ √ κ′/n
√ gap (stochastic) or √ κ√ gap (non-stochastic).
",1.2 Our Side Results on Doubly-Stochastic Methods,[0],[0]
The constant c is usually 1/2.,1.2 Our Side Results on Doubly-Stochastic Methods,[0],[0]
"We call such methods doubly-stochastic.
",1.2 Our Side Results on Doubly-Stochastic Methods,[0],[0]
"Unfortunately, doubly-stochastic methods are usually slower than stochastic ones.",1.2 Our Side Results on Doubly-Stochastic Methods,[0],[0]
Take 1-CCA as an example.,1.2 Our Side Results on Doubly-Stochastic Methods,[0],[0]
"The best stochastic running time (obtained exclusively by
us) for 1-CCA is nnz(X,Y ) ·",1.2 Our Side Results on Doubly-Stochastic Methods,[0],[0]
Õ ( 1+√κ′/n √ gap ) .,1.2 Our Side Results on Doubly-Stochastic Methods,[0],[0]
"In contrast, if one uses a doubly-stochastic method —either (Wang et al., 2016) or our LazyCCA— the running time becomes nnz(X,Y ) ·",1.2 Our Side Results on Doubly-Stochastic Methods,[0],[0]
"Õ ( 1 +
√ κ′/n1/4√",1.2 Our Side Results on Doubly-Stochastic Methods,[0],[0]
"gap·σ1
) .",1.2 Our Side Results on Doubly-Stochastic Methods,[0],[0]
"Therefore, for 1-CCA,
doubly-stochastic methods are faster than stochastic ones
only when κ ′
σ1 ≤ o(n1/2) .
",1.2 Our Side Results on Doubly-Stochastic Methods,[0],[0]
The above condition is usually not satisfied.,1.2 Our Side Results on Doubly-Stochastic Methods,[0],[0]
"For instance,
• κ′ is usually around n for most interesting data-sets, cf.
the experiments of (Shalev-Shwartz & Zhang, 2014);
• κ′ is between n1/2 and 100n in all the CCA experiments of (Wang et al., 2016); and
• by Fact 2.5 it satisfies κ′ ≥ d so κ′ cannot be smaller than o(n1/2) unless d n1/2.5 Even worse, parameter σ1 ∈",1.2 Our Side Results on Doubly-Stochastic Methods,[0],[0]
"[0, 1] is usually much smaller than 1.",1.2 Our Side Results on Doubly-Stochastic Methods,[0],[0]
"Note that σ1 is scaling invariant: even if one scales X and Y up by the same factor, σ1 remains unchanged.
",1.2 Our Side Results on Doubly-Stochastic Methods,[0],[0]
"Nevertheless, to compare our LazyCCA with all relevant prior works, we obtain doubly-stochastic running times for k-CCA as well.",1.2 Our Side Results on Doubly-Stochastic Methods,[0],[0]
"Our running time matches that of (Wang et al., 2016) when k = 1, and no doubly-stochastic running time for k > 1 was known before our work.",1.2 Our Side Results on Doubly-Stochastic Methods,[0],[0]
"For the easier task of PCA and SVD, the first gap-free result was obtained by Musco and Musco (Musco & Musco, 2015), the first stochastic result was obtained by Shamir (Shamir, 2015), and the first accelerated stochastic result was obtained by Garber et al. (Garber & Hazan, 2015; Garber et al., 2016).",1.3 Other Related Works,[0],[0]
"The shift-and-invert preconditioning technique of Garber et al. is also used in this paper.
",1.3 Other Related Works,[0],[0]
"For another related problem PCR (principle compo-
5Note that item (3) κ′ ≥ d may not hold in the more general setting of CCA, see Remark A.1.
",1.3 Other Related Works,[0],[0]
"λ1
∈",1.3 Other Related Works,[0],[0]
"[0, 1] and κB = λmax(B)λmin(B) > 1.
",1.3 Other Related Works,[0],[0]
"In CCA, gap = σ1−σ2 σ1 ∈",1.3 Other Related Works,[0],[0]
"[0, 1], κ = λmax(diag{Sxx,Syy}) λmin(diag{Sxx,Syy}) > 1, κ′ = 2maxi{‖Xi‖ 2,‖Yi‖2} λmin(diag{Sxx,Syy}) ∈",1.3 Other Related Works,[0],[0]
"[κ, 2nκ], and σ1 ∈",1.3 Other Related Works,[0],[0]
"[0, 1].
",1.3 Other Related Works,[0],[0]
Remark 1.,1.3 Other Related Works,[0],[0]
Stochastic methods depend on modified condition number κ′; the reason κ′ ∈,1.3 Other Related Works,[0],[0]
"[κ, 2nκ] is in Def. 2.4.",1.3 Other Related Works,[0],[0]
Remark 2.,1.3 Other Related Works,[0],[0]
"All non-stochastic CCA methods in this table have been outperformed because 1 + √ κ′/n ≤ O(κ).
",1.3 Other Related Works,[0],[0]
Remark 3.,1.3 Other Related Works,[0],[0]
Doubly-stochastic methods are not necessarily interesting.,1.3 Other Related Works,[0],[0]
"We discuss them in Section 1.2.
",1.3 Other Related Works,[0],[0]
Remark 4.,1.3 Other Related Works,[0],[0]
Some CCA methods have a running time dependency on σ1 ∈,1.3 Other Related Works,[0],[0]
"[0, 1], and this is intrinsic and cannot be removed.",1.3 Other Related Works,[0],[0]
"In particular, if we scale the data matrix X and Y , the value σ1 stays the same.
",1.3 Other Related Works,[0],[0]
Remark 5.,1.3 Other Related Works,[0],[0]
"The only (non-doubly-stochastic) doubly-accelerated method before our work is SI (Wang et al., 2016) (for 1-CCA only).",1.3 Other Related Works,[0],[0]
Our LazyEV is faster than theirs by a factor Ω( √ nκ/κ′ × √ 1/σ1).,1.3 Other Related Works,[0],[0]
"Here, nκ/κ′ ≥ 1/2 and 1/σ1 ≥ 1 are two scaling-invariant quantities usually much greater than 1.
nent regression), we recently obtained an accelerated method (Allen-Zhu & Li, 2017) as opposed the previously non-accelerated one (Frostig et al., 2016); however, the acceleration techniques there are not relevant to this paper.
",1.3 Other Related Works,[0],[0]
"For GenEV and CCA, many scalable algorithms have been designed recently (Ma et al., 2015; Wang & Livescu, 2015; Michaeli et al., 2015; Witten et al., 2009; Lu & Foster, 2014).",1.3 Other Related Works,[0],[0]
"However, as summarized by the authors of CCALin, these cited methods are more or less heuristics and do not have provable guarantees.",1.3 Other Related Works,[0],[0]
"Furthermore, for k > 1, the AppGrad method (Ma et al., 2015) only provides local convergence guarantees and thus requires a warm-start whose
computational complexity is not discussed in their paper.
",1.3 Other Related Works,[0],[0]
"Finally, our algorithms on GenEV and CCA are based on finding vectors one-by-one, which is advantageous in practice because one does not need k to be known and can stop the algorithm whenever the eigenvalues (or correlation values) are too small.",1.3 Other Related Works,[0],[0]
"Known approaches for k > 1 cases (such as GenELin, CCALin, AppGrad) find all k vectors at once, therefore requiring k to be known beforehand.",1.3 Other Related Works,[0],[0]
"As a separate note, these known approaches do not need the user to know the desired accuracy a priori but our LazyEV and LazyCCA algorithms do.",1.3 Other Related Works,[0],[0]
We denote by ‖x‖ or ‖x‖2 the Euclidean norm of vector x.,2 Preliminaries,[0],[0]
"We denote by ‖A‖2, ‖A‖F , and ‖A‖Sq respectively the spectral, Frobenius, and Schatten q-norm of matrix A (for q ≥ 1).",2 Preliminaries,[0],[0]
"We write A B if A,B are symmetric and A−B is positive semi-definite (PSD), and write A B if A,B are symmetric but A − B is positive definite (PD).",2 Preliminaries,[0],[0]
"We denote by λmax(M) and λmin(M) the largest and smallest eigenvalue of a symmetric matrix M , and by κM",2 Preliminaries,[0],[0]
"the condition number λmax(M)/λmin(M) of a PSD matrix M .
",2 Preliminaries,[0],[0]
"Throughout this paper, we use nnz(M) to denote the time to multiply matrix M to any arbitrary vector.",2 Preliminaries,[0],[0]
"For two matricesX,Y , we denote by nnz(X,Y ) = nnz(X)+nnz(Y ), and by Xi or Yi the i-th row vector of X or Y .",2 Preliminaries,[0],[0]
"We also use poly(x1, x2, . . .",2 Preliminaries,[0],[0]
", xt) to represent a quantity that is asymptotically at most polynomial in terms of variables x1, . .",2 Preliminaries,[0],[0]
.,2 Preliminaries,[0],[0]
", xt.",2 Preliminaries,[0],[0]
"Given a column orthonormal matrix U ∈ Rn×k, we denote by U⊥ ∈ Rn×(n−k)",2 Preliminaries,[0],[0]
"the column orthonormal matrix consisting of an arbitrary basis in the space orthogonal to the span of U ’s columns.
",2 Preliminaries,[0],[0]
"Given a PSD matrixB and a vector v, v>Bv is theB-seminorm of v. Two vectors v, w are B-orthogonal if v>Bw = 0.",2 Preliminaries,[0],[0]
"We denote by B−1 the Moore-Penrose pseudoinverse of B if B is not invertible, and by B1/2 the matrix square root of B (satisfying B1/2 0).",2 Preliminaries,[0],[0]
"All occurrences of B−1, B1/2 and B−1/2 are for analysis purpose only.",2 Preliminaries,[0],[0]
"Our final algorithms only require multiplications of B to vectors.
",2 Preliminaries,[0],[0]
Definition 2.1 (GenEV).,2 Preliminaries,[0],[0]
"Given symmetric matrices A,B ∈ Rd×d where B is positive definite.",2 Preliminaries,[0],[0]
"The generalized eigenvectors ofA with respect toB are v1, . . .",2 Preliminaries,[0],[0]
", vd, where each vi is
vi ∈ arg max v∈Rd {∣∣v>Av∣∣ s.t. v>",2 Preliminaries,[0],[0]
Bv = 1 v>Bvj = 0 ∀j ∈,2 Preliminaries,[0],[0]
"[i− 1] } The generalized eigenvalues λ1, . .",2 Preliminaries,[0],[0]
.,2 Preliminaries,[0],[0]
", λd satisfy λi = v",2 Preliminaries,[0],[0]
>,2 Preliminaries,[0],[0]
"i Avi which can be negative.
",2 Preliminaries,[0],[0]
"Following (Wang et al., 2016; Garber & Hazan, 2015), we assume without loss of generality that λi ∈",2 Preliminaries,[0],[0]
"[−1, 1].
",2 Preliminaries,[0],[0]
Definition 2.2 (CCA).,2 Preliminaries,[0],[0]
"Given X ∈ Rn×dx , Y ∈",2 Preliminaries,[0],[0]
"Rn×dy , letting Sxx = 1nX >X , Sxy = 1nX >Y , Syy = 1nY
>Y , the canonical-correlation vectors are {(φi, ψi)}ri=1",2 Preliminaries,[0],[0]
"where r = min{dx, dy} and for all i ∈",2 Preliminaries,[0],[0]
"[r]:
(φi, ψi) ∈ arg max φ∈Rdx ,ψ∈Rdy
{ φ>Sxyψ such that
{ φ>Sxxφ = 1 ∧ φ>Sxxφj = 0 ∀j ∈",2 Preliminaries,[0],[0]
[i− 1] ψ>Syyψ = 1 ∧ ψ>Syyψj = 0 ∀j ∈,2 Preliminaries,[0],[0]
"[i− 1] }} The corresponding canonical-correlation coefficients σ1, . . .",2 Preliminaries,[0],[0]
", σr satisfy σi = φ>i Sxyψi ∈",2 Preliminaries,[0],[0]
"[0, 1].
We emphasize that σi always lies in [0, 1] and is scalinginvariant.",2 Preliminaries,[0],[0]
"When dealing with a CCA problem, we also denote by d = dx + dy .
",2 Preliminaries,[0],[0]
Lemma 2.3 (CCA to GenEV).,2 Preliminaries,[0],[0]
"Given a CCA problem with matrices X ∈ Rn×dx , Y ∈ Rn×dy , let the canonicalcorrelation vectors and coefficients be {(φi, ψi, σi)}ri=1 where r = min{dx, dy}.",2 Preliminaries,[0],[0]
"Define A = ( 0 Sxy
S>xy 0
) and
B =",2 Preliminaries,[0],[0]
"( Sxx 0
0 Syy
) .",2 Preliminaries,[0],[0]
"Then, the GenEV problem of A with re-
spect to B has 2r eigenvalues {±σi}ri=1 and corresponding generalized eigenvectors {( φi ψi ) , ( −φi ψi )}n i=1 .",2 Preliminaries,[0],[0]
The remaining dx + dy,2 Preliminaries,[0],[0]
"− 2r eigenvalues are zeros.
",2 Preliminaries,[0],[0]
Definition 2.4.,2 Preliminaries,[0],[0]
"In CCA, let A and B be as defined in Lemma 2.3.",2 Preliminaries,[0],[0]
"We define condition numbers
κ def = κB = λmax(B) λmin(B) and κ′ def= 2 maxi{‖Xi‖ 2,‖Yi‖2} λmin(B) .",2 Preliminaries,[0],[0]
Fact 2.5.,2 Preliminaries,[0],[0]
κ′ ∈,2 Preliminaries,[0],[0]
"[κ, 2nκ] and κ′ ≥ d. (See full version.)
",2 Preliminaries,[0],[0]
Lemma 2.6.,2 Preliminaries,[0],[0]
"Given matrices X ∈ Rn×dx , Y ∈ Rn×dy , let A and B be as defined in Lemma 2.3.",2 Preliminaries,[0],[0]
"For every w ∈ Rd, the Katyusha method (Allen-Zhu, 2017) finds a vector w′ ∈",2 Preliminaries,[0],[0]
"Rd satisfying ‖w′ −B−1Aw‖ ≤ ε in time
O ( nnz(X,Y ) · ( 1 + √ κ′/n ) ·",2 Preliminaries,[0],[0]
"log κ‖w‖ 2
ε
) .",2 Preliminaries,[0],[0]
"We introduce AppxPCA±, the multiplicative approximation algorithm for computing the two-sided leading eigenvector of a symmetric matrix.",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"AppxPCA± uses the shift-and-invert framework (Garber & Hazan, 2015; Garber et al., 2016), and shall become our building block for the LazyEV and LazyCCA algorithms in the subsequent sections.
",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"Our pseudo-code Algorithm 1 is a modification of Algorithm 5 in (Garber & Hazan, 2015), and reduces the eigenvector problem to oracle calls to an arbitrary matrix inversion oracle A.",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"The main differences between AppxPCA± and (Garber & Hazan, 2015) are two-fold.
",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"First, given a symmetric matrix M , AppxPCA± simultaneously considers an upper-bounding shift together with a lower-bounding shift, and try to perform power methods with respect to (λI − M)−1 and (λI + M)−1.",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"This allows us to determine approximately how close λ is to the largest and the smallest eigenvalues of M , and decrease λ accordingly.",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"In the end, AppxPCA± outputs an approximate eigenvector of M that corresponds to a negative eigenvalue if needed.",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"Second, we provide a multiplicative-error guarantee rather than additive as appeared in (Garber & Hazan, 2015).",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"Without such guarantee, our final running time will depend on 1gap·λmax(M) rather than 1 gap .",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"6
6This is why the SI method of (Wang et al., 2016) also uses
Algorithm 1 AppxPCA±(A,M, δ×, ε, p)
",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"Input: A, an approximate matrix inversion method; M ∈ Rd×d, a symmetric matrix satisfying −I M I; δ× ∈ (0, 0.5], a multiplicative error; ε ∈ (0, 1), a numerical accuracy parameter; and p ∈ (0, 1), the confidence parameter.
",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"1: ŵ0 ← RanInit(d); s← 0; λ(0) ← 1 + δ×; ŵ0 is a random unit vector, see Def. 3.2 2: m1 ← ⌈ 4 log ( 288dθ p2 )⌉ , m2 ← ⌈ log ( 36dθ p2ε )⌉ ; θ is the parameter of RanInit, see Def. 3.2
3: ε̃1 ← 164m1 ( δ× 48 )m1 and ε̃2",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"← ε8m2 ( δ×48 )m2 4: repeat m1 = TPM(8, 1/32, p) and m2 = TPM(2, ε/4, p), see Lemma B.1 5:",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"s← s+ 1; 6: for t = 1 to m1 do 7: Apply A to find ŵt satisfying
∥∥ŵt − (λ(s−1)I −M)−1ŵt−1∥∥ ≤ ε̃1; 8: wa ← ŵm1/‖ŵm1‖; wa is roughly (λ(s−1)I −M)−m1 ŵ0 then normalized 9: Apply A to find va satisfying
∥∥va",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
− (λ(s−1)I −M)−1wa∥∥ ≤ ε̃1; 10: for t = 1 to m1 do 11:,3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"Apply A to find ŵt satisfying
∥∥ŵt − (λ(s−1)I +M)−1ŵt−1∥∥ ≤ ε̃1; 12: wb ← ŵm1/‖ŵm1‖; wb is roughly (λ(s−1)I +M)−m1 ŵ0 then normalized 13: Apply A to find vb satisfying
∥∥vb",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"− (λ(s−1)I +M)−1wb∥∥ ≤ ε̃1; 14: ∆(s) ← 12 ·
1 max{w>a va,w>b vb}−ε̃1
and λ(s)",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
← λ(s−1),3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"− ∆ (s)
2 ;
15: until ∆(s) ≤ δ×λ (s) 12 16: f ← s; 17: if the last w>a va ≥ w>b vb then 18: for t = 1 to m2 do 19: Apply A to find ŵt satisfying
∥∥ŵt − (λ(f)I −M)−1ŵt−1∥∥ ≤ ε̃2; 20: return (+, w) where w def= ŵm2/‖ŵm2‖. 21: else 22: for t = 1 to m2 do 23: Apply A to find ŵt satisfying
∥∥ŵt",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"− (λ(f)I +M)−1ŵt−1∥∥ ≤ ε̃2; 24: return (−, w) where w def= ŵm2/‖ŵm2‖. 25: end if
We prove in full version the following theorem:
Theorem 3.1 (AppxPCA±, informal).",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
Let M ∈ Rd×d be a symmetric matrix with eigenvalues 1 ≥ λ1 ≥ · · ·,3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
≥ λd ≥ −1,3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"and eigenvectors u1, . . .",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
", ud.",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"Let λ∗ = max{λ1,−λd}.",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"With probability at least 1− p, AppxPCA± produces a pair (sgn,w) satisfying
• if sgn = +, then w is an approx.",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"positive eigenvector: w>Mw ≥ (
1− δ× 2
)",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"λ∗ ∧ ∑
i∈[d]",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"λi≤(1−δ×/2)λ∗
(w>ui) 2 ≤ ε
• if sgn = −, then w is an approx.",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
negative eigenvector: w>Mw ≤,3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"− (
1−δ× 2
)",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"λ∗ ∧ ∑
i∈[d]",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"λi≥−(1−δ×/2)λ∗
(w>ui) 2 ≤ ε
The number of oracle calls toA is Õ(log(1/δ×)), and each time we call A it satisfies
shift-and-invert but depends on 1 gap·σ1 in Table 2.
•",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"λmax(λ (s)I−M) λmin(λ(s)I−M) , λmax(λ (s)I+M) λmin(λ(s)I+M) ∈",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"[1, 96δ× ] and
• 1 λmin(λ(s)I−M) , 1 λmin(λ(s)I+M) ≤ 48δ×λ∗ .
",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"We remark here that, unlike the original shift-and-invert method which chooses a random (Gaussian) unit vector in Line 1 of AppxPCA±, we have allowed this initial vector to be generated from an arbitrary θ-conditioned random vector generator (for later use), defined as follows:
Definition 3.2.",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"An algorithm RanInit(d) is a θconditioned random vector generator if w = RanInit(d) is a d-dimensional unit vector and, for every p ∈ (0, 1), every unit vector u ∈ Rd, with probability at least 1− p, it satisfies (u>w)2 ≤ p
2θ 9d .
",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
This modification is needed in order to obtain our efficient implementations of GenEV and CCA.,3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"One can construct a θ-conditioned random vector generator as follows:
Proposition 3.3.",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"Given a PSD matrix B ∈ Rd×d, if we set RanInit(d) def = B
1/2v (v>Bv)0.5
where v is a random Gaussian vector, then RanInit(d) is a θ-conditioned random vector
generator for θ = κB .",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"In this section, we construct an algorithm LazyEV that, given symmetric matrix M ∈ Rd×d, computes approximately the k leading eigenvectors ofM that have the largest absolute eigenvalues.",4 LazyEV: Generalized Eigendecomposition,[0],[0]
"Then, for the original k-GenEV problem, we set M = B−1/2AB−1/2 and run LazyEV.",4 LazyEV: Generalized Eigendecomposition,[0],[0]
"This is our plan to find the top k leading generalized eigenvectors of A with respect to B.
Our algorithm LazyEV is formally stated in Algorithm 2.",4 LazyEV: Generalized Eigendecomposition,[0],[0]
"The algorithm applies k times AppxPCA±, each time computing an approximate leading eigenvector of M with a multiplicative error δ×/2, and projects the matrix M into the orthogonal space with respect to the obtained leading eigenvector.",4 LazyEV: Generalized Eigendecomposition,[0],[0]
"We state our main approximation theorem below.
",4 LazyEV: Generalized Eigendecomposition,[0],[0]
Theorem 4.1 (informal).,4 LazyEV: Generalized Eigendecomposition,[0],[0]
"Let M ∈ Rd×d be a symmetric matrix with eigenvalues λ1, . . .",4 LazyEV: Generalized Eigendecomposition,[0],[0]
", λd ∈",4 LazyEV: Generalized Eigendecomposition,[0],[0]
"[−1, 1] and corresponding eigenvectors u1, . . .",4 LazyEV: Generalized Eigendecomposition,[0],[0]
", ud, and |λ1| ≥ · · · ≥ |λd|.",4 LazyEV: Generalized Eigendecomposition,[0],[0]
"If εpca is sufficiently small,7 LazyEV outputs a (column) orthonormal matrix Vk = (v1, . . .",4 LazyEV: Generalized Eigendecomposition,[0],[0]
", vk) ∈ Rd×k which, with probability at least 1− p, satisfies: (a) ‖V >",4 LazyEV: Generalized Eigendecomposition,[0],[0]
k,4 LazyEV: Generalized Eigendecomposition,[0],[0]
"U‖2 ≤ ε where U = (uj , . . .",4 LazyEV: Generalized Eigendecomposition,[0],[0]
", ud) and j is the
smallest index satisfying |λj | ≤ (1− δ×)λk.",4 LazyEV: Generalized Eigendecomposition,[0],[0]
(b) For every i ∈,4 LazyEV: Generalized Eigendecomposition,[0],[0]
"[k], (1−δ×)|λi| ≤ |v>i Mvi| ≤ 11−δ× |λi|.
",4 LazyEV: Generalized Eigendecomposition,[0],[0]
"Above, property (a) ensures the k columns of Vk have negligible correlation with the eigenvectors of M whose absolute eigenvalues are ≤ (1− δ×)λk; property (b) ensures the Rayleigh quotients v>",4 LazyEV: Generalized Eigendecomposition,[0],[0]
i Mvi are all correct up to a 1±δ× error.,4 LazyEV: Generalized Eigendecomposition,[0],[0]
"We in fact have shown two more useful properties in the full version that may be of independent interest.
",4 LazyEV: Generalized Eigendecomposition,[0],[0]
"The next theorem states that, if M = B−1/2AB−1/2, our LazyEV can be implemented without the necessity to compute B1/2 or B−1/2.
",4 LazyEV: Generalized Eigendecomposition,[0],[0]
Theorem 4.2 (running time).,4 LazyEV: Generalized Eigendecomposition,[0],[0]
"Let A,B ∈ Rd×d be two symmetric matrices satisfying B 0 and −B A B. Suppose M = B−1/2AB−1/2 and RanInit(d) is defined in Proposition 3.3 with respect to B. Then, the computation of V ← B−1/2LazyEV(A,M, k, δ×, εpca, p) can be implemented to run in time
• Õ ( knnz(B)+k2d+kΥ√
δ×
) where Υ is the time to multiply
B−1A to a vector, or • Õ ( k √ κBnnz(B)+knnz(A)+k
2d√ δ×
) if we use Conjugate gra-
dient to multiply B−1A to a vector.
",4 LazyEV: Generalized Eigendecomposition,[0],[0]
7Meaning εpca ≤,4 LazyEV: Generalized Eigendecomposition,[0],[0]
"O ( poly(ε, δ×,
|λ1| |λk+1| , 1 d ) ) .",4 LazyEV: Generalized Eigendecomposition,[0],[0]
"The complete
specifications of εpca is included in the full version.",4 LazyEV: Generalized Eigendecomposition,[0],[0]
"Since our final running time only depends on log(1/εpca), we have not attempted to improve the constants in this polynomial dependency.
",4 LazyEV: Generalized Eigendecomposition,[0],[0]
"Choosing parameter δ× as either gap or ε, our two main theorems above immediately imply the following results for the k-GenEV problem: (proved in full version)
Theorem 4.3 (gap-dependent GenEV, informal).",4 LazyEV: Generalized Eigendecomposition,[0],[0]
"Let A,B ∈ Rd×d be two symmetric matrices satisfying B 0 and −B A B. Suppose the generalized eigenvalue and eigenvector pairs of A with respect to B are {(λi, ui)}di=1, and it satisfies 1 ≥ |λ1| ≥ · · · ≥ |λd|.",4 LazyEV: Generalized Eigendecomposition,[0],[0]
"Then, LazyEV outputs V k ∈ Rd×k satisfying
V > k",4 LazyEV: Generalized Eigendecomposition,[0],[0]
BV k = I and ‖V > k BW‖2 ≤,4 LazyEV: Generalized Eigendecomposition,[0],[0]
"ε
in time Õ (k√κBnnz(B) + knnz(A)",4 LazyEV: Generalized Eigendecomposition,[0],[0]
"+ k2d√
gap )",4 LazyEV: Generalized Eigendecomposition,[0],[0]
"Here, W = (uk+1, . . .",4 LazyEV: Generalized Eigendecomposition,[0],[0]
", ud) and gap =
|λk|−|λk+1| |λk| .
",4 LazyEV: Generalized Eigendecomposition,[0],[0]
"Theorem 4.4 (gap-free GenEV, informal).",4 LazyEV: Generalized Eigendecomposition,[0],[0]
"In the same setting as Theorem 4.3, our LazyEV outputs V k = (v1, . . .",4 LazyEV: Generalized Eigendecomposition,[0],[0]
", vk) ∈ Rd×k satisfying",4 LazyEV: Generalized Eigendecomposition,[0],[0]
V > k BV,4 LazyEV: Generalized Eigendecomposition,[0],[0]
"k = I and
∀s ∈",4 LazyEV: Generalized Eigendecomposition,[0],[0]
[k] : ∣∣v>s Avs∣∣ ∈,4 LazyEV: Generalized Eigendecomposition,[0],[0]
"[(1− ε)|λs|, |λs|1− ε]
in time Õ (k√κBnnz(B) + knnz(A) + k2d√
ε
) .",4 LazyEV: Generalized Eigendecomposition,[0],[0]
"In Section 5.1 we discuss how to ensure accuracy: that is, why does LazyEV guarantee to approximately find the top eigenvectors ofM .",5 Ideas Behind Theorems 4.1 and 4.2,[0],[0]
"In the full version of this paper, we also discuss how to implement LazyEV without compute B1/2 explicitly, thus proving Theorem 4.2.",5 Ideas Behind Theorems 4.1 and 4.2,[0],[0]
"Our approximation guarantee in Theorem 4.1 is a natural generalization of the recent work on fast iterative methods to find the top k eigenvectors of a PSD matrix M (Allen-Zhu & Li, 2016).",5.1 Ideas Behind Theorem 4.1,[0],[0]
"That method is called LazySVD and we summarize it as follows.
",5.1 Ideas Behind Theorem 4.1,[0],[0]
"At a high level, LazySVD finds the top k eigenvectors oneby-one and approximately.",5.1 Ideas Behind Theorem 4.1,[0],[0]
"Starting with M0 = M , in the s-th iteration where s ∈",5.1 Ideas Behind Theorem 4.1,[0],[0]
"[k], LazySVD computes approximately the leading eigenvector of matrix Ms−1 and call it vs. Then, LazySVD projects Ms ← (I − vsv>s )",5.1 Ideas Behind Theorem 4.1,[0],[0]
Ms−1(I,5.1 Ideas Behind Theorem 4.1,[0],[0]
"− vsv > s ) and proceeds to the next iteration.
",5.1 Ideas Behind Theorem 4.1,[0],[0]
"While the algorithmic idea of LazySVD is simple, the analysis requires some careful linear algebraic lemmas.",5.1 Ideas Behind Theorem 4.1,[0],[0]
"Most notably, if vs is an approximate leading eigenvector of Ms−1, then one needs to prove that the small eigenvectors of Ms−1 somehow still “embed” into that of Ms after projection.",5.1 Ideas Behind Theorem 4.1,[0],[0]
"This is achieved by a gap-free variant of the Wedin theorem plus a few other technical lemmas, and we recommend interested readers to see the high-level overview section of (Allen-Zhu & Li, 2016).
",5.1 Ideas Behind Theorem 4.1,[0],[0]
"Algorithm 2 LazyEV(A,M, k, δ×, εpca, p)
",5.1 Ideas Behind Theorem 4.1,[0],[0]
"Input: A, an approximate matrix inversion method; M ∈ Rd×d, a matrix satisfying −I M I; k ∈",5.1 Ideas Behind Theorem 4.1,[0],[0]
"[d], the desired rank; δ× ∈ (0, 1), a multiplicative error; εpca ∈ (0, 1), a numerical accuracy; and p ∈ (0, 1), a confidence parameter.
1: M0 ←M ; V0 =",5.1 Ideas Behind Theorem 4.1,[0],[0]
"[]; 2: for s = 1 to k do 3: (∼, v′s)← AppxPCA±(A,Ms−1, δ×/2, εpca, p/k); v′s is an approximate two-sided leading eigenvector of Ms−1 4: vs ← ( (I − Vs−1V >s−1)v′s ) / ∥∥(I",5.1 Ideas Behind Theorem 4.1,[0],[0]
"− Vs−1V >s−1)v′s∥∥; project v′s to V ⊥s−1 5: Vs ← [Vs−1, vs]; 6: Ms ← (I − vsv>s )Ms−1(I − vsv>s ) we also have Ms = (I − VsV >s )M(I",5.1 Ideas Behind Theorem 4.1,[0],[0]
"− VsV >s ) 7: end for 8: return Vk.
",5.1 Ideas Behind Theorem 4.1,[0],[0]
"In this paper, to relax the assumption thatM is PSD, and to find leading eigenvectors whose absolute eigenvalues are large, we have to make several non-trivial changes.",5.1 Ideas Behind Theorem 4.1,[0],[0]
"On the algorithm side, LazyEV uses our two-sided shift-andinvert method in Section 3 to find the leading eigenvector of Ms−1 with largest absolute eigenvalue.",5.1 Ideas Behind Theorem 4.1,[0],[0]
"On the analysis side, we have to make sure all lemmas properly deal with negative eigenvalues.",5.1 Ideas Behind Theorem 4.1,[0],[0]
"For instance:
• If we perform a projection M ′",5.1 Ideas Behind Theorem 4.1,[0],[0]
← (I − vv>)M(I,5.1 Ideas Behind Theorem 4.1,[0],[0]
"− vv>) where v correlates by at most ε with all eigenvectors ofM whose absolute eigenvalues are smaller than a threshold µ, then, after the projection, we need to prove that these eigenvectors can be approximately “embedded” into the eigenspace spanned by all eigenvectors of M ′",5.1 Ideas Behind Theorem 4.1,[0],[0]
whose absolute eigenvalues are smaller than µ+ τ .,5.1 Ideas Behind Theorem 4.1,[0],[0]
"The approximation of this embedding should depend on ε, µ and τ .
",5.1 Ideas Behind Theorem 4.1,[0],[0]
The full proof of Theorem 4.1 is in the arXiv version.,5.1 Ideas Behind Theorem 4.1,[0],[0]
It relies on a few matrix algebraic lemmas (including the aforementioned “embedding lemma”).,5.1 Ideas Behind Theorem 4.1,[0],[0]
In this paper we propose new iterative methods to solve the generalized eigenvector and the canonical correlation analysis problems.,6 Conclusion,[0],[0]
"Our methods find the most significant k eigenvectors or correlation vectors, and have running times that linearly scales with k.
Most importantly, our methods are doubly-accelerated: the running times have square-root dependencies both with respect to the condition number of the matrix (i.e., κ) and with respect to the eigengap (i.e., gap).",6 Conclusion,[0],[0]
They are the first doubly-accelerated iterative methods at least for k > 1.,6 Conclusion,[0],[0]
"They can also be made gap-free, and are the first gap-free iterative methods even for 1-GenEV or 1-CCA.
",6 Conclusion,[0],[0]
"Although this is a theory paper, we believe that if implemented carefully, our methods can outperform not only previous iterative methods (such as GenELin, AppGrad, CCALin), but also the commercial mathematics libraries for sparse matrices of dimension more than 10, 000.",6 Conclusion,[0],[0]
"We
leave it a future work for such careful comparisons.",6 Conclusion,[0],[0]
"We study k-GenEV, the problem of finding the top k generalized eigenvectors, and k-CCA, the problem of finding the top k vectors in canonicalcorrelation analysis.",abstractText,[0],[0]
We propose algorithms LazyEV and LazyCCA to solve the two problems with running times linearly dependent on the input size and on,abstractText,[0],[0]
"k. Furthermore, our algorithms are doubly-accelerated: our running times depend only on the square root of the matrix condition number, and on the square root of the eigengap.",abstractText,[0],[0]
This is the first such result for both kGenEV or k-CCA.,abstractText,[0],[0]
"We also provide the first gapfree results, which provide running times that depend on 1/ √ ε rather than the eigengap.",abstractText,[0],[0]
Doubly Accelerated Methods for Faster CCA  and Generalized Eigendecomposition,title,[0],[0]
Regularized empirical risk minimization with linear predictors is a key workhorse in machine learning.,1. Introduction,[0],[0]
"It has the following general form:
min x2Rd
( P (x) def = 1
n
nX
i=1
i
(a > i x) + g(x)
) (1)
where a i 2 Rd is one of the n data samples with d features.",1. Introduction,[0],[0]
"i
: R!",1. Introduction,[0],[0]
"R is a convex loss function of the linear predictor a
> i x, for i = 1, · · · , n, and g : Rd !",1. Introduction,[0],[0]
R is a convex regularization function for the coefficient vector x 2 Rd.,1. Introduction,[0],[0]
"The loss function
i assigns a cost to the difference between the linear predictor a>
i
x and the associated label b i .
",1. Introduction,[0],[0]
"With continuous and discrete b i , (1) captures regression and classification problems respectively.",1. Introduction,[0],[0]
"As a popular instance,
1Department of ICES, University of Texas, Austin 2Department of CS, Carnegie Mellon University, Pittsburgh 3Department of CS, University of Texas, Austin 4Amazon/A9, Palo Alto.",1. Introduction,[0],[0]
Correspondence to: Qi Lei,1. Introduction,[0],[0]
"<leiqi@ices.utexas.edu>, Ian E.H. Yen <eyan@cs.cmu.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"when i (z) = max{0, 1 b i z} and g(x) = µ/2kxk22, (1) reduces to the linear SVM (support vector machine) classification problem.",1. Introduction,[0],[0]
"While setting
i (z) = log(1+exp( b i z)), we obtain the logistic regression problem.
",1. Introduction,[0],[0]
We are interested in developing efficient algorithms for solving this general problem (1) for the setting where the coefficient vector x is assumed to be sparse.,1. Introduction,[0],[0]
"Applications where such a sparsity is natural include large-scale multiclass/multi-label classification, low-degree polynomial data mapping (Chang et al., 2010), n-gram feature mapping (Sonnenburg & Franc, 2010), and random feature kernel machines (Rahimi & Recht, 2007), specifically with a sparsity constraint on the random features (Yen et al., 2014).
",1. Introduction,[0],[0]
Our paper is organized as follows:,1. Introduction,[0],[0]
"In Section 2 we review existing algorithms to solve the primal, dual as well as primal-dual formulations of the problem (1).",1. Introduction,[0],[0]
"In Section 3, we present our Doubly Greedy Primal-Dual Coordinate Descent method for the convex-concave saddle point formulation of the problem (1).",1. Introduction,[0],[0]
We propose an alternative method that is more efficient in practice with the use of incrementally increased active sets in both primal and dual variables.,1. Introduction,[0],[0]
"In Section 4 we show linear convergence for our proposed algorithm, and demonstrate the advantages of greedy methods with sparse variables.",1. Introduction,[0],[0]
"Finally in Section 5 we compare the performance of our method with other state-of-the-art methods on some real-world datasets, both with respect to time and iterations.",1. Introduction,[0],[0]
"Notations: We use A to denote the data matrix, with rows A
i
= a
i corresponding to samples, and the column Aj corresponding to features.",2. Formulation and related work,[0],[0]
"We use [n] to compactly denote {1, 2, · · ·n}.",2. Formulation and related work,[0],[0]
"Throughout the paper, k · k denotes l2-norm unless otherwise specified.
Assumptions:",2. Formulation and related work,[0],[0]
"In order to establish equivalence of the primal, dual problem and the convex-concave saddle point formulation, we make the following assumptions.
",2. Formulation and related work,[0],[0]
"• g, the regularization for primal variable, is assumed to be µ-strongly convex, formally,
g(y) g(x) +",2. Formulation and related work,[0],[0]
"hrg(x),y xi+ µ 2 ky xk2, for any sub-gradient rg(x) 2 @g(x),x,y 2 Rd.",2. Formulation and related work,[0],[0]
"We
also assume that g has decomposable structure, i.e., g(x) =",2. Formulation and related work,[0],[0]
P,2. Formulation and related work,[0],[0]
i g,2. Formulation and related work,[0],[0]
"i (x i ).
",2. Formulation and related work,[0],[0]
"• i is 1 -smooth, for i 2",2. Formulation and related work,[0],[0]
[n]: i (y)  i (x)+ 0,2. Formulation and related work,[0],[0]
"i
(x)(y x)+ 2",2. Formulation and related work,[0],[0]
"(y x)2, x, y 2 R or equivalently, 0
i is Lipschitz continuous, i.e., | 0
i (x) 0",2. Formulation and related work,[0],[0]
i (y)|  1 |x y|.,2. Formulation and related work,[0],[0]
"Under the assumption of strongly convex regularization g and smooth loss function
i , minimizing (1) is equivalent to maximizing its dual formulation:
max y2Rn
( D(y) ⌘","2.1. Primal, dual and primal-dual formulations",[0],[0]
"g⇤( A > y
n )","2.1. Primal, dual and primal-dual formulations",[0],[0]
"1 n
nX
i=1
","2.1. Primal, dual and primal-dual formulations",[0],[0]
⇤ i,"2.1. Primal, dual and primal-dual formulations",[0],[0]
"(y i )
) (2)
or the unique solution for the following convex-concave saddle point problem:
max y2Rn min x2Rd
( L(x,y) = g(x) + 1
n","2.1. Primal, dual and primal-dual formulations",[0],[0]
y >,"2.1. Primal, dual and primal-dual formulations",[0],[0]
"Ax 1 n
nX
i=1
⇤","2.1. Primal, dual and primal-dual formulations",[0],[0]
"i (y i )
)
(3)
Note that i (a > i x) in (1) is also smooth with respect to x, since r
x","2.1. Primal, dual and primal-dual formulations",[0],[0]
"i
(a > i x) = 0","2.1. Primal, dual and primal-dual formulations",[0],[0]
i (a > i x)a,"2.1. Primal, dual and primal-dual formulations",[0],[0]
"i , therefore i (a > i x)
is R2/ -smooth with respect to x, where R is defined as R = max
i ka i k2.","2.1. Primal, dual and primal-dual formulations",[0],[0]
"(Zhang & Xiao, 2014) thus defined the condition number for the primal-dual form as:
 def =
R2 µ .
","2.1. Primal, dual and primal-dual formulations",[0],[0]
We share this definition in this paper.,"2.1. Primal, dual and primal-dual formulations",[0],[0]
"The commonly used condition number for the gradient descent of the primal form is simply (R2/ + µ)/µ = 1 + , see (Nesterov, 2004).","2.1. Primal, dual and primal-dual formulations",[0],[0]
There has been a long line of work over the years to derive fast solvers for the generic optimization problem (1).,2.2. Related work,[0],[0]
"In Table 1, we review the time complexity to achieve ✏ error with respect to either primal, dual or primal-dual optimality for existing methods.
",2.2. Related work,[0],[0]
"Primal (accelerated) gradient descent (Nesterov, 2004; 2005) require O((1+)",2.2. Related work,[0],[0]
log(1/✏)) (or O((1+p) log(1/✏)),2.2. Related work,[0],[0]
if accelerated) iterations to achieve primal error less than ✏.,2.2. Related work,[0],[0]
Note that 1+ is the condition number of (1).,2.2. Related work,[0],[0]
"Since each iteration takes O(nd) operations, the overall time complexity is O(nd (1 + )",2.2. Related work,[0],[0]
log(1/✏)) (or O(nd (1 + p) log(1/✏)) if accelerated).,2.2. Related work,[0],[0]
"Due to the large per iteration cost for large n, stochastic variants that separately optimize each i
have proved more popular in big data settings.",2.2. Related work,[0],[0]
"Examples include SGD (Bottou, 2010), SAG (Schmidt et al., 2013), SVRG (Johnson & Zhang, 2013), SAGA (Defazio et al., 2014), MISO (Mairal, 2015) and their accel-
erated versions (Xiao & Zhang, 2014).",2.2. Related work,[0],[0]
"The stochastic scheme of optimizing individual
i is similar to updating each dual coordinate individually.",2.2. Related work,[0],[0]
"Their time complexity thus matches that of dual coordinate descent methods (Hsieh et al., 2008; Shalev-Shwartz & Zhang, 2013b; Yang, 2013; Qu et al., 2014), which enjoy a time complexity of O(nd (1+/n) log(1/✏)), and a further acceleration step (Shalev-Shwartz & Zhang, 2016; 2013a) will improve the complexity to O(nd (1 +p/n)",2.2. Related work,[0],[0]
log(1/✏)).,2.2. Related work,[0],[0]
"These stochastic methods have a lower per iteration cost of O(d), but each step optimizes terms that are much less well-conditioned, and consequently have a larger iteration complexity, for instance of O(n (1 + p /n)",2.2. Related work,[0],[0]
"log(1/✏)) in the accelerated case.
",2.2. Related work,[0],[0]
"With the primal-dual formulation, (Zhang & Xiao, 2014) introduce a novel stochastic primal-dual coordinate method (SPDC), which with acceleration achieves a time complexity of O(nd (1 +p/n)",2.2. Related work,[0],[0]
"log(1/✏)), matching that of accelerated stochastic dual coordinate descent methods.
",2.2. Related work,[0],[0]
"However, in practice, SPDC could lead to more expensive computations for sparse data matrices due to dense updates.",2.2. Related work,[0],[0]
"For some special choices of the model, (Zhang & Xiao, 2014) provided efficient implementation for sparse feature structures, but the average update time for each coordinate is still much longer than that of dual coordinate descent.",2.2. Related work,[0],[0]
"Moreover, they cannot exploit intermediate sparse iterates by methods such as shrinking technique (Hsieh et al., 2008).",2.2. Related work,[0],[0]
"We note moreover that acceleration is not always practically useful in many real-world settings, other than in extremely ill-conditioned situations.",2.2. Related work,[0],[0]
"In particular,  is typically of the order of p n or n as shown in (Bousquet & Elisseeff, 2002; Zhang & Xiao, 2014), and therefore the conditioning of O(1+p/n) is not necessarily much better than O(1+ /n).",2.2. Related work,[0],[0]
"Our experiments also corroborate this, showing that vanilla dual coordinate descent under reasonable precision or condition number is not improved upon by SDPC.
",2.2. Related work,[0],[0]
"Therefore we raise the following question: Does the primaldual formulation have other good properties that could be leveraged to improve optimization performance?
",2.2. Related work,[0],[0]
"For instance, some recent work with the primal-dual formulation updates stochastically sampled coordinates (Yu et al., 2015), which has a reduced cost per iteration, provided the data admits a low-rank factorization or when the proximal mapping for primal and dual variables are relatively computational expensive, which however may not hold in practice, so that the the noise caused by this preprocessing could hurt test performance.",2.2. Related work,[0],[0]
"Moreover, even when their assumptions hold, their low-rank matrix factorization step itself may dominate the total computation time.",2.2. Related work,[0],[0]
"In this paper, we try to address the key question above in the setting of empirical risk minimization problems with very large n and d, and where the set of primal (and/or dual) variables are assumed to be sparse.",2.3. Our Contribution,[0],[0]
"We then show that the primal-dual formulation of the problem allows for naturally leveraging available primal and/or dual sparsity.
2",2.3. Our Contribution,[0],[0]
"[
,].
",2.3. Our Contribution,[0],[0]
"In Table 1, we review the total time complexity to achieve ✏ accuracy.",2.3. Our Contribution,[0],[0]
"We can see that all algorithms that achieve a linear convergence rate require running time that has a factor nd, and in particular, none of their convergence rates involve the sparsity of the primal or dual variables.
",2.3. Our Contribution,[0],[0]
"There have been some attempts to modify existing primal or dual coordinate approaches in order to exploit sparsity of either primal or dual variables, but these do not perform very well in practice.",2.3. Our Contribution,[0],[0]
"One popular approach uses a shrinking heuristic in updating dual coordinates (Hsieh et al., 2008), which however still requires complexity linear to the number of coordinates d and does not guarantee rate of convergence.",2.3. Our Contribution,[0],[0]
"(Nutini et al., 2015) consider the idea of searching more important active coordinates to update in each iteration.",2.3. Our Contribution,[0],[0]
"Their greedy updates yield an iteration complexity linear in 1/µ1 instead of d/µ, where µ and µ1 are the parameters of strong convexity with respect to L2 and L1 norms respectively.",2.3. Our Contribution,[0],[0]
"However, with the commonly used L2 regularization term µk · k2 that is used to ensure µ-strong convexity, the term is exactly µ1 = µ
d l1-strongly convex.",2.3. Our Contribution,[0],[0]
"Moreover, in practice, searching active coordinates causes considerable overhead.",2.3. Our Contribution,[0],[0]
"While there have been some strategies proposed to address this such as (Dhillon et al., 2011) that leverages nearest neighbor search to reduce the searching time, these have further requirements on the data structure used to store the data.",2.3. Our Contribution,[0],[0]
"Overall, it thus remains to more carefully study the optimization problem in order to facilitate the use of greedy
approaches to exploit primal or dual sparsity.
",2.3. Our Contribution,[0],[0]
"In this paper, we propose a Doubly Greedy Primal-Dual (DGPD) Coordinate method that greedily selects and updates both primal and dual variables.",2.3. Our Contribution,[0],[0]
"This method enjoys an overall low time complexity under a sparsity assumption on the primal variables:
Theorem 2.1.",2.3. Our Contribution,[0],[0]
"Main result: (informal) For the empirical risk minimization problem (1) with l1 + l2 regularization, there exists an algorithm (DGPD) that achieves ✏ error in O(s(n+d)(1+ 
n
) log 1 ✏ )) time, where s is an upper bound of the sparsity of the primal variables.",2.3. Our Contribution,[0],[0]
"Coordinate-wise updates are most natural when g is separable, as is assumed for instance in the Stochastic Primal-Dual Coordinate method of (Zhang & Xiao, 2014).",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"In this paper, to exploit sparsity in primal variables, we additionally focus on the case where g(x) =",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
µ2 kxk2 + kxk1.,3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"With respect to the loss function , it is assumed to be 1
-smooth and convex.",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"For instance, setting
i as the smooth hinge loss(Shalev-Shwartz & Zhang, 2013b):
i
(z) =
8 <
:
0",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"if b i z 1 1 2 biz if biz  0
( 1 2 biz)2 otherwise,
the smoothness parameter = 12 .",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"For the logit function i (z) = log(1 + exp( b i
z), the smoothness parameter = 4.
",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"When iterates are sparse, it is more efficient to perform greedy coordinate descent.",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
We will provide a brief theoretical vignette of this phenomenon in Section 4.1.,3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"With this motivation, our proposed method Doubly Greedy PrimalDual Coordinate Descent (DGPD) greedily selects and updates both the primal and dual variables, one coordinate a time.",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"Our overall method is detailed in Algorithm 1.
",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"In Algorithm 1, we start from all zero vectors x(0), z(0) 2 Rn, and y(0),w(0) 2 Rd, where x(0), and y(0) are the iterates for primal and dual variables, and w(0) and z(0) are two auxiliary vectors, maintained as w ⌘",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
Ax and z ⌘,3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"A>y to cache and reduce computations.
",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
Primal Updates.,3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"In each iteration, we first compute the optimal primal variable x̄(t 1) for the current y(t 1), i.e.,
¯ x (t 1) = argmin
x L(x,y(t 1)))",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
Eqn.(4),3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"Then, we only update the coordinate j(t) that will decrease L(x,y) the most, i.e., j(t) = argmin
k2[d] L(x(t)+(x̄(t 1) k x(t) k )e k ,y(t 1)))",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
Eqn.(5),3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
Both two processes cost O(d) operations.,3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"Afterwards, we update the value of w with Eqn.",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"(6) such that w(t) = Ax(t)
Algorithm 1 Doubly Greedy Primal-Dual Coordinate method 1: Input: Training data A 2 Rn⇥d, dual step size ⌘ > 0.",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
2: Initialize: x(0) 0,3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
2,3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"Rd, y(0) 0 2",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"Rn,w(0) ⌘",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"Ax = 0 2 Rn, z(0) ⌘",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"A>y = 0 2 Rd 3: for t = 1, 2, · · · , T do 4: Choose greedily the primal coordinate to update:
x̄
(t)
k
argmin ↵
1
n
z (t 1)",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"k ↵+ g k (↵)
, 8k 2",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"[d] (4)
j (t) argmin k2[d]
1
n
z (t 1) k",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
(x̄ (t) k x(t 1) k ),3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
+ g k (x̄ (t) k ),3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"g k (x (t 1) k )
(5)
x
(t)
k
(
x̄
(t)
k",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"if k = j(t), x
(t 1) k otherwise.",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"5: Update w to maintain the value of Ax:
w (t) w(t 1)",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
+ (x(t) j x(t 1) j ),3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"A j (6) 6: Choose greedily the dual coordinate to update:
i (t) argmax",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"k2[n]
|w(t 1) k
1 n",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"(
⇤ k ) 0",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
(y (t 1) k ),3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"| (7)
y
(t)
",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"k
(
argmax
1
n
w
(t)
k
⇤ k",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"( ) 1 2⌘ ( y(t 1) k ) 2
if k = i(t)
y (t 1)",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"k
otherwise.",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"(8)
7: Update z to maintain the value of A>y z
(t) z(t 1) + (y(t)",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
i (t) y(t 1),3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
i (t) ),3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"A i (t) (9) 8: end for 9: Output: x(T ),y(T )
in O(d) or O(nnz(Aj)) operations.",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"This greedy choice of j(t) and aggressive update induces a sufficient primal progress, as shown in Lemma A.1.
",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
Dual Updates.,3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
We note that the updates are not exactly symmetric in the primal x and dual y variables.,3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"The updates for the dual variables y do follow along similar lines as x, except that we use the Gauss-Southwell rule to select variables, and introduce a step size ⌘.",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"This is motivated by our convergence analysis, which shows that each primal update step yields a large descent in the objective, while each dual update only ascends the dual objective modulo an error term.",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
This required a subtle analysis to show that the error terms were canceled out in the end by the progress made in the primal updates.,3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"But to proceed with such an analysis required the use of a step size in the dual updates, to balance the progress made in the dual updates, and the error term it introduced.",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"Note moreover, that we are using the Gauss-Southwell rule to choose the variable to optimize in the dual variables y, while we simply use the coordinate that causes the most function descent in the primal variables x.",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
This is because our choice of step size in the dual updates required computations that are shared with our current approach of selecting the optimal primal variable.,3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"This does incur more overhead when compared to the Gauss Southwell rule however, so that we simply use the latter for optimizing y.
The most significant feature in our method is that we select
and update one coordinate in both the primal and dual coordinates greedily.",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
With a simple trick that maintains the value of w ⌘,3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
Ax and z ⌘,3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
A>y,3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"(Lei et al., 2016), we are able to select and update primal and dual coordinates in O(n) and O(d) operations respectively.",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"This happens when computing the value of Ax and A>y, which are the bottleneck in computing the gradient or updating the variables.",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
An extension to choose and update a batch of primal and dual coordinate is straightforward.,3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"We provide further discussions on the designing of Algorithm 1 in Section 4.
",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"In this paper, we have not incorporated an extrapolation/acceleration scheme to our algorithm.",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"As noted earlier, in practice the condition number  is usually comparable to n, thus adding an extrapolation term that reduces the conditioning from /n to p /n is not necessarily materially advantageous in real applications.",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"Meanwhile, an extrapolation step usually worsens the stability of the algorithm, and is not easily combined with incorporating greedy updates, which is crucial to the leveraging the primal or dual sparsity structure in this paper.",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"We thus defer an accelerated extension of our algorithm incorporating extrapolation term to future work.
",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"For Algorithm 1, each iteration can be seen to have a cost of O(n + d), while in Section 4 we show that the iteration complexity for our method is O((1 + 
n )s log(1/✏)) assuming that the primal variables are s-sparse.",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"Therefore the overall time complexity for our algorithm is O (1 + 
n
)",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"s(n + d) log(1/✏) , which is cheaper than the
time complexity of even the accelerated SPDC algorithm",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"O (1 +p
n
)nd log(1/✏) except for extremely ill conditioned cases.",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"In real application settings, Algorithm 1 has some drawbacks.",3.1. A Practical Extension of DGPD,[0],[0]
"When data is sparse, we still require O(n) and O(d) operations to update primal and dual variables.",3.1. A Practical Extension of DGPD,[0],[0]
"Even when the data is dense, to find the greedy coordinate and to update it requires comparable time complexity, which suggests we should find some ways to eliminate overhead in practice.
",3.1. A Practical Extension of DGPD,[0],[0]
"To resolve these issues, we introduce the Doubly Greedy Primal-Dual Coordinate method with Active Sets in Algorithm 2.",3.1. A Practical Extension of DGPD,[0],[0]
"We make use of what we call active sets, that contains the newly selected coordinates as well as the current non-zero variables.",3.1. A Practical Extension of DGPD,[0],[0]
"We construct these active sets A
x
and A y
for both primal and dual variables.",3.1. A Practical Extension of DGPD,[0],[0]
"Initially, they are set as empty sets.",3.1. A Practical Extension of DGPD,[0],[0]
"In each iteration, we recurrently select coordinates outside the active sets with the Gauss-Southwell rule, and add them to A
x and A y
.",3.1. A Practical Extension of DGPD,[0],[0]
We then optimize all the variables within the active sets.,3.1. A Practical Extension of DGPD,[0],[0]
"Once a primal/dual variable gets set to 0, we can drop it from the corresponding active sets.",3.1. A Practical Extension of DGPD,[0],[0]
"This practice keeps the active sets A
x and A y
as the support of primal and dual variables.",3.1. A Practical Extension of DGPD,[0],[0]
"Notice g0
k
(x k ) is 0 when x
k is zero, so that the variable selection step for primal variables can be simplified as stated in (10).
",3.1. A Practical Extension of DGPD,[0],[0]
"Now the time complexity per iteration becomes |A x |n + |A
y |d.",3.1. A Practical Extension of DGPD,[0],[0]
The sparsity in primal variables is encouraged by the choice of `1 + `2 regularization.,3.1. A Practical Extension of DGPD,[0],[0]
"Meanwhile, as shown by (Yen et al., 2016), a sparse set of primal variables usually induces a sparse set of dual variables.",3.1. A Practical Extension of DGPD,[0],[0]
"Therefore |A
x |⌧ d and |A
y | ⌧ n in practice, and the cost per iteration is sub-linear to nd.",3.1. A Practical Extension of DGPD,[0],[0]
We present further details in Section 3.2.,3.1. A Practical Extension of DGPD,[0],[0]
"Suppose we are given a sparse data matrix A with number of non-zero elements of each column and each row bounded by nnz
y and nnz x respectively, one can further reduce the cost for computing (10) and (12) from O(d|A
y | + n|A x |) to O(nnz
x |A y | + nnz y |A x |) by storing both {A i }n i=1",3.2. Efficient Implementation for Sparse Data Matrix,[0],[0]
"and
{Aj}d j=1 as sparse vectors and computing A>y and Ax as
A>y = X
i2A y
A>",3.2. Efficient Implementation for Sparse Data Matrix,[0],[0]
i y,3.2. Efficient Implementation for Sparse Data Matrix,[0],[0]
"i
, Ax = X
j2A x
Ajx j .",3.2. Efficient Implementation for Sparse Data Matrix,[0],[0]
"(14)
In our implementation, whenever the active sets A y , A x are expanded, we further maintain a submatrix [A]A which contains only rows in A
y and columns in A x
, so the primal and dual updates (11), (13) only cost P i2A
y
nnz([A i ]",3.2. Efficient Implementation for Sparse Data Matrix,[0],[0]
A x ).,3.2. Efficient Implementation for Sparse Data Matrix,[0],[0]
"This results in each update costing less than the search steps, and therefore, in practice, one can conduct multiple rounds of updates (11), (13) before conducting the search (10), (12), which in our experiment speeds up convergence
significantly.",3.2. Efficient Implementation for Sparse Data Matrix,[0],[0]
"In this section, we introduce the primal gap
p
and dual gap
d and analyze the convergence rate in terms of their sum, which we call primal and dual sub-optimality =
p
+
d
.
Definition 4.1.",4. Convergence Analysis,[0],[0]
"For the following convex-concave function L(x,y) def= g(x) + 1
n
y >",4. Convergence Analysis,[0],[0]
"Ax 1 n
P n
i=1",4. Convergence Analysis,[0],[0]
"⇤ i (y i ), with
its primal form P (x) def=",4. Convergence Analysis,[0],[0]
"min y L(x,y), and dual form D(y) def = max
x L(x,y), we define the primal gap at iteration t as
(t) p
def = L(x(t+1),y(t))",4. Convergence Analysis,[0],[0]
"D(y(t))
",4. Convergence Analysis,[0],[0]
", the dual gap at iteration t as
(t) d def = D⇤ D(y(t))
and sub-optimality as
(t) def =
(t) p + (t) d .
Theorem 4.2.",4. Convergence Analysis,[0],[0]
"Suppose in (1), g is µ-strongly convex (`1 + `2) regularization, and i is 1
-smooth.",4. Convergence Analysis,[0],[0]
Let R = max i2[n] kaik2.,4. Convergence Analysis,[0],[0]
"Then DGPD achieves
(t+1)  2n 2n+ ⌘
(t), (15)
if step size ⌘(t) satisfies that
⌘(t)  2n 2µ
kx(t) ¯x(t)k0(5R2 + n µ) (16)
",4. Convergence Analysis,[0],[0]
"Suppose kx(t) ¯x(t)k0  s, if we choose step size ⌘ = 2n2µ (5R2+n µ)s , then it requires
O(s( n + 1) log 1 ✏ )
iterations for achieving ✏ primal and dual sub-optimality.1
Proof sketch:",4. Convergence Analysis,[0],[0]
The proof analysis is straightforward with the introduction of primal and dual sub-optimality .,4. Convergence Analysis,[0],[0]
"We divide the proof into primal-dual progress, primal progress, and dual progress.
",4. Convergence Analysis,[0],[0]
"• Primal-Dual Progress (Lemma A.2).
",4. Convergence Analysis,[0],[0]
(t) d + (t) p ( (t 1),4. Convergence Analysis,[0],[0]
"d + (t 1) p )
 L(x(t+1),yt) L(xt,yt) +⌘( 1
n hA",4. Convergence Analysis,[0],[0]
"i (t) ,x(t)",4. Convergence Analysis,[0],[0]
"¯x(t)i)2
⌘( 1 n hA",4. Convergence Analysis,[0],[0]
"i (t) , ¯x(t)i 1 n",4. Convergence Analysis,[0],[0]
( ⇤ i (t)) 0,4. Convergence Analysis,[0],[0]
(y(t),4. Convergence Analysis,[0],[0]
i (t))),4. Convergence Analysis,[0],[0]
"2(17)
1This result can be easily connected to traditional convergence analysis in primal or dual form.",4. Convergence Analysis,[0],[0]
"Notice  ✏ is sufficient requirement that dual gap
d
= D ⇤ D(y)  ✏, therefore the dual variable y(t) converges to optimal y⇤ with the same convergence rate.
",4. Convergence Analysis,[0],[0]
"Algorithm 2 Doubly Greedy Primal-Dual Coordinate method with Active Sets
1: Input: Training data A 2 Rn⇥d, dual step size ⌘ > 0.",4. Convergence Analysis,[0],[0]
2: Initialize: x(0) 0,4. Convergence Analysis,[0],[0]
2,4. Convergence Analysis,[0],[0]
"Rd, y(0) 0 2 Rn, A(0)
",4. Convergence Analysis,[0],[0]
"x
?,A(0) y ?",4. Convergence Analysis,[0],[0]
"3: for t 1, 2, · · · , T do 4: Update the active set A(t)
x
greedily based on the optimal primal variable ¯x(t 1) and update x in its active set.
x̄(t)",4. Convergence Analysis,[0],[0]
"k
argmin ↵
1 n hAk,y(t 1)i↵+ g k (↵) , 8k 2",4. Convergence Analysis,[0],[0]
"[d]
j(t) argmax k2[d] |x̄(t 1) k",4. Convergence Analysis,[0],[0]
"| (10)
A(t) x
A(t 1) x",4. Convergence Analysis,[0],[0]
"[ {j(t)} x(t) j
(
x̄(t 1)",4. Convergence Analysis,[0],[0]
"j , if j 2 A(t) x x(t 1) j , if j /2 A(t) x
(11)
5: Update the active set A(t) y greedily based on the value of r y L(x(t),y(t 1)) and update y in its active set.",4. Convergence Analysis,[0],[0]
"i(t) argmax
k2[n] A(t 1) y
|hA",4. Convergence Analysis,[0],[0]
"k ,x(t)i 1 n",4. Convergence Analysis,[0],[0]
( ⇤ k ) 0,4. Convergence Analysis,[0],[0]
(y(t 1) k ),4. Convergence Analysis,[0],[0]
|.,4. Convergence Analysis,[0],[0]
"(12)
A(t) y
A(t 1)",4. Convergence Analysis,[0],[0]
"y [ {i(t)}
y(t)",4. Convergence Analysis,[0],[0]
"i
(
argmax 1 n hA",4. Convergence Analysis,[0],[0]
"i ,x(t)i",4. Convergence Analysis,[0],[0]
1 n,4. Convergence Analysis,[0],[0]
"⇤ i ( ) 12⌘ ( y(t 1)k ) , if i 2 A(t) y y(t 1)",4. Convergence Analysis,[0],[0]
"i , if i /2 A(t) y
(13)
6: Kick out 0 variables from active sets.",4. Convergence Analysis,[0],[0]
"A(t)
y
A(t) y
[
i,y (t)",4. Convergence Analysis,[0],[0]
"i =0
{i}, A(t) x
A(t) x
[
j,x (t) j =0
{j}
7: end for 8: Output: x(T ),y(T )
",4. Convergence Analysis,[0],[0]
This lemma connects the descent in PD sub-optimality with primal progress and dual progress.,4. Convergence Analysis,[0],[0]
"The third term and the second terms respectively represent the potential dual progress if we used the optimal ¯x(t), and the irrelevant part generated from the difference between ¯x(t) and x(t).
",4. Convergence Analysis,[0],[0]
• Primal Progress (Lemma A.1).,4. Convergence Analysis,[0],[0]
"L(x(t),y(t))",4. Convergence Analysis,[0],[0]
"L(x(t+1),y(t)) 1kx(t) ¯x(t)k0 1 (t) p
(18)
",4. Convergence Analysis,[0],[0]
"This inequality simply demonstrates function loss from primal update is at least a ratio of primal gap.
",4. Convergence Analysis,[0],[0]
• Dual Progress (Lemma A.3).,4. Convergence Analysis,[0],[0]
"( 1
n hA",4. Convergence Analysis,[0],[0]
"i (t) ,x(t)",4. Convergence Analysis,[0],[0]
"¯x(t)i)2
( 1 n hA",4. Convergence Analysis,[0],[0]
"i (t) , ¯x(t)i 1 n",4. Convergence Analysis,[0],[0]
( ⇤ i (t)) 0,4. Convergence Analysis,[0],[0]
(y(t),4. Convergence Analysis,[0],[0]
i (t))),4. Convergence Analysis,[0],[0]
"2
 2n (t) d +
5R2 2n2 kx(t) ¯x(t)k2 (19)
",4. Convergence Analysis,[0],[0]
"Finally, we establish the relation between the dual progress in our algorithm with dual gap and difference between ¯x(t) and x(t).",4. Convergence Analysis,[0],[0]
"Now we can prove our main theorem 4.2.
",4. Convergence Analysis,[0],[0]
"For cleaner notation, write a = ⌘ 2n , b = 5⌘R2 2n2 .",4. Convergence Analysis,[0],[0]
k¯x(t) x,4. Convergence Analysis,[0],[0]
"(t)k0  s. By combining (18) and (19) to (17), we get:
(t)",4. Convergence Analysis,[0],[0]
"d
(t 1) d + (t) p
(t 1) p
 L(x(t+1),yt) L(xt,yt) a (t) d
+
2
bkx(t) ¯x(t)k2
 L(x(t+1),yt) L(xt,yt) a (t) d
+b L(x(t),y(t))",4. Convergence Analysis,[0],[0]
"L(¯x(t),y(t))
",4. Convergence Analysis,[0],[0]
"= (1 b) L(x(t+1),yt) L(xt,yt) a (t) d
+b L(x(t+1),yt) L(¯x(t),yt)
 ",4. Convergence Analysis,[0],[0]
1 b s 1 (t) p a (t),4. Convergence Analysis,[0],[0]
"d
+b L(x(t+1),yt) L(¯x(t),yt)
",4. Convergence Analysis,[0],[0]
"= 1 b s 1 b (t) p a (t) d
Here the second inequality comes from strong convexity of L(·,y(t)).",4. Convergence Analysis,[0],[0]
The fourth inequality comes from Lemma A.1.,4. Convergence Analysis,[0],[0]
"Therefore when a  1 b
s 1 b, or sufficiently a  (s(1 + 5/n)) 1, we get (t)  ",4. Convergence Analysis,[0],[0]
11+a (t 1).,4. Convergence Analysis,[0],[0]
"Since a < 1, (a + 1) 1/a  1/2, therefore (t)  (1 + a) t (0) 
2
at (0).",4. Convergence Analysis,[0],[0]
"Therefore when T O(s(1 + /n) log2 (0)
✏
),
(T )  ✏.",4. Convergence Analysis,[0],[0]
"In this section, we give a simple analysis of the greedy variable selection rule showing that when iterate and minimizer of a generic optimization problem are sparse, its convergence rate is faster than choosing random coordinates.",4.1. Analysis on greedy methods for sparse iterates,[0],[0]
"We define the optimization problem in the space of Rn:
min x2Rn f(x)
, where f is µ-strongly convex L-smooth: |r
i f(x+ ↵e i )",4.1. Analysis on greedy methods for sparse iterates,[0],[0]
"r i f(x)|  L|↵|, 8x 2 Rn Under this setting, a random coordinate descent method with step size 1
L , achieves E[f(x+)",4.1. Analysis on greedy methods for sparse iterates,[0],[0]
"f⇤]  (1 µ nL ) f(x)
",4.1. Analysis on greedy methods for sparse iterates,[0],[0]
"f⇤ , where x+ is the next iterate of x.
Under the assumption that the current iterate x and the optimal x⇤ are both k-sparse, we thereby conduct greedy coordinate descent rule, i.e., x+ = x + ⌘e
i ⇤ , where ⌘, i⇤ satisfies f(x+⌘e
i
⇤ ) =",4.1. Analysis on greedy methods for sparse iterates,[0],[0]
"min
i, f(x+ e i ).",4.1. Analysis on greedy methods for sparse iterates,[0],[0]
"With L-Lipchitz continuity, we have:
f(x+ ⌘",4.1. Analysis on greedy methods for sparse iterates,[0],[0]
e,4.1. Analysis on greedy methods for sparse iterates,[0],[0]
"i ⇤ ) f(x)
 min ,i hrf(x), e",4.1. Analysis on greedy methods for sparse iterates,[0],[0]
"i i+ L 2 2
= min
,i
hrf(x),",4.1. Analysis on greedy methods for sparse iterates,[0],[0]
"e i i+ L 2 k e i k21
= min
x hrf(x), xi+ L 2 k xk21
 min x
f(x+ x) f(x) + L
2
k xk21
 min 0 1
f(x+ (x⇤ x)) f(x) +",4.1. Analysis on greedy methods for sparse iterates,[0],[0]
"L
2
2kx⇤ xk21
 min 0 1
(f⇤ f(x))",4.1. Analysis on greedy methods for sparse iterates,[0],[0]
"+ L
2
2kx⇤ xk21
The last two inequalities are obtained by constraining x to be of the form (x⇤ x) and by the convexity of f .",4.1. Analysis on greedy methods for sparse iterates,[0],[0]
"For the k-sparse x, and x⇤, x x⇤ is at most 2k-sparse, and for any 2k-sparse vector a, kak21  2kkak22.",4.1. Analysis on greedy methods for sparse iterates,[0],[0]
"Hereby we obtain:
min
0 1
(f⇤ f(x))",4.1. Analysis on greedy methods for sparse iterates,[0],[0]
+,4.1. Analysis on greedy methods for sparse iterates,[0],[0]
"L
2
2kx⇤ xk21
 min 0 1 (f⇤ f(x))",4.1. Analysis on greedy methods for sparse iterates,[0],[0]
+,4.1. Analysis on greedy methods for sparse iterates,[0],[0]
"Lk 2kx⇤ xk22
 min 0 1
(f⇤ f(x))",4.1. Analysis on greedy methods for sparse iterates,[0],[0]
"2kL
µ 2(f⇤ f(x))
",4.1. Analysis on greedy methods for sparse iterates,[0],[0]
"=
µ
8kL",4.1. Analysis on greedy methods for sparse iterates,[0],[0]
"(f⇤ f(x))
",4.1. Analysis on greedy methods for sparse iterates,[0],[0]
Therefore f(x+),4.1. Analysis on greedy methods for sparse iterates,[0],[0]
"f⇤  (1 µ8kL )(f(x) f⇤), and when k ⌧ n, this convergence rate could be much better than randomized coordinate descent.",4.1. Analysis on greedy methods for sparse iterates,[0],[0]
"In this section, we implement the Doubly-Greedy PrimalDual Coordinate Descent algorithm with Active Sets, and compare its performance with other state-of-the-art methods for `1+`2-regularized Empirical Risk minimization, including Primal Randomized Coordinate Descent (PrimalRCD) (Richtárik & Takác, 2014), Dual Randomized Coordinate Descent (DualRCD, i.e., SDCA)",5. Experiment,[0],[0]
"(Shalev-Shwartz & Zhang, 2013b) and the Stochastic Primal-Dual Coordinate Method (SPDC) (Zhang & Xiao, 2014).
",5. Experiment,[0],[0]
"We conduct experiments on large-scale multi-class data sets with linear and non-linear feature mappings, as shown in Table 2.",5. Experiment,[0],[0]
"For Mnist and Aloi we use Random Fourier (RF) and Random Binning (RB) feature proposed in (Rahimi & Recht, 2007) to approximate effect of RBF Gaussian kernel and Laplacian Kernel respectively.",5. Experiment,[0],[0]
"The features generated by Random Fourier are dense, while Random Binning gives highly sparse data.
",5. Experiment,[0],[0]
"We give results for 2 {0.1, 0.01} and µ 2 {1, 0.1, 0.01}, where Figure 1 shows results for = 0.1, µ = 0.01 and others can be found in Appendix B. In the above six figures, we compare the running time with objective function.",5. Experiment,[0],[0]
"While in the below figures, the x-axis is number of iterations.",5. Experiment,[0],[0]
"For the baseline methods, one iteration is one pass over all the variables, and for our method, it is several (5) passes over the active sets.",5. Experiment,[0],[0]
"From the figures, we can see that in all cases, DGPD has better performance than other methods.",5. Experiment,[0],[0]
"Notice for clear presentation purposes we use log-scale for Mnist-RB-time, Aloi-RB-time and RCV-time, where our algorithm achieves improvements over others of orders of magnitude.
",5. Experiment,[0],[0]
"The result shows that, by exploiting sparsity in both the primal and dual, DGPD has much less cost per iteration and thus is considerably faster in terms of training time, while by maintaining an active set it does not sacrifice much in terms of convergence rate.",5. Experiment,[0],[0]
"Note since in practice we perform multiple updates after each search, the convergence rate (measured in outer iterations) can be sometimes even better than DualRCD.",5. Experiment,[0],[0]
"I.D. acknowledges the support of NSF via CCF-1320746, IIS-1546452, and CCF-1564000.",6. Acknowledgements,[0],[0]
"P.R. acknowledges the support of ARO via W911NF-12-1-0390 and NSF via IIS1149803, IIS-1320894, IIS-1447574, and DMS-1264033, and NIH via R01 GM117594-01 as part of the Joint DMS/NIGMS Initiative to Support Research at the Interface of the Biological and Mathematical Sciences.",6. Acknowledgements,[0],[0]
We consider the popular problem of sparse empirical risk minimization with linear predictors and a large number of both features and observations.,abstractText,[0],[0]
"With a convex-concave saddle point objective reformulation, we propose a Doubly Greedy PrimalDual Coordinate Descent algorithm that is able to exploit sparsity in both primal and dual variables.",abstractText,[0],[0]
"It enjoys a low cost per iteration and our theoretical analysis shows that it converges linearly with a good iteration complexity, provided that the set of primal variables is sparse.",abstractText,[0],[0]
We then extend this algorithm further to leverage active sets.,abstractText,[0],[0]
"The resulting new algorithm is even faster, and experiments on large-scale Multi-class data sets show that our algorithm achieves up to 30 times speedup on several state-of-the-art optimization methods.",abstractText,[0],[0]
Doubly Greedy Primal-Dual Coordinate Descent for Sparse Empirical Risk Minimization,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 1460–1469 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
Natural Language Inference (NLI; a.k.a.,1 Introduction,[0],[0]
"Recognizing Textual Entailment, or RTE) is an important and challenging task for natural language understanding (MacCartney and Manning, 2008).",1 Introduction,[0],[0]
"The goal of NLI is to identify the logical relationship (entailment, neutral, or contradiction) between a premise and a corresponding hypothesis.",1 Introduction,[0],[0]
"Table 1 shows few example relationships from the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015).
",1 Introduction,[0],[0]
"Recently, NLI has received a lot of attention from the researchers, especially due to the avail-
∗ ArXiv version of this work can be found here (arxiv.org/pdf/1802.05577.pdf).
†",1 Introduction,[0],[0]
"This work was conducted as part of an internship program at Philips Research.
ability of large annotated datasets like SNLI (Bowman et al., 2015).",1 Introduction,[0],[0]
"Various deep learning models have been proposed that achieve successful results for this task (Gong et al., 2017; Wang et al., 2017; Chen et al., 2017; Yu and Munkhdalai, 2017a; Parikh et al., 2016; Zhao et al., 2016; Sha et al., 2016).",1 Introduction,[0],[0]
Most of these existing NLI models use attention mechanism to jointly interpret and align the premise and hypothesis.,1 Introduction,[0],[0]
Such models use simple reading mechanisms to encode the premise and hypothesis independently.,1 Introduction,[0],[0]
"However, such a complex task require explicit modeling of dependency relationships between the premise and the hypothesis during the encoding and inference processes to prevent the network from the loss of relevant, contextual information.",1 Introduction,[0],[0]
"In this paper, we refer to such strategies as dependent reading.
",1 Introduction,[0],[0]
"There are some alternative reading mechanisms available in the literature (Sha et al., 2016; Rocktäschel et al., 2015) that consider dependency aspects of the premise-hypothesis relationships.",1 Introduction,[0],[0]
"However, these mechanisms have two major limitations:
• So far, they have only explored dependency aspects during the encoding stage, while ignoring its benefit during inference.
",1 Introduction,[0],[0]
"• Such models only consider encoding a hy-
1460
pothesis depending on the premise, disregarding the dependency aspects in the opposite direction.
",1 Introduction,[0],[0]
We propose a dependent reading bidirectional LSTM (DR-BiLSTM) model to address these limitations.,1 Introduction,[0],[0]
"Given a premise u and a hypothesis v, our model first encodes them considering dependency on each other (u|v and v|u).",1 Introduction,[0],[0]
"Next, the model employs a soft attention mechanism to extract relevant information from these encodings.",1 Introduction,[0],[0]
"The augmented sentence representations are then passed to the inference stage, which uses a similar dependent reading strategy in both directions, i.e. u→ v and v →",1 Introduction,[0],[0]
"u. Finally, a decision is made through a multi-layer perceptron (MLP) based on the aggregated information.
",1 Introduction,[0],[0]
"Our experiments on the SNLI dataset show that DR-BiLSTM achieves the best single model and ensemble model performance obtaining improvements of a considerable margin of 0.4% and 0.3% over the previous state-of-the-art single and ensemble models, respectively.
",1 Introduction,[0],[0]
"Furthermore, we demonstrate the importance of a simple preprocessing step performed on the SNLI dataset.",1 Introduction,[0],[0]
Evaluation results show that such preprocessing allows our single model to achieve the same accuracy as the state-of-the-art ensemble model and improves our ensemble model to outperform the state-of-the-art ensemble model by a remarkable margin of 0.7%.,1 Introduction,[0],[0]
"Finally, we perform an extensive analysis to clarify the strengths and weaknesses of our models.",1 Introduction,[0],[0]
"Early studies use small datasets while leveraging lexical and syntactic features for NLI (MacCartney and Manning, 2008).",2 Related Work,[0],[0]
"The recent availability of large-scale annotated datasets (Bowman et al., 2015; Williams et al., 2017) has enabled researchers to develop various deep learning-based architectures for NLI.
Parikh et al. (2016) propose an attention-based model (Bahdanau et al., 2014) that decomposes the NLI task into sub-problems to solve them in parallel.",2 Related Work,[0],[0]
They further show the benefit of adding intra-sentence attention to input representations.,2 Related Work,[0],[0]
Chen et al. (2017) explore sequential inference models based on chain LSTMs with attentional input encoding and demonstrate the effectiveness of syntactic information.,2 Related Work,[0],[0]
We also use similar attention mechanisms.,2 Related Work,[0],[0]
"However, our model is distinct
from these models as they do not benefit from dependent reading strategies.
",2 Related Work,[0],[0]
Rocktäschel et al. (2015) use a word-by-word neural attention mechanism while Sha et al. (2016) propose re-read LSTM units by considering the dependency of a hypothesis on the information of its premise (v|u) to achieve promising results.,2 Related Work,[0],[0]
"However, these models suffer from weak inferencing methods by disregarding the dependency aspects from the opposite direction (u|v).",2 Related Work,[0],[0]
"Intuitively, when a human judges a premise-hypothesis relationship, s/he might consider back-and-forth reading of both sentences before coming to a conclusion.",2 Related Work,[0],[0]
"Therefore, it is essential to encode the premise-hypothesis dependency relations from both directions to optimize the understanding of their relationship.
",2 Related Work,[0],[0]
"Wang et al. (2017) propose a bilateral multiperspective matching (BiMPM) model, which resembles the concept of matching a premise and hypothesis from both directions.",2 Related Work,[0],[0]
Their matching strategy is essentially similar to our attention mechanism that utilizes relevant information from the other sentence for each word sequence.,2 Related Work,[0],[0]
"They use similar methods as Chen et al. (2017) for encoding and inference, without any dependent reading mechanism.
",2 Related Work,[0],[0]
"Although NLI is well studied in the literature, the potential of dependent reading and interaction between a premise and hypothesis is not rigorously explored.",2 Related Work,[0],[0]
"In this paper, we address this gap by proposing a novel deep learning model (DRBiLSTM).",2 Related Work,[0],[0]
Experimental results demonstrate the effectiveness of our model.,2 Related Work,[0],[0]
"Our proposed model (DR-BiLSTM) is composed of the following major components: input encoding, attention, inference, and classification.",3 Model,[0],[0]
"Figure 1 demonstrates a high-level view of our proposed NLI framework.
",3 Model,[0],[0]
Let u =,3 Model,[0],[0]
"[u1, · · · , un] and v =",3 Model,[0],[0]
"[v1, · · · , vm] be the given premise with length n and hypothesis with length m respectively, where ui, vj ∈",3 Model,[0],[0]
Rr is an word embedding of r-dimensional vector.,3 Model,[0],[0]
The task is to predict a label y that indicates the logical relationship between premise u and hypothesis v.,3 Model,[0],[0]
"RNNs are the natural solution for variable length sequence modeling, consequently, we utilize a
bidirectional LSTM (BiLSTM) (Hochreiter and Schmidhuber, 1997) for encoding the given sentences.",3.1 Input Encoding,[0],[0]
"For ease of presentation, we only describe how we encode u depending on v. The same procedure is utilized for the reverse direction (v|u).
",3.1 Input Encoding,[0],[0]
"To dependently encode u, we first process v using the BiLSTM.",3.1 Input Encoding,[0],[0]
Then we read u through the BiLSTM that is initialized with previous reading final states (memory cell and hidden state).,3.1 Input Encoding,[0],[0]
Here we represent a word (e.g. ui) and its context depending on the other sentence (e.g. v).,3.1 Input Encoding,[0],[0]
"Equations 1 and 2 formally represent this component.
",3.1 Input Encoding,[0],[0]
"v̄, sv = BiLSTM(v, 0) û,− = BiLSTM(u, sv) (1)
ū, su = BiLSTM(u, 0)
v̂,− = BiLSTM(v, su) (2)
where {ū ∈ Rn×2d, û ∈ Rn×2d, su} and {v̄ ∈ Rm×2d, v̂ ∈ Rm×2d, sv} are the independent reading sequences, dependent reading sequences, and BiLSTM final state of independent reading of u and v respectively.",3.1 Input Encoding,[0],[0]
"Note that, “−” in these equations means that we do not care about the associated variable and its value.",3.1 Input Encoding,[0],[0]
BiLSTM inputs are the word embedding sequences and initial state vectors.,3.1 Input Encoding,[0],[0]
"û and v̂ are passed to the next layer as the output of the input encoding component.
",3.1 Input Encoding,[0],[0]
The proposed encoding mechanism yields a richer representation for both premise and hypothesis by taking the history of each other into account.,3.1 Input Encoding,[0],[0]
Using a max or average pooling over the independent and dependent readings does not further improve our model.,3.1 Input Encoding,[0],[0]
This was expected since dependent reading produces more promising and relevant encodings.,3.1 Input Encoding,[0],[0]
We employ a soft alignment method to associate the relevant sub-components between the given premise and hypothesis.,3.2 Attention,[0],[0]
"In deep learning models, such purpose is often achieved with a soft attention mechanism.",3.2 Attention,[0],[0]
"Here we compute the unnormalized attention weights as the similarity of hidden states of the premise and hypothesis with Equation 3 (energy function).
",3.2 Attention,[0],[0]
"eij = ûiv̂ T j , i ∈",3.2 Attention,[0],[0]
"[1, n], j ∈",3.2 Attention,[0],[0]
"[1,m] (3)
where ûi and v̂j are the dependent reading hidden representations of u and v respectively which are computed earlier in Equations 1 and 2.",3.2 Attention,[0],[0]
"Next, for each word in either premise or hypothesis, the relevant semantics in the other sentence is extracted and composed according to eij .",3.2 Attention,[0],[0]
"Equations 4 and 5 provide formal and specific details of this procedure.
",3.2 Attention,[0],[0]
"ũi = m∑
j=1
exp(eij)∑m",3.2 Attention,[0],[0]
"k=1 exp(eik) v̂j , i ∈",3.2 Attention,[0],[0]
"[1, n] (4)
ṽj =
n∑
i=1
exp(eij)∑n k=1 exp(ekj) ûi, j ∈",3.2 Attention,[0],[0]
"[1,m] (5)
where ũi represents the extracted relevant information of v̂ by attending to ûi while ṽj represents the extracted relevant information of û by attending to v̂j .
",3.2 Attention,[0],[0]
"To further enrich the collected attentional information, a trivial next step would be to pass the concatenation of the tuples (ûi, ũi) or (v̂j , ṽj) which provides a linear relationship between them.",3.2 Attention,[0],[0]
"However, the model would suffer from the absence of similarity and closeness measures.",3.2 Attention,[0],[0]
"Therefore, we calculate the difference and element-wise product for the tuples (ûi, ũi) and (v̂j , ṽj) that represent the similarity and closeness information respectively (Chen et al., 2017; Kumar et al., 2016).
",3.2 Attention,[0],[0]
"The difference and element-wise product are then concatenated with the computed vectors, (ûi, ũi) or (v̂j , ṽj), respectively.",3.2 Attention,[0],[0]
"Finally, a feedforward neural layer with ReLU activation function projects the concatenated vectors from 8ddimensional vector space into a d-dimensional vector space (Equations 6 and 7).",3.2 Attention,[0],[0]
"This helps the model to capture deeper dependencies between the sentences besides lowering the complexity of vector representations.
",3.2 Attention,[0],[0]
ai =,3.2 Attention,[0],[0]
"[ûi, ũi, ûi − ũi, ûi ũi] pi = ReLU(Wpai + bp)
(6)
bj =",3.2 Attention,[0],[0]
"[v̂j , ṽj , v̂j − ṽj , v̂j ṽj ] qj = ReLU(Wpbj + bp)
(7)
Here stands for element-wise product while Wp ∈ R8d×d and bp ∈ Rd are the trainable weights and biases of the projector layer respectively.",3.2 Attention,[0],[0]
"During this phase, we use another BiLSTM to aggregate the two sequences of computed matching vectors, p and q from the attention stage (Section 3.2).",3.3 Inference,[0],[0]
"This aggregation is performed in a sequential manner to avoid losing effect of latent variables that might rely on the sequence of matching vectors.
",3.3 Inference,[0],[0]
"Instead of aggregating the sequences of matching vectors individually, we propose a similar dependent reading approach for the inference stage.",3.3 Inference,[0],[0]
We employ a BiLSTM reading process (Equations 8 and 9) similar to the input encoding step discussed in Section 3.1.,3.3 Inference,[0],[0]
"But rather than passing just the dependent reading information to the next step, we feed both independent reading (p̄ and q̄) and dependent reading (p̂ and q̂) to a max pooling layer, which selects maximum values from
each sequence of independent and dependent readings (p̄i and p̂i) as shown in Equations 10 and 11.",3.3 Inference,[0],[0]
"The main intuition behind this architecture is to maximize the inferencing ability of the model by considering both independent and dependent readings.
",3.3 Inference,[0],[0]
"q̄, sq = BiLSTM(q, 0) p̂,− = BiLSTM(p, sq) (8)
p̄, sp = BiLSTM(p, 0) q̂,− = BiLSTM(q, sp) (9)
p̃ = MaxPooling(p̄, p̂) (10)
q̃ = MaxPooling(q̄, q̂) (11)
Here {p̄ ∈ Rn×2d, p̂ ∈ Rn×2d, sp} and {q̄ ∈ Rm×2d, q̂ ∈ Rm×2d, sq} are the independent reading sequences, dependent reading sequences, and BiLSTM final state of independent reading of p and q respectively.",3.3 Inference,[0],[0]
"BiLSTM inputs are the word embedding sequences and initial state vectors.
",3.3 Inference,[0],[0]
"Finally, we convert p̃ ∈ Rn×2d and q̃ ∈ Rm×2d to fixed-length vectors with pooling, U ∈ R4d and V ∈ R4d.",3.3 Inference,[0],[0]
"As shown in Equations 12 and 13, we employ both max and average pooling and describe the overall inference relationship with concatenation of their outputs.
",3.3 Inference,[0],[0]
U =,3.3 Inference,[0],[0]
"[MaxPooling(p̃),AvgPooling(p̃)]",3.3 Inference,[0],[0]
"(12)
V = [MaxPooling(q̃),AvgPooling(q̃)]",3.3 Inference,[0],[0]
(13),3.3 Inference,[0],[0]
"Here, we feed the concatenation of U and V ([U, V ]) into a multilayer perceptron (MLP) classifier that includes a hidden layer with tanh activation and softmax output layer.",3.4 Classification,[0],[0]
"The model is trained in an end-to-end manner.
",3.4 Classification,[0],[0]
"Output = MLP([U, V ]) (14)",3.4 Classification,[0],[0]
The Stanford Natural Language Inference (SNLI) dataset contains 570K human annotated sentence pairs.,4.1 Dataset,[0],[0]
"The premises are drawn from the Flickr30k (Plummer et al., 2015) corpus, and then the hypotheses are manually composed for each relationship class (entailment, neutral, contradiction, and -).",4.1 Dataset,[0],[0]
"The “-” class indicates that there is no consensus decision among the annotators, consequently, we remove them during the training and evaluation following the literature.",4.1 Dataset,[0],[0]
We use the same data split as provided in Bowman et al. (2015) to report comparable results with other models.,4.1 Dataset,[0],[0]
"We use pre-trained 300-D Glove 840B vectors (Pennington et al., 2014) to initialize our word embedding vectors.",4.2 Experimental Setup,[0],[0]
All hidden states of BiLSTMs during input encoding and inference have 450 dimensions (r = 300 and d = 450).,4.2 Experimental Setup,[0],[0]
"The weights are learned by minimizing the log-loss on the training data via the Adam optimizer (Kingma and Ba, 2014).",4.2 Experimental Setup,[0],[0]
The initial learning rate is 0.0004.,4.2 Experimental Setup,[0],[0]
"To avoid overfitting, we use dropout (Srivastava et al., 2014) with the rate of 0.4 for regularization, which is applied to all feedforward connections.",4.2 Experimental Setup,[0],[0]
"During training, the word embeddings are updated to learn effective representations for the NLI task.",4.2 Experimental Setup,[0],[0]
We use a fairly small batch size of 32 to provide more exploration power to the model.,4.2 Experimental Setup,[0],[0]
Our observation indicates that using larger batch sizes hurts the performance of our model.,4.2 Experimental Setup,[0],[0]
Ensemble methods use multiple models to obtain better predictive performance.,4.3 Ensemble Strategy,[0],[0]
"Previous works typically utilize trivial ensemble strategies by either using majority votes or averaging the probability distributions over the same model with different initialization seeds (Wang et al., 2017; Gong et al., 2017).
",4.3 Ensemble Strategy,[0],[0]
"By contrast, we use weighted averaging of the probability distributions where the weight of each model is learned through its performance on the SNLI development set.",4.3 Ensemble Strategy,[0],[0]
"Furthermore, the differences between our models in the ensemble originate from: 1) variations in the number of dependent readings (i.e. 1 and 3 rounds of dependent reading), 2) projection layer activation (tanh and
ReLU in Equations 6 and 7), and 3) different initialization seeds.
",4.3 Ensemble Strategy,[0],[0]
The main intuition behind this design is that the effectiveness of a model may depend on the complexity of a premise-hypothesis instance.,4.3 Ensemble Strategy,[0],[0]
"For a simple instance, a simple model could perform better than a complex one, while a complex instance may need further consideration toward disambiguation.",4.3 Ensemble Strategy,[0],[0]
"Consequently, using models with different rounds of dependent readings in the encoding stage should be beneficial.
",4.3 Ensemble Strategy,[0],[0]
Figure 2 demonstrates the observed performance of our ensemble method with different number of models.,4.3 Ensemble Strategy,[0],[0]
The performance of the models are reported based on the best obtained accuracy on the development set.,4.3 Ensemble Strategy,[0],[0]
"We also study the effectiveness of other ensemble strategies e.g. majority voting, and averaging the probability distributions.",4.3 Ensemble Strategy,[0],[0]
"But, our ensemble strategy performs the best among them (see Section 1 in the supplementary material for additional details).",4.3 Ensemble Strategy,[0],[0]
We perform a trivial preprocessing step on SNLI to recover some out-of-vocabulary words found in the development set and test set.,4.4 Preprocessing,[0],[0]
"Note that our vocabulary contains all words that are seen in the training set, so there is no out-of-vocabulary word in it.",4.4 Preprocessing,[0],[0]
"The SNLI dataset is not immune to human
errors, specifically, misspelled words.",4.4 Preprocessing,[0],[0]
We noticed that misspelling is the main reason for some of the observed out-of-vocabulary words.,4.4 Preprocessing,[0],[0]
"Consequently, we simply fix the unseen misspelled words using Microsoft spell-checker (other approaches like edit distance can also be used).",4.4 Preprocessing,[0],[0]
"Moreover, while dealing with an unseen word during evaluation, we try to: 1) replace it with its lower case, or 2) split the word when it contains a “-” (e.g. “marsh-like”) or starts with “un” (e.g. “unloading”).",4.4 Preprocessing,[0],[0]
"If we still could not find the word in our vocabulary, we consider it as an unknown word.",4.4 Preprocessing,[0],[0]
"In the next subsection, we demonstrate the importance and impact of such trivial preprocessing (see Section 2 in the supplementary material for additional details).",4.4 Preprocessing,[0],[0]
Table 2 shows the accuracy of the models on training and test sets of SNLI.,4.5 Results,[0],[0]
The first row represents a baseline classifier presented by Bowman et al. (2015) that utilizes handcrafted features.,4.5 Results,[0],[0]
All other listed models are deep-learning based.,4.5 Results,[0],[0]
The gap between the traditional model and deep learning models demonstrates the effectiveness of deep learning methods for this task.,4.5 Results,[0],[0]
"We also report the estimated human performance on the SNLI dataset, which is the average accuracy of five annotators in comparison to the gold labels (Gong et al., 2017).",4.5 Results,[0],[0]
"It is noteworthy that recent deep learning models surpass the human performance in the NLI task.
",4.5 Results,[0],[0]
"As shown in Table 2, previous deep learning models (rows 2-19) can be divided into three categories: 1) sentence encoding based models (rows 2-7), 2) single inter-sentence attention-based models (rows 8-16), and 3) ensemble inter-sentence attention-based models (rows 17-19).",4.5 Results,[0],[0]
"We can see that inter-sentence attention-based models perform better than sentence encoding based models, which supports our intuition.",4.5 Results,[0],[0]
Natural language inference requires a deep interaction between the premise and hypothesis.,4.5 Results,[0],[0]
"Inter-sentence attention-based approaches can provide such interaction while sentence encoding based models fail to do so.
",4.5 Results,[0],[0]
"To further enhance the modeling of interaction between the premise and hypothesis for efficient disambiguation of their relationship, we introduce the dependent reading strategy in our proposed DR-BiLSTM model.",4.5 Results,[0],[0]
The results demonstrate the effectiveness of our model.,4.5 Results,[0],[0]
"DR-BiLSTM (Single)
achieves 88.5% accuracy on the test set which is noticeably the best reported result among the existing single models for this task.",4.5 Results,[0],[0]
"Note that the difference between DR-BiLSTM and Chen et al. (2017) is statistically significant with a p-value of < 0.001 over the Chi-square test1.
To further improve the performance of NLI systems, researchers have built ensemble models.",4.5 Results,[0],[0]
"Previously, ensemble systems obtained the best performance on SNLI with a huge margin.",4.5 Results,[0],[0]
Table 2 shows that our proposed single model achieves competitive results compared to these reported ensemble models.,4.5 Results,[0],[0]
"Our ensemble model considerably outperforms the current state-of-the-art by obtaining 89.3% accuracy.
",4.5 Results,[0],[0]
"Up until this point, we discussed the performance of our models where we have not con-
1Chi-square test (χ2 test) is used to determine if there is a significant difference between two categorical variables (i.e. models’ outputs).
",4.5 Results,[0],[0]
sidered preprocessing for recovering the out-ofvocabulary words.,4.5 Results,[0],[0]
"In Table 2, “DR-BiLSTM (Single) + Process”, and “DR-BiLSTM (Ensem.)",4.5 Results,[0],[0]
+ Process” represent the performance of our models on the preprocessed dataset.,4.5 Results,[0],[0]
We can see that our preprocessing mechanism leads to further improvements of 0.4% and 0.3% on the SNLI test set for our single and ensemble models respectively.,4.5 Results,[0],[0]
"In fact, our single model (“DR-BiLSTM (Single) + Process”) obtains the state-of-the-art performance over both reported single and ensemble models by performing a simple preprocessing step.",4.5 Results,[0],[0]
"Furthermore, “DR-BiLSTM (Ensem.)",4.5 Results,[0],[0]
+,4.5 Results,[0],[0]
Process” outperforms the existing state-of-the-art remarkably (0.7% improvement).,4.5 Results,[0],[0]
"For more comparison and analyses, we use “DR-BiLSTM (Single)” and “DR-BiLSTM (Ensemble)” as our single and ensemble models in the rest of the paper.",4.5 Results,[0],[0]
We conducted an ablation study on our model to examine the importance and effect of each major component.,4.6 Ablation and Configuration Study,[0],[0]
"Then, we study the impact of BiLSTM dimensionality on the performance of the development set and training set of SNLI.",4.6 Ablation and Configuration Study,[0],[0]
"We investigate all settings on the development set of the SNLI dataset.
",4.6 Ablation and Configuration Study,[0],[0]
"Table 3 shows the ablation study results on the development set of SNLI along with the statistical significance test results in comparison to the proposed model, DR-BiLSTM.",4.6 Ablation and Configuration Study,[0],[0]
"We can see that all modifications lead to a new model and their differ-
ences are statistically significant with a p-value of < 0.001 over Chi square test.
",4.6 Ablation and Configuration Study,[0],[0]
Table 3 shows that removing any part from our model hurts the development set accuracy which indicates the effectiveness of these components.,4.6 Ablation and Configuration Study,[0],[0]
"Among all components, three of them have noticeable influences: max pooling, difference in the attention stage, and dependent reading.
",4.6 Ablation and Configuration Study,[0],[0]
"Most importantly, the last four study cases in Table 3 (rows 8-11) verify the main intuitions behind our proposed model.",4.6 Ablation and Configuration Study,[0],[0]
"They illustrate the importance of our proposed dependent reading strategy which leads to significant improvement, specifically in the encoding stage.",4.6 Ablation and Configuration Study,[0],[0]
"We are convinced that the importance of dependent reading in the encoding stage originates from its ability to focus on more important and relevant aspects of the sentences due to its prior knowledge of the other sentence during the encoding procedure.
",4.6 Ablation and Configuration Study,[0],[0]
Figure 3 shows the behavior of the proposed model accuracy on the training set and development set of SNLI.,4.6 Ablation and Configuration Study,[0],[0]
"Since the models are selected based on the best observed development set accuracy during the training procedure, the training accuracy curve (red, top) is not strictly increasing.",4.6 Ablation and Configuration Study,[0],[0]
Figure 3 demonstrates that we achieve the best performance with 450-dimensional BiLSTMs.,4.6 Ablation and Configuration Study,[0],[0]
"In other words, using BiLSTMs with lower dimensionality causes the model to suffer from the lack of space for capturing proper information and dependencies.",4.6 Ablation and Configuration Study,[0],[0]
"On the other hand, using higher dimensionality leads to overfitting which hurts the performance on the development set.",4.6 Ablation and Configuration Study,[0],[0]
"Hence, we use 450-dimensional BiLSTM in our proposed
model.",4.6 Ablation and Configuration Study,[0],[0]
We first investigate the performance of our models categorically.,4.7 Analysis,[0],[0]
"Then, we show a visualization of the energy function in the attention stage (Equation 3) for an instance from the SNLI test set.
",4.7 Analysis,[0],[0]
"To qualitatively evaluate the performance of our models, we design a set of annotation tags that can be extracted automatically.",4.7 Analysis,[0],[0]
This design is inspired by the reported annotation tags in Williams et al. (2017).,4.7 Analysis,[0],[0]
"The specifications of our annotation tags are as follows:
• High Overlap: premise and hypothesis sentences share more than 70% tokens.
",4.7 Analysis,[0],[0]
"• Regular Overlap: sentences share between 30% and 70% tokens.
",4.7 Analysis,[0],[0]
"• Low Overlap: sentences share less than 30% tokens.
",4.7 Analysis,[0],[0]
"• Long Sentence: either sentence is longer than 20 tokens.
",4.7 Analysis,[0],[0]
"• Regular Sentence: premise or hypothesis length is between 5 and 20 tokens.
",4.7 Analysis,[0],[0]
"• Short Sentence: either sentence is shorter than 5 tokens.
",4.7 Analysis,[0],[0]
"• Negation: negation is present in a sentence.
",4.7 Analysis,[0],[0]
"• Quantifier: either of the sentences contains one of the following quantifiers: much, enough, more, most, less, least, no, none, some, any, many, few, several, almost, nearly.
",4.7 Analysis,[0],[0]
"• Belief: either of the sentences contains one of the following belief verbs: know, believe, understand, doubt, think, suppose, recognize, forget, remember, imagine, mean, agree, disagree, deny, promise.
",4.7 Analysis,[0],[0]
"Table 4 shows the frequency of aforementioned annotation tags in the SNLI test set along with the performance (accuracy) of ESIM (Chen et al., 2017), DR-BiLSTM (Single), and DR-BiLSTM (Ensemble).",4.7 Analysis,[0],[0]
"Table 4 can be divided into four major categories: 1) gold label data, 2) word overlap, 3) sentence length, and 4) occurrence of special words.",4.7 Analysis,[0],[0]
We can see that DR-BiLSTM (Ensemble) performs the best in all categories which matches our expectation.,4.7 Analysis,[0],[0]
"Moreover, DR-BiLSTM (Single)
performs noticeably better than ESIM in most of the categories except “Entailment”, “High Overlap”, and “Long Sentence”, for which our model is not far behind (gaps of 0.2%, 0.5%, and 0.9%, respectively).",4.7 Analysis,[0],[0]
It is noteworthy that DR-BiLSTM (Single) performs better than ESIM in more frequent categories.,4.7 Analysis,[0],[0]
"Specifically, the performance of our model in “Neutral”, “Negation”, and “Quantifier” categories (improvements of 1.4%, 3.5%, and 1.9%, respectively) indicates the superiority of our model in understanding and disambiguating complex samples.",4.7 Analysis,[0],[0]
Our investigations indicate that ESIM generates somewhat uniform attention for most of the word pairs while our model could effectively attend to specific parts of the given sentences and provide more meaningful attention.,4.7 Analysis,[0],[0]
"In other words, the dependent reading strategy enables our model to achieve meaningful representations, which leads to better attention to obtain further gains on such categories like Negation and Quantifier sentences (see Section 3 in the supplementary material for additional details).
",4.7 Analysis,[0],[0]
"Finally, we show a visualization of the normalized attention weights (energy function, Equation 3) of our model in Figure 4.",4.7 Analysis,[0],[0]
"We show a sentence pair, where the premise is “Male in a blue jacket decides to lay the grass.”, and the hypothesis is “The guy in yellow is rolling on the grass.”, and its logical relationship is contradiction.",4.7 Analysis,[0],[0]
"Figure 4 indicates the model’s ability in attending to critical pairs of words like <Male, guy>, <decides, rolling>, and <lay, rolling>.",4.7 Analysis,[0],[0]
"Finally, high attention between {decides, lay} and
{rolling}, and {Male} and {guy} leads the model to correctly classify the sentence pair as contradiction (for more samples with attention visualizations, see Section 4 in the supplementary material).",4.7 Analysis,[0],[0]
We propose a novel natural language inference model (DR-BiLSTM) that benefits from a dependent reading strategy and achieves the state-of-theart results on the SNLI dataset.,5 Conclusion,[0],[0]
We also introduce a sophisticated ensemble strategy and illustrate its effectiveness through experimentation.,5 Conclusion,[0],[0]
"Moreover, we demonstrate the importance of a simple preprocessing step on the performance of our proposed models.",5 Conclusion,[0],[0]
Evaluation results show that the preprocessing step allows our DR-BiLSTM (single) model to outperform all previous single and ensemble methods.,5 Conclusion,[0],[0]
Similar superior performance is also observed for our DR-BiLSTM (ensemble) model.,5 Conclusion,[0],[0]
We show that our ensemble model outperforms the existing state-of-the-art by a considerable margin of 0.7%.,5 Conclusion,[0],[0]
"Finally, we perform an extensive analysis to demonstrate the strength and weakness of the proposed model, which would pave the way for further improvements in this domain.",5 Conclusion,[0],[0]
We present a novel deep learning architecture to address the natural language inference (NLI) task.,abstractText,[0],[0]
Existing approaches mostly rely on simple reading mechanisms for independent encoding of the premise and hypothesis.,abstractText,[0],[0]
"Instead, we propose a novel dependent reading bidirectional LSTM network (DR-BiLSTM) to efficiently model the relationship between a premise and a hypothesis during encoding and inference.",abstractText,[0],[0]
"We also introduce a sophisticated ensemble strategy to combine our proposed models, which noticeably improves final predictions.",abstractText,[0],[0]
"Finally, we demonstrate how the results can be improved further with an additional preprocessing step.",abstractText,[0],[0]
Our evaluation shows that DR-BiLSTM obtains the best single model and ensemble model results achieving the new state-of-the-art scores on the Stanford NLI dataset.,abstractText,[0],[0]
DR-BiLSTM: Dependent Reading Bidirectional LSTM for Natural Language Inference,title,[0],[0]
"Deep learning models have been used to obtain state-ofthe-art results on many tasks (Krizhevsky et al., 2012; Szegedy et al., 2014; Sutskever et al., 2014; Sundermeyer et al., 2012; Mikolov et al., 2010; Kalchbrenner & Blunsom, 2013), and in many pipelines these models have replaced the more traditional Bayesian probabilistic models (Sennrich et al., 2016).",1. Introduction,[0],[0]
"But unlike deep learning models, Bayesian probabilistic models can capture parameter uncertainty and its induced effects over predictions, capturing the models’ ignorance about the world, and able to convey their increased uncertainty on out-of-data examples.",1. Introduction,[0],[0]
"This
1University of Cambridge, UK 2The Alan Turing Institute, UK.",1. Introduction,[0],[0]
Correspondence to:,1. Introduction,[0],[0]
"Yingzhen Li <yl494@cam.ac.uk>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
information can be used, for example, to identify when a vision model is given an adversarial image (studied below), or to tackle many problems in AI safety (Amodei et al., 2016).",1. Introduction,[0],[0]
"With model uncertainty at hand, applications as farreaching as safety in self-driving cars can be explored, using models which can propagate their uncertainty up the decision making pipeline (Gal, 2016).",1. Introduction,[0],[0]
"With deterministic deep learning models this invaluable uncertainty information is often lost.
",1. Introduction,[0],[0]
"Bayesian deep learning – an approach to combining Bayesian probability theory together with deep learning – allows us to use state-of-the-art models and at the same time obtain model uncertainty (Gal, 2016; Gal & Ghahramani, 2016a).",1. Introduction,[0],[0]
"Originating in the 90s (Neal, 1995; MacKay, 1992; Denker & LeCun, 1991), Bayesian neural networks (BNNs) in particular have started gaining in popularity again (Graves, 2011; Blundell et al., 2015; HernandezLobato & Adams, 2015).",1. Introduction,[0],[0]
BNNs are standard neural networks (NNs) with prior probability distributions placed over their weights.,1. Introduction,[0],[0]
"Given observed data, inference is then performed to find what are the more likely and less likely weights to explain the data.",1. Introduction,[0],[0]
"But as easy it is to formulate BNNs, is as difficult to perform inference in them.",1. Introduction,[0],[0]
"Many approximations have been proposed over the years (Denker & LeCun, 1991; Neal, 1995; Graves, 2011; Blundell et al., 2015; Hernandez-Lobato & Adams, 2015; Hernández-Lobato et al., 2016), some more practical and some less practical.",1. Introduction,[0],[0]
"A practical approximation for inference in Bayesian neural networks should be able to scale well to large data and complex models (such as convolutional neural networks (CNNs) (Rumelhart et al., 1985; LeCun et al., 1989)).",1. Introduction,[0],[0]
"Much more importantly perhaps, it would be impractical to change existing model architectures that have been well studied, and it is often impractical to work with complex and cumbersome techniques which are difficult to explain to non-experts.",1. Introduction,[0],[0]
"Many existing approaches to obtain model confidence often do not scale to complex models or large amounts of data, and require us to develop new models for existing tasks for which we already have well performing tools (Gal, 2016).
",1. Introduction,[0],[0]
"One possible solution for practical inference in BNNs is variational inference (VI) (Jordan et al., 1999), a ubiquitous technique for approximate inference.",1. Introduction,[0],[0]
"Dropout variational distributions in particular (a mixture of two Gaussians with
small standard deviations, and with one component fixed at zero) can be used to obtain a practical inference technique (Gal & Ghahramani, 2016b).",1. Introduction,[0],[0]
"These have been used for machine vision and medical applications (Kendall & Cipolla, 2016; Kendall et al., 2015; Angermueller & Stegle, 2015; Yang et al., 2016).",1. Introduction,[0],[0]
"Dropout variational inference can be implemented by adding dropout layers (Hinton et al., 2012; Srivastava et al., 2014) before every weight layer in the NN model.",1. Introduction,[0],[0]
"Inference is then carried out by Monte Carlo (MC) integration over the variational distribution, in practice implemented by simulating stochastic forward passes through the model at test time (referred to as MC dropout).",1. Introduction,[0],[0]
"Although dropout VI is a practical technique for approximate inference, it also has some major limitations.",1. Introduction,[0],[0]
"Dropout VI can severely underestimate model uncertainty (Gal, 2016, Section 3.3.2) – a property many VI methods share (Turner & Sahani, 2011).",1. Introduction,[0],[0]
"This can lead to devastating results in applications that must rely on good uncertainty estimates such as AI safety applications.
",1. Introduction,[0],[0]
Alternative objectives to VI’s objective are therefore needed.,1. Introduction,[0],[0]
"Black-box α-divergence minimisation (Hernández-Lobato et al., 2016; Li & Turner, 2016; Minka, 2005) is a class of approximate inference methods extending on VI, approximating EP’s energy function (Minka, 2001) as well as the Hellinger distance (Hellinger, 1909).",1. Introduction,[0],[0]
These were proposed as a solution to some of the difficulties encountered with VI.,1. Introduction,[0],[0]
"However, the main difficulty with α-divergences is that the divergences are hard to use in practice.",1. Introduction,[0],[0]
"Existing inference techniques only use Gaussian approximating distributions, with the density over the approximation having to be evaluated explicitly many times.",1. Introduction,[0],[0]
"The objective offers a limited intuitive interpretation which is difficult to explain to non-experts, and of limited use for engineers (Gal, 2016, Section 2.2.2).",1. Introduction,[0],[0]
"Perhaps more importantly, current α-divergence inference techniques require existing models and code-bases to be changed radically to perform inference in the Bayesian counterpart to these models.",1. Introduction,[0],[0]
"To implement a complex CNN structure with the inference and code of (Hernández-Lobato et al., 2016), for example, one would be required to re-implement many already-implemented software tools.
",1. Introduction,[0],[0]
"In this paper we propose a re-parametrisation of the induced α-divergence objectives, and by relying on some mild assumptions (justified below), derive a simple approximate inference technique which can easily be implemented with existing models.",1. Introduction,[0],[0]
"Further, we rely on the dropout approximate variational distribution and demonstrate how inference can be done in a practical way – requiring us to only change the loss of the NN, L(θ), and to perform multiple stochastic forward passes at training time.",1. Introduction,[0],[0]
"Precisely, given l(·, ·) some standard NN loss such as cross entropy or the Euclidean loss, and {f ω̂k(xn)}Kk=1 a set of K stochastic dropout network outputs on input xn
with randomly masked weights ω̂k, our proposed objective is:
L(θ) =",1. Introduction,[0],[0]
"− 1 α ∑ n log-sum-exp [ −αl(yn, f ω̂k(xn)) ]",1. Introduction,[0],[0]
"+ L2(θ)
with α a real number, θ the set of network weights to be optimised, and an L2 regulariser over θ.",1. Introduction,[0],[0]
"By selecting α = 1 this objective directly optimises the per-point predictive log-likelihood, while picking α → 0 would focus on increasing the training accuracy, recovering VI.
",1. Introduction,[0],[0]
"Specific choices of αwill result in improved uncertainty estimates (and accuracy) compared to VI in dropout BNNs, without slowing convergence time.",1. Introduction,[0],[0]
"We demonstrate this through a myriad of applications, including an assessment of fully connected NNs in regression and classification, and an assessment of Bayesian CNNs.",1. Introduction,[0],[0]
"Finally, we study the uncertainty estimates resulting from our approximate inference technique.",1. Introduction,[0],[0]
"We show that our models’ uncertainty increases on adversarial images generated from the MNIST dataset, suggesting that these lie outside of the training data distribution.",1. Introduction,[0],[0]
This in practice allows us to tell-apart such adversarial images from non-adversarial images by examining epistemic model uncertainty.,1. Introduction,[0],[0]
We review background in Bayesian neural networks and approximate variational inference.,2. Background,[0],[0]
In the next section we discuss α-divergences.,2. Background,[0],[0]
"Given training inputs X = {x1, . . .",2.1. Bayesian Neural Networks,[0],[0]
",xN} and their corresponding outputs Y = {y1, . .",2.1. Bayesian Neural Networks,[0],[0]
.,2.1. Bayesian Neural Networks,[0],[0]
",yN}, in parametric Bayesian regression we would like to infer a distribution over parameters ω of a function y = fω(x) that could have generated the outputs.",2.1. Bayesian Neural Networks,[0],[0]
"Following the Bayesian approach, to find parameters that could have generated our data, we put some prior distribution over the space of parameters p0(ω).",2.1. Bayesian Neural Networks,[0],[0]
This distribution captures our prior belief as to which parameters are likely to have generated our outputs before observing any data.,2.1. Bayesian Neural Networks,[0],[0]
"We further need to define a probability distribution over the outputs given the inputs p(y|x, ω).",2.1. Bayesian Neural Networks,[0],[0]
"For classification tasks we assume a softmax likelihood,
p ( y|x, ω )",2.1. Bayesian Neural Networks,[0],[0]
"= Softmax (fω(x))
or a Gaussian likelihood for regression.",2.1. Bayesian Neural Networks,[0],[0]
"Given a dataset X,Y, we then look for the posterior distribution over the space of parameters: p(ω|X,Y).",2.1. Bayesian Neural Networks,[0],[0]
"This distribution captures how likely the function parameters are, given our observed data.",2.1. Bayesian Neural Networks,[0],[0]
"With it we can predict an output for a new input point x∗ by integrating
p(y∗|x∗,X,Y) = ∫ p(y∗|x∗, ω)p(ω|X,Y)dω.",2.1. Bayesian Neural Networks,[0],[0]
"(1)
One way to define a distribution over a parametric set of functions is to place a prior distribution over a neural network’s weights ω",2.1. Bayesian Neural Networks,[0],[0]
"= {Wi}Li=1, resulting in a Bayesian NN (MacKay, 1992; Neal, 1995).",2.1. Bayesian Neural Networks,[0],[0]
Given weight matrices Wi and bias vectors bi for layer,2.1. Bayesian Neural Networks,[0],[0]
"i, we often place standard matrix Gaussian prior distributions over the weight matrices, p0(Wi) = N",2.1. Bayesian Neural Networks,[0],[0]
"(Wi;0, I) and often assume a point estimate for the bias vectors for simplicity.",2.1. Bayesian Neural Networks,[0],[0]
"In approximate inference, we are interested in finding the distribution of weight matrices (parametrising our functions) that have generated our data.",2.2. Approximate Variational Inference in Bayesian Neural Networks,[0],[0]
"This is the posterior over the weights given our observables X,Y: p(ω|X,Y), which is not tractable in general.",2.2. Approximate Variational Inference in Bayesian Neural Networks,[0],[0]
Existing approaches to approximate this posterior are through variational inference (as was done in Hinton & Van Camp (1993); Barber & Bishop (1998); Graves (2011); Blundell et al. (2015)).,2.2. Approximate Variational Inference in Bayesian Neural Networks,[0],[0]
"We need to define an approximating variational distribution qθ(ω) (parametrised by variational parameters θ), and then minimise w.r.t.",2.2. Approximate Variational Inference in Bayesian Neural Networks,[0],[0]
"θ the KL divergence (Kullback & Leibler, 1951; Kullback, 1959) between the approximating distribution and the full posterior:
KL ( qθ(ω)||p(ω|X,Y) ) ∝",2.2. Approximate Variational Inference in Bayesian Neural Networks,[0],[0]
"− ∫ qθ(ω) log p(Y|X, ω)dω + KL(qθ(ω)||p0(ω))",2.2. Approximate Variational Inference in Bayesian Neural Networks,[0],[0]
=,2.2. Approximate Variational Inference in Bayesian Neural Networks,[0],[0]
"− N∑ i=1 ∫ qθ(ω) log p(yi|fω(xi))dω + KL(qθ(ω)||p0(ω)),
where A ∝ B is slightly abused here to denote equality up to an additive constant (w.r.t. variational parameters θ).",2.2. Approximate Variational Inference in Bayesian Neural Networks,[0],[0]
"Given a (deterministic) neural network, stochastic regularisation techniques in the model (such as dropout (Hinton et al., 2012; Srivastava et al., 2014)) can be interpreted as variational Bayesian approximations in a Bayesian NN with the same network structure (Gal & Ghahramani, 2016b).",2.3. Dropout Approximate Inference,[0],[0]
This is because applying a stochastic regularisation technique is equivalent to multiplying the NN weight matrices Mi by some random noise,2.3. Dropout Approximate Inference,[0],[0]
i (with a new noise realisation for each data point).,2.3. Dropout Approximate Inference,[0],[0]
"The resulting stochastic weight matrices Wi = iMi can be seen as draws from the approximate posterior over the BNN weights, replacing the deterministic NN’s weight matrices Mi.",2.3. Dropout Approximate Inference,[0],[0]
Our set of variational parameters is then the set of matrices θ = {Mi}Li=1.,2.3. Dropout Approximate Inference,[0],[0]
"For example, dropout can be seen as an approximation to Bayesian NN inference with dropout approximating distributions, where the rows of the matrices Wi distribute according to a mixture of two Gaussians with small variances
and the mean of one of the Gaussians fixed at zero.",2.3. Dropout Approximate Inference,[0],[0]
"The uncertainty in the weights induces prediction uncertainty by marginalising over the approximate posterior using Monte Carlo integration:
p(y = c|x,X,Y) = ∫",2.3. Dropout Approximate Inference,[0],[0]
"p(y = c|x, ω)p(ω|X,Y)dω
≈ ∫ p(y = c|x, ω)qθ(ω)dω
≈ 1 K K∑ k=1 p(y = c|x, ω̂k)
with ω̂k ∼ qθ(ω), where qθ(ω) is the Dropout distribution (Gal, 2016).",2.3. Dropout Approximate Inference,[0],[0]
"Given its popularity, we concentrate on the dropout stochastic regularisation technique throughout the rest of the paper, although any other stochastic regularisation technique could be used instead (such as multiplicative Gaussian noise (Srivastava et al., 2014) or dropConnect (Wan et al., 2013)).
",2.3. Dropout Approximate Inference,[0],[0]
"Dropout VI is an example of practical approximate inference, but it also underestimates model uncertainty (Gal, 2016, Section 3.3.2).",2.3. Dropout Approximate Inference,[0],[0]
"This is because minimising the KL divergence between q(ω) and p(ω|X,Y) penalises q(ω) for placing probability mass where p(ω|X,Y) has no mass, but does not penalise q(ω) for not placing probability mass at locations where p(ω|X,Y) does have mass.",2.3. Dropout Approximate Inference,[0],[0]
We next discuss α-divergences as an alternative to the VI objective.,2.3. Dropout Approximate Inference,[0],[0]
"In this section we provide a brief review of the black box alpha (BB-α, Hernández-Lobato et al. (2016)) method upon which the main derivation in this paper is based.",3. Black-box α-divergence minimisation,[0],[0]
"Consider approximating the following distribution:
p(ω) = 1
Z p0(ω) ∏ n fn(ω).
",3. Black-box α-divergence minimisation,[0],[0]
"In Bayesian neural networks context, these factors fn(ω) represent the likelihood terms p(yn|xn, ω), Z = p(Y|X), and the approximation target p(ω) is the exact posterior p(ω|X,Y).",3. Black-box α-divergence minimisation,[0],[0]
"Popular methods of approximate inference include variational inference (VI) (Jordan et al., 1999) and expectation propagation (EP) (Minka, 2001), where these two algorithms are special cases of power EP (Minka, 2004) that minimises Amari’s α-divergence (Amari, 1985)",3. Black-box α-divergence minimisation,[0],[0]
"Dα[p||q] in a local way:
Dα[p||q] = 1
α(1− α)
( 1− ∫ p(ω)αq(ω)1−αdω ) .
",3. Black-box α-divergence minimisation,[0],[0]
"We provide details of α-divergences and local approximation methods in the appendix, and in the rest of the paper we consider three special cases in this rich family:
1.",3. Black-box α-divergence minimisation,[0],[0]
Exclusive KL divergence: D0[p||q] = KL[q||p] =,3. Black-box α-divergence minimisation,[0],[0]
"Eq [ log q(ω)
p(ω)
] ;
2.",3. Black-box α-divergence minimisation,[0],[0]
"Hellinger distance: D0.5[p||q] = 4Hel2[q||p] = 2 ∫ (√ p(ω)− √ q(ω) )2 dω;
3.",3. Black-box α-divergence minimisation,[0],[0]
Inclusive KL divergence: D1[p||q] = KL[p||q] =,3. Black-box α-divergence minimisation,[0],[0]
"Ep [ log p(ω)
q(ω)
] .
",3. Black-box α-divergence minimisation,[0],[0]
"Since α = 0 is used in VI and α = 1.0 is used in EP, in later sections we will also refer to these alpha settings as the VI value, Hellinger value, and EP value, respectively.
",3. Black-box α-divergence minimisation,[0],[0]
"Power-EP, though providing a generic variational framework, does not scale with big data.",3. Black-box α-divergence minimisation,[0],[0]
"It maintains approximating factors attached to every likelihood term fn(ω), resulting in space complexity O(N) for the posterior approximation which is clearly undesirable.",3. Black-box α-divergence minimisation,[0],[0]
"The recently proposed stochastic EP (Li et al., 2015) and BB-α (HernándezLobato et al., 2016) inference methods reduce this memory overhead to O(1) by sharing these approximating factors.",3. Black-box α-divergence minimisation,[0],[0]
"Moreover, optimisation in BB-α is done by descending the so called BB-α energy function, where Monte Carlo (MC) methods and automatic differentiation are also deployed to allow fast prototyping.
",3. Black-box α-divergence minimisation,[0],[0]
"BB-α has been successfully applied to Bayesian neural networks for regression, classification (Hernández-Lobato et al., 2016) and model-based reinforcement learning (Depeweg et al., 2016).",3. Black-box α-divergence minimisation,[0],[0]
They all found that using α 6= 0,3. Black-box α-divergence minimisation,[0],[0]
often returns better approximations than the VI case.,3. Black-box α-divergence minimisation,[0],[0]
The reasons for the worse results of VI are two fold.,3. Black-box α-divergence minimisation,[0],[0]
"From the perspective of inference, due to the zero-forcing behaviour of exclusive KL discussed before, VI often fits to a local mode of the exact posterior and is over-confident in prediction.",3. Black-box α-divergence minimisation,[0],[0]
"On hyper-parameter learning point of view, as the variational lower-bound is used as a (biased) approximation to the maximum likelihood objective, the learned model could be biased towards over-simplified cases (Turner & Sahani, 2011).",3. Black-box α-divergence minimisation,[0],[0]
These problems could potentially be addressed by using α-divergences.,3. Black-box α-divergence minimisation,[0],[0]
"For example, inclusive KL encourages the coverage of the support set (referred as mass-covering), and when used in local divergence minimisation (Minka, 2005), it can fit an approximation to a mode of p(ω) with better estimates of uncertainty.",3. Black-box α-divergence minimisation,[0],[0]
"Moreover the BB-α energy provides a better approximation to the marginal likelihood as well, meaning that the learned model will be less biased and thus fitting the data distribution better (Li & Turner, 2016).",3. Black-box α-divergence minimisation,[0],[0]
"Hellinger distance seems to provide a good balance between zero-forcing and masscovering, and empirically it has been found to achieve the best performance.
",3. Black-box α-divergence minimisation,[0],[0]
"Given the success of α-divergence methods, it is a natural idea to extend these algorithms to other classes of approximations such as dropout.",3. Black-box α-divergence minimisation,[0],[0]
However this task is non-trivial.,3. Black-box α-divergence minimisation,[0],[0]
"First, the original formulation of BB-α energy is an ad hoc adaptation of power-EP energy (see appendix), which applies to exponential family q distributions only.",3. Black-box α-divergence minimisation,[0],[0]
"Second, the energy function offers a limited intuitive interpretation to non-experts, thus of limited use for practitioners.",3. Black-box α-divergence minimisation,[0],[0]
"Third and most importantly, a naive implementation of BB-α using dropout would bring in a prohibitive computational burden.",3. Black-box α-divergence minimisation,[0],[0]
"To see this, we first review the BB-α energy function in the general case (Li & Turner, 2016) given α 6= 0:
Lα(q) =",3. Black-box α-divergence minimisation,[0],[0]
"− 1
α ∑ n logEq
[( fn(ω)p0(ω) 1 N
q(ω) 1 N
)α] .",3. Black-box α-divergence minimisation,[0],[0]
"(2)
One could verify that this is the same energy function as presented in (Hernández-Lobato et al., 2016) by considering q an exponential family distribution.",3. Black-box α-divergence minimisation,[0],[0]
"In practice (2) might be intractable, hence an MC approximation is introduced:
LMCα (q) =",3. Black-box α-divergence minimisation,[0],[0]
"− 1
α ∑ n log 1 K ∑ k
[( fn(ω̂k)p0(ω̂k) 1 N
q(ω̂k) 1 N
)α] (3)
with ω̂k ∼ q(ω).",3. Black-box α-divergence minimisation,[0],[0]
This is a biased approximation as the expectation in (2) is computed before taking the logarithm.,3. Black-box α-divergence minimisation,[0],[0]
"But empirically Hernández-Lobato et al. (2016) showed that the bias introduced by the MC approximation is often dominated by the variance of the samples, meaning that the effect of the bias is negligible.",3. Black-box α-divergence minimisation,[0],[0]
When α → 0,3. Black-box α-divergence minimisation,[0],[0]
"it returns the variational free energy (the VI objective)
L0(q) = LVFE(q) = KL[q||p0]− ∑ n",3. Black-box α-divergence minimisation,[0],[0]
"Eq [log fn(ω)] , (4)
and the corresponding MC approximation LMCVFE becomes an unbiased estimator of LVFE.",3. Black-box α-divergence minimisation,[0],[0]
"Also LMCα → LMCVFE as the number of samples K → 1.
",3. Black-box α-divergence minimisation,[0],[0]
"The original paper (Hernández-Lobato et al., 2016) proposed a naive implementation which directly evaluates the MC estimation (3) with samples ω̂k ∼ q(ω).",3. Black-box α-divergence minimisation,[0],[0]
"However as discussed before, dropout implicitly samples different masked weight matrices ω̂ ∼ q for different data points.",3. Black-box α-divergence minimisation,[0],[0]
"This indicates that the naive approach, when applied to dropout approximation, would gather all these samples for all M datapoints in a mini-batch (i.e. MK sets of neural network weight matrices in total), which brings prohibitive cost if the network is wide and deep.",3. Black-box α-divergence minimisation,[0],[0]
"Interestingly, the minimisation of the variational free energy (α = 0) with the dropout approximation can be computed very efficiently.",3. Black-box α-divergence minimisation,[0],[0]
The main reason for this success is due to the additive structure of the variational free energy: no evaluation of q density is required if the “regulariser” KL[q||p0] can be computed/approximated efficiently.,3. Black-box α-divergence minimisation,[0],[0]
"In the following section we
propose an improved version of BB-α energy to allow applications with dropout and other flexible approximation structures.",3. Black-box α-divergence minimisation,[0],[0]
We propose a reparamterisation of the BB-α energy to reduce the computational overhead.,4. A New Reparameterisation of BB-α Energy,[0],[0]
"First we denote q̃(ω) as a free-form “cavity distribution” (see appendix), and write the approximate posterior q as
q(ω) = 1
Zq q̃(ω)
( q̃(ω)
p0(ω)
) α N−α
, (5)
where we assume Zq < +∞ is the normalising constant to ensure q a valid distribution.",4. A New Reparameterisation of BB-α Energy,[0],[0]
"When α/N → 0, the unnormalised density in (5) converges to q̃(ω) for every ω, and Zq → 1 by the assumption of Zq < +∞ (Van Erven & Harremoës, 2014).",4. A New Reparameterisation of BB-α Energy,[0],[0]
"Hence q → q̃ when α/N → 0, and this happens for example when we choose α → 0, or N →",4. A New Reparameterisation of BB-α Energy,[0],[0]
+∞ as well as when α grows sub-linearly to N .,4. A New Reparameterisation of BB-α Energy,[0],[0]
"Now we rewrite the BB-alpha energy in terms of q̃:
Lα(q) =",4. A New Reparameterisation of BB-α Energy,[0],[0]
"− 1
α ∑ n log
∫ ( 1
Zq q̃(ω)
( q̃(ω)
p0(ω)
) α N−α )1− αN",4. A New Reparameterisation of BB-α Energy,[0],[0]
"p0(ω) α N fn(ω) αdω
=",4. A New Reparameterisation of BB-α Energy,[0],[0]
− 1 α ∑ n (∫ q̃(ω)fn(ω),4. A New Reparameterisation of BB-α Energy,[0],[0]
α − ( 1− α N ),4. A New Reparameterisation of BB-α Energy,[0],[0]
"logZq )
",4. A New Reparameterisation of BB-α Energy,[0],[0]
"= N
α
( 1− α
N
) log ∫ q̃(ω) ( q̃(ω)
p0(ω)
)",4. A New Reparameterisation of BB-α Energy,[0],[0]
"α N−α
dω
− 1 α ∑ n logEq̃",4. A New Reparameterisation of BB-α Energy,[0],[0]
"[fn(ω)α]
= Rβ [q̃||p0]− 1
α ∑ n",4. A New Reparameterisation of BB-α Energy,[0],[0]
logEq̃,4. A New Reparameterisation of BB-α Energy,[0],[0]
"[fn(ω)α] , β = N N",4. A New Reparameterisation of BB-α Energy,[0],[0]
"− α ,
where Rβ [q̃||p0] represents the Rényi divergence (Rényi (1961), see appendix) of order β.",4. A New Reparameterisation of BB-α Energy,[0],[0]
"Furthermore, provided Rβ [q̃||p0] <",4. A New Reparameterisation of BB-α Energy,[0],[0]
"+∞ (which holds when assuming Zq < +∞), we have Rβ [q̃||p0] → KL[q̃||p0] = KL[q||p0] as α N → 0.",4. A New Reparameterisation of BB-α Energy,[0],[0]
"This means that for a constant α that scales sublinearly with N , in large data settings we can further approximate the BB-α energy as
Lα(q)",4. A New Reparameterisation of BB-α Energy,[0],[0]
≈ L̃α(q),4. A New Reparameterisation of BB-α Energy,[0],[0]
= KL[q||p0]−,4. A New Reparameterisation of BB-α Energy,[0],[0]
"1
α ∑ n logEq",4. A New Reparameterisation of BB-α Energy,[0],[0]
"[fn(ω)α] .
Note that here we also use the fact",4. A New Reparameterisation of BB-α Energy,[0],[0]
that now q ≈ q̃.,4. A New Reparameterisation of BB-α Energy,[0],[0]
Critically,4. A New Reparameterisation of BB-α Energy,[0],[0]
", the proposed reparameterisation is continuous in α, and by taking α → 0 the variational free-energy (4) is recovered.
",4. A New Reparameterisation of BB-α Energy,[0],[0]
"Given a loss function l(·, ·), e.g. l2 loss in regression or cross entropy in classification, we can define the (unnormalised) likelihood term fn(ω) ∝",4. A New Reparameterisation of BB-α Energy,[0],[0]
"p(yn|xn, ω) ∝
exp[−l(yn, fω(xn))",4. A New Reparameterisation of BB-α Energy,[0],[0]
"], e.g. see (LeCun et al., 2006)1.",4. A New Reparameterisation of BB-α Energy,[0],[0]
"Swapping fn(ω) for this last expression, and approximating the expectation over q using Monte Carlo sampling, we obtain our proposed minimisation objective:
L̃MCα",4. A New Reparameterisation of BB-α Energy,[0],[0]
"(q) = KL[q||p0] + const (6)
",4. A New Reparameterisation of BB-α Energy,[0],[0]
"− 1 α ∑ n log-sum-exp[−αl(yn, f ω̂k(xn))",4. A New Reparameterisation of BB-α Energy,[0],[0]
"]
with log-sum-exp being the log-sum-exp operator over K samples from the approximate posterior ω̂k ∼ q(ω).",4. A New Reparameterisation of BB-α Energy,[0],[0]
This objective function also approximates the marginal likelihood.,4. A New Reparameterisation of BB-α Energy,[0],[0]
"Therefore, compared to the original formulation (2), the improved version (6) is considerably simpler (both to implement and to understand), has a similar form to standard objective functions used in deep learning research, yet remains an approximate Bayesian inference algorithm.
",4. A New Reparameterisation of BB-α Energy,[0],[0]
"To gain some intuitive understanding of this objective, we observe what it reduces to for different α and K settings.",4. A New Reparameterisation of BB-α Energy,[0],[0]
"By selecting α = 1 the per-point predictive log-likelihood logEq[p(yn|xn, ω)] is directly optimised.",4. A New Reparameterisation of BB-α Energy,[0],[0]
"On the other hand, picking the VI value (α → 0) would focus on increasing the training accuracy Eq[log p(yn|xn, ω)].",4. A New Reparameterisation of BB-α Energy,[0],[0]
"The Hellinger value could be used to achieve a balance between reducing training error and improving predictive likelihood, which has been found to be desirable (HernándezLobato et al., 2016; Depeweg et al., 2016).",4. A New Reparameterisation of BB-α Energy,[0],[0]
"Lastly, for K = 1 the log-sum-exp disappears, the α’s cancel out, and the original (stochastic) VI objective is recovered.
",4. A New Reparameterisation of BB-α Energy,[0],[0]
"In summary, our proposal modifies the loss function by multiplying it by α and then performing log-sum-exp with a sum over multiple stochastic forward passes sampled from the BNN approximate posterior.",4. A New Reparameterisation of BB-α Energy,[0],[0]
The remaining KLdivergence term (between q and the prior p) can often be approximated.,4. A New Reparameterisation of BB-α Energy,[0],[0]
"It can be viewed as a regulariser added to the objective function, and reduces to L2-norm regulariser for certain popular q choices (Gal, 2016).",4. A New Reparameterisation of BB-α Energy,[0],[0]
We now provide a concrete example where the approximate distribution is defined by dropout.,4.1. Dropout BB-α,[0],[0]
"With dropout VI, MC samples are used to approximate the expectation w.r.t.",4.1. Dropout BB-α,[0],[0]
"q, which in practice is implemented as performing stochastic forward passes through the dropout network – i.e. given an input x, the input is fed through the network and a new dropout mask is sampled and applied at each dropout layer.",4.1. Dropout BB-α,[0],[0]
This gives a stochastic output – a sample from the dropout network on the input x.,4.1. Dropout BB-α,[0],[0]
"A similar approximation is used in our case as well, where to implement the MC sampling in eq.",4.1. Dropout BB-α,[0],[0]
"(6) we perform multiple stochastic forward passes
1fn(ω) does not need to be a normalised density of yn unless one would like to optimise the associated hyper parameters.
through the network.
",4.1. Dropout BB-α,[0],[0]
Recall the neural network fω(x) is parameterised by the variable ω.,4.1. Dropout BB-α,[0],[0]
"In classification, cross entropy is often used as the loss function l(y, fω(x))",4.1. Dropout BB-α,[0],[0]
= −yT,4.1. Dropout BB-α,[0],[0]
"logpω(x), where the label yn is a one-hot binary vector, and the network output pω(xn) = Softmax(fω(xn))",4.1. Dropout BB-α,[0],[0]
encodes the probability vector of class assignments.,4.1. Dropout BB-α,[0],[0]
"Applying the re-formulated BB-α energy (6) with a Bayesian equivalent of the network, we arrive at the objective function L̃MCα (q) =",4.1. Dropout BB-α,[0],[0]
∑ i pi||Mi||22,4.1. Dropout BB-α,[0],[0]
− 1 α ∑ n yTn log 1 K ∑ k (pω̂k(xn)),4.1. Dropout BB-α,[0],[0]
"α
= 1
α ∑ n",4.1. Dropout BB-α,[0],[0]
"l
( yn, 1
K ∑ k pω̂k(xn) α )",4.1. Dropout BB-α,[0],[0]
"+ ∑ i L2(Mi)
with {pω̂k(xn)}Kk=1 being K stochastic network outputs on input xn, pi equals to one minus the dropout rate of the ith layer, and the L2 regularization terms coming from an approximation to the KL-divergence (Gal, 2016).",4.1. Dropout BB-α,[0],[0]
I.e. we raise network probability outputs to the power α and average them as an input to the standard cross entropy loss.,4.1. Dropout BB-α,[0],[0]
"Taking α 6= 1 can be viewed as training the neural network with an adjusted “power” loss, regularized by an L2 norm.",4.1. Dropout BB-α,[0],[0]
"Implementing this induced loss with Keras (Chollet, 2015) is as simple as a few lines of Python.",4.1. Dropout BB-α,[0],[0]
"A code snippet is given in Figure 1, with more details in the appendix.
",4.1. Dropout BB-α,[0],[0]
"In regression problems, the loss function is defined as l(y, fω(x))",4.1. Dropout BB-α,[0],[0]
"= τ2 ||y− f
ω(x)||22 and the likelihood term can be interpreted as y ∼ N (y; fω(x), τ−1I).",4.1. Dropout BB-α,[0],[0]
"Plugging this into the energy function returns the following objective
L̃MCα (q) =",4.1. Dropout BB-α,[0],[0]
"− 1
α ∑ n log-sum-exp [ −ατ 2 ||yn − f ω̂k(xn)||22 ]",4.1. Dropout BB-α,[0],[0]
"+ ND
2 log τ + ∑ i pi||Mi||22, (7)
with {f ω̂k(xn)}Kk=1 being K stochastic forward passes on input xn.",4.1. Dropout BB-α,[0],[0]
"Again, this is reminiscent of the l2 objective in standard deep learning, and can be implemented by simply passing the input through the dropout network multiple times, collecting the stochastic outputs, and feeding the set of outputs through our new BB-alpha loss function.",4.1. Dropout BB-α,[0],[0]
We test the reparameterised BB-α on Bayesian NNs with the dropout approximation.,5. Experiments,[0],[0]
"We assess the proposed inference in regression and classification tasks on standard benchmarking datasets, comparing different values of α.",5. Experiments,[0],[0]
This last experiment leads us to propose a technique that could be used to identify adversarial image attacks.,5. Experiments,[0],[0]
In the appendix we further provide a study of run time trade-off.,5. Experiments,[0],[0]
The first experiment considers Bayesian neural network regression with approximate posterior induced by dropout.,5.1. Regression,[0],[0]
We use benchmark UCI datasets2 that have been tested in related literature.,5.1. Regression,[0],[0]
"The model is a single-layer neural network with 50 ReLU units for all datasets except for Protein and Year, which use 100 units.",5.1. Regression,[0],[0]
"We consider α ∈ {0.0, 0.5, 1.0} in order to examine the effect of masscovering/zero-forcing behaviour in dropout.",5.1. Regression,[0],[0]
MC approximation with K = 10 samples is also deployed to compute the energy function.,5.1. Regression,[0],[0]
"Other initialisation settings are largely taken from (Li & Turner, 2016).
",5.1. Regression,[0],[0]
"We summarise the test negative log-likelihood (LL) and RMSE with standard error (across different random splits, the lower the better) for selected datasets in Figure 2 and 3, respectively.",5.1. Regression,[0],[0]
The full results are provided in the appendix.,5.1. Regression,[0],[0]
"Although optimal α may vary for different datasets, using non-VI values has significantly improved the test-LL performances, while remaining comparable in test error metric.",5.1. Regression,[0],[0]
"In particular, α = 0.5 produced overall good results for both test LL and RMSE, which is consistent with previous findings.",5.1. Regression,[0],[0]
"We also compare with a BNN with a Gaussian approximation (VI-G) (Li & Turner, 2016), a BNN with HMC, and a sparse Gaussian process model with 50 inducing points (Bui et al., 2016).",5.1. Regression,[0],[0]
"In test-LL metric our best dropout model out-performs the Gaussian approximation method on almost all datasets, and for some datasets is on par with HMC which is the current gold standard for Bayesian neural works, and with the GP model that is known to be superior in regression.",5.1. Regression,[0],[0]
"We further experiment with a classification task, comparing the accuracy of the various α values on the MNIST benchmark (LeCun & Cortes, 1998).",5.2. Classification,[0],[0]
We assessed a fully connect NN with 2 hidden layers and 100 units in each layer.,5.2. Classification,[0],[0]
"We used dropout probability 0.5 and α ∈ {0, 0.5, 1}.",5.2. Classification,[0],[0]
"Again, we use K = 10 samples at training time for all α values,
2http://archive.ics.uci.edu/ml/datasets.",5.2. Classification,[0],[0]
"html
and Ktest = 100 samples at test time.",5.2. Classification,[0],[0]
"We use weight decay 10−6, which is equivalent to prior lengthscale l2 = 0.1",5.2. Classification,[0],[0]
"(Gal & Ghahramani, 2016b).",5.2. Classification,[0],[0]
We repeat each experiment three times and plot mean and standard error.,5.2. Classification,[0],[0]
Test RMSE as well as test log likelihood are given in Figure 4.,5.2. Classification,[0],[0]
"As can be seen, Hellinger value α = 0.5 gives best test RMSE, with test log likelihood matching that of the EP value α = 1.",5.2. Classification,[0],[0]
"The VI value α = 0 under-performs according to both metrics.
",5.2. Classification,[0],[0]
We next assess a convolutional neural network model (CNN).,5.2. Classification,[0],[0]
"For this experiment we use the standard CNN example given in (Chollet, 2015) with 32 convolution filters, 100 hidden units at the top layer, and dropout probability 0.5 before each fully-connected layer.",5.2. Classification,[0],[0]
Other settings are as before.,5.2. Classification,[0],[0]
Average test accuracy and test log likelihood are given in Figure 5.,5.2. Classification,[0],[0]
"In this case, VI value α = 0 seems to supersede the EP value α = 1, and performs similarly to the Hellinger value α = 0.5 according to both metrics.",5.2. Classification,[0],[0]
The third set of experiments considers adversarial attacks on dropout-trained Bayesian neural networks.,5.3. Detecting Adversarial Examples,[0],[0]
"We test the hypothesis that certain techniques for generating adversarial examples will give images that lie outside of the image
manifold, i.e. far from the data distribution (note though that there exist techniques that will guarantee the images staying near the data manifold, by minimising the perturbation used to construct the adversarial example).",5.3. Detecting Adversarial Examples,[0],[0]
"By assessing the BNN uncertainty, we should see increased uncertainty for adversarial images if they indeed lie outside of the training data distribution.",5.3. Detecting Adversarial Examples,[0],[0]
The tested models are fully connected networks with 3 hidden layers of 1000 units trained using dropout rate 0.5 and different alpha values.,5.3. Detecting Adversarial Examples,[0],[0]
These models are also compared to a benchmark MLP with the same architecture but trained by maximum likelihood.,5.3. Detecting Adversarial Examples,[0],[0]
"The adversarial examples are generated on MNIST test data that is normalised to be in the range [0, 1].",5.3. Detecting Adversarial Examples,[0],[0]
"For the dropout trained networks we perform MC dropout at test time with Ktest = 10 MC samples.
",5.3. Detecting Adversarial Examples,[0],[0]
"The first attack in consideration is the Fast Gradient Sign (FGS) method (Goodfellow et al., 2014).",5.3. Detecting Adversarial Examples,[0],[0]
"This is an untargeted attack, which attempts to reduces the maximum value of the predicted class label probability
xadv = x− η · sgn(∇x max y log p(y|x)).
",5.3. Detecting Adversarial Examples,[0],[0]
"We use the single gradient step FGS implemented in Cleverhans (Papernot et al., 2016) with the stepsize η varied
between 0.0 and 0.5.",5.3. Detecting Adversarial Examples,[0],[0]
"The left panel in Figure 6 demonstrates the classification accuracy on adversarial examples, which shows that the dropout networks, especially the one trained with α = 1.0, are significantly more robust to adversarial attacks compared to the deterministic NN.",5.3. Detecting Adversarial Examples,[0],[0]
"For example, for η = 0.1 the adversarial samples still visually close to the original class, and the BNN trained with α = 0.0 achieves an accuracy level almost 3 times higher than the MLP and around 20% higher than the VI-trained version.",5.3. Detecting Adversarial Examples,[0],[0]
"More interestingly, the test data examples and adversarial images can be told-apart by investigating the uncertainty representation of the dropout models.",5.3. Detecting Adversarial Examples,[0],[0]
"In the right panel of Figure 6 we depict the predictive entropy computed on the neural network output probability vector, and show example corresponding adversarial images below the axis for each corresponding stepsize.",5.3. Detecting Adversarial Examples,[0],[0]
"Clearly the deterministic NN model produces over-confident predictions on adversarial samples, e.g. it predicts the wrong label very confidently even when the input is still visually close to digit “7” (η = 0.2).",5.3. Detecting Adversarial Examples,[0],[0]
"While dropout models, though producing wrong labels, are very uncertain about their predictions.",5.3. Detecting Adversarial Examples,[0],[0]
This uncertainty keeps increasing as we move away from the data manifold.,5.3. Detecting Adversarial Examples,[0],[0]
"Hence the dropout networks are much more immunised from noise-corrupted inputs, as they can be detected using uncertainty estimates in this example.
",5.3. Detecting Adversarial Examples,[0],[0]
"The second attack we consider is a targeted version of FGS (Goodfellow et al., 2014; Carlini & Wagner, 2016), which maximises the predictive probability of a selected class instead.",5.3. Detecting Adversarial Examples,[0],[0]
"As an example, we fix class 0 as the target and apply the iterative gradient-base attack to all non-zero digits in test data.",5.3. Detecting Adversarial Examples,[0],[0]
"At step t, the adversarial output is computed as
xtadv = x t−1 adv + η · sgn(∇x log p(ytarget|x t−1 adv )),
where the stepsize η is fixed at 0.01 in this case.",5.3. Detecting Adversarial Examples,[0],[0]
"Results are presented in the left panel of Figure 7, and again dropout
trained models are more robust to this attack compared with the MLP.",5.3. Detecting Adversarial Examples,[0],[0]
"Similarly these adversarial examples could be detected by the Bayesian neural networks’ uncertainty, by examining the predictive entropy.",5.3. Detecting Adversarial Examples,[0],[0]
"By visually inspecting the generated adversarial examples in the right panel of Figure 7, it is clear that the MLP overconfidently classifies a digit 7 to class",5.3. Detecting Adversarial Examples,[0],[0]
0.,5.3. Detecting Adversarial Examples,[0],[0]
"On the other hand, the dropout models are still fairly uncertain about their predictions even after 40 gradient steps.",5.3. Detecting Adversarial Examples,[0],[0]
"More interestingly, running this iterative attack on dropout models produces a smooth interpolation between different digits, and when the model is confident on predicting the target class, the corresponding adversarial images are visually close to digit zero.
",5.3. Detecting Adversarial Examples,[0],[0]
These initial results suggest that assessing the epistemic uncertainty of classification models can be used as a viable technique to identify adversarial examples.,5.3. Detecting Adversarial Examples,[0],[0]
"We would note though that we used this experiment to demonstrate our techniques’ uncertainty estimates, and much more research is needed to solve the difficulties faced with adversarial inputs.",5.3. Detecting Adversarial Examples,[0],[0]
We presented a practical extension of the BB-alpha objective which allows us to use the technique with dropout approximating distributions.,6. Conclusions,[0],[0]
"The technique often supersedes existing approximate inference techniques (even sparse Gaussian processes), and is easy to implement.",6. Conclusions,[0],[0]
A code snippet for our induced loss is given in the appendix.,6. Conclusions,[0],[0]
"We thank Rich Turner, Nicolas Papernot, and the reviewers for comments.",Acknowledgements,[0],[0]
YL thanks the Schlumberger Foundation FFTF fellowship for supporting her PhD study.,Acknowledgements,[0],[0]
"To obtain uncertainty estimates with real-world Bayesian deep learning models, practical inference approximations are needed.",abstractText,[0],[0]
"Dropout variational inference (VI) for example has been used for machine vision and medical applications, but VI can severely underestimates model uncertainty.",abstractText,[0],[0]
"Alpha-divergences are alternative divergences to VI’s KL objective, which are able to avoid VI’s uncertainty underestimation.",abstractText,[0],[0]
"But these are hard to use in practice: existing techniques can only use Gaussian approximating distributions, and require existing models to be changed radically, thus are of limited use for practitioners.",abstractText,[0],[0]
"We propose a re-parametrisation of the alpha-divergence objectives, deriving a simple inference technique which, together with dropout, can be easily implemented with existing models by simply changing the loss of the model.",abstractText,[0],[0]
We demonstrate improved uncertainty estimates and accuracy compared to VI in dropout networks.,abstractText,[0],[0]
"We study our model’s epistemic uncertainty far away from the data using adversarial images, showing that these can be distinguished from non-adversarial images by examining our model’s uncertainty.",abstractText,[0],[0]
Dropout Inference in Bayesian Neural Networks with Alpha-divergences,title,[0],[0]
"Proceedings of NAACL-HLT 2013, pages 168–178, Atlanta, Georgia, 9–14 June 2013. c©2013 Association for Computational Linguistics",text,[0],[0]
"Topic models aid exploration of the main thematic elements of large text corpora by revealing latent structure and producing a high level semantic view (Blei et al., 2003).",1 Introduction,[0],[0]
"Topic models have been used for understanding the contents of a corpus and identifying interesting aspects of a collection for more indepth analysis (Talley et al., 2011; Mimno, 2011).",1 Introduction,[0],[0]
"While standard topic models assume a flat semantic structure, there are potentially many dimensions of a corpus that contribute to word choice,
such as sentiment, perspective and ideology (Mei et al., 2007; Paul and Girju, 2010; Eisenstein et al., 2011).",1 Introduction,[0],[0]
"Rather than studying these factors in isolation, multi-dimensional topic models can consider multiple factors jointly.
",1 Introduction,[0],[0]
"Paul and Dredze (2012b) introduced factorial LDA (f-LDA), a general framework for multidimensional text models that capture an arbitrary number of factors (explained in §3).",1 Introduction,[0],[0]
"While a standard topic model learns distributions over “topics” in documents, f-LDA learns distributions over combinations of multiple factors (e.g. topic, perspective) called tuples (e.g. (HEALTHCARE,LIBERAL)).",1 Introduction,[0],[0]
"While f-LDA can model factors without supervision, it has not been used in situations where the user has prior information about the factors.
",1 Introduction,[0],[0]
"In this paper we consider a setting where the user has prior knowledge about the end application: mining recreational drug trends from user forums, an important clinical research problem (§2).",1 Introduction,[0],[0]
"We show how to incorporate available information from these forums into f-LDA as a novel hierarchical prior over the model parameters, guiding the model toward the desired output (§3.1).
",1 Introduction,[0],[0]
"We then demonstrate the model’s utility in exploring a corpus in a targeted manner by using it to automatically extract interesting sentences from the text, a simple form of extractive multi-document summarization (Goldstein et al., 2000).",1 Introduction,[0],[0]
"In the same way that topic models can be used for aspectspecific summarization (Titov and McDonald, 2008; Haghighi and Vanderwende, 2009), we use f-LDA to extract snippets corresponding to fine-grained information patterns.",1 Introduction,[0],[0]
"Our results demonstrate that our multi-dimensional modeling approach targets more informative text than a simpler model (§4).
168",1 Introduction,[0],[0]
Recreational drug use imposes a significant burden on the health infrastructure of the United States and other countries.,2 Analyzing Drug Trends on the Web,[0],[0]
"Accurate information on drugs, usage profiles and side effects are necessary for supporting a range of healthcare activities, such as addiction treatment programs, toxin diagnosis, prevention and awareness campaigns, and public policy.",2 Analyzing Drug Trends on the Web,[0],[0]
"These activities rely on up-to-date information on drug trends, but it is increasingly difficult to keep up with current drug information, as distribution and information-sharing of novel drugs is easier than ever via the web (Wax, 2002).",2 Analyzing Drug Trends on the Web,[0],[0]
"For the third consecutive year, a record number of new drugs (49) were detected in Europe in 2011 (EMCDDA, 2012).",2 Analyzing Drug Trends on the Web,[0],[0]
"About two-thirds of these new drugs were synthetic cannabinoids (used as legal marijuana substitutes), which led to 11,000 hospitalizations in the U.S. in 2010 (SAMHSA, 2012).",2 Analyzing Drug Trends on the Web,[0],[0]
"Treatment is complicated by the fact that novel substances like these may have unknown side effects and other properties.
",2 Analyzing Drug Trends on the Web,[0],[0]
"Accurate information on drug trends can be obtained by speaking directly with users, e.g. focus groups and interviews (Reyes et al., 2012; Hout and Bingham, 2012), but such studies are slow and costly, and can fail to identify the emergence of new drug classes, such as mephedrone (Dunn et al., 2011).",2 Analyzing Drug Trends on the Web,[0],[0]
"More recently, researchers have begun to recognize clinical value in information obtained from the web (Corazza et al., 2011).",2 Analyzing Drug Trends on the Web,[0],[0]
"By (manually) analyzing YouTube videos, Drugs-Forum (discussed below), and other social media websites and online communities, researchers have uncovered details about the use, effects, and popularity of a variety of new and emerging drugs (Morgan et al., 2010; Corazza et al., 2012; Gallagher et al., 2012), and comprehensive drug reviews now include nonstandard sources such as web forums in addition to standard sources (Hill and Thomas, 2011).
",2 Analyzing Drug Trends on the Web,[0],[0]
Organizing and understanding forums requires significant effort.,2 Analyzing Drug Trends on the Web,[0],[0]
We propose automated tools to aid in the exploration and analysis of these data.,2 Analyzing Drug Trends on the Web,[0],[0]
"While topic models are a natural fit for corpus exploration (Eisenstein et al., 2012; Chaney and Blei, 2012), and have been used for similar public health applications (Paul and Dredze, 2011), online forums can be organized in many ways beyond topic.",2 Analyzing Drug Trends on the Web,[0],[0]
"Guided by do-
main experts, we seek to model forums as a combination of drug type, route of intake (oral, injection, etc.)",2 Analyzing Drug Trends on the Web,[0],[0]
"and aspect (cultural settings, drug chemistry, etc.)",2 Analyzing Drug Trends on the Web,[0],[0]
"A multi-dimensional topic model can jointly capture these factors, providing a more informative understanding of the data, and can be used to produce fine-grained information such as the effects of taking a particular drug orally.",2 Analyzing Drug Trends on the Web,[0],[0]
Our hope is that models such as f-LDA can lead to exploratory tools that aide researchers in learning about new drugs.,2 Analyzing Drug Trends on the Web,[0],[0]
"Our data set is taken from drugs-forum.com, a site active for more than 10 years with over 100,000 members and more than 1 million monthly readers.",2.1 Corpus: Drugs-Forum,[0],[0]
"The site is an information hub where people can freely discuss recreational drugs with psychoactive effects, ranging from coffee to heroin, hosting information and discussions on specific drugs, as well as drug-related politics, law, news, recovery and addiction.",2.1 Corpus: Drugs-Forum,[0],[0]
"With current information on a variety of drugs and an extensive archive, Drugs-Forum provides an ideal information source for public health researchers (Corazza et al., 2012).
",2.1 Corpus: Drugs-Forum,[0],[0]
"Discussion threads are organized into numerous forums, including drugs, the law, addiction, etc.",2.1 Corpus: Drugs-Forum,[0],[0]
"Since we are modeling drug use, we focus on the drug forums.",2.1 Corpus: Drugs-Forum,[0],[0]
"Each thread is assigned to a specific forum or subforum (drug) and each thread has a user specified tag, which can indicate categories like “Effects” as well as routes of administration like “Oral.”",2.1 Corpus: Drugs-Forum,[0],[0]
"We organized the tags and subforum categorizations into factors and components, as shown in Table 1.",2.1 Corpus: Drugs-Forum,[0],[0]
We make use of these tags in §3.1.,2.1 Corpus: Drugs-Forum,[0],[0]
"Clinical researchers are interested in specific information about drug usage, including drug type, route of administration, and other aspects of drug use (e.g. dosage, side effects).",3 Multi-Dimensional Text Models,[0],[0]
"Rather than considering these factors independently, we would like to model these in a way that can capture interesting interactions between all three factors, because the effects and other aspects of drugs can vary by route of administration.",3 Multi-Dimensional Text Models,[0],[0]
"Oral consumption of drugs often produces longer lasting but milder effects than injection or smoking, for example.",3 Multi-Dimensional Text Models,[0],[0]
"Many mephedrone users report nose bleeds and nasal pain as a health effect of snorting the drug: this could be modeled as the triple (MEPHEDRONE,SNORTING,HEALTH), a particular combination of all three factors.
",3 Multi-Dimensional Text Models,[0],[0]
"To this end, we utilize the multi-dimensional text model factorial LDA (f-LDA) (Paul and Dredze, 2012b), which jointly models multiple semantic factors or dimensions.",3 Multi-Dimensional Text Models,[0],[0]
"In this section we summarize fLDA, then we describe an extension which incorporates user-generated metadata into the model (§3.1).
",3 Multi-Dimensional Text Models,[0],[0]
"In a standard topic model such as LDA (Blei et al., 2003), each word token is associated with a latent “topic” variable.",3 Multi-Dimensional Text Models,[0],[0]
"f-LDA is conceptually similar to LDA except that rather than a single topic variable, each token is associated with a K-dimensional vector of latent variables.",3 Multi-Dimensional Text Models,[0],[0]
"In a three-dimensional fLDA model, each token has three latent variables— drug, route, and aspect in this case.
",3 Multi-Dimensional Text Models,[0],[0]
"In f-LDA, each document has a distribution over all possible K-tuples (rather than topics), and each K-tuple is associated with its own word distribution.",3 Multi-Dimensional Text Models,[0],[0]
"Under this model, words are generated by first sampling a tuple from the document’s tuple distribution, then sampling a word from that tuple’s word distribution.",3 Multi-Dimensional Text Models,[0],[0]
"In our threedimensional model, we will consider triples such as (CANNABIS,SMOKING,EFFECTS).
",3 Multi-Dimensional Text Models,[0],[0]
"Formally, each document has a distribution θ(d) over triples, and each token is associated with a latent vector ~z of sizeK=3.",3 Multi-Dimensional Text Models,[0],[0]
"(We’ll describe the model in terms of the three factors we are modeling in this paper, but f-LDA generalizes toK dimensions.)",3 Multi-Dimensional Text Models,[0],[0]
"The Cartesian product of the three factors forms a set of triples and the vector ~z references three discrete components to form a triple ~t = (t1, t2, t3).",3 Multi-Dimensional Text Models,[0],[0]
"The car-
dinality of each dimension (denoted Zk) is the number of drugs, routes, and aspects, as shown in Table 1.",3 Multi-Dimensional Text Models,[0],[0]
"Each triple has a corresponding word distribution φ~t. The graphical model is shown in Figure 1.
",3 Multi-Dimensional Text Models,[0],[0]
"One would expect that triples that have components in common should have similar word distributions: (CANNABIS,SMOKING,EFFECTS) is expected to have some commonalities with (CANNABIS,ORAL,EFFECTS).",3 Multi-Dimensional Text Models,[0],[0]
"f-LDA models this intuition by sharing parameters across priors for triples which share components: all triples with CANNABIS as the drug include cannabis-specific parameters in the prior, and all triples with SMOKING as the route have smoking-specific parameters.",3 Multi-Dimensional Text Models,[0],[0]
"Formally, φ~t (the word distribution for tuple ~t) has a Dirichlet(ω̂(~t))",3 Multi-Dimensional Text Models,[0],[0]
"prior, where for each word w in the vector, ω̂( ~t) w is a log-linear function:
ω̂( ~t )",3 Multi-Dimensional Text Models,[0],[0]
"w , exp ( ω(B)+ω(0)w +ω",3 Multi-Dimensional Text Models,[0],[0]
(drug),3 Multi-Dimensional Text Models,[0],[0]
t1w,3 Multi-Dimensional Text Models,[0],[0]
+ω (route) t2w,3 Multi-Dimensional Text Models,[0],[0]
+ω (aspect) t3w ),3 Multi-Dimensional Text Models,[0],[0]
"(1) where ω(B) is a corpus-wide precision scalar (the bias), ω(0)w is a corpus-specific bias for word w, and ω
(k) tkw
is a bias parameter for word w for component tk of the kth factor.",3 Multi-Dimensional Text Models,[0],[0]
"That is, each drug, route, and aspect has a weight vector over the vocabulary, and the prior for a particular triple is influenced by the weight vectors of each of the three factors.",3 Multi-Dimensional Text Models,[0],[0]
The ω parameters are all independent and normally distributed around 0,3 Multi-Dimensional Text Models,[0],[0]
"(effectively L2 regularization).
",3 Multi-Dimensional Text Models,[0],[0]
"The prior over each document’s distribution over triples has a similar log-linear prior, where weights for each factor are combined to influence the distribution.",3 Multi-Dimensional Text Models,[0],[0]
"Under our model, θ(d) is drawn from Dirichlet(B · α̂(d)), where · denotes an element-wise product between B (described below) and α̂(d), with
α̂ (d) ~t for each triple ~t defined as:
α̂ (d) ~t
, exp ( α(B)",3 Multi-Dimensional Text Models,[0],[0]
"+α
(D,drug) t1",3 Multi-Dimensional Text Models,[0],[0]
"+α (d,drug)",3 Multi-Dimensional Text Models,[0],[0]
"t1
+α (D,route)",3 Multi-Dimensional Text Models,[0],[0]
"t2 +α (d,route)",3 Multi-Dimensional Text Models,[0],[0]
"t2 +α (D,aspect) t3",3 Multi-Dimensional Text Models,[0],[0]
"+α (d,aspect) t3 )",3 Multi-Dimensional Text Models,[0],[0]
"(2)
Similar to the ω formulation, α(B) is a global bias parameter, while the αD vectors are corpuswide weight vectors and αd are document-specific weight vectors over the components of each factor.",3 Multi-Dimensional Text Models,[0],[0]
"Structuring the prior in this way models the intuition that if a triple with a particular component has high probability, other triples containing that component are likely to also have high probability.",3 Multi-Dimensional Text Models,[0],[0]
"For example, if a message discusses triples of the form (CANNABIS,*,EFFECTS), it is more likely to discuss (CANNABIS,*,HEALTH) than (COCAINE,*,HEALTH), because the message is about cannabis.
",3 Multi-Dimensional Text Models,[0],[0]
"Finally, B is a 3-dimensional array that encodes a sparsity pattern over the space of possible triples.",3 Multi-Dimensional Text Models,[0],[0]
This is used to accommodate triples that can be generated by the model but are not supported by the data.,3 Multi-Dimensional Text Models,[0],[0]
"For example, not all routes of administration may be applicable to certain drugs, or certain aspects of a drug may happen to not be discussed in the forum.",3 Multi-Dimensional Text Models,[0],[0]
"Each element b~t of the array is a real-valued scalar in (0, 1) which is multiplied with α̂(d)~t to adjust the prior for that triple.",3 Multi-Dimensional Text Models,[0],[0]
"If the b value is near 0 for a particular triple, then it will have very low prior probability.",3 Multi-Dimensional Text Models,[0],[0]
"The b values have Beta(γ0,γ1) priors (γ < 1) which encourage them to be near 0 or 1, so that they function as binary variables.
",3 Multi-Dimensional Text Models,[0],[0]
"Posterior inference and parameter estimation consist of a Monte Carlo EM algorithm that alternates between an iteration of collapsed Gibbs sampler on the ~z variables (E-step), and an iteration of gradient ascent on the α and ω hyperparameters (M-step).",3 Multi-Dimensional Text Models,[0],[0]
See Paul and Dredze (2012b) for more details.,3 Multi-Dimensional Text Models,[0],[0]
"In an unsupervised setting, there is no reason f-LDA would actually infer parameters corresponding to the three factors we have been describing.",3.1 Tags and Word Priors,[0],[0]
"However, the forums include metadata that can help guide the model: the messages are organized into forums corresponding to drug type (factor 1), and some threads
are tagged with labels corresponding to routes of administration and other aspects (factors 2 and 3).",3.1 Tags and Word Priors,[0],[0]
"Tags for aspects are manually grouped into components: e.g. USAGE (tags: Dose, Storing, Weight).",3.1 Tags and Word Priors,[0],[0]
"Table 1 shows the factors and components in our model.
",3.1 Tags and Word Priors,[0],[0]
One could simply use these tags as labels in a simple supervised model—this will be our experimental baseline (§4.1).,3.1 Tags and Word Priors,[0],[0]
"However, this approach has limitations in that most documents are missing labels (less than a third of our corpus contains one of the labels in Table 1) and many messages discuss several components, not just the one implied by the tag.",3.1 Tags and Word Priors,[0],[0]
"For example, a message tagged “Side effects” may talk about both side effects and dosage.",3.1 Tags and Word Priors,[0],[0]
"While a supervised classifier may attribute all words to a single tag, f-LDA learns per-token assignments.
",3.1 Tags and Word Priors,[0],[0]
We will instead use the tags to inform the priors over our f-LDA word distribution parameters.,3.1 Tags and Word Priors,[0],[0]
We do this with a two-stage approach.,3.1 Tags and Word Priors,[0],[0]
"First, we use the tags to train parameters of a related but simplified model.",3.1 Tags and Word Priors,[0],[0]
"We then use the learned parameters as priors over the corresponding f-LDA parameters.
",3.1 Tags and Word Priors,[0],[0]
"In particular, we will place priors on the ω vectors, the Dirichlet hyperparameters which influence the word distributions.",3.1 Tags and Word Priors,[0],[0]
"Suppose that we are given a vector η(0) which is believed to contain desirable values for ω(0), the weight vector over words in the corpus, and similarly we are given vectors η(f)i over the vocabulary for the ith component of factor f , which are believed to be good values for ω(f)i .",3.1 Tags and Word Priors,[0],[0]
"One option
is to fix ω as η, forcing the component weights to match the provided weights.",3.1 Tags and Word Priors,[0],[0]
"However, in our case η will only be an approximation of the optimal component parameters since it is estimated from incomplete data (only some messages have tags) and the η vectors are learned using an approximate model (see below).",3.1 Tags and Word Priors,[0],[0]
"Instead, these weight vectors will merely guide learning as prior knowledge over model parameters ω.",3.1 Tags and Word Priors,[0],[0]
"While f-LDA assumes each ω is drawn from a 0-mean Gaussian, we alter the means of the appropriate ω parameters to use η.
ω(0)w ∼ N (η(0)w , σ2);ω (k) iw ∼ N (η (k) iw , σ 2) (3) Recall that ω(0)w are corpus-wide bias parameters for each word and ω(k)iw are component-specific parameters for each word.",3.1 Tags and Word Priors,[0],[0]
"This yields a hierarchical prior in which η parameterizes the prior over ω, while ω parameterizes the prior over φ (the word distributions).",3.1 Tags and Word Priors,[0],[0]
The resulting ω parameters can vary from the provided priors to adapt to the data.,3.1 Tags and Word Priors,[0],[0]
"An example of learned parameters is shown in Figure 2, illustrating the hierarchical process behind this model.
",3.1 Tags and Word Priors,[0],[0]
"Learning the Priors In various applications, priors can come from many different sources, such as labeled data (Jagarlamudi et al., 2012).",3.1 Tags and Word Priors,[0],[0]
We learn the prior means η from tagged messages.,3.1 Tags and Word Priors,[0],[0]
"However, these parameters imply a latent division of responsibility for observed words: some are present because of the tag while others are general words in the corpus.",3.1 Tags and Word Priors,[0],[0]
"As a result, they must be estimated.
",3.1 Tags and Word Priors,[0],[0]
"We learn these parameters from the tagged messages using SAGE, which model words in a document as combinations of background and topic word distributions.",3.1 Tags and Word Priors,[0],[0]
"Eisenstein et al. (2011) present SAGE models for Naive Bayes (one class per document), admixture models (one class per token), and admixture models where tokens come from multiple factors.",3.1 Tags and Word Priors,[0],[0]
"We combine the first and third models, such that a document has multiple factors which are given as labels across the entire document—the drug type and the tag, which could correspond to a component of either the route or aspect factors.",3.1 Tags and Word Priors,[0],[0]
"We posit the following model of text generation per document:
P (word w|drug = i, factorf = j) (4)
= exp(η
(0) w",3.1 Tags and Word Priors,[0],[0]
"+ η (drug) iw + η (f) jw )∑
w′ exp(η (0) w′",3.1 Tags and Word Priors,[0],[0]
+ η (drug) iw′ + η,3.1 Tags and Word Priors,[0],[0]
"(f) jw′)
",3.1 Tags and Word Priors,[0],[0]
"This log-linear model has a similar form as Eq. 1, but with two factors instead of three, and it is a distribution rather than a Dirichlet vector.",3.1 Tags and Word Priors,[0],[0]
"As in SAGE, we fix η(0) to be the observed vector of corpus log-frequencies over the vocabulary, which acts as an “overall” weight vector, while parameter estimation yields η(f)i , the logit parameters for the ith component of factor",3.1 Tags and Word Priors,[0],[0]
f .1,3.1 Tags and Word Priors,[0],[0]
"These parameters are then used as the mean of the Gaussian priors over ω.
",3.1 Tags and Word Priors,[0],[0]
Standard optimization methods can be used to estimate these parameters.,3.1 Tags and Word Priors,[0],[0]
"The partial derivative of the likelihood with respect to the parameter η(drug)iw is:
∂
∂η (drug) iw = ∑ f ∑ j∈f c(i, j, w)− π(i, j, w)c(i, j, ∗)
(5) where c(i, j, w) is the number of times word w appears in documents labeled with i (drug) and j (tag), and π(i, j, w) denotes the probability given by (4).",3.1 Tags and Word Priors,[0],[0]
The partial derivative of each η(f)j is similar.,3.1 Tags and Word Priors,[0],[0]
Our corpus consists of messages from drugs-forum.com (§2.1).,4 Experiments with Topic Modeling for Extractive Summarization,[0],[0]
"The site categorizes threads into many forums and subforums, including some on specific drugs, which are categorized hierarchically.",4 Experiments with Topic Modeling for Extractive Summarization,[0],[0]
"We treated higher-level categories with pharmacologically similar drugs as a single drug type (e.g. OPIOIDS, AMPHETAMINES); for others we took the finest-granularity subforum as the drug type.",4 Experiments with Topic Modeling for Extractive Summarization,[0],[0]
We selected 22 popular drugs and from these forums we crawled 410K messages.,4 Experiments with Topic Modeling for Extractive Summarization,[0],[0]
We selected a subset of tags to form components for the route and aspect factors.,4 Experiments with Topic Modeling for Extractive Summarization,[0],[0]
(Some tags were too general or infrequent to be useful.),4 Experiments with Topic Modeling for Extractive Summarization,[0],[0]
A list of the tags and drugs used appears in Table 1.,4 Experiments with Topic Modeling for Extractive Summarization,[0],[0]
"We also included a GENERAL component in the latter two factors to model word usage which does not pertain to a particular route or aspect; the prior parameters η for these components were simply set to 0.
",4 Experiments with Topic Modeling for Extractive Summarization,[0],[0]
We wish to demonstrate that our modified f-LDA model can be used to discover useful information in the text.,4 Experiments with Topic Modeling for Extractive Summarization,[0],[0]
"One way to demonstrate this is by using the model to extract relevant snippets of text from the
1SAGE models sparsity on the weights via a Laplacian prior.",4 Experiments with Topic Modeling for Extractive Summarization,[0],[0]
"Such sparsity is not modeled in f-LDA, so we ignore this here.
",4 Experiments with Topic Modeling for Extractive Summarization,[0],[0]
"forums, which will form the basis of our evaluation experiments.",4 Experiments with Topic Modeling for Extractive Summarization,[0],[0]
"Our goal is not to build a complete summarization system, but rather to use the model to direct researchers to interesting messages.
",4 Experiments with Topic Modeling for Extractive Summarization,[0],[0]
"While we model all 22 drugs, our summarization experiments will focus on five drugs which have been studied only relatively recently: mephedrone and MDPV (β-ketones), BromoDragonfly (synthetic phenethylamines), Spice/K2 (synthetic cannabinoids), and salvia divinorum.",4 Experiments with Topic Modeling for Extractive Summarization,[0],[0]
"We will consider these drugs in particular because these are the five drugs for which technical reports were created by the EU Psychonaut Project (Schifano et al., 2006), an online database of novel and emerging drugs, whose information is collected by reading drug websites, including Drugs-Forum.",4 Experiments with Topic Modeling for Extractive Summarization,[0],[0]
"Extensive technical reports were written about these five popular drugs, and we can use these reports to produce reference summaries for our experiments (§4.2).
",4 Experiments with Topic Modeling for Extractive Summarization,[0],[0]
"Of these five drugs, only salvia has its own subforum; the others belong to subforums representing the broader categories shown in parentheses.",4 Experiments with Topic Modeling for Extractive Summarization,[0],[0]
"We simply model the drug type as a proxy for the specific drug, as most of the drugs in each category have similar effects and properties.",4 Experiments with Topic Modeling for Extractive Summarization,[0],[0]
"The first two drugs are both in the same subforum, so for the purpose of our model we treat mephedrone and MDPV as the single drug type, β-ketones.",4 Experiments with Topic Modeling for Extractive Summarization,[0],[0]
"These two drugs are grouped together during summarization (§4.2), but the corresponding reference summaries incorporate excepts from the technical reports on both drugs.",4 Experiments with Topic Modeling for Extractive Summarization,[0],[0]
"Of the four drug types being considered for summarization, our data set contains 12K messages with one of the tags in Table 1 and 30K without.",4.1 Model Setup,[0],[0]
"Of those without tags, we set aside 5K as development data.",4.1 Model Setup,[0],[0]
There are also over 300K messages (140K tagged) from the remaining 18 drug types: some of these messages are utilized when training f-LDA.,4.1 Model Setup,[0],[0]
"Even though we only consider four drug types in our experiments, our intuition is that it can be beneficial to model other drugs as well, because this will help to learn parameters for the various aspects and routes of administration.",4.1 Model Setup,[0],[0]
"Our model of the effects of mephedrone can be informed by also modeling the effects of other stimulants such as cocaine.
",4.1 Model Setup,[0],[0]
"Each message was treated as a document, and we
only used documents with at least five word tokens after stop words, low-frequency words, and punctuation were removed.",4.1 Model Setup,[0],[0]
"The preprocessed data sets contained an average of 45 tokens per document.
",4.1 Model Setup,[0],[0]
"Below, we describe two f-LDA variants as well as the baseline used in our experiments.
",4.1 Model Setup,[0],[0]
Baseline,4.1 Model Setup,[0],[0]
Our baseline model is a unigram language model trained on the subset of messages which are tagged.,4.1 Model Setup,[0],[0]
"We treat the drug subforum as a label for the drug factor, and each message’s tag is used as a label for either the route or aspect factor.",4.1 Model Setup,[0],[0]
"For example, the word distribution for the pair (SALVIA,EFFECTS) is estimated as the empirical distribution from messages posted in the salvia forum and tagged with “Effects.”",4.1 Model Setup,[0],[0]
"We use add-λ smoothing where λ is chosen to optimize likelihood on the held-out development set.
",4.1 Model Setup,[0],[0]
"This is a two-dimensional model, since we explicitly model pairs such as (MEPHEDRONE,SNORTING) or (SALVIA,EFFECTS).",4.1 Model Setup,[0],[0]
"However, we also created word distributions for triples such as (SALVIA,ORAL,EFFECTS) by taking a mixture of the corresponding pairs: in this example, we estimate the unigram distribution from salvia documents tagged with either “Oral” or “Effects.”
Factorial LDA Because f-LDA does not rely on tagged data (the tags are only used to create priors), we can run inference on larger sets of data.",4.1 Model Setup,[0],[0]
"The drawback is that despite these priors, it is still mostly unsupervised and we want to be careful to ensure the model will learn the patterns we care about.",4.1 Model Setup,[0],[0]
"We thus add some reasonable constraints to the parameter space to guide the model further.
",4.1 Model Setup,[0],[0]
"First, we treat the drug type as an observed variable based on the subforum the message comes from, just as with the baseline.",4.1 Model Setup,[0],[0]
"For example, only tuples of the form (SALVIA,∗,∗) can be assigned to tokens in the salvia forum.",4.1 Model Setup,[0],[0]
"Second, we restrict the set of possible routes of administration that can be assigned to tokens in particular drug forums, since most drugs can be taken through only a subset of routes.",4.1 Model Setup,[0],[0]
"For example, marijuana is typically smoked or eaten orally, but rarely injected.",4.1 Model Setup,[0],[0]
We therefore restrict each drug’s allowable set of administration routes to those which are tagged (e.g. with “Oral” or “Snorting”) in at least 1% of that drug’s data.,4.1 Model Setup,[0],[0]
"Similar ideas are used in Labeled LDA (Ramage et al.,
2009), in which tags are used to restrict the space of allowed topics in a document.
",4.1 Model Setup,[0],[0]
"We use f-LDA as a three-dimensional model which explicitly models triples, but we also obtain distributions for pairs such as (SALVIA,EFFECTS) by marginalizing across all distributions of the form (SALVIA,∗,EFFECTS).",4.1 Model Setup,[0],[0]
"We trained f-LDA on two different data sets, yielding the following models:
• f-LDA-1: We use the 12K messages with tags and fill the set out with 13K messages with tags uniformly sampled from the 18 other drugs, for a total of 25K messages.
",4.1 Model Setup,[0],[0]
• f-LDA-2:,4.1 Model Setup,[0],[0]
"We use all 37K messages (many without tags) and fill the set out with 63K messages with tags uniformly sampled from the 18 other drugs, for a total of 100K messages.
",4.1 Model Setup,[0],[0]
All f-LDA instances are run with 5000 iterations alternating between a sweep of Gibbs sampling followed by a step of gradient ascent on the hyperparameters.,4.1 Model Setup,[0],[0]
"While we do not use the tags as strict labels during sampling, we initialize the Gibbs sampler so that each token in a document is assigned to its label given by the tag, when available.",4.1 Model Setup,[0],[0]
"In the absence of tags (in f-LDA-2), we initialize tokens
to the GENERAL components.",4.1 Model Setup,[0],[0]
"We initialized ω to its prior mean (Eq. 3), while the variance σ2 and the initialization of bias ω(B) are chosen to optimize likelihood on the held-out development set.
",4.1 Model Setup,[0],[0]
We optimized the hyperparameters and sparsity array using gradient descent after each Gibbs sweep.,4.1 Model Setup,[0],[0]
"We use a decreasing step size of a/(t+1000), where t is the current iteration and a=10 for α and 1 for ω and the sparsity values.",4.1 Model Setup,[0],[0]
"To learn priors η, we ran our version of SAGE for 100 iterations of gradient ascent (fixed step size of 0.1).",4.1 Model Setup,[0],[0]
See Paul and Dredze (2012a) for examples of parameters (the top words associated with various triples) learned by this model on this corpus.,4.1 Model Setup,[0],[0]
"We created twelve reference summaries by editing together excerpts from the five Psychonaut Project reports ((Psychonaut), 2009).",4.2 Summary Generation,[0],[0]
Each reference is matched to drug-specific pairs and triples.,4.2 Summary Generation,[0],[0]
"For example, a paragraph describing the differences in effects of salvia between smoking and oral routes was matched to distributions for (SALVIA,EFFECTS), (SALVIA,SMOKING,EFFECTS), (SALVIA,ORAL,EFFECTS).",4.2 Summary Generation,[0],[0]
"Descriptions of creating tinctures and blotters for oral consumption were matched to (SALVIA,ORAL,CHEMISTRY).",4.2 Summary Generation,[0],[0]
"We consider pairs in addition to triples because not all summaries correspond to particular routes or aspects.
",4.2 Summary Generation,[0],[0]
"For each tuple-specific word distribution (a pair or a triple), we create a “summary” by extracting a set of five text snippets which minimize KL-divergence to the target word distribution.",4.2 Summary Generation,[0],[0]
"We consider all overlapping text windows of widths {10,15,20} in the corpus as candidate snippets.",4.2 Summary Generation,[0],[0]
"Following Haghighi and Vanderwende (2009), we greedily add snippets one by one with the lowest KL-divergence at each step until we have added five.
",4.2 Summary Generation,[0],[0]
"We only considered candidate snippets within the subforum for the particular drug, and snippets are based on the preprocessed topic model input with no stop words.",4.2 Summary Generation,[0],[0]
"Before presenting snippets to users, we then map the snippets back to the raw text by taking all sentences which are at least partly spanned by the window of tokens.",4.2 Summary Generation,[0],[0]
"Because each reference may be matched to more than one tuple, there may be more than five snippets which correspond to a reference.",4.2 Summary Generation,[0],[0]
Recall that the reports used as reference summaries were themselves created by reading web forums.,4.3 Experimental Results,[0],[0]
Our hypothesis is that f-LDA could be used as an exploratory tool to expedite the creation of these reports.,4.3 Experimental Results,[0],[0]
Thus in our evaluation we want to measure how useful the extracted snippets would be in informing the writing of such reports.,4.3 Experimental Results,[0],[0]
We performed both human and automatic evaluation on the summaries generated by f-LDA (variants 1 and 2) as well as our baseline.,4.3 Experimental Results,[0],[0]
"We also included randomly selected snippets as a control (five per reference).
",4.3 Experimental Results,[0],[0]
Example output is shown in Figure 3.,4.3 Experimental Results,[0],[0]
Three annotators were presented snippets pooled from all four systems we are evaluating alongside the corresponding reference text.,4.3.1 Human Judgments of Quality,[0],[0]
"Within each set corresponding to a reference summary, the snippets were shown in a random order.",4.3.1 Human Judgments of Quality,[0],[0]
"Annotators were asked to judge each snippet independently on a 5- point Likert scale as to how useful each snippet would be in writing the reference text.
",4.3.1 Human Judgments of Quality,[0],[0]
The distribution of scores is shown in Figure 4 and summarized in Table 2.,4.3.1 Human Judgments of Quality,[0],[0]
Annotators generally agreed on the relative quality of snippets: the average correlation of scores between each pair of annotators was 0.49.,4.3.1 Human Judgments of Quality,[0],[0]
"Snippets produced by f-LDA were given more high scores and fewer low scores than the baseline, while the two f-LDA variants were rated comparably.",4.3.1 Human Judgments of Quality,[0],[0]
"The breakdown is more interesting when we compare scores for snippets that were matched
to word distributions for pairs versus word distributions for triples.",4.3.1 Human Judgments of Quality,[0],[0]
The gap in scores between fLDA and the baseline increases when we look at the scores for only triples: f-LDA beats the baseline by a margin of 0.45 for snippets matched to triples and 0.21 for pairs.,4.3.1 Human Judgments of Quality,[0],[0]
This suggests that we produce better triples by modeling them jointly.,4.3.1 Human Judgments of Quality,[0],[0]
"For triples, f-LDA2 (which uses more data) beats f-LDA-1 (which uses only tagged data), while the reverse is true for pairs.
",4.3.1 Human Judgments of Quality,[0],[0]
"While some of the randomly selected control snippets happened to be useful, the scores for these snippets were much lower than those extracted through model-based systems.",4.3.1 Human Judgments of Quality,[0],[0]
"This suggests that exploring the forums in a targeted way (e.g. through our topic model approach) would be more efficient than exploring the data in a non-targeted way (akin to the random approach).
",4.3.1 Human Judgments of Quality,[0],[0]
"Finally, we asked two expert annotators (faculty members in psychiatry and behavioral pharmacology, who have used drug forums in the past to study emerging drugs) to rate the snippets corresponding to mephedrone/MDPV.",4.3.1 Human Judgments of Quality,[0],[0]
The best f-LDA system had an average score of 2.57 compared to a baseline score of 2.45 and random score of 1.63.,4.3.1 Human Judgments of Quality,[0],[0]
"The human judgments effectively measured a form of precision, as the quality of snippets were judged by their correspondence to the reference text, without regard to how much of the reference text was covered by all snippets.",4.3.2 Automatic Evaluation of Recall,[0],[0]
"We also used the automatic evaluation metric ROUGE (Lin, 2004) as a rough estimate of summary recall: this metric computes the percentage of n-grams in the reference text that appeared in the generated summaries.
",4.3.2 Automatic Evaluation of Recall,[0],[0]
We computed ROUGE for both 1-grams and 2- grams.,4.3.2 Automatic Evaluation of Recall,[0],[0]
"When computing n-gram counts, we applied Porter’s stemmer to all tokens.",4.3.2 Automatic Evaluation of Recall,[0],[0]
"We excluded stop
words from 1-gram counts but included them in 2- gram counts where we care about longer phrases.2
Results are shown in Table 2.",4.3.2 Automatic Evaluation of Recall,[0],[0]
"We find that f-LDA1 has the highest score for both 1- and 2-grams, suggesting that it is extracting a more diverse set of relevant snippets.",4.3.2 Automatic Evaluation of Recall,[0],[0]
"When performing a paired t-test across the 12 reference summaries, we find that fLDA is better than the baseline with p-values 0.14 and 0.10 for 1-gram and 2-gram recall, respectively.",4.3.2 Automatic Evaluation of Recall,[0],[0]
f-LDA’s recall advantage may come from the fact that it learns from a larger amount of data and it may learn more diverse word distributions by directly modeling triples.,4.3.2 Automatic Evaluation of Recall,[0],[0]
"f-LDA-1 had slightly better recall (under ROUGE), while f-LDA-2 was slightly better according to the human annotators.",4.3.2 Automatic Evaluation of Recall,[0],[0]
We have proposed exploratory tools for the analysis of online drug communities.,5 Conclusion,[0],[0]
"Such communities are an emerging source of drug research, but manually browsing through large corpora is impractical and important information could be missed.",5 Conclusion,[0],[0]
"We have demonstrated that topic models are capable of modeling informative portions of text, and in particular multi-dimensional topic models can target desired structures such as the combination of aspect and route of administration for each drug.",5 Conclusion,[0],[0]
We have presented an extension to factorial LDA tailored to a particular application and data set which was demonstrated to induce desired properties.,5 Conclusion,[0],[0]
"As a technical contribution, this study lays out practical guidelines for customizing and incorporating prior knowledge into multi-dimensional text models.",5 Conclusion,[0],[0]
"We are grateful to Dr. Margaret S. Chisolm and Dr. Ryan Vandrey from the Johns Hopkins School of Medicine for providing the mephedrone/MDPV annotations, and Alex Lamb and Hieu Tran for assisting with the full annotations.",Acknowledgments,[0],[0]
"We also thank Dr. Matthew W. Johnson for additional advice, and the anonymous reviewers for helpful feedback and suggestions.",Acknowledgments,[0],[0]
"This research was partly supported by an NSF Graduate Research Fellowship.
",Acknowledgments,[0],[0]
"2In both cases, ROUGE scores were higher when stop words were included.",Acknowledgments,[0],[0]
f-LDA beats the baseline by similar margins regardless of whether we include stop words.,Acknowledgments,[0],[0]
"Multi-dimensional latent text models, such as factorial LDA (f-LDA), capture multiple factors of corpora, creating structured output for researchers to better understand the contents of a corpus.",abstractText,[0],[0]
"We consider such models for clinical research of new recreational drugs and trends, an important application for mining current information for healthcare workers.",abstractText,[0],[0]
"We use a “three-dimensional” f-LDA variant to jointly model combinations of drug (marijuana, salvia, etc.), aspect (effects, chemistry, etc.) and route of administration (smoking, oral, etc.)",abstractText,[0],[0]
"Since a purely unsupervised topic model is unlikely to discover these specific factors of interest, we develop a novel method of incorporating prior knowledge by leveraging user generated tags as priors in our model.",abstractText,[0],[0]
We demonstrate that this model can be used as an exploratory tool for learning about these drugs from the Web by applying it to the task of extractive summarization.,abstractText,[0],[0]
"In addition to providing useful output for this important public health task, our prior-enriched model provides a framework for the application of fLDA to other tasks.",abstractText,[0],[0]
Drug Extraction from the Web: Summarizing Drug Experiences with Multi-Dimensional Topic Models,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 917–927, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.
This is the first inference method for arbitrary graphical models over strings that does not require approximations such as random sampling, message simplification, or a bound on string length. Provided that the inference method terminates, it gives a certificate of global optimality (though MAP inference in our setting is undecidable in general). On our global phonological inference problems, it always terminates, and achieves more accurate results than max-product and sum-product loopy belief propagation.",text,[0],[0]
Graphical models allow expert modeling of complex relations and interactions between random variables.,1 Introduction,[0],[0]
"Since a graphical model with given parameters defines a probability distribution, it can be used to reconstruct values for unobserved variables.",1 Introduction,[0],[0]
The marginal inference problem is to compute the posterior marginal distributions of these variables.,1 Introduction,[0],[0]
"The MAP inference (or MPE) problem is to compute the single highest-probability joint assignment to all the unobserved variables.
",1 Introduction,[0],[0]
"Inference in general graphical models is NPhard even when the variables’ values are finite discrete values such as categories, tags or domains.",1 Introduction,[0],[0]
"In this paper, we address the more challenging setting
∗This material is based upon work supported by the National Science Foundation under Grant No. 1423276.
where the variables in the graphical models range over strings.",1 Introduction,[0],[0]
"Thus, the domain of the variables is an infinite space of discrete structures.
",1 Introduction,[0],[0]
"In NLP, such graphical models can deal with large, incompletely observed lexicons.",1 Introduction,[0],[0]
"They could be used to model diverse relationships among strings that represent spellings or pronunciations; morphemes, words, phrases (such as named entities and URLs), or utterances; standard or variant forms; clean or noisy forms; contemporary or historical forms; underlying or surface forms; source or target language forms.",1 Introduction,[0],[0]
"Such relationships arise in domains such as morphology, phonology, historical linguistics, translation between related languages, and social media text analysis.
",1 Introduction,[0],[0]
"In this paper, we assume a given graphical model, whose factors evaluate the relationships among observed and unobserved strings.1 We present a dual decomposition algorithm for MAP inference, which returns a certifiably optimal solution when it converges.",1 Introduction,[0],[0]
We demonstrate our method on a graphical model for phonology proposed by Cotterell et al. (2015).,1 Introduction,[0],[0]
"We show that the method generally converges and that it achieves better results than alternatives.
",1 Introduction,[0],[0]
"The rest of the paper is arranged as follows: We will review graphical models over strings in section 2, and briefly introduce our sample problem in section 3.",1 Introduction,[0],[0]
Section 4 develops dual decomposition inference for graphical models over strings.,1 Introduction,[0],[0]
"Then our experimental setup and results are presented in sections 5 and 6, with some discussion.",1 Introduction,[0],[0]
"To perform inference on a graphical model (directed or undirected), one first converts the model to a factor graph representation (Kschischang et al., 2001).",2.1 Factor Graphs and MAP Inference,[0],[0]
"A factor graph is a finite bipartite
1In some task settings, it is also necessary to discover the model topology along with the model parameters.",2.1 Factor Graphs and MAP Inference,[0],[0]
In this paper we do not treat that structure learning problem.,2.1 Factor Graphs and MAP Inference,[0],[0]
"However, both structure learning and parameter learning need to call inference—such as the method presented here—in order to evaluate proposed topologies or improve their parameters.
917
graph over a set X = {X1, X2, . . .",2.1 Factor Graphs and MAP Inference,[0],[0]
} of variables and a set F of factors.,2.1 Factor Graphs and MAP Inference,[0],[0]
An assignment to the variables is a vector of values x =,2.1 Factor Graphs and MAP Inference,[0],[0]
"(x1, x2, . . .).",2.1 Factor Graphs and MAP Inference,[0],[0]
"Each factor F ∈ F is a real-valued function of x, but it depends on a given xi only if F is connected to Xi in the graph.",2.1 Factor Graphs and MAP Inference,[0],[0]
"Thus, a degree d-factor scores some length-d subtuple of x.",2.1 Factor Graphs and MAP Inference,[0],[0]
"The score of the whole joint assignment simply sums over all factors:
score(x) def=",2.1 Factor Graphs and MAP Inference,[0],[0]
∑ F∈F F (x).,2.1 Factor Graphs and MAP Inference,[0],[0]
"(1)
We seek the x of maximum score that is consistent with our partial observation of x.",2.1 Factor Graphs and MAP Inference,[0],[0]
This is a generic constraint satisfaction problem with soft constraints.,2.1 Factor Graphs and MAP Inference,[0],[0]
"While our algorithm does not depend on a probabilistic interpretation of the factor graph,2 it can be regarded as peforming maximum a posteriori (MAP) inference of the unobserved variables, under the probability distribution p(x) def=",2.1 Factor Graphs and MAP Inference,[0],[0]
(1/Z) exp score(x).,2.1 Factor Graphs and MAP Inference,[0],[0]
Graphical models over strings have enjoyed some attention in the NLP community.,2.2 The String Case,[0],[0]
"Tree-shaped graphical models naturally model the evolutionary tree of word forms (Bouchard-Côté et al., 2007; Bouchard-Côté et al., 2008; Hall and Klein, 2010; Hall and Klein, 2011).",2.2 The String Case,[0],[0]
"Cyclic graphical
2E.g., it could be used for exactly computing the separation oracle when training a structural SVM (Tsochantaridis et al., 2005; Finley and Joachims, 2007).",2.2 The String Case,[0],[0]
"Another use is minimum Bayes risk decoding—computing the joint assignment having minimum expected loss—if the loss function does not decompose over the variables, but a factor graph can be constructed that evaluates the expected loss of any assignment.
models have been used to model morphological paradigms (Dreyer and Eisner, 2009; Dreyer and Eisner, 2011) and to reconstruct phonological underlying forms of words (Cotterell et al., 2015).
",2.2 The String Case,[0],[0]
The variables in such a model are strings of unbounded length: each variable Xi is permitted to range over Σ∗,2.2 The String Case,[0],[0]
"where Σ is some fixed, finite alphabet.",2.2 The String Case,[0],[0]
"As in previous work, we assume that a degree-d factor is a d-way rational relation, i.e., a function of d strings that can be computed by a d-tape weighted finite-state machine (WFSM) (Mohri et al., 2002; Kempe et al., 2004).",2.2 The String Case,[0],[0]
"Such a machine is called an acceptor (WFSA) if d = 1 or a transducer (WFST) if d = 2.3
Past work has shown how to approximately sample from the distribution over x defined by such a model (Bouchard-Côté et al., 2007), or approximately compute the distribution’s marginals using variants of sum-product belief propagation (BP) (Dreyer and Eisner, 2009) and expectation propagation (EP) (Cotterell and Eisner, 2015).",2.2 The String Case,[0],[0]
BP iteratively updates messages between factors and variables.,2.3 Finite-State Belief Propagation,[0],[0]
"Each message is a vector whose elements score the possible values of a variable.
",2.3 Finite-State Belief Propagation,[0],[0]
Murphy et al. (1999) discusses BP on cyclic (“loopy”) graphs.,2.3 Finite-State Belief Propagation,[0],[0]
"For pedagogical reasons, suppose momentarily that all factors have degree ≤ 2 (this loses no power).",2.3 Finite-State Belief Propagation,[0],[0]
"Then BP manipulates only vectors and matrices—whose dimensionality depends on the number of possible values of the vari-
3Finite-state software libraries often support only these cases.",2.3 Finite-State Belief Propagation,[0],[0]
"Accordingly, Cotterell and Eisner (2015, Appendix B.10) explain how to eliminate factors of degree d > 2.
ables.",2.3 Finite-State Belief Propagation,[0],[0]
"In the string case, they have infinitely many rows and columns, indexed by possible strings.
",2.3 Finite-State Belief Propagation,[0],[0]
"Dreyer and Eisner (2009) represented these infinite vectors and matrices by WFSAs and WFSTs, respectively.",2.3 Finite-State Belief Propagation,[0],[0]
They observed that the simple linear-algebra operations used by BP can be implemented by finite-state constructions.,2.3 Finite-State Belief Propagation,[0],[0]
The pointwise product of two vectors is the intersection of their WFSAs; the marginalization of a matrix is the projection of its WFST; a vector-matrix product is computed by composing the WFSA with the WFST and then projecting onto the output tape.,2.3 Finite-State Belief Propagation,[0],[0]
"For degree > 2, BP’s rank-d tensors become dtape WFSMs, and these constructions generalize.
",2.3 Finite-State Belief Propagation,[0],[0]
"Unfortunately, except in small acyclic models, the BP messages—which are WFSAs—usually become impractically large.",2.3 Finite-State Belief Propagation,[0],[0]
Each intersection or composition involves a cross-product construction.,2.3 Finite-State Belief Propagation,[0],[0]
"For example, when finding the marginal distribution at a degree-d variable, intersecting d WFSA messages having m states each may yield a WFSA with up to md states.",2.3 Finite-State Belief Propagation,[0],[0]
(Our models in section 6 include variables with d up to 156.),2.3 Finite-State Belief Propagation,[0],[0]
"Combining many cross products, as BP iteratively passes messages along a path in the factor graph, leads to blowup that is exponential in the length of the path—which in turn is unbounded if the graph has cycles (Dreyer and Eisner, 2009), as ours do.
",2.3 Finite-State Belief Propagation,[0],[0]
The usual solution is to prune or otherwise approximate the messages at each step.,2.3 Finite-State Belief Propagation,[0],[0]
"In particular, Cotterell and Eisner (2015) gave a principled way to approximate the messages using variablelength n-gram models, using an adaptive variant of Expectation Propagation (Minka, 2001).",2.3 Finite-State Belief Propagation,[0],[0]
"In section 4, we will present a dual decomposition (DD) method that decomposes the original complex problem into many small subproblems that are free of cycles and high degree nodes.",2.4 Dual Decomposition Inference,[0],[0]
"BP can solve each subproblem without approximation.4
The subproblems “communicate” through Lagrange multipliers that guide them towards agreement on a single global solution.",2.4 Dual Decomposition Inference,[0],[0]
This information is encoded in WFSAs that score possible values of a string variable.,2.4 Dual Decomposition Inference,[0],[0]
"DD incrementally adjusts the WFSAs so as to encourage values that agree with
4Such small BP problems commonly arise in NLP.",2.4 Dual Decomposition Inference,[0],[0]
"In particular, using finite-state methods to decode a composition of several finite-state noisy channels (Pereira and Riley, 1997; Knight and Graehl, 1998) can be regarded as BP on a graphical model over strings that has a linear-chain topology.
",2.4 Dual Decomposition Inference,[0],[0]
the variable’s average value across subproblems.,2.4 Dual Decomposition Inference,[0],[0]
"Unlike BP messages, the WFSAs in our DD method will be restricted to be variable-length n-gram models, similar to Cotterell and Eisner (2015).",2.4 Dual Decomposition Inference,[0],[0]
They may still grow over time; but DD often halts while the WFSAs are still small.,2.4 Dual Decomposition Inference,[0],[0]
"It halts when its strings agree exactly, rather than when it has converged up to a numerical tolerance, like BP.",2.4 Dual Decomposition Inference,[0],[0]
Our factors may be nondeterministic WFSMs.,2.5 Switching Between Semirings,[0],[0]
"So when F ∈ F scores a given d-tuple of string values, it may accept that d-tuple along multiple different WFSM paths with different scores, corresponding to different alignments of the strings.
",2.5 Switching Between Semirings,[0],[0]
"For purposes of MAP inference, we define F to return the maximum of these path scores.",2.5 Switching Between Semirings,[0],[0]
"That is, we take the WFSMs to be defined with weights in the (max,+) semiring (Mohri et al., 2002).",2.5 Switching Between Semirings,[0],[0]
"Equivalently, we are seeking the “best global solution” in the sense of choosing not only the strings xi but also the alignments of the d-tuples.5
To do so, we must solve each DD subproblem in the same sense.",2.5 Switching Between Semirings,[0],[0]
We use max-product BP.,2.5 Switching Between Semirings,[0],[0]
This still applies the Dreyer-Eisner method of section 2.3.,2.5 Switching Between Semirings,[0],[0]
"Since these WFSMs are defined in the (max,+) semiring, the method’s finite-state operations will combine weights using max and +.
MAP inference in our setting is in general computationally undecidable.6 However, if DD converges (as in our experiments), then its solution is guaranteed to be the true MAP assignment.
",2.5 Switching Between Semirings,[0],[0]
"In section 6, we will compare DD with (loopy) max-product BP and (loopy) sum-product BP.",2.5 Switching Between Semirings,[0],[0]
These respectively approximate MAP inference and marginal inference over the entire factor graph.,2.5 Switching Between Semirings,[0],[0]
Marginal inference computes marginal string probabilities that sum (rather than maximize) over the choices of other strings and the choices of paths.,2.5 Switching Between Semirings,[0],[0]
"Thus, for sum-product BP, we re-interpret the factor WFSMs as defined over the (logadd,+) semiring.",2.5 Switching Between Semirings,[0],[0]
"This means that the exponentiated score assigned by a WFSM is the sum of the exponentiated scores of the accepting paths.
",2.5 Switching Between Semirings,[0],[0]
5This problem is more specifically called MPE inference.,2.5 Switching Between Semirings,[0],[0]
6The trouble is that we cannot bound the length of the latent strings.,2.5 Switching Between Semirings,[0],[0]
"If we could, then we could encode them using a finite set of boolean variables, and solve as an ILP problem.",2.5 Switching Between Semirings,[0],[0]
But that would allow us to determine whether there exists a MAP assignment with score ≥ 0.,2.5 Switching Between Semirings,[0],[0]
"That is impossible in general, because it would solve Post’s Correspondence Problem as a simple special case (see Dreyer and Eisner (2009)).",2.5 Switching Between Semirings,[0],[0]
"Before giving the formal details of our DD method, we give a motivating example: a recently proposed graphical model for morphophonology.",3 A Sample Task: Generative Phonology,[0],[0]
Cotterell et al. (2015) defined a Bayesian network to describe the generative process of phonological words.,3 A Sample Task: Generative Phonology,[0],[0]
"Our Figure 1 shows a conversion of their model to a factor graph and explains what the variables and factors mean.
",3 A Sample Task: Generative Phonology,[0],[0]
Inference on this graph performs unsupervised discovery of latent strings.,3 A Sample Task: Generative Phonology,[0],[0]
"Given observed surface representations of words (SRs), inference aims to recover the underlying representations (URs) of the words and their shared constituent morphemes.",3 A Sample Task: Generative Phonology,[0],[0]
"The latter can then be used to predict held-out SRs.
",3 A Sample Task: Generative Phonology,[0],[0]
Notice that the 8 edges in the first layer of Figure 1 form a cycle; such cycles make BP inexact.,3 A Sample Task: Generative Phonology,[0],[0]
"Moreover, the figure shows only a schematic fragment of the graphical model.",3 A Sample Task: Generative Phonology,[0],[0]
"In the actual experiments, the graphical models have up to 829 variables, and the variables representing morpheme URs are connected to up to 156 factors (because many words share the same affix).
",3 A Sample Task: Generative Phonology,[0],[0]
"To handle the above challenges without approximation, we want to decompose the original problem into subproblems where each subproblem can be solved efficiently.",3 A Sample Task: Generative Phonology,[0],[0]
"In particular, we want the subproblems to be free of cycles and highdegree nodes.",3 A Sample Task: Generative Phonology,[0],[0]
"In our phonology example, each observed word along with its correspondent latent URs forms an ideal subproblem.",3 A Sample Task: Generative Phonology,[0],[0]
"This decomposition is shown in Figure 2.
",3 A Sample Task: Generative Phonology,[0],[0]
"While the subproblems can be solved efficiently in isolation, they may share variables, as shown by the dashed lines in Figure 2.",3 A Sample Task: Generative Phonology,[0],[0]
DD repeatedly modifies and re-solves the subproblems until they agree on their shared variables.,3 A Sample Task: Generative Phonology,[0],[0]
Dual decomposition is a general technique for solving constrained optimization problems.,4 Dual Decomposition,[0],[0]
"It has been widely used for MAP inference in graphical models (Komodakis et al., 2007; Komodakis and Paragios, 2009; Koo et al., 2010; Martins et al., 2011; Sontag et al., 2011; Rush and Collins, 2014).",4 Dual Decomposition,[0],[0]
"However, previous work has focused on variables Xi whose values are in R or a small finite set; we will consider the infinite set Σ∗.",4 Dual Decomposition,[0],[0]
"To apply dual decomposition, we must partition the original problem into a union of K subproblems, each of which can be solved exactly and efficiently (and in parallel).",4.1 Review of Dual Decomposition,[0],[0]
"For example, our experiments partition Figure 1 as shown in Figure 2.
",4.1 Review of Dual Decomposition,[0],[0]
"Specifically, we partition the factors into K sets F1, . . .",4.1 Review of Dual Decomposition,[0],[0]
",FK .",4.1 Review of Dual Decomposition,[0],[0]
Each factor F ∈ F appears in exactly one of these sets.,4.1 Review of Dual Decomposition,[0],[0]
This lets us rewrite the score (1) as ∑ k ∑ F∈Fk F (x).,4.1 Review of Dual Decomposition,[0],[0]
"Instead of simply seeking its maximizer x, we equivalently seek
argmax x1,...,xK K∑ k=1",4.1 Review of Dual Decomposition,[0],[0]
( ∑ F∈Fk F (xk) ) s.t.,4.1 Review of Dual Decomposition,[0],[0]
"x1 = · · · = xK (2)
",4.1 Review of Dual Decomposition,[0],[0]
"If we dropped the equality constraint, (2) could be solved by separately maximizing∑
F∈Fk F (x k) for each k.",4.1 Review of Dual Decomposition,[0],[0]
This “subproblem” is itself a MAP problem which considers only the factors,4.1 Review of Dual Decomposition,[0],[0]
Fk and the variables X k adjacent to them in the original factor graph.,4.1 Review of Dual Decomposition,[0],[0]
"The subproblem objective does not depend on the other variables.
",4.1 Review of Dual Decomposition,[0],[0]
"We now attempt to enforce the equality constraint indirectly, by adding Lagrange multipliers that encourage agreement among the subproblems.",4.1 Review of Dual Decomposition,[0],[0]
Assume for the moment that the variables in the factor graph are real-valued (each xki is in R).,4.1 Review of Dual Decomposition,[0],[0]
"Then consider the Lagrangian relaxation of (2),
max x1,...,xK K∑ k=1",4.1 Review of Dual Decomposition,[0],[0]
( ∑ F∈Fk F (xk) + ∑ i λki · xki ),4.1 Review of Dual Decomposition,[0],[0]
"(3)
This can still be solved by separate maximizations.",4.1 Review of Dual Decomposition,[0],[0]
"For any choices of λki ∈ R having (∀i) ∑ k λ k i = 0, it upper-bounds the objective of (2).",4.1 Review of Dual Decomposition,[0],[0]
Why?,4.1 Review of Dual Decomposition,[0],[0]
"The solution to (2) achieves the same value in (3), yet (3) may do even better by considering solutions that do not satisfy the constraint.",4.1 Review of Dual Decomposition,[0],[0]
Our goal is to find λki values that tighten this upper bound as much as possible.,4.1 Review of Dual Decomposition,[0],[0]
"If we can find λki values so that
the optimum of (3) satisfies the equality constraint, then we have a tight bound and a solution to (2).
",4.1 Review of Dual Decomposition,[0],[0]
"To improve the method, recall that subproblem k considers only variables",4.1 Review of Dual Decomposition,[0],[0]
X k.,4.1 Review of Dual Decomposition,[0],[0]
It is indifferent to the value ofXi ifXi /∈,4.1 Review of Dual Decomposition,[0],[0]
"X k, so we just leave xki undefined in the subproblem’s solution.",4.1 Review of Dual Decomposition,[0],[0]
We treat that as automatically satisfying the equality constraint; thus we do not need any Lagrange multiplier λki to force equality.,4.1 Review of Dual Decomposition,[0],[0]
"Our final solution x ignores undefined values, and sets xi to the value agreed on by the subproblems that did consider Xi.7",4.1 Review of Dual Decomposition,[0],[0]
But what do we do if the variables are strings?,4.2 Substring Count Features,[0],[0]
The Lagrangian term λki ·xki in (3) is now ill-typed.,4.2 Substring Count Features,[0],[0]
"We replace it with λki · γ(xki ), where γ(·) extracts a real-valued feature vector from a string, and λki is a vector of Lagrange multipliers.
",4.2 Substring Count Features,[0],[0]
This corresponds to changing the constraint in (2).,4.2 Substring Count Features,[0],[0]
"Instead of requiring x1i = · · · = xKi for each i, we are now requiring γ(x1i ) = · · · = γ(xKi ), i.e., these strings must agree in their features.
",4.2 Substring Count Features,[0],[0]
"We want each possible string to have a unique feature vector, so that matching features forces the actual strings to match.",4.2 Substring Count Features,[0],[0]
We follow Paul and Eisner (2012) and use a substring count feature for each w ∈,4.2 Substring Count Features,[0],[0]
"Σ∗. In other words, γ(x) is an infinitely long vector, which maps each w to the number of times that w appears in x as a substring.8
Computing λki · γ(xki ) in (3) remains possible because in practice, λki will have only finitely many nonzeros.",4.2 Substring Count Features,[0],[0]
"This is so because our feature vector γ(x) has only finitely many nonzeros for any string x, and the subgradient algorithm in section 4.3 below always updates λki by adding multiples of such γ(x) vectors.
",4.2 Substring Count Features,[0],[0]
We will use a further trick below to prevent rapid growth of this finite set of nonzeros.,4.2 Substring Count Features,[0],[0]
"Each variable Xi maintains an active set of features, Wi.",4.2 Substring Count Features,[0],[0]
Only these features may have nonzero Lagrange multipliers.,4.2 Substring Count Features,[0],[0]
"While the active set can grow over time, it will be finite at any given step.
",4.2 Substring Count Features,[0],[0]
"Given the Lagrange multipliers, subproblem k of (3) is simply MAP inference on the factor graph consisting of the variables X k and factors",4.2 Substring Count Features,[0],[0]
"Fk as well as an extra unary factor Gki at each Xi ∈ X k:
7Without this optimization, the Lagrangian term λki · xki would have driven xki to match that value anyway.
8More precisely, the number of times that w appears in BOS x EOS, where BOS, EOS are distinguished boundary symbols.",4.2 Substring Count Features,[0],[0]
"We allow w to start with BOS and/or end with EOS, which yields prefix and suffix indicator features.
",4.2 Substring Count Features,[0],[0]
Gki (x k) def=,4.2 Substring Count Features,[0],[0]
"λki · γ(xki ) (4)
",4.2 Substring Count Features,[0],[0]
These unary factors penalize strings according to the Lagrange multipliers.,4.2 Substring Count Features,[0],[0]
"They can be encoded as WFSAs (Allauzen et al., 2003; Cotterell and Eisner, 2015, Appendices B.1–B.5), allowing us to solve the subproblem by max-product BP as usual.",4.2 Substring Count Features,[0],[0]
"The topology of the WFSA for Gki depends only onWi, while its weights come from λki .",4.2 Substring Count Features,[0],[0]
We aim to adjust the collection λ of Lagrange multipliers to minimize the upper bound (3).,4.3 Projected Subgradient Method,[0],[0]
"Following Komodakis et al. (2007), we solve this convex dual problem using a projected subgradient method.",4.3 Projected Subgradient Method,[0],[0]
We initialize λ = 0 and compute (3) by solving the K subproblems.,4.3 Projected Subgradient Method,[0],[0]
"Then we take a step to adjust λ, and repeat in hopes of eventually satisfying the equality condition.
",4.3 Projected Subgradient Method,[0],[0]
"The projected subgradient step is
λki := λ k",4.3 Projected Subgradient Method,[0],[0]
i + η · ( µi − γ(xki ) ),4.3 Projected Subgradient Method,[0],[0]
"(5)
where η > 0 is the current step size, and µi is the mean of γ(xk
′",4.3 Projected Subgradient Method,[0],[0]
i ) over all subproblems k ′,4.3 Projected Subgradient Method,[0],[0]
that consider Xi.,4.3 Projected Subgradient Method,[0],[0]
"This update modifies (3) to encourage solutions xk such that γ(xki ) comes closer to µi.
",4.3 Projected Subgradient Method,[0],[0]
"For each i, we update all λki at once to preserve the property that (∀i)∑k λki = 0.",4.3 Projected Subgradient Method,[0],[0]
"However, we are only allowed to update components of the λki that correspond to features in the active setWi.",4.3 Projected Subgradient Method,[0],[0]
"To ensure that we continue to make progress even after we agree on these features, we first expandWi by adding the minimal strings (if any) on which the xki do not yet all agree.",4.3 Projected Subgradient Method,[0],[0]
"For example, we will add the abc feature only when the xki already agree on their counts of its substrings ab and bc.9
Algorithm 1 summarizes the whole method.",4.3 Projected Subgradient Method,[0],[0]
"Table 1 illustrates how one active setWi (section 4.3) evolves, in our experiments, as it tries to enforce agreement on a particular string xi.",4.3 Projected Subgradient Method,[0],[0]
Our DD algorithm is an extension of one that Paul and Eisner (2012) developed for the simpler implicit intersection problem.,4.4 Past Work: Implicit Intersection,[0],[0]
"Given many WFSAs F1, . . .",4.4 Past Work: Implicit Intersection,[0],[0]
", FK , they were able to find the string x with maximum total score ∑K k=1 Fk(x).",4.4 Past Work: Implicit Intersection,[0],[0]
"(They applied this to solve instances of the NP-hard Steiner 9In principle, we should check that they also (still) agree on a, b, and c, but we skip this check.",4.4 Past Work: Implicit Intersection,[0],[0]
"Our active set heuristic is almost identical to that of Paul and Eisner (2012).
",4.4 Past Work: Implicit Intersection,[0],[0]
Algorithm 1 DD for graphical models over strings 1: initialize the active setWi for each variable Xi ∈ X 2: initialize λki = 0 for each Xi and each subproblem k 3: for t,4.4 Past Work: Implicit Intersection,[0],[0]
= 1 to T do .,4.4 Past Work: Implicit Intersection,[0],[0]
max number of iterations 4: for k = 1 to K do .,4.4 Past Work: Implicit Intersection,[0],[0]
"solve all primal subproblems 5: if any of the λki have changed then 6: run max-product BP on the acyclic graph de-
fined by variablesX k and factorsFk andGki 7: extract MAP strings: ∀i with Xi ∈ X k, xki
is the label of the max-scoring accepting path in the WFSA that represents the belief at Xi
8: for each Xi ∈ X do .",4.4 Past Work: Implicit Intersection,[0],[0]
improve dual bound 9: if the defined strings xki are not all equal then 10: Expand active feature setWi .,4.4 Past Work: Implicit Intersection,[0],[0]
section 4.3 11: Update each λki .,4.4 Past Work: Implicit Intersection,[0],[0]
"equation (5) 12: Update each Gki from Θi,λ k i .",4.4 Past Work: Implicit Intersection,[0],[0]
"see (4) 13: if none of the Xi required updates then 14: return any defined xki (all are equal) for each i 15: return {x1i , . . .",4.4 Past Work: Implicit Intersection,[0],[0]
", xKi } for each i . failed to converge
string problem, i.e., finding the string x of minimum total edit distance to a collection ofK ≈ 100 given strings.)",4.4 Past Work: Implicit Intersection,[0],[0]
The naive solution to this problem would be to find the highest-weighted path in the intersection F1 ∩ · · · ∩ FK .,4.4 Past Work: Implicit Intersection,[0],[0]
"Unfortunately, the intersection of WFSAs takes the Cartesian product of their state sets.",4.4 Past Work: Implicit Intersection,[0],[0]
"Thus materializing this intersection would have taken time exponential in K.
To put this another way, inference is NP-hard even on a “trivial” factor graph: a single variable X1 attached to K factors.",4.4 Past Work: Implicit Intersection,[0],[0]
Recall from section 2.3 that BP would solve this via the expensive intersection above.,4.4 Past Work: Implicit Intersection,[0],[0]
Paul and Eisner (2012) instead applied DD with one subproblem per factor.,4.4 Past Work: Implicit Intersection,[0],[0]
"We generalize their method to handle arbitrary factor graphs, with multiple latent variables and cycles.",4.4 Past Work: Implicit Intersection,[0],[0]
We also explored a possible speedup for our algorithm.,4.5 Block Coordinate Update,[0],[0]
We used a block coordinate update variant of the algorithm when performing inference on the phonology problem and observed an empirical speedup.,4.5 Block Coordinate Update,[0],[0]
"Block coordinate updates are widely used in Lagrangian relaxation and have also been explored specifically for dual decomposition.
",4.5 Block Coordinate Update,[0],[0]
"In general, block algorithms minimize the objective by holding some variables fixed while updating others.",4.5 Block Coordinate Update,[0],[0]
Sontag et al. (2011) proposed a sophisticated block method called MPLP that considers all values of variable Xi instead of the ones obtained from the best assignments for the subproblems.,4.5 Block Coordinate Update,[0],[0]
"However, it is not clear how to apply their technique to string-valued variables.",4.5 Block Coordinate Update,[0],[0]
"Instead, the algorithm we propose here is much simpler—it
divides the primal variables into groups and updates each group’s associated dual variables in turn, using a single subgradient step (5).",4.5 Block Coordinate Update,[0],[0]
"Note that this way of partitioning the dual variables has the nice property that we can still use the projected subgradient update we gave in (5) and preserve the property that (∀i)∑k λki = 0.
",4.5 Block Coordinate Update,[0],[0]
"In the graphical model for generative phonology, there are two types of underlying morphemes in the first layer: word stems and word affixes.",4.5 Block Coordinate Update,[0],[0]
Our block coordinate update algorithm thus alternates between subgradient updates to the dual variables for the stems and the dual variables for the affixes.,4.5 Block Coordinate Update,[0],[0]
"Note that when performing block coordinate update on the dual variables, the primal variables are not held constant, but rather are chosen by optimizing the corresponding subproblem.",4.5 Block Coordinate Update,[0],[0]
"We compare DD to belief propagation, using the graphical model for generative phonology discussed in section 3.",5.1 Datasets,[0],[0]
Inference in this model aims to reconstruct underlying morphemes.,5.1 Datasets,[0],[0]
"Since our focus is inference, we will evaluate these reconstructions directly (whereas Cotterell et al. (2015) evaluated their ability to predict novel surface forms using the reconstructions).
",5.1 Datasets,[0],[0]
Our factor graphs have a similar topology to the pedagogical fragment shown in Figure 1.,5.1 Datasets,[0],[0]
"How-
ever, they are actually derived from datasets constructed by Cotterell et al. (2015), which are available with full descriptions at http://hubal.cs.",5.1 Datasets,[0],[0]
"jhu.edu/tacl2015/. Briefly:
EXERCISE Small datasets of Catalan, English, Maori, and Tangale, drawn from phonology textbooks.",5.1 Datasets,[0],[0]
"Each dataset contains 55 to 106 surface words, formed from a collection of 16 to 55 morphemes.",5.1 Datasets,[0],[0]
"CELEX Larger datasets of German, English, and Dutch, drawn from the CELEX database (Baayen et al., 1995).",5.1 Datasets,[0],[0]
"Each dataset contains 1000 surface words, formed from 341 to 381 underlying morphemes.",5.1 Datasets,[0],[0]
"We compared three types of inference:
DD Use DD to perform exact MAP inference.",5.2 Evaluation Scheme,[0],[0]
"SP Perform approximate marginal inference by
sum-product loopy BP with pruning (Cotterell et al., 2015).
",5.2 Evaluation Scheme,[0],[0]
MP Perform approximate MAP inference by max-product loopy BP with pruning.,5.2 Evaluation Scheme,[0],[0]
"DD and SP improve this baseline in different ways.
",5.2 Evaluation Scheme,[0],[0]
DD predicts a string value for each variable.,5.2 Evaluation Scheme,[0],[0]
"For SP and MP, we deem the prediction at a variable to be the string that is scored most highly by the belief at that variable.
",5.2 Evaluation Scheme,[0],[0]
"We report the fraction of predicted morpheme URs that exactly match the gold-standard URs proposed by a human (Cotterell et al., 2015).",5.2 Evaluation Scheme,[0],[0]
"We also compare these predicted URs to one another, to see how well the methods agree.",5.2 Evaluation Scheme,[0],[0]
The model of Cotterell et al. (2015) has two factor types whose parameters must be chosen.10 The first is a unary factor Mφ.,5.3 Parameterization,[0],[0]
"Each underlyingmorpheme variable (layer 1 of Figure 1) is connected to a copy of Mφ, which gives the prior distribution over its values.",5.3 Parameterization,[0],[0]
The second is a binary factor Sθ.,5.3 Parameterization,[0],[0]
"For each surface word (layer 3), a copy of Sθ gives its conditional distribution given the corresponding underlying word (layer 2).",5.3 Parameterization,[0],[0]
"Mφ and Sθ respectively model the lexicon and the phonology of the specific language; both are encoded as WFSMs.
",5.3 Parameterization,[0],[0]
"10The model also has a three-way factor, connecting layers 1 and 2 of Figure 1.",5.3 Parameterization,[0],[0]
"This represents deterministic concatenation (appropriate for these languages) and has no parameters.
",5.3 Parameterization,[0],[0]
"Mφ is a 0-gram generative model: at each step it emits a character chosen uniformly from the alphabet Σ with probability φ, or halts with probability 1−φ.",5.3 Parameterization,[0],[0]
"It favors shorter strings in general, but φ determines how weak this preference is.",5.3 Parameterization,[0],[0]
"Sθ is a sequential edit model that produces a word’s SR by stochastically copying, inserting, substituting, and deleting the phonemes of its UR.",5.3 Parameterization,[0],[0]
"We explore two ways of parameterizing it.
",5.3 Parameterization,[0],[0]
"Model 1 is a simple model in which θ is a scalar, specifying the probability of copying the next character of the underlying word as it is transduced to the surface word.",5.3 Parameterization,[0],[0]
"The remaining probability mass 1−θ is apportioned equally among insertion, substitution and deletion operations.11 This models phonology as “noisy concatenation”—the minimum necessary to account for the fact that surface words cannot quite be obtained as simple concatenations of their shared underlying morphemes.
",5.3 Parameterization,[0],[0]
"Model 2 is a replication of the much more complicated parametric model of Cotterell et al. (2015), which can handle linguistic phonology.",5.3 Parameterization,[0],[0]
"Here the factor Sθ is a contextual edit FST (Cotterell et al., 2014).",5.3 Parameterization,[0],[0]
The probabilities of competing edits in a given context are determined by a loglinear model with weight vector θ and features that are meant to pick up on phonological phenomena.,5.3 Parameterization,[0],[0]
"When evaluating an inference method from section 5.2, we use the same inference method both for prediction and within training.
",5.4 Training,[0],[0]
We train Model 1 by grid search.,5.4 Training,[0],[0]
"Specifically, we choose φ ∈",5.4 Training,[0],[0]
"[0.65, 1) and θ ∈ [0.25, 1) such that the predicted forms maximize the joint score (1) (always using the (max,+) semiring).
",5.4 Training,[0],[0]
"For Model 2, we compared two methods for training the φ and θ parameters (θ is a vector):
Model 2S Supervised training, which observes the “true” (hand-constructed) values of the URs.",5.4 Training,[0],[0]
This idealized setting uses the best possible parameters (trained on the test data).,5.4 Training,[0],[0]
"Model 2E Expectation maximization (EM), whose E step imputes the unobserved URs.
",5.4 Training,[0],[0]
"EM’s E step calls for exact marginal inference, which is intractable for our model.",5.4 Training,[0],[0]
"So we substitute the same inference method that we are test-
11That is, probability mass of (1− θ)/3 is divided equally among the |Σ| possible insertions; another (1 − θ)/3 is divided equally among the |Σ|−1 possible substitutions; and the final (1− θ)/3 is allocated to deletion.
",5.4 Training,[0],[0]
ing.,5.4 Training,[0],[0]
"This gives us three approximations to EM, based on DD, SP and MP.",5.4 Training,[0],[0]
"Note that DD specifically gives the Viterbi approximation to EM— which sometimes gets better results than true EM (Spitkovsky et al., 2010).",5.4 Training,[0],[0]
"For MP (but not SP), we extract only the 1-best predictions for the E step, since we study MP as an approximation to DD.
",5.4 Training,[0],[0]
"As initialization, our first E step uses the trained version of Model 1 for the same inference method.",5.4 Training,[0],[0]
We run SP and MP for 20 iterations (usually the predictions converge within 10 iterations).,5.5 Inference Details,[0],[0]
We run DD to convergence (usually< 600 iterations).,5.5 Inference Details,[0],[0]
"DD iterations are much faster since each variable considers d strings, not d distributions over strings.",5.5 Inference Details,[0],[0]
"Hence DD does not intersect distributions, and many parts of the graph settle down early because discrete values can converge in finite time.12
We follow Paul and Eisner (2012, section 5.1) fairly closely.",5.5 Inference Details,[0],[0]
"In particular: Our stepsize in (5) is η = α/(t + 500), where t is the iteration number; α = 1 for Model 2S and α = 10 otherwise.",5.5 Inference Details,[0],[0]
"We proactively include all 1-gram and 2-gram substring features in the active sets Wi at initialization, rather than adding them only as needed.",5.5 Inference Details,[0],[0]
"At iterations 200, 400, and 600, we proactively add all 3-, 4-, and 5-gram features (respectively) on which the counts still disagree; this accelerates convergence on the few variables that have not already converged.",5.5 Inference Details,[0],[0]
We handle negative-weight cycles as Paul and Eisner do.,5.5 Inference Details,[0],[0]
"If we had ever failed to converge within 2000 iterations, we would have used their heuristic to extract a prediction anyway.
",5.5 Inference Details,[0],[0]
Model 1 suffers from a symmetry-breaking problem.,5.5 Inference Details,[0],[0]
"Many edits have identical probability, and when we run inference, many assignments will tie for highest scoring configuration.",5.5 Inference Details,[0],[0]
This can prevent DD from converging and makes performance hard to measure.,5.5 Inference Details,[0],[0]
"To break these ties, we add “jitter” separately to each copy of Mφ in Figure 1.",5.5 Inference Details,[0],[0]
"Specifically, if Fi is the unary factor attached to Xi, we expand our 0-gram model Fi(x) = log((p/|Σ|)|x| · (1 − p)) to become Fi(x) = log( ∏ c∈Σ p |x|c c,i · (1 − p)), where |x|c denotes the count of character c in string x, and pc,",5.5 Inference Details,[0],[0]
i ∝,5.5 Inference Details,[0],[0]
(p/|Σ|) ·,5.5 Inference Details,[0],[0]
"exp εc,i where εc,i ∼ N(0, 0.01) and we preserve ∑ c∈Σ pc,i = p.
12A variable need not update λ if its strings agree; a subproblem is not re-solved if none of its variables updated λ.",5.5 Inference Details,[0],[0]
"As linguists know, reconstructing an underlying stem or suffix can be difficult.",6.1 Convergence and Speed of DD,[0],[0]
We may face insufficient evidence or linguistic irregularity—or regularity that goes unrecognized because the phonological model is impoverished (Model 1) or poorly trained (early EM iterations on Model 2).,6.1 Convergence and Speed of DD,[0],[0]
DD may then require extensive negotiation to resolve disagreements among subproblems.,6.1 Convergence and Speed of DD,[0],[0]
"Furthermore, DD must renegotiate as conditions change elsewhere in the factor graph (Table 1).
",6.1 Convergence and Speed of DD,[0],[0]
DD converged in all of our experiments.,6.1 Convergence and Speed of DD,[0],[0]
Note that DD (section 4.3) has converged when all the equality constraints in (2) are satisfied.,6.1 Convergence and Speed of DD,[0],[0]
"In this case, we have found the true MAP configuration.
",6.1 Convergence and Speed of DD,[0],[0]
"In section 4.5, we discussed a block coordinate update variation (BCDD) of our DD algorithm.",6.1 Convergence and Speed of DD,[0],[0]
Figure 3 shows the convergence behavior of BCDD against the naive projected subgradient algorithm (NVDD) on the four EXERCISE languages under Model 1.,6.1 Convergence and Speed of DD,[0],[0]
"The dual objective (3) always upper-bounds the primal score (i.e., the score (1) of an assignment derived heuristically from the current subproblem solutions).",6.1 Convergence and Speed of DD,[0],[0]
The dual decreases as the algorithm progresses.,6.1 Convergence and Speed of DD,[0],[0]
"When the two objectives meet, we have found an optimal solution to the primal problem.",6.1 Convergence and Speed of DD,[0],[0]
We can see in Figure 3 that our DD algorithm converges quickly on the four EXERCISE languages and BCDD converges consistently faster than NVDD.,6.1 Convergence and Speed of DD,[0],[0]
"We use BCDD in the remaining experiments.
",6.1 Convergence and Speed of DD,[0],[0]
"When DD runs fast, it is competitive with the
other methods.",6.1 Convergence and Speed of DD,[0],[0]
"It is typically faster on the EXERCISE data, and a few times slower on the CELEX data.",6.1 Convergence and Speed of DD,[0],[0]
"But we stop the other methods after 20 iterations, whereas DD runs until it gets an exact answer.",6.1 Convergence and Speed of DD,[0],[0]
We find that this runtime is unpredictable and sometimes quite long.,6.1 Convergence and Speed of DD,[0],[0]
"In the grid search for training Model 1, we observed that changes in the parameters (φ, θ) could cause the runtime of DD inference to vary by 2 orders of magnitude.",6.1 Convergence and Speed of DD,[0],[0]
"Similarly, on the CELEX data, the runtime on Model 1 (over 10 different N = 600 subsets of English) varied from about 1 hour to nearly 2 days.13",6.1 Convergence and Speed of DD,[0],[0]
"For each language, we constructed several different unsupervised prediction problems.",6.2 Comparison of Inference,[0],[0]
"In each problem, we observe some size-N subset of the words in our dataset, and we attempt to predict the URs of the morphemes in those words.",6.2 Comparison of Inference,[0],[0]
"For each CELEX language, we took N = 600, and used three of the size-N training sets from (Cotterell et al., 2015).",6.2 Comparison of Inference,[0],[0]
"For each EXERCISE language, we took N to be one less than the dataset size, and used all N + 1 subsets of size N , again similar to (Cotterell et al., 2015).",6.2 Comparison of Inference,[0],[0]
"We report the unweighted macro-average of all these accuracy numbers.
13Note that our implementation is not optimized; e.g., it uses Python (not Cython).
",6.2 Comparison of Inference,[0],[0]
"We compare DD, SP, and MP inference on each language under different settings.",6.2 Comparison of Inference,[0],[0]
"Table 2 shows aggregate results, as an unweighted average over multiple languages and training sets.",6.2 Comparison of Inference,[0],[0]
We present various additional results at http://cs.,6.2 Comparison of Inference,[0],[0]
"jhu.edu/˜npeng/emnlp2015/, including a perlanguage breakdown of the results, runtime numbers, and significance tests.
",6.2 Comparison of Inference,[0],[0]
The results for Model 1 are shown in Tables 2a and 2b.,6.2 Comparison of Inference,[0],[0]
"As we can see, in both datasets, dual decomposition performed the best at recovering the URs, while MP performed the worst.",6.2 Comparison of Inference,[0],[0]
"Both DD and MP are doing MAP inference, so the differences reflect the search error in MP.",6.2 Comparison of Inference,[0],[0]
"Interestingly, DD agrees more with SP than with MP, even though SP uses marginal inference.
",6.2 Comparison of Inference,[0],[0]
"Although the aggregate results on the EXERCISE dataset show a large improvement of DD over both of the BP algorithms, the gain all comes from the English language.",6.2 Comparison of Inference,[0],[0]
"SP actually does better than DD on Catalan and Maori, and MP also gets better results than DD on Maori, tying with SP.
",6.2 Comparison of Inference,[0],[0]
"For Model 2S, all inference methods achieved 100% accuracy on the EXERCISE dataset, so we do not show a table.",6.2 Comparison of Inference,[0],[0]
The results on the CELEX dataset are shown in Table 2c.,6.2 Comparison of Inference,[0],[0]
"Here both DD and MP performed equally well, and outperformed BP—a result like (Spitkovsky et al., 2010).",6.2 Comparison of Inference,[0],[0]
This trend is consistent over all three languages: DD and MP always achieve similar results and both outperform SP.,6.2 Comparison of Inference,[0],[0]
"Of course, one advantage of DD in the setting is that it actually finds the true MAP prediction of the model; the errors are known to be due to the model, not the search procedure.
",6.2 Comparison of Inference,[0],[0]
"For Model 2E, we show results on the EXERCISE dataset in Table 2d.",6.2 Comparison of Inference,[0],[0]
Here the results resemble the pattern of Model 1.,6.2 Comparison of Inference,[0],[0]
"We presented a general dual decomposition algorithm for MAP inference on graphical models over strings, and applied it to an unsupervised learning task in phonology.",7 Conclusion and Future Work,[0],[0]
"The experiments show that our DD algorithm converges and gets better results than both max-product and sum-product BP.
Techniques should be explored to speed up the DD method.",7 Conclusion and Future Work,[0],[0]
"Adapting the MPLP algorithm (Sontag et al., 2011) to the string-valued case would be a nontrivial extension.",7 Conclusion and Future Work,[0],[0]
"We could also explore other serial update schemes, which generally speed up message-passing algorithms over parallel update.",7 Conclusion and Future Work,[0],[0]
We investigate dual decomposition for joint MAP inference of many strings.,abstractText,[0],[0]
"Given an arbitrary graphical model, we decompose it into small acyclic sub-models, whose MAP configurations can be found by finite-state composition and dynamic programming.",abstractText,[0],[0]
"We force the solutions of these subproblems to agree on overlapping variables, by tuning Lagrange multipliers for an adaptively expanding set of variable-length n-gram count features.",abstractText,[0],[0]
"This is the first inference method for arbitrary graphical models over strings that does not require approximations such as random sampling, message simplification, or a bound on string length.",abstractText,[0],[0]
"Provided that the inference method terminates, it gives a certificate of global optimality (though MAP inference in our setting is undecidable in general).",abstractText,[0],[0]
"On our global phonological inference problems, it always terminates, and achieves more accurate results than max-product and sum-product loopy belief propagation.",abstractText,[0],[0]
Dual Decomposition Inference for Graphical Models over Strings,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4725–4730 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
4725
In this paper, we propose a new approach to employ the fixed-size ordinally-forgetting encoding (FOFE) (Zhang et al., 2015b) in neural languages modelling, called dual-FOFE. The main idea behind dual-FOFE is that it allows to use two different forgetting factors so that it can avoid the trade-off in choosing either small or large values for the single forgetting factor in the original FOFE. In our experiments, we have compared the dual-FOFE based neural network language models (NNLM) against the original FOFE counterparts and various traditional NNLMs. Our results on the challenging Google Billion Words corpus show that both FOFE and dual FOFE yield very strong performance while significantly reducing the computational complexity over other NNLMs. Furthermore, the proposed dual-FOFE method further gives over 10% relative improvement in perplexity over the original FOFE model.",text,[0],[0]
"Language modelling is an essential task for many natural language processing (NLP) applications including speech recognition, machine translation and text summarization.",1 Introduction,[0],[0]
"The goal of language modelling is to learn the distribution over a sequence of characters or words; this distribution may be utilized for encoding the language structure (e.g. the grammatical structure) as well as extracting information from the corpora (Jozefowicz et al., 2016).",1 Introduction,[0],[0]
"In the recent years, the popularity of neural networks (NN) has been a significant driving force for language modelling (LM) research; the well-known NN-LM models includes the feedforward NN-LMs (FNN-LMs) (Bengio et al., 2001, 2003), recurrent NN-LMs (RNN-LMs) (Mikolov et al., 2010; Mikolov
∗Equal contribution.
",1 Introduction,[0],[0]
"and Zweig, 2012) and the long short-term memory (LSTM-LMs) (Hochreiter and Schmidhuber, 1997).",1 Introduction,[0],[0]
"Among all, FNN-LMs often have a simpler and more efficient learning process, but they tend to underperform the other NN-LMs due to the limited capability to memorize the long term dependency in natural languages (Zhang et al., 2015b).",1 Introduction,[0],[0]
However this drawback could be addressed by applying the fixed-size ordinally-forgetting encoding (FOFE) to FNN’s inputs.,1 Introduction,[0],[0]
"FOFE is an encoding method, which relies on the ordinally-forgetting mechanism to encode any word sequence based on the positions of words; this also allows the FOFE code to capture the long-term dependency (Zhang et al., 2015b).",1 Introduction,[0],[0]
"As shown in Zhang (2015b), FNNLMs with FOFE can easily yield comparable performance as other NN-LMs.",1 Introduction,[0],[0]
"The key parameter in the FOFE method is the forgetting factor, which is responsible for determining the degree of sensitivity of the encoding with respect to the past context.",1 Introduction,[0],[0]
"However, the choice of a good value for the forgetting factor could be tricky since both small and large forgetting factors are offering different benefits.
",1 Introduction,[0],[0]
"In this paper, we propose a simple alteration to FOFE method, which allows to incorporate two forgetting factors into the fixed-size encoding of the variable-length word sequences.",1 Introduction,[0],[0]
We name this approach as dual-FOFE.,1 Introduction,[0],[0]
"Our hypothesis is that by incorporating both the small and large forgetting factors in the FOFE encoding, the dual-FOFE is able to simultaneously optimize the abilities to capture the positional information as well as to model long term dependency.",1 Introduction,[0],[0]
"In our experiments, we have evaluated the proposed dual FOFE models on two large scale language modeling tasks, namely enwiki9 and Google Billion Words (GBW) corpora.",1 Introduction,[0],[0]
"Experimental results have shown that both FOFE models yield very competitive performance on these tasks, comparable with the state-
of-the-art systems but with significantly reduced learning complexity.",1 Introduction,[0],[0]
"Furthermore, the proposed dual-FOFE method further gives over 10% relative improvement in perplexity over the original FOFE model.",1 Introduction,[0],[0]
"In this section, we will briefly review the NN-LMs and the original FOFE method.",2 Related Work,[0],[0]
"The general idea behind NN-LM is to project the discrete words onto a continuous space, then learn to estimate the conditional probabilities of each known word within the projected space.",2 Related Work,[0],[0]
The training of NNLMs are often incredibly slow due to the inefficiency of softmax normalization when applied to the extremely large output layer.,2 Related Work,[0],[0]
"The solution currently used by many NN-LMs (including our models in this work) is to use noise contrastive estimation (NCE) (Gutmann and Hyvrinen, 2010).",2 Related Work,[0],[0]
"The basic idea of NCE is to reduce the probability estimation problem into a probabilistic binary classification problem (Mnih and Teh, 2012; Mnih and Kavukcuoglu, 2013).",2 Related Work,[0],[0]
"Fixed-size ordinally-forgetting Encoding (FOFE) is an encoding method which generates a fixedsize representation, namely the FOFE code, for any variable-length word sequence (Zhang et al., 2015b).",2.1 Fixed-Size Ordinally Forgetting Encoding,[0],[0]
"For a given word sequence S = {w1, w2, ..., wT }, let et denote the one-hot representation of the word wt, zt for the FOFE code of the partial word sequence up to word wt, zt is computed as follows:
zt = α · zt−1 + et (1 ≤ t ≤ T ) (1)
where α (0< α < 1) denotes the forgetting factor, a parameter responsible for determining the degree of influence each time step of the past context has on the FOFE code.",2.1 Fixed-Size Ordinally Forgetting Encoding,[0],[0]
"Obviously, FOFE can convert any variable-length sequence into a fixed-size code with length equal to the size of vocabulary.
",2.1 Fixed-Size Ordinally Forgetting Encoding,[0],[0]
"In regard to uniqueness of FOFE code, the code is said to be (almost) unique under the two theorems (proven in Zhang (2015a)):",2.1 Fixed-Size Ordinally Forgetting Encoding,[0],[0]
Theorem 1,2.1 Fixed-Size Ordinally Forgetting Encoding,[0],[0]
"If 0 < α ≤ 0.5, then FOFE code is guarantee uniqueness for any values of vocabulary’s size and sequence’s length.",2.1 Fixed-Size Ordinally Forgetting Encoding,[0],[0]
Theorem 2,2.1 Fixed-Size Ordinally Forgetting Encoding,[0],[0]
"If 0.5 < α < 1, then FOFE code is guarantee almost uniqueness for any finite values
of vocabulary’s size and sequence’s length, except for a finite set of countable choices of α.
",2.1 Fixed-Size Ordinally Forgetting Encoding,[0],[0]
"Furthermore, the chance of actually having any collisions for α between 0.5 and 1 is nearly impossible in practice, due to quantization errors in real computer systems.",2.1 Fixed-Size Ordinally Forgetting Encoding,[0],[0]
"Hence in practice, it is safe to argue that FOFE is able to uniquely encodes any variable-length sequence into a fixed-size representation.",2.1 Fixed-Size Ordinally Forgetting Encoding,[0],[0]
"The idea of FOFE based NN-LMs is to use FOFE to encode the partial history sequence of past words in a sentence, then feed this fixed-size FOFE code to a feedforward neural network as an input to predict next word.",2.2 FOFE for FNN-LMs,[0],[0]
"As shown in Figure 1, the FOFE code could be efficiently computed via time-delayed recursive structure, where the symbol z−1 in the figure represents a unit time delay (or equivalently a memory unit) from zt to zt−1.
",2.2 FOFE for FNN-LMs,[0],[0]
The basic architecture for FOFE based FNNLMs (called 1st-order) is the standard FNN architecture with an additional layer for encoding the input into FOFE code.,2.2 FOFE for FNN-LMs,[0],[0]
"However, in this work, we use the 2nd-order and 3rd-order FOFE FNNLMs, which are shown to produce slightly better results (Zhang et al., 2015b).",2.2 FOFE for FNN-LMs,[0],[0]
"In a 2nd-order FOFE model, both the current partial sequences FOFE code (denoted as zt) and the previous partial sequences FOFE code (denoted as zt−1) are utilized to predict next word.",2.2 FOFE for FNN-LMs,[0],[0]
"In a 3rd-order FOFE model, all zt, zt−1 and zt−2 are used as inputs to neural networks.
",2.2 FOFE for FNN-LMs,[0],[0]
"More recently, the FOFE methods have been successfully applied to many NLP tasks, including word embedding (Sanu et al., 2017), named entity recognition (Xu et al., 2017a), entity discovery and linking (Xu et al., 2016, 2017b).",2.2 FOFE for FNN-LMs,[0],[0]
The main idea of dual-FOFE is to generate augmented FOFE encoding codes by concatenating two FOFE codes using two different forgetting factors.,3 Dual-FOFE,[0],[0]
Each of these FOFE codes is still computed in the same way as the mathematical formulation shown in Equation (1).,3 Dual-FOFE,[0],[0]
The difference between them is that we may select to use two different values for the forgetting factor (denoted as α) for additional modeling benefits.,3 Dual-FOFE,[0],[0]
"As mentioned in the subsection 2.1, the values in a FOFE code are used to encode both the content and the order information in a sequence.",3.1 Intuition behind Dual-FOFE,[0],[0]
This is achieved by a recursive encoding method where at each recursive step the code will be multiplied by the forgetting factor (α) whose value is bounded by 0 < α < 1.,3.1 Intuition behind Dual-FOFE,[0],[0]
"In a practical computer with finite precision, this has an impact on the FOFE’s abilities to precisely memorize the long-term dependency of past context as well as to properly represent the positional information.
",3.1 Intuition behind Dual-FOFE,[0],[0]
The FOFE’s ability to represent the positional information would improve with smaller forgetting factors.,3.1 Intuition behind Dual-FOFE,[0],[0]
"The reason is that that when α is small, the FOFE code (zt) for each word vastly differs from its neighbour in magnitude.",3.1 Intuition behind Dual-FOFE,[0],[0]
"If α is too large (close to 1), the contribution of a word may not change too much no matter where it is.",3.1 Intuition behind Dual-FOFE,[0],[0]
This may hamper the following neural networks to model the positional information.,3.1 Intuition behind Dual-FOFE,[0],[0]
"Conversely, the FOFE’s ability to model the long-term dependency of the older context would improve with larger forgetting factors.",3.1 Intuition behind Dual-FOFE,[0],[0]
"This is because when α is small, the contribution of a word from the older history may quickly underflow to become irrelevant (i.e. forgotten) when computing the current word.
",3.1 Intuition behind Dual-FOFE,[0],[0]
"In the original FOFE with just a single forgetting factor, we would have to determine the best trade-off between these two benefits.",3.1 Intuition behind Dual-FOFE,[0],[0]
"On the other hand, the dual-FOFE does not face such issues since it is composed of two FOFE codes: the half of the dual-FOFE code using a smaller forgetting factor is solely optimized and responsible for representing the positional information of all words in the sequence; meanwhile the other half of the dual-FOFE code using a larger forgetting factor is optimized and responsible for maintaining the long-term dependency of past context.",3.1 Intuition behind Dual-FOFE,[0],[0]
"As shown in Figure 2, the architecture of dualFOFE based FNN-LMs is very similar to the original FOFE FNN-LMs.1 In the Dual-FOFE FNNLMs, the input word sequence would have to pass through two branches of the FOFE layers (using two different forgetting factors) and each encoding branch will produce a FOFE code representing the input sequence.",3.2 Dual-FOFE based FNN-LM,[0],[0]
"These two FOFE codes are then joined to produce the dual-FOFE code, which would be fed to FNNs to predict the next word.
",3.2 Dual-FOFE based FNN-LM,[0],[0]
"It might also be worth noting that in our implementation we do not explicitly reset FOFE codes, i.e. zt value, at sentence boundaries.",3.2 Dual-FOFE based FNN-LM,[0],[0]
"However, faraway histories will be gradually forgotten by the recursive calculation in FOFE due to 0 <",3.2 Dual-FOFE based FNN-LM,[0],[0]
α < 1 and finite precision in computers.,3.2 Dual-FOFE based FNN-LM,[0],[0]
"As mentioned previously in the subsection 2.2, the higer order FOFE codes would utilize both the current and the previous sequence FOFE codes for prediction.",3.3 Dual-FOFE vs. Higher Order FOFE,[0],[0]
"Hence similar to dual-FOFE, the higher order FOFE could also maintain the sensitivity to both nearby and faraway context.",3.3 Dual-FOFE vs. Higher Order FOFE,[0],[0]
Obviously a much higher order FOFE code may be required in order to achieve the same effect as dual-FOFE in terms of modelling long-term dependency.,3.3 Dual-FOFE vs. Higher Order FOFE,[0],[0]
"In this case, the higher order FOFE may also significantly increase the number of parameters in the input layer.",3.3 Dual-FOFE vs. Higher Order FOFE,[0],[0]
"At last, the dual FOFE
1The difference in the location of the projection layer between Figure 1 and 2 simply indicates two equivalent ways to do word projection.",3.3 Dual-FOFE vs. Higher Order FOFE,[0],[0]
"Figure 1 was originally from Zhang (2015b), but they mentioned in text (without a figure) that it is more efficient to do projection as in Figure 2 and both methods are mathematically equivalent since both projection and FOFE steps are linear.
",3.3 Dual-FOFE vs. Higher Order FOFE,[0],[0]
and the higher order FOFE are largely complementary since we have observed consistent performance gains when combining dual FOFE with either 2nd-order or 3rd-order FOFE in our experiments.,3.3 Dual-FOFE vs. Higher Order FOFE,[0],[0]
"In this work, we have evaluated the proposed dual-FOFE based FNN-LMs against various traditional neural language models on two corpora: i) enwik9 corpus: it consists of the first 1 billion bytes of English wikipedia dump, having total size of 170.8 million words; the corpus was divided into three parts: the training set (153M words), the test set (8.9M words), and the validation set (8.9M words); the vocabulary size is limited to 80k words (Zhang et al., 2015b).",4 Experiments,[0],[0]
"ii) Google Billion Words (GBW) corpus: it contains about 800 million words and the corpus is divided into two parts: the training set (792M words) and the test set (8M words); the vocabulary size for this corpus is limited to 800k words (Chelba et al., 2013).",4 Experiments,[0],[0]
"In the experiments on the enwiki9 corpus, we have trained three dual-FOFE FNN-LMs with different forgetting factor pairs, one FOFE FNN-LM, and one tri-FOFE FNN-LM.",4.1 Results on enwiki9,[0],[0]
"All five models adopt a 2nd-order FOFE structure, employing a word embeddings of 256 dimensions, three hidden layers of 400, 600, 600 neurons and an output layer of 80k words (reflecting the vocabulary).",4.1 Results on enwiki9,[0],[0]
2 Note that the dual-FOFE FNN-LMs have to double the size of input context windows since dual-FOFE essentially contain two FOFE codes.,4.1 Results on enwiki9,[0],[0]
"But this increase only accounts for a negligible faction of total model parameters.
",4.1 Results on enwiki9,[0],[0]
"As shown in Table 1, all three dual-FOFE FNNLMs, using three pairs of forgetting factors as (0.5, 0.7) and (0.7, 0.9) and (0.5, 0.9), can significantly outperform other traditional models previously reported on this corpus.",4.1 Results on enwiki9,[0],[0]
"We also note that it is beneficial to include a relatively large forgetting factor, such as 0.9, in the dual FOFE models since such a large alpha may help to memorize much longer context in the inputs.",4.1 Results on enwiki9,[0],[0]
"When compared with the original FOFE counterpart, the best dual-FOFE model using forgetting factors (0.5, 0.9) offers a relative gain of around 8% in test PPL.
2Comparing with Zhang (2015b), our single FOFE FNNLM baseline use a slightly larger model, which lead to slightly better perplexity.
",4.1 Results on enwiki9,[0],[0]
It is worth noting that our dual-FOFE models can be extended to incorporate more than two alpha values.,4.1 Results on enwiki9,[0],[0]
"In fact after we have obtained a strong result supporting our dual-FOFE hypothesis, we have performed additional experiments using three alpha values, the so-called tri-FOFE model.",4.1 Results on enwiki9,[0],[0]
The result on Table 1 has shown that the tri-FOFE FNN-LMs still slightly outperforms the dual-FOFE models.,4.1 Results on enwiki9,[0],[0]
"However, the gain is marginal.",4.1 Results on enwiki9,[0],[0]
This leads us to believing that further extension of more alpha values in FOFE would be of limited use.,4.1 Results on enwiki9,[0],[0]
"In the experiments on the GBW corpus, we have trained one dual-FOFE FNN-LM and one FOFE FNN-LM.",4.2 Results on Google Billion Words (GBW),[0],[0]
"Following the best dual-FOFE model configuration on the previous corpus, this dualFOFE FNN-LM uses the same pair of dual forgetting factors (0.5, 0.9).",4.2 Results on Google Billion Words (GBW),[0],[0]
"Both models adopt a 3rd-order structure, employing word embeddings of 256 dimensions, three hidden layers each of 4096 neurons, a compression layer with 720 neurons, and an output layer of 800k words (reflecting the vocabulary).",4.2 Results on Google Billion Words (GBW),[0],[0]
"Although dual-FOFE FNNLM has doubled the size of input context windows of FOFE FNN-LM, the total number of model parameters in both models are almost equal, roughly 0.82 billion parameters.
",4.2 Results on Google Billion Words (GBW),[0],[0]
"As shown in Table 2, the dual-FOFE FNN-LM is able to produce a very competitive performance, comparable with the best previously reported results on this task, such as GCNN-13 (Dauphin et al., 2016) and LSTM-LM (Jozefowicz et al., 2016).",4.2 Results on Google Billion Words (GBW),[0],[0]
The dual-FOFE FNN-LM are among the few single-model systems that are able to achieve test PPL below 40 on this task.,4.2 Results on Google Billion Words (GBW),[0],[0]
"Furthermore, our proposed dual FOFE model can significantly reduce the computational complexity, e.g., our model has a relatively smaller number of parameter (0.82B parameters) and it requires much less hardware resource to train (using only 1 GPU in our experiments).",4.2 Results on Google Billion Words (GBW),[0],[0]
"When compared with the original FOFE counterpart, the dual-FOFE FNN-LM is able to provide approximately 11% relative improvement in PPL.",4.2 Results on Google Billion Words (GBW),[0],[0]
"In this paper, we have proposed a new approach of utilizing the fixed-size ordinally-forgetting encoding (FOFE) method for neural network lan-
guage models (NN-LMs), known as dual-FOFE.",5 Conclusions,[0],[0]
"As the name implies, this approach involves to produce a new fixed-sized representation for any variable-length sequence from a concatenation of two FOFE codes.",5 Conclusions,[0],[0]
This would have allowed us to select two values for the forgetting factors.,5 Conclusions,[0],[0]
One FOFE code with a smaller forgetting factor is responsible for representing the positional information of all words in the sequence while the other using a larger forgetting factor is responsible for modelling the even longer term dependency in far away history.,5 Conclusions,[0],[0]
Our experiments on both enwiki9 and Google Billion Words (GBW) tasks have both demonstrated the effectiveness of the dual-FOFE modeling approach.,5 Conclusions,[0],[0]
"Experimental results on the challenging GBW corpus have shown that the dual-FOFE FNN-LM has achieved over 10% improvement in perplexity over the original FOFE FNN-LM, without any significant drawback in model and learning complexity.",5 Conclusions,[0],[0]
"When compared with other traditional neural language models, the dual-FOFE FNN-LM has achieved com-
petitive performance with significantly lower computational complexity.",5 Conclusions,[0],[0]
"This work is supported mainly by a research donation from iFLYTEK Co., Ltd., Hefei, China, and partially by a discovery grant from Natural Sciences and Engineering Research Council (NSERC) of Canada.",Acknowledgement,[0],[0]
"In this paper, we propose a new approach to employ the fixed-size ordinally-forgetting encoding (FOFE) (Zhang et al., 2015b) in neural languages modelling, called dual-FOFE.",abstractText,[0],[0]
The main idea behind dual-FOFE is that it allows to use two different forgetting factors so that it can avoid the trade-off in choosing either small or large values for the single forgetting factor in the original FOFE.,abstractText,[0],[0]
"In our experiments, we have compared the dual-FOFE based neural network language models (NNLM) against the original FOFE counterparts and various traditional NNLMs.",abstractText,[0],[0]
Our results on the challenging Google Billion Words corpus show that both FOFE and dual FOFE yield very strong performance while significantly reducing the computational complexity over other NNLMs.,abstractText,[0],[0]
"Furthermore, the proposed dual-FOFE method further gives over 10% relative improvement in perplexity over the original FOFE model.",abstractText,[0],[0]
Dual Fixed-Size Ordinally Forgetting Encoding (FOFE) for Competitive Neural Language Models,title,[0],[0]
"Sparse learning has emerged as an effective approach to alleviate model overfitting when feature dimension
1Department of CS, Rutgers University, Piscataway, NJ, 08854, USA.",1. Introduction,[0],[0]
"2B-DAT Lab, Nanjing University of Information Science & Technology, Nanjing, Jiangsu, 210044, China.",1. Introduction,[0],[0]
Correspondence to:,1. Introduction,[0],[0]
"Bo Liu <lb507@cs.rutgers.edu>.
Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
outnumbers training sample.,1. Introduction,[0],[0]
"Given a set of training samples{(xi, yi)}Ni=1 in which xi ∈ Rd is the feature representation and yi ∈ R the corresponding label, the following sparsity-constrained `2-norm regularized loss minimization problem is often considered in high-dimensional analysis:
min ‖w‖0≤k
P (w) := 1
N N∑ i=1",1. Introduction,[0],[0]
"l(w>xi, yi) +",1. Introduction,[0],[0]
λ 2 ‖w‖2.,1. Introduction,[0],[0]
"(1)
Here l(·; ·) is a convex loss function, w ∈ Rd is the model parameter vector and λ controls the regularization strength.",1. Introduction,[0],[0]
"For example, the squared loss l(a, b) =",1. Introduction,[0],[0]
"(b − a)2 is used in linear regression and the hinge loss l(a, b) = max{0, 1− ab} in support vector machines.",1. Introduction,[0],[0]
"Due to the presence of cardinality constraint ‖w‖0 ≤ k, the problem (1) is simultaneously non-convex and NP-hard in general, and thus is challenging for optimization.",1. Introduction,[0],[0]
"A popular way to address this challenge is to use proper convex relaxation, e.g., `1 norm (Tibshirani, 1996) and k-support norm (Argyriou et al., 2012), as an alternative of the cardinality constraint.",1. Introduction,[0],[0]
"However, the convex relaxation based techniques tend to introduce bias for parameter estimation.
",1. Introduction,[0],[0]
"In this paper, we are interested in algorithms that directly minimize the non-convex formulation in (1).",1. Introduction,[0],[0]
"Early efforts mainly lie in compressed sensing for signal recovery, which is a special case of (1) with squared loss.",1. Introduction,[0],[0]
"Among others, a family of the so called Iterative Hard Thresholding (IHT) methods (Blumensath & Davies, 2009; Foucart, 2011) have gained significant interests and they have been witnessed to offer the fastest and most scalable solutions in many cases.",1. Introduction,[0],[0]
"More recently, IHT-style methods have been generalized to handle generic convex loss functions (Beck & Eldar, 2013; Yuan et al., 2014; Jain et al., 2014) as well as structured sparsity constraints (Jain et al., 2016).",1. Introduction,[0],[0]
"The common theme of these methods is to iterate between gradient descent and hard thresholding to maintain sparsity of solution while minimizing the objective value.
",1. Introduction,[0],[0]
"Although IHT-style methods have been extensively studied, the state-of-the-art is only designed for the primal formulation (1).",1. Introduction,[0],[0]
"It remains an open problem to investigate the feasibility of solving the original NP-hard/non-convex formulation in a dual space that might potentially further im-
prove computational efficiency.",1. Introduction,[0],[0]
"To fill this gap, inspired by the recent success of dual methods in regularized learning problems, we systematically build a sparse duality theory and propose an IHT-style algorithm along with its stochastic variant for dual optimization.
",1. Introduction,[0],[0]
Overview of our contribution.,1. Introduction,[0],[0]
The core contribution of this work is two-fold in theory and algorithm.,1. Introduction,[0],[0]
"As the theoretical contribution, we have established a novel sparse Lagrangian duality theory for the NP-hard/non-convex problem (1) which to the best of our knowledge has not been reported elsewhere in literature.",1. Introduction,[0],[0]
We provide in this part a set of sufficient and necessary conditions under which one can safely solve the original non-convex problem through maximizing its concave dual objective function.,1. Introduction,[0],[0]
"As the algorithmic contribution, we propose the dual IHT (DIHT) algorithm as a super-gradient method to maximize the nonsmooth dual objective.",1. Introduction,[0],[0]
"In high level description, DIHT iterates between dual gradient ascent and primal hard thresholding pursuit until convergence.",1. Introduction,[0],[0]
A stochastic variant of DIHT is proposed to handle large-scale learning problems.,1. Introduction,[0],[0]
"For both algorithms, we provide non-asymptotic convergence analysis on parameter estimation error, sparsity recovery, and primal-dual gap as well.",1. Introduction,[0],[0]
"In sharp contrast to the existing analysis for primal IHT-style algorithms, our analysis is not relying on Restricted Isometry Property (RIP) conditions and thus is less restrictive in real-life highdimensional statistical settings.",1. Introduction,[0],[0]
Numerical results on synthetic datasets and machine learning benchmark datasets demonstrate that dual IHT significantly outperforms the state-of-the-art primal IHT algorithms in accuracy and efficiency.,1. Introduction,[0],[0]
"The theoretical and algorithmic contributions of this paper are highlighted in below:
• Sparse Lagrangian duality theory: we established a sparse saddle point theorem (Theorem 1), a sparse mini-max theorem (Theorem 2) and a sparse strong duality theorem (Theorem 3).
",1. Introduction,[0],[0]
• Dual optimization: we proposed an IHT-style algorithm along with its stochastic extension for nonsmooth dual maximization.,1. Introduction,[0],[0]
"These algorithms have been shown to converge at the rate of 1 ln 1 in dual
parameter estimation error and 1 2 ln 1 2 in primal-dual gap (see Theorem 4 and Theorem 5).",1. Introduction,[0],[0]
"These guarantees are invariant to RIP conditions which are required by virtually all the primal IHT-style methods without using relaxed sparsity levels.
",1. Introduction,[0],[0]
Notation.,1. Introduction,[0],[0]
"Before continuing, we define some notations to be used.",1. Introduction,[0],[0]
Let x ∈ Rd be a vector and F be an index set.,1. Introduction,[0],[0]
We use HF (x) to denote the truncation operator that restricts x to the set F . Hk(x) is a truncation operator which preserves the top k (in magnitude) entries of x and sets the remaining to be zero.,1. Introduction,[0],[0]
"The notation supp(x) represents the index set
of nonzero entries of x. We conventionally define ‖x‖∞ = maxi |[x]i| and define xmin = mini∈supp(x) |[x]i|.",1. Introduction,[0],[0]
"For a matrixA, σmax(A) (σmin(A)) denotes its largest (smallest) singular value.
Organization.",1. Introduction,[0],[0]
The rest of this paper is organized as follows: In §2 we briefly review some relevant work.,1. Introduction,[0],[0]
In §3 we develop a Lagrangian duality theory for sparsityconstrained minimization problems.,1. Introduction,[0],[0]
The dual IHT-style algorithms along with convergence analysis are presented in §4.,1. Introduction,[0],[0]
The numerical evaluation results are reported in §5.1.,1. Introduction,[0],[0]
"Finally, the concluding remarks are made in §6.",1. Introduction,[0],[0]
All the technical proofs are deferred to the appendix sections.,1. Introduction,[0],[0]
"For generic convex objective beyond quadratic loss, the rate of convergence and parameter estimation error of IHT-style methods were established under proper RIP (or restricted strong condition number) bound conditions (Blumensath, 2013; Yuan et al., 2014; 2016).",2. Related Work,[0],[0]
"In (Jain et al., 2014), several relaxed variants of IHT-style algorithms were presented for which the estimation consistency can be established without requiring the RIP conditions.",2. Related Work,[0],[0]
"In (Bahmani et al., 2013), a gradient support pursuit algorithm is proposed and analyzed.",2. Related Work,[0],[0]
"In large-scale settings where a full gradient evaluation on all data becomes a bottleneck, stochastic and variance reduction techniques have been adopted to improve the computational efficiency of IHT (Nguyen et al., 2014; Li et al., 2016; Chen & Gu, 2016).
",2. Related Work,[0],[0]
"Dual optimization algorithms have been widely used in various learning tasks including SVMs (Hsieh et al., 2008) and multi-task learning (Lapin et al., 2014).",2. Related Work,[0],[0]
"In recent years, stochastic dual optimization methods have gained significant attention in large-scale machine learning (ShalevShwartz & Zhang, 2013a;b).",2. Related Work,[0],[0]
"To further improve computational efficiency, some primal-dual methods are developed to alternately minimize the primal objective and maximize the dual objective.",2. Related Work,[0],[0]
"The successful examples of primal-dual methods include learning total variation regularized model (Chambolle & Pock, 2011) and generalized Dantzig selector (Lee et al., 2016).",2. Related Work,[0],[0]
"More recently, a number of stochastic variants (Zhang & Xiao, 2015; Yu et al., 2015) and parallel variants (Zhu & Storkey, 2016) were developed to make the primal-dual algorithms more scalable and efficient.",2. Related Work,[0],[0]
"In this section, we establish weak and strong duality theory that guarantees the original non-convex and NP-hard problem in (1) can be equivalently solved in a dual space.",3. A Sparse Lagrangian Duality Theory,[0],[0]
"The results in this part build the theoretical foundation of developing dual IHT methods.
",3. A Sparse Lagrangian Duality Theory,[0],[0]
"From now on we abbreviate li(w>xi) = l(w>xi, yi).",3. A Sparse Lagrangian Duality Theory,[0],[0]
"The convexity of l(w>xi, yi) implies that li(u) is also convex.",3. A Sparse Lagrangian Duality Theory,[0],[0]
Let l∗i (αi) = maxu {αiu − li(u)} be the convex conjugate of li(u) and F ⊆ R be the feasible set of αi.,3. A Sparse Lagrangian Duality Theory,[0],[0]
"According to the well-known expression of li(u) = maxαi∈F {αiu− l∗i (αi)}, the problem (1) can be reformulated into the following mini-max formulation:
min ‖w‖0≤k
1
N N∑ i=1",3. A Sparse Lagrangian Duality Theory,[0],[0]
max αi∈F {αiw>xi − l∗i (αi)}+ λ 2 ‖w‖2.,3. A Sparse Lagrangian Duality Theory,[0],[0]
"(2)
The following Lagrangian form will be useful in analysis:
L(w,α) = 1
N N∑ i=1",3. A Sparse Lagrangian Duality Theory,[0],[0]
( αiw >xi − l∗i (αi) ),3. A Sparse Lagrangian Duality Theory,[0],[0]
"+ λ 2 ‖w‖2,
where α =",3. A Sparse Lagrangian Duality Theory,[0],[0]
"[α1, ..., αN ]",3. A Sparse Lagrangian Duality Theory,[0],[0]
∈ FN is the vector of dual variables.,3. A Sparse Lagrangian Duality Theory,[0],[0]
"We now introduce the following concept of sparse saddle point which is a restriction of the conventional saddle point to the setting of sparse optimization.
",3. A Sparse Lagrangian Duality Theory,[0],[0]
Definition 1 (Sparse Saddle Point).,3. A Sparse Lagrangian Duality Theory,[0],[0]
"A pair (w̄, ᾱ) ∈",3. A Sparse Lagrangian Duality Theory,[0],[0]
"Rd × FN is said to be a k-sparse saddle point for L if ‖w̄‖0 ≤ k and the following holds for all ‖w‖0 ≤ k, α ∈ FN :
L(w̄, α) ≤",3. A Sparse Lagrangian Duality Theory,[0],[0]
"L(w̄, ᾱ) ≤ L(w, ᾱ).",3. A Sparse Lagrangian Duality Theory,[0],[0]
"(3)
Different from the conventional definition of saddle point, the k-sparse saddle point only requires the inequality (3) holds for any arbitrary k-sparse vector w. The following result is a basic sparse saddle point theorem for L. Throughout the paper, we will use f ′(·) to denote a subgradient (or super-gradient) of a convex (or concave) function f(·), and use ∂f(·) to denote its sub-differential (or super-differential).
",3. A Sparse Lagrangian Duality Theory,[0],[0]
Theorem 1 (Sparse Saddle Point Theorem).,3. A Sparse Lagrangian Duality Theory,[0],[0]
Let w̄,3. A Sparse Lagrangian Duality Theory,[0],[0]
∈,3. A Sparse Lagrangian Duality Theory,[0],[0]
Rd be,3. A Sparse Lagrangian Duality Theory,[0],[0]
a k-sparse primal vector and ᾱ ∈ FN be a dual vector.,3. A Sparse Lagrangian Duality Theory,[0],[0]
"Then the pair (w̄, ᾱ) is a sparse saddle point for L if and only if the following conditions hold:
(a) w̄ solves the primal problem in (1);
(b) ᾱ ∈",3. A Sparse Lagrangian Duality Theory,[0],[0]
"[∂l1(w̄>x1), ..., ∂lN (w̄>xN )]; (c) w̄",3. A Sparse Lagrangian Duality Theory,[0],[0]
"= Hk ( − 1λN ∑N i=1 ᾱixi ) .
",3. A Sparse Lagrangian Duality Theory,[0],[0]
Proof.,3. A Sparse Lagrangian Duality Theory,[0],[0]
"A proof of this result is given in Appendix A.1.
",3. A Sparse Lagrangian Duality Theory,[0],[0]
Remark 1.,3. A Sparse Lagrangian Duality Theory,[0],[0]
"Theorem 1 shows that the conditions (a)∼(c) are sufficient and necessary to guarantee the existence of a sparse saddle point for the Lagrangian form L. This result is different from from the traditional saddle point theorem which requires the use of the Slater Constraint Qualification to guarantee the existence of saddle point.
",3. A Sparse Lagrangian Duality Theory,[0],[0]
Remark 2.,3. A Sparse Lagrangian Duality Theory,[0],[0]
Let us consider P ′(w̄) = 1N ∑N i=1,3. A Sparse Lagrangian Duality Theory,[0],[0]
ᾱixi + λw̄ ∈ ∂P (w̄).,3. A Sparse Lagrangian Duality Theory,[0],[0]
Denote F̄ = supp(w̄).,3. A Sparse Lagrangian Duality Theory,[0],[0]
"It is easy to verify that the condition (c) in Theorem 1 is equivalent to
HF̄ (P ′(w̄))",3. A Sparse Lagrangian Duality Theory,[0],[0]
"= 0, w̄min ≥
1 λ ‖P ′(w̄)‖∞.
The following sparse mini-max theorem guarantees that the min and max in (2) can be safely switched if and only if there exists a sparse saddle point for L(w,α).
",3. A Sparse Lagrangian Duality Theory,[0],[0]
Theorem 2 (Sparse Mini-Max Theorem).,3. A Sparse Lagrangian Duality Theory,[0],[0]
"The mini-max relationship
max α∈FN min ‖w‖0≤k L(w,α) =",3. A Sparse Lagrangian Duality Theory,[0],[0]
"min ‖w‖0≤k max α∈FN L(w,α) (4)
holds if and only if there exists a sparse saddle point (w̄, ᾱ) for L.
Proof.",3. A Sparse Lagrangian Duality Theory,[0],[0]
"A proof of this result is given in Appendix A.2.
",3. A Sparse Lagrangian Duality Theory,[0],[0]
"The sparse mini-max result in Theorem 2 provides sufficient and necessary conditions under which one can safely exchange a min-max for a max-min, in the presence of sparsity constraint.",3. A Sparse Lagrangian Duality Theory,[0],[0]
"The following corollary is a direct consequence of applying Theorem 1 to Theorem 2.
",3. A Sparse Lagrangian Duality Theory,[0],[0]
Corollary 1.,3. A Sparse Lagrangian Duality Theory,[0],[0]
"The mini-max relationship
max α∈FN min ‖w‖0≤k L(w,α) =",3. A Sparse Lagrangian Duality Theory,[0],[0]
"min ‖w‖0≤k max α∈FN L(w,α)
holds if and only if there exist a k-sparse primal vector w̄ ∈ Rd and a dual vector ᾱ ∈ FN such that the conditions (a)∼(c) in Theorem 1 are satisfied.
",3. A Sparse Lagrangian Duality Theory,[0],[0]
The mini-max result in Theorem 2 can be used as a basis for establishing sparse duality theory.,3. A Sparse Lagrangian Duality Theory,[0],[0]
"Indeed, we have already shown the following:
min ‖w‖0≤k max α∈FN L(w,α) =",3. A Sparse Lagrangian Duality Theory,[0],[0]
"min ‖w‖0≤k P (w).
",3. A Sparse Lagrangian Duality Theory,[0],[0]
This is called the primal minimization problem and it is the min-max side of the sparse mini-max theorem.,3. A Sparse Lagrangian Duality Theory,[0],[0]
"The other side, the max-min problem, will be called as the dual maximization problem with dual objective function D(α) := min‖w‖0≤k L(w,α), i.e.,
max α∈FN D(α) = max α∈FN min",3. A Sparse Lagrangian Duality Theory,[0],[0]
‖w‖0≤k,3. A Sparse Lagrangian Duality Theory,[0],[0]
"L(w,α).",3. A Sparse Lagrangian Duality Theory,[0],[0]
"(5)
The following Lemma 1 shows that the dual objective function D(α) is concave and explicitly gives the expression of its super-differential.
",3. A Sparse Lagrangian Duality Theory,[0],[0]
Lemma 1.,3. A Sparse Lagrangian Duality Theory,[0],[0]
"The dual objective function D(α) is given by
D(α) = 1
N N∑ i=1 −l∗i",3. A Sparse Lagrangian Duality Theory,[0],[0]
"(αi)− λ 2 ‖w(α)‖2,
where w(α) = Hk ( − 1Nλ ∑N i=1 αixi ) .",3. A Sparse Lagrangian Duality Theory,[0],[0]
"Moreover, D(α) is concave and its super-differential is given by
∂D(α)",3. A Sparse Lagrangian Duality Theory,[0],[0]
"= 1
N",3. A Sparse Lagrangian Duality Theory,[0],[0]
"[w(α)>x1−∂l∗1(α1), ..., w(α)>xN−∂l∗N (αN )].
",3. A Sparse Lagrangian Duality Theory,[0],[0]
"Particularly, if w(α) is unique at α and {l∗i }i=1,...,N are differentiable, then ∂D(α) is unique and it is the supergradient of D(α).
",3. A Sparse Lagrangian Duality Theory,[0],[0]
Proof.,3. A Sparse Lagrangian Duality Theory,[0],[0]
"A proof of this result is given in Appendix A.3.
",3. A Sparse Lagrangian Duality Theory,[0],[0]
"Based on Theorem 1 and Theorem 2, we are able to further establish a sparse strong duality theorem which gives the sufficient and necessary conditions under which the optimal values of the primal and dual problems coincide.
",3. A Sparse Lagrangian Duality Theory,[0],[0]
Theorem 3 (Sparse Strong Duality Theorem).,3. A Sparse Lagrangian Duality Theory,[0],[0]
Let w̄,3. A Sparse Lagrangian Duality Theory,[0],[0]
∈,3. A Sparse Lagrangian Duality Theory,[0],[0]
Rd is a k-sparse primal vector and ᾱ ∈ FN be a dual vector.,3. A Sparse Lagrangian Duality Theory,[0],[0]
"Then ᾱ solves the dual problem in (5), i.e., D(ᾱ) ≥ D(α), ∀α ∈ FN , and P (w̄) = D(ᾱ) if and only if the pair (w̄, ᾱ) satisfies the conditions (a)∼(c) in Theorem 1.
",3. A Sparse Lagrangian Duality Theory,[0],[0]
Proof.,3. A Sparse Lagrangian Duality Theory,[0],[0]
"A proof of this result is given in Appendix A.4.
",3. A Sparse Lagrangian Duality Theory,[0],[0]
"We define the sparse primal-dual gap PD(w,α) := P (w) − D(α).",3. A Sparse Lagrangian Duality Theory,[0],[0]
"The main message conveyed by Theorem 3 is that the sparse primal-dual gap reaches zero at the primal-dual pair (w̄, ᾱ) if and only if the conditions (a)∼(c) in Theorem 1 hold.
",3. A Sparse Lagrangian Duality Theory,[0],[0]
The sparse duality theory developed in this section suggests a natural way for finding the global minimum of the sparsity-constrained minimization problem in (1) via maximizing its dual problem in (5).,3. A Sparse Lagrangian Duality Theory,[0],[0]
"Once the dual maximizer ᾱ is estimated, the primal sparse minimizer w̄ can then be recovered from it according to the prima-dual connection w̄ =",3. A Sparse Lagrangian Duality Theory,[0],[0]
Hk ( − 1λN ∑N i=1 ᾱixi ) as given in the condition (c).,3. A Sparse Lagrangian Duality Theory,[0],[0]
"Since the dual objective function D(α) is shown to be concave, its global maximum can be estimated using any convex/concave optimization method.",3. A Sparse Lagrangian Duality Theory,[0],[0]
"In the next section, we present a simple projected super-gradient method to solve the dual maximization problem.",3. A Sparse Lagrangian Duality Theory,[0],[0]
"Generally, D(α) is a non-smooth function since: 1) the conjugate function l∗i of an arbitrary convex loss li is generally non-smooth and 2) the term ‖w(α)‖2 is non-smooth with respect to α due to the truncation operation involved in computing w(α).",4. Dual Iterative Hard Thresholding,[0],[0]
"Therefore, smooth optimization methods are not directly applicable here and we resort to subgradient-type methods to solve the non-smooth dual maximization problem in (5).
",4. Dual Iterative Hard Thresholding,[0],[0]
"Algorithm 1 Dual Iterative Hard Thresholding (DIHT) Input : Training set {xi, yi}Ni=1.",4. Dual Iterative Hard Thresholding,[0],[0]
Regularization strength parameter λ.,4. Dual Iterative Hard Thresholding,[0],[0]
Cardinality constraint k. Step-size η.,4. Dual Iterative Hard Thresholding,[0],[0]
"Initialization w(0) = 0, α(0)1 = ...",4. Dual Iterative Hard Thresholding,[0],[0]
= α (0) N = 0.,4. Dual Iterative Hard Thresholding,[0],[0]
"for t = 1, 2, ..., T do (S1) Dual projected super-gradient ascent: ∀ i ∈ {1, 2, ..., N},
α (t) i = PF
( α
(t−1) i + η (t−1)g (t−1) i
) , (6)
where g(t−1)i = 1 N (x > i",4. Dual Iterative Hard Thresholding,[0],[0]
w (t−1) − l∗′i,4. Dual Iterative Hard Thresholding,[0],[0]
(α (t−1) i )) is the super-gradient and PF (·) is the Euclidian projection operator with respect to feasible set F .,4. Dual Iterative Hard Thresholding,[0],[0]
"(S2) Primal hard thresholding:
w(t) =",4. Dual Iterative Hard Thresholding,[0],[0]
Hk ( − 1 λN N∑ i=1,4. Dual Iterative Hard Thresholding,[0],[0]
α (t) i xi ) .,4. Dual Iterative Hard Thresholding,[0],[0]
"(7)
end Output: w(T ).",4. Dual Iterative Hard Thresholding,[0],[0]
"The Dual Iterative Hard Thresholding (DIHT) algorithm, as outlined in Algorithm 1, is essentially a projected super-gradient method for maximizing D(α).",4.1. Algorithm,[0],[0]
"The procedure generates a sequence of prima-dual pairs (w(0), α(0)), (w(1), α(1)), . . .",4.1. Algorithm,[0],[0]
from an initial pair w(0) = 0 and α(0) = 0.,4.1. Algorithm,[0],[0]
"At the t-th iteration, the dual update step S1 conducts the projected super-gradient ascent in (6) to update α(t) from α(t−1) and w(t−1).",4.1. Algorithm,[0],[0]
"Then in the primal update step S2, the primal variable w(t) is constructed from α(t) using a k-sparse truncation operation in (7).
",4.1. Algorithm,[0],[0]
"When a batch estimation of super-gradient D′(α) becomes expensive in large-scale applications, it is natural to consider the stochastic implementation of DIHT, namely SDIHT, as outlined in Algorithm 2.",4.1. Algorithm,[0],[0]
"Different from the batch computation in Algorithm 1, the dual update step S1 in Algorithm 2 randomly selects a block of samples (from a given block partition of samples) and update their corresponding dual variables according to (8).",4.1. Algorithm,[0],[0]
"Then in the primal update step S2.1, we incrementally update an intermediate accumulation vector w̃(t) which records− 1λN ∑N i=1",4.1. Algorithm,[0],[0]
α (t),4.1. Algorithm,[0],[0]
i xi as a weighted sum of samples.,4.1. Algorithm,[0],[0]
"In S2.2, the primal vector w(t) is updated by applying k-sparse truncation on w̃(t).",4.1. Algorithm,[0],[0]
The SDIHT is essentially a block-coordinate super-gradient method for the dual problem.,4.1. Algorithm,[0],[0]
"Particularly, in the extreme case m = 1, SDIHT reduces to the batch DIHT.",4.1. Algorithm,[0],[0]
"At the opposite extreme end with m = N , i.e., each block contains one sample, SDIHT becomes a stochastic coordinate-wise super-gradient method.
",4.1. Algorithm,[0],[0]
"Algorithm 2 Stochastic Dual Iterative Hard Thresholding (SDIHT) Input : Training set {xi, yi}Ni=1.",4.1. Algorithm,[0],[0]
"Regularization strength
parameter λ.",4.1. Algorithm,[0],[0]
Cardinality constraint k. Step-size η.,4.1. Algorithm,[0],[0]
"A block disjoint partition {B1, ..., Bm} of the sample index set [1, ..., N ].
",4.1. Algorithm,[0],[0]
Initialization w(0) = w̃(0),4.1. Algorithm,[0],[0]
"= 0, α(0)1 = ...",4.1. Algorithm,[0],[0]
= α (0) N = 0.,4.1. Algorithm,[0],[0]
"for t = 1, 2, ..., T do (S1) Dual projected super-gradient ascent: Uniformly randomly select an index block B(t)i ∈ {B1, ..., Bm}.",4.1. Algorithm,[0],[0]
"For all j ∈ B(t)i update α (t) j as
α (t) j = PF
( α
(t−1) j + η (t−1)g (t−1) j
) .",4.1. Algorithm,[0],[0]
"(8)
Set α(t)j = α (t−1) j , ∀j /∈",4.1. Algorithm,[0],[0]
B (t) i .,4.1. Algorithm,[0],[0]
"(S2) Primal hard thresholding: – (S2.1) Intermediate update:
w̃(t) = w̃(t−1) − 1 λN ∑ j∈B(t)i (α (t) j",4.1. Algorithm,[0],[0]
− α (t−1) j )xj .,4.1. Algorithm,[0],[0]
"(9)
– (S2.2) Hard thresholding: w(t) = Hk(w̃(t)).",4.1. Algorithm,[0],[0]
"end Output: w(T ).
",4.1. Algorithm,[0],[0]
The dual update (8) in SDIHT is much more efficient than DIHT as the former only needs to access a small subset of samples at a time.,4.1. Algorithm,[0],[0]
"If the hard thresholding operation in primal update becomes a bottleneck, e.g., in high-dimensional settings, we suggest to use SDIHT with relatively smaller number of blocks so that the hard thresholding operation in S2.2 can be less frequently called.",4.1. Algorithm,[0],[0]
We now analyze the non-asymptotic convergence behavior of DIHT and SDIHT.,4.2. Convergence analysis,[0],[0]
"In the following analysis, we will denote ᾱ = arg maxα∈FN D(α) and use the abbreviation (t)PD := PD(w
(t), α(t)).",4.2. Convergence analysis,[0],[0]
"Let r = maxa∈F |a| be the bound of the dual feasible set F and ρ = maxi,a∈F |l∗ ′
i (a)|.",4.2. Convergence analysis,[0],[0]
"For example, such quantities exist when li and l∗i are Lipschitz continuous (Shalev-Shwartz & Zhang, 2013b).",4.2. Convergence analysis,[0],[0]
We also assume without loss of generality that ‖xi‖ ≤ 1.,4.2. Convergence analysis,[0],[0]
Let X =,4.2. Convergence analysis,[0],[0]
"[x1, ..., xN ] ∈ Rd×N be the data matrix.",4.2. Convergence analysis,[0],[0]
"Given an index set F , we denote XF as the restriction of X with rows restricted to F .",4.2. Convergence analysis,[0],[0]
"The following quantities will be used in our analysis:
σ2max(X, s) = sup u∈Rn,F
{ u>X>FXFu | |F | ≤ s, ‖u‖ = 1 } ,
σ2min(X, s) = inf u∈Rn,F
{ u>X>FXFu | |F | ≤ s, ‖u‖ = 1 } .
",4.2. Convergence analysis,[0],[0]
"Particularly, σmax(X, d) = σmax(X) and σmin(X, d) = σmin(X).",4.2. Convergence analysis,[0],[0]
"We say a univariate differentiable function f(x) is γ-smooth if ∀x, y, f(y) ≤ f(x)+〈f ′(x), y−x〉+ γ2 |x− y|2.",4.2. Convergence analysis,[0],[0]
"The following is our main theorem on the dual parameter estimation error, support recovery and primal-dual gap of DIHT.
",4.2. Convergence analysis,[0],[0]
Theorem 4.,4.2. Convergence analysis,[0],[0]
Assume that li is 1/µ-smooth.,4.2. Convergence analysis,[0],[0]
Set η(t) = λN,4.2. Convergence analysis,[0],[0]
"2
(λNµ+σmin(X,k))(t+1) .",4.2. Convergence analysis,[0],[0]
"Define constants c1 =
N3(r+λρ)2
(λNµ+σmin(X,k))2 and c2 =",4.2. Convergence analysis,[0],[0]
"(r + λρ)2 ( 1 + σmax(X,k)µλN )2 .
(a) Parameter estimation error: The sequence {α(t)}t≥1 generated by Algorithm 1 satisfies the following estimation error inequality:
‖α(t)",4.2. Convergence analysis,[0],[0]
"− ᾱ‖2 ≤ c1 ( 1
t +
ln t
t
) ,
(b) Support recovery and primal-dual gap: Assume additionally that ̄",4.2. Convergence analysis,[0],[0]
:= w̄min,4.2. Convergence analysis,[0],[0]
"− 1λ‖P
′(w̄)‖∞ > 0.",4.2. Convergence analysis,[0],[0]
"Then, supp(w(t))",4.2. Convergence analysis,[0],[0]
"= supp(w̄) when
t ≥ t0 = ⌈ 12c1σ 2 max(X)
λ2N2̄2 ln
12c1σ 2 max(X)
λ2N2̄2
⌉ .
",4.2. Convergence analysis,[0],[0]
"Moreover, for any > 0, the primal-dual gap satisfies (t)PD ≤ when t ≥ max{t0, t1} where t1 =⌈
3c1c2 λ2N 2 ln 3c1c2 λ2N 2
⌉ .
",4.2. Convergence analysis,[0],[0]
Proof.,4.2. Convergence analysis,[0],[0]
"A proof of this result is given in Appendix A.5.
",4.2. Convergence analysis,[0],[0]
Remark 3.,4.2. Convergence analysis,[0],[0]
"The theorem allows µ = 0 when σmin(X, k) > 0.",4.2. Convergence analysis,[0],[0]
"If µ > 0, then σmin(X, k) is allowed to be zero and thus the step-size can be set as η(t) = Nµ(t+1) .
Consider primal sub-optimality (t)P := P (w (t))",4.2. Convergence analysis,[0],[0]
− P (w̄).,4.2. Convergence analysis,[0],[0]
"Since (t)P ≤ (t) PD always holds, the convergence rates in Theorem 4 are applicable to the primal sub-optimality as well.",4.2. Convergence analysis,[0],[0]
"An interesting observation is that these convergence results on (t)P are not relying on the Restricted Isometry Property (RIP) (or restricted strong condition number) which is required in most existing analysis of IHT-style algorithms (Blumensath & Davies, 2009; Yuan et al., 2014).",4.2. Convergence analysis,[0],[0]
"In (Jain et al., 2014), several relaxed variants of IHT-style algorithms are presented for which the estimation consistency can be established without requiring the RIP conditions.",4.2. Convergence analysis,[0],[0]
"In contrast to the RIP-free sparse recovery analysis in (Jain et al., 2014), our Theorem 4 does not require the sparsity level k to be relaxed.
",4.2. Convergence analysis,[0],[0]
"For SDIHT, we can establish similar non-asymptotic convergence results as summarized in the following theorem.
",4.2. Convergence analysis,[0],[0]
Theorem 5.,4.2. Convergence analysis,[0],[0]
Assume that li is 1/µ-smooth.,4.2. Convergence analysis,[0],[0]
"Set η(t) = λmN2 (λNµ+σmin(X,k))(t+1) .
",4.2. Convergence analysis,[0],[0]
"(a) Parameter estimation error: The sequence {α(t)}t≥1 generated by Algorithm 2 satisfies the following expected estimation error inequality:
E[‖α(t)",4.2. Convergence analysis,[0],[0]
"− ᾱ‖2] ≤ mc1 ( 1
t +
ln t
t
) ,
(b) Support recovery and primal-dual gap: Assume additionally that ̄",4.2. Convergence analysis,[0],[0]
:= w̄min,4.2. Convergence analysis,[0],[0]
"− 1λ‖P
′(w̄)‖∞ > 0.",4.2. Convergence analysis,[0],[0]
"Then, for any δ ∈ (0, 1), with probability at least 1 − δ, it holds that supp(w(t))",4.2. Convergence analysis,[0],[0]
"= supp(w̄) when
t ≥ t2 = ⌈ 12mc1σ 2 max(X)
λ2δ2N2̄2 ln
12mc1σ 2 max(X)
λ2δ2N2̄2
⌉ .
",4.2. Convergence analysis,[0],[0]
"Moreover, with probability at least 1 − δ, the primaldual gap satisfies (t)PD ≤ when t ≥ max{4t2, t3} where t3 = ⌈ 12mc1c2 λ2δ2N 2 ln 12mc1c2",4.2. Convergence analysis,[0],[0]
"λ2δ2N 2 ⌉ .
",4.2. Convergence analysis,[0],[0]
Proof.,4.2. Convergence analysis,[0],[0]
"A proof of this result is given in Appendix A.6.
Remark 4.",4.2. Convergence analysis,[0],[0]
"Theorem 5 shows that, up to scaling factors, the expected or high probability iteration complexity of SDIHT is almost identical to that of DIHT.",4.2. Convergence analysis,[0],[0]
The scaling factor m appeared in t2 and t3 reflects a trade-off between the decreased per-iteration cost and the increased iteration complexity.,4.2. Convergence analysis,[0],[0]
This section dedicates in demonstrating the accuracy and efficiency of the proposed algorithms.,5. Experiments,[0],[0]
We first show the model estimation performance of DIHT when applied to sparse ridge regression models on synthetic datasets.,5. Experiments,[0],[0]
Then we evaluate the efficiency of DIHT/SDIHT on sparse `2- regularized Huber loss and Hinge loss minimization tasks using real-world datasets.,5. Experiments,[0],[0]
A synthetic model is generated with sparse model parameter w̄ =,5.1. Model parameter estimation accuracy evaluation,[0],[0]
"[1, 1, · · · , 1︸ ︷︷ ︸
k̄
, 0, 0, · · · , 0︸ ︷︷ ︸ d−k̄ ].",5.1. Model parameter estimation accuracy evaluation,[0],[0]
"Each xi ∈ Rd of the
N training data examples {xi}Ni=1 is designed to have two components.",5.1. Model parameter estimation accuracy evaluation,[0],[0]
"The first component is the top-k̄ feature dimensions drawn from multivariate Gaussian distribution N(µ1,Σ).",5.1. Model parameter estimation accuracy evaluation,[0],[0]
Each entry in µ1 ∈ Rk̄ independently follows standard normal distribution.,5.1. Model parameter estimation accuracy evaluation,[0],[0]
"The entries of covariance
Σij = { 1 i = j 0.25",5.1. Model parameter estimation accuracy evaluation,[0],[0]
i 6= j .,5.1. Model parameter estimation accuracy evaluation,[0],[0]
The second component consists the left d − k̄ feature dimensions.,5.1. Model parameter estimation accuracy evaluation,[0],[0]
"It follows N(µ2, I) where each entry in µ2 ∈ Rd−k̄ is drawn from standard normal distribution.",5.1. Model parameter estimation accuracy evaluation,[0],[0]
"We simulate two data parameter settings: (1) d = 500, k̄ = 100; (2) d = 300, k̄ = 100.",5.1. Model parameter estimation accuracy evaluation,[0],[0]
"In each data parameter setting 150 random data copies are
produced independently.",5.1. Model parameter estimation accuracy evaluation,[0],[0]
"The task is to solve the following `2-regularized sparse linear regression problem:
min ‖w‖≤k
1
N N∑ i=1",5.1. Model parameter estimation accuracy evaluation,[0],[0]
"lsq(yi, w >xi) + λ 2 ‖w‖2,
where lsq(yi, w>xi) =",5.1. Model parameter estimation accuracy evaluation,[0],[0]
(yi − w>xi)2.,5.1. Model parameter estimation accuracy evaluation,[0],[0]
The responses {yi}Ni=1 are produced by yi = w̄>xi,5.1. Model parameter estimation accuracy evaluation,[0],[0]
"+ εi, where εi ∼ N(0, 1).",5.1. Model parameter estimation accuracy evaluation,[0],[0]
"The convex conjugate of lsq(yi, w>xi) is known as l∗sq(αi) = α2i 4 +yiαi (Shalev-Shwartz & Zhang, 2013b).",5.1. Model parameter estimation accuracy evaluation,[0],[0]
We consider solving the problem under the sparsity level k = k̄.,5.1. Model parameter estimation accuracy evaluation,[0],[0]
Two measurements are calculated for evaluation.,5.1. Model parameter estimation accuracy evaluation,[0],[0]
The first is parameter estimation error ‖w,5.1. Model parameter estimation accuracy evaluation,[0],[0]
− w̄‖/‖w̄‖. Apart from it we calculate the percentage of successful support recovery (PSSR) as the second performance metric.,5.1. Model parameter estimation accuracy evaluation,[0],[0]
A successful support recovery is obtained if supp(w̄) = supp(w).,5.1. Model parameter estimation accuracy evaluation,[0],[0]
The evaluation is conducted on the generated batch data copies to calculate the percentage of successful support recovery.,5.1. Model parameter estimation accuracy evaluation,[0],[0]
"We use 50 data copies as validation set to select the parameter λ from {10−6, ..., 102} and the percentage of successful support recovery is evaluated on the other 100 data copies.
",5.1. Model parameter estimation accuracy evaluation,[0],[0]
"Iterative hard thresholding (IHT) (Blumensath & Davies, 2009) and hard thresholding pursuit (HTP) (Foucart, 2011) are used as the baseline primal algorithms.",5.1. Model parameter estimation accuracy evaluation,[0],[0]
The parameter estimation error and percentage of successful support recovery curves under varying training size are illustrated in Figure 1.,5.1. Model parameter estimation accuracy evaluation,[0],[0]
We can observe from this group of curves that DIHT consistently achieves lower parameter estimation error and higher rate of successful support recovery than IHT and HTP.,5.1. Model parameter estimation accuracy evaluation,[0],[0]
It is noteworthy that most significant performance gap between DIHT and the baselines occurs when the training sizeN is comparable to or slightly smaller than the sparsity level k̄.,5.1. Model parameter estimation accuracy evaluation,[0],[0]
"We now evaluate the considered algorithms on the following `2-regularized sparse Huber loss minimization problem:
min ‖w‖0≤k
1
N N∑ i=1",5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
lHuber(yix >,5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
i w),5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
"+ λ 2 ‖w‖2, (10)
",5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
"where
lHuber(yix > i w) =  0",5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
yix >,5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
i,5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
w ≥ 1 1−,5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
yix,5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
>,5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
i w,5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
− γ 2,5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
yix >,5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
i,5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
w < 1−,5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
"γ
1 2γ (1− yix > i w)
2 otherwise .
",5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
"It is known that (Shalev-Shwartz & Zhang, 2013b)
l∗Huber(αi) =
{ yiαi + γ 2α 2 i if yiαi ∈",5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
"[−1, 0]
+∞ otherwise .
",5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
"Two binary benchmark datasets from LibSVM data repository1, RCV1 (d = 47, 236) and News20 (d = 1, 355, 191), are used for algorithm efficiency evaluation and comparison.",5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
"We select 0.5 million samples from RCV1 dataset for
1https://www.csie.ntu.edu.tw/˜cjlin/ libsvmtools/datasets/binary.html
model training (N d).",5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
"For news20 dataset, all of the 19, 996 samples are used as training data (d N ).
",5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
"We evaluate the algorithm efficiency of DIHT and SDIHT by comparing their running time against three primal baseline algorithms: IHT, HTP and gradient hard thresholding with stochastic variance reduction (SVR-GHT) (Li et al., 2016).",5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
"We first run IHT by setting its convergence criterion to be |P (w
(t))−P (w(t−1))| P (w(t)) ≤ 10−4 or maximum number of iteration is reached.",5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
After that we test the time cost spend by other algorithms to make the primal loss reach P (w(t)).,5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
The parameter update step-size of all the considered algorithms is tuned by grid search.,5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
The parameter γ is set to be 0.25.,5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
"For the two stochastic algorithms SDIHT and SVRGHT we randomly partition the training data into |B| = 10 mini-batches.
",5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
"Figure 2 shows the running time curves on both datasets under varying sparsity level k and regularization strength λ = 0.002, 0.0002.",5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
"It is obvious that under all tested (k, λ) configurations on both datasets, DIHT and SDIHT need much less time than the primal baseline algorithms, IHT, HTP and SVR-GHT to reach the same primal suboptimality.",5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
Figure 3 shows the primal-dual gap convergence curves with respect to the number of epochs.,5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
This group of results support the theoretical prediction in Theorem 4 and 5 that PD converges non-asymptotically.,5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
"We further test the performance of our algorithms when applied to the following `2-regularized sparse hinge loss minimization problem:
min ‖w‖0≤k
1
N N∑ i=1",5.2.2. HINGE LOSS MODEL LEARNING,[0],[0]
lHinge(yix >,5.2.2. HINGE LOSS MODEL LEARNING,[0],[0]
i w),5.2.2. HINGE LOSS MODEL LEARNING,[0],[0]
"+ λ 2 ‖w‖2,
where lHinge(yix>i w) = max(0, 1 − yiw>xi)",5.2.2. HINGE LOSS MODEL LEARNING,[0],[0]
.,5.2.2. HINGE LOSS MODEL LEARNING,[0],[0]
"It is standard to know (Hsieh et al., 2008)
l∗Hinge(αi) = { yiαi if yiαi ∈",5.2.2. HINGE LOSS MODEL LEARNING,[0],[0]
"[−1, 0] +∞ otherwise .
We follow the same experiment protocol as in § 5.2.1 to compare the considered algorithms on the benchmark datasets.",5.2.2. HINGE LOSS MODEL LEARNING,[0],[0]
The time cost comparison is illustrated in Figure 4 and the prima-dual gap sub-optimality is illustrated in Figure 5.,5.2.2. HINGE LOSS MODEL LEARNING,[0],[0]
This group of results indicate that DIHT and SDIHT still exhibit remarkable efficiency advantage over the considered primal IHT algorithms even when the loss function is non-smooth.,5.2.2. HINGE LOSS MODEL LEARNING,[0],[0]
"In this paper, we systematically investigate duality theory and algorithms for solving the sparsity-constrained minimization problem which is NP-hard and non-convex in its
primal form.",6. Conclusion,[0],[0]
"As a theoretical contribution, we develop a sparse Lagrangian duality theory which guarantees strong duality in sparse settings, under mild sufficient and necessary conditions.",6. Conclusion,[0],[0]
This theory opens the gate to solve the original NP-hard/non-convex problem equivalently in a dual space.,6. Conclusion,[0],[0]
We then propose DIHT as a first-order method to maximize the non-smooth dual concave formulation.,6. Conclusion,[0],[0]
The algorithm is characterized by dual super-gradient ascent and primal hard thresholding.,6. Conclusion,[0],[0]
"To further improve iteration efficiency in large-scale settings, we propose SDIHT as a block stochastic variant of DIHT.",6. Conclusion,[0],[0]
"For both algorithms we have proved sub-linear primal-dual gap convergence rate when the primal loss is smooth, without assuming RIPstyle conditions.",6. Conclusion,[0],[0]
"Based on our theoretical findings and numerical results, we conclude that DIHT and SDIHT are theoretically sound and computationally attractive alternatives to the conventional primal IHT algorithms, especially when the sample size is smaller than feature dimensionality.",6. Conclusion,[0],[0]
"Xiao-Tong Yuan is supported in part by Natural Science Foundation of China (NSFC) under Grant 61402232, Grant 61522308, and in part by Natural Science Foundation of Jiangsu Province of China (NSFJPC) under Grant BK20141003.",Acknowledgements,[0],[0]
Qingshan Liu is supported in part by NSFC under Grant 61532009.,Acknowledgements,[0],[0]
"Iterative Hard Thresholding (IHT) is a class of projected gradient descent methods for optimizing sparsity-constrained minimization models, with the best known efficiency and scalability in practice.",abstractText,[0],[0]
"As far as we know, the existing IHT-style methods are designed for sparse minimization in primal form.",abstractText,[0],[0]
It remains open to explore duality theory and algorithms in such a non-convex and NP-hard problem setting.,abstractText,[0],[0]
"In this paper, we bridge this gap by establishing a duality theory for sparsity-constrained minimization with `2-regularized loss function and proposing an IHT-style algorithm for dual maximization.",abstractText,[0],[0]
Our sparse duality theory provides a set of sufficient and necessary conditions under which the original NP-hard/non-convex problem can be equivalently solved in a dual formulation.,abstractText,[0],[0]
The proposed dual IHT algorithm is a super-gradient method for maximizing the non-smooth dual objective.,abstractText,[0],[0]
"An interesting finding is that the sparse recovery performance of dual IHT is invariant to the Restricted Isometry Property (RIP), which is required by virtually all the existing primal IHT algorithms without sparsity relaxation.",abstractText,[0],[0]
"Moreover, a stochastic variant of dual IHT is proposed for large-scale stochastic optimization.",abstractText,[0],[0]
Numerical results demonstrate the superiority of dual IHT algorithms to the state-of-the-art primal IHT-style algorithms in model estimation accuracy and computational efficiency.,abstractText,[0],[0]
Dual Iterative Hard Thresholding: From Non-convex Sparse Minimization to Non-smooth Concave Maximization,title,[0],[0]
"Deep learning brings state-of-the-art results to many artificial intelligence tasks, such as neural machine translation (Wu et al., 2016), image classification (He et al., 2016b;c), image generation (van den Oord et al., 2016b;a), speech recognition (Graves et al., 2013; Amodei et al., 2016), and speech generation/synthesis (Oord et al., 2016).
",1. Introduction,[0],[0]
"Interestingly, we find that many of the aforementioned AI tasks are emerged in dual forms, i.e., the input and output of one task are exactly the output and input of the other task respectively.",1. Introduction,[0],[0]
"Examples include translation from language A to language B vs. translation from language B to A, image classification vs. image generation, and speech
1School of Information Science and Technology, University of Science and Technology of China, Hefei, Anhui, China 2Microsoft Research, Beijing, China.",1. Introduction,[0],[0]
Correspondence to: Tao Qin,1. Introduction,[0],[0]
"<taoqin@microsoft.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
recognition vs. speech synthesis.,1. Introduction,[0],[0]
"Even more interestingly (and somehow surprisingly), this natural duality is largely ignored in the current practice of machine learning.",1. Introduction,[0],[0]
"That is, despite the fact that two tasks are dual to each other, people usually train them independently and separately.",1. Introduction,[0],[0]
"Then a question arises: Can we exploit the duality between two tasks, so as to achieve better performance for both of them?",1. Introduction,[0],[0]
"In this work, we give a positive answer to the question.
",1. Introduction,[0],[0]
"To exploit the duality, we formulate a new learning scheme, which involves two tasks: a primal task and its dual task.",1. Introduction,[0],[0]
"The primal task takes a sample from space X as input and maps to space Y , and the dual task takes a sample from space Y as input and maps to space X .",1. Introduction,[0],[0]
"Using the language of probability, the primal task learns a conditional distribution P (y|x; θxy) parameterized by θxy , and the dual task learns a conditional distribution P (x|y; θyx) parameterized by θyx, where x ∈ X and y ∈ Y .",1. Introduction,[0],[0]
"In the new scheme, the two dual tasks are jointly learned and their structural relationship is exploited to improve the learning effectiveness.",1. Introduction,[0],[0]
"We name this new scheme as dual supervised learning (briefly, DSL).
",1. Introduction,[0],[0]
There could be many different ways of exploiting the duality in DSL.,1. Introduction,[0],[0]
"In this paper, we use it as a regularization term to govern the training process.",1. Introduction,[0],[0]
"Since the joint probability P (x, y) can be computed in two equivalent ways: P (x, y) = P (x)P (y|x) = P (y)P (x|y), for any x ∈ X , y ∈ Y , ideally the conditional distributions of the primal and dual tasks should satisfy the following equality:
P (x)P (y|x; θxy) = P (y)P (x|y; θyx).",1. Introduction,[0],[0]
"(1) However, if the two models (conditional distributions) are learned separately by minimizing their own loss functions (as in the current practice of machine learning), there is no guarantee that the above equation will hold.",1. Introduction,[0],[0]
The basic idea of DSL is to jointly learn the two models θxy and θyx by minimizing their loss functions subject to the constraint of Eqn.(1).,1. Introduction,[0],[0]
"By doing so, the intrinsic probabilistic connection between θyx and θxy are explicitly strengthened, which is supposed to push the learning process towards the right direction.",1. Introduction,[0],[0]
"To solve the constrained optimization problem of DSL, we convert the constraint Eqn.(1) to a penalty term by using the method of Lagrange multipliers (Boyd & Vandenberghe, 2004).",1. Introduction,[0],[0]
"Note that the penalty term could also be seen as a data-dependent regularization term.
",1. Introduction,[0],[0]
"ar X
iv :1
70 7.
00 41
5v 1
[ cs
.L",1. Introduction,[0],[0]
"G
] 3
J ul
2 01
7
To demonstrate the effectiveness of DSL, we apply it to three artificial intelligence applications 1:
(1) Neural Machine Translation (NMT)",1. Introduction,[0],[0]
"We first apply DSL to NMT, which formulates machine translation as a sequence-to-sequence learning problem, with the sentences in the source language as inputs and those in the target language as outputs.",1. Introduction,[0],[0]
"The input space and output space of NMT are symmetric, and there is almost no information loss while mapping from x to y or from y to x.",1. Introduction,[0],[0]
"Thus, symmetric tasks in NMT fits well into the scope of DSL.",1. Introduction,[0],[0]
"Experimental studies illustrate significant accuracy improvements by applying DSL to NMT: +2.07/0.86 points measured by BLEU scores for English↔French translation, +1.37/0.12 points for English↔Germen translation and +0.74/1.69 points on English↔Chinese.
",1. Introduction,[0],[0]
"(2) Image Processing We then apply DSL to image processing, in which the primal task is image classification and the dual task is image generation conditioned on category labels.",1. Introduction,[0],[0]
Both tasks are hot research topics in the deep learning community.,1. Introduction,[0],[0]
"We choose ResNet (He et al., 2016b) as our baseline for image classification, and PixelCNN++(Salimans et al., 2017) as our baseline for image generation.",1. Introduction,[0],[0]
"Experimental results show that on CIFAR-10, DSL could reduce the error rate of ResNet-110 from 6.43 to 5.40 and obtain a better image generation model with both clearer images and smaller bits per dimension.",1. Introduction,[0],[0]
Note that these primal and dual tasks do not yield a pair of completely symmetric input and output spaces since there is information loss while mapping from an image to its class label.,1. Introduction,[0],[0]
"Therefore, our experimental studies reveal that DSL can also work well for dual tasks with information loss.
(3) Sentiment Analysis Finally, we apply DSL to sentiment analysis, in which the primal task is sentiment classification (i.e., to predict the sentiment of a given sentence) and the dual one is sentence generation with given sentiment polarity.",1. Introduction,[0],[0]
"Experiments on the IMDB dataset show that DSL can improve the error rate of a widely-used sentiment classification model by 0.9 point, and can generate sentences with clearer/richer styles of sentiment expression.
",1. Introduction,[0],[0]
"All of above experiments on real artificial intelligence applications have demonstrated that DSL can improve practical performance of both tasks, simultaneously.",1. Introduction,[0],[0]
"In this section, we formulate the problem of dual supervised learning (DSL), describe an algorithm for DSL, and discuss its connections with existing learning schemes and
1In our experiments, we chose the most cited models with either open-source codes or enough implementation details, to ensure that we can reproduce the results reported in previous papers.",2. Framework,[0],[0]
"All of our experiments are done on a single Telsa K40m GPU.
",2. Framework,[0],[0]
its application scope.,2. Framework,[0],[0]
"To exploit the duality, we formulate a new learning scheme, which involves two tasks: a primal task that takes a sample from space X as input and maps to space Y , and a dual task takes a sample from space Y as input and maps to space X .
",2.1. Problem Formulation,[0],[0]
"Assume we have n training pairs {(xi, yi)}ni=1 i.i.d.",2.1. Problem Formulation,[0],[0]
sampled from the space X × Y according to some unknown distribution P .,2.1. Problem Formulation,[0],[0]
"Our goal is to reveal the bi-directional relationship between the two inputs x and y. To be specific, we perform the following two tasks: (1) the primal learning task aims at finding a function f : X 7→ Y such that the prediction of f for x is similar to its real counterpart y; (2) the dual learning task aims at finding a function g : Y 7→ X such that the prediction of g for y is similar to its real counterpart x.",2.1. Problem Formulation,[0],[0]
The dissimilarity is penalized by a loss function.,2.1. Problem Formulation,[0],[0]
"Given any (x, y), let `1(f(x), y) and `2(g(y), x) denote the loss functions for f and g respectively, both of which are mappings from X × Y to R.
A common practice to design (f, g) is the maximum likelihood estimation based on the parameterized conditional distributions P (·|x; θxy) and P (·|y; θyx):
f(x; θxy) , arg max y′∈Y
P (y′|x; θxy),
g(y; θyx) , arg max x′∈X
P (x′|y; θyx),
where θxy and θyx are the parameters to be learned.
",2.1. Problem Formulation,[0],[0]
"By standard supervised learning, the primal model f is learned by minimizing the empirical risk in space Y:
minθxy (1/n)",2.1. Problem Formulation,[0],[0]
"∑n i=1`1(f(xi; θxy), yi);
and dual model g is learned by minimizing the empirical risk in space X :
minθyx(1/n)",2.1. Problem Formulation,[0],[0]
"∑n i=1`2(g(yi; θyx), xi).
",2.1. Problem Formulation,[0],[0]
"Given the duality of the primal and dual tasks, if the learned primal and dual models are perfect, we should have
P (x)P (y|x; θxy) = P (y)P (x|y; θyx) = P (x, y),∀x, y.
We call this property probabilistic duality, which serves as a necessary condition for the optimality of the learned two dual models.
",2.1. Problem Formulation,[0],[0]
"By the standard supervised learning scheme, probabilistic duality is not considered during the training, and the primal and the dual models are trained independently and separately.",2.1. Problem Formulation,[0],[0]
"Thus, there is no guarantee that the learned dual models can satisfy probabilistic duality.",2.1. Problem Formulation,[0],[0]
"To tackle this problem, we propose explicitly reinforcing the empirical probabilistic duality of the dual modes by solving the fol-
lowing multi-objective optimization problem instead:
objective 1: min θxy
(1/n)",2.1. Problem Formulation,[0],[0]
"∑n i=1`1(f(xi; θxy), yi),
objective 2: min θyx
(1/n)",2.1. Problem Formulation,[0],[0]
"∑n i=1`2(g(yi; θyx), xi),
s.t. P (x)P (y|x; θxy) = P (y)P (x|y; θyx),∀x, y,
(2)
where P (x) and P (y) are the marginal distributions.",2.1. Problem Formulation,[0],[0]
"We call this new learning scheme dual supervised learning (abbreviated as DSL).
",2.1. Problem Formulation,[0],[0]
We provide a simple theoretical analysis which shows that DSL has theoretical guarantees in terms of generalization bound.,2.1. Problem Formulation,[0],[0]
"Since the analysis is straightforward, we put it in Appendix A.",2.1. Problem Formulation,[0],[0]
"In practical artificial intelligence applications, the groundtruth marginal distributions are usually not available.",2.2. Algorithm Description,[0],[0]
"As an alternative, we use the empirical marginal distributions P̂ (x) and P̂ (y) to fulfill the constraint in Eqn.(2).
",2.2. Algorithm Description,[0],[0]
"To solve the DSL problem, following the common practice in constraint optimization, we introduce Lagrange multipliers and add the equality constraint of probabilistic duality into the objective functions.",2.2. Algorithm Description,[0],[0]
"First, we convert the probabilistic duality constraint into the following regularization term (with the empirical marginal distributions included):
`duality =(log P̂ (x) + logP (y|x; θxy) − log P̂ (y)− logP (x|y; θyx))2.
(3)
",2.2. Algorithm Description,[0],[0]
"Then, we learn the models of the two tasks by minimizing the weighted combination between the original loss functions and the above regularization term.",2.2. Algorithm Description,[0],[0]
"The algorithm is shown in Algorithm 1.
",2.2. Algorithm Description,[0],[0]
"In the algorithm, the choice of optimizers Opt1 and Opt2 is quite flexible.",2.2. Algorithm Description,[0],[0]
"One can choose different optimizers such as Adadelta (Zeiler, 2012), Adam (Kingma & Ba, 2014), or SGD for different tasks, depending on common practice in the specific task and personal preferences.",2.2. Algorithm Description,[0],[0]
"The duality between tasks has been used to enable learning from unlabeled data in (He et al., 2016a).",2.3. Discussions,[0],[0]
"As an early attempt to exploit the duality, this work actually uses the exterior connection between dual tasks, which helps to form a closed feedback loop and enables unsupervised learning.",2.3. Discussions,[0],[0]
"For example, in the application of machine translation, the primal task/model first translates an unlabeled English sentence x to a French sentence y′; then, the dual task/model translates y′ back to an English sentence x′; finally, both the primal and the dual models get optimized by minimiz-
Algorithm 1 Dual Supervise Learning Algorithm Input: Marginal distributions P̂ (xi) and P̂ (yi) for any i ∈",2.3. Discussions,[0],[0]
"[n]; Lagrange parameters λxy and λyx; optimizers Opt1 and Opt2; repeat
Get a minibatch of m pairs {(xj , yj)}mj=1; Calculate the gradients as follows:
",2.3. Discussions,[0],[0]
Gf = ∇θxy (1/m) ∑m j=1,2.3. Discussions,[0],[0]
"[ `1(f(xj ; θxy), yj)
+ λxy`duality(xj , yj ; θxy, θyx) ] ;
Gg = ∇θyx(1/m) ∑m j=1",2.3. Discussions,[0],[0]
"[ `2(g(yj ; θyx), xj)
+",2.3. Discussions,[0],[0]
"λyx`duality(xj , yj ; θxy, θyx) ] ;
(4)
Update the parameters of f and g: θxy ← Opt1(θxy, Gf ), θyx ← Opt2(θyx, Gg).
",2.3. Discussions,[0],[0]
"until models converged
ing the difference between x′ with x.",2.3. Discussions,[0],[0]
"In contrast, by making use of the intrinsic probabilistic connection between the primal and dual models, DSL takes an innovative attempt to extend the benefit of duality to supervised learning.
",2.3. Discussions,[0],[0]
"While `duality can be regarded as a regularization term, it is data dependent, which makes DSL different from Lasso (Tibshirani, 1996) or SVM (Hearst et al., 1998), where the regularization term is data-independent.",2.3. Discussions,[0],[0]
"More accurately speaking, in DSL, every training sample contributes to the regularization term, and each model contributes to the regularization of the other model.
",2.3. Discussions,[0],[0]
"DSL is different from the following three learning schemes: (1) Co-training focuses on single-task learning and assumes that different subsets of features can provide enough and complementary information about data, while DSL targets at learning two tasks with structural duality simultaneously and does not yield any prerequisite or assumptions on features.",2.3. Discussions,[0],[0]
(2) Multi-task learning requires that different tasks share the same input space and coherent feature representation while DSL does not.,2.3. Discussions,[0],[0]
"(3) Transfer Learning uses auxiliary tasks to boost the main task, while there is no difference between the roles of two tasks in DSL, and DSL enables them to boost the performance of each other simultaneously.
",2.3. Discussions,[0],[0]
We would like to point that there are several requirements to apply DSL to a certain scenario: (1) Duality should exist for the two tasks.,2.3. Discussions,[0],[0]
(2) Both the primal and dual models should be trainable.,2.3. Discussions,[0],[0]
(3) P̂ (X) and P̂ (Y ) in Eqn.,2.3. Discussions,[0],[0]
(3) should be available.,2.3. Discussions,[0],[0]
"If these conditions are not satisfied, DSL might not work very well.",2.3. Discussions,[0],[0]
"Fortunately, as we have discussed in the paper, many machine learning tasks related to image, speech, and text satisfy these conditions.",2.3. Discussions,[0],[0]
We first apply our dual supervised learning algorithm to machine translation and study whether it can improve the translation qualities by utilizing the probabilistic duality of dual translation tasks.,3. Application to Machine Translation,[0],[0]
"In the following of the section, we perform experiments on three pairs of dual tasks 2: English↔French (En↔Fr), English↔Germany (En↔De), and English↔Chinese (En↔Zh).",3. Application to Machine Translation,[0],[0]
"Datasets We employ the same datasets as used in (Jean et al., 2015) to conduct experiments on En↔Fr and En↔De.",3.1. Settings,[0],[0]
"As a part of WMT’14, the training data consists of 12M sentences pairs for En↔Fr and 4.5M for En↔De, respectively (WMT, 2014).",3.1. Settings,[0],[0]
We combine newstest2012 and newstest2013 together as the validation sets and use newstest2014 as the test sets.,3.1. Settings,[0],[0]
"For the dual tasks of En↔Zh, we use 10M sentence pairs obtained from a commercial company as training data.",3.1. Settings,[0],[0]
We leverage NIST2006 as the validation set and NIST2008 as well as NIST2012 as the test sets3.,3.1. Settings,[0],[0]
"Note that, during the training of all three pairs of dual tasks, we drop all sentences with more than 50 words.
",3.1. Settings,[0],[0]
Marginal Distributions P̂ (x) and P̂ (y),3.1. Settings,[0],[0]
"We use the LSTMbased language modeling approach (Sundermeyer et al., 2012; Mikolov et al., 2010) to characterize the marginal distribution of a sentence x, defined as ∏Tx i=1",3.1. Settings,[0],[0]
"P (xi|x<i), where xi is the ith word in x, Tx denotes the number of words in x, and the index <",3.1. Settings,[0],[0]
"i indicates {1, 2, · · · , i − 1}.",3.1. Settings,[0],[0]
"More details about such language modeling approach can be referred to Appendix B.
Model We apply the GRU as the recurrent module to implement the sequence-to-sequence model, which is the same as (Bahdanau et al., 2015; Jean et al., 2015).",3.1. Settings,[0],[0]
The word embedding dimension is 620 and the number of hidden node is 1000.,3.1. Settings,[0],[0]
"Regarding the vocabulary size of the source and target language, we set it as 30k, 50k, and 30k for En↔Fr, En↔De, and En↔Zh, respectively.",3.1. Settings,[0],[0]
The out-of-vocabulary words are replaced by a special token UNK.,3.1. Settings,[0],[0]
"Following the common practice, we denote the baseline algorithm proposed in (Bahdanau et al., 2015; Jean et al., 2015) as RNNSearch.",3.1. Settings,[0],[0]
"We implement the whole NMT learning system based on an open source code4.
2Since both tasks in each pair are symmetric, they play the same role in the dual supervised learning framework.",3.1. Settings,[0],[0]
"Consequently, any one of the dual tasks can be viewed as the primal task while the other as the dual task.
3The three NIST datasets correspond to Zh→En translation task, in which each Chinese sentence has four English references.",3.1. Settings,[0],[0]
"To build the test set for En→Zh, we use the Chinese sentence with one randomly picked English sentence to form up a En→Zh validation/test pair.
",3.1. Settings,[0],[0]
"4https://github.com/nyu-dl/dl4mt-tutorial
Evaluation Metrics",3.1. Settings,[0],[0]
"The translation qualities are measured by tokenized case-sensitive BLEU (Papineni et al., 2002) scores, which is implemented by (multi bleu, 2015).",3.1. Settings,[0],[0]
"The larger the BLEU score is, the better the translation quality is.",3.1. Settings,[0],[0]
"During the evaluation process, we use beam search with beam width 12 to generate sentences.",3.1. Settings,[0],[0]
"Note that, following the common practice, the Zh→En is evaluated by case-insensitive BLEU score.
",3.1. Settings,[0],[0]
Training Procedure,3.1. Settings,[0],[0]
"We initialize the two models in DSL (i.e., the θxy and θyx) by using two warm-start models, which is generated by following the same process as (Jean et al., 2015).",3.1. Settings,[0],[0]
"Then, we use SGD with the minibatch size of 80 as the optimization method for dual training.",3.1. Settings,[0],[0]
"During the training process, we first set the initial learning rate η to 0.2 and then halve it if the BLEU score on the validation set cannot grow for a certain number of mini batches.",3.1. Settings,[0],[0]
"In order to stabilize parameters, we will freeze the embedding matrix once halving learning rates can no long improve the BLEU score on the validation set.",3.1. Settings,[0],[0]
"The gradient clip is set as 1.0, 5.0 and 1.0 during the training for En↔Fr, En↔De, and En↔Zh, respectively (Pascanu et al., 2013).",3.1. Settings,[0],[0]
The value of both λxy and λyx in Algorithm 1 are set as 0.01 according to empirical performance on the validation set.,3.1. Settings,[0],[0]
"Note that, during the optimization process, the LSTM-based language models will not be updated.",3.1. Settings,[0],[0]
Table 1 shows the BLEU scores on the dual tasks by the DSL method with that by the baseline RNNSearch method.,3.2. Results,[0],[0]
"Note that, in this table, we use (MT08) and (MT12) to denote results carried out on NIST2008 and NIST2012, respectively.",3.2. Results,[0],[0]
"From this table, we can find that, on all these three pairs of symmetric tasks, DSL can improve the performance of both dual tasks, simultaneously.
",3.2. Results,[0],[0]
"To better understand the effects of applying the probabilistic duality constraint as the regularization, we compute the `duality on the test set by DSL compared with RNNSearch.",3.2. Results,[0],[0]
"In particular, after applying DSL to En→Fr, the `duality decreases from 1545.68 to 1468.28, which also indicates that
the two models become more coherent in terms of probabilistic duality.
",3.2. Results,[0],[0]
"(Jean et al., 2015) proposed an effective post-process technique, which can achieve better translation performance by replacing the “UNK” with the corresponding word-level translations.",3.2. Results,[0],[0]
"After applying this technique into DSL, we report its results on En→Fr in Table 2, compared with several baselines with the same model structures as ours that also integrate the “UNK” post-processing technique.",3.2. Results,[0],[0]
"From this table, it is clear to see that DSL can achieve better performance than all baseline methods.
",3.2. Results,[0],[0]
"In the previous experiments, we use a warm-start approach in DSL using the models trained by RNNSearch.",3.2. Results,[0],[0]
"Actually, we can use stronger models for initialization to achieve even better accuracy.",3.2. Results,[0],[0]
We conduct a light experiment to verify this.,3.2. Results,[0],[0]
"We use the models trained by (He et al., 2016a) as the initializations in DSL on En↔Fr translation.",3.2. Results,[0],[0]
"We find that BLEU score can be improved from 34.83 to 35.95 for En→Fr translation, and from 32.94 to 33.40 for Fr→En translation.
",3.2. Results,[0],[0]
Effects of λ There are two hyperparameters λxy and λyx in our DSL algorithm.,3.2. Results,[0],[0]
We conduct some experiments to investigate their effects.,3.2. Results,[0],[0]
"Since the input and output space are symmetric, we set λxy = λyx = λ and plot the validation accuracy of different λ’s in Figure 1(a).",3.2. Results,[0],[0]
"From this figure, we can see that both En→Fr and Fr→En reach the best performance when λ = 10−2, and thus the results of DSL reported in Table 1 are obtained with λ = 10−2.",3.2. Results,[0],[0]
"Moreover, we find that, within a relatively large interval of λ, DSL outperforms standard supervised learning, i.e., the point with λ = 0.",3.2. Results,[0],[0]
"We also plot
the BLEU scores for λ = 10−2 on the validation and test sets in Figure 1(b) with respect to training iterations.",3.2. Results,[0],[0]
"We can see that, in the first couple of rounds, the test BLEU curves fluctuate with large variance.",3.2. Results,[0],[0]
"The reason is that two separately initialized models of dual tasks yield are not consistent with each other, i.e., Eqn. (1) does not hold, which causes the declination of the performance of both models as they play as the regularizer for each other.",3.2. Results,[0],[0]
"As the training goes on, two models become more consistent and finally boost the performance of each other.
",3.2. Results,[0],[0]
Case studies Table 3 shows a couple of translation examples produced by RNNSearch compared with DSL.,3.2. Results,[0],[0]
"From this table, we find that DSL demonstrates three major advantages over RNNSearch.",3.2. Results,[0],[0]
"First, by leveraging the structural duality of sentences, DSL can result in the improvement of mutual translation, e.g. “when it comes to” and “lorsqu qu’il s’agit de”, which better fit the semantics expressed in the sentences.",3.2. Results,[0],[0]
"Second, DSL can consider more contextual information in translation.",3.2. Results,[0],[0]
"For example, in Fr→En, une société is translated to company, however, in the baseline, it is translated to society.",3.2. Results,[0],[0]
"Although the word level translation is not bad, it should definitely be translated as “company” given the contextual semantics.",3.2. Results,[0],[0]
"Furthermore, DSL can better handle the plural form.",3.2. Results,[0],[0]
"For example, DSL can correctly translate “the French are the worst”, which are of plural form, while the baseline deals with it by singular form.",3.2. Results,[0],[0]
"In the domain of image processing, image classification (image→label) and image generation (label→image) are in
the dual form.",4. Application to Images Processing,[0],[0]
"In this section, we apply our dual supervised learning framework to these two tasks and conduct experimental studies based on a public dataset, CIFAR10 (Krizhevsky & Hinton, 2009), with 10 classes of images.",4. Application to Images Processing,[0],[0]
"In our experiments, we employ a popular method, ResNet5, for image classification and a most recent method, PixelCNN++6, for image generation.",4. Application to Images Processing,[0],[0]
Let X denote the image space and Y denote the category space related to CIFAR10.,4. Application to Images Processing,[0],[0]
"Marginal Distributions In our experiments, we simply use the uniform distribution to set the marginal distribution P̂ (y) of 10-class labels, which means the marginal distribution of each class equals 0.1.",4.1. Settings,[0],[0]
The image distribution P̂ (x) is usually defined as ∏m i=1,4.1. Settings,[0],[0]
P{xi|x<,4.1. Settings,[0],[0]
"i}, where all pixels of the image is serialized and xi is the value of the i-th pixel of an m-pixel image.",4.1. Settings,[0],[0]
Note that the model can predict xi only based on the previous pixels xj with index j < i.,4.1. Settings,[0],[0]
"We use the PixelCNN++, which is so far the best algorithm, to model the image distribution.
",4.1. Settings,[0],[0]
"Models For the task of image classification, we choose 32- layer ResNet (denoted as ResNet-32) and 110-layer ResNet (denoted as ResNet-110) as two baselines, respectively, in order to examine the power of DSL on both relatively simple and complex models.",4.1. Settings,[0],[0]
"For the task of image generation, we use PixelCNN++ again.",4.1. Settings,[0],[0]
"Compared to the PixelCNN++ used for modeling distribution, the difference lies in the training process: When used for image generation given a certain class, PixelCNN++ takes the class label as an additional input, i.e., it tries to characterize ∏m i=1",4.1. Settings,[0],[0]
P{xi|x<,4.1. Settings,[0],[0]
"i, y}, where y is the 1-hot label vector.
",4.1. Settings,[0],[0]
Evaluation Metrics,4.1. Settings,[0],[0]
We use the classification error rates to measure the performance of image classification.,4.1. Settings,[0],[0]
"We use bits per dimension (briefly, bpd) (Salimans et al., 2017), to assess the performance of image generation.",4.1. Settings,[0],[0]
"In particular, for an image x with label y, the bpd is defined as:
− (∑Nx i=1",4.1. Settings,[0],[0]
"logP (xi|x<i, y) ) /",4.1. Settings,[0],[0]
"( Nx log(2) ) , (5)
where Nx is the number of pixels in image x.",4.1. Settings,[0],[0]
"By using the dataset CIFAR-10, Nx is 3072 for any image x, and we will report the average bpd on the test set.
",4.1. Settings,[0],[0]
Training Procedure,4.1. Settings,[0],[0]
We first initialize both the primal and the dual models with the ResNet model and PixelCNN++ model pre-trained independently and separately.,4.1. Settings,[0],[0]
We obtain a 32-layer ResNet with error rate of 7.65 and a 110-layer ResNet with error rate of 6.54 as the pre-trained models for image classification.,4.1. Settings,[0],[0]
"The error rates of these two pretrained models are comparable to results reported in (He
5https://github.com/tensorflow/models/tree/master/resnet 6https://github.com/openai/pixel-cnn
et al., 2016b).",4.1. Settings,[0],[0]
"We generate a pre-trained conditional image generation model with the test bpd of 2.94, which is the same as reported in (Salimans et al., 2017).",4.1. Settings,[0],[0]
"For DSL training, we set the initial learning rate of image classification model as 0.1 and that of image generation model as 0.0005.",4.1. Settings,[0],[0]
"The learning rates follow the same decay rules as those in (He et al., 2016b) and (Salimans et al., 2017).",4.1. Settings,[0],[0]
The whole training process takes about two weeks before convergence.,4.1. Settings,[0],[0]
Note that experimental results below are based on the training with λxy = (30/3072)2 and λyx = (1.2/3072)2.,4.1. Settings,[0],[0]
"Table 4 compares the error rates of two image classification models, i.e., DSL vs. Baseline, on the test set.",4.2. Results on Image Classification,[0],[0]
"From this table, we find that, with using either ResNet-32 or ResNet-110, DSL achieves better accuracy than the baseline method.
",4.2. Results on Image Classification,[0],[0]
"Interestingly, we observe from Table 4 that, DSL leads to higher relative performance improvement on the ResNet110 over the ResNet-32.",4.2. Results on Image Classification,[0],[0]
"We hypothesize one possible reason is that, due to the limited training data, an appropriate regularization can benefit more to the 110-layer ResNet with higher model complexity, and the dualityoriented regularization `duality indeed plays this role and consequently gives rise to higher relative improvement.",4.2. Results on Image Classification,[0],[0]
"Our further experimental results show that, based on ResNet-110, DSL can decrease the test bpd from 2.94 (baseline) to 2.93 (DSL), which is a new state-of-the-art result on CIFAR-10.",4.3. Results on Image Generation,[0],[0]
"Indeed, it is quite difficult to improve bpd by 0.01 which though seems like a minor change.",4.3. Results on Image Generation,[0],[0]
"We also find that, there is no significant improvement on test bpd based on ResNet-32.",4.3. Results on Image Generation,[0],[0]
"An intuitive explanation is that, since ResNet-110 is stronger than ResNet-32 in modeling the conditional probability P (y|x), it can better help the task of image generation through the constraint/regularization of the probabilistic duality.
",4.3. Results on Image Generation,[0],[0]
"As pointed out in (Theis et al., 2015), bpd is not the only evaluation rule of image generation.",4.3. Results on Image Generation,[0],[0]
"Therefore, we further conduct a qualitative analysis by comparing images generated by dual supervised learning with those by the base-
line model for each of image categories, some examples of which are shown in Figure 2.
",4.3. Results on Image Generation,[0],[0]
"Each row in Figure 2 corresponds to one category in CIFAR-10, the five images in the left side are generated by the baseline model, and the five ones in the right side are generated by the model trained by DSL.",4.3. Results on Image Generation,[0],[0]
"From this figure, we find that DSL generally generates images with clearer and more distinguishable characteristics regarding the corresponding category.",4.3. Results on Image Generation,[0],[0]
"Specifically, those right five images in Row 3, 4, and 6 can illustrate more distinguishable characteristics of birds, cats and dogs respectively, which is mainly due to benefits of introducing the probabilistic duality into DSL.",4.3. Results on Image Generation,[0],[0]
"But, there are still some cases that neither the baseline model nor DSL can perform well, like deers it Row 5 and frogs in Row 7.",4.3. Results on Image Generation,[0],[0]
"One reason is that the bpd of images in the category of deer and frogs are 3.17 and 3.32, which are significant larger than the average 2.94.",4.3. Results on Image Generation,[0],[0]
This shows that the images of these two categories are harder to generate.,4.3. Results on Image Generation,[0],[0]
"Finally, we apply the dual supervised learning framework to the domain of sentiment analysis.",5. Application to Sentiment Analysis,[0],[0]
"In this domain, the primal task, sentiment classification (Maas et al., 2011; Dai & Le, 2015), is to predict the sentiment polarity label of a given sentence; and the dual task, though not quite apparent but really existed, is sentence generation based on a sentiment polarity.",5. Application to Sentiment Analysis,[0],[0]
"In this section, let X denote the sentences and Y denote the sentiment related to our task.",5. Application to Sentiment Analysis,[0],[0]
"Dataset Our experiments are performed based on the IMDB movie review dataset (IMDB, 2011), which consists of 25k training and 25k test sentences.",5.1. Experimental Setup,[0],[0]
Each sentence in this dataset is associated with either a positive or a negative sentiment label.,5.1. Experimental Setup,[0],[0]
"We randomly sample a subset of 3750 sentences from the training data as the validation set for hyperparameter tuning and use the remaining training data for model training.
",5.1. Experimental Setup,[0],[0]
"Marginal Distributions We simply use the uniform distribution to set the marginal distribution P̂ (y) of polarity labels, which means the marginal distribution of positive or negative class equals 0.5.",5.1. Experimental Setup,[0],[0]
"On the other side, we take advantage of the LSTM-based language modeling to model the marginal distribution P̂ (x) of a sentence x.",5.1. Experimental Setup,[0],[0]
"The test perplexities (Bengio et al., 2003) of the obtained language model is 58.74.
",5.1. Experimental Setup,[0],[0]
"Model Implementation We leverage the widely used LSTM (Dai & Le, 2015) modeling approach for sentiment classification7 model.",5.1. Experimental Setup,[0],[0]
We set the embedding dimension as 500 and the hidden layer size as 1024.,5.1. Experimental Setup,[0],[0]
"For sentence generation, we use another LSTM model with W ewEwxt−1 + W e sEsy as input, where xt−1 denotes the t−1’th word,Ew andEs represent the embedding matrices for word and sentiment label respectively, and W ’s represent the connections between embedding matrix and LSTM cells.",5.1. Experimental Setup,[0],[0]
"A sentence is generated word by word sequentially, and the probability that word xt is generated is proportional to exp(W dwEwxt−1",5.1. Experimental Setup,[0],[0]
"+W d s Esy +Whht−1), where ht−1 is the hidden state outputted by LSTM.",5.1. Experimental Setup,[0],[0]
Note the W ’s and the E’s are the parameters to learn in training.,5.1. Experimental Setup,[0],[0]
"In the following, we call the model for sentiment based sentence generation as contextual language model (briefly, CLM).
",5.1. Experimental Setup,[0],[0]
Evaluation Metrics,5.1. Experimental Setup,[0],[0]
"We measure the performance of sentiment classification by the error rate, and that of sentence generation, i.e., CLM, by test perplexity.
",5.1. Experimental Setup,[0],[0]
"Training Procedure To obtain baseline models, we use Adadelta as the optimization method to train both the sentiment classification and sentence generation model.",5.1. Experimental Setup,[0],[0]
"Then, we use them to initialization the two models for DSL.",5.1. Experimental Setup,[0],[0]
"At the beginning of DSL training, we use plain SGD with an initial learning rate of 0.2 and then decrease it to 0.02 for both models once there is no further improvement on the validation set.",5.1. Experimental Setup,[0],[0]
"For each (x, y) pair, we set λxy =",5.1. Experimental Setup,[0],[0]
"(5/lx)2 and λyx = (0.5/lx)2, where lx is the length of x.",5.1. Experimental Setup,[0],[0]
"The whole training process of DSL takes less than two days.
",5.1. Experimental Setup,[0],[0]
"7Both supervised and semi-supervised sentiment classification are studied in (Dai & Le, 2015).",5.1. Experimental Setup,[0],[0]
We focus on supervised learning here.,5.1. Experimental Setup,[0],[0]
"Therefore, we do not compare with the models trained with semi-supervised (labeled + unlabeled) data.",5.1. Experimental Setup,[0],[0]
Table 5 compares the performance of DSL with the baseline method in terms of both the error rates of sentiment classification and the perplexity of sentence generation.,5.2. Results,[0],[0]
"Note that the test error of the baseline classification model, which is 10.10 as shown in the table, is comparable to the recent results as reported in (Dai & Le, 2015).",5.2. Results,[0],[0]
We have two observations from the table.,5.2. Results,[0],[0]
"First, DSL can reduce the classification error by 0.90 without modifying the LSTMbased model structure.",5.2. Results,[0],[0]
"Second, DSL slightly improves the perplexity for sentence generation, but the improvement is not very significant.",5.2. Results,[0],[0]
"We hypothesize the reason is that the sentiment label can merely supply at most 1 bit information such that the perplexity difference between the language model (i.e., the marginal distribution P̂ (x)) and CLM (i.e., the conditional distribution P (x|y)) are not large, which limits the improvement brought by DSL.
",5.2. Results,[0],[0]
"Qualitative analysis on sentence generation
In addition to quantitative studies as shown above, we further conduct qualitative analysis on the performance of sentence generation.",5.2. Results,[0],[0]
Table 6 demonstrates some examples of generated sentences based on sentiment labels.,5.2. Results,[0],[0]
"From this table, we can find that both the baseline model and DSL succeed in generating sentences expressing the certain sentiment.",5.2. Results,[0],[0]
"The baseline model prefers to produce the sentence with those words yielding high-frequency in the training data, such as the “the plot is simple/predictable, the acting is great/bad”, etc.",5.2. Results,[0],[0]
"This is because the sentence generation model itself is essentially a language model based generator, which aims at catching the high-frequency words in the training data.",5.2. Results,[0],[0]
"Meanwhile, since the training of CLM in DSL can leverage the signals provided by the classifier, DSL makes it more possible to select those words, phrases, or textual patterns that can present more specific and more intense sentiment, such as “nothing but good, 10/10, don’t waste your time”, etc.",5.2. Results,[0],[0]
"As a result, the CLM in DSL can generate sentences with richer expressions for sentiments.",5.2. Results,[0],[0]
"In previous experiments, we start DSL training with welltrained primal and dual models.",5.3. Discussions,[0],[0]
We conduct some further experiments to verify whether warm start is a must for DSL.,5.3. Discussions,[0],[0]
(1) We train DSL from a warm-start sentence generator and a cold-start (randomly initialized) sentence classifier.,5.3. Discussions,[0],[0]
"In this case, DSL achieves a classification error of 9.44%, which is better than the baseline classifier in Ta-
ble 5.",5.3. Discussions,[0],[0]
(2) We train DSL from a warm-start classifier and a cold-start sentence generator.,5.3. Discussions,[0],[0]
"The perplexity of the generator after DSL training reach 58.79, which is better than the baseline generator.",5.3. Discussions,[0],[0]
(3) We train DSL from both cold-start models.,5.3. Discussions,[0],[0]
"The final classification error is 9.50% and the perplexity of the generator is 58.82, which are both better than the baselines.",5.3. Discussions,[0],[0]
"These results show that the success of DSL does not necessarily require warm-start models, although they can speed up the training of DSL.",5.3. Discussions,[0],[0]
"Observing the existence of structure duality among many AI tasks, we have proposed a new learning framework, dual supervised learning, which can greatly improve the performance for both the primal and the dual tasks, simultaneously.",6. Conclusions and Future Work,[0],[0]
We have introduced a probabilistic duality term to serve as a data-dependent regularizer to better guide the training.,6. Conclusions and Future Work,[0],[0]
"Empirical studies have validated the effectiveness of dual supervised learning.
",6. Conclusions and Future Work,[0],[0]
There are multiple directions to explore in the future.,6. Conclusions and Future Work,[0],[0]
"First, we will test dual supervised learning on more dual tasks, such as speech recognition and speech synthesis.",6. Conclusions and Future Work,[0],[0]
"Second, we will enrich theoretical study to better understand dual supervised learning.",6. Conclusions and Future Work,[0],[0]
"Third, it is interesting to combine dual supervised learning with unsupervised dual learning (He et al., 2016a) to leverage unlabeled data so as to further improve the two dual tasks.",6. Conclusions and Future Work,[0],[0]
"Fourth, we will combine dual supervised learning with dual inference (Xia et al., 2017) so as to leverage structural duality to enhance both the training and inference procedures.
",6. Conclusions and Future Work,[0],[0]
Appendix,6. Conclusions and Future Work,[0],[0]
"As we know, the final goal of the dual learning is to give correct predictions for the unseen test data.",A. Theoretical Analysis,[0],[0]
"That is to say, we want to minimize the (expected) risk of the dual models, which is defined as follows8:
R(f, g) =",A. Theoretical Analysis,[0],[0]
"E [ `1(f(x), y) + `2(g(y), x)
2
] ,∀f ∈ F , g ∈ G,
whereF = {f(x; θxy); θxy∈Θxy}, G = {g(x; θyx); θyx ∈ Θyx}, Θxy and Θyx are parameter spaces, and the E is taken over the underlying distribution P .",A. Theoretical Analysis,[0],[0]
"Besides, let D denote the product space of the two models satisfying probabilistic duality, i.e., the constraint in Eqn.(4).",A. Theoretical Analysis,[0],[0]
"For ease of reference, defineHdual as (F × G) ∩ D.
Define the empirical risk on the n sample as follows: for any f ∈ F , g ∈ G,
Rn(f, g) = 1
n ∑n i=1 `1(f(xi), yi) + `2(g(yi), xi) 2 .
",A. Theoretical Analysis,[0],[0]
"Following (Bartlett & Mendelson, 2002), we introduce Rademacher complexity for dual supervised learning, a measure for the complexity of the hypothesis.",A. Theoretical Analysis,[0],[0]
Definition 1.,A. Theoretical Analysis,[0],[0]
"Define the Rademacher complexity of DSL, RDSLn , as follows:
RDSLn = E z,σ
[ sup
(f,g)∈Hdual
∣∣ 1 n n∑ i=1",A. Theoretical Analysis,[0],[0]
σi,A. Theoretical Analysis,[0],[0]
"( `1(f(xi), yi)+`2(g(yi), xi) )∣∣",A. Theoretical Analysis,[0],[0]
"], where z = {z1, z2, · · · , zn} ∼ Pn, zi = (xi, yi) in which xi ∈ X and yi ∈ Y , σ = {σ1, · · · , σm} are i.i.d sampled with P (σi = 1) =",A. Theoretical Analysis,[0],[0]
"P (σi = −1) = 0.5.
",A. Theoretical Analysis,[0],[0]
"Based on RDSLn , we have the following theorem for dual supervised learning:
Theorem 1 ((Mohri et al., 2012)).",A. Theoretical Analysis,[0],[0]
"Let 12`1(f(x), y) + 1 2`2(g(y), x) be a mapping from X × Y to [0, 1].",A. Theoretical Analysis,[0],[0]
"Then, for any δ ∈ (0, 1), with probability at least 1−δ, the following inequality holds for any (f, g) ∈ Hdual,
R(f, g) ≤",A. Theoretical Analysis,[0],[0]
"Rn(f, g) + 2RDSLn",A. Theoretical Analysis,[0],[0]
"+ √ 1
2n ln(
1 δ ).",A. Theoretical Analysis,[0],[0]
"(6)
Similarly, we define the Rademacher complexity for the standard supervised learning RSLn under our framework by replacing theHdual in Definition 1 byF×G. With probability at least 1 − δ, the generation error bound of supervised learning is smaller than 2RSLn + √ 1 2n ln( 1 δ ).
",A. Theoretical Analysis,[0],[0]
"8The parameters θxy and θyx in the dual models will be omitted when the context is clear.
",A. Theoretical Analysis,[0],[0]
"Since Hdual ∈ F × G, by the definition of Rademacher complexity, we have RDSLn ≤ RSLn .",A. Theoretical Analysis,[0],[0]
"Therefore, DSL enjoys a smaller generation error bound than supervised learning.
",A. Theoretical Analysis,[0],[0]
"The approximation of dual supervised learning is defined as
R(f∗F , g ∗ F )−R∗ (7)
in which
R(f∗F , g ∗ F ) = inf R(f, g), s.t. (f, g) ∈ Hdual; R∗ = inf R(f, g).
",A. Theoretical Analysis,[0],[0]
"The approximation error for supervised learning is similarly defined.
",A. Theoretical Analysis,[0],[0]
"Define Py|x = {P (y|x; θxy)|θxy ∈ Θxy}, Px|y = {P (x|y; θyx)|θyx ∈ Θyx}.",A. Theoretical Analysis,[0],[0]
Let P ∗y|x and P ∗ x|y denote the two conditional probabilities derived from P .,A. Theoretical Analysis,[0],[0]
"We have the following theorem:
Theorem 2.",A. Theoretical Analysis,[0],[0]
If P ∗y|x ∈ Py|x and P ∗,A. Theoretical Analysis,[0],[0]
"x|y ∈ Px|y , then supervised learning and DSL has the same approximation error.
",A. Theoretical Analysis,[0],[0]
Proof.,A. Theoretical Analysis,[0],[0]
"By definition, we can verify both of the two approximation errors are zero.",A. Theoretical Analysis,[0],[0]
"We use the LSTM language models (Sundermeyer et al., 2012; Mikolov et al., 2010) to characterize the marginal distribution of a sentence x, defined as ∏Tx i=1",B. Details about the Language Models for Marginal Distributions,[0],[0]
"P (xi|x<i), where xi is the i-th word in x, Tx denotes the number of words in x, and the index <",B. Details about the Language Models for Marginal Distributions,[0],[0]
"i indicates {1, 2, · · · , i − 1}.",B. Details about the Language Models for Marginal Distributions,[0],[0]
The embedding dimension and hidden node are both 1024.,B. Details about the Language Models for Marginal Distributions,[0],[0]
We apply 0.5 dropout to the input embedding and the last hidden layer before softmax.,B. Details about the Language Models for Marginal Distributions,[0],[0]
"The validation perplexities of the language models are shown in Table 7, where the validation sets are the same.
",B. Details about the Language Models for Marginal Distributions,[0],[0]
"For the marginal distributions for sentences of sentiment classification, we choose the LSTM language model again like those for machine translation applications.",B. Details about the Language Models for Marginal Distributions,[0],[0]
The two differences are: (i) the vocabulary size is 10000; (ii) the word embedding dimension is 500.,B. Details about the Language Models for Marginal Distributions,[0],[0]
The perplexity of this language model is 58.74.,B. Details about the Language Models for Marginal Distributions,[0],[0]
"Many supervised learning tasks are emerged in dual forms, e.g., English-to-French translation vs. French-to-English translation, speech recognition vs. text to speech, and image classification vs. image generation.",abstractText,[0],[0]
Two dual tasks have intrinsic connections with each other due to the probabilistic correlation between their models.,abstractText,[0],[0]
"This connection is, however, not effectively utilized today, since people usually train the models of two dual tasks separately and independently.",abstractText,[0],[0]
"In this work, we propose training the models of two dual tasks simultaneously, and explicitly exploiting the probabilistic correlation between them to regularize the training process.",abstractText,[0],[0]
"For ease of reference, we call the proposed approach dual supervised learning.",abstractText,[0],[0]
"We demonstrate that dual supervised learning can improve the practical performances of both tasks, for various applications including machine translation, image processing, and sentiment analysis.",abstractText,[0],[0]
Dual Supervised Learning,title,[0],[0]
Recent years have seen rapid progress in generative modeling made possible by advances in deep learning and stochastic variational inference.,1. Introduction,[0],[0]
"The reparameterization trick (Kingma & Welling, 2014; Rezende et al., 2014) has made stochastic variational inference efficient by providing lower-variance gradient estimates.",1. Introduction,[0],[0]
"However, reparameterization, as originally proposed, does not easily extend to semi-supervised learning, binary latent attribute models, topic modeling, variational memory addressing, hard attention models, or clustering, which require discrete latentvariables.
",1. Introduction,[0],[0]
"Continuous relaxations have been proposed for accommodating discrete variables in variational inference (Maddison et al., 2016; Jang et al., 2016; Rolfe, 2016).",1. Introduction,[0],[0]
"The Gumbel-
1Quadrant.ai, D-Wave Systems Inc., Burnaby, BC, Canada.",1. Introduction,[0],[0]
"Correspondence to: Arash Vahdat <arash@quadrant.ai>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"Softmax technique (Maddison et al., 2016; Jang et al., 2016) defines a temperature-based continuous distribution that in the zero-temperature limit converges to a discrete distribution.",1. Introduction,[0],[0]
"However, it is limited to categorical distributions and does not scale to multivariate models such as Boltzmann machines (BM).",1. Introduction,[0],[0]
"The approach presented in (Rolfe, 2016) can train models with BM priors but requires careful handling of the gradients during training.
",1. Introduction,[0],[0]
We propose a new class of smoothing transformations for relaxing binary latent variables.,1. Introduction,[0],[0]
The method relies on two distributions with overlapping support that in the zero temperature limit converge to a Bernoulli distribution.,1. Introduction,[0],[0]
"We present two variants of smoothing transformations using a mixture of exponential and a mixture of logistic distributions.
",1. Introduction,[0],[0]
"We demonstrate that overlapping transformations can be used to train discrete directed latent models as in (Maddison et al., 2016; Jang et al., 2016), and models with BMs in their prior as in (Rolfe, 2016).",1. Introduction,[0],[0]
"In the case of BM priors, we show that the Kullback-Leibler (KL) contribution to the variational bound can be approximated using an analytic expression that can be optimized using automatic differentiation without requiring the special treatment of gradients in (Rolfe, 2016).
",1. Introduction,[0],[0]
"Using this analytic bound, we develop a new variational autoencoder (VAE) architecture called DVAE++, which uses a BM prior to model discontinuous latent factors such as object categories or scene configuration in images.",1. Introduction,[0],[0]
"DVAE++ is inspired by (Rolfe, 2016) and includes continuous local latent variables to model locally smooth features in the data.",1. Introduction,[0],[0]
DVAE++ achieves comparable results to the state-of-the-art techniques on several datasets and captures semantically meaningful discrete aspects of the data.,1. Introduction,[0],[0]
"We show that even when all continuous latent variables are removed, DVAE++ still attains near state-of-the-art generative likelihoods.",1. Introduction,[0],[0]
Training of models with discrete latent variables z requires low-variance estimates of gradients of the form ∇φEqφ(z)[f(z)].,1.1. Related Work,[0],[0]
"Only when z has a modest number of configurations (as in semi-supervised learning (Kingma et al., 2014) or semi-supervised generation (Maaløe et al., 2017))
can the gradient of the expectation be decomposed into a summation over configurations.
",1.1. Related Work,[0],[0]
"The REINFORCE technique (Williams, 1992) is a more scalable method that migrates the gradient inside the expectation: ∇φEqφ(z)f(z)",1.1. Related Work,[0],[0]
= Eqφ(z)[f(z)∇φ log qφ(z)].,1.1. Related Work,[0],[0]
"Although the REINFORCE estimate is unbiased, it suffers from high variance and carefully designed “control variates” are required to make it practical.",1.1. Related Work,[0],[0]
Several works use this technique and differ in their choices of the control variates.,1.1. Related Work,[0],[0]
"NVIL (Mnih & Gregor, 2014) uses a running average of the function, f(z), and an input-dependent baseline.",1.1. Related Work,[0],[0]
"VIMCO (Mnih & Rezende, 2016) is a multi-sample version of NVIL that has baselines tailored for each sample based on all the other samples.",1.1. Related Work,[0],[0]
"MuProp (Gu et al., 2015) and DARN (Gregor et al., 2013) are two other REINFORCE-based methods (with non-zero biases) that use a Taylor expansion of the function f(z) to create control variates.
",1.1. Related Work,[0],[0]
"To address the high variance of REINFORCE, other work strives to make discrete variables compatible with the reparametrization technique.",1.1. Related Work,[0],[0]
A primitive form arises from estimating the discrete variables by a continuous function during back-propagation.,1.1. Related Work,[0],[0]
"For instance, in the case of Bernoulli distribution, the latent variables can be approximated by their mean value.",1.1. Related Work,[0],[0]
"This approach is called the straight-through (ST) estimator (Bengio et al., 2013).",1.1. Related Work,[0],[0]
Another way to make discrete variables compatible with the reparametrization is to relax them into a continuous distribution.,1.1. Related Work,[0],[0]
"Concrete (Maddison et al., 2016) or Gumbel-Softmax (Jang et al., 2016) adopt this strategy by adding Gumbel noise to the logits of a softmax function with a temperature hyperparameter.",1.1. Related Work,[0],[0]
"A slope-annealed version of the ST estimator is proposed by (Chung et al., 2016) and is equivalent to the Gumbel-Softmax approach for binary variables.",1.1. Related Work,[0],[0]
"REBAR (Tucker et al., 2017) is a recent method that blends REINFORCE with Concrete to synthesize control variates.",1.1. Related Work,[0],[0]
"(Rolfe, 2016) pairs discrete variables with auxiliary continuous variables and marginalizes out the discrete variables.
",1.1. Related Work,[0],[0]
"Both overlapping transformations and Gumbel-based approaches offer smoothing through non-zero temperature; however, overlapping transformations offer additional freedom through the choice of the mixture distributions.",1.1. Related Work,[0],[0]
Let x represent observed random variables and z latent variables.,2. Background,[0],[0]
"The joint distribution over these variables is defined by the generative model p(x,z) = p(z)p(x|z), where p(z) is a prior distribution and p(x|z) is a probabilistic decoder.",2. Background,[0],[0]
"Given a dataset X = {x(1), . . .",2. Background,[0],[0]
",x(N)}, the parameters of the model are trained by maximizing the log-likelihood:
log p(X ) =
N�
i=1
log p(x(i)).
",2. Background,[0],[0]
"Typically, computing log p(x) requires an intractable marginalization over the latent variables z .",2. Background,[0],[0]
"To address this problem, the VAE (Kingma & Welling, 2014) introduces an inference model or probabilistic encoder q(z |x) that infers latent variables for each observation.",2. Background,[0],[0]
"In the VAE, instead of the maximizing the marginal log-likelihood, a variational lower bound (ELBO) is maximized:
log p(x) ≥ Eq(z|x) � log p(x|z) �",2. Background,[0],[0]
− KL � q(z |x)||p(z) � .,2. Background,[0],[0]
"(1)
The gradient of this objective is computed for the parameters of both the encoder and decoder using the reparameterization trick.",2. Background,[0],[0]
"With reparametrization, the expectation with respect to q(z |x) in Eq.",2. Background,[0],[0]
(1) is replaced with an expectation with respect to a known optimization-parameterindependent base distribution and a differentiable transformation from the base distribution to q(z |x).,2. Background,[0],[0]
"This transformation may be a scale-shift transformation, in the case of Gaussian base distributions, or rely on the inverse cumulative distribution function (CDF) in the general case.",2. Background,[0],[0]
"Following the law of the unconscious statistician, the gradient is then estimated using samples from the base distribution.
",2. Background,[0],[0]
"Unfortunately, the reparameterization trick cannot be applied directly to the discrete latent variables because there is no differentiable transformation that maps a base distribution to a discrete distribution.",2. Background,[0],[0]
"Current remedies address this difficulty using a continuous relaxation of the discrete latent variables (Maddison et al., 2016; Jang et al., 2016).",2. Background,[0],[0]
"The discrete variational autoencoder (DVAE) (Rolfe, 2016) develops a different approach which applies the reparameterization trick to a marginal distribution constructed by pairing each discrete variable with an auxiliary continuous random variable.
",2. Background,[0],[0]
"For example, let z ∈ {0, 1} represent a binary random variable with the probability mass function q(z|x).",2. Background,[0],[0]
"A smoothing transformation is defined using spike-and-exponential transformation r(ζ|z), where r(ζ|z = 0) = δ(ζ) is a Dirac δ distribution and r(ζ|z = 1) ∝",2. Background,[0],[0]
exp(βζ) is an exponential distribution defined for ζ ∈,2. Background,[0],[0]
"[0, 1] with inverse temperature β that controls the sharpness of the distribution.",2. Background,[0],[0]
"(Rolfe, 2016) notes that the autoencoding term can be defined as:
�
z
q(z|x) � dζ r(ζ|z) log p(x|ζ) = � dζ q(ζ|x) log p(x|ζ),
where the marginal
q(ζ|x) = �
z
q(z|x)r(ζ|z) (2)
is a mixture of two continuous distributions.",2. Background,[0],[0]
"By factoring the inference model so that x depends on ζ rather than z, the discrete variables can be explicitly eliminated from the ELBO and the reparameterization trick applied.
",2. Background,[0],[0]
"The smoothing transformations in (Rolfe, 2016) are limited to spike-and-X type of transformations (e.g., spike-and-exp and spike-and-Gaussian) where r(ζ|z = 0) is assumed to be a Dirac δ distribution.",2. Background,[0],[0]
This property is required for computing the gradient of the KL term in the variational lower bound.,2. Background,[0],[0]
"A symmetric smoothing transformation of binary variables can also be defined using two exponential distributions:
r(ζ|z = 0) =",3. Overlapping Transformations,[0],[0]
"e −βζ
Zβ and r(ζ|z = 1) = e
β(ζ−1)
",3. Overlapping Transformations,[0],[0]
"Zβ ,
for ζ ∈",3. Overlapping Transformations,[0],[0]
"[0, 1], where Zβ = (1−e−β)/β.",3. Overlapping Transformations,[0],[0]
"These conditionals, visualized in Fig. 1(a), define the mixture distribution q(ζ|x) of Eq.",3. Overlapping Transformations,[0],[0]
(2).,3. Overlapping Transformations,[0],[0]
"The scalar β acts as an inverse temperature as in the Gumbel softmax relaxation, and as β → ∞, q(ζ|x) approaches q(z = 0|x)δ(ζ) + q(z = 1|x)δ(ζ − 1).",3. Overlapping Transformations,[0],[0]
Application of the reparameterization trick for q(ζ|x) requires the inverse CDF of q(ζ|x).,3. Overlapping Transformations,[0],[0]
"In Appendix A of the supplementary material, we show that the inverse CDF is
F−1q(ζ|x)(ρ) =",3. Overlapping Transformations,[0],[0]
"− 1
β log −b+ √ b2 − 4c 2
(3)
where b =",3. Overlapping Transformations,[0],[0]
[ρ + e−β(q − ρ)]/(1 − q) − 1 and c = −[qe−β ]/(1 − q).,3. Overlapping Transformations,[0],[0]
"Eq. (3) is a differentiable function that converts a sample ρ from the uniform distribution U(0, 1) to a sample from q(ζ|x).",3. Overlapping Transformations,[0],[0]
As shown in Fig. 1(b),3. Overlapping Transformations,[0],[0]
the inverse CDF approaches a step function as β →,3. Overlapping Transformations,[0],[0]
"∞. However, to benefit from gradient information during training, β is set to a finite value.",3. Overlapping Transformations,[0],[0]
"Appendix C provides further visualizations comparing overlapping transformations to Concrete smoothing (Maddison et al., 2016; Jang et al., 2016).
",3. Overlapping Transformations,[0],[0]
"The overlapping exponential distributions defined here can
be generalized to any pair of smooth distributions converging to δ(ζ) and δ(ζ − 1).",3. Overlapping Transformations,[0],[0]
"In Appendix B, we provide analogous results for logistic smoothing distributions.
",3. Overlapping Transformations,[0],[0]
"Next, we apply overlapping transformations to the training of generative models with discrete latent variables.",3. Overlapping Transformations,[0],[0]
We consider both directed and undirected latent variable priors.,3. Overlapping Transformations,[0],[0]
"The simplest discrete prior is factorial; however, with conditioning, we can build complex dependencies.",4. Directed Prior,[0],[0]
"To simplify presentation, we illustrate a VAE prior with one or two groups of conditioning variables, but note that the approach straight-forwardly generalizes to many conditioning groups.
",4. Directed Prior,[0],[0]
"Our approach parallels the method developed in (Rolfe, 2016) for undirected graphical models.",4. Directed Prior,[0],[0]
Consider the generative model in Fig. 2(a) and its corresponding inference model in Fig. 2(b).,4. Directed Prior,[0],[0]
"To train this model using smoothing transformations, we introduce the continuous ζ in Figs. 2(c) and 2(d) in which dependencies on z are transferred to dependencies on ζ.",4. Directed Prior,[0],[0]
"In this way, binary latent variables influence other variables only through their continuous counterparts.",4. Directed Prior,[0],[0]
In Figs. 2(e) and 2(f) we show the same model but with z marginalized out.,4. Directed Prior,[0],[0]
"The joint (z,ζ ) model of Figs. 2(c) and 2(d) gives rise to a looser ELBO than the marginal ζ model of Figs. 2(e) and 2(f).",4. Directed Prior,[0],[0]
"Assuming that p(z1), p(z2|ζ1), q(z1|x), q(z2|x,ζ1), r(ζ1|z1), and r(ζ2|z2) are factorial in both the inference and generative models, then q(ζ1|x) and q(ζ2|ζ1,x) are also factorial with q(ζ1|x) =",4.1. Joint ELBO,[0],[0]
"� i q(ζ1,i|x)
where q(ζ1,i|x) = �
z1,i r(ζ1,i|z1,i)q(z1,i|x), and
q(ζ2|ζ1,x) = �
i q(ζ2,i|ζ1,x) where q(ζ2,i|ζ1,x) =� z2,i r(ζ2,i|z2,i)q(z2,i|ζ1,x).",4.1. Joint ELBO,[0],[0]
"In this case, the ELBO for
the model in Fig. 2(c) and 2(d) is Eq(ζ1|x)",4.1. Joint ELBO,[0],[0]
"� Eq(ζ2|ζ1,x) [log p(x|ζ1,ζ2)] �",4.1. Joint ELBO,[0],[0]
"− KL(q(z1|x)||p(z1))
",4.1. Joint ELBO,[0],[0]
− Eq(ζ1|x),4.1. Joint ELBO,[0],[0]
"[KL(q(z2|x,ζ1)||p(z2|ζ1))] .",4.1. Joint ELBO,[0],[0]
"(4)
The KL terms corresponding to the divergence between factorial Bernoulli distributions have a closed form.",4.1. Joint ELBO,[0],[0]
The expectation over ζ1 and ζ2 is reparameterized using the technique presented in Sec. 3.,4.1. Joint ELBO,[0],[0]
The ELBO for the marginal graphical model of Fig. 2(e) and Fig. 2(f) is Eq(ζ1|x),4.2. Marginal ELBO,[0],[0]
"� Eq(ζ2|x,ζ1)",4.2. Marginal ELBO,[0],[0]
"[log p(x|ζ1,ζ2)]",4.2. Marginal ELBO,[0],[0]
"� − KL(q(ζ1|x)||p(ζ1))
",4.2. Marginal ELBO,[0],[0]
− Eq(ζ1|x),4.2. Marginal ELBO,[0],[0]
"[KL(q(ζ2|x,ζ1)||p(ζ2|ζ1))]",4.2. Marginal ELBO,[0],[0]
"(5)
with p(ζ1) =",4.2. Marginal ELBO,[0],[0]
"�
i p(ζ1,i) where p(ζ1,i) =� zi r(ζ1,i|z1,i)p(z1,i) and p(ζ2|ζ1) = � i p(ζ2,i|ζ1)
where p(ζ2,i|ζ1) = � z2,i r(ζ2,i|z2,i)p(z2,i|ζ1).",4.2. Marginal ELBO,[0],[0]
The KL terms no longer have a closed form but can be estimated with the Monte Carlo method.,4.2. Marginal ELBO,[0],[0]
"In Appendix D, we show that Eq. (5) provides a tighter bound on log p(x) than does Eq. (4).",4.2. Marginal ELBO,[0],[0]
"(Rolfe, 2016) defined an expressive prior over binary latent variables by using a Boltzmann machine.",5. Boltzmann Machine Prior,[0],[0]
"We build upon that work and present a simpler objective that can still be trained with a low-variance gradient estimate.
",5. Boltzmann Machine Prior,[0],[0]
"To simplify notation, we assume that the prior distribution over the latent binary variables is a restricted Boltzmann machine (RBM), but these results can be extended to general BMs.",5. Boltzmann Machine Prior,[0],[0]
"An RBM defines a probability distribution over binary random variables arranged on a bipartite graph as p(z1, z2) = e−E(z1,z2)/Z where E(z1, z2) =",5. Boltzmann Machine Prior,[0],[0]
"−aT1 z1 − aT2 z2 − zT1Wz2 is an energy function with linear biases a1 and a2, and pairwise interactions W .",5. Boltzmann Machine Prior,[0],[0]
"Z is the partition function.
",5. Boltzmann Machine Prior,[0],[0]
Fig. 2(g) visualizes a generative model with a BM prior.,5. Boltzmann Machine Prior,[0],[0]
"As in Figs. 2(c) and 2(d), conditionals are formed on the auxiliary variables ζ instead of the binary variables z .",5. Boltzmann Machine Prior,[0],[0]
"The inference model in this case is identical to the model in Fig. 2(d) and it infers both z and ζ in a hierarchical structure.
",5. Boltzmann Machine Prior,[0],[0]
The autoencoding contribution to the ELBO with an RBM prior is again the first term in Eq.,5. Boltzmann Machine Prior,[0],[0]
(4) since both models share the same inference model structure.,5. Boltzmann Machine Prior,[0],[0]
"However, computing the KL term with the RBM prior is more challenging.",5. Boltzmann Machine Prior,[0],[0]
"Here, a novel formulation for the KL term is introduced.",5. Boltzmann Machine Prior,[0],[0]
"Our derivation can be used for training discrete variational autoencoders with a BM prior without any manual coding of gradients.
",5. Boltzmann Machine Prior,[0],[0]
"We use Eq(z,ζ |x)[f ] = Eq(ζ |x)",5. Boltzmann Machine Prior,[0],[0]
�,5. Boltzmann Machine Prior,[0],[0]
"Eq(z|x,ζ)[f ] � to compute the KL contribution to the ELBO: KL � q(z1, z2,ζ1,ζ2|x)�p(z1, z2,ζ1,ζ2) �",5. Boltzmann Machine Prior,[0],[0]
"=
logZ − H � q(z1|x) �",5. Boltzmann Machine Prior,[0],[0]
"− Eq(ζ1|x) � H � q(z2|x,ζ1) �� + (6) + Eq(ζ1|x)",5. Boltzmann Machine Prior,[0],[0]
"� Eq(ζ2|x,ζ1) �",5. Boltzmann Machine Prior,[0],[0]
"Eq(z1|x,ζ1) �",5. Boltzmann Machine Prior,[0],[0]
"Eq(z2|x,ζ1,ζ2) �",5. Boltzmann Machine Prior,[0],[0]
"E(z1, z2) �� � �� �
cross-entropy
�� .
",5. Boltzmann Machine Prior,[0],[0]
"Here, H(q) is the entropy of the distribution q, which has a closed form when q is factorial Bernoulli.",5. Boltzmann Machine Prior,[0],[0]
"The conditionals q(z1|x,ζ1) and q(z2|x,ζ1,ζ2) are both factorial distributions that have analytic expressions.",5. Boltzmann Machine Prior,[0],[0]
"Denoting
µ1,i(x) ≡",5. Boltzmann Machine Prior,[0],[0]
"q(z1,i = 1|x), ν1,i(x,ζ1) ≡ q(z1,i = 1|x,ζ1), µ2,i(x,ζ1) ≡",5. Boltzmann Machine Prior,[0],[0]
"q(z2,i = 1|x,ζ1),
ν2,i(x,ζ1,ζ2) ≡",5. Boltzmann Machine Prior,[0],[0]
"q(z2,i = 1|x,ζ1,ζ2), it is straightforward to show that
ν1,i(x, ζ1) = q(z1,i = 1|x)r(ζ1,i|z1,i = 1)�
z1,i q(z1,i|x)r(ζ1,i|z1,i)
=
= σ � g(µ1,i(x))",5. Boltzmann Machine Prior,[0],[0]
"+ log �r(ζ1,i|z = 1) r(ζ1,i|z = 0) �� ,
where σ(x) = 1/(1 + e−x) is the logistic function, and g(µ) ≡ log � µ/ � 1 − µ �� is the logit function.",5. Boltzmann Machine Prior,[0],[0]
"A similar expression holds for ν2(x,ζ1,ζ2).",5. Boltzmann Machine Prior,[0],[0]
"The expectation marked as cross-entropy in Eq. (6) corresponds to the cross-entropy between a factorial distribution and an unnormalized Boltzmann machine which is
−aT1 ν1(x,ζ1)−aT2 ν2(x,ζ1,ζ2)−ν1(x,ζ1)TWν2(x,ζ1,ζ2).",5. Boltzmann Machine Prior,[0],[0]
"Finally, we use the equalities Eq(ζ1|x)[ν1(x,ζ1)] = µ1(x) and Eq(ζ2|x,ζ1)[ν2(x,ζ1,ζ2)]",5. Boltzmann Machine Prior,[0],[0]
"= µ2(x,ζ1) to simplify the cross-entropy term which defines the KL as
KL � q(z1, z2,ζ1,ζ2|x)�p(z1, z2,ζ1,ζ2)",5. Boltzmann Machine Prior,[0],[0]
"� = logZ
− H � q(z1|x) �",5. Boltzmann Machine Prior,[0],[0]
"− Eq(ζ1|x) � H � q(z2|x,ζ1) ��",5. Boltzmann Machine Prior,[0],[0]
−,5. Boltzmann Machine Prior,[0],[0]
"aT1 µ1(x)− Eq(ζ1|x) � aT2 µ2(x,ζ1)",5. Boltzmann Machine Prior,[0],[0]
�,5. Boltzmann Machine Prior,[0],[0]
− Eq(ζ1|x) �,5. Boltzmann Machine Prior,[0],[0]
"ν1(x,ζ1) TWµ2(x,ζ1) � .
",5. Boltzmann Machine Prior,[0],[0]
All terms contributing to the KL other than logZ can be computed analytically given samples from the hierarchical encoder.,5. Boltzmann Machine Prior,[0],[0]
Expectations with respect to q(ζ1|x) are reparameterized using the inverse CDF function.,5. Boltzmann Machine Prior,[0],[0]
Any automatic differentiation (AD) library can then back-propagate gradients through the network.,5. Boltzmann Machine Prior,[0],[0]
Only logZ requires special treatment.,5. Boltzmann Machine Prior,[0],[0]
"In Appendix E, we show how this term can also be included in the objective function so that its gradient is computed automatically.",5. Boltzmann Machine Prior,[0],[0]
"The ability of AD to calculate gradients stands in contrast to (Rolfe, 2016) where gradients must be manually coded.",5. Boltzmann Machine Prior,[0],[0]
"This pleasing property is a result of r(ζ|z) having the same support for both z = 0 and z = 1, and having a probabilistic q(z|x, ζ) which is not the case for the spike-and-X transformations of (Rolfe, 2016).",5. Boltzmann Machine Prior,[0],[0]
"In previous sections, we have illustrated with simple examples how overlapping transformations can be used to train discrete latent variable models with either directed or undirected priors.",6. DVAE++,[0],[0]
"Here, we develop a network architecture (DVAE++) that improves upon convolutional VAEs for generative image modeling.
",6. DVAE++,[0],[0]
"DVAE++ features both global discrete latent variables (to capture global properties such as scene or object type) and local continuous latent variables (to capture local properties such as object pose, orientation, or style).",6. DVAE++,[0],[0]
Both generative and inference networks rely on an autoregressive structure defined over groups of latent and observed variables.,6. DVAE++,[0],[0]
"As we are modeling images, conditional dependencies between groups of variables are captured with convolutional neural networks.",6. DVAE++,[0],[0]
"DVAE++ is similar to the convolutional VAEs used in (Kingma et al., 2016; Chen et al., 2016), but does not use normalizing flows.",6. DVAE++,[0],[0]
The DVAE++ graphical model is visualized in Fig. 3.,6.1. Graphical Model,[0],[0]
Global and local variables are indicated by z and h respectively.,6.1. Graphical Model,[0],[0]
Subscripts indicate different groups of random variables.,6.1. Graphical Model,[0],[0]
"The conditional distribution of each group is factorial – except for z1 and z2 in the prior, which is modeled with an RBM.",6.1. Graphical Model,[0],[0]
"Global latent variables are represented with boxes and local variables are represented with 3D volumes as they are convolutional.
",6.1. Graphical Model,[0],[0]
Groups of local continuous variables are factorial (independent).,6.1. Graphical Model,[0],[0]
This assumption limits the ability of the model to capture correlations at different spatial locations and different depths.,6.1. Graphical Model,[0],[0]
"While the autoregressive structure mitigates this defect, we rely mainly on the discrete global latent variables to capture long-range dependencies.",6.1. Graphical Model,[0],[0]
"The discrete nature of the global RBM prior allows DVAE++ to capture richlycorrelated discontinuous hidden factors that influence data generation.
",6.1. Graphical Model,[0],[0]
"Fig. 3(a) defines the generative model as
p(z,ζ ,h,x) = p(z) �
i r(ζ1,i|z1,i)r(ζ2,i|z2,i)× �
j
p(hj |h<j ,ζ )p(x|ζ ,h)
where p(z) is an RBM, ζ =",6.1. Graphical Model,[0],[0]
"[ζ1,ζ2], and r is the smoothing transformation that is applied elementwise to z .",6.1. Graphical Model,[0],[0]
"The conditional p(hj |h<j ,ζ ) is defined over the jth local variable group using a factorial normal distribution.",6.1. Graphical Model,[0],[0]
"Inspired by (Reed et al., 2017; Denton et al., 2015), the conditional on the data variable p(x|ζ ,h) is decomposed into several
factors defined on different scales of x:
p(x|ζ ,h) =",6.1. Graphical Model,[0],[0]
"p(x0|ζ ,h) �
i
p(xi|ζ ,h,x<i)
Here, x0 is of size 4 × 4 and it represents downsampled x in the lowest scale.",6.1. Graphical Model,[0],[0]
"Conditioned on x0, we generate x1 in the next scale, which is of the size 8 × 8.",6.1. Graphical Model,[0],[0]
This process is continued until the full-scale image is generated (see Appendix G.1 for more details).,6.1. Graphical Model,[0],[0]
"Here, each conditional is represented using a factorial distribution.",6.1. Graphical Model,[0],[0]
"For binary images, a factorial Bernoulli distribution is used; for colored images a factorial mixture of discretized logistic distributions is used (Salimans et al., 2017).
",6.1. Graphical Model,[0],[0]
The inference model of Fig.,6.1. Graphical Model,[0],[0]
"3(b) conditions over latent variables in a similar order as the generative model: q(z,ζ ,h|x) = q(z1|x)",6.1. Graphical Model,[0],[0]
"�
i
r(ζ1,i|z1,i)×
q(z2|x,ζ1)",6.1. Graphical Model,[0],[0]
"�
k
r(ζ2,k|z2,k) �
j
q(hj |ζ ,h<j).
",6.1. Graphical Model,[0],[0]
"The conditionals q(z1|x) and q(z2|x,ζ1) are each modeled with a factorial Bernoulli distribution, and q(hj |ζ ,h<j) represents the conditional on the jth group of local variables.
",6.1. Graphical Model,[0],[0]
"DVAE++ is related to VAEs with mixture priors (Makhzani et al., 2015; Tomczak & Welling, 2017).",6.1. Graphical Model,[0],[0]
The discrete variables z1 and z2 take exponentially many joint configurations where each configuration corresponds to a mixture component.,6.1. Graphical Model,[0],[0]
"These components are mixed by p(z1, z2) in the generative model.",6.1. Graphical Model,[0],[0]
"During training, the inference model maps each data point to a small subset of all the possible mixture components.",6.1. Graphical Model,[0],[0]
"Thus, the discrete prior learns to suppress the probability of configurations that are not used by the inference model.",6.1. Graphical Model,[0],[0]
"Training results in a multimodal p(z1, z2) that assigns similar images to a common discrete mode.",6.1. Graphical Model,[0],[0]
We use a novel neural network architecture to realize the conditional probabilities within the graphical model Fig. 3.,6.2. Neural Network Architecture,[0],[0]
"The network uses residual connections (He et al., 2016) with squeeze-and-excitation (SE) blocks (Hu et al., 2017) that have shown state-of-the-art image classification performance.",6.2. Neural Network Architecture,[0],[0]
"Our architecture is explained fully in Appendix G, and here we sketch the main components.",6.2. Neural Network Architecture,[0],[0]
"We refer to a SEResNet block as a residual block, and the network is created by combining either residual blocks, fully-connected layers, or convolutional layers.
",6.2. Neural Network Architecture,[0],[0]
The encoder uses a series of downsampling residual blocks to extract convolutional features from an input image.,6.2. Neural Network Architecture,[0],[0]
This residual network is considered as a pre-processing step that extracts convolutional feature maps at different scales.,6.2. Neural Network Architecture,[0],[0]
"The output of this network at the highest level is fed to fullyconnected networks that define q(z i|x,ζ<i) successively for
all the global latent variables.",6.2. Neural Network Architecture,[0],[0]
"The feature maps at an intermediate scale are fed to another set of residual networks that define q(hj |x,ζ ,h<j) successively for all the local latent variables.
",6.2. Neural Network Architecture,[0],[0]
The decoder uses an upsampling network to scale-up the global latent variables to the intermediate scale.,6.2. Neural Network Architecture,[0],[0]
"Then, the output of this network is fed to a set of residual networks that define p(hj |ζ ,h<j) one at a time at the same scale.",6.2. Neural Network Architecture,[0],[0]
"Finally, another set of residual networks progressively scales the samples from the latent variables up to the data space.",6.2. Neural Network Architecture,[0],[0]
"In the data space, a distribution on the smallest scale x0 is formed using a residual network.",6.2. Neural Network Architecture,[0],[0]
"Given samples at this scale, the distribution at the next scale is formed using another upsampling residual network.",6.2. Neural Network Architecture,[0],[0]
"This process is repeated until the image is generated at full scale.
",6.2. Neural Network Architecture,[0],[0]
"With many layers of latent variables, the VAE objective often turns off many of the latent variables by matching their distribution in the inference model to the prior.",6.2. Neural Network Architecture,[0],[0]
The latent units are usually removed differentially across different groups.,6.2. Neural Network Architecture,[0],[0]
Appendix H presents a technique that enables efficient use of latent variables across all groups.,6.2. Neural Network Architecture,[0],[0]
"To provide a comprehensive picture of overlapping transformations and DVAE++, we conduct three sets of experiments.",7. Experiments,[0],[0]
In Sec. 7.1 and Sec. 7.2 we train a VAE with several layers of latent variables with a feed-forward encoder and decoder.,7. Experiments,[0],[0]
This allows to compare overlapping transformations with previous work on discrete latent variables.,7. Experiments,[0],[0]
"In Sec. 7.3, we
then compare DVAE++ to several baselines.",7. Experiments,[0],[0]
"We compare overlapping transformations to NVIL (Mnih & Gregor, 2014), MuProp (Gu et al., 2015), REBAR (Tucker et al., 2017), and Concrete (Maddison et al., 2016) for training discrete single-layer latent variable models.",7.1. Comparison with Previous Discrete Latent Variable Models,[0],[0]
"We follow the structure used by (Tucker et al., 2017) in which the prior distribution and inference model are factorial Bernoulli with 200 stochastic variables.",7.1. Comparison with Previous Discrete Latent Variable Models,[0],[0]
"In this setting, the inference and generative models are either linear or nonlinear functions.",7.1. Comparison with Previous Discrete Latent Variable Models,[0],[0]
"In the latter case, two layers of deterministic hidden units of the size 200 with tanh activation are used.
",7.1. Comparison with Previous Discrete Latent Variable Models,[0],[0]
"We use the settings in (Tucker et al., 2017) to initialize the parameters, define the model, and optimize the parameters for the same number of iterations.",7.1. Comparison with Previous Discrete Latent Variable Models,[0],[0]
"However, (Tucker et al., 2017) uses the Adam optimizer with β2 = 0.99999 in training.",7.1. Comparison with Previous Discrete Latent Variable Models,[0],[0]
We used Adam with its default parameters except for � which is set to 10−3.,7.1. Comparison with Previous Discrete Latent Variable Models,[0],[0]
"The learning rate is selected from the set {1 · 10−4, 5 · 10−4}.",7.1. Comparison with Previous Discrete Latent Variable Models,[0],[0]
"The inverse temperature β for smoothing is annealed linearly during training with initial and final values chosen using cross validation from {5, 6, 7, 8} and {12, 14, 16, 18} respectively.",7.1. Comparison with Previous Discrete Latent Variable Models,[0],[0]
"In Table 1, the performance of our model is compared with several stateof-the-art techniques proposed for training binary latent models on (statically) binarized MNIST (Salakhutdinov & Murray, 2008) and OMNIGLOT (Lake et al., 2015).",7.1. Comparison with Previous Discrete Latent Variable Models,[0],[0]
"At test time, all models are evaluated in the binary limit (β = ∞).",7.1. Comparison with Previous Discrete Latent Variable Models,[0],[0]
"Smoothing transformations slightly outperform previous
techniques in most cases.",7.1. Comparison with Previous Discrete Latent Variable Models,[0],[0]
"In the case of the nonlinear model on OMNIGLOT, the difference is about 2.8 nats.",7.1. Comparison with Previous Discrete Latent Variable Models,[0],[0]
"Techniques such as KL annealing (Sønderby et al., 2016), batch normalization (Ioffe & Szegedy, 2015), autoregressive inference/prior, and learning-rate decay can significantly improve the performance of a VAE beyond the results reported in Sec. 7.1.",7.2. Comparison with Previous RBM Prior VAE,[0],[0]
"In this second set of experiments, we evaluate overlapping transformations by comparing the training of a VAE with an RBM prior to the original DVAE (Rolfe, 2016), both of which include these improvements.",7.2. Comparison with Previous RBM Prior VAE,[0],[0]
"For a fair comparison, we apply only those techniques that were also used in (Rolfe, 2016).",7.2. Comparison with Previous RBM Prior VAE,[0],[0]
We examine VAEs with one and two latent layers with feed-forward linear or nonlinear inference and generative models.,7.2. Comparison with Previous RBM Prior VAE,[0],[0]
"In the one-latent-layer case, the KL term in both our model and (Rolfe, 2016) reduces to the mean-field approximation.",7.2. Comparison with Previous RBM Prior VAE,[0],[0]
"The only difference in this case lies in the overlapping transformations used here and the original smoothing method of (Rolfe, 2016).",7.2. Comparison with Previous RBM Prior VAE,[0],[0]
"In the two-latent-layer case, our inference and generative model have the forms depicted in Fig. 2(d) and Fig. 2(g).",7.2. Comparison with Previous RBM Prior VAE,[0],[0]
"Again, all models are evaluated in the binary limit at the test time.
",7.2. Comparison with Previous RBM Prior VAE,[0],[0]
Comparisons are reported in Table 2.,7.2. Comparison with Previous RBM Prior VAE,[0],[0]
"For reference, we also provide the performance of the directed VAE models with the structures visualized in Fig. 2(c) to Fig. 2(f).",7.2. Comparison with Previous RBM Prior VAE,[0],[0]
Implementation details are provided in Appendix F. Two observations can be made from Table 2.,7.2. Comparison with Previous RBM Prior VAE,[0],[0]
"First, our smoothing transformation outperforms (Rolfe, 2016) in most cases.",7.2. Comparison with Previous RBM Prior VAE,[0],[0]
In some cases the difference is as large as 5.1 nats.,7.2. Comparison with Previous RBM Prior VAE,[0],[0]
"Second, the RBM prior performs better than a directed prior of the same size.",7.2. Comparison with Previous RBM Prior VAE,[0],[0]
"Lastly, we explore the performance of DVAE++ for density estimation on 2D images.",7.3. Experiments on DVAE++,[0],[0]
"In addition to statically binarized MNIST and OMNIGLOT, we test dynamically binarized MNIST (LeCun et al., 1998) and Caltech-101 silhouettes (Marlin et al., 2010).",7.3. Experiments on DVAE++,[0],[0]
All datasets have 28 × 28 binary pixel images.,7.3. Experiments on DVAE++,[0],[0]
"We use the same architecture for the MNIST and OMNIGLOT datasets, but because the Caltech101 silhouettes dataset is smaller, our model easily overfits.",7.3. Experiments on DVAE++,[0],[0]
"Consequently, we use a shallower architecture for Caltech101.",7.3. Experiments on DVAE++,[0],[0]
"We also evaluate DVAE++ on the CIFAR10 dataset, which consists of 32×32 pixel natural images.",7.3. Experiments on DVAE++,[0],[0]
"Appendix G lists the details of our architecture for different datasets.
",7.3. Experiments on DVAE++,[0],[0]
"Our goal is to determine whether we can use overlapping transformations to train a convolutional VAE with an RBM prior, and whether the RBM prior in DVAE++ captures global discrete hidden factors.",7.3. Experiments on DVAE++,[0],[0]
"In addition to DVAE++ (which uses binary global latent variables and continuous local latent variables), four different baselines are introduced by modifying the global and local distributions.",7.3. Experiments on DVAE++,[0],[0]
These baselines are listed in Table 3.,7.3. Experiments on DVAE++,[0],[0]
"For RBM (Rolfe), the spikeand-exp smoothing transformation is used and the ELBO is optimized using the derivation supplied in (Rolfe, 2016).",7.3. Experiments on DVAE++,[0],[0]
"For Bernoulli latent variables, we used the marginal distributions proposed in Sec. 4.2.",7.3. Experiments on DVAE++,[0],[0]
"For all the models, we used 16 layers of local latent variables each with 32 random variables at each spatial location.",7.3. Experiments on DVAE++,[0],[0]
"For the RBM global variables, we used 16 binary variables for all the binary datasets and 128 binary variables for CIFAR10.",7.3. Experiments on DVAE++,[0],[0]
"We cross-validated the number of the hierarchical layers in the inference model for the global variables from the set {1, 2, 4}.",7.3. Experiments on DVAE++,[0],[0]
"We used an unconditional decoder (i.e., factorial p(x|ζ ,h)) for the MNIST
datasets.",7.3. Experiments on DVAE++,[0],[0]
"We measure performance by estimating test set log-likelihood (again, according to the binary model) with 4000 importance weighted samples.",7.3. Experiments on DVAE++,[0],[0]
"Appendix I presents additional ablation experiments.
",7.3. Experiments on DVAE++,[0],[0]
"Table 3 groups the baselines into three categories: all continuous latent, discrete global and continuous local (mixed), and all discrete.",7.3. Experiments on DVAE++,[0],[0]
"Within the mixed group, DVAE++ with RBM prior generally outperforms the same model trained with (Rolfe, 2016)’s.",7.3. Experiments on DVAE++,[0],[0]
Replacing the continuous normal local variables with Bernoulli variables does not dramatically hurt the performance.,7.3. Experiments on DVAE++,[0],[0]
"For example, in the case of statically and dynamically binarized MNIST dataset, we achieve −79.72 and −79.55 respectively with unconditional decoder and 3.59 on CIFAR10 with conditional decoder.",7.3. Experiments on DVAE++,[0],[0]
To the best of our knowledge these are the best reported results on these datasets with binary latent variables.,7.3. Experiments on DVAE++,[0],[0]
Samples generated from DVAE++ are visualized in Fig. 4.,7.3. Experiments on DVAE++,[0],[0]
"As shown, the discrete global prior clearly captures discontinuous latent factors such as digit category or scene configuration.
",7.3. Experiments on DVAE++,[0],[0]
"DVAE++ results are comparable to current state-of-theart convolutional latent variable models such as VampPrior (Tomczak & Welling, 2017) and variational lossy autoencoder (VLAE)",7.3. Experiments on DVAE++,[0],[0]
"(Chen et al., 2016).",7.3. Experiments on DVAE++,[0],[0]
We note two features of these models that may offer room for further improvement for DVAE++.,7.3. Experiments on DVAE++,[0],[0]
"First, the conditional decoder used here
makes independence assumptions in each scale, whereas the state-of-the-art techniques are based on PixelCNN (Van Den Oord et al., 2016), which assumes full autoregressive dependencies.",7.3. Experiments on DVAE++,[0],[0]
"Second, methods such as VLAE use normalizing flows for flexible inference models that reduce the KL cost on the convolutional latent variables.",7.3. Experiments on DVAE++,[0],[0]
"Here, the independence assumption in each local group in DVAE++ can cause a significant KL penalty.",7.3. Experiments on DVAE++,[0],[0]
We have introduced a new family of smoothing transformations consisting of a mixture of two overlapping distributions and have demonstrated that these transformations can be used for training latent variable models with either directed or undirected priors.,8. Conclusions,[0],[0]
"Using variational bounds derived for both cases, we developed DVAE++ having a global RBM prior and local convolutional latent variables.",8. Conclusions,[0],[0]
"All experiments used exponential mixture components, but it would be interesting to explore the efficacy of other choices.",8. Conclusions,[0],[0]
Training of discrete latent variable models remains challenging because passing gradient information through discrete units is difficult.,abstractText,[0],[0]
"We propose a new class of smoothing transformations based on a mixture of two overlapping distributions, and show that the proposed transformation can be used for training binary latent models with either directed or undirected priors.",abstractText,[0],[0]
We derive a new variational bound to efficiently train with Boltzmann machine priors.,abstractText,[0],[0]
"Using this bound, we develop DVAE++, a generative model with a global discrete prior and a hierarchy of convolutional continuous variables.",abstractText,[0],[0]
"Experiments on several benchmarks show that overlapping transformations outperform other recent continuous relaxations of discrete latent variables including Gumbel-Softmax (Maddison et al., 2016; Jang et al., 2016), and discrete variational autoencoders (Rolfe, 2016).",abstractText,[0],[0]
DVAE++: Discrete Variational Autoencoders with Overlapping Transformations,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1303–1313 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
1303",text,[0],[0]
"The shift-reduce transition-based framework was initially introduced, and successfully adapted from the dependency formalism, into constituent parsing by Sagae and Lavie (2005), significantly increasing phrase-structure parsing performance.
",1 Introduction,[0],[0]
"A shift-reduce algorithm uses a sequence of transitions to modify the content of two main data structures (a buffer and a stack) and create partial phrase-structure trees (or constituents) in the stack to finally produce a complete syntactic analysis for an input sentence, running in linear time.",1 Introduction,[0],[0]
"Initially, Sagae and Lavie (2005) suggested that those partial phrase-structure trees be built in a bottom-up manner: two adjacent nodes already in the stack are combined under a non-terminal to become a new constituent.",1 Introduction,[0],[0]
"This strategy was followed by many researchers (Zhang and Clark, 2009; Zhu et al., 2013; Watanabe and Sumita, 2015; Mi and Huang, 2015; Crabbé, 2015; Cross and Huang, 2016b; Coavoux and Crabbé, 2016; FernándezGonzález and Gómez-Rodrı́guez, 2018) who managed to improve the accuracy and speed of the original Sagae and Lavie’s bottom-up parser.",1 Introduction,[0],[0]
"With this, shift-reduce algorithms have become com-
petitive, and are the fastest alternative to perform phrase-structure parsing to date.
",1 Introduction,[0],[0]
"Some of these attempts (Cross and Huang, 2016b; Coavoux and Crabbé, 2016; FernándezGonzález and Gómez-Rodrı́guez, 2018) introduced dynamic oracles (Goldberg and Nivre, 2012), originally designed for transition-based dependency algorithms, to bottom-up constituent parsing.",1 Introduction,[0],[0]
They propose to use these dynamic oracles to train shift-reduce parsers instead of a traditional static oracle.,1 Introduction,[0],[0]
The latter follows the standard procedure that uses a gold sequence of transitions to train a model for parsing new sentences at test time.,1 Introduction,[0],[0]
"A shift-reduce parser trained with this approach tends to be prone to suffer from error propagation (i.e. errors made in previous states are propagated to subsequent states, causing further mistakes in the transition sequence).",1 Introduction,[0],[0]
"Dynamic oracles (Goldberg and Nivre, 2012) were developed to minimize the effect of error propagation by training parsers under closer conditions to those found at test time, where mistakes are inevitably made.",1 Introduction,[0],[0]
They are designed to guide the parser through any state it might reach during learning time.,1 Introduction,[0],[0]
"This makes it possible to introduce error exploration to force the parser to go through nonoptimal states, teaching it how to recover from mistakes and lose the minimum number of gold constituents.
",1 Introduction,[0],[0]
"Alternatively, some researchers decided to follow a different direction and explore non-bottomup strategies for producing phrase-structure syntactic analysis.
",1 Introduction,[0],[0]
"On the one hand, (Dyer et al., 2016; Kuncoro et al., 2017) proposed a top-down transition-based algorithm, which creates a phrase structure tree in the stack by first choosing the non-terminal on the top of the tree, and then considering which should be its child nodes.",1 Introduction,[0],[0]
"In contrast to the bottom-up approach, this top-down strategy adds a lookahead
guidance to the parsing process, while it loses rich local features from partially-built trees.
",1 Introduction,[0],[0]
"On the other hand, Liu and Zhang (2017a) recently developed a novel strategy that finds a compromise between the strengths of top-down and bottom-up approaches, resulting in state-of-the-art accuracy.",1 Introduction,[0],[0]
"Concretely, this parser builds the tree following an in-order traversal: instead of starting the tree from the top, it chooses the non-terminal of the resulting subtree after having the first child node in the stack.",1 Introduction,[0],[0]
"In that way each partial constituent tree is created in a bottom-up manner, but the non-terminal node is not chosen when all child nodes are in the stack (as a purely bottom-up parser does), but after the first child is considered.
",1 Introduction,[0],[0]
Liu and Zhang (2017a) report that the top-down approach is on par with the bottom-up strategy in terms of accuracy and the in-order parser yields the best accuracy to date on the WSJ.,1 Introduction,[0],[0]
"However, despite being two adequate alternatives to the traditional bottom-up strategy, no further work has been undertaken to improve their performance.1
We propose what, to our knowledge, are the first optimal dynamic oracles for both the topdown and in-order shift-reduce parsers, allowing us to train these algorithms with exploration.",1 Introduction,[0],[0]
"The resulting parsers outperform the existing versions trained with static oracles on the WSJ Penn Treebank (Marcus et al., 1993) and Chinese Treebank (CTB) benchmarks (Xue et al., 2005).",1 Introduction,[0],[0]
The version of the in-order parser trained with our dynamic oracle achieves the highest accuracy obtained so far by a single fully-supervised greedy shift-reduce system on the WSJ.,1 Introduction,[0],[0]
"The original transition system of Sagae and Lavie (2005) parses a sentence from left to right by reading (moving) words from a buffer to a stack, where partial subtrees are built.",2 Preliminaries,[0],[0]
"This process is per-
1In parallel to this work, Fried and Klein (2018) present a non-optimal dynamic oracle for training the top-down parser.
formed by a sequence of Shift (for reading) and Reduce (for building) transitions that will lead the parser through different states or parser configurations until a terminal one is reached.",2 Preliminaries,[0],[0]
"While in the bottom-up strategy the Reduce transition is in charge of labeling the partial subtree with a nonterminal at the same time the tree is built, Dyer et al. (2016) and Liu and Zhang (2017a) introduce a novel transition to choose the non-terminal on top, leaving the Reduce transition just to create the subtree under the previously decided nonterminal.",2 Preliminaries,[0],[0]
"We will now explain more in detail both the top-down and the in-order transition systems.
",2 Preliminaries,[0],[0]
"In both transition systems, parser configurations have the form c = 〈Σ, i, f, γ, α〉, where Σ is a stack of constituents, i is the position of the leftmost unprocessed word in the buffer (which is the next to be pushed onto the stack), f is a boolean variable used by the in-order transition system to mark if a configuration is terminal or not and with no value in top-down parser configurations, γ is the set of constituents that have already been built, and α is the set of non-terminal nodes that are currently in the stack.
",2 Preliminaries,[0],[0]
"Each constituent is represented as a tuple (X, l, r), where X is a non-terminal and l and r are integers defining its span.",2 Preliminaries,[0],[0]
"Constituents are composed of one or several words or constituents, and just one non-terminal node on top.",2 Preliminaries,[0],[0]
"Each word wi is represented as (w, i, i+ 1).",2 Preliminaries,[0],[0]
"To define our oracles, we will need to represent each non-terminal node of the tree as (X, j), where j has the value of i when X is included in the stack and is used to keep them in order.2
For instance, the phrase-structure tree in Figure 1 can be decomposed as the following set of gold constituents: {(S, 0, 6), (NP, 0, 2), (VP, 2, 5), (ADVP, 3, 4), (ADJP, 4, 5)}.",2 Preliminaries,[0],[0]
"In addition, the ordered set of gold non-terminal nodes added to the stack while following a top-down strategy will be {(S, 0), (NP, 0), (VP, 2), (ADVP, 3), (ADJP, 4)} and, according to an in-order approach, {(NP, 1), (S, 2), (VP, 3), (ADVP, 4), (ADJP, 5)}.",2 Preliminaries,[0],[0]
It is worth mentioning that the index of non-terminal nodes in the top-down method is the same as the leftmost span index of the constituent that it will produce.,2 Preliminaries,[0],[0]
"However, this does not hold in the in-order approach, as the leftmost child is fully processed before the node is added to the stack, so the index
2When two or more non-terminals share their labels within the tree, we use a secondary index to make them unique.
for the node will point to the leftmost span index of the second leftmost child.
",2 Preliminaries,[0],[0]
"Note that the information about the span of a constituent, the set of predicted constituents γ and the set α of predicted non-terminal nodes in the stack is not used by the original top-down and inorder parsers.",2 Preliminaries,[0],[0]
"However, we need to include it in parser configurations at learning time to allow an efficient implementation of the proposed dynamic oracles.
",2 Preliminaries,[0],[0]
"Given an input string w0 · · ·wn−1, the in-order parsing process starts at the initial configuration cs(w0 . . .",2 Preliminaries,[0],[0]
wn−1) = 〈,2 Preliminaries,[0],[0]
"[ ], 0, false, {}, {}〉 and, after applying a sequence of transitions, it ends in a terminal configuration 〈(S, 0, n), n, true, γ, α〉, where n is the number of words in the input sentence.",2 Preliminaries,[0],[0]
"The top-down transition system shares the same form for the initial and terminal configurations, except for the fact that variable f has no value in both cases.
",2 Preliminaries,[0],[0]
Figure 2 shows the available transitions in the top-down algorithm.,2 Preliminaries,[0],[0]
"In particular, the Shift transition moves the first (leftmost) word in the buffer to the stack; the Non-Terminal-X transition pushes onto the stack the non-terminal node X that should be on top of a coming constituent, and the Reduce transition pops the topmost stack nodes until the first non-terminal node appears (which is also popped) and combines them into a constituent with this non-terminal node as their parent, pushing this new constituent into the stack.",2 Preliminaries,[0],[0]
"Note that every reduction action will add a new constituent to γ and remove a non-terminal node from α, and every Non-Terminal transition will include a new non-terminal node in α.",2 Preliminaries,[0],[0]
"Figure 3 shows the top-down transition sequence that produces the phrase-structure tree in Figure 1.
",2 Preliminaries,[0],[0]
In Figure 4 we describe the available transitions in the in-order algorithm.,2 Preliminaries,[0],[0]
"The Shift, Non-Terminal-X and Reduce transitions have the same behavior as defined for the top-down transition system, except that the Reduce transition not only pops stack nodes until finding a non-terminal node (also removed from the stack), but also the node below this non-terminal node, and combines them into a constituent spanning all the popped nodes with the non-terminal node on top.",2 Preliminaries,[0],[0]
"And, finally, a Finish transition is also available to end the parsing process.",2 Preliminaries,[0],[0]
"Figure 5 shows the in-order transition sequence that outputs the constituent tree in Figure 1.
",2 Preliminaries,[0],[0]
"The standard procedure to train a greedy shiftreduce parser consists of training a classifier to approximate an oracle, which chooses optimal transitions with respect to gold parse trees.",2 Preliminaries,[0],[0]
"This classifier will greedily choose which transition sequence the parser should apply at test time.
",2 Preliminaries,[0],[0]
"Depending on the strategy used for training the parser, oracles can be static or dynamic.",2 Preliminaries,[0],[0]
"A static oracle trains the parser only on gold transition sequences, while a dynamic one can guide the parser through any possible transition path, allowing the exploration of non-optimal sequences.",2 Preliminaries,[0],[0]
"Previous work such as (Cross and Huang, 2016b; Coavoux and Crabbé, 2016; Fernández-González and Gómez-Rodrı́guez, 2018) has introduced and successfully applied dynamic oracles for bottomup phrase-structure parsing.",3 Dynamic Oracles,[0],[0]
"We present dynamic oracles for training the top-down and in-order transition-based constituent parsers.
",3 Dynamic Oracles,[0],[0]
Goldberg and Nivre (2012) show that implementing a dynamic oracle reduces to defining a loss function on configurations to measure the distance from the best tree they can produce to the gold parse.,3 Dynamic Oracles,[0],[0]
This allows us to compute which transitions will lead the parser to configurations where the minimum number of mistakes are made.,3 Dynamic Oracles,[0],[0]
"According to Fernández-González and GómezRodrı́guez (2018), we can define a loss function in constituent parsing as follows: given a parser configuration c and a gold tree tG, a loss function `(c) is implemented as the minimum Hamming loss between t and tG, (L(t, tG)), where t is the already-built tree of a configuration c′ reachable from c (written as c t).",3.1 Loss function,[0],[0]
"This Hamming loss is computed as the size of the symmetric difference between the set of constituents γ and γG in the trees t and tG, respectively.",3.1 Loss function,[0],[0]
"Therefore, the loss function is defined as:
`(c) =",3.1 Loss function,[0],[0]
min γ|c,3.1 Loss function,[0],[0]
"γ
L(γ, γG) = |γG",3.1 Loss function,[0],[0]
\,3.1 Loss function,[0],[0]
γ|+,3.1 Loss function,[0],[0]
"|γ \ γG|
and, according to the authors, it can be efficiently computed for a non-binary bottom-up transition system by counting the individually unreachable arcs from configuration c (|U(c, γG)|) plus the erroneous constituents created so far (|γc \ γG|):
`(c) =",3.1 Loss function,[0],[0]
min γ|c,3.1 Loss function,[0],[0]
"γ
L(γ, γG) = |U(c, γG)|+ |γc \ γG|
We adapt the latter to efficiently implement a loss function for the top-down and in-order strategies.
",3.1 Loss function,[0],[0]
"While in bottom-up parsing constituents are created at once by a Reduce transition, in the other two approaches a Non-Terminal transition begins the process by naming the future constituent and a Reduce transition builds it by setting its span and children.",3.1 Loss function,[0],[0]
"Therefore, a Non-Terminal transition that deviates from the non-terminals expected in the gold tree will eventually produce a wrong constituent in future configurations, so it should be penalized.",3.1 Loss function,[0],[0]
"Additionally, a sequence of gold Non-Terminal transitions may also lead to a wrong final parse if they are applied in an incorrect order.",3.1 Loss function,[0],[0]
"Then, the computation of the Hamming loss in top-down and in-order phrase-structure parsing adds two more terms to the bottom-up loss expression: (1) the number of predicted non-terminal nodes that are currently in the stack (αc),3 but not included in the set of gold non-terminal nodes (αG), and (2) the number of gold non-terminal
3Note that we only consider predicted non-terminal nodes still in the stack, since wrong non-terminal nodes that have been already reduced are included in the loss as erroneous constituents.
nodes in the stack that are out of order with respect to the order needed in the gold tree:
`(c) =",3.1 Loss function,[0],[0]
min γ|c,3.1 Loss function,[0],[0]
"γ
L(γ, γG) = |U(c, γG)|+ |γc \ γG|
+|αc \ αG|+ out of order(αc, αG)
",3.1 Loss function,[0],[0]
"This loss function is used to implement a dynamic oracle that, when given any parser configuration, will return the set of transitions τ that do not increase the overall loss (i.e., `(τ(c))",3.1 Loss function,[0],[0]
"− `(c) = 0), leading the system through optimal configurations that minimize Hamming loss with respect to tG.
As suggested by (Coavoux and Crabbé, 2016; Fernández-González and Gómez-Rodrı́guez, 2018), constituent reachability can be used to efficiently compute the first term of the symmetric difference (|γG \ γ|), by simply counting the gold constituents that are individually unreachable from configuration c, as we describe in the next subsection.
",3.1 Loss function,[0],[0]
"The second and third terms of the loss (|γc \γG| and |αc \ αG|) can be trivially computed and are used to penalize false positives (extra erroneous constituents) so that final F-score is not harmed due to the decrease of precision, as pointed out by (Coavoux and Crabbé, 2016; Fernández-González and Gómez-Rodrı́guez, 2018).",3.1 Loss function,[0],[0]
"Note that it is crucial that the creation of non-gold Non-Terminal transitions is avoided, since these might not affect the creation of gold constituents, however, they will certainly lead the parser to the creation of extra erroneous constituents in future steps.
",3.1 Loss function,[0],[0]
"Finally, the function out of order of the last term can be implemented by computing the longest increasing subsequence of gold nonterminal nodes in the stack, where the order relation is given by the order of non-terminals (provided by their associated index) in the transition sequence that builds the gold tree (this order is unique, as none of our two parsers of interest have spurious ambiguity).",3.1 Loss function,[0],[0]
Obtaining the longest increasing subsequence is a well-known problem solvable in time O(n log n),3.1 Loss function,[0],[0]
"(Fredman, 1975), where n denotes the length of the input sequence.",3.1 Loss function,[0],[0]
"Once we have the largest possible sub-
sequence of gold non-terminal nodes in our configuration’s stack that is compatible with the gold order, the remaining ones give us the number of erroneous constituents that we will unavoidably generate, even in the best case, due to building them in an incorrect order.
",3.1 Loss function,[0],[0]
We will prove below that this loss formulation returns the exact loss and the resulting dynamic oracle is correct.,3.1 Loss function,[0],[0]
"We now show how the computation of the set of reachable constituents developed for bottomup parsing in (Coavoux and Crabbé, 2016; Fernández-González and Gómez-Rodrı́guez, 2018) can be extended to deal with the top-down and in-order strategies.
",3.2 Constituent reachability,[0],[0]
"Top-down transition system Let γG and αG be the set of gold constituents and the set of gold non-terminal nodes, respectively, for our current input.",3.2 Constituent reachability,[0],[0]
"We say that a gold constituent (X, l, r) ∈ γG is reachable from a con-
figuration c = 〈Σ, j, /, γc, αc〉 with Σ =",3.2 Constituent reachability,[0],[0]
"[(Yp, ip, ip−1) · · · (Y2, i2, i1)|(Y1, i1, j)], and it is included in the set of individually reachable constituentsR(c, γG), iff it satisfies one of the following conditions:4
(i) (X, l, r) ∈ γc",3.2 Constituent reachability,[0],[0]
"(i.e. it has already been created and, therefore, it is reachable by definition).",3.2 Constituent reachability,[0],[0]
(ii) j ≤,3.2 Constituent reachability,[0],[0]
l < r,3.2 Constituent reachability,[0],[0]
"∧ (X, l) /∈",3.2 Constituent reachability,[0],[0]
αc,3.2 Constituent reachability,[0],[0]
"(i.e. the words dominated by the gold constituent are still in the buffer and the non-terminal node that begins its creation has not been added to the stack yet; therefore, it can be still created after pushing the correct non-terminal node and shifting the necessary words).",3.2 Constituent reachability,[0],[0]
(iii) l ∈ {ik | 1 ≤ k ≤ p} ∧,3.2 Constituent reachability,[0],[0]
"j ≤ r ∧ (X, l) ∈ αc",3.2 Constituent reachability,[0],[0]
"(i.e. its span is partially or completely in the stack and the corresponding non-terminal node was already added to the stack, then, by shifting more words or/and reducing, the constituent can still be created).
",3.2 Constituent reachability,[0],[0]
"In-order transition system Let γG and αG be the set of gold constituents and the set of gold non-terminal nodes, respectively, for our current input.",3.2 Constituent reachability,[0],[0]
"We say that a gold constituent (X, l, r) ∈ γG is reachable from a configuration c = 〈Σ, j, false, γc, αc〉 with Σ =",3.2 Constituent reachability,[0],[0]
"[(Yp, ip, ip−1) · · · (Y2, i2, i1)|(Y1, i1, j)], and it is included in the set of individually reachable constituentsR(c, γG), iff it satisfies one of the following conditions:
(i) (X, l, r) ∈ γc",3.2 Constituent reachability,[0],[0]
(i.e. it has already been created).,3.2 Constituent reachability,[0],[0]
(ii) j ≤,3.2 Constituent reachability,[0],[0]
"l < r (i.e. the constituent is entirely in the buffer, then it can be still built).",3.2 Constituent reachability,[0],[0]
(iii) l ∈ {ik | 1 ≤ k ≤ p} ∧,3.2 Constituent reachability,[0],[0]
"j ≤ r ∧ (X,m) /∈",3.2 Constituent reachability,[0],[0]
αc,3.2 Constituent reachability,[0],[0]
"(i.e. its first child is still a totally- or partiallybuilt constituent on top of the stack and the non-terminal node has not been created yet;
4Please note that elements from the stack can be an already-built constituent, a shifted word or a non-terminal node.",3.2 Constituent reachability,[0],[0]
"Therefore, (Yp, ip, ip−1), (Y2, i2, i1) and (Y1, i1, j) should be represented as (Yp, ip−1), (Y2, i1) and (Y1, j), respectively, when they are non-terminal nodes.",3.2 Constituent reachability,[0],[0]
"We omit this for simplicity.
",3.2 Constituent reachability,[0],[0]
"therefore, it has to wait till the first child is completed (if it is still pending) and, then, it can be still created by pushing onto the stack the correct non-terminal node and shifting more words if necessary).
(iv) l ∈ {ik | 1 ≤ k ≤ p} ∧",3.2 Constituent reachability,[0],[0]
"j ≤ r ∧ (X,m) ∈ αc ∧ ∃(Y, l,m) ∈ Σ",3.2 Constituent reachability,[0],[0]
"(i.e. its span is partially or completely in the stack, and its first child (which is an alredy-built constituent) and the non-terminal node assigned are adjacent, thus, by shifting more words or/and reducing, the constituent can still be built).",3.2 Constituent reachability,[0],[0]
"In both transition systems, the set of individually unreachable constituents U(c, γG) with respect to the set of gold constituents γG can be easily computed as γG \ R(c, γG) and will contain the gold constituents that can no longer be built.",3.2 Constituent reachability,[0],[0]
"We will now prove that the above expression of `(c) indeed provides the minimum possible Hamming loss to the gold tree among all the trees that are reachable from configuration c. This implies correctness (or optimality) of our oracle.
",3.3 Correctness,[0],[0]
"To do so, we first show that both algorithms are constituent-decomposable.",3.3 Correctness,[0],[0]
"This amounts to saying that if we take a set of m constituents that are tree-compatible (can appear together in a constituent tree, meaning that no pair of constituent spans overlap unless one is a subset of the other) and individually reachable from a configuration c, then the set is also reachable as a whole.
",3.3 Correctness,[0],[0]
We prove this by induction on m. The base case (m = 1) is trivial.,3.3 Correctness,[0],[0]
Let us suppose that constituent-decomposability holds for any set of m tree-compatible constituents.,3.3 Correctness,[0],[0]
"We will show that it also holds for any set T ofm+1 tree-compatible constituents.
",3.3 Correctness,[0],[0]
"Let (X, l, r) be one of the constituents in T such that r = min{r′",3.3 Correctness,[0],[0]
"| (X ′, l′, r′) ∈ T} and l = max{l′ | (X ′, l′, r) ∈ T}.",3.3 Correctness,[0],[0]
Let T ′,3.3 Correctness,[0],[0]
"= T \ {(X, l, r)}.",3.3 Correctness,[0],[0]
"Since T ′ has m constituents, by induction hypothesis, T ′ is a reachable set from configuration c.
Since (X, l, r) is individually reachable by hypothesis, it must satisfy at least one of the conditions for constituent reachability.",3.3 Correctness,[0],[0]
"As these conditions are different for each particular algorithm, we continue the proof separately for each:
Top-down constituent-decomposability In this case, we enumerated three constituent reachability
conditions, so we divide the proof into three cases: If the first condition holds, then the constituent (X, l, r) has already been created in c.",3.3 Correctness,[0],[0]
"Thus, it will still be present after applying any of the possible transition sequences that build T ′ starting from c. Hence, T = T ′ ∪ {(X, l, r)} is reachable from c.
",3.3 Correctness,[0],[0]
"If the second condition holds, then j ≤",3.3 Correctness,[0],[0]
"l < r and the constituent (X, l, r) can be created by l−j Shift transitions, followed by one Non-Terminal transition, r",3.3 Correctness,[0],[0]
− l Shift transitions and one Reduce transition.,3.3 Correctness,[0],[0]
"This will leave the parser in a configuration whose value of j is r, and where stack elements with left span index ≤",3.3 Correctness,[0],[0]
l (apart from those referencing the new non-terminal and its leftmost child) have not changed.,3.3 Correctness,[0],[0]
"Thus, constituents of T ′ are still individually reachable in this configuration, as their left span index is either ≥ r (and then they meet the second reachability condition)",3.3 Correctness,[0],[0]
"or≤ l (and then they meet the third), so T is reachable from c.
Finally, if the third condition holds, then we can create (X, l, r) by applying r − j Shift transitions followed by a sequence of Reduce transitions stopping when we obtain (X, l, r) on the stack (this will always happen after a finite number of such transitions, as the reachability condition guarantees that l is the left span index of some constituent already on the stack, and that (X, l) is on the stack).",3.3 Correctness,[0],[0]
"Following the same reasoning as in the previous case regarding the resulting parser configuration, we conclude that T is reachable from c.
With this we have shown the induction step, and thus constituent decomposability for the top-down parser.
",3.3 Correctness,[0],[0]
In-order constituent decomposability The inorder parser has four constituent reachability conditions.,3.3 Correctness,[0],[0]
"Analogously to the previous case, we prove the reachability of T by case analysis.
",3.3 Correctness,[0],[0]
"If the first condition holds, then we have a situation where the constituent (X, l, r) has already been created in c, so reachability of T follows from the same reasoning as for the first condition in the top-down case.
",3.3 Correctness,[0],[0]
"If the second condition holds, we have j ≤",3.3 Correctness,[0],[0]
"l < r and the constituent (X, l, r) can be created by l − j + 1 Shift transitions (where the last one shifts a word that will be assigned as left child of the new constituent), followed by the relevant Non-Terminal-X transition, r",3.3 Correctness,[0],[0]
− l,3.3 Correctness,[0],[0]
− 1 more Shift transitions and one Reduce transition.,3.3 Correctness,[0],[0]
"After this,
the parser will be in a configuration where j takes the value r, where we can use the same reasoning as in the second condition of the top-down parser to show that all constituents of T ′ are still reachable, proving reachability of T .
",3.3 Correctness,[0],[0]
"For the third condition, the proof is analogous but the combination of transitions that creates the non-terminal starts with a sequence composed of Reduce transitions (when there is a non-terminal at the top of the stack) or Non-Terminal-Y transitions for arbitrary Y (when the top of the stack is a constituent) until the top node on the stack is a constituent with left span index",3.3 Correctness,[0],[0]
"l (this ensures that the constituent at the top of the stack can serve as leftmost child for our desired constituent), followed by a Non-Terminal-X, r−j Shift transitions and one Reduce transition.
",3.3 Correctness,[0],[0]
"Finally, for the fourth condition, the reasoning is again analogous, but the computation leading to the non-terminal starts with as many Reduce transitions as non-terminal nodes located above (X,m) in the stack (if any).",3.3 Correctness,[0],[0]
"If we call j the index associated to the resulting transition, then it only remains to apply r − j Shift transitions followed by a Reduce transition.
",3.3 Correctness,[0],[0]
"Optimality With this, we have shown constituent decomposability for both parsing algorithms.",3.3 Correctness,[0],[0]
"This means that, for a configuration c, and a set of constituents that are individually reachable from c, there is always some computation that can build them all.",3.3 Correctness,[0],[0]
"This facilitates the proof that the loss function is correct.
",3.3 Correctness,[0],[0]
"To finish the proof, we observe the following: • Let c′ be a final configuration reachable from c.",3.3 Correctness,[0],[0]
"The set (γc′ \ γG), representing erroneous constituents that have been built, will always contain at least |γc \ γG|, as the algorithm never deletes constituents.",3.3 Correctness,[0],[0]
"• In addition, c′ will contain one erroneous
constituent for each element of (αc \ αG), as once a non-terminal node is on the stack, there is no way to reach a final configuration without using it to create an erroneous constituent.",3.3 Correctness,[0],[0]
"Note that these erroneous constituents do not overlap those arising from the previous item, as γc stores already-built constituents and αc non-terminals that have still not been used to build a constituent.",3.3 Correctness,[0],[0]
•,3.3 Correctness,[0],[0]
"Given a subset S of R(c, γG), the previously
shown constituent decomposability property implies that there exists at least one transition
sequence starting from c that generates the tree S∪(γc\γG)∪E, whereE is a set of erroneous constituents containing one such constituent per element of (αc \ αG).",3.3 Correctness,[0],[0]
This tree has loss |tG|−(|γc∪S|)+|γc\γG|+|αc\αG|.,3.3 Correctness,[0],[0]
"The term |tG| − (|γc ∪ S|) corresponds to missed constituents (gold constituents that have not been already created and are not created as part of S), the other two to erroneous constituents.",3.3 Correctness,[0],[0]
"• As we have shown that the erroneous con-
stituents arising from (γc′ \γG) and (αc\αG) are unavoidable, computations yielding a tree with minimum loss are those that maximize |γc ∪ S| in the previous term.",3.3 Correctness,[0],[0]
"In general, the largest possible |S| is for S = R(c, γG).",3.3 Correctness,[0],[0]
"In that case, we would correctly generate every reachable constituent and the loss would be
`(c) = |U(c, γG)|+ |γc \ γG|
+|αc \",3.3 Correctness,[0],[0]
"αG|
However, we additionally want to generate constituents in the correct order, and this may not be possible if we have already shifted some of them into the stack in a wrong order.",3.3 Correctness,[0],[0]
The function out of order gives us the number of reachable constituents that are lost for this cause in the best case.,3.3 Correctness,[0],[0]
"Thus, indeed, the expression
`(c) = |U(c, γG)|+ |γc \ γG|
+|αc \ αG|+ out of order(αc, αG)
provides the minimum loss from configuration c.",3.3 Correctness,[0],[0]
"We test the two proposed approaches on two widely-used benchmarks for constituent parsers: the Wall Street Journal (WSJ) sections of the English Penn Treebank5 (Marcus et al., 1993) and version 5.1 of the Penn Chinese Treebank (CTB)6 (Xue et al., 2005).",4.1 Data,[0],[0]
"We use the same predicted POS tags and pre-trained word embeddings as Dyer et al. (2016) and Liu and Zhang (2017a).
",4.1 Data,[0],[0]
"5Sections 2-21 are used as training data, Section 22 for development and Section 23 for testing
6Articles 001- 270 and 440-1151 are taken for training, articles 301-325 for system development, and articles 271- 300 for final testing",4.1 Data,[0],[0]
"To perform a fair comparison, we define the novel dynamic oracles on the original implementations of the top-down parser by Dyer et al. (2016) and in-order parser by Liu and Zhang (2017a), where parsers are trained with a traditional static oracle.",4.2 Neural Model,[0],[0]
"Both implementations follow a stack-LSTM approach to represent the stack and the buffer, as well as a vanilla LSTM to represent the action history.",4.2 Neural Model,[0],[0]
"In addition, they also use a bi-LSTM as a compositional function for representing constituents in the stack.",4.2 Neural Model,[0],[0]
"Concretely, this consists in computing the composition representation scomp as:
scomp = (LSTMfwd[ent, s0, ..., sm];
LSTMbwd[ent, sm, ..., s0])
where ent is the vector representation of a nonterminal, and si, i ∈",4.2 Neural Model,[0],[0]
"[0,m] is the ith child node.
",4.2 Neural Model,[0],[0]
"Finally, the exact same word representation strategy and hyper-parameter values as (Dyer et al., 2016) and (Liu and Zhang, 2017a) are used to conduct the experiments.",4.2 Neural Model,[0],[0]
"In order to benefit from training a parser by a dynamic oracle, errors should be made during the training process so that the parser can learn to avoid and recover from them.",4.3 Error exploration,[0],[0]
"Unlike more complex error-exploration strategies as those studied in (Ballesteros et al., 2016; Cross and Huang, 2016b; Fried and Klein, 2018), we decided to consider a simple one that follows a non-optimal transition when it is the highest-scoring one, but with a certain probability.",4.3 Error exploration,[0],[0]
"In that way, we easily simulate test time conditions, when the parser greedily chooses the highest-scoring transition, even when it is not an optimal one, placing the parser in an incorrect state.
",4.3 Error exploration,[0],[0]
"In particular, we run experiments on development sets for each benchmark/algorithm with three different error exploration probabilities and choose the one that achieves the best F-score.",4.3 Error exploration,[0],[0]
"Table 1 reports all results, including those obtained by the top-down and in-order parsers trained by a dynamic oracle without error exploration (equivalent to a traditional static oracle).",4.3 Error exploration,[0],[0]
Table 2 compares our system’s accuracy to other state-of-the-art shift-reduce constituent parsers on the WSJ and CTB benchmarks.,4.4 Results,[0],[0]
"For comparison,
we also include some recent state-of-the-art parsers with global chart decoding that achieve the highest accuracies to date on WSJ, but are much slower than shift-reduce algorithms.
",4.4 Results,[0],[0]
Top-down and in-order parsers benefit from being trained by these new dynamic oracles in both datasets.,4.4 Results,[0],[0]
"The top-down strategy achieves a gain of 0.5 and 0.7 points in F-score on WSJ and CTB benchmarks, respectively.",4.4 Results,[0],[0]
"The in-order parser obtains similar improvements on the CTB (0.5 points), but less notable accuracy gain on the WSJ (0.2 points).",4.4 Results,[0],[0]
"Although a case of diminishing returns might explain the latter, the in-order parser trained with the proposed dynamic oracle still achieves the highest accuracy to date in greedy transition-based constituent parsing on the WSJ.7
While this work was under review, Fried and Klein (2018) proposed to train the top-down and in-order parsers with a policy gradient method instead of custom designed dynamic oracles.",4.4 Results,[0],[0]
"They also present a non-optimal dynamic oracle for the top-down parser that, combined with more complex error-exploration strategies and size-10 beam search, significantly outperforms the policy gradient-trained version, confirming that even non-optimal dynamic oracles are a good option.8",4.4 Results,[0],[0]
"Dan Bikel’s randomized parsing evaluation comparator (Bikel, 2004) was used to perform significance tests on precision and recall metrics on WSJ §23 and CTB §271-300.",4.5 Analysis,[0],[0]
"The top-down parser trained with dynamic oracles achieves statistically significant improvements (p < 0.05) in precision
7Note that the proposed dynamic oracles are orthogonal to approaches like beam search, re-ranking or semi-supervision, that can boost accuracy but at a large cost to parsing speed.
",4.5 Analysis,[0],[0]
"8Unfortunately, we cannot directly compare our approach to theirs, since they use beam-search decoding with size 10 in all experiments, gaining up to 0.3 points in F-score, while penalizing speed with respect to greedy decoding.",4.5 Analysis,[0],[0]
"However, by extrapolating the results above, we hypothesize that our optimal dynamic oracles (especially the one designed for the in-order algorithm) with their same training and beam-search decoding setup might achieve the best scores to date in shiftreduce parsing.
",4.5 Analysis,[0],[0]
"both on the WSJ and CTB benchmarks, and in recall on WSJ.",4.5 Analysis,[0],[0]
"The in-order parser trained with the proposed technique obtains significant improvements (p < 0.05) in recall in both benchmarks, although not in precision.
",4.5 Analysis,[0],[0]
We also undertake an analysis to check if dynamic oracles are able to mitigate error propagation.,4.5 Analysis,[0],[0]
We report in Table 3 the F-score obtained in constituents with different number of children on WSJ §23 by the top-down and in-order algorithms trained with both static and dynamic oracles.,4.5 Analysis,[0],[0]
"Please note that creating a constituent with a great number of children is more prone to suffer from error propagation, since a larger number of transitions is required to build it.",4.5 Analysis,[0],[0]
"The results seem to confirm that, indeed, dynamic oracles manage to alleviate error propagation, since improvements in F-score are more notable for larger constituents.",4.5 Analysis,[0],[0]
We develop the first optimal dynamic oracles for training the top-down and the state-of-the-art inorder parsers.,5 Conclusion,[0],[0]
"Apart from improving the systems’ accuracies in both cases, we achieve the best result to date in greedy shift-reduce parsing on the WSJ.",5 Conclusion,[0],[0]
"In addition, these promising techniques could easily benefit from recent studies in error-exploration strategies and yield stateof-the-art accuracies in transition-based parsing in the near future.",5 Conclusion,[0],[0]
The parser’s source code is freely available at https://github.com/ danifg/Dynamic-InOrderParser.,5 Conclusion,[0],[0]
"This work has received funding from the European Research Council (ERC), under the European Union’s Horizon 2020 research and innovation programme (FASTPARSE, grant agreement No 714150), from MINECO (FFI2014-51978-C2-2R, TIN2017-85160-C2-1-R) and from Xunta de Galicia (ED431B 2017/01).",Acknowledgments,[0],[0]
We introduce novel dynamic oracles for training two of the most accurate known shiftreduce algorithms for constituent parsing: the top-down and in-order transition-based parsers.,abstractText,[0],[0]
"In both cases, the dynamic oracles manage to notably increase their accuracy, in comparison to that obtained by performing classic static training.",abstractText,[0],[0]
"In addition, by improving the performance of the state-of-the-art in-order shift-reduce parser, we achieve the best accuracy to date (92.0 F1) obtained by a fullysupervised single-model greedy shift-reduce constituent parser on the WSJ benchmark.",abstractText,[0],[0]
Dynamic Oracles for Top-Down and In-Order Shift-Reduce Constituent Parsing,title,[0],[0]
"Online convex optimization is a powerful paradigm for sequential decision making (Zinkevich, 2003).",1. Introduction,[0],[0]
"It can be viewed as a game between a learner and an adversary: In the t-th round, the learner selects a decision wt ∈ Ω, simultaneously the adversary chooses a function ft(·) : Ω",1. Introduction,[0],[0]
"7→ R, and then the learner suffers an instantaneous loss ft(wt).",1. Introduction,[0],[0]
"This study focuses on the full-information setting, where the learner can query the value and gradient of ft (Cesa-Bianchi & Lugosi, 2006).",1. Introduction,[0],[0]
The goal of the learner is to minimize the cumulative loss over T periods .,1. Introduction,[0],[0]
"The standard performance measure is regret, which is the difference between the loss
1National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China 2Department of Computer Science, The University of Iowa, Iowa City, USA 3Alibaba Group, Seattle, USA.",1. Introduction,[0],[0]
"Correspondence to: Lijun Zhang <zhanglj@lamda.nju.edu.cn>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
incurred by the learner and that of the best fixed decision in hindsight, i.e.,
Regret(T ) =",1. Introduction,[0],[0]
"T∑ t=1 ft(wt)− min w∈Ω T∑ t=1 ft(w).
",1. Introduction,[0],[0]
The above regret is typically referred to as static regret in the sense that the comparator is time-invariant.,1. Introduction,[0],[0]
The rationale behind this evaluation metric is that one of the decision in Ω is reasonably good over the T rounds.,1. Introduction,[0],[0]
"However, when the underlying distribution of loss functions changes, the static regret may be too optimistic and fails to capture the hardness of the problem.
",1. Introduction,[0],[0]
"To address this limitation, new forms of performance measure, including adaptive regret (Hazan & Seshadhri, 2007; 2009) and dynamic regret (Zinkevich, 2003; Hall & Willett, 2013), were proposed and received significant interest recently.",1. Introduction,[0],[0]
"Following the terminology of Daniely et al. (2015), we define the strongly adaptive regret as the maximum static regret over intervals of length τ , i.e.,
SA-Regret(T, τ)
",1. Introduction,[0],[0]
"= max [s,s+τ−1]⊆[T ] ( s+τ−1∑ t=s ft(wt)− min w∈Ω s+τ−1∑ t=s ft(w) ) .
",1. Introduction,[0],[0]
"(1)
Minimizing the adaptive regret enforces the learner to have a small static regret over any interval of length τ .",1. Introduction,[0],[0]
"Since the best decision for different intervals could be different, the learner is essentially competing with a changing comparator.
",1. Introduction,[0],[0]
"A parallel line of research introduces the concept of dynamic regret, where the cumulative loss of the learner is compared against a comparator sequence u1, . . .",1. Introduction,[0],[0]
",uT ∈ Ω, i.e.,
D-Regret(u1, . . .",1. Introduction,[0],[0]
",uT ) =",1. Introduction,[0],[0]
T∑ t=1 ft(wt)− T∑ t=1 ft(ut).,1. Introduction,[0],[0]
"(2)
It is well-known that in the worst case, a sublinear dynamic regret is impossible unless we impose some regularities on the comparator sequence or the function sequence (Jadbabaie et al., 2015).",1. Introduction,[0],[0]
"A representative example is the functional variation defined below
VT = T∑ t=2 max w∈Ω |ft(w)− ft−1(w)|.",1. Introduction,[0],[0]
"(3)
Besbes et al. (2015) have proved that as long as VT is sublinear in T , there exists an algorithm that achieves a sublinear dynamic regret.",1. Introduction,[0],[0]
"Furthermore, a general restarting procedure is developed, and it enjoys O(T 2/3V 1/3T ) andO(log T √ TVT ) rates for convex functions and strongly convex functions, respectively.",1. Introduction,[0],[0]
"However, the restarting procedure can only be applied when an upper bound of VT is known beforehand, thus limiting its application in practice.
",1. Introduction,[0],[0]
"While both the adaptive and dynamic regrets aim at coping with changing environments, little is known about their relationship.",1. Introduction,[0],[0]
This paper makes a step towards understanding their connections.,1. Introduction,[0],[0]
"Specifically, we show that the strongly adaptive regret in (1), together with the functional variation, can be used to upper bound the dynamic regret in (2).",1. Introduction,[0],[0]
"Thus, an algorithm with a small strongly adaptive regret is automatically equipped with a tight dynamic regret.",1. Introduction,[0],[0]
"As a result, we obtain a series of algorithms for minimizing the dynamic regret that do not need any prior knowledge of the functional variation.",1. Introduction,[0],[0]
"The main contributions of this work are summarized below.
",1. Introduction,[0],[0]
• We provide a general theorem that upper bounds the dynamic regret in terms of the strongly adaptive regret and the functional variation.,1. Introduction,[0],[0]
•,1. Introduction,[0],[0]
"For convex functions, we show that the strongly adaptive algorithm of Jun et al. (2017) has a dynamic regret of O(T 2/3V 1/3T log
1/3 T ), which matches the minimax rate (Besbes et al., 2015), up to a polylogarithmic factor.",1. Introduction,[0],[0]
•,1. Introduction,[0],[0]
"For exponentially concave functions, we propose a strongly adaptive algorithm that allows us to control the tradeoff between the adaptive regret and the computational cost explicitly.",1. Introduction,[0],[0]
"Then, we demonstrate that its dynamic regret is O(d √ TVT log T ), where d is the
dimensionality.",1. Introduction,[0],[0]
"To the best of our knowledge, this is the first time that exponential concavity is utilized in the analysis of dynamic regret.",1. Introduction,[0],[0]
•,1. Introduction,[0],[0]
"For strongly convex functions, our proposed algorithm can also be applied and yields a dynamic regret of O( √ TVT log T ), which is also minimax optimal up to
a polylogarithmic factor.",1. Introduction,[0],[0]
"We give a brief introduction to previous work on static, adaptive, and dynamic regrets in the context of online convex optimization.",2. Related Work,[0],[0]
"The majority of studies in online learning are focused on static regret (Shalev-Shwartz & Singer, 2007; Langford et al., 2009; Shalev-Shwartz, 2011; Zhang et al., 2013).",2.1. Static Regret,[0],[0]
"For general convex functions, the classical online gradient
descent achieves O( √ T ) and O(log T ) regret bounds for convex and strongly convex functions, respectively (Zinkevich, 2003; Hazan et al., 2007; Shalev-Shwartz et al., 2007).",2.1. Static Regret,[0],[0]
"Both the O( √ T ) and O(log T ) rates are known to be minimax optimal (Abernethy et al., 2009).",2.1. Static Regret,[0],[0]
"When functions are exponentially concave, a different algorithm, named online Newton step, is developed and enjoys an O(d log T ) regret bound, where d is the dimensionality (Hazan et al., 2007).",2.1. Static Regret,[0],[0]
"The concept of adaptive regret is introduced by Hazan & Seshadhri (2007), and later strengthened by Daniely et al. (2015).",2.2. Adaptive Regret,[0],[0]
"Specifically, Hazan & Seshadhri (2007) introduce the weakly adaptive regret
WA-Regret(T )
",2.2. Adaptive Regret,[0],[0]
"= max [s,q]⊆[T ] ( q∑ t=s ft(wt)− min w∈Ω q∑ t=s ft(w) ) .
",2.2. Adaptive Regret,[0],[0]
"To minimize the adaptive regret, Hazan & Seshadhri (2007) have developed two meta-algorithms: an efficient algorithm with O(log T ) computational complexity per iteration and an inefficient one with O(T ) computational complexity per iteration.",2.2. Adaptive Regret,[0],[0]
"These meta-algorithms use an existing online method (that was possibly designed to have small static regret) as a subroutine.1 For convex functions, the efficient and inefficient meta-algorithms have O( √ T log3 T )
and O( √ T log T ) regret bounds, respectively.",2.2. Adaptive Regret,[0],[0]
"For exponentially concave functions, those rates are improved to O(d log2 T ) and O(d log T ), respectively.",2.2. Adaptive Regret,[0],[0]
"We can see that the price paid for the adaptivity is very small: The rates of weakly adaptive regret differ from those of static regret only by logarithmic factors.
",2.2. Adaptive Regret,[0],[0]
A major limitation of weakly adaptive regret is that it does not respect short intervals well.,2.2. Adaptive Regret,[0],[0]
"Taking convex functions as an example, the O( √ T log3 T ) and O( √ T log T ) bounds are meaningless for intervals of length O( √ T ).",2.2. Adaptive Regret,[0],[0]
"To overcome this limitation, Daniely et al. (2015) proposed the strongly adaptive regret SA-Regret(T, τ) which takes the length of the interval τ as a parameter, as indicated in (1).",2.2. Adaptive Regret,[0],[0]
"From the definitions, we have SA-Regret(T, τ) ≤ WA-Regret(T ), but it does not mean the notation of weakly adaptive regret is stronger, because an upper bound for WA-Regret(T ) could be very loose for SA-Regret(T, τ) when τ is small.
",2.2. Adaptive Regret,[0],[0]
"If the strongly adaptive regret is small for all τ < T , we can guarantee the learner has a small regret over any interval of
1For brevity, we ignored the factor of subroutine in the statements of computational complexities.",2.2. Adaptive Regret,[0],[0]
"The O(·) computational complexity should be interpreted as O(·) × s space complexity and O(·)× t time complexity, where s and t are space and time complexities of the subroutine per iteration, respectively.
any length.",2.2. Adaptive Regret,[0],[0]
"In particular, Daniely et al. (2015) introduced the following definition.
",2.2. Adaptive Regret,[0],[0]
Definition 1 Let R(τ) be the minimax static regret bound of the learning problem over τ periods.,2.2. Adaptive Regret,[0],[0]
"An algorithm is strongly adaptive, if
SA-Regret(T, τ) = O(poly(log T ) ·R(τ)), ∀τ.
",2.2. Adaptive Regret,[0],[0]
"It is easy to verify that the meta-algorithms of Hazan & Seshadhri (2007) are strongly adaptive for exponentially concave functions,2 but not for convex functions.",2.2. Adaptive Regret,[0],[0]
"Thus, Daniely et al. (2015) developed a new meta-algorithm that satisfies SA-Regret(T, τ) =",2.2. Adaptive Regret,[0],[0]
"O( √ τ log T ) for convex functions, and thus is strongly adaptive.",2.2. Adaptive Regret,[0],[0]
The algorithm is also efficient and the computational complexity per iteration is O(log T ).,2.2. Adaptive Regret,[0],[0]
"Later, the strongly adaptive regret of convex functions was improved to O( √ τ log T ) by Jun et al. (2017), and the computational complexity remains O(log T ) per iteration.",2.2. Adaptive Regret,[0],[0]
All the previously mentioned algorithms for minimizing adaptive regret need to query the gradient of the loss function at least O(log t) times in the t-th iteration.,2.2. Adaptive Regret,[0],[0]
"In a recent study, Wang et al. (2018) demonstrate that the number of gradient evaluations per iteration can be reduced to 1 by introducing the surrogate loss.",2.2. Adaptive Regret,[0],[0]
"In a seminal work, Zinkevich (2003) proposed to use the path-length defined as
P(u1, . . .",2.3. Dynamic Regret,[0],[0]
",uT ) =",2.3. Dynamic Regret,[0],[0]
T∑ t=2 ‖ut,2.3. Dynamic Regret,[0],[0]
"− ut−1‖2
to upper bound the dynamic regret, where u1, . . .",2.3. Dynamic Regret,[0],[0]
",uT ∈ Ω is a comparator sequence.",2.3. Dynamic Regret,[0],[0]
"Specifically, Zinkevich (2003) proved that for any sequence of convex functions, the dynamic regret of online gradient descent can be upper bounded by O( √ TP(u1, . . .",2.3. Dynamic Regret,[0],[0]
",uT )).",2.3. Dynamic Regret,[0],[0]
"Another regularity of the comparator sequence, which is similar to the path-length, is defined as
P ′(u1, . . .",2.3. Dynamic Regret,[0],[0]
",uT ) =",2.3. Dynamic Regret,[0],[0]
T∑ t=2 ‖ut,2.3. Dynamic Regret,[0],[0]
"− Φt(ut−1)‖2
where Φt(·) is a dynamic model that predicts a reference point for the t-th round.",2.3. Dynamic Regret,[0],[0]
"Hall & Willett (2013) developed a novel algorithm named dynamic mirror descent and proved that its dynamic regret is on the order of√ TP ′(u1, . . .",2.3. Dynamic Regret,[0],[0]
",uT ).",2.3. Dynamic Regret,[0],[0]
"The advantage of P ′(u1, . . .",2.3. Dynamic Regret,[0],[0]
",uT ) is that when the comparator sequence follows the dynamical
2That is because (i) SA-Regret(T, τ) ≤ WA-Regret(T ), and (ii) there is a poly(log T ) factor in the definition of strong adaptivity.
",2.3. Dynamic Regret,[0],[0]
"model closely, it can be much smaller than the path-length P(u1, . . .",2.3. Dynamic Regret,[0],[0]
",uT ).
",2.3. Dynamic Regret,[0],[0]
Let w∗t,2.3. Dynamic Regret,[0],[0]
∈ argminw∈Ω ft(w) be a minimizer of ft(·).,2.3. Dynamic Regret,[0],[0]
"For any sequence of u1, . . .",2.3. Dynamic Regret,[0],[0]
",uT ∈ Ω, we have
D-Regret(u1, . . .",2.3. Dynamic Regret,[0],[0]
",uT ) =",2.3. Dynamic Regret,[0],[0]
"T∑ t=1 ft(wt)− T∑ t=1 ft(ut)
",2.3. Dynamic Regret,[0],[0]
"≤D-Regret(w∗1, . .",2.3. Dynamic Regret,[0],[0]
.,2.3. Dynamic Regret,[0],[0]
",w∗T ) = T∑ t=1 ft(wt)− T∑ t=1",2.3. Dynamic Regret,[0],[0]
"min w∈Ω ft(w).
",2.3. Dynamic Regret,[0],[0]
"Thus, D-Regret(w∗1, . . .",2.3. Dynamic Regret,[0],[0]
",w ∗ T ) can be treated as the worst case of the dynamic regret, and there are many works that were devoted to minimizing D-Regret(w∗1, . . .",2.3. Dynamic Regret,[0],[0]
",w ∗ T ) (Jadbabaie et al., 2015; Mokhtari et al., 2016; Yang et al., 2016; Zhang et al., 2017).
",2.3. Dynamic Regret,[0],[0]
"When a prior knowledge of P(w∗1, . . .",2.3. Dynamic Regret,[0],[0]
",w∗T ) is available, D-Regret(w∗1, . . .",2.3. Dynamic Regret,[0],[0]
",w ∗ T ) can be upper bounded by
O( √ TP(w∗1, . . .",2.3. Dynamic Regret,[0],[0]
",w∗T ))",2.3. Dynamic Regret,[0],[0]
"(Yang et al., 2016).",2.3. Dynamic Regret,[0],[0]
"If all the functions are strongly convex and smooth, the upper bound can be improved to O(P(w∗1, . . .",2.3. Dynamic Regret,[0],[0]
",w∗T ))",2.3. Dynamic Regret,[0],[0]
"(Mokhtari et al., 2016).",2.3. Dynamic Regret,[0],[0]
"The O(P(w∗1, . . .",2.3. Dynamic Regret,[0],[0]
",w∗T )) rate is also achievable when all the functions are convex and smooth, and all the minimizers w∗t ’s lie in the interior of Ω (Yang et al., 2016).",2.3. Dynamic Regret,[0],[0]
"In a recent study, Zhang et al. (2017) introduced a new regularity—squared path-length
S(w∗1, . . .",2.3. Dynamic Regret,[0],[0]
",w∗T ) = T∑ t=2 ‖w∗t −w∗t−1‖22
which could be much smaller than the path-length P(w∗1, . . .",2.3. Dynamic Regret,[0],[0]
",w∗T ) when the difference between successive minimizers is small.",2.3. Dynamic Regret,[0],[0]
"Zhang et al. (2017) developed a novel algorithm named online multiple gradient descent, and proved that D-Regret(w∗1, . . .",2.3. Dynamic Regret,[0],[0]
",w ∗ T ) is on the order of min(P(w∗1, . . .",2.3. Dynamic Regret,[0],[0]
",w∗T ),S(w∗1, . . .",2.3. Dynamic Regret,[0],[0]
",w∗T ))",2.3. Dynamic Regret,[0],[0]
"for (semi-) strongly convex and smooth functions.
Discussions Although closely related, adaptive regret and dynamic regret are studied independently and there are few discussions of their relationships.",2.3. Dynamic Regret,[0],[0]
"In the literature, dynamic regret is also referred to as tracking regret or shifting regret (Littlestone & Warmuth, 1994; Herbster & Warmuth, 1998; 2001).",2.3. Dynamic Regret,[0],[0]
"In the setting of “prediction with expert advice”, Adamskiy et al. (2012) have shown that the tracking regret can be derived from the adaptive regret.",2.3. Dynamic Regret,[0],[0]
"In the setting of “online linear optimization in the simplex”, Cesa-bianchi et al. (2012) introduced a generalized notion of shifting regret which unifies adaptive regret and shifting regret.",2.3. Dynamic Regret,[0],[0]
"Different from previous work, this paper considers the setting of online convex optimization, and illustrates that the dynamic regret can be upper bounded by the adaptive regret and the functional variation.",2.3. Dynamic Regret,[0],[0]
"In this section, we introduce a unified approach for minimizing the adaptive regret of exponentially concave functions, as well as strongly convex functions.",3. A Unified Adaptive Algorithm,[0],[0]
We first provide the definition of exponentially concave (abbr.,3.1. Motivation,[0],[0]
"exp-concave) functions (Cesa-Bianchi & Lugosi, 2006).
",3.1. Motivation,[0],[0]
Definition 2,3.1. Motivation,[0],[0]
A function f(·) : Ω,3.1. Motivation,[0],[0]
"7→ R is α-exp-concave if exp(−αf(·)) is concave over Ω.
For exp-concave functions, Hazan & Seshadhri (2007) have developed two meta-algorithms that take the online Newton step as its subroutine, and proved the following properties.
",3.1. Motivation,[0],[0]
"• The inefficient one has O(T ) computational complexity per iteration, and its adaptive regret is O(d log T ).",3.1. Motivation,[0],[0]
•,3.1. Motivation,[0],[0]
"The efficient one hasO(log T ) computational complexity per iteration, and its adaptive regret is O(d log2 T ).
",3.1. Motivation,[0],[0]
"As can be seen, there is a tradeoff between the computational complexity and the adaptive regret: A lighter computation incurs a looser bound and a tighter bound requires a higher computation.",3.1. Motivation,[0],[0]
"Our goal is to develop a unified approach, that allows us to trade effectiveness for efficiency explicitly.",3.1. Motivation,[0],[0]
"Let E be an online learning algorithm that is designed to minimize the static regret of exp-concave functions or strongly convex functions, e.g., online Newton step (Hazan et al., 2007) or online gradient descent (Zinkevich, 2003).",3.2. Improved Following the Leading History (IFLH),[0],[0]
"Similar to the approach of following the leading history (FLH) (Hazan & Seshadhri, 2007), at any time t, we will instantiate an expert by applying the online learning algorithm E to the sequence of loss functions ft, ft+1, . . ., and utilize the strategy of learning from expert advice to combine solutions of different experts (Herbster & Warmuth, 1998).",3.2. Improved Following the Leading History (IFLH),[0],[0]
"Our method is named as improved following the leading history (IFLH), and is summarized in Algorithm 1.
",3.2. Improved Following the Leading History (IFLH),[0],[0]
"Let Et be the expert that starts to work at time t. To control the computational complexity, we will associate an ending time et for eachEt.",3.2. Improved Following the Leading History (IFLH),[0],[0]
"The expertEt is alive during the period [t, et − 1].",3.2. Improved Following the Leading History (IFLH),[0],[0]
"In each round t, we maintain a working set of experts St, which contains all the alive experts, and assign a probability pjt for each E
j ∈ St.",3.2. Improved Following the Leading History (IFLH),[0],[0]
"In Steps 6 and 7, we remove all the experts whose ending times are no larger than t. Since the number of alive experts has changed, we need to update the probability assigned to them, which is performed in Steps 12 to 14.",3.2. Improved Following the Leading History (IFLH),[0],[0]
"In Steps 15 and 16, we add a new expert Et to St, calculate its ending time according to Definition 3 introduced below, and set ptt = 1 t .",3.2. Improved Following the Leading History (IFLH),[0],[0]
"It is easy
Algorithm 1 Improved Following the Leading History (IFLH)
",3.2. Improved Following the Leading History (IFLH),[0],[0]
"1: Input: An integer K 2: Initialize S0 = ∅. 3: for t = 1, . . .",3.2. Improved Following the Leading History (IFLH),[0],[0]
", T do 4: Set Zt = 0 {Remove some existing experts} 5: for Ej ∈ St−1 do 6: if ej ≤ t then 7: Update St−1 ← St−1 \ {Ej} 8: else 9: Set Zt = Zt + p̂ j t
10: end if 11: end for {Normalize the probability} 12:",3.2. Improved Following the Leading History (IFLH),[0],[0]
"for Ej ∈ St−1 do 13: Set pjt = p̂jt Zt ( 1− 1t ) 14: end for {Add a new expert Et} 15: Set St = St−1 ∪ {Et} 16: Compute the ending time et = EK(t) according to
Definition 3 and set ptt = 1 t
{Compute the final predicted model} 17: Submit the solution
wt = ∑ Ej∈St pjtw j t
and suffer loss ft(wt) {Update weights and expert}
18: Set Zt+1 = 0 19: for Ej ∈ St do 20: Compute pjt+1 = p j t exp(−αft(w j t )) and Zt+1 =
Zt+1 + p j t+1
21: Pass the function ft(·) to Ej 22: end for 23: for Ej ∈ St do 24: Set p̂jt+1 = pjt+1 Zt+1 25: end for 26: end for
to verify ∑ Ej∈St p j t = 1.",3.2. Improved Following the Leading History (IFLH),[0],[0]
"Let w j t be the output of E
j at the t-th round, where t ≥ j. In Step 17, we submit the weighted average of wjt with coefficient p j t as the output wt, and suffer the loss ft(wt).",3.2. Improved Following the Leading History (IFLH),[0],[0]
"From Steps 18 to 25, we use the exponential weighting scheme to update the weight for each expert Ej based on its loss ft(w j t ).",3.2. Improved Following the Leading History (IFLH),[0],[0]
"In Step 21, we pass the loss function to all the alive experts such that they can update their predictions for the next round.
",3.2. Improved Following the Leading History (IFLH),[0],[0]
The difference between our IFLH and the original FLH is how to decide the ending time et of expert Et.,3.2. Improved Following the Leading History (IFLH),[0],[0]
"In this paper, we propose the following base-K ending time.
",3.2. Improved Following the Leading History (IFLH),[0],[0]
"Definition 3 (Base-K Ending Time) Let K be an integer, and the representation of t in the base-K number system as
t = ∑ τ≥0 βτK τ
where 0 ≤ βτ < K, for all τ ≥ 0.",3.2. Improved Following the Leading History (IFLH),[0],[0]
"Let k be the smallest integer such that βk > 0, i.e., k = min{τ : βτ > 0}.",3.2. Improved Following the Leading History (IFLH),[0],[0]
"Then, the base-K ending time of t is defined as
EK(t) =",3.2. Improved Following the Leading History (IFLH),[0],[0]
"∑
τ≥k+1
βτK τ +Kk+1.
",3.2. Improved Following the Leading History (IFLH),[0],[0]
"In other words, the ending time is the number represented by the new sequence obtained by setting the first nonzero element in the sequence β0, β1, . . .",3.2. Improved Following the Leading History (IFLH),[0],[0]
"to be 0 and adding 1 to the element after it.
",3.2. Improved Following the Leading History (IFLH),[0],[0]
"Let’s take the decimal system as an example (i.e., K = 10).",3.2. Improved Following the Leading History (IFLH),[0],[0]
"Then,
E10(1) = E10(2) = · · · = E10(9) = 10, E10(11) = E10(12)",3.2. Improved Following the Leading History (IFLH),[0],[0]
= · · ·,3.2. Improved Following the Leading History (IFLH),[0],[0]
"= E10(19) = 20, E10(10) = E10(20) = · · · = E10(90) = 100.",3.2. Improved Following the Leading History (IFLH),[0],[0]
"When the base-K ending time is used in Algorithm 1, we have the following properties.
",3.3. Theoretical Guarantees,[0],[0]
"Lemma 1 Suppose we use the base-K ending time in Algorithm 1.
1.",3.3. Theoretical Guarantees,[0],[0]
"For any t ≥ 1, we have |St| ≤ (blogK tc+ 1) (K − 1) =",3.3. Theoretical Guarantees,[0],[0]
"O ( K log t
logK
) .
",3.3. Theoretical Guarantees,[0],[0]
2.,3.3. Theoretical Guarantees,[0],[0]
For any interval I =,3.3. Theoretical Guarantees,[0],[0]
"[r, s] ⊆",3.3. Theoretical Guarantees,[0],[0]
"[T ], we can always find m segments",3.3. Theoretical Guarantees,[0],[0]
Ij =,3.3. Theoretical Guarantees,[0],[0]
"[tj , etj",3.3. Theoretical Guarantees,[0],[0]
"− 1], j ∈",3.3. Theoretical Guarantees,[0],[0]
"[m] with m ≤ dlogK(s− r + 1)e+ 1, such that t1 = r, etj = tj+1, j ∈",3.3. Theoretical Guarantees,[0],[0]
"[m− 1], and",3.3. Theoretical Guarantees,[0],[0]
"etm > s.
The first part of Lemma 1 implies that the size of St is O(K log t/ logK).",3.3. Theoretical Guarantees,[0],[0]
"An example of St in the decimal system is given below.
",3.3. Theoretical Guarantees,[0],[0]
"S486 =  481, 482, . . .",3.3. Theoretical Guarantees,[0],[0]
", 486, 410, 420, . . .",3.3. Theoretical Guarantees,[0],[0]
", 480,
100, 200, . . .",3.3. Theoretical Guarantees,[0],[0]
", 400  .",3.3. Theoretical Guarantees,[0],[0]
The second part of Lemma 1 implies that for any interval I =,3.3. Theoretical Guarantees,[0],[0]
"[r, s], we can find O(log s/ logK) experts such that their survival periods cover I .",3.3. Theoretical Guarantees,[0],[0]
"Again, we present an example in the decimal system: The interval [111, 832] can be covered by
[111, 119],",3.3. Theoretical Guarantees,[0],[0]
"[120, 199], and [200, 999]
which are the survival periods of experts E111, E120, and E200, respectively.",3.3. Theoretical Guarantees,[0],[0]
"Recall that E10(111) = 120, E10(120) = 200, and E10(200) = 1000.
",3.3. Theoretical Guarantees,[0],[0]
We note that a similar strategy for deciding the ending time was proposed by György et al. (2012) in the study of “prediction with expert advice”.,3.3. Theoretical Guarantees,[0],[0]
"The main difference is that their strategy is built upon base-2 number system and introduces an additional parameter g to compromise between the computational complexity and the regret, in contrast our method relies on base-K number system and uses K to control the tradeoff.",3.3. Theoretical Guarantees,[0],[0]
"Lemma 2 of György et al. (2012) indicates an O(g log t) bound on the number of alive experts, which is worse than our O(K log t/ logK) bound by a logarithmic factor.
",3.3. Theoretical Guarantees,[0],[0]
"To present adaptive regret bounds, we introduce the following common assumption.
",3.3. Theoretical Guarantees,[0],[0]
"Assumption 1 Both the gradient and the domain are bounded.
",3.3. Theoretical Guarantees,[0],[0]
"• The gradients of all the online functions are bounded by G, i.e., maxw∈Ω ‖∇ft(w)‖ ≤ G for all ft. •",3.3. Theoretical Guarantees,[0],[0]
"The diameter of the domain Ω is bounded by B, i.e., maxw,w′∈Ω ‖w",3.3. Theoretical Guarantees,[0],[0]
−w′‖ ≤,3.3. Theoretical Guarantees,[0],[0]
"B.
Based on Lemma 1, we have the following theorem regarding the adaptive regret of exp-concave functions.
",3.3. Theoretical Guarantees,[0],[0]
"Theorem 1 Suppose Assumption 1 holds, Ω ⊂ Rd, and all the functions are α-exp-concave.",3.3. Theoretical Guarantees,[0],[0]
"If online Newton step is used as the subroutine in Algorithm 1, we have
s∑ t=r ft(wt)− min w∈Ω s∑ t=r ft(w)
≤",3.3. Theoretical Guarantees,[0],[0]
"( (5d+ 1)m+ 2
α + 5dmGB
) log T
where [r, s]",3.3. Theoretical Guarantees,[0],[0]
⊆,3.3. Theoretical Guarantees,[0],[0]
[T ] and m ≤ dlogK(s− r + 1)e+ 1.,3.3. Theoretical Guarantees,[0],[0]
"Thus,
SA-Regret(T, τ) ≤ ( (5d+ 1)m̄+ 2
α + 5dm̄GB
) log T =",3.3. Theoretical Guarantees,[0],[0]
"O ( d log2 T
logK ) where m̄ = dlogK τe+ 1.
",3.3. Theoretical Guarantees,[0],[0]
"From Lemma 1 and Theorem 1, we observe that the adaptive regret is a decreasing function of K, while the computational cost is an increasing function of K.",3.3. Theoretical Guarantees,[0],[0]
"Thus, we can control the tradeoff by tuning the value of K. Specifically, Lemma 1 indicates the proposed algorithm has
(blogK T c+ 1) (K − 1) =",3.3. Theoretical Guarantees,[0],[0]
"O ( K log T
logK ) computational complexity per iteration.",3.3. Theoretical Guarantees,[0],[0]
"On the other hand, Theorem 1 implies that for α-exp-concave functions that
satisfy Assumption 1, the strongly adaptive regret of Algorithm 1 is(
(5d+ 1)m̄+ 2
α + 5dm̄GB
) log T = O ( d log2 T
logK ) where d is the dimensionality and m̄ = dlogK(τ)e+ 1.
",3.3. Theoretical Guarantees,[0],[0]
"We list several choices of K and the resulting theoretical guarantees in Table 1, and have the following observations.
",3.3. Theoretical Guarantees,[0],[0]
"• When K = 2, we recover the guarantee of the efficient algorithm of Hazan & Seshadhri (2007), and when K = T , we obtain the inefficient one.",3.3. Theoretical Guarantees,[0],[0]
•,3.3. Theoretical Guarantees,[0],[0]
"By setting K = dT 1/γe where γ > 1 is a small constant, such as 10, the strongly adaptive regret can be viewed as O(d log T ), and at the same time, the computational complexity is also very low for a large range of T .
",3.3. Theoretical Guarantees,[0],[0]
"Next, we consider strongly convex functions.
",3.3. Theoretical Guarantees,[0],[0]
Definition 4,3.3. Theoretical Guarantees,[0],[0]
A function f(·) :,3.3. Theoretical Guarantees,[0],[0]
"Ω 7→ R is λ-strongly convex if
f(y) ≥ f(x)+ 〈∇f(x),y − x〉+ λ 2 ‖y−x‖22, ∀x,y ∈",3.3. Theoretical Guarantees,[0],[0]
"Ω.
It is easy to verify that strongly convex functions with bounded gradients are also exp-concave (Hazan et al., 2007).
",3.3. Theoretical Guarantees,[0],[0]
Lemma 2 Suppose f(·) :,3.3. Theoretical Guarantees,[0],[0]
Ω 7→ R is λ-strongly convex and ‖∇f(w)‖ ≤ G for all w ∈,3.3. Theoretical Guarantees,[0],[0]
"Ω. Then, f(·) is λG2 -expconcave.
",3.3. Theoretical Guarantees,[0],[0]
"According to the above lemma, we still use Algorithm 1 as the meta-algorithm, but choose online gradient descent as the subroutine.",3.3. Theoretical Guarantees,[0],[0]
"In this way, the adaptive regret does not depend on the dimensionality d.
Theorem 2 Suppose Assumption 1 holds, and all the functions are λ-strongly convex.",3.3. Theoretical Guarantees,[0],[0]
"If online gradient descent is used as the subroutine in Algorithm 1, we have s∑ t=r ft(wt)− min w∈Ω s∑ t=r ft(w)",3.3. Theoretical Guarantees,[0],[0]
"≤ G2 2λ ( m+ (3m+ 4) log T )
",3.3. Theoretical Guarantees,[0],[0]
"where [r, s] ⊆",3.3. Theoretical Guarantees,[0],[0]
[T ] and m ≤ dlogK(s− r + 1)e+ 1.,3.3. Theoretical Guarantees,[0],[0]
"Thus
SA-Regret(T, τ)
≤G 2
2λ
( m̄+ (3m̄+ 4) log T ) =",3.3. Theoretical Guarantees,[0],[0]
"O
( log2 T
logK ) where m̄ = dlogK τe+ 1.",3.3. Theoretical Guarantees,[0],[0]
"In this section, we first introduce a general theorem that bounds the dynamic regret by the adaptive regret, and then derive specific regret bounds for convex functions, exponentially concave functions, and strongly convex functions.",4. From Adaptive to Dynamic,[0],[0]
Let I1 =,4.1. Adaptive-to-Dynamic Conversion,[0],[0]
"[s1, q1], I2 = [s2, q2], . . .",4.1. Adaptive-to-Dynamic Conversion,[0],[0]
", Ik =",4.1. Adaptive-to-Dynamic Conversion,[0],[0]
"[sk, qk] be a partition of [1, T ].",4.1. Adaptive-to-Dynamic Conversion,[0],[0]
"That is, they are successive intervals such that
s1 = 1, qi + 1 = si+1, i ∈",4.1. Adaptive-to-Dynamic Conversion,[0],[0]
"[k − 1], and qk = T. (4)
",4.1. Adaptive-to-Dynamic Conversion,[0],[0]
"Define the local functional variation of the i-th interval as
VT (i) =",4.1. Adaptive-to-Dynamic Conversion,[0],[0]
"qi∑ t=si+1 max w∈Ω |ft(w)− ft−1(w)|
and it is obvious that ∑k i=1",4.1. Adaptive-to-Dynamic Conversion,[0],[0]
VT (i) ≤ VT .3,4.1. Adaptive-to-Dynamic Conversion,[0],[0]
"Then, we have the following theorem for bounding the dynamic regret in terms of the strongly adaptive regret and the functional variation.
",4.1. Adaptive-to-Dynamic Conversion,[0],[0]
Theorem 3 Let w∗t,4.1. Adaptive-to-Dynamic Conversion,[0],[0]
∈,4.1. Adaptive-to-Dynamic Conversion,[0],[0]
argminw∈Ω ft(w).,4.1. Adaptive-to-Dynamic Conversion,[0],[0]
For all integer k ∈,4.1. Adaptive-to-Dynamic Conversion,[0],[0]
"[T ], we have
D-Regret(w∗1, . . .",4.1. Adaptive-to-Dynamic Conversion,[0],[0]
",w ∗ T )
≤ min I1,...,Ik k∑ i=1",4.1. Adaptive-to-Dynamic Conversion,[0],[0]
"( SA-Regret(T, |Ii|) +",4.1. Adaptive-to-Dynamic Conversion,[0],[0]
"2|Ii| · VT (i) ) where the minimization is taken over any sequence of intervals that satisfy (4).
",4.1. Adaptive-to-Dynamic Conversion,[0],[0]
"The above theorem is analogous to Proposition 2 of Besbes et al. (2015), which provides an upper bound for a special choice of the interval sequence.",4.1. Adaptive-to-Dynamic Conversion,[0],[0]
"The main difference is that there is a minimization operation in our bound, which allows us to get rid of the issue of parameter selection.",4.1. Adaptive-to-Dynamic Conversion,[0],[0]
"For a specific type of problems, we can plug in the corresponding
3Note that in certain cases, the sum of local functional variation ∑k i=1",4.1. Adaptive-to-Dynamic Conversion,[0],[0]
VT (i) can be much smaller than the total functional variation VT .,4.1. Adaptive-to-Dynamic Conversion,[0],[0]
"For example, when the sequence of functions only changes k times, we can construct the intervals based on the changing rounds such that ∑k i=1",4.1. Adaptive-to-Dynamic Conversion,[0],[0]
"VT (i) = 0.
upper bound of strongly adaptive regret, and then choose any sequence of intervals to obtain a concrete upper bound.",4.1. Adaptive-to-Dynamic Conversion,[0],[0]
"In particular, the choice of the intervals may depend on the (possibly unknown) functional variation.",4.1. Adaptive-to-Dynamic Conversion,[0],[0]
"For convex functions, we choose the meta-algorithm of Jun et al. (2017) and take the online gradient descent as its subroutine.",4.2. Convex Functions,[0],[0]
"The following theorem regarding the adaptive regret can be obtained from that paper.
",4.2. Convex Functions,[0],[0]
Theorem 4,4.2. Convex Functions,[0],[0]
"Under Assumption 1, the meta-algorithm of Jun et al. (2017) is strongly adaptive with
SA-Regret(T, τ) ≤ (
12BG√ 2− 1
+ 8 √ 7 log T + 5 )√ τ =",4.2. Convex Functions,[0],[0]
"O( √ τ log T ).
",4.2. Convex Functions,[0],[0]
"From Theorems 3 and 4, we derive the following bound for the dynamic regret.
",4.2. Convex Functions,[0],[0]
"Corollary 5 Under Assumption 1, the meta-algorithm of Jun et al. (2017) satisfies
D-Regret(w∗1, . . .",4.2. Convex Functions,[0],[0]
",w ∗ T )
≤max  (c+ 9 √ 7 log T + 5) √ T (c+ 8 √ 5)T 2/3V 1/3 T
log1/6 T + 24T 2/3V 1/3 T log 1/3 T
=O ( max {√
T log T , T 2/3V 1/3 T log
1/3 T })
where c = 12BG/( √ 2− 1).
",4.2. Convex Functions,[0],[0]
"According to Theorem 2 of Besbes et al. (2015), we know that the minimax dynamic regret of convex functions is O(T 2/3V
1/3 T ).",4.2. Convex Functions,[0],[0]
"Thus, our upper bound is minimax optimal
up to a polylogarithmic factor.",4.2. Convex Functions,[0],[0]
"Although the restarted online gradient descent of Besbes et al. (2015) achieves a dynamic regret of O(T 2/3V 1/3T ), it requires to know an upper bound of the functional variation VT .",4.2. Convex Functions,[0],[0]
"In contrast, the metaalgorithm of Jun et al. (2017) does not need any prior knowledge of VT .",4.2. Convex Functions,[0],[0]
"We note that the meta-algorithm of Daniely et al. (2015) can also be used here, and its dynamic regret is on the order of max {√ T log T, T 2/3V
1/3 T log
2/3 T } .",4.2. Convex Functions,[0],[0]
"We proceed to consider exp-concave functions, defined in Definition 2.",4.3. Exponentially Concave Functions,[0],[0]
Exponential concavity is stronger than convexity but weaker than strong convexity.,4.3. Exponentially Concave Functions,[0],[0]
"It can be used to model many popular losses used in machine learning, such as the square loss in regression, logistic loss in classification and negative logarithm loss in portfolio management (Koren, 2013).
",4.3. Exponentially Concave Functions,[0],[0]
"For exp-concave functions, we choose Algorithm 1 in this paper, and take the online Newton step as its subroutine.",4.3. Exponentially Concave Functions,[0],[0]
"Based on Theorems 1 and 3, we derive the dynamic regret of the proposed algorithm.
",4.3. Exponentially Concave Functions,[0],[0]
"Corollary 6 Let K = dT 1/γe, where γ > 1 is a small constant.",4.3. Exponentially Concave Functions,[0],[0]
"Suppose Assumption 1 holds, Ω ⊂ Rd, and all the functions are α-exp-concave.",4.3. Exponentially Concave Functions,[0],[0]
"Algorithm 1, with online Newton step as its subroutine, is strongly adaptive with
SA-Regret(T, τ) ≤",4.3. Exponentially Concave Functions,[0],[0]
"( (5d+ 1)(γ + 1) + 2
α + 5d(γ + 1)GB
) log T
=O (γd log T ) =",4.3. Exponentially Concave Functions,[0],[0]
"O (d log T )
and its dynamic regret satisfies
D-Regret(w∗1, . . .",4.3. Exponentially Concave Functions,[0],[0]
",w ∗ T ) ≤",4.3. Exponentially Concave Functions,[0],[0]
"( (5d+ 1)(γ + 1) + 2
α + 5d(γ + 1)GB + 2 ) ·max { log T, √ TVT log T
} =O ( d ·max { log T, √ TVT log T }) .
",4.3. Exponentially Concave Functions,[0],[0]
"To the best of our knowledge, this is the first dynamic regret that exploits exponential concavity.",4.3. Exponentially Concave Functions,[0],[0]
"Furthermore, according to the minimax dynamic regret of strongly convex functions (Besbes et al., 2015), our upper bound is minimax optimal, up to a polylogarithmic factor.",4.3. Exponentially Concave Functions,[0],[0]
"Finally, we study strongly convex functions.",4.4. Strongly Convex Functions,[0],[0]
"According to Lemma 2, we know that strongly convex functions with bounded gradients are also exp-concave.",4.4. Strongly Convex Functions,[0],[0]
"Thus, Corollary 6 can be directly applied to strongly convex functions, and yields a dynamic regret of O(d √ TVT log T ).",4.4. Strongly Convex Functions,[0],[0]
"However, the upper bound depends on the dimensionality d. To address this limitation, we use online gradient descent as the subroutine in Algorithm 1.
",4.4. Strongly Convex Functions,[0],[0]
"From Theorems 2 and 3, we have the following theorem, in which both the adaptive and dynamic regrets are independent from d.
Corollary 7 Let K = dT 1/γe, where γ > 1 is a small constant.",4.4. Strongly Convex Functions,[0],[0]
"Suppose Assumption 1 holds, and all the functions are λ-strongly convex.",4.4. Strongly Convex Functions,[0],[0]
"Algorithm 1, with online gradient descent as its subroutine, is strongly adaptive with
SA-Regret(T, τ)
≤G",4.4. Strongly Convex Functions,[0],[0]
"2
2λ
( γ + 1 + (3γ + 7) log T )",4.4. Strongly Convex Functions,[0],[0]
=O (γ log T ) =,4.4. Strongly Convex Functions,[0],[0]
"O (log T )
and its dynamic regret satisfies
D-Regret(w∗1, . . .",4.4. Strongly Convex Functions,[0],[0]
",w ∗ T )
≤max  ",4.4. Strongly Convex Functions,[0],[0]
"γG2 λ + ( 5γG2 λ + 2 ) log T γG2
λ √ TVT log T +",4.4. Strongly Convex Functions,[0],[0]
(,4.4. Strongly Convex Functions,[0],[0]
5γG2 λ + 2 ),4.4. Strongly Convex Functions,[0],[0]
"√ TVT log T
=O ( max { log T, √ TVT log T }) .
",4.4. Strongly Convex Functions,[0],[0]
"According to Theorem 4 of Besbes et al. (2015), the minimax dynamic regret of strongly convex functions is O( √ TVT ), which implies our upper bound is almost minimax optimal.",4.4. Strongly Convex Functions,[0],[0]
"By comparison, the restarted online gradient descent of Besbes et al. (2015) has a dynamic regret of O(log T √ TVT ), but it requires to know an upper bound of VT .",4.4. Strongly Convex Functions,[0],[0]
We here present the proof of Theorem 3.,5. Analysis,[0],[0]
The omitted proofs are provided in the supplementary.,5. Analysis,[0],[0]
"First, we upper bound the dynamic regret in the following way
D-Regret(w∗1, . . .",5.1. Proof of Theorem 3,[0],[0]
",w ∗ T )
",5.1. Proof of Theorem 3,[0],[0]
= k∑ i=1,5.1. Proof of Theorem 3,[0],[0]
"( qi∑ t=si ft(wt)− qi∑ t=si min w∈Ω ft(w) )
",5.1. Proof of Theorem 3,[0],[0]
= k∑ i=1  ,5.1. Proof of Theorem 3,[0],[0]
qi∑,5.1. Proof of Theorem 3,[0],[0]
t=si ft(wt)− min w∈Ω qi∑,5.1. Proof of Theorem 3,[0],[0]
"t=si
ft(w)︸ ︷︷ ︸ :=ai
+ min w∈Ω qi∑",5.1. Proof of Theorem 3,[0],[0]
t=si ft(w)− qi∑,5.1. Proof of Theorem 3,[0],[0]
"t=si min w∈Ω
ft(w)︸ ︷︷ ︸ :=bi
 .
(5)
",5.1. Proof of Theorem 3,[0],[0]
"From the definition of strongly adaptive regret, we can upper bound ai by
qi∑ t=si ft(wt)− min w∈Ω qi∑",5.1. Proof of Theorem 3,[0],[0]
t=si ft(w) ≤,5.1. Proof of Theorem 3,[0],[0]
"SA-Regret(T, |Ii|).
",5.1. Proof of Theorem 3,[0],[0]
"To upper bound bi, we follow the analysis of Proposition 2
of Besbes et al. (2015):
min w∈Ω qi∑",5.1. Proof of Theorem 3,[0],[0]
t=si ft(w)− qi∑,5.1. Proof of Theorem 3,[0],[0]
"t=si min w∈Ω ft(w)
",5.1. Proof of Theorem 3,[0],[0]
= min w∈Ω qi∑,5.1. Proof of Theorem 3,[0],[0]
t=si ft(w)− qi∑,5.1. Proof of Theorem 3,[0],[0]
"t=si ft(w ∗ t )
≤ qi∑",5.1. Proof of Theorem 3,[0],[0]
t=si ft(w ∗ si)− qi∑,5.1. Proof of Theorem 3,[0],[0]
"t=si ft(w ∗ t )
≤|Ii| · max t∈[si,qi]
( ft(w ∗ si)− ft(w ∗ t ) ) .
",5.1. Proof of Theorem 3,[0],[0]
"(6)
Furthermore, for any t ∈",5.1. Proof of Theorem 3,[0],[0]
"[si, qi], we have
ft(w ∗ si)− ft(w ∗ t )
=ft(w ∗ si)− fsi(w ∗ si) + fsi(w ∗ si)− ft(w ∗ t ) ≤ft(w∗si)− fsi(w ∗ si) + fsi(w ∗ t )",5.1. Proof of Theorem 3,[0],[0]
− ft(w∗t ) ≤2VT,5.1. Proof of Theorem 3,[0],[0]
"(i).
(7)
Combining (6) with (7), we have
min w∈Ω qi∑",5.1. Proof of Theorem 3,[0],[0]
t=si ft(w)− qi∑,5.1. Proof of Theorem 3,[0],[0]
t=si min w∈Ω ft(w),5.1. Proof of Theorem 3,[0],[0]
"≤ 2|Ii| · VT (i).
",5.1. Proof of Theorem 3,[0],[0]
"Substituting the upper bounds of ai and bi into (5), we arrive at
D-Regret(w∗1, . . .",5.1. Proof of Theorem 3,[0],[0]
",w ∗ T )
≤ k∑ i=1",5.1. Proof of Theorem 3,[0],[0]
"(SA-Regret(T, |Ii|) + 2|Ii| · VT (i)) .
",5.1. Proof of Theorem 3,[0],[0]
"Since the above inequality holds for any partition of [1, T ], we can take minimization to get a tight bound.",5.1. Proof of Theorem 3,[0],[0]
"In this paper, we demonstrate that the dynamic regret can be upper bounded by the adaptive regret and the functional variation, which implies strongly adaptive algorithms are automatically equipped with tight dynamic regret bounds.",6. Conclusions and Future Work,[0],[0]
"As a result, we are able to derive dynamic regret bounds for convex functions, exp-concave functions, and strongly convex functions.",6. Conclusions and Future Work,[0],[0]
"Moreover, we provide a unified approach for minimizing the adaptive regret of exp-concave functions, as well as strongly convex functions.
",6. Conclusions and Future Work,[0],[0]
The adaptive-to-dynamic conversion leads to a series of dynamic regret bounds in terms of the functional variation.,6. Conclusions and Future Work,[0],[0]
"As we mentioned before, dynamic regret can also be upper bounded by other regularities such as the path-length.",6. Conclusions and Future Work,[0],[0]
It is interesting to investigate whether those kinds of upper bounds can also be established for strongly adaptive algorithms.,6. Conclusions and Future Work,[0],[0]
"This work was partially supported by the National Key R&D Program of China (2018YFB1004300), NSFC (61603177, 61333014), JiangsuSF (BK20160658), YESS (2017QNRC001), NSF (IIS-1545995), and the Collaborative Innovation Center of Novel Software Technology and Industrialization.",Acknowledgements,[0],[0]
"To cope with changing environments, recent developments in online learning have introduced the concepts of adaptive regret and dynamic regret independently.",abstractText,[0],[0]
"In this paper, we illustrate an intrinsic connection between these two concepts by showing that the dynamic regret can be expressed in terms of the adaptive regret and the functional variation.",abstractText,[0],[0]
This observation implies that strongly adaptive algorithms can be directly leveraged to minimize the dynamic regret.,abstractText,[0],[0]
"As a result, we present a series of strongly adaptive algorithms that have small dynamic regrets for convex functions, exponentially concave functions, and strongly convex functions, respectively.",abstractText,[0],[0]
"To the best of our knowledge, this is the first time that exponential concavity is utilized to upper bound the dynamic regret.",abstractText,[0],[0]
"Moreover, all of those adaptive algorithms do not need any prior knowledge of the functional variation, which is a significant advantage over previous specialized methods for minimizing dynamic regret.",abstractText,[0],[0]
Dynamic Regret of Strongly Adaptive Methods,title,[0],[0]
"Language evolves over time and words change their meaning due to cultural shifts, technological inventions, or political events.",1. Introduction,[0],[0]
We consider the problem of detecting shifts in the meaning and usage of words over a given time span based on text data.,1. Introduction,[0],[0]
"Capturing these semantic shifts requires a dynamic language model.
",1. Introduction,[0],[0]
"Word embeddings are a powerful tool for modeling semantic relations between individual words (Bengio et al., 2003; Mikolov et al., 2013a; Pennington et al., 2014; Mnih & Kavukcuoglu, 2013; Levy & Goldberg, 2014; Vilnis & McCallum, 2014; Rudolph et al., 2016).",1. Introduction,[0],[0]
"Word embed-
1Disney Research, 4720 Forbes Avenue, Pittsburgh, PA 15213, USA.",1. Introduction,[0],[0]
"Correspondence to: Robert Bamler <Robert.Bamler@disneyresearch.com>, Stephan Mandt <Stephan.Mandt@disneyresearch.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
dings model the distribution of words based on their surrounding words in a training corpus, and summarize these statistics in terms of low-dimensional vector representations.",1. Introduction,[0],[0]
"Geometric distances between word vectors reflect semantic similarity (Mikolov et al., 2013a) and difference vectors encode semantic and syntactic relations (Mikolov et al., 2013c), which shows that they are sensible representations of language.",1. Introduction,[0],[0]
"Pre-trained word embeddings are useful for various supervised tasks, including sentiment analysis (Socher et al., 2013b), semantic parsing (Socher et al., 2013a), and computer vision (Fu & Sigal, 2016).",1. Introduction,[0],[0]
"As unsupervised models, they have also been used for the exploration of word analogies and linguistics (Mikolov et al., 2013c).
",1. Introduction,[0],[0]
"Word embeddings are currently formulated as static models, which assumes that the meaning of any given word is the same across the entire text corpus.",1. Introduction,[0],[0]
"In this paper, we propose a generalization of word embeddings to sequential data, such as corpora of historic texts or streams of text in social media.
",1. Introduction,[0],[0]
"Current approaches to learning word embeddings in a dynamic context rely on grouping the data into time bins and training the embeddings separately on these bins (Kim et al., 2014; Kulkarni et al., 2015; Hamilton et al., 2016).",1. Introduction,[0],[0]
"This approach, however, raises three fundamental problems.",1. Introduction,[0],[0]
"First, since word embedding models are non-convex, training them twice on the same data will lead to different results.",1. Introduction,[0],[0]
"Thus, embedding vectors at successive times can only be approximately related to each other, and only if the embedding dimension is large (Hamilton et al., 2016).",1. Introduction,[0],[0]
"Second, dividing a corpus into separate time bins may lead to training sets that are too small to train a word embedding model.",1. Introduction,[0],[0]
"Hence, one runs the risk of overfitting to few data whenever the required temporal resolution is fine-grained, as we show in the experimental section.",1. Introduction,[0],[0]
"Third, due to the finite corpus size the learned word embedding vectors are subject to random noise.",1. Introduction,[0],[0]
"It is difficult to disambiguate this noise from systematic semantic drifts between subsequent times, in particular over short time spans, where we expect only minor semantic drift.
",1. Introduction,[0],[0]
"In this paper, we circumvent these problems by introducing a dynamic word embedding model.",1. Introduction,[0],[0]
"Our contributions are as follows:
ar X
iv :1
70 2.
",1. Introduction,[0],[0]
"08 35
9v 2
[ st
at .M
L ]
1 7
Ju l 2
01 7
•",1. Introduction,[0],[0]
We derive a probabilistic state space model where word and context embeddings evolve in time according to a diffusion process.,1. Introduction,[0],[0]
"It generalizes the skip-gram model (Mikolov et al., 2013b; Barkan, 2017) to a dynamic setup, which allows end-to-end training.",1. Introduction,[0],[0]
"This leads to continuous embedding trajectories, smoothes out noise in the word-context statistics, and allows us to share information across all times.
",1. Introduction,[0],[0]
"• We propose two scalable black-box variational inference algorithms (Ranganath et al., 2014; Rezende et al., 2014) for filtering and smoothing.",1. Introduction,[0],[0]
These algorithms find word embeddings that generalize better to held-out data.,1. Introduction,[0],[0]
"Our smoothing algorithm carries out efficient black-box variational inference for structured Gaussian variational distributions with tridiagonal precision matrices, and applies more broadly.
",1. Introduction,[0],[0]
• We analyze three massive text corpora that span over long periods of time.,1. Introduction,[0],[0]
Our approach allows us to automatically find the words whose meaning changes the most.,1. Introduction,[0],[0]
"It results in smooth word embedding trajectories and therefore allows us to measure and visualize the continuous dynamics of the entire embedding cloud as it deforms over time.
",1. Introduction,[0],[0]
Figure 1 exemplifies our method.,1. Introduction,[0],[0]
The plot shows a fit of our dynamic skip-gram model to Google books (we give details in section 5).,1. Introduction,[0],[0]
We show the ten words whose meaning changed most drastically in terms of cosine distance over the last 150 years.,1. Introduction,[0],[0]
"We thereby automatically discover words such as “computer” or “radio” whose meaning changed due to technological advances, but also words like
“peer” and “notably” whose semantic shift is less obvious.
",1. Introduction,[0],[0]
Our paper is structured as follows.,1. Introduction,[0],[0]
"In section 2 we discuss related work, and we introduce our model in section 3.",1. Introduction,[0],[0]
In section 4 we present two efficient variational inference algorithms for our dynamic model.,1. Introduction,[0],[0]
We show experimental results in section 5.,1. Introduction,[0],[0]
Section 6 summarizes our findings.,1. Introduction,[0],[0]
"Probabilistic models that have been extended to latent time series models are ubiquitous (Blei & Lafferty, 2006; Wang et al., 2008; Sahoo et al., 2012; Gultekin & Paisley, 2014; Charlin et al., 2015; Ranganath et al., 2015; Jerfel et al., 2017), but none of them relate to word embeddings.",2. Related Work,[0],[0]
"The closest of these models is the dynamic topic model (Blei & Lafferty, 2006; Wang et al., 2008), which learns the evolution of latent topics over time.",2. Related Work,[0],[0]
Topic models are based on bag-of-word representations and thus treat words as symbols without modelling their semantic relations.,2. Related Work,[0],[0]
"They therefore serve a different purpose.
",2. Related Work,[0],[0]
Mikolov et al. (2013a;b) proposed the skip-gram model with negative sampling (word2vec) as a scalable word embedding approach that relies on stochastic gradient descent.,2. Related Work,[0],[0]
"This approach has been formulated in a Bayesian setup (Barkan, 2017), which we discuss separately in section 3.1.",2. Related Work,[0],[0]
"These models, however, do not allow the word embedding vectors to change over time.
",2. Related Work,[0],[0]
"Several authors have analyzed different statistics of text data to analyze semantic changes of words over time (Mihalcea & Nastase, 2012; Sagi et al., 2011; Kim et al., 2014;
Kulkarni et al., 2015; Hamilton et al., 2016).",2. Related Work,[0],[0]
"None of them explicitly model a dynamical process; instead, they slice the data into different time bins, fit the model separately on each bin, and further analyze the embedding vectors in post-processing.",2. Related Work,[0],[0]
"By construction, these static models can therefore not share statistical strength across time.",2. Related Work,[0],[0]
"This limits the applicability of static models to very large corpora.
",2. Related Work,[0],[0]
Most related to our approach are methods based on word embeddings.,2. Related Work,[0],[0]
"Kim et al. (2014) fit word2vec separately on different time bins, where the word vectors obtained for the previous bin are used to initialize the algorithm for the next time bin.",2. Related Work,[0],[0]
"The bins have to be sufficiently large and the found trajectories are not as smooth as ours, as we demonstrate in this paper.",2. Related Work,[0],[0]
Hamilton et al. (2016) also trained word2vec separately on several large corpora from different decades.,2. Related Work,[0],[0]
"If the embedding dimension is large enough (and hence the optimization problem less non-convex), the authors argue that word embeddings at nearby times approximately differ by a global rotation in addition to a small semantic drift, and they approximately compute this rotation.",2. Related Work,[0],[0]
"As the latter does not exist in a strict sense, it is difficult to distinguish artifacts of the approximate rotation from a true semantic drift.",2. Related Work,[0],[0]
"As discussed in this paper, both variants result in trajectories which are noisier.1",2. Related Work,[0],[0]
"We propose the dynamic skip-gram model, a generalization of the skip-gram model (word2vec) (Mikolov et al., 2013b) to sequential text data.",3. Model,[0],[0]
"The model finds word embedding vectors that continuously drift over time, allowing to track changes in language and word usage over short and long periods of time.",3. Model,[0],[0]
"Dynamic skip-gram is a probabilistic model which combines a Bayesian version of the skip-gram model (Barkan, 2017) with a latent time series.",3. Model,[0],[0]
"It is jointly
1 Rudolph & Blei (2017) independently developed a similar model, using a different likelihood model.",3. Model,[0],[0]
"Their approach uses a non-Bayesian treatment of the latent embedding trajectories, which makes the approach less robust to noise when the data per time step is small.
",3. Model,[0],[0]
"trained end-to-end and scales to massive data by means of approximate Bayesian inference.
",3. Model,[0],[0]
"The observed data consist of sequences of words from a finite vocabulary of size L. In section 3.1, all sequences (sentences from books, articles, or tweets) are considered time-independent; in section 3.2 they will be associated with different time stamps.",3. Model,[0],[0]
The goal is to maximize the probability of every word that occurs in the data given its surrounding words within a so-called context window.,3. Model,[0],[0]
"As detailed below, the model learns two vectors ui, vi ∈ Rd for each word i in the vocabulary, where d is the embedding dimension.",3. Model,[0],[0]
We refer to ui as the word embedding vector and to vi as the context embedding vector.,3. Model,[0],[0]
"The Bayesian skip-gram model (Barkan, 2017) is a probabilistic version of word2vec (Mikolov et al., 2013b) and forms the basis of our approach.",3.1. Bayesian Skip-Gram Model,[0],[0]
The graphical model is shown in Figure 2a).,3.1. Bayesian Skip-Gram Model,[0],[0]
"For each pair of words i, j in the vocabulary, the model assigns probabilities that word i appears in the context of word j.",3.1. Bayesian Skip-Gram Model,[0],[0]
This probability is σ(u>i vj) with the sigmoid function σ(x) = 1/(1 + e−x).,3.1. Bayesian Skip-Gram Model,[0],[0]
"Let zij ∈ {0, 1} be an indicator variable that denotes a draw from that probability distribution, hence p(zij = 1) = σ(u>i vj).",3.1. Bayesian Skip-Gram Model,[0],[0]
"The generative model assumes that many word-word pairs (i, j) are uniformly drawn from the vocabulary and tested for being a word-context pair; hence a separate random indicator zij is associated with each drawn pair.
",3.1. Bayesian Skip-Gram Model,[0],[0]
"Focusing on words and their neighbors in a context window, we collect evidence of word-word pairs for which zij = 1.",3.1. Bayesian Skip-Gram Model,[0],[0]
These are called the positive examples.,3.1. Bayesian Skip-Gram Model,[0],[0]
"Denote n+ij the number of times that a word-context pair (i, j) is observed in the corpus.",3.1. Bayesian Skip-Gram Model,[0],[0]
"This is a sufficient statistic of the model, and its contribution to the likelihood is p(n+ij |ui, vj) = σ(u>i vj)n + ij .",3.1. Bayesian Skip-Gram Model,[0],[0]
"However, the generative process also assumes the possibility to reject word-word pairs if zij = 0.",3.1. Bayesian Skip-Gram Model,[0],[0]
"Thus, one needs to construct a fictitious second training set of rejected word-word pairs, called negative examples.",3.1. Bayesian Skip-Gram Model,[0],[0]
Let the corresponding counts be n−ij .,3.1. Bayesian Skip-Gram Model,[0],[0]
"The total likelihood of both positive and negative examples is then
p(n+, n−|U, V ) =",3.1. Bayesian Skip-Gram Model,[0],[0]
"L∏
i,j=1
σ(u>i vj) n+ijσ(−u>",3.1. Bayesian Skip-Gram Model,[0],[0]
i vj)n − ij .,3.1. Bayesian Skip-Gram Model,[0],[0]
"(1)
",3.1. Bayesian Skip-Gram Model,[0],[0]
Above we used the antisymmetry σ(−x) = 1 − σ(x).,3.1. Bayesian Skip-Gram Model,[0],[0]
"In our notation, dropping the subscript indices for n+ and n− denotes the entire L × L matrices, U = (u1, · · · , uL) ∈ Rd×L is the matrix of all word embedding vectors, and V is defined analogously for the context vectors.",3.1. Bayesian Skip-Gram Model,[0],[0]
"To construct negative examples, one typically chooses n−ij ∝ P (i)P (j)3/4 (Mikolov et al., 2013b), where P (i) is the
frequency of word i in the training corpus.",3.1. Bayesian Skip-Gram Model,[0],[0]
"Thus, n− is well-defined up to a constant factor which has to be tuned.
",3.1. Bayesian Skip-Gram Model,[0],[0]
"Defining n± = (n+, n−) the combination of both positive and negative examples, the resulting log likelihood is
log p(n±|U, V ) = L∑
i,j=1
( n+ij log σ(u",3.1. Bayesian Skip-Gram Model,[0],[0]
> i vj) + n,3.1. Bayesian Skip-Gram Model,[0],[0]
− ij log σ(−u>i vj) ) .,3.1. Bayesian Skip-Gram Model,[0],[0]
"(2)
This is exactly the objective of the (non-Bayesian) skipgram model, see (Mikolov et al., 2013b).",3.1. Bayesian Skip-Gram Model,[0],[0]
The count matrices n+,3.1. Bayesian Skip-Gram Model,[0],[0]
"and n− are either pre-computed for the entire corpus, or estimated based on stochastic subsamples from the data in a sequential way, as done by word2vec.",3.1. Bayesian Skip-Gram Model,[0],[0]
Barkan (2017) gives an approximate Bayesian treatment of the model with Gaussian priors on the embeddings.,3.1. Bayesian Skip-Gram Model,[0],[0]
"The key extension of our approach is to use a Kalman filter as a prior for the time-evolution of the latent embeddings (Welch & Bishop, 1995).",3.2. Dynamic Skip-Gram Model,[0],[0]
"This allows us to share information across all times while still allowing the embeddings to drift.
",3.2. Dynamic Skip-Gram Model,[0],[0]
Notation.,3.2. Dynamic Skip-Gram Model,[0],[0]
We consider a corpus of T documents which were written at time stamps τ1 < . . .,3.2. Dynamic Skip-Gram Model,[0],[0]
< τT .,3.2. Dynamic Skip-Gram Model,[0],[0]
"For each time step t ∈ {1, . . .",3.2. Dynamic Skip-Gram Model,[0],[0]
", T} the sufficient statistics of word-context pairs are encoded in the L×L matrices n+t , n−t of positive and negative counts with matrix elements n+ij,t and n − ij,t, respectively.",3.2. Dynamic Skip-Gram Model,[0],[0]
"Denote Ut = (u1,t, · · · , uL,t) ∈",3.2. Dynamic Skip-Gram Model,[0],[0]
"Rd×L the matrix of word embeddings at time t, and define Vt correspondingly for the context vectors.",3.2. Dynamic Skip-Gram Model,[0],[0]
"Let U, V ∈ RT×d×L denote the tensors of word and context embeddings across all times, respectively.
",3.2. Dynamic Skip-Gram Model,[0],[0]
Model.,3.2. Dynamic Skip-Gram Model,[0],[0]
The graphical model is shown in Figure 2b).,3.2. Dynamic Skip-Gram Model,[0],[0]
We consider a diffusion process of the embedding vectors over time.,3.2. Dynamic Skip-Gram Model,[0],[0]
"The variance σ2t of the transition kernel is
σ2t =",3.2. Dynamic Skip-Gram Model,[0],[0]
"D(τt+1 − τt), (3) whereD is a global diffusion constant and (τt+1−τt) is the time between subsequent observations (Welch & Bishop, 1995).",3.2. Dynamic Skip-Gram Model,[0],[0]
"At every time step t, we add an additional Gaussian prior with zero mean and variance σ20 which prevents the embedding vectors from growing very large, thus
p(Ut+1|Ut) ∝",3.2. Dynamic Skip-Gram Model,[0],[0]
"N (Ut, σ2t )N (0, σ20).",3.2. Dynamic Skip-Gram Model,[0],[0]
"(4) Computing the normalization, this results in
Ut+1|Ut ∼ N",3.2. Dynamic Skip-Gram Model,[0],[0]
"(
Ut 1 + σ2t /σ 2 0 , 1 σ−2t + σ −2 0",3.2. Dynamic Skip-Gram Model,[0],[0]
"I
) , (5)
Vt+1|Vt ∼ N (
Vt 1 + σ2t /σ 2 0 , 1 σ−2t + σ −2 0",3.2. Dynamic Skip-Gram Model,[0],[0]
"I
) .",3.2. Dynamic Skip-Gram Model,[0],[0]
"(6)
In practice, σ0 σt, so the damping to the origin is very weak.",3.2. Dynamic Skip-Gram Model,[0],[0]
"This is also called Ornstein-Uhlenbeck process (Uhlenbeck & Ornstein, 1930).",3.2. Dynamic Skip-Gram Model,[0],[0]
"We recover the Wiener process for σ0 → ∞, but σ0 < ∞ prevents the latent time series from diverging to infinity.",3.2. Dynamic Skip-Gram Model,[0],[0]
"At time index t = 1, we define p(U1|U0) ≡ N (0, σ20I) and do the same for V1.",3.2. Dynamic Skip-Gram Model,[0],[0]
"Our joint distribution factorizes as follows:
p(n±, U, V ) =",3.2. Dynamic Skip-Gram Model,[0],[0]
"T−1∏
t=0
p(Ut+1|Ut) p(Vt+1|Vt)
",3.2. Dynamic Skip-Gram Model,[0],[0]
"× T∏
t=1
L∏
i,j=1
p(n±ij,t|ui,t, vj,t) (7)
",3.2. Dynamic Skip-Gram Model,[0],[0]
The prior model enforces that the model learns embedding vectors which vary smoothly across time.,3.2. Dynamic Skip-Gram Model,[0],[0]
This allows to associate words unambiguously with each other and to detect semantic changes.,3.2. Dynamic Skip-Gram Model,[0],[0]
"The model efficiently shares information across the time domain, which allows to fit the model even in setups where the data at every given point in time are small, as long as the data in total are large.",3.2. Dynamic Skip-Gram Model,[0],[0]
We discuss two scalable approximate inference algorithms.,4. Inference,[0],[0]
Filtering uses only information from the past; it is required in streaming applications where the data are revealed to us sequentially.,4. Inference,[0],[0]
"Smoothing is the other inference method, which learns better embeddings but requires the full sequence of documents ahead of time.
",4. Inference,[0],[0]
"In Bayesian inference, we start by formulating a joint distribution (Eq. 7) over observations n± and parameters U and V , and we are interested in the posterior distribution over parameters conditioned on observations,
p(U, V |n±)",4. Inference,[0],[0]
"= p(n ±, U, V )∫
p(n±, U, V ) dUdV (8)
The problem is that the normalization is intractable.",4. Inference,[0],[0]
"In variational inference (VI) (Jordan et al., 1999; Blei et al., 2016) one sidesteps this problem and approximates the posterior with a simpler variational distribution qλ(U, V ) by minimizing the Kullback-Leibler (KL) divergence to the posterior.",4. Inference,[0],[0]
"Here, λ summarizes all parameters of the variational distribution, such as the means and variances of a Gaussian, see below.",4. Inference,[0],[0]
Minimizing the KL divergence is equivalent to optimizing the evidence lower bound (ELBO),4. Inference,[0],[0]
"(Blei et al., 2016),
L(λ) =",4. Inference,[0],[0]
"Eqλ [log p(n±, U, V )]−Eqλ",4. Inference,[0],[0]
"[log qλ(U, V )].",4. Inference,[0],[0]
"(9)
For a restricted class of models, the ELBO can be computed in closed-form (Hoffman et al., 2013).",4. Inference,[0],[0]
"Our model is
non-conjugate and requires instead black-box VI using the reparameterization trick (Rezende et al., 2014; Kingma & Welling, 2014).",4. Inference,[0],[0]
"In many applications such as streaming, the data arrive sequentially.",4.1. Skip-Gram Filtering,[0],[0]
"Thus, we can only condition our model on past and not on future observations.",4.1. Skip-Gram Filtering,[0],[0]
"We will first describe inference in such a (Kalman) filtering setup (Kalman et al., 1960; Welch & Bishop, 1995).
",4.1. Skip-Gram Filtering,[0],[0]
"In the filtering scenario, the inference algorithm iteratively updates the variational distribution q as evidence from each time step t becomes available.",4.1. Skip-Gram Filtering,[0],[0]
"We thereby use a variational distribution that factorizes across all times, q(U, V ) =∏T t=1 q(Ut, Vt) and we update the variational factor at a given time t based on the evidence at time t and the approximate posterior of the previous time step.",4.1. Skip-Gram Filtering,[0],[0]
"Furthermore, at every time t we use a fully-factorized distribution:
q(Ut, Vt) =
L∏
i=1
N (ui,t;µui,t,Σui,t)N (vi,t;µvi,t.Σvi,t),
The variational parameters are the means µui,t, µvi,t ∈ Rd and the covariance matrices Σui,t and Σvi,t, which we restrict to be diagonal (mean-field approximation).
",4.1. Skip-Gram Filtering,[0],[0]
"We now describe how we sequentially compute q(Ut, Vt) and use the result to proceed to the next time step.",4.1. Skip-Gram Filtering,[0],[0]
"As other Markovian dynamical systems, our model assumes the following recursion,
p(Ut, Vt|n±1:t) ∝ p(n±t |Ut, Vt) p(Ut, Vt|n±1:t−1).",4.1. Skip-Gram Filtering,[0],[0]
"(10)
Within our variational approximation, the ELBO (Eq. 9) therefore separates into a sum of T terms, L = ∑t Lt with
Lt = E[log p(n±t |Ut, Vt)]",4.1. Skip-Gram Filtering,[0],[0]
"+ E[log p(Ut, Vt|n±1:t−1)]",4.1. Skip-Gram Filtering,[0],[0]
"− E[log q(Ut, Vt)], (11)
where all expectations are taken under q(Ut, Vt).",4.1. Skip-Gram Filtering,[0],[0]
"We compute the entropy term −E[log q] in Eq. 11 analytically and estimate the gradient of the log likelihood by sampling from the variational distribution and using the reparameterization trick (Kingma & Welling, 2014; Salimans & Kingma, 2016).",4.1. Skip-Gram Filtering,[0],[0]
"However, the second term of Eq. 11, containing the prior at time t, is still intractable.",4.1. Skip-Gram Filtering,[0],[0]
"We approximate the prior as
p(Ut, Vt|n±1:t−1) ≡",4.1. Skip-Gram Filtering,[0],[0]
"Ep(Ut−1,Vt−1|n±1:t−1) [ p(Ut, Vt|Ut−1, Vt−1) ]
",4.1. Skip-Gram Filtering,[0],[0]
"≈ Eq(Ut−1,Vt−1) [ p(Ut, Vt|Ut−1, Vt−1) ] .",4.1. Skip-Gram Filtering,[0],[0]
"(12)
The remaining expectation involves only Gaussians and can be carried-out analytically.",4.1. Skip-Gram Filtering,[0],[0]
"The resulting approximate
prior is a fully factorized distribution p(Ut, Vt|n±1:t−1) ≈∏L i=1N (ui,t; µ̃ui,t, Σ̃ui,t)N (vi,t; µ̃vi,t, Σ̃vit) with
µ̃ui,t = Σ̃ui,t ( Σui,t−1 + σ 2 t",4.1. Skip-Gram Filtering,[0],[0]
"I )−1 µui,t−1; Σ̃ui,t =",4.1. Skip-Gram Filtering,[0],[0]
"[( Σui,t−1 + σ 2",4.1. Skip-Gram Filtering,[0],[0]
t,4.1. Skip-Gram Filtering,[0],[0]
I )−1,4.1. Skip-Gram Filtering,[0],[0]
+ (1/σ20)I ],4.1. Skip-Gram Filtering,[0],[0]
−1 .,4.1. Skip-Gram Filtering,[0],[0]
"(13)
",4.1. Skip-Gram Filtering,[0],[0]
"Analogous update equations hold for µ̃vi,t and Σ̃vi,t. Thus, the second contribution in Eq. 11 (the prior) yields a closedform expression.",4.1. Skip-Gram Filtering,[0],[0]
We can therefore compute its gradient.,4.1. Skip-Gram Filtering,[0],[0]
"In contrast to filtering, where inference is conditioned on past observations until a given time t, (Kalman) smoothing performs inference based on the entire sequence of observations n±1:T .",4.2. Skip-Gram Smoothing,[0],[0]
"This approach results in smoother trajectories and typically higher likelihoods than with filtering, because evidence is used from both future and past observations.
",4.2. Skip-Gram Smoothing,[0],[0]
"Besides the new inference scheme, we also use a different variational distribution.",4.2. Skip-Gram Smoothing,[0],[0]
"As the model is fitted jointly to all time steps, we are no longer restricted to a variational distribution that factorizes in time.",4.2. Skip-Gram Smoothing,[0],[0]
For simplicity we focus here on the variational distribution for the word embeddings U ; the context embeddings V are treated identically.,4.2. Skip-Gram Smoothing,[0],[0]
"We use a factorized distribution over both embedding space and vocabulary space,
q(U1:T ) =
L∏
i=1
d∏
k=1
q(uik,1:T ).",4.2. Skip-Gram Smoothing,[0],[0]
"(14)
In the time domain, our variational approximation is structured.",4.2. Skip-Gram Smoothing,[0],[0]
"To simplify the notation we now drop the indices for words i and embedding dimension k, hence we write q(u1:T ) for q(uik,1:T ) where we focus on a single factor.",4.2. Skip-Gram Smoothing,[0],[0]
"This factor is a multivariate Gaussian distribution in the time domain with tridiagonal precision matrix Λ,
q(u1:T )",4.2. Skip-Gram Smoothing,[0],[0]
"= N (µ,Λ−1) (15) Both the means µ = µ1:T and the entries of the tridiagonal precision matrix Λ ∈ RT×T are variational parameters.",4.2. Skip-Gram Smoothing,[0],[0]
"This gives our variational distribution the interpretation of a posterior of a Kalman filter (Blei & Lafferty, 2006), which captures correlations in time.
",4.2. Skip-Gram Smoothing,[0],[0]
"We fit the variational parameters by training the model jointly on all time steps, using black-box VI and the reparameterization trick.",4.2. Skip-Gram Smoothing,[0],[0]
"As the computational complexity of an update step scales as Θ(L2), we first pretrain the model by drawing minibatches of L′ <",4.2. Skip-Gram Smoothing,[0],[0]
"L random words and L′ random contexts from the vocabulary (Hoffman et al., 2013).",4.2. Skip-Gram Smoothing,[0],[0]
We then switch to the full batch to reduce the sampling noise.,4.2. Skip-Gram Smoothing,[0],[0]
"Since the variational distribution does not factorize in the time domain we always include all time steps {1, . . .",4.2. Skip-Gram Smoothing,[0],[0]
", T} in the minibatch.
",4.2. Skip-Gram Smoothing,[0],[0]
"We also derive an efficient algorithm that allows us to estimate the reparametrization gradient using Θ(T ) time and memory, while a naive implementation of black-box variational inference with our structured variational distribution would require Θ(T 2) of both resources.",4.2. Skip-Gram Smoothing,[0],[0]
"The main idea is to parametrize Λ = B>B in terms of its Cholesky decomposition B, which is bidiagonal (Kılıç & Stanica, 2013), and to express gradients of B−1 in terms of gradients of B. We use mirror ascent (Ben-Tal et al., 2001; Beck & Teboulle, 2003) to enforce positive definiteness of B. The algorithm is detailed in our supplementary material.",4.2. Skip-Gram Smoothing,[0],[0]
We evaluate our method on three time-stamped text corpora.,5. Experiments,[0],[0]
We demonstrate that our algorithms find smoother embedding trajectories than methods based on a static model.,5. Experiments,[0],[0]
This allows us to track semantic changes of individual words by following nearest-neighbor relations over time.,5. Experiments,[0],[0]
"In our quantitative analysis, we find higher predictive likelihoods on held-out data compared to our baselines.
",5. Experiments,[0],[0]
Algorithms and Baselines.,5. Experiments,[0],[0]
"We report results from our proposed algorithms from section 4 and compare against baselines from section 2:
• SGI denotes the non-Bayesian skip-gram model with independent random initializations of word vectors (Mikolov et al., 2013b).",5. Experiments,[0],[0]
We used our own implementation of the model by dropping the Kalman filtering prior and point-estimating embedding vectors.,5. Experiments,[0],[0]
"Word vectors at nearby times are made comparable by approximate orthogonal transformations, which corresponds to Hamilton et al. (2016).
",5. Experiments,[0],[0]
"• SGP denotes the same approach as above, but with word and context vectors being pre-initialized with the values from the previous year, as in Kim et al. (2014).
",5. Experiments,[0],[0]
• DSG-F: dynamic skip-gram filtering (proposed).,5. Experiments,[0],[0]
"• DSG-S: dynamic skip-gram smoothing (proposed).
",5. Experiments,[0],[0]
Data and preprocessing.,5. Experiments,[0],[0]
"Our three corpora exemplify opposite limits both in the covered time span and in the amount of text per time step.
1.",5. Experiments,[0],[0]
We used data from the Google books corpus2,5. Experiments,[0],[0]
"(Michel et al., 2011) from the last two centuries (T = 209).",5. Experiments,[0],[0]
This amounts to 5 million digitized books and approximately 1010 observed words.,5. Experiments,[0],[0]
"The corpus consists of n-gram tables with n ∈ {1, . . .",5. Experiments,[0],[0]
", 5}, annotated by year of publication.",5. Experiments,[0],[0]
"We considered the years from 1800 to
2http://storage.googleapis.com/books/ ngrams/books/datasetsv2.html
2008 (the latest available).",5. Experiments,[0],[0]
"In 1800, the size of the data is approximately∼ 7 ·107 words.",5. Experiments,[0],[0]
"We used the 5-gram counts, resulting in a context window size of 4.
2.",5. Experiments,[0],[0]
"We used the “State of the Union” (SoU) addresses of U.S. presidents, which spans more than two centuries, resulting in T = 230 different time steps and approximately 106 observed words.3",5. Experiments,[0],[0]
Some presidents gave both a written and an oral address; if these were less than a week apart we concatenated them and used the average date.,5. Experiments,[0],[0]
"We converted all words to lower case and constructed the positive sample counts n+ij using a context window size of 4.
3.",5. Experiments,[0],[0]
We used a Twitter corpus of news tweets for 21 randomly drawn dates from 2010 to 2016.,5. Experiments,[0],[0]
The median number of tweets per day is 722.,5. Experiments,[0],[0]
"We converted all tweets to lower case and used a context window size of 4, which we restricted to stay within single tweets.
Hyperparameters.",5. Experiments,[0],[0]
"The vocabulary for each corpus was constructed from the 10,000 most frequent words throughout the given time period.",5. Experiments,[0],[0]
"In the Google books corpus, the number of words per year grows by a factor of 200 from the year 1800 to 2008.",5. Experiments,[0],[0]
"To avoid that the vocabulary is dominated by modern words we normalized the word frequencies separately for each year before adding them up.
",5. Experiments,[0],[0]
"For the Google books corpus, we chose the embedding dimension d = 200, which was also used in Kim et al. (2014).",5. Experiments,[0],[0]
"We set d = 100 for SoU and Twitter, as d = 200 resulted in overfitting on these much smaller corpora.",5. Experiments,[0],[0]
The ratio η = ∑ ij n,5. Experiments,[0],[0]
"− ij,t/ ∑ ij n + ij,t of negative to positive wordcontext pairs was η = 1.",5. Experiments,[0],[0]
The precise construction of the matrices n±t is explained in the supplementary material.,5. Experiments,[0],[0]
"We used the global prior variance σ20 = 1 for all corpora and all algorithms, including the baselines.",5. Experiments,[0],[0]
The diffusion constant D controls the time scale on which information is shared between time steps.,5. Experiments,[0],[0]
The optimal value for D depends on the application.,5. Experiments,[0],[0]
"A single corpus may exhibit semantic shifts of words on different time scales, and the optimal choice for D depends on the time scale in which one is interested.",5. Experiments,[0],[0]
"We used D = 10−3 per year for Google books and SoU, and D = 1 per year for the Twitter corpus, which spans a much shorter time range.",5. Experiments,[0],[0]
"In the supplementary material, we provide details of the optimization procedure.
",5. Experiments,[0],[0]
Qualitative results.,5. Experiments,[0],[0]
We show that our approach results in smooth word embedding trajectories on all three corpora.,5. Experiments,[0],[0]
"We can automatically detect words that undergo significant semantic changes over time.
",5. Experiments,[0],[0]
"Figure 1 in the introduction shows a fit of the dynamic skip-gram filtering algorithm to the Google books corpus.
3http://www.presidency.ucsb.edu/sou.php
Here, we show the ten words whose word vectors change most drastically over the last 150 years in terms of cosine distance.",5. Experiments,[0],[0]
"Figure 3 visualizes word embedding clouds over four subsequent years of Google books, where we compare DSG-F against SGI.",5. Experiments,[0],[0]
"We mapped the normalized embedding vectors to two dimensions using dynamic t-SNE (Rauber et al., 2016) (see supplement for details).",5. Experiments,[0],[0]
Lines indicate shifts of word vectors relative to the preceding year.,5. Experiments,[0],[0]
"In our model only few words change their position in the embedding space rapidly, while embeddings using SGI show strong fluctuations, making the cloud’s motion hard to track.
",5. Experiments,[0],[0]
Figure 4 visualizes the smoothness of the trajectories directly in the embedding space (without the projection to two dimensions).,5. Experiments,[0],[0]
We consider differences between word vectors in the year 1998 and the subsequent 10 years.,5. Experiments,[0],[0]
"In more detail, we compute histograms of the Euclidean distances ||uit − ui,t+δ|| over the word indexes i, where δ = 1, . . .",5. Experiments,[0],[0]
", 10 (as discussed previously, SGI uses a global rotation to optimally align embeddings first).",5. Experiments,[0],[0]
"In our model, embedding vectors gradually move away from their original position as time progresses, indicating a directed motion.",5. Experiments,[0],[0]
"In contrast, both baseline models show only little directed motion after the first time step, suggesting that most temporal changes are due to finite-size fluctuations of n±ij,t. Initialization schemes alone, thus, seem to have a minor effect on smoothness.
",5. Experiments,[0],[0]
Our approach allows us to detect semantic shifts in the usage of specific words.,5. Experiments,[0],[0]
Figures 5 and 1 both show the cosine distance between a given word and its neighboring words (colored lines) as a function of time.,5. Experiments,[0],[0]
Figure 5 shows results on all three corpora and focuses on a comparison across methods.,5. Experiments,[0],[0]
"We see that DSG-S and DSG-F (both proposed)
result in trajectories which display less noise than the baselines SGP and SGI.",5. Experiments,[0],[0]
"The fact that the baselines predict zero cosine distance (no correlation) between the chosen word pairs on the SoU and Twitter corpora suggests that these corpora are too small to successfully fit these models, in contrast to our approach which shares information in the time domain.",5. Experiments,[0],[0]
"Note that as in dynamic topic models, skipgram smoothing (DSG-S) may diffuse information into the past (see ”presidential” to ”clinton-trump” in Fig. 5).
",5. Experiments,[0],[0]
Quantitative results.,5. Experiments,[0],[0]
We show that our approach generalizes better to unseen data.,5. Experiments,[0],[0]
"We thereby analyze held-out predictive likelihoods on word-context pairs at a given time t, where t is excluded from the training set,
1 |n±t | log p(n±t |Ũt, Ṽt).",5. Experiments,[0],[0]
"(16)
Above, |n±t | = ∑ i,j ( n+ij,t + n",5. Experiments,[0],[0]
"− ij,t ) denotes the total number of word-context pairs at time τt.",5. Experiments,[0],[0]
"Since inference is different in all approaches, the definitions of word and context embedding matrices Ũt and Ṽt in Eq. 16 have to be adjusted:
• For SGI and SGP, we did a chronological pass through the time sequence and used the embeddings Ũt = Ut−1 and Ṽt = Vt−1 from the previous time step to predict the statistics",5. Experiments,[0],[0]
"n±ij,t at time step t. • For DSG-F, we did the same pass to test n±ij,t.",5. Experiments,[0],[0]
"We thereby set Ũt and Ṽt to be the modes Ut−1, Vt−1 of the approximate posterior at the previous time step.
",5. Experiments,[0],[0]
"• For DSG-S, we held out 10%, 10% and 20% of the documents from the Google books, SoU, and Twitter corpora for testing, respectively.",5. Experiments,[0],[0]
"After training, we estimated the word (context) embeddings Ũt (Ṽt) in
Eq. 16 by linear interpolation between the values of Ut−1 (Vt−1) and Ut+1 (Vt+1) in the mode of the variational distribution, taking into account that the time stamps τt are in general not equally spaced.
",5. Experiments,[0],[0]
The predictive likelihoods as a function of time τt are shown in Figure 6.,5. Experiments,[0],[0]
"For the Google Books corpus (left panel in figure 6), the predictive log-likelihood grows over time with all four methods.",5. Experiments,[0],[0]
This must be an artifact of the corpus since SGI does not carry any information of the past.,5. Experiments,[0],[0]
A possible explanation is the growing number of words per year from 1800 to 2008 in the Google Books corpus.,5. Experiments,[0],[0]
"On all three corpora, differences between the two implementations of the static model (SGI and SGP) are small, which suggests that pre-initializing the embeddings with the previous result may improve their continuity but seems to have little impact on the predictive power.",5. Experiments,[0],[0]
"Log-likelihoods for the skip-gram filter (DSG-F) grow over the first few time steps as the filter sees more data, and then saturate.",5. Experiments,[0],[0]
"The improvement of our approach over the static model is particularly pronounced in the SoU and Twitter corpora, which are much smaller than the massive Google books corpus.",5. Experiments,[0],[0]
"There, sharing information between across time is crucial because there is little data at every time slice.",5. Experiments,[0],[0]
"Skip-gram smoothing outperforms skip-gram filtering as it shares in-
formation in both time directions and uses a more flexible variational distribution.",5. Experiments,[0],[0]
We presented the dynamic skip-gram model: a Bayesian probabilistic model that combines word2vec with a latent continuous time series.,6. Conclusions,[0],[0]
We showed experimentally that both dynamic skip-gram filtering (which conditions only on past observations) and dynamic skip-gram smoothing (which uses all data) lead to smoothly changing embedding vectors that are better at predicting word-context statistics at held-out time steps.,6. Conclusions,[0],[0]
"The benefits are most drastic when the data at individual time steps is small, such that fitting a static word embedding model is hard.",6. Conclusions,[0],[0]
"Our approach may be used as a data mining and anomaly detection tool when streaming text on social media, as well as a tool for historians and social scientists interested in the evolution of language.",6. Conclusions,[0],[0]
"We would like to thank Marius Kloft, Cheng Zhang, Andreas Lehrmann, Brian McWilliams, Romann Weber, Michael Clements, and Ari Pakman for valuable feedback.",Acknowledgements,[0],[0]
"To create the word-clouds in Figure 1 of the main text we mapped the fitted word embeddings from Rd to the twodimensional plane using dynamic t-SNE (Rauber et al., 2016).",1. Dimensionality Reduction in Figure 1,[0],[0]
Dynamic t-SNE is a non-parametric dimensionality reduction algorithm for sequential data.,1. Dimensionality Reduction in Figure 1,[0],[0]
The algorithm finds a projection to a lower dimension by solving a non-convex optimization problem that aims at preserving nearest-neighbor relations at each individual time step.,1. Dimensionality Reduction in Figure 1,[0],[0]
"In
1Disney Research, 4720 Forbes Avenue, Pittsburgh, PA 15213, USA.",1. Dimensionality Reduction in Figure 1,[0],[0]
"Correspondence to: Robert Bamler <Robert.Bamler@disneyresearch.com>, Stephan Mandt <Stephan.Mandt@disneyresearch.com>.
",1. Dimensionality Reduction in Figure 1,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Dimensionality Reduction in Figure 1,[0],[0]
"Copyright 2017 by the author(s).
",1. Dimensionality Reduction in Figure 1,[0],[0]
"addition, projections at neighboring time steps are aligned with each other by a quadratic penalty with prefactor λ ≥ 0 for sudden movements.
",1. Dimensionality Reduction in Figure 1,[0],[0]
"There is a trade-off between finding good local projections for each individual time step (λ → 0), and finding smooth projections (large λ).",1. Dimensionality Reduction in Figure 1,[0],[0]
"Since we want to analyze the smoothness of word embedding trajectories, we want to avoid bias towards smooth projections.",1. Dimensionality Reduction in Figure 1,[0],[0]
"Unfortunately, setting λ = 0 is not an option since, in this limit, the optimization problem is invariant under independent rotations at each time, rendering trajectories in the two-dimensional projection plane meaningless.",1. Dimensionality Reduction in Figure 1,[0],[0]
"To still avoid bias towards smooth projections, we anneal λ exponentially towards zero over the course of the optimization.",1. Dimensionality Reduction in Figure 1,[0],[0]
"We start the optimizer with λ = 0.01, and we reduce λ by 5% with each training step.",1. Dimensionality Reduction in Figure 1,[0],[0]
"We run 100 optimization steps in total, so that λ",1. Dimensionality Reduction in Figure 1,[0],[0]
≈ 6×10−6 at the end of the training procedure.,1. Dimensionality Reduction in Figure 1,[0],[0]
"We used the opensource implementation,1 set the target perplexities to 200, and used default values for all other parameters.",1. Dimensionality Reduction in Figure 1,[0],[0]
Table S1 lists the hyperparameters used in our experiments.,2. Hyperparemeters and Construction of n±1:T,[0],[0]
"For the Google books corpus, we used the same context window size cmax and embedding dimension d as in (Kim et al., 2014).",2. Hyperparemeters and Construction of n±1:T,[0],[0]
"We reduced d for the SoU and Twitter corpora to avoid overfitting to these much smaller data sets.
",2. Hyperparemeters and Construction of n±1:T,[0],[0]
"In constrast to word2vec, we construct our positive and negative count matrices",2. Hyperparemeters and Construction of n±1:T,[0],[0]
"n±ij,t deterministically in a preprocessing step.",2. Hyperparemeters and Construction of n±1:T,[0],[0]
"As detailed below, this is done such that it resembles as closely as possible the stochastic approach in word2vec (Mikolov et al., 2013).",2. Hyperparemeters and Construction of n±1:T,[0],[0]
"In every update step, word2vec stochastically samples a context window size uniformly in an interval [1, · · · , cmax], thus the context size fluctuates and nearby words appear more often in the same context than words that are far apart from each other in the sentence.",2. Hyperparemeters and Construction of n±1:T,[0],[0]
We follow a deterministic scheme that results in similar statistics.,2. Hyperparemeters and Construction of n±1:T,[0],[0]
"For each pair of words (w1, w2) in a given sentence, we increase the counts n+iw1 jw2 by max (0, 1− k/cmax), where 0 ≤ k ≤ cmax is the number of words that appear between w1 and w2, and iw1 and jw2 are the words’ unique indices in the vocabulary.
",2. Hyperparemeters and Construction of n±1:T,[0],[0]
"1https://github.com/paulorauber/thesne
ar X
iv :1
70 2.
08 35
9v 2
[ st
at .M
L ]
1 7
Ju l 2
01 7
Algorithm 1 Skip-gram filtering; see section 4 of the main text.
",2. Hyperparemeters and Construction of n±1:T,[0],[0]
Remark: All updates are analogous for word and context vectors; we drop their indices for simplicity.,2. Hyperparemeters and Construction of n±1:T,[0],[0]
"Input: number of time steps T , time stamps τ1:T , positive and negative examples n±1:T , hyperparameters.",2. Hyperparemeters and Construction of n±1:T,[0],[0]
Init.,2. Hyperparemeters and Construction of n±1:T,[0],[0]
"prior means µ̃ik,1 ← 0 and variances Σ̃i,1 =",2. Hyperparemeters and Construction of n±1:T,[0],[0]
"Id×d Init. variational means µik,1 ← 0 and var.",2. Hyperparemeters and Construction of n±1:T,[0],[0]
"Σi,1",2. Hyperparemeters and Construction of n±1:T,[0],[0]
= Id×d for t,2. Hyperparemeters and Construction of n±1:T,[0],[0]
"= 1 to T do
if t 6= 1",2. Hyperparemeters and Construction of n±1:T,[0],[0]
then Update approximate Gaussian prior with params.,2. Hyperparemeters and Construction of n±1:T,[0],[0]
"µ̃ik,t and Σ̃i,t using µik,t−1 and Σi,t−1, see Eq. 13.",2. Hyperparemeters and Construction of n±1:T,[0],[0]
end if Compute entropy Eq[log q(·)] analytically.,2. Hyperparemeters and Construction of n±1:T,[0],[0]
"Compute expected log Gaussian prior with parameters µ̃ik,t and Σ̃k,t analytically.",2. Hyperparemeters and Construction of n±1:T,[0],[0]
"Maximize Lt in Eq. 11, using black-box VI with the reparametrization trick.",2. Hyperparemeters and Construction of n±1:T,[0],[0]
"Obtain µik,t and Σi,t as outcome of the optimization.
end for
We also used a deterministic variant of word2vec to construct the negative count matrices n−t .",2. Hyperparemeters and Construction of n±1:T,[0],[0]
"In word2vec, η negative samples (i, j) are drawn for each positive sample (i, j′) by drawing η independent values for j from a distribution P ′t (j) defined below.",2. Hyperparemeters and Construction of n±1:T,[0],[0]
"We define n − ij,t such that it matches the expectation value of the number of times that word2vec would sample the negative word-context pair (i, j).",2. Hyperparemeters and Construction of n±1:T,[0],[0]
"Specifically, we define
Pt(i) =
∑L j=1 n
+ ij,t∑L
i′,j=1 n + i′j,t
, (S1)
P ′t (j) =
( Pt(j) )",2. Hyperparemeters and Construction of n±1:T,[0],[0]
γ ∑L j′=1 ( Pt(j′) ),2. Hyperparemeters and Construction of n±1:T,[0],[0]
"γ , (S2)
n−ij,t =
( L∑
i′,j′=1
n+i′j′,t ) ηPt(i)P ′",2. Hyperparemeters and Construction of n±1:T,[0],[0]
t (j).,2. Hyperparemeters and Construction of n±1:T,[0],[0]
"(S3)
We chose γ = 0.75 as proposed in (Mikolov et al., 2013), and we set η = 1.",2. Hyperparemeters and Construction of n±1:T,[0],[0]
"In practice, it is not necessary to explicitly construct the full matrices n−t , and it is more efficient to keep only the distributions Pt(i) and P ′t (j) in memory.",2. Hyperparemeters and Construction of n±1:T,[0],[0]
The skip-gram filtering algorithm is described in section 4 of the main text.,3. Skip-gram Filtering Algorithm,[0],[0]
We provide a formulation in pseudocode in Algorithm 1.,3. Skip-gram Filtering Algorithm,[0],[0]
"In this section, we give details for the skip-gram smoothing algorithm, see section 4 of the main text.",4. Skip-gram Smoothing Algorithm,[0],[0]
"A summary is
Algorithm 2 Skip-gram smoothing; see section 4.",4. Skip-gram Smoothing Algorithm,[0],[0]
"We drop indices i, j, and k for word, context, end embedding dimension, respectively, when they are clear from context.
",4. Skip-gram Smoothing Algorithm,[0],[0]
"Input: number of time steps T , time stamps τ1:T , wordcontext counts n+1",4. Skip-gram Smoothing Algorithm,[0],[0]
":T , hyperparameters in Table S1 Obtain n−t ∀t using Eqs.",4. Skip-gram Smoothing Algorithm,[0],[0]
"S1–S3 Initialize µu,1:T , µv,1:T ← 0",4. Skip-gram Smoothing Algorithm,[0],[0]
"Initialize νu,1:T , νv,1:T , ωu,1:T−1, and ωv,1:T−1 such
that B>u Bu = B > v",4. Skip-gram Smoothing Algorithm,[0],[0]
Bv = Π (see Eqs.,4. Skip-gram Smoothing Algorithm,[0],[0]
S5 and S11) for step = 1 to N ′tr do Draw I ⊂,4. Skip-gram Smoothing Algorithm,[0],[0]
{,4. Skip-gram Smoothing Algorithm,[0],[0]
"1, . . .",4. Skip-gram Smoothing Algorithm,[0],[0]
", L′} with |I| = L′ uniformly Draw J ⊂ {1, . . .",4. Skip-gram Smoothing Algorithm,[0],[0]
", L′} with |J | = L′ uniformly for all i ∈",4. Skip-gram Smoothing Algorithm,[0],[0]
"I do
Draw",4. Skip-gram Smoothing Algorithm,[0],[0]
"[s]ui,1:T ∼ N (0, I)",4. Skip-gram Smoothing Algorithm,[0],[0]
"Solve Bu,ixui,1:T = ui,1:T for xui,1:T
end for Obtain xvj,1:T by repeating last loop ∀j ∈ J Calculate gradient estimates of L for minibatch
(I,J ) using Eqs.",4. Skip-gram Smoothing Algorithm,[0],[0]
"S10, S14, and S15 Obtain update steps",4. Skip-gram Smoothing Algorithm,[0],[0]
"d[·] for all variational parameters
using Adam optimizer with parameters in Table S1 Update µu,1:T ← µu,1:T +d[µu,1:T ], and analogously
for µv,1:T , ωu,1:T−1, and ωv,1:T−1 Update νu,1:T and νv,1:T according to Eq.",4. Skip-gram Smoothing Algorithm,[0],[0]
"S18
end for Repeat above loop for Ntr more steps, this time without
minibatch sampling (i.e., setting L′ = L)
provided in Algorithm 2.
",4. Skip-gram Smoothing Algorithm,[0],[0]
Variational distribution.,4. Skip-gram Smoothing Algorithm,[0],[0]
"For now, we focus on the word embeddings, and we simplify the notation by dropping the indices for the vocabulary and embedding dimensions.",4. Skip-gram Smoothing Algorithm,[0],[0]
"The variational distribution for a single embedding dimension of a single word embedding trajectory is
q(u1:T )",4. Skip-gram Smoothing Algorithm,[0],[0]
"= N (µu,1:T , (B>u Bu)−1).",4. Skip-gram Smoothing Algorithm,[0],[0]
"(S4)
Here, µu,1:T is the vector of mean values, and Bu is the Cholesky decomposition of the precision matrix.",4. Skip-gram Smoothing Algorithm,[0],[0]
"We restrict the latter to be bidiagonal,
Bu =   νu,1 ωu,1 νu,2 ωu,2 . . . . . .",4. Skip-gram Smoothing Algorithm,[0],[0]
"νu,T−1 ωu,T−1
νT
  (S5)
with νu,t > 0",4. Skip-gram Smoothing Algorithm,[0],[0]
"for all t ∈ {1, . . .",4. Skip-gram Smoothing Algorithm,[0],[0]
", T}.",4. Skip-gram Smoothing Algorithm,[0],[0]
"The variational parameters are µu,1:T , νu,1:T , and ω1:T−1.",4. Skip-gram Smoothing Algorithm,[0],[0]
"The variational distribution of the context embedding trajectories v1:T has the same structure.
",4. Skip-gram Smoothing Algorithm,[0],[0]
"With the above form of Bu, the variational distribution is a Gaussian with an arbitrary tridiagonal symmetric precision matrix B>u Bu.",4. Skip-gram Smoothing Algorithm,[0],[0]
"We chose this variational distribution because it is the exact posterior of a hidden time-series model with a Kalman filtering prior and Gaussian noise (Blei & Lafferty, 2006).",4. Skip-gram Smoothing Algorithm,[0],[0]
"Note that our variational distribution is a generalization of a fully factorized (mean-field) distribution, which is obtained for ωu,t = 0 ∀t.",4. Skip-gram Smoothing Algorithm,[0],[0]
"In the general case, ωu,t 6= 0, the variational distribution can capture correlations between all time steps, with a dense covariance matrix (B>u Bu) −1.
",4. Skip-gram Smoothing Algorithm,[0],[0]
Gradient estimation.,4. Skip-gram Smoothing Algorithm,[0],[0]
"The skip-gram smoothing algorithm uses stochastic gradient ascent to find the variational parameters that maximize the ELBO,
L = Eq [ log p(U1:T , V1:T , n ± 1:T ) ]",4. Skip-gram Smoothing Algorithm,[0],[0]
"− Eq [ log q(U1:T , V1:T ) ] .
(S6)
Here, the second term is the entropy, which can be evaluated analytically.",4. Skip-gram Smoothing Algorithm,[0],[0]
"We obtain for each component in vocabulary and embedding space,
−Eq[log q(u1:T )]",4. Skip-gram Smoothing Algorithm,[0],[0]
"= − ∑
t
log(νu,t) + const.",4. Skip-gram Smoothing Algorithm,[0],[0]
"(S7)
and analogously for −Eq[log q(v1:T )].",4. Skip-gram Smoothing Algorithm,[0],[0]
The first term on the right-hand side of Eq.,4. Skip-gram Smoothing Algorithm,[0],[0]
S6 cannot be evaluated analytically.,4. Skip-gram Smoothing Algorithm,[0],[0]
"We approximate its gradient by sampling from q using the reparameterization trick (Kingma & Welling, 2014; Rezende et al., 2014).",4. Skip-gram Smoothing Algorithm,[0],[0]
"A naive calculation would require Ω(T 2) computing time since the derivatives of L with respect to νu,t and ωu,t for each t depend on the count matrices n±t′ of all t
′. However, as we show next, there is a more efficient way to obtain all gradient estimates in Θ(T ) time.
",4. Skip-gram Smoothing Algorithm,[0],[0]
"We focus again on a single dimension of a single word embedding trajectory u1:T , and we drop the indices",4. Skip-gram Smoothing Algorithm,[0],[0]
i and k.,4. Skip-gram Smoothing Algorithm,[0],[0]
"We draw S independent samples u[s]1:T with s ∈ {1, . . .",4. Skip-gram Smoothing Algorithm,[0],[0]
", S} from q(u1:T ) by parameterizing
u [s] 1:T = µu,1:T + x",4. Skip-gram Smoothing Algorithm,[0],[0]
"[s] u,1:T (S8)
with
x",4. Skip-gram Smoothing Algorithm,[0],[0]
"[s] u,1:T = B −1",4. Skip-gram Smoothing Algorithm,[0],[0]
u,4. Skip-gram Smoothing Algorithm,[0],[0]
"[s] u,1:T",4. Skip-gram Smoothing Algorithm,[0],[0]
"where [s] u,1:T ∼ N (0, I).",4. Skip-gram Smoothing Algorithm,[0],[0]
"(S9)
We obtain x[s]u,1:T in Θ(T ) time by solving the bidiagonal linear system",4. Skip-gram Smoothing Algorithm,[0],[0]
"Bux [s] u,1:T =",4. Skip-gram Smoothing Algorithm,[0],[0]
"[s] u,1:T .",4. Skip-gram Smoothing Algorithm,[0],[0]
Samples v,4. Skip-gram Smoothing Algorithm,[0],[0]
[s] 1:T for the context embedding trajectories are obtained analogously.,4. Skip-gram Smoothing Algorithm,[0],[0]
"Our implementation uses S = 1, i.e., we draw only a single sample per training step.",4. Skip-gram Smoothing Algorithm,[0],[0]
"Averaging over several samples is done implicitly since we calculate the update steps
using the Adam optimizer (Kingma & Ba, 2014), which effectively averages over several gradient estimates in its first moment estimate.
",4. Skip-gram Smoothing Algorithm,[0],[0]
"The derivatives of L with respect to µu,1:T can be obtained using Eq. S8 and the chain rule.",4. Skip-gram Smoothing Algorithm,[0],[0]
"We find
∂L ∂µu,1:T ≈ 1 S
S∑
s=1
[ Γ [s] u,1:T −Πu [s] 1:T ] .",4. Skip-gram Smoothing Algorithm,[0],[0]
"(S10)
Here, Π ∈ RT×T is the precision matrix of the prior u1:T ∼ N (0,Π−1).",4. Skip-gram Smoothing Algorithm,[0],[0]
It is tridiagonal and therefore the matrix-multiplication Πu[s]1:T can be carried out efficiently.,4. Skip-gram Smoothing Algorithm,[0],[0]
"The non-zero matrix elements of Π are
Π11 = σ −2 0",4. Skip-gram Smoothing Algorithm,[0],[0]
+ σ −2,4. Skip-gram Smoothing Algorithm,[0],[0]
"1
ΠTT = σ −2 0",4. Skip-gram Smoothing Algorithm,[0],[0]
"+ σ −2 T−1
Πtt = σ −2 0",4. Skip-gram Smoothing Algorithm,[0],[0]
+ σ −2,4. Skip-gram Smoothing Algorithm,[0],[0]
t−1 + σ −2,4. Skip-gram Smoothing Algorithm,[0],[0]
"t ∀t ∈ {2, . . .",4. Skip-gram Smoothing Algorithm,[0],[0]
", T − 1}
Π1,t+1 = Πt+1,1 = −σ−2t .",4. Skip-gram Smoothing Algorithm,[0],[0]
"(S11)
",4. Skip-gram Smoothing Algorithm,[0],[0]
"The term Γ[s]u,1:T on the right-hand side of Eq.",4. Skip-gram Smoothing Algorithm,[0],[0]
S10 comes from the expectation value of the log-likelihood under q.,4. Skip-gram Smoothing Algorithm,[0],[0]
"It is given by
Γ",4. Skip-gram Smoothing Algorithm,[0],[0]
"[s] ui,t =
L∑
j=1
[( n+ij,t + n",4. Skip-gram Smoothing Algorithm,[0],[0]
"− ij,t ) σ",4. Skip-gram Smoothing Algorithm,[0],[0]
"( −u[s]>i,t v",4. Skip-gram Smoothing Algorithm,[0],[0]
"[s] j,t ) − n−ij,t ] v",4. Skip-gram Smoothing Algorithm,[0],[0]
"[s] j,t
(S12)
where we temporarily restored the indices i and j for words and contexts, respectively.",4. Skip-gram Smoothing Algorithm,[0],[0]
"In deriving Eq. S12, we used the relations ∂ log σ(x)/∂x = σ(−x) and σ(−x) = 1− σ(x).",4. Skip-gram Smoothing Algorithm,[0],[0]
"The derivatives of L with respect to νu,t and ωu,t are more intricate.",4. Skip-gram Smoothing Algorithm,[0],[0]
Using the parameterization in Eqs.,4. Skip-gram Smoothing Algorithm,[0],[0]
"S8–S9, the derivatives are functions of ∂B−1u /∂νt and ∂B −1",4. Skip-gram Smoothing Algorithm,[0],[0]
"u /∂ωt, respectively, where B−1u is a dense (upper triangular) T × T matrix.",4. Skip-gram Smoothing Algorithm,[0],[0]
"An efficient way to obtain these derivatives is to use the relation
∂B−1u ∂νt = −B−1u ∂Bu",4. Skip-gram Smoothing Algorithm,[0],[0]
"∂νt B−1u (S13)
and similarly for ∂B−1u /∂ωt.",4. Skip-gram Smoothing Algorithm,[0],[0]
Using this relation and Eqs.,4. Skip-gram Smoothing Algorithm,[0],[0]
"S8–S9, we obtain the gradient estimates
∂L ∂νu,t ≈ − 1 S
S∑
s=1
y",4. Skip-gram Smoothing Algorithm,[0],[0]
"[s] u,tx",4. Skip-gram Smoothing Algorithm,[0],[0]
"[s] u,t −
1
νu,t , (S14)
∂L ∂ωu,t ≈ − 1 S
S∑
s=1
y",4. Skip-gram Smoothing Algorithm,[0],[0]
"[s] u,tx",4. Skip-gram Smoothing Algorithm,[0],[0]
"[s] u,t+1.",4. Skip-gram Smoothing Algorithm,[0],[0]
"(S15)
",4. Skip-gram Smoothing Algorithm,[0],[0]
The second term on the right-hand side of Eq.,4. Skip-gram Smoothing Algorithm,[0],[0]
"S14 is the derivative of the entropy, Eq. S7, and
y",4. Skip-gram Smoothing Algorithm,[0],[0]
"[s] u,1:T = (B > u ) −1",4. Skip-gram Smoothing Algorithm,[0],[0]
"[ Γ [s] u,1:T −Πu",4. Skip-gram Smoothing Algorithm,[0],[0]
[s] 1:T ] .,4. Skip-gram Smoothing Algorithm,[0],[0]
"(S16)
",4. Skip-gram Smoothing Algorithm,[0],[0]
"The values y[s]u,1",4. Skip-gram Smoothing Algorithm,[0],[0]
":T can again be obtained in Θ(T ) time by bringing B>u to the left-hand side and solving the corresponding bidiagonal linear system of equations.
",4. Skip-gram Smoothing Algorithm,[0],[0]
Sampling in vocabulary space.,4. Skip-gram Smoothing Algorithm,[0],[0]
"In the above paragraph, we described an efficient strategy to obtain gradient estimates in only Θ(T ) time.",4. Skip-gram Smoothing Algorithm,[0],[0]
"However, the gradient estimation scales quadratic in the vocabulary size L because all L2 elements of the positive count matrices n+t contribute to the gradients.",4. Skip-gram Smoothing Algorithm,[0],[0]
"In order speed up the optimization, we pretrain the model using a minibatch of size",4. Skip-gram Smoothing Algorithm,[0],[0]
L′ <,4. Skip-gram Smoothing Algorithm,[0],[0]
L in vocabulary space as explained below.,4. Skip-gram Smoothing Algorithm,[0],[0]
The computational complexity of a single training step in this setup scales as (L′)2 rather than L2.,4. Skip-gram Smoothing Algorithm,[0],[0]
"After N ′tr = 5000 training steps with minibatch size L′, we switch to the full batch size of L and train the model for another Ntr = 1000 steps.
",4. Skip-gram Smoothing Algorithm,[0],[0]
The subsampling in vocabulary space works as follows.,4. Skip-gram Smoothing Algorithm,[0],[0]
"In each training step, we independently draw a set I of L′ random distinct words and a set J of L′ random distinct contexts from a uniform probability over the vocabulary.",4. Skip-gram Smoothing Algorithm,[0],[0]
We then estimate the gradients of L with respect to only the variational parameters that correspond to words i ∈,4. Skip-gram Smoothing Algorithm,[0],[0]
I and contexts j ∈ J .,4. Skip-gram Smoothing Algorithm,[0],[0]
This is possible because both the prior of our dynamic skip-gram model and the variational distribution factorize in the vocabulary space.,4. Skip-gram Smoothing Algorithm,[0],[0]
"The likelihood of the model, however, does not factorize.",4. Skip-gram Smoothing Algorithm,[0],[0]
"This affects only the definition of Γ[s]uik,t in Eq. S12.",4. Skip-gram Smoothing Algorithm,[0],[0]
"We replace Γ [s] uik,t by an estimate Γ[s]′uik,t based on only the contexts j ∈ J in the current minibatch,
Γ [s] ui,t =
L L′ ∑
j∈J
[ ( n+ij,t + n",4. Skip-gram Smoothing Algorithm,[0],[0]
"− ij,t ) σ",4. Skip-gram Smoothing Algorithm,[0],[0]
"( −u[s]>i,t v",4. Skip-gram Smoothing Algorithm,[0],[0]
"[s] j,t )
− n−ij,t ] v",4. Skip-gram Smoothing Algorithm,[0],[0]
"[s] j,t. (S17)
",4. Skip-gram Smoothing Algorithm,[0],[0]
"Here, the prefactor L/L′ restores the correct ratio between evidence and prior knowledge (Hoffman et al., 2013).
",4. Skip-gram Smoothing Algorithm,[0],[0]
Enforcing positive definiteness.,4. Skip-gram Smoothing Algorithm,[0],[0]
"We update the variational parameters using stochastic gradient ascent with the Adam optimizer (Kingma & Ba, 2014).",4. Skip-gram Smoothing Algorithm,[0],[0]
"The parameters νu,1:T are the eigenvalues of the matrix Bu, which is the Cholesky decomposition of the precision matrix of q. Therefore, νu,t has to be positive for all t ∈ {1, . . .",4. Skip-gram Smoothing Algorithm,[0],[0]
", T}.",4. Skip-gram Smoothing Algorithm,[0],[0]
"We use mirror ascent (Ben-Tal et al., 2001; Beck & Teboulle, 2003) to enforce νu,t > 0.",4. Skip-gram Smoothing Algorithm,[0],[0]
"Specifically, we update νt to a new value ν′t defined by
ν′u,t = 1
2 νu,td[νu,t] +
√( 1
2 νu,td[νu,t]
)2 + ν2u,t (S18)
where d[νu,t] is the step size obtained from the Adam optimizer.",4. Skip-gram Smoothing Algorithm,[0],[0]
Eq.,4. Skip-gram Smoothing Algorithm,[0],[0]
"S18 can be derived from the general mirror ascent update rule Φ′(ν′u,t) =",4. Skip-gram Smoothing Algorithm,[0],[0]
"Φ ′(νu,t) +",4. Skip-gram Smoothing Algorithm,[0],[0]
"d[νu,t] with the
mirror map Φ : x 7→",4. Skip-gram Smoothing Algorithm,[0],[0]
"−c1 log(x)+c2x2/2, where we set the parameters to c1 = νu,t and c2 = 1/νu,t for dimensional reasons.",4. Skip-gram Smoothing Algorithm,[0],[0]
The update step in Eq.,4. Skip-gram Smoothing Algorithm,[0],[0]
"S18 increases (decreases) νu,t for positive (negative) d[νu,t], while always keeping its value positive.
",4. Skip-gram Smoothing Algorithm,[0],[0]
Natural basis.,4. Skip-gram Smoothing Algorithm,[0],[0]
"As a final remark, let us discuss an optional extension to the skip-gram smoothing algorithm that converges in less training steps.",4. Skip-gram Smoothing Algorithm,[0],[0]
This extension only increases the efficiency of the algorithm.,4. Skip-gram Smoothing Algorithm,[0],[0]
It does not change the underlying model or the choice of variational distribution.,4. Skip-gram Smoothing Algorithm,[0],[0]
Observe that the prior of the dynamic skip-gram model connects only neighboring time-steps with each other.,4. Skip-gram Smoothing Algorithm,[0],[0]
"Therefore, the gradient of L with respect to µu,t depends only on the values of µu,t−1 and µu,t+1.",4. Skip-gram Smoothing Algorithm,[0],[0]
"A naive implementation of gradient ascent would thus require T−1 update steps until a change of µu,1 affects updates of µu,T .
",4. Skip-gram Smoothing Algorithm,[0],[0]
"This problem can be avoided with a change of basis from µu,1:T to new parameters ρu,1:T ,
µu,1:T = Aρu,1:T (S19)
with an appropriately chosen invertible matrix A ∈ RT×T .",4. Skip-gram Smoothing Algorithm,[0],[0]
"Derivatives of L with respect to ρu,1:T are given by the chain rule, ∂L/∂ρu,1:T = (∂L/∂µu,1:T )A.",4. Skip-gram Smoothing Algorithm,[0],[0]
"A natural (but inefficient) choice for A is to stack the eigenvectors of the prior precision matrix Π, see Eq.",4. Skip-gram Smoothing Algorithm,[0],[0]
"S11, into a matrix.",4. Skip-gram Smoothing Algorithm,[0],[0]
The eigenvectors of Π are the Fourier modes of the Kalman filtering prior (with a regularization due to σ0).,4. Skip-gram Smoothing Algorithm,[0],[0]
"Therefore, there is a component ρu,t that corresponds to the zero-mode of Π, and this component learns an average word embedding over all times.",4. Skip-gram Smoothing Algorithm,[0],[0]
Higher modes correspond to changes of the embedding vector over time.,4. Skip-gram Smoothing Algorithm,[0],[0]
"A single update to the zero immediately affects all elements of µu,1:T , and therefore changes the word embeddings at all time steps.",4. Skip-gram Smoothing Algorithm,[0],[0]
"Thus, information propagates quickly along the time dimension.",4. Skip-gram Smoothing Algorithm,[0],[0]
The downside of this choice for A is that the transformation in Eq.,4. Skip-gram Smoothing Algorithm,[0],[0]
"S19 has complexity Ω(T 2), which makes update steps slow.
",4. Skip-gram Smoothing Algorithm,[0],[0]
"As a compromise between efficiency and a natural basis, we propose to set A in Eq.",4. Skip-gram Smoothing Algorithm,[0],[0]
S19 to the Cholesky decomposition of the prior covariance matrix Π−1 ≡,4. Skip-gram Smoothing Algorithm,[0],[0]
AA>.,4. Skip-gram Smoothing Algorithm,[0],[0]
"Thus, A is still a dense (upper triangular) matrix, and, in our experiments, updates to the last component ρu,T affect all components of µu,1:T in an approximately equal amount.",4. Skip-gram Smoothing Algorithm,[0],[0]
"Since Π is tridiagonal, the inverse of A is bidiagonal, and Eq.",4. Skip-gram Smoothing Algorithm,[0],[0]
"S19 can be evaluated in Θ(T ) time by solving Aµu,1:T = ρu,1:T for µu,1:T .",4. Skip-gram Smoothing Algorithm,[0],[0]
This is the parameterization we used in our implementation of the skip-gram smoothing algorithm.,4. Skip-gram Smoothing Algorithm,[0],[0]
We present a probabilistic language model for time-stamped text data which tracks the semantic evolution of individual words over time.,abstractText,[0],[0]
The model represents words and contexts by latent trajectories in an embedding space.,abstractText,[0],[0]
"At each moment in time, the embedding vectors are inferred from a probabilistic version of word2vec (Mikolov et al., 2013b).",abstractText,[0],[0]
These embedding vectors are connected in time through a latent diffusion process.,abstractText,[0],[0]
We describe two scalable variational inference algorithms—skipgram smoothing and skip-gram filtering—that allow us to train the model jointly over all times; thus learning on all data while simultaneously allowing word and context vectors to drift.,abstractText,[0],[0]
Experimental results on three different corpora demonstrate that our dynamic model infers word embedding trajectories that are more interpretable and lead to higher predictive likelihoods than competing methods that are based on static models trained separately on time slices.,abstractText,[0],[0]
Dynamic Word Embeddings,title,[0],[0]
Deep convolutional neural networks (CNNs) have been crucial to the success of deep learning.,1. Introduction,[0],[0]
"Architectures based on CNNs have achieved unprecedented accuracy in domains ranging across computer vision (Krizhevsky et al., 2012), speech recognition (Hinton et al., 2012), natural language processing (Collobert et al., 2011; Kalchbrenner et al., 2014;
1Google Brain 2Work done as part of the Google AI Residency program (g.co/airesidency).",1. Introduction,[0],[0]
Correspondence to:,1. Introduction,[0],[0]
"Lechao Xiao <xlc@google.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"Kim, 2014), and recently even the board game Go (Silver et al., 2016; 2017).
",1. Introduction,[0],[0]
The performance of deep convolutional networks has improved as these networks have been made ever deeper.,1. Introduction,[0],[0]
"For example, some of the best-performing models on ImageNet (Deng et al., 2009) have employed hundreds or even a thousand layers (He et al., 2016a;b).",1. Introduction,[0],[0]
"However, these extremely deep architectures have been trainable only in conjunction with techniques like residual connections (He et al., 2016a) and batch normalization (Ioffe & Szegedy, 2015).",1. Introduction,[0],[0]
It is an open question whether these techniques qualitatively improve model performance or whether they are necessary crutches that solely make the networks easier to train.,1. Introduction,[0],[0]
"In this work, we study vanilla CNNs using a combination of theory and experiment to disentangle the notions of trainability and generalization performance.",1. Introduction,[0],[0]
"In doing so, we show that through a careful, theoretically-motivated initialization scheme, we can train vanilla CNNs with 10,000 layers using no architectural tricks.
",1. Introduction,[0],[0]
"Recent work has used mean field theory to build a theoretical understanding of neural networks with random parameters (Poole et al., 2016; Schoenholz et al., 2017; Yang & Schoenholz, 2017; Schoenholz et al., 2017; Karakida et al., 2018; Hayou et al., 2018; Hanin & Rolnick, 2018; Yang & Schoenholz, 2018).",1. Introduction,[0],[0]
"These studies revealed a maximum depth through which signals can propagate at initialization, and verified empirically that networks are trainable precisely when signals can travel all the way through them.",1. Introduction,[0],[0]
"In the fully-connected setting, the theory additionally predicts the existence of an order-to-chaos phase transition in the space of initialization hyperparameters.",1. Introduction,[0],[0]
"For networks initialized on the critical line separating these phases, signals can propagate indefinitely and arbitrarily deep networks can be trained.",1. Introduction,[0],[0]
While mean field theory captures the “average” dynamics of random neural networks it does not quantify the scale of gradient fluctuations that are crucial to the stability of gradient descent.,1. Introduction,[0],[0]
"A related body of work (Saxe et al., 2013; Pennington et al., 2017; 2018) has examined the input-output Jacobian and used random matrix theory to quantify the distribution of its singular values in terms of the activation function and the distribution from which the initial random weight matrices are drawn.",1. Introduction,[0],[0]
"These works concluded that networks can be trained most efficiently when the Jacobian is well-conditioned, a criterion that can be achieved with orthogonal, but not Gaussian, weight matrices.",1. Introduction,[0],[0]
"Together, these approaches have allowed researchers to efficiently train extremely deep network architectures, but so far they have been limited to neural networks composed of fully-connected layers.
",1. Introduction,[0],[0]
"In the present work, we continue this line of research and extend it to the convolutional setting.",1. Introduction,[0],[0]
"We show that a welldefined mean-field theory exists for convolutional networks in the limit that the number of channels is large, even when the size of the image is small.",1. Introduction,[0],[0]
"Moreover, convolutional networks have precisely the same order-to-chaos transition as fully-connected networks, with vanishing gradients in the ordered phase and exploding gradients in the chaotic phase.",1. Introduction,[0],[0]
"And just like fully-connected networks, very deep CNNs that are initialized on the critical line separating those two phases can be trained with relative ease.
",1. Introduction,[0],[0]
"Moving beyond mean field theory, we additionally show that the random matrix analysis of (Pennington et al., 2017; 2018) carries over to the convolutional setting.",1. Introduction,[0],[0]
"Furthermore, we identify an efficient construction from the wavelet literature that generates random orthogonal matrices with the block-circulant structure that corresponds to convolution operators.",1. Introduction,[0],[0]
This construction facilitates random orthogonal initialization for convolulational layers and enables good conditioning of the end-to-end Jacobian matrices of arbitrarily deep networks.,1. Introduction,[0],[0]
"We show empirically that networks with this initialization can train significantly more quickly than standard convolutional networks.
",1. Introduction,[0],[0]
"Finally, we emphasize that although the order-to-chaos phase boundaries of fully-connected and convolutional networks look identical, the underlying mean-field theories are in fact quite different.",1. Introduction,[0],[0]
"In particular, a novel aspect of the convolutional theory is the existence of multiple depth scales that control signal propagation at different spatial frequencies.",1. Introduction,[0],[0]
"In the large depth limit, signals can only propagate along modes with minimal spatial structure; all other modes end up deteriorating, even at criticality.",1. Introduction,[0],[0]
"We hypothesize that this type of signal degradation is harmful for generalization, and we develop a modified initialization scheme that allows for balanced propagation of signals among all frequencies.",1. Introduction,[0],[0]
"In this scheme, which we call Delta-Orthogonal initialization, the orthogonal kernel is drawn from a spatially non-uniform distribution, and it allows us to train vanilla CNNs of 10,000 layers or more with no degradation in performance.",1. Introduction,[0],[0]
"In this section, we first derive a mean field theory for signal propagation in random convolutional neural networks.",2. Theoretical results,[0],[0]
We will follow the general methodology established in Poole et al. (2016); Schoenholz et al. (2017); Yang & Schoenholz (2017).,2. Theoretical results,[0],[0]
We will then arrive at a theory for the singular value distribution of the Jacobian following Pennington et al. (2017; 2018).,2. Theoretical results,[0],[0]
"Together, this will allow us to derive theoretically motivated initialization schemes for convolutional neural networks that we call orthogonal kernels and Delta-Orthogonal kernels.",2. Theoretical results,[0],[0]
Later we will demonstrate experimentally that these kernels outperform existing initialization schemes for very deep vanilla convolutional networks.,2. Theoretical results,[0],[0]
"Consider an L-layer 1D1 CNN with periodic boundary conditions, filter width 2k + 1, number of channels c, spatial size n, per-layer weight tensors !",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
"l 2 R(2k+1)⇥c⇥c, and biases bl 2 Rc.",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
Let :,2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
"R! R be the activation function and let hlj(↵) denote the pre-activation unit at layer l, channel j, and spatial location ↵ 2 sp, where we define the set of spatial locations sp = {1, ..., n}.",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
"The forward-propagation dynamics can be described by the recurrence relation,
hl+1j (↵) = X
i2chn 2ker
(hli(↵ + ))!",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
l+1 ij ( ) +,2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
"b l+1 j , (2.1)
",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
"where ker = { 2 Z : | |  k} and chn = {1, . . .",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
", c}.",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
"At initialization, we take the weights !",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
lij ( ) to be drawn i.i.d.,2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
"from the Gaussian N (0, 2!/(c(2k + 1)))",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
"and the biases blj
1For notational simplicity, we consider one-dimensional convolutions, but the d-dimensional case proceeds identically.
to be drawn i.i.d.",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
"from the Gaussian N (0, 2b ).",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
Note that hli(↵) =,2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
h,2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
l i(↵ + n),2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
=,2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
h,2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
l i(↵ n),2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
since we assume periodic boundary conditions.,2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
We wish to understand how signals propagate through these networks.,2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
"As in previous work in this vein, we will take the large network limit, which in this context corresponds to the number of channels c!1.",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
This allows us to use powerful theoretical tools such as mean field theory and random matrix theory.,2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
"Moreover, this approximation has been shown to give results that agree well with experiments on finite-size networks.
",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
"In the limit of a large number of channels, the central limit theorem implies that the pre-activation vectors hlj are i.i.d.",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
"Gaussian with mean zero and covariance matrix ⌃l↵,↵0 = E[hlj(↵)hlj(↵0)].",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
"Here, the expectation is taken over the weights and biases and it is independent of the channel index j.",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
"In this limit, the covariance matrix takes the form (see Supplemental Materials (SM)),
⌃ l+1 ↵,↵0 = 2 b + 2w 2k+1
X
2ker E ⇥",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
(hlj(↵+ )),2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
(h l j(↵ 0 + )),2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
"⇤ ,
(2.2) and is independent of j. A more compact representation of this equation can be given as,
⌃l+1 ⌘ A ?",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
"C(⌃l) , (2.3)
where A = 12k+1I2k+1 and ?",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
"denotes 2D circular crosscorrelation, i.e. for any matrix C, A ?",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
"C is defined as,
",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
"[A ? C]↵,↵0 = 1
2k + 1
X 2ker C↵+ ,↵0+ .",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
"(2.4)
",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
The function C : PSDn !,2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
"PSDn is related to the C-map defined in Poole et al. (2016) (see also (Daniely et al., 2016)) and is given by,
[C(⌃)]↵,↵0 = 2 !",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
"Eh⇠N (0,⌃)",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
[ (h↵) (h↵0)],2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
+,2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
2b .,2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
"(2.5)
All but the two dimensions ↵ and ↵0 in eqn.",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
"(2.5) marginalize, so, as in (Poole et al., 2016), the C-map can be computed by a two-dimensional integral.",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
"Unlike in (Poole et al., 2016), ↵ and ↵0 do not correspond to different examples but rather to different spatial positions and eqn.",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
(2.5) characterizes how signals from a single input propagate through convolutional networks in the mean-field approximation2.,2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
We now seek to study the dynamics induced by eqn.,2.1.2. DYNAMICS OF SIGNAL PROPAGATION,[0],[0]
(2.3).,2.1.2. DYNAMICS OF SIGNAL PROPAGATION,[0],[0]
"Schematically, our approach will be to identify fixed points of eqn.",2.1.2. DYNAMICS OF SIGNAL PROPAGATION,[0],[0]
"(2.3) and then linearize the dynamics around these
2The multi-input analysis proceeds in precisely the same manner as we present here, but comes with increased notational complexity and features no qualitatively different behavior, so we focus our presentation on the single-input case.
fixed points.",2.1.2. DYNAMICS OF SIGNAL PROPAGATION,[0],[0]
"These linearized dynamics will dictate the stability and rate of decay towards the fixed points, which determines the depth scales over which signals in the network can propagate.
",2.1.2. DYNAMICS OF SIGNAL PROPAGATION,[0],[0]
"Schoenholz et al. (2017) found that for many activation functions (e.g. tanh) and any choice of w and b, the C-map has a fixed point ⌃⇤ (i.e. C(⌃⇤) = ⌃⇤) of the form,
⌃",2.1.2. DYNAMICS OF SIGNAL PROPAGATION,[0],[0]
"⇤ ↵,↵0 = q ⇤ ( ↵,↵0 + (1 ↵,↵0)c ⇤ ) , (2.6)
where a,b is the Kronecker- , q⇤ is the fixed-point variance of a single input, and c⇤ is the fixed-point correlation between two inputs.",2.1.2. DYNAMICS OF SIGNAL PROPAGATION,[0],[0]
It follows from the form of eqn.,2.1.2. DYNAMICS OF SIGNAL PROPAGATION,[0],[0]
(2.4) that ⌃⇤ is also a fixed point of the layer-to-layer covariance map in the convolutional case (eqn.,2.1.2. DYNAMICS OF SIGNAL PROPAGATION,[0],[0]
"(2.3)), i.e. ⌃⇤ = A ?",2.1.2. DYNAMICS OF SIGNAL PROPAGATION,[0],[0]
"C(⌃⇤).
",2.1.2. DYNAMICS OF SIGNAL PROPAGATION,[0],[0]
"To analyze the dynamics of the iteration map (2.3) near the fixed point ⌃⇤, we define ✏l = ⌃⇤ ⌃l and expand eqn.",2.1.2. DYNAMICS OF SIGNAL PROPAGATION,[0],[0]
(2.3) to lowest order in ✏.,2.1.2. DYNAMICS OF SIGNAL PROPAGATION,[0],[0]
"This expansion requires the Jacobian of the C-map evaluated at the fixed point, the properties of which we analyze in the SM.",2.1.2. DYNAMICS OF SIGNAL PROPAGATION,[0],[0]
"In brief, perturbations in q⇤ and c⇤ evolve independently and the Jacobian decomposes into a diagonal eigenspace Vd with eigenvalue q⇤ , and an off-diagonal eigenspace Vo.d. with eigenvalue c⇤ .",2.1.2. DYNAMICS OF SIGNAL PROPAGATION,[0],[0]
"The eigenvalues are given by3,
c⇤ = 2 wEh⇠N (0,C⇤)[ 0(h1) 0(h2)] , h1 6= h2 , q⇤ = 2 wEh⇠N (0,C⇤)[ 00(h1) (h1) + 0(h1)2] , (2.7)
and the eigenspaces have bases,
Bd = {M ↵,↵ : M↵,↵↵̄,↵̄0 = ↵,↵̄ ↵,↵̄0 + ↵̄,↵ + ↵̄0,↵}↵2sp
Bo.d. =",2.1.2. DYNAMICS OF SIGNAL PROPAGATION,[0],[0]
"{M ↵,↵0 : M↵,↵ 0 ↵̄,↵̄0 = ↵,↵̄ ↵0,↵̄0 + ↵,↵̄0 ↵0,↵̄}↵ 6=↵0 ,
(2.8)
i.e. Vd = span(Bd) and Vo.d = span(Bo.d.).",2.1.2. DYNAMICS OF SIGNAL PROPAGATION,[0],[0]
Note that q⇤ and c⇤ also were found in Schoenholz et al. (2017) to control signal propagation in the fully-connected case.,2.1.2. DYNAMICS OF SIGNAL PROPAGATION,[0],[0]
The constant is given in Lemma B.2 of the SM but does not concern us here.,2.1.2. DYNAMICS OF SIGNAL PROPAGATION,[0],[0]
This eigen-decomposition implies that the layer-wise deviations from the fixed point evolve under eqn.,2.1.2. DYNAMICS OF SIGNAL PROPAGATION,[0],[0]
"(2.3) as,
✏l+1 = q⇤A ?",2.1.2. DYNAMICS OF SIGNAL PROPAGATION,[0],[0]
✏ l d + c⇤A ?,2.1.2. DYNAMICS OF SIGNAL PROPAGATION,[0],[0]
✏ l o.d. +,2.1.2. DYNAMICS OF SIGNAL PROPAGATION,[0],[0]
"O((✏ l ) 2 ) , (2.9)
",2.1.2. DYNAMICS OF SIGNAL PROPAGATION,[0],[0]
"where ✏d and ✏o.d. are decomposition of ✏ into the eigenspaces Vd and Vo.d..
Eqn. (2.9) defines the linear dynamics of random convolutional neural networks near their fixed points and is the basis for the in-depth analysis of the following subsections.
",2.1.2. DYNAMICS OF SIGNAL PROPAGATION,[0],[0]
"3By the symmetry of ⌃⇤, these expectations are independent of spatial location and of the choice of h1 and h2.",2.1.2. DYNAMICS OF SIGNAL PROPAGATION,[0],[0]
"In the fully-connected setting, the dynamics of signal propagation near the fixed point are governed by scalar evolution equations.",2.1.3. MULTI-DIMENSIONAL SIGNAL PROPAGATION,[0],[0]
"In contrast, the convolutional setting enjoys much richer dynamics, as eqn.",2.1.3. MULTI-DIMENSIONAL SIGNAL PROPAGATION,[0],[0]
"(2.9) describes a multi-dimensional system that we now analyze.
",2.1.3. MULTI-DIMENSIONAL SIGNAL PROPAGATION,[0],[0]
It follows from eqns.,2.1.3. MULTI-DIMENSIONAL SIGNAL PROPAGATION,[0],[0]
"(2.4) and (2.8) (see also the SM) that A does not mix the diagonal and off-diagonal eigenspaces, i.e. A ?",2.1.3. MULTI-DIMENSIONAL SIGNAL PROPAGATION,[0],[0]
✏d 2 Vd and A ?,2.1.3. MULTI-DIMENSIONAL SIGNAL PROPAGATION,[0],[0]
"✏o.d. 2 Vo.d.. To see this, note that for M↵,↵ 0 2 Vo.d., the definition implies M↵,↵ 0
↵̄+ ,↵̄0+ =
M↵ ,↵ 0 ↵̄,↵̄0 .",2.1.3. MULTI-DIMENSIONAL SIGNAL PROPAGATION,[0],[0]
"This property ensures that A ? M ↵,↵0 can be expressed as a linear combination of matrices in Vo.d., which means it also belongs to Vo.d.",2.1.3. MULTI-DIMENSIONAL SIGNAL PROPAGATION,[0],[0]
"The same argument applies to M↵,↵ 2 Vd.. As a result, these eigenspaces evolve entirely independently under the linearization of the covariance iteration map (2.3).
",2.1.3. MULTI-DIMENSIONAL SIGNAL PROPAGATION,[0],[0]
Let l0 denote the depth over which transient effects persist and after which eqn.,2.1.3. MULTI-DIMENSIONAL SIGNAL PROPAGATION,[0],[0]
(2.9) accurately describes the linearized dynamics.,2.1.3. MULTI-DIMENSIONAL SIGNAL PROPAGATION,[0],[0]
"Therefore, at depths larger than l0, we have
✏l ⇡ A ? · · · A ?| {z } l l0",2.1.3. MULTI-DIMENSIONAL SIGNAL PROPAGATION,[0],[0]
( l l0q⇤,2.1.3. MULTI-DIMENSIONAL SIGNAL PROPAGATION,[0],[0]
✏ l0 d +,2.1.3. MULTI-DIMENSIONAL SIGNAL PROPAGATION,[0],[0]
l l0 c⇤ ✏ l0 o.d.) .,2.1.3. MULTI-DIMENSIONAL SIGNAL PROPAGATION,[0],[0]
"(2.10)
",2.1.3. MULTI-DIMENSIONAL SIGNAL PROPAGATION,[0],[0]
"This matrix-valued equation is still somewhat complicated owing to the nested applications of A. To further elucidate the dynamics, we can move to a Fourier basis, which diagonalizes the circular cross-correlation operator and decouples the modes of eqn.",2.1.3. MULTI-DIMENSIONAL SIGNAL PROPAGATION,[0],[0]
(2.10).,2.1.3. MULTI-DIMENSIONAL SIGNAL PROPAGATION,[0],[0]
"In particular, let F denote the 2D discrete Fourier transform and ✏̃↵,↵0 ⌘ F(✏)↵,↵0 denote a Fourier mode of ✏.",2.1.3. MULTI-DIMENSIONAL SIGNAL PROPAGATION,[0],[0]
"Then eqn. (2.10) becomes a simple scalar equation,
",2.1.3. MULTI-DIMENSIONAL SIGNAL PROPAGATION,[0],[0]
"✏̃l↵,↵0 ⇡ ( ↵,↵0 q⇤)",2.1.3. MULTI-DIMENSIONAL SIGNAL PROPAGATION,[0],[0]
l l0,2.1.3. MULTI-DIMENSIONAL SIGNAL PROPAGATION,[0],[0]
"[✏̃l0d ]↵,↵0+( ↵,↵0 c⇤) l l0",2.1.3. MULTI-DIMENSIONAL SIGNAL PROPAGATION,[0],[0]
"[✏̃l0o.d.]↵,↵0 , (2.11) with ↵,↵0 = F(A)⇤↵,↵0 .",2.1.3. MULTI-DIMENSIONAL SIGNAL PROPAGATION,[0],[0]
"Thus, the linearized dynamics of convolutional neural networks decouple into independentlyevolving Fourier modes that evolve near the fixed point at frequency-dependent rates.",2.1.3. MULTI-DIMENSIONAL SIGNAL PROPAGATION,[0],[0]
The stability of the fixed point ⌃⇤ is determined by whether nearby points move closer or farther from ⌃⇤ under the dynamics described by eqn.,2.1.4. FIXED-POINT ANALYSIS,[0],[0]
(2.9).,2.1.4. FIXED-POINT ANALYSIS,[0],[0]
"Eqn. (2.11) shows that this condition depends on the whether the quantities ↵,↵0 q⇤ and ↵,↵0 c⇤ are less than or greater than one.
",2.1.4. FIXED-POINT ANALYSIS,[0],[0]
"Since A is a diagonal matrix, the eigenvalues ↵,↵0 have a specific structure.",2.1.4. FIXED-POINT ANALYSIS,[0],[0]
"In particular, the set of eigenvalues is comprised of n copies of the 1D discrete Fourier transform of the diagonal entries of A. Furthermore, since the diagonal entries of A are non-negative and sum to one, their Fourier coefficients have absolute value no larger than one and the zero-frequency coefficient is equal to one; see Figure 4
for the full distribution in the case of 2D convolutions.",2.1.4. FIXED-POINT ANALYSIS,[0],[0]
"It follows that the fixed point ⌃⇤ will be stable if and only if q⇤ < 1 and c⇤ < 1.
",2.1.4. FIXED-POINT ANALYSIS,[0],[0]
"These stability conditions are precisely the ones found to govern fully-connected networks (Poole et al., 2016; Schoenholz et al., 2017).",2.1.4. FIXED-POINT ANALYSIS,[0],[0]
"Moreover, the fixed point matrix ⌃
⇤ is also the same as in the fully-connected case.",2.1.4. FIXED-POINT ANALYSIS,[0],[0]
"Together, these observations imply that the entire fixed-point structure of the convolutional case is identical to that of the fullyconnected case.",2.1.4. FIXED-POINT ANALYSIS,[0],[0]
"In particular, based on the results of (Poole et al., 2016), we can immediately conclude that the ( w, b) hyperparameter plane is separated by the line 1 = 1 into an ordered phase with c⇤ = 1 in which all pixels approach the same value, and a chaotic phase with c⇤ < 1 in which the pixels become decorrelated with one another; see the SM for a review of this phase diagram analysis.",2.1.4. FIXED-POINT ANALYSIS,[0],[0]
"We now assume that the conditions for a stable fixed point are met, i.e. q⇤ < 1 and c⇤ < 1, and we consider the rate at which the fixed point is approached.",2.1.5. DEPTH SCALES OF SIGNAL PROPAGATION,[0],[0]
"As in (Schoenholz et al., 2017), it is convenient to additionally assume q",2.1.5. DEPTH SCALES OF SIGNAL PROPAGATION,[0],[0]
⇤ < c,2.1.5. DEPTH SCALES OF SIGNAL PROPAGATION,[0],[0]
⇤ so that the dynamics in the diagonal subspace can be neglected.,2.1.5. DEPTH SCALES OF SIGNAL PROPAGATION,[0],[0]
"In this case, eqn.",2.1.5. DEPTH SCALES OF SIGNAL PROPAGATION,[0],[0]
"(2.11) can be rewritten as
✏̃l↵,↵0 ⇡ e (l l0)/⇠↵,↵0 [✏̃o.d.]",2.1.5. DEPTH SCALES OF SIGNAL PROPAGATION,[0],[0]
"l0 ↵,↵0 , (2.12)
where ⇠↵,↵0 = 1/ log( c⇤ ↵,↵0) are depth scales governing the convergence of the different modes.",2.1.5. DEPTH SCALES OF SIGNAL PROPAGATION,[0],[0]
"In particular, we expect signals corresponding to a specific Fourier mode
f↵,↵0 to be able to travel a depth commensurate to ⇠↵,↵0 through the network.",2.1.5. DEPTH SCALES OF SIGNAL PROPAGATION,[0],[0]
"Thus, unlike fully-connected networks which exhibit only a single depth scale, convolutional networks feature a hierarchy of depth scales.
",2.1.5. DEPTH SCALES OF SIGNAL PROPAGATION,[0],[0]
"Recalling that ↵,n ↵ = 1, it follows that ⇠c ⌘ ⇠↵,n ↵ = 1/ log c⇤ , which is identical to the depth scale governing signal propagation through fully-connected networks.",2.1.5. DEPTH SCALES OF SIGNAL PROPAGATION,[0],[0]
"It follows from (Schoenholz et al., 2017) that when 1 = 1, ⇠↵,n ↵ diverges and thus convolutional networks can propagate signals arbitrarily far through the f↵,n ↵ modes.",2.1.5. DEPTH SCALES OF SIGNAL PROPAGATION,[0],[0]
"Since | ↵,↵0 | < 1 for ↵0 6= n",2.1.5. DEPTH SCALES OF SIGNAL PROPAGATION,[0],[0]
"↵, these are the only modes through which signals can propagate without attenuation.",2.1.5. DEPTH SCALES OF SIGNAL PROPAGATION,[0],[0]
"Finally, we note that the f↵,n ↵ modes correspond to perturbations that are spatially uniform along the cyclic diagonals of the covariance matrix.",2.1.5. DEPTH SCALES OF SIGNAL PROPAGATION,[0],[0]
"The fact that all signals with additional spatial structure attenuate for large depth suggests that deep critical convolutional networks behave quite similarly to fully-connected networks, which also cannot propagate spatially-structured signals.",2.1.5. DEPTH SCALES OF SIGNAL PROPAGATION,[0],[0]
The similarities between signal propagation in convolutional neural networks and fully-connected networks in the limit of large depth are surprising.,2.1.6. NON-UNIFORM KERNELS,[0],[0]
A consequence may be that the performance of very deep convolutional networks degrades as the signal is forced to propagate along modes with minimal spatial structure.,2.1.6. NON-UNIFORM KERNELS,[0],[0]
"Indeed, Fig. 3 shows that the generalization performance decreases with depth, and that for very large depth it barely surpasses the performance of a
fully-connected network.
",2.1.6. NON-UNIFORM KERNELS,[0],[0]
"If increased spatial uniformity is the problem, eqn.",2.1.6. NON-UNIFORM KERNELS,[0],[0]
(2.12) holds the solution.,2.1.6. NON-UNIFORM KERNELS,[0],[0]
"In order for all modes to propagate without attenuation, it is necessary that ↵,↵0 = 1 for all ↵, ↵0.",2.1.6. NON-UNIFORM KERNELS,[0],[0]
"In fact, it is easy to show that the distribution of { ↵,↵0} can be modified by allowing for spatial non-uniformity in the variance of the weights within the kernel.",2.1.6. NON-UNIFORM KERNELS,[0],[0]
"To this end, we introduce a non-negative vector v = (v )",2.1.6. NON-UNIFORM KERNELS,[0],[0]
"2ker chosen such that P v = 1, and initialize the weights of the net-
work according to wlij( ) ⇠ N (0, 2wv /c).",2.1.6. NON-UNIFORM KERNELS,[0],[0]
Each choice of v will induce a new dynamical equation analogous to eqn.,2.1.6. NON-UNIFORM KERNELS,[0],[0]
"(2.3) (see SM),
⌃l+1 = Av ? C(⌃ l ) , (2.13)
where Av = diag(v).",2.1.6. NON-UNIFORM KERNELS,[0],[0]
It follows directly from the previous analysis that the linearized dynamics of eqn.,2.1.6. NON-UNIFORM KERNELS,[0],[0]
"(2.13) will be identical to the dynamics of eqn (2.3), only now with ↵,↵0 = F(Av)⇤↵,↵0 .",2.1.6. NON-UNIFORM KERNELS,[0],[0]
"By the same argument presented in Section 2.1.3, the set of eigenvalues is now comprised of n copies of the 1D Fourier transform of v. As a result, it is possible to control the depth scales over which different modes of the signal can propagate through the network by changing the variance vector v. We will return to this point in section 2.4.",2.1.6. NON-UNIFORM KERNELS,[0],[0]
We now turn our attention to the back-propgation of error signals through a convolutional network.,2.2. Back-propagation of signal,[0],[0]
"Let E denote the loss and lj(↵) the back-propagated signal at layer l, channel j and spatial location ↵, i.e.,
lj(↵) = @E
@hlj(↵) .",2.2. Back-propagation of signal,[0],[0]
"(2.14)
",2.2. Back-propagation of signal,[0],[0]
"The recurrence relation is given by
lj(↵) = X
i2chn
X",2.2. Back-propagation of signal,[0],[0]
2ker l+1i (↵ )!,2.2. Back-propagation of signal,[0],[0]
l+1 ji ( ) 0,2.2. Back-propagation of signal,[0],[0]
"(hlj(↵)).
",2.2. Back-propagation of signal,[0],[0]
"As in (Schoenholz et al., 2017), we additionally make the assumption that the weights used during back-propagation are drawn independently from the weights used in forward propagation, in which case the random variables { lj}j2chn are independent for each l.",2.2. Back-propagation of signal,[0],[0]
The covariance matrices ⌃̃l ⌘ E ⇥,2.2. Back-propagation of signal,[0],[0]
"lj( l j) T ⇤ back-propagate according to, ⌃̃l↵,↵0 = X
2ker v ⌃̃
l+1 ↵ ,↵0 · 2 wEh⇠N (0,⌃l)[ 0(h↵) 0(h↵0)] .
(2.15)",2.2. Back-propagation of signal,[0],[0]
"We are primarily interested in the diagonal of ⌃̃l, which measures the variance of back-propagated signals.",2.2. Back-propagation of signal,[0],[0]
We will also assume l > l0 (see section 2.1.3) so that ⌃l is wellapproximated by ⌃⇤.,2.2. Back-propagation of signal,[0],[0]
"In this case,
⌃̃l↵,↵ ⇡ 1 X
2ker v ⌃̃
l+1 ↵ ,↵ , (2.16)
where we used eqn.",2.2. Back-propagation of signal,[0],[0]
(2.7).,2.2. Back-propagation of signal,[0],[0]
"Therefore we find that, ⌃̃l↵,↵ ⇠ L l1 ⌃̃ L ↵,↵, where L is the total depth of the network.",2.2. Back-propagation of signal,[0],[0]
"As in the fully-connected case, 1 = 1 is a necessary condition for gradient signals to neither explode nor vanish as they back-propagate through a convolutional network.",2.2. Back-propagation of signal,[0],[0]
"However, as discussed in (Pennington et al., 2017; 2018), this is not always a sufficient condition for trainability.",2.2. Back-propagation of signal,[0],[0]
"To further understand backward signal propagation, we need to push our analysis beyond mean field theory.",2.2. Back-propagation of signal,[0],[0]
"We have observed that the quantity 1 is crucial for determining signal propagation in CNNs, both in the forward and backward directions.",2.2.1. BEYOND MEAN FIELD THEORY,[0],[0]
"As discussed in (Poole et al., 2016), 1 equals the the mean squared singular value of the Jacobian J l of the layer-to-layer transition operator.",2.2.1. BEYOND MEAN FIELD THEORY,[0],[0]
"Beyond just the second moment, higher moments and indeed the whole distribution of singular values of the entire end-to-end Jacobian J = Q l J
l are important for ensuring trainability of very deep fully-connected networks (Pennington et al., 2017; 2018).",2.2.1. BEYOND MEAN FIELD THEORY,[0],[0]
"Specifically, networks train well when their input-output Jacobians exhibit dynamical isometry, namely the property that the entire distribution of singular values is close to 1.
",2.2.1. BEYOND MEAN FIELD THEORY,[0],[0]
"In fact, we can adopt the entire analysis of (Pennington et al., 2017; 2018) into the convolutional setting with essentially no modification.",2.2.1. BEYOND MEAN FIELD THEORY,[0],[0]
"The reason stems from the fact that, because convolution is a linear operator, it has a matrix representation, W l, which appears in the end-to-end Jacobian in precisely the same manner as do the weight matrices in the fully-connected case.",2.2.1. BEYOND MEAN FIELD THEORY,[0],[0]
"In particular, J = QL l=1 D
lW",2.2.1. BEYOND MEAN FIELD THEORY,[0],[0]
"l, where Dl is the diagonal matrix whose diagonal elements contain the vectorized representation of derivatives of postactivation neurons in layer l. Roughly speaking, since this is the same expression as in (Pennington et al., 2017; 2018), the conclusions found in that work regarding dynamical isometry apply equally well in the convolutional setting.
",2.2.1. BEYOND MEAN FIELD THEORY,[0],[0]
The analysis of Pennington et al. (2017; 2018) reveals that the singular values of J depends crucially on the distribution of singular values of W l and Dl.,2.2.1. BEYOND MEAN FIELD THEORY,[0],[0]
"In particular, to achieve dynamical isometry, all of these matrices should be close to orthogonal.",2.2.1. BEYOND MEAN FIELD THEORY,[0],[0]
"As in the fully-connected case, the singular values of Dl can be made arbitrarily close to 1 by choosing a small value for q⇤ and by using an activation function like tanh that is smooth and linear near the origin.",2.2.1. BEYOND MEAN FIELD THEORY,[0],[0]
"In the convolutional setting, the matrix representation of the convolution operator W l is a c ⇥ c block matrix with n ⇥ n circulant blocks.",2.2.1. BEYOND MEAN FIELD THEORY,[0],[0]
"Note that in the large c limit, n/c! 0 and the relative size of the blocks vanishes.",2.2.1. BEYOND MEAN FIELD THEORY,[0],[0]
"Therefore, if the weights are i.i.d. random variables, we can invoke universality results from random matrix theory to conclude its singular value distribution converges to the Marcenko-Pastur distribution; see Fig. S3 in the SM.",2.2.1. BEYOND MEAN FIELD THEORY,[0],[0]
"As such, we find that CNNs with i.i.d. weights cannot achieve dynamical isometry.",2.2.1. BEYOND MEAN FIELD THEORY,[0],[0]
We address this issue in the next section.,2.2.1. BEYOND MEAN FIELD THEORY,[0],[0]
"In (Pennington et al., 2017; 2018), it was observed that dynamical isometry can lead to dramatic improvements in training speed, and that achieving these favorable conditions requires orthogonal weight initializations.",2.3. Orthogonal Initialization for CNNs,[0],[0]
"While the procedure to generate random orthogonal weight matrices in the fully-connected setting is well-known, it is less obvious how to do so in the convolutional setting, and at first sight it is not at all clear whether it is even possible.",2.3. Orthogonal Initialization for CNNs,[0],[0]
"We resolve this question by invoking a result from the wavelet literature (Kautsky & Turcajov, 1994) and provide an explicit construction.",2.3. Orthogonal Initialization for CNNs,[0],[0]
"We will focus on the two-dimensional convolution here and begin with some notation.
",2.3. Orthogonal Initialization for CNNs,[0],[0]
Definition 2.1.,2.3. Orthogonal Initialization for CNNs,[0],[0]
We say K 2 Rk⇥k⇥cin⇥cout is an orthogonal kernel,2.3. Orthogonal Initialization for CNNs,[0],[0]
"if for all x 2 Rn⇥n⇥cin , kK ⇤ xk2 = kxk2.
",2.3. Orthogonal Initialization for CNNs,[0],[0]
Definition 2.2.,2.3. Orthogonal Initialization for CNNs,[0],[0]
"Consider the block matrices B = {Bi,j}0i,jp 1 2 Rpn⇥pn and C = {Ci,j}0i,jq 1 2 Rqn⇥qn, with constituent blocks Bi,j 2 Rn⇥n and Ci,j 2
Rn⇥n. Define the block-wise convolution operator ⇤ by,
[B⇤C]i,j = X
i0,j0
Bi0,j0Ci i0,j j0 , (2.17)
where the out-of-range matrices are taken to be zero.
",2.3. Orthogonal Initialization for CNNs,[0],[0]
Algorithm 1 shows how to construct orthogonal kernels for 2D convolutions of size k ⇥,2.3. Orthogonal Initialization for CNNs,[0],[0]
k ⇥ cin ⇥ cout with cin  cout.,2.3. Orthogonal Initialization for CNNs,[0],[0]
One can employ the same method to construct kernels of higher (or lower) dimensions.,2.3. Orthogonal Initialization for CNNs,[0],[0]
This new initialization method can dramatically boost the learning speed of deep CNNs; see Fig. 5 and Section 3.2.,2.3. Orthogonal Initialization for CNNs,[0],[0]
"In Section 2.1.5 it was observed that, in contrast to fullyconnected networks, CNNs have multiple depth scales controlling propagation of signals along different Fourier modes.",2.4. Delta-Orthogonal Initialization,[0],[0]
"Even at criticality, for generic variance-averaging vectors v, the majority of these depth scales are finite.",2.4. Delta-Orthogonal Initialization,[0],[0]
"However, there does exist one special averaging vector for which all of the depth scales are infinite: a one-hot vector, i.e. vi = k,i.",2.4. Delta-Orthogonal Initialization,[0],[0]
This kernel places all of its variance in the spatial center of the kernel and zero variance elsewhere.,2.4. Delta-Orthogonal Initialization,[0],[0]
"In this case, the eigenvalues ↵,↵0 are all equal to 1 and all depth scales diverge, implying that signals can propagate arbitrarily far along all Fourier modes.
",2.4. Delta-Orthogonal Initialization,[0],[0]
"If we combine this special averaging vector with the orthogonal initialization of the previous section, we obtain a powerful new initialization scheme that we call Delta-Orthogonal Initialization.",2.4. Delta-Orthogonal Initialization,[0],[0]
"Matrices of this type can be generated from Algorithm 1 with k = 1 and padding with appropriate zeros or directly from Algorithm 2 in the SM.
",2.4. Delta-Orthogonal Initialization,[0],[0]
"In the following sections, we demonstrate experimentally that extraordinarily deep convolutional networks can be trained with these initialization techniques.
",2.4. Delta-Orthogonal Initialization,[0],[0]
"Algorithm 1 2D orthogonal kernels for CNNs, available in TensorFlow via the ConvolutionOrthogonal initializer.
",2.4. Delta-Orthogonal Initialization,[0],[0]
"Input: k kernel size, cin number of input channels, cout number of output channels.",2.4. Delta-Orthogonal Initialization,[0],[0]
Return: a k⇥ k⇥ cin ⇥ cout tensor K. Step 1.,2.4. Delta-Orthogonal Initialization,[0],[0]
"Let K be the 1⇥ 1⇥ cout⇥ cout tensor such that K[0, 0] = I , where I is the cout ⇥ cout identity matrix.",2.4. Delta-Orthogonal Initialization,[0],[0]
Step 2.,2.4. Delta-Orthogonal Initialization,[0],[0]
Repeat the following (k 1) times: Randomly generate two orthogonal projection matrices P and Q of size cout ⇥ cout and set (see eqn.,2.4. Delta-Orthogonal Initialization,[0],[0]
"(2.17))
",2.4. Delta-Orthogonal Initialization,[0],[0]
"K K⇤ 
PQ P (1 Q) (1 P )Q (1 P )(1 Q) .
",2.4. Delta-Orthogonal Initialization,[0],[0]
Step 3.,2.4. Delta-Orthogonal Initialization,[0],[0]
"Randomly generate a cin ⇥ cout matrix H with orthonormal rows and for i = 0, . . .",2.4. Delta-Orthogonal Initialization,[0],[0]
",",2.4. Delta-Orthogonal Initialization,[0],[0]
"k 1 and j = 0, . . .",2.4. Delta-Orthogonal Initialization,[0],[0]
", k 1, set K[i, j] HK[i, j].",2.4. Delta-Orthogonal Initialization,[0],[0]
Return K.,2.4. Delta-Orthogonal Initialization,[0],[0]
"To support the theoretical results built up in Section 2, we trained a large number of very deep CNNs on MNIST and CIFAR-10 with tanh as the activation function.",3. Experiments,[0],[0]
We use the following vanilla CNN architecture.,3. Experiments,[0],[0]
First we apply three 3 ⇥ 3 ⇥,3. Experiments,[0],[0]
"c convolutions with strides 1, 2 and 2 in order to increase the channel size to c and reduce the spatial dimension to 7 ⇥ 7 (or 8 ⇥ 8 for CIFAR-10), and then a block of d 3 ⇥ 3 ⇥",3. Experiments,[0],[0]
"c convolutions with d varying from 2 to 10, 000.",3. Experiments,[0],[0]
"Finally, an average pooling layer and a fullyconnected layer are applied.",3. Experiments,[0],[0]
Here c = 256 when d  256 and c = 128 otherwise.,3. Experiments,[0],[0]
"To maximally support our theories, we applied no common techniques (including learning rate decay).",3. Experiments,[0],[0]
"Note that the early downsampling is necessary from a computational perspective, but it does diminish the maximum achievable performance; e.g. our best achieved test accuracy with downsampling was 82% on CIFAR-10.",3. Experiments,[0],[0]
We performed an additional experiment training a 50 layers network without downsampling.,3. Experiments,[0],[0]
"This resulted in a test accuracy of 89.90%, which is comparable to the best performance on CIFAR-10 using a tanh architecture that we were able to find (89.82%, (Mishkin & Matas, 2015)).",3. Experiments,[0],[0]
The analysis in Section 2.1 gives a prediction for precisely which initialization hyperparameters a CNN will be trainable.,3.1. Trainability and Critical Initialization,[0],[0]
"In particular, we predict that the network ought to be trainable provided L .",3.1. Trainability and Critical Initialization,[0],[0]
"⇠c. To test this, we train a large number of convolutional neural networks on MNIST with depth varying between L = 10 and L = 600 and with weights initialized with 2w 2",3.1. Trainability and Critical Initialization,[0],[0]
"[0, 4].",3.1. Trainability and Critical Initialization,[0],[0]
In Fig. 2 we plot – using a heatmap – the training accuracy obtained by these networks after different numbers of steps.,3.1. Trainability and Critical Initialization,[0],[0]
"Additionally we
overlay the depth scale predicted by our theory, ⇠c. We find strikingly good agreement between our theory of random networks and the results of our experiments.",3.1. Trainability and Critical Initialization,[0],[0]
We argued in Section 2.2.1 that the input-output Jacobian of CNNs with i.i.d. weights will become increasingly illconditioned as the number of layers grows.,3.2. Orthogonal Initialization and Ultra-deep CNNs,[0],[0]
"On the other hand, orthogonal weight initializations can achieve dynamical isometry and dramatically boost the training speed.",3.2. Orthogonal Initialization and Ultra-deep CNNs,[0],[0]
"To verify this, we train a 4,000-layer CNN on MNIST using a critically-tuned Gaussian weight initialization and the orthogonal initialization scheme developed in Section 2.3.",3.2. Orthogonal Initialization and Ultra-deep CNNs,[0],[0]
"Fig. 5 shows that the network with Gaussian initialization learns slowly (test and training accuracy is below 60% after 90, 000 steps, about 60 epochs).",3.2. Orthogonal Initialization and Ultra-deep CNNs,[0],[0]
"In contrast, orthogonal initialization learns quickly with test accuracy above 60% after only 1 epoch, and achieves 95% after 10, 000 steps or about 7 epochs.",3.2. Orthogonal Initialization and Ultra-deep CNNs,[0],[0]
The analysis in Section 2.1.3 and Section 2.1.6 suggest that CNNs initialized with kernels with spatially uniform variance may suffer a degradation in generalization performance as the depth increases.,3.3. Multi-dimensional Signal Propagation,[0],[0]
Fig. 3 shows the learning curves of CNNs on CIFAR-10 with depth varying from 32 to 8192.,3.3. Multi-dimensional Signal Propagation,[0],[0]
"Although the orthogonal initialization enables even the deepest model to reach 100% training accuracy, the test accuracy decays as the depth increases with the deepest mode generalizing only marginally better than a fully-connected network.
",3.3. Multi-dimensional Signal Propagation,[0],[0]
"To test whether this degradation in performance may be the result of attenuation of spatially non-uniform signals, we trained a variety of models on CIFAR-10 whose kernels were initialized with spatially non-uniform variance.",3.3. Multi-dimensional Signal Propagation,[0],[0]
"According to the analysis in Section 2.1.6, changing the shape of this non-uniformity controls the depth scales over which different Fourier components of the signal can propagate through the network.",3.3. Multi-dimensional Signal Propagation,[0],[0]
We examined five different non-uniform critical Gaussian initialization methods.,3.3. Multi-dimensional Signal Propagation,[0],[0]
"The variance vectors v were chosen in the following way: GS0 refers to the one-hot delta initialization for which the eigenvalues ↵,↵0 are all equal to 1.",3.3. Multi-dimensional Signal Propagation,[0],[0]
"GS1, GS2 and GS3 are obtained by interpolating between GS0 and GS4, which is the uniform variance initialization.
",3.3. Multi-dimensional Signal Propagation,[0],[0]
"Each variance vector has exactly 8⇥ 8 singular values, plotted in Fig. 4(b) in descending order.",3.3. Multi-dimensional Signal Propagation,[0],[0]
"Note that from GS0 to GS4, the singular values become more poorly-conditioned (the distribution becomes more concentrated around 0).",3.3. Multi-dimensional Signal Propagation,[0],[0]
Fig. 4(a) shows that the relative fall-off of generalization performance with depth follows the same pattern: the more poorly-conditioned the singular values the worse the model generalizes.,3.3. Multi-dimensional Signal Propagation,[0],[0]
"These observations suggest that salient infor-
mation may be propagating along multiple Fourier modes.",3.3. Multi-dimensional Signal Propagation,[0],[0]
Our theory predicts that an ultra-deep CNNs can train faster and perform better if critically initialized using DeltaOrthogonal kernels.,"3.4. Training 10,000-layers: Delta-Orthogonal Initialization.",[0],[0]
"To test this theory, we train CNNs of 1,250, 2,500, 5,000 and 10,000 layers on both MNIST and CIFAR-10 (Fig. 1).","3.4. Training 10,000-layers: Delta-Orthogonal Initialization.",[0],[0]
"All these networks learn surprisingly quickly and, remarkably, the learning time measured in number of training epochs is independent of depth.","3.4. Training 10,000-layers: Delta-Orthogonal Initialization.",[0],[0]
"Furthermore, our experimental results match well with the predicted benefits of this initialization: 99% test accuracy on MNIST for a 10,000-layer network, and 82% on CIFAR-10.","3.4. Training 10,000-layers: Delta-Orthogonal Initialization.",[0],[0]
"To isolate the benefits of the Delta-Orthogonal init, we also train a 2048- layer CNN (Fig. 3) using the spatially-uniform orthogonal initialization proposed in Section 2.3; the testing accuracy is about 70%.","3.4. Training 10,000-layers: Delta-Orthogonal Initialization.",[0],[0]
Note that the test accuracy using (spatially uniform),"3.4. Training 10,000-layers: Delta-Orthogonal Initialization.",[0],[0]
Gaussian (non-orthogonal) initialization is already below 70% when the depth is 259.,"3.4. Training 10,000-layers: Delta-Orthogonal Initialization.",[0],[0]
"In this work, we developed a theoretical framework based on mean field theory to study the propagation of signals in deep convolutional neural networks.",4. Discussion,[0],[0]
"By examining the necessary conditions for signals to flow both forward and backward through the network without attenuation, we derived an initialization scheme that facilitates training of vanilla CNNs of unprecedented depths.",4. Discussion,[0],[0]
"We presented an algorithm for the generation of random orthogonal convolutional kernels, an ingredient that is necessary to enable dynamical isometry, i.e. good conditioning of the network’s input-output Jacobian.",4. Discussion,[0],[0]
"In contrast to the fully-connected case, signal propagation in CNNs is intrinsically multi-dimensional – we showed how to decompose those signals into independent Fourier modes and how to promote uniform signal propagation across them.",4. Discussion,[0],[0]
"By leveraging these various theoretical insights, we demonstrated empirically that it is possible to train vanilla CNNs with 10,000 layers or more.
",4. Discussion,[0],[0]
Our results indicate that we have removed all the major fundamental obstacles to training arbitrarily deep vanilla convolutional networks.,4. Discussion,[0],[0]
"In doing so, we have layed the groundwork to begin addressing some outstanding questions in the deep learning community, such as whether depth alone can deliver enhanced generalization performance.",4. Discussion,[0],[0]
"Our initial results suggest that past a certain depth, on the order of tens or hundreds of layers, the test performance for vanilla convolutional architecture saturates.",4. Discussion,[0],[0]
"These observations suggest that architectural features such as residual connections and batch normalization are likely to play an important role in defining a good model class, rather than simply enabling efficient training.",4. Discussion,[0],[0]
"We thank Xinyang Geng, Justin Gilmer, Alex Kurakin, Jaehoon Lee, Hoang Trieu Trinh, and Greg Yang for useful discussions and feedback.",Acknowledgements,[0],[0]
"In recent years, state-of-the-art methods in computer vision have utilized increasingly deep convolutional neural network architectures (CNNs), with some of the most successful models employing hundreds or even thousands of layers.",abstractText,[0],[0]
A variety of pathologies such as vanishing/exploding gradients make training such deep networks challenging.,abstractText,[0],[0]
"While residual connections and batch normalization do enable training at these depths, it has remained unclear whether such specialized architecture designs are truly necessary to train deep CNNs.",abstractText,[0],[0]
"In this work, we demonstrate that it is possible to train vanilla CNNs with ten thousand layers or more simply by using an appropriate initialization scheme.",abstractText,[0],[0]
"We derive this initialization scheme theoretically by developing a mean field theory for signal propagation and by characterizing the conditions for dynamical isometry, the equilibration of singular values of the input-output Jacobian matrix.",abstractText,[0],[0]
These conditions require that the convolution operator be an orthogonal transformation in the sense that it is norm-preserving.,abstractText,[0],[0]
We present an algorithm for generating such random initial orthogonal convolution kernels and demonstrate empirically that they enable efficient training of extremely deep architectures.,abstractText,[0],[0]
"Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks",title,[0],[0]
"Recurrent Neural Networks (RNNs) (Rumelhart et al., 1986; Elman, 1990) have found widespread use across a variety of domains from language modeling (Mikolov et al., 2010; Kiros et al., 2015; Jozefowicz et al., 2016) and machine translation (Bahdanau et al., 2014) to speech recogni-
*",1. Introduction,[0],[0]
Equal contribution 1Google 2Google Brain.,1. Introduction,[0],[0]
"Correspondence to: Minmin Chen <minminc@google.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
tion (Graves et al., 2013) and recommendation systems (Hidasi et al., 2015; Wu et al., 2017).",1. Introduction,[0],[0]
"However, RNNs as originally proposed are difficult to train and are rarely used in practice.",1. Introduction,[0],[0]
"Instead, variants of RNNs - such as Long ShortTerm Memory (LSTM) networks (Hochreiter & Schmidhuber, 1997) and Gated Recurrent Units (GRU) (Chung et al., 2014) - that feature various forms of “gating” perform significantly better than their vanilla counterparts.",1. Introduction,[0],[0]
"Often, these models must be paired with techniques such as normalization layers (Ioffe & Szegedy, 2015b; Ba et al., 2016) and gradient clipping (Pascanu et al., 2013) to achieve good performance.
",1. Introduction,[0],[0]
"A rigorous explanation for the remarkable success of gated recurrent networks remains illusive (Jozefowicz et al., 2015; Greff et al., 2017).",1. Introduction,[0],[0]
"Recent work (Collins et al., 2016) provides empirical evidence that the benefits of gating are mostly rooted in improved trainability rather than increased capacity or expressivity.",1. Introduction,[0],[0]
The problem of disentangling trainability from expressivity is widespread in machine learning since state-of-the-art architectures are nearly always the result of sparse searches in high dimensional spaces of hyperparameters.,1. Introduction,[0],[0]
"As a result, we often mistake trainability for expressivity.",1. Introduction,[0],[0]
"Seminal early work (Glorot & Bengio; Bertschinger et al.) showed that a major hindrance to trainability was the vanishing and exploding of gradients.
",1. Introduction,[0],[0]
"Recently, progress has been made in the feed-forward setting (Schoenholz et al., 2017; Pennington et al., 2017; Yang & Schoenholz, 2017) by developing a theory of both the forward-propagation of signal and the backwardpropagation of gradients.",1. Introduction,[0],[0]
This theory is based on studying neural networks whose weights and biases are randomly distributed.,1. Introduction,[0],[0]
"This is equivalent to studying the behavior of neural networks after random initialization or, equivalently, to studying the prior over functions induced by a particular choice of hyperparameters (Lee et al., 2017).",1. Introduction,[0],[0]
"It was shown that randomly initialized neural networks are trainable if three conditions are satisfied: (1) the size of the output of the network is finite for finite inputs, (2) the output of the network is sensitive to changes in the input, and (3) gradients neither explode nor vanish.",1. Introduction,[0],[0]
"Moreover, neural networks achieving dynamical isometry, i.e. having input-output Jacobian matrices that are well-conditioned, were shown to ar X iv :1
80 6.
05 39
4v 2
[ st
at .M
L ]
1 5
A ug
2 01
8
train orders of magnitude faster than networks that do not.
",1. Introduction,[0],[0]
"In this work, we combine mean field theory and random matrix theory to extend these results to the recurrent setting.",1. Introduction,[0],[0]
We will be particularly focused on understanding the role that gating plays in trainability.,1. Introduction,[0],[0]
"As we will see, there are a number of subtleties that must be addressed for (gated) recurrent networks that were not present in the feed-forward setting.",1. Introduction,[0],[0]
"To clarify the discussion, we will therefore contrast vanilla RNNs with a gated RNN cell, that we call the minimalRNN, which is significantly simpler than LSTMs and GRUs but implements a similar form of gating.",1. Introduction,[0],[0]
"We expect the framework introduced here to be applicable to more complicated gated architectures.
",1. Introduction,[0],[0]
The first main contribution of this paper is the development of a mean field theory for forward propagation of signal through vanilla RNNs and minimalRNNs.,1. Introduction,[0],[0]
"In doing so, we identify a theory of the maximum timescale over which signal can propagate in each case.",1. Introduction,[0],[0]
"Next, we produce a random matrix theory for the end-to-end Jacobian of the minimalRNN.",1. Introduction,[0],[0]
"As in the feed-forward setting, we establish that the duality between the forward propagation of signal and the backward propagation of gradients persists in the recurrent setting.",1. Introduction,[0],[0]
We then show that our theory is indeed predictive of trainability in recurrent neural networks by comparing the maximum trainable number of steps of RNNs with the timescale predicted by the theory.,1. Introduction,[0],[0]
"Overall, we find remarkable alignment between theory and practice.",1. Introduction,[0],[0]
"Additionally, we develop a closed-form initialization procedure for both networks and show that on a variety of tasks RNNs initialized to be dynamically isometric are significantly easier to train than those lacking this property.
",1. Introduction,[0],[0]
"Corroborating the experimental findings of Collins et al. (2016), we show that both signal propagation and dynamical isometry in vanilla RNNs is far more precarious than in the case of the minimalRNN.",1. Introduction,[0],[0]
"Indeed the vanilla RNN achieves dynamical isometry only if the network is initialized with orthogonal weights at the boundary between order-and-chaos, a one-dimensional line in parameter space.",1. Introduction,[0],[0]
"Owing to its gating mechanism, the minimalRNN on the other hand enjoys a robust multi-dimensional subspace of good initializations which all enable dynamical isometry.",1. Introduction,[0],[0]
"Based on these insights, we conjecture that more complex gated recurrent neural networks also benefit from the similar effects.",1. Introduction,[0],[0]
"Identity and Orthogonal initialization schemes have been identified as a promising approach to improve trainability of deep neural networks (Le et al., 2015; Mishkin & Matas, 2015).",2. Related Work,[0],[0]
"Additionally, Arjovsky et al. (2016); Hyland & Rätsch (2017); Xie et al. (2017) advocate going beyond initialization to constrain the transition matrix to be orthog-
onal throughout the entire learning process either through re-parametrisation or by constraining the optimization to the Stiefel manifold (Wisdom et al., 2016).",2. Related Work,[0],[0]
"However, as was pointed out in Vorontsov et al. (2017), strictly enforcing orthogonality during training may hinder training speed and generalization performance.",2. Related Work,[0],[0]
"While these contributions are similar to our own, in the sense that they attempt to construct networks that feature dynamical isometry, it is worth noting that orthogonal weight matrices do not guarantee dynamical isometry.",2. Related Work,[0],[0]
This is due to the nonlinear nature of deep neural networks as shown in Pennington et al. (2017).,2. Related Work,[0],[0]
"In this paper we continue this trend and show that orthogonality has little impact on the conditioning of the Jacobian (and so trainability) in gated RNNs.
",2. Related Work,[0],[0]
The notion of “edge of chaos” initialization has been explored previously especially in the case of recurrent neural networks.,2. Related Work,[0],[0]
Bertschinger et al.; Glorot & Bengio propose edge-of-chaos initialization schemes that they show leads to improved performance.,2. Related Work,[0],[0]
"Additionally, architectural innovations such as batch normalization (Ioffe & Szegedy, 2015a), orthogonal matrix initialization (Saxe et al., 2013), random walk initialization (Sussillo & Abbott, 2014), composition kernels (Daniely et al., 2016), or residual network architectures (He et al., 2015) all share a common goal of stabilizing gradients and improving training dynamics.
",2. Related Work,[0],[0]
There is a long history of applying mean field-like approaches to understand the behavior of neural networks.,2. Related Work,[0],[0]
"Indeed several pieces of seminal work used statistical physics (Derrida & Pomeau; Sompolinsky et al., 1988) and Gaussian Processes (Neal, 2012) to show that neural networks exhibit remarkable regularity as the width of the network gets large.",2. Related Work,[0],[0]
"Mean field theory also has long been used to study Boltzmann machines (Ackley et al.) and sigmoid belief networks (Saul et al., 1996).",2. Related Work,[0],[0]
"More recently, there has been a revitalization of mean field theory to explore questions of trainability and expressivity in fully-connected networks and residual networks (Poole et al., 2016; Schoenholz et al., 2017; Yang & Schoenholz, 2017; Schoenholz et al., 2017; Karakida et al., 2018; Hayou et al., 2018; Hanin & Rolnick, 2018; Yang & Schoenholz, 2018).",2. Related Work,[0],[0]
Our approach will closely follow these later contributions and extend many of their techniques to the case of recurrent networks with gating.,2. Related Work,[0],[0]
"Beyond mean field theory, there have been several attempts in understanding signal propagation in RNNs, e.g., using Gers̆gorin circle theorem (Zilly et al., 2016) or time invariance (Tallec & Ollivier, 2018).",2. Related Work,[0],[0]
We begin by developing a mean field theory for vanilla RNNs and discuss the notion of dynamical isometry.,3. Theory and Critical Initialization,[0],[0]
"Afterwards, we move on to a simple gated architecture to explain the role of gating in facilitating signal propagation in RNNs.",3. Theory and Critical Initialization,[0],[0]
"Vanilla RNNs are described by the recurrence relation,
et = Wht−1 + V xt + b ht = φ(et).",3.1. Vanilla RNN,[0],[0]
"(1)
Here xt ∈ RM is the input, et ∈ RN is the pre-activation, and ht ∈ RN is the hidden state after applying an arbitrary activation function φ :",3.1. Vanilla RNN,[0],[0]
R → R. For the purposes of this discussion we set φ = tanh.,3.1. Vanilla RNN,[0],[0]
"Furthermore, W ∈ RN×N and V ∈ RN×M are weight matrices that multiply the hidden state and inputs respectively and b ∈ RN is a bias.
",3.1. Vanilla RNN,[0],[0]
"Next, we apply mean-field theory to vanilla RNNs following a similar strategy introduced in (Poole et al., 2016; Schoenholz et al., 2017).",3.1. Vanilla RNN,[0],[0]
"At the level of mean-field theory, vanilla RNNs will prove to be intimately related to feed-forward networks and so this discussion proceeds analogously.",3.1. Vanilla RNN,[0],[0]
"For a more detailed discussion, see these earlier studies.
",3.1. Vanilla RNN,[0],[0]
"Consider two sequences of inputs {xt1} and {xt2}, described by the covariance matrix Rt ∈ R2×2 with Rtab = 1 ME[xa · xb], a, b ∈ {1, 2}.",3.1. Vanilla RNN,[0],[0]
"To simplify notation, we assume the input sequences have been standardized so that Rt11 = R t 22 = R independent of time.",3.1. Vanilla RNN,[0],[0]
"This allows us to write Rt = RΣt, where Σt is a matrix whose diagonal terms are 1 and whose off-diagonal terms are the cosine similarity between the inputs at time t. These sequences are then passed into two identical copies of an RNN to produce two corresponding pre-activation sequences {et1} and {et2}.",3.1. Vanilla RNN,[0],[0]
"As in Poole et al. (2016) we let the weights and biases be Gaussian distributed so that Wij ∼ N (0, σ2w/N), Vij ∼ N (0, σ2v/M), and bi ∼ N (µb, σ2b )1, and we consider the wide network limit, N →∞.",3.1. Vanilla RNN,[0],[0]
"As in the fully-connected setting, we would like to invoke the Central Limit Theorem (CLT) to conclude that the pre-activations of hidden states are jointly Gaussian distributed.",3.1. Vanilla RNN,[0],[0]
"Unfortunately, the CLT is violated in the recurrent setting as ht−1 is correlated with W due to weight sharing between steps of the RNN.
",3.1. Vanilla RNN,[0],[0]
"To make progress, we proceed by developing the theory of signal propagation for RNNs with untied weights.",3.1. Vanilla RNN,[0],[0]
"This allows for several simplifications, including the application of the CLT to conclude that etia are jointly Gaussian distributed,
[eti1, e t j2] T ∼ N (µb1, qtδij), i, j ∈ {1, · · · , N}
where the covariance matrix qt ∈ R2×2 is independent of neuron index, i. We explore the ramifications of this approximation by comparing simulations of RNNs with tied and untied weights.",3.1. Vanilla RNN,[0],[0]
"Overall, we will see that while ignoring weight tying leads to quantitative differences between theory and experiment, it does not change the qualitative picture that emerges.",3.1. Vanilla RNN,[0],[0]
See figs.,3.1. Vanilla RNN,[0],[0]
"1 and 2 for verification.
",3.1. Vanilla RNN,[0],[0]
"With this approximation in mind, we will now quantify how the pre-activation hidden states {et1} and {et2} evolve by
1in practice we will set µb = 0 for vanillaRNN.
deriving the recurrence relation of the covariance matrix qt from the recurrence on et in eq.",3.1. Vanilla RNN,[0],[0]
(1).,3.1. Vanilla RNN,[0],[0]
"Using identical arguments to Poole et al. (2016) one can show that,
qt = σ2w ∫ Dqt−1z φ(z)φ(z)",3.1. Vanilla RNN,[0],[0]
>,3.1. Vanilla RNN,[0],[0]
"+ σ2vRΣ t + σ2bI. (2)
",3.1. Vanilla RNN,[0],[0]
"where z = [z1, z2]>, and∫ Dqz = 1
2π √ |q|
∫ dze−(z−µb1) T q−1(z−µb1) (3)
is a Gaussian measure with covariance matrix q. By symmetry, our normalization allows us to define qt11 = q t 22 = q t to be the magnitude of the pre-activation hidden state and ct = qt12/q
t to be the cosine similarity between the hidden states.",3.1. Vanilla RNN,[0],[0]
"We will be particularly concerned with understanding the dynamics of the cosine similarity, ct.
",3.1. Vanilla RNN,[0],[0]
"In feed-forward networks, the inputs dictate the initial value of the cosine similarity, c0 and then the evolution of ct is determined solely by the network architecture.",3.1. Vanilla RNN,[0],[0]
"By contrast in recurrent networks, inputs perturb ct at each timestep.",3.1. Vanilla RNN,[0],[0]
"Analyzing the dynamics of ct for arbitrary Σt is therefore challenging, however significant insight can be gained by studying the off-diagonal entries of eq.",3.1. Vanilla RNN,[0],[0]
(2) for Σt = Σ independent of time.,3.1. Vanilla RNN,[0],[0]
"In the case of time-independent Σt, as t → ∞ both qt → q∗ and ct → c∗ where q∗ and c∗ are fixed points of the variance of the pre-activation hidden state and the cosine-similarity between pre-activation hidden states respectively.",3.1. Vanilla RNN,[0],[0]
"As was discussed previously (Poole et al., 2016; Schoenholz et al., 2017), the dynamics of qt are generally uninteresting provided q∗ is finite.",3.1. Vanilla RNN,[0],[0]
"We therefore choose to normalize the hidden state such that q0 = q∗ which implies that qt = q∗ independent of time.
",3.1. Vanilla RNN,[0],[0]
"In this setting it was shown in Schoenholz et al. (2017) that in the vicinity of a fixed point, the off-diagonal term in eq.",3.1. Vanilla RNN,[0],[0]
(2) can be expanded to lowest order in t = c∗,3.1. Vanilla RNN,[0],[0]
"− ct to give the linearized dynamics, t = χc∗ t−1 where
χc∗ = σ 2 w ∫ Dq∗zφ ′(z1)φ ′(z2).",3.1. Vanilla RNN,[0],[0]
"(4)
These dynamics have the solution t = χt−t0c∗ t0 where t0 is the time when ct is close enough to c∗ for the linear approximation to be valid.",3.1. Vanilla RNN,[0],[0]
If χc∗ < 1 it follows that ct approaches c∗ exponentially quickly over a timescale τ = −1/,3.1. Vanilla RNN,[0],[0]
logχc∗ and c∗ is called a stable fixed point.,3.1. Vanilla RNN,[0],[0]
"When ct gets too close to c∗ to be distinguished from it to within numerical precision, information about the initial inputs has been lost.",3.1. Vanilla RNN,[0],[0]
"Thus, τ sets the maximum timescale over which we expect the RNN to be able to remember information.",3.1. Vanilla RNN,[0],[0]
If χc∗ > 1 then ct gets exponentially farther from c∗ over time and c∗ is an unstable fixed point.,3.1. Vanilla RNN,[0],[0]
"In this case, for the activation function considered here, another fixed point that
is stable will emerge.",3.1. Vanilla RNN,[0],[0]
"Note that χc∗ is independent of Σ and so the dynamics of ct near c∗ do not depend on Σ.
In vanilla fully-connected networks c∗",3.1. Vanilla RNN,[0],[0]
"= 1 is always a fixed point of ct, but it is not always stable.",3.1. Vanilla RNN,[0],[0]
"Indeed, it was shown that these networks exhibit a phase transition where c∗ = 1 goes from being a stable fixed point to an unstable one as a function of the network’s hyperparameters.",3.1. Vanilla RNN,[0],[0]
This is known as the order-to-chaos transition and it occurs exactly when χ1 = 1.,3.1. Vanilla RNN,[0],[0]
"Since τ = −1/ log(χ1), signal can propagate infinitely far at the boundary between order and chaos.",3.1. Vanilla RNN,[0],[0]
Comparing the diagonal and off-diagonal entries of eq.,3.1. Vanilla RNN,[0],[0]
"(2), we see that in recurrent networks, c∗ = 1 is a fixed point only when Σ12",3.1. Vanilla RNN,[0],[0]
"= 1, and in this case the discussion is identical to the feed-forward setting.",3.1. Vanilla RNN,[0],[0]
"When Σ12 < 1, it is easy to see that c∗ < 1 since if ct = 1 at some time t then ct+1 = 1−σ2vR(1−Σ12)/q∗ < 1.",3.1. Vanilla RNN,[0],[0]
We see that in recurrent networks noise from the inputs destroys the ordered phase and there is no ordered-to-chaos critical point.,3.1. Vanilla RNN,[0],[0]
"As a result we should expect the maximum timescale over which memory may be stored in vanilla RNNs to be fundamentally limited by noise from the inputs.
",3.1. Vanilla RNN,[0],[0]
"The end-to-end Jacobian of a vanilla RNN with untied weights is in fact formally identical to the input-output Jacobian of a feedforward network, and thus the results from (Pennington et al., 2017) regarding conditions for dynamical isometry apply directly.",3.1. Vanilla RNN,[0],[0]
"In particular, dynamical isometry is achieved with orthogonal state-to-state transition matrices W , tanh non-linearities, and small values of q∗.",3.1. Vanilla RNN,[0],[0]
"Perhaps surprisingly, these conclusions continue to be valid if the assumption of untied weights is relaxed.",3.1. Vanilla RNN,[0],[0]
"To understand why this is the case, consider the example of a linear network.",3.1. Vanilla RNN,[0],[0]
"For untied weights, the end-to-end Jacobian is given by Ĵ = ∏T t=1",3.1. Vanilla RNN,[0],[0]
"Wt, while for tied weights the Jacobian is given by J = W T .",3.1. Vanilla RNN,[0],[0]
"It turns out that as N →∞ there is sufficient self-averaging to overcome the dependencies induced by weight tying and the asymptotic singular value distributions of Ĵ and J are actually identical (Haagerup & Larsen, 2000).",3.1. Vanilla RNN,[0],[0]
"To study the role of gating, we introduce the minimalRNN which is simpler than other gated RNN architectures but nonetheless features the same gating mechanism.",3.2.1. MEAN-FIELD THEORY,[0],[0]
"A sequence of inputs xt ∈ RM , is first mapped to the hidden space through x̃t = Φ(xt)2.",3.2.1. MEAN-FIELD THEORY,[0],[0]
"From here on, we refer to x̃t ∈ RN as the inputs to minimalRNN.",3.2.1. MEAN-FIELD THEORY,[0],[0]
"The minimalRNN
2Φ(·) here can be any highly flexible functions such as a feed-forward network.",3.2.1. MEAN-FIELD THEORY,[0],[0]
"In our experiments, we take Φ(·) to be a fully connected layer with tanh activation, that is, Φ(xt) = tanh(Wxx t).
is then described by the recurrence relation,
et = Wht−1 + V x̃t + b ut = σ(et) (5)
ht = ut ht−1 + (1− ut) x̃t
where et ∈ RN is the pre-activation to the gating function,",3.2.1. MEAN-FIELD THEORY,[0],[0]
ut ∈ RN the update gate and ht ∈ RN the hidden state.,3.2.1. MEAN-FIELD THEORY,[0],[0]
"The minimalRNN retains the most essential gate in LSTMs (Jozefowicz et al., 2015; Greff et al., 2017) and achieves competitive performance.",3.2.1. MEAN-FIELD THEORY,[0],[0]
"The simplified update of this cell on the other hand, enables us to pinpoint the role of gating in a more controlled setting.
",3.2.1. MEAN-FIELD THEORY,[0],[0]
"As in the previous case we consider two sequences of inputs to the network, {x̃t1} and {x̃t2}.",3.2.1. MEAN-FIELD THEORY,[0],[0]
"We take Wij ∼ N (0, σ2w/N), Vij ∼ N (0, σ2v/N) and bi ∼ N (µb, σ2b ).",3.2.1. MEAN-FIELD THEORY,[0],[0]
"By analogy to the vanilla case, we can make the mean field approximation that the etia are jointly Gaussian distributed with covariance matrix qtδij ∈ R2×2.",3.2.1. MEAN-FIELD THEORY,[0],[0]
"Here,
qt = σ2wQ t−1 + σ2vR t + σ2bI (6)
where we have defined Qt as the second-moment matrix with Qtab = E[htiahtib].",3.2.1. MEAN-FIELD THEORY,[0],[0]
3,3.2.1. MEAN-FIELD THEORY,[0],[0]
"As in the vanilla case, Rt is the covariance between inputs so that Rtab = 1 NE[x̃a · x̃b].
",3.2.1. MEAN-FIELD THEORY,[0],[0]
"We note that Rt is fixed by the input, but it remains for us to work out Qt.",3.2.1. MEAN-FIELD THEORY,[0],[0]
"We find that (see SI section B),
",3.2.1. MEAN-FIELD THEORY,[0],[0]
"Qt = Qt−1 ∫ Dqtz σ(z)σ(z) > (7)
+ Rt ∫ Dqtz (1− σ(z))(1− σ(z))",3.2.1. MEAN-FIELD THEORY,[0],[0]
">
Here we assume that the expectation factorizes so that ht−1 and ut are approximately independent.",3.2.1. MEAN-FIELD THEORY,[0],[0]
"We believe this approximation becomes exact in the N →∞ limit.
",3.2.1. MEAN-FIELD THEORY,[0],[0]
We choose to normalize the data in a similar manner to the vanilla case so that Rt11 = R t 22 = R independent of time.,3.2.1. MEAN-FIELD THEORY,[0],[0]
"An immediate consequence of this normalization is that Qt11 = Q t 22 = Q t and qt11 = q t 22 = q
t.",3.2.1. MEAN-FIELD THEORY,[0],[0]
We then write Ct = Qt12/Q t and ct = qt12/q t as the cosine similarities between the hidden states and the pre-activations respectively.,3.2.1. MEAN-FIELD THEORY,[0],[0]
"With this normalization, we can work out the mean-field recurrence relation characterizing the covariance matrix for the minimalRNN.",3.2.1. MEAN-FIELD THEORY,[0],[0]
This analysis can be done by deriving the recurrence relation for either Qt or qt.,3.2.1. MEAN-FIELD THEORY,[0],[0]
"We will choose to study the dynamics of qt, however the two are trivially related by eq. (6).",3.2.1. MEAN-FIELD THEORY,[0],[0]
"In SI section C, we analyze the dynamics of the diagonal term in the recurrence relation and prove that there is always a fixed point at some q∗.",3.2.1. MEAN-FIELD THEORY,[0],[0]
"In SI section D, we compute the depth scale over which qt approaches q∗.",3.2.1. MEAN-FIELD THEORY,[0],[0]
"However, as in the case of the vanillaRNN, the dynamics of q∗ are generally uninteresting.
",3.2.1. MEAN-FIELD THEORY,[0],[0]
"3ht will be centered under mean field approximation if h0 is initialized with mean zero.
",3.2.1. MEAN-FIELD THEORY,[0],[0]
"We now turn our attention to the dynamics of the cosine similarity between the pre-activations, ct.",3.2.1. MEAN-FIELD THEORY,[0],[0]
"As in the case of vanilla RNNs, we note that qt approaches q∗ quickly relative to the dynamics of ct.",3.2.1. MEAN-FIELD THEORY,[0],[0]
We therefore choose to normalize the hidden state of the RNN so that Q0 = Q∗ in which case both Qt = Q∗ and qt = q∗ independent of time.,3.2.1. MEAN-FIELD THEORY,[0],[0]
"From eq. (6) and (7) it follows that the cosine similarity of the pre-activation evolves as,
ct =",3.2.1. MEAN-FIELD THEORY,[0],[0]
"[ ct−1 + (σ2w − σ2v)ρt−1 ] ∫ Dqt−1z σ(z1)σ(z2)
",3.2.1. MEAN-FIELD THEORY,[0],[0]
"− 2σ2wρt−1 ∫ Dqt−1z σ(z1) + σ 2 wρ t−1 + σ2vρ t (8)
where we have defined ρt = RΣt12/q ∗.",3.2.1. MEAN-FIELD THEORY,[0],[0]
"As in the case of the vanilla RNN, we can study the behavior of ct in the vicinity of a fixed point, c∗. By expanding eq.",3.2.1. MEAN-FIELD THEORY,[0],[0]
(8) to lowest order in t = c∗,3.2.1. MEAN-FIELD THEORY,[0],[0]
"− ct we arrive at a linearized recurrence relation that has an exponential solution t+1 = χc∗ t where here,
χc∗ = ∫ Dq∗zσ(z1)σ(z2)",3.2.1. MEAN-FIELD THEORY,[0],[0]
"(9)
+ ( q∗c∗ + (σ2w − σ2v)RΣ12 ) ∫",3.2.1. MEAN-FIELD THEORY,[0],[0]
"Dq∗zσ ′(z1)σ ′(z2).
",3.2.1. MEAN-FIELD THEORY,[0],[0]
The discussion above in the vanilla case carries over directly to the minimalRNN with the appropriate replacement of χc∗ .,3.2.1. MEAN-FIELD THEORY,[0],[0]
"Unlike in the case of the vanilla RNN, here we see that χc∗ itself depends on Σ12.
",3.2.1. MEAN-FIELD THEORY,[0],[0]
Again c∗ = 1 is a fixed point of the dynamics only when Σ12 = 1.,3.2.1. MEAN-FIELD THEORY,[0],[0]
"In this case, the minimalRNN experiences an order-to-chaos phase transition when χ1 = 1 at which point the maximum timescale over which signal can propagate goes to infinity.",3.2.1. MEAN-FIELD THEORY,[0],[0]
"Similar to the vanilla RNN, when Σ12 < 1, we expect that the phase transition will be destroyed and the maximum duration of signal propagation will be severely limited.",3.2.1. MEAN-FIELD THEORY,[0],[0]
"However, in a significant departure from the vanilla case, when µb → ∞ we notice that σ(z + µb) → 1, and σ′(z+µb)→ 0 for all z. Considering eq. (9) we notice that in this regime χc∗ → 1 independent of Σ12.",3.2.1. MEAN-FIELD THEORY,[0],[0]
"In other words, gating allows for arbitrarily long term signal propagation in recurrent neural networks independent of Σ12.
",3.2.1. MEAN-FIELD THEORY,[0],[0]
We explore agreement between our theory and MC simulations of the minimalRNN in fig.,3.2.1. MEAN-FIELD THEORY,[0],[0]
1.,3.2.1. MEAN-FIELD THEORY,[0],[0]
"In this set of experiments, we consider inputs such that Σt12 = 0 for t < 10 and Σt12 = 1 for t ≥ 10.",3.2.1. MEAN-FIELD THEORY,[0],[0]
"Fig. 1 (a,c,d) show excellent quantitative agreement between our theory and MC simulations.",3.2.1. MEAN-FIELD THEORY,[0],[0]
In fig.,3.2.1. MEAN-FIELD THEORY,[0],[0]
"1 (a,b) we compare the MC simulations of the minimalRNN with and without weight tying.",3.2.1. MEAN-FIELD THEORY,[0],[0]
"While we observe that for many choices of hyperparameters the untied weight approximation is quite good (particularly when c∗ ≈ 1), deeper into the chaotic phase the quantitative agreement between breaks down.",3.2.1. MEAN-FIELD THEORY,[0],[0]
"Nonetheless, we observe that the untied approximation describes the qualitative behavior of the real
minimalRNN overall.",3.2.1. MEAN-FIELD THEORY,[0],[0]
In fig.,3.2.1. MEAN-FIELD THEORY,[0],[0]
"1 (e) we plot the timescale for signal propagation for Σ12 = 1, 0.99, and 0 for the minimalRNN with identical choices of hyperparameters.",3.2.1. MEAN-FIELD THEORY,[0],[0]
"We see that while τ → ∞ as µb gets large independent of Σ12, a critical point at µb = 0 is only observed when Σ12 = 1.",3.2.1. MEAN-FIELD THEORY,[0],[0]
"In the previous subsection, we derived a quantity χ1 that defines the boundary between the ordered and the chaotic phases of forward propagation.",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
Here we show that it also defines the boundary between exploding and vanishing gradients.,3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"To see this, consider the Jacobian of the state-to-state transition operator,
Jt = ∂ht+1
∂ht = Dut + Dσ′(et) (ht−1−zt)W , (10)
where Dx denotes a diagonal matrix with x along its diagonal.",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"We can compute the expected norm-squared of back-propagated error signals, which measures the growth or shrinkage of gradients.",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"It is equal to the mean-squared singular value of the Jacobian (Poole et al., 2016; Schoenholz et al., 2017) or the first moment of JtJTt ,
1
N E[tr(JtJTt )] = E[(ut1)2] + σ2wE[σ′(et1)2(h",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"t−1 1 − zt1)2],
(11) where we have used the fact that the elements of ut, ht and zt are i.i.d.",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"Since we assume convergence to the fixed point, these distributions are independent of t and it is easy to see that 1NE[tr(JtJ T t )]",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
= χ1.,3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"The variance of backpropagated error signals through T time steps is therefore 1 NE[tr(JJ
T )] = χT1 .",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"As such, the constraint χ1 = 1 defines the boundary between phases of exponentially exploding and exponentially vanishing gradient norm (variance).",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"Note that unlike in the case of forward signal propagation, in the case of backpropagation this is independent of Σ.
As argued in (Pennington et al., 2017; 2018), controlling the variance of back-propagated gradients is necessary but not sufficient to guarantee trainability, especially for very deep networks.",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"Beyond the first moment, the entire distribution of eigenvalues of JJT (or of singular values of J) is relevant.",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"Indeed, it was found in (Pennington et al., 2017; 2018) that enabling dynamical isometry, namely the condition that all singular values of J are close to unity, can drastically improve training speed for very deep feed-forward networks.
",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"Following (Pennington et al., 2017; 2018), we use tools from free probability theory to compute the variance σ2JJT of the limiting spectral density of JJT ; however, unlike previous work, in our case the relevant matrices are not symmetric and therefore we must invoke tools from nonHermitian free probability, see (Cakmak, 2012) for a review.",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"As in previous section, we make the simplifying assumption that the weights are untied, relying on the same motivations
given in section 3.1.",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"Using these tools, an un-illuminating calculation reveals that,
σ2JJT = χ 2T 1
( 1 + T
2(µ1 − s1)µ2 + σ21 + σ22 χ21
) , (12)
where,
χ1 = µ1 + µ2 (13)
µ1 =
∫",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"Dz σ2( √ q∗z + µb)
σ21 = −µ21 + ∫",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"Dz σ4( √ q∗z + µb)
µ2 = σ 2 w(Q ∗ +R)
∫",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"Dz [σ′( √ q∗z + µb)] 2
σ22 = −µ22 + σ4w((Q∗)2 +R2) ∫",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"Dz [σ′( √ q∗z + µb)] 4
and s1 is the first term in the Taylor expansion of the Stransform of the eigenvalue distribution of WWT (Pennington et al., 2018).",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"For example, for Gaussian matrices, s1 = −1 and for orthogonal matrices s1 = 0.
",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
Some remarks are in order about eq.,3.2.2. DYNAMICAL ISOMETRY,[0],[0]
(12).,3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"First, we note the duality between the forward and backward signal propagation (eq. (9) and eq.",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
(13)).,3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"For critical initializations, χ1 = 1, so σ2JJT does not grow exponentially, but it still grows linearly with T .",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"This situation is entirely analogous to the feed-forward analysis of (Pennington et al., 2017; 2018).",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"In the case of the vanilla RNN, the coefficient of the linear term is proportion to q∗, and can only be reduced by taking the weight and bias variances (σ2w, σ 2 b )→ (1, 0).",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"A crucial difference in the minimalRNN is that the coefficient of the linear term can be made arbitrarily small by simply adjusting the bias mean µb to be positive, which will send µ2 → 0 and µ1 → 1 independent of Σ. Therefore the conditions for dynamical isometry decouple from the weight
and bias variances, implying that trainability can occur for a higher-dimensional, more robust, slice of parameter space.",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"Moreover, the value of s1 has no effect on the capacity of the minimalRNN to achieve dynamical isometry.",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"We believe these are fundamental reasons why gated cells such as the minimalRNN perform well in practice.
",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"Algorithm 1 describes the procedure to find σ2w, σ 2 v and σ 2 b to achieve χ1 condition for minimalRNN.",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"Given σ2w, σ 2 v , σ 2 b , we then construct the weight matrices and biases accordingly.",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"Q∗ is used to initialize the h0 to avoid transient phase.
",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"Algorithm 1: Critical initialization for minimalRNNs Require: q∗, µb, R
1: E[u2]← ∫",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"Dz σ2( √ q∗z + µb)
2: E[(1− u)2]← ∫ Dz (1− σ( √ q∗z + µb))",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
2 3: Q∗ ← R · E[(1− u)2]/(1− E[u2]) (eq.(7)) 4: E[u′2]← ∫ Dz[σ′( √ q∗z + µb)] 2 5: σ2w ← (1− E[u2])/(Q∗ +R)/E[u′2] (eq.(13)) 6: σ2b ← 0 7: σ2v ← (q∗ −Q∗σ2w − σ2b )/R,3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"Having established a theory for the behavior of random vanilla RNNs and minimalRNNs, we now discuss the connection between our theory and trainability in practice.",4. Experiments,[0],[0]
We begin by corroborating the claim that the maximum timescale over which memory can be stored in a RNN is controlled by the timescale τ identified in the previous section.,4. Experiments,[0],[0]
"We will then investigate the role of dynamical isometry in speeding up learning.
6⌧ 6⌧ 6⌧ 6⌧",4. Experiments,[0],[0]
Dataset.,4.1. Trainability,[0],[0]
"To verify the results of our theoretical calculation, we consider a task that is reflective of the theory above.",4.1. Trainability,[0],[0]
"To that end, we constructed a sequence dataset for training RNNs from MNIST (LeCun et al., 1998).",4.1. Trainability,[0],[0]
Each of the 28× 28 digit image is flattened into a vector of 784 pixels and sent as the first input to a RNN.,4.1. Trainability,[0],[0]
"We then send T random inputs xt ∼ N (0, σ2x), 0 < t < T into the RNN varying T between 10 and 1000 steps.",4.1. Trainability,[0],[0]
"As the only salient information about the digit is in the first layer, the network will need to propagate information through T layers to accurately identify the MNIST digit.",4.1. Trainability,[0],[0]
"The random inputs are drawn independently for each example and so this is a regime where Σt = 0 for all t > 0.
",4.1. Trainability,[0],[0]
We then performed a series of experiments on this task to make connection with our theory.,4.1. Trainability,[0],[0]
In each case we experimented with both tied and untied weights.,4.1. Trainability,[0],[0]
The result are shown in fig.,4.1. Trainability,[0],[0]
2.,4.1. Trainability,[0],[0]
"In the case of untied weights, we observe strong quantitative agreement between our theoretical prediction for τ and the maximum depth T where the network is still trainable.",4.1. Trainability,[0],[0]
"When the weights of the network are tied, we observe quantitative deviations between our thoery and experiments, but the overall qualitative picture remains.
",4.1. Trainability,[0],[0]
We train vanilla RNNs for 103 steps (around 10 epochs) varying σw ∈,4.1. Trainability,[0],[0]
"[0.5, 1.5] while fixing σv = 0.025.",4.1. Trainability,[0],[0]
The results of this experiment are shown in fig.,4.1. Trainability,[0],[0]
2 (a-b).,4.1. Trainability,[0],[0]
We train minimalRNNs for 102 steps (around 1 epoch) fixing σv = 1.39.,4.1. Trainability,[0],[0]
"We perform three different experiments here: 1)
varying µb ∈",4.1. Trainability,[0],[0]
"[−4, 8] with σw = 6.88 shown in fig.",4.1. Trainability,[0],[0]
"2 (c-d), 2) varying σw ∈",4.1. Trainability,[0],[0]
"[0.5, 10] with µb = 4 shown in fig.",4.1. Trainability,[0],[0]
"2 (e-f), 3) varying σw ∈",4.1. Trainability,[0],[0]
"[0.5, 10] with µb = 6 shown in fig.",4.1. Trainability,[0],[0]
2 (g-h).,4.1. Trainability,[0],[0]
Comparing fig.,4.1. Trainability,[0],[0]
"2(a,b) with fig.",4.1. Trainability,[0],[0]
"2(c,d, g,h), the minimalRNN with large depth T is trainable over a much wider range of hyperparameters than the vanillaRNN despite the fact that the network was trained for an order of magnitude less time.",4.1. Trainability,[0],[0]
Dataset.,4.2. Critical initialization,[0],[0]
"To study the impact of critical initialization on training speed, we constructed a more realistic sequence dataset from MNIST.",4.2. Critical initialization,[0],[0]
"We unroll the pixels into a sequence of T inputs, each containing 784/T pixels.",4.2. Critical initialization,[0],[0]
"We tested T = 196 and T = 784 to vary the difficulty of the tasks.
",4.2. Critical initialization,[0],[0]
Note that we are more interested in the training speed of these networks under different initialization conditions than the test accuracy.,4.2. Critical initialization,[0],[0]
"We compare the convergence speed of vanilla RNN and minimalRNN under four initialization conditions: 1) critical initialization with orthogonal weights (solid blue); 2) critical initialization with Gaussian distributed weights (sold red); 3) off-critical initialization with orthogonal weights (dotted green); 4) off-critical initialization with Gaussian distributed weights (dotted black).
",4.2. Critical initialization,[0],[0]
We fix σ2b to zero in all settings.,4.2. Critical initialization,[0],[0]
"Under critical initialization, σ2w and σ 2 v are carefully chosen to achieve χ1 = 1 as defined in eqn.(4) for vanilla RNN and eqn.(13) (detailed in algorithm 1) for minimalRNN respectively.",4.2. Critical initialization,[0],[0]
"When testing networks off criticality, we employ a common initialization
procedure in which, σ2w = 1.0 and σ 2 v = 1.0.
",4.2. Critical initialization,[0],[0]
Figure 3 summarizes our findings: there is a clear difference in training speed between models trained with critical initialization compared with models initialized far from criticality.,4.2. Critical initialization,[0],[0]
We observe two orders of magnitude difference in training speed between a critical and off-critical initialization for vanilla RNNs.,4.2. Critical initialization,[0],[0]
"While a critically initialized model reaches a test accuracy of 90% after 750 optimization steps, the offcritical nework takes over 16,000 updates.",4.2. Critical initialization,[0],[0]
A similar trend was observed for the minimalRNN.,4.2. Critical initialization,[0],[0]
This difference is even more pronounced in the case of the longer sequence with T = 784.,4.2. Critical initialization,[0],[0]
Both vanilla RNNs and minimalRNNs initialized off-criticality failed at task.,4.2. Critical initialization,[0],[0]
The well-conditioned minimalRNN trains a factor of three faster than the vanilla RNN.,4.2. Critical initialization,[0],[0]
"As predicted above, the difference in training speed between orthogonal and Gaussian initialization schemes is significant for vanilla RNNs but is insignificant for the minimalRNN.",4.2. Critical initialization,[0],[0]
This is corroborated in fig.,4.2. Critical initialization,[0],[0]
"3 (b,d) where the distribution of the weights has no impact on the training speed.",4.2. Critical initialization,[0],[0]
"We compare the minimalRNN against more complex gated RNNs such as LSTM and GRU on the Penn Tree-Bank corpus (Marcus et al., 1993).",5. Language modeling,[0],[0]
"Language modeling is a difficult task, and competitive performance is often achieved by more complicated RNN cells.",5. Language modeling,[0],[0]
"We show that the minimalRNN achieves competitive performance despite its simplicity.
",5. Language modeling,[0],[0]
"We follow the precise setup of (Mikolov et al., 2010; Zaremba et al., 2014), and train RNNs of two sizes: a small configuration with 5M parameters and a medium-sized configuration with 20M parameters 4.",5. Language modeling,[0],[0]
We report the perplexity on the validation and test sets.,5. Language modeling,[0],[0]
"We focus our comparison on single layer RNNs, however we also report perplexities for multi-layer RNNs from the literature for reference.",5. Language modeling,[0],[0]
"We
4The hidden layer size of these networks are adjusted accordingly to reach the target model size.
follow the learning schedule of Zaremba et al. (2014) and (Jozefowicz et al., 2015).",5. Language modeling,[0],[0]
"We review additional hyperparameter ranges in section F of the supplementary material.
",5. Language modeling,[0],[0]
Table 1 summarizes our results.,5. Language modeling,[0],[0]
We find that single layer RNNs perform on par with their multi-layer counterparts.,5. Language modeling,[0],[0]
"Despite being a significantly simpler model, the minimalRNN performs comparably to GRUs.",5. Language modeling,[0],[0]
"Given the closed-form critical initialization developed here that significantly boosts convergence speed, the minimalRNN might be a favorable alternative to GRUs.",5. Language modeling,[0],[0]
There is a gap in perplexity between the performance of LSTMs and minimalRNNs.,5. Language modeling,[0],[0]
We hypothesize that this is due to the removal of an independent gate on the input.,5. Language modeling,[0],[0]
The same strategy is employed in GRUs and may cause a conflict between keeping longer-range memory and updating new information as was originally pointed out by Hochreiter & Schmidhuber (1997).,5. Language modeling,[0],[0]
We have developed a theory of signal propagation for random vanilla RNNs and a simple gated RNNs.,6. Discussion,[0],[0]
We demonstrate rigorously that the theory predicts trainability of these networks and gating mechanisms allow for a significantly larger trainable region.,6. Discussion,[0],[0]
We are planning to extend the theory to more complicated RNN cells as well as RNNs with multiple layers.,6. Discussion,[0],[0]
We thank Jascha Sohl-Dickstein and Greg Yang for helpful discussions and Ashish Bora for many contributions to early stages of this project.,Acknowledgements,[0],[0]
Figure Supp.1.,A. MinimalRNN Architecture,[0],[0]
Model architecture of minimalRNN.,A. MinimalRNN Architecture,[0],[0]
Here we analyze the mean field dynamics of the minimalRNN.,B. Diagonal Recurrence Relation,[0],[0]
The minimalRNN features a hidden state ht ∈ RN and inputs xt.,B. Diagonal Recurrence Relation,[0],[0]
The inputs are transformed via a fully-connected network zt = Φ(xt) ∈ RM before being fed into the network.,B. Diagonal Recurrence Relation,[0],[0]
"The RNN cell is then described by the equations,
vti;a = ∑ j Wijh t−1 j;a + ∑ j Vijz t j;a + bi (14)
uti;a",B. Diagonal Recurrence Relation,[0],[0]
"= σ(v t i;a) (15)
hti;a = u t i;ah t−1 i;a + (1− u t i;a)z t i;a.",B. Diagonal Recurrence Relation,[0],[0]
"(16)
Here i denotes the (pre)-activation and a denotes an input to the network.",B. Diagonal Recurrence Relation,[0],[0]
"Thus, uti acts as a gate on the t’th step.",B. Diagonal Recurrence Relation,[0],[0]
"We take Wij ∼ N (0, σ2w/N), Vij ∼ N (0, σ2v/M) and bi ∼ N (µb, σ2b ).
",B. Diagonal Recurrence Relation,[0],[0]
"By the CTL we can make a mean field assumption that vti;a ∼ N (µb, qtab)",B. Diagonal Recurrence Relation,[0],[0]
"where,
qtab",B. Diagonal Recurrence Relation,[0],[0]
= σ,B. Diagonal Recurrence Relation,[0],[0]
2 wE[h t−1 i;a h t−1 i;b ],B. Diagonal Recurrence Relation,[0],[0]
"+ σ 2 vE[zti;azti;b] + σ2b = σ2wQ t−1 ab + σ 2 vR t ab + σ 2 b (17)
where we have defined Qtab =",B. Diagonal Recurrence Relation,[0],[0]
[h t i;ah t i;b] and R t ab = E[zti;azti;b].,B. Diagonal Recurrence Relation,[0],[0]
"We note that Rtab is fixed by the input, but it remains for us to work out Qtab.",B. Diagonal Recurrence Relation,[0],[0]
"We find that,
Qtab = E[hti;ahti;b] = E[uti;ah t−1 i;a u t i;bh t−1 i;b ]",B. Diagonal Recurrence Relation,[0],[0]
"+ E[u t i;ah t−1 i;a (1− u t i;b)z t i;b] (18)
+ E[(1− uti;a)zti;auti;bht−1i;b ] +",B. Diagonal Recurrence Relation,[0],[0]
"E[(1− u t i;a)z t i;a(1− uti;b)zti;b] (19)
",B. Diagonal Recurrence Relation,[0],[0]
≈ E[uti;auti;b]E[ht−1i;a h t−1 i;b ],B. Diagonal Recurrence Relation,[0],[0]
"+ E[(1− u t i;a)(1− uti;b)]E[zti;azti;b] (20)
where we have assumed that the expectation factorizes so that ht−1i;a and u t i;a are approximately independent.
",B. Diagonal Recurrence Relation,[0],[0]
We choose to normalize the data so that Rtaa = R t bb = R independent of time.,B. Diagonal Recurrence Relation,[0],[0]
An immediate consequence of this normalization is that Qtaa = Q t bb = Q t and qtaa = q t bb = q,B. Diagonal Recurrence Relation,[0],[0]
t.,B. Diagonal Recurrence Relation,[0],[0]
"We then write Rtab = RΣ t, Qtab = Q tCt and qtab = q tct where Σt, Ct, and ct are cosine similarities between the inputs, the hidden states, and the vta,b respectively.",B. Diagonal Recurrence Relation,[0],[0]
"With this normalization, we can work out the mean-field recurrence relation characterizing the covariance matrix for the minimalRNN.
",B. Diagonal Recurrence Relation,[0],[0]
We begin by considering the diagonal recurrence relations.,B. Diagonal Recurrence Relation,[0],[0]
"We find that the dynamics are described by the equation,",B. Diagonal Recurrence Relation,[0],[0]
Qt = Qt−1 ∫ Dzσ2( √ qtz + µb),B. Diagonal Recurrence Relation,[0],[0]
"+R ∫ Dz [ 1− σ( √ qtz + µb) ]2 (21)
",B. Diagonal Recurrence Relation,[0],[0]
"qt = σ2wQ t−1 + σ2vR+ σ 2 b (22)
",B. Diagonal Recurrence Relation,[0],[0]
"As expected, the first and second integrands determine how much of the update of the random network is controlled by the norm of the hidden state and how much is determined by the norm of the input.",B. Diagonal Recurrence Relation,[0],[0]
Since σ(z) = 1− σ(−z),B. Diagonal Recurrence Relation,[0],[0]
"it follows that when µb = 0 the first and second term will be equal and so,
",B. Diagonal Recurrence Relation,[0],[0]
Qt = (Qt−1 +R) ∫,B. Diagonal Recurrence Relation,[0],[0]
Dzσ2( √ qtz).,B. Diagonal Recurrence Relation,[0],[0]
"(23)
In general, µb will therefore control the degree to which the hidden state of the random minimalRNN is updated based on the previous hidden state or based on the inputs with µb = 0 implying parity between the two.",B. Diagonal Recurrence Relation,[0],[0]
This is reflected in eq. (23).,B. Diagonal Recurrence Relation,[0],[0]
"In the event that the norm of the inputs is time-independent, Rt = R for all t, then the minimalRNN will have a fixed point provided there exists a Q∗ that satisfies a transcendental equation,",C. Existence of a Q∗ Fixed Point,[0],[0]
"namely that
F(Q∗) ≡ ∫ Dq∗z",C. Existence of a Q∗ Fixed Point,[0],[0]
[1− σ (z)]2∫ Dq∗z,C. Existence of a Q∗ Fixed Point,[0],[0]
[1− σ2 (z)],C. Existence of a Q∗ Fixed Point,[0],[0]
− Q ∗ R = 0 .,C. Existence of a Q∗ Fixed Point,[0],[0]
"(24)
It is easy to see that such a solution always exists.",C. Existence of a Q∗ Fixed Point,[0],[0]
WhenQ∗,C. Existence of a Q∗ Fixed Point,[0],[0]
→∞,C. Existence of a Q∗ Fixed Point,[0],[0]
the first term of F(Q∗) approaches 1 while the magnitude of the second increases without bound and so F(Q∗) < 0.,C. Existence of a Q∗ Fixed Point,[0],[0]
"Conversely, when Q∗ → 0 the first term is positive while Q∗/R→ 0 and so F(Q∗) > 0.",C. Existence of a Q∗ Fixed Point,[0],[0]
The existence of a Q∗ satisfying the transcendental equation then follows directly from the intermediate value theorem.,C. Existence of a Q∗ Fixed Point,[0],[0]
We can now investigate the dynamics of the norm of the hidden state in the vicinity of Q∗. To do this suppose that Qt = Q∗ + t with 1.,D. Q∗ Dynamics,[0],[0]
"Our goal is then to expand eq.(21) about Q∗. First, we note that,
σ( √ qtz + µb) = σ( √ q∗ + σ2w",D. Q∗ Dynamics,[0],[0]
"tz + µb) (25)
",D. Q∗ Dynamics,[0],[0]
"≈ σ (√ q∗z + µb + 1
2 √ q∗ σ2w tz
) (26)
≈ σ (√ q∗z + µb )",D. Q∗ Dynamics,[0],[0]
"+ 1
2 √ q∗ σ2w
tzσ′ (√ q∗z + µb )",D. Q∗ Dynamics,[0],[0]
+O(( t)2).,D. Q∗ Dynamics,[0],[0]
"(27)
Letting ζ(z) = √ q∗z + µb this implies that,
Qt = Qt−1 ∫ Dzσ2( √ q∗ + σ2w",D. Q∗ Dynamics,[0],[0]
t−1z + µb),D. Q∗ Dynamics,[0],[0]
+R ∫ Dz [ 1− σ( √ q∗ + σ2w,D. Q∗ Dynamics,[0],[0]
"t−1z + µb) ]2
(28) Q∗ + t = Q∗ ∫ Dzσ2 (ζ(z))",D. Q∗ Dynamics,[0],[0]
+,D. Q∗ Dynamics,[0],[0]
R ∫,D. Q∗ Dynamics,[0],[0]
"Dz [1− σ(ζ(z))]2 + t−1 [ ∫ Dzσ2 (ζ(z)) (29)
+ Q∗√ q∗ σ2w
∫ Dzzσ(ζ(z))σ′(ζ(z))−R √ q∗σ2w ∫ Dzz(1− σ(ζ(z)))σ′(ζ(z)) ]",D. Q∗ Dynamics,[0],[0]
"(30)
t = t−1 [ ∫ Dzσ2 (ζ(z))",D. Q∗ Dynamics,[0],[0]
+,D. Q∗ Dynamics,[0],[0]
"Q
∗ √ q∗ σ2w
∫ Dzzσ(ζ(z))σ′(ζ(z))−R √ q∗σ2w ∫ Dzz(1− σ(ζ(z)))σ′(ζ(z)) ]",D. Q∗ Dynamics,[0],[0]
"(31)
= t−1 ∫",D. Q∗ Dynamics,[0],[0]
Dz [ σ2(ζ(z)),D. Q∗ Dynamics,[0],[0]
"+
σ2w√ q∗ {(Q∗ +R)σ(ζ(z))−R} zσ′(ζ(z))
] (32)
= t−1 ∫ Dz [ σ2(ζ(z))",D. Q∗ Dynamics,[0],[0]
"+
σ2w√ q∗ {(Q∗ +R)σ(ζ(z))−R} zσ(ζ(z))(1− σ(ζ(z)))
] (33)
= t−1 ∫ Dz [ σ2(ζ(z))",D. Q∗ Dynamics,[0],[0]
"+
σ2w√ q∗ {(Q∗ +R)σ(ζ(z))−R} zσ(ζ(z))(1− σ(ζ(z)))
] (34)
",D. Q∗ Dynamics,[0],[0]
"It follows that qt → q∗ as, |qt − q∗| ∼ e−t/ξQ (35)
with
ξ−1Q =",D. Q∗ Dynamics,[0],[0]
− log (∫ Dz [ σ2(ζ(z)),D. Q∗ Dynamics,[0],[0]
+ σ2w√ q∗ {(Q∗ +R)σ(ζ(z))−R}σ′(ζ(z)),D. Q∗ Dynamics,[0],[0]
"]) (36)
as expected.",D. Q∗ Dynamics,[0],[0]
We now turn our attention to the off-diagonal term.,E. Off-Diagonal Recurrence Relation,[0],[0]
"From eq. (7) it follows that,
QtCt = Qt−1Ct−1 ∫ Dz1Dz2σ(ut1)σ(ut2) +RΣt ∫ Dz1Dz2(1− σ(ut1))(1− σ(ut2))",E. Off-Diagonal Recurrence Relation,[0],[0]
"(37)
qtct = σ2wQ t−1Ct−1 + σ2vRΣ t + σ2b (38)
where ut1 = √ qtz1 + µb and ut2 = √ qt ( ctz1 + √ 1− (ct)2z2 ) + µb.",E. Off-Diagonal Recurrence Relation,[0],[0]
"(39)
",E. Off-Diagonal Recurrence Relation,[0],[0]
"By expanding eq (37) as ct = c∗ + t we find t+1 = χc∗ t where,
χc∗ = ∫",E. Off-Diagonal Recurrence Relation,[0],[0]
Dz1Dz2σ(u1)σ(u2),E. Off-Diagonal Recurrence Relation,[0],[0]
+ q∗(c∗ + J−) ∫,E. Off-Diagonal Recurrence Relation,[0],[0]
Dz1Dz2σ′(u1)σ′(u2).,E. Off-Diagonal Recurrence Relation,[0],[0]
"(40)
We note that when c∗ = 1 it follows that χc∗ = χ1.",E. Off-Diagonal Recurrence Relation,[0],[0]
"We tune the learning hyper-parameters in the following ranges for all the models:
• learning rate: {0.1, 0.2, 0.3, 0.5, 1, 2} • max-epoch: {4, 7, 11} • decay: {0.5, 0.65, 0.8} • dropout: {0.0, 0.2, 0.3, 0.5}",F. Additional Hyperparameter Ranges,[0],[0]
Recurrent neural networks have gained widespread use in modeling sequence data across various domains.,abstractText,[0],[0]
"While many successful recurrent architectures employ a notion of gating, the exact mechanism that enables such remarkable performance is not well understood.",abstractText,[0],[0]
We develop a theory for signal propagation in recurrent networks after random initialization using a combination of mean field theory and random matrix theory.,abstractText,[0],[0]
"To simplify our discussion, we introduce a new RNN cell with a simple gating mechanism that we call the minimalRNN and compare it with vanilla RNNs.",abstractText,[0],[0]
Our theory allows us to define a maximum timescale over which RNNs can remember an input.,abstractText,[0],[0]
We show that this theory predicts trainability for both recurrent architectures.,abstractText,[0],[0]
"We show that gated recurrent networks feature a much broader, more robust, trainable region than vanilla RNNs, which corroborates recent experimental findings.",abstractText,[0],[0]
"Finally, we develop a closed-form critical initialization scheme that achieves dynamical isometry in both vanilla RNNs and minimalRNNs.",abstractText,[0],[0]
We show that this results in significantly improved training dynamics.,abstractText,[0],[0]
"Finally, we demonstrate that the minimalRNN achieves comparable performance to its more complex counterparts, such as LSTMs or GRUs, on a language modeling task.",abstractText,[0],[0]
Dynamical Isometry and a Mean Field Theory of RNNs: Gating Enables Signal Propagation in Recurrent Neural Networks,title,[0],[0]
"Dependency-based syntactic representations of sentences are central to many language processing tasks (Kübler et al., 2009).",1 Introduction,[0],[0]
"Dependency parse-trees encode not only the syntactic structure of a sentence but also many aspects of its semantics.
",1 Introduction,[0],[0]
"A recent trend in NLP is concerned with encoding sentences as vectors (“sentence embeddings”), which can then be used for further prediction tasks.",1 Introduction,[0],[0]
"Recurrent neural networks (RNNs) (Elman, 1990), and in particular methods based on the LSTM architecture (Hochreiter and Schmidhuber, 1997), work very well for modeling sequences, and constantly obtain state-of-the-art results on both languagemodeling and prediction tasks (see, e.g. (Mikolov et al., 2010)).
",1 Introduction,[0],[0]
"Several works attempt to extend recurrent neural networks to work on trees (see Section 8 for a brief overview), giving rise to the so-called recursive neural networks (Goller and Kuchler, 1996; Socher et al., 2010).",1 Introduction,[0],[0]
"However, recursive neural networks
do not cope well with trees with arbitrary branching factors – most work require the encoded trees to be binary-branching, or have a fixed maximum arity.",1 Introduction,[0],[0]
"Other attempts allow arbitrary branching factors, at the expense of ignoring the order of the modifiers.
",1 Introduction,[0],[0]
"In contrast, we propose a tree-encoding that naturally supports trees with arbitrary branching factors, making it particularly appealing for dependency trees.",1 Introduction,[0],[0]
"Our tree encoder uses recurrent neural networks as a building block: we model the left and right sequences of modifiers using RNNs, which are composed in a recursive manner to form a tree (Section 3).",1 Introduction,[0],[0]
"We use our tree representation for encoding the partially-built parse trees in a greedy, bottom-up dependency parser which is based on the easy-first transition-system of Goldberg and Elhadad (2010).
",1 Introduction,[0],[0]
"Using the Hierarchical Tree LSTM representation, and without using any external embeddings, our parser achieves parsing accuracies of 92.6 UAS and 90.2 LAS on the PTB (Stanford dependencies) and 86.1 UAS and 84.4 LAS on the Chinese treebank, while relying on greedy decoding.
",1 Introduction,[0],[0]
"To the best of our knowledge, this is the first work to demonstrate competitive parsing accuracies for full-scale parsing while relying solely on recursive, compositional tree representations, and without using a reranking framework.",1 Introduction,[0],[0]
"We discuss related work in Section 8.
",1 Introduction,[0],[0]
"While the parsing experiments demonstrate the suitability of our representation for capturing the structural elements in the parse tree that are useful for predicting parsing decisions, we are interested in exploring the use of the RNN-based compositional vector representation of parse trees also for seman-
445
Transactions of the Association for Computational Linguistics, vol. 4, pp.",1 Introduction,[0],[0]
"445–461, 2016.",1 Introduction,[0],[0]
Action Editor: Noah Smith.,1 Introduction,[0],[0]
"Submission batch: 11/2015; Revision batch: 2/2016; Published 8/2016.
",1 Introduction,[0],[0]
c©2016 Association for Computational Linguistics.,1 Introduction,[0],[0]
"Distributed under a CC-BY 4.0 license.
tic tasks such as sentiment analysis (Socher et al., 2013b; Tai et al., 2015), sentence similarity judgements (Marelli et al., 2014) and textual entailment (Bowman et al., 2015).",1 Introduction,[0],[0]
A dependency-based syntactic representation is centered around syntactic modification relations between head words and modifier words.,2.1 Dependency-based Representation,[0],[0]
"The result are trees in which each node is a word in the sentence, and every node except for one designated root node has a parent node.",2.1 Dependency-based Representation,[0],[0]
"A dependency tree over a sentence with n words w1, . . .",2.1 Dependency-based Representation,[0],[0]
", wn can be represented as a list of n pairs of the form (h,m), where 0 ≤ h ≤ n and 1 ≤ m ≤ n.",2.1 Dependency-based Representation,[0],[0]
"Each such pair represents an edge in the tree in which h is the index of a head word (including the special ROOT node 0), and m is the index of a modifier word.",2.1 Dependency-based Representation,[0],[0]
"In order for the dependency trees to be useful for actual downstream language processing tasks, each edge is labeled with a syntactic relation.",2.1 Dependency-based Representation,[0],[0]
"The tree representation then becomes a list of triplets (h,m, `), where 1 ≤ ` ≤ L is the index of a dependency relation out of a designated set of L syntactic relations.
",2.1 Dependency-based Representation,[0],[0]
"Dependency trees tend to be relatively shallow, with some nodes having many children.",2.1 Dependency-based Representation,[0],[0]
"Looking at trees in the PTB training set we find that 94% of the trees have a height of at most 10, and 49% of the trees a height of at most 6.",2.1 Dependency-based Representation,[0],[0]
"In terms of width, 93% of the trees have at least one node with an arity of 4 or more, and 56% of the trees have at least one node with an arity of 6 or more.",2.1 Dependency-based Representation,[0],[0]
"Recurrent neural networks (RNNs), first proposed by Elman (1990) are statistical learners for modeling sequential data.",2.2 Recurrent Networks and LSTMs,[0],[0]
"In this work, we use the RNN abstraction as a building block, and recursively combine several RNNs to obtain our tree representation.",2.2 Recurrent Networks and LSTMs,[0],[0]
We briefly describe the RNN abstraction below.,2.2 Recurrent Networks and LSTMs,[0],[0]
"For further detail on RNNs, the reader is referred to sources such as (Goldberg, 2015; Bengio and Courville, 2016; Cho, 2015).
",2.2 Recurrent Networks and LSTMs,[0],[0]
"The RNN abstraction is a function RNN that takes in a sequence of inputs vectors x1, . . .",2.2 Recurrent Networks and LSTMs,[0],[0]
", xn (xi ∈ Rdin), and produces a sequence of state vec-
tors (also called output vectors) y1, . . .",2.2 Recurrent Networks and LSTMs,[0],[0]
", yn (yi ∈ Rdout).",2.2 Recurrent Networks and LSTMs,[0],[0]
"Each yi is conditioned on all the inputs x1, . . .",2.2 Recurrent Networks and LSTMs,[0],[0]
", xi preceding it.",2.2 Recurrent Networks and LSTMs,[0],[0]
"Ignoring the intermediate outputs y1, . . .",2.2 Recurrent Networks and LSTMs,[0],[0]
", yn−1, the RNN can be thought of as encoding the sequence x1, . . .",2.2 Recurrent Networks and LSTMs,[0],[0]
", xn into a final state yn.",2.2 Recurrent Networks and LSTMs,[0],[0]
"Our notation in this paper follows this view.
",2.2 Recurrent Networks and LSTMs,[0],[0]
"The RNN is defined recursively using two functions:1
RNN(s0, x1, . . .",2.2 Recurrent Networks and LSTMs,[0],[0]
", xn) = yn = O(sn) si = N(si−1, xi)
",2.2 Recurrent Networks and LSTMs,[0],[0]
"Here, a function N takes as input a vector xi and a state vector si−1 and returns as output a new state si.",2.2 Recurrent Networks and LSTMs,[0],[0]
"One can then extract an output vector yi from si using the function O (the function O is usually the identity function, or a function that returns a subset of the elements in si).
",2.2 Recurrent Networks and LSTMs,[0],[0]
"Taking an algorithmic perspective, one can view the RNN as a state object with three operations: s = RNN.initial() returns a new initial state, s.advance(x) takes an input vector and returns a new state, and s.output() returns the output vector for the current state.",2.2 Recurrent Networks and LSTMs,[0],[0]
"When clear from the context, we abbreviate and use the state’s name (s) instead of s.output() to refer to the output vector at the state.
",2.2 Recurrent Networks and LSTMs,[0],[0]
"The functions N and O defining the RNN are parameterized by parameters θ (matrices and vectors), which are trained from data.",2.2 Recurrent Networks and LSTMs,[0],[0]
"Specifically, one is usually interested in using some of the outputs yi for making predictions.",2.2 Recurrent Networks and LSTMs,[0],[0]
The RNN is trained such that the encoding yi is good for the prediction task.,2.2 Recurrent Networks and LSTMs,[0],[0]
"That is, the RNN learns which aspects of the sequence x1, . . .",2.2 Recurrent Networks and LSTMs,[0],[0]
", xi are informative for the prediction.
",2.2 Recurrent Networks and LSTMs,[0],[0]
"We use subscripts (i.e. RNNL, RNNR) to indicate different RNNs, that is, RNNs that have different sets of parameters.
",2.2 Recurrent Networks and LSTMs,[0],[0]
Specific instantiations of N and O yield different recurrent network mechanisms.,2.2 Recurrent Networks and LSTMs,[0],[0]
"In this work we use the Long Short Term Memory (LSTM) variant (Hochreiter and Schmidhuber, 1997) which is shown to be a very capable sequence learner.",2.2 Recurrent Networks and LSTMs,[0],[0]
"However, our algorithm and encoding method do not rely on any specific property of the LSTM architecture, and the
1We follow the notation of Goldberg (2015), with the exception of taking the output of the RNN to be a single vector rather than a sequence, and renaming R to N .
",2.2 Recurrent Networks and LSTMs,[0],[0]
LSTM can be transparently switched for any other RNN variant.,2.2 Recurrent Networks and LSTMs,[0],[0]
We now describe our method for representing a tree as a d-dimensional vector.,3 Tree Representation,[0],[0]
We assume trees in which the children are ordered and there are kl ≥ 0 children before the parent node (left children) and kr ≥ 0 children after it (right children).,3 Tree Representation,[0],[0]
Such trees correspond well to dependency tree structures.,3 Tree Representation,[0],[0]
"We refer to the parent node as a head, and to its children as modifiers.",3 Tree Representation,[0],[0]
"For a node t, we refer to its left modifiers as t.l1, t.l2, . . .",3 Tree Representation,[0],[0]
", t.lkl and its right modifiers as t.r1, t.r2, . . .",3 Tree Representation,[0],[0]
", t.rkr The indices of the modifier are always from the parent outward, that is t.l1 is the left modifier closest to the head t:
t
t.r1 t.r2 t.r3",3 Tree Representation,[0],[0]
"t.r4t.l1t.l2t.l3
The gist of the idea is to treat the modifiers of a node as a sequence, and encode this sequence using an RNN.",3 Tree Representation,[0],[0]
"We separate left-modifiers from rightmodifiers, and use two RNNs: the first RNN encodes the sequence of left-modifiers from the head outwards, and the second RNN the sequence of right-modifiers from the head outwards.",3 Tree Representation,[0],[0]
"The first input to each RNN is the vector representation of the head word, and the last input is the vector representation of the left-most or the right-most modifier.",3 Tree Representation,[0],[0]
The node’s representation is then a concatenation of the RNN encoding of the left-modifiers with the RNN encoding of the right-modifiers.,3 Tree Representation,[0],[0]
"The encoding is recursive: the representation for each of the modifier nodes is computed in a similar fashion.
",3 Tree Representation,[0],[0]
"t
R
L
t.r1
R
t.r2
R
t.r3
R
t.r4
R
t t.l1
L
t.l2
L
t.l3
L enc(t)
",3 Tree Representation,[0],[0]
"RNNR
RNNL concatenate
and compress
More formally, consider a node t. Let i(t) be the sentence index of the word corresponding to the head node t, and let vi be a vector corresponding to the ith word in the sentence (this vector captures information such as the word form and its part of speech tag, and will be discussed shortly).",3 Tree Representation,[0],[0]
"The vec-
tor encoding of a node enc(t) ∈ Rdenc is then defined as follows:
enc(t) =g(W e · (el(t) ◦ er(t))",3 Tree Representation,[0],[0]
"+ be) el(t) =RNNL(vi(t), enc(t.l1), . . .",3 Tree Representation,[0],[0]
", enc(t.lkl))
er(t) =RNNR(vi(t), enc(t.r1), . . .",3 Tree Representation,[0],[0]
", enc(t.rkr))
",3 Tree Representation,[0],[0]
"First, the sequences consisting of the head-vector vi(t) followed by left-modifiers and the head-vector followed by right-modifiers are encoded using two RNNs, RNNL andRNNR, resulting in RNN states el(t) ∈ Rdout and er(t) ∈ Rdout .",3 Tree Representation,[0],[0]
"Then, the RNN states are concatenated, resulting in a 2doutdimensional vector (el(t) ◦ er(t)), which is reduced back to d-dimensions using a linear transformation followed by a non-linear activation function",3 Tree Representation,[0],[0]
"g. The recursion stops at leaf nodes, for which:
enc(leaf) =g(W e · (el(leaf) ◦ er(leaf))",3 Tree Representation,[0],[0]
"+ be) el(leaf) =RNNL(vi(leaf))
er(leaf) =RNNR(vi(leaf))
",3 Tree Representation,[0],[0]
Figure 1 shows the network used for encoding the sentence “the black fox who really likes apples did not jump over a lazy dog yesterday”.,3 Tree Representation,[0],[0]
In the discussion above we assume a vector representation vi associated with the ith sentence word.,3.1 Representing words,[0],[0]
What does vi look like?,3.1 Representing words,[0],[0]
"A sensible approach would be to take vi to be a function of the word-form and the part-of-speech (POS) tag of the ith word, that is:
vi = g(W v · (wi ◦ pi) + bv)
where wi and pi are the embedded vectors of the word-form and POS-tag of the ith word.
",3.1 Representing words,[0],[0]
"This encodes each word in isolation, disregarding its context.",3.1 Representing words,[0],[0]
The context of a word can be very informative regarding its meaning.,3.1 Representing words,[0],[0]
"One way of incorporating context is the Bidirectional RNN (Schuster and Paliwal, 1997).",3.1 Representing words,[0],[0]
"Bidirectional RNNs are shown to be an effective representation for sequence tagging (Irsoy and Cardie, 2014).",3.1 Representing words,[0],[0]
"Bidirectional RNNs represent a word in the sentence using a concatenation of the end-states of two RNNs, one running
yesterday”.",3.1 Representing words,[0],[0]
"Top: the network structure: boxed nodes represent LSTM cells, where L are cells belonging to the leftmodifiers sequence model RNNL, and R to the right-modifiers sequence model RNNR.",3.1 Representing words,[0],[0]
Circle nodes represent a concatenation followed by a linear transformation and a non-linearity.,3.1 Representing words,[0],[0]
"Bottom: the dependency parse of the sentence.
",3.1 Representing words,[0],[0]
from the beginning of the sentence to the word and the other running from the end to the word.,3.1 Representing words,[0],[0]
"The result is a vector representation for each word which captures not only the word but also its context.
",3.1 Representing words,[0],[0]
"We adopt the Bidirectional LSTM scheme to enrich our node vector representation, and for an nwords sentence compute the vector representations vi as follows:
v′i =g(W v · (wi ◦ pi) + bv) fi =LSTMF (v ′ 1, v ′ 2, . . .",3.1 Representing words,[0],[0]
", v ′",3.1 Representing words,[0],[0]
"i)
bi =LSTMB(v ′ n, v ′",3.1 Representing words,[0],[0]
"n−1, . .",3.1 Representing words,[0],[0]
.,3.1 Representing words,[0],[0]
", v′i) vi =(fi ◦ bi)
",3.1 Representing words,[0],[0]
"We plug this word representation as word vectors, allowing each word vector vi to capture information regarding the word form and POS-tag, as well as the sentential context it appears in.",3.1 Representing words,[0],[0]
"The BILSTM encoder is trained jointly with the rest of the network towards the parsing objective, using backpropagation.
",3.1 Representing words,[0],[0]
Embedding vectors,3.1 Representing words,[0],[0]
The word and POS embeddings wi and pi are also trained together with the network.,3.1 Representing words,[0],[0]
"For the word embeddings, we experiment with random initialization, as well as with initialization using pre-trained word embeddings.",3.1 Representing words,[0],[0]
"Our main goal in this work is not to provide top parsing accuracies, but rather to evaluate the ability of the proposed compositional architecture to learn and capture the structural cues that are needed for accurate parsing.",3.1 Representing words,[0],[0]
"Thus, we are most interested in the random initialization setup: what can the network learn from the training corpus alone, without relying on external resources.
",3.1 Representing words,[0],[0]
"However, the ability to perform semi-supervised learning by initializing the word-embeddings with vectors that are pre-trained on large amount of unannotated data is an appealing property of the neuralnetwork approaches, and we evaluate our parser also in this semi-supervised setup.",3.1 Representing words,[0],[0]
"When using pretrained word embeddings, we follow (Dyer et al., 2015) and use embedding vectors which are trained using positional context (Ling et al., 2015), as these
were shown to work better than traditional skipgram vectors for syntactic tasks such as part-ofspeech tagging and parsing.",3.1 Representing words,[0],[0]
"Why did we choose to encode the children from the head outward, and not the other way around?",3.2 A note on the head-outward generation,[0],[0]
"The head outward generation order is needed to facilitate incremental tree construction and allow for efficient parsing, as we show in section 4 below.",3.2 A note on the head-outward generation,[0],[0]
"Besides the efficiency considerations, using the headoutward encoding puts more emphasis on the outermost dependants, which are known to be the most informative for predicting parse structure.2 We rely on the RNN capability of extracting information from arbitrary positions in the sequence to incorporate information about the head word itself, which appears in the beginning of the sequence.",3.2 A note on the head-outward generation,[0],[0]
"This seems to work well, which is expected considering that the average maximal number of siblings in one direction in the PTB is 4.1, and LSTMs were demonstrated to capture much longer-range interactions.",3.2 A note on the head-outward generation,[0],[0]
"Still, when using the tree encoding in a situation where the tree is fully specified in advance, i.e. for sentence classification, sentence similarity or translation tasks, using a head-inward generation order (or even a bidirectional RNN) may prove to work better.",3.2 A note on the head-outward generation,[0],[0]
"We leave this line of inquiry to future work.
",3.2 A note on the head-outward generation,[0],[0]
"The head-outward modifier generation approach has a long history in the parsing literature, and goes back to at least Eisner (1996) and Collins (1997).",3.2 A note on the head-outward generation,[0],[0]
"In contrast to previous work in which each modifier could condition only on a fixed small number of modifiers preceding it, and in which the left- and right- sequences of modifiers were treated as independent from one another for computational efficiency reasons, our approach allows the model to access information from the entirety of both the left and the right sequences jointly.
",3.2 A note on the head-outward generation,[0],[0]
"2Features in transition-based dependency parsers often look at the current left-most and right-most dependents of a given node, and almost never look further than the second left-most or second right-most dependents.",3.2 A note on the head-outward generation,[0],[0]
"Second-order graph based dependency parsers (McDonald, 2006; Eisner, 2000) also condition on the current outermost dependent when generating its sibling.",3.2 A note on the head-outward generation,[0],[0]
We now turn to explain how to parse using the tree encoder defined above.,4 Parsing Algorithm,[0],[0]
"We begin by describing our bottom-up parsing algorithm, and then show how the encoded vector representation can be built and maintained throughout the parsing process.",4 Parsing Algorithm,[0],[0]
"We follow a (projective) bottom-up parsing strategy, similar to the easy-first parsing algorithm of Goldberg and Elhadad (2010).
",4.1 Bottom-up Parsing,[0],[0]
The main data-structure in the parser is a list of partially-built parse trees we call pending.,4.1 Bottom-up Parsing,[0],[0]
"For a sentence with words w1, . . .",4.1 Bottom-up Parsing,[0],[0]
", wn, the pending list is initialized with n nodes, where pending[i] corresponds to word wi.",4.1 Bottom-up Parsing,[0],[0]
"The algorithm then chooses two neighbouring trees in the pending list pending[i] and pending[i + 1] and either attaches the root of pending[i+1] as the right-most modifier of the root of pending[i], or attaches the root of pending[i] as the left-most modifier of the root of pending[i+ 1].",4.1 Bottom-up Parsing,[0],[0]
"The tree which was treated as modifier is then removed from the pending list, shortening it by one.",4.1 Bottom-up Parsing,[0],[0]
"The process ends after n−1 steps, when a single tree remains in the pending list, which is taken to be the output parse tree.",4.1 Bottom-up Parsing,[0],[0]
"The parsing process is described in Algorithm 1.
",4.1 Bottom-up Parsing,[0],[0]
"Algorithm 1 Parsing 1: Input: Sentence w = w1, . . .",4.1 Bottom-up Parsing,[0],[0]
", wn 2: for i ∈ 1, . . .",4.1 Bottom-up Parsing,[0],[0]
", n do 3: pend[i].id←",4.1 Bottom-up Parsing,[0],[0]
i 4: arcs←,4.1 Bottom-up Parsing,[0],[0]
"[] 5: while |pend| > 1 do 6: A← {(i, d) | 1 ≤",4.1 Bottom-up Parsing,[0],[0]
"i < |pend|, d ∈",4.1 Bottom-up Parsing,[0],[0]
"{l, r}} 7: i, d← select(A) 8: if d = l then 9: m,h← pend[i], pend[i+ 1] 10: pend.remove(i) 11: else 12: h,m← pend[i], pend[i+ 1] 13: pend.remove(i+ 1) 14: arcs.append(h.id,m.id) 15: return arcs
This parsing algorithm is both sound and complete with respect to the class of projective depen-
dency trees (Goldberg and Elhadad, 2010).",4.1 Bottom-up Parsing,[0],[0]
The algorithm depends on non-deterministic choices of an index in the pending list and an attachment direction (line 7).,4.1 Bottom-up Parsing,[0],[0]
"When parsing in practice, the nondeterministic choice will be replaced by using a trained classifier to assign a score to each indexdirection pair, and selecting the highest scoring pair.",4.1 Bottom-up Parsing,[0],[0]
"We discuss the scoring function in Section 4.4, and the training algorithm in Section 5.",4.1 Bottom-up Parsing,[0],[0]
We would like the scoring function to condition on the vector encodings of the subtrees it aims to connect.,4.2 Bottom-up Tree-Encoding,[0],[0]
"Algorithm 2 shows how to maintain the vector encodings together with the parsing algorithm, so that at every stage in the parsing process each item pending[i] is associated with a vector encoding of the corresponding tree.
",4.2 Bottom-up Tree-Encoding,[0],[0]
"Algorithm 2 Parsing while maintaining tree representations
1: Input: Sentence w = w1, . . .",4.2 Bottom-up Tree-Encoding,[0],[0]
", wn 2: Input: Vectors vi corresponding to words wi 3: arcs←",4.2 Bottom-up Tree-Encoding,[0],[0]
"[] 4: for i ∈ 1, . .",4.2 Bottom-up Tree-Encoding,[0],[0]
.,4.2 Bottom-up Tree-Encoding,[0],[0]
", n do 5: pend[i].id← i 6: pend[i].el ← RNNL.init().append(vi) 7: pend[i].er ← RNNR.init().append(vi) 8: while |pend|",4.2 Bottom-up Tree-Encoding,[0],[0]
"> 1 do 9: A← {(i, d)",4.2 Bottom-up Tree-Encoding,[0],[0]
"| 1 ≤ i < |pend|, d ∈ {l, r}}
10: i, d← select(A) 11: if d = l then 12: m,h← pend[i], pend[i+ 1] 13: m.c = m.el ◦m.er 14: m.enc = g(W (m.c) + b) 15: h.el.append(m.enc) 16: pend.remove(i) 17: else 18: h,m← pend[i], pend[i+ 1] 19: m.c = m.el ◦m.er 20: m.enc = g(W (m.c) + b) 21: h.er.append(m.enc) 22: pend.remove(i+ 1) 23: arcs.add(h.id,m.id) 24: return arcs",4.2 Bottom-up Tree-Encoding,[0],[0]
The tree representation described above does not account for the relation labels ` the parsing algorithm assigns each edge.,4.3 Labeled Tree Representation,[0],[0]
"In cases the tree is fully specified in advance, the relation of each word to its head can be added to the word representations vi.",4.3 Labeled Tree Representation,[0],[0]
"However, in the context of parsing, the labels become known only when the modifier is attached to its parent.",4.3 Labeled Tree Representation,[0],[0]
We thus extend the tree representation by concatenating the node vector representation with a vector representation assigned to the label connecting the subtree to its parent.,4.3 Labeled Tree Representation,[0],[0]
"Formally, only the final enc(t) equation changes:
enc(t) = g(W e · (el ◦ er ◦ `) + be) where ` is a learned embedding vector associated with the given label.",4.3 Labeled Tree Representation,[0],[0]
The parsing algorithm relies on a function select(A) for choosing the action to take at each stage.,4.4 Scoring Function,[0],[0]
"We model this function as:
select(A) = argmax(i,d,`)∈AScore(pend, i, d, `)
where Score(.) is a learned function whose job is to assign scores to possible actions to reflect their quality.",4.4 Scoring Function,[0],[0]
"Ideally, it will not only score correct actions above incorrect ones, but also more confident (easier) actions above less confident ones, in order to minimize error propagation in the greedy parsing process.
",4.4 Scoring Function,[0],[0]
"When scoring a possible attachment between a head h and a modifier m with relation `, the scoring function should attempt to reflect the following pieces of information: • Are the head words of h and m compatible un-
der relation",4.4 Scoring Function,[0],[0]
l?,4.4 Scoring Function,[0],[0]
"• Is the modifier m compatible with the already
existing modifiers of h?",4.4 Scoring Function,[0],[0]
"In other words, is m a good subtree to connect as an outer-most modifier in the subtree h?",4.4 Scoring Function,[0],[0]
"• Is m complete, in the sense that it already ac-
quired all of its own modifiers?",4.4 Scoring Function,[0],[0]
"to this end, the scoring function looks at a window of k subtrees to each side of the head-modifier pair (pend[i− k], . . .",4.4 Scoring Function,[0],[0]
", pend[i+1+ k]) where the neighbouring subtrees are used for providing hints regarding possible additional modifiers ofm",4.4 Scoring Function,[0],[0]
"and h that are
yet to be acquired.",4.4 Scoring Function,[0],[0]
"We use k = 2 in our experiments, for a total of 6 subtrees in total.",4.4 Scoring Function,[0],[0]
"This window approach is also used in the Easy-First parser of Goldberg and Elhadad (Goldberg and Elhadad, 2010) and works that extend it (Tratz and Hovy, 2011; Ma et al., 2012; Ma et al., 2013).",4.4 Scoring Function,[0],[0]
"However, unlike the previous work, which made use of extensive feature engineering and rich feature functions aiming at extracting the many relevant linguistic sub-structures from the 6 subtrees and their interactions, we provide the scoring function solely with the vector-encoding of the 6 subtrees in the window.
",4.4 Scoring Function,[0],[0]
Modeling the labeled attachment score is more difficult than modeling the unlabeled score and is prone to more errors.,4.4 Scoring Function,[0],[0]
"Moreover, picking the label for an attachment will cause less cascading error in contrast to picking the wrong attachment, which will necessarily preclude the parser from reaching the correct tree structure.",4.4 Scoring Function,[0],[0]
"In order to partially overcome this issue, our scoring function is a sum of two auxiliary scoring function, one scoring unlabeled and the other scoring labeled attachments.",4.4 Scoring Function,[0],[0]
"The unlabeled attachment score term in the sum functions as a fallback which makes it easier for a parser to predict the attachment direction even when there is no sufficient certainty as to the label.
",4.4 Scoring Function,[0],[0]
"Score(pend, i, d, `) = ScoreU (pend, i, d)
+ ScoreL(pend, i, d, `)
",4.4 Scoring Function,[0],[0]
"Each of ScoreU and ScoreL are modeled as multilayer perceptrons:
ScoreU (pend, i, d) =MLPU (xi)[d]
ScoreL(pend, i, d, `) =MLPL(xi)[(d, `)]",4.4 Scoring Function,[0],[0]
"xi = pend[i− 2].c ◦ · · · ◦ pend[i+ 3].c
where MLPU and MLPL are standard multilayer perceptron classifiers with one hidden layer (MLPX(x) =",4.4 Scoring Function,[0],[0]
W 2g(W,4.4 Scoring Function,[0],[0]
1x+,4.4 Scoring Function,[0],[0]
b1)+ b2),4.4 Scoring Function,[0],[0]
"and have output layers with size 2 and 2L respectively,",4.4 Scoring Function,[0],[0]
"[.] is an indexing operation, and we assume the values of d and (d, `) are mapped to integer values.",4.4 Scoring Function,[0],[0]
"The Easy-First parsing algorithm works in O(n log n) time (Goldberg and Elhadad, 2010).
",4.5 Computational Complexity,[0],[0]
"The parser in this works differ by three aspects: running a BI-LSTM encoder prior to parsing (O(n)); maintaining the tree representation during parsing (lines 11–22 in Algorithm 2) which take a constant time at each parsing step; and local scoring using an MLP rather than a linear classifier (again, a constant-time operation).",4.5 Computational Complexity,[0],[0]
"Thus, the parser maintains the O(n log n) complexity of the Easy-First parser.",4.5 Computational Complexity,[0],[0]
"At each step of the parsing process we select the highest scoring action (i, d, `).",5.1 Loss and Parameter Updates,[0],[0]
The goal of training is to set the Score function such that correct actions are scored above incorrect ones.,5.1 Loss and Parameter Updates,[0],[0]
"We use a marginbased objective, aiming to maximize the margin between the highest scoring correct action and the set of incorrect actions.",5.1 Loss and Parameter Updates,[0],[0]
"Formally, we define a hinge loss for each parsing step as follows:
max{0, 1−max(i,d,`)∈GScore(pend, i, d, `)",5.1 Loss and Parameter Updates,[0],[0]
"+max(i′,d′,`′)∈A\GScore(pend, i, d, `)}
where A is the set of all possible actions and G is the set of correct actions at the current stage.
",5.1 Loss and Parameter Updates,[0],[0]
"As the scoring function depends on vectorencodings of all trees in the window, and each treeencoding depends on the network’s parameters, each parameter update will invalidate all the vector encodings, requiring a re-computation of the entire network.",5.1 Loss and Parameter Updates,[0],[0]
"We thus sum the local losses throughout the parsing process, and update the parameter with respect to the sum of the losses at sentence boundaries.",5.1 Loss and Parameter Updates,[0],[0]
Since we are using hinge loss the gradients will become sparser as the training progresses.,5.1 Loss and Parameter Updates,[0],[0]
Fewer non-zero gradients could translate to unreliable updates.,5.1 Loss and Parameter Updates,[0],[0]
"In order to increase gradient stability and training speed, we use a variation of mini-batch in which we update the parameters only after 50 errors were made.",5.1 Loss and Parameter Updates,[0],[0]
This assures us a sufficient number of gradients for every update thus minimizing the effect of gradient instability.,5.1 Loss and Parameter Updates,[0],[0]
The gradients of the entire network with respect to the sum of the losses are calculated using the backpropagation algorithm.,5.1 Loss and Parameter Updates,[0],[0]
Initial experiments with an SGD optimizer showed very instable results.,5.1 Loss and Parameter Updates,[0],[0]
"We settled instead on using the ADAM optimizer (Kingma and Ba, 2015) which
worked well without requiring fiddling with learning rates.",5.1 Loss and Parameter Updates,[0],[0]
"At each stage in the training process, the parser assigns scores to all the possible actions (i, d, `)",5.2 Error-Exploration and Dynamic Oracle Training,[0],[0]
∈ A.,5.2 Error-Exploration and Dynamic Oracle Training,[0],[0]
"It then selects an action, applies it, and moves to the next step.",5.2 Error-Exploration and Dynamic Oracle Training,[0],[0]
Which action should be chosen?,5.2 Error-Exploration and Dynamic Oracle Training,[0],[0]
"A sensible option is to defineG as the set of actions that can lead to the gold tree, and following the highest scoring actions in this set.",5.2 Error-Exploration and Dynamic Oracle Training,[0],[0]
"However, using training in this manner tends to suffer from error propagation at test time.",5.2 Error-Exploration and Dynamic Oracle Training,[0],[0]
The parser sees only states that result from following correct actions.,5.2 Error-Exploration and Dynamic Oracle Training,[0],[0]
The lack of examples containing errors in the training phase makes it hard for the parser to infer the best action given partly erroneous trees.,5.2 Error-Exploration and Dynamic Oracle Training,[0],[0]
"In order to cope with this, we follow the error exploration training strategy, in which we let the parser follow the highest scoring action in A during training even if this action is incorrect, exposing it to states that result from erroneous decisions.",5.2 Error-Exploration and Dynamic Oracle Training,[0],[0]
This strategy requires defining the set G such that the correct actions to take are well-defined also for states that cannot lead to the gold tree.,5.2 Error-Exploration and Dynamic Oracle Training,[0],[0]
Such a set G is called a dynamic oracle.,5.2 Error-Exploration and Dynamic Oracle Training,[0],[0]
"Error-exploration and dynamic-oracles were introduced by Goldberg and Nivre (2012).
",5.2 Error-Exploration and Dynamic Oracle Training,[0],[0]
"The Dynamic Oracle A dynamic-oracle for the easy-first parsing system we use is presented in (Goldberg and Nivre, 2013).",5.2 Error-Exploration and Dynamic Oracle Training,[0],[0]
"Briefly, the dynamicoracle version of G defines the set of gold actions as the set of actions which does not increase the number of erroneous attachments more than the minimum possible (given previous erroneous actions).",5.2 Error-Exploration and Dynamic Oracle Training,[0],[0]
The number of erroneous attachments is increased in three cases: (1) connecting a modifier to its head prematurely.,5.2 Error-Exploration and Dynamic Oracle Training,[0],[0]
"Once the modifier is attached it is removed from the pending list and therefore can no longer acquire any of its own modifiers; (2) connecting a modifier to an erroneous head, when the correct head is still on the pending list; (3) connecting a modifier to a correct head, but an incorrect label.
",5.2 Error-Exploration and Dynamic Oracle Training,[0],[0]
Dealing with cases (2) and (3) is trivial.,5.2 Error-Exploration and Dynamic Oracle Training,[0],[0]
"To deal with (1), we consider as correct only actions in which the modifier is complete.",5.2 Error-Exploration and Dynamic Oracle Training,[0],[0]
"To efficiently identify complete modifiers we hold a counter for each word which is initialized to the number of modifiers
the word has in the gold tree.",5.2 Error-Exploration and Dynamic Oracle Training,[0],[0]
When applying an attachment the counter of the modifier’s gold head word is decreased.,5.2 Error-Exploration and Dynamic Oracle Training,[0],[0]
"When the counter reaches 0, the sub-tree rooted at that word has no pending modifiers, and is considered complete.
",5.2 Error-Exploration and Dynamic Oracle Training,[0],[0]
"Aggressive Exploration We found that even when using error-exploration, after one iteration the model remembers the training set quite well, and does not make enough errors to make error-exploration effective.",5.2 Error-Exploration and Dynamic Oracle Training,[0],[0]
"In order to expose the parser to more errors, we employ a cost augmentation scheme: we sometimes follow incorrect actions also if they score below correct actions.",5.2 Error-Exploration and Dynamic Oracle Training,[0],[0]
"Specifically, when the score of the correct action is greater than that of the wrong action but the difference is smaller than the margin constant, we chose to follow the wrong action with probability paug (we use paug = 0.1 in our experiments).",5.2 Error-Exploration and Dynamic Oracle Training,[0],[0]
Pseudocode for the entire training algorithm is given in the supplementary material.,5.2 Error-Exploration and Dynamic Oracle Training,[0],[0]
"Due to the sparsity of natural language, we are likely to encounter at test time a substantial number of the words that did not appear in the training data (OOV words).",5.3 Out-of-vocabulary items and word-dropout,[0],[0]
OOV words are likely even when pre-training the word representations on a large unannotated corpora.,5.3 Out-of-vocabulary items and word-dropout,[0],[0]
"A common approach is to designate a special “unknown-word” symbol, whose associated vector will be used as the word representation whenever an OOV word is encountered at test time.",5.3 Out-of-vocabulary items and word-dropout,[0],[0]
"In order to train the unknown-word vector, a possible approach is to replace all the words appearing in the training corpus less than a certain number of times with the unknown-word symbol.",5.3 Out-of-vocabulary items and word-dropout,[0],[0]
"This approach gives a good vector representation for unknown words but at the expense of ignoring many of the words from the training corpus.
",5.3 Out-of-vocabulary items and word-dropout,[0],[0]
"We instead propose a variant of the word-dropout approach (Iyyer et al., 2015).",5.3 Out-of-vocabulary items and word-dropout,[0],[0]
"During training, we replace a word with the unknown-word symbol with probability that is inversely proportional to frequency of the word.",5.3 Out-of-vocabulary items and word-dropout,[0],[0]
"Formally, we replace a word w appearing #(w) times in the training corpus with the unknown symbol with a probability:
punk(w) = α
#(w) + α
Using this approach we learn a vector representation for unknown words with minimal impact on the training of sparse words.",5.3 Out-of-vocabulary items and word-dropout,[0],[0]
Our Python implementation will be made available at the first author’s website.,6 Implementation Details,[0],[0]
"We use the PyCNN wrapper of the CNN library3 for building the computation graph of the network, computing the gradients using automatic differentiation, and performing parameter updates.",6 Implementation Details,[0],[0]
"We noticed the error on the development set does not improve after 20 iterations over the training set, therefore, we ran the training for 20 iterations.",6 Implementation Details,[0],[0]
The sentences where shuffled between iterations.,6 Implementation Details,[0],[0]
Non-projective sentences were skipped during training.,6 Implementation Details,[0],[0]
"We use the default parameters initialization, step sizes and regularization values provided by the PyCNN toolkit.",6 Implementation Details,[0],[0]
"The hyperparameters of the final networks used for all the reported experiments are detailed in Table 1.
",6 Implementation Details,[0],[0]
Weiss et al (2015) stress the importance of careful hyperparameter tuning for achieving top accuracy in neural network based parser.,6 Implementation Details,[0],[0]
"We did not follow this advice and made very few attempts at hyper-parameter tuning, using manual hill climbing until something seemed to work with reasonable accuracy, and then sticking with it for the rest of the experiments.
",6 Implementation Details,[0],[0]
3https://github.com/clab/cnn/tree/ master/pycnn,6 Implementation Details,[0],[0]
We evaluated our parsing model to English and Chinese data.,7 Experiments and Results,[0],[0]
"For comparison purposes we followed the setup of (Dyer et al., 2015).
",7 Experiments and Results,[0],[0]
"Data For English, we used the Stanford Dependency (SD) (de Marneffe and Manning, 2008) conversion of the Penn Treebank (Marcus et al., 1993), using the standard train/dev/test splitswith the same predicted POS-tags as used in (Dyer et al., 2015; Chen and Manning, 2014).",7 Experiments and Results,[0],[0]
This dataset contains a few non-projective trees.,7 Experiments and Results,[0],[0]
"Punctuation symbols are excluded from the evaluation.
",7 Experiments and Results,[0],[0]
"For Chinese, we use the Penn Chinese Treebank 5.1 (CTB5), using the train/test/dev splits of (Zhang and Clark, 2008; Dyer et al., 2015) with gold partof-speech tags, also following (Dyer et al., 2015; Chen and Manning, 2014).
",7 Experiments and Results,[0],[0]
"When using external word embeddings, we also use the same data as (Dyer et al., 2015).4
Experimental configurations We evaluated the parser in several configurations BOTTOMUPPARSER is the baseline parser, not using the tree-encoding, and instead representing each item in pending solely by the vector-representation (word and POS) of its head word.",7 Experiments and Results,[0],[0]
BOTTOMUPPARSER+HTLSTM is using our Hierarchical Tree LSTM representation.,7 Experiments and Results,[0],[0]
BOTTOMUPPARSER+HTLSTM+BI-LSTM is the Hierarchical Tree LSTM where we additionally use a BI-LSTM encoding for the head words.,7 Experiments and Results,[0],[0]
"Finally, we added external, pre-trained word embeddings to the BOTTOMUPPARSER+HTLSTM+BI-LSTM setup.",7 Experiments and Results,[0],[0]
"We also evaluated the final parsers in a –POS setup, in which we did not feed the parser with any POS-tags.
",7 Experiments and Results,[0],[0]
Results Results for English and Chinese are presented in Tables 2 and 3 respectively.,7 Experiments and Results,[0],[0]
"For comparison, we also show the results of the Stack-LSTM transition-based parser model of Dyer et al (2015), which we consider to be a state-of-the-art greedy model which is also very competitive with searchbased models, with and without pre-trained embeddings, and with and without POS-tags.
",7 Experiments and Results,[0],[0]
"4We thank Dyer et al for sharing their data with us.
",7 Experiments and Results,[0],[0]
The trends are consistent across the two languages.,7 Experiments and Results,[0],[0]
The baseline Bottom-Up parser performs very poorly.,7 Experiments and Results,[0],[0]
"This is expected, as only the headword of each subtree is used for prediction.",7 Experiments and Results,[0],[0]
"When adding the tree-encoding, results jump to near stateof-the-art accuracy, suggesting that the composed vector representation is indeed successful in capturing predictive structural information.",7 Experiments and Results,[0],[0]
"Replacing the head-words with their BI-LSTM encodings results in another increase in accuracy for English, outperforming the Dyer et al (S-LSTM no external) models on the test-set.",7 Experiments and Results,[0],[0]
"Adding the external pre-trained embeddings further improves the results for both our parser and Dyer et al’s model, closing the gap between them.",7 Experiments and Results,[0],[0]
"When POS-tags are not provided as input, the numbers for both parsers drop.",7 Experiments and Results,[0],[0]
"The drop is small for English and large for Chinese, and our parser seem to suffer a little less than the Dyer et al model.
",7 Experiments and Results,[0],[0]
"Importance of the dynamic oracle We also evaluate the importance of using the dynamic oracle and error-exploration training, and find that they are indeed important for achieving high parsing accura-
cies with our model (Table 4).
",7 Experiments and Results,[0],[0]
"When training without error-exploration (that is, the parser follows only correct actions during training and not using the dynamic aspect of the oracle), accuracies of unseen sentences drop by between 0.4 and 0.8 accuracy points (average 0.58).",7 Experiments and Results,[0],[0]
"This is consistent with previous work on training with error-exploration and dynamic oracles (Goldberg and Nivre, 2013), showing that the technique is not restricted to models trained with sparse linear models.
",7 Experiments and Results,[0],[0]
"Comparison to other state-of-the-art parsers Our main point of comparison is the model of Dyer et al, which was chosen because it is (a) a very strong parsing model; and (b) is the closest to ours in the literature: a greedy parsing model making heavy use of LSTMs.",7 Experiments and Results,[0],[0]
"To this end, we tried to make the comparison to Dyer et al as controlled as possible, using the same dependency annotation schemes, as well as the same predicted POS-tags and the pre-trained embeddings (when applicable).
",7 Experiments and Results,[0],[0]
"It is also informative to position our results with respect to other state-of-the-art parsing results reported in the literature, as we do in Table 5.",7 Experiments and Results,[0],[0]
"Here, some of the comparisons are less direct: some of the results use different dependency annotation schemes5, as well as different predicted POS-tags, and different pre-trained word embeddings.",7 Experiments and Results,[0],[0]
"While the numbers are not directly comparable, they do give a good reference as to the expected range of
5Our English parsing experiments use the Stanford Dependencies scheme, while other work use less informative dependency relations which are based on the Penn2Malt converter, using the Yamada and Matsumoto head rules.",7 Experiments and Results,[0],[0]
"From our experience, this conversion is somewhat easier to parse, resulting in numbers which are about 0.3-0.4 points higher than Stanford Dependencies.
state-of-the-art parsing results.",7 Experiments and Results,[0],[0]
Our system’s English parsing results are in range of state-of-the-art and the Chinese parsing results surpass it.,7 Experiments and Results,[0],[0]
"These numbers are achieved while using a greedy, bottom up parsing method without any search, and while relying solely on the compositional tree representations.",7 Experiments and Results,[0],[0]
"We survey two lines of related work: methods for encoding trees as vectors, and methods for parsing with vector representations.
",8 Related Work,[0],[0]
"The popular approach for encoding trees as vectors is using recursive neural networks (Goller and Kuchler, 1996; Socher et al., 2010; Tai et al., 2015).",8 Related Work,[0],[0]
Recursive neural networks represent the vector of a parent node in a tree as a function of its children nodes.,8 Related Work,[0],[0]
"However, the functions are usually restricted to having a fixed maximum arity (usually two) (Socher et al., 2010; Tai et al., 2015; Socher, 2014).",8 Related Work,[0],[0]
"While trees can be binarized to cope with the arity restriction, doing so results in deep trees which in turn leads to the vanishing gradient problem when training.",8 Related Work,[0],[0]
"To cope with the vanishing gradients, (Tai et al., 2015) enrich the composition function with a gating mechanism similar to that of the LSTM, resulting in the so-called Tree-LSTM model.",8 Related Work,[0],[0]
"Another approach is to allow arbitrary arities but ignoring the sequential nature of the modifiers, e.g. by using a bag-of-modifiers representation or a convolutional layer (Tai et al., 2015; Zhu et al., 2015).",8 Related Work,[0],[0]
"In contrast, our tree encoding method naturally allows for arbitrary branching trees by relying on the well established LSTM sequence model, and using it as a black box.",8 Related Work,[0],[0]
"Very recently, Zhang et al. (2015) proposed an RNN-based tree encoding which is similar to ours in encoding the sequence of modifiers as an RNN.",8 Related Work,[0],[0]
"Unlike our bottom-up encoder, their method works top-down, and is therefore not readily applicable for parsing.",8 Related Work,[0],[0]
On the other hand the top-down approach is well suited for generation.,8 Related Work,[0],[0]
"In future work, it could be interesting to combine the bottomup and top-down approaches in an encoder-decoder framework (Sutskever et al., 2014; Kiros et al., 2015).",8 Related Work,[0],[0]
"Work by Dyer et al (2016), that was submitted in parallel to ours, introduces a similar LSTMbased representation of syntactic constituents in the context of phrase-grammar parsing.
",8 Related Work,[0],[0]
"In terms of parsing with vector representations, there are four dominant approaches: search based parsers that use local features that are fed to a neural-network classifier (Pei et al., 2015; Durrett and Klein, 2015); greedy transition based parsers that use local features that are fed into a neuralnetwork classifier (Chen and Manning, 2014; Weiss et al., 2015), sometimes coupled with a node composition function (Dyer et al., 2015; Watanabe and Sumita, 2015); bottom up parsers that rely solely on recursively combined vector encodings of subtrees (Socher et al., 2010; Stenetorp, 2013; Socher et al., 2013a); and parse-reranking approaches that first produce a k-best list of parses using a traditional parsing technique, and then score the trees based on a recursive vector encoding of each node (Le and Zuidema, 2014; Le and Zuidema, 2015; Zhu et al., 2015).
",8 Related Work,[0],[0]
"Our parser is a greedy, bottom up parser that relies on compositional vector encodings of subtrees as its sole set of features.",8 Related Work,[0],[0]
"Unlike the re-ranking approaches, we do not rely on an external parser to provide k-best lists.",8 Related Work,[0],[0]
"Unlike the bottom-up parser in (Socher et al., 2010) that only parses sentences of up to 15 words and the parser of (Stenetorp, 2013) that achieves very low parsing accuracies, we parse arbitrary sentences with near state-of-the-art accuracy.",8 Related Work,[0],[0]
"Unlike the bottom up parser in (Socher et al., 2013a)",8 Related Work,[0],[0]
we do not make use of a grammar.,8 Related Work,[0],[0]
"The parser of (Weiss et al., 2015) obtains exceptionally high results using local features and no composition function.",8 Related Work,[0],[0]
The greedy version of their parser uses extensive tuning of hyper-parameters and network depth in order to squeeze every possible bit of accuracy.,8 Related Work,[0],[0]
Adding beam search on top of that further improves results.,8 Related Work,[0],[0]
"Due to our much more limited resources, we did not perform a methodological search over hyper-parameters, and explored only a tiny space of the possible hyper-parameters, and our parser does not perform search.",8 Related Work,[0],[0]
"Finally, perhaps closest to our approach is the greedy, transition-based parser of (Dyer et al., 2015) that also works in a bottomup fashion, and incorporates an LSTM encoding of the input tokens and hierarchical vector composition into its scoring mechanism.",8 Related Work,[0],[0]
"Indeed, that parser obtains similar scores to ours, although we obtain somewhat better results when not using pre-trained embeddings.",8 Related Work,[0],[0]
"We differ from the parser of Dyer et
al by having a more elaborate vector-composition function, relying solely on the compositional representations, and performing fully bottom-up parsing without being guided by a stack-and-buffer control structure.",8 Related Work,[0],[0]
"We suggest a compositional vector representation of parse trees that relies on a recursive combination of recurrent-neural network encoders, and demonstrate its effectiveness by integrating it in a bottom-up easy-first parser.",9 Conclusions and Future Work,[0],[0]
"Future extensions in terms of parsing include the addition of beam search, handling of unknown-words using character-embeddings, and adapting the algorithm to constituency trees.",9 Conclusions and Future Work,[0],[0]
"We also plan to establish the effectiveness of our Hierarchical Tree-LSTM encoder by applying it to more semantic vector representation tasks, i.e. training tree representation for capturing sentiment (Socher et al., 2013b; Tai et al., 2015), semantic sentence similarity (Marelli et al., 2014) or textual inference (Bowman et al., 2015).
",9 Conclusions and Future Work,[0],[0]
Acknowledgements This research is supported by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI) and the Israeli Science Foundation (grant number 1555/15).,9 Conclusions and Future Work,[0],[0]
"Algorithm 3 Training on annotated corpus 1: Input: Sentences w1, . . .",Appendix: Training Algorithm Pseudocode,[0],[0]
", wm 2: Input: Tree annotations T 1, . . .",Appendix: Training Algorithm Pseudocode,[0],[0]
", Tm 3: Input: Number of epochs to train
4: V ← InitializeV ectors() 5:",Appendix: Training Algorithm Pseudocode,[0],[0]
"Loss← []
6: for epoch ∈ {1, . . .",Appendix: Training Algorithm Pseudocode,[0],[0]
", Epochs} do 7: for S, T ∈ {(w1, T 1), . . .",Appendix: Training Algorithm Pseudocode,[0],[0]
", (wm, Tm)} do 8:",Appendix: Training Algorithm Pseudocode,[0],[0]
"Loss← TrainSentence (S, V [w1, . . .",Appendix: Training Algorithm Pseudocode,[0],[0]
", wn], T, Loss)
9: if |Loss| > 50 then 10: SumLoss← sum(Loss) 11: Call ADAM to minimize SumLoss 12:",Appendix: Training Algorithm Pseudocode,[0],[0]
"Loss← []
(See Algorithm 4, training of a single sentence, on next page.)
",Appendix: Training Algorithm Pseudocode,[0],[0]
"Algorithm 4 Training on a single sentence with dynamic oracle algorithm 1: function TRAINSENTENCE(w, v, T, Loss) 2:",Appendix: Training Algorithm Pseudocode,[0],[0]
"Input: Sentence w = w1, . . .",Appendix: Training Algorithm Pseudocode,[0],[0]
", wn 3: Input: Vectors vi corresponding to inputs wi 4: Input: Annotated tree T in the form of (h,m, rel) triplets 5: Input: List Loss to which loss expressions are added
6: for i ∈ 1, . . .",Appendix: Training Algorithm Pseudocode,[0],[0]
", n do 7: unassigned[i]← |Children(wi)| 8: pend[i].id← i 9: pend[i].el ← RNNL.init().append(vi)
10: pend[i].er ← RNNR.init().append(vi)
11: while |pend| > 1 do 12: G,W ← {} , {}
13: for (i, d, rel) ∈ {1 ≤",Appendix: Training Algorithm Pseudocode,[0],[0]
"i < |pend|, d ∈ {l, r},",Appendix: Training Algorithm Pseudocode,[0],[0]
rel ∈ Relations} do 14: if d =,Appendix: Training Algorithm Pseudocode,[0],[0]
"l then m,h← pend[i], pend[i+ 1] 15: else m,h← pend[i+ 1], pend[i]
16:",Appendix: Training Algorithm Pseudocode,[0],[0]
"if unassigned[m.id] 6= 0 ∨ ∃`6=rel(h,m, `) ∈ T then 17: W.append((h,m, rel)) 18: else G.append((h,m, rel))
19: hG,mG, relG ← argmax(i,d,`)∈GScore(pend, i, d, `) 20: hW ,mW , relW ← argmax(i,d,`)∈WScore(pend, i, d, `) 21: scoreG ← Score(hG,mG, relG) 22: scoreW ← Score(hW ,mW , relW )
23: if scoreG − scoreW < 0 then 24: h,m, rel, score← hW ,mW , relW , scoreW 25: else if scoreG − scoreW > 1 ∨ random() < paug then 26: h,m, rel, score← hG,mG, relG, scoreG 27: else 28: h,m, rel, score← hW ,mW , relW , scoreW 29: if scoreG − score < 1 then 30: Loss.append(1− scoreG + score)
31: m.c = m.el ◦m.er 32: m.enc = g(W (m.c ◦ rel) + b) 33: if h.id < m.id then h.el.append(m.enc) 34: else h.er.append(m.enc)
35: unassigned[TParent(m).id]← unassigned[TParent(m).id]− 1 36: pend.remove(m) 37: return Loss",Appendix: Training Algorithm Pseudocode,[0],[0]
We suggest a compositional vector representation of parse trees that relies on a recursive combination of recurrent-neural network encoders.,abstractText,[0],[0]
"To demonstrate its effectiveness, we use the representation as the backbone of a greedy, bottom-up dependency parser, achieving very strong accuracies for English and Chinese, without relying on external word embeddings.",abstractText,[0],[0]
The parser’s implementation is available for download at the first author’s webpage.,abstractText,[0],[0]
Easy-First Dependency Parsing with Hierarchical Tree LSTMs,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1412–1421, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
"Neural Machine Translation (NMT) achieved state-of-the-art performances in large-scale translation tasks such as from English to French (Luong et al., 2015) and English to German (Jean et al., 2015).",1 Introduction,[0],[0]
NMT is appealing since it requires minimal domain knowledge and is conceptually simple.,1 Introduction,[0],[0]
The model by Luong et al. (2015) reads through all the source words until the end-of-sentence symbol <eos> is reached.,1 Introduction,[0],[0]
"It then starts emitting one target word at a time, as illustrated in Figure 1.",1 Introduction,[0],[0]
"NMT
1All our code and models are publicly available at http: //nlp.stanford.edu/projects/nmt.
is often a large neural network that is trained in an end-to-end fashion and has the ability to generalize well to very long word sequences.",1 Introduction,[0],[0]
"This means the model does not have to explicitly store gigantic phrase tables and language models as in the case of standard MT; hence, NMT has a small memory footprint.",1 Introduction,[0],[0]
"Lastly, implementing NMT decoders is easy unlike the highly intricate decoders in standard MT (Koehn et al., 2003).
",1 Introduction,[0],[0]
"In parallel, the concept of “attention” has gained popularity recently in training neural networks, allowing models to learn alignments between different modalities, e.g., between image objects and agent actions in the dynamic control problem (Mnih et al., 2014), between speech frames and text in the speech recognition task (Chorowski et al., 2014), or between visual features of a picture and its text description in the image caption generation task (Xu et al., 2015).",1 Introduction,[0],[0]
"In the context of NMT, Bahdanau et al. (2015) has successfully applied such attentional mechanism to jointly translate and align words.",1 Introduction,[0],[0]
"To the best of our knowledge, there has not been any other work exploring the use of attention-based architectures for NMT.
",1 Introduction,[0],[0]
"In this work, we design, with simplicity and effectiveness in mind, two novel types of attention-
1412
based models: a global approach in which all source words are attended and a local one whereby only a subset of source words are considered at a time.",1 Introduction,[0],[0]
"The former approach resembles the model of (Bahdanau et al., 2015) but is simpler architecturally.",1 Introduction,[0],[0]
"The latter can be viewed as an interesting blend between the hard and soft attention models proposed in (Xu et al., 2015): it is computationally less expensive than the global model or the soft attention; at the same time, unlike the hard attention, the local attention is differentiable, making it easier to implement and train.2 Besides, we also examine various alignment functions for our attention-based models.
",1 Introduction,[0],[0]
"Experimentally, we demonstrate that both of our approaches are effective in the WMT translation tasks between English and German in both directions.",1 Introduction,[0],[0]
Our attentional models yield a boost of up to 5.0 BLEU over non-attentional systems which already incorporate known techniques such as dropout.,1 Introduction,[0],[0]
"For English to German translation, we achieve new state-of-the-art (SOTA) results for both WMT’14 and WMT’15, outperforming previous SOTA systems, backed by NMT models and n-gram LM rerankers, by more than 1.0 BLEU.",1 Introduction,[0],[0]
"We conduct extensive analysis to evaluate our models in terms of learning, the ability to handle long sentences, choices of attentional architectures, alignment quality, and translation outputs.",1 Introduction,[0],[0]
"A neural machine translation system is a neural network that directly models the conditional probability p(y|x) of translating a source sentence, x1, . . .",2 Neural Machine Translation,[0],[0]
", xn, to a target sentence, y1, . . .",2 Neural Machine Translation,[0],[0]
", ym.3",2 Neural Machine Translation,[0],[0]
"A basic form of NMT consists of two components: (a) an encoder which computes a representation s for each source sentence and (b) a decoder which generates one target word at a time and hence decomposes the conditional probability as:
log p(y|x)",2 Neural Machine Translation,[0],[0]
"= ∑m
j=1 log p (yj|y<j , s) (1)
",2 Neural Machine Translation,[0],[0]
"A natural choice to model such a decomposition in the decoder is to use a recurrent neural network (RNN) architecture, which most of the re-
2There is a recent work by Gregor et al. (2015), which is very similar to our local attention and applied to the image generation task.",2 Neural Machine Translation,[0],[0]
"However, as we detail later, our model is much simpler and can achieve good performance for NMT.
",2 Neural Machine Translation,[0],[0]
"3All sentences are assumed to terminate with a special “end-of-sentence” token <eos>.
cent NMT work such as (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Jean et al., 2015) have in common.",2 Neural Machine Translation,[0],[0]
"They, however, differ in terms of which RNN architectures are used for the decoder and how the encoder computes the source sentence representation s.
Kalchbrenner and Blunsom (2013) used an RNN with the standard hidden unit for the decoder and a convolutional neural network for encoding the source sentence representation.",2 Neural Machine Translation,[0],[0]
"On the other hand, both Sutskever et al. (2014) and Luong et al. (2015) stacked multiple layers of an RNN with a Long Short-Term Memory (LSTM) hidden unit for both the encoder and the decoder.",2 Neural Machine Translation,[0],[0]
"Cho et al. (2014), Bahdanau et al. (2015), and Jean et al. (2015) all adopted a different version of the RNN with an LSTM-inspired hidden unit, the gated recurrent unit (GRU), for both components.4
In more detail, one can parameterize the probability of decoding each word yj as: p (yj|y<j, s) = softmax (g (hj))",2 Neural Machine Translation,[0],[0]
"(2) with g being the transformation function that outputs a vocabulary-sized vector.5 Here, hj is the RNN hidden unit, abstractly computed as:
hj = f(hj−1, s), (3)
where f computes the current hidden state given the previous hidden state and can be either a vanilla RNN unit, a GRU, or an LSTM unit.",2 Neural Machine Translation,[0],[0]
"In (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Luong et al., 2015), the source representation s is only used once to initialize the decoder hidden state.",2 Neural Machine Translation,[0],[0]
"On the other hand, in (Bahdanau et al., 2015; Jean et al., 2015) and this work, s, in fact, implies a set of source hidden states which are consulted throughout the entire course of the translation process.",2 Neural Machine Translation,[0],[0]
"Such an approach is referred to as an attention mechanism, which we will discuss next.
",2 Neural Machine Translation,[0],[0]
"In this work, following (Sutskever et al., 2014; Luong et al., 2015), we use the stacking LSTM architecture for our NMT systems, as illustrated in Figure 1.",2 Neural Machine Translation,[0],[0]
"We use the LSTM unit defined in (Zaremba et al., 2015).",2 Neural Machine Translation,[0],[0]
"Our training objective is formulated as follows:
",2 Neural Machine Translation,[0],[0]
"Jt = ∑
(x,y)∈D",2 Neural Machine Translation,[0],[0]
− log p(y|x) (4) 4They all used a single RNN layer except for the latter two works which utilized a bidirectional RNN for the encoder.,2 Neural Machine Translation,[0],[0]
"5One can provide g with other inputs such as the currently predicted word yj as in (Bahdanau et al., 2015).
",2 Neural Machine Translation,[0],[0]
with D being our parallel training corpus.,2 Neural Machine Translation,[0],[0]
"Our various attention-based models are classifed into two broad categories, global and local.",3 Attention-based Models,[0],[0]
These classes differ in terms of whether the “attention” is placed on all source positions or on only a few source positions.,3 Attention-based Models,[0],[0]
"We illustrate these two model types in Figure 2 and 3 respectively.
",3 Attention-based Models,[0],[0]
"Common to these two types of models is the fact that at each time step t in the decoding phase, both approaches first take as input the hidden state ht at the top layer of a stacking LSTM.",3 Attention-based Models,[0],[0]
The goal is then to derive a context vector ct that captures relevant source-side information to help predict the current target word yt.,3 Attention-based Models,[0],[0]
"While these models differ in how the context vector ct is derived, they share the same subsequent steps.
",3 Attention-based Models,[0],[0]
"Specifically, given the target hidden state ht and the source-side context vector ct, we employ a simple concatenation layer to combine the information from both vectors to produce an attentional hidden state as follows:
h̃t = tanh(Wc[ct;ht]) (5)
The attentional vector h̃t is then fed through the softmax layer to produce the predictive distribution formulated as:
p(yt|y<t, x) = softmax(Wsh̃t) (6) We now detail how each model type computes
the source-side context vector ct.",3 Attention-based Models,[0],[0]
The idea of a global attentional model is to consider all the hidden states of the encoder when deriving the context vector ct.,3.1 Global Attention,[0],[0]
"In this model type, a variable-length alignment vector at, whose size equals the number of time steps on the source side, is derived by comparing the current target hidden state ht with each source hidden state h̄s:
at(s) = align(ht, h̄s) (7)
= exp
( score(ht, h̄s) )∑ s′ exp ( score(ht, h̄s′)
)",3.1 Global Attention,[0],[0]
"Here, score is referred as a content-based function for which we consider three different alternatives:
score(ht, h̄s) =  h⊤t h̄s dot h⊤t Wah̄s general Wa[ht; h̄s] concat (8)
Besides, in our early attempts to build attentionbased models, we use a location-based function in which the alignment scores are computed from solely the target hidden state ht as follows:
at = softmax(Waht) location (9)
",3.1 Global Attention,[0],[0]
"Given the alignment vector as weights, the context vector ct is computed as the weighted average over all the source hidden states.6
Comparison to (Bahdanau et al., 2015) – While our global attention approach is similar in spirit to the model proposed by Bahdanau et al. (2015), there are several key differences which reflect how we have both simplified and generalized from the original model.",3.1 Global Attention,[0],[0]
"First, we simply use hidden states at the top LSTM layers in both the encoder and decoder as illustrated in Figure 2.",3.1 Global Attention,[0],[0]
"Bahdanau et al. (2015), on the other hand, use the concatenation of the forward and backward source hidden states in the bi-directional encoder and target hidden states in their non-stacking uni-directional decoder.",3.1 Global Attention,[0],[0]
"Second, our computation path is simpler; we go from ht → at → ct → h̃t then make a prediction as detailed in Eq.",3.1 Global Attention,[0],[0]
"(5), Eq. (6), and Figure 2.",3.1 Global Attention,[0],[0]
"On the other hand, at any time t, Bahdanau et al. (2015) build from the previous hidden state ht−1 → at → ct → ht, which, in turn,
6Eq.",3.1 Global Attention,[0],[0]
(9) implies that all alignment vectors at are of the same length.,3.1 Global Attention,[0],[0]
"For short sentences, we only use the top part of at and for long sentences, we ignore words near the end.
",3.1 Global Attention,[0],[0]
"goes through a deep-output and a maxout layer before making predictions.7 Lastly, Bahdanau et al. (2015) only experimented with one alignment function, the concat product; whereas we show later that the other alternatives are better.",3.1 Global Attention,[0],[0]
"The global attention has a drawback that it has to attend to all words on the source side for each target word, which is expensive and can potentially render it impractical to translate longer sequences, e.g., paragraphs or documents.",3.2 Local Attention,[0],[0]
"To address this deficiency, we propose a local attentional mechanism that chooses to focus only on a small subset of the source positions per target word.
",3.2 Local Attention,[0],[0]
This model takes inspiration from the tradeoff between the soft and hard attentional models proposed by Xu et al. (2015) to tackle the image caption generation task.,3.2 Local Attention,[0],[0]
"In their work, soft attention refers to the global attention approach in which weights are placed “softly” over all patches in the source image.",3.2 Local Attention,[0],[0]
"The hard attention, on the other hand, selects one patch of the image to attend to at a time.",3.2 Local Attention,[0],[0]
"While less expensive at inference time, the hard attention model is non-differentiable and requires more complicated techniques such as variance reduction or reinforcement learning to train.
",3.2 Local Attention,[0],[0]
"7We will refer to this difference again in Section 3.3.
",3.2 Local Attention,[0],[0]
Our local attention mechanism selectively focuses on a small window of context and is differentiable.,3.2 Local Attention,[0],[0]
"This approach has an advantage of avoiding the expensive computation incurred in the soft attention and at the same time, is easier to train than the hard attention approach.",3.2 Local Attention,[0],[0]
"In concrete details, the model first generates an aligned position pt for each target word at time t. The context vector ct is then derived as a weighted average over the set of source hidden states within the window",3.2 Local Attention,[0],[0]
"[pt−D, pt+D]; D is empirically selected.8 Unlike the global approach, the local alignment vector at is now fixed-dimensional, i.e., ∈ R2D+1.",3.2 Local Attention,[0],[0]
"We consider two variants of the model as below.
",3.2 Local Attention,[0],[0]
Monotonic alignment (local-m) – we simply set pt = t assuming that source and target sequences are roughly monotonically aligned.,3.2 Local Attention,[0],[0]
The alignment vector at is defined according to Eq.,3.2 Local Attention,[0],[0]
"(7).9
Predictive alignment (local-p) – instead of assuming monotonic alignments, our model predicts an aligned position as follows:
pt = S · sigmoid(v⊤p tanh(Wpht)), (10) Wp and vp are the model parameters which will be learned to predict positions.",3.2 Local Attention,[0],[0]
S is the source sentence length.,3.2 Local Attention,[0],[0]
"As a result of sigmoid, pt ∈",3.2 Local Attention,[0],[0]
"[0, S].",3.2 Local Attention,[0],[0]
"To favor alignment points near pt, we place a Gaussian distribution centered around pt .",3.2 Local Attention,[0],[0]
"Specifically, our alignment weights are now defined as:
at(s) = align(ht, h̄s) exp ( −(s− pt) 2
2σ2
) (11)
",3.2 Local Attention,[0],[0]
We use the same align function as in Eq. (7) and the standard deviation is empirically set as σ= D2 .,3.2 Local Attention,[0],[0]
"It is important to note that pt is a real nummber; whereas s is an integer within the window centered at pt.10
Comparison to (Gregor et al., 2015) – have proposed a selective attention mechanism, very similar to our local attention, for the image generation task.",3.2 Local Attention,[0],[0]
Their approach allows the model to select an image patch of varying location and zoom.,3.2 Local Attention,[0],[0]
"We, instead, use the same “zoom” for all target positions, which greatly simplifies the formulation and still achieves good performance.
8If the window crosses the sentence boundaries, we simply ignore the outside part and consider words in the window.
",3.2 Local Attention,[0],[0]
"9local-m is the same as the global model except that the vector at is fixed-length and shorter.
",3.2 Local Attention,[0],[0]
"10local-p is similar to the local-m model except that we dynamically compute pt and use a Gaussian distribution to modify the original alignment weights align(ht, h̄s) as shown in Eq.",3.2 Local Attention,[0],[0]
(11).,3.2 Local Attention,[0],[0]
"By utilizing pt to derive at, we can compute backprop gradients for Wp and vp.",3.2 Local Attention,[0],[0]
"In our proposed global and local approaches, the attentional decisions are made independently, which is suboptimal.",3.3 Input-feeding Approach,[0],[0]
"Whereas, in standard MT, a coverage set is often maintained during the translation process to keep track of which source words have been translated.",3.3 Input-feeding Approach,[0],[0]
"Likewise, in attentional NMTs, alignment decisions should be made jointly taking into account past alignment information.",3.3 Input-feeding Approach,[0],[0]
"To address that, we propose an inputfeeding approach in which attentional vectors h̃t are concatenated with inputs at the next time steps as illustrated in Figure 4.11 The effects of having such connections are two-fold: (a) we hope to make the model fully aware of previous alignment choices and (b) we create a very deep network spanning both horizontally and vertically.
",3.3 Input-feeding Approach,[0],[0]
"Comparison to other work – Bahdanau et al. (2015) use context vectors, similar to our ct, in building subsequent hidden states, which can also achieve the “coverage” effect.",3.3 Input-feeding Approach,[0],[0]
"However, there has not been any analysis of whether such connections are useful as done in this work.",3.3 Input-feeding Approach,[0],[0]
"Also, our approach is more general; as illustrated in Figure 4, it can be applied to general stacking recurrent architectures, including non-attentional models.
",3.3 Input-feeding Approach,[0],[0]
Xu et al. (2015) propose a doubly attentional approach with an additional constraint added to the training objective to make sure the model pays equal attention to all parts of the image during the caption generation process.,3.3 Input-feeding Approach,[0],[0]
"Such a constraint can
11If n is the number of LSTM cells, the input size of the first LSTM layer is 2n; those of subsequent layers are n.
also be useful to capture the coverage set effect in NMT that we mentioned earlier.",3.3 Input-feeding Approach,[0],[0]
"However, we chose to use the input-feeding approach since it provides flexibility for the model to decide on any attentional constraints it deems suitable.",3.3 Input-feeding Approach,[0],[0]
We evaluate the effectiveness of our models on the WMT translation tasks between English and German in both directions.,4 Experiments,[0],[0]
newstest2013 (3000 sentences) is used as a development set to select our hyperparameters.,4 Experiments,[0],[0]
"Translation performances are reported in case-sensitive BLEU (Papineni et al., 2002) on newstest2014 (2737 sentences) and newstest2015 (2169 sentences).",4 Experiments,[0],[0]
"Following (Luong et al., 2015), we report translation quality using two types of BLEU: (a) tokenized12 BLEU to be comparable with existing NMT work and (b) NIST13",4 Experiments,[0],[0]
BLEU to be comparable with WMT results.,4 Experiments,[0],[0]
"All our models are trained on the WMT’14 training data consisting of 4.5M sentences pairs (116M English words, 110M German words).",4.1 Training Details,[0],[0]
"Similar to (Jean et al., 2015), we limit our vocabularies to be the top 50K most frequent words for both languages.",4.1 Training Details,[0],[0]
"Words not in these shortlisted vocabularies are converted into a universal token <unk>.
",4.1 Training Details,[0],[0]
"When training our NMT systems, following (Bahdanau et al., 2015; Jean et al., 2015), we filter out sentence pairs whose lengths exceed 50 words and shuffle mini-batches as we proceed.",4.1 Training Details,[0],[0]
"Our stacking LSTM models have 4 layers, each with 1000 cells, and 1000-dimensional embeddings.",4.1 Training Details,[0],[0]
"We follow (Sutskever et al., 2014; Luong et al., 2015) in training NMT with similar settings: (a) our parameters are uniformly initialized in [−0.1, 0.1], (b) we train for 10 epochs using plain SGD, (c) a simple learning rate schedule is employed – we start with a learning rate of 1; after 5 epochs, we begin to halve the learning rate every epoch, (d) our mini-batch size is 128, and (e) the normalized gradient is rescaled whenever its norm exceeds 5.",4.1 Training Details,[0],[0]
"Additionally, we also use dropout for our LSTMs as suggested by (Zaremba et al., 2015).",4.1 Training Details,[0],[0]
"For dropout models, we train for 12 epochs and start halving the learning rate after 8 epochs.
",4.1 Training Details,[0],[0]
Our code is implemented in MATLAB.,4.1 Training Details,[0],[0]
"When
12All texts are tokenized with tokenizer.perl and BLEU scores are computed with multi-bleu.perl.
13With the mteval-v13a script as per WMT guideline.
running on a single GPU device Tesla K40, we achieve a speed of 1K target words per second.",4.1 Training Details,[0],[0]
It takes 7–10 days to completely train a model.,4.1 Training Details,[0],[0]
We compare our NMT systems in the EnglishGerman task with various other systems.,4.2 English-German Results,[0],[0]
"These include the winning system in WMT’14 (Buck et al., 2014), a phrase-based system whose language models were trained on a huge monolingual text, the Common Crawl corpus.",4.2 English-German Results,[0],[0]
"For end-to-end neural machine translation systems, to the best of our knowledge, (Jean et al., 2015) is the only work experimenting with this language pair and currently the SOTA system.",4.2 English-German Results,[0],[0]
"We only present results for some of our attention models and will later analyze the rest in Section 5.
",4.2 English-German Results,[0],[0]
"As shown in Table 1, we achieve progressive improvements when (a) reversing the source sentence, +1.3 BLEU, as proposed in (Sutskever et al., 2014) and (b) using dropout, +1.4 BLEU.",4.2 English-German Results,[0],[0]
"On top of that, (c) the global attention approach gives a significant boost of +2.8 BLEU, making our model slightly better than the base attentional system of Bahdanau et al. (2015) (row RNNSearch).",4.2 English-German Results,[0],[0]
"When (d) using the input-feeding approach, we seize another notable gain of +1.3 BLEU and outperform their system.",4.2 English-German Results,[0],[0]
"The local attention model with predictive alignments (row local-p) proves to be even better, giving us a further improvement of +0.9 BLEU on top of the global attention
model.",4.2 English-German Results,[0],[0]
"It is interesting to observe the trend previously reported in (Luong et al., 2015) that perplexity strongly correlates with translation quality.",4.2 English-German Results,[0],[0]
"In total, we achieve a significant gain of 5.0 BLEU points over the non-attentional baseline, which already includes known techniques such as source reversing and dropout.
",4.2 English-German Results,[0],[0]
"The unknown replacement technique proposed in (Luong et al., 2015; Jean et al., 2015) yields another nice gain of +1.9 BLEU, demonstrating that our attentional models do learn useful alignments for unknown works.",4.2 English-German Results,[0],[0]
"Finally, by ensembling 8 different models of various settings, e.g., using different attention approaches, with and without dropout etc., we were able to achieve a new SOTA result of 23.0 BLEU, outperforming the existing best system (Jean et al., 2015) by +1.4 BLEU.
",4.2 English-German Results,[0],[0]
"Latest results in WMT’15 – despite the fact that our models were trained on WMT’14 with slightly less data, we test them on newstest2015 to demonstrate that they can generalize well to different test sets.",4.2 English-German Results,[0],[0]
"As shown in Table 2, our best system es-
tablishes a new SOTA performance of 25.9 BLEU, outperforming the existing best system backed by NMT and a 5-gram LM reranker by +1.0 BLEU.",4.2 English-German Results,[0],[0]
We carry out a similar set of experiments for the WMT’15 translation task from German to English.,4.3 German-English Results,[0],[0]
"While our systems have not yet matched the performance of the SOTA system, we nevertheless show the effectiveness of our approaches with large and progressive gains in terms of BLEU as illustrated in Table 3.",4.3 German-English Results,[0],[0]
"The attentional mechanism gives us +2.2 BLEU gain and on top of that, we obtain another boost of up to +1.0 BLEU from the input-feeding approach.",4.3 German-English Results,[0],[0]
"Using a better alignment function, the content-based dot product one, together with dropout yields another gain of +2.7 BLEU.",4.3 German-English Results,[0],[0]
"Lastly, when applying the unknown word replacement technique, we seize an additional +2.1 BLEU, demonstrating the usefulness of attention in aligning rare words.",4.3 German-English Results,[0],[0]
"We conduct extensive analysis to better understand our models in terms of learning, the ability to handle long sentences, choices of attentional architectures, and alignment quality.",5 Analysis,[0],[0]
All models considered here are English-German NMT systems tested on newstest2014.,5 Analysis,[0],[0]
We compare models built on top of one another as listed in Table 1.,5.1 Learning curves,[0],[0]
It is pleasant to observe in Figure 5 a clear separation between non-attentional and attentional models.,5.1 Learning curves,[0],[0]
"The input-feeding ap-
proach and the local attention model also demonstrate their abilities in driving the test costs lower.",5.1 Learning curves,[0],[0]
"The non-attentional model with dropout (the blue + curve) learns slower than other non-dropout models, but as time goes by, it becomes more robust in terms of minimizing test errors.",5.1 Learning curves,[0],[0]
"We follow (Bahdanau et al., 2015) to group sentences of similar lengths together and compute a BLEU score per group.",5.2 Effects of Translating Long Sentences,[0],[0]
"As demonstrated in Figure 6, our attentional models are more effective than the other non-attentional model in handling long sentences: the translation quality does not degrade as sentences become longer.",5.2 Effects of Translating Long Sentences,[0],[0]
Our best model (the blue + curve) outperforms all other systems in all length buckets.,5.2 Effects of Translating Long Sentences,[0],[0]
"We examine different attention models (global, local-m, local-p) and different alignment functions (location, dot, general, concat) as described in Section 3.",5.3 Choices of Attentional Architectures,[0],[0]
"Due to limited resources, we cannot run all the possible combinations.",5.3 Choices of Attentional Architectures,[0],[0]
"However,
results in Table 4 do give us some idea about different choices.",5.3 Choices of Attentional Architectures,[0],[0]
"The location-based function does not learn good alignments: the global (location) model can only obtain a small gain when performing unknown word replacement compared to using other alignment functions.14 For content-based functions, our implementation of concat does not yield good performances and more analysis should be done to understand the reason.15",5.3 Choices of Attentional Architectures,[0],[0]
It is interesting to observe that dot works well for the global attention and general is better for the local attention.,5.3 Choices of Attentional Architectures,[0],[0]
"Among the different models, the local attention model with predictive alignments (local-p) is best, both in terms of perplexities and BLEU.",5.3 Choices of Attentional Architectures,[0],[0]
A by-product of attentional models are word alignments.,5.4 Alignment Quality,[0],[0]
"While (Bahdanau et al., 2015) visualized alignments for some sample sentences and observed gains in translation quality as an indication of a working attention model, no work has assessed the alignments learned as a whole.",5.4 Alignment Quality,[0],[0]
"In contrast, we set out to evaluate the alignment quality using the alignment error rate (AER) metric.
",5.4 Alignment Quality,[0],[0]
"Given the gold alignment data provided by RWTH for 508 English-German Europarl sentences, we “force” decode our attentional models to produce translations that match the references.",5.4 Alignment Quality,[0],[0]
"We extract only one-to-one alignments by selecting the source word with the highest alignment
14There is a subtle difference in how we retrieve alignments for the different alignment functions.",5.4 Alignment Quality,[0],[0]
"At time step t in which we receive yt−1 as input and then compute ht,at, ct, and h̃t before predicting yt, the alignment vector at is used as alignment weights for (a) the predicted word yt in the location-based alignment functions and (b) the input word yt−1 in the content-based functions.
",5.4 Alignment Quality,[0],[0]
"15With concat, the perplexities achieved by different models are 6.7 (global), 7.1 (local-m), and 7.1 (local-p).
",5.4 Alignment Quality,[0],[0]
weight per target word.,5.4 Alignment Quality,[0],[0]
"Nevertheless, as shown in Table 6, we were able to achieve AER scores comparable to the one-to-many alignments obtained by the Berkeley aligner (Liang et al., 2006).16
",5.4 Alignment Quality,[0],[0]
We also found that the alignments produced by local attention models achieve lower AERs than those of the global one.,5.4 Alignment Quality,[0],[0]
"The AER obtained by the ensemble, while good, is not better than the local-m AER, suggesting the well-known observation that AER and translation scores are not well correlated (Fraser and Marcu, 2007).",5.4 Alignment Quality,[0],[0]
"Due to space constraint, we can only show alignment visualizations in the arXiv version of our paper.17",5.4 Alignment Quality,[0],[0]
We show in Table 5 sample translations in both directions.,5.5 Sample Translations,[0],[0]
It it appealing to observe the effect of attentional models in correctly translating names such as “Miranda Kerr” and “Roger Dow”.,5.5 Sample Translations,[0],[0]
"Non-attentional models, while producing sensible names from a language model perspective, lack the direct connections from the source side to make correct translations.
",5.5 Sample Translations,[0],[0]
"We also observed an interesting case in the second English-German example, which requires translating the doubly-negated phrase, “not incompatible”.",5.5 Sample Translations,[0],[0]
The attentional model correctly produces “nicht . . .,5.5 Sample Translations,[0],[0]
"unvereinbar”; whereas the non-attentional model generates “nicht vereinbar”, meaning “not compatible”.18",5.5 Sample Translations,[0],[0]
The attentional model also demonstrates its superiority in translating long sentences as in the last example.,5.5 Sample Translations,[0],[0]
"In this paper, we propose two simple and effective attentional mechanisms for neural machine
16We concatenate the 508 sentence pairs with 1M sentence pairs from WMT and run the Berkeley aligner.
",6 Conclusion,[0],[0]
"17http://arxiv.org/abs/1508.04025 18The reference uses a more fancy translation of “incompatible”, which is “im Widerspruch zu etwas stehen”.",6 Conclusion,[0],[0]
"Both models, however, failed to translate “passenger experience”.
",6 Conclusion,[0],[0]
translation: the global approach which always looks at all source positions and the local one that only attends to a subset of source positions at a time.,6 Conclusion,[0],[0]
We test the effectiveness of our models in the WMT translation tasks between English and German in both directions.,6 Conclusion,[0],[0]
Our local attention yields large gains of up to 5.0 BLEU over non-attentional models that already incorporate known techniques such as dropout.,6 Conclusion,[0],[0]
"For the English to German translation direction, our ensemble model has established new state-of-the-art results for both WMT’14 and WMT’15.
",6 Conclusion,[0],[0]
We have compared various alignment functions and shed light on which functions are best for which attentional models.,6 Conclusion,[0],[0]
"Our analysis shows that attention-based NMT models are superior to nonattentional ones in many cases, for example in
translating names and handling long sentences.",6 Conclusion,[0],[0]
We gratefully acknowledge support from a gift from Bloomberg L.P. and the support of NVIDIA Corporation with the donation of Tesla K40 GPUs.,Acknowledgment,[0],[0]
We thank Andrew Ng and his group as well as the Stanford Research Computing for letting us use their computing resources.,Acknowledgment,[0],[0]
We thank Russell Stewart for helpful discussions on the models.,Acknowledgment,[0],[0]
"Lastly, we thank Quoc Le, Ilya Sutskever, Oriol Vinyals, Richard Socher, Michael Kayser, Jiwei Li, Panupong Pasupat, Kelvin Gu, members of the Stanford NLP Group and the annonymous reviewers for their valuable comments and feedback.",Acknowledgment,[0],[0]
An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation.,abstractText,[0],[0]
"However, there has been little work exploring useful architectures for attention-based NMT.",abstractText,[0],[0]
This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time.,abstractText,[0],[0]
We demonstrate the effectiveness of both approaches on the WMT translation tasks between English and German in both directions.,abstractText,[0],[0]
"With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems that already incorporate known techniques such as dropout.",abstractText,[0],[0]
"Our ensemble model using different attention architectures yields a new state-of-the-art result in the WMT’15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.1",abstractText,[0],[0]
Effective Approaches to Attention-based Neural Machine Translation,title,[0],[0]
How can the elements from two sets be paired one-to-one to have the largest sum of pairwise utilities?,1 Introduction,[0],[0]
This maximum weighted perfect bipartite matching problem is a classical combinatorial optimization problem in computer science.,1 Introduction,[0],[0]
"It can be formulated and efficiently solved in polynomial time as a linear program or using more specialized Hungarian algorithm techniques (Kuhn, 1955).",1 Introduction,[0],[0]
"This has made it an
*Equal contribution 1Department of Computer Science, University of Illinois at Chicago.",1 Introduction,[0],[0]
"Correspondence to: Rizal Fathony <rfatho2@uic.edu>, Sima Behpour <sbehpo2@uic.edu>.
",1 Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1 Introduction,[0],[0]
"Copyright 2018 by the author(s).
attractive formalism for posing a wide range of problems, including recognizing correspondences in similar images (Belongie et al., 2002; Liu et al., 2008; Zhu et al., 2008; Rui et al., 2007), finding word alignments in text (Chan & Ng, 2008), and providing ranked lists of items for information retrieval tasks (Amini et al., 2008).
",1 Introduction,[0],[0]
Machine learning methods seek to estimate the pairwise utilities of bipartite graphs so that the maximum weighted complete matching is most compatible with the (distribution of) ground truth matchings of training data.,1 Introduction,[1.0],['Machine learning methods seek to estimate the pairwise utilities of bipartite graphs so that the maximum weighted complete matching is most compatible with the (distribution of) ground truth matchings of training data.']
"When these utilities are learned abstractly, they can be employed to make predictive matchings for test samples.",1 Introduction,[0],[0]
"Unfortunately, important measures of incompatibility (e.g., the Hamming loss) are often non-continuous with many local optima in the predictors’ parameter spaces, making direct minimization intractable.",1 Introduction,[0],[0]
"Given this difficulty, two natural desiderata for any predictor are:
• Efficiency: learning from training data and making predictions must be computed efficiently in (lowdegree) polynomial time; and • Consistency: the predictor’s training objectives must also minimize the underlying Hamming loss, at least under ideal learning conditions (given the true distribution and fully expressive model parameters).
",1 Introduction,[0.9999999583850854],"['Given this difficulty, two natural desiderata for any predictor are: • Efficiency: learning from training data and making predictions must be computed efficiently in (lowdegree) polynomial time; and • Consistency: the predictor’s training objectives must also minimize the underlying Hamming loss, at least under ideal learning conditions (given the true distribution and fully expressive model parameters).']"
"Existing methods for learning bipartite matchings fail in one or the other of these desiderata; exponentiated potential fields models (Lafferty et al., 2001; Petterson et al., 2009) are intractable for large sets of items, while maximum margin methods based on the hinge loss surrogate (Taskar et al., 2005a; Tsochantaridis et al., 2005) lack Fisher consistency (Tewari & Bartlett, 2007; Liu, 2007).",1 Introduction,[0],[0]
"We discuss these limitations formally in Section 2.
",1 Introduction,[0],[0]
"Given the deficiencies of the existing methods, we contribute the first approach for learning bipartite matchings that is both computationally efficient and Fisher consistent.",1 Introduction,[1.0],"['Given the deficiencies of the existing methods, we contribute the first approach for learning bipartite matchings that is both computationally efficient and Fisher consistent.']"
"Our approach is based on an adversarial formulation for learning (Topsøe, 1979; Grünwald & Dawid, 2004; Asif et al., 2015) that poses prediction-making as a dataconstrained zero-sum game between a player seeking to minimize the expected loss and an adversarial data approximator seeking to maximize the expected loss.",1 Introduction,[0],[0]
"We present two approaches for solving the corresponding zero-sum game arising from our formulation: (1) using the double
oracle method of constraint generation to find a sparselysupported equilibrium for the zero-sum game; and (2) decomposing the game’s solution into marginal probabilities and optimizes these marginal probabilities directly to obtain an equilibrium saddle point for the game.",1 Introduction,[0],[0]
We then establish the computational efficiency and consistency of this approach and demonstrate its benefits experimentally.,1 Introduction,[0],[0]
"Given two sets of elements A and B of equal size (|A| = |B|), a maximum weighted bipartite matching π is the one-toone mapping (e.g., Figure 1) from each element in A to each element in B that maximizes the sum of potentials: maxπ∈Π ψ(π) = maxπ∈Π ∑ i ψi(πi).",2.1 Bipartite Matching Task,[1.0],"['Given two sets of elements A and B of equal size (|A| = |B|), a maximum weighted bipartite matching π is the one-toone mapping (e.g., Figure 1) from each element in A to each element in B that maximizes the sum of potentials: maxπ∈Π ψ(π) = maxπ∈Π ∑ i ψi(πi).']"
"Here πi ∈
",2.1 Bipartite Matching Task,[0],[0]
"[n] := {1, 2, . . .",2.1 Bipartite Matching Task,[0],[0]
", n} is the entry in B that is matched with the i-th entry of A. The set of possible solutions Π is simply all permutation of [n].",2.1 Bipartite Matching Task,[0],[0]
"Many machine learning tasks pose prediction as the solution to this problem, including: word alignment for natural language processing tasks (Taskar et al., 2005b; Padó & Lapata, 2006; MacCartney et al., 2008); learning correspondences between images in computer vision applications (Belongie et al., 2002; Dellaert et al., 2003); protein structure analysis in computational biology (Taylor, 2002; Wang et al., 2004); and learning to rank a set of items for information retrieval tasks (Dwork et al., 2001; Le & Smola, 2007).",2.1 Bipartite Matching Task,[1.0],"['Many machine learning tasks pose prediction as the solution to this problem, including: word alignment for natural language processing tasks (Taskar et al., 2005b; Padó & Lapata, 2006; MacCartney et al., 2008); learning correspondences between images in computer vision applications (Belongie et al., 2002; Dellaert et al., 2003); protein structure analysis in computational biology (Taylor, 2002; Wang et al., 2004); and learning to rank a set of items for information retrieval tasks (Dwork et al., 2001; Le & Smola, 2007).']"
"Thus, learning appropriate weights ψi(·) for bipartite graph matchings is a key problem for many application areas.",2.1 Bipartite Matching Task,[1.0],"['Thus, learning appropriate weights ψi(·) for bipartite graph matchings is a key problem for many application areas.']"
"Given a predicted permutation, π′, and the “ground truth” permutation, π, the Hamming loss counts the number of mistaken pairings: lossHam(π, π′) =",2.2 Performance Evaluation and Fisher Consistency,[0],[0]
∑n i=1,2.2 Performance Evaluation and Fisher Consistency,[0],[0]
1(π ′,2.2 Performance Evaluation and Fisher Consistency,[0],[0]
"i 6= πi), where 1(·) = 1 if · is true and 0 otherwise.",2.2 Performance Evaluation and Fisher Consistency,[0],[0]
"When the “ground truth” is a distribution over permutations, P (π), rather than a single permutation, the (set of) Bayes optimal prediction(s) is: argminπ′ ∑ π P (π) lossHam(π, π
′).",2.2 Performance Evaluation and Fisher Consistency,[0],[0]
"For a predictor to be Fisher consistent, it must provide a Bayes optimal prediction for any possible distribution P (π) when trained from that exact distribution using the predictor’s most general possible parameterization (e.g., all measurable functions ψ for potential-based models).",2.2 Performance Evaluation and Fisher Consistency,[0],[0]
"A probabilistic approach to learning bipartite graphs uses an exponential family distribution over permutations,
Pψ(π) = e",2.3 Exponential Family Random Field Approach,[0],[0]
∑n i=1,2.3 Exponential Family Random Field Approach,[0],[0]
"ψi(πi)/Zψ , trained by maximizing training data likelihood.",2.3 Exponential Family Random Field Approach,[0],[0]
"This provides certain statistical consistency guarantees for its marginal probability estimates (Petterson et al., 2009).",2.3 Exponential Family Random Field Approach,[0],[0]
"Specifically, if the potentials ψ are chosen from the space of all measurable functions to maximize the likelihood of the true distribution of permutations P (π), then Pψ(π) will match the marginal probabilities of the true distribution: ∀i, j, Pψ(πi = j) = P (πi = j).",2.3 Exponential Family Random Field Approach,[0],[0]
"This implies Fisher consistency because the MAP estimate under this distribution, which can be obtained as a maximum weighted bipartite matching, is Bayes optimal.
",2.3 Exponential Family Random Field Approach,[1.0000000083829454],"['This implies Fisher consistency because the MAP estimate under this distribution, which can be obtained as a maximum weighted bipartite matching, is Bayes optimal.']"
The key challenge with this approach is its computational complexity.,2.3 Exponential Family Random Field Approach,[0],[0]
"The normalization term, Zψ , is the permanent of a matrix defined in terms of exponentiated potential terms:",2.3 Exponential Family Random Field Approach,[0],[0]
"Zψ = ∑ π ∏n i=1 e
ψi(πi) = perm(M) where Mi,j = e
ψi(j).",2.3 Exponential Family Random Field Approach,[0],[0]
"For sets of small size (e.g., n = 5), enumerating the permutations is tractable and learning using the exponential random field model incurs a run-time cost that is acceptable in practice (Petterson et al., 2009).",2.3 Exponential Family Random Field Approach,[0],[0]
"However, the matrix permanent computation is a #P-hard problem to compute exactly (Valiant, 1979).",2.3 Exponential Family Random Field Approach,[0],[0]
"Monte Carlo sampling approaches are used instead of permutation enumeration to maximize the data likelihood (Petterson et al., 2009; Volkovs & Zemel, 2012).",2.3 Exponential Family Random Field Approach,[0],[0]
"Though exact samples can be generated efficiently in polynomial time (Huber & Law, 2008), the number of samples needed for reliable likelihood or gradient estimates makes this approach infeasible for applications with even modestly-sized sets of n = 20 elements (Petterson et al., 2009).",2.3 Exponential Family Random Field Approach,[0],[0]
Maximum margin methods for structured prediction seek potentials ψ,2.4 Maximum Margin Approach,[0],[0]
"that minimize the training sample hinge loss:
min ψ",2.4 Maximum Margin Approach,[0],[0]
"Eπ∼P̃ [ max π′ {loss(π, π′) + ψ(π′)}",2.4 Maximum Margin Approach,[0],[0]
"− ψ(π) ] , (1)
where P̃ is the empirical distribution.",2.4 Maximum Margin Approach,[0],[0]
"Finding the optimal ψ is a convex optimization problem (Boyd & Vandenberghe, 2004) that can generally be tractably solved using constraint generation methods as long as the maximizing assignments can be found efficiently.",2.4 Maximum Margin Approach,[1.0],"['Finding the optimal ψ is a convex optimization problem (Boyd & Vandenberghe, 2004) that can generally be tractably solved using constraint generation methods as long as the maximizing assignments can be found efficiently.']"
"In the case of permutation learning, finding the permutation π′ with highest hinge loss reduces to a maximum weighted bipartite matching problem and can therefore be solved efficiently.
",2.4 Maximum Margin Approach,[0],[0]
"Though computationally efficient, maximum margin approaches for learning to make perfect bipartite matches lack Fisher consistency, which requires the prediction π∗ = argmaxπ ψ(π) resulting from Equation (1) to minimize the expected risk,",2.4 Maximum Margin Approach,[0],[0]
Eπ∼P̃,2.4 Maximum Margin Approach,[0],[0]
"[loss(π, π′)], for all distributions P̃ .",2.4 Maximum Margin Approach,[0],[0]
"We consider a distribution over permutations that is an extension of a counterexample for multiclass classification consistency analysis with no majority label (Liu,
2007): P (π = [1 2 3])",2.4 Maximum Margin Approach,[0],[0]
= 0.4;P,2.4 Maximum Margin Approach,[0],[0]
(π = [2 3 1]) = 0.3; and P (π = [3 1 2]) = 0.3.,2.4 Maximum Margin Approach,[0],[0]
"The potential function ψi(j) = 1 if i = j and 0 otherwise, provides a Bayes optimal permutation prediction for this distribution and an expected hinge loss of 3.6 = 0.4(3 − 3) + 0.3(3 + 3) + 0.3(3 + 3).",2.4 Maximum Margin Approach,[0],[0]
"However, the expected hinge loss is optimally minimized with a value of 3 when ψi(j) = 0,∀i, j, which is indifferent between all permutations and is not Bayes optimal.",2.4 Maximum Margin Approach,[0],[0]
"Thus, Fisher consistency is not guaranteed.",2.4 Maximum Margin Approach,[0],[0]
"To overcome the computational inefficiency of exponential random field methods and the Fisher inconsistency of maximum margin methods, we formulate the task of learning for bipartite matching problems as an adversarial structured prediction task.",3 Approach,[0],[0]
We present two approaches for efficiently solving the resulting game over permutations.,3 Approach,[0],[0]
"The training data for bipartite matching consists of triplets (A,B, π) where A and B are two sets of nodes with equal size and π is the assignment.",3.1 Permutation Mixture Formulation,[0],[0]
"To simplify the notation, we denote x as the bipartite graph containing the nodes A and B. We also denote φ(x, π) as a vector that enumerates the joint feature representations based on the bipartite graph x and the matching assignment π.",3.1 Permutation Mixture Formulation,[0],[0]
"This joint feature is defined additively over each node assignment, i.e., φ(x, π) = ∑n i=1",3.1 Permutation Mixture Formulation,[0],[0]
"φi(x, πi).
",3.1 Permutation Mixture Formulation,[0],[0]
Our approach seeks a predictor that robustly minimizes the Hamming loss against the worst-case permutation mixture probability that is consistent with the statistics of the training data.,3.1 Permutation Mixture Formulation,[1.0],['Our approach seeks a predictor that robustly minimizes the Hamming loss against the worst-case permutation mixture probability that is consistent with the statistics of the training data.']
"In this setting, a predictor makes a probabilistic prediction over the set of all possible assignments (denoted as P̂ ).",3.1 Permutation Mixture Formulation,[0],[0]
"Instead of evaluating the predictor with the empirical distribution, the predictor is pitted against an adversary that also makes a probabilistic prediction (denoted as P̌ ).",3.1 Permutation Mixture Formulation,[1.0],"['Instead of evaluating the predictor with the empirical distribution, the predictor is pitted against an adversary that also makes a probabilistic prediction (denoted as P̌ ).']"
"The predictor’s objective is to minimize the expected loss function calculated from the predictor’s and adversary’s probabilistic predictions, while the adversary seeks to maximize the loss.",3.1 Permutation Mixture Formulation,[0],[0]
"The adversary (and only the adversary) is constrained to select a probabilistic prediction that matches the statistical summaries of the empirical training distribution (denoted as P̃ ) via moment matching constraints on joint features φ(x, π).",3.1 Permutation Mixture Formulation,[1.0],"['The adversary (and only the adversary) is constrained to select a probabilistic prediction that matches the statistical summaries of the empirical training distribution (denoted as P̃ ) via moment matching constraints on joint features φ(x, π).']"
"Formally, we write our formulation as:
min P̂ (π̂|x) max P̌ (π̌|x) Ex∼P̃ ;π̂|x∼P̂ ;π̌|x∼P̌ [loss(π̂, π̌)] s.t. (2)
Ex∼P̃ ;π̌|x∼P̌ [ n∑ i=1",3.1 Permutation Mixture Formulation,[0],[0]
"φi(x, π̌i)",3.1 Permutation Mixture Formulation,[0],[0]
"] = E(x,π)∼P̃ [ n∑ i=1",3.1 Permutation Mixture Formulation,[0],[0]
"φi(x, πi) ] .
",3.1 Permutation Mixture Formulation,[0],[0]
"This follows a recent line of work for adversarial classification under additive (Asif et al., 2015) and non-additive
(Wang et al., 2015) loss functions that has been employed for chain-structured prediction (Li et al., 2016), object detection (Behpour et al., 2017), and robust cut learning (Behpour et al., 2018).",3.1 Permutation Mixture Formulation,[0],[0]
"Using the method of Lagrangian multipliers and strong duality for convex-concave saddle point problems (Von Neumann & Morgenstern, 1945; Sion, 1958), The optimization in Eq. (2) can be equivalently solved in the dual formulation:
min θ Ex,π∼P̃ min P̂ (π̂|x) max P̌ (π̌|x) Eπ̂|x∼P̂ π̌|x∼P̌
[ loss(π̂, π̌)+ (3)
",3.1 Permutation Mixture Formulation,[0],[0]
θ · n∑ i=1,3.1 Permutation Mixture Formulation,[0],[0]
"(φi(x, π̌i)− φi(x, πi)) ] ,
where θ is the Lagrange dual variable for the moment matching constraints.",3.1 Permutation Mixture Formulation,[0],[0]
"We refer the reader to Appendix A in the supplementary materials for a more detailed explanation of this construction (i.e., the transformation from Eq.",3.1 Permutation Mixture Formulation,[0],[0]
(2) to Eq. (3)).,3.1 Permutation Mixture Formulation,[0],[0]
"In this paper, we use Hamming distance, loss(π̂, π̌) = ∑n i=1",3.1 Permutation Mixture Formulation,[0],[0]
"1(π̂i 6= π̌i), as the loss function.
",3.1 Permutation Mixture Formulation,[0],[0]
Table 1 shows the payoff matrix for the game of size n = 3 with 3!,3.1 Permutation Mixture Formulation,[0],[0]
"actions (permutations) for the predictor player π̂ and for the adversarial approximation player π̌. Here, we define the difference between the Lagrangian potential of the adversary’s action and the ground truth permutation as δπ̌ = ψ(π̌)− ψ(π) = θ · ∑n i=1",3.1 Permutation Mixture Formulation,[0],[0]
"(φi(x, π̌i)− φi(x, πi)) .
",3.1 Permutation Mixture Formulation,[0],[0]
"Unfortunately, the number of permutations, π, grows factorially (O(n!))",3.1 Permutation Mixture Formulation,[0],[0]
with the number of elements being matched (n).,3.1 Permutation Mixture Formulation,[0],[0]
This makes explicit construction of the Lagrangian minimax game intractable for modestly-sized problems.,3.1 Permutation Mixture Formulation,[1.0],['This makes explicit construction of the Lagrangian minimax game intractable for modestly-sized problems.']
"Our first approach for taming the factorial computational complexity of explicitly constructing games for matching tasks is a constraint-generation approach known as the double oracle method (McMahan et al., 2003).",3.2 Optimization by Constraint Generation,[1.0],"['Our first approach for taming the factorial computational complexity of explicitly constructing games for matching tasks is a constraint-generation approach known as the double oracle method (McMahan et al., 2003).']"
It obtains the equilibrium solution to the adversarial prediction game without explicitly constructing the entire game matrix (Table 1).,3.2 Optimization by Constraint Generation,[0],[0]
"Based on the key observation that the equilibrium of the zero-sum game is typically supported by a relatively small number of permutations, it seeks to efficiently uncover this sparse set of permutations for each player.
",3.2 Optimization by Constraint Generation,[0.9999999524401763],"['Based on the key observation that the equilibrium of the zero-sum game is typically supported by a relatively small number of permutations, it seeks to efficiently uncover this sparse set of permutations for each player.']"
Algorithm 1 Double Oracle Algorithm for Adversarial Bipartite Matching Equilibria.,3.2 Optimization by Constraint Generation,[0],[0]
"Input: Lagrangian potentials Ψ(·); Initial label πinitial Output: The (sparse) Nash equilibrium (Š, Ŝ, P̂ , P̌ )
1: Š ← Ŝ ← {πinitial} 2: repeat 3: (P̂ , P̌ , V̌ )← solveGame(Ψ(Š), lossHam(Ŝ, Š))",3.2 Optimization by Constraint Generation,[0],[0]
"4: (π̌new,Vmax)←argmaxπ̌Eπ̂∼P̂ [lossHam(π̂, π̌)+Ψ(π̌)] 5: if (V̌ 6= Vmax) then Š ← Š ∪ π̌new 6: (P̂ , P̌ , V̂ )← solveGame(Ψ(Š), lossHam(Ŝ, Š)) 7: (π̂new, Vmin)← argminπ̂",3.2 Optimization by Constraint Generation,[0],[0]
"Eπ̌∼P̌ [lossHam(π̂, π̌)] 8: if (V̂ 6= Vmin) then Ŝ ← Ŝ ∪ π̂new 9: until V̌ = Vmax = V̂ = Vmin
10: return (Š, Ŝ, P̂ , P̌ )
Algorithm 1 produces this set of “active” permutations for each player, Ŝ and Š (subsets of rows and columns in Table 1), and the associated Nash equilibrium (P̂ , P̌ ).",3.2 Optimization by Constraint Generation,[0],[0]
"Starting from an initial permutation, πinitial (Line 1), it repeatedly obtains the Nash equilibrium solution (P̂ , P̌ ) with value V̂ or V̌ for the zero-sum game defined only by permutations in Ŝ and Š (Lines 3 and 6).",3.2 Optimization by Constraint Generation,[0],[0]
"This is efficiently accomplished using a linear program (Von Neumann & Morgenstern, 1945).",3.2 Optimization by Constraint Generation,[0],[0]
"The algorithm then obtains the other player’s best response to either P̂ or P̌ (Lines 4 and 7) with values Vmax and Vmin using the Kuhn-Munkres (Hungarian) algorithm in O(n3) time for sets of size n. These best responses, π̌new and π̂new, are added to the set of active permutations (i.e., new rows or columns in the game matrix) if they have better values than the previous equilibrium values (Lines 5 and 8).",3.2 Optimization by Constraint Generation,[1.0],"['The algorithm then obtains the other player’s best response to either P̂ or P̌ (Lines 4 and 7) with values Vmax and Vmin using the Kuhn-Munkres (Hungarian) algorithm in O(n3) time for sets of size n. These best responses, π̌new and π̂new, are added to the set of active permutations (i.e., new rows or columns in the game matrix) if they have better values than the previous equilibrium values (Lines 5 and 8).']"
"This is repeated until no game value improvement exists for either player (Line 9), at which point a Nash equilibrium for the full game has been obtained.
",3.2 Optimization by Constraint Generation,[0.9999999961549002],"['This is repeated until no game value improvement exists for either player (Line 9), at which point a Nash equilibrium for the full game has been obtained.']"
We solve the convex optimization of Lagrange parameters θ in Eq.,3.2 Optimization by Constraint Generation,[0],[0]
(3) using the results of Algorithm 1.,3.2 Optimization by Constraint Generation,[0],[0]
"We employ AdaGrad (Duchi et al., 2011) with the gradient calculated as the difference between expected features under the adversary’s distribution and the empirical training data: Ex∼P̃ ;π̌|x∼P̌ [ ∑n i=1 φi(x, π̌i)]− Ex,π∼P̃ [ ∑n i=1",3.2 Optimization by Constraint Generation,[0],[0]
"φi(x, πi)].
",3.2 Optimization by Constraint Generation,[0],[0]
"In contrast with SSVM, which compute the hinge loss for each training instance using only a single run of the Hungarian algorithm, our double oracle method must solve this problem repeatedly to find the equilibrium.",3.2 Optimization by Constraint Generation,[0],[0]
Though in practice the total number of active permutations is much smaller than the n!,3.2 Optimization by Constraint Generation,[0],[0]
"possibilities, no formal polynomial bound is known—and, consequentially, the run time of the approach as a whole cannot be characterized as polynomial.",3.2 Optimization by Constraint Generation,[0],[0]
"Our second approach, which significantly improves the efficiency of solving the adversarial bipartite matching game,
leverages the key insight that all quantities of interest for evaluating the loss and satisfying the constraints depend only on marginal probabilities of the permutation’s value assignments.",3.3 Marginal Distribution Formulation,[0],[0]
"Based on this, we employ a marginal distribution decomposition of the game.
",3.3 Marginal Distribution Formulation,[0],[0]
"We begin this reformulation by first defining a matrix representation of permutation π as Y(π) ∈ Rn×n (or simply Y) where the value of its cell Yi,j is 1 when πi = j and 0 otherwise.",3.3 Marginal Distribution Formulation,[0],[0]
"To be a valid complete bipartite matching or permutation, each column and row of Y can only have one entry of 1.",3.3 Marginal Distribution Formulation,[0],[0]
"For each feature function φ(k)i (x, πi), we also denote its matrix representation as Xk whose (i, j)-th cell represents the k-th entry of φi(x, j).",3.3 Marginal Distribution Formulation,[0],[0]
"For a given distribution of permutations, P (π), we denote the marginal probabilities of matching i with j as pi,j , P (πi = j).",3.3 Marginal Distribution Formulation,[1.0],"['For a given distribution of permutations, P (π), we denote the marginal probabilities of matching i with j as pi,j , P (πi = j).']"
"We let P = ∑ π P (π)Y(π) be the predictor’s marginal probability matrix where its (i, j) cell represents P̂ (π̂i = j), and similarly let Q be the adversary’s marginal probability matrix (based on P̌ ), as shown in Table 2.
",3.3 Marginal Distribution Formulation,[1.0000000078381472],"['We let P = ∑ π P (π)Y(π) be the predictor’s marginal probability matrix where its (i, j) cell represents P̂ (π̂i = j), and similarly let Q be the adversary’s marginal probability matrix (based on P̌ ), as shown in Table 2.']"
"The Birkhoff–von Neumann theorem (Birkhoff, 1946; Von Neumann, 1953) states that the convex hull of the set of n× n permutation matrices forms a convex polytope in Rn2 (known as the Birkhoff polytope Bn) in which points are doubly stochastic matrices, i.e., the n×nmatrices with non-negative elements where each row and column must sum to one.",3.3 Marginal Distribution Formulation,[1.0],"['The Birkhoff–von Neumann theorem (Birkhoff, 1946; Von Neumann, 1953) states that the convex hull of the set of n× n permutation matrices forms a convex polytope in Rn2 (known as the Birkhoff polytope Bn) in which points are doubly stochastic matrices, i.e., the n×nmatrices with non-negative elements where each row and column must sum to one.']"
This implies that both marginal probability matrices P and Q are doubly stochastic matrices.,3.3 Marginal Distribution Formulation,[0],[0]
"In contrast to the space of distributions over permutation of n objects, which grows factorially (O(n!) with n!",3.3 Marginal Distribution Formulation,[0],[0]
"− 1 free parameters), the size of this marginal matrices grows only quadratically (O(n2) with n2 − 2n free parameters).",3.3 Marginal Distribution Formulation,[0],[0]
"This provides a significant benefit in terms of the optimization.
",3.3 Marginal Distribution Formulation,[0],[0]
"Starting with the minimax over P̂ (π̂) and P̌ (π̌) in the permutation mixture formulation, and using the matrix notation above, we rewrite Eq.",3.3 Marginal Distribution Formulation,[0],[0]
"(3) as a minimax over marginal probability matrices P and Q with additional constraints that both P and Q are doubly-stochastic matrices, i.e., P ≥ 0 (elementwise), Q ≥ 0, P1 = P>1 = Q1 = Q>1 = 1 where 1 = (1, . .",3.3 Marginal Distribution Formulation,[0],[0]
.,3.3 Marginal Distribution Formulation,[0],[0]
", 1)>).",3.3 Marginal Distribution Formulation,[0],[0]
"That is:
min θ EX,Y∼P̃ minP≥0 maxQ≥0",3.3 Marginal Distribution Formulation,[0],[0]
"[n−〈P,Q〉+〈Q−Y, ∑ k θkXk〉]
s.t. :",3.3 Marginal Distribution Formulation,[0],[0]
"P1 = P>1 = Q1 = Q>1 = 1, (4)
where 〈·, ·〉 denotes the Frobenius inner product between two matrices, i.e., 〈A,B〉 = ∑ i,j Ai,jBi,j .",3.3 Marginal Distribution Formulation,[0],[0]
We reduce the computational costs of the optimization in Eq.,3.3.1 OPTIMIZATION,[0],[0]
"(4) by focusing on optimizing the adversary’s marginal probability Q. By strong duality, we then push the maximization over Q in the formulation above to the outermost level of Eq.",3.3.1 OPTIMIZATION,[0],[0]
(4).,3.3.1 OPTIMIZATION,[0],[0]
"Note that the objective above is a non-smooth function (i.e., piece-wise linear).",3.3.1 OPTIMIZATION,[0],[0]
"For the purpose of smoothing the objective, we add a small amount of strongly convex prox-functions to both P and Q. We also add a regularization penalty to the parameter θ to improve the generalizability of our model.",3.3.1 OPTIMIZATION,[0],[0]
We unfold Eq.,3.3.1 OPTIMIZATION,[0],[0]
"(4) by replacing the empirical expectation with an average over all training examples, resulting in the following optimization:
max Q≥0 min θ
1
m m∑ i=1",3.3.1 OPTIMIZATION,[0],[0]
"min Pi≥0 [ 〈Qi −Yi, ∑ k θkXi,k〉 − 〈Pi,Qi〉
+ µ2 ‖Pi‖ 2 F − µ 2 ‖Qi‖ 2 F ] +",3.3.1 OPTIMIZATION,[0],[0]
"λ2 ‖θ‖ 2 2
s.t. : Pi1 = P>i 1 = Qi1 =",3.3.1 OPTIMIZATION,[0],[0]
Q >,3.3.1 OPTIMIZATION,[0],[0]
"i 1 = 1, ∀i, (5)
where m is the number of bipartite matching problems in the training set, λ is the regularization penalty parameter, µ is the smoothing penalty parameter, and ‖A‖F denotes the Frobenius norm of matrixA. The subscript i in Pi,Qi,Xi, and Yi refers to the i-th example in the training set.
",3.3.1 OPTIMIZATION,[0],[0]
"In the formulation above, given a fixed Q, the inner minimization over θ and P can then be solved separately.",3.3.1 OPTIMIZATION,[0],[0]
"The optimal θ in the inner minimization admits a closed-form solution, in which the k-th element of θ∗ is:
θ∗k =",3.3.1 OPTIMIZATION,[0],[0]
"− 1
λm m∑ i=1",3.3.1 OPTIMIZATION,[0],[0]
"〈Qi −Yi,Xi,k〉 .",3.3.1 OPTIMIZATION,[0],[0]
"(6)
The inner minimization over P can be solved independently for each training example.",3.3.1 OPTIMIZATION,[0],[0]
"Given the adversary’s marginal probability matrix Qi for the i-th example, the optimal Pi can be formulated as:
P∗i = argmin {Pi≥0|Pi1=P>i 1=1} µ 2 ‖Pi‖ 2 F − 〈Pi,Qi〉 (7)
= argmin {Pi≥0|Pi1=P>i 1=1}
‖Pi − 1µQi‖ 2 F .",3.3.1 OPTIMIZATION,[0],[0]
"(8)
We can interpret this minimization as projecting the matrix 1 µQi to the set of doubly-stochastic matrices.",3.3.1 OPTIMIZATION,[0],[0]
"We will discuss our projection technique in the upcoming subsection.
",3.3.1 OPTIMIZATION,[0],[0]
"For solving the outer optimization over Q with the doublystochastic constraints, we employ a projected QuasiNewton algorithm (Schmidt et al., 2009).",3.3.1 OPTIMIZATION,[0],[0]
Each iteration of the algorithm optimizes the quadratic approximation of the objective function (using limited-memory Quasi-Newton) over the the convex set.,3.3.1 OPTIMIZATION,[0],[0]
"In each update step, a projection to the set of doubly-stochastic matrices is needed, akin to the inner minimization of P in Eq.",3.3.1 OPTIMIZATION,[0],[0]
"(8).
",3.3.1 OPTIMIZATION,[0],[0]
"The optimization above provides the adversary’s optimal marginal probability Q∗. To achieve our learning goal, we recover θ∗ using Eq.",3.3.1 OPTIMIZATION,[0],[0]
(6) computed over the optimal Q∗. We use the θ∗ that our model learns from this optimization to construct a weighted bipartite graph for making predictions for test examples.,3.3.1 OPTIMIZATION,[0],[0]
"The projection from an arbitrary matrix R to the set of doubly-stochastic matrices can be formulated as:
min P≥0 ‖P−R‖2F , s.t. : P1 = P>1 = 1.",3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION,[0],[0]
"(9)
We employ the alternating direction method of multipliers (ADMM) technique (Douglas & Rachford, 1956; Glowinski & Marroco, 1975; Boyd et al., 2011) to solve the optimization problem above.",3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION,[0],[0]
"We divide the doubly-stochastic matrix constraint into two sets of constraints C1 : P1 = 1 and P ≥ 0, andC2 :",3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION,[0],[0]
P>1 = 1 and P ≥ 0.,3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION,[0],[0]
"Using this construction, we convert the optimization above into ADMM form as follows:
min P,S
1 2‖P−R‖ 2 F + 1 2‖S−R‖ 2 F + IC1(P) + IC2(S)
s.t. :",3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION,[0],[0]
P− S = 0.,3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION,[0],[0]
"(10)
The augmented Lagrangian for this optimization is:
Lρ(P,S,W) = 12‖P−R‖ 2 F + 1 2‖S−R‖ 2 F + IC1(P)
+ IC2(S)",3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION,[0],[0]
"+ ρ 2‖P− S + W‖ 2 F , (11)
where ρ is the ADMM penalty parameter and W is the scaled dual variable.",3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION,[0],[0]
"From the augmented Lagrangian, we compute the update for P as:
Pt+1 = argmin P Lρ(P,St,Wt) (12)
= argmin {P≥0|P1=1}
1 2‖P−R‖ 2 F + ρ 2‖P− S t + Wt‖2F
= argmin {P≥0|P1=1}
‖P− 11+ρ ( R + ρ",3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION,[0],[0]
( St −Wt )),3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION,[0],[0]
"‖2F .
",3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION,[0],[0]
The minimization above can be interpreted as a projection to the set {P ≥ 0|P1 = 1} which can be realized by projecting to the probability simplex independently for each row of the matrix,3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION,[0],[0]
11+ρ (R + ρ,3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION,[0],[0]
"(S
t −Wt)).",3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION,[0],[0]
"Similarly, the ADMM update for S can also be formulated as a columnwise probability simplex projection.",3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION,[0],[0]
"The technique for projecting a point to the probability simplex has been studied previously, e.g., by Duchi et al. (2008).",3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION,[0],[0]
"Therefore, our ADMM algorithm consists of the following updates:
Pt+1 = ProjC1 ( 1 1+ρ ( R + ρ",3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION,[0],[0]
( St −Wt ))),3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION,[0],[0]
"(13)
St+1 =",3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION,[0],[0]
ProjC2 ( 1 1+ρ ( R + ρ,3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION,[0],[0]
( Pt+1 + Wt ))),3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION,[0],[0]
"(14)
Wt+1 = Wt + Pt+1",3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION,[0],[0]
− St+1.,3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION,[0],[0]
"(15)
We run this series of updates until the stopping conditions are met.",3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION,[0],[0]
Our stopping conditions are based on the primal and dual residual optimality as described in Boyd et al. (2011).,3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION,[0],[0]
"In our overall algorithm, this ADMM projection algorithm is used both in the projected Quasi-Newton algorithm for optimizing Q (Eq. (5)) and in the inner optimization for minimizing Pi (Eq. (8)).",3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION,[0],[0]
"The convergence rate of ADMM is O(log 1 ) thanks to the strong convexity of the objective (Deng & Yin, 2016).",3.3.3 CONVERGENCE PROPERTY,[0],[0]
"Each step inside ADMM is simply a projection to a simplex, hence costing Õ(n) computations (Duchi et al., 2008).
",3.3.3 CONVERGENCE PROPERTY,[0],[0]
"In terms of optimization on Q, since no explicit rates of convergence are available for the projected Quasi-Newton algorithm (Schmidt et al., 2009) that finely characterize the dependency on the condition numbers, we simply illustrate the √ L/µ log 1 rate using Nesterov’s accelerated gradient algorithm (Nesterov, 2003), where L is the Lipschitz continuous constant of the gradient.",3.3.3 CONVERGENCE PROPERTY,[0],[0]
"In our case, L = 1m2λ ∑ k ∑m i=1",3.3.3 CONVERGENCE PROPERTY,[0],[0]
"‖Xi,k‖ 2 F + 1/µ.
",3.3.3 CONVERGENCE PROPERTY,[0],[0]
Comparison with Structured SVM (SSVM),3.3.3 CONVERGENCE PROPERTY,[0],[0]
"Conventional SSVMs for learning bipartite matchings have only O(1/ ) rates due to the lack of smoothness (Joachims et al., 2009; Teo et al., 2010).",3.3.3 CONVERGENCE PROPERTY,[0],[0]
"If smoothing is added, then similar linear convergence rates can be achieved with similar condition numbers.",3.3.3 CONVERGENCE PROPERTY,[0],[0]
"However, it is noteworthy that at each iteration we need to apply ADMM to solve a projection problem to the doubly stochastic matrix set (Eq. (9)), while SSVMs (without smoothing) solves a matching problem with the Hungarian algorithm, incurring O(n3) time.",3.3.3 CONVERGENCE PROPERTY,[0],[0]
"Despite its apparent differences from standard empirical risk minimization (ERM), adversarial loss minimization (Eq. (3)) can be equivalently recast as an ERM:
min θ E x∼P π|x∼P̃
[ ALpermfθ (x, π) ] where ALpermfθ (x, π) ,
min P̂ (π̂|x) max P̌ (π̌|x) Eπ̂|x∼P̂ π̌|x∼P̌
[ loss(π̂, π̌) + fθ(x, π̌)− fθ(x, π) ]
and fθ(x, π) = θ · ∑n i=1",3.4 Consistency Analysis,[0],[0]
"φ(x, πi) is the Lagrangian potential function.",3.4 Consistency Analysis,[0],[0]
"Here we consider fθ as the linear discriminant function for a proposed permutation π, using parameter value θ.",3.4 Consistency Analysis,[0],[0]
"ALpermfθ (x, π) is then the surrogate loss for input x and permutation π.
",3.4 Consistency Analysis,[0],[0]
"As described in Section 2.2, Fisher consistency is an important property for a surrogate loss L. It requires that under the true distribution P (x, π), the hypothesis that minimizes L is Bayes optimal (Tewari & Bartlett, 2007; Liu,
2007).",3.4 Consistency Analysis,[0],[0]
"For the cases of multiclass classification and ordinal regression, Fisher consistency for adversarial surrogate loss has been established by Fathony et al. (2016; 2017).",3.4 Consistency Analysis,[1.0],"['For the cases of multiclass classification and ordinal regression, Fisher consistency for adversarial surrogate loss has been established by Fathony et al. (2016; 2017).']"
"In our setting, the Fisher consistency ofALpermf can be written as:
f∗ ∈ F∗ , argmin f
Eπ|x∼P [ ALpermf (x, π) ] (16)
⇒ argmax π f∗(x, π) ⊆ Π , argmin π Eπ̄|x∼P",3.4 Consistency Analysis,[0],[0]
"[loss(π, π̄)].
",3.4 Consistency Analysis,[0],[0]
Note that in Eq.,3.4 Consistency Analysis,[0],[0]
"(16) we allow f to be optimized over the set of all measurable functions on the input space (x, π).",3.4 Consistency Analysis,[0],[0]
"In our formulation, we have restricted f to be additively decomposable over individual elements of permutation, f(x, π) = ∑ i gi(x, πi).",3.4 Consistency Analysis,[0],[0]
"In the sequel, we will show that the condition in Eq.",3.4 Consistency Analysis,[0],[0]
"(16) also holds for this restricted set provided that g is allowed to be optimized over the set of all measurable functions on the space of individual input (x, πi).",3.4 Consistency Analysis,[0],[0]
"We start by establishing Fisher consistency for the case of singleton loss minimizing sets Π in Theorem 1 and then for more general cases in Theorem 2.
Theorem 1.",3.4 Consistency Analysis,[0],[0]
"Suppose loss(π, π̄) =",3.4 Consistency Analysis,[0],[0]
"loss(π̄, π) (symmetry) and loss(π, π) <",3.4 Consistency Analysis,[0],[0]
"loss(π̄, π) for all π̄ 6= π.",3.4 Consistency Analysis,[0],[0]
"Then the adversarial permutation loss ALpermf is Fisher consistent if f is over all measurable functions and Π is a singleton.
",3.4 Consistency Analysis,[0],[0]
Theorem 2.,3.4 Consistency Analysis,[0],[0]
"Suppose loss(π, π̄) = loss(π̄, π) (symmetry) and loss(π, π) <",3.4 Consistency Analysis,[0],[0]
"loss(π̄, π) for all π̄ 6= π.",3.4 Consistency Analysis,[0],[0]
"Furthermore if f is over all measurable functions, then:
(a) there exists f∗ ∈ F∗ such that argmaxπ f∗(x, π) ⊆ Π (i.e., satisfies the Fisher consistency requirement).",3.4 Consistency Analysis,[0],[0]
"In fact, all elements in Π can be recovered by some f∗ ∈ F∗ .
",3.4 Consistency Analysis,[0],[0]
"(b) if argminπ ∑ π′∈Π απ′ loss(π
′, π) ⊆ Π for all α(·) ≥ 0; ∑ π′∈Π απ′ = 1, then argmaxπ f ∗(x, π) ⊆ Π for all f∗ ∈",3.4 Consistency Analysis,[0],[0]
F∗.,3.4 Consistency Analysis,[0],[0]
"In this case, all f∗ ∈ F∗ satisfy the Fisher consistency requirement.
",3.4 Consistency Analysis,[0],[0]
"These assumptions of loss functions in the theorems above are quite mild, requiring only that wrong predictions suffer higher loss than correct ones.",3.4 Consistency Analysis,[0],[0]
We refer the reader to Appendix B for the detailed proofs of theorems.,3.4 Consistency Analysis,[0],[0]
"The key to the proofs is the observation that for the optimal potential function f∗, f∗(x, π) + loss(π, π ) is invariant to π when Π = {π }.",3.4 Consistency Analysis,[0],[0]
We refer to this as the loss reflective property.,3.4 Consistency Analysis,[0],[0]
"Note that this generalizes the observation for the case of ordinal regression loss (Fathony et al., 2017) into matching loss functions, subject to the mild pre-conditions assumed by the theorem.
",3.4 Consistency Analysis,[0],[0]
Theorem 3.,3.4 Consistency Analysis,[0],[0]
"Suppose the loss is Hamming loss, and the potential function f(x, π) decomposes additively by∑ i gi(x, πi).",3.4 Consistency Analysis,[0],[0]
"Then, the adversarial permutation loss ALpermf is Fisher consistent provided that gi is allowed to
be optimized over the set of all measurable functions on the space of individual inputs (x, πi).
",3.4 Consistency Analysis,[0],[0]
Proof.,3.4 Consistency Analysis,[0],[0]
"Simply choose gi such that for each sample x in the population, gi(x, πi) = −(πi 6= π i ).",3.4 Consistency Analysis,[0],[0]
This renders the loss reflective property under the Hamming loss.,3.4 Consistency Analysis,[0],[0]
"To evaluate our approach, we apply our adversarial bipartite matching model to video tracking tasks using public benchmark datasets (Leal-Taixé et al., 2015).",4 Experimental Evaluation,[0],[0]
"In this problem, we are given a set of images (video frames) and a list of objects in each image.",4 Experimental Evaluation,[0],[0]
We are also given the correspondence matching between objects in frame t and objects in frame t + 1.,4 Experimental Evaluation,[0],[0]
Figure 2 shows an example of the problem setup.,4 Experimental Evaluation,[0],[0]
It is important to note that the number of objects are not the same in every frames.,4 Experimental Evaluation,[0],[0]
"Some of the objects may enter, leave, or remain in the consecutive frames.",4 Experimental Evaluation,[0],[0]
"To handle the this issue, we setup our experiment as follows.",4 Experimental Evaluation,[0],[0]
"Let kt be the number of objects in frame t and k∗ be the maximum number of objects a frame can have, i.e., k∗ = maxt∈T kt.",4 Experimental Evaluation,[0],[0]
"Starting from k∗ nodes to represent the objects, we add k∗ more nodes as “invisible” nodes to allow new objects to enter and existing objects to leave.",4 Experimental Evaluation,[0],[0]
"As a result, the total number of nodes in each frame doubles to n = 2k∗.",4 Experimental Evaluation,[0],[0]
"We define the features for pairs of bounding boxes (i.e., φi(x, j) for pairing bounding box i with bounding box j) in two consecutive video frames so that we can compute the associative feature vectors, φ(x, π) = ∑n i=1",4.1 Feature Representation,[0],[0]
"φi(x, πi),",4.1 Feature Representation,[0],[0]
for each possible matching π.,4.1 Feature Representation,[0],[0]
"To define the feature vector φi(·, ·), we follow the feature representation reported by Kim et al. (2012) using six different types of features:
• Intersection over union (IoU) overlap ratio between bounding boxes, area(BBti ∩ BBt+1j )/",4.1 Feature Representation,[0],[0]
"area(BBti ∪ BBt+1j ), where BB t i denotes the bounding
box of object i at time frame t; • Euclidean distance between object centers; • 21 color histogram distance features (RGB) from
the Bhattacharyaa distance, 14 ln ( 1 4 (σ2p σ2q + σ2q σ2p + 2 ))",4.1 Feature Representation,[0],[0]
"+
We explain this feature representation in more detail in Appendix C.",4.1 Feature Representation,[0],[0]
"We compare our approach with the Structured SVM (SSVM) model (Taskar et al., 2005a; Tsochantaridis et al., 2005) implemented based on Kim et al. (2012) using SVM-Struct (Joachims, 2008; Vedaldi, 2011).",4.2 Experimental Setup,[0],[0]
"We implement our marginal version of adversarial bipartite matching using minConf (Schmidt, 2008) for performing projected Quasi-Newton optimization.
",4.2 Experimental Setup,[0],[0]
We consider two different groups of datasets in our experiment: TUD datasets and ETH datasets.,4.2 Experimental Setup,[0],[0]
"Each dataset contains different numbers of elements (i.e., the number of pedestrian bounding box in the frame plus the number of extra nodes to indicate entering or leaving) and different numbers of examples (i.e., pairs of two consecutive frames that we want to match).",4.2 Experimental Setup,[0],[0]
"Table 3 contains the detailed information about the datasets.
",4.2 Experimental Setup,[0],[0]
"To avoid having test examples that are too similar with the training set, we train the models on one dataset and test the model on another dataset that has similar characteristics.",4.2 Experimental Setup,[0],[0]
"In particular, we perform evaluations for every pair of datasets in TUD and ETH collections.",4.2 Experimental Setup,[0],[0]
"This results in eight pairs of training/test datasets, as shown in Table 4.
",4.2 Experimental Setup,[0],[0]
"To tune the regularization parameter (λ in adversarial matching, and C in SSVM), we perform 5-fold cross validation based on the training dataset only.",4.2 Experimental Setup,[0],[0]
"The resulting best regularization parameter is used to train the model over all training examples to obtain parameters θ, which we then use to predict the matching for the testing data.",4.2 Experimental Setup,[0],[0]
"For SSVM and the marginal version of adversarial matching, the pre-
diction is done by finding the bipartite matching that maximizes the potential value, i.e., argmaxY 〈Y, ∑ k θkXk〉 which can be solved using the Hungarian algorithm.",4.2 Experimental Setup,[0],[0]
The double oracle version of adversarial matching makes predictions by finding the most likely permutation from the predictor’s strategy in the equilibrium.,4.2 Experimental Setup,[0],[0]
"We report the average accuracy, which in this case is defined as (1 − the average Hamming loss) over all examples in the testing dataset.",4.3 Results,[0],[0]
Table 4 shows the mean and the standard deviation of our metric across different dataset pairs.,4.3 Results,[0],[0]
We report the results for both the double-oracle (DO) and marginal (MARG) versions of the adversarial model.,4.3 Results,[0],[0]
Our experiment indicates that both methods result in very similar values of θ.,4.3 Results,[0],[0]
The slight advantage of the double-oracle version is caused by the difference in prediction techniques between the double-oracle (argmax over predictor’s equilibrium strategy) and marginal version (argmax over potentials).,4.3 Results,[0],[0]
We also observe that the double-oracle approach requires only a small number of augmenting permutations to converge as shown in the last column (the average number of permutations) of Table 4.,4.3 Results,[0],[0]
"This indicates the sparseness of the set of permutations that support the equilibrium.
",4.3 Results,[0],[0]
"To compare with SSVM, we highlight (using bold font) the cases in which our result is better with statistical significance (under paired t-test with α < 0.05) in Table 4.",4.3 Results,[0],[0]
"Compared with SSVM, our proposed adversarial matching outperforms SSVM in all pairs of datasets—with statistical
significance on all six pairs of the ETH datasets and slightly better than SSVM on the TUD datasets.",4.3 Results,[0],[0]
"This suggests that our adversarial bipartite matching model benefits from its Fisher consistency property.
",4.3 Results,[0],[0]
"In terms of the running time, Table 5 shows that the marginal version of adversarial method is relatively fast.",4.3 Results,[1.0],"['In terms of the running time, Table 5 shows that the marginal version of adversarial method is relatively fast.']"
"It only takes a few seconds to train until convergence in the case of 50 examples, with the number of elements varied up to 34.",4.3 Results,[0],[0]
"The running time grows roughly quadratically in the number of elements, which is natural since the size of the marginal probability matrices P and Q also grow quadratically in the number of elements.",4.3 Results,[1.0],"['The running time grows roughly quadratically in the number of elements, which is natural since the size of the marginal probability matrices P and Q also grow quadratically in the number of elements.']"
"This shows that our approach is much more efficient than the CRF approach, which has a running time that is impractical even for small problems with 20 elements.",4.3 Results,[1.0],"['This shows that our approach is much more efficient than the CRF approach, which has a running time that is impractical even for small problems with 20 elements.']"
"The training time of SSVM is faster than the adversarial methods due to two different factors: (1) the inner optimization of SSVM can be solved using a single execution of the Hungarian algorithm compared with the inner optimization of adversarial method which requires ADMM optimization for projection to doubly stochastic matrix set; (2) different tools for implementation, i.e., C++ for SSVM and MATLAB for our method, which benefits the running time of SSVM.",4.3 Results,[1.0],"['The training time of SSVM is faster than the adversarial methods due to two different factors: (1) the inner optimization of SSVM can be solved using a single execution of the Hungarian algorithm compared with the inner optimization of adversarial method which requires ADMM optimization for projection to doubly stochastic matrix set; (2) different tools for implementation, i.e., C++ for SSVM and MATLAB for our method, which benefits the running time of SSVM.']"
"In addition, though the game size is relatively small, as indicated by the final column in Table 4, the double oracle version of adversarial method takes much longer to train compared to the marginal version.",4.3 Results,[1.0],"['In addition, though the game size is relatively small, as indicated by the final column in Table 4, the double oracle version of adversarial method takes much longer to train compared to the marginal version.']"
"In this paper, we have presented an adversarial approach for learning bipartite matchings that is not only computationally efficient to employ but also provides Fisher consistency guarantees.",5 Conclusions and Future Work,[1.0],"['In this paper, we have presented an adversarial approach for learning bipartite matchings that is not only computationally efficient to employ but also provides Fisher consistency guarantees.']"
We showed that these theoretical advantages translate into better empirical performance for our model compared with previous approaches.,5 Conclusions and Future Work,[1.0],['We showed that these theoretical advantages translate into better empirical performance for our model compared with previous approaches.']
Our future work will explore matching problems with different loss functions and other graphical structures.,5 Conclusions and Future Work,[1.0],['Our future work will explore matching problems with different loss functions and other graphical structures.']
We thank our anonymous reviewers for their useful feedback and suggestions.,Acknowledgements,[0],[0]
This research was supported in part by NSF Grants RI-#1526379 and CAREER-#1652530.,Acknowledgements,[0],[0]
"Many important structured prediction problems, including learning to rank items, correspondence-based natural language processing, and multi-object tracking, can be formulated as weighted bipartite matching optimizations.",abstractText,[0],[0]
Existing structured prediction approaches have significant drawbacks when applied under the constraints of perfect bipartite matchings.,abstractText,[0],[0]
"Exponential family probabilistic models, such as the conditional random field (CRF), provide statistical consistency guarantees, but suffer computationally from the need to compute the normalization term of its distribution over matchings, which is a #P-hard matrix permanent computation.",abstractText,[0],[0]
"In contrast, the structured support vector machine (SSVM) provides computational efficiency, but lacks Fisher consistency, meaning that there are distributions of data for which it cannot learn the optimal matching even under ideal learning conditions (i.e., given the true distribution and selecting from all measurable potential functions).",abstractText,[0],[0]
We propose adversarial bipartite matching to avoid both of these limitations.,abstractText,[0],[0]
"We develop this approach algorithmically, establish its computational efficiency and Fisher consistency properties, and apply it to matching problems that demonstrate its empirical benefits.",abstractText,[0],[0]
Efficient and Consistent Adversarial Bipartite Matching,title,[0],[0]
