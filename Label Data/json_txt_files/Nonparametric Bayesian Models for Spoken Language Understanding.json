{"sections": [{"text": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2144\u20132152, Austin, Texas, November 1-5, 2016. c\u00a92016 Association for Computational Linguistics"}, {"heading": "1 Introduction", "text": "Spoken language understanding (SLU) refers to the challenge of recognizing a speaker\u2019s intent from a natural language utterance, which is typically defined as a slot filling task. For example, in the utterance \u201cRemind me to call John at 9am tomorrow\u201d, the specified information {\u201ctime\u201d: \u201c9am tomorrow\u201d} and {\u201csubject\u201d: \u201cto call John\u201d} should be extracted. The term slot refers to a variable such as the time or subject that is expected to be filled with a value provided through the user\u2019s utterance.\nThe slot filling task is typically formulated as a sequential labeling problem as shown in Figure 1. This labeling scheme naturally represents the recognition of arbitrary phrases that appear in the transcription of an utterance. Formally speaking, when we assume a given set of slots {s1, ..., sM} and denote the corresponding slot values by {vs1 , ..., vsM }\nwhere vsi \u2208 Vsi , the domain of each slot value Vsi is an infinite set of word sequences. In this paper, we use the term arbitrary slot filling task to refer to this implicit problem statement, which inherently underlies the sequential labeling formulation.\nIn contrast, a different line of work has explored the case where Vsi is provided as a finite set of possible values that can be handled by a backend system (Henderson, 2015). We refer to this type of task as a categorical slot filling task. In this case, the slot filling task is regarded as a classification problem that explicitly considers a value-based prediction, as shown in Figure 2. From this point of view, we can say that a distribution of slot values is actually concentrated in a small set of typical phrases, even in the arbitrary slot filling task, because users basically know what kind of function is offered by the system.\nTo reflect this observation, in this paper we explore the value-based formulation approach for arbitrary slot filling tasks. Unlike the sequential labeling formulation, which is basically position-based label prediction, our method directly estimates the posterior distribution over an infinite set of possible values for each slot Vsi . The distribution is represented by using a Dirichlet process (Gershman and Blei, 2012), which is a nonparametric Bayesian formalism that generates a categorical distribution for any space. We demonstrate that this approach improves estimation accuracy in the arbitrary slot filling task compared with conventional sequential labeling approach.\nThe rest of this paper is organized as follows. In Section 2, we review the existing approaches for categorical and arbitrary slot filling tasks and intro-\n2144\nduce related work. In Section 3, we present our nonparametric Bayesian formulation, the hierarchical Dirichlet process slot model (HDPSM), which directly models an infinite set of slot values. On the basis of the HDPSM, we develop a generative utterance model that allows us to compute the posterior probability of slot values in Section 4. In Section 5, we introduce a two-stage slot filling algorithm that consists of a candidate generation step and a candidate ranking step using the proposed model. In Section 6, we show the experimental results for multiple datasets in different domains to demonstrate that the proposed algorithm performs better than the baseline sequential labeling method. We conclude in Section 7 with a brief summary."}, {"heading": "2 Related Work", "text": "The difference between the categorical and arbitrary slot filling approaches has not been explicitly discussed in a comparative manner to date. In this section, we review existing work for both approaches.\nFor the categorical slot filling approach, various algorithms that directly model the distribution of slot values have been proposed, including generative models (Williams, 2010), maximum entropy linear classifiers (Metallinou et al., 2013), and neural networks (Ren et al., 2014). However, none of these models are applicable for predicting a variable that ranges over an infinite set, and it is not straightforward to extend them suitably. In particular, a discriminative approach is not applicable for arbitrary slot filling tasks because it requires a fixed finite set of slot values to take statistics.\nThe arbitrary slot filling approach is a natural application of shallow semantic parsing (Gildea, 2002), which is naturally formulated as a sequential labeling problem. Various sequential labeling algorithms have been applied to this task, including support vector machines, conditional random fields (CRF) (Lafferty et al., 2001; Hahn et al., 2011), and deep neural networks (Mesnil et al., 2015; Xu and Sarikaya, 2013). Vukotic et al. (2015) reported that the CRF is still the most accurate, rapid, and stable method among them. Because the focus of this paper is arbitrary slot filling tasks, we use CRFs as our baseline method.\nIn this paper, we apply nonparametric Bayesian models (Gershman and Blei, 2012) to represent the distribution over arbitrary phrases for each slot. The effectiveness of this phrase modeling approach has been examined in various applications including morphological analysis (Goldwater et al., 2011) and infinite vocabulary topic models (Zhai and Boydgraber, 2013). Our method can be regarded as an application of this idea, although it is not straightforward to integrate it with the utterance generation process, as we explain later.\nConsequently, our proposed method is categorized as a generative approach. There are many advantages inherent in generative approaches that have been examined, including unsupervised SLU (Chen et al., 2015), automatic feature extraction (Tur et al., 2013), and integration with syntactic modeling (Lorenzo et al., 2013). Another convenient property of generative models is that prior knowledge can be integrated in an intuitive way (Raymond et al., 2006). This often leads to better performance with less training data compared with discriminative models trained completely from scratch (Komatani et al., 2010)."}, {"heading": "3 Hierarchical Dirichlet Process Slot Model", "text": "In this section, we present a nonparametric Bayesian formulation that directly models the distribution over an infinite set of possible values for each slot. Let S = {s1, ..., sMS} be a given set of slots and MS be the number of slots. We define each slot si as a random variable ranging over an infinite set of\nletter sequences V , which is represented as follows:\nV = {b1, ..., bL|b\u03b9 \u2208 C,L \u2265 0}\nwhere C is a set of characters including the blank character and any other character that potentially appears in the transcription of an utterance. Consequently, we regard the set of slots S as also being a random variable that ranges over VMS . The objective of this section is to develop the formulation of the probabilistic distribution p(S)."}, {"heading": "3.1 Dirichlet Process", "text": "We apply the Dirichlet process (DP) to model both the distribution for an individual slot pi(si) and the joint distribution p(S). In this subsection, we review the definition and key properties of DP with general notation for the target distribution G over the domain X . In the DP for the prior of pi(si) that is described in Section 3.2, the domain X corresponds to a set of slot values V , e.g., \u201cfen ditton\u201d, \u201cnew chesterton\u201d, and None. In the DP for p(S) presented in Section 3.3, X indicates a set of tuples of slot values VMS , e.g., (\u201crestaurant\u201d, \u201cnew chesterton\u201d, \u201cfast food\u201d) and (\u201crestaurant\u201d, \u201cfen ditton\u201d, None).\nThe DP is a probabilistic distribution over the distribution G. DP is parameterized by \u03b10 and G0, where \u03b10 > 0 is a concentration parameter and G0 is a base distribution over X . If G is drawn from DP (\u03b10, G0) (i.e., G \u223c DP (\u03b10, G0)), then the following Dirichlet distributed property holds for any partition of X denoted by {A1, ..., AL}:\n(G(A1), ..., G(AL)) \u223c Dir(\u03b1(A1), ..., \u03b1(AL))\nwhere \u03b1(A) = \u03b10G0(A), which is known as the base measure of DP.\nFerguson (1973) proved an important property of a posterior distribution of repeated i.i.d. samples x1:N = {x1, ..., xN} drawn from G \u223c DP (\u03b10, G0). Consider a countably infinite set of atoms \u03c6 = {\u03c61, \u03c62, ...} that are independently drawn from G0. Let ci \u2208 N be the assignment of an atom for sample xi, which is generated by a sequential draw with the following conditional probability:\np(cN+1 = k|c1:N ) = { nk N+\u03b10 k \u2264 K \u03b10\nN+\u03b10 k = K + 1\nwhere nk is the number of times that the kth atom appears in c1:N and K is the number of different atoms in c1:N . Given the assignment c1:N , the predictive distribution of xN+1 \u2208 X is represented in the following form:\nP (xN+1 = \u03b8|c1:N ,\u03c61:K , \u03b10, G0)\n= K\u2211\nk=1\nnk N + \u03b10 \u03b4(\u03c6k, \u03b8) + \u03b10 N + \u03b10 G0(\u03b8)\nThe base distribution possibly generates an identical value for different atoms, such as (\u03c61 = \u201cfen ditton\u201d, \u03c62 = \u201cnew chesterton\u201d, \u03c63 = \u201cfen ditton\u201d). The assignment ci is an auxiliary variable to indicate which of these atoms is assigned to the ith data point xi; when xi = \u201cfen ditton\u201d, ci can be 1 or 3. The posterior distribution above depends on the frequency of atom nk, not on the frequency of \u03b8 itself. The atoms \u03c6 and the assignment c are latent variables that should be determined at runtime."}, {"heading": "3.2 Individual Slot Model", "text": "First we formulate the distribution for an individual slot as pi(si) \u223c DP (\u03b10i , G0i ) where G0i is a base distribution over the set of phrases V . 1 We define G0i as a generative model that consists of two-step generation: generation of the phrase length 0 \u2264 Li \u2264 Lmax using a categorical distribution and generation of a letter sequence s1:Li using an n-gram model, as follows:\nLi \u223c Categorical(\u03bbi) s\u03b9i \u223c p(s\u03b9i|s\u03b9\u2212n+1:\u03b9\u22121i ,\u03b7i)\nwhere \u03bbi and \u03b7i are parameters for the categorical distribution and the n-gram model for slot si, respectively. This explicit modeling of the length helps avoid the bias toward shorter phrases and leads to a better distribution, as reported by Zhai and Boydgraber (2013). We define G0i as a joint distribution of these models:\nG0i (s 1:Li i ) = p(Li|\u03bbi)\nLi\u220f\n\u03b9=1\np(s\u03b9i|s\u03b9\u2212n+1:\u03b9\u22121i ,\u03b7i) (1)\nG0i potentially generates an empty phrase of Li = 0 to express the case that the slot value vsi is not\n1Note that the subscript i for s, p, \u03b10 and G0 indicates the slot type such as \u201ctype\u201d, \u201carea\u201d and \u201cfood\u201d in Figure 2.\nprovided by an utterance. Therefore, the distribution pi(si) can naturally represent the probability of None , which is shown in Figure 2.\nWe consider prior distributions of the parameters \u03bbi and \u03b7i to treat the n-gram characteristics of each slot in a fully Bayesian manner. p(\u03bb) is given as a Lmax-dimensional symmetric Dirichlet distribution with parameter a. We also define the |C|dimensional symmetric Dirichlet distributions with parameter b for each n-gram context, since given the context p(s\u03b9i|s\u03b9\u2212n+1:\u03b9\u22121i ,\u03b7i) is just a categorical distribution that ranges over C. Consider we observe N phrases si for slot i. Let nLi\u03b9 be the number of phrases that have length \u03b9 and n\u03b3ih be the number of times that letter s\u03b9 = h appears after context s\u03b9\u2212n+1:\u03b9\u22121 = \u03b3. The predictive probability of a phrase is represented as follows:\nG0i (s 1:Li i |si) =\nnLi\u03b9 + b\nN + bC\nLi\u220f\n\u03b9=1\nn\u03b3is\u03b9i + a\n\u2211 c n \u03b3 ic + a \u2211Lmax l=1 n L il"}, {"heading": "3.3 Generative Model for a Set of Slot Values", "text": "A naive definition of the joint distribution p(S) is a product of all slot probabilities \u220fMS i=1 pi(si) for making an independence assumption. However, the slot values are generally correlated with each other (Chen et al., 2015). To obtain more accurate distribution, we formulate p(S) using another DP that recognizes a frequent combination of slot values, as p(S) \u223c DP (\u03b11, G2) where G2 is a base distribution over VMS . We apply the naive independence assumption to G2 as follows:\nG2(S) =\nMS\u220f\ni=1\npi(si)\nThe whole generation process of S involves twolayered DPs that share atoms among them. In this sense, this generative model is regarded as a hierarchical Dirichlet process (Teh et al., 2005).\nLet G1i (si) = pi(si) and G 3(S) = p(S) for consistent notations. In summary, we define the hierarchical Dirichlet process slot model (HDPSM) as a generative model that has the following generation process.\nG1i \u223c DP (\u03b10i , G0i ) G3 \u223c DP (\u03b11, G2) S \u223c G3"}, {"heading": "3.4 Inference of HDPSM", "text": "In a slot filling task, observations of S1:T = {S1, ..., ST } are available as training data. The inference of HDPSM refers to the estimation of \u03bb, \u03b7 and the atom assignments for each DP.\nWe formulate the HDPSM in a form of the Chinese restaurant franchise process, which is one of the explicit representations of hierarchical DPs obtained by marginalizing out the base distributions. Teh et al. (2005) presents a Gibbs sampler for this representation, which involves a repetitive resampling of atoms and assignment. In our method, we prefer to adopt a single pass inference, which samples the assignment for each observation only once. Our preliminary experiments showed that the quality of inference is not affected because S is observed unlike the settings in Teh et al. (2005).\nWe denote the atoms and the atom assignment in the first level DP DP (\u03b11, G2) by \u03c61 and c11:N , respectively. The posterior probability of atom assignment for a new observation SN+1 is represented as follows:\np(c1N+1 = k|c11:N ,\u03c61, SN+1)\n\u221d { n1k\u03b4(\u03c6 1 k, SN+1) k \u2264 K\n\u03b11G2(SN+1) k = K + 1\nwhere n1k is the number of times that the kth atom appears in c11:N and K is the number of different atoms in c11:N . \u03c60i and c 0 i1:K denote the atoms and the assignment in the second level DPs DP (\u03b10i , G 0 i ). The second level DPs assign atoms to each first level atom \u03c61k, i.e. the second level atom \u03c60it is generated only when a new atom is assigned for SN+1 at the first level. The posterior probability of atom assignment at the second level is:\np(c0iK+1 = t|c0i1:K ,\u03c60i , sN+1i)\n\u221d { n0it\u03b4(\u03c6 0 it, SN+1) t \u2264 Ti\n\u03b10iG 0(SN+1) t = Ti + 1\nwhere n0it is the number of times that the tth atom appears in c0i1:K and Ti is the number of different atoms in c0i1:K .\nThe single pass inference procedure is presented in Algorithm 1. Given the atoms \u03c6 and the assignments c, the predictive distribution of SN+1 =\nAlgorithm 1 Single pass inference of HDPSM Input: A set of observations S1:N\n1: Set empty list to c1 and c0i 2: for d = 1 to N do 3: k \u223c p(c1d = k|c11:d\u22121,\u03c61, Sd) 4: if k = K + 1 then 5: for i = 1 to MS do 6: ti \u223c p(c0iK+1 = ti|c0i1:K ,\u03c60i , sdi) 7: if ti = Ti + 1 then 8: Update nLi and n \u03b3 i with sdi\n9: end if 10: c0K+1 \u2190 ti and \u03c60iti \u2190 sdi 11: end for 12: end if 13: c1d \u2190 k and \u03c61k \u2190 S 14: end for\n{sN+11, ..., sN+1MS} is calculated as follows:\nP (SN+1|c,\u03c6) = K\u2211\nk=1\nn1k N + \u03b11 \u03b4(\u03c61k, SN+1) (2)\n+ \u03b11\nN + \u03b11\nMS\u220f\ni=1\nP (sN+1i|c0i ,\u03c60i )\nP (sN+1i|c0i ,\u03c60i ) = Ti\u2211\nt=1\nn0it K + \u03b10i \u03b4(\u03c60it, sN+1i)\n+ \u03b10i\nK + \u03b10i G0i (sN+1i|\u03c60i )"}, {"heading": "4 Generative Model for an Utterance", "text": "We present a generative utterance model to derive a slot estimation algorithm given utterance u. Figure 3 presents the basic concept of our generative model. In the proposed model, we formulate the distribution of slot values as well as the distribution of non-slot parts. In Figure 3, the phrases \u201chi we\u2019re in um\u201d and \u201cand we need a\u201d should be removed to identify the slot information. We call these non-slot phrases as functional fillers because they more or less have a function to convey information. Identifying the set of non-slot phrases is equivalent to identifying the set of slot phrases. Therefore, we define a generative model of functional fillers in the same way as the slot values."}, {"heading": "4.1 Functional Filler", "text": "We assume an utterance u is a concatenation of slot values S and functional fillers F . A functional filler is represented as a phrase that ranges over V . To derive the utterance model, we first formulate a generative model for functional fillers.\nIn our observation, the distribution of the functional filler depends on its position in an utterance. For example, utterances often begin with typical phrases such as \u201cHello I\u2019m looking for ...\u201d or \u201cHi please find ...\u201d, which can hardly ever appear at other positions. To reflect this observation, we introduce a filler slot to separately model the functional fillers based on a position feature. Specifically, we define three filler slots: beginning filler f1, which precedes any slot value, ending filler f3, which appears at the end of an utterance, and middle filler f2, which is inserted between slot values. We use the term content slot to refer to S when we intend to explicitly distinguish it from a filler slot.\nLet F = {f1, f2, f3} be a set of filler slots and MF = 3 be the number of filler slots. Each slot fi is a random variable ranging over V and F is a random variable over VMF . These notations for filler slots indicate compatibility to a content slot, which suggests that we can formulate F using HDPSMs, as follows:\nH1i \u223c DP (\u03b20i , H0i ) H3 \u223c DP (\u03b21, H2) F \u223c H3\nwhere H0i is an n-gram-based distribution over V that is defined in an identical way to (1) and H2(F ) = \u220fMF i=1 H 1 i (F )."}, {"heading": "4.2 Utterance Model", "text": "Figure 4 presents the graphical model of our utterance model. We assume that an utterance u is built with phrases provided by S and F . Therefore, the conditional distribution p(u|S, F ) basically involves a distribution over the permutation of these slot values with two constraints: f1 is placed first and f3 has to be placed last. In our formulation, we simply adopt a uniform distribution over all possible permutations.\nFor training the utterance model, we assume that a set of annotated utterances is available. Each training instance consists of utterance u and annotated slot values S. Given u and S, we assume that the functional fillers F can be uniquely identified. For the example in Figure 3, we can identify the subsequence in u that corresponds to each content slot value of \u201crestaurant\u201d and \u201cfen ditton\u201d. This matching result leads to the identification of filler slot values. Consequently, a triple (u, S, F ) is regarded as an observation. Because the HDPSMs of the content slot and of the filler slot are conditionally independent given S and F , we can separately apply Algorithm 1 to train each HDPSM.\nFor slot filling, we examine the posterior probability of content slot values S given u, which can be reformed as follows:\nP (S|u) \u221d \u2211\nF\nP (u|S, F )P (S)P (F )\nIn this equation, we can remove the summation of F because filler slot values F are uniquely identified regarding u and S in our assumption. Additionally, we approximately regard P (u|S, F ) as a constant if u can be built with S and F . By using these assumptions, the posterior probability is reduced to the following formula:\nP (S|u) \u221d P (S)P (F ) (3)\nwhere F in this formula is fillers identified given u and S. Consequently, the proposed method attempts to find the most likely combination of the slot values and the non-slot phrases, since all words in an utterance have to belong to either of them. By using trained HDPSM (i.e., the posterior given all training data), P (S) and P (F ) can be computed by (2)."}, {"heading": "5 Candidate Generation", "text": "For estimating slot values given u, we adopt a candidate generation approach (Williams, 2014) that leverages another slot filling algorithm to enumerate likely candidates. 2 Specifically, we assume a candidate generation function g(u) that generates N candidates {S1, ..., SN} regarding u. Our slot filling algorithm computes the posterior probability by (3) for each candidate slot Sj and takes the candidate that has the highest posterior probability. In this estimation process, our utterance model works as a secondary filter that covers the error of the primary analysis.\nFigure 5 provides an example of candidate generation by using a sequential labeling algorithm with IOB tags. The subsequences to which the O tag is assigned can be regarded as functional fillers. The values for each filler slot are identified depending on the position of the subsequence, as the figure shows."}, {"heading": "6 Experiments", "text": "We evaluate the performance of the proposed generative model with an experiment using the algorithm\n2The direct inference of the generative utterance model is a topic for near future work. The MCMC method will circumvent the difficulty of searching the entire candidate space.\ndescribed in Section 5. We adopt a conditional random field (CRF) as a candidate generation algorithm that generates N -best estimation as candidates. For the CRF, we apply commonly used features including unigram and bigram of the surface form and part of speech of the word. We used CRF++3 as the CRF implementation."}, {"heading": "6.1 Dataset", "text": "The performance of our method is evaluated using two datasets from different languages, as summarized in Table 1. The first dataset is provided by the third Dialog State Tracking Challenge (Henderson, 2015), hereafter referred to as the DSTC corpus. The DSTC corpus consists of dialogs in the tourist information domain. In our experiment, we use the user\u2019s first utterance in each dialog, which typically describes the user\u2019s query to the system. Utterances without any slot information are excluded. We manually modified the annotated slot values into \u201casis form\u201d to allow a sequential labeling method to extract the ground-truth values. This identification process can be done in a semi-automatic manner that involves no expert knowledge. We apply the part of speech tagger in NLTK4 for the CRF application.\nThe second dataset is a weather corpus consisting of user utterances in an in-house corpus of humanmachine dialogues in the weather domain. It contains 1,442 questions spoken in Japanese. In this corpus, the number of value types for each slot is higher than DSTC, which indicates a more challenging task. We applied the Japanese morphological analyzer MeCab (Kudo et al., 2004) to segment the Japanese text into words before applying CRF.\nFor both datasets, we examine the effect of the amount of available annotated utterances by varying the number of training data in 25, 50, 75, 100, 200, 400, 800, all.\n3https://taku910.github.io/crfpp/ 4http://www.nltk.org/"}, {"heading": "25 0.560 0.706* 0.684*", "text": ""}, {"heading": "50 0.709 0.791* 0.765*", "text": ""}, {"heading": "75 0.748 0.824* 0.817*", "text": ""}, {"heading": "25 0.327 0.452* 0.480*", "text": ""}, {"heading": "50 0.379 0.488* 0.499*", "text": ""}, {"heading": "75 0.397 0.504* 0.522*", "text": ""}, {"heading": "6.2 Evaluation Metrics", "text": "The methods are compared in terms of slot estimation accuracy. Let nc be the number of utterances for which the estimated slot S and the ground-truth slot S\u0302 are perfectly matched, and let ne be the number of the utterances including an estimation error. The slot estimation accuracy is simply calculated as nc nc+ne . All evaluation scores are calculated as the average of 10-fold cross validation. We also conduct a binomial test to examine the statistical significance of the improvement in the proposed algorithm compared to the CRF baseline."}, {"heading": "6.3 Results", "text": "Tables 2 and 3 present the slot estimation accuracy for the DSTC corpus and the Japanese weather corpus, respectively. The baseline (CRF best) is a method that takes only one best output of CRF for slot estimation. HDP with N = 5 and N = 300 is the proposed method, where N is the number of candidates generated by the CRF candidate genera-\ntor. The asterisks (*) beside the HDP accuracy indicate the statistical significance against CRF best, which is tested using the binomial test.\nResults show that our proposed method performs significantly better than CRF. Especially when the amount of training data is limited, the proposed method outperforms the baseline. This property is attractive for practical speech recognition systems that offer many different functions. Accurate recognition at an early stage of development allows a practitioner to launch a service that results in quickly collecting hundreds of speech examples.\nSince we use the CRF as a candidate generator, we expect that the CRF N-best can rank the correct answer higher in the candidate list. In fact, the top five candidates cover almost all of the correct answers. Therefore, the result in the comparison of N = 5 and N = 300 suggests the stability of the proposed method against the mostly noisy 295 candidates. Because the proposed algorithm makes no use of the original ranking order, N = 300 is a harder condition in which to identify the correct answer. Nevertheless, the result shows that the drop in the performance is limited; the accuracy is still significantly better than the baseline. This result suggests that the proposed method is less dependent on the performance of the candidate generator.\nTable 4 presents some examples of the slot values estimated by CRF best and HDP with N = 5 for the condition where the number of training utterances is 800. The first two are samples where CRF best failed to predict the correct values. These errors are attributed to infrequent sequential patterns caused by the less trained expressions \u201cthat serves fast food\u201d and \u201cmoderate restaurant\u201d because CRF is a position-based classifier. The value-based formulation allows the model to learn that the phrase\n\u201cfast food\u201d is more likely to be a food name than to be a functional filler and to reject the candidate.\nThe third example in Table 4 shows an error using HDP, which extracted \u201cchine chinese takeaway\u201d which includes a reparandum of disfluency (Georgila et al., 2010). This error can be attributed to the fact that this kind of disfluency resembles the true slot value, which leads to a higher probability of \u201cchine\u201d in the food slot model compared to in the functional filler model. Regarding this type of error, preliminary application of a disfluency detection method (Zayats et al., 2016) is promising for improving accuracy.\nThe execution time for training the proposed HDP utterance model with 1297 training data in the Japanese weather corpus was about 0.3 seconds. This is a good performance since the CRF training takes about 5.5 seconds. Moreover, the training of the proposed HDP model is scalable and works in an online manner because it is a single pass algorithm. When we have a very large number of training examples, the bottleneck is the CRF training, which requires scanning the whole dataset repeatedly."}, {"heading": "7 Conclusion", "text": "In this paper, we proposed an arbitrary slot filling method that directly deals with the posterior probability of slot values by using nonparametric Bayesian models. We presented a two-stage method that involves an N-best candidate generation step, which is typically done using a CRF. Experimental results show that our method significantly improves recognition accuracy. This empirical evidence suggests that the value-based formulation is a promising approach for arbitrary slot filling tasks, which is worth exploring further in future work."}], "year": 2016, "references": [{"title": "Matrix Factorization with Knowledge Graph Propagation for Unsupervised Spoken Language Understanding", "authors": ["Yun-Nung Chen", "William Yang Wang", "Anatole Gershman", "Alexander Rudnicky."], "venue": "Proc. Annual Meeting of the Association for Computational", "year": 2015}, {"title": "A Bayesian Analysis of Some Nonparametric Problems", "authors": ["Thomas S. Ferguson."], "venue": "The Annual of Statistics, 1(2):209\u2013230.", "year": 1973}, {"title": "Cross-Domain Speech Disfluency Detection", "authors": ["Kallirroi Georgila", "Ning Wang", "Jonathan Gratch."], "venue": "Proc. Annual SIGDIAL Meeting on Discourse and Dialogue.", "year": 2010}, {"title": "A tutorial on Bayesian nonparametric models", "authors": ["Samuel J. Gershman", "David M. Blei."], "venue": "Journal of Mathematical Psychology, 56(1):1\u201312.", "year": 2012}, {"title": "Automatic labeling of semantic roles", "authors": ["Daniel Gildea."], "venue": "Computational Linguistics, 28(3):245\u2013288.", "year": 2002}, {"title": "Producing Power-Law Distributions and Damping Word Frequencies with Two-Stage Language Models", "authors": ["Sharon Goldwater", "Thomas L. Griffiths", "Mark Johnson."], "venue": "Journal of Machine Learning Research, 12:2335\u20132382.", "year": 2011}, {"title": "Comparing stochastic approaches to spoken language understanding in multiple languages", "authors": ["Stefan Hahn", "Marco Dinarelli", "Christian Raymond", "Fabrice Lefevre", "Patrick Lehnen", "Renato De Mori", "Alessandro Moschitti", "Hermann Ney", "Giuseppe Riccardi"], "year": 2011}, {"title": "Machine Learning for Dialog State Tracking: A Review", "authors": ["Matthew Henderson."], "venue": "Proc. Workshop on Machine Learning in Spoken Language Processing.", "year": 2015}, {"title": "Automatic Allocation of Training Data for Rapid Prototyping", "authors": ["Kazunori Komatani", "Masaki Katsumaru", "Mikio Nakano", "Kotaro Funakoshi", "Tetsuya Ogata", "Hiroshi G. Okuno."], "venue": "Proc. International Conference on Computational Linguistics.", "year": 2010}, {"title": "Applying Conditional Random Fields to Japanese Morphological Analysis", "authors": ["Taku Kudo", "Kaoru Yamamoto", "Yuji Matsumoto."], "venue": "Proc. Empirical Methods in Natural Language Processing.", "year": 2004}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "authors": ["John Lafferty", "Andrew McCallum", "Fernando C N Pereira."], "venue": "Proc. International Conference on Machine Learning.", "year": 2001}, {"title": "Unsupervised structured semantic inference for spoken dialog reservation tasks", "authors": ["Alejandra Lorenzo", "Lina M Rojas-barahona", "Christophe Cerisara."], "venue": "Proc. Annual SIGDIAL Meeting on Discourse and Dialogue.", "year": 2013}, {"title": "Using Recurrent Neural Networks for Slot Filling in Spoken Language Understanding", "authors": ["Gregoire Mesnil", "Yann Dauphin", "Kaisheng Yao", "Yoshua Bengio", "Li Deng", "Dilek Hakkani-Tur", "Xiaodong He", "Larry Heck", "Gokhan Tur", "Dong Yu", "Geoffrey Zweig"], "year": 2015}, {"title": "Discriminative state tracking for spoken dialog systems", "authors": ["Angeliki Metallinou", "Dan Bohus", "Jason Williams."], "venue": "Proc. Annual Meeting of the Association for Computational Linguistics.", "year": 2013}, {"title": "On the use of finite state transducers for semantic interpretation", "authors": ["Christian Raymond", "Fr\u00e9d\u00e9ric B\u00e9chet", "Renato De Mori", "G\u00e9raldine Damnati."], "venue": "Speech Communication, 48(3-4):288\u2013304.", "year": 2006}, {"title": "Markovian discriminative modeling for dialog state tracking", "authors": ["Hang Ren", "Weiqun Xu", "Yonghong Yan."], "venue": "Proc. Annual SIGDIAL Meeting on Discourse and Dialogue.", "year": 2014}, {"title": "Hierarchical Dirichlet Processes", "authors": ["Yee W. Teh", "Michael I. Jordan", "Matthew J. Beal", "David M. Blei."], "venue": "Journal of the American Statistical Association, 101:1566\u20131581.", "year": 2005}, {"title": "Latent Semantic Modeling for Slot Filling in Conversational Understanding", "authors": ["Gokhan Tur", "Asli Celikyilmaz", "Dilek Hakkani-Tur."], "venue": "Proc. International Conference on Acoustics, Speech and Signal Processing.", "year": 2013}, {"title": "Is it Time to Switch to Word Embedding and Recurrent Neural Networks for Spoken Language Understanding? In Proc", "authors": ["Vedran Vukotic", "Christian Raymond", "Guillaume Gravier."], "venue": "Interspeech.", "year": 2015}, {"title": "Incremental partition recombination for efficient tracking of multiple dialog states", "authors": ["Jason D. Williams."], "venue": "Proc. International Conference on Acoustics, Speech and Signal Processing.", "year": 2010}, {"title": "Web-style ranking and SLU combination for dialog state tracking", "authors": ["Jason D Williams."], "venue": "Proc. Annual SIGDIAL Meeting on Discourse and Dialogue.", "year": 2014}, {"title": "Convolutional neural network based triangular CRF for joint intent detection and slot filling", "authors": ["Puyang Xu", "Ruhi Sarikaya."], "venue": "Proc. IEEE Workshop on Automatic Speech Recognition and Understanding.", "year": 2013}, {"title": "Disfluency Detection using a Bidirectional LSTM", "authors": ["Vicky Zayats", "Mari Ostendorf", "Hannaneh Hajishirzi."], "venue": "arXiv preprint arXiv:1604.03209.", "year": 2016}, {"title": "Online Latent Dirichlet Allocation with Infinite Vocabulary", "authors": ["Ke Zhai", "Jordan Boyd-graber."], "venue": "Proc. International Conference on Machine Learning.", "year": 2013}], "id": "SP:c82fb23b008c11b064570724364d4f52910701bf", "authors": [{"name": "Kei Wakabayashi", "affiliations": []}, {"name": "Johane Takeuchi", "affiliations": []}, {"name": "Kotaro Funakoshi", "affiliations": []}, {"name": "Mikio Nakano", "affiliations": []}], "abstractText": "In this paper, we propose a new generative approach for semantic slot filling task in spoken language understanding using a nonparametric Bayesian formalism. Slot filling is typically formulated as a sequential labeling problem, which does not directly deal with the posterior distribution of possible slot values. We present a nonparametric Bayesian model involving the generation of arbitrary natural language phrases, which allows an explicit calculation of the distribution over an infinite set of slot values. We demonstrate that this approach significantly improves slot estimation accuracy compared to the existing sequential labeling algorithm.", "title": "Nonparametric Bayesian Models for Spoken Language Understanding"}