0,1,label2,summary_sentences
"Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 93–102, Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics",text,[0],[0]
"Authorship Attribution (AA) tackles the problem of determining who, among a set of authors, wrote the document at hand.",1 Introduction,[0],[0]
"AA has relevant applications ranging from plagiarism detection (Stamatatos, 2011) to Forensic Linguistics, such as identifying authorship of threatening emails or malicious code.",1 Introduction,[0],[0]
"Applied areas such as law and journalism can also benefit from authorship attribution, where identifying the true author of a piece of text (such as a ransom note) may help save lives or catch the offenders.
",1 Introduction,[0],[0]
"We know from state of the art research in AA that the length of the documents and the number of po-
tential candidate authors have an important effect on the accuracy of AA approaches (Moore, 2001; Luyckx and Daelemans, 2008; Luyckx and Daelemans, 2010).",1 Introduction,[0],[0]
"We can also point out the most common features that have been used successfully in AA work, including: bag-of-words (Madigan et al., 2005; Stamatatos, 2006), stylistic features (Zheng et al., 2006; Stamatatos et al., 2000), and word and character level n-grams (Kjell et al., 1994; Keselj et al., 2003; Peng et al., 2003; Juola, 2006).
",1 Introduction,[0],[0]
"The utility of bag-of-words features is well understood: they effectively capture correlations between authors and topics (Madigan et al., 2005; Kaster et al., 2005).",1 Introduction,[0],[0]
"The discriminative value of these features is thus directly related to the level of content divergence among authors and among train and test sets.
",1 Introduction,[0],[0]
"The utility of stylistic features is also well understood: they model author preferences for the use of punctuation marks, emoticons, white spaces, and other traces of writing style.",1 Introduction,[0],[0]
"Such preferences are less influenced by topic, and directly reflect some of the unique writing patterns of an author.
",1 Introduction,[0],[0]
Character n,1 Introduction,[0],[0]
"-grams are the single most successful feature in authorship attribution (Koppel et al., 2009; Frantzeskou et al., 2007; Koppel et al., 2011), but the reason for their success is not well understood.",1 Introduction,[0],[0]
"One hypothesis is that character n-grams carry a little bit of everything: lexical content, syntactic content, and even style by means of punctuation and white spaces (Koppel et al., 2011).",1 Introduction,[0],[0]
"While this argument seems plausible, it falls short of a rigorous explanation.
",1 Introduction,[0],[0]
"In this paper, we investigate what in the make-up
93
of these small units of text makes them so powerful.",1 Introduction,[0],[0]
"Our goal is two-fold: on the one hand we want to have a principled understanding of character ngrams that will inform their use as features for AA and other tasks; on the other hand we want to make AA approaches more accessible to non-experts so that, for example, they could be acceptable pieces of evidence in criminal cases.
",1 Introduction,[0],[0]
"The research questions we aim to answer are:
• Are all character n-grams equally important?",1 Introduction,[0],[0]
"For example, are the prefix of ‘there’, the suffix of ‘breathe’ and the whole word ‘the’ all equivalent?",1 Introduction,[0],[0]
"More generally, are character n-grams that capture morpho-syntactic information, thematic information and style information equally important?
",1 Introduction,[0],[0]
• Are the character n-grams that are most important for single-domain settings also the most important for cross-domain settings?,1 Introduction,[0],[0]
"Which character n-grams are more like bag-of-words features (which tend to track topics), and which are more like stylistic features (which tend to track authors)?
",1 Introduction,[0],[0]
• Do different classifiers agree on the importance of the different types of character n-grams?,1 Introduction,[0],[0]
"Are some character n-grams consistently the best regardless of the learning algorithm?
",1 Introduction,[0],[0]
• Are some types of character n-grams irrelevant in AA tasks?,1 Introduction,[0],[0]
Are there categories of character n-grams that we can exclude and get similar (or better) performance than using all n-grams?,1 Introduction,[0],[0]
"If there are, are they the same for both singledomain and cross-domain AA settings?
",1 Introduction,[0],[0]
"Our study shows that using the default bag-ofwords representation of char n-grams results in collapsing sequences of characters that correspond to different linguistic aspects, and that this yields suboptimal prediction performance.",1 Introduction,[0],[0]
We further show that we can boost accuracy by loosing some categories of n-grams.,1 Introduction,[0],[0]
"Char n-grams closely related to thematic content can be completely removed without loss of accuracy, even in cases where the train and test sets have the same topics represented, a counter-intuitive argument.",1 Introduction,[0.9523386071961953],"['While this observation is rather immediate, it has profound implications owing to the fact that path regularization provides size-independent capacity control in deep learning, thereby supporting empirical evidence that dropout finds good solutions in over-parametrized settings.']"
"Given the wide spread use of char n-grams
in text classification tasks, our findings have significant implications for future work in related areas.",1 Introduction,[0],[0]
"To answer our research questions and explore the value of character n-grams in authorship attribution, we propose to separate character n-grams into ten distinct categories.",2 Categories of Character N -grams,[0],[0]
"Unlike previous AA work where all character n-grams were combined into a single bagof-n-grams, we evaluate each category separately to understand its behavior and effectiveness in AA tasks.",2 Categories of Character N -grams,[0],[0]
"These categories are related to the three linguistic aspects hypothesized to be represented by character n-grams: morpho-syntax (as represented by affix-like n-grams), thematic content (as represented by word-like n-grams) and style (as represented by punctuation-based n-grams).",2 Categories of Character N -grams,[0],[0]
"We refer to these three aspects as super categories (SC).
",2 Categories of Character N -grams,[0],[0]
The following sections describe the different types of n-grams.,2 Categories of Character N -grams,[0],[0]
We use the sentence in Table 1 as a running example for the classes and in Table 2 we show the resulting n-grams in that sentence.,2 Categories of Character N -grams,[0],[0]
"For ease of understanding, we replace spaces in n-grams with underscores ( ).
2.1 Affix n-grams Character n-grams are generally too short to represent any deep syntax, but some of them can reflect morphology to some degree.",2 Categories of Character N -grams,[0],[0]
"In particular, we consider the following affix-like features by looking at n-grams that begin or end a word:
prefix A character n-gram that covers the first n characters of a word that is at least n+ 1 characters long.
suffix A character n-gram that covers the last n characters of a word that is at least n + 1 characters long.
space-prefix A character n-gram that begins with a space.
space-suffix A character n-gram that ends with a space.
",2 Categories of Character N -grams,[0],[0]
"2.2 Word n-grams While character n-grams are often too short to capture entire words, some types can capture partial words and other word-relevant tokens.",2 Categories of Character N -grams,[0],[0]
"We consider the following such features:
whole-word A character n-gram that covers all characters of a word that is exactly n characters long.
",2 Categories of Character N -grams,[0],[0]
mid-word,2 Categories of Character N -grams,[0],[0]
"A character n-gram that covers n characters of a word that is at least n + 2 characters long, and that covers neither the first nor the last character of the word.
",2 Categories of Character N -grams,[0],[0]
"multi-word N -grams that span multiple words, identified by the presence of a space in the middle of the n-gram.
",2 Categories of Character N -grams,[0],[0]
2.3 Punctuation n-grams,2 Categories of Character N -grams,[0],[0]
The main stylistic choices that character n-grams can capture are the author’s preferences for particular patterns of punctuation.,2 Categories of Character N -grams,[0],[0]
"The following features characterize punctuation by its location in the n-gram.
beg-punct A character n-gram",2 Categories of Character N -grams,[0],[0]
"whose first character is punctuation, but middle characters are not.
",2 Categories of Character N -grams,[0],[0]
"mid-punct A character n-gram with at least one punctuation character that is neither the first nor the last character.
end-punct A character n-gram whose last character is punctuation, but middle characters are not.
",2 Categories of Character N -grams,[0],[0]
"The above ten categories are intended to be disjoint, so that a character n-gram belongs to exactly one of the categories.",2 Categories of Character N -grams,[0],[0]
"For n-grams that contain both spaces and punctuation, we first categorize by punctuation and then by spaces.",2 Categories of Character N -grams,[0],[0]
"For example, ‘e, ’ is assigned to the mid-punct category, not the spacesuffix category.
",2 Categories of Character N -grams,[0],[0]
We have observed that in our data almost 80% of the n-grams in the punct-beg and punct-mid categories contain a space.,2 Categories of Character N -grams,[0],[0]
This tight coupling of punctuation and spaces is due to the rules of English orthography: most punctuation marks require a space following them.,2 Categories of Character N -grams,[0],[0]
"The 20% of n-grams that have punctuation but no spaces correspond mostly to the exceptions to this rule: quotation marks, mid-word hyphens, etc.",2 Categories of Character N -grams,[0],[0]
An interesting experiment for future work would be to split out these two types of punctuation into separate feature categories.,2 Categories of Character N -grams,[0],[0]
"We consider two corpora, a single-domain corpus, where there is only one topic that all authors are writing about, and a multi-domain corpus, where there are multiple different topics.",3 Datasets,[0],[0]
"The latter allows us to test the generalization of AA models, by testing them on a different topic from that used for training.
",3 Datasets,[0],[0]
"The first collection is the CCAT topic class, a subset of the Reuters Corpus Volume 1 (Lewis et al., 2004).",3 Datasets,[0],[0]
"Although this collection was not gathered for the goal of doing authorship attribution studies, previous work has reported results for AA with 10 and 50 authors (Stamatatos, 2008; Plakias and Stamatatos, 2008; Escalante et al., 2011).",3 Datasets,[0],[0]
"We refer to these as CCAT 10 and CCAT 50, respectively.",3 Datasets,[0],[0]
"Both CCAT 10 and CCAT 50 belong to CCAT category (about corporate/industrial news) and are balanced across authors, with 100 documents sampled for each author.",3 Datasets,[0],[0]
Manual inspection of the dataset revealed that some of the authors in this collection consistently used signatures at the end of documents.,3 Datasets,[0.9533739265465123],"['As in the case of autoencoders with tied weights in Section 2, a complete characterization of the implicit bias of dropout is given by considering the global optimality in terms of the network, i.e. in terms of the product of the weight matrices UV>.']"
"Also, we noticed some writers use quotations a lot.",3 Datasets,[0],[0]
"Con-
sidering these parts of text for measuring the frequencies of character n-grams is not a good idea because signatures provide direct clues about the authorship of document and quotations do not reflect the author’s writing style.",3 Datasets,[0],[0]
"Therefore, to clean up the CCAT collection, we preprocessed it to remove signatures and quotations from each document.",3 Datasets,[0],[0]
"Since the CCAT collection contains documents belonging to only corporate/industrial topic category, this will be our single-domain collection.
",3 Datasets,[0],[0]
"The other collection consists of texts published in The Guardian daily newspaper written by 13 authors in four different topics (Stamatatos, 2013).",3 Datasets,[0],[0]
"This dataset contains opinion articles on the topics: World, U.K., Society, and Politics.",3 Datasets,[0],[0]
"Following prior work, to make the collection balanced across authors, we choose at most ten documents per author for each of the four topics.",3 Datasets,[0],[0]
We refer to this corpus as Guardian1.,3 Datasets,[0],[0]
"We also consider a variation of this corpus that makes it more challenging but that more closely matches realistic scenarios of forensic investigation that deal with short texts such as tweets, SMS, and emails.",3 Datasets,[0],[0]
We chunk each of the documents by sentence boundaries into five new short documents.,3 Datasets,[0],[0]
"We refer to this corpus as Guardian2.
",3 Datasets,[0],[0]
"Table 3 shows some of the statistics of the CCAT and Guardian corpora and Table 4 presents some of the top character n-grams for each category (taken from an author in the Guardian data, but the top ngrams look qualitatively similar for other authors).",3 Datasets,[0],[0]
We performed various experiments using different categories of character n-grams.,4 Experimental Settings,[0],[0]
We chose n=3 since our preliminary experiments found character 3-grams to be more effective than other higher level character n-grams.,4 Experimental Settings,[0],[0]
"For each category, we considered only those 3-grams that occur at least five times in the training documents.
",4 Experimental Settings,[0],[0]
"The performance of different authorship attribu-
tion models was measured in terms of accuracy.",4 Experimental Settings,[0],[0]
"In the single-domain CCAT experiments, accuracy was measured using the train/test partition of prior work.",4 Experimental Settings,[0],[0]
"In the cross-domain Guardian experiments, accuracy was measured by considering all 12 possible pairings of the 4 topics, treating one topic as training data and the other as testing data, and averaging accuracy over these 12 scenarios.",4 Experimental Settings,[0],[0]
"This ensured that in the crossdomain experiments, the topics of the training data were always different from that of the test data.
",4 Experimental Settings,[0],[0]
"We trained support vector machine (SVM) classifiers using the Weka implementation (Witten and Frank, 2005) with default parameters.",4 Experimental Settings,[0],[0]
We also ran some comparative experiments with the Weka implementation of naive Bayes classifiers and the LibSVM implementation of SVMs.,4 Experimental Settings,[0],[0]
"In the results below, when performance of a single classifier is presented, it is the result of Weka’s SVM, which generally gave the best performance.",4 Experimental Settings,[0],[0]
"When performance of other classifiers are presented, the classifiers are explicitly indicated.",4 Experimental Settings,[0],[0]
"In this section, we present various results on authorship attribution tasks using both single as well as cross-domain datasets.",5 Experimental Results and Evaluation,[0],[0]
"We will explore character ngrams in depth and try to understand why they are so effective in discriminating authors.
5.1 Which n-gram Categories are Most Author-Discriminative?
After breaking character n-grams into ten disjoint categories, we empirically illustrate what categories are
Single Domain (CCAT)
most discriminative.",5 Experimental Results and Evaluation,[0],[0]
"Table 5 shows the accuracy of each type of n-gram for each of the different corpora.
",5 Experimental Results and Evaluation,[0],[0]
"Table 5(a) shows that the top four categories for single-domain AA are: prefix, suffix, space-prefix, and mid-word.",5 Experimental Results and Evaluation,[0],[0]
These four categories have the best performance on both CCAT 10 and CCAT 50.,5 Experimental Results and Evaluation,[0],[0]
"In contrast, Table 5(b) shows that the top four categories for cross-domain AA are: prefix, space-prefix, beg-
punct, and mid-punct.",5 Experimental Results and Evaluation,[0],[0]
"For both single-domain and cross-domain AA, prefix and space-prefix are strong features, and are generally better than the suffix features, perhaps because authors have more control over prefixes in English, while suffixes are often obligatory for grammatical reasons.",5 Experimental Results and Evaluation,[0],[0]
"For cross-domain AA, beg-punct and midpunct are the top features, likely because an author’s
use of punctuation is consistent even when the topic changes.",5 Experimental Results and Evaluation,[0],[0]
"For single-domain AA, mid-word was also a good feature, probably because it captured lexical information that correlates with authors’ preferences towards writing about specific topics.
",5 Experimental Results and Evaluation,[0],[0]
"Figure 1 shows an alternate view of these results, graphing the rank of each n-gram type.",5 Experimental Results and Evaluation,[0],[0]
"For computing the rank, the accuracies of the ten different n-gram type classifiers are sorted in decreasing order and ranked from 1 to 10 respectively with ties getting the same rank.",5 Experimental Results and Evaluation,[0],[0]
"For the Guardian corpora, the average rank of each n-gram category was computed by averaging its rank across the 12 possible test/train cross-domain combinations.",5 Experimental Results and Evaluation,[0],[0]
"In both of the single-domain CCAT corpora, the classifier based on prefix n-grams had the top accuracy (rank 1), and the classifier based on mid-punct had the worst accuracy (rank 10).",5 Experimental Results and Evaluation,[0],[0]
"In both of the cross-domain Guardian corpora, on the other hand, mid-punct was among the top-ranked n-gram categories.",5 Experimental Results and Evaluation,[0],[0]
"This suggests that punctuation features generalize the best across topic, but if AA is more of a topic classification task (as in the single-domain CCAT corpora), then punctuation adds little over other features that more directly capture the topic.
",5 Experimental Results and Evaluation,[0],[0]
"Since our cross-domain datasets are small, we performed a small number of planned comparisons using a two-tailed t-test over the accuracies on the Guardian1 and Guardian2 corpora.",5 Experimental Results and Evaluation,[0],[0]
"We found that in both corpora, the best punctuation category (punctmid) is better than the best word category (wholeword) with p < 0.001.",5 Experimental Results and Evaluation,[0],[0]
"In the Guardian2 corpus, the best affix category (space-prefix) is also better than the best word category (whole-word) with p < 0.05, but this does not hold in the Guardian1 corpus (p = 0.14).",5 Experimental Results and Evaluation,[0],[0]
"Also, we observed that in both Guardian1 and Guardian2 datasets, both punct-mid and spaceprefix are better than multi-word (p < 0.01).
",5 Experimental Results and Evaluation,[0],[0]
"Overall, we see that affix n-grams are generally effective in both single-domain and cross-domain settings, punctuation n-grams are effective in crossdomain settings, and mid-word is the only effective word n-gram, and only in the single-domain setting.",5 Experimental Results and Evaluation,[0],[0]
"Importance of Different n-gram Types?
",5.2 Do Different Classifiers Agree on the,[0],[0]
"The previous experiments have shown, for example, that prefix n-grams are universally predictive in AA
tasks, that mid-word n-grams are good predictors in single-domain settings, and that beg-punct n-grams are good predictors in cross-domain settings.",5.2 Do Different Classifiers Agree on the,[0],[0]
"But are these facts about the n-gram types themselves, or are these results only true for the specific SVM classifiers we trained?
",5.2 Do Different Classifiers Agree on the,[0],[0]
"To see whether certain types of n-grams are fundamentally good or bad, regardless of the classifier, we compare performance of the different n-gram types for three classifiers: Weka SVM classifiers (as used in our other experiments), LibSVM classifiers and Weka’s naive Bayes classifiers1.",5.2 Do Different Classifiers Agree on the,[0],[0]
"Figure 2 shows the n-gram category rankings for all these classifiers2 for both the single-domain CCAT and the cross-domain Guardian settings.
",5.2 Do Different Classifiers Agree on the,[0],[0]
"Across the different classifiers, the pattern of feature rankings are similar.",5.2 Do Different Classifiers Agree on the,[0],[0]
Table 6 shows the Spearman’s rank correlation coefficient (ρ) for the per-ngram-type accuracies of each pair of classifiers.,5.2 Do Different Classifiers Agree on the,[0],[0]
"We observe fairly high correlations, with ρ above 0.70 for all single-domain pairings, and between 0.44 and 0.81 for cross-domain pairings.
",5.2 Do Different Classifiers Agree on the,[0],[0]
"As in Section 5.1, prefix and space-prefix are among the most predictive n-gram types.",5.2 Do Different Classifiers Agree on the,[0],[0]
"In the single-domain settings, we again see that suffix and mid-word are also highly predictive, while in the cross-domain settings, we again see that beg-punct and mid-punct are highly predictive.",5.2 Do Different Classifiers Agree on the,[0],[0]
"These results all confirm that some types of n-grams are fundamentally more predictive than others, and our results are not specific to the particular type of classifier used.
",5.2 Do Different Classifiers Agree on the,[0],[0]
"1Weka SVM and LibSVM are both support vector machine classifiers, but Weka uses Platt’s sequential minimal optimization algorithm while LibSVM uses working set selection with second order information.",5.2 Do Different Classifiers Agree on the,[0],[0]
"The result is that they achieve different performance on our AA tasks.
2We also tried a decision tree classifier, C4.5 (J48) from WEKA, and it produced similar patterns (not shown).
",5.2 Do Different Classifiers Agree on the,[0],[0]
Single Domain (CCAT),5.2 Do Different Classifiers Agree on the,[0],[0]
"In the previous sections, we have seen that some types of character n-grams are more predictive than others - affix n-grams performed well in both single domain and cross-domain settings and punctuation n-grams performed well in cross-domain settings.",5.3 Are Some Character N -grams Irrelevant?,[0],[0]
"In general, word n-grams were not as predictive as other types of n-grams (with the one exception being mid-word n-grams in the single domain setting).",5.3 Are Some Character N -grams Irrelevant?,[0],[0]
"Given this poor performance of word n-grams, a natural question is: could we exclude these features entirely and achieve similar performance?
",5.3 Are Some Character N -grams Irrelevant?,[0],[0]
Our goal then is to compare a model trained on affix n-grams and punct n-grams against a model trained on “all” n-grams.,5.3 Are Some Character N -grams Irrelevant?,[0],[0]
"We consider two definitions of “all”:
all-untyped The traditional approach to extracting n-grams where n-gram types are ignored (e.g., ‘the’ as a whole word is no different from ‘the’ in the middle of a word)
all-typed The approach discussed in this paper, where n-grams of different types are distinguished (equivalent to the set of all affix+punct+word n-grams).
",5.3 Are Some Character N -grams Irrelevant?,[0],[0]
"We compare these models trained on all the n-grams to our affix+punct model.
",5.3 Are Some Character N -grams Irrelevant?,[0],[0]
Table 7 shows this analysis.,5.3 Are Some Character N -grams Irrelevant?,[0],[0]
"For either definition of “all”, the model that discards all word features achieves performance as high or higher than the model with all of the features, and does so with only about two thirds of the features.",5.3 Are Some Character N -grams Irrelevant?,[0],[0]
"This is not too surprising in the cross-domain Guardian tasks, where the word n-grams were among the worst features.",5.3 Are Some Character N -grams Irrelevant?,[0],[0]
"On the single-domain CCAT tasks this result is more surprising, since we have discarded the mid-word n-grams, which was one of the best single-domain n-gram types.",5.3 Are Some Character N -grams Irrelevant?,[0],[0]
This indicates that whatever information mid-word is capturing it is also being captured in other ways via affix and punct n-grams.,5.3 Are Some Character N -grams Irrelevant?,[0],[0]
"Of all 1024 possible combinations of features, we tried a
number of different combinations and were unable to identify one that outperformed affix+punct.",5.3 Are Some Character N -grams Irrelevant?,[0],[0]
"Overall, this experiment gives compelling evidence that affix and punct n-grams are more important than word n-grams.",5.3 Are Some Character N -grams Irrelevant?,[0],[0]
We did a manual exploration of our datasets.,6 Analysis,[0],[0]
"In our cross-domain dataset, the character 3-gram ‘sti’ shows up as both prefix and mid-word.",6 Analysis,[0],[0]
"All 13 authors use ‘sti’ frequently as a mid-word n-gram in words such as institution, existing, justice, and distinction.",6 Analysis,[0],[0]
"For example:
• The government’s story is that the existing warheads might be deteriorating.
",6 Analysis,[0],[0]
"• For all the justice of many of his accusations, the result is occasionally as dreadful as his title suggests.
",6 Analysis,[0],[0]
"But only six authors use ‘sti’ as a prefix, in examples like:
• Their mission was to convince tourists that Britain was still open for business.
",6 Analysis,[0],[0]
"• There aren’t even any dead people on it, since by the very act of being dead and still famous, they assert their long-term impact.
",6 Analysis,[0],[0]
Thus ‘sti’ as a prefix is predictive of authorship even though ‘sti’ as a mid-word n-gram is not.,6 Analysis,[0],[0]
"Notably, under the traditional untyped bag-of-n-grams approach, both versions of ‘sti’ would have been treated the same, and this discriminative power would have been lost.
",6 Analysis,[0],[0]
"As already demonstrated in Section 5 that affix+punct features perform better than using all the features, we would like to use an example from our dataset to visualize the text when features in SC word are discarded.",6 Analysis,[0],[0]
Out of seven categories in affix and punct,6 Analysis,[0],[0]
", we computed in how many of them each character belongs to, three being the maximum possible value.",6 Analysis,[0],[0]
"Therefore, we show each character with different opacity level depending on number of categories it belongs to: zero will get white color (word related n-grams), one will get 33% black, two will get 67% black, and three will get 100% black.",6 Analysis,[0],[0]
"In Table 8, we show an example sentence before (first row of Table 8) and after (second row of Table 8) showing the opacity level of each character.",6 Analysis,[0],[0]
"It is clear that the darkest characters are those around the punctuation characters and those around spaces are second darkest, while the lightest (with 0% darkness) are the ones in the middle of long words.",6 Analysis,[0],[0]
This gives us an idea about the characters in a text that are important for AA tasks.,6 Analysis,[0],[0]
"Various hypotheses have been put forth to explain the “black magic” (Kestemont, 2014) behind the success of character n-gram features in authorship attribution.",7 Discussion,[0],[0]
Kestemont (2014) conjectured that their utility was in capturing function words and morphology.,7 Discussion,[0],[0]
"Koppel et al. (2009) suggested that they were capturing topic information in single domain settings, and style and syntactic information in cross-domain settings.",7 Discussion,[0],[0]
Our study provides empirical evidence for testing these claims.,7 Discussion,[0],[0]
"We did indeed find that the ability of character n-grams to capture morphology is useful, as reflected in the high prediction performance of af-
fix n-grams in both single-domain and cross-domain settings.",7 Discussion,[0],[0]
"And we found that word n-grams (capturing topic information) were useful in single domain settings, while puct n-grams (capturing style information) were useful in cross-domain settings.",7 Discussion,[0],[0]
"We further found that word n-grams are unnecessary, even in single-domain settings.",7 Discussion,[0],[0]
"Models based only on affix and punct n-grams performed as well as models with all n-grams regardless of whether it was a single-domain or cross-domain authorship attribution task.
",7 Discussion,[0],[0]
Our findings on the value of selecting n-grams according to the linguistic aspect they represent may also be beneficial in other classification tasks where character n-grams are commonly used.,7 Discussion,[0],[0]
"Promising tasks are those related to the stylistic analysis of texts, such as native language identification, document similarity and plagiarism detection.
",7 Discussion,[0],[0]
"Morphologically speaking, English is a poor language.",7 Discussion,[0],[0]
The fact that we identified significant differences in performance by selecting n-gram categories that are related to affixation in this poorly inflected language suggests that we may find even larger differences in performance in morphologically richer languages.,7 Discussion,[0],[0]
We leave this research question for future work.,7 Discussion,[0],[0]
This research was partially supported by NSF awards 1462141 and 1254108.,Acknowledgements,[0],[0]
It was also supported in part by the CONACYT grant 134186 and the WIQ-EI IRSES project (grant no. 269180) within the FP 7 Marie Curie.,Acknowledgements,[0],[0]
"Character n-grams have been identified as the most successful feature in both singledomain and cross-domain Authorship Attribution (AA), but the reasons for their discriminative value were not fully understood.",abstractText,[0],[0]
"We identify subgroups of character n-grams that correspond to linguistic aspects commonly claimed to be covered by these features: morphosyntax, thematic content and style.",abstractText,[0],[0]
We evaluate the predictiveness of each of these groups in two AA settings: a single domain setting and a cross-domain setting where multiple topics are present.,abstractText,[0],[0]
We demonstrate that character ngrams that capture information about affixes and punctuation account for almost all of the power of character n-grams as features.,abstractText,[0],[0]
Our study contributes new insights into the use of n-grams for future AA work and other classification tasks.,abstractText,[0],[0]
Not All Character N -grams Are Created Equal: A Study in Authorship Attribution,title,[0],[0]
"Proceedings of the SIGDIAL 2017 Conference, pages 384–394, Saarbrücken, Germany, 15-17 August 2017. c©2017 Association for Computational Linguistics
Neural conversational models require substantial amounts of dialogue data to estimate their parameters and are therefore usually learned on large corpora such as chat forums, Twitter discussions or movie subtitles. These corpora are, however, often challenging to work with, notably due to their frequent lack of turn segmentation and the presence of multiple references external to the dialogue itself. This paper shows that these challenges can be mitigated by adding a weighting model into the neural architecture. The weighting model, which is itself estimated from dialogue data, associates each training example to a numerical weight that reflects its intrinsic quality for dialogue modelling. At training time, these sample weights are included into the empirical loss to be minimised. Evaluation results on retrieval-based models trained on movie and TV subtitles demonstrate that the inclusion of such a weighting model improves the model performance on unsupervised metrics.",text,[0],[0]
"The development of conversational agents (such as mobile assistants, chatbots or interactive robots) is increasingly based on data-driven methods aiming to infer conversational patterns from dialogue data.",1 Introduction,[0],[0]
"One major trend in the last recent years is the emergence of neural conversation models (Vinyals and Le, 2015; Sordoni et al., 2015; Shang et al., 2015; Serban et al., 2016; Lowe et al., 2017; Li et al., 2017).",1 Introduction,[0],[0]
"These neural models can be directly
∗ Also affiliated with Universidad Central del Ecuador (Quito, Ecuador).
",1 Introduction,[0],[0]
"estimated from raw (non-annotated) dialogue corpora, allowing them to be deployed with a limited amount of domain-specific knowledge and feature engineering.
",1 Introduction,[0],[0]
"Due to their large parameter space, the estimation of neural conversation models requires considerable amounts of dialogue data.",1 Introduction,[0],[0]
"They are therefore often trained on conversations collected from various online resources, such as Twitter discussions (Ritter et al., 2010) online chat logs (Lowe et al., 2017), movie scripts (DanescuNiculescu-Mizil and Lee, 2011) and movie and TV subtitles (Lison and Tiedemann, 2016).
",1 Introduction,[0],[0]
"Although these corpora are undeniably useful, they also face some limitations from a dialogue modelling perspective.",1 Introduction,[0],[0]
"First of all, several dialogue corpora, most notably those extracted from subtitles, do not include any explicit turn segmentation or speaker identification (Serban and Pineau, 2015; Lison and Meena, 2016).",1 Introduction,[0],[0]
"In other words, we do not know whether two consecutive sentences are part of the same dialogue turn or were uttered by different speakers.",1 Introduction,[0],[0]
"The neural conversation model may therefore inadvertently learn responses that remain within the same dialogue turn instead of starting a new turn.
",1 Introduction,[0],[0]
"Furthermore, these dialogues contain multiple references to named entities (in particular, person names such as fictional characters) that are specific to the dialogue in question.",1 Introduction,[0],[0]
"These named entities should ideally not be part of the conversation model, since they often draw on an external context that is absent from the inputs provided to the conversation model.",1 Introduction,[0],[0]
"For instance, the mention of character names in a movie is associated with a visual context (for instance, the characters appearing in a given scene) that is not captured in the training data.",1 Introduction,[0],[0]
"Finally, a substantial portion of the utterances observed in these corpora is made of neutral, commonplace responses (“Perhaps”, “I
384
don’t know”, “Err”, ...) that can be used in most conversational situations but fall short of creating meaningful and engaging conversations with human users (Li et al., 2016a).
",1 Introduction,[0],[0]
The present paper addresses these limitations by adding a weighting model to the neural architecture.,1 Introduction,[0],[0]
"The purpose of this model is to associate each 〈context, response〉 example pair to a numerical weight that reflects the intrinsic “quality” of each example.",1 Introduction,[0],[0]
The instance weights are then included in the empirical loss to minimise when learning the parameters of the neural conversation model.,1 Introduction,[0],[0]
The weights are themselves computed via a neural model learned from dialogue data.,1 Introduction,[0],[0]
Experimental results demonstrate that the use of instance weights improves the performance of neural conversation models on unsupervised metrics.,1 Introduction,[0],[0]
"Human evaluation results are, however, inconclusive.
",1 Introduction,[0],[0]
The rest of this paper is as follows.,1 Introduction,[0],[0]
The next section presents a brief overview of existing work on neural conversation models.,1 Introduction,[0],[0]
Section 3 provides a description of the instance weighting approach.,1 Introduction,[0],[0]
"Section 4 details the experimental validation of the proposed model, using both unsupervised metrics and a human evaluation of the selected responses.",1 Introduction,[0],[0]
"Finally, Section 5 discusses the advantages and limitations of the approach, and Section 6 concludes this paper.",1 Introduction,[0],[0]
Neural conversation models are a family of neural architectures (generally based on deep convolutional or recurrent networks) used to represent mappings between dialogue contexts (or queries) and possible responses.,2 Related Work,[0],[0]
"Compared to previous statistical approaches to dialogue modelling based on Markov processes (Levin et al., 2000; Rieser and Lemon, 2011; Young et al., 2013), one benefit of these neural models is their ability to be estimated from raw dialogue corpora, without having to rely on additional annotation layers for intermediate representations such as state variables or dialogue acts.",2 Related Work,[0],[0]
"Rather, neural conversation models automatically derive latent representations of the dialogue state based on the observed utterances.
",2 Related Work,[0],[0]
"Neural conversation models can be divided into two main categories, retrieval models and generative models.",2 Related Work,[0],[0]
"Retrieval models are used to select the most relevant response for a given context amongst a (possibly large) set of predefined responses, such as the set of utterances extracted
from a corpus (Lowe et al., 2015; Prakash et al., 2016).",2 Related Work,[0],[0]
"Generative models, on the other hand, rely on sequence-to-sequence models (Sordoni et al., 2015) to generate new, possibly unseen responses given the provided context.",2 Related Work,[0],[0]
"These models are built by linking together two recurrent architectures: one encoder which maps the sequence of input tokens in the context utterance(s) to a fixedsized vector, and one decoder that generates the response token by token given the context vector (Vinyals and Le, 2015; Sordoni et al., 2015).",2 Related Work,[0],[0]
"Recent papers have shown that the performance of these generative models can be improved by incorporating attentional mechanisms (Yao et al., 2016) and accounting for the structure of conversations through hierarchical networks (Serban et al., 2016).",2 Related Work,[0],[0]
"Neural conversation models can also be learned using adversarial learning (Li et al., 2017).",2 Related Work,[0],[0]
"In this setting, two neural models are jointly learned: a generative model producing the response, and a discriminator optimised to distinguish between human-generated responses and machine-generated ones.",2 Related Work,[0],[0]
"The discriminator outputs are then used to bias the generative model towards producing more human-like responses.
",2 Related Work,[0],[0]
"The linguistic coherence and diversity of the models can be enhanced by including speakeraddressee information (Li et al., 2016b) and by expressing the objective function in terms of Maximum Mutual Information to enhance the diversity of the generated responses (Li et al., 2016a).",2 Related Work,[0],[0]
"As demonstrated by (Ghazvininejad et al., 2017), neural conversation models can also be combined with external knowledge sources in the form of factual information or entity-grounded opinions, which is an important requirement for developing task-oriented dialogue systems that must ground their action in an external context.
",2 Related Work,[0],[0]
"Dialogue is a sequential decision-making process where the conversational actions of each participant influence not only the current turn but the long-term evolution of the dialogue (Levin et al., 2000).",2 Related Work,[0],[0]
"To incorporate the prediction of future outcomes in the generation process, several papers have explored the use of reinforcement learning techniques, using deep neural networks to model the expected future reward (Li et al., 2016c; Cuayáhuitl, 2017).",2 Related Work,[0],[0]
"In particular, the Hybrid Code Networks model of (Williams et al., 2017) demonstrate how a mixture of supervised learning, reinforcement learning and domain-specific knowl-
edge can be used to optimise dialogue strategies from limited amount of training data.
",2 Related Work,[0],[0]
"In contrast with the approaches outlined above, this paper does not present a new neural architecture for conversational models.",2 Related Work,[0],[0]
"Rather, it investigates how the performance of existing models can be improved “upstream”, by adapting how these models can be trained on large, noisy corpora with varying levels of quality.",2 Related Work,[0],[0]
"It should be noted that, although the experiments presented in Section 4 focus on a limited range of neural models, the approach presented in this paper is designed to be model-independent and can be applied as a preprocessing step to any data-driven model of dialogue.",2 Related Work,[0],[0]
"As mentioned in the introduction, the interactions extracted from large dialogue corpora do not all have the same intrinsic quality, due for instance to the frequent lack of turn segmentation or the presence of external, unresolvable references to person names.",3 Approach,[0],[0]
"In other words, there is a discrepancy between the actual 〈context, response〉 pairs found in these corpora and the conversational patterns that should be accounted for in the neural model.
",3 Approach,[0],[0]
"One way to address this discrepancy is by framing the problem as one of domain adaptation, the source domain being the original dialogue corpus and the target domain representing the dialogues we want our model to produce.",3 Approach,[0],[0]
"The target domain is in this case not necessarily another dialogue domain, but simply reflects the fact that the distribution of responses in the raw corpus does not necessarily reflect the distribution of responses we ultimately wish to encode in the conversational model.
",3 Approach,[0.9570784301039157],"['Since the goal is to understand the implicit bias of dropout, we specify the global optimum in terms of the true concept, M. Theorem 2.4.']"
"A popular strategy for domain adaptation in natural language processing, which has notably been used in POS-tagging, sentiment analysis, spam filtering and machine translation (Bickel et al., 2007; Jiang and Zhai, 2007; Foster et al., 2010; Xia et al., 2013), is to assign a higher weight to training instances whose properties are similar to the target domain.",3 Approach,[0],[0]
We present below such an instance weighting approach tailored for neural conversational models.,3 Approach,[0],[0]
"The quality of a particular 〈context, response〉 pair is difficult to determine using handcrafted rules – for instance, the probability of a turn bound-
ary may depend on multiple factors such as the presence of turn-yielding cues or the time gap between the utterances (Lison and Meena, 2016).",3.1 Weighting model,[0.96413525331181],"['Note that while the goal was to minimize the expected squared loss, using dropout with gradient descent amounts to finding a minimum of the objective in equation (2); we argue that the additional term in the objective serves as a regularizer, R(U,V) := λ ∑r i=1 ‖ui‖2‖vi‖2, and is an explicit instantiation of the implicit bias of dropout.']"
"To overcome these limitations, we adopt a datadriven approach and automatically learn a weighting model from examples of “high-quality” responses.",3.1 Weighting model,[0],[0]
"What constitutes a high-quality response depends in practice on the specific criteria we wish to uphold in the conversation model – for instance, favouring responses that are likely to form a new dialogue turn (rather than a continuation of the current turn), avoiding the use of dull, commonplace responses, or disfavouring the selection of responses that contain unresolved references to person names.
",3.1 Weighting model,[0],[0]
"The weighting model can be expressed as a neural model which associates each 〈context, response〉 example pair to a numerical weight.",3.1 Weighting model,[0],[0]
The architecture of this neural network is depicted in Figure 1.,3.1 Weighting model,[0],[0]
"It is composed of two recurrent sub-networks with shared weights, one for the context and one for the response.",3.1 Weighting model,[0],[0]
Each sub-network takes a sequence of tokens as input and pass them through an embedding layer and a recurrent layer with LSTM or GRU cells.,3.1 Weighting model,[0],[0]
"The fixed-size vectors for the context and response are then fed to a regular densely-connected layer, and finally to the final weight value through a sigmoid activation function.",3.1 Weighting model,[0],[0]
"Additional features can also be included whenever available – for instance, timing information for movie and TV subtitles (such as the duration gap between the context and its response, in milliseconds), or document-level features such as the dialogue genre or the total duration of the dialogue.
",3.1 Weighting model,[0],[0]
"To estimate its parameters, the neural model is provided with positive examples of “high-quality” responses along with negative examples sampled at random from the corpus.",3.1 Weighting model,[0],[0]
"Based on this training data, the network learns to assign higher weights to the 〈context, response〉 pairs whose output vectors (combined with the additional inputs) are close from the high-quality examples, and a lower weight for those further away.",3.1 Weighting model,[0],[0]
"In practice, the selection of high-quality example pairs from a given corpus can be performed through a combination of simple heuristics, as detailed in Section 4.1.",3.1 Weighting model,[0],[0]
"Once the weighting model is estimated, the next step is to run it on the entire dia-
logue corpus to compute the expected weight of each 〈context, response〉 pair.",3.2 Instance weighting,[0],[0]
These sample weights are then included in the empirical loss that is being minimised during training.,3.2 Instance weighting,[0],[0]
"Formally, assuming a set of context-response pairs {(c1, r1), (c2, r2), ...(cn, rn)} with associated weights {w1, ...wn}, the estimation of the model parameters θ is expressed as a minimisation problem.",3.2 Instance weighting,[0],[0]
"For retrieval models, this minimisation is expressed as:
θ∗ = minθ n∑ 1 wi L(yi, f(ci, ri; θ)) (1)
where L is a loss function (for instance, the cross-entropy loss), and yi is set to either 1 if ri is the response to ci, and 0 otherwise (when ri is a negative example).",3.2 Instance weighting,[0],[0]
"For generative models, the minimisation is similarly expressed as:
θ∗ = minθ n∑ 1 wi L(ri, f(ci; θ)) (2)
",3.2 Instance weighting,[0],[0]
"In both cases, the loss computed from each example pair is multiplied by the weight value determined by the weight model.",3.2 Instance weighting,[0],[0]
Examples associated with a larger weight wi will therefore have a larger influence on the gradient update steps.,3.2 Instance weighting,[0],[0]
"The approach is evaluated on the basis of retrievalbased neural models trained on English-language subtitles from (Lison and Tiedemann, 2016).",4 Evaluation,[0],[0]
"Three alternative models are evaluated:
1.",4 Evaluation,[0],[0]
"A traditional TF-IDF model,
2.",4 Evaluation,[0],[0]
"A Dual Encoder model trained directly on the corpus examples,
3.",4 Evaluation,[0],[0]
A Dual Encoder model combined with the weighting model from Section 3.1.,4 Evaluation,[0],[0]
"TF-IDF model The TF-IDF (Term Frequency - Inverse Document Frequency) model computes the similarity between the context and its response using methods from information retrieval (Ramos, 2003).",4.1 Models,[0],[0]
TFIDF measures the importance of a word in a “document” (in this case the context or response) relative to the whole corpus.,4.1 Models,[0],[0]
The model transforms the context and response (represented as bag-ofwords) into TF-IDF-weighted vectors.,4.1 Models,[0],[0]
"These vectors are sparse vectors of a size equivalent to the vocabulary size, where each row corresponds, if the given word is present in the context or response, to its TF-IDF weight, and is 0 otherwise.",4.1 Models,[0],[0]
"The matching score between the context and its response is then determined as the cosine similarity between the two vectors:
similarity = vc · vr
‖vc‖2 ‖vr‖2 (3)
where vc and vr respectively denote the TF-IDFweighted vectors for the context and response.
",4.1 Models,[0],[0]
"Dual Encoder The Dual Encoder model (Lowe et al., 2017) consists of two recurrent networks, one for the context and one for the response.",4.1 Models,[0],[0]
"The tokens are first
passed through an embedding layer and then to a recurrent layer with LSTM or GRU cells.",4.1 Models,[0],[0]
"In the original formalisation of this model (Lowe et al., 2015), the context vector is transformed through a dense layer of same dimension, representing the “predicted” response.",4.1 Models,[0],[0]
"The inner product of the predicted and actual responses is then calculated and normalised, yielding a similarity score.",4.1 Models,[0],[0]
"This model, however, only seeks to capture the semantic similarity between the two sequences, while the selection of the most adequate response in a given context may also need to account for other factors such as the grammaticality and coherence of the response.",4.1 Models,[0.9519633272297834],"['Furthermore, we note that this regularizer is closely related to path regularization which is given as the square-root of the sum over all paths, from input to output, of the product of the squared weights along the path (Neyshabur et al., 2015).']"
We therefore extend the Dual Encoder model in two ways.,4.1 Models,[0],[0]
"First, both the context and response vectors are transformed through a dense layer at the end of the recurrent layer (instead of just the context vector).",4.1 Models,[0],[0]
"Second, the final prediction is connected to both the inner product of the two vectors and to the response vector itself, as depicted in Figure 2.
",4.1 Models,[0],[0]
"Dual Encoder with instance weighting
Finally, the third model relies on the exact same Dual Encoder model as above, but applies the weighting model described in Section 3.1 prior to learning in order to assign weights to each training example.",4.1 Models,[0],[0]
The weighting model is estimated on a subset of the movie and TV subtitles augmented with speaker information and filtered through heuristics to ensure a good cohesion between the context and its response.,4.1 Models,[0],[0]
"These heuristics are detailed in the next section.
",4.1 Models,[0],[0]
"Although the architecture of the Dual Encoder
is superficially similar to the weighting model of Figure 1, the two models serve a different purpose: the weighting model returns the expected quality of a training example, while the Dual Encoder returns a score expressing the adequacy between the context and the response.",4.1 Models,[0],[0]
"Training data for the conversation models The dataset used for training the three retrieval models is the English-language portion of the OpenSubtitles corpus of movie and TV subtitles (Lison and Tiedemann, 2016).",4.2 Datasets,[0],[0]
"The full dataset is composed of 105 445 subtitles and 95.5 million utterances, each utterance being associated with a start and end time (in milliseconds).
",4.2 Datasets,[0],[0]
"Training data for the weighting model For training the weighting model, we extracted a small subset of the full corpus of subtitles corresponding to 〈context, response〉 pairs satisfying specific quality criteria.",4.2 Datasets,[0],[0]
"The first step was to align at the sentence level the subtitles with an online collection of movie and TV scripts (1 069 movies and 6 398 TV episodes), following the approach described in (Lison and Meena, 2016).
",4.2 Datasets,[0],[0]
This alignment enabled us to annotate the subtitles with speaker names and turn boundaries.,4.2 Datasets,[0],[0]
"Based on these subtitles, we then selected example pairs with two heuristics:
1.",4.2 Datasets,[0],[0]
"To ensure the response constitutes an actual reply from another speaker and not simply a continuation of the current turn, the
subtitles were segmented into sub-dialogues.",4.2 Datasets,[0],[0]
"〈context, response〉 pairs including a change of speaker from the context to the response were then extracted from these subdialogues.",4.2 Datasets,[0],[0]
"Since multi-party dialogues make it harder to determine who replies to whom, only sub-dialogues with two participants were considered in the subset.
",4.2 Datasets,[0],[0]
2.,4.2 Datasets,[0],[0]
"To ensure the response is intelligible given the context (without drawing on unresolved references to e.g. fictional person names), we also filtered out from the subset the dialogue turns including mentions of fictional character names and out-of-vocabulary words.
",4.2 Datasets,[0],[0]
"A total of 95 624 〈context, response〉 pairs can be extracted using these two heuristics.",4.2 Datasets,[0],[0]
This corresponds to about 0.1 % of the total number of examples for the OpenSubtitles corpus.,4.2 Datasets,[0],[0]
"These pairs are used as positive examples for the weighting model, along with negative pairs sampled at random from the corpus.
Test data Two distinct corpora are used as test sets for the evaluation.",4.2 Datasets,[0],[0]
"The first corpus, whose genre is relatively close to the training set, is the Cornell Movie Dialog Corpus (Danescu-Niculescu-Mizil and Lee, 2011), which is a collection of fictional conversations extracted from movie scripts (unrelated to the ones used for training the weighting model).",4.2 Datasets,[0],[0]
The transcripts from this corpus are segmented into conversations.,4.2 Datasets,[0],[0]
Each conversation is represented as a sequence of dialogue turns.,4.2 Datasets,[0],[0]
"As this paper concentrates on the selection of relevant responses in a given context, we limited the test pairs to the ones where the context ends with a question, which yields a total of 67 305 〈context, response〉 pairs.
",4.2 Datasets,[0],[0]
"The second test set comes from a slightly different conversational genre, namely theatre plays.",4.2 Datasets,[0],[0]
The scripts of 62 English-language theatre plays were downloaded from public websites.,4.2 Datasets,[0],[0]
"We also limited the test pairs to the pairs where the context ends with a question, for a total of 3 427 pairs.",4.2 Datasets,[0],[0]
"The utterances from all datasets were tokenised, lemmatised and POS-tagged using the spaCy NLP library1.",4.2.1 Experimental design Preprocessing,[0],[0]
"We also ran the named entity recogniser
1https://spacy.io/
from the same library to extract named entities.",4.2.1 Experimental design Preprocessing,[0],[0]
"Since the person names mentioned in movies and theatre plays typically refer to fictional characters, we replaced their occurrences by tags, one distinct tag per entity.",4.2.1 Experimental design Preprocessing,[0],[0]
"For instance, the pair:
Dana: Frank, do you think you could give me a hand with these bags?",4.2.1 Experimental design Preprocessing,[0],[0]
"Frank: I’m not a doorman, Miss Barrett.",4.2.1 Experimental design Preprocessing,[0],[0]
"I’m a building superintendent.
is simplified as:
Dana: <person1>, do you think you could give me a hand with these bags?",4.2.1 Experimental design Preprocessing,[0],[0]
"Frank: I’m not a doorman, <person2>.",4.2.1 Experimental design Preprocessing,[0],[0]
"I’m a building superintendent.
",4.2.1 Experimental design Preprocessing,[0],[0]
Named entities of locations and numbers are also replaced by similar tags.,4.2.1 Experimental design Preprocessing,[0],[0]
"To account for the turn structure, turn boundaries were annotated with a <newturn> tag.",4.2.1 Experimental design Preprocessing,[0],[0]
The vocabulary is capped to 25 000 words determined from their frequency in the training corpus.,4.2.1 Experimental design Preprocessing,[0],[0]
"Tokens not covered in this vocabulary are replaced by <unknown>.
",4.2.1 Experimental design Preprocessing,[0],[0]
"Training details
The dialogue contexts were limited to the last 10 utterances preceding the response and a maximum of 60 tokens.",4.2.1 Experimental design Preprocessing,[0],[0]
"The responses were defined as the next dialogue turn after the context, and limited to a maximum of 5 utterances and 30 tokens.
",4.2.1 Experimental design Preprocessing,[0],[0]
The embedding layers of the Dual Encoders were initialised with Skip-gram embeddings trained on the OpenSubtitles corpus.,4.2.1 Experimental design Preprocessing,[0],[0]
"For the recurrent layers, we tested the use of both GRU and LSTM cells, along with their bidirectional equivalents (Chung et al., 2014), without noticeable differences in accuracy.",4.2.1 Experimental design Preprocessing,[0],[0]
"As GRU cells are faster to train than LSTM cells, we opted for the use of GRU-based recurrent layers.",4.2.1 Experimental design Preprocessing,[0],[0]
The dimensionality of the output vectors from the recurrent layers was 400.,4.2.1 Experimental design Preprocessing,[0],[0]
"The neural networks are trained with a batch size of 256, binary cross-entropy as cost function and RMSProp as optimisation algorithm.",4.2.1 Experimental design Preprocessing,[0],[0]
"To avoid overfitting issues, a dropout of 0.2 was applied at all layers of the neural model.
",4.2.1 Experimental design Preprocessing,[0],[0]
"Both the weighting model and the Dual Encoder models were training with a 1:1 ratio between positive examples (actual 〈 context, response 〉 pairs) and negative examples with a response sampled at random from the training set.",4.2.1 Experimental design Preprocessing,[0],[0]
"The three models (the TF-IDF model, the baseline Dual Encoder and the Dual Encoder combined with the weighting model) are evaluated using the Recallm@i metric, which is the most common metric for the evaluation of retrieval-based models.",4.3 Results,[0],[0]
"Let {〈ci, ri〉, 1 ≤ i ≤ n} be the list of m context-response pairs from the test set.",4.3 Results,[0],[0]
"For each context ci, we create a set ofm alternative responses, one response being the actual response ri, and them−1 other responses being sampled at random from the same corpus.",4.3 Results,[0],[0]
"The m alternative responses are then ranked based on the output from the conversational model, and the Recallm@imeasures how often the correct response appears in the top i results of this ranked list.",4.3 Results,[0],[0]
"The Recallm@i metric is often used for the evaluation of retrieval models as several responses may be equally “correct” given a particular context.
",4.3 Results,[0],[0]
The experimental results are shown in Table 1.,4.3 Results,[0],[0]
"As detailed in the table, the Dual Encoder model combined with the weighting model outperforms the Dual Encoder baseline on both test sets (the Cornell Movie Dialogs corpus and the smaller corpus of theatre plays).",4.3 Results,[0],[0]
"Our hypothesis is that the weighting model biases the responses selected by the conversation model towards more cohesive adjacency pairs between context and response2.
",4.3 Results,[0],[0]
"Figure 3 illustrates the learning curve for the two Dual Encoder models, where the accuracy is measured on a validation set composed of the high-quality example pairs described in the previous section along with randomly sampled alternative responses (using a 1:1 ratio of positive vs. negative examples).",4.3 Results,[0.9510561233708754],"['In this paper, we focus on studying the optimization landscape of the objective in equation (2) for a single hiddenlayer linear network with dropout and the special case of an autoencoder with tied weights.']"
"We can observe that the Dual Encoder with instance weights outperforms the baseline model on this validation set – which is not per se a surprising result, since the purpose
2Contrary to the OpenSubtitles corpus which is made of subtitles with no turn segmentation, the Cornell Movie Dialogs corpus and the corpus of theatre plays are derived from scripts and are therefore segmented in dialogue turns.
of the weighting model is precisely to bias the conversation model to give more importance to these types of example pairs.",4.3 Results,[0.9530969607463946],['A single hidden-layer linear network is said to be equalized if the product of the norms of the incoming and outgoing weights are equal for all hidden nodes.']
"To further investigate the potential of this weighting strategy for neural conversational models, we conducted a human evaluation of the responses generated by the two neural models included in the evaluation.",4.4 Human evaluation,[0],[0]
"We collected human judgements on 〈context, response〉 pairs using a crowdsourcing platform.",4.4 Human evaluation,[0],[0]
"We extracted 115 random contexts from the Cornell Movie Dialogs corpus and used four distinct strategies to generate dialogue responses: a random predictor (used to identify the lower bound), the two Dual Encoder models (both without and with instance weights), and expert responses (used to identify the upper bound).",4.4 Human evaluation,[0],[0]
The expert responses were manually authored by two human annotators.,4.4 Human evaluation,[0],[0]
"The resulting 460 〈context, response〉 pairs were evaluated by 8 distinct human judges each (920 ratings per model).",4.4 Human evaluation,[0],[0]
"The human judges were asked to rate the consistency between context and response on a 5-points scale, from Inconsistent to Consistent.",4.4 Human evaluation,[0],[0]
"In total,
118 individuals participated in the crowdsourced evaluation.
",4.4 Human evaluation,[0],[0]
The results of this human evaluation are presented in Figure 4.,4.4 Human evaluation,[0],[0]
"There is unfortunately no statistically significant difference between the baseline Dual Encoder (M = 2.97, SD = 1.27) and the one combined with the weighting model (M = 3.04, SD = 1.27), as established by a Wilcoxon rank-sum test, W (1838) = 410360, p = 0.23.",4.4 Human evaluation,[0],[0]
These inconclusive results are probably due to the very low agreement between the evaluation participants (Krippendorff’s α for continuous variable = 0.36).,4.4 Human evaluation,[0],[0]
The fact that the lower and upper bounds are only separated by 2 standard deviations confirms the difficulty for the raters to discriminate between responses.,4.4 Human evaluation,[0],[0]
"We hypothesise that the nature of the corpus, which is heavily dependent on an external context (the movie scenes), makes it particularly difficult to assess the consistency of the responses.
",4.4 Human evaluation,[0],[0]
Some examples of responses produced by the two Dual Encoder models illustrate the improvements brought by the weighting model.,4.4 Human evaluation,[0],[0]
"In (1), the baseline Dual Encoder selected a turn continuation rather than a reply, while the second model avoids this pitfall.",4.4 Human evaluation,[0],[0]
"Both (1) and (2) also show that the dual encoder with instance weighting tends to select utterances with fewer named entities.
",4.4 Human evaluation,[0],[0]
(1) Context of conversation: – This is General Ripper speaking.,4.4 Human evaluation,[0],[0]
"– Yes, sir. –",4.4 Human evaluation,[0],[0]
Do you recognize my voice?”,4.4 Human evaluation,[0],[0]
"⇒ Response of Dual Encoder: – This is General Nikolas Pherides, Commander of the Third Army.",4.4 Human evaluation,[0],[0]
"I’m Oliver
Davis.",4.4 Human evaluation,[0],[0]
⇒,4.4 Human evaluation,[0],[0]
"Response of Dual Encoder + weighting: – Yes, sir.",4.4 Human evaluation,[0],[0]
"I’m Gideon.
(2) Context of conversation: – Let me finish dinner before you eat it...",4.4 Human evaluation,[0],[0]
Chop the peppers... – Are you all right?,4.4 Human evaluation,[0],[0]
⇒,4.4 Human evaluation,[0],[0]
"Response of Dual Encoder: – No thanks, not hungry.",4.4 Human evaluation,[0],[0]
Harry Dunne. ⇒,4.4 Human evaluation,[0],[0]
Response of Dual Encoder + weighting: –,4.4 Human evaluation,[0],[0]
Yes I’m fine.,4.4 Human evaluation,[0],[0]
Everything is ok.,4.4 Human evaluation,[0],[0]
"The limitations of neural conversational models trained on large, noisy dialogue corpora such as movie and TV subtitles have been discussed in several papers.",5 Discussion,[0],[0]
"Some of the issues raised in previous papers are the absence of turn segmentation in subtitling corpus (Vinyals and Le, 2015; Serban and Pineau, 2015; Lison and Meena, 2016), the lack of long-term consistency and “personality” in the generated responses (Li et al., 2016b), and the ubiquity of dull, commonplace responses when training generative models (Li et al., 2016a).",5 Discussion,[0],[0]
"To the best of our knowledge, this paper is the first to propose an instance weighting approach to address some of these limitations.",5 Discussion,[0],[0]
"One related approach is described in (Zhang et al., 2017) which also relies on domain adaptation for neural response generation, using a combination of online and offline human judgement.",5 Discussion,[0],[0]
"Their focus is, however, on the construction of personalised conversation models and not on instance weighting.
",5 Discussion,[0],[0]
The empirical results corroborate the hypothesis that assigning weights to the training examples of “noisy” dialogue corpora can boost the performance of neural conversation models.,5 Discussion,[0],[0]
"In essence, the proposed approach replaces a one-pass training regime with a two-pass procedure: the first pass to determine the quality of each example pair, and a second pass to update the model based on the observed pair and its associated weight.",5 Discussion,[0],[0]
"We also showed that these weights can be determined in a data-driven manner with a neural model trained on example pairs selected for their adherence to specific quality criteria.
",5 Discussion,[0],[0]
"Instead of this two-pass procedure, an alternative approach is to directly learn a conversation model on the subset of example pairs that are known to be of high-quality.",5 Discussion,[0],[0]
"However, one major shortcoming of this approach is that it consider-
ably limits the size of the training set that can be exploited.",5 Discussion,[0],[0]
"For instance, the data used to estimate the weighting model in Section 4.2 corresponds to a mere 0.1 % of the total English-language part of the OpenSubtitles corpus (since the utterances had to be associated with speaker names derived from aligned scripts in order to apply the heuristics).",5 Discussion,[0],[0]
"In contract, the proposed two-pass procedure can scale to datasets of any size.
",5 Discussion,[0],[0]
The results from Section 4 are limited to retrieval-based models.,5 Discussion,[0],[0]
"One important question for future work is to investigate whether the results carry over to generative, sequence-to-sequence models.",5 Discussion,[0],[0]
"As generative models are more computationally intensive to train than retrieval models, the presented approach may bring another important benefit, namely the ability to filter out part of the training data to concentrate the training time on “interesting” examples with a high cohesion between the context and its response.",5 Discussion,[0],[0]
Dialogue corpora such as chat logs or movie subtitles are very useful resources for developing opendomain conversation models.,6 Conclusion,[0],[0]
"They do, however, also raise a number of challenges for conversation modelling.",6 Conclusion,[0],[0]
"Two notable challenges are the lack of segmentation in dialogue turns (at least for the movie subtitles) and the presence of external context that is not captured in the dialogue transcripts themselves (leading to mentions of person names and unresolvable named entities).
",6 Conclusion,[0],[0]
This paper showed how to mitigate these challenges through the use of a weighting model applied on the training examples.,6 Conclusion,[0],[0]
"This weighting model can be estimated in a data-driven manner, by providing example of “high-quality” training pairs along with random pairs extracted from the same corpus.",6 Conclusion,[0],[0]
The criteria that determine how these training pairs should be selected depend in practice on the type of conversational model one wishes to learn.,6 Conclusion,[0],[0]
"This instance weighting approach can be viewed as a form of domain adaptation, where the data points from the source domain (in this case, the original corpus) are re-weighted to improve the model performance in a target domain (in this case, the interactions in which the conversation model will be deployed).
",6 Conclusion,[0],[0]
Evaluation results on retrieval-based neural models demonstrate the potential of this approach.,6 Conclusion,[0],[0]
"The weighting model is essentially a preprocess-
ing step and can therefore be combined with any type of conversational model.
",6 Conclusion,[0],[0]
Future work will focus on two directions.,6 Conclusion,[0],[0]
"The first is to extend the weighting model to account for other criteria, such as ensuring diversity of responses and coherence across turns.",6 Conclusion,[0],[0]
"The second is to evaluate the approach on other types of neural conversational models, and more particularly on generative models.",6 Conclusion,[0],[0]
"Neural conversational models require substantial amounts of dialogue data to estimate their parameters and are therefore usually learned on large corpora such as chat forums, Twitter discussions or movie subtitles.",abstractText,[0],[0]
"These corpora are, however, often challenging to work with, notably due to their frequent lack of turn segmentation and the presence of multiple references external to the dialogue itself.",abstractText,[0],[0]
This paper shows that these challenges can be mitigated by adding a weighting model into the neural architecture.,abstractText,[0],[0]
"The weighting model, which is itself estimated from dialogue data, associates each training example to a numerical weight that reflects its intrinsic quality for dialogue modelling.",abstractText,[0],[0]
"At training time, these sample weights are included into the empirical loss to be minimised.",abstractText,[0],[0]
Evaluation results on retrieval-based models trained on movie and TV subtitles demonstrate that the inclusion of such a weighting model improves the model performance on unsupervised metrics.,abstractText,[0],[0]
Not All Dialogues are Created Equal: Instance Weighting for Neural Conversational Models,title,[0],[0]
"We propose to mitigate this phenomenon with a principled importance sampling scheme that focuses computation on “informative” examples, and reduces the variance of the stochastic gradients during training. Our contribution is twofold: first, we derive a tractable upper bound to the persample gradient norm, and second we derive an estimator of the variance reduction achieved with importance sampling, which enables us to switch it on when it will result in an actual speedup.
The resulting scheme can be used by changing a few lines of code in a standard SGD procedure, and we demonstrate experimentally, on image classification, CNN fine-tuning, and RNN training, that for a fixed wall-clock time budget, it provides a reduction of the train losses of up to an order of magnitude and a relative improvement of test errors between 5% and 17%.",text,[0],[0]
"The dramatic increase in available training data has made the use of deep neural networks feasible, which in turn has significantly improved the state-of-the-art in many fields, in particular computer vision and natural language processing.",1. Introduction,[0.9506495035605691],"['While dropout has enjoyed tremendous success in training deep neural networks, the theoretical understanding of how dropout (and other algorithmic heuristics) provide regularization in deep learning remains somewhat limited.']"
"However, due to the complexity of the resulting optimization problem, computational cost is now the core issue in training these large architectures.
",1. Introduction,[0],[0]
"When training such models, it appears to any practitioner that not all samples are equally important; many of them are properly handled after a few epochs of training, and most could be ignored at that point without impacting the final
1Idiap Research Institute, Switzerland 2École Polytechique Fédérale de Lausanne, Switzerland.",1. Introduction,[0],[0]
"Correspondence to: Angelos Katharopoulos <firstname.lastname@idiap.ch>.
model.",1. Introduction,[0],[0]
"To this end, we propose a novel importance sampling scheme that accelerates the training of any neural network architecture by focusing the computation on the samples that will introduce the biggest change in the parameters which reduces the variance of the gradient estimates.
",1. Introduction,[0],[0]
"For convex optimization problems, many works (Bordes et al., 2005; Zhao & Zhang, 2015; Needell et al., 2014; Canévet et al., 2016; Richtárik & Takáč, 2013) have taken advantage of the difference in importance among the samples to improve the convergence speed of stochastic optimization methods.",1. Introduction,[0],[0]
"On the other hand, for deep neural networks, sample selection methods were mainly employed to generate hard negative samples for embedding learning problems or to tackle the class imbalance problem (Schroff et al., 2015; Wu et al., 2017; Simo-Serra et al., 2015).
",1. Introduction,[0],[0]
"Recently, researchers have shifted their focus on using importance sampling to improve and accelerate the training of neural networks (Alain et al., 2015; Loshchilov & Hutter, 2015; Schaul et al., 2015).",1. Introduction,[0],[0]
"Those works, employ either the gradient norm or the loss to compute each sample’s importance.",1. Introduction,[0],[0]
"However, the former is prohibitively expensive to compute and the latter is not a particularly good approximation of the gradient norm.
",1. Introduction,[0],[0]
"Compared to the aforementioned works, we derive an upper bound to the per sample gradient norm that can be computed in a single forward pass.",1. Introduction,[0],[0]
This results in reduced computational requirements of more than an order of magnitude compared to Alain et al. (2015).,1. Introduction,[0],[0]
"Furthermore, we quantify the variance reduction achieved with the proposed importance sampling scheme and associate it with the batch size increment required to achieve an equivalent variance reduction.",1. Introduction,[0],[0]
"The benefits of this are twofold, firstly we provide an intuitive metric to predict how useful importance sampling is going to be, thus we are able to decide when to switch on importance sampling during training.",1. Introduction,[0],[0]
"Secondly, we also provide theoretical guarantees for speedup, when variance reduction is above a threshold.",1. Introduction,[0],[0]
"Based on our analysis, we propose a simple to use algorithm that can be used to accelerate the training of any neural network architecture.
",1. Introduction,[0],[0]
"Our implementation is generic and can be employed by adding a single line of code in a standard Keras model
ar X
iv :1
80 3.
00 94
2v 3
[ cs
.L G
] 2
8 O
ct 2
01 9
training.",1. Introduction,[0],[0]
"We validate it on three independent tasks: image classification, fine-tuning and sequence classification with recurrent neural networks.",1. Introduction,[0],[0]
"Compared to existing batch selection schemes, we show that our method consistently achieves lower training loss and test error for equalized wall-clock time.",1. Introduction,[0],[0]
Existing importance sampling methods can be roughly categorized in methods applied to convex problems and methods designed for deep neural networks.,2. Related Work,[0],[0]
Importance sampling for convex optimization problems has been extensively studied over the last years.,2.1. Importance Sampling for Convex Problems,[0],[0]
"Bordes et al. (2005) developed LASVM, which is an online algorithm that uses importance sampling to train kernelized support vector machines.",2.1. Importance Sampling for Convex Problems,[0],[0]
"Later, Richtárik & Takáč (2013) proposed a generalized coordinate descent algorithm that samples coordinate sets in a way that optimizes the algorithm’s convergence rate.
",2.1. Importance Sampling for Convex Problems,[0],[0]
"More recent works (Zhao & Zhang, 2015; Needell et al., 2014) make a clear connection with the variance of the gradient estimates of stochastic gradient descent and show that the optimal sampling distribution is proportional to the per sample gradient norm.",2.1. Importance Sampling for Convex Problems,[0],[0]
"Due to the relatively simple optimization problems that they deal with, the authors resort to sampling proportionally to the norm of the inputs, which in simple linear classification is proportional to the Lipschitz constant of the per sample loss function.
",2.1. Importance Sampling for Convex Problems,[0],[0]
"Such simple importance measures do not exist for Deep Learning and the direct application of the aforementioned theory (Alain et al., 2015), requires clusters of GPU workers just to compute the sampling distribution.",2.1. Importance Sampling for Convex Problems,[0],[0]
Importance sampling has been used in Deep Learning mainly in the form of manually tuned sampling schemes.,2.2. Importance Sampling for Deep Learning,[0],[0]
Bengio et al. (2009) manually design a sampling scheme inspired by the perceived way that human children learn; in practice they provide the network with examples of increasing difficulty in an arbitrary manner.,2.2. Importance Sampling for Deep Learning,[0],[0]
"Diametrically opposite, it is common for deep embedding learning to sample hard examples because of the plethora of easy non informative ones (Simo-Serra et al., 2015; Schroff et al., 2015).
",2.2. Importance Sampling for Deep Learning,[0],[0]
"More closely related to our work, Schaul et al. (2015) and Loshchilov & Hutter (2015) use the loss to create the sampling distribution.",2.2. Importance Sampling for Deep Learning,[0],[0]
"Both approaches keep a history of losses for previously seen samples, and sample either proportionally to the loss or based on the loss ranking.",2.2. Importance Sampling for Deep Learning,[0],[0]
"One of the
main limitations of history based sampling, is the need for tuning a large number of hyperparameters that control the effects of “stale” importance scores; i.e. since the model is constantly updated, the importance of samples fluctuate and previous observations may poorly reflect the current situation.",2.2. Importance Sampling for Deep Learning,[0],[0]
"In particular, Schaul et al. (2015) use various forms of smoothing for the losses and the importance sampling weights, while Loshchilov & Hutter (2015) introduce a large number of hyperparameters that control when the losses are computed, when they are sorted as well as how the sampling distribution is computed based on the rank.
",2.2. Importance Sampling for Deep Learning,[0],[0]
"In comparison to all the above methods, our importance sampling scheme based on an upper bound to the gradient norm has a solid theoretical basis with clear objectives, very easy to choose hyperparameters, theoretically guaranteed speedup and can be applied to any type of network and loss function.",2.2. Importance Sampling for Deep Learning,[0.9556617380402029],"['We argue that a prerequisite for understanding implicit regularization due to various algorithmic heuristics in deep learning, including dropout, is to analyze their behavior in simpler models.']"
"For completeness, we mention the work of Wu et al. (2017), who design a distribution (suitable only for the distance based losses) that maximizes the diversity of the losses in a single batch.",2.3. Other Sample Selection Methods,[0],[0]
"In addition, Fan et al. (2017) use reinforcement learning to train a neural network that selects samples for another neural network in order to optimize the convergence speed.",2.3. Other Sample Selection Methods,[0],[0]
"Although their preliminary results are promising, the overhead of training two networks makes the wall-clock speedup unlikely and their proposal not as appealing.",2.3. Other Sample Selection Methods,[0],[0]
"Finally, a class of algorithms that aim to accelerate the convergence of Stochastic Gradient Descent (SGD) through variance reduction are SVRG type algorithms (Johnson & Zhang, 2013; Defazio et al., 2014; Allen-Zhu, 2017; Lei et al., 2017).",2.4. Stochastic Variance Reduced Gradient,[0],[0]
"Although asymptotically better, those algorithms typically perform worse than plain SGD with momentum for the low accuracy optimization setting of Deep Learning.",2.4. Stochastic Variance Reduced Gradient,[0],[0]
"Contrary to the aforementioned algorithms, our proposed importance sampling does not improve the asymptotic convergence of SGD but results in pragmatic improvements in all the metrics given a fixed time budget.",2.4. Stochastic Variance Reduced Gradient,[0],[0]
Importance sampling aims at increasing the convergence speed of SGD by focusing computation on samples that actually induce a change in the model parameters.,3. Variance Reduction for Deep Neural Networks,[0],[0]
This formally translates into a reduced variance of the gradient estimates for a fixed computational cost.,3. Variance Reduction for Deep Neural Networks,[0],[0]
"In the following sections, we analyze how this works and present an efficient algorithm that can be used to train any Deep Learning model.",3. Variance Reduction for Deep Neural Networks,[0],[0]
"Let xi, yi be the i-th input-output pair from the training set, Ψ(·; θ) be a Deep Learning model parameterized by the vector θ, and L(·, ·) be the loss function to be minimized during training.",3.1. Introduction to Importance Sampling,[0],[0]
"The goal of training is to find
θ∗ = arg min θ
1
N N∑ i=1",3.1. Introduction to Importance Sampling,[0],[0]
"L(Ψ(xi; θ), yi) (1)
where N corresponds to the number of examples in the training set.
",3.1. Introduction to Importance Sampling,[0],[0]
"We use an SGD procedure with learning rate η, where the update at iteration t depends on the sampling distribution pt1, . . .",3.1. Introduction to Importance Sampling,[0],[0]
", p t N and re-scaling coefficients w t 1, . . .",3.1. Introduction to Importance Sampling,[0],[0]
", w t N .",3.1. Introduction to Importance Sampling,[0],[0]
"Let It be the data point sampled at that step, we have P (It = i) =",3.1. Introduction to Importance Sampling,[0],[0]
"pti and
θt+1 = θt − ηwIt∇θtL(Ψ(xIt ; θt), yIt) (2)
Plain SGD with uniform sampling is achieved with wti = 1 and pti = 1 N for all t and i.
If we define the convergence speed S of SGD as the reduction of the distance of the parameter vector θ from the optimal parameter vector θ∗ in two consecutive iterations t and t+ 1
S = −EPt",3.1. Introduction to Importance Sampling,[0],[0]
[ ‖θt+1 − θ∗‖22,3.1. Introduction to Importance Sampling,[0],[0]
"− ‖θt − θ ∗‖22 ] , (3)
and if we have wi = 1Npi such that
EPt [wIt∇θtL(Ψ(xIt ; θt), yIt)]",3.1. Introduction to Importance Sampling,[0],[0]
(4) = ∇θt,3.1. Introduction to Importance Sampling,[0],[0]
1N ∑N i=1,3.1. Introduction to Importance Sampling,[0],[0]
"L(Ψ(xi; θt), yi), (5)
and set Gi = wi∇θtL(Ψ(xi; θt), yi), then we get (this is a different derivation of the result by Wang et al., 2016)
",3.1. Introduction to Importance Sampling,[0],[0]
S = −EPt,3.1. Introduction to Importance Sampling,[0],[0]
"[ (θt+1−θ∗)T (θt+1−θ∗)− (θt−θ∗)T (θt−θ∗) ] = −EPt [ θTt+1θt+1−2θt+1θ∗ − θTt θt + 2θtθ∗
] = −EPt [ (θt−ηGIt) T (θt−ηGIt) + 2ηGTItθ ∗−θTt",3.1. Introduction to Importance Sampling,[0],[0]
"θt ]
= −EPt [ −2η (θt−θ∗)GIt + η2GTItGIt ] = 2η (θt−θ∗)EPt [GIt ]− η2 EPt [GIt ]
TEPt [GIt ]− η2Tr (VPt [GIt ])
(6)
Since the first two terms, in the last expression, are the speed of batch gradient descent, we observe that it is possible to gain a speedup by sampling from the distribution that minimizes Tr (VPt [GIt ]).",3.1. Introduction to Importance Sampling,[0],[0]
"Several works (Needell et al., 2014; Zhao & Zhang, 2015; Alain et al., 2015) have shown the optimal distribution to be proportional to the per-sample gradient norm.",3.1. Introduction to Importance Sampling,[0],[0]
"However, computing this distribution is computationally prohibitive.",3.1. Introduction to Importance Sampling,[0],[0]
"Given an upper bound Ĝi ≥ ‖∇θtL(Ψ(xi; θt), yi)‖2 and due to
arg min P Tr (VPt [GIt ]) = arg min P
EPt [ ‖GIt‖ 2 2 ] , (7)
we propose to relax the optimization problem in the following way
min P
EPt [ ‖GIt‖ 2 2 ] ≤",3.2. Beyond the Full Gradient Norm,[0],[0]
min P EPt,3.2. Beyond the Full Gradient Norm,[0],[0]
[ w2ItĜ 2 It ] .,3.2. Beyond the Full Gradient Norm,[0],[0]
"(8)
The minimizer of the second term of equation 8, similar to the first term, is pi ∝ Ĝi.",3.2. Beyond the Full Gradient Norm,[0],[0]
"All that remains, is to find a proper expression for Ĝi which is significantly easier to compute than the norm of the gradient for each sample.
",3.2. Beyond the Full Gradient Norm,[0],[0]
"In order to continue with the derivation of our upper bound Ĝi, let us introduce some notation specific to a multi-layer perceptron.",3.2. Beyond the Full Gradient Norm,[0],[0]
Let θ(l) ∈ RMl×Ml−1 be the weight matrix for layer l and σ(l)(·) be a Lipschitz continuous activation function.,3.2. Beyond the Full Gradient Norm,[0],[0]
"Then, let
x(0) = x (9)
z(l) = θ(l) x(l−1)",3.2. Beyond the Full Gradient Norm,[0],[0]
"(10)
x(l) = σ(l)(z(l)) (11)
Ψ(x; Θ) = x(L) (12)
",3.2. Beyond the Full Gradient Norm,[0],[0]
"Although our notation describes simple fully connected neural networks without bias, our analysis holds for any affine operation followed by a slope-bounded non-linearity (|σ′(x)| ≤ K).",3.2. Beyond the Full Gradient Norm,[0],[0]
"With
Σ′l(z) = diag ( σ′(l)(z1), . . .",3.2. Beyond the Full Gradient Norm,[0],[0]
", σ ′(l)(zMl) ) , (13)
∆",3.2. Beyond the Full Gradient Norm,[0],[0]
(l) i = Σ ′ l(z (l) i )θ T l+1 . .,3.2. Beyond the Full Gradient Norm,[0],[0]
.Σ ′,3.2. Beyond the Full Gradient Norm,[0],[0]
"L−1(z (L−1) i )θ T L , (14)
∇",3.2. Beyond the Full Gradient Norm,[0],[0]
x (L) i L = ∇,3.2. Beyond the Full Gradient Norm,[0],[0]
x,3.2. Beyond the Full Gradient Norm,[0],[0]
"(L) i L(Ψ(xi; Θ), yi) (15)
we get
‖∇θlL(Ψ(xi; Θ), yi)‖2 (16)
",3.2. Beyond the Full Gradient Norm,[0],[0]
= ∥∥∥∥(∆(l)i Σ′L(z(L)i ),3.2. Beyond the Full Gradient Norm,[0],[0]
∇x(L)i L)(x(l−1)i ),3.2. Beyond the Full Gradient Norm,[0],[0]
"T ∥∥∥∥
2
(17)
≤",3.2. Beyond the Full Gradient Norm,[0],[0]
"∥∥∥∆(l)i ∥∥∥
2 ∥∥∥Σ′L(z(L)i )∇x(L)i L∥∥∥2 ∥∥∥x(l−1)i ∥∥∥2 (18) ≤",3.2. Beyond the Full Gradient Norm,[0],[0]
"max
l,i (∥∥∥x(l−1)i ∥∥∥ 2 ∥∥∥∆(l)i ∥∥∥ 2 ) ︸",3.2. Beyond the Full Gradient Norm,[0],[0]
"︷︷ ︸
ρ
∥∥∥Σ′L(z(L)i )",3.2. Beyond the Full Gradient Norm,[0],[0]
"∇x(L)i L∥∥∥2(19)
Various weight initialization (Glorot & Bengio, 2010) and activation normalization techniques (Ioffe & Szegedy, 2015; Ba et al., 2016) uniformise the activations across samples.",3.2. Beyond the Full Gradient Norm,[0],[0]
"As a result, the variation of the gradient norm is mostly captured by the gradient of the loss function with respect
to the pre-activation outputs of the last layer of our neural network.",3.2. Beyond the Full Gradient Norm,[0],[0]
"Consequently we can derive the following upper bound to the gradient norm of all the parameters
‖∇ΘL(Ψ(xi; Θ), yi)‖2 ≤",3.2. Beyond the Full Gradient Norm,[0],[0]
"Lρ ∥∥∥Σ′L(z(L)i )∇x(L)i L∥∥∥2︸ ︷︷ ︸
Ĝi
,
(20)
which is marginally more difficult to compute than the value of the loss since it can be computed in a closed form in terms of z(L).",3.2. Beyond the Full Gradient Norm,[0],[0]
"However, our upper bound depends on the time step t, thus we cannot generate a distribution once and sample from it during training.",3.2. Beyond the Full Gradient Norm,[0],[0]
This is intuitive because the importance of each sample changes as the model changes.,3.2. Beyond the Full Gradient Norm,[0],[0]
Computing the importance score from equation 20 is more than an order of magnitude faster compared to computing the gradient norm for each sample.,3.3. When is Variance Reduction Possible?,[0],[0]
"Nevertheless, it still costs one forward pass through the network and can be wasteful.",3.3. When is Variance Reduction Possible?,[0],[0]
"For instance, during the first iterations of training, the gradients with respect to every sample have approximately equal norm; thus we would waste computational resources trying to sample from the uniform distribution.",3.3. When is Variance Reduction Possible?,[0],[0]
"In addition, computing the importance score for the whole dataset is still prohibitive and would render the method unsuitable for online learning.
",3.3. When is Variance Reduction Possible?,[0],[0]
"In order to solve the problem of computing the importance for the whole dataset, we pre-sample a large batch of data points, compute the sampling distribution for that batch and re-sample a smaller batch with replacement.",3.3. When is Variance Reduction Possible?,[0],[0]
The above procedure upper bounds both the speedup and variance reduction.,3.3. When is Variance Reduction Possible?,[0],[0]
"Given a large batch consisting of B samples and a small one consisting of b, we can achieve a maximum variance reduction of 1b − 1 B and a maximum speedup of B+3b 3B assuming that the backward pass requires twice the amount of time as the forward pass.
",3.3. When is Variance Reduction Possible?,[0],[0]
"Due to the large cost of computing the importance per sample, we only perform importance sampling when we know that the variance of the gradients can be reduced.",3.3. When is Variance Reduction Possible?,[0],[0]
"In the following equation, we show that the variance reduction is proportional to the squared L2 distance of the sampling distribution, g, to the uniform distribution u. Due to lack of space, the complete derivation is included in the supplementary material.",3.3. When is Variance Reduction Possible?,[0],[0]
Let gi ∝,3.3. When is Variance Reduction Possible?,[0],[0]
"‖∇θtL(Ψ(xi; θt), yi)‖2 =",3.3. When is Variance Reduction Possible?,[0],[0]
‖Gi‖2,3.3. When is Variance Reduction Possible?,[0],[0]
and u = 1B,3.3. When is Variance Reduction Possible?,[0],[0]
"the uniform probability.
",3.3. When is Variance Reduction Possible?,[0],[0]
Tr (Vu[Gi])− Tr (Vg[wiGi]) (21) =,3.3. When is Variance Reduction Possible?,[0],[0]
Eu [ ‖Gi‖22 ],3.3. When is Variance Reduction Possible?,[0],[0]
"− Eg [ w2i ‖Gi‖ 2 2 ] (22)
=
( 1
B B∑ i=1",3.3. When is Variance Reduction Possible?,[0],[0]
‖Gi‖2 ),3.3. When is Variance Reduction Possible?,[0],[0]
2 B ‖g,3.3. When is Variance Reduction Possible?,[0],[0]
− u‖22 .,3.3. When is Variance Reduction Possible?,[0],[0]
"(23)
Equation 23 already provides us with a useful metric to decide if the variance reduction is significant enough to justify using importance sampling.",3.3. When is Variance Reduction Possible?,[0],[0]
"However, choosing a suitable threshold for the L2 distance squared would be tedious and unintuitive.",3.3. When is Variance Reduction Possible?,[0],[0]
We can do much better by dividing the variance reduction with the original variance to derive the increase in the batch size that would achieve an equivalent variance reduction.,3.3. When is Variance Reduction Possible?,[0],[0]
"Assuming that we increase the batch size by τ , we achieve variance reduction 1τ ; thus we have 1
( 1 B ∑B i=1",3.3. When is Variance Reduction Possible?,[0],[0]
‖Gi‖2 ),3.3. When is Variance Reduction Possible?,[0],[0]
2 B ‖g,3.3. When is Variance Reduction Possible?,[0],[0]
"− u‖22 Tr (Vu[Gi]) ≥ (24)(
1 B ∑B i=1 ‖Gi‖2 )",3.3. When is Variance Reduction Possible?,[0],[0]
2 B ‖g,3.3. When is Variance Reduction Possible?,[0],[0]
"− u‖22
1 B ∑B i=1 ‖Gi‖",3.3. When is Variance Reduction Possible?,[0],[0]
"2 2
=",3.3. When is Variance Reduction Possible?,[0],[0]
"(25)
1∑B i=1",3.3. When is Variance Reduction Possible?,[0],[0]
g 2,3.3. When is Variance Reduction Possible?,[0],[0]
i ‖g,3.3. When is Variance Reduction Possible?,[0],[0]
"− u‖22 = 1− 1 τ ⇐⇒ (26) 1
τ = 1− 1∑B
i=1",3.3. When is Variance Reduction Possible?,[0],[0]
"g 2 i
‖g",3.3. When is Variance Reduction Possible?,[0],[0]
"− u‖22 (27)
Using equation 27, we have a hyperparameter that is very easy to select and can now design our training procedure which is described in pseudocode in algorithm 1.",3.3. When is Variance Reduction Possible?,[0],[0]
Computing τ from equation 27 allows us to have guaranteed speedup when B + 3b < 3τb.,3.3. When is Variance Reduction Possible?,[0],[0]
"However, as it is shown in the experiments, we can use τth smaller than B+3b3b and still get a significant speedup.
",3.3. When is Variance Reduction Possible?,[0],[0]
"Algorithm 1 Deep Learning with Importance Sampling 1: Inputs B, b, τth, aτ , θ0 2: t← 1 3: τ ← 0 4: repeat 5: if τ > τth then 6: U ← B uniformly sampled datapoints 7: gi ∝",3.3. When is Variance Reduction Possible?,[0],[0]
Ĝi ∀i ∈ U according to eq 20 8: G ← b datapoints sampled with gi from U 9: wi ← 1Bgi,3.3. When is Variance Reduction Possible?,[0],[0]
"∀i ∈ G 10: θt ← sgd step(wi,G, θt−1) 11: else 12: U ← b uniformly sampled datapoints 13: wi ← 1 ∀i ∈ U 14: θt ← sgd step(wi,U , θt−1) 15: gi ∝",3.3. When is Variance Reduction Possible?,[0],[0]
Ĝi,3.3. When is Variance Reduction Possible?,[0],[0]
"∀i ∈ U 16: end if
17: τ ← aττ +",3.3. When is Variance Reduction Possible?,[0],[0]
(,3.3. When is Variance Reduction Possible?,[0],[0]
"1− aτ ) (
1− 1∑ i g 2 i ∥∥∥g − 1|U|∥∥∥2 2 )−1 18: until convergence
1In the first version we mistakenly assume 1 τ2 which made the algorithm unnecessarily conservative.",3.3. When is Variance Reduction Possible?,[0],[0]
"All the experiments are run using the square root of line 17 in Algorithm 1.
",3.3. When is Variance Reduction Possible?,[0],[0]
"The inputs to the algorithm are the pre-sampling size B, the batch size b, the equivalent batch size increment after which we start importance sampling τth and the exponential moving average parameter aτ used to compute a smooth estimate of τ .",3.3. When is Variance Reduction Possible?,[0],[0]
θ0 denotes the initial parameters of our deep network.,3.3. When is Variance Reduction Possible?,[0],[0]
"We would like to point out that in line 15 of the algorithm, we compute gi for free since we have done the forward pass in the previous step.
",3.3. When is Variance Reduction Possible?,[0],[0]
The only parameter that has to be explicitly defined for our algorithm is the pre-sampling size B because τth can be set using equation 27.,3.3. When is Variance Reduction Possible?,[0],[0]
We provide a small ablation study for B in the supplementary material.,3.3. When is Variance Reduction Possible?,[0],[0]
"In this section, we analyse experimentally the performance of the proposed importance sampling scheme based on our upper-bound of the gradient norm.",4. Experiments,[0],[0]
"In the first subsection, we compare the variance reduction achieved with our upper bound to the theoretically maximum achieved with the true gradient norm.",4. Experiments,[0],[0]
"We also compare against sampling based on the loss, which is commonly used in practice.",4. Experiments,[0],[0]
"Subsequently, we conduct experiments which demonstrate that we are able to achieve non-negligible wall-clock speedup for a variety of tasks using our importance sampling scheme.
",4. Experiments,[0],[0]
"In all the subsequent sections, we use uniform to refer to the usual training algorithm that samples points from a uniform distribution, we use loss to refer to algorithm 1 but instead of sampling from a distribution proportional to our upperbound to the gradient norm Ĝi (equations 8 and 20), we sample from a distribution proportional to the loss value and finally upper-bound to refer to our proposed method.",4. Experiments,[0],[0]
"All the other baselines from published methods are referred to using the names of the authors.
",4. Experiments,[0],[0]
"In addition to batch selection methods, we compare with various SVRG implementations including the accelerated Katyusha (Allen-Zhu, 2017) and the online SCSG (Lei et al., 2017) method.",4. Experiments,[0],[0]
"In all cases, SGD with uniform sampling performs significantly better.",4. Experiments,[0],[0]
"Due to lack of space, we report the detailed results in the supplementary material.
",4. Experiments,[0],[0]
"Experiments were conducted using Keras (Chollet et al., 2015) with TensorFlow (Abadi et al., 2016), and the code can be found at http://github.com/idiap/ importance-sampling.",4. Experiments,[0],[0]
"For all the experiments, we use Nvidia K80 GPUs and the reported time is calculated by subtracting the timestamps before starting one epoch and after finishing one; thus it includes the time needed to transfer data between CPU and GPU memory.
",4. Experiments,[0],[0]
Our implementation provides a wrapper around models that substitutes the standard uniform sampling with our importance-sampling method.,4. Experiments,[0],[0]
"This means that adding a sin-
gle line of code to call this wrapper before actually fitting the model is sufficient to switch from the standard uniform sampling to our importance-sampling scheme.",4. Experiments,[0],[0]
"And, as specified in § 3.3 and Algorithm 1, our procedure reliably estimates at every iteration if the importance sampling will provide a speed-up and sticks to uniform sampling otherwise.",4. Experiments,[0],[0]
"As already mentioned, several works (Loshchilov & Hutter, 2015; Schaul et al., 2015) use the loss value, directly or indirectly, to generate sampling distributions.",4.1. Ablation study,[0],[0]
"In this section, we present experiments that validate the superiority of our method with respect to the loss in terms of variance reduction.",4.1. Ablation study,[0],[0]
"For completeness, in the supplementary material we include a theoretical analysis that explains why sampling based on the loss also achieves variance reduction during
the late stages of training.
",4.1. Ablation study,[0],[0]
"Our experimental setup is as follows: we train a wide residual network (Zagoruyko & Komodakis, 2016) on the CIFAR100 dataset (Krizhevsky, 2009), following closely the training procedure of Zagoruyko & Komodakis (2016) (the details are presented in § 4.2).",4.1. Ablation study,[0],[0]
"Subsequently, we sample 1, 024 images uniformly at random from the dataset.",4.1. Ablation study,[0],[0]
"Using the weights of the trained network, at intervals of 3, 000 updates, we resample 128 images from the large batch of 1, 024 images using uniform sampling or importance sampling with probabilities proportional to the loss, our upper-bound or the gradient-norm.",4.1. Ablation study,[0],[0]
"The gradient-norm is computed by running the backpropagation algorithm with a batch size of 1.
",4.1. Ablation study,[0],[0]
Figure 1 depicts the variance reduction achieved with every sampling scheme in comparison to uniform.,4.1. Ablation study,[0],[0]
"We measure this directly as the distance between the mini-batch gradient and the batch gradient of the 1, 024 samples.",4.1. Ablation study,[0],[0]
For robustness we perform the sampling 10 times and report the average.,4.1. Ablation study,[0],[0]
"We observe that our upper bound and the gradient norm result in very similar variance reduction, meaning that the bound is relatively tight and that the produced probability distributions are highly correlated.",4.1. Ablation study,[0],[0]
"This can also be deduced by observing figure 2, where the probabilities proportional to the loss and the upper-bound are plotted against the optimal ones (proportional to the gradient-norm).",4.1. Ablation study,[0],[0]
"We observe that our upper bound is almost perfectly correlated with the gradient norm, in stark contrast to the loss which is only correlated at the regime of very small gradients.",4.1. Ablation study,[0],[0]
"Quantitatively the sum of squared error of 16, 384 points in figure 2 is 0.017 for the loss and 0.002 for our proposed upper bound.
",4.1. Ablation study,[0],[0]
"Furthermore, we observe that sampling hard examples (with high loss), increases the variance, especially in the beginning of training.",4.1. Ablation study,[0],[0]
"Similar behaviour has been observed in problems such as embedding learning where semi-hard sample mining is preferred over sampling using the loss (Wu et al., 2017; Schroff et al., 2015).",4.1. Ablation study,[0],[0]
"In this section, we use importance sampling to train a residual network on CIFAR10 and CIFAR100.",4.2. Image classification,[0],[0]
"We follow the experimental setup of Zagoruyko & Komodakis (2016), specifically we train a wide resnet 28-2 with SGD with momentum.",4.2. Image classification,[0],[0]
"We use batch size 128, weight decay 0.0005, momentum 0.9, initial learning rate 0.1 divided by 5 after 20, 000 and 40, 000 parameter updates.",4.2. Image classification,[0],[0]
"Finally, we train for a total of 50, 000 iterations.",4.2. Image classification,[0],[0]
"In order for our history based baselines to be compatible with the data augmentation of the CIFAR images, we pre-augment both datasets to generate 1.5 × 106 images for each one.",4.2. Image classification,[0],[0]
Our method does not have this limitation since it can work on infinite datasets in a true online fashion.,4.2. Image classification,[0],[0]
"To compare between methods, we
use a learning rate schedule based on wall-clock time and we also fix the total seconds available for training.",4.2. Image classification,[0],[0]
"A faster method should have smaller training loss and test error given a specific time during training.
",4.2. Image classification,[0],[0]
"For this experiment, we compare the proposed method to uniform, loss, online batch selection by Loshchilov & Hutter (2015) and the history based sampling of Schaul et al. (2015).",4.2. Image classification,[0],[0]
"For the method of Schaul et al. (2015), we use their proportional sampling since the rank based is very similar to Loshchilov & Hutter (2015) and we select the best parameters from the grid a = {0.1, 0.5, 1.0} and β = {0.5, 1.0}.",4.2. Image classification,[0],[0]
"Similarly, for online batch selection, we use s = {1, 10, 102} and a recomputation of all the losses every r = {600, 1200, 3600} updates.
",4.2. Image classification,[0],[0]
"For our method, we use a presampling size of 640.",4.2. Image classification,[0],[0]
One of the goals of this experiment is to show that even a smaller reduction in variance can effectively stabilize training and provide wall-clock time speedup; thus we set τth = 1.5.,4.2. Image classification,[0],[0]
"We perform 3 independent runs and report the average.
",4.2. Image classification,[0],[0]
The results are depicted in figure 3.,4.2. Image classification,[0],[0]
"We observe that in the relatively easy CIFAR10 dataset, all methods can provide some speedup over uniform sampling.",4.2. Image classification,[0],[0]
"However, for the more complicated CIFAR100, only sampling with our proposed upper-bound to the gradient norm reduces the variance of the gradients and provides faster convergence.",4.2. Image classification,[0],[0]
"Examining the training evolution in detail, we observe that on CIFAR10 our method is the only one that achieves a significant improvement in the test error even in the first stages of training (4, 000 to 8, 000 seconds).",4.2. Image classification,[0],[0]
"Quantitatively, on CIFAR10 we achieve more than an order of magnitude lower training loss and 8% lower test error from 0.087 to 0.079 while on CIFAR100 approximately 3 times lower training loss and 5% lower test error from 0.34 to 0.32 compared to uniform sampling.
",4.2. Image classification,[0],[0]
"At this point, we would also like to discuss the performance of the loss compared to other methods that also select batches based on this metric.",4.2. Image classification,[0],[0]
"Our experiments show, that using “fresh” values for the loss combined with a warmup stage so that importance sampling is not started too early outperforms all the other baselines on the CIFAR10 dataset.",4.2. Image classification,[0],[0]
Our second experiment shows the application of importance sampling to the significant task of fine tuning a pre-trained large neural network on a new dataset.,4.3. Fine-tuning,[0],[0]
"This task is of particular importance because there exists an abundance of powerful models pre-trained on large datasets such as ImageNet (Deng et al., 2009).
",4.3. Fine-tuning,[0],[0]
"Our experimental setup is the following, we fine-tune a ResNet-50 (He et al., 2015) previously trained on ImageNet.",4.3. Fine-tuning,[0],[0]
"We replace the last classification layer and then train the
whole network end-to-end to classify indoor images among 67 possible categories (Quattoni & Torralba, 2009).",4.3. Fine-tuning,[0],[0]
We use SGD with learning rate 10−3 and momentum 0.9.,4.3. Fine-tuning,[0],[0]
We set the batch size to 16 and for our importance sampling algorithm we pre-sample 48.,4.3. Fine-tuning,[0],[0]
"The variance reduction threshold is set to 2 as designated by equation 27.
",4.3. Fine-tuning,[0],[0]
"To assess the performance of both our algorithm and our gradient norm approximation, we compare the convergence speed of our importance sampling algorithm using our upper-bound and using the loss.",4.3. Fine-tuning,[0],[0]
"Once again, for robustness, we run 3 independent runs and report the average.
",4.3. Fine-tuning,[0],[0]
The results of the experiment are depicted in figure 4.,4.3. Fine-tuning,[0],[0]
"As expected, importance sampling is very useful for the task of fine-tuning since a lot of samples are handled correctly very early in the training process.",4.3. Fine-tuning,[0],[0]
"Our upper-bound, once again, greatly outperforms sampling proportionally to the loss when the network is large and the problem is non trivial.",4.3. Fine-tuning,[0],[0]
"Compared to uniform sampling, in just half an hour importance sampling has converged close to the best performance (28.06% test error) that can be expected on this dataset without any data augmentation or multiple crops (Razavian et al.,
2014), while uniform achieves only 33.74%.",4.3. Fine-tuning,[0],[0]
"To showcase the generality of our method, we use our importance sampling algorithm to accelerate the training of an LSTM in a sequence classification problem.",4.4. Pixel by Pixel MNIST,[0],[0]
"We use the pixel by pixel classification of randomly permuted MNIST digits (LeCun et al., 2010), as defined by Le et al. (2015).",4.4. Pixel by Pixel MNIST,[0],[0]
"The problem may seem trivial at first, however as shown by Le et al. (2015)",4.4. Pixel by Pixel MNIST,[0],[0]
"it is particularly suited to benchmarking the training of recurrent neural networks, due to the long range dependency problems inherent in the dataset (784 time steps).
",4.4. Pixel by Pixel MNIST,[0],[0]
"For our experiment, we fix a permutation matrix for all the pixels to generate a training set of 60, 000 samples with 784 time steps each.",4.4. Pixel by Pixel MNIST,[0],[0]
"Subsequently, we train an LSTM (Hochreiter & Schmidhuber, 1997) with 128 dimensions in the hidden space, tanh(·) as an activation function and sigmoid(·) as the recurrent activation function.",4.4. Pixel by Pixel MNIST,[0],[0]
"Finally, we use a linear classifier on top of the LSTM to choose a digit based on the hidden representation.",4.4. Pixel by Pixel MNIST,[0],[0]
"To train the aforemen-
tioned architecture, we use the Adam optimizer (Kingma & Ba, 2014) with a learning rate of 10−3 and a batch size of 32.",4.4. Pixel by Pixel MNIST,[0],[0]
"We have also found gradient clipping to be necessary for the training not to diverge; thus we clip the norm of all gradients to 1.
",4.4. Pixel by Pixel MNIST,[0],[0]
The results of the experiment are depicted in figure 5.,4.4. Pixel by Pixel MNIST,[0],[0]
"Both for the loss and our proposed upper-bound, importance sampling starts at around 2, 000 seconds by setting τth = 1.8 and the presampling size to 128.",4.4. Pixel by Pixel MNIST,[0],[0]
We could set τth = 2.33 (equation 27) which would only result in our algorithm being more conservative and starting importance sampling later.,4.4. Pixel by Pixel MNIST,[0],[0]
We clearly observe that sampling proportionally to the loss hurts the convergence in this case.,4.4. Pixel by Pixel MNIST,[0],[0]
"On the other hand, our algorithm achieves 20% lower training loss and 7% lower test error in the given time budget.",4.4. Pixel by Pixel MNIST,[0],[0]
We have presented an efficient algorithm for accelerating the training of deep neural networks using importance sampling.,5. Conclusions,[0],[0]
"Our algorithm takes advantage of a novel upper bound to the
gradient norm of any neural network that can be computed in a single forward pass.",5. Conclusions,[0],[0]
"In addition, we show an equivalence of the variance reduction with importance sampling to increasing the batch size; thus we are able to quantify both the variance reduction and the speedup and intelligently decide when to stop sampling uniformly.
",5. Conclusions,[0],[0]
Our experiments show that our algorithm is effective in reducing the training time for several tasks both on image and sequence data.,5. Conclusions,[0],[0]
"More importantly, we show that not all data points matter equally in the duration of training, which can be exploited to gain a speedup or better quality gradients or both.
",5. Conclusions,[0],[0]
Our analysis opens several avenues of future research.,5. Conclusions,[0],[0]
The two most important ones that were not investigated in this work are automatically tuning the learning rate based on the variance of the gradients and decreasing the batch size.,5. Conclusions,[0],[0]
The variance of the gradients can be kept stable by increasing the learning rate proportionally to the batch increment or by decreasing the number of samples for which we compute the backward pass.,5. Conclusions,[0],[0]
"Thus, we can speed up convergence by increasing the step size or reducing the time per update.",5. Conclusions,[0],[0]
This work is supported by the Swiss National Science Foundation under grant number FNS-30209 “ISUL”.,6. Acknowledgement,[0],[0]
In the following equations we quantify the variance reduction achieved with importance sampling using the gradient norm.,A. Differences of variances,[0],[0]
Let gi ∝,A. Differences of variances,[0],[0]
"‖∇θtL(Ψ(xi; θt), yi)‖2 =",A. Differences of variances,[0],[0]
‖Gi‖2,A. Differences of variances,[0],[0]
and u = 1B,A. Differences of variances,[0],[0]
"the uniform probability.
",A. Differences of variances,[0],[0]
"We want to compute
Tr (Vu[Gi])− Tr (Vg[wiGi])",A. Differences of variances,[0],[0]
= Eu [ ‖Gi‖22 ] − Eg [ w2i ‖Gi‖ 2 2 ] .,A. Differences of variances,[0],[0]
"(28)
Using the fact that wi = 1Bgi we have
Eg [ w2i ‖Gi‖ 2 2 ] =
( 1
B B∑ i=1",A. Differences of variances,[0],[0]
"‖Gi‖2
)",A. Differences of variances,[0],[0]
"2 , (29)
thus
",A. Differences of variances,[0],[0]
"Tr (Vu[Gi])− Tr (Vg[wiGi]) (30)
= 1
B B∑ i=1 ‖Gi‖22",A. Differences of variances,[0],[0]
"−
( 1
B B∑ i=1",A. Differences of variances,[0],[0]
"‖Gi‖2
)",A. Differences of variances,[0],[0]
"2 (31)
",A. Differences of variances,[0],[0]
"=
(∑B i=1 ‖Gi‖2 )",A. Differences of variances,[0],[0]
2 B3 B∑ i=1,A. Differences of variances,[0],[0]
"( B2 ‖Gi‖22 ( ∑B i=1 ‖Gi‖2)2 − 1 ) (32)
",A. Differences of variances,[0],[0]
"=
(∑B i=1 ‖Gi‖2 )",A. Differences of variances,[0],[0]
2 B B∑ i=1,A. Differences of variances,[0],[0]
( g2i − u2 ) .,A. Differences of variances,[0],[0]
"(33)
Completing the squares at equation 33 and using the fact that ∑B i=1",A. Differences of variances,[0],[0]
"u = 1 we complete the derivation.
",A. Differences of variances,[0],[0]
"Tr (Vu[Gi])− Tr (Vg[wiGi]) (34)
=
(∑B i=1 ‖Gi‖2 )",A. Differences of variances,[0],[0]
2 B B∑ i=1,A. Differences of variances,[0],[0]
"(gi − u)2 (35)
=
( 1
B B∑ i=1",A. Differences of variances,[0],[0]
‖Gi‖2 ),A. Differences of variances,[0],[0]
2 B ‖g,A. Differences of variances,[0],[0]
− u‖22 .,A. Differences of variances,[0],[0]
(36),A. Differences of variances,[0],[0]
"In this section, we reiterate the analysis from the main paper (§ 3.2) with more details.
",B. An upper bound to the gradient norm,[0],[0]
Let θ(l) ∈ RMl×Ml−1 be the weight matrix for layer l and σ(l)(·) be a Lipschitz continuous activation function.,B. An upper bound to the gradient norm,[0],[0]
"Then, let
x(0) = x (37)
z(l) = θ(l) x(l−1)",B. An upper bound to the gradient norm,[0],[0]
"(38)
x(l) = σ(l)(z(l)) (39)
Ψ(x; Θ) = x(L).",B. An upper bound to the gradient norm,[0],[0]
"(40)
Equations 37-40 define a simple fully connected neural network without bias to simplify the closed form definition of the gradient with respect to the parameters Θ.
",B. An upper bound to the gradient norm,[0],[0]
"In addition we define the gradient of the loss with respect to the output of the network as
∇",B. An upper bound to the gradient norm,[0],[0]
x (L) i L = ∇,B. An upper bound to the gradient norm,[0],[0]
x,B. An upper bound to the gradient norm,[0],[0]
"(L) i L(Ψ(xi; Θ), yi) (41)
and the gradient of the loss with respect to the output of layer l as
∇ x",B. An upper bound to the gradient norm,[0],[0]
(l) i L = ∆(l)i Σ ′,B. An upper bound to the gradient norm,[0],[0]
L(z (L) i ),B. An upper bound to the gradient norm,[0],[0]
"∇x(L)i L (42)
where
∆",B. An upper bound to the gradient norm,[0],[0]
(l) i = Σ ′ l(z (l) i )θ T l+1 . .,B. An upper bound to the gradient norm,[0],[0]
.Σ ′,B. An upper bound to the gradient norm,[0],[0]
"L−1(z (L−1) i )θ T L (43)
propagates the gradient from the last layer (pre-activation) to layer l and
Σ′l(z) = diag ( σ′(l)(z1), . . .",B. An upper bound to the gradient norm,[0],[0]
", σ ′(l)(zMl) )
(44)
defines the gradient of the activation function of layer l.
Finally, the gradient with respect to the parameters of the l-th layer can be written
‖∇θlL(Ψ(xi; Θ), yi)‖2 (45)
= ∥∥∥∥(∆(l)i Σ′L(z(L)i )",B. An upper bound to the gradient norm,[0],[0]
∇x(L)i L)(x(l−1)i ),B. An upper bound to the gradient norm,[0],[0]
"T ∥∥∥∥
2
(46)
≤ ∥∥∥x(l−1)i ∥∥∥
",B. An upper bound to the gradient norm,[0],[0]
2 ∥∥∥∆(l)i ∥∥∥ 2 ∥∥∥Σ′L(z(L)i )∇x(L)i L∥∥∥2 .,B. An upper bound to the gradient norm,[0],[0]
"(47) We observe that x(l)i and ∆ (l) i depend only on zi and Θ. However, we theorize that due to various weight initialization and activation normalization techniques those quantities do not capture the important per sample variations of the
gradient norm.",B. An upper bound to the gradient norm,[0],[0]
"Using the above, which is also shown experimentally to be true in § 4.1, we deduce the following upper bound per layer
‖∇θlL(Ψ(xi; Θ), yi)‖2 (48)
≤",B. An upper bound to the gradient norm,[0],[0]
"max l,i (∥∥∥x(l−1)i ∥∥∥ 2 ∥∥∥∆(l)i ∥∥∥ 2 )∥∥∥Σ′L(z(L)i )∇x(L)i L∥∥∥2(49) = ρ
∥∥∥Σ′L(z(L)i )",B. An upper bound to the gradient norm,[0],[0]
"∇x(L)i L∥∥∥2 , (50) which can then be used to derive our final upper bound
‖∇ΘL(Ψ(xi; Θ), yi)‖2 ≤",B. An upper bound to the gradient norm,[0],[0]
"Lρ ∥∥∥Σ′L(z(L)i )∇x(L)i L∥∥∥2︸ ︷︷ ︸
Ĝi
.
(51)
Intuitively, equation 51 means that the variations of the gradient norm are mostly captured by the final classification layer.",B. An upper bound to the gradient norm,[0],[0]
"Consequently, we can use the gradient of the loss with respect to the pre-activation outputs of our neural network as an upper bound to the per-sample gradient norm.",B. An upper bound to the gradient norm,[0],[0]
"For completeness, we also compare our proposed method with Stochastic Variance Reduced Gradient methods and present the results in this section.",C. Comparison with SVRG methods,[0],[0]
We follow the experimental setup of § 4.2 and evaluate on the augmented CIFAR10 and CIFAR100 datasets.,C. Comparison with SVRG methods,[0],[0]
"The algorithms we considered were SVRG (Johnson & Zhang, 2013), accelerated SVRG with Katyusha momentum (Allen-Zhu, 2017) and, the most suitable for Deep Learning, SCSG (Lei et al., 2017) which in practice is a mini-batch version of SVRG. SAGA (Defazio et al., 2014) was not considered due to the prohibitive memory requirements for storing the per sample gradients.
",C. Comparison with SVRG methods,[0],[0]
"For all methods, we tune the learning rate and the epochs per batch gradient computation (m in SVRG literature).",C. Comparison with SVRG methods,[0],[0]
"For SCSG, we also tune the large batch (denoted as Bj in Lei et al. (2017)) and its growth rate.",C. Comparison with SVRG methods,[0],[0]
The results are depicted in figure 6.,C. Comparison with SVRG methods,[0],[0]
We observe that SGD with momentum performs significantly better than all SVRG methods.,C. Comparison with SVRG methods,[0],[0]
Full batch SVRG and Katyusha perform a small number of parameter updates thus failing to optimize the networks.,C. Comparison with SVRG methods,[0],[0]
"In all cases, the best variance reduced method achieves more than an order of magnitude higher training loss than our proposed importance sampling scheme.",C. Comparison with SVRG methods,[0],[0]
"The only hyperparameter that is somewhat hard to define in our algorithm is the pre-sampling size B. As mentioned in the main paper, it controls the maximum possible variance reduction and also how much wall-clock time one iteration with importance sampling will require.
",D. Ablation study on B,[0],[0]
In figure 7 we depict the results of training with importance sampling and different pre-sampling sizes on CIFAR10.,D. Ablation study on B,[0],[0]
"We follow the same experimental setup as in the paper.
",D. Ablation study on B,[0],[0]
"We observe that larger presampling size results in lower training loss, which follows from our theory since the maximum variance reduction is smaller with small B.",D. Ablation study on B,[0],[0]
In this experiment we use the same τth for all the methods and we observe that B = 384 reaches first to 0.6 training loss.,D. Ablation study on B,[0],[0]
"This is justified because computing the importance for 1, 024 samples in the beginning of training is wasteful according to our analysis.
",D. Ablation study on B,[0],[0]
"According to this preliminary ablation study for B, we conclude that choosing B = kb with 2 < k",D. Ablation study on B,[0],[0]
< 6 is a good strategy for achieving a speedup.,D. Ablation study on B,[0],[0]
"However, regardless of the choice of B, pairing it with a threshold τth designated by the analysis in the paper guarantees that the algorithm will be spending time on importance sampling only when the variance can be greatly reduced.
",D. Ablation study on B,[0],[0]
E. Importance Sampling with the Loss,D. Ablation study on B,[0],[0]
"In this section we will present a small analysis that provides intuition regarding using the loss as an approximation or an upper bound to the per sample gradient norm.
",D. Ablation study on B,[0],[0]
"Let L(ψ, y) :",D. Ablation study on B,[0],[0]
"D → R be either the negative log likelihood through a sigmoid or the squared error loss function defined respectively as
L1(ψ, y) =",D. Ablation study on B,[0],[0]
"− log ( exp(yψ)
1 + exp(yψ)
)",D. Ablation study on B,[0],[0]
"y ∈ {−1, 1} ψ ∈ R
L2(ψ, y) =",D. Ablation study on B,[0],[0]
‖y,D. Ablation study on B,[0],[0]
− ψ‖22 y ∈ R d ψ ∈,D. Ablation study on B,[0],[0]
"Rd
(52)
",D. Ablation study on B,[0],[0]
"Given our upper bound to the gradient norm, we can write
‖∇θtL(Ψ(xi; θt), yi)‖2 ≤",D. Ablation study on B,[0],[0]
"Lρ ‖∇ψL(Ψ(xi; θt), yi)‖2 .",D. Ablation study on B,[0],[0]
"(53)
Moreover, for the losses that we are considering, when L(ψ, y)→ 0 then ‖∇ψL(Ψ(xi; θt), yi)‖2 → 0.",D. Ablation study on B,[0],[0]
"Using this fact in combination to equation 53, we claim that so does the per sample gradient norm thus small loss values imply small gradients.",D. Ablation study on B,[0],[0]
"However, large loss values are not well correlated with the gradient norm which can also be observed in § 4.1 in the paper.
",D. Ablation study on B,[0],[0]
"To summarize, we conjecture that due to the above facts, sampling proportionally to the loss reduces the variance only when the majority of the samples have losses close to 0.",D. Ablation study on B,[0],[0]
"Our assumption is validated from our experiments, where the loss struggles to achieve a speedup in the early stages of training where most samples still have relatively large loss values.",D. Ablation study on B,[0],[0]
"Deep neural network training spends most of the computation on examples that are properly handled, and could be ignored.",abstractText,[0],[0]
"We propose to mitigate this phenomenon with a principled importance sampling scheme that focuses computation on “informative” examples, and reduces the variance of the stochastic gradients during training.",abstractText,[0],[0]
"Our contribution is twofold: first, we derive a tractable upper bound to the persample gradient norm, and second we derive an estimator of the variance reduction achieved with importance sampling, which enables us to switch it on when it will result in an actual speedup.",abstractText,[0],[0]
"The resulting scheme can be used by changing a few lines of code in a standard SGD procedure, and we demonstrate experimentally, on image classification, CNN fine-tuning, and RNN training, that for a fixed wall-clock time budget, it provides a reduction of the train losses of up to an order of magnitude and a relative improvement of test errors between 5% and 17%.",abstractText,[0],[0]
Not All Samples Are Created Equal:  Deep Learning with Importance Sampling,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2104–2115 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
2104",text,[0],[0]
Language models (LMs) are statistical models that assign a probability over sequences of words.,1 Introduction,[0],[0]
"Language models can often help with other tasks, such as speech recognition (Mikolov et al., 2010; Prabhavalkar et al., 2017), machine translation (Luong et al., 2015; Gülçehre et al., 2017), text summarisation (Filippova et al., 2015; Gambhir and Gupta, 2017), question answering (Wang et al., 2017), semantic error detection (Rei and Yannakoudakis, 2017; Spithourakis et al., 2016a), and fact checking (Rashkin et al., 2017).
",1 Introduction,[0],[0]
"Numeracy and literacy refer to the ability to comprehend, use, and attach meaning to numbers and words, respectively.",1 Introduction,[0],[0]
"Language models exhibit literacy by being able to assign higher probabilities to sentences that
are both grammatical and realistic, as in this example:
‘I eat an apple’ (grammatical and realistic)",1 Introduction,[0],[0]
"‘An apple eats me’ (unrealistic)
",1 Introduction,[0],[0]
"‘I eats an apple’ (ungrammatical)
",1 Introduction,[0],[0]
"Likewise, a numerate language model should be able to rank numerical claims based on plausibility:
’John’s height is 1.75 metres’ (realistic) ’",1 Introduction,[0],[0]
"John’s height is 999.999 metres’ (unrealistic)
Existing approaches to language modelling treat numerals similarly to other words, typically using categorical distributions over a fixed vocabulary.
",1 Introduction,[0],[0]
"However, this maps all unseen numerals to the same unknown type and ignores the smoothness of continuous attributes, as shown in Figure 1.",1 Introduction,[0],[0]
"In that respect, existing work on language modelling does not explicitly evaluate or optimise for numeracy.",1 Introduction,[0],[0]
"Numerals are often neglected and low-resourced, e.g. they are often masked (Mitchell and Lapata, 2009), and there are only 15,164 (3.79%) numerals among GloVe’s 400,000 embeddings pretrained on 6 billion tokens (Pennington et al., 2014).",1 Introduction,[0],[0]
"Yet, numbers appear ubiquitously, from children’s magazines (Joram et al., 1995) to clinical reports (Bigeard et al., 2015), and grant objectivity to sciences (Porter, 1996).
",1 Introduction,[0],[0]
"Previous work finds that numerals have higher out-of-vocabulary rates than other words and proposes solutions for representing unseen numerals as inputs to language models, e.g. using numerical magnitudes as features (Spithourakis et al., 2016b,a).",1 Introduction,[0],[0]
"Such work identifies that the perplexity of language models on the subset of numerals can be very high, but does not directly address the issue.",1 Introduction,[0],[0]
This paper focuses on evaluating and improving the ability of language models to predict numerals.,1 Introduction,[0],[0]
"The main contributions of this paper are as follows:
1.",1 Introduction,[0],[0]
"We explore different strategies for modelling numerals, such as memorisation and digit-bydigit composition, and propose a novel neural architecture based on continuous probability density functions.
",1 Introduction,[0],[0]
2.,1 Introduction,[0],[0]
"We propose the use of evaluations that adjust for the high out-of-vocabulary rate of numerals and account for their numerical value (magnitude).
3.",1 Introduction,[0],[0]
We evaluate on a clinical and a scientific corpus and provide a qualitative analysis of learnt representations and model predictions.,1 Introduction,[0],[0]
"We find that modelling numerals separately from other words can drastically improve the perplexity of LMs, that different strategies for modelling numerals are suitable for different textual contexts, and that continuous probability density functions can improve the LM’s prediction accuracy for numbers.",1 Introduction,[0],[0]
"Let s1,s2,...,sL denote a document, where st is the token at position t. A language model estimates the probability of the next token given previous tokens, i.e. p(st|s1,...,st−1).",2 Language Models,[0],[0]
"Neural LMs estimate this probability by feeding embeddings, i.e. vectors that represent each token, into a Recurrent Neural Network (RNN) (Mikolov et al., 2010).
",2 Language Models,[0],[0]
Token Embeddings Tokens are most commonly represented by aD-dimensional dense vector that is unique for each word from a vocabulary V of known words.,2 Language Models,[0],[0]
"This vocabulary includes special symbols (e.g. ‘UNK’) to handle out-of-vocabulary tokens, such as unseen words or numerals.",2 Language Models,[0],[0]
"Let ws be the one-hot representation of token s, i.e. a sparse binary vector with a single element set to 1 for that token’s index in the vocabulary, andE∈RD×|V| be the token embeddings matrix.",2 Language Models,[0],[0]
"The token embedding for s is the vector etokens =Ews.
",2 Language Models,[0],[0]
Character-Based Embeddings,2 Language Models,[0],[0]
"A representation for a token can be build from its constituent characters (Luong and Manning, 2016; Santos and Zadrozny, 2014).",2 Language Models,[0],[0]
Such a representation takes into account the internal structure of tokens.,2 Language Models,[0],[0]
"Let d1,d2,...,dN be the characters of token s. A character-based embedding for s is the final hidden state of a D-dimensional character-level RNN:",2 Language Models,[0],[0]
"echarss =RNN(d0,d1,...dL).
",2 Language Models,[0],[0]
Recurrent and Output Layer,2 Language Models,[0],[0]
The computation of the conditional probability of the next token involves recursively feeding the embedding of the current token est and the previous hidden state ht−1 into a D-dimensional token-level RNN to obtain the current hidden state ht.,2 Language Models,[0],[0]
"The output probability is estimated using the softmax function, i.e.
p(st|ht)=softmax(ψ(st))= 1Ze ψ(st) Z= ∑ s′∈V eψ(s ′), (1)
where ψ(.) is a score function.
",2 Language Models,[0],[0]
"Training and Evaluation Neural LMs are typically trained to minimise the cross entropy on the training corpus:
Htrain=− 1
N ∑ st∈train logp(st|s<t) (2)
A common performance metric for LMs is per token perplexity (Eq. 3), evaluated on a test corpus.",2 Language Models,[0],[0]
"It can also be interpreted as the branching factor: the size of an equally weighted distribution with equivalent uncertainty, i.e. how many sides you need on a fair die to get the same uncertainty as the model distribution.
",2 Language Models,[0],[0]
PPtest=exp(Htest) (3),2 Language Models,[0],[0]
"In this section we describe models with different strategies for generating numerals and propose the
use of number-specific evaluation metrics that adjust for the high out-of-vocabulary rate of numerals and account for numerical values.",3 Strategies for Modelling Numerals,[0],[0]
We draw inspiration from theories of numerical cognition.,3 Strategies for Modelling Numerals,[0],[0]
"The triple code theory (Dehaene et al., 2003) postulates that humans process quantities through two exact systems (verbal and visual) and one approximate number system that semantically represents a number on a mental number line.",3 Strategies for Modelling Numerals,[0],[0]
"Tzelgov et al. (2015) identify two classes of numbers: i) primitives, which are holistically retrieved from long-term memory; and ii) non-primitives, which are generated online.",3 Strategies for Modelling Numerals,[0],[0]
An in-depth review of numerical and mathematical cognition can be found in Kadosh and Dowker (2015) and Campbell (2005).,3 Strategies for Modelling Numerals,[0],[0]
This class of models assumes that numerals come from a finite vocabulary that can be memorised and retrieved later.,3.1 Softmax Model and Variants,[0],[0]
"The softmax model treats all tokens (words and numerals) alike and directly uses Equation 1 with score function:
ψ(st)=h T t e token st =h T t Eoutwst, (4)
where Eout ∈ RD×|V| is an output embeddings matrix.",3.1 Softmax Model and Variants,[0],[0]
"The summation in Equation 1 is over the complete target vocabulary, which requires mapping any out-of-vocabulary tokens to special symbols, e.g. ‘UNKword’ and ‘UNKnumeral’.
",3.1 Softmax Model and Variants,[0],[0]
"Softmax with Digit-Based Embeddings The softmax+rnn variant considers the internal syntax of a numeral’s digits by adjusting the score function:
ψ(st)=h T t e token st +h T t e chars st
=hTt Eoutwst+h T t E RNN out wst,
(5)
where the columns of ERNNout are composed of character-based embeddings for in-vocabulary numerals and token embeddings for the remaining vocabulary.",3.1 Softmax Model and Variants,[0],[0]
"The character set comprises digits (0-9), the decimal point, and an end-of-sequence character.",3.1 Softmax Model and Variants,[0],[0]
"The model still requires normalisation over the whole vocabulary, and the special unknown tokens are still needed.
Hierarchical Softmax A hierarchical softmax (Morin and Bengio, 2005a) can help us decouple the modelling of numerals from that of words.",3.1 Softmax Model and Variants,[0],[0]
"The probability of the next token st is decomposed to that of its class ct and the probability of the exact token from within the class:
p(st|ht)= ∑ ct∈C",3.1 Softmax Model and Variants,[0],[0]
"p(ct|ht)p(st|ct,ht)
p(ct|ht)=σ ( hTt b ) (6)
where the valid token classes are C = {word, numeral}, σ is the sigmoid function and b is a D-dimensional vector.",3.1 Softmax Model and Variants,[0],[0]
"Each of the two branches of p(st|ct,ht) can now be modelled by independently normalised distributions.",3.1 Softmax Model and Variants,[0],[0]
The hierarchical variants (h-softmax and h-softmax+rnn) use two independent softmax distributions for words and numerals.,3.1 Softmax Model and Variants,[0],[0]
"The two branches share no parameters, and thus words and numerals will be embedded into separate spaces.
",3.1 Softmax Model and Variants,[0],[0]
The hierarchical approach allows us to use any well normalised distribution to model each of its branches.,3.1 Softmax Model and Variants,[0],[0]
"In the next subsections, we examine different strategies for modelling the branch of numerals, i.e. p(st|ct=numeral,ht).",3.1 Softmax Model and Variants,[0],[0]
"For simplicity, we will abbreviate this to p(s).",3.1 Softmax Model and Variants,[0],[0]
"Let d1,d2...dN be the digits of numeral s. A digit-bydigit composition strategy estimates the probability of the numeral from the probabilities of its digits:
p(s)=p(d1)p(d2|d1)...",3.2 Digit-RNN Model,[0],[0]
"p(dN |d<N) (7)
The d-RNN model feeds the hidden state ht of the token-level RNN into a character-level RNN (Graves, 2013; Sutskever et al., 2011) to estimate this probability.",3.2 Digit-RNN Model,[0],[0]
"This strategy can accommodate an open vocabulary, i.e. it eliminates the need for an UNKnumeral symbol, as the probability is normalised one digit at a time over the much smaller vocabulary of digits (digits 0-9, decimal separator, and end-of-sequence).",3.2 Digit-RNN Model,[0],[0]
"Inspired by the approximate number system and the mental number line (Dehaene et al., 2003), our proposed MoG model computes the probability of numerals from a probability density function (pdf) over real numbers, using a mixture of Gaussians for the underlying pdf:
q(v)= K∑ k=1 πkNk(v;µk,σ2k)
πk=softmax ( BTht ) ,
(8)
where K is the number of components, πk are mixture weights that depend on hidden state ht of the token-level RNN,",3.3 Mixture of Gaussians Model,[0],[0]
"Nk is the pdf of the normal distribution with mean µk ∈R and variance σ2k ∈R, andB∈RD×K is a matrix.
",3.3 Mixture of Gaussians Model,[0],[0]
"The difficulty with this approach is that for any continuous random variable, the probability that it equals a specific value is always zero.",3.3 Mixture of Gaussians Model,[0],[0]
"To resolve this,
we consider a probability mass function (pmf) that discretely approximates the pdf:
Q̃(v|r)= v+",3.3 Mixture of Gaussians Model,[0],[0]
"r∫ v− r q(u)du=F(v+ r)−F(v− r), (9)
where F(.) is the cumulative density function of q(.), and r =0.5×10−r is the number’s precision.",3.3 Mixture of Gaussians Model,[0],[0]
"The level of discretisation r, i.e. how many decimal digits to keep, is a random variable in N with distribution p(r).",3.3 Mixture of Gaussians Model,[0],[0]
"The mixed joint density is:
p(s)=p(v,r)=p(r)Q̃(v|r) (10)
",3.3 Mixture of Gaussians Model,[0],[0]
"Figure 2 summarises this strategy, where we model the level of discretisation by converting the numeral into a pattern and use a RNN to estimate the probability of that pattern sequence:
p(r)=p(SOS INT_PART .",3.3 Mixture of Gaussians Model,[0],[0]
r decimal digits︷ ︸︸,3.3 Mixture of Gaussians Model,[0],[0]
︷ \d ... \d EOS) (11),3.3 Mixture of Gaussians Model,[0],[0]
Different mechanisms might be better for predicting numerals in different contexts.,3.4 Combination of Strategies,[0],[0]
"We propose a combination model that can select among different
strategies for modelling numerals: p(s)= ∑ ∀m∈M αmp(s|m)
αm=softmax ( ATht ) ,
(12)
where M={h-softmax, d-RNN, MoG}, and A∈RD×|M|.",3.4 Combination of Strategies,[0],[0]
"Since both d-RNN and MoG are openvocabulary models, the unknown numeral token can now be removed from the vocabulary of h-softmax.",3.4 Combination of Strategies,[0],[0]
Numeracy skills are centred around the understanding of numbers and numerals.,3.5 Evaluating the Numeracy of LMs,[0],[0]
"A number is a mathematical object with a specific magnitude, whereas a numeral is its symbolic representation, usually in the positional decimal Hindu–Arabic numeral system (McCloskey and Macaruso, 1995).",3.5 Evaluating the Numeracy of LMs,[0],[0]
"In humans, the link between numerals and their numerical values boosts numerical skills (Griffin et al., 1995).
",3.5 Evaluating the Numeracy of LMs,[0],[0]
Perplexity Evaluation Test perplexity evaluated only on numerals will be informative of the symbolic component of numeracy.,3.5 Evaluating the Numeracy of LMs,[0],[0]
"However, model comparisons based on naive evaluation using Equation 3 might be problematic: perplexity is sensitive to outof-vocabulary (OOV) rate, which might differ among models, e.g. it is zero for open-vocabulary models.",3.5 Evaluating the Numeracy of LMs,[0],[0]
"As an extreme example, in a document where all words are out of vocabulary, the best perplexity is achieved by a trivial model that predicts everything as unknown.
",3.5 Evaluating the Numeracy of LMs,[0],[0]
"Ueberla (1994) proposed Adjusted Perplexity (APP; Eq. 14), also known as unknown-penalised perplexity (Ahn et al., 2016), to cancel the effect of the out-of-vocabulary rate on perplexity.",3.5 Evaluating the Numeracy of LMs,[0],[0]
"The APP is the perplexity of an adjusted model that uniformly redistributes the probability of each out-of-vocabulary class over all different types in that class:
p′(s)= { p(s) 1|OOVc| if s∈OOVc p(s) otherwise
(13)
where OOVc is an out-of-vocabulary class (e.g. words and numerals), and |OOVc| is the cardinality of each OOV set.",3.5 Evaluating the Numeracy of LMs,[0],[0]
"Equivalently, adjusted perplexity can be calculated as:
APPtest=exp ( Htest+
∑ c Hcadjust
)
",3.5 Evaluating the Numeracy of LMs,[0],[0]
"Hcadjust=− ∑ t |st∈OOVc| N log 1 |OOVc|
(14)
",3.5 Evaluating the Numeracy of LMs,[0],[0]
"whereN is the total number of tokens in the test set and |s∈OOVc| is the count of tokens from the test set belonging in each OOV set.
",3.5 Evaluating the Numeracy of LMs,[0],[0]
"Evaluation on the Number Line While perplexity looks at symbolic performance on numerals, this evaluation focuses on numbers and particularly on their numerical value, which is their most prominent semantic content (Dehaene et al., 2003; Dehaene and Cohen, 1995).
",3.5 Evaluating the Numeracy of LMs,[0],[0]
Let vt be the numerical value of token st from the test corpus.,3.5 Evaluating the Numeracy of LMs,[0],[0]
"Also, let v̂t be the value of the most probable numeral under the model st = argmax (p(st|ht,ct=num)).",3.5 Evaluating the Numeracy of LMs,[0],[0]
Any evaluation metric from the regression literature can be used to measure the models performance.,3.5 Evaluating the Numeracy of LMs,[0],[0]
"To evaluate on the number line, we can use any evaluation metric from the regression literature.",3.5 Evaluating the Numeracy of LMs,[0],[0]
"In reverse order of tolerance to extreme errors, some of the most popular are Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and Median Absolute Error (MdAE):
ei = vi−v̂i
RMSE = √ 1 N N∑ i=1",3.5 Evaluating the Numeracy of LMs,[0],[0]
"e2i
MAE = 1N N∑ i=1",3.5 Evaluating the Numeracy of LMs,[0],[0]
"|ei|
MdAE = median{|ei|}
(15)
",3.5 Evaluating the Numeracy of LMs,[0],[0]
The above are sensitive to the scale of the data.,3.5 Evaluating the Numeracy of LMs,[0],[0]
"If the data contains values from different scales, percentage metrics are often preferred, such as the Mean/Median Absolute Percentage Error (MAPE/MdAPE):
pei = vi−v̂i",3.5 Evaluating the Numeracy of LMs,[0],[0]
"vi
MAPE = 1N N∑ i=1",3.5 Evaluating the Numeracy of LMs,[0],[0]
"|pei|
MdAPE = median{|pei|}
(16)",3.5 Evaluating the Numeracy of LMs,[0],[0]
"To evaluate our models, we created two datasets with documents from the clinical and scientific domains, where numbers abound (Bigeard et al., 2015; Porter, 1996).",4 Data,[0],[0]
"Furthermore, to ensure that the numbers will be informative of some attribute, we only selected texts that reference tables.
",4 Data,[0],[0]
Clinical Data,4 Data,[0],[0]
Our clinical dataset comprises clinical records from the London Chest Hospital.,4 Data,[0],[0]
"The records where accompanied by tables with 20 numeric attributes (age, heart volumes, etc.) that they partially describe, as well as include numbers not found in the tables.",4 Data,[0],[0]
"Numeric tokens constitute only a small proportion of each sentence (4.3%), but account
for a large part of the unique tokens vocabulary (>40%) and suffer high OOV rates.
",4 Data,[0],[0]
Scientific Data,4 Data,[0],[0]
"Our scientific dataset comprises paragraphs from Cornell’s ARXIV 1 repository of scientific articles, with more than half a million converted papers in 37 scientific sub-fields.",4 Data,[0],[0]
"We used the preprocessed ARXMLIV (Stamerjohanns et al., 2010; Stamerjohanns and Kohlhase, 2008) 2 version, where papers have been converted from LATEX into a custom XML format using the LATEXML 3 tool.",4 Data,[0],[0]
"We then kept all paragraphs with at least one reference to a table and a number.
",4 Data,[0],[0]
"For both datasets, we lowercase tokens and normalise numerals by omitting the thousands separator (""2,000"" becomes ""2000"") and leading zeros (""007"" becomes ""7"").",4 Data,[0],[0]
"Special mathematical symbols are tokenised separately, e.g. negation (“-1” as “-”, “1”), fractions (“3/4” as “3”, “/”, “4”), etc.",4 Data,[0],[0]
"For this reason, all numbers were non-negative.",4 Data,[0],[0]
Table 1 shows descriptive statistics for both datasets.,4 Data,[0],[0]
"We set the vocabularies to the 1,000 and 5,000 most frequent token types for the clinical and scientific datasets, respectively.",5 Experimental Results and Discussion,[0],[0]
"We use gated token-character embeddings (Miyamoto and Cho, 2016) for the input of numerals and token embeddings for the input and output of words, since the scope of our paper is numeracy.",5 Experimental Results and Discussion,[0],[0]
"We set the models’ hidden dimensions to D = 50 and initialise all token embeddings to pretrained GloVe (Pennington et al., 2014).",5 Experimental Results and Discussion,[0],[0]
"All our
1ARXIV.ORG.",5 Experimental Results and Discussion,[0],[0]
"Cornell University Library at http://arxiv.org/, visited December 2016
2ARXMLIV.",5 Experimental Results and Discussion,[0],[0]
"Project home page at http://arxmliv.kwarc.info/, visited December 2016
3LATEXML.",5 Experimental Results and Discussion,[0],[0]
"http://dlmf.nist.gov, visited December 2016
RNNs are LSTMs (Hochreiter and Schmidhuber, 1997) with the biases of LSTM forget gate were initialised to 1.0 (Józefowicz et al., 2015).",5 Experimental Results and Discussion,[0],[0]
"We train using mini-batch gradient decent with the Adam optimiser (Kingma and Ba, 2014) and regularise with early stopping and 0.1 dropout rate (Srivastava, 2013) in the input and output of the token-based RNN.
",5 Experimental Results and Discussion,[0],[0]
"For the mixture of Gaussians, we select the mean and variances to summarise the data at different granularities by fitting 7 separate mixture of Gaussian models on all numbers, each with twice as many components as the previous, for a total of 27+1− 1 = 256 components.",5 Experimental Results and Discussion,[0],[0]
These models are initialised at percentile points from the data and trained with the expectation-minimisation algorithm.,5 Experimental Results and Discussion,[0],[0]
The means and variances are then fixed and not updated when we train the language model.,5 Experimental Results and Discussion,[0],[0]
"Perplexities Table 2 shows perplexities evaluated on the subsets of words, numerals and all tokens of
the test data.",5.1 Quantitative Results,[0],[0]
"Overall, all models performed better on the clinical than on the scientific data.",5.1 Quantitative Results,[0],[0]
"On words, all models achieve similar perplexities in each dataset.
",5.1 Quantitative Results,[0],[0]
"On numerals, softmax variants perform much better than other models in PP, which is an artefact of the high OOV-rate of numerals.",5.1 Quantitative Results,[0],[0]
"APP is significantly worse, especially for non-hierarchical variants, which perform about 2 and 4 orders of magnitude worse than hierarchical ones.
",5.1 Quantitative Results,[0],[0]
"For open-vocabulary models, i.e. d-RNN, MoG, and combination, PP is equivalent to APP.",5.1 Quantitative Results,[0],[0]
"On numerals, d-RNN performed better than softmax variants in both datasets.",5.1 Quantitative Results,[0],[0]
"The MoG model performed twice as well as softmax variants on the clinical dataset, but had the third worse performance in the scientific dataset.",5.1 Quantitative Results,[0],[0]
"The combination model had the best overall APP results for both datasets.
",5.1 Quantitative Results,[0],[0]
"Evaluations on the Number Line To factor out model specific decoding processes for finding the best next numeral, we use our models to rank a set
of candidate numerals: we compose the union of in-vocabulary numbers and 100 percentile points from the training set, and we convert numbers into numerals by considering all formats up to n decimal points.",5.1 Quantitative Results,[0],[0]
"We select n to represent 90% of numerals seen at training, which yields n=3 and n=4 for the clinical and scientific data, respectively.
",5.1 Quantitative Results,[0],[0]
"Table 3 shows evaluation results, where we also include two naive baselines of constant predictions: with the mean and median of the training data.",5.1 Quantitative Results,[0],[0]
"For both datasets, RMSE and MAE were too sensitive to extreme errors to allow drawing safe conclusions, particularly for the scientific dataset, where both metrics were in the order of 109.",5.1 Quantitative Results,[0],[0]
"MdAE can be of some use, as 50% of the errors are absolutely smaller than that.
",5.1 Quantitative Results,[0],[0]
"Along percentage metrics, MoG achieved the best MAPE in both datasets (18% and 54% better that the second best) and was the only model to perform better than the median baseline for the clinical data.",5.1 Quantitative Results,[0],[0]
"However, it had the worst MdAPE, which means that MoG mainly reduced larger percentage errors.",5.1 Quantitative Results,[0],[0]
"The d-RNN model came third and second in the clinical and scientific datasets, respectively.",5.1 Quantitative Results,[0],[0]
"In the latter it achieved the best MdAPE, i.e. it was effective at reducing errors for 50% of the numbers.",5.1 Quantitative Results,[0],[0]
The combination model did not perform better than its constituents.,5.1 Quantitative Results,[0],[0]
This is possibly because MoG is the only strategy that takes into account the numerical magnitudes of the numerals.,5.1 Quantitative Results,[0],[0]
Softmax versus Hierarchical Softmax Figure 3 visualises the cosine similarities of the output token embeddings of numerals for the softmax and h-softmax models.,5.2 Learnt Representations,[0],[0]
"Simple softmax enforced high similarities among all numerals and the unknown numeral token, so as to make them more dissimilar to words, since the model embeds both in the same space.",5.2 Learnt Representations,[0],[0]
"This is not the case for h-softmax that uses two different spaces: similarities are concentrated along the diagonal and fan out as the magnitude grows, with the exception of numbers with special meaning, e.g. years and percentile points.
",5.2 Learnt Representations,[0],[0]
Digit embeddings Figure 4 shows the cosine similarities between the digits of the d-RNN output mode.,5.2 Learnt Representations,[0],[0]
We observe that each primitive digit is mostly similar to its previous and next digit.,5.2 Learnt Representations,[0],[0]
Similar behaviour was found for all digit embeddings of all models.,5.2 Learnt Representations,[0],[0]
"Next Numeral Figure 5 shows the probabilities of different numerals under each model for two
examples from the clinical development set.",5.3 Predictions from the Models,[0],[0]
Numerals are grouped by number of decimal points.,5.3 Predictions from the Models,[0],[0]
"The h-softmax model’s probabilities are spiked, d-RNNs are saw-tooth like and MoG’s are smooth, with the occasional spike, whenever a narrow component allows for it.",5.3 Predictions from the Models,[0],[0]
"Probabilities rapidly decrease for more decimal digits, which is reminiscent of the theoretical expectation that the probability of en exact value for a continuous variable is zero.
",5.3 Predictions from the Models,[0],[0]
"Selection of Strategy in Combination Model Table 4 shows development set examples with high selection probabilities for each strategy of the combination model, along with numerals with the highest average selection per mode.",5.3 Predictions from the Models,[0],[0]
"The h-softmax model is responsible for mostly integers with special functions,
e.g. years, typical drug dosages, percentile points, etc.",5.3 Predictions from the Models,[0],[0]
"In the clinical data, d-RNN picks up two-digit integers (mostly dimensions) and MoG is activated for continuous attributes, which are mostly out of vocabulary.",5.3 Predictions from the Models,[0],[0]
"In the scientific data, d-RNN and MoG
showed affinity to different indices from catalogues of astronomical objects: d-RNN mainly to NGC (Dreyer, 1888) and MoG to various other indices, such as GL (Gliese, 1988) and HIP (Perryman et al., 1997).",5.3 Predictions from the Models,[0],[0]
"In this case, MoG was wrongly selected for numerals with a labelling function, which also highlights a limitation of evaluating on the number line, when a numeral is not used to represent its magnitude.
",5.3 Predictions from the Models,[0],[0]
"Significant Digits Figure 5 shows the distributions of the most significant digits under the d-RNN model
and from data counts.",5.3 Predictions from the Models,[0],[0]
"The theoretical estimate has been overlayed, according to Benford’s law (Benford, 1938), also called the first-digit law, which applies to many real-life numerals.",5.3 Predictions from the Models,[0],[0]
The law predicts that the first digit is 1 with higher probability (about 30%) than 9 (< 5%) and weakens towards uniformity at higher digits.,5.3 Predictions from the Models,[0],[0]
Model probabilities closely follow estimates from the data.,5.3 Predictions from the Models,[0],[0]
"Violations from Benford’s law can be due to rounding (Beer, 2009) and can be used as evidence for fraud detection (Lu et al., 2006).",5.3 Predictions from the Models,[0],[0]
"Numerical quantities have been recognised as important for textual entailment (Lev et al., 2004; Dagan et al., 2013).",6 Related Work,[0],[0]
"Roy et al. (2015) proposed a quantity entailment sub-task that focused on whether a given quantity can be inferred from a given text and, if so, what its value should be.",6 Related Work,[0],[0]
"A common framework for acquiring common sense about numerical attributes of objects has been to collect a corpus of numerical values in pre-specified templates and then model attributes as a normal distribution (Aramaki et al., 2007; Davidov and Rappoport, 2010; Iftene and Moruz, 2010; Narisawa et al., 2013; de Marneffe et al., 2010).",6 Related Work,[0],[0]
"Our model embeds these approaches into a LM that has a sense for numbers.
",6 Related Work,[0],[0]
Other tasks that deal with numerals are numerical information extraction and solving mathematical problems.,6 Related Work,[0],[0]
"Numerical relations have at least one argument that is a number and the aim of the task is to extract all such relations from a corpus, which can range from identifying a few numerical attributes (Nguyen and Moschitti, 2011; Intxaurrondo et al., 2015) to generic numerical relation extraction (Hoffmann et al., 2010; Madaan et al., 2016).",6 Related Work,[0.9503352229645378],"['Since dropout is a first-order method (see Algorithm 1) and the landscape of Problem 4 is highly non-convex, we can perhaps only hope to find a local minimum, that too provided if the problem has no degenerate saddle points (Lee et al., 2016; Ge et al., 2015).']"
"Our model does not extract values, but rather produces an probabilistic estimate.
",6 Related Work,[0],[0]
"Much work has been done in solving arithmetic (Mitra and Baral, 2016; Hosseini et al., 2014; Roy and Roth, 2016), geometric (Seo et al., 2015), and algebraic problems (Zhou et al., 2015; Koncel-Kedziorski et al., 2015; Upadhyay et al., 2016; Upadhyay and Chang, 2016; Shi et al., 2015; Kushman et al., 2014) expressed in natural language.",6 Related Work,[0],[0]
"Such models often use mathematical background knowledge, such as linear system solvers.",6 Related Work,[0],[0]
"The output of our model is not based on such algorithmic operations, but could be extended to do so in future work.
",6 Related Work,[0],[0]
"In language modelling, generating rare or unknown words has been a challenge, similar to our unknown numeral problem.",6 Related Work,[0],[0]
"Gulcehre et al. (2016) and Gu et al. (2016) adopted pointer networks (Vinyals et al., 2015)
to copy unknown words from the source in translation and summarisation tasks.",6 Related Work,[0],[0]
"Merity et al. (2016) and Lebret et al. (2016) have models that copy from context sentences and from Wikipedia’s infoboxes, respectively.",6 Related Work,[0],[0]
Ahn et al. (2016) proposed a LM that retrieves unknown words from facts in a knowledge graph.,6 Related Work,[0],[0]
They draw attention to the inappropriateness of perplexity when OOV-rates are high and instead propose an adjusted perplexity metric that is equivalent to APP.,6 Related Work,[0],[0]
"Other methods aim at speeding up LMs to allow for larger vocabularies (Chen et al., 2015), such as hierarchical softmax (Morin and Bengio, 2005b), target sampling (Jean et al., 2014), etc., but still suffer from the unknown word problem.",6 Related Work,[0],[0]
"Finally, the problem is resolved when predicting one character at a time, as done by the character-level RNN (Graves, 2013; Sutskever et al., 2011) used in our d-RNN model.",6 Related Work,[0],[0]
"In this paper, we investigated several strategies for LMs to model numerals and proposed a novel openvocabulary generative model based on a continuous probability density function.",7 Conclusion,[0],[0]
"We provided the first thorough evaluation of LMs on numerals on two corpora, taking into account their high out-of-vocabulary rate and numerical value (magnitude).",7 Conclusion,[0],[0]
"We found that modelling numerals separately from other words through a hierarchical softmax can substantially improve the perplexity of LMs, that different strategies are suitable for different contexts, and that a combination of these strategies can help improve the perplexity further.",7 Conclusion,[0],[0]
"Finally, we found that using a continuous probability density function can improve prediction accuracy of LMs for numbers by substantially reducing the mean absolute percentage metric.
",7 Conclusion,[0],[0]
"Our approaches in modelling and evaluation can be used in future work in tasks such as approximate information extraction, knowledge base completion, numerical fact checking, numerical question answering, and fraud detection.",7 Conclusion,[0],[0]
Our code and data are available at: https://github.com/uclmr/ numerate-language-models.,7 Conclusion,[0],[0]
The authors would like to thank the anonymous reviewers for their insightful comments and also Steffen Petersen for providing the clinical dataset and advising us on the clinical aspects of this work.,Acknowledgments,[0],[0]
This research was supported by the Farr Institute of Health Informatics Research and an Allen Distinguished Investigator award.,Acknowledgments,[0],[0]
Numeracy is the ability to understand and work with numbers.,abstractText,[0],[0]
"It is a necessary skill for composing and understanding documents in clinical, scientific, and other technical domains.",abstractText,[0],[0]
"In this paper, we explore different strategies for modelling numerals with language models, such as memorisation and digit-by-digit composition, and propose a novel neural architecture that uses a continuous probability density function to model numerals from an open vocabulary.",abstractText,[0],[0]
"Our evaluation on clinical and scientific datasets shows that using hierarchical models to distinguish numerals from words improves a perplexity metric on the subset of numerals by 2 and 4 orders of magnitude, respectively, over nonhierarchical models.",abstractText,[0],[0]
A combination of strategies can further improve perplexity.,abstractText,[0],[0]
"Our continuous probability density function model reduces mean absolute percentage errors by 18% and 54% in comparison to the second best strategy for each dataset, respectively.",abstractText,[0],[0]
Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 177–187 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
Natural Language generation (NLG) is a long standing goal in natural language processing.,1 Introduction,[0],[0]
"There have already been several successes in applications such as financial reporting (Kukich, 1983; Smadja and McKeown, 1990), or weather forecasts (Konstas and Lapata, 2012; Wen et al., 2015), however it is still a challenging task for less structured and open domains.",1 Introduction,[0],[0]
"Given recent progress in training robust visual recognition models using convolutional neural networks, the task of generating natural language descriptions for ar-
bitrary images has received considerable attention (Vinyals et al., 2015; Karpathy and Fei-Fei, 2015; Mao et al., 2015).",1 Introduction,[0],[0]
"In general, generating visually descriptive language can be useful for various tasks such as human-machine communication, accessibility, image retrieval, and search.",1 Introduction,[0],[0]
"However this task is still challenging and it depends on developing both a robust visual recognition model, and a reliable language generation model.",1 Introduction,[0],[0]
"In this paper, we instead tackle a task of describing object layouts where the categories for the objects in an input scene and their corresponding locations are known.",1 Introduction,[0],[0]
"Object layouts are commonly used for story-boarding, sketching, and computer graphics applications.",1 Introduction,[0],[0]
"Additionally, using our object layout captioning model on the outputs of an object detector we are also able to improve image caption-
177
ing models.",1 Introduction,[0],[0]
"Object layouts contain rich semantic information, however they also abstract away several other visual cues such as color, texture, and appearance, thus introducing a different set of challenges than those found in traditional image captioning.
",1 Introduction,[0],[0]
"We propose OBJ2TEXT, a sequence-tosequence model that encodes object layouts using an LSTM network (Hochreiter and Schmidhuber, 1997), and decodes natural language descriptions using an LSTM-based neural language model1.",1 Introduction,[0],[0]
"Natural language generation systems usually consist of two steps: content planning, and surface realization.",1 Introduction,[0],[0]
"The first step decides on the content to be included in the generated text, and the second step connects the concepts using structural language properties.",1 Introduction,[0],[0]
"In our proposed model, OBJ2TEXT, content planning is performed by the encoder, and surface realization is performed by the decoder.",1 Introduction,[0],[0]
"Our model is trained in the standard MS-COCO dataset (Lin et al., 2014), which includes both object annotations for the task of object detection, and textual descriptions for the task of image captioning.",1 Introduction,[0],[0]
"While most previous research has been devoted to any one of these two tasks, our paper presents, to our knowledge, the first approach for learning mappings between object annotations and textual descriptions.",1 Introduction,[0],[0]
"Using several lesioned versions of the proposed model we explored the effect of object counts and locations in the quality and accuracy of the generated natural language descriptions.
",1 Introduction,[0],[0]
"Generating visually descriptive language requires beyond syntax, and semantics; an understanding of the physical word.",1 Introduction,[0],[0]
We also take inspiration from recent work by Schmaltz et al. (2016) where the goal was to reconstruct a sentence from a bag-of-words (BOW) representation using a simple surface-level language model based on an encoder-decoder sequence-to-sequence architecture.,1 Introduction,[0],[0]
"In contrast to this previous approach, our model is grounded on visual data, and its corresponding spatial information, so it goes beyond word re-ordering.",1 Introduction,[0],[0]
Also relevant to our work is Yao et al. (2016a) which previously explored the task of oracle image captioning by providing a language generation model with a list of manually defined visual concepts known to be present in the image.,1 Introduction,[0],[0]
"In addition, our model is able to leverage
1We build on neuraltalk2 and make our Torch code, and an interactive demo of our model available in the following url: http://vision.cs.virginia.edu/obj2text
both quantity and spatial information as additional cues associated with each object/concept, thus allowing it to learn about verbosity, and spatial relations in a supervised fashion.
",1 Introduction,[0],[0]
"In summary, our contributions are as follows:
• We demonstrate that despite encoding object layouts as a sequence using an LSTM, our model can still effectively capture spatial information for the captioning task.",1 Introduction,[0],[0]
"We perform ablation studies to measure the individual impact of object counts, and locations.
",1 Introduction,[0],[0]
"• We show that a model relying only on object annotations as opposed to pixel data, performs competitively in image captioning despite the ambiguity of the setup for this task.
",1 Introduction,[0],[0]
• We show that more accurate and comprehensive descriptions can be generated on the image captioning task by combining our OBJ2TEXT model using the outputs of a state-of-the-art object detector with a standard image captioning approach.,1 Introduction,[0],[0]
"We evaluate OBJ2TEXT in the task of object layout captioning, and image captioning.",2 Task,[0],[0]
"In the first task, the input is an object layout that takes the form of a set of object categories and bounding box pairs 〈o, l〉 = {〈oi, li〉}, and the output is natural language.",2 Task,[0],[0]
This task resembles the second task of image captioning except that the input is an object layout instead of a standard raster image represented as a pixel array.,2 Task,[0],[0]
We experiment in the MS-COCO dataset for both tasks.,2 Task,[0],[0]
"For the first task, object layouts are derived from ground-truth bounding box annotations, and in the second task object layouts are obtained using the outputs of an object detector over the input image.",2 Task,[0],[0]
"Our work is related to previous works that used clipart scenes for visually-grounded tasks including sentence interpretation (Zitnick and Parikh, 2013; Zitnick et al., 2013), and predicting object dynamics (Fouhey and Zitnick, 2014).",3 Related Work,[0],[0]
"The cited advantage of abstract scene representations such as the ones provided by the clipart scenes dataset proposed in (Zitnick and Parikh, 2013) is their ability to separate the complexity of pattern recognition from semantic visual representation.",3 Related Work,[0],[0]
"Abstract scene representations also maintain
common-sense knowledge about the world.",3 Related Work,[0],[0]
"The works of Vedantam et al. (2015b); Eysenbach et al. (2016) proposed methods to learn common-sense knowledge from clipart scenes, while the method of Yatskar et al. (2016), similar to our work, leverages object annotations for natural images.",3 Related Work,[0],[0]
"Understanding abstract scenes has demonstrated to be a useful capability for both language and vision tasks and our work is another step in this direction.
",3 Related Work,[0],[0]
"Our work is also related to other language generation tasks such as image and video captioning (Farhadi et al., 2010; Ordonez et al., 2011; Mason and Charniak, 2014; Ordonez et al., 2015; Xu et al., 2015; Donahue et al., 2015; Mao et al., 2015; Fang et al., 2015).",3 Related Work,[0],[0]
"This problem is interesting because it combines two challenging but perhaps complementary tasks: visual recognition, and generating coherent language.",3 Related Work,[0],[0]
"Fueled by recent advances in training deep neural networks (Krizhevsky et al., 2012) and the availability of large annotated datasets with images and captions such as the MS-COCO dataset (Lin et al., 2014), recent methods on this task perform endto-end learning from pixels to text.",3 Related Work,[0],[0]
"Most recent approaches use a variation of an encoderdecoder model where a convolutional neural network (CNN) extracts visual features from the input image (encoder), and passes its outputs to a recurrent neural network (RNN) that generates a caption as a sequence of words (decoder) (Karpathy and Fei-Fei, 2015; Vinyals et al., 2015).",3 Related Work,[0],[0]
"However, the MS-COCO dataset, containing object annotations, is also a popular benchmark in computer vision for the task of object detection, where the objective is to go from pixels to a collection of object locations.",3 Related Work,[0],[0]
"In this paper, we instead frame our problem as going from a collection of object categories and locations (object layouts) to image captions.",3 Related Work,[0],[0]
"This requires proposing a novel encoding approach to encode these object layouts instead of pixels, and allows for analyzing the image captioning task from a different perspective.",3 Related Work,[0],[0]
"Several other recent works use a similar sequenceto-sequence approach to generate text from source code input (Iyer et al., 2016), or to translate text from one language to another (Bahdanau et al., 2015).
",3 Related Work,[0],[0]
There have also been a few previous works explicitly analyzing the role of spatial and geometric relations between objects for vision and language related tasks.,3 Related Work,[0],[0]
"The work of Elliott and Keller
(2013) manually defined a dictionary of objectobject relations based on geometric cues.",3 Related Work,[0],[0]
The work of Ramisa et al. (2015) is focused on predicting preposition given two entities and their locations in an image.,3 Related Work,[0],[0]
Previous works of Plummer et al. (2015) and Rohrbach et al. (2016) showed that switching from classification-based CNN network to detection-based Fast RCNN network improves performance for phrase localization.,3 Related Work,[0],[0]
The work of Hu et al. (2016) showed that encoding image regions with spatial information is crucial for natural language object retrieval as the task explicitly asks for locations of target objects.,3 Related Work,[0],[0]
"Unlike these previous efforts, our model is trained endto-end for the language generation task, and takes as input a holistic view of the scene layout, potentially learning higher order relations between objects.",3 Related Work,[0],[0]
"In this section we describe our base OBJ2TEXT model for encoding object layouts to produce text (section 4.1), as well as two further variations to use our model to generate captions for real images: OBJ2TEXT-YOLO which uses the YOLO object detector (Redmon and Farhadi, 2017) to generate layouts of object locations from real images (section 4.2), and OBJ2TEXT-YOLO + CNN-RNN which further combines the previous model with an encoder-decoder image captioning which uses a convolutional neural network to encode the image (section 4.3).",4 Model,[0],[0]
"OBJ2TEXT is a sequence-to-sequence model that encodes an input object layout as a sequence, and decodes a textual description by predicting the next word at each time step.",4.1 OBJ2TEXT,[0],[0]
"Given a training data set comprising N observations
{〈o(n), l(n)〉}, where 〈o(n), l(n)〉 is a pair of sequences of input category and location vectors, together with a corresponding set of target captions { s(n) } , the encoder and decoder are trained jointly by minimizing a loss function over the training set using stochastic gradient descent:
W ∗ = arg min W N∑ n=1 L(〈o(n), l(n)〉, s(n)), (1)
in which W = ( W1 W2 ) is the group of encoder parameters W1 and decoder parameters W2.",4.1 OBJ2TEXT,[0],[0]
"The loss
function is a negative log likelihood function of the generated description given the encoded object layout
L(〈o(n), l(n)〉, s(n))",4.1 OBJ2TEXT,[0],[0]
=,4.1 OBJ2TEXT,[0],[0]
"− log p(sn|hnL, W2), (2) where hnL is computed using the LSTM-based encoder (eqs. 3, and 4) from the object layout inputs 〈o(n), l(n)〉, and p(sn|hnL, W2) is computed using the LSTM-based decoder (eqs. 5, 6 and 7).
",4.1 OBJ2TEXT,[0],[0]
"At inference time we encode an input layout 〈o, l〉 into its representation hL, and sample a sentence word by word based on p(st|hL, s<t) as computed by the decoder in time-step t. Finding the optimal sentence s∗ = arg maxs p(s|hL) requires the evaluation of an exponential number of sentences as in each time-step we have K number of choices for a word vocabulary of size K.",4.1 OBJ2TEXT,[0],[0]
"As a common practice for an approximate solution, we follow (Vinyals et al., 2015) and use beam search to limit the choices for words at each time-step by only using the ones with the highest probabilities.",4.1 OBJ2TEXT,[0],[0]
"Encoder: The encoder at each time-step t takes as input a pair 〈ot, lt〉, where ot is the object category encoded as a one-hot vector of size V , and lt =",4.1 OBJ2TEXT,[0],[0]
"[Bxt , B y t , B w t , B h t ] is the location configuration vector that contains left-most position, topmost position, and the width and height of the bounding box corresponding to object ot, all normalized in the range [0,1] with respect to input image dimensions.",4.1 OBJ2TEXT,[0],[0]
"ot and lt are mapped to vectors with the same size k and added to form the input xt to one time-step of the LSTM-based encoder as follows:
xt = Woot",4.1 OBJ2TEXT,[0],[0]
"+ (Wllt + bl),",4.1 OBJ2TEXT,[0],[0]
"xt ∈ Rk, (3) in which Wo ∈ Rk×V is a categorical embedding matrix (the word encoder), and Wl ∈ Rk×4 and bias",4.1 OBJ2TEXT,[0],[0]
bl ∈,4.1 OBJ2TEXT,[0],[0]
"Rk are parameters of a linear transformation unit (the object location encoder).
",4.1 OBJ2TEXT,[0],[0]
"Setting initial value of cell state vector ce0 = 0 and hidden state vector he0 = 0, the LSTM-based encoder takes the sequence of input (x1, ..., xT1) and generates a sequence of hidden state vectors (he1, ..., h e T1
) using the following step function (we omit cell state variables and internal transition gates for simplicity as we use a standard LSTM cell definition):
het = LSTM(h e t−1, xt; W1).",4.1 OBJ2TEXT,[0],[0]
"(4)
We use the last hidden state vector hL = heT1 as the encoded representation of the input layout 〈ot, lt〉 to generate the corresponding description s.
Decoder: The decoder takes the encoded layout hL as input and generates a sequence of multinomial distributions over a vocabulary of words using an LSTM neural language model.",4.1 OBJ2TEXT,[0],[0]
"The joint probability distribution of generated sentence s = (s1, ..., sT2) is factorized into products of conditional probabilities:
p(s|hL)",4.1 OBJ2TEXT,[0],[0]
"= T2∏ t=1 p(st|hL, s<t), (5)
where each factor is computed using a softmax function over the hidden states of the decoder LSTM as follows:
p(st|hL, s<t) = softmax(Whhdt−1 + bh), (6)
hdt = LSTM(h d t−1, Wsst; W2), (7)
where Ws is the categorical embedding matrix for the one-hot encoded caption sequence of symbols.",4.1 OBJ2TEXT,[0],[0]
"By setting hd−1 = 0 and cd−1 = 0 for the initial hidden state and cell state, the layout representation is encoded into the decoder network at the 0 time step as a regular input:
hd0 = LSTM(h d −1, hL; W2).",4.1 OBJ2TEXT,[0],[0]
"(8)
We use beam search to sample from the LSTM as is routinely performed in previous literature in order to generate text.",4.1 OBJ2TEXT,[0],[0]
For the task of image captioning we propose OBJ2TEXT-YOLO.,4.2 OBJ2TEXT-YOLO,[0],[0]
"This model takes an image as input, extracts an object layout (object categories and locations) with a state-of-the-art object detection model YOLO (Redmon and Farhadi, 2017), and uses OBJ2TEXT as described in section 4.1 to generate a natural language description of the input layout and hence, the input image.",4.2 OBJ2TEXT-YOLO,[0],[0]
"The model is trained using the standard back-propagation algorithm, but the error is not back-propagated to the object detection module.",4.2 OBJ2TEXT-YOLO,[0],[0]
"For the image captioning task we experiment with a combined model (see Figure 2) where we take an image as input, and then use two separate computation branches to extract visual feature information and object layout information.",4.3 OBJ2TEXT-YOLO + CNN-RNN,[0],[0]
These two streams of information are then passed to an LSTM neural language model to generate a description.,4.3 OBJ2TEXT-YOLO + CNN-RNN,[0],[0]
"Visual features are extracted using the
VGG-16 (Simonyan and Zisserman, 2015) convolutional neural network pre-trained on the ImageNet classification task (Russakovsky et al., 2015).",4.3 OBJ2TEXT-YOLO + CNN-RNN,[0],[0]
Object layouts are extracted using the YOLO object detection system and its output object locations are encoded using our proposed OBJ2TEXT encoder.,4.3 OBJ2TEXT-YOLO + CNN-RNN,[0],[0]
These two streams of information are encoded into vectors of the same size and their sum is input to the language model to generate a textual description.,4.3 OBJ2TEXT-YOLO + CNN-RNN,[0],[0]
The model is trained using the standard back-propagation algorithm where the error is back-propagated to both branches but not the object detection module.,4.3 OBJ2TEXT-YOLO + CNN-RNN,[0],[0]
The weights of the image CNN model are fine-tuned only after the layout encoding branch is well trained but no significant overall performance improvements were observed.,4.3 OBJ2TEXT-YOLO + CNN-RNN,[0],[0]
"We evaluate the proposed models on the MSCOCO (Lin et al., 2014) dataset which is a popular image captioning benchmark that also contains object extent annotations.",5 Experimental Setup,[0],[0]
"In the object layout captioning task the model uses the groundtruth object extents as input object layouts, while in the image captioning task the model takes raw images as input.",5 Experimental Setup,[0],[0]
The qualities of generated descriptions are evaluated using both human evaluations and automatic metrics.,5 Experimental Setup,[0],[0]
"We train and validate our models based on the commonly adopted split regime (113,287 training images, 5000 validation and 5000 test images) used in (Karpathy et al., 2016), and also test our model in the MSCOCO official test benchmark.
",5 Experimental Setup,[0],[0]
"We implement our models based on the open source image captioning system Neuraltalk2 (Karpathy et al., 2016).",5 Experimental Setup,[0],[0]
Other configurations including data preprocessing and training hyper-parameters also follow Neuraltalk2.,5 Experimental Setup,[0],[0]
"We trained our models using a GTX1080 GPU with 8GB of memory for 400k iterations using a batch
size of 16 and an Adam optimizer with alpha of 0.8, beta of 0.999 and epsilon of 1e-08.",5 Experimental Setup,[0],[0]
"Descriptions of the CNN-RNN approach are generated using the publicly available code and model checkpoint provided by Neuraltalk2 (Karpathy et al., 2016).",5 Experimental Setup,[0],[0]
"Captions for online test set evaluations are generated using beam search of size 2, but score histories on split validation set are based on captions generated without beam search (i.e. max sampling at each time-step).
",5 Experimental Setup,[0],[0]
Ablation on Object Locations and Counts:.,5 Experimental Setup,[0],[0]
"We setup an experiment where we remove the input locations from the OBJ2TEXT encoder to study the effects on the generated captions, and confirm whether the model is actually using spatial information during surface realization.",5 Experimental Setup,[0],[0]
In this restricted version of our model the LSTM encoder at each time step only takes the object category embedding vector as input.,5 Experimental Setup,[0],[0]
"The OBJ2TEXT model additionally encodes different instances of the same object category in different time steps, potentially encoding in some of its hidden states information about how many objects of a particular class are in the image.",5 Experimental Setup,[0],[0]
"For example, in the object annotation presented in the input in Figure 1, there are two instances of “person”.",5 Experimental Setup,[0],[0]
"We perform an additional experiment where our model does not have access neither to object locations, nor the number of object instances by providing only a set of object categories.",5 Experimental Setup,[0],[0]
"Note that in this set of experiments the object layouts are given as inputs, thus we assume full access to ground-truth object annotations, even in the test split.",5 Experimental Setup,[0],[0]
"In the experimental results section we use the “-GT” postfix to indicate that input object layouts are obtained from ground-truth object annotations provided by the MS-COCO dataset.
",5 Experimental Setup,[0],[0]
"Image Captioning Experiment: In this experiment we assess whether the image captioning model OBJ2TEXT-YOLO that only relies on object categories and locations could give comparable performance with a CNN-RNN model based on Neuraltalk2 (Karpathy et al., 2016) that has full access to visual image features.",5 Experimental Setup,[0],[0]
"We also explore how much does a combined OBJ2TEXT-YOLO + CNN-RNN model could improve over a CNNRNN model by fusing object counts and location information that is not explicitly encoded in a traditional CNN-RNN approach.
",5 Experimental Setup,[0],[0]
Human Evaluation Protocol.,5 Experimental Setup,[0],[0]
"We use a twoalternative forced-choice evaluation (2AFC) ap-
proach to compare two methods that generate captions.",5 Experimental Setup,[0],[0]
"For this, we setup a task on Amazon Mechanical Turk where users are presented with an image and two alternative captions, and they have to choose the caption that best describes the image.",5 Experimental Setup,[0],[0]
"Users are not prompted to use any single criteria but rather a holistic assessment of the captions, including their semantics, syntax, and the degree to which they describe the image content.",5 Experimental Setup,[0],[0]
"In our experiment we randomly sample 500 captions generated by various models for MS COCO online test set images, and use three users per image to obtain annotations.",5 Experimental Setup,[0],[0]
Note that three users choosing randomly between two options have a chance of 25% to select the same caption for a given image.,5 Experimental Setup,[0],[0]
"In our experiments comparing method A vs method B, we report the percentage of times A was picked over B (Choice-all), the percentage of times all users selected the same method, either A or B, (Agreement), and the percentage of times A was picked over B only for these cases where all users agreed (Choice-agreement).",5 Experimental Setup,[0],[0]
"Impact of Object Locations and Counts: Figure 3a shows the CIDEr (Vedantam et al., 2015a), and BLEU-4 (Papineni et al., 2002) score history on our validation set during 400k iterations of training of OBJ2TEXT, as well as a version of our model that does not use object locations, and a version of our model that does not use neither object locations nor object counts.",6 Results,[0],[0]
"These results show that our model is effectively using both object locations and counts to generate better captions, and absence of any one of these two cues affects performance.",6 Results,[0],[0]
"Table 1 confirms these results on the test split after a full round of training.
",6 Results,[0],[0]
"Furthermore, human evaluation results in the first row of Table 2 show that the OBJ2TEXT model with access to object locations is preferred by users, especially in cases where all evaluators agreed on their choice (62% over the baseline that does not have access to locations).",6 Results,[0],[0]
In Figure 4 we additionally present qualitative examples showing predictions side-by-side between OBJ2TEXT-GT and OBJ2TEXT-GT (no obj-locations).,6 Results,[0],[0]
"These results indicate that 1) perhaps not surprisingly, object counts is useful for generating better quality descriptions, and 2) object location information when properly encoded, is an important cue for generating more accurate descriptions.",6 Results,[0],[0]
"We ad-
ditionally implemented a nearest neighbor baseline by representing the objects in the input layout using an orderless bag-of-words representation of object counts and the CIDEr score on the test split was only 0.387.
",6 Results,[0],[0]
"On top of OBJ2TEXT we additionally experimented with the global attention model proposed in (Luong et al., 2015) so that a weighted combination of the encoder hidden states are forwarded to the decoding neural language model, however we did not notice any overall gains in terms of accuracy from this formulation.",6 Results,[0],[0]
"We observed that this model provided gains only for larger input sequences where it is more likely that the LSTM network forgets its past history (Bahdanau et al., 2015).",6 Results,[0],[0]
"However in MS-COCO the average number of objects in each image is rather modest, so the last hidden state can capture well the overall nuances of the visual input.
",6 Results,[0],[0]
Object Layout Encoding for Image Captioning:,6 Results,[0],[0]
"Figure 3b shows the CIDEr, and BLEU-4 score history on the validation set during 400k iterations of training of OBJ2TEXT-YOLO, CNN-RNN, and their combination.",6 Results,[0],[0]
"These results show that OBJ2TEXT-YOLO performs surprisingly close to CNN-RNN, and the model resulting from combining the two, clearly outperforms each method alone.",6 Results,[0],[0]
"Table 3 shows MS-COCO evaluation results on the test set using their online benchmark service, and confirms results obtained in the validation split, where CNN-RNN seems to have only a slight edge over OBJ2TEXT-YOLO which lacks access to pixel data after the object detection stage.",6 Results,[0],[0]
"Human evaluation results in Table 2 rows 2, and 3, further confirm these findings.",6 Results,[0],[0]
"These results show that meaningful descriptions could be generated solely based on object categories and locations information, even without access to color and texture input.
",6 Results,[0],[0]
"The combined model performs better than the two models, improving the CIDEr score of the basic CNN-RNN model from 0.863 to 0.950, and human evaluation results show that the combined model is preferred over the basic CNNRNN model for 65.3% of the images for which all evaluators were in agreement about the selected method.",6 Results,[0],[0]
"These results show that explicitly encoded object counts and location information, which is often overlooked in traditional image captioning approaches, could boost the performance of existing models.",6 Results,[0],[0]
"Intuitively, object lay-
out and visual features are complementary: neural network models for visual feature extraction are trained on a classification task where object-level information such as number of instances and locations are ignored in the objective.",6 Results,[0],[0]
"Object layouts on the other hand, contain categories and their bounding-boxes but don’t have access to rich image features such as image background, object attributes and objects with categories not present in the object detection vocabulary.
",6 Results,[0],[0]
"Figure 5 provides a three-way comparison of captions generated by the three image captioning models, with preferred captions by human evaluators annotated in bold text.",6 Results,[0],[0]
Analysis on actual outputs gives us insights into the benefits of combing object layout information and visual features obtained using a CNN.,6 Results,[0],[0]
"Our OBJ2TEXT-YOLO model makes many mistakes because of lack of image context information since it only has access to object layout, while CNN-RNN makes many mistakes because the visual recognition model is imperfect at predicting the correct content.",6 Results,[0],[0]
"The combined model is usually able to generate more accurate and comprehensive descriptions.
",6 Results,[0],[0]
"In this work we only explored encoding spatial information with object labels, but object la-
bels could be readily augmented with rich semantic features that are more detailed descriptions of objects or image regions.",6 Results,[0],[0]
"For example, the work of You et al. (2016) and Yao et al. (2016b) showed that visual features trained with semantic concepts (text entities mentioned in captions) instead of object labels is useful for image captioning, although they didn’t consider encoding semantic concepts with spatial information.",6 Results,[0],[0]
"In case of object annotations the MS-COCO dataset only provides object labels and bounding-boxes, but there are other datasets such as Flick30K Entities (Plummer et al., 2015), and the Visual Genome dataset (Krishna et al., 2017) that provide richer region-tophrase correspondence annotations.",6 Results,[0],[0]
"In addition, the fusion of object counts and spatial information with CNN visual features could in principle benefit other vision and language tasks such as visual question answering.",6 Results,[0],[0]
We leave these possible extensions as future work.,6 Results,[0],[0]
"We introduced OBJ2TEXT, a sequence-tosequence model to generate visual descriptions for object layouts where only categories and locations are specified.",7 Conclusion,[0],[0]
"Our proposed model
shows that an orderless visual input representation of concepts is not enough to produce good descriptions, but object extents, locations, and object counts, all contribute to generate more accurate image descriptions.",7 Conclusion,[0],[0]
"Crucially we show that our encoding mechanism is able to capture useful spatial information using an LSTM network to produce image descriptions, even when the input is provided as a sequence rather than as an explicit 2D representation of objects.",7 Conclusion,[0],[0]
"Additionally, using
our proposed OBJ2TEXT model in combination with an existing image captioning model and a robust object detector we showed improved results in the task of image captioning.",7 Conclusion,[0],[0]
This work was supported in part by an NVIDIA Hardware Grant.,Acknowledgments,[0],[0]
"We are also thankful for the feedback from Mark Yatskar and anonymous reviewers of this paper.
",Acknowledgments,[0],[0]
"7/16/2017 vision.cs.virginia.edu:8001
http://vision.cs.virginia.edu:8001/ 1/1
(a) a yellow fire hydrant sitting on the side of a road (b) a man is standing in the snow with a snowboard (c) a fire hydrant in the snow near a road (a) a small boat in a body of water (b) a boat with a bunch of people on it (c) a boat is docked in a large body of water (a) a man and a woman are sitting on a bench (b) a man and a woman sitting on a couch (c) a man and woman are talking on their cell phones (a) a man sitting on a bench in a park (b) a woman sitting on a bench with a cell phone (c) a woman sitting on a bench with her legs crossed (a) a bird sitting on a tree branch in a tree (b) a bird sitting on a branch in a tree (c) two birds are sitting on a tree branch (a) a zebra standing in a field of grass (b) a man riding a wave on top of a surfboard (c) a zebra standing in the water near a rock wall
(a) a chair and a table in a room (b) a pile of luggage sitting on top of a wooden floor (c) a room with a table and chairs and a suitcase (a) a white plate topped with meat and vegetables (b) a white plate topped with meat and vegetables (c) a plate of food with meat and vegetables (a) a herd of cattle grazing on a lush green field (b) a herd of elephants walking across a river (c) a group of cows standing in a river (a) a man is swinging a bat at a ball (b) a man is playing with a frisbee in a park (c) a man is swinging a bat in a field (a) a little girl sitting at a table with a cake (b) a bride and groom cutting their wedding cake (c) a bride and groom cutting their wedding cake (a) a street sign with a street name sign on it (b) a large clock tower towering over a city
(c) a large body of water with a clock tower in the background
(a) a couple of giraffe standing next to each other (b) a giraffe is standing in a tree in a forest (c) two giraffes standing next to each other in a tree (a) a group of people standing around a pizza (b) a man holding a box of food in front of him (c) a couple of people that are holding a pizza (a) a group of people playing a game with remote controllers (b) a man and a woman playing a video game (c) a group of people sitting around a living room (a) a man is standing in the middle of a street (b) a man sitting on a beach with a surfboard (c) a man and woman sitting on the beach (a) a glass vase with some flowers in it (b) a vase filled with flowers on top of a table (c) a vase filled with flowers on top of a table (a) a man and a woman sitting at a table eating pizza (b) a woman is taking a picture of herself in a mirror (c) two women sitting at a table with a pizza
(a) a man sitting in a kitchen next to a woman (b) a woman standing in front of a counter in a kitchen (c) two women in a kitchen preparing food on a table (a) a man and a woman standing next to each other (b) a man in a suit and tie standing in a room (c) a man in a suit and tie standing next to a man (a) a man riding a bike down a street (b) a man riding a motorcycle down a street (c) a man riding a bike with a helmet on his head (a) a woman is playing tennis on a court (b) a woman is playing tennis on a tennis court (c) a tennis player in action on the court (a) a woman in a bathroom with a sink and a mirror (b) a man standing in a kitchen next to a stove (c) a man standing in a kitchen next to a counter (a) a man holding a nintendo wii game controller (b) a young boy sitting on a couch holding a remote control (c) a young child is holding a toy in his hand
(a) a person riding a horse on a beach (b) a woman is riding a horse in a field (c) a girl is riding a horse in a field (a) a bunch of vases sitting on a shelf (b) a bunch of flowers in a vase on a table (c) a bunch of colorful vases sitting on a table (a) a man riding a bike down a street next to tall buildings (b) a man is on a boat in the water (c) a couple of people standing on top of a bridge (a) a man riding a skateboard with a dog (b) a dog is riding a skateboard on a street (c) a dog on a skateboard in the middle of the street (a) a young boy in a baseball uniform holding a glove
(b) a man is sitting on the ground holding a skateboard (c) a man sitting on the ground with a baseball glove
(a) a woman holding a tennis racquet on a court (b) a dog with a tennis racket in a basket (c) a person holding a tennis racket in a park
Figure 5:",Acknowledgments,[0],[0]
"Qualitative examples comparing the generated captions of (a) OBJ2TEXT-YOLO, (b) CNNRNN and (c)",Acknowledgments,[0],[0]
OBJ2TEXT-YOLO + CNN-RNN.,Acknowledgments,[0],[0]
Images are selected from the 500 human evaluation images and annotated with YOLO object detection results.,Acknowledgments,[0],[0]
Captions preferred by human evaluators with agreement are highlighted in bold text.,Acknowledgments,[0],[0]
Generating captions for images is a task that has recently received considerable attention.,abstractText,[0],[0]
"In this work we focus on caption generation for abstract scenes, or object layouts where the only information provided is a set of objects and their locations.",abstractText,[0],[0]
"We propose OBJ2TEXT, a sequence-tosequence model that encodes a set of objects and their locations as an input sequence using an LSTM network, and decodes this representation using an LSTM language model.",abstractText,[0],[0]
"We show that our model, despite encoding object layouts as a sequence, can represent spatial relationships between objects, and generate descriptions that are globally coherent and semantically relevant.",abstractText,[0],[0]
We test our approach in a task of object-layout captioning by using only object annotations as inputs.,abstractText,[0],[0]
"We additionally show that our model, combined with a state-of-the-art object detector, improves an image captioning model from 0.863 to 0.950 (CIDEr score) in the test benchmark of the standard MS-COCO Captioning task.",abstractText,[0],[0]
OBJ2TEXT: Generating Visually Descriptive Language from Object Layouts,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 444–451 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
"Visual Reasoning (Antol et al., 2015; Andreas et al., 2016; Bisk et al., 2016; Johnson et al., 2017) requires a sophisticated understanding of the compositional language instruction and its relationship with the corresponding image.",1 Introduction,[0],[0]
Suhr et al. (2017) recently proposed a challenging new NLVR task and dataset in this direction with natural and complex language statements that have to be classified as true or false given a multi-image set (shown in Fig. 1).,1 Introduction,[0],[0]
"Specifically, each task instance consists of an image with three sub-images and a statement which describes the image.",1 Introduction,[0],[0]
"The model is asked to answer the question whether the given statement is consistent with the image or not.
",1 Introduction,[0],[0]
"To solve the task, the designed model needs to fuse the information from two different domains,
the visual objects and the language, and learn accurate relationships between the two.",1 Introduction,[0],[0]
Another difficulty is that the objects in the image do not have a fixed order and the number of objects also varies.,1 Introduction,[0],[0]
"Moreover, each statement reasons for truth over three sub-images (instead of the usual single image setup), which also breaks most of the existing models.",1 Introduction,[0],[0]
"In our paper, we introduce a novel end-to-end model to address these three problems, leading to strong gains over the previous best model.",1 Introduction,[0],[0]
"Our pointer network based LSTM-RNN sorts and learns recurrent representations of the objects in each sub-image, so as to match it better with the order of the phrases in the language statement.",1 Introduction,[0],[0]
"For this, it employs an RL-based policy gradient method with a reward extracted from the subsequent comprehension model.",1 Introduction,[0],[0]
"With these strong representations of the visual objects and the statement units, a joint-bidirectional attention flow model builds consistent, two-way matchings between the representations in different domains.",1 Introduction,[0],[0]
"Finally, since the scores computed by the bidirectional attention are about the three sub-images, a pooling combination layer over the three subimage representations is required to give the final score of the whole image.
",1 Introduction,[0],[0]
"On the structured-object-representation version of the dataset, our pointer-based, end-to-end bidi-
444
rectional attention model achieves an accuracy of 73.9%, outperforming the previous (end-to-end) state-of-the-art method by 6.2% absolute, where both the pointer network and the bidirectional attention modules contribute significantly.",1 Introduction,[0],[0]
"We also contribute several other strong baselines for this new NLVR task based on Relation Networks (Santoro et al., 2017) and BiDAF (Seo et al., 2016).",1 Introduction,[0],[0]
"Furthermore, we also show the result of our joint bidirectional attention model on the raw-image version (with pixel-level, spatial-filter CNNs) of the NLVR dataset, where our model achieves an accuracy of 69.7% and outperforms the previous best result by 3.6%.",1 Introduction,[0],[0]
"On the unreleased leaderboard test set, our model achieves an accuracy of 71.8% and 66.1% on the structured and raw-image versions, respectively, leading to 4% absolute improvements on both tasks.",1 Introduction,[0],[0]
"Besides the NLVR corpus with a focus on complex and natural compositional language (Suhr et al., 2017), other useful visual reasoning datasets have been proposed for navigation and assembly tasks (MacMahon et al., 2006; Bisk et al., 2016), as well as for visual Q&A tasks which focus more on complex real-world images (Antol et al., 2015; Johnson et al., 2017).",2 Related work,[0],[0]
"Specifically for the NLVR dataset, previous models have incorporated property- and count-based features of the objects and the language (Suhr et al., 2017), or extra semantic parsing (logical form) annotations (Goldman et al., 2017) – we focus on end-toend models for this visual reasoning task.
",2 Related work,[0],[0]
"Attention mechanism (Bahdanau et al., 2014; Luong et al., 2015; Xu et al., 2015) has been widely used for conditioned language generation tasks.",2 Related work,[0],[0]
"It is further used to learn alignments between different modalities (Lu et al., 2016; Wang and Jiang, 2016; Seo et al., 2016; Andreas et al., 2016; Chaplot et al., 2017).",2 Related work,[0],[0]
"In our work, a bidirectional attention mechanism is used to learn a joint representation of the visual objects and the words by building matchings between them.
",2 Related work,[0],[0]
"Pointer network (Vinyals et al., 2015) was introduced to learn the conditional probability of an output sequence.",2 Related work,[0],[0]
Bello et al. (2016) extended this to near-optimal combinatorial optimization via reinforcement learning.,2 Related work,[0],[0]
"In our work, a policy gradient based pointer network is used to “sort” the objects conditioned on the statement, such that the sequence of ordered objects is sent to the subse-
quent comprehension model for a reward.",2 Related work,[0],[0]
"The training datum for this task consists of the statement s, the structured-representation objects o in the image I , and the ground truth label y (which is 1 for true and 0 for false).",3 Model,[0],[0]
"Our BiATTPointer model (shown in Fig. 2) for the structuredrepresentation task uses the pointer network to sort the object sequence (optimized by policy gradient), and then uses the comprehension model to calculate the probability P (s, o) of the statement s being consistent with the image.",3 Model,[0],[0]
"Our CNNBiATT model for the raw-image I dataset version is similar but learns the structure directly via pixellevel, spatial-filter CNNs – details in Sec. 5 and the appendix.",3 Model,[0],[0]
"In the remainder of this section, we first describe our BiATT comprehension model and then the pointer network.",3 Model,[0],[0]
"We use one bidirectional LSTM-RNN (Hochreiter and Schmidhuber, 1997) (denoted by LANGLSTM) to read the statement s = w1, w2, . . .",3.1 Comprehension Model with Joint Bidirectional Attention,[0],[0]
", wT, and output the hidden state representations {hi}.",3.1 Comprehension Model with Joint Bidirectional Attention,[0],[0]
A word embedding layer is added before the LSTM to project the words to high-dimension vectors {w̃i}.,3.1 Comprehension Model with Joint Bidirectional Attention,[0],[0]
"h1,h2, . .",3.1 Comprehension Model with Joint Bidirectional Attention,[0],[0]
.,3.1 Comprehension Model with Joint Bidirectional Attention,[0],[0]
", hT = LSTM (w̃1, w̃2, . . .",3.1 Comprehension Model with Joint Bidirectional Attention,[0],[0]
", w̃T) (1)
The raw features of the objects in the j-th subimage are {ojk} (since the NLVR dataset has 3 subimages per task).",3.1 Comprehension Model with Joint Bidirectional Attention,[0],[0]
A fully-connected (FC) layer without nonlinearity projects the raw features to object embeddings {ejk}.,3.1 Comprehension Model with Joint Bidirectional Attention,[0],[0]
"We then go through all the objects in random order (or some learnable order, e.g., via our pointer network, see Sec. 3.2) by another bidirectional LSTM-RNN (denoted by OBJ-LSTM), whose output is a sequence of vectors {gjk}which is used as the (left plus right memory) representation of the objects (the objects in different sub-images are handled separately):
ejk = W o",3.1 Comprehension Model with Joint Bidirectional Attention,[0],[0]
j k + b,3.1 Comprehension Model with Joint Bidirectional Attention,[0],[0]
"(2)
gj1, g j 2, . . .",3.1 Comprehension Model with Joint Bidirectional Attention,[0],[0]
", g j Nj = LSTM (e j 1, e j 2, . . .",3.1 Comprehension Model with Joint Bidirectional Attention,[0],[0]
", e j Nj ) (3)
where Nj is the number of the objects in jth subimage.",3.1 Comprehension Model with Joint Bidirectional Attention,[0],[0]
"Now, we have two vector sequences for the representations of the words and the objects, using which the bidirectional attention then calculates the score measuring the correspondence be-
tween the statement and the image’s object structure.",3.1 Comprehension Model with Joint Bidirectional Attention,[0],[0]
"To simplify the notation, we will ignore the sub-image index j. We first merge the LANGLSTM hidden outputs {hi} and the object-aware context vectors {ci} together to get the joint representation {ĥi}.",3.1 Comprehension Model with Joint Bidirectional Attention,[0],[0]
"The object-aware context vector ci for a particular word wi is calculated based on the bilinear attention between the word representation hi and the representations of the objects {gk}:
αi,k = softmaxk (h ᵀ i B1 gk) (4)
ci = ∑
k
αi,k · gk (5)
ĥi",3.1 Comprehension Model with Joint Bidirectional Attention,[0],[0]
"= relu (WLANG [hi; ci; hi−ci; hi◦ci]) (6) where the symbol ◦ denotes element-wise multiplication.
",3.1 Comprehension Model with Joint Bidirectional Attention,[0],[0]
Improvement over BiDAF The BiDAF model of Seo et al. (2016) does not use a full objectto-words attention mechanism.,3.1 Comprehension Model with Joint Bidirectional Attention,[0],[0]
The query-todocument attention module in BiDAF added the attended-context vector to the document representation instead of the query representation.,3.1 Comprehension Model with Joint Bidirectional Attention,[0],[0]
"However, the inverse attention from the objects to the words is important in our task because the representation of the object depends on its corresponding words.",3.1 Comprehension Model with Joint Bidirectional Attention,[0],[0]
"Therefore, different from the BiDAF model, we create an additional ‘symmetric’ attention to merge the OBJ-LSTM hidden outputs {gk} and the statement-aware context vectors {dk} together to get the joint representation {ĝk}.",3.1 Comprehension Model with Joint Bidirectional Attention,[0],[0]
"The improvement (6.1%) of our BiATT model over the BiDAF model is shown in Table 1.
",3.1 Comprehension Model with Joint Bidirectional Attention,[0],[0]
"βk,i = softmaxi",3.1 Comprehension Model with Joint Bidirectional Attention,[0],[0]
"( gᵀk B2 hi ) (7)
dk = ∑
i
βk,i · hi (8)
ĝk = relu",3.1 Comprehension Model with Joint Bidirectional Attention,[0],[0]
"(WOBJ [gk; dk; gk−dk; gk◦dk]) (9)
These above vectors {ĥi} and {ĝk} are the representations of the words and the objects which
are aware of each other bidirectionally.",3.1 Comprehension Model with Joint Bidirectional Attention,[0],[0]
"To make the final decision, two additional bidirectional LSTM-RNNs are used to further process the above attention-based representations via an additional memory-based layer.",3.1 Comprehension Model with Joint Bidirectional Attention,[0],[0]
"Lastly, two max pooling layers over the hidden output states create two single-vector outputs for the statement and the sub-image, respectively:
h̄1, h̄2, . . .",3.1 Comprehension Model with Joint Bidirectional Attention,[0],[0]
", h̄T = LSTM(ĥ1, ĥ2, . . .",3.1 Comprehension Model with Joint Bidirectional Attention,[0],[0]
", ĥT)",3.1 Comprehension Model with Joint Bidirectional Attention,[0],[0]
"(10)
ḡ1, ḡ2, . . .",3.1 Comprehension Model with Joint Bidirectional Attention,[0],[0]
", ḡN = LSTM(ĝ1, ĝ2, . . .",3.1 Comprehension Model with Joint Bidirectional Attention,[0],[0]
", ĝN) (11)
h̄ = ele max i
{ h̄i }
(12)
ḡ = ele max k {ḡk} (13)
where the operator ele max denotes the elementwise maximum over the vectors.",3.1 Comprehension Model with Joint Bidirectional Attention,[0],[0]
"The final scalar score for the sub-image is given by a 2-layer MLP over the concatenation of h̄ and ḡ as follows:
score = W2 tanh ( W1[h̄; ḡ] + b1 ) (14)
Max-Pooling over Sub-Images In order to address the 3 sub-images present in each NLVR task, a max-pooling layer is used to combine the above-defined scores of the sub-images.",3.1 Comprehension Model with Joint Bidirectional Attention,[0],[0]
"Given that the sub-images do not have any specific ordering among them (based on the data collection procedure (Suhr et al., 2017)), a pooling layer is suitable because it is permutation invariant.",3.1 Comprehension Model with Joint Bidirectional Attention,[0],[0]
"Moreover, many of the statements are about the existence of a special object or relationship in one sub-image (see Fig. 1) and hence the max-pooling layer effectively captures the meaning of these statements.",3.1 Comprehension Model with Joint Bidirectional Attention,[0],[0]
"We also tried other combination methods (meanpooling, concatenation, LSTM, early pooling on the features/vectors, etc.); the max pooling (on scores) approach was the simplest and most effective method among these (based on the dev set).
",3.1 Comprehension Model with Joint Bidirectional Attention,[0],[0]
"The overall probability that the statement correctly describes the full image (with three subimages) is the sigmoid of the final max-pooled
score.",3.1 Comprehension Model with Joint Bidirectional Attention,[0],[0]
"The loss of the comprehension model is the negative log probability (i.e., the cross entropy):
P (s, o) =σ",3.1 Comprehension Model with Joint Bidirectional Attention,[0],[0]
"( max j scorej )
(15)
L(s, o, y) =",3.1 Comprehension Model with Joint Bidirectional Attention,[0],[0]
"− y logP (s, o)",3.1 Comprehension Model with Joint Bidirectional Attention,[0],[0]
"− (1− y) log(1− P (s, o))",3.1 Comprehension Model with Joint Bidirectional Attention,[0],[0]
"(16)
where y is the ground truth label.",3.1 Comprehension Model with Joint Bidirectional Attention,[0],[0]
"Instead of randomly ordering the objects, humans look at the objects in an appropriate order w.r.t.",3.2 Pointer Network,[0],[0]
their reading of the given statement and after the first glance of the image.,3.2 Pointer Network,[0],[0]
"Following this idea, we use an additional pointer network (Vinyals et al., 2015) to find the best object ordering for the subsequent language comprehension model.",3.2 Pointer Network,[0],[0]
"The pointer network contains two RNNs, the encoder and the decoder.",3.2 Pointer Network,[0],[0]
The encoder reads all the objects in a random order.,3.2 Pointer Network,[0],[0]
"The decoder then learns a permutation π of the objects’ indices, by recurrently outputting a distribution over the objects based on the attention over the encoder hidden outputs.",3.2 Pointer Network,[0],[0]
"At each time step, an object is sampled without replacement following this distribution.",3.2 Pointer Network,[0],[0]
"Thus, the pointer network models a distribution p(π | s, o) over all the permutations:
p(π | s, o) =",3.2 Pointer Network,[0],[0]
"∏
i
p (π(i) | π(< i), s, o) (17)
",3.2 Pointer Network,[0],[0]
"Furthermore, the appropriate order of the objects depends on the language statement, and hence the decoder importantly attends to the hidden outputs of the LANG-LSTM (see Eqn. 1).
",3.2 Pointer Network,[0],[0]
The pointer network is trained via reinforcement learning (RL) based policy gradient optimization.,3.2 Pointer Network,[0],[0]
"The RL loss LRL(s, o, y) is defined as the expected comprehension loss (expectation over the distribution of permutations):
LRL(s, o, y) =",3.2 Pointer Network,[0],[0]
"Eπ∼p(·|s,o)L(s, o[π], y) (18)
where o[π] denotes the permuted input objects for permutation π, and L is the loss function defined in Eqn. 16.",3.2 Pointer Network,[0],[0]
"Suppose that we sampled a permutation π∗ from the distribution p(π|s, o); then the above RL loss could be optimized via policy gradient methods (Williams, 1992).",3.2 Pointer Network,[0],[0]
"The reward R is the negative loss of the subsequent comprehension model L(s, o[π∗], y).",3.2 Pointer Network,[0],[0]
"A baseline b is subtracted from the reward to reduce the variance (we use the
self-critical baseline of Rennie et al. (2016)).",3.2 Pointer Network,[0],[0]
"The gradient of the loss LRL could then be approximated as:
R =− L(s, o[π∗], y) (19) ∇θLRL(s, o, y)",3.2 Pointer Network,[0],[0]
"≈ − (R− b)∇θ log p(π∗ | s, o)
+∇θL(s, o[π∗], y) (20)
",3.2 Pointer Network,[0],[0]
This overall BiATT-Pointer model (for the structured-representation task) is shown in Fig. 2.,3.2 Pointer Network,[0],[0]
"We evaluate our model on the NLVR dataset (Suhr et al., 2017), for both the structured and raw-image versions.",4 Experimental Setup,[0],[0]
All model tuning was performed on the dev set.,4 Experimental Setup,[0],[0]
"Given the fact that the dataset is balanced (the number of true labels and false labels are roughly the same), the accuracy of the whole corpus is used as the metric.",4 Experimental Setup,[0],[0]
"We only use the raw features of the statement and the objects with minimal standard preprocessing (e.g., tokenization and UNK replacement; see appendix for reproducibility training details).",4 Experimental Setup,[0],[0]
Results on Structured Representations Dataset: Table 1 shows our primary model results.,5 Results and Analysis,[0],[0]
"In terms of previous work, the state-of-the-art result for end-to-end models is ‘MAXENT’, shown in Suhr et al. (2017).1 Our proposed BiATT-Pointer model (Fig. 2) achieves a 6.2% improvement on the public test set and a 4.0% improvement on the unreleased test set over this SotA model.",5 Results and Analysis,[0],[0]
"To show the individual effectiveness of our BiATT and Pointer components, we also provide two ablation results: (1) the bidirectional attention BiATT model without the pointer network; and (2) our BiENC baseline model without any attention or the pointer mechanisms.",5 Results and Analysis,[0],[0]
"The BiENC model uses the similarity between the last hidden outputs of the LANGLSTM and the OBJ-LSTM as the score (Eqn. 14).
",5 Results and Analysis,[0],[0]
"Finally, we also reproduce some recent popular frameworks, i.e., Relationship Network (Santoro et al., 2017) and BiDAF model (Seo et al., 2016), which have been proven to be successful in other machine comprehension and visual reasoning tasks.",5 Results and Analysis,[0],[0]
The results of these models are weaker than our proposed model.,5 Results and Analysis,[0],[0]
"Reimplementation details are shown in the appendix.
",5 Results and Analysis,[0],[0]
"1There is also recent work by Goldman et al. (2017), who use extra, manually-labeled semantic parsing data to achieve a released/unreleased test accuracy of 80.4%/83.5%, resp.",5 Results and Analysis,[0],[0]
"Results on Raw Images Dataset: To further show the effectiveness of our BiATT model, we apply this model to the raw image version of the NLVR dataset, with minimal modification.",Negative Examples,[0],[0]
"We simply replace each object-related LSTM with a visual feature CNN that directly learns the structure via pixel-level, spatial filters (instead of a pointer network which addresses an unordered sequence of structured object representations).",Negative Examples,[0],[0]
"As shown in Table 1, this CNN-BiATT model outperforms the neural module networks (NMN) (Andreas et al., 2016) previous-best result by 3.6% on the public test set and 4.1% on the unreleased test set.",Negative Examples,[0],[0]
More details and the model figure are in the appendix.,Negative Examples,[0],[0]
Output Example Analysis:,Negative Examples,[0],[0]
"Finally, in Fig. 1, we show some output examples which were successfully solved by our BiATT-Pointer model but failed in our strong baselines.",Negative Examples,[0],[0]
The left two examples in Fig. 1 could not be handled by the BiENC model.,Negative Examples,[0],[0]
The right two examples are incorrect for the BiATT model without the ordering-based pointer network.,Negative Examples,[0],[0]
"Our model can quite successfully understand the complex meanings of the attributes and their relationships with the diverse objects, as well as count the occurrence of and reason over objects without any specialized features.
",Negative Examples,[0],[0]
"Next, in Fig. 3, we also show some negative examples on which our model fails to predict the correct answer.",Negative Examples,[0],[0]
"The top two examples involve com-
plex high-level phrases e.g., “touching any edge” or “touching the base”, which are hard for an endto-end model to capture, given that such statements are rare in the training data.",Negative Examples,[0],[0]
"Based on the result of the validation set, the max-pooling layer is selected as the combination method in our model.",Negative Examples,[0],[0]
The max-pooling layer will choose the highest score from the sub-images as the final score.,Negative Examples,[0],[0]
"Thus, the layer could easily handle statements about single-subimage-existence based reasoning (e.g., the 4 positively-classified examples in Fig. 1).",Negative Examples,[0],[0]
"However, the bottom two negatively-classified examples in Fig. 3 could not be resolved because of the limitation of the max-pooling layer on scenarios that consider multiple-subimage-existence.",Negative Examples,[0],[0]
"We did try multiple other pooling and combination methods, as mentioned in Sec. 3.1.",Negative Examples,[0],[0]
"Among these methods, the concatenation, early pooling and LSTM-fusion approaches might have the ability to solve these particular bottom-two failed statements.",Negative Examples,[0],[0]
"In our future work, we are addressing multiple types of pooling methods jointly.",Negative Examples,[0],[0]
We presented a novel end-to-end model with joint bidirectional attention and object-ordering pointer networks for visual reasoning.,6 Conclusion,[0],[0]
We evaluate our model on both the structured-representation and raw-image versions of the NLVR dataset and achieve substantial improvements over the previous end-to-end state-of-the-art results.,6 Conclusion,[0],[0]
We thank the anonymous reviewers for their helpful comments.,Acknowledgments,[0],[0]
"This work was supported by a Google Faculty Research Award, a Bloomberg Data Science Research Grant, an IBM Faculty Award, and NVidia GPU awards.",Acknowledgments,[0],[0]
"As shown in Fig. 4, we apply our BiATT model to the raw image dataset with minimal modification.",A.1 CNN-BiATT Model Details,[0],[0]
The visual input of the model for this task is changed from the unordered structured representation set of objects o to the raw image pixels I .,A.1 CNN-BiATT Model Details,[0],[0]
"Hence, we replace all object-related LSTMs (e.g., the OBJ-LSTM and the LSTM-RNN in the bidirectional attention in Fig. 2) with visual feature convolutional neural networks (CNNs) that directly learn the structure via pixel-level, spatial filters (instead of a pointer network which addresses an unordered sequence of structured object representations).
",A.1 CNN-BiATT Model Details,[0],[0]
"The training datum for the NLVR raw-image version consists of the statement s, the image I and the ground truth label y.",A.1 CNN-BiATT Model Details,[0],[0]
"The image I contains three sub-images x1, x2 and x3.",A.1 CNN-BiATT Model Details,[0],[0]
We will use x to indicate any sub-image.,A.1 CNN-BiATT Model Details,[0],[0]
The superscript which indicates the index of the sub-image is ignored to simplify the notation.,A.1 CNN-BiATT Model Details,[0],[0]
The representation of the statement {hi} is calculated by the LANGLSTM as before.,A.1 CNN-BiATT Model Details,[0],[0]
"For the image representation, we project the sub-image to a sequence of feature vectors (i.e., the feature map) {al : l = 1, . . .",A.1 CNN-BiATT Model Details,[0],[0]
", L} corresponding to the different image locations.",A.1 CNN-BiATT Model Details,[0],[0]
L = m × m is the size of the features and m is the width and height of the feature map.,A.1 CNN-BiATT Model Details,[0],[0]
"The projection consists of ResNet-V2-101 (He et al., 2016) and a following fully-connected (FC) layer.",A.1 CNN-BiATT Model Details,[0],[0]
"We only use the blocks in the ResNet before the average pooling layer and the output of the ResNet is a feature map of size m×m×2048.
f1, . . .",A.1 CNN-BiATT Model Details,[0],[0]
", fL = ResNet(x) (21)
al = relu(Wx fl + bx) (22)
",A.1 CNN-BiATT Model Details,[0],[0]
The joint-representation of the statement {ĥi} is the combination of the LANG-LSTM,A.1 CNN-BiATT Model Details,[0],[0]
"hidden output states {hi} and the image-aware context vectors {ci}:
αi,l = softmaxl",A.1 CNN-BiATT Model Details,[0],[0]
"(h ᵀ i B1 al) (23)
ci = ∑
l
αi,l · al (24)
ĥi",A.1 CNN-BiATT Model Details,[0],[0]
"= relu (WLANG [hi; ci; hi−ci; hi◦ci]) (25)
",A.1 CNN-BiATT Model Details,[0],[0]
"The joint-representation of the image {âl} is cal-
culated in the same way:
βl,i = softmaxi ( aᵀl B2 hi ) (26)
",A.1 CNN-BiATT Model Details,[0],[0]
"dl = ∑
i
βl,i · hi (27)
âl = relu",A.1 CNN-BiATT Model Details,[0],[0]
"(WIMG [al; dl; al−dl; al◦dl]) (28)
",A.1 CNN-BiATT Model Details,[0],[0]
The joint-representation of the statement is further processed by a LSTM-RNN.,A.1 CNN-BiATT Model Details,[0],[0]
"Different from our BiATT model, a 3-layers CNN is used for modeling the joint-representation of the image {âl}.",A.1 CNN-BiATT Model Details,[0],[0]
The output of the CNN layer is another feature map {āl}.,A.1 CNN-BiATT Model Details,[0],[0]
"Each CNN layer has kernel size 3 × 3 and uses relu as the activation function, and then we finally use element-wise max operator similar to Sec. 3.1:
h̄1, h̄2, . . .",A.1 CNN-BiATT Model Details,[0],[0]
", h̄T = LSTM(ĥ1, ĥ2, . . .",A.1 CNN-BiATT Model Details,[0],[0]
", ĥT) (29)
ā1, ā2, . . .",A.1 CNN-BiATT Model Details,[0],[0]
", āL’ = CNN(â1, â2, . . .",A.1 CNN-BiATT Model Details,[0],[0]
", âL) (30)
h̄ = ele max i
{ h̄i }
(31)
ā = ele max l {āl} (32)
",A.1 CNN-BiATT Model Details,[0],[0]
"At last, we use the same method as our BiATT model to calculate the score and the loss function:
score(s, x) =W2 tanh ( W1[h̄; ā] + b1 ) (33)
P (s, I) =σ",A.1 CNN-BiATT Model Details,[0],[0]
"( max j score(s, xj) ) (34)
",A.1 CNN-BiATT Model Details,[0],[0]
"L(s, I, y) =",A.1 CNN-BiATT Model Details,[0],[0]
"− y logP (s, I)",A.1 CNN-BiATT Model Details,[0],[0]
"− (1− y) log(1− P (s, I))",A.1 CNN-BiATT Model Details,[0],[0]
(35),A.1 CNN-BiATT Model Details,[0],[0]
"We reimplement a Relationship Network (Santoro et al., 2017), using a three-layer MLP with
256 units per layer in the G-net and a three-layer MLP consisting of 256, 256 (with 0.3 dropout), and 1 units with ReLU nonlinearities for F-net.",A.2 Reimplementation Details for Relationship Network and BiDAF Models,[0],[0]
"We also reimplement a BiDAF model (Seo et al., 2016) using 128-dimensional word embedding, 256-dimensional LSTM-RNN and 0.3 dropout rate.",A.2 Reimplementation Details for Relationship Network and BiDAF Models,[0],[0]
A max pooling layer on top of the modeling layer of BiDAF is used to merge the hidden outputs to a single vector.,A.2 Reimplementation Details for Relationship Network and BiDAF Models,[0],[0]
"For preprocessing, we replace the words whose occurrence is less than 3 with the “UNK” token.",A.3.1 BiATT-Pointer,[0],[0]
We create a 9 dimension vector as the feature of each object.,A.3.1 BiATT-Pointer,[0],[0]
"This feature contains the location (x, y) in 2D coordinate, the size of the object and two 3-dimensional hot vectors for the shape and the color.",A.3.1 BiATT-Pointer,[0],[0]
"The (x, y) coordinates are normalized to the range [−1, 1].
",A.3.1 BiATT-Pointer,[0],[0]
"For the model hyperparameters (all lightly tuned on dev set), the dimension of the word embedding is 128, and the number of units in an LSTM cell is 256.",A.3.1 BiATT-Pointer,[0],[0]
The word embedding is trained from scratch.,A.3.1 BiATT-Pointer,[0],[0]
The object feature is projected to a 64-dimensional vector.,A.3.1 BiATT-Pointer,[0],[0]
The dimensions of joint representation ĥi,A.3.1 BiATT-Pointer,[0],[0]
and ĝk are both 512.,A.3.1 BiATT-Pointer,[0],[0]
The first fully-connected layer in calculating the subimages score has 512 units.,A.3.1 BiATT-Pointer,[0],[0]
All the trainable variables are initialized with the Xavier initializer.,A.3.1 BiATT-Pointer,[0],[0]
"To regularize the training process, we add a dropout rate 0.3 to the hidden output of the LSTM-RNNs and before the last MLP layer which calculates the score for sub-images.",A.3.1 BiATT-Pointer,[0],[0]
We also clip the gradients by their norm to avoid gradient exploding.,A.3.1 BiATT-Pointer,[0],[0]
"The losses are optimized by a single Adam optimizer and the learning rate is fixed at 1e-4.
",A.3.1 BiATT-Pointer,[0],[0]
"For the pointer network, we sample the objects following the distribution of the objects at each decoder step during training.",A.3.1 BiATT-Pointer,[0],[0]
"In inference, we select the object with maximum probability.",A.3.1 BiATT-Pointer,[0],[0]
"We use the self-critical baseline (Rennie et al., 2016) to stabilize the RL training, where the final score in inference (choosing object with maximum probability) is subtracted from the reward.",A.3.1 BiATT-Pointer,[0],[0]
"To reduce the number of parameters, we share the weight of the fully-connected layer which projects the raw object feature to the high dimensional vector in the pointer encoder, the pointer decoder, and the OBJ-
LSTM.",A.3.1 BiATT-Pointer,[0],[0]
"The pointer decoder attends to the hidden outputs of the LANG-LSTM using bilinear attention (Luong et al., 2015).",A.3.1 BiATT-Pointer,[0],[0]
We initialize our model with weights of the public pretrained ResNet-V2-101 (based on the ImageNet dataset) and freeze it during training.,A.3.2 CNN-BiATT,[0],[0]
The ResNet projects the sub-image to a feature map of 10× 10 × 2048.,A.3.2 CNN-BiATT,[0],[0]
The feature map is normalized to a mean of 0 and a standard deviation of 1 before feeding into the FC layer.,A.3.2 CNN-BiATT,[0],[0]
The fully connected layer after the ResNet has 512 units.,A.3.2 CNN-BiATT,[0],[0]
Each layer of the 3-layers CNN in the bidirectional attention has kernel size 3× 3 with 512 filters and no padding.,A.3.2 CNN-BiATT,[0],[0]
The BiENC model uses LANG-LSTM and OBJLSTM to read the statement and the objects.,A.3.3 BiENC,[0],[0]
A bilinear form calculates the similarity between the last hidden outputs of the two LSTM-RNNs.,A.3.3 BiENC,[0],[0]
The similarity is directly used as the score of the subimage.,A.3.3 BiENC,[0],[0]
The CNN-BiENC model replaces the OBJLSTM with a CNN.,A.3.3 BiENC,[0],[0]
"Visual reasoning with compositional natural language instructions, e.g., based on the newly-released Cornell Natural Language Visual Reasoning (NLVR) dataset, is a challenging task, where the model needs to have the ability to create an accurate mapping between the diverse phrases and the several objects placed in complex arrangements in the image.",abstractText,[0],[0]
"Further, this mapping needs to be processed to answer the question in the statement given the ordering and relationship of the objects across three similar images.",abstractText,[0],[0]
"In this paper, we propose a novel end-to-end neural model for the NLVR task, where we first use joint bidirectional attention to build a two-way conditioning between the visual information and the language phrases.",abstractText,[0],[0]
"Next, we use an RL-based pointer network to sort and process the varying number of unordered objects (so as to match the order of the statement phrases) in each of the three images and then pool over the three decisions.",abstractText,[0],[0]
Our model achieves strong improvements (of 4-6% absolute) over the state-of-theart on both the structured representation and raw image versions of the dataset.,abstractText,[0],[0]
Object Ordering with Bidirectional Matchings for Visual Reasoning,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 243–254 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1023",text,[0],[0]
"Expressions referring to objects in visual scenes typically include a word naming the type of the object: E.g., house in Figure 1 (a), or, as a very general type, thingy in Figure 1 (d).",1 Introduction,[0],[0]
"Determining such a name is a crucial step for referring expression generation (REG) systems, as many other decisions concerning, e.g., the selection of attributes follow from it (Dale and Reiter, 1995; Krahmer and Van Deemter, 2012).",1 Introduction,[0],[0]
"For a long time, however, research on REG mostly assumed the availability of symbolic representations of ref-
erent and scene, and sidestepped questions about how speakers actually choose these names, due to the lack of models capable of capturing what a word like house refers to in the real world.
",1 Introduction,[0],[0]
"Recent advances in image processing promise to fill this gap, with state-of-the-art computer vision systems being able to classify images into thousands of different categories (e.g. Szegedy et al. (2015)).",1 Introduction,[0],[0]
"However, classification is not naming (Ordonez et al., 2016).",1 Introduction,[0],[0]
"Standard object classification schemes are inherently “flat”, and treat object labels as mutually exclusive (Deng et al., 2014).",1 Introduction,[0],[0]
"A state-of-the-art object recognition system would be trained to classify the object in e.g. Figure 1 (a) as either house or building, ignoring the lexical similarity between these two names.",1 Introduction,[0],[0]
"In contrast, humans seem to be more flexible as to the chosen level of generality.",1 Introduction,[0],[0]
"Depending on the prototypicality of the object to name, and possibly other visual properties, a general name might be more or less appropriate.",1 Introduction,[0],[0]
"For instance, a robin can be named bird, but a penguin is better referred
243
to as “penguin” (Rosch, 1978); along the same lines, the rather unusual building in Figure 1 (c) that is not easy to otherwise categorise was named “structure”.
",1 Introduction,[0],[0]
"Other work at the intersection of image and language processing has investigated models that learn to directly associate visual objects with a continuous representation of word meaning, i.e. through cross-modal transfer into distributional vector spaces (Frome et al., 2013; Norouzi et al., 2013).",1 Introduction,[0],[0]
"Here, the idea is to exploit a powerful model of lexical similarity induced from large amounts text for being able to capture inherent lexical relations between object categories.",1 Introduction,[0],[0]
"Thus, under the assumption that such semantic spaces represent, in some form at least, taxonomic knowledge, this makes labels on different levels of specificity available for a given object.",1 Introduction,[0.9516893390447864],"['In particular, dropout, which is the focus of this paper, randomly drops hidden nodes along with their connections at training time.']"
"Moreover, if the mapping is sufficiently general, it should be able to map objects to an appropriate label, even if during training of the mapping this label has not been seen (zero-shot learning).
",1 Introduction,[0],[0]
"While cross-modal transfer seems to be a conceptually attractive model for learning object names, it is based on an important assumption that, in our view, has not received sufficient attention in previous works: it assumes that a given distributional vector space constitutes an optimal target representation that visual instances of objects can be mapped to.",1 Introduction,[0],[0]
"However, distributional representations of word meaning are known to capture a rather fuzzy notion of lexical similarity, e.g. car is similar to van and to street.",1 Introduction,[0],[0]
"A cross-modal transfer model is “forced” to learn to map objects into the same area in the semantic space if their names are distributionally similar, but regardless of their actual visual similarity.",1 Introduction,[0],[0]
"Indeed, we have found in a recent study that the contribution of distributional information to learning referential word meanings is restricted to certain types of words and does not generalize across the vocabulary (Zarrieß and Schlangen, 2017).
",1 Introduction,[0],[0]
"The goal of this work is to learn a model of referential word meaning that makes accurate object naming predictions and goes beyond treating words as independent, mutually exclusive labels in a flat classification scheme.",1 Introduction,[0],[0]
"We extend upon work on learning models of referential word use from corpora of images paired with referring expressions (Schlangen et al., 2016; Zarrieß and Schlangen, 2017) that treats words as individual
predictors capturing referential appropriateness.",1 Introduction,[0],[0]
"We explore different ways of linking these predictors to distributional knowledge, during application and during training.",1 Introduction,[0],[0]
"We find that these different models achieve very similar performance in a standard object naming task, though experiments on model combination suggest that they capture complementary aspects of referential meaning.",1 Introduction,[0],[0]
"In a zero-shot setup of an object naming task, we find that combining lexical and visual information during training is most beneficial, outperforming variants of cross-modal transfer.",1 Introduction,[0],[0]
"Grounding and Reference An early example for work in REG that goes beyond Dale and Reiter (1995)’s dominant symbolic paradigm is Deb Roy’s work from the early 2000s (Roy et al., 2002; Roy, 2002, 2005).",2 Related Work,[0],[0]
"Roy et al. (2002) use computer vision techniques to process a video feed, and to compute colour, positional and spatial features.",2 Related Work,[0],[0]
"These features are then associated in a learning process with certain words, resulting in an association of colour features with colour words, spatial features with prepositions, etc., and based on this, these words can be interpreted with reference to the scene currently presented to the video feed.",2 Related Work,[0],[0]
"Whereas Roy’s work still looked at relatively simple scenes with graphical objects, research on REG has recently started to investigate set-ups based on real-world images (Kazemzadeh et al., 2014; Gkatzia et al., 2015; Zarrieß and Schlangen, 2016; Mao et al., 2015).",2 Related Work,[0],[0]
"Importantly, the lowlevel visual features that can be extracted from these scenes correspond less directly to particular word classes.",2 Related Work,[0],[0]
"Moreover, the visual scenes contain many different types of objects, which poses new challenges for REG.",2 Related Work,[0],[0]
"For instance, Zarrieß and Schlangen (2016) find that semantic errors related to mismatches between nouns (e.g. the system generates tree vs. man) are particularly disturbing for users.",2 Related Work,[0],[0]
"Whereas Zarrieß and Schlangen (2016) propose a strategy to avoid object names when the systems confidence is low, we focus on improving the generation of object names, using distributional knowledge as an additional source.",2 Related Work,[0],[0]
"Similarly, Ordonez et al. (2016) have studied the problem of deriving appropriate object names, or so-called entry-level categories, from the output of an object recognizer.",2 Related Work,[0],[0]
"Their approach focusses on linking abstract object categories in ImageNet
to actual words via various translation procedures.",2 Related Work,[0],[0]
"We are interested in learning referential appropriateness and extensional word meanings directly from actual human referring expressions (REs) paired with objects in images, using an existing object recognizer for feature extraction.
",2 Related Work,[0],[0]
"Multi-modal distributional semantics Distributional semantic models are a well-known method for capturing lexical word meaning in a variety of tasks (Turney and Pantel, 2010; Mikolov et al., 2013; Erk, 2016).",2 Related Work,[0],[0]
"Recent work on multimodal distributional vector spaces (Feng and Lapata, 2010; Silberer and Lapata, 2014; Kiela and Bottou, 2014; Lazaridou et al., 2015b; Kottur et al., 2016) has aimed at capturing semantic similarity even more accurately by integrating distributional and perceptual features associated with words (mostly taken from images) into a single representation.
",2 Related Work,[0],[0]
"Cross-modal transfer Rather than fusing different modalities into a single, joint space, other work has looked at cross-modal mapping between spaces.",2 Related Work,[0],[0]
"Herbelot and Vecchi (2015) present a model that learns to map vectors in a distributional space to vectors in a set-theoretic space, showing that there is a functional relationship between distributional information and conceptual knowledge representing quantifiers and predicates.",2 Related Work,[0],[0]
"More related to our work are cross-modal mapping models,that learn to transfer from a representation of an object or image in the visual space to a vector in a distributional space (Socher et al., 2013; Frome et al., 2013; Norouzi et al., 2013; Lazaridou et al., 2014).",2 Related Work,[0],[0]
"Here, the motivation is to exploit the rich lexical knowledge encoded in a distributional space for learning visual classifications.",2 Related Work,[0],[0]
"In practice, these models are mostly used for zeroshot learning where the test set contains object categories not observed during training.",2 Related Work,[0],[0]
"When tested on standard object recognition tasks, transfer, however, comes at a price.",2 Related Work,[0],[0]
Frome et al. (2013) and Norouzi et al. (2013) both find that it slightly degrades performance as compared to a plain object classification using standard accuracy metrics (called flat “hit @k metric” in their paper).,2 Related Work,[0],[0]
"Interestingly though, Frome et al. (2013) report better performance using “hierarchical precision”, which essentially means that transfer predicts words that are ontologically closer to the gold label and makes “semantically more reasonable er-
rors”.",2 Related Work,[0],[0]
"To the best of our knowledge, this pattern has not been systematically investigated any further.",2 Related Work,[0],[0]
"Another known problem with cross-modal transfer is that it seems to generalize less well than expected, i.e. tends to reproduce word vectors observed during training (Lazaridou et al., 2015a).",2 Related Work,[0],[0]
"In this work, we present a model that exploits distributional knowledge for learning referential word meaning as well, but explore and compare different ways of combining visual and lexical aspects of referential word meaning.",2 Related Work,[0],[0]
We define object naming as follows:,3 Task and Data,[0],[0]
"Given an object x in an image, the task is to predict a word w that could be used as the head noun of a realistic referring expression.",3 Task and Data,[0],[0]
"(Cf. discussion above: “bird” when naming a robin, but “penguin” when naming a penguin.)",3 Task and Data,[0],[0]
"To get at this, we develop our approach using a corpus of referring expressions produced by human users under natural, interactive conditions (Kazemzadeh et al., 2014), and train and test on the corresponding head nouns in these REs.",3 Task and Data,[0],[0]
This is similar to picture naming setups used in psycholinguistic research (cf.,3 Task and Data,[0],[0]
"Levelt et al. (1991)) and based on the simplifying assumption that the name used for referring to an object can be determined successfully without looking at other objects in the image.
",3 Task and Data,[0],[0]
"We now summarise the details of our setup:
Corpus We train and test on the REFERIT corpus (Kazemzadeh et al., 2014), which is based on the SAIAPR image collection (Grubinger et al., 2006) (99.5k image regions;120K REs).",3 Task and Data,[0],[0]
"We follow (Schlangen et al., 2016) and select words with a minimum frequency of 40 in these two data sets, which gives us a vocabulary of 793 words.
",3 Task and Data,[0],[0]
"Names For most of our experiments, we only use a subset of this vocabulary, namely the set of object names.",3 Task and Data,[0],[0]
"As the REs contain nouns that cannot be considered to be object names (background, bottom, etc.), we extract a list of names from the semantically annotated held-out set released with the REFERIT.",3 Task and Data,[0],[0]
These correspond to ‘entry-level’ nouns mentioned in Kazemzadeh et al. (2014).,3 Task and Data,[0],[0]
This gives us a list of 159 names.,3 Task and Data,[0],[0]
"This set corresponds to the majority of object names in the corpus: out of the 99.5K available image regions, we use 80K for training and testing.",3 Task and Data,[0],[0]
"Thus, our experiments are on a smaller scale as compared
to (Ordonez et al., 2016).",3 Task and Data,[0],[0]
"Nevertheless, the data is challenging, as the corpus contains references to objects that fall outside of the object labeling scheme that available object recognition systems are typically optimized for, cf. Hu et al. (2015)’s discussion on “stuff” entities such “sky” or “grass” in the REFERIT data.",3 Task and Data,[0],[0]
"For testing, we remove relational REs (containing a relational preposition such as ‘left of X’), because here we cannot be sure that the head noun of the target is fully informative; we also remove REs with more than one head noun from our list (i.e. these are mostly relational expressions as well such as ‘girl laughing at boy’).",3 Task and Data,[0],[0]
"We pair each image region from the test set with its corresponding names from the remaining REs.
",3 Task and Data,[0],[0]
Image and Word Embeddings,3 Task and Data,[0],[0]
"Following Schlangen et al. (2016), we derive representations of our visual inputs with a convolutional neural network, ‘GoogleNet’ (Szegedy et al., 2015), which was trained on the ImageNet corpus (Deng et al., 2009), and extract the final fully-connected layer before the classification layer, to give us a 1024 dimensional representation of the region.",3 Task and Data,[0],[0]
"We add 7 features that encode information about the region relative to the image, thus representing each object as a vector of 1031 features.",3 Task and Data,[0],[0]
"As distributional word vectors, we use the word2vec representations provided by Baroni et al. (2014) (trained with CBOW, 5-word context window, 10 negative samples, 400 dimensions).",3 Task and Data,[0],[0]
Distributional Information,4 Three Models of Interfacing Visual and,[0],[0]
"Following Lazaridou et al. (2014), referential meaning can be represented as a translation function that projects visual representations of objects to linguistic representations of words in a distributional vector space.",4.1 Direct Cross-Modal Mapping,[0],[0]
"Thus, in contrast to standard object recognition systems or the other models we will use here, cross-modal mapping does not treat words as individual labels or classifiers, but learns to directly predict continuous representations of words in a vector space, such as the space defined by the word2vec embeddings that we use in this work.",4.1 Direct Cross-Modal Mapping,[0],[0]
"This model will be called TRANSFER below.
",4.1 Direct Cross-Modal Mapping,[0],[0]
"During training, we pair each object with the distributional embedding of its name, and use standard Ridge regression for learning the trans-
formation.",4.1 Direct Cross-Modal Mapping,[0],[0]
Lazaridou et al. (2014) and Lazaridou et al. (2015a) test a range of technical tweaks and different algorithms for cross-modal mapping.,4.1 Direct Cross-Modal Mapping,[0],[0]
"For ease of comparison with other models, we stick with simple Ridge Regression in this work.
",4.1 Direct Cross-Modal Mapping,[0],[0]
"For decoding, we map an object into the distributional space, and retrieve the nearest neighbors of the predicted vector using cosine similarity.",4.1 Direct Cross-Modal Mapping,[0],[0]
"In theory, the model should generalize easily to words that it has not observed in a pair with an object during training as it can map an object anywhere in the distributional space.",4.1 Direct Cross-Modal Mapping,[0],[0]
"Another approach is to keep visual and distributional information separate, by training a separate visual classifier for each word w in the vocabulary.",4.2 Lexical Mapping Through Individual Word Classifiers,[0],[0]
Predictions can then be mapped into distributional space during application time via the vectors of the predicted words.,4.2 Lexical Mapping Through Individual Word Classifiers,[0],[0]
"Here, we use Schlangen et al. (2016)’s WAC model, building the training set for each word w as follows: all visual objects in a corpus that have been referred to as w are used as positive instances, the remaining objects as negative instances.",4.2 Lexical Mapping Through Individual Word Classifiers,[0],[0]
"Thus, the classifiers learn to predict referential appropriateness for individual words based on the visual features of the objects they refer to, in isolation of other words.
",4.2 Lexical Mapping Through Individual Word Classifiers,[0],[0]
"During decoding, we apply all word classifiers from the model’s vocabulary to the given object, and take the argmax over the individual word probabilities.",4.2 Lexical Mapping Through Individual Word Classifiers,[0],[0]
"The model predicts names directly, without links into a distributional space.
",4.2 Lexical Mapping Through Individual Word Classifiers,[0],[0]
"In order to extend the model’s vocabulary for zero-shot learning, we follow Norouzi et al. (2013) and associate the top n words with their corresponding distributional vector and compute the convex combination of these vectors.",4.2 Lexical Mapping Through Individual Word Classifiers,[0],[0]
"Then, in parallel to cross-modal mapping, we retrieve the nearest neighbors of the combined embedding from the distributional space.",4.2 Lexical Mapping Through Individual Word Classifiers,[0],[0]
"Thus, with this model, we use two different modes of decoding: one that projects into distributional space, one that only applies the available word classifiers.
",4.2 Lexical Mapping Through Individual Word Classifiers,[0],[0]
"We did some small-scale experiments to find an optimal value for n, similar to Norouzi et al. (2013).",4.2 Lexical Mapping Through Individual Word Classifiers,[0],[0]
"In our case, performance started to decrease systematically with n > 10, but did not differ significantly for values below 10.",4.2 Lexical Mapping Through Individual Word Classifiers,[0],[0]
"In Section 5, we will report results for n set to 5 and 10.",4.2 Lexical Mapping Through Individual Word Classifiers,[0],[0]
"Finally, we implement an approach that combines ideas from cross-modal mapping with the WAC model: we train individual predictors for each word in the vocabulary, but, during training, we exploit lexical similarity relations encoded in a distributional space.",4.3 Word Prediction via Cross-Modal Similarity Mapping,[0],[0]
"Instead of treating a word as a binary classifier, we annotate its training instances with a fine-grained similarity signal according to their object names.",4.3 Word Prediction via Cross-Modal Similarity Mapping,[0],[0]
"When building the training set for such a word predictor w, instead of simply dividing objects into w and ¬w instances, we label each object with a real-valued similarity obtained from cosine similarity between w and v in a distributional vector space, where v is the word that was used to refer to the object.",4.3 Word Prediction via Cross-Modal Similarity Mapping,[0],[0]
"Thus, we task the model with jointly learning similarities and referential appropriateness, by training it with Ridge regression on a continuous output space.",4.3 Word Prediction via Cross-Modal Similarity Mapping,[0],[0]
"Object instances where v = w (i.e., the positive instances in the binary setup) have maximal similarity; the remaining instances have a lower value which is more or less close to maximal similarity.",4.3 Word Prediction via Cross-Modal Similarity Mapping,[0],[0]
"This is the SIM-WAP model, recently proposed in Zarrieß and Schlangen (2017).
",4.3 Word Prediction via Cross-Modal Similarity Mapping,[0],[0]
"Importantly, and going beyond Zarrieß and Schlangen (2017), this model allows for an innovative treatment of words that only exist in a distributional space (without being paired with visual referents in the image corpus): as the predictors are trained on a continuous output space, no genuine positive instances of a word’s referent are needed.",4.3 Word Prediction via Cross-Modal Similarity Mapping,[0],[0]
"When training a predictor for such a word w, we use all available objects from our corpus and annotate them with the expected lexical similarity between w and the actual object names v, which for all objects will be below the maximal value that marks genuine positive instances.",4.3 Word Prediction via Cross-Modal Similarity Mapping,[0],[0]
"During decoding, this model does not need to project its predictions into a distributional space, but it simply applies all available predictors to the object, and takes the argmax over the predicted referential appropriateness scores.",4.3 Word Prediction via Cross-Modal Similarity Mapping,[0],[0]
This Section reports on experiments in a standard setup of the object naming task where all object names are paired with visual instances of their referents during training.,5 Experiment 1: Naming Objects,[0],[0]
"In a comparable task, i.e. object recognition with known ob-
ject categories, cross-modal projection or transfer approaches have been reported to perform worse than standard object classification methods (Frome et al., 2013; Norouzi et al., 2013).",5 Experiment 1: Naming Objects,[0],[0]
This seems to suggest that lexical or at least distributional knowledge is detrimental when learning what a word refers to in the real world and that referential meaning should potentially be learned from visual object representation only.,5 Experiment 1: Naming Objects,[0],[0]
"Setup We use the train/test split of REFERIT data as in (Schlangen et al., 2016).",5.1 Model comparison,[0],[0]
We consider image regions with non-relational referring expressions that contain at least one of the 159 head nouns from the list of entry-level nouns (see section 3).,5.1 Model comparison,[0],[0]
"This amounts to 6208 image regions for testing and 73K instances for training.
",5.1 Model comparison,[0],[0]
"Results Table 1 shows accuracies in the object naming task for the TRANSFER, WAC and SIMWAP models according to their accuracies in the top n, including two variants of WAC where its top 5 and top 10 predictions are projected into the distributional space.",5.1 Model comparison,[0],[0]
"Overall, the models achieve very similar performance.",5.1 Model comparison,[0],[0]
"However, there is an interesting pattern when comparing accuracies @1 and @2 to accuracies in the top 5 predictions.",5.1 Model comparison,[0],[0]
"Thus, looking at accuracies for the top (two) predictions, the various models that link referential meaning to word representations in the distributional space all perform slightly worse than the plain WAC model, i.e. individual word classifiers trained on visual features only.",5.1 Model comparison,[0],[0]
This might suggest that certain aspects of referential word meaning are learned less accurately when mapping from visual to distributional space (which replicates results reported in the literature on standard object recognition benchmarks).,5.1 Model comparison,[0],[0]
"On the other hand, the SIM-WAP model is on a par with WAC in terms of the @5 accuracy.",5.1 Model comparison,[0],[0]
"This effect suggests that distributional knowledge that SIM-WAP has access to during training sometimes distracts the model from predicting the exact name chosen by a human speaker, but that SIM-WAP is still able to rank it among the most probable names.",5.1 Model comparison,[0],[0]
"As a simple accuracy-based evaluation is not suited to fully explain this pattern, we carry out a more detailed analysis in Section 5.3.",5.1 Model comparison,[0],[0]
"In order to get more insight into why the TRANSFER and SIM-WAP models produce slightly worse results than individual visual word classifiers, we now test to what extent the different models are complementary and combine them by aggregating over their naming predictions.",5.2 Model combination,[0],[0]
"If the models are complementary, their combination should lead to more confident and accurate naming decisions.
",5.2 Model combination,[0],[0]
"Setup We combine TRANSFER, SIM-WAP and WAC by aggregating the scores they predict for different object names for a given object.",5.2 Model combination,[0],[0]
"During testing, we apply all models to an image region and consider words ranked among the top 10.",5.2 Model combination,[0],[0]
We first normalize the referential appropriateness scores in each top-10 list and then compute their sum.,5.2 Model combination,[0],[0]
"This aggregation scheme will give more weight to words that appear in the top 10 list of different models, and less weight to words that only get top-ranked by a single model.",5.2 Model combination,[0],[0]
"We test on the same data as in Section 5.1.
",5.2 Model combination,[0],[0]
"Results Table 2 shows that all model combinations improve over the results of their isolated models in Table 1, suggesting that WAC, TRANSFER and SIM-WAP indeed do capture complementary aspects of referential word meaning.",5.2 Model combination,[0],[0]
"On their own, the distributionally informed models are less tuned to specific word occurrences than the visual word classifiers in the WAC model, but they can add useful information which leads to a clear overall improvement.",5.2 Model combination,[0],[0]
"We take this as a promising finding, supporting our initial hypothesis that knowledge on lexical distributional meaning should and
can be exploited when learning how to use words for reference.",5.2 Model combination,[0],[0]
"Figure 2 illustrates objects from our test set where the combination of TRANSFER, SIM-WAP and WAC predicts an accurate name, whereas the models in isolation do not.",5.3 Analysis,[0],[0]
"These examples give some interesting insight into why the models capture different aspects of referential word meaning.
",5.3 Analysis,[0],[0]
"Word Similarities Many of the examples in Figure 2 suggest that the object names ranked among the top 3 by the TRANSFER and SIMWAP model are semantically similar to each other, whereas WAC generates object names on top that describe very different underlying object categories, such as seal / rock in Figure 2(a), animal / lamp in Figure 2(g) or chair / shirt in Figure 2(c).",5.3 Analysis,[0],[0]
"To quantify this general impression, Table 3 shows cosine similarities among words in the top n generated by our models, using their word2vec embeddings.",5.3 Analysis,[0],[0]
The average cosine similarity between words in our vocabulary is 0.17.,5.3 Analysis,[0],[0]
"The TRANSFER and SIM-WAP model rank words on top that are clearly more similar to each other than word pairs on average, whereas words ranked top by the WAC model are more dissimilar to each other.",5.3 Analysis,[0],[0]
"Another remarkable finding is that the words generated by TRANSFER and SIM-WAP are not only more similar among the top predictions, but also more similar to the gold name (Table 3 , right columns).",5.3 Analysis,[0],[0]
This result is noteworthy since the accuracies for the top predictions shown in Table 1 are slightly below WAC.,5.3 Analysis,[0],[0]
"In general, this suggests that there is a trade-off between optimizing a model of referential word meaning to exact naming decisions, or tailoring it to make lexically consistent predictions.",5.3 Analysis,[0],[0]
"This parallels findings by Frome et al. (2013) who found that their transfer-based object recognition made “semantically more reasonable” errors than a standard convolutional network while
not improving accuracies for object recognition, see discussion in Section 2.",5.3 Analysis,[0],[0]
"Additional evaluation metrics, such as success rates in a human evaluation (cf. Zarrieß and Schlangen (2016)), would be an interesting direction for more detailed investigation here.
",5.3 Analysis,[0],[0]
Word Use,5.3 Analysis,[0],[0]
"But even though the WAC classifiers lack knowledge on lexical similarities, they seem to able to detect relatively specific instances of word use such as hut in Figure 2(b), shirt in 2(c) or lamp in 2(h).",5.3 Analysis,[0],[0]
"Here, the combination with TRANSFER and SIM-WAP is helpful to give more weight to the object name that is taxonomically correct (sometimes pushing up words below the top-3 and hence not shown in Figure 2).",5.3 Analysis,[0],[0]
"In Figure 1(e), SIMWAP and TRANSFER give more weight to typical names for persons, whereas WAC top-ranks more unusual names, reflecting that the person is difficult to identify visually.",5.3 Analysis,[0],[0]
Another observation is that the mapping models have difficulties dealing with object names in singular and plural.,5.3 Analysis,[0],[0]
"As these words have very similar representations in the distributional space, they are often predicted as likely variants among the top 10 by SIM-WAP and TRANSFER, whereas the WAC model seems to predict inappropriate plural words less often among the top 3.",5.3 Analysis,[0],[0]
Such specific phenomena at the intersection of visual and semantic similarity have found very little attention in the literature.,5.3 Analysis,[0],[0]
We will investigate them further in our Experiments on zeroshot naming in the following Section.,5.3 Analysis,[0],[0]
"Zero-shot learning is an attractive prospect for REG from images, as it promises to overcome dependence on pairings of visual instances and natural names being available for all names, if visual/referential data can be generalised from other types of information.",6 Zero-Shot Naming,[0],[0]
"Previous work has looked at the feasibility of zero-shot learning as a function of semantic similarity or ontological closeness between unknown and known categories, and confirmed the intuition that the task is harder the less close unknown categories are to known ones (Frome et al., 2013; Norouzi et al., 2013).
",6 Zero-Shot Naming,[0],[0]
Our experiments on object naming in Section 5 suggest that lexical similarities encoded in a distributional space might not always fully carry over to referential meaning.,6 Zero-Shot Naming,[0],[0]
"This could constitute an additional challenge for zero-shot learning, as distributional similarities might be misleading when
the model has to fully rely on them for learning referential word meanings.",6 Zero-Shot Naming,[0],[0]
"Therefore, the following experiments investigate the performance of our models in zero-shot naming as a function of the lexical relation between unknown and known object names, i.e. namely hypernyms and singular/plurals.",6 Zero-Shot Naming,[0],[0]
"Both relations are typically captured by distributional models of word meaning in terms of closeness in the vector space, but their visual and referential relation is clearly different.",6 Zero-Shot Naming,[0],[0]
"Random As in previous work on zero-shot learning, we consider zero-shot naming for words of varying degrees of similarity.",6.1 Vocabulary Splits and Testsets,[0],[0]
We randomly split our 159 names from Experiment 1 into 10 subsets.,6.1 Vocabulary Splits and Testsets,[0],[0]
We train the models on 90% of the nouns (and all their visual instances in the image corpus) and test on the set of image regions that are named with words which the model did not observe during training.,6.1 Vocabulary Splits and Testsets,[0],[0]
"Results reported in Table 4 on the random test set correspond to averaged scores from cross-validation over the 10 splits.
",6.1 Vocabulary Splits and Testsets,[0],[0]
Hypernyms We manually split the model’s vocabulary into set of hypernyms (see Appendix A) and the remaining nouns.,6.1 Vocabulary Splits and Testsets,[0],[0]
"We train the models on those 84K image regions that where not named with a hypernym, and test on 8895 image regions that were named with a hypernym in the corpus.",6.1 Vocabulary Splits and Testsets,[0],[0]
"We checked that for each of these hypernyms, the vocabulary contains at least one or two names that can be considered as hyponyms, i.e. the model sees objects during training that are instances of vehicle for example, but never encounters actual uses of that name.",6.1 Vocabulary Splits and Testsets,[0],[0]
"This test set is particularly interesting from an REG perspective, as objects named with very general terms by human speakers are often difficult to describe with more common, but more specific terms, as is illustrated by the uses of structure and thingy in Figure 1.
",6.1 Vocabulary Splits and Testsets,[0],[0]
Singulars/Plurals We pick 68 words from our vocabulary that can be grouped into 34 singularplural noun pairs (see Appendix A).,6.1 Vocabulary Splits and Testsets,[0],[0]
"From each pair, we randomly include the singular or plural noun in the set of zero-shot nouns.",6.1 Vocabulary Splits and Testsets,[0],[0]
"Thus, we make sure that the model encounters singular and plural names during training, but it never encounters both variants of a name.",6.1 Vocabulary Splits and Testsets,[0],[0]
This results training split of 23K image regions and a test split of 13825 instances.,6.1 Vocabulary Splits and Testsets,[0],[0]
"Some previous work on zero-shot image labeling assumes additional components that first identify whether an image should be labelled by a known or unknown word (Frome et al., 2013).",6.2 Evaluation,[0],[0]
We follow Lazaridou et al. (2014) and let the model decide whether to refer to an object by a known or unknown name.,6.2 Evaluation,[0],[0]
"Related to that, distinct evaluation procedures have been used in the literature on zero-shot learning:
Testing on full vocabulary A realistic way to test zero-shot learning performance is to consider all words from a given vocabulary during testing, though the testset only contains instances of objects that have been named with a ‘zero-shot word’ (for which no visual instances were seen during training).",6.2 Evaluation,[0],[0]
"Accuracies in this setup reflect how well the model is able to generalize, i.e. how often it decides to deviate from the words it was trained on, and (implicitly) predicts that the given object requires a “new” name.",6.2 Evaluation,[0],[0]
"In case of the (i) hypernym and (ii) singular/plural test set, this accuracy also reflects to what extent the model is able to detect cases where (i) a more general or vague term is needed, where (ii) an unknown singular/plural counterpart of a known object type occurs.
",6.2 Evaluation,[0],[0]
"Testing on disjoint vocabulary Alternatively, the model’s vocabulary can be restricted during testing to zero-shot words only, such that names encountered during training and testing are disjoint, see e.g. (Lampert et al., 2009, 2013).",6.2 Evaluation,[0],[0]
"This setup factors out the generalization problem, and assesses to what extent a model is able to capture the referential meaning of a word that does not have instances in the training data.",6.2 Evaluation,[0],[0]
"As compared to Experiment 1 where models achieved similar performance, differences are more pronounced in the zero-shot setup, as shown in Table 4.",6.3 Results,[0],[0]
"In particular, we find that the SIMWAP model which induces individual predictors for words that have not been observed in the training data is clearly more successful than TRANSFER or WAC that project predictions into the distributional space.",6.3 Results,[0],[0]
"When tested on the full vocabulary, we find that TRANSFER and WAC very rarely generate names whose referents were excluded from training, which is in line with observations made by Lazaridou et al. (2015a).",6.3 Results,[0],[0]
"The SIM-WAP
predictors generalize much better, in particular on the singular/plural testset.
",6.3 Results,[0],[0]
"An interesting exception is the good performance of the TRANSFER model on the hypernym test set, when evaluated with a disjoint vocabulary.",6.3 Results,[0],[0]
"This corroborates evidence from Experiment 1, namely that the transfer model captures taxonomic aspects of object names better than the other models.",6.3 Results,[0],[0]
"Projection via individual word classifiers, on the other hand, seems to generalize better than TRANSFER, at least when looking at accuracies @2 ... @10.",6.3 Results,[0],[0]
"Thus, combining several vectors predicted by a model of referential word meaning can provide additional information, as compared to mapping an object to a single vector in distributional space.",6.3 Results,[0],[0]
More work is needed to establish how these approaches can be integrated more effectively.,6.3 Results,[0],[0]
"In this paper, we have investigated models of referential word meaning, using different ways of combining visual information about a word’s referent and distributional knowledge about its lexical similarities.",7 Discussion and Conclusion,[0],[0]
Previous cross-modal mapping models essentially force semantically similar objects to be mapped into the same area in the semantic space regardless of their actual visual similarity.,7 Discussion and Conclusion,[0],[0]
"We found that cross-modal mapping produces semantically appropriate and mutually highly similar object names in its top-n list, but does not preserve differences in referential word use (e.g. appropriatness of person vs. woman) especially within the same semantic field.",7 Discussion and Conclusion,[0],[0]
"We have shown that it is beneficial for performance in standard and zeroshot object naming to treat words as individual predictors that capture referential appropriateness and are only indirectly linked to a distributional space, either through lexical mapping during application or through cross-modal similarity mapping during training.",7 Discussion and Conclusion,[0],[0]
"As we have tested these approaches on a rather small vocabulary, which may limit generality of conclusions, future work will be devoted to scaling up these findings to larger test sets, as e.g. recently collected through conversational agents (Das et al., 2016) that circumvent the need for human-human interaction data.",7 Discussion and Conclusion,[0],[0]
"Also from an REG perspective, various extensions of this approach are possible, such as the inclusion of contextual information during object naming and its combination with attribute selection.",7 Discussion and Conclusion,[0],[0]
"We acknowledge support by the Cluster of Excellence “Cognitive Interaction Technology” (CITEC; EXC 277) at Bielefeld University, which is funded by the German Research Foundation (DFG).",Acknowledgments,[0],[0]
"We thank the anonymous reviewers for their very valuable, very detailed and highly interesting comments.",Acknowledgments,[0],[0]
"We investigate object naming, which is an important sub-task of referring expression generation on real-world images.",abstractText,[0],[0]
"As opposed to mutually exclusive labels used in object recognition, object names are more flexible, subject to communicative preferences and semantically related to each other.",abstractText,[0],[0]
"Therefore, we investigate models of referential word meaning that link visual to lexical information which we assume to be given through distributional word embeddings.",abstractText,[0],[0]
We present a model that learns individual predictors for object names that link visual and distributional aspects of word meaning during training.,abstractText,[0],[0]
"We show that this is particularly beneficial for zero-shot learning, as compared to projecting visual objects directly into the distributional space.",abstractText,[0],[0]
"In a standard object naming task, we find that different ways of combining lexical and visual information achieve very similar performance, though experiments on model combination suggest that they capture complementary aspects of referential meaning.",abstractText,[0],[0]
Obtaining referential word meanings from visual and distributional information: Experiments on object naming,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 174–184 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
174",text,[0],[0]
Words are the smallest meaningful utterances in language.,1 Introduction,[0],[0]
They play a central role in our understanding and descriptions of the world around us.,1 Introduction,[0],[0]
Some believe that the structure of a language even affects how we think (principle of linguistic relativity aka the SapirWhorf hypothesis).,1 Introduction,[0],[0]
"Several influential factor analysis studies have shown that the three most important, largely independent, dimensions of word meaning are valence (positiveness–negativeness/pleasure– displeasure), arousal (active–passive), and dominance (dominant–submissive) (Osgood et al., 1957; Russell, 1980, 2003).1 Thus, when comparing the meanings of two words, we can compare their degrees of valence, arousal, or domi-
1We will refer to the three dimensions individually as V, A, and D, and together as VAD.
nance.",1 Introduction,[0],[0]
"For example, the word banquet indicates more positiveness than the word funeral; nervous indicates more arousal than lazy; and fight indicates more dominance than delicate.
",1 Introduction,[0],[0]
"Access to these degrees of valence, arousal, and dominance of words is beneficial for a number of applications, including those in natural language processing (e.g., automatic sentiment and emotion analysis of text), in cognitive science (e.g., for understanding how humans represent and use language), in psychology (e.g., for understanding how people view the world around them), in social sciences (e.g., for understanding relationships between people), and even in evolutionary linguistics (e.g., for understanding how language and behaviour inter-relate to give us an advantage).
",1 Introduction,[0],[0]
"Existing VAD lexicons (Bradley and Lang, 1999; Warriner et al., 2013) were created using rating scales and thus suffer from limitations associated with the method (Presser and Schuman, 1996; Baumgartner and Steenkamp, 2001).",1 Introduction,[0],[0]
"These include: inconsistencies in annotations by different annotators, inconsistencies in annotations by the same annotator, scale region bias (annotators often have a bias towards a portion of the scale), and problems associated with a fixed granularity.
",1 Introduction,[0],[0]
"In this paper, we describe how we obtained human ratings of valence, arousal, and dominance for more than 20,000 commonly used English words by crowdsourcing.",1 Introduction,[0],[0]
"Notably, we use a comparative annotation technique called Best-Worst Scaling (BWS) that addresses the limitations of traditional rating scales (Louviere, 1991; Cohen, 2003; Louviere et al., 2015).",1 Introduction,[0],[0]
"The scores are finegrained real-valued numbers in the interval from 0 (lowest V, A, or D) to 1 (highest V, A, or D).",1 Introduction,[0],[0]
"We will refer to this new lexicon as the NRC Valence, Arousal, and Dominance (VAD) Lexicon.2
2NRC refers to National Research Council Canada.
",1 Introduction,[0],[0]
"Correlations (r) between repeated annotations, through metrics such as split-half reliability (SHR), are a common way to evaluate the reliabilities of ordinal and rank annotations.",1 Introduction,[0],[0]
"We show that our annotations have SHR scores of r = 0.95 for valence, r = 0.90 for arousal, and r = 0.91 for dominance.",1 Introduction,[0],[0]
"These scores are well above the SHR scores obtained by Warriner et al. (2013), and indicate high reliability.
",1 Introduction,[0],[0]
"Respondents who provided valence, arousal, and dominance annotations, were given the option of additionally filling out a brief demographic questionnaire to provide details of their age, gender, and personality traits.",1 Introduction,[0],[0]
"This demographic information along with the VAD annotations allows us to determine whether attributes such as age, gender, and personality impact our understanding of the valence, arousal, and dominance of words.",1 Introduction,[0],[0]
"We show that even though overall the annotations are consistent (as seen from the high SHR scores), people aged over 35 are significantly more consistent in their annotations than people aged 35 or less.",1 Introduction,[0],[0]
"We show for the first time that men have a significantly higher shared understanding of dominance and valence of words, whereas women have a higher shared understanding of the degree of arousal of words.",1 Introduction,[0],[0]
"We find that some personality traits significantly impact a person’s annotations of one or more of valence, arousal, and dominance.",1 Introduction,[0],[0]
"We hope that these and other findings described in the paper foster further research into how we use language, how we represent concepts in our minds, and how certain aspects of the world are more important to certain demographic groups leading to higher degrees of shared representations of those concepts within those groups.
",1 Introduction,[0],[0]
"All of the annotation tasks described in this paper were approved by our institution’s review board, which examined the methods to ensure that they were ethical.",1 Introduction,[0],[0]
Special attention was paid to obtaining informed consent and protecting participant anonymity.,1 Introduction,[0],[0]
The NRC VAD Lexicon is made freely available for research and non-commercial use through our project webpage.3,1 Introduction,[0],[0]
"Primary Dimensions of Meaning: Osgood et al. (1957) asked human participants to rate words along dimensions of opposites such as heavy– light, good–bad, strong–weak, etc.",2 Related Work,[0],[0]
"Factor analysis
3http://saifmohammad.com/WebPages/nrc-vad.html
of these judgments revealed that the three most prominent dimensions of meaning are evaluation (good–bad), potency (strong–weak), and activity (active–passive).",2 Related Work,[0],[0]
"Russell (1980, 2003) showed through similar analyses of emotion words that the three primary independent dimensions of emotions are valence or pleasure (positiveness– negativeness/pleasure–displeasure), arousal (active–passive), and dominance (dominant– submissive).",2 Related Work,[0],[0]
"He argues that individual emotions such as joy, anger, and fear are points in a three-dimensional space of valence, arousal, and dominance.",2 Related Work,[0],[0]
"It is worth noting that even though the names given by Osgood et al. (1957) and Russell (1980) are different, they describe similar dimensions (Bakker et al., 2014).
",2 Related Work,[0],[0]
"Existing Affect Lexicons: Bradley and Lang (1999) asked annotators to rate valence, arousal, and dominance—for more than 1,000 words—on a 9-point rating scale.",2 Related Work,[0],[0]
"The ratings from multiple annotators were averaged to obtain a score between 1 (lowest V, A, or D) to 9 (highest V, A, or D).",2 Related Work,[0],[0]
"Their lexicon, called the Affective Norms of English Words (ANEW), has since been widely used across many different fields of study.",2 Related Work,[0],[0]
"More than a decade later, Warriner et al. (2013) created a similar lexicon for more than 13,000 words, using a similar annotation method.",2 Related Work,[0],[0]
"There exist a small number of VAD lexicons in non-English languages as well, such as the ones created by Moors et al. (2013) for Dutch, by Võ et al. (2009) for German, and by Redondo et al. (2007) for Spanish.",2 Related Work,[0],[0]
"The NRC VAD lexicon is the largest manually created VAD lexicon (in any language), and the only one that was created via comparative annotations (instead of rating scales).
",2 Related Work,[0],[0]
"Best-Worst Scaling: Best-Worst Scaling (BWS) was developed by (Louviere, 1991), building on work in the 1960’s in mathematical psychology and psychophysics.",2 Related Work,[0],[0]
"Annotators are given n items (an n-tuple, where n > 1 and commonly n = 4).4",2 Related Work,[0],[0]
They are asked which item is the best (highest in terms of the property of interest) and which is the worst (least in terms of the property of interest).,2 Related Work,[0],[0]
"When working on 4-tuples, best–worst annotations are particularly efficient because each best and worst annotation will reveal the order of five of the six item pairs (e.g., for a 4-tuple with items
4At its limit, when n = 2, BWS becomes a paired comparison (Thurstone, 1927; David, 1963), but then a much larger set of tuples need to be annotated (closer to N2).
",2 Related Work,[0],[0]
"A, B, C, and D, if A is the best, and D is the worst, then A > B, A > C, A > D, B > D, and C > D).",2 Related Work,[0],[0]
"Real-valued scores of association between the items and the property of interest can be determined using simple arithmetic on the number of times an item was chosen best and number of times it was chosen worst (as described in Section 3) (Orme, 2009; Flynn and Marley, 2014).
",2 Related Work,[0],[0]
"It has been empirically shown that three annotations each for 2N 4-tuples is sufficient for obtaining reliable scores (where N is the number of items) (Louviere, 1991; Kiritchenko and Mohammad, 2016).",2 Related Work,[0],[0]
Kiritchenko and Mohammad (2017) showed through empirical experiments that BWS produces more reliable and more discriminating scores than those obtained using rating scales.,2 Related Work,[0],[0]
"(See Kiritchenko and Mohammad (2016, 2017) for further details on BWS.)
",2 Related Work,[0],[0]
"Within the NLP community, BWS has been used for creating datasets for relational similarity (Jurgens et al., 2012), word-sense disambiguation (Jurgens, 2013), word–sentiment intensity (Kiritchenko and Mohammad, 2016), word–emotion intensity (Mohammad, 2018), and tweet–emotion intensity (Mohammad and Bravo-Marquez, 2017; Mohammad et al., 2018; Mohammad and Kiritchenko, 2018).",2 Related Work,[0],[0]
"Automatically Creating Affect Lexicons: There is growing work on automatically determining word–sentiment and word–emotion associations (Yang et al., 2007; Mohammad and Kiritchenko, 2015; Yu et al., 2015; Staiano and Guerini, 2014).",2 Related Work,[0],[0]
"The VAD Lexicon can be used to evaluate how accurately the automatic methods capture valence, arousal, and dominance.",2 Related Work,[0],[0]
We now describe how we selected the terms to be annotated and how we crowdsourced the annotation of the terms using best–worst scaling.,"3 Obtaining Human Ratings of Valence, Arousal, and Dominance",[0],[0]
We chose to annotate commonly used English terms.,3.1 Term Selection,[0],[0]
We especially wanted to include terms that denotate or connotate emotions.,3.1 Term Selection,[0],[0]
"We also include terms common in tweets.5 Specifically, we include terms from the following sources:
5Tweets include non-standard language such as emoticons, emojis, creatively spelled words (happee), hashtags (#takingastand, #lonely) and conjoined words (loveumom).
",3.1 Term Selection,[0],[0]
"• All terms in the NRC Emotion Lexicon (Mohammad and Turney, 2013).",3.1 Term Selection,[0],[0]
"It has about 14,000 words with labels indicating whether they are associated with any of the eight basic emotions: anger, anticipation, disgust, fear, joy, sadness, surprise, and trust (Plutchik, 1980).
",3.1 Term Selection,[0],[0]
•,3.1 Term Selection,[0],[0]
"All 4,206 terms in the positive and negative lists of the General Inquirer (Stone et al., 1966).
",3.1 Term Selection,[0],[0]
"• All 1,061 terms listed in ANEW (Bradley and Lang, 1999).
",3.1 Term Selection,[0],[0]
"• All 13,915 terms listed in the Warriner et al. (2013) lexicon.
",3.1 Term Selection,[0],[0]
"• 520 words from the Roget’s Thesaurus categories corresponding to the eight basic Plutchik emotions.6
• About 1000 high-frequency content terms, including emoticons, from the Hashtag Emotion Corpus (HEC) (Mohammad, 2012).7
The union of the above sets resulted in 20,007 terms that were then annotated for valence, arousal, and dominance.",3.1 Term Selection,[0],[0]
We describe below how we annotated words for valence.,3.2 Annotating VAD via Best–Worst Scaling,[0],[0]
The same approach is followed for arousal and dominance.,3.2 Annotating VAD via Best–Worst Scaling,[0],[0]
The annotators were presented with four words at a time (4-tuples) and asked to select the word with the highest valence and the word with the lowest valence.,3.2 Annotating VAD via Best–Worst Scaling,[0],[0]
The questionnaire uses a set of paradigm words that signify the two ends of the valence dimension.,3.2 Annotating VAD via Best–Worst Scaling,[0],[0]
"The paradigm words were taken from past literature on VAD (Bradley and Lang, 1999; Osgood et al., 1957; Russell, 1980).",3.2 Annotating VAD via Best–Worst Scaling,[0],[0]
"The questions used for valence are shown below.
Q1.",3.2 Annotating VAD via Best–Worst Scaling,[0],[0]
"Which of the four words below is associated with the
MOST happiness / pleasure / positiveness / satisfaction / con-
tentedness / hopefulness OR LEAST unhappiness / annoy-
ance / negativeness / dissatisfaction / melancholy / despair?
",3.2 Annotating VAD via Best–Worst Scaling,[0],[0]
"(Four words listed as options.)
Q2.",3.2 Annotating VAD via Best–Worst Scaling,[0],[0]
"Which of the four words below is associated with the
LEAST",3.2 Annotating VAD via Best–Worst Scaling,[0],[0]
"happiness / pleasure / positiveness / satisfaction /
contentedness / hopefulness",3.2 Annotating VAD via Best–Worst Scaling,[0],[0]
"OR MOST unhappiness / annoy-
ance / negativeness / dissatisfaction / melancholy / despair?
",3.2 Annotating VAD via Best–Worst Scaling,[0],[0]
"(Four words listed as options.)
",3.2 Annotating VAD via Best–Worst Scaling,[0],[0]
"6http://www.gutenberg.org/ebooks/10681 7All tweets in the HEC include at least one of the eight basic emotion words as a hashtag word (#anger, #sadness, etc.).
",3.2 Annotating VAD via Best–Worst Scaling,[0],[0]
"Questions for arousal and dominance are similar.8
Detailed directions and example questions (with suitable responses) were provided in advance.",3.2 Annotating VAD via Best–Worst Scaling,[0],[0]
"2 × N distinct 4-tuples were randomly generated in such a manner that each word is seen in eight different 4-tuples and no two 4-tuples have more than two items in common (where N is the number of words to be annotated).9
Crowdsourcing: We setup three separate crowdsourcing tasks corresponding to valence, arousal, and dominance.",3.2 Annotating VAD via Best–Worst Scaling,[0],[0]
"The 4-tuples of words were uploaded for annotation on the crowdsourcing platform, CrowdFlower.10 We obtained annotations from native speakers of English residing around the world.",3.2 Annotating VAD via Best–Worst Scaling,[0],[0]
Annotators were free to provide responses to as many 4-tuples as they wished.,3.2 Annotating VAD via Best–Worst Scaling,[0],[0]
"The annotation tasks were approved by our institution’s review board.
",3.2 Annotating VAD via Best–Worst Scaling,[0],[0]
About 2% of the data was annotated beforehand by the authors.,3.2 Annotating VAD via Best–Worst Scaling,[0],[0]
These questions are referred to as gold questions.,3.2 Annotating VAD via Best–Worst Scaling,[0],[0]
CrowdFlower interspersed the gold questions with the other questions.,3.2 Annotating VAD via Best–Worst Scaling,[0],[0]
"If a crowd worker answered a gold question incorrectly, then they were immediately notified, the annotation was discarded, and an additional annotation was requested from a different annotator.",3.2 Annotating VAD via Best–Worst Scaling,[0],[0]
"If an annotator’s accuracy on the gold questions fell below 80%, then they were refused further annotation, and all of their annotations were discarded.",3.2 Annotating VAD via Best–Worst Scaling,[0],[0]
This served as a mechanism to avoid malicious and random annotations.,3.2 Annotating VAD via Best–Worst Scaling,[0],[0]
"The gold questions also served as examples to guide the annotators.
",3.2 Annotating VAD via Best–Worst Scaling,[0],[0]
"8The two ends of the arousal dimension were described with the words: arousal, activeness, stimulation, frenzy, jitteriness, alertness AND unarousal, passiveness, relaxation, calmness, sluggishness, dullness, sleepiness.",3.2 Annotating VAD via Best–Worst Scaling,[0],[0]
"The two ends of the dominance dimension were described with the words: dominant, in control of the situation, powerful, influential, important, autonomous AND submissive, controlled by outside factors, weak, influenced, cared-for, guided.
",3.2 Annotating VAD via Best–Worst Scaling,[0],[0]
"9We used the script provided by Kiritchenko and Mohammad (2016) to generate the 4-tuples from the list of terms: http://saifmohammad.com/WebPages/BestWorst.html
10CrowdFlower later changed its name to Figure Eight: https://www.figure-eight.com
",3.2 Annotating VAD via Best–Worst Scaling,[0],[0]
"In the task settings for CrowdFlower, we specified that we needed annotations from six people for each word.11 However, because of the way the gold questions work in CrowdFlower, they were annotated by more than six people.",3.2 Annotating VAD via Best–Worst Scaling,[0],[0]
Both the minimum and the median number of annotations per item was six.,3.2 Annotating VAD via Best–Worst Scaling,[0],[0]
"See Table 1 for summary statistics on the annotations.12
Annotation Aggregation:",3.2 Annotating VAD via Best–Worst Scaling,[0],[0]
"The final VAD scores were calculated from the BWS responses using a simple counting procedure (Orme, 2009; Flynn and Marley, 2014):",3.2 Annotating VAD via Best–Worst Scaling,[0],[0]
"For each item, the score is the proportion of times the item was chosen as the best (highest V/A/D) minus the proportion of times the item was chosen as the worst (lowest V/A/D).",3.2 Annotating VAD via Best–Worst Scaling,[0],[0]
The scores were linearly transformed to the interval: 0 (lowest V/A/D) to 1 (the highest V/A/D).,3.2 Annotating VAD via Best–Worst Scaling,[0],[0]
"We refer to the list of words along with their scores for valence, arousal, and dominance as the NRC Valence, Arousal, and Dominance Lexicon, or the NRC VAD Lexicon for short.",3.2 Annotating VAD via Best–Worst Scaling,[0],[0]
"Table 2 shows entries from the lexicon with the highest and lowest scores for V, A, and D.
11Note that since each word occurs in eight different 4- tuples, it is involved in 8× 6 = 48 best–worst judgments.
",3.2 Annotating VAD via Best–Worst Scaling,[0],[0]
"12In a post-annotation survey, the respondents gave the task high scores for clarity of instruction (an average of 4.5 out of 5) and overall satisfaction (an average of 4.3 out of 5).",3.2 Annotating VAD via Best–Worst Scaling,[0],[0]
"Respondents who annotated our VAD questionnaires were given a special code through which they could then optionally respond to a separate CrowdFlower survey asking for their demographic information: age, gender, country they live in, and personality traits.",4 Demographic Survey,[0],[0]
"For the latter, we asked how they viewed themselves across the big five (Barrick and Mount, 1991) personality traits:
• Agreeableness (Ag) – Disagreeableness (Di): friendly and compassionate or careful in whom to trust, argumentative
•",4 Demographic Survey,[0],[0]
"Conscientiousness (Co) – Easygoing (Ea): efficient and organized (prefer planned and self-disciplined behaviour) or easy-going and carefree (prefer flexibility and spontaneity)
•",4 Demographic Survey,[0],[0]
"Extrovert (Ex) – Introvert (In): outgoing, energetic, seek the company of others or solitary, reserved, meeting many people causes anxiety
• Neurotic (Ne) – Secure (Se): sensitive and nervous (often feel anger, anxiety, depression, and vulnerability) or secure and confident (rarely feel anger, anxiety, depression, and vulnerability)
",4 Demographic Survey,[0],[0]
"• Open to experiences (Op) – Closed to experiences (Cl): inventive and curious (seek out new experiences) or consistent and cautious (anxious about new experiences)
",4 Demographic Survey,[0],[0]
"The questionnaire described the two sides of the dimension using only the texts after the colons above.13 The questionnaire did not ask for identifying information such as name or date of birth.
",4 Demographic Survey,[0],[0]
"In total, 991 people (55% of the VAD annotators) chose to provide their demographic information.",4 Demographic Survey,[0],[0]
Table 3 shows the details.,4 Demographic Survey,[0],[0]
We calculated the Pearson correlations r between the NRC VAD Lexicon scores and the Warriner et al. Lexicon scores.,5.1 A Comparsion of the NRC VAD Lexicon and the Warriner et al. Lexicon Scores,[0],[0]
Table 4 shows the results.,5.1 A Comparsion of the NRC VAD Lexicon and the Warriner et al. Lexicon Scores,[0],[0]
"(These numbers were calculated for the 13,915 common terms across the two lexicons.)",5.1 A Comparsion of the NRC VAD Lexicon and the Warriner et al. Lexicon Scores,[0],[0]
Observe that the especially low correlations for dominance and arousal indicate that our lexicon has substantially different scores and rankings of terms by these dimensions.,5.1 A Comparsion of the NRC VAD Lexicon and the Warriner et al. Lexicon Scores,[0],[0]
"Even for valence, a correlation of 0.81 indicates a marked amount of differences in scores.",5.1 A Comparsion of the NRC VAD Lexicon and the Warriner et al. Lexicon Scores,[0],[0]
"Russell (1980) found through his factor analysis work that valence, arousal, and dominance are nearly independent dimensions.",5.2 Independence of Dimensions,[0],[0]
"However, Warriner et al. (2013) report that their scores for valence and dominance have substantial correlation (r = 0.717).",5.2 Independence of Dimensions,[0],[0]
"Given that the split-half reliability score for their dominance annotations is only 0.77, the high V–D correlations raises the suspicion whether annotators sufficiently understood the difference between dominance and valence.",5.2 Independence of Dimensions,[0],[0]
"Table 5 shows the correlations between various pair-wise combinations of valence, arousal, and dominance for both our lexicon and the Warriner lexicon.",5.2 Independence of Dimensions,[0],[0]
"Observe that unlike the Warriner annotations where V and D are highly correlated, our annotations show that V and D are only slightly correlated.",5.2 Independence of Dimensions,[0],[0]
"The correlations for V–A and A–D are low in both our and Warriner annotations, albeit slightly higher in magnitude in our annotations.
",5.2 Independence of Dimensions,[0],[0]
13How people view themselves may be different from what they truly are.,5.2 Independence of Dimensions,[0],[0]
The conclusions in this paper apply to groups that view themselves to be a certain personality type.,5.2 Independence of Dimensions,[0],[0]
A useful measure of quality is reproducibility of the end result—repeated independent manual annotations from multiple respondents should result in similar scores.,5.3 Reliability of the Annotations,[0],[0]
"To assess this reproducibility, we calculate average split-half reliability (SHR) over 100 trials.",5.3 Reliability of the Annotations,[0],[0]
"All annotations for an item (in our case, 4-tuples) are randomly split into two halves.",5.3 Reliability of the Annotations,[0],[0]
Two sets of scores are produced independently from the two halves.,5.3 Reliability of the Annotations,[0],[0]
Then the correlation between the two sets of scores is calculated.,5.3 Reliability of the Annotations,[0],[0]
"If the annotations are of good quality, then the correlation between the two halves will be high.",5.3 Reliability of the Annotations,[0],[0]
"Table 6 shows the split-half reliabilities (SHR) for valence, arousal, and dominance annotations.",5.3 Reliability of the Annotations,[0],[0]
Row a. shows the SHR on the full set of terms in the VAD lexicon.,5.3 Reliability of the Annotations,[0],[0]
Row b. shows the SHR on just the Warriner subset of terms in the VAD lexicon.,5.3 Reliability of the Annotations,[0],[0]
Row c. shows the SHR reported by Warriner et al. (2013) on their annotations.,5.3 Reliability of the Annotations,[0],[0]
"Observe that the SHR scores for our annotations are markedly higher than those reported by Warriner et al. (2013), especially for arousal and dominance.",5.3 Reliability of the Annotations,[0],[0]
"All differences in SHR scores between rows b and c are statistically significant.
",5.3 Reliability of the Annotations,[0],[0]
Summary of Main Results: The low correlations between the scores in our lexicon and the Warriner lexicon (especially for D and A) show that the scores in the two lexicons are substantially different.,5.3 Reliability of the Annotations,[0],[0]
The scores for correlations across all pairs of dimensions in our lexicon are low (r < 0.5).,5.3 Reliability of the Annotations,[0],[0]
"SHR scores of 0.95 for valence, 0.9 for arousal, and 0.9 for dominance show for the first time that highly reliable fine-grained ratings can be obtained for valence, arousal, and dominance.",5.3 Reliability of the Annotations,[0],[0]
Human cognition and behaviour is impacted by evolutionary and socio-cultural factors.,6 Shared Understanding of VAD Within and Across Demographic Groups,[0],[0]
"These factors are known to impact different groups of people differently (men vs. women, young vs. old, etc.).",6 Shared Understanding of VAD Within and Across Demographic Groups,[0],[0]
"Thus it is not surprising that our understanding of the world may be slightly different de-
pending on our demographic attributes.",6 Shared Understanding of VAD Within and Across Demographic Groups,[0],[0]
"Consider gender—a key demographic attribute.14 Men, women, and other genders are substantially more alike than they are different.",6 Shared Understanding of VAD Within and Across Demographic Groups,[0],[0]
"However, they have encountered different socio-cultural influences for thousands of years.",6 Shared Understanding of VAD Within and Across Demographic Groups,[0],[0]
Often these disparities have been a means to exert unequal status and asymmetric power relations.,6 Shared Understanding of VAD Within and Across Demographic Groups,[0],[0]
"Thus a crucial area in gender studies is to examine both the overt and subtle impacts of these socio-cultural influences, as well as ways to mitigate the inequity.",6 Shared Understanding of VAD Within and Across Demographic Groups,[0],[0]
Understanding how different genders perceive and use language is an important component of that research.,6 Shared Understanding of VAD Within and Across Demographic Groups,[0],[0]
"Language use is also relevant to the understanding and treatment of neuropsychiatric disorders, such as sleep, mood, and anxiety disorders, which have been shown to occur more frequently in women than men (Bao and Swaab, 2011; Lewinsohn et al., 1998; McLean et al., 2011; Johnson et al., 2006; Chmielewski et al., 1995).
",6 Shared Understanding of VAD Within and Across Demographic Groups,[0],[0]
"In addition to the VAD Lexicon (created by aggregating human judgments), we also make available the demographic information of the annotators.",6 Shared Understanding of VAD Within and Across Demographic Groups,[0],[0]
This demographic information along with the individual judgments on the best–worst tuples forms a significant resource in the study of how demographic attributes are correlated with our understanding of language.,6 Shared Understanding of VAD Within and Across Demographic Groups,[0],[0]
"The data can be used to shed light on research questions such as: ‘are there significant differences in the shared understanding of word meanings in men and women?’, ‘how is the social construct of gender reflected in language, especially in socio-political interactions?’, ‘does age impact our view of the valence, arousal, and dominance of concepts?’, ‘do people that view themselves as conscientious have slightly different judgments of valence, arousal, and dominance, than people who view themselves as easy going?’, and so on.
",6 Shared Understanding of VAD Within and Across Demographic Groups,[0],[0]
"14Note that the term sex refers to a biological attribute pertaining to the anatomy of one’s reproductive system and sex chromosomes, whereas gender refers to a psycho-sociocultural construct based on a person’s sex or a person’s self identification of levels of masculinity and femininity.",6 Shared Understanding of VAD Within and Across Demographic Groups,[0],[0]
"One may identify their gender as female, male, agender, trans, queer, etc.",6 Shared Understanding of VAD Within and Across Demographic Groups,[0],[0]
"We now describe experiments we conducted to determine whether demographic attributes impact how we judge words for valence, arousal, and dominance.",6.1 Experiments,[0],[0]
"For each demographic attribute, we partitioned the annotators into two groups: male (m) and female (f), ages 18 to 35 (≤35) and ages over 35 (>35), and so on.15 For each of the five personality traits, annotators are partitioned into the two groups shown in the bullet list of Section 4.",6.1 Experiments,[0],[0]
"We then calculated the extent to which people within the same group agreed with each other, and the extent to which people across groups agreed with each other on the VAD annotations (as described in the paragraph below).",6.1 Experiments,[0],[0]
"We also determined if the differences in agreement were statistically significant.
",6.1 Experiments,[0],[0]
"For each dimension (V, A, and D), we first collected only those 4-tuples where at least two female and at least two male responses were available.",6.1 Experiments,[0],[0]
We will refer to this set as the base set.,6.1 Experiments,[0],[0]
"For each of the base set 4-tuples, we calculated three agreement percentages: 1.",6.1 Experiments,[0],[0]
"the percentage of all female–female best–worst responses where the two agreed with each other, 2.",6.1 Experiments,[0],[0]
"the percentage of all male–male responses where the two agreed with each other, and 3.",6.1 Experiments,[0],[0]
the percentage of all female–male responses where the two agreed with each other.,6.1 Experiments,[0],[0]
We then calculated the averages of the agreement percentages across all the 4-tuples in the base set.,6.1 Experiments,[0],[0]
"We conducted similar experiments for age groups and personality traits.
",6.1 Experiments,[0],[0]
"15For age, we chose 35 to create the two groups because several psychology and medical studies report changes in health and well-being at this age.",6.1 Experiments,[0],[0]
"Nonetheless, other partitions of age are also worth exploring.",6.1 Experiments,[0],[0]
Table 7 shows the results for gender.,6.2 Results,[0],[0]
Note that the average agreement numbers are not expected to be high because often a 4-tuple may include two words that are close to each other in terms of the property of interest (V/A/D).16,6.2 Results,[0],[0]
"However, the relative values of the agreement percentages indicate the relative levels of agreements within groups and across groups.
",6.2 Results,[0],[0]
"Table 7 numbers indicate that women have a higher shared understanding of the degree of arousal of words (higher f–f average agreement scores on A), whereas men have a higher shared understanding of dominance and valence of words (higher m–m average agreement scores on V and D).",6.2 Results,[0],[0]
"The table also shows the cross-group (f–m) average agreements are the lowest for valence and arousal, but higher than f–f pairs for dominance.",6.2 Results,[0],[0]
"(Each of these agreements was determined from 1 to 1.5 million judgment pairs.)
",6.2 Results,[0],[0]
Table 8 shows which of the Table 7 average agreements are statistically significantly different (shown with a ‘y’).,6.2 Results,[0],[0]
Significance values were calculated using the chi-square test for independence and significance level of 0.05.,6.2 Results,[0],[0]
"Observe that all score differences are statistically significant except for between f–f and f–m scores for V and m– m and f–m scores for A.
Tables 9 through 12 are similar to Tables 7 and 8, but for age groups and personality traits.",6.2 Results,[0],[0]
"Tables 9 and 10 show that respondents over the age of 35 obtain significantly higher agreements with each other on valence and arousal and lower agreements on dominance, than respondents aged 35 and under (with each other).",6.2 Results,[0],[0]
"Tables 11 and 12 show that
16Such disagreements are useful as they cause the two words to obtain scores close to each other.
",6.2 Results,[0],[0]
"some personality traits significantly impact a person’s annotations of one or more of V, A, and D. Notably, those who view themselves as conscientious have a particularly higher shared understanding of the dominance of words, as compared to those who view themselves as easy going.",6.2 Results,[0],[0]
"They also have higher in-group agreement for arousal, than those who view themselves as easy going, but the difference for valence is not statistically significant.",6.2 Results,[0],[0]
"Also notable, is that those who view themselves as extroverts have a particularly higher shared understanding of the valence, arousal, and dominance of words, as compared to those who view themselves as introverts.
",6.2 Results,[0],[0]
"Finally, as a sanity check, we divided respondents into those whose CrowdFlower worker ids are odd and those whose worker ids are even.",6.2 Results,[0],[0]
"We then determined average agreements for even–even, odd-odd, and even–odd groups just as we did for the demographic variables.",6.2 Results,[0],[0]
"We found that, as expected, there were no significant differences in average agreements.
",6.2 Results,[0],[0]
"Summary of Main Results: We showed that several demographic attributes such as age, gender, and personality traits impact how we judge words for valence, arousal, and dominance.",6.2 Results,[0],[0]
"Further,
people that share certain demographic attributes show a higher shared understanding of the relative rankings of words by (one or more of) V, A, or D than others.",6.2 Results,[0],[0]
"However, this raises new questions: why do certain demographic attributes impact our judgments of V, A, and D?",6.2 Results,[0],[0]
"Are there evolutionary forces that caused some groups such as women to develop a higher shared understanding or the arousal, whereas different evolutionary forces caused some groups, such as men, to have a higher shared understanding of dominance?",6.2 Results,[0],[0]
We hope that the data collected as part of this project will spur further inquiry into these and other questions.,6.2 Results,[0],[0]
The large number of entries in the VAD Lexicon and the high reliability of the scores make it useful for a number of research projects and applications.,7 Applications and Future Work,[0],[0]
"We list a few below:
• To provide features for sentiment or emotion detection systems.",7 Applications and Future Work,[0],[0]
"They can also be used to obtain sentiment-aware word embeddings and sentiment-aware sentence representations.
",7 Applications and Future Work,[0],[0]
• To study the interplay between the basic emotion model and the VAD model of affect.,7 Applications and Future Work,[0],[0]
"The VAD lexicon can be used along with lists of words associated with emotions such as joy, sadness, fear, etc. to study the correlation of V, A, and D, with those emotions.
",7 Applications and Future Work,[0],[0]
• To study the role emotion words play in high emotion intensity sentences or tweets.,7 Applications and Future Work,[0],[0]
"The Tweet Emotion Intensity Dataset has emotion intensity and valence scores for whole tweets (Mohammad and Bravo-Marquez, 2017).",7 Applications and Future Work,[0],[0]
"We will use the VAD lexicon to determine the extent to which high intensity and high valence tweets consist of high V, A, and D words, and to identify sentences that express high emotional intensity without using high V, A, and D words.
",7 Applications and Future Work,[0],[0]
•,7 Applications and Future Work,[0],[0]
"To identify syllables that tend to occur in words with high VAD scores, which in turn can be used to generate names for literary characters and commercial products that have the desired affectual response.
",7 Applications and Future Work,[0],[0]
"• To identify high V, A, and D words in books and literature.",7 Applications and Future Work,[0],[0]
To facilitate research in digital humanities.,7 Applications and Future Work,[0],[0]
"To facilitate work on literary analysis.
",7 Applications and Future Work,[0],[0]
•,7 Applications and Future Work,[0],[0]
"As a source of gold (reference) scores, the entries in the VAD lexicon can be used in the evaluation of automatic methods of determining V, A, and D.
• To analyze V, A, ad D annotations for different groups of words, such as: hashtag words and emojis common in tweets, emotion denotating words, emotion associated words, neutral terms, words belonging to particular parts of speech such as nouns, verbs, and adjectives, etc.
",7 Applications and Future Work,[0],[0]
"• To analyze interactions between demographic groups and specific groups of words, for example, whether younger annotators have a higher shared understanding of tweet terms, whether a certain gender is associated with a higher shared understanding of adjectives, etc.
",7 Applications and Future Work,[0],[0]
"• To analyze the shared understanding of V, A, and D within and across geographic and language groups.",7 Applications and Future Work,[0],[0]
We are interested in creating VAD lexicons for other languages.,7 Applications and Future Work,[0],[0]
"We can then explore characteristics of valence, arousal, and dominance that are common across cultures.",7 Applications and Future Work,[0],[0]
"We can also test whether some of the conclusions reached in this work apply only to English, or more broadly to multiple languages.
",7 Applications and Future Work,[0],[0]
"• The dataset is of use to psychologists and evolutionary linguists interested in determining how evolution shaped our representation of the world around us, and why certain personality traits are associated with higher or lower shared understanding of V, A, and D.",7 Applications and Future Work,[0],[0]
"We obtained reliable human ratings of valence, arousal, and dominance for more than 20,000 English words.",8 Conclusions,[0],[0]
(It has about 40% more words than the largest existing manually created VAD lexicon).,8 Conclusions,[0],[0]
We used best–worst scaling to obtain finegrained scores (and word rankings) and addressed issues of annotation consistency that plague traditional rating scale methods of annotation.,8 Conclusions,[0],[0]
"We showed that the lexicon has split-half reliability scores of 0.95 for valence, 0.90 for arousal, and 0.90 for dominance.",8 Conclusions,[0],[0]
"These scores are markedly higher than that of existing lexicons.
",8 Conclusions,[0],[0]
"We analyzed demographic information to show that even though the annotations overall lead to consistent scores in repeated annotations, there exist statistically significant differences in agreements across demographic groups such as males and females, those above the age of 35 and those that are 35 or under, and across personality dimensions (extroverts and introverts, neurotic and secure, etc.).",8 Conclusions,[0],[0]
"These results show that certain demographic attributes impact how we view the world around us in terms of the relative valence, arousal, and dominance of the concepts in it.
",8 Conclusions,[0],[0]
"The NRC Valence, Arousal, and Dominance Lexicon is made available.17 It can be used in combination with other manually created affect lexicons such as the NRC Word–Emotion Association Lexicon (Mohammad and Turney, 2013)18 and the NRC Affect Intensity Lexicon (Mohammad, 2018).19",8 Conclusions,[0],[0]
"Many thanks to Svetlana Kiritchenko, Michael Wojatzki, Norm Vinson, and Tara Small for helpful discussions.
",Acknowledgments,[0],[0]
"17The NRC Valence, Arousal, and Dominance Lexicon provides human ratings of valence, arousal, and dominance for more than 20,000 English words: http://saifmohammad.com/WebPages/nrc-vad.html
18The NRC Emotion Lexicon includes about 14,000 words annotated to indicate whether they are associated with any of the eight basic emotions (anger, anticipation, disgust, fear, joy, sadness, surprise, and trust): http://saifmohammad.com/WebPages/NRC-EmotionLexicon.htm
19The NRC Affect Intensity Lexicon provides realvalued affect intensity scores for four basic emotions (anger, fear, sadness, joy): http://saifmohammad.com/WebPages/AffectIntensity.htm",Acknowledgments,[0],[0]
Words play a central role in language and thought.,abstractText,[0],[0]
"Factor analysis studies have shown that the primary dimensions of meaning are valence, arousal, and dominance (VAD).",abstractText,[0],[0]
"We present the NRC VAD Lexicon, which has human ratings of valence, arousal, and dominance for more than 20,000 English words.",abstractText,[0],[0]
We use Best–Worst Scaling to obtain fine-grained scores and address issues of annotation consistency that plague traditional rating scale methods of annotation.,abstractText,[0],[0]
We show that the ratings obtained are vastly more reliable than those in existing lexicons.,abstractText,[0],[0]
"We also show that there exist statistically significant differences in the shared understanding of valence, arousal, and dominance across demographic variables such as age, gender, and personality.",abstractText,[0],[0]
"Obtaining Reliable Human Ratings of Valence, Arousal, and Dominance for 20,000 English Words",title,[0],[0]
"Many datasets of interest in machine learning are comprised of high-dimensional, complex objects.",1. Introduction,[0],[0]
"Often, one is interested in describing these observations using a lowdimensional latent subspace that captures the statistical variations.",1. Introduction,[0],[0]
"Such approaches fall under the umbrella of factor analysis (Bishop, 2016), where we wish to learn a mapping between the latent and observed spaces.",1. Introduction,[0],[0]
"The motivation is two-fold: (i) factor models provide a compact representation
1Paul G. Allen School of Computer Science and Engineering, University of Washington 2Institute for Learning & Brain Sciences and Department of Speech and Hearing Sciences, University of Washington.",1. Introduction,[0],[0]
"Correspondence to: Samuel K. Ainsworth <skainsworth@gmail.com>, Nicholas J. Foti <nfoti@uw.edu>, Adrian K. C. Lee <akclee@uw.edu>, Emily B. Fox <ebfox@uw.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
of the data, and (ii) the mapping can be used to describe the correlation structure of the high-dimensional data.",1. Introduction,[0],[0]
"In many applications, we are particularly interested in mappings that elucidate interpretable interactions.
",1. Introduction,[0],[0]
The challenge arises from the push and pull between interpretability and expressivity in factor modeling approaches.,1. Introduction,[0],[0]
"Methods emphasizing interpretability have focused primarily on linear models, resulting in lower expressivity.",1. Introduction,[0],[0]
"A popular choice in these settings is to consider sparse linear factor models (Zhao et al., 2016; Carvalho et al., 2008).",1. Introduction,[0],[0]
"However, it is well known that neural (Vejmelka et al., 2010), genomic (Prill et al., 2010), and financial data (Harvey et al., 1994), for example, exhibit complex nonlinearities.
",1. Introduction,[0],[0]
"Recently, there has been a significant amount of work on expressive models for complex, high dimensional data.",1. Introduction,[0],[0]
"In particular, deep generative models (Kingma & Welling, 2013; Rezende et al., 2014; Goodfellow et al., 2014; Damianou & Lawrence, 2013) have proven wildly successful in efficiently modeling complex observations—such as images—as nonlinear mappings of simple latent representations.",1. Introduction,[0],[0]
"These nonlinear maps are based on deep neural networks that parameterize an observation distribution, often referred to as the generator.",1. Introduction,[0],[0]
"We focus on the class of variational autoencoders (VAEs) (Kingma & Welling, 2013).",1. Introduction,[0],[0]
"Unlike linear models which posit a latent variable per observation, VAEs introduce a mapping from observations to a distribution on the latent space; when parameterized by a deep neural network, this mapping is called the inference network.",1. Introduction,[0],[0]
"The generator and inference network are jointly trained to minimize a variational objective.
",1. Introduction,[0],[0]
The VAE can be viewed as a nonlinear factor model that provides a scalable means of learning latent representations.,1. Introduction,[0],[0]
"The focus, however, has primarily been on their use as a generative mechanism.",1. Introduction,[0],[0]
"One shortcoming of the VAE is that, due to the tangled web of connections between neural network layers, it is not possible to interpret how changes in the latent code influence changes in the observations—as in linear latent factor models.",1. Introduction,[0],[0]
"For example, imagine you are trying to synthesize human body poses.",1. Introduction,[0],[0]
"One might hope to have a disentangled representation where a given latent dimension controls a subset of highly correlated body parts; unfortunately, the standard VAE cannot yield these types of interpretations.",1. Introduction,[0],[0]
"Another shortcoming of the VAE
is that training—as in most neural network-based models— typically requires a massive amount of data.",1. Introduction,[0],[0]
"In many applications, we have limited access to training data.
",1. Introduction,[0],[0]
One natural way to encourage disentangled latent representations is by introducing structure and sparsity into the generator.,1. Introduction,[0],[0]
"Specifically, we propose an output interpretable VAE (oi-VAE) that factorizes the generator across observation dimensions, with a separate generator per group of variables.",1. Introduction,[0],[0]
"The generators are coupled both through a shared latent space, and by jointly training with a single inference network.",1. Introduction,[0],[0]
"We also introduce a sparsity-inducing penalty that leads each latent dimension to influence a limited subset of groups, resulting in a disentangled latent representation.",1. Introduction,[0],[0]
"We develop an amortized variational inference algorithm for a collapsed objective, allowing us to use efficient proximal updates to learn latent-dimension-to-group interactions.
",1. Introduction,[0],[0]
The factorization of generators across dimensions is readily apparent when the data are inherently group structured.,1. Introduction,[0],[0]
There are many applications where this is the case.,1. Introduction,[0],[0]
"In the analysis of neuroimaging data, studies are typically done at the level of regions of interest that aggregate over corticallylocalized signals.",1. Introduction,[0],[0]
"In genomics, there are different treatment regimes.",1. Introduction,[0],[0]
"In finance, the data might be described in terms of asset classes (stocks, bonds, . . . ).",1. Introduction,[0],[0]
"And for motion capture data, multiple angle measurements are grouped by their associated joints.",1. Introduction,[0],[0]
In these group-structured scenarios we may additionally garner interpretability from the oi-VAE mappings.,1. Introduction,[0],[0]
"For example, we may learn that a given latent dimension controls a collection of highly correlated joints— e.g., joints in a limb—that comprise a system of interest.",1. Introduction,[0],[0]
"A side benefit of this structured oi-VAE framework is its ability to handle scenarios with limited amounts of data.
",1. Introduction,[0],[0]
We evaluate the oi-VAE on motion capture and magnetoencephalography datasets.,1. Introduction,[0],[0]
"In these scenarios where there is a natural notion of groupings of observations, we demonstrate the interpretability of the learned features and how these structures of interaction correspond to physically meaningful systems.",1. Introduction,[0],[0]
"Furthermore, in such cases we show that the regularization employed by oi-VAE leads to better generalization and synthesis capabilities, especially in limited training data scenarios or when the training data might not fully capture the observed space of interest.",1. Introduction,[0],[0]
"In addition, we found that oi-VAE produces unconditional samples that are qualitatively superior to standard VAEs due to oi-VAE’s bias towards disentangled representations in the latent space.",1. Introduction,[0],[0]
Nonlinear factor analysis aims to relax the strict linearity assumption of classical factor analysis and has a long history in the statistics community.,2. Background,[0],[0]
"The work of (Gibson, 1959) initially circumvented the issues of linear factor analysis by
discretizing continuous nonlinearities.",2. Background,[0],[0]
"However, (McDonald, 1962) was the first to develop a parametric nonlinear factor analysis model.",2. Background,[0],[0]
"Significant progress has been made since then as described in Yalcin & Amemiya (2001), including developments in the Bayesian context (Arminger & Muthén, 1998).",2. Background,[0],[0]
"Recent work in machine learning has also considered similar approaches leveraging Gaussian processes (Lawrence, 2003; Damianou et al., 2012).",2. Background,[0],[0]
"Despite the resemblance to autoencoding models (Ballard, 1987)— especially in the age of “disentanglement”—little work exists exploring connections between the two.
",2. Background,[0],[0]
The study of deep generative models is an active area of research in the machine learning community.,2. Background,[0],[0]
"The variational autoencoder (VAE) (Kingma & Welling, 2013) is one such example that efficiently trains a generative model via amortized inference (see also Rezende et al., 2014).",2. Background,[0],[0]
"Though deep generative models like the VAE have demonstrated an ability to produce convincing samples of complex data (cf., Archer et al., 2015; Johnson et al., 2017), the learned latent representations are not readily interpretable due to the entangled interactions between latent dimensions and the observations, as depicted in Fig. 2.",2. Background,[0],[0]
"We further review the VAE specification in Sec. 3 and its implementation in Sec. 5.
",2. Background,[0],[0]
"A common approach to encourage simple and interpretable models is through use of sparsity inducing penalties such as the lasso (Tibshirani, 1994) and group lasso (Yuan & Lin, 2006).",2. Background,[0],[0]
"These methods work by shrinking many model parameters toward zero and have seen great success in regression models, covariance selection (Danaher et al.), and linear factor analysis (Hirose & Konishi, 2012).",2. Background,[0],[0]
The group lasso penalty is of particular interest to us as it simultaneously shrinks entire groups of model parameters toward zero.,2. Background,[0],[0]
"The usage of group lasso penalties for learning structured inputs to neural networks was explored in Tank et al. (2018) previously, and was inspirational to this work.
",2. Background,[0],[0]
"To specify a valid generative model, we focus on sparsityinducing priors for the parameters of the generator network.",2. Background,[0],[0]
"Historically, the spike-and-slab prior (Mitchell & Beauchamp, 1988) was used to encourage sparsity in Bayesian models.",2. Background,[0],[0]
The prior consists of a two-component mixture with mass on a model parameter being exactly zero.,2. Background,[0],[0]
"Unfortunately, inference in spike-and-slab models is difficult because of the combinatorial nature of the resulting posterior.",2. Background,[0],[0]
"A more computationally tractable family arises from the class of global-local shrinkage priors (Polson & Scott, 2010).",2. Background,[0],[0]
"One popular example is the horseshoe prior (Bhadra et al., 2016).",2. Background,[0],[0]
"However, these priors do not result in exact zeros, making interpretability difficult.
",2. Background,[0],[0]
A sophisticated hierarchical Bayesian prior for sparse group linear factor analysis has recently been developed by Zhao et al. (2016).,2. Background,[0],[0]
"This prior encourages both a sparse set of factors to be used as well as having the factors themselves
be sparse.",2. Background,[0],[0]
The resulting model admits an efficient EM algorithm.,2. Background,[0],[0]
"This builds on previous work on group factor analysis (Virtanen et al., 2012; Klami et al., 2015).",2. Background,[0],[0]
"Sparsity inducing hierarchical Bayesian priors have also been applied to learn the complexity of the Bayesian deep neural networks (Louizos et al., 2017; Ghosh & Doshi-Velez, 2017).",2. Background,[0],[0]
"Our focus, however, is on using (structured) sparsityinducing hierarchical Bayesian priors in the context of deep learning for the sake of interpretability, as in linear factor analysis, rather than model selection.",2. Background,[0],[0]
We frame our proposed output interpretable VAE (oi-VAE) method using the same terminology as the VAE.,3. The oi-VAE model,[0],[0]
Let x ∈,3. The oi-VAE model,[0],[0]
"RD denote a D-dimensional observation and z ∈ RK denote the associated latent representation of fixed dimension K. We then write the generative process of the model as:
z ∼ N (0, I) (1) x",3. The oi-VAE model,[0],[0]
"∼ N (fθ(z),D), (2)
where D is a diagonal matrix containing the marginal variances of each component of x.",3. The oi-VAE model,[0],[0]
The generator is encoded with the function fθ(·) specified as a deep neural network with parameters θ.,3. The oi-VAE model,[0],[0]
Note that the formulation in Eq.,3. The oi-VAE model,[0],[0]
(2) is simpler than that described in Kingma & Welling (2013) where the noise variances were observation specific.,3. The oi-VAE model,[0],[0]
"This simplifying assumption is common with traditional factor models, but could easily be relaxed.
",3. The oi-VAE model,[0],[0]
"When our observations x admit a natural grouping over the components, we write x =",3. The oi-VAE model,[0],[0]
"[x(1), . . .",3. The oi-VAE model,[0],[0]
",x(G)] for each of the G groups.",3. The oi-VAE model,[0],[0]
We model the components within each group g ∈,3. The oi-VAE model,[0],[0]
[G] with separate generative networks f (g)θg parameterized by θg.,3. The oi-VAE model,[0],[0]
"It is possible to share generator parameters θg
across groups, however we chose to model each separately.",3. The oi-VAE model,[0],[0]
"Critically, the latent representation z is shared across all of the group-specific generators.",3. The oi-VAE model,[0],[0]
"In particular:
z ∼ N (0, I) (3)
x(g)",3. The oi-VAE model,[0],[0]
"∼ N (f (g)θg (z),Dg).",3. The oi-VAE model,[0],[0]
"(4)
To this point, our specified group-structured VAE can describe within-group and cross-group correlation structure.",3. The oi-VAE model,[0],[0]
"However, one of the primary goals of this framework is to capture interpretable relationships between groups through the latent representation.
",3. The oi-VAE model,[0],[0]
"Inspired by the sparse factor analysis literature, we extract notions of interpretable interactions by encouraging sparse latent-to-group mappings.",3. The oi-VAE model,[0],[0]
"Specifically, we insert a groupspecific linear transformation W(g) ∈ Rp×K between the latent representation z and the group generator f (g):
x(g) ∼ N (f (g)θ (W (g)z),Dg).",3. The oi-VAE model,[0],[0]
"(5)
We refer to W(g) as the latent-to-group matrix.",3. The oi-VAE model,[0],[0]
"For simplicity, we assume that each generator has input dimension p. When the jth column of the latent-to-group matrix for group g, W(g)·,j , is all zeros then the jth latent dimension, zj , will have no influence on group g in the generative process.",3. The oi-VAE model,[0],[0]
"To induce this column-wise sparsity, we place a hierarchical Bayesian prior on the columns W(g)·,j as follows (Kyung et al., 2010):
γ2gj ∼ Gamma ( p+ 1
2 , λ2 2
) (6)
W (g) ·,j ∼ N (0, γ 2 gjI) (7)
where Gamma(·, ·) is defined by shape and rate.",3. The oi-VAE model,[0],[0]
"The rate parameter λ defines the amount of sparsity, with larger λ implying more column-wise sparsity in W(g).",3. The oi-VAE model,[0],[0]
"Marginalizing over γ2gj induces group sparsity over the columns of W(g); the MAP of the resulting posterior is equivalent to a group lasso penalized objective (Kyung et al., 2010).
",3. The oi-VAE model,[0],[0]
"Unlike linear factor models, the deep structure of our model allows rescaling of the parameters across layer boundaries without affecting the end behavior of the network (Neyshabur et al., 2015).",3. The oi-VAE model,[0],[0]
"In particular, it is possible— and in fact encouraged behavior—to learn a set of W(g) matrices with very small weights and a subsequent layer with very large weights that nullify the shrinkage imposed by the sparsity-inducing prior.",3. The oi-VAE model,[0],[0]
"In order to mitigate this we additionally place a standard normal prior with fixed scale on the parameters of each generative network, θg ∼ N (0, I).
",3. The oi-VAE model,[0],[0]
Special cases of the oi-VAE There are a few notable special cases of our oi-VAE framework.,3. The oi-VAE model,[0],[0]
"When we treat the observations as forming a single group, the model resembles
a traditional VAE since there is a single generator.",3. The oi-VAE model,[0],[0]
"However, the sparsity inducing prior still has an effect that differs from the standard VAE.",3. The oi-VAE model,[0],[0]
"In particular, by shrinking columns of W (dropping the g superscript) the prior will essentially encourage a sparse subset of the components of z to be used to explain the data, similar to a traditional sparse factor model.",3. The oi-VAE model,[0],[0]
"Note that the z’s themselves will still be standard normal, but the columns of W will dictate which components are used.",3. The oi-VAE model,[0],[0]
"This regularization may be advantageous even in the classical, single-group setting as it can provide improved generalization performance in the case of limited training data.",3. The oi-VAE model,[0],[0]
Another special case arises when the generator networks are given by the identity mapping.,3. The oi-VAE model,[0],[0]
"In this case, the only transformation of the latent representation is given by W(g) and the oi-VAE reduces to a classical group sparse linear factor model.",3. The oi-VAE model,[0],[0]
"In oi-VAE, each latent factor influences a sparse set of the observational groups.",4. Interpretability of the oi-VAE,[0],[0]
"The interpretability garnered from this sparse structure is two-fold:
Disentanglement of latent embeddings By associating each component of z with only a sparse subset of the observational groups, we are able to quickly identify disentangled representations in the latent space.",4. Interpretability of the oi-VAE,[0],[0]
"That is, by penalizing interactions between the components of z and each of the groups, we effectively force the model to arrive at a representation that minimizes correlation across the components of z, encouraging each dimension to capture distinct modes of variation.",4. Interpretability of the oi-VAE,[0],[0]
"For example, in Table 1 we see that each of the dimensions of the latent space learned on motion capture recordings of human motion corresponds to a direction of variation relevant to only a subset of the joints (groups) that are used in specific submotions related to walking.",4. Interpretability of the oi-VAE,[0],[0]
"Additionally, it is observed that although the VAE and oi-VAE have similar reconstruction performance the meaningfully disentangled latent representation allows oi-VAE to produce superior unconditional random samples.
",4. Interpretability of the oi-VAE,[0],[0]
"Discovery of group interactions Disregarding any interest in the learned representation z, each latent dimension influences only a sparse subset of the observational groups.",4. Interpretability of the oi-VAE,[0],[0]
"As such, we can view the observational groups associated with a specific latent dimension as a related system of sorts.",4. Interpretability of the oi-VAE,[0],[0]
"For example, in neuroscience often our goal is to uncover functionally-connected brain networks.",4. Interpretability of the oi-VAE,[0],[0]
In this setting we may split the signal into groups based on a standard parcellation.,4. Interpretability of the oi-VAE,[0],[0]
"Then networks can be identified by inspecting the subset of groups influenced by a component in the latent code, zi.",4. Interpretability of the oi-VAE,[0],[0]
Such an approach is attractive in the context of analyzing functional connectivity from MEG data where we seek modules of highly correlated regions.,4. Interpretability of the oi-VAE,[0],[0]
"See the ex-
periments of Sec. 6.3.",4. Interpretability of the oi-VAE,[0],[0]
"Likewise, in our motion capture experiments of Sec. 6.2, we see (again from Table 1) how we can treat collections of joints as a system that covary in meaningful ways within a given human motion category.
",4. Interpretability of the oi-VAE,[0],[0]
"Broadly speaking, the relationship between dimensions of z and observational groups can be thought of as a bipartite graph in which we can quickly identify correlation and independence relationships among the groups themselves.",4. Interpretability of the oi-VAE,[0],[0]
The ability to expose or refute correlations among observational groups is attractive as an exploratory scientific tool independent of building a generative model.,4. Interpretability of the oi-VAE,[0],[0]
"This is especially useful since standard measures of correlation are linear, leaving much to be desired in the face of high-dimensional data with many potential nonlinear relationships.",4. Interpretability of the oi-VAE,[0],[0]
"Our hope is that oi-VAE serves as one initial tool to spark a new wave of interest in nonlinear factor models and their application to complicated and rich data across a variety of fields.
",4. Interpretability of the oi-VAE,[0],[0]
It is worth emphasizing that the goal is not to learn sparse representations in the z’s.,4. Interpretability of the oi-VAE,[0],[0]
"Sparsity in z may be desirable in certain contexts, but it does not actually provide any interpretability in the data generating process.",4. Interpretability of the oi-VAE,[0],[0]
"Still, we find that oi-VAE does prune dimensions that are not necessary in synthetic examples.",4. Interpretability of the oi-VAE,[0],[0]
"Traditionally, VAEs are learned by applying stochastic gradient methods directly to the evidence lower bound (ELBO):
log p(x) ≥ Eqφ(z|x)[log pθ(x, z)− log qφ(z|x)],
where qφ(z|x) denotes the amortized posterior distribution of z given observation x, parameterized by a neural network with weights φ.",5. Collapsed variational inference,[0],[0]
Using a neural network to parameterize the observation distribution p(x|z) as in Eq.,5. Collapsed variational inference,[0],[0]
(1) makes the expectation in the ELBO intractable.,5. Collapsed variational inference,[0],[0]
"To address this, the VAE employs Monte Carlo variational inference (MCVI) (Kingma & Welling, 2013):",5. Collapsed variational inference,[0],[0]
"The troublesome expectation is approximated with samples of the latent variables from the variational distribution, z ∼ qφ(z|x), where qφ(z|x) is reparameterized to allow differentiating through the expectation operator in order to reduce gradient variance.
",5. Collapsed variational inference,[0],[0]
We extend the basic VAE amortized inference procedure to incorporate our sparsity inducing prior over the columns of the latent-to-group matrices.,5. Collapsed variational inference,[0],[0]
"The naive approach of optimizing variational distributions for γ2gj and W (g) ·,j will not result in true sparsity of the columns W(g)·,j .",5. Collapsed variational inference,[0],[0]
"Instead, we consider a collapsed variational objective function.",5. Collapsed variational inference,[0],[0]
"Since our sparsity inducing prior over W(g)·,j is marginally equivalent to the convex group lasso penalty we can use proximal gradient descent on the collapsed objective and obtain true group sparsity (Parikh & Boyd, 2013).",5. Collapsed variational inference,[0],[0]
"Following the standard VAE approach of Kingma & Welling (2013), we use sim-
ple point estimates for the variational distributions on the neural network parametersW =",5. Collapsed variational inference,[0],[0]
"( W(1), · · · ,W(G) )",5. Collapsed variational inference,[0],[0]
"and θ = (θ1, . . .",5. Collapsed variational inference,[0],[0]
", θG)",5. Collapsed variational inference,[0],[0]
.,5. Collapsed variational inference,[0],[0]
"We take qφ(z|x) = N (µ(x), σ2(x)))",5. Collapsed variational inference,[0],[0]
where the mean and variances are parameterized by an inference network with parameters φ.,5. Collapsed variational inference,[0],[0]
"We construct a collapsed variational objective by marginalizing the γ2gj to compute log p(x) as:
log ∫ p(x|z,W, θ)p(z)p(W|γ2)p(γ2)p(θ) dγ2 dz
= log ∫ (∫ p(W, γ2) dγ2 )",5.1. The collapsed objective,[0],[0]
"p(x|z,W, θ)p(z)p(θ) qφ(z|x)/qφ(z|x) dz
≥ Eqφ(z|x)",5.1. The collapsed objective,[0],[0]
"[log p(x|z,W, θ)]−KL(qφ(z|x)||p(z))",5.1. The collapsed objective,[0],[0]
"+ log p(θ)− λ ∑ g,j ||W(g)·,j ||2
, L(φ, θ,W).
",5.1. The collapsed objective,[0],[0]
"Importantly, the columns of the latent-to-group matrices W
(g) ·,j appear in a 2-norm penalty in the collapsed ELBO.",5.1. The collapsed objective,[0],[0]
"This is exactly a group lasso penalty on the columns of W
(g) ·,j and encourages the entire column to be set to zero.
",5.1. The collapsed objective,[0],[0]
"Now our goal becomes maximizing this collapsed ELBO over φ, θ, andW .",5.1. The collapsed objective,[0],[0]
"Since this objective contains a standard group lasso penalty, we can leverage efficient proximal gradient descent updates on the latent-to-group matrices W as detailed in Sec. 5.2.",5.1. The collapsed objective,[0],[0]
Proximal algorithms achieve better rates of convergence than sub-gradient methods and have shown great success in solving convex objectives with group lasso penalties.,5.1. The collapsed objective,[0],[0]
"We can use any off-the-shelf optimization method for the remaining neural net parameters, θg and φ.",5.1. The collapsed objective,[0],[0]
"Proximal gradient descent algorithms are a broad class of optimization techniques for separable objectives with both differentiable and potentially non-differentiable components,
min x g(x) + h(x), (8)
where g(x) is differentiable and h(x) is potentially nonsmooth or non-differentiable (Parikh & Boyd, 2013).",5.2. Proximal gradient descent,[0],[0]
Stochastic proximal algorithms are well-studied for convex optimization problems.,5.2. Proximal gradient descent,[0],[0]
"Recent work has shown that some variants are guaranteed to converge to a first-order stationary point even if the objective is comprised of a non-convex g(x) as long as the non-smooth h(x) is convex (Reddi et al., 2016).",5.2. Proximal gradient descent,[0],[0]
"The usual tactic is to take gradient steps on g(x) followed by “corrective” proximal steps to respect h(x):
xt+1 = proxηh(xt − η∇g(xt))",5.2. Proximal gradient descent,[0],[0]
"(9)
Algorithm 1 Collapsed VI for oi-VAE
Input: data x(i), sparsity parameter λ Let L̃ = L(φ, θ,W) +",5.2. Proximal gradient descent,[0],[0]
"λ ∑ g,j ||W (g) ·,j ||2.",5.2. Proximal gradient descent,[0],[0]
"repeat Calculate∇φL̃,∇θL̃, and ∇W L̃.",5.2. Proximal gradient descent,[0],[0]
Update φ,5.2. Proximal gradient descent,[0],[0]
and θ with an optimizer of your choice.,5.2. Proximal gradient descent,[0],[0]
LetWt+1 =Wt − η∇W L̃. for all groups g and j,5.2. Proximal gradient descent,[0],[0]
"= 1 to K do
Set W(g)·,j ← W
(g) ·,j
||W(g)·,j ||2
( ||W(g)·,j ||2 − ηλ )",5.2. Proximal gradient descent,[0],[0]
"+
end for until convergence in both L̂ and −λ ∑ g,j ||W (g) ·,j ||2
where proxf (x) is the proximal operator for the function f .",5.2. Proximal gradient descent,[0],[0]
"For example, if h(x) is the indicator function for a convex set then the proximal operator is simply the projection operator onto the set and the update in Eq.",5.2. Proximal gradient descent,[0],[0]
(9) is projected gradient.,5.2. Proximal gradient descent,[0],[0]
Expanding the definition of proxηh in Eq.,5.2. Proximal gradient descent,[0],[0]
"(9), one can see that the proximal step corresponds to minimizing h(x) plus a quadratic approximation to g(x) centered on xt.",5.2. Proximal gradient descent,[0],[0]
"For h(x) = λ||x||2, the proximal operator is given by
proxηh(x) = x
||x||2 (||x||2 − ηλ)+",5.2. Proximal gradient descent,[0],[0]
"(10)
",5.2. Proximal gradient descent,[0],[0]
"where (v)+ , max(0, v) (Parikh & Boyd, 2013)",5.2. Proximal gradient descent,[0],[0]
.,5.2. Proximal gradient descent,[0],[0]
"Geometrically, this operator reduces the norm of x by ηλ, and shrinks x’s with ||x||2 ≤ ηλ to zero.",5.2. Proximal gradient descent,[0],[0]
"This operator is especially convenient since it is both cheap to compute and results in machine-precision zeros, unlike many Bayesian approaches to sparsity that result in small but non-zero values and thus require an extra thresholding step to attain exact zeros.
",5.2. Proximal gradient descent,[0],[0]
"We experimented with standard (non-collapsed) variational inference as well other schemes, but found that collapsed variational inference with proximal updates provided faster convergence and succeeded in identifying sparser models than other techniques.",5.2. Proximal gradient descent,[0],[0]
In practice we apply proximal stochastic gradient updates per Eq.,5.2. Proximal gradient descent,[0],[0]
"(9) on theW matrices and Adam (Kingma & Ba, 2014) on the remaining parameters.",5.2. Proximal gradient descent,[0],[0]
See Alg. 1 for complete pseudocode.,5.2. Proximal gradient descent,[0],[0]
"To evaluate oi-VAE’s ability to identify sparse models on well-understood data, we generated 8× 8 images with one randomly selected row of pixels shaded and additive noise corrupting the entire image.",6.1. Synthetic data,[0],[0]
We then built and trained an oi-VAE model on the images with each group defined as an entire row of pixels in the image.,6.1. Synthetic data,[0],[0]
We used an 8-dimensional latent space in order to encourage the model to associate each dimension of z with a unique row in the image.,6.1. Synthetic data,[0],[0]
"Results
are shown in Fig. 2.",6.1. Synthetic data,[0],[0]
Our oi-VAE successfully disentangles each of the dimensions of z to correspond to exactly one row (group) of the image.,6.1. Synthetic data,[0],[0]
We also trained an oi-VAE model with a 16-dimensional latent space (see the Supplement) and see that when additional latent components are not needed to describe any group they are pruned from the model.,6.1. Synthetic data,[0],[0]
Using data collected from CMU’s motion capture database we evaluated oi-VAE’s ability to handle complex physical constraints and interactions across groups of joint angles while simultaneously identifying a sparse decomposition of human motion.,6.2. Motion Capture,[0],[0]
The dataset consists of 11 examples of walking and one example of brisk walking from the same subject.,6.2. Motion Capture,[0],[0]
The recordings measure 59 joint angles split across 29 distinct joints.,6.2. Motion Capture,[0],[0]
The joint angles were normalized from their full ranges to lie between zero and one.,6.2. Motion Capture,[0],[0]
"We treat the set of measurements from each distinct joint as a group; since each joint has anywhere from 1 to 3 observed degrees of freedom, this setting demonstrates how oi-VAE can handle variable-sized groups.",6.2. Motion Capture,[0],[0]
"For training, we randomly sample 1 to 10 walking trials, resulting in up to 3791 frames.",6.2. Motion Capture,[0],[0]
"Our experiments evaluate the following performance metrics: interpretability of the learned interaction structure amongst groups and of the latent representation; test log-likelihood, assessing the model’s generalization ability; and both conditional and unconditional samples to evaluate the quality of the learned generative process.",6.2. Motion Capture,[0],[0]
"In all experiments, we use λ = 1.",6.2. Motion Capture,[0],[0]
"For further details on the specification of all considered models (VAE and oi-VAE), see the Supplement.
To begin, we train our oi-VAE on the full set of 10 training trials with the goal of examining the learned latent-togroup mappings.",6.2. Motion Capture,[0],[0]
"To explore how the learned disentangled latent representation varies with latent dimension K, we use K = 4, 8, and 16.",6.2. Motion Capture,[0],[0]
The results are summarized in Fig. 3.,6.2. Motion Capture,[0],[0]
"We see that as K increases, individual “features” (i.e., components of z) are refined to capture more localized anatomical structures.",6.2. Motion Capture,[0],[0]
"For example, feature 2 in the K = 4 case turns into feature 7 in the K = 16 case, but in that case we also add feature 3 to capture just variations of lfingers, lthumb separate from head, upperneck, lowerneck.",6.2. Motion Capture,[0],[0]
"Likewise, feature 2 when K = 16 repre-
sents head, upperneck, lowerneck separately from lfingers, lthumb.",6.2. Motion Capture,[0],[0]
"To help interpret the learned disentangled latent representation, for the K = 16 embedding we provide lists of the 3 joints per dimension that are most strongly influenced by that component.",6.2. Motion Capture,[0],[0]
"From these lists, we see how the learned decomposition of the latent representation has an intuitive anatomical interpretation.",6.2. Motion Capture,[0],[0]
"For example, one of the very prominent features is feature 14, which jointly influences the thorax, upperback, and lowerback.",6.2. Motion Capture,[0],[0]
"Collectively, these results clearly demonstrate how the oi-VAE provides meaningful interpretability.",6.2. Motion Capture,[0],[0]
"We emphasize that it is not even possible to make these types of images or lists for the VAE.
",6.2. Motion Capture,[0],[0]
"One might be concerned that by gaining interpretability, we lose out on expressivity.",6.2. Motion Capture,[0],[0]
"However, as we demonstrate in Table 2 and Figs. 4-5, the regularization provided by our sparsity-inducing penalty actually leads to as good or better performance.",6.2. Motion Capture,[0],[0]
We first examine oi-VAE and VAE’s ability to generalize to held out data.,6.2. Motion Capture,[0],[0]
"To examine robustness to different amounts of training data, we consider training on increasing numbers of walk trials and testing on a single heldout example of either walk or brisk walk.",6.2. Motion Capture,[0],[0]
"The latter represents an example of data that is a variation of what was trained on, whereas the former is a heldout example, very similar to the training data.",6.2. Motion Capture,[0],[0]
"In Table 2, we see the benefit of the regularization in oi-VAE in both test scenarios in the limited data regime.",6.2. Motion Capture,[0],[0]
"Unsurprisingly, for the full 10 trials, there are little to no differences between the generalization abilities of oi-VAE and VAE (though of course the oi-VAE still provides interpretability).",6.2. Motion Capture,[0],[0]
"We highlight that
when we have both a limited amount of training data that might not be fully representative of the full possible dataset of interest (e.g., all types of walking), the regularization provided by oi-VAE provides dramatic improvements for generalization.",6.2. Motion Capture,[0],[0]
"Finally, in almost all scenarios, the more decomposed oi-VAE K = 16 setting has better or comparable performance to smaller K settings.",6.2. Motion Capture,[0],[0]
"We leave choosing K and investigating the effects of pruning to future work.
",6.2. Motion Capture,[0],[0]
"Next, we turn to assessing the learned oi-VAE’s generative process relative to that of the VAE.",6.2. Motion Capture,[0],[0]
"In Fig. 4 we take our test trial of walk, run each frame through the learned inference network to get a set of approximate posteriors.",6.2. Motion Capture,[0],[0]
"For every such qφ(z|x), we sample 32 times from the distribution and run each sample through the generator networks to synthesize a batch of reconstructions.",6.2. Motion Capture,[0],[0]
"To fully explore the space of human motion the learned generators can capture, we take 100 unconditional samples from both the oi-VAE and VAE models and show a representative subset in Fig. 5.",6.2. Motion Capture,[0],[0]
The full set of 100 random samples from both oi-VAE and VAE are provided in the Supplement.,6.2. Motion Capture,[0],[0]
"Note that even when trained on the full set of 10 walk trials where we see little to no difference in test log-likelihood between the oi-VAE and VAE, we do see that the learned generator for the oi-VAE is more representative of physically plausible human motion poses.",6.2. Motion Capture,[0],[0]
"We attribute this to the fact that the test log-likelihood does not encourage quality unconditional samples, but a disentangled latent representation should yield qualitatively better results on samples from the prior.",6.2. Motion Capture,[0],[0]
"Magnetoencephalography (MEG) records the weak magnetic field produced by the brain during cognitive activity
with great temporal resolution and good spatial resolution.",6.3. Magnetoencephalography,[0],[0]
Analyzing this data holds great promise for understanding the neural underpinnings of cognitive behaviors and for characterizing neurological disorders such as autism.,6.3. Magnetoencephalography,[0],[0]
"A common step when analyzing MEG data is to project the MEG sensor data into source-space where we obtain observations over time on a mesh (≈ 5-10K vertices) of the cortical surface (Gramfort et al., 2013).",6.3. Magnetoencephalography,[0],[0]
The resulting source-space signals likely live on a low-dimensional manifold making methods such as the VAE attractive.,6.3. Magnetoencephalography,[0],[0]
"Still, neuroscientists have meticulously studied particular brain regions of interest and what behaviors they are involved in by hand.
",6.3. Magnetoencephalography,[0],[0]
"We apply our oi-VAE method to infer low-rank representations of source-space MEG data where the groups are specified as the ≈ 40 regions defined in the HCP-MMP1 brain parcellation (Glasser et al., 2016).",6.3. Magnetoencephalography,[0],[0]
See Fig.,6.3. Magnetoencephalography,[0],[0]
6(left).,6.3. Magnetoencephalography,[0],[0]
The recordings were collected from a single subject performing an auditory attention task where they were asked to maintain their attention to one of two auditory streams.,6.3. Magnetoencephalography,[0],[0]
We use 106 trials each of length 385.,6.3. Magnetoencephalography,[0],[0]
"We treat each time point
Table 2.",6.3. Magnetoencephalography,[0],[0]
"Test log-likelihood for VAE and oi-VAE trained on 1,2,5, or 10 trials of walk data.",6.3. Magnetoencephalography,[0],[0]
Table includes results for a test walk (same as training) or brisk walk trial (unseen in training).,6.3. Magnetoencephalography,[0],[0]
Bold numbers indicate the best performance.,6.3. Magnetoencephalography,[0],[0]
"The standard VAE uses the same structure as oi-VAE for a consistent comparison (equivalent to λ = 0).
",6.3. Magnetoencephalography,[0],[0]
"STANDARD WALK BRISK WALK
# TRIALS 1 2 5 10 1 2 5 10
VAE (K = 16) −3, 518 −251 18 114 −723, 795 −15, 413, 445 −19, 302, 644 −19, 303, 072",6.3. Magnetoencephalography,[0],[0]
"OI-VAE (K = 4) −2,722 −214 27 70 −664, 608 −13, 438, 602 −19,289,548 −19, 302, 680 OI-VAE (K = 8) −3, 196 −195 29 75 −283, 352 −10, 305, 693 −19, 356, 218 −19, 302, 764 OI-VAE (K = 16) −3, 550 −188 31 108 −198,663 −6,781,047 −19, 299, 964 −19, 302, 924
of each trial as an i.i.d.",6.3. Magnetoencephalography,[0],[0]
observation resulting in ≈ 41K observations.,6.3. Magnetoencephalography,[0],[0]
"For details on the specification of all considered models, see the Supplement.
",6.3. Magnetoencephalography,[0],[0]
For each region we compute the average source-space activity over all vertices in each region resulting in 44- dimensional observations.,6.3. Magnetoencephalography,[0],[0]
"We applied oi-VAE withK = 20, λ = 10, and Alg.",6.3. Magnetoencephalography,[0],[0]
"1 for 10, 000 iterations.",6.3. Magnetoencephalography,[0],[0]
"In Fig. 6 we depict the learned group-weights ||W(g)·,j ||2 for all groups g and components j. We observe that each component manifests itself in a sparse subset of the regions.",6.3. Magnetoencephalography,[0],[0]
"Next, we dig into specific latent components and evaluate whether each influences a subset of regions in a neuroscientifically interpretable manner.
",6.3. Magnetoencephalography,[0],[0]
"For a given latent component zj , the value ||W(g)·,j ||2 allows us to interpret how much component j influences region g.",6.3. Magnetoencephalography,[0],[0]
We visualize some of these weights for two prominent learned components in Fig. 7.,6.3. Magnetoencephalography,[0],[0]
"Specifically, we find that component 6 captures the regions that make up the dorsal attention network pertaining to an auditory spatial task, viz., early visual, auditory sensory areas as well as inferior parietal sulcus and the region covering the right temporoparietal junction (Lee et al., 2014).",6.3. Magnetoencephalography,[0],[0]
"We also find that component 15 corresponds to regions associated with the default mode network, viz., medial prefrontal as well as posterior cingulate cortex (Buckner et al., 2008).",6.3. Magnetoencephalography,[0],[0]
Again the oi-VAE leads to interpretable results that align with meaningful and previously studied physiological systems.,6.3. Magnetoencephalography,[0],[0]
"These systems can be
Figure 7.",6.3. Magnetoencephalography,[0],[0]
Influence of z6 (top) and z15 (bottom) on the HCPMMP1 regions.,6.3. Magnetoencephalography,[0],[0]
"Active regions (shaded) correspond to the dorsal attention network and default mode network, respectively.
further probed through functional connectivity analysis.",6.3. Magnetoencephalography,[0],[0]
See the Supplement for the analysis of more components.,6.3. Magnetoencephalography,[0],[0]
We proposed an output interpretable VAE (oi-VAE) that can be viewed as either a nonlinear group latent factor model or as a structured VAE with disentangled latent embeddings.,7. Conclusion,[0],[0]
The approach combines deep generative models with a sparsity-inducing prior that leads to our ability to extract meaningful notions of latent-to-observed interactions when the observations are structured into groups.,7. Conclusion,[0],[0]
"From this interaction structure, we can infer correlated systems of interaction amongst the observational groups.",7. Conclusion,[0],[0]
In our motion capture and MEG experiments we demonstrated that the resulting systems are physically meaningful.,7. Conclusion,[0],[0]
"Importantly, this interpretability does not appear to come at the cost of expressiveness, and in our group-structured case can actually lead to improved generalization and generative processes.
",7. Conclusion,[0],[0]
"In contrast to alternative approaches for nonlinear group sparse factor analysis, leveraging the amortized inference associated with VAEs leads to computational efficiencies.",7. Conclusion,[0],[0]
We see even more significant gains through our proposed collapsed objective.,7. Conclusion,[0],[0]
"The proximal updates we can apply lead quickly to true sparsity.
",7. Conclusion,[0],[0]
Note that nothing fundamentally prevents applying this architecture to other generative models du jour.,7. Conclusion,[0],[0]
"Extending this work to generative adversarial models, for example, should be straightforward (Goodfellow et al., 2014).",7. Conclusion,[0],[0]
Oy-vey!,7. Conclusion,[0],[0]
Special thanks to Ian Covert and Alex Tank for helpful discussions and insight.,Acknowledgements,[0],[0]
"This work was supported by ONR Grant N00014-15-1-2380, NSF CAREER Award IIS-1350133, NSF CRCNS Grant NSF-IIS-1607468, and AFOSR Grant FA9550-16-1-0038.",Acknowledgements,[0],[0]
The authors also gratefully acknowledge the support of NVIDIA Corporation for the donated GPU used for this research.,Acknowledgements,[0],[0]
Deep generative models have recently yielded encouraging results in producing subjectively realistic samples of complex data.,abstractText,[0],[0]
Far less attention has been paid to making these generative models interpretable.,abstractText,[0],[0]
"In many scenarios, ranging from scientific applications to finance, the observed variables have a natural grouping.",abstractText,[0],[0]
"It is often of interest to understand systems of interaction amongst these groups, and latent factor models (LFMs) are an attractive approach.",abstractText,[0],[0]
"However, traditional LFMs are limited by assuming a linear correlation structure.",abstractText,[0],[0]
"We present an output interpretable VAE (oi-VAE) for grouped data that models complex, nonlinear latent-to-observed relationships.",abstractText,[0],[0]
We combine a structured VAE comprised of group-specific generators with a sparsity-inducing prior.,abstractText,[0],[0]
We demonstrate that oi-VAE yields meaningful notions of interpretability in the analysis of motion capture and MEG data.,abstractText,[0],[0]
"We further show that in these situations, the regularization inherent to oi-VAE can actually lead to improved generalization and learned generative processes.",abstractText,[0],[0]
oi-VAE: Output Interpretable VAEs for Nonlinear Group Factor Analysis,title,[0],[0]
Low rank matrix optimization stands as a major tool in modern dimensionality reduction and unsupervised learning.,1. Introduction,[0],[0]
The singular value decomposition can be used when the optimization objective is rotationally invariant to the parameters.,1. Introduction,[0],[0]
"However, if we wish to optimize over more complex, non-convex objectives we must choose to either rely on convex relaxations (Recht et al., 2010; Negahban & Wainwright, 2011; Rohde & Tsybakov, 2011) or directly optimize over the non-convex space (Park et al., 2016; Jain et al., 2013; Chen & Wainwright, 2015; Lee & Bresler, 2013; Jain et al., 2014).
",1. Introduction,[0],[0]
"More concretely, in the low rank matrix optimization problem, we wish to solve
argmax
⇥
`(⇥) s.t. rank(⇥)  ",1. Introduction,[0],[0]
"r. (1)
Rather than perform the computationally intractable optimization above researchers have studied convex relaxations of the form
argmax
⇥
`(⇥) |||⇥||| nuc .
",1. Introduction,[0],[0]
1UT Austin 2Yale University.,1. Introduction,[0],[0]
"Correspondence to: Rajiv Khanna <rajivak@utexas.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"Unfortunately, the above optimization can be computationally taxing.",1. Introduction,[0],[0]
"General purpose solvers for the above optimization problem that rely on semidefinite programming (SDP) require ⇥(n3d3) computation, which is prohibitive.",1. Introduction,[0],[0]
Gradient descent techniques require ⇥(✏ 1/2(n3 + d3)) computational cost for an epsilon accurate solution.,1. Introduction,[0],[0]
This improvement is sizable in comparison to SDP solvers.,1. Introduction,[0],[0]
"Unfortunately, it is still prohibitive for large scale matrix estimation.
",1. Introduction,[0],[0]
An alternate vein of research has focused on directly optimizing the non-convex problem (1).,1. Introduction,[0],[0]
"To that end, authors have seen recent theoretical success in studying the convergence properties of
argmax U2Rn⇥r,V2Rd⇥r",1. Introduction,[0],[0]
"`(UVT ).
",1. Introduction,[0],[0]
"Solving the problem above automatically forces the solution to be low rank, and recent results have shown promising behavior.
",1. Introduction,[0],[0]
"Another approach is to optimize (1) incrementally via rank one updates to the current estimate (Shalev-Shwartz et al., 2011; Wang et al., 2015).",1. Introduction,[0],[0]
"This approach has also been studied in more general contexts such as boosting (Buhlmann & Yu, 2009), coordinate descent (Jaggi, 2013; Jaggi & Sulovský, 2010), and incremental atomic norm optimization (Gribonval & Vandergheynst, 2006; Barron et al., 2008; Khanna et al., 2016; Rao et al., 2015; Dudik et al., 2012; Locatello et al., 2017).",1. Introduction,[0],[0]
"In this paper, we interpret low rank matrix estimation as a set optimization problem over an infinite set of atoms.",1.1. Set Function Optimization and Coordinate Descent,[0],[0]
"Specifically, we wish to optimize
argmax {X1,...Xk}2A",1.1. Set Function Optimization and Coordinate Descent,[0],[0]
"`
kX
i=1
↵iXi
! ,
where the set of atoms A is the set of all rank one matrices with unit operator norm.",1.1. Set Function Optimization and Coordinate Descent,[0],[0]
"This settings is analogous to that taken in the results studying atomic norm optimization, coordinate descent via the total variation norm, and Frank-Wolfe style algorithms for atomic optimization.",1.1. Set Function Optimization and Coordinate Descent,[0],[0]
"This formulation allows us to connect the problem of low rank matrix estimation to that of submodular set function optimization, which
we discuss in the sequel.",1.1. Set Function Optimization and Coordinate Descent,[0],[0]
Before proceeding we discuss related work and an informal statement of our main result.,1.1. Set Function Optimization and Coordinate Descent,[0],[0]
Our result demonstrates an exponential decrease in the amount of error incurred by greedily adding rank one matrices to the low rank matrix approximation.,1.2. Informal Result and Related Work,[0],[0]
"Theorem 1 (Approximation Guarantee, Informal).",1.2. Informal Result and Related Work,[0],[0]
"If we let ⇥k be our estimate of the rank r matrix ⇥⇤ at iteration k, then for some universal constant c related to the restricted condition number of the problem we have
`(⇥k) `(0) (1 exp( ck/r))(`(⇥⇤) `(0)).
",1.2. Informal Result and Related Work,[0],[0]
"Note that after k iterations the matrix ⇥k will be at most rank k.
Related work: There has been a wide array of studies looking at the computational and statistical benefits of rank one updates to estimating a low rank matrix.",1.2. Informal Result and Related Work,[0],[0]
"At its most basic, the singular value decomposition will keep adding rank one approximations through deflation steps.",1.2. Informal Result and Related Work,[0],[0]
This work can be generally segmented into two sets of results – the ones that present sublinear rates of convergence and those that obtain linear rates.,1.2. Informal Result and Related Work,[0],[0]
"Interestingly, parallel lines of work have also demonstrated similar convergence bounds for more general atomic or dictionary element approximations (Buhlmann & Yu, 2009; Gribonval & Vandergheynst, 2006; Barron et al., 2008; Khanna et al., 2016).",1.2. Informal Result and Related Work,[0],[0]
"For space constraints, we will summarize these results into two categories rather than explicitly state the results for each individual paper.
",1.2. Informal Result and Related Work,[0],[0]
"If we define the atomic norm of a matrix M 2 Rn⇥d written as |||M|||
nuc to be the sum of the singular values of that matrix, then the bounds establishing sublinear convergence behave as
`(⇥⇤) `(⇥k)  |||⇥⇤|||2 nuc
k ,
where we take ⇥⇤ to be the best rank r solution.",1.2. Informal Result and Related Work,[0],[0]
What we then see is convergence towards the optimal bound.,1.2. Informal Result and Related Work,[0],[0]
"However, we expect our statistical error to behave as r(n+ d)/n where n is the number of samples that we have received from our statistical model and ⇥⇤ is rank r (Negahban & Wainwright, 2011; Rohde & Tsybakov, 2011).",1.2. Informal Result and Related Work,[0],[0]
"We can take |||⇥⇤|||
nuc ⇡ r, which would then imply that we would need k to behave as n/(n+ d).",1.2. Informal Result and Related Work,[0],[0]
"However, that would then imply that the rank of our matrix should grow linearly in the number of observations in order to achieve the same statistical error bounds.",1.2. Informal Result and Related Work,[0],[0]
The above error bounds are “fast”.,1.2. Informal Result and Related Work,[0],[0]
"If we consider a model that yields slow error bounds, then we expect the error to behave like |||⇥⇤|||
nuc q n+d n .",1.2. Informal Result and Related Work,[0],[0]
"In that case,
we can take k |||⇥⇤|||",1.2. Informal Result and Related Work,[0],[0]
"nuc
q n
n+d , which looks better, but
still requires significant growth in k as a function of n. To overcome the above points, some authors have aimed to study similar greedy algorithms that then enjoy exponential rates of convergence as we show in our paper.",1.2. Informal Result and Related Work,[0],[0]
"These results share the most similarities with our own and behave as
`(⇥k) (1 k)`(⇥⇤).
",1.2. Informal Result and Related Work,[0],[0]
This result decays exponentially.,1.2. Informal Result and Related Work,[0],[0]
"However, when one looks at the behavior of it will typically act as exp ( 1/min(n,d)), for an n⇥ d matrix.",1.2. Informal Result and Related Work,[0],[0]
"As a result, we would need to choose k of the order of the dimensionality of the problem in order to begin to see gains.",1.2. Informal Result and Related Work,[0],[0]
"In contrast, for our result listed above, if we seek to only compare to the best rank r solution, then the gamma we find is = exp ( 1/r).",1.2. Informal Result and Related Work,[0],[0]
"Of course, if we wish to find a solution with full rank, then the bounds we stated above match the existing bounds.
",1.2. Informal Result and Related Work,[0],[0]
In order to establish our results we rely on a notion introduced in the statistical community called restricted strong convexity.,1.2. Informal Result and Related Work,[0],[0]
"This assumption has connections to ideas such as the restricted isometry property, restricted eigenvalue condition, and incoherence (Negahban & Wainwright, 2012).",1.2. Informal Result and Related Work,[0],[0]
"In the work by Shalev-Shwartz, Gonen, and Shamir (2011) a form of strong convexity condition is imposed over matrices.",1.2. Informal Result and Related Work,[0],[0]
"Under that setting, the authors demonstrate that
`(⇥k) `(⇥⇤) `(0)r
k ,
where r is the rank of ⇥⇤.",1.2. Informal Result and Related Work,[0],[0]
"In contrast, our bound behaves as
`(⇥k) `(⇥⇤) (`(⇥⇤) `(0))",1.2. Informal Result and Related Work,[0],[0]
"exp ( k/r).
",1.2. Informal Result and Related Work,[0],[0]
Our contributions: We improve upon the linear rates of convergence for low rank approximation using rank one updates by connecting the coordinate descent problem to that of submodular optimization.,1.2. Informal Result and Related Work,[0],[0]
We present this result in the sequel along with the algorithmic consequences.,1.2. Informal Result and Related Work,[0],[0]
We demonstrate the good performance of these rank one updates in the experimental section.,1.2. Informal Result and Related Work,[0],[0]
We begin by fixing some notation.,2. Background,[0],[0]
"We represent sets using sans script fonts e.g. A,B. Vectors are represented using lower case bold letters e.g. x,y, and matrices are represented using upper case bold letters e.g. X,Y. Non-bold face letters are used for scalars e.g. j,M, r and function names e.g. f(·).",2. Background,[0],[0]
The transpose of a vector or a matrix is represented by > e.g. X>.,2. Background,[0],[0]
Define,2. Background,[0],[0]
"[p] := {1, 2, . . .",2. Background,[0],[0]
", p}.",2. Background,[0],[0]
"For singleton sets, we write f(j) := f({j}).",2. Background,[0],[0]
"Size of a set S is denoted by |S|. h·, ·i is used for matrix inner product.
",2. Background,[0],[0]
Our goal is to analyze greedy algorithms for low rank estimation.,2. Background,[0],[0]
"Consider the classic greedy algorithm that picks up
the next element myopically i.e. given the solution set built so far, the algorithm picks the next element as the one which maximizes the gain obtained by adding the said element into the solution set.",2. Background,[0],[0]
Approximation guarantees for the greedy algorithm readily imply for the class of functions defined as follows.,2. Background,[0],[0]
Definition 1.,2. Background,[0],[0]
A set function f(·) :,2. Background,[0],[0]
[p]!,2. Background,[0],[0]
"R is submodular if for all A,B ✓",2. Background,[0],[0]
"[p],
f(A)",2. Background,[0],[0]
"+ f(B) f(A [ B) + f(A \ B).
",2. Background,[0],[0]
Submodular set functions are well studied and have many desirable properties that allow for efficient minimization and maximization with approximation guarantees.,2. Background,[0],[0]
Our low rank estimation problem also falls under the purview of another class of functions called monotone functions.,2. Background,[0],[0]
"A function is called monotone if and only if f(A)  f(B) for all A ✓ B. For the specific case of maximizing monotone submodular set functions, it is known that the greedy algorithm run for k iterations is guaranteed to return a solution that is within (1 1/e) of the optimum set of size",2. Background,[0],[0]
"k (Nemhauser et al., 1978).",2. Background,[0],[0]
"Without further assumptions or knowledge of the function, no other polynomial time algorithm can provide a better approximation guarantee unless P=NP (Feige, 1998).
",2. Background,[0],[0]
"More recently, the aforementioned greedy approximation guarantee has been extended to a larger class of functions called weakly submodular functions (Elenberg et al., 2016; Khanna et al., 2017).",2. Background,[0],[0]
Central to the notion of weak submodularity is a quantity called the submodularity ratio.,2. Background,[0],[0]
"Definition 2 (Submodularity Ratio (Das & Kempe, 2011)).",2. Background,[0],[0]
"Let S, L ⇢",2. Background,[0],[0]
"[p] be two disjoint sets, and f(·) :",2. Background,[0],[0]
[p]!,2. Background,[0],[0]
"R. The submodularity ratio of L with respect to S is given by
L,S := P j2S [f(L [ {j}) f(L)] f(L [ S) f(L) .",2. Background,[0],[0]
"(2)
The submodularity ratio of a set U with respect to an integer k is given by
U,k := min L,S:L\S=;, L✓U,|S|k
L,S. (3)
",2. Background,[0],[0]
"It is easy to show that f(·) is submodular if and only if L,S 1 for all sets L and S. However, an approximation guarantee is obtained when 0 <",2. Background,[0],[0]
"L,S 8 L, S (Das & Kempe, 2011; Elenberg et al., 2016).",2. Background,[0],[0]
"The subset of monotone functions which have L,S > 0 8 L, S are called weakly submodular functions in the sense that even though the function is not submodular, it still provides a provable bound for greedy selections.
",2. Background,[0],[0]
"Also vital to our analysis is the notion of restricted strong concavity and smoothness (Negahban et al., 2012; Loh & Wainwright, 2015).
",2. Background,[0],[0]
"Definition 3 (Low Rank Restricted Strong Concavity (RSC), Restricted Smoothness (RSM)).",2. Background,[0],[0]
A function ` : Rn⇥d !,2. Background,[0],[0]
"R is said to be restricted strong concave with parameter m
⌦
and restricted smooth with parameter M ⌦ if for all X,Y 2 ⌦ ⇢ Rn⇥d,
m⌦ 2 kY Xk2F",2. Background,[0],[0]
"`(Y) `(X) hr`(X),Y Xi
M⌦ 2 kY Xk2F .
",2. Background,[0],[0]
Remark 1.,2. Background,[0],[0]
"If a function `(·) has restricted strong concavity parameter m, then its negative `(·) has restricted strong convexity parameter m. We choose to use the nomenclature of concavity for ease of exposition in terms of relationship to submodular maximization.",2. Background,[0],[0]
"Further, note that we define RSC/RSM conditions on the space of matrices rather than vectors, on a domain ⌦ constrained by rank rather than sparsity.",2. Background,[0],[0]
"It is straightforward to see that if ⌦0 ✓ ⌦, then M
⌦ 0 M ⌦ and m ⌦ 0 m ⌦ .",2. Background,[0],[0]
"In this section, we delineate our setup of low rank estimation.",3. Setup,[0],[0]
"In order to connect to the weak submodular maximization framework more easily, we operate in the setting of maximization of a concave matrix variate function under a low rank constraint.",3. Setup,[0],[0]
This is equivalent to minimizing a convex matrix variate function under the low rank constraint as considered by Shalev-Shwartz et al. (2011) or under nuclear norm constraint or regularization as considered by Jaggi & Sulovský (2010).,3. Setup,[0],[0]
The goal is to maximize a function ` : Rn⇥d !,3. Setup,[0],[0]
"R:
max rank(X)r `(X).",3. Setup,[0],[0]
"(4)
Instead of using a convex relaxation of (4), our approach is to enforce the rank constraint directly by adding rank 1 matrices greedily until X is of",3. Setup,[0],[0]
rank k.,3. Setup,[0],[0]
The rank 1 matrices to be added are obtained as outer product of vectors from the given vector sets U and V .,3. Setup,[0],[0]
"While our results hold for general vector sets U ,V assuming an oracle access to subroutines GreedySel and OMPSel (to be detailed later), for the rest of the paper we focus on the case of norm 1 balls U := {x 2 Rn s.t. kxk
2 = 1} and V := {x 2 Rd s.t. kxk 2 = 1}.
",3. Setup,[0],[0]
The problem (4) can be interpreted in the context of sparsity assuming U and V are enumerable.,3. Setup,[0],[0]
"For example, by the SVD theorem, it is known that we can rewrite X",3. Setup,[0],[0]
"asPk
i=1",3. Setup,[0],[0]
↵iuiv >,3. Setup,[0],[0]
"i , where 8i, ui 2 U and vi 2 V .",3. Setup,[0],[0]
"By enumerating U and V under a finite precision representation of real values, one can rethink of the optimization (4) as finding a sparse solution for the infinite dimensional vector ↵ (Shalev-Shwartz et al., 2011; Dudik et al., 2012).",3. Setup,[0],[0]
"We can also optimize over support sets, similar to the classical setting of support selection for sparse vectors.",3. Setup,[0],[0]
"For a specified support set L consisting of vectors from U and V , let
UL and VL be the matrices formed by stacking the chosen elements of U and V respectively.",3. Setup,[0],[0]
"We define the following set function to maximize `(·) given L.
f(L) = max H2R|L|⇥|L|
`(U>L HVL) `(0).",3. Setup,[0],[0]
"(5)
We will denote the optimizing matrix for a support set L as B(L).",3. Setup,[0],[0]
"In other words, letting ˆHL be the argmax obtained in (5), then B(L)",3. Setup,[0],[0]
:= U>L ˆHLVL.,3. Setup,[0],[0]
"Thus, the low rank matrix estimation problem (4) can be reinterpreted as the following equivalent combinatorial optimization problem:
max |S|k f(S).",3. Setup,[0],[0]
(6),3. Setup,[0],[0]
"Our greedy algorithm, illustrated in Algorithm 1, builds the support set incrementally – adding rank 1 matrices one at a time such that at iteration i for 1  i  k the size of the chosen support set (and hence rank of the current iterate) is",3.1. Algorithms,[0],[0]
i. We assume access to a subroutine GreedySel for the greedy selection (Step 4).,3.1. Algorithms,[0],[0]
"This subroutine solves an inner optimization problem by calling a subroutine GreedySel which returns an atom s from the candidate support set that ensures
f(SGi 1 [ {s}) f(SGi 1) ⌧ f(SGi 1 [ {s?}) f(SGi 1) ,
where
s? argmax a2(U⇥V)?SGi 1 f(SGi 1",3.1. Algorithms,[0],[0]
"[ {a}) f(SGi 1).
",3.1. Algorithms,[0],[0]
"In words, the subroutine GreedySel ensures that the gain in f(·) obtained by using the selected atom is within ⌧ 2 (0, 1] multiplicative approximation to the atom with the best possible gain in f(·).",3.1. Algorithms,[0],[0]
"The hyperparameter ⌧ governs a tradeoff allowing a compromise in myopic gain for a possibly quicker selection.
",3.1. Algorithms,[0],[0]
"The greedy selection requires fitting and scoring every candidate support, which is often prohibitively expensive.",3.1. Algorithms,[0],[0]
"An alternative is to choose the next atom by using the linear maximization oracle used by Frank-Wolfe (Jaggi, 2013) or Matching Pursuit algorithms (Gribonval & Vandergheynst, 2006; Locatello et al., 2017).",3.1. Algorithms,[0],[0]
This step replaces Step 4 of Algorithm 1 as illustrated in Algorithm 2.,3.1. Algorithms,[0],[0]
Let L = SOi 1 be the set constructed by the algorithm at iteration (i 1).,3.1. Algorithms,[0],[0]
"The linear oracle OMPSel returns an atom s for iteration i ensuring
hr`(B(L)),usv>s i ⌧ max (u,v)2(U⇥V)?SOi 1 hr`(B(L)),uv>i.
",3.1. Algorithms,[0],[0]
The linear problem OMPSel can be considerably faster that GreedySel.,3.1. Algorithms,[0],[0]
"OMPSel reduces to finding the left and right
singular vectors of r`(B(L)) corresponding to its largest singular value.",3.1. Algorithms,[0],[0]
"If t is the number of non-zero entries in r`(B(L)), then this takes O( t
1 ⌧ (log n+ log d)) time.
",3.1. Algorithms,[0],[0]
We note that Algorithm 2 is the same as considered by Shalev-Shwartz et al. (2011) as GECO (Greedy Efficient Component Optimization).,3.1. Algorithms,[0],[0]
"However, as we shall see, our analysis provides stronger bounds than their Theorem 2.
",3.1. Algorithms,[0],[0]
"Algorithm 1 GREEDY(U , V , k, ⌧ ) 1: Input: vector sets U , V , sparsity parameter k, subrou-
tine hyperparameter ⌧ 2: SG
0 ; 3: for i = 1 . . .",3.1. Algorithms,[0],[0]
k do 4: s GreedySel(⌧) 5: SGi SGi 1,3.1. Algorithms,[0],[0]
"[ {s} 6: end for 7: return SGk , B(S G k ), f(SGk ).
",3.1. Algorithms,[0],[0]
"Algorithm 2 GECO(U , V , k, ⌧ ) same as Algorithm 1 except
4: s OMPSel(⌧)
Remark 2.",3.1. Algorithms,[0],[0]
We note that Step 5 of Algorithms 1 and 2 requires solving the RHS of (5) which is a matrix variate problem of size i2 at iteration i.,3.1. Algorithms,[0],[0]
"This refitting is equivalent to the “fully-corrective” versions of Frank-Wolfe/Matching Pursuit algorithms (Locatello et al., 2017; Lacoste-Julien & Jaggi, 2015) which, intuitively speaking, extract out all the information",3.1. Algorithms,[0],[0]
"w.r.t `(·) from the chosen set of atoms, thereby ensuring that the next rank 1 atom chosen has row and column space orthogonal to the previously chosen atoms.",3.1. Algorithms,[0],[0]
"Thus the constrained maximization on the orthogonal complement of SOi in subroutine OMPSel (SGi in GreedySel) need not be explicitly enforced, but is still shown for clarity.",3.1. Algorithms,[0],[0]
"In this section, we prove that low rank matrix optimization over the rank one atoms satisfies weak submodularity.",4. Analysis,[0],[0]
We explicitly delineate some notation and assumptions.,4. Analysis,[0],[0]
"With slight abuse of notation, we assume `(·) is mi-strongly concave and Mi-smooth over pairs of matrices of rank i.",4. Analysis,[0],[0]
"For i  j, note that mi mj and Mi Mj .",4. Analysis,[0],[0]
"Additionally, let ˜
⌦ := {(X,Y) :",4. Analysis,[0],[0]
"rank(X Y)  1}, and assume `(·) is ˜M 1 -smooth over ˜⌦.",4. Analysis,[0],[0]
"It is easy to see ˜M 1 M 1 .
",4. Analysis,[0],[0]
"First we prove that if the low rank RSC holds (Definition 3), then the submodularity ratio (Definition 2) is lower-bounded by the inverse condition number.",4. Analysis,[0],[0]
Theorem 2.,4. Analysis,[0],[0]
"Let L be a set of k rank 1 atoms and S be a set of r rank 1 atoms where we sequentially orthogonalize the atoms against L. If `(·) is mi-strongly con-
cave over matrices of rank i, and ˜M 1 -smooth over the set ˜
⌦ := {(X,Y) :",4. Analysis,[0],[0]
"rank(X Y) = 1}, then
L,r :=
P a2S",4. Analysis,[0],[0]
"[f(L [ {a}) f(L)]
f(L [ S) f(L) mr+k ˜M 1 .
",4. Analysis,[0],[0]
The proof of Theorem 2 is structured around individually obtaining a lower bound for the numerator and an upper bound for the denominator of the submodularity ratio by exploiting the concavity and convexity conditions.,4. Analysis,[0],[0]
Bounding the submodularity ratio is crucial to obtaining approximation guarantees for Algorithm 1.,4.1. Greedy Improvement,[0],[0]
Theorem 3.,4.1. Greedy Improvement,[0],[0]
"Let S := SGk be the greedy solution set obtained by running Algorithm 1 for k iterations, and let S? be an optimal support set of size r. Let `(·) be mi strongly concave on the set of matrices with rank less than or equal to i, and ˜M
1 smooth on the set of matrices in the set ˜⌦.",4.1. Greedy Improvement,[0],[0]
"Then,
f(S)",4.1. Greedy Improvement,[0],[0]
"✓ 1 1
ec1
◆ f(S?)",4.1. Greedy Improvement,[0],[0]
"✓ 1 1
ec2
◆ f(S?),
where c 1 = ⌧ S,r k r and c2 = ⌧ mr+k ˜M1 k r .
",4.1. Greedy Improvement,[0],[0]
The proof technique for the first inequality of Theorem 3 relies on lower bounding the progress made in each iteration of Algorithm 1.,4.1. Greedy Improvement,[0],[0]
"Intuitively, it exploits weak submodularity to make sure that each iteration makes enough progress, and then applies an induction argument for r iterations.",4.1. Greedy Improvement,[0],[0]
We also emphasize that the bounds in Theorem 3 are for normalized set function f(·) (which means f(;) = 0).,4.1. Greedy Improvement,[0],[0]
"A more detailed proof is presented in the appendix.
",4.1. Greedy Improvement,[0],[0]
"The bounds obtained in Theorem 3 are similar to the one obtained in submodular maximization of monotone normalized functions (Nemhauser et al., 1978).",4.1. Greedy Improvement,[0],[0]
"In fact, our result can be re-interpreted as an extension to previous results.",4.1. Greedy Improvement,[0],[0]
The greedy algorithm for submodular maximization assumes finite ground sets.,4.1. Greedy Improvement,[0],[0]
We extend this for infinite ground sets.,4.1. Greedy Improvement,[0],[0]
We can do this (for matrices) as long as we have an implementation of the oracle GreedySel.,4.1. Greedy Improvement,[0],[0]
"Once the choice is made by the oracle, standard analysis holds.",4.1. Greedy Improvement,[0],[0]
Remark 3.,4.1. Greedy Improvement,[0],[0]
Theorem 3 provides the approximation guarantees for running the greedy selection algorithm up to k iterations to obtain a rank k matrix iterate vis-a-vis the best rank r approximation.,4.1. Greedy Improvement,[0],[0]
"For r = k, and ⌧ = 1, we get an approximation bound (1 e m/M) which is reminiscent of the greedy bound of (1 1/e) under the framework of submodularity.",4.1. Greedy Improvement,[0],[0]
Note that our analysis can not be used to establish classical submodularity.,4.1. Greedy Improvement,[0],[0]
"However, establishing weak submodularity that lower bounds is sufficient to provide slightly weaker than classical submodularity guarantees.
",4.1. Greedy Improvement,[0],[0]
Remark 4.,4.1. Greedy Improvement,[0],[0]
"Theorem 3 implies that to obtain (1 ✏) approximation guarantee in the worst case, running Algorithm 1 for k = rMm⌧ log 1 ✏ ) = O(r log 1/✏) iterations suffices.",4.1. Greedy Improvement,[0],[0]
This is useful when the application allows a tradeoff: compromising on the low rank constraint a little to achieve tighter approximation guarantees.,4.1. Greedy Improvement,[0],[0]
Remark 5.,4.1. Greedy Improvement,[0],[0]
"Das & Kempe (2011) considered the special case of greedily maximizing R2 statistic for linear regression, which corresponds to classical sparsity in vectors.",4.1. Greedy Improvement,[0],[0]
"They also obtain a bound of (1 1/e ), where is the submodularity ratio for their respective setup.",4.1. Greedy Improvement,[0],[0]
This was generalized by Elenberg et al. (2016) to general concave functions under sparsity constraints.,4.1. Greedy Improvement,[0],[0]
"Our analysis is for the low rank constraint, as opposed to sparsity in vectors that was considered by them.",4.1. Greedy Improvement,[0],[0]
"In this section, we obtain approximation guarantees for Algorithm 2.",4.2. GECO Improvement,[0],[0]
"The greedy search over the infinitely many candidate atoms is infeasible, especially when ⌧ = 1.",4.2. GECO Improvement,[0],[0]
"Thus while Algorithm 1 establishes interesting theoretical connections with submodularity, it is not practical in general.",4.2. GECO Improvement,[0],[0]
"To obtain a tractable and practically useful algorithm, the greedy search is replaced by a Frank Wolfe or Matching Pursuit style linear optimization which can be easily implemented as finding the top singular vectors of the gradient at iteration i.",4.2. GECO Improvement,[0],[0]
"In this section, we show that despite the speedup, we lose very little in terms of approximation guarantees.",4.2. GECO Improvement,[0],[0]
"In fact, if the approximation factor ⌧ in OMPSel() is 1, we get the same bounds as those obtained for the greedy algorithm.",4.2. GECO Improvement,[0],[0]
Theorem 4.,4.2. GECO Improvement,[0],[0]
"Let S := SOk be the greedy solution set obtained using Algorithm 2 for k iterations, and let S? be the optimum size r support set.",4.2. GECO Improvement,[0],[0]
"Let `(·) be mr+k strongly concave on the set of matrices with rank less than or equal to (r + k), and ˜M
1 smooth on the set of matrices with rank in the set ˜⌦.",4.2. GECO Improvement,[0],[0]
"Then,
f(S) ✓ 1 1
ec3
◆ f(S?),
where c 3 = ⌧2mr+k ˜M1 k r .
",4.2. GECO Improvement,[0],[0]
The proof of Theorem 4 follows along the lines of Theorem 3.,4.2. GECO Improvement,[0],[0]
"The central idea is similar – to exploit the RSC conditions to make sure that each iteration makes sufficient progress, and then provide an induction argument for r iterations.",4.2. GECO Improvement,[0],[0]
"Unlike the greedy algorithm, however, using the submodularity ratio is no longer required.",4.2. GECO Improvement,[0],[0]
"Note that the bound obtained in Theorem 4 is similar to Theorem 3, except the exponent on the approximation factor ⌧ .",4.2. GECO Improvement,[0],[0]
Remark 6.,4.2. GECO Improvement,[0],[0]
"Our proof technique for Theorem 4 can be applied for classical sparsity to improve the bounds obtained by Elenberg et al. (2016) for OMP for support selection
under RSC, and by Das & Kempe (2011) for R2 statistic.",4.2. GECO Improvement,[0],[0]
"If ⌧ = 1, r = k, their bounds involve terms of the form O(m2/M2) in the exponent, as opposed to our bounds which only has m/M in the exponent.
",4.2. GECO Improvement,[0],[0]
Remark 7.,4.2. GECO Improvement,[0],[0]
"Similar to the greedy algorithm, to achieve a tighter approximation to best rank k solution, one can relax the low rank constraint a little by running the algorithm for r > k",4.2. GECO Improvement,[0],[0]
greedy iterations.,4.2. GECO Improvement,[0],[0]
"The result obtained by our Theorem 4 can be compared to the bound obtained by (ShalevShwartz et al., 2011)",4.2. GECO Improvement,[0],[0]
[Theorem 2] for the same algorithm.,4.2. GECO Improvement,[0],[0]
"For an ✏ multiplicative approximation, Theorem 4 implies we need r/k = O(log 1/✏).",4.2. GECO Improvement,[0],[0]
"On the other hand, Shalev-Shwartz et al. (2011) obtain an additive approximation bound with r/k = O(1/""), which is an exponential improvement.",4.2. GECO Improvement,[0],[0]
"While understanding approximation guarantees are useful, providing parameter recovery bounds can further help us understand the practical utility of greedy algorithms.",5. Recovery Guarantees,[0],[0]
"In this section, we present a general theorem that provides us with recovery bounds of the true underlying low rank structure.",5. Recovery Guarantees,[0],[0]
Theorem 5.,5. Recovery Guarantees,[0],[0]
"Suppose that an algorithm achieves the approximation guarantee:
f(Sk) Cr,kf(S?r),
where Sk is the set of size k at iteration k of the algorithm, S?r be the optimal solution for r-cardinality constrained maximization of f(·), and Cr,k be the corresponding approximation ratio guaranteed by the algorithm.",5. Recovery Guarantees,[0],[0]
"Recall that we represent by US,VS the matrices formed by stacking the vectors represented by the support set S chosen from U ,V respectively, s.t. |S| =",5. Recovery Guarantees,[0],[0]
r.,5. Recovery Guarantees,[0],[0]
"Then under mk+r RSC, with Br = U>S HVS for any H 2",5. Recovery Guarantees,[0],[0]
"Rr⇥r, we have
kB(Sk) Brk2F  4(k",5. Recovery Guarantees,[0],[0]
"+ r) kr`(Br)k2 2
m2k+r
+ 4(1",5. Recovery Guarantees,[0],[0]
"Cr,k) mk+r",5. Recovery Guarantees,[0],[0]
"[`(Br) `(0)]
Theorem 5 can be applied for Br = B(S ?",5. Recovery Guarantees,[0],[0]
"r), which is the argmax for maximizing `(·) under the low rank constraint.",5. Recovery Guarantees,[0],[0]
"It is general in the sense that it can be applied for getting recovery bounds from approximation guarantees for any algorithm, and hence is applicable for both Algorithms 1 and 2.
",5. Recovery Guarantees,[0],[0]
Statistical recovery guarantees can be obtained from Theorem 5 for specific choice of `(·) and statistical model.,5. Recovery Guarantees,[0],[0]
Consider the case of low rank matrix estimation from noisy linear measurements.,5. Recovery Guarantees,[0],[0]
Let Xi 2 Rm1⇥m2 for i 2,5. Recovery Guarantees,[0],[0]
"[n] be generated so that each entry of Xi is N (0, 1).",5. Recovery Guarantees,[0],[0]
"We observe yi = hXi,⇥?i",5. Recovery Guarantees,[0],[0]
"+ "", where ⇥?",5. Recovery Guarantees,[0],[0]
"is low rank, and say "" ⇠
N (0, 2).",5. Recovery Guarantees,[0],[0]
"Let N = m 1 m 2 , and let '(⇥) : Rm1⇥m2 !",5. Recovery Guarantees,[0],[0]
Rn be the linear operator so that ['(⇥),5. Recovery Guarantees,[0],[0]
"]i = hXi,⇥i.",5. Recovery Guarantees,[0],[0]
Our corresponding function is now `(⇥) = 1nky '(⇥),5. Recovery Guarantees,[0],[0]
"k 2 2
.",5. Recovery Guarantees,[0],[0]
"For this function, using arguments by Negahban et al. (2012), we know kr`(BS?r )",5. Recovery Guarantees,[0],[0]
"k2
2  logNn and `(B S?r )",5. Recovery Guarantees,[0],[0]
"`(0)  (r + 1)
with high probability.",5. Recovery Guarantees,[0],[0]
"It is also straightforward to apply their results to bound mk+r ⇣ 1
32 162(k+r) logNn ⌘
, and M
1  1, which gives explicit bounds as per Theorem 5 for Algorithms 1, 2 for the considered function and the design matrix.",5. Recovery Guarantees,[0],[0]
"In this section, we empirically evaluate the proposed algorithms.",6. Experiments,[0],[0]
"First, we test empirically the performance of GECO (Algorithm 2 with ⌧ = 1) for a clustering task.",6.1. Clustering under Stochastic Block Model,[0],[0]
We are provided with a graph with nodes and the respective edges between the nodes.,6.1. Clustering under Stochastic Block Model,[0],[0]
The observed graph is assumed to have been noisily generated from a true underlying clustering.,6.1. Clustering under Stochastic Block Model,[0],[0]
The goal is to recover the underlying clustering structure from the noisy graph provided to us.,6.1. Clustering under Stochastic Block Model,[0],[0]
Our greedy framework is applicable because the adjacency matrix of the true clustering is low rank.,6.1. Clustering under Stochastic Block Model,[0],[0]
We compare performance of Algorithm 2 on simulated data against standard baselines of spectral clustering which are commonly used for this task.,6.1. Clustering under Stochastic Block Model,[0],[0]
"We begin by describing a generative model for creating edges between nodes given the ground truth.
",6.1. Clustering under Stochastic Block Model,[0],[0]
The Stochastic Block Model is a model to generate random graphs.,6.1. Clustering under Stochastic Block Model,[0],[0]
"It takes its input the set of n nodes, and a partition of [n] which form a set of disjoint clusters, and returns the graph with nodes and the generated edges.",6.1. Clustering under Stochastic Block Model,[0],[0]
"The model has two additional parameters, the generative probabilities (p, q).",6.1. Clustering under Stochastic Block Model,[0],[0]
"A pair of nodes within the same cluster have an edge between them with probability p, while a pair of nodes belonging to different clusters have an edge between them with probability q.",6.1. Clustering under Stochastic Block Model,[0],[0]
For simplicity we assume q = (1 p).,6.1. Clustering under Stochastic Block Model,[0],[0]
The model then iterates over each pair of nodes.,6.1. Clustering under Stochastic Block Model,[0],[0]
"For each such pair that belongs to same cluster, it samples an edge as Bernoulli(p), otherwise as Bernoulli(1 p).",6.1. Clustering under Stochastic Block Model,[0],[0]
"This provides us with a {0, 1} adjacency matrix.
",6.1. Clustering under Stochastic Block Model,[0],[0]
"We compare against two versions of spectral clustering, which is a standard technique applied to find communities in a graph.",6.1. Clustering under Stochastic Block Model,[0],[0]
"The method takes as input the n⇥ n adjacency matrix A, which is a {0, 1} matrix with an entry Aij = 1 if there is an edge between node i and j, and is 0 otherwise.",6.1. Clustering under Stochastic Block Model,[0],[0]
"From the adjacency matrix, the graph Laplacian L is constructed.",6.1. Clustering under Stochastic Block Model,[0],[0]
"The Laplacian may be unnormalized, in which case it is simply L = D A, where D is the diagonal matrix of degrees of nodes.",6.1. Clustering under Stochastic Block Model,[0],[0]
"A normalized Laplacian is
computed as Lnorm = D 1/2LD 1/2.",6.1. Clustering under Stochastic Block Model,[0],[0]
"After calculating the Laplacian, the algorithm solves for bottom k eigenvectors of the Laplacian, and then apply k-means clustering on the rows of the thus obtained eigenvector matrix.",6.1. Clustering under Stochastic Block Model,[0],[0]
"We refer to the works of Shi & Malik (2000); Ng et al. (2001) for the specific details of clustering algorithms using unnormalized and normalized graph Laplacian respectively.
",6.1. Clustering under Stochastic Block Model,[0],[0]
"We use our greedy algorithm to cluster the graph by optimizing a logistic PCA objective function, which is a special case of the exponential family PCA (Collins et al., 2001).",6.1. Clustering under Stochastic Block Model,[0],[0]
"For a given matrix X, each entry Xij is assumed to be independently drawn with likelihood proportional to exp h⇥ij ,Xiji G(⇥ij), where ⇥ is the true underlying parameter, and G(·) is the partition function corresponding to a generalized linear model (GLM).",6.1. Clustering under Stochastic Block Model,[0],[0]
"It is easy to see we can apply our framework of greedy selection by defining `(·) as the log-likelihood:
`(⇥) =",6.1. Clustering under Stochastic Block Model,[0],[0]
"h⇥,Xi X
i,j
logG(⇥ij),
where ⇥ is the true parameter matrix of p and q that generates a realization of A. Since the true ⇥ is low rank, we get the low rank constrained optimization problem:
max rank(⇥)k `(⇥),
where k is a hyperparameter suggesting the true number of clusters.",6.1. Clustering under Stochastic Block Model,[0],[0]
"Note that lack of knowledge of true value of k is not more restrictive than spectral clustering algorithms which typically also require the true value of k. Having cast the clustering problem in the same form as (4), we can apply our greedy selection algorithm as opposed to the more costly alternating minimizing algorithms suggested by Collins et al. (2001).
",6.1. Clustering under Stochastic Block Model,[0],[0]
We generate the data as follows.,6.1. Clustering under Stochastic Block Model,[0],[0]
"For n = 100 nodes, and fixed number of cluster k = 5, we vary the within cluster edge generation probability p from 0.55 to 0.95 in increments of 0.05, and use the Stochastic Block model to generate a noisy graph with each p.",6.1. Clustering under Stochastic Block Model,[0],[0]
"Note that smaller p implies that the sampled graph will be more noisy and likely to be more different than the underlying clustering.
",6.1. Clustering under Stochastic Block Model,[0],[0]
"We compare against the spectral clustering algorithm using unnormalized Laplacian of Shi & Malik (2000) which we label “Spectral unnorm{k}” for k = {3, 5, 10}, and the spectral clustering algorithm using normalized Laplacian of Ng et al. (2001) which we label “Spectral norm{k}” for k = {3, 5, 10}.",6.1. Clustering under Stochastic Block Model,[0],[0]
"We use Algorithm 2 which we label “Greedy{k}” for k = {3, 5, 10}.",6.1. Clustering under Stochastic Block Model,[0],[0]
"For each of these models, the referred k is the supplied hyperparameter.",6.1. Clustering under Stochastic Block Model,[0],[0]
We report the least squares error of the output from each model to the true underlying ⇥,6.1. Clustering under Stochastic Block Model,[0],[0]
"(generalization error), and to the instantiation used for training X (reconstruction error).
",6.1. Clustering under Stochastic Block Model,[0],[0]
Figure 1 shows that the greedy logistic PCA performs well in not only recreating the given noisy matrix (reconstruction) but also captures the true low rank structure better (generalization).,6.1. Clustering under Stochastic Block Model,[0],[0]
"Further, note that providing the true hyperparameter k is vital for spectral clustering algorithms, while on the other hand greedy is less sensitive to k.",6.1. Clustering under Stochastic Block Model,[0],[0]
This is very useful in practice as k is typically not known.,6.1. Clustering under Stochastic Block Model,[0],[0]
Spectral clustering algorithms typically select k by computing an SVD and rerunning k-means for different values of k.,6.1. Clustering under Stochastic Block Model,[0],[0]
"In addition to being more robust, our greedy algorithm does not need to be rerun for different values of k – it produces solutions incrementally.",6.1. Clustering under Stochastic Block Model,[0],[0]
"Algorithms for embedding text into a vector space yield representations that can be quite beneficial in many applications, e.g. features for sentiment analysis.",6.2. Word Embeddings,[0],[0]
Mikolov et al. (2013b) proposed a context-based embedding called skipgram or word2vec.,6.2. Word Embeddings,[0],[0]
"The context of a word can be defined as a set of words before, around, or after the respective word.",6.2. Word Embeddings,[0],[0]
Their model strives to find an embedding of each word so that the representation predicts the embedding of each context word around it.,6.2. Word Embeddings,[0],[0]
Levy & Goldberg (2014) subsequently showed that the word embedding model proposed by Mikolov et al. (2013b) can be reinterpreted as matrix factorization of the PMI matrix constructed as follows.,6.2. Word Embeddings,[0],[0]
A word c is in context of w if it lies within the respective window of w.,6.2. Word Embeddings,[0],[0]
"The PMI matrix is then calculated as
PMIw,c = log ✓ p(w, c)
p(w)p(c)
◆ .
",6.2. Word Embeddings,[0],[0]
"In practice the probabilities p(w, c), p(w), p(c) are replaced by their empirical counterparts.",6.2. Word Embeddings,[0],[0]
"Further, note that p(w, c) is 0 if words c and w do not coexist in the same context, which yields 1 for PMI.",6.2. Word Embeddings,[0],[0]
"Levy & Goldberg (2014) suggest using an alternative: PPMIw,c = max{PMIw,c, 0}.",6.2. Word Embeddings,[0],[0]
"They also suggest variations of PMI hyper parameterized by k which corresponds to the number of negative samples in the training of the original skip gram model.
",6.2. Word Embeddings,[0],[0]
"We employ the binomial PCA model on the normalized count matrix (instead of the PMI), in a manner similar to the clustering approach in Section 6.1.",6.2. Word Embeddings,[0],[0]
"The normalized count matrix is calculated simply as p(w,c)p(w) , without taking logarithms.",6.2. Word Embeddings,[0],[0]
"This gives us a probability matrix which has each entry between 0 and 1, and which can be factorized under the binomial model greedily as per Algorithm 2.
",6.2. Word Embeddings,[0],[0]
We empirically study the embeddings obtained by binomial factorization on two tasks – word similarity and analogies.,6.2. Word Embeddings,[0],[0]
"For word similarity, we use the W353 dataset (Finkelstein et al., 2001) and the MEN data (Bruni et al., 2012).",6.2. Word Embeddings,[0],[0]
Both these datasets contain words with human assigned similarity scores.,6.2. Word Embeddings,[0],[0]
"We evaluate the embeddings by their cosine similarity, and measuring the correlation with the available human ratings.",6.2. Word Embeddings,[0],[0]
The fraction of correctly answered queries are returned as the metric.,6.2. Word Embeddings,[0],[0]
"For the analogy task, we use the Microsoft Research (MSR) syntactic analogies (Mikolov et al., 2013c) and the Google mixed analogies dataset (Mikolov et al., 2013a).",6.2. Word Embeddings,[0],[0]
"For completing analogy a:b::c:x, the prediction is calculated as argmaxx cos(c,x) cos(b,x) cos(a,x) .",6.2. Word Embeddings,[0],[0]
"To compute accuracy, we use the multiplication similarity metric as used by Levy & Goldberg (2014).",6.2. Word Embeddings,[0],[0]
"To train the word embeddings, we use the 2013 news crawl dataset1.",6.2. Word Embeddings,[0],[0]
"We filter out stop words, non-ASCII characters, and words occurring less than
1http://www.statmt.org/wmt14/ training-monolingual-news-crawl
2000 times (which yields a vocabulary of 6713).",6.2. Word Embeddings,[0],[0]
"Note that since we keep only the most common words, several queries from the datasets are invalid because we do not have embeddings for words appearing in them.",6.2. Word Embeddings,[0],[0]
"However, we do include them by assigning invalid queries a value of 0 and reporting the overall average over the entire dataset.
",6.2. Word Embeddings,[0],[0]
Table 1 shows the empirical evaluation.,6.2. Word Embeddings,[0],[0]
"SVD and PPMI are the models proposed by Levy & Goldberg (2014), while SGNS is the skipgram with negative sampling model of Mikolov et al. (2013b).",6.2. Word Embeddings,[0],[0]
"We run each of these for k = {5, 10, 15, 20} and report the best results.",6.2. Word Embeddings,[0],[0]
"This shows that alternative factorizations such as our application of binomial PCA can be more consistent and competitive with other embedding methods.
",6.2. Word Embeddings,[0],[0]
Conclusion: We have connected the problem of greedy low rank matrix estimation to that of submodular optimization.,6.2. Word Embeddings,[0],[0]
Through that connection we have provided improved exponential rates of convergence for the algorithm.,6.2. Word Embeddings,[0],[0]
An interesting area of future study will be to connect these ideas to general atoms or dictionary elements.,6.2. Word Embeddings,[0],[0]
We thank the anonymous reviewers for their helpful feedback.,Acknowledgements,[0],[0]
"Research supported by William Hartwig Fellowship, NSF Grants CCF 1344179, 1344364, 1407278, 1422549, 1618689, IIS 1421729, and ARO YIP W911NF-14-1-0258.",Acknowledgements,[0],[0]
We provide new approximation guarantees for greedy low rank matrix estimation under standard assumptions of restricted strong convexity and smoothness.,abstractText,[0],[0]
"Our novel analysis also uncovers previously unknown connections between the low rank estimation and combinatorial optimization, so much so that our bounds are reminiscent of corresponding approximation bounds in submodular maximization.",abstractText,[0],[0]
"Additionally, we also provide statistical recovery guarantees.",abstractText,[0],[0]
"Finally, we present empirical comparison of greedy estimation with established baselines on two important real-world problems.",abstractText,[0],[0]
On Approximation Guarantees for Greedy Low Rank Optimization,title,[0],[0]
"Recent advances in deep learning have dramatically improved neural network accuracy (Simonyan & Zisserman, 2015; Srivastava et al., 2015; He et al., 2016; Huang et al., 2016; 2017).",1. Introduction,[0],[0]
"As a result, neural networks are now entrusted with making complex decisions in applications, such as object detection (Girshick, 2015), speech recognition (Hannun et al., 2014), and medical diagnosis (Caruana et al., 2015).",1. Introduction,[0],[0]
"In these settings, neural networks are an essential component of larger decision making pipelines.
",1. Introduction,[0],[0]
"In real-world decision making systems, classification networks must not only be accurate, but also should indicate when they are likely to be incorrect.",1. Introduction,[0],[0]
"As an example, consider a self-driving car that uses a neural network to detect pedestrians and other obstructions (Bojarski et al., 2016).
",1. Introduction,[0],[0]
"*Equal contribution, alphabetical order.",1. Introduction,[0],[0]
1Cornell University.,1. Introduction,[0],[0]
"Correspondence to: Chuan Guo <cg563@cornell.edu>, Geoff Pleiss <geoff@cs.cornell.edu>, Yu Sun <ys646@cornell.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
0.0 0.2 0.4 0.6 0.8 1.0 0.0
0.2
0.4
0.6
0.8
1.0
% of
Sa m
pl es
A vg
.c on
fid en ce",1. Introduction,[0],[0]
"A cc ur ac y
LeNet (1998) CIFAR-100
0.0 0.2 0.4 0.6 0.8 1.0
A vg
.c on
fid en
ce
A cc
ur ac
y
ResNet (2016) CIFAR-100
0.0 0.2 0.4 0.6 0.8 1.0 0.0
0.2
0.4
0.6
0.8 1.0 A cc ur ac y
Error=44.9
Outputs Gap
0.0 0.2 0.4 0.6 0.8 1.0
Error=30.6
Outputs Gap
Confidence
Figure 1.",1. Introduction,[0],[0]
Confidence histograms (top) and reliability diagrams (bottom) for a 5-layer LeNet (left) and a 110-layer ResNet (right) on CIFAR-100.,1. Introduction,[0],[0]
"Refer to the text below for detailed illustration.
",1. Introduction,[0],[0]
"If the detection network is not able to confidently predict the presence or absence of immediate obstructions, the car should rely more on the output of other sensors for braking.",1. Introduction,[0],[0]
"Alternatively, in automated health care, control should be passed on to human doctors when the confidence of a disease diagnosis network is low (Jiang et al., 2012).",1. Introduction,[0],[0]
"Specifically, a network should provide a calibrated confidence measure in addition to its prediction.",1. Introduction,[0],[0]
"In other words, the probability associated with the predicted class label should reflect its ground truth correctness likelihood.
",1. Introduction,[0],[0]
Calibrated confidence estimates are also important for model interpretability.,1. Introduction,[0],[0]
"Humans have a natural cognitive intuition for probabilities (Cosmides & Tooby, 1996).",1. Introduction,[0],[0]
"Good confidence estimates provide a valuable extra bit of information to establish trustworthiness with the user – especially for neural networks, whose classification decisions are often difficult to interpret.",1. Introduction,[0],[0]
"Further, good probability estimates can be used to incorporate neural networks into other probabilistic models.",1. Introduction,[0],[0]
"For example, one can improve performance by combining network outputs with a lan-
guage model in speech recognition (Hannun et al., 2014; Xiong et al., 2016), or with camera information for object detection (Kendall & Cipolla, 2016).
",1. Introduction,[0],[0]
"In 2005, Niculescu-Mizil & Caruana (2005) showed that neural networks typically produce well-calibrated probabilities on binary classification tasks.",1. Introduction,[0],[0]
"While neural networks today are undoubtedly more accurate than they were a decade ago, we discover with great surprise that modern neural networks are no longer well-calibrated.",1. Introduction,[0],[0]
"This is visualized in Figure 1, which compares a 5-layer LeNet (left) (LeCun et al., 1998) with a 110-layer ResNet (right) (He et al., 2016) on the CIFAR-100 dataset.",1. Introduction,[0],[0]
The top row shows the distribution of prediction confidence (i.e. probabilities associated with the predicted label) as histograms.,1. Introduction,[0],[0]
"The average confidence of LeNet closely matches its accuracy, while the average confidence of the ResNet is substantially higher than its accuracy.",1. Introduction,[0],[0]
"This is further illustrated in the bottom row reliability diagrams (DeGroot & Fienberg, 1983; Niculescu-Mizil & Caruana, 2005), which show accuracy as a function of confidence.",1. Introduction,[0],[0]
"We see that LeNet is well-calibrated, as confidence closely approximates the expected accuracy (i.e. the bars align roughly along the diagonal).",1. Introduction,[0],[0]
"On the other hand, the ResNet’s accuracy is better, but does not match its confidence.
",1. Introduction,[0],[0]
"Our goal is not only to understand why neural networks have become miscalibrated, but also to identify what methods can alleviate this problem.",1. Introduction,[0],[0]
"In this paper, we demonstrate on several computer vision and NLP tasks that neural networks produce confidences that cannot represent true probabilities.",1. Introduction,[0],[0]
"Additionally, we offer insight and intuition into network training and architectural trends that may cause miscalibration.",1. Introduction,[0],[0]
"Finally, we compare various postprocessing calibration methods on state-of-the-art neural networks, and introduce several extensions of our own.",1. Introduction,[0],[0]
"Surprisingly, we find that a single-parameter variant of Platt scaling (Platt et al., 1999) – which we refer to as temperature scaling – is often the most effective method at obtaining calibrated probabilities.",1. Introduction,[0],[0]
"Because this method is straightforward to implement with existing deep learning frameworks, it can be easily adopted in practical settings.",1. Introduction,[0],[0]
The problem we address in this paper is supervised multiclass classification with neural networks.,2. Definitions,[0],[0]
The inputX ∈ X and label Y ∈,2. Definitions,[0],[0]
"Y = {1, . . .",2. Definitions,[0],[0]
",K} are random variables that follow a ground truth joint distribution π(X,Y ) = π(Y |X)π(X).",2. Definitions,[0],[0]
"Let h be a neural network with h(X) = (Ŷ , P̂ ), where Ŷ is a class prediction and P̂ is its associated confidence, i.e. probability of correctness.",2. Definitions,[0],[0]
"We would like the confidence estimate P̂ to be calibrated, which intuitively means that P̂ represents a true probability.",2. Definitions,[0],[0]
"For example, given 100 predictions, each with confidence of
0.8, we expect that 80 should be correctly classified.",2. Definitions,[0],[0]
"More formally, we define perfect calibration as
P ( Ŷ = Y | P̂ = p ) = p, ∀p ∈",2. Definitions,[0],[0]
"[0, 1] (1)
where the probability is over the joint distribution.",2. Definitions,[0],[0]
"In all practical settings, achieving perfect calibration is impossible.",2. Definitions,[0],[0]
"Additionally, the probability in (1) cannot be computed using finitely many samples since P̂ is a continuous random variable.",2. Definitions,[0],[0]
"This motivates the need for empirical approximations that capture the essence of (1).
",2. Definitions,[0],[0]
"Reliability Diagrams (e.g. Figure 1 bottom) are a visual representation of model calibration (DeGroot & Fienberg, 1983; Niculescu-Mizil & Caruana, 2005).",2. Definitions,[0],[0]
These diagrams plot expected sample accuracy as a function of confidence.,2. Definitions,[0],[0]
If the model is perfectly calibrated – i.e. if (1) holds – then the diagram should plot the identity function.,2. Definitions,[0],[0]
"Any deviation from a perfect diagonal represents miscalibration.
",2. Definitions,[0],[0]
"To estimate the expected accuracy from finite samples, we group predictions into M interval bins (each of size 1/M ) and calculate the accuracy of each bin.",2. Definitions,[0],[0]
Let Bm be the set of indices of samples whose prediction confidence falls into the interval,2. Definitions,[0],[0]
"Im = (m−1M , m M ].",2. Definitions,[0],[0]
"The accuracy of Bm is
acc(Bm) = 1 |Bm| ∑
i∈Bm 1(ŷi = yi),
where ŷi and yi are the predicted and true class labels for sample i. Basic probability tells us that acc(Bm) is an unbiased and consistent estimator of P(Ŷ = Y | P̂ ∈",2. Definitions,[0],[0]
Im).,2. Definitions,[0],[0]
"We define the average confidence within bin Bm as
conf(Bm) = 1 |Bm| ∑
i∈Bm p̂i,
where p̂i is the confidence for sample i. acc(Bm) and conf(Bm) approximate the left-hand and right-hand sides of (1) respectively for bin Bm.",2. Definitions,[0],[0]
"Therefore, a perfectly calibrated model will have acc(Bm) = conf(Bm) for all m ∈ {1, . . .",2. Definitions,[0],[0]
",M}.",2. Definitions,[0],[0]
"Note that reliability diagrams do not display the proportion of samples in a given bin, and thus cannot be used to estimate how many samples are calibrated.
",2. Definitions,[0],[0]
Expected Calibration Error (ECE).,2. Definitions,[0],[0]
"While reliability diagrams are useful visual tools, it is more convenient to have a scalar summary statistic of calibration.",2. Definitions,[0],[0]
"Since statistics comparing two distributions cannot be comprehensive, previous works have proposed variants, each with a unique emphasis.",2. Definitions,[0],[0]
"One notion of miscalibration is the difference in expectation between confidence and accuracy, i.e.
Ê P
[∣∣∣P ( Ŷ = Y | P̂ = p )",2. Definitions,[0],[0]
"− p ∣∣∣ ]
",2. Definitions,[0],[0]
"(2)
Expected Calibration Error (Naeini et al., 2015) – or ECE – approximates (2) by partitioning predictions into M equally-spaced bins (similar to the reliability diagrams) and
taking a weighted average of the bins’ accuracy/confidence difference.",2. Definitions,[0],[0]
"More precisely,
ECE = M∑
m=1
|Bm| n ∣∣∣∣ acc(Bm)− conf(Bm) ∣∣∣∣, (3)
where n is the number of samples.",2. Definitions,[0],[0]
The difference between acc and conf for a given bin represents the calibration gap (red bars in reliability diagrams – e.g. Figure 1).,2. Definitions,[0],[0]
We use ECE as the primary empirical metric to measure calibration.,2. Definitions,[0],[0]
"See Section S1 for more analysis of this metric.
",2. Definitions,[0],[0]
Maximum Calibration Error (MCE).,2. Definitions,[0],[0]
"In high-risk applications where reliable confidence measures are absolutely necessary, we may wish to minimize the worst-case deviation between confidence and accuracy:
max p∈[0,1]
∣∣∣P ( Ŷ = Y | P̂ = p )",2. Definitions,[0],[0]
− p ∣∣∣ .,2. Definitions,[0],[0]
"(4)
The Maximum Calibration Error (Naeini et al., 2015) – or MCE – estimates an upper bound of this deviation.",2. Definitions,[0],[0]
"Similarly to ECE, this approximation involves binning:
MCE = max m∈{1,...,M} |acc(Bm)− conf(Bm)| .",2. Definitions,[0],[0]
"(5)
In reliability diagrams, MCE measures the largest calibration gap (red bars) across all bins, whereas ECE measures a weighted average of all gaps.",2. Definitions,[0],[0]
"For perfectly calibrated classifiers, MCE and ECE both equal 0.
",2. Definitions,[0],[0]
"Negative log likelihood is a standard measure of a probabilistic model’s quality (Friedman et al., 2001).",2. Definitions,[0],[0]
"It is also referred to as the cross entropy loss in the context of deep learning (Bengio et al., 2015).",2. Definitions,[0],[0]
"Given a probabilistic model π̂(Y |X) and n samples, NLL is defined as:
L = − n∑
i=1
log(π̂(yi|xi))",2. Definitions,[0],[0]
"(6)
It is a standard result (Friedman et al., 2001) that, in expectation, NLL is minimized if and only if π̂(Y |X) recovers the ground truth conditional distribution π(Y |X).",2. Definitions,[0],[0]
The architecture and training procedures of neural networks have rapidly evolved in recent years.,3. Observing Miscalibration,[0],[0]
In this section we identify some recent changes that are responsible for the miscalibration phenomenon observed in Figure 1.,3. Observing Miscalibration,[0],[0]
"Though we cannot claim causality, we find that model capacity and lack of regularization are closely related to model (mis)calibration.
Model capacity.",3. Observing Miscalibration,[0],[0]
The model capacity of neural networks has increased at a dramatic pace over the past few years.,3. Observing Miscalibration,[0],[0]
"It is now common to see networks with hundreds, if not thousands of layers (He et al., 2016; Huang et al., 2016) and hundreds of convolutional filters per layer (Zagoruyko & Komodakis, 2016).",3. Observing Miscalibration,[0],[0]
"Recent work shows that very deep or wide models are able to generalize better than smaller ones, while exhibiting the capacity to easily fit the training set (Zhang et al., 2017).
",3. Observing Miscalibration,[0],[0]
"Although increasing depth and width may reduce classification error, we observe that these increases negatively affect model calibration.",3. Observing Miscalibration,[0],[0]
Figure 2 displays error and ECE as a function of depth and width on a ResNet trained on CIFAR-100.,3. Observing Miscalibration,[0],[0]
"The far left figure varies depth for a network with 64 convolutional filters per layer, while the middle left figure fixes the depth at 14 layers and varies the number of convolutional filters per layer.",3. Observing Miscalibration,[0],[0]
"Though even the smallest models in the graph exhibit some degree of miscalibration, the ECE metric grows substantially with model capacity.",3. Observing Miscalibration,[0],[0]
"During training, after the model is able to correctly classify (almost) all training samples, NLL can be further minimized by increasing the confidence of predictions.",3. Observing Miscalibration,[0],[0]
"Increased model capacity will lower training NLL, and thus the model will be more (over)confident on average.
",3. Observing Miscalibration,[0],[0]
"Batch Normalization (Ioffe & Szegedy, 2015) improves the optimization of neural networks by minimizing distribution shifts in activations within the neural network’s hid-
den layers.",3. Observing Miscalibration,[0],[0]
"Recent research suggests that these normalization techniques have enabled the development of very deep architectures, such as ResNets (He et al., 2016) and DenseNets (Huang et al., 2017).",3. Observing Miscalibration,[0],[0]
"It has been shown that Batch Normalization improves training time, reduces the need for additional regularization, and can in some cases improve the accuracy of networks.
",3. Observing Miscalibration,[0],[0]
"While it is difficult to pinpoint exactly how Batch Normalization affects the final predictions of a model, we do observe that models trained with Batch Normalization tend to be more miscalibrated.",3. Observing Miscalibration,[0],[0]
"In the middle right plot of Figure 2, we see that a 6-layer ConvNet obtains worse calibration when Batch Normalization is applied, even though classification accuracy improves slightly.",3. Observing Miscalibration,[0],[0]
"We find that this result holds regardless of the hyperparameters used on the Batch Normalization model (i.e. low or high learning rate, etc.).
",3. Observing Miscalibration,[0],[0]
"Weight decay, which used to be the predominant regularization mechanism for neural networks, is decreasingly utilized when training modern neural networks.",3. Observing Miscalibration,[0],[0]
"Learning theory suggests that regularization is necessary to prevent overfitting, especially as model capacity increases (Vapnik, 1998).",3. Observing Miscalibration,[0],[0]
"However, due to the apparent regularization effects of Batch Normalization, recent research seems to suggest that models with less L2 regularization tend to generalize better (Ioffe & Szegedy, 2015).",3. Observing Miscalibration,[0],[0]
"As a result, it is now common to train models with little weight decay, if any at all.",3. Observing Miscalibration,[0],[0]
"The top performing ImageNet models of 2015 all use an order of magnitude less weight decay than models of previous years (He et al., 2016; Simonyan & Zisserman, 2015).
",3. Observing Miscalibration,[0],[0]
We find that training with less weight decay has a negative impact on calibration.,3. Observing Miscalibration,[0],[0]
"The far right plot in Figure 2 dis-
plays training error and ECE for a 110-layer ResNet with varying amounts of weight decay.",3. Observing Miscalibration,[0],[0]
The only other forms of regularization are data augmentation and Batch Normalization.,3. Observing Miscalibration,[0],[0]
We observe that calibration and accuracy are not optimized by the same parameter setting.,3. Observing Miscalibration,[0],[0]
"While the model exhibits both over-regularization and under-regularization with respect to classification error, it does not appear that calibration is negatively impacted by having too much weight decay.",3. Observing Miscalibration,[0],[0]
"Model calibration continues to improve when more regularization is added, well after the point of achieving optimal accuracy.",3. Observing Miscalibration,[0],[0]
"The slight uptick at the end of the graph may be an artifact of using a weight decay factor that impedes optimization.
",3. Observing Miscalibration,[0],[0]
NLL can be used to indirectly measure model calibration.,3. Observing Miscalibration,[0],[0]
"In practice, we observe a disconnect between NLL and accuracy, which may explain the miscalibration in Figure 2.",3. Observing Miscalibration,[0],[0]
This disconnect occurs because neural networks can overfit to NLL without overfitting to the 0/1 loss.,3. Observing Miscalibration,[0],[0]
We observe this trend in the training curves of some miscalibrated models.,3. Observing Miscalibration,[0],[0]
Figure 3 shows test error and NLL (rescaled to match error) on CIFAR-100 as training progresses.,3. Observing Miscalibration,[0],[0]
"Both error and NLL immediately drop at epoch 250, when the learning rate is dropped; however, NLL overfits during the remainder of training.",3. Observing Miscalibration,[0],[0]
"Surprisingly, overfitting to NLL is beneficial to classification accuracy.",3. Observing Miscalibration,[0],[0]
"On CIFAR-100, test error drops from 29% to 27% in the region where NLL overfits.",3. Observing Miscalibration,[0],[0]
"This phenomenon renders a concrete explanation of miscalibration: the network learns better classification accuracy at the expense of well-modeled probabilities.
",3. Observing Miscalibration,[0],[0]
We can connect this finding to recent work examining the generalization of large neural networks.,3. Observing Miscalibration,[0],[0]
Zhang et al. (2017) observe that deep neural networks seemingly violate the common understanding of learning theory that large models with little regularization will not generalize well.,3. Observing Miscalibration,[0],[0]
"The observed disconnect between NLL and 0/1 loss suggests that these high capacity models are not necessarily immune from overfitting, but rather, overfitting manifests in probabilistic error rather than classification error.",3. Observing Miscalibration,[0],[0]
"In this section, we first review existing calibration methods, and introduce new variants of our own.",4. Calibration Methods,[0],[0]
All methods are post-processing steps that produce (calibrated) probabilities.,4. Calibration Methods,[0],[0]
"Each method requires a hold-out validation set, which in practice can be the same set used for hyperparameter tuning.",4. Calibration Methods,[0],[0]
"We assume that the training, validation, and test sets are drawn from the same distribution.",4. Calibration Methods,[0],[0]
"We first introduce calibration in the binary setting, i.e. Y = {0, 1}.",4.1. Calibrating Binary Models,[0],[0]
"For simplicity, throughout this subsection,
we assume the model outputs only the confidence for the positive class.1",4.1. Calibrating Binary Models,[0],[0]
"Given a sample xi, we have access to p̂i – the network’s predicted probability of yi = 1, as well as zi ∈ R – which is the network’s non-probabilistic output, or logit.",4.1. Calibrating Binary Models,[0],[0]
The predicted probability p̂i is derived from zi using a sigmoid function σ; i.e. p̂i = σ(zi).,4.1. Calibrating Binary Models,[0],[0]
"Our goal is to produce a calibrated probability q̂i based on yi, p̂i, and zi.
Histogram binning (Zadrozny & Elkan, 2001) is a simple non-parametric calibration method.",4.1. Calibrating Binary Models,[0],[0]
"In a nutshell, all uncalibrated predictions p̂i are divided into mutually exclusive bins B1, . . .",4.1. Calibrating Binary Models,[0],[0]
", BM .",4.1. Calibrating Binary Models,[0],[0]
"Each bin is assigned a calibrated score θm; i.e. if p̂i is assigned to bin Bm, then q̂i = θm.",4.1. Calibrating Binary Models,[0],[0]
"At test time, if prediction p̂te falls into bin Bm, then the calibrated prediction q̂te is θm.",4.1. Calibrating Binary Models,[0],[0]
"More precisely, for a suitably chosen M (usually small), we first define bin boundaries 0 =",4.1. Calibrating Binary Models,[0],[0]
a1 ≤ a2 ≤ . . .,4.1. Calibrating Binary Models,[0],[0]
≤,4.1. Calibrating Binary Models,[0],[0]
aM+1,4.1. Calibrating Binary Models,[0],[0]
"= 1, where the bin Bm is defined by the interval (am, am+1].",4.1. Calibrating Binary Models,[0],[0]
Typically the bin boundaries are either chosen to be equal length intervals or to equalize the number of samples in each bin.,4.1. Calibrating Binary Models,[0],[0]
"The predictions θi are chosen to minimize the bin-wise squared loss:
min θ1,...,θM
M∑
m=1
n∑
i=1
1(am ≤",4.1. Calibrating Binary Models,[0],[0]
"p̂i < am+1) (θm − yi)2 , (7)
where 1 is the indicator function.",4.1. Calibrating Binary Models,[0],[0]
"Given fixed bins boundaries, the solution to (7) results in θm that correspond to the average number of positive-class samples in bin Bm.
",4.1. Calibrating Binary Models,[0],[0]
"Isotonic regression (Zadrozny & Elkan, 2002), arguably the most common non-parametric calibration method, learns a piecewise constant function f to transform uncalibrated outputs; i.e. q̂i = f(p̂i).",4.1. Calibrating Binary Models,[0],[0]
"Specifically, isotonic regression produces f to minimize the square loss∑n i=1(f(p̂i)",4.1. Calibrating Binary Models,[0],[0]
− yi)2.,4.1. Calibrating Binary Models,[0],[0]
"Because f is constrained to be piecewise constant, we can write the optimization problem as:
min M
θ1,...,θM a1,...,aM+1
M∑
m=1
n∑
i=1
1(am ≤ p̂i < am+1) (θm − yi)2
subject to 0 = a1 ≤ a2 ≤ . . .",4.1. Calibrating Binary Models,[0],[0]
≤,4.1. Calibrating Binary Models,[0],[0]
aM+1,4.1. Calibrating Binary Models,[0],[0]
"= 1, θ1 ≤ θ2 ≤ . . .",4.1. Calibrating Binary Models,[0],[0]
"≤ θM .
",4.1. Calibrating Binary Models,[0],[0]
"where M is the number of intervals; a1, . . .",4.1. Calibrating Binary Models,[0],[0]
", aM+1 are the interval boundaries; and θ1, . . .",4.1. Calibrating Binary Models,[0],[0]
", θM are the function values.",4.1. Calibrating Binary Models,[0],[0]
"Under this parameterization, isotonic regression is a strict generalization of histogram binning in which the bin boundaries and bin predictions are jointly optimized.
",4.1. Calibrating Binary Models,[0],[0]
"Bayesian Binning into Quantiles (BBQ) (Naeini et al., 2015) is a extension of histogram binning using Bayesian
1 This is in contrast with the setting in Section 2, in which the model produces both a class prediction and confidence.
model averaging.",4.1. Calibrating Binary Models,[0],[0]
"Essentially, BBQ marginalizes out all possible binning schemes to produce q̂i.",4.1. Calibrating Binary Models,[0],[0]
"More formally, a binning scheme s is a pair (M, I) where M is the number of bins, and I is a corresponding partitioning of [0, 1] into disjoint intervals (0 = a1 ≤ a2 ≤ . . .",4.1. Calibrating Binary Models,[0],[0]
≤,4.1. Calibrating Binary Models,[0],[0]
aM+1 = 1).,4.1. Calibrating Binary Models,[0],[0]
"The parameters of a binning scheme are θ1, . . .",4.1. Calibrating Binary Models,[0],[0]
", θM .",4.1. Calibrating Binary Models,[0],[0]
"Under this framework, histogram binning and isotonic regression both produce a single binning scheme, whereas BBQ considers a space S of all possible binning schemes for the validation dataset D. BBQ performs Bayesian averaging of the probabilities produced by each scheme:2
P(q̂te",4.1. Calibrating Binary Models,[0],[0]
"| p̂te, D) = ∑
s∈S P(q̂te, S = s | p̂te, D)
= ∑
s∈S P(q̂te",4.1. Calibrating Binary Models,[0],[0]
"| p̂te, S=s,D)P(S=s | D).
",4.1. Calibrating Binary Models,[0],[0]
"where P(q̂te | p̂te, S = s,D) is the calibrated probability using binning scheme s. Using a uniform prior, the weight P(S=s | D) can be derived using Bayes’ rule:
P(S=s | D) =",4.1. Calibrating Binary Models,[0],[0]
P(D,4.1. Calibrating Binary Models,[0],[0]
"| S=s)∑ s′∈S P(D | S=s′) .
",4.1. Calibrating Binary Models,[0],[0]
"The parameters θ1, . . .",4.1. Calibrating Binary Models,[0],[0]
", θM can be viewed as parameters of M independent binomial distributions.",4.1. Calibrating Binary Models,[0],[0]
"Hence, by placing a Beta prior on θ1, . . .",4.1. Calibrating Binary Models,[0],[0]
", θM , we can obtain a closed form expression for the marginal likelihood",4.1. Calibrating Binary Models,[0],[0]
P(D | S= s).,4.1. Calibrating Binary Models,[0],[0]
This allows us to compute P(q̂te,4.1. Calibrating Binary Models,[0],[0]
"| p̂te, D) for any test input.
",4.1. Calibrating Binary Models,[0],[0]
"Platt scaling (Platt et al., 1999) is a parametric approach to calibration, unlike the other approaches.",4.1. Calibrating Binary Models,[0],[0]
"The nonprobabilistic predictions of a classifier are used as features for a logistic regression model, which is trained on the validation set to return probabilities.",4.1. Calibrating Binary Models,[0],[0]
"More specifically, in the context of neural networks (Niculescu-Mizil & Caruana, 2005), Platt scaling learns scalar parameters a, b ∈ R and outputs q̂i = σ(azi + b) as the calibrated probability.",4.1. Calibrating Binary Models,[0],[0]
Parameters a and b can be optimized using the NLL loss over the validation set.,4.1. Calibrating Binary Models,[0],[0]
It is important to note that the neural network’s parameters are fixed during this stage.,4.1. Calibrating Binary Models,[0],[0]
"For classification problems involving K > 2 classes, we return to the original problem formulation.",4.2. Extension to Multiclass Models,[0],[0]
The network outputs a class prediction ŷi and confidence score p̂i for each input xi.,4.2. Extension to Multiclass Models,[0],[0]
"In this case, the network logits zi are vectors, where ŷi = argmaxk z",4.2. Extension to Multiclass Models,[0],[0]
"(k) i , and p̂i is typically derived using the softmax function σSM:
σSM(zi) (k) =",4.2. Extension to Multiclass Models,[0],[0]
exp(z (k) i ),4.2. Extension to Multiclass Models,[0],[0]
"∑K
j=1 exp(z (j) i )
, p̂i = max k
σSM(zi) (k).
",4.2. Extension to Multiclass Models,[0],[0]
"The goal is to produce a calibrated confidence q̂i and (possibly new) class prediction ŷ′i based on yi, ŷi, p̂i, and zi.
2 Because the validation dataset is finite, S is as well.
",4.2. Extension to Multiclass Models,[0],[0]
Extension of binning methods.,4.2. Extension to Multiclass Models,[0],[0]
"One common way of extending binary calibration methods to the multiclass setting is by treating the problem as K one-versus-all problems (Zadrozny & Elkan, 2002).",4.2. Extension to Multiclass Models,[0],[0]
"For k = 1, . . .",4.2. Extension to Multiclass Models,[0],[0]
",K, we form a binary calibration problem where the label is 1(yi = k) and the predicted probability is σSM(zi)(k).",4.2. Extension to Multiclass Models,[0],[0]
"This gives us K calibration models, each for a particular class.",4.2. Extension to Multiclass Models,[0],[0]
"At test time, we obtain an unnormalized probability vector [q̂
(1) i , . . .",4.2. Extension to Multiclass Models,[0],[0]
", q̂ (K) i ], where q̂ (k) i is the calibrated probability for class k.",4.2. Extension to Multiclass Models,[0],[0]
"The new class prediction ŷ′i is the argmax of the vector, and the new confidence q̂′i is the max of the vector normalized by ∑K k=1 q̂ (k) i .",4.2. Extension to Multiclass Models,[0],[0]
"This extension can be applied to histogram binning, isotonic regression, and BBQ.
",4.2. Extension to Multiclass Models,[0],[0]
Matrix and vector scaling are two multi-class extensions of Platt scaling.,4.2. Extension to Multiclass Models,[0],[0]
Let zi be the logits vector produced before the softmax layer for input xi.,4.2. Extension to Multiclass Models,[0],[0]
"Matrix scaling applies a linear transformation Wzi + b to the logits:
q̂i = max k
σSM(Wzi + b) (k),
ŷ′i = argmax k",4.2. Extension to Multiclass Models,[0],[0]
"(Wzi + b) (k).
",4.2. Extension to Multiclass Models,[0],[0]
"(8)
The parameters W and b are optimized with respect to NLL on the validation set.",4.2. Extension to Multiclass Models,[0],[0]
"As the number of parameters for matrix scaling grows quadratically with the number of classes K, we define vector scaling as a variant where W is restricted to be a diagonal matrix.
",4.2. Extension to Multiclass Models,[0],[0]
"Temperature scaling, the simplest extension of Platt scaling, uses a single scalar parameter T > 0",4.2. Extension to Multiclass Models,[0],[0]
for all classes.,4.2. Extension to Multiclass Models,[0],[0]
"Given the logit vector zi, the new confidence prediction is
q̂i = max k
σSM(zi/T ) (k).",4.2. Extension to Multiclass Models,[0],[0]
"(9)
T is called the temperature, and it “softens” the softmax (i.e. raises the output entropy) with T > 1.",4.2. Extension to Multiclass Models,[0],[0]
"As T → ∞, the probability q̂i approaches 1/K, which represents maximum uncertainty.",4.2. Extension to Multiclass Models,[0],[0]
"With T = 1, we recover the original probability p̂i.",4.2. Extension to Multiclass Models,[0],[0]
"As T → 0, the probability collapses to a point mass (i.e. q̂i = 1).",4.2. Extension to Multiclass Models,[0],[0]
T is optimized with respect to NLL on the validation set.,4.2. Extension to Multiclass Models,[0],[0]
"Because the parameter T does not change the maximum of the softmax function, the class prediction ŷ′i remains unchanged.",4.2. Extension to Multiclass Models,[0],[0]
"In other words, temperature scaling does not affect the model’s accuracy.
",4.2. Extension to Multiclass Models,[0],[0]
"Temperature scaling is commonly used in settings such as knowledge distillation (Hinton et al., 2015) and statistical mechanics (Jaynes, 1957).",4.2. Extension to Multiclass Models,[0],[0]
"To the best of our knowledge, we are not aware of any prior use in the context of calibrating",4.2. Extension to Multiclass Models,[0],[0]
probabilistic models.3 The model is equivalent to maximizing the entropy of the output probability distribution subject to certain constraints on the logits (see Section S2).,4.2. Extension to Multiclass Models,[0],[0]
Calibration and confidence scores have been studied in various contexts in recent years.,4.3. Other Related Works,[0],[0]
"Kuleshov & Ermon (2016) study the problem of calibration in the online setting, where the inputs can come from a potentially adversarial source.",4.3. Other Related Works,[0],[0]
Kuleshov & Liang (2015) investigate how to produce calibrated probabilities when the output space is a structured object.,4.3. Other Related Works,[0],[0]
Lakshminarayanan et al. (2016) use ensembles of networks to obtain uncertainty estimates.,4.3. Other Related Works,[0],[0]
Pereyra et al. (2017) penalize overconfident predictions as a form of regularization.,4.3. Other Related Works,[0],[0]
"Hendrycks & Gimpel (2017) use confidence
3To highlight the connection with prior works we define temperature scaling in terms of 1
T instead of a multiplicative scalar.
scores to determine if samples are out-of-distribution.
",4.3. Other Related Works,[0],[0]
"Bayesian neural networks (Denker & Lecun, 1990; MacKay, 1992) return a probability distribution over outputs as an alternative way to represent model uncertainty.",4.3. Other Related Works,[0],[0]
"Gal & Ghahramani (2016) draw a connection between Dropout (Srivastava et al., 2014) and model uncertainty, claiming that sampling models with dropped nodes is a way to estimate the probability distribution over all possible models for a given sample.",4.3. Other Related Works,[0],[0]
Kendall & Gal (2017) combine this approach with a model that outputs a predictive mean and variance for each data point.,4.3. Other Related Works,[0],[0]
This notion of uncertainty is not restricted to classification problems.,4.3. Other Related Works,[0],[0]
"In contrast, our framework, which does not require model sampling, returns a confidence for a given output rather than returning a distribution of possible outputs.",4.3. Other Related Works,[0],[0]
We apply the calibration methods in Section 4 to image classification and document classification neural networks.,5. Results,[0],[0]
"For image classification we use 6 datasets:
1.",5. Results,[0],[0]
"Caltech-UCSD Birds (Welinder et al., 2010): 200 bird species.",5. Results,[0],[0]
5994/2897/2897 images for train/validation/test sets.,5. Results,[0],[0]
2.,5. Results,[0],[0]
"Stanford Cars (Krause et al., 2013): 196 classes of cars by make, model, and year.",5. Results,[0],[0]
8041/4020/4020 images for train/validation/test.,5. Results,[0],[0]
"3. ImageNet 2012 (Deng et al., 2009): Natural scene images from 1000 classes.",5. Results,[0],[0]
"1.3 million/25,000/25,000 images for train/validation/test.",5. Results,[0],[0]
"4. CIFAR-10/CIFAR-100 (Krizhevsky & Hinton, 2009):",5. Results,[0],[0]
Color images (32 × 32) from 10/100 classes.,5. Results,[0],[0]
"45,000/5,000/10,000 images for train/validation/test.",5. Results,[0],[0]
5.,5. Results,[0],[0]
"Street View House Numbers (SVHN) (Netzer et al., 2011): 32 × 32 colored images of cropped out house numbers from Google Street View.",5. Results,[0],[0]
"604,388/6,000/26,032 images for train/validation/test.
",5. Results,[0],[0]
We train state-of-the-art convolutional networks:,5. Results,[0],[0]
"ResNets (He et al., 2016), ResNets with stochastic depth (SD) (Huang et al., 2016), Wide ResNets (Zagoruyko & Komodakis, 2016), and DenseNets (Huang et al., 2017).",5. Results,[0],[0]
"We use the data preprocessing, training procedures, and hyperparameters as described in each paper.",5. Results,[0],[0]
"For Birds and Cars, we fine-tune networks pretrained on ImageNet.
",5. Results,[0],[0]
"For document classification we experiment with 4 datasets:
1.",5. Results,[0],[0]
"20 News: News articles, partitioned into 20 categories by content.",5. Results,[0],[0]
9034/2259/7528 documents for train/validation/test.,5. Results,[0],[0]
2.,5. Results,[0],[0]
"Reuters: News articles, partitioned into 8 categories by topic.",5. Results,[0],[0]
"4388/1097/2189 documents for train/validation/test.
3.",5. Results,[0],[0]
"Stanford Sentiment Treebank (SST) (Socher et al., 2013): Movie reviews, represented as sentence parse trees that are annotated by sentiment.",5. Results,[0],[0]
Each sample includes a coarse binary label and a fine grained 5-class label.,5. Results,[0],[0]
"As described in (Tai et al., 2015), the training/validation/test sets contain 6920/872/1821 documents for binary, and 544/1101/2210 for fine-grained.
",5. Results,[0],[0]
"On 20 News and Reuters, we train Deep Averaging Networks (DANs)",5. Results,[0],[0]
"(Iyyer et al., 2015) with 3 feed-forward layers and Batch Normalization.",5. Results,[0],[0]
These networks obtain competitive accuracy using the optimization hyperparameters suggested by the original paper.,5. Results,[0],[0]
"On SST, we train TreeLSTMs",5. Results,[0],[0]
"(Long Short Term Memory) (Tai et al., 2015) using the default settings in the authors’ code.
",5. Results,[0],[0]
Calibration Results.,5. Results,[0],[0]
"Table 1 displays model calibration, as measured by ECE (with M = 15 bins), before and after applying the various methods (see Section S3 for MCE, NLL, and error tables).",5. Results,[0],[0]
"It is worth noting that most datasets and models experience some degree of miscalibration, with ECE typically between 4 to 10%.",5. Results,[0],[0]
"This is not architecture specific: we observe miscalibration on convolutional networks (with and without skip connections), recurrent networks, and deep averaging networks.",5. Results,[0],[0]
"The two notable exceptions are SVHN and Reuters, both of which experience ECE values below 1%.",5. Results,[0],[0]
"Both of these datasets have very low error (1.98% and 2.97%, respectively); and therefore the ratio of ECE to error is comparable to other datasets.
",5. Results,[0],[0]
Our most important discovery is the surprising effectiveness of temperature scaling despite its remarkable simplicity.,5. Results,[0],[0]
"Temperature scaling outperforms all other methods on the vision tasks, and performs comparably to other methods on the NLP datasets.",5. Results,[0],[0]
"What is perhaps even more surprising is that temperature scaling outperforms the vector and matrix Platt scaling variants, which are strictly more general methods.",5. Results,[0],[0]
"In fact, vector scaling recovers essentially the same solution as temperature scaling – the learned vector has nearly constant entries, and therefore is no different than a scalar transformation.",5. Results,[0],[0]
"In other words, network miscalibration is intrinsically low dimensional.
",5. Results,[0],[0]
The only dataset that temperature scaling does not calibrate is the Reuters dataset.,5. Results,[0],[0]
"In this instance, only one of the above methods is able to improve calibration.",5. Results,[0],[0]
"Because this dataset is well-calibrated to begin with (ECE ≤ 1%), there is not much room for improvement with any method, and post-processing may not even be necessary to begin with.",5. Results,[0],[0]
"It is also possible that our measurements are affected by dataset split or by the particular binning scheme.
",5. Results,[0],[0]
"Matrix scaling performs poorly on datasets with hundreds of classes (i.e. Birds, Cars, and CIFAR-100), and fails to converge on the 1000-class ImageNet dataset.",5. Results,[0],[0]
"This is expected, since the number of parameters scales quadrat-
ically with the number of classes.",5. Results,[0],[0]
"Any calibration model with tens of thousands (or more) parameters will overfit to a small validation set, even when applying regularization.
",5. Results,[0],[0]
"Binning methods improve calibration on most datasets, but do not outperform temperature scaling.",5. Results,[0],[0]
"Additionally, binning methods tend to change class predictions which hurts accuracy (see Section S3).",5. Results,[0],[0]
"Histogram binning, the simplest binning method, typically outperforms isotonic regression and BBQ, despite the fact that both methods are strictly more general.",5. Results,[0],[0]
"This further supports our finding that calibration is best corrected by simple models.
",5. Results,[0],[0]
Reliability diagrams.,5. Results,[0],[0]
Figure 4 contains reliability diagrams for 110-layer ResNets on CIFAR-100 before and after calibration.,5. Results,[0],[0]
"From the far left diagram, we see that the uncalibrated ResNet tends to be overconfident in its predictions.",5. Results,[0],[0]
"We then can observe the effects of temperature scaling (middle left), histogram binning (middle right), and isotonic regression (far right) on calibration.",5. Results,[0],[0]
All three displayed methods produce much better confidence estimates.,5. Results,[0],[0]
"Of the three methods, temperature scaling most closely recovers the desired diagonal function.",5. Results,[0],[0]
"Each of the bins are well calibrated, which is remarkable given that all the probabilities were modified by only a single parameter.",5. Results,[0],[0]
"We include reliability diagrams for other datasets in Section S4.
",5. Results,[0],[0]
Computation time.,5. Results,[0],[0]
All methods scale linearly with the number of validation set samples.,5. Results,[0],[0]
"Temperature scaling is by far the fastest method, as it amounts to a onedimensional convex optimization problem.",5. Results,[0],[0]
"Using a conjugate gradient solver, the optimal temperature can be found in 10 iterations, or a fraction of a second on most modern hardware.",5. Results,[0],[0]
"In fact, even a naive line-search for the optimal temperature is faster than any of the other methods.",5. Results,[0],[0]
"The computational complexity of vector and matrix scaling are linear and quadratic respectively in the number of classes, reflecting the number of parameters in each method.",5. Results,[0],[0]
"For CIFAR-100 (K = 100), finding a near-optimal vector scal-
ing solution with conjugate gradient descent requires at least 2 orders of magnitude more time.",5. Results,[0],[0]
"Histogram binning and isotonic regression take an order of magnitude longer than temperature scaling, and BBQ takes roughly 3 orders of magnitude more time.
",5. Results,[0],[0]
Ease of implementation.,5. Results,[0],[0]
"BBQ is arguably the most difficult to implement, as it requires implementing a model averaging scheme.",5. Results,[0],[0]
"While all other methods are relatively easy to implement, temperature scaling may arguably be the most straightforward to incorporate into a neural network pipeline.",5. Results,[0],[0]
"In Torch7 (Collobert et al., 2011), for example, we implement temperature scaling by inserting a nn.MulConstant between the logits and the softmax, whose parameter is 1/T .",5. Results,[0],[0]
"We set T =1 during training, and subsequently find its optimal value on the validation set.",5. Results,[0],[0]
Modern neural networks exhibit a strange phenomenon: probabilistic error and miscalibration worsen even as classification error is reduced.,6. Conclusion,[0],[0]
"We have demonstrated that recent advances in neural network architecture and training – model capacity, normalization, and regularization – have strong effects on network calibration.",6. Conclusion,[0],[0]
It remains future work to understand why these trends affect calibration while improving accuracy.,6. Conclusion,[0],[0]
"Nevertheless, simple techniques can effectively remedy the miscalibration phenomenon in neural networks.",6. Conclusion,[0],[0]
"Temperature scaling is the simplest, fastest, and most straightforward of the methods, and surprisingly is often the most effective.",6. Conclusion,[0],[0]
"The authors are supported in part by the III-1618134, III1526012, and IIS-1149882 grants from the National Science Foundation, as well as the Bill and Melinda Gates Foundation and the Office of Naval Research.",Acknowledgments,[0],[0]
Confidence calibration – the problem of predicting probability estimates representative of the true correctness likelihood – is important for classification models in many applications.,abstractText,[0],[0]
"We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated.",abstractText,[0],[0]
"Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration.",abstractText,[0],[0]
We evaluate the performance of various post-processing calibration methods on state-ofthe-art architectures with image and document classification datasets.,abstractText,[0],[0]
"Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling – a singleparameter variant of Platt Scaling – is surprisingly effective at calibrating predictions.",abstractText,[0],[0]
On Calibration of Modern Neural Networks,title,[0],[0]
"In this paper, we consider a variant of the problem of dictionary learning, a widely used unsupervised technique for learning compact (sparse) representations of high dimensional data.",1.1. Motivation,[0],[0]
"At its core, the challenge in dictionary learning is to discover a basis (or dictionary) that can sparsely represent a given set of data samples with as little empirical representation error as possible.",1.1. Motivation,[0],[0]
"The study of sparse coding enjoys a rich history in image processing, machine learning, and compressive sensing (Elad & Aharon, 2006; Aharon et al., 2006; Olshausen & Field, 1997; Candes & Tao, 2005; Rubinstein et al., 2010; Gregor & LeCun, 2010; Boureau et al., 2010).",1.1. Motivation,[0],[0]
"While the majority of these aforementioned works involved heuristics, several exciting re-
1Iowa State University 2Yahoo!",1.1. Motivation,[0],[0]
Research.,1.1. Motivation,[0],[0]
"Correspondence to: Thanh V. Nguyen <thanhng@iastate.edu>.
",1.1. Motivation,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1.1. Motivation,[0],[0]
"Copyright 2018 by the author(s).
cent results (Spielman et al., 2012; Agarwal et al., 2013; 2014; Arora et al., 2014; 2015; Sun et al., 2015; Chatterji & Bartlett, 2017; Nguyen et al., 2018) have established rigorous conditions under which their algorithms recover the true dictionary under suitable generative models for the data.
",1.1. Motivation,[0],[0]
An important underlying assumption that guides the success of all existing dictionary learning algorithms is the availability of (sufficiently many) data samples that are fully observed.,1.1. Motivation,[0],[0]
"Our focus, on the other hand, is on the special case where the given data points are only partially observed, that is, we are given access to only a small fraction of the coordinates of the data samples.
",1.1. Motivation,[0],[0]
"Such a setting of incomplete observations is natural in many applications like image-inpainting and demosaicing (Rubinstein et al., 2010).",1.1. Motivation,[0],[0]
"For example, this routinely appears in hyper-spectral imaging (Xing et al., 2012) where entire spectral bands of signals could be missing or unobserved.",1.1. Motivation,[0],[0]
"Moreover, in other applications, collecting fully observed samples can be expensive (or in some cases, even infeasible).",1.1. Motivation,[0],[0]
"Examples include the highly unreliable continuous blood glucose (CBG) monitoring systems that suffer from signal dropouts, where often the task is to learn a dictionary from partially observed signals (Naumova & Schnass, 2017a).
",1.1. Motivation,[0],[0]
"Earlier works that tackle the incomplete variant of the dictionary learning problem only offer heuristic solutions (Xing et al., 2012; Naumova & Schnass, 2017a) or involve constructing intractable statistical estimators (Soni et al., 2016).",1.1. Motivation,[0],[0]
"Indeed, the recovery of the true dictionary involves analyzing an extremely non-convex optimization problem that is, in general, not solvable in polynomial time (Loh & Wainwright, 2011).",1.1. Motivation,[0],[0]
"To our knowledge, our work is the first to give a theoretically sound as well as tractable algorithm to recover the exact dictionary from missing data (provided certain natural assumptions are met).",1.1. Motivation,[0],[0]
"In this paper, we make concrete theoretical algorithmic progress to the dictionary learning problem with incomplete samples.",1.2. Our Contributions,[0],[0]
"Inspired by recent algorithmic advances in dictionary learning (Arora et al., 2014; 2015), we adopt a learning-theoretic setup.",1.2. Our Contributions,[0],[0]
"Specifically, we assume that each data sample is synthesized from a generative model with an unknown dictionary and a random k-sparse coefficient
vector (or sparse code).",1.2. Our Contributions,[0],[0]
"Mathematically, the data samples Y =",1.2. Our Contributions,[0],[0]
"[y(1), y(2), . . .",1.2. Our Contributions,[0],[0]
", y(p)]",1.2. Our Contributions,[0],[0]
"2 Rn⇥p are of the form
Y = A⇤X⇤ ,
where A⇤ 2 Rn⇥m denotes the dictionary and X⇤ 2 Rm⇥p denotes the (column-wise) k-sparse codes.
",1.2. Our Contributions,[0],[0]
"However, we do not have direct access to the data; instead, each high-dimensional data sample is further subsampled such that only a small fraction of the entries are observed.",1.2. Our Contributions,[0],[0]
"The assumption we make is that each entry of Y is observed independently with probability ⇢ 2 (0, 1].",1.2. Our Contributions,[0],[0]
"For reasons that will become clear, we also assume that the ground truth dictionary A⇤ is both incoherent (i.e., the columns of A⇤ are sufficiently close to orthogonal) and democratic (i.e., the energy of each atom is well spread).",1.2. Our Contributions,[0],[0]
Both these assumptions are standard in the compressive-sensing literature.,1.2. Our Contributions,[0],[0]
"We clarify the generative model more precisely in the sequel.
",1.2. Our Contributions,[0],[0]
"Given a set of such (partially observed) data samples, our goal is to recover the true dictionary A⇤.",1.2. Our Contributions,[0],[0]
"Towards this goal, we make the following contributions:
1.",1.2. Our Contributions,[0],[0]
"Let us assume, for a moment, that we are given a coarse estimate A0 that is sufficiently close to the true dictionary.",1.2. Our Contributions,[0],[0]
"We devise a descent-style algorithm that leverages the given incomplete data to iteratively refine the dictionary estimate; moreover, we show that it converges rapidly to an estimate within a small ball of the ground truth A⇤ (whose radius decreases given more samples).",1.2. Our Contributions,[0],[0]
"Our result can be informally summarized as follows:
Theorem 1 (Informal, descent).",1.2. Our Contributions,[0],[0]
"When given a “sufficientlyclose” initial estimate A 0 , there exists an iterative gradient
descent-type algorithm that linearly converges to the true dictionary with O(mk polylog(n))",1.2. Our Contributions,[0],[0]
"incomplete samples.
",1.2. Our Contributions,[0],[0]
"Our above result mirrors several recent results in non-convex learning that all develop a descent algorithm which succeeds given a good enough initialization (Yuan & Zhang, 2013; Cai et al., 2016; Tu et al., 2016).",1.2. Our Contributions,[0],[0]
"Indeed, similar guarantees for descent-style algorithms (such as alternating minimization) exist for the related problem of matrix completion (Jain et al., 2013), which coincides with our setting if m ⌧ n.",1.2. Our Contributions,[0],[0]
"However, our setting is distinct, since we are interested in learning overcomplete dictionaries, where m > n.
2.",1.2. Our Contributions,[0],[0]
"Having established the efficiency of the above refinement procedure, we then address the challenge of actually coming up with a coarse estimate of A⇤.",1.2. Our Contributions,[0],[0]
We do not know of a provable procedure that produces a good enough initial estimate using partial samples.,1.2. Our Contributions,[0],[0]
"To circumvent this issue, we assume availability of O(m) fully observed samples along with the partial samples1.",1.2. Our Contributions,[0],[0]
"Given this setting, we show
1While this might be a limitation of our analysis, we emphasize
that we can provide a “sufficiently close” initial estimate in polynomial time.",1.2. Our Contributions,[0],[0]
"Our result can be summarized as follows:
Theorem 2 (Informal, initialization).",1.2. Our Contributions,[0],[0]
"There exists an initialization algorithm that, given O(m polylog(n)) fully observed samples and an additional O(mk polylog(n)) partially observed samples, returns an initial estimate A 0 that is sufficiently close to A ⇤ in a column-wise sense.",1.2. Our Contributions,[0],[0]
"The majority of our theoretical contributions are fairly technical, so for clarity, we provide some non-rigorous intuition.
",1.3. Techniques,[0],[0]
"At a high level, our approach merges ideas from two main themes in the algorithmic learning theory literature.",1.3. Techniques,[0],[0]
"We build upon recent seminal, theoretically-sound algorithms for sparse coding (specifically, the framework of Arora et al. (2015)).",1.3. Techniques,[0],[0]
Their approach consists of a descent-based algorithm performed over the surface of a suitably defined loss function of the dictionary parameters.,1.3. Techniques,[0],[0]
The descent is achieved by alternating between updating the dictionary estimate and updating the sparse codes of the data samples.,1.3. Techniques,[0],[0]
"The authors prove that this algorithm succeeds provided that the codes are sparse enough, the columns of A⇤ are incoherent, and that we are given sufficiently many samples.
",1.3. Techniques,[0],[0]
"However, a direct application of the above framework to the partially observed setting does not seem to succeed.",1.3. Techniques,[0],[0]
"To resolve this, we leverage a specific property that is commonly assumed in the matrix completion literature: we suppose that the dictionaries are not “spiky” and that the energy of each atom is spread out among its coordinates; specifically, the sub-dictionaries formed by randomly sub-selecting rows are still incoherent.",1.3. Techniques,[0],[0]
"We call such dictionaries democratic, following the terminology of Davenport et al. (2009).",1.3. Techniques,[0],[0]
"(In matrix completion papers, this property is also sometimes referred to incoherence, but we avoid doing so since that overloads the term.)",1.3. Techniques,[0],[0]
"Our main contribution is to show that democratic, incoherent dictionaries can be learned via a similar alternating descent scheme if only a small fraction of the data entries are available.",1.3. Techniques,[0],[0]
"Our analysis is novel and distinct than that provided in (Arora et al., 2015).
",1.3. Techniques,[0],[0]
"Of course, the above analysis is somewhat local in nature since we are using a descent-style method.",1.3. Techniques,[0],[0]
"In order to get global guarantees for recovery of A⇤, we need to initialize carefully.",1.3. Techniques,[0],[0]
"Here too, the spectral initialization strategies suggested in earlier dictionary learning papers (Arora et al., 2014; 2015) do not succeed.",1.3. Techniques,[0],[0]
"To resolve this, we again appeal to the democracy property of A⇤.",1.3. Techniques,[0],[0]
"We also need
that the number of full samples needed by our method is relatively small.",1.3. Techniques,[0],[0]
"Indeed, the state-of-the-art approach for dictionary learning (Arora et al., 2015) requires O(mk polylog(n)) fully observed samples, while our method needs only O(m polylog(n)) samples, which represents a polynomial improvement since k can be as large as p n.
to assume that provided a small hold-out set of additional, fully observed samples is available2.",1.3. Techniques,[0],[0]
"Using this hold-out set (which can be construed as additional prior information or “side” information) together with the available samples gives us a spectral initialization strategy that provably gives a good enough initial estimate.
",1.3. Techniques,[0],[0]
"Putting the above two pieces together: if we are provided O(mk/⇢4 polylog n) partially observed samples from the generative model, together with an additional O(m polylog n) full samples, then we can guarantee a fast, provable algorithm for learning A⇤.",1.3. Techniques,[0],[0]
"See Table 1 for a summary of our results, and comparison with existing work.",1.3. Techniques,[0],[0]
"We remark that while our algorithms only succeed up to sparsity level k  O(⇢ p n), we obtain a running time improvement over the best available dictionary learning approaches.",1.3. Techniques,[0],[0]
The literature on dictionary learning (or sparse coding) is very vast and hence our references to prior work will necessarily be incomplete; we refer to the seminal work of Rubinstein et al. (2010) for a list of applications.,1.4. Relation to Prior Work,[0],[0]
"Dictionary learning with incompletely observed data, however, is far less well-understood.",1.4. Relation to Prior Work,[0],[0]
"Initial attempts in this direction (Xing et al., 2012) involve Bayesian-style techniques; more recent attempts have focused on alternating minimization techniques, along with incoherence- and democracy-type assumptions akin to our framework (Naumova & Schnass, 2017b;a).",1.4. Relation to Prior Work,[0],[0]
"However, none of these methods provide rigorous polynomial-time algorithms that provably succeed in recovering the dictionary parameters.
",1.4. Relation to Prior Work,[0],[0]
"Our setup can also be viewed as an instance of matrix completion, which has been a source of intense interest in the machine learning community over the last decade (Candès & Recht, 2009; Keshavan et al., 2010).",1.4. Relation to Prior Work,[0],[0]
"The typical assumption in such approaches is that the data matrix Y = A⇤X⇤ is low-rank (i.e., A⇤ typically spans a low-dimensional subspace).",1.4. Relation to Prior Work,[0],[0]
"This assumption leads to either feasible convex relaxations, or a bilinear form that can be solved approximately via alternating minimization.",1.4. Relation to Prior Work,[0],[0]
"However, our work differs significantly from this setup, since we are interested in the case where A⇤ is over-complete; moreover, our guarantees are not in terms of estimating the missing entries of Y , but rather obtaining the atoms in A⇤.",1.4. Relation to Prior Work,[0],[0]
"Note that our generative model also differs from the setup of high-rank matrix completion (Eriksson et al., 2012), where the data is sampled randomly from a finite union-of-subspaces.",1.4. Relation to Prior Work,[0],[0]
"In contrast, our data samples are synthesized via sparse linear combinations of a given dictionary.
",1.4. Relation to Prior Work,[0],[0]
"2We do not know how to remove this assumption, and it appears that techniques stronger than spectral initialization (e.g., involving higher-order moments) are required.
",1.4. Relation to Prior Work,[0],[0]
"In the context of matrix-completion, perhaps the most related work to ours is the statistical analysis of matrixcompletion under the sparse-factor model of Soni et al. (2016), which employs a similar generative data model to ours.",1.4. Relation to Prior Work,[0],[0]
"(Similar sparse-factor models have been studied in the work of Lan et al. (2014), but no complexity guarantees are provided.)",1.4. Relation to Prior Work,[0],[0]
"For this model, Soni et al. (2016) propose a highly non-convex statistical estimator for estimate Y and provide error bounds for this estimator under various noise models.",1.4. Relation to Prior Work,[0],[0]
"However, they do not discuss an efficient algorithm to realize that estimator.",1.4. Relation to Prior Work,[0],[0]
"In contrast, we provide rigorous polynomial time algorithms, together with error bounds on the estimation quality of A⇤.",1.4. Relation to Prior Work,[0],[0]
"Overall, we anticipate that our work can shed some light on the design of provable algorithms for matrix-completion in such more general settings.",1.4. Relation to Prior Work,[0],[0]
Notation.,2. Preliminaries,[0],[0]
"Given a vector x 2 Rm and a subset S ✓ [m], we denote xS 2 Rm as a vector which equals x in indices belonging to S and equals zero elsewhere.",2. Preliminaries,[0],[0]
"We use A•i and A T
j• respectively to denote the ith column and the jth row of matrix A 2 Rn⇥m. We use A•S as the submatrix of A with columns in S. In contrast, we use A • to indicate the submatrix of A with rows not in set to zero.",2. Preliminaries,[0],[0]
Let supp(x) and sgn(x) be the support and element-wise sign of x. Let thresholdK(x) be the hard-thresholding operator that sets all entries of x with magnitude less than K to zero.,2. Preliminaries,[0],[0]
"The symbol k·k refers to the `2-norm, unless otherwise specified.
",2. Preliminaries,[0],[0]
"For asymptotic analysis, we use e⌦(·) and eO(·) to represent ⌦(·) and O(·) up to (unspecified) poly-logarithmic factors depending on n. Besides, g(n) =",2. Preliminaries,[0],[0]
"O⇤(f(n)) denotes g(n)  Kf(n) for some sufficiently small constant K. Finally, the terms “with high probability” (abbreviated to w.h.p.) is used to indicate an event with failure probability O(n !(1)).",2. Preliminaries,[0],[0]
"We make use of the following definitions.
",2. Preliminaries,[0],[0]
Definition 1 (Incoherence).,2. Preliminaries,[0],[0]
The matrix A is incoherent with parameter µ if the following holds for all columns i 6=,2. Preliminaries,[0],[0]
"j:
|hA•i, A•ji| kA•ikkA•jk  µp n .
",2. Preliminaries,[0],[0]
"The incoherence property requires the columns of A to be approximately orthogonal, and is a canonical property to resolve identifiability issues in dictionary learning and sparse recovery.",2. Preliminaries,[0],[0]
We distinguish this from the conventional notion of “incoherence” widely used in the matrix completion literature.,2. Preliminaries,[0],[0]
"This notion is related to a notion that we call democracy, which we define next.
",2. Preliminaries,[0],[0]
Definition 2 (Democracy).,2. Preliminaries,[0],[0]
Suppose that the matrix A is µ-incoherent.,2. Preliminaries,[0],[0]
A is further said to be democratic if the submatrix A • is µ-incoherent for any subset ⇢,2. Preliminaries,[0],[0]
[n] of size p n  | |  ,2. Preliminaries,[0],[0]
"n.
This property tells us that the rows of A have roughly the same amount of “information”, and that the submatrix of A restricted to any subset of rows is also incoherent.",2. Preliminaries,[0],[0]
"A similar concept (stated in terms of the restricted isometry property) is well-known in the compressive sensing literature (Davenport et al., 2009).",2. Preliminaries,[0],[0]
Several probabilistic constructions of dictionaries satisfy this property; typical examples include random matrices drawn from i.i.d.,2. Preliminaries,[0],[0]
Gaussian or Rademacher distributions.,2. Preliminaries,[0],[0]
"The p n lower bound on | | is to ensure that the submatrix of A including only the rows in is balanced in terms of dimensions.
",2. Preliminaries,[0],[0]
We seek an algorithm that provides a provably “good” estimate of A⇤.,2. Preliminaries,[0],[0]
"For this, we need a suitable measure of “goodness”.",2. Preliminaries,[0],[0]
The following notion of distance records the maximal column-wise difference between any estimate A and A⇤ in `2-norm under a suitable permutation and sign flip.,2. Preliminaries,[0],[0]
"Definition 3 (( ,)-nearness).",2. Preliminaries,[0],[0]
"The matrix A is said to be -close to A
⇤ if k (i)A•⇡(i)",2. Preliminaries,[0],[0]
"A⇤•ik  holds for every i =
1, 2, . . .",2. Preliminaries,[0],[0]
",m and some permutation ⇡ : [m]!",2. Preliminaries,[0],[0]
[m] and sign flip : [m] : {±1}.,2. Preliminaries,[0],[0]
"In addition, if kA•⇡ A⇤k  kA⇤k holds, then A is said to be ( ,)-near to A⇤.
",2. Preliminaries,[0],[0]
"To keep notation simple, in our convergence theorems below, whenever we discuss nearness, we simply replace the transformations ⇡ and in the above definition with the identity mapping ⇡(i) =",2. Preliminaries,[0],[0]
"i and the positive sign (·) = +1 while keeping in mind that in reality, we are referring to finding one element in the equivalence class of all permutations and sign flips of A⇤.
",2. Preliminaries,[0],[0]
"Armed with the above concepts, we now posit a generative model for our observed data.",2. Preliminaries,[0],[0]
Suppose that the data samples Y =,2. Preliminaries,[0],[0]
"[y(1), y(2), . . .",2. Preliminaries,[0],[0]
", y(p)] are such that each column is generated according to the rule:
y = P (A⇤x⇤), (1)
where A⇤ is an unknown, ground truth dictionary; x⇤ and are drawn from some distribution D and P is the sampling
operator that keeps entries in untouched and zeroes out everything else.",2. Preliminaries,[0],[0]
"We emphasize that is independently chosen for each y(i), so more precisely, y(i) = y(i)
(i) 2 Rn.
",2. Preliminaries,[0],[0]
We ignore the superscript to keep the notation simple.,2. Preliminaries,[0],[0]
We also make the following assumptions: Assumption 1.,2. Preliminaries,[0],[0]
"The true dictionary A⇤ is over-complete with m  Kn for some constant K > 1, and democratic with parameter µ. All columns of A ⇤ have unit norms.
",2. Preliminaries,[0],[0]
Assumption 2.,2. Preliminaries,[0],[0]
The true dictionary A⇤ has bounded spectral and max (`1-) norms such that kA⇤k  O( p m/n) and kA⇤kmax  O(1/ p n).,2. Preliminaries,[0],[0]
Assumption 3.,2. Preliminaries,[0],[0]
The code vector x⇤ is k-sparse random with uniform support S.,2. Preliminaries,[0],[0]
"The nonzero entries of x ⇤ are pairwise
independent sub-Gaussian with variance 1, and bounded
below by some known constant C.
Assumption 4.",2. Preliminaries,[0],[0]
"Each entry of the sample A⇤x⇤ is independently observed with constant probability ⇢ 2 (0, 1].
",2. Preliminaries,[0],[0]
"The incoherence and spectral bound are ubiquitous in the dictionary learning literature (Arora et al., 2014; 2015).",2. Preliminaries,[0],[0]
"For the incomplete setting, we further require the democracy and max-norm bounds to control the spread of energy of the entries of A⇤, so that A⇤ is not “spiky”.",2. Preliminaries,[0],[0]
"Such conditions are often encountered in the matrix completion literature (Candès & Recht, 2009; Keshavan et al., 2010).",2. Preliminaries,[0],[0]
"The distributional assumptions on the code vectors x⇤ are standard in theoretical dictionary learning (Agarwal et al., 2014; Arora et al., 2014; Gribonval et al., 2015; Arora et al., 2015).",2. Preliminaries,[0],[0]
"Finally, we also require the sparsity k  O⇤(⇢ p n/ log n) throughout the paper.",2. Preliminaries,[0],[0]
We now design and analyze an algorithm for learning the dictionary A⇤ given incomplete samples of the form (1).,3. A Descent-Style Learning Algorithm,[0],[0]
"Our strategy will be to use a descent-like scheme to construct a sequence of estimates A which successively gets closer to
A ⇤ in the sense of ( ,)-nearness.
",3. A Descent-Style Learning Algorithm,[0],[0]
Let us first provide some intuition.,3. A Descent-Style Learning Algorithm,[0],[0]
The natural approach to solve this problem is to perform gradient descent over an appropriate empirical loss of the dictionary parameters.,3. A Descent-Style Learning Algorithm,[0],[0]
"More precisely, we consider the squared loss between observed entries of Y and their estimates (which is the typical loss function used in the incomplete observations setting (Jain et al., 2013)):
L(A) = 1 2
X
i,j2⌦ (Yij (AX)ij)2, (2)
where ⌦ is the set of locations of observed entries in the samples Y .",3. A Descent-Style Learning Algorithm,[0],[0]
"However, straightforward gradient descent over A is not possible for several reasons: (i) the gradient depends on the finite sample variability of Y ; (ii) the gradient with respect to A depends on the optimal code vectors of the data samples, x⇤
i , which are unknown a priori; (iii) since
we are working in the overcomplete setting, care has to be taken to ensure that the code vectors (i.e., columns of X) obey the sparsity model (as specified in Assumption 2).
",3. A Descent-Style Learning Algorithm,[0],[0]
The neurally-plausible sparse coding algorithm of Arora et al. (2015) provides a crucial insight into the understanding of the loss surface of LA in the fully observed setting.,3. A Descent-Style Learning Algorithm,[0],[0]
"Basically, within a small ball around the ground truth A⇤, the surface is well behaved such that a noisy version of X⇤ is sufficient to construct a good enough approximation to the gradient of L. Moreover, given an estimate within a small ball around A⇤, a noisy (but good enough) estimate of X⇤ can be quickly computed using a thresholding operation.
",3. A Descent-Style Learning Algorithm,[0],[0]
We extend this understanding to the (much more challenging) setting of incomplete observations.,3. A Descent-Style Learning Algorithm,[0],[0]
"Specifically, we show the loss surface in (2) behaves well even with missing data.",3. A Descent-Style Learning Algorithm,[0],[0]
This enables us to devise an algorithm similar to that of Arora et al. (2015) and obtain a descent property directly related to (the population parameter) A⇤.,3. A Descent-Style Learning Algorithm,[0],[0]
"The full procedure is detailed as Algorithm 1.
",3. A Descent-Style Learning Algorithm,[0],[0]
We now analyze our proposed algorithm.,3. A Descent-Style Learning Algorithm,[0],[0]
"Specifically, we can show that if initialized properly and with proper choice of step size, Algorithm 1 exhibits linear convergence to a ball of radius O( p k/n) around A⇤.",3. A Descent-Style Learning Algorithm,[0],[0]
"Formally, we have:
Theorem 3.",3. A Descent-Style Learning Algorithm,[0],[0]
"Suppose that the initial estimate A0 is ( , 2)- near to A ⇤ with = O⇤(1/ log n) and the sampling prob-
ability satisfies ⇢ 1/(k + 1).",3. A Descent-Style Learning Algorithm,[0],[0]
"If Algorithm 1 is given p = e⌦(mk) fresh partial samples at each step and uses learning rate ⌘ = ⇥(m/⇢k), then
E[kAs•i A⇤•ik 2]  (1 ⌧)skA0•i A⇤•ik
2 +O( p k/n)
for some 0 < ⌧ < 1/2 and s = 1, 2, . . .",3. A Descent-Style Learning Algorithm,[0],[0]
", T .",3. A Descent-Style Learning Algorithm,[0],[0]
"As a corollary, A s converges geometrically to A ⇤ until column-wise O( p k/n) error.
",3. A Descent-Style Learning Algorithm,[0],[0]
"Algorithm 1 Gradient descent-style algorithm Input: Partial samples Y with observed entry set (i) Initial A0 that is ( , 2)-near to A⇤ for s = 0, 1, . . .",3. A Descent-Style Learning Algorithm,[0],[0]
", T do
/*",3. A Descent-Style Learning Algorithm,[0],[0]
"Encoding step */ for i = 1, 2, . . .",3. A Descent-Style Learning Algorithm,[0],[0]
", p do
x",3. A Descent-Style Learning Algorithm,[0],[0]
"(i) thresholdC/2( 1⇢ (A s)T y(i))
",3. A Descent-Style Learning Algorithm,[0],[0]
end /*,3. A Descent-Style Learning Algorithm,[0],[0]
"Update step */ bgs 1
p
P p
i=1(P (i)(Asx(i))",3. A Descent-Style Learning Algorithm,[0],[0]
"y(i))sgn(x(i))T
A s+1 As ⌘bgs
end Output: A AT as a learned dictionary
We defer the full proof of Theorem 3 to Appendix C. To understand the working of the algorithm and its correctness, let us consider the setting where we have access to infinitely many samples.",3. A Descent-Style Learning Algorithm,[0],[0]
"This setting is, of course, fictional; however, expectations are easier to analyze than empirical averages, and moreover, this exercise reveals several key elements for proving Theorem 3.",3. A Descent-Style Learning Algorithm,[0],[0]
"More precisely, we first provide bounds on the expected value of bgs, denoted as
g s , Ey[(P (Asx) y)sgn(x)T ],
to establish the descent property for the infinite sample case.",3. A Descent-Style Learning Algorithm,[0],[0]
"The sample complexity argument emerges when we control the concentration of bgs, detailed in Appendix C. Here, we separately discuss the encoding and update steps in Algorithm 1.
",3. A Descent-Style Learning Algorithm,[0],[0]
Encoding step.,3. A Descent-Style Learning Algorithm,[0],[0]
The first main result is to show that the hard-thresholding (or pooling)-based rule for estimating the sparse code vectors is sufficiently accurate.,3. A Descent-Style Learning Algorithm,[0],[0]
"This rule adapts the encoding step of the dictionary learning algorithm proposed in (Arora et al., 2015), with an additional scaling factor 1/⇢.",3. A Descent-Style Learning Algorithm,[0],[0]
"This scaling is necessary to avoid biases arising due to the presence of incomplete information.
",3. A Descent-Style Learning Algorithm,[0],[0]
The primary novelty is in our analysis.,3. A Descent-Style Learning Algorithm,[0],[0]
"Specifically, we prove that the estimate of X obtained via the encoding step (even under partial observations) enables a good enough identification of the support of the true X⇤.",3. A Descent-Style Learning Algorithm,[0],[0]
"The key, here, is to leverage the fact that A⇤ is democratic and that As is near A⇤.",3. A Descent-Style Learning Algorithm,[0],[0]
"We call this property support consistency and establish it as follows.
",3. A Descent-Style Learning Algorithm,[0],[0]
Lemma 1.,3. A Descent-Style Learning Algorithm,[0],[0]
"Suppose that As is ( , 2)-near to A⇤ with = O
⇤(1/ log n).",3. A Descent-Style Learning Algorithm,[0],[0]
"With high probability over y = P (A⇤x⇤), the estimate x obtained by the encoding step of Algorithm 1 has the same sign as the true x ⇤ ; that is,
sgn thresholdC/2 1 ⇢ (As)T y = sgn(x⇤), (3)
This holds true for incoherence parameter µ  p n
2k , sparsity
parameter k ⌦(logm) and subsampling probability ⇢",3. A Descent-Style Learning Algorithm,[0],[0]
"1/(k + 1).
",3. A Descent-Style Learning Algorithm,[0],[0]
"Lemma 1 implies that when the “mass” of A⇤ is spread out across entries, within a small neighborhood of A⇤ the estimate x is reliable even if y is incompletely observed.",3. A Descent-Style Learning Algorithm,[0],[0]
"This lemma is the main ingredient for bounding the behavior of the update rule.
",3. A Descent-Style Learning Algorithm,[0],[0]
Update step.,3. A Descent-Style Learning Algorithm,[0],[0]
The support consistency property of the estimated x arising in the encoding step is key to rigorously analyzing the expected gradient gs.,3. A Descent-Style Learning Algorithm,[0],[0]
"This relatively ‘simple’ encoding enables an explicit form of the update rule, and gives an intuitive reasoning on how the descent property can be achieved.",3. A Descent-Style Learning Algorithm,[0],[0]
"In fact, we will see that
g s i = ⇢piqi( s i A s •i A⇤•i) + o(⇢piqi)
for pi = E[|x⇤i ||i 2 S], qi = P[i 2 S] and si = hA•i, A⇤•ii.",3. A Descent-Style Learning Algorithm,[0],[0]
"Since we assume that the current estimate As is (columnwise) sufficiently close to A⇤, each s
i is approximately
equal to 1, and hence gs i ⇡ ⇢piqi(As•i A⇤•i), i.e., the gradient points in the desired direction.",3. A Descent-Style Learning Algorithm,[0],[0]
"Combining this with standard analysis of gradient descent, we can prove that the overall algorithm geometrically decreases the error in each step s as long as the learning rate ⌘ is properly chosen.",3. A Descent-Style Learning Algorithm,[0],[0]
"Specifically, we get the following theoretical result.",3. A Descent-Style Learning Algorithm,[0],[0]
Theorem 4.,3. A Descent-Style Learning Algorithm,[0],[0]
"Suppose that A0 is ( , 2)-near to A⇤ with = O
⇤(1/ log n) and the sampling probability satisfies ⇢ 1/(k + 1).",3. A Descent-Style Learning Algorithm,[0],[0]
"Assuming infinitely many partial samples at each step, Algorithm 1 geometrically converges to A ⇤ until
column-wise error O(k/⇢n).",3. A Descent-Style Learning Algorithm,[0],[0]
"More precisely,
kAs+1•i",3. A Descent-Style Learning Algorithm,[0],[0]
"A ⇤ •ik 2  (1 ⌧)kAs•i A⇤•ik 2 +O k 2 /⇢ 2 n 2
for some 0 < ⌧ < 1/2 and for s = 1, 2, . . .",3. A Descent-Style Learning Algorithm,[0],[0]
", T provided the learning rate obeys ⌘ = ⇥(m/⇢k).
",3. A Descent-Style Learning Algorithm,[0],[0]
We provide the mathematical proof for the form of gs as well as the descent in Appendix A.2.,3. A Descent-Style Learning Algorithm,[0],[0]
"We also argue that the ( , 2)-nearness of As+1 and A⇤ is maintained after each update.",3. A Descent-Style Learning Algorithm,[0],[0]
This is studied in Lemma 7 in Appendix A.,3. A Descent-Style Learning Algorithm,[0],[0]
"In the previous section, we provided an algorithm that (accurately) recovers A⇤ in an iterative descent-style approach.",4. An Initialization Algorithm,[0],[0]
"In order to establish correctness guarantees, the algorithm requires a coarse estimate A0 that is -close to the ground truth with closeness parameter = O⇤(1/ log n).",4. An Initialization Algorithm,[0],[0]
"This section presents an initialization strategy to obtain such a good starting point for A⇤.
",4. An Initialization Algorithm,[0],[0]
"Again, we begin with some intuition.",4. An Initialization Algorithm,[0],[0]
"At a high level, our algorithm mimics the spectral initialization strategy for dictionary learning proposed by (Arora et al., 2015).",4. An Initialization Algorithm,[0],[0]
"In essence,
the idea is to re-weight the data samples (which are fully observed) appropriately.",4. An Initialization Algorithm,[0],[0]
"When this is the case, analyzing the spectral properties of the covariance matrix of the new re-weighted samples gives us the desired initialization.",4. An Initialization Algorithm,[0],[0]
"The re-weighting itself relies upon the computation of pairwise correlations between the samples with two fixed samples (say, u and v) chosen from an independent hold-out set.",4. An Initialization Algorithm,[0],[0]
"This strategy is appealing in both from the standpoint of statistical efficiency as well as computational ease.
",4. An Initialization Algorithm,[0],[0]
"Unfortunately, a straightforward application of this strategy to our setting of incomplete observations does not work.",4. An Initialization Algorithm,[0],[0]
"The major issue, of course, is that pairwise correlation (the inner product) of two high dimensional vectors is highly uninformative if each vector is only partially observed.",4. An Initialization Algorithm,[0],[0]
"We circumvent this issue via the following simple (but key) observation: provided the underlying dictionary is democratic and the representation is sufficiently sparse, the correlation between a partially observed data sample y with a fully observed sample u is indeed proportional to the actual correlation between y and u. Therefore, assuming that we are given a hold-out set that is fully observed, an adaptation of the spectral approach of Arora et al. (2015) provably succeeds.",4. An Initialization Algorithm,[0],[0]
"Moreover, the size of the hold-out set need not be large; in particular, we need only O(m polylog(n)) fully-observed samples, as opposed to the O(mk polylog(n))",4. An Initialization Algorithm,[0],[0]
samples required by the analysis of Arora et al. (2015).,4. An Initialization Algorithm,[0],[0]
"The parameter k can be as big as p n, so in fact we require polynomially fewer fully-observed samples.
",4. An Initialization Algorithm,[0],[0]
"In summary: in order to initialize our descent procedure, we assume the availability of a small (but fully observed) hold-out set.",4. An Initialization Algorithm,[0],[0]
"In practice, we can imagine expending some amount of effort in the beginning to collect all the entries of a small subset of the available data samples.",4. An Initialization Algorithm,[0],[0]
"The availability of such additional information (or “side-information”) has been made in the literature on matrix completion (Natarajan & Dhillon, 2014).
",4. An Initialization Algorithm,[0],[0]
The full procedure is described in pseudocode form as Algorithm 2.,4. An Initialization Algorithm,[0],[0]
"Our main theoretical result (Theorem 5) summarizes its performance.
",4. An Initialization Algorithm,[0],[0]
Theorem 5.,4. An Initialization Algorithm,[0],[0]
"Suppose that the available training dataset consists of p1 fully observed samples, together with p2 in-
completely observed samples according to the observation model (1).",4. An Initialization Algorithm,[0],[0]
"Suppose µ = O⇤ p n
k log3 n
, 1 ⇢ 1  k 
O ⇤( ⇢
p n
logn ).",4. An Initialization Algorithm,[0],[0]
"When p1 = e⌦(m) and p2 = e⌦(mk/⇢4), then with high probability, Algorithm 2 returns an initial estimate A 0 whose columns share the same support as A ⇤ and is ( , 2)-near to A⇤ with = O⇤(1/ log n).
",4. An Initialization Algorithm,[0],[0]
"The full proof is provided in Appendix B. To provide some intuition about the working of the algorithm and its proof, let us again consider the setting where we have access to infinitely many samples.",4. An Initialization Algorithm,[0],[0]
"These analyses result in key lemmas,
Algorithm 2 Spectral initialization algorithm Input: P1: p1 fully observed samples P2: p2 partially observed samples Set L = ; while |L| < m do
Pick u and v from P1 at random Construct the weighted covariance matrix cMu,v using samples y(i) from P2
cMu,v 1
p2⇢ 4
p2X
i=1
hy(i), uihy(i), viy(i)(y(i))T
1, 2 top singular values if 1 ⌦(k/m) and 2 < O⇤(k/m log n) then
z top singular vector if z is not within distance 1/ log n of vectors in L even with sign flip then
L L [ {z} end
end end",4. An Initialization Algorithm,[0],[0]
"Output: A0 ProjB(Ã) where Ã is the matrix whose columns in L and B = {A : kAk  2kA⇤k}
which we will reuse extensively for proving Theorem 5.
",4. An Initialization Algorithm,[0],[0]
"First, consider two fully observed data samples u = A⇤↵ and v = A⇤↵0 drawn from the hold-out set.",4. An Initialization Algorithm,[0],[0]
"(Here, A⇤,↵,↵0 are unknown.)",4. An Initialization Algorithm,[0],[0]
Consider also a partially observed sample y = A⇤ •x ⇤ under a random subset ✓,4. An Initialization Algorithm,[0],[0]
[n].,4. An Initialization Algorithm,[0],[0]
"Define:
= 1
⇢ A
⇤T •u, and
0 = 1
⇢ A
⇤T •v
respectively as (crude) estimates of ↵ and ↵0, simply obtained by applying a (scaled) adjoint of A • to u and v respectively.",4. An Initialization Algorithm,[0],[0]
"It follows from the above definition that:
= 1
⇢",4. An Initialization Algorithm,[0],[0]
"A
⇤T •A ⇤ ↵, and hy, ui = ⇢h , x⇤i.
",4. An Initialization Algorithm,[0],[0]
"Our main claim is that since A⇤ is assumed to satisfy the democracy property, 1
⇢ A ⇤T •A ⇤ resembles the identity, and hence “looks” like the true code vector ↵.",4. An Initialization Algorithm,[0],[0]
"In particular, we have the following lemma.
",4. An Initialization Algorithm,[0],[0]
Lemma 2.,4. An Initialization Algorithm,[0],[0]
"With high probability over the randomness in u and , we have: (a) | i ↵i|  µk lognpn + q 1 ⇢ ⇢n1/2 for each i = 1, 2, . . .",4. An Initialization Algorithm,[0],[0]
",m and (b) k k  p k logn ⇢ .
",4. An Initialization Algorithm,[0],[0]
Proof.,4. An Initialization Algorithm,[0],[0]
"Denote U = supp(↵) and W = U\{i}, then
| i ↵i| = 1
⇢",4. An Initialization Algorithm,[0],[0]
"A
⇤T ,iA ⇤ •W↵W + 1 ⇢ hA⇤ ,i, A⇤•ii 1 ↵i
 1 ⇢ A⇤T ,iA⇤•W↵W + ( 1 ⇢ A ⇤T ,iA ⇤ •i 1)↵i .
(4)
",4. An Initialization Algorithm,[0],[0]
We will bound these terms on the right hand side of (4) using the properties of A⇤ and ↵.,4. An Initialization Algorithm,[0],[0]
"First, we notice that for any ⇢ [n]:
kA⇤T ,iA⇤•W k 2 =
X j2W hA⇤ ,i, A⇤•ji 2  µ 2 n X j2W kA⇤ ,ik 2kA⇤ ,jk 2 ,
where we have used the democracy of A⇤ with respect to .",4. An Initialization Algorithm,[0],[0]
"Moreover, using the Chernoff bound for kA⇤ ,ik
2 = P
n i=1",4. An Initialization Algorithm,[0],[0]
"A ⇤2 li 1[l 2 ], we have kA⇤ ,ik 2  ⇢ + o(⇢) w.h.p.",4. An Initialization Algorithm,[0],[0]
"Hence, kA⇤T ,iA⇤•W k
2  ⇢2µ2k/n with high probability.",4. An Initialization Algorithm,[0],[0]
"In addition, k↵W k  p k log n w.h.p.",4. An Initialization Algorithm,[0],[0]
because ↵W is ksparse sub-Gaussian.,4. An Initialization Algorithm,[0],[0]
"Therefore, the first term in (4) gives 1 ⇢ |A⇤T ,iA⇤•W↵W |  µk lognp n with high probability.
",4. An Initialization Algorithm,[0],[0]
"For the second term in (4), consider a random variable T = ( 1
⇢ A ⇤T ,iA ⇤ •i 1)↵i over and ↵i. We first observe for
any vector w 2 Rn that:
E[(wT w)2] = nX
i=1
E[w4 i 1i2 ]",4. An Initialization Algorithm,[0],[0]
"+
nX i 6=j E[w2",4. An Initialization Algorithm,[0],[0]
"i w 2 j 1i,j2 ]
= ⇢(1 ⇢) nX
i=1
",4. An Initialization Algorithm,[0],[0]
w 4,4. An Initialization Algorithm,[0],[0]
"i + ⇢2.
",4. An Initialization Algorithm,[0],[0]
"Hence, T has mean 0 and variance 2 T
= (1 ⇢)/⇢",4. An Initialization Algorithm,[0],[0]
"P n
j=1 A 4 ji , which is bounded by O( 1 ⇢ ⇢n )",4. An Initialization Algorithm,[0],[0]
because kA⇤kmax  O(1/ p n),4. An Initialization Algorithm,[0],[0]
.,4. An Initialization Algorithm,[0],[0]
"By Chebyshev’s inequality, we
have |T |  q
1 ⇢ ⇢n1/2
with failure probability 1/ p n. Com-
bining everything, we get
| i ↵i|  µk log np
n + s 1 ⇢ ⇢n1/2 ,
w.h.p., which is the first part of the claim.
",4. An Initialization Algorithm,[0],[0]
"For the second part, we bound k k by expanding it as:
k k = 1 ⇢ kA⇤T •A⇤•U↵Uk  1 ⇢ kA⇤ •kkA⇤•Ukk↵Uk,
and again, if we use k↵Uk  p k log n w.h.p.and kA⇤k 
O(1), then k k  p k log n/⇢.
",4. An Initialization Algorithm,[0],[0]
We briefly compare the above result with that of Arora et al. (2015).,4. An Initialization Algorithm,[0],[0]
"Our upper bounds are more general, and are stated in terms of the incompleteness factor ⇢.",4. An Initialization Algorithm,[0],[0]
"Indeed,
and reconstruction error in sample size and sampling probability.
",4. An Initialization Algorithm,[0],[0]
our results match the previous bounds when ⇢ = 1.,4. An Initialization Algorithm,[0],[0]
The above lemma suggests the following interesting regime of parameters.,4. An Initialization Algorithm,[0],[0]
"Specifically, for µ = O⇤ p n
k",4. An Initialization Algorithm,[0],[0]
"log3 n
and 1 ⇢ 1 
k  ",4. An Initialization Algorithm,[0],[0]
"O⇤( ⇢ p n
logn ), one can see that | i ↵i|  O ⇤(1/ log2 n)
w.h.p., which implies that is a good estimate of ↵ even when a subset of rows in A⇤ is given.
",4. An Initialization Algorithm,[0],[0]
"In the next lemma, we show that that the pairwise correlation of u and any sample y is sufficiently informative for the same re-weighted spectral estimation strategy of Arora et al. (2015) to succeed in the incomplete setting.
",4. An Initialization Algorithm,[0],[0]
Lemma 3.,4. An Initialization Algorithm,[0],[0]
"Suppose that u, v are a pair of fully observed samples and y is an incomplete sample independent of u, v.
The weighted covariance matrix Mu,v has the form:
Mu,v , 1
⇢4 Ey[hy, uihy, viyyT ]
= X
i2U\V qici i
0",4. An Initialization Algorithm,[0],[0]
i A ⇤ •iA,4. An Initialization Algorithm,[0],[0]
⇤T •i,4. An Initialization Algorithm,[0],[0]
"+O ⇤(k/m log n),
where ci = E[x⇤4i |i 2 S] and qi = P[i 2 S].
",4. An Initialization Algorithm,[0],[0]
The complete proof is relegated to Appendix B. We will instead discuss some implications of this Lemma.,4. An Initialization Algorithm,[0],[0]
Recall that ci is a constant with 0,4. An Initialization Algorithm,[0],[0]
"< c < 1 and qi = ⇥(k/m).
",4. An Initialization Algorithm,[0],[0]
"Suppose, for a moment, that the sparse representations of u and v share exactly one common dictionary element, say A
⇤ •i (i.e., if U = supp(u) and V = supp(v) then U \ V = {i}.)",4. An Initialization Algorithm,[0],[0]
"The first term, qici i 0iA⇤•iA⇤T•i , has norm |qici i 0i|.",4. An Initialization Algorithm,[0],[0]
"From Claim 2, | i| |↵i| | i ↵i| C o(1).",4. An Initialization Algorithm,[0],[0]
"Therefore, qici i 0",4. An Initialization Algorithm,[0],[0]
i A ⇤ •iA,4. An Initialization Algorithm,[0],[0]
⇤T •i has norm at least ⌦(k/m) whereas the perturbation terms are at most O⇤(k/m log n).,4. An Initialization Algorithm,[0],[0]
"According to Wedin’s theorem, we conclude that the top singular vector of Mu,v must be O⇤(k/m log n)/⌦(k/m) =",4. An Initialization Algorithm,[0],[0]
O⇤(1/ log n) -close to A⇤•i.,4. An Initialization Algorithm,[0],[0]
"This gives us a coarse estimate of A⇤•i.
",4. An Initialization Algorithm,[0],[0]
"The question remains when and how whether we can a priori certify whether u, v share a unique dictionary atom among their sparse representations.",4. An Initialization Algorithm,[0],[0]
"Fortunately, the following Lemma provides a simple test for this via examining the decay of the singular vectors of the cross-covariance matrix Mu,v. The proof follows directly from that of Lemma 37 in (Arora et al., 2015).
",4. An Initialization Algorithm,[0],[0]
Lemma 4.,4. An Initialization Algorithm,[0],[0]
"When the top singular value of Mu,v is at least ⌦(k/m) and the second largest one is at most O
⇤(k/m log n), then u and v share a unique dictionary element with high probability.
",4. An Initialization Algorithm,[0],[0]
The above discussion isolates one of the columns of A⇤.,4. An Initialization Algorithm,[0],[0]
We can repeat this procedure several times by randomly choosing pairs of samples u and v from the hold-out set.,4. An Initialization Algorithm,[0],[0]
"Using the result of Arora et al. (2015), if |P1| is p1 = eO(m), then we can estimate all the m dictionary atoms.",4. An Initialization Algorithm,[0],[0]
"Overall, the sample complexity of Algorithm 2 is dominated by p2 = eO(mk/⇢4).",4. An Initialization Algorithm,[0],[0]
We corroborate our theory by demonstrating some representative numerical benefits of our proposed algorithms.,5. Experiments,[0],[0]
We generate a synthetic dataset based on the generative model described in Section 2.,5. Experiments,[0],[0]
The ground truth dictionary A⇤ is of size 256⇥ 256 with independent standard Gaussian entries.,5. Experiments,[0],[0]
We normalize columns of A⇤ to be unit norm.,5. Experiments,[0],[0]
"Then, we generate 6-sparse code vectors x⇤ with support drawn uniformly at random.",5. Experiments,[0],[0]
Entries in the support are sampled from ±1 with equal probability.,5. Experiments,[0],[0]
"We generate all full samples, and isolate 5000 samples as “side information” for the initialization step.",5. Experiments,[0],[0]
"The remaining are then subsampled with different parameters ⇢.
",5. Experiments,[0],[0]
We set the number of iterations to T = 3000 in the initialization procedure and the number of descent steps T = 50 for the descent scheme.,5. Experiments,[0],[0]
"Besides, we slightly modify the thresholding operator in the encoding step of Algorithm 1.",5. Experiments,[0],[0]
We use another operator that keeps k largest entries of the input untouched and sets everything else to zero due to its stability.,5. Experiments,[0],[0]
"For each Monte Carlo trial, we uniformly draw p partial samples.",5. Experiments,[0],[0]
"The task, for our algorithm, is to learn A⇤.",5. Experiments,[0],[0]
"An implementation of our method is available online3.
",5. Experiments,[0],[0]
"We evaluate our algorithm on two metrics against p and ⇢: (i) recovery rate, i.e., the fraction of trials in which each algorithm successfully recovers the ground truth A⇤; and (ii) reconstruction error.",5. Experiments,[0],[0]
All the metrics are averaged over 50 Monte Carlo simulations.,5. Experiments,[0],[0]
“Successful recovery” is defined according to a threshold ⌧ = 6 on the Frobenius norm of the difference between the estimate bA and the ground truth A⇤.,5. Experiments,[0],[0]
"(Since we can only estimate bA modulo a permutation and sign flip, the optimal column and sign matching is computed using the Hungarian algorithm.)
",5. Experiments,[0],[0]
Figure 1 shows our experimental results.,5. Experiments,[0],[0]
"Here, sample size refers to the number of incomplete samples.",5. Experiments,[0],[0]
"Our algorithms are able to recover the dictionary for ⇢ = 0.6, 0.8, 1.0.",5. Experiments,[0],[0]
"For ⇢ = 0.4, we can observe a “phase transition” in sample complexity of successful recovery around p = 10, 000 samples.
",5. Experiments,[0],[0]
"3https://github.com/thanh-isu
Acknowledgements The authors thank the anonymous reviewers for many insightful comments and suggestions during the review process.",5. Experiments,[0],[0]
"This work was supported in part by the National Science Foundation under grants CCF-1566281 and CCF-1750920, and in part by a Faculty Fellowship from the Black and Veatch Foundation.",5. Experiments,[0],[0]
Existing algorithms for dictionary learning assume that the entries of the (high-dimensional) input data are fully observed.,abstractText,[0],[0]
"However, in several practical applications, only an incomplete fraction of the data entries may be available.",abstractText,[0],[0]
"For incomplete settings, no provably correct and polynomialtime algorithm has been reported in the dictionary learning literature.",abstractText,[0],[0]
"In this paper, we provide provable approaches for learning – from incomplete samples – a family of dictionaries whose atoms have sufficiently “spread-out” mass.",abstractText,[0],[0]
"First, we propose a descent-style iterative algorithm that linearly converges to the true dictionary when provided a sufficiently coarse initial estimate.",abstractText,[0],[0]
"Second, we propose an initialization algorithm that utilizes a small number of extra fully observed samples to produce such a coarse initial estimate.",abstractText,[0],[0]
"Finally, we theoretically analyze their performance and provide asymptotic statistical and computational guarantees.",abstractText,[0],[0]
On Learning Sparsely Used Dictionaries from Incomplete Samples,title,[0],[0]
"The depth of deep neural networks confers representational power, but also makes model optimization more challenging.",1. Introduction,[0],[0]
"Training deep networks with gradient descent based methods is known to be difficult as a consequence of the vanishing and exploding gradient problem (Hochreiter & Schmidhuber, 1997).",1. Introduction,[0],[0]
"Typically, exploding gradients are avoided by clipping large gradients (Pascanu et al., 2013) or introducing an L2 or L1 weight norm penalty.",1. Introduction,[0],[0]
"The latter has the effect of bounding the spectral radius of the linear transformations, thus limiting the maximal gain across the transformation.",1. Introduction,[0],[0]
"Krueger & Memisevic (2015) attempt to
1École Polytechnique de Montréal, Montréal, Canada 2Montreal Institute for Learning Algorithms, Montréal, Canada 3CHUM Research Center, Montréal, Canada.",1. Introduction,[0],[0]
"Correspondence to: Eugene Vorontsov <eugene.vorontsov@gmail.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
stabilize the norm of propagating signals directly by penalizing differences in successive norm pairs in the forward pass and Pascanu et al. (2013) propose to penalize successive gradient norm pairs in the backward pass.,1. Introduction,[0],[0]
"These regularizers affect the network parameterization with respect to the data instead of penalizing weights directly.
",1. Introduction,[0],[0]
Both expansivity and contractivity of linear transformations can also be limited by more tightly bounding their spectra.,1. Introduction,[0],[0]
"By limiting the transformations to be orthogonal, their singular spectra are limited to unitary gain causing the transformations to be norm-preserving.",1. Introduction,[0],[0]
Le et al. (2015) and Henaff et al. (2016) have respectively shown that identity initialization and orthogonal initialization can be beneficial.,1. Introduction,[0],[0]
"Arjovsky et al. (2015) have gone beyond initialization, building unitary recurrent neural network (RNN) models with transformations that are unitary by construction which they achieved by composing multiple basic unitary transformations.",1. Introduction,[0],[0]
"The resulting transformations, for some n-dimensional input, cover only some subset of possible n × n unitary matrices but appear to perform well on simple tasks and have the benefit of having low complexity in memory and computation.
",1. Introduction,[0],[0]
The entire set of possible unitary or orthogonal parameterizations forms the Stiefel manifold.,1. Introduction,[0],[0]
"At a much higher computational cost, gradient descent optimization directly along this manifold can be done via geodesic steps (Nishimori, 2005; Tagare, 2011).",1. Introduction,[0],[0]
"Recent work (Wisdom et al., 2016) has proposed the optimization of unitary matrices along the Stiefel manifold using geodesic gradient descent.",1. Introduction,[0],[0]
"To produce a full-capacity parameterization for unitary matrices they use some insights from Tagare (2011), combining the use of canonical inner products and Cayley transformations.",1. Introduction,[0],[0]
Their experimental work indicates that full capacity unitary RNN models can solve the copy memory problem whereas both LSTM networks and restricted capacity unitary RNN models having similar complexity appear unable to solve the task for a longer sequence length (T = 2000).,1. Introduction,[0],[0]
"(Harandi & Fernando, 2016) also find that the use of fully connected “Stiefel layers” improves the performance of some convolutional neural networks.
",1. Introduction,[0],[0]
"We seek to gain a new perspective on this line of research by exploring the optimization of real valued matrices within a configurable margin about the Stiefel mani-
fold.",1. Introduction,[0],[0]
"We suspect that a strong constraint of orthogonality limits the model’s representational power, hindering its performance, and may make optimization more difficult.",1. Introduction,[0],[0]
We explore this hypothesis empirically by employing a factorization technique that allows us to limit the degree of deviation from the Stiefel manifold.,1. Introduction,[0],[0]
"While we use geodesic gradient descent, we simultaneously update the singular spectra of our matrices along Euclidean steps, allowing optimization to step away from the manifold while still curving about it.",1. Introduction,[0],[0]
"The issue of vanishing and exploding gradients as it pertains to the parameterization of neural networks can be illuminated by looking at the gradient back-propagation chain through a network.
",1.1. Vanishing and Exploding Gradients,[0],[0]
"A neural network with n hidden layers has pre-activations
ai(hi−1) =",1.1. Vanishing and Exploding Gradients,[0],[0]
"Wi hi−1 + bi, i ∈ {2, · · · , n} (1)
",1.1. Vanishing and Exploding Gradients,[0],[0]
"For notational convenience, we combine parameters Wi and bi to form an affine matrix θ.",1.1. Vanishing and Exploding Gradients,[0],[0]
"We can see that for some loss function L at layer n , the derivative with respect to parameters θi is:
∂L ∂θi = ∂an+1 ∂θi ∂L ∂an+1",1.1. Vanishing and Exploding Gradients,[0],[0]
"(2)
The partial derivatives for the pre-activations can be decomposed as follows:
∂ai+1 ∂θi = ∂ai ∂θi ∂hi ∂ai ∂ai+1 ∂hi
= ∂ai ∂θi DiWi+1 → ∂ai+1 ∂ai = DiWi+1,
(3)
where Di is the Jacobian corresponding to the activation function, containing partial derivatives of the hidden units at layer i + 1 with respect to the pre-activation inputs.",1.1. Vanishing and Exploding Gradients,[0],[0]
"Typically, D is diagonal.",1.1. Vanishing and Exploding Gradients,[0],[0]
"Following the above, the gradient in equation 2 can be fully decomposed into a recursive chain of matrix products:
∂L ∂θi = ∂ai",1.1. Vanishing and Exploding Gradients,[0],[0]
"∂θi n∏ j=i (DjWj+1) ∂L ∂an+1 (4)
",1.1. Vanishing and Exploding Gradients,[0],[0]
"In (Pascanu et al., 2013), it is shown that the 2-norm of ∂ai+1 ∂ai is bounded by the product of the norms of the nonlinearity’s Jacobian and transition matrix at time t (layer i ), as follows:∣∣∣∣∣∣∣∣∂at+1∂at
∣∣∣∣∣∣∣∣ ≤",1.1. Vanishing and Exploding Gradients,[0],[0]
||Dt|| ||Wt|| ≤,1.1. Vanishing and Exploding Gradients,[0],[0]
"λDt λWt = ηt, λDt , λWt ∈ R. (5)
where λDt and λWt are the largest singular values of the non-linearity’s Jacobian Dt and the transition matrix Wt .",1.1. Vanishing and Exploding Gradients,[0],[0]
"In RNNs, Wt is shared across time and can be simply denoted as W.
Equation 5 shows that the gradient can grow or shrink at each layer depending on the gain of each layer’s linear transformation W and the gain of the Jacobian D. The gain caused by each layer is magnified across all time steps or layers.",1.1. Vanishing and Exploding Gradients,[0],[0]
It is easy to have extreme amplification in a recurrent neural network where W is shared across time steps and a non-unitary gain in W is amplified exponentially.,1.1. Vanishing and Exploding Gradients,[0],[0]
"The phenomena of extreme growth or contraction of the gradient across time steps or layers are known as the exploding and the vanishing gradient problems, respectively.",1.1. Vanishing and Exploding Gradients,[0],[0]
"It is sufficient for RNNs to have ηt ≤ 1 at each time t to enable the possibility of vanishing gradients, typically for some large number of time steps T .",1.1. Vanishing and Exploding Gradients,[0],[0]
The rate at which a gradient (or forward signal) vanishes depends on both the parameterization of the model and on the input data.,1.1. Vanishing and Exploding Gradients,[0],[0]
"The parameterization may be conditioned by placing appropriate constraints on W. It is worth keeping in mind that the Jacobian D is typically contractive, thus tending to be norm-reducing) and is also data-dependent, whereas W can vary from being contractive to norm-preserving, to expansive and applies the same gain on the forward signal as on the back-propagated gradient signal.",1.1. Vanishing and Exploding Gradients,[0],[0]
"Vanishing and exploding gradients can be controlled to a large extent by controlling the maximum and minimum gain of W. The maximum gain of a matrix W is given by the spectral norm which is given by
||W||2 = max [ ||Wx|| ||x|| ] .",2. Our Approach,[0],[0]
"(6)
By keeping our weight matrix W close to orthogonal, one can ensure that it is close to a norm-preserving transformation (where the spectral norm is equal to one, but the minimum gain is also one).",2. Our Approach,[0],[0]
"One way to achieve this is via a simple soft constraint or regularization term of the form:
λ ∑ i ||WTi",2. Our Approach,[0],[0]
Wi − I||2.,2. Our Approach,[0],[0]
"(7)
However, it is possible to formulate a more direct parameterization or factorization for W which permits hard bounds on the amount of expansion and contraction induced by W. This can be achieved by simply parameterizing W according to its singular value decomposition, which consists of the composition of orthogonal basis matrices U and V with a diagonal spectral matrix S containing the singular values which are real and positive by defi-
nition.",2. Our Approach,[0],[0]
We have W = USVT .,2. Our Approach,[0],[0]
"(8)
Since the spectral norm or maximum gain of a matrix is equal to its largest singular value, this decomposition allows us to control the maximum gain or expansivity of the weight matrix by controlling the magnitude of the largest singular value.",2. Our Approach,[0],[0]
"Similarly, the minimum gain or contractivity of a matrix can be obtained from the minimum singular value.
",2. Our Approach,[0],[0]
We can keep the bases U and V orthogonal via geodesic gradient descent along the set of weights that satisfy UTU = I and VTV,2. Our Approach,[0],[0]
= I respectively.,2. Our Approach,[0],[0]
The submanifolds that satisfy these constraints are called Stiefel manifolds.,2. Our Approach,[0],[0]
"We discuss how this is achieved in more detail below, then discuss our construction for bounding the singular values.
",2. Our Approach,[0],[0]
"During optimization, in order to maintain the orthogonality of an orthogonally-initialized matrix M, i.e. where M = U, M = V or M = W if so desired, we employ a Cayley transformation of the update step onto the Stiefel manifold of (semi-)orthogonal matrices, as in Nishimori (2005) and Tagare (2011).",2. Our Approach,[0],[0]
"Given an orthogonally-initialized parameter matrix M and its Jacobian, G with respect to the objective function, an update is performed as follows:
A = GMT −MGT
Mnew = M+ (I+ η 2 A)−1(I− η 2 A),
(9)
where A is a skew-symmetric matrix (that depends on the Jacobian and on the parameter matrix) which is mapped to an orthogonal matrix via a Cayley transform and η is the learning rate.
",2. Our Approach,[0],[0]
"While the update rule in (9) allows us to maintain an orthogonal hidden to hidden transition matrix W if desired, we are interested in exploring the effect of stepping away from the Stiefel manifold.",2. Our Approach,[0],[0]
"As such, we parameterize the transition matrix W in factorized form, as a singular value decomposition with orthogonal bases U and V updated by geodesic gradient descent using the Cayley transform approach above.
",2. Our Approach,[0],[0]
"If W is an orthogonal matrix, the singular values in the diagonal matrix S are all equal to one.",2. Our Approach,[0],[0]
"However, in our formulation we allow these singular values to deviate from one and employ a sigmoidal parameterization to apply a hard constraint on the maximum and minimum amount of deviation.",2. Our Approach,[0],[0]
"Specifically, we define a margin m around 1 within which the singular values must lie.",2. Our Approach,[0],[0]
"This is achieved with the parameterization
si = 2m(σ(pi)− 0.5)",2. Our Approach,[0],[0]
"+ 1, si ∈ {diag(S)}, m ∈",2. Our Approach,[0],[0]
"[0, 1].",2. Our Approach,[0],[0]
"(10) The singular values are thus restricted to the range [1−m, 1 +m] and the underlying parameters pi are updated freely via stochastic gradient descent.",2. Our Approach,[0],[0]
"Note that this
parameterization strategy also has implications on the step sizes that gradient descent based optimization will take when updating the singular values – they tend to be smaller compared to models with no margin constraining their values.",2. Our Approach,[0],[0]
"Specifically, a singular value’s progression toward a margin is slowed the closer it is to the margin.",2. Our Approach,[0],[0]
The sigmoidal parameterization can also impart another effect on the step size along the spectrum which needs to be accounted for.,2. Our Approach,[0],[0]
"Considering 10, the gradient backpropagation of some loss L toward parameters pi is found as
dL dpi = dsi",2. Our Approach,[0],[0]
dpi dL dsi,2. Our Approach,[0],[0]
= 2m dσ(pi),2. Our Approach,[0],[0]
dpi dL dsi .,2. Our Approach,[0],[0]
"(11)
From (11), it can be seen that the magnitude of the update step for pi is scaled by the margin hyperparameter m .",2. Our Approach,[0],[0]
"This means for example that for margins less than one, the effective learning rate for the spectrum is reduced in proportion to the margin.",2. Our Approach,[0],[0]
"Consequently, we adjust the learning rate along the spectrum to be independent of the margin by renormalizing it by 2m .
",2. Our Approach,[0],[0]
This margin formulation both guarantees singular values lie within a well defined range and slows deviation from orthogonality.,2. Our Approach,[0],[0]
"Alternatively, one could enforce the orthogonality of U and V and impose a regularization term corresponding to a mean one Gaussian prior on these singular values.",2. Our Approach,[0],[0]
This encourages the weight matrix W to be norm preserving with a controllable strength equivalent to the variance of the Gaussian.,2. Our Approach,[0],[0]
We also explore this approach further below.,2. Our Approach,[0],[0]
"In this section, we explore hard and soft orthogonality constraints on factorized weight matrices for recurrent neural network hidden to hidden transitions.",3. Experiments,[0],[0]
"With hard orthogonality constraints on U and V, we investigate the effect of widening the spectral margin or bounds on convergence and performance.",3. Experiments,[0],[0]
Loosening these bounds allows increasingly larger margins within which the transition matrix W can deviate from orthogonality.,3. Experiments,[0],[0]
"We confirm that orthogonal initialization is useful as noted in Henaff et al. (2016), and we show that although strict orthogonality guarantees stable gradient norm, loosening orthogonality constraints can increase the rate of gradient descent convergence.",3. Experiments,[0],[0]
"We begin our analyses on tasks that are designed to stress memory: a sequence copying task and a basic addition task (Hochreiter & Schmidhuber, 1997).",3. Experiments,[0],[0]
"We then move on to tasks on real data that require models to capture long-range dependencies: digit classification based on sequential and permuted MNIST vectors (Le et al., 2015; LeCun et al., 1998).",3. Experiments,[0],[0]
"Finally, we look at a basic language modeling task using the Penn Treebank dataset (Marcus et al., 1993).
",3. Experiments,[0],[0]
"The copy and adding tasks, introduced by Hochreiter &
Schmidhuber (1997), are synthetic benchmarks with pathologically hard long distance dependencies that require long-term memory in models.",3. Experiments,[0],[0]
"The copy task consists of an input sequence that must be remembered by the network, followed by a series of blank inputs terminated by a delimiter that denotes the point at which the network must begin to output a copy of the initial sequence.",3. Experiments,[0],[0]
"We use an input sequence of T + 20 elements that begins with a subsequence of 10 elements to copy, each containing a symbol ai ∈",3. Experiments,[0],[0]
"{a1 , ..., ap} out of p = 8 possible symbols.",3. Experiments,[0],[0]
This sub-sequence is followed by T − 1 elements of the blank category a0 which is terminated at step T by a delimiter symbol ap+1 and 10 more elements of the blank category.,3. Experiments,[0],[0]
"The network must learn to remember the initial 10 element sequence for T time steps and output it after receiving the delimiter symbol.
",3. Experiments,[0],[0]
The goal of the adding task is to add two numbers together after a long delay.,3. Experiments,[0],[0]
Each number is randomly picked at a unique position in a sequence of length T .,3. Experiments,[0],[0]
The sequence is composed of T values sampled from a uniform distribution in the range,3. Experiments,[0],[0]
"[0, 1), with each value paired with an indicator value that identifies the value as one of the two numbers to remember (marked 1) or as a value to ignore (marked 0).",3. Experiments,[0],[0]
"The two numbers are positioned randomly in the sequence, the first in the range",3. Experiments,[0],[0]
"[0, T2 − 1] and the second in the range",3. Experiments,[0],[0]
"[T2 , T",3. Experiments,[0],[0]
"− 1], where 0 marks the first element.",3. Experiments,[0],[0]
"The network must learn to identify and remember the two numbers and output their sum.
",3. Experiments,[0],[0]
"In the sequential MNIST task from Le et al. (2015), MNIST digits are flattened into vectors that can be traversed sequentially by a recurrent neural network.",3. Experiments,[0],[0]
The goal is to classify the digit based on the sequential input of pixels.,3. Experiments,[0],[0]
The simple variant of this task is with a simple flattening of the image matrices; the harder variant of this task includes a random permutation of the pixels in the input vector that is determined once for an experiment.,3. Experiments,[0],[0]
"The latter formulation introduces longer distance dependencies between pixels that must be interpreted by the classification model.
",3. Experiments,[0],[0]
"The English Penn Treebank (PTB) dataset from Marcus et al. (1993) is an annotated corpus of English sentences, commonly used for benchmarking language models.",3. Experiments,[0],[0]
"We employ a sequential character prediction task: given a sentence, a recurrent neural network must predict the next character at each step, from left to right.",3. Experiments,[0],[0]
"We use input sequences of variable length, with each sequence containing one sentence.",3. Experiments,[0],[0]
"We model 49 characters including lowercase letters (all strings are in lowercase), numbers, common punctuation, and an unknown character placeholder.",3. Experiments,[0],[0]
"We use two subsets of the data in our experiments: in the first, we first use 23% of the data with strings with up to 75 characters and in the second we include over 99% of the dataset, picking strings with up to 300 characters.",3. Experiments,[0],[0]
"In this section, we experimentally explore the effect of loosening hard orthogonality constraints through loosening the spectral margin defined above for the hidden to hidden transition matrix.
",3.1. Loosening Hard Orthogonality Constraints,[0],[0]
"In all experiments, we employed RMSprop (Tieleman & Hinton, 2012) when not using geodesic gradient descent.",3.1. Loosening Hard Orthogonality Constraints,[0],[0]
"We used minibatches of size 50 and for generated data (the copy and adding tasks), we assumed an epoch length of 100 minibatches.",3.1. Loosening Hard Orthogonality Constraints,[0],[0]
We cautiously introduced gradient clipping at magnitude 100 (unless stated otherwise) in all of our RNN experiments although it may not be required and we consistently applied a small weight decay of 0.0001.,3.1. Loosening Hard Orthogonality Constraints,[0],[0]
"Unless otherwise specified, we trained all simple recurrent neural networks with the hidden to hidden matrix factorization as in (8) using geodesic gradient descent on the bases (learning rate 10−6) and RMSprop on the other parameters (learning rate 0.0001), using a tanh transition nonlinearity, and clipping gradients of 100 magnitude.",3.1. Loosening Hard Orthogonality Constraints,[0],[0]
"The neural network code was built on the Theano framework (Theano Development Team, 2016).",3.1. Loosening Hard Orthogonality Constraints,[0],[0]
"When parameterizing a matrix in factorized form, we apply the weight decay on the composite matrix rather than on the factors in order to be consistent across experiments.",3.1. Loosening Hard Orthogonality Constraints,[0],[0]
"For MNIST and PTB, hyperparameter selection and early stopping were performed targeting the best validation set accuracy, with results reported on the test set.",3.1. Loosening Hard Orthogonality Constraints,[0],[0]
"For different sequence lengths T of the copy and adding tasks, we trained a factorized RNN with 128 hidden units and various spectral margins m .",3.1.1. CONVERGENCE ON SYNTHETIC MEMORY TASKS,[0],[0]
"For the copy task, we used Elman networks without a transition non-linearity as in Henaff et al. (2016).",3.1.1. CONVERGENCE ON SYNTHETIC MEMORY TASKS,[0],[0]
"We also investigated the use of nonlinearities, as discussed below.
",3.1.1. CONVERGENCE ON SYNTHETIC MEMORY TASKS,[0],[0]
As shown in Figure 1 we see an increase in the rate of convergence as we increase the spectral margin.,3.1.1. CONVERGENCE ON SYNTHETIC MEMORY TASKS,[0],[0]
"This observation generally holds across the tested sequence lengths (T = 200, T = 500, T = 1000, T = 10000); however, large spectral margins hinder convergence on extremely long sequence lengths.",3.1.1. CONVERGENCE ON SYNTHETIC MEMORY TASKS,[0],[0]
"At sequence length T = 10000, parameterizations with spectral margins larger than 0.001 converge slower than when using a margin of 0.001.",3.1.1. CONVERGENCE ON SYNTHETIC MEMORY TASKS,[0],[0]
"In addition, the experiment without a margin failed to converge on the longest sequence length.",3.1.1. CONVERGENCE ON SYNTHETIC MEMORY TASKS,[0],[0]
"This follows the expected pattern where stepping away from the Stiefel manifold may help with gradient descent optimization but loosening orthogonality constraints can reduce the stability of signal propagation through the network.
",3.1.1. CONVERGENCE ON SYNTHETIC MEMORY TASKS,[0],[0]
"For the adding task, we trained a factorized RNN on T = 1000 length sequences, using a ReLU activation function on the hidden to hidden transition matrix.",3.1.1. CONVERGENCE ON SYNTHETIC MEMORY TASKS,[0],[0]
"The mean
squared error (MSE) is shown for different spectral margins in Figure 2.",3.1.1. CONVERGENCE ON SYNTHETIC MEMORY TASKS,[0],[0]
"Testing spectral margins m = 0, m = 1, m = 10, m = 100, and no margin, we find that the models with the purely orthogonal (m = 0) and the unconstrained (no margin) transition matrices failed to begin converging beyond baseline MSE within 2000 epochs.
",3.1.1. CONVERGENCE ON SYNTHETIC MEMORY TASKS,[0],[0]
"We found that nonlinearities such as a rectified linear unit (ReLU) (Nair & Hinton, 2010) or hyperbolic tangent (tanh) made the copy task far more difficult to solve.",3.1.1. CONVERGENCE ON SYNTHETIC MEMORY TASKS,[0],[0]
"Using tanh, a short sequence length (T = 100) copy task required both a soft constraint that encourages orthogonality and thousands of epochs for training.",3.1.1. CONVERGENCE ON SYNTHETIC MEMORY TASKS,[0],[0]
"It is worth noting that in the unitary evolution recurrent neural network of Arjovsky et al. (2015), the non-linearity (referred to as the ”modReLU”) is actually initialized as an identity operation that is free to deviate from identity during training.",3.1.1. CONVERGENCE ON SYNTHETIC MEMORY TASKS,[0],[0]
"Furthermore, Henaff et al. (2016) derive a solution mechanism for the copy task that drops the non-linearity from an RNN.",3.1.1. CONVERGENCE ON SYNTHETIC MEMORY TASKS,[0],[0]
"To explore this further, we experimented with a parametric leaky ReLU activation function (PReLU) which introduces a trainable slope α for negative valued inputs x , producing f (x ) = max(x , 0) + αmin(x , 0) (He et al., 2015).",3.1.1. CONVERGENCE ON SYNTHETIC MEMORY TASKS,[0],[0]
Setting the slope α to one would make the PReLU equivalent to an identity function.,3.1.1. CONVERGENCE ON SYNTHETIC MEMORY TASKS,[0],[0]
"We experimented with clamping α to 0.5, 0.7 or 1 in a factorized RNN with a spectral margin of 0.3 and found that only the model with α = 1 solved the T = 1000 length copy task.",3.1.1. CONVERGENCE ON SYNTHETIC MEMORY TASKS,[0],[0]
"We also experimented with a trainable slope α, initialized to 0.7 and found that it converges to 0.96, further suggesting the optimal solution for
the copy task is without a transition nonlinearity.",3.1.1. CONVERGENCE ON SYNTHETIC MEMORY TASKS,[0],[0]
"Since the copy task is purely a memory task, one may imagine that a transition nonlinearity such as a tanh or ReLU may be detrimental to the task as it can lose information.",3.1.1. CONVERGENCE ON SYNTHETIC MEMORY TASKS,[0],[0]
"Thus, we also tried a recent activation function that preserves information, called an orthogonal permutation linear unit (OPLU) (Chernodub & Nowicki, 2016).",3.1.1. CONVERGENCE ON SYNTHETIC MEMORY TASKS,[0],[0]
"The OPLU preserves norm, making a fully norm-preserving RNN possible.",3.1.1. CONVERGENCE ON SYNTHETIC MEMORY TASKS,[0],[0]
"Interestingly, this activation function allowed us to recover identical results on the copy task to those without a nonlinearity for different spectral margins.",3.1.1. CONVERGENCE ON SYNTHETIC MEMORY TASKS,[0],[0]
"Having confirmed that an orthogonality constraint can negatively impact convergence rate, we seek to investigate the effect on model performance for tasks on real data.",3.1.2. PERFORMANCE ON REAL DATA,[0],[0]
"In Table 1, we show the results of experiments on ordered and permuted sequential MNIST classification tasks and on the PTB character prediction task.
",3.1.2. PERFORMANCE ON REAL DATA,[0],[0]
"For the sequential MNIST experiments, loss curves are shown in Figure 3 and reveal an increased convergence rate for larger spectral margins.",3.1.2. PERFORMANCE ON REAL DATA,[0],[0]
We trained the factorized RNN models with 128 hidden units for 120 epochs.,3.1.2. PERFORMANCE ON REAL DATA,[0],[0]
"We also trained an LSTM with 128 hidden units (tanh activation) on both tasks for 150 epochs, configured with peephole connections, orthogonally initialized (and forget gate bias initialized to one), and trained with RMSprop (learning rate 0.0001, clipping gradients of magnitude 1).
",3.1.2. PERFORMANCE ON REAL DATA,[0],[0]
"For PTB character prediction, we evaluate results in terms of bits per character (bpc) and prediction accuracy.",3.1.2. PERFORMANCE ON REAL DATA,[0],[0]
Prediction results are shown in 1 both for a subset of short sequences (up to 75 characters; 23% of data) and for a subset of long sequences (up to 300 characters; 99% of data).,3.1.2. PERFORMANCE ON REAL DATA,[0],[0]
"We trained factorized RNN models with 512 hidden units for 200 epochs with geodesic gradient descent on the bases (learning rate 10−6) and RMSprop on the other parameters (learning rate 0.001), using a tanh transition nonlinearity, and clipping gradients of 30 magnitude.",3.1.2. PERFORMANCE ON REAL DATA,[0],[0]
"As a rough point of reference, we also trained an LSTM with 512 hidden units for each of the data subsets (configured as for MNIST).",3.1.2. PERFORMANCE ON REAL DATA,[0],[0]
"On sequences up to 75 characters, LSTM performance was
limited by early stopping of training due to overfitting.
",3.1.2. PERFORMANCE ON REAL DATA,[0],[0]
"Interestingly, for both the ordered and permuted sequential MNIST tasks, models with a non-zero margin significantly outperform those that are constrained to have purely orthogonal transition matrices (margin of zero).",3.1.2. PERFORMANCE ON REAL DATA,[0],[0]
"The best results on both the ordered and sequential MNIST tasks were yielded by models with a spectral margin of 0.1, at 94.10% accuracy and 91.44% accuracy, respectively.",3.1.2. PERFORMANCE ON REAL DATA,[0],[0]
"An LSTM outperformed the RNNs in both tasks; nevertheless, RNNs with hidden to hidden transitions initialized as orthogonal matrices performed admirably without a memory component and without all of the additional parameters associated with gates.",3.1.2. PERFORMANCE ON REAL DATA,[0],[0]
"Indeed, orthogonally initialized RNNs performed almost on par with the LSTM in the permuted sequential MNIST task which presents longer distance dependencies than the ordered task.",3.1.2. PERFORMANCE ON REAL DATA,[0],[0]
"Although the optimal margin appears to be 0.1, RNNs with large margins perform almost identically to an RNN without a margin, as long as the transition matrix is initialized as orthogonal.",3.1.2. PERFORMANCE ON REAL DATA,[0],[0]
"On these tasks, orthogonal initialization appears to significantly outperform Glorot normal initialization (Glorot &
Bengio, 2010) or initializing the matrix as identity.",3.1.2. PERFORMANCE ON REAL DATA,[0],[0]
"It is interesting to note that for the MNIST tasks, orthogonal initialization appears useful while orthogonality constraints appear mainly detrimental.",3.1.2. PERFORMANCE ON REAL DATA,[0],[0]
"This suggests that while orthogonality helps early training by stabilizing gradient flow across many time steps, orthogonality constraints may need to be loosened on some tasks so as not to over-constrain the model’s representational ability.
",3.1.2. PERFORMANCE ON REAL DATA,[0],[0]
"Curiously, larger margins and even models without sigmoidal constraints on the spectrum (no margin) performed well as long as they were initialized to be orthogonal, suggesting that evolution away from orthogonality is not a serious problem on MNIST.",3.1.2. PERFORMANCE ON REAL DATA,[0],[0]
It is not surprising that orthogonality is useful for the MNIST tasks since they depend on long distance signal propagation with a single output at the end of the input sequence.,3.1.2. PERFORMANCE ON REAL DATA,[0],[0]
"On the other hand, character prediction with PTB produces an output at every time step.",3.1.2. PERFORMANCE ON REAL DATA,[0],[0]
Constraining deviation from orthogonality proved detrimental for short sentences and beneficial when long sentences were included.,3.1.2. PERFORMANCE ON REAL DATA,[0],[0]
"Furthermore, Glorot normal initialization did not perform worse than orthogonal initialization for PTB.",3.1.2. PERFORMANCE ON REAL DATA,[0],[0]
"Since an output is generated for every character in a sentence, short distance signal propagation is possible.",3.1.2. PERFORMANCE ON REAL DATA,[0],[0]
"Thus it is possible that the RNN is first learning very local dependencies between neighbouring characters and that given enough context, constraining deviation from orthogonality can help force the network to learn longer distance dependencies.",3.1.2. PERFORMANCE ON REAL DATA,[0],[0]
It is interesting to note that even long sequence lengths (T=1000) in the copy task can be solved efficiently with rather large margins on the spectrum.,3.1.3. SPECTRAL AND GRADIENT EVOLUTION,[0],[0]
In Figure 4 we look at the gradient propagation of the loss from the last time step in the network with respect to the hidden activations.,3.1.3. SPECTRAL AND GRADIENT EVOLUTION,[0],[0]
"We can see that for a purely orthogonal parameterization of
the transition matrix (when the margin is zero), the gradient norm is preserved across time steps, as expected.",3.1.3. SPECTRAL AND GRADIENT EVOLUTION,[0],[0]
"We further observe that with increasing margin size, the number of update steps over which this norm preservation survives decreases, though surprisingly not as quickly as expected.
",3.1.3. SPECTRAL AND GRADIENT EVOLUTION,[0],[0]
"Although the deviation of singular values from one should be slowed by the sigmoidal parameterizations, even parameterizations without a sigmoid (no margin) can be effectively trained for all but the longest sequence lengths.",3.1.3. SPECTRAL AND GRADIENT EVOLUTION,[0],[0]
This suggests that the spectrum is not deviating far from orthogonality and that inputs to the hidden to hidden transitions are mostly not aligned along the dimensions of greatest expansion or contraction.,3.1.3. SPECTRAL AND GRADIENT EVOLUTION,[0],[0]
"We evaluated the spread of the spectrum in all of our experiments and found that indeed, singular values tend to stay well within their prescribed bounds and only reach the margin when using a very large learning rate that does not permit convergence.",3.1.3. SPECTRAL AND GRADIENT EVOLUTION,[0],[0]
"Furthermore, when transition matrices are initialized as orthogonal, singular values remain near one throughout training even without a sigmoidal margin for tasks that require long term memory (copy, adding, sequential MNIST).",3.1.3. SPECTRAL AND GRADIENT EVOLUTION,[0],[0]
"On the other hand, singular value distributions tend to drift away from one for PTB character prediction which may help explain why enforcing an orthogonality constraint can be helpful for this task, when modeling long sequences.",3.1.3. SPECTRAL AND GRADIENT EVOLUTION,[0],[0]
"Interestingly, singular values spread out less for longer sequence lengths (nevertheless, the T=10000 copy task could not be solved with no sigmoid on the spectrum).
",3.1.3. SPECTRAL AND GRADIENT EVOLUTION,[0],[0]
We visualize the spread of singular values for different model parameterizations on the permuted sequential MNIST task in Figure 5.,3.1.3. SPECTRAL AND GRADIENT EVOLUTION,[0],[0]
"Curiously, we find that the distribution of singular values tends to shift upward to a mean of approximately 1.05 on both the ordered and permuted sequential MNIST tasks.",3.1.3. SPECTRAL AND GRADIENT EVOLUTION,[0],[0]
"We note that in those experiments, a tanh transition nonlinearity was used which is contractive in both the forward signal pass and the gradient backward pass.",3.1.3. SPECTRAL AND GRADIENT EVOLUTION,[0],[0]
"An upward shift in the distribution of singular val-
ues of the transition matrix would help compensate for that contraction.",3.1.3. SPECTRAL AND GRADIENT EVOLUTION,[0],[0]
"Indeed, (Saxe et al., 2013) describe this as a possibly good regime for learning in deep neural networks.",3.1.3. SPECTRAL AND GRADIENT EVOLUTION,[0],[0]
That the model appears to evolve toward this regime suggests that deviating from it may incur a cost.,3.1.3. SPECTRAL AND GRADIENT EVOLUTION,[0],[0]
This is interesting because the cost function cannot take into account numerical issues such as vanishing or exploding gradients (or forward signals); we do not know what could make this deviation costly.,3.1.3. SPECTRAL AND GRADIENT EVOLUTION,[0],[0]
That the transition matrix may be compensating for the contraction of the tanh is supported by further experiments: applying a 1.05 pre-activation gain appears to allow a model with a margin of 0 to nearly match the top performance reached on both of the MNIST tasks.,3.1.3. SPECTRAL AND GRADIENT EVOLUTION,[0],[0]
"Furthermore, when using the OPLU norm-preserving activation function (Chernodub & Nowicki, 2016), we found that orthogonally initialized models performed equally well with all margins, achieving over 90% accuracy on the permuted sequential MNIST task.",3.1.3. SPECTRAL AND GRADIENT EVOLUTION,[0],[0]
"Unlike orthgonally initialized models, the RNN on the bottom right of Figure 5 with Glorot normal initialized transition matrices begins and ends with a wide singular spectrum.",3.1.3. SPECTRAL AND GRADIENT EVOLUTION,[0],[0]
"While there is no clear positive shift in the distribution of singular values, the mean value appears to very gradually increase for both the ordered and permuted sequential MNIST tasks.",3.1.3. SPECTRAL AND GRADIENT EVOLUTION,[0],[0]
"If the model is to be expected to positively shift singular values to compensate for the contractivity of the tanh nonlinearity, it is not doing so well for the Glorot-initialized case; however, this may be due to the inefficiency of training as a result of vanishing gradients, given that initialization.",3.1.3. SPECTRAL AND GRADIENT EVOLUTION,[0],[0]
"Having established that it may indeed be useful to step away from orthogonality, here we explore two forms of soft constraints (rather than hard bounds as above) on hidden to hidden transition matrix orthogonality.",3.2. Exploring Soft Orthogonality Constraints,[0],[0]
"The first is a simple penalty that directly encourages a transition matrix W to be orthogonal, of the form λ||WTW − I||22.",3.2. Exploring Soft Orthogonality Constraints,[0],[0]
This is similar to the orthogonality penalty introduced by Henaff et al. (2016).,3.2. Exploring Soft Orthogonality Constraints,[0],[0]
"In subfigures (A) and (B) of Figure 6, we explore the effect of weakening this form of regularization.",3.2. Exploring Soft Orthogonality Constraints,[0],[0]
We trained both a regular non-factorized RNN on the T = 200 copy task (A) and a factorized RNN with orthogonal bases on the T = 500 copy task (B).,3.2. Exploring Soft Orthogonality Constraints,[0],[0]
"For the regular RNN, we had to reduce the learning rate to 10−5.",3.2. Exploring Soft Orthogonality Constraints,[0],[0]
"Here again we see that weakening the strength of the orthogonalityencouraging penalty can increase convergence speed.
",3.2. Exploring Soft Orthogonality Constraints,[0],[0]
The second approach we explore replaces the sigmoidal margin parameterization with a mean one Gaussian prior on the singular values.,3.2. Exploring Soft Orthogonality Constraints,[0],[0]
"In subfigures (C) and (D) of Figure 6, we visualize the accuracy on the length 200 copy task, using geoSGD (learning rate 10−6) to keep U and V orthogonal and different strengths γ of a Gaussian prior with mean one on the singular values si",3.2. Exploring Soft Orthogonality Constraints,[0],[0]
: γ ∑ i ||si − 1||2.,3.2. Exploring Soft Orthogonality Constraints,[0],[0]
"We
trained these experiments with regular SGD on the spectrum and other non-orthogonal parameter matrices, using a 10−5 learning rate.",3.2. Exploring Soft Orthogonality Constraints,[0],[0]
We see that strong priors lead to slow convergence.,3.2. Exploring Soft Orthogonality Constraints,[0],[0]
Loosening the strength of the prior makes the optimization more efficient.,3.2. Exploring Soft Orthogonality Constraints,[0],[0]
"Furthermore, we compare a direct parameterization of the spectrum (no sigmoid) in (C) with a sigmoidal parameterization, using a large margin of 1 in (D).",3.2. Exploring Soft Orthogonality Constraints,[0],[0]
"Without the sigmoidal parameterization, optimization quickly becomes unstable; on the other hand, the optimization also becomes unstable if the prior is removed completely in the sigmoidal formulation (margin 1).",3.2. Exploring Soft Orthogonality Constraints,[0],[0]
"These results further motivate the idea that parameterizations that deviate from orthogonality may perform better than purely orthogonal ones, as long as they are sufficiently constrained to avoid instability during training.",3.2. Exploring Soft Orthogonality Constraints,[0],[0]
"We have explored a number of methods for controlling the expansivity of gradients during backpropagation based
learning in RNNs through manipulating orthogonality constraints and regularization on weight matrices.",4. Conclusions,[0],[0]
"Our experiments indicate that while orthogonal initialization may be beneficial, maintaining hard constraints on orthogonality can be detrimental.",4. Conclusions,[0],[0]
"Indeed, moving away from hard constraints on matrix orthogonality can help improve optimization convergence rate and model performance.",4. Conclusions,[0],[0]
"However, we also observe with synthetic tasks that relaxing regularization which encourages the spectral norms of weight matrices to be close to one too much, or allowing bounds on the spectral norms of weight matrices to be too wide, can reverse these gains and may lead to unstable optimization.",4. Conclusions,[0],[0]
We thank the Natural Sciences and Engineeering Research Council (NSERC) of Canada and Samsung for supporting this research.,ACKNOWLEDGMENTS,[0],[0]
It is well known that it is challenging to train deep neural networks and recurrent neural networks for tasks that exhibit long term dependencies.,abstractText,[0],[0]
The vanishing or exploding gradient problem is a well known issue associated with these challenges.,abstractText,[0],[0]
One approach to addressing vanishing and exploding gradients is to use either soft or hard constraints on weight matrices so as to encourage or enforce orthogonality.,abstractText,[0],[0]
Orthogonal matrices preserve gradient norm during backpropagation and may therefore be a desirable property.,abstractText,[0],[0]
"This paper explores issues with optimization convergence, speed and gradient stability when encouraging or enforcing orthogonality.",abstractText,[0],[0]
"To perform this analysis, we propose a weight matrix factorization and parameterization strategy through which we can bound matrix norms and therein control the degree of expansivity induced during backpropagation.",abstractText,[0],[0]
We find that hard constraints on orthogonality can negatively affect the speed of convergence and model performance.,abstractText,[0],[0]
On orthogonality and learning recurrent networks with long term dependencies,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 92–96 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-2015",text,[0],[0]
"Natural language processing (NLP) has made significant progress over the last two decades, in particular due to the success of data-driven machine learning methods.",1 Introduction,[0],[0]
"Recently, deep learning has led to another wave of remarkable improvements in NLP and other areas of machine learning and artificial intelligence (AI).",1 Introduction,[0],[0]
"Not surprisingly, many industry players are investing heavily in machine learning and AI to create new products and services (MIT Technology Review, 2016).
",1 Introduction,[0],[0]
"However, translating research into a successful product has its own challenges.",1 Introduction,[0],[0]
"Traditionally, technology transfer is often assumed to happen in a linear transition from pure research to applied research to commercialization (Stokes, 1997).",1 Introduction,[0],[0]
The model assumes that the discoveries from researchers will naturally be picked up by engineers and industry players who will use it to build new products.,1 Introduction,[0],[0]
"In reality, the transfer from research to commercial products is considerably more complex and far from guaranteed.",1 Introduction,[0],[0]
"In fact, many research projects fail to successfully transfer their discoveries to commercial products.
",1 Introduction,[0],[0]
"In this position paper, I highlight some of the reasons why it is so difficult to translate NLP research into successful products.",1 Introduction,[0],[0]
"This paper does not contain any new algorithms, experiments, or results.",1 Introduction,[0],[0]
"Instead, it seeks to share my experience working at the intersection of academic research and industry with the aim to stimulate a discussion how technology transfer of NLP research can be improved.",1 Introduction,[0],[0]
I want to emphasize upfront that the paper is not arguing that all NLP researchers should focus their efforts on building commercial products nor does every new product require a research breakthrough to be successful.,1 Introduction,[0],[0]
"The paper’s aim is rather to discuss how we can improve useinspired basic research that satisfies both the desire for fundamental understanding and considerations of use, sometimes referred to as Pasteur’s quadrant (Stokes, 1997).
",1 Introduction,[0],[0]
The contributions of this paper are twofold.,1 Introduction,[0],[0]
"First, I highlight common obstacles in the path of transferring research into commercial products.",1 Introduction,[0],[0]
"Second, I offer suggestions for increasing the chances of success based on my experience at SAP, the world’s largest enterprise software company.",1 Introduction,[0],[0]
This section highlights challenges in NLP research that make it difficult to translate the results into impactful innovation.,2 Challenges to Innovation,[0],[0]
The first step to creating a successful product is understanding your customers.,2.1 Lack of Value Focus,[0],[0]
"That is why many methodologies for creating new products or business models start with a user persona and how to create value for the user (Ries, 2011; Osterwalder et al., 2014).",2.1 Lack of Value Focus,[0],[0]
"Similarly, to conduct research with practical impact, it is worthwhile to consider what potential applications the research could enable
92
and what the value proposition for a potential user might be.",2.1 Lack of Value Focus,[0],[0]
"The value proposition is closely linked to the user persona and the tasks that she tries to solve in her daily life (Christensen and Raynor, 2013).",2.1 Lack of Value Focus,[0],[0]
"Thus, choosing the right research task is important when aiming for impactful research.",2.1 Lack of Value Focus,[0],[0]
"It is instructive that NLP tasks which solve practical problems, like machine translation or sentiment analysis, have seen significant adoption in commercial applications.",2.1 Lack of Value Focus,[0],[0]
"But many applications that are requested by industry are still beyond the capabilities of current NLP research, for example chatbots that can respond to arbitrary user questions.
",2.1 Lack of Value Focus,[0],[0]
It is also important for researchers to understand that the priorities in industry are different from priorities in academic research.,2.1 Lack of Value Focus,[0],[0]
"In academic research, the priorities are to create contributions to the body of knowledge in the field, e.g., defining a new task, a novel, elegant model, or a new stateof-the-art benchmark result.",2.1 Lack of Value Focus,[0],[0]
In industry the priorities are creating innovative products that delight users and create new revenue streams.,2.1 Lack of Value Focus,[0],[0]
"To have the best of both worlds, researchers should occasionally take a step back and consider what value proposition their work has for people outside the NLP community.",2.1 Lack of Value Focus,[0],[0]
Reproducible research is one of the pillars of the scientific method and thus important to good research work in general.,2.2 Lack of Reproducibility,[0],[0]
But the ability to reproduce a model is also a prerequisite to incorporating it into a product.,2.2 Lack of Reproducibility,[0],[0]
"As NLP models often depend on a complex set of parameters and pre-processing steps which cannot always be explained in all detail in a paper, it is often hard to reproduce other’s results.",2.2 Lack of Reproducibility,[0],[0]
The author himself has his own experience trying to (unsuccessfully) reproduce published results.,2.2 Lack of Reproducibility,[0],[0]
"As problems to reproduce research are seldom reported (but see (Bikel, 2004) for an exception), it is also hard for researchers to find information on how to improve their implementation when they struggle to re-produce published results.",2.2 Lack of Reproducibility,[0],[0]
Data is the fuel that powers machine learning and most of NLP research.,2.3 Lack of (Domain) Data,[0],[0]
"While the “big data” revolution has given us access to large quantities of text data from some domains, for many industry problems there is no or very limited data available to conduct research on.",2.3 Lack of (Domain) Data,[0],[0]
"For example, in my group we have been working on text classi-
fication for customer service tickets.",2.3 Lack of (Domain) Data,[0],[0]
"While there are many datasets available for text classification, these are primarily from newswire or online reviews.",2.3 Lack of (Domain) Data,[0],[0]
"For customer service, there is no public dataset to compare to.",2.3 Lack of (Domain) Data,[0],[0]
"Due to the confidential nature of the data and data privacy concerns, companies who have such data cannot easily release it for research purposes.",2.3 Lack of (Domain) Data,[0],[0]
"Some companies host shared tasks or data science competitions in which they make data available, for example on Kaggle1, but access to data remains one of the biggest obstacles for researcher who want to work on industry problems.
",2.3 Lack of (Domain) Data,[0],[0]
"Even when there is data available from public sources, e.g., from the web, using the data for commercial purposes can be tricky from a legal standpoint.",2.3 Lack of (Domain) Data,[0],[0]
"Crawling data from web (or using corpora created by others in this manner) might be acceptable for research purposes, but when building a commercial product the exact license, copyright, etc. of every data source needs to be checked.",2.3 Lack of (Domain) Data,[0],[0]
"The same holds for publicly available NLP models derived from such data.
",2.3 Lack of (Domain) Data,[0],[0]
"For everyone who believes that working in industry solves all data problem, I note here that working with real data sets has its own challenges.",2.3 Lack of (Domain) Data,[0],[0]
"Real data sets are often small, noisy, scrambled, or otherwise incomplete, making it hard to achieve good results.",2.3 Lack of (Domain) Data,[0],[0]
"To effectively use the data, researchers also have to understand the data schema and the business process behind the data.",2.3 Lack of (Domain) Data,[0],[0]
This can be challenging without and in-depth domain knowledge.,2.3 Lack of (Domain) Data,[0],[0]
"The empirical evaluation of statistical methods on common benchmarks has without a doubt revolutionized NLP (Johnson, 2009).",2.4 Overemphasis on Test Scores,[0],[0]
"However, sometimes the score on the test set is taken as the only factor that determines the success of a piece of research.",2.4 Overemphasis on Test Scores,[0],[0]
"For practical applications, the test score on a benchmark dataset is only one criteria among many when it comes to choosing an algorithm for practical use.",2.4 Overemphasis on Test Scores,[0],[0]
"Other factors include the time and costs required to implement the method, the computational resources required, speed and performance, the ease of integration, support for multi-lingual input, the ability to adapt and customize the method, the ability to incorporate prior knowledge, and the ability to interpret and explain
1https://www.kaggle.com
the model.",2.4 Overemphasis on Test Scores,[0],[0]
"For example, in our text categorization work, we encountered the requirement to accommodate changes in the the output classes, i.e., adding, merging, splitting, and removing classes, without re-training the model from scratch.",2.4 Overemphasis on Test Scores,[0],[0]
These factors are currently underrepresented in NLP research.,2.4 Overemphasis on Test Scores,[0],[0]
"No matter how good an NLP model is, it cannot have practical impact if it is never implemented.",2.5 Difficulty of Adoption,[0],[0]
"But in any application, the NLP model will only be one component in a larger software system.",2.5 Difficulty of Adoption,[0],[0]
How easily the NLP component can work together with the remaining components is important for the ease of adoption of the method into productive applications.,2.5 Difficulty of Adoption,[0],[0]
"Unlike rule-based methods, statistical NLP models often require expensive collection and labeling of data, data pre-processing, model (re-)training, parameter tuning, and monitoring of the model to avoid model staleness.",2.5 Difficulty of Adoption,[0],[0]
"This makes it harder to adopt statistical models in practical applications (Chiticariu et al., 2013).",2.5 Difficulty of Adoption,[0],[0]
The time horizon within which stakeholders expect results is generally shorter in industry projects.,2.6 Timelines,[0],[0]
"While research grants typically run for three to five years, industry research is under pressure to deliver tangible outcomes in less than two years.",2.6 Timelines,[0],[0]
"For projects with actual customers and proof of concepts, timelines are usually not longer than a few months.",2.6 Timelines,[0],[0]
This results in the following chicken and egg problem: it is difficult to produce groundbreaking research within a short time frame but long investments into research are hard to justify if the value the research will ultimately produce is not clear.,2.6 Timelines,[0],[0]
That is why academic research is generally better equipped to focus on fundamental research questions.,2.6 Timelines,[0],[0]
Fundamental research does not exclude practical usage but incremental research that fine-tunes every aspect of the implementation of an NLP model is often better done in industry labs.,2.6 Timelines,[0],[0]
"In this section, I offer some suggestions about how the disconnect between NLP research and commercial products can be reduced.",3 Bridging the Gap,[0],[0]
"The following approach describes the criteria that we typically apply in our team when we evaluate new machine learning use cases, including NLP use cases.
",3.1 A “Recipe” for Qualifying a Research Problem,[0],[0]
"First, we make sure we understand the “job to be done”: what is the business problem, who is the potential user and what problem are we trying to solve?",3.1 A “Recipe” for Qualifying a Research Problem,[0],[0]
"Once we have understood the task, a first question to ask is whether the task actually requires NLP.",3.1 A “Recipe” for Qualifying a Research Problem,[0],[0]
Is the data volume so high that automation is needed?,3.1 A “Recipe” for Qualifying a Research Problem,[0],[0]
Would it be easier or cheaper to solve the task manually?,3.1 A “Recipe” for Qualifying a Research Problem,[0],[0]
Can the task be solved via simple rules?,3.1 A “Recipe” for Qualifying a Research Problem,[0],[0]
"Typically, tasks with high data volume and complex or ambiguous rules are good candidates for NLP.
",3.1 A “Recipe” for Qualifying a Research Problem,[0],[0]
"To ensure that the use cases we work on have practical relevance, we include stakeholders from the lines of business and industry units in the company in any new project right from the beginning and gather feedback from actual customers.
",3.1 A “Recipe” for Qualifying a Research Problem,[0],[0]
"Once we believe that NLP is required, we try to formulate the problem as a machine learning task.",3.1 A “Recipe” for Qualifying a Research Problem,[0],[0]
"The simple template given X, predict Y together with the question what are the inputs and what are the outputs?",3.1 A “Recipe” for Qualifying a Research Problem,[0],[0]
helps significantly to get from a vague idea to a concrete task formulation.,3.1 A “Recipe” for Qualifying a Research Problem,[0],[0]
"At this stage, we can often already map the problem to a standard NLP task, e.g., text classification, sequence tagging, or sequence-to-sequence learning.
",3.1 A “Recipe” for Qualifying a Research Problem,[0],[0]
"Next, we establish whether data is available.",3.1 A “Recipe” for Qualifying a Research Problem,[0],[0]
"If real data is not available easily, can we work with publicly available proxy data?",3.1 A “Recipe” for Qualifying a Research Problem,[0],[0]
"For example for learning to classify customer service tickets, we can start with text classification on public datasets.",3.1 A “Recipe” for Qualifying a Research Problem,[0],[0]
"If it is unlikely that data will be available in the foreseeable future, we do not proceed with a use case.
",3.1 A “Recipe” for Qualifying a Research Problem,[0],[0]
"Next, we make a best guess whether the problem can be solved with the current state of the art in NLP.",3.1 A “Recipe” for Qualifying a Research Problem,[0],[0]
Is there an intuitive regularity in the data which we believe a statistical model could pick up?,3.1 A “Recipe” for Qualifying a Research Problem,[0],[0]
Can we represent the input via meaningful features?,3.1 A “Recipe” for Qualifying a Research Problem,[0],[0]
"Do we have a way to measure the success of the method with a well-defined metric?
",3.1 A “Recipe” for Qualifying a Research Problem,[0],[0]
"Finally, we determine the right approach to execute the use case.",3.1 A “Recipe” for Qualifying a Research Problem,[0],[0]
"If it is a hard problem which needs at least a few more years of research before it becomes useful, we would most likely decide on a research project.",3.1 A “Recipe” for Qualifying a Research Problem,[0],[0]
"We fund external
research projects at top universities around the world, where we provide the research problem and the data and let others try to crack the tough problems.",3.1 A “Recipe” for Qualifying a Research Problem,[0],[0]
"We also sponsor Ph.D. students who are working at SAP during their studies.
",3.1 A “Recipe” for Qualifying a Research Problem,[0],[0]
"If we think that the use case has a strong business case and the technology is mature enough, we will move it to building a proof of concept, and ultimately a commercial product.",3.1 A “Recipe” for Qualifying a Research Problem,[0],[0]
"While this “recipe” for qualifying an NLP use case is simple and common sense, we have found it helpful in prioritizing use cases.
",3.1 A “Recipe” for Qualifying a Research Problem,[0],[0]
Researchers in academia might not have access to a business unit to provide feedback on research ideas but many funding bodies are trying to encourage increased collaborations between industry and academia.,3.1 A “Recipe” for Qualifying a Research Problem,[0],[0]
"The European Union, for example, has specifically funded an initiative, LT-innovate2 to encourage commercial exploitation of NLP research.",3.1 A “Recipe” for Qualifying a Research Problem,[0],[0]
"I believe that a more rigorous application of (software) engineering principles and tools can greatly increase the odds of having practical impact with NLP research.
",3.2 Engineering Approach to NLP,[0],[0]
"To address the problem of reproducibility, I suggest the following.",3.2 Engineering Approach to NLP,[0],[0]
"First, the community should be more stringent about reproducibility.",3.2 Engineering Approach to NLP,[0],[0]
"In some research communities, for example databases, the criteria for reproducible research are a lot stricter.",3.2 Engineering Approach to NLP,[0],[0]
"If the results are not reproducible, the results are generally not considered valid.",3.2 Engineering Approach to NLP,[0],[0]
"However, the large number of parameters and implementation details in NLP systems makes it hard to exactly reproduce published results based on the paper alone.",3.2 Engineering Approach to NLP,[0],[0]
"Therefore, we should encourage the dissemination of results through software tools that make code reproducible.",3.2 Engineering Approach to NLP,[0],[0]
"To reproduce the results in a paper, we essentially need the code, the data, and the parameters of the experimental run that produced the results of the experiment.",3.2 Engineering Approach to NLP,[0],[0]
"Fortunately, the open source community has created great tools that make this possible.",3.2 Engineering Approach to NLP,[0],[0]
"First, social code repository platforms, such as GitHub3, make it easy to share code and data.",3.2 Engineering Approach to NLP,[0],[0]
"In fact, the ease of sharing and contributing to code has arguably accelerated the progress in machine learning significantly.",3.2 Engineering Approach to NLP,[0],[0]
"Second, interactive computational environments,
2http://www.lt-innovate.org 3https://github.com/
such as Jupyter notebooks4, that tie together data, code, and documentation, allow for reproducible results that can easily be shared and published.",3.2 Engineering Approach to NLP,[0],[0]
"Finally, software containers, such as Docker5, allow light-weight virtualization that pulls in all software dependencies and allow the same code to run in a reliable and reproducible manner.",3.2 Engineering Approach to NLP,[0],[0]
"If a Jupyter notebook or Dockerfile is published with the paper, it should be easier for other researchers to reproduce results and integrate them into larger systems.",3.2 Engineering Approach to NLP,[0],[0]
"Projects like CodaLab6 try to build online platforms for reproducible research with similar goals.
",3.2 Engineering Approach to NLP,[0],[0]
"On the problem of data availability, there is already a considerable amount of work in the area of building NLP models in low-resource environments (see for example (Duong et al., 2014; Garrette and Baldridge, 2013; Wang et al., 2015)) which deals with limited data availability.",3.2 Engineering Approach to NLP,[0],[0]
"Techniques like domain adaptation, semi-supervised learning and transfer learning (Pan and Yang, 2010) are extremely relevant to address the problem of data availability for industry applications.",3.2 Engineering Approach to NLP,[0],[0]
"Finally, recent work on learning models from private data (Papernot et al., 2016) and federated learning across many devices (McMahan et al., 2016) appear to be promising directions for practical NLP engineering research.",3.2 Engineering Approach to NLP,[0],[0]
"I believe that there is an opportunity to increase the exchange between industry and the research community by establishing an industry paper submission format, potentially with its own industry track at NLP conferences.",3.3 Industry Papers,[0],[0]
"Such a track could offer a venue to discuss practical challenges in building large-scale NLP systems and deploying NLP models in production settings, such as scalability, trade-offs between accuracy and computational costs, robustness, data quality, etc.",3.3 Industry Papers,[0],[0]
This would help to counter-balance the overemphasis on test scores in pure research papers and aid the adoption of research in industry applications.,3.3 Industry Papers,[0],[0]
"Industry tracks are common in other communities and have strong participation from industry players there.
",3.3 Industry Papers,[0],[0]
4http://jupyter.org/ 5https://www.docker.com/ 6http://codalab.org/,3.3 Industry Papers,[0],[0]
Wagstaff (2012) argues for making machine learning research more relevant.,4 Related Work,[0],[0]
He laments a hyperfocus on UCI benchmark datasets and abstract metrics.,4 Related Work,[0],[0]
"Spector et al. (2012) present Google’s hybrid approach to research, which tries to avoid separation between research and engineering.",4 Related Work,[0],[0]
"Recently, several groups at Google have published papers on practical challenges in deploying machine learning in production (Sculley et al., 2014; McMahan et al., 2013; Breck et al., 2016).",4 Related Work,[0],[0]
Belz (2009) discusses the practical applications of NLP research.,4 Related Work,[0],[0]
Mani (2011) gives suggestions for improving the review process.,4 Related Work,[0],[0]
None of the works provides a detailed discussion on the difficulties in bringing NLP research to commercial products – the main contribution of this paper.,4 Related Work,[0],[0]
I have highlighted difficulties that exist for researchers who try to bring NLP research into commercially products and offered suggestions for improving the odds of commercial success.,5 Conclusion,[0],[0]
I hope that my experience can stimulate creative thought and a healthy discussion in the NLP community.,5 Conclusion,[0],[0]
This paper highlights challenges in industrial research related to translating research in natural language processing into commercial products.,abstractText,[0],[0]
"While the interest in natural language processing from industry is significant, the transfer of research to commercial products is non-trivial and its challenges are often unknown to or underestimated by many researchers.",abstractText,[0],[0]
I discuss current obstacles and provide suggestions for increasing the chances for translating research to commercial success based on my experience in industrial research.,abstractText,[0],[0]
On the Challenges of Translating NLP Research into Commercial Products,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 79–84 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-2013",text,[0],[0]
"NLP for studying people has grown rapidly as more than one-third of the human population use social media actively.1 While traditional NLP tasks (e.g. POS tagging, parsing, sentiment analysis) mostly work at the word, sentence, or document level, the increased focus on social scientific applications has shifted attention to new levels of analysis (e.g. user-level and communitylevel) (Koppel et al., 2009; Sarawgi et al., 2011; Schwartz et al., 2013a; Coppersmith et al., 2014; Flekova et al., 2016).
",1 Introduction,[0],[0]
"Figure 1 shows the distribution of two unigrams, ‘the’ and ‘love’ at three levels of analysis.",1 Introduction,[0],[0]
"While both words have zero counts in most messages, ‘the’ starts to look Normal across
1Social Insights; Global social media research summary 2017
users, and both words are approximately Normal at the county level.",1 Introduction,[0],[0]
"Methods performing optimally at the document level may suffer at the user or community level due to this shift in the distribution of lexical features.2
In this paper, we ask a fundamental statistical question: How does the shift in unit-of-analysis from document-level to user-or-community level shift lexical distributions in social media?3",1 Introduction,[0],[0]
"The central limit theorem suggests that count data is better approximated by a Normal distribution as one increases the number of events, or as one aggregates more features (e.g. combining words using LDA topics or hand-built word sets).",1 Introduction,[0],[0]
"However, we do not know how far towards a Normal these new levels of analysis bring us.
",1 Introduction,[0],[0]
Related work.,1 Introduction,[0],[0]
"The question we ask harks back to work from pioneers in corpus-based computational linguistics, including Shannon (1948) who suggested that probabilistic distributions of ngrams could be used to solve a range of communications problems, and Mosteller and Wallace (1963) who found that a negative binomial distribution seemed to model unigram usage by authors of the Federalist Papers.",1 Introduction,[0],[0]
Numerous works have since continued the tradition of examining the distribution of lexical features.,1 Introduction,[0],[0]
"For example, McCallum et al. (1998) compares the results of probabilistic models based on multivariate Bernoulli with those based on multinomial distributions for document classification.",1 Introduction,[0],[0]
"Jansche
2While the distribution of word frequencies (i.e. a Zipfian distribution) is often discussed in NLP, it is important to note that we are focused on the distribution of single features (e.g. words) over documents, users, or communities.
",1 Introduction,[0],[0]
"3While other sources of corpora can also be aggregated to the user- or community-level (e.g. newswire, books), we believe the question of distributions is particularly important in social media because it often contains very short posts and a growing body of work in NLP for social science focuses on social media.
79
(2003) extended this line of work, observing lexical count data often display an extra probability mass concentrated at zero and suggesting ZeroInflated negative binomial distributions can capture this phenomenon better and are easier to implement than alternatives such as overdispersed binomial models.",1 Introduction,[0],[0]
"While these works are numerous, none, to the best of our knowledge, have focused on distributions across social media or at multiple levels of analysis.
",1 Introduction,[0],[0]
Contributions.,1 Introduction,[0],[0]
"Our study is perhaps unconventional in modern computational linguistics due to the elementary nature of our contributions, focusing on understanding the empirical distributions of lexical features in Twitter.",1 Introduction,[0],[0]
"First, we use zeroinflated kernel density estimated plots to show how distributions of different language features (words, LDA topics, and hand-curated word sets) vary with level of analysis (message, user, and county).",1 Introduction,[0],[0]
"Second, we quantify which distributions best describe the different feature types and analysis levels of social media.",1 Introduction,[0],[0]
"Finally, we show the utility of such information, finding that using the appropriate model for each feature type improves Naive Bayes classification results across three common social scientific tasks: sarcasm detection at the message-level, gender identification at the user-level, and political ideology classification at the community-level.",1 Introduction,[0],[0]
"Examining data at three different levels of analysis and across three different lexical feature types (unigrams, data-driven topics, and manual lexica), we seek to (1) visually characterize distributions, (2) empirically test which distributions best fit the data, and (3) evaluate classification models utilizing multiple distributions at each level.",2 Methods,[0],[0]
"Unigrams underlie all data where as each level of analysis
and feature type represent a different degree of aggregation and covariance structure.
",2 Methods,[0],[0]
Data preparation.,2 Methods,[0],[0]
"We start with a set of about two million Twitter posts and supplemental information about the users: their ID, county, and gender.",2 Methods,[0],[0]
"The data was based on that of Volkova et al. (2013), who provide tweet ids and gender, and mapped to counties using the method of Schwartz et al. (2013a).",2 Methods,[0],[0]
We limit our data to users who have used at least 1000 words and counties that have at least 30 users and a total word count of 5000.,2 Methods,[0],[0]
"Applying these constraints, the final set of data consists of 1,639,750 tweets (representing the message-level) from 5,226 users in 420 different counties (representing the community-level).
",2 Methods,[0],[0]
We consider three lexical features that are commonly used in NLP for social science:,2 Methods,[0],[0]
"1- grams (the top 10,000 most common unigrams found with happierFunTokenizing social media tokenizer), 2000 LDA topics downloaded from Schwartz et al. (2013b)), and lexica (64 categories from the linguistic inquiry and word count dictionary (Pennebaker et al., 2007)).",2 Methods,[0],[0]
"Note that the features progress from most sparse (1grams) to least sparse (lexica).
Distributions.",2 Methods,[0],[0]
Figure 2 shows the empirical distributions of different lexical features at different levels of analysis.,2 Methods,[0],[0]
"500 features were sampled from the top 20,000 unigrams 4, 2000 social media LDA topics (Schwartz et al., 2013a), and all 64 categories from the LIWC lexica (Pennebaker et al., 2007).",2 Methods,[0],[0]
"To encode the variables continuously we used relative frequencies for unigrams and lexica (count of word or category divided by count of all words), and probability of topics, calculated from the posterior probabilities from the LDA models.",2 Methods,[0],[0]
"Each line in the kernel density plot
4In social media analyses, the top 20,000 features are often used (Schwartz and Ungar, 2015)
is semi-transparent such that an aggregate trend across multiple features will emerge darkest.",2 Methods,[0],[0]
"As we move along a row ranging specific features (unigrams) to generic features (lexicon), the empirical distribution gradually changes from resembling a “power law” (or binomial distribution with low number of trials and probability of success) to something more “Normal”.",2 Methods,[0],[0]
"Similar shifts are also observed as we move across levels of modeling.
",2 Methods,[0],[0]
We investigate whether the best-fitting distributions vary across the three levels of analysis and three types of lexical features.,2 Methods,[0],[0]
"We consider the following candidate distributions to see how well they fit each of these empirical distributions:
• Continuous Distributions: (a) Power-law, (b), Log-normal and (c) Normal
• Discrete Distributions: (a) Bernoulli, (b) Multinomial, (c) Poisson, and (d) Zero Inflated Poisson
Since most of the distributions outlined above are standard distributions, we only briefly describe the zero-inflated variants which handle excess zero counts.",2 Methods,[0],[0]
Zero-inflated models explicitly model the idea that a distribution does not fully capture the mass at 0 in real world data.,2 Methods,[0],[0]
They assume that the data is generated from two components.,2 Methods,[0],[0]
"The first
component is governed by a Bernoulli distribution that generates excess zeros, while the second component generates counts, some of which also could be zero (Jansche, 2003).",2 Methods,[0],[0]
"We evaluate the distributions we considered by first characterizing the goodness of fit at different levels of analyses and then by their predictive performance on social media prediction tasks, both of which we describe below.",3 Evaluation,[0],[0]
"Following the central limit theorem, we seek to determine across the range levels of analysis and feature types, whether the distribution can be approximated by a Normal.",3.1 Goodness of fit,[0],[0]
"Focusing just on the non-zero portions of data encoded as relative frequencies, we quantify the fit of each candidate distribution to the data.
",3.1 Goodness of fit,[0],[0]
We estimate the parameters for each distribution using MLE on a training data set (i.e. 80% of data).,3.1 Goodness of fit,[0],[0]
"Then, we evaluate their likelihoods of a held-out test dataset, given the estimated parameters.",3.1 Goodness of fit,[0],[0]
"Since we are trying to approximate the discrete distribution with a continuous model, all data were converted to relative frequencies.",3.1 Goodness of fit,[0],[0]
"Finally, the distribution under which the test data is most likely
is chosen as the ’best fit’ distribution.",3.1 Goodness of fit,[0],[0]
"We repeat this 100 times and pick the most likely distribution over all these 100 independent runs.
Results.",3.1 Goodness of fit,[0],[0]
"Table 1 shows the percentage of features in each level that were best fit from an underlying distribution of Normal, Log-Normal, or Power Law.",3.1 Goodness of fit,[0],[0]
"We see empirically that there is a trend toward Normal approximation moving from message to county level, as well as 1grams to lexica.",3.1 Goodness of fit,[0],[0]
"In fact, a majority of lexica at the county-level were best approximated by a Normal distribution.",3.1 Goodness of fit,[0],[0]
"In the previous section, we showed that the distribution of lexical features depends on the scale of analysis considered (for example, the message level or the user level).",3.2 Predictive Power,[0],[0]
"Here, we demonstrate that predictive models which use these lexical features as co-variates can leverage this information to boost predictive performance.",3.2 Predictive Power,[0],[0]
We consider three predictive tasks using a generative predictive model.,3.2 Predictive Power,[0],[0]
"The primary purpose of this evaluation is not to characterize the best distribution at a level or task, but to demonstrate that the choice of distribution assumed when modeling features significantly affects the predictive performance.
",3.2 Predictive Power,[0],[0]
"Predictive Tasks : We consider the following common predictive tasks and also outline details of the datasets considered:
1.",3.2 Predictive Power,[0],[0]
Sarcasm Detection (Message level):,3.2 Predictive Power,[0],[0]
"This task consists of determining whether tweets contain a sarcastic expression (Bamman and Smith, 2015).",3.2 Predictive Power,[0],[0]
"The dataset consists of 16,833 messages with an average of 12 words per message.
2.",3.2 Predictive Power,[0],[0]
"Gender Identification (User level): This task involves determining the gender of the author utilizing a previously described Twitter dataset (Volkova et al., 2013).",3.2 Predictive Power,[0],[0]
"This dataset consists of 5,044 users each of which have
at least a 1,000 tokens as is standard in userlevel analyses (Schwartz et al., 2013b).
3.",3.2 Predictive Power,[0],[0]
Ideology Classification (Community level): We utilized county voting records from 2012 along with a dataset of tweets mapped to counties.,3.2 Predictive Power,[0],[0]
"This data consists of 2,175 counties with atleast 10,000 unigrams as is common in community level analyses (Eichstaedt et al., 2015).
",3.2 Predictive Power,[0],[0]
"We consider a Naive Bayes classifier (a generative model) which enables one to directly incorporate the inferred feature distribution at a particular level of analysis, the results of which we discuss in Table 2.",3.2 Predictive Power,[0],[0]
"Variable encoding for the classifiers varied from binary encoding of present or not (Bernoulli), to counts (Poisson, Zero-inflated Poisson), multivariate counts (Multinomial), and continuous relative frequencies (Normal).",3.2 Predictive Power,[0],[0]
"All distributions have closed form MLE solutions except for Zero-Inflated Poisson, in which case we used LBFGS optimization to fit both of its parameters (Head and Zerner, 1985).
",3.2 Predictive Power,[0],[0]
Results.,3.2 Predictive Power,[0],[0]
We report macro F1-score for each of the underlying distributions in Table 2.,3.2 Predictive Power,[0],[0]
"For each of the tasks, we used 80% of the data for training and evaluate on the held-out 20%.",3.2 Predictive Power,[0],[0]
"We observe a similar pattern as that observed in the goodness of fit setting, with a shift in the best performing distribution from Bernoulli (which simply models if a feature exists or not) toward something more Gaussian (Poisson or Normal) as we move along from message-level to county-level analysis and from unigrams to lexica.",3.2 Predictive Power,[0],[0]
"Specifically note that at higher levels of analysis (at user and county levels) as the distribution of features becomes closer to Normal, modeling features as Bernoulli is clearly sub-optimal where as at the message level modeling unigrams as a Bernoulli is superior.",3.2 Predictive Power,[0],[0]
"These observations underscore the main insight that the distribution family used to model features can be con-
sidered a function of level of analysis and featuretype considered and has a significant bearing on predictive performance.",3.2 Predictive Power,[0],[0]
"While computational linguistics has a long history of studying the distributions of lexical features, social media and social scientific studies have brought about a need to understand how these change at multiple levels of analyses.",4 Conclusion,[0],[0]
"Here, we explored empirical distributions of different types of linguistic features (unigrams, topics, lexica) in three different levels of analysis in Twitter data (message, user, and community).",4 Conclusion,[0],[0]
"To show which distribution can better describe features of different levels, we approached the problem in three different ways: (1) visualization of empirical distributions, (2) goodness-of-fit comparisons, and (3) for predictive tasks.
",4 Conclusion,[0],[0]
"We showed that the best-fit distribution depends on feature-type (i.e. unigram versus lexica) and the level of analysis (i.e. message-, user-, or community-level).",4 Conclusion,[0],[0]
"Following the central limit theorem, all user-level features were predominantly Log-normal, while a power law best fit unigrams at the message level and a Normal distribution best approximated lexica at the community level.",4 Conclusion,[0],[0]
"Finally, we demonstrated that predictive performance can also vary considerably by the level of analysis and feature-type, following a similar trend from Bernoulli distributions at the messagelevel to Poisson or Normal at the community-level.",4 Conclusion,[0],[0]
Our results underscore the significance of the level of analysis for the ever-growing focus in NLP on social scientific problems which seek to not only better model words and documents but also the people and communities generating them.,4 Conclusion,[0],[0]
"This work was supported in part by the Templeton Religion Trust, Grant TRT-0048.",Acknowledgements,[0],[0]
Natural language processing has increasingly moved from modeling documents and words toward studying the people behind the language.,abstractText,[0],[0]
This move to working with data at the user or community level has presented the field with different characteristics of linguistic data.,abstractText,[0],[0]
"In this paper, we empirically characterize various lexical distributions at different levels of analysis, showing that, while most features are decidedly sparse and non-normal at the message-level (as with traditional NLP), they follow the central limit theorem to become much more Log-normal or even Normal at the userand county-levels.",abstractText,[0],[0]
"Finally, we demonstrate that modeling lexical features for the correct level of analysis leads to marked improvements in common social scientific prediction tasks.",abstractText,[0],[0]
On the Distribution of Lexical Features at Multiple Levels of Analysis,title,[0],[0]
"(1) The complexity of the computed function grows exponentially with depth. We design measures of expressivity that capture the non-linearity of the computed function. Due to how the network transforms its input, these measures grow exponentially with depth.
(2) All weights are not equal (initial layers matter more). We find that trained networks are far more sensitive to their lower (initial) layer weights: they are much less robust to noise in these layer weights, and also perform better when these weights are optimized well.
(3) Trajectory Regularization works like Batch Normalization. We find that batch norm stabilizes the learnt representation, and based on this propose a new regularization scheme, trajectory regularization.",text,[0],[0]
"Deep neural networks have proved astoundingly effective at a wide range of empirical tasks, from image classification (Krizhevsky et al., 2012) to playing Go (Silver et al., 2016), and even modeling human learning (Piech et al., 2015).
",1. Introduction,[0],[0]
1Cornell University 2Google Brain 3Stanford University.,1. Introduction,[0],[0]
"Correspondence to: Maithra Raghu <maithrar@gmail.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"Despite these successes, understanding of how and why neural network architectures achieve their empirical successes is still lacking.",1. Introduction,[0],[0]
"This includes even the fundamental question of neural network expressivity, how the architectural properties of a neural network (depth, width, layer type) affect the resulting functions it can compute, and its ensuing performance.
",1. Introduction,[0],[0]
"This is a foundational question, and there is a rich history of prior work addressing expressivity in neural networks.",1. Introduction,[0],[0]
"However, it has been challenging to derive conclusions that provide both theoretical generality with respect to choices of architecture as well as meaningful insights into practical performance.
",1. Introduction,[0],[0]
"Indeed, the very first results on this question take a highly theoretical approach, from using functional analysis to show universal approximation results (Hornik et al., 1989; Cybenko, 1989), to analysing expressivity via comparisons to boolean circuits (Maass et al., 1994) and studying network VC dimension (Bartlett et al., 1998).",1. Introduction,[0],[0]
"While these results provided theoretically general conclusions, the shallow networks they studied are very different from the deep models that have proven so successful in recent years.
",1. Introduction,[0],[0]
"In response, several recent papers have focused on understanding the benefits of depth for neural networks (Pascanu et al., 2013; Montufar et al., 2014; Eldan and Shamir, 2015; Telgarsky, 2015; Martens et al., 2013; Bianchini and Scarselli, 2014).",1. Introduction,[0],[0]
"These results are compelling and take modern architectural changes into account, but they only show that a specific choice of weights for a deeper network results in inapproximability by a shallow (typically one or two hidden layers) network.
",1. Introduction,[0],[0]
"In particular, the goal of this new line of work has been to establish lower bounds — showing separations between shallow and deep networks — and as such they are based on hand-coded constructions of specific network weights.",1. Introduction,[0],[0]
"Even if the weight values used in these constructions are robust to small perturbations (as in (Pascanu et al., 2013; Montufar et al., 2014)), the functions that arise from these constructions tend toward extremal properties by design, and there is no evidence that a network trained on data ever resembles such a function.
",1. Introduction,[0],[0]
"This has meant that a set of fundamental questions about
ar X
iv :1
60 6.
05 33
6v 6
[ st
at .M
L ]
1 8
Ju n
20 17
neural network expressivity has remained largely unanswered.",1. Introduction,[0],[0]
"First, we lack a good understanding of the “typical” case rather than the worst case in these bounds for deep networks, and consequently have no way to evaluate whether the hand-coded extremal constructions provide a reflection of the complexity encountered in more standard settings.",1. Introduction,[0],[0]
"Second, we lack an understanding of upper bounds to match the lower bounds produced by this prior work; do the constructions used to date place us near the limit of the expressive power of neural networks, or are there still large gaps?",1. Introduction,[0],[0]
"Finally, if we had an understanding of these two issues, we might begin to draw connections between network expressivity and observed performance.
",1. Introduction,[0],[0]
"Our contributions: Measures of Expressivity and their Applications In this paper, we address this set of challenges by defining and analyzing an interrelated set of measures of expressivity for neural networks; our framework applies to a wide range of standard architectures, independent of specific weight choices.",1. Introduction,[0],[0]
"We begin our analysis at the start of training, after random initialization, and later derive insights connecting network expressivity and performance.
",1. Introduction,[0],[0]
"Our first measure of expressivity is based on the notion of an activation pattern: in a network where the units compute functions based on discrete thresholds, we can ask which units are above or below their thresholds (i.e. which units are “active” and which are not).",1. Introduction,[0],[0]
"For the range of standard architectures that we consider, the network is essentially computing a linear function once we fix the activation pattern; thus, counting the number of possible activation patterns provides a concrete way of measuring the complexity beyond linearity that the network provides.",1. Introduction,[0],[0]
"We give an upper bound on the number of possible activation patterns, over any setting of the weights.",1. Introduction,[0],[0]
"This bound is tight as it matches the hand-constructed lower bounds of earlier work (Pascanu et al., 2013; Montufar et al., 2014).
",1. Introduction,[0],[0]
"Key to our analysis is the notion of a transition, in which changing an input x to a nearby input x + δ changes the activation pattern.",1. Introduction,[0],[0]
We study the behavior of transitions as we pass the input along a one-dimensional parametrized trajectory x(t).,1. Introduction,[0],[0]
"Our central finding is that the trajectory length grows exponentially in the depth of the network.
",1. Introduction,[0],[0]
"Trajectory length serves as a unifying notion in our measures of expressivity, and it leads to insights into the behavior of trained networks.",1. Introduction,[0],[0]
"Specifically, we find that the exponential growth in trajectory length as a function of depth implies that small adjustments in parameters lower in the network induce larger changes than comparable adjustments higher in the network.",1. Introduction,[0],[0]
"We demonstrate this phenomenon through experiments on MNIST and CIFAR-10, where the network displays much less robustness to noise
in the lower layers, and better performance when they are trained well.",1. Introduction,[0],[0]
"We also explore the effects of regularization methods on trajectory length as the network trains and propose a less computationally intensive method of regularization, trajectory regularization, that offers the same performance as batch normalization.
",1. Introduction,[0],[0]
"The contributions of this paper are thus:
(1) Measures of expressivity: We propose easily computable measures of neural network expressivity that capture the expressive power inherent in different neural network architectures, independent of specific weight settings.
",1. Introduction,[0],[0]
"(2) Exponential trajectories: We find an exponential depth dependence displayed by these measures, through a unifying analysis in which we study how the network transforms its input by measuring trajectory length
(3) All weights are not equal (the lower layers matter more): We show how these results on trajectory length suggest that optimizing weights in lower layers of the network is particularly important.
(4) Trajectory Regularization Based on understanding the effect of batch norm on trajectory length, we propose a new method of regularization, trajectory regularization, that offers the same advantages as batch norm, and is computationally more efficient.
",1. Introduction,[0.9501172789364625],"['We show in Section 4.1 that (a) local minima of Problem 4 inherit the same implicit bias as the global optima, i.e. all local minima are equalized.']"
"In prior work (Poole et al., 2016), we studied the propagation of Riemannian curvature through random networks by developing a mean field theory approach.",1. Introduction,[0],[0]
"Here, we take an approach grounded in computational geometry, presenting measures with a combinatorial flavor and explore the consequences during and after training.",1. Introduction,[0],[0]
"Given a neural network of a certain architecture A (some depth, width, layer types), we have an associated function, FA(x;W ), where x is an input and W represents all the parameters of the network.",2. Measures of Expressivity,[0],[0]
"Our goal is to understand how the behavior of FA(x;W ) changes asA changes, for values of W that we might encounter during training, and across inputs x.
The first major difficulty comes from the high dimensionality of the input.",2. Measures of Expressivity,[0],[0]
Precisely quantifying the properties of FA(x;W ) over the entire input space is intractable.,2. Measures of Expressivity,[0],[0]
"As a tractable alternative, we study simple one dimensional trajectories through input space.",2. Measures of Expressivity,[0],[0]
"More formally:
Definition:",2. Measures of Expressivity,[0],[0]
"Given two points, x0, x1 ∈ Rm, we say x(t) is a trajectory (between x0 and x1) if x(t) is a curve
parametrized by a scalar t ∈",2. Measures of Expressivity,[0],[0]
"[0, 1], with x(0) = x0 and x(1) =",2. Measures of Expressivity,[0],[0]
"x1.
",2. Measures of Expressivity,[0],[0]
"Simple examples of a trajectory would be a line (x(t) = tx1 + (1 − t)x0) or a circular arc (x(t) = cos(πt/2)x0 + sin(πt/2)x1), but in general x(t) may be more complicated, and potentially not expressible in closed form.
",2. Measures of Expressivity,[0],[0]
"Armed with this notion of trajectories, we can begin to define measures of expressivity of a network FA(x;W ) over trajectories x(t).",2. Measures of Expressivity,[0],[0]
"In (Montufar et al., 2014) the notion of a “linear region” is introduced.",2.1. Neuron Transitions and Activation Patterns,[0],[0]
"Given a neural network with piecewise linear activations (such as ReLU or hard tanh), the function it computes is also piecewise linear, a consequence of the fact that composing piecewise linear functions results in a piecewise linear function.",2.1. Neuron Transitions and Activation Patterns,[0],[0]
"So one way to measure the “expressive power” of different architectures A is to count the number of linear pieces (regions), which determines how nonlinear the function is.
",2.1. Neuron Transitions and Activation Patterns,[0],[0]
"In fact, a change in linear region is caused by a neuron transition in the output layer.",2.1. Neuron Transitions and Activation Patterns,[0],[0]
"More precisely:
Definition For fixed W , we say a neuron with piecewise linear region transitions between inputs x, x+ δ if its activation function switches linear region between x and x+δ.
",2.1. Neuron Transitions and Activation Patterns,[0],[0]
So a ReLU transition would be given by a neuron switching from off to on (or vice versa) and for hard tanh by switching between saturation at −1 to its linear middle region to saturation at 1.,2.1. Neuron Transitions and Activation Patterns,[0],[0]
"For any generic trajectory x(t), we can thus define T (FA(x(t);W )) to be the number of transitions undergone by output neurons (i.e. the number of linear regions) as we sweep the input x(t).",2.1. Neuron Transitions and Activation Patterns,[0],[0]
"Instead of just concentrating on the output neurons however, we can look at this pattern over the entire network.",2.1. Neuron Transitions and Activation Patterns,[0],[0]
"We call this an activation patten:
Definition We can defineAP(FA(x;W )) to be the activation pattern – a string of form {0, 1}num neurons (for ReLUs) and {−1, 0, 1}num neurons (for hard tanh) of the network encoding the linear region of the activation function of every neuron, for an input x and weights W .
",2.1. Neuron Transitions and Activation Patterns,[0],[0]
"Overloading notation slightly, we can also define (similarly to transitions) A(FA(x(t);W ))",2.1. Neuron Transitions and Activation Patterns,[0],[0]
as the number of distinct activation patterns as we sweep x along x(t).,2.1. Neuron Transitions and Activation Patterns,[0],[0]
"As each distinct activation pattern corresponds to a different linear function of the input, this combinatorial measure captures how much more expressive A is over a simple linear mapping.
",2.1. Neuron Transitions and Activation Patterns,[0],[0]
"Returning to Montufar et al, they provide a construction i.e. a specific set of weights W0, that results in an exponen-
tial increase of linear regions with the depth of the architectures.",2.1. Neuron Transitions and Activation Patterns,[0],[0]
"They also appeal to Zaslavsky’s theorem (Stanley, 2011) from the theory of hyperplane arrangements to show that a shallow network, i.e. one hidden layer, with the same number of parameters as a deep network, has a much smaller number of linear regions than the number achieved by their choice of weights W0 for the deep network.
",2.1. Neuron Transitions and Activation Patterns,[0],[0]
"More formally, letting A1 be a fully connected network with one hidden layer, and Al a fully connected network with the same number of parameters, but l hidden layers, they show
∀WT (FA1([0, 1];W ))",2.1. Neuron Transitions and Activation Patterns,[0],[0]
"< T (FA1([0, 1];W0) (*)
",2.1. Neuron Transitions and Activation Patterns,[0],[0]
"We derive a much more general result by considering the ‘global’ activation patterns over the entire input space, and prove that for any fully connected network, with any number of hidden layers, we can upper bound the number of linear regions it can achieve, over all possible weight settings W .",2.1. Neuron Transitions and Activation Patterns,[0],[0]
"This upper bound is asymptotically tight, matched by the construction given in (Montufar et al., 2014).",2.1. Neuron Transitions and Activation Patterns,[0],[0]
"Our result can be written formally as:
Theorem 1.",2.1. Neuron Transitions and Activation Patterns,[0],[0]
"(Tight) Upper Bound for Number of Activation Patterns Let A(n,k) denote a fully connected network with n hidden layers of width k, and inputs in Rm.",2.1. Neuron Transitions and Activation Patterns,[0],[0]
"Then the number of activation patterns A(FAn,k(Rm;W ) is upper bounded byO(kmn) for ReLU activations, andO((2k)mn) for hard tanh.
",2.1. Neuron Transitions and Activation Patterns,[0],[0]
From this we can derive a chain of inequalities.,2.1. Neuron Transitions and Activation Patterns,[0],[0]
"Firstly, from the theorem above we find an upper bound of A(FAn,k(Rm;W ))",2.1. Neuron Transitions and Activation Patterns,[0],[0]
"over all W , i.e.
∀W A(FA(n,k))(R m;W ) ≤",2.1. Neuron Transitions and Activation Patterns,[0],[0]
"U(n, k,m).
",2.1. Neuron Transitions and Activation Patterns,[0],[0]
"Next, suppose we haveN neurons in total.",2.1. Neuron Transitions and Activation Patterns,[0],[0]
"Then we want to compare (for wlog ReLUs), quantities like U(n′, N/n′,m) for different n′.
But U(n′, N/n′,m) = O((N/n′)mn ′ ), and so, noting that the maxima of ( a x )mx (for a > e) is x = a/e, we get, (for n, k > e), in comparison to (*),
U(1, N,m) < U(2, N
2 ,m) < · · ·
· · · < U(n− 1, N n− 1 ,m) <",2.1. Neuron Transitions and Activation Patterns,[0],[0]
"U(n, k,m)
",2.1. Neuron Transitions and Activation Patterns,[0],[0]
We prove this via an inductive proof on regions in a hyperplane arrangement.,2.1. Neuron Transitions and Activation Patterns,[0],[0]
The proof can be found in the Appendix.,2.1. Neuron Transitions and Activation Patterns,[0],[0]
"As noted in the introduction, this result differs from earlier lower-bound constructions in that it is an upper bound that applies to all possible sets of weights.",2.1. Neuron Transitions and Activation Patterns,[0],[0]
"Via our analysis, we also prove
-1 0 1 x0
-1
0 1 x 1 Layer 0
-1 0 1 x0
-1
0
1 Layer 1
-1 0 1 x0
-1
0
1 Layer 2
Figure 1.",2.1. Neuron Transitions and Activation Patterns,[0],[0]
Deep networks with piecewise linear activations subdivide input space into convex polytopes.,2.1. Neuron Transitions and Activation Patterns,[0],[0]
"We take a three hidden layer ReLU network, with input x ∈ R2, and four units in each layer.",2.1. Neuron Transitions and Activation Patterns,[0],[0]
The left pane shows activations for the first layer only.,2.1. Neuron Transitions and Activation Patterns,[0],[0]
"As the input is in R2, neurons in the first hidden layer have an associated line in R2, depicting their activation boundary.",2.1. Neuron Transitions and Activation Patterns,[0],[0]
The left pane thus has four such lines.,2.1. Neuron Transitions and Activation Patterns,[0],[0]
"For the second hidden layer each neuron again has a line in input space corresponding to on/off, but this line is different for each region described by the first layer activation pattern.",2.1. Neuron Transitions and Activation Patterns,[0],[0]
"So in the centre pane, which shows activation boundary lines corresponding to second hidden layer neurons in green (and first hidden layer in black), we can see the green lines ‘bend’ at the boundaries.",2.1. Neuron Transitions and Activation Patterns,[0],[0]
(The reason for this bending becomes apparent through the proof of Theorem 2.),2.1. Neuron Transitions and Activation Patterns,[0],[0]
"Finally, the right pane adds the on/off boundaries for neurons in the third hidden layer, in purple.",2.1. Neuron Transitions and Activation Patterns,[0],[0]
"These lines can bend at both black and green boundaries, as the image shows.",2.1. Neuron Transitions and Activation Patterns,[0],[0]
"This final set of convex polytopes corresponds to all activation patterns for this network (with its current set of weights) over the unit square, with each polytope representing a different linear function.
",2.1. Neuron Transitions and Activation Patterns,[0],[0]
Theorem 2.,2.1. Neuron Transitions and Activation Patterns,[0],[0]
"Regions in Input Space Given the corresponding function of a neural network FA(Rm;W ) with ReLU or hard tanh activations, the input space is partitioned into convex polytopes, with FA(Rm;W ) corresponding to a different linear function on each region.
",2.1. Neuron Transitions and Activation Patterns,[0],[0]
This result is of independent interest for optimization – a linear function over a convex polytope results in a well behaved loss function and an easy optimization problem.,2.1. Neuron Transitions and Activation Patterns,[0],[0]
"Understanding the density of these regions during the training process would likely shed light on properties of the loss surface, and improved optimization methods.",2.1. Neuron Transitions and Activation Patterns,[0],[0]
A picture of a network’s regions is shown in Figure 1.,2.1. Neuron Transitions and Activation Patterns,[0],[0]
We empirically tested the growth of the number of activations and transitions as we varied x along x(t) to understand their behavior.,2.1.1. EMPIRICALLY COUNTING TRANSITIONS,[0],[0]
"We found that for bounded non linearities, especially tanh and hard-tanh, not only do we observe exponential growth with depth (as hinted at by the upper bound) but that the scale of parameter initialization also affects the observations (Figure 2).",2.1.1. EMPIRICALLY COUNTING TRANSITIONS,[0],[0]
"We also experimented with sweeping the weights W of a layer through a trajectory W (t), and counting the different labellings output by the network.",2.1.1. EMPIRICALLY COUNTING TRANSITIONS,[0],[0]
"This ‘dichotomies’ measure is discussed further in the Appendix, and also exhibits the same growth properties, Figure 14.",2.1.1. EMPIRICALLY COUNTING TRANSITIONS,[0],[0]
"In fact, there turns out to be a reason for the exponential growth with depth, and the sensitivity to initialization scale.",2.2. Trajectory Length,[0],[0]
"Returning to our definition of trajectory, we can define an immediately related quantity, trajectory length
Definition:",2.2. Trajectory Length,[0],[0]
"Given a trajectory, x(t), we define its length, l(x(t)), to be the standard arc length:
l(x(t)) = ∫",2.2. Trajectory Length,[0],[0]
t ∣∣∣∣∣∣∣∣dx(t)dt,2.2. Trajectory Length,[0],[0]
∣∣∣∣∣∣∣∣,2.2. Trajectory Length,[0],[0]
"dt
Intuitively, the arc length breaks x(t) up into infinitesimal intervals and sums together the Euclidean length of these intervals.
",2.2. Trajectory Length,[0],[0]
"If we letA(n,k) denote, as before, fully connected networks with n hidden layers each of width k, and initializing with weights ∼ N (0, σ2w/k) (accounting for input scaling as typical), and biases ∼ N (0, σ2b ), we find that: Theorem 3.",2.2. Trajectory Length,[0],[0]
"Bound on Growth of Trajectory Length Let FA(x
′,W ) be a ReLU or hard tanh random neural network and x(t) a one dimensional trajectory with x(t+ δ) having a non trival perpendicular component to x(t) for all t, δ
(i.e, not a line).",2.2. Trajectory Length,[0],[0]
Then defining z(d)(x(t)),2.2. Trajectory Length,[0],[0]
"= z(d)(t) to be the image of the trajectory in layer d of the network, we have
(a)
E [ l(z(d)(t)) ]",2.2. Trajectory Length,[0],[0]
"≥ O ( σw √ k√
k + 1
)d l(x(t))
for ReLUs
(b)
",2.2. Trajectory Length,[0],[0]
E [ l(z(d)(t)) ],2.2. Trajectory Length,[0],[0]
"≥ O  σw√k√ σ2w + σ 2 b + k √ σ2w + σ 2 b d l(x(t)) for hard tanh
That is, l(x(t) grows exponentially with the depth of the network, but the width only appears as a base (of the exponent).",2.2. Trajectory Length,[0],[0]
"This bound is in fact tight in the limits of large σw and k.
A schematic image depicting this can be seen in Figure 3 and the proof can be found in the Appendix.",2.2. Trajectory Length,[0],[0]
"A rough outline is as follows: we look at the expected growth of the difference between a point z(d)(t) on the curve and a small perturbation z(d)(t+dt), from layer d to layer d+1.",2.2. Trajectory Length,[0],[0]
"Denoting this quantity
∣∣∣∣δz(d)(t)∣∣∣∣, we derive a recurrence relating ∣∣∣∣δz(d+1)(t)∣∣∣∣ and ∣∣∣∣δz(d)(t)∣∣∣∣ which can be composed to give the desired growth rate.
",2.2. Trajectory Length,[0],[0]
The analysis is complicated by the statistical dependence on the image of the input z(d+1)(t).,2.2. Trajectory Length,[0],[0]
"So we instead form a recursion by looking at the component of the difference perpendicular to the image of the input in that layer, i.e.
∣∣∣∣∣∣δz(d+1)⊥",2.2. Trajectory Length,[0],[0]
"(t)∣∣∣∣∣∣, which results in the condition on x(t) in the statement.
",2.2. Trajectory Length,[0],[0]
"In Figures 4, 12, we see the growth of an input trajectory for ReLU networks on CIFAR-10 and MNIST.",2.2. Trajectory Length,[0],[0]
The CIFAR10 network is convolutional but we observe that these layers also result in similar rates of trajectory length increases to the fully connected layers.,2.2. Trajectory Length,[0],[0]
"We also see, as would be expected, that pooling layers act to reduce the trajectory length.",2.2. Trajectory Length,[0],[0]
"We discuss upper bounds in the Appendix.
",2.2. Trajectory Length,[0],[0]
"For the hard tanh case (and more generally any bounded non-linearity), we can formally prove the relation of trajectory length and transitions under an assumption: assume that while we sweep x(t) all neurons are saturated unless transitioning saturation endpoints, which happens very rapidly.",2.2. Trajectory Length,[0],[0]
(This is the case for e.g. large initialization scales).,2.2. Trajectory Length,[0],[0]
"Then we have:
Theorem 4.",2.2. Trajectory Length,[0],[0]
"Transitions proportional to trajectory length Let FAn,k be a hard tanh network with n hidden layers each of width k. And let
g(k, σw, σb, n) =",2.2. Trajectory Length,[0],[0]
"O  √k√ 1 +
σ2b σ2w
n
Then T (FAn,k(x(t);W ) = O(g(k, σw, σb, n)) for W initialized with weight and bias scales σw, σb.
Note that the expression for g(k, σw, σb, n) is exactly the expression given by Theorem 3 when σw is very large and dominates σb.",2.2. Trajectory Length,[0],[0]
"We can also verify this experimentally in settings where the simpilfying assumption does not hold, as in Figure 5.",2.2. Trajectory Length,[0],[0]
"Here we explore the insights gained from applying our measurements of expressivity, particularly trajectory length, to understand network performance.",3. Insights from Network Expressivity,[0],[0]
"We examine the connection of expressivity and stability, and inspired by this, propose a new method of regularization, trajectory
regularization that offers the same advantages as the more computationally intensive batch normalization.",3. Insights from Network Expressivity,[0],[0]
The analysis of network expressivity offers interesting takeaways related to the parameter and functional stability of a network.,3.1. Expressivity and Network Stability,[0],[0]
"From the proof of Theorem 3, we saw that a perturbation to the input would grow exponentially in the depth of the network.",3.1. Expressivity and Network Stability,[0],[0]
"It is easy to see that this analysis is not limited to the input layer, but can be applied to any layer.",3.1. Expressivity and Network Stability,[0],[0]
"In this form, it would say
A perturbation at a layer grows exponentially in the remaining depth after that layer.
",3.1. Expressivity and Network Stability,[0],[0]
"This means that perturbations to weights in lower layers should be more costly than perturbations in the upper layers, due to exponentially increasing magnitude of noise, and result in a much larger drop of accuracy.",3.1. Expressivity and Network Stability,[0],[0]
"Figure 6, in which we train a conv network on CIFAR-10 and add noise of varying magnitudes to exactly one layer, shows exactly this.
",3.1. Expressivity and Network Stability,[0],[0]
"We also find that the converse (in some sense) holds: after initializing a network, we trained a single layer at different depths in the network and found monotonically increasing performance as layers lower in the network were trained.",3.1. Expressivity and Network Stability,[0],[0]
This is shown in Figure 7 and Figure 17 in the Appendix.,3.1. Expressivity and Network Stability,[0],[0]
"Expressivity measures, especially trajectory length, can also be used to better understand the effect of regulariza-
tion.",3.2. Trajectory Length and Regularization: The Effect of Batch Normalization,[0],[0]
"One regularization technique that has been extremely successful for training neural networks is Batch Normalization (Ioffe and Szegedy, 2015).
",3.2. Trajectory Length and Regularization: The Effect of Batch Normalization,[0],[0]
"By taking measures of trajectories during training we find that without batch norm, trajectory length tends to increase during training, as shown in Figures 8 and Figure 18 in the Appendix.",3.2. Trajectory Length and Regularization: The Effect of Batch Normalization,[0],[0]
"In these experiments, two networks were initialized with σ2w = 2 and trained to high test accuracy on CIFAR10 and MNIST.",3.2. Trajectory Length and Regularization: The Effect of Batch Normalization,[0],[0]
"We see that in both cases, trajectory length increases as training progresses.
",3.2. Trajectory Length and Regularization: The Effect of Batch Normalization,[0],[0]
"A surprising observation is σ2w = 2 is not in the exponential growth increase regime at initialization for the CIFAR10
architecture (Figure 8 at Step 0.).",3.2. Trajectory Length and Regularization: The Effect of Batch Normalization,[0],[0]
"But note that even with a smaller weight initialization, weight norms increase during training, shown in Figure 9, pushing typically initialized networks into the exponential growth regime.
",3.2. Trajectory Length and Regularization: The Effect of Batch Normalization,[0],[0]
"While the initial growth of trajectory length enables greater functional expressivity, large trajectory growth in the learnt representation results in an unstable representation, witnessed in Figure 6.",3.2. Trajectory Length and Regularization: The Effect of Batch Normalization,[0],[0]
"In Figure 10 we train another conv net on CIFAR10, but this time with batch normalization.",3.2. Trajectory Length and Regularization: The Effect of Batch Normalization,[0],[0]
"We see that the batch norm layers reduce trajectory length, helping stability.",3.2. Trajectory Length and Regularization: The Effect of Batch Normalization,[0],[0]
"Motivated by the fact that batch normalization decreases trajectory length and hence helps stability and generalization, we consider directly regularizing on trajectory length: we replace every batch norm layer used in the conv net in Figure 10 with a trajectory regularization layer.",3.3. Trajectory Regularization,[0],[0]
"This layer adds to the loss λ(current length/orig length), and
then scales the outgoing activations by λ, where λ is a parameter to be learnt.",3.3. Trajectory Regularization,[0],[0]
"In implementation, we typically scale the additional loss above with a constant (0.01) to reduce magnitude in comparison to classification loss.",3.3. Trajectory Regularization,[0],[0]
"Our results, Figure 11 show that both trajectory regularization and batch norm perform comparably, and considerably better than not using batch norm.",3.3. Trajectory Regularization,[0],[0]
"One advantage of using Trajectory Regularization is that we don’t require different computations to be performed for train and test, enabling more efficient implementation.",3.3. Trajectory Regularization,[0],[0]
"Characterizing the expressiveness of neural networks, and understanding how expressiveness varies with parameters of the architecture, has been a challenging problem due to the difficulty in identifying meaningful notions of expressivity and in linking their analysis to implications for these networks in practice.",4. Discussion,[0],[0]
"In this paper we have presented an interrelated set of expressivity measures; we have shown tight exponential bounds on the growth of these measures in the depth of the networks, and we have offered a unifying view of the analysis through the notion of trajectory length.",4. Discussion,[0],[0]
"Our analysis of trajectories provides insights for the performance of trained networks as well, suggesting that networks in practice may be more sensitive to small perturbations in weights at lower layers.",4. Discussion,[0],[0]
"We also used this to explore the empirical success of batch norm, and developed a new regularization method – trajectory regularization.
",4. Discussion,[0],[0]
This work raises many interesting directions for future work.,4. Discussion,[0],[0]
"At a general level, continuing the theme of ‘principled deep understanding’, it would be interesting to link
measures of expressivity to other properties of neural network performance.",4. Discussion,[0],[0]
"There is also a natural connection between adversarial examples, (Goodfellow et al., 2014), and trajectory length: adversarial perturbations are only a small distance away in input space, but result in a large change in classification (the output layer).",4. Discussion,[0],[0]
Understanding how trajectories between the original input and an adversarial perturbation grow might provide insights into this phenomenon.,4. Discussion,[0],[0]
"Another direction, partially explored in this paper, is regularizing based on trajectory length.",4. Discussion,[0],[0]
"A very simple version of this was presented, but further performance gains might be achieved through more sophisticated use of this method.",4. Discussion,[0],[0]
"We thank Samy Bengio, Ian Goodfellow, Laurent Dinh, and Quoc Le for extremely helpful discussion.",Acknowledgements,[0],[0]
Here we include the full proofs from sections in the paper.,Appendix,[0],[0]
Proof.,Proof of Theorem 2,[0],[0]
We show inductively that FA(x;W ) partitions the input space into convex polytopes via hyperplanes.,Proof of Theorem 2,[0],[0]
Consider the image of the input space under the first hidden layer.,Proof of Theorem 2,[0],[0]
"Each neuron v(1)i defines hyperplane(s) on the input space: letting W (0)i be the ith row of W (0), b(0)i the bias, we have the hyperplane W (0)",Proof of Theorem 2,[0],[0]
i x + bi = 0 for a ReLU and hyperplanes W (0) i x + bi = ±1 for a hard-tanh.,Proof of Theorem 2,[0],[0]
"Considering all such hyperplanes over neurons in the first layer, we get a hyperplane arrangement in the input space, each polytope corresponding to a specific activation pattern in the first hidden layer.
",Proof of Theorem 2,[0],[0]
"Now, assume we have partitioned our input space into convex polytopes with hyperplanes from layers ≤ d",Proof of Theorem 2,[0],[0]
− 1.,Proof of Theorem 2,[0],[0]
Consider v (d) i and a specific polytope Ri.,Proof of Theorem 2,[0],[0]
Then the activation pattern on layers ≤ d,Proof of Theorem 2,[0],[0]
"− 1 is constant on Ri, and so the input to v (d)",Proof of Theorem 2,[0],[0]
"i
on Ri is a linear function of the inputs ∑ j λjxj + b and some constant term, comprising of the bias and the output of saturated units.",Proof of Theorem 2,[0],[0]
"Setting this expression to zero (for ReLUs) or to ±1 (for hard-tanh) again gives a hyperplane equation, but this time, the equation is only valid in Ri (as we get a different linear function of the inputs in a different region.)",Proof of Theorem 2,[0],[0]
So the defined hyperplane(s) either partition Ri (if they intersect Ri) or the output pattern of v (d) i is also constant on Ri.,Proof of Theorem 2,[0],[0]
"The theorem then follows.
",Proof of Theorem 2,[0],[0]
"This implies that any one dimensional trajectory x(t), that does not ‘double back’ on itself (i.e. reenter a polytope it has previously passed through), will not repeat activation patterns.",Proof of Theorem 2,[0],[0]
"In particular, after seeing a transition (crossing a hyperplane to a different region in input space) we will never return to the region we left.",Proof of Theorem 2,[0],[0]
"A simple example of such a trajectory is a straight line:
Corollary 1.",Proof of Theorem 2,[0],[0]
"Transitions and Output Patterns in an Affine Trajectory For any affine one dimensional trajectory x(t) = x0 + t(x1 − x0) input into a neural network FW ,",Proof of Theorem 2,[0],[0]
we partition R 3 t into intervals every time a neuron transitions.,Proof of Theorem 2,[0],[0]
"Every interval has a unique network activation pattern on FW .
",Proof of Theorem 2,[0],[0]
"Generalizing from a one dimensional trajectory, we can ask how many regions are achieved over the entire input – i.e. how many distinct activation patterns are seen?",Proof of Theorem 2,[0],[0]
"We first prove a bound on the number of regions formed by k hyperplanes in Rm (in a purely elementary fashion, unlike the proof presented in (Stanley, 2011))
",Proof of Theorem 2,[0],[0]
Theorem 5.,Proof of Theorem 2,[0],[0]
Upper Bound on Regions in a Hyperplane Arrangement Suppose we have k hyperplanes in Rm - i.e. k equations of form αix = βi.,Proof of Theorem 2,[0],[0]
"for αi ∈ Rm, βi ∈ R. Let the number of regions (connected open sets bounded on some sides by the hyperplanes) be r(k,m).",Proof of Theorem 2,[0],[0]
"Then
r(k,m) ≤",Proof of Theorem 2,[0],[0]
m∑ i=0,Proof of Theorem 2,[0],[0]
( k i ),Proof of Theorem 2,[0],[0]
Proof.,Proof of Theorem 5,[0],[0]
"Let the hyperplane arrangement be denoted H, and let H ∈ H be one specific hyperplane.",Proof of Theorem 5,[0],[0]
Then the number of regions in H is precisely the number of regions in H − H plus the number of regions in H ∩ H .,Proof of Theorem 5,[0],[0]
"(This follows from the fact that H subdivides into two regions exactly all of the regions inH ∩H , and does not affect any of the other regions.)
",Proof of Theorem 5,[0],[0]
"In particular, we have the recursive formula
r(k,m) =",Proof of Theorem 5,[0],[0]
"r(k − 1,m) + r(k − 1,m− 1)
We now induct on k + m to assert the claim.",Proof of Theorem 5,[0],[0]
"The base cases of r(1, 0) = r(0, 1) = 1 are trivial, and assuming the claim
for ≤ k",Proof of Theorem 5,[0],[0]
"+m− 1 as the induction hypothesis, we have
r(k − 1,m) +",Proof of Theorem 5,[0],[0]
"r(k − 1,m− 1) ≤",Proof of Theorem 5,[0],[0]
m∑ i=0 ( k − 1 i ) + m−1∑,Proof of Theorem 5,[0],[0]
i=0,Proof of Theorem 5,[0],[0]
(,Proof of Theorem 5,[0],[0]
"k − 1 i )
≤ ( k − 1
0
) + d−1∑ i=0 ( k − 1 i ) +",Proof of Theorem 5,[0],[0]
"( k − 1 i+ 1 )
≤ ( k
0
) + m−1∑",Proof of Theorem 5,[0],[0]
"i=0 ( k i+ 1 ) where the last equality follows by the well known identity(
a
b
)",Proof of Theorem 5,[0],[0]
+,Proof of Theorem 5,[0],[0]
"( a
b+ 1
) = ( a+ 1
b+ 1
)
",Proof of Theorem 5,[0],[0]
"This concludes the proof.
",Proof of Theorem 5,[0],[0]
"With this result, we can easily prove Theorem 1 as follows:",Proof of Theorem 5,[0],[0]
Proof.,Proof of Theorem 1,[0],[0]
First consider the ReLU case.,Proof of Theorem 1,[0],[0]
"Each neuron has one hyperplane associated with it, and so by Theorem 5, the first hidden layer divides up the inputs space into r(k,m) regions, with r(k,m) ≤ O(km).
",Proof of Theorem 1,[0],[0]
Now consider the second hidden layer.,Proof of Theorem 1,[0],[0]
"For every region in the first hidden layer, there is a different activation pattern in the first layer, and so (as described in the proof of Theorem 2) a different hyperplane arrangement of k hyperplanes in an m dimensional space, contributing at most r(k,m) regions.
",Proof of Theorem 1,[0],[0]
"In particular, the total number of regions in input space as a result of the first and second hidden layers is ≤ r(k,m) ∗ r(k,m) ≤",Proof of Theorem 1,[0],[0]
O(k2m).,Proof of Theorem 1,[0],[0]
"Continuing in this way for each of the n hidden layers gives the O(kmn) bound.
",Proof of Theorem 1,[0],[0]
"A very similar method works for hard tanh, but here each neuron produces two hyperplanes, resulting in a bound of O((2k)mn).",Proof of Theorem 1,[0],[0]
"Difference of points on trajectory Given x(t) = x, x(t+ dt) = x+ δx in the trajectory, let δz(d) =",B.1. Notation and Preliminary Results,[0],[0]
"z(d)(x+ δx)− z(d)(x)
Parallel and Perpendicular Components:",B.1. Notation and Preliminary Results,[0],[0]
"Given vectors x, y, we can write y = y⊥ + y‖ where y⊥ is the component of y perpendicular to x, and y‖ is the component parallel to x. (Strictly speaking, these components should also have a subscript x, but we suppress it as the direction with respect to which parallel and perpendicular components are being taken will be explicitly stated.)
",B.1. Notation and Preliminary Results,[0],[0]
"This notation can also be used with a matrix W , see Lemma 1.
",B.1. Notation and Preliminary Results,[0],[0]
"Before stating and proving the main theorem, we need a few preliminary results.
",B.1. Notation and Preliminary Results,[0],[0]
Lemma 1.,B.1. Notation and Preliminary Results,[0],[0]
"Matrix Decomposition Let x, y ∈",B.1. Notation and Preliminary Results,[0],[0]
"Rk be fixed non-zero vectors, and let W be a (full rank) matrix.",B.1. Notation and Preliminary Results,[0],[0]
"Then, we can write
W = ‖W‖ + ‖W⊥",B.1. Notation and Preliminary Results,[0],[0]
+,B.1. Notation and Preliminary Results,[0],[0]
"⊥W‖ + ⊥W⊥
such that
‖W⊥x",B.1. Notation and Preliminary Results,[0],[0]
= 0 ⊥W⊥x,B.1. Notation and Preliminary Results,[0],[0]
= 0,B.1. Notation and Preliminary Results,[0],[0]
"yT⊥W‖ = 0 y T⊥W⊥ = 0
i.e. the row space of W is decomposed to perpendicular and parallel components with respect to x (subscript on right), and the column space is decomposed to perpendicular and parallel components of y (superscript on left).
",B.1. Notation and Preliminary Results,[0],[0]
Proof.,B.1. Notation and Preliminary Results,[0],[0]
"Let V,U be rotations such that V x = (||x|| , 0..., 0)T and Uy = (||y|| , 0...0)T .",B.1. Notation and Preliminary Results,[0],[0]
"Now let W̃ = UWV T , and let W̃ = ‖W̃‖ + ‖W̃⊥ + ⊥W̃‖",B.1. Notation and Preliminary Results,[0],[0]
"+
⊥W̃⊥, with ‖W̃‖ having non-zero term exactly W̃11, ‖W̃⊥ having non-zero entries exactly W̃1i for 2 ≤",B.1. Notation and Preliminary Results,[0],[0]
i ≤,B.1. Notation and Preliminary Results,[0],[0]
"k. Finally, we let ⊥W̃‖ have non-zero entries exactly W̃i1, with 2 ≤ i ≤ k",B.1. Notation and Preliminary Results,[0],[0]
"and ⊥W̃⊥ have the remaining entries non-zero.
",B.1. Notation and Preliminary Results,[0],[0]
"If we define x̃ = V x and ỹ = Uy, then we see that
‖W̃⊥x̃ = 0",B.1. Notation and Preliminary Results,[0],[0]
⊥W̃⊥x̃ = 0 ỹT⊥W̃‖,B.1. Notation and Preliminary Results,[0],[0]
"= 0 ỹ T⊥W̃⊥ = 0
as x̃, ỹ have only one non-zero term, which does not correspond to a non-zero term in the components of W̃ in the equations.
",B.1. Notation and Preliminary Results,[0],[0]
"Then, defining ‖W‖ = UT ‖W̃‖V , and the other components analogously, we get equations of the form
‖W⊥x",B.1. Notation and Preliminary Results,[0],[0]
= U T ‖W̃⊥V x = U T ‖W̃⊥x̃,B.1. Notation and Preliminary Results,[0],[0]
"= 0
Observation 1.",B.1. Notation and Preliminary Results,[0],[0]
"Given W,x as before, and considering W‖, W⊥ with respect to x (wlog a unit vector) we can express them directly in terms of W as follows: Letting W (i) be the ith row of W , we have
W‖ =",B.1. Notation and Preliminary Results,[0],[0]
"((W (0))T · x)x
... ((W (k))T · x)x  i.e. the projection of each row in the direction of x.",B.1. Notation and Preliminary Results,[0],[0]
"And of course
W⊥",B.1. Notation and Preliminary Results,[0],[0]
"= W −W‖
The motivation to consider such a decomposition of W is for the resulting independence between different components, as shown in the following lemma.
",B.1. Notation and Preliminary Results,[0],[0]
Lemma 2.,B.1. Notation and Preliminary Results,[0],[0]
Independence of Projections Let x be a given vector (wlog of unit norm.),B.1. Notation and Preliminary Results,[0],[0]
"If W is a random matrix with Wij ∼ N (0, σ2), then W‖ and W⊥ with respect to x are independent random variables.
",B.1. Notation and Preliminary Results,[0],[0]
Proof.,B.1. Notation and Preliminary Results,[0],[0]
"There are two possible proof methods:
(a) We use the rotational invariance of random Gaussian matrices, i.e. if W is a Gaussian matrix, iid entries N (0, σ2), and R is a rotation, then RW is also iid Gaussian, entries N (0, σ2).",B.1. Notation and Preliminary Results,[0],[0]
"(This follows easily from affine transformation rules for multivariate Gaussians.)
",B.1. Notation and Preliminary Results,[0],[0]
Let V be a rotation as in Lemma 1.,B.1. Notation and Preliminary Results,[0],[0]
"Then W̃ = WV T is also iid Gaussian, and furthermore, W̃‖ and W̃⊥ partition the entries of W̃ , so are evidently independent.",B.1. Notation and Preliminary Results,[0],[0]
"But then W‖ = W̃‖V T and W⊥ = W̃⊥V T are also independent.
",B.1. Notation and Preliminary Results,[0],[0]
(b) From the observation note that W‖ and W⊥ have a centered multivariate joint Gaussian distribution (both consist of linear combinations of the entries Wij in W .),B.1. Notation and Preliminary Results,[0],[0]
So it suffices to show that W‖ and W⊥ have covariance 0.,B.1. Notation and Preliminary Results,[0],[0]
"Because both are centered Gaussians, this is equivalent to showing E(< W‖,W⊥ >) = 0.",B.1. Notation and Preliminary Results,[0],[0]
"We have that
E(< W‖,W⊥ >) = E(W‖WT⊥ ) = E(W‖WT )",B.1. Notation and Preliminary Results,[0],[0]
"− E(W‖WT‖ )
",B.1. Notation and Preliminary Results,[0],[0]
"As any two rows of W are independent, we see from the observation that E(W‖WT ) is a diagonal matrix, with the ith diagonal entry just ((W (0))T · x)2.",B.1. Notation and Preliminary Results,[0],[0]
"But similarly, E(W‖WT‖ ) is also a diagonal matrix, with the same diagonal entries - so the claim follows.
",B.1. Notation and Preliminary Results,[0],[0]
"In the following two lemmas, we use the rotational invariance of Gaussians as well as the chi distribution to prove results about the expected norm of a random Gaussian vector.
Lemma 3.",B.1. Notation and Preliminary Results,[0],[0]
Norm of a Gaussian vector Let X ∈,B.1. Notation and Preliminary Results,[0],[0]
"Rk be a random Gaussian vector, with Xi iid, ∼ N (0, σ2).",B.1. Notation and Preliminary Results,[0],[0]
"Then
E [||X||] = σ",B.1. Notation and Preliminary Results,[0],[0]
"√ 2 Γ((k + 1)/2)
Γ(k/2)
",B.1. Notation and Preliminary Results,[0],[0]
Proof.,B.1. Notation and Preliminary Results,[0],[0]
"We use the fact that if Y is a random Gaussian, and Yi ∼ N (0, 1) then ||Y || follows a chi distribution.",B.1. Notation and Preliminary Results,[0],[0]
"This means that E(||X/σ||) = √ 2Γ((k + 1)/2)/Γ(k/2), the mean of a chi distribution with k degrees of freedom, and the result follows by noting that the expectation in the lemma is σ multiplied by the above expectation.
",B.1. Notation and Preliminary Results,[0],[0]
"We will find it useful to bound ratios of the Gamma function (as appear in Lemma 3) and so introduce the following inequality, from (Kershaw, 1983) that provides an extension of Gautschi’s Inequality.
",B.1. Notation and Preliminary Results,[0],[0]
Theorem 6.,B.1. Notation and Preliminary Results,[0],[0]
An Extension of Gautschi’s Inequality For 0,B.1. Notation and Preliminary Results,[0],[0]
"< s < 1, we have
( x+ s
2
)1−s ≤ Γ(x+ 1)
Γ(x+ s) ≤
( x− 1
2 +
( s+ 1
4
) 1 2 )1−s
We now show:
Lemma 4.",B.1. Notation and Preliminary Results,[0],[0]
"Norm of Projections Let W be a k by k random Gaussian matrix with iid entries ∼ N (0, σ2), and x, y two given vectors.",B.1. Notation and Preliminary Results,[0],[0]
Partition W into components as in Lemma 1 and let x⊥ be a nonzero vector perpendicular to x.,B.1. Notation and Preliminary Results,[0],[0]
"Then
(a)
",B.1. Notation and Preliminary Results,[0],[0]
E,B.1. Notation and Preliminary Results,[0],[0]
[∣∣∣∣⊥W⊥x⊥∣∣∣∣] =,B.1. Notation and Preliminary Results,[0],[0]
"||x⊥||σ√2 Γ(k/2)
",B.1. Notation and Preliminary Results,[0],[0]
"Γ((k − 1)/2 ≥ ||x⊥||σ
√ 2
( k
2 − 3 4 )1/2 (b)",B.1. Notation and Preliminary Results,[0],[0]
If 1A is an identity matrix with non-zeros diagonal entry i iff,B.1. Notation and Preliminary Results,[0],[0]
i ∈,B.1. Notation and Preliminary Results,[0],[0]
A ⊂,B.1. Notation and Preliminary Results,[0],[0]
"[k], and |A| > 2, then
E",B.1. Notation and Preliminary Results,[0],[0]
"[∣∣∣∣1A⊥W⊥x⊥∣∣∣∣] ≥ ||x⊥||σ√2 Γ(|A|/2)
",B.1. Notation and Preliminary Results,[0],[0]
Γ((|A|,B.1. Notation and Preliminary Results,[0],[0]
"− 1)/2) ≥ ||x⊥||σ
√ 2 ( |A| 2 − 3 4 )1/2 Proof.",B.1. Notation and Preliminary Results,[0],[0]
"(a) Let U, V, W̃ be as in Lemma 1.",B.1. Notation and Preliminary Results,[0],[0]
"As U, V are rotations, W̃ is also iid Gaussian.",B.1. Notation and Preliminary Results,[0],[0]
"Furthermore for any fixed W ,
with ã = V a, by taking inner products, and square-rooting, we see that ∣∣∣∣∣∣W̃ ã∣∣∣∣∣∣ = ||Wa||.",B.1. Notation and Preliminary Results,[0],[0]
"So in particular
E [∣∣∣∣⊥W⊥x⊥∣∣∣∣] =",B.1. Notation and Preliminary Results,[0],[0]
"E [∣∣∣∣∣∣⊥W̃⊥x̃⊥∣∣∣∣∣∣]
But from the definition of non-zero entries of ⊥W̃⊥, and the form of x̃⊥ (a zero entry in the first coordinate), it follows that ⊥W̃⊥x̃⊥ has exactly k−1 non zero entries, each a centered Gaussian with variance (k−1)σ2 ||x⊥||2.",B.1. Notation and Preliminary Results,[0],[0]
"By Lemma 3, the expected norm is as in the statement.",B.1. Notation and Preliminary Results,[0],[0]
"We then apply Theorem 6 to get the lower bound.
",B.1. Notation and Preliminary Results,[0],[0]
"(b) First note we can view 1A⊥W⊥ = ⊥1AW⊥. (Projecting down to a random (as W is random) subspace of fixed size |A| = m and then making perpendicular commutes with making perpendicular and then projecting everything down to the subspace.)
",B.1. Notation and Preliminary Results,[0],[0]
"So we can viewW as a randomm by k matrix, and for x, y as in Lemma 1 (with y projected down ontom dimensions), we can again define U, V as k by k and m by m rotation matrices respectively, and W̃ = UWV T , with analogous
properties to Lemma 1.",B.1. Notation and Preliminary Results,[0],[0]
"Now we can finish as in part (a), except that ⊥W̃⊥x̃ may have only m− 1 entries, (depending on whether y is annihilated by projecting down by1A) each of variance (k − 1)σ2 ||x⊥||2.
",B.1. Notation and Preliminary Results,[0],[0]
Lemma 5.,B.1. Notation and Preliminary Results,[0],[0]
"Norm and Translation Let X be a centered multivariate Gaussian, with diagonal covariance matrix, and µ a constant vector.
",B.1. Notation and Preliminary Results,[0],[0]
"E(||X − µ||) ≥ E(||X||)
Proof.",B.1. Notation and Preliminary Results,[0],[0]
"The inequality can be seen intuitively geometrically: as X has diagonal covariance matrix, the contours of the pdf of ||X|| are circular centered at 0, decreasing radially.",B.1. Notation and Preliminary Results,[0],[0]
"However, the contours of the pdf of ||X − µ|| are shifted to be centered around ||µ||, and so shifting back µ to 0 reduces the norm.
",B.1. Notation and Preliminary Results,[0],[0]
A more formal proof can be seen as follows: let the pdf of X be fX(·).,B.1. Notation and Preliminary Results,[0],[0]
"Then we wish to show∫ x ||x− µ|| fX(x)dx ≥ ∫ x ||x|| fX(x)dx
Now we can pair points x,−x, using the fact that fX(x) = fX(−x) and the triangle inequality on the integrand to get∫ |x|",B.1. Notation and Preliminary Results,[0],[0]
(||x− µ||+ ||−x−,B.1. Notation and Preliminary Results,[0],[0]
µ||) fX(x)dx ≥ ∫,B.1. Notation and Preliminary Results,[0],[0]
|x| ||2x||,B.1. Notation and Preliminary Results,[0],[0]
fX(x)dx = ∫ |x|,B.1. Notation and Preliminary Results,[0],[0]
(||x||+ ||−x||),B.1. Notation and Preliminary Results,[0],[0]
fX(x)dx,B.1. Notation and Preliminary Results,[0],[0]
We use v(d)i to denote the i th neuron in hidden layer d.,B.2. Proof of Theorem,[0],[0]
"We also let x = z(0) be an input, h(d) be the hidden representation at layer d, and φ the non-linearity.",B.2. Proof of Theorem,[0],[0]
The weights and bias are called W (d) and b(d) respectively.,B.2. Proof of Theorem,[0],[0]
"So we have the relations
h(d)",B.2. Proof of Theorem,[0],[0]
=,B.2. Proof of Theorem,[0],[0]
"W (d)z(d) + b(d), z(d+1) = φ(h(d)).",B.2. Proof of Theorem,[0],[0]
"(1)
Proof.",B.2. Proof of Theorem,[0],[0]
We first prove the zero bias case.,B.2. Proof of Theorem,[0],[0]
"To do so, it is sufficient to prove that
E",B.2. Proof of Theorem,[0],[0]
[∣∣∣∣∣∣δz(d+1)(t)∣∣∣∣∣∣] ≥,B.2. Proof of Theorem,[0],[0]
O ( √σk√ σ + k )d+1∣∣∣∣∣∣δz(0)(t)∣∣∣∣∣∣,B.2. Proof of Theorem,[0],[0]
"(**) as integrating over t gives us the statement of the theorem.
",B.2. Proof of Theorem,[0],[0]
"For ease of notation, we will suppress the t in z(d)(t).
",B.2. Proof of Theorem,[0],[0]
"We first write W (d) = W
(d) ⊥ +W (d) ‖
where the division is done with respect to z(d).",B.2. Proof of Theorem,[0],[0]
Note that this means h(d+1) =,B.2. Proof of Theorem,[0],[0]
"W (d)‖ z (d) as the other component annihilates (maps to 0) z(d).
",B.2. Proof of Theorem,[0],[0]
We can also define A W (d) ‖,B.2. Proof of Theorem,[0],[0]
= {i : i ∈,B.2. Proof of Theorem,[0],[0]
"[k], |h(d+1)i | < 1} i.e. the set of indices for which the hidden representation is not saturated.",B.2. Proof of Theorem,[0],[0]
"Letting Wi denote the ith row of matrix W , we now claim that:
EW (d)",B.2. Proof of Theorem,[0],[0]
"[∣∣∣∣∣∣δz(d+1)∣∣∣∣∣∣] = EW (d)‖ EW (d)⊥   ∑ i∈A
W (d) ‖
((W (d) ⊥ )iδz (d) +",B.2. Proof of Theorem,[0],[0]
"(W (d) ‖ )iδz (d))2  1/2  (*)
Indeed, by Lemma 2 we first split the expectation over W (d) into a tower of expectations over the two independent parts of W to get EW (d)",B.2. Proof of Theorem,[0],[0]
"[∣∣∣∣∣∣δz(d+1)∣∣∣∣∣∣] = EW (d)‖ EW (d)⊥ [∣∣∣∣∣∣φ(W (d)δz(d))∣∣∣∣∣∣]
But conditioning on W (d)‖ in the inner expectation gives us h (d+1) and A W (d)
‖ , allowing us to replace the norm over
φ(W (d)δz(d)) with the sum in the term on the right hand side of the claim.
",B.2. Proof of Theorem,[0],[0]
"Till now, we have mostly focused on partitioning the matrix W (d).",B.2. Proof of Theorem,[0],[0]
But we can also set δz(d) = δz(d)‖ + δz (d) ⊥ where the perpendicular and parallel are with respect to z(d).,B.2. Proof of Theorem,[0],[0]
"In fact, to get the expression in (**), we derive a recurrence as below:
EW (d) [∣∣∣∣∣∣δz(d+1)⊥ ∣∣∣∣∣∣] ≥ O ( √ σk√ σ",B.2. Proof of Theorem,[0],[0]
+ k ),B.2. Proof of Theorem,[0],[0]
EW (d),B.2. Proof of Theorem,[0],[0]
"[∣∣∣∣∣∣δz(d)⊥ ∣∣∣∣∣∣]
To get this, we first need to define z̃(d+1)",B.2. Proof of Theorem,[0],[0]
=,B.2. Proof of Theorem,[0],[0]
"1A W
(d) ‖
h(d+1) - the latent vector h(d+1) with all saturated units zeroed out.
",B.2. Proof of Theorem,[0],[0]
"We then split the column space of W (d) = ⊥W (d) + ‖W (d), where the split is with respect to z̃(d+1).",B.2. Proof of Theorem,[0],[0]
"Letting δz(d+1)⊥ be the part perpendicular to z(d+1), and A the set of units that are unsaturated, we have an important relation: Claim ∣∣∣∣∣∣δz(d+1)⊥ ∣∣∣∣∣∣ ≥ ∣∣∣∣∣∣⊥W",B.2. Proof of Theorem,[0],[0]
"(d)δz(d)1A∣∣∣∣∣∣ (where the indicator in the right hand side zeros out coordinates not in the active set.)
",B.2. Proof of Theorem,[0],[0]
"To see this, first note, by definition,
δz (d+1) ⊥ = W (d)δz(d) · 1A − 〈W (d)δz(d) · 1A, ẑ(d+1)〉ẑ(d+1) (1)
where the ·̂ indicates a unit vector.
",B.2. Proof of Theorem,[0],[0]
"Similarly ⊥W (d)δz(d) = W (d)δz(d) − 〈W (d)δz(d), ˆ̃z(d+1)〉ˆ̃z(d+1) (2)
Now note that for any index",B.2. Proof of Theorem,[0],[0]
i ∈,B.2. Proof of Theorem,[0],[0]
"A, the right hand sides of (1) and (2) are identical, and so the vectors on the left hand side agree for all i ∈ A.",B.2. Proof of Theorem,[0],[0]
"In particular,
δz (d+1) ⊥ · 1A = ⊥W",B.2. Proof of Theorem,[0],[0]
"(d)δz(d) · 1A
Now the claim follows easily by noting that ∣∣∣∣∣∣δz(d+1)⊥",B.2. Proof of Theorem,[0],[0]
"∣∣∣∣∣∣ ≥ ∣∣∣∣∣∣δz(d+1)⊥ · 1A∣∣∣∣∣∣.
Returning to (*), we split δz(d) =",B.2. Proof of Theorem,[0],[0]
"δz(d)⊥ + δz (d) ‖ , W (d) ⊥ = ‖W (d) ⊥ + ⊥W",B.2. Proof of Theorem,[0],[0]
"(d) ⊥ (and W (d) ‖ analogously), and after some cancellation, we have
EW (d)",B.2. Proof of Theorem,[0],[0]
"[∣∣∣∣∣∣δz(d+1)∣∣∣∣∣∣] = EW (d)‖ EW (d)⊥   ∑ i∈A
W (d) ‖
( (⊥W
(d) ⊥ + ‖W (d) ⊥ )",B.2. Proof of Theorem,[0],[0]
iδz (d) ⊥ +,B.2. Proof of Theorem,[0],[0]
( ⊥W (d) ‖ + ‖W (d) ‖ ),B.2. Proof of Theorem,[0],[0]
"iδz (d) ‖ )2 1/2 
We would like a recurrence in terms of only perpendicular components however, so we first drop the ‖W (d)⊥ , ‖W (d) ‖ (which can be done without decreasing the norm as they are perpendicular to the remaining terms) and using the above claim, have
EW (d)",B.2. Proof of Theorem,[0],[0]
"[∣∣∣∣∣∣δz(d+1)⊥ ∣∣∣∣∣∣] ≥ EW (d)‖ EW (d)⊥   ∑ i∈A
W (d) ‖
( (⊥W
(d) ⊥ )iδz (d) ⊥ + ( ⊥W (d) ‖ )",B.2. Proof of Theorem,[0],[0]
"iδz (d) ‖ )2 1/2 
",B.2. Proof of Theorem,[0],[0]
"But in the inner expectation, the term ⊥W (d)‖ δz (d) ‖ is just a constant, as we are conditioning on W (d) ‖ .",B.2. Proof of Theorem,[0],[0]
"So using Lemma 5 we have
E W
(d) ⊥   ∑ i∈A
W (d) ‖
( (⊥W
(d) ⊥ )iδz (d) ⊥ +",B.2. Proof of Theorem,[0],[0]
"( ⊥W (d) ‖ )iδz (d) ‖ )2 1/2  ≥ EW (d)⊥   ∑ i∈A
W (d) ‖
( (⊥W
(d) ⊥ )iδz (d) ⊥ )2 1/2 
We can then apply Lemma 4 to get
E W
(d) ⊥   ∑ i∈A
W (d) ‖
( (⊥W
(d) ⊥ )iδz (d) ⊥ )2 1/2  ≥ σ√k√2 √ 2|A",B.2. Proof of Theorem,[0],[0]
W (d) ‖,B.2. Proof of Theorem,[0],[0]
"| − 3 2 E [∣∣∣∣∣∣δz(d)⊥ ∣∣∣∣∣∣]
The outer expectation on the right hand side only affects the term in the expectation through the size of the active set of units.",B.2. Proof of Theorem,[0],[0]
"For ReLUs, p = P(h(d+1)i > 0) and for hard tanh, we have p = P(|h (d+1) i | < 1), and noting that we get a non-zero norm only if |A W (d) ‖",B.2. Proof of Theorem,[0],[0]
"| ≥ 2 (else we cannot project down a dimension), and for |A W (d) ‖",B.2. Proof of Theorem,[0],[0]
"| ≥ 2,
√ 2
√ 2|A
W (d) ‖",B.2. Proof of Theorem,[0],[0]
"| − 3
2 ≥ 1√
2
√ |A
W (d) ‖",B.2. Proof of Theorem,[0],[0]
"|
we get
EW (d)",B.2. Proof of Theorem,[0],[0]
[∣∣∣∣∣∣δz(d+1)⊥ ∣∣∣∣∣∣] ≥ 1√2  k∑ j=2,B.2. Proof of Theorem,[0],[0]
( k j ) pj(1− p)k−j σ√ k √ j E,B.2. Proof of Theorem,[0],[0]
[∣∣∣∣∣∣δz(d)⊥ ∣∣∣∣∣∣],B.2. Proof of Theorem,[0],[0]
"We use the fact that we have the probability mass function for an (k, p) binomial random variable to bound the √ j term:
k∑ j=2",B.2. Proof of Theorem,[0],[0]
( k j ) pj(1− p)k−j σ√ k √ j =,B.2. Proof of Theorem,[0],[0]
− ( k 1 ) p(1− p)k−1 σ√ k + k∑ j=0,B.2. Proof of Theorem,[0],[0]
"( k j ) pj(1− p)k−j σ√ k √ j
= −σ",B.2. Proof of Theorem,[0],[0]
"√ kp(1− p)k−1 + kp · σ√
k k∑ j=1",B.2. Proof of Theorem,[0],[0]
1√ j,B.2. Proof of Theorem,[0],[0]
(,B.2. Proof of Theorem,[0],[0]
"k − 1 j − 1 ) pj−1(1− p)k−j
",B.2. Proof of Theorem,[0],[0]
"But by using Jensen’s inequality with 1/ √ x, we get
k∑ j=1 1√ j",B.2. Proof of Theorem,[0],[0]
( k − 1 j − 1 ),B.2. Proof of Theorem,[0],[0]
pj−1(1− p)k−j ≥ 1√∑k j=1 j,B.2. Proof of Theorem,[0],[0]
( k−1 j−1 ),B.2. Proof of Theorem,[0],[0]
"pj−1(1− p)k−j = 1√ (k − 1)p+ 1
where the last equality follows by recognising the expectation of a binomial(k−1, p) random variable.",B.2. Proof of Theorem,[0],[0]
"So putting together, we get
EW (d)",B.2. Proof of Theorem,[0],[0]
[∣∣∣∣∣∣δz(d+1)⊥ ∣∣∣∣∣∣] ≥ 1√2 ( −σ √ kp(1− p)k−1 + σ ·,B.2. Proof of Theorem,[0],[0]
√ kp√ 1 + (k − 1)p ),B.2. Proof of Theorem,[0],[0]
E,B.2. Proof of Theorem,[0],[0]
"[∣∣∣∣∣∣δz(d)⊥ ∣∣∣∣∣∣] (a)
From here, we must analyse the hard tanh and ReLU cases separately.",B.2. Proof of Theorem,[0],[0]
"First considering the hard tanh case:
To lower bound p, we first note that as h(d+1)i is a normal random variable with variance ≤ σ2, if A ∼ N (0, σ2)
",B.2. Proof of Theorem,[0],[0]
P(|h(d+1)i | < 1),B.2. Proof of Theorem,[0],[0]
≥,B.2. Proof of Theorem,[0],[0]
"P(|A| < 1) ≥ 1
σ",B.2. Proof of Theorem,[0],[0]
√ 2π,B.2. Proof of Theorem,[0],[0]
(,B.2. Proof of Theorem,[0],[0]
"b)
where the last inequality holds for σ ≥ 1 and follows by Taylor expanding e−x2/2 around 0.",B.2. Proof of Theorem,[0],[0]
"Similarly, we can also show that p ≤",B.2. Proof of Theorem,[0],[0]
"1σ .
",B.2. Proof of Theorem,[0],[0]
"So this becomes
E [∣∣∣∣∣∣δz(d+1)∣∣∣∣∣∣] ≥  1√ 2  1 (2π)1/4 √ σk√
σ √ 2π",B.2. Proof of Theorem,[0],[0]
+,B.2. Proof of Theorem,[0],[0]
(k − 1),B.2. Proof of Theorem,[0],[0]
"− √ k
( 1− 1
σ
)k−1E",B.2. Proof of Theorem,[0],[0]
"[∣∣∣∣∣∣δz(d)⊥ ∣∣∣∣∣∣]
=",B.2. Proof of Theorem,[0],[0]
O ( √ σk√ σ,B.2. Proof of Theorem,[0],[0]
+ k ) E,B.2. Proof of Theorem,[0],[0]
"[∣∣∣∣∣∣δz(d)⊥ ∣∣∣∣∣∣]
Finally, we can compose this, to get
E [∣∣∣∣∣∣δz(d+1)∣∣∣∣∣∣] ≥  1√ 2  1 (2π)1/4 √ σk√
σ √ 2π + (k − 1)",B.2. Proof of Theorem,[0],[0]
"− √ k
( 1− 1
σ )k−1d+1 c · ||δx(t)|| (c) with the constant c being the ratio of ||δx(t)⊥|| to ||δx(t)||.",B.2. Proof of Theorem,[0],[0]
"So if our trajectory direction is almost orthogonal to x(t) (which will be the case for e.g. random circular arcs, c can be seen to be ≈ 1 by splitting into components as in Lemma 1, and using Lemmas 3, 4.)
",B.2. Proof of Theorem,[0],[0]
The ReLU case (with no bias) is even easier.,B.2. Proof of Theorem,[0],[0]
"Noting that for random weights, p = 1/2, and plugging in to equation (a), we get
EW (d)",B.2. Proof of Theorem,[0],[0]
[∣∣∣∣∣∣δz(d+1)⊥ ∣∣∣∣∣∣] ≥ 1√2 ( −σ √ k 2k + σ ·,B.2. Proof of Theorem,[0],[0]
√ k√ 2(k + 1) ),B.2. Proof of Theorem,[0],[0]
E,B.2. Proof of Theorem,[0],[0]
"[∣∣∣∣∣∣δz(d)⊥ ∣∣∣∣∣∣] (d)
But the expression on the right hand side has exactly the asymptotic form O(σ √ k/ √ k + 1), and we finish as in (c).
",B.2. Proof of Theorem,[0],[0]
"Result for non-zero bias In fact, we can easily extend the above result to the case of non-zero bias.",B.2. Proof of Theorem,[0],[0]
"The insight is to note that because δz(d+1) involves taking a difference between z(d+1)(t + dt) and z(d+1)(t), the bias term does not enter at all into the expression for δz(d+1).",B.2. Proof of Theorem,[0],[0]
"So the computations above hold, and equation (a) becomes
EW (d)",B.2. Proof of Theorem,[0],[0]
[∣∣∣∣∣∣δz(d+1)⊥ ∣∣∣∣∣∣] ≥ 1√2 ( −σw √ kp(1− p)k−1 + σw · √ kp√ 1 + (k − 1)p ),B.2. Proof of Theorem,[0],[0]
E,B.2. Proof of Theorem,[0],[0]
"[∣∣∣∣∣∣δz(d)⊥ ∣∣∣∣∣∣]
For ReLUs, we require h(d+1)i = w (d+1) i z",B.2. Proof of Theorem,[0],[0]
"(d) i + b (d+1) i > 0 where the bias and weight are drawn from N (0, σ2b ) and N (0, σ2w) respectively.",B.2. Proof of Theorem,[0],[0]
"But with p ≥ 1/4, this holds as the signs for w, b are purely random.",B.2. Proof of Theorem,[0],[0]
"Substituting in and working through results in the same asymptotic behavior as without bias.
",B.2. Proof of Theorem,[0],[0]
"For hard tanh, not that as h(d+1)i is a normal random variable with variance ≤ σ2w",B.2. Proof of Theorem,[0],[0]
"+ σ2b (as equation (b) becomes
P(|h(d+1)i | < 1) ≥ 1√
(σ2w + σ 2 b )",B.2. Proof of Theorem,[0],[0]
"√ 2π
This gives Theorem 3
E",B.2. Proof of Theorem,[0],[0]
[∣∣∣∣∣∣δz(d+1)∣∣∣∣∣∣] ≥ O  σw (σ2w + σ 2 b ) 1/4 ·,B.2. Proof of Theorem,[0],[0]
"√ k√√
σ2w",B.2. Proof of Theorem,[0],[0]
"+ σ 2 b + k
E",B.2. Proof of Theorem,[0],[0]
"[∣∣∣∣∣∣δz(d)⊥ ∣∣∣∣∣∣]
Statement and Proof of Upper Bound for Trajectory Growth for Hard Tanh Replace hard-tanh with a linear coordinate-wise identity map, h(d+1)i = (W
(d)z(d))i + bi.",B.2. Proof of Theorem,[0],[0]
This provides an upper bound on the norm.,B.2. Proof of Theorem,[0],[0]
"We also then recover a chi distribution with k terms, each with standard deviation σw
k 1 2
,
E",B.2. Proof of Theorem,[0],[0]
[∣∣∣∣∣∣δz(d+1)∣∣∣∣∣∣] ≤ √2Γ,B.2. Proof of Theorem,[0],[0]
"((k + 1)/2)
",B.2. Proof of Theorem,[0],[0]
"Γ (k/2)
σw k 1 2 ∣∣∣∣∣∣δz(d)∣∣∣∣∣∣ (2) ≤ σw ( k + 1
k
) 1 2 ∣∣∣∣∣∣δz(d)∣∣∣∣∣∣ , (3)
where the second step follows from (Laforgia and Natalini, 2013), and holds for k > 1.",B.2. Proof of Theorem,[0],[0]
Proof.,Proof of Theorem 4,[0],[0]
"For σb = 0: For hidden layer d < n, consider neuron v(d)1 .",Proof of Theorem 4,[0],[0]
This has as input ∑k i=1W (d−1) i1 z,Proof of Theorem 4,[0],[0]
(d−1) i .,Proof of Theorem 4,[0],[0]
"As we are in the large σ case, we assume that |z(d−1)i | = 1.",Proof of Theorem 4,[0],[0]
"Furthermore, as signs for z (d−1) i and W (d−1) i1 are both completely random, we can also assume wlog that z(d−1)i = 1.",Proof of Theorem 4,[0],[0]
"For a particular input, we can define v (d) 1 as sensitive to v (d−1) i if v (d−1) i transitioning (to
wlog −1) will induce a transition in node v(d)1 .",Proof of Theorem 4,[0],[0]
A sufficient condition for this to happen is if |Wi1| ≥ | ∑ j 6=iWj1|.,Proof of Theorem 4,[0],[0]
"But
X = Wi1 ∼ N (0, σ2/k) and ∑ j 6=iWj1 = Y
′",Proof of Theorem 4,[0],[0]
"∼ N (0, (k− 1)σ2/k).",Proof of Theorem 4,[0],[0]
So we want to compute P(|X| > |Y ′|).,Proof of Theorem 4,[0],[0]
"For ease of computation, we instead look at P(|X| > |Y |), where Y ∼ N (0, σ2).
",Proof of Theorem 4,[0],[0]
But this is the same as computing P(|X|/|Y | > 1) = P(X/Y < −1) + P(X/Y > 1).,Proof of Theorem 4,[0],[0]
"But the ratio of two centered independent normals with variances σ21 , σ 2 2 follows a Cauchy distribution, with parameter σ1/σ2, which in this case is
1/ √ k. Substituting this in to the cdf of the Cauchy distribution, we get that
P ( |X| |Y | > 1 )",Proof of Theorem 4,[0],[0]
"= 1− 2 π arctan( √ k)
Finally, using the identity arctan(x)+arctan(1/x) and the Laurent series for arctan(1/x), we can evaluate the right hand side to be O(1/ √ k).",Proof of Theorem 4,[0],[0]
In particular P ( |X| |Y | > 1 ) ≥ O ( 1√ k ),Proof of Theorem 4,[0],[0]
(c),Proof of Theorem 4,[0],[0]
"This means that in expectation, any neuron in layer d will be sensitive to the transitions of √ k neurons in the layer below.",Proof of Theorem 4,[0],[0]
"Using this, and the fact the while v(d−1)i might flip very quickly from say −1 to 1, the gradation in the transition ensures that neurons in layer d sensitive to v(d−1)i will transition at distinct times, we get the desired growth rate in expectation as follows:
Let T (d) be a random variable denoting the number of transitions in layer d.",Proof of Theorem 4,[0],[0]
"And let T (d)i be a random variable denoting the number of transitions of neuron i in layer d. Note that by linearity of expectation and symmetry, E [ T (d) ]",Proof of Theorem 4,[0],[0]
"= ∑ i E [ T (d) i ] =
kE [ T
(d) 1 ]",Proof of Theorem 4,[0],[0]
"Now, E [ T
(d+1) 1
] ≥",Proof of Theorem 4,[0],[0]
E,Proof of Theorem 4,[0],[0]
"[∑ i 1(1,i)T (d) i ] = kE [ 1(1,1)T (d) 1 ] where 1(1,i) is the indicator function of neuron 1 in layer d+ 1
being sensitive to neuron i in layer d.",Proof of Theorem 4,[0],[0]
"But by the independence of these two events, E [ 1(1,1)T (d) 1 ] = E [ 1(1,1) ]",Proof of Theorem 4,[0],[0]
· E [ T (d) 1 ] .,Proof of Theorem 4,[0],[0]
"But the firt time on the right hand side is O(1/ √ k) by (c), so putting it all together, E [ T
(d+1) 1
] ≥ √ kE [ T
(d) 1
] .
",Proof of Theorem 4,[0],[0]
"Written in terms of the entire layer, we have E [ T (d+1) ]",Proof of Theorem 4,[0],[0]
"≥ √ kE [ T (d) ] as desired.
",Proof of Theorem 4,[0],[0]
"For σb > 0: We replace √ k with √ k(1 + σ2b/σ 2 w), by noting that Y ∼ N (0, σ2w + σ2b ).",Proof of Theorem 4,[0],[0]
"This results in a growth rate of form
O( √ k/ √
1 + σ2b σ2w ).
",Proof of Theorem 4,[0],[0]
B.3.,Proof of Theorem 4,[0],[0]
"Dichotomies: a natural dual
Our measures of expressivity have mostly concentrated on sweeping the input along a trajectory x(t) and taking measures of FA(x(t);W ).",Proof of Theorem 4,[0],[0]
"Instead, we can also sweep the weights W along a trajectory W (t), and look at the consequences (e.g. binary labels – i.e. dichotomies), say for a fixed set of inputs x1, ..., xs.
",Proof of Theorem 4,[0],[0]
"In fact, after random initialization, sweeping the first layer weights is statistically very similar to sweeping the input along a trajectory x(t).",Proof of Theorem 4,[0],[0]
"In particular, letting W ′ denote the first layer weights, for a particular input x0, x0W ′ is a vector, each coordinate is iid, ∼ N (0, ||x0||2σ2w).",Proof of Theorem 4,[0],[0]
"Extending this observation, we see that (providing norms are chosen appropriately), x0W ′ cos(t) +x1W ′",Proof of Theorem 4,[0],[0]
"sin(t) (fixed x0, x1,W ) has the same distribution as x0W ′0 cos(t) +x0W ′",Proof of Theorem 4,[0],[0]
"1 sin(t) (fixed x0,W ′",Proof of Theorem 4,[0],[0]
"0,W ′",Proof of Theorem 4,[0],[0]
"1).
",Proof of Theorem 4,[0],[0]
"So we expect that there will be similarities between results for sweeping weights and for sweeping input trajectories, which we explore through some synthetic experiments, primarily for hard tanh, in Figures 15, 16.",Proof of Theorem 4,[0],[0]
"We find that the proportionality of transitions to trajectory length extends to dichotomies, as do results on the expressive power afforded by remaining depth.
",Proof of Theorem 4,[0],[0]
"For non-random inputs and non-random functions, this is a well known question upper bounded by the Sauer-Shelah lemma (Sauer, 1972).",Proof of Theorem 4,[0],[0]
We discuss this further in Appendix ??.,Proof of Theorem 4,[0],[0]
"In the random setting, the statistical duality of weight sweeping and input sweeping suggests a direct proportion to transitions and trajectory length for a fixed input.",Proof of Theorem 4,[0],[0]
"Furthermore, if the xi ∈ S are sufficiently uncorrelated (e.g. random) class label transitions should occur independently for each xi",Proof of Theorem 4,[0],[0]
"Indeed, we show this in Figure 14.",Proof of Theorem 4,[0],[0]
Here we include additional experiments from Section 3,C. Addtional Experiments from Section 3,[0],[0]
"We propose a new approach to the problem of neural network expressivity, which seeks to characterize how structural properties of a neural network family affect the functions it is able to compute.",abstractText,[0],[0]
"Our approach is based on an interrelated set of measures of expressivity, unified by the novel notion of trajectory length, which measures how the output of a network changes as the input sweeps along a one-dimensional path.",abstractText,[0],[0]
Our findings can be summarized as follows: (1) The complexity of the computed function grows exponentially with depth.,abstractText,[0],[0]
We design measures of expressivity that capture the non-linearity of the computed function.,abstractText,[0],[0]
"Due to how the network transforms its input, these measures grow exponentially with depth.",abstractText,[0],[0]
(2) All weights are not equal (initial layers matter more).,abstractText,[0],[0]
"We find that trained networks are far more sensitive to their lower (initial) layer weights: they are much less robust to noise in these layer weights, and also perform better when these weights are optimized well.",abstractText,[0],[0]
(3) Trajectory Regularization works like Batch Normalization.,abstractText,[0],[0]
"We find that batch norm stabilizes the learnt representation, and based on this propose a new regularization scheme, trajectory regularization.",abstractText,[0],[0]
On the Expressive Power of Deep Neural Networks,title,[0],[0]
"ar X
iv :1
80 2.
03 69
0v 3
[ st
at .M
L ]
1 0
N ov
2 01
tremely successful in the image recognition domain because they ensure equivariance to translations. There have been many recent attempts to generalize this framework to other domains, including graphs and data lying on manifolds. In this paper we give a rigorous, theoretical treatment of convolution and equivariance in neural networks with respect to not just translations, but the action of any compact group. Our main result is to prove that (given some natural constraints) convolutional structure is not just a sufficient, but also a necessary condition for equivariance to the action of a compact group. Our exposition makes use of concepts from representation theory and noncommutative harmonic analysis and derives new generalized convolution formulae.",text,[0],[0]
"One of the most successful neural network architectures is convolutional neural networks (CNNs) (LeCun et al., 1989).",1. Introduction,[0],[0]
"In the image recognition domain, where CNNs were originally conceived, convolution plays two crucial roles.",1. Introduction,[0],[0]
"First, it ensures that in any given layer, exactly the same filters are applied to each part of the image.",1. Introduction,[0],[0]
"Consequently, if the input image is translated, the activations of the network in each layer will translate the same way.",1. Introduction,[0],[0]
"This property is called equivariance (Cohen & Welling, 2016).",1. Introduction,[0],[0]
"Second, in conjunction with pooling, convolution ensures that each neuron’s effective receptive field is a spatially contiguous domain.",1. Introduction,[0],[0]
"As we move higher in the network, these domains generally get larger, allowing the CNN to capture structure in images at multiple different scales.
",1. Introduction,[0],[0]
"1Departments of Statistics and Computer Science, The University of Chicago 2Toyota Technological Institute at Chicago.",1. Introduction,[0],[0]
Correspondence to:,1. Introduction,[0],[0]
"Risi Kondor <risi@cs.uchicago.edu>, Shubhendu Trivedi <shubhendu@ttic.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"Recently, there has been considerable interest in extending neural networks to more exotic types of data, such as graphs or functions on manifolds (Niepert et al., 2016; Defferrard et al., 2016; Duvenaud et al., 2015; Li et al., 2016; Cohen et al., 2018; Monti et al., 2017; Masci et al., 2015).",1. Introduction,[0],[0]
"In these domains, equivariance and multiscale structure are just as important as for images, but finding the right notion of convolution is not obvious.
",1. Introduction,[0],[0]
"On the other hand, mathematics does offer a sweeping generalization of convolution tied in deeply with some fundamental ideas of abstract algebra: if G is a compact group and f and g are two functionsG→ C, then the convolution of f with g is defined
(f ∗ g)(u) = ∫
G
f(uv−1) g(v) dµ(v).",1. Introduction,[0],[0]
"(1)
Note the striking similarity of this formula to the ordinary notion of convolution, except that in the argument of f , u − v has been replaced by the group operation uv−1, and integration is with respect to the Haar measure, µ.
The goal of this paper is to relate (1) to the various looser notions of convolution used in the neural networks literature, and show that several practical neural networks implicitly already take advantange of the above group theoretic concept of convolution.",1. Introduction,[0],[0]
"In particular, we prove the following theorem (paraphrased here for simplicity).
",1. Introduction,[0],[0]
Theorem 1.,1. Introduction,[0],[0]
"A feed forward neural network N is equivariant to the action of a compact group G on its inputs if and only if each layer of N implements a generalized form of convolution derived from (1).
",1. Introduction,[0],[0]
"To the best of our knowledge, this is the first time that the connection between equivariance and convolution in neural networks has been stated at this level of generality.",1. Introduction,[0],[0]
One of the technical challenges in proving our theorem is that the activations in each layer of a neural net correspond to functions on a sequence of spaces acted on byG (called homogeneous spaces or quotient spaces) rather than functions on G itself.,1. Introduction,[0],[0]
"This necessitates a discussion of group convolution that is rather more thoroughgoing than is customary in pure algebra.
",1. Introduction,[0],[0]
"This paper does not present any new algorithms or neural
network architectures.",1. Introduction,[0],[0]
"Rather, its goal is to provide the language for thinking about generalized notions of equivariance and convolution in neural networks, and thereby facilitate the development of future architectures for data with non-trivial symmetries.",1. Introduction,[0],[0]
"To avoid interruptions in the flow of our exposition, we first present the theory in its abstract form, and then illustrate it with examples in Section 6.",1. Introduction,[0],[0]
"For better understanding, the reader might choose to skip back and forth between these sections.",1. Introduction,[0],[0]
"One work that is close in spirit to the present paper but only considers discrete groups is (Ravanbakhsh et al., 2017).",1. Introduction,[0],[0]
"In the following [a] will denote the set {1, 2, . . .",2. Notation,[0],[0]
", a}.",2. Notation,[0],[0]
"Given a set X and a vector space V , LV (X ) will denote the space of functions {f : X → V }.",2. Notation,[0],[0]
A feed-forward neural network consists of some number of “neurons” arranged in L+1 distinct layers.,3. Equivariance in neural networks,[0],[0]
Layer ℓ,3. Equivariance in neural networks,[0],[0]
"= 0 is the input layer, where data is presented to the network, while layer ℓ=L is where the output is read out.",3. Equivariance in neural networks,[0],[0]
Each neuron nℓx (denoting neuron number x in layer ℓ) has an activation f ℓx.,3. Equivariance in neural networks,[0],[0]
"For the input layer, the activations come directly from the data, whereas in higher layers they are computed via a simple function of the activations of the previous layer, such as f ℓx = σ ( bℓx + ∑ y w ℓ x,y f ℓ−1 y ) .",3. Equivariance in neural networks,[0],[0]
"(2) Here, the {bℓx} bias terms and the {wℓx,y} weights are the network’s learnable parameters, while σ is a fixed nonlinear function, such as the ReLU function σ(z) =max(0, z).",3. Equivariance in neural networks,[0],[0]
"In the simplest case, each f ℓx is a scalar, but, in the second half of the paper we consider neural networks with more general, vector or tensor valued activations.
",3. Equivariance in neural networks,[0],[0]
"For the purposes of the following discussion it is actually helpful to take a slightly more abstract view, and, instead of focusing on the individual activations, consider the activations in any given layer collectively as a function f ℓ",3. Equivariance in neural networks,[0],[0]
:,3. Equivariance in neural networks,[0],[0]
"Xℓ → Vℓ, where Xℓ is a set indexing the neurons and Vℓ is a vector space.",3. Equivariance in neural networks,[0],[0]
"Omitting the bias terms in (2) for simplicity, each layer ℓ = 1, 2, . . .",3. Equivariance in neural networks,[0],[0]
", L can then just be thought of as implementing a linear transformation φℓ : LVℓ−1(Xℓ−1) → LVℓ(Xℓ) followed by the pointwise nonlinearity σ.",3. Equivariance in neural networks,[0],[0]
"Our operational definition of neural networks for the rest of this paper will be as follows.
",3. Equivariance in neural networks,[0],[0]
Definition 1.,3. Equivariance in neural networks,[0],[0]
"Let X0, . . .",3. Equivariance in neural networks,[0],[0]
",XL be a sequence of index sets, V0, . . .",3. Equivariance in neural networks,[0],[0]
", VL vector spaces, φ1, . . .",3. Equivariance in neural networks,[0],[0]
", φL linear maps φℓ : LVℓ−1(Xℓ−1) −→ LVℓ(Xℓ), and σℓ :",3. Equivariance in neural networks,[0],[0]
"Vℓ → Vℓ appropriate pointwise nonlinearities, such as the ReLU operator.",3. Equivariance in neural networks,[0],[0]
"The corresponding multilayer feed-forward neural network (MFF-NN) is then a
sequence of maps f0 7→ f1 7→ f2 7→ . . .",3. Equivariance in neural networks,[0],[0]
"7→ fL, where fℓ(x) = σℓ(φℓ(fℓ−1)(x)).
",3. Equivariance in neural networks,[0],[0]
"If we are interested in constructing a neural net for recognizing m × m pixel images, it is tempting to take X0 =",3. Equivariance in neural networks,[0],[0]
"[m]× [m] and define X1, . . .",3. Equivariance in neural networks,[0],[0]
",XL similarly.",3. Equivariance in neural networks,[0],[0]
"However, again for notational simplicity, we extend each of these index sets to the entire integer plane Z2, and simply assume that outside of the square region [m]× [m], f0(x1, x2) = 0.",3. Equivariance in neural networks,[0],[0]
"A traditional convolutional neural network (CNN) is a network of this type where the φℓ functions are constrained to have the special form
φℓ(fℓ−1)(x1, x2) = w∑
u1=1
w∑
u2=1
fℓ−1(x1−u1, x2−u2) χℓ(u1, u2).",3. Equivariance in neural networks,[0],[0]
"(3)
The above function is known as the discrete convolution of f ℓ−1 with the filter χ, and is usually denoted fℓ−1 ∗ χℓ.",3. Equivariance in neural networks,[0],[0]
"In most CNNs the width w of the filters is quite small, on the order of 3 ∼ 10, while the number of layers can be as small as 3 or as large as a few dozen.
",3. Equivariance in neural networks,[0],[0]
"Some of the key features of CNNs are immediately apparent from the convolution formula (3):
1.",3. Equivariance in neural networks,[0],[0]
"The number of parameters in CNNs is much smaller
than in general (fully connected) feed-forward networks, since we only have to learn the w2 numbers defining the χℓ filters rather than O((m
2)2) weights.",3. Equivariance in neural networks,[0],[0]
"2. (3) applies the same filter to every part of the image.
",3. Equivariance in neural networks,[0],[0]
"Therefore, if the networks learns to recognize a certain feature, e.g., eyes, in one part of the image, then it will be able to do so in any other part as well.",3. Equivariance in neural networks,[0],[0]
3.,3. Equivariance in neural networks,[0],[0]
"Equivalently to the above, if the input image is trans-
lated by any vector (t1, t2) (i.e., f 0′(x1, x2)",3. Equivariance in neural networks,[0],[0]
"= f 0(x1− t1, x2 − t2), then all higher layers will translate in exactly the same way.",3. Equivariance in neural networks,[0],[0]
"This property is called equivariance (sometimes covariance) to translations.
",3. Equivariance in neural networks,[0],[0]
"The goal of the present paper is to understand the mathematical generalization of the above properties to other domains, such as graphs, manifolds, and so on.",3. Equivariance in neural networks,[0],[0]
"The jumping off point to our analysis is the observation that the above is a special case of the following scenario.
",3.1. Group actions,[0],[0]
1.,3.1. Group actions,[0],[0]
We have a set X and a function f :,3.1. Group actions,[0],[0]
X → C. 2.,3.1. Group actions,[0],[0]
We have a group G acting on X .,3.1. Group actions,[0],[0]
This means that each g ∈G has a corresponding transformation,3.1. Group actions,[0],[0]
"Tg : X → X , and for any g1, g2 ∈G, Tg2g1 = Tg2 ◦ Tg1 .",3.1. Group actions,[0],[0]
3.,3.1. Group actions,[0],[0]
The action of G on X extends to functions on X by Tg : f 7→ f ′ f ′(Tg(x)),3.1. Group actions,[0],[0]
"= f(x).
",3.1. Group actions,[0],[0]
"In the case of translation invariant image recognition, X = Z2, G is the group of integer translations, which is isomorphic to Z2 (note that this is a very special case, in general
X and G are different objects), the action is
T(t1,t2)(x1, x2) =",3.1. Group actions,[0],[0]
"(x1+ t1, x2+ t2) (t1, t2)∈Z2,
and the corresponding (induced) action on functions is
T : f 7→ f ′ f ′(x1, x2) = f(x1−",3.1. Group actions,[0],[0]
"t1, x2− t2).
",3.1. Group actions,[0],[0]
"We give several other (more interesting) examples of group actions in Section 6, but for now continue with our abstract development.",3.1. Group actions,[0],[0]
"Also note that to simplify notation, in the following, where this does not cause confusion, we will simply write group actions as x 7→ g(x) rather than the more cumbersome x 7→ Tg(x).",3.1. Group actions,[0],[0]
"Most of the actions considered in this paper have the property that taking any x0 ∈X , any other x∈X can be reached by the action of some g ∈G, i.e., x= g(x0).",3.1. Group actions,[0],[0]
"This property is called transitivity, and if the action of G on X is transitive, we say that X is a homogeneous space of G.",3.1. Group actions,[0],[0]
"Equivariance is a concept that applies very broadly, whenever we have a group acting on a pair of spaces and there is a map from functions on one to functions on the other.
",3.2. Equivariance,[0],[0]
Definition 2.,3.2. Equivariance,[0],[0]
"LetG be a group and X1,X2 be two sets with correspondingG-actions
Tg : X1 → X1, T ′g : X2 → X2.
",3.2. Equivariance,[0],[0]
"Let V1 and V2 be vector spaces, and T and T ′ be the induced actions of G on LV1(X1) and LV2(X2).",3.2. Equivariance,[0],[0]
We say that a (linear or non-linear) map φ : LV1(X1) → LV2(X2) is equivariant with the action of G (or G–equivariant for short),3.2. Equivariance,[0],[0]
"if
φ(Tg(f))",3.2. Equivariance,[0],[0]
= T ′ g(φ(f)) ∀f,3.2. Equivariance,[0],[0]
"∈LV1(X1)
for any group element g ∈G.
Equivariance is represented graphically by a so-called commutative diagram, in our case
LV1(X1)",3.2. Equivariance,[0],[0]
"Tg //
φ
LV1(X1)
φ
LV2(X2) T ′",3.2. Equivariance,[0],[0]
"g // LV2(X2)
",3.2. Equivariance,[0],[0]
"We are finally in a position to define the objects that we study in this paper, namely generalized equivariant neural networks.
",3.2. Equivariance,[0],[0]
Definition 3.,3.2. Equivariance,[0],[0]
"Let N be a feed-forward neural network as defined in Definition 1, andG be a group that acts on each index spaceX0, . . .",3.2. Equivariance,[0],[0]
",XL.",3.2. Equivariance,[0],[0]
"Let T0,T1, . . .",3.2. Equivariance,[0],[0]
",TL be the corresponding actions on LV0(X0), . . .",3.2. Equivariance,[0],[0]
", LVL(XL).",3.2. Equivariance,[0],[0]
"We say that N is a G–equivariant feed-forward network if, when the inputs are transformed f0 7→ T0g(f0)",3.2. Equivariance,[0],[0]
"(for any g ∈ G), the activations of the other layers correspondingly transform as fℓ 7→ Tℓg(fℓ).
",3.2. Equivariance,[0],[0]
It is important to note how general the above framework is.,3.2. Equivariance,[0],[0]
"In particular, we have not said whether G and X0, . . .",3.2. Equivariance,[0],[0]
",XL are discrete or continuous.",3.2. Equivariance,[0],[0]
"In any actual implementation of a neural network, the index sets would of course be finite.",3.2. Equivariance,[0],[0]
"However, it has been observed before that in certain cases, specifically whenX0 is an object such as the sphere or other manifold which does not have a discretization that fully takes into account its symmetries, it is easier to describe the situation in terms of abstract “continuous” neural networks than seemingly simpler discrete ones (Cohen et al., 2018).
",3.2. Equivariance,[0],[0]
"Note also that invariance is a special case of equivariance, where Tg = id for all g. In fact, this is another major reason why equivariant architectures are so prevalent in the literature: any equivariant network can be turned into a G– invariant network simply by tacking on an extra layer that is equivariant in this degenerate sense (in practice, this often means either averaging or creating a histogram of the activations of the last layer).",3.2. Equivariance,[0],[0]
"Nowhere is this more important than in graph learning, where it is a hard constraint that whatever representation is learnt by a neural network, it must be invariant to reordering the vertices.",3.2. Equivariance,[0],[0]
"Today’s state of the art solution to this problem are message passing networks (Gilmer et al., 2017), whose invariance behavior we discuss in section 6.",3.2. Equivariance,[0],[0]
"Another architecture that achieves invariance by stacking equivariant layers followed by a final invariant one is that of scattering networks (Mallat, 2012).",3.2. Equivariance,[0],[0]
"According to its usual definition in signal processing, the convolution of two functions f, g : R → R is
(f ∗ g)(x) = ∫ f(x−y) g(y) dy.",4. Convolution on groups and quotient spaces,[0],[0]
"(4)
Intuitively, we can think of f as a template and g as a modulating function (or the other way round, since convolution on R is commutative): we get f ∗ g by a placing a “copy” of f at each point on the x axis, but scaled by the value of g at that point, and superimposing the results.",4. Convolution on groups and quotient spaces,[0],[0]
"The discrete variant of (4) for f, g : Z → R is of course
(f ∗ g)(x) = ∑
y∈Z
f(x− y) g(y), (5)
and both the above formulae have natural generalizations to higher dimensions.",4. Convolution on groups and quotient spaces,[0],[0]
"In particular, (3) is just the two dimensional version of (5) with a limited width filter.
",4. Convolution on groups and quotient spaces,[0],[0]
"What we are interested in for this paper, however, is the much broader generalization of convolution to the case when f and g are functions on a compact group G. As mentioned in the Introduction, this takes the form
(f ∗ g)(u) = ∫
G
f(uv−1) g(v) dµ(v).",4. Convolution on groups and quotient spaces,[0],[0]
"(6)
Note that (6) only differs from (4) in that x−y is replaced by the group operation uv−1, which is not surprising, since the group operation on R in fact is exactly (x, y) 7→ x+y, and the “inverse” of y in the group sense is −y.",4. Convolution on groups and quotient spaces,[0],[0]
"Furthermore, the Haar measure µmakes an appearance.",4. Convolution on groups and quotient spaces,[0],[0]
"At this point, the main reason that we restrict ourselves to compact groups is because this guarantees that µ is essentially unique1.",4. Convolution on groups and quotient spaces,[0],[0]
"The discrete counterpart of (6) for countable (including finite) groups is
(f ∗ g)(u) = ∑
v∈G
f(uv−1) g(v).",4. Convolution on groups and quotient spaces,[0],[0]
"(7)
All these definitions are standard and have deep connections to the algebraic properties of groups.",4. Convolution on groups and quotient spaces,[0],[0]
"In contrast, the various extensions of convolution to homogeneous spaces that we derive below are not often discussed in pure algebra.",4. Convolution on groups and quotient spaces,[0],[0]
"The major complication in neural networks is that X0, . . .",4.1. Convolution on quotient spaces,[0],[0]
",XL (which are the spaces that the f0, . . .",4.1. Convolution on quotient spaces,[0],[0]
", fL activations are defined on) are homogeneous spaces of G, rather than being G itself.",4.1. Convolution on quotient spaces,[0],[0]
"Fortunately, the strong connection between the structure of groups and their homogeneous spaces (see boxed text) allows generalizing convolution to this case as well.",4.1. Convolution on quotient spaces,[0],[0]
"Note that from now on, to keep the exposition as simple as possible, we present our results assuming thatG is countable (or finite).",4.1. Convolution on quotient spaces,[0],[0]
The generalization to continuous groups is straightforward.,4.1. Convolution on quotient spaces,[0],[0]
"We also allow all our functions to be complex valued, because representation theory itself, which is the workhorse behind our results, is easiest to formulate over C.
Definition 4.",4.1. Convolution on quotient spaces,[0],[0]
"Let G be a finite or countable group, X and Y be (left or right) quotient spaces of G, f : X → C, and g : Y → C. We then define the convolution of f with g as
(f ∗ g)(u) = ∑
v∈G
f↑G(uv−1) g↑G(v), u∈G. (8)
1Non-compact groups would also cause trouble because their representation theory is much more involved.",4.1. Convolution on quotient spaces,[0],[0]
"R2, which is the group behind traditional CCNs, is of course not compact.",4.1. Convolution on quotient spaces,[0],[0]
"The reason that it is still amenable to our analysis (with small modifications) is that it belongs to one of a handful of families of exceptional non-compact groups that are easy to handle.
ESSENTIAL DEFINITIONS FOR QUOTIENT SPACES
Certain connections between the structure of a group G and its homogeneous space X are crucial for our exposition.",4.1. Convolution on quotient spaces,[0],[0]
"First, by definition, fixing an “origin” x0 ∈X , any x∈X can be reached as x= g(x0) for some g ∈G.",4.1. Convolution on quotient spaces,[0],[0]
"This allows us to “index” elements of X by elements of G. Since we use this mechanism so often, we introduce the shorthand",4.1. Convolution on quotient spaces,[0],[0]
"[g ]X = g(x0), which hides the dependence on the (arbitrary) choice of x0.",4.1. Convolution on quotient spaces,[0],[0]
"Second, elementary group theory tells us that the set of group elements that fix x0 actually form a subgroup H .",4.1. Convolution on quotient spaces,[0],[0]
"By further elementary results (see Appendix), the set of group elements that map x0 7→ x is a so-called left coset gH",4.1. Convolution on quotient spaces,[0],[0]
:,4.1. Convolution on quotient spaces,[0],[0]
= {gh | h∈H }.,4.1. Convolution on quotient spaces,[0],[0]
The set of all such cosets forms the (left) quotient space G/H .,4.1. Convolution on quotient spaces,[0],[0]
"Therefore, X can be identified with G/H .",4.1. Convolution on quotient spaces,[0],[0]
"Now for each gH coset we may pick a coset representative g′ ∈ gH , and let x denote the representative of the coset of group elements that map x0 to x. Note",4.1. Convolution on quotient spaces,[0],[0]
"that while the map g 7→ [g ]G/H is well defined, the map x 7→ x going in the opposite direction is more arbitary, since it depends on the choice of coset representatives.",4.1. Convolution on quotient spaces,[0],[0]
The right quotient space H\G is similarly defined as the space of right cosets,4.1. Convolution on quotient spaces,[0],[0]
Hg := {hg | h∈H }.,4.1. Convolution on quotient spaces,[0],[0]
"Furthermore, if K is another subgroup of G, we can talk about double cosets HgK = {hgk | h∈H, k ∈K } and the corresponding space H\G/K .",4.1. Convolution on quotient spaces,[0],[0]
"Given f :G→C, we define its projection to X =G/H
f↓X : X → C f↓X (x) = 1 |H | ∑
g∈xH
f(g).
",4.1. Convolution on quotient spaces,[0],[0]
"Conversely, given f : X → C, we define the lifting of f to G f↑G : G→ C f↑G(g) = f([g ]X ).",4.1. Convolution on quotient spaces,[0],[0]
"Projection and lifting to/from right quotient spaces and double quotient spaces is defined analogously.
",4.1. Convolution on quotient spaces,[0],[0]
"This definition includes X =G or Y =G as special cases, since any group is a quotient space of itself with respect to the trivial subgroup H = {e}.
",4.1. Convolution on quotient spaces,[0],[0]
"Definition 4 hides the facts that depending on the choice of X and Y: (a) the summation might only have to extend over a quotient space of G rather than the entire group, (b) the result f ∗g",4.1. Convolution on quotient spaces,[0],[0]
might have symmetries that effectively make it a function on a quotient space rather than G itself (this is exactly what the case will be in generalized convolutional networks).,4.1. Convolution on quotient spaces,[0],[0]
"Therefore we now discuss three special cases.
",4.1. Convolution on quotient spaces,[0],[0]
CASE,4.1. Convolution on quotient spaces,[0],[0]
I: X =G AND Y =G/H,4.1. Convolution on quotient spaces,[0],[0]
When f :,4.1. Convolution on quotient spaces,[0],[0]
G→ C,4.1. Convolution on quotient spaces,[0],[0]
"but g : G/H → C for some subgroup H of G, (8) reduces to
(f ∗ g)(u) = ∑
v∈G
f(uv−1) g↑G(v).
",4.1. Convolution on quotient spaces,[0],[0]
"Plugging u′ = uh into this formula (for any h ∈ H) and changing the variable of summation to w := vh−1 gives
(f ∗ g)(u′) =",4.1. Convolution on quotient spaces,[0],[0]
"∑
v∈G
f(uhv−1) g↑G(v)
= ∑
w∈G
f(uw−1) g↑G(wh).
",4.1. Convolution on quotient spaces,[0],[0]
"However, since w and wh are in the same left H–coset, g↑G(wh) = g↑G(w), so (f ∗ g)(u′) =",4.1. Convolution on quotient spaces,[0],[0]
"(f ∗ g)(u), i.e., f ∗ g is constant on left H–cosets.",4.1. Convolution on quotient spaces,[0],[0]
This makes it natural to interpret f ∗ g,4.1. Convolution on quotient spaces,[0],[0]
as a function on G/H rather than the full group,4.1. Convolution on quotient spaces,[0],[0]
.,4.1. Convolution on quotient spaces,[0],[0]
"Thus, we have the following definition.
",4.1. Convolution on quotient spaces,[0],[0]
"If f : G→C, and g : G/H→C then f ∗g : G/H → C with
(f ∗ g)(x) = ∑
v∈G
f(xv−1) g([v",4.1. Convolution on quotient spaces,[0],[0]
]G/H ).,4.1. Convolution on quotient spaces,[0],[0]
"(9)
CASE II: X =G/H AND Y =H\G",4.1. Convolution on quotient spaces,[0],[0]
"When f : G/H → C, but g : G→ C, (8) reduces to
(f ∗ g)(u) = ∑
v∈G
f↑G(uv−1) g(v).",4.1. Convolution on quotient spaces,[0],[0]
"(10)
This time it is not f ∗ g, but g that shows a spurious symmetry.",4.1. Convolution on quotient spaces,[0],[0]
Letting v′ =,4.1. Convolution on quotient spaces,[0],[0]
"hv (for any h∈H), by the right H–invariance of f↑G, f↑G(uv′−1) = f↑G(uv−1h−1) = f↑G(uv).",4.1. Convolution on quotient spaces,[0],[0]
"Considering that any v can be uniquely written as v = hy, where y is the representative of one of its cosets, while h ∈ H , we get that (10) factorizes in the form
(f ∗ g)(u) = ∑
y∈H\G
f↑G(uy−1) ∑
h∈H
g(hy)
= ∑
y∈H\G
f↑G(uy−1) g̃(y),
where g̃(y) := ∑
h∈H g(hy).",4.1. Convolution on quotient spaces,[0],[0]
"In other words, without loss of generality we can take g to be a function on H\G rather than the full group.
",4.1. Convolution on quotient spaces,[0],[0]
"If f : G/H → C, and g : H\G→ C, then f∗g : G→ C with
(f ∗ g)(u) = |H",4.1. Convolution on quotient spaces,[0],[0]
"| ∑
y∈H\G
f([uy−1",4.1. Convolution on quotient spaces,[0],[0]
]G/H) g(y).,4.1. Convolution on quotient spaces,[0],[0]
"(11)
CASE III: X =G/H AND Y =H\G/K",4.1. Convolution on quotient spaces,[0],[0]
"Finally, we consider the case when f : G/H → C and g : G/K → C for two subgroups H,K of G, which might or might not be the same.",4.1. Convolution on quotient spaces,[0],[0]
"This combines features of the above two cases in the sense that, similarly to Case I, setting u′ = uk for any k ∈K and letting w = vk−1,
(f ∗ g)(u′)",4.1. Convolution on quotient spaces,[0],[0]
=,4.1. Convolution on quotient spaces,[0],[0]
"∑
v∈G
f↑G(u′v−1) g↑G(v) =
= ∑
v∈G
f↑G(ukv−1) g↑G(v) =",4.1. Convolution on quotient spaces,[0],[0]
"∑
w∈G
f↑G(uw−1) g↑G(wk)
",4.1. Convolution on quotient spaces,[0],[0]
"= ∑
w∈G
f↑G(uw−1) g↑G(w) =",4.1. Convolution on quotient spaces,[0],[0]
"(f ∗ g)(u),
showing that f ∗ g is right K–invariant, and therefore can be regarded as a function G/K → C. At the same time, similarly to (10), letting v = hy,
(f ∗ g)(u) = ∑
y∈H\G
f↑G(uy−1) ∑
h∈H
g↑G(hy)
",4.1. Convolution on quotient spaces,[0],[0]
"= ∑
y∈H\G
f↑G(uy−1) g̃(y),
",4.1. Convolution on quotient spaces,[0],[0]
"where g̃(y) := ∑
h∈H g(hy), which is left H–invariant.",4.1. Convolution on quotient spaces,[0],[0]
"Therefore, without loss of generality, we can take g to be a functionH\G/K → C.
If f : G/H → C, and g : H\G/K → C then we define the convolution of f with g as f",4.1. Convolution on quotient spaces,[0],[0]
"∗ g : G/K → C with
(f ∗ g)(x) = |H | ∑
y∈H\G
",4.1. Convolution on quotient spaces,[0],[0]
f([xy−1 ],4.1. Convolution on quotient spaces,[0],[0]
X ) g([y,4.1. Convolution on quotient spaces,[0],[0]
"]H\G/K).
(12)
",4.1. Convolution on quotient spaces,[0],[0]
Since f 7→ f ∗,4.1. Convolution on quotient spaces,[0],[0]
"g is a map from one homogeneous space, X = G/H , to another homogeneous space, Y = H/K , it is this last defintion that will be of most relevance to us in constructing neural networks.",4.1. Convolution on quotient spaces,[0],[0]
"The nature of convolution on homogeneous spaces is further explicated by considering its form in Fourier space (see (Terras, 1999)).",4.2. Relationship to Fourier analysis,[0],[0]
"Recall that the Fourier transform of a function f on a countable group is defined
f̂(ρi) = ∑
u∈G
f(u)ρi(u), i = 0, 1, 2, . . .",4.2. Relationship to Fourier analysis,[0],[0]
", (13)
where ρ0, ρ1, . . .",4.2. Relationship to Fourier analysis,[0],[0]
are matrix valued functions called irreducible representations or irreps of G (see the Appendix for details).,4.2. Relationship to Fourier analysis,[0],[0]
"As expected, the generalization of this to the case when f is a function on G/H , H\G or H\G/K is
f̂(ρi)",4.2. Relationship to Fourier analysis,[0],[0]
=,4.2. Relationship to Fourier analysis,[0],[0]
"∑
u∈G
ρi(u)f↑G(u), i = 1, 2, . . .",4.2. Relationship to Fourier analysis,[0],[0]
".
",4.2. Relationship to Fourier analysis,[0],[0]
"Analogous formulae hold for continuous groups, involving integration with respect to the Haar measure.
",4.2. Relationship to Fourier analysis,[0],[0]
"At first sight it might be surprising that the Fourier transform of a function on a quotient space consists of the same number of matrices of the same sizes as the Fourier transform of a function on G itself, since G/H , H\G or H\G/K are smaller objects than G. This puzzle is resolved by the following proposition, which tells us that in the latter cases, the Fourier matrices have characteristic sparsity patterns.
",4.2. Relationship to Fourier analysis,[0],[0]
Proposition 1.,4.2. Relationship to Fourier analysis,[0],[0]
"Let ρ be an irrep of G, and assume that on restriction to H it decomposes into irreps of H in the form ρ|H = µ1 ⊕ µ2 ⊕ . . .",4.2. Relationship to Fourier analysis,[0],[0]
⊕ µk.,4.2. Relationship to Fourier analysis,[0],[0]
"Let f̂ be the Fourier transform of a function f : G/H → C. Then [f̂(ρ)]∗,j = 0 unless the block at column j in the decomposition of ρ|H is the trivial representation.",4.2. Relationship to Fourier analysis,[0],[0]
"Similarly, if f : H\G → C, then [f̂(ρ)]i,∗ = 0 unless the block of ρ|H at row i is the trivial representation.",4.2. Relationship to Fourier analysis,[0],[0]
"Finally, if f : H\G/K → C, then [f̂(ρ)]i,j = 0 unless the block of ρ|H at row i is the trivial representation of H and the block at column j in the decomposition of ρ|K is the trivial representation of K .
",4.2. Relationship to Fourier analysis,[0],[0]
"Schematically, this proposition implies that in the three different cases, the Fourier matrices have three different forms of sparsity:
G/K H\G H\G/K
Fortuitously, just like in the classical, Euclidean case, convolution also takes on a very nice form in the Fourier domain, even when f or g (or both) are defined on homogeneous spaces.
",4.2. Relationship to Fourier analysis,[0],[0]
Proposition 2 (Convolution theorem on groups).,4.2. Relationship to Fourier analysis,[0],[0]
"Let G be a compact group, H and K subgroups of G, and f, g be complex valued functions on G, G/H , H\G or H\G/K .",4.2. Relationship to Fourier analysis,[0],[0]
"In any combination of these cases,
f̂ ∗g(ρi) = f̂(ρi) ĝ(ρi) (14)
for any given system of irreps RG = {ρ0, ρ1, . . .}.
",4.2. Relationship to Fourier analysis,[0],[0]
"Plugging in matrices with the appropriate sparsity patterns into (19) now gives us an intuitive way of thinking about Cases I–III above.
",4.2. Relationship to Fourier analysis,[0],[0]
CASE,4.2. Relationship to Fourier analysis,[0],[0]
"I: X =G AND Y =G/H Mutiplying a column sparse matrix with a dense matrix from the left gives a column sparse matrix with the same
pattern, therefore f ∗ g is a function on G/H :    
f̂ ∗ g(ρ)
=
 
 
f̂(ρ)
",4.2. Relationship to Fourier analysis,[0],[0]
"×
 
 
ĝ↑G(ρ)
.
",4.2. Relationship to Fourier analysis,[0],[0]
CASE II: X =G/H AND Y =H\G Multiplying a column sparse matrix from the right by another matrix picks out the corresponding rows of the second matrix.,4.2. Relationship to Fourier analysis,[0],[0]
"Therefore, if f is a function on G/H , then w.l.o.g.",4.2. Relationship to Fourier analysis,[0],[0]
"we can take g to be a function on H\G.
 
 
f̂ ∗ g(ρ)
=
 
 
f̂↑G(ρ)
×
 
 
ĝ↑G(ρ)
.
",4.2. Relationship to Fourier analysis,[0],[0]
CASE III: f : G/H → C AND g : H\G/K → C,4.2. Relationship to Fourier analysis,[0],[0]
"Finally, if f is a function on G/H , and we want to make f ∗ g to be a function on G/K , then we should take g : H\G/K:
 
 
f̂ ∗ g(ρ)
=
 
 
f̂↑G(ρ)
×
 
 
ĝ↑G(ρ)
.",4.2. Relationship to Fourier analysis,[0],[0]
"We are finally in a position to define the notion of generalized convolutional networks, and state our main result connecting convolutions and equivariance.
",5. Main result: the connection between convolution and equivariance,[0],[0]
Definition 5.,5. Main result: the connection between convolution and equivariance,[0],[0]
"Let G be a compact group and N an L+1 layer feed-forward network in which the i’th index set is G/Hi for some subgroup Hi of G. We say that N is a G–convolutional neural network (or G-CNN for short) if each of the linear maps φ1, . . .",5. Main result: the connection between convolution and equivariance,[0],[0]
", φL in N is a generalized convolution (see Definition 4) of the form
φℓ(fℓ−1) = fℓ−1 ∗ χℓ
with some filter χℓ ∈LVℓ−1×Vℓ(Hℓ−1\G/Hℓ).
",5. Main result: the connection between convolution and equivariance,[0],[0]
Theorem 1.,5. Main result: the connection between convolution and equivariance,[0],[0]
"Let G be a compact group and N be an L + 1 layer feed-forward neural network in which the ℓ’th index set is of the form Xℓ = G/Hℓ, where Hℓ is some subgroup of G. Then N is equivariant to the action ofG in the sense of Definition 3 if and only if it is a G-CNN.
",5. Main result: the connection between convolution and equivariance,[0],[0]
"Proving this theorem in the forward direction is relatively easy and only requires some elementary facts about cosets and group actions.
",5. Main result: the connection between convolution and equivariance,[0],[0]
Proof of Theorem 1 (forward direction).,5. Main result: the connection between convolution and equivariance,[0],[0]
"Assume that we translate fℓ−1 by some group element g ∈G and get f ′ℓ−1, i.e., f ′ℓ−1 = T ℓ−1 g (fℓ−1), where f ′ ℓ−1(x) = fℓ−1(g
−1x).",5. Main result: the connection between convolution and equivariance,[0],[0]
"Then
φℓ(f ′",5. Main result: the connection between convolution and equivariance,[0],[0]
"ℓ−1)(u) = (f ′ ℓ−1 ∗ χℓ)(u)
= ∑
v∈G
f ′ℓ−1([uv −1]X )χℓ(v)
= ∑
v∈G
fℓ−1(g −1([uv−1]X ))χℓ(v).
",5. Main result: the connection between convolution and equivariance,[0],[0]
By g−1([uv−1]X ),5. Main result: the connection between convolution and equivariance,[0],[0]
=,5. Main result: the connection between convolution and equivariance,[0],[0]
"[g −1uv−1]X this is further equal to
∑
v∈G
fℓ−1([g −1uv−1]X )χℓ(v)
= (fℓ−1 ∗ χℓ)(g−1u) = φℓ(fℓ−1)(g−1u).
",5. Main result: the connection between convolution and equivariance,[0],[0]
"Therefore, φℓ(fℓ−1) is equivariant with fℓ−1.",5. Main result: the connection between convolution and equivariance,[0],[0]
"Since σℓ is a pointwise operator, so is fℓ = σℓ(φℓ(fℓ−1)).",5. Main result: the connection between convolution and equivariance,[0],[0]
"By induction on ℓ, using the transitivity of equivariance, this implies that every layer of N is equivariant with layer 0.",5. Main result: the connection between convolution and equivariance,[0],[0]
"Note that this proof holds not only in the base case, when each fℓ is a function X → C, but also in the more general case when fℓ :",5. Main result: the connection between convolution and equivariance,[0],[0]
"Xℓ → Vℓ and the filters are χℓ : Xℓ → Vℓ−1 × Vℓ.
Proving the “only if” part of Theorem 1 is more technical, therefore we leave it to the Appendix.",5. Main result: the connection between convolution and equivariance,[0],[0]
We are not aware of any prior papers that have exposed the above algebraic theory of equivariance and convolution in its full generality.,6. Examples of algebraic convolution in neural networks,[0],[0]
"However, there are a few recent publications that implicitly exploit these ideas in specific contexts.",6. Examples of algebraic convolution in neural networks,[0],[0]
In image recognition applications it is a natural goal to achieve equivariance to both translation and rotation.,6.1. Rotation equivariant networks,[0],[0]
"The most common approach is to use CNNs, but with filters that are replicated at a certain number of rotational angles (typically multiples of 90 degrees), connected in such as a way as to achieve a generalization of equivariance called steerability.",6.1. Rotation equivariant networks,[0],[0]
"Steerability also has a group theoretic interpretation, which is most lucidly explained in (Cohen & Welling, 2017).
",6.1. Rotation equivariant networks,[0],[0]
"The recent papers (Marcos et al., 2017) and (Worrall et al., 2017) extend these architectures by considering continuous rotations at each point of the visual field.",6.1. Rotation equivariant networks,[0],[0]
"Thus, putting
aside the steerability aspect for now and only considering the behavior of the network at a single point, both these papers deal with the case where G = SO(2) (the two dimensional rotation group) and X is the circle S1.",6.1. Rotation equivariant networks,[0],[0]
"The group SO(2) is commutative, therefore its irreducible representations are one dimensional, and are, in fact, ρj(θ)",6.1. Rotation equivariant networks,[0],[0]
"= e 2πιjθ ,
where ι = √ −1.",6.1. Rotation equivariant networks,[0],[0]
"While not calling it a group Fourier transform, Worrall et al. (2017) explicitly expand the local activations in this basis and scale them with weights, which, by virtue of Proposition 2, amounts to convolution on the group, as prescribed by our main theorem.
",6.1. Rotation equivariant networks,[0],[0]
"The form of the nonlinearity in (Worrall et al., 2017) is different from that prescribed in Definition 3, which leads to a coupling between the indices of the Fourier components in any path from the input layer to the output layer.",6.1. Rotation equivariant networks,[0],[0]
"This is compensated by what they call their “equivariance condition”, asserting that only Fourier components for which M = ∑ ℓ jℓ is the same may mix.",6.1. Rotation equivariant networks,[0],[0]
"This restores equivariance in the last layer, but analyzing it group theoretically is beyond the scope of the present paper.",6.1. Rotation equivariant networks,[0],[0]
"Very close in spirit to our present exposition are the recent papers (Cohen et al., 2018; Kondor et al., 2018), which propose convolutional architectures for recognizing images painted on the sphere, satisfying equivariance with respect to rotations.",6.2. Spherical CNNs,[0],[0]
"Thus, in this case, G = SO(3), the group of three dimensional rotations, and Xℓ is the sphere, S2.",6.2. Spherical CNNs,[0],[0]
The case of rotations acting on the sphere is one of the textbook examples of continuous group actions.,6.2. Spherical CNNs,[0],[0]
"In particular, letting x0 be the North pole, we see that two-dimensional rotations in the x–z plane fix x0, therefore, S 2 is identified with the quotient space SO(3)/SO(2).",6.2. Spherical CNNs,[0],[0]
The irreducible representations of SO(3) are given by the so-called Wigner matrices.,6.2. Spherical CNNs,[0],[0]
"The ℓ’th irreducible representation is 2ℓ+1 dimensional and of the form
[ρℓ(θ, φ, ψ)]m,m′",6.2. Spherical CNNs,[0],[0]
"= e −ιm′φ dℓm′,m(θ)",6.2. Spherical CNNs,[0],[0]
"e −ιmψ,
where m,m′ ∈",6.2. Spherical CNNs,[0],[0]
"{−ℓ, . . .",6.2. Spherical CNNs,[0],[0]
", ℓ}, (θ, φ, ψ) are the Euler angles of the rotation and the dℓm′,m(θ) funcion is related to the spherical harmonics.",6.2. Spherical CNNs,[0],[0]
"It is immediately clear that on restriction to SO(2) (corresponding to θ, φ = 0) only the middle column in each of these matrices reduces to the trivial representation of SO(2), therefore, by Proposition 1, in the case f : SO(3)/SO(2) → C, only the middle column of each f̂(ρℓ) matrix will be nonzero.",6.2. Spherical CNNs,[0],[0]
"In fact, up to constant scaling factors, the entries in that middle column are just the customary spherical harmonic expansion coefficients.
",6.2. Spherical CNNs,[0],[0]
"Cohen et al. (2018) explicitly make this connection between spherical harmonics and SO(3) Fourier transforms, and store the activations in terms of this representation.",6.2. Spherical CNNs,[0],[0]
"Moreover, just like in the present paper, they define convo-
lution in terms of the noncommutative convolution theorem (Proposition 2), use pointwise nonlinearities, and prove that the resulting neural network is SO(3)–equivariant.",6.2. Spherical CNNs,[0],[0]
"However, they do not prove the converse, i.e., that equivariance implies that the network must be convolutional.",6.2. Spherical CNNs,[0],[0]
"To apply the nonlinearity, the algorithm presented in (Cohen et al., 2018) requires repeated forward and backward SO(3) fast Fourier transforms.",6.2. Spherical CNNs,[0],[0]
"While this leads to a non-conventional architecture, the discussion echoes our observation that when dealing with continuous symmetries such as rotations, one must generalize to more abstract “continuous” neural networks, as afforded by Definition 3.",6.2. Spherical CNNs,[0],[0]
"There has been considerable interest in extending the convolutional network formalism to learning from graphs (Niepert et al., 2016; Defferrard et al., 2016; Duvenaud et al., 2015), and the current consensus for approaching this problem is to use neural networks based on the message passing idea (Gilmer et al., 2017).",6.3. Message passing neural networks,[0],[0]
Let G be a graph with n vertices.,6.3. Message passing neural networks,[0],[0]
"Message passing neural networks (MPNNs) are usually presented in terms of an iterative process, where in each round ℓ, each vertex v collects the labels of its neighbors w1, . . .",6.3. Message passing neural networks,[0],[0]
", wk, and updates its own label f̃v according to a simple formula such as
f̃ ℓv = Φ ( f̃ ℓ−1w1 + . .",6.3. Message passing neural networks,[0],[0]
".+ f̃ ℓ−1 wk ) .
",6.3. Message passing neural networks,[0],[0]
"An equivalent way of seeing this process, however, is in terms of the “effective receptive fields” Sℓv of each vertex at round ℓ, i.e., the set of all vertices from which information can propagate to v by round ℓ.
MPNNs can also be viewed as group convolutional networks.",6.3. Message passing neural networks,[0],[0]
"A receptive field of size k is just a subset {s1, . . .",6.3. Message passing neural networks,[0],[0]
", sk} ⊆ {1, 2, . . .",6.3. Message passing neural networks,[0],[0]
",",6.3. Message passing neural networks,[0],[0]
"n}, and the symmetric group Sn (the group of permutations of {1, 2, . .",6.3. Message passing neural networks,[0],[0]
.,6.3. Message passing neural networks,[0],[0]
",",6.3. Message passing neural networks,[0],[0]
"n}) acts on the set of such subsets transitively by {s1, . . .",6.3. Message passing neural networks,[0],[0]
", sk} σ7→ {σ(s1), . . .",6.3. Message passing neural networks,[0],[0]
", σ(sk)} σ ∈ Sn.",6.3. Message passing neural networks,[0],[0]
"Since permuting the n− k vertices not in S amongst themselves, as well as permuting the k vertices that are in S both leave S invariant, the stablizier of this action is Sn−k × Sk.",6.3. Message passing neural networks,[0],[0]
"Thus, the set of all k-subsets of vertices is identified with the quotient space X = Sn/(Sk × Sn−k), and the labeling function for k-element receptive fields is identified with fk : X → C. Effectively, this turns the MPNN into a generalized feed-forward network in the sense of Definition 3.",6.3. Message passing neural networks,[0],[0]
Note that fk is a redundant representation of the labeling function because Sn/(Sk×Sn−k) also includes subsets that do not correspond to contiguous neighborhoods.,6.3. Message passing neural networks,[0],[0]
However this is not a problem because for such S we simply set fk(S) = 0.,6.3. Message passing neural networks,[0],[0]
"The key feature of the message passing formalism is that, by construction, it ensures that the f̃ ℓv labels only depend
on the graph topology and are invariant to renumbering the vertices of G.",6.3. Message passing neural networks,[0],[0]
In terms of our “k–subset network” this means that each fk must be Sn–equivariant.,6.3. Message passing neural networks,[0],[0]
"Thus, in contrast to the previous two examples, now each index set Xℓ = Sn/(Sn−ℓ × Sℓ) is different.",6.3. Message passing neural networks,[0],[0]
The form of the corresponding convolutions LVℓ−1(Xℓ−1) → LVℓ(Xℓ) are best described in the Fourier domain.,6.3. Message passing neural networks,[0],[0]
"Unfortunately, the representation theory of symmetric groups is beyond the scope of the present paper (Sagan, 2001).",6.3. Message passing neural networks,[0],[0]
"We content ourselves by stating that the irreps of Sn are indexed by so-called integer partitions, (λ1, . . .",6.3. Message passing neural networks,[0],[0]
", λm), where λ1 ≥ . . .",6.3. Message passing neural networks,[0],[0]
≥ λm and∑,6.3. Message passing neural networks,[0],[0]
"i λi = n. Moreover, the structure of the Fourier transform of a function f :",6.3. Message passing neural networks,[0],[0]
"Sn/(Sn−ℓ × Sℓ) dictated by Proposition 1 in this case is that each of the Fourier matrices are zero except for a single column in each of the f̂((n−p, p)) components, where 0 ≤ p ≤",6.3. Message passing neural networks,[0],[0]
ℓ. The main theorem of our paper dictates that the linear map φℓ in each layer must be a convolution.,6.3. Message passing neural networks,[0],[0]
"In the case of Fourier matrices with such extreme sparsity structure, this means that each of the ℓ+1 Fourier matrices can be multiplied by a scalar, χℓp.",6.3. Message passing neural networks,[0],[0]
These are the learnable parameters of the network.,6.3. Message passing neural networks,[0],[0]
"A real MPNN of course has multiple channels and various corresponding parameters, which could also be introduced in the k– subset network.",6.3. Message passing neural networks,[0],[0]
"The above observation about the form of χℓ is nonetheless interesting, because it at once implies that permutation equivariance is a severe constraint the significantly limits the form of the convolutional filters, yet the framework is still richer than traditional MPNNs where the labels of the neighbors are simply summed.",6.3. Message passing neural networks,[0],[0]
Convolution has emerged as one of the key organizing principles of deep neural network architectures.,7. Conclusions,[0],[0]
"Nonetheless, depending on their background, the word “convolution” means different things to different researchers.",7. Conclusions,[0],[0]
"The goal of this paper was to show that in the common setting when there is a group acting on the data that the architecture must be equivariant to, convolution has a specific mathematical meaning that has far reaching consequences: we proved that a feed forward network is equivariant to the group action if and only if it respects this notion of convolution.
",7. Conclusions,[0],[0]
"Our theory gives a clear prescription to practitioners on how to design neural networks for data with non-trivial symmetries, such as data on the sphere, etc..",7. Conclusions,[0],[0]
"In particular, we argue for Fourier space representations, similar to those that have appeared in (Worrall et al., 2017; Cohen et al., 2018; Kondor et al., 2018)), and, even more recently, since the submission of the original version of the present paper in (Thomas et al., 2018; Kondor, 2018; Weiler et al., 2018).",7. Conclusions,[0],[0]
This work was supported in part by DARPA Young Faculty Award D16AP00112.,Acknowledgements,[0],[0]
"For a more detailed background on representation theory, we point the reader to Serre, 1977.
Groups.",A. Background from group and representation theory,[0],[0]
"A group is a set G endowed with an operation G×G→ G (usually denoted multiplicatively) obeying the following axioms:
G1.",A. Background from group and representation theory,[0],[0]
"for any g1, g2 ∈G, g1g2",A. Background from group and representation theory,[0],[0]
"∈G (closure);
G2.",A. Background from group and representation theory,[0],[0]
"for any g1, g2, g3 ∈G, g1(g2g3)",A. Background from group and representation theory,[0],[0]
= (g1g2)g3 (associativity); G3.,A. Background from group and representation theory,[0],[0]
"there is a unique e∈G, called the identity ofG, such that eg = ge = g for any u∈G; G4. for any g ∈G, there is a corresponding element g−1∈ G called the inverse of g, such that gg−1 = g−1g = e.
We do not require that the group operation be commutative, i.e., in general, g1g2",A. Background from group and representation theory,[0],[0]
6= g2g1.,A. Background from group and representation theory,[0],[0]
"Groups can be finite or infinite, countable or uncountable, compact or non-compact.",A. Background from group and representation theory,[0],[0]
"While most of the results in this paper would generalize to any compact group, to keep the exposition as simple as possible, throughout we assume that G is finite or countably infinite.",A. Background from group and representation theory,[0],[0]
"As usual, |G| will denote the size (cardinality) of G, sometimes also called the order of the group.",A. Background from group and representation theory,[0],[0]
"A subset H of G is called a subgroup of G, denoted H ≤ G, if H itself forms a group under the same operation as G, i.e., if for any g1, g2 ∈H , g1g2 ∈H .
",A. Background from group and representation theory,[0],[0]
"Homogeneous Spaces.
",A. Background from group and representation theory,[0],[0]
Definition 6.,A. Background from group and representation theory,[0],[0]
Let G be a group acting on a set X .,A. Background from group and representation theory,[0],[0]
"We say that X is a homogeneous space of G if for any x, y ∈ X , there is a g ∈G such that y= g(x).
",A. Background from group and representation theory,[0],[0]
"The significance of homogeneous spaces for our purposes is that once we fix the “origin” x0, the above correspondence between points in X and the group elements that map x0 to them allows to lift various operations on the homogeneous space to the group.",A. Background from group and representation theory,[0],[0]
"Because expressions like g(x0) appear so often in the following, we introduce the shorthand",A. Background from group and representation theory,[0],[0]
[g]X :=g(x0).,A. Background from group and representation theory,[0],[0]
"Note that this hides the dependency on the (arbitrary) choice of x0.
",A. Background from group and representation theory,[0],[0]
"For some examples, we see that Z2 is a homogeneous space of itself with respect to the trivial action (i, j) 7→ (g1+i, g2+ j), and the sphere is a homogeneous space of the rotation group with respect to the action:
x 7→",A. Background from group and representation theory,[0],[0]
R(x),A. Background from group and representation theory,[0],[0]
R(x),A. Background from group and representation theory,[0],[0]
=,A. Background from group and representation theory,[0],[0]
"Rx x∈S2, (15)
",A. Background from group and representation theory,[0],[0]
"On the other hand, the entries of the adjacency matrix are not a homogeneous space of Sn with respect to
(i, j) 7→ (σ(i), σ(j))",A. Background from group and representation theory,[0],[0]
σ ∈,A. Background from group and representation theory,[0],[0]
Sn.,A. Background from group and representation theory,[0],[0]
"(16)
, because if we take some (i, j) with i 6= j, then 16 can map it to any other (i′, j′) with i′ 6= j′, but not to any of the diagonal elements, where i′ = j′. If we split the matrix into its “diagonal”, and “off-diagonal” parts, individually these two parts are homogeneous spaces.
",A. Background from group and representation theory,[0],[0]
Representations.,A. Background from group and representation theory,[0],[0]
"A (finite dimensional) representation of a group G over a field F is a matrix-valued function ρ : G → Fdρ×dρ such that ρ(g1)ρ(g2) = ρ(g1g2) for any
g1,",A. Background from group and representation theory,[0],[0]
g2 ∈ G.,A. Background from group and representation theory,[0],[0]
"In this paper, unless stated otherwise, we always assume that F = C. A representation ρ is said to be unitary if ρ(g−1) = ρ(g)† for any g ∈ G. One representation shared by every group is the trivial representation ρtr that simply evaluates to the one dimensional matrix ρtr(g) = (1) on every group element.
",A. Background from group and representation theory,[0],[0]
"Equivalence, reducibility and irreps.",A. Background from group and representation theory,[0],[0]
"Two representations ρ and ρ′ of the same dimensionality d are said to be equivalent if for some invertible matrix Q ∈ Cd×d, ρ(g) = Q−1ρ′(g)Q for any g ∈ G.",A. Background from group and representation theory,[0],[0]
"A representation ρ is said to be reducible if it decomposes into a direct sum of smaller representations in the form
ρ(g)
= Q−1 (ρ1(g)⊕ρ2(g))",A. Background from group and representation theory,[0],[0]
Q = Q−1 ( ρ1(g) 0 0 ρ2(g) ),A. Background from group and representation theory,[0],[0]
Q ∀,A. Background from group and representation theory,[0],[0]
"g ∈G
for some invertible matrix Q ∈ Cdρ×dρ .",A. Background from group and representation theory,[0],[0]
We use RG to denote a complete set of inequivalent irreducible representations ofG.,A. Background from group and representation theory,[0],[0]
"However, since this is quite a mouthful, in this paper we also use the alternative term system of irreps to refer to RG.",A. Background from group and representation theory,[0],[0]
"Note that the choice of irreps in RG is far from unique, since each ρ ∈ RG can be replaced by an equivalent irrep Q⊤ρ(g)Q, where Q is any orthogonal matrix of the appropriate size.
",A. Background from group and representation theory,[0],[0]
Complete reducibility and irreps.,A. Background from group and representation theory,[0],[0]
Representation theory takes on its simplest form when G is compact (and F = C).,A. Background from group and representation theory,[0],[0]
"One of the reasons for this is that it is possible to prove (“theorem of complete reducibility”) that any representation ρ of a compact group can be reduced into a direct sum of irreducible ones, i.e.,
ρ(g) = Q−1",A. Background from group and representation theory,[0],[0]
( ρ(1)(g)⊕ρ(2)(g)⊕ . .,A. Background from group and representation theory,[0],[0]
.⊕,A. Background from group and representation theory,[0],[0]
ρ(k)(g) ),A. Background from group and representation theory,[0],[0]
"Q, g ∈G
(17)
for some sequence ρ(1), ρ(2), . . .",A. Background from group and representation theory,[0],[0]
", ρ(k)",A. Background from group and representation theory,[0],[0]
of irreducible representations of G and some Q ∈ Cd×d.,A. Background from group and representation theory,[0],[0]
"In this sense, for compact groups, RG plays a role very similar to the primes in arithmetic.",A. Background from group and representation theory,[0],[0]
"Fixing RG, the number of times that a particular ρ′ ∈ RG appears in (17) is a well-defined quantity called the multiplicity of ρ′ in ρ, denoted mρ(ρ
′).",A. Background from group and representation theory,[0],[0]
"Compactness also has a number of other advantages:
1.",A. Background from group and representation theory,[0],[0]
"When G is compact, RG is a countable set, therefore we can refer to the individual irreps as ρ1, ρ2, . .",A. Background from group and representation theory,[0],[0]
..,A. Background from group and representation theory,[0],[0]
"(WhenG is finite, RG is not only countable but finite.)",A. Background from group and representation theory,[0],[0]
2.,A. Background from group and representation theory,[0],[0]
"The system of irreps of a compact group is essentially
unique in the sense that if R′G is any other system of irreps, then there is a bijection φ : RG → R′G mapping each irrep ρ∈RG to an equivalent irrep φ(ρ)∈R′G. 3.",A. Background from group and representation theory,[0],[0]
"When G is compact, RG can be chosen in such a way that each ρ∈R is unitary.
",A. Background from group and representation theory,[0],[0]
Restricted representations.,A. Background from group and representation theory,[0],[0]
"Given any representation ρ of G and subgroup H ≤ G, the restriction of ρ to H is defined as the function ρ|H : H → Cdρ×dρ , where ρ|H(h) = ρ(h) for all h ∈ H .",A. Background from group and representation theory,[0],[0]
"It is trivial to check that ρ|H is a representation of H , but, in general, it is not irreducible (even when ρ itself is irreducible).
",A. Background from group and representation theory,[0],[0]
Fourier Transforms.,A. Background from group and representation theory,[0],[0]
"In the Euclidean domain convolution and cross-correlation have close relationships with the Fourier transform
f̂(k) = ∫ e−2πιkx f(x) dx, (18)
where ι is the imaginary unit, √ −1.",A. Background from group and representation theory,[0],[0]
"In particular, the Fourier transform of f ∗ g is just the pointwise product of the Fourier transforms of f and g,
f̂ ∗ g(k) = f̂(k) ĝ(k), (19)
while cross-correlation is
f̂ ⋆ g(k) = f̂(k)∗ ĝ(k).",A. Background from group and representation theory,[0],[0]
"(20)
",A. Background from group and representation theory,[0],[0]
The concept of group representations (see Section A) allows generalizing the Fourier transform to any compact group.,A. Background from group and representation theory,[0],[0]
"The Fourier transform of f : G → C is defined as:
f̂(ρi) =
∫
G
ρi(u) f(u) dµ(u), i = 1, 2, . . .",A. Background from group and representation theory,[0],[0]
", (21)
which, in the countable (or finite) case simplifies to
f̂(ρi) = ∑
u∈G
f(u)ρ(u), i = 1, 2, . . . .",A. Background from group and representation theory,[0],[0]
"(22)
",A. Background from group and representation theory,[0],[0]
"Despite R not being a compact group, (18) can be seen as a special case of (21), since e−2πιkx trivially obeys e−2πιk(x1+x2) = e−2πιkx1e−2πιkx2 , and the functions ρk(x) =",A. Background from group and representation theory,[0],[0]
"e −2πιkx are, in fact, the irreducible representations of R. The fundamental novelty in (21) and (22) compared to (18), however, is that since, in general (in particular, when G is not commutative), irreducible representations are matrix valued functions, each “Fourier compo-
nent” f̂(ρ) is now a matrix.",A. Background from group and representation theory,[0],[0]
"In other respects, Fourier transforms on groups behave very similarly to classical Fourier transforms.",A. Background from group and representation theory,[0],[0]
"For example, we have an inverse Fourier transform
f(u) = 1 |G| ∑
ρ∈R
dρ tr [ f(ρ)ρ(u)−1 ] ,
and also an analog of the convolution theorem, which is stated in the main body of the paper.",A. Background from group and representation theory,[0],[0]
"Since neural nets have multiple channels, we need to further extend equations 6-12 to vector/matrix valued functions.",B. Convolution of vector valued functions,[0],[0]
"Once again, there are multiple cases to consider.
",B. Convolution of vector valued functions,[0],[0]
Definition 7.,B. Convolution of vector valued functions,[0],[0]
"Let G be a finite or countable group, and X and Y be (left or right) quotient spaces of G. 1.",B. Convolution of vector valued functions,[0],[0]
"If f : X → Cm, and g : Y → Cm, we define f∗g : G→
C with
(f ∗ g)(u) = ∑
v∈G
f↑G(uv−1) · g↑G(v), (23)
where · denotes the dot product.",B. Convolution of vector valued functions,[0],[0]
2.,B. Convolution of vector valued functions,[0],[0]
"If f : X → Cn×m, and g : Y → Cm, we define f ∗",B. Convolution of vector valued functions,[0],[0]
g,B. Convolution of vector valued functions,[0],[0]
:,B. Convolution of vector valued functions,[0],[0]
"G→ Cn with
(f ∗ g)(u) = ∑
v∈G
f↑G(uv−1) × g↑G(v), (24)
where × denotes the matrix/vector product.",B. Convolution of vector valued functions,[0],[0]
3.,B. Convolution of vector valued functions,[0],[0]
"If f : X → Cm, and g : Y → Cn×m, we define f ∗ g :",B. Convolution of vector valued functions,[0],[0]
"G→ Cm with
(f ∗ g)(u) = ∑
v∈G
f↑G(uv−1) ×̃ g↑G(v), (25)
where v×̃A denotes the “reverse matrix/vector product” Av.
Since in cases 2 and 3 the nature of the product is clear from the definition of f and g, we will omit the × and ×̃ symbols.",B. Convolution of vector valued functions,[0],[0]
"The specializations of these formulae to the cases
of Equations 6-12 are as to be expected.",B. Convolution of vector valued functions,[0],[0]
Proposition 1 has three parts.,C. Proof of Proposition 1,[0],[0]
"To proceed with the proof, we introduce two simple lemmas.
",C. Proof of Proposition 1,[0],[0]
"Recall that if H is a subgroup of G, a function f : G → C is called right H–invariant if f(uh) = f(u) for all h∈H and all u∈G, and it is called left H–invariant if f(hu) = f(u) for all h∈H and all u∈G. Lemma 1.",C. Proof of Proposition 1,[0],[0]
"Let H and K be two subgroups of a group G. Then
1.",C. Proof of Proposition 1,[0],[0]
"If f : G/H → C, then f↑G : G → C is right H– invariant.",C. Proof of Proposition 1,[0],[0]
2.,C. Proof of Proposition 1,[0],[0]
"If f : H\G → C, then f↑G : G → C is left H– invariant. 3.",C. Proof of Proposition 1,[0],[0]
"If f : K\G/H → C, then f↑G : G → C is right H invariant and left K–invariant.
",C. Proof of Proposition 1,[0],[0]
Lemma 2.,C. Proof of Proposition 1,[0],[0]
Let ρ be an irreducible representation of a countable group G. Then ∑ u∈G ρ(u) = 0,C. Proof of Proposition 1,[0],[0]
"unless ρ is the trivial representation, ρtr(u) = (1).
",C. Proof of Proposition 1,[0],[0]
Proof.,C. Proof of Proposition 1,[0],[0]
"Let us define the functions rρi,j(u) =",C. Proof of Proposition 1,[0],[0]
"[ρ(u)]i,j .",C. Proof of Proposition 1,[0],[0]
"Recall that for f, g : G → C, the inner product 〈f, g〉 is
defined 〈f, g〉 = ∑u∈G f(u)∗g(u).",C. Proof of Proposition 1,[0],[0]
"The Fourier transform of a function f can then be written element-wise as [f̂(ρ)]i,j = 〈rρi,j ∗ , f〉.",C. Proof of Proposition 1,[0],[0]
"However, since the Fourier transform is a unitary transformation, for any ρ, ρ′ ∈RG, unless ρ = ρ′, i = i′ and j = j′, we must have 〈rρi,j , rρ ′
i′,j′〉 = 0.",C. Proof of Proposition 1,[0],[0]
"In particular, [∑ u∈G ρ(u) ]",C. Proof of Proposition 1,[0],[0]
"i,j
= 〈rρtr1,1, rρi,j〉 = 0, unless ρ = ρtr (and i= j =1).
",C. Proof of Proposition 1,[0],[0]
"Now recall that given an irrep ρ of G, the restriction of ρ to H is ρ|H : H → Cdρ×dρ , where ρ|H(h) = ρ(h) for all h ∈ H .",C. Proof of Proposition 1,[0],[0]
"It is trivial to check that ρ|H is a representation of H , but, in general, it is not irreducible.",C. Proof of Proposition 1,[0],[0]
"Thus, by the Theorem of Complete Decomposability (see section A), it must decompose in the form ρ|H(h) = Q(µ1(h)⊕µ2(h)⊕ . .",C. Proof of Proposition 1,[0],[0]
.⊕,C. Proof of Proposition 1,[0],[0]
µk(h))Q†,C. Proof of Proposition 1,[0],[0]
"for some sequence µ1, . . .",C. Proof of Proposition 1,[0],[0]
", µk of irreps of H and some unitary martrix Q. In the special case when the irreps of G and H are adapted to H ≤ G, however, Q is just the unity.
",C. Proof of Proposition 1,[0],[0]
This is essentially the case that we consider in Proposition 1.,C. Proof of Proposition 1,[0],[0]
"Now, armed with the above lemmas, we are in a position to prove Proposition 1.
C.0.1.",C. Proof of Proposition 1,[0],[0]
"PROOF OF PART 1
Proof.",C. Proof of Proposition 1,[0],[0]
The fact that any u ∈ G can be written uniquely as u = gh where g is the representative of one of the gH cosets and h∈H immediately tells us that f̂(ρ) factors as f̂(ρ) =,C. Proof of Proposition 1,[0],[0]
"∑
u∈G
f↑G(u)ρ(u) =",C. Proof of Proposition 1,[0],[0]
"∑
x∈G/H
∑
h∈H
f↑G(xh)ρ(xh)
",C. Proof of Proposition 1,[0],[0]
"= ∑
x∈G/H
∑
h∈H
f(x)ρ(xh) =",C. Proof of Proposition 1,[0],[0]
"∑
x∈G/H
∑
h∈H
f(x)ρ(x)ρ(h)
= ∑
x∈G/H
f(x)ρ(x)",C. Proof of Proposition 1,[0],[0]
"[∑
h∈H
ρ(h) ] .
",C. Proof of Proposition 1,[0],[0]
"However, ρ(h) = µ1(h) ⊕ µ2(h) ⊕ . . .",C. Proof of Proposition 1,[0],[0]
"⊕ µk(h) for some sequence of irreps µ1, . . .",C. Proof of Proposition 1,[0],[0]
", µk of H , so
∑
h∈H
ρ(h) =",C. Proof of Proposition 1,[0],[0]
"[∑
h∈H
µ1(h) ] ⊕ [∑
h∈H
µ2(h) ]",C. Proof of Proposition 1,[0],[0]
⊕. . .⊕,C. Proof of Proposition 1,[0],[0]
"[∑
h∈H
µk(h) ] ,
and by Lemma 2 each of the terms in this sum where µi is not the trivial representation (on H) is a zero matrix,
zeroing out all the corresponding columns in f̂(ρ).
",C. Proof of Proposition 1,[0],[0]
C.0.2.,C. Proof of Proposition 1,[0],[0]
"PROOF OF PART 2
Proof.",C. Proof of Proposition 1,[0],[0]
"Analogous to the proof of part 1, using u = hg and
a factorization similar to that of f̂(ρ) in C.0.1 except that∑ h∈H ρ(h) will now multiply ∑ x∈H\G f(x)ρ(x) from the left.
C.0.3.",C. Proof of Proposition 1,[0],[0]
"PROOF OF PART 3
Proof.",C. Proof of Proposition 1,[0],[0]
Immediate from combining case 3 of Lemma 1 with Parts 1 and 2 of Proposition 1.,C. Proof of Proposition 1,[0],[0]
Proof.,D. Proof of Proposition 2,[0],[0]
Let us assume that G is countable.,D. Proof of Proposition 2,[0],[0]
"Then
f̂ ∗g(ρi) = ∑
u∈G
[∑
v∈G
f(uv−1) g(v) ] ρi(u)
= ∑
u∈G
∑
v∈G
f(uv−1) g(v)ρi(uv −1)ρi(v)
= ∑
v∈G
∑
u∈G
f(uv−1) g(v)ρi(uv −1)ρi(v)
= ∑
v∈G
[∑
u∈G
f(uv−1) ρi(uv −1)",D. Proof of Proposition 2,[0],[0]
"] g(v)ρi(v)
= ∑
v∈G
[∑
w∈G
f(w) ρi(w) ]",D. Proof of Proposition 2,[0],[0]
"g(v)ρi(v)
=",D. Proof of Proposition 2,[0],[0]
"[∑
w∈G
f(w) ρi(w) ][∑
v∈G
g(v)ρi(v) ]
= f̂(ρi) ĝ(ρi).
",D. Proof of Proposition 2,[0],[0]
The continuous case is proved similarly but with integrals with respect Haar measure instead of sums.,D. Proof of Proposition 2,[0],[0]
E.1.,E. Proof of Theorem 1,[0],[0]
"Reverse Direction
Proving the “only if” part of Theorem 1 requires concepts from representation theory and the notion of generalized Fourier transforms (Section A)).",E. Proof of Theorem 1,[0],[0]
"We also need two versions of Schur’s Lemma.
Lemma 3.",E. Proof of Theorem 1,[0],[0]
(Schur’s lemma I),E. Proof of Theorem 1,[0],[0]
Let {ρ(g) : U →U}g∈G and {ρ′(g) : V → V }g∈G be two irreducible representations of a compact group G. Let φ : U → V be a linear (not necessarily invertible) mapping that is equivariant with these representations in the sense that φ(ρ(g)(u)),E. Proof of Theorem 1,[0],[0]
= ρ′(g)(φ(u)) for any u∈U .,E. Proof of Theorem 1,[0],[0]
"Then, unless φ is the zero map, ρ and ρ′ are equivalent representations.
",E. Proof of Theorem 1,[0],[0]
Lemma 4.,E. Proof of Theorem 1,[0],[0]
(Schur’s lemma II),E. Proof of Theorem 1,[0],[0]
"Let {ρ(g) : U → U}g∈G be an irreducible representation of a compact group G on a space U , and φ : U → U a linear map that commutes with each ρ(g) (i.e., ρ(g) ◦ φ = φ ◦ ρ(g) for any g ∈ G).",E. Proof of Theorem 1,[0],[0]
"Then φ is a multiple of the identity.
",E. Proof of Theorem 1,[0],[0]
"We build up the proof through a sequence of lemmas.
",E. Proof of Theorem 1,[0],[0]
Lemma 5.,E. Proof of Theorem 1,[0],[0]
Let U and V be two vector spaces on which a compact group G acts by the linear actions {,E. Proof of Theorem 1,[0],[0]
"Tg : U →
U}g∈G and {T ′g : V→V }g∈G, respectively.",E. Proof of Theorem 1,[0],[0]
"Let φ : U→V be a linear map that is equivariant with the {Tg} and {T ′g} actions, and W be an irreducible subspace of U (with respect to {Tg}).",E. Proof of Theorem 1,[0],[0]
"Then Z =φ(W ) is an irreducible subspace of V , and the restriction of {Tg} to W , as a representation, is equivalent with the restriction of {T ′g} to Z .
",E. Proof of Theorem 1,[0],[0]
Proof.,E. Proof of Theorem 1,[0],[0]
"Assume for contradiction that Z is reducible, i.e., that it has a proper subspace Z ⊂ Z that is fixed by {T ′g} (in other words, T ′g(v) ∈Z for all v ∈Z",E. Proof of Theorem 1,[0],[0]
and g ∈G).,E. Proof of Theorem 1,[0],[0]
"Let v be any nonzero vector in Z , u ∈ U be such that φ(u) = v, and W = span {Tg(u) | g ∈G }.",E. Proof of Theorem 1,[0],[0]
"Since W is irreducible, W cannot be a proper subspace of W , so W =W .",E. Proof of Theorem 1,[0],[0]
"Thus,
Z = φ(span {Tg(u) | g ∈G }) = span{T ′g(φ(u))|g ∈G} = span{T ′g(v)|g ∈G} ⊆ Z,
(26)
contradicting our assumption.",E. Proof of Theorem 1,[0],[0]
"Thus, the restriction {Tg|W } of {Tg} to W and the restriction {T ′g|Z} of {T ′g} to Z are both irreducible representations, and φ : W → Z is a linear map that is equivariant with them.",E. Proof of Theorem 1,[0],[0]
"By Schur’s lemma it follows that {Tg|W } and {T ′g|Z} are equivalent representations.
",E. Proof of Theorem 1,[0],[0]
Lemma 6.,E. Proof of Theorem 1,[0],[0]
Let U and V be two vector spaces on which a compact group G acts by the linear actions {,E. Proof of Theorem 1,[0],[0]
"Tg : U → U}g∈G and {T ′g : V → V }g∈G, and let U = U1 ⊕ U2 ⊕ . . .",E. Proof of Theorem 1,[0],[0]
and V = V1 ⊕ V2 ⊕ . .,E. Proof of Theorem 1,[0],[0]
.,E. Proof of Theorem 1,[0],[0]
be the corresponding isotypic decompositions.,E. Proof of Theorem 1,[0],[0]
Let φ : U → V be a linear map that is equivariant with the {Tg} and {T ′g} actions.,E. Proof of Theorem 1,[0],[0]
Then φ(Ui),E. Proof of Theorem 1,[0],[0]
"⊆ Vi for any i.
Proof.",E. Proof of Theorem 1,[0],[0]
Let Ui = U 1,E. Proof of Theorem 1,[0],[0]
i ⊕ U2i ⊕ . .,E. Proof of Theorem 1,[0],[0]
.,E. Proof of Theorem 1,[0],[0]
"be the decomposition of Ui into irreducible G–modules, and V j i = φ(U j i ).",E. Proof of Theorem 1,[0],[0]
"By Lemma 5, each V ji is an irreducible G–module that is equivalent with U ji , hence V j i ⊆ Vi.",E. Proof of Theorem 1,[0],[0]
"Consequently, φ(Ui) = φ(U 1 i ⊕ U2i ⊕ . . .)",E. Proof of Theorem 1,[0],[0]
"⊆ Vi.
Lemma 7.",E. Proof of Theorem 1,[0],[0]
"Let X = G/H and X ′ = G/K be two homogeneous spaces of a compact group G, let {Tg : L(X ) → L(X )}g∈G and {T′g : L(X ′)",E. Proof of Theorem 1,[0],[0]
"→ L(X ′)}g∈G be the corresponding translation actions, and let φ : L(X )",E. Proof of Theorem 1,[0],[0]
→ L(X ′) be a linear map that is equivariant with these actions.,E. Proof of Theorem 1,[0],[0]
"Given f ∈ L(X ) let f̂ denote its Fourier transform with respect to a specific choice of origin x0 ∈ X and system or irreps RG = {ρ1, ρ2, . . .}.",E. Proof of Theorem 1,[0],[0]
"Similarly, f̂ ′ is the Fourier transform of f ′ ∈L(X ′), with respect to some x′0",E. Proof of Theorem 1,[0],[0]
∈X ′,E. Proof of Theorem 1,[0],[0]
"and the same system of irreps.
",E. Proof of Theorem 1,[0],[0]
"Now if f ′ = φ(f), then each Fourier component of f ′ is a linear function of the corresponding Fourier component of f , i.e., there is a sequence of linear maps {Φi} such that f̂ ′(ρi) = Φi(f̂(ρi)).
",E. Proof of Theorem 1,[0],[0]
Proof.,E. Proof of Theorem 1,[0],[0]
Let U1 ⊕ U2 ⊕ . . .,E. Proof of Theorem 1,[0],[0]
and V1 ⊕ V2 ⊕ . . .,E. Proof of Theorem 1,[0],[0]
be the isotypic decompositions of L(X ) and L(X ′) with respect to the {Tg} and {T′g} actions.,E. Proof of Theorem 1,[0],[0]
"By our discussion in Section ??, each Fourier component f̂(ρi) captures the part of f falling in the corresponding isotypic subspace Ui.",E. Proof of Theorem 1,[0],[0]
"Similarly, f̂ ′(ρj) captures the part of f
′ falling in Vj .",E. Proof of Theorem 1,[0],[0]
"Lemma 6 tells us that because φ is equivariant with the translation actions, it maps each Ui to the corresponding isotypic Vi.",E. Proof of Theorem 1,[0],[0]
"Therefore, f̂ ′(ρi)",E. Proof of Theorem 1,[0],[0]
= Φi(f̂(ρi)),E. Proof of Theorem 1,[0],[0]
for some function Φi.,E. Proof of Theorem 1,[0],[0]
"By the linearity of φ, each Φi must be linear.
",E. Proof of Theorem 1,[0],[0]
"Lemma 7 is a big step towards describing what form equivariant mappings take in Fourier space, but it doesn’t yet fully pin down the individual Φi maps.",E. Proof of Theorem 1,[0],[0]
"We now focus on a single pair of isotypics (Ui, Vi) and the corresponding map Φi taking f̂(ρi) 7→ f̂ ′(ρi).",E. Proof of Theorem 1,[0],[0]
"We will say that Φi is an allowable map if there is some equivariant φ such
that φ̂(f)(ρi) = Φi(f̂(ρi)).",E. Proof of Theorem 1,[0],[0]
"Clearly, if Φ1,Φ2, . .",E. Proof of Theorem 1,[0],[0]
.,E. Proof of Theorem 1,[0],[0]
"are individually allowable, then they are also jointly allowable.
",E. Proof of Theorem 1,[0],[0]
Lemma 8.,E. Proof of Theorem 1,[0],[0]
All linear maps of the form,E. Proof of Theorem 1,[0],[0]
"Φi : M 7→ MB where B ∈Cδ×δ are allowable.
",E. Proof of Theorem 1,[0],[0]
Proof.,E. Proof of Theorem 1,[0],[0]
"Recall that the {Tg} action takes f 7→ fg , where fg(x) = f(g−1x).",E. Proof of Theorem 1,[0],[0]
"In Fourier space,
f̂g(ρi) =",E. Proof of Theorem 1,[0],[0]
"∑
u∈G
ρi(u)f g↑G(u)
= ∑
u∈G
ρi(u)f↑G(g−1u)
= ∑
w∈G
ρi(gw)f↑G(w)
",E. Proof of Theorem 1,[0],[0]
"= ρi(g) ∑
w∈G
ρi(w)f↑G(w)
= ρi(g) f̂(ρi).",E. Proof of Theorem 1,[0],[0]
"(27)
(This is actually a general result called the (left) translation theorem.)",E. Proof of Theorem 1,[0],[0]
"Thus,
Φi ( T̂g(f)(ρi) ) = Φi ( ρi(g)f̂(ρi) )",E. Proof of Theorem 1,[0],[0]
"= ρi(g) f̂(ρi)B.
",E. Proof of Theorem 1,[0],[0]
"Similarly, the {T′g} action maps f̂ ′(ρi) 7→ g(ρi)f̂ ′(ρi), so
T′g ( Φi(f̂(ρi)) )",E. Proof of Theorem 1,[0],[0]
"= T′g ( f̂(ρi)B ) = ρi(g) f̂(ρi)B.
",E. Proof of Theorem 1,[0],[0]
"Therefore, Φi is equivariant with the {T} and {T′} actions.
",E. Proof of Theorem 1,[0],[0]
Lemma 9.,E. Proof of Theorem 1,[0],[0]
Let Φi : M 7→ BM for some B ∈ Cδ×δ.,E. Proof of Theorem 1,[0],[0]
Then Φi is not allowable unless B is a multiple of the identity.,E. Proof of Theorem 1,[0],[0]
"Moreover, this theorem also hold in the columnwise sense that if Φi : M → M ′ such that [M ′]∗,j = Bj [M ]∗,j for some sequence of matrices B1, . . .",E. Proof of Theorem 1,[0],[0]
", Bd, then Φi is not allowable unless each Bj is a multiple of the identity.
",E. Proof of Theorem 1,[0],[0]
Proof.,E. Proof of Theorem 1,[0],[0]
"Following the same steps as in the proof of Lemma 8, we now have
Φi ( T̂g(f)(ρi) )",E. Proof of Theorem 1,[0],[0]
"= Bρi(g) f̂(ρi),
T′g ( Φi(f̂(ρi)) )",E. Proof of Theorem 1,[0],[0]
"= ρi(g)Bf̂ (ρi).
",E. Proof of Theorem 1,[0],[0]
"However, by the second form of Schur’s Lemma, we cannot have Bρi(g) = ρi(g)B for all g ∈G, unless B is a multiple of the identity.
",E. Proof of Theorem 1,[0],[0]
Lemma 10.,E. Proof of Theorem 1,[0],[0]
"Φi is allowable if and only if it is of the form M 7→MB for some B ∈Cδ×δ .
",E. Proof of Theorem 1,[0],[0]
Proof.,E. Proof of Theorem 1,[0],[0]
"For the “if” part of this lemma, see Lemma 8.",E. Proof of Theorem 1,[0],[0]
"For the “only if” part, note that the set of allowable Φi form a subspace of all linear maps Cδ×δ → Cδ×δ, and any allowable Φi can be expressed in the form
[Φi(M)]a,b = ∑
c,d
αa,b,c,dMc,d.
By Lemma 9, if a 6= c but b = d, then αa,b,c,d = 0.",E. Proof of Theorem 1,[0],[0]
"On the other hand, by Lemma 8 if a= c, then αa,b,c,d can take on any value, regardless of the values of b and d, as long as αa,b,a,d is constant across varying a.
Now consider the remaining case a 6= c and b 6= d, and assume that αa,b,c,d 6= 0 while Φi is still allowable.",E. Proof of Theorem 1,[0],[0]
"Then, by Lemma 8, it is possible to construct a second allowable mapΦ′i (namely one in whichα ′",E. Proof of Theorem 1,[0],[0]
"a,d,a,b = 1 andα ′",E. Proof of Theorem 1,[0],[0]
"a,d,x,y = 0 for all (x, y) 6=",E. Proof of Theorem 1,[0],[0]
"(c, d)) such that in the composite map Φ′′i =",E. Proof of Theorem 1,[0],[0]
Φ ′,E. Proof of Theorem 1,[0],[0]
i ◦,E. Proof of Theorem 1,[0],[0]
"Φi, α′′a,d,c,d 6= 0.",E. Proof of Theorem 1,[0],[0]
"Thus, Φ′′i is not allowable.",E. Proof of Theorem 1,[0],[0]
"However, the composition of one allowable map with another allowable map is allowable, contradicting our assumption that Φi is allowable.
",E. Proof of Theorem 1,[0],[0]
"Thus, we have established that if Φi is allowable, then αa,b,c,d=0, unless a= c. To show that any allowable Φi of the form M 7→ MB, it remains to prove that additionally αa,b,a,d is constant across a. Assume for contradiction that Φi is allowable, but for some (a, e, b, d) indices αa,b,a,d 6= αe,b,e,d. Now let Φ0 be the allowable map that zeros out every column except column d (i.e., α0x,d,x,d = 1 for all x, but all other coefficients are zero), and let Φ′ be the allowable map that moves column b to column d (i.e., α′x,d,x,b = 1 for any x, but all other coeffcients are zero).",E. Proof of Theorem 1,[0],[0]
"Since the composition of allowable maps is allowable, we expect Φ′′ = Φ′ ◦",E. Proof of Theorem 1,[0],[0]
Φ ◦ Φ0 to be allowable.,E. Proof of Theorem 1,[0],[0]
"However Φ′′ is a map that falls under the purview of Lemma 9, yet α′′a,d,a,d 6= α′′e,d,e,d (i.e., Mj is not a multiple of the identity) creating a contradiction.
",E. Proof of Theorem 1,[0],[0]
Proof of Theorem 1 (reverse direction).,E. Proof of Theorem 1,[0],[0]
"For simplicty we first prove the theorem assuming Yℓ=C for each ℓ.
Since N is a G-CNN, each of the mappings (σℓ ◦ φℓ) : L(Xℓ−1) → L(Xℓ) is equivariant with the corresponding translation actions {Tℓ−1g }g∈G and {Tℓg}g∈G. Since σℓ is a pointwise operator, this is equivalent to asserting that φℓ is equivariant with {Tℓ−1g }g∈G and {Tℓg}g∈G. Letting X = Xℓ−1 and X ′",E. Proof of Theorem 1,[0],[0]
"= Xℓ, Lemma 8 then tells us the the Fourier transforms of fℓ−1 and φℓ(fℓ−1) are related by
̂φℓ(fℓ−1)(ρi) =",E. Proof of Theorem 1,[0],[0]
"Φ ( f̂ℓ−1(ρi) )
for some fixed set of linear maps Φ1,Φ2, . .",E. Proof of Theorem 1,[0],[0]
..,E. Proof of Theorem 1,[0],[0]
"Furthermore, by Lemma 10, each Φi must be of the formM 7→MBi for some appropriate matrix Bi ∈ Cdρ×dρ .",E. Proof of Theorem 1,[0],[0]
"If we then define χℓ as the inverse Fourier transform of (B1, B2, . . .), then by the convolution theorem (Proposition 2), φℓ(fℓ−1) = fℓ−1 ∗ χ, confirming that N is a G-CNN.",E. Proof of Theorem 1,[0],[0]
"The extension of this result to the vector valued case, fℓ :",E. Proof of Theorem 1,[0],[0]
"Xℓ → Vℓ, is straightforward.",E. Proof of Theorem 1,[0],[0]
Convolutional neural networks have been extremely successful in the image recognition domain because they ensure equivariance to translations.,abstractText,[0],[0]
"There have been many recent attempts to generalize this framework to other domains, including graphs and data lying on manifolds.",abstractText,[0],[0]
"In this paper we give a rigorous, theoretical treatment of convolution and equivariance in neural networks with respect to not just translations, but the action of any compact group.",abstractText,[0],[0]
"Our main result is to prove that (given some natural constraints) convolutional structure is not just a sufficient, but also a necessary condition for equivariance to the action of a compact group.",abstractText,[0],[0]
Our exposition makes use of concepts from representation theory and noncommutative harmonic analysis and derives new generalized convolution formulae.,abstractText,[0],[0]
On the Generalization of Equivariance and Convolution in Neural Networks  to the Action of Compact Groups,title,[0],[0]
"Modern machine learning systems based on deep neural networks are usually over-parameterized, i.e. the number of parameters in the model is much larger than the size of the training data, which makes these systems prone to overfitting.",1. Introduction,[0],[0]
"Several explicit regularization strategies have been used in practice to help these systems generalize, including `1 and `2 regularization of the parameters (Nowlan and Hinton, 1992).",1. Introduction,[0],[0]
"Recently, (Neyshabur et al., 2015) showed that a variety of such norm-based regularizers can provide sizeindependent capacity control, suggesting that the network size is not a good measure of complexity in such settings.",1. Introduction,[0],[0]
"Such a view had been previously motivated in the context of matrix factorization (Srebro et al., 2005), where it is preferable to have many factors of limited overall influence rather than a few important ones.
",1. Introduction,[0],[0]
"Besides explicit regularization techniques, practitioners have used a spectrum of algorithmic approaches to improve the generalization ability of over-parametrized models.",1. Introduction,[0],[0]
"This includes early stopping of back propagation (Caruana et al., 2001), batch normalization (Ioffe and Szegedy, 2015) and
1Department of Computer Science, Johns Hopkins University, Baltimore, USA 2Department of Biomedical Engineering, Johns Hopkins University, Baltimore, USA.",1. Introduction,[0],[0]
"Correspondence to: Raman Arora <arora@cs.jhu.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
dropout (Srivastava et al., 2014).",1. Introduction,[0],[0]
"In particular, dropout, which is the focus of this paper, randomly drops hidden nodes along with their connections at training time.",1. Introduction,[0],[0]
"Dropout was introduced by Srivastava et al. (2014) as a way of breaking up co-adaptation among neurons, drawing insights from the success of the sexual reproduction model in the evolution of advanced organisms.",1. Introduction,[1.0],"['Dropout was introduced by Srivastava et al. (2014) as a way of breaking up co-adaptation among neurons, drawing insights from the success of the sexual reproduction model in the evolution of advanced organisms.']"
"While dropout has enjoyed tremendous success in training deep neural networks, the theoretical understanding of how dropout (and other algorithmic heuristics) provide regularization in deep learning remains somewhat limited.
",1. Introduction,[0],[0]
"We argue that a prerequisite for understanding implicit regularization due to various algorithmic heuristics in deep learning, including dropout, is to analyze their behavior in simpler models.",1. Introduction,[0],[0]
"Therefore, in this paper, we consider the following learning problem.",1. Introduction,[0],[0]
Let x ∈ Rd2 represent an input feature vector with some unknown distribution D such that Ex∼D[xx>] = I.,1. Introduction,[1.0],['Let x ∈ Rd2 represent an input feature vector with some unknown distribution D such that Ex∼D[xx>] = I.']
The output label vector y ∈ Rd1 is given as y = Mx for some M ∈ Rd1×d2 .,1. Introduction,[1.0],['The output label vector y ∈ Rd1 is given as y = Mx for some M ∈ Rd1×d2 .']
"We consider the hypothesis class represented by a single hidden-layer linear network parametrized as hU,V(x) = UV>x, where V ∈ Rd2×r and U ∈ Rd1×r are the weight matrices in the first and the second layers, respectively.",1. Introduction,[1.0],"['We consider the hypothesis class represented by a single hidden-layer linear network parametrized as hU,V(x) = UV>x, where V ∈ Rd2×r and U ∈ Rd1×r are the weight matrices in the first and the second layers, respectively.']"
"The goal of learning is to find weight matrices U,V that minimize the expected loss `(U,V) := Ex∼D[‖y−hU,V(x)‖2] = Ex∼D[‖y−UV>x‖2].
",1. Introduction,[0.9999999646087776],"['The goal of learning is to find weight matrices U,V that minimize the expected loss `(U,V) := Ex∼D[‖y−hU,V(x)‖2] = Ex∼D[‖y−UV>x‖2].']"
"A natural learning algorithm to consider is back-propagation with dropout, which can be seen as an instance of stochastic gradient descent on the following objective:
f(U,V) :",1. Introduction,[0],[0]
"=Ebi∼Ber(θ),x∼D [∥∥∥∥y− 1θU diag(b)V>x ∥∥∥∥2 ] , (1)
where the expectation is w.r.t.",1. Introduction,[0],[0]
the underlying distribution on data as well as randomization due to dropout (each hidden unit is dropped independently with probability 1− θ).,1. Introduction,[0],[0]
"This procedure, which we simply refer to as dropout in this paper, is given in Algorithm 1.
",1. Introduction,[0],[0]
"It is easy to check (see Lemma A.1 in the supplementary) that the objective in equation (1) can be written as
f(U,V) =",1. Introduction,[0],[0]
"`(U,V) + λ r∑ i=1",1. Introduction,[0],[0]
"‖ui‖2‖vi‖2, (2)
where λ = 1−θθ is the regularization parameter, and ui and vi represent the ith columns of U and V, respectively.",1. Introduction,[0.9508672908851566],"['(3) Interestingly, the dropout regularizer is equal to the square of the path regularizer, i.e. R(U,V) = λψ22(U,V).']"
"Note
that while the goal was to minimize the expected squared loss, using dropout with gradient descent amounts to finding a minimum of the objective in equation (2); we argue that the additional term in the objective serves as a regularizer, R(U,V) := λ ∑r i=1",1. Introduction,[0],[0]
"‖ui‖2‖vi‖2, and is an explicit instantiation of the implicit bias of dropout.",1. Introduction,[0],[0]
"Furthermore, we note that this regularizer is closely related to path regularization which is given as the square-root of the sum over all paths, from input to output, of the product of the squared weights along the path (Neyshabur et al., 2015).",1. Introduction,[0],[0]
"Formally, for a single layer network, path regularization is given as
ψ2(U,V) =  r∑ i=1",1. Introduction,[0],[0]
d1∑ j=1 d2∑ k=1 u2jiv 2 ki  12 .,1. Introduction,[0],[0]
"(3) Interestingly, the dropout regularizer is equal to the square of the path regularizer, i.e. R(U,V) = λψ22(U,V).",1. Introduction,[0],[0]
"While this observation is rather immediate, it has profound implications owing to the fact that path regularization provides size-independent capacity control in deep learning, thereby supporting empirical evidence that dropout finds good solutions in over-parametrized settings.
",1. Introduction,[0],[0]
"In this paper, we focus on studying the optimization landscape of the objective in equation (2) for a single hiddenlayer linear network with dropout and the special case of an autoencoder with tied weights.",1. Introduction,[0],[0]
"Furthermore, we are interested in characterizing the solutions to which dropout (i.e. Algorithm 1) converges.",1. Introduction,[0],[0]
"We make the following progress toward addressing these questions.
",1. Introduction,[0],[0]
1.,1. Introduction,[0],[0]
We formally characterize the implicit bias of dropout.,1. Introduction,[0],[0]
"We show that, when minimizing the expected loss `(U,V) with dropout, any global minimum (Ũ, Ṽ) satisfies ψ2(Ũ, Ṽ) = min{ψ2(U,V) s.t. UV> = ŨṼ
>}.",1. Introduction,[0],[0]
"More importantly, for auto-encoders with tied weights, we show that all local minima inherit this property.
2.",1. Introduction,[0],[0]
"Despite the non-convex nature of the problem, we completely characterize the global optima by giving necessary and sufficient conditions for optimality.
3.",1. Introduction,[0],[0]
We describe the optimization landscape of the dropout problem.,1. Introduction,[0],[0]
"In particular, we show that for a sufficiently small dropout rate, all local minima of the objective in equation (2) are global and all saddle points are non-degenerate.",1. Introduction,[0],[0]
"This allows Algorithm 1 to efficiently escape saddle points and converge to a global optimum.
",1. Introduction,[0],[0]
The rest of the paper is organized as follows.,1. Introduction,[0],[0]
"In Section 2, we study dropout for single hidden-layer linear auto-encoder networks with weights tied between the first and the second layers.",1. Introduction,[0],[0]
"This gives us the tools to study the dropout problem in a more general setting of single hidden-layer linear
Algorithm 1 Dropout with Stochastic Gradient Descent
input Data {(xt, yt)} T−1 t=0 , dropout rate 1−θ, learning rate η
1: Initialize U0,V0 2: for t = 0, 1, . . .",1. Introduction,[0],[0]
", T − 1 do 3: sample bt element-wise from Bernoulli(θ) 4: Update the weights
Ut+1←Ut−η ( 1
θ Ut diag(bt)V>t xt−yt
)",1. Introduction,[0],[0]
"x>t Vt diag(bt)
Vt+1←Vt−ηxt",1. Introduction,[0],[0]
"( 1
θ x>t Vt diag(bt)U > t −y>t
)",1. Introduction,[0],[0]
"Ut diag(bt)
5: end for output UT ,VT
networks in Section 3.",1. Introduction,[0],[0]
"In Section 4, we characterize the optimization landscape of the objective in (2), show that it satisfies the strict saddle property, and that there are no spurious local minima.",1. Introduction,[0],[0]
"We specialize our results to matrix factorization in Section 5, and in Section 6, we discuss preliminary experiments to support our theoretical results.",1. Introduction,[0],[0]
"We denote matrices, vectors, scalar variables and sets by Roman capital letters, Roman small letters, small letters and script letters respectively (e.g. X, x, x, and X ).",1.1. Notation,[1.0],"['We denote matrices, vectors, scalar variables and sets by Roman capital letters, Roman small letters, small letters and script letters respectively (e.g. X, x, x, and X ).']"
"For any integer d, we represent the set {1, . . .",1.1. Notation,[0],[0]
", d} by [d].",1.1. Notation,[0],[0]
"For any integer i, ei denotes the i-th standard basis.",1.1. Notation,[1.0],"['For any integer i, ei denotes the i-th standard basis.']"
"For any integer d, 1d ∈ Rd is the vector of all ones, ‖x‖ represents the `2-norm of vector x, and ‖X‖, ‖X‖F , ‖X‖∗ and λi(X) represent the spectral norm, the Frobenius norm, the nuclear norm and the i-th largest singular value of matrix X, respectively.",1.1. Notation,[0],[0]
"〈·, ·〉 represents the standard inner product, for vectors or matrices, where 〈X,X′〉 = Tr(X>X′).",1.1. Notation,[0],[0]
"For a matrix X ∈ Rd1×d2 , diag(X) ∈ Rmin{d1,d2} returns its diagonal elements.",1.1. Notation,[0],[0]
"Similarly, for a vector x ∈ Rd, diag(x) ∈ Rd×d is a diagonal matrix with x on its diagonal.",1.1. Notation,[0],[0]
"For any scalar x, we define (x)+ = max{x, 0}, and for a matrix X, (X)+ is the elementwise application of (·)+ to X. For a matrix X with a compact singular value decomposition X = UΣV>, and for any scalar α ≥ 0, we define the singular-value shrinkagethresholding operator as Sα(X) := U(Σ− αI)+V>.",1.1. Notation,[0],[0]
We begin with a simpler hypothesis family of single hiddenlayer linear auto-encoders with weights tied such that U = V.,2. Linear autoencoders with tied weights,[0],[0]
Studying the problem in this setting helps our intuition about the implicit bias that dropout induces on weight matrices U.,2. Linear autoencoders with tied weights,[0],[0]
"This analysis will be extended to the more general setting of single hidden-layer linear networks in the next section.
",2. Linear autoencoders with tied weights,[0],[0]
"Recall that the goal here is to find an autoencoder network represented by a weight matrix U ∈ Rd2×r that solves:
min U∈Rd2×r `(U,U) + λ r∑ i=1",2. Linear autoencoders with tied weights,[0],[0]
‖ui‖4,2. Linear autoencoders with tied weights,[0],[0]
", (4)
where ui is the ith column of U. Note that the loss function `(U,U) is invariant under rotations, i.e., for any orthogonal transformation Q ∈ Rd×d,Q>Q = QQ> =",2. Linear autoencoders with tied weights,[0],[0]
"Id, it holds that
`(U,U)=Ex∼D[‖y− UQQ>U>x‖2]=`(UQ,UQ),
so that applying a rotation matrix to a candidate solution U does not change the value of the loss function.",2. Linear autoencoders with tied weights,[0.9531540479227267],"['Note that the loss function `(U,U) is invariant under rotations, i.e., for any orthogonal transformation Q ∈ Rd×d,Q>Q = QQ> = Id, it holds that `(U,U)=Ex∼D[‖y− UQQ>U>x‖2]=`(UQ,UQ), so that applying a rotation matrix to a candidate solution U does not change the value of the loss function.']"
"However, the regularizer is not rotation-invariant and clearly depends on the choice of Q. Therefore, in order to solve Problem (4), we need to find a rotation matrix that minimizes the value of the regularizer for a given weight matrix.
",2. Linear autoencoders with tied weights,[0],[0]
"To that end, let us denote the squared column norms of the weight matrix U by nu = (‖u1‖2, . . .",2. Linear autoencoders with tied weights,[0],[0]
", ‖ur‖2)",2. Linear autoencoders with tied weights,[0],[0]
and let 1r ∈,2. Linear autoencoders with tied weights,[0],[0]
Rr be the vector of all ones.,2. Linear autoencoders with tied weights,[0],[0]
"Then, for any U,
R(U,U) = λ r∑ i=1 ‖ui‖4",2. Linear autoencoders with tied weights,[0],[0]
"= λ r ‖1r‖2‖nu‖2
≥ λ r 〈1r, nu〉2 = λ r ( r∑ i=1 ‖ui‖2 )",2. Linear autoencoders with tied weights,[0],[0]
"2 = λ r ‖U‖4F ,
where the inequality follows from Cauchy-Schwartz inequality.",2. Linear autoencoders with tied weights,[0],[0]
"Hence, the regularizer is lower bounded by λr ‖U‖ 4 F , with equality if and only if nu is parallel to 1r, i.e. when all the columns of U have equal norms.",2. Linear autoencoders with tied weights,[0],[0]
"Since the loss function is rotation invariant, one can always decrease the value of the overall objective by rotating U such that UQ has a smaller regularizer.",2. Linear autoencoders with tied weights,[0],[0]
"A natural question to ask, therefore, is if there always exists a rotation matrix Q such that the matrix UQ has equal column norms.",2. Linear autoencoders with tied weights,[0],[0]
"In order to formally address this question, we introduce the following definition.",2. Linear autoencoders with tied weights,[0],[0]
"Definition 2.1 (Equalized weight matrix, equalized autoencoder, equalizer).",2. Linear autoencoders with tied weights,[0],[0]
A weight matrix U is said to be equalized if all its columns have equal norms.,2. Linear autoencoders with tied weights,[0],[0]
An autoencoder with tied weights is said to be equalized if the norm of the incoming weight vector is equal across all hidden nodes in the network.,2. Linear autoencoders with tied weights,[0],[0]
"An orthogonal transformation Q is said to be an equalizer of U (equivalently, of the corresponding autoencoder) if UQ is equalized.
",2. Linear autoencoders with tied weights,[0],[0]
"Next, we show that any matrix U can be equalized.",2. Linear autoencoders with tied weights,[0],[0]
Theorem 2.2.,2. Linear autoencoders with tied weights,[0],[0]
Any weight matrix U ∈,2. Linear autoencoders with tied weights,[0],[0]
"Rd×r (equivalently, the corresponding autoencoder network hU,U) can be equalized.",2. Linear autoencoders with tied weights,[0],[0]
"Furthermore, there exists a polynomial time algorithm (Algorithm 2) that returns an equalizer for a given matrix.
",2. Linear autoencoders with tied weights,[0],[0]
"The key insight here is that if GU := U>U is the Gram matrix associated with the weight matrix U, then hU,U is equalized by Q if and only if all diagonal elements of Q>GUQ
Algorithm 2 EQZ(U) equalizer of an auto-encoder hU,U input U ∈",2. Linear autoencoders with tied weights,[0],[0]
"Rd×r
1:",2. Linear autoencoders with tied weights,[0],[0]
"G← U>U 2: Q← Ir 3: for i = 1 to r do 4: [V,Λ]←eig(G) {G=VΛV> eigendecomposition} 5: w = 1√ r−i+1 ∑r−i+1 i=1",2. Linear autoencoders with tied weights,[0],[0]
vi 6: Qi,2. Linear autoencoders with tied weights,[0],[0]
←,2. Linear autoencoders with tied weights,[0],[0]
[w w⊥] {w⊥ ∈ R(r−i+1)×(r−i) orthonormal basis for the Null space of w} 7:,2. Linear autoencoders with tied weights,[0],[0]
"G← Q>i GQi {Making first diagonal element zero} 8: G← G(2 : end, 2 : end) {First principal submatrix}
9: Q← Q",2. Linear autoencoders with tied weights,[0],[0]
"[
Ii−1 0 0",2. Linear autoencoders with tied weights,[0],[0]
"Qi ] 10: end for output Q {such that UQ is equalized}
are equal.",2. Linear autoencoders with tied weights,[0],[0]
"More importantly, if GU = VΛV> is an eigendecomposition of GU, then for w = 1√r ∑r i=1",2. Linear autoencoders with tied weights,[0],[0]
"vi, it holds that w>GUw = Tr GUr ; Proof of Theorem 2.2 uses this property to recursively equalize all diagonal elements of GU.
",2. Linear autoencoders with tied weights,[0],[0]
"Finally, we argue that the implicit bias induced by dropout is closely related to the notion of equalized network introduced above.",2. Linear autoencoders with tied weights,[0],[0]
"In particular, our main result of the section states that the dropout enforces any globally optimal network to be equalized.",2. Linear autoencoders with tied weights,[0],[0]
"Formally, we show the following.",2. Linear autoencoders with tied weights,[0],[0]
Theorem 2.3.,2. Linear autoencoders with tied weights,[0],[0]
"If U is a global optimum of Problem 4, then U is equalized.",2. Linear autoencoders with tied weights,[0],[0]
"Furthermore, it holds that
R(U) = λ
r ‖U‖4F .
",2. Linear autoencoders with tied weights,[0],[0]
Theorem 2.3 characterizes the effect of regularization induced by dropout in learning autoencoders with tied weights.,2. Linear autoencoders with tied weights,[0],[0]
"It states that for any globally optimal network, the columns of the corresponding weight matrix have equal norms.",2. Linear autoencoders with tied weights,[0],[0]
"In other words, dropout tends to give equal weights to all hidden nodes – it shows that dropout implicitly biases the optimal networks towards having hidden nodes with limited overall influence rather than a few important ones.
",2. Linear autoencoders with tied weights,[0],[0]
"While Theorem 2.3 makes explicit the bias of dropout and gives a necessary condition for global optimality in terms of the weight matrix U∗, it does not characterize the bias induced in terms of the network (i.e. in terms of U∗U>∗ ).",2. Linear autoencoders with tied weights,[0],[0]
The following theorem completes the characterization by describing globally optimal autoencoder networks.,2. Linear autoencoders with tied weights,[0],[0]
"Since the goal is to understand the implicit bias of dropout, we specify the global optimum in terms of the true concept, M. Theorem 2.4.",2. Linear autoencoders with tied weights,[0.9518873530883124],"['Therefore, in this section, we pose the following questions: What is the implicit bias of dropout in terms of local minima?']"
For any j ∈,2. Linear autoencoders with tied weights,[0],[0]
"[r], let κj := 1j ∑j i=1",2. Linear autoencoders with tied weights,[0],[0]
λi(M).,2. Linear autoencoders with tied weights,[0],[0]
"Furthermore, define ρ := max{j ∈",2. Linear autoencoders with tied weights,[0],[0]
[r] : λj(M) > λjκjr+λj }.,2. Linear autoencoders with tied weights,[0],[0]
"Then, if U∗ is a global optimum of Problem 4, it satisfies that U∗U>∗ = S λρκρ
r+λρ
(M).
",2. Linear autoencoders with tied weights,[0.958632115318836],"['Then, if (U∗,V∗) is a global optimum of Problem 6, it satisfies that U∗V>∗ = S λρκρ r+λρ (M).']"
Remark 2.5.,2. Linear autoencoders with tied weights,[0],[0]
"In light of Theorem 2.3, the proof of Theorem 2.4 entails solving the following optimization problem
min U∈Rd×r
`(U,U) + λ
r ‖U‖4F , (5)
instead of Problem 4.",2. Linear autoencoders with tied weights,[0],[0]
"This follows since the loss function `(U,U) is invariant under rotations, hence a weight matrix U cannot be optimal if there exists a rotation matrix Q such that R(UQ,UQ) < R(U,U).",2. Linear autoencoders with tied weights,[0],[0]
"Now, while the objective in Problem 5 is a lower bound on the objective in Problem 4, by Theorem 2.2, we know that any weight matrix can be equalized.",2. Linear autoencoders with tied weights,[0],[0]
"Thus, it follows that the minimum of the two problems coincide.",2. Linear autoencoders with tied weights,[0],[0]
"Although Problem 5 is still non-convex, it is easier to study owing to a simpler form of the regularizer.",2. Linear autoencoders with tied weights,[0],[0]
Figure 1 shows how optimization landscape changes with different dropout rates for a single hidden layer linear autoencoder with one dimensional input and output and with a hidden layer of width two.,2. Linear autoencoders with tied weights,[1.0],['Figure 1 shows how optimization landscape changes with different dropout rates for a single hidden layer linear autoencoder with one dimensional input and output and with a hidden layer of width two.']
"Next, we consider the more general setting of a shallow linear network with a single hidden layer.",3. Single hidden-layer linear networks,[0],[0]
"Recall, that the
goal is to find weight matrices U,V that solve
min U∈Rd1×r,V∈Rd2×r `(U,V) + λ r∑ i=1",3. Single hidden-layer linear networks,[0],[0]
‖ui‖2‖vi‖2.,3. Single hidden-layer linear networks,[0],[0]
"(6)
As in the previous section, we note that the loss function is rotation invariant, i.e. `(UQ,VQ) =",3. Single hidden-layer linear networks,[0],[0]
"`(U,V) for any rotation matrix Q, however the regularizer is not invariant to rotations.",3. Single hidden-layer linear networks,[0],[0]
"Furthermore, it is easy to verify that both the loss function and the regularizer are invariant under rescaling of the incoming and outgoing weights to hidden neurons.",3. Single hidden-layer linear networks,[0],[0]
Remark 3.1 (Rescaling invariance).,3. Single hidden-layer linear networks,[0],[0]
"The objective function in Problem (2) is invariant under rescaling of weight matrices, i.e. invariant to transformations of the form Ū = UD, V̄ = VD−1, where D is a diagonal matrix with positive entries.",3. Single hidden-layer linear networks,[0],[0]
"This follows since ŪV̄> = UDD−>V> = UV>, so that `(Ū, V̄) =",3. Single hidden-layer linear networks,[0],[0]
"`(U,V), and also R(Ū, V̄) = R(U,V) since r∑ i=1",3. Single hidden-layer linear networks,[0],[0]
‖ūi‖2‖v̄i‖2 = r∑ i=1,3. Single hidden-layer linear networks,[0],[0]
‖diui‖2‖ 1 di vi‖2 = r∑ i=1,3. Single hidden-layer linear networks,[0],[0]
"‖ui‖2‖vi‖2.
",3. Single hidden-layer linear networks,[0],[0]
"As a result of rescaling invariance, f(Ū, V̄) = f(U,V).",3. Single hidden-layer linear networks,[0],[0]
"Now, following similar arguments as in the previous section,
we define nu,v = (‖u1‖‖v1‖, . . .",3. Single hidden-layer linear networks,[0],[0]
", ‖ur‖‖vr‖), and note that
R(U,V) = λ r∑ i=1",3. Single hidden-layer linear networks,[0],[0]
‖ui‖2‖vi‖2,3. Single hidden-layer linear networks,[0],[0]
"= λ r ‖1r‖2‖nu,v‖2
≥ λ r 〈",3. Single hidden-layer linear networks,[0],[0]
"1r, nu,v〉2 = λ r ( r∑ i=1 ‖ui‖‖vi‖ )2 ,
where the inequality is due to Cauchy-Schwartz, and the lower bound is achieved if and only if nu,v is a scalar multiple of 1r, i.e. iff ‖ui‖‖vi‖",3. Single hidden-layer linear networks,[0],[0]
= ‖u1‖‖v1‖,3. Single hidden-layer linear networks,[0],[0]
"for all i = 1, . . .",3. Single hidden-layer linear networks,[0],[0]
", r. This observation motivates the following definition.",3. Single hidden-layer linear networks,[0],[0]
"Definition 3.2 (Jointly equalized weight matrices, equalized linear networks).",3. Single hidden-layer linear networks,[0],[0]
"A pair of weight matrices (U,V) ∈ Rd1×r×Rd2×r is said to be jointly equalized if ‖ui‖‖vi‖ = ‖u1‖‖v1‖ for all i ∈",3. Single hidden-layer linear networks,[0],[0]
[r].,3. Single hidden-layer linear networks,[0],[0]
A single hidden-layer linear network is said to be equalized if the product of the norms of the incoming and outgoing weights are equal for all hidden nodes.,3. Single hidden-layer linear networks,[0],[0]
"Equivalently, a single hidden-layer network parametrized by weight matrices U,V, is equalized if U,V are jointly equalized.",3. Single hidden-layer linear networks,[0],[0]
"An orthogonal transformation Q ∈ Rr×r is an equalizer of a single hidden-layer network hU,V parametrized by weight matrices U,V, if hUQ,VQ is equalized.",3. Single hidden-layer linear networks,[0],[0]
"The network hU,V (the pair(U,V)) then are said to be jointly equalizable by Q.
Note that Theorem 2.2 only guarantees the existence of an equalizer for an autoencoder with tied weights.",3. Single hidden-layer linear networks,[0],[0]
"It does not inform us regarding the existence of a rotation matrix that jointly equalizes a general network parameterized by a pair of weight matrices (U,V); in fact, it is not true in general that any pair (U,V) is jointly equalizable.",3. Single hidden-layer linear networks,[0],[0]
"Indeed, the general case requires a more careful treatment.",3. Single hidden-layer linear networks,[0],[0]
"It turns out that while a given pair of matrices (U,V) may not be jointly equalizable there exists a pair (Ũ, Ṽ) that is jointly equalizable and implements the same network function, i.e. hŨ,Ṽ = hU,V. Formally, we state the following result.",3. Single hidden-layer linear networks,[0],[0]
Theorem 3.3.,3. Single hidden-layer linear networks,[0],[0]
"For any given pair of weight matrices (U,V) ∈ Rd1×r×Rd2×r, there exists another pair (Ũ, Ṽ) ∈ Rd1×r × Rd2×r and a rotation matrix Q ∈ Rr×r such that hŨ,Ṽ = hU,V and hŨ,Ṽ is jointly equalizable by Q. Furthermore, for Ū := ŨQ and V̄ := ṼQ it holds that ‖ūi‖2 = ‖v̄i‖2 = 1r‖UV >‖∗ for i = 1, . . .",3. Single hidden-layer linear networks,[0],[0]
", r.
Theorem 3.3 implies that for any network hU,V there exists an equalized network hŪ,V̄ such that hŪ,V̄ = hU,V. Hence, it is always possible to reduce the objective by equalizing the network, and a network hU,V is globally optimal only if it is equalized.",3. Single hidden-layer linear networks,[0],[0]
Theorem 3.4.,3. Single hidden-layer linear networks,[0],[0]
"If (U,V) is a global optimum of Problem 6, then U,V are jointly equalized.",3. Single hidden-layer linear networks,[0],[0]
"Furthermore, it holds that
R(U,V) = λ
r ( r∑ i=1 ‖ui‖‖vi‖",3. Single hidden-layer linear networks,[0],[0]
),3. Single hidden-layer linear networks,[0],[0]
"2 = λ r ‖UV>‖2∗
Remark 3.5.",3. Single hidden-layer linear networks,[0],[0]
"As in the case of autoencoders with tied weights in Section 2, a complete characterization of the implicit bias of dropout is given by considering the global optimality in terms of the network, i.e. in terms of the product of the weight matrices UV>.",3. Single hidden-layer linear networks,[0.9559370059432798],"['For the sake of simplicity of analysis, we focus on the case of autoencoders with tied weight as in Section 2.']"
"Not surprisingly, even in the case of single hidden-layer networks, dropout promotes sparsity, i.e. favors low-rank weight matrices.
",3. Single hidden-layer linear networks,[1.000000040548527],"['Not surprisingly, even in the case of single hidden-layer networks, dropout promotes sparsity, i.e. favors low-rank weight matrices.']"
Theorem 3.6.,3. Single hidden-layer linear networks,[0],[0]
For any j ∈,3. Single hidden-layer linear networks,[0],[0]
"[r], let κj := 1j ∑j i=1",3. Single hidden-layer linear networks,[0],[0]
λi(M).,3. Single hidden-layer linear networks,[0],[0]
"Furthermore, define ρ := max{j ∈",3. Single hidden-layer linear networks,[0],[0]
[r] : λj(M) > λjκjr+λj }.,3. Single hidden-layer linear networks,[0],[0]
"Then, if (U∗,V∗) is a global optimum of Problem 6, it satisfies that U∗V>∗ = S λρκρ
r+λρ
(M).",3. Single hidden-layer linear networks,[0],[0]
"While the focus in Section 2 and Section 3 was on understanding the implicit bias of dropout in terms of the global optima of the resulting regularized learning problem, here we focus on computational aspects of dropout as an optimization procedure.",4. Geometry of the Optimization Problem,[0],[0]
"Since dropout is a first-order method (see Algorithm 1) and the landscape of Problem 4 is highly non-convex, we can perhaps only hope to find a local minimum, that too provided if the problem has no degenerate saddle points (Lee et al., 2016; Ge et al., 2015).",4. Geometry of the Optimization Problem,[0],[0]
"Therefore, in this section, we pose the following questions: What is the implicit bias of dropout in terms of local minima?",4. Geometry of the Optimization Problem,[0],[0]
Do local minima share anything with global minima structurally or in terms of the objective?,4. Geometry of the Optimization Problem,[1.0],['Do local minima share anything with global minima structurally or in terms of the objective?']
"Can dropout find a local optimum?
",4. Geometry of the Optimization Problem,[0.9999999718157814],['Can dropout find a local optimum?']
"For the sake of simplicity of analysis, we focus on the case of autoencoders with tied weight as in Section 2.",4. Geometry of the Optimization Problem,[0],[0]
"We show in Section 4.1 that (a) local minima of Problem 4 inherit the same implicit bias as the global optima, i.e. all local minima are equalized.",4. Geometry of the Optimization Problem,[0],[0]
"Then, in Section 4.2, we show that for sufficiently small regularization parameter, (b) there are no spurious local minima, i.e. all local minima are global, and (c) all saddle points are non-degenerate (see Definition 4.2).",4. Geometry of the Optimization Problem,[0],[0]
"We begin by recalling that the loss `(U,U) is rotation invariant, i.e. `(UQ,UQ) =",4.1. Implicit bias in local optima,[0],[0]
"`(U,U) for any rotation matrix Q.",4.1. Implicit bias in local optima,[0],[0]
"Now, if the weight matrix U were not equalized, then there exist indices",4.1. Implicit bias in local optima,[0],[0]
"i, j ∈",4.1. Implicit bias in local optima,[0],[0]
[r] such that ‖ui‖,4.1. Implicit bias in local optima,[0],[0]
>,4.1. Implicit bias in local optima,[0],[0]
‖uj‖.,4.1. Implicit bias in local optima,[0],[0]
We show that it is easy to design a rotation matrix (equal to identity everywhere expect for columns i and j) that moves mass from ui to uj such that the difference in the norms of the corresponding columns of UQ decreases strictly while leaving the norms of other columns invariant.,4.1. Implicit bias in local optima,[0],[0]
"In other words, this rotation strictly reduces the regularizer and hence the objective.",4.1. Implicit bias in local optima,[0],[0]
"Formally, this implies the following result.
",4.1. Implicit bias in local optima,[0],[0]
Lemma 4.1.,4.1. Implicit bias in local optima,[0],[0]
"All local optima of Problem 4 are equalized, i.e. if U is a local optimum, then ‖ui‖ = ‖uj‖ ∀i, j ∈",4.1. Implicit bias in local optima,[0],[0]
"[r].
Lemma 4.1 unveils a fundamental property of dropout.",4.1. Implicit bias in local optima,[0],[0]
"As
soon as we perform dropout in the hidden layer – no matter how small the dropout rate – all local minima become equalized.",4.1. Implicit bias in local optima,[0],[0]
"Next, we characterize the solutions to which dropout (i.e. Algorithm 1) converges.",4.2. Landscape properties,[0],[0]
We do so by understanding the optimization landscape of Problem 4.,4.2. Landscape properties,[0],[0]
"Central to our analysis, is the following notion of strict saddle property.",4.2. Landscape properties,[0],[0]
Definition 4.2 (Strict saddle point/property).,4.2. Landscape properties,[0],[0]
Let f : U → R be a twice differentiable function and let U ∈ U be a critical point of f .,4.2. Landscape properties,[0],[0]
"Then, U is a strict saddle point of f if the Hessian of f at U has at least one negative eigenvalue, i.e. λmin(∇2f(U))",4.2. Landscape properties,[0],[0]
< 0.,4.2. Landscape properties,[0],[0]
"Furthermore, f satisfies strict saddle property if all saddle points of f are strict saddle.
",4.2. Landscape properties,[0],[0]
"Strict saddle property ensures that for any critical point U that is not a local optimum, the Hessian has a significant negative eigenvalue which allows first order methods such as gradient descent (GD) and stochastic gradient descent (SGD) to escape saddle points and converge to a local minimum (Lee et al., 2016; Ge et al., 2015).",4.2. Landscape properties,[0],[0]
"Following this idea, there has been a flurry of works on studying the landscape of different machine learning problems, including low rank matrix recovery (Bhojanapalli et al., 2016), generalized phase retrieval problem (Sun et al., 2016), matrix completion (Ge et al., 2016), deep linear networks (Kawaguchi, 2016), matrix sensing and robust PCA (Ge et al., 2017) and tensor decomposition (Ge et al., 2015), making a case for global optimality of first order methods.
",4.2. Landscape properties,[0],[0]
"For the special case of no regularization (i.e. λ = 0; equivalently, no dropout), Problem 4 reduces to standard squared loss minimization which has been shown to have no spurious local minima and satisfy strict saddle property (see, e.g. (Baldi and Hornik, 1989; Jin et al., 2017)).",4.2. Landscape properties,[0],[0]
"However, the regularizer induced by dropout can potentially introduce new spurious local minima as well as degenerate saddle points.",4.2. Landscape properties,[0],[0]
"Our next result establishes that that is not the case, at least when the dropout rate is sufficiently small.",4.2. Landscape properties,[0],[0]
Theorem 4.3.,4.2. Landscape properties,[0],[0]
For regularization parameter λ < rλr(M)∑r i=1,4.2. Landscape properties,[0],[0]
"λi(M)−rλr(M)
, (a) all local minima of Problem 4 are global, and (b) all saddle points are strict saddle points.
",4.2. Landscape properties,[0],[0]
A couple of remarks are in order.,4.2. Landscape properties,[0],[0]
"First, Theorem 4.3 guarantees that any critical point U that is not a global optimum is a strict saddle point, i.e. ∇2f(U,U) has a negative eigenvalue.",4.2. Landscape properties,[0],[0]
"This property allows first order methods, such as dropout given in Algorithm 1, to escape such saddle points.",4.2. Landscape properties,[0],[0]
"Second, note that the guarantees in Theorem 4.3 hold when the regularization parameter λ is sufficiently small.",4.2. Landscape properties,[0],[0]
"Assumptions of this kind are common in the literature (see, for example (Ge et al., 2017)).",4.2. Landscape properties,[0],[0]
"While this is a sufficient condition for the result in Theorem 4.3, it is not clear if it is necessary.",4.2. Landscape properties,[0],[0]
"The optimization problem associated with learning a shallow network, i.e. Problem 6, is closely related to the optimization problem for matrix factorization.",5. Matrix Factorization with Dropout,[0],[0]
"Recall that in matrix factorization, given a matrix M ∈ Rd1×d2 , one seeks to find factors U,V that minimize `(U,V) =",5. Matrix Factorization with Dropout,[0],[0]
‖M − UV>‖2F .,5. Matrix Factorization with Dropout,[0],[0]
"Matrix factorization has recently been studied with dropout by Zhai and Zhang (2015); He et al. (2016) and Cavazza et al. (2018) where at each iteration of gradient descent on the loss function, the columns of factors U,V are dropped independently and with equal probability.",5. Matrix Factorization with Dropout,[0],[0]
"Following Cavazza et al. (2018), we can write the resulting problem as
min U∈Rd1×r,V∈Rd2×r ‖M− UV>‖2F",5. Matrix Factorization with Dropout,[0],[0]
+ λ r∑ i=1,5. Matrix Factorization with Dropout,[0],[0]
"‖ui‖2‖vi‖2, (7)
which is identical to Problem 6.",5. Matrix Factorization with Dropout,[0],[0]
"However, there are two key distinctions.",5. Matrix Factorization with Dropout,[0],[0]
"First, we are interested in stochastic optimization problem whereas the matrix factorization problem is typically posed for a given matrix.",5. Matrix Factorization with Dropout,[0],[0]
"Second, for the learning problem that we consider here, it is unreasonable to assume access to the true model (i.e. matrix M).",5. Matrix Factorization with Dropout,[0],[0]
"Nonetheless, many of the insights we develop here as well as the technical results and algorithmic contributions apply to matrix factorization.",5. Matrix Factorization with Dropout,[0],[0]
"Therefore, the goal in this section is to bring to bear the results in Sections 2, 3 and 4 to matrix factorization.
",5. Matrix Factorization with Dropout,[0],[0]
"We note that Theorem 3.6 and Theorem 3.3, both of which hold for matrix factorization, imply that there is a polynomial time algorithm to solve the matrix factorization problem.",5. Matrix Factorization with Dropout,[0],[0]
"In order to find a global optimum of Problem 7, we first compute the optimal M̄",5. Matrix Factorization with Dropout,[0],[0]
= ŨṼ > using shrinkagethresholding operation (see Theorem 3.6).,5. Matrix Factorization with Dropout,[0],[0]
"A global optimum (Ū, V̄) is then obtained by joint equalization of (Ũ, Ṽ) (see Theorem 3.3) using Algorithm 2.",5. Matrix Factorization with Dropout,[0],[0]
The whole procedure is described in Algorithm 3.,5. Matrix Factorization with Dropout,[0],[0]
"Few remarks are in order.
",5. Matrix Factorization with Dropout,[0],[0]
Remark 5.1 (Computational cost of Algorithm 3).,5. Matrix Factorization with Dropout,[0],[0]
"It is easy to check that computing ρ, M̄, Ũ and Ṽ requires computing a rank-r SVD of M, which costs O(d2r), where
Algorithm 3 Polynomial time solver for Problem 7
input Matrix M ∈ Rd2×d1 to be factorized, size of factorization r, regularization parameter λ
1: ρ← max{j ∈",5. Matrix Factorization with Dropout,[0],[0]
"[r] : λj(M) > λjκjr+λj }, where κj = 1j ∑j i=1",5. Matrix Factorization with Dropout,[0],[0]
λi(M),5. Matrix Factorization with Dropout,[0],[0]
for j ∈,5. Matrix Factorization with Dropout,[0],[0]
[r].,5. Matrix Factorization with Dropout,[0],[0]
2:,5. Matrix Factorization with Dropout,[0],[0]
"M̄← S λρκρ r+λρ (M) 3: (U,Σ,V)← svd(M̄) 4: Ũ← UΣ 12 , Ṽ← VΣ 12 5: Q← EQZ(Ũ) {Algorithm 2} 6: Ū← ŨQ, V̄← ṼQ
output Ū, V̄ {global optimum of Problem 7}
d = max{d1, d2}.",5. Matrix Factorization with Dropout,[0],[0]
"Algorithm 2 entails computing GU = U>U, which costs O(r2d) and the cost of each iterate of Algorithm 2 is dominated by computing the eigendecomposition which is O(r3).",5. Matrix Factorization with Dropout,[0],[0]
"Overall, the computational cost of Algorithm 3 is O(d2r +",5. Matrix Factorization with Dropout,[0],[0]
"dr2 + r4).
",5. Matrix Factorization with Dropout,[0],[0]
Remark 5.2 (Universal Equalizer).,5. Matrix Factorization with Dropout,[0],[0]
"While Algorithm 2 is efficient (only linear in the dimension) for any rank r, there is a more effective equalization procedure when r is a power of 2.",5. Matrix Factorization with Dropout,[0],[0]
"In this case, we can give a universal equalizer which works simultaneously for all matrices in Rd×r.",5. Matrix Factorization with Dropout,[0],[0]
Let U ∈,5. Matrix Factorization with Dropout,[0],[0]
"Rd×r, r = 2k, k ∈ N",5. Matrix Factorization with Dropout,[0],[0]
and let U = WΣV> be its full SVD.,5. Matrix Factorization with Dropout,[0],[0]
"The matrix Ũ = UQ is equalized, where Q = VZk and
Zk :=  1 k = 1 2 −k+1 2 [ Zk−1 Zk−1 −Zk−1 Zk−1 ]",5. Matrix Factorization with Dropout,[0],[0]
"k > 1 .
",5. Matrix Factorization with Dropout,[0],[0]
"Finally, we note that Problem 7 is an instance of regularized matrix factorization which has recently received considerable attention in the machine learning literature (Ge et al., 2016; 2017; Haeffele and Vidal, 2017).",5. Matrix Factorization with Dropout,[0],[0]
"These works show that the saddle points of a class of regularized matrix factorization problems have certain “nice” properties (i.e. escape directions characterized by negative curvature around saddle points) which allow variants of first-order methods such as perturbed gradient descent (Ge et al., 2015; Jin et al., 2017) to converge to a local optimum.",5. Matrix Factorization with Dropout,[0],[0]
"Distinct from that line of research, we completely characterize the set of global optima of Problem 7, and provide a polynomial time algorithm to find a global optimum.
",5. Matrix Factorization with Dropout,[0],[0]
"The work most similar to the matrix factorization problem we consider in this section is that of Cavazza et al. (2018), with respect to which we make several important contributions: (I)",5. Matrix Factorization with Dropout,[0],[0]
"Cavazza et al. (2018) characterize optimal solu-
tions only in terms of the product of the factors, and not in terms of the factors themselves, whereas we provide globally optimal solutions in terms of the factors; (II) Cavazza et al. (2018) require the rank r of the desired factorization to be variable and above some threshold, whereas we consider fixed rank-r factorization for any r; (III) Cavazza et al. (2018) can only find low rank solutions using an adaptive dropout rate, which is not how dropout is used in practice, whereas we consider any fixed dropout rate; and (IV) we give an efficient poly time algorithm to find optimal factors.",5. Matrix Factorization with Dropout,[0],[0]
Dropout is a popular algorithmic technique used for avoiding overfitting when training large deep neural networks.,6. Empirical Results,[0],[0]
The goal of this section is not to attest to the already wellestablished success of dropout.,6. Empirical Results,[0],[0]
"Instead, the purpose of this section is to simply confirm the theoretical results we showed in the previous section, as a proof of concept.
",6. Empirical Results,[0],[0]
We begin with a toy example in order to visually illustrate the optimization landscape.,6. Empirical Results,[0],[0]
We use dropout to learn a simple linear auto-encoder with one-dimensional input and output (i.e. a network represented by a scalar M = 2) and a single hidden layer of width r = 2.,6. Empirical Results,[0],[0]
The input features are sampled for a standard normal distribution.,6. Empirical Results,[0],[0]
"Figure 2 shows the optimization landscape along with the contours of the level sets, and a trace of iterates of dropout (Algorithm 1).",6. Empirical Results,[0],[0]
"The initial iterates and global optima (given by Theorem 2.4) are shown by red and green dots, respectively.",6. Empirical Results,[0],[0]
"Since at any global optimum the weights are equalized, the optimal weight vector in this case is parallel to the vector (±1,±1).",6. Empirical Results,[0],[0]
"We see that dropout converges to a global minimum.
",6. Empirical Results,[0],[0]
"For a second illustrative experiment, we use Algorithm 1 to train a shallow linear network, where the input x ∈ R80",6. Empirical Results,[0],[0]
is distributed according to the standard Normal distribution.,100 101 102 103 104 105,[0],[0]
"The output y ∈ R120 is generated as y = Mx, where M ∈ R120×80 is drawn randomly by uniformly sampling the right and left singular subspaces and with a spectrum decaying exponentially.",100 101 102 103 104 105,[0],[0]
"Figure 3 illustrates the behavior of Algorithm 1 for different values of the regularization parameter (λ ∈ {0.1, 0.5, 1}), and for different sizes of factors (r ∈ {20, 80}).",100 101 102 103 104 105,[0],[0]
"The curve in blue shows the objective value for the iterates of dropout, and the line in red shows the optimal value of the objective (i.e. objective for a global optimum found using Theorem 3.6).",100 101 102 103 104 105,[0],[0]
"All plots are averaged over 50 runs of Algorithm 1 (averaged over different random initializations, random realizations of Bernoulli dropout, as well as random draws of training examples).
",100 101 102 103 104 105,[0],[0]
"To verify that the solution found by dropout actually has equalized factors, we consider the following measure.",100 101 102 103 104 105,[0],[0]
"At each iteration, we compute the “importance scores”, α(i)t = ‖uti‖‖vti‖, i ∈",100 101 102 103 104 105,[0],[0]
"[r], where uti and vti are the i-th columns of Ut and Vt, respectively.",100 101 102 103 104 105,[0],[0]
"The rightmost panel of Figure 3 shows the variance of α(i)t ’s, over the hidden nodes",100 101 102 103 104 105,[0],[0]
i ∈,100 101 102 103 104 105,[0],[0]
"[r], at each iterate t. Note that a high variance in αt corresponds to large variation in the values of ‖uti‖‖vti‖. When the variance is equal to zero, all importance scores are equal, thus the factors are equalized.",100 101 102 103 104 105,[0],[0]
"We see that iterations of Algorithm 1 decrease this measure monotonically, and the larger the value of λ, the faster the weights become equalized.",100 101 102 103 104 105,[0],[0]
"There has been much effort in recent years to understand the theoretical underpinnings of dropout (see Baldi and Sad-
owski (2013); Gal and Ghahramani (2016); Wager et al. (2013); Helmbold and Long (2015)).",7. Discussion,[0],[0]
"In this paper, we study the implicit bias of dropout in shallow linear networks.",7. Discussion,[0],[0]
We show that dropout prefers solutions with minimal path regularization which yield strong capacity control guarantees in deep learning.,7. Discussion,[0],[0]
"Despite being a non-convex optimization problem, we are able to fully characterize the global optima of the dropout objective.",7. Discussion,[0],[0]
Our analysis shows that dropout favors low-rank weight matrices that are equalized.,7. Discussion,[0],[0]
"This theoretical finding confirms that dropout as a procedure uniformly allocates weights to different subnetworks, which is akin to preventing co-adaptation.
",7. Discussion,[0],[0]
We characterize the optimization landscape of learning autoencoders with dropout.,7. Discussion,[0],[0]
"We first show that the local optima inherit the same implicit bias as global optimal, i.e. all local optima are equalized.",7. Discussion,[0],[0]
"Then, we show that for sufficiently small dropout rates, there are no spurious local minima in the landscape, and all saddle points are non-degenerate.",7. Discussion,[0],[0]
"These properties suggest that dropout – as an optimization procedure – can efficiently converge to a globally optimal solution specified by our theorems.
",7. Discussion,[0],[0]
Understanding dropout in shallow linear networks is a prerequisite for understanding dropout in deep learning.,7. Discussion,[0],[0]
"We see natural extensions of our results in two directions: 1) shallow networks with non-linear activation function such as rectified linear units (ReLU) which have been shown to enable faster training (Glorot et al., 2011) and are better understood in terms of the family of functions represented by ReLU-nets (Arora et al., 2018), and 2) exploring the global optimality in deeper networks, even for linear activations.",7. Discussion,[0],[0]
This research was supported in part by NSF BIGDATA grant IIS-1546482 and NSF grant IIS-1618485.,Acknowledgements,[0],[0]
Algorithmic approaches endow deep learning systems with implicit bias that helps them generalize even in over-parametrized settings.,abstractText,[0],[0]
"In this paper, we focus on understanding such a bias induced in learning through dropout, a popular technique to avoid overfitting in deep learning.",abstractText,[0],[0]
"For single hidden-layer linear neural networks, we show that dropout tends to make the norm of incoming/outgoing weight vectors of all the hidden nodes equal.",abstractText,[0],[0]
"In addition, we provide a complete characterization of the optimization landscape induced by dropout.",abstractText,[0],[0]
On the Implicit Bias of Dropout,title,[0],[0]
