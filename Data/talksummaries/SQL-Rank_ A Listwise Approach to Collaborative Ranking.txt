0	33	We study a novel approach to collaborative ranking—the personalized ranking of items for users based on their observed preferences—through the use of listwise losses, which are dependent only on the observed rankings of items by users.
5	13	Collaborative filtering, introduced in (Hill et al., 1995), refers to the use of an entire community’s preferences to better predict the preferences of an individual (see (Schafer et al., 2007) for an overview).
6	11	In systems where users provide ratings of items, collaborative filtering can be approached as a pointwise prediction task, in which we attempt to predict the unobserved ratings (Pan et al., 2017).
11	15	Many other techniques are also introduced (Kabbur et al., 2013; Wang et al., 2017; Wu et al., 2016).
15	28	Because users have different standards for ratings, it is often desirable for ranking algorithms to rely only on relative rankings and not absolute ratings.
16	13	A ranking loss is one that only considers a user’s relative preferences between items, and ignores the absolute value of the ratings entirely, thus deviating from the pointwise framework.
17	69	Ranking losses can be characterized as pairwise and listwise.
18	12	A pairwise method decomposes the objective into pairs of items j, k for a user i, and effectively asks ‘did we successfully predict the comparison between j and k for user i?’.
20	7	Because the pairwise model has cast the problem in the classification framework, then tools like support vector machines were used to learn rankings; (Joachims, 2002) introduces rankSVM and efficient solvers can be found in (Chapelle & Keerthi, 2010).
23	13	Pairwise methods for personalized ranking have seen great advances in recent years, with the AltSVM algorithm of (Park et al., 2015), Bayesian personalized ranking (BPR) of (Rendle et al., 2009), and the near linear-time algorithm of (Wu et al., 2017).
26	12	The listwise permutation model, introduced in (Cao et al., 2007), can be thought of as a weighted urn model, where items correspond to balls in an urn and they are sequentially plucked from the urn with probability proportional to (X ij ) where X ij is the latent score for user i and item j and is some non-negative function.
39	12	• We provide an O(iter · (|⌦|r)) linear algorithm based on stochastic gradient descent, where ⌦ is the set of observed ratings and r is the rank.
42	10	The permutation probability, (Cao et al., 2007), is a generative model for the ranking parametrized by latent scores.
43	16	First assume there exists a ranking function that assigns scores to all the items.
44	11	Let’s say we have m items, then the scores assigned can be represented as a vector s = (s1, s2, ..., sm).
48	17	is an increasing and strictly positive function.
63	16	We will discuss how we can deal with ties in the following subsection, namely, when the ranking is derived from ratings and multiple items receive the same rating, then there is ambiguity as to the order of the tied items.
64	7	This is a common occurrence when the data is implicit, namely the output is whether the user engaged with the item or not, yet did not provide explicit feedback.
69	17	Note that for simplicity we assume all the users have the same number m̄ of ratings, but this can be easily generalized to the nonuniform case by replacing m̄ with m i (number of ratings for user i).
72	26	.⇧n, row by row, to get the permutation matrix ⇧ 2 Rn⇥m.
73	8	Assuming users are independent with each other, the probability of observing a particular ⇧ given the scoring matrix X can be written as P (k,m̄) X (⇧) = nY i=1 P (k,m̄) Xi (⇧ i ).
77	13	In such cases, the permutation ⇧ is no longer unique and there is a set of permutations that coincides with rating because with any candidate ⇧ we can arbitrarily shuffle the ordering of items with the same relevance scores to generate a new candidate matrix ⇧0 which is still valid (see Figure 1).
79	77	We call this shuffling process the Stochastic Queuing Process, since one can imagine that by permuting ties we are stochastically queuing new ⇧’s for future use in the algorithm.
82	7	To enforce low-rankness, we use the nuclear norm regularization X = {X : kXk⇤  r}.
86	10	In the explicit feedback setting, it is assumed that the matrix R is partially observed and the observed entries are explicit ratings in a range (e.g., 1 to 5).
87	69	We will show in the experiments that k = m̄ (using the full list) leads to the best results.
88	9	(Huang et al., 2015) also observed that increasing k is useful for their cross-entropy loss, but they were not able to increase k since their model has time complexity exponential to k. In the implicit feedback setting each element of R ij is either 1 or 0, where 1 means positive actions (e.g., click or like) and 0 means no action is observed.
89	17	Directly solving (5) will be expensive since m̄ = m and the computation will involve all the mn elements at each iteration.
90	14	Moreover, the 0’s in the matrix could mean either a lower relevance score or missing, thus should contribute less to the objective function.
92	17	For each user (row of R), assume there are m̃ 1’s, we then sample ⇢m̃ unobserved entries uniformly from the same row and append Algorithm 1 SQL-Rank: General Framework Input: ⌦, {R ij : (i, j) 2 ⌦}, 2 R+, ss, rate, ⇢ Output: U 2 Rr⇥n and V 2 Rr⇥m Randomly initialize U, V from Gaussian Distribution repeat Generate a new permutation matrix ⇧ {see alg 2} Apply gradient update to U while fixing V Apply gradient update to V while fixing U {see alg 4} until performance for validation set is good return U, V {recover score matrix X} Algorithm 2 Stochastic Queuing Process Input: ⌦, {R ij : (i, j) 2 ⌦}, ⇢ Output: ⇧ 2 Rn⇥m for i = 1 to n do Sort items based on observed relevance levels R i Form ⇧ i based on indices of items in the sorted list Shuffle ⇧ i for items within the same relevance level if Dataset is implicit feedback then Uniformly sample ⇢m̃ items from unobserved items Append sampled indices to the back of ⇧ i end if end for Stack ⇧ i as rows to form matrix ⇧ Return ⇧ {Used later to compute gradient} to the back of the list.
