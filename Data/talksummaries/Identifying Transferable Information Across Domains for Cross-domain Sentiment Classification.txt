1	26	For example, entertaining and boring are frequently used in the movie domain to express an opinion; however, finding these words in the electronics domain is rare.
2	32	Moreover, there are words which are likely to be used across domains in the same proportion, but may change their polarity orientation from one domain to another (Choi et al., 2009).
3	42	For example, a word like unpredictable is positive in the movie domain (un- predictable plot), but negative in the automobile domain (unpredictable steering).
4	27	Such a polarity changing word should be assigned positive orientation in the movie domain and negative orientation in the automobile domain.1 Due to these differences across domains, a supervised algorithm trained on a labeled source domain, does not generalize well on an unlabeled target domain and the cross-domain performance degrades.
7	39	On the other hand, domain adaptation techniques work in contrast to traditional supervised techniques on the principle of transferring learned knowledge across domains (Blitzer et al., 2007; Pan et al., 2010; Bhatt et al., 2015).
11	36	In this paper, we propose that the words which are equally significant with a consistent polarity across domains represent the usable information for cross-domain sentiment analysis.
47	38	Moreover, words that are significant in both the domains, but have different polarity orientation transfer the wrong information to the target domain through a supervised classifier trained in the labeled source domain, which also downgrade the cross-domain performance.
52	79	We have used goodness of fit chi2 test with equal number of reviews in positive and negative corpora.
57	28	There is an inverse relation between χ2 value and the p-value which is probability of the data given null hypothesis is true.
63	44	For instance, if c w p is higher than cwn , then the word is positive, else negative.
66	19	We presume that a word which is significant in the source domain as per χ2 test and occurs with a frequency greater than a certain threshold (θ) in the target domain is significant in the target domain also.
68	41	Here, function significants assures the significance of the word w in the labeled source (s) domain and countt gives the normalized count of the w in t.5 χ2 test has one key assumption that the expected value of an observed variable should not be less than 5 to be significant.
69	37	Considering this assumption as a base, we fix the value of θ as 10.6 Polarity of Words in the Unlabeled Target Domain: Generally, in a polar corpus, a positive word occurs more frequently in context of other positive words, while a negative word occurs in context of other negative words (Sharma et al., 2015).7 Based on this hypothesis, we explore the contextual information of a word that is captured well by its context vector to assign polarity to words in the target domain (Rill et al., 2012; Rong, 2014).
70	31	Mikolov et al., (2013) showed that similarity between context vector of words in vicinity such as ‘go’ and ‘to’ is higher compared to distant words or words that are not in the neighborhood of each other.
71	53	Here, the observed concept is that if a word is positive, then its context vector learned from the polar review corpus will give higher cosine-similarity with a known positive polarity word in comparison to a known negative polarity word or vice versa.
73	20	We term known polarity words as Positivepivot and Negative-pivot.
76	36	The decision method given in Equation 3 defines the polarity assignment to the unknown polarity words of the target domain.
82	23	The words which are found significant in both the domains with the same polarity orientation form a set of SCP features for cross-domain sentiment classification.
88	84	The proposed cross-domain adaptation approach (Algorithm 1) attempts to learn such domain specific features from the target domain using a classifier trained on SCP words in the source domain.
92	37	Identify SCP features from the labeled source and the unlabeled target domain data.
98	39	The top n confidently predicted pseudo labeled instances (Rnt ) are used to train classifier Ct, where n depends on a threshold that is empirically set to | ± 0.2|.10 The classifier Cs trained on the SCP features (transferred knowledge) from the source domain and the classifier Ct trained on self-discovered target specific features from the pseudo labeled target domain instances bring in complementary information from the two domains.
101	29	Input: Dls = {r1s , r2s , r3s , ....rjs}, Dut = {r1t , r2t , r3t , ....rkt }, Vs = {w1s , w2s , w3s , ....wps}, Vt = {w1t , w2t , w3t , ....wqt } Output: Sentiment Classifier in the Target Domain 1: SCP = sigPol(Dls) ∩ sigPol(Dut ) 2: Cs = Train-SVM(Dls), where f = SCP 3: Predict Label: Cs(Dut )→ Dlt 4: Select: Rnt | ∀rit ∈ Dut , Cs(rit) > φ, where i ∈ {1, 2....k} and n <= k 5: Ct = Train-SVM(Rnt ), where f = {unigrams(Rnt )} 6: WSM = (Cs ∗Ws + Ct ∗Wt)/(Ws +Wt) 7: Sentiment Classifier in the Target Domain = WSM ALGORITHM 1: Building of target domain classifier from the source domain Weighted Sum Model (WSM): The weighted ensemble of classifiers helps to overcome the errors produced by the individual classifier.
103	44	If Cs has wrongly predicted a document at boundary point and Ct has predicted the same document confidently, then weighted sum of Cs and Ct predicts the document correctly or vice versa.
104	23	For example, a document is classified by Cs as negative (wrong prediction) with a classification-score of −0.07, while the same document is classified by Ct as positive (correct prediction) with a classification-score of 0.33, the WSM of Cs and Ct will classify the document as positive with a classification-score of 0.12 (Equation 4).
128	65	It is used to determine whether two sets of data are significantly different or not.14 Our approach performs significantly better than SCL and common unigrams, while SCL performs better than common unigrams as per ttest.
131	30	Therefore, in Table 4 results are reported with electronics as the source domain and movie as the target domain.15 In all four cases, there is difference in the transferred information from the source to the target, but the ensemblebased classification algorithm (Section 3.2) is the same.
150	54	We observed the same trend after the inclusion of the iterative process also, as the SCPbased system-6 performed the best in all 12 cases.
151	72	On the other hand, SCL-based system-5 performs better than the common-unigrams based system4.
152	60	Table 7 shows the results of significance test (ttest) performed on the accuracy distributions produced by the six different systems.
159	20	To validate our assertion that polarity preserving significant words (SCP) across source and target domains make a less erroneous set of transferable knowledge from the source domain to the target domain, we computed Pearson productmoment correlation between F-score obtained for our approach (cf.
