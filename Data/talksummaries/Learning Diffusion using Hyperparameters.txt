23	4	Our goal in this paper is to explore a hyperparametric approach for learning the independent cascade diffusion model.
31	1	Specifically, when comparing with the state-of-the-art bound for learning the independent cascade model (without the hyperparametric assumption) we show that the sample complexity can be reduced by a factor of |E|/d, as foreshadowed by the example above.
34	13	Lastly, we show that the hyperparametric approach does work in practice.
36	1	We show that with a hyperparameter of dimension 40 one can estimate the diffusion probabilities with remarkably high accuracy.
46	1	Every node u ∈ V is associated with a vector of features containing information about it.
54	1	The input to a learning algorithm is a collection of labeled samples.
55	1	We assume that there is some unknown distribution D0 over subset of nodes, that activates the initial seed of the cascade, V0.
56	1	Subsequently, as we discussed before, we can partition V \ V0 into subsets of nodes V1, V2, .
59	1	, n− 1} consider all the nodes v /∈ ∪τ−1t=0 Vt that are within distance of 1 from Vτ .
80	3	Theorem 1 ((Shalev-Shwartz & Ben-David, 2014)).
81	3	Assume that for every sample s ∼ D and every θ ∈ H we have that: |L(s, θ)| ≤ C. Let S ∼ Dm and θ̂ = arg maxθ∈H { 1 m ∑m i=1 L (si, θ) } .
97	2	Hence, we can focus on bounding the Rademacher complexity of H instead of that of H. Since H is finite, the well-known Massart’s lemma apply yielding: R(S,H) ≤ R(S,H ) + 2 ≤ 2 max θ∈H ∣∣∣∣∣∣(L(si, θ))mi=1∣∣∣∣∣∣2 √ 2 log(|H |) m + 2 ≤ 2 √ m∆ ln 1 λ · √ 2 log(|H |) m + 2 = 2∆ ln 1 λ √ 2 log(|H |) m + 2 = O ( ∆ log 1 λ √ d log(Bρd/λ∆ ) m ) + 2 where the first inequality holds because of Lemma 4 (discretization), the second because of Massart’s lemma (finite hypothesis class), the third because of Lemma 3 (bounded L), and the last one because of Lemma 1 (covering number).
104	3	To understand this structure, note that there are only three distinct cases for a sample s = ((X, v), y) in the training set S: (i) node v was not influenced, (ii) node v was influenced and there is only one neighbor of v in X and (iii) node v was influenced and there is more than one neighbor of v in X .
109	2	The magnitude of the noise depends on the probability of seeing an obfuscated sample, which characterizes the difficulty of the optimization problem and can be computed in simple cases (see e.g. Lemma 8 in Appendix F).
120	3	First, using synthetic datasets we show that if the hyperparametric assumption holds in a network, we can accurately learn the edge probabilities despite the non-concavity of (2), and significantly outperform methods that do not include information about the node features, for small training sets4.
124	2	We synthetically generate the social graph and the hyperparameter that determines the diffusion probabilities.
137	1	• Hyperparametric MLE, reduced information: We com- pare ourselves against a hyperparametric model that is unaware of the exact features that are important, and selects only a subset of them.
147	3	Regarding the two benchmarks that involve the hyperparametric assumption it is worth noting that reduced information does not allow convergence to 0 error, while augmented information does (see also Figure 4d for a more detailed investigation).
149	6	The learning effect is evident and universal though, since the error converges to 0 independently of the underlying network.
152	3	Recall that if we ignore the obfuscated samples, the optimization problem becomes concave.
160	3	Hence, the information obtained becomes “noisier” and convergence is slower.
161	1	• Noisy model: Until now we assumed that the hyperparametric model is the ground truth.
163	1	We generate each edge probability as before, and subsequently add noise uniform in [−N,N ], for increasing values ofN .
176	3	A user is influenced if she marked herself as “going” to the event.
186	2	An important difference with the experiments of Sections 5.1 and 5.2 is that here we don’t know the true diffusion probability of every edge by construction.
195	2	In the case of the reduced model we used only 20% of the most important features of each edge (measured using Mutual Information).
197	13	As in the experiments of Section 5.1, the reduced model converges to higher average error than the models that use more information, while the augmented model successfully ignores all the redundant features.
198	15	We also evaluated the sensitivity of the hyperparametric model when we include all versus few selected features.
199	184	The picture that we see matches the synthetic experiments (Figure 4d), i.e. the hyperparametric model is supported on several features and if we fail to include all of them our error won’t converge to 0.
200	178	However, an important difference with the synthetic experiments is that here not all the features are equally important, hence by applying feature-selection algorithms we can collect a small subset that performs almost as well as using the entire feature vector (see e.g. the difference in the error between 20% and 90% of the features).
