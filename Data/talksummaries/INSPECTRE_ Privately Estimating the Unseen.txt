69	24	We will start with some definitions.
70	8	, p(k)) : p(i) ≥ 0, ∑k i=1 p(i) = 1, 1 ≤ k ≤ ∞} be the set of discrete distributions over a countable support.
71	9	Let ∆k be the set of distributions in ∆ with at most k non-zero probability values.
72	51	A property f(p) is a mapping from ∆→ R. We now describe the classical distribution property estimation problem, and then state the problem under differential privacy.
74	35	The sample complexity of f̂ , Cf̂ (f, α, β) def= min{n : Pr (∣∣∣f̂(Xn1 )− f(p)∣∣∣ > α) < β} is the smallest number of samples to estimate f to accuracy α, and error β.
80	25	Similar to the non-private setting, the sample complexity of ε-differentially private estimation problem is C(f, α, ε) = minf̂ :f̂ is ε-DP Cf̂ (f, α, 1/3), the smallest number of samples n for which there exists such an ε-DP ±α estimator with error probability at most 1/3.
81	25	In their original paper (Dwork et al., 2006) provides a scheme for differential privacy, known as the Laplace mechanism.
82	38	This method adds Laplace noise to a non-private scheme in order to make it private.
84	14	The sensitivity of an estimator f̂ : [k]n → R is ∆n,f̂ def= maxdham(Xn1 ,Y n1 )≤1 ∣∣∣f̂(Xn1 )− f̂(Y n1 )∣∣∣ .
87	12	Recall that the probability density function of Lap(b) is 12be − |x|b , hence we have Pr (|X| > α/2) < 1e2 .
93	7	The support size of a distribution p is S(p) = |{x : p(x) > 0}|, the number of symbols with nonzero probability values.
95	27	To circumvent this issue, (Raskhodnikova et al., 2009) proposed to study the problem when the smallest probability is bounded.
97	15	For p ∈ ∆≥ 1k , our goal is to estimate S(p) up to ±αk with the least number of samples from p. Support Coverage.
99	42	Support coverage arises in many ecological and biological studies (Colwell et al., 2012) to quantify the number of new elements (gene mutations, species, words, etc) that can be expected to be seen in the future.
101	8	The Shannon entropy of a distribution p is H(p) = ∑ x p(x) log 1 p(x) , H(p) is a central object in information theory (Cover & Thomas, 2006), and also arises in many fields such as machine learning (Nowozin, 2012), neuroscience (Berry et al., 1997; Nemenman et al., 2004), and others.
102	19	Estimating H(p) is hard with any finite number of samples due to the possibility of infinite support.
115	8	Furthermore, C(H,α, ε) = Ω ( k α log k + log2(min{k, n}) α2 + log k αε ) .
126	22	In other words, for most meaningful values of ε, privacy comes for free.
127	10	In the nonsublinear regime for these problems, we provide upper and lower bounds which match in a number of cases.
128	18	We note that in this regime, the cost of privacy may not be a lower order term – however, this regime only occurs when one requires very high accuracy, or unreasonably large privacy, which we consider to be of somewhat lesser interest.
137	12	We then show that this choice of parameter generalizes, giving highly-accurate private estimation in other instances, on both synthetic and real-world data.
139	11	We first describe and analyze our algorithms, and then go on to describe and analyze a lower bound construction, showing that our upper bounds are almost tight.
142	35	Privatize this estimate by adding Laplace noise, where the parameter is determined through analysis of the estimator and potentially computation of the estimator’s sensitivity.
143	9	We split the analysis into two regimes.
145	11	Note that the problem is identical for any α < 1m , since this corresponds to estimating the support coverage exactly, and the above bound simplifies to O ( m2 + mε ) .
156	32	Our private estimator of support coverage is derived by adding Laplace noise to this non-private estimator with the appropriate noise parameter, and thus the performance of our private estimator, is analyzed by bounding the sensitivity and the bias of this non-private estimator according to Lemma 1.
178	10	Hence we know there support coverage differs by Ω(αm).
182	8	samples from the two distributions with an expected Hamming distance of dTV(p, q) · n. Using Lemma 5 and dTV(u1, u2) = α1+α , we have Lemma 6.
187	14	Overall, we found that privacy is quite cheap: private estimators achieve accuracy which is comparable or near-indistinguishable to non-private estimators in many settings.
190	14	We compare the performance of our entropy estimator with a number of alternatives, both private and non-private.
