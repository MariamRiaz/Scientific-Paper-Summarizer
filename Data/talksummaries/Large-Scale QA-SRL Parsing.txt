2	59	In this paper, we show for the first time that it is possible to go even further by crowdsourcing a large ∗ Much of this work was done while these authors were at the Allen Institute for Artificial Intelligence.
3	21	scale dataset that can be used to train high quality parsers at modest cost.
4	22	We adopt the Question-Answer-driven Semantic Role Labeling (QA-SRL) (He et al., 2015) annotation scheme.
7	58	He et al. (2015) showed that high precision QA-SRL annotations can be gathered with limited training but that high recall is challenging to achieve; it is relatively easy to gather answerable questions, but difficult to ensure that every possible question is labeled for every verb.
8	21	For this reason, they hired and trained hourly annotators and only labeled a relatively small dataset (3000 sentences).
14	55	Using this data, our second contribution is a comparison of several new models for learning a QA-SRL parser.
24	89	A QA-SRL annotation consists of a set of question-answer pairs for each verbal predicate in a sentence, where each answer is a set of contiguous spans from the sentence.
26	43	We introduce a crowdsourcing pipeline to collect annotations rapidly, cheaply, and at large scale.
28	31	In the generation step, a sentence with one of its verbs marked is shown to a single worker, who must write QASRL questions for the verb and highlight their answers in the sentence.
29	22	The questions are passed to the validation step, where n workers answer each question or mark it as invalid.
31	25	Instructions Workers are instructed that a valid question-answer pair must satisfy three criteria: 1) the question is grammatical, 2) the questionanswer pair is asking about the time, place, participants, etc., of the target verb, and 3) all correct answers to each question are given.
32	51	Autocomplete We provide an autocomplete drop-down to streamline question writing.
39	30	The validation step pays 8c per verb, plus a 2c bonus per question beyond four.
45	27	Annotation in our pipeline with n = 2 valida- 3www.wiktionary.org tors took 9 days on Amazon Mechanical Turk.4 1,165 unique workers participated, annotating a total of 299,308 questions.
74	31	Unlabeled span detection selects a set Sv of spans as arguments for a given verb v. 3.
77	31	We describe two models for unlabeled span detection in section 3.1, followed by question generation in section 3.2.
78	27	All models are built on an LSTM encoding of the sentence.
79	23	Like He et al. (2017), we start with an input Xv = {x0 .
92	23	(2) The probability that the span is an argument of predicate v is computed by the sigmoid function: p(yij |Xv) = σ(wᵀspanMLP(svij) + bspan) (3) At training time, we minimize the binary cross entropy summed over all n2 possible spans, counting a span as a positive example if it appears as an answer to any question.
93	33	At test time, we choose a threshold τ and select every span that the model assigns probability greater than τ , allowing us to trade off precision and recall.
98	33	The local model predicts the words for each slot independently: p(yk |Xv, svij) ∝ exp(wᵀkMLP(svij) + bk).
101	48	Since each question slot predicts from a different set of words, we found it beneficial to use separate weights for the LSTM cells at each slot k. During training, we feed in the gold token at the previous slot, while at test time, we use the predicted token.
114	22	For instance, the question “Who did someone blame something on?” may be rephrased as “Who was blamed for something?” However, due to the constrained space of possible questions defined by QA-SRL’s slot format, accuracy-based metrics can still be informative.
118	47	The sequential model’s exact match accuracy is significantly higher, while word-level accuracy is roughly comparable, reflecting the fact that the local model learns the slot-level posteriors.
121	29	In Section 6, we use human evaluation to get a more accurate assessment of our model’s accuracy.
141	26	We test 3 parsers: the span-based span detection model paired with each of the local and sequential question generation models trained on the initial dataset, and our final model (span-based span detection and sequential question generation) trained with the expanded data.
144	43	We then compute question and span accuracy as follows: A question is considered correct if 5 out of 6 annotators consider it valid, and a span is considered correct if its generated question is correct and the span is among those selected for the question by validators.
149	25	If we choose a threshold value which gives a similar number of questions per sentence as were labeled in the original data annotation (2 questions / verb), question and span accuracy are 82.64% and 77.61%, respectively.
151	277	The model was overall highly accurate—only one question and 3 spans are considered incorrect, and each mistake is nearly correct,6 even when the sentence contains a negation.
163	43	We demonstrated the utility of this data by training the first parser which is able to produce high-quality QA-SRL structures.
