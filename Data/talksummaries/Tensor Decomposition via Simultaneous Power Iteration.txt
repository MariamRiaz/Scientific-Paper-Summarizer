1	20	They are used to model multi-relational or multi-modal data, and their decompositions often reveal some underlying structures behind the observed data.
2	98	See (Kolda & Bader, 2009) for a survey of such results.
3	53	Recently, they have found applications in machine learning, particularly for learning various latent variable models (Anandkumar et al., 2012; Chaganty & Liang, 2014; Anandkumar et al., 2014a).
4	33	One popular decomposition method in such applications is the CP (Candecomp/Parafac) decomposition, which decomposes the given tensor as a sum of rank-one components.
5	25	This is similar to the singular value decomposition (SVD) of matrices, and a popular approach for SVD is the power method, which is well-understood and has nice theoretical guarantee.
6	41	As tensors can be seen as generalization of matrices to higher orders, one would hope that a natural generalization of the power method to tensors could inherit the success from the matrix case.
7	23	However, the situation turns out to be much more complicated for tensors (see e.g. the discussion in (Anandkumar et al., 2014a)), and in fact several problems related to tensor decomposition are known to be NP-hard (Hillar & Lim, 2013).
8	5	Nevertheless, when the given tensor has some additional structure, the tensor decomposition problem becomes tractable again.
11	30	The first is that while the matrix power method can guarantee that a randomly selected initial vector will almost surely converge to the top singular vector, we have much less control of where the convergence goes in the tensor case.
12	7	Consequently, most previous works based on the tensor power method with theoretical guarantee, such as (Anandkumar et al., 2014a;b; Wang & Anandkumar, 2016), require much more complicated procedures.
14	17	Moreover, to find each vector, they need to sample several initial vectors and apply the power method on all of them, before selecting just one from them.
17	13	Then a natural question is: can we inherit the best of both worlds?
18	7	Namely, is it possible to have a simple algorithm which can find the k eigenvectors of a tensor simultaneously and converge faster than that for matrices?
20	18	This arises in applications such as learning latent variable models, in which the tensor we have access to is obtained from some empirical average of the observed data.
24	24	To apply such a result, we need an efficient way to find such initial vectors.
25	53	We show how to do this by choosing a good direction to project the tensor down to a matrix while preserving the eigengaps, and then applying the matrix power method for only a few iterations just to obtain vectors meeting that sufficient condition.
27	29	The result stated above is for orthogonal tensors.
31	13	Our second contribution is to provide an efficient way to find a whitening matrix, by simply applying only one iteration of the matrix power method.
32	22	While most previous works on tensor decomposition focus on the batch setting, storing even a tensor of order three requires Ω(d3) space, which is infeasible for a large d. We show to avoid this in the streaming setting, with a stream of data arriving one at a time, which is the only source of information about the tensor.
33	6	We provide a streaming algorithm using only O(kd) space, which is the smallest possible, just enough to store the k eigenvectors of dimension d. To achieve an approximation error ε, the total number of samples we need is O(kd log d+ 1ε2 log(d log 1 ε )).
38	31	On the other hand, one advantage of their algorithm is that its running time does not depend on the eigengaps, while ours has the dependence hidden above as some constant.
39	5	In the streaming setting, Wang & Anandkumar (2016) provided an algorithm using O(dk log k) memory and O( kε2 log( d ε )) samples, while ours only uses O(dk) memory andO( 1ε2 log(d log log 1 ε )) samples.
40	8	1 Nevertheless, the sample complexity of Wang & Anandkumar (2016) is also independent of the eigengaps, while ours has the dependence hidden above as a constant factor.
43	15	Although not directed related, let us also compare to previous works on SVD.
45	5	Both bounds are worse than ours and also depend on the eigengaps.
48	14	For example, the online SGD approach of (Ge et al., 2015) works only for tensors of even orders and its sample complexity has a poor dependency on the dimension d. Organization of the paper.
49	11	First, we provide some preliminaries in Section 2.
54	14	Let us first introduce some notations and definitions which we will use later.
55	7	Let R denote the set of real numbers and N the set of positive integers.
56	8	Let N (0, 1) denote the standard normal distribution with mean 0 and variance 1, and letN d(0, 1), for d ∈ N, denote the d-variate one which has each of its d dimensions sampled independently from N (0, 1).
