7	19	Moreover, high testtime cost prevents DNNs from being deployed on resource constrained platforms, such as those found in Internet of Things (IoT) devices, smart phones, and wearables.
9	26	Most of the work on this topic focuses on designing more efficient network topologies and on compressing pre-trained models using various techniques (see related work below).
15	45	Our second approach, an adaptive network selection method, takes a set of pre-trained DNNs, each with a different cost/accuracy trade-off, and arranges them in a directed acyclic graph (Trapeznikov & Saligrama, 2013; Wang et al., 2015), with the the cheapest model first and the most expensive one last.
18	9	We demonstrate the merits of our techniques on the ImageNet object recognition task, using a number of popular pretrained DNNs.
19	12	The early exit technique speeds up the average test-time evaluation of GoogLeNet (Szegedy et al., 2015), and Resnet50 (He et al., 2016) by 20-30% within reasonable accuracy margins.
20	120	The network cascade achieves 2.8x speed-up compared to pure Resnet50 model at 1% top-5 accuracy loss and 1.9x speed-up with no change in model accuracy.
21	86	We also show that our method can approximate a oracle policy that can see true errors suffered for each instance.
22	10	In addition to reducing the average test-time cost of DNNs, it is worth noting that our techniques are compatible with the common design of large systems of mobile devices, such as smart phone networks or smart surveillance-camera networks.
24	20	One of the main challenges involved in designing these systems is determining whether the machine-learned models will run in the devices or in the cloud.
25	57	Offloading all of the work to the cloud can be problematic due to network latency, limited cloud ingress bandwidth, cloud availability and reliability issues, and privacy concerns.
26	17	Our approach can be used to design such a system, by deploying a small inaccurate model and an exit policy on each device and a large accurate model in the cloud.
27	103	Easy examples would be handled by the devices, while difficult ones would be forwarded to the cloud.
29	82	Such designs allow our method to be used in memory constrained settings as well due to offloading of complex models from the device.
47	17	Our first approach to reducing the test-time cost of deep neural networks is an early exit strategy.
49	30	We denote a labeled example as (x, y) ∈ Rd × {1, .
50	36	,L} is the set of classes represented in the data.
52	47	For a predicted label ŷ, we denote the loss L(ŷ, y).
54	14	In practice we upper bound the indicator functions with logistic loss for computational efficiency.
55	12	As a running DNN example, we consider the AlexNet architecture (Krizhevsky et al., 2012), which is composed of 5 convolutional layers followed 3 fully connected layers.
56	103	During evaluation of the network, computing each convolutional layer takes more than 3 times longer than computing a fully connected layer, so we consider a system that allows an example to exit the network after each of the first 4 convolutional layers.
57	112	Let ŷ(x) denote the label predicted by the network for example x and assume that computing this prediction takes a constant time of T .
58	11	Moreover, let σk(x) denote the output of the kth convolutional layer for example x and let tk denote the time it takes to compute this value (from the time that x is fed to the input layer).
59	85	Finally, let ŷk(x) be the predicted label if we exit after the kth layer.
60	41	After computing the kth convolutional layer, we introduce a decision function γk that determines whether the example should exit the network with a label of ŷk(x) or proceed to the next layer for further evaluation.
63	14	Globally, our goal is to minimize the evaluation time of the network such that the error rate of the adaptive system is no more than some user-chosen value B greater than the full network: min γ1,...,γ4 Ex∼X [Tγ1,...,γ4(x)] .
64	27	E(x,y)∼X×Y [ (L(ŷγ1, ..., γ4(x), y)− L(ŷ(x), y))+ ] ≤ B Here, Tγ1,...,γ4(x) is the prediction time for example x for the adaptive system, ŷγ1, ..., γ4(x) is the label predicted by the adaptive system for example x.
65	30	In practice, the time required to predict a label and the excess loss introduced by the adaptive system can be recursively defined.
66	131	As in (Wang et al., 2015) we can reduce the early exit policy training for minimizing the global risk to a WBC problem.
67	34	The key idea is that, for each input, a policy must identify whether or not the future reward (expected future accuracy minus comp.
69	38	To this end, we first focus on the problem of learning the decision function γ4, which determines if an example should exit after the fourth convolutional layer or whether it will be classified using the entire network.
