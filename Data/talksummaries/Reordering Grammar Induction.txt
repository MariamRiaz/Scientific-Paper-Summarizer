0	52	Preordering (Collins et al., 2005) aims at permuting the words of a source sentence s into a new order ś, hopefully close to a plausible target word order.
2	20	Preordering is often broken down into two steps: finding a suitable tree structure, and then finding a transduction function over it.
4	18	The (direct correspondence) assumption underlying this approach is that permuting the siblings of nodes in a source syntactic tree can produce a plausible target order.
5	20	An alternative approach creates reordering rules manually and then learns the right structure for applying these rules (Katz-Brown et al., 2011).
12	19	We obtain permutations in the training data by segmenting every word-aligned source-target pair into minimal phrase pairs; the resulting alignment between minimal phrases is written as a permutation (1:1 and onto) on the source side.
55	25	A similar problem was encountered in non-lexicalized monolingual parsing, and one solution was to lexicalize the productions (Collins, 2003) using head words.
56	22	But linguistic heads do not make sense for PETs, so we opt for the alternative approach (Matsuzaki et al., 2005), which splits the nonterminals and softly percolates the splits through the trees gradually fitting them to the training data.
58	112	Suppose for example node P21 could split into P211 and P212 and similarly P2413 splits into P24131 and 24132.
59	128	This means that rule P21 → P12 P2413 will form eight new rules: P211 → P121 P24131 P211 → P121 P24132 P211 → P122 P24131 P211 → P122 P24132 P212 → P121 P24131 P212 → P121 P24132 P212 → P122 P24131 P212 → P122 P24132 Should we want to split each nonterminal into 30 subcategories, then an n-ary rule will split into 30n+1 new rules, which is prohibitively large.
61	22	The superscript on the nonterminals denotes the child position from left to right.
62	76	For example P2121 means that this node is a second child, and the mother nonterminal label is P211.
66	96	Obtaining permutations Given a source sentence s and its alignment a to a target sentence t in the training corpus, we segment 〈s,a, t〉 into a sequence of minimal phrases sm (maximal sequence) such that the reordering between these minimal phrases constitutes a permutation πm.
67	43	We do not extract non-contiguous or non-minimal phrases because reordering them often involves complicated transductions which could hamper the performance of our learning algorithm.3 Unaligned words Next we describe the use of the factorization of permutations into PET forests for training a PCFG model.
69	31	An unaligned word is joined with a neighboring phrase to the left or the right, depending on the source language properties (e.g., whether the language is head-initial or -final (Chomsky, 1970)).
71	24	This modifies a PET by adding a new binary branching node µ (dominating the unaligned word and the phrase it is joined to) which is labeled with a dedicated nonterminal: P01 if the unaligned word joins to the right and P10 if it joins to the left.
72	153	We decompose the permutation πm into a forest of permutation trees PEF (πm) in O(n3), following algorithms in (Zhang et al., 2008; Zhang and Gildea, 2007) with trivial modifications.
73	72	Each PET ∆ ∈ PEF (πm) is a different bracketing (differing in binary branching structure only).
74	26	We consider the bracketing hidden in the latent treebank, and apply unsupervised learning to induce a distribution over possible bracketings.
76	36	This demands summing over all PETs ∆ in the forest PEF (πm), and for every PET also over all its label splits, which are given by the grammar derivations d: P (sm, πm) = ∑ ∆∈PEF (πm) ∑ d∈∆ P (d, sm) (1) The probability of a derivation d is a product of probabilities of all the rules r that build it: P (sm, πm) = ∑ ∆∈PEF (πm) ∑ d∈∆ ∏ r∈d P (r) (2) As usual, the parameters of this model are the PCFG rule probabilities which are estimated from the latent treebank using EM as explained next.
86	38	We do Monte Carlo sampling of 10000 derivations from the chart of the s and then find the least risky permutation in terms of this loss.
92	19	We use the fact that Kendall τ decomposes as a linear function over all skip-bigrams b that could be built for any permutation of length k: Kendall(π, πr) = ∑ b 1− δ(π, b) k(k−1) 2 δ(πr, b) (4) Here δ returns 1 if permutation π contains the skip bigram b, otherwise it returns 0.
93	20	With this decomposition we can use the method from (DeNero et al., 2009) to efficiently compute the MBR hypothesis.
98	105	• Baseline B: Rule based preordering (Isozaki et al., 2010b), which first obtains an HPSG parse tree using Enju parser 4 and after that swaps the children by moving the syntactic head to the final position to account for different head orientation in English and Japanese.
99	44	• Baseline C: LADER (Neubig et al., 2012): latent variable preordering that is based on ITG and large-margin training with latent variables.
100	54	We used LADER in standard settings without any linguistic features (POS tags or syntactic trees).
101	142	And we test four variants of our model: • RGleft - only canonical left branching PET • RGright - only canonical right branching PET • RGITG-forest - all PETs that are binary (ITG) • RGPET-forest - all PETs.
102	33	We test these models on English-Japanese NTCIR-8 Patent Translation (PATMT) Task.
115	31	The only strange result here is that rule-based preordering obtains a lower score than no preordering, which might be an artifact of the Enju parser changing the tokenization of its input, so the Kendall τ of this system might not really reflect the real quality of the preordering.
117	82	The reordered output of all the mentioned baselines and versions of our model are translated with phrase-based MT system (Koehn et al., 2007) (distortion limit set to 6 with distance based reordering model) that is trained on gold preordering of the training data 7 ś − t. The only exception is Baseline A which is trained on original s− t. We use a 5-gram language model trained with KenLM 8, tune 3 times with kb-mira (Cherry and Foster, 2012) to account for tuner instability and evaluated using Multeval 9 for statistical significance on 3 metrics: BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and TER (Snover et al., 2006).
118	26	We additionally report RIBES score (Isozaki et al., 2010a) that concentrates on word order more than other metrics.
