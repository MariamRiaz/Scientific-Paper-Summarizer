0	77	We address the task of recovering the original word order of a shuffled sentence, referred to as bag generation (Brown et al., 1990), shake-and-bake generation (Brew, 1992), or more recently, linearization, as standardized in a recent line of research as a method useful for isolating the performance of text-to-text generation models (Zhang and Clark, 2011; Liu et al., 2015; Liu and Zhang, 2015; Zhang and Clark, 2015).
1	43	The predominant argument of the more recent works is that jointly recovering explicit syntactic structure is crucial for determining the correct word order of the original sentence.
2	12	As such, these methods either generate or rely on given parse structure to reproduce the order.
4	22	Elman judged the capacity of early recurrent neural networks via, in part, the network’s ability to predict word order in simple sentences.
5	13	He notes, The order of words in sentences reflects a number of constraints.
6	16	Syntactic structure, selective restrictions, subcategorization, and discourse considerations are among the many factors which join together to fix the order in which words occur.
7	13	[T]here is an abstract structure which underlies the surface strings and it is this structure which provides a more insightful basis for understanding the constraints on word order.
8	9	It is, therefore, an interesting question to ask whether a network can learn any aspects of that underlying abstract structure (Elman, 1990).
9	17	Recently, recurrent neural networks have reemerged as a powerful tool for learning the latent structure of language.
11	24	We revisit Elman’s question by applying LSTMs to the word-ordering task, without any explicit syntactic modeling.
13	18	The task of linearization is to recover the original order of a shuffled sentence.
47	37	, xy(n−1)) where the phrase probabilities are calculated wordby-word by our language model.
49	25	, |Bm| do 5: (y,R, s,h)← B(k)m 6: for i ∈ R do 7: (s′,h′)← (s,h) 8: for word w in phrase xi do 9: s′ ← s′ + log q(w,h′) 10: h′ ← δ(w,h′) 11: j ← m+ |xi| 12: Bj ← Bj + (y + xi,R− i, s′,h′) 13: keep top-K of Bj by f(x, y) + g(R) 14: return BM Searching over all permutations Y is intractable, so we instead follow past work on linearization (Liu et al., 2015) and LSTM generation (Sutskever et al., 2014) in adapting beam search for our generation step.
50	73	Our work differs from the beam search approach for the WORDS+BNPS case of previous work in that we maintain multiple beams, as in stack decoding for phrase-based machine translation (Koehn, 2010), allowing us to incorporate the probabilities of internal, non-boundary words in the BNPs.
51	32	Additionally, for both WORDS and WORDS+BNPS, we also include an estimate of future cost in order to improve search accuracy.
52	24	Beam search maintains M + 1 beams, B0, .
54	11	A partial hypothesis is a 4-tuple (y,R, s,h), where y is a partial ordering,R is the set of remaining indices to be ordered, s is the score of the partial linearization f(x, y), and h is the current LM state.
71	12	We compare the performance of the models using the BLEU metric (Papineni et al., 2002).
77	25	Differences on the WORDS task are smaller, but show a similar pattern.
79	11	Notably, the NGRAM model outperforms the combined result of ZGEN-64+LM+GW+POS from Liu and Zhang (2015), which uses a 4-gram model trained on Gigaword.
80	115	We believe this is because the combined ZGEN model incorporates the n-gram scores as discretized indicator features instead of using the probability directly.7 A beam of 512 yields a further improvement at the cost of search time.
81	145	To further explore the impact of search accuracy, Table 2 shows the results of various models with beam widths ranging from 1 (greedy search) to 512, and also with and without future costs g. We see that for the better models there is a steady increase in accuracy even with large beams, indicating that search errors are made even with relatively large beams.
82	71	One proposed advantage of syntax in linearization models is that it can better capture long-distance relationships.
83	13	Figure 1 shows results by sentence length and distortion, which is defined as the absolute difference between a token’s index position in y∗ and ŷ, normalized by M .
84	51	The LSTM model exhibits consistently better performance than existing syntax models across sentence lengths and generates fewer long-range distortions than the ZGEN model.
87	44	We first align the gold head to each output token.
88	28	(In cases where the alignment is not one-to-one, we randomly sample among the possibilities.)
89	28	The models with no knowledge of syntax are able to recover a higher proportion of gold arcs.
90	34	Strong surface-level language models recover word order more accurately than the models trained with explicit syntactic annotations appearing in a recent series of papers.
91	80	This has implications for the utility of costly syntactic annotations in generation models, for both high- and low- resource languages and domains.
