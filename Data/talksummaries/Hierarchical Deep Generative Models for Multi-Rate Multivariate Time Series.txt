0	66	Multivariate time series (MTS) analysis (Hamilton, 1994; Reinsel, 2003) has attracted a lot of attention in machine learning, signal processing, and other related areas, due to its impact and usefulness in many real world applications such as healthcare, climate, and financial forecasting.
1	9	Statespace models such as Kalman filters (Kalman et al., 1960) and hidden Markov models (Rabiner, 1989) have been developed to model MTS and have shown promising results on prediction tasks such as forecasting and interpolation.
2	10	However, in many applications, the MTS observations usually come from multiple sources and are often characterized by various sampling rates.
3	34	For example, in healthcare, vital signs such as heart rate are sampled frequently, while lab results such as pH are measured infrequently; in finance, the stock prices are sampled daily or even more frequently, while macro-economic data such as employment, GDP are sampled monthly or quarterly.
5	40	Modeling the MR-MTS using state-space models is challenging since MR-MTS naturally comes with multiple temporal dependencies and these dependencies may not have direct relationship to the sampling rates.
8	30	Upsampling or downsampling MR-MTS to a single rate time series cannot address this challenge, since these simple techniques may artifically introduce or remove some naturally occurring dependencies present in MR-MTS.
9	14	For example, forward/backward imputation will introduce long-term dependencies.
10	22	Therefore, building models which can capture multiple temporal dependencies directly from the MR-MTS data is still an open problem in the time series analysis field.
38	21	In this section, we present our proposed Multi RateHierarchical Deep Markov Model (MR-HDMM).
41	43	, T , and Dl is the dimension of time series with lth rate.
43	7	To make the notations succinct, we use xl:l ′ t:t′ to denote all observed time series of lth to l′th rates and from time t to t′.
44	28	to denote the parameter sets for generation model pθ and inference network qφ respectively.
57	18	Due to the flexibility of auxiliary connections, our MR-HDMM can also handle irregularly sampled time series data or missing data.
60	13	The generation process of our MR-HDMM follows the transition and emission framework, which is obtained by applying deep recurrent neural networks to non-linear continuous state space models.
62	59	Transition We design the transition process of the latent state z to capture the hierarchical structure for multiple temporal dependencies with learnable binary switches s. For each non-bottom layer l > 1 and time step t ≥ 1, we use a binary switch state slt to control the updates of the corresponding latent states zlt, as shown in Figure 2. slt is obtained based on the values of the previous latent states zlt−1 and the lower layer latent states z l−1 t by a de- terministic mapping slt = I ( gθs(z l t−1, z l−1 t ) ≥ 0 ) .
65	17	When the switch is off (i.e., reuse operation, slt = 0), z l t will be drawn from the same distribution as its previous states zlt−1, which is N ( µlt−1,Σ l t−1 ) .
71	42	In order to embed the multiple temporal dependencies in the generated MR-MTS, we introduce auxiliary connections (denoted by the dashed lines in Figure 1(a)) from the higher latent layers to the lower rate time series.
94	31	First, we maintain the Markov properties of z in the inference network, which leads to the factorization: qφ ( z1:L1:T , s 2:L 1:T |x1:L1:T ,z1:L0 ) = T∏ t=1 qφ ( z1:Lt , s 2:L t |z1:Lt−1,x1:L1:T ) (3) We then leverage the hierarchical structure and inherit the switches from the generation model into the Table 1.
97	16	That is, the same gθs from the generation model is used in the inference network, i.e., qφ ( slt|zlt−1, zl−1t ,x1:L1:T ) = qφs ( slt|zlt−1, zl−1t ) = pθs ( slt|zlt−1, zl−1t ) .
98	13	Then, for each term in the righthand side of Equation (3) and for all t = 1, · · · , T , we have: qφ ( z1:Lt , s 2:L t |z1:Lt−1,x1:L1:T ) =qφ ( z1t |z1t−1,x1:L1:T ) · L∏ l=2 qφ ( slt|zlt−1,zl−1t ,x1:L1:T ) qφ ( zlt|zlt−1,zl−1t , slt,x1:L1:T ) =qφ ( z1t |z1t−1,x1:L1:T ) · L∏ l=2 pθs ( slt|zlt−1,zl−1t ) qφ ( zlt|zlt−1,zl−1t , slt,x1:L1:T ) (4) Thus, the inference network can be factorized by Equation (3) and (4).
100	16	Given these, we further factorize the ELBO in Equation (2) as a summation of expectations of conditional log likelihood and KL divergence terms over time steps and hierarchical layers: F(θ, φ) = T∑ t=1 L∑ l=1 EQ∗(z1:lt ) log pθx ( xlt|z1:lt ) + T∑ t=1 EQ∗(z1t−1)DKL ( qφ ( z1t |x1:L1:T ,z1t−1 )∥∥∥pθ (z1t |z1t−1)) + T∑ t=1 L∑ l=2 EQ∗(z1t−1,zl−1t ) DKL ( qφ ( zlt|x1:L1:T ,zlt−1,zl−1t )∥∥∥pθ (z1t |z1t−1,zl−1t )) (5) where Q∗ (·) denotes the marginal distribution of (·) from qφ .
133	9	To ensure fair comparison, we only evaluate and compare all the models on the original time-series (i.e. non-imputed data).
185	8	In Figure 3(a) and 3(b), we visualize the latent hierarchical structure of MR-HDMM learned from the first 48 hours of an admission in MIMICIII dataset and one-year climate observations in USHCN dataset.
186	11	A color block indicates that the latent state zlt is updated from zlt−1 and z l−1 t (update), while the white block indicates zlt is generated from the same distribution of z l t−1 (reuse).
187	15	As expected, the higher latent layers tend to update less frequently and capture the long-term temporal dependencies.
188	17	To understand learned hierarchical structure more intuitively, we also show precipitation time series from USCHN dataset along with learned switches in Figure 3(b).
189	22	We observe that the higher latent layer tends to update along with the precipitation, which is reasonable since precipitation makes significant changes to the underlying weather condition which is captured by the higher latent layer.
190	19	We proposed the Multi-Rate Hierarchical Deep Markov Model (MR-HDMM) - a novel deep generative model for forecasting and interpolation tasks on multi-rate multivariate time series (MR-MTS) data.
191	19	MR-HDMM models the data generation process by learning a latent hierarchical structure using auxiliary connections and learnable switches to capture the temporal dependencies.
192	19	Empirically we showed that our proposed model outperforms the existing single-rate and multi-rate models on healthcare and climate datasets.
