5	17	A well-known problem with spectral methods is that—due to their close relationship with random walks—they sometimes spread mass “too aggressively,” and thereby they don’t find the “right” partition.
10	13	Here, we introduce a novel Capacity Releasing Diffusion (CRD) Process to address this problem.
12	67	Our CRD Process has better properties with respect to limiting the spread of mass inside local well-connected clusters.
13	17	It does so with improved running time properties.
17	15	Later (in Section 2) we also present a concrete CRD algorithm for the specific task of local clustering that exploits the dynamics of the generic CRD process1.
18	38	The entire CRD process (Figure 1) repeatedly applies the generic CRD inner process (which we call a CRD step), and then it doubles the amount of mass at all vertices between invocations.
19	46	A CRD step starts with each vertex u having mass m(u) ≤ 2d(u), where d(u) is the degree of u, and spreads the mass so that at the end each vertex u has mass m(u) ≤ d(u).
20	14	Observe that, essentially, each CRD step spreads the mass to a region of roughly twice the volume comparing to the previous step.
21	30	The generic CRD inner process (Figure 2) implements a modification of the classic “push-relabel” algorithm (Goldberg & Tarjan, 1988; 2014) for routing a source-sink flow.
22	22	The crucial property of our process (different from the standard push-relabel) is that edge capacity is made available to the process slowly by releasing.
23	13	That is, we only allow l(u) units of mass to move across any edge (u, v), where l(u) is the label (or height) maintained by the CRD inner process.
25	60	As we will see, this difference is critical to the theoretical and empirical performance of the CRD algorithm.
26	18	To give insight into the differences between classical spectral diffusion and our CRD Process, consider the graph in Figure 3.
28	58	There is one edge from u to the rest of the graph, and we assume the other endpoint v has very high degree such that the vast majority of the mass arriving there is absorbed by its neighbors in B.
30	11	Consider first classical spectral diffusion, with a random walk starting from some vertex in B.
33	13	Thus, when ` is Ω(k), the random walk is expected to leave B and never return, i.e., the classical diffusion will leak out all the probability mass before even spreading beyond a constant fraction of B.
35	15	Assume that at some point the mass is spread along z neighboring vertices on each of the k paths.
36	17	To continue the spread to 2z vertices in the next CRD step, the labels will be raised to (at most) 2z to allow the mass to spread over the path of length 2z.
38	39	Since in this call, a total of 2zk mass is in the set B, at most 1/k of the mass escapes.
39	12	After log ` CRD steps, the mass is spread over all the k length-` paths, and only a (2 log `)/k fraction of the mass has escaped from B.
43	14	This drawback of spectral techniques can perhaps be resolved using sophisticated methods such as evolving sets (Andersen & Peres, 2009), though it comes easily with CRD.
53	70	The rather weak Assumption 1 states that B’s internal connectivity φS(B) (see Section 3 for definition) is a constant factor better (i.e., larger) than the conductance φ(B).
55	65	Under these conditions, we can recover B starting from any vertex in B.
56	63	Both assumptions formalize the idea that the signal of the local structure is stronger than the noise of the cluster by some moderately large factor.
59	18	Their condition is considerably stricter than our condition on the ratio between φS(B) and φ(B), especially when φ(B) is small, as is common.
60	43	Their algorithm relies on proving that a classical diffusion starting at a typical node keeps most of its mass inside of B.
61	20	However, they do not need something like our smoothness condition.
62	15	With the additional smoothness condition, we break the dependence on √ φ(B) that is central to all approaches using spectral diffusions, including Zhu et al. (2013), for the first time with a local algorithm.
72	10	Spectral algorithms for computing eigenvalues use some variant of repeated matrix multiplication, which for graphs is a type of classical diffusion.
75	63	See, e.g., Orecchia et al. (2012) for more details on this.
