1	100	When applied to graph data, each vertex in G is described by an n-dimensional binary vector, namely its corresponding column (or row) in adjacency matrix A ∈ {0, 1}n×n, and we wish to associate with each vertex v ∈ V a lower-dimensional representation, say xv ∈ S. The two most commonly-used approaches for graph embeddings are the graph Laplacian embedding and its variants (Belkin & Niyogi, 2003; Coifman & Lafon, 2006) and the adjacency spectral embedding (ASE, Sussman et al., 2012).
8	22	In the standard out-of-sample (OOS) extension, we are presented with training dataD = {z1, z2, .
19	30	We note that some authors (see, for example, Chung, 1997) use I −D−1/2AD−1/2 to be the normalized graph Laplacian, but since this matrix has the same eigenspace as our L, results concerning the eigenvectors of either of these matrices are equivalent.
26	32	In general, OOS extensions for eigenvector-based embeddings can be derived as in Bengio et al. (2003) as the solution of a least-squares problem min f(x)∈Rd n∑ i=1 ( K(x, xi)− 1 n d∑ t=1 λtft(xi)ft(x) )2 , where {xi}ni=1 are the in-sample observations, and ft(xi) = [vt]i is ith component of vt. Belkin et al. (2006) presented a slightly different approach that incorporates regularization in both the intrinsic geometry of the data distribution and the geometry of the similarity function K. Their approach applies to Laplacian eigenmaps as well as to regularized least squares and SVM.
31	40	The only existing work to date on the ASE OOS extension of which we are aware appears in Tang et al. (2013a).
32	59	The authors considered the OOS extension for ASE applied to latent position graphs (see, for example Hoff et al., 2002), in which each vertex is associated with an element of a vector space and edge probabilities are given by a suitably-chosen inner product.
41	22	For a matrix B ∈ Rn1×n2 , we let σi(B) denote the i-th singular value of B, so that σ1(B) ≥ σ2(B) ≥ · · · ≥ σk(B) ≥ 0, where k = min{n1, n2}.
57	27	Given a graph G encoded by adjacency matrix A ∈ {0, 1}n×n, the adjacency spectral embedding (ASE) pro- duces a d-dimensional embedding of the vertices ofG, given by the rows of the n-by-d matrix X̂ = UAS 1/2 A , (2) where UA ∈ Rn×d is a matrix with orthonormal columns given by the d eigenvectors corresponding to the top d eigenvalues of A, which we collect in the diagonal matrix SA ∈ Rd×d.
58	39	We note that in general, one would be better-suited to consider the matrix [ATA]1/2, so that all eigenvalues are guaranteed to be nonnegative, but we will see that in the random dot product graph, the model that is the focus of this paper, the top d eigenvalues of A are positive with high probability (see, for example, either Lemma 1 in Athreya et al. (2016) or Observation 2 in Levin et al. (2017), or refer to the technical report, (Levin et al., 2018)).
59	31	The random dot product graph (RDPG, Young & Scheinerman, 2007) is an edge-independent random graph model in which the graph structure arises from the geometry of a set of latent positions, i.e., vectors associated to the vertices of the graph.
60	28	As such, the adjacency spectral embedding is particularly well-suited to this model.
61	22	(Random Dot Product Graph) Let F be a distribution on Rd such that xT y ∈ [0, 1] whenever x, y ∈ suppF , and let X1, X2, .
62	32	Collect these n random points in the rows of a matrix X ∈ Rn×d.
65	28	If G is the random graph corresponding to adjacency matrix A, we say that G is a random dot product graph with latent positions X1, X2, .
67	35	A number of results exist showing that the adjacency spectral embedding yields consistent estimates of the latent positions in a random dot product graph (Sussman et al., 2012; Tang et al., 2013b) and recovers community structure in the stochastic block model (Lyzinski et al., 2014).
69	31	Owing to this nonidentifiability, we can only hope to recover the latent positions in X up to some orthogonal rotation.
73	23	Suppose that, given adjacency matrix A, we compute embedding X̂ = [X̂1X̂2 .
74	20	X̂n] T , where X̂i ∈ Rd denotes the embedding of the i-th vertex.
75	37	Now suppose we add a vertex v with latent position w̄ ∈ Rd to the original graph G, obtaining an augmented graph G̃ = ([n] ∪ {v}, E ∪ Ev), where Ev denotes the set of edges between v and the vertices of G. One would like to embed vertex v according to the same distribution as the original n vertices and obtain an estimate of w̄.
76	25	Let the binary vector ~a ∈ {0, 1}n encode the edges Ev incident upon vertex v, with entries ai = (~a)i ∼ Bernoulli(XTi w̄).
77	31	The augmented graph G̃ then has the adjacency matrix as in (1).
107	37	The proof of this result relies upon a classic result for solutions of perturbed linear systems to establish that with high probability, ‖WŵLS − wLS‖ ≤ cn−1/2 log n, where W ∈ Rd×d is the orthogonal matrix guaranteed by Lemma 1 andwLS is the LS estimate based on the true latent positions {Xi} rather than on the estimates {X̂i}.
110	31	A detailed version of this proof can be found in the technical report (Levin et al., 2018).
111	52	As mentioned in Section 3, we would like to consider a maximum-likelihood OOS extension based on the likelihood ˆ̀(w) = ∑n i=1 ai log X̂ T i w+ (1−ai) log(1− X̂Ti w).
112	36	Toward this end, we would ideally like to use the solution to the optimization problem arg max w∈Rd ˆ̀(w), but to ensure a sensible solution, we instead consider ŵML = arg max w∈T̂ ˆ̀(w), (7) where we remind the reader that T̂ = {w ∈ Rd : ≤ X̂Ti w ≤ 1 − , i = 1, 2, .
113	188	Theorem 2 shows that ŵML recovers the true latent position of the OOS vertex, up to rotation, with error decaying at the same rate as that obtained in Theorem 1 for the LS OOS extension.
114	64	With notation as above, let ŵML be the estimate defined in Equation (7), and let > 0 be such that x, y ∈ suppF implies < xT y < 1 − .
119	35	A detailed proof can be found in (Levin et al., 2018).
120	137	Given our in-sample embedding X̂ and the vector of edge indicators ~a, we can think of the OOS extension as an estimate of w̄, the latent position of the OOS vertex v. Lemma 1 implies that if we took the naı̈ve approach of applying ASE to the adjacency matrix Ã in (1), our estimate would have error of order at most O(n−1/2 log n).
123	31	Let (A,X) ∼ RDPG(F, n) be a ddimensional RDPG.
