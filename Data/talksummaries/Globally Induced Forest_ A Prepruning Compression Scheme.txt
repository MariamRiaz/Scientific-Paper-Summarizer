2	9	However, datasets have become bigger and bigger over the past decade.
4	20	The former has led to bigger trees, as the number of nodes in a tree is O(N).
5	29	The latter, on the other hand, tends to steer toward larger forests.
6	8	Indeed, the variance of individual trees tends to increase with the dimensionality P of the problem (Joly et al., 2012).
12	16	All in all, tree-based models might benefit from lighter memory footprint in many different ways.
13	31	In this paper, we propose the Globally Induced Forest (GIF), an algorithm which, under a node budget constraint, iteratively and greedily deepens multiple trees by optimizing globally the sequence of nodes to develop and their associated weights, while still choosing locally, based on the standard local score criterion, the splitting variables and cut points at all tree nodes.
17	12	In Section 4, we show that our proposed algorithm, with its default setting, performs well on many datasets, sometimes even surpassing much larger models.
69	26	Given that zj(xi) 6= 0 only for the instances reaching node j, Equation (3) can be simplified into: j∗t = argmax j∈Ct ∑ i∈Zj (err(t−1)i − err (t) j,i ) (4) where Zj = {1 ≤ i ≤ N |zj(xi) = 1}.
70	15	Due to the partitioning induced by the tree, at each iteration, computing the optimal weights for all the nodes of a given tree is at most O(N), assuming a single weight optimization runs in linear time in the number of instances reaching that node.
72	68	Note that, since the optimization is global, the candidate node weights must be recomputed at each iteration as the addition of the chosen node impacts the optimal weights of all the candidates it is sharing learning instances with.
81	12	The optimal weight is the average residual: w (t) j = 1 |Zj | ∑ i∈Zj r (t−1) i (6) In the case of a unit learning rate (λ = 1) and a single tree (T = 1), the model predictions coincide with the ones the underlying tree would provide (see Supplementary material).
82	8	Extending to the multi-output case is straightforward: one only needs to fit a weight independently for each output.
109	20	Since the selection is uniform over the candidates, it also implies that well-developed trees are more likely to get developed further, as choosing a node means replacing it in the candidate list by its two children (unless it is a leaf).
114	10	If it is low, the node will not be fully exploited and the algorithm will look for similar nodes at subsequent steps.
120	16	Our first experiment was to test the GIF against the Extremely randomized trees (ET).
122	10	We then examined how GIF compared to ET for 1% and 10% of the original budget.
124	38	For ET, we built forests of 10 (ET1%) and 100 (ET10%) trees.
131	27	We will refer to this parameter setting as the default one.
155	20	For instance, on CT slice at 1%, we can reach 20.54 ± 0.76 by enlarging the candidate window size to 10.
158	19	Average mean square error at 1% and 10% budgets (m = √ p, λ = 10−1.5, T = 1000, CW = 1).
165	8	The three in the middle are their binary versions.
170	10	A unit learning rate will usually decrease the test set error rapidly but will then either saturate or overfit.
182	16	More interestingly, the smaller window size (CW = 1) performs best on all four datasets.
184	37	Although this is representative of the regression and binary classification problems, this is not exactly the case of multiclassification, where increasing CW over 1 might improve performance slightly (see Table 4).
193	20	Only one value is reported for the case CW = 1 as the forest has always the same shape, whatever the learning rate λ.
210	10	Interestingly, Boosting also overfits on Abalone and Hwang.
226	20	In other words, letting the algorithm optimize the forest shape is—surprisingly—harmful.
228	30	The main focus of subsequent works should be to handle multiclass problems better.
229	21	Several extensions can also be thought of.
231	21	Finally, we would also like to explore further the comparison between GIF and boosting methods, in particular Johnson & Zhang (2014)’s regularized greedy forests, which share similar traits with GIF.
