1	55	Such a task is typically characterized by the named entity recognition task (Grishman, 1997; Tjong Kim Sang and De Meulder, 2003), or the more general mention recognition task, where mentions are defined as references to entities that could be named, nominal or pronominal (Florian et al., 2004).
4	46	However, as highlighted in several prior research efforts (Alex et al., 2007; Finkel and Manning, 2009; Lu and Roth, 2015), mentions may overlap with one another in practice.
13	60	However, the model is unable to handle overlapping mentions of the same type.
25	19	Further experiments show that our model yields competitive results when evaluated on data that does not have overlapping mentions annotated when comparing against other recently proposed state-ofthe-art neural models that are capable of extracting non-overlapping mentions only.
53	111	Specifically, our segmental hypergraph consists of the following 5 types of nodes: • Ai encodes all such mentions that start with the i-th or a later word • Ei encodes all mentions that start exactly with the i-th word • Tki represents all mentions of type k starting with the i-th word • Iki,j represents all mentions of type k that con- tain the j-th word and start with the i-th word • X marks the end of a mention.
60	15	Three hyperedges {Iki,j → Iki,j+1}, {Iki,j → X}, and {Iki,j → (Iki,j+1,X)} from Iki,j indicate the following three cases respectively: 1) both the j-th and (j + 1)-th words belong to at least one mention that starts with the i-th word, 2) there exists one mention that starts with the i-th word and ends with the j-th word, and 3) both cases are valid.
63	41	Here, “Israeli UN Ambassador” of type PERSON is captured by the following sequence of nodes (along a hyperpath): “A1, E1, T21, I 2 1,1, I 2 1,2, I 2 1,3, X”, while “Israeli UN Ambassador Danny” of type PERSON corresponds to the following node sequence: “A1, E1, T21, I 2 1,1, I 2 1,2, I 2 1,3, I 2 1,4, X”.
77	33	In both of the previous approaches, their models would make local predictions and assign both “A” and “B” as left boundaries, and both “C” and “D” as right boundaries.
78	17	However, based on such local predictions one could also interpret “A B C” as a mention – this is where the ambiguity arises.
79	22	In contrast, our model enjoys the structural ambiguity free property as it uses our newly defined I nodes (together with X nodes) to jointly capture the complete boundary information of mentions.
86	15	The hyperedge between I nodes can capture the interactions between partial mentions and hyperedge between Iki,j and X precisely represents the mention [i, j] with type k. We note that such features and interactions cannot be captured by the models of (Lu and Roth, 2015) and (Muis and Lu, 2017).
96	14	Since every valid mention hyperpath contains the first and second kind of hyperedges, defining scores over such hyperedges are unnecessary as their scores would serve as a constant factor that can be eliminated in the overall loss function of the log-linear model.
99	74	For the hyperedges that involve more than two nodes, the score is computed as follows: φ({Iki,j → (X, Iki,j+1)},x) = W ′(k) II · [h s i:j ,h s i:j+1] + W ′(k) IX · h s i:j (12) where W′II ∈ R2d2×m, W′IX ∈ Rd2×m.
106	12	Inference can be done efficiently using a generalized inside-outside style message-passing algorithm (Baker, 1979).
107	20	The partition function of (1) can be computed using the inside algorithm applied to the complete hypergraph G, where we traverse from leaf nodes X to the root node A1, passing messages to a parent node p from its child nodes: µ[p]← log (∑ e:h(e)≡p exp ( ψ(e,x) + ∑ c∈T (e) µ[c] )) (13) where h(e) is the head of the hyperedge e, and T (e) is the collection of nodes that form the tail of e – they are the child nodes of h(e) given e. The message passing step for the outside algorithm can be defined analogously.
111	20	Since one node is incident to 3 hyperedges maximally, the time complexity of inference algorithm can be implied by the number of nodes in the graph, which is O(cmn), where c is the maximal length for any mention.
119	41	Most mentions (over 93%) are not longer than 6 tokens which we select as maximal length (c) for the restricted models.
125	11	• Semi-CRF: the semi-Markov CRF model (Sarawagi and Cohen, 2005).
143	14	Using the same set of handcrafted features, our unrestricted non-neural model SH (-NN, c=n) achieves the best performance compared with other nonneural models, revealing the effectiveness of our newly proposed segmental hypergraph representation.
170	46	The dropout and pre-trained embeddings can improve the performance of our model significantly and this behavior is consistent with previous neural models for NER (Chiu and Nichols, 2016; Lample et al., 2016).
173	15	To further understand how well our model can handle overlapping mentions, we split the test data into two portions: sentences with and without overlapping mentions.
174	10	We compare our model with the two state-of-the-art models and report results on ACE-05 in Table 5.10 In both portions, SH achieves significant improvements, especially in the portion with overlapping mentions.
175	13	This observation indicates that our model can better capture the structure of overlapping mentions than these two previous models.
178	19	Running time Since other compared models also feature linear time complexity (see Table 1), we examine the decoding speed in terms of the number of words processed per second.
180	12	The model of (Wang et al., 2018) is also tested with the same environment.
186	11	We compared our model with recent state-of-the-art neural network based models.
200	10	LSTM that serves as feature representation may capture such interactions implicitly, but building the connections could still be an important aspect for improvement.
203	38	Through extensive experiments, we show that our model is general and robust in handling both overlapping and non-overlapping mentions.
204	37	The model achieves the state-of-the-art results in three standard datasets for recognizing overlapping mentions.
205	14	We anticipate this model could be leveraged in other similar sequence modeling tasks that involve predicting overlapping structures such as recognizing overlapping and discontinuous entities (Muis and Lu, 2016) which frequently exist in the biomedical domain.
