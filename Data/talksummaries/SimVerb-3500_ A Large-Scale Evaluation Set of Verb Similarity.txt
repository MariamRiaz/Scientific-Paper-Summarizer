32	44	Hill et al. (2015) argue that comprehensive highquality evaluation resources have to satisfy the following three criteria: (C1) Representative (the resource covers the full range of concepts occurring in natural language); (C2) Clearly defined (it clearly defines the annotated relation, e.g., similarity); (C3) Consistent and reliable (untrained native speakers must be able to quantify the target relation consistently relying on simple instructions).
38	19	SimVerb-3500 does not inherit this anomaly (see Tab.
41	50	The rating scale goes from 0 (not similar at all) to 10 (synonymous).
42	15	We employed the SimLex-999 annotation guidelines.
46	31	To ensure a wide coverage of a variety of syntacticosemantic phenomena (C1), the choice of verb pairs is steered by two standard semantic resources available online: (1) the USF norms data set3 (Nelson et al., 2004), and (2) the VerbNet verb lexicon4 (Kipper et al., 2004; Kipper et al., 2008).
47	61	The USF norms data set (further USF) is the largest database of free association collected for English.
48	16	It was generated by presenting human subjects with one of 5, 000 cue concepts and asking them to write the first word coming to mind that is associated with that concept.
50	30	For each such pair, the proportion of participants who produced associate a when presented with cue c can be used as a proxy for the strength of association between the two words.
51	72	The norming process guarantees that two words in a pair have a degree of semantic association which correlates well with semantic relatedness and similarity.
57	18	According to the official VerbNet guidelines,5 “Verb Classes are numbered according to shared semantics and syntax, and classes which share a toplevel number (9-109) have corresponding semantic relationships.” For instance, all verbs from the toplevel Class 9 are labelled “Verbs of Putting”, all verbs from Class 30 are labelled “Verbs of Perception”, while Class 39 contains “Verbs of Ingesting”.
58	21	Among others, three basic types of information are covered in VN: (1) verb subcategorization frames (SCFs), which describe the syntactic realization of the predicate-argument structure (e.g. The window broke), (2) selectional preferences (SPs), which capture the semantic preferences verbs have for their arguments (e.g. a breakable physical object broke) and (3) lexical-semantic verb classes (VCs) which provide a shared level of abstraction for verbs similar in their (morpho-)syntactic and semantic properties (e.g. BREAK verbs, sharing the VN class 45.1, and the top-level VN class 45).6 The basic overview of the VerbNet structure already suggests that measuring verb similarity is far from trivial as it revolves around a complex interplay between various semantic and syntactic properties.
63	25	(Step 2) We then manually cleaned and simplified the list of pairs by removing all pairs with multi-word verbs (e.g., quit / give up), all pairs that contained the non-infinitive form of a verb (e.g., accomplished / finished, hidden / find), removing all pairs containing at least one auxiliary verb (e.g., must / to see, must / to be).
100	17	If just one of the checkpoint questions is answered incorrectly, the survey ends immediately and all scores from the annotator in question are discarded.
107	58	We excluded ratings of annotators who (a) answered one of the checkpoint questions incorrectly (75% of exclusions); (b) did not give equal ratings to duplicate pairs; (c) showed suspicious rating patterns (e.g., randomly alternating between two ratings or using one single rating throughout).
109	43	We then calculated the average of all ratings from the accepted raters ( ≥ 10 ) for each pair.
115	42	SimVerb-3500 obtains ρ = 0.84 (IAA-1) and ρ = 0.86 (IAA-2), a very good agreement compared to other benchmarks (see Tab.
116	20	Vector Space Models We compare the performance of prominent representation models on SimVerb-3500.
117	68	We include: (1) unsupervised models that learn from distributional information in text, including the skip-gram negative-sampling model (SGNS) with various contexts (BOW = bag of words; DEPS = dependency contexts) as in Levy and Goldberg (2014), the symmetric-pattern based vectors by Schwartz et al. (2015), and count-based PMIweighted vectors (Baroni et al., 2014); (2) Models that rely on linguistic hand-crafted resources or curated knowledge bases.
119	28	Descriptions of these models are in the supplementary material.
124	37	Since the number of evaluation pairs may influence the results, we ideally want to compare sets of equal size for a fair comparison.
142	37	Frequency In the first analysis, we select pairs based on their lemma frequency in the BNC corpus and form three groups, with 390-490 pairs in each group (Fig.
143	32	1 suggest that the performance of all models improves as the frequency of the verbs in the pair increases, with much steeper curves for the purely distributional models (e.g., SGNS and SymPat).
151	43	We find that verbs in the same top-level VerbNet class are often not assigned high similarity score.
154	189	When a verb be- longs to multiple classes, we count it for each class (see Footnote 2).
155	68	We run the analysis on the five largest VN classes, each with more than 100 pairs with paired verbs belonging to the same class.
157	41	Lexical Relations SimVerb-3500 contains relation annotations (e.g., antonyms, synonyms, hyper/hyponyms, no relation) for all pairs extracted automatically from WordNet.
158	148	Evaluating per-relation subsets, we observe that some models draw their strength from good performance across different re- lations.
159	65	Others have low performance on these pairs, but do very well on synonyms and hyper-/hyponyms.
161	21	5.12 Human Agreement Motivated by the varying performance of computational models regarding frequency and ambiguous words with many synsets, we analyse what disagreement effects may be captured in human ratings.
162	36	We therefore compute the average standard deviation of ratings per subset: avgstdd(S) = 1n ∑ p∈S σ(rp), where S is one subset of pairs, n is the number of pairs in this subset, p is one pair, and rp are all human ratings for this pair.
