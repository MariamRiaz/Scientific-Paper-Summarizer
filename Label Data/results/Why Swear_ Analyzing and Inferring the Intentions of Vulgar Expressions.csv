0,1,label2,summary_sentences
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1214–1223, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics",text,[0],[0]
"People engage in argumentation in various contexts, both online and in the real life.",1 Introduction,[0],[0]
"Existing definitions of argumentation do not solely focus on giving reasons and laying out a logical framework of premises and conclusions, but also highlight its social purpose which is to convince or to persuade (O’Keefe,
2011; van Eemeren et al., 2014; Blair, 2011).",1 Introduction,[0],[0]
Assessing the quality and strength of perceived arguments therefore plays an inherent role in argumentative discourse.,1 Introduction,[0],[0]
"Despite strong theoretical foundations and plethora of normative theories, such as Walton’s schemes and their critical questions (Walton, 1989), an ideal model of critical discussion in the pragma-dialectic view (Van Eemeren and Grootendorst, 1987), or research into fallacies (Boudry et al., 2015), assessing qualitative criteria of everyday argumentation represents a challenge for argumentation scholars and practitioners (Weltzer-Ward et al., 2009; Swanson et al., 2015; Rosenfeld and Kraus, 2015).
",1 Introduction,[0.9593908691080356],"['Vulgarity is a common element of conversation (Jay, 2009; Mehl et al., 2007) and is used even more frequently in social networks such as Twitter (Wang et al., 2014).']"
Addressing qualitative aspects of arguments has recently started gaining attention in the field of computational argumentation.,1 Introduction,[0],[0]
"Scoring strength of persuasive essays (Farra et al., 2015; Persing and Ng, 2015), exploring interaction in persuasive dialogues on Reddit (Tan et al., 2016), or detecting convincing arguments (Habernal and Gurevych, 2016) are among recent attempts to tackle the quality of argumentation.",1 Introduction,[0],[0]
"However, these approaches are holistic and do not necessarily explain why a given argument is strong or convincing.
",1 Introduction,[0],[0]
We asked the following research questions.,1 Introduction,[0],[0]
"First, can we assess what makes an argument convincing in a purely empirical fashion as opposite to theoretical normative approaches?",1 Introduction,[0],[0]
"Second, to what extent can the problem be tackled by computational models?",1 Introduction,[0],[0]
"To address these questions, we exploit our recently introduced UKPConvArg1 corpus (Habernal and Gurevych, 2016).",1 Introduction,[0],[0]
"This data set consists of 11,650 argument pairs – two arguments with the
1214
Prompt: Should physical education be mandatory in schools?",1 Introduction,[0],[0]
Stance:,1 Introduction,[0],[0]
"Yes!
Argument 1 Argument 2 PE should be compulsory because it keeps us constantly fit and healthy.",1 Introduction,[0],[0]
"If you really dislike sports, then you can quit it when you’re an adult.",1 Introduction,[0],[0]
"But when you’re a kid, the best thing for you to do is study, play and exercise.",1 Introduction,[0],[0]
If you prefer to be lazy and lie on the couch all day then you are most likely to get sick and unfit.,1 Introduction,[0],[0]
"Besides, PE helps kids be better at teamwork.",1 Introduction,[0],[0]
"physical education should be mandatory cuhz 112,000 people have died in the year 2011 so far and it’s because of the lack of physical activity and people are becoming obese!!!!
",1 Introduction,[0],[0]
"A1 is more convincing than A2, because: • “A1 is more intelligently written and makes
same standpoint to the given topic, annotated with a binary relation describing which argument from the pair is more convincing.",1 Introduction,[0],[0]
Each pair also contains several reasons written in natural language explaining which properties of the arguments influence their convincingness.,1 Introduction,[0],[0]
"An example of such an argument pair is shown in Figure 1.
",1 Introduction,[0],[0]
We use these natural language reasons as a proxy to assess qualitative properties of the arguments in each argument pair.,1 Introduction,[0],[0]
Our main contributions are: (1) We propose empirically inspired labels of quality properties of Web arguments and design a hierarchical annotation scheme.,1 Introduction,[0],[0]
"(2) We create a new large crowd-sourced benchmark data set containing 9,111 argument pairs multi-labeled with 17 categories which is improved by local and global filtering techniques.",1 Introduction,[0],[0]
"(3) We experiment with several computational models, both traditional and neu-
ral network-based, and evaluate their performance quantitatively and qualitatively.
",1 Introduction,[0],[0]
The newly created data set UKPConvArg2 is available under CC-BY-SA license along with the experimental software for full reproducibility at GitHub.1,1 Introduction,[0],[0]
"The growing field of computational argumentation has been traditionally devoted to structural tasks, such as argument component detection and classification (Habernal and Gurevych, 2017; Habernal and Gurevych, 2015), argument structure parsing (Peldszus and Stede, 2015; Stab and Gurevych, 2014), or argument schema classification (Lawrence and Reed, 2015), leaving the issues of argument evaluation or quality assessment as an open future work.
",2 Related Work,[0],[0]
"There are only few attempts to tackle the qualitative aspects of arguments, especially in the Web discourse.",2 Related Work,[0],[0]
Park and Cardie (2014) classified propositions in Web arguments into four classes with respect to their level of verifiability.,2 Related Work,[0],[0]
"Focusing on convincingness of Web arguments, Habernal and Gurevych (2016) annotated 16k pairs of arguments with a binary relation “is more convincing” and also elicited explanation for the annotators’ decisions.
",2 Related Work,[0],[0]
"Recently, research in persuasive essay scoring has started combining holistic approaches based on rubrics for several dimensions typical to this genre with explicit argument detection.",2 Related Work,[0],[0]
"Persing and Ng (2015) manually labeled 1,000 student persuasive essays with a single score on the 1–4 scale and trained a regression predictor with a rich feature set using LIBSVM.",2 Related Work,[0],[0]
"Among traditional features (such as POS or semantic frames), an argument structure parser by Stab and Gurevych (2014) was employed.",2 Related Work,[0],[0]
"Farra et al. (2015) also deal with essay scoring but rather then tackling the argument structure, they focus on methods for detecting opinion expressions.",2 Related Work,[0],[0]
"Persuasive essays however represent a genre with a rather strict qualitative and formal requirements (as taught in curricula) and substantially differ from online argumentation.
",2 Related Work,[0],[0]
"Argument evaluation belongs to the central research topics among argumentation scholars (Toul-
1https://github.com/UKPLab/ emnlp2016-empirical-convincingness
min, 2003; Walton et al., 2008; Van Eemeren and Grootendorst, 1987).",2 Related Work,[0],[0]
"Yet treatment of assessing argumentation quality, persuasiveness, or convincingness is traditionally based on evaluating relevance, sufficiency or acceptability of premises (Govier, 2010; Johnson and Blair, 2006) or categorizing fallacies (Hamblin, 1970; Tindale, 2007).",2 Related Work,[0],[0]
"However, the nature of these normative approaches causes a gap between the ‘ideal’ models and empirically encountered real-world arguments, such as those on the Web (van Eemeren et al., 2014; Walton, 2012).
",2 Related Work,[0],[0]
"Regarding the methodology utilized later in this paper, deep (recursive) neural networks have gained extreme popularity in NLP in recent years.",2 Related Work,[0],[0]
"Long Short-Term Memory networks (LSTM) with Attention mechanism have been applied on textual entailment (Rocktäschel et al., 2016), QuestionAnswering (Golub and He, 2016), or source-code summarization (Allamanis et al., 2016).",2 Related Work,[0],[0]
"As our source data set, we took the publicly available UKPConvArg1 corpus.2 It is based on arguments originated from 16 debates from Web debate platforms createdebate.com and convinceme.net, each debate has two sides (usually pro and con).",3 Data,[0],[0]
"Arguments from each of the 32 debate sides are connected into a set of argument pairs, and each argument pair is annotated with a binary relation (argument A is more/less convincing than argument B), resulting in total into 11,650 argument pairs.",3 Data,[0],[0]
"Annotations performed by Habernal and Gurevych (2016) also contain several reasons written by crowd-workers that explain why a particular argument is more or less convincing; see an example in Figure 1.
",3 Data,[0],[0]
"As these reasons were written in an uncontrolled setting, they naturally reflect the main properties of argument quality in a downstream task, which is to decide which argument from a pair is more convincing.",3 Data,[0.9509483232553501],['Understanding the motivation behind the choice to be vulgar and the way in which vulgarity is manifested in naturally occurring environments is of interdisciplinary interest.']
"It differs from scoring arguments in isolation, which is inherently harder not only due to subjectivity in argument “strength” decision but also because of possible annotator’s prior bias (Habernal and Gurevych, 2016).",3 Data,[0],[0]
"Assessing an argument
2https://github.com/UKPLab/ acl2016-convincing-arguments
in context helps to emphasize its main flaws or strengths.",3 Data,[0],[0]
This approach is also known as knowledge elicitation – acquiring appropriate information from experts by asking ”why?”,3 Data,[0],[0]
"(Reed and Rowe, 2004).
",3 Data,[0],[0]
We therefore used the reasons as a proxy for developing a scheme for labeling argument quality attributes.,3 Data,[0],[0]
"This was done in a purely bottom-up empirical manner, as opposed to using ‘standard’ evaluation criteria known from argumentation literature (Johnson and Blair, 2006; Schiappa and Nordin, 2013).",3 Data,[0],[0]
"In particular, we split all reasons into several reason units by simple preprocessing (splitting using Stanford CoreNLP",3 Data,[0],[0]
"(Manning et al., 2014), segmentation into Elementary Discourse Units by RST tools (Surdeanu et al., 2015)) and identified the referenced arguments (A1 or A2) by pattern matching and dependency parsing.",3 Data,[0],[0]
"For example, each reason from Figure 1 would be transformed into two reason units.3 Overall, we obtained about 70k reason units from the entire UKPConvArg1 corpus.",3 Data,[0],[0]
"In order to develop a code book for assigning a label to each reason unit, we ran several pilot expert annotation studies (each with 200-300 reason units).",3.1 Annotation scheme,[0],[0]
"Having a set of ≈ 25 distinct labels, we ran two larger studies on Amazon Mechanical Turk (AMT), each with 500 reason units and 10 workers.",3.1 Annotation scheme,[0],[0]
"The workers were split into two groups; we then estimated gold labels for each group using MACE (Hovy et al., 2013) and compared both groups’ results in order to find systematic discrepancies.",3.1 Annotation scheme,[0],[0]
"Finally, we ended up with a set of 19 distinct labels (classes).",3.1 Annotation scheme,[0],[0]
"As the number of classes is too big for non-expert crowd workers, we developed a hierarchical annotation process guided by questions that narrow down the final class decision.",3.1 Annotation scheme,[0],[0]
"The scheme is depicted in Figure 2.4 Workers were shown only the reason units without seeing the original arguments.
",3.1 Annotation scheme,[0],[0]
"3We picked this example for its simplicity, in reality the texts are much more fuzzy.
",3.1 Annotation scheme,[0],[0]
"4It might seem that some labels are missing, such as C8-2 and C8-3; these belong to those removed during the pilot studies.",3.1 Annotation scheme,[0],[0]
"We sampled 26,000 unique reason units ordered by the original author competence provided as part of the UKPConvArg corpus.",3.2 Annotation,[0],[0]
We expected that workers with higher competence tend to write better reasons for their explanations.,3.2 Annotation,[0],[0]
"Using the previously introduced scheme, 776 AMT workers annotated the batch during two weeks; we required assignments from 5 workers for a single item.",3.2 Annotation,[0],[0]
"We employed MACE (Hovy et al., 2013) for gold label and worker competence estimation with 95% threshold to ignore the less confident labels.",3.2 Annotation,[0],[0]
"Several workers were rejected based on their low computed competence and other criteria, such as too short submission times.",3.2 Annotation,[0],[0]
"We performed several cleaning procedures to increase quality and consistency of the annotated data (apart from initial MACE filtering already explained above).
",3.3 Data cleaning,[0],[0]
"Local cleaning First, we removed 3,859 reason units annotated either with C1-2 (”not an explanation”) and C8-6 (”too topic-specific”, which usually paraphrases some details from the related argument and is not general enough).",3.3 Data cleaning,[0],[0]
"In the next step, we removed reason units with wrong polarity.",3.3 Data cleaning,[0],[0]
"In particular, all reason units labeled with C8-* or C9-* should refer to the more convincing argument in the argument pair (as they describe positive properties), whereas all reasons with labels C5-*, C6-*, and C7-* should refer to the less convincing argument.",3.3 Data cleaning,[0],[0]
"The target arguments for reason units were known from the heuristic preprocessing (see above); in this step 2,455 units were removed.
",3.3 Data cleaning,[0],[0]
"Global cleaning Since the argument pairs from one debate can be projected into an argument graph (Habernal and Gurevych, 2016), we utilized this ‘global’ context for further consistency cleaning.
",3.3 Data cleaning,[0],[0]
"Suppose we have two argument pairs, P1(A → B) and P2(B → C) (where→ means “is more convincing than”).",3.3 Data cleaning,[0],[0]
"Let P1(RB) be reason unit targeting
B in argument pair P1 and similarly P2(RB) reason unit targeting B in argument pair P2.",3.3 Data cleaning,[0],[0]
"In other words, two reason units target the same argument in two different argument pairs (in one of them the argument is more convincing while in the other pair it is less convincing).",3.3 Data cleaning,[0],[0]
There might then exist contradicting combination of classes for P1(RB) and P2(RB).,3.3 Data cleaning,[0],[0]
"For example classes C9-2 and C7-3 are contradicting, as the same argument cannot be both ”on the topic” and ”off-topic” at the same time.
",3.3 Data cleaning,[0],[0]
"When such a conflict between two reason units occurred, we selected the reason with a higher score using the following formula:
wW ∗ σ   ∑
A=G
wA",3.3 Data cleaning,[0],[0]
"− λ ∑
A 6=G wA
  (1)
where wW is the competence of the original author of the reason unit (originated from the UKPConvArg corpus), A = G are crowdsourced assignments for a single reason unit that match the final predicted gold label, A 6= G are assignments that differ from the final predicted gold label, wA is the competence of worker for assignment A, λ is a penalty for non-gold labels, and σ is the sigmoid function to squeeze the score between 0 and 1.
",3.3 Data cleaning,[0],[0]
"We found 25 types of global contradictions between labels for reason units and used them for cleaning the data; in total 3,790 reason units were removed in this step.",3.3 Data cleaning,[0],[0]
"After all cleaning procedures, annotations from reason units were mapped back to argument pairs, resulting into a multi-label annotation of one or both arguments from the given pair.",3.3 Data cleaning,[0],[0]
"In total 9,111 pairs from the UKPConvArg corpus were annotated.
",3.3 Data cleaning,[0],[0]
"For example, the final annotations of argument pair shown in Figure 1 contain four labels – C8-1 (as the more convincing argument “has more details, information, facts, or examples / more reasons / better reasoning / goes deeper / is more specific”), C9-3 (as the more convincing argument “has provoking question / makes you think”), C5-2 (as the less convincing argument “has language issues / bad grammar /...”), and C6-1 (as the less convincing argument “provides not enough support / ...” ).",3.3 Data cleaning,[0],[0]
"Only four of six reason units for this argument pair were annotated because of the competence score of their authors.
",3.3 Data cleaning,[0],[0]
Table 1 shows number of labels per argument pairs; about a half of the argument pairs have only one label.,3.3 Data cleaning,[0],[0]
Figure 3 shows distribution of label in the entire data set which is heavily skewed towards C8-1 label.,3.3 Data cleaning,[0],[0]
"This is not surprising, as this label was used for reason units pointing out that the more convincing argument provided more reasons, details, information or better reasoning – a feature inherent to argumentation seen as giving reasons (Freeley and Steinberg, 2008).",3.3 Data cleaning,[0],[0]
"Since the qualitative attributes of arguments were annotated indirectly by labeling their corresponding reason units without seeing the original arguments, we wanted to validate correctness of this approach.",3.4 Data validation,[0],[0]
"We designed a validation study, in which workers were shown the original argument pair and two sets of labels.",3.4 Data validation,[0],[0]
"The first set contained the true labels as annotated previously, while we randomly replaced few labels in the second set.",3.4 Data validation,[0],[0]
"The goal was then to decide which set of labels better explains that argument A is
more convincing than argument B.",3.4 Data validation,[0],[0]
"For example, for the argument pair from Figure 1, one set of shown labels would be {C8-1, C9-3, C5-2, C6-1} (the correct set) while the other ‘distracting’ set would be {C8-1, C9-3, C5-1, C7-3} .
",3.4 Data validation,[0],[0]
We randomly sampled 500 argument pairs and collected 9 assignments per pair on AMT; we again used MACE with 95% threshold.,3.4 Data validation,[0],[0]
Accuracy of workers on 235 argument pairs achieved 82%.,3.4 Data validation,[0],[0]
We can thus conclude that workers tend to prefer explanations based on labels from the reason units and using the annotation process presented in this section is reliable.,3.4 Data validation,[0],[0]
"Total costs of the annotations including pilot studies, bonuses, and data validation were USD 3,300.",3.4 Data validation,[0],[0]
"We propose two experiments, both performed in 16- fold cross-domain validation.",4 Experiments,[0],[0]
"In each fold, argument pairs from 15 debates are used and the remaining one is used for testing.",4 Experiments,[0],[0]
"In both experiments, it is assumed that the more convincing argument in a pair is known and we concatenate (using a particular delimiter) both arguments such that the more convincing argument comes first.",4 Experiments,[0],[0]
This experiment is a multi-label classification.,4.1 Predicting full multi-label distribution,[0],[0]
"Given an argument pair annotated with several labels, the goal is to predict all these labels.
",4.1 Predicting full multi-label distribution,[0],[0]
We use two deep learning models.,4.1 Predicting full multi-label distribution,[0],[0]
"Our first model, Bidirectional Long Short-Term Memory (BLSTM) network contains two LSTM blocks (forward and backward), each with 64 hidden units on the output.",4.1 Predicting full multi-label distribution,[0],[0]
The output is concatenated into a single vector and pushed through sigmoid layer with 17 output units (corresponding to 17 labels).,4.1 Predicting full multi-label distribution,[0],[0]
"We use cross entropy loss function in order to minimize distance of label distributions in training and test data (Nam et al., 2014).",4.1 Predicting full multi-label distribution,[0],[0]
"In the input layer, we rely on pre-trained word embeddings from Glove (Pennington et al., 2014) whose weights are updated during training the network.
",4.1 Predicting full multi-label distribution,[0],[0]
"The second models is BLSTM extended with an attention mechanism (Rocktäschel et al., 2016; Golub and He, 2016) combined with convolution layers over the input.",4.1 Predicting full multi-label distribution,[0],[0]
"In particular, the input em-
bedding layer is convoluted using 4 different convolution sizes (2, 3, 5, 7), each with 1,000 randomly initialized weight vectors.",4.1 Predicting full multi-label distribution,[0],[0]
Then we perform maxover-time pooling and concatenate the output into a single vector.,4.1 Predicting full multi-label distribution,[0],[0]
"This vector is used as the attention module in BLSTM.
",4.1 Predicting full multi-label distribution,[0],[0]
We evaluate the system using two widely used metrics in multi-label classification.,4.1 Predicting full multi-label distribution,[0],[0]
"First, Hamming loss is the average per-item per-class total error; the smaller the better (Zhang and Zhou, 2007).",4.1 Predicting full multi-label distribution,[0],[0]
"Second, we report One-error (Sokolova and Lapalme, 2009) which corresponds to the error of the predicted label with highest probability; the smaller the better.",4.1 Predicting full multi-label distribution,[0],[0]
"We do not report other metrics (such as Area Under PRC-curves, MAP, or cover) as they require tuning a threshold parameter, see a survey by Zhang and Zhou (2014).
",4.1 Predicting full multi-label distribution,[0],[0]
Results from Table 2 do not show significant differences between the two models.,4.1 Predicting full multi-label distribution,[0],[0]
"Putting the oneerror numbers into human performance context can be done only indirectly, as the data validation pre-
sented in Section 3.4 had a different set-up.",4.1 Predicting full multi-label distribution,[0],[0]
"Here we can see that the error rate of the most confident predicted label is about 30%, while human performed similarly by choosing from a two different label sets in a binary settings, so their task was inherently harder.
",4.1 Predicting full multi-label distribution,[0],[0]
Error analysis and discussion We examined outputs from the label distribution prediction for BLSTM/ATT/CNN.,4.1 Predicting full multi-label distribution,[0],[0]
"It turns out that the output layer leans toward predicting the dominant label C8-1, while prediction of other labels is seldom.",4.1 Predicting full multi-label distribution,[0],[0]
"We suspect two causes, first, the highly skewed distribution of labels (see Figure 3) and, second, insufficient training data sizes where 13 classes have less than 1k training examples (while Goodfellow et al. (2016) recommend at least 5k instances per class).
",4.1 Predicting full multi-label distribution,[0],[0]
"Although multi-label classification may be viewed as a set of binary classification tasks that decides for each label independently (and thus allows for employing other ‘standard’ classifiers such as SVM), this so-called binary relevance approach ignores dependencies between the labels.",4.1 Predicting full multi-label distribution,[0],[0]
"That is why we focused directly on deep-learning methods, as they are capable of learning and predicting a full label distribution (Nam et al., 2014).",4.1 Predicting full multi-label distribution,[0],[0]
"In the second experiment, we focus on predicting flaws in arguments using coarse-grained labels.",4.2 Predicting flaws in less convincing arguments,[0],[0]
"While this task makes several simplifications in the labeling, it still provides meaningful insights into argument quality assessment.",4.2 Predicting flaws in less convincing arguments,[0],[0]
"For this purpose, we use only argument pairs where the less convincing argument is labeled with a single label (no multi-label classification).",4.2 Predicting flaws in less convincing arguments,[0],[0]
"Second, we merged all labels from categories C5-",4.2 Predicting flaws in less convincing arguments,[0],[0]
*,4.2 Predicting flaws in less convincing arguments,[0],[0]
C6-* C7-* into three classes corresponding to their parent nodes in the annotation decision schema from Figure 2.,4.2 Predicting flaws in less convincing arguments,[0],[0]
Table 3 shows distribution of the gold data for this task with explanation of the labels.,4.2 Predicting flaws in less convincing arguments,[0],[0]
"It is worth noting that predicting flaws in the less convincing argument is still contextdependent and requires the entire argument pair because some of the quality labels are relative to the more convincing argument (such as “less reasoning” or “not enough support”).
",4.2 Predicting flaws in less convincing arguments,[0],[0]
"For this experiment, we modified the output layer
of the neural models from the previous experiment.",4.2 Predicting flaws in less convincing arguments,[0],[0]
The non-linear output function is softmax and we train the networks using categorical cross-entropy loss.,4.2 Predicting flaws in less convincing arguments,[0],[0]
"We also add another baseline model that employs SVM with RBF kernel5 and a rich set of linguistically motivated features, similarly to (Habernal and Gurevych, 2016).",4.2 Predicting flaws in less convincing arguments,[0],[0]
"The feature set includes uni- and bi-gram presence, ratio of adjective and adverb endings that may signalize neuroticism (Corney et al., 2002), contextuality measure (Heylighen and Dewaele, 2002), dependency tree depth, ratio of exclamation or quotation marks, ratio of modal verbs, counts of several named entity types, ratio of past vs. future tense verbs, POS n-grams, presence of dependency tree production rules, seven different readability measures (e.g., Ari (Senter and Smith, 1967), Coleman-Liau (Coleman and Liau, 1975), Flesch (Flesch, 1948), and others), five sentiment scores (from very negative to very positive) (Socher et al., 2013), spell-checking using standard Unix words, ratio of superlatives, and some surface features such as sentence lengths, longer words count, etc.6 It results into a sparse 60k-dimensional feature vector space.
",4.2 Predicting flaws in less convincing arguments,[0],[0]
Results in Table 4 suggest that the SVM-RBF baseline system performs poorly and its results are on par with a majority class baseline (not reported in detail).,4.2 Predicting flaws in less convincing arguments,[0],[0]
"Both deep learning models significantly outperform the baseline, yielding Macro-F1 score about 0.35.",4.2 Predicting flaws in less convincing arguments,[0],[0]
"The attention-based model performs better than simple BLSTM in two classes (C5 and C6), but the overall Macro-F1 score is not significantly better.
",4.2 Predicting flaws in less convincing arguments,[0],[0]
"5We used LISBVM (Chang and Lin, 2011) with the default hyper-parameters.",4.2 Predicting flaws in less convincing arguments,[0],[0]
"As Fernández-Delgado et al. (2014) show, SVM with gaussian kernels is a reasonable best choice on average.
",4.2 Predicting flaws in less convincing arguments,[0],[0]
"6Detailed explanation of the features can be found directly in the attached source codes.
",4.2 Predicting flaws in less convincing arguments,[0],[0]
Error analysis We manually examined several dozens of predictions where the BLSTM model failed but the BLSTM/ATT/CNN model was correct in order to reveal some phenomena that the system is capable to cope with.,4.2 Predicting flaws in less convincing arguments,[0],[0]
"First, the BLSTM/ATT/CNN model started catching some purely abusive, sarcastic, and attacking arguments.",4.2 Predicting flaws in less convincing arguments,[0],[0]
"Also, the language/grammar issues were revealed in many cases, as well as using slang in arguments.
",4.2 Predicting flaws in less convincing arguments,[0],[0]
Examining predictions in which both systems failed reveal some fundamental limitations of the current purely data-driven computational approach.,4.2 Predicting flaws in less convincing arguments,[0],[0]
"While the problem of not catching off-topic arguments can be probably modeled by incorporating the debate description or some sort of debate topic model into the attention vector, the more common issue of non-sense arguments or fallacious arguments (which seem like actual arguments on the first view) needs much deeper understanding of realworld knowledge, logic, and reasoning.",4.2 Predicting flaws in less convincing arguments,[0.9544165445753386],"['However, even with no word function prior, the predictive performance is still relatively high (55.3 macro F1 across six classes), showing that only the content and context is substantially predictive for the function of a vulgar word Removing tweet content features or part-ofspeech context introduce a similar drop in predictive performance, showing that the overall tweet content and the local syntactic context of the mention play complimentary roles in inference.']"
"This paper presented a novel task in the field of computational argumentation, namely empirical assessment of reasons for argument convincingness.",5 Conclusion,[0],[0]
We created a new large benchmark data set by utilizing a new annotation scheme and several filtering strategies for crowdsourced data.,5 Conclusion,[0],[0]
"Then we tackled two challenging tasks, namely multi-label classification of argument pairs in order to reveal qualitative properties of the arguments, and predicting flaws in the less convincing argument from the given argument pair.",5 Conclusion,[0.9529362385570952],['We use logistic regression5 to build six one vs. all binary classifiers for each of the six functions using information from the immediate lexical and syntactic context surrounding the word and general usage of the word in training data.']
We performed all evaluations in a cross-domain scenario and experimented with feature-rich SVM and two state-of-the-art neural network models.,5 Conclusion,[0],[0]
The results are promising but show that the task is inherently complex as it requires deep reasoning about the presented arguments that goes beyond capabilities of the current computational models.,5 Conclusion,[0],[0]
"By releasing the
UKPConvArg2 data and code to the community, we believe more progress can be made in this direction in the near future.",5 Conclusion,[0],[0]
"This work has been supported by the Volkswagen Foundation as part of the Lichtenberg-Professorship Program under grant No I/82806, by the German Institute for Educational Research (DIPF), by the German Research Foundation (DFG) via the GermanIsraeli Project Cooperation (DIP, grant DA 1600/1- 1), by the GRK 1994/1",Acknowledgments,[0],[0]
"AIPHES (DFG), by the ArguAna Project GU 798/20-1 (DFG), and by Amazon Web Services in Education Grant award.",Acknowledgments,[0],[0]
"Lastly, we would like to thank the anonymous reviewers for their valuable feedback.",Acknowledgments,[0],[0]
This article tackles a new challenging task in computational argumentation.,abstractText,[0],[0]
"Given a pair of two arguments to a certain controversial topic, we aim to directly assess qualitative properties of the arguments in order to explain why one argument is more convincing than the other one.",abstractText,[0],[0]
We approach this task in a fully empirical manner by annotating 26k explanations written in natural language.,abstractText,[0],[0]
"These explanations describe convincingness of arguments in the given argument pair, such as their strengths or flaws.",abstractText,[0],[0]
"We create a new crowd-sourced corpus containing 9,111 argument pairs, multilabeled with 17 classes, which was cleaned and curated by employing several strict quality measures.",abstractText,[0],[0]
"We propose two tasks on this data set, namely (1) predicting the full label distribution and (2) classifying types of flaws in less convincing arguments.",abstractText,[0],[0]
Our experiments with feature-rich SVM learners and Bidirectional LSTM neural networks with convolution and attention mechanism reveal that such a novel fine-grained analysis of Web argument convincingness is a very challenging task.,abstractText,[0],[0]
We release the new corpus UKPConvArg2 and the accompanying software under permissive licenses to the research community.,abstractText,[0],[0]
What makes a convincing argument? Empirical analysis and detecting attributes of convincingness in Web argumentation,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4208–4219 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
4208",text,[0],[0]
"Evaluating natural language understanding (NLU) systems is a long-established problem in AI (Levesque, 2014).",1 Introduction,[0],[0]
"One approach to doing so is the machine reading comprehension (MRC) task, in which a system answers questions about given texts (Hirschman et al., 1999).",1 Introduction,[0],[0]
"Although recent studies have made advances (Yu et al., 2018), it is still unclear to what precise extent questions require understanding of texts (Jia and Liang, 2017).
",1 Introduction,[0],[0]
"In this study, we examine MRC datasets and discuss what is needed to create datasets suit-
able for the detailed testing of NLU.",1 Introduction,[0],[0]
"Our motivation originates from studies that demonstrated unintended biases in the sourcing of other NLU tasks, in which questions contain simple patterns and systems can recognize these patterns to answer them (Gururangan et al., 2018; Mostafazadeh et al., 2017).
",1 Introduction,[0],[0]
We conjecture that a situation similar to this occurs in MRC datasets.,1 Introduction,[0],[0]
"Consider the question shown in Figure 1, for example.",1 Introduction,[0],[0]
"Although the question, starting with when, requires an answer that is expressed as a moment in time, there is only one such expression (i.e., November 2014) in the given text (we refer to the text as the context).",1 Introduction,[0],[0]
"In other words, the question has only a single candidate answer.",1 Introduction,[0],[0]
The system can solve it merely by recognizing the entity type required by when.,1 Introduction,[0],[0]
"In addition to this, even if another expression of time appears in other sentences, only one sentence (i.e., s1) appears to be related to the question; thus, the system can easily determine the correct answer by attention, that is, by matching the words appearing both in the context and the ques-
tion.",1 Introduction,[0],[0]
"Therefore, this kind of question does not require a complex understanding of language—e.g., multiple-sentence reasoning, which is known as a more challenging task (Richardson et al., 2013).
",1 Introduction,[0],[0]
"In Section 3, we define two heuristics, namely entity-type recognition and attention.",1 Introduction,[0],[0]
We specifically analyze the differences in the performance of baseline systems for the following two configurations: (i) questions answerable or unanswerable with the first k tokens; and (ii) questions whose correct answer appears or does not appear in the context sentence that is most similar to the question (henceforth referred to as the most similar sentence).,1 Introduction,[0],[0]
"Although similar heuristics are proposed by Weissenborn et al. (2017), ours are utilized for question filtering, rather than system development; Using these simple heuristics, we split each dataset into easy and hard subsets for further investigation of the baseline performance.
",1 Introduction,[0],[0]
"After conducting the experiments, we analyze the following two points in Section 4.",1 Introduction,[0],[0]
"First, we consider which questions are valid for testing, i.e., reasonably solvable.",1 Introduction,[0],[0]
"Second, we consider what reasoning skills are required and whether this exposes any differences among the subsets.",1 Introduction,[0],[0]
"To investigate these two concerns, we manually annotate sample questions from each subset in terms of validity and required reasoning skills, such as word matching, knowledge inference, and multiple sentence reasoning.
",1 Introduction,[0],[0]
"We examine 12 recently proposed MRC datasets (Table 1), which include answer extraction, description, and multiple-choice styles.",1 Introduction,[0],[0]
We also observe differences based on these styles.,1 Introduction,[0],[0]
"For our baselines, we use two neural-based systems, namely, the Bidirectional Attention Flow (Seo et al., 2017) and the Gated-Attention Reader (Dhingra et al., 2017).
",1 Introduction,[0],[0]
"In Section 5, we describe the advantages and disadvantages of different question styles with regard to evaluating NLU systems.",1 Introduction,[0],[0]
"We also interpret our heuristics for constructing realistic MRC datasets.
",1 Introduction,[0],[0]
"Our contributions are as follows:
•",1 Introduction,[0],[0]
"This study is the first large-scale investigation across recent 12 MRC datasets with three question styles.
",1 Introduction,[0],[0]
"• We propose to employ simple heuristics to split each dataset into easy and hard subsets and examine the performance of two baseline models for each of the subsets.
",1 Introduction,[0],[0]
"• We manually annotate questions sampled from each subset with both validity and requisite reasoning skills to investigate which skills explain the difference between easy and hard questions.
",1 Introduction,[0],[0]
"We observed the following:
•",1 Introduction,[0],[0]
"The baseline performances for the hard subsets remarkably degrade compared to those of entire datasets.
",1 Introduction,[0],[0]
"• Our annotation study shows that hard questions require knowledge inference and multiplesentence reasoning in comparison with easy questions.
",1 Introduction,[0],[0]
"• Compared to questions with answer extraction and description styles, multiple-choice questions tend to require a broader range of reasoning skills while exhibiting answerability, multiple answer candidates, and unambiguity.
",1 Introduction,[0],[0]
These findings suggest that one might overestimate recent advances in MRC systems.,1 Introduction,[0],[0]
"They also emphasize the importance of considering simple answer-seeking heuristics when sourcing questions, in that a dataset could be easily biased unless such heuristics are employed.1",1 Introduction,[0],[0]
"We analyzed 12 MRC datasets with three question styles: answer extraction, description, and
1All scripts used in this study, along with the subsets of the datasets and the annotation results, are available at https://github.com/Alab-NII/ mrc-heuristics.
multiple choice (Table 1).",2.1 Datasets,[0],[0]
"Our aim was to select datasets varying in terms of corpus genre, context length, and question sourcing methods.2 Other datasets that are not covered in our study, but can be analyzed using the same method, include: QA4MRE (Sutcliffe et al., 2013), CNN/Daily Mail (Hermann et al., 2015), Children’s Book Test (Hill et al., 2016), bAbI (Weston et al., 2015),",2.1 Datasets,[0],[0]
"WikiReading (Hewlett et al., 2016), LAMBADA (Paperno et al., 2016), Who-did-What (Onishi et al., 2016), ProPara (Dalvi et al., 2018), MultiRC (Khashabi et al., 2018), CliCR (Suster and Daelemans, 2018), SQuAD (v2.0) (Rajpurkar et al., 2018), and DuoRC (Saha et al., 2018).",2.1 Datasets,[0],[0]
"We employed the following two widely used baselines.
",2.2 Baseline Systems,[0],[0]
"Bidirectional Attention Flow (BiDAF) (Seo et al., 2017) was used for the answer extraction and description datasets.",2.2 Baseline Systems,[0],[0]
BiDAF models bi-directional attention between the context and question.,2.2 Baseline Systems,[0],[0]
"It achieved state-of-the-art performance on the SQuAD dataset.
",2.2 Baseline Systems,[0],[0]
"Gated-Attentive Reader (GA) (Dhingra et al., 2017) was used for the multiple-choice datasets.",2.2 Baseline Systems,[0],[0]
GA has a multi-hop architecture with an attention mechanism.,2.2 Baseline Systems,[0],[0]
"It achieved state-of-the-artperformance on the CNN/Daily Mail and Whodid-What datasets.
",2.2 Baseline Systems,[0],[0]
"Why we used different baseline systems: The multiple-choice style can be transformed to answer extraction, as mentioned in Clark et al. (2018).",2.2 Baseline Systems,[0],[0]
"However, in some datasets, many questions have no textual overlap to determine the correct answer span in the context.",2.2 Baseline Systems,[0],[0]
"Therefore, in order to avoid underestimating the baseline performance of those datasets, we used the GA system which is applicable to multiple choice questions.
",2.2 Baseline Systems,[0],[0]
"We scored the performance using exact match (EM)/F1 (Rajpurkar et al., 2016), Rouge-L (Lin, 2004), and accuracy for the answer extraction, description, and multiple-choice datasets, respectively (henceforth, we refer to these collectively as the score, for simplicity).",2.2 Baseline Systems,[0],[0]
"For the description datasets, we determined in advance the answer span of the context that gives the highest Rouge-L score to the human-generated gold answer.",2.2 Baseline Systems,[0],[0]
"We computed the Rouge-L score between
2The ARC Easy and Challenge were collected using different methods; hence, we treated them as different datasets (see Clark et al. (2018) for further details).
",2.2 Baseline Systems,[0],[0]
"the predicted span and the gold answer.3
Reproduction of the baseline performance: We used the same architecture as the official baseline systems unless specified otherwise.",2.2 Baseline Systems,[0.9510519810690221],['We directly and explicitly include the function of the vulgar word present in the tweet by introducing six new features to the hate speech detection model which represent the scores with which the vulgar word is associated with the six functions.']
All systems were trained on the training set and tested on the development/test set of each dataset.,2.2 Baseline Systems,[0],[0]
We also used different hyperparameters for each dataset according to characteristics such as context length (see Appendix A for details).,2.2 Baseline Systems,[0],[0]
We show the baseline performance of both the official results and those from our implementations in Tables 2 and 3.,2.2 Baseline Systems,[0],[0]
Our implementations outperformed or showed comparable performance to the official baseline on most datasets.,2.2 Baseline Systems,[0],[0]
"However, in TriviaQA, MCTest, RACE, and ARC-E, our baseline performance did not reach that of the official baseline, due to differences in architecture or the absence of reported hyperparameters in the literature.",2.2 Baseline Systems,[0],[0]
The first goal of this paper is to determine whether there are unintended biases of the kind exposed in Figure 1 in MRC datasets.,3 Two Filtering Heuristics,[0],[0]
We examined the influence of the two filtering heuristics: (i) entity type recognition (Section 3.1) and (ii) attention (Section 3.2).,3 Two Filtering Heuristics,[0],[0]
We then investigated the performance of the baseline systems on the questions filtered by the defined heuristics (Section 3.3).,3 Two Filtering Heuristics,[0],[0]
"The aim of this heuristic was to detect questions that can be solved based on (i) the existence of a single candidate answer that is restricted by expressions such as “wh-” and “how many,” and (ii) lexical patterns that appear around the correct answer.",3.1 Entity Type-based Heuristic,[0],[0]
"Because the query styles are not uniform across datasets (e.g., MARCO uses search engine queries), we could not directly use interrogatives.",3.1 Entity Type-based Heuristic,[0],[0]
"Instead, we simply provided the first k tokens of questions to the baseline systems.",3.1 Entity Type-based Heuristic,[0],[0]
We chose smaller values for k than the (macro) average of the question length across the datasets (= 12.2 tokens).,3.1 Entity Type-based Heuristic,[0],[0]
"For example, for k = 4 of the question will I qualify for OSAP if I’m new in Canada (excerpted from MARCO), we use will I qualify for.",3.1 Entity Type-based Heuristic,[0],[0]
"Even if the tokens do not have an interrogative, the system may recognize lexical patterns around the correct answer.",3.1 Entity Type-based Heuristic,[0],[0]
"Questions that can be solved
3We used the official evaluation scripts of SQuAD and MS MARCO to compute the EM/F1 and Rouge-L, respectively.
by examining these patterns were also of interest when filtering.
",3.1 Entity Type-based Heuristic,[0],[0]
"Results: Tables 2 and 3 present the results for k = 1, 2, 4.",3.1 Entity Type-based Heuristic,[0],[0]
"In addition, to know the exact ratio of the questions that are solved rather than the scores for the answer extraction and description styles, we counted questions with k = 2 that achieved the score ≥ 0.5.4",3.1 Entity Type-based Heuristic,[0],[0]
"As k decreased, so too did the baseline performance on all datasets in Table 2 except QAngaroo.",3.1 Entity Type-based Heuristic,[0],[0]
"By contrast, in QAngaroo and the multiple-choice datasets, the performance did not degrade so strongly.",3.1 Entity Type-based Heuristic,[0],[0]
"In particular, the difference between the scores on the full and k = 1 questions in QAngaroo was 1.8.",3.1 Entity Type-based Heuristic,[0],[0]
"Because the questions in QAngaroo are not complete sentences, but rather knowledge-base entries that have a blank, such as country of citizenship Henry VI of England, this result implies that the baseline system can infer the answer merely by the first token of questions, i.e., the type of knowledge-base entry.
",3.1 Entity Type-based Heuristic,[0],[0]
"In most multiple-choice datasets, the k = 1 scores were significantly higher than randomchoice scores.",3.1 Entity Type-based Heuristic,[0],[0]
"Given that multiple-choice ques-
4We considered that this threshold is sufficient to judge that the system attends to the correct span because of the potential ambiguity of these styles (see Section 4).
tions offer multiple options that are of valid entity/event types, this gap was not necessarily caused by the limited number of candidate answers, as in the case with the answer extraction datasets.",3.1 Entity Type-based Heuristic,[0],[0]
"Therefore, we inferred that in the solved questions, incorrect options appeared less than the correct option did or did not appear at all in the context (such questions were regarded as solvable exclusively using the word match skill, which we analyzed in Section 4).",3.1 Entity Type-based Heuristic,[0],[0]
"Remarkably, although we failed to achieve a higher baseline performance, the score for the complete questions in MCTest was lower than that of the k = 1 questions.",3.1 Entity Type-based Heuristic,[0],[0]
This result showed that the MCTest questions were sufficiently difficult such that it was not especially useful for the baseline system to consider the entire question statement.,3.1 Entity Type-based Heuristic,[0],[0]
"Next, we examined in each dataset (i) how many questions have their correct answers in the most similar sentence and (ii) whether a performance gap exists for such questions (i.e., whether such questions are easier than the others).
",3.2 Attention-based Heuristic,[0],[0]
"We used uni-gram overlap as a similarity mea-
sure.5 We counted how many times question words appeared in each sentence, where question words were stemmed and stopwords were dropped.",3.2 Attention-based Heuristic,[0],[0]
We then checked whether the correct answer appeared in the most similar sentence.,3.2 Attention-based Heuristic,[0],[0]
"For the multiple-choice datasets, we selected the text span that provided the highest Rouge-L score with the correct option as the correct answer.
",3.2 Attention-based Heuristic,[0],[0]
Results: Tables 2 and 3 show the results.,3.2 Attention-based Heuristic,[0],[0]
"Considering the average number of context sentences, most datasets contained a significantly high proportion of questions whose answers were in the most similar sentence.
",3.2 Attention-based Heuristic,[0],[0]
"In the answer extraction and description datasets, except QAngaroo, the baseline performance improved when the correct answer appeared in the most similar sentence, and gaps were found between the performances on these questions and the others.",3.2 Attention-based Heuristic,[0],[0]
These gaps indicated that the dataset may lack balance for testing NLU.,3.2 Attention-based Heuristic,[0],[0]
"If these questions tend to require the word matching skill exclusively, attending the other portion is useful in studying a more realistic NLU, e.g., common-sense reasoning and discourse understanding.",3.2 Attention-based Heuristic,[0],[0]
"Therefore, we investigated whether
5Although there are other similarity measures, we used this basic measure to obtain an intuitive result.
",3.2 Attention-based Heuristic,[0],[0]
"these questions merely require word matching (see Section 4).
",3.2 Attention-based Heuristic,[0],[0]
"Meanwhile, in the first three multiple-choice datasets, the performance differences were marginal or inversed, implying that although the baseline performance was not especially high, the difficulty of these questions for the baseline system was not affected by whether their correct answers appeared in the most similar sentence.
",3.2 Attention-based Heuristic,[0],[0]
We further analyzed the baseline performance after removing the context and leaving only the most similar sentence.,3.2 Attention-based Heuristic,[0],[0]
"In AddSent and QAngaroo, the scores remarkably improved (>20 F1).",3.2 Attention-based Heuristic,[0],[0]
"From this result, we can infer that on these datasets the baseline systems were distracted by other sentences in the context.",3.2 Attention-based Heuristic,[0],[0]
"This observation was supported by the results from the AddSent dataset (Jia and Liang, 2017), which contains manually injected distracting sentences (i.e., adversarial examples).
",3.2 Attention-based Heuristic,[0],[0]
"3.3 Performance on Hard Subsets
In the previous two sections, we observed that in the examined datasets (i) some questions were solved by the baseline systems merely with the first k tokens and/or (ii) the baseline performances increased for questions whose answers were in the most similar sentence.",3.2 Attention-based Heuristic,[0],[0]
"We were concerned that these two will become dominant factors in measuring the baseline performance using the datasets; Hence, we split each development/test set into easy and hard subsets for further investigation.
",3.2 Attention-based Heuristic,[0],[0]
Hard subsets: A hard subset comprised questions (i) whose score is not positive when k = 2 and (ii) whose correct answer does not appear in the most similar sentence.,3.2 Attention-based Heuristic,[0],[0]
The easy subsets comprised the remaining questions.,3.2 Attention-based Heuristic,[0],[0]
We aimed to investigate the gap of the performance values between the easy and hard subsets.,3.2 Attention-based Heuristic,[0],[0]
"If the gap is large, the dataset may be strongly biased toward questions that are solved by recognizing entity types or lexical patterns and may not be suitable for measuring the system’s ability for complex reasoning.
",3.2 Attention-based Heuristic,[0],[0]
Results and clarification: The bottom row of Tables 2 and 3 shows that the baseline performances on the hard subset remarkably decreased in almost all examined datasets.,3.2 Attention-based Heuristic,[0],[0]
These results revealed that we may overestimate the ability of the baseline systems previously perceived.,3.2 Attention-based Heuristic,[0],[0]
"How-
ever, we clarify that our intention is not to remove the questions solved or mitigated by our defined heuristics to create a new hard subset because this may generate new biases as indicated in Gururangan et al. (2018).",3.2 Attention-based Heuristic,[0],[0]
"Rather, we would like to emphasize the importance of the defined heuristics when sourcing questions.",3.2 Attention-based Heuristic,[0],[0]
"Indeed, ill attention to these heuristics can lead to unintended biases.",3.2 Attention-based Heuristic,[0],[0]
"Objectives: To complement the observations in the previous sections, we annotated sampled questions from each subset of the datasets.",4.1 Annotation Specifications,[0],[0]
Our motivation can be summarized as follows: (i) How many questions are valid in each dataset?,4.1 Annotation Specifications,[0],[0]
"That is, the hard questions may not in fact be hard, but just unsolvable, as indicated in Chen et al. (2016).",4.1 Annotation Specifications,[0],[0]
(ii) What kinds of reasoning skills explain the easy/hard questions?,4.1 Annotation Specifications,[0],[0]
"(iii) Are there any differences among the datasets and the question styles?
",4.1 Annotation Specifications,[0],[0]
We annotated the minimum skills required to choose the correct answer among other candidates.,4.1 Annotation Specifications,[0],[0]
"We assumed that the solver knows what type of entity or event is entailed by the question.
",4.1 Annotation Specifications,[0],[0]
Annotation labels:,4.1 Annotation Specifications,[0],[0]
"Our annotation labels (Table 4) were inspired by previous works such as Chen et al. (2016), Trischler et al. (2017), and Lai et al. (2017).",4.1 Annotation Specifications,[0],[0]
"The major modifications were twofold: (i) detailed question validity, including a number of reasonable candidate answers and answer ambiguity, and (ii) posing multiple-sentence reasoning as a skill compatible with other skills.
",4.1 Annotation Specifications,[0],[0]
Reasoning types indeed have other classifications.,4.1 Annotation Specifications,[0],[0]
"For instance, Lai et al. (2017) defined five reasoning types, including attitude analysis and whole-picture reasoning.",4.1 Annotation Specifications,[0],[0]
We incorporated them into the knowledge and meta/whole classes.,4.1 Annotation Specifications,[0],[0]
"Clark et al. (2018) proposed detailed knowledge and reasoning types, but these were specific to science exams and, thus, omitted from our study.
",4.1 Annotation Specifications,[0],[0]
"Independent of the abovementioned reasoning types, we checked whether the question required multiple-sentence reasoning to answer the questions.",4.1 Annotation Specifications,[0],[0]
"As another modification, we extended the notion of “sentence” in our annotation and considered a subordinate clause as a sentence.",4.1 Annotation Specifications,[0],[0]
"This modification was intended to deal with the internal complexity of a sentence with multiple clauses, which can also render a question difficult.
",4.1 Annotation Specifications,[0],[0]
"Settings: For each subset of the datasets, 30 questions were annotated.",4.1 Annotation Specifications,[0],[0]
Therefore we obtained annotations for 30× 2× 12 = 720 questions.,4.1 Annotation Specifications,[0],[0]
The annotation was performed by the authors.,4.1 Annotation Specifications,[0],[0]
"The annotator was given the context, question, and candidate answers for multiple-choice questions along with the correct answer.",4.1 Annotation Specifications,[0],[0]
"To reduce bias, the annotator did not know which easy or hard subset the questions were in, and was not told the predictions and scores of the respective baseline systems.",4.1 Annotation Specifications,[0],[0]
Tables 5 and 6 show the annotation results.,4.2 Annotation Results,[0],[0]
"Validity: TriviaQA, QAngaroo, and ARCs revealed a relatively high unsolvability, which seemed to be caused by the unrelatedness between the questions and their context.",4.2 Annotation Results,[0],[0]
"For example, QAngaroo’s context was gathered from Wikipedia articles that were not necessarily related to the questions.6",4.2 Annotation Results,[0],[0]
"The context passages in ARCs were
6Nonetheless, it is remarkable that even though the dataset was automatically constructed, the remaining valid hard
curated from textbooks that may not provide sufficient information to answer the questions.7 Note
questions were difficult for the baseline system.",4.2 Annotation Results,[0],[0]
"7Our analysis was not intended to undermine the quality
that it is possible for unsolvable questions to be permitted, and that the system must indicate them in some datasets, such as QA4MRE, NewsQA, MARCO, and SQuAD (v2.0).
",4.2 Annotation Results,[0],[0]
"However, for single candidate, we found that few questions had only single-candidate answers.",4.2 Annotation Results,[0],[0]
"Furthermore, there were even fewer singlecandidate answers in AddSent than in SQuAD.",4.2 Annotation Results,[0],[0]
"This result supported the claim that the adversarial examples augmented the number of possible candidate answers, thereby degrading the baseline performance.
",4.2 Annotation Results,[0],[0]
"In our annotation, ambiguous questions were
of these questions.",4.2 Annotation Results,[0],[0]
"We refer readers to Clark et al. (2018).
found to be those with multiple correct spans.",4.2 Annotation Results,[0],[0]
Figure 2 shows an example.,4.2 Annotation Results,[0],[0]
"In this case, several answers aside from “93” were correct.",4.2 Annotation Results,[0],[0]
"Ambiguity is an important feature insofar because it can lead to unstable scoring in EM/F1.
",4.2 Annotation Results,[0],[0]
"The multiple-choice datasets mostly comprised valid questions, with the exception of the unsolvable questions in the ARC datasets.
",4.2 Annotation Results,[0],[0]
"Reasoning skills: We can see that word matching was more important in the easy subsets, and knowledge was more pertinent to the hard subsets in 10 of the 12 datasets.",4.2 Annotation Results,[0],[0]
These results confirmed that the manner by which we split the subsets was successful at filtering questions that were relatively easy in terms of reasoning skills.,4.2 Annotation Results,[0],[0]
"However, we did not observe this trend with paraphrasing, which seemed difficult to distinguish from word matching and knowledge.",4.2 Annotation Results,[0],[0]
"With regard to meta/whole and math/logic, we can see that these skills were needed less in the answer extraction and description datasets.",4.2 Annotation Results,[0],[0]
"They were more pertinent to the multiple-choice datasets.
",4.2 Annotation Results,[0],[0]
Multiple-sentence reasoning:,4.2 Annotation Results,[0],[0]
Multiplesentence reasoning was more correlated with the hard subsets in 10 of the 12 datasets.,4.2 Annotation Results,[0],[0]
"Although NewsQA showed the inverse tendency for word matching, knowledge, and multiple-sentence reasoning, we suspect that this was caused by annotation variance and filtering a large portion of ambiguous questions.",4.2 Annotation Results,[0.9546851099691447],"['Our hypothesis is that explicitly modeling the function a vulgar word has in context will benefit the hate speech prediction task, by differentiating between aggression and other usages.']"
"For relational types, we did not see a significant trend in any particular type.
",4.2 Annotation Results,[0],[0]
"Correlation of labels and baseline scores: Across all examined datasets, we analyzed the correlations between the annotation labels and the scores of each baseline system in Table 7.",4.2 Annotation Results,[0],[0]
"In spite of the small size of the annotated samples, we derived statistically significant correlations for six labels.",4.2 Annotation Results,[0],[0]
These results confirmed that BiDAF performed well for the word matching questions and relatively poorly with the knowledge questions.,4.2 Annotation Results,[0],[0]
"By contrast, we did not observe this trend in GA.",4.2 Annotation Results,[0],[0]
"In this section, we discuss the advantages and disadvantages of the question styles.",5 Discussion,[0],[0]
"We also interpret the defined heuristics in terms of constructing more realistic MRC datasets.
",5 Discussion,[0],[0]
"Differences among the question styles: The biggest advantage to the answer extraction style is its ease in generating questions, which enables us to produce large-scale datasets.",5 Discussion,[0],[0]
"In contrast, a disadvantage to this style is that it rarely demands meta/whole and math/logic skills, which can require answers not contained in the context.",5 Discussion,[0],[0]
"Moreover, as observed in Section 4, it seems difficult to guarantee that all possible answer spans are given as the correct answers.",5 Discussion,[0],[0]
"By contrast, the description and multiple-choice styles have the advantage of having no such restrictions on the appearance of candidate answers (Kočiský et al., 2018; Khashabi et al., 2018).",5 Discussion,[0],[0]
"Nonetheless, the description style is difficult to evaluate because the Rouge-L and BLEU scores are insufficient for testing NLU.",5 Discussion,[0],[0]
"Whereas it is easy to evaluate the performance on multiple-choice questions, generating multiple reasonable options requires considerable effort.
",5 Discussion,[0],[0]
"Interpretation of our heuristics: When we regard the MRC task as recognizing textual entailment (RTE) (Dagan et al., 2006), the task requires the reader to construct one or more premises from the context and form the most reasonable hypothesis from the question and candidate answer (Sachan et al., 2015).",5 Discussion,[0.9525859773432106],"['For the complete description of the composition and construction of the Vulgar Twitter corpus, we refer the interested reader to the original paper (Cachola et al., 2018).']"
"Thus, easier questions are those (i) where the reader needs to generate only one hypothesis, and (ii) where the premises directly describe the correct hypothesis.",5 Discussion,[0],[0]
Our two heuristics can also be seen as the formalizations of these criteria.,5 Discussion,[0],[0]
"Therefore, to make questions more realistic, we need to create multiple hypotheses that require complex reasoning to be distinguished.",5 Discussion,[0],[0]
"Moreover, the integration of premises should be complemented by external knowledge to provide sufficient information to verify the correct hypothesis.",5 Discussion,[0],[0]
"Our heuristics and annotation were motivated by unintended biases (Levesque, 2014) and evaluation overfitting (Whiteson et al., 2011), respectively.
",6 Related Work,[0],[0]
"Unintended biases: The MRC task tests a reading process that involves retrieving stored information and performing inferences (Sutcliffe et al.,
2013).",6 Related Work,[0],[0]
"However, constructing datasets that comprehensively require those skills is difficult.",6 Related Work,[0],[0]
"As Levesque (2014) discussed as a desideratum for testing AI, we should avoid creating questions that can be solved by matching patterns, using unintended biases, and selectional restrictions.",6 Related Work,[0],[0]
"For the unintended biases, one suggestive example is the Story Cloze Test (Mostafazadeh et al., 2016), in which a system chooses a sentence among candidates to conclude a given paragraph of the story.",6 Related Work,[0],[0]
"A recent attempt at this task showed that recognizing superficial features in the correct candidate is critical to achieve the state of the art (Schwartz et al., 2017).
",6 Related Work,[0],[0]
"Similarly, in MRC, Weissenborn et al. (2017) proposed context/type matching heuristic to develop a simple neural system.",6 Related Work,[0],[0]
"Min et al. (2018) observed that, in SQuAD, 92% of answerable questions can be answered only using a single context sentence.",6 Related Work,[0],[0]
"In visual question answering, Agrawal et al. (2016) analyzed the behavior of models with the variable length of the first question words.",6 Related Work,[0],[0]
"Khashabi et al. (2018) more recently proposed a dataset with questions for multisentence reasoning.
",6 Related Work,[0],[0]
"Evaluation overfitting: The theory behind evaluating AI distinguishes between taskand skill-oriented approaches (Hernández-Orallo, 2017).",6 Related Work,[0],[0]
"In the task-oriented approach, we usually develop a system and test it on a specific dataset.",6 Related Work,[0],[0]
The developed system sometimes lacks generality but achieves the state of the art for that specific dataset.,6 Related Work,[0],[0]
"Further, it becomes difficult to verify and explain the solution to tasks.",6 Related Work,[0],[0]
"The situation in which we are biased to the specific tasks is called evaluation overfitting (Whiteson et al., 2011).",6 Related Work,[0],[0]
"By contrast, with the skill-oriented approach, we aim to interpret the relationships between tasks and skills.",6 Related Work,[0],[0]
"This orientation can encourage the development of more realistic NLU systems.
",6 Related Work,[0],[0]
"As One of our goals was to investigate whether easy questions are dominant in recent datasets, it did not necessarily require a detailed classification of reasoning types.",6 Related Work,[0],[0]
"Nonetheless, we recognize there are more fine-grained classifications of the required skills for NLU.",6 Related Work,[0],[0]
"For example, Weston et al. (2015) defined 20 skills as a set of toy tasks.",6 Related Work,[0],[0]
Sugawara et al. (2017) also organized 10 prerequisite skills for MRC.,6 Related Work,[0],[0]
LoBue and Yates (2011) and Sammons et al. (2010) analyzed entailment phenomena using detailed classifications in RTE.,6 Related Work,[0],[0]
"For
the ARC dataset, Boratko et al. (2018) proposed knowledge and reasoning types.",6 Related Work,[0],[0]
This study examined MRC questions from 12 datasets to determine what makes such questions easier to answer.,7 Conclusion,[0],[0]
We defined two heuristics that limit candidate answers and thereby mitigate the difficulty of questions.,7 Conclusion,[0],[0]
"Using these heuristics, the datasets were split into easy and hard subsets.",7 Conclusion,[0],[0]
We further annotated the questions with their validity and the reasoning skills needed to answer them.,7 Conclusion,[0],[0]
"Our experiments revealed that the baseline performance degraded with the hard questions, which required knowledge inference and multiple-sentence reasoning compared to easy questions.",7 Conclusion,[0],[0]
These results suggest that one might overestimate the ability of the baseline systems.,7 Conclusion,[0],[0]
They also emphasize the importance of analyzing and reporting the properties of new datasets when released.,7 Conclusion,[0],[0]
One limitation of this work was the heavy cost of the annotation.,7 Conclusion,[0],[0]
"In future research, we plan to explore a method for automatically classifying reasoning types.",7 Conclusion,[0],[0]
This will enable us to evaluate systems through a detailed organization of the datasets.,7 Conclusion,[0],[0]
"We would like to thank Rajarshi Das, Shehzaad Dhuliawala, and anonymous reviewers for their insightful comments.",Acknowledgments,[0],[0]
This work was supported by JSPS KAKENHI Grant Numbers 18H03297 and 18J12960.,Acknowledgments,[0],[0]
"We used different hyperparameters for each dataset because of the different characteristics of the datasets, e.g., the context length.",A Hyperparameters of the Baseline Systems,[0],[0]
Tables 8 and 9 show the hyperparameters.,A Hyperparameters of the Baseline Systems,[0],[0]
A challenge in creating a dataset for machine reading comprehension (MRC) is to collect questions that require a sophisticated understanding of language to answer beyond using superficial cues.,abstractText,[0],[0]
"In this work, we investigate what makes questions easier across recent 12 MRC datasets with three question styles (answer extraction, description, and multiple choice).",abstractText,[0],[0]
We propose to employ simple heuristics to split each dataset into easy and hard subsets and examine the performance of two baseline models for each of the subsets.,abstractText,[0],[0]
We then manually annotate questions sampled from each subset with both validity and requisite reasoning skills to investigate which skills explain the difference between easy and hard questions.,abstractText,[0],[0]
"From this study, we observed that (i) the baseline performances for the hard subsets remarkably degrade compared to those of entire datasets, (ii) hard questions require knowledge inference and multiple-sentence reasoning in comparison with easy questions, and (iii) multiplechoice questions tend to require a broader range of reasoning skills than answer extraction and description questions.",abstractText,[0],[0]
These results suggest that one might overestimate recent advances in MRC.,abstractText,[0],[0]
What Makes Reading Comprehension Questions Easier?,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 25–32 New Orleans, Louisiana, June 1 - 6, 2018. c©2017 Association for Computational Linguistics",text,[0],[0]
"Many commercial applications of artificial agents require task-oriented conversational agents that help customers achieve a specific goal, such as making or cancelling a payment or reservation (Zue et al., 2000; Bennacef et al., 1996).",1 Introduction,[0],[0]
"These chatbots must extract relevant information from the user, provide relevant knowledge to her, and issue appropriate system calls to achieve the goal.
",1 Introduction,[0],[0]
"Supervised approaches such as seq2seq models (Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2015; Sordoni et al., 2015b), have recently gained attention in non-task oriented dialog, due to their ability to perform end-to-end learning from expert dialogues1, removing the need for many of the independent modules in traditional systems such as, natural language understanding, dialog state tracker and natural language generator.
",1 Introduction,[0],[0]
Seq2Seq models have also shown promising results on small domain or synthetic task-oriented dialog datasets.,1 Introduction,[0],[0]
"However, performance was much worse when we applied these models to real world
1We refer to an entire session of text exchanges between an agent and a customer as a dialogue.
datasets.",1 Introduction,[0],[0]
"This is in part because end-to-end methods, in general, require large amounts of data before they are able to generate fluent textual responses.",1 Introduction,[0],[0]
"In real world settings, words chosen by human users and agents are not constrained to a fixed vocabulary, and hence we see many lexical variations even among semantically similar dialogs.
",1 Introduction,[0],[0]
"To ensure that information is both conveyed and understood, we want responses to be fluent as well as coherent.",1 Introduction,[0],[0]
We say a response is coherent if it is a sensible response in the dialogue context.,1 Introduction,[0],[0]
"Table 1 shows responses generated by a variant of the seq2seq model, when trained on real customeragent chat transcripts.",1 Introduction,[0],[0]
"The response of the chatbot during the fourth turn2 in Table 1, accepting the customer’s expression of gratitude, is coherent and fluent.",1 Introduction,[0],[0]
Coherence of a response does not necessarily guarantee fluency.,1 Introduction,[0],[0]
"The generated response during the second turn is coherent but not fluent.
",1 Introduction,[0],[0]
"On our customer support dataset, seq2seq models performed well with salutations, but performed poorly both in terms of fluency and coherency on intermediate responses.",1 Introduction,[0],[0]
"The reason being, salutations contain minimal lexical variations across dialogs and occur more frequently when compared to other utterances.",1 Introduction,[0],[0]
"(Koehn and Knowles, 2017) use beam search decoding in Neural Machine Translation to mitigate fluency issues on larger translation datasets.",1 Introduction,[0],[0]
"Typically increasing the beam size improves translation quality, however, increasing beam sizes in Neural MT has shown to produce poor translations (Koehn and Knowles, 2017).
",1 Introduction,[0],[0]
We propose nearest neighbor based approaches that can directly use and replay available expert utterances.,1 Introduction,[0],[0]
"This removes the need for the models to learn the grammar of the language, and allows the models to focus on learning what to say, rather than how to say it.",1 Introduction,[0],[0]
"The nearest neighbor-based
2We define a turn as a pair of text exchanges between the customer and the agent.
25
methods we propose naturally generate more fluent responses, since they use actual agent responses.",1 Introduction,[0],[0]
"However, our results in Table 3 show that they perform poorly in predicting external actions and at ensuring dialogue level coherency.",1 Introduction,[0],[0]
"In contrast, the skip-connection seq2seq models we propose here, learn when to produce external actions and produce more coherent dialogues.",1 Introduction,[0],[0]
"We propose a hybrid model that brings together the strengths of both the approaches.
",1 Introduction,[0],[0]
"The contributions of this paper are as follows:
• We propose skip-connections to handle multiturn dialogue that outperforms previous models.
",1 Introduction,[0],[0]
• We propose a hybrid model where nearest neighbor-based models generate fluent responses and skip-connection models generate accurate responses and external actions.,1 Introduction,[0],[0]
"We show the effectiveness of the belief state representations obtained from the skip-connection model by comparing against previous approaches.
",1 Introduction,[0],[0]
"• To the best of our knowledge, our paper makes the first attempt at evaluating state of the art models on a large real world task with human users.",1 Introduction,[0],[0]
"We show that methods that achieve state of the art performance on synthetic datasets, perform poorly in real world dialog tasks.",1 Introduction,[0],[0]
"Comparing Tables 2 and 3, we see the impact of moving from synthetic to real world datasets, and as a result, find issues with previously proposed models that may have been obscured by the simplicity and regularity of synthetic datasets.",1 Introduction,[0],[0]
"Although seq2seq models have been applied in taskoriented settings (Wen et al., 2017; Williams and
Zweig, 2016; Bordes and Weston, 2016; Zhao and Eskénazi, 2016), they have only been evaluated on small domain or synthetic datasets.
",2 Related Work,[0],[0]
More recent work has focused on representation learning for multi-turn dialogue.,2 Related Work,[0],[0]
Sordoni et al. (2015b) use a single bag-of-words representation of the entire dialog history.,2 Related Work,[0],[0]
"Such a representation ignores the order of responses, which is crucial to ensure that utterances are coherent across turns.",2 Related Work,[0],[0]
An alternative approach is to use a hierarchical encoder-decoder network (HRED),2 Related Work,[0],[0]
"(Sordoni et al., 2015a) which uses a complex three layered RNN network, a query level encoder, a session level encoder and a decoder.",2 Related Work,[0],[0]
"Attentional networks (Bordes and Weston, 2016; Dodge et al., 2015) use a weighted combination of all the context vectors upto the current turn.",2 Related Work,[0],[0]
Attentional networks proved to be a stronger baseline over HRED during our evaluation.,2 Related Work,[0],[0]
"We propose models that learn fixed size representations of the history using simpler skip-connection models showing comparable performance with attentional networks (Bordes and Weston, 2016; Dodge et al., 2015).
",2 Related Work,[0],[0]
Our work is closely related to retrieval-based chatbots.,2 Related Work,[0],[0]
"Williams and Zweig (2016), select a response from a small set of templates.",2 Related Work,[0],[0]
"Zhou et al. (2016); Yan et al. (2016) perform multi-turn dialogue by treating the dialogue history as the query, and perform classification with the number of classes equal to the number of possible responses.",2 Related Work,[0],[0]
"They evaluate precision@K, from a restricted list, but do not indicate how this list is obtained in practice.",2 Related Work,[0],[0]
"In our real world dataset, the number of possible responses grows with the dataset size.",2 Related Work,[0],[0]
"In addition, responses are unevenly distributed with salutations occurring frequently.",2 Related Work,[0],[0]
"As a
result, the classification based approach performed poorly, with most of the outputs being salutations.",2 Related Work,[0],[0]
Complete automation of customer service is still not possible as chatbots are not perfect yet.,3 Proposed Approach,[0],[0]
"However, automation where possible in the workflow could still result in considerable savings.",3 Proposed Approach,[0],[0]
"In order to ensure that the end user experience is not substandard, in live user testing, we ask a human agent to play intermediary role between the chatbot and the user.",3 Proposed Approach,[0],[0]
A user initiates a chat by entering an initial query or an issue that requires resolution (Figure 1).,3 Proposed Approach,[0],[0]
The chatbot responds with 5 diverse responses.,3 Proposed Approach,[0],[0]
"The agent selects the most relevant response, and may choose to modify it.",3 Proposed Approach,[0],[0]
"If the response is not relevant, she may type a different response.",3 Proposed Approach,[0],[0]
"During offline testing, the chatbot returns only one response and no human agent is used.",3 Proposed Approach,[0],[0]
The following section describes our skip connection seq2seq model for representation learning and our nearest neighbor approach for response selection.,3 Proposed Approach,[0],[0]
First we describe the datasets and metrics we use.,3 Proposed Approach,[0],[0]
"We use data from bAbI (Task1 and Task2) (Bordes and Weston, 2016) to evaluate our models.",3.1 Dataset and Metrics,[0],[0]
"Other dialog tasks in bAbI require the model to mimic a knowledge base i.e., memorize it.",3.1 Dataset and Metrics,[0],[0]
"This is not a suitable strategy for our application, since in practice knowledge bases undergo frequent changes, making this infeasible.",3.1 Dataset and Metrics,[0],[0]
"In the bAbI task, the user interacts with an agent in a simulated restaurant reservation application, by providing her constraints, such as place, cuisine, number of people or price range.",3.1 Dataset and Metrics,[0],[0]
"The agent or chatbot performs external actions or SQL-like queries (api call) to retrieve information
from the knowledge base of restaurants.",3.1 Dataset and Metrics,[0],[0]
"We used 80% of the data for training (of which 10% was used for validation) and the remaining 20% for testing.
",3.1 Dataset and Metrics,[0],[0]
We also evaluate our models on an internal customer support dataset of 160k chat transcripts containing 3 million interactions.,3.1 Dataset and Metrics,[0],[0]
We limit the number of turns to 20.,3.1 Dataset and Metrics,[0],[0]
We will refer to this dataset as CS large.,3.1 Dataset and Metrics,[0],[0]
"We perform spell correction, deidentification to remove customer sensitive information, lexical normalization particularly of lingo words such as, lol and ty.",3.1 Dataset and Metrics,[0],[0]
Generalizing such entities reduces the amount of training data required.,3.1 Dataset and Metrics,[0],[0]
"The values must be reinserted, currently by a human in the loop.",3.1 Dataset and Metrics,[0],[0]
"We have also masked product and the organization name in the examples.
",3.1 Dataset and Metrics,[0],[0]
"The use of MT evaluation metrics to evaluate dialogue fluency with just one reference has been debated (Liu et al., 2016).",3.1 Dataset and Metrics,[0],[0]
"There is still no good alternative to evaluate dialog systems, and so we continue to report fluency using BLEU (BiLingual Evaluation Understudy (Papineni et al., 2002)), in addition to other metrics and human evaluations.",3.1 Dataset and Metrics,[0.9507124706671629],"['Previous research showed that liberals are more likely to use more vulgarity overall in social media (Sylwester and Purver, 2015; Preoţiuc-Pietro et al., 2017) and are perceived by others to use more frequently than they do vulgar words (Carpenter et al., 2016), but this analysis shows this is especially due to vulgar word use to emphasise.']"
"Coherency also requires measuring correctness of the external actions which we measure using a metric we call, Exact Query Match (EQM), which represents the fraction of times the api call matched the ground truth query issued by the human agent.",3.1 Dataset and Metrics,[0],[0]
We do not assign any credit to partial matches.,3.1 Dataset and Metrics,[0],[0]
"In addition, we report the precision (P), recall (R) and accuracy (Acc) achieved by the models in predicting whether to make an api call (positive) or not (negative).",3.1 Dataset and Metrics,[0],[0]
Obtaining and aligning api calls with the chat transcripts is often complex as such information is typically stored in multiple confidential logs.,3.1 Dataset and Metrics,[0],[0]
"In order to measure coherency with respect to api calls, we randomly sampled 1000 chat tran-
scripts and asked human agents to hand annotate the api calls wherever appropriate.",3.1 Dataset and Metrics,[0],[0]
We will refer to this labeled dataset as CS small.,3.1 Dataset and Metrics,[0],[0]
"Seq2seq models are an application of Long ShortTerm Memory (Hochreiter and Schmidhuber, 1997) architecture where inputs and outputs are variable length sequences.",3.1.1 Skip Connection Seq2Seq Model,[0],[0]
We unroll the basic seq2seq model and make one copy for each turn.,3.1.1 Skip Connection Seq2Seq Model,[0],[0]
This is illustrated in Figure 2.,3.1.1 Skip Connection Seq2Seq Model,[0],[0]
"Input words are one hot encoded, and projected using a linear layer to obtain xtk for the input word at position k in turn t, resulting in a sequence Xt = {xt1, xt2, ...xtL}.",3.1.1 Skip Connection Seq2Seq Model,[0],[0]
"The output sequence to be generated is represented by Yt = {yt1, yt2, ...ytL′}.",3.1.1 Skip Connection Seq2Seq Model,[0],[0]
"The encoder at turn t receives the user’s projected input, as well as the context vectors from the final hidden units of the encoder and the decoder at turn t − 1, forming a skip connection.",3.1.1 Skip Connection Seq2Seq Model,[0],[0]
This ensures that a fixed size vector is used to represent the dialogue history at every turn.,3.1.1 Skip Connection Seq2Seq Model,[0],[0]
Orange-solid-square boxes in Figure 2 represent LSTM cells of the encoder.,3.1.1 Skip Connection Seq2Seq Model,[0],[0]
"htL,enc is the context vector which is sent to every LSTM cell in the decoder (dec) at any turn t (Cho et al., 2014).
",3.1.1 Skip Connection Seq2Seq Model,[0],[0]
Green-dashed-square cells in the decoder represent the LSTM and dense layers with a softmax non-linearity.,3.1.1 Skip Connection Seq2Seq Model,[0],[0]
These are trained to predict each word in the agent’s utterance.,3.1.1 Skip Connection Seq2Seq Model,[0],[0]
Each of the seq2seq copies share the same parameters.,3.1.1 Skip Connection Seq2Seq Model,[0],[0]
"Once the training is complete, we use only one copy of the seq2seq model to make predictions.",3.1.1 Skip Connection Seq2Seq Model,[0],[0]
The results obtained with the vanilla seq2seq model on the bAbI dataset is shown in the first row (Model 1) of Table 2.,3.1.2 Results with Skip-Connections,[0],[0]
"The EQM is 0%, even though the BLEU scores look reasonable.",3.1.2 Results with Skip-Connections,[0],[0]
"Model 2 is the skip-connection seq2seq model, where only the output of the hidden states from the decoder at turn t− 1 is appended to the input at time t, i.e., ht−1L,enc from the encoder history is not explicitly presented to turn t.
Model 3 extends Model 1 by adding an attentional layer.",3.1.2 Results with Skip-Connections,[0],[0]
Model 3 is a variant of Bordes and Weston (2016); Dodge et al. (2015) where the output of the attentional layer is sent to the decoder for generating the responses rather than classifying as one of the known responses.,3.1.2 Results with Skip-Connections,[0],[0]
This variant performed better on the customer support data compared to a direct implementation of Bordes and Weston (2016).,3.1.2 Results with Skip-Connections,[0],[0]
"The reason being, salutations occurred more frequently in the customer support data and hence, the classification based approach originally proposed by Bordes and Weston (2016) classified most of the outputs as salutations.",3.1.2 Results with Skip-Connections,[0],[0]
"Finally, Model 4 extends Model 2 by providing ht−1L,enc to turn t.
We see that explicitly adding skip-connections substantially improves performance in EQM, from 0 or 6% to 55%, and has a positive effect on BLEU.",3.1.2 Results with Skip-Connections,[0],[0]
The models show similar behavior on CS small.,3.1.2 Results with Skip-Connections,[0],[0]
"In this case, when an api call is executed, the result is treated as a response and sent as input to the next turn.",3.1.2 Results with Skip-Connections,[0],[0]
"Although Model 4 performed the best
on CS small and CS large, our analysis showed that the generated responses were most often incoherent and not fluent, a phenomenon that did not arise in the synthetic dataset.",3.1.2 Results with Skip-Connections,[0],[0]
"We now proceed to explain the nearest neighbor based approach, which we show is able to produce reasonable responses that are more fluent.",3.1.2 Results with Skip-Connections,[0],[0]
"In our nearest neighbor approach, an agent’s response is chosen from human generated transcripts or the training data - ensuring fluency.",3.2 Nearest Neighbor-based approach,[0],[0]
"However, this does not necessarily ensure that the responses are coherent in the context of the dialogue.",3.2 Nearest Neighbor-based approach,[0.955938152378247],['Controlling for faith and political ideology with partial correlation does not alter the significance of this result.']
"The nearest neighbor approach starts with a representation of the entire dialogue history bst,i for turn t and dialogue i. Together with at,i, the action the agent took while in this state i.e., the natural language response or api call query issued by the agent, this results in a tuple < bst,i, at,i >.",3.2 Nearest Neighbor-based approach,[0],[0]
"The entire training data is converted into a set of tuples S, that contains pairwise relationships between dialog state representations and agent actions.
",3.2 Nearest Neighbor-based approach,[0],[0]
"In the online or test phase, given an embedding of the dialogue so far, testV ec, we find the nearest neighbor bstestV ec in S. We return the nearest neighbor’s corresponding response, atestV ec, as the predicted agent’s response.",3.2 Nearest Neighbor-based approach,[0],[0]
"We use ball trees (Kibriya and Frank, 2007) to perform efficient nearest neighbor search.",3.2 Nearest Neighbor-based approach,[0],[0]
"Since we want to provide more flexibility to the human agent in choosing the most
appropriate response, we extended this approach to find k = 100 responses and then used a diversitybased ranking approach (Zhu et al., 2007) to return 5 diverse responses.",3.2 Nearest Neighbor-based approach,[0],[0]
"To construct the adjacency matrix for diversity ranking, we use word overlap between responses after stop word removal.
",3.2 Nearest Neighbor-based approach,[0],[0]
"Numerous techniques have been proposed for representating text including word2vec and sent2vec (Mikolov et al., 2013b,a; Pagliardini et al., 2017; Pennington et al., 2014).",3.2 Nearest Neighbor-based approach,[0.9716966762509875],"['Several data sets and approaches to automatic hate speech detection have been recently proposed (Djuric et al., 2015; Burnap and Williams, 2015; Waseem and Hovy, 2016; Nobata et al., 2016; Davidson et al., 2017).']"
"In the following sections, we compare these approaches against our proposed representations using skip connections.",3.2 Nearest Neighbor-based approach,[0],[0]
"In our first baseline, Model 6, for a dialogue, i, the user’s response at turn t, usert, is concatenated with his/her responses in previous turns (useri,1:t−1) and the agent’s responses upto turn t − 1 (agenti,1:t−1), to obtain, pi,t = (useri,1:t, agenti,1:t−1).",3.2.1 Dialogue Embeddings from Word/Sentence Embeddings,[0],[0]
"We obtain a belief state vector representation as the average of the word2vec (Mikolov et al., 2013b) representations of words in pi,t. We then apply the nearest neighbor approach described in Section 3.2.",3.2.1 Dialogue Embeddings from Word/Sentence Embeddings,[0],[0]
"Results obtained with this approach on CS small are in Table 3.
",3.2.1 Dialogue Embeddings from Word/Sentence Embeddings,[0],[0]
We emphasize a subtle but important oracle advantage that we give this baseline algorithm.,3.2.1 Dialogue Embeddings from Word/Sentence Embeddings,[0],[0]
"When we obtain the embeddings of a test dialogue, we use the true utterances of the expert agent so far,
which would not be available in practice.",3.2.1 Dialogue Embeddings from Word/Sentence Embeddings,[0],[0]
"However, we will show that our proposed representation, described in Section 3.3, performs better, even without access to this information.
",3.2.1 Dialogue Embeddings from Word/Sentence Embeddings,[0],[0]
Pagliardini et al. (2017) recently described a method that leads to better sentence-level representations.,3.2.1 Dialogue Embeddings from Word/Sentence Embeddings,[0],[0]
We use their approach as another baseline.,3.2.1 Dialogue Embeddings from Word/Sentence Embeddings,[0],[0]
bst is represented by the average of the sentence embeddings of all agent’s responses upto turn t− 1 and user’s responses upto turn t. We also explore geometric discounting to give higher importance to recent responses.,3.2.1 Dialogue Embeddings from Word/Sentence Embeddings,[0],[0]
We use a similar process to obtain representations for the user’s responses during the test phase.,3.2.1 Dialogue Embeddings from Word/Sentence Embeddings,[0],[0]
"As done with word-embeddings, we provide true agent responses upto turn t− 1 for predicting the agent’s response at turn t. Results obtained on CS small by averaging (Model 7) and discounted averaging (Model 8) are given in Table 3.",3.2.1 Dialogue Embeddings from Word/Sentence Embeddings,[0],[0]
Model 8 performs better than Model 7 across all measures.,3.2.1 Dialogue Embeddings from Word/Sentence Embeddings,[0],[0]
"A comparison between Model 6, 7 and 8 with Model 4 in Table 3, would not be a fair one as Model 4 does not use previous true agent responses to predict the agent’s next response.",3.2.1 Dialogue Embeddings from Word/Sentence Embeddings,[0],[0]
"We suggest using the outputs of the hidden units in the decoder of our skip connection seq2seq model, as suitable representations for the belief states.",3.3 Hybrid model: Nearest Neighbor with Seq2Seq Embeddings,[0.9593115513868851],"['Part of Speech Context –We encode the part of speech of the target word, the previous word and the next word as one-hot vectors as we expect syntactic information to be an indicator of different functions in context.']"
The seq2seq model for handling multi-turn dialogue is trained as before (Section 3.1.1).,3.3 Hybrid model: Nearest Neighbor with Seq2Seq Embeddings,[0],[0]
"Once the parameters have been learned, we proceed to generate representations for all turns in the training data.",3.3 Hybrid model: Nearest Neighbor with Seq2Seq Embeddings,[0],[0]
"The output of the last hidden unit of the encoder or the decoder before turn t is used to represent the belief state vector at turn t. As before, we obtain a set S consisting of pairs of belief state vectors and next actions taken by the agent.
",3.3 Hybrid model: Nearest Neighbor with Seq2Seq Embeddings,[0],[0]
"We test the models as done in Section 3.1.1, except now we select responses using the nearest neighbor approach (Figure 2).",3.3 Hybrid model: Nearest Neighbor with Seq2Seq Embeddings,[0],[0]
Results obtained are in Table 3 (Models 9 and 10).,3.3 Hybrid model: Nearest Neighbor with Seq2Seq Embeddings,[0],[0]
Model 9 uses the output of the last hidden unit of the encoder.,3.3 Hybrid model: Nearest Neighbor with Seq2Seq Embeddings,[0],[0]
"Model 10 uses previous turn’s decoder’s last hidden unit.
",3.3 Hybrid model: Nearest Neighbor with Seq2Seq Embeddings,[0],[0]
Both the models show a significant improvement in BLEU when compared to generating the agent’s response (Model 4).,3.3 Hybrid model: Nearest Neighbor with Seq2Seq Embeddings,[0],[0]
"Although Model 10 was not exposed to the past true agent responses, it still achieved comparable performance to that of Model 8.",3.3 Hybrid model: Nearest Neighbor with Seq2Seq Embeddings,[0],[0]
"Appending both the encoder and the decoder outputs did not have significant impact.
",3.3 Hybrid model: Nearest Neighbor with Seq2Seq Embeddings,[0],[0]
The results also show that the seq2seq model achieved a better EQM when compared to the nearest neighbor approach.,3.3 Hybrid model: Nearest Neighbor with Seq2Seq Embeddings,[0],[0]
"The final hybrid model, we propose (Model 11) combines both strategies.",3.3 Hybrid model: Nearest Neighbor with Seq2Seq Embeddings,[0],[0]
We run both the Models 4 and 10 in parallel.,3.3 Hybrid model: Nearest Neighbor with Seq2Seq Embeddings,[0],[0]
"When Model 4 predicts an API response, we use the output generated by Model 4 as the agent’s response, otherwise we use the output of Model 10 as the predicted agent’s response.",3.3 Hybrid model: Nearest Neighbor with Seq2Seq Embeddings,[0],[0]
"This model achieved the best results among all models we study, both in terms of fluency (BLEU) as well as correctness of external actions (EQM).",3.3 Hybrid model: Nearest Neighbor with Seq2Seq Embeddings,[0],[0]
"The hybrid model achieves a 78% relative improvement (from 9.91 to 17.67) in fluency scores, and 200% improvement in EQM over previous approaches (from 0.10 to 0.30).
",3.3 Hybrid model: Nearest Neighbor with Seq2Seq Embeddings,[0],[0]
Table 4 shows results obtained on CS large (column 3) using models that performed the best on the other datasets.,3.3 Hybrid model: Nearest Neighbor with Seq2Seq Embeddings,[0],[0]
Another obvious baseline is to use traditional retrieval approaches.,3.3 Hybrid model: Nearest Neighbor with Seq2Seq Embeddings,[0],[0]
"(query, agent response) pairs are created for each agent response, with a query constructed by concatenating all the agent’s responses upto turn t− 1 and user’s responses upto turn t, for an agent response at time t. For a given dialogue history query, the corresponding agent response is retrieved using Lucene3.",3.3 Hybrid model: Nearest Neighbor with Seq2Seq Embeddings,[0],[0]
"Since CS large did not contain labeled api calls, we report results using Model 10.",3.3 Hybrid model: Nearest Neighbor with Seq2Seq Embeddings,[0],[0]
"As seen, Model 10 provides a substantial boost in performance.",3.3 Hybrid model: Nearest Neighbor with Seq2Seq Embeddings,[0],[0]
"One caveat to the above evaluations is that they are based on customer responses to the actual human agent interactions, and are not fully indicative of how customers would react to the real automated system in practice.",3.4 Manual Online Evaluation,[0],[0]
"Another disadvantage of using
3https://lucene.apache.org/
automated evaluation with just one reference, is that the score (BLEU) penalizes valid responses that may be lexically different from the available agent response.",3.4 Manual Online Evaluation,[0],[0]
"To overcome this issue, we conducted online experiments with human agents.
",3.4 Manual Online Evaluation,[0],[0]
We used 5 human users and 2 agents.,3.4 Manual Online Evaluation,[0],[0]
On average each user interacted with an agent on 10 different issues that needed resolution.,3.4 Manual Online Evaluation,[0],[0]
"To compare against our baseline, each user interacted with the Model 4, 5 and 10 using the same issues.",3.4 Manual Online Evaluation,[0],[0]
This resulted in ≈ 50 dialogues from each of the models.,3.4 Manual Online Evaluation,[0],[0]
"After every response from the user, the human agent was allowed to select one of the top five responses the system selected.",3.4 Manual Online Evaluation,[0],[0]
"We refer to the selected response as A. The human agent was asked to make minimal modifications to the selected response, resulting in a response A′. If the responses suggested were completely irrelevant, the human agent was allowed to type in the most suitable response.
",3.4 Manual Online Evaluation,[0],[0]
"We then computed the BLEU between the system generated responses (As) and human generated responses (A′s), referred to as Online-BLEU in Table 4.",3.4 Manual Online Evaluation,[0],[0]
"Since the human agent only made minimal changes where appropriate, we believe the BLEU score would now be more correlated to human judgments.",3.4 Manual Online Evaluation,[0],[0]
"Since CS large did not contain any api calls, we only report BLEU scores.",3.4 Manual Online Evaluation,[0],[0]
"The results obtained with models 4, 5 and 10 on CS large are shown in Table 4 (column 4).",3.4 Manual Online Evaluation,[0],[0]
Model 10 performs better than Models 4 and 5.,3.4 Manual Online Evaluation,[0],[0]
"We do not measure inter-annotator agreement as each human user can take a different dialog trajectory.
",3.4 Manual Online Evaluation,[0],[0]
We noticed that the approach mimics certain interesting human behavior.,3.4 Manual Online Evaluation,[0],[0]
"For example, in Table 5, the chatbot detects that the user is frustrated and responds with smileys and even makes exceptions on the return policy.",3.4 Manual Online Evaluation,[0],[0]
We demonstrated limitations of previous end-end dialog approaches and proposed variants to make them suitable for real world settings.,4 Conclusion and Future Work,[0],[0]
"In ongoing work, we explore reinforcement learning tech-
niques to reach the goal state quicker thereby reducing the number of interactions.",4 Conclusion and Future Work,[0],[0]
"In task-oriented dialog, agents need to generate both fluent natural language responses and correct external actions like database queries and updates.",abstractText,[0],[0]
"We show that methods that achieve state of the art performance on synthetic datasets, perform poorly in real world dialog tasks.",abstractText,[0],[0]
"We propose a hybrid model, where nearest neighbor is used to generate fluent responses and Sequence-to-Sequence (Seq2Seq) type models ensure dialogue coherency and generate accurate external actions.",abstractText,[0],[0]
"The hybrid model on an internal customer support dataset achieves a 78% relative improvement in fluency, and a 200% improvement in external call accuracy.",abstractText,[0],[0]
What we need to learn if we want to do and not just talk,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2126–2136 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
2126",text,[0],[0]
"Despite Ray Mooney’s quip that you cannot cram the meaning of a whole %&!$# sentence into a single $&!#* vector, sentence embedding methods have achieved impressive results in tasks ranging from machine translation (Sutskever et al., 2014; Cho et al., 2014) to entailment detection (Williams et al., 2018), spurring the quest for “universal embeddings” trained once and used in a variety of applications (e.g., Kiros et al., 2015; Conneau et al., 2017; Subramanian et al., 2018).",1 Introduction,[0],[0]
Positive results on concrete problems suggest that embeddings capture important linguistic properties of sentences.,1 Introduction,[0],[0]
"However, real-life “downstream” tasks require complex forms of inference, making it difficult to pinpoint the information a model is relying upon.",1 Introduction,[0],[0]
"Impressive as it might be that a system can tell that the sentence “A movie that doesn’t aim too high, but it doesn’t need to” (Pang and Lee, 2004) expresses a subjective viewpoint, it is
hard to tell how the system (or even a human) comes to this conclusion.",1 Introduction,[0],[0]
"Complex tasks can also carry hidden biases that models might lock onto (Jabri et al., 2016).",1 Introduction,[0],[0]
"For example, Lai and Hockenmaier (2014) show that the simple heuristic of checking for explicit negation words leads to good accuracy in the SICK sentence entailment task.
",1 Introduction,[0],[0]
Model introspection techniques have been applied to sentence encoders in order to gain a better understanding of which properties of the input sentences their embeddings retain (see Section 5).,1 Introduction,[0],[0]
"However, these techniques often depend on the specifics of an encoder architecture, and consequently cannot be used to compare different methods.",1 Introduction,[0],[0]
"Shi et al. (2016) and Adi et al. (2017) introduced a more general approach, relying on the notion of what we will call probing tasks.",1 Introduction,[0],[0]
A probing task is a classification problem that focuses on simple linguistic properties of sentences.,1 Introduction,[0],[0]
"For example, one such task might require to categorize sentences by the tense of their main verb.",1 Introduction,[0],[0]
"Given an encoder (e.g., an LSTM) pre-trained on a certain task (e.g., machine translation), we use the sentence embeddings it produces to train the tense classifier (without further embedding tuning).",1 Introduction,[0],[0]
"If the classifier succeeds, it means that the pre-trained encoder is storing readable tense information into the embeddings it creates.",1 Introduction,[0],[0]
Note that: (i),1 Introduction,[0],[0]
"The probing task asks a simple question, minimizing interpretability problems.",1 Introduction,[0],[0]
"(ii) Because of their simplicity, it is easier to control for biases in probing tasks than in downstream tasks.",1 Introduction,[0],[0]
"(iii) The probing task methodology is agnostic with respect to the encoder architecture, as long as it produces a vector representation of sentences.
",1 Introduction,[0],[0]
We greatly extend earlier work on probing tasks as follows.,1 Introduction,[0],[0]
"First, we introduce a larger set of probing tasks (10 in total), organized by the type of linguistic properties they probe.",1 Introduction,[0],[0]
"Second, we systematize the probing task methodology, controlling for
a number of possible nuisance factors, and framing all tasks so that they only require single sentence representations as input, for maximum generality and to ease result interpretation.",1 Introduction,[0],[0]
"Third, we use our probing tasks to explore a wide range of state-of-the-art encoding architectures and training methods, and further relate probing and downstream task performance.",1 Introduction,[0],[0]
"Finally, we are publicly releasing our probing data sets and tools, hoping they will become a standard way to study the linguistic properties of sentence embeddings.1",1 Introduction,[0],[0]
"In constructing our probing benchmarks, we adopted the following criteria.",2 Probing tasks,[0],[0]
"First, for generality and interpretability, the task classification problem should only require single sentence embeddings as input (as opposed to, e.g., sentence and word embeddings, or multiple sentence representations).",2 Probing tasks,[0],[0]
"Second, it should be possible to construct large training sets in order to train parameter-rich multi-layer classifiers, in case the relevant properties are non-linearly encoded in the sentence vectors.",2 Probing tasks,[0],[0]
"Third, nuisance variables such as lexical cues or sentence length should be controlled for.",2 Probing tasks,[0],[0]
"Finally, and most importantly, we want tasks that address an interesting set of linguistic properties.",2 Probing tasks,[0],[0]
"We thus strove to come up with a set of tasks that, while respecting the previous constraints, probe a wide range of phenomena, from superficial properties of sentences such as which words they contain to their hierarchical structure to subtle facets of semantic acceptability.",2 Probing tasks,[0],[0]
"We think the current task set is reasonably representative of different linguistic domains, but we are not claiming that it is exhaustive.",2 Probing tasks,[0],[0]
"We expect future work to extend it.
",2 Probing tasks,[0],[0]
"The sentences for all our tasks are extracted from the Toronto Book Corpus (Zhu et al., 2015), more specifically from the random pre-processed portion made available by Paperno et al. (2016).",2 Probing tasks,[0],[0]
We only sample sentences in the 5-to-28 word range.,2 Probing tasks,[0],[0]
"We parse them with the Stanford Parser (2017-06-09 version), using the pre-trained PCFG model (Klein and Manning, 2003), and we rely on the part-of-speech, constituency and dependency parsing information provided by this tool where needed.",2 Probing tasks,[0],[0]
"For each task, we construct training sets containing 100k sentences, and 10k-sentence val-
1https://github.com/facebookresearch/ SentEval/tree/master/data/probing
idation and test sets.",2 Probing tasks,[0],[0]
"All sets are balanced, having an equal number of instances of each target class.
",2 Probing tasks,[0],[0]
Surface information These tasks test the extent to which sentence embeddings are preserving surface properties of the sentences they encode.,2 Probing tasks,[0],[0]
One can solve the surface tasks by simply looking at tokens in the input sentences: no linguistic knowledge is called for.,2 Probing tasks,[0],[0]
The first task is to predict the length of sentences in terms of number of words (SentLen).,2 Probing tasks,[0],[0]
"Following Adi et al. (2017), we group sentences into 6 equal-width bins by length, and treat SentLen as a 6-way classification task.",2 Probing tasks,[0],[0]
The word content (WC) task tests whether it is possible to recover information about the original words in the sentence from its embedding.,2 Probing tasks,[0],[0]
"We picked 1000 mid-frequency words from the source corpus vocabulary (the words with ranks between 2k and 3k when sorted by frequency), and sampled equal numbers of sentences that contain one and only one of these words.",2 Probing tasks,[0],[0]
The task is to tell which of the 1k words a sentence contains (1k-way classification).,2 Probing tasks,[0],[0]
"This setup allows us to probe a sentence embedding for word content without requiring an auxiliary word embedding (as in the setup of Adi and colleagues).
",2 Probing tasks,[0],[0]
Syntactic information The next batch of tasks test whether sentence embeddings are sensitive to syntactic properties of the sentences they encode.,2 Probing tasks,[0],[0]
The bigram shift (BShift) task tests whether an encoder is sensitive to legal word orders.,2 Probing tasks,[0],[0]
"In this binary classification problem, models must distinguish intact sentences sampled from the corpus from sentences where we inverted two random adjacent words (“What you are doing out there?”).
",2 Probing tasks,[0],[0]
"The tree depth (TreeDepth) task checks whether an encoder infers the hierarchical structure of sentences, and in particular whether it can group sentences by the depth of the longest path from root to any leaf.",2 Probing tasks,[0],[0]
"Since tree depth is naturally correlated with sentence length, we de-correlate these variables through a structured sampling procedure.",2 Probing tasks,[0],[0]
"In the resulting data set, tree depth values range from 5 to 12, and the task is to categorize sentences into the class corresponding to their depth (8 classes).",2 Probing tasks,[0],[0]
"As an example, the following is a long (22 tokens) but shallow (max depth: 5) sentence: “[1 [2 But right now, for the time being, my past, my fears, and my thoughts [3 were [4 my [5business]]].]",2 Probing tasks,[0],[0]
"]” (the outermost brackets correspond to the ROOT and S nodes in the parse).
",2 Probing tasks,[0],[0]
"In the top constituent task (TopConst), sentences must be classified in terms of the sequence of top constituents immediately below the sentence (S) node.",2 Probing tasks,[0],[0]
"An encoder that successfully addresses this challenge is not only capturing latent syntactic structures, but clustering them by constituent types.",2 Probing tasks,[0],[0]
TopConst was introduced by Shi et al. (2016).,2 Probing tasks,[0],[0]
"Following them, we frame it as a 20-way classification problem: 19 classes for the most frequent top constructions, and one for all other constructions.",2 Probing tasks,[0],[0]
"As an example, “[Then] [very dark gray letters on a black screen]",2 Probing tasks,[0],[0]
[appeared],2 Probing tasks,[0],[0]
"[.]” has top constituent sequence: “ADVP NP VP .”.
",2 Probing tasks,[0],[0]
"Note that, while we would not expect an untrained human subject to be explicitly aware of tree depth or top constituency, similar information must be implicitly computed to correctly parse sentences, and there is suggestive evidence that the brain tracks something akin to tree depth during sentence processing (Nelson et al., 2017).
",2 Probing tasks,[0],[0]
"Semantic information These tasks also rely on syntactic structure, but they further require some understanding of what a sentence denotes.",2 Probing tasks,[0],[0]
"The Tense task asks for the tense of the main-clause verb (VBP/VBZ forms are labeled as present, VBD as past).",2 Probing tasks,[0],[0]
"No target form occurs across the train/dev/test split, so that classifiers cannot rely on specific words (it is not clear that Shi and colleagues, who introduced this task, controlled for this factor).",2 Probing tasks,[0],[0]
The subject number (SubjNum) task focuses on the number of the subject of the main clause (number in English is more often explicitly marked on nouns than verbs).,2 Probing tasks,[0],[0]
"Again, there is no target overlap across partitions.",2 Probing tasks,[0],[0]
"Similarly, object number (ObjNum) tests for the number of the direct object of the main clause (again, avoiding lexical overlap).",2 Probing tasks,[0],[0]
"To solve the previous tasks correctly, an encoder must not only capture tense and number, but also extract structural information (about the main clause and its arguments).",2 Probing tasks,[0],[0]
"We grouped Tense, SubjNum and ObjNum with the semantic tasks, since, at least for models that treat words as unanalyzed input units (without access to morphology), they must rely on what a sentence denotes (e.g., whether the described event took place in the past), rather than on structural/syntactic information.",2 Probing tasks,[0],[0]
"We recognize, however, that the boundary between syntactic and semantic tasks is somewhat arbitrary.
",2 Probing tasks,[0],[0]
"In the semantic odd man out (SOMO) task, we modified sentences by replacing a random noun
or verb o with another noun or verb r. To make the task more challenging, the bigrams formed by the replacement with the previous and following words in the sentence have frequencies that are comparable (on a log-scale) with those of the original bigrams.",2 Probing tasks,[0],[0]
"That is, if the original sentence contains bigrams wn−1o and own+1, the corresponding bigrams",2 Probing tasks,[0],[0]
wn−1r and rwn+1 in the modified sentence will have comparable corpus frequencies.,2 Probing tasks,[0],[0]
"No sentence is included in both original and modified format, and no replacement is repeated across train/dev/test sets.",2 Probing tasks,[0],[0]
The task of the classifier is to tell whether a sentence has been modified or not.,2 Probing tasks,[0],[0]
An example modified sentence is: “ No one could see this Hayes,2 Probing tasks,[0],[0]
and I wanted to know if it was real or a spoonful (orig.: ploy).”,2 Probing tasks,[0],[0]
"Note that judging plausibility of a syntactically well-formed sentence of this sort will often require grasping rather subtle semantic factors, ranging from selectional preference to topical coherence.
",2 Probing tasks,[0],[0]
The coordination inversion (CoordInv) benchmark contains sentences made of two coordinate clauses.,2 Probing tasks,[0],[0]
"In half of the sentences, we inverted the order of the clauses.",2 Probing tasks,[0],[0]
The task is to tell whether a sentence is intact or modified.,2 Probing tasks,[0],[0]
"Sentences are balanced in terms of clause length, and no sentence appears in both original and inverted versions.",2 Probing tasks,[0],[0]
"As an example, original “They might be only memories, but I can still feel each one” becomes: “I can still feel each one, but they might be only memories.”",2 Probing tasks,[0],[0]
"Often, addressing CoordInv requires an understanding of broad discourse and pragmatic factors.
",2 Probing tasks,[0],[0]
Row Hum.,2 Probing tasks,[0],[0]
Eval.,2 Probing tasks,[0],[0]
"of Table 2 reports humanvalidated “reasonable” upper bounds for all the tasks, estimated in different ways, depending on the tasks.",2 Probing tasks,[0],[0]
"For the surface ones, there is always a straightforward correct answer that a human annotator with enough time and patience could find.",2 Probing tasks,[0],[0]
The upper bound is thus estimated at 100%.,2 Probing tasks,[0],[0]
"The TreeDepth, TopConst, Tense, SubjNum and ObjNum tasks depend on automated PoS and parsing annotation.",2 Probing tasks,[0],[0]
"In these cases, the upper bound is given by the proportion of sentences correctly annotated by the automated procedure.",2 Probing tasks,[0],[0]
"To estimate this quantity, one linguistically-trained author checked the annotation of 200 randomly sampled test sentences from each task.",2 Probing tasks,[0],[0]
"Finally, the BShift, SOMO and CoordInv manipulations can accidentally generate acceptable sentences.",2 Probing tasks,[0],[0]
"For
example, one modified SOMO sentence is: “He pulled out the large round onion (orig.: cork) and saw the amber balm inside.”",2 Probing tasks,[0],[0]
", that is arguably not more anomalous than the original.",2 Probing tasks,[0],[0]
"For these tasks, we ran Amazon Mechanical Turk experiments in which subjects were asked to judge whether 1k randomly sampled test sentences were acceptable or not.",2 Probing tasks,[0],[0]
Reported human accuracies are based on majority voting.,2 Probing tasks,[0],[0]
See Appendix for details.,2 Probing tasks,[0],[0]
"In this section, we present the three sentence encoders that we consider and the seven tasks on which we train them.",3 Sentence embedding models,[0],[0]
A wide variety of neural networks encoding sentences into fixed-size representations exist.,3.1 Sentence encoder architectures,[0],[0]
"We focus here on three that have been shown to perform well on standard NLP tasks.
",3.1 Sentence encoder architectures,[0],[0]
BiLSTM-last/max,3.1 Sentence encoder architectures,[0],[0]
"For a sequence of T words {wt}t=1,...,T , a bidirectional LSTM computes a set of T vectors {ht}t.",3.1 Sentence encoder architectures,[0],[0]
For t ∈,3.1 Sentence encoder architectures,[0],[0]
"[1, . . .",3.1 Sentence encoder architectures,[0],[0]
", T ], ht is the concatenation of a forward LSTM and a backward LSTM that read the sentences in two opposite directions.",3.1 Sentence encoder architectures,[0],[0]
"We experiment with two ways of combining the varying number of (h1, . . .",3.1 Sentence encoder architectures,[0],[0]
", hT ) to form a fixed-size vector, either by selecting the last hidden state of hT or by selecting the maximum value over each dimension of the hidden units.",3.1 Sentence encoder architectures,[0],[0]
"The choice of these models are motivated by their demonstrated efficiency in seq2seq (Sutskever et al., 2014) and universal sentence representation learning (Conneau et al., 2017), respectively.2
Gated ConvNet We also consider the nonrecurrent convolutional equivalent of LSTMs, based on stacked gated temporal convolutions.",3.1 Sentence encoder architectures,[0],[0]
"Gated convolutional networks were shown to perform well as neural machine translation encoders (Gehring et al., 2017) and language modeling decoders (Dauphin et al., 2017).",3.1 Sentence encoder architectures,[0],[0]
"The encoder is composed of an input word embedding table that is augmented with positional encodings (Sukhbaatar et al., 2015), followed by a stack of temporal convolutions with small kernel size.",3.1 Sentence encoder architectures,[0],[0]
"The output of each convolutional layer is filtered by a gating mechanism, similar to the one of LSTMs.",3.1 Sentence encoder architectures,[0],[0]
"Finally,
2We also experimented with a unidirectional LSTM, with consistently poorer results.
",3.1 Sentence encoder architectures,[0],[0]
"max-pooling along the temporal dimension is performed on the output feature maps of the last convolution (Collobert and Weston, 2008).",3.1 Sentence encoder architectures,[0],[0]
"Seq2seq systems have shown strong results in machine translation (Zhou et al., 2016).",3.2 Training tasks,[0],[0]
"They consist of an encoder that encodes a source sentence into a fixed-size representation, and a decoder which acts as a conditional language model and that generates the target sentence.",3.2 Training tasks,[0],[0]
"We train Neural Machine Translation systems on three language pairs using about 2M sentences from the Europarl corpora (Koehn, 2005).",3.2 Training tasks,[0],[0]
"We pick English-French, which involves two similar languages, English-German, involving larger syntactic differences, and English-Finnish, a distant pair.",3.2 Training tasks,[0],[0]
"We also train with an AutoEncoder objective (Socher et al., 2011) on Europarl source English sentences.",3.2 Training tasks,[0],[0]
"Following Vinyals et al. (2015), we train a seq2seq architecture to generate linearized grammatical parse trees (see Table 1) from source sentences (Seq2Tree).",3.2 Training tasks,[0],[0]
We use the Stanford parser to generate trees for Europarl source English sentences.,3.2 Training tasks,[0],[0]
"We train SkipThought vectors (Kiros et al., 2015) by predicting the next sentence given the current one (Tang et al., 2017), on 30M sentences from the Toronto Book Corpus, excluding those in the probing sets.",3.2 Training tasks,[0],[0]
"Finally, following Conneau et al. (2017), we train sentence encoders on Natural Language Inference using the concatenation of the SNLI (Bowman et al., 2015) and MultiNLI (Bowman et al., 2015) data sets (about 1M sentence pairs).",3.2 Training tasks,[0],[0]
"In this task, a sentence encoder is trained to encode two sentences, which are fed to a classifier and whose role is to distinguish whether the sentences are contradictory, neutral or entailed.",3.2 Training tasks,[0],[0]
"Finally, as in Conneau et al. (2017), we also include Untrained encoders with random weights, which act as random projections of pre-trained word embeddings.",3.2 Training tasks,[0],[0]
"BiLSTM encoders use 2 layers of 512 hidden units (∼4M parameters), Gated ConvNet has 8 convolutional layers of 512 hidden units, kernel size 3 (∼12M parameters).",3.3 Training details,[0],[0]
"We use pre-trained fastText word embeddings of size 300 (Mikolov et al., 2018) without fine-tuning, to isolate the impact of encoder architectures and to handle words outside the training sets.",3.3 Training details,[0],[0]
Training task performance and further details are in Appendix.,3.3 Training details,[0],[0]
Baselines Baseline and human-bound performance are reported in the top block of Table 2.,4 Probing task experiments,[0],[0]
Length is a linear classifier with sentence length as sole feature.,4 Probing task experiments,[0],[0]
"NB-uni-tfidf is a Naive Bayes classifier using words’ tfidf scores as features, NBbi-tfidf its extension to bigrams.",4 Probing task experiments,[0],[0]
"Finally, BoVfastText derives sentence representations by averaging the fastText embeddings of the words they contain (same embeddings used as input to the encoders).3
Except, trivially, for Length on SentLen and the NB baselines on WC, there is a healthy gap between top baseline performance and human upper bounds.",4 Probing task experiments,[0],[0]
NB-uni-tfidf evaluates to what extent our tasks can be addressed solely based on knowledge about the distribution of words in the training sentences.,4 Probing task experiments,[0],[0]
"Words are of course to some extent informative for most tasks, leading to relatively high performance in Tense, SubjNum and ObjNum.",4 Probing task experiments,[0],[0]
"Recall that the words containing the probed features are disjoint between train and test partitions, so we are not observing a confound here, but rather the effect of the redundancies one expects in natural language data.",4 Probing task experiments,[0],[0]
"For example, for Tense, since sentences often contain more than one verb in the same tense, NB-uni-tfidf can exploit nontarget verbs as cues: the NB features most associated to the past class are verbs in the past tense (e.g “sensed”, “lied”, “announced”), and similarly for present (e.g “uses”, “chuckles”, “frowns”).",4 Probing task experiments,[0],[0]
"Using bigram features (NB-bi-tfidf) brings in general little or no improvement with respect to the unigram baseline, except, trivially, for the BShift
3Similar results are obtained summing embeddings, and using GloVe embeddings (Pennington et al., 2014).
task, where NB-bi-tfidf can easily detect unlikely bigrams.",4 Probing task experiments,[0],[0]
"NB-bi-tfidf has below-random performance on SOMO, confirming that the semantic intruder is not given away by superficial bigram cues.
",4 Probing task experiments,[0],[0]
"Our first striking result is the good overall performance of Bag-of-Vectors, confirming early insights that aggregated word embeddings capture surprising amounts of sentence information (Pham et al., 2015; Arora et al., 2017; Adi et al., 2017).",4 Probing task experiments,[0],[0]
BoV’s good WC and SentLen performance was already established by Adi et al. (2017).,4 Probing task experiments,[0],[0]
"Not surprisingly, word-order-unaware BoV performs randomly in BShift and in the more sophisticated semantic tasks SOMO and CoordInv.",4 Probing task experiments,[0],[0]
"More interestingly, BoV is very good at the Tense, SubjNum, ObjNum, and TopConst tasks (much better than the word-based baselines), and well above chance in TreeDepth.",4 Probing task experiments,[0],[0]
"The good performance on Tense, SubjNum and ObjNum has a straightforward explanation we have already hinted at above.",4 Probing task experiments,[0],[0]
"Many sentences are naturally “redundant”, in the sense that most tensed verbs in a sentence are in the same tense, and similarly for number in nouns.",4 Probing task experiments,[0],[0]
"In 95.2% Tense, 75.9% SubjNum and 78.7% ObjNum test sentences, the target tense/number feature is also the majority one for the whole sentence.",4 Probing task experiments,[0],[0]
"Word embeddings capture features such as number and tense (Mikolov et al., 2013), so aggregated word embeddings will naturally track these features’ majority values in a sentence.",4 Probing task experiments,[0],[0]
BoV’s TopConst and TreeDepth performance is more surprising.,4 Probing task experiments,[0],[0]
"Accuracy is well above NB, showing that BoV is exploiting cues beyond specific words strongly associated to the target classes.",4 Probing task experiments,[0],[0]
"We conjecture that more abstract word features captured
Task SentLen WC TreeDepth TopConst",4 Probing task experiments,[0],[0]
BShift,4 Probing task experiments,[0],[0]
Tense SubjNum ObjNum SOMO CoordInv Baseline representations Majority vote 20.0 0.5 17.9 5.0 50.0 50.0 50.0 50.0 50.0 50.0 Hum.,4 Probing task experiments,[0],[0]
Eval.,4 Probing task experiments,[0],[0]
"100 100 84.0 84.0 98.0 85.0 88.0 86.5 81.2 85.0 Length 100 0.2 18.1 9.3 50.6 56.5 50.3 50.1 50.2 50.0 NB-uni-tfidf 22.7 97.8 24.1 41.9 49.5 77.7 68.9 64.0 38.0 50.5 NB-bi-tfidf 23.0 95.0 24.6 53.0 63.8 75.9 69.1 65.4 39.9 55.7 BoV-fastText 66.6 91.6 37.1 68.1 50.8 89.1 82.1 79.8 54.2 54.8
BiLSTM-last encoder Untrained 36.7 43.8 28.5 76.3 49.8 84.9 84.7 74.7 51.1 64.3 AutoEncoder 99.3 23.3 35.6 78.2 62.0 84.3 84.7 82.1 49.9 65.1 NMT En-Fr 83.5 55.6 42.4 81.6 62.3 88.1 89.7 89.5 52.0 71.2 NMT En-De 83.8 53.1 42.1 81.8 60.6 88.6 89.3 87.3 51.5 71.3 NMT En-Fi 82.4 52.6 40.8 81.3 58.8 88.4 86.8 85.3 52.1 71.0 Seq2Tree 94.0 14.0 59.6 89.4 78.6 89.9 94.4 94.7 49.6 67.8 SkipThought 68.1 35.9 33.5 75.4 60.1 89.1 80.5 77.1 55.6 67.7 NLI 75.9 47.3 32.7 70.5 54.5 79.7 79.3 71.3 53.3 66.5
BiLSTM-max encoder Untrained 73.3 88.8 46.2 71.8 70.6 89.2 85.8 81.9 73.3 68.3 AutoEncoder 99.1 17.5 45.5 74.9 71.9 86.4 87.0 83.5 73.4 71.7 NMT En-Fr 80.1 58.3 51.7 81.9 73.7 89.5 90.3 89.1 73.2 75.4 NMT En-De 79.9 56.0 52.3 82.2 72.1 90.5 90.9 89.5 73.4 76.2 NMT En-Fi 78.5 58.3 50.9 82.5 71.7 90.0 90.3 88.0 73.2 75.4 Seq2Tree 93.3 10.3 63.8 89.6 82.1 90.9 95.1 95.1 73.2 71.9 SkipThought 66.0 35.7 44.6 72.5 73.8 90.3 85.0 80.6 73.6 71.0 NLI 71.7 87.3 41.6 70.5 65.1 86.7 80.7 80.3 62.1 66.8
GatedConvNet encoder Untrained 90.3 17.1 30.3 47.5 62.0 78.2 72.2 70.9 61.4 59.6 AutoEncoder 99.4 16.8 46.3 75.2 71.9 87.7 88.5 86.5 73.5 72.4 NMT En-Fr 84.8 41.3 44.6 77.6 67.9 87.9 88.8 86.6 66.1 72.0 NMT En-De 89.6 49.0 50.5 81.7 72.3 90.4 91.4 89.7 72.8 75.1 NMT En-Fi 89.3 51.5 49.6 81.8 70.9 90.4 90.9 89.4 72.4 75.1 Seq2Tree 96.5 8.7 62.0 88.9 83.6 91.5 94.5 94.3 73.5 73.8 SkipThought 79.1 48.4 45.7 79.2 73.4 90.7 86.6 81.7 72.4 72.3 NLI 73.8 29.2 43.2 63.9 70.7 81.3 77.5 74.4 73.3 71.0
Table 2: Probing task accuracies.",4 Probing task experiments,[0],[0]
"Classification performed by a MLP with sigmoid nonlinearity, taking pre-learned sentence embeddings as input (see Appendix for details and logistic regression results).
",4 Probing task experiments,[0],[0]
by the embeddings (such as the part of speech of a word) might signal different syntactic structures.,4 Probing task experiments,[0],[0]
"For example, sentences in the “WHNP SQ .”",4 Probing task experiments,[0],[0]
"top constituent class (e.g., “How long before you leave us again?”) must contain a wh word, and will often feature an auxiliary or modal verb.",4 Probing task experiments,[0],[0]
"BoV can rely on this information to noisily predict the correct class.
",4 Probing task experiments,[0],[0]
"Encoding architectures Comfortingly, proper encoding architectures clearly outperform BoV.",4 Probing task experiments,[0],[0]
"An interesting observation in Table 2 is that different encoder architectures trained with the same objective, and achieving similar performance on the training task,4 can lead to linguistically different embeddings, as indicated by the probing tasks.",4 Probing task experiments,[0],[0]
"Coherently with the findings of Conneau et al. (2017) for the downstream tasks, this sug-
4See Appendix for details on training task performance.
",4 Probing task experiments,[0],[0]
gests that the prior imposed by the encoder architecture strongly preconditions the nature of the embeddings.,4 Probing task experiments,[0],[0]
"Complementing recent evidence that convolutional architectures are on a par with recurrent ones in seq2seq tasks (Gehring et al., 2017), we find that Gated ConvNet’s overall probing task performance is comparable to that of the best LSTM architecture (although, as shown in Appendix, the LSTM has a slight edge on downstream tasks).",4 Probing task experiments,[0],[0]
We also replicate the finding of Conneau et al. (2017) that BiLSTM-max outperforms BiLSTM-last both in the downstream tasks (see Appendix) and in the probing tasks (Table 2).,4 Probing task experiments,[0],[0]
"Interestingly, the latter only outperforms the former in SentLen, a task that captures a superficial aspect of sentences (how many words they contain), that could get in the way of inducing more useful linguistic knowledge.
",4 Probing task experiments,[0],[0]
"Training tasks We focus next on how different training tasks affect BiLSTM-max, but the patterns are generally representative across architectures.",4 Probing task experiments,[0],[0]
"NMT training leads to encoders that are more linguistically aware than those trained on the NLI data set, despite the fact that we confirm the finding of Conneau and colleagues that NLI is best for downstream tasks (Appendix).",4 Probing task experiments,[0],[0]
"Perhaps, NMT captures richer linguistic features useful for the probing tasks, whereas shallower or more adhoc features might help more in our current downstream tasks.",4 Probing task experiments,[0],[0]
"Suggestively, the one task where NLI clearly outperforms NMT is WC.",4 Probing task experiments,[0],[0]
"Thus, NLI training is better at preserving shallower word features that might be more useful in downstream tasks (cf.",4 Probing task experiments,[0],[0]
"Figure 2 and discussion there).
",4 Probing task experiments,[0],[0]
"Unsupervised training (SkipThought and AutoEncoder) is not on a par with supervised tasks, but still effective.",4 Probing task experiments,[0],[0]
"AutoEncoder training leads, unsurprisingly, to a model excelling at SentLen, but it attains low performance in the WC prediction task.",4 Probing task experiments,[0],[0]
"This curious result might indicate that the latter information is stored in the embeddings in a complex way, not easily readable by our MLP.",4 Probing task experiments,[0],[0]
"At the other end, Seq2Tree is trained to predict annotation from the same parser we used to create some of the probing tasks.",4 Probing task experiments,[0],[0]
"Thus, its high performance on TopConst, Tense, SubjNum, ObjNum and TreeDepth is probably an artifact.",4 Probing task experiments,[0],[0]
"Indeed, for most of these tasks, Seq2Tree performance is above the human bound, that is, Seq2Tree learned to mimic the parser errors in our benchmarks.",4 Probing task experiments,[0],[0]
"For the more challenging SOMO and CoordInv tasks, that only indirectly rely on tagging/parsing information, Seq2Tree is comparable to NMT, that does not use explicit syntactic information.
",4 Probing task experiments,[0],[0]
"Perhaps most interestingly, BiLSTM-max already achieves very good performance without any training (Untrained row in Table 2).",4 Probing task experiments,[0],[0]
Untrained BiLSTM-max also performs quite well in the downstream tasks (Appendix).,4 Probing task experiments,[0],[0]
This architecture must encode priors that are intrinsically good for sentence representations.,4 Probing task experiments,[0],[0]
"Untrained BiLSTM-max exploits the input fastText embeddings, and multiplying the latter by a random recurrent matrix provides a form of positional encoding.",4 Probing task experiments,[0],[0]
"However, good performance in a task such as SOMO, where BoV fails and positional information alone should not help (the intruder is randomly distributed across the sentence), suggests that other architectural biases are at work.",4 Probing task experiments,[0],[0]
"In-
triguingly, a preliminary comparison of untrained BiLSTM-max and human subjects on the SOMO sentences evaluated by both reveals that, whereas humans have a bias towards finding sentences acceptable (62% sentences are rated as untampered with, vs. 48% ground-truth proportion), the model has a strong bias in the opposite direction (it rates 83% of the sentences as modified).",4 Probing task experiments,[0],[0]
"A cursory look at contrasting errors confirms, unsurprisingly, that those made by humans are perfectly justified, while model errors are opaque.",4 Probing task experiments,[0],[0]
"For example, the sentence “I didn’t come here to reunite",4 Probing task experiments,[0],[0]
"(orig. undermine) you” seems perfectly acceptable in its modified form, and indeed subjects judged it as such, whereas untrained BiLSTM-max “correctly” rated it as a modified item.",4 Probing task experiments,[0],[0]
"Conversely, it is difficult to see any clear reason for the latter tendency to rate perfectly acceptable originals as modified.",4 Probing task experiments,[0],[0]
We leave a more thorough investigation to further work.,4 Probing task experiments,[0],[0]
"See similar observations on the effectiveness of untrained ConvNets in vision by Ulyanov et al. (2017).
",4 Probing task experiments,[0],[0]
"Probing task comparison A good encoder, such as NMT-trained BiLSTM-max, shows generally good performance across probing tasks.",4 Probing task experiments,[0],[0]
"At one extreme, performance is not particularly high on the surface tasks, which might be an indirect sign of the encoder extracting “deeper” linguistic properties.",4 Probing task experiments,[0],[0]
"At the other end, performance is still far from the human bounds on TreeDepth, BShift, SOMO and CoordInv.",4 Probing task experiments,[0],[0]
The last 3 tasks ask if a sentence is syntactically or semantically anomalous.,4 Probing task experiments,[0],[0]
"This is a daunting job for an encoder that has not been explicitly trained on acceptability, and it is interesting that the best models are, at least to a certain extent, able to produce reasonable anomaly judgments.",4 Probing task experiments,[0],[0]
The asymmetry between the difficult TreeDepth and easier TopConst is also interesting.,4 Probing task experiments,[0],[0]
"Intuitively, TreeDepth requires more nuanced syntactic information (down to the deepest leaf of the tree) than TopConst, that only requires identifying broad chunks.
",4 Probing task experiments,[0],[0]
Figure 1 reports how probing task accuracy changes in function of encoder training epochs.,4 Probing task experiments,[0],[0]
"The figure shows that NMT probing performance is largely independent of target language, with strikingly similar development patterns across French, German and Finnish.",4 Probing task experiments,[0],[0]
"Note in particular the similar probing accuracy curves in French and Finnish, while the corresponding BLEU scores (in lavender) are consistently higher in the former lan-
guage.",4 Probing task experiments,[0],[0]
"For both NMT and SkipThought, WC performance keeps increasing with epochs.",4 Probing task experiments,[0],[0]
"For the other tasks, we observe instead an early flattening of the NMT probing curves, while BLEU performance keeps increasing.",4 Probing task experiments,[0],[0]
"Most strikingly, SentLen performance is actually decreasing, suggesting again that, as a model captures deeper linguistic properties, it will tend to forget about this superficial feature.",4 Probing task experiments,[0],[0]
"Finally, for the challenging SOMO task, the curves are mostly flat, suggesting that what BiLSTM-max is able to capture about this task is already encoded in its architecture, and further training doesn’t help much.
",4 Probing task experiments,[0],[0]
"Probing vs. downstream tasks Figure 2 reports correlation between performance on our probing tasks and the downstream tasks available in the SentEval5 suite, which consists of classification (MR, CR, SUBJ, MPQA, SST2, SST5, TREC), natural language inference (SICK-E), semantic relatedness (SICK-R, STSB), paraphrase detection (MRPC) and semantic textual similarity (STS 2012 to 2017) tasks.",4 Probing task experiments,[0],[0]
"Strikingly, WC is significantly positively correlated with all downstream tasks.",4 Probing task experiments,[0],[0]
"This suggests that, at least for current models, the latter do not require extracting particularly abstract knowledge from the data.",4 Probing task experiments,[0],[0]
"Just relying on the words contained in the input sentences
5https://github.com/facebookresearch/ SentEval
can get you a long way.",4 Probing task experiments,[0],[0]
"Conversely, there is a significant negative correlation between SentLen and most downstream tasks.",4 Probing task experiments,[0],[0]
The number of words in a sentence is not informative about its linguistic contents.,4 Probing task experiments,[0],[0]
"The more models abstract away from such information, the more likely it is they will use their capacity to capture more interesting features, as the decrease of the SentLen curve along training (see Figure 1) also suggests.",4 Probing task experiments,[0],[0]
"CoordInv and, especially, SOMO, the tasks requiring the most sophisticated semantic knowledge, are those that positively correlate with the largest number of downstream tasks after WC.",4 Probing task experiments,[0],[0]
"We observe intriguing asymmetries: SOMO correlates with the SICK-E sentence entailment test, but not with SICK-R, which is about modeling sentence relatedness intuitions.",4 Probing task experiments,[0],[0]
"Indeed, logical entailment requires deeper semantic analysis than modeling similarity judgments.",4 Probing task experiments,[0],[0]
"TopConst and the number tasks negatively correlate with various similarity and sentiment data sets (SST, STS, SICK-R).",4 Probing task experiments,[0],[0]
"This might expose biases in these tasks: SICK-R, for example, deliberately contains sentence pairs with opposite voice, that will have different constituent structure but equal meaning (Marelli et al., 2014).",4 Probing task experiments,[0],[0]
"It might also mirrors genuine factors affecting similarity judgments (e.g., two sentences differing only in object number are very similar).",4 Probing task experiments,[0],[0]
"Remarkably, TREC question type classification is the downstream task correlating with most probing tasks.",4 Probing task experiments,[0],[0]
"Question classification is certainly an outlier among our downstream tasks, but we must leave a full understanding of this behaviour to future work (this is exactly the sort of analysis our probing tasks should stimulate).",4 Probing task experiments,[0],[0]
"Adi et al. (2017) introduced SentLen, WC and a word order test, focusing on a bag-of-vectors baseline, an autoencoder and skip-thought (all trained on the same data used for the probing tasks).",5 Related work,[0],[0]
"We recast their tasks so that they only require a sentence embedding as input (two of their tasks also require word embeddings, polluting sentencelevel evaluation), we extend the evaluation to more tasks, encoders and training objectives, and we relate performance on the probing tasks with that on downstream tasks.",5 Related work,[0],[0]
"Shi et al. (2016) also use 3 probing tasks, including Tense and TopConst.",5 Related work,[0],[0]
"It is not clear that they controlled for the same factors we considered (in particular, lexical overlap and
sentence length), and they use much smaller training sets, limiting classifier-based evaluation to logistic regression.",5 Related work,[0],[0]
"Moreover, they test a smaller set of models, focusing on machine translation.
",5 Related work,[0],[0]
"Belinkov et al. (2017a), Belinkov et al. (2017b) and Dalvi et al. (2017) are also interested in understanding the type of linguistic knowledge encoded in sentence and word embeddings, but their focus is on word-level morphosyntax and lexical semantics, and specifically on NMT encoders and decoders.",5 Related work,[0],[0]
"Sennrich (2017) also focuses on NMT systems, and proposes a contrastive test to assess how they handle various linguistic phenomena.",5 Related work,[0],[0]
"Other work explores the linguistic behaviour of recurrent networks and related models by using visualization, input/hidden representation deletion techniques or by looking at the word-by-word behaviour of the network (e.g., Nagamine et al., 2015; Hupkes et al., 2017; Li et al., 2016; Linzen et al., 2016; Kàdàr et al., 2017; Li et al., 2017).",5 Related work,[0],[0]
"These methods, complementary to ours, are not agnostic to encoder architecture, and cannot be used for general-purpose cross-model evaluation.
",5 Related work,[0],[0]
"Finally, Conneau et al. (2017) propose a largescale, multi-task evaluation of sentence embeddings, focusing entirely on downstream tasks.",5 Related work,[0],[0]
We introduced a set of tasks probing the linguistic knowledge of sentence embedding methods.,6 Conclusion,[0],[0]
"Their purpose is not to encourage the development of ad-hoc models that attain top performance on them, but to help exploring what information is
captured by different pre-trained encoders.",6 Conclusion,[0],[0]
We performed an extensive linguistic evaluation of modern sentence encoders.,6 Conclusion,[0],[0]
"Our results suggest that the encoders are capturing a wide range of properties, well above those captured by a set of strong baselines.",6 Conclusion,[0],[0]
"We further uncovered interesting patterns of correlation between the probing tasks and more complex “downstream” tasks, and presented a set of intriguing findings about the linguistic properties of various embedding methods.",6 Conclusion,[0],[0]
"For example, we found that Bag-of-Vectors is surprisingly good at capturing sentence-level properties, thanks to redundancies in natural linguistic input.",6 Conclusion,[0],[0]
"We showed that different encoder architectures trained with the same objective with similar performance can result in different embeddings, pointing out the importance of the architecture prior for sentence embeddings.",6 Conclusion,[0.9518504141218529],"['We use the following feature types in our experiments: Intention Distribution –We include six features encoding the distribution over intentional classes of the target word in training data, as some words use only several functions and some more predominantly than others.']"
"In particular, we found that BiLSTM-max embeddings are already capturing interesting linguistic knowledge before training, and that, after training, they detect semantic acceptability without having been exposed to anomalous sentences before.",6 Conclusion,[0],[0]
"We hope that our publicly available probing task set will become a standard benchmarking tool of the linguistic properties of new encoders, and that it will stir research towards a better understanding of what they learn.
",6 Conclusion,[0],[0]
"In future work, we would like to extend the probing tasks to other languages (which should be relatively easy, given that they are automatically generated), investigate how multi-task training affects probing task performance and leverage our probing tasks to find more linguistically-aware universal encoders.",6 Conclusion,[0],[0]
"We thank David Lopez-Paz, Holger Schwenk, Hervé Jégou, Marc’Aurelio Ranzato and Douwe Kiela for useful comments and discussions.",Acknowledgments,[0],[0]
"Although much effort has recently been devoted to training high-quality sentence embeddings, we still have a poor understanding of what they are capturing.",abstractText,[0],[0]
"“Downstream” tasks, often based on sentence classification, are commonly used to evaluate the quality of sentence representations.",abstractText,[0],[0]
The complexity of the tasks makes it however difficult to infer what kind of information is present in the representations.,abstractText,[0],[0]
"We introduce here 10 probing tasks designed to capture simple linguistic features of sentences, and we use them to study embeddings generated by three different encoders trained in eight distinct ways, uncovering intriguing properties of both encoders and training methods.",abstractText,[0],[0]
What you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2032–2037, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
There is much interest in automatic recognition of demographic information of Internet users to improve the quality of online interactions.,1 Introduction,[0],[0]
"Researchers have looked into identifying a variety of factors about users, including age, gender, language, religious beliefs and political views.",1 Introduction,[0],[0]
"Most work leverages multiple sources of information, such as search query history, Twitter feeds, Facebook likes, social network links, and user profiles.",1 Introduction,[0],[0]
"However, in many situations, little of this information is available.",1 Introduction,[0],[0]
"Conversely, usernames are almost always available.
",1 Introduction,[0],[0]
"In this work, we look specifically at classifying gender and language based only on the username.",1 Introduction,[0],[0]
"Prior work by sociologists has established a link between usernames and gender (Cornetto and Nowak, 2006), and studies have linked usernames to other attributes, such as individual beliefs (Crabill, 2007; Hassa, 2012) and shown how usernames shape perceptions of gender and ethnicity in the absence of common nonverbal cues (Pelletier, 2014).",1 Introduction,[0.953201959582137],"['Research in linguistics and psychology has identified several types of usage for vulgar words (Andersson and Trudgill, 1990; Pinker, 2007; Wang, 2013).']"
"The connections to ethnicity motivate the exploration of language identification.
",1 Introduction,[0],[0]
"Gender identification based on given names is very effective for English (Liu and Ruths, 2013), since many names are strongly associated with a
particular gender, like “Emily” or “Mark”.",1 Introduction,[0],[0]
"Unfortunately, the requirement that each username be unique precludes use of given names alone.",1 Introduction,[0],[0]
"Instead, usernames are typically a combination of component words, names and numbers.",1 Introduction,[0],[0]
"For example, the Twitter name @taylorswift13 might decompose into “taylor”, “swift” and “13”.",1 Introduction,[0],[0]
"The sub-units carry meaning and, importantly, they are shared with many other individuals.",1 Introduction,[0],[0]
"Thus, our approach is to leverage automatic decomposition of usernames into sub-units for use in classification.
",1 Introduction,[0],[0]
"We use the Morfessor algorithm (Creutz and Lagus, 2006; Virpioja et al., 2013) for unsupervised morphology induction to learn the decomposition of the usernames into sub-units.",1 Introduction,[0],[0]
"Morfessor has been used successfully in a variety of language modeling frameworks applied to a number of languages, particularly for learning concatenative morphological structure.",1 Introduction,[0],[0]
"The usernames that we analyze are a good match to the Morfessor framework, which allows us to push the boundary of how much can be done with only a username.
",1 Introduction,[0],[0]
"The classifier design is described in the next section, followed by a description of experiments on gender and language recognition that demonstrate the utility of morph-based features compared to character n-gram features.",1 Introduction,[0],[0]
The paper closes with a discussion of related work and a summary of key findings.,1 Introduction,[0],[0]
"In linguistics, a morpheme is the “minimal linguistic unit with lexical or grammatical meaning” (Booij, 2012).",2.1 Unsupervised Morphology Learning,[0],[0]
Morphemes are combined in various ways to create longer words.,2.1 Unsupervised Morphology Learning,[0],[0]
"Similarly, usernames are frequently made up of a concatenated sequence of smaller units.",2.1 Unsupervised Morphology Learning,[0],[0]
"These sub-units will be referred to as u-morphs to highlight the fact that they play an analogous role to morphemes but for
2032
purposes of encoding usernames rather than standard words in a language.",2.1 Unsupervised Morphology Learning,[0],[0]
"The u-morphs are subunits that are small enough to be shared across different usernames but retain some meaning.
",2.1 Unsupervised Morphology Learning,[0],[0]
"Unsupervised morphology induction using Morfessor (Creutz and Lagus, 2006) is based on a minimum description length (MDL) objective, which balances two competing goals: maximizing both the likelihood of the data and of the model.",2.1 Unsupervised Morphology Learning,[0],[0]
The likelihood of the data is maximized by longer tokens and a bigger lexicon whereas the likelihood of the model is maximized by a smaller lexicon with shorter tokens.,2.1 Unsupervised Morphology Learning,[0],[0]
"A parameter controls the trade-off between the two parts of the objective function, which alters the average u-morph length.",2.1 Unsupervised Morphology Learning,[0],[0]
"We tune this parameter on held-out data to optimize the classification performance of the demographic tasks.
",2.1 Unsupervised Morphology Learning,[0],[0]
Maximizing the Morfessor objective exactly is computationally intractable.,2.1 Unsupervised Morphology Learning,[0],[0]
The Morfessor algorithm searches for the optimal lexicon using an iterative approach.,2.1 Unsupervised Morphology Learning,[0],[0]
"First, the highest probability decomposition for each training token is found given the current model.",2.1 Unsupervised Morphology Learning,[0],[0]
"Then, the model is updated with the counts of the u-morphs.",2.1 Unsupervised Morphology Learning,[0],[0]
"A umorph is added to the lexicon when it increases the weighted likelihood of the data by more than the cost of increasing the size of the lexicon.
",2.1 Unsupervised Morphology Learning,[0],[0]
"Usernames can be mixed-case, e.g. “JohnDoe”.",2.1 Unsupervised Morphology Learning,[0],[0]
"The case change gives information about a likely u-morph boundary, but at the cost of doubling the size of the character set.",2.1 Unsupervised Morphology Learning,[0],[0]
"To more effectively leverage this cue, all characters are made lowercase but each change from lower to uppercase is marked with a special token, e.g. “john$doe”.",2.1 Unsupervised Morphology Learning,[0],[0]
"Using this encoding reduces the u-morph inventory size, and we found it to give slightly better results in language identification.
",2.1 Unsupervised Morphology Learning,[0],[0]
Character 3-grams and 4-grams are used as baseline features.,2.1 Unsupervised Morphology Learning,[0],[0]
Before extracting the n-grams a “#” token is placed at the start and end of each username.,2.1 Unsupervised Morphology Learning,[0],[0]
The n-grams are overlapping to give them the best chance of finding a semantically meaningful sub-unit.,2.1 Unsupervised Morphology Learning,[0],[0]
"Given a decomposition of the username into a sequence of u-morphs (or character n-grams), we represent the relationship between the observed features and each class with a unigram language model.",2.2 Classifier Design,[0],[0]
"If a username u has decomposition
m1, . . .",2.2 Classifier Design,[0],[0]
",mn then it is assigned to the class ci for which the unigram model gives it the highest posterior probability, or equivalently:
argmaxi pC(ci) n∏
k=1
p(mk|ci),
where pC(ci) is the class prior and p(mk|ci) is the class-dependent unigram.1
For some demographics, the class prior can be very skewed, as in the case of language detection where English is the dominant language.",2.2 Classifier Design,[0],[0]
"The choice of smoothing algorithm can be important in such cases, since minority classes have much less training data for estimating the language model and benefit from having more probability mass assigned to unseen words.",2.2 Classifier Design,[0],[0]
"Here, we follow the approach proposed in (Frank and Bouckaert, 2006) that normalizes the token count vectors for each class to have the same L1 norm, specifically:
p(mk|ci) = 1 Z
( 1 +
β · n(mk, ci) n(ci)
) ,
where n(·) indicates counts and β controls the strength of the smoothing.",2.2 Classifier Design,[0],[0]
Setting β equal to the number of training examples approximately matches the strength of the smoothing to the addone-smoothing algorithm.,2.2 Classifier Design,[0],[0]
"Z = β + |M | is a constant to make the probabilities sum to one.
",2.2 Classifier Design,[0],[0]
Only a small portion of usernames on the Internet come with gender labels.,2.2 Classifier Design,[0],[0]
"In these situations, semi-supervised learning algorithms can use the unlabeled data to improve the performance of the classifier.",2.2 Classifier Design,[0],[0]
"We use a self-training expectationmaximization (EM) algorithm similar to that described in (Nigam et al., 2000).",2.2 Classifier Design,[0],[0]
The algorithm first learns a classifier on the labeled data.,2.2 Classifier Design,[0],[0]
"In the E-step, the classifier assigns probabilistic labels to the unlabeled data.",2.2 Classifier Design,[0],[0]
"In the M-step, the labeled data and the probabilistic labels are combined to learn a new classifier.",2.2 Classifier Design,[0],[0]
"These steps are iterated until convergence, which usually requires three iterations for our tasks.
",2.2 Classifier Design,[0],[0]
"1Note that the unigram model used here, which considers only the observed u-morphs or n-grams, is not the same as using a Naive Bayes (NB) classifier based on a vector of u-morph counts.",2.2 Classifier Design,[0],[0]
"In the former, unobserved u-morphs do not impact the class-dependent probability, whereas the zero counts do impact the probability for the NB classifier.",2.2 Classifier Design,[0],[0]
"Since the vast majority of possible u-morphs are unobserved in a username, it is better to base the decision only on the observed u-morphs.",2.2 Classifier Design,[0],[0]
"The n-gram model is actually a unigram with an n-gram “vocabulary” rather than an n-gram language model.
",2.2 Classifier Design,[0],[0]
Nigam et al. (2000) call their method EM-λ because it uses a parameter λ to reduce the weight of the unlabeled examples relative to the labeled data.,2.2 Classifier Design,[0],[0]
This is important because the independence assumptions of the unigram model lead to overconfident predictions.,2.2 Classifier Design,[0],[0]
We used another method that directly corrects the estimated posterior probabilities.,2.2 Classifier Design,[0],[0]
"Using a small validation set, we binned the probability estimates and calculated the true class probability for each bin.",2.2 Classifier Design,[0],[0]
The EM algorithm used the corrected probabilities for each bin for the unlabeled data during the maximization step.,2.2 Classifier Design,[0],[0]
Samples with a prediction confidence of less than 60% are not used for training.,2.2 Classifier Design,[0],[0]
"Data was collected from the OkCupid dating site by downloading up to 1,000 profiles from 27 cities in the United States, first for men seeking women and again for women seeking men to obtain a balanced set of 44,000 usernames.",3.1 Gender Identification,[0],[0]
The data is partitioned into three sets with 80% assigned to training and 10% each to validation and test.,3.1 Gender Identification,[0],[0]
"We also use 3.5M usernames from the photo messaging app Snapchat (McCormick, 2014): 1.5M are used for u-morph learning and 2M are for self-training.",3.1 Gender Identification,[0],[0]
"All names in this task used only lower case, due to the nature of the available data.
",3.1 Gender Identification,[0],[0]
The top features ranked by likelihood ratios are given in Table 1.,3.1 Gender Identification,[0],[0]
"The u-morphs clearly carry semantic meaning, and the trigram features appear to be substrings of the top u-morph features.",3.1 Gender Identification,[0],[0]
The trigram features have an advantage when the u-morphs are under-segmented such as if the u-morph “niceguy” or “thatguy” is included in the lexicon.,3.1 Gender Identification,[0],[0]
"Conversely, the n-grams can suffer from over-segmentation.",3.1 Gender Identification,[0],[0]
"For example, the trigram “guy” is inside the surname “Nguyen” even though it is better to ignore that substring in this context.",3.1 Gender Identification,[0],[0]
"Many other tokens suffer from this problem, e.g. “miss” is in “mission”.
",3.1 Gender Identification,[0],[0]
The variable-length u-morphs are longer on average than the character n-grams (4.9 characters).,3.1 Gender Identification,[0],[0]
"The u-morph inventory size is similar to that for 3-grams but 5-10 times smaller than the 4-gram inventory, depending on the amount of data used since the inventory is expanded in semi-supervised training.",3.1 Gender Identification,[0],[0]
"By using the MDL criterion in unsupervised morphology learning, the u-morphs provide a more efficient representation of usernames than n-grams and make it easier to control the tradeoff between vocabulary size and average segment length.",3.1 Gender Identification,[0],[0]
"The smaller inventory is less sensitive to sparse data in language model training.
",3.1 Gender Identification,[0],[0]
The experiment results are presented in Table 2.,3.1 Gender Identification,[0],[0]
"For the supervised learning method, the character 3-gram and 4-gram features give equivalent performance, and the u-morph features give the lowest error rate by a small amount (3% relative).",3.1 Gender Identification,[0],[0]
"More significantly, the character n-gram systems do not benefit from semi-supervised learning, but the u-morph features do.",3.1 Gender Identification,[0],[0]
"The semi-supervised u-morph features obtain an error rate of 25.8%, which represents a 10% relative reduction over the baseline character n-gram results.",3.1 Gender Identification,[0],[0]
This experiment takes usernames from the Twitter streaming API.,3.2 Language Identification on Twitter,[0],[0]
"Each username is associated with a tweet, for which the Twitter API identifies a language.",3.2 Language Identification on Twitter,[0],[0]
"The language labels are noisy, so we remove approximately 35% of the tweets where the Twitter API does not agree with the langid.py classifier (Lui and Baldwin, 2012).",3.2 Language Identification on Twitter,[0],[0]
Both training and test sets are restricted to the nine languages that comprise at least 1% of the training set.,3.2 Language Identification on Twitter,[0],[0]
These languages cover 96% of the observed tweets (see Table 4).,3.2 Language Identification on Twitter,[0],[0]
"About 110,000 usernames were reserved for testing and 430,000 were used for training both u-morphs and the classifier.",3.2 Language Identification on Twitter,[0],[0]
Semi-supervised methods are not used because of the abundant labeled data.,3.2 Language Identification on Twitter,[0],[0]
"For each language, we train a one-vs.all classifier.",3.2 Language Identification on Twitter,[0],[0]
"The mixed case encoding technique (see sec. 2.1) gives a small increase (0.5%) in the
accuracy of the model and reduces the u-morph model size by 5%.
",3.2 Language Identification on Twitter,[0],[0]
"The results in Tables 3 and 4 contrast systems using 4-grams, u-morphs, and a combination model, showing precision-recall trade-offs for all users together and F1 scores broken down by specific languages, respectively.",3.2 Language Identification on Twitter,[0],[0]
The combination system simply uses the average of the posterior log-probabilities for each class giving equal weight to each model.,3.2 Language Identification on Twitter,[0],[0]
"While the overall F1 scores are similar for the 4-gram and u-morph systems, their precision and recall trade-offs are quite different, making them effective in combination.",3.2 Language Identification on Twitter,[0],[0]
"The 4-gram system has higher recall, and the u-morph system has higher precision.",3.2 Language Identification on Twitter,[0],[0]
"With the combination, we obtain a substantial gain in precision over the 4-gram system with a modest loss in recall, resulting in a 3% absolute improvement in average F1 score.
",3.2 Language Identification on Twitter,[0],[0]
"Looking at performance on the different languages, we find that the F1 score for the combination model is higher than the 4-gram for every language, with precision always improving.",3.2 Language Identification on Twitter,[0],[0]
"For the dominant languages, the difference in recall is negligible.",3.2 Language Identification on Twitter,[0],[0]
"The infrequent languages have a 4- 8% drop in recall, but the gains in precision are substantial for these languages, ranging from 50- 100% relative.",3.2 Language Identification on Twitter,[0],[0]
"The greatest contrast between the 4-gram and the combination system can be seen for the least frequent languages, i.e. the languages with the least amount of training data.",3.2 Language Identification on Twitter,[0],[0]
"In particular, for French, the precision of the combination system (0.36) is double that of the 4-gram model (0.18) with only a 34% loss in recall (0.24 to 0.16).
",3.2 Language Identification on Twitter,[0],[0]
Looking at the most important features from the classifier highlights the ability of the morphemes to capture relevant meaning.,3.2 Language Identification on Twitter,[0],[0]
"The presence of the morpheme “juan”, “jose” or “flor” increase the probability of a Spanish language tweet by five times.",3.2 Language Identification on Twitter,[0],[0]
The same is true for Portuguese and the morpheme “bieber”.,3.2 Language Identification on Twitter,[0],[0]
"The morpheme “q8”
increases the odds of an Arabic language tweet by thirteen times due to its phonetic similarity to the name of the Arabic speaking country Kuwait.",3.2 Language Identification on Twitter,[0],[0]
Other features may simply reflect cultural norms.,3.2 Language Identification on Twitter,[0],[0]
"For example, having an underscore in the username makes it five percent less likely to observe an English tweet.",3.2 Language Identification on Twitter,[0],[0]
These highly discriminative morphemes are both long and short.,3.2 Language Identification on Twitter,[0],[0]
It is hard for the fixed-length n-grams to capture this information as well as the morphemes do.,3.2 Language Identification on Twitter,[0],[0]
"Of the many studies on automatic classification of online user demographics, few have leveraged names or usernames at all, and the few that do mainly explore their use in combination with other features.",4 Related Work,[0],[0]
"The work presented here differs in its use of usernames alone, but more importantly in the introduction of morphological analysis to handle a large number of usernames.
",4 Related Work,[0],[0]
Two studies on gender recognition are particularly relevant.,4 Related Work,[0],[0]
"Burger et al. (2011) use the Twitter username (or screen name) in combination with other profile and text features to predict gender, but they also look at the use of username features alone.",4 Related Work,[0],[0]
"The results are not directly comparable to ours, because of differences in the data set used (150k Twitter users) and the classifier framework (Winnow), but the character n-gram performance is similar to ours (21-22% different from the majority baseline).",4 Related Work,[0],[0]
The study uses over 400k character n-grams (n=1-5) for screen names alone; our study indicatess that the u-morphs can reduce this number by a factor of 10.,4 Related Work,[0],[0]
"Burger et al. (2011) used the same strategy with the self-identified full
name of the user as entered into their profile, obtaining 89% gender recognition (vs. 77% for screen names).",4 Related Work,[0],[0]
"Later, Liu and Ruths (2013) use the full first name from a user’s profile for gender detection, finding that for the names that are highly predictive of gender, performance improves by relying on this feature alone.",4 Related Work,[0],[0]
"However, more than half of the users have a name that has an unknown gender association.",4 Related Work,[0],[0]
"Manual inspection of these cases indicated that the majority includes strings formed like usernames, nicknames or other types of word concatenations.",4 Related Work,[0],[0]
"These examples are precisely what the u-morph approach tries to address.
",4 Related Work,[0],[0]
"Language identification is an active area of research (Bergsma et al., 2012; Zubiaga et al., 2014), but the username has not been used as a feature.",4 Related Work,[0],[0]
"Again, results are difficult to compare due to the lack of a common test set, but it is notable that the average F1 score for the combination model approaches the scores obtained on a similar Twitter language identification task where the algorithm has access to the full text of the tweet (Lui and Baldwin, 2014): 73% vs. 77% .
",4 Related Work,[0],[0]
"A study that is potentially relevant to our work is automatic classification of ethnicity of Twitter users, specifically whether a user is AfricanAmerican (Pennacchiotti and Popescu, 2011).",4 Related Work,[0],[0]
"Again, a variety of content, profile and behavioral features are used.",4 Related Work,[0],[0]
"Orthographic features of the username are used (e.g. length, number of numeric/alpha characters), and names of users that a person retweets or replies to.",4 Related Work,[0],[0]
"The profile name features do not appear to be useful, but examples of related usernames point to the utility of our approach for analysis of names in other fields.",4 Related Work,[0],[0]
"In summary, this paper has introduced the use of unsupervised morphological analysis of usernames to extract features (u-morphs) for identifying user demographics, particularly gender and language.",5 Conclusions,[0],[0]
"The experimental results demonstrate that usernames contain useful personal information, and that the u-morphs provide a more efficient and complementary representation than character n-grams.2 The result for language identification is particularly remarkable because it comes close to matching the performance achieved by us-
2In order to allow the replicability of the experiments, software and data for building and evaluating our classifiers using pre-trained Morfessor models is available at http: //github.com/ajaech/username_analytics.
ing the full text of a tweet.",5 Conclusions,[0],[0]
"The work is complementary to other demographic studies in that the username prediction can be used together with other features, both for the user and members of his/her social network.
",5 Conclusions,[0],[0]
The methods proposed here could be extended in different directions.,5 Conclusions,[0],[0]
The unsupervised morphology learning algorithm could incorporate priors related to capitalization and non-alphabetic characters to better model these phenomena than our simple text normalization approach.,5 Conclusions,[0],[0]
"More sophisticated classifiers could also be used, such as variable-length n-grams or neural-network-based n-gram language models, as opposed to the unigram model used here.",5 Conclusions,[0],[0]
"Of course the sophistication of the classifier will be limited by the amount of training data available.
",5 Conclusions,[0],[0]
A large amount of data is not necessary to build a high precision username classifier.,5 Conclusions,[0],[0]
"For example, less than 7,000 training examples were available for Turkish in the language identification experiment and the classifier had a precision of 76%.",5 Conclusions,[0],[0]
"Since little data is required, there may be many more applications of this type of model.
",5 Conclusions,[0],[0]
Prior work on unsupervised morphological induction focused on applying the algorithm to natural language input.,5 Conclusions,[0],[0]
"By using those techniques with a new type of input, this paper shows that there are other applications of morphology learning.",5 Conclusions,[0],[0]
"Usernames are ubiquitous on the Internet, and they are often suggestive of user demographics.",abstractText,[0],[0]
This work looks at the degree to which gender and language can be inferred from a username alone by making use of unsupervised morphology induction to decompose usernames into sub-units.,abstractText,[0],[0]
Experimental results on the two tasks demonstrate the effectiveness of the proposed morphological features compared to a character n-gram baseline.,abstractText,[0],[0]
What Your Username Says About You,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2304–2314, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.
Recursive neural models, which use syntactic parse trees to recursively generate representations bottom-up, are a popular architecture. However there have not been rigorous evaluations showing for exactly which tasks this syntax-based method is appropriate. In this paper, we benchmark recursive neural models against sequential recurrent neural models, enforcing applesto-apples comparison as much as possible. We investigate 4 tasks: (1) sentiment classification at the sentence level and phrase level; (2) matching questions to answerphrases; (3) discourse parsing; (4) semantic relation extraction.
Our goal is to understand better when, and why, recursive models can outperform simpler models. We find that recursive models help mainly on tasks (like semantic relation extraction) that require longdistance connection modeling, particularly on very long sequences. We then introduce a method for allowing recurrent models to achieve similar performance: breaking long sentences into clause-like units at punctuation and processing them separately before combining. Our results thus help understand the limitations of both classes of models, and suggest directions for improving recurrent models.",text,[0],[0]
"Deep learning based methods learn lowdimensional, real-valued vectors for word tokens, mostly from large-scale data corpus (e.g., (Mikolov et al., 2013; Le and Mikolov, 2014; Collobert et al., 2011)), successfully capturing syntactic and semantic aspects of text.
",1 Introduction,[0],[0]
"For tasks where the inputs are larger text units (e.g., phrases, sentences or documents), a compositional model is first needed to aggregate tokens into a vector with fixed dimensionality that can be used as a feature for other NLP tasks.",1 Introduction,[0],[0]
"Models for achieving this usually fall into two categories: recurrent models and recursive models:
Recurrent models (also referred to as sequence models) deal successfully with time-series data (Pearlmutter, 1989; Dorffner, 1996) like speech (Robinson et al., 1996; Lippmann, 1989; Graves et al., 2013) or handwriting recognition (Graves and Schmidhuber, 2009; Graves, 2012).",1 Introduction,[0],[0]
"They were applied early on to NLP (Elman, 1990), by modeling a sentence as tokens processed sequentially and at each step combining the current token with previously built embeddings.",1 Introduction,[0],[0]
Recurrent models can be extended to bidirectional ones from both leftto-right and right-to-left.,1 Introduction,[0],[0]
"These models generally consider no linguistic structure aside from word order.
",1 Introduction,[0],[0]
"Recursive neural models (also referred to as tree models), by contrast, are structured by syntactic parse trees.",1 Introduction,[0],[0]
"Instead of considering tokens sequentially, recursive models combine neighbors based on the recursive structure of parse trees, starting from the leaves and proceeding recursively in a bottom-up fashion until the root of the parse tree is reached.",1 Introduction,[0],[0]
"For example, for the phrase the food is delicious, following the operation sequence ( (the food) (is delicious) )",1 Introduction,[0],[0]
rather than the sequential order (((the food) is) delicious),1 Introduction,[0],[0]
.,1 Introduction,[0],[0]
"Many recursive models have been proposed (e.g., (Paulus et al., 2014; Irsoy and Cardie, 2014)), and applied to various NLP tasks, among them entailment (Bowman, 2013; Bowman et al., 2014), sentiment analysis (Socher et al., 2013; Irsoy and Cardie, 2013; Dong et al., 2014), question-answering (Iyyer et al., 2014), relation classification (Socher et al., 2012; Hashimoto et al., 2013), and discourse (Li and Hovy, 2014).
2304
",1 Introduction,[0],[0]
"One possible advantage of recursive models is their potential for capturing long-distance dependencies: two tokens may be structurally close to each other, even though they are far away in word sequence.",1 Introduction,[0],[0]
"For example, a verb and its corresponding direct object can be far away in terms of tokens if many adjectives lies in between, but they are adjacent in the parse tree (Irsoy and Cardie, 2013).",1 Introduction,[0],[0]
"However we do not know if this advantage is truly important, and if so for which tasks, or whether other issues are at play.",1 Introduction,[0],[0]
"Indeed, the reliance of recursive models on parsing is also a potential disadvantage, given that parsing is relatively slow, domain-dependent, and can be errorful.
",1 Introduction,[0],[0]
"On the other hand, recent progress in multiple subfields of neural NLP has suggested that recurrent nets may be sufficient to deal with many of the tasks for which recursive models have been proposed.",1 Introduction,[0],[0]
"Recurrent models without parse structures have shown good results in sequenceto-sequence generation (Sutskever et al., 2014) for machine translation (e.g., (Kalchbrenner and Blunsom, 2013; 3; Luong et al., 2014)), parsing (Vinyals et al., 2014), and sentiment, where for example recurrent-based paragraph vectors (Le and Mikolov, 2014) outperform recursive models (Socher et al., 2013) on the Stanford sentimentbank dataset.
",1 Introduction,[0],[0]
"Our goal in this paper is thus to investigate a number of tasks with the goal of understanding for which kinds of problems recurrent models may be sufficient, and for which kinds recursive models offer specific advantages.",1 Introduction,[0],[0]
"We investigate four tasks with different properties.
",1 Introduction,[0],[0]
"• Binary sentiment classification at the sentence level (Pang et al., 2002) and phrase level (Socher et al., 2013) that focus on understanding the role of recursive models in dealing with semantic compositionally in various scenarios such as different lengths of inputs and whether or not supervision is comprehensive.
",1 Introduction,[0],[0]
•,1 Introduction,[0],[0]
"Phrase Matching on the UMD-QA dataset (Iyyer et al., 2014) can help see the difference between outputs from intermediate components from different models, i.e., representations for intermediate parse tree nodes and outputs from recurrent models at different time steps.",1 Introduction,[0],[0]
"It also helps see whether pars-
ing is useful for finding similarities between question sentences and target phrases.
",1 Introduction,[0],[0]
"• Semantic Relation Classification on the SemEval-2010 (Hendrickx et al., 2009)",1 Introduction,[0],[0]
"data can help understand whether parsing is helpful in dealing with long-term dependencies, such as relations between two words that are far apart in the sequence.
",1 Introduction,[0],[0]
• Discourse parsing (RST dataset) is useful for measuring the extent to which parsing improves discourse tasks that need to combine meanings of larger text units.,1 Introduction,[0],[0]
"Discourse parsing treats elementary discourse units (EDUs) as basic units to operate on, which are usually short clauses.",1 Introduction,[0],[0]
"The task also sheds light on the extent to which syntactic structures help acquire shot text representations.
",1 Introduction,[0],[0]
"The principal motivation for this paper is to understand better when, and why, recursive models are needed to outperform simpler models by enforcing apples-to-apples comparison as much as possible.",1 Introduction,[0],[0]
"This paper applies existing models to existing tasks, barely offering novel algorithms or tasks.",1 Introduction,[0],[0]
"Our goal is rather an analytic one, to investigate different versions of recursive and recurrent models.",1 Introduction,[0],[0]
"This work helps understand the limitations of both classes of models, and suggest directions for improving recurrent models.
",1 Introduction,[0],[0]
"The rest of this paper organized as follows: We detail versions of recursive/recurrent models in Section 2, present the tasks and results in Section 3, and conclude with discussions in Section 4.",1 Introduction,[0],[0]
"We assume that the text unit S, which could be a phrase, a sentence or a document, is comprised of a sequence of tokens/words: S = {w1, w2, ..., wNS}, where Ns denotes the number of tokens in S. Each word w is associated with a K-dimensional vector embedding ew = {e1w, e2w, ..., eKw }.",2.1 Notations,[0],[0]
"The goal of recursive and recurrent models is to map the sequence to a Kdimensional eS , based on its tokens and their correspondent embeddings.
",2.1 Notations,[0],[0]
"Standard Recurrent/Sequence Models successively take word wt at step t, combines its vector representation et with the previously built hidden vector ht−1 from time t− 1, calculates the re-
sulting current embedding ht, and passes it to the next step.",2.1 Notations,[0],[0]
"The embedding ht for the current time t is thus:
ht = f(W · ht−1 + V · et) (1)
whereW and V denote compositional matrices.",2.1 Notations,[0],[0]
"If Ns denotes the length of the sequence, hNs represents the whole sequence S.
Standard recursive/Tree models work in a similar way, but processing neighboring words by parse tree order rather than sequence order.",2.1 Notations,[0],[0]
It computes a representation for each parent node based on its immediate children recursively in a bottom-up fashion until reaching the root of the tree.,2.1 Notations,[0],[0]
"For a given node η in the tree and its left child ηleft (with representation eleft) and right child ηright (with representation eright), the standard recursive network calculates eη as follows:
eη = f(W · eηleft + V · eηright) (2)
Bidirectional Models (Schuster and Paliwal, 1997) add bidirectionality to the recurrent framework where embeddings for each time are calculated both forwardly and backwardly:
h→t = f(W → · h→t−1 + V→ · et) h←t = f(W ← · h←t+1 + V← · et)
(3)
Normally, final representations for sentences can be achieved either by concatenating vectors calculated from both directions",2.1 Notations,[0],[0]
"[e←1 , e→NS ] or using further compositional operation to preserve vector dimensionality
ht = f(WL ·",2.1 Notations,[0],[0]
"[h←t , h→t ]) (4)
where WL denotes a K×2K dimensional matrix.",2.1 Notations,[0],[0]
"Long Short Term Memory (LSTM) LSTM models (Hochreiter and Schmidhuber, 1997) are defined as follows: given a sequence of inputs X = {x1, x2, ..., xnX}, an LSTM associates each timestep with an input, memory and output gate, respectively denoted as it, ft and ot.",2.1 Notations,[0],[0]
"We notationally disambiguate e and h: et denotes the vector for individual text units (e.g., word or sentence) at time step t, while ht denotes the vector computed by the LSTM model at time t by combining et and ht−1.",2.1 Notations,[0],[0]
σ denotes the sigmoid function.,2.1 Notations,[0],[0]
"The vector representation ht for each time-step t is given by:
 it ft ot lt  =  σ σ σ",2.1 Notations,[0],[0]
"tanh W · [ ht−1et ]
(5)
ct = ft · ct−1 + it · lt (6) hst = ot · ct (7)
where W ∈ R4K×2K .",2.1 Notations,[0],[0]
"Labels at the phrase/sentence level are predicted representations outputted from the last time step.
",2.1 Notations,[0],[0]
"Tree LSTMs Recent research has extended the LSTM idea to tree-based structures (Zhu et al., 2015; Tai et al., 2015) that associate memory and forget gates to nodes of the parse trees.
",2.1 Notations,[0],[0]
Bi-directional LSTMs These combine bidirectional models and LSTMs.,2.1 Notations,[0],[0]
"In this section, we detail our experimental settings and results.",3 Experiments,[0],[0]
"We consider the following tasks, each representative of a different class of NLP tasks.
",3 Experiments,[0],[0]
• Binary sentiment classification on the Pang et al. (2002) dataset.,3 Experiments,[0],[0]
"This addresses the issues where supervision only appears globally after a long sequence of operations.
",3 Experiments,[0],[0]
"• Sentiment Classification on the Stanford Sentiment Treebank (Socher et al., 2013): comprehensive labels are found for words and phrases where local compositionally (such as from negation, mood, or others cued by phrase-structure) is to be learned.
",3 Experiments,[0],[0]
"• Sentence-Target Matching on the UMDQA dataset (Iyyer et al., 2014):",3 Experiments,[0],[0]
"Learns matches between target and components in the source sentences, which are parse tree nodes for recursive models and different time-steps for recurrent models.
•",3 Experiments,[0],[0]
"Semantic Relation Classification on the SemEval-2010 task (Hendrickx et al., 2009).",3 Experiments,[0],[0]
"Learns long-distance relationships between two words that may be far apart sequentially.
",3 Experiments,[0],[0]
"• Discourse Parsing (Li et al., 2014; Hernault et al., 2010): Learns sentence-to-sentence relations based on calculated representations.
",3 Experiments,[0],[0]
In each case we followed the protocols described in the original papers.,3 Experiments,[0],[0]
"We first group the algorithm variants into two groups as follows:
• Standard tree models vs standard sequence models vs standard bi-directional sequence models
• LSTM tree models, LSTM sequence models vs LSTM bi-directional sequence models.
",3 Experiments,[0],[0]
"We employed standard training frameworks for neural models: for each task, we used stochastic gradient decent using AdaGrad (Duchi et al., 2011) with minibatches (Cotter et al., 2011).",3 Experiments,[0],[0]
Parameters are tuned using the development dataset if available in the original datasets or from crossvalidation if not.,3 Experiments,[0],[0]
"Derivatives are calculated from standard back-propagation (Goller and Kuchler, 1996).",3 Experiments,[0],[0]
"Parameters to tune include size of mini batches, learning rate, and parameters for L2 penalizations.",3 Experiments,[0],[0]
"The number of running iterations is treated as a parameter to tune and the model achieving best performance on the development set is used as the final model to be evaluated.
",3 Experiments,[0],[0]
"For settings where no repeated experiments are performed, the bootstrap test is adopted for statistical significance testing (Efron and Tibshirani, 1994).",3 Experiments,[0],[0]
Test scores that achieve significance level of 0.05 are marked by an asterisk (*).,3 Experiments,[0],[0]
"Task Description We start with the Stanford Sentiment TreeBank (Socher et al., 2013).",3.1 Stanford Sentiment TreeBank,[0],[0]
"This dataset contains gold-standard labels for every parse tree constituent, from the sentence to phrases to individual words.
",3.1 Stanford Sentiment TreeBank,[0],[0]
"Of course, any conclusions drawn from implementing sequence models on a dataset that was based on parse trees may have to be weakened, since sequence models may still benefit from the way that the dataset was collected.",3.1 Stanford Sentiment TreeBank,[0],[0]
"Nevertheless we add an evaluation on this dataset because it has been a widely used benchmark dataset for neural model evaluations.
",3.1 Stanford Sentiment TreeBank,[0],[0]
"For recursive models, we followed the protocols in Socher et al. (2013) where node embeddings in the parse trees are obtained from recursive models and then fed to a softmax classifier.",3.1 Stanford Sentiment TreeBank,[0],[0]
We transformed the dataset for recurrent model use as illustrated in Figure 1.,3.1 Stanford Sentiment TreeBank,[0],[0]
Each phrase is reconstructed from parse tree nodes and treated as a separate data point.,3.1 Stanford Sentiment TreeBank,[0],[0]
"As the treebank contains 11,855
sentences with 215,154 phrases, the reconstructed dataset for recurrent models comprises 215,154 examples.",3.1 Stanford Sentiment TreeBank,[0],[0]
"Models are evaluated at both the phrase level (82,600 instances) and the sentence root level (2,210 instances).
",3.1 Stanford Sentiment TreeBank,[0],[0]
Results are shown in Table 1 and 21.,3.1 Stanford Sentiment TreeBank,[0],[0]
"When comparing the standard version of tree models to sequence models, we find it helps a bit at root level identification (for sequences but not bisequences), but yields no significant improvement at the phrase level.
",3.1 Stanford Sentiment TreeBank,[0],[0]
LSTM Tai et al. (2015) discovered that LSTM tree models generate better performances in terms of sentence root level evaluation than sequence models.,3.1 Stanford Sentiment TreeBank,[0],[0]
We explore this task a bit more by training deeper and more sophisticated models.,3.1 Stanford Sentiment TreeBank,[0],[0]
"We examine the following three models:
1.",3.1 Stanford Sentiment TreeBank,[0],[0]
"Tree-structured LSTM models (Tai et al., 2015)2.
2.",3.1 Stanford Sentiment TreeBank,[0],[0]
"Deep Bi-LSTM sequence models (denoted as Sequence) that treat the whole sentence as just one sequence.
3.",3.1 Stanford Sentiment TreeBank,[0],[0]
"Deep Bi-LSTM hierarchical sequence models (denoted as Hierarchical Sequence) that first slice the sentence into a sequence of subsentences by using a look-up table of punctuations (i.e., comma, period, question mark
1The performance of our implementations of recursive models is not exactly identical to that reported in Socher et al. (2013), but the relative difference is around 1% to 2%.
",3.1 Stanford Sentiment TreeBank,[0],[0]
"2Tai et al.. achieved 0.510 accuracy in terms of finegrained evaluation at the root level as reported in (Tai et al., 2015), similar to results from our implementations (0.504).
and exclamation mark).",3.1 Stanford Sentiment TreeBank,[0],[0]
"The representation for each sub-sentence is first computed separately, and another level of sequence LSTM (one-directional) is then used to join the subsentences.",3.1 Stanford Sentiment TreeBank,[0],[0]
"Illustrations are shown in Figure2.
",3.1 Stanford Sentiment TreeBank,[0],[0]
We consider the third model because the dataset used in Tai et al. (2015) contains long sentences and the evaluation is performed only at the sentence root level.,3.1 Stanford Sentiment TreeBank,[0],[0]
"Since a parsing algorithm will naturally break long sentences into sub-sentences, we would like to know whether any performance boost is introduced by the intra-clause parse tree structure or just by this broader segmentation of a sentence into clause-like units; this latter advantage could be approximated by using punctuationbased approximations to clause boundaries.
",3.1 Stanford Sentiment TreeBank,[0],[0]
We run 15 iterations for each algorithm.,3.1 Stanford Sentiment TreeBank,[0],[0]
Parameters are harvested at the end of each iteration; those performing best on the development set are used on the test set.,3.1 Stanford Sentiment TreeBank,[0],[0]
The whole process takes roughly 15-20 minutes on a single GPU machine3.,3.1 Stanford Sentiment TreeBank,[0],[0]
"For a more convincing comparison, we did not use the bootstrap test where parallel examples are generated from one same dataset.",3.1 Stanford Sentiment TreeBank,[0],[0]
"Instead, we repeated the aforementioned procedure for each algorithm 20 times and report accuracies
3Tesla K40m, 2880 Cuda cores.
with standard deviation in Table 3.
",3.1 Stanford Sentiment TreeBank,[0],[0]
"Tree LSTMs are equivalent or marginally better than standard bi-directional sequence model (two-tailed p-value equals 0.041*, and only at the root level, with p-value for the phrase level at 0.376).",3.1 Stanford Sentiment TreeBank,[0],[0]
"The hierarchical sequence model achieves the same performance with a p-value of 0.198.
",3.1 Stanford Sentiment TreeBank,[0],[0]
"Discussion The results above suggest that clausal segmentation of long sentences offers a slight performance boost, a result also supported by the fact that very little difference exists between the three models for phrase-level sentiment evaluation.",3.1 Stanford Sentiment TreeBank,[0],[0]
"Clausal segmentation of long sentences thus provides a simple approximation to parse-tree based models.
",3.1 Stanford Sentiment TreeBank,[0],[0]
"We suggest a few reasons for this slightly better performances introduced by clausal segmentation:
1.",3.1 Stanford Sentiment TreeBank,[0],[0]
"Treating clauses as basic units (to the extent that punctuation approximates clauses) preserves the semantic structure of text.
2.",3.1 Stanford Sentiment TreeBank,[0],[0]
Semantic compositions such as negations or conjunctions usually appear at the clause level.,3.1 Stanford Sentiment TreeBank,[0],[0]
"Working on clauses individually and then combining them model inter-clause compositions.
",3.1 Stanford Sentiment TreeBank,[0],[0]
3.,3.1 Stanford Sentiment TreeBank,[0],[0]
Errors are back-propagated to individual tokens using fewer steps in hierarchical models than in standard models.,3.1 Stanford Sentiment TreeBank,[0],[0]
"Consider a movie
review “simple as the plot was , i still like it a lot”.",3.1 Stanford Sentiment TreeBank,[0],[0]
"With standard recurrent models it takes 12 steps before the prediction error gets back to the first token “simple”:
error→lot→a→it→like→still→i→,→was →plot→",3.1 Stanford Sentiment TreeBank,[0],[0]
"the→as→simple In a hierarchical model, the second clause is compacted into one component, and the error propagation is thus given by:
error→",3.1 Stanford Sentiment TreeBank,[0],[0]
second-clause → first-clause → was→plot→the→as→simple.,3.1 Stanford Sentiment TreeBank,[0],[0]
Propagation with clause segmentation consists of only 8 operations.,3.1 Stanford Sentiment TreeBank,[0],[0]
"Such a procedure thus tends to attenuate the gradient vanishing problem, potentially yielding better performance.",3.1 Stanford Sentiment TreeBank,[0],[0]
Task Description: The sentiment dataset of Pang et al. (2002) consists of sentences with a sentiment label for each sentence.,3.2 Binary Sentiment Classification (Pang),[0],[0]
We divide the original dataset into training(8101)/dev(500)/testing(2000).,3.2 Binary Sentiment Classification (Pang),[0],[0]
No pretraining procedure as described in Socher et al. (2011b) is employed.,3.2 Binary Sentiment Classification (Pang),[0],[0]
Word embeddings are initialized using skip-grams and kept fixed in the learning procedure.,3.2 Binary Sentiment Classification (Pang),[0],[0]
We trained skip-gram embeddings on the Wikipedia+Gigaword dataset using the word2vec package4.,3.2 Binary Sentiment Classification (Pang),[0],[0]
Sentence level embeddings are fed into a sigmoid classifier.,3.2 Binary Sentiment Classification (Pang),[0],[0]
"Performances for 50 dimensional vectors are given in the table below:
Discussion Why don’t parse trees help on this task?",3.2 Binary Sentiment Classification (Pang),[0],[0]
"One possible explanation is the distance
4https://code.google.com/p/word2vec/
of the supervision signal from the local compositional structure.",3.2 Binary Sentiment Classification (Pang),[0],[0]
"The Pang et al. dataset has an average sentence length of 22.5 words, which means it takes multiple steps before sentiment related evidence comes up to the surface.",3.2 Binary Sentiment Classification (Pang),[0],[0]
"It is therefore unclear whether local compositional operators (such as negation) can be learned; there is only a small amount of training data (around 8,000 examples) and the sentiment supervision only at the level of the sentence may not be easy to propagate down to deeply buried local phrases.",3.2 Binary Sentiment Classification (Pang),[0],[0]
Task Description:,3.3 Question-Answer Matching,[0],[0]
"In the question-answering dataset QANTA5, each answer is a token or short phrase.",3.3 Question-Answer Matching,[0],[0]
"The task is different from standard generation focused QA task but formalized as a multiclass classification task that matches a source question with a candidates phrase from a predefined pool of candidate phrases We give an illustrative example here:
Question: He left unfinished a novel whose title character forges his father’s signature to get out of school and avoids the draft by feigning desire to join.",3.3 Question-Answer Matching,[0],[0]
"Name this German author of The Magic Mountain and Death in Venice.
",3.3 Question-Answer Matching,[0],[0]
Answer: Thomas Mann from the pool of phrases.,3.3 Question-Answer Matching,[0],[0]
"Other candidates might include George Washington, Charlie Chaplin, etc.
",3.3 Question-Answer Matching,[0],[0]
The model of Iyyer et al. (2014) minimizes the distances between answer embeddings and node embeddings along the parse tree of the question.,3.3 Question-Answer Matching,[0],[0]
"Concretely, let c denote the correct answer to question S, with embedding ~c, and z denoting any random wrong answer.",3.3 Question-Answer Matching,[0],[0]
"The objective function sums over the dot product between representation for every node η along the question parse trees and the answer representations:
L = ∑
η∈[parse tree] ∑ z max(0, 1−~c ·eη+~z ·eη) (8)
5http://cs.umd.edu/˜miyyer/qblearn/. Because the publicly released dataset is smaller than the version used in (Iyyer et al., 2014) due to privacy issues, our numbers are not comparable to those in (Iyyer et al., 2014).
",3.3 Question-Answer Matching,[0],[0]
where eη denotes the embedding for parse tree node calculated from the recursive neural model.,3.3 Question-Answer Matching,[0],[0]
"Here the parse trees are dependency parses following (Iyyer et al., 2014).
",3.3 Question-Answer Matching,[0],[0]
"By adjusting the framework to recurrent models, we minimize the distance between the answer embedding and the embeddings calculated from each timestep t of the sequence:
L = ∑
t∈[1,Ns]
∑ z max(0, 1− ~c · et + ~z · et) (9)
",3.3 Question-Answer Matching,[0],[0]
"At test time, the model chooses the answer (from the set of candidates) that gives the lowest loss score.",3.3 Question-Answer Matching,[0],[0]
"As can be seen from results presented in Table 5, the difference is only significant for the LSTM setting between the tree model and the sequence model; no significant difference is observed for other settings.
",3.3 Question-Answer Matching,[0],[0]
"Discussion The UMD-QA task represents a group of situations where because we have insufficient supervision about matching (it’s hard to know which node in the parse tree or which timestep provides the most direct evidence for the answer), decisions have to be made by looking at and iterating over all subunits (all nodes in parse trees or timesteps).",3.3 Question-Answer Matching,[0],[0]
"Similar ideas can be found in pooling structures (e.g. Socher et al. (2011a)).
",3.3 Question-Answer Matching,[0],[0]
"The results above illustrate that for tasks where we try to align the target with different source components (i.e., parse tree nodes for tree models and different time steps for sequence models), components from sequence models are able to embed important information, despite the fact that sequence model components are just sentence fragments and hence usually not linguistically meaningful components in the way that parse tree constituents are.",3.3 Question-Answer Matching,[0],[0]
"Task Description: SemEval-2010 Task 8 (Hendrickx et al., 2009) is to find semantic relationships between pairs of nominals, e.g., in “My [apartment]e1 has a pretty large [kitchen]e2”
classifying the relation between [apartment] and",3.4 Semantic Relationship Classification,[0],[0]
[kitchen] as component-whole.,3.4 Semantic Relationship Classification,[0],[0]
"The dataset contains 9 ordered relationships, so the task is formalized as a 19-class classification problem, with directed relations treated as separate labels; see Hendrickx et al. (2009; Socher et al. (2012) for details.
",3.4 Semantic Relationship Classification,[0],[0]
"For the recursive implementations, we follow the neural framework defined in Socher et al. (2012).",3.4 Semantic Relationship Classification,[0],[0]
"The path in the parse tree between the two nominals is retrieved, and the embedding is calculated based on recursive models and fed to a softmax classifier6.",3.4 Semantic Relationship Classification,[0],[0]
"Retrieved paths are transformed for the recurrent models as shown in Figure 5.
",3.4 Semantic Relationship Classification,[0],[0]
"Discussion Unlike for earlier tasks, here recursive models yield much better performance than the corresponding recurrent versions for all versions (e.g., standard tree vs. standard sequence, p = 0.004).",3.4 Semantic Relationship Classification,[0],[0]
These results suggest that it is the need to integrate structures far apart in the sentence that characterizes the tasks where recursive models surpass recurrent models.,3.4 Semantic Relationship Classification,[0],[0]
"In parse-based models, the two target words are drawn together much earlier in the decision process than in recurrent models, which must remember one target until the other one appears.",3.4 Semantic Relationship Classification,[0],[0]
"Task Description: Our final task, discourse parsing based on the RST-DT corpus (Carlson et
6(Socher et al., 2012) achieve state-of-art performance by combining a sophisticated model, MV-RNN, in which each word is presented with both a matrix and a vector with human-feature engineering.",3.5 Discourse Parsing,[0],[0]
"Again, because MV-RNN is difficult to adapt to a recurrent version, we do not employ this state-of-the-art model, adhering only to the general versions of recursive models described in Section 2, since our main goal is to compare equivalent recursive and recurrent models rather than implement the state of the art.
",3.5 Discourse Parsing,[0],[0]
"al., 2003), is to build a discourse tree for a document, based on assigning Rhetorical Structure Theory (RST) relations between elementary discourse units (EDUs).",3.5 Discourse Parsing,[0],[0]
"Because discourse relations express the coherence structure of discourse, they presumably express different aspects of compositional meaning than sentiment or nominal relations.",3.5 Discourse Parsing,[0],[0]
"See Hernault et al. (2010) for more details on discourse parsing and the RST-DT corpus.
",3.5 Discourse Parsing,[0],[0]
"Representations for adjacent EDUs are fed into binary classification (whether two EDUs are related) and multi-class relation classification models, as defined in Li et al. (2014).",3.5 Discourse Parsing,[0],[0]
"Related EDUs are then merged into a new EDU, the representation of which is obtained through an operation of neural composition based on the previous two related EDUs.",3.5 Discourse Parsing,[0],[0]
"This step is repeated until all units are merged.
",3.5 Discourse Parsing,[0],[0]
"Discourse parsing takes EDUs as the basic units to operate on; EDUs are short clauses, not full sentences, with an average length of 7.2 words.",3.5 Discourse Parsing,[0],[0]
Recursive and recurrent models are applied on EDUs to create embeddings to be used as inputs for discourse parsing.,3.5 Discourse Parsing,[0],[0]
We use this task for two reasons: (1) to illustrate whether syntactic parse trees are useful for acquiring representations for short clauses.,3.5 Discourse Parsing,[0],[0]
"(2) to measure the extent to which pars-
ing improves discourse tasks that need to combine the meanings of larger text units.
",3.5 Discourse Parsing,[0],[0]
"Models are traditionally evaluated in terms of three metrics, i.e., spans7, nuclearity8, and identifying the rhetorical relation between two clauses.",3.5 Discourse Parsing,[0],[0]
"Due to space limits, we only focus the last one, rhetorical relation identification, because (1) relation labels are treated as correct only if spans and nuclearity are correctly labeled (2) relation identification between clauses offer more insights about model’s abilities to represent sentence semantics.",3.5 Discourse Parsing,[0],[0]
"In order to perform a plain comparison, no additional human-developed features are added.
",3.5 Discourse Parsing,[0],[0]
Discussion We see no large differences between equivalent recurrent and recursive models.,3.5 Discourse Parsing,[0],[0]
We suggest two possible explanations.,3.5 Discourse Parsing,[0],[0]
"(1) EDUs tend to be short; thus for some clauses, parsing might not change the order of operations on words.",3.5 Discourse Parsing,[0],[0]
"Even for those whose orders are changed by parse trees, the influence of short phrases on the final representation may not be great enough.",3.5 Discourse Parsing,[0],[0]
"(2) Unlike earlier tasks, where text representations are immediately used as inputs into classifiers, the algorithm presented here adopts additional levels of neural composition during the process of EDU merging.",3.5 Discourse Parsing,[0],[0]
"We suspect that neural layers may act as information filters, separating the informational chaff from the wheat, which in turn makes the model a bit more immune to the initial inputs.",3.5 Discourse Parsing,[0],[0]
"We compared recursive and recurrent neural models for representation learning on 5 distinct NLP tasks in 4 areas for which recursive neural models are known to achieve good performance (Socher et al., 2012; Socher et al., 2013; Li et al., 2014; Iyyer et al., 2014).
",4 Discussions and Conclusions,[0],[0]
"As with any comparison between models, our results come with some caveats: First, we explore the most general or basic forms of recur-
7on blank tree structures.",4 Discussions and Conclusions,[0],[0]
"8on tree structures with nuclearity indication.
",4 Discussions and Conclusions,[0],[0]
sive/recurrent models rather than various sophisticated algorithm variants.,4 Discussions and Conclusions,[0],[0]
"This is because fair comparison becomes more and more difficult as models get complex (e.g., the number of layers, number of hidden units within each layer, etc.).",4 Discussions and Conclusions,[0],[0]
Thus most neural models employed in this work are comprised of only one layer of neural compositions—despite the fact that deep neural models with multiple layers give better results.,4 Discussions and Conclusions,[0],[0]
"Our conclusions might thus be limited to the algorithms employed in this paper, and it is unclear whether they can be extended to other variants or to the latest state-of-the-art.",4 Discussions and Conclusions,[0],[0]
"Second, in order to compare models “fairly”, we force every model to be trained exactly in the same way:",4 Discussions and Conclusions,[0],[0]
"AdaGrad with minibatches, same set of initializations, etc.",4 Discussions and Conclusions,[0],[0]
"However, this may not necessarily be the optimal way to train every model; different training strategies tailored for specific models may improve their performances.",4 Discussions and Conclusions,[0],[0]
"In that sense, our attempts to be “fair” in this paper may nevertheless be unfair.
",4 Discussions and Conclusions,[0],[0]
"Pace these caveats, our conclusions can be summarized as follows:
•",4 Discussions and Conclusions,[0],[0]
"In tasks like semantic relation extraction, in which single headwords need to be associated across a long distance, recursive models shine.",4 Discussions and Conclusions,[0],[0]
"This suggests that for the many other kinds of tasks in which long-distance semantic dependencies play a role (e.g., translation between languages with significant reordering like Chinese-English translation), syntactic structures from recursive models may offer useful power.
",4 Discussions and Conclusions,[0],[0]
"• Tree models tend to help more on long sequences than shorter ones with sufficient supervision: tree models slightly help root level identification on the Stanford Sentiment Treebank, but do not help much at the phrase level.",4 Discussions and Conclusions,[0],[0]
"Adopting bi-directional versions of recurrent models seem to largely bridge this gap, producing equivalent or sometimes better results.
",4 Discussions and Conclusions,[0],[0]
"• On long sequences where supervision is not sufficient, e.g., in Pang at al.,’s dataset (supervision only exists on top of long sequences), no significant difference is observed between tree based and sequence based models.
",4 Discussions and Conclusions,[0],[0]
"• In cases where tree-based models do well, a simple approximation to tree-based models
seems to improve recurrent models to equivalent or almost equivalent performance: (1) break long sentences (on punctuation) into a series of clause-like units, (2) work on these clauses separately, and (3) join them together.",4 Discussions and Conclusions,[0],[0]
"This model sometimes works as well as tree models for the sentiment task, suggesting that one of the reasons tree models help is by breaking down long sentences into more manageable units.
",4 Discussions and Conclusions,[0],[0]
"• Despite that the fact that components (outputs from different time steps) in recurrent models are not linguistically meaningful, they may do as well as linguistically meaningful phrases (represented by parse tree nodes) in embedding informative evidence, as demonstrated in UMD-QA task.",4 Discussions and Conclusions,[0],[0]
"Indeed, recent work in parallel with ours (Bowman et al., 2015) has shown that recurrent models like LSTMs can discover implicit recursive compositional structure.",4 Discussions and Conclusions,[0],[0]
"We would especially like to thank Richard Socher and Kai-Sheng Tai for insightful comments, advice, and suggestions.",5 Acknowledgments,[0],[0]
"We would also like to thank Sam Bowman, Ignacio Cases, Jon Gauthier, Kevin Gu, Gabor Angeli, Sida Wang, Percy Liang and other members of the Stanford NLP group, as well as the anonymous reviewers for their helpful advice on various aspects of this work.",5 Acknowledgments,[0],[0]
"We acknowledge the support of NVIDIA Corporation with the donation of Tesla K40 GPUs We gratefully acknowledge support from an Enlight Foundation Graduate Fellowship, a gift from Bloomberg L.P., the Defense Advanced Research Projects Agency (DARPA) Deep Exploration and Filtering of Text (DEFT) Program under Air Force Research Laboratory (AFRL) contract no.",5 Acknowledgments,[0],[0]
"FA8750-13-2-0040, and the NSF via award IIS-1514268.",5 Acknowledgments,[0],[0]
"Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of Bloomberg L.P., DARPA, AFRL, NSF, or the US government.",5 Acknowledgments,[0],[0]
"Recursive neural models, which use syntactic parse trees to recursively generate representations bottom-up, are a popular architecture.",abstractText,[0],[0]
However there have not been rigorous evaluations showing for exactly which tasks this syntax-based method is appropriate.,abstractText,[0],[0]
"In this paper, we benchmark recursive neural models against sequential recurrent neural models, enforcing applesto-apples comparison as much as possible.",abstractText,[0],[0]
We investigate 4 tasks: (1) sentiment classification at the sentence level and phrase level; (2) matching questions to answerphrases; (3) discourse parsing; (4) semantic relation extraction.,abstractText,[0],[0]
"Our goal is to understand better when, and why, recursive models can outperform simpler models.",abstractText,[0],[0]
"We find that recursive models help mainly on tasks (like semantic relation extraction) that require longdistance connection modeling, particularly on very long sequences.",abstractText,[0],[0]
We then introduce a method for allowing recurrent models to achieve similar performance: breaking long sentences into clause-like units at punctuation and processing them separately before combining.,abstractText,[0],[0]
"Our results thus help understand the limitations of both classes of models, and suggest directions for improving recurrent models.",abstractText,[0],[0]
When Are Tree Structures Necessary for Deep Learning of Representations?,title,[0],[0]
"In the last two decades, statistical machine learning algorithms for processing massive datasets have been intensively studied for a wide-range of applications in computer vision, biology, chemistry and healthcare (Murdoch & Detsky, 2013; Tarca et al., 2007).",1. Introduction,[0],[0]
"While the challenges posed by large scale datasets are compelling, one is often faced with a fairly distinct set of technical issues for studies in biological and health sciences.",1. Introduction,[0],[0]
"For instance, a sizable portion
1University of Wisconsin-Madison 2William S. Middleton Memorial Veteran’s Affairs Hospital.",1. Introduction,[0],[0]
"Correspondence to: Hao Zhou <hzhou@stat.wisc.edu>, Vikas Singh <",1. Introduction,[0],[0]
"vsingh@biostat.wisc.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"of scientific research is carried out by small or mediumsized groups (Fortin & Currie, 2013) supported by modest budgets (Lauer, 2016).",1. Introduction,[0],[0]
"Hence, there are logistic/financial constraints on the number of experiments and/or number of participants within a trial, leading to small size datasets.",1. Introduction,[0],[0]
"While the analysis may be sufficiently powered to evaluate the primary hypothesis of the study/experiment, interesting follow-up scientific questions (often more nuanced), come up during the course of the project.",1. Introduction,[0],[0]
These tasks may be underpowered for the sample sizes available.,1. Introduction,[0],[0]
"This necessitates efforts to identify similar datasets elsewhere so that the combined sample size of the “pooled” dataset is enough to determine significant associations between a response and a set of predictors, e.g., within linear regression.
",1. Introduction,[0],[0]
Motivating Application.,1. Introduction,[0],[0]
"In genomics, funding agencies have invested effort into standardizing/curating data collection across large international projects (ENCODE Project Consortium, 2004).",1. Introduction,[0],[0]
"In other disciplines, such as in the study of neurological disorders, heterogeneity in disease etiology, variations in scanners and/or acquisition tools make standardization more difficult.",1. Introduction,[0],[0]
"For Alzheimer’s disease (AD), a motivation of this work, efforts such as ADNI (Weiner et al., 2015) provide a variety of clinical, imaging and cognitive tests data for 800+ older adults.",1. Introduction,[0],[0]
"However, the research focus has now moved to the early stages of disease – as early as late middle age – where treatments are expected to be more effective.",1. Introduction,[0],[0]
"But (a) the statistical signal at this stage is weak and difficult to demonstrate without large sample sizes and (b) such “preclinical” participants are not well represented, even in large ADNI sized studies.",1. Introduction,[0],[0]
"Hence, there is a concerted effort in general for smaller standalone projects (focused on a specific disease stage), that can be retrospectively pooled for analysis towards addressing a challenging scientific hypothesis (Jahanshad et al., 2013).",1. Introduction,[0],[0]
"Unfortunately, acquisition protocols for various measures across sites are usually different and data are heterogeneous.",1. Introduction,[0],[0]
These issues raise a fundamental technical question.,1. Introduction,[0],[0]
"When is it meaningful to pool datasets for estimating a simple statistical model (e.g., linear regression)?",1. Introduction,[0],[0]
"When can we guarantee improvements in statistical power, and when are such pooling efforts not worth it?",1. Introduction,[0],[0]
Can we give a hypothesis test and obtain p-values to inform our policies/decisions?,1. Introduction,[0],[0]
"While related problems have been stud-
ied in machine learning from an algorithm design perspective, even simple hypothesis tests which can be deployed by a researcher in practice, are currently unavailable.",1. Introduction,[0],[0]
"Our goal is to remove this significant limitation.
",1. Introduction,[0],[0]
Putting our development in context.,1. Introduction,[0],[0]
The realization that “similar” datasets from multiple sites can be pooled to potentially improve statistical power is not new.,1. Introduction,[0],[0]
"With varying empirical success, models tailored to perform regression in multi-site studies (Group, 2002), (Haase et al., 2009), (Klunk et al., 2015) have been proposed, where due to operational reasons, recruitment and data acquisition are distributed over multiple sites, or even countries.",1. Introduction,[0],[0]
"When the pooling is being performed retrospectively (i.e., after the data has been collected), resolving site-specific confounds, such as distributional shifts or biases in measurements, is essential before estimation/inference of a statistical model.",1. Introduction,[0],[0]
"We will not develop new algorithms for estimating the distributional mismatch or for performing multi-site regression — rather, our primary goal is to identify the regimes (and give easily computable checks) where this regression task on a pooled dataset is statistically meaningful, assuming that good pre-processing schemes are available.",1. Introduction,[0],[0]
"We will present a rigorous yet simple to implement hypothesis test, analyze its behavior, and show extensive experimental evidence (for an important scientific problem).",1. Introduction,[0],[0]
"The practitioner is free to use his/her preferred procedure for the “before step” (estimating the distributional shifts).
",1. Introduction,[0],[0]
Contributions: a),1. Introduction,[0],[0]
Our main result is a hypothesis test to evaluate whether pooling data across multiple sites for regression (before or after correcting for site-specific distributional shifts) can improve the estimation (mean squared error) of the relevant coefficients (while permitting an influence from a set of confounding variables).,1. Introduction,[0],[0]
b),1. Introduction,[0],[0]
We derive analogous results in the high-dimensional setting by leveraging a different set of analysis techniques.,1. Introduction,[0],[0]
"Using an existing sparse multi-task Lasso model, we show how the utility of pooling can be evaluated even when the support set of the features (predictors) is not exactly the same across sites using ideas broadly related to high dimensional simultaneous inference (Dezeure et al., 2015).",1. Introduction,[0],[0]
"We show `2-consistency rate, which supports the use of sparse multitask Lasso when sparsity patterns are not totally identical.",1. Introduction,[0],[0]
c),1. Introduction,[0],[0]
"On an important scientific problem of analyzing early Alzheimer’s disease (AD) individuals, we provide compelling experimental results showing consistent acceptance rate and statistical power.",1. Introduction,[0],[0]
"Via a publicly available software package, this will facilitate many multi-site regression analysis efforts in the short to medium term future.",1. Introduction,[0],[0]
Meta-analysis approaches.,1.1. Related Work,[0],[0]
"If datasets at multiple different sites cannot be shared or pooled, the task of deriving
meaningful scientific conclusions from results of multiple independently conducted analyses generally falls under the umbrella term of “meta analysis”.",1.1. Related Work,[0],[0]
The literature provides various strategies to cumulate the general findings from analyses on different datasets.,1.1. Related Work,[0],[0]
"But even experts believe that, minor violations of assumptions can lead to misleading scientific conclusions (Greco et al., 2013), and substantial personal judgment (and expertise) is needed to conduct them.",1.1. Related Work,[0],[0]
"It is widely accepted that when the ability to pool the data is an option, simpler schemes may perform better.
",1.1. Related Work,[0],[0]
Domain adaptation/shift.,1.1. Related Work,[0],[0]
"Separately, the idea of addressing “shift” within datasets has been rigorously studied within statistical machine learning, see (Patel et al., 2015; Li, 2012).",1.1. Related Work,[0],[0]
"For example, domain adaptation, including dataset and covariate shift, seeks to align (the distributions of) multiple datasets to enable follow-up processing (Ben-David & Schuller, 2003).",1.1. Related Work,[0],[0]
"Typically, such algorithms assume a bias in the sampling process, and adopt reweighting as the solution (Huang et al., 2007; Gong et al., 2013).",1.1. Related Work,[0],[0]
"Alternatively, a family of such methods assume that sites (or datasets) differ due to feature distortions (e.g., calibration error), which are resolved, in general, by minimizing some distance measure between appropriate distributions (Baktashmotlagh et al., 2013; Pan et al., 2011; Long et al., 2015; Ganin et al., 2016).",1.1. Related Work,[0],[0]
"In general, these approaches have nice theoretical properties (Ben-David et al., 2010; Cortes & Mohri, 2011; Zhou et al., 2016).",1.1. Related Work,[0],[0]
"However, it is important to note that the domain adaptation literature focuses on the algorithm itself – to resolve the distributional site-wise differences.",1.1. Related Work,[0],[0]
"It does not address the issue of whether pooling the datasets, after applying the calculated adaptation (i.e., transformation), is beneficial.",1.1. Related Work,[0],[0]
Our goal in this work is to assess whether multiple datasets can be pooled — either before or usually after applying the best domain adaptation methods — for improving our estimation of the relevant coefficients within linear regression.,1.1. Related Work,[0],[0]
"We propose a hypothesis test to directly address this question.
",1.1. Related Work,[0],[0]
The high-dimensional case.,1.1. Related Work,[0],[0]
"High dimensional scenarios, in general, involve predicting a response (e.g., cognitive score) from high dimensional predictors such as image scans (or derived features) and genetic data, which in general, entails Lasso-type formulations unlike the classical regression models.",1.1. Related Work,[0],[0]
"Putting multi-task representation learning (Maurer et al., 2016; Ando & Zhang, 2005; Maurer et al., 2013) together with a sparsity regularizer, we get the multitask Lasso model (Liu et al., 2009; Kim & Xing, 2010).",1.1. Related Work,[0],[0]
"Although this seems like a suitable model (Chen et al., 2012), it assumes that the multiple tasks (sites here) have an identical active set of predictors.",1.1. Related Work,[0],[0]
"Instead, we find that the sparse multi-task Lasso (Lee et al., 2010), roughly, a multitask version of sparse group Lasso (Simon et al., 2013; Lee et al., 2010) is a better starting point.",1.1. Related Work,[0],[0]
"There is no theoretical analysis in (Simon et al., 2013); although a `2-consistency
for sparse group lasso is derived in (Chatterjee et al., 2012) using a general proof procedure for M-estimators, it does not take into account the specific sparse group Lasso properties.",1.1. Related Work,[0],[0]
"This makes the result non-informative for sparse group Lasso (much less, sparse multi-task Lasso).",1.1. Related Work,[0],[0]
"Specifically, as we will see shortly, in sparse multi-task Lasso, the joint effects of two penalties induces a special type of asymmetric structure.",1.1. Related Work,[0],[0]
"We show a new result, in the style of Lasso (Meinshausen & Yu, 2009; Liu & Zhang, 2009b), for `2 convergence rate for this model.",1.1. Related Work,[0],[0]
"It matches known results for Lasso/group Lasso, and identifies regimes where the sparse multi-task (multi-site) setting is advantageous.
",1.1. Related Work,[0],[0]
Simultaneous High dimensional Inference.,1.1. Related Work,[0],[0]
"Simultaneous high dimensional inference models such as multi sample-splitting and de-biased Lasso is an active research topic in statistics (Dezeure et al., 2015).",1.1. Related Work,[0],[0]
Multi samplesplitting use half of the dataset for variable selection and the rest for calculating p-values.,1.1. Related Work,[0],[0]
De-biased Lasso chooses one feature as a response and the others as predictors to estimate a Lasso model; this procedure is repeated for each feature.,1.1. Related Work,[0],[0]
"Estimators from De-biased Lasso asymptotically follow the multi-normal distribution (Dezeure et al., 2016), and using Bonferroni-Holm adjustment produces simultaneous p-values.",1.1. Related Work,[0],[0]
"Such ideas together with the `2- convergence results for sparse multitask Lasso, will help extend our analysis to the high dimensional setting.",1.1. Related Work,[0],[0]
We first describe a simple setting where one seeks to apply standard linear regression to data pooled from multiple sites.,2. Hypothesis Test for Multi-Site Regression,[0],[0]
"For presentation purposes, we will deal with variable selection issues later.",2. Hypothesis Test for Multi-Site Regression,[0],[0]
"Within this setup, we will introduce our main result — a hypothesis test to evaluate statistical power improvements (e.g., mean squared error) when running a regression model on a pooled dataset.",2. Hypothesis Test for Multi-Site Regression,[0],[0]
"We will see that the proposed test is transparent to the use of adaptation algorithms, if any, to pre-process the multi-site data (more details in appendix).",2. Hypothesis Test for Multi-Site Regression,[0],[0]
"In later sections, we will present convergence analysis and extensions to the large p setting.",2. Hypothesis Test for Multi-Site Regression,[0],[0]
Matrices (vectors/scalars) are upper case (and lower case).,2. Hypothesis Test for Multi-Site Regression,[0],[0]
"‖.‖∗ is the nuclear norm.
",2. Hypothesis Test for Multi-Site Regression,[0],[0]
We first introduce the single-site regression model.,2. Hypothesis Test for Multi-Site Regression,[0],[0]
Let X ∈ Rn×p and y ∈ Rn×1 denote the feature matrix of predictors and the response vector respectively.,2. Hypothesis Test for Multi-Site Regression,[0],[0]
"If β corresponds to the coefficient vector (i.e., predictor weights), then the regression model is
min β
1 n ‖y",2. Hypothesis Test for Multi-Site Regression,[0],[0]
"−Xβ‖22 (1)
where y = Xβ∗ +",2. Hypothesis Test for Multi-Site Regression,[0],[0]
"and ∼ N(0, σ2) I.I.D. if β∗ is the true coefficient vector from which y is generated.",2. Hypothesis Test for Multi-Site Regression,[0],[0]
"The mean-squared error (MSE) and `2-consistency of regres-
sion is well-known.",2. Hypothesis Test for Multi-Site Regression,[0],[0]
The mean-squared error (MSE) of (1) is E‖β̂ − β∗‖22 = tr ( (XTX)−1 ) σ2.,2. Hypothesis Test for Multi-Site Regression,[0],[0]
"If k denotes the number of sites, then one may first apply a domain adaptation scheme to account for the distributional shifts between the k different predictors {Xi}ki=1, and then run a regression model.",2. Hypothesis Test for Multi-Site Regression,[0],[0]
"If the underlying “concept” (i.e., predictors and responses relationship) can be assumed to be the same across the different sites, then it is reasonable to impose the same β for all sites.",2. Hypothesis Test for Multi-Site Regression,[0],[0]
"For instance, the influence of CSF protein measurements on cognitive scores of an individual may be invariant to demographics.",2. Hypothesis Test for Multi-Site Regression,[0],[0]
"Nonetheless, if the distributional mismatch correction is imperfect, we may define ∆βi = βi − β∗ where i ∈ {1, . . .",2. Hypothesis Test for Multi-Site Regression,[0],[0]
", k} as the residual difference between the site-specific coefficients and the true shared coefficient vector (in the ideal case, we have ∆βi = 0).",2. Hypothesis Test for Multi-Site Regression,[0],[0]
"In the multi-site setting, we can write
min β k∑ i=1",2. Hypothesis Test for Multi-Site Regression,[0],[0]
"τ2i ‖yi −Xiβ‖22 (2)
where for each site i we have yi = Xiβ∗+Xi∆βi+ i",2. Hypothesis Test for Multi-Site Regression,[0],[0]
"and i ∼ N (0, σ2i )",2. Hypothesis Test for Multi-Site Regression,[0],[0]
"I.I.D. Here, τi is a weighting parameter for each site, if such information is available.
",2. Hypothesis Test for Multi-Site Regression,[0],[0]
Our main goal is to test if the combined regression improves the estimation for a single site.,2. Hypothesis Test for Multi-Site Regression,[0],[0]
We can pose this question in terms of improvements in the mean squared error (MSE).,2. Hypothesis Test for Multi-Site Regression,[0],[0]
"Hence, W.L.O.G. using site 1 as the reference, we set τ1 = 1 in (2) and consider β∗ = β1,
min β ‖y1 −X1β‖22 + k∑ i=2",2. Hypothesis Test for Multi-Site Regression,[0],[0]
"τ2i ‖yi −Xiβ‖22 (3)
β1 β2
β̂
Figure 1.",2. Hypothesis Test for Multi-Site Regression,[0],[0]
β1 and β2 are 1st and 2nd site coefficients.,2. Hypothesis Test for Multi-Site Regression,[0],[0]
"After combination, β1’s bias increases but variance reduces, resulting in a smaller MSE.
",2. Hypothesis Test for Multi-Site Regression,[0],[0]
"Clearly, when the sample size is not large enough, the multi-site formulation in (3) may reduce variance significantly, because of the averaging effect in the objective function, while increasing the bias by a little.",2. Hypothesis Test for Multi-Site Regression,[0],[0]
"This reduces the Mean Squared Error (MSE), see Figure 1.",2. Hypothesis Test for Multi-Site Regression,[0],[0]
"Note that while traditionally, the unbiasedness property was desirable, an extensive body of literature on ridge regression suggests that the quantity of interest should really be E ‖β̂",2. Hypothesis Test for Multi-Site Regression,[0],[0]
"−
β∗‖22 (Hoerl & Kennard, 1970; James & Stein, 1961).",2. Hypothesis Test for Multi-Site Regression,[0],[0]
These ideas are nicely studied within papers devoted to the “bias-variance” trade-off.,2. Hypothesis Test for Multi-Site Regression,[0],[0]
"Similar to these results, we will focus on the mean squared error because the asymptotic consistency properties that come with an unbiased estimator are not meaningful here anyway — the key reason we want to pool datasets in the first place is because of small sample sizes.",2. Hypothesis Test for Multi-Site Regression,[0],[0]
"We now provide a result showing how the tuning parameters τ2, . . .",2. Hypothesis Test for Multi-Site Regression,[0],[0]
", τk can be chosen.
",2. Hypothesis Test for Multi-Site Regression,[0],[0]
"Theorem 2.1 τi = σ1σi achieves the smallest variance in β̂.
Remarks: This result follows from observing that the each site’s contribution is inversely proportional to site-specific noise level, σi.",2. Hypothesis Test for Multi-Site Regression,[0],[0]
We will show that this choice of τis also leads to a simple mechanism to setup a hypothesis test.,2. Hypothesis Test for Multi-Site Regression,[0],[0]
"In the specification above, the estimates of βi across all k sites are restricted to be the same.",2.1. Sharing all βs,[0],[0]
"Without this constraint, (3) is equivalent to fitting a regression separately on each site.",2.1. Sharing all βs,[0],[0]
"So, a natural question is whether this constraint improves estimation.",2.1. Sharing all βs,[0],[0]
"To evaluate whether MSE is reduced, we first need to quantify the change in the bias and variance of (3) compared to (1).",2.1. Sharing all βs,[0],[0]
"To do so, we introduce a few notations.",2.1. Sharing all βs,[0],[0]
"Let ni be the sample size of site i, and let β̂i denote the regression estimate from a specific site i. We have ∆β̂i = β̂i",2.1. Sharing all βs,[0],[0]
− β̂1.,2.1. Sharing all βs,[0],[0]
"We define the length kp vector ∆βT as ∆βT = (∆βT2 , ...,∆β T k ) (similarly for ∆β̂
T ).",2.1. Sharing all βs,[0],[0]
"We use Σ̂i for the sample covariance matrix of the data (predictors) from the site i and G ∈ R(k−1)p×(k−1)p for the covariance matrix of ∆β̂, where Gii = (n1Σ̂1)−1 + (niτ2i Σ̂i)
−1 and Gij = (n1Σ̂1) −1",2.1. Sharing all βs,[0],[0]
whenever i 6=,2.1. Sharing all βs,[0],[0]
"j.
Let the difference in bias and variance between the single site model in (1) and the multi-site model in (3) be Biasβ and V arβ respectively.",2.1. Sharing all βs,[0],[0]
Let Σ̂k2 = ∑k i=2,2.1. Sharing all βs,[0],[0]
niτ 2,2.1. Sharing all βs,[0],[0]
i Σ̂i and Σ̂ k 1 = n1Σ̂1 +,2.1. Sharing all βs,[0],[0]
Σ̂,2.1. Sharing all βs,[0],[0]
k 2 .,2.1. Sharing all βs,[0],[0]
"We have,
Lemma 2.2 For model (3), we have
‖Biasβ‖22 ‖G−1/2∆β‖22 ≤ ‖(Σ̂k1)−2(Σ̂k2(n1Σ̂1)−1Σ̂k2",2.1. Sharing all βs,[0],[0]
"+ Σ̂k2)‖∗, (4)
V arβ = σ 2 1 ∥∥∥(n1Σ̂1)−1",2.1. Sharing all βs,[0],[0]
− (n1Σ̂1 + Σ̂k2)−1∥∥∥ ∗ .,2.1. Sharing all βs,[0],[0]
"(5)
Remarks: The above result bounds the increase in bias and the reduction in variance (see discussion of Figure 1).",2.1. Sharing all βs,[0],[0]
"Since our goal is to test MSE reduction — in principle, we can use bootstrapping to calculate MSE approximately.",2.1. Sharing all βs,[0],[0]
This procedure has a significant computational footprint.,2.1. Sharing all βs,[0],[0]
"Instead, (4) (from a one-step Cauchy-Schwartz inequality), gives a sufficient condition for MSE reduction as shown below.
",2.1. Sharing all βs,[0],[0]
"Theorem 2.3 a) Model (3) has smaller MSE of β̂ than model (1) whenever
H0 : ‖G−1/2∆β‖22 ≤ σ21 .",2.1. Sharing all βs,[0],[0]
"(6)
b)",2.1. Sharing all βs,[0],[0]
"Further, we have the following test statistic,∥∥∥∥∥G−1/2∆β̂σ1 ∥∥∥∥∥ 2
2
∼ χ2(k−1)∗p (∥∥∥∥G−1/2∆βσ1 ∥∥∥∥2 2 ) , (7)
where ‖G−1/2∆β/σ1‖2 is called a “condition value”.
",2.1. Sharing all βs,[0],[0]
Remarks:,2.1. Sharing all βs,[0],[0]
This is our main test result.,2.1. Sharing all βs,[0],[0]
"Although σi is typically unknown, it can be easily replaced using its sitespecific estimation.",2.1. Sharing all βs,[0],[0]
Theorem 2.3 implies that we can conduct a non-central χ2 distribution test based on the statistic.,2.1. Sharing all βs,[0],[0]
"Also, (6) shows that the non-central χ2 distribution, which the test statistics will follow, has a non-central parameter smaller than 1 when the sufficient condition H0 holds.",2.1. Sharing all βs,[0],[0]
"Meanwhile, in obtaining the (surprisingly simple) sufficient condition H0, no other arbitrary assumption is needed except the application of Cauchy-Schwartz inequality.",2.1. Sharing all βs,[0],[0]
"From a practical perspective, Theorem 2.3 implies that the sites, in fact, do not even need to share the full dataset to assess whether pooling will be useful.",2.1. Sharing all βs,[0],[0]
"Instead, the test only requires very high-level information such as β̂i, Σ̂i, σi and ni for all participating sites – which can be transferred very cheaply with no additional cost of data storage, or privacy implications.",2.1. Sharing all βs,[0],[0]
"The following result deals with the special case where we have two participating sites.
",2.1. Sharing all βs,[0],[0]
"Corollary 2.4 For the case where we have two participating sites, the condition (6) from Theorem 2.3 reduces to
H0 : ∆β T ((n1Σ̂1) −1",2.1. Sharing all βs,[0],[0]
+ (n2τ 2 2 Σ̂2) −1)−1∆β ≤ σ21 .,2.1. Sharing all βs,[0],[0]
"(8)
Remarks: The left side above relates to the Mahalanobis distance between β1, β2 with covariance (n1Σ̂1)−1 + (n2τ 2 2 Σ̂2)
−1, implying that the test statistic is a type of a normalized metric between the two regression models.",2.1. Sharing all βs,[0],[0]
"In numerous pooling scenarios, we are faced with certain systemic differences in the way predictors and responses associate across sites.",2.2. Sharing a subset of βs,[0],[0]
"For example, socio-economic status may (or may not) have a significant association with a health outcome (response) depending on the country of the study (e.g., due to insurance coverage policies).",2.2. Sharing a subset of βs,[0],[0]
"Unlike in Section 2.1, we now relax the restriction that all coefficients are the same across sites, see Fig. 2.",2.2. Sharing a subset of βs,[0],[0]
"The model in (3) will now include another design matrix of predictors Z ∈ Rn×q and corresponding coefficients γi for each site i,
min β,γ k∑ i=1",2.2. Sharing a subset of βs,[0],[0]
"τ2i ‖yi −Xiβ − Ziγi‖22 (9)
yi = Xiβ ∗",2.2. Sharing a subset of βs,[0],[0]
+Xi∆βi +,2.2. Sharing a subset of βs,[0],[0]
"Ziγ ∗ i + i, τ1 = 1 (10)
",2.2. Sharing a subset of βs,[0],[0]
Our goal is still to evaluate whether the MSE of β reduces.,2.2. Sharing a subset of βs,[0],[0]
We do not take into account the MSE change in γ because they correspond to site-specific variables.,2.2. Sharing a subset of βs,[0],[0]
"For estimation, β̂ can first be computed from (9).",2.2. Sharing a subset of βs,[0],[0]
"Treating it as a fixed entity now, γ̂i can be computed using yi and Zi on each site independently.",2.2. Sharing a subset of βs,[0],[0]
"Clearly, if β̂ is close to the “true” β∗, it will also enable a better estimation of site-specific variables.",2.2. Sharing a subset of βs,[0],[0]
"It turns out that, if Σ̂is are replaced by the conditional covariance, the analysis from Section 2.1 still holds for this case.
",2.2. Sharing a subset of βs,[0],[0]
"Specifically, let Σ̂abi be the sample covariance matrix between features a and b from some site i.",2.2. Sharing a subset of βs,[0],[0]
"We have,
Theorem 2.5 Analysis in Section 2.1 holds for β in (9) by replacing Σ̂i with Σ̃i = Σ̂xxi − Σ̂xzi(Σ̂zzi)−1Σ̂zxi
Remarks: The test now allows evaluating power improvements focused only on the subset of coefficients that is shared and permits site-specific confounds.",2.2. Sharing a subset of βs,[0],[0]
"For example, we can test which subset of parameters might benefit from parameter estimation on pooled data from multiple sites.",2.2. Sharing a subset of βs,[0],[0]
We now describe our analysis of pooling multi-site data in the high-dimensional setting where p n.,3. Pooling in High Dimensional Regression,[0],[0]
The challenge here is that variable section has to be a first order concern.,3. Pooling in High Dimensional Regression,[0],[0]
"In classical regression, `2 consistency properties are well known and so our focus in Section 2 was devoted to deriving sufficient conditions for the hypothesis test.",3. Pooling in High Dimensional Regression,[0],[0]
"In other words, imposing the same β across sites works in (3) because we understand its consistency.",3. Pooling in High Dimensional Regression,[0],[0]
"In contrast, here, one cannot enforce a shared β for all sites before the active set of predictors within each site are selected — directly imposing the same β leads to a loss of `2-consistency, making follow-up analysis problematic.",3. Pooling in High Dimensional Regression,[0],[0]
"Therefore, once a suitable model for high-dimensional multi-site regression is chosen, the first requirement is to characterize its consistency.
",3. Pooling in High Dimensional Regression,[0],[0]
"We start with the multi-task Lasso (a special case of group Lasso) (Liu et al., 2009), where the authors show that the strategy selects better explanatory features compared to separately fitting Lasso on each site.",3. Pooling in High Dimensional Regression,[0],[0]
"But this algorithm underperforms when the sparsity pattern of the predictors is not identical across sites, so we use a recent variant called sparse multi-task Lasso (Lee et al., 2010) – essentially substituting “sites” for “tasks”.",3. Pooling in High Dimensional Regression,[0],[0]
"The sparse multi-site Lasso in p n setting (p is the number of predictors) is given as
B̂λ = arg min β k∑ i=1",3. Pooling in High Dimensional Regression,[0],[0]
‖yi,3. Pooling in High Dimensional Regression,[0],[0]
"−Xiβi‖22 + λΛ(B) (11)
Λ(B)",3. Pooling in High Dimensional Regression,[0],[0]
= α p∑ j=1,3. Pooling in High Dimensional Regression,[0],[0]
"‖βj‖1 + (1− α) √ k p∑ j=1 ‖βj‖2, (12)
where λ is the Lasso regularization parameter.",3. Pooling in High Dimensional Regression,[0],[0]
"Here, B ∈
Rk×p is a matrix where the ith row gives the coefficients from ith site (k sites in total).",3. Pooling in High Dimensional Regression,[0],[0]
"Also, βi with subscript denotes the ith row (site) of B, we use βj with superscript to give the j-th column (coefficients) of B. The hyperparameter α ∈",3. Pooling in High Dimensional Regression,[0],[0]
"[0, 1] balances the two penalties (and will be used shortly); a larger α weighs the `1 penalty more and a smaller α puts more weight on the grouping.",3. Pooling in High Dimensional Regression,[0],[0]
"Similar to a Lasso-based regularization parameter, λ here will produce a solution path (to select coefficients) for a given α.",3. Pooling in High Dimensional Regression,[0],[0]
"We first address the consistency behavior of the sparse multisite Lasso in (11), which was not known in the literature.",3. Pooling in High Dimensional Regression,[0],[0]
"Our analysis of (11) is related to known results for Lasso (Meinshausen & Yu, 2009) and the group Lasso (Liu & Zhang, 2009a).",3.1. `2 consistency,[0],[0]
"Recall that X1, . . .",3.1. `2 consistency,[0],[0]
", Xk are the data matrices from k sites.",3.1. `2 consistency,[0],[0]
"We define n̄ = maxki=1{ni} and C = n̄−1DIAG(XT1 X1, ..., X T k Xk) where DIAG(A,B) corresponds to constructing a block-diagonal matrix with A and B as blocks on the diagonal.",3.1. `2 consistency,[0],[0]
"We require the following useful properties of C (‖·‖0 denotes `0-norm).
",3.1. `2 consistency,[0],[0]
"Definition 3.1 The m-sparse minimal and maximal eigenvalues of C, denoted by φmin(m) and φmax(m), are
min ν:‖ν‖0≤dme
νTCν
νT ν and max ν:‖ν‖0≤dme
νTCν
νT ν",3.1. `2 consistency,[0],[0]
"(13)
We call a feature “active” if its coefficient is non-zero.",3.1. `2 consistency,[0],[0]
"Now, each site may have different active features: let sh ≤ kp be the sum of the number of active features over all sites.",3.1. `2 consistency,[0],[0]
"Similarly, sp is the cardinality of the union of features that are active in at least one site (sh ≤ ksp, sp ≤ p).",3.1. `2 consistency,[0],[0]
"Recall that when α 6= 0, we add the Lasso penalty to the multi-site Lasso penalty.",3.1. `2 consistency,[0],[0]
"When the sparsity patterns are assumed to be similar across all sites, α is small.",3.1. `2 consistency,[0],[0]
"In contrast, to encourage site-specific sparsity patterns, we may set α to be large.",3.1. `2 consistency,[0],[0]
"We now analyze these cases independently.
",3.1. `2 consistency,[0],[0]
Theorem 3.2 Let 0 ≤ α ≤ 0.4.,3.1. `2 consistency,[0],[0]
Assume there exist constants 0,3.1. `2 consistency,[0],[0]
≤ ρmin ≤,3.1. `2 consistency,[0],[0]
"ρmax ≤ ∞ such that
lim inf n→∞ φmin
( sp ( 1 + 2α
1− 2α
)2)",3.1. `2 consistency,[0],[0]
"≥ ρmin
lim sup n→∞",3.1. `2 consistency,[0],[0]
φmax(sp + min{ k∑ i=1,3.1. `2 consistency,[0],[0]
"ni, kp}) ≤",3.1. `2 consistency,[0],[0]
"ρmax.
(14)
",3.1. `2 consistency,[0],[0]
"Then, for λ ∝ σ √ n̄ log(kp), there exists a constant ω > 0
such that, with probability converging to 1 for n→∞,
1 k ‖B̂λ −B∗‖2F ≤ ωσ2 s̄ log(kp) n̄ , (15)
where s̄ = {(1−α)√sp+α √ sh/k}2, σ is the noise level.
",3.1. `2 consistency,[0],[0]
"Remarks: The above result agrees with known results for multi-task Lasso (Liu et al., 2009; Liu & Zhang, 2009b) when the sparsity patterns are the same across sites.",3.1. `2 consistency,[0],[0]
"The simplest way to interpret Theorem 3.2 is via the ratio r = sh sp
.",3.1. `2 consistency,[0],[0]
"Here, r = k when the sparsity patterns are the same across sites.",3.1. `2 consistency,[0],[0]
"As r decreases, the sparsity patterns across sites start to differ, in turn, the sparse multi-site Lasso from (11) will provide stronger consistency compared to the multi-site Lasso (which corresponds to α = 0).",3.1. `2 consistency,[0],[0]
"In other words, whenever we expect site-specific active features, the `2 consistency of (11) will improve as one includes an additional `1-penalty together with multi-site Lasso.
",3.1. `2 consistency,[0],[0]
"Observe that for the non-sparse βj , we can verify that ‖βj‖1 and √ k‖βj‖2 have the same scale.",3.1. `2 consistency,[0],[0]
"On the other hand, for sparse βj , ‖βj‖1 has the same scale as ‖βj‖2, i.e., with no √ k penalization (see appendix).",3.1. `2 consistency,[0],[0]
"Unlike Theorem 3.2 where the sparsity patterns across sites are similar, due to this scaling issue, the parameters α and λ need to be ‘corrected’ for the setting where sparsity patterns have little overlap.",3.1. `2 consistency,[0],[0]
"We denote these corrected versions by α̃ = α
(1−α) √ k+α
and λ̃ = ((1− α) √ k + α)λ.
",3.1. `2 consistency,[0],[0]
Theorem 3.3 Let 0.4 ≤ α̃ ≤ 1.,3.1. `2 consistency,[0],[0]
Assume there exist constants 0,3.1. `2 consistency,[0],[0]
≤ ρmin ≤,3.1. `2 consistency,[0],[0]
"ρmax ≤ ∞ such that
lim inf",3.1. `2 consistency,[0],[0]
"n→∞ φmin
( sh ( 1 +
(1− α̃) α̃
)2) ≥ ρmin
lim sup n→∞ φmax(sh + min{ k∑ i=1 ni, kp}) ≤",3.1. `2 consistency,[0],[0]
"ρmax.
(16)
",3.1. `2 consistency,[0],[0]
"Then, for λ̃ ∝ σ",3.1. `2 consistency,[0],[0]
"√ n̄ log(kp), there exists ω > 0",3.1. `2 consistency,[0],[0]
"such that,
with probability converging to 1 for n → ∞, we have (15) with s̃ = {(1− α̃) √ sp/k + α̃ √ sh/k}2 instead of s̄.
Remarks: This result agrees with known results for Lasso (Meinshausen & Yu, 2009) when the sparsity patterns are completely different across sites.",3.1. `2 consistency,[0],[0]
"In this case (i.e., α is large), the sparse multi-site Lasso has stronger consistency compared to Lasso (α = 1).",3.1. `2 consistency,[0],[0]
The sparse multi-site Lasso is preferable as r = shsp increases.,3.1. `2 consistency,[0],[0]
"Note that although α̃ and λ̃ are used for the results instead of α and λ, in practice, one can simply scale the chosen αs appropriately, e.g., with k = 100, we see that α ≈ 0.99 corresponds to α̃ = 0.95.
",3.1. `2 consistency,[0],[0]
Performing hypothesis tests: Theorems 3.2 and 3.3 show consistency of sparse multi-site Lasso estimation.,3.1. `2 consistency,[0],[0]
"Hence, if the hyper-parameters α and λ are known, we can estimate the coefficients B∗. This variable selection phase can be followed by a hypothesis test, similar to Theorem 2.3 from Section 2.",3.1. `2 consistency,[0],[0]
The only remaining issue is the choice of α.,3.1. `2 consistency,[0],[0]
"The existing methods show that joint crossvalidation for α and λ performs unsatisfactorily and instead use a heuristic: set it to 0.05 when it is known that sparsity
patterns are similar across sites and 0.95 otherwise (Simon et al., 2013).",3.1. `2 consistency,[0],[0]
"Below, instead of a fixed α, we provide a data-driven alternative that works well in practice.
",3.1. `2 consistency,[0],[0]
Choosing α using simultaneous inference: Our results in Thm. 3.2 (and Thm. 3.3 resp.) seem to suggest that increasing (and decreasing resp.),3.1. `2 consistency,[0],[0]
"α will always improve consistency; however, this ends up requiring stronger msparsity conditions.",3.1. `2 consistency,[0],[0]
We now describe a procedure to choose α.,3.1. `2 consistency,[0],[0]
"First, recall that an active feature corresponds to a variable with non-zero coefficient.",3.1. `2 consistency,[0],[0]
"We call a feature “siteactive” if it is active at a site, an “always-active” feature is active at all k sites.",3.1. `2 consistency,[0],[0]
The proposed solution involves three steps.,3.1. `2 consistency,[0],[0]
"(1) First, we apply simultaneous inference (like multi sample-splitting or de-biased Lasso) using all features at each of the k sites with FWER control.",3.1. `2 consistency,[0],[0]
"This step yields “site-active” features for each site, and therefore, gives the set of always-active features and the sparsity patterns.",3.1. `2 consistency,[0],[0]
"(2) Then, each site runs a Lasso and chooses a λi based on cross-validation.",3.1. `2 consistency,[0],[0]
We then set λmulti−site to be the minimum among the best λ’s from each site.,3.1. `2 consistency,[0],[0]
"Using λmulti−site, we can vary α to fit various sparse multi-site Lasso models – each run will select some number of always-active features.",3.1. `2 consistency,[0],[0]
We plot α versus the number of always-active features.,3.1. `2 consistency,[0],[0]
"(3) Finally, based on the sparsity patterns from the site-active set, we estimate whether the sparsity patterns across sites are similar or different (i.e., share few active features).",3.1. `2 consistency,[0],[0]
"Then, based on the plot from step (2), if the sparsity patterns from the site-active sets are different (similar) across sites, then the smallest (largest) value of α that selects the minimum (maximum) number of always-active features is chosen.",3.1. `2 consistency,[0],[0]
The appendix includes details.,3.1. `2 consistency,[0],[0]
Our experiments are two-fold.,4. Experiments,[0],[0]
First we perform simulations evaluating the hypothesis test from §2 and sparse multi-site Lasso from §3.,4. Experiments,[0],[0]
"We then evaluate pooling two Alzheimer’s disease (AD) datasets from different studies to evaluate improvements in power, and checking whether the proposed tests provide insights into the regimes when pooling is beneficial for regression, and will yield tangible statistical benefits in investigating scientific hypotheses.
",4. Experiments,[0],[0]
Power and Type I Error of Theorem 2.3:,4. Experiments,[0],[0]
The first set of simulations evaluate the setting from Section 2.1 where the coefficients are same across two different sites.,4. Experiments,[0],[0]
"The inputs for the two sites are set as X1, X2(∈ Rn×3) ∼ N (0,Σ) with Σ = 0.5(I + E) (where I is identity and E is a 3 × 3 matrix of 1s).",4. Experiments,[0],[0]
"The true coefficients are given by β1 ∼ U(0, 4I) and β2 = β1+0.1 (where U(·) is multivariate uniform), and the noise corresponds to 1 ∼ N (0, 3I) and 2 ∼ N (0, 0.5I) for the two sites respectively.",4. Experiments,[0],[0]
"With this design, the responses are set as y1 = X1β1 + 1 and y2 = X2β2 + 2.",4. Experiments,[0],[0]
"Using {X1, y1} and {X2, y2}, the shared
β̂ are estimated.",4. Experiments,[0],[0]
"The simulation is repeated 100 times with 9 different sample sizes (n = 2b with b = 4, . . .",4. Experiments,[0],[0]
", 12) for each repetition.",4. Experiments,[0],[0]
Fig. 3(a) shows the MSE of two-site (blue bars) and a baseline single-site (red bars) model computed using the corresponding β̂s on site 1.,4. Experiments,[0],[0]
"Although both MSEs decrease as n increases, the two-sites model consistently produces smaller MSE – with large gains for small sample sizes (left-end of Fig. 3(a)).",4. Experiments,[0],[0]
Fig. 3(d) shows the acceptance rates of our proposed hypothesis test (from (6) and (8)) with 0.05 significance level.,4. Experiments,[0],[0]
"The purple solid line is the sufficient condition from Theorem 2.3, while the dotted line is where the MSE of the baseline single-site model starts to decrease below that of the two-site model.",4. Experiments,[0],[0]
"The trend in Fig. 3(d) implies that as n increases, the test tends to reject pooling the multi-site data with power→ 1.",4. Experiments,[0],[0]
"Further, the type I error is well-controlled to the left of the solid line, and is low between the two lines.",4. Experiments,[0],[0]
"See appendix for additional details about Figs. 3(a,d).
",4. Experiments,[0],[0]
Power and Type I Error of Theorem 2.5:,4. Experiments,[0],[0]
The second set of simulations evaluate the confounding variables setup from Section 2.2.,4. Experiments,[0],[0]
"Similar to Section 4, here we have (X1, Z1), (X2, Z2) ∼ N (0,Σ) with Σ =(
0.5I3×3 + 0.5E3×3, 0.2E3×5 0.2E5×3, 0.8I5×5 + 0.2E5×5
) .",4. Experiments,[0],[0]
"β1 and
β2 are the same as before.",4. Experiments,[0],[0]
"γ1 = (1, 1, 2, 2, 2)T and γ2 = (2, 2, 2, 1, 1)
T are the coefficients for Z1 and Z2 respectively.",4. Experiments,[0],[0]
The new responses y1 and y2 will have the extra terms Z1γ1 and Z2γ2 respectively.,4. Experiments,[0],[0]
"Fig. 3(b,e) shows the results.",4. Experiments,[0],[0]
"All the observations from Fig. 3(a,d) hold here as well.",4. Experiments,[0],[0]
"For small n, MSE of two-site model is much smaller than baseline, and as sample size increases this difference reduces.",4. Experiments,[0],[0]
"The test accepts with high probability for small n, and as sample size increases it rejects with high power.",4. Experiments,[0],[0]
"The regimes of low type I error and high power in Fig. 3(e) are similar to those from Fig. 3(d).
4.1.",4. Experiments,[0],[0]
"Sparse multi-sites Lasso `2-consistency
We now use 4 sites with n = 150 samples each and p = 400 features to test the sparse multi-site model from §3.",4. Experiments,[0],[0]
"We set the design matrices Xi (i = 1, . . .",4. Experiments,[0],[0]
", 4) ∼ N (0,Σ) with Σ = 0.8Ip×p + 0.2Ep×p.",4. Experiments,[0],[0]
"We consider the two cases (sparsity patterns shared/not shared) separately.
",4. Experiments,[0],[0]
Few sparsity patterns shared: 6 shared features and 14 site-specific features (out of the 400) are set to be active in 4 sites.,4. Experiments,[0],[0]
"Each shared feature is sampled from U(0, 4) for the first two sites and U(0, 0.5) for the rest.",4. Experiments,[0],[0]
"All the sitespecific features are ∼ U(0, 4).",4. Experiments,[0],[0]
"The noise i ∼ N (0, 1), and the responses are yi = Xiβi + i. Fig. 3(c) shows the 10-fold cross validation error as λ changes (i.e., solution path) for different α settings, including the value from our proposed selection procedure (from Section 3.1), Lasso (α = 1), group Lasso (α = 0) and arbitrary values α = 0.05, 0.95 (as suggested by (Simon et al., 2013)).
",4. Experiments,[0],[0]
"Our chosen α = 0.97 (the blue curve in Fig. 3(c)) has the smallest error, across all λs, thereby implying a better `2 consistency.",4. Experiments,[0],[0]
"Table 1 in the appendix includes more details, including α = 0.97 discovers more always-active features, while preserving the ratio of correctly discovered active features to all the discovered ones.
",4. Experiments,[0],[0]
"Most sparsity patterns shared: Unlike the earlier case, here we set 16 shared and 4 site-specific features (both ∼ U(0, 4)) to be active among all 400 features.",4. Experiments,[0],[0]
"The result, shown in Fig. 3(f), is similar to Fig. 3(c).",4. Experiments,[0],[0]
The proposed choice of α = 0.25 competes favorably with alternate choices while preserving the correctly discovered number of always-active features.,4. Experiments,[0],[0]
"Unlike the previous case, the ratio of correctly discovered active features to all discovered ones increases here (see appendix).",4. Experiments,[0],[0]
We now evaluate whether two AD datasets acquired at different sites – an Alzheimer’s Disease Neuroimage Initiative (ADNI) dataset and a local dataset from Wisconsin,4.2. Combining AD datasets from multiple sites,[0],[0]
ADRC (ADlocal) can be combined (appendix has dataset details).,4.2. Combining AD datasets from multiple sites,[0],[0]
The sample sizes are 318 and 156 respectively.,4.2. Combining AD datasets from multiple sites,[0],[0]
"Cerebrospinal fluid (CSF) protein levels are the inputs, and the response is hippocampus volume.",4.2. Combining AD datasets from multiple sites,[0],[0]
"Using 81 age-matched samples from each dataset, we first perform domain adaptation (using a maximum mean discrepancy objective as a measure of distance between the two marginals), and then transform CSF proteins from ADlocal to match with ADNI.",4.2. Combining AD datasets from multiple sites,[0],[0]
The transformed data is then used to evaluate whether adding ADlocal data to ADNI will improve the regression performed on the ADNI data.,4.2. Combining AD datasets from multiple sites,[0],[0]
"This is done by training a regression model on the ‘transformed’ ADlocal and a subset of ADNI data, and then testing the resulting model on the remaining ADNI samples.",4.2. Combining AD datasets from multiple sites,[0],[0]
"We use two baseline models each of which are trained using – ADNI data alone; and non-transformed ADlocal (with ADNI subset).
",4.2. Combining AD datasets from multiple sites,[0],[0]
"Fig. 4(a,b) show the resulting mean prediction error (MPE) scaled by the estimated noise level in ADNI responses, and the corresponding acceptance rate (with significance level 0.05) respectively.",4.2. Combining AD datasets from multiple sites,[0],[0]
"The x-axis in Fig. 4(a,b) represents the size of ADNI subset used for training.",4.2. Combining AD datasets from multiple sites,[0],[0]
"As expected, the MPE reduces as this subset size increases.",4.2. Combining AD datasets from multiple sites,[0],[0]
"Most importantly, pooling after transformation (green bars) seems to be the most beneficial in terms of MPE reduction.",4.2. Combining AD datasets from multiple sites,[0],[0]
"As shown in Fig. 4(a), to the left of purple line where the subset size is smaller than ADlocal datasize, pooling the datasets improves estimation.",4.2. Combining AD datasets from multiple sites,[0],[0]
This is the small sample size regime which necessitates pooling efforts in general.,4.2. Combining AD datasets from multiple sites,[0],[0]
"As the dataset size increases (to the right of x-axis in Fig. 4(a)) the resulting MPE for the pooled model is close to what we will achieve using the ADNI data by itself.
",4.2. Combining AD datasets from multiple sites,[0],[0]
"Since pooling after transformation is at least as good as us-
ing ADNI data alone, our hypothesis test accepts the combination with high rate (≈ 95%), see Fig. 4(b).",4.2. Combining AD datasets from multiple sites,[0],[0]
"The test rejects the pooling strategy with high power for combining before domain adaptation (see Fig. 4(b)), as one would expect.",4.2. Combining AD datasets from multiple sites,[0],[0]
"This rejection power increases rapidly as sample size increases, see red curve in Fig. 4(b).",4.2. Combining AD datasets from multiple sites,[0],[0]
"The results in Fig. 4(c,d) show the setting where one cannot change the dataset sizes at the sites i.e., the training set uses an equal number of labeled samples from both the ADNI and ADlocal (x-axis in Fig. 4(c)), and the testing set always corresponds to 20% of ADNI data.",4.2. Combining AD datasets from multiple sites,[0],[0]
"This is a more interesting scenario for a practitioner compared to Fig. 4(a,b), because in Fig. 4(c,d) we use same sample sizes for both datasets.",4.2. Combining AD datasets from multiple sites,[0],[0]
"The trends in Fig. 4(c,d) are the same as Fig. 4(a,b).",4.2. Combining AD datasets from multiple sites,[0],[0]
"We present a hypothesis test to answer whether pooling multiple datasets acquired from different sites is guaran-
teed to increase statistical power for regression models.",5. Conclusions,[0],[0]
"For both standard and high dimensional linear regression, we identify regimes where such pooling is sensible, and show how such policy decisions can be made via simple checks executable on each site before any data transfer ever happens.",5. Conclusions,[0],[0]
"We also show empirical results by combining two Alzheimer’s disease datasets in the context of different regimes proposed by our analysis, and see that the regression fit improves as suggested by the theory.",5. Conclusions,[0],[0]
"The code is available at https://github.com/hzhoustat/ICML2017.
Acknowledgments: This work is supported by NIH grants R01 AG040396, R01 EB022883, UW CPCP AI117924, R01 AG021155, and NSF awards DMS 1308877, CAREER 1252725 and CCF 1320755.",5. Conclusions,[0],[0]
"The authors are grateful for partial support from UW ADRC AG033514, UW ICTR 1UL1RR025011 and funding from a UW-Madison/DZNE collaboration initiative.",5. Conclusions,[0],[0]
Many studies in biomedical and health sciences involve small sample sizes due to logistic or financial constraints.,abstractText,[0],[0]
"Often, identifying weak (but scientifically interesting) associations between a set of predictors and a response necessitates pooling datasets from multiple diverse labs or groups.",abstractText,[0],[0]
"While there is a rich literature in statistical machine learning to address distributional shifts and inference in multi-site datasets, it is less clear when such pooling is guaranteed to help (and when it does not) – independent of the inference algorithms we use.",abstractText,[0],[0]
"In this paper, we present a hypothesis test to answer this question, both for classical and high dimensional linear regression.",abstractText,[0],[0]
"We precisely identify regimes where pooling datasets across multiple sites is sensible, and how such policy decisions can be made via simple checks executable on each site before any data transfer ever happens.",abstractText,[0],[0]
"With a focus on Alzheimer’s disease studies, we present empirical results showing that in regimes suggested by our analysis, pooling a local dataset with data from an international study improves power.",abstractText,[0],[0]
"When can Multi-Site Datasets be Pooled for Regression? Hypothesis Tests, 2-consistency and Neuroscience Applications",title,[0],[0]
"Proceedings of the SIGDIAL 2016 Conference, pages 360–369, Los Angeles, USA, 13-15 September 2016. c©2016 Association for Computational Linguistics",text,[0],[0]
"Studies about laughter in interaction have been mainly focused on the acoustic or perceptual features, and often observations of the events preceding to it have been the base for claims concerning what laughter is about.",1 Introduction,[0],[0]
"(Provine, 1993) made a claim that has been subsequently adopted in much of the literature: laughter is, for the most part, not related to humour, because it is found to most frequently follow banal comments.",1 Introduction,[0],[0]
Similar reasoning has been adopted by several other studies on the kind of situations that elicit laughter.,1 Introduction,[0],[0]
The deduction process in these studies rely on an important yet untested assumption: what laughter follows is what it is about.,1 Introduction,[0],[0]
Our paper investigates this assumption.,1 Introduction,[0],[0]
We first briefly discuss previous studies on laughter in interaction; we then argue for a semantic/pragmatic account in which we treat laughter as a gestural event anaphora referring to a laughable.,1 Introduction,[0],[0]
We present a corpus study of laughables and evaluate our results against previous proposals.,1 Introduction,[0],[0]
"In (Provine, 1993), the researcher observed natural conversations, and “when an observer heard laughter, she recorded in a notebook the comment immediately preceding the laughter and if the speaker and/or the audience laughed, the gender, and the estimated age of the speaker and the audience",1.1 Studies on what laughter is about,[0],[0]
[...].,1.1 Studies on what laughter is about,[0],[0]
A laugh episode was defined as the occurrence of audible laughter and included any laughter by speaker or audience that followed within an estimated 1 s of the initial laugh event.,1.1 Studies on what laughter is about,[0],[0]
The laugh episode included the last comment by a speaker if it occurred within an estimated 1 s preceding the onset of the initial laughter.,1.1 Studies on what laughter is about,[0],[0]
"A laugh episode was terminated if an estimated 1 s passed without speaker or audience laughter, or if either the speaker or the audience spoke.”.",1.1 Studies on what laughter is about,[0],[0]
"They found that “Only about 10-20% of episodes were estimated by the observers to be humorous” (Provine, 1993), and thus derived the conclusion which is now widely adopted in the literature: laughter is, for the most part, not related to humour but about social interaction.",1.1 Studies on what laughter is about,[0],[0]
"An additional conclusion based on this study is that laughter never interrupts speech but “punctuates” it occurring exclusively at phrase boundaries.
",1.1 Studies on what laughter is about,[0],[0]
"Similarly, (Vettin and Todt, 2004) used exclusively timing parameters – i.e., what precedes and what follows the laugh (within a threshold of 3s) – to distinguish 6 different contexts (see table 1) for laughter occurrence to support claims about situations that elicit laughter.",1.1 Studies on what laughter is about,[0],[0]
"In (Provine, 1993), the author assumed that laughter always immediately follows the laughable.",1.2 Weaknesses,[0],[0]
"Not only do the methods described above provide imprecise data (timing information was estimated during observation), it prevents the possibility of recording any data where laughter does not follow the laughable.",1.2 Weaknesses,[0],[0]
"In addition, even when the com-
360
ment that immediately precedes laughter is the actual trigger for a laugh, and it is not “amusing” in itself (i.e. it is a “banal comment”), it doesn’t necessarily entail that the laughable is not humourous.",1.2 Weaknesses,[0],[0]
"The funniness might arise from the “banal comment” in relation to the previous utterance, the context of the interaction, shared experiences between the speakers, world knowledge and cultural conventions.",1.2 Weaknesses,[0],[0]
"For example, in (1) “what’s funny” resides in the implicit content that the utterance refers to.",1.2 Weaknesses,[0],[0]
"In (2), the preceding utterance is funny only in relation to the context.
",1.2 Weaknesses,[0],[0]
(1) A: Do you remember that time?,1.2 Weaknesses,[0],[0]
B and A: < laughter/,1.2 Weaknesses,[0],[0]
>.,1.2 Weaknesses,[0],[0]
"Laughable= the enriched denotation of ‘that time’.
",1.2 Weaknesses,[0],[0]
"(2) (Context: the speakers are discussing the plan of an imagined shared apartment, and they have already planned two bathrooms).",1.2 Weaknesses,[0],[0]
A: I want another bathroom.,1.2 Weaknesses,[0],[0]
B:,1.2 Weaknesses,[0],[0]
"< laughter/ > Laughable= “I want another bathroom”
(Vettin and Todt, 2004) is methodologically more precise than (Provine, 1993), and they allow for the possibility that in addition to laughter occurring after the laughable, a laughter may precede an utterance, or occur during an exophoric situation.",1.2 Weaknesses,[0],[0]
"However, this analysis excludes laughters that occur in the middle of or overlaps with an utterance, and it uses exclusively timing parameters to determine what laughter is about (as illustrated in figure 1).",1.2 Weaknesses,[0],[0]
"For example, whether a laugh is considered to be about the preceding utterance or about the following utterance is decided purely on the difference in the length of gaps with the two utterances.",1.2 Weaknesses,[0],[0]
"Crucially, the conclusion is also drawn assuming an adjacency relationship between laughter and laughable.",1.2 Weaknesses,[0],[0]
"We argue that previous studies have ignored analysing the laughable because they did not attempt to integrate their account with an explicit
semantic/pragmatic module on the basis of which content is computed.1 The sole recent exception to this, as far as we are aware, is the account of (Ginzburg et al., 2015), which sketches an information state–based account of the meaning and use of laughter in dialogue.
",2 Laughter as an event anaphor,[0],[0]
"Taking this as a starting point, we argue that laughter is a gestural event anaphor, whose meaning contains two dimensions: one dimension about the arousal and the other about the trigger or the laughable.",2 Laughter as an event anaphor,[0],[0]
"In line with (Morreall, 1983)",2 Laughter as an event anaphor,[0],[0]
"we think that laughter effects a “positive psychological shift”, and the “arousal” dimension signals the amplitude in the shift.2.",2 Laughter as an event anaphor,[0],[0]
"The positive psychological shift is triggered by an appraisal of an event - the laughable l, and the second dimension communicates the type of the appraisal.",2 Laughter as an event anaphor,[0],[0]
"(Ginzburg et al., 2015) propose two basic types of meaning in the laughable dimension: the person laughing may express her perception of the laughable l as being incongruous, or just that l is enjoyable (playful).",2 Laughter as an event anaphor,[0],[0]
"We propose that in addition, certain uses of laughter in dialogue may suggest the need for a third possible type: expressing that l is a socially close ingroup situation.",2 Laughter as an event anaphor,[0],[0]
Here we sketch a formal semantic and pragmatic treatment of laughter.,2.1 Formal treatment of laughter,[0],[0]
"On the approach developed in KoS (Ginzburg, 2012), information states comprise a private part and the dialogue gameboard that represents information arising from publicized interactions.",2.1 Formal treatment of laughter,[0],[0]
"In addition to tracking shared assumptions/visual space, Moves, and QUD, the dialogue gameboard also tracks topoi and enthymemes that conversational participants exploit during an interaction (e.g., in reasoning about rhetorical relations.).",2.1 Formal treatment of laughter,[0],[0]
"Here topoi represent general inferential patterns (e.g., given two routes choose
1This is not the case for some theories of humour, e.g., that due to (Raskin, 1985), who offers a reasonably explicit account of incongruity emanating from verbal content without, however, attempting to offer a theory of laughter in conversation.
2The amplitudes in the shift depend on both the trigger itself and on the individual current information/emotional state.",2.1 Formal treatment of laughter,[0],[0]
"It is important to point out that laughter does not signal that the speaker’s current emotional state is positive, merely that there was a shift which was positive.",2.1 Formal treatment of laughter,[0],[0]
The speaker could have a very negative baseline emotional state (being very sad or angry) but the recognition of the incongruity in the laughable or its enjoyment can provoke a positive shift (which could be very minor),2.1 Formal treatment of laughter,[0],[0]
"The distinction between the overall emotional state and the direction of the shift explains why laughter can be produced when one is sad or angry.
the shortest one) represented as functions from records to record types, and enthymemes are instances of topoi (e.g., given that the route via Walnut street is shorter than the route via Alma choose Walnut street).",2.1 Formal treatment of laughter,[0],[0]
"An enthymeme belongs to a topos if its domain type is a subtype of the domain type of the topos.
",2.1 Formal treatment of laughter,[0],[0]
"(Ginzburg et al., 2015) posit distinct, though quite similar lexical entries for enjoyment and incongruous laughter.",2.1 Formal treatment of laughter,[0],[0]
For reasons of space in (3) we exhibit a unified entry with two distinct contents.,2.1 Formal treatment of laughter,[0],[0]
"(3) associates an enjoyment laugh with the laugher’s judgement of a proposition whose situational component l is active as enjoyable; for incongruity, a laugh marks a proposition whose situational component l is active as incongruous, relative to the currently maximal enthymeme under discussion.",2.1 Formal treatment of laughter,[0],[0]
(3) makes appeal to a notion of an active situation.,2.1 Formal treatment of laughter,[0],[0]
"This pertains to the accessible situational antecedents of a laughter act, given that (Ginzburg et al., 2015) proposed viewing laughter as an event anaphor.",2.1 Formal treatment of laughter,[0],[0]
"However, given the existence of a significant amount of speech laughter, as we discuss below, this notion apparently needs to be rethought somewhat, viewing laughter in gestural terms.",2.1 Formal treatment of laughter,[0],[0]
"This requires interfacing the two channels, a problem we will not address here, though see (Rieser, 2015) for a recent discussion in the context of manual gesture.
",2.1 Formal treatment of laughter,[0],[0]
(3) phon :,2.1 Formal treatment of laughter,[0],[0]
laughterphontype dgb-params :  spkr : Ind addr : Ind t : TIME c1 :,2.1 Formal treatment of laughter,[0],[0]
"addressing(spkr,addr,t) MaxEud = e : (Rec)RecType p = [ sit = l sit-type = L ] : prop c2 : ActiveSit(l)  contentenjoyment = Enjoy(spkr,p) : RecType contentincongruity = Incongr(p,e,τ ) :",2.1 Formal treatment of laughter,[0],[0]
"RecType  The dialogue gameboard parameters utilised in the account of (Ginzburg et al., 2015) are all ‘informational’ or utterance related ones.",2.1 Formal treatment of laughter,[0],[0]
"However, in order to deal with notions such as arousal and psychological shift, one needs to introduce also parameters that track appraisal (see e.g.,
(Scherer, 2009)).",2.1 Formal treatment of laughter,[0],[0]
"For current purposes, we mention merely one such parameter we dub pleasantness that relates to the appraisal issue—in Scherer’s formulation—Is the event intrinsically pleasant or unpleasant?.",2.1 Formal treatment of laughter,[0],[0]
"We assume that this parameter is scalar in value, with positive and negative values corresponding to varying degrees of pleasantness or unpleasantness.
",2.1 Formal treatment of laughter,[0],[0]
"This enables us to formulate conversational rules of the form ‘if A laughs and pleasantness is set to k, then reset pleasantness to k + θ(α)’, where α is a parameter corresponding to arousal.",2.1 Formal treatment of laughter,[0],[0]
The study is part of a broader project where we analyse laughter using a multi-layered scheme and propose a semantic/ pragmatic account of the meaning and effects of laughter.,2.2 Research questions,[0],[0]
"The focus of the current study is the positioning of laughter in relation to its laughable.
",2.2 Research questions,[0],[0]
Our account suggests that resolving the laughable is crucial for deriving the content of a laughter event.,2.2 Research questions,[0],[0]
We hypothesize that laughter is not always adjacent to its laughable.,2.2 Research questions,[0],[0]
"Rather, the sequential distribution between laughter and laughable is somewhat free, illustrated in Figure 2.",2.2 Research questions,[0],[0]
"We hypothesize that laughter can occur before, during and after the laughable, and that it is possible for intervening materials to occur between a laughter event and its laughable.
",2.2 Research questions,[0],[0]
"In more detail, we make the following hypotheses in relation to our research questions:
Q1: Does laughter always follow its laughable?
–If not, does laughter-laughable alignment differ among different types of laughters?
",2.2 Research questions,[0],[0]
"We hypothesize that laughter can occur before, during or after the laughable; laughter and laughable should not have a one-to-one relationship: one laughable can be the referent of several laughter events.
–More specifically, laughter-laughable alignment may vary depending on at least the source of the laughable (self or partner) and whether it is speech laugh or laughter bouts.
",2.2 Research questions,[0],[0]
"Q2: Does laughter interrupt speech?
",2.2 Research questions,[0],[0]
"We hypothesize that laughter can occur both at utterance boundaries and at utterance-medial position.
",2.2 Research questions,[0],[0]
"Q3: Is laughter-laughable alignment pattern language specific?
",2.2 Research questions,[0],[0]
"We hypothesize that language/culture influence alignment and thus predict to find differences between, in this case, French and Chinese.",2.2 Research questions,[0],[0]
"We analyzed a portion of the DUEL corpus (Hough et al., 2016a)",3.1 Corpus,[0],[0]
"The corpus consists of 30 dyads (10 per language)/ 24 hours of natural, faceto-face, loosely task-directed dialogue in French, Mandarin Chinese and German.",3.1 Corpus,[0],[0]
Each dyad conversed in three tasks which in total lasted around 45 minutes.,3.1 Corpus,[0],[0]
"The three tasks used were:
1.",3.1 Corpus,[0],[0]
"Dream Apartment: the participants are told that they are to share a large open-plan apartment, and will receive a large amount of money to furnish and decorate it.",3.1 Corpus,[0],[0]
"They discuss the layout, furnishing and decoration decisions;
2.",3.1 Corpus,[0],[0]
Film Script:,3.1 Corpus,[0],[0]
"The participants spend 15 minutes creating a scene for a film in which something embarrassing happens to the main character;
3.",3.1 Corpus,[0],[0]
"Border control: one participant plays the role of a traveller attempting to pass through the border control of an imagined country, and is interviewed by an officer.",3.1 Corpus,[0],[0]
The traveller has a personal situation that disfavours him/her in this interview.,3.1 Corpus,[0],[0]
The officer asks questions that are general as well as specific.,3.1 Corpus,[0],[0]
"In addition, the traveller happens to be a parent-in-law of the officer.
",3.1 Corpus,[0],[0]
The corpus is transcribed in the target language and glossed in English.,3.1 Corpus,[0],[0]
"Disfluency, laughter, and exclamations are annotated.",3.1 Corpus,[0],[0]
The current paper presents analysis of laughter in two dyads in French and Chinese (3 tasks x 2 pairs x 2 languages).,3.1 Corpus,[0],[0]
"Coding was conducted by the first and second authors and by 2 trained, but naı̈ve to the aim of the study, masters students: each video was observed until a laugh occurred.",3.2 Audio-video coding of laughter,[0],[0]
"The coder detected the exact onset and offset in Praat (Boersma and others, 2002), and conducted a multi-layer analysis as explained shortly.",3.2 Audio-video coding of laughter,[0],[0]
"A laugh was identified referring to the same criteria used in (Nwokah et al., 1994), based on the facial expression and vocalization descriptions of laughter elaborated by (Apte, 1985) and (Ekman and Friesen, 1975).",3.2 Audio-video coding of laughter,[0],[0]
"Following (Urbain and Dutoit, 2011)",3.2 Audio-video coding of laughter,[0],[0]
"we counted laughter offset (final laughter in-breath inhalation) as part of the laughter event itself, thus resulting in laughter timings longer than other authors (Bachorowski and Owren, 2001; Rothgänger et al., 1998).
",3.2 Audio-video coding of laughter,[0],[0]
"All laughter events were categorised according to different parameters: formal and contextual aspects, semantic meaning and functions (see Table 2).",3.2 Audio-video coding of laughter,[0],[0]
"The formal and contextual level analysis include whether a laughter overlaps speech (speech laugh), whether it co-occurs with or immediately follows a partner’s laughter (dyadic/ antiphonal laughter), and its position in relation to the laughable.",3.2 Audio-video coding of laughter,[0],[0]
The semantic meaning level analysis include perceived arousal and whether it contains an element of incongruity could be identified by the coders.,3.2 Audio-video coding of laughter,[0],[0]
"The function analysis codes the effect of laughter on the interaction, and distinguishes whether the effect is cooperative, i.e., promotes interaction (e.g. showing enjoyment, smoothing) or non-cooperative, i.e., in some way disaffects interaction (e.g., mocking or evade questions).",3.2 Audio-video coding of laughter,[0],[0]
"Due to space constraints and current focus, we do not provide a detailed explanation of the multi-level laughter coding scheme, for which see (Mazzocconi et al., 2016).",3.2 Audio-video coding of laughter,[0],[0]
Reliability was assessed by having a masters student as a second coder for 10% of the material observed.,3.2 Audio-video coding of laughter,[0],[0]
"Percentage agreements between the two coders for French and Chinese data averaged respectively 87% and 87.76, with an overall Krippendorff α (Krippendorff, 2012) across all tiers of 0.672 and 0.636.
",3.2 Audio-video coding of laughter,[0],[0]
"For the main analysis, we include in our analysis both laughter and speech laughter (Nwokah et al., 1999).",3.2 Audio-video coding of laughter,[0],[0]
"In the current study we restrict our observations about the aspects pertaining to the form, to the contextual distribution and positioning of a laugh in relation to others’ laughter, the laughable and laugher’s herself speech.",3.2 Audio-video coding of laughter,[0],[0]
"We consider as the laughable the event which, after appraisal, produces a positive psychological shift in the laugher.",3.3 Identifying laughables,[0],[0]
"We distinguish three different kinds of laughable types: described events, metalinguistic stimuli and exophoric events.",3.3 Identifying laughables,[0],[0]
"We also mark whether they originated from the laugher him/herself or by the partner.
(4) Described event A: il y a (un: + un) de mes potes?",3.3 Identifying laughables,[0],[0]
"idiot comme il est, qui (< p = pose > po-",3.3 Identifying laughables,[0],[0]
< /p,3.3 Identifying laughables,[0],[0]
> qui pose) un steak sur le rebord (de: + du) balcon?,3.3 Identifying laughables,[0],[0]
B:< laughter/,3.3 Identifying laughables,[0],[0]
>.,3.3 Identifying laughables,[0],[0]
< laughspeech > ils sont bizarres tes potes < /laughspeech,3.3 Identifying laughables,[0],[0]
">
(Translation) A: There is (one + one) of my buddies, stupid as he is, who put a steak on the border of the: of the balcony B: < laughter/",3.3 Identifying laughables,[0],[0]
>.,3.3 Identifying laughables,[0],[0]
< laughspeech > you have weird buddies < /laughspeech,3.3 Identifying laughables,[0],[0]
">
Laughable= “who put a steak on the border of the balcony”: described event
(5) Metalinguistic stimuli B: Alors je viens pour {euh} avoir mon passeport?",3.3 Identifying laughables,[0],[0]
pour Inra:schabella?,3.3 Identifying laughables,[0],[0]
"< laughter/ >
(Translation) B:",3.3 Identifying laughables,[0],[0]
"So I’m here for, euh, having my passport?",3.3 Identifying laughables,[0],[0]
for Inraschabella? < laughter/,3.3 Identifying laughables,[0],[0]
">
Laughable= “Inraschabella” (linguistic form, laugh after laugher’s speech)
(6) Exophoric event The examiner is asking A to move the arms because of technical issues A: movement arms mimicking a robot B:",3.3 Identifying laughables,[0],[0]
< laughter/,3.3 Identifying laughables,[0],[0]
>,3.3 Identifying laughables,[0],[0]
"A: < laughter/ >
",3.3 Identifying laughables,[0],[0]
Laughable=the way A moved his arms: exophoric event,3.3 Identifying laughables,[0],[0]
"Every time a laugh was identified, coders would mark on the Praat TextGrid, based on personal inference, the laughable the laugh would refer to.
",3.4 Audio-video coding of laughable,[0],[0]
"The time boundaries were marked, the content (whether verbal or not) was annotated and an index was assigned in order to map laughter (or multiple laughters) and laughable.",3.4 Audio-video coding of laughable,[0],[0]
"Laughables were classified according to three main categories: described, metalinguistic and exhophoric event.",3.4 Audio-video coding of laughable,[0],[0]
Reliability of type assignement was assessed by having a masters student as a second coder for 10% of the material observed.,3.4 Audio-video coding of laughable,[0],[0]
"Percentage agreements between the two coders for French and Chinese averaged 92.5% with a Krippendorff α (Krippendorff, 2012) of 0.77.",3.4 Audio-video coding of laughable,[0],[0]
"In our data sample (summarized in Table2), laughter is very frequent, constituting 17% of the conversation duration in French and 7.2% in Chinese.",4 Results,[0],[0]
Each laughable is ”laughed about” more than once (1.7 times in French and 1.4 times in Chinese).,4 Results,[0],[0]
"To investigate the time alignment between laughter and laughable, we calculated “start of laughter minus start of laughable”, “end of laughter minus end of laughable”, and “start of laughter minus end of laughable”.",4.1 Does laughter always follow the laughable?,[0],[0]
"If laughter always follow the laughable, all three measurements should be above zero.",4.1 Does laughter always follow the laughable?,[0],[0]
This was not the case.,4.1 Does laughter always follow the laughable?,[0],[0]
"In both Chinese and French, on average, laughter starts during rather than after the laughable, and finishes af-
ter the laughable.",4.1 Does laughter always follow the laughable?,[0],[0]
"In general, laughs in Chinese are more likely to overlap with the laughable than in French.",4.1 Does laughter always follow the laughable?,[0],[0]
The distribution varies over a wide range.,4.1 Does laughter always follow the laughable?,[0],[0]
"Table 3 summarizes the gaps between the boundaries of laughter and laughable, and figure 3 plots specifically the gap between the end of the laughable and the start of laughter.",4.1 Does laughter always follow the laughable?,[0],[0]
"They show that it is common for laughs to start before, during and after the laughable.",4.1 Does laughter always follow the laughable?,[0],[0]
"When a laugh has no overlap with its laughable, they are not always adjacent to each other (average utterance duration is under 2 seconds while the gap can be up to 10 seconds).",4.1 Does laughter always follow the laughable?,[0],[0]
"In the following example, the first two instances of speech laugh refer to a laughable in a later utterance.
",4.1 Does laughter always follow the laughable?,[0],[0]
"(7) 那 个 老 师(要 他+要 求 小 诗) 用“不 约 而 同”造 句 子, 后 来 小 明< laughspeech >就 想 了 一",4.1 Does laughter always follow the laughable?,[0],[0]
想< /laughspeech,4.1 Does laughter always follow the laughable?,[0],[0]
">, 然后说呃说呃这样吧?",4.1 Does laughter always follow the laughable?,[0],[0]
"< laughspeech >(我 就+小 诗)< /laughspeech > 就想了想说,呃:呃:我在路上碰见一个美女,然后我 就问她,约吗?< laughspeech > 然后美女说, 滚,我 们不约儿童< /laughspeech",4.1 Does laughter always follow the laughable?,[0],[0]
">.
(Translation) B:",4.1 Does laughter always follow the laughable?,[0],[0]
The teacher asked Xiaoshi to make a sentence with ”bu yue er tong” (coincidentally together).,4.1 Does laughter always follow the laughable?,[0],[0]
"Xiaoshi < laughspeech > then < laughspeech/ > thought about it, and said, uh, < laughspeech >",4.1 Does laughter always follow the laughable?,[0],[0]
(,4.1 Does laughter always follow the laughable?,[0],[0]
I + Xiaoshi),4.1 Does laughter always follow the laughable?,[0],[0]
"< laughspeech/ > thought about it and said, uh, uh I saw a pretty girl in the street, and I asked her ”shall we go for a date?”, and < laughspeech >",4.1 Does laughter always follow the laughable?,[0],[0]
the girl said “shouldn’t date children” < laughspeech/ >.,4.1 Does laughter always follow the laughable?,[0],[0]
"(note: “shouldn’t date children” is phonologically identical to ”incidentally together”)
",4.1 Does laughter always follow the laughable?,[0],[0]
"Laughable= “the girl said ‘shouldn’t date children’ ”
Based on whether laughter occurs entirely outside or overlapping with the laughable, we grouped the laughters into 4 alignment categories: “before”, “overlap”, “immediately after” and “other after” (see figure 4).",4.1 Does laughter always follow the laughable?,[0],[0]
"We found that in both languages, laughters that immediately follow (within 0.3s) the laughable constitute 30% .",4.1 Does laughter always follow the laughable?,[0],[0]
"There are more overlapping laughters in Chinese than in French (χ2(1)=6.9, p= .008).",4.1 Does laughter always follow the laughable?,[0],[0]
"Our analysis mainly focuses on the distinction between self and partner produced laughables, and
between speech laugh and laughter bouts, presented separately below.",4.2 Does laughter-laughable alignment differ among different “types” of laughables and laughters?,[0],[0]
"Due to space constraints, the effect of the rest of the tiers are not discussed.",4.2 Does laughter-laughable alignment differ among different “types” of laughables and laughters?,[0],[0]
"We coded whether the laughables are described events, meta-linguistic, or exophoric events.",4.2.1 Self vs. partner produced laughables,[0],[0]
"In our corpus described events are the commonest (92% in French and 89% in Chinese), followed by exophoric laughables (7% in French and 10%).",4.2.1 Self vs. partner produced laughables,[0],[0]
"Metalinguistic (1% in both languages) laughables are rare, so we grouped them with described events in the current analysis.",4.2.1 Self vs. partner produced laughables,[0],[0]
"On average, there are more self-produced than partnerproduced laughables, supporting the idea that speakers laugh more often than the audience.",4.2.1 Self vs. partner produced laughables,[0],[0]
"Interestingly, 3% of the laughables are jointly produced (one person finishing the other’s sentence, or both saying roughly the same thing at the same time) (see (8)).",4.2.1 Self vs. partner produced laughables,[0],[0]
"With the former two categories, we also coded whether the laughable is produced by the laugher or her partner, which allow us to compare our results with studies of “speaker” or “audience” laughter.
",4.2.1 Self vs. partner produced laughables,[0],[0]
"(8) (totally overlapping turns are italicized)
",4.2.1 Self vs. partner produced laughables,[0],[0]
B: c’est une personne qui est aux toilettes dans < laughter,4.2.1 Self vs. partner produced laughables,[0],[0]
>,4.2.1 Self vs. partner produced laughables,[0],[0]
des toilettes publiques A: < laughter,4.2.1 Self vs. partner produced laughables,[0],[0]
>,4.2.1 Self vs. partner produced laughables,[0],[0]
X,4.2.1 Self vs. partner produced laughables,[0],[0]
ah: oui: oui un mec qui parle a cute‘ < laughter/,4.2.1 Self vs. partner produced laughables,[0],[0]
> B: dans < laughter > des toilettes,4.2.1 Self vs. partner produced laughables,[0],[0]
"publiques voila sauf que l’autre il est au
telephone et l’autre il lui croit qu’il parle .",4.2.1 Self vs. partner produced laughables,[0],[0]
C’est genant < laughter/,4.2.1 Self vs. partner produced laughables,[0],[0]
">
(Translation) B: it is a person who is in the bathroom in < laughter > in public bathroom",4.2.1 Self vs. partner produced laughables,[0],[0]
A:< laughter >,4.2.1 Self vs. partner produced laughables,[0],[0]
Ah yes yes a guy who is talking in the next stall,4.2.1 Self vs. partner produced laughables,[0],[0]
< laughter/ > B: in < laughter,4.2.1 Self vs. partner produced laughables,[0],[0]
>,4.2.1 Self vs. partner produced laughables,[0],[0]
in public bathroom exactly but the other is on the phone and the other thinks he is speaking with him.,4.2.1 Self vs. partner produced laughables,[0],[0]
That’s embarrassing < laughter/,4.2.1 Self vs. partner produced laughables,[0],[0]
">
",4.2.1 Self vs. partner produced laughables,[0],[0]
"Laughable= “exactly but the other is on the phone and the other thinks he is speaking with him”
We found that laughters about a partnerproduced laughable start later than those about a self-produced laughable, but still the average starting time is before the end of the laughable.",4.2.1 Self vs. partner produced laughables,[0],[0]
"With partner-produced laughables, the average gap between the end of laughable and start of laughter is -0.02s in French and -0.3s in Chinese, while with self-produced laughables, the average gap is -0.7s in French and -1.3s in Chinese.",4.2.1 Self vs. partner produced laughables,[0],[0]
Laughter frequently overlaps with speech.,4.2.2 Speech laugh vs. laughter bouts,[0],[0]
36% of laughter events in French and 47% of laughter events in Chinese contain speech laughter.,4.2.2 Speech laugh vs. laughter bouts,[0],[0]
Speech laughter is on average 0.3 seconds longer than stand alone laughter bouts.,4.2.2 Speech laugh vs. laughter bouts,[0],[0]
Speech laughs overlap with the laughable more than laughter bouts.,4.2.2 Speech laugh vs. laughter bouts,[0],[0]
52% of speech laughters in French and 70% in Chinese overlap with the laughables.,4.2.2 Speech laugh vs. laughter bouts,[0],[0]
"In comparison, 33% of laughter bouts in French and 34% in Chinese overlap with the laughable.",4.2.2 Speech laugh vs. laughter bouts,[0],[0]
The reason why speech laugh more often overlap with the laughables is likely to do with the difference in function between speech laugh and laughter bouts.,4.2.2 Speech laugh vs. laughter bouts,[0],[0]
"Laughters that mark an upcoming laughable most frequently overlaps with speech, and these laughter events are also ones that tend to stretch until the middle or the end of the laughable.",4.2.2 Speech laugh vs. laughter bouts,[0],[0]
"A more detailed analysis of the function/effect of laughter is reported in (Mazzocconi et al., 2016).
",4.2.2 Speech laugh vs. laughter bouts,[0],[0]
"Notice that not all speech laughs overlap with the laughable, suggesting that often, laughter that co-occurs with speech is not about the cooccurring speech (47.8% in French and 30% in Chinese).",4.2.2 Speech laugh vs. laughter bouts,[0],[0]
"In the following example, speaker B says that she’ll take the bigger bedroom, and laughs.",4.2.2 Speech laugh vs. laughter bouts,[0],[0]
"Speaker A joins the laughter but starts a new utterance.
",4.2.2 Speech laugh vs. laughter bouts,[0],[0]
(9) B:,4.2.2 Speech laugh vs. laughter bouts,[0],[0]
okay.,4.2.2 Speech laugh vs. laughter bouts,[0],[0]
les chambres maintenant A:alo:rs F euh: bon évidemment F euh: B: je prends la plus grande <,4.2.2 Speech laugh vs. laughter bouts,[0],[0]
laughter/,4.2.2 Speech laugh vs. laughter bouts,[0],[0]
> A: c’est là < laughter >,4.2.2 Speech laugh vs. laughter bouts,[0],[0]
où il y a un problème t’vois < /laughter,4.2.2 Speech laugh vs. laughter bouts,[0],[0]
">
(Translation) B:",4.2.2 Speech laugh vs. laughter bouts,[0],[0]
okay.,4.2.2 Speech laugh vs. laughter bouts,[0],[0]
the bedrooms now,4.2.2 Speech laugh vs. laughter bouts,[0],[0]
A: well euh:,4.2.2 Speech laugh vs. laughter bouts,[0],[0]
well obviously euh:,4.2.2 Speech laugh vs. laughter bouts,[0],[0]
B: I take the bigger one < laughter/,4.2.2 Speech laugh vs. laughter bouts,[0],[0]
>,4.2.2 Speech laugh vs. laughter bouts,[0],[0]
"A: It’s there < laughspeech > where there is a problem you see < /laughspeech >
",4.2.2 Speech laugh vs. laughter bouts,[0],[0]
Laughable= “je prends la plus grande”,4.2.2 Speech laugh vs. laughter bouts,[0],[0]
"We investigated whether laughter occurs at utterance-medial positions when one party is speaking, and when the partner is speaking.
",4.3 Does laughter interrupt speech?,[0],[0]
Does laughter interrupt partners’ utterances?,4.3 Does laughter interrupt speech?,[0],[0]
Yes.,4.3 Does laughter interrupt speech?,[0],[0]
"We found that 51.8% of laughter bouts in French and 56.7% of laughter bouts in Chinese start during the partner’s utterances (not necessarily laughables), for example:
(10) B: pour faire un mur de son quoi < laughspeech > en fait c’est une < english > ra:",4.3 Does laughter interrupt speech?,[0],[0]
ve < /english > notre appartement < /laughspeech,4.3 Does laughter interrupt speech?,[0],[0]
>,4.3 Does laughter interrupt speech?,[0],[0]
"A: < laughter/ >
(Translation) B: to create a sound barrier which < laughspeech > in fact it is a rave, our apartment < /laughspeech",4.3 Does laughter interrupt speech?,[0],[0]
>,4.3 Does laughter interrupt speech?,[0],[0]
"A:< laughter/ >
Laughable= “in fact it is a rave, our apartment”
Does laughter interrupt one’s own utterances?
",4.3 Does laughter interrupt speech?,[0],[0]
We found 14 laughter bouts (5%) in French and 12 (8.6%) in Chinese that occurred in utterancemedial positions.,4.3 Does laughter interrupt speech?,[0],[0]
These proportions are statistically higher than zero:,4.3 Does laughter interrupt speech?,[0],[0]
"French χ2(1)=12.3, p=.0004; Chinese χ2(1)=10.5, p=.001.",4.3 Does laughter interrupt speech?,[0],[0]
Most of these interruptions at not at phrase boundaries.,4.3 Does laughter interrupt speech?,[0],[0]
"For example:
(11) 那你之前有没有啊:.有过什么...",4.3 Does laughter interrupt speech?,[0],[0]
< laughter/,4.3 Does laughter interrupt speech?,[0],[0]
"> < laughter >犯罪记录吗?
(Translation) Do you have, uh, have any < laughter/ >",4.3 Does laughter interrupt speech?,[0],[0]
"criminal records?
",4.3 Does laughter interrupt speech?,[0],[0]
Laughable= “criminal records”,4.3 Does laughter interrupt speech?,[0],[0]
"The aim of the current study was to deepen the little research available on the relation between laughter, laughable and speech in natural conversation, starting from the observation of their temporal sequence and alignment.",5 Discussion,[0],[0]
"We investigated three questions: whether laughter always follows, or at least is adjacent to its laughable, as is commonly assumed; whether this sequential alignment differ depending on differeht “types” of laughters; and whether laughter always punctuates speech.",5 Discussion,[0],[0]
"Our main findings are:
1.",5 Discussion,[0],[0]
Time alignment between laughter and laughable is rather free.,5 Discussion,[0],[0]
— Laughter and laughable does not have a one-to-one relationship.,5 Discussion,[0],[0]
A laughable can be referred to by more than one laughters.,5 Discussion,[0],[0]
"— Contrary to popular belief, only 30% of laughters occur immediately after the laughable.",5 Discussion,[0],[0]
Laughters frequently start during the laughable (more so with “speaker” laughter than “audience“ laughter).,5 Discussion,[0],[0]
"— Laughters can occur long before or long after the laughable, and not be adjacent to their laughable.",5 Discussion,[0],[0]
"— Between 30 to 50 percent of speech laughs do not overlap with the laughable, suggesting that frequently laughs are not about the cooccurring speech.",5 Discussion,[0],[0]
"If looking just at laughter bouts, about 40% occur immediately after the laughable.
2.",5 Discussion,[0],[0]
Laughter-laughable alignment may differ depending on the different “types” of laughable and laughter.,5 Discussion,[0],[0]
"Specifically, laughters about a partner-produced laughable (audience laughter) start later than those about a self-produced laughable (speaker laughter).",5 Discussion,[0],[0]
"Speech laughs occur earlier than laughter bouts, and overlaps more with the laughable.
3.",5 Discussion,[0],[0]
"Comparing Chinese and French, the majority of the patterns are similar, except that in Chinese, laughs are more likely to overlap with the laughable than in French.",5 Discussion,[0],[0]
"This provides an initial indication that while certain aspects of laughter behaviour are influenced by culture/language, generally we use laughter similarly in interaction.",5 Discussion,[0],[0]
"3
4.",5 Discussion,[0],[0]
Laughter does interrupt speech: we often laugh when others are speaking (half of all laughter bouts) and occasionally we insert stand-alone laughters mid-sentence (less than 10%).,5 Discussion,[0],[0]
"Moreover, very frequently laughter overlaps speech (around 40% of all laughters).
",5 Discussion,[0],[0]
"The relatively free alignment between laughter and speech seems analogous at a first approximation to the relation between manual gesture and speech (Rieser, 2015).",5 Discussion,[0],[0]
"We propose to consider
3Of course a caveat to this conclusion is the small number of speakers for each language.",5 Discussion,[0],[0]
"We will expand the study with more speakers and more genres of interaction.
laughter as a verbal gesture, having an independent channel from speech, with which it communicates through an interface.",5 Discussion,[0],[0]
Our results discredit the method of inferring what the laughter is about by looking at the elements that immediately precede or follow it.,5.1 Is laughter rarely about funny stimuli?,[0],[0]
"Therefore, previous conclusions using this method should be revisited (Provine, 1993; Provine, 1996; Provine, 2001; Provine and Emmorey, 2006; Vettin and Todt, 2004).",5.1 Is laughter rarely about funny stimuli?,[0],[0]
"One such conclusion is that because they follow “banal comments”, laughter is mostly about not about funny stimuli.",5.1 Is laughter rarely about funny stimuli?,[0],[0]
"We have shown that the logic does not hold, as very often, those preceding “banal comments” are not the laughables.",5.1 Is laughter rarely about funny stimuli?,[0],[0]
"And even if they are, the “funniness” or incongruity may reside between the laughable and something else, e.g., the context of occurrence, world knowledge, cultural norms, experiences, informational and intentional states shared between interlocutors.",5.1 Is laughter rarely about funny stimuli?,[0],[0]
"For example, in the following exchange, the exchange seems rather banal, but in fact, they are laughing about the exophoric situation that they are acting.
",5.1 Is laughter rarely about funny stimuli?,[0],[0]
(12) A: Oh comment allez-vous? <,5.1 Is laughter rarely about funny stimuli?,[0],[0]
laughter/,5.1 Is laughter rarely about funny stimuli?,[0],[0]
> B:,5.1 Is laughter rarely about funny stimuli?,[0],[0]
ça va et toi?,5.1 Is laughter rarely about funny stimuli?,[0],[0]
tu vas bien?,5.1 Is laughter rarely about funny stimuli?,[0],[0]
"A : très bien merci:
(Translation)",5.1 Is laughter rarely about funny stimuli?,[0],[0]
A:,5.1 Is laughter rarely about funny stimuli?,[0],[0]
Oh how are you?,5.1 Is laughter rarely about funny stimuli?,[0],[0]
<,5.1 Is laughter rarely about funny stimuli?,[0],[0]
laughter/,5.1 Is laughter rarely about funny stimuli?,[0],[0]
> B: fine and you?,5.1 Is laughter rarely about funny stimuli?,[0],[0]
are you ok?,5.1 Is laughter rarely about funny stimuli?,[0],[0]
"A: very well thanks
Laughable= exophoric situation (they started acting)
",5.1 Is laughter rarely about funny stimuli?,[0],[0]
Exactly what proportion of laughables contain funny incongruity is a topic for further research.,5.1 Is laughter rarely about funny stimuli?,[0],[0]
"For now, our results questions the validity of existing proposals on this score.",5.1 Is laughter rarely about funny stimuli?,[0],[0]
"It has been suggested (notably by Provine) that laughter bouts almost never (0.1%) disrupt phrases but punctuate them (Provine, 1993; Provine, 1996; Provine, 2001).",5.2 Laughter Punctuating Speech?,[0],[0]
He explains this finding on the basis of an organic constraint: laughter and speech share the same vocal apparatus and speech has “priority access”.,5.2 Laughter Punctuating Speech?,[0],[0]
"Curiously enough, Provine has always excluded speech-laughs from his investigations, without any justification.",5.2 Laughter Punctuating Speech?,[0],[0]
"A more recent study on laughter in deaf ASL signers (Provine and Emmorey, 2006) showed that signers rarely laugh during their own utterances, where no competition for the same channel of expression is present.",5.2 Laughter Punctuating Speech?,[0],[0]
"Provine and Emmory conclude that the
punctuation effect of laughter holds even for signers, and possibly is not a simple physical constraint that determines the placement of laughter in dialogues, but due to a higher order linguistic ordered structure (Provine, 2006).
",5.2 Laughter Punctuating Speech?,[0],[0]
"On the surface, their findings in speakers and signers are similar: speakers do not stop midsentence to insert a laugh, and signers do not laugh while signing a sentence.",5.2 Laughter Punctuating Speech?,[0],[0]
"However, this “similarity” may be a difference in disguise.",5.2 Laughter Punctuating Speech?,[0],[0]
We have shown that speakers frequently overlap laughter and speech.,5.2 Laughter Punctuating Speech?,[0],[0]
"If it were indeed true that signers do not laugh while signing, it raises the question why speech laughter is common for speakers but rare for signers.",5.2 Laughter Punctuating Speech?,[0],[0]
"(Provine and Emmory, 2006) hypothesised that the placement of laughter in dialogue is controlled by a higher linguistic ordered structure, where laughter is secondary to language.",5.2 Laughter Punctuating Speech?,[0],[0]
"Therefore, even when the two don’t occur in competing channels, e.g., for signers, laughter still only occurs at phrase boundaries.
",5.2 Laughter Punctuating Speech?,[0],[0]
We argue for a different explanation.,5.2 Laughter Punctuating Speech?,[0],[0]
"Assuming speech laughter data (laughter that overlaps utterances) were not excluded in the ASL study as they were in spoken dialogue studies, in deaf signers, since the laughter is perceived only visually and involves marked facial movements, it would interfere with the perception of the message conveyed by language.",5.2 Laughter Punctuating Speech?,[0],[0]
"In sign languages, body and face movements constitute important communicative elements at all linguistic levels from phonology to morphology, semantics, syntax and prosody (Liddell, 1978; Campbell, 1999).",5.2 Laughter Punctuating Speech?,[0],[0]
"Despite the fact that emotional facial expressions can overlap with linguistic facial movements (Dachkovsky and Sandler, 2009), a laugh, implying a significant alteration of facial configuration (see identification of a laughter episode) could be excessively disruptive for the message aimed to be conveyed.",5.2 Laughter Punctuating Speech?,[0],[0]
"While in verbal language the laughter signal can be completely fused in the speech as a paralinguistic feature (Crystal, 1976) and used in a sophisticated manner to enrich and facilitate communication, (Nwokah et al., 1999) report that not even from an acoustic perspective is laughter secondary to speech: when co-occurring the laugh indeed does not resemble the speech spectral patterns nor does the speech resemble the laughter ones, but together they create a new idiosyncratic pattern.",5.2 Laughter Punctuating Speech?,[0],[0]
"Laughter is fully meaningful and communicative in itself, universally across cultures, and the emo-
tional components that it carries are not secondary to speech or trivial.",5.2 Laughter Punctuating Speech?,[0],[0]
"Our study provides the first systematic analysis of laughables, and demonstrates the existence of a corpus, the DUEL corpus (Hough et al., 2016b) in which less than a third of the laughs immediately follow their referents.",6 Conclusion and future work,[0],[0]
"Instead, the laugh can occur before, during or after the laughable with wide time ranges.",6 Conclusion and future work,[0],[0]
"In addition, laughter does “interrupt” speech: we frequently start laughing in the middle of an utterance of the interlocutor or of ourselves (often speech-laugh).",6 Conclusion and future work,[0],[0]
"Our results challenge the assumption that what laughter follows is what it is about, and thus question previous claims based on this assumption.
",6 Conclusion and future work,[0],[0]
"In future work, we will study to what extent laughter-laughable alignment differs by the function/effect of laughter, and what the limit is for the “free” alignment.",6 Conclusion and future work,[0],[0]
This work may be useful for dialogue systems which allows a computer agent to generate laughter at appropriate times depending on the type and location of the laughable.,6 Conclusion and future work,[0],[0]
We would like to thank three anonymous reviewers for SigDial 2016 for their very helpful comments.,Acknowledgments,[0],[0]
"We acknowledge the support of the French Investissements d’Avenir-Labex EFL program (ANR-10-LABX-0083) and the Disfluency, Exclamations, and Laughter in Dialogue (DUEL) project within the projets franco-allemand en sciences humaines et sociales funded by the ANR and the DFG.",Acknowledgments,[0],[0]
Studies on laughter in dialogue have proposed resolving what laughter is about by looking at what laughter follows.,abstractText,[0],[0]
This paper investigates the sequential relation between the laughter and the laughable.,abstractText,[0],[0]
We propose a semantic/pragmatic account treating laughter as a gestural event anaphor referring to a laughable.,abstractText,[0],[0]
Data from a French and Chinese dialogue corpus suggest a rather free time alignment between laughter and laughable.,abstractText,[0],[0]
"Laughter can occur (long) before, during, or (long) after the laughable.",abstractText,[0],[0]
"Our results challenge the assumption that what laughter follows is what it is about, and thus question claims which rely on this assumption.",abstractText,[0],[0]
When do we laugh?,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 157–166 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"In this work, we present a model for grounding spatial descriptors in 3D scenes.",1 Introduction,[0],[0]
Consider interpreting the instructions: Take the book and put it on the shelf.,1 Introduction,[0],[0]
One critical element of being able to interpret this sentence is associating the referring expression the book with the corresponding object in the world.,1 Introduction,[0],[0]
Another important component of understanding the command above is translating the phrase on the shelf to a location in space.,1 Introduction,[0],[0]
We call such phrases spatial descriptors.,1 Introduction,[0],[0]
"While spatial descriptors are closely related to referring expressions, they are distinct in that they can refer to locations even when there is nothing there.",1 Introduction,[0],[0]
"An intuitive way to model this is to reason over spatial regions as first-class entities, rather than taking an object-centric approach.
",1 Introduction,[0],[0]
"Following a long tradition of using game environments for AI, we adopt Minecraft as the setting for our work.",1 Introduction,[0],[0]
"Minecraft has previously been used
for work on planning and navigation (Oh et al., 2016; Tessler et al., 2016), and we expand on this by using it for grounded language understanding.",1 Introduction,[0],[0]
"As a sandbox game, it can be used to construct a wide variety of environments that capture many interesting aspects of the real world.",1 Introduction,[0],[0]
"At the same time, it is easy to extract machine-interpretable representations from the game.
",1 Introduction,[0],[0]
"We construct a dataset of Minecraft scenes with natural-language annotations, and propose a task that evaluates understanding spatial descriptors.",1 Introduction,[0],[0]
"Our task is formulated in terms of locating a pink, cube-shaped character named Misty given a scene, a natural language description, and a set of locations to choose from.",1 Introduction,[0],[0]
An example from our dataset is shown in Figure 1.,1 Introduction,[0],[0]
"The Minecraft scene representation does not provide ground-truth information about object identity or segmentation, reflecting the fact that perceptual ambiguity is always
157
present in real-world scenarios.",1 Introduction,[0],[0]
"We do, however, assume the availability of 3D depth information (which, for real-world conditions, can be acquired using depth sensors such as RGBD cameras or LiDAR).
",1 Introduction,[0],[0]
We propose and evaluate a neural network that combines convolutional layers operating over 3D regions in space with recurrent layers for processing language.,1 Introduction,[0],[0]
"Our model jointly learns to segment objects, associate them with words, and understand spatial relationships – all in an end-to-end manner.",1 Introduction,[0],[0]
"We compare with a strong neural baseline and demonstrate a relative error reduction of 32%.
",1 Introduction,[0],[0]
The dataset and model described in this paper are available online.1,1 Introduction,[0],[0]
Our task includes some of the same elements as referring-expression generation and interpretation.,2 Related Work,[0],[0]
"Past work on these tasks includes Golland et al. (2010), Krishnamurthy and Kollar (2013), Socher et al. (2014) and Kazemzadeh et al. (2014).",2 Related Work,[0],[0]
"A key difference is that spatial descriptors (as modeled in this paper) refer to locations in space, rather than to objects alone.",2 Related Work,[0],[0]
"For example, Krishnamurthy and Kollar (2013) convert natural language to a logical form that is matched against image segments, an approach that is only capable of reasoning about objects already present in the scene (and not skipped over by the segmentation process).",2 Related Work,[0],[0]
"Our model’s ability to reason over spatial regions also differentiates it from past approaches to tasks beyond referring expressions, such as the work by Tellex et al. (2011) on natural-language commanding of robots.",2 Related Work,[0],[0]
"Recent work by Hu et al. (2016) on interpreting referring expressions can capture relationships between objects, relying on the construction of (subject, object, relation) tuples.",2 Related Work,[0],[0]
Their model is limited in that it can only handle one such tuple per utterance.,2 Related Work,[0],[0]
"Our model does not have such a restriction, and it additionally expands to a 3D setting.
",2 Related Work,[0],[0]
"Our task is also related to work on Visual Question Answering, or VQA (Agrawal et al., 2015).",2 Related Work,[0],[0]
"While VQA uses free-form textual answers, our task places targeted emphasis on spatial reasoning by requiring outputs to be locations in the scene.",2 Related Work,[0],[0]
"Spatial reasoning remains an important capability for VQA systems, and is one of the elements featured in CLEVR (Johnson et al., 2016), a di-
1https://github.com/nikitakit/voxelworld
agnostic dataset for VQA.",2 Related Work,[0],[0]
"Like in our dataset, visual percepts in CLEVR are based on machinegenerated scenes.",2 Related Work,[0],[0]
"CLEVR also makes use of machine-generated language, while all language in our dataset is written by humans.
",2 Related Work,[0],[0]
"Another related task in NLP is spatial role labeling, which includes the identification of spatial descriptors and the assigning of roles to each of their constituent words.",2 Related Work,[0],[0]
"This task was studied by Kordjamshidi et al. (2011) and led to the creation of shared tasks such as SpaceEval (Pustejovsky et al., 2015).",2 Related Work,[0],[0]
"Our setting differs in that we consider grounded environments instead of studying text in isolation, and evaluate on task performance rather than logical correctness of interpretation.
",2 Related Work,[0],[0]
Spatial descriptors are also present in the task of generating 3D scenes given natural language descriptions.,2 Related Work,[0],[0]
"Compared to a recent model by Chang et al. (2017) for scene generation, our model works with lower-level 3D percepts rather than libraries of segmented and tagged objects.",2 Related Work,[0],[0]
"We are also able to incorporate learning of vocabulary, perception, and linguistic structure into a single neural network that is trainable end-to-end.",2 Related Work,[0],[0]
"At its core, the ability to understand spatial descriptors can be formulated as mapping from a natural-language description to a particular location in space.",3 Task,[0],[0]
"In Figure 1, we show an instance of our task, which consists of the following components:
• W : a perceptual representation of the world • x: the natural language description • {y1, y2, . . .",3 Task,[0],[0]
", yn}: the candidate set of loca-
tions that are under consideration
• y?: the true location that is being referred to in the scene
Given W and x, a model must select which candidate location yi best matches the description x.
We will address the particulars of the above representation as we discuss the process for constructing our dataset.",3 Task,[0],[0]
"Each example (W, x, {y1, . . .",3 Task,[0],[0]
", yn}, y?) in the dataset is made by generating a Minecraft scene (Section 3.1) and selecting a location as the target of description (Section 3.2).",3 Task,[0],[0]
We then crowdsource natural language descriptions of the target location in space.,3 Task,[0],[0]
"To better anchor the language, we populate
the target location with a cube-shaped character we name Misty, and ask workers to describe Misty’s location (Section 3.3).",3 Task,[0],[0]
We repeat this process for each example in the dataset.,3 Task,[0],[0]
Each of our Minecraft scenes is set in a randomlygenerated room.,3.1 Scene Generation and Representation,[0],[0]
"We select a random size for this room, and then populate it with a variety of objects.",3.1 Scene Generation and Representation,[0],[0]
"We include objects that can be placed on the floor (e.g. tables), mounted on the wall (e.g. torches), embedded in the wall (e.g. doors), or hanging from the ceiling (e.g. cobwebs).
",3.1 Scene Generation and Representation,[0],[0]
We then discard ground-truth knowledge about object segmentation or identity in the process of saving our dataset.,3.1 Scene Generation and Representation,[0],[0]
"This allows our task to evaluate not only models’ capacity for understanding language, but also their ability to integrate with perceptual systems.",3.1 Scene Generation and Representation,[0],[0]
"One way of approximating real-world observations would be to take a screenshot of the scene – however, a 2D projection does not provide all of the spatial information that a language user would reasonably have access to.",3.1 Scene Generation and Representation,[0],[0]
"We would like to use a 3D encoding instead, and Minecraft naturally offers a low-level (albeit low-resolution) voxel-based representation that we adopt for this work.
",3.1 Scene Generation and Representation,[0],[0]
"Each Minecraft world W is encoded as a 3D grid of voxels, where a voxel may be empty or contain a particular type of “block,” e.g. stone or wood.",3.1 Scene Generation and Representation,[0],[0]
"In general, what humans would interpret as single objects will be made of multiple Minecraft blocks – for example, the table in Figure 1 consists of a “wooden pressure plate” block on top of a “wooden fencepost” block.",3.1 Scene Generation and Representation,[0],[0]
"These same blocks can be used for other purposes as well: the “wooden fencepost” block is also part of fences, lamp-posts, and pillars, while the “wooden pressure plate” block can form shelves, countertops, as well as being placed on the ground to detect when something walks over it.",3.1 Scene Generation and Representation,[0],[0]
"We construct our Minecraft scenes specifically to include examples of such re-use, so that models capable of achieving high performance on this task must demonstrate the capacity to work without ground-truth segmentation or perfect object labeling.
",3.1 Scene Generation and Representation,[0],[0]
The voxel-grid 3D representation is not specific to the virtual Minecraft setting: it is equally applicable to real-world data where depth information is available.,3.1 Scene Generation and Representation,[0],[0]
"The main difference is that each voxel would need to be associated with a fea-
ture vector rather than a block type.",3.1 Scene Generation and Representation,[0],[0]
"One use of such a representation is in Maturana and Scherer (2015)’s work on object classification from data collected with RGBD cameras and LiDAR, which uses a 3D convolutional neural network over a voxel grid.",3.1 Scene Generation and Representation,[0],[0]
"We do not explicitly handle occlusion in this work, but we imagine that real-world extensions can approach it using a combination of multi-viewpoint synthesis, occlusion-aware voxel embeddings, and restricting the set of voxels considered by the model.",3.1 Scene Generation and Representation,[0],[0]
"After constructing a scene with representation W , we proceed to sample a location y?",3.2 Location Sampling,[0],[0]
in the scene.,3.2 Location Sampling,[0],[0]
"Given our voxel-based scene representation, our location sampling is at voxel granularity.",3.2 Location Sampling,[0],[0]
"The candidate set we sample from, {y1, . . .",3.2 Location Sampling,[0],[0]
", yn}, consists of empty voxels in the scene.",3.2 Location Sampling,[0],[0]
"Locations that occur in the middle of a large section of empty space are hard to distinguish visually and to describe precisely, so we require that each candidate yi be adjacent to at least one object.",3.2 Location Sampling,[0],[0]
"For each scene-location pair (W, y?)",3.3 Natural Language Descriptions,[0],[0]
"we crowdsource a natural language description x.
The choice of prompt for human annotators is important in eliciting good descriptions.",3.3 Natural Language Descriptions,[0],[0]
"At the location we are asking workers to refer to, we insert a pink-colored cube that we personify and name “Misty.”",3.3 Natural Language Descriptions,[0],[0]
We then ask workers to describe Misty’s location such that someone can find her if she were to turn invisible.,3.3 Natural Language Descriptions,[0],[0]
"Having a visually salient target helps anchor human perception, which is why we chose a pink color that contrasts with other visual elements in the scene.",3.3 Natural Language Descriptions,[0],[0]
"We make sure to emphasize the name “Misty” in the instructions, which results in workers almost always referring to Misty by name or with the pronoun she.",3.3 Natural Language Descriptions,[0],[0]
"This avoids having to disambiguate a myriad of generic descriptions (the pink block, the block, the target, etc.) for what is fundamentally an artificial construct.
",3.3 Natural Language Descriptions,[0],[0]
"To make sure that humans understand the 3D structure of the scene as they describe it, we give them access to a 3D view of the environment and require that they move the camera before submitting a description.",3.3 Natural Language Descriptions,[0],[0]
"This helped increase the quality of our data.
",3.3 Natural Language Descriptions,[0],[0]
"just torch
under thetable andMisty",3.3 Natural Language Descriptions,[0],[0]
We next present our model for this task.,4 Model,[0],[0]
"Our model architecture is shown in Figure 2, with some of the quantities it operates over highlighted in Figure 3.",4 Model,[0],[0]
"Throughout this section, we will use the example description Misty is to the right of the table and just under the torch.",4 Model,[0],[0]
"Note that while the accompanying scene illustrations are shown in 2D for visual clarity, our actual model operates in 3D and on larger scene sizes.
",4 Model,[0],[0]
Our model first associates words with regions in the world.,4 Model,[0],[0]
"There is no notion of object segmentation in the dataset, so the references it produces are just activations over space given a word.",4 Model,[0],[0]
"Activations are computed for all words in the sentence, though they will only be meaningful for words such as table and torch (Figure 3a).",4 Model,[0],[0]
"Our model next determines the spatial relationships between referenced objects and Misty, using information provided by context words such as right and under.",4 Model,[0],[0]
These relationships are represented as 3D convolutional offset filters (Figure 3b).,4 Model,[0],[0]
"For each word, its reference and offset filter are convolved to get a localization, i.e. an estimate of Misty’s location (Figure 3c).",4 Model,[0],[0]
"Finally, our model aggregates localizations across all words in the sentence, combining the information provided by the phrases to the right of the table and just under the torch (Figure 3e).
",4 Model,[0],[0]
"The following sections describe in more detail how references (Section 4.1), offsets (Section 4.2),
and localizations (Section 4.3) are computed.",4 Model,[0],[0]
The first component of our model is responsible for associating words with the voxels that they refer to.,4.1 Input and References,[0],[0]
"It assigns a real-valued score s(xt, y) to each pair consisting of word xt and voxel coordinate y.
High scores correspond to high compatibility; for any given word, we can visualize the set s(xt, ·) of scores assigned to different voxels by interpreting it as logits that encode a probability distribution over blocks in the scene.",4.1 Input and References,[0],[0]
"In the example, the word table would ideally be matched to the uniform reference distribution over blocks that are part of a table, and similarly for the word torch (Figure 3a).
",4.1 Input and References,[0],[0]
The word-voxel scores are computed by combining word and block embeddings.,4.1 Input and References,[0],[0]
"To take advantage of additional unsupervised language and world data, we start with pretrained word embeddings and context-aware location embeddings f(W, y).",4.1 Input and References,[0],[0]
The function f consists of the first two layers of a convolutional neural network that is pretrained on the task of predicting a voxel’s identity given the 5x5x5 neighborhood around it.,4.1 Input and References,[0],[0]
"Since f fails to take into account the actual voxel’s identity, we add additional embeddings V that only consider single blocks.",4.1 Input and References,[0],[0]
"The score is then computed as s(xt, y) =",4.1 Input and References,[0],[0]
"w>t Af(W, y) +",4.1 Input and References,[0],[0]
"w>t vy, where wt is the word embedding and vy is the single-block embedding.",4.1 Input and References,[0],[0]
"The parameter matrix
A and the single-block embedding matrix V are trained end-to-end with the rest of the model.
",4.1 Input and References,[0],[0]
References are computed for all words in the sentence – including function words like to or the.,4.1 Input and References,[0],[0]
"To signify that a word does not refer to any objects in the world, the next layer of the network expects that we output a uniform distribution over all voxels.",4.1 Input and References,[0],[0]
"Outputting uniform distributions also serves as a good initialization for our model, so we set the elements of A and V to zero at the start of training (our pretrained word embeddings are sufficient to break symmetry).",4.1 Input and References,[0],[0]
The per-word references described in Section 4.1 do not themselves indicate Misty’s location.,4.2 Offsets,[0],[0]
"Rather, they are used in a spatial descriptor like to the right of the table.",4.2 Offsets,[0],[0]
"For every word, our model outputs a distribution over offset vectors that is used to redistribute scores from object locations to possible locations for Misty (Figure 3b).",4.2 Offsets,[0],[0]
"For example, if probability mass is placed on the “oneblock-to-the-right” offset vector, this corresponds to predicting that Misty will be one block to the right of the voxels that a word refers to.",4.2 Offsets,[0],[0]
"Offset scores ot are assigned based on the context the word xt occurs in, which allows the model to incorporate information from words such as right or under in its decisions.",4.2 Offsets,[0],[0]
"This is accomplished by running a bidirectional LSTM over the embeddings wt of the words in the sentence, and using its output to compute offset probabilities:
",4.2 Offsets,[0],[0]
"[z0, z1, . . .]",4.2 Offsets,[0],[0]
"= BiLSTM([w0, w1, . . .]) o′t",4.2 Offsets,[0],[0]
"= Mzt
ot(i) ∝",4.2 Offsets,[0],[0]
exp ( o′t(i) ),4.2 Offsets,[0],[0]
"Each set of offset scores ot is reshaped into a 3x3x3 convolutional filter, except that we structurally disallow assigning any probability to the no-offset vector in the center.",4.2 Offsets,[0],[0]
"As a parametertying technique, the trainable matrix M is not fullrank; we instead decompose it such that the logprobability of an offset vector factors additively over the components in a cylindrical coordinate system.",4.2 Offsets,[0],[0]
"For each word, the 3D tensor of word-voxel scores s(xt, ·) is convolved with the offset distribution ot to produce a distribution of localizations for Misty, dt(y).",4.3 Localizations and Output,[0],[0]
A 2D illustration of the result is shown in Figure 3c.,4.3 Localizations and Output,[0],[0]
"Localizations are then summed across all words in the sentence, resulting in a single score for each voxel in the scene (Figure 3e).",4.3 Localizations and Output,[0],[0]
"These scores are interpreted as logits corresponding to a probability distribution over possible locations for Misty:
dt(y) = s(xt, y) ∗",4.3 Localizations and Output,[0],[0]
ot p(y) ∝,4.3 Localizations and Output,[0],[0]
"exp {∑ t dt(y) }
Not all words will have localizations that provide information about Misty – for some words
the localizations will just be a uniform distribution.",4.3 Localizations and Output,[0],[0]
"We will refer to words that have low-entropy localizations as landmarks, with the understanding that being a landmark is actually a soft notion in our model.
",4.3 Localizations and Output,[0],[0]
"Our offset filters ot are much smaller than our voxel grid, which means that convolving any offset filter with a uniform reference distribution over the voxel grid will also result in a uniform localization distribution (edge effects are immaterial given the small filter size and the fact that Misty is generally not at the immediate edges of the scene).",4.3 Localizations and Output,[0],[0]
"Conversely, given non-uniform references almost any set of offsets will result in a non-uniform localization.",4.3 Localizations and Output,[0],[0]
"The architecture for computing references can output uniform references for function words (like to or the), but it lacks the linguistic context to determine when words refer to objects but should not be interpreted as landmarks (e.g. when they are part of exposition or a negated expression).",4.3 Localizations and Output,[0],[0]
We therefore include an additional not-a-landmark class that is softmax-normalized jointly with the offset vector distribution ot.,4.3 Localizations and Output,[0],[0]
"Probability assigned to this class subtracts from the probability mass for the true offset directions (and therefore from the localizations) – if this class receives a probability of 1, the corresponding localizations will not contribute to the model output.",4.3 Localizations and Output,[0],[0]
We use a softmax cross-entropy loss for training our model.,4.4 Loss and Training,[0],[0]
"During training, we find that it helps to not use the candidate set {y1, y2, . . .",4.4 Loss and Training,[0],[0]
", yn} and instead calculate a probability p(y) for all blocks in the scene, including solid blocks that cannot possibly contain Misty (perhaps because this penalizes inferring nonsensical spatial relationships).
",4.4 Loss and Training,[0],[0]
"We run the Adam optimizer (Kingma and Ba, 2014) with step size 0.001 for 100 epochs using batch size 10.",4.4 Loss and Training,[0],[0]
"We keep an exponential moving average of our trainable parameters, which we save every two epochs.",4.4 Loss and Training,[0],[0]
"We then select the saved model that has the highest performance on our development set.
",4.4 Loss and Training,[0],[0]
We perform several regularization and data augmentation techniques in order to achieve better generalization.,4.4 Loss and Training,[0],[0]
"Each time we sample a training example, we select a random 19x19x19 crop from the full scene (as long as Misty’s location is not cropped out).",4.4 Loss and Training,[0],[0]
"We also disallow using the contextbased block embeddings for the first 20 epochs by
holding the parameter matrix A described in Section 4.1 fixed at zero, forcing the model to first learn to associate vocabulary with local features and only later expand to capture the compositional aspects of the environment.
",4.4 Loss and Training,[0],[0]
"For the natural language descriptions, all tokens are converted to lowercase as part of preprocessing.",4.4 Loss and Training,[0],[0]
During training we apply word-level dropout (i.e. replacing words with an UNK token) in the LSTM responsible for computing offsets.,4.4 Loss and Training,[0],[0]
"In evaluating this task, we would like to use a metric that can provide meaningful comparison of our model with baseline and human performance.",5.1 Evaluation Metric,[0],[0]
"The set of all possible locations for Misty is large enough that it is hard even for a human to guess the correct block on the first try, especially when some descriptions are only precise to within 1 or 2 blocks.",5.1 Evaluation Metric,[0],[0]
"The size of this set also varies from scene to scene.
",5.1 Evaluation Metric,[0],[0]
"Therefore for our evaluation, we restrict the set {y1, . . .",5.1 Evaluation Metric,[0],[0]
", yn} to 6 possible locations: Misty’s true location and 5 distractors.",5.1 Evaluation Metric,[0],[0]
"This represents a less ambiguous problem that is much easier for humans, while also allowing for the evaluation of future models that may require an expensive computation for each candidate location considered.",5.1 Evaluation Metric,[0],[0]
Our procedure for selecting the distractors is designed to ensure that we test both local and global scene understanding.,5.1 Evaluation Metric,[0],[0]
Each set of six choices is constructed to consist of three clusters of two candidates each.,5.1 Evaluation Metric,[0],[0]
"Each cluster location is anchored to a landmark – we sample a landmark block adjacent to Misty and two additional landmark blocks from the entire scene, such that the pairwise distances between landmarks are at least 4 units.",5.1 Evaluation Metric,[0],[0]
We then sample one distractor near Misty’s landmark and two distractors near both of the other landmarks.,5.1 Evaluation Metric,[0],[0]
"To make our development and test sets, we construct this six-option variation from a subset of our collected data.",5.2 Dataset,[0],[0]
For each such example we crowdsource two human solutions using Mechanical Turk.,5.2 Dataset,[0],[0]
Examples where both humans answered correctly are partitioned into a development and a test set.,5.2 Dataset,[0],[0]
"This filtering procedure serves as our primary method of excluding confusing or uninformative descriptions from the evaluation con-
ditions.",5.2 Dataset,[0],[0]
We also collect a third human solution to each example in the development and test sets to get an independent estimate of human performance on our task.,5.2 Dataset,[0],[0]
"The final dataset consists of 2321 training examples, 120 dev set examples, and 200 test set examples.
",5.2 Dataset,[0],[0]
The natural-language descriptions across the full dataset use a vocabulary of 1015 distinct tokens (case-insensitive but including punctuation).,5.2 Dataset,[0],[0]
"The average description length is 19.02 tokens, with a standard deviation of 10.00 tokens.",5.2 Dataset,[0],[0]
"The large spread partially reflects the fact that some people gave short descriptions that referenced a few landmarks, while others gave sequences of instructions on how to find Misty.",5.2 Dataset,[0],[0]
"As a point of comparison, the ReferIt dataset (Kazemzadeh et al., 2014) has a larger vocabulary of 9124 tokens, but a shorter average description length of 3.52 tokens (with a standard deviation of 2.67 tokens).
",5.2 Dataset,[0],[0]
A random sampling of descriptions from our dataset is shown in Table 1.,5.2 Dataset,[0],[0]
Quantitative results are shown in Table 2.,5.3 Quantitative Results,[0],[0]
Our evaluation metric is constructed such that there is an easily interpretable random baseline.,5.3 Quantitative Results,[0],[0]
We also evaluate a strong neural baseline that uses an approach we call Seq2Emb.,5.3 Quantitative Results,[0],[0]
"This baseline converts the sentence into a vector using a bidirectional LSTM encoder, and also assigns vector embeddings to each voxel using a two-layer convolutional neural network.",5.3 Quantitative Results,[0],[0]
"The voxel with an embedding that most closely matches the sentence embedding is chosen as the answer.
",5.3 Quantitative Results,[0],[0]
Our model achieves noticeable gains over the baseline approaches.,5.3 Quantitative Results,[0],[0]
"At the same time, there remains a gap between our model and individual human performance.",5.3 Quantitative Results,[0],[0]
"We see this as an indication that we have constructed a task with appropriate difficulty: it is approachable by building on the current state-of-the-art in machine learning and NLP, while presenting challenges that can motivate continued work on understanding language and how it
relates to descriptions of the world.",5.3 Quantitative Results,[0],[0]
We next conduct an ablation study to evaluate the contribution of the individual elements in our model.,5.4 Ablation Study,[0],[0]
"Our ablation results on the development set are shown in Table 3.
",5.4 Ablation Study,[0],[0]
"In our first ablation, we remove the compositional block embeddings that make use of multiple blocks.",5.4 Ablation Study,[0],[0]
"The resulting performance drop of 2.5% reflects the fact that our model uses multi-block information to match words with objects.
",5.4 Ablation Study,[0],[0]
We next replace the LSTM in our full model with a 3-word-wide convolutional layer.,5.4 Ablation Study,[0],[0]
"A single word of left- and right-context provides limited ability to incorporate spatial descriptor words like left and right, or to distinguish landmarks used to locate Misty from words providing exposition about the scene.",5.4 Ablation Study,[0],[0]
"This ablation solves 5% fewer examples than our full model, reflecting our LSTM’s ability to capture such phenomena.
",5.4 Ablation Study,[0],[0]
"Finally, we try holding the distribution over offset vectors fixed, by making it a trainable variable rather than a function of the language.",5.4 Ablation Study,[0],[0]
"This corresponds to enforcing the use of only one spatial
Misty is floating in the middle of the room.",5.4 Ablation Study,[0],[0]
"She in the upper half of the room, between the two poles.
",5.4 Ablation Study,[0],[0]
Figure 4: Reference distribution representing our model’s belief of which blocks the word poles refers to.,5.4 Ablation Study,[0],[0]
"Our model assigns the majority of the probability mass to the poles, while ignoring a table leg that is made of the same block type.",5.4 Ablation Study,[0],[0]
"Note that the seven numbers overlaid on top account for more than 99% of the total probability mass, and that each of the remaining blocks in the scene has a probability of at most 0.025%.
operator that roughly means ‘near.’",5.4 Ablation Study,[0],[0]
"We retain the LSTM for the sole purpose of assigning a score to the not-a-landmark class, meaning that contextual information is still incorporated in the decision of whether to classify a word as a landmark or not.",5.4 Ablation Study,[0],[0]
"The resulting accuracy is 5.8% lower than our full model, which makes this the worst-performing of our ablations.",5.4 Ablation Study,[0],[0]
These results suggest that the ability to infer spatial directions is important to our model’s overall performance.,5.4 Ablation Study,[0],[0]
"The modular design of our model allows us to examine the individual behavior of each component in the network, which we explore in this section.
",5.5 Qualitative Examination,[0],[0]
We find that our algorithm is able to learn to associate words with the corresponding voxels in the world.,5.5 Qualitative Examination,[0],[0]
"Figure 4 shows the reference distribution associated with the word poles, which is constructed by applying a softmax operation to the word-voxel scores for that word.",5.5 Qualitative Examination,[0],[0]
Our algorithm is able to correctly segment out the voxels that are a part of the pole.,5.5 Qualitative Examination,[0],[0]
"Moreover, the table on the right side of the scene has a table leg made of the same block type as the pole – and yet, it is is assigned a low probability.",5.5 Qualitative Examination,[0],[0]
"This shows that our model is capable of representing compositional objects, and can learn to do so in an end-to-end manner.
",5.5 Qualitative Examination,[0],[0]
We next examine the offset distributions computed by our model.,5.5 Qualitative Examination,[0],[0]
Consider the scene and description shown in Figure 5a.,5.5 Qualitative Examination,[0],[0]
"The offset vector distribution at the word platform, shown in Figure 5b, shows that the model assigns high proba-
Misty is between the wall and the flowers that are close to the corner.
",5.5 Qualitative Examination,[0],[0]
Figure 6: Our algorithm interprets this sentence as Misty is near the wall and the flowers and close to the corner.,5.5 Qualitative Examination,[0],[0]
"This intersective interpretation is sufficient to correctly guess Misty’s location in this scene (as well as others in the dataset).
",5.5 Qualitative Examination,[0],[0]
bility to Misty being above the platform.,5.5 Qualitative Examination,[0],[0]
"In Figure 5c, we show the effects of replacing the phrase right above with the words in front of.",5.5 Qualitative Examination,[0],[0]
This example illustrates our model’s capacity for learning spatial directions.,5.5 Qualitative Examination,[0],[0]
"We note that the offset distribution given the phrase in front of is not as peaked as it is for right above, and that distributions for descriptions saying left or right are even less peaked (and are mostly uniform on the horizontal plane).",5.5 Qualitative Examination,[0],[0]
One explanation for this is the ambiguity between speaker-centric and object-centric reference frames.,5.5 Qualitative Examination,[0],[0]
"The reference frame of our convolutional filters is the same as the initial camera frame for our our annotators, but this may not be the true speaker-centric frame because we mandate that annotators move the camera before submitting a description.
",5.5 Qualitative Examination,[0],[0]
We next highlight our model’s ability to incorporate multiple landmarks in making its decisions.,5.5 Qualitative Examination,[0],[0]
Consider the scene and description shown in Figure 6.,5.5 Qualitative Examination,[0],[0]
"The room has four walls, two flowers, and four corners – no single landmark is sufficient to correctly guess Misty’s location.",5.5 Qualitative Examination,[0],[0]
"Our model is able to localize the flowers, walls, and corners in this scene and intersect them to locate Misty.",5.5 Qualitative Examination,[0],[0]
"Strictly speaking, this approach is not logically equivalent to applying a two-argument between operator and recognizing the role of that as a relativizer.",5.5 Qualitative Examination,[0],[0]
"This is a limitation of our specific model, but the general approach of manipulating spatial region masks need not be constrained in this way.",5.5 Qualitative Examination,[0],[0]
It would be possible to introduce operations into the neural network to model recursive structure in the language.,5.5 Qualitative Examination,[0],[0]
"In practice, however, we find that the intersective interpretation suffices for many of the descriptions that occur in our dataset.",5.5 Qualitative Examination,[0],[0]
"In this paper, we define the task of interpreting spatial descriptors, construct a new dataset based on Minecraft, and propose a model for this task.",6 Conclusion,[0],[0]
We show that convolutional neural networks can be used to reason about regions in space as firstclass entities.,6 Conclusion,[0],[0]
"This approach is trainable end-toend while also having interpretable values at the intermediate stages of the neural network.
",6 Conclusion,[0],[0]
"Our architecture handles many of the linguistic phenomena needed to solve this task, including object references and spatial regions.",6 Conclusion,[0],[0]
"However, there is more work to be done before we can say that the network completely understands the sentences that it reads.",6 Conclusion,[0],[0]
Our dataset can be used to investigate future models that expand to handle relativization and other recursive phenomena in language.,6 Conclusion,[0],[0]
"We thank the many workers on Mechanical Turk who contributed to the creation of our dataset.
",Acknowledgments,[0],[0]
"This work was made possible by the open source tooling developed around and inspired by Minecraft; in particular we would like to thank the developers of the voxel.js project and associated plugins, as well as the developers of mcedit2.
",Acknowledgments,[0],[0]
Nikita Kitaev is supported by an NSF Graduate Research Fellowship.,Acknowledgments,[0],[0]
This research was supported by DARPA through the XAI program.,Acknowledgments,[0],[0]
We present a model for locating regions in space based on natural language descriptions.,abstractText,[0],[0]
"Starting with a 3D scene and a sentence, our model is able to associate words in the sentence with regions in the scene, interpret relations such as on top of or next to, and finally locate the region described in the sentence.",abstractText,[0],[0]
All components form a single neural network that is trained end-to-end without prior knowledge of object segmentation.,abstractText,[0],[0]
"To evaluate our model, we construct and release a new dataset consisting of Minecraft scenes with crowdsourced natural language descriptions.",abstractText,[0],[0]
We achieve a 32% relative error reduction compared to a strong neural baseline.,abstractText,[0],[0]
Where is Misty? Interpreting Spatial Descriptors by Modeling Regions in Space,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1285–1296 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1285",text,[0],[0]
"Geocoding1 is a specific case of text geolocation, which aims at disambiguating place references in text.",1 Introduction,[0],[0]
"For example, Melbourne can refer to more than ten possible locations and a geocoder’s task is to identify the place coordinates for the intended Melbourne in a context such as “Melbourne hosts one of the four annual Grand Slam tennis tournaments.”",1 Introduction,[0],[0]
"This is central to the success of tasks such as indexing and searching documents by geography (Bhargava et al., 2017), geospatial
1Also called Toponym Resolution in related literature.
analysis of social media (Buchel and Pennington, 2017), mapping of disease risk using integrated data (Hay et al., 2013), and emergency response systems (Ashktorab et al., 2014).",1 Introduction,[0],[0]
Previous geocoding methods (Section 2) have leveraged lexical semantics to associate the implicit geographic information in natural language with coordinates.,1 Introduction,[0],[0]
These models have achieved good results in the past.,1 Introduction,[0],[0]
"However, focusing only on lexical features, to the exclusion of other feature spaces such as the Cartesian Coordinate System, puts a ceiling on the amount of semantics we are able to extract from text.",1 Introduction,[0],[0]
"Our proposed solution is the Map Vector (MapVec), a sparse, geographic vector for explicit modelling of geographic distributions of location mentions.",1 Introduction,[0],[0]
"As in previous work, we use population data and geographic coordinates, observing that the most populous Melbourne is also the most likely to be the intended location.",1 Introduction,[0],[0]
"However, MapVec is the first instance, to our best knowledge, of the topological semantics of context locations explicitly isolated into a standardized vector representation, which can then be easily transferred to an independent task and combined with other features.",1 Introduction,[0],[0]
MapVec is able to encode the prior geographic distribution of any number of locations into a single vector.,1 Introduction,[0],[0]
Our extensive evaluation shows how this representation of context locations can be integrated with linguistic features to achieve a significant improvement over a SOTA lexical model.,1 Introduction,[0],[0]
"MapVec can be deployed as a standalone neural geocoder, significantly beating the population baseline, while remaining effective with simpler machine learning algorithms.
",1 Introduction,[0],[0]
"This paper’s contributions are: (1) Lexical Geocoder outperforming existing systems by analysing only the textual context; (2) MapVec, a geographic representation of locations using a sparse, probabilistic vector to extract and isolate spatial features; (3) CamCoder, a novel geocoder
that exploits both lexical and geographic knowledge producing SOTA results across multiple datasets; and (4) GeoVirus, an open-source dataset for the evaluation of geoparsing (Location Recognition and Disambiguation) of news events covering global disease outbreaks and epidemics.",1 Introduction,[0],[0]
"Depending on the task objective, geocoding methodologies can be divided into two distinct categories: (1) document geocoding, which aims at locating a piece of text as a whole, for example geolocating Twitter users (Rahimi et al., 2016, 2017; Roller et al., 2012; Rahimi et al., 2015), Wikipedia articles and/or web pages (Cheng et al., 2010; Backstrom et al., 2010; Wing and Baldridge, 2011; Dredze et al., 2013; Wing and Baldridge, 2014).",2 Background,[0],[0]
"This is an active area of NLP research (Hulden et al., 2015; Melo and Martins, 2017, 2015; Iso et al., 2017); (2) geocoding of place mentions, which focuses on the disambiguation of location (named) entities i.e. this paper and (Karimzadeh et al., 2013; Tobin et al., 2010; Grover et al., 2010; DeLozier et al., 2015; Santos et al., 2015; Speriosu and Baldridge, 2013; Zhang and Gelernter, 2014).",2 Background,[0],[0]
"Due to the differences in evaluation and objective, the categories cannot be directly or fairly compared.",2 Background,[0],[0]
Geocoding is typically the second step in Geoparsing.,2 Background,[0],[0]
"The first step, usually referred to as Geotagging, is a Named Entity Recognition component which extracts all location references in a given text.",2 Background,[0],[0]
"This phase may optionally include metonymy resolution, see (Zhang and Gelernter, 2015; Gritta et al., 2017a).",2 Background,[0],[0]
The goal of geocoding is to choose the correct coordinates for a location mention from a set of candidates.,2 Background,[0],[0]
Gritta et al. (2017b) provided a comprehensive survey of five recent geoparsers.,2 Background,[0],[0]
"The authors established an evaluation framework, with a new dataset, for their experimental analysis.",2 Background,[0],[0]
We use this evaluation framework in our experiments.,2 Background,[0],[0]
"We briefly describe the methodology of each geocoder featured in our evaluation (names are capitalised and appear in italics) as well as survey the related work in geocoding.
",2 Background,[0],[0]
"Computational methods in geocoding broadly divide into rule-based, statistical and machine learning-based.",2 Background,[0],[0]
"Edinburgh Geoparser (Tobin et al., 2010; Grover et al., 2010) is a fully rulebased geocoder that uses hand-built heuristics
combined with large lists from Wikipedia and the Geonames2 gazetteer.",2 Background,[0],[0]
"It uses metadata (feature type, population, country code) with heuristics such as contextual information, spatial clustering and user locality to rank candidates.",2 Background,[0],[0]
"GeoTxT (Karimzadeh et al., 2013) is another rule-based geocoder with a free web service3 for identifying locations in unstructured text and grounding them to coordinates.",2 Background,[0],[0]
"Disambiguation is driven by multiple heuristics and uses the administrative level (country, province, city), population size, the Levenshtein Distance of the place referenced and the candidate’s name and spatial minimisation to resolve ambiguous locations.",2 Background,[0],[0]
"(Dredze et al., 2013) is a rule-based Twitter geocoder using only metadata (coordinates in tweets, GPS tags, user’s reported location) and custom place lists for fast and simple geocoding.",2 Background,[0],[0]
"CLAVIN (Cartographic Location And Vicinity INdexer)4 is an open-source geocoder, which offers contextbased entity recognition and linking.",2 Background,[0],[0]
"It seems to be mostly rule-based though details of its algorithm are underspecified, short of reading the source code.",2 Background,[0],[0]
"Unlike the Edinburgh Parser, this geocoder seems to overly rely on population data, seemingly mirroring the behaviour of a naive population baseline.",2 Background,[0],[0]
Rule-based systems can perform well though the variance in performance is high (see Table 1).,2 Background,[0],[0]
Yahoo! Placemaker is a free web service with a proprietary geo-database and algorithm from Yahoo!5 letting anyone geoparse text in a globally-aware and language-independent manner.,2 Background,[0],[0]
"It is unclear how geocoding is performed, however, the inclusion of proprietary methods makes evaluation broader and more informative.
",2 Background,[0],[0]
"The statistical geocoder Topocluster (DeLozier et al., 2015) divides the world surface into a grid (0.5 x 0.5 degrees, approximately 60K tiles) and uses lexical features to model the geographic distribution of context words over this grid.",2 Background,[0],[0]
"Building on the work of Speriosu and Baldridge (2013), it uses a window of 15 words (our approach scales this up by more than 20 times) to perform hot spot analysis using Getis-Ord Local Statistic of individual words’ association with geographic space.",2 Background,[0],[0]
"The classification decision was made by finding the grid square with the strongest overlap of
2http://www.geonames.org/ 3http://www.geotxt.org/ 4https://clavin.bericotechnologies.com 5https://developer.yahoo.com/geo/
individual geo-distributions.",2 Background,[0],[0]
"Hulden et al. (2015) used Kernel Density Estimation to learn the word distribution over a world grid with a resolution of 0.5 x 0.5 degrees and classified documents with Kullback-Leibler divergence or a Naive Bayes model, reminiscent of an earlier approach by Wing and Baldridge (2011).",2 Background,[0],[0]
Roller et al. (2012) used the Good-Turing Frequency Estimation to learn document probability distributions over the vocabulary with Kullback-Leibler divergence as the similarity function to choose the correct bucket in the k-d tree (world representation).,2 Background,[0],[0]
"Iso et al. (2017) combined Gaussian Density Estimation with a CNN-model to geolocate Japanese tweets with Convolutional Mixture Density Networks.
",2 Background,[0],[0]
"Among the recent machine learning methods, bag-of-words representations combined with a Support Vector Machine (Melo and Martins, 2015) or Logistic Regression (Wing and Baldridge, 2014) have also achieved good results.",2 Background,[0],[0]
"For Twitter-based geolocation (Zhang and Gelernter, 2014), bag-of-words classifiers were successfully augmented with social network data (Jurgens et al., 2015; Rahimi et al., 2016, 2015).",2 Background,[0],[0]
"The machine learning-based geocoder by Santos et al. (2015) supplemented lexical features, represented as a bag-of-words, with an exhaustive set of manually generated geographic features and spatial heuristics such as geospatial containment and geodesic distances between entities.",2 Background,[0],[0]
"The ranking of locations was learned with LambdaMART (Burges, 2010).",2 Background,[0],[0]
"Unlike our geocoder, the addition of geographic features did not significantly improve scores, reporting: “The geo-specific features seem to have a limited impact over a strong baseline system.”",2 Background,[0],[0]
"Unable to obtain a codebase, their results feature in Table 1.",2 Background,[0],[0]
"The latest neural network approaches (Rahimi et al., 2017) with normalised bag-of-word representations have achieved SOTA scores when augmented with social network data for Twitter document (user’s concatenated tweets) geolocation (Bakerman et al., 2018).",2 Background,[0],[0]
"Figure 1 shows our new geocoder CamCoder implemented in Keras (Chollet, 2015).",3 Methodology,[0],[0]
"The lexical part of the geocoder has three inputs, from the top: Context Words (location mentions excluded), Location Mentions (context words excluded) and the Target Entity (up to 15 words long) to be
geocoded.",3 Methodology,[0],[0]
"Consider an example disambiguation of Cairo in a sentence: “The Giza pyramid complex is an archaeological site on the Giza Plateau, on the outskirts of Cairo, Egypt.”.",3 Methodology,[0],[0]
"Here, Cairo is the Target Entity; Egypt, Giza and Giza Plateau are the Location Mentions; the rest of the sentence forms the Context Words (excluding stopwords).",3 Methodology,[0],[0]
"The context window is up to 200 words each side of the Target Entity, approximately an order of magnitude larger than most previous approaches.
",3 Methodology,[0],[0]
"We used separate layers, convolutional and/or dense (fully-connected), with ReLu activations (Nair and Hinton, 2010) to break up the task into smaller, focused modules in order to learn distinct lexical feature patterns, phrases and keywords for different types of inputs, concatenating only at a higher level of abstraction.",3 Methodology,[0],[0]
"Unigrams and bigrams were learned for context words and location mentions (1,000 filters of size 1 and 2 for each input), trigrams for the target entity (1,000 filters of size 3).",3 Methodology,[0],[0]
Convolutional Neural Networks (CNNs) with Global Maximum Pooling were chosen for their position invariance (detecting location-indicative words anywhere in context) and efficient input size scaling.,3 Methodology,[0],[0]
"The dense layers have 250 units each, with a dropout layer (p = 0.5) to prevent overfitting.",3 Methodology,[0],[0]
"The fourth input is MapVec, the geographic vector representation of location mentions.",3 Methodology,[0],[0]
"It feeds into two dense layers with 5,000 and 1,000 units respectively.",3 Methodology,[0],[0]
The concatenated hidden layers then get fully connected to the softmax layer.,3 Methodology,[0],[0]
"The model is optimised with RMSProp (Tieleman and Hinton, 2012).",3 Methodology,[0],[0]
"We approach geocoding as a classification task where the model predicts one of
7,823 classes (units in the softmax layer in Figure 1), each being a 2x2 degree tile representing part of the world’s surface, slightly coarser than MapVec (see Section 3.1 next).",3 Methodology,[0],[0]
"The coordinates of the location candidate with the smallest FD (Equation 1) are the model’s final output.
",3 Methodology,[0],[0]
FD = error − error candidatePop maximumPop,3 Methodology,[0],[0]
"Bias (1)
FD for each candidate is computed by reducing the prediction error (the distance from predicted coordinates to candidate coordinates) by the value of error multiplied by the estimated prior probability (candidate population divided by maximum population) multiplied by the Bias parameter.",3 Methodology,[0],[0]
The value of Bias = 0.9 was determined to be optimal for highest development data scores and is identical for all highly diverse test datasets.,3 Methodology,[0],[0]
Equation 1 is designed to bias the model towards more populated locations to reflect real-world data.,3 Methodology,[0],[0]
Word embeddings and/or distributional vectors encode a word’s meaning in terms of its linguistic context.,3.1 The Map Vector (MapVec),[0],[0]
"However, location (named) entities also carry explicit topological semantic knowledge such as a coordinate position and a population count for all places with an identical name.",3.1 The Map Vector (MapVec),[0],[0]
"Until now, this knowledge was only used as part of simple disparate heuristics and manual disambiguation procedures.",3.1 The Map Vector (MapVec),[0],[0]
"However, it is possible to plot this spatial data on a world map, which can then be reshaped into a 1D feature vector, or a Map Vector, the geographic representation of location mentions.",3.1 The Map Vector (MapVec),[0],[0]
MapVec is a novel standardised method for generating geographic features from text documents beyond lexical features.,3.1 The Map Vector (MapVec),[0],[0]
This enables a strong geocoding classification performance gain by extracting additional spatial knowledge that would normally be ignored.,3.1 The Map Vector (MapVec),[0],[0]
Geographic semantics cannot be inferred from language alone (too imprecise and incomplete).,3.1 The Map Vector (MapVec),[0],[0]
Word embeddings and distributional vectors use language/words as an implicit container of geographic information.,3.1 The Map Vector (MapVec),[0],[0]
"Map Vector uses a lowresolution, probabilistic world map as an explicit container of geographic information, giving us two types of semantic features from the same text.",3.1 The Map Vector (MapVec),[0],[0]
"In related papers on the generation of location representations, Rahimi et al. (2017) inverted the task of geocoding Twitter users to predict word
probability from a set of coordinates.",3.1 The Map Vector (MapVec),[0],[0]
A continuous representation of a region was generated by using the hidden layer of the neural network.,3.1 The Map Vector (MapVec),[0],[0]
"However, all locations in the same region will be assigned an identical vector, which assumes that their semantics are also identical.",3.1 The Map Vector (MapVec),[0],[0]
"Another way to obtain geographic representations is by generating embeddings directly from Geonames data using heuristics-driven DeepWalk (Perozzi et al., 2014) with geodesic distances (Kejriwal and Szekely, 2017).",3.1 The Map Vector (MapVec),[0],[0]
"However, to assign a vector, places must first be disambiguated (catch-22).",3.1 The Map Vector (MapVec),[0],[0]
"While these generation methods are original and interesting in theory, deploying them in the real-world is infeasible, hence we invented the Map Vector.
",3.1 The Map Vector (MapVec),[0],[0]
MapVec initially begins as a 180x360 world map of geodesic tiles.,3.1 The Map Vector (MapVec),[0],[0]
"There are other ways of representing the surface of the Earth such as using nested hierarchies (Melo and Martins, 2015) or k-dimensional trees (Roller et al., 2012), however, this is beyond the scope of this work.",3.1 The Map Vector (MapVec),[0],[0]
"The 1x1 tile size, in degrees of geographic coordinates, was empirically determined to be optimal to keep MapVec’s size computationally efficient while maintaining meaningful resolution.",3.1 The Map Vector (MapVec),[0],[0]
This map is then populated with the prior geographic distribution of each location mentioned in context (see Figure 2 for an example).,3.1 The Map Vector (MapVec),[0],[0]
We use population count to estimate a location’s prior probability as more populous places are more likely to be mentioned in common discourse.,3.1 The Map Vector (MapVec),[0],[0]
"For each location mention and for each of its ambiguous candidates, their prior probability is added to the correct tile indicating its geographic position (see Algorithm 1).",3.1 The Map Vector (MapVec),[0],[0]
Tiles that cover areas of open water (64.1%) were removed to reduce size.,3.1 The Map Vector (MapVec),[0],[0]
"Finally,
Data:",3.1 The Map Vector (MapVec),[0],[0]
"Text← article, paragraph, tweet, etc.",3.1 The Map Vector (MapVec),[0],[0]
"Result: MapVec location(s) representation
Locs← extractLocations(Text); MapVec← new array(length=23,002); for each l in Locs do
Cands← queryCandidatesFromDB(l); maxPop← maxPopulationOf(Cands); for each c in Cands do
prior← populationOf(c) / maxPop; i← coordinatesToIndex(c); MapVec[i]←MapVec[i] + prior;
end end m← max(MapVec); return MapVec / m;
Algorithm 1: MapVec generation.",3.1 The Map Vector (MapVec),[0],[0]
"For each extracted location l in Locs, estimate the prior probability of each candidate c. Add c’s prior probability to the appropriate array position at index i representing its geographic position/tile.",3.1 The Map Vector (MapVec),[0],[0]
"Finally, normalise the array (to a [0− 1] range) by dividing by the maximum value of the MapVec array.
",3.1 The Map Vector (MapVec),[0],[0]
"this world map is reshaped into a one-dimensional Map Vector of length 23,002.
",3.1 The Map Vector (MapVec),[0],[0]
The following features of MapVec are the most salient:,3.1 The Map Vector (MapVec),[0],[0]
"Interpretability: Word vectors typically need intrinsic (Gerz et al., 2016) and extrinsic tasks (Senel et al., 2017) to interpret their semantics.",3.1 The Map Vector (MapVec),[0],[0]
"MapVec generation is a fully transparent, human readable and modifiable method.",3.1 The Map Vector (MapVec),[0],[0]
Efficiency:,3.1 The Map Vector (MapVec),[0],[0]
MapVec is an efficient way of embedding any number of locations using the same standardised vector.,3.1 The Map Vector (MapVec),[0],[0]
"The alternative means creating, storing, disambiguating and computing with millions of unique location vectors.",3.1 The Map Vector (MapVec),[0],[0]
Domain Independence:,3.1 The Map Vector (MapVec),[0],[0]
"Word vectors vary depending on the source, time, type and language of the training data and the parameters of generation.",3.1 The Map Vector (MapVec),[0],[0]
"MapVec is language-independent and stable over time, domain, size of dataset since the world geography is objectively measured and changes very slowly.",3.1 The Map Vector (MapVec),[0],[0]
Training data was generated from geographically annotated Wikipedia pages (dumped February 2017).,3.2 Data and Preprocessing,[0],[0]
"Each page provided up to 30 training instances, limited to avoid bias from large pages.",3.2 Data and Preprocessing,[0],[0]
"This resulted in collecting approximately 1.4M
training instances, which were uniformly subsampled down to 400K to shorten training cycles as further increases offer diminishing returns.",3.2 Data and Preprocessing,[0],[0]
"We used the Python-based NLP toolkit Spacy6 (Honnibal and Johnson, 2015) for text preprocessing.",3.2 Data and Preprocessing,[0],[0]
"All words were lowercased, lemmatised, any stopwords, dates, numbers and so on were replaced with a special token (“0”).",3.2 Data and Preprocessing,[0],[0]
"Word vectors were initialised with pretrained word embeddings7 (Pennington et al., 2014).",3.2 Data and Preprocessing,[0],[0]
"We do not employ explicit feature selection as in (Bo et al., 2012), only a minimum frequency count, which was shown to work almost as well as deliberate selection (Van Laere et al., 2014).",3.2 Data and Preprocessing,[0],[0]
"The vocabulary size was limited to the most frequent 331K words, minimum ten occurrences for words and two for location references in the 1.4M training corpus.",3.2 Data and Preprocessing,[0],[0]
"A final training instance comprises four types of context information: Context Words (excluding location mentions, up to 2x200 words), Location Mentions (excluding context words, up to 2x200 words), Target Entity (up to 15 words) and the MapVec geographic representation of context locations.",3.2 Data and Preprocessing,[0],[0]
We have also checked for any overlaps between our Wikipedia-based training data and the WikToR dataset.,3.2 Data and Preprocessing,[0],[0]
Those examples were removed.,3.2 Data and Preprocessing,[0],[0]
The aforementioned 1.4M Wikipedia training corpus was once again uniformly subsampled to generate a disjoint development set of 400K instances.,3.2 Data and Preprocessing,[0],[0]
"While developing our models mainly on this data, we also used small subsets of LGL (18%), GeoVirus (26%) and WikToR (9%) described in Section 4.2 to verify that development set improvements generalised to target domains.",3.2 Data and Preprocessing,[0],[0]
"Our evaluation compares the geocoding performance of six systems from Section 2, our geocoder (CamCoder) and the population baseline.",4 Evaluation,[0],[0]
"Among these, our CNN-based model is the only neural approach.",4 Evaluation,[0],[0]
We have included all open-source/free geocoders in working order we were able to find and they are the most up-to-date versions.,4 Evaluation,[0],[0]
"Tables 1 and 2 feature several machine learning algorithms including Long-Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) to reproduce context2vec (Melamud et al., 2016), Naive Bayes (Zhang, 2004) and Random Forest (Breiman, 2001) using three diverse datasets.
",4 Evaluation,[0],[0]
6https://spacy.io/ 7https://nlp.stanford.edu/,4 Evaluation,[0],[0]
"We use the three standard and comprehensive metrics, each measuring an important aspect of geocoding, giving an accurate, holistic evaluation of performance.",4.1 Geocoding Metrics,[0],[0]
"A more detailed costbenefit analysis of geocoding metrics is available in (Karimzadeh, 2016) and (Gritta et al., 2017b).",4.1 Geocoding Metrics,[0],[0]
(1) Average (Mean) Error is the sum of all geocoding errors per dataset divided by the number of errors.,4.1 Geocoding Metrics,[0],[0]
It is an informative metric as it also indicates the total error but treats all errors as equivalent and is sensitive to outliers; (2) Accuracy@161km is the percentage of errors that are smaller than 161km (100 miles).,4.1 Geocoding Metrics,[0],[0]
"While it is easy to interpret, giving fast and intuitive understanding of geocoding performance in percentage terms, it ignores all errors greater than 161km; (3) Area Under the Curve (AUC) is a comprehensive metric, initially introduced for geocoding in (Jurgens et al., 2015).",4.1 Geocoding Metrics,[0],[0]
"AUC reduces the importance of large errors (1,000km+) since accuracy on successfully resolved places is more desirable.",4.1 Geocoding Metrics,[0],[0]
"While it is not an intuitive metric, AUC is robust to outliers and measures all errors.",4.1 Geocoding Metrics,[0],[0]
A versatile geocoder should be able to maximise all three metrics.,4.1 Geocoding Metrics,[0],[0]
"News Corpus: The Local Global Corpus (LGL) by Lieberman et al. (2010) contains 588 news articles (4460 test instances), which were collected from geographically distributed newspaper sites.
",4.2 Evaluation Datasets,[0],[0]
This is the most frequently used geocoding evaluation dataset to date.,4.2 Evaluation Datasets,[0],[0]
The toponyms are mostly smaller places no larger than a US state.,4.2 Evaluation Datasets,[0],[0]
"Approximately 16% of locations in the corpus do not have any coordinates assigned; hence, we do not use those in the evaluation, which is also how the previous figures were obtained.",4.2 Evaluation Datasets,[0],[0]
Wikipedia Corpus:,4.2 Evaluation Datasets,[0],[0]
This corpus was deliberately designed for ambiguity hence the population heuristic is not effective.,4.2 Evaluation Datasets,[0],[0]
"Wikipedia Toponym Retrieval (WikToR) by Gritta et al. (2017b) is a programmatically created corpus and although not necessarily representative of the real world distribution, it is a test of ambiguity for geocoders.",4.2 Evaluation Datasets,[0],[0]
"It is also a large corpus (25,000+ examples) containing the first few paragraphs of 5,000 Wikipedia pages.",4.2 Evaluation Datasets,[0],[0]
"High quality, free and open datasets are not readily available (GeoVirus tries to address this).",4.2 Evaluation Datasets,[0],[0]
"The following corpora could not be included: WoTR (DeLozier et al., 2016) due to limited coverage (southern US) and domain type (historical language, the 1860s), (De Oliveira et al., 2017) contains fewer than 180 locations, GeoCorpora (Wallgrün et al., 2017) could not be retrieved in full due to deleted Twitter users/tweets, GeoText (Eisenstein et al., 2010) only allows for user geocoding, SpatialML",4.2 Evaluation Datasets,[0],[0]
"(Mani et al., 2010) involves prohibitive costs, GeoSemCor (Buscaldi and Rosso, 2008) was annotated with WordNet senses (rather than coordinates).",4.2 Evaluation Datasets,[0],[0]
"We now introduce GeoVirus, an open-source test dataset for the evaluation of geoparsing of news events covering global disease outbreaks and epidemics.",4.3 GeoVirus: a New Test Dataset,[0],[0]
It was constructed from free WikiNews8 and collected during 08/2017 - 09/2017.,4.3 GeoVirus: a New Test Dataset,[0],[0]
The dataset is suitable for the evaluation of Geotagging/Named Entity Recognition and Geocoding/Toponym Resolution.,4.3 GeoVirus: a New Test Dataset,[0],[0]
"Articles were identified using the WikiNews search box and keywords such as Ebola, Bird Flu, Swine Flu, AIDS, Mad Cow Disease, West Nile Disease, etc.",4.3 GeoVirus: a New Test Dataset,[0],[0]
Off-topic articles were not included.,4.3 GeoVirus: a New Test Dataset,[0],[0]
"Buildings, POIs, street names and rivers were not annotated.
",4.3 GeoVirus: a New Test Dataset,[0],[0]
Annotation Process.,4.3 GeoVirus: a New Test Dataset,[0],[0]
"(1) The WikiNews contributor(s) who wrote the article annotated most, but not all location references.",4.3 GeoVirus: a New Test Dataset,[0],[0]
"The first author checked those annotations and identified further references, then proceeded to extract the place name, indices of the start and end characters in
8https://en.wikinews.org
text, assigned coordinates and the Wikipedia page URL for each location.",4.3 GeoVirus: a New Test Dataset,[0],[0]
(2) A second pass over the entire dataset by the first author to check and/or remedy annotations.,4.3 GeoVirus: a New Test Dataset,[0],[0]
"(3) A computer program checked that locations were tagged correctly, checking coordinates against the Geonames Database, URL correctness, eliminating any duplicates and validating XML formatting.",4.3 GeoVirus: a New Test Dataset,[0],[0]
Places without a Wikipedia page (0.6%) were assigned Geonames coordinates.,4.3 GeoVirus: a New Test Dataset,[0],[0]
"(4) The second author annotated a random 10% sample to obtain an Inter-Annotator Agreement, which was 100% for geocoding and an F-Score of 92.3 for geotagging.",4.3 GeoVirus: a New Test Dataset,[0],[0]
"GeoVirus in Numbers: Annotated locations: 2,167, Unique: 685, Continents: 94, Number of articles: 229, Most frequent places (21% of total): US, Canada, China, California, UK, Mexico, Kenya, Africa, Australia, Indonesia; Mean location occurrence: 3.2, Total word count: 63,205.",4.3 GeoVirus: a New Test Dataset,[0],[0]
"All tested models (except CamCoder) operate as end-to-end systems; therefore, it is not possible to perform geocoding separately.",5 Results,[0],[0]
"Each system geoparses its particular majority of the dataset to obtain a representative data sample, shown in Table 1 as strongly correlated scores for subsets of different sizes, with which to assess model performance.",5 Results,[0],[0]
"Table 1 also shows scores in brackets for the overlapping partition of all systems in order to compare performance on identical instances: GeoVirus 601 (26%), LGL 787 (17%) and WikToR 2,202 (9%).",5 Results,[0],[0]
"The geocoding difficulty based on the ambiguity of each dataset is: LGL (moderate to hard), WIK (very hard), GEO (easy to mod-
erate).",5 Results,[0],[0]
A population baseline also features in the evaluation.,5 Results,[0],[0]
"The baseline is conceptually simple: choose the candidate with the highest population, akin to the most frequent word sense in WSD.",5 Results,[0],[0]
"Table 1 shows the effectiveness of this heuristic, which is competitive with many geocoders, even outperforming some.",5 Results,[0],[0]
"However, the baseline is not effective on WikToR as the dataset was deliberately constructed as a tough ambiguity test.",5 Results,[0],[0]
Table 1 shows how several geocoders mirror the behaviour of the population baseline.,5 Results,[0],[0]
"This simple but effective heuristic is rarely used in system comparisons, and where evaluated (Santos et al., 2015; Leidner, 2008), it is inconsistent with expected figures (due to unpublished resources, we are unable to investigate).
",5 Results,[0],[0]
We note that no single computational paradigm dominates Table 1.,5 Results,[0],[0]
"The rule-based (Edinburgh, GeoTxt, CLAVIN), statistical (Topocluster), machine learning (CamCoder, Santos) and other (Yahoo!, Population) geocoders occupy different ranks across the three datasets.",5 Results,[0],[0]
"Due to space constraints, Table 1 does not show figures for another type of scenario we tested, a shorter lexical context, using 200 words instead of the standard 400.",5 Results,[0],[0]
"CamCoder proved to be robust to reduced context, with only a small performance decline.",5 Results,[0],[0]
"Using the same format as Table 1, AUC errors for LGL increased from 22 (18) to 23 (19), WIK from 33 (37) to 37 (40) and GEO remained the same at 31 (32).",5 Results,[0],[0]
This means that reducing model input size to save computational resources would still deliver accurate results.,5 Results,[0],[0]
"Our CNN-based lexical model performs at SOTA levels (Table 2) proving the effectiveness of linguistic features while being
the outstanding geocoder on the highly ambiguous WikToR data.",5 Results,[0],[0]
"The Multi-Layer Perceptron (MLP) model using only MapVec with no lexical features is almost as effective but more importantly, it is significantly better than the population baseline (Table 2).",5 Results,[0],[0]
"This is because the Map Vector benefits from wide contextual awareness, encoded in Algorithm 1, while a simple population baseline does not.",5 Results,[0],[0]
"When we combined the lexical and geographic feature spaces in one model (CamCoder9), we observed a substantial increase in the SOTA scores.",5 Results,[0],[0]
"We have also reproduced the context2vec model to obtain a continuous context representation using bidirectional LSTMs to encode lexical features, denoted as LSTM10 in Table 2.",5 Results,[0],[0]
This enabled us to test the effect of integrating MapVec into another deep learning model as opposed to CNNs.,5 Results,[0],[0]
"Supplemented with MapVec, we observed a significant improvement, demonstrating how enriching various neural models with a geographic vector representation boosts classification results.
",5 Results,[0],[0]
Deep learning is the dominant paradigm in our experiments.,5 Results,[0],[0]
"However, it is important that MapVec is still effective with simpler machine learning algorithms.",5 Results,[0],[0]
"To that end, we have evaluated it with the Random Forest without using any lexical features.",5 Results,[0],[0]
"This model was well suited to the geocoding task despite training with only half of the 400K training data (due to memory constraints, partial fit is unavailable for batch training in SciKit Learn).",5 Results,[0],[0]
Scores were on par with more sophisticated systems.,5 Results,[0],[0]
"The Naive Bayes was less ef-
9Single model settings/parameters for all tests.",5 Results,[0],[0]
"10https://keras.io/layers/recurrent/
fective with MapVec though still somewhat viable as a geocoder given the lack of lexical features and a naive algorithm, narrowly beating population.",5 Results,[0],[0]
GeoVirus scores remain highly competitive across most geocoders.,5 Results,[0],[0]
"This is due to the nature of the dataset; locations skewed towards their dominant “senses” simulating ideal geocoding conditions, enabling high accuracy for the population baseline.",5 Results,[0],[0]
"GeoVirus alone may not serve as the best scenario to assess a geocoder’s performance, however, it is nevertheless important and valuable to determine behaviour in a standard environment.",5 Results,[0],[0]
"For example, GeoVirus helped us diagnose Yahoo! Placemaker’s lower accuracy in what should be an easy test for a geocoder.",5 Results,[0],[0]
"The figures show that while the average error is low, the accuracy@161km is noticeably lower than most systems.",5 Results,[0],[0]
"When coupled with other complementary datasets such as LGL and WikToR, it facilitates a comprehensive assessment of geocoding behaviour in many types of scenarios, exposing potential domain dependence.",5 Results,[0],[0]
"We note that GeoVirus has a dual function, NER (not evaluated but useful for future work) and Geocoding.",5 Results,[0],[0]
"We made all of our resources freely available11 for full reproducibility (Goodman et al., 2016).",5 Results,[0],[0]
The Pearson correlation coefficient of the target entity ambiguity and the error size was only r ≈ 0.2 suggesting that CamCoder’s geocoding errors do not simply rise with location ambiguity.,5.1 Discussion and Errors,[0],[0]
Errors were also not correlated (r ≈ 0.0) with population size with all types of locations geocoded to various degrees of accuracy.,5.1 Discussion and Errors,[0],[0]
"All error curves follow
11https://github.com/milangritta/
a power law distribution with between 89% and 96% of errors less than 1500km, the rest rapidly increasing into thousands of kilometers.",5.1 Discussion and Errors,[0],[0]
Errors also appear to be uniformly geographically distributed across the world.,5.1 Discussion and Errors,[0],[0]
The strong lexical component shown in Table 2 is reflected by the lack of a relationship between error size and the number of locations found in the context.,5.1 Discussion and Errors,[0],[0]
The number of total words in context is also independent of geocoding accuracy.,5.1 Discussion and Errors,[0],[0]
This suggests that CamCoder learns strong linguistic cues beyond simple association of place names with the target entity and is able to cope with flexible-sized contexts.,5.1 Discussion and Errors,[0],[0]
A CNN Geocoder would expect to perform well for the following reasons: Our context window is 400 words rather than 10-40 words as in previous approaches.,5.1 Discussion and Errors,[0],[0]
"The model learns 1,000 feature maps per input and per feature type, tracking 5,000 different word patterns (unigrams, bigrams and trigrams), a significant text processing capability.",5.1 Discussion and Errors,[0],[0]
"The lexical model also takes advantage of our own 50-dimensional word embeddings, tuned on geographic Wikipedia pages only, allowing for greater generalisation than bag-of-unigrams models; and the large training/development datasets (400K each), optimising geocoding over a diverse global set of places allowing our model to generalise to unseen instances.",5.1 Discussion and Errors,[0],[0]
We note that MapVec generation is sensitive to NER performance with higher F-Scores leading to better quality of the geographic vector representation(s).,5.1 Discussion and Errors,[0],[0]
Precision errors can introduce noise while recall errors may withhold important locations.,5.1 Discussion and Errors,[0],[0]
The average F-Score for the featured geoparsers is F ≈ 0.7 (standard deviation ≈ 0.1).,5.1 Discussion and Errors,[0],[0]
Spacy’s NER performance over the three datasets is also F ≈ 0.7 with a similar variation between datasets.,5.1 Discussion and Errors,[0],[0]
"In order to further interpret scores in Tables 1 and 2, with respect to maximising geocoding performance, we briefly discuss the Oracle score.",5.1 Discussion and Errors,[0],[0]
"Oracle is the geocoding performance upper bound given by the Geonames data, i.e. the highest possible score(s) using Geonames coordinates as the geocoding output.",5.1 Discussion and Errors,[0],[0]
"In other words, it quantifies the minimum error for each dataset given the perfect location disambiguation.",5.1 Discussion and Errors,[0],[0]
This means it quantifies the difference between “gold standard” coordinates and the coordinates in the Geonames database.,5.1 Discussion and Errors,[0],[0]
"The following are the Oracle scores for LGL (AUC=0.04, a@161km=99) annotated with Geonames, WikToR (AUC=0.14, a@161km=92) and GeoVirus
(AUC=0.27, a@161km=88), which are annotated with Wikipedia data.",5.1 Discussion and Errors,[0],[0]
"Subtracting the Oracle score from a geocoder’s score quantifies the scope of its theoretical future improvement, given a particular database/gazetteer.",5.1 Discussion and Errors,[0],[0]
"Geocoding methods commonly employ lexical features, which have proved to be very effective.",6 Conclusions and Future Work,[0],[0]
Our lexical model was the best languageonly geocoder in extensive tests.,6 Conclusions and Future Work,[0],[0]
"It is possible, however, to go beyond lexical semantics.",6 Conclusions and Future Work,[0],[0]
"Locations also have a rich topological meaning, which has not yet been successfully isolated and deployed.",6 Conclusions and Future Work,[0],[0]
We need a means of extracting and encoding this additional knowledge.,6 Conclusions and Future Work,[0],[0]
"To that end, we introduced MapVec, an algorithm and a container for encoding context locations in geodesic vector space.",6 Conclusions and Future Work,[0],[0]
"We showed how CamCoder, using lexical and MapVec features, outperformed both approaches, achieving a new SOTA.",6 Conclusions and Future Work,[0],[0]
"MapVec remains effective with various machine learning frameworks (Random Forest, CNN and MLP) and substantially improves accuracy when combined with other neural models (LSTMs).",6 Conclusions and Future Work,[0],[0]
"Finally, we introduced GeoVirus, an open-source dataset that helps facilitate geoparsing evaluation across more diverse domains with different lexical-geographic distributions (Flatow et al., 2015; Dredze et al., 2016).",6 Conclusions and Future Work,[0],[0]
"Tasks that could benefit from our methods include social media placing tasks (Choi et al., 2014), inferring user location on Twitter (Zheng et al., 2017), geolocation of images based on descriptions (Serdyukov et al., 2009) and detecting/analyzing incidents from social media (Berlingerio et al., 2013).",6 Conclusions and Future Work,[0],[0]
Future work may see our methods applied to document geolocation to assess the effectiveness of scaling geodesic vectors from paragraphs to entire documents.,6 Conclusions and Future Work,[0],[0]
"We gratefully acknowledge the funding support of the Natural Environment Research Council (NERC) PhD Studentship NE/M009009/1 (Milan Gritta, DREAM CDT), EPSRC (Nigel Collier) Grant Number EP/M005089/1 and MRC (Mohammad Taher Pilehvar) Grant Number MR/M025160/1 for PheneBank.",Acknowledgements,[0],[0]
We also gratefully acknowledge NVIDIA Corporation’s donation of the Titan Xp GPU used for this research.,Acknowledgements,[0],[0]
"The purpose of text geolocation is to associate geographic information contained in a document with a set (or sets) of coordinates, either implicitly by using linguistic features and/or explicitly by using geographic metadata combined with heuristics.",abstractText,[0],[0]
We introduce a geocoder (location mention disambiguator) that achieves state-of-the-art (SOTA) results on three diverse datasets by exploiting the implicit lexical clues.,abstractText,[0],[0]
"Moreover, we propose a new method for systematic encoding of geographic metadata to generate two distinct views of the same text.",abstractText,[0],[0]
"To that end, we introduce the Map Vector (MapVec), a sparse representation obtained by plotting prior geographic probabilities, derived from population figures, on a World Map.",abstractText,[0],[0]
We then integrate the implicit (language) and explicit (map) features to significantly improve a range of metrics.,abstractText,[0],[0]
We also introduce an open-source dataset for geoparsing of news events covering global disease outbreaks and epidemics to help future evaluation in geoparsing.,abstractText,[0],[0]
Which Melbourne? Augmenting Geocoding with Maps,title,[0],[0]
"Online discussion forums and community question-answering websites provide one of the primary avenues for online users to share information. In this paper, we propose text mining techniques which aid users navigate troubleshooting-oriented data such as questions asked on forums and their suggested solutions. We introduce Bayesian generative models of the troubleshooting data and apply them to two interrelated tasks: (a) predicting the complexity of the solutions (e.g., plugging a keyboard in the computer is easier compared to installing a special driver) and (b) presenting them in a ranked order from least to most complex. Experimental results show that our models are on par with human performance on these tasks, while outperforming baselines based on solution length or readability.",text,[0],[0]
"Online forums and discussion boards have created novel ways for discovering, sharing, and distributing information.",1 Introduction,[0],[0]
Users typically post their questions or problems and obtain possible solutions from other users.,1 Introduction,[0],[0]
"Through this simple mechanism of community-based question answering, it is possible to find answers to personal, open-ended, or highly specialized questions.",1 Introduction,[0],[0]
"However, navigating the information available in web-archived data can be challenging given the lack of appropriate search and browsing facilities.
",1 Introduction,[0],[0]
Table 1 shows examples typical of the problems and proposed solutions found in troubleshootingoriented online forums.,1 Introduction,[0],[0]
The first problem concerns a shaky monitor and has three solutions with increasing degrees of complexity.,1 Introduction,[0],[0]
"Solution (1) is probably easiest to implement in terms of user time, effort, and expertise; solution (3) is most complex (i.e., the user should understand what signal timing is and
then try to establish whether it is within the specification of the monitor), whereas solution (2) is somewhere in between.",1 Introduction,[0],[0]
"In most cases, the solutions are not organized in any particular fashion, neither in terms of content nor complexity.
",1 Introduction,[0],[0]
"In this paper, we present models to automatically predict the complexity of troubleshooting solutions, which we argue could improve user experience, and potentially help solve the problem faster (e.g., by prioritizing easier solutions).",1 Introduction,[0],[0]
Automatically structuring solutions according to complexity could also facilitate search through large archives of solutions or serve as a summarization tool.,1 Introduction,[0],[0]
"From a linguistic perspective, learning how complexity is verbalized can be viewed as an instance of grounded language acquisition.",1 Introduction,[0],[0]
"Solutions direct users to carry out certain actions (e.g., on their computers or devices) and complexity is an attribute of these actions.",1 Introduction,[0],[0]
Information access systems incorporating a notion of complexity would allow to take user intentions into account and how these translate into natural language.,1 Introduction,[0],[0]
Current summarization and information retrieval methods are agnostic of such types of text semantics.,1 Introduction,[0],[0]
"Moreover, the models presented here could be used for analyzing collaborative prob-
73
Transactions of the Association for Computational Linguistics, vol. 3, pp.",1 Introduction,[0],[0]
"73–85, 2015.",1 Introduction,[0],[0]
Action Editor: Eric Fosler-Lussier.,1 Introduction,[0],[0]
Submission batch: 9/2014; Revision batch 1/2015; Published 2/2015.,1 Introduction,[0],[0]
"c©2015 Association for Computational Linguistics.
lem solving and its social networks.",1 Introduction,[0],[0]
"Characterizing the content of discussion forums by their complexity can provide additional cues for identifying user authority and if there is a need for expert intervention.
",1 Introduction,[0],[0]
We begin by validating that the task is indeed meaningful and that humans perceive varying degrees of complexity when reading troubleshooting solutions.,1 Introduction,[0],[0]
We also show experimentally that users agree in their intuitions about the relative complexity of different solutions to the same problem.,1 Introduction,[0],[0]
"We define “complexity” as an aggregate notion of the time, expertise, and money required to implement a solution.",1 Introduction,[0],[0]
"We next model the complexity prediction task, following a Bayesian approach.",1 Introduction,[0],[0]
"Specifically, we learn to assign complexity levels to solutions based on their linguistic makeup.",1 Introduction,[0],[0]
We leverage weak supervision in the form of lists of solutions (to different problems) approximately ordered from low to high complexity (see Table 1).,1 Introduction,[0],[0]
We assume that the data is generated from a fixed number of discrete complexity levels.,1 Introduction,[0],[0]
Each level has a probability distribution over the vocabulary and there is a canonical ordering between levels indicating their relative complexity.,1 Introduction,[0],[0]
"During inference, we recover the vocabularies of the complexity levels and the ordering of levels that explains the solutions and their attested sequences in the training data.
",1 Introduction,[0],[0]
We explore two Bayesian models differing in how they learn an ordering among complexity levels.,1 Introduction,[0],[0]
"The first model is local, it assigns an expected position (in any list of solutions) to each complexity level and orders the levels based on this expected position value.",1 Introduction,[0],[0]
"The second model is global, it defines probabilities over permutations of complexity levels and directly uncovers a consensus ordering from the training data.",1 Introduction,[0],[0]
"We evaluate our models on a solution ordering task, where the goal is to rank solutions from least to most complex.",1 Introduction,[0],[0]
We show that a supervised ranking approach using features based on the predictions of our generative models is on par with human performance on this task while outperforming competitive baselines based on length and readability of the solution text.,1 Introduction,[0],[0]
"There is a long tradition of research on decisiontheoretic troubleshooting where the aim is to find a cost efficient repair strategy for a malfunctioning device (Heckerman et al., 1995).",2 Related work,[0],[0]
"Typically, a
diagnostic procedure (i.e., a planner) is developed that determines the next best troubleshooting step by estimating the expected cost of repair for various plans.",2 Related work,[0],[0]
Costs are specified by domain experts and are usually defined in terms of time and/or money incurred by carrying out a particular repair action.,2 Related work,[0],[0]
"Our notion of complexity is conceptually similar to the cost of an action, however we learn to predict complexity levels rather than calibrate them manually.",2 Related work,[0],[0]
Also note that our troubleshooting task is not device specific.,2 Related work,[0],[0]
"Our models learn from troubleshootingoriented data without any restrictions on the problems being solved.
",2 Related work,[0],[0]
Previous work on web-based user support has mostly focused on thread analysis.,2 Related work,[0],[0]
"The idea is to model the content structure of forum threads by analyzing the requests for information and suggested solutions in the thread data (Wang et al., 2011; Kim et al., 2010).",2 Related work,[0],[0]
"Examples of such analysis include identifying which earlier post(s) a given post responds to and in what manner (e.g., is it a question, an answer or a confirmation).",2 Related work,[0],[0]
"Other related work (Lui and Baldwin, 2009) identifies user characteristics in such data, i.e., whether users express themselves clearly, whether they are technically knowledgeable, and so on.",2 Related work,[0],[0]
"Although our work does not address threaded discourse, we analyze the content of troubleshooting data and show that it is possible to predict the complexity levels for suggested solutions from surface lexical cues.
",2 Related work,[0],[0]
"Our work bears some relation to language grounding, the problem of extracting representations of the meaning of natural language tied to the physical world.",2 Related work,[0],[0]
"Mapping instructions to executable actions is an instance of language grounding with applications to automated troubleshooting (Branavan et al., 2009; Eisenstein et al., 2009), navigation (Vogel and Jurafsky, 2010), and game-playing (Branavan et al., 2011).",2 Related work,[0],[0]
"In our work, there is no direct attempt to model the environment or the troubleshooting steps.",2 Related work,[0],[0]
"Rather, we study the language of instructions and how it correlates with the complexity of the implied actions.",2 Related work,[0],[0]
"Our results show that it possible to predict complexity, while being agnostic about the semantics of the domain or the effect of the instructions in the corresponding environment.
",2 Related work,[0],[0]
"Our generative models are trained on existing archives of problems with corresponding solutions (approximately ordered from least to most complex)
and learn to predict an ordering for new sets of solutions.",2 Related work,[0],[0]
This setup is related to previous studies on information ordering where the aim is to learn statistical patterns of document structure which can be then used to order new sentences or paragraphs in a coherent manner.,2 Related work,[0],[0]
"Some approaches approximate the structure of a document via topic and entity sequences using local dependencies such as conditional probabilities (Lapata, 2003; Barzilay and Lapata, 2008) or Hidden Markov Models (Barzilay and Lee, 2004).",2 Related work,[0],[0]
"More recently, global approaches which directly model the permutations of topics in the document have been proposed (Chen et al., 2009b).",2 Related work,[0],[0]
"Following this line of work, one of our models uses the Generalized Mallows Model (Fligner and Verducci, 1986) in its generative process which allows to model permutations of complexity levels in the training data.",2 Related work,[0],[0]
Our aim in this work is to learn models which can automatically reorder solutions to a problem from low to high complexity.,3 Problem formulation,[0],[0]
"Let G = (c1, c2, .. cN ) be a collection of solutions to a specific problem.",3 Problem formulation,[0],[0]
"We wish to output a list G′ = (c′1, c ′ 2, .. c ′ N ), such that D(c′j) ≤ D(c′j+1), where D(x) refers to the complexity of solution x.",3 Problem formulation,[0],[0]
As training data we are given problem-solution sets similar to the examples in Table 1 where the solutions are approximately ordered from low to high complexity.,3.1 Corpus Collection,[0],[0]
"A solution set Si is specific to problem Pi, and contains an ordered list of NPi solutions",3.1 Corpus Collection,[0],[0]
"Si = (x1, x2, . . .",3.1 Corpus Collection,[0],[0]
",",3.1 Corpus Collection,[0],[0]
xNPi ) such that D(xj) <D(xj+1).,3.1 Corpus Collection,[0],[0]
"We refer to the number of solutions related to a problem, NPi , as its solution set size.
",3.1 Corpus Collection,[0],[0]
"For our experiments, we collected 300 problems1 and their solutions from multiple web sites including the computing support pages of Microsoft, Apple, HP, as well as amateur computer help websites such as www.computerhope.com.",3.1 Corpus Collection,[0],[0]
The problems were mostly frequently asked questions (FAQs) referring to malfunctioning personal computers and smart phones.,3.1 Corpus Collection,[0],[0]
"The solutions were provided by computer experts or experienced users in the absence
1The corpus can be downloaded from http: //www.homepages.inf.ed.ac.uk/alouis/ solutionComplexity.html.
of any interaction with other users or their devices and thus constitute a generic list of steps to try out.",3.1 Corpus Collection,[0],[0]
"We assume that in such a situation, the solution providers are likely to suggest simpler solutions before other complex ones, leading to the solution lists being approximately ordered from low to high complexity.",3.1 Corpus Collection,[0],[0]
"In the next section, we verify this assumption experimentally.",3.1 Corpus Collection,[0],[0]
"In this dataset, the solution set size varies between 2 and 16 and the average number is 4.61.",3.1 Corpus Collection,[0],[0]
Figure 1 illustrates the histogram of solution set sizes found in our corpus.,3.1 Corpus Collection,[0],[0]
We only considered problems which have no less than two solutions.,3.1 Corpus Collection,[0],[0]
All words in the corpus were lemmatized and html links and numbers were replaced with placeholders.,3.1 Corpus Collection,[0],[0]
"The resulting vocabulary was approximately 2,400 word types (62,152 tokens).
",3.1 Corpus Collection,[0],[0]
Note that our dataset provides only weak supervision for learning.,3.1 Corpus Collection,[0],[0]
"The relative complexity of solutions for the same problem is observed, however, the relative complexity of solutions across different problems is unknown.",3.1 Corpus Collection,[0],[0]
"For example, a hardware issue may generally receive highly complex solutions whereas a microphone issue mostly simple ones.",3.1 Corpus Collection,[0],[0]
"In this section, we detail an annotation experiment where we asked human judges to rank the randomly permuted contents of a solution set according to perceived complexity.",3.2 Task Validation,[0],[0]
We performed this study for two reasons.,3.2 Task Validation,[0],[0]
"Firstly, to ascertain that participants are able to distinguish degrees of complexity and agree on the complexity level of a solution.",3.2 Task Validation,[0],[0]
"Secondly, to examine whether the ordering produced by participants corresponds to the (gold-standard) FAQ order of the solutions.",3.2 Task Validation,[0],[0]
"If true, this would support our hy-",3.2 Task Validation,[0],[0]
"pothesis that the solutions in our FAQ corpus are frequently presented according to complexity and that this ordering is reasonable supervision for our models.
",D 0.402,[0],[0]
Method We randomly sampled 100 solution sets (their sizes vary between 2 and 16) from the FAQ corpus described in the previous section and randomly permuted the contents of each set.,D 0.402,[0],[0]
"Four annotators, one an author of this paper, and three graduate and undergraduate students in Computer Science were asked to order the solutions in each set from easy to most complex.",D 0.402,[0],[0]
An easier solution was defined as one “which takes less time or effort to carry out by a user”.,D 0.402,[0],[0]
The annotators saw a list of solutions for the same problem on a web interface and assigned a rank to each solution to create an order.,D 0.402,[0],[0]
No ties were allowed and a complete ordering was required to keep the annotation simple.,D 0.402,[0],[0]
The annotators were fluent English speakers and had some knowledge of computer hardware and software.,D 0.402,[0],[0]
"We refrained from including novice users in our study as they are likely to have very different personal preferences resulting in more divergent rankings.
",D 0.402,[0],[0]
"Results We measured inter-annotator agreement using Kendall’s τ , a metric of rank correlation which has been reliably used in information ordering evaluations (Lapata, 2006; Bollegala et al., 2006; Madnani et al., 2007).",D 0.402,[0],[0]
"τ ranges between −1 and +1, where +1 indicates equivalent rankings, −1 completely reverse rankings, and 0 independent rankings.",D 0.402,[0],[0]
Table 2 shows the pairwise inter-annotator agreement as well as the agreement between each annotator and the original FAQ order.,D 0.402,[0],[0]
The table shows fair agreement between the annotators confirming that this is a reasonable task for humans to do.,D 0.402,[0],[0]
"As can be seen, there are some individual differences, with the inter-annotator agreement varying from 0.421 (for A,B) to 0.625 (for A,D).
",D 0.402,[0],[0]
"The last column in Table 2 reports the agreement
between our annotator rankings and the original ordering of solutions in the FAQ data.",D 0.402,[0],[0]
"Although there is fair agreement with the FAQ providing support for its use as a gold-standard, the overall τ values are lower compared to inter-annotator agreement.",D 0.402,[0],[0]
"This implies that the ordering may not be strictly increasing in complexity in our dataset and that our models should allow for some flexibility during learning.
",D 0.402,[0],[0]
"Several reasons contributed to disagreements between annotators and with the FAQ ordering, such as the users’ expertise, personal preferences, or the nature of the solutions.",D 0.402,[0],[0]
"For instance, annotators disagreed when multiple solutions were of similar complexity.",D 0.402,[0],[0]
"For the first example in Table 1, all annotators agreed perfectly and also matched the FAQ order.",D 0.402,[0],[0]
"For the second example, the annotators disagreed with each other and the FAQ.",D 0.402,[0],[0]
In the following we introduce two Bayesian topic models for the complexity prediction (and ranking) task.,4 Generative Models,[0],[0]
"In these models, complexity is captured through a discrete set D of L levels and a total ordering between the levels reflects their relative complexity.",4 Generative Models,[0],[0]
"In other words, D = (d1, d2, ...dL), where d1 is easiest level and D(dm) < D(dm+1) .",4 Generative Models,[0],[0]
"Each complexity level is parametrized by a unigram language model which captures words likely to occur in solutions with that level.
",4 Generative Models,[0],[0]
Our two models are broadly similar.,4 Generative Models,[0],[0]
Their generative process assigns a complexity level from D to each solution such that it explains the words in the solution and also the ordering of solutions within each solution set.,4 Generative Models,[0],[0]
Words are generated for each solution by mixing problem-specific words with solution-specific (and hence complexityrelated) ones.,4 Generative Models,[0],[0]
"Also, each problem has its own distribution over complexity levels which allows for some problems to have more complex solutions on average, some a mix of high and low complexity solutions, or otherwise predominantly easier solutions.
",4 Generative Models,[0],[0]
The main difference between the two models is in the way they capture the ordering between levels.,4 Generative Models,[0],[0]
Our first model infers a distribution for each level over the positions at which a solution with that complexity can occur and uses this distribution to order the levels.,4 Generative Models,[0],[0]
Levels which on average occur at greater positions have higher complexity.,4 Generative Models,[0],[0]
"The second model defines probabilities over orderings of levels in the
generative process itself.",4 Generative Models,[0],[0]
The inference process of this model allows to directly uncover a canonical ordering of the levels which explains the training data.,4 Generative Models,[0],[0]
This model infers the vocabulary associated with a complexity level and a distribution over the numerical positions in a solution set where such a complexity level is likely to occur.,4.1 Expected Position model,[0],[0]
"After inference, the model uses the position distribution to compute the expected position of each complexity level.",4.1 Expected Position model,[0],[0]
"The levels are ordered from low to high expected position and taken as the order of increasing complexity.
",4.1 Expected Position model,[0],[0]
The generative process for our model is described in Figure 2.,4.1 Expected Position model,[0],[0]
A first phase generates the latent variables which are drawn once for the entire corpus.,4.1 Expected Position model,[0],[0]
"Then, variables are drawn for a solution set, next for each solution in the set, and finally for the words in the solutions.",4.1 Expected Position model,[0],[0]
"The number of complexity levels L is a parameter in the model, while the vocabulary
size V is fixed.",4.1 Expected Position model,[0],[0]
"For each complexity level dm, we draw one multinomial distribution φ over the vocabulary V , and another multinomial γ over the possible positions.",4.1 Expected Position model,[0],[0]
These two distributions are drawn from symmetric Dirichlet priors with hyperparameters α and ρ.,4.1 Expected Position model,[0],[0]
Solutions will not only contain words relating to their complexity but also to the problem or malfunctioning component at hand.,4.1 Expected Position model,[0],[0]
We assume these words play a minor role in determining complexity and thus draw a binomial distribution ψ that balances the amount of problem-specific versus complexity-specific vocabulary.,4.1 Expected Position model,[0],[0]
"This distribution has a Beta prior with hyperparameters δ0 and δ1.
",4.1 Expected Position model,[0],[0]
"For each solution set, we draw a distribution over the complexity levels θ from another Dirichlet prior with concentration β.",4.1 Expected Position model,[0],[0]
This distribution allows each problem to take a different preference and mix of complexity levels for its solutions.,4.1 Expected Position model,[0],[0]
Another multinomial λ over the vocabulary is drawn for the problemspecific content of each solution set.,4.1 Expected Position model,[0],[0]
"λ is given a symmetric Dirichlet prior with concentration ω.
",4.1 Expected Position model,[0],[0]
"For each individual solution in a set, we draw a complexity level z from θ, i.e., the complexity level proportions for that problem.",4.1 Expected Position model,[0],[0]
"A position for the solution is then drawn from the position distribution for that level, i.e., γz .",4.1 Expected Position model,[0],[0]
The words in the solution are generated by first drawing a switch value for each word indicating if the word came from the problem’s technical or complexity vocabulary.,4.1 Expected Position model,[0],[0]
"Accordingly, the word is drawn from λ or φz .
",4.1 Expected Position model,[0],[0]
"During inference, we are interested in the posterior of the model given the FAQ training data.",4.1 Expected Position model,[0],[0]
"Based on the conditional independencies of the model, the posterior is proportional to:
P (ψ|δ0, δ1)× L∏
m=1",4.1 Expected Position model,[0],[0]
[P (φm|α)]× L∏ m=1,4.1 Expected Position model,[0],[0]
"[P (γm|ρ)]
× N∏ i=1",4.1 Expected Position model,[0],[0]
[P (θi|β)]× N∏ i=1,4.1 Expected Position model,[0],[0]
[P (λi|ω)] × N∏ i=1,4.1 Expected Position model,[0],[0]
NPi∏ j=1,4.1 Expected Position model,[0],[0]
[P (zij |θi)P,4.1 Expected Position model,[0],[0]
(rij |γzij )],4.1 Expected Position model,[0],[0]
× N∏ i=1,4.1 Expected Position model,[0],[0]
NPi∏ j=1 |xij |∏,4.1 Expected Position model,[0],[0]
k=1,4.1 Expected Position model,[0],[0]
"[P (sijk|ψ)P (wijk|sijk, φzij , λi)]
where L is the number of complexity levels, N the number of problems in the training corpus, NPi the size of solution set for problem Pi, and |xij | the number of words in solution xij .
",4.1 Expected Position model,[0],[0]
"The use of conjugate priors for the multinomial
and binomial distributions allows us to integrate out the ψ, φ, γ, λ and θ distributions.",4.1 Expected Position model,[0],[0]
"The simplified posterior is proportional to:
N∏ i=1
L∏ m=1 Γ(Rmi",4.1 Expected Position model,[0],[0]
"+βm)
Γ
( L∑
m=1 Rmi +βm
) ×",4.1 Expected Position model,[0],[0]
"L∏
m=1
G∏ r=1 Γ(Qrm+ρr)
",4.1 Expected Position model,[0],[0]
"Γ
( G∑
r=1 Qrm+ρr
)
",4.1 Expected Position model,[0],[0]
"× 1∏ u=0 Γ(Tu+δu)
Γ
( 1∑
u=0 Tu+δu
)",4.1 Expected Position model,[0],[0]
×,4.1 Expected Position model,[0],[0]
"L∏
m=1
V∏ v=1 Γ(T 0m(v)+αv)
",4.1 Expected Position model,[0],[0]
"Γ
( V∑
v=1 T 0m(v)+αv
)
× N∏ i=1
V∏ v=1 Γ(T 1i (v)+ωv)
",4.1 Expected Position model,[0],[0]
"Γ
( V∑
v=1 T 1i (v)+ωv
)
where Rmi is the number of times level m is assigned to a solution in problem i. Qrm is the number of times a solution with position r is given complexity m over the full corpus.",4.1 Expected Position model,[0],[0]
Positions are integer values between 1 and G. T 0 and T 1 count the number of switch assignments of value 0 (complexityrelated word) and 1 (technical word) respectively in the corpus.,4.1 Expected Position model,[0],[0]
"T 0m(v) is a refined count of the number of times word type v is assigned switch value 0 in a solution of complexity m. T 1i (v) counts the number of times switch value 1 is given to word type v in a solution set for problem i.
",4.1 Expected Position model,[0],[0]
We sample from this posterior using a collapsed Gibbs sampling algorithm.,4.1 Expected Position model,[0],[0]
The sampling sequence starts with a random initialization to the hidden variables.,4.1 Expected Position model,[0],[0]
"During each iteration, the sampler chooses a complexity level for each solution based on the current assignments to all other variables.",4.1 Expected Position model,[0],[0]
Then the switch values for the words in each solution are sampled one by one.,4.1 Expected Position model,[0],[0]
The hyperparameters are tuned using grid search on development data.,4.1 Expected Position model,[0],[0]
"The language model concentrations α, ρ and ω are given values less than 1 to obtain sparse distributions.",4.1 Expected Position model,[0],[0]
"The prior on θ, the topic proportions, is chosen to be greater than 1 to encourage different complexity levels to be used within the same problem rather than assigning all solutions to the same one or two levels.",4.1 Expected Position model,[0],[0]
"Similarly, δ0 and δ1 are > 1.",4.1 Expected Position model,[0],[0]
"We run 5,000 sampling iterations and use the last sample as a draw from the posterior.",4.1 Expected Position model,[0],[0]
"Using these assignments, we also compute an estimate for the parameters φm, λi and γm.",4.1 Expected Position model,[0],[0]
"For example, the probability of a word v in φm is computed as T 0 m(v)+αv∑
v(T 0 m(v)+αv) .",4.1 Expected Position model,[0],[0]
"After inference, we obtain probability distribu-
tions for each complexity level dm over the vocabulary φm and positions γm.",4.1 Expected Position model,[0],[0]
"We compute the expected position Ep of dm as:
Ep(dm) = G∑
pos=1
pos ∗ γm(pos)",4.1 Expected Position model,[0],[0]
"(1)
where pos indicates position values.",4.1 Expected Position model,[0],[0]
"Then, we rank the levels in increasing order of Ep.",4.1 Expected Position model,[0],[0]
In our second model we incorporate the ordering of complexity levels in the generative process itself.,4.2 Permutations-based model,[0],[0]
This is achieved by using the Generalized Mallows Model (GMM; Fligner and Verducci (1986)) within our hierarchical generative process.,4.2 Permutations-based model,[0],[0]
The GMM is a probabilistic model over permutations of items and is frequently used to learn a consensus ordering given a set of different rankings.,4.2 Permutations-based model,[0],[0]
"It assumes there is an underlying canonical order of items and concentrates probability mass on those permutations that differ from the canonical order by a small amount, while assigning lesser probability to very divergent permutations.",4.2 Permutations-based model,[0],[0]
"Probabilistic inference in this model uncovers the canonical ordering.
",4.2 Permutations-based model,[0],[0]
"The standard Mallows model (Mallows, 1957) has two parameters, a canonical ordering σ and a dispersion penalty ρ > 0.",4.2 Permutations-based model,[0],[0]
"The probability of an observed ordering π is defined as:
P (π|ρ,σ) = e −ρd(π,σ)
ξ(ρ) ,
where d(π,σ) is a distance measure such as Kendall’s τ , between the canonical ordering σ and an observed ordering π.",4.2 Permutations-based model,[0],[0]
"The GMM decomposes d(π,σ) in a way that captures item-specific distance.",4.2 Permutations-based model,[0],[0]
"This is done by computing an inversion vector representation of d(π,σ).",4.2 Permutations-based model,[0],[0]
"A permutation π of n items can be equivalently represented by a vector of inversion counts v of length n−1, where each component vi equals the number of items",4.2 Permutations-based model,[0],[0]
j >,4.2 Permutations-based model,[0],[0]
i that occur before item i in π.,4.2 Permutations-based model,[0],[0]
The dimension of v is n− 1 since there can be no items greater than the highest value element.,4.2 Permutations-based model,[0],[0]
"A unique inversion vector can be computed for any permutation and vice versa, and the sum of the inversion vector elements is equal to d(π,σ).",4.2 Permutations-based model,[0],[0]
Each vi is also given a separate dispersion penalty ρi.,4.2 Permutations-based model,[0],[0]
"Then, the GMM is defined as:
GMM(v|ρ) ∝",4.2 Permutations-based model,[0],[0]
"∏
i
e−ρivi (2)
",4.2 Permutations-based model,[0],[0]
"And can be further factorized into item-specific components:
GMMi(vi|ρi) ∝",4.2 Permutations-based model,[0],[0]
"e−ρivi (3)
Since the GMM is a member of the exponential family, a conjugate prior can be defined for each dispersion parameter ρi which allows for efficient inference.",4.2 Permutations-based model,[0],[0]
"We refer the interested reader to Chen et al. (2009a) for details on the prior distribution and normalization factor for the GMM distribution.
",4.2 Permutations-based model,[0],[0]
Figure 3 formalizes the generative story of our own model which uses the GMM as a component.,4.2 Permutations-based model,[0],[0]
"We assume the canonical order is the strictly increasing (1, 2, .., L) order.",4.2 Permutations-based model,[0],[0]
"For each complexity level dm, we draw a distribution φ over the vocabulary.",4.2 Permutations-based model,[0],[0]
We also drawL− 1 dispersion parameters from the conjugate prior GMM0 density.,4.2 Permutations-based model,[0],[0]
Hyperparameters for this prior are set in a similar fashion to Chen et al. (2009a).,4.2 Permutations-based model,[0],[0]
"As in the position model, we draw
a binomial distribution, ψ",4.2 Permutations-based model,[0],[0]
(with a beta prior) over complexity- versus problem-specific vocabulary.,4.2 Permutations-based model,[0],[0]
"At the solution set level, we draw a multinomial distribution λ over the vocabulary and a multinomial distribution θ for the proportion of L levels for this problem.",4.2 Permutations-based model,[0],[0]
Both these distributions have Dirichlet priors.,4.2 Permutations-based model,[0],[0]
"Next, we generate an ordering for the complexity levels.",4.2 Permutations-based model,[0],[0]
"We draw NPi complexity levels from θ, one for each solution in the set.",4.2 Permutations-based model,[0],[0]
"Let b denote this bag of levels (e.g., b = (1, 1, 2, 3, 4, 4, 4) assuming 4 complexity levels and 7 solutions for a particular problem).",4.2 Permutations-based model,[0],[0]
We also draw an inversion vector v from the GMM distribution which advantageously allows for small differences from the canonical order.,4.2 Permutations-based model,[0],[0]
"The z assignments are deterministically computed by ordering the elements of b according to the permutation defined by v.
Given the conditional independencies of our model, the posterior is proportional to:
L∏ m=1",4.2 Permutations-based model,[0],[0]
[P (φm|α)]× L−1∏ r=1,4.2 Permutations-based model,[0],[0]
"[P (ρr|µ0, v0r)]× P (ψ|δ0, δ1) × N∏ i=1",4.2 Permutations-based model,[0],[0]
[P (θi|β)× P (λi|ω)× P (vi|ρ)× P (bi|θi)],4.2 Permutations-based model,[0],[0]
×,4.2 Permutations-based model,[0],[0]
N∏ i=1,4.2 Permutations-based model,[0],[0]
NPi∏ j=1 |xij |∏,4.2 Permutations-based model,[0],[0]
k=1,4.2 Permutations-based model,[0],[0]
"[P (sijk|ψ)P (wijk|sijk, φzij , λi)]
where L is the number of complexity levels, N the total problems in the training corpus, NPi the size of solution set for problem Pi, and |xij | the number of words in solution xij .",4.2 Permutations-based model,[0],[0]
"A simplified posterior can be obtained by integrating out the ψ, φ, λ, and θ distributions which is proportional to:
N∏ i=1
L∏ m=1 Γ(Rmi",4.2 Permutations-based model,[0],[0]
"+βm)
Γ
( L∑
m=1",4.2 Permutations-based model,[0],[0]
"Rmi +βm
) × L−1∏ r=1 GMM0(ρr|µ0, v0r)
×",4.2 Permutations-based model,[0],[0]
N∏ i=1,4.2 Permutations-based model,[0],[0]
"L−1∏ r=1
GMMr(vir|ρr)× 1∏ u=0 Γ(Tu+δu)
",4.2 Permutations-based model,[0],[0]
"Γ
( 1∑
u=0 Tu+δu
)
× L∏
m=1
V∏ v=1 Γ(T 0m(v)+αv)
",4.2 Permutations-based model,[0],[0]
"Γ
( V∑
v=1 T 0m(v)+αv
) ×",4.2 Permutations-based model,[0],[0]
"N∏ i=1
V∏ v=1 Γ(T 1i (v)+ωv)
",4.2 Permutations-based model,[0],[0]
"Γ
( V∑
v=1 T 1i (v)+ωv
)
where theR and T counts are defined similarly as in the Expected Position model.
",4.2 Permutations-based model,[0],[0]
We use collapsed Gibbs sampling to compute samples from this posterior.,4.2 Permutations-based model,[0],[0]
"The sampling sequence
randomly initializes the hidden variables.",4.2 Permutations-based model,[0],[0]
"For a chosen solution set Si, the sampler draws NPi levels (bi), one at a time conditioned on the assignments to all other hidden variables of the model.",4.2 Permutations-based model,[0],[0]
Then the inversion vector vi is created by sampling each vij in turn.,4.2 Permutations-based model,[0],[0]
"At this point, the complexity level assignments zi can be done deterministically given bi and vi.",4.2 Permutations-based model,[0],[0]
Then the words in each solution set are sampled one at a time.,4.2 Permutations-based model,[0],[0]
"For the dispersion parameters, ρ, the normalization constant of the conjugate prior is not known.",4.2 Permutations-based model,[0],[0]
"We sample from the unnormalized GMM0 distribution using slice sampling.
",4.2 Permutations-based model,[0],[0]
Other hyperparameters of the model are tuned using development data.,4.2 Permutations-based model,[0],[0]
"The language model Dirichlet concentrations (α, ω) are chosen to encourage sparsity and β > 1 as in the position model.",4.2 Permutations-based model,[0],[0]
"We run the Gibbs sampler for 5,000 iterations; the dispersion parameters are resampled every 10 iterations.",4.2 Permutations-based model,[0],[0]
The last sample is used as a draw from the posterior.,4.2 Permutations-based model,[0],[0]
In this section we present examples of the complexity assignments created by our models.,4.3 Model Output,[0],[0]
Table 3 shows the output of the permutations-based model with 20 levels.,4.3 Model Output,[0],[0]
Each row contains the highest probability words in a single level (from the distribution φm).,4.3 Model Output,[0],[0]
"For the sake of brevity, we only show the two least and most complex levels.",4.3 Model Output,[0],[0]
"In general, we observe more specialized, technical terms in higher levels (e.g., restore, scanreg, registry) which one would expect to correlate with complex solutions.",4.3 Model Output,[0],[0]
"Also note that higher levels contain uncertainty denoting words (e.g., can, find, may) which again are indicative of increasing complexity.
",4.3 Model Output,[0],[0]
"Using these complexity vocabularies, our models
can compute the expected complexity for any solution text, x.",4.3 Model Output,[0],[0]
This value is given by [ ∑L m=1m ∗ p(m|x)].,4.3 Model Output,[0],[0]
"We estimate the second term, p(m|x), using a) the complexity level language models φm and b) a prior over levels given by the overall frequency of different levels on the training data.",4.3 Model Output,[0],[0]
Table 4 presents examples of solution texts from our training data and their expected complexity under the position-model.,4.3 Model Output,[0],[0]
We find that the model is able to distinguish intuitively complex solutions from simpler ones.,4.3 Model Output,[0],[0]
"Aside from measuring expected complexity in absolute terms, our models can also also order solutions in terms of relative complexity (see the evaluation in Section 5) and assign a complexity value to a problem as a whole.
",4.3 Model Output,[0],[0]
"As mentioned earlier, our models only observe the relative ordering of solutions to individual problems; the relative complexity of two solutions from different problems is not known.",4.3 Model Output,[0],[0]
"Nevertheless, the models are able to rate solutions on a global scale while accommodating problem-specific ordering sequences.",4.3 Model Output,[0],[0]
"Specifically, we can compute the expected complexity of the solution set for problem",4.3 Model Output,[0],[0]
"i, using the inferred distribution over levels θi: ∑L m=1m × θim.",4.3 Model Output,[0],[0]
Table 5 shows the complexity of different problems as predicted by the position model (with 10 levels).,4.3 Model Output,[0],[0]
"As can be seen, easy problems are associated with accessory components (e.g., mouse or keyboard), whereas complex problems are related to core hardware and operating system errors.",4.3 Model Output,[0],[0]
"In the previous section, we showed how our models can assign an expected complexity value to a solution text or an entire problem.",5 Evaluation Experiments,[0],[0]
"Now, we present evaluations based on model ability to order solutions according to relative complexity.",5 Evaluation Experiments,[0],[0]
We evaluated our models by presenting them with a randomly permuted set of solutions to a problem and examining the accuracy with which they reorder them from least to most complex.,5.1 Solution Ordering Task,[0],[0]
"At first instance, it would be relatively straightforward to search for the sequence of solutions which has high likelihood under the models.",5.1 Solution Ordering Task,[0],[0]
"Unfortunately, there are two problems with this approach.",5.1 Solution Ordering Task,[0],[0]
"Firstly, the likelihood under our models is intractable to compute, so we
would need to adopt a simpler and less precise approximation (such as the Hidden Markov Model discussed below).",5.1 Solution Ordering Task,[0],[0]
"Secondly, when the solution set size is large, we cannot enumerate all permutations and need to adopt an approximate search procedure.
",5.1 Solution Ordering Task,[0],[0]
We opted for a discriminative ranking approach instead which uses the generative models to compute a rich set of features.,5.1 Solution Ordering Task,[0],[0]
This choice allows us to simultaneously obtain features tapping on to different aspects learned by the models and to use well-defined objective functions.,5.1 Solution Ordering Task,[0],[0]
"Below, we briefly describe the features based on our generative models.",5.1 Solution Ordering Task,[0],[0]
"We also present additional features used to create baselines for system comparison.
",5.1 Solution Ordering Task,[0],[0]
Likelihood We created a Hidden Markov Model based on the sample from the posterior of our models (for a similar HMM approximation of a Bayesian model see Elsner et al. (2007)).,5.1 Solution Ordering Task,[0],[0]
"For our model, the HMM has L states, and each state sm corresponds to a complexity level dm.",5.1 Solution Ordering Task,[0],[0]
We used the complexity language models φm estimated from the posterior as the emission probability distribution for the corresponding states.,5.1 Solution Ordering Task,[0],[0]
The transition probabilities of the HMM were computed based on the complexity level assignments for the training solution sequences in our posterior sample.,5.1 Solution Ordering Task,[0],[0]
"The probability of transitioning to state sj from state si, p(sj |si), is the conditional probability p(dj |di) computed as c(di,dj)c(di) , where c(di, dj) is the number of times the complexity level dj is assigned to a solution immediately following a solution which was given complexity di.",5.1 Solution Ordering Task,[0],[0]
c(di) is the number of times complexity level di is assigned overall in the training corpus.,5.1 Solution Ordering Task,[0],[0]
"We perform Laplace smoothing to avoid zero probability transitions between states:
p(sj |si) = c(di, dj) + 1
c(di) +",5.1 Solution Ordering Task,[0],[0]
"L (4)
",5.1 Solution Ordering Task,[0],[0]
"This HMM formulation allows us to use efficient dynamic programming to compute the likelihood of a sequence of solutions.
",5.1 Solution Ordering Task,[0],[0]
"Given a solution set, we compute an ordering as follows.",5.1 Solution Ordering Task,[0],[0]
"We enumerate all orderings for sets with size less than 6, and select the sequence with the highest likelihood.",5.1 Solution Ordering Task,[0],[0]
"For larger sizes, we use a simulated annealing search procedure which swaps two adjacent solutions in each step.",5.1 Solution Ordering Task,[0],[0]
The temperature was set to 50 initially and gradually reduced to 1.,5.1 Solution Ordering Task,[0],[0]
"These
values were set using minimal tuning on the development data.",5.1 Solution Ordering Task,[0],[0]
"After estimating the most likely sequence for a solution set, we used the predicted rank of each solution as a feature in our discriminative model.
",5.1 Solution Ordering Task,[0],[0]
"Expected Complexity As mentioned earlier, we computed the expected complexity of a solution x as [ ∑L m=1m ∗ p(m|x)], where the second term was estimated using a complexity level specific language model φm and a uniform prior over levels on the test set.",5.1 Solution Ordering Task,[0],[0]
"As additional features, we used the solution’s perplexity under each φm, and under each of the technical topics λi, and also the most likely level for the text argmaxm p(m|x).",5.1 Solution Ordering Task,[0],[0]
"Finally, we included features for each word in the training data.",5.1 Solution Ordering Task,[0],[0]
"The feature value is the word’s expected level multiplied by the probability of the word in the solution text.
",5.1 Solution Ordering Task,[0],[0]
"Length We also investigated whether solution length is a predictor of complexity (e.g., simple solutions may vary in length and amount of detail from complex ones).",5.1 Solution Ordering Task,[0],[0]
"We devised three features based on the number of sentences (within a solution), words, and average sentence length.
",5.1 Solution Ordering Task,[0],[0]
Syntax/Semantics Another related class of features estimates solution complexity based on sentence structure and meaning.,5.1 Solution Ordering Task,[0],[0]
"We obtained eight syntactic features based on the number of nouns, verbs, adjectives and adverbs, prepositions, pronouns, wh-adverbs, modals, and punctuation.",5.1 Solution Ordering Task,[0],[0]
Other features compute the average and maximum depth of constituent parse trees.,5.1 Solution Ordering Task,[0],[0]
"The part-of-speech tags and parse trees were obtained using the Stanford CoreNLP toolkit (Manning et al., 2014).",5.1 Solution Ordering Task,[0],[0]
"In addition, we computed 10 semantic features using WordNet (Miller, 1995).",5.1 Solution Ordering Task,[0],[0]
"They are the average number of senses for each category (noun, verb, adjective/adverb), and the maximum number of senses for the same three classes.",5.1 Solution Ordering Task,[0],[0]
We also include the average and maximum lengths of the path to the root of the hypernym tree for nouns and verbs.,5.1 Solution Ordering Task,[0],[0]
"This class of features roughly approximates the indicators typically used in predicting text readability (Schwarm and Ostendorf, 2005; McNamara et al., 2014).",5.1 Solution Ordering Task,[0],[0]
We performed 10-fold cross-validation.,5.2 Experimental Setup,[0],[0]
"We trained the ranking model on 240 problem-solution sets; 30 sets were reserved for development and 30 for
testing (in each fold).",5.2 Experimental Setup,[0],[0]
The most frequent 20 words in each training set were filtered as stopwords.,5.2 Experimental Setup,[0],[0]
The development data was used to tune the parameters and hyperparameters of the models and the number of complexity levels.,5.2 Experimental Setup,[0],[0]
"We experimented with ranges [5– 20] and found that the best number of levels was 10 for the position model and 20 for the permutationbased model, respectively.",5.2 Experimental Setup,[0],[0]
"For the expected position model, positions were normalized before training.",5.2 Experimental Setup,[0],[0]
"Let solution xir denote the rth solution in the solution set for problem Pi, where 1 ≤ r ≤ NPi .",5.2 Experimental Setup,[0],[0]
"We normalize r to a value between 0 and 1 using a min-max method: r′ = r−1NPi−1
.",5.2 Experimental Setup,[0],[0]
Then the [0–1] range is divided into k bins.,5.2 Experimental Setup,[0],[0]
"The identity of the bin containing r′ is taken as the normalized position, r. We tuned k experimentally during development and found that k = 3 performed best.
",5.2 Experimental Setup,[0],[0]
For our ordering experiments we used Joachims’ (2006) SVMRank package for training and testing.,5.2 Experimental Setup,[0],[0]
"During training, the classifier learns to minimize the number of swapped pairs of solutions over the training data.",5.2 Experimental Setup,[0],[0]
We used a linear kernel and the regularization parameter was tuned using grid search on the development data of each fold.,5.2 Experimental Setup,[0],[0]
We evaluate how well the model’s output agrees with gold-standard ordering using Kendall’s τ .,5.2 Experimental Setup,[0],[0]
Table 6 summarizes our results (average Kendall’s τ across folds).,5.3 Results,[0],[0]
"We present the results of the discriminative ranker when using a single feature class based on likelihood and expected complexity (Position, Permutation), length, and syntactico-semantic features (SynSem), and their combinations (denoted via +).",5.3 Results,[0],[0]
We also report the performance of a baseline which computes a random permutation for each solution set (Random; results are averaged over five runs).,5.3 Results,[0],[0]
"We show results for all solution sets (All) and broken down into different set sizes (e.g., 2–3, 4–5).
",5.3 Results,[0],[0]
"As can be seen, the expected position model obtains an overall τ of 0.30 and the permutation model of 0.26.",5.3 Results,[0],[0]
These values lie in the range of human annotator agreement with the FAQ order (see Section 3.2).,5.3 Results,[0],[0]
"In addition, we find that the models perform consistently across solution set sizes, with even higher correlations on longer sequences where our methods are likely to be more useful.",5.3 Results,[0],[0]
Position and Permutation outperform Random rankings and a model based solely on Length features.,5.3 Results,[0],[0]
"The
SynSem features which measure the complexity of writing in the text are somewhat better but still inferior compared to Position and Permutation.",5.3 Results,[0],[0]
"Using a paired Wilcoxon signed-rank test, we compared the τ values obtained by the different models.",5.3 Results,[0],[0]
"The Position and Permutation performed significantly better (p < 0.05) compared to Random, Length and SynSem baselines.",5.3 Results,[0],[0]
"However, τ differences between Position and Permutation are not statistically significant.",5.3 Results,[0],[0]
"With regard to feature combinations, we observe that both models yield better performance when combined with Length or SynSem.",5.3 Results,[0],[0]
"The Position model improves with the addition of both Length and SynSem, whereas the Permutation model combines best with SynSem features.",5.3 Results,[0],[0]
"The Position+SynSem+Length model is significantly better than Permutation (p < 0.05) but not Permutation+SynSem+Length or Position alone (again under the Wilcoxon test).
",5.3 Results,[0],[0]
"These results suggest that the solution ordering task is challenging with several factors influencing how a solution is perceived: the words used and their meaning, the writing style of the solution, and the amount of detail present in it.",5.3 Results,[0],[0]
Our data comes from the FAQs produced by computer and operating system manufacturers and other well-managed websites.,5.3 Results,[0],[0]
"As a result, the text in the FAQ solutions is of high quality.",5.3 Results,[0],[0]
"However, the same is not true
for community-generated solution texts on discussion forums.",5.3 Results,[0],[0]
"In the latter case, we conjecture that the style of writing is likely to play a bigger role in how users perceive complexity.",5.3 Results,[0],[0]
"We thus expect that the benefit of adding Length and SynSem features will become stronger when we apply our models to texts from online forums.
",5.3 Results,[0],[0]
We also computed how accurately the models identify the least and most complex solutions.,5.3 Results,[0],[0]
For solution sets of size 5 and above (so that the task is non-trivial) we computed the number of times the rank one solution given by the models was also the easiest according to the FAQ gold-standard.,5.3 Results,[0],[0]
"Likewise, we also computed how often the models correctly predict the most complex solution.",5.3 Results,[0],[0]
"With Random rankings, the easiest and most difficult solutions are predicted correctly 15% of the time.",5.3 Results,[0],[0]
Getting both correct for a single problem happens only 3% of the time.,5.3 Results,[0],[0]
"The Position+Length model overall performs best, identifying the easiest and most difficult solution 36% of the time.",5.3 Results,[0],[0]
Both types of solution are identified correctly 18% of the time.,5.3 Results,[0],[0]
"Interestingly, the generative models are better at predicting the most difficult solution (35–37%) compared to the easiest one (28–36%).",5.3 Results,[0],[0]
"One reason for this could be that there are multiple easy solutions to try out but the most difficult one is probably more unique and so easier to identify.
",5.3 Results,[0],[0]
"Overall, we observe that the two generative models perform comparably, with Position having a slight lead over Permutation.",5.3 Results,[0],[0]
"A key difference be-
tween the models is that during training Permutation observes the full ordering of solutions while Position observes solutions coming from a few normalized position bins.",5.3 Results,[0],[0]
"Also note that in the Permutation model, multiple solutions with the same complexity level are grouped together in a solution set.",5.3 Results,[0],[0]
This property of the model is advantageous for ordering as solutions with similar complexity should be placed adjacent to each other.,5.3 Results,[0],[0]
"At the same time, if levels 1 and 2 are flipped in the permutation sampled from the GMM, then any solution with complexity level 1 will be ordered after the solution with complexity 2.",5.3 Results,[0],[0]
"The Position model on the other hand, contains no special facility for grouping solutions with the same complexity.",5.3 Results,[0],[0]
"In sum, Position can more flexibly assign complexity levels to individual solutions.",5.3 Results,[0],[0]
This work contains a first proposal to organize and navigate crowd-generated troubleshooting data according to the complexity of the troubleshooting action.,6 Conclusion,[0],[0]
"We showed that users perceive and agree on the complexity of alternative suggestions, and presented Bayesian generative models of the troubleshooting data which can sort solutions by complexity with a performance close to human agreement on the task.
",6 Conclusion,[0],[0]
Our results suggest that search and summarization tools for troubleshooting forum archives can be greatly improved by automatically predicting and using the complexity of the posted solutions.,6 Conclusion,[0],[0]
It should also be possible to build broad coverage automated troubleshooting systems by bootstrapping from conversations in discussion forums.,6 Conclusion,[0],[0]
"In the future, we plan to deploy our models in several tasks such as user authority prediction, expert intervention, and thread analysis.",6 Conclusion,[0],[0]
"Furthermore, we aim to specialize our models to include category-specific complexity levels and also explore options for personalizing rankings for individual users based on their knowledge of a topic and the history of their troubleshooting actions.",6 Conclusion,[0],[0]
We would like to thank the editor and the anonymous reviewers for their valuable feedback on an earlier draft of this paper.,Acknowledgements,[0],[0]
"We are also thankful to members of the Probabilistic Models of Language reading group at the University of Edinburgh for
their many suggestions and helpful discussions.",Acknowledgements,[0],[0]
The first author was supported by a Newton International Fellowship (NF120479) from the Royal Society and the British Academy.,Acknowledgements,[0],[0]
Online discussion forums and community question-answering websites provide one of the primary avenues for online users to share information.,abstractText,[0],[0]
"In this paper, we propose text mining techniques which aid users navigate troubleshooting-oriented data such as questions asked on forums and their suggested solutions.",abstractText,[0],[0]
"We introduce Bayesian generative models of the troubleshooting data and apply them to two interrelated tasks: (a) predicting the complexity of the solutions (e.g., plugging a keyboard in the computer is easier compared to installing a special driver) and (b) presenting them in a ranked order from least to most complex.",abstractText,[0],[0]
"Experimental results show that our models are on par with human performance on these tasks, while outperforming baselines based on solution length or readability.",abstractText,[0],[0]
Which Step Do I Take First? Troubleshooting with Bayesian Models,title,[0],[0]
"In application domains where the number of features exceeds the number of available samples, sparsity-inducing regularisers have a long history of success.",1. Introduction,[0],[0]
"Genomic prediction of complex phenotypes, biomedical imaging, astronomy or finance are a few examples.",1. Introduction,[0],[0]
"In particular the least squares with `1 regularisation, known as the LASSO (Tibshirani, 1996), has been extensively studied.",1. Introduction,[0],[0]
"It enjoys desirable statistical properties, since the number of samples required for exact support recovery of a sparse model scales as the logarithm of the number of features, under some
1MINES ParisTech, PSL Research University, CBIO-Centre for Computational Biology, 75006 Paris, France 2Institut Curie, PSL Research University, 75005 Paris, France 3INSERM, U900, 75005 Paris, France 4Ecole Normale Supérieure, Department of Mathematics and Applications, 75005 Paris, France.",1. Introduction,[0],[0]
"Correspondence to: Jean-Philippe Vert <jean-philippe.vert@mines-paristech.fr>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
assumptions (Wainwright, 2009).",1. Introduction,[0],[0]
"It also enjoys practical advantages, notably the interpretability of the learned models and the availability of fast solvers.
",1. Introduction,[0],[0]
"Indeed, a lot of research effort has been devoted to accelerating solvers for sparsity constrained problems in high dimension.",1. Introduction,[0],[0]
A central idea is to exploit the sparsity of the solution to develop algorithms that do not spend too much time on optimising coefficients that will end up being 0.,1. Introduction,[0],[0]
"For example, safe screening rules identify features which are guaranteed to be inactive at the optimum so that their corresponding coefficients can be safely zeroed and set aside from the pool of coefficients to update (El Ghaoui et al., 2012; Xiang et al., 2011; Xiang & Ramadge, 2012; Fercoq et al., 2015; Wang et al., 2013; Raj et al., 2016).",1. Introduction,[0],[0]
"Dynamic screening rules (Bonnefoy et al., 2015) such as the GAP safe rules (Fercoq et al., 2015) are particularly useful since more and more coefficients can be safely zeroed while the solver approaches the optimal solution.",1. Introduction,[0],[0]
"In spite of this, safe rules tend to be conservative, thereby limiting the potential speed-up.",1. Introduction,[0],[0]
"To remedy this drawback, new working set heuristics have been proposed.",1. Introduction,[0],[0]
"Working set algorithms enjoy great success in practice, as exemplified by the popular GLMNET package (Friedman et al., 2010).",1. Introduction,[0],[0]
"They iteratively solve subproblems, either problems restricted to a subset of features in the primal or to a subset of constraints in the dual, until convergence.",1. Introduction,[0],[0]
Working set methods allow to focus coefficient updates on a set of features which can be significantly smaller than that yielded by safe rules.,1. Introduction,[0],[0]
"However this comes at a cost, that of checking the optimality conditions for all features at each iteration.",1. Introduction,[0],[0]
"BLITZ (Johnson & Guestrin, 2015) is a recently proposed working set algorithm that has been shown to have state-of-the-art performance for `1 regularised problems.",1. Introduction,[0],[0]
"Interestingly, the choice of the working sets in BLITZ can be seen as an aggressive use of the GAP safe rules (as noted in Massias et al., 2017) where the size of the working set is chosen to maximise the progress towards convergence.",1. Introduction,[0],[0]
BLITZ can therefore be combined with the GAP safe rules (or the FLEX constraint elimination according to Johnson et al. terminology) at no cost.,1. Introduction,[0],[0]
A direct comparison between BLITZ and the GAP safe rules by Ndiaye et al. (2017) illustrates the effectiveness of the working set approach.,1. Introduction,[0],[0]
"Further developments have also focused on coordinate descent (CD) to avoid wasteful
coordinate updates, which represent most of the time spent by the solver (Fujiwara et al., 2016; Johnson & Guestrin, 2017).
",1. Introduction,[0],[0]
The problem of fitting sparse linear models with two-way interactions has also attracted attention during the past decade.,1. Introduction,[0],[0]
By two-way interactions we mean the entry-wise multiplication between two features; this is for example important in genomics to detect possible epistasis between genes.,1. Introduction,[0],[0]
"Surprisingly, very few of these works have links with the aforementioned literature.",1. Introduction,[0],[0]
"A majority of them focus on the design of sparsity-inducing penalties which enforce heredity assumptions and apply to moderate-dimensional settings (p < 1, 000) (Radchenko & James, 2010; Bien et al., 2013; Lim & Hastie, 2015; Haris et al., 2016).",1. Introduction,[0],[0]
Heredity assumptions state that an interaction can be included in the model only if one or both of its corresponding main effects are included.,1. Introduction,[0],[0]
"We note however that glinternet (Lim & Hastie, 2015) was applied to higher dimensional problems and in particular to a dataset with roughly p = 27, 000 main effects, although the size of the learned model is not specified and the running time for the experiment is not reported by the authors.",1. Introduction,[0],[0]
"Interestingly, glinternet uses an active set strategy.",1. Introduction,[0],[0]
Comparatively few works have been devoted to learning sparse regression models with interactions when the number of interactions is higher.,1. Introduction,[0],[0]
Most of them are heuristics which start by selecting main effects and then incorporate interactions generated under the heredity constraint in a possibly iterative fashion.,1. Introduction,[0],[0]
"The simplest form of such heuristics consists in fitting a sparse linear model with the main effects only, and then fitting a second sparse linear model on all previously selected main effects and their interactions.",1. Introduction,[0],[0]
This has been used in practice for example by Wu et al. (2009).,1. Introduction,[0],[0]
"Iterative refinements have been proposed where the LASSO is fit several times, and each time the set of candidate interactions considered is updated either by subsets, with the interactions between the K most relevant main effects selected at the previous fit (Bickel et al., 2010), or in a greedy fashion, where new interactions are included in the model as soon as a new main effect enters the LASSO path (Shah, 2016).",1. Introduction,[0],[0]
"In a similar vein, Hao & Zhang (2014) is based on a greedy model selection procedure instead of several LASSO fits.",1. Introduction,[0],[0]
"While these heuristics can deal with higher-dimensional problems than previous methods and enjoy some desirable statistical properties, they do not provide exact solutions and do not enjoy statistical properties as strong as those of the LASSO estimator.
",1. Introduction,[0],[0]
An interesting link between the literature on interactions and that of solver acceleration with sparsity inducing norms has been made recently by Nakagawa et al. (2016).,1. Introduction,[0],[0]
"In the case where variables are binary or with values in [0, 1], they propose an approach called Safe Pattern Pruning (SPP) which is able to provide the optimal solution of the LASSO with two-way interactions (or possibly higher-order interactions)
for fairly high-dimensional problems, with no heredity constraint.",1. Introduction,[0],[0]
"Typically, for a problem with 1,000 samples and 10,000 main effects where two-way interactions are considered, SPP can provide solutions for a grid of regularisation parameters within one or two hours on a laptop with one core.",1. Introduction,[0],[0]
SPP relies on the recently developed GAP safe screening rules.,1. Introduction,[0],[0]
"More precisely, the authors propose a safe pattern pruning criterion that can safely discard subsets of interactions from the model to speed up convergence.",1. Introduction,[0],[0]
The performance of SPP is however hindered by several factors.,1. Introduction,[0],[0]
One of them is that safe screening rules can be quite conservative even in the sequential setting.,1. Introduction,[0],[0]
"This property is inherited and amplified by the SPP criterion which can lead to heavy computations.
",1. Introduction,[0],[0]
"Inspired by SPP and the acceleration of solvers for sparsity constrained problems we propose a scalable algorithm, WHInter, to compute the optimal solution of `1-regularised linear problems with two-way interactions.",1. Introduction,[0],[0]
WHInter is a working set method that efficiently delineates working sets among all interactions and main effects thanks to two contributions.,1. Introduction,[0],[0]
"First, we introduce a cheap and effective bound to rule out subsets of interactions that are guaranteed to be outside of the working set.",1. Introduction,[0],[0]
"Second, the identification of the working set among the remaining features is cast as a variant of the Maximum Inner Product Search (MIPS) problem to alleviate the corresponding computational load.",1. Introduction,[0],[0]
We find that WHInter is up to two orders of magnitude faster than SPP.,1. Introduction,[0],[0]
"For example, a problem with roughly 700 samples and 100,000 main effects can be solved for a grid of regularisation parameters in half an hour on a laptop with one core compared to more than 30 hours with SPP.",1. Introduction,[0],[0]
This improvement in the scalability opens up new horizons in several application fields.,1. Introduction,[0],[0]
The rest of the paper is organised as follows.,1. Introduction,[0],[0]
"In Section 2, we present useful knowledge and notations used throughout the paper.",1. Introduction,[0],[0]
In Section 3 we describe in details our algorithm and our main contributions.,1. Introduction,[0],[0]
"In Section 4, we evaluate WHInter on simulated datasets and finally in Section 5, we report results on a toxicogenomics prediction task.",1. Introduction,[0],[0]
"For any integer d ∈ N, we note JdK = {1, . . .",2.1. Setting and notations,[0],[0]
", d} and 1d ∈ Rd the d-dimensional vector of 1’s.",2.1. Setting and notations,[0],[0]
"For any vector u = (u1, . . .",2.1. Setting and notations,[0],[0]
",ud) ∈ Rd, we note ‖u ‖1 = ∑d i=1",2.1. Setting and notations,[0],[0]
|ui,2.1. Setting and notations,[0],[0]
"|,
‖u ‖2 = (∑d i=1",2.1. Setting and notations,[0],[0]
"u 2 i )1/2 , supp(u) =",2.1. Setting and notations,[0],[0]
{i ∈ JdK : ui 6= 0} and ‖u ‖0 = | supp(u) |.,2.1. Setting and notations,[0],[0]
"For any two vectors u,v ∈ Rd, u v is the vector of entry-wise products, i.e., (u v)i := uivi for i = 1, . . .",2.1. Setting and notations,[0],[0]
", d.",2.1. Setting and notations,[0],[0]
"For any matrix M, we denote by Mi,j its (i, j)-th entry, Mj its j-th column and by mi its i-th row.",2.1. Setting and notations,[0],[0]
For any u ∈ Rd,2.1. Setting and notations,[0],[0]
"and I ⊂ JdK, uI = (ui)i∈I ,
and similarly, if M is a matrix with d columns, MI is the sub-matrix with | I | columns MI =",2.1. Setting and notations,[0],[0]
"(Mi)i∈I .
",2.1. Setting and notations,[0],[0]
"Throughout the text we consider a design matrix X ∈ {0, 1}n×p corresponding to n samples and p binary features, together with a response vector y ∈",2.1. Setting and notations,[0],[0]
Rn.,2.1. Setting and notations,[0],[0]
"We define an expanded design matrix Z ∈ {0, 1}n×D, with D = p(p + 1)/2, which contains all p features from X plus the p(p − 1)/2 interaction features.",2.1. Setting and notations,[0],[0]
"For clarity purposes, we define a symmetric indexing function τ :",2.1. Setting and notations,[0],[0]
"JpK2 7→ JDK that uniquely assigns to every main effect and interaction an index in the expanded matrix Z such that Zτ(j,k) = Zτ(k,j) := Xj Xk.",2.1. Setting and notations,[0],[0]
"In particular Zτ(i,i) = Xi Xi = Xi represents the ith main effect.",2.1. Setting and notations,[0],[0]
"Since X is a binary matrix, the interaction feature Xj Xk corresponds to a logical AND between features Xi and Xj .",2.1. Setting and notations,[0],[0]
"We organise the main effects and interactions in a simple tree as depicted in Figure 1 so as to reflect the property that ∀(j, k) ∈ JpK2 ,Zτ(j,k) ≤",2.1. Setting and notations,[0],[0]
"Xj and Zτ(j,k) ≤",2.1. Setting and notations,[0],[0]
Xk.,2.1. Setting and notations,[0],[0]
"In the sequel, the set composed of a main effect and its interactions with all other main effects will be referred to as a branch and for any j ∈ JpK, we note branch(j)",2.1. Setting and notations,[0],[0]
"= {τ(j, k) : k ∈ JpK}.
",2.1. Setting and notations,[0],[0]
"We consider the convex optimisation problem:
min (w,b)∈RD×R PZ,λ(w, b) , (1)
with
PZ,λ(w, b) =",2.1. Setting and notations,[0],[0]
"F (Zw + b1n) + λ ‖w‖1
= n∑ i=1",2.1. Setting and notations,[0],[0]
fi,2.1. Setting and notations,[0],[0]
"(ziw + b) + λ ‖w‖1 ,
where λ > 0 is a regularisation parameter and, for any i ∈ JnK, fi : R 7→",2.1. Setting and notations,[0],[0]
"[−∞,+∞] is a loss function parametrised by yi and assumed to be convex and differentiable.",2.1. Setting and notations,[0],[0]
Table 1 provides examples of classical loss functions in classification and regression.,2.1. Setting and notations,[0],[0]
"A dual formulation of (1) reads:
max θ∈∆Z,λ D(θ) := − n∑ i=1 f∗i (−θi) , (2)
where ∆Z,λ = { θ ∈",2.1. Setting and notations,[0],[0]
"Rn : ∣∣Z>θ∣∣ ≤ λ1D ,1>n θ = 0} , (3) and where f∗i is the Fenchel-Legendre transform of the loss fi, i.e., the function f∗i : R 7→",2.1. Setting and notations,[0],[0]
"[−∞,+∞] defined by f∗i (u) =",2.1. Setting and notations,[0],[0]
supv∈R uv − fi(v).,2.1. Setting and notations,[0],[0]
"For the derivation of the dual problem, we refer the reader to Johnson & Guestrin (2015, Appendix E).",2.1. Setting and notations,[0],[0]
The constraint 1>n,2.1. Setting and notations,[0],[0]
θ = 0 comes from the bias term b1n in the primal problem (1).,2.1. Setting and notations,[0],[0]
"We denote by (w∗, b∗) and θ∗ a set of primal and dual optimal solutions to
problems (1) and (2) respectively.",2.1. Setting and notations,[0],[0]
"Strong duality holds and therefore (w∗, b∗) and θ∗ satisfy Fermat’s rules (Ndiaye et al., 2017):
θ∗ = −∇F",2.1. Setting and notations,[0],[0]
"(Zw∗ + b∗1n) , (4)
and ∀i ∈ JDK , Z>i θ∗ ∈ { {−λ, λ} if w∗i 6= 0 ,",2.1. Setting and notations,[0],[0]
"[−λ, λ] if w∗i = 0 .
(5)",2.1. Setting and notations,[0],[0]
"A general strategy to solve (1) is to follow a working set approach, as summarised in Algorithm 1.",2.2. Basic working set algorithm,[0],[0]
"At each iteration, it solves (1) restricted to a small subset of featuresW called the working set.",2.2. Basic working set algorithm,[0],[0]
W is typically chosen as the set of features that violate the optimality condition (5) at the current iteration.,2.2. Basic working set algorithm,[0],[0]
"In the sequel, we will call such features violating features.",2.2. Basic working set algorithm,[0],[0]
"The algorithm converges when no violating feature remains, which occurs in a finite number of iterations as shown in Kowalski et al. (2011).",2.2. Basic working set algorithm,[0],[0]
"When the number of interaction features runs into the billions, Algorithm 1 is not tractable since the delineation of the working set (line 3 in Alg. 1) requires O(p2n) operations at each iteration.
",2.2. Basic working set algorithm,[0],[0]
"Algorithm 1 Working set algorithm
Input: Z ∈ {0, 1}n×D,y ∈ Rn, λ > 0",2.2. Basic working set algorithm,[0],[0]
"Output: w∗, b∗
1: Set θ ← −∇F (0n),W = ∅. 2: while true do 3: W ′",2.2. Basic working set algorithm,[0],[0]
"= { i ∈ JDK : ∣∣Z>i θ∣∣ ≥ λ} 4: if maxi∈W′
∣∣Z>i θ ∣∣ ≤ λ then Break elseW ←W ′",2.2. Basic working set algorithm,[0],[0]
"5: w∗W , b
∗ ← argmin wW ,b PZW ,λ(wW , b)
6: θ ← −∇F (ZWw∗W + b∗1n).",2.2. Basic working set algorithm,[0],[0]
7: end while,2.2. Basic working set algorithm,[0],[0]
WHInter is a working set algorithm that follows the general scheme of Algorithm 1 but implements an efficient strategy to delineate the working set among all main effects and interactions.,3.1. Overview,[0],[0]
It is described in Algorithm 2.,3.1. Overview,[0],[0]
The identification of the working set (line 3 in Algorithm 1) corresponds to lines 11-18 in Algorithm 2.,3.1. Overview,[0],[0]
"Instead of scanning through all features to build the working set, WHInter first identifies branches that are guaranteed to contain no violating feature.",3.1. Overview,[0],[0]
"These branches are identified via the evaluation of a branch bound η(Xj ,Θ ref j ,θ,m ref j ) (line 13), which is presented in Section 3.2 together with the parameters it takes as input.",3.1. Overview,[0],[0]
"The branch bound is cheap to evaluate since it solely depends on main effects and not on their numerous
Table 1 – Summary of useful functions for the LASSO and logistic regression: loss function fi, its derivative f ′i , its FenchelLegendre transform f∗i .
",3.1. Overview,[0],[0]
fi(u) f ′,3.1. Overview,[0],[0]
i(u) f ∗,3.1. Overview,[0],[0]
"i (u)
LASSO 12 (yi − u) 2
u− yi 12 (yi + u) 2 − 12y 2",3.1. Overview,[0],[0]
"i
Logistic regr. log(1 + exp(−yiu))",3.1. Overview,[0],[0]
− uyi log(− u yi ),3.1. Overview,[0],[0]
"+ (1 + uyi ) log(1 + u yi ) −yi1+exp(yiu)
",3.1. Overview,[0],[0]
interactions.,3.1. Overview,[0],[0]
"Moreover, it is designed to efficiently rule out branches thanks to the exploitation of the shared structure among features in a branch, as well as the correlation among dual variables for two sufficiently close points in the optimisation path.",3.1. Overview,[0],[0]
"In cases where a branch cannot be ruled out, features in the branch are considered one by one to build the working set, which is very computationally expensive.",3.1. Overview,[0],[0]
"In order to reduce this cost, we cast the problem as a variant of the Maximum Inner Product Search (MIPS) problem, which is described in Section 3.3.",3.1. Overview,[0],[0]
If no violating feature is identified then the algorithm has converged.,3.1. Overview,[0],[0]
"Otherwise, a new candidate solution is obtained by solving problem (1) restricted to the features in the working set (line 20), and the process is repeated until no violating feature remains.",3.1. Overview,[0],[0]
"While any solver can be used to solve the restricted problem, we implemented in WHInter a coordinate descent approach with safe pruning.",3.1. Overview,[0],[0]
"As WHInter iterates, it produces candidate solutions (w∗, b∗) and corresponding dual variables θ (lines 20 and 21 of Algorithm 2).",3.2. The Branch bound η,[0],[0]
"For two sufficiently close iterations, or for two problems with sufficiently close regularisation parameters, the candidate solutions are likely to be close to one another, as well as the corresponding dual variables provided that the function F does not vary too quickly.",3.2. The Branch bound η,[0],[0]
WHInter exploits this intuition to speed up the identification of the working set from an iteration to another or from one problem to another.,3.2. The Branch bound η,[0],[0]
The following results relate the criteria used to identify the working set for two distinct dual variables (line 3 of Algorithm 1).,3.2. The Branch bound η,[0],[0]
Lemma 3.1.,3.2. The Branch bound η,[0],[0]
"For any X ∈ {0, 1}n×p, v ∈ Rn+, θ1,θ2 ∈",3.2. The Branch bound η,[0],[0]
"Rn, j ∈ JpK, I ⊂ JpK and α ∈ R, the following holds:
max k∈I ∣∣θ>2 (v Xk) ∣∣ ≤ |α |max
k∈I
∣∣θ>1 (v Xk) ∣∣+ ζ(θ2",3.2. The Branch bound η,[0],[0]
"− αθ1,v) , (6)
where ∀(u,v) ∈",3.2. The Branch bound η,[0],[0]
"Rn × Rn+ ,
ζ(u,v) = max",3.2. The Branch bound η,[0],[0]
"( ∑ i:ui>0 uivi,− ∑ i:ui<0 uivi ) .
",3.2. The Branch bound η,[0],[0]
The proof of Lemma 3.1 is provided in Appendix A.,3.2. The Branch bound η,[0],[0]
"It is based on the decomposition θ2 = αθ1 + (θ2 − αθ1), and exploits the tree structure among features in a branch.",3.2. The Branch bound η,[0],[0]
"To exploit Lemma 3.1 in WHInter, we define for α ∈ R and for all (v,θ1,θ2,m) ∈",3.2. The Branch bound η,[0],[0]
Rn+ ×,3.2. The Branch bound η,[0],[0]
"Rn × Rn × R the function:
ηα (v,θ1,θ2,m) = |α |m+ ζ (θ2 − αθ1,v) , (7)
and we maintain an active setW ⊂",3.2. The Branch bound η,[0],[0]
"JDK, a matrix Θref ∈ Rn×p that contains reference dual variables Θrefj ∈",3.2. The Branch bound η,[0],[0]
"Rn for each branch j ∈ JpK, and the vector mref ∈",3.2. The Branch bound η,[0],[0]
"Rp defined by:
∀j ∈ JpK , mrefj = max k∈JpK:τ(j,k)/∈W ∣∣∣Z>τ(j,k)Θrefj ∣∣∣ .",3.2. The Branch bound η,[0],[0]
"(8) We now state our main theorem which allows to identify branches that are guaranteed to not contain any violating feature (line 13 of Algorithm 2):
Theorem 3.1 (Branch pruning).",3.2. The Branch bound η,[0],[0]
"For any Θref ∈ Rn×p, W ⊂ JpK, j ∈ JpK, let mrefj ∈ R+ be given by (8).",3.2. The Branch bound η,[0],[0]
"Then for any θ ∈ Rn, α ∈ R and λ > 0, if
ηα ( Xj ,Θ ref j ,θ,m ref j )",3.2. The Branch bound η,[0],[0]
"< λ , (9)
then any feature from branch j that belongs to the working set { i ∈ JDK : ∣∣Z>i θ∣∣ ≥ λ} is already inW .",3.2. The Branch bound η,[0],[0]
"This holds in particular if
ηmin := min α∈R ηα
( Xj ,Θ ref j ,θ,m ref j )",3.2. The Branch bound η,[0],[0]
< λ .,3.2. The Branch bound η,[0],[0]
"(10)
Algorithm 2 WHInter
Input: X ∈ {0, 1}n×p, y ∈ Rn, λ1 > · · ·",3.2. The Branch bound η,[0],[0]
> λT .,3.2. The Branch bound η,[0],[0]
"Output: (W,w∗W , b∗)t for each λt
# Initialisation 1: θ ← −∇F (0n) 2: for j in JpK do 3:",3.2. The Branch bound η,[0],[0]
"Θrefj ← θ 4: end for 5: W,mref ← update W(X,θ, JpK , λ1, ∅) 6: for t = 1 to T do # Pre-Solve 7: w∗W , b
∗ ← argmin wW ,b PZW ,λt(wW , b)
8: θ ← −∇F (ZWw∗W + b∗1n).",3.2. The Branch bound η,[0],[0]
"9: W,mref ← clean W(W, λt,θ,Θref ,mref )
10: while true do # Branch pruning 11: V ← ∅ 12: for j in JpK do 13: if η(Xj ,Θrefj ,θ,m ref j ) ≥",3.2. The Branch bound η,[0],[0]
λt then 14: V ← V ∪ {j} 15:,3.2. The Branch bound η,[0],[0]
"Θrefj ← θ 16: end if 17: end for # Identify the working set 18: W ′,mrefV ← update W(X,θ,V, λt,W) 19: if maxi∈W′
∣∣Z>i θ ∣∣ ≤ λ then 20:",3.2. The Branch bound η,[0],[0]
Break 21: else 22:,3.2. The Branch bound η,[0],[0]
W ←W ′,3.2. The Branch bound η,[0],[0]
"23: end if # Solve subproblem 24: w∗W , b
∗ ← argmin wW ,b PZW ,λt(wW , b)",3.2. The Branch bound η,[0],[0]
"26: W,mref ← clean W(W, λt,θ,Θref ,mref ) 27: end while 28: (W,w∗W , b∗)k ← (W,w∗W , b∗) 29: end for
30: function clean W(W, λ,θ,Θref ,mref ) 31: for i inW do 32: if ∣∣Z>i θ∣∣ < λ then 33: Remove {i} fromW 34: for b in branch(i) do 35: if mrefb <
∣∣∣Z>i Θrefb ∣∣∣ then 36: mrefb ←
∣∣∣Z>",25: θ ← −∇F (ZWw∗W + b∗1n).,[0],[0]
"i Θrefb ∣∣∣ 37: returnW ,mref
Proof.",25: θ ← −∇F (ZWw∗W + b∗1n).,[0],[0]
"Take I = {k ∈ JpK : τ(j, k) /∈",25: θ ← −∇F (ZWw∗W + b∗1n).,[0],[0]
"W}, v = Xj , θ1 = Θrefj and θ2 = θ in Lemma 3.1.",25: θ ← −∇F (ZWw∗W + b∗1n).,[0],[0]
"Then if (9) holds, we deduce from (6) that
max k∈JpK:τ(j,k)/∈W ∣∣∣Z>τ(j,k)θ ∣∣∣ < λ .",25: θ ← −∇F (ZWw∗W + b∗1n).,[0],[0]
This shows that there is no feature i in branch j such that∣∣Z>i θ∣∣ ≥ λ,25: θ ← −∇F (ZWw∗W + b∗1n).,[0],[0]
and i is not already inW .,25: θ ← −∇F (ZWw∗W + b∗1n).,[0],[0]
"The fact that for fixed arguments, the function α→ ηα has a minimum α∗ ∈ R is shown in Appendix B, along with an algorithm to compute it in O (‖Xj ‖0 ln ‖Xj ‖0) operations .",25: θ ← −∇F (ZWw∗W + b∗1n).,[0],[0]
"Since the statement is true for any α, it is a fortiori true for α∗.
Theorem 3.1 provides criteria (9) and (10) that can be computed for each branch j, and which if satisfied allow to skip the search for violating variables in the branch.",25: θ ← −∇F (ZWw∗W + b∗1n).,[0],[0]
"Importantly, the features that are already in the working set W are not taken into account to compute the criterion for a given branch.",25: θ ← −∇F (ZWw∗W + b∗1n).,[0],[0]
This subtlety allows to rule out branches even if they already contain features that were previously incorporated in the working set.,25: θ ← −∇F (ZWw∗W + b∗1n).,[0],[0]
"Note that the reference dual variable for branch j, i.e, Θrefj , is kept unchanged as long as branch j is pruned, and is otherwise updated to the latest dual variable (line 15 of Algorithm 2).",25: θ ← −∇F (ZWw∗W + b∗1n).,[0],[0]
"As mrefj depends on the reference dual variable instead of the current one, it is solely reevaluated each time the reference residual is updated (line 18 of Algorithm 2) or when a feature from branch j leaves the working set (line 22 of Algorithm 2) .
",25: θ ← −∇F (ZWw∗W + b∗1n).,[0],[0]
"Criterion (10) is the most stringent one, and therefore the most efficient one to prune branches, but it takes O (‖Xj ‖0 ln ‖Xj ‖0) operations to compute.",25: θ ← −∇F (ZWw∗W + b∗1n).,[0],[0]
"In order to balance computational complexity of the bound with its efficacy to prune branches, criterion (9) can be used as an alternative for a specific value of α.",25: θ ← −∇F (ZWw∗W + b∗1n).,[0],[0]
"One simple choice is to just take α = 1, which leads to the criterion
η1 ( Xj ,Θ ref j ,θ,m ref j ) =",25: θ ← −∇F (ZWw∗W + b∗1n).,[0],[0]
mrefj +ζ,25: θ ← −∇F (ZWw∗W + b∗1n).,[0],[0]
"( θ −Θrefj ,Xj ) < λ .",25: θ ← −∇F (ZWw∗W + b∗1n).,[0],[0]
"(11) Alternatively, a simple heuristic to expect a more efficient pruning is to choose an α that minimises ‖",25: θ ← −∇F (ZWw∗W + b∗1n).,[0],[0]
( θ − αΘrefj ),25: θ ← −∇F (ZWw∗W + b∗1n).,[0],[0]
"Xj ‖2, i.e,
α`2 = θ> ( Θrefj Xj ) ‖Θrefj",25: θ ← −∇F (ZWw∗W + b∗1n).,[0],[0]
Xj ‖22 .,25: θ ← −∇F (ZWw∗W + b∗1n).,[0],[0]
"(12)
ηα`2 is expected to be more effective than η1 since it is reasonable to expect that ζ",25: θ ← −∇F (ZWw∗W + b∗1n).,[0],[0]
"( θ − α`2Θ ref j ,Xj ) is smaller
than ζ (",25: θ ← −∇F (ZWw∗W + b∗1n).,[0],[0]
"θ −Θrefj ,Xj ) .",25: θ ← −∇F (ZWw∗W + b∗1n).,[0],[0]
"Overall, computing α = α`2 as in (12) is an O(‖Xj ‖0) operation.",25: θ ← −∇F (ZWw∗W + b∗1n).,[0],[0]
Since computing ζ(θ,25: θ ← −∇F (ZWw∗W + b∗1n).,[0],[0]
"− αΘrefj ,Xj) for a fixed α is also a O(‖Xj ‖0) computation, the total cost of identifying branch j as violated is O(‖Xj ‖0) for criterion (9) with α = 1 or α = α`2 ,
compared to O (‖Xj ‖0",25: θ ← −∇F (ZWw∗W + b∗1n).,[0],[0]
ln ‖Xj ‖0) for criterion (10).,25: θ ← −∇F (ZWw∗W + b∗1n).,[0],[0]
"In Algorithm 2, the notation η refers to a user-defined function among η1, ηα`2 or ηmin.",25: θ ← −∇F (ZWw∗W + b∗1n).,[0],[0]
"When some branches V ⊂ JpK cannot be pruned, the simultaneous updates of the working setW and of mrefV requires scanning through all features in the branches V (lines 5 and 18 in Algorithm 2).",3.3. Updating the working set,[0],[0]
In what follows we discuss strategies to make these updates efficient.,3.3. Updating the working set,[0],[0]
"For that purpose, let us first notice that:
∀j, k ∈ JpK , ∣∣∣Z>τ(j,k)θ∣∣∣ = ∣∣∣(Xj Xk)>",3.3. Updating the working set,[0],[0]
"θ∣∣∣
= ∣∣∣(Xj θ)>Xk∣∣∣
= ∣∣Q>j Xk∣∣ ,
where for any j ∈ JpK ,Qj = Xj θ.",3.3. Updating the working set,[0],[0]
This allows us to write the updates ofW andmrefV as:W ′,3.3. Updating the working set,[0],[0]
"=W ∪ { τ(j, k) : j ∈ V, k ∈ JpK , ∣∣Q>j Xk∣∣ ≥ λ} , mrefj = max
k: |Q>j Xk|<λ ∣∣Q>j Xk∣∣ , ∀j ∈ V .
(13)",3.3. Updating the working set,[0],[0]
This highlights the fact that the updates of the working set W and of mrefV can be cast as particular variants of the Maximum Inner Product Search (MIPS) problem.,3.3. Updating the working set,[0],[0]
MIPS aims at finding a vector in a database of probes which maximises the inner product with a given query vector.,3.3. Updating the working set,[0],[0]
"If we consider X as a set of probes, andQj as a query, then (13) is a variant of MIPS where (i) the set of probe vectors satisfies some constraints and is not known upfront and (ii) the problem is a maximum absolute inner product search.",3.3. Updating the working set,[0],[0]
"The update of W involves what is sometimes referred to as above-λ-MIPS problems where again, maximum absolute inner products are considered.",3.3. Updating the working set,[0],[0]
The interest of casting these updates as variants of MIPS problems is to exploit the ideas developed in the literature for solving these problems efficiently.,3.3. Updating the working set,[0],[0]
Teflioudi & Gemulla (2016) and Fontoura et al. (2011) give good overviews of MIPS solvers developed for recommender systems and information retrieval applications respectively.,3.3. Updating the working set,[0],[0]
"In both cases, the proposed methods rely on two main ideas: (i) adequate indexing techniques or data structures and (ii) pruning criteria which allow to not compute all inner products entirely.",3.3. Updating the working set,[0],[0]
"Since none of these methods can directly be applied to problem (13) because of its specificities, we propose an appropriate algorithm based on a simple inverted index approach, which we will refer to as IL (standing for Inverted Lists), and which exploits the sparsity of the problem.",3.3. Updating the working set,[0],[0]
Another option would be to leverage pruning techniques.,3.3. Updating the working set,[0],[0]
"We detail such an attempt in Appendix C. However, since our preliminary results with the pruning technique were not conclusive compared to IL
on the simulated and real data, we only focus on the inverted index approach below.",3.3. Updating the working set,[0],[0]
"IL is detailed in Algorithm 3.
",3.3. Updating the working set,[0],[0]
"Algorithm 3 update W
Input: X ∈ {0, 1}n×p, θ ∈ Rn, V ⊂ JpK , λ ∈ R, W ⊂ JDK Output: W, mref 1: for j ∈ V do 2: mrefj = 0 3: Set ak = 0 for all k ∈ JpK 4: for each i in supp(Xj) do 5: for each k in supp(xi) do 6: ak = ak + θi 7: end for 8: end for 9: for each k s.t. ak 6= 0",3.3. Updating the working set,[0],[0]
do 10: if mrefj < |ak,3.3. Updating the working set,[0],[0]
| < λ then set m ref j = |ak,3.3. Updating the working set,[0],[0]
"| 11: if |ak | ≥ λ and τ(j, k) /∈",3.3. Updating the working set,[0],[0]
"W then add τ(j, k) to W 12: end for 13: end for 14: returnW,mref
The inverted lists consist of n lists, one for each dimension, where each list supp(xi) records the indices of the features in X which have a non-zero element for the ith dimension.",3.3. Updating the working set,[0],[0]
"These inverted lists can be computed once for all when WHInter starts and be reused for all MIPS problems, and therefore building the inverted lists requires a negligible additional computational cost.",3.3. Updating the working set,[0],[0]
"Algorithm (3) computes inner product following a term-at-a-time (TAAT) scheme (Fontoura et al., 2011), i.e, the inner products are accumulated simultaneously across probes and the contribution of the ith dimension to the inner products is entirely processed before moving to the next one.",3.3. Updating the working set,[0],[0]
We first test the performances of WHInter on synthetic LASSO datasets.,4. Simulation study,[0],[0]
"We assess the performances of the different branch pruning bounds presented in 3.2, i.e, ηmin, η1 and ηα`2 , and further compare WHInter to a working set method that uses the bound ζ(θ,Xj) instead of ηα, but is otherwise equivalent to WHInter.",4. Simulation study,[0],[0]
We refer to this method as ζ + IL.,4. Simulation study,[0],[0]
It is expected to prune less branches than WHInter but does not require to maintain mref .,4. Simulation study,[0],[0]
"We also compare WHInter to SPP (Nakagawa et al., 2016) and BLITZ (Johnson & Guestrin, 2015).",4. Simulation study,[0],[0]
"In our experiments, we use a slightly modified, more efficient version of the code provided by the authors of SPP (cf Appendix D).",4. Simulation study,[0],[0]
"As for BLITZ, since the method is not tailored for interaction problems, we first compute the matrix Z which is fed as input to BLITZ.",4. Simulation study,[0],[0]
"For this reason we could not solve problems when p is too large (e.g.,
p = 10, 000 in the simulations) since, even in sparse format, storing Z requires too much memory.",4. Simulation study,[0],[0]
"Importantly, the performances reported for BLITZ do not include the time required to compute Z fromX , which clearly advantages BLITZ compared to the other methods.
",4. Simulation study,[0],[0]
"We simulate five datasets X ∈ {0, 1}n×p with varying number of features and samples: three datasets with p = 1, 000 fixed and n ∈ {300, 1, 000, 10, 000}, and two more with n = 1, 000 fixed and p ∈ {3, 000, 10, 000}.",4. Simulation study,[0],[0]
The features are drawn from a Bernoulli distribution with parameter q ∈,4. Simulation study,[0],[0]
"[0.1, 0.5] itself drawn from a uniform distribution U[0.1,0.5].",4. Simulation study,[0],[0]
"We then randomly pick a set S of 100 features among the main effects and interactions and compute the response as y = ZSw∗S where w ∗ S ∼ N (0|S|, I|S|).",4. Simulation study,[0],[0]
"In all experiments, the LASSO is solved for a sequence (λt)t∈JT K, T = 100, logarithmically spaced between λmax and max(0.01λmax, λ′) where λmax is the largest value of λ for which at least one feature is selected, and λ′ is the first λi for which 150 features or more are selected in the model.",4. Simulation study,[0],[0]
"For all methods, the time to compute λmax is included in the total time required to solve the regularisation path.",4. Simulation study,[0],[0]
"In WHInter, λmax can easily be deduced from the initialisation of mref since λmax = maxj∈JpKm ref j .",4. Simulation study,[0],[0]
All algorithms are implemented in C++ and compiled with the -O3 optimisation flag.,4. Simulation study,[0],[0]
"The experiments are run on a 64-bit machine with Intel Core i7 Processor 2.5 GHz, 16GB of memory and 6MB of cache.
",4. Simulation study,[0],[0]
Results are shown in Figure 2.,4. Simulation study,[0],[0]
"For n = 1, 000",4. Simulation study,[0],[0]
"(Figure 2a), LASSO solutions are computed for 42, 32 and 28 values of λ for p = 1, 000, p = 3, 000 and p = 10, 000 respectively.",4. Simulation study,[0],[0]
In these cases smaller values of λ result in model sizes exceeding 150 features.,4. Simulation study,[0],[0]
"For the remaining settings where p = 1, 000 and n = 300 or n = 10, 000 (Figure 2b),
LASSO solutions are computed for 34 and all 100 values of λ between λmax and 0.01λmax, respectively.",4. Simulation study,[0],[0]
"All methods returned the exact same support for all values of λ.
",4. Simulation study,[0],[0]
"In all settings, WHInter is the fastest method.",4. Simulation study,[0],[0]
Its better performance compared to ζ+IL highlights the benefit of using reference dual variables even if it implies to maintainmref .,4. Simulation study,[0],[0]
"The results also show the importance of α, since WHInter with η`2 is always better (×1.2 to ×1.8) than WHInter with η1 for example.",4. Simulation study,[0],[0]
Figure 2c confirms that the choice of α has an impact on the pruning efficiency and consequently on the performance.,4. Simulation study,[0],[0]
"It shows, however, that on this experiment ηmin does not allow to prune many more branches than η`2 .",4. Simulation study,[0],[0]
"This explains why η`2 tends to outperform ηmin, notably for large n, since the higher computational complexity of ηmin does not sufficiently enhance the pruning.",4. Simulation study,[0],[0]
"We also notice that SPP is the slowest algorithm, and in particular ζ + IL is ×17 faster than SPP on average.",4. Simulation study,[0],[0]
This speed-up is mostly explained by the fact that ζ + IL relies on inverted lists to update the working set while SPP identifies the safe set naively.,4. Simulation study,[0],[0]
"Overall, WHInter offers a signifiant speed-up of two orders of magnitude or more compared to its safe screening counterpart.",4. Simulation study,[0],[0]
"We now illustrate the performance of the different algorithms on a real-world problem, where we want to predict the cytotoxic response of 884 lymphoblastoid cell lines split into a train (n = 620) and a test (n = 264) set, and characterized by about 1.2×106 single nucleotide polymorphisms (SNP) that represent their genotypes.",5. Results on real world data,[0],[0]
"The data was released as part of the Dialogue on Reverse Engineering Assessment and Methods 8 (DREAM 8) toxicogenetics challenge (Ed-
uati et al., 2015).",5. Results on real world data,[0],[0]
We encode the SNP data as a binary matrix were 1 stand for the presence of a minor allele on one or both copies of the chromosomes.,5. Results on real world data,[0],[0]
As preprocessing we removed SNP with less than 5% of 1’s and corrected the data for population structure as in Price et al. (2006).,5. Results on real world data,[0],[0]
"To focus on problems of increasing scales, we first considered the SNPs of the smallest chromosome only (chr. 22), then of the largest only (chr. 1) and finally of all chromosomes together.",5. Results on real world data,[0],[0]
"This leads to train matrices with n = 620 and p = 18, 168 SNPs for chromosome 22, p = 89, 027 SNPs for chromosome 1 and p = 1, 166, 836 SNPs for the whole genome.",5. Results on real world data,[0],[0]
"We consider a sequence of regularisation parameter λ logarithmically spaced between λmax and 0.01λmax, and by default stop computations as soon as 150 features or more are selected.",5. Results on real world data,[0],[0]
"This occurs after the 12th, the 11th and the 9th value of λ for chromosome 22, chromosome 1 and all chromosomes respectively.",5. Results on real world data,[0],[0]
The time required to compute the regularisation paths are shown in Fig. 4.,5. Results on real world data,[0],[0]
The relative performances of the methods are the same as for the simulations.,5. Results on real world data,[0],[0]
ηα`2 provides a ×1.4 (resp. ×1.8) speed up compared to using η1 for chromosome 22 (resp.,5. Results on real world data,[0],[0]
chr. 1).,5. Results on real world data,[0],[0]
"and compared to SPP, there is a ×81 (resp. ×73)",5. Results on real world data,[0],[0]
speed up for chromosome 22 (resp chr. 1).,5. Results on real world data,[0],[0]
"In the case of the whole genome, we only ran WHInter with ηα`2 which takes two days and a half.",5. Results on real world data,[0],[0]
"While this can seem a lot, we recall that this corresponds to a problem with roughly 680 billion features.",5. Results on real world data,[0],[0]
"We did not run other methods on the whole genome since most of them are expected to take too long.
",5. Results on real world data,[0],[0]
"Out of curiosity, we also obtained preliminary results concerning the predictive performance of WHInter compared to a LASSO with no interactions on such high-dimensional problems.",5. Results on real world data,[0],[0]
"The results, presented in Figure 3 , suggest that interactions are relevant predictors for this data.",5. Results on real world data,[0],[0]
"For the chromosomes 1 and 22 taken independently, the predictive accuracy of WHInter is better than that of the simple LASSO for almost every value of λ.",5. Results on real world data,[0],[0]
"By contrast, for the whole genome, the LASSO clearly performs better, which may underline statistical issues due to the huge number of variables in this case (Donoho & Tanner, 2009).",5. Results on real world data,[0],[0]
"We presented WHInter, a working set algorithm designed to solve large scale `1-penalised linear problems with interaction terms.",6. Discussion,[0],[0]
"WHInter implements a new branch pruning bound to efficiently delineate the working set among the many possible interaction variables, and a variant of MIPS solver that provides a further speed up.",6. Discussion,[0],[0]
We showed that WHInter is up to two orders of magnitudes faster than competing approaches.,6. Discussion,[0],[0]
"While we presented WHInter for binary data, it could also be used for data rescaled in [0, 1], provided that an appropriate solver is picked for the MIPS problems.",6. Discussion,[0],[0]
"As for future work, one could exploit the recent works on approximate MIPS (Shrivastava & Li, 2014; Teflioudi & Gemulla, 2016) to obtain an additional speed up for the computationally intensive updates, and possibly rely on recent post selection-inference (Suzumura et al., 2017) frameworks to characterise the approximate solution obtained.",6. Discussion,[0],[0]
We thank our anonymous reviewers for their useful comments as well as Nino Shervashidze for thoughtful discussions.,Acknowledgements,[0],[0]
Learning sparse linear models with two-way interactions is desirable in many application domains such as genomics.,abstractText,[0],[0]
"`1-regularised linear models are popular to estimate sparse models, yet standard implementations fail to address specifically the quadratic explosion of candidate two-way interactions in high dimensions, and typically do not scale to genetic data with hundreds of thousands of features.",abstractText,[0],[0]
"Here we present WHInter, a working set algorithm to solve large `1-regularised problems with two-way interactions for binary design matrices.",abstractText,[0],[0]
"The novelty of WHInter stems from a new bound to efficiently identify working sets while avoiding to scan all features, and on fast computations inspired from solutions to the maximum inner product search problem.",abstractText,[0],[0]
We apply WHInter to simulated and real genetic data and show that it is more scalable and two orders of magnitude faster than the state of the art.,abstractText,[0],[0]
WHInter: A Working set algorithm for High-dimensional sparse second order Interaction models,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2230–2235, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics
We have constructed a new “Who-did-What” dataset of over 200,000 fill-in-the-gap (cloze) multiple choice reading comprehension problems constructed from the LDC English Gigaword newswire corpus. The WDW dataset has a variety of novel features. First, in contrast with the CNN and Daily Mail datasets (Hermann et al., 2015) we avoid using article summaries for question formation. Instead, each problem is formed from two independent articles — an article given as the passage to be read and a separate article on the same events used to form the question. Second, we avoid anonymization — each choice is a person named entity. Third, the problems have been filtered to remove a fraction that are easily solved by simple baselines, while remaining 84% solvable by humans. We report performance benchmarks of standard systems and propose the WDW dataset as a challenge task for the community.1",text,[0],[0]
"Researchers distinguish the problem of general knowledge question answering from that of reading comprehension (Hermann et al., 2015; Hill et al., 2016).",1 Introduction,[0],[0]
Reading comprehension is more difficult than knowledge-based or IR-based question answering in two ways.,1 Introduction,[0],[0]
"First, reading comprehension systems must infer answers from a given unstructured passage rather than structured knowledge sources such as Freebase (Bollacker et al., 2008)
1Available at tticnlp.github.io/who did what
or the Google Knowledge Graph (Singhal, 2012).",1 Introduction,[0],[0]
"Second, machine comprehension systems cannot exploit the large level of redundancy present on the web to find statements that provide a strong syntactic match to the question (Yang et al., 2015).",1 Introduction,[0],[0]
"In contrast, a machine comprehension system must use the single phrasing in the given passage, which may be a poor syntactic match to the question.
",1 Introduction,[0],[0]
"In this paper, we describe the construction of a new reading comprehension dataset that we refer to as “Who-did-What”.",1 Introduction,[0],[0]
Two typical examples are shown in Table 1.2 The process of forming a problem starts with the selection of a question article from the English Gigaword corpus.,1 Introduction,[0],[0]
The question is formed by deleting a person named entity from the first sentence of the question article.,1 Introduction,[0],[0]
"An information retrieval system is then used to select a passage with high overlap with the first sentence of the question article, and an answer choice list is generated from the person named entities in the passage.
",1 Introduction,[0],[0]
"Our dataset differs from the CNN and Daily Mail comprehension tasks (Hermann et al., 2015) in that it forms questions from two distinct articles rather than summary points.",1 Introduction,[0],[0]
This allows problems to be derived from document collections that do not contain manually-written summaries.,1 Introduction,[0],[0]
"This also reduces the syntactic similarity between the question and the relevant sentences in the passage, increasing the need for deeper semantic analysis.
",1 Introduction,[0],[0]
"To make the dataset more challenging we selectively remove problems so as to suppress four simple
2The passages here only show certain salient portions of the passage.",1 Introduction,[0],[0]
"In the actual dataset, the entire article is given.",1 Introduction,[0],[0]
"The correct answers are (3) and (2).
",1 Introduction,[0],[0]
"2230
baselines — selecting the most mentioned person, the first mentioned person, and two language model baselines.",1 Introduction,[0],[0]
"This is also intended to produce problems requiring deeper semantic analysis.
",1 Introduction,[0],[0]
The resulting dataset yields a larger gap between human and machine performance than existing ones.,1 Introduction,[0],[0]
"Humans can answer questions in our dataset with an 84% success rate compared to the estimates of 75% for CNN (Chen et al., 2016) and 82% for the CBT named entities task (Hill et al., 2016).",1 Introduction,[0],[0]
"In spite of this higher level of human performance, various existing readers perform significantly worse on our dataset than they do on the CNN dataset.",1 Introduction,[0],[0]
"For example, the Attentive Reader (Hermann et al., 2015) achieves 63% on CNN but only 55% on Who-didWhat and the Attention Sum Reader (Kadlec et al., 2016) achieves 70% on CNN but only 59% on Whodid-What.
",1 Introduction,[0],[0]
"In summary, we believe that our Who-did-What dataset is more challenging, and requires deeper semantic analysis, than existing datasets.",1 Introduction,[0],[0]
Our Who-did-What dataset is related to several recently developed datasets for machine comprehension.,2 Related Work,[0],[0]
"The MCTest dataset (Richardson et al., 2013) consists of 660 fictional stories with 4 multiple choice questions each.",2 Related Work,[0],[0]
"This dataset is too small
to train systems for the general problem of reading comprehension.
",2 Related Work,[0],[0]
"The bAbI synthetic question answering dataset (Weston et al., 2016) contains passages describing a series of actions in a simulation followed by a question.",2 Related Work,[0],[0]
"For this synthetic data a logical algorithm can be written to solve the problems exactly (and, in fact, is used to generate ground truth answers).
",2 Related Work,[0],[0]
"The Children’s Book Test (CBT) dataset, created by Hill et al. (2016), contains 113,719 cloze-style named entity problems.",2 Related Work,[0],[0]
"Each problem consists of 20 consecutive sentences from a children’s story, a 21st sentence in which a word has been deleted, and a list of ten choices for the deleted word.",2 Related Work,[0],[0]
The CBT dataset tests story completion rather than reading comprehension.,2 Related Work,[0],[0]
The next event in a story is often not determined — surprises arise.,2 Related Work,[0],[0]
This may explain why human performance is lower for CBT than for our dataset — 82% for CBT vs. 84% for Who-did-What.,2 Related Work,[0],[0]
The 16% error rate for humans on Who-did-What seems to be largely due to noise in problem formation introduced by errors in named entity recognition and parsing.,2 Related Work,[0],[0]
Reducing this noise in future versions of the dataset should significantly improve human performance.,2 Related Work,[0],[0]
Another difference compared to CBT is that Who-did-What has shorter choice lists on average.,2 Related Work,[0],[0]
Random guessing achieves only 10% on CBT but 32% on Who-did-What.,2 Related Work,[0],[0]
"The reduction
in the number of choices seems likely to be responsible for the higher performance of an LSTM system on Who-did-What – contextual LSTMs (the attentive reader of Hermann et al., 2015) improve from 44% on CBT (as reported by Hill et al., 2016) to 55% on Who-did-What.
",2 Related Work,[0],[0]
Above we referenced the comprehension datasets created from CNN and Daily Mail articles by Hermann et al. (2015).,2 Related Work,[0],[0]
"The CNN and Daily Mail datasets together consist of 1.4 million questions constructed from approximately 300,000 articles.",2 Related Work,[0],[0]
"Of existing datasets, these are the most similar to Who-did-What in that they consist of cloze-style question answering problems derived from news articles.",2 Related Work,[0],[0]
"As discussed in Section 1, our Who-did-What dataset differs from these datasets in not being derived from article summaries, in using baseline suppression, and in yielding a larger gap between machine and human performance.",2 Related Work,[0],[0]
"The Who-did-What dataset also differs in that the person named entities are not anonymized, permitting the use of external resources to improve performance while remaining difficult for language models due to suppression.",2 Related Work,[0],[0]
We now describe the construction of our Who-didWhat dataset in more detail.,3 Dataset Construction,[0],[0]
To generate a problem we first generate the question by selecting a random article — the “question article” — from the Gigaword corpus and taking the first sentence of that article — the “question sentence” — as the source of the cloze question.,3 Dataset Construction,[0],[0]
The hope is that the first sentence of an article contains prominent people and events which are likely to be discussed in other independent articles.,3 Dataset Construction,[0],[0]
"To convert the question sentence to a cloze question, we first extract named entities using the Stanford NER system (Finkel et al., 2005) and parse the sentence using the Stanford PCFG parser (Klein and Manning, 2003).
",3 Dataset Construction,[0],[0]
The person named entities are candidates for deletion to create a cloze problem.,3 Dataset Construction,[0],[0]
For each person named entity we then identify a noun phrase in the automatic parse that is headed by that person.,3 Dataset Construction,[0],[0]
"For example, if the question sentence is “President Obama met yesterday with Apple Founder Steve Jobs” we identify the two person noun phrases “President Obama” and “Apple Founder
Steve Jobs”.",3 Dataset Construction,[0],[0]
"When a person named entity is selected for deletion, the entire noun phrase is deleted.",3 Dataset Construction,[0],[0]
"For example, when deleting the second named entity, we get “President Obama met yesterday with XXX” rather than “President Obama met yesterday with Apple founder XXX”.",3 Dataset Construction,[0],[0]
This increases the difficulty of the problems because systems cannot rely on descriptors and other local contextual cues.,3 Dataset Construction,[0],[0]
"About 700,000 question sentences are generated from Gigaword articles (8% of the total number of articles).
",3 Dataset Construction,[0],[0]
Once a cloze question has been formed we select an appropriate article as a passage.,3 Dataset Construction,[0],[0]
The article should be independent of the question article but should discuss the people and events mentioned in the question sentence.,3 Dataset Construction,[0],[0]
"To find a passage we search the Gigaword dataset using the Apache Lucene information retrieval system (McCandless et al., 2010), using the question sentence as the query.",3 Dataset Construction,[0],[0]
The named entity to be deleted is included in the query and required to be included in the returned article.,3 Dataset Construction,[0],[0]
We also restrict the search to articles published within two weeks of the date of the question article.,3 Dataset Construction,[0],[0]
Articles containing sentences too similar to the question in word overlap and phrase matching near the blanked phrase are removed.,3 Dataset Construction,[0],[0]
We select the best matching article satisfying our constraints.,3 Dataset Construction,[0],[0]
"If no such article can be found, we abort the process and move on to a new question.
",3 Dataset Construction,[0],[0]
Given a question and a passage we next form the list of choices.,3 Dataset Construction,[0],[0]
We collect all person named entities in the passage except unblanked person named entities in the question.,3 Dataset Construction,[0],[0]
Choices that are subsets of longer choices are eliminated.,3 Dataset Construction,[0],[0]
For example the choice “Obama” would be eliminated if the list also contains “Barack Obama”.,3 Dataset Construction,[0],[0]
"We also discard ambiguous cases where a part of a blanked NE appears in multiple candidate answers, e.g., if a passage has “Bill Clinton” and “Hillary Clinton” and the blanked phrase is “Clinton”.",3 Dataset Construction,[0],[0]
We found this simple coreference rule to work well in practice since news articles usually employ full names for initial mentions of persons.,3 Dataset Construction,[0],[0]
"If the resulting choice list contains fewer than two or more than five choices, the process is aborted and we move on to a new question.3
After forming an initial set of problems we then
3The maximum of five helps to avoid sports articles containing structured lists of results.
remove “duplicated” problems.",3 Dataset Construction,[0],[0]
Duplication arises because Gigaword contains many copies of the same article or articles where one is clearly an edited version of another.,3 Dataset Construction,[0],[0]
Our duplication-removal process ensures that no two problems have very similar questions.,3 Dataset Construction,[0],[0]
"Here, similarity is defined as the ratio of the size of the bag of words intersection to the size of the smaller bag.
",3 Dataset Construction,[0],[0]
"In order to focus our dataset on the most interesting problems, we remove some problems to suppress the performance of the following simple baselines:
• First person in passage: Select the person that appears first in the passage.",3 Dataset Construction,[0],[0]
•,3 Dataset Construction,[0],[0]
"Most frequent person: Select the most frequent
person in the passage.",3 Dataset Construction,[0],[0]
• n,3 Dataset Construction,[0],[0]
"-gram: Select the most likely answer to fill the
blank under a 5-gram language model trained on Gigaword minus articles which are too similar to one of the questions in word overlap and phrase matching.",3 Dataset Construction,[0],[0]
"• Unigram: Select the most frequent last name us-
ing the unigram counts from the 5-gram model.
",3 Dataset Construction,[0],[0]
"To minimize the number of questions removed we solve an optimization problem defined by limiting the performance of each baseline to a specified target value while removing as few problems as possible, i.e.,
max α(C)
∑
C∈{0,1}|b| α(C)|T (C)| (1)
subject to
∀i ∑
C:Ci=1
α(C)|T (C)| N ≤",3 Dataset Construction,[0],[0]
"k
N = ∑
C∈{0,1}|b| α(C)|T (C)| (2)
where T (C) is the subset of the questions solved by the subset C of the suppressed baselines, α(C) is a keeping rate for question set T (C), Ci = 1 indicates the i-th baseline is in the subset, |b| is the number of baselines, N is a total number of questions, and k is the upper bound for the baselines after suppression.",3 Dataset Construction,[0],[0]
We choose k to yield random performance for the baselines.,3 Dataset Construction,[0],[0]
The performance of the baselines before and after suppression is shown in Table 2.,3 Dataset Construction,[0],[0]
"The suppression removed 49.9% of the questions.
",3 Dataset Construction,[0],[0]
Table 3 shows statistics of our dataset after suppression.,3 Dataset Construction,[0],[0]
"We split the final dataset into train, validation, and test by taking the validation and test to be a random split of the most recent 20,000 problems as measured by question article date.",3 Dataset Construction,[0],[0]
In this way there is very little overlap in semantic subject matter between the training set and either validation or test.,3 Dataset Construction,[0],[0]
We also provide a larger “relaxed” training set formed by applying less baseline suppression (a larger value of k in the optimization).,3 Dataset Construction,[0],[0]
"The relaxed training set then has a slightly different distribution from the train, validation, and test sets which are all fully suppressed.",3 Dataset Construction,[0],[0]
"We report the performance of several systems to characterize our dataset:
• Word overlap: Select the choice c inserted to the question q which is the most similar to any sentence s in the passage, i.e., CosSim(bag(c + q),bag(s)).",4 Performance Benchmarks,[0],[0]
"• Sliding window and Distance baselines (and their
combination) from Richardson et al. (2013).",4 Performance Benchmarks,[0],[0]
"• Semantic features: NLP feature based system
from Wang et al. (2015).
",4 Performance Benchmarks,[0],[0]
"• Attentive Reader: LSTM with attention mechanism (Hermann et al., 2015).",4 Performance Benchmarks,[0],[0]
•,4 Performance Benchmarks,[0],[0]
Stanford Reader:,4 Performance Benchmarks,[0],[0]
"An attentive reader modified
with a bilinear term (Chen et al., 2016).",4 Performance Benchmarks,[0],[0]
"• Attention Sum (AS) Reader: GRU with a point-
attention mechanism (Kadlec et al., 2016).",4 Performance Benchmarks,[0],[0]
"• Gated-Attention (GA) Reader: Attention Sum
Reader with gated layers (Dhingra et al., 2016).
",4 Performance Benchmarks,[0],[0]
Table 4 shows the performance of each system on the test data.,4 Performance Benchmarks,[0],[0]
"For the Attention and Stanford Readers, we anonymized the Who-did-What data by replacing named entities with entity IDs as in the CNN and Daily Mail datasets.
",4 Performance Benchmarks,[0],[0]
We see consistent reductions in accuracy when moving from CNN to our dataset.,4 Performance Benchmarks,[0],[0]
The Attentive and Stanford Reader drop by up to 10% and the AS and GA readers drop by up to 17%.,4 Performance Benchmarks,[0],[0]
The ranking of the systems also changes.,4 Performance Benchmarks,[0],[0]
"In contrast to the Attentive/Stanford readers, the AS/GA readers explicitly leverage the frequency of the answer in the passage, a heuristic which appears beneficial for the CNN and Daily Mail tasks.",4 Performance Benchmarks,[0],[0]
Our suppression of the mostfrequent-person baseline appears to more strongly affect the performance of these latter systems.,4 Performance Benchmarks,[0],[0]
We presented a large-scale person-centered cloze dataset whose scalability and flexibility is suitable for neural methods.,5 Conclusion,[0],[0]
This dataset is different in a variety of ways from existing large-scale cloze datasets and provides a significant extension to the training and test data for machine comprehension.,5 Conclusion,[0],[0]
We thank NVIDIA Corporation for donating GPUs used in this research.,Acknowledgments,[0],[0]
"We have constructed a new “Who-did-What” dataset of over 200,000 fill-in-the-gap (cloze) multiple choice reading comprehension problems constructed from the LDC English Gigaword newswire corpus.",abstractText,[0],[0]
The WDW dataset has a variety of novel features.,abstractText,[0],[0]
"First, in contrast with the CNN and Daily Mail datasets (Hermann et al., 2015) we avoid using article summaries for question formation.",abstractText,[0],[0]
"Instead, each problem is formed from two independent articles — an article given as the passage to be read and a separate article on the same events used to form the question.",abstractText,[0],[0]
"Second, we avoid anonymization — each choice is a person named entity.",abstractText,[0],[0]
"Third, the problems have been filtered to remove a fraction that are easily solved by simple baselines, while remaining 84% solvable by humans.",abstractText,[0],[0]
We report performance benchmarks of standard systems and propose the WDW dataset as a challenge task for the community.1,abstractText,[0],[0]
Who did What: A Large-Scale Person-Centered Cloze Dataset,title,[0],[0]
"In this paper we argue that crime drama exemplified in television programs such as CSI: Crime Scene Investigation is an ideal testbed for approximating real-world natural language understanding and the complex inferences associated with it. We propose to treat crime drama as a new inference task, capitalizing on the fact that each episode poses the same basic question (i.e., who committed the crime) and naturally provides the answer when the perpetrator is revealed. We develop a new dataset1 based on CSI episodes, formalize perpetrator identification as a sequence labeling problem, and develop an LSTM-based model which learns from multi-modal data. Experimental results show that an incremental inference strategy is key to making accurate guesses as well as learning from representations fusing textual, visual, and acoustic input.",text,[0],[0]
"The success of neural networks in a variety of applications (Sutskever et al., 2014; Vinyals et al., 2015) and the creation of large-scale datasets have played a critical role in advancing machine understanding of natural language on its own or together with other modalities.",1 Introduction,[0],[0]
"The problem has assumed several guises in the literature such as reading comprehension (Richardson et al., 2013; Rajpurkar et al., 2016), recognizing textual entailment (Bowman et al., 2015; Rocktäschel et al., 2016), and notably question answering based on text (Hermann et al.,
1Our dataset is available at https://github.com/ EdinburghNLP/csi-corpus.
2015; Weston et al., 2015), images (Antol et al., 2015), or video (Tapaswi et al., 2016).
",1 Introduction,[0],[0]
"In order to make the problem tractable and amenable to computational modeling, existing approaches study isolated aspects of natural language understanding.",1 Introduction,[0],[0]
"For example, it is assumed that understanding is an offline process, models are expected to digest large amounts of data before being able to answer a question, or make inferences.",1 Introduction,[0],[0]
"They are typically exposed to non-conversational texts or still images when focusing on the visual modality, ignoring the fact that understanding is situated in time and space and involves interactions between speakers.",1 Introduction,[0],[0]
"In this work we relax some of these simplifications by advocating a new task for natural language understanding which is multi-modal, exhibits spoken conversation, and is incremental, i.e., unfolds sequentially in time.
",1 Introduction,[0],[0]
"Specifically, we argue that crime drama exemplified in television programs such as CSI: Crime Scene Investigation can be used to approximate real-world natural language understanding and the complex inferences associated with it.",1 Introduction,[0],[0]
"CSI revolves around a team of forensic investigators trained to solve criminal cases by scouring the crime scene, collecting irrefutable evidence, and finding the missing pieces that solve the mystery.",1 Introduction,[0],[0]
Each episode poses the same “whodunnit” question and naturally provides the answer when the perpetrator is revealed.,1 Introduction,[0],[0]
"Speculation about the identity of the perpetrator is an integral part of watching CSI and an incremental process: viewers revise their hypotheses based on new evidence gathered around the suspect/s or on new inferences which they make as the episode evolves.
",1 Introduction,[0],[0]
"We formalize the task of identifying the perpetrator in a crime series as a sequence labeling problem.
1
Transactions of the Association for Computational Linguistics, vol. 6, pp. 1–15, 2018.",1 Introduction,[0],[0]
Action Editor: Marco Baroni.,1 Introduction,[0],[0]
"Submission batch: 8/2017; Revision batch: 10/2017; Published 1/2018.
",1 Introduction,[0],[0]
c©2018 Association for Computational Linguistics.,1 Introduction,[0],[0]
"Distributed under a CC-BY 4.0 license.
",1 Introduction,[0],[0]
"Like humans watching an episode, we assume the model is presented with a sequence of inputs comprising information from different modalities such as text, video, or audio (see Section 4 for details).",1 Introduction,[0],[0]
The model predicts for each input whether the perpetrator is mentioned or not.,1 Introduction,[0],[0]
Our formulation generalizes over episodes and crime series.,1 Introduction,[0],[0]
It is not specific to the identity and number of persons committing the crime as well as the type of police drama under consideration.,1 Introduction,[0],[0]
"Advantageously, it is incremental, we can track model predictions from the beginning of the episode and examine its behavior, e.g., how often it changes its mind, whether it is consistent in its predictions, and when the perpetrator is identified.
",1 Introduction,[0],[0]
We develop a new dataset based on 39 CSI episodes which contains goldstandard perpetrator mentions as well as viewers’ guesses about the perpetrator while each episode unfolds.,1 Introduction,[0],[0]
The sequential nature of the inference task lends itself naturally to recurrent network modeling.,1 Introduction,[0],[0]
"We adopt a generic architecture which combines a one-directional long-short term memory network (Hochreiter and Schmidhuber, 1997) with a softmax output layer over binary labels indicating whether the perpetrator is mentioned.",1 Introduction,[0],[0]
"Based on this architecture, we investigate the following questions:
1.",1 Introduction,[0],[0]
What type of knowledge is necessary for performing the perpetrator inference task?,1 Introduction,[0],[0]
"Is the textual modality sufficient or do other modalities (i.e., visual and auditory input) also play a role?
2.",1 Introduction,[0],[0]
What type of inference strategy is appropriate?,1 Introduction,[0],[0]
"In other words, does access to past information matter for making accurate inferences?
3.",1 Introduction,[0],[0]
To what extent does model behavior simulate humans?,1 Introduction,[0],[0]
"Does performance improve over time and how much of an episode does the model need to process in order to make accurate guesses?
",1 Introduction,[0],[0]
Experimental results on our new dataset reveal that multi-modal representations are essential for the task at hand boding well with real-world natural language understanding.,1 Introduction,[0],[0]
"We also show that an incremental inference strategy is key to guessing the perpetrator accurately although the model tends to be
less consistent compared to humans.",1 Introduction,[0],[0]
"In the remainder, we first discuss related work (Section 2), then present our dataset (Section 3) and formalize the modeling problem (Section 4).",1 Introduction,[0],[0]
We describe our experiments in Section 5.,1 Introduction,[0],[0]
"Our research has connections to several lines of work in natural language processing, computer vision, and more generally multi-modal learning.",2 Related Work,[0],[0]
"We review related literature in these areas below.
",2 Related Work,[0],[0]
Language Grounding Recent years have seen increased interest in the problem of grounding language in the physical world.,2 Related Work,[0],[0]
"Various semantic space models have been proposed which learn the meaning of words based on linguistic and visual or acoustic input (Bruni et al., 2014; Silberer et al., 2016; Lazaridou et al., 2015; Kiela and Bottou, 2014).",2 Related Work,[0],[0]
"A variety of cross-modal methods which fuse techniques from image and text processing have also been applied to the tasks of generating image descriptions and retrieving images given a natural language query (Vinyals et al., 2015; Xu et al., 2015; Karpathy and Fei-Fei, 2015).",2 Related Work,[0],[0]
"Another strand of research focuses on how to explicitly encode the underlying semantics of images making use of structural representations (Ortiz et al., 2015; Elliott and Keller, 2013; Yatskar et al., 2016; Johnson et al., 2015).",2 Related Work,[0],[0]
Our work shares the common goal of grounding language in additional modalities.,2 Related Work,[0],[0]
"Our model is, however, not static, it learns representations which evolve over time.
",2 Related Work,[0],[0]
"Video Understanding Work on video understanding has assumed several guises such as generating descriptions for video clips (Venugopalan et al., 2015a; Venugopalan et al., 2015b), retrieving video clips with natural language queries (Lin et al., 2014), learning actions in video (Bojanowski et al., 2013), and tracking characters (Sivic et al., 2009).",2 Related Work,[0],[0]
"Movies have also been aligned to screenplays (Cour et al., 2008), plot synopses (Tapaswi et al., 2015), and books (Zhu et al., 2015) with the aim of improving scene prediction and semantic browsing.",2 Related Work,[0],[0]
"Other work uses low-level features (e.g., based on face detection) to establish social networks of main characters in order to summarize movies or perform genre
classification (Rasheed et al., 2005; Sang and Xu, 2010; Dimitrova et al., 2000).",2 Related Work,[0],[0]
"Although visual features are used mostly in isolation, in some cases they are combined with audio in order to perform video segmentation (Boreczky and Wilcox, 1998) or semantic movie indexing (Naphide and Huang, 2001).
",2 Related Work,[0],[0]
A few datasets have been released recently which include movies and textual data.,2 Related Work,[0],[0]
"MovieQA (Tapaswi et al., 2016) is a large-scale dataset which contains 408 movies and 14,944 questions, each accompanied with five candidate answers, one of which is correct.",2 Related Work,[0],[0]
"For some movies, the dataset also contains subtitles, video clips, scripts, plots, and text from the Described Video Service (DVS), a narration service for the visually impaired.",2 Related Work,[0],[0]
"MovieDescription (Rohrbach et al., 2017) is a related dataset which contains sentences aligned to video clips from 200 movies.",2 Related Work,[0],[0]
"Scriptbase (Gorinski and Lapata, 2015) is another movie database which consists of movie screenplays (without video) and has been used to generate script summaries.
",2 Related Work,[0],[0]
"In contrast to the story comprehension tasks envisaged in MovieQA and MovieDescription, we focus on a single cinematic genre (i.e., crime series), and have access to entire episodes (and their corresponding screenplays) as opposed to video-clips or DVSs for some of the data.",2 Related Work,[0],[0]
"Rather than answering multiple factoid questions, we aim to solve a single problem, albeit one that is inherently challenging to both humans and machines.
",2 Related Work,[0],[0]
Question Answering A variety of question,2 Related Work,[0],[0]
answering tasks (and datasets) have risen in popularity in recent years.,2 Related Work,[0],[0]
"Examples include reading compre-
hension, i.e., reading text and answering questions about it (Richardson et al., 2013; Rajpurkar et al., 2016), open-domain question answering, i.e., finding the answer to a question from a large collection of documents (Voorhees and Tice, 2000; Yang et al., 2015), and cloze question completion, i.e., predicting a blanked-out word of a sentence (Hill et al., 2015; Hermann et al., 2015).",2 Related Work,[0],[0]
"Visual question answering (VQA; Antol et al. (2015)) is a another related task where the aim is to provide a natural language answer to a question about an image.
",2 Related Work,[0],[0]
"Our inference task can be viewed as a form of question answering over multi-modal data, focusing on one type of question.",2 Related Work,[0],[0]
"Compared to previous work on machine reading or visual question answering, we are interested in the temporal characteristics of the inference process, and study how understanding evolves incrementally with the contribution of various modalities (text, audio, video).",2 Related Work,[0],[0]
"Importantly, our formulation of the inference task as a sequence labeling problem departs from conventional question answering allowing us to study how humans and models alike make decisions over time.",2 Related Work,[0],[0]
"In this work, we make use of episodes of the U.S. TV show “Crime Scene Investigation Las Vegas” (henceforth CSI), one of the most successful crime series ever made.",3 The CSI Dataset,[0],[0]
Fifteen seasons with a total of 337 episodes were produced over the course of fifteen years.,3 The CSI Dataset,[0],[0]
"CSI is a procedural crime series, it follows a team of investigators employed by the Las Vegas Police Department as they collect and evaluate ev-
idence to solve murders, combining forensic police work with the investigation of suspects.
",3 The CSI Dataset,[0],[0]
"We paired official CSI videos (from seasons 1–5) with screenplays which we downloaded from a website hosting TV show transcripts.2 Our dataset comprises 39 CSI episodes, each approximately 43 minutes long.",3 The CSI Dataset,[0],[0]
"Episodes follow a regular plot, they begin with the display of a crime (typically without revealing the perpetrator) or a crime scene.",3 The CSI Dataset,[0],[0]
A team of five recurring police investigators attempt to reconstruct the crime and find the perpetrator.,3 The CSI Dataset,[0],[0]
"During the investigation, multiple (innocent) suspects emerge, while the crime is often committed by a single person, who is eventually identified and convicted.",3 The CSI Dataset,[0],[0]
Some CSI episodes may feature two or more unrelated cases.,3 The CSI Dataset,[0],[0]
At the beginning of the episode the CSI team is split and each investigator is assigned a single case.,3 The CSI Dataset,[0],[0]
"The episode then alternates between scenes covering each case, and the stories typically do not overlap.",3 The CSI Dataset,[0],[0]
Figure 1 displays a small excerpt from a CSI screenplay.,3 The CSI Dataset,[0],[0]
"Readers unfamiliar with script writing conventions should note that scripts typically consist of scenes, which have headings indicating where the scene is shot (e.g., inside someone’s house).",3 The CSI Dataset,[0],[0]
"Character cues preface the lines the actors speak (see boldface in Figure 1), and scene descriptions explain what the camera sees (see second and fifth panel in Figure 1).
",3 The CSI Dataset,[0],[0]
"Screenplays were further synchronized with the 2http://transcripts.foreverdreaming.org/
video using closed captions which are time-stamped and provided in the form of subtitles as part of the video data.",3 The CSI Dataset,[0],[0]
"The alignment between screenplay and closed captions is non-trivial, since the latter only contain dialogue, omitting speaker information or scene descriptions.",3 The CSI Dataset,[0],[0]
We first used dynamic time warping (DTW; Myers and Rabiner (1981)) to approximately align closed captions with the dialogue in the scripts.,3 The CSI Dataset,[0],[0]
"And then heuristically time-stamped remaining elements of the screenplay (e.g., scene descriptions), allocating them to time spans between spoken utterances.",3 The CSI Dataset,[0],[0]
"Table 1 shows some descriptive statistics on our dataset, featuring the number of cases per episode, its length (in terms of number of sentences), the type of crime, among other information.
",3 The CSI Dataset,[0],[0]
"The data was further annotated, with two goals in mind.",3 The CSI Dataset,[0],[0]
"Firstly, in order to capture the characteristics of the human inference process, we recorded how participants incrementally update their beliefs about the perpetrator.",3 The CSI Dataset,[0],[0]
"Secondly, we collected goldstandard labels indicating whether the perpetrator is mentioned.",3 The CSI Dataset,[0],[0]
"Specifically, while a participant watches an episode, we record their guesses about who the perpetrator is (Section 3.1).",3 The CSI Dataset,[0],[0]
"Once the episode is finished and the perpetrator is revealed, the same participant annotates entities in the screenplay referring to the true perpetrator (Section 3.2).",3 The CSI Dataset,[0],[0]
All annotations were collected through a webinterface.,3.1 Eliciting Behavioral Data,[0],[0]
"We recruited three annotators, all postgraduate students and proficient in English, none of them regular CSI viewers.",3.1 Eliciting Behavioral Data,[0],[0]
"We obtained annotations for 39 episodes (comprising 59 cases).
",3.1 Eliciting Behavioral Data,[0],[0]
A snapshot of the annotation interface is presented in Figure 2.,3.1 Eliciting Behavioral Data,[0],[0]
"The top of the interface provides a short description of the episode, i.e., in the form of a one-sentence summary (carefully designed to not give away any clues about the perpetrator).",3.1 Eliciting Behavioral Data,[0],[0]
Summaries were adapted from the CSI season summaries available in Wikipedia.3,3.1 Eliciting Behavioral Data,[0],[0]
"The annotator watches the episode (i.e., the video without closed captions) as a sequence of three minute intervals.",3.1 Eliciting Behavioral Data,[0],[0]
"Every three minutes, the video halts, and the annotator is pre-
3See e.g., https://en.wikipedia.org/wiki/ CSI:_Crime_Scene_Investigation_(season_1).
sented with the screenplay corresponding to the part of the episode they have just watched.",3.1 Eliciting Behavioral Data,[0],[0]
"While reading through the screenplay, they must indicate for every sentence whether they believe the perpetrator is mentioned.",3.1 Eliciting Behavioral Data,[0],[0]
"This way, we are able to monitor how humans create and discard hypotheses about perpetrators incrementally.",3.1 Eliciting Behavioral Data,[0],[0]
"As mentioned earlier, some episodes may feature more than one case.",3.1 Eliciting Behavioral Data,[0],[0]
"Annotators signal for each sentence, which case it belongs to or whether it is irrelevant (see the radio buttons in Figure 2).",3.1 Eliciting Behavioral Data,[0],[0]
"In order to obtain a more fine-grained picture of the human guesses, annotators are additionally asked to press a large red button (below the video screen) as soon as they “think they know who the perpetrator is”, i.e., at any time while they are
watching the video.",3.1 Eliciting Behavioral Data,[0],[0]
"They are allowed to press the button multiple times throughout the episode in case they change their mind.
",3.1 Eliciting Behavioral Data,[0],[0]
"Even though the annotation task just described reflects individual rather than gold-standard behavior, we report inter-annotator agreement (IAA) as a means of estimating variance amongst participants.",3.1 Eliciting Behavioral Data,[0],[0]
We computed IAA using Cohen’s (1960) Kappa based on three episodes annotated by two participants.,3.1 Eliciting Behavioral Data,[0],[0]
Overall agreement on this task (second column in Figure 2) is 0.74.,3.1 Eliciting Behavioral Data,[0],[0]
"We also measured percent agreement on the minority class (i.e., sentences tagged as “perpetrator mentioned”) and found it to be reasonably good at 0.62, indicating that despite individual differences, the process of guessing the perpetrator is broadly comparable across participants.",3.1 Eliciting Behavioral Data,[0],[0]
"Finally, annotators had no trouble distinguishing which utterances refer to which case (when the episode revolves around several), achieving an IAA of κ = 0.96.",3.1 Eliciting Behavioral Data,[0],[0]
"After watching the entire episode, the annotator reads through the screenplay for a second time, and tags entity mentions, now knowing the perpetrator.",3.2 Gold Standard Mention Annotation,[0],[0]
"Each word in the script has three radio buttons attached to it, and the annotator selects one only if a word refers to a perpetrator, a suspect, or a character who falls into neither of these classes (e.g., a police investigator or a victim).",3.2 Gold Standard Mention Annotation,[0],[0]
"For the majority of words, no button will be selected.",3.2 Gold Standard Mention Annotation,[0],[0]
A snapshot of our interface for this second layer of annotations is shown in Figure 3.,3.2 Gold Standard Mention Annotation,[0],[0]
"To ensure consistency, annotators were given detailed guidelines about what constitutes an entity.",3.2 Gold Standard Mention Annotation,[0],[0]
"Examples include proper names and their titles (e.g., Mr Collins, Sgt.",3.2 Gold Standard Mention Annotation,[0],[0]
"O’ Reilly),
pronouns (e.g., he, we), and other referring expressions including nominal mentions (e.g., let’s arrest the guy with the black hat).
",3.2 Gold Standard Mention Annotation,[0],[0]
Inter-annotator agreement based on three episodes and two annotators was κ = 0.90 on the perpetrator class and κ = 0.89 on other entity annotations (grouping together suspects with other entities).,3.2 Gold Standard Mention Annotation,[0],[0]
Percent agreement was 0.824 for perpetrators and 0.823 for other entities.,3.2 Gold Standard Mention Annotation,[0],[0]
The high agreement indicates that the task is well-defined and the elicited annotations reliable.,3.2 Gold Standard Mention Annotation,[0],[0]
"After the second pass, various entities in the script are disambiguated in terms of whether they refer to the perpetrator or other individuals.
",3.2 Gold Standard Mention Annotation,[0],[0]
Note that in this work we do not use the tokenlevel gold standard annotations directly.,3.2 Gold Standard Mention Annotation,[0],[0]
"Our model is trained on sentence-level annotations which we obtain from token-level annotations, under the assumption that a sentence mentions the perpetrator if it contains a token that does.",3.2 Gold Standard Mention Annotation,[0],[0]
We formalize the problem of identifying the perpetrator in a crime series episode as a sequence labeling task.,4 Model Description,[0],[0]
"Like humans watching an episode, our model is presented with a sequence of (possibly multi-modal) inputs, each corresponding to a sentence in the script, and assigns a label l indicating whether the perpetrator is mentioned in the sentence (l = 1) or not (l = 0).",4 Model Description,[0],[0]
"The model is fully incremental, each labeling decision is based solely on information derived from previously seen inputs.
",4 Model Description,[0],[0]
We could have formalized our inference task as a multi-label classification problem where labels correspond to characters in the script.,4 Model Description,[0],[0]
"Although perhaps more intuitive, the multi-class framework results in an output label space different for each episode which renders comparison of model performance across episodes problematic.",4 Model Description,[0],[0]
"In contrast, our formulation has the advantage of being directly applicable to any episode or indeed any crime series.
",4 Model Description,[0],[0]
A sketch of our inference task is shown in Figure 4.,4 Model Description,[0],[0]
The core of our model (see Figure 5) is a one-directional long-short term memory network (LSTM; Hochreiter and Schmidhuber (1997); Zaremba et al. (2014)).,4 Model Description,[0],[0]
"LSTM cells are a variant of recurrent neural networks with a more complex
computational unit which have emerged as a popular architecture due to their representational power and effectiveness at capturing long-term dependencies.",4 Model Description,[0],[0]
"LSTMs provide ways to selectively store and forget aspects of previously seen inputs, and as a consequence can memorize information over longer time periods.",4 Model Description,[0],[0]
"Through input, output, and forget gates, they can flexibly regulate the extent to which inputs are stored, used, and forgotten.
",4 Model Description,[0],[0]
"The LSTM processes a sequence of (possibly multi-modal) inputs s = {xh1 , xh2 , ..., xhN}.",4 Model Description,[0],[0]
It utilizes a memory slot ct and a hidden state ht which are incrementally updated at each time step t.,4 Model Description,[0],[0]
"Given input xt, the previous latent state ht−1 and previous memory state ct−1, the latent state ht for time t and the
updated memory state ct, are computed as follows:
 
it ft ot ĉt
  =   σ σ",4 Model Description,[0],[0]
"σ
tanh
 ",4 Model Description,[0],[0]
"W [ ht−1 xt ]
ct = ft ct−1 + it ĉt ht = ot tanh(ct).
",4 Model Description,[0],[0]
"The weight matrix W is estimated during inference, and i, o, and f are memory gates.
",4 Model Description,[0],[0]
"As mentioned earlier, the input to our model consists of a sequence of sentences, either spoken utterances or scene descriptions (we do not use speaker information).",4 Model Description,[0],[0]
"We further augment textual input with multi-modal information obtained from the alignment of screenplays to video (see Section 3).
",4 Model Description,[0],[0]
"Textual modality Words in each sentence are mapped to 50-dimensional GloVe embeddings, pretrained on Wikipedia and Gigaword (Pennington et al., 2014).",4 Model Description,[0],[0]
Word embeddings are subsequently concatenated and padded to the maximum sentence length observed in our data set in order to obtain fixed-length input vectors.,4 Model Description,[0],[0]
The resulting vector is passed through a convolutional layer with maxpooling to obtain a sentence-level representation xs.,4 Model Description,[0],[0]
"Word embeddings are fine-tuned during training.
",4 Model Description,[0],[0]
"Visual modality We obtain the video corresponding to the time span covered by each sentence and sample one frame per sentence from the center of the associated period.4 We then map each frame to a 1,536-dimensional visual feature vector xv using the final hidden layer of a pre-trained convolutional network which was optimized for object classification (inception-v4; Szegedy et al. (2016)).
",4 Model Description,[0],[0]
"Acoustic modality For each sentence, we extract the audio track from the video which includes all sounds and background music but no spoken dialog.",4 Model Description,[0],[0]
We then obtain Mel-frequency cepstral coefficient (MFCC) features from the continuous signal.,4 Model Description,[0],[0]
"MFCC features were originally developed in the context of speech recognition (Davis and Mermelstein, 1990; Sahidullah and Saha, 2012), but
4We also experimented with multiple frames per sentence but did not observe any improvement in performance.
have also been shown to work well for more general sound classification (Chachada and Kuo, 2014).",4 Model Description,[0],[0]
We extract a 13-dimensional MFCC feature vector for every five milliseconds in the video.,4 Model Description,[0],[0]
"For each input sentence, we sample five MFCC feature vectors from its associated time interval, and concatenate them in chronological order into the acoustic input xa.5
Modality Fusion",4 Model Description,[0],[0]
Our model learns to fuse multimodal input as part of its overall architecture.,4 Model Description,[0],[0]
"We use a general method to obtain any combination of input modalities (i.e., not necessarily all three).",4 Model Description,[0],[0]
Single modality inputs are concatenated into an m-dimensional vector (where m is the sum of dimensionalities of all the input modalities).,4 Model Description,[0],[0]
"We then multiply this vector with a weight matrix W h of dimension m× n, add an m-dimensional bias bh, and pass the result through a rectified linear unit (ReLU):
xh = ReLU([xs;xv;xa]W h + bh)
The resulting multi-modal representation xh is of dimension n and passed to the LSTM (see Figure 5).",4 Model Description,[0],[0]
In our experiments we investigate what type of knowledge and strategy are necessary for identifying the perpetrator in a CSI episode.,5 Evaluation,[0],[0]
In order to shed light on the former question we compare variants of our model with access to information from different modalities.,5 Evaluation,[0],[0]
We examine different inference strategies by comparing the LSTM to three baselines.,5 Evaluation,[0],[0]
"The first one lacks the ability to flexibly fuse multi-modal information (a CRF), while the second one does not have a notion of history, classifying inputs independently (a multilayer perceptron).",5 Evaluation,[0],[0]
Our third baseline is a rule-base system that neither uses multi-modal inputs nor has a notion of history.,5 Evaluation,[0],[0]
We also compare the LSTM to humans watching CSI.,5 Evaluation,[0],[0]
"Before we report our results, we describe our setup and comparison models in more detail.",5 Evaluation,[0],[0]
Our CSI data consists of 39 episodes giving rise to 59 cases (see Table 1).,5.1 Experimental Settings,[0],[0]
"The model was trained on
5Preliminary experiments showed that concatenation outperforms averaging or relying on a single feature vector.
53 cases using cross-validation (five splits with 47/6 training/test cases).",5.1 Experimental Settings,[0],[0]
"The remaining 6 cases were used as truly held-out test data for final evaluation.
",5.1 Experimental Settings,[0],[0]
We trained our model using ADAM with stochastic gradient-descent and mini-batches of six episodes.,5.1 Experimental Settings,[0],[0]
"Weights were initialized randomly, except for word embeddings which were initialized with pre-trained 50-dimensional GloVe vectors (Pennington et al., 2014), and fine-tuned during training.",5.1 Experimental Settings,[0],[0]
We trained our networks for 100 epochs and report the best result obtained during training.,5.1 Experimental Settings,[0],[0]
All results are averages of five runs of the network.,5.1 Experimental Settings,[0],[0]
"Parameters were optimized using two cross-validation splits.
",5.1 Experimental Settings,[0],[0]
"The sentence convolution layer has three filters of sizes 3, 4, 5 each of which after convolution returns 75-dimensional output.",5.1 Experimental Settings,[0],[0]
The final sentence representation xs is obtained by concatenating the output of the three filters and is of dimension 225.,5.1 Experimental Settings,[0],[0]
We set the size of the hidden representation of merged crossmodal inputs xh to 300.,5.1 Experimental Settings,[0],[0]
The LSTM has one layer with 128 nodes.,5.1 Experimental Settings,[0],[0]
"We set the learning rate to 0.001 and apply dropout with probability of 0.5.
",5.1 Experimental Settings,[0],[0]
We compared model output against the gold standard of perpetrator mentions which we collected as part of our annotation effort (second pass).,5.1 Experimental Settings,[0],[0]
"CRF Conditional Random Fields (Lafferty et al., 2001) are probabilistic graphical models for sequence labeling.",5.2 Model Comparison,[0],[0]
The comparison allows us to examine whether the LSTM’s use of long-term memory and (non-linear) feature integration is beneficial for sequence prediction.,5.2 Model Comparison,[0],[0]
"We experimented with a variety of features for the CRF, and obtained best results when the input sentence is represented by concatenated word embeddings.
",5.2 Model Comparison,[0],[0]
"MLP We also compared the LSTM against a multi-layer perceptron with two hidden layers, and a softmax output layer.",5.2 Model Comparison,[0],[0]
"We replaced the LSTM in our overall network structure with the MLP, keeping the methodology for sentence convolution and modality fusion and all associated parameters fixed to the values described in Section 5.1.",5.2 Model Comparison,[0],[0]
"The hidden layers of the MLP have ReLU activations and a layer-size of 128, as in the LSTM.",5.2 Model Comparison,[0],[0]
We set the learning rate to 0.0001.,5.2 Model Comparison,[0],[0]
The MLP makes independent predictions for each element in the sequence.,5.2 Model Comparison,[0],[0]
"This comparison
sheds light on the importance of sequential information for the perpetrator identification task.",5.2 Model Comparison,[0],[0]
"All results are best checkpoints over 100 training epochs, averaged over five runs.
",5.2 Model Comparison,[0],[0]
"PRO Aside from the supervised models described so far, we developed a simple rule-based system which does not require access to labeled data.",5.2 Model Comparison,[0],[0]
"The system defaults to the perpetrator class for any sentence containing a personal (e.g., you), possessive (e.g., mine) or reflexive pronoun",5.2 Model Comparison,[0],[0]
"(e.g., ourselves).",5.2 Model Comparison,[0],[0]
"In other words, it assumes that every pronoun refers to the perpetrator.",5.2 Model Comparison,[0],[0]
Pronoun mentions were identified using string-matching and a precompiled list of 31 pronouns.,5.2 Model Comparison,[0],[0]
"This system cannot incorporate any acoustic or visual data.
",5.2 Model Comparison,[0],[0]
"Human Upper Bound Finally, we compared model performance against humans.",5.2 Model Comparison,[0],[0]
"In our annotation task (Section 3.1), participants annotate sentences incrementally, while watching an episode for the first time.",5.2 Model Comparison,[0],[0]
The annotations express their belief as to whether the perpetrator is mentioned.,5.2 Model Comparison,[0],[0]
We evaluate these first-pass guesses against the gold standard (obtained in the second-pass annotation).,5.2 Model Comparison,[0],[0]
"We report precision, recall and f1 on the minority class, focusing on how accurately the models identify perpetrator mentions.",5.3 Which Model Is the Best Detective?,[0],[0]
"Table 2 summarizes our results, averaged across five cross-validation splits (left) and on the truly held-out test episodes (right).
",5.3 Which Model Is the Best Detective?,[0],[0]
"Overall, we observe that humans outperform all comparison models.",5.3 Which Model Is the Best Detective?,[0],[0]
"In particular, human precision is superior, whereas recall is comparable, with the exception of PRO which has high recall (at the expense of precision) since it assumes that all pronouns refer to perpetrators.",5.3 Which Model Is the Best Detective?,[0],[0]
We analyze the differences between model and human behavior in more detail in Section 5.5.,5.3 Which Model Is the Best Detective?,[0],[0]
"With regard to the LSTM, both visual and acoustic modalities bring improvements over the textual modality, however, their contribution appears to be complementary.",5.3 Which Model Is the Best Detective?,[0],[0]
"We also experimented with acoustic and visual features on their own, but without high-level textual information, the LSTM converges towards predicting the majority class only.",5.3 Which Model Is the Best Detective?,[0],[0]
"Results on the held-out test set reveal that our model generalizes well to unseen episodes, despite being trained on a relatively small data sample compared to standards in deep learning.
",5.3 Which Model Is the Best Detective?,[0],[0]
The LSTM consistently outperforms the nonincremental MLP.,5.3 Which Model Is the Best Detective?,[0],[0]
This shows that the ability to utilize information from previous inputs is essential for this task.,5.3 Which Model Is the Best Detective?,[0],[0]
"This is intuitively plausible; in order to identify the perpetrator, viewers must be aware of the plot’s development and make inferences while the episode evolves.",5.3 Which Model Is the Best Detective?,[0],[0]
"The CRF is outperformed by
all other systems, including rule-based PRO.",5.3 Which Model Is the Best Detective?,[0],[0]
"In contrast to the MLP and PRO, the CRF utilizes sequential information, but cannot flexibly fuse information from different modalities or exploit non-linear mappings like neural models.",5.3 Which Model Is the Best Detective?,[0],[0]
The only type of input which enabled the CRF to predict perpetrator mentions were concatenated word embeddings (see Table 2).,5.3 Which Model Is the Best Detective?,[0],[0]
"We trained CRFs on audio or visual features, together with word embeddings, however these models converged to only predicting the majority class.",5.3 Which Model Is the Best Detective?,[0],[0]
This suggests that CRFs do not have the capacity to model long complex sequences and draw meaningful inferences based on them.,5.3 Which Model Is the Best Detective?,[0],[0]
PRO achieves a reasonable f1 score but does so because it achieves high recall at the expense of very low precision.,5.3 Which Model Is the Best Detective?,[0],[0]
The precision-recall tradeoff is much more balanced for the neural systems.,5.3 Which Model Is the Best Detective?,[0],[0]
In this section we assess more directly how the LSTM compares against humans when asked to identify the perpetrator by the end of a CSI episode.,5.4 Can the Model Identify the Perpetrator?,[0],[0]
"Specifically, we measure precision in the final 10% of an episode, and compare human performance (first-pass guesses) and an LSTM model which uses all three modalities.",5.4 Can the Model Identify the Perpetrator?,[0],[0]
"Figure 6 shows precision results for 30 test episodes (across five cross-validation splits) and average precision as horizontal bars.
",5.4 Can the Model Identify the Perpetrator?,[0],[0]
"Perhaps unsurprisingly, human performance is superior; however, the model achieves an average precision of 60% which is encouraging (compared to
Episode 12 (Season 03): “Got Murder?”",5.4 Can the Model Identify the Perpetrator?,[0],[0]
"Episode 19 (Season 03): “A Night at the Movies”
85% achieved by humans).",5.4 Can the Model Identify the Perpetrator?,[0],[0]
Our results also show a moderate correlation between the model and humans: episodes which are difficult for the LSTM (see left side of the plot in Figure 6) also result in lower human precision.,5.4 Can the Model Identify the Perpetrator?,[0],[0]
Two episodes on the very left of the plot have 0% precision and are special cases.,5.4 Can the Model Identify the Perpetrator?,[0],[0]
"The first one revolves around a suicide, which is not strictly speaking a crime, while the second one does not mention the perpetrator in the final 10%.",5.4 Can the Model Identify the Perpetrator?,[0],[0]
We next analyze how the model’s guessing ability compares to humans.,5.5 How Is the Model Guessing?,[0],[0]
"Figure 7 tracks model behavior over the course of two episodes, across 100 equally sized intervals.",5.5 How Is the Model Guessing?,[0],[0]
"We show the cumulative development of f1 (top plot), cumulative true positive counts (center plot), and true positive counts within each interval (bottom plot).",5.5 How Is the Model Guessing?,[0],[0]
"Red bars indicate times at which annotators pressed the red button.
",5.5 How Is the Model Guessing?,[0],[0]
Figure 7 (right) shows that humans may outperform the LSTM in precision (but not necessarily in recall).,5.5 How Is the Model Guessing?,[0],[0]
"Humans are more cautious at guessing the perpetrator: the first human guess appears around sentence 300 (see the leftmost red vertical bars in
Figure 7 right), the first model guess around sentence 190, and the first true mention around sentence 30.",5.5 How Is the Model Guessing?,[0],[0]
"Once humans guess the perpetrator, however, they are very precise and consistent.",5.5 How Is the Model Guessing?,[0],[0]
"Interestingly, model guesses at the start of the episode closely follow the pattern of gold-perpetrator mentions (bottom plots in Figure 7).",5.5 How Is the Model Guessing?,[0],[0]
"This indicates that early model guesses are not noise, but meaningful predictions.
",5.5 How Is the Model Guessing?,[0],[0]
Further analysis of human responses is illustrated in Figure 8.,5.5 How Is the Model Guessing?,[0],[0]
For each of our three annotators we plot the points in each episode where they press the red button to indicate that they know the perpetrator (bottom).,5.5 How Is the Model Guessing?,[0],[0]
We also show the number of times (all three) annotators pressed the red button individually for each interval and cumulatively over the course of the episode.,5.5 How Is the Model Guessing?,[0],[0]
"Our analysis reveals that viewers tend to press the red button more towards the end, which is not unexpected since episodes are inherently designed to obfuscate the identification of the perpetrator.",5.5 How Is the Model Guessing?,[0],[0]
"Moreover, Figure 8 suggests that there are two types of viewers: eager viewers who like our model guess early on, change their mind often, and therefore press the red button frequently (annotator 1 pressed the red button 6.1 times on average per
episode) and conservative viewers who guess only late and press the red button less frequently (on average annotator 2 pressed the red button 2.9 times per episode, and annotator 3 and 3.7 times).",5.5 How Is the Model Guessing?,[0],[0]
"Notice that statistics in Figure 8 are averages across several episodes each annotator watched and thus viewer behavior is unlikely to be an artifact of individual episodes (e.g., featuring more or less suspects).",5.5 How Is the Model Guessing?,[0],[0]
Table 3 provides further evidence that the LSTM behaves more like an eager viewer.,5.5 How Is the Model Guessing?,[0],[0]
It presents the time in the episode (by sentence count) where the model correctly identifies the perpetrator for the first time.,5.5 How Is the Model Guessing?,[0],[0]
"As can be seen, the minimum and average identification times are lower for the LSTM compared to human viewers.",5.5 How Is the Model Guessing?,[0],[0]
Table 4 shows model predictions on two CSI screenplay excerpts.,5.5 How Is the Model Guessing?,[0],[0]
We illustrate the degree of the model’s belief in a perpetrator being mentioned by color intensity.,5.5 How Is the Model Guessing?,[0],[0]
True perpetrator mentions are highlighted in blue.,5.5 How Is the Model Guessing?,[0],[0]
"In the first example, the model mostly identifies perpetrator mentions correctly.",5.5 How Is the Model Guessing?,[0],[0]
"In the second example, it identifies seemingly plausible sentences which, however, refer to a suspect and not the true perpetrator.",5.5 How Is the Model Guessing?,[0],[0]
"In our experiments, we trained our model on CSI episodes which typically involve a crime, committed by a perpetrator, who is ultimately identified.",5.6 What if There Is No Perpetrator?,[0],[0]
"How does the LSTM generalize to episodes without
a crime, e.g., because the “victim” turns out to have committed suicide?",5.6 What if There Is No Perpetrator?,[0],[0]
"To investigate how model and humans alike respond to atypical input we present both with an episode featuring a suicide, i.e., an episode which did not have any true positive perpetrator mentions.
",5.6 What if There Is No Perpetrator?,[0],[0]
Figure 8 tracks the incremental behavior of a human viewer and the model while watching the suicide episode.,5.6 What if There Is No Perpetrator?,[0],[0]
"Both are primed by their experience with CSI episodes to identify characters in the plot as potential perpetrators, and consequently predict false positive perpetrator mentions.",5.6 What if There Is No Perpetrator?,[0],[0]
"The human realizes after roughly two thirds of the episode that there is no perpetrator involved (he does not annotate any subsequent sentences as “perpetrator mentioned”), whereas the LSTM continues to make perpetrator predictions until the end of the episode.",5.6 What if There Is No Perpetrator?,[0],[0]
The LSTM’s behavior is presumably an artifact of the recurring pattern of discussing the perpetrator in the very end of an episode.,5.6 What if There Is No Perpetrator?,[0],[0]
"In this paper we argued that crime drama is an ideal testbed for models of natural language understanding and their ability to draw inferences from complex, multi-modal data.",6 Conclusions,[0],[0]
The inference task is welldefined and relatively constrained: every episode poses and answers the same “whodunnit” question.,6 Conclusions,[0],[0]
We have formalized perpetrator identification as a sequence labeling problem and developed an LSTM-based model which learns incrementally from complex naturalistic data.,6 Conclusions,[0],[0]
"We showed that multi-modal input is essential for our task, as well
an incremental inference strategy with flexible access to previously observed information.",6 Conclusions,[0],[0]
"Compared to our model, humans guess cautiously in the beginning, but are consistent in their predictions once they have a strong suspicion.",6 Conclusions,[0],[0]
"The LSTM starts guessing earlier, leading to superior initial true-positive rates, however, at the cost of consistency.
",6 Conclusions,[0],[0]
There are many directions for future work.,6 Conclusions,[0],[0]
"Beyond perpetrators, we may consider how suspects emerge and disappear in the course of an episode.",6 Conclusions,[0],[0]
Note that we have obtained suspect annotations but did not use them in our experiments.,6 Conclusions,[0],[0]
"It should also be interesting to examine how the model behaves out-of-domain, i.e., when tested on other crime series, e.g., “Law and Order”.",6 Conclusions,[0],[0]
"Finally, more detailed analysis of what happens in an episode (e.g., what actions are performed, by who, when, and where) will give rise to deeper understanding enabling applications like video summarization and skimming.
",6 Conclusions,[0],[0]
"Acknowledgments The authors gratefully acknowledge the support of the European Research Council (award number 681760; Frermann, Lapata) and H2020 EU project SUMMA (award number 688139/H2020-ICT-2015; Cohen).",6 Conclusions,[0],[0]
"We also thank our annotators, the TACL editors and anonymous reviewers whose feedback helped improve the present paper, and members of EdinburghNLP for helpful discussions and suggestions.",6 Conclusions,[0],[0]
In this paper we argue that crime drama exemplified in television programs such as CSI: Crime Scene Investigation is an ideal testbed for approximating real-world natural language understanding and the complex inferences associated with it.,abstractText,[0],[0]
"We propose to treat crime drama as a new inference task, capitalizing on the fact that each episode poses the same basic question (i.e., who committed the crime) and naturally provides the answer when the perpetrator is revealed.",abstractText,[0],[0]
"We develop a new dataset1 based on CSI episodes, formalize perpetrator identification as a sequence labeling problem, and develop an LSTM-based model which learns from multi-modal data.",abstractText,[0],[0]
"Experimental results show that an incremental inference strategy is key to making accurate guesses as well as learning from representations fusing textual, visual, and acoustic input.",abstractText,[0],[0]
Whodunnit? Crime Drama as a Case for Natural Language Understanding,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 582–586 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
582",text,[0],[0]
"This paper brings together two fascinating research topics in natural language processing (NLP), namely understanding the properties of word embeddings (Mikolov et al., 2013; Mitchell and Steedman, 2015; Mimno and Thompson, 2017) and unsupervised bilingual dictionary induction (Conneau et al., 2018; Zhang et al., 2017; Søgaard et al., 2018).",1 Introduction,[0],[0]
"In an effort to better understand when unsupervised bilingual dictionary induction is possible, we factored out linguistic differences between languages, and studied EnglishEnglish alignability (by learning to align English embeddings trained on different samples of the English Wikipedia), when we came across a puzzling phenomena: English-English can be aligned with almost 100% precision, if you use the same
embedding algorithms for the two samples, but not at all (0% precision), if you use different embedding algorithms.",1 Introduction,[0],[0]
This results suggest that the properties of word embeddings induced by different algorithms challenge unsupervised bilingual dictionary algorithms.,1 Introduction,[0],[0]
"Understanding why will enable us to develop more stable adversarial learning algorithms and give us a better understanding of how embedding algorithms differ.
",1 Introduction,[0],[0]
"Contributions We are, to the best of our knowledge, the first to study unsupervised alignability of pairs of English word embeddings.",1 Introduction,[0],[0]
"We show that unsupervised alignment – specifically the MUSE system (Conneau et al., 2018) – fails when the algorithms used to induce the two embeddings differ, and that this is not because there is no linear transformation between the two spaces.",1 Introduction,[0],[0]
"We further show that poor initialization, as a result of MUSE initially applying an identity transform to two word embeddings far apart in space, is not the sole reason the discriminator suffers from local optima.",1 Introduction,[0],[0]
"Finally, we present an experiment showing what the minimal corpus size is for unsupervised alignment to succeed, in the absence of linguistic differences.",1 Introduction,[0],[0]
"MUSE (Conneau et al., 2018) uses a vanilla generative adversarial network (GAN) with a linear generator to learn alignments between embedding spaces without supervision.",2.1 Unsupervised alignment using generative adversarial networks,[0],[0]
"In a two-player game, a discriminator D aims to tell the two language spaces apart, while a generator G aims to map the source language into the target language space, fooling the discriminator.",2.1 Unsupervised alignment using generative adversarial networks,[0],[0]
"While MUSE achieves impressive results at times, MUSE is highly unstable, e.g., with different initializations precision
scores vary between 0% and 45% for EnglishGreek (Søgaard et al., 2018).
",2.1 Unsupervised alignment using generative adversarial networks,[0],[0]
"The parameters of a GAN with a linear generator are (Ω, w).",2.1 Unsupervised alignment using generative adversarial networks,[0],[0]
"They are obtained by solving the following min-max problem:
min Ω max w E[log(Dw(X))",2.1 Unsupervised alignment using generative adversarial networks,[0],[0]
+ log(1−Dw(gΩ(Z)))],2.1 Unsupervised alignment using generative adversarial networks,[0],[0]
"(1) which reduces to
min Ω
JS (PX | PΩ) (2)
Ω is initialized as the identity matrix I .",2.1 Unsupervised alignment using generative adversarial networks,[0],[0]
"If G wins the game against an ideal discriminator on a very large number of samples, then F (the source vector space) and ΩE (with E being the target vector space) can be shown to be close in Jensen-Shannon divergence, and thus the model has learned the true distribution.",2.1 Unsupervised alignment using generative adversarial networks,[0],[0]
"This result, referring to the distributions of the data, pdata , and the distribution, pg, G is sampling from, is from Goodfellow et al. (2014): If G and D have enough capacity, and at each step of training, the discriminator is allowed to reach its optimum given G, and pg is updated so as to improve the criterion
Ex∼pdata",2.1 Unsupervised alignment using generative adversarial networks,[0],[0]
[logD ∗ G(x)] + Ex∼pg,2.1 Unsupervised alignment using generative adversarial networks,[0],[0]
"[log(1−D∗G(x))]
then pg converges to pdata .",2.1 Unsupervised alignment using generative adversarial networks,[0],[0]
This result relies on a number of assumptions that do not hold in practice.,2.1 Unsupervised alignment using generative adversarial networks,[0],[0]
"Our generator, which learns a linear transform Ω, has very limited capacity, for example, and we are updating Ω rather than pg.",2.1 Unsupervised alignment using generative adversarial networks,[0],[0]
"In practice, therefore, during training, we alternate between k steps of optimizing the discriminator and one step of optimizing the generator.",2.1 Unsupervised alignment using generative adversarial networks,[0],[0]
"If the GAN-based alignment is not successful, this can thus be a result of two things: Either that G does not have enough capacity, or that D is stuck in a local optimum.",2.1 Unsupervised alignment using generative adversarial networks,[0],[0]
"Our results in §3 show that the inability to align English-English in the case of different word embedding algorithms is not a result of limited capacity, but a result of the GAN being trapped in one of the many local optima of the loss function.",2.1 Unsupervised alignment using generative adversarial networks,[0],[0]
"Procrustes Analysis (Schönemann, 1966) has been commonly used for supervised alignment of word embeddings (Smith et al., 2017; Artetxe et al., 2018).",2.2 Supervised alignment using Procrustes Analysis,[0],[0]
"Here, the optimal alignment between
two embedding spaces is computed using singular value decomposition of the aligned embeddings in a seed dictionary.",2.2 Supervised alignment using Procrustes Analysis,[0],[0]
Conneau et al. (2018) use Procrustes Analysis to refine an initial seed dictionary learned by the generative adversarial network without supervision.,2.2 Supervised alignment using Procrustes Analysis,[0],[0]
"In our supervised experiments, we use 5000 seed words as supervision for learning the alignment between embeddings.",2.2 Supervised alignment using Procrustes Analysis,[0],[0]
"Below we summarize some previous findings about the geometry of monolingual embeddings (Mimno and Thompson, 2017), and add some new observations.",2.3 Geometry of embeddings,[0],[0]
"We discuss five embedding algorithms: SVD on positive PMI matrices (Hyperwords-SVD) (Levy et al., 2015), skip-gram negative sampling applied to co-occurrence matrices (Hyperwords-SGNS)",2.3 Geometry of embeddings,[0],[0]
"(Levy et al., 2015), continuous bag-of-words (CBOW) (Mikolov et al., 2013a), GloVe (Pennington et al., 2014), and FastText (Bojanowski et al., 2017).",2.3 Geometry of embeddings,[0],[0]
"To analyze the geometry of our monolingual embeddings in space, we report average inner product to mean vector; see Mimno and Thompson (2017) for details.
",2.3 Geometry of embeddings,[0],[0]
"Hyperwords-SVD have a small average inner product (0.0032), suggesting they are welldispersed through space; like Hyperwords-SGNS and standard SGNS (Mimno and Thompson, 2017), they do not exhibit a clear word frequency bias.",2.3 Geometry of embeddings,[0],[0]
"Hyperwords-SGNS vectors also have a small average inner product (0.0002), in contrast with standard SGNS vectors, which are narrowly clustered in a single orthant (Mimno and Thompson, 2017).",2.3 Geometry of embeddings,[0],[0]
"In line with standard SGNS vectors, the frequency of words has relatively little effect on their inner product, with the exception of the rare words, which have slightly less positive inner products.",2.3 Geometry of embeddings,[0],[0]
CBOW vectors have a relatively large average inner product (4.2985).,2.3 Geometry of embeddings,[0],[0]
"The vectors trained by GloVe show a clear relationship with word frequency, with low-frequency words opposing the frequency-balanced mean vector.",2.3 Geometry of embeddings,[0],[0]
"The embeddings are well-dispersed, with an average inner product of 0.0002.",2.3 Geometry of embeddings,[0],[0]
"Finally, FastText vectors have a large, positive inner product with the mean (0.2988), indicating that they are not evenly dispersed through the space, but pointing in roughly the same direction.",2.3 Geometry of embeddings,[0],[0]
"The FastText vectors exhibit a frequency bias, much like what has been previously observed with GloVe vectors.",2.3 Geometry of embeddings,[0],[0]
"The differences are the results of the inductive biases of the
different embedding algorithms.",2.3 Geometry of embeddings,[0],[0]
"This section presents our data, the hyperparameters of our embeddings, our experimental protocols, and our results.",3 Experiments,[0],[0]
"In the following experiments we learn word embeddings on samples of a publicly available Wikipedia dump from March 2018.1 The data is preprocessed using a publicly available preprocessing script2, extracting text, removing nonalphanumeric characters, converting digits to text, and lowercasing the text.",3.1 Data,[0],[0]
"We train 300-dimensional word embeddings using the algorithms’ recommended hyperparameter settings, listed in the following:3",3.2 Hyper-parameters,[0],[0]
"For HyperwordsSGNS, the window size is set to 2 and the subsampling of frequent words and smoothing of the context distribution are disabled.",3.2 Hyper-parameters,[0],[0]
The minimal word count for being in the vocabulary is 100.,3.2 Hyper-parameters,[0],[0]
"The same applies for Hyperwords-SVD, and the exponent for weighting the eigenvalue matrix is 0.5.",3.2 Hyper-parameters,[0],[0]
"For CBOW, the window size is set to 8, the number of negative samples is 25, and the subsampling threshold for frequent words is 1e-4.",3.2 Hyper-parameters,[0],[0]
"For GloVe, the window size is set to 15 and the cutoff parameter xmax to 10.",3.2 Hyper-parameters,[0],[0]
"Finally, for FastText, the window size is 5, the number of negatives samples is 5 and the sampling threshold is 0.0001.",3.2 Hyper-parameters,[0],[0]
"We train word embeddings using the different embedding algorithms listed in §3.2 on two non-overlapping 10% samples of the English Wikipedia dump (the samples contain 463,576 and 528,556 distinct words, with an overlap in vocabulary of 351,858 words).",3.3 Main experiments,[0],[0]
"We learn unsupervised and supervised alignments for embeddings (as described in §2) trained by different algorithms on the same datasplits, and for embeddings trained by the same algorithm on the two different datasplits.",3.3 Main experiments,[0],[0]
"For the unsupervised alignments, we use the
1https://dumps.wikimedia.org/enwiki/ 2http://mattmahoney.net/dc/textdata.
html 3We also ran experiments with one of the embedding algorithms (FastText) to check if our results were robust across hyper-parameter settings
default parameters of the MUSE system for the adversarial training, i.e. a discriminator with 2 fully connected layers of 2048 units trained over 5 epochs, 1,000,000 iterations per epoch with 5 discriminator steps per iteration and a batch size of 32.
",3.3 Main experiments,[0],[0]
We evaluate the alignments in terms of Precision@1 in the word translation retrieval task for the 1500 test words used by Bojanowski et al. (2017).,3.3 Main experiments,[0],[0]
The results are shown in Table 14.,3.3 Main experiments,[0],[0]
Our main observations are: (a) MUSE learns perfect alignments for embeddings learned by the same algorithm on different data splits.,3.3 Main experiments,[0],[0]
"(b) MUSE cannot learn alignments for embeddings learned by different algorithms on the same data splits, even if there exists a linear transformation aligning both sets of embeddings (the supervised algorithm learns perfect alignments).",3.3 Main experiments,[0],[0]
We also verify that MUSE cannot learn to align embeddings from different algorithms even when induced from the same sample.,3.3 Main experiments,[0],[0]
"As already mentioned, we also ran experiments to check that the failure of MUSE to learn good alignments was not a result of the differences in hyper-parameter settings.",3.3 Main experiments,[0],[0]
"§3.4 presents additional experiments with normalization, for control; §3.5 addresses how much data is needed to align independently induced embeddings from the same algorithm.",3.3 Main experiments,[0],[0]
§4 discusses potential answers to why MUSE fails when embeddings are induced using different algorithms.,3.3 Main experiments,[0],[0]
The embeddings in the main experiments differ in several ways; see §2.,3.4 Experiments with normalization,[0],[0]
One possible explanation for the inability of MUSE to align embeddings from different algorithms could be that the two embeddings are so far apart in space that the discriminator learns to discriminate between them too quickly.,3.4 Experiments with normalization,[0],[0]
"Recall that Ω is initialized as the identity matrix I , which means that the generator initially presents the discriminator with the source embedding as is.",3.4 Experiments with normalization,[0],[0]
"This is an effect that has often been observed with GANs (Arjovsky and Bottou, 2017); could this also be the explanation for our results?",3.4 Experiments with normalization,[0],[0]
"At a first glance, this seems a possible explanation.",3.4 Experiments with normalization,[0],[0]
"The inner products with the mean differ significantly for the five embedding
4We report Precision at 1 scores but find that the pattern is the same for Precision at 10, with perfect alignments for embeddings from the same algorithm and 0 scores for alignments between embeddings from different algorithms in the unsupervised experiments.
",3.4 Experiments with normalization,[0],[0]
algorithms (see §2).,3.4 Experiments with normalization,[0],[0]
"The only embeddings that have roughly the same directionality are Hyperwords and GloVe, and their centroids are very far apart in cosine space.",3.4 Experiments with normalization,[0],[0]
"The cosine similarity of the centroids of the two versions of Hyperwords is - 0.006, and the cosine similarity for HyperwordsSVD and GloVe is 0.019.",3.4 Experiments with normalization,[0],[0]
"However, poor initialization as a result of applying the identity transform to very distant word embeddings is not the explanation for the poor performance of MUSE in this set-up: Both sets of Hyperwords embeddings were normalized, but alignment still failed.",3.4 Experiments with normalization,[0],[0]
"To verify this holds in general, i.e., that results are not affected by normalization in general, we also ran experiments with the remaining 14 embedding pairs, normalizing and/or centering both embeddings.",3.4 Experiments with normalization,[0],[0]
Results stayed the same: Precision at 1 scores of 0.,3.4 Experiments with normalization,[0],[0]
MUSE perfectly aligns independently induced word embeddings induced by the same algorithm.,3.5 Learning curve,[0],[0]
"For FastText, it correctly aligns 99.7% of all words in the evaluation lexicon with itself.",3.5 Learning curve,[0],[0]
"Our samples are 10% of a publicly available Wikipedia dump, amounting to more than 400 million tokens per sample.",3.5 Learning curve,[0],[0]
"English-English alignment is an interesting control experiment for unsupervised bilingual dictionary induction, abstracting away from linguistic differences, and we ran a series of experiments to see how small samples MUSE can align in the absence of linguistic differences.",3.5 Learning curve,[0],[0]
"The learn-
ing curve is presented in Figure 1.",3.5 Learning curve,[0],[0]
"We have shown that the fact that MUSE cannot align two embedding spaces for English induced by different algorithms (even if using the same corpus), is not a result of there not being a linear transformation, and not a result of (lack of) normalization or trivial differences in model hyperparameters.",4 Discussion,[0],[0]
"The only explanation left seems to be that the inductive biases of the different algorithms lead to a loss landscape so riddled with local optima that MUSE cannot possible escape them.
",4 Discussion,[0],[0]
"To support this hypothesis, compare the loss curves for the MUSE runs aligning embeddings induced with the same algorithms (black curves) to the runs aligning embeddings induced with dif-
ferent algorithms, in Figure 2.",4 Discussion,[0],[0]
"When the embeddings are induced by the same algorithm, we clearly see the contours of a min-max game, suggesting that the generator and discriminator challenge each other, both contributing to a good alignment.",4 Discussion,[0],[0]
"When the embeddings are induced by different algorithms, however, the discriminator quickly drops, with the generator unable to push the discriminator out of a local optimum.",4 Discussion,[0],[0]
"Understanding when biases induce highly non-convex landscapes, and how to make adversarial training less sensitive to such scenarios, remains an open problem, which we think will be key to the success of unsupervised machine translation and related tasks.",4 Discussion,[0],[0]
"This paper presents a challenge to the community: Generative adversarial networks (GANs) can perfectly align independent English word embeddings induced using the same algorithm, based on distributional information alone; but fails to do so, for two different embeddings algorithms.",abstractText,[0],[0]
Why is that?,abstractText,[0],[0]
"We believe understanding why, is key to understand both modern word embedding algorithms and the limitations and instability dynamics of GANs.",abstractText,[0],[0]
"This paper shows that (a) in all these cases, where alignment fails, there exists a linear transform between the two embeddings (so algorithm biases do not lead to non-linear differences), and (b) similar effects can not easily be obtained by varying hyper-parameters.",abstractText,[0],[0]
"One plausible suggestion based on our initial experiments is that the differences in the inductive biases of the embedding algorithms lead to an optimization landscape that is riddled with local optima, leading to a very small basin of convergence, but we present this more as a challenge paper than a technical contribution.",abstractText,[0],[0]
Why is unsupervised alignment of English embeddings from different algorithms so hard?,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2278–2282, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics",text,[0],[0]
"The neural encoder-decoder framework for machine translation (Neco and Forcada, 1997; Castaño and Casacuberta, 1997; Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015) provides new tools for addressing the field’s difficult challenges.",1 Introduction,[0],[0]
"In this framework (Figure 1), we use a recurrent neural network (encoder) to convert a source sentence into a dense, fixed-length vector.",1 Introduction,[0],[0]
We then use another recurrent network (decoder) to convert that vector into a target sentence.,1 Introduction,[0],[0]
"In this paper, we train long shortterm memory (LSTM) neural units (Hochreiter and Schmidhuber, 1997) trained with back-propagation through time (Werbos, 1990).
",1 Introduction,[0],[0]
A remarkable feature of this simple neural MT (NMT) model is that it produces translations of the right length.,1 Introduction,[0],[0]
"When we evaluate the system on previously unseen test data, using BLEU (Papineni et al., 2002), we consistently find the length ratio between MT outputs and human references translations to be very close to 1.0.",1 Introduction,[0],[0]
"Thus, no brevity penalty is incurred.",1 Introduction,[0],[0]
"This behavior seems to come for free, without special design.
",1 Introduction,[0],[0]
"By contrast, builders of standard statistical MT (SMT) systems must work hard to ensure correct length.",1 Introduction,[0],[0]
"The original mechanism comes from the
IBM SMT group, whose famous Models 1-5 included a learned table (y|x), with x and y being the lengths of source and target sentences (Brown et al., 1993).",1 Introduction,[0],[0]
But they did not deploy this table when decoding a foreign sentence f into an English sentence e; it did not participate in incremental scoring and pruning of candidate translations.,1 Introduction,[0],[0]
"As a result (Brown et al., 1995):
“However, for a given f, if the goal is to discover the most probable e, then the product P(e) P(f|e) is too small for long English strings as compared with short ones.",1 Introduction,[0],[0]
"As a result, short English strings are improperly favored over longer English strings.",1 Introduction,[0],[0]
"This tendency is counteracted in part by the following modification: Replace P(f|e) with clength(e) · P(f|e) for some empirically chosen constant c. This modification is treatment of the symptom rather than treatment of the disease itself, but it offers some temporary relief.",1 Introduction,[0],[0]
"The cure lies in better modeling.”
More temporary relief came from Minimum Error-Rate Training (MERT) (Och, 2003), which automatically sets c to maximize BLEU score.",1 Introduction,[0],[0]
"MERT also sets weights for the language model P(e), translation model P(f|e), and other features.",1 Introduction,[0],[0]
"The length feature combines so sensitively with other features that MERT frequently returns to it as it revises one weight at a time.
",1 Introduction,[0],[0]
"NMT’s ability to correctly model length is remarkable for these reasons: • SMT relies on maximum BLEU training to ob-
tain a length ratio that is prized by BLEU, while NMT obtains the same result through generic maximum likelihood training.",1 Introduction,[0],[0]
"• Standard SMT models explicitly “cross off”
2278
source words and phrases as they are translated, so it is clear when an SMT decoder has finished translating a sentence.",1 Introduction,[0],[0]
NMT systems lack this explicit mechanism.,1 Introduction,[0],[0]
"• SMT decoding involves heavy search, so if one
MT output path delivers an infelicitous ending, another path can be used.",1 Introduction,[0],[0]
"NMT decoding explores far fewer hypotheses, using a tight beam without recombination.
",1 Introduction,[0],[0]
"In this paper, we investigate how length regulation works in NMT.",1 Introduction,[0],[0]
We start with a simple problem in which source strings are composed of symbols a and b.,2 A Toy Problem for Neural MT,[0],[0]
The goal of the translator is simply to copy those strings.,2 A Toy Problem for Neural MT,[0],[0]
"Training cases look like this:
a a a b b a <EOS> → a a a b b a <EOS> b b",2 A Toy Problem for Neural MT,[0],[0]
"a <EOS> → b b a <EOS> a b a b a b a a <EOS> → a b a b a b a a <EOS> b b a b b a b b a <EOS> → b b a b b a b b a <EOS> The encoder must summarize the content of any
source string into a fixed-length vector, so that the decoder can then reconstruct it.1 With 4 hidden LSTM units, our NMT system can learn to solve this problem after being trained on 2500 randomly chosen strings of lengths up to 9.2 3
To understand how the learned system works, we encode different strings and record the resulting LSTM cell values.",2 A Toy Problem for Neural MT,[0],[0]
"Because our LSTM has four hidden units, each string winds up at some point in four-
1We follow Sutskever et al. (2014) in feeding the input string backwards to the encoder.
2Additional training details: 100 epochs, 100 minibatch size, 0.7 learning rate, 1.0 gradient clipping threshold.
",2 A Toy Problem for Neural MT,[0],[0]
"3We use the toolkit: https://github.com/isi-nlp/Zoph RNN
dimensional space.",2 A Toy Problem for Neural MT,[0],[0]
"We plot the first two dimensions (unit1 and unit2) in the left part of Figure 2, and we plot the other two dimensions (unit3 and unit4) in the right part.",2 A Toy Problem for Neural MT,[0],[0]
There is no dimension reduction in these plots.,2 A Toy Problem for Neural MT,[0],[0]
"Here is what we learn: • unit1 records the approximate length of the
string.",2 A Toy Problem for Neural MT,[0],[0]
Encoding a string of length 7 may generate a value of -6.99 for unit1.,2 A Toy Problem for Neural MT,[0],[0]
"• unit2 records the number of b’s minus the num-
ber of a’s, thus assigning a more positive value to b-heavy strings.",2 A Toy Problem for Neural MT,[0],[0]
It also includes a +1 bonus if the string ends with a. • unit3 records a prefix of the string.,2 A Toy Problem for Neural MT,[0],[0]
"If its value
is less than 1.0, the string starts with b.",2 A Toy Problem for Neural MT,[0],[0]
"Otherwise, it records the number of leading a’s.",2 A Toy Problem for Neural MT,[0],[0]
• unit4 has a more diffuse function.,2 A Toy Problem for Neural MT,[0],[0]
"If its value is
positive, then the string consists of all b’s (with a possible final a).",2 A Toy Problem for Neural MT,[0],[0]
"Otherwise, its value correlates with both negative length and the preponderance of b’s.
",2 A Toy Problem for Neural MT,[0],[0]
"For our purposes, unit1 is the interesting one.",2 A Toy Problem for Neural MT,[0],[0]
"Figure 3 shows the progression of “a b a b b b” as it gets encoded (top figure), then decoded (bottom two figures).",2 A Toy Problem for Neural MT,[0],[0]
"During encoding, the value of unit1 decreases by approximately 1.0 each time a letter is read.",2 A Toy Problem for Neural MT,[0],[0]
"During decoding, its value increases each time a letter is written.",2 A Toy Problem for Neural MT,[0],[0]
"When it reaches zero, it signals the decoder to output <EOS>.
",2 A Toy Problem for Neural MT,[0],[0]
The behavior of unit1 shows that the translator incorporates explicit length regulation.,2 A Toy Problem for Neural MT,[0],[0]
"It also explains two interesting phenomena: • When asked to transduce previously-unseen
strings up to length 14, the system occasionally makes a mistake, mixing up an a or b.",2 A Toy Problem for Neural MT,[0],[0]
"However, the output length is never wrong.4
4Machine translation researchers have also noticed that
• When we ask the system to transduce very long strings, beyond what it has been trained on, its output length may be slightly off.",2 A Toy Problem for Neural MT,[0],[0]
"For example, it transduces a string of 28 b’s into a string of 27 b’s.",2 A Toy Problem for Neural MT,[0],[0]
This is because unit1 is not incremented and decremented by exactly 1.0.,2 A Toy Problem for Neural MT,[0],[0]
Next we turn to full-scale NMT.,3 Full-Scale Neural Machine Translation,[0],[0]
"We train on data from the WMT 2014 English-to-French task, consisting of 12,075,604 sentence pairs, with 303,873,236 tokens on the English side, and 348,196,030 on the French side.",3 Full-Scale Neural Machine Translation,[0],[0]
We use 1000 hidden LSTM units.,3 Full-Scale Neural Machine Translation,[0],[0]
"We also use two layers of LSTM units between source and target.5
After the LSTM encoder-decoder is trained, we send test-set English strings through the encoder portion.",3 Full-Scale Neural Machine Translation,[0],[0]
"Every time a word token is consumed, we record the LSTM cell values and the length of the
when the translation is completely wrong, the length is still correct (anonymous).
",3 Full-Scale Neural Machine Translation,[0],[0]
"5Additional training details: 8 epochs, 128 minibatch size, 0.35 learning rate, 5.0 gradient clipping threshold.
string so far.",3 Full-Scale Neural Machine Translation,[0],[0]
"Over 143,379 token observations, we investigate how the LSTM encoder tracks length.
",3 Full-Scale Neural Machine Translation,[0],[0]
"With 1000 hidden units, it is difficult to build and inspect a heat map analogous to Figure 3.",3 Full-Scale Neural Machine Translation,[0],[0]
"Instead, we seek to predict string length from the cell values, using a weighted, linear combination of the 1000 LSTM cell values.",3 Full-Scale Neural Machine Translation,[0],[0]
"We use the least-squares method to find the best predictive weights, with resulting R2 values of 0.990 (for the first layer, closer to source text) and 0.981 (second layer).",3 Full-Scale Neural Machine Translation,[0],[0]
"So the entire network records length very accurately.
",3 Full-Scale Neural Machine Translation,[0],[0]
"However, unlike in the toy problem, no single unit tracks length perfectly.",3 Full-Scale Neural Machine Translation,[0],[0]
"The best unit in the second layer is unit109, which correlates with R2=0.894.
",3 Full-Scale Neural Machine Translation,[0],[0]
We therefore employ three mechanisms to locate,3 Full-Scale Neural Machine Translation,[0],[0]
a subset of units responsible for tracking length.,"7 334, 442, 109, 53, 46, 433, 663 0.955",[0],[0]
"We select the top k units according to: (1) individual R2 scores, (2) greedy search, which repeatedly adds the unit which maximizes the set’s R2 value, and (3) beam search.","7 334, 442, 109, 53, 46, 433, 663 0.955",[0],[0]
Table 1 shows different subsets we obtain.,"7 334, 442, 109, 53, 46, 433, 663 0.955",[0],[0]
These are quite predictive of length.,"7 334, 442, 109, 53, 46, 433, 663 0.955",[0],[0]
Table 2 shows how R2 increases as beam search augments the subset of units.,"7 334, 442, 109, 53, 46, 433, 663 0.955",[0],[0]
"For the toy problem, Figure 3 (middle part) shows how the cell value of unit1 moves back to zero as the target string is built up.",4 Mechanisms for Decoding,[0],[0]
"It also shows (lower part) how the probability of target word <EOS> shoots up once the correct target length has been achieved.
",4 Mechanisms for Decoding,[0],[0]
"MT decoding is trickier, because source and target strings are not necessarily the same length, and
target length depends on the words chosen.",4 Mechanisms for Decoding,[0],[0]
Figure 4 shows the action of unit109 and unit334 for a sample sentence.,4 Mechanisms for Decoding,[0],[0]
"They behave similarly on this sentence, but not identically.",4 Mechanisms for Decoding,[0],[0]
"These two units do not form a simple switch that controls length—rather, they are high-level features computed from lower/previous states that contribute quantitatively to the decision to end the sentence.
",4 Mechanisms for Decoding,[0],[0]
"Figure 4 also shows the log P(<EOS>) curve, where we note that the probability of outputting <EOS> rises sharply (from 10−8 to 10−4 to 0.998), rather than gradually.",4 Mechanisms for Decoding,[0],[0]
We determine how target length is regulated in NMT decoding.,5 Conclusion,[0],[0]
"In future work, we hope to determine how other parts of the translator work, especially with reference to grammatical structure and transformations.",5 Conclusion,[0],[0]
"This work was supported by ARL/ARO (W911NF10-1-0533), DARPA (HR0011-15-C-0115), and the Scientific and Technological Research Council of Turkey (TÜBİTAK) (grants 114E628 and 215E201).",Acknowledgments,[0],[0]
"We investigate how neural, encoder-decoder translation systems output target strings of appropriate lengths, finding that a collection of hidden units learns to explicitly implement this functionality.",abstractText,[0],[0]
Why Neural Translations are the Right Length,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4263–4272 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
4263",text,[0],[0]
"Different architectures have been shown to be effective for neural machine translation (NMT), ranging from recurrent architectures (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2015; Sutskever et al., 2014; Luong et al., 2015) to convolutional (Kalchbrenner and Blunsom, 2013; Gehring et al., 2017) and, most recently, fully selfattentional (Transformer) models (Vaswani et al., 2017).",1 Introduction,[0],[0]
"Since comparisons (Gehring et al., 2017; Vaswani et al., 2017; Hieber et al., 2017) are mainly carried out via BLEU (Papineni et al.,
∗Work carried out during a visit to the machine translation group at the University of Edinburgh.
2002), it is inherently difficult to attribute gains in BLEU to architectural properties.
",1 Introduction,[0],[0]
"Recurrent neural networks (RNNs) (Elman, 1990) can easily deal with variable-length input sentences and thus are a natural choice for the encoder and decoder of NMT systems.",1 Introduction,[0],[0]
"Modern variants of RNNs, such as GRUs (Cho et al., 2014) and LSTMs (Hochreiter and Schmidhuber, 1997), address the difficulty of training recurrent networks with long-range dependencies.",1 Introduction,[0],[0]
"Gehring et al. (2017) introduce a neural architecture where both the encoder and decoder are based on CNNs, and report better BLEU scores than RNN-based NMT models.",1 Introduction,[0],[0]
"Moreover, the computation over all tokens can be fully parallelized during training, which increases efficiency.",1 Introduction,[0],[0]
"Vaswani et al. (2017) propose Transformer models, which are built entirely with attention layers, without convolution or recurrence.",1 Introduction,[0],[0]
They report new state-of-art BLEU scores for EN→DE and EN→FR.,1 Introduction,[0],[0]
"Yet, the BLEU metric is quite coarse-grained, and offers no insight as to which aspects of translation are improved by different architectures.
",1 Introduction,[0],[0]
"To explain the observed improvements in BLEU, previous work has drawn on theoretical arguments.",1 Introduction,[0],[0]
"Both Gehring et al. (2017) and Vaswani et al. (2017) argue that the length of the paths in neural networks between co-dependent elements affects the ability to learn these dependencies: the shorter the path, the easier the model learns such dependencies.",1 Introduction,[0],[0]
"The papers argue that Transformers and CNNs are better suited than RNNs to capture long-range dependencies.
",1 Introduction,[0],[0]
"However, this claim is based on a theoretical argument and has not been empirically tested.",1 Introduction,[0],[0]
We argue other abilities of non-recurrent networks could be responsible for their strong performance.,1 Introduction,[0],[0]
"Specifically, we hypothesize that the improvements in BLEU are due to CNNs and Transformers being strong semantic feature extractors.
",1 Introduction,[0],[0]
"In this paper, we evaluate all three popular NMT architectures: models based on RNNs (referred to as RNNS2S in the remainder of the paper), based on CNNs (referred to as ConvS2S) and self-attentional models (referred to as Transformers).",1 Introduction,[0],[0]
"Motivated by the aforementioned theoretical claims regarding path length and semantic feature extraction, we evaluate their performance on a subject-verb agreement task (that requires modeling long-range dependencies) and a word sense disambiguation (WSD) task (that requires extracting semantic features).",1 Introduction,[0],[0]
"Both tasks build on test sets of contrastive translation pairs, Lingeval97 (Sennrich, 2017) and ContraWSD (Rios et al., 2017).
",1 Introduction,[0],[0]
"The main contributions of this paper can be summarized as follows:
• We test the theoretical claims that architectures with shorter paths through networks are better at capturing long-range dependencies.",1 Introduction,[0],[0]
"Our experimental results on modeling subject-verb agreement over long distances do not show any evidence that Transformers or CNNs are superior to RNNs in this regard.
",1 Introduction,[0],[0]
• We empirically show that the number of attention heads in Transformers impacts their ability to capture long-distance dependencies.,1 Introduction,[0],[0]
"Specifically, many-headed multi-head attention is essential for modeling longdistance phenomena with only self-attention.
",1 Introduction,[0],[0]
"• We empirically show that Transformers excel at WSD, indicating that they are strong semantic feature extractors.",1 Introduction,[0],[0]
"Yin et al. (2017) are the first to compare CNNs, LSTMs and GRUs on several NLP tasks.",2 Related work,[0],[0]
"They find that CNNs are better at tasks related to semantics, while RNNs are better at syntax-related tasks, especially for longer sentences.
",2 Related work,[0],[0]
"Based on the work of Linzen et al. (2016), Bernardy and Lappin (2017) find that RNNs perform better than CNNs on a subject-verb agreement task, which is a good proxy for how well long-range dependencies are captured.",2 Related work,[0],[0]
Tran et al. (2018) find that a Transformer language model performs worse than an RNN language model on a subject-verb agreement task.,2 Related work,[0],[0]
"They, too, note that this is especially true as the distance between subject and verb grows, even if RNNs resulted in a
higher perplexity on the validation set.",2 Related work,[0],[0]
"This result of Tran et al. (2018) is clearly in contrast to the general finding that Transformers are better than RNNs for NMT tasks.
",2 Related work,[0],[0]
Bai et al. (2018) evaluate CNNs and LSTMs on several sequence modeling tasks.,2 Related work,[0],[0]
They conclude that CNNs are better than RNNs for sequence modeling.,2 Related work,[0],[0]
"However, their CNN models perform much worse than the state-of-art LSTM models on some sequence modeling tasks, as they themselves state in the appendix.
",2 Related work,[0],[0]
Tang et al. (2018) evaluate different RNN architectures and Transformer models on the task of historical spelling normalization which translates a historical spelling into its modern form.,2 Related work,[0],[0]
"They find that Transformer models surpass RNN models only in high-resource conditions.
",2 Related work,[0],[0]
"In contrast to previous studies, we focus on the machine translation task, where architecture comparisons so far are mostly based on BLEU.",2 Related work,[0],[0]
"We evaluate three different NMT architectures: RNN-based models, CNN-based models, and Transformer-based models.",3.1 NMT Architectures,[0],[0]
All of them have a bipartite structure in the sense that they consist of an encoder and a decoder.,3.1 NMT Architectures,[0],[0]
"The encoder and the decoder interact via a soft-attention mechanism (Bahdanau et al., 2015; Luong et al., 2015), with one or multiple attention layers.
",3.1 NMT Architectures,[0],[0]
"In the following sections, hli is the hidden state at step i of layer l, hli−1 represents the hidden state at the previous step of layer l, hl−1i means the hidden state at i of l − 1 layer, Exi represents the embedding of xi, and epos,i denotes the positional embedding at position i.",3.1 NMT Architectures,[0],[0]
"RNNs are stateful networks that change as new inputs are fed to them, and each state has a direct connection only to the previous state.",3.1.1 RNN-based NMT,[0],[0]
"Thus, the path length of any two tokens with a distance of n in RNNs is exactly n. Figure 1 (a) shows an illustration of RNNs.
",3.1.1 RNN-based NMT,[0],[0]
hli = h l−1,3.1.1 RNN-based NMT,[0],[0]
i + frnn(h l−1,3.1.1 RNN-based NMT,[0],[0]
"i , h l i−1) (1)
",3.1.1 RNN-based NMT,[0],[0]
"In deep architectures, two adjacent layers are commonly connected with residual connections.",3.1.1 RNN-based NMT,[0],[0]
"In the lth encoder layer, hli is generated by Equation 1,
where frnn is the RNN (GRU or LSTM) function.",3.1.1 RNN-based NMT,[0],[0]
"In the first layer, h0i = frnn(Exi , h 0 i−1).
",3.1.1 RNN-based NMT,[0],[0]
"In addition to the connection between the encoder and decoder via attention, the initial state of the decoder is usually initialized with the average of the hidden states or the last hidden state of the encoder.",3.1.1 RNN-based NMT,[0],[0]
"CNNs are hierarchical networks, in that convolution layers capture local correlations.",3.1.2 CNN-based NMT,[0],[0]
The local context size depends on the size of the kernel and the number of layers.,3.1.2 CNN-based NMT,[0],[0]
"In order to keep the output the same length as the input, CNN models add padding symbols to input sequences.",3.1.2 CNN-based NMT,[0],[0]
"Given an Llayer CNN with a kernel size k, the largest context size is L(k−1).",3.1.2 CNN-based NMT,[0],[0]
"For any two tokens in a local context with a distance of n, the path between them is only dn/(k − 1)e.
",3.1.2 CNN-based NMT,[0],[0]
"As Figure 1 (b) shows, a 2-layer CNN with kernel size 3 “sees” an effective local context of 5 tokens.",3.1.2 CNN-based NMT,[0],[0]
"The path between the first token and the fifth token is only 2 convolutions.1 Since CNNs do not have a means to infer the position of elements in a sequence, positional embeddings are introduced.
",3.1.2 CNN-based NMT,[0],[0]
hli = h l−1,3.1.2 CNN-based NMT,[0],[0]
i + fcnn(W,3.1.2 CNN-based NMT,[0],[0]
"l[hl−1i−bk/2c; ...;h l−1 i+bk/2c]
+ bl) (2)
",3.1.2 CNN-based NMT,[0],[0]
The hidden state hli shown in Equation 2 is related to the hidden states in the same convolution and the hidden state hl−1i from the previous layer.,3.1.2 CNN-based NMT,[0],[0]
k denotes the kernel size of CNNs and fcnn is a nonlinearity.,3.1.2 CNN-based NMT,[0],[0]
ConvS2S chooses Gated Linear Units (GLU) which can be viewed as a gated variation of ReLUs.,3.1.2 CNN-based NMT,[0],[0]
W l are called convolutional filters.,3.1.2 CNN-based NMT,[0],[0]
"In the input layer, h0i = Exi + epos,i.
1Note that the decoder employs masking to avoid conditioning the model on future information, which reduces the effective context size to L k−1
2 .",3.1.2 CNN-based NMT,[0],[0]
Transformers rely heavily on self-attention networks.,3.1.3 Transformer-based NMT,[0],[0]
Each token is connected to any other token in the same sentence directly via selfattention.,3.1.3 Transformer-based NMT,[0],[0]
"Moreover, Transformers feature attention networks with multiple attention heads.",3.1.3 Transformer-based NMT,[0],[0]
"Multi-head attention is more fine-grained, compared to conventional 1-head attention mechanisms.",3.1.3 Transformer-based NMT,[0],[0]
Figure 1 (c) illustrates that any two tokens are connected directly: the path length between the first and the fifth tokens is 1.,3.1.3 Transformer-based NMT,[0],[0]
"Similar to CNNs, positional information is also preserved in positional embeddings.
",3.1.3 Transformer-based NMT,[0],[0]
The hidden state in the Transformer encoder is calculated from all hidden states of the previous layer.,3.1.3 Transformer-based NMT,[0],[0]
"The hidden state hli in a self-attention network is computed as in Equation 3.
",3.1.3 Transformer-based NMT,[0],[0]
hli = h l−1,3.1.3 Transformer-based NMT,[0],[0]
i + f(self-attention(h l−1 i )),3.1.3 Transformer-based NMT,[0],[0]
"(3)
where f represents a feedforward network with ReLU as the activation function and layer normalization.",3.1.3 Transformer-based NMT,[0],[0]
"In the input layer, h0i = Exi + epos,i. The decoder additionally has a multi-head attention over the encoder hidden states.",3.1.3 Transformer-based NMT,[0],[0]
"Since we evaluate different NMT architectures explicitly on subject-verb agreement and WSD (both happen implicitly during machine translation), BLEU as a measure of overall translation quality is not helpful.",3.2 Contrastive Evaluation of Machine Translation,[0],[0]
"In order to conduct these targeted evaluations, we use contrastive test sets.
",3.2 Contrastive Evaluation of Machine Translation,[0],[0]
Sets of contrastive translations can be used to analyze specific types of errors.,3.2 Contrastive Evaluation of Machine Translation,[0],[0]
"Human reference translations are paired with one or more contrastive variants, where a specific type of error is introduced automatically.
",3.2 Contrastive Evaluation of Machine Translation,[0],[0]
The evaluation procedure then exploits the fact that NMT models are conditional language models.,3.2 Contrastive Evaluation of Machine Translation,[0],[0]
"By virtue of this, given any source sentence S
and target sentence T , any NMT model can assign to them a probability P (T |S).",3.2 Contrastive Evaluation of Machine Translation,[0],[0]
"If a model assigns a higher score to the correct target sentence than to a contrastive variant that contains an error, we consider it a correct decision.",3.2 Contrastive Evaluation of Machine Translation,[0],[0]
"The accuracy of a model on such a test set is simply the percentage of cases where the correct target sentence is scored higher than all contrastive variants.
",3.2 Contrastive Evaluation of Machine Translation,[0],[0]
Contrastive evaluation tests the sensitivity of NMT models to specific translation errors.,3.2 Contrastive Evaluation of Machine Translation,[0],[0]
The contrastive examples are designed to capture specific translation errors rather than evaluating the global quality of NMT models.,3.2 Contrastive Evaluation of Machine Translation,[0],[0]
"Although they do not replace metrics such as BLEU, they give further insights into the performance of models, on specific linguistic phenomena.
3.2.1 Lingeval97 Lingeval97 has over 97,000 English→German contrastive translation pairs featuring different linguistic phenomena, including subject-verb agreement, noun phrase agreement, separable verbparticle constructions, transliterations and polarity.",3.2 Contrastive Evaluation of Machine Translation,[0],[0]
"In this paper, we are interested in evaluating the performance on long-range dependencies.",3.2 Contrastive Evaluation of Machine Translation,[0],[0]
"Thus, we focus on the subject-verb agreement category which consists of 35,105 instances.
",3.2 Contrastive Evaluation of Machine Translation,[0],[0]
"In German, verbs must agree with their subjects in both grammatical number and person.",3.2 Contrastive Evaluation of Machine Translation,[0],[0]
"Therefore, in a contrastive translation, the grammatical number of a verb is swapped.",3.2 Contrastive Evaluation of Machine Translation,[0],[0]
"Table 1 gives an example.
",3.2 Contrastive Evaluation of Machine Translation,[0],[0]
3.2.2 ContraWSD,3.2 Contrastive Evaluation of Machine Translation,[0],[0]
"In ContraWSD, given an ambiguous word in the source sentence, the correct translation is replaced by another meaning of the ambiguous word which is incorrect.",3.2 Contrastive Evaluation of Machine Translation,[0],[0]
"For example, in a case where the English word line is the correct translation of the German source word Schlange, ContraWSD replaces line with the other translations of Schlange, such as snake, serpent, to generate contrastive translations.
",3.2 Contrastive Evaluation of Machine Translation,[0],[0]
"For German→English, ContraWSD contains 84 different German word senses.",3.2 Contrastive Evaluation of Machine Translation,[0],[0]
"It has 7,200
German→English lexical ambiguities, each lexical ambiguity instance has 3.5 contrastive translations on average.",3.2 Contrastive Evaluation of Machine Translation,[0],[0]
"For German→French, it consists of 71 different German word senses.",3.2 Contrastive Evaluation of Machine Translation,[0],[0]
"There are 6,700 German→French lexical ambiguities, with an average of 2.2 contrastive translations each lexical ambiguity instance.",3.2 Contrastive Evaluation of Machine Translation,[0],[0]
All the ambiguous words are nouns so that the disambiguation is not possible simply based on syntactic context.,3.2 Contrastive Evaluation of Machine Translation,[0],[0]
"The subject-verb agreement task is the most popular choice for evaluating the ability to capture long-range dependencies and has been used in many studies (Linzen et al., 2016; Bernardy and Lappin, 2017; Sennrich, 2017; Tran et al., 2018).",4 Subject-verb Agreement,[0],[0]
"Thus, we also use this task to evaluate different NMT architectures on long-range dependencies.",4 Subject-verb Agreement,[0],[0]
Different architectures are hard to compare fairly because many factors affect performance.,4.1 Experimental Settings,[0],[0]
"We aim to create a level playing field for the comparison by training with the same toolkit, Sockeye (Hieber et al., 2017) which is based on MXNet",4.1 Experimental Settings,[0],[0]
"(Chen et al., 2015).",4.1 Experimental Settings,[0],[0]
"In addition, different hyperparameters and training techniques (such as label smoothing or layer normalization) have been found to affect the performance (Chen et al., 2018).",4.1 Experimental Settings,[0],[0]
We apply the same hyperparameters and techniques for all architectures except the parameters of each specific architecture.,4.1 Experimental Settings,[0],[0]
"Since the best hyperparameters for different architectures may be diverse, we verify our hyperparameter choice by comparing our results to those published previously.",4.1 Experimental Settings,[0],[0]
Our models achieve similar performance to that reported by Hieber et al. (2017) with the best available settings.,4.1 Experimental Settings,[0],[0]
"In addition, we extend Sockeye with an interface that enables scoring of existing translations, which is required for contrastive evaluation.
",4.1 Experimental Settings,[0],[0]
All the models are trained with 2 GPUs.,4.1 Experimental Settings,[0],[0]
"During training, each mini-batch contains 4096 tokens.",4.1 Experimental Settings,[0],[0]
"A model checkpoint is saved every 4,000 updates.",4.1 Experimental Settings,[0],[0]
"We use Adam (Kingma and Ba, 2015) as the optimizer.",4.1 Experimental Settings,[0],[0]
The initial learning rate is set to 0.0002.,4.1 Experimental Settings,[0],[0]
"If the performance on the validation set has not improved for 8 checkpoints, the learning rate is multiplied by 0.7.",4.1 Experimental Settings,[0],[0]
We set the early stopping patience to 32 checkpoints.,4.1 Experimental Settings,[0],[0]
All the neural networks have 8 layers.,4.1 Experimental Settings,[0],[0]
"For RNNS2S, the encoder has 1 bi-directional LSTM and 6 stacked
uni-directional LSTMs, and the decoder is a stack of 8 uni-directional LSTMs.",4.1 Experimental Settings,[0],[0]
The size of embeddings and hidden states is 512.,4.1 Experimental Settings,[0],[0]
We apply layer normalization and label smoothing (0.1) in all models.,4.1 Experimental Settings,[0],[0]
We tie the source and target embeddings.,4.1 Experimental Settings,[0],[0]
The dropout rate of embeddings and Transformer blocks is set to 0.1.,4.1 Experimental Settings,[0],[0]
The dropout rate of RNNs and CNNs is 0.2.,4.1 Experimental Settings,[0],[0]
The kernel size of CNNs is 3.,4.1 Experimental Settings,[0],[0]
"Transformers have an 8-head attention mechanism.
",4.1 Experimental Settings,[0],[0]
"To test the robustness of our findings, we also test a different style of RNN architecture, from a different toolkit.",4.1 Experimental Settings,[0],[0]
"We evaluate bi-deep transitional RNNs (Miceli Barone et al., 2017) which are state-of-art RNNs in machine translation.",4.1 Experimental Settings,[0],[0]
"We use the bi-deep RNN-based model (RNN-bideep) implemented in Marian (Junczys-Dowmunt et al., 2018).",4.1 Experimental Settings,[0],[0]
"Different from the previous settings, we use the Adam optimizer with β1 = 0.9, β2 = 0.98, = 10−9.",4.1 Experimental Settings,[0],[0]
The initial learning rate is 0.0003.,4.1 Experimental Settings,[0],[0]
We tie target embeddings and output embeddings.,4.1 Experimental Settings,[0],[0]
"Both the encoder and decoder have 4 layers of LSTM units, only the encoder layers are bi-directional.",4.1 Experimental Settings,[0],[0]
"LSTM units consist of several cells (deep transition): 4 in the first layer of the decoder, 2 cells everywhere else.
",4.1 Experimental Settings,[0],[0]
We use training data from the WMT17 shared task.2,4.1 Experimental Settings,[0],[0]
"We use newstest2013 as the validation set, and use newstest2014 and newstest2017 as the test sets.",4.1 Experimental Settings,[0],[0]
"All BLEU scores are computed with SacreBLEU (Post, 2018).",4.1 Experimental Settings,[0],[0]
There are about 5.9 million sentence pairs in the training set after preprocessing with Moses scripts.,4.1 Experimental Settings,[0],[0]
"We learn a joint BPE model with 32,000 subword units (Sennrich et al., 2016).",4.1 Experimental Settings,[0],[0]
We employ the model that has the best perplexity on the validation set for the evaluation.,4.1 Experimental Settings,[0],[0]
"Table 2 reports the BLEU scores on newstest2014 and newstest2017, the perplexity on the validation set, and the accuracy on long-range dependencies.3 Transformer achieves the highest accuracy on this task and the highest BLEU scores on both newstest2014 and newstest2017.",4.2 Overall Results,[0],[0]
"Compared to RNNS2S, ConvS2S has slightly better results regarding BLEU scores, but a much lower accuracy on long-range dependencies.",4.2 Overall Results,[0],[0]
"The RNN-bideep model achieves distinctly better BLEU scores and a higher accuracy on long-range dependencies.
",4.2 Overall Results,[0],[0]
"2http://www.statmt.org/wmt17/ translation-task.html
3We report average accuracy on instances where the distance between subject and verb is longer than 10 words.
",4.2 Overall Results,[0],[0]
"However, it still cannot outperform Transformers on any of the tasks.
",4.2 Overall Results,[0],[0]
Figure 2 shows the performance of different architectures on the subject-verb agreement task.,4.2 Overall Results,[0],[0]
"It is evident that Transformer, RNNS2S, and RNNbideep perform much better than ConvS2S on long-range dependencies.",4.2 Overall Results,[0],[0]
"However, Transformer, RNNS2S, and RNN-bideep are all robust over long distances.",4.2 Overall Results,[0],[0]
"Transformer outperforms RNN-bideep for distances 11-12, but RNN-bideep performs equally or better for distance 13 or higher.",4.2 Overall Results,[0],[0]
"Thus, we cannot conclude that Transformer models are particularly stronger than RNN models for long distances, despite achieving higher average accuracy on distances above 10.",4.2 Overall Results,[0],[0]
"Theoretically, the performance of CNNs will drop when the distance between the subject and the verb exceeds the local context size.",4.2.1 CNNs,[0],[0]
"However, ConvS2S is also clearly worse than RNNS2S for subject-verb agreement within the local context size.
",4.2.1 CNNs,[0],[0]
"In order to explore how the ability of ConvS2S to capture long-range dependencies depends on the local context size, we train additional systems,
varying the number of layers and kernel size.",4.2.1 CNNs,[0],[0]
Table 3 shows the performance of different ConvS2S models.,4.2.1 CNNs,[0],[0]
"Figure 3 displays the performance of two 8-layer CNNs with kernel size 3 and 7, a 6-layer CNN with kernel size 3, and RNNS2S. The results indicate that the accuracy increases when the local context size becomes larger, but the BLEU score does not.",4.2.1 CNNs,[0],[0]
"Moreover, ConvS2S is still not as good as RNNS2S for subject-verb agreement.
",4.2.1 CNNs,[0],[0]
"Regarding the explanation for the poor performance of ConvS2S, we identify the limited context size as a major problem.",4.2.1 CNNs,[0],[0]
"One assumption to explain the remaining difference is that, scale invariance of CNNs is relatively poor (Xu et al., 2014).",4.2.1 CNNs,[0],[0]
"Scale-invariance is important in NLP, where the distance between arguments is flexible, and current recurrent or attentional architectures are better suited to handle this variance.
",4.2.1 CNNs,[0],[0]
Our empirical results do not confirm the theoretical arguments in Gehring et al. (2017) that CNNs can capture long-range dependencies better with a shorter path.,4.2.1 CNNs,[0],[0]
The BLEU score does not correlate well with the targeted evaluation of long-range distance interactions.,4.2.1 CNNs,[0],[0]
"This is due to the locality
of BLEU, which only measures on the level of ngrams, but it may also indicate that there are other trade-offs between the modeling of different phenomena depending on hyperparameters.",4.2.1 CNNs,[0],[0]
"If we aim to get better performance on long-range dependencies, we can take this into account when optimizing hyperparameters.
4.2.2 RNNs vs. Transformer Even though Transformer achieves much better BLEU scores than RNNS2S and RNN-bideep, the accuracies of these architectures on long-range dependencies are close to each other in Figure 2.
",4.2.1 CNNs,[0],[0]
Our experimental result contrasts with the result from Tran et al. (2018).,4.2.1 CNNs,[0],[0]
"They find that Transformers perform worse than LSTMs on the subjectverb agreement task, especially when the distance between the subject and the verb becomes longer.",4.2.1 CNNs,[0],[0]
"We perform several experiments to analyze this discrepancy with Tran et al. (2018).
",4.2.1 CNNs,[0],[0]
"A first hypothesis is that this is caused by the amount of training data, since we used much larger datasets than Tran et al. (2018).",4.2.1 CNNs,[0],[0]
"We retrain all the models with a small amount of training data similar to the amount used by Tran et al. (2018), about 135K sentence pairs.",4.2.1 CNNs,[0],[0]
The other training settings are the same as in Section 4.1.,4.2.1 CNNs,[0],[0]
"We do not see the expected degradation of Transformer-s, compared to RNNS2S-s (see Figure 4).",4.2.1 CNNs,[0],[0]
"In Table 4, the performance of RNNS2S-s and Transformer-s is similar, including the BLEU scores on newstest2014, newstest2017, the perplexity on the validation set, and the accuracy on the long-range dependencies.
",4.2.1 CNNs,[0],[0]
A second hypothesis is that the experimental settings lead to the different results.,4.2.1 CNNs,[0],[0]
"In order to investigate this, we do not only use a small training set,
but also replicate the experimental settings of Tran et al. (2018).",4.2.1 CNNs,[0],[0]
"The main changes are neural network layers (8→4); embedding size (512→128); multihead size (8→2); dropout rate (0.1→0.2); checkpoint save frequency (4,000→1,000), and initial learning rate (0.0002→0.001).
",4.2.1 CNNs,[0],[0]
"In the end, we get a result that is similar to Tran et al. (2018).",4.2.1 CNNs,[0],[0]
"In Figure 5, Transformer-re-h2 performs clearly worse than RNNS2S-re on longrange dependencies.",4.2.1 CNNs,[0],[0]
"By increasing the number of heads in multi-head attention, subject-verb accuracy over long distances can be improved substantially, even though it remains below that of RNNS2S-re.",4.2.1 CNNs,[0],[0]
"Also, the effect on BLEU is small.
",4.2.1 CNNs,[0],[0]
"Our results suggest that the importance of multihead attention with a large number of heads is larger than BLEU would suggest, especially for the modeling of long-distance phenomena, since multi-head attention provides a way for the model
to attend to both local and distant context, whereas distant context may be overshadowed by local context in an attention mechanism with a single or few heads.
",4.2.1 CNNs,[0],[0]
"Although our study is not a replication of Tran et al. (2018), who work on a different task and a different test set, our results do suggest an alternative interpretation of their findings, namely that the poor performance of the Transformer in their experiments is due to hyperparameter choice.",4.2.1 CNNs,[0],[0]
"Rather than concluding that RNNs are superior to Transformers for the modeling of long-range dependency phenomena, we find that the number of heads in multi-head attention affects the ability of Transformers to model long-range dependencies in subject-verb agreement.",4.2.1 CNNs,[0],[0]
"Our experimental results on the subject-verb agreement task demonstrate that CNNs and Transformer are not better at capturing long-range dependencies compared to RNNs, even though the paths in CNNs and Transformers are shorter.",5 WSD,[0],[0]
This finding is not in accord with the theoretical argument in both Gehring et al. (2017) and Vaswani et al. (2017).,5 WSD,[0],[0]
"However, these architectures perform well empirically according to BLEU.",5 WSD,[0],[0]
"Thus, we further evaluate these architectures on WSD, to test our hypothesis that non-recurrent architectures are better at extracting semantic features.",5 WSD,[0],[0]
We evaluate all architectures on ContraWSD on both DE→EN and DE→FR.,5.1 Experimental settings,[0],[0]
"We reuse the parameter settings in Section 4.1, except that: the initial learning rate of ConvS2S is reduced from 0.0003 to 0.0002 in DE→EN; the checkpoint saving frequency is changed from 4,000 to 1,000 in DE→FR because of the training data size.
",5.1 Experimental settings,[0],[0]
"For DE→EN, the training set, validation set, and test set are the same as the other direction EN→DE.",5.1 Experimental settings,[0],[0]
"For DE→FR, we use around 2.1 million sentence pairs from Europarl (v7) (Tiedemann, 2012)4 and News Commentary (v11) cleaned by Rios et al. (2017)5 as our training set.",5.1 Experimental settings,[0],[0]
"We use newstest2013 as the evaluation set, and use newstest2012 as the test set.",5.1 Experimental settings,[0],[0]
"All the data is preprocessed with Moses scripts.
",5.1 Experimental settings,[0],[0]
"4http://opus.nlpl.eu/Europarl.php 5http://data.statmt.org/ContraWSD/
",5.1 Experimental settings,[0],[0]
"In addition, we also compare to the best result reported for DE→EN, achieved by uedin-wmt17 (Sennrich et al., 2017), which is an ensemble of 4 different models and reranked with right-to-left models.6 uedin-wmt17 is based on the bi-deep RNNs (Miceli Barone et al., 2017) that we mentioned before.",5.1 Experimental settings,[0],[0]
"To the original 5.9 million sentence pairs in the training set, they add 10 million synthetic pairs with back-translation.",5.1 Experimental settings,[0],[0]
"Table 5 gives the performance of all the architectures, including the perplexity on validation sets, the BLEU scores on newstest, and the accuracy on ContraWSD.",5.2 Overall Results,[0],[0]
Transformers distinctly outperform RNNS2S and ConvS2S models on DE→EN and DE→FR.,5.2 Overall Results,[0],[0]
"Moreover, the Transformer model on DE→EN also achieves higher accuracy than uedin-wmt17, although the BLEU score on newstest2017 is 1.4 lower than uedin-wmt17.",5.2 Overall Results,[0],[0]
"We attribute this discrepancy between BLEU and WSD performance to the use of synthetic news training data in uedin-wmt17, which causes a large boost in BLEU due to better domain adaptation to newstest, but which is less helpful for ContraWSD, whose test set is drawn from a variety of domains.
",5.2 Overall Results,[0],[0]
"For DE→EN, RNNS2S and ConvS2S have the same BLEU score on newstest2014, ConvS2S has a higher score on newstest2017.",5.2 Overall Results,[0],[0]
"However, the WSD accuracy of ConvS2S is 1.7% lower than RNNS2S. For DE→FR, ConvS2S achieves slightly better results on both BLEU scores and accuracy than RNNS2S.
The Transformer model strongly outperforms the other architectures on this WSD task, with a gap of 4–8 percentage points.",5.2 Overall Results,[0],[0]
"This affirms our hypothesis that Transformers are strong semantic features extractors.
",5.2 Overall Results,[0],[0]
6https://github.com/a-rios/ContraWSD/ tree/master/baselines,5.2 Overall Results,[0],[0]
"In recent work, Chen et al. (2018) find that hybrid architectures with a Transformer encoder and an RNN decoder can outperform a pure Transformer model.",5.3 Hybrid Encoder-Decoder Model,[0],[0]
"They speculate that the Transformer encoder is better at encoding or extracting features than the RNN encoder, whereas the RNN is better at conditional language modeling.
",5.3 Hybrid Encoder-Decoder Model,[0],[0]
"For WSD, it is unclear whether the most important component is the encoder, the decoder, or both.",5.3 Hybrid Encoder-Decoder Model,[0],[0]
"Following the hypothesis that Transformer encoders excel as semantic feature extractors, we train a hybrid encoder-decoder model (TransRNN) with a Transformer encoder and an RNN decoder.
",5.3 Hybrid Encoder-Decoder Model,[0],[0]
"The results (in Table 5) show that TransRNN performs better than RNNS2S, but worse than the pure Transformer, both in terms of BLEU and WSD accuracy.",5.3 Hybrid Encoder-Decoder Model,[0],[0]
"This indicates that WSD is not only done in the encoder, but that the decoder also affects WSD performance.",5.3 Hybrid Encoder-Decoder Model,[0],[0]
"We note that Chen et al. (2018); Domhan (2018) introduce the techniques in Transformers into RNN-based models, with reportedly higher BLEU.",5.3 Hybrid Encoder-Decoder Model,[0],[0]
"Thus, it would be interesting to see if the same result holds true with their architectures.",5.3 Hybrid Encoder-Decoder Model,[0],[0]
"In this paper, we evaluate three popular NMT architectures, RNNS2S, ConvS2S, and Transformers, on subject-verb agreement and WSD by scoring contrastive translation pairs.
",6 Conclusion,[0],[0]
We test the theoretical claims that shorter path lengths make models better capture long-range dependencies.,6 Conclusion,[0],[0]
"Our experimental results show that:
• There is no evidence that CNNs and Transformers, which have shorter paths through networks, are empirically superior to RNNs in modeling subject-verb agreement over long distances.
",6 Conclusion,[0],[0]
"• The number of heads in multi-head attention affects the ability of a Transformer to model long-range dependencies in the subject-verb agreement task.
",6 Conclusion,[0],[0]
"• Transformer models excel at another task, WSD, compared to the CNN and RNN architectures we tested.
",6 Conclusion,[0],[0]
"Lastly, our findings suggest that assessing the performance of NMT architectures means finding their inherent trade-offs, rather than simply computing their overall BLEU score.",6 Conclusion,[0],[0]
A clear understanding of those strengths and weaknesses is important to guide further work.,6 Conclusion,[0],[0]
"Specifically, given the idiosyncratic limitations of recurrent and selfattentional models, combining them is an exciting line of research.",6 Conclusion,[0],[0]
"The apparent weakness of CNN architectures on long-distance phenomena is also a problem worth tackling, and we can find inspiration from related work in computer vision (Xu et al., 2014).",6 Conclusion,[0],[0]
We thank all the anonymous reviews and Joakim Nivre who give a lot of valuable and insightful comments.,Acknowledgments,[0],[0]
We appreciate the grants provided by Erasmus+ Programme and Anna Maria Lundin’s scholarship committee.,Acknowledgments,[0],[0]
GT is funded by the Chinese Scholarship Council (grant number 201607110016).,Acknowledgments,[0],[0]
"MM, AR and RS have received funding from the Swiss National Science Foundation (grant number 105212 169888).",Acknowledgments,[0],[0]
"Recently, non-recurrent architectures (convolutional, self-attentional) have outperformed RNNs in neural machine translation.",abstractText,[0],[0]
"CNNs and self-attentional networks can connect distant words via shorter network paths than RNNs, and it has been speculated that this improves their ability to model long-range dependencies.",abstractText,[0],[0]
"However, this theoretical argument has not been tested empirically, nor have alternative explanations for their strong performance been explored in-depth.",abstractText,[0],[0]
"We hypothesize that the strong performance of CNNs and self-attentional networks could also be due to their ability to extract semantic features from the source text, and we evaluate RNNs, CNNs and self-attention networks on two tasks: subject-verb agreement (where capturing long-range dependencies is required) and word sense disambiguation (where semantic feature extraction is required).",abstractText,[0],[0]
Our experimental results show that: 1) self-attentional networks and CNNs do not outperform RNNs in modeling subject-verb agreement over long distances; 2) self-attentional networks perform distinctly better than RNNs and CNNs on word sense disambiguation.,abstractText,[0],[0]
Why Self-Attention? A Targeted Evaluation of Neural Machine Translation Architectures,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4405–4414 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
4405",text,[0],[0]
"Vulgarity is a common element of conversation (Jay, 2009; Mehl et al., 2007) and is used even more frequently in social networks such as Twitter (Wang et al., 2014).",1 Introduction,[0],[0]
Understanding the motivation behind the choice to be vulgar and the way in which vulgarity is manifested in naturally occurring environments is of interdisciplinary interest.,1 Introduction,[0],[0]
Pragmatic functions that dictate patterns of vulgarity usages may interact with speaker cultural background and demographics.,1 Introduction,[0],[0]
"This makes them appealing—and challenging—to model in NLP applications
Yet, to date, there has been no empirical study on the type of vulgar word usage.",1 Introduction,[0],[0]
"Research in linguistics and psychology has identified several
types of usage for vulgar words (Andersson and Trudgill, 1990; Pinker, 2007; Wang, 2013).",1 Introduction,[0],[0]
"These range from use as an intensifier for an opinion or emotion, to offend others, or simply as a way of speaking or to signal the level of (in)formality in a conversation (Pinker, 2007).",1 Introduction,[0],[0]
"Table 1 shows example tweets with the six general functions of vulgar word usage.
",1 Introduction,[1.0000000018992325],['Table 1 shows example tweets with the six general functions of vulgar word usage.']
"We notice that in one of the examples, the vulgar word ass is used to verbally abuse another user, while the same word can also be employed to emphasize a feeling (‘good ass day’) or to express an emotion (‘pain in the ass’).",1 Introduction,[1.0],"['We notice that in one of the examples, the vulgar word ass is used to verbally abuse another user, while the same word can also be employed to emphasize a feeling (‘good ass day’) or to express an emotion (‘pain in the ass’).']"
"Hence, explicitly modeling vulgar words use is expected to positively impact the performance of practical tasks such as hate speech detection or the way in which profanity filtering is performed.
",1 Introduction,[0],[0]
The goal of our study is to present a comprehensive and multi-faceted analysis of the types of vulgar word usage.,1 Introduction,[0],[0]
"To this end, this paper presents: 1.",1 Introduction,[0],[0]
"The first data set of written utterances that con-
tain vulgar words, where each vulgar word is labeled for one of six functions of use1 2.",1 Introduction,[1.0000000178430561],"['The first data set of written utterances that con- tain vulgar words, where each vulgar word is labeled for one of six functions of use1 2.']"
A quantitative analysis of vulgar word usage across different user demographic traits 3.,1 Introduction,[0],[0]
A machine learning approach to predicting one of six types of vulgar word usage from context 4.,1 Introduction,[0],[0]
"Experiments demonstrating that modeling the type of vulgar word usage in context can im1https://github.com/ericholgate/
VulgarFunctionsTwitter
prove predictive performance of the hate speech prediction task on a benchmark data set
Our novel data set contains 7,800 tweets with 8,524 vulgar word labels annotated for one of six functions by seven annotators.",1 Introduction,[0.9906538990902601],"['Experiments demonstrating that modeling the type of vulgar word usage in context can im VulgarFunctionsTwitter prove predictive performance of the hate speech prediction task on a benchmark data set Our novel data set contains 7,800 tweets with 8,524 vulgar word labels annotated for one of six functions by seven annotators.']"
We find that the way in which vulgarity is used interacts with user demographic variables such as age or political ideology.,1 Introduction,[0],[0]
We then build a model for predicting the usage type of each vulgar word in the tweet using the tweet context.,1 Introduction,[0],[0]
"Finally, we explicitly model the vulgar word usage type in the task of hate speech detection to discriminate between hate speech and tweets including profanity, demonstrating an improvement in predictive accuracy of 3.7 F1.",1 Introduction,[0],[0]
"This demonstrates that using insights into vulgar word usage developed in linguistics and psychology, we can achieve quantitative improvements on downstream NLP applications and inform the way models are built and tailored to the task.",1 Introduction,[0],[0]
Vulgar language and its uses and pragmatic functions have been studied in several linguistic and psychological studies and the phenomenon has many names.,2 Related Work,[0],[0]
"In this paper, we will use vulgarity, profanity, and swear/curse words interchangeably.
",2 Related Work,[0],[0]
"Vulgar words were found to be very versatile, with a vulgar word being able to perform different interpersonal functions according to different contexts.",2 Related Work,[0],[0]
"Four types of usage are identified in Andersson and Trudgill (1990), including abusive (intended to harm the hearer), expletive (used to express emotions; not directed towards others), humorous (looks like abusive swearing, but has the opposite function) and auxiliary (swearing as a way of speaking, often or always nonemphatic).",2 Related Work,[0],[0]
"The five functions of swear words suggested in Pinker (2007) are: dysphemistic (conveying negative sentiment), idiomatic (signaling informality or simply used as a manner of speaking), abusive (intending to offend or harm), emphatic (intending to stress a claim or intensify emotive content) and cathartic (communicating pain).",2 Related Work,[0],[0]
"Finally, Wang (2013) identifies four pragmatic roles for profanity with a considerable degree of overlap with Pinker: emoting, emphasizing, aggressing, and group identity signaling.
",2 Related Work,[0],[0]
"For the scope of this study, we aim to cover all the functions identified by past research that can be identified from text with restricted content and context such as tweets.",2 Related Work,[0],[0]
"Thus, we dropped the
cathartic function from Pinker (2007), which is an instantaneous reaction more specific to speech to relieve the effect of physical pain (Stephens et al., 2009).",2 Related Work,[0],[0]
This would thus be very rarely – if ever – expressed through social media and would be very hard to annotate with textual content alone while lacking the broader context of its utterance.,2 Related Work,[0],[0]
"We also considered the abuse and aggression functions as equivalent across categorizations, as they imply a face-threatening act (Brown and Levinson, 1987).",2 Related Work,[0],[0]
"We considered the auxiliary and idiomatic categories as equivalent across classes, but maintained signaling group identity as in Wang (2013).",2 Related Work,[0],[0]
"We also created a non-vulgar classification in case the vulgar word is used in a non-vulgar context (e.g., a name that doubles as a vulgar term).
",2 Related Work,[0],[0]
"Due to their affective impact, vulgar words are often used as strange synonyms by substituting each other in context or idioms, even when they have no affinity in syntax or meaning (Quang, 1971; Pinker, 2007) (e.g., for God’s sake – for fuck’s sake; ‘I don’t give a damn/fuck/shit).",2 Related Work,[0],[0]
This heavily contributes to vulgar word volatility across different functions and higher ambiguity in context.,2 Related Work,[0],[0]
"However, this type of usage can allow computational approaches that model the immediate context around a word to generalize across words to functions.",2 Related Work,[0],[0]
"To date, there has been no research on quantitatively modeling the function of vulgar words in context.
",2 Related Work,[0],[0]
The overall frequency of usage of vulgar words has been quantitatively studied in social media and online communities.,2 Related Work,[0],[0]
"For example, Wang et al. (2014) estimates that vulgar posts constitute upwards of 1.15% of tweets and examines vulgar token frequency and how it varies with time, geolocation, and gender.",2 Related Work,[0],[0]
"An analysis of profanity across gender and age also appears in Gauthier et al. (2015).
",2 Related Work,[0],[0]
"In fact, gender is the most studied sociodemographic factor in relation to the use of profanity.",2 Related Work,[0],[0]
"Many studies have shown that maleidentifying users employ vulgar terms more frequently than female-identifying users (e.g. Selnow, 1985; Wang et al., 2014).
",2 Related Work,[0],[0]
"Jay and Janschewitz (2008) demonstrate that profanity is moderated by pragmatic or contextual factors that go beyond gender, including occupation, social status, and even the nature of the relationship between interlocutors – though this last point, proves difficult to explore via Twitter where
the identity of the audience is at least partially obfuscated.",2 Related Work,[0],[0]
"Other social factors such as age, religiosity or social status have also been shown to vary with vulgar frequency (McEnery, 2004), as has political ideology (Sylwester and Purver, 2015; Preoţiuc-Pietro et al., 2017).",2 Related Work,[0],[0]
"It is thus likely that sociodemographic factors also influence the functions with which vulgar words are used.
",2 Related Work,[0],[0]
"This study expands the scope of this type of research, by going beyond simple frequency of usage to pragmatic functions of vulgar words and how they are used differently by different sociodemographic groups.",2 Related Work,[0],[0]
"We use social media as our data source as this contains a high level of expression of thoughts, opinions and emotions (Java et al., 2007; Kouloumpis et al., 2011) and represents a platform for observing written interactions and conversations between users (Ritter et al., 2010).
",3 Data,[0],[0]
Social media and Twitter in particular provide vast volumes of text which are more informal and less curated compared to other domains such as newswire.,3 Data,[0],[0]
An additional advantage of Twitter data is that it allows us to study the sociodemographic context.,3 Data,[0],[0]
"We use the corpus of tweets utilized to construct the Vulgar Twitter corpus introduced in prior work (Cachola et al., 2018).",3.1 Identifying Vulgar Tweets,[0],[0]
Every tweet in this corpus contains at least one vulgar term.,3.1 Identifying Vulgar Tweets,[0],[0]
We then annotate each instance of a vulgar token for type of use.,3.1 Identifying Vulgar Tweets,[0],[0]
"Note that we use the full dataset of 7,800 tweets which contains 1K more tweets2 than the released version of the Vulgar Twitter corpus.
",3.1 Identifying Vulgar Tweets,[0],[0]
The Vulgar Twitter corpus was constructed by identifying tweets containing vulgarity through use of the vulgarity lexicon available at www. noswearing.com.,3.1 Identifying Vulgar Tweets,[0.9965229468567496],['The Vulgar Twitter corpus was constructed by identifying tweets containing vulgarity through use of the vulgarity lexicon available at www.']
"A total of 82 tokens were removed from this list as they were deemed not to be unambiguously vulgar after manual inspection.3 Regular expressions were utilized to identify common intentional spelling variations (e.g.,
2These tweets are excluded due to low sentiment agreement in Cachola et al. (2018).
",3.1 Identifying Vulgar Tweets,[0],[0]
"3These terms were largely anatomical words or general verbs like penis, vagina, and blow, but some identity descriptors like gay, queer, and lesbian were also excluded after manual review of a large sampling of uses revealed they were not overwhelmingly employed as slurs.
vowel reduplication such as fuuuuuck or selfcensorship such as a$$ ).
",3.1 Identifying Vulgar Tweets,[0],[0]
"For the complete description of the composition and construction of the Vulgar Twitter corpus, we refer the interested reader to the original paper (Cachola et al., 2018).",3.1 Identifying Vulgar Tweets,[0.956377869506301],"['We extract parts of speech using the Twitter version of the Stanford POS tagger which demonstrated good results on tagging tweets and uses the finer grained Penn Treebank tagset (Derczynski et al., 2013).']"
The Vulgar Twitter corpus overlaps with PreoţiucPietro et al. (2017) which allows us to consider the relationship between sociodemographic features and vulgar functionality.,3.2 Data Sampling,[0],[0]
"The tweets are compiled from up to 3,200 most recent tweets (per Twitter Developer API) of 4,132 twitter users who provided sociodemographic information via self-report in an online survey (Preoţiuc-Pietro et al., 2017).",3.2 Data Sampling,[0],[0]
"This sociodemographic data has been utilized in our previous research Cachola et al. (2018); for a full description of its collection, we refer the reader to Preoţiuc-Pietro et al. (2017).",3.2 Data Sampling,[0],[0]
"Demographic information (including gender, age, level of education, level of annual income, faith, an political ideology) was was self-reported via online survey.",3.3 Demographic Variables and Coding,[1.0],"['Demographic information (including gender, age, level of education, level of annual income, faith, an political ideology) was was self-reported via online survey.']"
"To control for cultural variation, data was only solicited from residents of the United States.",3.3 Demographic Variables and Coding,[0],[0]
"All demographic variables are ordinal with the exception of gender, which is binary.
",3.3 Demographic Variables and Coding,[0],[0]
"• Gender: a binary4 variable (Female as 1; Male as 0).
",3.3 Demographic Variables and Coding,[0],[0]
"• Age: an ordinal, integer-valued variable",3.3 Demographic Variables and Coding,[0],[0]
"[13- 90].
",3.3 Demographic Variables and Coding,[0],[0]
"• Income: An ordinal variable [1-8]; the lowest level (1) stands for ‘< $20,000’ and the highest (8) stands for ‘> $200,000’.
",3.3 Demographic Variables and Coding,[0],[0]
• Education: An ordinal variable,3.3 Demographic Variables and Coding,[0],[0]
"[1-6]; the lowest level (1) stands for ‘no high school degree’ and the highest (6) stands for ‘Advanced Degree (e.g., Ph.D.)’.
",3.3 Demographic Variables and Coding,[0],[0]
"• Political ideology: an ordinal variable on the liberal-conservative spectrum (a common form of representation of US political ideology (Ellis and Stimson, 2012)).",3.3 Demographic Variables and Coding,[0],[0]
"Reporting options ranged from ‘Very Conservative’
4Users were asked to identify their gender as Male, Female, or via an open field.",3.3 Demographic Variables and Coding,[0],[0]
"Users who did not respond as either Female or Male were excluded from data collection as there was not a sufficient population to be confident that data would be representative.
",3.3 Demographic Variables and Coding,[0],[0]
(1) to ‘Moderate’ (4) to ’Very Liberal’ (7).,3.3 Demographic Variables and Coding,[0],[0]
"Two additional responses, ‘Other’ (8) and ‘Apathetic’(9) were included to cover the full breadth of the ideological spectrum, but users selecting these options were excluded from our dataset (1,290 in total) in order to maintain an ordinal scale.
",3.3 Demographic Variables and Coding,[0],[0]
• Faith: an ordinal variable,3.3 Demographic Variables and Coding,[0],[0]
[1-6]; users were asked to report the average number of religious services attended.,3.3 Demographic Variables and Coding,[0],[0]
Available responses ranged from ‘Never’ (1) to ‘Multiple times per week’ (6).,3.3 Demographic Variables and Coding,[1.0],['Available responses ranged from ‘Never’ (1) to ‘Multiple times per week’ (6).']
We follow the same preprocessing procedure as in Cachola et al. (2018).,3.4 Data Processing,[0],[0]
URL’s and usernames are replaced by<URL> and<USER> tokens respectively to protect user privacy.,3.4 Data Processing,[0],[0]
Punctuation is then removed and all words are lowercased.,3.4 Data Processing,[0],[0]
"We have collected annotations via Amazon Mechanical Turk (MTurk) for vulgar word usage type for 8,524 instances of vulgar words across the 7,800 tweets present in the Vulgar Twitter corpus (Cachola et al., 2018).
",3.5 Annotation,[0],[0]
The task guidelines follow previous research from linguistics and psychology described in Section 2.,3.5 Annotation,[0],[0]
"For generality, we use a union of the different classes proposed and grouped classes where it was possible.",3.5 Annotation,[0],[0]
"The final guidelines include six dif-
ferent functions of vulgar words described in Table 2.
",3.5 Annotation,[0],[0]
"For quality control, we asserted the following qualifications on MTurk: locale=US, approval rate >90%, number of HITs approved >100.",3.5 Annotation,[0],[0]
"Further, we removed all ratings from users that have a Cohen’s Kappa of lower than 0.2 when compared to the majority rating of the other six annotations resulting in the removal of 8,430 ratings (14% of the total number) from 150 out of 663 users.",3.5 Annotation,[0],[0]
"These users were banned and annotations were recollected until 7 ratings were obtained for all instances.
",3.5 Annotation,[0],[0]
We measured inter-annotator agreement using Krippendorf’s Alpha as this can handle cases where each item was labeled by different groups of users.,3.5 Annotation,[0],[0]
The overall Krippendorfs Alpha is 0.506 despite there being a large number of classes (6).,3.5 Annotation,[0],[0]
"This alpha value (0.506) is regarded as a moderate level of agreement (Artstein and Poesio, 2008).",3.5 Annotation,[1.0],"['This alpha value (0.506) is regarded as a moderate level of agreement (Artstein and Poesio, 2008).']"
"To reduce uncertainty, we aggregate our labels across seven different annotators.",3.5 Annotation,[0],[0]
"In cases where no majority class emerged from the seven annotations (10.6% of the instances), the tie was broken by one of the authors of the paper, who have significant training and experience in linguistic annotation.
",3.5 Annotation,[1.0000000076179816],"['In cases where no majority class emerged from the seven annotations (10.6% of the instances), the tie was broken by one of the authors of the paper, who have significant training and experience in linguistic annotation.']"
The distribution of the final vulgar word type is presented in the last column from Table 2.,3.5 Annotation,[0],[0]
Table 3 shows the confusion matrix between aggregated function (row) and individual annotations (column); each cell is normalized by the row sum.,3.5 Annotation,[0],[0]
"Some patterns in disagreements include: (1) Vulgar words used to signal group identity are sometimes confused with aggression as annotating these may require additional social context about the user (e.g. if they are female, AfricanAmerican, etc.) or about social relationships (e.g. “lmao yeah cause a bitch can’t sing”).",3.5 Annotation,[0],[0]
"(2) Emotion confused with auxiliary usage in idioms or where there is a lack on context about what is
the author’s intent or target (e.g. “Stop cryin..",3.5 Annotation,[0],[0]
Damn you got the foul”).,3.5 Annotation,[0],[0]
"(3) Auxiliary use of vulgar words in an emotional tweet (e.g, “ok knicks.",3.5 Annotation,[0],[0]
we’re winning.,3.5 Annotation,[0],[0]
dont fuck it up.”).,3.5 Annotation,[0],[0]
(4) Short tweets lacking context drive confusion about the target of vulgarity or if an emotion is expressed (e.g. “Fuck yeah”).,3.5 Annotation,[0],[0]
We start with a quantitative analysis of our data.,4 Analysis,[1.0],['We start with a quantitative analysis of our data.']
"First, we examine the extent to which the same vulgar word is used for different functions.",4 Analysis,[0],[0]
"Then, we identify if sociodemographic factors impact the functions with which vulgar words are used.",4 Analysis,[0],[0]
"To quantitatively measure which vulgar words are most used with different functions, we first compute its distribution over the six functions in our entire data set.",4.1 Vulgar Word Analysis,[0],[0]
"Then, we compute the entropy as a measure of how evenly distributed the distribution over functions of each word is.",4.1 Vulgar Word Analysis,[0],[0]
"To avoid uncertainly associated computing statistics over distributions with low counts, we keep only words that appear more than 10 times in our data set (43 words) after collapsing variants of the same word (e.g. fuck – f*ck – fuuuck).
",4.1 Vulgar Word Analysis,[0],[0]
"The average entropy of all vulgar words is µ = 0.835 (σ = 0.36), with 0 being the minimum entropy (i.e., all words are used with one function) and 1.791 being the maximum entropy (i.e., all words are used with the same frequency with all six functions).",4.1 Vulgar Word Analysis,[0],[0]
"The words with the highest entropy are presented in Table 4.
",4.1 Vulgar Word Analysis,[0],[0]
"The table shows that four of the most frequent five words are in the top ten words by entropy, with all of them having significant numbers of oc-
currences in at least three vulgar functions.",4.1 Vulgar Word Analysis,[0],[0]
"Actually, the average entropy of words used at least 100 times in our data set (15 words) is 0.930 compared to 0.835 for words used at least 10 times.
",4.1 Vulgar Word Analysis,[0],[0]
We see that all words in the table are used significantly with three or more functions.,4.1 Vulgar Word Analysis,[0],[0]
"On average, in the entire data set, each word is used at least once with µ = 4.00 functions (σ = 1.34).
",4.1 Vulgar Word Analysis,[0],[0]
"This highlights both the challenges in modeling vulgar word functions and the opportunity of using the function to improve practical applications.
",4.1 Vulgar Word Analysis,[0],[0]
"In contrast, Table 5 shows the vulgar words which are most likely to be used with each of the six functions.",4.1 Vulgar Word Analysis,[0],[0]
Sociodemographic factors may impact the distribution with which each function of vulgar words is used.,4.2 Demographic Analysis,[0],[0]
"To measure this, we compute for each user a normalized distribution over the functions of vulgar words used in our data set.",4.2 Demographic Analysis,[0],[0]
"Then, we compute Pearson correlation where the dependent variable is the fraction of each vulgar word function and the independent variables are the user sociodemographic trait values.",4.2 Demographic Analysis,[0],[0]
"Following previous work (Schwartz and et al., 2013; Preoţiuc-Pietro et al., 2017), for all analyses we consider gender and age basic traits and control for potential data skew by introducing both variables as controls in partial correlation.",4.2 Demographic Analysis,[0],[0]
"When studying age and gender, we use the other trait as the control.",4.2 Demographic Analysis,[0],[0]
"Since we are running 36 tests at once without pre-stated hypotheses, we correct the correlations for multiple comparisons using Bonferroni correction.",4.2 Demographic Analysis,[0],[0]
"Results of these analyses are presented in Table 6.
",4.2 Demographic Analysis,[0],[0]
The results show several vulgar word functions are specific of age.,4.2 Demographic Analysis,[0],[0]
Younger users of Twitter are more likely to use vulgar words to signal group identity and to express emotion.,4.2 Demographic Analysis,[1.0],['Younger users of Twitter are more likely to use vulgar words to signal group identity and to express emotion.']
Older age is more likely to be related to use of words that are vulgar with non-vulgar functions.,4.2 Demographic Analysis,[0],[0]
"These correlations show that there are differences in how younger generations are using vulgar words, even if tweets were posted in the same time interval, signaling a diachronic change in usage.
",4.2 Demographic Analysis,[0],[0]
The analysis shows that the only significant correlation with the other five demographic variables is between both political ideology and faith and using vulgar words for emphasis and in non-vulgar functions.,4.2 Demographic Analysis,[0],[0]
Liberals are more likely to use vulgar words for emphasis and less likely to use them with non-vulgar functions.,4.2 Demographic Analysis,[0],[0]
"Previous research
showed that liberals are more likely to use more vulgarity overall in social media (Sylwester and Purver, 2015; Preoţiuc-Pietro et al., 2017) and are perceived by others to use more frequently than they do vulgar words (Carpenter et al., 2016), but this analysis shows this is especially due to vulgar word use to emphasise.",4.2 Demographic Analysis,[0],[0]
"The results are reversed for faith, which is known to be strongly correlated to conservative political ideology.",4.2 Demographic Analysis,[0],[0]
"Controlling for faith and political ideology with partial correlation does not alter the significance of this result.
",4.2 Demographic Analysis,[0],[0]
"Intriguingly, all other traits (gender, education and income) are not significantly correlated with an increased usage in any of the functions.
",4.2 Demographic Analysis,[0],[0]
"The vulgar word functions of aggression and auxiliary usage, which are more standard and traditional usages of vulgar words, do not show any significant differences with any sociodemographic trait.",4.2 Demographic Analysis,[0],[0]
The previous section showed that the same vulgar words can be used with several different functions.,5 Modeling Vulgar Word Use,[0],[0]
"In this section, we use machine learning approaches to explicitly predict the function of a vulgar word given the tweet it appears in as context.",5 Modeling Vulgar Word Use,[0],[0]
We use logistic regression5 to build six one vs. all binary classifiers for each of the six functions using information from the immediate lexical and syntactic context surrounding the word and general usage of the word in training data.,5.1 Method,[0],[0]
"We use the following feature types in our experiments: Intention Distribution –We include six features encoding the distribution over intentional classes of the target word in training data, as some words use only several functions and some more predominantly than others.",5.2 Features,[0],[0]
Tweet Content –We derive a tweet-level representation of the entire content of the tweet by averaging vector representations of its constituent words.,5.2 Features,[0],[0]
"We utilize 200-dimensional GloVe embeddings pre-trained on 2B tweets (Pennington et al., 2014).",5.2 Features,[0],[0]
"Sentiment Content –We include two features which represent the number of positive and negative valence words in the tweet, normalized by tweet length.",5.2 Features,[1.0],"['Sentiment Content –We include two features which represent the number of positive and negative valence words in the tweet, normalized by tweet length.']"
"For this feature group, we utilize the opinion lexicon introduced in Hu and Liu (2004).",5.2 Features,[0],[0]
"Part of Speech Context –We encode the part of speech of the target word, the previous word and the next word as one-hot vectors as we expect syntactic information to be an indicator of different functions in context.",5.2 Features,[0],[0]
"We extract parts of speech using the Twitter version of the Stanford POS tagger which demonstrated good results on tagging tweets and uses the finer grained Penn Treebank tagset (Derczynski et al., 2013).",5.2 Features,[0],[0]
"Brown Clusters –Finally, we include two one-hot feature groups which indicate the Brown Cluster (Brown et al., 1992) membership of word immediately before and immediately after the vulgar term.
",5.2 Features,[0.9999999863911436],"['Brown Clusters –Finally, we include two one-hot feature groups which indicate the Brown Cluster (Brown et al., 1992) membership of word immediately before and immediately after the vulgar term.']"
"5In preliminary experiments, we attempted to utilize a BiLSTM to encode tweet context, but it did significantly worse than the logistic regression model, possibly due to many parameters and classes compared to the size of the training data.
",5.2 Features,[0],[0]
Brown Clusters are obtained by hierarchical clustering tokens based on contexts in which they immediately co-occur.,5.2 Features,[0],[0]
We use the precomputed cluster representations as seen in Turian et al. (2010).,5.2 Features,[0],[0]
"We also experimented with Twitter-specific clusters (Owoputi et al., 2012), but found they did not perform as well on our development set.
",5.2 Features,[0],[0]
"Additionally, we experimented with a personal pronoun indicator feature in a three word window around the target, a one-hot lexical feature encoding the target vulgar item, and NRC emotion scores (Mohammad and Turney, 2013), but found there to be no improvement in performance as a result.",5.2 Features,[0],[0]
We did not experiment with using the demographic variables as features as these are generally unavailable for use in predictive systems.,5.2 Features,[0],[0]
"We split our data into a training set of 6,883 tweets and a testing set of 1,087 tweets, and held out a set of 554 tweets as a validation set on which to test different hyperparameter settings.
",5.3 Experimental Results,[0],[0]
"Performance statistics for our predictive model are presented in Table 7, as well as ablation experiments for each feature group.
",5.3 Experimental Results,[0],[0]
"Our predictive model vastly outperforms the most frequent baseline, which uniformly selects the most frequent class overall (emphasis) and scores very low due to the very even distribution over functions.",5.3 Experimental Results,[1.0],"['Our predictive model vastly outperforms the most frequent baseline, which uniformly selects the most frequent class overall (emphasis) and scores very low due to the very even distribution over functions.']"
"Our best model achieves a macroaveraged F1 score of 67.4 across the six classes.
",5.3 Experimental Results,[0],[0]
"In the ablation experiments, we see by removing one feature group at a time, which feature type adds most predictive value over others.",5.3 Experimental Results,[0],[0]
Withholding the intention distribution feature group from the model shows the greatest loss in performance (12.1 macro F1).,5.3 Experimental Results,[0],[0]
"This is somewhat expected, as this feature gives a prior distribution over functions for the target word based on training data and, as most words are rarely used with some
functions, allows the model to downweigh them.",5.3 Experimental Results,[0],[0]
"However, even with no word function prior, the predictive performance is still relatively high (55.3 macro F1 across six classes), showing that only the content and context is substantially predictive for the function of a vulgar word
Removing tweet content features or part-ofspeech context introduce a similar drop in predictive performance, showing that the overall tweet content and the local syntactic context of the mention play complimentary roles in inference.",5.3 Experimental Results,[0],[0]
"The sentiment feature groups are the least informative, yielding an negligible increase in performance of only 0.1 macro F1.
",5.3 Experimental Results,[0],[0]
"The predictive model’s F1 scores by class is as follows: Aggression – 65.6, Emotion – 62.4, Emphasis – 76.4, Signal Group Identity – 56.5, Auxiliary – 62.2, Not Vulgar – 81.4.
",5.3 Experimental Results,[0],[0]
"The highest predictive performance is obtained for vulgar words used in a non vulgar context, which is due to the different tweet content of these tweets and the restricted sets of words which are usually used as non-vulgar.",5.3 Experimental Results,[0],[0]
"The emphasis functions is the second most accurately predictable using our model, due to the very distinctive syntactic patterns of usage of this function (usually as an adjective).",5.3 Experimental Results,[0],[0]
The least predictable function is signaling group identity.,5.3 Experimental Results,[0],[0]
We observed that this function is usually used as part of larger conversational context and often relies on a shared social context.,5.3 Experimental Results,[0],[0]
"Finally, we aim to show that modeling the function of vulgar words explicitly has practical implications by using this in a downstream application.",6 Hate Speech Prediction,[0],[0]
"Automatic hate-speech detection on social media is the task defined as generally identifying abusive speech targeting specific group characteristics, such as ethnic origin, religion, gender, or sexual orientation (Warner and Hirschberg, 2012) with a clear intention to incite harm, or to promote hatred (Zhang and Luo, 2018).",6.1 Task,[0],[0]
"Several data sets and approaches to automatic hate speech detection have been recently proposed (Djuric et al., 2015; Burnap and Williams, 2015; Waseem and Hovy, 2016; Nobata et al., 2016; Davidson et al., 2017).
",6.1 Task,[0],[0]
"The task of predicting hate-speech is challenging for natural language processing using lexical information as it aims to predict the intention of
the message and several words used in conveying hate speech can have other common uses (Davidson et al., 2017; Malmasi and Zampieri, 2018).
",6.1 Task,[0],[0]
"Hate speech is very often confused with offensive language, as highlighted in the error analyses of past hate speech detection papers (Davidson et al., 2017).",6.1 Task,[0],[0]
"Quantitative analysis of the machine learning models suggest that obscene words are very informative for both the hate speech and offensive classes of tweets (Malmasi and Zampieri, 2018), hinting that the functions of vulgar words usage are a major source of ambiguity.
",6.1 Task,[0],[0]
"Our hypothesis is that explicitly modeling the function a vulgar word has in context will benefit the hate speech prediction task, by differentiating between aggression and other usages.",6.1 Task,[0],[0]
Data.,6.2 Experiments,[0],[0]
"We use the dataset introduced in Davidson et al. (2017) as this is publicly available, contains tweets collected using vulgar words and explicitly differentiates between offensive tweets and tweets containing hate speech.",6.2 Experiments,[1.0],"['We use the dataset introduced in Davidson et al. (2017) as this is publicly available, contains tweets collected using vulgar words and explicitly differentiates between offensive tweets and tweets containing hate speech.']"
"The three classes in this data set are hate speech, offensive, and neither.
",6.2 Experiments,[0],[0]
"Another public dataset on tweets, introduced in Waseem and Hovy (2016), focuses on specific forms of hate speech (sexist and racist), but is collected with a restricted set of keywords, has low coverage of vulgar words and does not explicitly distinguish between hate speech and other offensive language.",6.2 Experiments,[0],[0]
"Other datasets for this task are not publicly available e.g., Nobata et al. (2016).",6.2 Experiments,[0],[0]
Setup.,6.2 Experiments,[0],[0]
"In order to directly measure the impact on predicting performance introduced by explicitly modeling the function of the vulgar word in the tweet, we follow the same methodology to identify hate speech as described in Davidson et al. (2017), as implemented through the openly available code provided by the authors.6 We thus train three one-vs-all logistic regression classifiers with L2 regularization as implemented in scikit-learn (Pedregosa et al., 2011).",6.2 Experiments,[0],[0]
"Features used in the model include unigram to trigram TFIDF weighted word features, Part-of-Speech unigram to trigrams, reading level, sentiment words, Twitter specific features (e.g., hashtags, mentions, retweets, and URLs) as well as generic tweet-level features (e.g., number of characters, words, and syllables in each tweet).
",6.2 Experiments,[0],[0]
"6https://github.com/t-davidson/ hate-speech-and-offensive-language
Vulgar Function Features.",6.2 Experiments,[0],[0]
We directly and explicitly include the function of the vulgar word present in the tweet by introducing six new features to the hate speech detection model which represent the scores with which the vulgar word is associated with the six functions.,6.2 Experiments,[0],[0]
"If multiple vulgar words exist in a tweet, we use the average predictions over the six functions.",6.2 Experiments,[0],[0]
Metrics.,6.2 Experiments,[0],[0]
We run the model from Davidson et al. (2017) using the provided code on 10-fold cross validation and report the average F1 score for each class as well as the macro-averaged F1 score across all ten folds.,6.2 Experiments,[1.0],['We run the model from Davidson et al. (2017) using the provided code on 10-fold cross validation and report the average F1 score for each class as well as the macro-averaged F1 score across all ten folds.']
"Using the available code, we could not reproduce exactly results presented in the Davidson et al. (2017) paper.",6.2 Experiments,[0],[0]
"For predicting the function of the vulgar words from context, we use our best predictive model described in Section 5.",6.2 Experiments,[0],[0]
We also re-scale our six function features by multiplying them with a large exponent in order to make them significant in model training.,6.2 Experiments,[0],[0]
Results.,6.2 Experiments,[0],[0]
"As presented in Table 8, the addition of the six vulgar function features improves the F1 score for each of three classes to up to 6.1 F1 for the hate speech class, which had the lowest performance.",6.2 Experiments,[0],[0]
This results in an improvement of the macro-F1 score for the entire classification task of 3.7 in F1.,6.2 Experiments,[0],[0]
This demonstrates the importance of the proposed vulgar function modeling task in detecting hate speech.,6.2 Experiments,[0],[0]
This paper presents the first empirical study on the pragmatic functions of vulgar words.,7 Conclusion,[0],[0]
"We created a novel, freely available data set of 7,800 vulgar tweets having 8,524 instances of vulgar words labeled for one of six functions by seven annotators and expert adjudication.",7 Conclusion,[0],[0]
"We quantitatively showed, leveraging research in linguistics and psychology, that vulgar words are frequently used with different functions and, in the first quantitative analysis on this topic, uncovered that vulgar words are used with different functions by younger users to signal group identity and for expressing emotions.
",7 Conclusion,[0],[0]
"We have built the first machine learning model
for predicting vulgar word function from context, achieving a performance of 67.4 macro F1, demonstrating the practical feasibility of this task.",7 Conclusion,[0],[0]
"We showed the usefulness of this task, by integrating predicted vulgar word function in the downstream task of hate speech detection, achieving an improvement of 3.7 in F1 on a benchmark data set.
",7 Conclusion,[0],[0]
This study showed that modeling pragmatic function is of practical importance.,7 Conclusion,[0],[0]
"Future work will use this linguistic information to inform more complex machine learning models, e.g., deep neural networks, in an attempt to increase predictive gains.",7 Conclusion,[0],[0]
"As two of the most used functions of vulgar words relate to expressing sentiment or emotions, we will also explore collecting sentiment annotations for joint sentiment and vulgar word function inference and use this to improve the task of sentiment analysis using multi-task methods.",7 Conclusion,[0],[0]
"Vulgar words are employed in language use for several different functions, ranging from expressing aggression to signaling group identity or the informality of the communication.",abstractText,[0],[0]
This versatility of usage of a restricted set of words is challenging for downstream applications and has yet to be studied quantitatively or using natural language processing techniques.,abstractText,[0],[0]
"We introduce a novel data set of 7,800 tweets from users with known demographic traits where all instances of vulgar words are annotated with one of the six categories of vulgar word use.",abstractText,[0],[0]
"Using this data set, we present the first analysis of the pragmatic aspects of vulgarity and how they relate to social factors.",abstractText,[0],[0]
We build a model able to predict the category of a vulgar word based on the immediate context it appears in with 67.4 macro F1 across six classes.,abstractText,[0],[0]
"Finally, we demonstrate the utility of modeling the type of vulgar word use in context by using this information to achieve state-of-the-art performance in hate speech detection on a benchmark data set.",abstractText,[0],[0]
Why Swear? Analyzing and Inferring the Intentions of Vulgar Expressions,title,[0],[0]
