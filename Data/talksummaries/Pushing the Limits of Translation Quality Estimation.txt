0	40	The goal of quality estimation (QE) is to evaluate a translation system’s quality without access to reference translations (Blatz et al., 2004; Specia et al., 2013).
1	90	This has many potential usages: informing an end user about the reliability of translated content; deciding if a translation is ready for publishing or if it requires human post-editing; highlighting the words that need to be changed.
2	32	QE systems are particularly appealing for crowd-sourced and professional translation services, due to their potential to dramatically reduce post-editing times and to save labor costs (Specia, 2011).
3	27	The increasing interest in this problem from an industrial angle comes as no surprise (Turchi et al., 2014; de Souza et al., 2015; Martins et al., 2016; Kozlova et al., 2016).
4	95	In this paper, we tackle word-level QE, whose goal is to assign a label of OK or BAD to each word in the translation (Figure 1).
6	12	We start by proposing a “pure” QE system (§3) consisting of a new, carefully engineered neural model (NEURALQE), stacked into a linear feature-rich classifier (LINEARQE).
44	15	It is a pure but rather complex QE system, ensembling a linear feature-based classifier with three different neural networks with different configurations.
52	16	tN is the translated sentence, and A ⊆ {(m,n) | 1 ≤ m ≤ M, 1 ≤ n ≤ N} is a set of word alignments.
56	17	(1) Above, w is a vector of weights, φu(s, t,A, yi) are unigram features (depending only on a single output label), φb(s, t,A, yi, yi−1) are bigram features (depending on consecutive output labels), and y0 and yN+1 are special start/stop symbols.
58	14	Like the baseline systems provided in WMT15/16, we include features that depend on the target word and its aligned source word, as well as the context surrounding them.3 A distinctive aspect of our system is the inclusion of syntactic features, which will show to be useful to detect grammatically incorrect constructions.4 We use features that involve the dependency relation, the head word, and secondorder sibling and grandparent structures.
68	34	The impact of these features is more prominent in WMT16: the rich bigram features lead to scores about 3 points above a sequential model with a single indicator bigram feature, and the syntactic features contribute another 2.5 points.
76	13	The system receives as input the source and target sentences s and t, their word-level alignments A, and their corresponding POS tags obtained from TurboTagger.
77	13	The input layer follows a similar architecture as QUETCH, with the addition of POS features.
78	41	A vector representing each target word is obtained by concatenating the embedding of that word with those of the aligned word in the source.6 The immediate left and right contexts for source and target words are also concatenated.
88	23	Two more feed-forward ReLU layers of sizes 100 and 50, respectively.
89	17	As the output layer, a softmax transformation over the OK/BAD labels is applied.
100	17	The use of recurrent layers yields the largest contribution to the performance of NEURALQE, as the scores drop sharply (by more than 4 points) if they are replaced by feed-forward layers (which would correspond to a mere deeper QUETCH model).
119	47	As unigram features, we used one real-valued feature for every model prediction at each position, conjoined with the label.
125	29	Now that we have described a pure QE system, we move on to an APE-based QE system (APEQE).
126	19	Our starting point is the system submitted by the Adam Mickiewicz University (AMU) team to the APE task of WMT16 (Junczys-Dowmunt and Grundkiewicz, 2016).
130	31	The penalty fires if the APE system proposes a word in its output that has not been seen in t. To overcome the problem of too little training data, Junczys-Dowmunt and Grundkiewicz (2016) generated large amounts of artificial data via roundtrip translations: a large corpus of monolingual sentences is first gathered for the target language in the domain of interest (each sentence is regarded as an artificial post-edited sentence p); then an MT system is ran to translate these sentences to the source language (which are regarded as the source sentences s), and another MT system in the reverse direction translates the latter back to the target language (playing the role of the translations t).
136	18	• For each of the four new training sets, we train one APE model on a concatenation of a smaller set of artificial data (denoted as “round-trip.n1” in Junczys-Dowmunt and Grundkiewicz (2016), consisting of 531,839 sentence triples) and a 20- fold oversampled new training set.
159	21	The procedure is analogous to that described in §3.3, with one extra binary feature for the APE-based word quality label predictions.
163	47	We can see that the APE-based and the pure QE systems are complementary: the full combination of the linear, neural, and APE-based systems improves the scores with respect to the best individual system (APEQE) by about 1 point in WMT15 and 2 points in WMT16.
167	233	In §6 we analyze the errors made by the pure and the APE-based QE systems to better understand how they complement each other.
168	91	Encouraged by the strong results obtained with the FULLSTACKEDQE system in word-level QE, we investigate how we can adapt this system for HTER prediction at sentence level.
169	19	Prior work (de Souza et al., 2014) incorporated word-level quality predictions as features in a sentence-level QE system, training a feature-based linear classifier.
174	76	Words that are not in the translated sentence but exist in the reference post-edited sentence do not originate BAD labels, and therefore will not contribute to the HTER estimate.
175	20	Yet, as we will see, this procedure applied to the STACKEDQE system (i.e. without the APEQE component) is already sufficient to obtain state of the art results.
177	18	Table 12 shows the results obtained with our pure QE system (STACKEDQE), with our APEbased system (APEQE), and with the combination of the two (FULLSTACKEDQE).
