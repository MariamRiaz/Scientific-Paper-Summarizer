{"sections": [{"heading": "1. Introduction", "text": "Probabilistic inference in complex models generally requires the evaluation of intractable, high-dimensional integrals. One powerful and generic approach to inference is to use Markov chain Monte Carlo (MCMC) methods to generate asymptotically exact (but correlated) samples from a posterior distribution for inference and learning. Hamiltonian Monte Carlo (HMC) (Duane et al., 1987; Neal, 2011) is a state-of-the-art MCMC method which uses gradient information from an absolutely continuous target density to encourage efficient sampling and exploration. Crucially, HMC utilizes proposals inspired by Hamiltonian dynamics (corresponding to the classical mechanics of a point particle) which can traverse long distances in parameter space. HMC, and variants like NUTS (which eliminates the need to hand-tune the algorithm\u2019s hyperparameters), have been successfully applied to a large class of probabilistic inference problems where they are often the gold standard for\n1UC Berkeley, USA 2University of Cambridge, UK 3Uber AI Labs, USA. Correspondence to: Nilesh Tripuraneni <nileshtrip@gmail.com>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\n(asymptotically) exact inference (Neal, 1996; Hoffman & Gelman, 2014; Carpenter et al., 2016).\nIn this paper, we first review important properties of Hamiltonian dynamics, namely energy-preservation, symplecticity, and time-reversibility, and derive a more general class of dynamics with these properties which we refer to as noncanonical Hamiltonian dynamics. We then discuss the relationship of non-canonical Hamiltonian dynamics to wellknown variants of HMC and propose a novel extension of HMC. We refer to this method as magnetic HMC (see Algorithm 1) since it corresponds to a particular subset of the non-canonical dynamics that in 3 dimensions map onto to the mechanics of a charged particle coupled to a magnetic field \u2013 see Figure 1 for an example of these dynamics. Furthermore, we construct an explicit, symplectic, leapfroglike integrator for magnetic HMC which allows for an efficient numerical integration scheme comparable to that of ordinary HMC. Finally, we evaluate the performance of magnetic HMC on several sampling problems where we show how its non-canonical dynamics can lead to improved mixing. The proofs of all results in this paper are presented in the corresponding sections of the Appendix."}, {"heading": "2. Markov chain Monte Carlo", "text": "Given an unnormalized target density \u03c1(\u03b8) defined on Rd, an MCMC algorithm constructs an ergodic Markov chain (\u0398n)n\u2208N such that the distribution of \u0398n converges to \u03c1 (e.g. in total variation) (Robert & Casella, 2004). Often, the transition kernel of such a Markov chain is specified by the Metropolis-Hastings (MH) algorithm which (i) given the\nar X\niv :1\n60 7.\n02 73\n8v 2\n[ st\nat .M\nL ]\n1 9\nA ug\n2 01\n7\ncurrent state \u0398n = \u03b8, proposes a new state \u03b8\u0303 by sampling from a proposal distribution Q(\u00b7|\u03b8), and (ii) sets \u0398n+1 = \u03b8\u0303 with probability min ( 1, \u03c1(\u03b8\u0303)Q(\u03b8|\u03b8\u0303)\n\u03c1(\u03b8)Q(\u03b8\u0303|\u03b8)\n) and \u0398n+1 = \u03b8 oth-\nerwise. The role of the acceptance step is to enforce reversibility (or detailed balance) of the Markov chain with respect to \u03c1 \u2013 which implies \u03c1 is a stationary distribution of the transition kernel.\nHeuristically, a good MH algorithm should have low intersample correlation while maintaining a high acceptance ratio. Hamiltonian Monte Carlo provides an elegant mechanism to do this by simulating a particle moving along the contour lines of a dynamical system, constructed from the target density, to use as a MCMC proposal."}, {"heading": "2.1. Hamiltonian Monte Carlo", "text": "In Hamiltonian Monte Carlo, the target distribution is augmented with \u201cmomentum\u201d variables p which are independent of the \u03b8 variables but of equal dimension. For the remainder of the paper, we take the distribution over the momentum variables to be Gaussian, as is common in the literature (indeed, there is evidence that in many cases, the choice of a Gaussian distribution may be optimal (Betancourt, 2017)). The joint target distribution is therefore:\n\u03c1(\u03b8,p) \u221d e\u2212U(\u03b8)\u2212p>p/2 \u2261 e\u2212H(\u03b8,p). (1)\nCrucially, this augmentation allows Hamiltonian dynamics to be used as a proposal for an MCMC algorithm over the space (\u03b8,p), where we interpret \u03b8 (resp., p) as position (resp., momentum) coordinates of a physical particle with total energy H(\u03b8,p), given by the sum of its potential energy U(\u03b8) and kinetic energy p>p/2. We briefly review the Markov chain construction below; see (Neal, 2011) or (Duane et al., 1987) for a more detailed description. Given the Markov chain state (\u03b8n,pn) at time n, the new state for time n + 1 is obtained by first resampling momentum pn \u223c N (0, I), and then proposing a new state according to the following steps: (i) Simulate the deterministic Hamiltonian flow defined by the differential equation\nd\ndt [ \u03b8(t) p(t) ] = [ 0 I \u2212I 0 ] \ufe38 \ufe37\ufe37 \ufe38\nA\n[ \u2207\u03b8H(p(t), \u03b8(t)) \u2207pH(p(t), \u03b8(t)) ]\n\u2261 [\np(t) \u2212\u2207\u03b8U(\u03b8(t))\n] . (2)\nfor time \u03c4 , with initial condition (\u03b8n,pn), to obtain (\u03b8\u2032n,p \u2032 n) = \u03a6\u03c4,H(\u03b8n,pn)\n1; (ii) Flip the resulting momentum component with the map \u03a6p(\u03b8,p) = (\u03b8,\u2212p) to ob-\n1Throughout this paper, we use \u03a6\u03c4,H to denote the map that takes a given position-momentum pair as initial conditions for the\ntain (\u03b8\u0303n+1, p\u0303n+1) = \u03a6p(\u03b8\u2032n,p \u2032 n) = \u03a6\u0303\u03c4,H(\u03b8n,pn); (iii) Apply a MH-type accept/reject step to enforce detailed balance with respect to the target distribution; (iv) Flip the momentum again with \u03a6p so it points in the original direction.\nNote that because the map \u03a6\u03c4,H is time-reversible (in the sense that if the path (\u03b8(t),p(t)) is a solution to (2) then the path with negated momentum traversed in reverse (\u03b8(\u2212t),\u2212p(\u2212t)) is also a solution), the map \u03a6\u0303\u03c4,H is selfinverse. From this, the acceptance ratio in step (iii) enforcing detailed balance can be shown (see e.g. (Green, 1995)) to have the form:\nmin ( 1,\nexp(\u2212H(\u03b8\u0303n+1, p\u0303n+1)) exp(\u2212H(\u03b8n,pn)) \u2223\u2223\u2223det\u2207\u03b8,p\u03a6\u0303\u03c4,H(\u03b8n,pn)\u2223\u2223\u2223) . (3) Note that the Hamiltonian flow & momentum flip operator \u03a6\u0303\u03c4,H is volume-preserving2, which immediately yields that the Jacobian term in the acceptance ratio (3) is simply 1. The acceptance probability therefore reduces to min(1, exp(H(\u03b8n,pn) \u2212 H(\u03b8\u0303n+1, p\u0303n+1))). Furthermore, since the Hamiltonian flow defined in (2) is energypreserving (i.e. H(\u03b8\u0303n+1, p\u0303n+1) = H(\u03b8n,pn)) \u2013 the acceptance ratio is identically 1. Moreover, the momentum resampling in (i) and momentum flip in (iv) both leave the joint distribution invariant.\nWhile the momentum resampling ensures the Markov chain explores the joint (\u03b8,p) space, the proposals inspired by Hamiltonian dynamics can traverse long distances in parameter space \u03b8, reducing the random-walk behavior of MH that often results in highly correlated samples (Neal, 2011)."}, {"heading": "2.2. Symplectic Numerical Integration", "text": "Unfortunately, it is rarely possible to integrate the flow defined in (2) analytically; instead an efficient numerical integration scheme must be used to generate a proposal for the MH-type accept/reject test. Typically, the leapfrog (Sto\u0308rmer-Verlet) integrator is used since it is an explicit method that is both symplectic and time-reversible (Neal, 2011). One elegant way to motivate this integrator is by decomposing the Hamiltonian into a symmetric splitting:\nH(\u03b8,p) = U(\u03b8)/2\ufe38 \ufe37\ufe37 \ufe38 H1(\u03b8) + p>p/2\ufe38 \ufe37\ufe37 \ufe38 H2(p) +U(\u03b8)/2\ufe38 \ufe37\ufe37 \ufe38 H1(\u03b8)\n(4)\nand then defining \u03a6 ,H1(\u03b8) and \u03a6 ,H2(p) to be the exactlyintegrated flows for the sub-Hamiltonians H1(\u03b8) and\nHamiltonian flow associated withH for time \u03c4 . In addition, \u03a6\u0303\u03c4,H denotes the composition of \u03a6\u03c4,H with the momentum flip map \u03a6p.\n2In fact the Hamiltonian flow satisfies the stronger condition of symplecticity with respect to the A matrix ([\u2207\u03b8,p\u03a6\u03c4,H(\u03b8,p)]>A\u22121[\u2207\u03b8,p\u03a6\u03c4,H(\u03b8,p)] = A\u22121) which immediately implies it is volume-preserving by taking determinants of this relation.\nH2(p), respectively. These updates (which are equivalent to Euler translations) can be written:\n\u03a6 ,H1(\u03b8) [ \u03b8 p ] = [ \u03b8\np\u2212 2\u2207\u03b8U(\u03b8) ] \u03a6 ,H2(p) [ \u03b8 p ] = [ \u03b8 + p p ] (5)\nsince the Hamilton equations (2) for the sub-Hamiltonians H1(\u03b8) and H2(p) are linear, and hence analytically integrable. One leapfrog step is then defined as:\n\u03a6frog ,H(\u03b8,p) = \u03a6 ,H1(\u03b8) \u25e6\u03a6 ,H2(p) \u25e6\u03a6 ,H1(\u03b8) (6)\nwith the overall proposal given by L leapfrog steps, followed by the momentum flip operator \u03a6p as before:\n\u03a6\u0303frogL, ,H = \u03a6p \u25e6 ( \u03a6frog ,H(\u03b8,p) )L .\nAs each of the flows \u03a6 ,H1(\u03b8), \u03a6 ,H2(p) exactly integrates a sub-Hamiltonian, they inherit the symplecticity, volumepreservation, and time-reversibility of the exact dynamics. Moreover, since the composition of symplectic flows is also symplectic and the splitting scheme is symmetric (implying the composition of time-reversible flows is also timereversible), the Jacobian term in the acceptance probability (3) is exactly 1 as in the case of perfect simulation.\nThe leapfrog scheme will not exactly preserve the Hamiltonian H , so the remaining acceptance ratio exp(H(\u03b8n,pn) \u2212 H(\u03b8\u0303n+1, p\u0303n+1)) must be calculated. However, the leapfrog integrator has error O( 3) in one leapfrog step (Hairer et al., 2006). This error scaling will lead to good energy conservation properties (and thus high acceptance rates in the MH step), even when simulating over long trajectories."}, {"heading": "3. Non-Canonical Hamiltonian Monte Carlo", "text": "In Section 2, we noted the role time-reversibility, volumepreservation, and energy conservation of canonical Hamiltonian dynamics play in making them useful candidates for MCMC. In this section, we develop the properties of a general class of flows we refer to as non-canonical Hamiltonian systems that parallel these properties, we use to construct our method magnetic HMC (see Algorithm 1):\nLemma 1. The map \u03a6A\u03c4,H(\u03b8,p) defined by integrating the non-canonical Hamiltonian system\nd\ndt [ \u03b8(t) p(t) ] = A\u2207\u03b8,pH(\u03b8(t),p(t)) (7)\nwith initial conditions (\u03b8,p) for time \u03c4 , where A \u2208 M2n\u00d72n is any invertible, antisymmetric matrix induces a flow on the coordinates (\u03b8,p) that is still energyconserving (\u2202\u03c4H(\u03a6A\u03c4,H(\u03b8,p)) = 0) and symplectic with\nrespect to A ([\u2207\u03b8,p\u03a6\u03c4,H(\u03b8,p)]>A\u22121[\u2207\u03b8,p\u03a6\u03c4,H(\u03b8,p)] = A\u22121) which also implies volume-preservation of the flow.\nWithin the formal construction of classical mechanics, it is known that any Hamiltonian flow defined on the cotangent bundle (\u03b8,p) of a configuration manifold, which is equipped with an arbitrary symplectic 2-form, will preserve its symplectic structure and admit the corresponding Hamiltonian as a first integral invariant (Arnold, 1989). The statement of Lemma 1 is simply a restatement of this fact grounded in a coordinate system. Similar arbitrary, antisymmetric terms have also appeared in the study of MCMC algorithms based on diffusion processes; such samplers often do not enforce detailed balance with respect to the target density and are often implemented as discretizations of stochastic differential equations (Rey-Bellet & Spiliopoulos, 2015; Ma et al., 2015), in contrast to the approach taken here.\nOur second observation is that the dynamics in (17) are not time-reversible in the traditional sense. Instead, if we consider the parametrization of A as:\nA = [ E F \u2212F> G ] (8)\nwhere E, G are antisymmetric and F is taken to be general such that A is invertible, then the non-canonical dynamics have a (pseudo) time-reversibility symmetry:\nLemma 2. If (\u03b8(t),p(t)) is a solution to the non-canonical dynamics:\nd\ndt [ \u03b8(t) p(t) ] = [ E F \u2212F> G ] \ufe38 \ufe37\ufe37 \ufe38\nA\n[ \u2207\u03b8H(\u03b8(t),p(t)) \u2207pH(\u03b8(t),p(t)) ] (9)\nthen (\u03b8\u0303(t), p\u0303(t)) = (\u03b8(\u2212t),\u2212p(\u2212t)) is a solution to the modified non-canonical dynamics:\nd\ndt [ \u03b8\u0303(t) p\u0303(t) ] = [ \u2212E F \u2212F> \u2212G ] \ufe38 \ufe37\ufe37 \ufe38\nA\u0303\n[ \u2207\u03b8\u0303H(\u03b8\u0303(t),p(t)) \u2207p\u0303H(\u03b8\u0303(t), p\u0303(t)) ] (10)\nif H(\u03b8,p) = H(\u03b8,\u2212p). In particular if E = G = 0 then A = A\u0303, which reduces to the traditional time-reversal symmetry of canonical Hamiltonian dynamics.\nLemma 1 suggests a generalization of HMC that can utilize an arbitrary invertible antisymmetric A matrix in its dynamics; however Lemma 2 indicates the non-canonical dynamics lack a traditional time-reversibility symmetry which poses a potential difficulty to satisfying detailed balance. In particular, we cannot compose \u03a6p with an exact/approximate simulation of \u03a6A\u03c4,H to make \u03a6\u0303 A \u03c4,H = \u03a6p \u25e6\u03a6A\u03c4,H self-inverse.\nOur solution to obtaining a time-reversible proposal is simply to flip the elements of the E and G matrices just as ordinary HMC flips the auxiliary variable p i) at the end of Hamiltonian flow in the proposal and ii) once again after the MH acceptance step to return p to its original direction. In this vein, we view the parameters E and G as auxiliary variables in the state space, and simultaneously flip p, E, and G after having simulated the dynamics, rendering the proposal time-reversible according to Lemma 2 \u2013 see Section 2 in the Appendix for full details of this construction. This ensures that detailed balance is satisfied for this entire proposal. To avoid \u201crandom walk\u201d behaviour in the resulting Markov chain, we can apply a sign flip to E and G, in addition to p, to return them to their original directions after the MH acceptance step.\nThe validity of this construction relies on equipping E and G with symmetric auxiliary distributions. For the remainder of this paper, we further restrict to binary symmetric auxiliary distributions supported on a given antisymmetric matrix V0 and its sign flip \u2212V0 \u2013 see Appendix 1.1 for full details. This restriction is not necessary, but gives rise to a simple and interpretable class of algorithms, which is in spirit closest to using fixed parameters E and G, whilst ensuring the proposal satisfies detailed balance. This construction is also reminiscent of lifting constructions prevalent in the discrete Markov chain literature (Chen et al., 1999); heuristically, the signed variables E and G favour proposals in opposing directions."}, {"heading": "3.1. Symplectic Numerical Integration for Non-Canonical Dynamics", "text": "As with standard HMC, exactly simulating the flow \u03a6A\u03c4,H is rarely tractable, and a numerical integrator is required to approximate the flow. It is not absolutely necessary to use an explicit, symplectic integration scheme; indeed implicit integrators are used in Riemannian HMC to maintain symplecticity of the proposal which comes at a greater complexity and computational cost (Girolami et al., 2009). However explicit, symplectic integrators are simple, have good energy-conservation properties, and are volumepreserving/time-reversible (Hairer et al., 2006), so for the present discussion we restrict our attention to investigating leapfrog-like schemes.\nWe begin, as in Section 2.2, by considering the symmetric splitting (34), yielding the sub-Hamiltonians H1(\u03b8) = U(\u03b8)/2, H2(p) = p>p/2. The corresponding noncanonical dynamics for the sub-Hamiltonians H1(\u03b8) and H2(p) are:\nd\ndt [ \u03b8 p ] = [ E F \u2212F> G ] \ufe38 \ufe37\ufe37 \ufe38\nA\n[ \u2207\u03b8U(\u03b8)/2\n0\n] = [ E\u2207\u03b8U(\u03b8)/2 \u2212F>\u2207\u03b8U(\u03b8)/2 ]\nand:\nd\ndt [ \u03b8 p ] = [ E F \u2212F> G ] \ufe38 \ufe37\ufe37 \ufe38\nA\n[ 0 p ] = [ Fp Gp ] .\nWe denote the corresponding flows by \u03a6A ,H1(\u03b8) and \u03a6A ,H2(p) respectively. The flow \u03a6 A ,H1(\u03b8)\nis generally not explicitly tractable unless we take E = 0 \u2013 in which case it is solved by an Euler translation as before. Crucially, the flow in \u03a6A ,H2(p) is a linear differential equation and hence analytically integrable. If G is invertible (and F = I) then:\n\u03a6 ,H2(p) [ \u03b8 p ] = [ \u03b8 + G\u22121(exp(G )\u2212 I)p exp(G )p ] . (11)\nSee the Appendix for a detailed derivation which also handles the general case where G is not invertible. Thus when E = 0, the flows \u03a6A ,H1(\u03b8) and \u03a6 A ,H2(p)\nare analytically tractable and will inherit the generalized symplecticity and (pseudo) time-reversibility of the exact dynamics in (17). Therefore if we use the symmetric splitting (34) to construct a leapfrog-like step:\n\u03a6frog,A ,H(\u03b8,p) = \u03a6 A ,H1(\u03b8) \u25e6\u03a6A ,H2(p) \u25e6\u03a6 A ,H1(\u03b8)\n(12)\nwe can construct a total proposal that consists of several leapfrog steps, followed by a flip of the momentum and G, \u03a6p \u25e6\u03a6G, which will be a volume-preserving, self-inverse map:\n\u03a6\u0303 frog,A ,H(\u03b8,p) = \u03a6p \u25e6\u03a6G \u25e6 (\u03a6A ,H1(\u03b8) \u25e6\u03a6 A ,H2(p) \u25e6\u03a6A ,H1(\u03b8)) L . (13)\nHenceforth we will always take E = 0 when we use \u03a6frog,A ,H(\u03b8,p) to generate leapfrog proposals, which interestingly corresponds to a magnetic dynamics as discussed in Lemma 4. A full description of the magnetic HMC algorithm using this numerical integrator is described in Section 5."}, {"heading": "4. Special Cases", "text": "Here, we describe several tractable subcases of the general formulation of non-canonical Hamiltonian dynamics since these they have interesting physical interpretations."}, {"heading": "4.1. Mass Preconditioned Dynamics", "text": "One simple variant of HMC is preconditioned HMC where p \u223c N (0,M) (Neal, 2011), and can be implemented nearly identically to ordinary HMC. We note that preconditioning can be recovered within our framework using a simple form for the non-canonical A matrix:\nLemma 3. i) Preconditioned HMC with momentum variable p \u223c N (0,M) in the (\u03b8,p) coordinates is exactly equivalent to simulating non-canonical HMC with p\u2032 = M\u22121/2p \u223c N (0, I) and the non-canonical matrix A =\nMagnetic Hamiltonian Monte Carlo[ 0 M1/2\n\u2212(M1/2)> 0\n] , and then transforming back to\n(\u03b8,p) coordinates using p = M1/2p\u2032. Here M1/2 is a Cholesky factor for M.\nii) On the other hand if we apply a change of basis (via an invertible matrix F) to our coordinates \u03b8\u2032 = F\u22121\u03b8, simulate HMC in the (\u03b8\u2032,p) coordinates, and transform back to the original basis using F, this is exactly equivalent to\nnon-canonical HMC with A = [\n0 F \u2212F> 0\n] .\nLemma 3 illustrates a fact alluded to in (Neal, 2011); using a mass preconditioning matrix M and a change of basis given by matrix \u2212(M\u22121/2)> acting on \u03b8 leaves the HMC algorithm invariant."}, {"heading": "4.2. Magnetic Dynamics", "text": "The primary focus of this paper is to investigate the subcase of the dynamics where:\nA = ( 0 I \u2212I G ) (14)\nfor two important reasons3. Firstly for this choice of A we can construct an explicit, symplectic, leapfrog-like integration scheme which is important for developing an efficient HMC sampler as discussed in Section 3.1. Secondly, the dynamics have an interesting physical interpretation quite distinct from mass preconditioning and other HMC variants like Riemannian HMC (Girolami et al., 2009):\nLemma 4. In 3 dimensions the non-canonical Hamiltonian dynamics corresponding to the Hamiltonian H(\u03b8,p) = U(\u03b8) + 12p\n>p and A matrix as in (14) are equivalent to the Newtonian mechanics of a charged particle (with unit mass and charge) coupled to a magnetic field ~B (given by a particular function of G - see Appendix): d\n2\u03b8 dt2 =\n\u2212\u2207\u03b8U(\u03b8) + d\u03b8dt \u00d7 ~B.\nThis interpretation is perhaps surprising since Hamiltonian formulations of classical magnetism are uncommon, although the quantum mechanical treatment naturally incorporates a Hamiltonian framework. However, in light of Lemma 3 we might wonder if by a clever rewriting of the Hamiltonian we can reproduce this system of ODEs using the canonical A matrix (i.e. E = G = 0, F = I). This is not the case:\nLemma 5. The non-canonical Hamiltonian dynamics with magnetic A and Hamiltonian H(\u03b8,p) = U(\u03b8) + 12p\n>p cannot be obtained using canonical Hamiltonian dynamics for any choice of smooth Hamiltonian. (See Appendix).\n3Note that the effect of a non-identity F matrix can be achieved by simply composing these magnetic dynamics with a coordinate-transformation as suggested in Lemma 3."}, {"heading": "5. The Magnetic HMC (MHMC) Algorithm", "text": "Using the results discussed in Section 3 and Section 3.1 we can now propose Magnetic HMC \u2013 see Algorithm 1.\nAlgorithm 1 Magnetic HMC (MHMC)"}, {"heading": "Input: H , G, L,", "text": "Initialize (\u03b80,p0), and set G0 \u2190 G for n = 1, . . . , N do\nResample pn\u22121 \u223c N(0, I) Set (\u03b8\u0303n, p\u0303n) \u2190 LF(H,L, , (\u03b8n\u22121,pn\u22121,Gn\u22121)) with \u03a6A ,H2(p) as in (11) Flip momentum (\u03b8\u0303n, p\u0303n) \u2190 (\u03b8\u0303n,\u2212p\u0303n) and set G\u0303n \u2190 \u2212Gn\u22121 if Unif([0, 1]) < min(1, exp(H(\u03b8n\u22121,pn\u22121) \u2212 H(\u03b8\u0303n, p\u0303n))) then\nSet (\u03b8n,pn,Gn)\u2190 (\u03b8\u0303n, p\u0303n, G\u0303n) else\nSet (\u03b8n,pn,Gn)\u2190 (\u03b8n\u22121,pn\u22121,Gn\u22121) end if Flip momentum pn \u2190 \u2212pn and flip Gn \u2190 \u2212Gn\nend for Output: (\u03b8n)Nn=0\nOne further remark is that by construction the integrator for magnetic HMC is expected to have similarly good energy conservation properties to the integrator of standard HMC:\nLemma 6. The symplectic leapfrog-like integrator for magnetic HMC will have the same local (\u223c O( 3)) and global (\u223c O( 2)) error scaling (over \u03c4 \u223c L steps), as the canonical leapfrog integrator of standard HMC if the Hamiltonian is separable. (See Appendix). It is worthwhile to contrast the algorithmic differences between magnetic HMC and ordinary HMC. Intuitively, the role of the flow \u03a6A ,H2(p) \u2013 which reduces to the standard Euler translation update of ordinary HMC when G = 0 \u2013 is to introduce a rotation into the momentum space of the flow. In particular, a non-zero element Gij will allow momentum to periodically flow between pi and pj . If we regard G as an element in the Lie algebra of antisymmetric matrices, which can be thought of as infinitesimal rotations, then the exponential map exp(G ) will project this transformation into the Lie group of real orthogonal linear maps.\nWith respect to computational cost, although magnetic HMC requires matrix exponentiation/diagonalization to simulate \u03a6A ,H2(p), this only needs to be computed once upfront for \u00b1G and cached; moreover, as \u00b1G is diagonalizable, the exact exponential can be calculated inO(d3) time. Additionally, there is an O(d2) cost for the matrix-vector products needed to implement the flow \u03a6A ,H2(p) as with preconditioning. However, it is possible to design sparsi-\nfied matrix representations of A which will translate into sparsified rotations if we only wish to \u201dcurl\u201d in a specific subspace of dimension d0 \u2013 which will translate into a computational cost of O(d30) and O(d20) respectively. An important problem to address is the selection of the G matrix, which affords a great deal of flexibility to MHMC relative to HMC; this point is further discussed in the Experiments section, where we argue that in certain cases intuitive heuristics can be used to select the G matrix."}, {"heading": "6. Experiments", "text": "Here we investigate the performance of magnetic HMC against standard HMC in several examples; in each case commenting on our choice of the magnetic field term G. Step sizes ( ) and number of leapfrog steps (L) were tuned to achieve an acceptance rate between .7 \u2212 .8, after which the norm of the non-zero elements in G was set to\u223c .1\u2212.2 which was found to work well.\nIn the Appendix we also display illustrations of different MHMC proposals across several targets in order to provide more intuition for MHMC\u2019s dynamics. Further experimental details and an additional experiment on a Gaussian funnel target are also provided in the Appendix."}, {"heading": "6.1. Multiscale Gaussians", "text": "We consider two highly ill-conditioned Gaussians similar to as in (Sohl-Dickstein et al., 2014) to illustrate a heuristic for G matrix selection and demonstrate properties of the magnetic dynamics. In particular we consider a centered, uncorrelated 2D Gaussian with covariance eigenvalues of 106 and 1 as well as a centered, uncorrelated 10D Gaussian with two large covariance eigenvalues of 106 and remaining eigenvalues of 1. We denote their coordinates as x = (x1, x2) \u2208 R2 and x = (x1, . . . , x10) \u2208 R10 respectively. HMC will have difficulty exploring the directions of\n0 5\u00d7 104 Lag\n0\n1\nA u to co rr e la ti o n\nHMC\nMHMC\n0 5\u00d7 104 Lag\n0\n1\nA u to co rr e la ti o n\nHMC\nMHMC\nFigure 2. Averaged Autocorrelation of HMC vs MHMC on a 10D ill-conditioned Gaussian (left) and Averaged Autocorrelation of HMC vs MHMC on a 2D ill-conditioned Gaussian.\nlarge marginal variance since its exploration will often be limited by the smaller variance directions. Accordingly, in order to induce a periodic momentum flow between the directions of small and large variance, we introduce nonzero components Gij into the subspaces spanned directly between the large and small eigenvalues. Indeed, we find that magnetic G term is encouraging more efficient exploration as we can see from the averaged autocorrelation of samples generated from the HMC/MHMC chains \u2013 see Figure 2. Further, by running the 50 parallel chains for 107 timesteps, we computed both the bias and Monte Carlo standard errors (MCSE) of the estimators of the target moments as shown in Table 1 and Table 2."}, {"heading": "6.2. Mixture of Gaussians", "text": "We compare MHMC vs. HMC on a simple, but interesting, 2D density over x = (x, y) \u2208 R2 comprised of an evenly weighted mixture of isotropic Gaussians: p(x) = 12N (x;\u00b5,\u03a3) + 12N (x;\u2212\u00b5,\u03a3) for \u03c32x = \u03c32y = 1, \u03c1xy = 0 and \u00b5 = (2.5,\u22122.5). This problem is challenging for HMC because the gradients in canonical Hamiltonian dynamics force it to one of the two modes. We tuned HMC to achieve an acceptance rate of \u223c .75 and used the same , L for MHMC, generating 15000 samples from both HMC and MHMC with these settings. The addition of the magnetic field term G \u2013 which has one degree of freedom in this case \u2013 introduces an asymmetric \u201ccurl\u201d into the dynamics that pushes the sampler across the saddlepoint to the other mode allowing it to efficiently mix around both modes and between them \u2013 see Figure 3. The maximum mean discrepancy between exact samples generated from the target density and samples generated from both HMC and MHMC chains was also estimated for various magnitudes of G, using a quadratic ker-\nnel k(x,x\u2032) = (1 + \u3008x,x\u2032\u3009)2 and averaged over 100 runs of the Markov chains (Borgwardt et al., 2006). Here, we clearly see that for various values of the nonzero component of G, denoted g, the samples generated by MHMC more faithfully reflect the structure of the posterior. As before, we ran 50 parallel chains for 107 timesteps to compute both the bias and Monte Carlo standard errors (MCSE) of the estimators of the target moments as shown in Table 3. Additional experiments over a range of , L (and corre-\nMHMC .00239 \u00b1 .012 0.000596 \u00b1 0.00365\nsponding acceptance rates) and details are included in the Appendix for this example, demonstrating similar behavior."}, {"heading": "6.3. FitzHugh-Nagumo model", "text": "Finally, we consider the problem of Bayesian inference over the parameters of the FitzHugh-Nagumo model (a set of nonlinear ordinary differential equations, originally developed to model the behavior of axial spiking potentials in neurons) as in (Ramsay et al., 2007; Girolami & Calderhead, 2011). The FitzHugh-Nagumo model is a dynamical system (V (t), R(t)) defined by the following coupled differential equations:\nV\u0307 (t) = c(V (t)\u2212 V (t)3/3 +R(t)) R\u0307(t) = \u2212(V (t)\u2212 a+ bR(t))/c (15)\nWe consider the problem where the initial conditions\n(V (0), R(0)) of the system (15) are known, and a set of noise-corrupted observations (V\u0303 (tn), R\u0303(tn))Nn=0 = (Va,b,c(tn)+\u03b5 V n , Ra,b,c(tn)+\u03b5 R n ) N n=0 at discrete time points 0 = t0 < t1 < \u00b7 \u00b7 \u00b7 < tN , are available - note that we illustrate dependence of the trajectories on the model parameters explicitly via subscripts. It is not possible to recover the true parameter values of the model from these observations, but we can obtain a posterior distribution over them by specifying a model for the observation noise and a prior distribution over the model parameters.\nSimilar to (Ramsay et al., 2007; Girolami & Calderhead, 2011), we assume that the observation noise variables (\u03b5Vn ) N n=0 and (\u03b5 V n ) N n=0 are iid N (0, 0.12), and take an independent N (0, 1) prior over each parameter a, b, and c. This yields a posterior distribution of the form\np(a, b, c) \u221d N (a; 0, 1)N (b; 0, 1)N (c; 0, 1) \u00d7 N\u220f n=0 N (V\u0303 (tn);Va,b,c(tn), 0.12) (16)\nImportantly, the highly non-linear dependence of the trajectory on the parameters a, b and c yields a complex posterior distribution - see Figure 4. Full details of the model set-up can be found in (Ramsay et al., 2007; Girolami & Calderhead, 2011).\nFor our experiments, we used fixed parameter settings of a = 0.2, b = 0.2, c = 3.0 to generate 200 evenlyspaced noise-corrupted observations over the time interval t = [0, 20] (as in (Ramsay et al., 2007; Girolami & Calderhead, 2011)). We performed inference over the posterior distribution of parameters (a, b, c) with this set of observations using both the HMC and MHMC algorithms, which was perturbed with a magnetic field in each of the 3 axial planes of parameters \u2013 along the ab, ac, and bc axes with magnitude g = 0.1. The chains were run to gener-\nate 1000 samples over 100 repetitions with settings of = 0.015, L = 10, which resulted in an average acceptance rate of\u223c .8. The effective sample size of each of the chains normalized per unit time was then computed for each chain.\nSince each query to the posterior log-likelihood or posterior gradient log-likelihood requires solving an augmented set of differential equations as in (15), the computation time (\u223c 238s) of all the methods was nearly identical. Moreover,\nnote that all methods achieved nearly perfect mixing over the first coordinate so their effective sample size were truncated at 1000 for the a coordinate. In this example, we can see that all magnetic perturbations slightly increase the mixing rate of the sampler over each of the (b, c) coordinates with the ab perturbation performing best."}, {"heading": "7. Discussion and Conclusion", "text": "We have investigated a framework for MCMC algorithms based on non-canonical Hamiltonian dynamics and have given a construction for an explicit, symplectic integrator that is used to implement a generalization of HMC we refer to as magnetic HMC. We have also shown several examples where the non-canonical dynamics of MHMC can improve upon the sampling performance of standard HMC. Important directions for further research include finding more automated, adaptive mechanisms to set the matrix G, as well as investigating positionally-dependent magnetic field components, similar to how Riemannian HMC corresponds to local preconditioning. We believe that exploiting more general deterministic flows (such as also maintaining a non-zero E in the top left-block of a general A matrix) could form a fruitful area for further research on MCMC methods."}, {"heading": "Acknowledgements", "text": "The authors thank John Aston, Adrian Weller, Maria Lomeli, Yarin Gal and the anonymous reviewers for helpful comments. MR acknowledges support by the UK Engineering and Physical Sciences Research Council (EPSRC) grant EP/L016516/1 for the University of Cambridge Centre for Doctoral Training, the Cambridge Centre for Analysis. RET thanks EPSRC grants EP/M0269571 and EP/L000776/1 as well as Google for funding."}, {"heading": "A. Section 3 and 4 Proofs", "text": "Here we provide proofs for results discussed in Section 3 of the main text regarding non-canonical dynamics. Lemma 1. The map \u03a6A\u03c4,H(\u03b8,p) defined by integrating the non-canonical Hamiltonian system\nd\ndt [ \u03b8(t) p(t) ] = A\u2207\u03b8,pH(\u03b8(t),p(t)) (17)\nwith initial conditions (\u03b8,p) for time \u03c4 , where A \u2208 M2n\u00d72n is any invertible, antisymmetric matrix induces a flow on the coordinates (\u03b8,p) that is still energy-conserving (\u2202\u03c4H(\u03a6A\u03c4,H(\u03b8,p)) = 0) and symplectic with respect to A ([\u2207\u03b8,p\u03a6\u03c4,H(\u03b8,p)]>A\u22121[\u2207\u03b8,p\u03a6\u03c4,H(\u03b8,p)] = A\u22121) which also implies volume-preservation of the flow.\nProof. The proofs of both results simply uses the antisymmetry of A.\nEnergy-Conservation \u2013 Simply, we have that:\n\u2202\u03c4H(\u03a6\u03c4,H(\u03b8,p)) = \u2207\u03b8,pH(\u03a6\u03c4,H(\u03b8,p))\u2202\u03c4\u03a6\u03c4,H(\u03b8,p) = (18) \u2207\u03b8,pH(\u03a6\u03c4,H(\u03b8,p))>A\u2207\u03b8,pH(\u03a6\u03c4,H(\u03b8,p)) = 0 (19)\nusing the antisymmetry of A and symmetry of\u2207\u03b8,pH(\u03a6\u03c4,H(\u03b8,p))\u2207\u03b8,pH(\u03a6\u03c4,H(\u03b8,p))>. Symplecticness \u2013 We must show that the Jacobian of the flow generated by the dynamics preserves the non-canonical structure matrix A, which amounts to showing:\n[\u2207\u03b8,p\u03a6\u03c4,H(\u03b8,p)]>\ufe38 \ufe37\ufe37 \ufe38 F (\u03c4)> A\u22121 [\u2207\u03b8,p\u03a6\u03c4,H(\u03b8,p)]\ufe38 \ufe37\ufe37 \ufe38 F (\u03c4) = A\u22121 (20)\nwhere we define F (\u03c4) = \u2207\u03b8,p\u03a6\u03c4,H(\u03b8,p) as the time-evolving Jacobian of the flow. First, note that F (\u03c4) can be equivalently described as the solution to the differential equation:\nd\nd\u03c4 F (\u03c4) = A\u2207\u03b8,p\u2207\u03b8,pH(\u03a6\u03c4,H(\u03b8,p))F (\u03c4) (21)\nwith the initial condition F (0) = I2d (the Jacobian for the identity map at t = 0). Trivially, we have:\nF (0)A\u22121F (0) = A\u22121 (22)\nThen note that:\nd\nd\u03c4 (F (\u03c4)>A\u22121F (\u03c4)) = F (\u03c4)>A\u22121A\u2207\u03b8,p\u2207\u03b8,pH(\u03a6\u03c4,H(\u03b8,p))F (\u03c4) + F (\u03c4)>\u2207\u03b8,p\u2207\u03b8,pH(\u03a6\u03c4,H(\u03b8,p))A>A\u22121F (\u03c4) = F (\u03c4)>\u2207\u03b8,p\u2207\u03b8,pH(\u03a6\u03c4,H(\u03b8,p))F (\u03c4)\u2212 F (\u03c4)>\u2207\u03b8,p\u2207\u03b8,pH(\u03a6\u03c4,H(\u03b8,p))F (\u03c4) = 0\nas desired by simply using A> = \u2212A.\nTime-Reversibility \u2013 However, crucially it is not the case that the Hamilton equations are time-reversible in the traditional sense of canonical Hamiltonian dynamics. Lemma 2. If (\u03b8(t),p(t)) is a solution to the non-canonical dynamics:\nd\ndt [ \u03b8(t) p(t) ] = [ E F \u2212F> G ] \ufe38 \ufe37\ufe37 \ufe38\nA\n[ \u2207\u03b8H(\u03b8(t),p(t)) \u2207pH(\u03b8(t),p(t)) ] (23)\nthen (\u03b8\u0303(t), p\u0303(t)) = (\u03b8(\u2212t),\u2212p(\u2212t)) is a solution to the modified non-canonical dynamics:\nd\ndt [ \u03b8\u0303(t) p\u0303(t) ] = [ \u2212E F \u2212F> \u2212G ] \ufe38 \ufe37\ufe37 \ufe38\nA\u0303\n[ \u2207\u03b8\u0303H(\u03b8\u0303(t),p(t)) \u2207p\u0303H(\u03b8\u0303(t), p\u0303(t)) ] (24)\nifH(\u03b8,p) = H(\u03b8,\u2212p). In particular if E = G = 0 then A = A\u0303, which reduces to the traditional time-reversal symmetry of canonical Hamiltonian dynamics.\nProof. A direct calculation yields\nd\ndt [ \u03b8\u0303(t) p\u0303(t) ] = [ \u2212 ddt\u03b8(\u2212t)\nd dtp(\u2212t)\n] = [ \u2212E\u2207\u03b8H(\u03b8(\u2212t))\u2212 F\u2207pH(\u03b8(\u2212t)) \u2212F>\u2207\u03b8H(\u03b8(\u2212t)) + G\u2207pH(\u03b8(\u2212t)) ] =\n[ \u2212E\u2207\u03b8\u0303H(\u03b8\u0303(t)) + F\u2207p\u0303H(\u03b8\u0303(t)) \u2212F>\u2207\u03b8\u0303H(\u03b8\u0303(t))\u2212G\u2207p\u0303H(\u03b8\u0303(t)) ] = [ \u2212E F \u2212F> \u2212G ] \ufe38 \ufe37\ufe37 \ufe38\nA\u0303\n[ \u2207\u03b8\u0303H(\u03b8\u0303(t)) \u2207p\u0303H(\u03b8\u0303(t)) ]"}, {"heading": "A.1. Non-Canonical Dynamics Variable Augmentation", "text": "As remarked in the main text it is necessary to flip the E and G matrices at the end of a deterministic simulation of the Hamiltonian dynamics in order to render the proposal time-reversible which is in turn necessary to satisfy detailed balance. This is crucial for the correctness of the algorithm especially when an approximate simulation of the dynamics is used (as is always often the case).\nIn particular, say that we wish to use \u03a6A\u03c4,H(\u03b8,p) as a transition kernel with fixed, non-zero values of E = E0 and G = G0. We first augment the state-space by placing a symmetric, binary distribution independently over E and G such that p(E = E0) = p(E = \u2212E0) = 1/2 and p(G = G0) = p(G = \u2212G0) = 1/2, independently of \u03b8,p:\n\u03c1(\u03b8,p,E,G) \u221d e\u2212H(\u03b8,p)p(E)p(G). (25)\nImportantly, this augmentation leaves the distribution over \u03b8,p intact when E and G are marginalized out. Just as applying the momentum flip operator, \u03a6p : (\u03b8,p,E,G)\u2192 (\u03b8,\u2212p,E,G), is a deterministic, energy-preserving, volume-preserving transformation, the E, G flip operators, \u03a6E : (\u03b8,p,E,G) \u2192 (\u03b8,p,\u2212E,G) and \u03a6G : (\u03b8,p,E,G) \u2192 (\u03b8,p,E,\u2212G), are also deterministic, energy-preserving, volume-preserving transformations that leave (25) invariant for this particular augmentation with p(E) and p(G). We can now build a self-inverse operator \u03a6\u0303A\u03c4,H(\u03b8,p), composed of simulating the Hamiltonian flow as \u03a6A\u03c4,H(\u03b8,p) plus \u03a6E \u25e6\u03a6G \u25e6\u03a6p, a flip of p, E, G, as:\n\u03a6\u0303A\u03c4,H(\u03b8,p) = \u03a6E \u25e6\u03a6G \u25e6\u03a6p \u25e6\u03a6A\u03c4,H(\u03b8,p) (26)\nNow we have constructed a deterministic, self-inverse map \u03a6\u0303A\u03c4,H(\u03b8,p). \u03a6\u0303 A \u03c4,H(\u03b8,p) can now be used as the proposal for a reversible MCMC algorithm.\nAn important point to note is that our choice of variable augmentation strategy, namely augmenting with binary distribution, is certainly not unique. However, it is perhaps the most natural and simplest choice which avoids the repetitive computation of matrix exponentials/diagonalizations since the approximate flow detailed in Section B.2 will only need to compute matrix exponentials once upfront for \u00b1G."}, {"heading": "A.2. Mass Preconditioning Proofs", "text": "A common variation on standard HMC dynamics is to set the kinetic energy term in the HamiltonianH(\u03b8,p) to 12p >M\u22121p for some symmetric positive-definite matrix M, and sample the initial momentum variable p from the corresponding distribution N (0,M). However, we can contextualize preconditioning using a non-canonical A matrix in the following manner:\nLemma 3. i) Preconditioned HMC with momentum variable p \u223c N (0,M) in the (\u03b8,p) coordinates, is exactly equivalent to simulating non-canonical HMC with p\u2032 = M\u22121/2p \u223c N (0, I) and the non-canonical matrix:\nA =\n[ 0 M1/2\n\u2212(M1/2)> 0 ] and then transforming back to (\u03b8,p) coordinates using p = M1/2p\u2032. Here M1/2 is a Cholesky factor for M.\nii) On the other hand if we apply a change of basis (via an invertible matrix F) to our coordinates \u03b8\u2032 = F\u22121\u03b8, simulate HMC in the (\u03b8\u2032,p) coordinates, and transform back to the original basis using F, this is exactly equivalent to noncanonical HMC with\nA = [ 0 F \u2212F> 0 ] Proof. We first prove the equivalence regarding the change of basis in momentum space. Under the M mass matrix variant of HMC, p is drawn from a N (0,M) distribution, and the dynamics of \u03b8,p are then given by\nd\ndt [ \u03b8 p ] = [ M\u22121p \u2212\u2207\u03b8U(\u03b8) ] Denoting the upper-triangular Cholesky factor of M\u22121 by M\u22121/2, and introducing a new variable p\u2032 = M\u22121/2p, we obtain the following dynamics for the joint variable (\u03b8,p\u2032):\nd\ndt [ \u03b8 p\u2032 ] = d dt [ \u03b8 M\u22121/2p ] = [ (M\u22121/2)>p\u2032 \u2212M\u22121/2\u2207\u03b8U(\u03b8) ] = [ 0 (M\u22121/2)> \u2212M\u22121/2 0 ] [ \u2207\u03b8U(\u03b8) p\u2032 ] Further, note that if the marginal distribution of p is N (0,M), then under this change of variables p\u2032 has the marginal distributionN (0, I). Thus, simulating canonical HMC with a non-identity mass matrix is equivalent to making a change of basis in momentum space, simulating non-canonical HMC with a particular choice of non-canonical A matrix, and finally reverting back to the original basis.\nWe now prove the equivalence regarding the change of basis in \u03b8 space, which follows similarly. Consider non-canonical HMC on the state-momentum pair (\u03b8,p), with the antisymmetric matrix A taking the particular form\nA = ( 0 F \u2212F> 0 ) The states \u03b8,p obtained from this algorithm are equal to those obtained by first changing basis to \u03b8\u2032 = F\u22121\u03b8, then simulating standard HMC dynamics for the pair (\u03b8\u2032,p) with respect to the Hamiltonian\nH \u2032(\u03b8\u2032,p) = U \u2032(\u03b8\u2032) + 1\n2 p>p\n= U(F\u03b8) + 1\n2 p>p\nand then reverting to the original basis as \u03b8 = F\u03b8\u2032. To see this, first note that if we denote the distribution on the coordinates \u03b8 corresponding to the potential U by \u03c0(\u03b8) = e\u2212U(\u03b8), then the corresponding distribution on the coordinates \u03b8\u2032 is given by \u03c0\u2032, where\n\u03c0\u2032(\u03b8\u2032) = det(F)\u03c0(F\u03b8\u2032)\nThe corresponding potential U \u2032 is therefore given by\nU \u2032(\u03b8\u2032) = U(F\u03b8\u2032)\nRunning canonical HMC dynamics targeting the Hamiltonian H \u2032 yields the dynamics:\nd\ndt\n[ \u03b8\u2032\np\n] = [ p\n\u2212\u2207\u03b8\u2032U(\u03b8\u2032) ] But note then that the dynamics of the original coordinates are then given by:\nd\ndt [ \u03b8 p ] = d dt [ F\u03b8\u2032 p ] = [ Fp \u2212\u2207\u03b8\u2032U(\u03b8\u2032) ] = [ Fp \u2212\u2207\u03b8\u2032U(F\u03b8) ] = [ Fp \u2212(F>)\u2207\u03b8U(\u03b8) ] = [ 0 F \u2212F> 0 ] [ \u2207\u03b8U(\u03b8) p\n] which are exactly a special case of non-canonical HMC dynamics described above."}, {"heading": "B. Magnetic HMC (MHMC)", "text": "Here we provide proofs related to the dynamics of magnetic HMC and it\u2019s symplectic integration scheme."}, {"heading": "B.1. Non-Canonical Dynamics and Magnetism", "text": "We first establish the connection between the particular subcase of non-canonical Hamiltonian dynamics where\nA = [ 0 I \u2212I G ]\nand Newton\u2019s law for a charged particle coupled to a magnetic field.\nLemma 4. In 3-dimensions the non-canonical Hamiltonian dynamics, with Hamiltonian H(\u03b8,p) = U(\u03b8) + 12p >p, correspond to simulating the differential equations:\nd\ndt [ \u03b8 p ] = [ 0 I \u2212I G ] \ufe38 \ufe37\ufe37 \ufe38\nA\n[ \u2207\u03b8H \u2207pH ] \u2261 [ 0 I \u2212I G ] \ufe38 \ufe37\ufe37 \ufe38\nA\n[ \u2207\u03b8U(\u03b8)\np\n] (27)\nwhere\nG =  0 \u2212b3 b2b3 0 \u2212b1 \u2212b2 b1 0  are equivalent to the Newtonian mechanics of a charged particle (with unit mass and charge) coupled to a magnetic field\n~B = b1b2 b3  which take the form: d2\u03b8\ndt2 = \u2212\u2207\u03b8U(\u03b8) +\nd\u03b8 dt \u00d7 ~B (28)\nwhere \u03b8 is simply a 3-dimensional vector and \u00d7 the cross-product.\nProof. If we let \u03b8 and p denote our position and momentum coordinates in 3 dimensions then Newton\u2019s law for a charged particle in a magnetic field (with m = q = 1) is:\nd2\u03b8 dt2 = \u2212\u2207\u03b8U(\u03b8) + d\u03b8 dt \u00d7 ~B (29)\nDefining momentum canonically as d\u03b8dt = p we have:\nd\ndt [ \u03b8 p ] = [ p\n\u2212\u2207\u03b8U(\u03b8) + d\u03b8dt \u00d7 ~B\n] = [ p\n\u2212\u2207\u03b8U(\u03b8) + Gp\n] \u2261 [\n0 I \u2212I G ] \ufe38 \ufe37\ufe37 \ufe38\nA\n[ \u2207\u03b8U(\u03b8)\np\n] (30)\nWe now show that the dynamics used in magnetic HMC cannot be reproduced by simply choosing a different smooth Hamiltonian, H \u2032(\u03b8,p) and using the canonical A matrix:\nA = [ 0 I \u2212I 0 ] to generate the dynamics.\nLemma 5. The non-canonical Hamiltonian dynamics with magnetic A and HamiltonianH(\u03b8,p) = U(\u03b8)+ 12p >p cannot be obtained using canonical Hamiltonian dynamics for any choice of smooth Hamiltonian.\nProof. Consider the ODEs corresponding to non-canonical dynamics with magnetic A and H(\u03b8,p) = U(\u03b8) + 12p >p:\nd\ndt [ \u03b8 p ] = [ 0 I \u2212I G ] [ \u2207\u03b8U(\u03b8) p ] = [ p \u2212\u2207\u03b8U(\u03b8) + Gp ] . (31)\nAssume, to obtain a contradiction, that these canonical Hamiltonian dynamics can be reproduced for some choice of smooth H \u2032(\u03b8,p) and canonical A matrix (i.e. E = G = 0, F = I):\nd\ndt [ \u03b8 p ] = [ 0 I \u2212I 0 ] [ \u2207\u03b8H \u2032(\u03b8,p) \u2207pH \u2032(\u03b8,p) ] = [ p \u2212\u2207\u03b8U(\u03b8) + Gp ] . (32)\nThis implies: [ \u2207pH \u2032(\u03b8,p) \u2207\u03b8H \u2032(\u03b8,p) ] = [ p \u2207\u03b8U(\u03b8)\u2212Gp ] =\u21d2 [ \u2207\u03b8\u2207pH \u2032(\u03b8,p) \u2207p\u2207\u03b8H \u2032(\u03b8,p) ] = [ 0 \u2212G ] . (33)\nHowever, as long as the 2nd-order mixed partial derivatives are continuous they must be equal; so the conclusion follows."}, {"heading": "B.2. Symplectic Integrator for Magnetic Dynamics", "text": "We begin by considering the symmetric splitting:\nH(\u03b8,p) = U(\u03b8)/2\ufe38 \ufe37\ufe37 \ufe38 H1(\u03b8) + pTp/2\ufe38 \ufe37\ufe37 \ufe38 H2(p) +U(\u03b8)/2\ufe38 \ufe37\ufe37 \ufe38 H1(\u03b8)\n(34)\nThe corresponding non-canonical dynamics for the sub-Hamiltonians H1(\u03b8) and H2(p) are:\nd\ndt [ \u03b8 p ] = [ E F \u2212F> G ] \ufe38 \ufe37\ufe37 \ufe38\nA\n[ \u2207\u03b8U(\u03b8)/2\n0\n] = [ E\u2207\u03b8U(\u03b8)/2 \u2212F>\u2207\u03b8U(\u03b8)/2 ] (35)\nand\nd\ndt [ \u03b8 p ] = [ E F \u2212F> G ] \ufe38 \ufe37\ufe37 \ufe38\nA\n[ 0 p ] = [ Fp Gp ] . (36)\nWe denote the corresponding flows by \u03a6A ,H1(\u03b8) and \u03a6 A ,H2(p) respectively. The flow in (35) is generally not explicitly tractable unless we take E = 0 \u2013 in which case it is solved by an Euler translation as for standard Hamiltonian dynamics.\nCrucially, the flow in (36) is a linear differential equation and hence analytically integrable. Without loss of generality, we restrict ourselves to the case F = I (the case for general F follows similarly). The dynamics associated with the flow H2(p) introduced in Lemma 4 become\nd\ndt [ \u03b8(t) p(t) ] = [ p(t) Gp(t) ] with initial condition (\u03b80,p0). Using the power series representation of the matrix exponential, the second differential equation for p may be integrated analytically to yield the following flow in p-space:\np(t) = exp(Gt)p0\nSubstituting this result into the differential equation for \u03b8 yields\nd\u03b8 dt = exp(Gt)p0\nIf G is invertible then once again using the power series representation of the matrix exponential and rearranging yields the solution\n\u03a6 ,H2(p) [ \u03b8 p ] = [ \u03b8 + G\u22121(exp(G )\u2212 I)p exp(G )p ]\nIf G is not invertible, then slightly more care must be taken to first diagonalize G and separate its invertible/singular components. Since G is strictly antisymmetric it can be written as iH where H is a Hermitian matrix. Thus it can be diagonalized over C as:\nG = [ U\u039b U0 ] [\u039b 0 0 0 ] [ U>\u039b U>0 ]\nwhere \u039b is a diagonal submatrix consisting of the nonzero eigenvalues of G. [ U\u039b U0 ] and [ U>\u039b U>0 ] are unitary matrices where the columns of U\u039b are the eigenvectors of G corresponding to its nonzero eigenvalues while the columns of U0 are the eigenvectors of G corresponding to its zero eigenvalues. Even if G is not invertible we still have:\np(t) = exp(Gt)p0\nHowever it is more convenient to represent the matrix exponential as:\nexp(Gt) = [ U\u039b U0 ] [exp(\u039bt) 0 0 I ] [ U>\u039b U>0 ] Substituting this result into the differential equation for \u03b8, this representation of exp(Gt) implies the non-identity block can be handled as in the invertible case while the I block follows trivially to give:\n\u03b8(t) = \u03b80 + [ U\u039b U0 ] [\u039b\u22121(exp(\u039bt)\u2212 I) 0 0 tI ] [ U>\u039b U>0 ] p0\nNote that if G = 0 then the flow map will simply reduce to an Euler translation as in ordinary HMC. We can also combine the ideas of Section A.2 to obtain a preconditioned, magnetic HMC algorithm corresponding to a general A-matrix of the form (\n0 F \u2212F> G\n)\nDealing with a non-zero E becomes more subtle, since the corresponding sub-Hamiltonian is no longer exactly integrable under the splitting construction. In order to exactly integrate this sub-block a more costly implicit integrator is needed.\nB.3. Integration Error of Magnetic HMC\nSince we are using a symmetric, leapfrog splitting scheme for magnetic HMC that exactly integrates each sub-Hamiltonian we obtain identical error scaling to the leapfrog integrator applied to canonical HMC. Indeed, symplectic integrators are well-known to have many nice error properties in general and so perhaps this result is not so surprising (Hairer et al., 2006).\nLemma 6. The symplectic leapfrog-like integrator for magnetic HMC will have the same local (\u223c O( 3)) and global (\u223c O( 2)) error scaling (over \u03c4 \u223c L steps), as the canonical leapfrog integrator of standard HMC if the Hamiltonian is separable.\nProof. Note that for the parametrization of A corresponding to magnetic HMC the Hamiltonian vector field ~H = \u2207pH\u2207\u03b8 + (\u2212\u2207\u03b8 + G\u2207pH)\u2207p \u2261 ~A + ~B will generate the exact flow corresponding to exactly simulating the dynamics. We obtain an O( 3) local error by simply exploiting the separability of the Hamiltonian. The leapfrog integration scheme splits the Hamiltonian as: H(\u03b8,p) = H1(\u03b8) +H2(p) +H1(\u03b8) and exactly integrates each sub-Hamiltonian so:\n\u03a6frog ,H = \u03a6 ,H1(\u03b8) \u25e6\u03a6 ,H2(p) \u25e6\u03a6 ,H1(\u03b8) = exp ( 2 ~B ) \u25e6 exp ( ~A ) \u25e6 exp ( 2 ~B )\n(37)\n(38)\nVia repeated applications of the Baker-Campbell-Hausdorff formula (Hairer et al., 2006) obtain:\nexp ( 2 ~B ) \u25e6 exp ( ~A + 2 ~B + 2 2 [~A, ~B] ) +O( 3) = (39)\nexp\n(\n2 ~B + ~A + 2 ~B +\n2\n2 [~A, ~B] +\n1 2 [ 2 ~B, ~A + 2 ~B +\n2\n2 [~A, ~B]]\n) +O( 3) = (40)\nexp ( ~H + 2\n4 [~A, ~B] +\n2\n4 [~B, ~A] +\n2\n8 [~B, ~B]\n) +O( 3) = exp ( ~H ) +O( 3) (41)\nwhere we have used the antisymmetry of the commutator. The global error scaling, for an integration time of \u03c4 = L follows straightforwardly:\n\u03a6frog\u03c4,H = ( exp ( 2 ~B ) \u25e6 exp ( ~A ) \u25e6 exp ( 2 ~B ))L\n(42) = ( exp ( ~H ) +O( 3) )L (43)\n= exp ( L~H ) + L O( 2) (44)\n= exp ( \u03c4 ~H ) + \u03c4O( 2) (45)\n= exp ( \u03c4 ~H ) +O( 2) (46)\nas desired."}, {"heading": "C. Section 6 Experimental Details", "text": "Here we provide relevant experimental details for some of the Experiments presented in the main text."}, {"heading": "C.1. Gaussians", "text": "In both experiments the reported autocorrelation measures are averaged over all coordinates as well as over 100 independent runs of the HMC/MHMC chains.\nC.1.1. 2D GAUSSIAN\nFor the uncorrelated, ill-conditioned 2D Gaussian experiment presented in the main text the magnetic G component only has one non-zero parameter which was set to g = .2.\nC.1.2. 10D GAUSSIAN\nFor the uncorrelated, ill-conditioned 10D Gaussian experiment presented in the main text, the G matrix was set to encourage the flow of momentum between the directions of large marginal variance with covariance eigenvalues 106 and the remaining 8 directions of directions of small marginal variance with covariance eigenvalues of 1. We denote the directions of large marginal variance as x1, x2, and the other 8 directions of directions of small marginal variance as xi. G was set such that G1i = G2i = g, Gi1 = Gi2 = \u2212g and G12 = G21 = 0 for g = .2."}, {"heading": "C.2. Mixture of Gaussians", "text": "The superior mixing of MHMC relative to HMC in this example holds true for a wide range of ( , L) settings as we can see by looking at the maximum mean discrepancy as a function of the number of samples in both Figures 5 and 6.\nWe found that tuning the parameters ( , L) via Bayesian optimization often resulted in worse performance for ordinary HMC since the values found for ( , L) were too conservative to encourage exploration between both modes. Moreover, more aggressive choices for ( , L) for ordinary HMC led to a sharp drop in acceptance rate and significantly worse performance."}, {"heading": "C.3. Gaussian Funnel", "text": "In this additional experiment, we consider the Gaussian funnel of (Neal, 2003) with density\np(x, v) = \u03a0ni=1N (xi|0, e\u2212v)N (v|0, 32)\nin 10+1 dimensions (i.e. n = 10). This density illustrates the pathological correlation present in many hierarchical models between x, a vector of low-level parameters, and v, a hyperparameter controlling their variability. As noted in (Betancourt & Girolami, 2015; Zhang & Sutton, 2014), Riemannian HMC methods, which incorporate local curvature information of the target, are well-suited to this problem as they help the dynamics traverse the energy surface which rapidly changes as a v varies. HMC (as well as MHMC) do not exploit curvature information and will have more difficulty exploring the v direction due to the rapid variation in density \u2013 see (Betancourt & Girolami, 2015) for a detailed discussion. Despite this difficulty, we might intuitively expect that introducing a \u201ccurl\u201d term into the entries of G which couple each xi and v could increase exploration of the dynamics since these variables are nonlinearly correlated. In order to encourage the periodic flow of momentum between the marginal direction v and the coordinates xi the G matrix was set such that Gvi = g, Giv = \u2212g, Gij = 0 with g = .2. To investigate this, we generated 10000 samples from both HMC and MHMC, discarding 1000 burn-in samples and computed the minimum effective sample size across x and v and bias in the moments of the v parameter similar to the set-up in (Zhang & Sutton, 2014) for various , L (see Table 5). We report results averaged over 100 different runs of the Markov chains.\nWe find that adding the magnetic field component decreases the bias in the moments and marginally increases the ESS,\nalthough both samplers struggle to explore the full target density, as the relatively low ESS figures indicate. Further details and experiments are provided in the Appendix.\nRecall the density of the Gaussian funnel p(x, v) = \u03a0ni=1N (xi|0, e\u2212v)N (v|0, 32). In order to encourage the periodic flow of momentum between the marginal direction v and the coordinates xi the G matrix was set such that Gvi = g, Giv = \u2212g, Gij = 0 with g = .2. Moreover the reported results were averaged over 100 different runs of the Markov chains."}, {"heading": "D. MHMC Proposals and Dynamics", "text": "In this section, we provide illustrations of the proposal distributions of MHMC in simple low-dimensional settings, to aid intuition and demonstrate the divergence of its behaviour from standard HMC."}, {"heading": "D.1. Gaussian Densities", "text": "We first consider the case of an isotropic Gaussian target, and illustrate the proposal distribution of standard HMC, as well as MHMC with a variety of settings for the skew-symmetric matrix A = (\nE F F G\n) - see Figure 7. As in previous sections,\nwe denote the off-diagonal element of the G matrix by g. We also provide proposal plots for an anisotropic Gaussian target distribution - see Figure 8."}, {"heading": "D.2. Banana Density", "text": "We also provide proposal illustrations for the banana density of (Haario et al., 1999), as shown in Figure 9."}], "year": 2017, "references": [{"title": "Mathematical Methods of Classical Mechanics", "authors": ["Arnold", "Vladimir"], "year": 1989}, {"title": "A Conceptual Introduction to Hamiltonian Monte Carlo", "authors": ["Betancourt", "Michael"], "year": 2017}, {"title": "Hamiltonian Monte Carlo for Hierarchical Models", "authors": ["Betancourt", "Michael", "Girolami", "Mark"], "year": 2015}, {"title": "Integrating structured biological data by Kernel Maximum Mean Discrepancy", "authors": ["Borgwardt", "Karsten M", "Gretton", "Arthur", "Rasch", "Malte J", "Kriegel", "Hans Peter", "Scholkopf", "Bernhard", "Smola", "Alex J"], "venue": "In Bioinformatics,", "year": 2006}, {"title": "Lifting markov chains to speed up mixing", "authors": ["Chen", "Fang", "Lov\u00e1sz", "L\u00e1szl\u00f3", "Pak", "Igor"], "venue": "In Proceedings of the Thirtyfirst Annual ACM Symposium on Theory of Computing,", "year": 1999}, {"title": "Hybrid Monte Carlo", "authors": ["Duane", "Simon", "A.D. Kennedy", "Pendleton", "Brian J", "Roweth", "Duncan"], "venue": "Physics Letters B,", "year": 1987}, {"title": "Riemann manifold Langevin and Hamiltonian Monte Carlo methods", "authors": ["Girolami", "Mark", "Calderhead", "Ben"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "year": 2011}, {"title": "Reversible Jump Markov Chain Monte Carlo Computation and Bayesian Model Determination", "authors": ["Green", "Peter J"], "year": 1995}, {"title": "Adaptive proposal distribution for random walk metropolis algorithm", "authors": ["Haario", "Heikki", "Saksman", "Eero", "Tamminen", "Johanna"], "venue": "Computational Statistics,", "year": 1999}, {"title": "Geometric Numerical Integration", "authors": ["Hairer", "Ernst", "Hochbruck", "Marlis", "Iserles", "Arieh", "Lubich", "Christian"], "venue": "Oberwolfach Reports, pp", "year": 2006}, {"title": "The No-U-Turn sampler: Adaptively setting path lengths in Hamiltonian Monte Carlo", "authors": ["Hoffman", "Matt", "Gelman", "Andrew"], "venue": "Journal of Machine Learning Research,", "year": 2008}, {"title": "A Complete Recipe for Stochastic Gradient MCMC", "authors": ["Ma", "Yi-an", "Chen", "Tianqi", "Fox", "Emily B"], "venue": "NIPS, pp", "year": 2015}, {"title": "Bayesian Learning for Neural Networks, volume", "authors": ["Neal", "Radford"], "year": 1996}, {"title": "MCMC using Hamiltonian Dynamics", "authors": ["Neal", "Radford M"], "venue": "Handbook of Markov Chain Monte Carlo,", "year": 2011}, {"title": "Parameter estimation for differential equations: A generalized smoothing approach", "authors": ["J.O. Ramsay", "G. Hooker", "D. Campbell", "J. Cao"], "venue": "JOURNAL OF THE ROYAL STATISTICAL SOCIETY, SERIES B,", "year": 2007}, {"title": "Irreversible Langevin Samplers and Variance Reduction: a Large Deviations Approach", "authors": ["Rey-Bellet", "Luc", "Spiliopoulos", "Konstantinos"], "year": 2015}, {"title": "Monte Carlo Statistical Methods, volume", "authors": ["Robert", "Christian P", "Casella", "George"], "year": 2004}, {"title": "Hamiltonian Monte Carlo Without Detailed Balance", "authors": ["J Sohl-Dickstein", "Mudigonda", "Mayur", "M. DeWeese"], "venue": "Proceedings of The 31st International Conference on Machine Learning,", "year": 2014}, {"title": "Semi-separable Hamiltonian Monte Carlo for Inference in Bayesian Hierarchical Models", "authors": ["Zhang", "Yichuan", "Sutton", "Charles"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2014}], "id": "SP:461201c852e33903c6e27c8d008aa00198b3de7b", "authors": [{"name": "Nilesh Tripuraneni", "affiliations": []}, {"name": "Mark Rowland", "affiliations": []}, {"name": "Zoubin Ghahramani", "affiliations": []}, {"name": "Richard Turner", "affiliations": []}], "abstractText": "Hamiltonian Monte Carlo (HMC) exploits Hamiltonian dynamics to construct efficient proposals for Markov chain Monte Carlo (MCMC). In this paper, we present a generalization of HMC which exploits non-canonical Hamiltonian dynamics. We refer to this algorithm as magnetic HMC, since in 3 dimensions a subset of the dynamics map onto the mechanics of a charged particle coupled to a magnetic field. We establish a theoretical basis for the use of non-canonical Hamiltonian dynamics in MCMC, and construct a symplectic, leapfrog-like integrator allowing for the implementation of magnetic HMC. Finally, we exhibit several examples where these non-canonical dynamics can lead to improved mixing of magnetic HMC relative to ordinary HMC.", "title": "Magnetic Hamiltonian Monte Carlo"}