0	49	Over the past few years, Abstract Meaning Representations (AMRs, Banarescu et al. (2013)) have become a popular target representation for semantic parsing.
1	100	AMRs are graphs which describe the predicate-argument structure of a sentence.
2	14	Because they are graphs and not trees, they can capture reentrant semantic relations, such as those induced by control verbs and coordination.
3	62	However, it is technically much more challenging to parse a string into a graph than into a tree.
8	14	We represent this structure as terms over the AM algebra as defined in Groschwitz et al. (2017).
11	16	More specifically, we combine a neural supertagger for identifying the elementary graphs for the individual words with a neural dependency model along the lines of Kiperwasser and Goldberg (2016) for identifying the operations of the algebra.
37	66	We use these sources to mark open argument slots; for example, Gsleep in Fig.
61	18	1, the graph for “writer” has the empty type [ ],Gsleep has type [S], andGwant has type [S, O[S]].
68	24	Linguistically, modification is optional; we therefore want the modified graph to be derivationally just like the unmodified graph, in that exactly the same operations can apply to it.
92	78	The same AM dependency tree may represent multiple indexed AM terms, because the order of apply and modify operations is not specified in the dependency tree.
102	43	In order to train a model that parses sentences into AM dependency trees, we need to convert an AMR corpus – in which sentences are annotated with AMRs – into a treebank of AM dependency trees.
133	23	and train the neural network using a cross-entropy loss function.
135	21	Predicting the edge scores amounts to a dependency parsing problem.
140	13	We train the model using K&G’s original DyNet implementation.
149	18	We then minimize the cross-entropy loss for the gold edge into k under softmax(ω(• → k)), maximizing the likelihood of the gold edges.
152	27	Given learned estimates for the graph and edge scores, we now tackle the challenge of computing the best well-typed dependency tree t for the input string w, under the score model (equation (1)).
155	44	In this section, we develop two different approximation algorithms for AM dependency parsing: one which assumes the (unlabeled) dependency tree structure as known, and one which assumes that the AM dependency tree is projective.
156	50	The projective decoder assumes that the AM dependency tree is projective, i.e. has no crossing dependency edges.
157	14	Because of this assumption, it can recursively combine adjacent substrings using dynamic programming.
161	68	First, the Init rule generates an item for each graph fragment G[i] that the supertagger predicted for the token wi, along with the score and type of that graph fragment.
165	19	After all possible items have been derived, we extract the best well-typed tree from the item of the form ([1, n], r, τ) with the highest score, where τ = [ ].
168	21	The fixed-tree decoder computes the best unlabeled dependency tree tr for w, using the edge scores ω(i→ k), and then computes the best AM dependency tree forw whose unlabeled version is tr.
169	26	The Chu-Liu-Edmonds algorithm produces a forest of dependency trees, which we want to combine into tr.
174	13	We write Ch(i) for the set of children of i in tr.
185	39	Furthermore, the AM algebra is designed to handle short-range reentrancies, modeling grammatical phenomena such as control and coordination, as in the derivation in Fig.
186	13	It cannot easily handle the long-range reentrancies in AMRs which are caused by coreference, a non-compositional phenomenon.4 We remove such reentrancies from our training data (about 60% of the roughly 20,000 reentrant edges).
187	301	Despite this, our model performs well on reentrant edges (see Table 2).
197	29	Each elementary as-graph generated by the procedure of Section 4.2 has a unique node whose label corresponds most closely to the aligned word (e.g. the “want” node in Gwant and the “write” node in Gwriter).
203	22	For names, we just look up name nodes with their children and wiki entries observed for the name string in the training data, and for unseen names use the literal tokens as the name, and no wiki entry.
207	13	We evaluated the accuracy of the supertagger on the converted development set (in which each token has a supertag) of the 2015 data set, and achieved an accuracy of 73%.
