0	25	The success of recurrent neural network (RNN) models in complex tasks like machine translation and audio synthesis has inspired immense interest in learning from sequence data (Eck & Schmidhuber, 2002; Graves, 2013; Sutskever et al., 2014; Karpathy, 2015).
6	28	For example: a random sequence of words will almost never form a coherent sentence that reads naturally, and a random amino-acid sequence is highly unlikely to specify a biologically active protein.
7	43	In this work, we consider applications where each sequence x is associated with a corresponding outcome y P R. For example: a news article title or Twitter post can be associated with the number of shares it subsequently received online, or the amino-acid sequence of a synthetic protein can be associated with its clinical efficacy.
8	38	We operate under the standard supervised learning setting, assuming availability of a dataset D n “ tpx i , y i qun i“1 iid„ p XY of sequence-outcome pairs.
9	34	The marginal distribution p X is assumed as a generative model of the natural sequences, and may be concentrated in a small subspace of X .
11	50	After fitting models to D n , we are presented a new sequence x0 P X (with unknown outcome), and our goal is to quickly identify a revised version that is expected to have superior outcome.
12	102	Formally, we seek the revised sequence: x˚ “ argmax xPC x0 ErY | X “ xs (1) Here, we want the set C x0 of feasible revisions to ensure that x˚ remains natural and is merely a minor revision of x0.
15	20	This optimization is difficult because the constraint-set and objective may be highly complex and are both unknown (must be learned from data).
17	24	Levenshtein distance or TF-IDF similarity) are inadequate to capture meaningful similarities, even though these can be faithfully reflected by a simple metric over an appropriately learned space of continuous latent factors (Mueller & Thyagarajan, 2016).
18	44	In this work, we introduce a generative-modeling framework which transforms (1) into a simpler differentiable optimiza- tion by leveraging continuous-valued latent representations learned using neural networks.
33	47	Here, latent factors Z P Rd specify a (continuous) configuration of the generative process for X,Y (both sequences and outcomes), and we adopt the prior p Z “ Np0, Iq.
34	23	Relationships between these variables are summarized by the maps F,E,D which we parameterize using three neural networks F ,E ,D trained to enable efficient approximate inference under this model.
52	25	Our revision methodology employs the encoding procedure Epxq “ µ z|x which maps a sequence to the maximum a posteriori (MAP) configuration of the latent values z (as estimated by the encoder network E ).
55	44	negative log-likelihood under the decoder model) is efficiently approximated using just one Monte-Carlo sample z „ q E pz | xq.
73	50	Happening to learn this sort of latent representation would be troubling, since subsequent optimization of the inferred y with respect to z might not actually lead to a superior revised sequence.
74	16	To mitigate this issue, we carefully ensure the dimensionality d of our latent Z does not significantly exceed the bottleneck capacity needed to produce accurate outcome-predictions and VAE reconstructions (Gupta et al., 2016).
82	25	Step 2: Grow pri from 0 to 1 following the sigmoid annealing schedule proposed by Bowman et al. (2016), which is needed to ensure the variational sequence to sequence model does not simply ignore the encodings z (note that the formal variational lower bound is attained at pri “ 1).
87	41	REVISE Algorithm Input: sequence x0 P X , constant ↵ P p0, |2⇡⌃ z|x0 |´ 1 2 q Output: revised sequence x˚ P X 1) Use E to compute q E pz | x0q 2) Define C x0 “ z P Rd : q E pz | x0q • ↵ ( 3) Find z˚ “ argmax zPC x0 F pzq (gradient ascent) 4) Return x˚ “ Dpz˚q (beam search) Intuitively, the level-set constraint C x0 Ñ Rd ensures that z˚, the latent configuration from which we decode x˚, is likely similar to the latent characteristics responsible for the generation of x0.
95	15	In terms of resulting revision quality, we found this log barrier method outperformed other standard first-order techniques for constrained optimization such as the projected gradient and Franke-Wolfe algorithms.
105	47	Thus, when revising a sequence x0 which looks natural (has substantial probability under p X ), our procedure is highly likely to produce a revised sequence x˚ which also looks natural.
130	48	Here, ✏mse and ✏inv quantify the approximation error of our neural networks for predicting expected outcomes and ensuring encoding-decoding invariance with respect to F .
151	44	The model with inv “ pri “ 0 is a similar method using a deterministic sequence-to-sequence autoencoder rather than our probabilistic VAE formulation (no variational posterior approximation or invariance-enforcing) where the latent encodings are still jointly trained to predict outcomes via F .
177	33	Moreover, Tables 3 and S2 show that our probabilisticallyconstrained VAE revision approach produces much more coherent sentences than the other strategies.
178	15	For our final application, we assemble a dataset of „100K short sentences which are either from Shakespeare or a more contemporary source (details in §S2.3).
180	20	When applied in this domain, our REVISE procedure thus attempts to alter a sentence so that the author is increasingly expected to be Shakespeare rather than a more contemporary source.
181	42	Tables 4 and S3 show revisions (of held-out sentences) proposed by our REVISE procedure with adaptive decoding (see §S1), together with sentences generated by applying the adaptive decoder at various points along an unconstrained gradient-ascent path in latent Z space (following gradients of F ).
184	36	Nevertheless, we find that many of the revised sentences look realistic and resemble text written by Shakespeare.
185	18	Furthermore, these examples demonstrate how the probabilistic constraint in our REVISE optimization prevents the revision-generating latent Z configurations from straying into regions where decodings begin to look very unnatural.
187	15	Leveraging a latent-variable generative model, our procedure does not require any examples of revisions in order to propose natural-looking sequences with improved outcomes.
189	107	However, ensuring semantic similarity in textrevisions remains difficult for this approach, and might be improved via superior VAE models or utilizing additional similarity labels to shape the latent geometry.
