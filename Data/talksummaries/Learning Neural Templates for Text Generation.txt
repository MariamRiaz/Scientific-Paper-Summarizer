6	57	The core system uses a novel, neural hidden semimarkov model (HSMM) decoder, which provides a principled approach to template-like text generation.
8	29	Generating with the template-like structures induced by the neural HSMM allows for the explicit representation of what the system intends to say (in the form of a learned template) and how it is attempting to say it (in the form of an instantiated template).
65	29	Under a standard encoder-decoder style model, one could filter out this information either from the encoder or decoder, but in practice this would lead to unexpected changes in output that might propagate through the whole system.
66	65	As n xampl of the difficulty of interpreting mistakes, consider following actual generation from an encoder-decoder style system for the records in Figure 2: ”frederick parker-rhodes (21 november 1914 - 2 march 1987) was an english mycology and plant pathology, mathematics at the university of uk.” In addition to not being fluent, it is unclear what the end of this sentence is even attempting to convey: it may be attempting to convey a fact not actually in the knowledge base (e.g., where Parker-Rhodes studied), or perhaps it is simply failing to fluently realize information that is in the knowledge base (e.g., ParkerRhodes’s country of residence).
72	57	What does it mean to learn a template?
73	51	It is natural to think of a template as a sequence of typed text-segments, perhaps with some segments acting as the template’s “backbone” (Wang and Cardie, 2013), and the remaining segments filled in from the knowledge base.
74	33	A natural probabilistic model conforming with this intuition is the hidden semi-markov model (HSMM) (Gales and Young, 1993; Ostendorf et al., 1996), which models latent segmentations in an output sequence.
76	27	We briefly review HSMMs following Murphy (2002).
78	23	yT and a discrete, latent state zt ∈{1, .
84	46	Thus, the likelihood is given by the product of the probabilities of each discrete state transition made, the probability of the length of each segment given its discrete state, and the probability of the observations in each segment, given its state and length.
93	41	Inspired by the models used for neural NLG, we base this model on an RNN decoder, and write a segment’s probability as a product over token-level probabilities, p(yt−lt+1:t | zt= k, lt= l, x) = lt∏ i=1 p(yt−lt+i | yt−lt+1:t−lt+i−1, zt= k, x) × p(</seg> | yt−lt+1:t, zt= k, x)× 1{lt = l}, where </seg> is an end of segment token.
94	25	The RNN decoder uses attention and copy-attention over the embedded records rj , and is conditioned on zt= k by concatenating an embedding corresponding to the k’th latent state to the RNN’s input; the RNN is also conditioned on the entire x by initializing its hidden state with xa.
99	29	To additionally implement a kind of slot filling, we allow emissions to be directly copied from the value portion of the records rj using copy attention (Gülçehre et al., 2016; Gu et al., 2016; Yang et al., 2016).
112	32	(1) Since the quantities in (1) are obtained from a dynamic program, which is itself differentiable, we may simply maximize with respect to the parameters θ by back-propagating through the dynamic program; this is easily accomplished with automatic differentiation packages, and we use pytorch (Paszke et al., 2017) in all experiments.
113	21	After training, we could simply condition on a new database and generate with beam search, as is standard with encoder-decoder models.
123	22	These MAP segmentations can be used in an exploratory way, as a sort of dimensionality reduction of the generations in the corpus.
126	27	Each “template” z(i) consists of a sequence of latent states, with z(i)= z(i)1 , .
127	57	z (i) S representing the S distinct segments in the i’th extracted template (recall that we will technically have a zt for each time-step, and so z(i) is obtained by collapsing adjacent zt’s with the same value); see Figure 4 for an example template (with S=17) that can be extracted from the E2E corpus.
128	46	The bottom of Figure 1 shows a visualization of this extracted template, where discrete states are replaced by the phrases they frequently generate in the training data.
130	46	In particular, given a new input x, we may generate by computing ŷ(i) = argmax y′ p(y′, z(i) |x), (2) which gives us a generation ŷ(i) for each extracted template z(i).
134	57	Returning to the discussion of controllability and interpretability, we note that with the proposed model (a) it is possible to explicitly force the generation to use a chosen template z(i), which is itself automatically learned from training data, and (b) that every segment in the generated ŷ(i) is typed by its corresponding latent variable.
166	33	On the E2E data, for example, we see in Table 1 that the SUB baseline, despite having fairly impressive performance for a nonparametric model, fares the worst.
167	34	The neural HSMM models are largely competitive with the encoder-decoder system on the validation data, despite offering the benefits of interpretability and controllability; however, the gap increases on test.
168	31	Table 2 evaluates our system’s performance on the test portion of the WikiBio dataset, comparing with the systems and baselines implemented by Lebret et al. (2016).
170	37	Here the HSMMs are competitive with the best model of Lebret et al. (2016), and even outperform it on ROUGE.
185	23	Moreover, because the discrete states align with particular fields (see below), it is generally simple to automatically infer to which fields particular latent states correspond, allowing users to choose which template best meets their requirements.
190	37	Moreover, we see that particular discrete states correspond in a consistent way to particular pieces of information, allowing us to align states with particular field types.
195	35	Table 5 indicates that discrete states learned on the E2E data are quite pure.
243	145	| aftab ahmed anderson da silva david jones ... | is a is a former is a female ... | member of the party member of the recipient of the ... | knesset scottish parliament fc lokomotiv liski ... |.
244	81	Table 7: Five templates extracted from the WikiBio data with the NTemp model (top) and the Ntemp+AR model (bottom).
