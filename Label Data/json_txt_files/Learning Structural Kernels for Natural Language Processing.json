{"sections": [{"heading": "1 Introduction", "text": "Kernel-based methods are a staple machine learning approach in Natural Language Processing (NLP). Frequentist kernel methods like the Support Vector Machine (SVM) pushed the state of the art in many NLP tasks, especially classification and regression. One interesting aspect of kernels is their ability to be defined directly on structured objects like strings, trees and graphs. This approach has the potential to move the modelling effort from feature engineering to kernel engineering. This is useful when we do not have much prior knowledge about how the data\nbehaves, as we can more readily define a similarity metric between inputs instead of trying to characterize which features are the best for the task at hand.\nKernels are a very flexible framework: they can be combined and parameterized in many different ways. Complex kernels, however, lead to the problem of model selection, where the aim is to obtain the best kernel configuration in terms of hyperparameter values. The usual approach for model selection in frequentist methods is to employ grid search on some development data disjoint from the training data. This approach can rapidly become impractical when using complex kernels which increase the number of model hyperparameters. Grid search also requires the user to explicitly set the grid values, making it difficult to fine tune the hyperparameters. Recent advances in model selection tackle some of these issues, but have several limitations (see \u00a76 for details).\nOur proposed approach for model selection relies on Gaussian Processes (GPs) (Rasmussen and Williams, 2006), a widely used Bayesian kernel machine. GPs allow efficient and fine-grained model selection by maximizing the evidence on the training data using gradient-based methods, dropping the requirement for development data. As a Bayesian procedure, GPs also naturally balance between model capacity and generalization. GPs have been shown to achieve state of the art performance in various regression tasks (Hensman et al., 2013; Cohn and Specia, 2013). Therefore, we base our approach on this framework.\nWhile prediction performance is important to consider (as we show in our experiments), we are\n461\nTransactions of the Association for Computational Linguistics, vol. 3, pp. 461\u2013473, 2015. Action Editor: Stefan Riezler. Submission batch: 2/2015; Revision batch 7/2015; Published 8/2015.\nc\u00a92015 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license.\nmainly interested in two other significant aspects that are enabled by our approach:\n\u2022 Gradient-based methods are more efficient than grid search for high dimensional spaces. This allows us to easily propose new rich kernel extensions that rely on a large number of hyperparameters, which in turn can result in better modelling capacity.\n\u2022 Since the model selection process is now finegrained, we can interpret the resulting hyperparameter values, depending on how the kernel is defined.\nIn this work we focus on tree kernels, which have been successfully used in a number of NLP tasks (see \u00a76). In most cases, these kernels are used as an SVM component and model selection is not considered an important issue. Hyperparameters are usually set to default values, which work reasonably well in terms of prediction performance. However, this is only possible due to the small number of hyperparameters these kernels contain.\nWe perform experiments comprising synthetic data (\u00a74) and two real NLP regression tasks: Emotion Analysis (\u00a75.1) and Translation Quality Estimation (\u00a75.2). Our findings show that our approach outperforms SVMs using the same kernels."}, {"heading": "2 Gaussian Process Regression", "text": "Our definition of GPs closely follows that of Rasmussen and Williams (2006). Consider a setting where we have a dataset X = {(x1, y1), (x2, y2), . . . , (xn, yn)}, where xi is a ddimensional input and yi the corresponding output. Our goal is to infer an underlying function f : Rd \u2192 R to explain this data, i.e. f(xi) \u2248 yi. Formally, f is drawn from a GP prior,\nf(x) \u223c GP(\u00b5(x), k(x,x\u2032)),\nwhere \u00b5(x) is the mean function, which is usually the 0 constant, and k(x,x\u2032) is the kernel function.\nIn a regression setting, we assume that the response variables are noisy latent function evaluations, i.e., yi = f(xi) + \u03b7, where \u03b7 \u223c N (0, \u03c32n) is added white noise. We assume a Gaussian likelihood, which allows us to obtain a closed formula\nsolution for the posterior, namely\ny\u2217 \u223c N (k\u2217(K + \u03c3nI)\u22121yT , k(x\u2217,x\u2217)\u2212 kT\u2217 (K + \u03c3nI)\u22121k\u2217),\nwhere x\u2217 and y\u2217 are respectively the test input and its response variable, K is the Gram matrix corresponding to the training inputs and k\u2217 = [\u3008x1,x\u2217\u3009, \u3008x2,x\u2217\u3009, . . . , \u3008xn,x\u2217\u3009] is the vector of kernel evaluations between the test input and each training input.\nA key property of GP models is their ability to perform efficient model selection. This is achieved by employing gradient-based methods to maximize the marginal likelihood,\np(y|X,\u03b8) = \u222b p(y|X,\u03b8, f)p(f)df,\nwhere \u03b8 represents the vector of model hyperparameters and y is the vector of response variables from the training data. For a Gaussian likelihood, we can take the log of the expression above to obtain in closed-form1,\nlog p(y|X,\u03b8) =\n\u22121 2 yTG\u22121y\n\ufe38 \ufe37\ufe37 \ufe38 data fit\n\u22121 2\nlog |G| \ufe38 \ufe37\ufe37 \ufe38\ncomplexity penalty\n\u2212n 2\nlog 2\u03c0 \ufe38 \ufe37\ufe37 \ufe38\nconstant\nwhere G = K+\u03c3nI. The data fit term is dependent on the training response variables, while the complexity penalty term relies only on the kernel and training inputs. Since the first two terms have conflicting objectives, optimizing the log marginal likelihood will naturally achieve a compromise and thus limit overfitting (without the need for any validation step or additional data).\nTo enable gradient-based optimization we need to derive the gradients w.r.t. the hyperparameters:\n\u2202 \u2202\u03b8j log p(y|X,\u03b8) =1 2 yTG\u22121 \u2202G \u2202\u03b8j G\u22121y\n\u2212 1 2\ntrace ( G\u22121 \u2202G\n\u2202\u03b8j\n) .\n1See Rasmussen and Williams (2006, pp. 113-114) for details on the derivation of this formula and also its correspondent gradient calculation.\nThe gradients of G depend on the underlying kernel. Therefore we can employ any kind of valid kernel in this procedure as long as its gradients can be computed. This not only allows for fine-tuning of hyperparameters but also allows for kernel extensions which are richly parameterized."}, {"heading": "3 Tree Kernels", "text": "The seminal work on Convolution Kernels by Haussler (1999) defines a broad class of kernels on discrete structures by counting and weighting the number of substructures they share. Applying Haussler\u2019s formulation to trees we reach a general formula for a tree kernel between two trees t1 and t2, namely\nk(t1, t2) = \u2211\nf\u2208F w(f)c1(f)c2(f), (1)\nwhere F is the set of all tree fragments, c1(f) and c2(f) return the counts for fragment f in trees t1 and t2, respectively, and w(f) assigns a weight to fragment f . In other words, we can consider the kernel a weighted dot product over vectors of fragment counts. The actual fragment set F is deliberately left undefined: different concepts of tree fragments define different tree kernels.\nIn this paper, we will focus on Subset Tree Kernels (henceforth SSTK), first introduced by Collins and Duffy (2001). This kernel considers tree fragments that contains complete grammar rules (see Figure 1 for an example). Consider the set of nodes in the two trees as N1 and N2 respectively. We define Ii(n) as an indicator function that returns 1 if fragment fi \u2208 F has root n and 0 otherwise. A SSTK can then be defined as:\nk(t1, t2) = \u2211\nn1\u2208N1\n\u2211\nn2\u2208N2 \u2206(n1, n2) , (2)\nwhere \u2206(n1, n2) = |F|\u2211\ni=1\n\u03bb s(i) 2 Ii(n1)Ii(n2)\nand s(i) is the number of fragments in i with at least one child2.\nThe formulation in Equation 2 is the same as the one shown in Equation 1, except that we are now restricting the weights w(f) to be a function of a\n2See Pighin and Moschitti (2010) for details and a proof on this derivation.\nhyperparameter \u03bb. The original goal of \u03bb is to act as a decay factor that penalizes contributions from larger fragments cf smaller ones (and therefore, it should be in the [0, 1] interval). Without this factor, the resulting distribution over tree pairs is skewed, giving extremely large values when trees are equal and rapidly decreasing for small differences over fragment counts. The decay factor helps to spread this distribution, effectively giving smaller weights to larger fragments.\nThe function \u2206 can be defined recursively,\n\u2206(n1, n2) =    0 pr(n1) 6= pr(n2) \u03bb pr(n1) = pr(n2) \u2227\npreterm(n1) \u03bbg(n1, n2) otherwise,\nwhere pr(n) is the grammar production at node n and preterm(n) returns true if n is a pre-terminal node. The function g is defined as follows:\ng(n1, n2) =\n|n1|\u220f\ni=1\n(\u03b1+ \u2206(cin1 , c i n2)) , (3)\nwhere |n| is the number of children of node n and cin is the i\nth child of node n. This recursive definition is calculated efficiently by employing dynamic programming to cache intermediate \u2206 results.\nEquation 3 also adds another hyperparameter, \u03b1. This hyperparameter was introduced by Moschitti (2006b)3 as a way to select between two different tree kernels. If \u03b1 = 1, we get the original SSTK, if \u03b1 = 0, then we obtain the Subtree Kernel, which only allows fragments with terminal symbols\n3In his original formulation, this hyperparameter was named \u03c3 but here we use \u03b1 to not confuse it with the GP noise hyperparameter.\nas leaves. We can also interpret the Subtree Kernel as a \u201csparse\u201d version of the SSTK, where the \u201cnonsubtree\u201d fragments have their weights equal to zero.\nEven though fragment weights are affected by both kernel hyperparameters, previous work did not discuss their effects. The usual procedure fixes \u03b1 to 1 (selecting the original SSTK) and sets \u03bb to a default value (around 0.4). As explained in \u00a72, the GP model selection procedure enables us to learn finegrained values for these hyperparameters, which can lead to better performing models and aid interpretation. Furthermore, it also allows us to extend these kernels by adding new hyperparameters. We propose one such kernel in the next Section."}, {"heading": "3.1 Symbol-aware Subset Tree Kernel", "text": "While varying the SSTK hyperparameters can lead to different weight schemes, they do that in a very coarse way. For some applications, it may be necessary to give more weight to specific fragments or set of fragments (e.g., NPs being more important than ADVP in an information extraction setting). The Symbol-aware Subset Tree Kernel (henceforth, SASSTK), which we introduce here, allows a more fine-grained control over the weights by employing one \u03bb and one \u03b1 hyperparameter for each non-terminal symbol in the training data. The calculation uses a similar recursive formula to the SSTK, namely:\n\u2206(n1, n2) =    0 pr(n1) 6= pr(n2) \u03bbx pr(n1) = pr(n2) \u2227\npreterm(n1) \u03bbxgx(n1, n2) otherwise,\nwhere x is the symbol at node n1 and\ngx(n1, n2) =\n|n1|\u220f\ni=1\n(\u03b1x + \u2206(c i n1 , c i n2)) . (4)\nThe SASSTK can be interpreted as a generalization of the SSTK: we can recover the latter by tying all \u03bb and setting all \u03b1 = 1. By employing different hyperparameter values for each specific symbol, we can effectively modify the weights of all fragments where the symbol appears. Table 1 shows an example where we unrolled a kernel computation into its corresponding feature space, showing the resulting weighted counts for each feature."}, {"heading": "3.2 Kernel Gradients", "text": "To enable hyperparameter optimization via gradient descent we must provide gradients for the kernels. In this Section we derive the gradients for SASSTK.\nFrom Equation 2 we know that the kernel is a double summation over the \u2206 function. Therefore all gradients are also double summations, but over the gradients of \u2206. We can obtain these in a vectorized way, by considering the gradients of the hyperparameter vectors \u03bb and \u03b1 over \u2206. Let k be the number of symbols considered in the model and \u03bb and \u03b1 be k-dimensional vectors containing the respective hyperparameters.\nIn the following, we use the notation \u2206i as a shorthand for \u2206(cin1 , c i n2) and we also omit the parameters of gx. We start with the \u03bb gradient:\n\u2202\u2206 \u2202\u03bb =    0 pr(n1) 6= pr(n2) u pr(n1) = pr(n2) \u2227 preterm(n1) \u2202(\u03bbxgx)\n\u2202\u03bb otherwise,\nwhere x is the symbol at n1, gx is defined in Equation 4 and u is the k-dimensional unit vector with the element corresponding to symbol x equal to 1 and all others equal to 0. The gradient in the third case is defined recursively,\n\u2202(\u03bbxgx)\n\u2202\u03bb = ugx + \u03bbx \u2202gx \u2202\u03bb\n= ugx + \u03bbx\n|n1|\u2211\ni=1\ngx \u03b1x + \u2206i \u2202\u2206i \u2202\u03bb .\nThe \u03b1 gradient is derived in a similar way,\n\u2202\u2206 \u2202\u03b1 =    0 pr(n1) 6= pr(n2) \u2228 preterm(n1)\n\u2202(\u03bbxgx) \u2202\u03b1 otherwise,\nand the gradient at the second case is also defined recursively,\n\u2202(\u03bbxgx)\n\u2202\u03b1 = \u03bbx \u2202gx \u2202\u03b1\n= \u03bbx\n|n1|\u2211\ni=1\ngx \u03b1x + \u2206i\n( u+\n\u2202\u2206i \u2202\u03b1\n) .\nGradients can be efficiently obtained using dynamic programming. In fact, they can be calculated at the same time as \u2206 to improve performance since they all share many terms in their derivations. Finally, we can easily obtain the gradients for the original SSTK by letting u = 1."}, {"heading": "3.3 Kernel Normalization", "text": "It is common practice when using tree kernels to normalize the kernel. This helps reduce the random effect of tree size. Normalization can be achieved using the following, where k\u0302 is the normalized kernel:\nk\u0302(t1, t2) = k(t1, t2)\u221a\nk(t1, t1)k(t2, t2) .\nTo apply this normalized version in the optimization procedure we must also derive gradients for the normalization function. In the following equation, we use kij and k\u0302ij as a shorthand for k(ti, tj) and k\u0302(ti, tj), respectively:\n\u2202k\u030212 \u2202\u03b8 = \u2202k12 \u2202\u03b8\u221a k11k22 \u2212 k\u030212 \u2202k11 \u2202\u03b8 k22 + k11 \u2202k22 \u2202\u03b8 2k11k22 ."}, {"heading": "3.4 Other Extensions", "text": "Many other structural kernels rely on recursive definitions and dynamic programming to perform their calculations. Examples include other tree kernels like the Partial Tree Kernel (Moschitti, 2006a) and string kernels like the ones defined on character ngrams (Lodhi et al., 2002) or word sequences (Cancedda et al., 2003). While in this paper we focus\non the SSTK (and our proposed SASSTK), our approach can easily be extended to these other kernels, as long as all the corresponding recursive definitions are differentiable."}, {"heading": "4 Synthetic Data Experiments", "text": "A natural question that arises in the proposed method is how much data is needed to accurately learn the kernel hyperparameters. To answer this question, we run a set of experiments using synthetic data. We generate this data by using a set of 1000 natural language syntactic trees, where we fix a random subset of 200 instances for testing and use the remaining 800 instances as training. For each training set size we define a GP over the full dataset, sample a function from it and use the function output as the response variable for each tree. We try two different GP priors, one using the SSTK and another one using the SASSTK.\nThe conditions above provide a controlled environment to check the modelling capacities of our approach since we know the exact distribution where the data comes from. The reasoning behind these experiments is that to be able to provide benefits in real tasks, where the data distribution is not known, our models have to be learnable in this controlled setting as well using a reasonable amount of data.\nFinally, we also provide an empirical evaluation comparing the speed performance between our approach and grid search."}, {"heading": "4.1 SSTK Prior", "text": "Our first experiments use a SSTK as the kernel with \u03bb = 0.001, \u03b1 = 1 and \u03c32n = 0.01. After obtaining the input trees and their sampled labels, we define a new GP model using only the training data plus the obtained response variables, this time using a SSTK with randomized hyperparameter values. Then we optimize the GP and check if the learned hyperparameters are close to the original ones, using 10 random restarts to limit the effect of local optima. We also use the optimized GP to predict response variables on the test set and measure Root Mean Squared Error (RMSE). Our hypothesis is that with a reasonable sample size we can retrieve the original hyperparameter values and obtain low RMSE. For each training set size, we repeat the experiment 20 times.\nFigure 2 shows the results of these experiments. For small sizes the variance in the resulting hyperparameter values is large but as soon as we reach 200 instances we are able to retrieve the original values with high confidence. In other words, in an ideal setting 200 instances are enough to learn the kernel. It is also interesting to note that test RMSE after optimization steadily decreases as we increase training data size. This shows that if one is more interested in predictions themselves, it is still worth optimizing hyperparameters even if the training data is small."}, {"heading": "4.2 SASSTK Prior", "text": "The large number of hyperparameters of the SASSTK makes it more prone to optimization and overfitting issues when compared to the SSTK. This raises the question of how much data is needed to justify its use. To address this question, we run similar experiments to those above for the SSTK, except that now we sample from a GP using a SASSTK as the kernel.\nInstead of optimizing all hyperparameters freely we use a simpler version where we tie \u03bb and \u03b1 for each symbol to the same value, except for the symbol \u2019S\u2019. Effectively this version has one extra \u03bb and one extra \u03b1 (henceforth \u03bbS and \u03b1S) when compared to the SSTK. The GP prior hyperparameter values are set to \u03bb = 0.001, \u03bbS = 0.5, \u03b1 = 0.1, \u03b1S = 1 and \u03c32n = 0.01. For each training set size, we train two GPs, one using this SASSTK and one using the original SSTK, optimize them using 10 random restarts and measure RMSE on the test set.\nResults are shown in Figure 3. For all training set sizes the SASSTK reaches lower RMSE than SSTK, with a substantial difference after reaching 100 instances. This shows that even for small datasets our proposed kernel manages to capture aspects which can not be explained by the original SSTK. Note that this is an ideal setting, and real datasets may need to be larger to realize gains from SASSTK. Nevertheless, these are promising results since they give evidence of a small lower bound on the dataset size for SASSTK to be effective."}, {"heading": "4.3 Performance Experiments", "text": "To provide an overview of how efficient is the gradient-based method compared to grid search we also run a set of experiments measuring wall clock training time vs. RMSE on a test set. For both GP and SVM models we employ the SSTK as the kernel and we use the same synthetic data from the previous experiments4. We perform 20 runs, keeping the test set as the same 200 instances for all runs and randomly sampling 200 instances from the remaining instances as training data.\nFigure 4 shows the curves for both GP and SVM models. The GP curve is obtained by increasing the maximum number of iterations of the gradient-based method (in this case, L-BFGS) and the SVM curve is obtained by increasing the granularity of the grid size.\nWe can see that optimizing the GP model is consistently much faster than doing grid search on the SVM model (notice the logarithmic scale), even though it shows some variance when letting L-BFGS run for a larger number of iterations. The GP model also is able to better predictions in general. Even when taking the variances into account, grid search would still need around 10 times more computation\n4For specific details on the SVM models used in all experiments performed in this paper we refer the reader to Appendix A.\ntime to achieve the same predictions obtained by the GP model. In real settings, SVMs predictions tend to be more on par with the ones provided by a GP (as shown in \u00a75) but nevertheless these figures show that the GP can be much more time efficient when optimizing hyperparameters of a tree kernel.\nAn important performance aspect to take into account is parallelization. Grid search is embarassingly parallelizable since each grid point can run in a different core. However, the GP optimization can also benefit from multiple cores by running each kernel computation inside the Gram matrix in parallel. To keep the comparisons simpler, the results shown in this section use a single core but all experiments in \u00a75 employ parallelization in the Gram matrix computation level (for both SVM and GP models)."}, {"heading": "5 NLP Experiments", "text": "Our experiments with NLP data address two regression tasks: Emotion Analysis and Quality Estimation. For both tasks, we use the Stanford parser (Manning et al., 2014) to obtain constituency trees for all sentences. Also, rather than using data official splits, we perform 5-fold cross-validation in order to obtain more reliable results."}, {"heading": "5.1 Emotion Analysis", "text": "The goal of Emotion Analysis is to automatically detect emotions in a text (Strapparava and Mihalcea, 2008). This problem is closely related to Opinion Mining (Pang and Lee, 2008), with similar applications, but it is usually done at a more fine-grained level and involves the prediction of a set of labels for each text (one for each emotion) instead of a single label.\nBeck et al. (2014a) used a multi-task GP for this task with a bag-of-words feature representation. In theory, it is possible to combine their multi-task kernel with our tree kernels, but to keep the focus of the experiments on testing tree kernel approaches, here we use independently trained models, one per emotion.\nDataset We use the dataset provided by the \u201cAffective Text\u201d shared task in SemEval2007 (Strapparava and Mihalcea, 2007), which is composed of 1000 news headlines annotated in terms of six emotions: Anger, Disgust, Fear, Joy, Sadness and Sur-\nprise. For each emotion, a score between 0 and 100 is given, 0 meaning total lack of emotion and 100, maximally emotional. Scores are mean-normalized before training the models.\nModels We perform experiments using the following tree kernels:\n\u2022 SSTK: the SSTK formulation introduced by Moschitti (2006b);\n\u2022 SASSTKfull: our proposed Symbol-Aware SSTK;\n\u2022 SASSTKS: same as before, but using only two \u03bb and two \u03b1 hyperparameters: one for symbols corresponding to full sentences5 and another for all other symbols. This configuration is similar to that in Section 4.2.\nFor all kernels, we also use a variation fixing the \u03b1 hyperparameters to 1 to emulate the original SSTK.\nBaselines and evaluation Our results are compared against three baselines:\n\u2022 SVM SSTK: a SVM using an SSTK kernel.\n\u2022 SVM BOW: same as before, but using an RBF kernel with a bag-of-words representation.\n\u2022 GP BOW: same as SVM BOW but using a GP instead.\nThe SVM models are trained using a wrapper for LIBSVM6 (Chang and Lin, 2001) provided by the scikit-learn toolkit7 (Pedregosa et al., 2011) and optimized via grid search. Following previous work, we use Pearson\u2019s correlation coefficient as evaluation metric. Pearson\u2019s scores are obtained by concatenating all six emotions outputs together.\nTable 2 shows the results. The best GP model with tree kernels outperforms the SVMs, showing that the fine-grained model selection procedure provided by the GP models is helpful when dealing with tree kernels. However, using the SASSTK models do not help in the case of free \u03b1 and the SASSTKfull actually performs worse than the original SSTK,\n5In this dataset, symbols are S, SQ, SBARQ and SINV . 6http://www.csie.ntu.edu.tw/\u02dccjlin/\nlibsvm 7http://scikit-learn.org\neven though the optimized marginal likelihood was higher. This is evidence that the SASSTKfull model is overfitting the training data, probably due to its large number of hyperparameters.\nAnother interesting finding in Table 2 is that fixing the \u03b1 values often harms performance. Inspecting the free \u03b1 models showed that the values found by the optimizer were very close to zero. This indicates that the model selection procedure prefer towards giving smaller weights to incomplete tree fragments. We can interpret this as the model selecting a more lexicalized feature space, which also explains why the GP RBF model on bag-of-words performed the best in this task.\nFinally, to understand how the optimized kernels could provide more interpretability, Table 3 shows the top 15 \u03bb values obtained by the SASSTKfull (fixed \u03b1 variant) with their corresponding symbols. In this specific case the kernel does not give the best performance so there are limitations in doing a full linguistic analysis. Nevertheless, we believe this example shows the potential for developing more interpretable kernels. This is especially interesting because these models take into account a much richer feature space than what it is allowed by parametric models."}, {"heading": "5.2 Quality Estimation", "text": "The goal of Quality Estimation is to provide a quality prediction for new, unseen machine translated texts (Blatz et al., 2004; Bojar et al., 2014). Exam-\nples of applications include filtering machine translated sentences that would require more post-editing effort than translation from scratch (Specia et al., 2009), selecting the best translation from different MT systems (Specia et al., 2010) or between an MT system and a translation memory (He et al., 2010), and highlighting segments that need revision (Bach et al., 2011). While various quality metrics exist, here we focus on post-editing time prediction.\nTree kernels have been used before in this task (with SVMs) by Hardmeier (2011) and Hardmeier et al. (2012). While their best models combine tree kernels with a set of explicit features, they also show good results using only the tree kernels. This makes Quality Estimation a good benchmark task to test our models.\nDatasets We use two publicly available datasets containing post-edited machine translated sentences. Both are composed of a set of source sentences, their machine translated outputs and the corresponding post-editing time.\n\u2022 French-English (fr-en): This dataset, described in (Specia, 2011), contains 2524 French sentences translated into English and postedited by a novice translator.\n\u2022 English-Spanish (en-es): This dataset was used in the WMT14 Quality Estimation shared task (Bojar et al., 2014), containing 858 sentences translated from English into Spanish and post-edited by an expert translator.\nFor each dataset, post-editing times are first divided by the translation output length (obtaining the post-editing time per word) and then mean normalized.\nModels Since our data consists of pairs of trees, our models in this task use a pair of tree kernels. We combine these two kernels by either summing or multiplying them. As for underlying tree kernels, we try both SSTK and SASSTKS . As in the Emotion Analysis task, we also experiment with a set of kernel configurations with the \u03b1 hyperparameters fixed at 1. We also test models that combine our tree kernels with an RBF kernel on a set of 17 features extracted using the QuEst framework (Specia et al., 2013). These features are part of a strong baseline model used by the WMT14 shared task.\nBaselines and evaluation We compare our results with a number of SVM models:\n\u2022 SVM SSTK: same as in the Emotion Analysis task, using either a sum (+) or a product (\u00d7) of SSTKs.\n\u2022 SVM RBF: this is an SVM trained on the 17 features extracted by Quest.\n\u2022 SVM RBF SSTK: a combination of the two models above.\nFor further comparison, we also show results obtained using a GP model and an RBF kernel on the QuEst-only features. Following previous work, we measure prediction performance using both Mean Absolute Error (MAE) and RMSE.\nThe prediction results are given in Table 4. They indicate a number of interesting findings:\n\u2022 For the fr-en dataset, the GP models combining tree kernels with an RBF kernel outperform all other models. Results for the en-es dataset are less consistent, probably due to the small size of the dataset, but on average they are better than their SVM counterparts.\n\u2022 The SVMs using a combination of kernels performs worse than using the RBF kernel alone. Inspecting the models, we found that grid search actually harms performance. For instance, for the fr-en dataset, MAE and RMSE for the RBF + SSTK \u00d7 model before grid search are 0.4681 and 0.6016, respectively. On the other hand, for this dataset all GP models achieve better results after optimization.\n\u2022 Unlike in the Emotion Analysis task, fixing \u03b1 results in better performance, even though the resulting models have lower marginal likelihood than the ones with free \u03b1. The same effect happened when comparing the SASSTK models with the SSTK ones for the en-es dataset. Both cases are evidence of model overfitting.\nWe also inspect the resulting hyperparameters to obtain insights about the features used by the model. Table 5 shows the optimized \u03bb values for the GP SSTK models with fixed \u03b1 for the fr-en dataset. The \u03bb values obtained are higher for the target sentence kernels than for the source sentence ones. We can interpret this as the model giving preference to features from the target trees instead of the source trees, which is what we would expect for this task."}, {"heading": "5.3 Overfitting", "text": "Both NLP tasks show evidence that the GP models with large number of hyperparameters (SASSTKfull in the case of Emotion Analysis and the free \u03b1 models in Quality Estimation) are overfitting the corresponding datasets. While the Bayesian formula-\ntion for the marginal likelihood does help limiting overfitting, it does not prevent it completely. Small datasets or invalid assumptions about the Gaussian distribution of the data may still lead to poorly fitting models. Another means of reducing overfitting is by taking a fully Bayesian approach in which hyperparameters are considered as random variables and are marginalized out (Osborne, 2010); this is a research direction we plan to pursue in the future.8"}, {"heading": "5.4 Extensions to Other Tasks", "text": "The GP framework introduced in Section 2 can be extended to non-regression problems by changing the likelihood function. For instance, models for classification (Rasmussen and Williams, 2006, Chap. 3), ordinal regression (Chu and Ghahramani, 2005) and structured prediction (Altun et al., 2004; Bratie\u0300res et al., 2013) were proposed in the literature. Since the likelihood is independent of the kernel, a natural future step is to apply the kernels and models introduced in this paper to different NLP tasks.\nIn light of that, we did initial experiments with constituency parsing reranking.9 The first results were inconclusive but we do believe this is because we employed naive approaches using classification (1-best result vs. all) and regression (using PARSEVAL metrics as the response variable) models. A more appropriate way to tackle this task is by employing a reranking-based likelihood and this is a direction we plan to pursue in the future."}, {"heading": "6 Related Work", "text": "Interest in model selection procedures for kernelbased methods has been growing in the last years.\n8See also Rasmussen and Williams (2006, Chap. 5) for an in-depth discussion on this issue.\n9We thank the anonymous reviewers for this suggestion.\nOne widely used approach for that is Multiple Kernel Learning (MKL) (Go\u0308nen and Alpayd\u0131n, 2011). MKL is based on the idea of using combinations of kernels to model the data and developing algorithms to tune the kernel coefficients. This is different from our method, where we focus on learning the hyperparameters of a single structural kernel. An approach similar to ours was proposed by Igel et al. (2007). They combine oligo kernels (a kind of ngram kernel) with MKL, derive their gradients and optimize towards a kernel alignment metric. Compared to our approach, they restrict the length of the n-grams being considered, while we rely on dynamic programming to explore the whole substructure space. Also, their method does not take into account the underlying learning algorithm. Another recent approach proposed for model selection is random search (Bergstra and Bengio, 2012). Like grid search, it has the drawback of not employing gradient information, as it is designed for any kind of hyperparameters (including categorical ones).\nStructural kernels have been successfully employed in a number of NLP tasks. The original SSTK proposed by Collins and Duffy (2001) was used to rerank the output of syntactic parsers. Recently, this reranking idea was also applied to discourse parsing (Joty and Moschitti, 2014). Other tree kernel applications include Semantic Role Labelling (Moschitti et al., 2008) and Relation Extraction (Plank and Moschitti, 2013). String kernels were mostly used in Text Classification (Lodhi et al., 2002; Cancedda et al., 2003), while graph kernels have been used for recognizing Textual Entailment (Zanzotto and Dell\u2019Arciprete, 2009). However, these previous works focused on frequentist methods like SVM or voted perceptron while we employ a Bayesian approach.\nGaussian Processes are a major framework in machine learning nowadays: applications include Robotics (Ko et al., 2007), Geolocation (Schwaighofer et al., 2004) and Computer Vision (Sinz et al., 2004). Only very recently they have been successfully employed in a few NLP tasks such as translation quality estimation (Cohn and Specia, 2013; Beck et al., 2014b), detection of temporal patterns in text (Preot\u0327iuc-Pietro and Cohn, 2013), semantic similarity (Rios and Specia, 2014) and emotion analysis (Beck et al., 2014a). In terms of feature\nrepresentations, previous work focused on the vectorial inputs and applied well-known kernels for these inputs, e.g. the RBF kernel. As shown on \u00a75.2, our approach is orthogonal to these previous ones, since kernels can be easily combined in different ways.\nIt is important to note that we are not the first ones to combine GPs with kernels on structured inputs. Driessens et al. (2006) employed a combination of GPs and graph kernels for reinforcement learning. However, unlike our approach, they did not attempt model selection, evaluating only a few hyperparameter values empirically."}, {"heading": "7 Conclusions", "text": "This paper describes a Bayesian approach for structural kernel learning, based on Gaussian Processes for easy model selection. Experiments applying our models to synthetic data showed that it is possible to learn structural kernel hyperparameters using a fairly small amount of data. Furthermore we obtained promising results in two NLP tasks, including Quality Estimation, where we beat the state of the art. Finally, we showed how these rich parameterizations can lead to more interpretable kernels.\nBeyond empirical improvements, an important goal of this paper is to present a method that enables new kernel developments through the extension of the number of hyperparameters. We focused on the Subset Tree Kernel, proposing an extension and then deriving its gradients. This approach can be applied to any structural kernel, as long as gradients are available. It is our hope that this work will serve as a starting point for future developments in these research directions."}, {"heading": "Acknowledgements", "text": "Daniel Beck was supported by funding from CNPq/Brazil (No. 237999/2012-9). Dr. Cohn is the recipient of an Australian Research Council Future Fellowship (project number FT130101105). The authors would also like to thank the three anonymous reviewers for their helpful comments and suggestions."}, {"heading": "A Details on SVM Baselines", "text": "All SVM baselines employ the -insensitve loss function. Grid search optimization is done via 3- fold cross-validation on the respective training set and use RMSE as the metric to be minimized. After obtained the best hyperparameter values, the SVM is retrained using these values on the full respective training set. The specific intervals used in grid search depend on the task.\nFor the performance experiments on synthetic data, we employed an interval of [10\u22122, 10] for C (regularization coefficient) and , [10\u22128, 1] for \u03bb and [10\u22124, 2] for \u03b1. In each run we incrementally increase the size of the grid by adding intermediate values on each interval. We keep a linear scale for the SSTK hyperparameters and a logarithmic scale for C and . As an example, Table 6 shows the resulting grids when the grid value is 4 for each hyperparameter. For all NLP experiments the grid is fixed for all hyperparameters (including \u03b3, the lengthscale value in the RBF kernel), with its corresponding values shown on Table 7."}, {"heading": "C / [10\u22122, 10\u22121, 1, 10]", "text": ""}], "year": 2015, "references": [{"title": "Goodness: A Method for Measuring Machine Translation Confidence", "authors": ["Nguyen Bach", "Fei Huang", "Yaser Al-Onaizan."], "venue": "Proceedings of ACL, pages 211\u2013 219.", "year": 2011}, {"title": "Joint Emotion Analysis via Multi-task Gaussian Processes", "authors": ["Daniel Beck", "Trevor Cohn", "Lucia Specia."], "venue": "Proceedings of EMNLP, pages 1798\u20131803.", "year": 2014}, {"title": "SHEF-Lite 2.0 : Sparse Multi-task Gaussian Processes for Translation Quality Estimation", "authors": ["Daniel Beck", "Kashif Shah", "Lucia Specia"], "venue": "In Proceedings of WMT14,", "year": 2014}, {"title": "Random Search for Hyper-Parameter Optimization", "authors": ["James Bergstra", "Yoshua Bengio."], "venue": "Journal of Machine Learning Research, 13:281\u2013305.", "year": 2012}, {"title": "Confidence estimation for machine translation", "authors": ["John Blatz", "Erin Fitzgerald", "George Foster."], "venue": "Proceedings of the 20th Conference on Computational Linguistics, pages 315\u2013321.", "year": 2004}, {"title": "Bayesian Structured Prediction using Gaussian Processes", "authors": ["S\u00e9bastien Brati\u00e8res", "Novi Quadrianto", "Zoubin Ghahramani."], "venue": "arXiv:1307.3846, pages 1\u2013", "year": 2013}, {"title": "Word-Sequence Kernels", "authors": ["Nicola Cancedda", "Eric Gaussier", "Cyril Goutte", "JeanMichel Renders."], "venue": "The Journal of Machine Learning Research, 3:1059\u20131082.", "year": 2003}, {"title": "LIBSVM : A Library for Support Vector Machines", "authors": ["Chih-Chung Chang", "Chih-Jen Lin."], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST), 2(3):1\u201339.", "year": 2001}, {"title": "Gaussian Processes for Ordinal Regression", "authors": ["Wei Chu", "Zoubin Ghahramani."], "venue": "Journal of Machine Learning Research, 6:1019\u20131041.", "year": 2005}, {"title": "Modelling Annotator Bias with Multi-task Gaussian Processes: An Application to Machine Translation Quality Estimation", "authors": ["Trevor Cohn", "Lucia Specia."], "venue": "Proceedings of ACL, pages 32\u201342.", "year": 2013}, {"title": "Convolution Kernels for Natural Language", "authors": ["Michael Collins", "Nigel Duffy."], "venue": "Proceedings of NIPS, pages 625\u2013632.", "year": 2001}, {"title": "Graph Kernels and Gaussian Processes for Relational Reinforcement Learning", "authors": ["Kurt Driessens", "Jan Ramon", "Thomas G\u00e4rtner."], "venue": "Machine Learning, 64(13):91\u2013119.", "year": 2006}, {"title": "Multiple kernel learning algorithms", "authors": ["Mehmet G\u00f6nen", "Ethem Alpayd\u0131n."], "venue": "Journal of Machine Learning Research, 12:2211\u20132268.", "year": 2011}, {"title": "Tree Kernels for Machine Translation Quality Estimation", "authors": ["Christian Hardmeier", "Joakim Nivre", "J\u00f6rg Tiedemann."], "venue": "Proceedings of WMT12, pages 109\u2013 113.", "year": 2012}, {"title": "Improving Machine Translation Quality Prediction with Syntactic Tree Kernels", "authors": ["Christian Hardmeier."], "venue": "Proceedings of EAMT, pages 233\u2013240.", "year": 2011}, {"title": "Convolution Kernels on Discrete Structures", "authors": ["David Haussler."], "venue": "Technical report, University of California at Santa Cruz.", "year": 1999}, {"title": "Bridging SMT and TM with Translation Recommendation", "authors": ["Yifan He", "Yanjun Ma", "Josef van Genabith", "Andy Way."], "venue": "Proceedings of ACL, pages 622\u2013 630.", "year": 2010}, {"title": "Gaussian Processes for Big Data", "authors": ["James Hensman", "Nicol\u00f2 Fusi", "Neil D. Lawrence."], "venue": "Proceedings of UAI, pages 282\u2013290.", "year": 2013}, {"title": "Gradient-based Optimization of Kernel-Target Alignment for Sequence Kernels Applied to Bacterial Gene Start Detection", "authors": ["Christian Igel", "Tobias Glasmachers", "Britta Mersch", "Nico Pfeifer."], "venue": "IEEE/ACM Transactions on Computational Biology and Bioinfor-", "year": 2007}, {"title": "Discriminative Reranking of Discourse Parses Using Tree Kernels", "authors": ["Shafiq Joty", "Alessandro Moschitti."], "venue": "EMNLP, pages 2049\u20132060.", "year": 2014}, {"title": "Gaussian Processes and Reinforcement Learning for Identification and Control of an Autonomous Blimp", "authors": ["Jonathan Ko", "Daniel J. Klein", "Dieter Fox", "Dirk Haehnel."], "venue": "Proceedings of IEEE International Conference on Robotics and Automation, pages", "year": 2007}, {"title": "Text Classification using String Kernels", "authors": ["Huma Lodhi", "Craig Saunders", "John Shawe-Taylor", "Nello Cristianini", "Chris Watkins."], "venue": "The Journal of Machine Learning Research, 2:419\u2013444.", "year": 2002}, {"title": "The Stanford CoreNLP Natural Language Processing Toolkit", "authors": ["Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky."], "venue": "Proceedings of ACL Demo Session, pages 55\u201360.", "year": 2014}, {"title": "Tree Kernels for Semantic Role Labeling", "authors": ["Alessandro Moschitti", "Daniele Pighin", "Roberto Basili."], "venue": "Computational Linguistics, pages 1\u201332.", "year": 2008}, {"title": "Efficient Convolution Kernels for Dependency and Constituent Syntactic Trees", "authors": ["Alessandro Moschitti."], "venue": "Proceedings of ECML, pages 318\u2013329.", "year": 2006}, {"title": "Making Tree Kernels practical for Natural Language Learning", "authors": ["Alessandro Moschitti."], "venue": "EACL, pages 113\u2013120.", "year": 2006}, {"title": "Bayesian Gaussian Processes for Sequential Prediction, Optimisation and Quadrature", "authors": ["Michael Osborne."], "venue": "Ph.D. thesis, University of Oxford.", "year": 2010}, {"title": "Opinion Mining and Sentiment Analysis", "authors": ["Bo Pang", "Lillian Lee."], "venue": "Foundations and Trends in Information Retrieval, 2(12):1\u2013135.", "year": 2008}, {"title": "Scikit-learn: Machine learning in Python", "authors": ["rot", "\u00c9douard Duchesnay"], "venue": "Journal of Machine Learning Research,", "year": 2011}, {"title": "On Reverse Feature Engineering of Syntactic Tree Kernels", "authors": ["Daniele Pighin", "Alessandro Moschitti."], "venue": "Proceedings of CONLL, pages 223\u2013233.", "year": 2010}, {"title": "Embedding Semantic Similarity in Tree Kernels for Domain Adaptation of Relation Extraction", "authors": ["Barbara Plank", "Alessandro Moschitti."], "venue": "Proceedings of ACL, pages 1498\u20131507.", "year": 2013}, {"title": "A temporal model of text periodicities using Gaussian Processes", "authors": ["Daniel Preo\u0163iuc-Pietro", "Trevor Cohn."], "venue": "Proceedings of EMNLP, pages 977\u2013988.", "year": 2013}, {"title": "Gaussian processes for machine learning, volume 1", "authors": ["Carl Edward Rasmussen", "Christopher K.I. Williams."], "venue": "MIT Press Cambridge.", "year": 2006}, {"title": "UoW : Multi-task Learning Gaussian Process for Semantic Textual Similarity", "authors": ["Miguel Rios", "Lucia Specia."], "venue": "Proceedings of SemEval, pages 779\u2013784.", "year": 2014}, {"title": "GPPS: A Gaussian Process Positioning System for Cellular Networks", "authors": ["Anton Schwaighofer", "Marian Grigoras", "Volker Tresp", "Clemens Hoffmann."], "venue": "Proceedings of NIPS, pages 579\u2013586.", "year": 2004}, {"title": "Learning Depth from Stereo", "authors": ["Fabian H. Sinz", "Joaquin Qui\u00f1onero Candela", "G\u00f6khan H. Bak\u0131r", "Carl E. Rasmussen", "Matthias O. Franz."], "venue": "Pattern Recognition, pages 1\u20138.", "year": 2004}, {"title": "Estimating the sentence-level quality of machine translation systems", "authors": ["Lucia Specia", "Nicola Cancedda", "Marc Dymetman", "Marco Turchi", "Nello Cristianini."], "venue": "Proceedings of EAMT, pages 28\u201335.", "year": 2009}, {"title": "Machine translation evaluation versus quality estimation", "authors": ["Lucia Specia", "Dhwaj Raj", "Marco Turchi."], "venue": "Machine Translation, 24(1):39\u201350.", "year": 2010}, {"title": "QuEst - A translation quality estimation framework", "authors": ["Lucia Specia", "Kashif Shah", "Jos\u00e9 G.C. De Souza", "Trevor Cohn."], "venue": "Proceedings of ACL Demo Session, pages 79\u201384.", "year": 2013}, {"title": "Exploiting Objective Annotations for Measuring Translation Post-editing Effort", "authors": ["Lucia Specia."], "venue": "Proceedings of EAMT, pages 73\u201380.", "year": 2011}], "id": "SP:aa825cbf9b034f8706f56a6935ec181fb2c7e4da", "authors": [{"name": "Daniel Beck", "affiliations": []}, {"name": "Trevor Cohn", "affiliations": []}, {"name": "Christian Hardmeier", "affiliations": []}, {"name": "Lucia Specia", "affiliations": []}], "abstractText": "Structural kernels are a flexible learning paradigm that has been widely used in Natural Language Processing. However, the problem of model selection in kernel-based methods is usually overlooked. Previous approaches mostly rely on setting default values for kernel hyperparameters or using grid search, which is slow and coarse-grained. In contrast, Bayesian methods allow efficient model selection by maximizing the evidence on the training data through gradient-based methods. In this paper we show how to perform this in the context of structural kernels by using Gaussian Processes. Experimental results on tree kernels show that this procedure results in better prediction performance compared to hyperparameter optimization via grid search. The framework proposed in this paper can be adapted to other structures besides trees, e.g., strings and graphs, thereby extending the utility of kernel-based methods.", "title": "Learning Structural Kernels for Natural Language Processing"}