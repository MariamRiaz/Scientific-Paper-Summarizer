0	34	Current research in anaphora (or coreference) resolution is focused on resolving noun phrases referring to concrete objects or entities in the real †Leo Born, Juri Opitz and Anette Frank contributed equally to this work.
1	27	world, which is arguably the most frequently occurring type.
2	20	Distinct from these are diverse types of abstract anaphora (AA) (Asher, 1993) where reference is made to propositions, facts, events or properties.
3	91	An example is given in (1) below.1 While recent approaches address the resolution of selected abstract shell nouns (Kolhatkar and Hirst, 2014), we aim to resolve a wide range of abstract anaphors, such as the NP this trend in (1), as well as pronominal anaphors (this, that, or it).
4	170	Henceforth, we refer to a sentence that contains an abstract anaphor as the anaphoric sentence (AnaphS), and to a constituent that the anaphor refers to as the antecedent (Antec) (cf.
8	30	A major obstacle for solving this task is the lack of sufficient amounts of annotated training data.
9	28	We propose a method to generate large amounts of training instances covering a wide range of abstract anaphor types.
68	38	Then hc,s is fed into a feed-forward layer of ELUs to obtain the final joint representation, h̃c,s, of the pair (c, s).
70	34	We train the described mention-ranking model with the max-margin training objective from Wiseman et al. (2015), used for the antecedent ranking subtask.
74	79	We create large-scale training data for abstract anaphora resolution by exploiting a common construction, consisting of a verb with an embedded sentence (complement or adverbial) (cf.
75	21	We detect this pattern in a parsed corpus, ’cut off’ the S′ constituent and replace it with a suitable anaphor to create the anaphoric sentence (AnaphS), while S yields the antecedent (Antec).
76	52	This method covers a wide range of anaphoraantecedent constellations, due to diverse semantic or discourse relations that hold between the clause hosting the verb and the embedded sentence.
78	50	In (4), the verb doubt establishes a specific semantic relation between the embedding sentence and its sentential complement.
90	35	For comparability we train and evaluate our model for shell noun resolution, using the original training (CSN) and test (ASN) corpus of Kolhatkar et al. (2013a,b).9 We follow the data preparation and evaluation protocol of Kolhatkar et al. (2013b) (KZH13).
115	22	Following KZH13, we report success@n (s@n), which measures whether the antecedent, or a candidate that differs in one word14, is in the first n ranked candidates, for n ∈ {1, 2, 3, 4}.
116	24	Additionally, we report the preceding sentence baseline (PSBL) that chooses the previous sentence for the antecedent and TAGbaseline (TAGBL) that randomly chooses a candidate with the constituent tag label in {S, VP, ROOT, SBAR}.
141	47	For the outlier reason we tuned HPs (on ARRAU-AA) for different variants of the architecture: the full architecture, without embedding of the context of the anaphor (ctx), of the anaphor (aa), of both constituent tag em- bedding and shortcut (tag,cut), dropping only the shortcut (cut), using only word embeddings as input (ctx,aa,tag,cut), without the first (ffl1) and second (ffl2) layer.
142	19	From Table 4 we observe: (1) with HPs tuned on ARRAU-AA, we obtain results well beyond KZH13, (2) all ablated model variants perform worse than the full model, (3) a large performance drop when omitting syntactic information (tag,cut) suggests that the model makes good use of it.
143	55	However, this could also be due to a bias in the tag distribution, given that all candidates stem from the single sentence that contains antecedents.
144	38	The median occurrence of the S tag among both antecedents and negative candidates is 1, thus the model could achieve 50.00 s@1 by picking S-type constituents, just as TAGBL achieves 42.02 for reason and 48.66 for possibility.
146	29	For example, without tuning the model with and without syntactic information achieves 71.27 and 19.68 (not shown in table) s@1 score, respectively, and with tuning: 87.78 and 68.10.
148	245	Table 5 shows the performance of different variants of the MR-LSTM with HPs tuned on the ASN corpus (always better than the default HPs), when evaluated on 3 different subparts of the ARRAUAA: all 600 abstract anaphors, 397 nominal and 203 pronominal ones.
149	44	HPs were tuned on the ASN corpus for every variant separately, without shuffling of the training data.
150	70	For the best performing variant, without syntactic information (tag,cut), we report the results with HPs that yielded the best s@1 test score for all anaphors (row 4), when training with those HPs on shuffled training data (row 5), and with HPs that yielded the best s@1 score for pronominal anaphors (row 6).
158	26	Contrary to shell noun resolution, omitting syntactic information boosts performance in ARRAUAA.
159	31	We conclude that when the model is provided with syntactic information, it learns to pick S-type candidates, but does not continue to learn deeper features to further distinguish them or needs more data to do so.
161	27	This is what we can observe from row 2 vs. row 6 in Table 5: the MR-LSTM without context embedding (ctx) achieves a comparable s@2 score with the variant that omits syntactic information, but better s@3-4 scores.
167	26	We finally analyze deeper aspects of the model: (1) whether a learned representation between the anaphoric sentence and an antecedent establishes a relation between a specific anaphor we want to resolve and the antecedent and (2) whether the maxmargin objective enforces a separation of the joint representations in the shared space.
187	20	By contrast, if the model is not provided with syntactic information, it learns deeper features that enable it to pick the correct antecedent without narrowing down the choice of candidates.
188	30	Thus, in order to improve performance, the model should be enforced to first select reasonable candidates and then continue to learn features to distinguish them, using a larger training set that is easy to provide.
