{"sections": [{"heading": "1. Motivation and Overview", "text": "This paper considers the general learning problem in which we have m observation vectors X1, . . . , Xm \u2208 Rn, with matching response values y1, . . . , ym \u2208 R. Each response yi is a possibly noisy evaluation of an unknown function f : Rn \u2192 R at Xi, that is, yi = f(Xi) + ei, where ei \u2208 R represents the noise or measurement error. The goal is to estimate f by some f\u0302 : Rn \u2192 R such that f\u0302(Xi) is a good fit for yi, that is, |f\u0302(Xi) \u2212 yi| tends to be small. The estimate f\u0302 may then be used to predict the response value y corresponding to a newly encountered observation x \u2208 Rn through the prediction y\u0302 = f\u0302(x). A classical linear regression model is one simple example of the many possible techniques one might employ for constructing f\u0302 . The classical regression approach to this problem is to posit\n1Management Science and Information Systems, Rutgers University, Piscataway, NJ, USA 2Department of Management, Bar-Ilan University, Ramat Gan, Israel 3Doctoral Program in Operations Research, Rutgers University, Piscataway, NJ, USA. Correspondence to: Jonathan Eckstein <jeckstei@business.rutgers.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\na particular functional form for f\u0302(x) (for example, an affine function of x) and then use an optimization procedure to estimate the parameters in this functional form.\nHere, we are interested in cases in which a concise candidate functional form for f\u0302 is not readily apparent, and we wish to estimate f\u0302 by searching over a very highdimensional space of parameters. For example, Breiman (2001) proposed the method of random forests, which constructs f\u0302 by training regression trees on multiple random subsamples of the data, and then averaging the resulting predictors. Another proposal is the RuleFit algorithm (Friedman & Popescu, 2008), which enhances L1regularized regression by generating box-based rules to use as additional explanatory variables. Given a, b \u2208 Rn with a \u2264 b, the rule function r(a,b) : Rn \u2192 {0, 1} is given by\nr(a,b)(x) = I ( \u2227j\u2208{1,...,n}(aj \u2264 xj \u2264 bj) ) , (1)\nthat is r(a,b)(x) = 1 if a \u2264 x \u2264 b (componentwise) and r(a,b)(x) = 0 otherwise. RuleFit generates rules through a two-phase procedure: first, it determines a regression tree ensemble, and then decomposes these trees into rules and determines the regression model coefficients (including for the rules).\nThe approach of Dembczyn\u0301ski et al. (2008a) generates rules more directly (without having to rely on an initial ensemble of decision trees) within gradient boosting (Friedman, 2001) for non-regularized regression. In this scheme, a greedy procedure generates the rules within a gradient descent method runs that for a predetermined number of iterations. Aho et al. (2012) extended the RuleFit method to solve more general multi-target regression problems. For the special case of single-target regression, however, their experiments suggest that random forests and RuleFit outperform several other methods, including their own extended implementation and the algorithm of Dembczyn\u0301ski et al. (2008a). Compared with random forests and other popular learning approaches such as kernel-based methods and neural networks, rule-based approaches have the advantage of generally being considered more accessible and easier to interpret by domain experts. Rule-based methods also have a considerable history in classification settings, as in for example Weiss & Indurkhya (1993), Cohen & Singer\n(1999), and Dembczyn\u0301ski et al. (2008b).\nHere, we propose an iterative optimization-based regression procedure called REPR (Rule-Enhanced Penalized Regression). Its output models resemble those of RuleFit, but our methodology draws more heavily on exact optimization techniques from the field of mathematical programming. While it is quite computationally intensive, its prediction performance appears promising. As in RuleFit, we start with a linear regression model (in this case, with L1-penalized coefficients to promote sparsity), and enhance it by synthesizing rules of the form (1). We incrementally adjoin such rules to our (penalized) linear regression model as if they were new observation variables. Unlike RuleFit, we control the generation of new rules using the classical mathematical programming technique of column generation. Our employment of column generation roughly resembles its use in the LPBoost ensemble classification method of Demiriz et al. (2002).\nColumn generation involves cyclical alternation between optimization of a restricted master problem (in our case a linear or convex quadratic program) and a pricing problem that finds the most promising new variables to adjoin to the formulation. In our case, the pricing problem is equivalent to an NP-hard combinatorial problem we call Rectangular Maximum Agreement (RMA), which generalizes the Maximum Mononial Agreement (MMA) problem as formulated and solved by Eckstein & Goldberg (2012). We solve the RMA problem by a similar branch-and-bound method procedure, implemented using parallel computing techniques.\nTo make our notation below more concise, we let X denote the matrix whose rows are X>1 , . . . , X > m, and also let y = (y1, . . . , ym) \u2208 Rm. We may then express a problem instance by the pair (X, y). We also let xij denote the (i, j)th element of this matrix, that is, the value of variable j in observation i."}, {"heading": "2. A Penalized Regression Model with Rules", "text": "Let K be a set of pairs (a, b) \u2208 Rn \u00d7 Rn with a \u2264 b, constituting a catalog of all the possible rules of the form (1) that we wish to be available to our regression model. The set K will typically be extremely large: restricting each aj and bj to values that appear as xij for some i, which is sufficient to describe all possible distinct behaviors of rules of the form (1) on the dataset X , there are still\u220fn j=1 `j(`j + 1)/2 \u2265 3n possible choices for (a, b), where\n`j = | \u22c3m i=1{xij}| is the number of distinct values for xij .\nThe predictors f\u0302 that our method constructs are of the form\nf\u0302(x) = \u03b20 + n\u2211 j=1 \u03b2jxj + \u2211 k\u2208K \u03b3krk(x) (2)\nfor some \u03b20, \u03b21, . . . , \u03b2n, (\u03b3k)k\u2208K \u2208 R. Finding an f\u0302 of this form is a matter of linear regression, but with the regression coefficients in a space with the potentially very high dimension of 1 +n+ |K|. As is now customary in regression models in which the number of explanatory variables potentially outnumbers the number of observations, we employ a LASSO-class model in which all explanatory variables except the constant term have L1 penalties. Letting \u03b2 = (\u03b21, . . . , \u03b2n) \u2208 Rn and \u03b3 \u2208 R|K|, let f\u03b20,\u03b2,\u03b3( \u00b7 ) denote the predictor function in (2). We then propose to estimate \u03b20, \u03b2, \u03b3 by solving\nmin \u03b20,\u03b2,\u03b3 { m\u2211 i=1 |f\u03b20,\u03b2,\u03b3(Xi)\u2212 yi| p + C \u2016\u03b2\u20161 + E \u2016\u03b3\u20161 } ,\n(3) where p \u2208 {1, 2} and C,E \u2265 0 are scalar parameters. For p = 2 and C = E > 0, this model is essentially the classic LASSO as originally proposed by Tibshirani (1996).\nTo put (3) into a more convenient form for our purposes, we split the regression coefficient vectors into positive and negative parts, so \u03b2 = \u03b2+ \u2212 \u03b2\u2212 and \u03b3 = \u03b3+ \u2212 \u03b3\u2212, with \u03b2+, \u03b2\u2212 \u2208 Rn+ and \u03b3+, \u03b3\u2212 \u2208 R |K| + . Introducing one more vector of variables \u2208 Rm, the model shown as (4) in Figure 1 is equivalent to (3). The model is constructed so that i = |f\u03b20,\u03b2,\u03b3(Xi)\u2212 yi| for i = 1, . . . ,m. If p = 1, the model is a linear program, and if p = 2 it is a convex, linearly constrained quadratic program. In either case, there are 2m constraints (other than nonnegativity), but the number of variables is 1 +m+ 2n+ 2 |K|.\nBecause of this potentially unwieldy number of variables, we propose to solve (4) by using the classical technique of column generation, which dates back to Ford & Fulkerson (1958) and Gilmore & Gomory (1961); see for example Section 7.3 of Griva et al. (2009) for a recent textbook treatment. In brief, column generation cycles between solving two optimization problems, the restricted master problem and the pricing problem. In our case, the restricted master problem is the same as (4), but with K replaced by some (presumably far smaller) K \u2032 \u2286 K. We initially choose K \u2032 = \u2205. Solving the restricted master problem yields optimal Lagrange multipliers \u03bd \u2208 Rm+ and \u00b5 \u2208 Rm+ (for the constraints other than simple nonnegativity). For each rule k \u2208 K, these Lagrange multipliers yield respective reduced costs rc[\u03b3+k ], rc[\u03b3 \u2212 k ] for the variables \u03b3 + k , \u03b3 \u2212 k that are in the master problem, but not the restricted master. One then solves the pricing problem, whose job is to identify the smallest of these reduced costs. The reduced cost rc[v] of a variable v indicates the rate of change of the objective function as one increases v away from 0. If the smallest reduced\nm\u2211 n\u2211 \u2211\ncost is nonnegative, then clearly all the reduced costs are nonnegative, which means that the current restricted master problem yields an optimal solution to the master problem by setting \u03b3+k = \u03b3 \u2212 k = 0 for all k \u2208 K\\K \u2032, and the process terminates. If the smallest reduced cost is negative, we adjoin elements to K \u2032, including at least one corresponding to a variable \u03b3+k or \u03b3 \u2212 k with a negative reduced cost, and we repeat the process, re-solving the expanded restricted master problem.\nIn our case, the reduced costs take the form\nrc[\u03b3+k ] = E \u2212 m\u2211 i=1 rk(xi)\u03bdi + m\u2211 i=1 rk(xi)\u00b5i\nrc[\u03b3\u2212k ] = E + m\u2211 i=1 rk(xi)\u03bdi \u2212 m\u2211 i=1 rk(xi)\u00b5i\nand hence we have for each k \u2208 K that\nmin { rc[\u03b3+k ], rc[\u03b3 \u2212 k ] } = E \u2212 \u2223\u2223\u2223\u2223\u2223 m\u2211 i=1 rk(xi)(\u03bdi \u2212 \u00b5i) \u2223\u2223\u2223\u2223\u2223 . (5) Therefore, the pricing problem may be solved by maximizing the second term on the right-hand side of (5), that is, finding\nz\u2217 = max k\u2208K \u2223\u2223\u2223\u2223\u2223 m\u2211 i=1 rk(xi)(\u03bdi \u2212 \u00b5i) \u2223\u2223\u2223\u2223\u2223 , (6) and the stopping condition for the column generation procedure is z\u2217 \u2264 E. This problem turns out to be equivalent to the RMA problem, whose formulation and solution we now describe."}, {"heading": "3. The RMA Problem", "text": ""}, {"heading": "3.1. Formulation and Input Data", "text": "Suppose we have m observations and n explanatory variables, expressed using a matrix X \u2208 Rm\u00d7n as above. Each observation i \u2208 {1, . . . ,m} is assigned a nonegative weight wi \u2208 R+. For any set S \u2286 {1, . . . ,m},\nlet w(S) = \u2211 i\u2208S wi. We also assume we are given a partition of the observations into two subsets, a \u201cpositive\u201d subset \u2126+ \u2282 {1, . . . ,m} and a \u201cnegative\u201d subset \u2126\u2212 = {1, . . . ,m}\\\u2126+.\nGiven two vectors a, b \u2208 Rn, let B(a, b) denote the \u201cbox\u201d {x \u2208 Zn | a \u2264 x \u2264 b}. Given the input data X , the coverage CvrX(a, b) of B(a, b) consists of the indices of the observations from X falling within B(a, b), that is,\nCvrX(a, b) = {i \u2208 {1, . . . ,m} | a \u2264 Xi \u2264 b} .\nThe rectangular maximum agreement (RMA) problem is\nmax \u2223\u2223w(\u2126+ \u2229 CvrX(a, b))\u2212 w(\u2126\u2212 \u2229 CvrX(a, b))\u2223\u2223 s.t. a, b \u2208 Rn, (7) with decision variables a, b \u2208 Rn. Essentially implicit in this formulation is the constraint that a \u2264 b, since if a 6\u2264 b then CvrX(a, b) = \u2205 and the objective value is 0. The previously mentioned MMA problem is the special case of RMA in which all the observations are binary, X \u2208 {0, 1}m\u00d7n. Since the MMA problem is NPhard (Eckstein & Goldberg, 2012), so is RMA.\nIf we take K to be the set of all possible boxes on Rn, the pricing problem (6) may be reduced to RMA by setting\n(\u2200 i = 1, . . . ,m) : wi = |\u03bdi \u2212 \u00b5i| (8)\n\u2126+ = {i \u2208 {1, . . . ,m} | \u03bdi \u2265 \u00b5i } , (9)\nand thus \u2126\u2212 = {i \u2208 {1, . . . ,m} | \u03bdi < \u00b5i }."}, {"heading": "3.2. Preprocessing and Restriction to N", "text": "Any RMA problem instance may be converted to an equivalent instance in which all the observation data are integer. Essentially, for each coordinate j = 1, . . . , n, one may simply record the distinct values of xij and replace each xij with its ordinal position among these values. Algorithm 1, with its parameter \u03b4 set to 0, performs exactly this procedure, outputing a equivalent data matrix X \u2208 Nm\u00d7n and a vector ` \u2208 Nn whose jth element is `j = | \u22c3m i=1{xij}|\nAlgorithm 1 Preprocessing discretization algorithm 1: Input: X \u2208 Rm\u00d7n, \u03b4 \u2265 0 2: Output: X \u2208 Nm\u00d7n, ` \u2208 Nn 3: ProcessData 4: for j = 1 to n do 5: `j \u2190 0 6: Sort x1j , . . . , xmj and set (k1, . . . , km) such that\nxk1j \u2264 xk2j \u2264 \u00b7 \u00b7 \u00b7 \u2264 xkmj 7: x\u0304k1,j \u2190 0 8: for i = 1 to m\u2212 1 do 9: if xki+1j \u2212 xkij > \u03b4 \u00b7 (xkmj \u2212 xk1j) then\n10: `j \u2190 `j + 1 11: end if 12: x\u0304ki+1j \u2190 `j 13: end for 14: `j \u2190 `j + 1 15: end for 16: return (X, `)\nas defined in the previous section. Algorithm 1\u2019s output values x\u0304ij for attribute j vary between 0 and `j \u2212 1.\nThe number of distinct values `j of each explanatory variable j directly influences the difficulty of RMA instances. To obtain easier instances, Algorithm 1 can combine its \u201cintegerization\u201d process with some binning of nearby values. Essentially, if the parameter \u03b4 is positive, the algorithm bins together consecutive values xij that are within relative tolerance \u03b4, resulting in a smaller number of distinct values `j for each explanatory variable j.\nSome datasets contain both categorical and numerical data. In addition to Algorithm 1, we also convert each k-way categorical attribute into k \u2212 1 binary attributes.\nWithin the context of our REPR regression method, we set the RMA weight vector and data partition as in (8)-(9), integerize the data X using Algorithm 1 with some (small) parameter value \u03b4, solve the RMA problem, and then translate the resulting boxes back to the original, pre-integerized coordinate system. We perform this translation by expanding box boundaries to lie halfway between the boundaries of the clusters of points grouped by Algorithm 1, except when the lower boundary of the box has the lowest possible value or the upper boundary has the largest possible value. In these cases, we expand the box boundaries to \u2212\u221e or +\u221e, respectively. More precisely, for each observation variable j and v \u2208 {0, . . . , `j \u2212 1}, let xminj,v be the smallest value of xij assigned to the integer value v by Algorithm 1, and xmaxj,v be the largest. If a\u0302, b\u0302 \u2208 Nn, a\u0302 \u2264 b\u0302 describe an integerized box arising from the solution of the preprocessed RMA problem, we choose the corresponding box boundaries a, b \u2208 Rn in the original coordinate system\nto be given by, for j = 1, . . . , n,\naj = { \u2212\u221e, if a\u0302j = 0 1 2 (x max j,a\u0302j\u22121 + x min j,a\u0302j ), otherwise\nbj = { +\u221e, if b\u0302j = `j \u2212 1 1 2 (x max j,b\u0302j + xmin j,b\u0302j+1 ), otherwise.\nOverall, our procedure is equivalent to solving the pricing problem (6) over some set of boxes K = K\u03b4(X). For \u03b4 = 0, the resulting set of boxesK0(X) is such that the corresponding set of rules {rk | k \u2208 K0(X)} comprises every box-based rule distinguishable on the dataset X . For small positive values of \u03b4, the set of boxes K\u03b4(X) excludes those corresponding to rules that \u201ccut\u201d between very closely spaced observations."}, {"heading": "3.3. Branch-and-Bound Subproblems", "text": "In this and the following two subsections, we describe the key elements of our branch-and-bound procedure for solving the RMA problem, assuming that the data X have already been preprocessed as above. For brevity, we omit some details which will instead be covered in a forthcoming publication. For general background on branch-andbound algorithms, Morrison et al. (2016) provide a recent survey with numerous citations.\nBranch-and-bound methods search a tree of subproblems, each describing some subset of the search space. In our RMA method, each subproblem P is characterized by four vectors a(P ), a(P ), b(P ), b(P ) \u2208 Nn, and represents search space subset consisting of vector pairs (a, b) for which a(P ) \u2264 a \u2264 a(P ) and b(P ) \u2264 b \u2264 b(P ). Any valid subproblem conforms to a(P ) \u2264 a(P ), b(P ) \u2264 b(P ), a(P ) \u2264 b(P ), and a(P ) \u2264 b(P ). The root problem R of the branch-and-bound tree is R = (0, ` \u2212 1,0, ` \u2212 1), where where ` \u2208 Nn is as output from Algorithm 1, and 0 and 1 respectively denote the vectors (0, 0, . . . , 0) \u2208 Nn and (1, 1, . . . , 1) \u2208 Nn."}, {"heading": "3.4. Inseparability and the Bounding Function", "text": "In branch-and-bound methods, the bounding function provides an upper bound (when maximizing) on the best possible objective value in the region of the search space corresponding to a subproblem. Our bounding function is based on an extension of the notion of inseparability developed by Eckstein & Goldberg (2012). Consider any subproblem P = (a, a, b, b) and two observations i and i\u2032. If xij = xi\u2032j or aj \u2264 xij , xi\u2032j \u2264 bj for each j = 1, . . . , n, then xi, xi\u2032 \u2208 Nn are inseparable with respect to a, b \u2208 Nn, in the sense that any box B(a, b) with a \u2264 a and b \u2265 b must either cover both of xi, xi\u2032 or neither of them.\nInseparability with respect to a, b is an equivalence relation,\nand we denote the equivalence classes it induces among the observation indices 1, . . . ,m by E(a, b). That is, observation indices i and i\u2032 are in the same equivalence class of E(a, b) if xi and xi\u2032 are inseparable with respect to a, b.\nOur bounding function \u03b2(a, a, b, b) for each subproblem P = (a, a, b, b) is shown in (10) in Figure 2. The reasoning behind this bound is that each possible box in the set specified by (a, a, b, b) must either cover or not cover the entirety of each C \u2208 E(a, b). The first argument to the \u201cmax\u201d operation reflects the situation that every equivalence class C with a positive net weight is covered, and no classes with negative net weight are covered; this is the best possible situation if the box ends up covering a higher weight of positive observations than of negative. The second \u201cmax\u201d argument reflects the opposite situation, the best possible case in which the box covers a greater weight of negative observations than of positive ones."}, {"heading": "3.5. Branching", "text": "The branching scheme of a branch-and-bound algorithm divides subproblems into smaller ones in order to improve their bounds. In our case, branching a subproblem P = (a, a, b, b) involves choosing an explanatory variable j \u2208 {1, . . . , n} and a cutpoint v \u2208 {aj , . . . , bj \u2212 1} \u2208 Nn.\nThere are three possible cases, the first of which is when bj < aj and v \u2208 {bj , . . . , aj \u2212 1}. In this case, our scheme creates three children based on the disjunction that either bj \u2264 v \u2212 1 (the box lies below v), aj \u2264 v \u2264 bj (the box straddles v), or aj \u2265 v+ 1 (the box lies above v). The next case is that v \u2208 { aj , . . . ,min{aj , bj} \u2212 1 } , in which case the box cannot lie below v and we split P into two children based on the disjunction that either aj \u2264 v (the box straddles v) or aj \u2265 v + 1 (the box is above v). The third case occurs when v \u2208 { max{aj , bj}, . . . , bj\u22121 } , in which case we split P into two children based on the disjunction that either bj \u2264 v (the box is does not extend above v) or bj \u2265 v+ 1 (the box extends above v). If no v falling under one of these three cases exists for any dimension j, then the subproblem represents a single possible box, that is, a = a and b = b. Such a subproblem is a terminal node of the branch-and-bound tree, and in this case we simply compute the RMA objective value for a = a = a and b = b = b as the subproblem bound.\nWhen more than one possible variable-cutpoint pair (j, v) exists, as is typically the case, our algorithm must select one. We use two related procedures for branching selection: strong branching and cutpoint caching. In strong branching, we simply experiment with all applicable variable-cutpoint pairs (j, v), and select one that the maximizes the minimum bound of the resulting two or three children. This is a standard technique in branch-and-bound algorithms, and involves evaluating the bounds of all the potential children of the current search node. To make this process as efficient as possible, we have developed specialized data structures for manipulating equivalence classes, and we analyze the branching possibilities in a particular order. In cutpoint caching, some subproblems use strong branching, while others select from a list of cutpoints that were chosen by strong branching for previously processed search nodes. The details of these procedures will be covered in a forthcoming companion publication."}, {"heading": "4. Full Algorithm and Implementation", "text": "The pseudocode in Algorithm 2 describes our full REPR column generation procedure for solving (4), using the RMA preprocessing and branch-and-bound methods described above to solve the pricing problem. Several points bear mentioning: first, the nonnegative scalar parameter \u03b8 allows us to incorporate a tolerance into the column generation stopping criterion, so that we terminate when all reduced costs exceed \u2212\u03b8 instead of when all reduced costs are nonnegative. This kind of tolerance is customary in column generation methods. The tolerance \u03b4, on the other hand, controls the space of columns searched over. Furthermore, our implementation of the RMA branch-and-bound algorithm can identify any desired number t \u2265 1 of the best possible RMA solutions, as opposed to just one value of k attaining the maximum in (11). This t is also a parameter to our procedure, so at each iteration of Algorithm 2 we may adjoin up to t new rules to K \u2032. Adding multiple columns per iteration is a common technique in column generation methods. Finally, the algorithm has a parameter S specifying a limit on the number of column generation iterations, meaning that at the output model will contain at most St rules.\nWe implemented the algorithm in C++, using the GuRoBi\nAlgorithm 2 REPR: Rule-enhanced penalized regression 1: Input: data X \u2208 Rm\u00d7n, y \u2208 Rm, penalty parameters C,E \u2265 0, column generation tolerance \u03b8 \u2265 0, integer t \u2265 1, aggregation tolerance \u03b4 \u2265 0, iteration limit S\n2: Output: \u03b20 \u2208 R, \u03b2 \u2208 Rn,K \u2032 \u2282 K\u03b4(X), \u03b3 \u2208 R|K \u2032| 3: REPR 4: K \u2032 \u2190 \u2205 5: for s = 1, . . . , S do 6: Solve the restricted master problem to obtain opti-\nmal primal variables (\u03b20, \u03b2+, \u03b2\u2212, \u03b3+, \u03b3\u2212) and dual variables (\u03bd, \u00b5)\n7: Use the RMA branch-and-bound algorithm, with preprocessing as in Algorithm 1, to identify a t-best solution k1, . . . , kt to\nmax k\u2208K\u03b4(X) \u2223\u2223\u2223\u2223\u2223 m\u2211 i=1 rk(xi)(\u03bdi \u2212 \u00b5i) \u2223\u2223\u2223\u2223\u2223 , (11) with k1, . . . , kt having respective objective values z1 \u2265 z2 \u2265 \u00b7 \u00b7 \u00b7 \u2265 zt\n8: if z1 \u2264 E + \u03b8 break 9: for each l \u2208 {1, . . . , t} with zl > E + \u03b8 do\n10: K \u2032 \u2190 K \u2032 \u222a {kl} 11: end for 12: end for 13: return (\u03b20, \u03b2 := \u03b2+ \u2212 \u03b2\u2212,K \u2032, \u03b3 := \u03b3+ \u2212 \u03b3\u2212)\ncommercial optimizer (Gurobi Optimization, 2016) to solve the restricted master problems. We implemented the RMA algorithm using using the PEBBL C++ class library (Eckstein et al., 2015), an open-source C++ framework for parallel branch and bound. PEBBL employs MPIbased parallelism (Gropp et al., 1994). Since solving the RMA pricing problem is by far the most time-consuming part of Algorithm 2, we used true parallel computing only in that portion of the algorithm. The remainder of the algorithm, including solving the restricted master problems, was executed in serial and redundantly on all processors."}, {"heading": "5. Preliminary Testing of REPR", "text": "For preliminary testing of REPR, we selected 8 datasets from the UCI repository (Lichman, 2013), choosing small datasets with continuous response variables. The first four columns of Table 1 summarize the number of observations m, the number of attributes n, and the maximum number of distinguishable box-based rules |K0(X)| for these data sets.\nIn our initial testing, we focused on the p = 2 case in which fitting errors are penalized quadratically, and set t = 1, that is, we added one model rule per REPR iteration. We set the iteration limit S to 100 and effectively set the termination\ntolerance \u03b8 so that REPR terminated when z1 \u2264 max { 0, E \u00b7 ( |E[y]| \u2212 0.1\u03c3[y] )} + 0.001,\nwhere E[y] denotes the sample mean of the response variable and \u03c3[y] its sample standard deviation. We found this rule of thumb to work well in pracice, but it likely merits further study. We also chose C = 1 and E = 1. We used \u03b4 = 0 for SERVO, YACHT, and MPG, and \u03b4 = 0.005 for the remaining datasets.\nWith the fixed parameters given above, we tested REPR and some competing regression procedures on ten different randomly-chosen partitions of each dataset; each partition consists of 80% training data and 20% testing data. The competing procedures are RuleFit, random forests, LASSO, and classical linear regression. The penalty parameter in LASSO is the same as the value of C chosen for REPR. To implement RuleFit and random forests, we used their publicly available R packages. Table 2 shows the averages of the resulting mean square errors and Table 3 shows their standard deviations. REPR has the smallest average MSE for 5 of the 8 datasets and has the second smallest average MSE on the remaining 3 datasets, coming very close to random forests on MPG. For the standard deviation of the MSE, which we take as a general measure of prediction stability, REPR has the lowest values for 6 of the 8 datasets. The box plots in Figures 3 and 4 visualize these results in more detail for HEAT and MACHINE, respectively. Figure 5 displays the average MSEs in a bar-chart format, with the MSE of REPR normalized to 1.\nFigures 6-9 give more detailed information for specific datasets. Figure 6 and 7 respectively show how REPR\u2019s prediction MSEs for HEAT and CONCRETE evolve with each iteration, with each data point averaged over the 10 different REPR runs; the horizontal lines indicate the average MSE level for the competing procedures. MSE generally declines as REPR adds rules, although some diminish-\ning returns are evident for CONCRETE. Interestingly, neither of these figures shows appreciable evidence of overfitting by REPR, even when large numbers of rules are incorporated into the model. Figures 8 and 9 display testing-set predictions for specific (arbitrarily chosen) partitions of the MACHINE and CONCRETE datasets, respectively, with the observations sorted by response value. REPR seems to outperform the other methods in predicting extreme response values, although it is somewhat worse than the other methods at predicting non-extreme values for MACHINE.\nThe last two columns of Table 1 show, for a 16-core Xeon E5-2660 workstation, REPR\u2019s average total run time per data partition and the average number of search node per invocation of RMA. The longer runs could likely be accelerated by the application of more parallel processors."}, {"heading": "6. Conclusions and Future Research", "text": "The results presented here suggest that REPR has significant potential as a regression tool, at least for small datasets. Clearly, it should be tested on more datasets and larger datasets.\nHere, we have tested REPR using fixed values of most of its parameters, and we expect we should be able to improve its performance by using intelligent heuristics or crossvalidation procedures to select key parameters such as C andE. Improved preprocessing may also prove helpful: judicious normalization of the input data (X, y) should assist in finding good parameter choices, and we are also working on more sophisticated discretization technique for preprocessing the RMA solver input, as well as branch selection heuristics that are more efficient for large `j .\nIt would be interesting to see how well REPR performs if the pricing problems are solved less exactly. For example, one could use various techniques for truncating the branchand-bound search, such as setting a limit on the number of subproblems explored or loosening the conditions for pruning unpromising subtrees. Or one could use, perhaps selectively, some entirely heuristic procedure to identify rules to add to the restricted master problem.\nFor problems with large numbers of observations m, it is conceivable that solving the restricted master problems could become a serial bottleneck in our current implementation strategy. If this phenomenon is observed in practice, it could be worth investigating parallel solution strategies for the restricted master."}], "year": 2017, "references": [{"title": "Multi-target regression with rule ensembles", "authors": ["References Aho", "Timo", "\u017denko", "Bernard", "D\u017eeroski", "Sa\u0161o", "Elomaa", "Tapio"], "venue": "J. Mach. Learn. Res.,", "year": 2012}, {"title": "A simple, fast, and effective rule learner", "authors": ["Cohen", "William W", "Singer", "Yoram"], "venue": "In Proc. of the 16th Nat. Conf. on Artificial Intelligence,", "year": 1999}, {"title": "Solving regression by learning an ensemble of decision rules", "authors": ["Dembczy\u0144ski", "Krzysztof", "Kot\u0142owski", "Wojciech", "S\u0142owi\u0144ski", "Roman"], "venue": "In International Conference on Artificial Intelligence and Soft Computing,", "year": 2008}, {"title": "Maximum likelihood rule ensembles", "authors": ["Dembczy\u0144ski", "Krzysztof", "Kot\u0142owski", "Wojciech", "S\u0142owi\u0144ski", "Roman"], "venue": "In Proceedings of the 25th International Conference on Machine Learning,", "year": 2008}, {"title": "Linear programming boosting via column generation", "authors": ["Demiriz", "Ayhan", "Bennett", "Kristin P", "Shawe-Taylor", "John"], "venue": "Mach. Learn.,", "year": 2002}, {"title": "An improved branch-and-bound method for maximum monomial agreement", "authors": ["Eckstein", "Jonathan", "Goldberg", "Noam"], "venue": "INFORMS J. Comput.,", "year": 2012}, {"title": "PEBBL: an object-oriented framework for scalable parallel branch and bound", "authors": ["Eckstein", "Jonathan", "Hart", "William E", "Phillips", "Cynthia A"], "venue": "Math. Program. Comput.,", "year": 2015}, {"title": "A suggested computation for maximal multi-commodity network flows", "authors": ["Ford", "Jr.", "Lester R", "Fulkerson", "David R"], "venue": "Manage. Sci.,", "year": 1958}, {"title": "Greedy function approximation: a gradient boosting machine", "authors": ["Friedman", "Jerome H"], "venue": "Ann. of Stat., pp", "year": 2001}, {"title": "Predictive learning via rule ensembles", "authors": ["Friedman", "Jerome H", "Popescu", "Bogdan E"], "venue": "Ann. Appl. Stat.,", "year": 2008}, {"title": "A linear programming approach to the cutting-stock problem", "authors": ["Gilmore", "Paul C", "Gomory", "Ralph E"], "venue": "Oper. Res.,", "year": 1961}, {"title": "Linear and Nonlinear Optimization", "authors": ["Griva", "Igor", "Nash", "Stephen G", "Sofer", "Ariela"], "venue": "SIAM, second edition,", "year": 2009}, {"title": "Using MPI: Portable Parallel Programming with the Message-Passing Interface", "authors": ["Gropp", "William", "Lusk", "Ewing", "Skjellum", "Anthony"], "year": 1994}, {"title": "Branch-and-bound algorithms: a survey of recent advances in searching, branching, and pruning", "authors": ["Morrison", "David R", "Jacobson", "Sheldon H", "Sauppe", "Jason J", "Sewell", "Edward C"], "venue": "Discrete Optim.,", "year": 2016}, {"title": "Regression shrinkage and selection via the lasso", "authors": ["Tibshirani", "Robert"], "venue": "J. R. Statist. Soc. B,", "year": 1996}, {"title": "Optimized rule induction", "authors": ["Weiss", "Sholom M", "Indurkhya", "Nitin"], "venue": "IEEE Expert,", "year": 1993}], "id": "SP:9cedd186edb19fdb632e0cde02d8195dc77ec440", "authors": [{"name": "Jonathan Eckstein", "affiliations": []}, {"name": "Noam Goldberg", "affiliations": []}, {"name": "Ai Kagawa", "affiliations": []}], "abstractText": "We describe a procedure enhancingL1-penalized regression by adding dynamically generated rules describing multidimensional \u201cbox\u201d sets. Our rule-adding procedure is based on the classical column generation method for highdimensional linear programming. The pricing problem for our column generation procedure reduces to the NP-hard rectangular maximum agreement (RMA) problem of finding a box that best discriminates between two weighted datasets. We solve this problem exactly using a parallel branch-and-bound procedure. The resulting rule-enhanced regression method is computation-intensive, but has promising prediction performance. 1. Motivation and Overview This paper considers the general learning problem in which we have m observation vectors X1, . . . , Xm \u2208 R, with matching response values y1, . . . , ym \u2208 R. Each response yi is a possibly noisy evaluation of an unknown function f : R \u2192 R at Xi, that is, yi = f(Xi) + ei, where ei \u2208 R represents the noise or measurement error. The goal is to estimate f by some f\u0302 : R \u2192 R such that f\u0302(Xi) is a good fit for yi, that is, |f\u0302(Xi) \u2212 yi| tends to be small. The estimate f\u0302 may then be used to predict the response value y corresponding to a newly encountered observation x \u2208 R through the prediction \u0177 = f\u0302(x). A classical linear regression model is one simple example of the many possible techniques one might employ for constructing f\u0302 . The classical regression approach to this problem is to posit Management Science and Information Systems, Rutgers University, Piscataway, NJ, USA Department of Management, Bar-Ilan University, Ramat Gan, Israel Doctoral Program in Operations Research, Rutgers University, Piscataway, NJ, USA. Correspondence to: Jonathan Eckstein <jeckstei@business.rutgers.edu>. Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s). a particular functional form for f\u0302(x) (for example, an affine function of x) and then use an optimization procedure to estimate the parameters in this functional form. Here, we are interested in cases in which a concise candidate functional form for f\u0302 is not readily apparent, and we wish to estimate f\u0302 by searching over a very highdimensional space of parameters. For example, Breiman (2001) proposed the method of random forests, which constructs f\u0302 by training regression trees on multiple random subsamples of the data, and then averaging the resulting predictors. Another proposal is the RuleFit algorithm (Friedman & Popescu, 2008), which enhances L1regularized regression by generating box-based rules to use as additional explanatory variables. Given a, b \u2208 R with a \u2264 b, the rule function r(a,b) : R \u2192 {0, 1} is given by r(a,b)(x) = I ( \u2227j\u2208{1,...,n}(aj \u2264 xj \u2264 bj) ) , (1) that is r(a,b)(x) = 1 if a \u2264 x \u2264 b (componentwise) and r(a,b)(x) = 0 otherwise. RuleFit generates rules through a two-phase procedure: first, it determines a regression tree ensemble, and then decomposes these trees into rules and determines the regression model coefficients (including for the rules). The approach of Dembczy\u0144ski et al. (2008a) generates rules more directly (without having to rely on an initial ensemble of decision trees) within gradient boosting (Friedman, 2001) for non-regularized regression. In this scheme, a greedy procedure generates the rules within a gradient descent method runs that for a predetermined number of iterations. Aho et al. (2012) extended the RuleFit method to solve more general multi-target regression problems. For the special case of single-target regression, however, their experiments suggest that random forests and RuleFit outperform several other methods, including their own extended implementation and the algorithm of Dembczy\u0144ski et al. (2008a). Compared with random forests and other popular learning approaches such as kernel-based methods and neural networks, rule-based approaches have the advantage of generally being considered more accessible and easier to interpret by domain experts. Rule-based methods also have a considerable history in classification settings, as in for example Weiss & Indurkhya (1993), Cohen & Singer Rule-Enhanced Penalized Regression by Column Generation using Rectangular Maximum Agreement (1999), and Dembczy\u0144ski et al. (2008b). Here, we propose an iterative optimization-based regression procedure called REPR (Rule-Enhanced Penalized Regression). Its output models resemble those of RuleFit, but our methodology draws more heavily on exact optimization techniques from the field of mathematical programming. While it is quite computationally intensive, its prediction performance appears promising. As in RuleFit, we start with a linear regression model (in this case, with L1-penalized coefficients to promote sparsity), and enhance it by synthesizing rules of the form (1). We incrementally adjoin such rules to our (penalized) linear regression model as if they were new observation variables. Unlike RuleFit, we control the generation of new rules using the classical mathematical programming technique of column generation. Our employment of column generation roughly resembles its use in the LPBoost ensemble classification method of Demiriz et al. (2002). Column generation involves cyclical alternation between optimization of a restricted master problem (in our case a linear or convex quadratic program) and a pricing problem that finds the most promising new variables to adjoin to the formulation. In our case, the pricing problem is equivalent to an NP-hard combinatorial problem we call Rectangular Maximum Agreement (RMA), which generalizes the Maximum Mononial Agreement (MMA) problem as formulated and solved by Eckstein & Goldberg (2012). We solve the RMA problem by a similar branch-and-bound method procedure, implemented using parallel computing techniques. To make our notation below more concise, we let X denote the matrix whose rows are X 1 , . . . , X > m, and also let y = (y1, . . . , ym) \u2208 R. We may then express a problem instance by the pair (X, y). We also let xij denote the (i, j)th element of this matrix, that is, the value of variable j in observation i. 2. A Penalized Regression Model with Rules Let K be a set of pairs (a, b) \u2208 R \u00d7 R with a \u2264 b, constituting a catalog of all the possible rules of the form (1) that we wish to be available to our regression model. The set K will typically be extremely large: restricting each aj and bj to values that appear as xij for some i, which is sufficient to describe all possible distinct behaviors of rules of the form (1) on the dataset X , there are still \u220fn j=1 `j(`j + 1)/2 \u2265 3 possible choices for (a, b), where `j = | \u22c3m i=1{xij}| is the number of distinct values for xij . The predictors f\u0302 that our method constructs are of the form f\u0302(x) = \u03b20 + n \u2211", "title": "Rule-Enhanced Penalized Regression by Column Generation  using Rectangular Maximum Agreement"}