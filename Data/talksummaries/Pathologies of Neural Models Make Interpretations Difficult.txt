0	49	Many interpretation methods for neural networks explain the model’s prediction as a counterfactual: how does the prediction change when the input is modified?
2	17	A common, non-adversarial form of model interpretation is feature attribution: features that are crucial for predictions are highlighted in a heatmap.
7	17	Both perturbation and gradient-based methods can generate heatmaps, implying that the model’s prediction is highly influenced by the highlighted, important words.
28	18	For each word in an input sentence, we measure its importance by the change in the confidence of the original prediction when we remove that word from the sentence.
31	37	(1) To calculate the importance of each word in a sentence with n words, we need n forward passes of the model, each time with one of the words left out.
39	31	Intuitively, the important words should remain after the unimportant ones are removed.
40	19	Our input reduction process iteratively removes the unimportant words.
52	23	In VQA, each example consists of an image and a natural language question.
55	46	During the iterative reduction process, we ensure that the prediction does not change (exact same span for SQUAD); consequently, the model accuracy on the reduced examples is identical to the original.
66	18	Figure 4 compares the model’s confidence on original and reduced inputs.
78	60	The workers’ choice is almost fifty-fifty (the vs. Random in Table 1): the reduced examples appear almost random to humans.
79	19	These results leave us with two puzzles: why are the models highly confident on the nonsensical reduced examples?
80	65	And why, when the leave-oneout method selects important words that appear reasonable to humans, the input reduction process selects ones that are nonsensical?
86	18	Neural models are also overconfident on examples outside the training data distribution.
87	46	As Goodfellow et al. (2015) observe for image classification, samples from pure noise can sometimes trigger highly confident predictions.
88	179	These socalled rubbish examples are degenerate inputs that a human would trivially classify as not belonging to any class but for which the model predicts with high confidence.
89	65	Goodfellow et al. (2015) argue that the rubbish examples exist for the same reason that adversarial examples do: the surprising linear nature of neural models.
90	44	In short, the confidence of a neural model is not a robust estimate of its prediction uncertainty.
93	15	The nonsensical, almost random results are best explained by looking at a complete reduction path (Figure 5).
94	15	In this example, the transition from valid to rubbish happens immediately after the first step: following the removal of “Broncos”, humans can no longer determine which team the question is asking about, but model confidence remains high.
96	58	In this example, the leave-one-out method will not highlight “Broncos”.
98	28	The model assigns a low importance to “Broncos” in the first step, causing it to be removed—leave-one-out would be able to expose this particular issue by not highlighting “Broncos”.
99	21	However, in cases where a similar issue only appear after a few unimportant words are removed, the leave-one-out method would fail to expose the unreasonable model behavior.
102	48	We call this first-order sensitivity, because interpretation based on input gradient is a first-order Taylor expansion of the model near the input (Simonyan et al., 2014).
106	18	The heatmap shifts when, with respect to the removed word, the model has low first-order sensitivity but high second-order sensitivity.
114	94	When the model composes representations by a non-linear combination of words, a linear interpretation oblivious to second-order sensitivity can be misleading.
118	82	To maximize model uncertainty on reduced examples, we use the entropy of the output distribution as an objective.
119	27	Given a model f trained on a dataset (X ,Y), we generate reduced examples using input reduction for all training examples X .
126	15	In the SQUAD example from Figure 1, the reduced question changed from “did” to “spend Astor money on ?” after fine-tuning.
129	46	Human accuracy increases across all three tasks (Table 3).
