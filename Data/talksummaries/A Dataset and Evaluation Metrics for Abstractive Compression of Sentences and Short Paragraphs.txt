0	84	Automated sentence compression condenses a sentence or paragraph to its most important content in order to enhance writing quality, meet document length constraints, and build more accurate document summarization systems (Berg-Kirkpatrick et al., 2011; Vanderwende et al., 2007).
1	114	Though word deletion is extensively used (e.g., (Clarke and Lapata, 2008)), state-of-the-art compression models (Cohn and Lapata, 2008; Rush et al., 2015) benefit crucially from data that can represent complex abstractive compression operations, including substitution of words and phrases and reordering.
2	142	In the first half, we introduce a manually-created multi-reference dataset for abstractive compression of sentences and short paragraphs, with the following features: • It contains approximately 6,000 source texts with multiple compressions (about 26,000 pairs of source and compressed texts), representing business letters, newswire, journals, and technical documents sampled from the Open American National Corpus (OANC1).
3	24	• Each source text is accompanied by up to five crowd-sourced rewrites constrained to a preset compression ratio and annotated with quality judgments.
4	86	Multiple rewrites permit study of the impact of operations on human compression quality and facilitate automatic evaluation.
5	26	• This dataset is the first to provide compressions at the multi-sentence (two-sentence paragraph) level, which may present a stepping stone to whole document summarization.
34	21	We sampled single sentences and two-sentence paragraphs from several genres in the written text section of the Manually Annotated Sub-Corpus (MASC) (Ide et al., 2008; Ide et al., 2010) of the Open American National Corpus (OANC), supplemented by additional data from the written section of OANC.
43	68	Generating compressions: In the first round, we asked five workers (editors) to abridge each source text by at least 25%, while remaining grammatical and fluent, and retaining the meaning of the original.
71	15	Overall, agreement of experts with the average crowdsourced ratings is moderate (approaching substantial) for meaning, and fair for grammar.
80	34	Of the compressions for two-sentence paragraphs, 72.4% had two sentences in the output, 0.4% had one sentence deleted, and 27.3% had the two source sentences merged.
81	23	Impact of operations: Because the dataset contains multiple compressions of the same sources, we are able to estimate the impact of different editing operations.
88	23	Reordering has no significant impact on meaning, but leads to substantial degradation in grammatically.
94	75	The goal of this analysis is to develop an understanding of the performance of automatic evaluation metrics for text compression, and the factors contributing to their performance.
97	22	Prior work on compression evaluation has indicated that a parsebased metric is superior to one based on surface substrings (Clarke and Lapata, 2006), but the contribution of the linguistic units has not been isolated, and surface n-gram units have otherwise been successfully used for evaluation in related tasks (Graham, 2015).
98	34	Accordingly, we empirically compare metrics based on surface uni-grams (LR-1), bi-grams (LR-2), tri-grams (LR-3), and four-grams (LR-4), as well skip bi-grams (with a maximum of four intervening words as in ROUGE-S4) (SKIP-2), and dependency tree triples obtained from collapsed dependencies output from the Stanford parser (PARSE2).7 The second criterion is the scoring measure used to evaluate the match between two sets of linguistic units corresponding to a system output and a reference compression.
112	16	We distinguish three methods for aggregating information from multiple references: MULT-MAX which uses the single reference out of a set that results in the highest single-reference score, and two further methods, MULT-ALL and MULT-PROB, that construct an aggregate linguistic unit vector Φ(r1, .
113	15	MULT-ALL is the standard method used in multi-reference BLEU, where the vector for a set of references is defined as the union of the features of the set.
115	29	MULT-PROB, a new method that we propose here, is motivated by the observation that although judgments of importance of content are subjective, the more annotators assert some information is important, the more this information should contribute to the matching score.9 In MULT-PROB we define the weight of a linguistic unit in the combined reference vector as the proportion of references that include the unit.
117	25	For the purpose of analysis, we trained and evaluated four compression systems.
118	47	These include both deletion-based and abstractive models: (1) ILP, an integer linear programing approach for deletionbased compression (Clarke and Lapata, 2008), (2) T3, a tree transducer-based model for abstractive compression (Cohn and Lapata, 2008), (3) Seq2seq, a neural network model for deletion-based compression (Filippova et al., 2015), and (4) NAMAS, a neural model for abstractive compression and summarization (Rush et al., 2015).
140	28	A deletion-based model, it uses the deletionbased subset of our training dataset and the deletionbased subset from the external data in Table 7.
152	27	Even though the performance of some systems is similar, the differences between all pairs of systems in meaning and grammar are significant p < 0.0001 according to a paired t-test.
153	14	It is interesting to note that ILP outperforms the more recently developed neural network systems Seq2Seq and NAMAS.
155	17	We note however that performance on the test corpus in our study might not substantially improve through the use of large automatically mined data-sets of headlines and corresponding news article sentences, due to differences in genre and domain.
167	23	For each pair of systems A and B, and each metric m, we compute the difference in quality for corresponding system outputs for each input xi: m(oiA) −m(oiB) and the difference in quality according to human judgments: Q(oiA) − Q(oiB), and compute the correlation between these two sequences.
169	13	For each pair of metrics m1 and m2, and for each pair of systems A and B, we compute the statistical significance of the difference between the Pearson correlations of these metrics with human judgements.
188	27	We see that precision-based metrics are substantially dominated by metrics that incorporate recall, except for grammar evaluation.
191	49	Finally, we observe that standard BLEU metrics and ROUGE-L were not competitive.
192	40	We have introduced a large manually collected multi-reference abstractive dataset and quantified the impact of editing operations and context on human compression quality, showing that substitution and rephrasing operations are more meaning preserving than other operations, and that compression in context improves quality.
193	40	Further, in the first systematic study of automatic evaluation metrics for text compression, we have demonstrated the importance of utilizing multiple references and suitable linguistic units, and incorporating recall.
