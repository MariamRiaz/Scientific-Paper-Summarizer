3	30	In order to make accurate (local) attachment decisions, historically, TBDPs have required a large set of features in order to access rich information about particular positions in the stack and buffer of the current parser configuration.
4	22	But consulting many positions means that although polynomial-time exact-decoding algo- rithms do exist, having been introduced by Huang and Sagae (2010) and Kuhlmann et al. (2011), unfortunately, they are prohibitively costly in practice, since the number of positions considered can factor into the exponent of the running time.
6	33	As an extreme case, Dyer et al. (2015) use an LSTM to summarize arbitrary information on the stack, which completely rules out dynamic programming.
7	20	Recently, Kiperwasser and Goldberg (2016a) and Cross and Huang (2016a) applied bidirectional long short-term memory networks (Graves and Schmidhuber, 2005, bi-LSTMs) to derive feature representations for parsing, because these networks capture wide-window contextual information well.
10	36	Our minimal feature set plugs into Huang and Sagae’s and Kuhlmann et al.’s dynamic program- 12 ming framework to produce the first implementation of Opn3q exact decoders for arc-hybrid and arc-eager parsers.
11	16	We also enable and implement Opn3q global training methods.
13	117	TBDPs incrementally process a sentence by making transitions through search states representing parser configurations.
15	97	To featurize configurations for use in a scoring function, it is common to have features that extract information about the first several elements on the stack and the buffer, such as their word forms and part-of-speech (POS) tags.
30	25	, ÑÐ wns, where each ÑÐ wi is the output of the bi-LSTM at time step i.
34	59	Finally, as in Chen and Manning (2014), we use a multilayer perceptron to score possible transitions from the given configuration, where the input is the concatenation of some selection of the ÑÐ s js and ÑÐ b ks.
36	28	Table 1 reports the parsing accuracy that results for feature sets of size four, three, two, and one for three commonly-used transition systems.
45	47	Formally, a transition system is defined as S “ pC, T, cs, Cτ q, where C is a nonempty set of configurations, each t P T : C á C is a transition function between configurations, cs is an initialization function that maps an input sentence to an initial configuration, and Cτ Ď C is a set of terminal configurations.
46	49	All systems we consider share a common tripartite representation for configurations: when we write c “ pσ, β,Aq for some c P C, we are referring to a stack σ of partially parsed subtrees; a buffer β of unprocessed tokens and, optionally, at its beginning, a subtree with only left descendants; and a set A of elements ph,mq, each of which is an attachment (dependency arc) with head h and modifier m.4 We write mðh to indicate that m left-modifies h, and hñm to indicate thatm rightmodifies h. For a sentence w “ w1, ..., wn, the initial configuration is pσ0, β0, A0q, where σ0 and A0 are empty and β0 “ rROOT|w1, ..., wns; ROOT is a special node denoting the root of the parse tree5 (vertical bars are a notational convenience for indicating different parts of the buffer or stack; our convention is to depict the buffer first element leftmost, and to depict the stack first element rightmost).
47	56	All terminal configurations have an empty buffer and a stack containing only ROOT.
49	190	The three transitions, shift (sh, move a token from the buffer to the stack), right-reduce (reñ, reduce and attach a right modifier), and left-reduce (reð, reduce and attach a left modifier), are defined as: shrpσ, b0|β,Aqs “ pσ|b0, β, Aq reñrpσ|s1|s0, β, Aqs “ pσ|s1, β, AY tps1, s0quq reðrpσ|s1|s0, β, Aqs “ pσ|s0, β, AY tps0, s1quq Arc-Hybrid The arc-hybrid system (Yamada and Matsumoto, 2003; Gómez-Rodrı́guez et al., 2008; Kuhlmann et al., 2011) has the same definitions of sh and reñ as arc-standard, but forces the collection of left modifiers before right modifiers via its b0-modifier reð transition.
54	39	Kuhlmann et al. (2011) reformulate the three transition systems just discussed as deduction systems (Pereira and Warren, 1983; Shieber et al., 1995), wherein transitions serve as inference rules; these are given as the lefthand sides of the first three subfigures in Figure 1.
55	56	For a given w “ w1, ..., wn, assertions take the form ri, j, ks (or, when applicable, a two-index shorthand to be discussed soon), meaning that there exists a sequence of transitions that, starting from a configuration wherein headps0q “ wi, results in an ending configuration wherein headps0q “ wj and headpb0q “ wk.
62	25	Since an Opn4q running time is not sufficiently practical even in the simple-feature case, in the remainder of this paper we consider only the archybrid and arc-eager systems, not arc-standard.
71	42	However, our use of minimal feature sets enables direct computation of an argmax over the entire space of transition sequences, argmaxt F ptq, via dynamic programming, because our positions don’t rely on any information “outside” the deduction rule indices, thus eliminating the need for ad- ditional state-keeping.
74	29	The left-reduce rule says that we can first take the sequence of transitions asserted by rkb, is, which has a score of v1, and then a shift transition moving wi from b0 to s0.
80	26	Computing this max can again be done efficiently with a slight modification to the scoring of reduce transitions: rkb, is : v1 ri0, js : v2 rkb, js : v1 ` v2 `∆1 preðq where ∆1 “ ∆ ` 1 pheadpwiq ‰ wjq.
131	47	Of the three types of global models, the arceager arguably has the edge, an empirical finding resonating with our theoretical comparison of their model expressiveness.
132	65	Comparison with State-of-the-Art Models Figure 2 compares our algorithms’ results with those of the state-of-the-art.9 Our models are competitive and an ensemble of 15 globallytrained models (5 models each for arc-eager DP, arc-hybrid DP and edge-factored) achieves 95.33 and 90.22 on PTB and CTB, respectively, reaching the highest reported UAS on the CTB dataset, and the second highest reported on the PTB dataset among dependency-based approaches.
141	21	• The bi-LSTM-powered feature set tÑÐs0, ÑÐ b 0u is minimal yet highly effective for arc-hybrid and arc-eager transition-based parsing.
142	83	• Since DP algorithms for exact decoding (Huang and Sagae, 2010; Kuhlmann et al., 2011) have a run-time dependence on the number of positional features, using our mere two effective positional features results in a running time of Opn3q, feasible for practice.
143	56	• Combining exact decoding with global training — which is also enabled by our minimal feature set — with an ensemble of parsers achieves 90.22 UAS on the Chinese Treebank and 95.33 UAS on the Penn Treebank: these are, to our knowledge, the best and secondbest results to date on these data sets among “purely” dependency-based approaches.
145	81	Two possibilities are to create even better training methods, and to find some way to extend our run-time improvements to other transition systems.
146	17	It would also be interesting to further investigate relationships between graph-based and dependency-based parsing.
148	36	In our work, we have brought exact decoding, which was formerly the province solely of graphbased parsing, to the transition-based paradigm.
149	88	We hope that the future will bring more inspiration from an integration of the two perspectives.
