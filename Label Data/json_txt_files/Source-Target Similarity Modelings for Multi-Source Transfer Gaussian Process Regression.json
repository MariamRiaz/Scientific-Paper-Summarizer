{"sections": [{"heading": "1. Introduction", "text": "Transfer learning (TL) methods show specially appealing for real-world applications where the data from the target domain is scarce but a good amount of data from another source domain is available. With research efforts largely confined to the single-source setting (Pan et al., 2011; Wei et al., 2016; Zhou et al., 2016), an increasing amount of studies are contributing to a realistic applicability of TL by addressing the multi-source scenario, mainly for classifica-\n1School of Computer Science and Engineering, Nanyang Technological University, Singapore 2Rolls-Royce@Nanyang Technological University Corporate Lab 3Rolls-Royce Advanced Technology Centre, Singapore. Correspondence to: Pengfei Wei <Pwei001@e.ntu.edu.sg>, Ramon Sagarna <saramon@ntu.edu.sg>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\ntion (Tommasi et al., 2014; Fang et al., 2015; Bhatt et al., 2016). The problem of regression, however, has been much less studied, despite of the variety of real-world domains in which it arises; for instance wifi or indoor signal location (Pan et al., 2008), biological data analysis (Lam et al., 2016), or mechanical system design (Ghosh et al., 2015). In this work, we concentrate on multi-source transfer regression (MSTR) based on Gaussian process (GP) models.\nAll the way through, the TL community has been paying attention to modeling the similarity between different domains so that only the source knowledge that is helpful for the target domain is transferred. This is because designing a TL method based on the assumption that domains are mutually relevant may lead to negative transfer (Pan & Yang, 2010). Similarity capture is particularly crucial in multi-source TL as the transfer capacity to the target task may differ considerably across the diverse source domains. Thus, TL methods that are capable of tuning the strength of the knowledge transfer to the similarity of the domains are attracting increasing interest (Luo et al., 2008; Wang et al., 2014; Al-Stouhi & Reddy, 2011).\nAs regards to MSTR, a key issue is to capture the diverse Source-Target (S-T) similarities. The relatively few efforts to date have focused on ensemble methods. Particularly, an amount of works rely on the boosting strategy due to its capability to capture fine-grained S-T similarities be weighting the contribution of train instances individually (Dai et al., 2007; Pardoe & Stone, 2010; Yao & Doretto, 2010). However, as outlined in (Al-Stouhi & Reddy, 2011), such an instance-based similarity strategy in boosting has shown issues with slow/premature weights convergence that have seriously penalized the computational cost or the transfer performance. Another type of ensemble strategy for multisource transfer is stacking (Wolpert, 1992). Pardoe and Stone propose a meta-model that aggregates the predictions of several base models previously learned with each source in isolation (Pardoe & Stone, 2010) . The aggregation is done by assigning each base model a model importance. In this case, the S-T similarities can be captured through the model importance. However, in such stacking-based methods, the base models suffer from a lack of consideration of the dependencies between the different source domains.\nAnother popular idea to model the S-T similarity is to construct a transfer covariance function that relates two data points from distinct domains through the similarity coefficients (Bonilla et al., 2008; Williams et al., 2009). Such idea has been proposed in multi-task learning (Bonilla et al., 2008), where each task pair is assigned a particular similarity coefficient. Note, however, that multi-task learning differs from the TL problem in that the former aims at improving performance across all the domains while the objective of the latter focuses on the target domain only. Nevertheless, the idea of transfer covariance function is referential for the TL problem. In (Cao et al., 2010), a single source transfer covariance function (TCSS) was proposed. In the corresponding transfer covariance matrix, one similarity coefficient was assigned to the S-T block to model the inter-domain similarity. A GP with such TCSS (called GP-TCSS) was then trained for the transfer task.\nWhen generalizing to MSTR, one may naturally consider a multi-source transfer covariance function (TCMS) with different similarity coefficients attached to distinct S-T blocks in the corresponding transfer covariance matrix. In this work, we investigate the feasibility of such covariance function. We theoretically prove that a general GP with TCMS (GP-TCMS) fails to capture the similarity diversities of various S-T domain pairs. Although TCMS intends to utilize different similarity coefficients, the learnt GPTCMS would give the same similarity coefficient for all the S-T domain pairs. The generalization error bounds of the learnt GP-TCMS show that this coefficient is taking effect in every source domain. Considering the diverse S-T similarities between the sources and the target, this may jeopardize the transfer performance, especially when the number of sources increases. Moreover, the learning of GPTCMS rapidly poses a computational issue with increasing amounts of source domains as usually O(m3) computations are required to evaluate a model for m data points.\nThe unsatisfactory performance of GP-TCMS leads us to exploit the transfer covariance function in another way. Considering that both the stacking strategy and the transfer covariance function can model the S-T similarity and using the transfer covariance function at the base models would therefore add flexibility to the similarity capture capability of the stacking approach, we propose to integrate them into one unified model. Specifically, we first discuss TCSSStack, a method that simply stacks GP-TCSS base models. TCSSStack alleviates the computational issue of GP since it allows to stretch the number of sources due to its O(Nn3) cost for N sources with n points each. However, TCSSStack still suffers from the aforementioned limitation of conventional stacking. Thus, we propose a more involved TCMSStack. Two salient features make TCMSStack significantly different from TCSSStack: (i) it associates the similarity coefficients in the base GP-TCSS\nwith the model importance during learning, and (ii) it learns the model importance and the base GP-TCSS jointly. By doing so, on the one hand, TCMSStack further reduces the computational cost by lowering the number of optimization variables. On the other hand, although the similarity coefficient in TCMSStack represents bivariate S-T similarity relations, they are elicited by pondering all the inter-domain dependencies. In the experiments, we show the superiority of TCMSStack on the transfer performance compared to TCSSStack, GP-TCMS , and other MSTR methods."}, {"heading": "2. Related Work", "text": "A main challenge in MSTR is to precisely capture the diverse S-T transfer capacities across the different sources. Ensemble approaches (Dai et al., 2007), which can provide an explicit, fine-grained similarity capture, are widely used to handle the MSTR problems. In (Pardoe & Stone, 2010), TrAdaBoost.R2 was proposed, a boosting based algorithm that weights the contribution of train instances individually, and thus delivers a model accounting for the S-T similarities for every instance. However, such boostinglike methods suffer from slow/premature convergence issues that tremendously jeopardize the transfer performance (Al-Stouhi & Reddy, 2011). Pardoe and Stone also introduced a multi-source transfer stacking in which base models are pretrained in different source domains separately, and a meta-model is trained by aggregating the outputs of the base models (Pardoe & Stone, 2010). By doing so, the S-T similarities are captured at meta-model level by the learnt model importance. Although the stacking methods show success in some MSTR problems, they have the limitation that inter-domain dependencies between sources are ignored at the base models.\nAt the other end of the spectrum are transfer covariance function representing a multivariate similarity relation over sources and target domains. A popular representative of this family is the work by Bonilla et al. (Bonilla et al., 2008) on multi-task learning, where a free-form kernel relates each pair of tasks. Apart from the difference of the application domain (multi-task learning versus TL), this kind of models often imply fitting an increasingly large number of hyperparameters; e.g. in the free-form kernel, this number grows as (N2 \u2212 N)/2, where N is the number of sources. Motivated by (Bonilla et al., 2008), (Cao et al., 2010) develops another transfer covariance function for the single source transfer.\nIn this work, we first describe a family of transfer covariance functions, and investigate their feasibility for MSTR. With the theoretical analysis showing the unsatisfactory performance of such transfer covariance function, we propose to unify the S-T similarity capture of stacking and the transfer covariance function. To the best of our knowledge,\nthis is the first work that analyzes the feasibility and performance of such family of transfer covariance functions for MSTR, and further combines them with stacking."}, {"heading": "3. Problem Statement", "text": "We denote a domain set for MSTR as D = S \u222a T where S = {Si : 1 \u2264 i \u2264 N} is a set of source domains and T is the target domain. All source domain data and few target domain data are labeled. Denote the data matrix and its corresponding label vector in each source domain Si as X(Si) \u2208 RnSi\u00d7d and y(Si) \u2208 RnSi . Likewise, we represent the target data set with X(T ) = {X(Tl),X(Tu)} where X(Tl) \u2208 RnTl\u00d7d is the labeled target data matrix and X(Tu) \u2208 RnTu\u00d7d is the unlabeled one. We further define y(Tl) \u2208 RnTl as the label vector for X(Tl). Moreover, we assume nTl min(nS1 , ..., nSN , nTu). Our objective is to utilize {X(Si),y(Si)}Ni=1 and {X(Tl),y(Tl)} to predict labels for X(Tu).\nWe use the GP model for this regression task. We denote the underlying latent function between the inputs x and the outputs y as f , and the noise variance as \u03c32. Thus, f denotes the function vector over X. A GP model defines a Gaussian distribution over the functions, f \u223c N (\u00b5,K) in which \u00b5 is the mean vector and K is the covariance matrix which is positive semi-definite (PSD, or equivalently denoted as K 0). Usually \u00b5 is assumed to be 0, and thus the GP model is completely specified by K given a covariance function which is parameterized by \u2126."}, {"heading": "4. GP with Transfer Covariance Function", "text": "In this section, we analyze the transfer performance of GP using a specific family of transfer covariance function."}, {"heading": "4.1. Transfer Covariance Function for Multi-Source", "text": "Since the GP model is specified by K, one straightforward way to achieve the knowledge transfer across multiple source domains and the target domain is to design a transfer covariance function for multi-source. Different from a classical GP which uses a fixed covariance function for the data from different domains, we focus on the covariance function of the form (TCMS):\nk\u2217(x,x \u2032) =  \u03bbik(x,x \u2032), x \u2208 X(Si) & x\u2032 \u2208 X(T ) or x \u2208 X(T )& x\u2032 \u2208 X(Si),\nk(x,x\u2032), otherwise.\n(1)\nwhere k(\u00b7, \u00b7) is any valid covariance function, and \u03bbi is the metric measuring the similarity between the source Si and the target T . Through the learning, \u03bbi is expected to capture the different transfer strengths in different S-T domain pairs. Those highly target-related sources will play a more\nimportant role in transfer, while those completely targetunrelated sources will not be considered. However, to guarantee the GP model is always valid, any covariance matrix K\u2217 constructed by k\u2217(\u00b7, \u00b7) should be PSD. Theorem 1 gives the sufficient and necessary condition for a PSD K\u2217. Theorem 1. Let KDiDj (Di,Dj \u2208 D) denote a covariance matrix for points in Di and Dj . A Gram matrix\nK\u2217 =  KS1S1 ... KS1SN \u03bb1KS1T ... ... ... ...\nKSNS1 ... KSNSN \u03bbNKSNT \u03bb1KT S1 ... \u03bbNKT SN KT T  is PSD for any covariance matrix K in the form\nK =  KS1S1 ... KS1SN KS1T ... ... ... ...\nKSNS1 ... KSNSN KSNT KT S1 ... KT SN KT T  if and only if \u03bb1 = ... = \u03bbN and |\u03bbi| \u2264 1.\nProof. Necessary condition: Let K\u2217 be a PSD matrix. We use KSS to represent the sources-to-sources block matrix, and KST , KST \u2217 to represent the sources-to-target block matrix in K and K\u2217, respectively. Thus, we have:\nK = [ KSS KST KTST KT T ] ,K\u2217 = [ KSS KST \u2217 KTST \u2217 KT T ] .\nSince K is PSD, according to the Schur complement theorem (Zhang, 2006), we have:\n(I\u2212KSSK\u0303SS)KST = 0, (2)\nKT T \u2212KTST K\u0303SSKST 0, (3)\nwhere K\u0303SS is the generalized inverse of KSS . By rewriting K\u0303SS as a block matrix using K\u0303SiSj as the element, we further derive eq. (2) and eq. (3) as: KS1T \u2212 N\u2211 i=1 N\u2211 j=1 KS1SiK\u0303SiSjKSjT ...\nKSNT \u2212 N\u2211 i=1 N\u2211 j=1 KSNSiK\u0303SiSjKSjT\n = 0, (4)\nKT T \u2212 N\u2211 i=1 N\u2211 j=1 KT SiK\u0303SiSjKSjT 0. (5)\nLikewise, for the PSD matrix K\u2217, we have the following two Schur complement derivations: \u03bb1KS1T \u2212 N\u2211 i=1 N\u2211 j=1 \u03bbjKS1SiK\u0303SiSjKSjT ...\n\u03bbNKSNT \u2212 N\u2211 i=1 N\u2211 j=1 \u03bbjKSNSiK\u0303SiSjKSjT  = 0. (6)\nKT T \u2212 N\u2211 i=1 N\u2211 j=1 \u03bbi\u03bbjKT SiK\u0303SiSjKSjT 0. (7)\nCombining eq. (4) and eq. (6), we get: N\u2211 i=1 N\u2211 j=1 (\u03bb1 \u2212 \u03bbj)KS1SiK\u0303SiSjKSjT\n... N\u2211 i=1 N\u2211 j=1 (\u03bbN \u2212 \u03bbj)KSNSiK\u0303SiSjKSjT\n = 0. (8)\nSince Eq. (8) must hold for all PSD K, we induce \u03bb1 = ... = \u03bbN = \u03bb. Based on such conclusion, we combine eq. (5) and eq. (7):\n(1\u2212 \u03bb2)KT T + \u03bb2M 0, (9)\nwhere M = KT T \u2212 N\u2211 i=1 N\u2211 j=1 KT SiK\u0303SiSjKSjT . Since eq. (9) must hold for all PSD KT T and PSD M, we resolve that |\u03bb| \u2264 1.\nSufficient condition: Let \u03bb1 = ... = \u03bbN = \u03bb, and |\u03bb| \u2264 1. According to the Theorem 1 in (Cao et al., 2010), we obtain that K\u2217 is a PSD matrix.\nTo sum up, we conclude that if K\u2217 is a PSD matrix, \u03bbi should satisfy \u03bb1 = ... = \u03bbN and |\u03bbi| \u2264 1.\nFrom Theorem 1, we can see that |\u03bbi| \u2264 1, which means a highly target-related source results in a full transfer of KSi,T , but a completely target-unrelated source results in a zero block matrix. This indicates the adaptiveness of \u03bbi. However, Theorem 1 also shows that k\u2217(\u00b7, \u00b7) can just give one similarity coefficient for all S-T domain pairs to ensure the validity of GP-TCMS . Such single similarity coefficient compromises the diverse similarities between different S-T domain pairs. This violates the original intention of \u03bbi which is to distinguish the similarity diversity between different S-T domain pairs.\n4.2. Generalization Bounds of GP-TCMS\nTo investigate the effect of a single compromised similarity coefficient on the performance of GP-TCMS , we derive its generalization error bounds. In (Chai, 2009), an earlier analysis can be found for the generalization errors and learning curves in multi-task learning (specifically, two learning tasks with the same noise variance). Our investigation is different from that work however as we are working on a TL setting, and more importantly, on multiple sources with different noise variances.\nWe denote the single compromised similarity coefficient as \u03bb, and the noise variance for different domains as \u03c32d, d \u2208 {S1, ...,SN , T }. Thus, the transfer covariance matrix of\nthe noisy training data is C\u2217 = K\u2217 + \u03a3 where\nK\u2217 = [ KSS \u03bbKST \u03bbKTST KT T ] ,\u03a3 = [ \u03a3S 0 0 \u03c32T IT T ] and \u03a3S is a diagonal block matrix with the diagonal block elements {\u03c32S1IS1S1 , ..., \u03c3 2 SN ISNSN }. According to (Rasmussen, 2006), if the GP prior is correctly specified, the generalization error at a point is also the posterior variance at such point. Specifically, for GP-TCMS , the posterior variance at the target point xt is:\n\u03b42T (xt, \u03bb, {X(Si), \u03c32Si} N i=1,XT , \u03c3 2 T )\n= ktt \u2212 kT\u2217tC\u22121\u2217 k\u2217t, (10)\nwhere kT\u2217t = (\u03bbk T St,k T T t), kSt (kT t) is the vector of covariances between {X(Si)}Ni=1 (X(T )) and xt, and ktt is the prior variance at xt. Wherever it is not misleading, we will simplify the posterior variance expression using \u03b42T (\u03bb, {\u03c32Si} N i=1, \u03c3 2 T ). The generalization error for the target domain can be obtained by averaging eq. (10) over xt:\nT (\u03bb, {X(Si), \u03c32Si} N i=1,XT , \u03c3 2 T )\n= \u222b \u03b42T (xt, \u03bb, {\u03c32Si} N i=1, \u03c3 2 T )p(xt)dxt.\n(11)\nTo derive the generalization error bounds for GP-TCMS , we first rewrite\nC\u2217 = \u039b\n[ \u03bb\u22122(KSS + \u03a3S) KST\nKTST KT T + \u03c3 2 T IT T\n] \u039b,\nwhere \u039b = [ \u03bbISS 0\n0 IT T\n] . Thus, the posterior variance\nat point xt becomes:\n\u03b42T (\u03bb, {\u03c32Si} N i=1, \u03c3 2 T ) = ktt\u2212kTt \u03a6(\u03bb, {\u03c32Si} N i=1, \u03c3 2 T ) \u22121kt, (12) where kTt = (k T St,k T T t) and\n\u03a6(\u03bb, {\u03c32Si} N i=1, \u03c3 2 T )\n=\n[ \u03bb\u22122(KSS + \u03a3S) KST\nKTST KT T + \u03c3 2 T IT T\n] .\nNote that the above derivation excludes the situation where \u03bb = 0. When \u03bb = 0, all the source domains are unrelated to the target domain, and thus no knowledge is transferred. This is easy to verify by plugging \u03bb = 0 into eq. (12).\nFurther, we observe that \u03b42T is equal for \u03bb and \u2212\u03bb, so we only investigate the case \u03bb \u2208 (0, 1]. For eq. (12), we further decompose it as:\n\u03a6(\u03bb, {\u03c32Si} N i=1, \u03c3 2 T )\n= [ KSS KST KTST KT T ] + [ \u03c32T ISS 0 0 \u03c32T IT T ] +\n(\u03bb\u22122 \u2212 1) [\nKSS + \u03a3S 0 0 0\n] + [ \u03a3S \u2212 \u03c32T ISS 0\n0 0 ] = \u03a6(1, {\u03c32T , ..., \u03c32T }N , \u03c32T ) + E1 + E2,\n(13)\nwhere E1 = (\u03bb\u22122 \u2212 1) [\nKSS + \u03a3S 0 0 0 ] and E2 =[\n\u03a3S \u2212 \u03c32T ISS 0 0 0\n] . Eq. (13) unveils that the posterior\nvariance of having instances from different source domains is equivalent to the posterior variance of having those instances from target domain with two additional correlated noise terms, E1 and E2. This shows us the two main factors that matter in the transfer performance; namely, the S-T similarity and the noise variances. To further analyze how the S-T similarity affects the transfer performance, we focus on one factor and fix the other. Assuming that all the sources are totally related to the target, i.e. \u03bb = 1, and consequently, the noise variance for each source becomes \u03be2Si , we define the difference:\n\u2206 = \u03b42T (1, {\u03be2Si} N i=1, \u03c3 2 T )\u2212 \u03b42T (\u03bb, {\u03c32Si} N i=1, \u03c3 2 T ).\nTo obtain the upper (lower) bound of \u03b42T (\u03bb, {\u03c32Si} N i=1, \u03c3 2 T ), we are interested in those \u03be 2\nSi (\u03be 2 Si ) that make \u2206 \u2265 0 (\u2206 <\n0) for all the target points.\nProposition 1. Let \u03b4 and \u03b4 be the maximum and minimum eigenvalues of KSS , \u03be 2 Si = \u03bb \u22122\u03c32Si \u2212 (1 \u2212 \u03bb \u22122)\u03b4 and \u03be2Si = \u03bb \u22122\u03c32Si \u2212 (1 \u2212 \u03bb\n\u22122)\u03b4 for every source Si. Then, for all the target data points, \u03b42T (1, {\u03be 2\nSi }Ni=1, \u03c32T ) \u2264\n\u03b42T (\u03bb, {\u03c32Si} N i=1, \u03c3 2 T ) \u2264 \u03b42T (1, {\u03be\n2 Si} N i=1, \u03c3 2 T ).\nProof. By applying eq. (12), we have:\n\u2206 = \u03b42T (1, {\u03be2Si} N i=1, \u03c3 2 T )\u2212 \u03b42T (\u03bb, {\u03c32Si} N i=1, \u03c3 2 T )\n= kTt [\u03a6(\u03bb, {\u03c32Si} N i=1, \u03c3 2 T ) \u22121 \u2212 \u03a6(1, {\u03be2Si} N i=1, \u03c3 2 T ) \u22121]kt\nTo make \u2206 \u2265 0 for all the target data points, we need to prove \u03a6(\u03bb, {\u03c32Si} N i=1, \u03c3 2 T ) \u22121\u2212\u03a6(1, {\u03be2Si} N i=1, \u03c3 2 T ) \u22121 is PSD, which means:\n\u03a6(\u03bb, {\u03c32Si} N i=1, \u03c3 2 T ) \u22121 \u2212 \u03a6(1, {\u03be2Si} N i=1, \u03c3 2 T ) \u22121 0\n\u21d0\u21d2 \u03a6(\u03bb, {\u03c32Si} N i=1, \u03c3 2 T ) \u03a6(1, {\u03be2Si} N i=1, \u03c3 2 T ) \u21d0\u21d2 [\n(1\u2212 \u03bb\u22122)KSS + (\u03a3\u2032S \u2212 \u03bb\u22122\u03a3S) 0 0 0\n] 0\nwhere \u03a3\u2032S is a diagonal block matrix with the diagonal block elements {\u03be2S1IS1S1 , ..., \u03be 2 SN ISNSN }\n\u21d0\u21d2 (1\u2212 \u03bb\u22122)KSS + (\u03a3\u2032S \u2212 \u03bb\u22122\u03a3S) 0\n\u21d0\u21d2 KSS (\u03bb\u22122\u03a3S \u2212\u03a3\u2032S)\n(1\u2212 \u03bb\u22122)\n\u21d0\u21d2 \u03b4 \u2264 (\u03bb\u22122\u03c32Si \u2212 \u03be 2 Si)\n(1\u2212 \u03bb\u22122) for every Si\n\u21d0\u21d2 \u03be2Si \u2265 \u03bb \u22122\u03c32Si \u2212 (1\u2212 \u03bb \u22122)\u03b4 for every Si\nNote that \u2206 is a monotonically increasing function of \u03be2Si , thus we take the minimum \u03bb\u22122\u03c32Si \u2212 (1\u2212 \u03bb \u22122)\u03b4 as \u03be 2\nSi to be the smallest upper bound of \u03c32T (\u03bb, {\u03c32Si} N i=1, \u03c3 2 T ). Similarly, we have \u03be2Si = \u03bb \u22122\u03c32Si \u2212 (1 \u2212 \u03bb\n\u22122)\u03b4 to construct the largest lower bound of \u03c32T (\u03bb, {\u03c32Si} N i=1, \u03c3 2 T ).\nProposition 1 gives the lower and upper bounds of the posterior variance \u03b42T (\u03bb, {\u03c32Si} N i=1, \u03c3 2 T ). By applying eq. (11), we readily obtain the generalization error bounds.\nCorollary 1. Let\nT (\u03bb, {\u03c32Si} N i=1, \u03c3 2 T ) = T (1, {\u03be\n2 Si} N i=1, \u03c3 2 T )\nT (\u03bb, {\u03c32Si} N i=1, \u03c3 2 T ) = T (1, {\u03be 2 Si }Ni=1, \u03c32T )\nThen, T (\u03bb, {\u03c32Si} N i=1, \u03c3 2 T ) \u2264 T (\u03bb, {\u03c32Si} N i=1, \u03c3 2 T ) \u2264\nT (\u03bb, {\u03c32Si} N i=1, \u03c3 2 T ).\nProposition 1 serves to demonstrate that \u03bb takes effect in every source on the final transfer performance. With the assumption that different source domains have different ST similarities with the target domain, a single \u03bb that works for every sources has a great difficulty capturing such ST similarity diversity. This leads us to exploit the transfer covariance function in another way."}, {"heading": "5. Transfer Covariance Function Stacking", "text": "Considering the effectiveness showed by the stacking strategy for MSTR (Pardoe & Stone, 2010), we propose a framework that can integrate the capability for S-T similarity capture of both the transfer covariance function and stacking. We first introduce TCSSStack, a conventional way of stacking the transfer covariance function. Then, we design a more involved stacking-inspired approach that overcomes some limitations of the conventional stacking method.\n5.1. Conventional Transfer Stacking TCSSStack\nMotivated by the fact that both the stacking strategy and the transfer covariance function can model the S-T similarity and using the transfer covariance function at the base models would therefore add flexibility to the similarity capture capability of the stacking approach, we propose a TCSSStack method. In TCSSStack, we first train multiple GP-TCSS models using each Si and T (denoted as {f (Si,T )(\u00b7|\u2126i, \u03bbi)}Ni=1) and then apply the conventional stacking strategy to combine their predictions. Given a target point x, the final prediction is given by:\nf(x) = \u2211N\ni=1 \u03c9if\n(Si,T )(x|\u2126i, \u03bbi), \u2211N\ni=1 \u03c9i = 1 (14)\nwhere \u03c9i are coefficients learned by minimizing the least square error on the target labeled data.\nThere are two major issues for the above model. (1) Since each f (Si,T ) is pretrained separately, the parameters learnt for each f (Si,T ) do not take the inter-domain dependencies between different source domains into account. (2) Both \u03bbi and \u03c9i reflect the S-T domain similarity. However, TCSSStack takes them as two different variables and learns them separately. Intuitively, the model importance \u03c9i should be positively correlated with the similarity coefficient \u03bbi. For example, the prediction of a GP-TCSS using an unrelated source is less trustful, and should be assigned a smaller coefficient in the stacking.\n5.2. Improved Transfer Stacking TCMSStack\nTo overcome the above issues, we propose a new transfer stacking model (TCMSStack) as follows:\nf\u2217(x) = \u2211N\ni=1 (g(\u03bbi)/Z)f\n(Si,T )(x,\u2126i, \u03bbi). (15)\nwhere \u03bbi refers to the similarity coefficient in the GP-TCSS for the i-th source, Z = \u2211N i=1 g(\u03bbi) is the normalization term, and g(\u03bbi) is any function preserving the monotonicity of |\u03bbi| so that it coordinates the model importance and the similarity coefficient. This also reduces the search efforts by lowering the number of free parameters to fit. Moreover, instead of pretraining f (Si,T )(\u00b7|\u2126i, \u03bbi) separately, we jointly learn f (Si,T )(\u00b7,\u2126i, \u03bbi) for all the source domains. By doing so, the multiple GP-TCSS models are learned together with the dependencies between multiple sources taken into account.\nNotice that the model in eq. (15) allows for multiple options to characterize the relative importance of GP-TCSS models through g(\u00b7). In this paper, we use a simple function g(\u03bbi) = |\u03bbi|. However, the absolute value function is not smooth at the origin. Thus, we use a smooth function studied in (Yong, 2015) to approximate it as follows:\n|\u03bbi| \u2248 \u03b1Ln( 1 2 e \u03bbi \u03b1 + 1 2 e\u2212 \u03bbi \u03b1 ).\nWe set \u03b1 = 0.01 which is the best approximation stated in (Yong, 2015). Since Theorem 1 also tells us \u22121 \u2264 \u03bbi \u2264 1, we propose to define \u03bbi = 2(1/(1 + \u00b5i))bi \u2212 1 (\u00b5i \u2265 0 and bi \u2265 0), as in (Cao et al., 2010). Then, we conduct the learning by minimizing the squared errors:\nmin {\u2126i,\u00b5i,bi}Ni=1\n\u2211nTl j=1 (y (Tl) j \u2212 f \u2217(x (Tl) j )) 2 . (16)\nIn the optimization, we propose to use the conjugate gradient method. Other optimization methods can also be applied to solve this objective function."}, {"heading": "5.3. Complexity Analysis", "text": "As in usual GP model training, the computational time complexity of each f (Si,T ) is dominated by the calculation\nof the inverse of its covariance matrix, i.e. O((nTl+nSi)3). Considering nTl nSi and assuming nS1 = ... = nSN = nS , the evaluation of a TCMSStack model takes then O(Nn3S). Notice that by following the stacking strategy of eq. (14) the training involves the steps of learning each f (Si,T ) and subsequently learning the \u03c9i coefficients. The latter calls for some cross-validation approach to evaluate a meta-model, as the f (Si,T ) from the previous step have been induced using the target data (Pardoe & Stone, 2010). In the extreme case of leave-one-out, this would take O(nTlNn3S). Even if we also choose a leave-one-out validation to solve eq. (16) the cost of a TCMSStack model evaluation would be lower, since the first step of stacking is not required. On the other side, by following TrAdaBoost.R2 or the GP-TCMS approach, a GP model evaluation requires O((NnS)3), which even exceeds the cost for TCMSStack using leave-one-out whenever N > \u221a nTl ."}, {"heading": "6. Experimental Study", "text": "In the following experimental study we aim at two main goals: (i) to assess the ability of TCMSStack in capturing inter-domain similarity, and (ii) to evaluate its predictive effectiveness compared to other approaches."}, {"heading": "6.1. Experiment Setting", "text": "All the GPs herein build upon a standard squared exponential covariance function. The hyperparameters of each method are optimized using the conjugate gradient implementation from the gpml package (Rasmussen & Nickisch, 2010). For each search, we allow a maximum of 200 evaluations. The reported results correspond to the model providing the best objective function value over 5 independent runs with random initial solutions each. We use one synthetic dataset and two real-world datasets.\nSynthetic dataset. We consider a linear function f(x) = wT0 x + , where w0 \u2208 R100 and is a zero-mean Gaussian noise term, as the target. We use this function to generate 100 points as target test data, and 20 points as target train data. For the source task, we use g(x) = (wT0 +\u03b4\u2206w)x+ , where \u2206w is a random fluctuation vector and \u03b4 is the variable controlling the similarity between f and g (higher \u03b4 indicates lower similarity), to generate 380 points for each source with different \u03b4.\nAmazon reviews. We extract the raw data containing 15\nproduct reviews from (McAuley et al., 2015), and categorize the products into four top categories according to the Amazon website. Products in the same category are conceptually similar. Each product is taken as a domain, and we select one as target from each category (see Table 1). Reviews in each domain are represented by the count features and are labeled using stars in the set {1, 2, 3, 4, 5}.\nUJIIndoorLoc. The building location dataset covers three buildings of Universitat Jaume I with four floors each (Torres-Sospedra et al., 2014). We build 12 domains by taking the location data from each floor of each building as a domain. The first floor of each building is taken as the target. Domains from the same building are taken as similar. The received signal strength intensity from 520 wireless access points is used as the features, and the location represented by the latitude and longitude is taken as label."}, {"heading": "6.2. Domain Similarity Capture", "text": "We first elucidate the ability of TCMSStack in capturing the diverse S-T similarities through the \u03bbi coefficients. To rationalize the assessment, we use the synthetic dataset and consider a variety of problems covering a broad spectrum of TL settings. Precisely, we build four scenarios of N = 2, 5, 10, 15 sources. In each scenario, we specify six problems, each given by a different combination of sources. Three problems represent settings in which all the sources are equally similar to the target, with high (\u03b4 = 0), medium (\u03b4 = 15) and low (\u03b4 = 35) similarity strength. The other three problems reflect diverse S-T similarities. Each source is given by a \u03b4 randomly sampled from the set {0, 4, 7, 10, 15, 20, 25, 30, 35} and with replacement. We enforce the three problems to be different and avoid all the sources to be equal. We show the results in Figure 1.\nIn the figure, the first three problems of each scenario are the cases with equal S-T similarities. It can be observed in the bar plots on the left hand side that the \u03bb values learnt by TCMSStack are strictly reverse-correlated with the predefined \u03b4 values, which indicates an accurate capture of the high, medium and low strengths of S-T similarity. We further observe from the black dots that GP-TCMS is also able to strongly coordinate \u03b4 with a single compromised \u03bb. This is because all the sources share the same \u03b4 with the target, and thus can be regarded as a single larger source.\nThe remaining three problems of each scenario reflect diverse similarities across source domains. In this case, Figure 1 shows that the \u03bb values of TCMSStack reflect the relative differences of \u03b4 across different sources fairly well in general. The learnt \u03bb is generally reverse-correlated with the predefined \u03b4 values, but it is not strict and tight. For instance, in problem 6 of the 5-sources scenario or in the problem 5 of the 15-sources scenario, such reversecorrelated relations do not hold in all the sources. This is"}, {"heading": "2 Sources", "text": ""}, {"heading": "5 Sources", "text": ""}, {"heading": "10 Sources", "text": ""}, {"heading": "15 Sources", "text": "because, although the learnt \u03bbs only represent the bivariate S-T similarities, each of them is specified during learning by considering its influence relative to the rest of similarity coefficients, i.e. the inter-domain dependencies between different sources are taken into account during the learning of the \u03bbs. Thus, in some cases, the learnt \u03bbs may not strictly approximate the real S-T similarities. However, \u03bbs are always learnt to guarantee the outcome of the best transfer performance. That is the reason why the above two cases\nstill achieve satisfactory transfer performance in terms of RMSE. By contrast, we find that GP-TCMS only gives a trade-off value of \u03bb over the diverse \u03b4 values. The right hand side of the figure shows a consistently lower RMSE for TCMSStack than for GP-TCMS in all the problems. In particular, a dramatic improvement is observed by utilizing TCMSStack for the diverse problems 4-6. These results indicate the superiority of TCMSStack over GP-TCMS for MSTR. We further verify this conclusion on the two realworld datasets in the next section."}, {"heading": "6.3. Performance on Real-World Datasets", "text": "We compare TCMSStack with several MSTR approaches, namely: Tradaboost.R2, GP-TCMS , TCSSStack, a variant of TCSSStack with joint learning of model importance coefficient and \u03bb which we call TCSSStack-Joint, and a variant of TCSSStack using the learnt \u03bb as the model importance which we call \u03bb-Stacking. The evaluation comprises both the Amazon and the UJIIndoorLoc datasets. For each source domain we sample 500 points uniformly at random for training. Likewise, train and test data from each target domain are obtained by sampling 25 points and 1000 points, respectively. For the Amazon dataset, we generated a set of problems by using each target domain in Table 1, and by randomly choosing a number of source domains subsets. More precisely, for each scenario of 2, 3 and 5 sources, ten different source combinations were randomly constructed. In addition, a scenario of 11 sources with all the source domains was selected. Thus, we construct 40 transfer problems for each scenario of 2, 3 and 5 sources, and 4 problems for the scenario of 11 sources. For the UJIIndoorLoc dataset, we generate the transfer problems in a similar way to the Amazon dataset described above.\nIn Figure 2, we show the average RMSE results over all the problems in each scenario for the two datasets. Overall, TCMSStack is the winner among all the baselines on the two datasets, improving the transfer performance across\nthe different amounts of source domains. This showcases the capability of TCMSStack to transfer knowledge from various sources with different S-T similarities.\nFor the other baselines, we observe that TrAdaBoost.R2 gives the poorest results due to the premature weights convergence issue. If, in addition, we consider the high computational cost for the models involved, TrAdaBoost.R2 does not seem to be a good choice for MSTR, especially when the number of source domains is large. As for GPTCMS , it presents a steadily inferior performance than TCMSStack. Overall, the outcomes are in line with those in the synthetic dataset, offering further support to the superiority of TCMSStack to GP-TCMS . Notice that, since the current benchmark was generated randomly, it is likely a scenario to comprise diverse problem settings. Therefore, capturing the diverse similarities through a single \u03bb coefficient may compromise the performance of GP-TCMS . As opposed to GP-TCMS , TCMSStack offers more robust performance improvements.\nFinally, the comparison with the other stacking-based methods exposes the benefits of the two salient features of TCMSStack. Both TCSSStack and \u03bb-Stacking are beaten by TCSSStack-Joint and TCMSStack. Since these two sets of methods only differ in the joint learning of the parameters, the outcomes point at the benefits of bringing in the inter-domain dependencies of the other sources during the learning. On the other side, the results for \u03bbStacking and TCMSStack are better or comparable to those by TCSSStack and TCSSStack-Joint, repectively. This provides support to the correlation of the model importance with the similarity coefficients, which allows to specify the model by estimating fewer hyper-parameters while preserving the similarity capture capability."}, {"heading": "7. Conclusions", "text": "We investigate a family of transfer covariance functions that represent the pairwise similarity between each source and the target domain for the MSTR problem. We prove that, GP-TCMS , a Gaussian process with such a transfer covariance function can only capture the same similarity coefficient for all the sources. By further analyzing the generalization errors of GP-TCMS , we conclude the bounds depend on the single similarity coefficient, which may penalize the transfer performance. As an alternative, we propose TCMSStack, an approach that integrates the transfer covariance function and the stacking strategy into one unified model. TCMSStack aligns the S-T similarity coefficients with the model importance and jointly learns the base models. Extensive experiments on one synthetic and two real-world datasets, with learning settings of up to 11 sources for the latter, show the superiority of TCMSStack to other MSTR methods."}, {"heading": "Acknowledgments", "text": "This work was conducted within the Rolls-Royce@Nanyang Technological University Corporate Lab with support from the National Research Foundation (NRF) Singapore under the Corp Lab@University Scheme. It is also partially supported by the School of Computer Science and Engineering at Nanyang Technological University."}], "year": 2017, "references": [{"title": "Adaptive boosting for transfer learning using dynamic updates", "authors": ["Al-Stouhi", "Samir", "Reddy", "Chandan"], "venue": "Machine Learning and Knowledge Discovery in Databases,", "year": 2011}, {"title": "Multi-source iterative adaptation for cross-domain classification", "authors": ["Bhatt", "Himanshu Sharad", "Rajkumar", "Arun", "Roy", "Shourya"], "venue": "In Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence,", "year": 2016}, {"title": "Multi-task gaussian process prediction", "authors": ["Bonilla", "Edwin V", "Chai", "Kian M", "Williams", "Christopher"], "venue": "Advances in Neural Information Processing Systems", "year": 2008}, {"title": "Adaptive transfer learning", "authors": ["Cao", "Bin", "Pan", "Sinno Jialin", "Zhang", "Yu", "Yeung", "Dit-Yan", "Yang", "Qiang"], "venue": "In Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence,", "year": 2010}, {"title": "Generalization errors and learning curves for regression with multi-task gaussian processes", "authors": ["Chai", "Kian M"], "venue": "In Advances in neural information processing systems,", "year": 2009}, {"title": "Boosting for transfer learning", "authors": ["Dai", "Wenyuan", "Yang", "Qiang", "Xue", "Gui-Rong", "Yu", "Yong"], "venue": "In Proceedings of the 24th International Conference on Machine Learning,", "year": 2007}, {"title": "Multi-source transfer learning based on label shared subspace", "authors": ["Fang", "Min", "Guo", "Yong", "Zhang", "Xiaosong", "Li", "Xiao"], "venue": "Pattern Recognition Letters,", "year": 2015}, {"title": "Multisource surrogate modeling with bayesian hierarchical regression", "authors": ["Ghosh", "Sayan", "Jacobs", "Ryan", "Mavris", "Dimitri N"], "venue": "In 17th AIAA Non-Deterministic Approaches Conference,", "year": 2015}, {"title": "Fused regression for multi-source gene regulatory network inference", "authors": ["Lam", "Kari Y", "Westrick", "Zachary M", "M\u00fcller", "Christian L", "Christiaen", "Lionel", "Bonneau", "Richard"], "venue": "PLOS Computational Biology,", "year": 2016}, {"title": "Transfer learning from multiple source domains via consensus regularization", "authors": ["Luo", "Ping", "Zhuang", "Fuzhen", "Xiong", "Hui", "Yuhong", "He", "Qing"], "venue": "In Proceedings of the 17th ACM conference on Information and knowledge management,", "year": 2008}, {"title": "A survey on transfer learning", "authors": ["Pan", "Sinno Jialin", "Yang", "Qiang"], "venue": "Knowledge and Data Engineering, IEEE Transactions on,", "year": 2010}, {"title": "Transfer learning via dimensionality reduction", "authors": ["Pan", "Sinno Jialin", "Kwok", "James T", "Yang", "Qiang"], "venue": "In AAAI,", "year": 2008}, {"title": "Domain adaptation via transfer component analysis", "authors": ["Pan", "Sinno Jialin", "Tsang", "Ivor W", "Kwok", "James T", "Yang", "Qiang"], "venue": "IEEE Transactions on Neural Networks,", "year": 2011}, {"title": "Boosting for regression transfer", "authors": ["Pardoe", "David", "Stone", "Peter"], "venue": "Proceedings of the 27th International Conference on Machine Learning", "year": 2010}, {"title": "Gaussian processes for machine learning", "authors": ["Rasmussen", "Carl Edward"], "venue": "Citeseer,", "year": 2006}, {"title": "Gaussian processes for machine learning (gpml) toolbox", "authors": ["Rasmussen", "Carl Edward", "Nickisch", "Hannes"], "venue": "Journal of Machine Learning Research,", "year": 2010}, {"title": "Learning categories from few examples with multi model knowledge transfer", "authors": ["Tommasi", "Tatiana", "Orabona", "Francesco", "Caputo", "Barbara"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "year": 2014}, {"title": "Adaptive knowledge transfer for multiple instance learning in image classification", "authors": ["Wang", "Qifan", "Ruan", "Lingyun", "Si", "Luo"], "venue": "In AAAI,", "year": 2014}, {"title": "Deep nonlinear feature coding for unsupervised domain adaptation", "authors": ["Wei", "Pengfei", "Ke", "Yiping", "Goh", "Chi Keong"], "venue": "Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence,", "year": 2016}, {"title": "Multi-task gaussian process learning of robot inverse dynamics", "authors": ["Williams", "Christopher", "Klanke", "Stefan", "Vijayakumar", "Sethu", "Chai", "Kian M"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2009}, {"title": "Stacked generalization", "authors": ["Wolpert", "David H"], "venue": "Neural Networks,", "year": 1992}, {"title": "Boosting for transfer learning with multiple sources. In Computer vision and pattern recognition", "authors": ["Yao", "Yi", "Doretto", "Gianfranco"], "venue": "IEEE conference on,", "year": 2010}, {"title": "Uniform smooth approximation functions for absolute value function", "authors": ["Yong", "Longquan"], "venue": "Mathematics in practice and theory,", "year": 2015}, {"title": "The Schur complement and its applications, volume 4", "authors": ["Zhang", "Fuzhen"], "venue": "Springer Science & Business Media,", "year": 2006}, {"title": "Transfer learning for cross-language text categorization through active correspondences construction", "authors": ["Zhou", "Joey Tianyi", "Pan", "Sinno Jialin", "Tsang", "Ivor W", "Ho", "Shen-Shyang"], "venue": "In AAAI,", "year": 2016}], "id": "SP:2a5d8a97d99e4f8c95a36632bd78fcdab015942b", "authors": [{"name": "Pengfei Wei", "affiliations": []}, {"name": "Ramon Sagarna", "affiliations": []}, {"name": "Yiping Ke", "affiliations": []}, {"name": "Yew-Soon Ong", "affiliations": []}, {"name": "Chi-Keong Goh", "affiliations": []}], "abstractText": "A key challenge in multi-source transfer learning is to capture the diverse inter-domain similarities. In this paper, we study different approaches based on Gaussian process models to solve the multi-source transfer regression problem. Precisely, we first investigate the feasibility and performance of a family of transfer covariance functions that represent the pairwise similarity of each source and the target domain. We theoretically show that using such a transfer covariance function for general Gaussian process modelling can only capture the same similarity coefficient for all the sources, and thus may result in unsatisfactory transfer performance. This leads us to propose TCMSStack, an integrated strategy incorporating the benefits of the transfer covariance function and stacking. Extensive experiments on one synthetic and two realworld datasets, with learning settings of up to 11 sources for the latter, demonstrate the effectiveness of our proposed TCMSStack.", "title": "Source-Target Similarity Modelings for Multi-Source Transfer Gaussian Process Regression"}