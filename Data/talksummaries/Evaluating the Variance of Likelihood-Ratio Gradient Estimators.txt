5	34	The likelihood-ratio method (Glynn, 1990; Williams, 1992) and the reparameterization trick (Williams, 1992; Kingma & Welling, 2014; Rezende et al., 2014; Titsias & Lzarogredilla, 2014) are widely used for the gradient estimation.
6	21	The likelihood-ratio method only requires the computation of density functions and their derivatives, and therefore it is applicable to a wide range of models including those with discrete variables.
10	20	The reparameterization trick, on the other hand, has a small estimation variance in practice and is only applicable to models with certain continuous variables.
13	15	It is derived by the reparameterization and the local marginalization analogous to the local expectation gradient (Titsias & Lázaro-Gredilla, 2015).
22	26	We overview the related work in Sec.2 and formulate the gradient estimation problem in Sec.3.
25	18	We then show experimental results in Sec.7 and give a conclusion in Sec.8.
49	45	The objective function is given as F (φ;x) = Eqφ(z|x)f(x, z), where f is a feasible function, qφ(z|x) = ∏M i=1 qφi(zi|pai) is a directed graphical model of M variables z = (z1, .
50	40	, zM ) conditioned on an input to the system x, pai are the parent nodes of zi, and φ are the model parameters.
51	21	Each conditional qφi(zi|pai) is continuously differentiable w.r.t.
52	12	φi and is typically a simple distribution such as a Bernoulli or Gaussian whose parameters are computed by a neural network with weights φi.
56	31	In this case, the objective function is the expectation of f(x, z) = log pθ(x, z) − log qφ(z|x), which gives a lower bound of the log likelihood log pθ(x).
62	16	Here we give a general formulation of our framework of gradient estimation.
69	19	(1) Unlike the reparameterization trick, this equation holds even if the function gφ is not continuous because the local expectation E if(x, gφ(x, )) is differentiable.
85	34	The likelihood-ratio estimator is derived by using the logderivative trick for the local gradient estimation.
93	29	• Constant baseline is a constant of all variables {x, }.
95	13	In this case, it holds that Ci = 0.
106	12	• Fully-informed baseline is a baseline that depends on all of x and , possibly in a nonlinear way.
107	23	This is the most general class of baselines.
112	12	(3) Note that this equation holds only if the function gφ(x, ) is differentiable, and therefore the reparameterization trick is only applicable to continuous variables.
115	21	The optimal estimator is obtained by analytically computing the local gradient ∇φiE if(x, gφ(x, )).
116	16	When we fix \i and modify the value of zi, the descendant variables of zi might be changed because they are functions of zi and noise variables.
185	13	This baseline does not depend on i, and therefore we conclude the proof by letting b?i = bi.
216	13	The task is variational learning of sigmoid belief networks (SBN) (Neal, 1992), which is a directed graphical model with layers of Bernoulli variables.
222	14	In our experiments, the prior of each variable zL,i ∈ ZL is independently parameterized by its logit.
256	16	However, the gap between these practical methods and the optimal one is not negligible, and there is still room for improvements.
260	24	We can infer the performance with aligned computational budget by comparing the variance and the computational cost.
261	17	We introduced a novel framework of gradient estimation for stochastic computations using reparameterization.
267	50	While this study does not provide a way to compare such models in general, it bridges the gradient estimators of them through the optimal case, and therefore provides some insights on their relationships.
268	48	Observing the experimental results, the modern estimators for Bernoulli variables achieve variance close to the optimal one, and therefore we can expect that the modern estimators for Bernoulli variables are maturing and could be applied to much larger models capturing discrete phenomena.
269	47	We thank members of Preferred Networks, especially Daisuke Okanohara, for the helpful discussions.
