2	14	Mostafazadeh et al. (2016) introduced the Story Cloze Test (SCT) evaluation framework to address * This work was performed at University of Rochester.
3	70	This test evaluates a story comprehension system where the system is given a foursentence short story as the ‘context’ and two alternative endings and to the story, labeled ‘right ending’ and ’wrong ending.’ Then, the system’s task is to choose the right ending.
4	21	In order to support this task, Mostafazadeh et al. also provide the ROC Stories dataset, which is a collection of crowd-sourced complete five sentence stories through Amazon Mechanical Turk (MTurk).
6	32	Several shallow and neural models, including the state-of-the-art script learning approaches, were presented as baselines (Mostafazadeh et al., 2016) for tackling the task, where they show that all their models perform only slightly better than a random baseline suggesting that richer models are required for tackling this task.
8	23	Surprisingly, one of the models made a staggering improvement of 15% to the accuracy, partially due to using stylistic features isolated in the ending choices (Schwartz et al., 2017b), discarding the narrative context.
11	27	The contribution of this paper is threefold: (1) we provide an extensive analysis of the SCT dataset to shed some light on the ending data characteristics (Section 3) (2) we develop a new strong classifier for tackling the SCT that uses a variety of features inspired by all the top-performing systems on the task (Section 4) (3) we design a new crowd-sourcing scheme that yields a new SCT dataset; we benchmark various models on the new dataset (Section 5).
32	15	For determining the sentiment, we used Stanford CoreNLP (Manning et al., 2014) and the VADER sentiment analyzer (Hutto and Gilbert, 2014).
33	30	For measuring the syntactic complexity, we used Yngve and Frazier metrics (Yngve, 1960; Frazier, 1985).
34	37	Table 2 compares these statistics between the right and wrong endings in the SCTv1.0 dataset.
35	17	The feature distribution plots can be found in the supplementary material.
36	27	Furthermore, we conducted an extensive ngram analysis, using word tokens, characters, partof-speech, and token-POS (similar to Schwartz et al. (Schwartz et al., 2017b)) as features.
38	64	In ‘right endings’, pronouns are used more frequently versus proper nouns used in ‘wrong endings’.
56	34	As the data analysis revealed, the token count, sentiment, and the complexity are not as important features for classification as the ending n-grams are.
59	19	Have similar distributions of token n-grams and char-grams 3.
66	23	The new restrictions were: ‘Each sentence should stay within the same subject area of the story,’ and ‘The number of words in the Right and Wrong sentences should not differ by more than 2 words,’ and ‘When possible, the Right and Wrong sentences should try to keep a similar tone/sentiment as one another.’ The motivation behind this technique was to reduce the statistical differences by asking the user to be mindful of considerations.
68	31	Here, the prompt instructs the workers to make sure the new ‘wrong ending’ sentence makes sense standalone, that it does not differ in the number of words from the original sentence by more than three words, and that the changes cannot be as simple as e.g., putting the word ‘not’ in front of a description or a verb.
72	17	The filtering was done by splitting each SCT-v1.0’s two alternative endings into two independent five-sentence stories and asking three different MTurk users to categorize the story as either: one where the story made complete sense, one where the story made sense until the last sentence and one where the story does not make sense for another reason.
77	35	The results are presented in Table 5, which indicates the drop in the standard deviations in our new dataset.
79	25	The drop in accuracy of the EndingReg model between the SCT-v1.0 and SCT-v1.5 shows a significant improvement on the statistical weight of the stylistic features generated by the model.
80	34	Since the main features used are the token length and the various n-grams, this suggests that the new ‘right endings’ and ‘wrong endings’ have much more similar token n-gram, pos n-gram, postoken n-gram and char-gram overlap.
84	16	These results place the classification accuracy of this top performing model on par with or worse than the models that did not use the ending features of the old SCT-v1.0 dataset (Mostafazadeh et al., 2017), which suggest that the gap that once was held by models using the ending biases seems to be corrected for.
85	19	Al- though we did not get to test all the other models published on SCT-v1.0 directly, we predict similar trends.
86	35	It is important to point out that the 64.4% performance attained by our EndingReg model is still high for a model which completely discards the context.
87	121	This indicates that although we could correct for some of the stylistic biases, there are some other hidden patterns in the new endings that would not have been accounted for without having the EndingReg baseline.
89	22	We propose the community to report accuracies on both SCT-v1.0 and SCT-v1.5, both of which still have a huge gap between the best system and the human performance.
91	16	Using that analysis, along with a classifier we developed for testing new data collection schemes, we created a new SCT dataset, SCT-v1.5, which overcomes some of the biases.
94	20	We believe that evaluation benchmarks should evolve and improve over time and we are planning to incrementally update the Story Cloze Test benchmark.
96	26	The success of our modified data collection method shows how extreme care must be given for sourcing new datasets.
100	21	rochester.edu/nlp/rocstories/.
101	33	We hope that this work ignites further interest in the community for making progress on story understanding.
