0	13	In this paper, we aim at solving the following convex constrained optimization problem: min x∈Rd f(x), s.t.
1	10	c(x) ≤ 0, (1) where f(x) is a smooth or non-smooth convex function and c(x) is a lower-semicontinuous and convex function.
3	15	For example, in distance metric learning one needs to learn a positive semi-definite (PSD) matrix such that similar examples are close to each other and dissimilar examples are far from each other (Weinberger et al., 2006; Xing et al., 2003), where the positive semi-definite constraint can be cast into a convex inequality constraint.
6	15	Another solution is to employ the projected gradient (PG) method (Nesterov, 2004) or the conditional gradient (CG) method (Frank & Wolfe, 1956), where the PG method needs to compute the projection into the constrained domain at each iteration and CG needs to solve a linear optimization problem under the constraint.
7	36	However, for many constraints (e.g., PSD, quadratic constraints) both projection into the constrained domain and the linear optimization under the constraint are time-consuming, which restrict their capabilities to solving these problems.
9	19	In the seminal paper (Mahdavi et al., 2012), the authors have proposed two algorithms with only one projection at the end of iterations for non-smooth convex and strongly convex optimization, respectively.
10	8	The idea of both algorithms is to move the constraint function into the objective function and to control the violation of constraint for intermediate solutions.
11	26	While their developed algorithms enjoy an optimal convergence rate for nonsmooth optimization (i.e., O(1/ 2) iteration complexity) and a close-to-optimal convergence rate for strongly convex optimization (i.e., Õ(1/ ) 1), there still lack of theory and algorithms with reduced projections and faster rates for smooth convex optimization and for convex optimization without strong convexity assumptions.
12	55	In this paper, we make significant contributions by developing a richer theory of convex constrained optimization with reduced projections and faster rates.
16	19	In addition, when equipped with an optimal algorithm for strongly convex optimization the general theory implies the optimal iteration complexity of O(1/ ) for strongly convex optimization with only one projection.
18	23	• Building on the general framework and theory, we further develop an improved theory with faster convergence rates for non-strongly convex optimization at the price of a logarithmic number of projections.
50	7	Throughout the paper, we make the the following assumptions to facilitate the development of our algorithms and theory.
52	12	(3) (ii) there exists a strictly feasible solution such that c(x) < 0; (iii) both f(x) and c(x) are defined everywhere and are Lipschitz continuous with their Lipschitz constants denoted by G and Gc, respectively.
69	20	We will construct such a penalty function hγ(x) for non-smooth and smooth optimization in next two subsections.
70	14	We propose to optimize the following augmented objective function min x∈Rd Fγ(x) = f(x) + hγ(x).
83	12	Inequality (6) implies that f(x̂T ) +λ[c(x̂T )]+ ≤ f(x∗) +Cγ+BT (γ;x∗,x1).
86	14	Since an optimal convergence rate for general non-smooth optimization with only one projection has been attained in (Mahdavi et al., 2012), in this subsection we present an optimal convergence result for strongly convex problems.
108	26	establish the convergence for f(x) using Nesterov’s optimal accelerated gradient (NAG) methods.
112	10	Suppose that Assumption 1 holds, dist(y0,Ω∗) ≤ D, f(x) is Lf -smooth and c(x) is Lc-smooth.
115	12	Remark: The convergence results above indicate an O(1/ ) iteration complexity for smooth optimization and O(1/ 1/(2α)) with α ∈ (1/2, 1) for smooth and strongly convex optimization with only one projection.
117	12	In this section, we will develop improved convergence for non-strongly convex optimization at a price of a logarithmic number of projections by considering an additional condition on the target problem.
123	11	For a convex minimization problem (1), we assume (i) there exist x0 ∈ Ω and 0 ≥ 0 such that f(x0)− minx∈Ω f(x) ≤ 0; (ii) Ω∗ is a non-empty convex compact set; (iii) the optimization problem (1) satisfies a local error bound condition, i.e., there exist θ ∈ (0, 1] and σ > 0 such that for any x ∈ S we have dist(x,Ω∗) ≤ σ(f(x) − f∗) θ where Ω∗ denotes the optimal set and f∗ denotes the optimal value.
125	11	In particular, Assumption 2 (i) supposes there is a lower bound of the optimal value f∗, which usually holds in machine learning applications where the objective function if non-negative; Assumption 2 (ii) ensures that S is also bounded (Rockafellar, 1970), therefore the σ in the local error bound is finite, which can be easily satisfied for a norm regularized or constraint problems; the local error bound condition holds for a broad family of functions (e.g., semi-algebraic functions or real subanalytic functions (Jerome Bolte, 2015; Yang & Lin, 2016)).
136	8	Remark: Since the projection is only conducted at the end of each epoch and the total number of epochs is at Algorithm 1 LoPGD 1: INPUT: K ∈ N+ , t ∈ N+, η1 2: Initialization: x0 ∈ Ω, 0 3: for k = 1, 2, .
137	16	, t− 1 do 6: Update xks+1 = x k s − ηk∂F (xks) 7: end for 8: Let x̂k = ∑t s=1 x k s/t 9: Let xk = ΠΩ[x̂k] and ηk+1 = ηk/2 10: end for Algorithm 2 LoPNAG 1: INPUT: K ∈ N+ , t1, .
138	10	, tk − 1 do 6: Update xks+1 = y k s − 1Lk∇Fγk(x k s) 7: Update yks+1 = x k s+1 + βs+1(x k s+1 − xks) 8: end for 9: Let x̂k = xktk , xk = ΠΩ[x̂k] and γk+1 = γk/2 10: end for most K = dlog2( 0/ )e, so the total number of projections is only a logarithmic number K. The iteration complexity in Theorem 4 is Õ(1/ 2(1−θ)) that improves the standard result of O(1/ 2) without strong convexity.
139	74	With θ = 1/2, we can achieve Õ(1/ ) iteration complexity with only O(log(1/ )) projections.
145	14	To simplify notations, we let L̄ = Lf + λLc.
153	12	Below, we discuss the condition in details about three types of constraints.
154	17	First, we show that when c(x) is a polyhedral function, i.e., its epigraph is a polyhedron (not necessarily bounded), the inequality (3) is satisfied.
