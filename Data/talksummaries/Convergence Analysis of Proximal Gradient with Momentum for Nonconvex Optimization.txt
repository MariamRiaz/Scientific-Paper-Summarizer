0	13	Many problems in machine learning, data mining, and signal processing can be formulated as the following composite minimization problem min x∈Rd F (x) = f(x) + g(x).
2	12	The second term g : Rd → R is the regularizer that promotes desired structures on the solution based on prior knowledge of the problem.
3	12	Algorithm 1 APG Input: y1 = x1 = x0, t1 = 1, t0 = 0, η < 1L .
5	16	In particular, such convex problems can be efficiently minimized by many firstorder algorithms, among which the accelerated proximal gradient (APG) method (also referred to as FISTA (Beck & Teboulle, 2009b)) is proven to be the best for minimizing such class of convex functions.
8	34	It has been shown (Beck & Teboulle, 2009b) that the APG method reduces the function value gap at a rate of O(1/k2) where k denotes the number of iterations.
12	12	for k = 1, 2, · · · do yk = xk + tk−1 tk (zk − xk) + tk−1−1tk (xk − xk−1).
21	23	However, the analysis of APGnc in (Yao & Kwok, 2016) does not exploit the KL property and no convergence rate of the function value is established.
29	18	For APGnc applied to nonconvex problems of (P), we show that the limit points of the sequences generated by APGnc are critical points of the objective function.
38	20	This is the first analysis of the SVRG proximal algorithm with momentum that exploits the KŁ structure to establish linear convergence rate for nonconvex programming.
40	24	We demonstrate that APGnc/APGnc+ outperforms APG and mAPG for nonconvex problems in both exact and inexact cases, and in both deterministic and stochastic variants of the algorithms.
66	10	A point x ∈ Rd is a critical point of h iff 0 ∈ ∂h(x).
68	12	(1) Definition 4 (Proximal map, e.g. (Rockafellar & Wets, 1997)).
76	9	The KŁ property is a generalization of the Łojasiewicz gradient inequality to nonsmooth functions (Bolte et al., 2007), and it is a powerful tool to analyze a class of first-order descent algorithms (Attouch & Bolte, 2009; Attouch et al., 2010; Bolte et al., 2014).
80	16	We adopt the following assumptions on the problem (P) in this paper.
86	28	In this section, we provide our main results on the convergence analysis of APGnc and SVRG-APGnc as well as inexact variants of these algorithms.
89	14	Our first result characterizes the behavior of the limit points of the sequence generated by APGnc.
98	11	Let F (x) ≡ F ∗ for all x ∈ Ω (the set of limit points), and denote rk := F (xk)− F ∗.
100	10	If θ ∈ (0, 12 ), then rk ≤ ( c (k−k0)d2(1−2θ) ) 1 1−2θ , where d1 = ( 1η + L) 2/( 12η − L 2 ) and d2 = min{ 12cd1 , c 1−2θ (2 2θ−1 2θ−2 − 1)r2θ−1k0 }.
109	21	Algorithm 4 APGnc with adaptive momentum (APGnc+) Input: y1 = x0, β, t ∈ (0, 1), η < 1L .
111	13	We here propose an alternative choice of the momentum stepsize that is more intuitive for nonconvex problems, and refer to the resulting algorithm as APGnc+ (See Algorithm 4).
125	20	Consider the above two cases for inexact APGnc under Assumption 1.{1,2,3}.
127	15	Correspondingly, a smaller stepsize η < 12C+L should be used.
136	23	if F (xmk ) ≤ F (zk) then yk+1 = x m k , else if F (zk) ≤ F (xmk ) then yk+1 = zk.
156	31	Even though this sequence is not actually generated by the algorithm, we can reach to the convergence rate by analyzing the relation between the reference sequence and the actual sequence generated by the algorithm.
157	31	Compared to the exact case, the convergence rate remains at the same order, i.e., the linear convergence, but the convergence is slower due to the larger parameter d caused by the error parameter α.
174	9	In particular, APGnc+ performs the best with our adaptive momentum strategy, justifying its empirical advantage.
184	10	We then compare the performance among SVRG-APGnc, SVRG-APGnc+ and the original proximal SVRG methods, and pick the stepsize η = 1/8mL with m = n. The results are presented in Figures 3 and 4.
185	35	In the error free case in Figure 3 (a), one can see that SVRG-AGPnc+ method outperforms the others due to the adaptive momentum, and the SVRG-APGnc method also performs better than the original proximal SVRG method.
186	10	For the inexact case, we set the proximal error as k = min( 1100k3 , 10 −7), where 10−7 is chosen to suppress the large inexactness during the initial few iterations.
187	117	One can see from Figure 3 (b) that the performance is degraded compared to the exact case, and converges to a different local minimum.
