0	21	Decentralized partially observable Markov decision processes (Dec-POMDPs) emerged as the standard framework for sequential decision making by a team of collaborative agents (Bernstein et al., 2000).
3	47	The literature of multi-agent reinforcement learning (MARL) can be divided into two main categories: concurrent and team approaches (Tan, 1998; Panait & Luke, 2005).
38	33	A finite Dec-POMDP is a tuple M .“ pn,X, tU iu, tZiu, p, r, `, γ, b0q, where n denotes the number of agents involved in the decentralized stochastic control process; X is a finite set of hidden world states, denoted x or y; U i is a finite private control set of agent i P v1;nw, where U “ U1 ˆ ¨ ¨ ¨ ˆ Un specifies the set of controls u “ pu1, .
39	15	, unq; Zi is a finite private observation set of agent i, where Z “ Z1 ˆ ¨ ¨ ¨ ˆ Zn specifies the set of observations z “ pz1, .
40	11	, znq; p describes a transition function with conditional density pu,zpx, yq; r is a reward model with immediate reward rpx, uq, we assume rewards are two-side bounded, i.e., for some c P R`, @x P X,u P U : |rpx, uq| ď c; ` is the planning horizon; γ P r0, 1s denotes the discount factor; and b0 is the initial belief state with density b0px0q.
43	21	Hence, the goal of solving M is to find a plan, i.e., a tuple of individual decision rules, one for each agent and time step: ρ .“ pa10:`, .
44	17	A tth individual decision rule ait : O i t ÞÑ PpU iq of agent i prescribes private controls based on the whole information available to the agent up to the tth time step, i.e., history of controls and observations oit “ pui0:t´1, zi1:tq, where oi0 “ H and oit P Oit.
45	31	A tth joint decision rule, denoted at : Ot ÞÑ PpUq, can be specified as atpu|oq .“ śn i“1 a i tpui|oiq, where Ot .“ O1t ˆ ¨ ¨ ¨ ˆ Ont , oi P Oit and o .“ po1, .
47	19	For any control interval t, joint plans a0:t of interest are those that achieve the highest performance measure Jpa0:tq .“ Ea0:t tR0 | b0u starting at b0, where Ea0:tt¨u denotes the expectation with respect to the probability distribution over state-action pairs joint plan a0:t induces, in particular Jpρq .“ Jpa0:`´1q for ρ .“ a0:`´1.
48	22	One can show that, in Dec-POMDPs, there always exists a deterministic plan that is as good as any stochastic plan (see Puterman, 1994, Lemma 4.3.1).
49	9	Unfortunately, there is no direct way to apply the theory developed for Markov decision processes (Bellman, 1957; Puterman, 1994) to Dec-POMDPs, including: the Bellman optimality equation; or the policy improvement theorem.
50	32	To overcome these limitations, we rely on a recent theory by Dibangoye et al. that recasts M into an MDP, thereby allowing knowledge transfer from the MDP setting to Dec-POMDPs.
51	12	To overcome the fact that agents can neither see the actual state of the system nor explicitly communicate their noisy observations with each other, Szer et al. (2005) and later on Dibangoye et al. (2016) suggest formalizing M from the perspective of a centralized algorithm.
52	53	A centralized algorithm acts on behalf of the agents by selecting a joint decision rule to be executed at each control interval based on all data available about the system, namely the information state.
53	23	The information state at the end of control interval t, denoted ιt`1 .“ pb0, a0:tq, is a sequence of joint decision rules the centralized algorithm selected starting at the initial belief state.
56	10	The occupancy state at control interval t, denoted st .“ Ppxt, ot|ιtq, is a distribution over hidden states and joint histories conditional on information state ιt at control interval t. Interestingly, the occupancy state has many important properties.
58	10	In addition, it describes a deterministic and fully observable Markov decision process, where the next occupancy state depends only on the current occupancy state and next joint decision rule, for all y P X, o P O, u P U, z P Z: T pst, atq .“ st`1 st`1py, po, u, zqq .“ atpu|oq ÿ x stpx, oq ¨ pu,zpx, yq.
62	16	It is worth noticing that there is no need to construct explicitly M 1; instead we use M (when available) as a generative model for the occupancy states T pst, atq and rewards Rpst, atq, for all control intervals t. To better understand why we use plans instead of policies and how they relate, consider the MDP case.
63	7	The solution of any finite MDP called a policy π : S ÞÑ A can be represented as a decision tree, where nodes are labelled with actions and arcs are labelled with states.
93	17	Second, even if we assume a complete knowledge of the occupancy states, they lie in a continuum, which precludes exact RL methods to accurately predict α-vectors even in the limit of infinite time and data.
94	34	Finally, the greedy maximization required to improve the value function proved to be NP-hard in finite settings (Radner, 1962; Dibangoye et al., 2009; Kumar & Zilberstein, 2009; Oliehoek et al., 2010).
95	79	Although mappings T and R in M 1 are unknown to either agents or a centralized algorithm, one can instead estimate on the fly both T ps0, a0:t´1q and RpT ps0, a0:t´1q, atq for some fixed plan ρ .“ a0:`´1 through successive interactions of agents with the environment.
97	20	The first one assumes a generative model is available during the centralized learning phase, e.g. a black box simulator; and the second does not.
106	17	K. The proof follows from the performance guarantee of the policy-search algorithm by Bagnell et al. (2004).
108	17	When no generative model is available, the best we can do is to store samples agents collected during the learning phase into replay pools Dρ, one experience for each episode within the limit size of K. We maintain only the K recent experiences, and may discard1 hidden states since they are unnecessary for the updates of future replay pools and the performance measure.
112	25	One can further show this approach preserves performance guarantees similar to those obtained when using a generative model.
118	7	As a consequence, finite sets Ω˚0:`´1 of αvectors can produce solutions arbitrarily close to the optimal Q-value function Q˚0:`´1.
126	14	The former approach may require less iterations before convergence to an optimal joint policy, but the computational cost of each iteration shall increase with the number of α-vectors maintained.
130	10	Suppose we have determined the value function V ρ0:`´1 for any arbitrary ρ .“ a0:`´1.
132	38	One way to answer this question is to consider selecting ā0:t at control interval t and thereafter following decision rules at`1:`´1 of the existing ρ.
