1	44	This task is interesting because a well-performing acquisition model can serve as a good baseline for examining factors of grounding (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010), or as a piece of evidence (Clark, 2001; Zuidema, 2003) about the Distributional Hypothesis (Harris, 1954) against the poverty of the stimulus (Chomsky, 1965).
2	15	Unfortunately, previous attempts at inducing unbounded context-free grammars (Johnson et al., 2007; Liang et al., 2009) converged to weak modes of a very multimodal distribution of grammars.
3	26	There has been recent interest in applying cognitively- or empirically-motivated bounds on recursion depth to limit the search space of grammar induction models (Ponvert et al., 2011; Noji and Johnson, 2016; Shain et al., 2016).
31	34	A left-corner parser (Rosenkrantz and Lewis, 1970; JohnsonLaird, 1983; Abney and Johnson, 1991; Resnik, 1992) uses a stack of memory elements to store derivation fragments during incremental processing.
33	24	For example, Figure 1 shows the derivation fragments in a traversal of a phrase structure tree for the sentence The cart the horse the man bought pulled broke.
35	82	Derivation fragments at every time step are numbered top-down by depth d to a maximum depth of D. A left-corner parser requires more derivation fragments — and thus more memory — to process center-embedded constructions than to process left- or right-embedded constructions, consistent with observations that center embedding is more difficult for humans to process (Chomsky and Miller, 1963; Miller and Isard, 1964).
37	13	For sequences of observed word tokens wt for time steps t ∈ {1..T }, sequence models like Ponvert et al. (2011) and Shain et al. (2016) hypothesize sequences of hidden states qt.
38	111	Models like Shain et al. (2016) implement bounded grammar rules as depth bounds on a hierarchical sequence model implementation of a left-corner parser, using random variables within each hidden state qt for: 1. preterminal labels pt and labels of top and bottom signs, adt and b d t , of derivation fragments at each depth level d (which correspond to left and right children in tree structure), and 2.
40	54	Probabilities from these distributions are then multiplied together to define a transition model M over hidden states: M[qt−1,qt] = P(qt | qt−1) (1a) def = P( ft pt jt a1..Dt b 1..D t | qt−1) (1b) = P( ft | qt−1) ·P(pt | qt−1 ft) ·P( jt | qt−1 ft pt) ·P(a1..Dt | qt−1 ft pt jt) ·P(b1..Dt | qt−1 ft pt jt a1..Dt ).
60	26	(7) A depth-bounded grammar is a set of side- and depth-specific distributions: GD = {Gs,d | s ∈ {L,R}, d ∈ {1..D}}.
62	29	(9) The likelihood is defined as a marginal over bounded PCFG trees τ of the probability of that tree given the grammar times the product of the probability of the word at each time step or token index t given this tree:8 P(w1..T | GD) = ∑ τ P(τ | GD) · ∏ t P(wt | τ).
96	16	K is the number of non-terminal categories in the grammar G, D is the maximum depth, and β is the parameter for the symmetric Dirichlet prior over multinomial distributions in the grammar G. As seen from the previous subsection, the prior is over all possible rules in an unbounded PCFG grammar.
102	16	and Eve parts of the CHILDES corpus (Macwhinney, 1992) to compare with other grammar induction systems on a human-like acquisition task.
118	16	These synthetic datasets are also used to tune the β hyperparameter of the model (as defined in Section 4) to enable it to find optimal modes more quickly.
123	18	Finally, as a gauge of the complexity of this task, results of the model described in this paper are compared with those of other grammar induction models on the center-embedding dataset.
131	25	Model performance is evaluated against Penn Treebank style annotations of both Adam and Eve corpora (Pearl and Sprouse, 2013).
132	19	Table 2 shows the PARSEVAL scores of the DB-PCFG system with different hyperparameters on the Adam corpus for development.The simplest configuration, D1K15 (depth 1 only with 15 non-terminal categories), obtains the best score, so this setting is applied to the test corpus, Eve.
135	38	The UHHMM system is run on the Eve corpus using settings in Shain et al. (2016), which also includes a post-process option to flatten trees (reported here as UHHMM-F).
140	23	When humans acquire grammar, they do not only learn tree structures, they also learn category types: noun phrases, verb phrases, prepositional phrases, and where each type can and cannot occur.
151	28	BMMM+DMV does not produce constituents with labels by default, but can be evaluated using this metric by converting dependency graphs into constituent trees, then labeling each constituent with the part-of-speech tag of the head.
153	19	Table 4 shows the scores for all systems on the Eve dataset and four runs of the DB-PCFG system on these two evaluation metrics.
154	16	Surprisingly the D=2, K=15 model which has the lowest PARSEVAL scores is most accurate at discovering noun phrases.
157	76	The low score of NP agg F1 of DB-PCFG at D1K15 shows a diffusion of induced syntactic categories when the model is trying to find a balance among labeling and branching decisions.
163	22	The first experiment uses the sentences from Wall Street Journal part of the Penn Treebank with at most 20 words (WSJ20).
165	17	We also extract sentences in WSJ20test with at most 10 words from the proposed parses from all systems and report results on them (WSJ10test).
169	17	The F1 difference between the best-performing previouswork system, CCL, and DB-PCFG is highly significant.
182	43	Unlike earlier work this model implements depth bounds directly on PCFGs by derivation, reducing the search space of possible trees for input words without exploding the search space of parameters with multiple sideand depth-specific copies of each rule.
184	126	Moreover, grammars acquired from this model demonstrate a consistent use of category labels, something which has not been demonstrated by other acquisition models.
186	30	First, the model does not assume any universals except independently motivated limits on working memory, which may help address the question of whether universals are indeed necessary for grammar induction.
187	60	Second, the distinction this model draws between its learned unbounded grammar G and its derived bounded grammar GD seems to align with Chomsky’s (1965) distinction between competence and performance, and has the potential to offer some formal guidance to linguistic inquiry about both kinds of models.
