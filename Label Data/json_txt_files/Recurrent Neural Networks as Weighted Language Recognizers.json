{"sections": [{"text": "Proceedings of NAACL-HLT 2018, pages 2261\u20132271 New Orleans, Louisiana, June 1 - 6, 2018. c\u00a92018 Association for Computational Linguistics"}, {"heading": "1 Introduction", "text": "Recurrent neural networks (RNNs) are an attractive apparatus for probabilistic language modeling (Mikolov and Zweig, 2012). Recent experiments show that RNNs significantly outperform other methods in assigning high probability to held-out English text (Jozefowicz et al., 2016).\nRoughly speaking, an RNN works as follows. At each time step, it consumes one input token, updates its hidden state vector, and predicts the next token by generating a probability distribution over all permissible tokens. The probability of an input string is simply obtained as the product of the predictions of the tokens constituting the string followed by a terminating token. In this manner, each RNN defines a weighted language; i.e. a total function from strings to weights. Siegelmann and Sontag (1995) showed that single-layer rational-weight RNNs with saturated linear activation can compute any computable function. To\nthis end, a specific architecture with 886 hidden units can simulate any Turing machine in real-time (i.e., each Turing machine step is simulated in a single time step). However, their RNN encodes the whole input in its internal state, performs the actual computation of the Turing machine when reading the terminating token, and then encodes the output (provided an output is produced) in a particular hidden unit. In this way, their RNN allows \u201cthinking\u201d time (equivalent to the computation time of the Turing machine) after the input has been encoded.\nWe consider a different variant of RNNs that is commonly used in natural language processing applications. It uses ReLU activations, consumes an input token at each time step, and produces softmax predictions for the next token. It thus immediately halts after reading the last input token and the weight assigned to the input is simply the product of the input token predictions in each step.\nOther formal models that are currently used to implement probabilistic language models such as finite-state automata and context-free grammars are by now well-understood. A fair share of their utility directly derives from their nice algorithmic properties. For example, the weighted languages computed by weighted finite-state automata are closed under intersection (pointwise product) and union (pointwise sum), and the corresponding unweighted languages are closed under intersection, union, difference, and complementation (Droste et al., 2013). Moreover, toolkits like OpenFST (Allauzen et al., 2007) and Carmel1 implement efficient algorithms on automata like minimization, intersection, finding the highestweighted path and the highest-weighted string.\nRNN practitioners naturally face many of these same problems. For example, an RNN-\n1https://www.isi.edu/licensed-sw/carmel/\n2261\nbased machine translation system should extract the highest-weighted output string (i.e., the most likely translation) generated by an RNN, (Sutskever et al., 2014; Bahdanau et al., 2014). Currently this task is solved by approximation techniques like heuristic greedy and beam searches. To facilitate the deployment of large RNNs onto limited memory devices (like mobile phones) minimization techniques would be beneficial. Again currently only heuristic approaches like knowledge distillation (Kim and Rush, 2016) are available. Meanwhile, it is unclear whether we can determine if the computed weighted language is consistent; i.e., if it is a probability distribution on the set of all strings. Without a determination of the overall probability mass assigned to all finite strings, a fair comparison of language models with regard to perplexity is simply impossible.\nThe goal of this paper is to study the above problems for the mentioned ReLU-variant of RNNs. More specifically, we ask and answer the following questions: \u2022 Consistency: Do RNNs compute consistent\nweighted languages? Is the consistency of the computed weighted language decidable? \u2022 Highest-weighted string: Can we (efficiently)\ndetermine the highest-weighted string in a computed weighted language? \u2022 Equivalence: Can we decide whether two\ngiven RNNs compute the same weighted language? \u2022 Minimization: Can we minimize the number\nof neurons for a given RNN?"}, {"heading": "2 Definitions and notations", "text": "Before we introduce our RNN model formally, we recall some basic notions and notation. An alphabet \u03a3 is a finite set of symbols, and we write |\u03a3| for the number of symbols in \u03a3. A string s over the alphabet \u03a3 is a finite sequence of zero or more symbols drawn from \u03a3, and we write \u03a3\u2217 for the set of all strings over \u03a3, of which is the empty string. The length of the string s \u2208 \u03a3\u2217 is denoted |s| and coincides with the number of symbols constituting the string. As usual, we write AB for the set of functions {f | f : B \u2192 A}. A weighted language L is a total function L : \u03a3\u2217 \u2192 R from strings to real-valued weights. For example, L(an) = e\u2212n for all n \u2265 0 is such a weighted language.\nWe restrict the weights in our RNNs to the ratio-\nnal numbers Q. In addition, we reserve the use of a special symbol $ to mark the start and end of an input string. To this end, we assume that $ /\u2208 \u03a3 for all considered alphabets, and we let \u03a3$ = \u03a3\u222a{$}. Definition 1. A single-layer RNN R is a 7-tuple \u3008\u03a3, N, h\u22121,W,W \u2032, E,E\u2032\u3009, in which \u2022 \u03a3 is an input alphabet, \u2022 N is a finite set of neurons, \u2022 h\u22121 \u2208 QN is an initial activation vector, \u2022 W \u2208 QN\u00d7N is a transition matrix, \u2022 W \u2032 = (W \u2032a)a\u2208\u03a3$ is a \u03a3$-indexed family of\nbias vectors W \u2032a \u2208 QN , \u2022 E \u2208 Q\u03a3$\u00d7N is a prediction matrix, and \u2022 E\u2032 \u2208 Q\u03a3$ is a prediction bias vector. Next, let us define how such an RNN works. We first prepare our input encoding and the effect of our activation function. For an input string s = s1s2 \u00b7 \u00b7 \u00b7 sn \u2208 \u03a3\u2217 with s1, . . . , sn \u2208 \u03a3, we encode this input as $s$ and thus assume that s0 = $ and sn+1 = $. Our RNNs use ReLUs (Rectified Linear Units), so for every v \u2208 QN we let \u03c3\u3008v\u3009 (the ReLU activation) be the vector \u03c3\u3008v\u3009 \u2208 QN such that\n\u03c3\u3008v\u3009(n) = max ( 0, v(n) ) for every n \u2208 N .\nIn other words, the ReLUs act like identities on nonnegative inputs, but clip negative inputs to 0. We use softmax-predictions, so for every vector p \u2208 Q\u03a3$ and a \u2208 \u03a3$ we let\nsoftmax\u3008p\u3009(a) = e p(a)\n\u2211 a\u2032\u2208\u03a3$ e p(a\u2032) .\nRNNs act in discrete time steps reading a single letter at each step. We now define the semantics of our RNNs. Definition 2. Let R = \u3008\u03a3, N, h\u22121,W,W \u2032, E,E\u2032\u3009 be an RNN, s an input string of length n and 0 \u2264 t \u2264 n a time step. We define \u2022 the hidden state vector hs,t \u2208 QN given by\nhs,t = \u03c3\u3008W \u00b7 hs,t\u22121 +W \u2032st\u3009 ,\nwhere hs,\u22121 = h\u22121 and we use standard matrix product and point-wise vector addition, \u2022 the next-token prediction vector Es,t \u2208 Q\u03a3$\nEs,t = E \u00b7 hs,t + E\u2032\n\u2022 the next-token distribution E\u2032s,t \u2208 R\u03a3$\nE\u2032s,t = softmax\u3008Es,t\u3009 .\nFinally, the RNN R computes the weighted language R : \u03a3\u2217 \u2192 R, which is given for every input s = s1 \u00b7 \u00b7 \u00b7 sn as above by\nR(s) = n\u220f\nt=0\nE\u2032s,t(st+1) .\nIn other words, each component hs,t(n) of the hidden state vector is the ReLU activation applied to a linear combination of all the components of the previous hidden state vector hs,t\u22121 together with a summand W \u2032st that depends on the t-th input letter st. Thus, we often specify hs,t(n) as linear combination instead of specifying the matrix W and the vectors W \u2032a. The semantics is then obtained by predicting the letters s1, . . . , sn of the input s and the final terminator $ and multiplying the probabilities of the individual predictions.\nLet us illustrate these notions on an example. We consider the RNN \u3008\u03a3, N, h\u22121,W,W \u2032, E,E\u2032\u3009 with \u03b3 \u2208 Q and \u2022 \u03a3 = {a} and N = {1, 2}, \u2022 h\u22121 = (\u22121, 0)T and\nW = ( 1 0 1 0 ) and W \u2032$ = W \u2032 a = ( 1 0 )\n\u2022 E($, \u00b7) = (M + 1, \u2212(M + 1)) and E(a, \u00b7) = (1, \u22121) and \u2022 E\u2032($) = \u2212M and E\u2032(a) = 0.\nIn this case, we obtain the linear combinations\nhs,t = \u03c3 \u2329 hs,t\u22121(1) + 1 hs,t\u22121(1) \u232a\ncomputing the next hidden state components. Given the initial activation, we thus obtain hs,t = \u03c3\u3008t, t \u2212 1\u3009. Using this information, we obtain\nEs,t($) = (M + 1) \u00b7 (t\u2212 \u03c3\u3008t\u2212 1\u3009)\u2212M Es,t(a) = t\u2212 \u03c3\u3008t\u2212 1\u3009 .\nConsequently, we assign weight e \u2212M\n1+e\u2212M to input \u03b5,\nweight 1 1+e\u2212M \u00b7\ne1\ne1+e1 to a, and, more generally,\nweight 1 1+e\u2212M \u00b7 1 2n to a n. Clearly the weight assigned by an RNN is always in the interval (0, 1), which enables a probabilistic view. Similar to weighted finite-state automata or weighted context-free grammars, each RNN is a compact, finite representation of a\nweighted language. The softmax-operation enforces that the probability 0 is impossible as assigned weight, so each input string is principally possible. In practical language modeling, smoothing methods are used to change distributions such that impossibility (probability 0) is removed. Our RNNs avoid impossibility outright, so this can be considered a feature instead of a disadvantage.\nThe hidden state hs,t of an RNN can be used as scratch space for computation. For example, with a single neuron n we can count symbols in s via:\nhs,t(n) = \u03c3\u3008hs,t\u22121(n) + 1\u3009 .\nHere the letter-dependent summand W \u2032a is universally 1. Similarly, for an alphabet \u03a3 = {a1, . . . , am} we can use the method of Siegelmann and Sontag (1995) to encode the complete input string s in base m+ 1 using:\nhs,t(n) = \u03c3\u3008(m+ 1)hs,t\u22121(n) + c(st)\u3009 ,\nwhere c : \u03a3$ \u2192 {0, . . . ,m} is a bijection. In principle, we can thus store the entire input string (of unbounded length) in the hidden state value hs,t(n), but our RNN model outputs weights at each step and terminates immediately once the final delimiter $ is read. It must assign a probability to a string incrementally using the chain rule decomposition p(s1 \u00b7 \u00b7 \u00b7 sn) = p(s1) \u00b7 . . . \u00b7 p(sn | s1 \u00b7 \u00b7 \u00b7 sn\u22121).\nLet us illustrate our notion of RNNs on some additional examples. They all use the alphabet \u03a3 = {a} and are illustrated and formally specified in Figure 1. The first column shows an RNN R1 that assigns R1(an) = 2\u2212(n+1). The next-token prediction matrix ensures equal values for a and $ at every time step. The second column shows the RNN R2, which we already discussed. In the beginning, it heavily biases the next symbol prediction towards a, but counters it starting at t = 1. The third RNN R3 uses another counting mechanism with hs,t = \u03c3\u3008t\u2212 100, t\u2212 101, t\u3009. The first two components are ReLU-thresholded to zero until t > 101, at which point they overwhelm the bias towards a turning all future predictions to $."}, {"heading": "3 Consistency", "text": "We first investigate the consistency problem for an RNN R, which asks whether the recognized weighted language R is indeed a probability distribution. Consequently, an RNN R is consistent\nn) = 2\u2212(n+1) R2(\u03b5) \u2248 0 R3(a100) \u2248 1\nif \u2211\ns\u2208\u03a3\u2217 R(s) = 1. We first show that there is an inconsistent RNN, which together with our examples shows that consistency is a nontrivial property of RNNs.2\nWe immediately use a slightly more complex example, which we will later reuse.\nExample 3. Let us consider an arbitrary RNN\nR = \u3008\u03a3, N, h\u22121,W,W \u2032, E,E\u2032\u3009\nwith the single-letter alphabet \u03a3 = {a}, the neurons {1, 2, 3, n, n\u2032} \u2286 N , initial activation h\u22121(i) = 0 for all i \u2208 {1, 2, 3, n, n\u2032}, and the following linear combinations:\nhs,t(1) = \u03c3\u3008hs,t\u22121(1) + hs,t\u22121(n)\u2212 hs,t\u22121(n\u2032)\u3009 2 For comparison, all probabilistic finite-state automata are consistent, provided no transitions exit final states. Not all probabilistic context-free grammars are consistent; necessary and sufficient conditions for consistency are given by Booth and Thompson (1973). However, probabilistic context-free grammars obtained by training on a finite corpus using popular methods (such as expectation-maximization) are guaranteed to be consistent (Nederhof and Satta, 2006).\nhs,t(2) = \u03c3\u3008hs,t\u22121(2) + 1\u3009 hs,t(3) = \u03c3\u3008hs,t\u22121(3) + 3hs,t\u22121(1)\u3009\nEs,t($) = hs,t(3)\u2212 hs,t(2) Es,t(a) = hs,t(2)\nNow we distinguish two cases: Case 1: If hs,t(n) \u2212 hs,t(n\u2032) = 0 for all t \u2208 N, then hs,t(1) = 0 and hs,t(2) = t + 1 and hs,t(3) = 0. Hence we have Es,t($) = \u2212(t + 1) and Es,t(a) = t + 1. In this case the termination probability\nE\u2032s,t($) = e\u2212(t+1)\ne\u2212(t+1) + et+1 =\n1\n1 + e2(t+1)\n(i.e., the likelihood of predicting $) shrinks rapidly towards 0, so the RNN assigns less than 15% of the probability mass to the terminating sequences (i.e., the finite strings), so the RNN is inconsistent (see Lemma 15 in the appendix).\nCase 2: Suppose that there exists a time\npoint T \u2208 N such that for all t \u2208 N\nhs,t(n)\u2212 hs,t(n\u2032) = {\n1 if t = T 0 otherwise.\nThen hs,t(1) = 0 for all t \u2264 T and hs,t(1) = 1 otherwise. In addition, we have hs,t(2) = t + 1 and hs,t(3) = \u03c3\u30083(t\u2212 T \u2212 1)\u3009. Hence we have\nEs,t($) = \u03c3\u30083(t\u2212 T \u2212 1)\u3009 \u2212 (t+ 1)\n= { \u2212(t+ 1) if t \u2264 T 2t\u2212 3T \u2212 4 otherwise\nEs,t(a) = t+ 1 ,\nwhich shows that the probability\nE\u2032s,t($) =    1 1+e2(t+1) if t \u2264 T et\u22123T\u22125\n1+et\u22123T\u22125 otherwise\nof predicting $ increases over time and eventually (for t 3T ) far outweighs the probability of predicting a. Consequently, in this case the RNN is consistent (see Lemma 16 in the appendix).\nWe have seen in the previous example that consistency is not trivial for RNNs, which takes us to the consistency problem for RNNs:\nConsistency: Given an RNN R, return \u201cyes\u201d if R is consistent and \u201cno\u201d otherwise.\nWe recall the following theorem, which, combined with our example, will prove that consistency is unfortunately undecidable for RNNs.\nTheorem 4 (Theorem 2 of Siegelmann and Sontag (1995)). Let M be an arbitrary deterministic Turing machine. There exists an RNN\nR = \u3008\u03a3, N, h\u22121,W,W \u2032, E,E\u2032\u3009\nwith saturated linear activation, input alphabet \u03a3 = {a}, and 1 designated neuron n \u2208 N such that for all s \u2208 \u03a3\u2217 and 0 \u2264 t \u2264 |s| \u2022 hs,t(n) = 0 if M does not halt on \u03b5, and \u2022 if M does halt on empty input after T steps,\nthen\nhs,t(n) = { 1 if t = T 0 otherwise.\nIn other words, such RNNs with saturated linear activation can semi-decide halting of an arbitrary Turing machine in the sense that a particular neuron achieves value 1 at some point during\nthe evolution if and only if the Turing machine halts on empty input. An RNN with saturated linear activation is an RNN following our definition with the only difference that instead of our ReLU-activation \u03c3 the following saturated linear activation \u03c3\u2032 : QN \u2192 QN is used. For every vector v \u2208 QN and n \u2208 N , let\n\u03c3\u2032\u3008v\u3009(n) =    0 if v(n) < 0 v(n) if 0 \u2264 v(n) \u2264 1 1 if v(n) > 1 .\nSince \u03c3\u2032\u3008v\u3009 = \u03c3\u3008v\u3009 \u2212 \u03c3\u3008v\u2212~1\u3009 for all v \u2208 QN , and the right-hand side is a linear transformation, we can easily simulate saturated linear activation in our RNNs. To this end, each neuron n \u2208 N of the original RNN R = \u3008\u03a3, N, h\u22121, U, U \u2032, E,E\u2032\u3009 is replaced by two neurons n1 and n2 in the new RNN R\u2032 = \u3008\u03a3, N \u2032, h\u2032\u22121, V, V \u2032, F, F \u2032\u3009 such that hs,t(n) = h \u2032 s,t(n1) \u2212 h\u2032s,t(n2) for all s \u2208 \u03a3\u2217 and 0 \u2264 t \u2264 |s|, where the evaluation of h\u2032s,t is performed in the RNNR\u2032. More precisely, we use the transition matrix V and bias function V \u2032, which is given by\nV (n1, n \u2032 1) = V (n2, n \u2032 1) = U(n, n \u2032) V (n1, n \u2032 2) = V (n2, n \u2032 2) = \u2212U(n, n\u2032)\nV \u2032a(n1) = U \u2032 a(n) V \u2032a(n2) = U \u2032 a(n)\u2212 1\nh\u2032\u22121(n1) = h\u22121(n) h\u2032\u22121(n2) = 0\nfor all n, n\u2032 \u2208 N and a \u2208 \u03a3 \u222a {$}, where n1 and n2 are the two neurons corresponding to n and n\u20321 and n \u2032 2 are the two neurons corresponding to n\u2032 (see Lemma 17 in the appendix).\nCorollary 5. Let M be an arbitrary deterministic Turing machine. There exists an RNN\nR = \u3008\u03a3, N, h\u22121,W,W \u2032, E,E\u2032\u3009\nwith input alphabet \u03a3 = {a} and 2 designated neurons n1, n2 \u2208 N such that for all s \u2208 \u03a3\u2217 and 0 \u2264 t \u2264 |s| \u2022 hs,t(n1) \u2212 hs,t(n2) = 0 if M does not halt\non \u03b5, and \u2022 if M does halt on empty input after T steps,\nthen\nhs,t(n1)\u2212 hs,t(n2) = {\n1 if t = T 0 otherwise.\nWe can now use this corollary together with the RNNR of Example 3 to show that the consistency problem is undecidable. To this end, we simulate a given Turing machine M and identify the two designated neurons of Corollary 5 as n and n\u2032 in Example 3. It follows that M halts if and only if R is consistent. Hence we reduced the undecidable halting problem to the consistency problem, which shows the undecidability of the consistency problem.\nTheorem 6. The consistency problem for RNNs is undecidable.\nAs mentioned in Footnote 2, probabilistic context-free grammars obtained after training on a finite corpus using the most popular methods are guaranteed to be consistent. At least for 2-layer RNNs this does not hold.\nTheorem 7. A two-layer RNN trained to a local optimum using Back-propagation-throughtime (BPTT) on a finite corpus is not necessarily consistent.\nProof. The first layer of the RNN R with a single alphabet symbol a uses one neuron n\u2032 and has the following behavior:\nh\u22121(n\u2032) = 0 hs,t(n \u2032) = \u03c3\u3008hs,t\u22121(n\u2032) + 1\u3009\nThe second layer uses neuron n and takes hs,t(n\u2032) as input at time t:\nhs,t(n) = \u03c3\u3008hs,t(n\u2032)\u2212 2\u3009 Es,t(a) = hs,t(n) Es,t($) = 0\nE\u2032s,t(a) = { 1 2 if t \u2264 1 e(t\u22121)\n1+e(t\u22121) otherwise.\nLet the training data be {a}. Then the objective we wish to maximize is simply R(a). The derivative of this objective with respect to each parameter is 0, so applying gradient descent updates does not change any of the parameters and we have converged to an inconsistent RNN.\nIt remains an open question whether there is a single-layer RNN that also exhibits this behavior."}, {"heading": "4 Highest-weighted string", "text": "Given a function f : \u03a3\u2217 \u2192 R we are often interested in the highest-weighted string. This corresponds to the most likely sentence in a language\nmodel or the most likely translation for a decoder RNN in machine translation.\nFor deterministic probabilistic finite-state automata or context-free grammars only one path or derivation exists for any given string, so the identification of the highest-weighted string is the same task as the identification of the most probable path or derivation. However, for nondeterministic devices, the highest-weighted string is often harder to identify, since the weight of a string is the sum of the probabilities of all possible paths or derivations for that string. A comparison of the difficulty of identifying the most probable derivation and the highest-weighted string for various models is summarized in Table 1, in which we marked our results in bold face.\nWe present various results concerning the difficulty of identifying the highest-weighted string in a weighted language computed by an RNN. We also summarize some available algorithms. We start with the formal presentation of the three studied problems.\n1. Best string: Given an RNNR and c \u2208 (0, 1), does there exist s \u2208 \u03a3\u2217 with R(s) > c? 2. Consistent best string: Given a consistent RNN R and c \u2208 (0, 1), does there exist s \u2208 \u03a3\u2217 with R(s) > c? 3. Consistent best string of polynomial length: Given a consistent RNN R, polynomial P with P(x) \u2265 x for x \u2208 N+, and c \u2208 (0, 1), does there exist s \u2208 \u03a3\u2217 with |s| \u2264 P(|R|) and R(s) > c? As usual the corresponding optimization problems are not significantly simpler than these decision problems. Unfortunately, the general problem is also undecidable, which can easily be shown using our example.\n3Restricted to solutions of polynomial length 4Dijkstra shortest path / (Knuth, 1977) 5(Casacuberta and de la Higuera, 2000) / (Simaan, 1996)\nTheorem 8. The best string problem for RNNs is undecidable.\nProof. Let M be an arbitrary Turing machine and again consider the RNN R of Example 3 with the neurons n and n\u2032 identified with the designated neurons of Corollary 5. We note that R(\u03b5) = 1\n1+e2 < 0.12 in both cases. If M does\nnot halt, then R(an) \u2264 1 1+e2(n+1) \u2264 1 1+e2\n< 0.12 for all n \u2208 N. On the other hand, if M halts after T steps, then\nR(a3T\u22125)\n= ( T\u220f\nt=0\ne2(t+1)\n1 + e2(t+1)\n) \u00b7 ( 3T\u22126\u220f\nt=T+1\n1\n1 + et\u22123T\u22125\n) \u00b7 1\n2\n\u2265 2 (\u22121, e\u22122)\u221e\n\u00b7 ( 3T\u22126\u220f\nt=T+1\ne3T+5\u2212t\ne3T+5\u2212t+1\n) \u00b7 1\n2\n\u2265 2 (\u22121, e\u22122)\u221e \u00b7 (\u22121, e\u22121)\u221e \u2265 0.25\nusing Lemma 14 in the appendix. Consequently, a string with weight above 0.12 exists if and only if M halts, so the best string problem is also undecidable.\nIf we restrict the RNNs to be consistent, then we can easily decide the best string problem by simple enumeration.\nTheorem 9. The consistent best string problem for RNNs is decidable.\nProof. Let R be the RNN over alphabet \u03a3 and c \u2208 (0, 1) be the bound. Since \u03a3\u2217 is countable, we can enumerate it via f : N \u2192 \u03a3\u2217. In the algorithm we compute Sn = \u2211n i=0R(f(i)) for increasing values of n. If we encounter a weight R(f(n)) > c, then we stop with answer \u201cyes.\u201d Otherwise we continue until Sn > 1\u2212 c, at which point we stop with answer \u201cno.\u201d\nSince R is consistent, limi\u2192\u221e Si = 1, so this algorithm is guaranteed to terminate and it obviously decides the problem.\nNext, we investigate the length |wmaxR | of the shortest string wmaxR of maximal weight in the weighted language R generated by a consistent RNN R in terms of its (binary storage) size |R|. As already mentioned by Siegelmann and Sontag (1995) and evidenced here, only small precision rational numbers are needed in our constructions, so we assume that |R| \u2264 c \u00b7 |N |2 for a (reasonably small) constant c, where N is the set of neurons\nof R. We show that no computable bound on the length of the best string can exist, so its length can surpass all reasonable bounds.\nTheorem 10. Let f : N+ \u2192 N be the function with\nf(n) = max consistent RNN R |R|\u2264n |wmaxR |\nfor all n \u2208 N+. There exists no computable function g : N\u2192 N with g(n) \u2265 f(n) for all n \u2208 N. Proof. In the previous section (before Theorem 6) we presented an RNN RM that simulates an arbitrary (single-track) Turing machine M with n states. By Siegelmann and Sontag (1995) we have |RM | \u2264 c \u00b7 (4n + 16). Moreover, we observed that this RNN RM is consistent if and only if the Turing machine M halts on empty input. In the proof of Theorem 8 we have additionally seen that the length |wmaxR | of its best string exceeds the number TM of steps required to halt.\nFor every n \u2208 N, let BB(n) be the n-th \u201cBusy Beaver\u201d number (Rado\u0301, 1962), which is\nBB(n) = max normalized n-state Turing machine M with\n2 tape symbols that halts on empty input\nTM\nIt is well-known that BB : N+ \u2192 N cannot be bounded by any computable function. However,\nBB(n) \u2264 max normalized n-state Turing machine M with and 2 tape symbols that halts on empty input |wmaxRM |\n\u2264 max consistent RNN R |R|\u2264c\u00b7(4n+16) |wmaxR |\n= f(4nc+ 16c) ,\nso f clearly cannot be computable and no computable function g can provide bounds for f .\nFinally, we investigate the difficulty of the best string problem for consistent RNN restricted to solutions of polynomial length.\nTheorem 11. Identifying the best string of polynomial length in a consistent RNN is NP-complete.\nProof. To show NP-hardness, we reduce from the 3-SAT problem. Let x1, . . . , xm be m Boolean variables and\nF = k\u2227\ni=1\n( `i1 \u2228 `i2 \u2228 `i3 ) ,\nbe a formula in conjunctive normal form, where `ij \u2208 {x1, . . . , xm,\u00acx1, . . . ,\u00acxm}. 3-SAT asks whether there is a setting of xis that makes F true.\nWe initialize h\u22121(n) = 0, \u2200 n \u2208 N = {x1, . . . , xm, c1, . . . , ck, c\u20321, . . . , c\u2032k, F, n1, n2, n3, ?}. Let s \u2208 {0, 1}\u2217 be the input string. Denote the value of F when xj = sj for all j \u2208 [m] as F (s). Let t \u2208 N with t \u2264 |s|. Set hs,t(xm) = \u03c3\u3008I(st)\u3009, where I(0) = I($) = 0 and I(1) = 1. This stores the current input symbol in neuron xm, so hs,t(xm) = I(st). In addition, we let hs,t(xj) = \u03c3\u3008hs,t\u22121(xj+1)\u3009 for all j \u2208 [m \u2212 1]. Consequently, for all j \u2208 [m]\nhs,t(xj) = { I(st\u2212(m\u2212j)) if m\u2212 j \u2264 t 0 otherwise.\nNext, we evaluate the clauses. For each i \u2208 [k], we use two neurons ci and c\u2032i such that\nhs,t(ci) = \u03c3\u3008fs,t(`i1) + fs,t(`i2) + fs,t(`i3)\u3009 hs,t(c \u2032 i) = \u03c3\u3008fs,t(`i1) + fs,t(`i2) + fs,t(`i3)\u2212 1\u3009,\nwhere fs,t(xm) = I(st), fs,t(\u00acxm) = 1 \u2212 I(st), and \u2200j \u2208 [m \u2212 1], fs,t(xj) = hs,t\u22121(xj+1), fs,t(\u00acxj) = 1 \u2212 hs,t\u22121(xj+1). Note that hs,t(ci) \u2212 hs,t(c\u2032i) contains the evaluation of the clause `i1 \u2228 `i2 \u2228 `i3. Let\nhs,t(F ) = \u03c3 \u2329 k\u2211\ni=1\n( hs,t\u22121(ci)\u2212hs,t\u22121(c\u2032i) ) \u2212k+1 \u232a ,\nso hs,t(F ) = F (s) contains the evaluation of the formula F using the values in neurons x1, . . . , xm.\nWe use three counters n1, n2, n3 to ensure that the only relevant inputs are of length m+ 2:\nhs,t(n1) = \u03c3\u3008hs,t\u22121(n3)\u2212 (m+ 2)\u3009 hs,t(n2) = \u03c3\u3008hs,t\u22121(n3)\u2212 (m+ 1)\u3009 hs,t(n3) = \u03c3\u3008hs,t\u22121(n3) + 1\u3009 ,\nwhich yields hs,t(n3) = t + 1, hs,t(n2) = \u03c3\u3008t \u2212 (m + 1)\u3009, and hs,t(n1) = \u03c3\u3008t\u2212 (m+ 2)\u3009.\nOur goal neuron is ?, which we set to\nhs,t(?) = \u03c3\u3008hs,t\u22121(F )\u2212hs,t\u22121(n1)+hs,t\u22121(n2)\u22121\u3009\nso that\nhs,t(?) = { hs,t\u22121(F ) if t = m+ 2 0 otherwise,\nso hs,t(?) = 1 if and only if t = m + 2 and F (s) = 1.\nLet m\u2032 = m+ 4. The output is set as follows:\nEs,t(0) = Es,t(1) = m \u2032(1\u2212 2hs,t(?) )\nEs,t($) = \u2212m\u2032 ( 1\u2212 2hs,t(?) ) ,\nThis yieldsEs,t(0) = Es,t(1) = \u2212Es,t($) = \u2212m\u2032 if t = m+2 and F (s) = 1, andm\u2032 otherwise. For a \u2208 {0, 1},\nE\u2032s,t(a)=    e\u2212m \u2032 2e\u2212m\u2032+em\u2032 if t=m+2 and F (s)=1 em \u2032\n2em\u2032+e\u2212m\u2032 otherwise\nE\u2032s,t($)=   \nem \u2032\n2e\u2212m\u2032+em\u2032 if t=m+2 and F (s)=1\ne\u2212m \u2032\n2em\u2032+e\u2212m\u2032 otherwise.\nFinally, we set the threshold \u03be = 3\u2212m \u2032 . When |s| 6= m + 2, sm+3 6= $, so the weight of s contains the factor e \u2212m\u2032\n2e\u2212m\u2032+em\u2032 = 1 2+e2m\u2032 and thus is\nupper-bounded by 1 2+e2m\u2032 < \u03be. Hence no input of length different from m+ 2 achieves a weight that exceeds \u03be. A string s of length m+ 2 achieves the weight ws given by\nws=   \nem \u2032 2e\u2212m\u2032+em\u2032 \u00b7\u220fm+2i=1 e m\u2032 2em\u2032+e\u2212m\u2032 if F (s)=1\ne\u2212m \u2032 2em\u2032+e\u2212m\u2032 \u00b7\u220fm+2i=1 e m\u2032 2em\u2032+e\u2212m\u2032 otherwise.\nWhen F (s) = 0, ws < e \u2212m\u2032\n2em\u2032+e\u2212m\u2032 < \u03be, so\nif F is unsatisfiable, no input string achieves a weight above the threshold \u03be. When F (s) = 1, ws = em \u2032 2e\u2212m\u2032+em\u2032 \u00b7 ( em \u2032 2em\u2032+e\u2212m\u2032 )m+2 > \u03be. An input string with weight above \u03be exists if and only if F is satisfiable. Obviously, the reduction can be computed in polynomial time since all constants can be computed in logarithmic space. The constructed RNN is consistent, since the output prediction is constant after m+ 3 steps."}, {"heading": "5 Equivalence", "text": "We prove that equivalence of two RNNs is undecidable. For comparison, equivalence of two deterministic WFSAs can be tested in time O(|\u03a3|(|QA| + |QB|)3), where |QA|, |QB| are the number of states of the two WFSAs and |\u03a3| is the size of the alphabet (Cortes et al., 2007); equivalence of nondeterministic WFSAs are undecidable (Griffiths, 1968). The decidability of language equivalence for deterministic probabilistic pushdowntown automata (PPDA) is still open (Forejt et al., 2014), although equivalence for deterministic unweighted push-downtown automata (PDA) is decidable (Se\u0301nizergues, 1997).\nThe equivalence problem is formulated as follows:\nEquivalence: Given two RNNsR andR\u2032, return \u201cyes\u201d if R(s) = R\u2032(s) for all s \u2208 \u03a3\u2217, and \u201cno\u201d otherwise.\nTheorem 12. The equivalence problem for RNNs is undecidable.\nProof. We prove by contradiction. Suppose Turing machine M decides the equivalence problem. Given any deterministic Turing Machine M \u2032, construct the RNN R that simulates M \u2032 on input as described in Corollary 5. Let Es,t(a) = 0 and Es,t($) = hs,t(n1)\u2212hs,t(n2). If M \u2032 does not halt on , for all t \u2208 N, E\u2032s,t(a) = E\u2032s,t($) = 1/2; if M \u2032 halts after T steps, E\u2032s,T (a) = 1/(e + 1), Es,T ($) = e/(e + 1). Let R\u2032 be the trivial RNN that computes {an : P (an) = 2\u2212(n+1), n \u2265 0}. We run M on input \u3008R,R\u2032\u3009. If M returns \u201cno\u201d, M \u2032 halts on x, else it does not halt. Therefore the Halting Problem would be decidable if equivalence is decidable. Therefore equivalence is undecidable."}, {"heading": "6 Minimization", "text": "We look next at minimization of RNNs. For comparison, state-minimization of a deterministic PFSA is O(|E| log |Q|) where |E| is the number of transitions and |Q| is the number of states (Aho et al., 1974). Minimization of a non-deterministic PFSA is PSPACE-complete (Jiang and Ravikumar, 1993).\nWe focus on minimizing the number of hidden neurons (|N |) in RNNs: Minimization: Given RNN R and non-negative integer n, return \u201cyes\u201d if \u2203 RNN R\u2032 with number of hidden units |N \u2032| \u2264 n such that R(s) = R\u2032(s) for all s \u2208 \u03a3\u2217, and \u201cno\u201d otherwise.\nTheorem 13. RNN minimization is undecidable.\nProof. We reduce from the Halting Problem. Suppose Turing Machine M decides the minimization problem. For any Turing Machine M \u2032, construct the same RNN R as in Theorem 12. We run M on input \u3008R, 0\u3009. Note that an RNN with no hidden unit can only output constant E\u2032s,t for all t. Therefore the number of hidden units in R can be minimized to 0 if and only if it always outputs E\u2032s,t(a) = E \u2032 s,t($) = 1/2. If M returns \u201cyes\u201d, M \u2032 does not halt on , else it halts."}, {"heading": "7 Conclusion", "text": "We proved the following hardness results regarding RNN as a recognizer of weighted languages:\n1. Consistency: (a) Inconsistent RNNs exist. (b) Consistency of RNNs is undecidable. 2. Highest-weighted string: (a) Finding the highest-weighted string for\nan arbitrary RNN is undecidable. (b) Finding the highest-weighted string for\na consistent RNN is decidable, but the solution length can surpass all computable bounds. (c) Restricting to solutions of polynomial length, finding the highest-weighted string is NP-complete. 3. Testing equivalence of RNNs and minimizing the number of neurons in an RNN are both undecidable.\nAlthough our undecidability results are upshots of the Turing-completeness of RNN (Siegelmann and Sontag, 1995), our NP-completeness result is original, and surprising, since the analogous hardness results in PFSA relies on the fact that there are multiple derivations for a single string (Casacuberta and de la Higuera, 2000). The fact that these results hold for the relatively simple RNNs we used in this paper suggests that the case would be the same for more complicated models used in NLP, such as long short term memory networks (LSTMs; Hochreiter and Schmidhuber 1997).\nOur results show the non-existence of (efficient) algorithms for interesting problems that researchers using RNN in natural language processing tasks may have hoped to find. On the other hand, the non-existence of such efficient or exact algorithms gives evidence for the necessity of approximation, greedy or heuristic algorithms to solve those problems in practice. In particular, since finding the highest-weighted string in RNN is the same as finding the most-likely translation in a sequence-to-sequence RNN decoder, our NPcompleteness result provides some justification for employing greedy and beam search algorithms in practice."}, {"heading": "Acknowledgments", "text": "This work was supported by DARPA (W911NF15-1-0543 and HR0011-15-C-0115). Andreas Maletti was financially supported by DFG Graduiertenkolleg 1763 (QuantLA)."}], "year": 2018, "references": [{"title": "The design and analysis of computer algorithms", "authors": ["Alfred V. Aho", "John E. Hopcroft", "Jeffrey D. Ullman."], "venue": "Addison-Wesley.", "year": 1974}, {"title": "OpenFst: A General and Efficient Weighted Finite-State Transducer Library", "authors": ["Cyril Allauzen", "Michael Riley", "Johan Schalkwyk", "Wojciech Skut", "Mehryar Mohri"], "year": 2007}, {"title": "Neural machine translation by jointly learning to align and translate", "authors": ["D. Bahdanau", "K. Cho", "Y. Bengio."], "venue": "Proc. ICLR.", "year": 2014}, {"title": "Applying probability measures to abstract languages", "authors": ["T.L. Booth", "R.A. Thompson."], "venue": "IEEE Transactions on Computers C-22(5):442\u2013450. https://doi.org/10.1109/t-c.1973.223746.", "year": 1973}, {"title": "Computational complexity of problems on probabilistic grammars and transducers", "authors": ["Francisco Casacuberta", "Colin de la Higuera."], "venue": "Grammatical Inference: Algorithms and Applications Lecture Notes in Computer Science pages 15\u201324. https://doi.org/", "year": 2000}, {"title": "Lp distance and equivalence of probabilistic automata", "authors": ["Corinna Cortes", "Mehryar Mohri", "Ashish Rastogi."], "venue": "International Journal of Foundations of Computer Science 18(04):761\u2013779. https://doi.org/", "year": 2007}, {"title": "Handbook of Weighted Automata", "authors": ["Manfred Droste", "Werner Kuich", "Heiko Vogler."], "venue": "Springer Berlin.", "year": 2013}, {"title": "Language equivalence of probabilistic pushdown automata", "authors": ["Vojtch Forejt", "Petr Janar", "Stefan Kiefer", "James Worrell."], "venue": "Information and Computation 237:1\u201311. https://doi.org/10.1016/j.ic.2014.04. 003.", "year": 2014}, {"title": "The unsolvability of the equivalence problem for \u2227-free nondeterministic generalized machines", "authors": ["T.V. Griffiths."], "venue": "Journal of the ACM 15(3):409\u2013413. https://doi.org/10.1145/321466.321473.", "year": 1968}, {"title": "Long short-term memory", "authors": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Comput. 9(8):1735\u2013 1780. https://doi.org/10.1162/neco.1997.9.8.1735.", "year": 1997}, {"title": "Minimal NFA problems are hard", "authors": ["Tao Jiang", "B. Ravikumar."], "venue": "SIAM Journal on Computing 22(6):1117\u20131141. https://doi.org/10.1137/0222067.", "year": 1993}, {"title": "Exploring the limits of language modeling", "authors": ["Rafal Jozefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu."], "venue": "https://arxiv.org/pdf/1602. 02410.pdf.", "year": 2016}, {"title": "Sequencelevel knowledge distillation", "authors": ["Yoon Kim", "Alexander M. Rush."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Austin, Texas, pages 1317\u20131327.", "year": 2016}, {"title": "A generalization of Dijkstra\u2019s algorithm", "authors": ["Donald E. Knuth."], "venue": "Information Processing Letters 6(1):1\u20135. https://doi.org/10.1016/0020-0190(77)90002-3.", "year": 1977}, {"title": "Context dependent recurrent neural network language model", "authors": ["T. Mikolov", "G. Zweig."], "venue": "2012 IEEE Spoken Language Technology Workshop (SLT). pages 234\u2013239. https://doi.org/10.1109/SLT. 2012.6424228.", "year": 2012}, {"title": "Estimation of consistent probabilistic context-free grammars", "authors": ["Mark-Jan Nederhof", "Giorgio Satta."], "venue": "Proceedings of the Human Language Technology Conference of the NAACL, Main Conference. Association for Computational Linguistics,", "year": 2006}, {"title": "On non-computable functions", "authors": ["Tibor Rad\u00f3."], "venue": "Bell System Technical Journal 41:877\u2013884.", "year": 1962}, {"title": "The equivalence problem for deterministic pushdown automata is decidable", "authors": ["G\u00e9raud S\u00e9nizergues."], "venue": "Proc. Automata, Languages and Programming: 24th International Colloquium, Springer Berlin Heidelberg, pages 671\u2013681.", "year": 1997}, {"title": "On the computational power of neural nets", "authors": ["Hava T. Siegelmann", "Eduardo D. Sontag."], "venue": "Journal of Computer and System Sciences 50(1):132\u2013150. https://doi.org/10.1006/jcss.1995.1013.", "year": 1995}, {"title": "Computational complexity of probabilistic disambiguation by means of treegrammars", "authors": ["Khalil Simaan."], "venue": "Proc. COLING. pages 1175\u20131180. https://doi.org/10.3115/993268.993392.", "year": 1996}, {"title": "Sequence to sequence learning with neural networks", "authors": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems", "year": 2014}], "id": "SP:526061c9d346a315efa64a889c04042673d3f5df", "authors": [{"name": "Yining Chen", "affiliations": []}, {"name": "Sorcha Gilroy", "affiliations": []}, {"name": "Andreas Maletti", "affiliations": []}, {"name": "Kevin Knight", "affiliations": []}], "abstractText": "We investigate the computational complexity of various problems for simple recurrent neural networks (RNNs) as formal models for recognizing weighted languages. We focus on the single-layer, ReLU-activation, rationalweight RNNs with softmax, which are commonly used in natural language processing applications. We show that most problems for such RNNs are undecidable, including consistency, equivalence, minimization, and the determination of the highest-weighted string. However, for consistent RNNs the last problem becomes decidable, although the solution length can surpass all computable bounds. If additionally the string is limited to polynomial length, the problem becomes NP-complete. In summary, this shows that approximations and heuristic algorithms are necessary in practical applications of those RNNs.", "title": "Recurrent Neural Networks as Weighted Language Recognizers"}