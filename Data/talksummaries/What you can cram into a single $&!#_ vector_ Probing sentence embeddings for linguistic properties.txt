0	126	Despite Ray Mooney’s quip that you cannot cram the meaning of a whole %&!$# sentence into a single $&!#* vector, sentence embedding methods have achieved impressive results in tasks ranging from machine translation (Sutskever et al., 2014; Cho et al., 2014) to entailment detection (Williams et al., 2018), spurring the quest for “universal embeddings” trained once and used in a variety of applications (e.g., Kiros et al., 2015; Conneau et al., 2017; Subramanian et al., 2018).
2	29	However, real-life “downstream” tasks require complex forms of inference, making it difficult to pinpoint the information a model is relying upon.
5	80	For example, Lai and Hockenmaier (2014) show that the simple heuristic of checking for explicit negation words leads to good accuracy in the SICK sentence entailment task.
32	33	For each task, we construct training sets containing 100k sentences, and 10k-sentence validation and test sets.
40	68	The task is to tell which of the 1k words a sentence contains (1k-way classification).
42	99	Syntactic information The next batch of tasks test whether sentence embeddings are sensitive to syntactic properties of the sentences they encode.
47	34	In the resulting data set, tree depth values range from 5 to 12, and the task is to categorize sentences into the class corresponding to their depth (8 classes).
50	21	In the top constituent task (TopConst), sentences must be classified in terms of the sequence of top constituents immediately below the sentence (S) node.
56	24	Note that, while we would not expect an untrained human subject to be explicitly aware of tree depth or top constituency, similar information must be implicitly computed to correctly parse sentences, and there is suggestive evidence that the brain tracks something akin to tree depth during sentence processing (Nelson et al., 2017).
74	47	The task is to tell whether a sentence is intact or modified.
82	24	To estimate this quantity, one linguistically-trained author checked the annotation of 200 randomly sampled test sentences from each task.
106	32	We use the Stanford parser to generate trees for Europarl source English sentences.
107	21	We train SkipThought vectors (Kiros et al., 2015) by predicting the next sentence given the current one (Tang et al., 2017), on 30M sentences from the Toronto Book Corpus, excluding those in the probing sets.
113	26	Training task performance and further details are in Appendix.
114	63	Baselines Baseline and human-bound performance are reported in the top block of Table 2.
115	20	Length is a linear classifier with sentence length as sole feature.
127	35	More interestingly, BoV is very good at the Tense, SubjNum, ObjNum, and TopConst tasks (much better than the word-based baselines), and well above chance in TreeDepth.
136	22	Classification performed by a MLP with sigmoid nonlinearity, taking pre-learned sentence embeddings as input (see Appendix for details and logistic regression results).
139	22	BoV can rely on this information to noisily predict the correct class.
141	49	An interesting observation in Table 2 is that different encoder architectures trained with the same objective, and achieving similar performance on the training task,4 can lead to linguistically different embeddings, as indicated by the probing tasks.
144	161	We also replicate the finding of Conneau et al. (2017) that BiLSTM-max outperforms BiLSTM-last both in the downstream tasks (see Appendix) and in the probing tasks (Table 2).
145	28	Interestingly, the latter only outperforms the former in SentLen, a task that captures a superficial aspect of sentences (how many words they contain), that could get in the way of inducing more useful linguistic knowledge.
153	34	AutoEncoder training leads, unsurprisingly, to a model excelling at SentLen, but it attains low performance in the WC prediction task.
154	74	This curious result might indicate that the latter information is stored in the embeddings in a complex way, not easily readable by our MLP.
161	68	This architecture must encode priors that are intrinsically good for sentence representations.
166	23	For example, the sentence “I didn’t come here to reunite (orig.
188	54	Just relying on the words contained in the input sentences can get you a long way.
199	43	Question classification is certainly an outlier among our downstream tasks, but we must leave a full understanding of this behaviour to future work (this is exactly the sort of analysis our probing tasks should stimulate).
214	54	We further uncovered interesting patterns of correlation between the probing tasks and more complex “downstream” tasks, and presented a set of intriguing findings about the linguistic properties of various embedding methods.
215	25	For example, we found that Bag-of-Vectors is surprisingly good at capturing sentence-level properties, thanks to redundancies in natural linguistic input.
