38	3	We show how to select features for the latter statistic to maximize a lower bound on the power of its corresponding statistical test.
40	4	Consider two random variables X ∈ X ⊆ Rdx and Y ∈ Y ⊆ Rdy .
43	7	Assume that k : X × X → R and l : Y × Y → R are positive definite kernels associated with reproducing kernel Hilbert spaces (RKHS)Hk andHl, respectively.
44	5	Let ‖ · ‖HS be the norm on the space ofHl → Hk Hilbert-Schmidt operators.
47	3	Gretton et al. (2005, Theorem 4) show that if the kernels k and l are universal (Steinwart & Christmann, 2008) on compact domains X and Y , then HSIC(X,Y ) = 0 if and only if X and Y are independent.
48	4	Given a joint sample Zn = {(xi,yi)}ni=1 ∼ Pxy, an empirical estimator of HSIC can be computed in O(n2) time by replacing the population expectations in (1) with their corresponding empirical expectations based on Zn.
54	4	, u(vJ ,wJ)) >, and {(vi,wi)}Ji=1 are realizations from an absolutely continuous distribution (wrt the Lebesgue measure).
56	3	Definition 1 (Analytic kernels (Chwialkowski et al., 2015)).
67	5	The intuitive explanation of this property is as follows.
80	8	A straightforward empirical estimator of FSIC2 is then given by F̂SIC2(Zn) = 1 J J∑ i=1 û(vi,wi) 2, û(v,w) := µ̂xy(v,w)− µ̂xµy(v,w) (3) = 2 n(n− 1) ∑ i<j h(v,w)((xi,yi), (xj ,yj)), (4) where h(v,w)((x,y), (x′,y′)) := 12 (k(x,v) − k(x′,v))(l(y,w) − l(y′,w)).
91	5	, tJ}, û is a one-sample secondorder multivariate U-statistic with a U-statistic kernel ht.
102	3	Let Σ̂ be a consistent estimate of Σ based on the joint sample Zn, where Σ is defined in Proposition 4.
138	4	If we set K = Kg,L = Lg, and V = V ,r for some , r > 0, then c̃ <∞ as Kg,Lg, and V ,r are compact.
139	3	In practice, these conditions do not necessarily create restrictions as they almost always hold implicitly.
154	3	The training set is used to compute λ̂n (an estimate of λn) to optimize for θ∗, and the test set is used for the actual independence test with the optimized θ∗.
156	3	To better understand the behaviour of N̂FSIC2, we visualize µ̂xy(v,w), µ̂xµy(v,w) and Σ̂(v,w) as a function of one test location (v,w) on a simple toy problem.
169	3	Code is available at https://github.com/wittawatj/fsic-test.
170	4	We compare the proposed NFSIC with optimization (NFSICopt) to five multivariate nonparametric tests.
189	5	We observe that initializing VJ by randomly picking J points from the training sample yields good performance.
190	5	The regularization parameter γn in NFSIC is fixed to a small value, and is not optimized.
191	3	It is worth emphasizing that the complexity of the optimization procedure is still linear-time.2 Since FSIC, NyHFSIC and RDC rely on a finitedimensional kernel approximation, these tests are consistent only if both the number of features increases with n. By constrast, the proposed NFSIC requires only n to go to infinity to achieve consistency i.e., J can be fixed.
195	6	The two variables are independently drawn from the standard multivariate normal distribution i.e., X ∼ N (0, Idx) and Y ∼ N (0, Idy ) where Id is the d× d identity matrix.
196	6	This problem represents a case in which H0 holds.
203	4	The main characteristic of interest in this problem is the local change in the density function.
224	7	Intuitively, one can think of the RKHS norm as taking into account all the locations (v,w).
244	5	We set α = 0.01, and repeat for 300 trials where the full sample is randomly subsampled to n points in each trial.
246	4	To make sure that the type-I error is correct, we use the permutation approach in the NFSIC tests to compute the threshold.
262	11	The problem is sufficiently challenging that all linear-time tests achieve a low power at n = 2000.
263	245	QHSIC performs exceptionally well on this problem, achieving a maximum power throughout.
264	243	NFSIC-opt has the highest sample efficiency among the linear-time tests, showing that the optimization procedure is also practical in a high dimensional setting.
