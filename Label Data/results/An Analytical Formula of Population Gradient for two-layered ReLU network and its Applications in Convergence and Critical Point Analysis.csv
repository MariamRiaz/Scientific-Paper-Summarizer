0,1,label2,summary_sentences
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 603–612 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1056",text,[0],[0]
Entering a new group is rarely easy.,1 Introduction,[0],[0]
"Adjusting to unfamiliar behavioral norms and donning a new identity can be cognitively and emotionally taxing, and failure to do so can lead to exclusion.",1 Introduction,[0],[0]
"But successful enculturation to the group often yields significant rewards, especially in organizational contexts.",1 Introduction,[0],[0]
"Fitting in has been tied to positive career outcomes such as faster time-to-promotion, higher performance ratings, and reduced risk of being fired (O’Reilly et al., 1991; Goldberg et al., 2016).
",1 Introduction,[0],[0]
"A major challenge for enculturation research is distinguishing between internalization and self-
regulation.",1 Introduction,[0],[0]
"Internalization, a more inwardly focused process, involves identifying as a group member and accepting group norms, while selfregulation, a more outwardly oriented process, entails deciphering the group’s normative code and adjusting one’s behavior to comply with it.",1 Introduction,[0],[0]
"Existing approaches, which generally rely on selfreports, are subject to various forms of reporting bias and typically yield only static snapshots of this process.",1 Introduction,[0],[0]
"Recent computational approaches that use language as a behavioral signature of group integration uncover dynamic traces of enculturation but cannot distinguish between internalization and self-regulation.
",1 Introduction,[0],[0]
"To overcome these limitations, we introduce a dynamic measure of directed linguistic accommodation between a newcomer and existing group members.",1 Introduction,[0],[0]
Our approach differentiates between an individual’s (1) base rate of word use and (2) linguistic alignment to interlocutors.,1 Introduction,[0],[0]
"The former corresponds to internalization of the group’s linguistic norms, whereas the latter reflects the capacity to regulate one’s language in response to peers’ language use.",1 Introduction,[0],[0]
"We apply this language model to a corpus of internal email communications and personnel records, spanning a seven-year period, from a mid-sized technology firm.",1 Introduction,[0],[0]
"We show that changes in base rates and alignment, especially with respect to pronoun use, are consistent with successful assimilation into a group and can predict eventual employment outcomes— continued employment, involuntary exit, or voluntary exit—at levels above chance.",1 Introduction,[0],[0]
We use this predictive problem to investigate the nature of linguistic alignment.,1 Introduction,[0],[0]
"Our results suggest that the common formulation of alignment as a lexical-level phenomenon is incomplete.
603",1 Introduction,[0],[0]
Linguistic alignment Linguistic alignment is the tendency to use the same or similar words as one’s conversational partner.,2 Linguistic Alignment and Group Fit,[0],[0]
"Alignment is an instance of a widespread and socially important human behavior: communication accommodation, the tendency of two interacting people to nonconsciously adopt similar behaviors.",2 Linguistic Alignment and Group Fit,[0],[0]
"Evidence of accommodation appears in many behavioral dimensions, including gestures, postures, speech rate, self-disclosure, and language or dialect choice (see Giles et al. (1991) for a review).",2 Linguistic Alignment and Group Fit,[0],[0]
"More accommodating people are rated by their interlocutors as more intelligible, attractive, and cooperative (Feldman, 1968; Ireland et al., 2011; Triandis, 1960).",2 Linguistic Alignment and Group Fit,[0],[0]
"These perceptions have material consequences—for example, high accommodation requests are more likely to be fulfilled, and pairs who accommodate more in how they express uncertainty perform better in lab-based tasks (Buller and Aune, 1988; Fusaroli et al., 2012).
",2 Linguistic Alignment and Group Fit,[0],[0]
"Although accommodation is ubiquitous, individuals vary in their levels of accommodation in ways that are socially informative.",2 Linguistic Alignment and Group Fit,[0],[0]
"Notably, more powerful people are accommodated more strongly in many settings, including trials (Gnisci, 2005), online forums (Danescu-Niculescu-Mizil et al., 2012), and Twitter (Doyle et al., 2016).",2 Linguistic Alignment and Group Fit,[0],[0]
"Most relevant for this work, speakers may increase their accommodation to signal camaraderie or decrease it to differentiate from the group.",2 Linguistic Alignment and Group Fit,[0],[0]
"For example, Bourhis and Giles (1977) found that Welsh English speakers increased their use of the Welsh accent and language in response to an English speaker who dismissed it.
",2 Linguistic Alignment and Group Fit,[0],[0]
Person-group fit and linguistic alignment These findings suggest that linguistic alignment is a useful avenue for studying how people assimilate into a group.,2 Linguistic Alignment and Group Fit,[0],[0]
"Whereas traditional approaches to studying person-group fit rely on self-reports that are subject to various forms of reporting bias and cannot feasibly be collected with high granularity across many points in time, recent studies have proposed language-based measures as a means to tracing the dynamics of person-group fit without having to rely on self-reports.",2 Linguistic Alignment and Group Fit,[0],[0]
"Building on Danescu-Niculescu-Mizil et al. (2013)’s research into language use similarities as a proxy for social distance between individuals, Srivastava et al. (forthcoming) and Goldberg et al. (2016) devel-
oped a measure of cultural fit based on the similarity in linguistic style between individuals and their colleagues in an organization.",2 Linguistic Alignment and Group Fit,[0],[0]
"Their timevarying measure highlights linguistic compatibility as an important facet of cultural fit and reveals distinct trajectories of enculturation for employees with different career outcomes.
",2 Linguistic Alignment and Group Fit,[0],[0]
"While this approach can help uncover the dynamics and consequences of an individual’s fit with her colleagues in an organization, it cannot disentangle the underlying reasons for this alignment.",2 Linguistic Alignment and Group Fit,[0],[0]
"For two primary reasons, it cannot distinguish between fit that arises from internalization and fit produced by self-regulation.",2 Linguistic Alignment and Group Fit,[0],[0]
"First, Goldberg et al. (2016) and Srivastava et al. (forthcoming) define fit using a symmetric measure, the Jensen-Shannon divergence, which does not take into account the direction of alignment.",2 Linguistic Alignment and Group Fit,[0],[0]
Yet the distinction between an individual adapting to peers versus peers adapting to the individual would appear to be consequential.,2 Linguistic Alignment and Group Fit,[0],[0]
"Second, this prior work considers fit across a wide range of linguistic categories but does not interrogate the role of particular categories, such as pronouns, that can be especially informative about enculturation.",2 Linguistic Alignment and Group Fit,[0],[0]
"For example, a person’s base rate use of the first-person singular (I) or plural (we) might indicate the degree of group identity internalization, whereas adjustment to we usage in response to others’ use of the pronoun might reveal the degree of self-regulation to the group’s normative expectations.
",2 Linguistic Alignment and Group Fit,[0],[0]
"Modeling fit with WHAM To address these limitations, we build upon and extend the WHAM alignment framework (Doyle and Frank, 2016) to analyze the dynamics of internalization and selfregulation using the complete corpus of email communications and personnel records from a mid-sized technology company over a seven-year period.",2 Linguistic Alignment and Group Fit,[0],[0]
"WHAM uses a conditional measure of alignment, separating overall homophily (unconditional similarity in people’s language use, driven by internalized similarity) from in-the-moment adaptation (adjusting to another’s usage, corresponding to self-regulation).",2 Linguistic Alignment and Group Fit,[0],[0]
"WHAM also provides a directed measure of alignment, in that it estimates a replier’s adaptation to the other conversational participant separately from the participant’s adaptation to the replier.
Level(s) of alignment The convention within linguistic alignment research, dating back to early
work on Linguistic Style Matching (Niederhoffer and Pennebaker, 2002), is to look at lexical alignment: the repetition of the same or similar words across conversation participants.",2 Linguistic Alignment and Group Fit,[0],[0]
"From a communication accommodation standpoint, this is justified by assuming that one’s choice of words represents a stylistic signal that is partially independent of the meaning one intends to express—similar to the accommodation on paralinguistic signals discussed above.",2 Linguistic Alignment and Group Fit,[0],[0]
"The success of previous linguistic alignment research shows that this is valid.
",2 Linguistic Alignment and Group Fit,[0],[0]
"However, words are difficult to divorce from their meanings, and sometimes repeating a word conflicts with repeating its referent.",2 Linguistic Alignment and Group Fit,[0],[0]
"In particular, pronouns often refer to different people depending on who uses the pronoun.",2 Linguistic Alignment and Group Fit,[0],[0]
"While there is evidence that one person using a first-person singular pronoun increases the likelihood that her conversation partner will as well (Chung and Pennebaker, 2007), we may also expect that one person using first-person singular pronouns may cause the other to use more second-person pronouns, so that both people are referring to the same person.",2 Linguistic Alignment and Group Fit,[0],[0]
"This is especially important under the Interactive Alignment Model view (Pickering and Garrod, 2004), where conversants align their entire mental representations, which predicts both lexical and referential alignment behaviors will be observed.",2 Linguistic Alignment and Group Fit,[0],[0]
"Discourse-strategic explanations for alignment also predict alignment at multiple levels (Doyle and Frank, 2016).
",2 Linguistic Alignment and Group Fit,[0],[0]
"Since we have access to a high-quality corpus with meaningful outcome measures, we can investigate the relative importance of these two types of alignment.",2 Linguistic Alignment and Group Fit,[0],[0]
"We will show that referential alignment is more predictive of employment outcomes than is lexical alignment, suggesting a need for alignment research to consider both levels rather than just the latter.",2 Linguistic Alignment and Group Fit,[0],[0]
"We use the complete corpus of internal emails exchanged among full-time employees at a midsized US-based technology company between 2009 to 2014 (Srivastava et al., forthcoming).",3 Data: Corporate Email Corpus,[0],[0]
Each email was summarized as a count of word categories in its text.,3 Data: Corporate Email Corpus,[0],[0]
"These categories are a subset of the Linguistic Information and Word Count system (Pennebaker et al., 2007).",3 Data: Corporate Email Corpus,[0],[0]
"The categories were chosen because they are likely to be indica-
tive of one’s standing/role within a group.1
We divided email chains into message-reply pairs to investigate conditional alignment between a message and its reply.",3 Data: Corporate Email Corpus,[0],[0]
"To limit these pairs to cases where the reply was likely related to the preceding message, we removed all emails with more than one sender or recipient (including CC/BCC), identical sender and recipient, or where the sender or recipient was an automatic notification system or any other mailbox that was not specific to a single employee.",3 Data: Corporate Email Corpus,[0],[0]
"We also excluded emails with no body text or more than 500 words in the body text, and pairs with more than a week’s latency between message and reply.
",3 Data: Corporate Email Corpus,[0],[0]
"Finally, because our analyses involve enculturation dynamics over the first six months of employment, we excluded replies sent by an employee whose overall tenure was less than six months.",3 Data: Corporate Email Corpus,[0],[0]
"This resulted in a collection of 407,779 messagereply pairs, with 485 distinct replying employees.",3 Data: Corporate Email Corpus,[0],[0]
We combined this with monthly updates of employees joining and leaving the company and whether they left voluntarily or involuntarily.,3 Data: Corporate Email Corpus,[0],[0]
"Of the 485, 66 left voluntarily, 90 left involuntarily, and 329 remained employed at the end of the observation period.
",3 Data: Corporate Email Corpus,[0],[0]
Privacy protections and ethical considerations Research based on employees’ archived electronic communications in organizational settings poses potential threats to employee privacy and company confidentiality.,3 Data: Corporate Email Corpus,[0],[0]
"To address these concerns, and following established ethical guidelines for the conduct of such research (Borgatti and Molina, 2003), we implemented the following procedures: (a) raw data were stored on secure research servers behind the company’s firewall; (b) messages exchanged with individuals outside the firm were eliminated; (c) all identifying information such as email addresses was transformed into hashed identifiers, with the company retaining access to the key code linking identifying information to hashed identifiers; and (d) raw message content was transformed into linguistic categories so that identities could not be inferred from message content.",3 Data: Corporate Email Corpus,[0],[0]
"Per terms of the non-disclosure agreement we signed with the firm, we are not able to share the data underlying the analyses reported below.
",3 Data: Corporate Email Corpus,[0],[0]
"1Six pronoun categories (first singular (I), first plural (we), second (you), third singular personal (he, she), third singular impersonal (it, this), and third plural (they)) and five time/certainty categories (past tense, present tense, future tense, certainty, and tentativity).
",3 Data: Corporate Email Corpus,[0],[0]
"We can, however, share the code and dummy test data, both of which can be accessed at http: //github.com/gabedoyle/acl2017.",3 Data: Corporate Email Corpus,[0],[0]
"To assess alignment, we use the Word-Based Hierarchical Alignment Model (WHAM) framework (Doyle and Frank, 2016).",4 Model: An Extended WHAM Framework,[0],[0]
"The core principle of WHAM is that alignment is a change, usually an increase, in the frequency of using a word category in a reply when the word category was used in the preceding message.",4 Model: An Extended WHAM Framework,[0],[0]
"For instance, a reply to the message What will we discuss at the meeting?, is likely to have more instances of future tense than a reply to the message What did we discuss at the meeting?",4 Model: An Extended WHAM Framework,[0],[0]
"Under this definition, alignment is the log-odds shift from the baseline reply frequency, the frequency of the word in a reply when the preceding message did not contain the word.
",4 Model: An Extended WHAM Framework,[0],[0]
"WHAM is a hierarchical generative modeling framework, so it uses information from related observations (e.g., multiple repliers with similar demographics) to improve its robustness on sparse data (Doyle et al., 2016).",4 Model: An Extended WHAM Framework,[0],[0]
"There are two key parameters, shown in Figure 2: ηbase, the log-odds of a given word category c when the preceding message did not contain c, and ηalign, the increase in the log-odds of c when the preceding message did contain c.
A dynamic extension To understand enculturation, we need to track changes in both the alignment and baseline over time.",4 Model: An Extended WHAM Framework,[0],[0]
"We add a month-bymonth change term to WHAM, yielding a piecewise linear model of these factors over the course of an employee’s tenure.",4 Model: An Extended WHAM Framework,[0],[0]
"Each employee’s tenure is broken into two or three segments: their first six months after being hired, their last six months before leaving (if they leave), and the rest of their tenure.2 The linear segments for their alignment are fit as an intercept term ηalign, based at their first month (for the initial period) or their last month (for the final period), and per-month slopes α.",4 Model: An Extended WHAM Framework,[0],[0]
"Baseline segments are fit similarly, with parameters ηbase and β.3",4 Model: An Extended WHAM Framework,[0],[0]
"To visualize the align-
2Within each segment, the employee’s alignment model is similar to that of Yurovsky et al. (2016), who introduced a constant by-month slope parameter to model changes in parent-child alignment during early linguistic development.
3The six month timeframe was chosen as previous research has found it to be a critical period for early enculturation (Bauer et al., 1998).",4 Model: An Extended WHAM Framework,[0],[0]
"Pilot investigations into the change
ment behaviors and the parameter values, we create “sawhorse” plots, with an example in Figure 1.
",4 Model: An Extended WHAM Framework,[0],[0]
"In our present work, we are focused on changes in cultural fit during the transitions into or out of the group, so we collapse observations outside the first/last six months into a stable point estimate, constraining their slopes to be zero.",4 Model: An Extended WHAM Framework,[0],[0]
"This simplification also circumvents the issue of different employees having different middle-period lengths.4
Model structure The graphical model for our instantiation of WHAM is shown in Figure 2.",4 Model: An Extended WHAM Framework,[0],[0]
"For each word category c, WHAM’s generative model represents each reply as a series of tokenby-token independent draws from a binomial distribution.",4 Model: An Extended WHAM Framework,[0],[0]
"The binomial probability µ is dependent on whether the preceding message did (µalign) or did not (µbase) contain a word from category c, and the inferred alignment value is the difference between these probabilities in log-odds space (ηalign).
",4 Model: An Extended WHAM Framework,[0],[0]
"The specific values of these variables depend on three hierarchical features: the word category c, the group g that a given employee falls into, and the time period t (a piece of the piece-wise
in baseline usage over time showed roughly linear changes over the first/last six months, but our linearity assumption may mask interesting variation in the enculturation trajectories.
",4 Model: An Extended WHAM Framework,[0],[0]
"4As shown in Figure 1, the pieces do not need to define a continuous function.",4 Model: An Extended WHAM Framework,[0],[0]
"Alignment behaviors continue to change in the middle of an employee’s tenure (Srivastava et al., forthcoming), so alignment six months in to the job is unlikely to be equal to alignment six months from leaving, or the average alignment over the middle tenure.
linear function: beginning, middle, or end).",4 Model: An Extended WHAM Framework,[0],[0]
"Note that the hierarchical ordering is different for the η chains and the α/β chains; c is above g and t for the η chains, but below them for the α/β chains.",4 Model: An Extended WHAM Framework,[0],[0]
"This is because we expect the static (η) values for a given word category to be relatively consistent across different groups and at different times, but we expect the values to be independent across the different word categories.",4 Model: An Extended WHAM Framework,[0],[0]
"Conversely, we expect that the enculturation trajectories across word categories (α/β) will be similar, while the trajectories may vary substantially across different groups and different times.",4 Model: An Extended WHAM Framework,[0],[0]
"Lastly, the month m in which a reply is written (measured from the start of the time period t) has a linear effect on the η value, as described below.
",4 Model: An Extended WHAM Framework,[0],[0]
"To estimate alignment, we first divide the replies up by group, time period, and calendar month.",4 Model: An Extended WHAM Framework,[0],[0]
We separate the replies into two sets based on whether the preceding message contained the category c (the “alignment” set) or not (the “baseline” set).,4 Model: An Extended WHAM Framework,[0],[0]
"All replies within a set are then aggregated in a single bag-of-words representation, with category token counts Calignc,g,t,m and C base c,g,t,m, and total token counts N basec,g,t,m and N base c,g,t,m comprising the observed variables on the far right of the model.",4 Model: An Extended WHAM Framework,[0],[0]
"Moving from right to left, these counts are assumed to come from binomial draws with prob-
ability µalignc,g,t,m or µ base c,g,t,m. The µ values are then in turn generated from η values in log-odds space by an inverse-logit transform, similar to linear predictors in logistic regression.
",4 Model: An Extended WHAM Framework,[0],[0]
"The ηbase variables are representations of the baseline frequency of a marker in log-odds space, and µbase is simply a conversion of ηbase to probability space, the equivalent of an intercept term in a logistic regression.",4 Model: An Extended WHAM Framework,[0],[0]
"ηalign is an additive value, with µalign = logit−1(ηbase + ηalign), the equivalent of a binary feature coefficient in a logistic regression.",4 Model: An Extended WHAM Framework,[0],[0]
"The specific month’s η variables are calculated as a linear function: ηalignc,g,t,m = η align c,g,t +",4 Model: An Extended WHAM Framework,[0],[0]
"mαc,g,t, and similarly with β for the baseline.",4 Model: An Extended WHAM Framework,[0],[0]
The remainder of the model is a hierarchy of normal distributions that integrate social structure into the analysis.,4 Model: An Extended WHAM Framework,[0],[0]
"In the present work, we have three levels in the hierarchy: category, group, and time period.",4 Model: An Extended WHAM Framework,[0],[0]
"In Analysis 1, employees are grouped by their employment outcome (stay, leave voluntarily, leave involuntarily); in Analyses 2 & 3, where we predict the employment outcomes, each group is a single employee.",4 Model: An Extended WHAM Framework,[0],[0]
The normal distributions that connect these levels have identical standard deviations σ2 = .25.5,4 Model: An Extended WHAM Framework,[0],[0]
"The hierarchies
5The deviation is not a theoretically motivated choice, and was chosen as a good empirical balance between reasonable parameter convergence (improved by smaller σ2) and good model log-probability (improved by larger σ2).
are headed by a normal distribution centered at 0, except for the ηbase hierarchy, which has a Cauchy(0, 2.5) distribution.6
",4 Model: An Extended WHAM Framework,[0],[0]
Message and reply length can affect alignment estimates; the WHAM model was developed in part to reduce this effect.,4 Model: An Extended WHAM Framework,[0],[0]
"As different employees had different email length distributions, we further accounted for length by dividing all replies into five quintile length bins, and treated each bin as separate observations for each employee.",4 Model: An Extended WHAM Framework,[0],[0]
"This design choice adds an additional control factor, but results were qualitatively similar without it.",4 Model: An Extended WHAM Framework,[0],[0]
"All of our analyses are based on parameter estimates from RStan fits of WHAM with 500 iterations over four chains.
",4 Model: An Extended WHAM Framework,[0],[0]
"While previous research on cultural fit has emphasized either its internalization (O’Reilly et al., 1991) or self-regulation (Goldberg et al., 2016) components, our extension to the WHAM framework helps disentangle them by estimating them as separate baseline and alignment trajectories.",4 Model: An Extended WHAM Framework,[0],[0]
"For example, we can distinguish between an archetypal individual who initially aligns to her colleagues and then internalizes this style of communication such that her baseline use also shifts and another archetypal person who aligns to her colleagues but does not change her baseline usage.",4 Model: An Extended WHAM Framework,[0],[0]
"The former exhibits high correspondence between internalization and self-regulation, whereas the latter demonstrates an ability to decouple them.",4 Model: An Extended WHAM Framework,[0],[0]
We perform three analyses on this data.,5 Analyses,[0],[0]
"First, we examine the qualitative behaviors of pronoun alignment and how they map onto employee outcomes in the data.",5 Analyses,[0],[0]
"Second, we show that these qualitative differences in early enculturation are meaningful, with alignment behaviors predicting employment outcome above chance.",5 Analyses,[0],[0]
"Lastly, we consider lexical versus referential levels of alignment and show that predictions are improved under the referential formulation, suggesting that alignment is not limited to low-level wordrepetition effects.
",5 Analyses,[0],[0]
"6As ηbase is the log-odds of each word in a reply being a part of the category c, it is expected to be substantially negative.",5 Analyses,[0],[0]
"For example, second person pronouns (you), are around 2% of the words in replies, approximately −4 in log-odds space.",5 Analyses,[0],[0]
We follow Gelman et al. (2008)’s recommendation of the Cauchy prior as appropriate for parameter estimation in logistic regression.,5 Analyses,[0],[0]
"We begin with descriptive analyses of the behavior of pronouns, which are likely to reflect incorporation into the company.",5.1 Analysis 1: Dynamic Qualitative Changes,[0],[0]
"In particular, we look at first-person singular (I), first-person plural (we), and second-person pronouns (you).",5.1 Analysis 1: Dynamic Qualitative Changes,[0],[0]
"We expect that increases in we usage will occur as the employee is integrated into the group, while I and you usage will decrease, and want to understand whether these changes manifest on baseline usage (i.e., internalization), alignment (i.e., self-regulation), or both.
",5.1 Analysis 1: Dynamic Qualitative Changes,[0],[0]
"Design We divided each employee’s emails by calendar month, and separated them into the employee’s first six months, their last six months (if an employee left the company within the observation period), and the middle of their tenure.",5.1 Analysis 1: Dynamic Qualitative Changes,[0],[0]
"Employees with fewer than twelve months at the company were excluded from this analysis, so that their first and last months did not overlap.
",5.1 Analysis 1: Dynamic Qualitative Changes,[0],[0]
We fit two WHAM models in this analysis.,5.1 Analysis 1: Dynamic Qualitative Changes,[0],[0]
"The first aggregated all employees, regardless of employment outcome, to minimize noise; the second separated them by outcome to analyze cultural fit differences.
",5.1 Analysis 1: Dynamic Qualitative Changes,[0],[0]
"Outcome-aggregated model We start with the aggregated behavior of all employees, shown in Figure 3.",5.1 Analysis 1: Dynamic Qualitative Changes,[0],[0]
"For baselines, we see decreased use of I
and you over the first six months, with we usage increasing over the same period, confirming the expected result that incorporating into the group is accompanied by more inclusive pronoun usage.",5.1 Analysis 1: Dynamic Qualitative Changes,[0],[0]
"Despite the baseline changes, alignment is fairly stable through the first six months.",5.1 Analysis 1: Dynamic Qualitative Changes,[0],[0]
"Alignment on first-person singular and second-person pronouns is lower than first-person plural pronouns, likely due to the fact that I or you have different referents when used by the two conversants, while both conversants could use we to refer to the same group.",5.1 Analysis 1: Dynamic Qualitative Changes,[0],[0]
We will consider this referential alignment in more detail in Analysis 3.,5.1 Analysis 1: Dynamic Qualitative Changes,[0],[0]
"Since employees with different outcomes have much different experiences over their last six months, we will not discuss them in aggregate, aside from noting the sharp decline in we alignment near the end of the employees’ tenures.
",5.1 Analysis 1: Dynamic Qualitative Changes,[0],[0]
"Outcome-separated model Figure 4 shows outcome-specific trajectories, with green lines showing involuntary leavers (i.e., those who are fired or downsized), blue showing voluntary leavers, and orange showing employees who remained at the company through the final month of the data.",5.1 Analysis 1: Dynamic Qualitative Changes,[0],[0]
"The use of I and you is similar to the aggregates in Figure 3, regardless of group.",5.1 Analysis 1: Dynamic Qualitative Changes,[0],[0]
"The last six months of I usage show an interesting difference, where involuntary leavers align more on I but retain a stable baseline while voluntary leavers retain a stable alignment but increase I overall, which is consistent with group separation.
",5.1 Analysis 1: Dynamic Qualitative Changes,[0],[0]
"The most compelling result we see here, though, is the changes in we usage by different groups of employees.",5.1 Analysis 1: Dynamic Qualitative Changes,[0],[0]
"Employees who eventually leave the
company involuntarily show signs of more selfregulation than internalization over the first six months, increasing their alignment while decreasing their baseline use (though they return to more similar levels as other employees later in their tenure).",5.1 Analysis 1: Dynamic Qualitative Changes,[0],[0]
"Employees who stay at the company, as well as those who later leave voluntarily, show signs of internalization, increasing their baseline usage to the company average, as well as adapting their alignment levels to the mean.",5.1 Analysis 1: Dynamic Qualitative Changes,[0],[0]
"This finding suggests that how quickly the employees internalize culturally-standard language use predicts their eventual employment outcome, even if they eventually end up near the average.",5.1 Analysis 1: Dynamic Qualitative Changes,[0],[0]
"This analysis tests the hypothesis that there are meaningful differences in employees’ initial enculturation, captured by alignment behaviors.",5.2 Analysis 2: Predicting Outcomes,[0],[0]
We examine the first six months of communications and attempt to predict whether the employee will leave the company.,5.2 Analysis 2: Predicting Outcomes,[0],[0]
"We find that, even with a simple classifier, alignment behaviors are predictive of employment outcome.
",5.2 Analysis 2: Predicting Outcomes,[0],[0]
Design We fit the WHAM model to only the first six months of email correspondence for all employees who had at least six months of email.,5.2 Analysis 2: Predicting Outcomes,[0],[0]
"The model estimated the initial level of baseline use (ηbase) and alignment (ηalign) for each employee, as well as the slope (α, β) for baseline and alignment over those first six months, over all 11 word categories mentioned in Section 3.
",5.2 Analysis 2: Predicting Outcomes,[0],[0]
"We then created logistic regression classifiers, using the parameter estimates to predict whether an employee would leave the company.",5.2 Analysis 2: Predicting Outcomes,[0],[0]
We fit separate classifiers for leaving voluntarily or involuntarily.,5.2 Analysis 2: Predicting Outcomes,[0],[0]
"Our results show that early alignment behaviors are better at identifying employees who will leave involuntarily than voluntarily, consistent with Srivastava et al.’s (forthcoming) findings that voluntary leavers are similar to stayers until late in their tenure.",5.2 Analysis 2: Predicting Outcomes,[0],[0]
"We fit separate classifiers using the alignment parameters and the baseline parameters to investigate their relative informativity.
",5.2 Analysis 2: Predicting Outcomes,[0],[0]
"For each model, we report the area under the curve (AUC).",5.2 Analysis 2: Predicting Outcomes,[0],[0]
"This value is estimated from the receiver operating characteristic (ROC) curve, which plots the true positive rate against the false positive rate over different classification thresholds.",5.2 Analysis 2: Predicting Outcomes,[0],[0]
An AUC of 0.5 represents chance performance.,5.2 Analysis 2: Predicting Outcomes,[0],[0]
"We use balanced, stratified cross-
validation to reduce AUC misestimation due to unbalanced outcome frequencies and high noise (Parker et al., 2007).
",5.2 Analysis 2: Predicting Outcomes,[0],[0]
"Results The left column of Figure 5 shows the results over 10 runs of 10-fold balanced logistic classifiers with stratified cross-validation in R. The alignment-based classifiers are both above chance at predicting that an employee will leave the company, whether involuntarily or voluntarily.",5.2 Analysis 2: Predicting Outcomes,[0],[0]
"The baseline-based classifiers perform worse, especially on voluntary leavers.",5.2 Analysis 2: Predicting Outcomes,[0],[0]
"This finding is consistent with the idea that voluntary leavers resemble stayers (who form the bulk of the employees) until late in their tenure when their cultural fit declines.
",5.2 Analysis 2: Predicting Outcomes,[0],[0]
"We fit a model using both alignment and baseline parameters, but this model yielded an AUC value below the alignment-only classifier.",5.2 Analysis 2: Predicting Outcomes,[0],[0]
"This suggests that where alignment and baseline behaviors are both predictive, they do not provide substantially different predictive power and lead to overfitting.",5.2 Analysis 2: Predicting Outcomes,[0],[0]
A more sophisticated classifier may overcome these challenges; our goal here was not to achieve maximal classification performance but to test whether alignment provided any useful information about employment outcomes.,5.2 Analysis 2: Predicting Outcomes,[0],[0]
"Our final analysis investigates the nature of linguistic alignment: specifically, whether there is an effect of referential alignment beyond that of the more commonly used lexical alignment.
",5.3 Analysis 3: Types of Alignment,[0],[0]
Testing this hypothesis requires a small change to the alignment calculations.,5.3 Analysis 3: Types of Alignment,[0],[0]
"Lexical alignment is based on the conditional probability of the replier using a word category c given that the preceding message used that same category c. For referential alignment, we examine the conditional probability of the replier using a word category cj given that the preceding message used the category ci, where ci and cj are likely to be referentially linked.",5.3 Analysis 3: Types of Alignment,[0],[0]
"We also consider cases where ci is likely to transition to cj throughout the course of the conversation, such as present tense verbs turning into past tense as the event being described recedes into the past.",5.3 Analysis 3: Types of Alignment,[0],[0]
"The pairs of categories that are likely to be referentially or transitionally linked are: (you, I); (we, I); (you, we); (past, present); (present, future); and (certainty, tentativity).",5.3 Analysis 3: Types of Alignment,[0],[0]
"We include both directions of these pairs, so this provides approximately the same number of predictor variables for both situa-
tions to maximize comparability (12 for the referential alignments, 11 for the lexical).",5.3 Analysis 3: Types of Alignment,[0],[0]
"This modification does not change the structure of the WHAM model, but rather changes its C and N counts by reclassifying replies between the baseline or alignment pathways.
",5.3 Analysis 3: Types of Alignment,[0],[0]
Results Figure 5 plots the differences in predictive model performance using lexical versus referential alignment parameters.,5.3 Analysis 3: Types of Alignment,[0],[0]
We find that the semantic parameters provide more accurate classification than the lexical both for voluntarily and involuntarily-leaving employees.,5.3 Analysis 3: Types of Alignment,[0],[0]
"This suggests that while previous work looking at lexical alignment successfully captures social structure, referential alignment may reflect a deeper and more accurate representation of the social structure.",5.3 Analysis 3: Types of Alignment,[0],[0]
"It is unclear if this behavior holds in less formal situations or with weaker organizational structure and shared goals, but these results suggest that the traditional alignment approach of only measuring lexical alignment should be augmented with referential alignment measures for a more complete analysis.",5.3 Analysis 3: Types of Alignment,[0],[0]
"A key finding from this work is that pronoun usage behaviors in employees’ email communication are consistent with social integration into the group; employees use “I” pronouns less and
“we” pronouns more as they integrate.",6 Discussion,[0],[0]
"Furthermore, we see the importance of using an alignment measure such as WHAM for distinguishing the base rate and alignment usage of words.",6 Discussion,[0],[0]
"Employees who leave the company involuntarily show increased “we” usage through greater alignment, using “we” more when prompted by a colleague, but introducing it less of their own accord.",6 Discussion,[0],[0]
"This suggests that these employees do not feel fully integrated into the group, although they are willing to identify as a part of it when a more fully-integrated group member includes them, corresponding to self-regularization over internalization.",6 Discussion,[0],[0]
"The fact that these alignment measures alone, without any job productivity or performance metrics, have some predictive capability for employees’ leaving the company suggests the potential for support or intervention programs to help highperforming but poorly-integrated employees integrate into the company better.
",6 Discussion,[0],[0]
"More generally, the prominence of pronominally-driven communication changes suggest that alignment analyses can provide insight into a range of social integration settings.",6 Discussion,[0],[0]
"This may be especially helpful in cases where there is great pressure to integrate smoothly, and people would be likely to adopt a self-regulating approach even if they do not internalize their group membership.",6 Discussion,[0],[0]
"Such settings not only include the high-stakes situation of keeping one’s job, but of transitioning from high school to college or moving to a new country or region.",6 Discussion,[0],[0]
Maximizing the chances for new members to become comfortable within a group is critical both for spreading useful aspects of the group’s existing culture to new members and for integrating new ideas from the new members’ knowledge and practices.,6 Discussion,[0],[0]
Alignment-based approaches can be a useful tool in separating effective interventions that cause internalization of the group dynamics from those that lead to more superficial self-regularization changes.,6 Discussion,[0],[0]
This paper described an effort to use directed linguistic alignment as a measure of cultural fit within an organization.,7 Conclusions,[0],[0]
"We adapted a hierarchical alignment model from previous work to estimate fit within corporate email communications, focusing on changes in language during employees’ entry to and exit from the company.",7 Conclusions,[0],[0]
"Our results
showed substantial changes in the use of pronouns, with pronoun patterns varying by employees’ outcomes within the company.",7 Conclusions,[0],[0]
The use of the firstperson plural “we” during an employee’s first six months is particularly instructive.,7 Conclusions,[0],[0]
"Whereas stayers exhibited increased baseline use, indicating internalization, those eventually departing involuntarily were on the one hand decreasingly likely to introduce “we” into conversation, but increasingly responsive to interlocutors’ use of the pronoun.",7 Conclusions,[0],[0]
"While not internalizing a shared identity with their peers, involuntarily departed employees were overly self-regulating in response to its invocation by others.
",7 Conclusions,[0],[0]
"Quantitatively, rates of usage and alignment in the first six months of employment carried information about whether employees left involuntarily, pointing towards fit within the company culture early on as an indicator of eventual employment outcomes.",7 Conclusions,[0],[0]
"Finally, we saw ways in which the application of alignment to cultural fit might help to refine ideas about alignment itself: preliminary analysis suggested that referential, rather than lexical, alignment was more predictive of employment outcomes.",7 Conclusions,[0],[0]
"More broadly, these results suggest ways that quantitative methods can be used to make precise application of concepts like “cultural fit” at scale.",7 Conclusions,[0],[0]
"This work was supported by NSF Grant #1456077; The Garwood Center for Corporate Innovation at the Haas School of Business, University of California, Berkeley; the Stanford Data Science Initiative; and the Stanford Graduate School of Business.",8 Acknowledgments,[0],[0]
Cultural fit is widely believed to affect the success of individuals and the groups to which they belong.,abstractText,[0],[0]
"Yet it remains an elusive, poorly measured construct.",abstractText,[0],[0]
Recent research draws on computational linguistics to measure cultural fit but overlooks asymmetries in cultural adaptation.,abstractText,[0],[0]
"By contrast, we develop a directed, dynamic measure of cultural fit based on linguistic alignment, which estimates the influence of one person’s word use on another’s and distinguishes between two enculturation mechanisms: internalization and selfregulation.",abstractText,[0],[0]
"We use this measure to trace employees’ enculturation trajectories over a large, multi-year corpus of corporate emails and find that patterns of alignment in the first six months of employment are predictive of individuals downstream outcomes, especially involuntary exit.",abstractText,[0],[0]
Further predictive analyses suggest referential alignment plays an overlooked role in linguistic alignment.,abstractText,[0],[0]
Alignment at Work: Using Language to Distinguish the Internalization and Self-Regulation Components of Cultural Fit in Organizations,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1165–1174, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
"In instruction-following tasks, an agent executes a sequence of actions in a real or simulated environment, in response to a sequence of natural language commands.",1 Introduction,[0],[0]
Examples include giving navigational directions to robots and providing hints to automated game-playing agents.,1 Introduction,[0],[0]
Plans specified with natural language exhibit compositionality both at the level of individual actions and at the overall sequence level.,1 Introduction,[0],[0]
"This paper describes a framework for learning to follow instructions by leveraging structure at both levels.
",1 Introduction,[0],[0]
"Our primary contribution is a new, alignmentbased approach to grounded compositional semantics.",1 Introduction,[0],[0]
"Building on related logical approaches (Reddy et al., 2014; Pourdamghani et al., 2014), we recast instruction following as a pair of nested, structured alignment problems.",1 Introduction,[0],[0]
"Given instructions and a candidate plan, the model infers a sequenceto-sequence alignment between sentences and
atomic actions.",1 Introduction,[0],[0]
"Within each sentence–action pair, the model infers a structure-to-structure alignment between the syntax of the sentence and a graphbased representation of the action.
",1 Introduction,[0],[0]
"At a high level, our agent is a block-structured, graph-valued conditional random field, with alignment potentials to relate instructions to actions and transition potentials to encode the environment model (Figure 3).",1 Introduction,[0],[0]
"Explicitly modeling sequenceto-sequence alignments between text and actions allows flexible reasoning about action sequences, enabling the agent to determine which actions are specified (perhaps redundantly) by text, and which actions must be performed automatically (in order to satisfy pragmatic constraints on interpretation).",1 Introduction,[0],[0]
"Treating instruction following as a sequence prediction problem, rather than a series of independent decisions (Branavan et al., 2009; Artzi and Zettlemoyer, 2013), makes it possible to use general-purpose planning machinery, greatly increasing inferential power.
",1 Introduction,[0],[0]
"The fragment of semantics necessary to complete most instruction-following tasks is essentially predicate–argument structure, with limited influence from quantification and scoping.",1 Introduction,[0],[0]
Thus the problem of sentence interpretation can reasonably be modeled as one of finding an alignment between language and the environment it describes.,1 Introduction,[0],[0]
We allow this structure-to-structure alignment— an “overlay” of language onto the world—to be mediated by linguistic structure (in the form of dependency parses) and structured perception (in what we term grounding graphs).,1 Introduction,[0],[0]
"Our model thereby reasons directly about the relationship between language and observations of the environment, without the need for an intermediate logical representation of sentence meaning.",1 Introduction,[0],[0]
"This, in turn, makes it possible to incorporate flexible feature representations that have been difficult to integrate with previous work in semantic parsing.
",1 Introduction,[0],[0]
"We apply our approach to three established
1165
2 1 3
instruction-following benchmarks: the map reading task of Vogel and Jurafsky (2010), the maze navigation task of MacMahon et al. (2006), and the puzzle solving task of Branavan et al. (2009).",1 Introduction,[0],[0]
An example from each is shown in Figure 1.,1 Introduction,[0],[0]
"These benchmarks exhibit a range of qualitative properties—both in the length and complexity of their plans, and in the quantity and quality of accompanying language.",1 Introduction,[0],[0]
"Each task has been studied in isolation, but we are unaware of any published approaches capable of robustly handling all three.",1 Introduction,[0],[0]
"Our general model outperforms strong, task-specific baselines in each case, achieving relative error reductions of 15–20% over several state-of-the-art results.",1 Introduction,[0],[0]
Experiments demonstrate the importance of our contributions in both compositional semantics and search over plans.,1 Introduction,[0],[0]
We have released all code for this project at github.com/jacobandreas/instructions.,1 Introduction,[0],[0]
"Existing work on instruction following can be roughly divided into two families: semantic parsers and linear policy estimators.
",2 Related work,[0],[0]
"Semantic parsers Parser-based approaches (Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013; Kim and Mooney, 2013) map from text into a formal language representing commands.",2 Related work,[0],[0]
"These take familiar structured prediction models for semantic parsing (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006), and train them with task-provided supervision.",2 Related work,[0],[0]
"Instead of attempting to match the structure of a manually-annotated semantic parse, semantic parsers for instruction following are trained to maximize a reward signal
provided by black-box execution of the predicted command in the environment.",2 Related work,[0],[0]
"(It is possible to think of response-based learning for question answering (Liang et al., 2013) as a special case.)
",2 Related work,[0],[0]
"This approach uses a well-studied mechanism for compositional interpretation of language, but is subject to certain limitations.",2 Related work,[0],[0]
"Because the environment is manipulated only through black-box execution of the completed semantic parse, there is no way to incorporate current or future environment state into the scoring function.",2 Related work,[0],[0]
It is also in general necessary to hand-engineer a task-specific formal language for describing agent behavior.,2 Related work,[0],[0]
"Thus it is extremely difficult to work with environments that cannot be modeled with a fixed inventory of predicates (e.g. those involving novel strings or arbitrary real quantities).
",2 Related work,[0],[0]
Much of contemporary work in this family is evaluated on the maze navigation task introduced by MacMahon et al. (2006).,2 Related work,[0],[0]
"Dukes (2013) also introduced a “blocks world” task for situated parsing of spatial robot commands.
",2 Related work,[0],[0]
"Linear policy estimators An alternative family of approaches is based on learning a policy over primitive actions directly (Branavan et al., 2009; Vogel and Jurafsky, 2010).1 Policybased approaches instantiate a Markov decision process representing the action domain, and apply standard supervised or reinforcement-learning approaches to learn a function for greedily selecting among actions.",2 Related work,[0],[0]
"In linear policy approximators, natural language instructions are incorporated directly into state observations, and reading order
1This is distinct from semantic parsers in which greedy inference happens to have an interpretation as a policy (Vlachos and Clark, 2014).
becomes part of the action selection process.",2 Related work,[0],[0]
"Almost all existing policy-learning approaches make use of an unstructured parameterization, with a single (flat) feature vector representing all text and observations.",2 Related work,[0],[0]
Such approaches are thus restricted to problems that are simple enough (and have small enough action spaces) to be effectively characterized in this fashion.,2 Related work,[0],[0]
"While there is a great deal of flexibility in the choice of feature function (which is free to inspect the current and future state of the environment, the whole instruction sequence, etc.), standard linear policy estimators have no way to model compositionality in language or actions.
",2 Related work,[0],[0]
"Agents in this family have been evaluated on a variety of tasks, including map reading (Anderson et al., 1991) and gameplay (Branavan et al., 2009).
",2 Related work,[0],[0]
"Though both families address the same class of instruction-following problems, they have been applied to a totally disjoint set of tasks.",2 Related work,[0],[0]
"It should be emphasized that there is nothing inherent to policy learning that prevents the use of compositional structure, and nothing inherent to general compositional models that prevents more complicated dependence on environment state.",2 Related work,[0],[0]
"Indeed, previous work (Branavan et al., 2011; Narasimhan et al., 2015) uses aspects of both to solve a different class of gameplay problems.",2 Related work,[0],[0]
"In some sense, our goal in this paper is simply to combine the strengths of semantic parsers and linear policy estimators for fully general instruction following.",2 Related work,[0],[0]
"As we shall see, however, this requires changes to many aspects of representation, learning and inference.",2 Related work,[0],[0]
We wish to train a model capable of following commands in a simulated environment.,3 Representations,[0],[0]
"We do so by presenting the model with a sequence of training pairs (x,y), where each x is a sequence of natural language instructions (x1, x2, . . .",3 Representations,[0],[0]
", xm), e.g.:
(Go down the yellow hall., Turn left., . . . )
",3 Representations,[0],[0]
"and each y is a demonstrated action sequence (y1, y2, . . .",3 Representations,[0],[0]
", yn), e.g.:
(rotate(90), move(2), . . . )
",3 Representations,[0],[0]
"Given a start state, y can equivalently be characterized by a sequence of (state, action, state)
triples resulting from execution of the environment model.",3 Representations,[0],[0]
An example instruction is shown in Figure 2a.,3 Representations,[0],[0]
"An example action, situated in the environment where it occurs, is shown in Figure 2e.
",3 Representations,[0],[0]
Our model performs compositional interpretation of instructions by leveraging existing structure inherent in both text and actions.,3 Representations,[0],[0]
"Thus we interpret xi and yj not as raw strings and primitive actions, but rather as structured objects.
",3 Representations,[0],[0]
"Linguistic structure We assume access to a pretrained parser, and in particular that each of the instructions xi is represented by a tree-structured dependency parse.",3 Representations,[0],[0]
"An example is shown in Figure 2b.
",3 Representations,[0],[0]
"Action structure By analogy to the representation of instructions as parse trees, we assume that each (state, action, state) triple (provided by the environment model) can be characterized by a grounding graph.",3 Representations,[0],[0]
The structure and content of this representation is task-specific.,3 Representations,[0],[0]
"An example grounding graph for the maze navigation task is
shown in Figure 2d.",3 Representations,[0],[0]
"The example contains a node corresponding to the primitive action move(2) (in the upper left), and several nodes corresponding to locations in the environment that are visible after the action is performed.
",3 Representations,[0],[0]
"Each node in the graph (and, though not depicted, each edge) is decorated with a list of features.",3 Representations,[0],[0]
"These features might be simple indicators (e.g. whether the primitive action performed was move or rotate), real values (the distance traveled) or even string-valued (English-language names of visible landmarks, if available in the environment description).",3 Representations,[0],[0]
"Formally, a grounding graph consists of a tuple (V,E,L, fV , fE), with
– V a set of vertices
– E ∈ V × V a set of (directed) edges – L a space of labels (numbers, strings, etc.) – fV : V → 2L a vertex feature function – fE : E → 2L an edge feature function In this paper we have tried to remain agnostic to details of graph construction.",3 Representations,[0],[0]
Our goal with the grounding graph framework is simply to accommodate a wider range of modeling decisions than allowed by existing formalisms.,3 Representations,[0],[0]
"Graphs might be constructed directly, given access to a structured virtual environment (as in all experiments in this paper), or alternatively from outputs of a perceptual system.",3 Representations,[0],[0]
"For our experiments, we have remained as close as possible to task representations described in the existing literature.",3 Representations,[0],[0]
"Details for each task can be found in the accompanying software package.
",3 Representations,[0],[0]
"Graph-based representations are extremely common in formal semantics (Jones et al., 2012; Reddy et al., 2014), and the version presented here corresponds to a simple generalization of familiar formal methods.",3 Representations,[0],[0]
"Indeed, if L is the set of all atomic entities and relations, fV returns a unique label for every v ∈ V , and fE always returns a vector with one active feature, we recover the existentially-quantified portion of first order logic exactly, and in this form can implement large parts of classical neo-Davidsonian semantics (Parsons, 1990) using grounding graphs.
",3 Representations,[0],[0]
"Crucially, with an appropriate choice of L this formalism also makes it possible to go beyond settheoretic relations, and incorporate string-valued features (like names of entities and landmarks) and real-valued features (like colors and positions) as well.
",3 Representations,[0],[0]
Lexical semantics We must eventually combine features provided by parse trees with features provided by the environment.,3 Representations,[0],[0]
"Examples here might include simple conjunctions (word=yellow ∧ rgb=(0.5, 0.5, 0.0)) or more complicated computations like edit distance between landmark names and lexical items.",3 Representations,[0],[0]
"Features of the latter kind make it possible to behave correctly in environments containing novel strings or other features unseen during training.
",3 Representations,[0],[0]
"This aspect of the syntax–semantics interface has been troublesome for some logic-based approaches: while past work has used related machinery for selecting lexicon entries (Berant and Liang, 2014) or for rewriting logical forms (Kwiatkowski et al., 2013), the relationship between text and the environment has ultimately been mediated by a discrete (and indeed finite) inventory of predicates.",3 Representations,[0],[0]
"Several recent papers have investigated simple grounded models with realvalued output spaces (Andreas and Klein, 2014; McMahan and Stone, 2015), but we are unaware of any fully compositional system in recent literature that can incorporate observations of these kinds.
",3 Representations,[0],[0]
"Formally, we assume access to a joining feature function φ : (2L × 2L)→ Rd.",3 Representations,[0],[0]
"As with grounding graphs, our goal is to make the general framework as flexible as possible, and for individual experiments have chosen φ to emulate modeling decisions from previous work.",3 Representations,[0],[0]
"As noted in the introduction, we approach instruction following as a sequence prediction problem.",4 Model,[0],[0]
Thus we must place a distribution over sequences of actions conditioned on instructions.,4 Model,[0],[0]
"We decompose the problem into two components, describing interlocking models of “path structure” and “action structure”.",4 Model,[0],[0]
"Path structure captures how sequences of instructions give rise to sequences of actions, while action structure captures the compositional relationship between individual utterances and the actions they specify.
",4 Model,[0],[0]
"Path structure: aligning utterances to actions
The high-level path structure in the model is depicted in Figure 3.",4 Model,[0],[0]
"Our goal here is to permit both under- and over-specification of plans, and to expose a planning framework which allows plans to be computed with lookahead (i.e. non-greedily).
",4 Model,[0],[0]
These goals are achieved by introducing a sequence of latent alignments between instructions and actions.,4 Model,[0],[0]
Consider the multi-step example in Figure 1b.,4 Model,[0],[0]
"If the first instruction go down the yellow hall were interpreted immediately, we would have a presupposition failure—the agent is facing a wall, and cannot move forward at all.",4 Model,[0],[0]
"Thus an implicit rotate action, unspecified by text, must be performed before any explicit instructions can be followed.
",4 Model,[0],[0]
"To model this, we take the probability of a (text, plan, alignment) triple to be log-proportional to the sum of two quantities:
1.",4 Model,[0],[0]
"a path-only score ψ(n; θ) + ∑
j ψ(yj ; θ)
2.",4 Model,[0],[0]
"a path-and-text score, itself the sum of all pair scores ψ(xi, yj ; θ) licensed by the alignment
(1) captures our desire for pragmatic constraints on interpretation, and provides a means of encoding the inherent plausibility of paths.",4 Model,[0],[0]
"We take ψ(n; θ) and ψ(y; θ) to be linear functions of θ. (2) provides context-dependent interpretation of text by means of the structured scoring function ψ(x, y; θ), described in the next section.
",4 Model,[0],[0]
"Formally, we associate with each instruction xi a sequence-to-sequence alignment variable ai ∈ 1 . . .",4 Model,[0],[0]
n,4 Model,[0],[0]
"(recalling that n is the number of actions).
",4 Model,[0],[0]
"Then we have2 p(y,a|x; θ) ∝ exp { ψ(n) + n∑ j=1 ψ(yj)
+",4 Model,[0],[0]
"m∑ i=1 n∑ j=1 1[aj = i] ψ(xi, yj) } (1)
We additionally place a monotonicity constraint on the alignment variables.",4 Model,[0],[0]
"This model is globally normalized, and for a fixed alignment is equivalent to a linear-chain CRF.",4 Model,[0],[0]
"In this sense it is analogous to IBM Model I (Brown et al., 1993), with the structured potentials ψ(xi, yj) taking the place of lexical translation probabilities.",4 Model,[0],[0]
"While alignment models from machine translation have previously been used to align words to fragments of semantic parses (Wong and Mooney, 2006; Pourdamghani et al., 2014), we are unaware of such models being used to align entire instruction sequences to demonstrations.
",4 Model,[0],[0]
"Action structure: aligning words to percepts Intuitively, this scoring function ψ(x, y) should capture how well a given utterance describes an action.",4 Model,[0],[0]
"If neither the utterances nor the actions had structure (i.e. both could be represented with simple bags of features), we would recover something analogous to the conventional policy-learning approach.",4 Model,[0],[0]
"As structure is essential for some of our tasks, ψ(x, y) must instead fill the role of a semantic parser in a conventional compositional model.
",4 Model,[0],[0]
"Our choice of ψ(x, y) is driven by the following fundamental assumptions: Syntactic relations approximately represent semantic relations.",4 Model,[0],[0]
Syntactic proximity implies relational proximity.,4 Model,[0],[0]
"In this view, there is an additional hidden structure-tostructure alignment between the grounding graph and the parsed text describing it.",4 Model,[0],[0]
"3 Words line up with nodes, and dependencies line up with relations.",4 Model,[0],[0]
"Visualizations are shown in Figure 2c and the zoomed-in portion of Figure 3.
",4 Model,[0],[0]
"As with the top-level alignment variables, this approach can viewed as a simple relaxation of a familiar model.",4 Model,[0],[0]
"CCG-based parsers assume that syntactic type strictly determines semantic type,
2Here and the remainder of this paper, we suppress the dependence of the various potentials on θ in the interest of readability.
",4 Model,[0],[0]
3It is formally possible to regard the sequence-tosequence and structure-to-structure alignments as a single (structured) random variable.,4 Model,[0],[0]
"However, the two kinds of alignments are treated differently for purposes of inference, so it is useful to maintain a notational distinction.
and that each lexical item is associated with a small set of functional forms.",4 Model,[0],[0]
"Here we simply allow all words to license all predicates, multiple words to specify the same predicate, and some edges to be skipped.",4 Model,[0],[0]
We instead rely on a scoring function to impose soft versions of the hard constraints typically provided by a grammar.,4 Model,[0],[0]
"Related models have previously been used for question answering (Reddy et al., 2014; Pasupat and Liang, 2015).
",4 Model,[0],[0]
For the moment let us introduce variables b to denote these structure-to-structure alignments.,4 Model,[0],[0]
"(As will be seen in the following section, it is straightforward to marginalize over all choices of b.",4 Model,[0],[0]
"Thus the structure-to-structure alignments are never explicitly instantiated during inference, and do not appear in the final form of ψ(x, y).)",4 Model,[0],[0]
"For a fixed alignment, we define ψ(x, y, b) according to a recurrence relation.",4 Model,[0],[0]
"Let xi be the ith word of the sentence, and let yj be the jth node in the action graph (under some topological ordering).",4 Model,[0],[0]
Let c(i) and c(j) give the indices of the dependents of xi and children of yj respectively.,4 Model,[0],[0]
"Finally, let xik and yjl denote the associated dependency type or relation.",4 Model,[0],[0]
"Define a “descendant” function:
d(i, j) = { (k, l) : k ∈ c(i), l ∈ c(j), (k, l) ∈ b}
Then, ψ(xi, yj , b) = exp { θ>φ(xi, yj)
+ ∑
(k,l)∈d(x,y)
",4 Model,[0],[0]
"[ θ>φ ( xik, yjl ) · ψ(xk, yl, b)]}
This is just an unnormalized synchronous derivation between x and y—at any aligned (node, word) pair, the score for the entire derivation is the score produced by combining that word and node, times the scores at all the aligned descendants.",4 Model,[0],[0]
"Observe that as long as there are no cycles in the dependency parse, it is perfectly acceptable for the relation graph to contain cycles and even self-loops— the recurrence still bottoms out appropriately.",4 Model,[0],[0]
"Given a sequence of training pairs (x,y), we wish to find a parameter setting that maximizes p(y|x; θ).",5 Learning and inference,[0],[0]
"If there were no latent alignments a or b, this would simply involve minimization of a convex objective.",5 Learning and inference,[0],[0]
The presence of latent variables complicates things.,5 Learning and inference,[0],[0]
"Ideally, we would like
Algorithm 1 Computing structure-to-structure alignments
xi are words in reverse topological order yj are grounding graph nodes (root last) chart is an m× n array for i = 1 to |x| do
for j = 1 to |y| do score← exp{θ>φ(xi, yj)} for (k, l) ∈ d(i, j) do
s←∑l∈c(j) [ exp{θ>φ(xik, yjl)} · chart[k, l]
]",5 Learning and inference,[0],[0]
"score← score · s
end for chart[i, j]← score
end for end for return chart[n,m]
to sum over the latent variables, but that sum is intractable.",5 Learning and inference,[0],[0]
"Instead we make a series of variational approximations: first we replace the sum with a maximization, then perform iterated conditional modes, alternating between maximization of the conditional probability of a and θ.",5 Learning and inference,[0],[0]
"We begin by initializing θ randomly.
",5 Learning and inference,[0],[0]
"As noted in the preceding section, the variable b does not appear in these equations.",5 Learning and inference,[0],[0]
"Conditioned on a, the sum over structure-to-structure ψ(x, y) = ∑ b ψ(x, y, b) can be performed exactly using a simple dynamic program which runs in time O(|x||y|)",5 Learning and inference,[0],[0]
"(assuming out-degree bounded by a constant, and with |x| and |y| the number of words and graph nodes respectively).",5 Learning and inference,[0],[0]
"This is Algorithm 1.
",5 Learning and inference,[0],[0]
"In our experiments, θ is optimized using LBFGS (Liu and Nocedal, 1989).",5 Learning and inference,[0],[0]
"Calculation of the gradient with respect to θ requires computation of a normalizing constant involving the sum over p(x,y′,a) for all y′.",5 Learning and inference,[0],[0]
"While in principle the normalizing constant can be computed using the forward algorithm, in practice the state spaces under consideration are so large that even this is intractable.",5 Learning and inference,[0],[0]
"Thus we make an additional approximation, constructing a set Ỹ of alternative actions and taking p(y,a|x)",5 Learning and inference,[0],[0]
≈ n∑ j=1 exp { ψ(yj)+ ∑m i=1,5 Learning and inference,[0],[0]
"1[ai=j]ψ(xi,yi) } ∑ ỹ∈Ỹ exp { ψ(ỹ)+ ∑m i=1",5 Learning and inference,[0],[0]
"1[ai=j]ψ(xi,ỹ) }
Ỹ is constructed by sampling alternative actions from the environment model.",5 Learning and inference,[0],[0]
"Meanwhile, maximization of a can be performed exactly using the Viterbi algorithm, without computation of normalizers.
",5 Learning and inference,[0],[0]
Inference at test time involves a slightly different pair of optimization problems.,5 Learning and inference,[0],[0]
"We again perform iterated conditional modes, here on the alignments a and the unknown output path y. Maximization of a is accomplished with the Viterbi algorithm, exactly as before; maximization of y also uses the Viterbi algorithm, or a beam search when this is computationally infeasible.",5 Learning and inference,[0],[0]
"If bounds on path length are known, it is straightforward to adapt these dynamic programs to efficiently consider paths of all lengths.",5 Learning and inference,[0],[0]
"As one of the main advantages of this approach is its generality, we evaluate on several different benchmark tasks for instruction following.",6 Evaluation,[0],[0]
These exhibit great diversity in both environment structure and language use.,6 Evaluation,[0],[0]
We compare our full system to recent state-of-the-art approaches to each task.,6 Evaluation,[0],[0]
"In the introduction, we highlighted two core aspects of our approach to semantics: compositionality (by way of grounding graphs and structure-to-structure alignments) and planning (by way of inference with lookahead and sequence-to-sequence alignments).",6 Evaluation,[0],[0]
"To evaluate these, we additionally present a pair of ablation experiments: no grounding graphs (an agent with an unstructured representation of environment state), and no planning (a reflex agent with no lookahead).
",6 Evaluation,[0],[0]
"Map reading Our first application is the map navigation task established by Vogel and Jurafsky (2010), based on data collected for a psychological experiment by Anderson et al. (1991) (Figure 1a).",6 Evaluation,[0],[0]
"Each training datum consists of a map with a designated starting position, and a collection of landmarks, each labeled with a spatial coordinate and a string name.",6 Evaluation,[0],[0]
"Names are not always unique, and landmarks in the test set are never observed during training.",6 Evaluation,[0],[0]
This map is accompanied by a set of instructions specifying a path from the starting position to some (unlabeled) destination point.,6 Evaluation,[0],[0]
"These instruction sets are informal and redundant, involving as many as a hundred utterances.",6 Evaluation,[0],[0]
"They are transcribed from spoken text, so grammatical errors, disfluencies, etc. are common.",6 Evaluation,[0],[0]
"This is a
prime example of a domain that does not lend itself to logical representation—grammars may be too rigid, and previously-unseen landmarks and real-valued positions are handled more easily with feature machinery than predicate logic.
",6 Evaluation,[0],[0]
"The map task was previously studied by Vogel and Jurafsky (2010), who implemented SARSA with a simple set of features.",6 Evaluation,[0],[0]
"By combining these features with our alignment model and search procedure, we achieve state-of-the-art results on this task by a substantial margin (Table 1).
",6 Evaluation,[0],[0]
Some learned feature values are shown in Table 2.,6 Evaluation,[0],[0]
The model correctly infers cardinal directions (the example shows the preferred side of a destination landmark modified by the word top).,6 Evaluation,[0],[0]
"Like Vogel et al., we see support for both allocentric references (you are on top of the hill) and egocentric references (the hill is on top of you).",6 Evaluation,[0],[0]
"We can also see pragmatics at work: the model learns useful text-independent constraints—in this case, that near destinations should be preferred to far ones.
",6 Evaluation,[0],[0]
Maze navigation The next application we consider is the maze navigation task of MacMahon et al. (2006) (Figure 1b).,6 Evaluation,[0],[0]
"Here, a virtual agent is sit-
uated in a maze (whose hallways are distinguished with various wallpapers, carpets, and the presence of a small set of standard objects), and again given instructions for getting from one point to another.",6 Evaluation,[0],[0]
"This task has been the subject of focused attention in semantic parsing for several years, resulting in a variety of sophisticated approaches.
",6 Evaluation,[0],[0]
"Despite superficial similarity to the previous navigation task, the language and plans required for this task are quite different.",6 Evaluation,[0],[0]
"The proportion of instructions to actions is much higher (so redundancy much lower), and the interpretation of language is highly compositional.
",6 Evaluation,[0],[0]
"As can be seen in Table 3, we outperform a number of systems purpose-built for this navigation task.",6 Evaluation,[0],[0]
"We also outperform both variants of our system, most conspicuously the variant without grounding graphs.",6 Evaluation,[0],[0]
This highlights the importance of compositional structure.,6 Evaluation,[0],[0]
"Recent work by Kim and Mooney (2013) and Artzi et al. (2014) has achieved better results; these systems make use of techniques and resources (respectively, discriminative reranking and a seed lexicon of handannotated logical forms) that are largely orthogonal to the ones used here, and might be applied to improve our own results as well.
",6 Evaluation,[0],[0]
Puzzle solving The last task we consider is the Crossblock task studied by Branavan et al. (2009) (Figure 1c).,6 Evaluation,[0],[0]
"Here, again, natural language is used to specify a sequence of actions, in this case the solution to a simple game.",6 Evaluation,[0],[0]
"The environment is simple enough to be captured with a flat feature
4We specifically targeted the single-sentence version of this evaluation, as an alternative full-sequence evaluation does not align precisely with our data condition.
",6 Evaluation,[0],[0]
"representation, so there is no distinction between the full model and the variant without grounding graphs.
",6 Evaluation,[0],[0]
"Unlike the other tasks we consider, Crossblock is distinguished by a challenging associated search problem.",6 Evaluation,[0],[0]
Here it is nontrivial to find any sequence that eliminates all the blocks (the goal of the puzzle).,6 Evaluation,[0],[0]
"Thus this example allows us measure the effectiveness of our search procedure.
",6 Evaluation,[0],[0]
Results are shown in Table 4.,6 Evaluation,[0],[0]
"As can be seen, our model achieves state-of-the-art performance on this task when attempting to match the humanspecified plan exactly.",6 Evaluation,[0],[0]
"If we are purely concerned with task completion (i.e. solving the puzzle, perhaps not with the exact set of moves specified in the instructions) we can measure this directly.",6 Evaluation,[0],[0]
"Here, too, we substantially outperform a no-text baseline.",6 Evaluation,[0],[0]
"Thus it can be seen that text induces a useful heuristic, allowing the model to solve a considerable fraction of problem instances not solved by naı̈ve beam search.
",6 Evaluation,[0],[0]
"The problem of inducing planning heuristics from side information like text is an important one in its own right, and future work might focus specifically on coupling our system with a more sophisticated planner.",6 Evaluation,[0],[0]
"Even at present, the results in this section demonstrate the importance of lookahead and high-level reasoning in instruction following.",6 Evaluation,[0],[0]
"We have described a new alignment-based compositional model for following sequences of natural language instructions, and demonstrated the effectiveness of this model on a variety of tasks.",7 Conclusion,[0],[0]
"A fully general solution to the problem of contextual interpretation must address a wide range of wellstudied problems, but the work we have described
here provides modular interfaces for the study of a number of fundamental linguistic issues from a machine learning perspective.",7 Conclusion,[0],[0]
"These include:
Pragmatics How do we respond to presupposition failures, and choose among possible interpretations of an instruction disambiguated only by context?",7 Conclusion,[0],[0]
"The mechanism provided by the sequence-prediction architecture we have described provides a simple answer to this question, and our experimental results demonstrate that the learned pragmatics aid interpretation of instructions in a number of concrete ways: ambiguous references are resolved by proximity in the map reading task, missing steps are inferred from an environment model in the maze navigation task, and vague hints are turned into real plans by knowledge of the rules in Crossblock.",7 Conclusion,[0],[0]
"A more comprehensive solution might explicitly describe the process by which instruction-givers’ own beliefs (expressed as distributions over sequences) give rise to instructions.
",7 Conclusion,[0],[0]
Compositional semantics,7 Conclusion,[0],[0]
"The graph alignment model of semantics presented here is an expressive and computationally efficient generalization of classical logical techniques to accommodate environments like the map task, or those explored in our previous work (Andreas and Klein, 2014).",7 Conclusion,[0],[0]
"More broadly, our model provides a compositional approach to semantics that does not require an explicit formal language for encoding sentence meaning.",7 Conclusion,[0],[0]
"Future work might extend this approach to tasks like question answering, where logicbased approaches have been successful.
",7 Conclusion,[0],[0]
Our primary goal in this paper has been to explore methods for integrating compositional semantics and the pragmatic context provided by sequential structures.,7 Conclusion,[0],[0]
"While there is a great deal of work left to do, we find it encouraging that this general approach results in substantial gains across multiple tasks and contexts.",7 Conclusion,[0],[0]
The authors would like to thank S.R.K. Branavan for assistance with the Crossblock evaluation.,Acknowledgments,[0],[0]
The first author is supported by a National Science Foundation Graduate Fellowship.,Acknowledgments,[0],[0]
This paper describes an alignment-based model for interpreting natural language instructions in context.,abstractText,[0],[0]
"We approach instruction following as a search over plans, scoring sequences of actions conditioned on structured observations of text and the environment.",abstractText,[0],[0]
"By explicitly modeling both the low-level compositional structure of individual actions and the high-level structure of full plans, we are able to learn both grounded representations of sentence meaning and pragmatic constraints on interpretation.",abstractText,[0],[0]
"To demonstrate the model’s flexibility, we apply it to a diverse set of benchmark tasks.",abstractText,[0],[0]
"On every task, we outperform strong task-specific baselines, and achieve several new state-of-the-art results.",abstractText,[0],[0]
Alignment-Based Compositional Semantics for Instruction Following,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1348–1358, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics",text,[0],[0]
"With more than one hundred thousand new scholarly articles being published each year, there is a rapid growth in the number of citations for the relevant scientific articles.",1 Introduction,[0],[0]
"In this context, we highlight the following interesting facts about the process of citing scientific articles: (i) the most commonly cited paper by Gerard Salton, titled “A Vector Space Model for Information Retrieval” (alleged to have been published in 1975) does not actually exist in reality (Dubin, 2004), (ii) the scientific authors read only 20% of the works they cite (Simkin and Roychowdhury, 2003), (iii) one third of the refer-
ences in a paper are redundant and 40% are perfunctory (Moravcsik and Murugesan, 1975), (iv) 62.7% of the references could not be attributed a specific function (definition, tool etc.)",1 Introduction,[0],[0]
"(Teufel et al., 2006).",1 Introduction,[0],[0]
"Despite these facts, the existing bibliographic metrics consider that all citations are equally significant.
",1 Introduction,[0],[0]
"In this paper, we would emphasize the fact that all the references of a paper are not equally influential.",1 Introduction,[0],[0]
"For instance, we believe that for our current paper, (Wan and Liu, 2014) is more influential reference than (Garfield, 2006), although the former has received lower citations (9) than the latter (1650) so far1.",1 Introduction,[0],[0]
"Therefore the influence of a cited paper completely depends upon the context of the citing paper, not the overall citation count of the cited paper.",1 Introduction,[0],[0]
"We further took the opinion of the original authors of few selective papers and realized that around 16% of the references in a paper are highly influential, and the rest are trivial (Section 4).",1 Introduction,[0],[0]
"This motivates us to design a prediction model, GraLap to automatically label the influence of a cited paper with respect to a citing paper.",1 Introduction,[0],[0]
"Here, we label paper-reference pairs rather than references alone, because a reference that is influential for one citing paper may not be influential with equal extent for another citing paper.
",1 Introduction,[0],[0]
"We experiment with ACL Anthology Network (AAN) dataset and show that GraLap along with the novel feature set, quite efficiently, predicts the intensity of references of papers, which achieves (Pearson) correlation of 0.90 with the human annotations.",1 Introduction,[0],[0]
"Finally, we present four interesting appli-
1The statistics are taken from Google Scholar on June 2, 2016.
1348
cations to show the efficacy of considering unequal intensity of references, compared to the uniform intensity.
",1 Introduction,[0],[0]
"The contributions of the paper are four-fold: (i) we acquire a rich annotated dataset where paperreference pairs are labeled based on the influence scores (Section 4), which is perhaps the first goldstandard for this kind of task; (ii) we propose a graph-based label propagation model GraLap for semi-supervised learning which has tremendous potential for any task where the training set is less in number and labels are non-uniformly distributed (Section 3); (iii) we propose a diverse set of features (Section 3.3); most of them turn out to be quite effective to fit into the prediction model and yield improved results (Section 5); (iv) we present four applications to show how incorporating the reference intensity enhances the performance of several stateof-the-art systems (Section 6).",1 Introduction,[0],[0]
All the references of a paper usually do not carry equal intensity/strength with respect to the citing paper because some papers have influenced the research more than others.,2 Defining Intensity of References,[0],[0]
"To pin down this intuition, here we discretize the reference intensity by numerical values within the range of 1 to 5, (5: most influential, 1: least influential).",2 Defining Intensity of References,[0],[0]
"The appropriate definitions of different labels of reference intensity are presented in Figure 1, which are also the basis of building the annotated dataset (see Section 4):
Note that “reference intensity” and “reference similarity” are two different aspects.",2 Defining Intensity of References,[0],[0]
It might happen that two similar reference are used with different intensity levels in a citing paper – while one is just mentioned somewhere in the paper and other is used as a baseline.,2 Defining Intensity of References,[0],[0]
"Here, we address the former problem as a semi-supervised learning problem with clues taken from content of the citing and cited papers.",2 Defining Intensity of References,[0],[0]
"In this section, we formally define the problem and introduce our prediction model.",3 Reference Intensity Prediction Model,[0],[0]
"We are given a set of papers P = {P1, P2, ..., PM} and a sets of references R = {R1, R2, ..., RM}, where Ri corresponds to the set of references (or cited papers) of Pi.",3.1 Problem Definition,[0],[0]
"There is a set of papers PL ∈ P whose references RL ∈ R are already labeled by ` ∈ L = {1, ..., 5} (each reference is labeled with exactly one value).",3.1 Problem Definition,[0],[0]
"Our objective is to define a predictive function f that labels the references RU ∈ {R \ RL} of the papers PU ∈ {P \ PL} whose reference intensities are unknown, i.e., f : (P,R, PL, RL, PU , RL) −→ L.
Since the size of the annotated (labeled) data is much smaller than unlabeled data (|PL| |PU |), we consider it as a semi-supervised learning problem.
",3.1 Problem Definition,[0],[0]
Definition 1.,3.1 Problem Definition,[0],[0]
(Semi-supervised Learning),3.1 Problem Definition,[0],[0]
"Given a set of entries X and a set of possible labels YL, let us assume that (x1, y1), (x2, y2),..., (xl, yl) be the set of labeled data where xi is a data point and yi ∈ YL is its corresponding label.",3.1 Problem Definition,[0],[0]
"We assume that at least one instance of each class label
is present in the labeled dataset.",3.1 Problem Definition,[0],[0]
"Let (xl+1, yl+1), (xl+2, yl+2),..., (xl+n, yl+u) be the unlabeled data points where YU = {yl+1, yl+2, ...yl+u} are unknown.",3.1 Problem Definition,[0],[0]
"Each entry x ∈ X is represented by a set of features {f1, f2, ..., fD}.",3.1 Problem Definition,[0],[0]
"The problem is to determine the unknown labels using X and YL.
3.2 GraLap: A Prediction Model We propose GraLap, a variant of label propagation (LP) model proposed by (Zhu et al., 2003) where a node in the graph propagates its associated label to its neighbors based on the proximity.",3.1 Problem Definition,[0],[0]
We intend to assign same label to the vertices which are closely connected.,3.1 Problem Definition,[0],[0]
"However unlike the traditional LP model where the original values of the labels continue to fade as the algorithm progresses, we systematically handle this problem in GraLap.",3.1 Problem Definition,[0],[0]
"Additionally, we follow a post-processing in order to handle “classimbalance problem”.",3.1 Problem Definition,[0],[0]
Graph Creation.,3.1 Problem Definition,[0],[0]
"The algorithm starts with the creation of a fully connected weighted graph G = (X,E) where nodes are data points and the weight wij of each edge eij ∈ E is determined by the radial basis function as follows:
wij = exp
( − ∑D
d=1(x",3.1 Problem Definition,[0],[0]
d i,3.1 Problem Definition,[0],[0]
"− xdj )2 σ2
) (1)
The weight is controlled by a parameter σ.",3.1 Problem Definition,[0],[0]
"Later in this section, we shall discuss how σ is selected.",3.1 Problem Definition,[0],[0]
"Each node is allowed to propagate its label to its neighbors through edges (the more the edge weight, the easy to propagate).",3.1 Problem Definition,[0],[0]
Transition Matrix.,3.1 Problem Definition,[0],[0]
"We create a probabilistic transition matrix T|X|×|X|, where each entry Tij indicates the probability of jumping from j to i based on the following: Tij = P (j → i) =",3.1 Problem Definition,[0],[0]
"wij∑|X|
k=1",3.1 Problem Definition,[0],[0]
"wkj .
",3.1 Problem Definition,[0],[0]
Label Matrix.,3.1 Problem Definition,[0],[0]
"Here, we allow a soft label (interpreted as a distribution of labels) to be associated with each node.",3.1 Problem Definition,[0],[0]
"We then define a label matrix Y|X|×|L|, where ith row indicates the label distribution for node xi.",3.1 Problem Definition,[0],[0]
"Initially, Y contains only the values of the labeled data; others are zero.",3.1 Problem Definition,[0],[0]
Label Propagation Algorithm.,3.1 Problem Definition,[0],[0]
"This algorithm works as follows:
After initializing Y and T , the algorithm starts by disseminating the label from one node to its neighbors (including self-loop) in one step (Step 3).",3.1 Problem Definition,[0],[0]
"Then we normalize each entry of Y by the sum of its cor-
1: Initialize T and Y 2: while (Y does not converge) do 3: Y ← TY 4: Normalize rows of Y , yij =
yij∑ k yik
5: Reassign original labels to XL
responding row in order to maintain the interpretation of label probability (Step 4).",3.1 Problem Definition,[0],[0]
Step 5 is crucial; here we want the labeled sources XL to be persistent.,3.1 Problem Definition,[0],[0]
"During the iterations, the initial labeled nodes XL may fade away with other labels.",3.1 Problem Definition,[0],[0]
"Therefore we forcefully restore their actual label by setting yil = 1 (if xi ∈ XL is originally labeled as l), and other entries (∀j 6=lyij) by zero.",3.1 Problem Definition,[0],[0]
We keep on “pushing” the labels from the labeled data points which in turn pushes the class boundary through high density data points and settles in low density space.,3.1 Problem Definition,[0],[0]
"In this way, our approach intelligently uses the unlabeled data in the intermediate steps of the learning.",3.1 Problem Definition,[0],[0]
Assigning Final Labels.,3.1 Problem Definition,[0],[0]
"Once YU is computed, one may take the most likely label from the label distribution for each unlabeled data.",3.1 Problem Definition,[0],[0]
"However, this approach does not guarantee the label proportion observed in the annotated data (which in this case is not well-separated as shown in Section 4).",3.1 Problem Definition,[0],[0]
"Therefore, we adopt a label-based normalization technique.",3.1 Problem Definition,[0],[0]
"Assume that the label proportions in the labeled data are c1, ..., c|L| (s.t. ∑|L| i=1",3.1 Problem Definition,[0],[0]
ci = 1).,3.1 Problem Definition,[0],[0]
"In case of YU , we try to balance the label proportion observed in the ground-truth.",3.1 Problem Definition,[0],[0]
"The label mass is the column sum of YU , denoted by YU.1 , ..., YU.|L| , each of which is scaled in such a way that YU.1 : ... : YU.|L| = c1 : ... : c|L|.",3.1 Problem Definition,[0],[0]
The label of an unlabeled data point is finalized as the label with maximum value in the row of Y .,3.1 Problem Definition,[0],[0]
Convergence.,3.1 Problem Definition,[0],[0]
Here we briefly show that our algorithm is guaranteed to converge.,3.1 Problem Definition,[0],[0]
"Let us combine Steps 3 and 4 as Y ← T̂ Y , where T̂ = Tij/ ∑ k Tik.",3.1 Problem Definition,[0],[0]
"Y is composed of YLl×|L| and YUu×|L| , where YU never changes because of the reassignment.",3.1 Problem Definition,[0],[0]
"We can split T̂ at the boundary of labeled and unlabeled data as follows:
F̂ =",3.1 Problem Definition,[0],[0]
"[ T̂ll T̂lu T̂ul T̂uu ]
Therefore, YU ← T̂uuYU+ T̂ulYL, which can lead to YU = limn→∞ T̂nuuY 0",3.1 Problem Definition,[0],[0]
+,3.1 Problem Definition,[0],[0]
[ ∑n i=1,3.1 Problem Definition,[0],[0]
"T̂ (i−1) uu ]T̂ulYL, where Y 0 is the shape of Y at iteration 0.",3.1 Problem Definition,[0],[0]
"We need
to show T̂nuuijY 0",3.1 Problem Definition,[0],[0]
← 0.,3.1 Problem Definition,[0],[0]
"By construction, T̂ij ≥ 0, and since T̂ is row-normalized, and T̂uu is a part of T̂ , it leads to the following condition: ∃γ < 1, ∑u
j=1 T̂uuij ≤",3.1 Problem Definition,[0],[0]
"γ, ∀i = 1, ..., u.",3.1 Problem Definition,[0],[0]
"So, ∑
j
T̂nuuij = ∑
j
∑
k
T̂ (n−1) uuik T̂uukj
= ∑
k
T̂ (n−1) uuik
∑
j
T̂uuik
≤ ∑
k
T̂ (n−1) uuik",3.1 Problem Definition,[0],[0]
"γ
≤ γn
Therefore, the sum of each row in T̂nuuij converges to zero, which indicates T̂nuuijY
0",3.1 Problem Definition,[0],[0]
← 0.,3.1 Problem Definition,[0],[0]
Selection of σ.,3.1 Problem Definition,[0],[0]
"Assuming a spatial representation of data points, we construct a minimum spanning tree using Kruskal’s algorithm (Kruskal, 1956) with distance between two nodes measured by Euclidean distance.",3.1 Problem Definition,[0],[0]
"Initially, no nodes are connected.",3.1 Problem Definition,[0],[0]
We keep on adding edges in increasing order of distance.,3.1 Problem Definition,[0],[0]
"We choose the distance (say, df ) of the first edge which connects two components with different labeled points in them.",3.1 Problem Definition,[0],[0]
"We consider df as a heuristic to the minimum distance between two classes, and arbitrarily set σ = d0/3, following 3σ rule of normal distribution (Pukelsheim, 1994).",3.1 Problem Definition,[0],[0]
"We use a wide range of features that suitably represent a paper-reference pair (Pi, Rij), indicating Pi refers to Pj through reference Rij .",3.3 Features for Learning Model,[0],[0]
These features can be grouped into six general classes.,3.3 Features for Learning Model,[0],[0]
"3.3.1 Context-based Features (CF)
",3.3 Features for Learning Model,[0],[0]
The “reference context” of Rij in Pi is defined by three-sentence window (sentence where Rij occurs and its immediate previous and next sentences).,3.3 Features for Learning Model,[0],[0]
"For multiple occurrences, we calculate its average score.",3.3 Features for Learning Model,[0],[0]
We refer to “reference sentence” to indicate the sentence where Rij appears.,3.3 Features for Learning Model,[0],[0]
(i) CF:Alone.,3.3 Features for Learning Model,[0],[0]
It indicates whether Rij is mentioned alone in the reference context or together with other references.,3.3 Features for Learning Model,[0],[0]
(ii) CF:First.,3.3 Features for Learning Model,[0],[0]
"When Rij is grouped with others, this feature indicates whether it is mentioned first (e.g., “[2]” is first in “[2,4,6]”).
",3.3 Features for Learning Model,[0],[0]
"Next four features are based on the occurrence of words in the corresponding lists created manually (see Table 1) to understand different aspects.
",3.3 Features for Learning Model,[0],[0]
(iii) CF:Relevant.,3.3 Features for Learning Model,[0],[0]
It indicates whether Rij is explicitly mentioned as relevant in the reference context (Rel in Table 1).,3.3 Features for Learning Model,[0],[0]
(iv) CF:Recent.,3.3 Features for Learning Model,[0],[0]
It tells whether the reference context indicates that Rij is new (Rec in Table 1).,3.3 Features for Learning Model,[0],[0]
(v) CF:Extreme.,3.3 Features for Learning Model,[0],[0]
It implies that Rij is extreme in some way (Ext in Table 1).,3.3 Features for Learning Model,[0],[0]
(vi) CF:Comp.,3.3 Features for Learning Model,[0],[0]
"It indicates whether the reference context makes some kind of comparison with Rij (Comp in Table 1).
",3.3 Features for Learning Model,[0],[0]
"Note we do not consider any sentiment-based features as suggested by (Zhu et al., 2015).",3.3 Features for Learning Model,[0],[0]
"3.3.2 Similarity-based Features (SF)
",3.3 Features for Learning Model,[0],[0]
It is natural that the high degree of semantic similarity between the contents of Pi and Pj indicates the influence of Pj in Pi.,3.3 Features for Learning Model,[0],[0]
"We assume that although the full text of Pi is given, we do not have access to the full text of Pj (may be due to the subscription charge or the unavailability of the older papers).",3.3 Features for Learning Model,[0],[0]
"Therefore, we consider only the title of Pj as a proxy of its full text.",3.3 Features for Learning Model,[0],[0]
Then we calculate the cosine-similarity2 between the title (T) of Pj and (i) SF:TTitle.,3.3 Features for Learning Model,[0],[0]
"the title, (ii) SF:TAbs.",3.3 Features for Learning Model,[0],[0]
"the abstract, SF:TIntro.",3.3 Features for Learning Model,[0],[0]
"the introduction, (iv) SF:TConcl.",3.3 Features for Learning Model,[0],[0]
"the conclusion, and (v) SF:TRest.",3.3 Features for Learning Model,[0],[0]
"the rest of the sections (sections other than abstract, introduction and conclusion) of Pi.
",3.3 Features for Learning Model,[0],[0]
We further assume that the “reference context” (RC) of Pj in Pi might provide an alternate way of summarizing the usage of the reference.,3.3 Features for Learning Model,[0],[0]
"Therefore, we take the same similarity based approach mentioned above, but replace the title of Pj with its RC and obtain five more features: (vi) SF:RCTitle, (vii) SF:RCAbs, (viii) SF:RCIntro, (ix) SF:RCConcl and (x) SF:RCRest.",3.3 Features for Learning Model,[0],[0]
"If a reference appears multiple times in a citing paper, we consider the aggregation of all RCs together.",3.3 Features for Learning Model,[0],[0]
"The underlying assumption of these features is that a reference which occurs more frequently in a citing paper is more influential than a single occurrence (Singh et al., 2015).",3.3.3 Frequency-based Feature (FF),[0],[0]
We count the frequency of Rij in (i),3.3.3 Frequency-based Feature (FF),[0],[0]
FF:Whole.,3.3.3 Frequency-based Feature (FF),[0],[0]
"the entire content, (ii) FF:Intro.",3.3.3 Frequency-based Feature (FF),[0],[0]
"the introduction, (iii) FF:Rel. the related work, (iv) FF:Rest.",3.3.3 Frequency-based Feature (FF),[0],[0]
"the rest of the sections (as
2We use the vector space based model (Turney and Pantel, 2010) after stemming the words using Porter stammer (Porter, 1997).
mentioned in Section 3.3.2) of Pi.",3.3.3 Frequency-based Feature (FF),[0],[0]
We also introduce (v) FF:Sec. to measure the fraction of different sections of Pi where Rij occurs (assuming that appearance of Rij in different sections is more influential).,3.3.3 Frequency-based Feature (FF),[0],[0]
These features are further normalized using the number of sentences in Pi in order to avoid unnecessary bias on the size of the paper.,3.3.3 Frequency-based Feature (FF),[0],[0]
"Position of a reference in a paper might be a predictive clue to measure the influence (Zhu et al., 2015).",3.3.4 Position-based Features (PF),[0],[0]
"Intuitively, the earlier the reference appears in the paper, the more important it seems to us.",3.3.4 Position-based Features (PF),[0],[0]
"For the first two features, we divide the entire paper into two parts equally based on the sentence count and then see whether Rij appears (i) PF:Begin.",3.3.4 Position-based Features (PF),[0],[0]
in the beginning or (ii) PF:End.,3.3.4 Position-based Features (PF),[0],[0]
in the end of Pi.,3.3.4 Position-based Features (PF),[0],[0]
"Importantly, if Rij appears multiple times in Pi, we consider the fraction of times it occurs in each part.
",3.3.4 Position-based Features (PF),[0],[0]
"For the other two features, we take the entire paper, consider sentences as atomic units, and measure position of the sentences where Rij appears, including (iii) PF:Mean.",3.3.4 Position-based Features (PF),[0],[0]
"mean position of appearance, (iv) PF:Std. standard deviation of different appearances.",3.3.4 Position-based Features (PF),[0],[0]
"These features are normalized by the total length (number of sentences) of Pi. , thus ranging from 0 (indicating beginning of Pi) to 1 (indicating the end of Pi).",3.3.4 Position-based Features (PF),[0],[0]
The linguistic evidences around the context ofRij sometimes provide clues to understand the intrinsic influence of Pj on Pi.,3.3.5 Linguistic Features (LF),[0],[0]
Here we consider word level and structural features.,3.3.5 Linguistic Features (LF),[0],[0]
(i) LF:NGram.,3.3.5 Linguistic Features (LF),[0],[0]
"Different levels of n-grams (1- grams, 2-grams and 3-grams) are extracted from the reference context to see the effect of different word combination (Athar and Teufel, 2012).
",3.3.5 Linguistic Features (LF),[0],[0]
(ii) LF:POS.,3.3.5 Linguistic Features (LF),[0],[0]
"Part-of-speech (POS) tags of the words in the reference sentence are used as features (Jochim and Schütze, 2012).",3.3.5 Linguistic Features (LF),[0],[0]
(iii) LF:Tense.,3.3.5 Linguistic Features (LF),[0],[0]
"The main verb of the reference sentence is used as a feature (Teufel et al., 2006).",3.3.5 Linguistic Features (LF),[0],[0]
(iv) LF:Modal.,3.3.5 Linguistic Features (LF),[0],[0]
"The presence of modal verbs (e.g., “can”, “may”) often indicates the strength of the claims.",3.3.5 Linguistic Features (LF),[0],[0]
"Hence, we check the presence of the modal verbs in the reference sentence.",3.3.5 Linguistic Features (LF),[0],[0]
(v) LF:MainV. We use the main-verb of the reference sentence as a direct feature in the model.,3.3.5 Linguistic Features (LF),[0],[0]
(vi) LF:hasBut.,3.3.5 Linguistic Features (LF),[0],[0]
"We check the presence of conjunction “but”, which is another clue to show less confidence on the cited paper.",3.3.5 Linguistic Features (LF),[0],[0]
(vii) LF:DepRel.,3.3.5 Linguistic Features (LF),[0],[0]
"Following (Athar and Teufel, 2012)",3.3.5 Linguistic Features (LF),[0],[0]
"we use all the dependencies present in the reference context, as given by the dependency parser (Marneffe et al., 2006).",3.3.5 Linguistic Features (LF),[0],[0]
(viii) LF:POSP.,3.3.5 Linguistic Features (LF),[0],[0]
"(Dong and Schfer, 2011) use seven regular expression patterns of POS tags to capture syntactic information; then seven boolean features mark the presence of these patterns.",3.3.5 Linguistic Features (LF),[0],[0]
"We also utilize the same regular expressions as shown below 3 with the examples (the empty parenthesis in each example indicates the presence of a reference token Rij in the corresponding sentence; while few examples are complete sentences, few are not):
• “.*\\(\\) VV[DPZN].*”: Chen () showed that cohesion is held in the vast majority of cases for English-French.
",3.3.5 Linguistic Features (LF),[0],[0]
• “.*(VHP|VHZ),3.3.5 Linguistic Features (LF),[0],[0]
"VV.*”: while Cherry and Lin () have shown it to be a strong feature for word alignment...
• “.*VH(D|G|N|P|Z) (RB )*VBN.*”: Inducing features for taggers by clustering has been tried by several researchers ().
",3.3.5 Linguistic Features (LF),[0],[0]
"• “.*MD (RB )*VB(RB )* VVN.*”: For example, the likelihood of those generative procedures can be accumulated to get the likelihood of the phrase pair ().
3The meaning of each POS tag can be found in http://nlp.stanford.edu/software/tagger.",3.3.5 Linguistic Features (LF),[0],[0]
"shtml(Toutanova and Manning, 2000).
",3.3.5 Linguistic Features (LF),[0],[0]
"• “[ IW.]*VB(D|P|Z) (RB )*VV[ND].*”: Our experimental set-up is modeled after the human evaluation presented in ().
",3.3.5 Linguistic Features (LF),[0],[0]
• “(RB )*PP (RB )*V.*”: We use CRF () to perform this tagging.,3.3.5 Linguistic Features (LF),[0],[0]
• “.*VVG (NP )*(CC )*(NP ).,3.3.5 Linguistic Features (LF),[0],[0]
"*”: Following (), we provide the an-
notators with only short sentences: those with source sentences between 10 and 25 tokens long.
",3.3.5 Linguistic Features (LF),[0],[0]
These are all considered as Boolean features.,3.3.5 Linguistic Features (LF),[0],[0]
"For each feature, we take all the possible evidences from all paper-reference pairs and prepare a vector.",3.3.5 Linguistic Features (LF),[0],[0]
"Then for each pair, we check the presence (absence) of tokens for the corresponding feature and mark the vector accordingly (which in turn produces a set of Boolean features).",3.3.5 Linguistic Features (LF),[0],[0]
This group provides other factors to explain why is a paper being cited.,3.3.6 Miscellaneous Features (MS),[0],[0]
(i) MS:GCount.,3.3.6 Miscellaneous Features (MS),[0],[0]
"To answer whether a highly-cited paper has more academic influence on the citing paper than the one which is less cited, we measure the number of other papers (except Pi) citing Pj .",3.3.6 Miscellaneous Features (MS),[0],[0]
"(ii) MS:SelfC. To see the effect of self-citation, we check whether at least one author is common in both Pi and Pj .",3.3.6 Miscellaneous Features (MS),[0],[0]
(iii) MG:Time.,3.3.6 Miscellaneous Features (MS),[0],[0]
"The fact that older papers are rarely cited, may not stipulate that these are less influential.",3.3.6 Miscellaneous Features (MS),[0],[0]
"Therefore, we measure the difference of the publication years of Pi and Pj .",3.3.6 Miscellaneous Features (MS),[0],[0]
(iv) MG:CoCite.,3.3.6 Miscellaneous Features (MS),[0],[0]
"It measures the co-citation counts of Pi and Pj defined by
|Ri∩Rj | |Ri∪Rj | , which in turn an-
swers the significance of reference-based similarity driving the academic influence (Small, 1973).
",3.3.6 Miscellaneous Features (MS),[0],[0]
"Following (Witten and Frank, 2005), we further make one step normalization and divide each feature by its maximum value in all the entires.",3.3.6 Miscellaneous Features (MS),[0],[0]
"We use the AAN dataset (Radev et al., 2009) which is an assemblage of papers included in ACL related venues.",4 Dataset and Annotation,[0],[0]
"The texts are preprocessed where sentences, paragraphs and sections are properly separated using different markers.",4 Dataset and Annotation,[0],[0]
"The filtered dataset contains 12,843 papers (on average 6.21 references per paper) and 11,092 unique authors.
",4 Dataset and Annotation,[0],[0]
"Next we use Parscit (Councill et al., 2008) to identify the reference contexts from the dataset and then extract the section headings from all the papers.",4 Dataset and Annotation,[0],[0]
"Then each section heading is mapped into one
of the following broad categories using the method proposed by (Liakata et al., 2012):",4 Dataset and Annotation,[0],[0]
"Abstract, Introduction, Related Work, Conclusion and Rest.",4 Dataset and Annotation,[0],[0]
Dataset Labeling.,4 Dataset and Annotation,[0],[0]
The hardest challenge in this task is that there is no publicly available dataset where references are annotated with the intensity value.,4 Dataset and Annotation,[0],[0]
"Therefore, we constructed our own annotated dataset in two different ways.",4 Dataset and Annotation,[0],[0]
(i) Expert Annotation: we requested members of our research group4 to participate in this survey.,4 Dataset and Annotation,[0],[0]
"To facilitate the labeling process, we designed a portal where all the papers present in our dataset are enlisted in a drop-down menu.",4 Dataset and Annotation,[0],[0]
"Upon selecting a paper, its corresponding references were shown with five possible intensity values.",4 Dataset and Annotation,[0],[0]
The citing and cited papers are also linked to the original texts so that the annotators can read the original papers.,4 Dataset and Annotation,[0],[0]
A total of 20 researchers participated and they were asked to label as many paperreference pairs as they could based on the definitions of the intensity provided in Section 2.,4 Dataset and Annotation,[0],[0]
The annotation process went on for one month.,4 Dataset and Annotation,[0],[0]
"Out of total 1640 pairs annotated, 1270 pairs were taken such that each pair was annotated by at least two annotators, and the final intensity value of the pair was considered to be the average of the scores.",4 Dataset and Annotation,[0],[0]
The Pearson correlation and Kendell’s τ among the annotators are 0.787 and 0.712 respectively.,4 Dataset and Annotation,[0],[0]
(ii) Author Annotation: we believe that the authors of a paper are the best experts to judge the intensity of references present in the paper.,4 Dataset and Annotation,[0],[0]
"With this intension, we launched a survey where we requested the authors whose papers are present in our dataset with significant numbers.",4 Dataset and Annotation,[0],[0]
We designed a web portal in similar fashion mentioned earlier; but each author was only shown her own papers in the drop-down menu.,4 Dataset and Annotation,[0],[0]
"Out of 35 requests, 22 authors responded and total 196 pairs are annotated.",4 Dataset and Annotation,[0],[0]
This time we made sure that each paper-reference pair was annotated by only one author.,4 Dataset and Annotation,[0],[0]
"The percentages of labels in the overall annotated dataset are as follows: 1: 9%, 2: 74%, 3: 9%, 4: 3%, 5: 4%.",4 Dataset and Annotation,[0],[0]
"In this section, we start with analyzing the importance of the feature sets in predicting the reference
4All were researchers with the age between 25-45 working on document summarization, sentiment analysis, and text mining in NLP.
intensity, followed by the detailed results.",5 Experimental Results,[0],[0]
Feature Analysis.,5 Experimental Results,[0],[0]
"In order to determine which features highly determine the gold-standard labeling, we measure the Pearson correlation between various features and the ground-truth labels.",5 Experimental Results,[0],[0]
"Figure 2(a) shows the average correlation for each feature group, and in each group the rank of features based on the correlation is shown in Figure 2(b).",5 Experimental Results,[0],[0]
"Frequencybased features (FF) turn out to be the best, among which FF:Rest is mostly correlated.",5 Experimental Results,[0],[0]
This set of features is convenient and can be easily computed.,5 Experimental Results,[0],[0]
Both CF and LF seem to be equally important.,5 Experimental Results,[0],[0]
"However, PF tends to be less important in this task.
",5 Experimental Results,[0],[0]
Results of Predictive Models.,5 Experimental Results,[0],[0]
"For the purpose of evaluation, we report the average results after 10- fold cross-validation.",5 Experimental Results,[0],[0]
"Here we consider five baselines to compare with GraLap: (i) Uniform: assign 3 to all the references assuming equal intensity, (ii) SVR+W: recently proposed Support Vector Regression (SVR) with the feature set mentioned in (Wan and Liu, 2014), (iii) SVR+O:",5 Experimental Results,[0],[0]
"SVR model with our feature set, (iv) C4.5SSL: C4.5 semisupervised algorithm with our feature set (Quinlan, 1993), and (v) GLM: the traditional graph-based LP model with our feature set (Zhu et al., 2003).",5 Experimental Results,[0],[0]
"Three metrics are used to compare the results of the competing models with the annotated labels: Root Mean Square Error (RMSE), Pearson’s correlation coeffi-
cient (ρ), and coefficient of determination (R2)5.",5 Experimental Results,[0],[0]
Table 2 shows the performance of the competing models.,5 Experimental Results,[0],[0]
We incrementally include each feature set into GraLap greedily on the basis of ranking shown in Figure 2(a).,5 Experimental Results,[0],[0]
We observe that GraLap with only FF outperforms SVR+O with 41% improvement of ρ.,5 Experimental Results,[0],[0]
"As expected, the inclusion of PF into the model improves the model marginally.",5 Experimental Results,[0],[0]
"However, the overall performance of GraLap is significantly higher than any of the baselines (p < 0.01).",5 Experimental Results,[0],[0]
"In this section, we provide four different applications to show the use of measuring the intensity of references.",6 Applications of Reference Intensity,[0],[0]
"To this end, we consider all the labeled entries for training and run GraLap to predict the intensity of rest of the paper-reference pairs.",6 Applications of Reference Intensity,[0],[0]
Influential papers in a particular area are often discovered by considering equal weights to all the citations of a paper.,6.1 Discovering Influential Articles,[0],[0]
We anticipate that considering the reference intensity would perhaps return more meaningful results.,6.1 Discovering Influential Articles,[0],[0]
"To show this, Here we use the following measures individually to compute the influence of a paper: (i) RawCite: total number of citations per paper, (ii) RawPR: we construct a citation network (nodes: papers, links: citations), and measure PageRank (Page et al., 1998) of each node n: PR(n) = 1−qN + q ∑ m∈M(n) PR(m)",6.1 Discovering Influential Articles,[0],[0]
"|L(m)| ; where, q, the damping factor, is set to 0.85, N is the total number of nodes, M(n) is the set of nodes that have edges to n, and L(m) is the set of nodes that m has an edge to, (iii) InfCite: the weighted version of RawCite, measured by the sum of intensities of all citations of a paper, (iv) InfPR: the weighted version of RawPR: PR(n) =",6.1 Discovering Influential Articles,[0],[0]
"1−qN + q ∑
m∈M(n) Inf(m→n)PR(m)∑
a∈L(m)Inf(m→a) , where Inf indicates
the influence of a reference.",6.1 Discovering Influential Articles,[0],[0]
We rank all the articles based on these four measures separately.,6.1 Discovering Influential Articles,[0],[0]
Table 3(a) shows the Spearman’s rank correlation between pair-wise measures.,6.1 Discovering Influential Articles,[0],[0]
"As expected, (i) and (ii) have high correlation (same for (iii) and (iv)), whereas across two types of measures the correlation is less.",6.1 Discovering Influential Articles,[0],[0]
"Further, in order to know which mea-
5The less (resp. more) the value of RMSE and R2 (resp.",6.1 Discovering Influential Articles,[0],[0]
"ρ), the better the performance of the models.
",6.1 Discovering Influential Articles,[0],[0]
"sure is more relevant, we conduct a subjective study where we select top ten papers from each measure and invite the experts (not authors) who annotated the dataset, to make a binary decision whether a recommended paper is relevant.",6.1 Discovering Influential Articles,[0],[0]
6.,6.1 Discovering Influential Articles,[0],[0]
"The average pairwise inter-annotator’s agreement (based on Cohen’s kappa (Cohen, 1960)) is 0.71.",6.1 Discovering Influential Articles,[0],[0]
"Table 3(b) presents that out of 10 recommendations of InfPR, 7 (5) papers are marked as influential by majority (all) of the annotators, which is followed by InfCite.",6.1 Discovering Influential Articles,[0],[0]
These results indeed show the utility of measuring reference intensity for discovering influential papers.,6.1 Discovering Influential Articles,[0],[0]
Top three papers based on InfPR from the entire dataset are shown in Table 4.,6.1 Discovering Influential Articles,[0],[0]
"H-index, a measure of impact/influence of an author, considers each citation with equal weight (Hirsch, 2005).",6.2 Identifying Influential Authors,[0],[0]
"Here we incorporate the notion of reference intensity into it and define hif-index.
",6.2 Identifying Influential Authors,[0],[0]
Definition 2.,6.2 Identifying Influential Authors,[0],[0]
"An author A with a set of papers P (A) has an hif-index equals to h, if h is the largest value such that |{p ∈ P (A)|Inf(p) ≥ h}| ≥ h; where Inf(p) is the sum of intensities of all citations of p.
",6.2 Identifying Influential Authors,[0],[0]
We consider 37 ACL fellows as the list of goldstandard influential authors.,6.2 Identifying Influential Authors,[0],[0]
"For comparative evaluation, we consider the total number of papers (TotP), total number of citations (TotC) and average citations per paper (AvgC) as three competing measures along with h-index and hif-index.",6.2 Identifying Influential Authors,[0],[0]
We arrange all the authors in our dataset in decreasing order of each measure.,6.2 Identifying Influential Authors,[0],[0]
Figure 3(a) shows the Spearman’s rank correlation among the common elements across pair-wise rankings.,6.2 Identifying Influential Authors,[0],[0]
Figure 3(b) shows the Precision@k for five competing measures at identifying ACL fellows.,6.2 Identifying Influential Authors,[0],[0]
"We observe that hif-index performs significantly well with an overall precision of 0.54, followed by AvgC (0.37),
6We choose papers from the area of “sentiment analysis” on which experts agree on evaluating the papers.
",6.2 Identifying Influential Authors,[0],[0]
"h-index (0.35), TotC (0.32) and TotP (0.34).",6.2 Identifying Influential Authors,[0],[0]
This result is an encouraging evidence that the referenceintensity could improve the identification of the influential authors.,6.2 Identifying Influential Authors,[0],[0]
Top three authors based on hif-index are shown in Table 4.,6.2 Identifying Influential Authors,[0],[0]
Here we show the effectiveness of referenceintensity by applying it to a real paper recommendation system.,6.3 Effect on Recommendation System,[0],[0]
"To this end, we consider FeRoSA7 (Chakraborty et al., 2016), a new (probably the first) framework of faceted recommendation for scientific articles, where given a query it provides facetwise recommendations with each facet representing the purpose of recommendation (Chakraborty et al., 2016).",6.3 Effect on Recommendation System,[0],[0]
The methodology is based on random walk with restarts (RWR) initiated from a query paper.,6.3 Effect on Recommendation System,[0],[0]
The model is built on AAN dataset and considers both the citation links and the content information to produce the most relevant results.,6.3 Effect on Recommendation System,[0],[0]
"Instead of using the unweighted citation network, here we use the weighted network with each edge labeled by the intensity score.",6.3 Effect on Recommendation System,[0],[0]
The final recommendation of FeRoSA is obtained by performing RWR with the transition probability proportional to the edge-weight (we call it Inf-FeRoSA).,6.3 Effect on Recommendation System,[0],[0]
"We observe that Inf-FeRoSA achieves an average precision of 0.81 at top 10 recommendations, which is 14% higher then FeRoSA while considering the flat version and 12.34% higher than FeRoSA while considering the faceted version.",6.3 Effect on Recommendation System,[0],[0]
"Recently, Thomson Reuters began screening for journals that exchange large number of anomalous citations with other journals in a cartel-like arrangement, often known as “citation stacking” (Jump, 2013; Hardcastle, 2015).",6.4 Detecting Citation Stacking,[0],[0]
"This sort of citation stacking is much more pernicious and difficult to detect.
",6.4 Detecting Citation Stacking,[0],[0]
"7www.ferosa.org
We anticipate that this behavior can be detected by the reference intensity.",6.4 Detecting Citation Stacking,[0],[0]
"Since the AAN dataset does not have journal information, we use DBLP dataset (Singh et al., 2015) where the complete metadata information (along with reference contexts and abstract) is available, except the full content of the paper (559,338 papers and 681 journals; more details in (Chakraborty et al., 2014)).",6.4 Detecting Citation Stacking,[0],[0]
"From this dataset, we extract all the features mentioned in Section 3.3 except the ones that require full text, and run our model using the existing annotated dataset as training instances.",6.4 Detecting Citation Stacking,[0],[0]
We measure the traditional impact factor (IF ) of the journals and impact factor after considering the reference intensity (IFif ).,6.4 Detecting Citation Stacking,[0],[0]
"Figure 4(a) shows that there are few journals whose IFif significantly deviates (3σ from the mean) from IF ; out of the suspected journals 70% suffer from the effect of self-journal citations as well (shown in Figure 4(b)), example including Expert Systems with Applications (current IF of 2.53).",6.4 Detecting Citation Stacking,[0],[0]
One of the future work directions would be to predict such journals as early as possible after their first appearance.,6.4 Detecting Citation Stacking,[0],[0]
"Although the citation count based metrics are widely accepted (Garfield, 2006; Hirsch, 2010), the belief that mere counting of citations is dubious has also been a subject of study (Chubin and Moitra, 1975).",7 Related Work,[0],[0]
"(Garfield, 1964) was the first who explained the reasons of citing a paper.",7 Related Work,[0],[0]
"(Pham and Hoffmann, 2003) introduced a method for the rapid development of complex rule bases for classifying text segments.
",7 Related Work,[0],[0]
"(Dong and Schfer, 2011) focused on a less manual approach by learning domain-insensitive features from textual, physical, and syntactic aspects To address concerns about h-index, different alternative measures are proposed (Waltman and van Eck, 2012).",7 Related Work,[0],[0]
However they too could benefit from filtering or weighting references with a model of influence.,7 Related Work,[0],[0]
"Several research have been proposed to weight citations based on factors such as the prestige of the citing journal (Ding, 2011; Yan and Ding, 2010), prestige of an author (Balaban, 2012), frequency of citations in citing papers (Hou et al., 2011).",7 Related Work,[0],[0]
"Recently, (Wan and Liu, 2014) proposed a SVR based approach to measure the intensity of citations.",7 Related Work,[0],[0]
"Our methodology differs from this approach in at lease four significant ways: (i) they used six very shallow level features; whereas we consider features from different dimensions, (ii) they labeled the dataset by the help of independent annotators; here we additionally ask the authors of the citing papers to identify the influential references which is very realistic (Gilbert, 1977); (iii) they adopted SVR for labeling, which does not perform well for small training instances; here we propose GraLap , designed specifically for small training instances; (iv) four applications of reference intensity mentioned here are completely new and can trigger further to reassessing the existing bibliometrics.",7 Related Work,[0],[0]
"We argued that the equal weight of all references might not be a good idea not only to gauge success of a research, but also to track follow-up work or recommending research papers.",8 Conclusion,[0],[0]
The annotated dataset would have tremendous potential to be utilized for other research.,8 Conclusion,[0],[0]
"Moreover, GraLap can be used for any semi-supervised learning problem.",8 Conclusion,[0],[0]
Each application mentioned here needs separate attention.,8 Conclusion,[0],[0]
"In future, we shall look into more linguistic evidences to improve our model.",8 Conclusion,[0],[0]
"Research accomplishment is usually measured by considering all citations with equal importance, thus ignoring the wide variety of purposes an article is being cited for.",abstractText,[0],[0]
"Here, we posit that measuring the intensity of a reference is crucial not only to perceive better understanding of research endeavor, but also to improve the quality of citation-based applications.",abstractText,[0],[0]
"To this end, we collect a rich annotated dataset with references labeled by the intensity, and propose a novel graph-based semisupervised model, GraLap to label the intensity of references.",abstractText,[0],[0]
Experiments with AAN datasets show a significant improvement compared to the baselines to achieve the true labels of the references (46% better correlation).,abstractText,[0],[0]
"Finally, we provide four applications to demonstrate how the knowledge of reference intensity leads to design better real-world applications.",abstractText,[0],[0]
All Fingers are not Equal: Intensity of References in Scientific Articles,title,[0],[0]
"Proceedings of the SIGDIAL 2015 Conference, pages 237–244, Prague, Czech Republic, 2-4 September 2015. c©2015 Association for Computational Linguistics
Our method involves clustering the document sentences into topics using a fuzzy clustering algorithm. Then each sentence is scored according to how well it covers the various topics. This is done using statistical features such as TF, sentence length, etc. Finally, the summary is constructed from the highest scoring sentences, while avoiding overlap between the summary sentences. This makes it language-independent, but we have to afford preprocessed data first (tokenization, stemming, etc.).",text,[0],[0]
"A document summary can be regarded as domainspecific or general-purpose, using the specificity as classification criterion (Hovy and Lin, 1998).",1 Introduction,[0],[0]
"We can, also, look at this criterion from language angle: language-specific or language-independent summarization.",1 Introduction,[0],[0]
Language-independent systems can handle more than one language.,1 Introduction,[0],[0]
"They can be partially language-independent, which means they use language-related resources, and therefore you can’t add a new language so easily.",1 Introduction,[0],[0]
"Inversely, they can be fully language-independent.
",1 Introduction,[0],[0]
"Recently, multilingual summarization has received the attention of the summarization community, such as Text Analysis Conference (TAC).",1 Introduction,[0],[0]
"The TAC 2011 workshop included a task called “MultiLing task”, which aims to evaluate languageindependent summarization algorithms on a variety of languages (Giannakopoulos et al., 2011).",1 Introduction,[0],[0]
"In
the task’s pilot, there were seven languages covering news texts: Arabic, Czech, English, French, Greek, Hebrew and Hindi, where each system has to participate for at least two languages.",1 Introduction,[0],[0]
MultiLing 2013 workshop is a community-driven initiative for testing and promoting multilingual summarization methods.,1 Introduction,[0],[0]
It aims to evaluate the application of (partially or fully) language-independent summarization algorithms on a variety of languages.,1 Introduction,[0],[0]
"There were three tasks: “Multi-document multilingual summarization”(Giannakopoulos, 2013), “Multilingual single document summarization” (Kubina et al., 2013) and “Multilingual summary evaluation”.",1 Introduction,[0],[0]
"The multi-document task uses the 7 past languages along with three new languages: Chinese, Romanian and Spanish.",1 Introduction,[0],[0]
"The single document task introduces 40 languages.
",1 Introduction,[0],[0]
"This paper contains a description of our method (Aries et al., 2013) which uses sentences’ clustering to define topics, and then trains on these topics to score each sentence.",1 Introduction,[0],[0]
"We will explain each task in the system (AllSummarizer), especially the preprocessing task which is languagedependent.",1 Introduction,[0],[0]
"Then, we will discuss how we fixed the summarization’s hyper-parameters (threshold and features) for each language.",1 Introduction,[0],[0]
The next section (Section 5) is reserved to discuss the experiments conducted in the MultiLing workshop.,1 Introduction,[0],[0]
"Finally, we will conclude by discussing possible improvements.",1 Introduction,[0],[0]
"Clustering has been used for summarization in many systems, either using documents as units, sentences or words.",2 Related works,[0],[0]
The resulted clusters are used to extract the summary.,2 Related works,[0],[0]
Some systems use just the biggest cluster to score sentences and get the top ones.,2 Related works,[0],[0]
"Others take from each cluster a representative sentence, in order to cover all topics.",2 Related works,[0],[0]
"While there are systems, like ours, which score sentences according to all clusters.
237
“CIST” (Liu et al., 2011; Li et al., 2013) is a system which uses hierarchical Latent Dirichlet Allocation topic (hLDA) model to cluster sentences into sub-topics.",2 Related works,[0],[0]
A sub,2 Related works,[0],[0]
-topic containing more sentences is more important and therefore those containing just one or two sentences can be neglected.,2 Related works,[0],[0]
The sentences are scored using hLDA model combined with some traditional features.,2 Related works,[0],[0]
"The system participated for multi-document summarization task, where all documents of the same topic are merged into a big text document.
",2 Related works,[0],[0]
"Likewise, “UoEssex” (El-Haj et al., 2011) uses a clustering method (K-Means) to regroup similar sentences.",2 Related works,[0],[0]
"The biggest cluster is used to extract the summary, while other clusters are ignored.",2 Related works,[0],[0]
"Then, the sentences are scored using their cosine similarities to the cluster’s centroid.",2 Related works,[0],[0]
"The use of the biggest cluster is justified by the assumption that a single cluster will give a coherent summary.
",2 Related works,[0],[0]
"The scoring functions of these two systems are based on statistical features like frequencies of words, cosine similarity, etc.",2 Related works,[0],[0]
"In the contrary, systems like those of Conroy et al. (2011) (“CLASSY”), Varma et al. (2011) (“SIEL IIITH”), El-Haj and Rayson (2013), etc. are corpus-based summarizers, which can make it hard to introduce new languages.",2 Related works,[0],[0]
“CLASSY” uses naı̈ve Bayes to estimate the probability that a term may be included in the summary.,2 Related works,[0],[0]
The classifier was trained on DUC 2005-2007 data.,2 Related works,[0],[0]
"As for backgrounds of each language, Wikinews are used to compute Dunning G-statistic.",2 Related works,[0],[0]
“SIEL IIITH” uses a probabilistic Hyperspace Analogue to Language model.,2 Related works,[0],[0]
"Given a word, it estimates the probability of observing another word with it in a window of size K, using a sufficiently large corpus.",2 Related works,[0],[0]
El-Haj and Rayson (2013) calculate the log-likelihood of each word using a corpus of words frequencies and the multiLing’13 dataset.,2 Related works,[0],[0]
"The score of each sentence is the sum of its words’ log-likelihoods.
",2 Related works,[0],[0]
"In our method (Aries et al., 2013), we use a simple fuzzy clustering algorithm.",2 Related works,[0],[0]
"We assume that a sentence can express many topics, and therefore it can belong to many clusters.",2 Related works,[0],[0]
"Also, we believe that a summary must take in consideration other topics than the main one (the biggest cluster).",2 Related works,[0],[0]
"To score sentences, we use a scoring function based on Naı̈ve Bayes classification.",2 Related works,[0],[0]
"It uses the clusters for training rather than a corpus, in order to avoid the problem of language dependency.",2 Related works,[0],[0]
One of multilingual summarization’s problem is the lack of resources such as labeled corpus used for learning.,3 System overview,[0],[0]
"Learning algorithms were used either to select the sentences that should be in the summary, or to estimate the features’ weights.",3 System overview,[0],[0]
Both cases need a training corpus given the language and the domain we want to adapt the summarizer to.,3 System overview,[0],[0]
"To design a language-neutral summarization system, either we adapt a system for input languages (Partly language-neutral), or we design a system that can process any language (Fully language-neutral).
",3 System overview,[0],[0]
"Our sentence extraction method can be applied to any language without any modifications, affording the pre-process step of the input language.",3 System overview,[0],[0]
"To do this, we had to find a new method to train our system other than using a corpus (language and topic dependent).",3 System overview,[0],[0]
The idea was to find different topics in the input text using similarity between sentences.,3 System overview,[0],[0]
"Then, we train the system using a scoring function based on Bayes classification algorithm and a set of features to find the probability of a feature given the topic.",3 System overview,[0],[0]
"Finally, we calculate for each sentence a score that reflects how it can represent all the topics.
",3 System overview,[0],[0]
"In our previous work (Aries et al., 2013), our system used only two features which have the same nature (TF: uni-grams and bi-grams).",3 System overview,[0],[0]
"When we add new features, this can affect the final result (summary).",3 System overview,[0],[0]
"Also, our clustering method lies on the clustering threshold which has to be estimated somehow.",3 System overview,[0],[0]
"To handle multi-document summarization, we just fuse all documents in the same topic and consider them as one document.",3 System overview,[0],[0]
Figure 1 represents the general architecture of AllSummarizer1.,3 System overview,[0],[0]
"This is the language-dependent part, which can be found in many information retrieval (IR) works.",3.1 Preprocessing,[0],[0]
"In our system, we are interested in four preprocessing tasks:
• Normalizer: in this step, we can delete special characters.",3.1 Preprocessing,[0],[0]
"For Arabic, we can delete diacritics (Tashkiil) if we don’t need them in the process (which is our case).
",3.1 Preprocessing,[0],[0]
"• Segmenter: The segmenter defines two func1 https://github.com/kariminf/AllSummarizer
tions: sentence segmentation and word tokenization.
",3.1 Preprocessing,[0],[0]
•,3.1 Preprocessing,[0],[0]
"Stemmer: The role of this task is to delete suffixes and prefixes so we can get the stem of a word.
",3.1 Preprocessing,[0],[0]
"• Stop-Words eliminator: It is used to remove the stop words, which are the words having no signification added to the text.
",3.1 Preprocessing,[0],[0]
"In this work, normalization is used just for Arabic and Persian to delete diacritics (Tashkiil).",3.1 Preprocessing,[0],[0]
"Concerning stop-word elimination, we use precompiled word-lists available on the web.",3.1 Preprocessing,[0],[0]
Table 1 shows each language and the tools used in the remaining pre-processing tasks.,3.1 Preprocessing,[0],[0]
"Each text contains many topics, where a topic is a set of sentences having some sort of relationship between each other.",3.2 Topics clustering,[0],[0]
"In our case, this relationship is the cosine similarity between each two sentences.",3.2 Topics clustering,[0],[0]
"It means, the sentences that have many terms in common are considered in the same topic.",3.2 Topics clustering,[0],[0]
"Given two sentences X and Y , the cosine similar-
2 https://opennlp.apache.org/ 3 https://github.com/mojtaba-khallash/JHazm 4 https://lucene.apache.org/ 5 http://zeus.cs.pacificu.edu/shereen/research.htm 6 http://code972.com/hebmorph 7 http://snowball.tartarus.org/
ity between them is expressed by equation 1.
",3.2 Topics clustering,[0],[0]
"cos(X,Y )",3.2 Topics clustering,[0],[0]
=,3.2 Topics clustering,[0],[0]
"∑ i xi.yi√∑
i(xi)2.",3.2 Topics clustering,[0],[0]
"√∑ i(yi)2 (1)
Where xi (yi) denotes frequencies for each term in the sentence X (Y ).
",3.2 Topics clustering,[0],[0]
"To generate topics, we use a simple algorithm (see algorithm 1) which uses cosine similarity and a clustering threshold th to cluster n sentences.
",3.2 Topics clustering,[0],[0]
Algorithm 1: clustering method Data: Pre-processed sentences Result: clusters of sentences (C) foreach sentence,3.2 Topics clustering,[0],[0]
"Si / i = 1 to n do
Ci += Si ; // Ci: ith cluster foreach sentence",3.2 Topics clustering,[0],[0]
"Sj / j = i + 1 to n do
Sim = cosine similarity(Si, Sj) ; if sim > th then
Ci += Sj ; end
end C += Ci ;
end foreach cluster Ci / i=n to 1 do
foreach cluster Cj / j=i-1 to 1 do if Ci is included in Cj then
C -= Ci ; break ;
end end
end",3.2 Topics clustering,[0],[0]
"A summary is a short text that is supposed to represent most information in the source text, and cover most of its topics.",3.3 Scoring function,[0],[0]
"Therefore, we assume that a sentence si can be in the summary when it is most probable to represent all topics (clusters) cj ∈ C using a set of features fk ∈ F .",3.3 Scoring function,[0],[0]
"We used Naı̈ve Bayes, assuming independence between different classes and different features (a sentence can have multiple classes).",3.3 Scoring function,[0],[0]
"So, the score of a sentence si is the product over classes of the product over features of its score in a specific class and feature (see equation. 2).
",3.3 Scoring function,[0],[0]
"Score(si, ⋂ j cj , F ) = ∏ j ∏ k Score(si, cj , fk)
(2)
",3.3 Scoring function,[0],[0]
The score of a sentence si in a specific class cj and feature fk is the sum of probability of the feature’s observations when si ∈ cj (see equation. 3).,3.3 Scoring function,[0],[0]
"We add one to the sum, to avoid multiplying by a features’ score of zero.
Score(si, cj , fk) = 1 + ∑ φ∈si
P (fk = φ|si ∈ cj) (3)
Where φ is an observation of the feature fk in the sentence si.",3.3 Scoring function,[0],[0]
"For example, assuming the feature f1 is term frequency, and we have a sentence: “I am studying at home.”.",3.3 Scoring function,[0],[0]
"The sentence after pre-processing would be: s1 = {“studi”(stem of “study”), “home”}.",3.3 Scoring function,[0],[0]
"So, φ may be “studi” or “home”, or any other term.",3.3 Scoring function,[0],[0]
"If we take another feature f2 which is sentence position, the observation φ may take 1st, 2nd, 3rd, etc. as values.",3.3 Scoring function,[0],[0]
"We use 5 statistical features to score the sentences: unigram term frequency (TFU), bigram term frequency (TFB), sentence position (Pos) and sentence length (Rleng, PLeng).
",3.4 Statistical features,[0],[0]
Each feature divides the sentences to several categories.,3.4 Statistical features,[0],[0]
"For example, if we have a text written just with three characters: a, b and c, and the feature is the characters of the text, then we will have three categories.",3.4 Statistical features,[0],[0]
"Each category has a probability to occur in a cluster, which is the number of its appearance in this cluster divided by all cluster’s terms, as shown in equation 4.
",3.4 Statistical features,[0],[0]
Pf (f = φ|cj) =,3.4 Statistical features,[0],[0]
|φ ∈ cj |∑,3.4 Statistical features,[0],[0]
"cl∈C |φ′ ∈ cl|
(4)
",3.4 Statistical features,[0],[0]
Where f is a given feature.,3.4 Statistical features,[0],[0]
φ and φ′ are observations (categories) of the feature f .,3.4 Statistical features,[0],[0]
C is the set of clusters.,3.4 Statistical features,[0],[0]
This feature is used to calculate the sentence pertinence depending on its terms.,3.4.1 Unigram term frequency,[0],[0]
Each term is considered as a category.,3.4.1 Unigram term frequency,[0],[0]
"This feature is similar to unigram term frequency, but instead of one term we use two consecutive terms.",3.4.2 Bigram term frequency,[0],[0]
We want to use sentence positions in the original texts as a feature.,3.4.3 Sentence position,[0],[0]
"The position feature used by Osborne (2002) divides the sentences into three
sets: the ones in the 8 first paragraphs, those in last 3 paragraphs and the others in between.",3.4.3 Sentence position,[0],[0]
"Following the assumption that the first sentences and last ones are more important than the others.
",3.4.3 Sentence position,[0],[0]
Three categories of sentence positions seem very small to express the diversity between the clusters.,3.4.3 Sentence position,[0],[0]
"Instead of just three categories, we divided the position space into 10 categories.",3.4.3 Sentence position,[0],[0]
"So, if we have 20 sentences, we will have 2 sentences per category.",3.4.3 Sentence position,[0],[0]
"One other feature applied in our system is the sentence length (number of words), which is used originally to penalize the short sentences.",3.4.4 Sentence length,[0],[0]
"Following a sentence’s length, we can put it in one of three categories: sentences with length less than 6 words, those with length more than 20 words, and those with length in between Osborne (2002).
",3.4.4 Sentence length,[0],[0]
"Like sentence position, three categories is a small number.",3.4.4 Sentence length,[0],[0]
"Therefore, we used each length as a category.",3.4.4 Sentence length,[0],[0]
"Suppose we have 4 sentences which the lengths are: 5, 6, 5 and 7, then we will have 3 categories of lengths: 5, 6 and 7.
",3.4.4 Sentence length,[0],[0]
"In our work, we use two types of sentence length:
• Real length (RLeng): which is the length of the sentence without removing stop-words.
",3.4.4 Sentence length,[0],[0]
• Pre-processed length (PLeng): which is the length of the sentence after pre-processing.,3.4.4 Sentence length,[0],[0]
"To extract sentences, we reorder them decreasingly using their scores.",3.5 Summary extraction,[0],[0]
Then we extract the first non similar sentences until we get the wanted size (see algorithm 2).,3.5 Summary extraction,[0],[0]
"In this section, we describe how the summarization parameters have been chosen.
",4 Summarization parameters,[0],[0]
"The first parameter is the clustering threshold, which will lead to few huge clusters if it is small, and inversely.",4 Summarization parameters,[0],[0]
The clustering threshold is used with sentences’ similarities to decide if two sentences are similar or not.,4 Summarization parameters,[0],[0]
Our idea is to use statistic measures over those similarities to estimate the clustering threshold.,4 Summarization parameters,[0],[0]
"Eight measures have been used:
• The median
Algorithm 2: extraction method Data: input text Result: a summary add the first sentence to the summary; foreach sentence in the text do
calculate cosine similarity between this sentence and the last accepted one; if the simularity is under the threshold then
add this sentence to the summary; end if the sum of the summary size and the current sentence’s is above the maximum size then
delete this sentence from the summary;
end end
•",4 Summarization parameters,[0],[0]
The mean •,4 Summarization parameters,[0],[0]
"The mode which can be divided to two: lower
mode and higher mode, since we can have many modes.
",4 Summarization parameters,[0],[0]
"• The variance • sDn = ∑ |s|
|D|∗n
• Dsn = |D| n∗ ∑ |s|
• Ds = |D|∑ |s|",4 Summarization parameters,[0],[0]
"Where, |s| is the number of different terms in a sentence s. |D| is the number of different terms in the document D. n is the number of sentences in this document.
",4 Summarization parameters,[0],[0]
"The second parameter is the features’ set, which is the combination of at least one of the five features described in section 3.4.",4 Summarization parameters,[0],[0]
"We want to know which features are useful and which are not for a given language.
",4 Summarization parameters,[0],[0]
"To fix the problem of the clustering threshold and the set of features, we used the training sets provided by the workshop organizers.",4 Summarization parameters,[0],[0]
"For each document (or topic in multi-document), we generated summaries using the 8 measures of th, and different combinations of the scoring features.",4 Summarization parameters,[0],[0]
"Then, we calculated the average ROUGE-2 score for each language.",4 Summarization parameters,[0],[0]
"The threshold measure and the set of features that maximize this average will be used as parameters for the trained language.
",4 Summarization parameters,[0],[0]
Table 2 represents an example of the 10 languages and their parameters used for both tasks: MSS and MMS.,4 Summarization parameters,[0],[0]
We have to point out that the average is not always the best choice for the individual documents (or topic in multi-document).,4 Summarization parameters,[0],[0]
"For example, in MSS, there is a document which gives a ROUGE-2 score of 0.28 when we use the parameters based on average scores.",4 Summarization parameters,[0],[0]
"When we use the mean as threshold and just TFB as feature for the same document, we get a ROUGE-2 score of 0.31.",4 Summarization parameters,[0],[0]
"We participated in all workshop’s languages, either in single document or multi-document tasks.",5 Experiments,[0],[0]
"To compare our system to others participated systems, we followed these steps (for every evaluation metric):
• For each system, calculate the average scores of all used languages.
",5 Experiments,[0],[0]
"• For our system, calculate the average scores of used languages by others.",5 Experiments,[0],[0]
"For example, BGU-SCE-M team uses Arabic, English and Hebrew; We calculate the average of scores of these languages for this system and ours.
",5 Experiments,[0],[0]
"• Then, we calculate the relative improvement using the averages oursystem−othersystemothersystem .",5 Experiments,[0],[0]
"In “Single document summarization” task, ROUGE (Recall-Oriented Understudy for Gisting Evaluation) (Lin, 2004) is used to evaluate the participated systems.",5.1 Evaluation metrics,[0],[0]
It allows us to evaluate automatic text summaries against human made abstracts.,5.1 Evaluation metrics,[0],[0]
The principle of this method is to compare N-grams of two summaries based on the number of matches between these two based on the recall measure.,5.1 Evaluation metrics,[0],[0]
"Five metrics are used: ROUGE-1, ROUGE-2, ROUGE-3, ROUGE-4 and ROUGE-SU4.
",5.1 Evaluation metrics,[0],[0]
"In “Multi-document summarization” task, Three metrics are officially used: AutoSummENG, MeMoG (Giannakopoulos and Karkaletsis, 2011) and NPowER (Giannakopoulos and Karkaletsis, 2013).",5.1 Evaluation metrics,[0],[0]
"Besides our system (AllSummarizer), there are two more systems which participated in all 38 languages (EXB and CCS).",5.2 Single document summarization,[0],[0]
"Table 3 shows the comparison between our system and the other systems
in single document task, using the relative improvement.
",5.2 Single document summarization,[0],[0]
"Looking at these results, our system took the fifth place out of seven participants.",5.2 Single document summarization,[0],[0]
It outperforms the Lead baseline.,5.2 Single document summarization,[0],[0]
It took the last place out of three participants in all 38 languages.,5.2 Single document summarization,[0],[0]
"Besides our system (AllSummarizer), there are 4 systems that participated with all the 10 languages.",5.3 Multi-document summarization,[0],[0]
"Table 4 shows a comparison between our system and the other systems in multi-document task, using the relative improvement.",5.3 Multi-document summarization,[0],[0]
"We used the parameters fixed for single document summarization to see if the same parameters are applicable for both single and multi-document summarizations.
",5.3 Multi-document summarization,[0],[0]
"Looking to the results, our system took the seventh place out of ten participants.",5.3 Multi-document summarization,[0],[0]
"When we use single document parameters, we can see that it doesn’t outperform the results when using the parameters fixed for multi-document summarization.",5.3 Multi-document summarization,[0],[0]
This shows that we can’t use the same parameters for both single and multi-document summarization.,5.3 Multi-document summarization,[0],[0]
Our intension is to create a method which is language and domain independent.,6 Conclusion,[0],[0]
"So, we consider the input text as a set of topics, where a sentence can belong to many topics.",6 Conclusion,[0],[0]
We calculated how much a sentence can represent all the topics.,6 Conclusion,[0],[0]
"Then, the score is used to reorder the sentences and extract the first non redundant ones.
",6 Conclusion,[0],[0]
"We tested our system using the average score of all languages, in single and multi-document summarization.",6 Conclusion,[0],[0]
"Compared to other systems, it affords fair results, but more improvements have to be done in the future.",6 Conclusion,[0],[0]
We have to point out that our system participated in all languages.,6 Conclusion,[0],[0]
"Also, it is easy to add new languages when you can afford tokenization and stemming.
",6 Conclusion,[0],[0]
We fixed the parameters (threshold and features) based on the average score of ROUGE-2 of all training documents.,6 Conclusion,[0],[0]
Further investigations must be done to estimate these parameters for each document based on statistical criteria.,6 Conclusion,[0],[0]
We want to investigate the effect of the preprocessing step and the clustering methods on the resulted summaries.,6 Conclusion,[0],[0]
"Finally, readability remains a challenge for extractive methods, especially when we want to use a multilingual method.",6 Conclusion,[0],[0]
"In this paper, we evaluate our automatic text summarization system in multilingual context.",abstractText,[0],[0]
We participated in both single document and multi-document summarization tasks of MultiLing 2015 workshop.,abstractText,[0],[0]
Our method involves clustering the document sentences into topics using a fuzzy clustering algorithm.,abstractText,[0],[0]
Then each sentence is scored according to how well it covers the various topics.,abstractText,[0],[0]
"This is done using statistical features such as TF, sentence length, etc.",abstractText,[0],[0]
"Finally, the summary is constructed from the highest scoring sentences, while avoiding overlap between the summary sentences.",abstractText,[0],[0]
"This makes it language-independent, but we have to afford preprocessed data first (tokenization, stemming, etc.).",abstractText,[0],[0]
AllSummarizer system at MultiLing 2015: Multilingual single and multi-document summarization,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 20–25 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-2004
MT evaluation metrics are tested for correlation with human judgments either at the sentence- or the corpus-level. Trained metrics ignore corpus-level judgments and are trained for high sentence-level correlation only. We show that training only for one objective (sentence or corpus level), can not only harm the performance on the other objective, but it can also be suboptimal for the objective being optimized. To this end we present a metric trained for corpus-level and show empirical comparison against a metric trained for sentencelevel exemplifying how their performance may vary per language pair, type and level of judgment. Subsequently we propose a model trained to optimize both objectives simultaneously and show that it is far more stable than–and on average outperforms– both models on both objectives.",text,[0],[0]
"Ever since BLEU (Papineni et al., 2002) many proposals for an improved automatic evaluation metric for Machine Translation (MT) have been made.",1 Introduction,[0],[0]
"Some proposals use additional information for extracting quality indicators, like paraphrasing (Denkowski and Lavie, 2011), syntactic trees (Liu and Gildea, 2005; Stanojević and Sima’an, 2015) or shallow semantics (Rios et al., 2011; Lo et al., 2012) etc.",1 Introduction,[0],[0]
"Whereas others use different matching strategies, like n-grams (Papineni et al., 2002), treelets (Liu and Gildea, 2005) and skip-bigrams (Lin and Och, 2004).",1 Introduction,[0],[0]
"Most metrics use several indicators of translation quality which are often combined in a linear model whose weights are estimated on a training set of human judgments.
",1 Introduction,[0],[0]
"Because the most widely available type of human judgments are relative ranking (RR) judgments, the main machine learning method used for training the metrics were based on the learningto-rank framework (Li, 2011).",1 Introduction,[0],[0]
"While the effectiveness of this framework for training evaluation metrics has been confirmed many times, e.g., (Ye et al., 2007; Duh, 2008; Stanojević and Sima’an, 2014; Ma et al., 2016), so far there is no prior work exploring alternative objective functions for training learning-to-rank models.",1 Introduction,[0],[0]
"Without exception, all existing learning-to-rank models are trained to rank sentences while completely ignoring the corpora judgments, likely because human judgments come in the form of sentence rankings.
",1 Introduction,[0],[0]
It might seem that sentence and corpus level tasks are very similar but that is not the case.,1 Introduction,[0],[0]
Empirically it has been shown that many metrics that perform well on the sentence level do not perform well on the corpus level and vice versa.,1 Introduction,[0],[0]
"By training to rank sentences the model does not necessarily learn to give scores that are well scaled, but only to give higher scores to better translations.",1 Introduction,[0],[0]
"Training for the corpus level score would force the metric to give well scaled scores on the sentence level.
",1 Introduction,[0],[0]
Human judgments of sentences can be aggregated in different ways to hypothesize human judgments of full corpora.,1 Introduction,[0],[0]
"However, this fact has not been used so far to train learning-to-rank models that are good for ranking different corpora.
",1 Introduction,[0],[0]
This work fills-in this gap by exploring the merits of different objective functions that take corpus level judgments into consideration.,1 Introduction,[0],[0]
We first create a learning-to-rank model for ranking corpora and compare it to the standard learning-to-rank model that is trained for ranking sentences.,1 Introduction,[0],[0]
This comparison shows that performance of these two objectives can vary radically depending on the chosen meta-evaluation method.,1 Introduction,[0],[0]
"To tackle this prob-
20
lem we contribute a new objective function, inspired by multi-task learning, in which we train for both objectives simultaneously.",1 Introduction,[0],[0]
This multiobjective model behaves a lot more stable over all methods of meta-evaluation and achieves a higher correlation than both single objective models.,1 Introduction,[0],[0]
"All the models that we define have one basic function in common, we call it a forward(·) function, that maps the features of any sentence to a single real number.",2 Models,[0],[0]
"That function can be any differentiable function including multi-layer neural networks as in (Ma et al., 2016), but here we will stick with the standard linear model:
forward(φ) = φTw + b
Here φ is a vector with feature values of a sentence, w is a weight vector and b is a bias term.",2 Models,[0],[0]
"Usually in training we would like to process a mini-batch of feature vectors Φ, where Φ is a matrix in which each column is a feature vector of individual sentence in the mini-batch or in the corpus.",2 Models,[0],[0]
"By using broadcasting we can rewrite the previous definition of the forward(·) function as:
forward(Φ) = ΦTw + b
Now we can define the score of a sentence as a sigmoid function applied over the output of the forward(·) function because we want to get a score between 0 and 1:
sentScore(φ) = σ(forward(φ))
",2 Models,[0],[0]
"As the corpus level score we will use just the average of sentence level scores:
corpScore(Φ) = 1
m
∑ sentScore(Φ)
where m is the number of sentences in the corpus.",2 Models,[0],[0]
Next we present several objective functions that are illustrated by the computation graph in Figure 1.,2 Models,[0],[0]
"Here we use the training objective very similar to BEER (Stanojević and Sima’an, 2014) which is a learning-to-rank framework that finds a separating hyper-plane between “good” and “bad” translations.",2.1 Training for Sentence Level Accuracy,[0],[0]
"Unlike BEER, we use a max-margin objective instead of logistic regression.
",2.1 Training for Sentence Level Accuracy,[0],[0]
For each mini-batch we randomly select m human relative ranking pairwise judgments and after extracting features for all the sentences taking part in these judgments we put features in two matrices Φswin and Φslos.,2.1 Training for Sentence Level Accuracy,[0],[0]
"These matrices are structured in such a way that for judgment i the column i in Φswin contains the features of the “good” translation in the judgment and the column i in Φslos the features of the “bad” translation.
",2.1 Training for Sentence Level Accuracy,[0],[0]
We would like to maximize the average margin that would separate sentence level scores of pairs of translations in each judgment.,2.1 Training for Sentence Level Accuracy,[0],[0]
"Because the squashing sigmoid function does not influence the ranking we can directly optimize on the unsquashed forward pass and require that the margin between “good” and “bad” translation is at least 1:
∆sent = forward(Φswin)− forward(Φslos)
",2.1 Training for Sentence Level Accuracy,[0],[0]
"LossSent = 1
m
∑ max(0, 1−∆sent)",2.1 Training for Sentence Level Accuracy,[0],[0]
At the corpus level we would like to do a similar thing as on the sentence level: maximize the distance between the scores of “good” and “bad” corpora.,2.2 Training for Corpus Level Accuracy,[0],[0]
"In this case we have additional information that is not present on the sentence level: we know not only which corpus is (according to humans) better, but also by how much it is better.",2.2 Training for Corpus Level Accuracy,[0],[0]
"For
that we can use one of the heuristics such as the Expected Wins (Koehn, 2012).",2.2 Training for Corpus Level Accuracy,[0],[0]
"We can use this information to guide the learning model by how much it should separate the scores of two corpora.
",2.2 Training for Corpus Level Accuracy,[0],[0]
"For doing this we use an approach similar to Max-Margin Markov Networks (Taskar et al., 2003) where for each training instance we dynamically scale the margin that should be enforced.",2.2 Training for Corpus Level Accuracy,[0],[0]
We want the margin between the scores ∆corp to be at least as big as the margin between the human scores ∆human assigned to these systems.,2.2 Training for Corpus Level Accuracy,[0],[0]
In one mini-batch we will use only a randomly chosen pair of corpora with feature matrices Φcwin and Φclos for which we have a human comparison.,2.2 Training for Corpus Level Accuracy,[0],[0]
"The corpus level loss function is given by:
∆corp = corpScore(Φcwin)− corpScore(Φclos) LossCorp = max(0,∆human −∆corp)",2.2 Training for Corpus Level Accuracy,[0],[0]
"In this model we optimize both objectives jointly in the style of multi-task learning (Caruana, 1997).",2.3 Training Jointly for Sentence and Corpus Level Accuracy,[0],[0]
"Here we employ the simplest approach of just tasking the interpolation of the previously introduced loss functions.
",2.3 Training Jointly for Sentence and Corpus Level Accuracy,[0],[0]
LossJoint = α · LossSent + (1− α) ·,2.3 Training Jointly for Sentence and Corpus Level Accuracy,[0],[0]
"LossCorp
The interpolation is controlled by the hyperparameter α which could in principle be tuned for good performance, but here we just fix it to 0.5 to give both objectives equal importance.",2.3 Training Jointly for Sentence and Corpus Level Accuracy,[0],[0]
The feature functions that are used are reimplementation of many (but not all) feature functions of BEER.,2.4 Feature Functions,[0],[0]
"Because the point of this paper is about the exploration of different objective functions we did not try to experiment with more complex feature functions based on paraphrasing, function words or permutation trees.
",2.4 Feature Functions,[0],[0]
"We use just simple precision, recall and 3 types of F-score (with β parameters 1, 2 and 0.5) over different “pieces” of translation:
• character n-grams of orders 1,2,3,4 and 5 • word n-grams of orders 1,2,3 and 4 • skip-bigrams of maximum skip 2 and ∞
(similar to ROUGE-S2 and ROUGE-S* (Lin and Och, 2004))
",2.4 Feature Functions,[0],[0]
One final feature deals with length-disbalance.,2.4 Feature Functions,[0],[0]
"If the length of the system and reference translation are a and b respectively then this feature is computed as max(a,b)−min(a,b)min(a,b) .",2.4 Feature Functions,[0],[0]
It is computed both for word and character length.,2.4 Feature Functions,[0],[0]
"Experiments are conducted on WMT13 (Macháček and Bojar, 2013), WMT14 (Machacek and Bojar, 2014) and WMT16 (Bojar et al., 2016) datasets which were used as training, validation and testing datasets respectively.
",3 Experiments,[0],[0]
All of the models are implemented using TensorFlow1 and trained with L2 regularization λ = 0.001 and ADAM optimizer with learning rate 0.001.,3 Experiments,[0],[0]
The mini-batch size for sentence level judgments is 2000 and for the corpus level is one comparison.,3 Experiments,[0],[0]
"Each model is trained for 200 epochs out of which the one performing best on the validation set for the objective function being optimized is used during the test time.
",3 Experiments,[0],[0]
We show the results for the relative ranking (RR) judgments correlation in Table 1.,3 Experiments,[0],[0]
"For all language pairs that are of the form en-X we show it under the column X and for all the language pairs that have English on the target side we present their average under the column en.
",3 Experiments,[0],[0]
"RR corpus vs. sentence objective The corpusobjective is better than the sentence-objective for both corpus and sentence level RR judgments on 5 out of 7 languages and also on average correlation.
",3 Experiments,[0],[0]
"RR joint vs. single-objectives Training for the joint objective improves even more on both levels of RR correlation and outperforms both singleobjective models on average and on 4 out of 7 languages.
",3 Experiments,[0],[0]
"Making confident conclusions from these results is difficult because, to the best of our knowledge, there is no principled way of measuring statistical significance on the RR judgments.",3 Experiments,[0],[0]
That is why we also tested on direct assessment (DA) judgments available from WMT16.,3 Experiments,[0],[0]
"On DA we can measure statistical significance on the sentence level using Williams test (Graham et al., 2015) and on the corpus level using combination of hybrid-supersampling and Williams test (Graham and Liu, 2016).",3 Experiments,[0],[0]
"The results of correlation with human judgment are for sentence and corpus level are shown in Table 2.
1https://www.tensorflow.org/
DA corpus vs. other objectives On DA judgments the results for corpus level objective are completely different than on the RR judgments.",3 Experiments,[0],[0]
"On DA judgments the corpus-objective model is significantly outperformed on both levels and on all languages by both of the other objectives.
",3 Experiments,[0],[0]
This shows that gambling on one objective function (being that sentence or corpus level objective) could give unpredictable results.,3 Experiments,[0],[0]
"This is precisely the motivation for creating the joint model with multi-objective training.
",3 Experiments,[0],[0]
DA joint vs. single objectives By choosing to jointly optimize both objectives we get a much more stable model that performs well both on DA and RR judgments and on both levels of judgment.,3 Experiments,[0],[0]
"On the DA sentence level, the joint model was not outperformed by any other model and on 3 out of 7 language pairs it significantly outperforms both alternative objectives.",3 Experiments,[0],[0]
"On the corpus level results are
a bit mixed, but still joint objective outperforms both other models on 4 out of 7 language pairs and also it gives higher correlation on average.",3 Experiments,[0],[0]
In this work we found that altering the objective function for training MT metrics can have radical effects on performance.,4 Conclusion,[0],[0]
Also the effects of the objective functions can sometimes be unexpected: the sentence objective might not be good for sentence level correlation (in case of RR judgments) and the corpus objective might not be good for corpus level correlation (in case of DA judgments).,4 Conclusion,[0],[0]
"The difference among objectives is better explained by different types of human judgments: the corpus objective is better for RR while sentence objective is better for DA judgments.
",4 Conclusion,[0],[0]
"Finally, the best results are achieved by training for both objectives at the same time.",4 Conclusion,[0],[0]
"This gives
an evaluation metric that is far more stable in its performance over all methods of meta-evaluation.",4 Conclusion,[0],[0]
"This work is supported by NWO VICI grant nr. 277-89-002, DatAptor project STW grant nr. 12271 and QT21 project H2020 nr. 645452.",Acknowledgments,[0],[0]
MT evaluation metrics are tested for correlation with human judgments either at the sentenceor the corpus-level.,abstractText,[0],[0]
Trained metrics ignore corpus-level judgments and are trained for high sentence-level correlation only.,abstractText,[0],[0]
"We show that training only for one objective (sentence or corpus level), can not only harm the performance on the other objective, but it can also be suboptimal for the objective being optimized.",abstractText,[0],[0]
"To this end we present a metric trained for corpus-level and show empirical comparison against a metric trained for sentencelevel exemplifying how their performance may vary per language pair, type and level of judgment.",abstractText,[0],[0]
Subsequently we propose a model trained to optimize both objectives simultaneously and show that it is far more stable than–and on average outperforms– both models on both objectives.,abstractText,[0],[0]
Alternative Objective Functions for Training MT Evaluation Metrics,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1831–1841 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1831",text,[0],[0]
"Over the past few years, Abstract Meaning Representations (AMRs, Banarescu et al. (2013)) have become a popular target representation for semantic parsing.",1 Introduction,[0],[0]
AMRs are graphs which describe the predicate-argument structure of a sentence.,1 Introduction,[0],[0]
"Because they are graphs and not trees, they can capture reentrant semantic relations, such as those induced by control verbs and coordination.",1 Introduction,[0],[0]
"However, it is technically much more challenging to parse a string into a graph than into a tree.",1 Introduction,[0],[0]
"For instance, grammar-based approaches (Peng et al., 2015; Artzi et al., 2015) require the induction of a grammar from the training corpus, which is hard because graphs can be decomposed into smaller pieces in far more ways than trees.",1 Introduction,[0],[0]
"Neural sequence-to-sequence models, which do very well on string-to-tree parsing (Vinyals et al., 2014), can be applied to AMRs but face the challenge that graphs cannot easily be represented as sequences (van Noord and Bos, 2017a,b).
",1 Introduction,[0],[0]
"In this paper, we tackle this challenge by making the compositional structure of the AMR explicit.",1 Introduction,[0],[0]
"As in our previous work, Groschwitz et al. (2017), we view an AMR as consisting of atomic graphs representing the meanings of the individual words,
which were combined compositionally using linguistically motivated operations for combining a head with its arguments and modifiers.",1 Introduction,[0],[0]
We represent this structure as terms over the AM algebra as defined in Groschwitz et al. (2017).,1 Introduction,[0],[0]
"This previous work had no parser; here we show that the terms of the AM algebra can be viewed as dependency trees over the string, and we train a dependency parser to map strings into such trees, which we then evaluate into AMRs in a postprocessing step.",1 Introduction,[0],[0]
"The dependency parser relies on type information, which encodes the semantic valencies of the atomic graphs, to guide its decisions.
",1 Introduction,[0],[0]
"More specifically, we combine a neural supertagger for identifying the elementary graphs for the individual words with a neural dependency model along the lines of Kiperwasser and Goldberg (2016) for identifying the operations of the algebra.",1 Introduction,[0],[0]
One key challenge is that the resulting term of the AM algebra must be semantically well-typed.,1 Introduction,[0],[0]
This makes the decoding problem NP-complete.,1 Introduction,[0],[0]
"We present two approximation algorithms: one which takes the unlabeled dependency tree as given, and one which assumes that all dependencies are projective.",1 Introduction,[0],[0]
"We evaluate on two data sets, achieving state-of-the-art results on one and near state-of-theart results on the other (Smatch f-scores of 71.0 and 70.2 respectively).",1 Introduction,[0],[0]
"Our approach clearly outperforms strong but non-compositional baselines.
",1 Introduction,[0],[0]
Plan of the paper.,1 Introduction,[0],[0]
"After reviewing related work in Section 2, we explain the AM algebra in Section 3 and extend it to a dependency view in Section 4.",1 Introduction,[0],[0]
We explain model training in Section 5 and decoding in Section 6.,1 Introduction,[0],[0]
Section 7 evaluates a number of variants of our system.,1 Introduction,[0],[0]
"Recently, AMR parsing has generated considerable research activity, due to the availability of large-
scale annotated data (Banarescu et al., 2013) and two successful SemEval Challenges (May, 2016; May and Priyadarshi, 2017).
",2 Related Work,[0],[0]
Methods from dependency parsing have been shown to be very successful for AMR parsing.,2 Related Work,[0],[0]
"For instance, the JAMR parser (Flanigan et al., 2014, 2016) distinguishes concept identification (assigning graph fragments to words) from relation identification (adding graph edges which connect these fragments), and solves the former with a supertagging-style method and the latter with a graph-based dependency parser.",2 Related Work,[0],[0]
"Foland and Martin (2017) use a variant of this method based on an intricate neural model, yielding state-of-the-art results.",2 Related Work,[0],[0]
"We go beyond these approaches by explicitly modeling the compositional structure of the AMR, which allows the dependency parser to combine AMRs for the words using a small set of universal operations, guided by the types of these AMRs.
",2 Related Work,[0],[0]
"Other recent methods directly implement a dependency parser for AMRs, e.g. the transitionbased model of Damonte et al. (2017), or postprocess the output of a dependency parser by adding missing edges (Du et al., 2014; Wang et al., 2015).",2 Related Work,[0],[0]
"In contrast to these, our model makes no strong assumptions on the dependency parsing algorithm that is used; here we choose that of Kiperwasser and Goldberg (2016).
",2 Related Work,[0],[0]
"The commitment of our parser to derive AMRs compositionally mirrors that of grammar-based AMR parsers (Artzi et al., 2015; Peng et al., 2015).",2 Related Work,[0],[0]
"In particular, there are parallels between the types we use in the AM algebra and CCG categories (see Section 3 for details).",2 Related Work,[0],[0]
"As a neural system, our parser struggles less with coverage issues than these, and avoids the complex grammar induction process these models require.
",2 Related Work,[0],[0]
"More generally, our use of semantic types to restrict our parser is reminiscent of Kwiatkowski et al. (2010), Krishnamurthy et al. (2017) and Zhang et al. (2017), and the idea of deriving semantic representations from dependency trees is also present in Reddy et al. (2017).",2 Related Work,[0],[0]
"A core idea of this paper is to parse a string into a graph by instead parsing a string into a dependencystyle tree representation of the graph’s compositional structure, represented as terms of the ApplyModify (AM) algebra (Groschwitz et al., 2017).
",3 The AM algebra,[0],[0]
"The values of the AM algebra are annotated s-
graphs, or as-graphs: directed graphs with node and edge labels in which certain nodes have been designated as sources (Courcelle and Engelfriet, 2012) and annotated with type information.",3 The AM algebra,[0],[0]
Some examples of as-graphs are shown in Fig. 1.,3 The AM algebra,[0],[0]
"Each as-graph has exactly one root, indicated by the bold outline.",3 The AM algebra,[0],[0]
"The sources are indicated by red labels; for instance, Gwant has an S-source and an O-source.",3 The AM algebra,[0],[0]
"The annotations, written in square brackets behind the red source names, will be explained below.",3 The AM algebra,[0],[0]
"We use these sources to mark open argument slots; for example, Gsleep in Fig. 1 represents an intransitive verb, missing its subject, which will be added at the S-source.
",3 The AM algebra,[0],[0]
The AM algebra can combine as-graphs with each other using two linguistically motivated operations: apply and modify.,3 The AM algebra,[0],[0]
Apply (APP) adds an argument to a predicate.,3 The AM algebra,[0],[0]
"For example, we can add a subject – the graph Gwriter in Fig. 1 – to the graph GVP in Fig. 2d using APPS, yielding the complete AMR in Fig. 2b.",3 The AM algebra,[0],[0]
"Linguistically, this is like filling the subject (S) slot of the predicate wants to sleep soundly with the argument the writer.",3 The AM algebra,[0],[0]
"In general, for a source a, APPa(GP , GA), combines the asgraph GP representing a predicate, or head, with the as-graph GA, which represents an argument.",3 The AM algebra,[0],[0]
"It does this by plugging the root node of GA into the a-source u of GP – that is, the node u of GP marked with source a.",3 The AM algebra,[0],[0]
"The root of the resulting as-graph G is the root of GP , and we remove the a marking on u, since that slot is now filled.
",3 The AM algebra,[0],[0]
The modify operation (MOD) adds a modifier to a graph.,3 The AM algebra,[0],[0]
"For example, we can combine two elementary graphs from Fig. 1 with MODm (Gsleep, Gsound), yielding the graph in Fig. 2c.",3 The AM algebra,[0],[0]
The Msource of the modifier Gsoundly attaches to the root of Gsleep.,3 The AM algebra,[0],[0]
The root of the result is the same as the root of Gsleep in the same sense that a verb phrase with an adverb modifier is still a verb phrase.,3 The AM algebra,[0],[0]
"In general, MODa(GH , GM ), combines a head GH with a modifier GM .",3 The AM algebra,[0],[0]
It plugs the root of GH into the a-source u of GM .,3 The AM algebra,[0],[0]
"Although this may add incoming edges to the root of GH , that node is still
the root of the resulting graph G. We remove the a marking from GM .
",3 The AM algebra,[0],[0]
"In both APP and MOD, if there is any other source b which is present in both graphs, the nodes marked with b are unified with each other.",3 The AM algebra,[0],[0]
"For example, when Gwant is O-applied to t1 in Fig. 2d, the S-sources of the graphs for “want” and “sleep soundly” are unified into a single node, creating a reentrancy.",3 The AM algebra,[0],[0]
"This falls out of the definition of merge for s-graphs which formally underlies both operations (see (Courcelle and Engelfriet, 2012)).
",3 The AM algebra,[0],[0]
"Finally, the AM algebra uses types to restrict its operations.",3 The AM algebra,[0],[0]
"Here we define the type of an as-graph as the set of its sources with their annotations1; thus for example, in Fig. 1, the graph for “writer” has the empty type [ ],Gsleep has type [S], andGwant has type [S, O[S]].",3 The AM algebra,[0],[0]
Each source in an as-graph specifies with its annotation the type of the as-graph which is plugged into it via APP.,3 The AM algebra,[0],[0]
"In other words, for a source a, we may only a-apply GP with GA if the annotation of the a-source in GP matches the type of GA.",3 The AM algebra,[0],[0]
"For example, the O-source of Gwants (Fig. 1) requires that we plug in an as-graph of type [S]; observe that this means that the reentrancy in Fig.",3 The AM algebra,[0],[0]
2b is lexically specified by the control verb “want”.,3 The AM algebra,[0],[0]
"All other source nodes in Fig. 1 have no annotation, indicating a type requirement of [ ].
",3 The AM algebra,[0],[0]
"Linguistically, modification is optional; we therefore want the modified graph to be derivationally just like the unmodified graph, in that exactly the same operations can apply to it.",3 The AM algebra,[0],[0]
"In a typed algebra, this means MOD should not change the type of the head.",3 The AM algebra,[0],[0]
"MODa therefore requires that the modifier GM have no sources not already present in the head GH , except a, which will be deleted anyway.
",3 The AM algebra,[0],[0]
"As in any algebra, we can build terms from constants (denoting elementary as-graphs) by recursively combining them with the operations of the AM algebra.",3 The AM algebra,[0],[0]
"By evaluating the operations bottomup, we obtain an as-graph as the value of such a term; see Fig. 2 for an example.",3 The AM algebra,[0],[0]
"However, as discussed above, an operation in the term may be undefined due to a type mismatch.",3 The AM algebra,[0],[0]
We call an AMterm well-typed if all its operations are defined.,3 The AM algebra,[0],[0]
Every well-typed AM-term evaluates to an as-graph.,3 The AM algebra,[0],[0]
"Since the applicability of an AM operation depends only on the types, we also write τ = f(τ1, τ2) if as-graphs of type τ1 and τ2 can be combined with the operation f and the result has type τ .
1See (Groschwitz et al., 2017) for a more formally complete definition.
",3 The AM algebra,[0],[0]
Relationship to CCG.,3 The AM algebra,[0],[0]
There is close relationship between the types of the AM algebra and the categories of CCG.,3 The AM algebra,[0],[0]
"A type [S, O] specifies that the as-graph needs to be applied to two arguments to be semantically complete, similar a CCG category such as S\NP/NP, where a string needs to be applied to two NP arguments to be syntactically complete.",3 The AM algebra,[0],[0]
"However, AM types govern the combination of graphs, while CCG categories control the combination of strings.",3 The AM algebra,[0],[0]
"This relieves AM types of the need to talk about word order; there are no “forward” or “backward” slashes in AM types, and a smaller set of operations.",3 The AM algebra,[0],[0]
"Also, the AM algebra spells out raising and control phenomena more explicitly in the types.",3 The AM algebra,[0],[0]
"In this paper, we connect AM terms to the input string w for which we want to produce a graph.",4 Indexed AM terms,[0],[0]
"We do this in an indexed AM term, exemplified in Fig. 3a.",4 Indexed AM terms,[0],[0]
"We assume that every elementary as-graph G at a leaf represents the meaning of an individual word token wi in w, and write G[i] to annotate the leaf G with the index i of this token.",4 Indexed AM terms,[0],[0]
"This induces a connection between the nodes of the AMR and the tokens of the string, in that the label of each node was contributed by the elementary as-graph of exactly one token.
",4 Indexed AM terms,[0],[0]
We define the head index of a subtree t to be the index of the token which contributed the root of the as-graph to which t evaluates.,4 Indexed AM terms,[0],[0]
"For a leaf with annotation i, the head index is i; for an APP or MOD node, the head index is the head index of the left child, i.e. of the head argument.",4 Indexed AM terms,[0],[0]
We annotate each APP and MOD operation with the head index of the left and right subtree.,4 Indexed AM terms,[0],[0]
"We can represent indexed AM terms more compactly as AM dependency trees, as shown in Fig.",4.1 AM dependency trees,[0],[0]
3b.,4.1 AM dependency trees,[0],[0]
The nodes of such a dependency tree are the tokens of w. We draw an edge with label f from i to k if there is a node with label f,4.1 AM dependency trees,[0],[0]
"[i, k] in the indexed AM term.",4.1 AM dependency trees,[0],[0]
"For example, the tree in 3b has an edge labeled MODm from 5 (Gsleep) to 6 (Gsoundly) because there is a node in the term in 3a labeled MODm[5, 6].",4.1 AM dependency trees,[0],[0]
"The same AM dependency tree may represent multiple indexed AM terms, because the order of apply and modify operations is not specified in the dependency tree.",4.1 AM dependency trees,[0],[0]
"However, it can be shown that all well-typed AM terms that map to
APPs[3,2]
Gwant[3]
APPo[3,5]
MODm[5,6]
Gsleep[5] Gsoundly[6]
Gwriter[2]
(a)
2: Gwriter
6: Gsoundly
4: ⊥5: Gsleep
AP P s AP P o
IGNORE
M O
D m
(b)
1: ⊥
IGNORE
3: Gwant
Figure 3: (a) An indexed AM term and (b) an AM dependency tree, linking the term in Fig.",4.1 AM dependency trees,[0],[0]
"2;a to the sentence “The writer wants to sleep soundly”.
",4.1 AM dependency trees,[0],[0]
the same AM dependency tree evaluate to the same as-graph.,4.1 AM dependency trees,[0],[0]
"We define a well-typed AM dependency tree as one that represents a well-typed AM term.
",4.1 AM dependency trees,[0],[0]
"Because not all words in the sentence contribute to the AMR, we include a mechanism for ignoring words in the input.",4.1 AM dependency trees,[0],[0]
"As a special case, we allow the constant ⊥, which represents a dummy as-graph (of type ⊥) which we use as the semantic value of words without a semantic value in the AMR.",4.1 AM dependency trees,[0],[0]
"We furthermore allow the edge label IGNORE in an AM dependency tree, where IGNORE(τ1, τ2) = τ1 if τ2 = ⊥ and is undefined otherwise; in particular, an AM dependency tree with IGNORE edges is only well-typed if all IGNORE edges point into ⊥ nodes.",4.1 AM dependency trees,[0],[0]
"We keep all other operations f(τ1, τ2) as is, i.e. they are undefined if either τ1 or τ2 is⊥, and never yield ⊥ as a result.",4.1 AM dependency trees,[0],[0]
"When reconstructing an AM term from the AM dependency tree, we skip IGNORE edges, such that the subtree below them will not contribute to the overall AMR.",4.1 AM dependency trees,[0],[0]
"In order to train a model that parses sentences into AM dependency trees, we need to convert an AMR corpus – in which sentences are annotated with AMRs – into a treebank of AM dependency trees.",4.2 Converting AMRs to AM terms,[0],[0]
"We do this in three steps: first, we break each AMR up into elementary graphs and identify their roots; second, we assign sources and annotations to make elementary as-graphs out of them; and third, combine them into indexed AM terms.
",4.2 Converting AMRs to AM terms,[0],[0]
"For the first step, an aligner uses hand-written heuristics to identify the string token to which each
node in the AMR corresponds (see Section C in the Supplementary Materials for details).",4.2 Converting AMRs to AM terms,[0],[0]
"We proceed in a similar fashion as the JAMR aligner (Flanigan et al., 2014), i.e. by starting from high-confidence token-node pairs and then extending them until the whole AMR is covered.",4.2 Converting AMRs to AM terms,[0],[0]
"Unlike the JAMR aligner, our heuristics ensure that exactly one node in each elementary graph is marked as the root, i.e. as the node where other graphs can attach their edges through APP and MOD.",4.2 Converting AMRs to AM terms,[0],[0]
"When an edge connects nodes of two different elementary graphs, we use the “blob decomposition” algorithm of Groschwitz et al. (2017) to decide to which elementary graph it belongs.",4.2 Converting AMRs to AM terms,[0],[0]
"For the example AMR in Fig. 2b, we would obtain the graphs in Fig. 1 (without source annotations).",4.2 Converting AMRs to AM terms,[0],[0]
"Note that ARG edges belong with the nodes at which they start, whereas the “manner” edge in Gsoundly goes with its target.
",4.2 Converting AMRs to AM terms,[0],[0]
In the second step we assign source names and annotations to the unlabeled nodes of each elementary graph.,4.2 Converting AMRs to AM terms,[0],[0]
Note that the annotations are crucial to our system’s ability to generate graphs with reentrancies.,4.2 Converting AMRs to AM terms,[0],[0]
"We mostly follow the algorithm of Groschwitz et al. (2017), which determines necessary annotations based on the structure of the given graph.",4.2 Converting AMRs to AM terms,[0],[0]
The algorithm chooses each source name depending on the incoming edge label.,4.2 Converting AMRs to AM terms,[0],[0]
"For instance, the two leaves of Gwant can have the source labels S and O because they have incoming edges labeled ARG0 and ARG1.",4.2 Converting AMRs to AM terms,[0],[0]
"However, the Groschwitz algorithm is not deterministic: It allows object promotion (the sources for an ARG3 edge may be O3, O2, or O), unaccusative subjects (promoting the minimal object to S if the elementary graph contains an ARGi-edge (i > 0) but no ARG0-edge (Perlmutter, 1978)), and passive alternation (swapping O and S).",4.2 Converting AMRs to AM terms,[0],[0]
"To make our as-graphs more consistent, we prefer constants that promote objects as far as possible, use unaccusative subjects, and no passive alternation, but still allow constants that do not satisfy these conditions if necessary.",4.2 Converting AMRs to AM terms,[0],[0]
"This increased our Smatch score significantly.
",4.2 Converting AMRs to AM terms,[0],[0]
"Finally, we choose an arbitrary AM dependency
tree that combines the chosen elementary as-graphs into the annotated AMR; in practice, the differences between the trees seem to be negligible.2",4.2 Converting AMRs to AM terms,[0],[0]
"We can now model the AMR parsing task as the problem of computing the best well-typed AM dependency tree t for a given sentence w. Because t is well-typed, it can be decoded into an (indexed) AM term and thence evaluated to an as-graph.
",5 Training,[0],[0]
We describe t in terms of the elementary asgraphs G[i],5 Training,[0],[0]
it uses for each token i and of its edges,5 Training,[0],[0]
f,5 Training,[0],[0]
"[i, k].",5 Training,[0],[0]
"We assume a node-factored, edge-factored model for the score ω(t) of t:
ω(t) = ∑
1≤i≤n ω(G[i])",5 Training,[0],[0]
+ ∑,5 Training,[0],[0]
f,5 Training,[0],[0]
"[i,k]∈E ω(f",5 Training,[0],[0]
"[i, k]), (1)
where the edge weight further decomposes into the sum ω(f",5 Training,[0],[0]
"[i, k])",5 Training,[0],[0]
= ω(i → k) + ω(f,5 Training,[0],[0]
| i → k) of a score ω(i→ k) for the presence of an edge from i to k and a score ω(f | i→ k) for this edge having label f .,5 Training,[0],[0]
"Our aim is to compute the well-typed t with the highest score.
",5 Training,[0],[0]
We present three models for ω: one for the graph scores and two for the edge scores.,5 Training,[0],[0]
"All of these are based on a two-layer bidirectional LSTM, which reads inputs x =",5 Training,[0],[0]
"(x1, . . .",5 Training,[0],[0]
", xn) token by token, concatenating the hidden states of the forward and the backward LSTMs in each layer.",5 Training,[0],[0]
"On the second layer, we thus obtain vector representations vi = BiLSTM(x, i) for the individual input tokens (see Fig. 4).",5 Training,[0],[0]
Our models differ in the inputs x and the way they predict scores from the vi.,5 Training,[0],[0]
"We construe the prediction of the as-graphs G[i] for each input position i as a supertagging task (Lewis et al., 2016).",5.1 Supertagging for elementary as-graphs,[0],[0]
"The supertagger reads inputs xi = (wi, pi, ci), where wi is the word token, pi its POS tag, and ci is a character-based LSTM encoding of wi.",5.1 Supertagging for elementary as-graphs,[0],[0]
"We use pretrained GloVe embeddings (Pennington et al., 2014) concatenated with learned embeddings for wi, and learned embeddings for pi.
",5.1 Supertagging for elementary as-graphs,[0],[0]
"To predict the score for each elementary as-graph out of a set of K options, we add a K-dimensional output layer as follows:
ω(G[i]) = log softmax(W · vi + b) 2Indeed, we conjecture that for a fixed set of constants and
a fixed AMR, there is only one dependency tree.
and train the neural network using a cross-entropy loss function.",5.1 Supertagging for elementary as-graphs,[0],[0]
This maximizes the likelihood of the elementary as-graphs in the training data.,5.1 Supertagging for elementary as-graphs,[0],[0]
Predicting the edge scores amounts to a dependency parsing problem.,5.2 Kiperwasser & Goldberg edge model,[0],[0]
"We chose the dependency parser of Kiperwasser and Goldberg (2016), henceforth K&G, to learn them, because of its accuracy and its fit with our overall architecture.",5.2 Kiperwasser & Goldberg edge model,[0],[0]
"The K&G parser scores the potential edge from i to k and its label from the concatenations of vi and vk:
MLPθ(v) = W2 · tanh(W1 · v + b1) + b2 ω(i→ k) = MLPE(vi ◦ vk)
ω(f | i→ k) = MLPLBL(vi ◦ vk)
",5.2 Kiperwasser & Goldberg edge model,[0],[0]
"We use inputs xi = (wi, pi, τi) including the type τi of the supertag G[i] at position i, using trained embeddings for all three.",5.2 Kiperwasser & Goldberg edge model,[0],[0]
"At evaluation time, we use the best scoring supertag according to the model of Section 5.1.",5.2 Kiperwasser & Goldberg edge model,[0],[0]
"At training time, we sample from q, where q(τi) =",5.2 Kiperwasser & Goldberg edge model,[0],[0]
(1− δ),5.2 Kiperwasser & Goldberg edge model,[0],[0]
"+ δ · p(τi|pi, pi−1), q(τ) = δ · p(τ |pi, pi−1) for any τ 6= τi",5.2 Kiperwasser & Goldberg edge model,[0],[0]
and δ is a hyperparameter controlling the bias towards the aligned supertag.,5.2 Kiperwasser & Goldberg edge model,[0],[0]
We train the model using K&G’s original DyNet implementation.,5.2 Kiperwasser & Goldberg edge model,[0],[0]
"Their algorithm uses a hinge loss function, which maximizes the score difference between the gold dependency tree and the best predicted dependency tree, and therefore requires parsing each training instance in each iteration.",5.2 Kiperwasser & Goldberg edge model,[0],[0]
"Because the AM dependency trees are highly non-projective, we replaced the projective parser used in the off-the-shelf implementation by the Chu-Liu-Edmonds algorithm implemented in the TurboParser (Martins et al., 2010), improving the LAS on the development set by 30 points.",5.2 Kiperwasser & Goldberg edge model,[0],[0]
"We also trained a local edge score model, which uses a cross-entropy rather than a hinge loss and therefore avoids the repeated parsing at training
time.",5.3 Local edge model,[0],[0]
"Instead, we follow the intuition that every node in a dependency tree has at most one incoming edge, and train the model to score the correct incoming edge as high as possible.",5.3 Local edge model,[0],[0]
"This model takes inputs xi = (wi, pi).
",5.3 Local edge model,[0],[0]
"We define the edge and edge label scores as in Section 5.2, with tanh replaced by ReLU.",5.3 Local edge model,[0],[0]
"We further add a learned parameter v⊥ for the “LSTM embedding” of a nonexistent node, obtaining scores ω(⊥",5.3 Local edge model,[0],[0]
"→ k) for k having no incoming edge.
",5.3 Local edge model,[0],[0]
"To train ω(i → k), we collect all scores for edges ending at the same node k into a vector ω(• → k).",5.3 Local edge model,[0],[0]
"We then minimize the cross-entropy loss for the gold edge into k under softmax(ω(• → k)), maximizing the likelihood of the gold edges.",5.3 Local edge model,[0],[0]
"To train the labels ω(f | i → k), we simply minimize the cross-entropy loss of the actual edge labels f of the edges which are present in the gold AM dependency trees.
",5.3 Local edge model,[0],[0]
The PyTorch code for this and the supertagger are available at bitbucket.org/tclup/ amr-dependency.,5.3 Local edge model,[0],[0]
"Given learned estimates for the graph and edge scores, we now tackle the challenge of computing the best well-typed dependency tree t for the input string w, under the score model (equation (1)).",6 Decoding,[0],[0]
"The requirement that t must be well-typed is crucial to ensure that it can be evaluated to an AMR graph, but as we show in the Supplementary Materials (Section A), makes the decoding problem NP-complete.",6 Decoding,[0],[0]
"Thus, an exact algorithm is not practical.",6 Decoding,[0],[0]
"In this section, we develop two different approximation algorithms for AM dependency parsing: one which assumes the (unlabeled) dependency tree structure as known, and one which assumes that the AM dependency tree is projective.",6 Decoding,[0],[0]
"The projective decoder assumes that the AM dependency tree is projective, i.e. has no crossing dependency edges.",6.1 Projective decoder,[0],[0]
"Because of this assumption, it can recursively combine adjacent substrings using dynamic programming.",6.1 Projective decoder,[0],[0]
"The algorithm is shown in Fig. 5 as a parsing schema (Shieber et al., 1995), which derives items of the form ([i, k], r, τ) with scores s. An item represents a well-typed derivation of the substring from i to k with head index r, and which evaluates to an as-graph of type τ .
",6.1 Projective decoder,[0],[0]
"The parsing schema consists of three types of
rules.",6.1 Projective decoder,[0],[0]
"First, the Init rule generates an item for each graph fragment G[i] that the supertagger predicted for the token wi, along with the score and type of that graph fragment.",6.1 Projective decoder,[0],[0]
"Second, given items for adjacent substrings [i, j] and [j, k], the Arc rules apply an operation f to combine the indexed AM terms for the two substrings, with Arc-R making the left-hand substring the head and the right-hand substring the argument or modifier, and Arc-L the other way around.",6.1 Projective decoder,[0],[0]
We ensure that the result is well-typed by requiring that the types can be combined with f .,6.1 Projective decoder,[0],[0]
"Finally, the Skip rules allow us to extend a substring such that it covers tokens which do not correspond to a graph fragment (i.e., their AM term is ⊥), introducing IGNORE edges.",6.1 Projective decoder,[0],[0]
"After all possible items have been derived, we extract the best well-typed tree from the item of the form ([1, n], r, τ) with the highest score, where τ =",6.1 Projective decoder,[0],[0]
"[ ].
",6.1 Projective decoder,[0],[0]
"Because we keep track of the head indices, the projective decoder is a bilexical parsing algorithm, and shares a parsing complexity of O(n5) with other bilexical algorithms such as the Collins parser.",6.1 Projective decoder,[0],[0]
It could be improved to a complexity of O(n4) using the algorithm of Eisner and Satta (1999).,6.1 Projective decoder,[0],[0]
"The fixed-tree decoder computes the best unlabeled dependency tree tr for w, using the edge scores ω(i→ k), and then computes the best AM dependency tree forw whose unlabeled version is tr.",6.2 Fixed-tree decoder,[0],[0]
"The Chu-Liu-Edmonds algorithm produces a forest of dependency trees, which we want to combine into tr.",6.2 Fixed-tree decoder,[0],[0]
"We choose the tree whose root r has the highest score for being the root of the AM dependency tree and make the roots of all others children of r.
At this point, the shape of tr is fixed.",6.2 Fixed-tree decoder,[0],[0]
"We choose
supertags for the nodes and edge labels for the edges by traversing tr bottom-up, computing types for the subtrees as we go along.",6.2 Fixed-tree decoder,[0],[0]
"Formally, we apply the parsing schema in Fig. 6.",6.2 Fixed-tree decoder,[0],[0]
"It uses items of the form (i, C, τ) : s, where 1 ≤ i ≤ n is a node of tr, C is the set of children of i for which we have already chosen edge labels, and τ is a type.",6.2 Fixed-tree decoder,[0],[0]
"We write Ch(i) for the set of children of i in tr.
",6.2 Fixed-tree decoder,[0],[0]
"The Init rule generates an item for each graph that the supertagger can assign to each token i in w, ensuring that every token is also assigned ⊥ as a possible supertag.",6.2 Fixed-tree decoder,[0],[0]
"The Edge rule labels an edge from a parent node i in tr to one of its children k, whose children already have edge labels.",6.2 Fixed-tree decoder,[0],[0]
"As above, this rule ensures that a well-typed AM dependency tree is generated by locally checking the types.",6.2 Fixed-tree decoder,[0],[0]
"In particular, if all types τ2 that can be derived for k are incompatible with τ1, we fall back to an item for k with τ2 = ⊥ (which always exists), along with an IGNORE edge from i to k.
The complexity of this algorithm is O(n · 2d · d), where d is the maximal arity of the nodes in tr.",6.2 Fixed-tree decoder,[0],[0]
We evaluate our models on the LDC2015E86 and LDC2017T103 datasets (henceforth “2015” and “2017”).,7 Evaluation,[0],[0]
Technical details and hyperparameters of our implementation can be found in Sections B to D of the Supplementary Materials.,7 Evaluation,[0],[0]
The original LDC datasets pair strings with AMRs.,7.1 Training data,[0],[0]
"We convert each AMR in the training and development set into an AM dependency tree, using the procedure of Section 4.2.",7.1 Training data,[0],[0]
About 10% of the training instances cannot be split into elementary as-graphs by our aligner; we removed these from the training data.,7.1 Training data,[0],[0]
"Of the remaining AM dependency trees, 37% are non-projective.
",7.1 Training data,[0],[0]
"Furthermore, the AM algebra is designed to handle short-range reentrancies, modeling grammati-
3https://catalog.ldc.upenn.edu/ LDC2017T10, identical to LDC2016E25.
cal phenomena such as control and coordination, as in the derivation in Fig. 2.",7.1 Training data,[0],[0]
"It cannot easily handle the long-range reentrancies in AMRs which are caused by coreference, a non-compositional phenomenon.4 We remove such reentrancies from our training data (about 60% of the roughly 20,000 reentrant edges).",7.1 Training data,[0],[0]
"Despite this, our model performs well on reentrant edges (see Table 2).",7.1 Training data,[0],[0]
We use simple pre- and postprocessing steps to handle rare words and some AMR-specific patterns.,7.2 Pre- and postprocessing,[0],[0]
"In AMRs, named entities follow a pattern shown in Fig. 7.",7.2 Pre- and postprocessing,[0],[0]
"Here the named entity is of type “person”, has a name edge to a “name” node whose children spell out the tokens of “Agatha Christie”, and a link to a wiki entry.",7.2 Pre- and postprocessing,[0],[0]
"Before training, we replace each “name” node, its children, and the corresponding span in the sentence with a special NAME token, and we completely remove wiki edges.",7.2 Pre- and postprocessing,[0],[0]
"In this example, this leaves us with only a “person” and a NAME node.",7.2 Pre- and postprocessing,[0],[0]
"Further, we replace numbers and some date patterns with NUMBER and DATE tokens.",7.2 Pre- and postprocessing,[0],[0]
"On the training data this is straightforward, since names and dates are explicitly annotated in the AMR.",7.2 Pre- and postprocessing,[0],[0]
"At evaluation time, we detect dates and numbers with regular expressions, and names with Stanford CoreNLP (Manning et al., 2014).",7.2 Pre- and postprocessing,[0],[0]
"We also use Stanford CoreNLP for our POS tags.
",7.2 Pre- and postprocessing,[0],[0]
Each elementary as-graph generated by the procedure of Section 4.2 has a unique node whose label corresponds most closely to the aligned word (e.g. the “want” node in Gwant and the “write” node in Gwriter).,7.2 Pre- and postprocessing,[0],[0]
"We replace these node labels with LEX in preprocessing, reducing the number of different elementary as-graphs from 28730 to 2370.",7.2 Pre- and postprocessing,[0],[0]
"We factor the supertagger model of Section 5.1 such that the unlexicalized version of G[i] and the label for LEX are predicted separately.
",7.2 Pre- and postprocessing,[0],[0]
"At evaluation, we re-lexicalize all LEX nodes in the predicted AMR.",7.2 Pre- and postprocessing,[0],[0]
"For words that were frequent in the training data (at least 10 times), we take the supertagger’s prediction for the label.",7.2 Pre- and postprocessing,[0],[0]
"For rarer words, we use simple heuristics, explained in the Supplementary Materials (Section D).",7.2 Pre- and postprocessing,[0],[0]
"For names, we just look up name nodes with their children and wiki entries observed for the name string in the training data, and for unseen names use the literal tokens as the name, and no wiki entry.",7.2 Pre- and postprocessing,[0],[0]
"Similarly,
4As Damonte et al. (2017) comment: “A valid criticism of AMR is that these two reentrancies are of a completely different type, and should not be collapsed together.”
we collect the type for each encountered name (e.g. “person” for “Agatha Christie”), and correct it in the output if the tagger made a different prediction.",7.2 Pre- and postprocessing,[0],[0]
We recover dates and numbers straightforwardly.,7.2 Pre- and postprocessing,[0],[0]
All of our models rely on the supertagger to predict elementary as-graphs; they differ only in the edge scores.,7.3 Supertagger accuracy,[0],[0]
"We evaluated the accuracy of the supertagger on the converted development set (in which each token has a supertag) of the 2015 data set, and achieved an accuracy of 73%.",7.3 Supertagger accuracy,[0],[0]
"The correct supertag is within the supertagger’s 4 best predictions for 90% of the tokens, and within the 10 best for 95%.
",7.3 Supertagger accuracy,[0],[0]
"Interestingly, supertags that introduce grammatical reentrancies are predicted quite reliably, although they are relatively rare in the training data.",7.3 Supertagger accuracy,[0],[0]
"The elementary as-graph for subject control verbs (see Gwant in Fig. 1) accounts for only 0.8% of supertags in the training data, yet 58% of its occurrences in the development data are predicted correctly (84% in 4-best).",7.3 Supertagger accuracy,[0],[0]
"The supertag for VP coordination (with type [OP1[S], OP2[S]]) makes up for 0.4% of the training data, but 74% of its occurrences are recognized correctly (92% in 4-best).",7.3 Supertagger accuracy,[0],[0]
Thus the prediction of informative types for individual words is feasible.,7.3 Supertagger accuracy,[0],[0]
Type-unaware fixed-tree baseline.,7.4 Comparison to Baselines,[0],[0]
The fixed-tree decoder is built to ensure well-typedness of the predicted AM dependency trees.,7.4 Comparison to Baselines,[0],[0]
"To investigate to what extent this is required, we consider a baseline which just adds the individually highest-scoring supertags and edge labels to the unlabeled dependency tree tu, ignoring types.",7.4 Comparison to Baselines,[0],[0]
This leads to AM dependency trees which are not well-typed for 75% of the sentences (we fall back to the largest welltyped subtree in these cases).,7.4 Comparison to Baselines,[0],[0]
"Thus, an off-theshelf dependency parser can reliably predict the tree structure of the AM dependency tree, but correct supertag and edge label assignment requires a decoder which takes the types into account.
",7.4 Comparison to Baselines,[0],[0]
JAMR-style baseline.,7.4 Comparison to Baselines,[0],[0]
"Our elementary asgraphs differ from the elementary graphs used in JAMR-style algorithms in that they contain explicit source nodes, which restrict the way in which they can be combined with other as-graphs.",7.4 Comparison to Baselines,[0],[0]
We investigate the impact of this choice by implementing a strong JAMR-style baseline.,7.4 Comparison to Baselines,[0],[0]
"We adapt the AMR-todependency conversion of Section 4.2 by removing all unlabeled nodes with source names from the
elementary graphs.",7.4 Comparison to Baselines,[0],[0]
"For instance, the graph Gwant in Fig. 1 now only consists of a single “want” node.",7.4 Comparison to Baselines,[0],[0]
"We then aim to directly predict AMR edges between these graphs, using a variant of the local edge scoring model of Section 5.3 which learns scores for each edge in isolation.",7.4 Comparison to Baselines,[0],[0]
"(The assumption for the original local model, that each node has only one incoming edge, does not apply here.)
",7.4 Comparison to Baselines,[0],[0]
"When parsing a string, we choose the highestscoring supertag for each word; there are only 628 different supertags in this setting, and 1-best supertagging accuracy is high at 88%.",7.4 Comparison to Baselines,[0],[0]
We then follow the JAMR parsing algorithm by predicting all edges whose score is over a threshold (we found -0.02 to be optimal) and then adding edges until the graph is connected.,7.4 Comparison to Baselines,[0],[0]
"Because we do not predict which node is the root of the AMR, we evaluated this model as if it always predicted the root correctly, overestimating its score slightly.",7.4 Comparison to Baselines,[0],[0]
"Table 1 shows the Smatch scores (Cai and Knight, 2013) of our models, compared to a selection of previously published results.",7.5 Results,[0],[0]
Our results are averages over 4 runs with 95% confidence intervals (JAMR-style baselines are single runs).,7.5 Results,[0],[0]
"On the 2015 dataset, our best models (local + projective, K&G + fixed-tree) outperform all previous work, with the exception of the Foland and Martin (2017) model; on the 2017 set we match state of the art results (though note that van Noord and Bos (2017b) use 100k additional sentences of silver data).",7.5 Results,[0],[0]
"The fixed-tree decoder seems to work well with either edge model, but performance of the projective decoder drops with the K&G edge scores.",7.5 Results,[0],[0]
"It may be that, while the hinge loss used in the K&G edge scoring model is useful to finding the correct un-
2015 2017 Metric W’15 F’16 D’17 PD FTD vN’17 PD FTD Smatch 67 67 64 70 70 71 71 70 Unlabeled 69 69 69 73 73 74 74 74 No WSD 64 68 65 71 70 72 72 70 Named Ent.",7.5 Results,[0],[0]
"75 79 83 79 78 79 78 77 Wikification 0 75 64 71 72 65 71 71 Negations 18 45 48 52 52 62 57 55 Concepts 80 83 83 83 84 82 84 84 Reentrancies 41 42 41 46 44 52 49 46 SRL 60 60 56 63 61 66 64 62
Table 2: Details for the LDC2015E86 and LDC2017T10 test sets
Agatha_Christiename
person
na me
wiki
Agatha Christie
op 1 op2
Figure 7: A named entity
labeled dependency tree in the fixed-tree decoder, scores for bad edges – which are never used when computing the hinge loss – are not trained accurately.",7.5 Results,[0],[0]
"Thus such edges may be erroneously used by the projective decoder.
",7.5 Results,[0],[0]
"As expected, the type-unaware baseline has low recall, due to its inability to produce well-typed trees.",7.5 Results,[0],[0]
"The fact that our models outperform the JAMR-style baseline so clearly is an indication that they indeed gain some of their accuracy from the type information in the elementary as-graphs, confirming our hypothesis that an explicit model of the compositional structure of the AMR can help the parser learn an accurate model.
",7.5 Results,[0],[0]
"Table 2 analyzes the performance of our two best systems (PD = projective, FTD = fixed-tree) in more detail, using the categories of Damonte et al. (2017), and compares them to Wang’s, Flanigan’s, and Damonte’s AMR parsers on the 2015 set and , and van Noord and Bos (2017b) for the 2017 dataset.",7.5 Results,[0],[0]
(Foland and Martin (2017) did not publish such results.),7.5 Results,[0],[0]
"The good scores we achieve on reentrancy identification, despite removing a large amount of reentrant edges from the training data, indicates that our elementary as-graphs successfully encode phenomena such as control and coordination.
",7.5 Results,[0],[0]
"The projective decoder is given 4, and the fixedtree decoder 6, supertags for each token.",7.5 Results,[0],[0]
We trained the supertagging and edge scoring models of Section 5 separately; joint training did not help.,7.5 Results,[0],[0]
"Not sampling the supertag types τi during training of the K&G model, removing them from the input, and removing the character-based LSTM encodings ci from the input of the supertagger, all reduced our models’ accuracy.",7.5 Results,[0],[0]
"Although the Smatch scores for our two best models are close, they sometimes struggle with different sentences.",7.6 Differences between the parsers,[0],[0]
"The fixed-tree parser is at the mercy of
the fixed tree; the projective parser cannot produce non-projective AM dependency trees.",7.6 Differences between the parsers,[0],[0]
"It is remarkable that the projective parser does so well, given the prevalence of non-projective trees in the training data.",7.6 Differences between the parsers,[0],[0]
"Looking at its analyses, we find that it frequently manages to find a projective tree which yields an (almost) correct AMR, by choosing supertags with unusual types, and by using modify rather than apply (or vice versa).",7.6 Differences between the parsers,[0],[0]
"We presented an AMR parser which applies methods from supertagging and dependency parsing to map a string into a well-typed AM term, which it then evaluates into an AMR.",8 Conclusion,[0],[0]
"The AM term represents the compositional semantic structure of the AMR explicitly, allowing us to use standard treebased parsing techniques.
",8 Conclusion,[0],[0]
The projective parser currently computes the complete parse chart.,8 Conclusion,[0],[0]
"In future work, we will speed it up through the use of pruning techniques.",8 Conclusion,[0],[0]
We will also look into more principled methods for splitting the AMRs into elementary as-graphs to replace our hand-crafted heuristics.,8 Conclusion,[0],[0]
"In particular, advanced methods for alignments, as in Lyu and Titov (2018), seem promising.",8 Conclusion,[0],[0]
"Overcoming the need for heuristics also seems to be a crucial ingredient for applying our method to other semantic representations.
",8 Conclusion,[0],[0]
Acknowledgements We would like to thank the anonymous reviewers for their comments.,8 Conclusion,[0],[0]
"We thank Stefan Grünewald for his contribution to our PyTorch implementation, and want to acknowledge the inspiration obtained from Nguyen et al. (2017).",8 Conclusion,[0],[0]
We also extend our thanks to the organizers and participants of the Oslo CAS Meaning Construction workshop on Universal Dependencies.,8 Conclusion,[0],[0]
This work was supported by the DFG grant KO 2916/2-1 and a Macquarie University Research Excellence Scholarship for Jonas Groschwitz.,8 Conclusion,[0],[0]
We present a semantic parser for Abstract Meaning Representations which learns to parse strings into tree representations of the compositional structure of an AMR graph.,abstractText,[0],[0]
"This allows us to use standard neural techniques for supertagging and dependency tree parsing, constrained by a linguistically principled type system.",abstractText,[0],[0]
"We present two approximative decoding algorithms, which achieve state-of-the-art accuracy and outperform strong baselines.",abstractText,[0],[0]
AMR Dependency Parsing with a Typed Semantic Algebra,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7–13 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-2002
AMR-to-text Generation with Synchronous Node Replacement Grammar
Linfeng Song, Xiaochang Peng, Yue Zhang, Zhiguo Wang and Daniel Gildea Department of Computer Science, University of Rochester, Rochester, NY 14627
IBM T.J. Watson Research Center, Yorktown Heights, NY 10598 Singapore University of Technology and Design
Abstract
This paper addresses the task of AMR-totext generation by leveraging synchronous node replacement grammar. During training, graph-to-string rules are learned using a heuristic extraction algorithm. At test time, a graph transducer is applied to collapse input AMRs and generate output sentences. Evaluated on a standard benchmark, our method gives the state-of-the-art result.",text,[0],[0]
"Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism encoding the meaning of a sentence as a rooted, directed graph.",1 Introduction,[0],[0]
"AMR uses a graph to represent meaning, where nodes (such as “boy”, “want-01”) represent concepts, and edges (such as “ARG0”, “ARG1”) represent relations between concepts.",1 Introduction,[0],[0]
"Encoding many semantic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015).
",1 Introduction,[0],[0]
"AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations.",1 Introduction,[0],[0]
"Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016).
",1 Introduction,[0],[0]
"Flanigan et al. (2016) transform a given AMR graph into a spanning tree, before translating it to a sentence using a tree-to-string transducer.",1 Introduction,[0],[0]
"Their method leverages existing machine translation techniques, capturing hierarchical correspondences between the spanning tree and the surface string.",1 Introduction,[0],[0]
"However, it suffers from error propagation since the output is constrained given a spanning tree due to the projective correspondence between them.",1 Introduction,[0],[0]
Information loss in the graph-to-tree transformation step cannot be recovered.,1 Introduction,[0],[0]
Song et al. (2016) directly generate sentences using graphfragment-to-string rules.,1 Introduction,[0],[0]
"They cast the task of finding a sequence of disjoint rules to transduce an AMR graph into a sentence as a traveling salesman problem, using local features and a language model to rank candidate sentences.",1 Introduction,[0],[0]
"However, their method does not learn hierarchical structural correspondences between AMR graphs and strings.
",1 Introduction,[0],[0]
We propose to leverage the advantages of hierarchical rules without suffering from graph-to-tree errors by directly learning graph-to-string rules.,1 Introduction,[0],[0]
"As shown in Figure 1, we learn a synchronous node replacement grammar (NRG) from a corpus of aligned AMR and sentence pairs.",1 Introduction,[0],[0]
"At test time, we apply a graph transducer to collapse input
7
AMR graphs and generate output strings according to the learned grammar.",1 Introduction,[0],[0]
"Our system makes use of a log-linear model with real-valued features, tuned using MERT (Och, 2003), and beam search decoding.",1 Introduction,[0],[0]
"It gives a BLEU score of 25.62 on LDC2015E86, which is the state-of-the-art on this dataset.",1 Introduction,[0],[0]
"A synchronous node replacement grammar (NRG) is a rewriting formalism: G = 〈N,Σ,∆, P, S〉, where N is a finite set of nonterminals, Σ and ∆ are finite sets of terminal symbols for the source and target sides, respectively.",2.1 Grammar Definition,[0],[0]
"S ∈ N is the start symbol, and P is a finite set of productions.",2.1 Grammar Definition,[0],[0]
"Each instance of P takes the form Xi → (〈F,E〉,∼), where Xi ∈ N is a nonterminal node, F is a rooted, connected AMR fragment with edge labels over Σ and node labels over N ∪ Σ, E is a corresponding target string over N ∪∆ and ∼ denotes the alignment of nonterminal symbols between F and E. A classic NRG (Engelfriet and Rozenberg, 1997, Chapter 1) also defines C, which is an embedding mechanism defining how F is connected to the rest of the graph when replacing Xi with F on the graph.",2.1 Grammar Definition,[0],[0]
Here we omit defining C and allow arbitrary connections.1,2.1 Grammar Definition,[0],[0]
"Following Chiang
1This may over generate, but does not affect our case, as in our bottom-up decoding procedure (section 3) when F is replaced with Xi, nodes previously connected to F are reconnected to Xi
Data:",2.1 Grammar Definition,[0],[0]
"training corpus C Result: rule instances R
1 R←",2.1 Grammar Definition,[0],[0]
"[]; 2 for (Sent,AMR,∼) in C do 3 Rcur ← FRAGMENTEXTRACT(Sent,AMR,∼); 4 for ri in Rcur do 5 R.APPEND(ri) ; 6 for rj in Rcur/{ri} do 7 if ri.CONTAINS(rj) then 8 rij ← ri.COLLAPSE(rj); 9 R.APPEND(rij) ;
10 end 11 end 12 end 13 end
Algorithm 1: Rule extraction
(2005), we use only one nonterminal X in addition to S, and use subscripts to distinguish different non-terminal instances.
",2.1 Grammar Definition,[0],[0]
Figure 2 shows an example derivation process for the sentence “the boy wants to go” given the rule set in Table 1.,2.1 Grammar Definition,[0],[0]
"Given the start symbol S, which is first replaced with X1, rule (c) is applied to generate “X2 to go” and its AMR counterpart.",2.1 Grammar Definition,[0],[0]
Then rule (b) is used to generate “X3 wants” and its AMR counterpart from X2.,2.1 Grammar Definition,[0],[0]
"Finally, rule (a) is used to generate “the boy” and its AMR counterpart from X3.",2.1 Grammar Definition,[0],[0]
"Our graph-to-string rules are inspired by synchronous grammars for machine translation (Wu, 1997; Yamada and Knight, 2002; Gildea, 2003; Chiang, 2005; Huang et al., 2006; Liu et al., 2006; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013).",2.1 Grammar Definition,[0],[0]
"There are three types of rules in our system, namely induced rules, concept rules and graph glue rules.",2.2 Induced Rules,[0],[0]
"Here we first introduce induced rules, which are obtained by a two-step procedure on a training corpus.",2.2 Induced Rules,[0],[0]
"Shown in Algorithm 1, the first step is to extract a set of initial rules from training 〈sentence, AMR, ∼〉2 pairs (Line 2) using the phrase-to-graph-fragment extraction algorithm of Peng et al. (2015) (Line 3).",2.2 Induced Rules,[0],[0]
"Here an initial rule
2∼ denotes alignment between words and AMR labels.
",2.2 Induced Rules,[0],[0]
"contains only terminal symbols in both F and E. As a next step, we match between pairs of initial rules ri and rj , and generate rij by collapsing ri with rj , if ri contains rj (Line 6-8).",2.2 Induced Rules,[0],[0]
"Here ri contains rj , if rj .F is a subgraph of ri.F and rj .E is a sub-phrase of ri.",2.2 Induced Rules,[0],[0]
E.,2.2 Induced Rules,[0],[0]
"When collapsing ri with rj , we replace the corresponding subgraph in ri.F with a new non-terminal node, and the sub-phrase in ri.E with the same non-terminal.",2.2 Induced Rules,[0],[0]
"For example, we obtain rule (b) by collapsing (d) with (a) in Table 1.",2.2 Induced Rules,[0],[0]
"All initial and generated rules are stored in a rule list R (Lines 5 and 9), which will be further normalized to obtain the final induced rule set.",2.2 Induced Rules,[0],[0]
"In addition to induced rules, we adopt concept rules (Song et al., 2016) and graph glue rules to ensure existence of derivations.",2.3 Concept Rules and Glue Rules,[0],[0]
"For a concept rule, F is a single node in the input AMR graph, and E is a morphological string of the node concept.",2.3 Concept Rules and Glue Rules,[0],[0]
A concept rule is used in case no induced rule can cover the node.,2.3 Concept Rules and Glue Rules,[0],[0]
We refer to the verbalization list3 and AMR guidelines4 for creating more complex concept rules.,2.3 Concept Rules and Glue Rules,[0],[0]
"For example, one concept rule created from the verbalization list is “(k / keep-01 :ARG1 (p / peace)) |||",2.3 Concept Rules and Glue Rules,[0],[0]
"peacekeeping”.
",2.3 Concept Rules and Glue Rules,[0],[0]
"Inspired by Chiang (2005), we define graph glue rules to concatenate non-terminal nodes connected with an edge, when no induced rules can be applied.",2.3 Concept Rules and Glue Rules,[0],[0]
Three glue rules are defined for each type of edge label.,2.3 Concept Rules and Glue Rules,[0],[0]
"Taking the edge label “ARG0” as an example, we create the following glue rules:
ID.",2.3 Concept Rules and Glue Rules,[0],[0]
F E r1 (X1 / #X1# :ARG0 (X2 / #X2#)),2.3 Concept Rules and Glue Rules,[0],[0]
#X1# #X2# r2,2.3 Concept Rules and Glue Rules,[0],[0]
(X1 / #X1# :ARG0 (X2 / #X2#)),2.3 Concept Rules and Glue Rules,[0],[0]
"#X2# #X1# r3 (X1 / #X1# :ARG0 X1) #X1#
where for both r1 and r2, F contains two nonterminal nodes with a directed edge connecting them, and E is the concatenation the two nonterminals in either the monotonic or the inverse order.",2.3 Concept Rules and Glue Rules,[0],[0]
"For r3, F contains one non-terminal node with a self-pointing edge, and E is the nonterminal.",2.3 Concept Rules and Glue Rules,[0],[0]
"With concept rules and glue rules in our final rule set, it is easily guaranteed that there are legal derivations for any input AMR graph.",2.3 Concept Rules and Glue Rules,[0],[0]
We adopt a log-linear model for scoring search hypotheses.,3 Model,[0],[0]
"Given an input AMR graph, we find
3http://amr.isi.edu/download/lists/verbalization-listv1.06.txt
4https://github.com/amrisi/amr-guidelines
the highest scored derivation t∗ from all possible derivations t:
t∗ = argmax t
exp ∑
i
wifi(g, t), (1)
where g denotes the input AMR, fi(·, ·) and wi represent a feature and the corresponding weight, respectively.",3 Model,[0],[0]
"The feature set that we adopt includes phrase-to-graph and graph-to-phrase translation probabilities and their corresponding lexicalized translation probabilities (section 3.1), language model score, word count, rule count, reordering model score (section 3.2) and moving distance (section 3.3).",3 Model,[0],[0]
"The language model score, word count and phrase count features are adopted from SMT (Koehn et al., 2003; Chiang, 2005).
",3 Model,[0],[0]
We perform bottom-up search to transduce input AMRs to surface strings.,3 Model,[0],[0]
"Each hypothesis contains the current AMR graph, translations of collapsed subgraphs, the feature vector and the current model score.",3 Model,[0],[0]
"Beam search is adopted, where hypotheses with the same number of collapsed edges and nodes are put into the same beam.",3 Model,[0],[0]
Production rules serve as a basis for scoring hypotheses.,3.1 Translation Probabilities,[0],[0]
We associate each synchronous NRG rule n →,3.1 Translation Probabilities,[0],[0]
"(〈F,E〉,∼) with a set of probabilities.",3.1 Translation Probabilities,[0],[0]
"First, phrase-to-fragment translation probabilities are defined based on maximum likelihood estimation (MLE), as shown in Equation 2, where c〈F,E〉 is the fractional count of 〈F,E〉.
p(F |E) = c〈F,E〉∑ F ′ c〈F ′,E〉
(2)
",3.1 Translation Probabilities,[0],[0]
"In addition, lexicalized translation probabilities are defined as:
pw(F |E) = ∏
l∈F
∑ w∈E p(l|w) (3)
",3.1 Translation Probabilities,[0],[0]
"Here l is a label (including both edge labels such as “ARG0” and concept labels such as “want-01”) in the AMR fragment F , and w is a word in the phrase E. Equation 3 can be regarded as a “soft” version of the lexicalized translation probabilities adopted by SMT, which picks the alignment yielding the maximum lexicalized probability for each translation rule.",3.1 Translation Probabilities,[0],[0]
"In addition to p(F |E) and pw(F |E), we use features in the reverse direction, namely p(E|F ) and pw(E|F ), the definitions of which are omitted as they are consistent with
Equations 2 and 3, respectively.",3.1 Translation Probabilities,[0],[0]
The probabilities associated with concept rules and glue rules are manually set to 0.0001.,3.1 Translation Probabilities,[0],[0]
"Although the word order is defined for induced rules, it is not the case for glue rules.",3.2 Reordering Model,[0],[0]
We learn a reordering model that helps to decide whether the translations of the nodes should be monotonic or inverse given the directed connecting edge label.,3.2 Reordering Model,[0],[0]
"The probabilistic model using smoothed counts is defined as:
p(M |h, l, t) = 1.0 + ∑ h ∑ t c(h, l, t,M)
2.0 + ∑ o∈{M,I} ∑ h ∑ t c(h, l, t, o) (4)
c(h, l, t,M) is the count of monotonic translations of head h and tail t, connected by edge l.",3.2 Reordering Model,[0],[0]
"The moving distance feature captures the distances between the subgraph roots of two consecutive rule matches in the decoding process, which controls a bias towards collapsing nearby subgraphs consecutively.",3.3 Moving Distance,[0],[0]
"We use LDC2015E86 as our experimental dataset, which contains 16833 training, 1368 dev and 1371 test instances.",4.1 Setup,[0],[0]
"Each instance contains a sentence, an AMR graph and the alignment generated by a heuristic aligner.",4.1 Setup,[0],[0]
"Rules are extracted from the training data, and model parameters are tuned on the dev set.",4.1 Setup,[0],[0]
"For tuning and testing, we filter out sentences with more than 30 words, resulting in 1103 dev instances and 1055 test instances.",4.1 Setup,[0],[0]
"We train a 4-gram language model (LM) on gigaword (LDC2011T07), and use BLEU (Papineni et al., 2002) as the evaluation metric.",4.1 Setup,[0],[0]
"MERT is used (Och, 2003) to tune model parameters on k-best outputs on the devset, where k is set 50.
",4.1 Setup,[0],[0]
"We investigate the effectiveness of rules and features by ablation tests: “NoInducedRule” does not adopt induced rules, “NoConceptRule” does not adopt concept rules, “NoMovingDistance” does not adopt the moving distance feature, and “NoReorderModel” disables the reordering model.",4.1 Setup,[0],[0]
"Given an AMR graph, if NoConceptRule cannot produce a legal derivation, we concatenate
existing translation fragments into a final translation, and if a subgraph can not be translated, the empty string is used as the output.",4.1 Setup,[0],[0]
"We also compare our method with previous works, in particular JAMR-gen (Flanigan et al., 2016) and TSP-gen (Song et al., 2016), on the same dataset.",4.1 Setup,[0],[0]
The results are shown in Table 2.,4.2 Main results,[0],[0]
"First, All outperforms all baselines.",4.2 Main results,[0],[0]
"NoInducedRule leads to the greatest performance drop compared with All, demonstrating that induced rules play a very important role in our system.",4.2 Main results,[0],[0]
"On the other hand, NoConceptRule does not lead to much performance drop.",4.2 Main results,[0],[0]
This observation is consistent with the observation of Song et al. (2016) for their TSP-based system.,4.2 Main results,[0],[0]
"NoMovingDistance leads to a significant performance drop, empirically verifying the fact that the translations of nearby subgraphs are also close.",4.2 Main results,[0],[0]
"Finally, NoReorderingModel does not affect the performance significantly, which can be because the most important reordering patterns are already covered by the hierarchical induced rules.",4.2 Main results,[0],[0]
"Compared with TSP-gen and JAMR-gen, our final model All improves the BLEU from 22.44 and 23.00 to 25.62, showing the advantage of our model.",4.2 Main results,[0],[0]
"To our knowledge, this is the best result reported so far on the task.",4.2 Main results,[0],[0]
We have shown the effectiveness of our synchronous node replacement grammar (SNRG) on the AMR-to-text generation task.,4.3 Grammar analysis,[0],[0]
"Here we further analyze our grammar as it is relatively less studied than the hyperedge replacement grammar (HRG) (Drewes et al., 1997).
",4.3 Grammar analysis,[0],[0]
"Statistics on the whole rule set We first categorize our rule set by the number of terminals and nonterminals in the AMR fragment F , and show the percentages of each type in Figure 3.",4.3 Grammar analysis,[0],[0]
"Each rule contains at most 1 nonterminal, as we collapse each initial rule only once.",4.3 Grammar analysis,[0],[0]
"First
of all, the percentage of rules containing nonterminals are much more than those without nonterminals, as we collapse each pair of initial rules (in Algorithm 1) and the results can be quadratic the number of initial rules.",4.3 Grammar analysis,[0],[0]
"In addition, most rules are small containing 1 to 3 terminals, meaning that they represent small pieces of meaning and are easier to matched on a new AMR graph.",4.3 Grammar analysis,[0],[0]
"Finally, there are a few large rules, which represent complex meaning.
",4.3 Grammar analysis,[0],[0]
"Statistics on the rules used for decoding In addition, we collect the rules that our well-tuned system used for generating the 1-best output on the testset, and categorize them into 3 types: (1) glue rules, (2) nonterminal rules, which are not glue rules but contain nonterminals on the righthand side and (3) terminal rules, whose right-hand side only contain terminals.",4.3 Grammar analysis,[0],[0]
"Over the rules used on the 1-best result, more than 30% are non-terminal rules, showing that the induced rules play an important role.",4.3 Grammar analysis,[0],[0]
"On the other hand, 30% are glue rules.",4.3 Grammar analysis,[0],[0]
"The reason is that the data sparsity for graph grammars is more severe than string-based grammars (such as CFG), as the graph structures are more complex than strings.",4.3 Grammar analysis,[0],[0]
"Finally, terminal rules take the largest percentage, while most are induced rules, but not concept rules.
",4.3 Grammar analysis,[0],[0]
"Rule examples Finally, we show some rules in Table 4, where F and E are the right-hand-side AMR fragment and phrase, respectively.",4.3 Grammar analysis,[0],[0]
"For the first rule, the root of F is a verb (“give-01”) whose subject is a nonterminal and object is a AMR fragment “(p / person :ARG0-of (u / use-01))”, which means “user”.",4.3 Grammar analysis,[0],[0]
So it is easy to see that the corresponding phrase E conveys the same meaning.,4.3 Grammar analysis,[0],[0]
"For the second rule, “(s3 / stay-01 :accompanier (i / i))” means “stay
with me”, which is also covered by its phrase.",4.3 Grammar analysis,[0],[0]
"Finally, we show an example in Table 5, where the top is the input AMR graph, and the bottom is the generation result.",4.4 Generation example,[0],[0]
"Generally, most of the meaning of the input AMR are correctly translated, such as “:example”, which means “such as”, and “thing”, which is an abstract concept and should not be translated, while there are a few errors, such as “that” in the result should be “what”, and there should be an “in” between “tmt” and “fairfax”.",4.4 Generation example,[0],[0]
"We showed that synchronous node replacement grammar is useful for AMR-to-text generation by developing a system that learns a synchronous NRG in the training time, and applies a graph transducer to collapse input AMR graphs and generate output strings according to the learned grammar at test time.",5 Conclusion,[0],[0]
"Our method performs better than the previous systems, empirically proving the advantages of our graph-to-string rules.",5 Conclusion,[0],[0]
This work was funded by a Google Faculty Research Award.,Acknowledgement,[0],[0]
Yue Zhang is funded by NSFC61572245 and T2MOE201301 from Singapore Ministry of Education.,Acknowledgement,[0],[0]
This paper addresses the task of AMR-totext generation by leveraging synchronous node replacement grammar.,abstractText,[0],[0]
"During training, graph-to-string rules are learned using a heuristic extraction algorithm.",abstractText,[0],[0]
"At test time, a graph transducer is applied to collapse input AMRs and generate output sentences.",abstractText,[0],[0]
"Evaluated on a standard benchmark, our method gives the state-of-the-art result.",abstractText,[0],[0]
AMR-to-text Generation with Synchronous Node Replacement Grammar,title,[0],[0]
"We consider the design of adaptive, nonparametric statistical tests of dependence: that is, tests of whether a joint distribution Pxy factorizes into the product of marginals PxPy with the null hypothesis that H0 : X and Y are independent.",1. Introduction,[0],[0]
"While classical tests of dependence, such as Pearson’s correlation and Kendall’s τ , are able to detect monotonic relations between univariate variables, more modern tests can address complex interactions, for instance changes in variance of X with the value of Y .",1. Introduction,[0],[0]
Key to many recent tests is to examine covariance or correlation between data features.,1. Introduction,[0],[0]
"These interactions become significantly harder to detect, and the features are more difficult to design, when the data reside in high dimensions.
",1. Introduction,[0],[0]
Zoltán Szabó’s ORCID ID: 0000-0001-6183-7603.,1. Introduction,[0],[0]
Arthur Gretton’s ORCID ID: 0000-0003-3169-7624.,1. Introduction,[0],[0]
"1Gatsby Unit, University College London, UK. 2CMAP, École Polytechnique, France.",1. Introduction,[0],[0]
"Correspondence to: Wittawat Jitkrittum <wittawatj@gmail.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"A basic nonlinear dependence measure is the HilbertSchmidt Independence Criterion (HSIC), which is the Hilbert-Schmidt norm of the covariance operator between feature mappings of the random variables (Gretton et al., 2005; 2008).",1. Introduction,[0],[0]
Each random variable X and Y is mapped to a respective reproducing kernel Hilbert space Hk and Hl.,1. Introduction,[0],[0]
"For sufficiently rich mappings, the covariance operator norm is zero if and only if the variables are independent.",1. Introduction,[0],[0]
"A second basic nonlinear dependence measure is the smoothed difference between the characteristic function of the joint distribution, and that of the product of marginals.",1. Introduction,[0],[0]
"When a particular smoothing function is used, the statistic corresponds to the covariance between distances ofX and Y variable pairs (Feuerverger, 1993; Székely et al., 2007; Székely & Rizzo, 2009), yielding a simple test statistic based on pairwise distances.",1. Introduction,[0],[0]
It has been shown by Sejdinovic et al. (2013) that the distance covariance (and its generalization to semi-metrics) is an instance of HSIC for an appropriate choice of kernels.,1. Introduction,[0],[0]
"A disadvantage of these feature covariance statistics, however, is that they require quadratic time to compute (besides in the special case of the distance covariance with univariate real-valued variables, where Huo & Székely (2016) achieve an O(n log n) cost).",1. Introduction,[0],[0]
"Moreover, the feature covariance statistics have intractable null distributions, and either a permutation approach or the solution of an expensive eigenvalue problem (e.g. Zhang et al., 2011) is required for consistent estimation of the quantiles.",1. Introduction,[0],[0]
Several approaches were proposed by Zhang et al. (2017) to obtain faster tests along the lines of HSIC.,1. Introduction,[0],[0]
"These include computing HSIC on finite-dimensional feature mappings chosen as random Fourier features (RFFs) (Rahimi & Recht, 2008), a block-averaged statistic, and a Nyström approximation to the statistic.",1. Introduction,[0],[0]
"Key to each of these approaches is a more efficient computation of the statistic and its threshold under the null distribution: for RFFs, the null distribution is a finite weighted sum of χ2 variables; for the block-averaged statistic, the null distribution is asymptotically normal; for Nyström, either a permutation approach is employed, or the spectrum of the Nyström approximation to the kernel matrix is used in approximating the null distribution.",1. Introduction,[0],[0]
"Each of these methods costs significantly less than theO(n2) cost of the full HSIC (the cost is linear in n, but also depends quadratically on the number of features retained).",1. Introduction,[0],[0]
"A potential disadvantage of the Nyström and Fourier approaches is that the features are not optimized to maximize test power,
but are chosen randomly.",1. Introduction,[0],[0]
"The block statistic performs worse than both, due to the large variance of the statistic under the null (which can be mitigated by observing more data).
",1. Introduction,[0],[0]
"In addition to feature covariances, correlation measures have also been developed in infinite dimensional feature spaces: in particular, Bach & Jordan (2002); Fukumizu et al. (2008) proposed statistics on the correlation operator in a reproducing kernel Hilbert space.",1. Introduction,[0],[0]
"While convergence has been established for certain of these statistics, their computational cost is high at O(n3), and test thresholds have relied on permutation.",1. Introduction,[0],[0]
"A number of much faster approaches to testing based on feature correlations have been proposed, however.",1. Introduction,[0],[0]
"For instance, Dauxois & Nkiet (1998) compute statistics of the correlation between finite sets of basis functions, chosen for instance to be step functions or low order B-splines.",1. Introduction,[0],[0]
The cost of this approach is O(n).,1. Introduction,[0],[0]
"This idea was extended by Lopez-Paz et al. (2013), who computed the canonical correlation between finite sets of basis functions chosen as random Fourier features; in addition, they performed a copula transform on the inputs, with a total cost of O(n log n).",1. Introduction,[0],[0]
"Finally, space partitioning approaches have also been proposed, based on statistics such as the KL divergence, however these apply only to univariate variables (Heller et al., 2016), or to multivariate variables of low dimension (Gretton & Györfi, 2010) (that said, these tests have other advantages of theoretical interest, notably distribution-independent test thresholds).
",1. Introduction,[0],[0]
The approach we take is most closely related to HSIC on a finite set of features.,1. Introduction,[0],[0]
"Our simplest test statistic, the Finite Set Independence Criterion (FSIC), is an average of covariances of analytic functions (i.e., features) defined on each of X and Y .",1. Introduction,[0],[0]
A normalized version of the statistic (NFSIC) yields a distribution-independent asymptotic test threshold.,1. Introduction,[0],[0]
"We show that our test is consistent, despite a finite number of analytic features being used, via a generalization of arguments in Chwialkowski et al. (2015).",1. Introduction,[0],[0]
"As in recent work on two-sample testing by Jitkrittum et al. (2016), our test is adaptive in the sense that we choose our features on a held-out validation set to optimize a lower bound on the test power.",1. Introduction,[0],[0]
"The design of features for independence testing turns out to be quite different to the case of two-sample testing, however: the task is to find correlated feature pairs on the respective marginal domains, rather than attempting to find a single, high-dimensional feature representation on the tensor product of the marginals, as we would need to do if we were comparing distributions Pxy and Qxy .",1. Introduction,[0],[0]
"While the use of coupled feature pairs on the marginals entails a smaller feature space dimension, it introduces significant complications in the proof of the lower bound, compared with the two-sample case.",1. Introduction,[0],[0]
"We demonstrate the performance of our tests on several challenging artificial and real-world datasets, including detection of dependence between music and its year of appearance, and between videos and captions.
",1. Introduction,[0],[0]
"In these experiments, we outperform competing linear and O(n log n) time tests.",1. Introduction,[0],[0]
"We introduce two test statistics: first, the Finite Set Independence Criterion (FSIC), which builds on the principle that dependence can be measured in terms of the covariance between data features.",2. Independence Criteria and Statistical Tests,[0],[0]
"Next, we propose a normalized version of this statistic (NFSIC), with a simpler asymptotic distribution when Pxy = PxPy.",2. Independence Criteria and Statistical Tests,[0],[0]
We show how to select features for the latter statistic to maximize a lower bound on the power of its corresponding statistical test.,2. Independence Criteria and Statistical Tests,[0],[0]
"We begin by recalling the Hilbert-Schmidt Independence Criterion (HSIC) as proposed in Gretton et al. (2005), since our unnormalized statistic is built along similar lines.",2.1. The Finite Set Independence Criterion,[0],[0]
Consider two random variables X ∈ X ⊆ Rdx and Y ∈,2.1. The Finite Set Independence Criterion,[0],[0]
Y ⊆ Rdy .,2.1. The Finite Set Independence Criterion,[0],[0]
Denote by Pxy the joint distribution betweenX and Y ; Px and Py are the marginal distributions of X and Y .,2.1. The Finite Set Independence Criterion,[0],[0]
"Let⊗ denote the tensor product, such that (a⊗ b) c = a 〈b, c〉.",2.1. The Finite Set Independence Criterion,[0],[0]
Assume that k :,2.1. The Finite Set Independence Criterion,[0],[0]
"X × X → R and l : Y × Y → R are positive definite kernels associated with reproducing kernel Hilbert spaces (RKHS)Hk andHl, respectively.",2.1. The Finite Set Independence Criterion,[0],[0]
Let ‖ · ‖HS be the norm on the space ofHl →,2.1. The Finite Set Independence Criterion,[0],[0]
Hk Hilbert-Schmidt operators.,2.1. The Finite Set Independence Criterion,[0],[0]
"Then, HSIC between X and Y is defined as
HSIC(X,Y ) = ∥∥µxy",2.1. The Finite Set Independence Criterion,[0],[0]
"− µx ⊗ µy∥∥2HS
= E(x,y),(x′,y′)",2.1. The Finite Set Independence Criterion,[0],[0]
"[k(x,x′)l(y,y′)]",2.1. The Finite Set Independence Criterion,[0],[0]
"+ ExEx′ [k(x,x′)]EyEy′",2.1. The Finite Set Independence Criterion,[0],[0]
"[l(y,y′)]",2.1. The Finite Set Independence Criterion,[0],[0]
"− 2E(x,y) [Ex′ [k(x,x′)]Ey′",2.1. The Finite Set Independence Criterion,[0],[0]
"[l(y,y′)]] , (1)
where Ex := Ex∼Px , Ey := Ey∼Py , Exy := E(x,y)∼Pxy , and x′ is an independent copy of x.",2.1. The Finite Set Independence Criterion,[0],[0]
"The mean embedding of Pxy belongs to the space of Hilbert-Schmidt operators from Hl to Hk, µxy := ∫ X×Y k(x, ·) ⊗ l(y, ·) dPxy(x,y) ∈
HS(Hl,Hk), and the marginal mean embeddings are µx :=∫ X k(x, ·) dPx(x) ∈ Hk and µy := ∫ Y l(y, ·) dPy(y)",2.1. The Finite Set Independence Criterion,[0],[0]
"∈ Hl (Smola et al., 2007).",2.1. The Finite Set Independence Criterion,[0],[0]
"Gretton et al. (2005, Theorem 4) show that if the kernels k and l are universal (Steinwart & Christmann, 2008) on compact domains X and Y , then HSIC(X,Y )",2.1. The Finite Set Independence Criterion,[0],[0]
= 0,2.1. The Finite Set Independence Criterion,[0],[0]
if and only if X and Y are independent.,2.1. The Finite Set Independence Criterion,[0],[0]
"Given a joint sample Zn = {(xi,yi)}ni=1 ∼ Pxy, an empirical estimator of HSIC can be computed in O(n2) time by replacing the population expectations in (1) with their corresponding empirical expectations based on Zn.
",2.1. The Finite Set Independence Criterion,[0],[0]
"We now propose our new linear-time dependence measure, the Finite Set Independence Criterion (FSIC).",2.1. The Finite Set Independence Criterion,[0],[0]
Let X ⊆ Rdx and Y ⊆ Rdy be open sets.,2.1. The Finite Set Independence Criterion,[0],[0]
"Let µxµy(x,y) := µx(x)µy(y)",2.1. The Finite Set Independence Criterion,[0],[0]
"The idea is to see µxy(v,w) = Exy[k(x,v)l(y,w)], µx(v) = Ex[k(x,v)] and µy(w) = Ey[l(y,w)] as smooth functions, and consider a new dis-
tance between µxy and µxµy instead of a Hilbert-Schmidt distance as in HSIC (Gretton et al., 2005).",2.1. The Finite Set Independence Criterion,[0],[0]
"The new measure is given by the average of squared differences between µxy and µxµy, evaluated at J random test locations VJ := {(vi,wi)}Ji=1 ⊂",2.1. The Finite Set Independence Criterion,[0],[0]
"X × Y .
",2.1. The Finite Set Independence Criterion,[0],[0]
"FSIC2(X,Y ) := 1
J J∑ i=1",2.1. The Finite Set Independence Criterion,[0],[0]
"[µxy(vi,wi)− µx(vi)µy(wi)]2
= 1
J J∑ i=1 u2(vi,wi) = 1 J ‖u‖22,
where
u(v,w) := µxy(v,w)− µx(v)µy(w) = Exy[k(x,v)l(y,w)]− Ex[k(x,v)]Ey[l(y,w)], (2) = covxy[k(x,v), l(y,w)],
u := (u(v1,w1), . . .",2.1. The Finite Set Independence Criterion,[0],[0]
", u(vJ ,wJ))",2.1. The Finite Set Independence Criterion,[0],[0]
">, and {(vi,wi)}Ji=1 are realizations from an absolutely continuous distribution (wrt the Lebesgue measure).
",2.1. The Finite Set Independence Criterion,[0],[0]
"Our first result in Proposition 2 states that FSIC(X,Y ) almost surely defines a dependence measure for the random variables X and Y , provided that the product kernel on the joint space X × Y is characteristic and analytic (see Definition 1).
",2.1. The Finite Set Independence Criterion,[0],[0]
"Definition 1 (Analytic kernels (Chwialkowski et al., 2015)).",2.1. The Finite Set Independence Criterion,[0],[0]
Let X be an open set in Rd.,2.1. The Finite Set Independence Criterion,[0],[0]
"A positive definite kernel k : X ×X → R is said to be analytic on its domain X ×X if for all v ∈ X , f(x) := k(x,v) is an analytic function on X .",2.1. The Finite Set Independence Criterion,[0],[0]
Assumption A.,2.1. The Finite Set Independence Criterion,[0],[0]
"The kernels k : X × X → R and l : Y × Y → R are bounded by Bk and Bl respectively [supx,x′∈X k(x,x
′) ≤",2.1. The Finite Set Independence Criterion,[0],[0]
"Bk, supy,y′∈Y l(y,y′) ≤",2.1. The Finite Set Independence Criterion,[0],[0]
"Bl] , and the product kernel g((x,y), (x′,y′))",2.1. The Finite Set Independence Criterion,[0],[0]
":= k(x,x′)l(y,y′) is characteristic (Sriperumbudur et al., 2010, Definition 6), and analytic (Definition 1) on (X ×",2.1. The Finite Set Independence Criterion,[0],[0]
Y)× (X × Y).,2.1. The Finite Set Independence Criterion,[0],[0]
Proposition 2 (FSIC is a dependence measure).,2.1. The Finite Set Independence Criterion,[0],[0]
"Assume that assumption A holds, and that the test locations VJ = {(vi,wi)}Ji=1 are drawn from an absolutely continuous distribution η.",2.1. The Finite Set Independence Criterion,[0],[0]
"Then, η-almost surely, it holds that FSIC(X,Y ) =",2.1. The Finite Set Independence Criterion,[0],[0]
"1√
J ‖u‖2 = 0",2.1. The Finite Set Independence Criterion,[0],[0]
"if and only if X and Y are
independent.
",2.1. The Finite Set Independence Criterion,[0],[0]
Proof.,2.1. The Finite Set Independence Criterion,[0],[0]
"Since g is characteristic,",2.1. The Finite Set Independence Criterion,[0],[0]
"the mean embedding map Πg : P 7→ E(x,y)∼P",2.1. The Finite Set Independence Criterion,[0],[0]
"[g((x,y), ·)] is injective (Sriperumbudur et al., 2010, Section 3), where P is a probability distribution on X × Y .",2.1. The Finite Set Independence Criterion,[0],[0]
"Since g is analytic, by Lemma 10 (Appendix), µxy and µxµy are analytic functions.",2.1. The Finite Set Independence Criterion,[0],[0]
"Thus, Lemma 11 (Appendix, setting Λ = Πg) guarantees that FSIC(X,Y ) = 0",2.1. The Finite Set Independence Criterion,[0],[0]
⇐⇒,2.1. The Finite Set Independence Criterion,[0],[0]
"Pxy = PxPy ⇐⇒ X and Y are independent almost surely.
",2.1. The Finite Set Independence Criterion,[0],[0]
"FSIC uses µxy as a proxy for Pxy , and µxµy as a proxy for PxPy.",2.1. The Finite Set Independence Criterion,[0],[0]
"Proposition 2 states that, to detect the dependence between X and Y , it is sufficient to evaluate the difference of the population joint embedding µxy and the embedding of the product of the marginal distributions µxµy at a finite number of locations (defined by VJ ).",2.1. The Finite Set Independence Criterion,[0],[0]
The intuitive explanation of this property is as follows.,2.1. The Finite Set Independence Criterion,[0],[0]
"If Pxy = PxPy, then u(v,w) = 0 everywhere, and FSIC(X,Y ) = 0 for any VJ .",2.1. The Finite Set Independence Criterion,[0],[0]
"If Pxy 6= PxPy, then u will not be a zero function, since the mean embedding map is injective (requires the product kernel to be characteristic).",2.1. The Finite Set Independence Criterion,[0],[0]
"Using the same argument as in Chwialkowski et al. (2015), since k and l are analytic, u is also analytic, and the set of roots",2.1. The Finite Set Independence Criterion,[0],[0]
"Ru := {(v,w) | u(v,w) = 0} has Lebesgue measure zero.",2.1. The Finite Set Independence Criterion,[0],[0]
"Thus, it is sufficient to draw (v,w) from an absolutely continuous distribution to have (v,w) /∈",2.1. The Finite Set Independence Criterion,[0],[0]
"Ru η-almost surely, and hence FSIC(X,Y ) >",2.1. The Finite Set Independence Criterion,[0],[0]
0.,2.1. The Finite Set Independence Criterion,[0],[0]
We note that a characteristic kernel which is not analytic may produce u such thatRu has a positive Lebesgue measure.,2.1. The Finite Set Independence Criterion,[0],[0]
"In this case, there is a positive probability",2.1. The Finite Set Independence Criterion,[0],[0]
"that (v,w) ∈ Ru, resulting in a potential failure to detect the dependence.
",2.1. The Finite Set Independence Criterion,[0],[0]
"The next proposition shows that Gaussian kernels k and l yield a product kernel which is characteristic and analytic; in other words, this is an example when Assumption A holds.",2.1. The Finite Set Independence Criterion,[0],[0]
Proposition 3 (A product of Gaussian kernels is characteristic and analytic).,2.1. The Finite Set Independence Criterion,[0],[0]
"Let k(x,x′) = exp ( −(x− x′)>A(x− x′) ) and l(y,y′)",2.1. The Finite Set Independence Criterion,[0],[0]
"=
exp ( −(y − y′)>B(y",2.1. The Finite Set Independence Criterion,[0],[0]
"− y′) ) be Gaussian kernels on Rdx × Rdx and Rdy × Rdy respectively, for positive definite matrices A and B. Then, g((x,y), (x′,y′))",2.1. The Finite Set Independence Criterion,[0],[0]
"= k(x,x′)l(y,y′) is characteristic and analytic on (Rdx × Rdy )×",2.1. The Finite Set Independence Criterion,[0],[0]
(Rdx × Rdy ).,2.1. The Finite Set Independence Criterion,[0],[0]
Proof (sketch).,2.1. The Finite Set Independence Criterion,[0],[0]
"The main idea is to use the fact that a Gaussian kernel is analytic, and a product of Gaussian kernels is a Gaussian kernel on the pair of variables.",2.1. The Finite Set Independence Criterion,[0],[0]
"See the full proof in Appendix D.
Plug-in Estimator Assume that we observe a joint sample Zn := {(xi,yi)}ni=1
i.i.d.∼ Pxy.",2.1. The Finite Set Independence Criterion,[0],[0]
"Unbiased estimators of µxy(v,w) and µxµy(v,w) are µ̂xy(v,w) :",2.1. The Finite Set Independence Criterion,[0],[0]
= 1n,2.1. The Finite Set Independence Criterion,[0],[0]
∑n i=1,2.1. The Finite Set Independence Criterion,[0],[0]
"k(xi,v)l(yi,w) and µ̂xµy(v,w) := 1 n(n−1)",2.1. The Finite Set Independence Criterion,[0],[0]
∑n i=1,2.1. The Finite Set Independence Criterion,[0],[0]
∑,2.1. The Finite Set Independence Criterion,[0],[0]
"j 6=i k(xi,v)l(yj ,w), respectively.",2.1. The Finite Set Independence Criterion,[0],[0]
"A straightforward empirical estimator of FSIC2 is then given by
F̂SIC2(Zn) = 1
J J∑ i=1",2.1. The Finite Set Independence Criterion,[0],[0]
"û(vi,wi) 2,
û(v,w) := µ̂xy(v,w)− µ̂xµy(v,w) (3)
",2.1. The Finite Set Independence Criterion,[0],[0]
"= 2 n(n− 1) ∑ i<j h(v,w)((xi,yi), (xj ,yj)), (4)
where h(v,w)((x,y), (x′,y′))",2.1. The Finite Set Independence Criterion,[0],[0]
":= 12 (k(x,v)",2.1. The Finite Set Independence Criterion,[0],[0]
"− k(x′,v))(l(y,w) − l(y′,w)).",2.1. The Finite Set Independence Criterion,[0],[0]
"For conciseness, we
define û := (û1, . . .",2.1. The Finite Set Independence Criterion,[0],[0]
", ûJ)> ∈ RJ where ûi := û(vi,wi) so that F̂SIC2(Zn) =",2.1. The Finite Set Independence Criterion,[0],[0]
"1J û >û.
F̂SIC2 can be efficiently computed inO((dx+dy)Jn) time which is linear in n",2.1. The Finite Set Independence Criterion,[0],[0]
"[see (3) which does not have nested double sums], assuming that the runtime complexity of evaluating k(x,v) is O(dx) and that of l(y,w) is O(dy).",2.1. The Finite Set Independence Criterion,[0],[0]
"Since FSIC satisfies FSIC(X,Y ) = 0 ⇐⇒ X ⊥ Y , in principle its empirical estimator can be used as a test statistic for an independence test proposing a null hypothesis H0 : “X and Y are independent” against an alternative H1 : “X and Y are dependent.”",2.1. The Finite Set Independence Criterion,[0],[0]
"The null distribution (i.e., distribution of the test statistic assuming that H0 is true) is challenging to obtain, however, and depends on the unknown Pxy.",2.1. The Finite Set Independence Criterion,[0],[0]
This prompts us to consider a normalized version of FSIC whose asymptotic null distribution takes a more convenient form.,2.1. The Finite Set Independence Criterion,[0],[0]
"We first derive the asymptotic distribution of û in Proposition 4, which we use to derive the normalized test statistic in Theorem 5.",2.1. The Finite Set Independence Criterion,[0],[0]
"As a shorthand, we write z := (x,y), t := (v,w), covz is covariance,Vz stands for variance.",2.1. The Finite Set Independence Criterion,[0],[0]
Proposition 4 (Asymptotic distribution of û).,2.1. The Finite Set Independence Criterion,[0],[0]
"Define u := (u(t1), . . .",2.1. The Finite Set Independence Criterion,[0],[0]
", u(tJ))
",2.1. The Finite Set Independence Criterion,[0],[0]
">, k̃(x,v) :",2.1. The Finite Set Independence Criterion,[0],[0]
"= k(x,v)",2.1. The Finite Set Independence Criterion,[0],[0]
"− Ex′k(x′,v), and l̃(y,w) := l(y,w)",2.1. The Finite Set Independence Criterion,[0],[0]
"− Ey′ l(y′,w).",2.1. The Finite Set Independence Criterion,[0],[0]
Let Σ =,2.1. The Finite Set Independence Criterion,[0],[0]
"[Σij ] ∈ RJ×J be the positive semi-definite matrix with entries Σij = covz(û(ti), û(tj))",2.1. The Finite Set Independence Criterion,[0],[0]
"= Exy[k̃(x,vi)l̃(y,wi)k̃(x,vj)l̃(y,wj)]−u(ti)u(tj).",2.1. The Finite Set Independence Criterion,[0],[0]
"Then, under both H0 and H1, for any fixed test locations {t1, . . .",2.1. The Finite Set Independence Criterion,[0],[0]
", tJ} for which Σ is full rank, and 0 < Vz[htj (z)]",2.1. The Finite Set Independence Criterion,[0],[0]
"< ∞ for j = 1, . . .",2.1. The Finite Set Independence Criterion,[0],[0]
", J , it holds that √ n(û − u) d→ N (0,Σ).
",2.1. The Finite Set Independence Criterion,[0],[0]
Proof.,2.1. The Finite Set Independence Criterion,[0],[0]
"For a fixed {t1, . . .",2.1. The Finite Set Independence Criterion,[0],[0]
", tJ}, û is a one-sample secondorder multivariate U-statistic with a U-statistic kernel ht.",2.1. The Finite Set Independence Criterion,[0],[0]
"Thus, by Lehmann (1999, Theorem 6.1.6) and Kowalski & Tu (2008, Section 5.1, Theorem 1), it follows directly that √ n(û − u) d→ N (0,Σ) where we note that Exy[k̃(x,v)l̃(y,w)]",2.1. The Finite Set Independence Criterion,[0],[0]
"= u(v,w).
",2.1. The Finite Set Independence Criterion,[0],[0]
Recall from Proposition 2 that u = 0 holds almost surely under H0.,2.1. The Finite Set Independence Criterion,[0],[0]
The asymptotic normality described in Proposition 4 implies that nF̂SIC2 = nJ,2.1. The Finite Set Independence Criterion,[0],[0]
"û
>û converges in distribution to a sum of J dependent weighted χ2 random variables.",2.1. The Finite Set Independence Criterion,[0],[0]
The dependence comes from the fact that the coordinates û1 . . .,2.1. The Finite Set Independence Criterion,[0],[0]
", ûJ of û all depend on the sample Zn.",2.1. The Finite Set Independence Criterion,[0],[0]
"This null distribution is not analytically tractable, and requires a large number of simulations to compute the rejection threshold",2.1. The Finite Set Independence Criterion,[0],[0]
Tα for a given significance value α.,2.1. The Finite Set Independence Criterion,[0],[0]
"For the purpose of an independence test, we will consider a normalized variant of F̂SIC2, which we call N̂FSIC2, whose tractable asymptotic null distribution is χ2(J), the
chi-squared distribution with J degrees of freedom.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
We then show that the independence test defined by N̂FSIC2 is consistent.,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"These results are given in Theorem 5.
",2.2. Normalized FSIC and Adaptive Test,[0],[0]
Theorem 5 (Independence test based on N̂FSIC2 is consistent).,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Let Σ̂ be a consistent estimate of Σ based on the joint sample Zn, where Σ is defined in Proposition 4.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Assume that VJ = {(vi,wi)}Ji=1 ∼ η where η is absolutely continuous wrt the Lebesgue measure.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"The N̂FSIC2 statistic is
defined as λ̂n := nû> ( Σ̂ + γnI )−1",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"û where γn ≥ 0 is a
regularization parameter.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Assume that
1.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Assumption A holds.
2.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Σ is invertible η-almost surely.
",2.2. Normalized FSIC and Adaptive Test,[0],[0]
3.,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"limn→∞ γn = 0.
",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Then, for any k, l and VJ satisfying the assumptions,
1.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Under H0, λ̂n d→ χ2(J) as n→∞. 2.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Under H1, for any r ∈ R, limn→∞ P ( λ̂n ≥ r ) = 1
η-almost surely.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"That is, the independence test based on N̂FSIC2 is consistent.
",2.2. Normalized FSIC and Adaptive Test,[0],[0]
Proof (sketch) .,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Under H0, nû>(Σ̂ + γnI)−1û asymptotically follows χ2(J) because √ nû is asymptotically normally distributed (see Proposition 4).",2.2. Normalized FSIC and Adaptive Test,[0],[0]
Claim 2 builds on the result in Proposition 2 stating that u 6= 0 under H1; it follows using the convergence of û to u.,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"The full proof can be found in Appendix E.
Theorem 5 states that if H1 holds, the statistic can be arbitrarily large as n increases, allowing H0 to be rejected for any fixed threshold.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
Asymptotically the test threshold,2.2. Normalized FSIC and Adaptive Test,[0],[0]
Tα is given by the (1− α)-quantile of χ2(J) and is independent of n.,2.2. Normalized FSIC and Adaptive Test,[0],[0]
The assumption on the consistency of Σ̂ is required to obtain the asymptotic chi-squared distribution.,2.2. Normalized FSIC and Adaptive Test,[0],[0]
The regularization parameter γn is to ensure that (Σ̂ + γnI)−1 can be stably computed.,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"In practice, γn requires no tuning, and can be set to be a very small constant.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"We emphasize that J need not increase with n for test consistency.
",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"The next proposition states that the computational complexity of the N̂FSIC2 estimator is linear in both the input dimension and sample size, and that it can be expressed in terms of the K =[Kij ] =",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"[k(vi,xj)] ∈ RJ×n,L =",2.2. Normalized FSIC and Adaptive Test,[0],[0]
[Lij ] =,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"[l(wi,yj)] ∈ RJ×n matrices.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"In contrast to typical kernel methods, a large Gram matrix of size n × n is not needed to compute N̂FSIC2.
",2.2. Normalized FSIC and Adaptive Test,[0],[0]
Proposition 6 (An empirical estimator of N̂FSIC2).,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Let 1n := (1, . . .",2.2. Normalized FSIC and Adaptive Test,[0],[0]
", 1)
",2.2. Normalized FSIC and Adaptive Test,[0],[0]
> ∈,2.2. Normalized FSIC and Adaptive Test,[0],[0]
Rn.,2.2. Normalized FSIC and Adaptive Test,[0],[0]
Denote by ◦ the element-wise matrix product.,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Then,
1.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"û = (K◦L)1nn−1 − (K1n)◦(L1n) n(n−1) .
",2.2. Normalized FSIC and Adaptive Test,[0],[0]
2.,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"A consistent estimator for Σ is Σ̂ = ΓΓ >
n where
Γ := (K− n−1K1n1>n ) ◦",2.2. Normalized FSIC and Adaptive Test,[0],[0]
(L− n−1L1n1>n ),2.2. Normalized FSIC and Adaptive Test,[0],[0]
"− ûb1>n , ûb = n−1 (K ◦ L)",2.2. Normalized FSIC and Adaptive Test,[0],[0]
1n,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"− n−2 (K1n) ◦ (L1n) .
",2.2. Normalized FSIC and Adaptive Test,[0],[0]
Assume that the complexity of the kernel evaluation is linear in the input dimension.,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Then the test statistic λ̂n =
nû> ( Σ̂ + γnI )−1",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"û can be computed in O(J3 + J2n +
(dx + dy)Jn) time.
",2.2. Normalized FSIC and Adaptive Test,[0],[0]
Proof (sketch).,2.2. Normalized FSIC and Adaptive Test,[0],[0]
Claim 1 for û is straightforward.,2.2. Normalized FSIC and Adaptive Test,[0],[0]
The expression for Σ̂ in claim 2 follows directly from the asymptotic covariance expression in Proposition 4.,2.2. Normalized FSIC and Adaptive Test,[0],[0]
The consistency of Σ̂ can be obtained by noting that the finite sample bound for P(‖Σ̂−Σ‖F > t) decreases as n increases.,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"This is implicitly shown in Appendix F.2.2 and its following sections.
",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Although the dependency of the estimator on J is cubic, we empirically observe that only a small value of J is required (see Section 3).",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"The number of test locations J relates to the number of regions in X × Y of pxy and pxpy that differ (see Figure 1).
",2.2. Normalized FSIC and Adaptive Test,[0],[0]
Theorem 5 asserts the consistency of the test for any test locations VJ drawn from an absolutely continuous distribution.,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"In practice, VJ can be further optimized to increase the test power for a fixed sample size.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Our final theoretical result gives a lower bound on the test power of N̂FSIC2 i.e., the probability of correctly rejecting H0.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
We will use this lower bound as the objective function to determine VJ and the kernel parameters.,2.2. Normalized FSIC and Adaptive Test,[0],[0]
Let ‖ · ‖F be the Frobenius norm.,2.2. Normalized FSIC and Adaptive Test,[0],[0]
Theorem 7 (A lower bound on the test power).,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Let NFSIC2(X,Y ) :",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"= λn := nu
>Σ−1u.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Let K be a kernel class for k, L be a kernel class for l, and V be a collection with each element being a set of J locations.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Assume that
1.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
There exist finite,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Bk and Bl such that supk∈K supx,x′∈X |k(x,x′)| ≤",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Bk and supl∈L supy,y′∈Y |l(y,y′)| ≤",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Bl.
2.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
c̃,2.2. Normalized FSIC and Adaptive Test,[0],[0]
:,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"= supk∈K supl∈L supVJ∈V ‖Σ −1‖F <∞.
Then, for any k ∈ K, l ∈ L, VJ ∈ V , and λn ≥ r, the test power satisfies P ( λ̂n ≥ r ) ≥ L(λn) where
L(λn) = 1− 62e−ξ1γ 2 n(λn−r) 2/n",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"− 2e−b0.5nc(λn−r)2/[ξ2n2]
− 2e−[(λn−r)γn(n−1)/3−ξ3n−c3γ2nn(n−1)] 2 /[ξ4n2(n−1)],
b·c is the floor function, ξ1 := 132c21J2B∗ , B ∗ is a constant depending on onlyBk andBl, ξ2 := 72c22JB 2,B := BkBl,
ξ3 := 8c1B 2J , c3 := 4B2Jc̃2, ξ4 := 28B4J2c21, c1 :=
4B2J √ Jc̃, and c2 := 4B √ Jc̃.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Moreover, for sufficiently large fixed n, L(λn) is increasing in λn.
",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"We provide the proof in Appendix F. To put Theorem 7 into perspective, assume that K ={ (x,v) 7→ exp ( −‖x−v‖ 2
2σ2x
)",2.2. Normalized FSIC and Adaptive Test,[0],[0]
| σ2x ∈,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"[σ2x,l, σ2x,u] } =:",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Kg
for some 0 < σ2x,l < σ 2 x,u < ∞ and L ={ (y,w) 7→ exp ( −‖y−w‖ 2
2σ2y
)",2.2. Normalized FSIC and Adaptive Test,[0],[0]
| σ2y ∈,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"[σ2y,l, σ2y,u] }",2.2. Normalized FSIC and Adaptive Test,[0],[0]
=:,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Lg
for some 0 < σ2y,l < σ",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"2 y,u < ∞ are Gaussian kernel classes.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Then, in Theorem 7, B = Bk = Bl = 1, and B∗ = 2.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"The assumption c̃ < ∞ is a technical condition to guarantee that the test power lower bound is finite for all θ defined by the feasible sets K,L, and V .",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Let V ,r := { VJ | ‖vi‖2, ‖wi‖2 ≤
r and ‖vi−vj‖22 + ‖wi−wj‖22 ≥ , for all i 6= j }
.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"If we set K = Kg,L = Lg, and V = V ,r for some , r > 0, then c̃ <∞ as Kg,Lg, and V ,r are compact.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"In practice, these conditions do not necessarily create restrictions as they almost always hold implicitly.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"We show in Appendix C that the objective function used to choose VJ will discourage any two locations to be in the same neighborhood.
",2.2. Normalized FSIC and Adaptive Test,[0],[0]
Parameter Tuning Let θ be the collection of all tuning parameters of the test.,2.2. Normalized FSIC and Adaptive Test,[0],[0]
If k ∈ Kg and l ∈,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Lg (i.e., Gaussian kernels), then θ = {σ2x, σ2y, VJ}.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
The test power lower bound L(λn) in Theorem 7 is a function of λn = nu>Σ−1u which is the population counterpart of the test statistic λ̂n.,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"As in FSIC, it can be shown that λn = 0",2.2. Normalized FSIC and Adaptive Test,[0],[0]
if and only if X are Y are independent (from Proposition 2).,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"According to Theorem 7, for a sufficiently large n, the test power lower bound is increasing in λn.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
One can therefore think of λn (a function of θ) as representing how easily the test rejects H0 given a problem Pxy .,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"The higher the λn, the greater the lower bound on the test power, and thus the more likely it is that the test will reject H0 when it is false.
",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"In light of this reasoning, we propose to set θ by maximizing the lower bound on the test power i.e., set θ to θ∗ = arg maxθ L(λn).",2.2. Normalized FSIC and Adaptive Test,[0],[0]
Assume that n is sufficiently large so that λn 7→ L(λn) is an increasing function.,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Then, arg maxθ L(λn) = arg maxθ λn.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
That this procedure is also valid under H0 can be seen as follows.,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Under H0, θ∗ = arg maxθ 0 will be arbitrary.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Since Theorem 7 guarantees that λ̂n
d→ χ2(J) as n→∞ for any θ, the asymptotic null distribution does not change by using θ∗.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"In practice, λn is a population quantity which is unknown.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
We propose dividing the sample Zn into two disjoint sets: training and test sets.,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"The training set is used to compute λ̂n (an estimate of λn) to optimize for θ∗, and the test set is used for the actual independence test with the optimized θ∗.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"The splitting is to guarantee the independence of θ∗ and the test sample to avoid overfitting.
",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"To better understand the behaviour of N̂FSIC2, we visualize µ̂xy(v,w), µ̂xµy(v,w) and Σ̂(v,w) as a function of one test location (v,w) on a simple toy problem.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"In this problem, Y = −X + Z where Z ∼ N (0, 0.32) is an independent noise variable.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"As we consider only one location (J = 1), Σ̂(v,w) is a scalar.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
The statistic can be written as λ̂n = n,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"(µ̂xy(v,w)−µ̂xµy(v,w))",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"2
Σ̂(v,w) .",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"These components are
shown in Figure 1, where we use Gaussian kernels for both X and Y , and the horizontal and vertical axes correspond to v ∈ R and w ∈ R, respectively.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Intuitively, û(v,w) = µ̂xy(v,w)",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"− µ̂xµy(v,w) captures the difference of the joint distribution and the product of the marginals as a function of (v,w).",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Squaring û(v,w) and dividing it by the variance shown in Figure 1c gives the statistic (also the parameter tuning objective) shown in Figure 1d.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
The latter figure illustrates that the parameter tuning objective function can be non-convex: non-convexity arises since there are multiple ways to detect the difference between the joint distribution and the product of the marginals.,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"In this case, the lower left and upper right regions equally indicate the largest difference.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
A convex objective would not be able to capture this phenomenon.,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"In this section, we empirically study the performance of the proposed method on both toy (Section 3.1) and real problems (Section 3.2).",3. Experiments,[0],[0]
"We are interested in challenging problems requiring a large number of samples, where a quadratic-time test might be computationally infeasible.",3. Experiments,[0],[0]
Our goal is not to outperform a quadratic-time test with a linear-time test uniformly over all testing problems.,3. Experiments,[0],[0]
"We will find, however, that our test does outperform the quadratic-time test in some cases.",3. Experiments,[0],[0]
"Code is available at https://github.com/wittawatj/fsic-test.
",3. Experiments,[0],[0]
We compare the proposed NFSIC with optimization (NFSICopt) to five multivariate nonparametric tests.,3. Experiments,[0],[0]
"The N̂FSIC2 test without optimization (NFSIC-med) acts as a baseline, allowing the effect of parameter optimization to be clearly
seen.",3. Experiments,[0],[0]
"For pedagogical reason, we consider the original HSIC test of Gretton et al. (2005) denoted by QHSIC, which is a quadratic-time test.",3. Experiments,[0],[0]
Nyström HSIC (NyHSIC) uses a Nyström approximation to the kernel matrices ofX and Y when computing the HSIC statistic.,3. Experiments,[0],[0]
"FHSIC is another variant of HSIC in which a random Fourier feature approximation (Rahimi & Recht, 2008) to the kernel is used.",3. Experiments,[0],[0]
NyHSIC,3. Experiments,[0],[0]
"and FHSIC are studied in Zhang et al. (2017) and can be computed in O(n), with quadratic dependency on the number of inducing points in NyHSIC, and quadratic dependency on the number of random features in FHSIC.",3. Experiments,[0],[0]
"Finally, the Randomized Dependence Coefficient (RDC) proposed in Lopez-Paz et al. (2013) is also considered.",3. Experiments,[0],[0]
The RDC can be seen as the primal form (with random Fourier features) of the kernel canonical correlation analysis of Bach & Jordan (2002) on copula-transformed data.,3. Experiments,[0],[0]
We consider RDC as a linear-time test even though preprocessing by an empirical copula transform costs O((dx,3. Experiments,[0],[0]
+ dy)n log n).,3. Experiments,[0],[0]
We use Gaussian kernel classes Kg and Lg for both X and Y in all the methods.,3. Experiments,[0],[0]
"Except NFSIC-opt, all other tests use full sample to conduct the independence test, where the Gaussian widths σx and σy are set according to the widely used median heuristic i.e., σx = median ({‖xi − xj‖2 | 1 ≤",3. Experiments,[0],[0]
"i < j ≤ n}), and σy is set in the same way using {yi}ni=1.",3. Experiments,[0],[0]
The J locations for NFSICmed are randomly drawn from the standard multivariate normal distribution in each trial.,3. Experiments,[0],[0]
"For a sample of size n, NFSIC-opt uses half the sample for parameter tuning, and the other disjoint half for the test.",3. Experiments,[0],[0]
We permute the sample 300 times in RDC1 and HSIC to simulate from the null distribution and compute the test threshold.,3. Experiments,[0],[0]
The null distributions for FHSIC and NyHSIC are given by a finite sum of weighted χ2(1) random variables given in Eq. 8 of Zhang et al. (2017).,3. Experiments,[0],[0]
"Unless stated otherwise, we set the test threshold of the two NFSIC tests to be the (1 − α)-quantile of χ2(J).",3. Experiments,[0],[0]
"To provide a fair comparison, we set J = 10, use 10 inducing points in NyHSIC, and 10 random Fourier features in FHSIC and RDC.
Optimization of NFSIC-opt The parameters of NFSIC-opt are σx, σy, and J locations of size (dx + dy)J .",3. Experiments,[0],[0]
We treat all the parameters as a long vector in R2+(dx+dy)J and use gradient ascent to optimize λ̂n/2.,3. Experiments,[0],[0]
We observe that initializing VJ by randomly picking J points from the training sample yields good performance.,3. Experiments,[0],[0]
"The regularization parameter γn in NFSIC is fixed to a small value, and is not optimized.",3. Experiments,[0],[0]
"It is worth emphasizing that the complexity of the optimization procedure is still linear-time.2
1We use a permutation test for RDC, following the authors’ implementation (https://github.com/lopezpaz/ randomized_dependence_coefficient, referred commit: b0ac6c0).
",3. Experiments,[0],[0]
2Our claim on linear runtime (with respect to n) is for the gradient ascent procedure to find a local optimum for θ.,3. Experiments,[0],[0]
"We do not
Since FSIC, NyHFSIC and RDC rely on a finitedimensional kernel approximation, these tests are consistent only if both the number of features increases with n.",3. Experiments,[0],[0]
"By constrast, the proposed NFSIC requires only n to go to infinity to achieve consistency i.e., J can be fixed.",3. Experiments,[0],[0]
We refer the reader to Appendix C for a brief investigation of the test power vs. increasing J .,3. Experiments,[0],[0]
The test power does not necessarily monotonically increase with J .,3. Experiments,[0],[0]
"We consider three toy problems.
",3.1. Toy Problems,[0],[0]
1.,3.1. Toy Problems,[0],[0]
Same Gaussian (SG).,3.1. Toy Problems,[0],[0]
"The two variables are independently drawn from the standard multivariate normal distribution i.e., X ∼ N (0, Idx) and Y ∼ N (0, Idy ) where Id is the d× d identity matrix.",3.1. Toy Problems,[0],[0]
"This problem represents a case in which H0 holds.
2.",3.1. Toy Problems,[0],[0]
Sinusoid (Sin).,3.1. Toy Problems,[0],[0]
Let pxy be the probability density of Pxy .,3.1. Toy Problems,[0],[0]
"In the Sinusoid problem, the dependency ofX and Y is characterized by (X,Y ) ∼ pxy(x, y) ∝",3.1. Toy Problems,[0],[0]
"1 + sin(ωx) sin(ωy), where the domains of X ,Y = (−π, π) and ω is the frequency of the sinusoid.",3.1. Toy Problems,[0],[0]
"As the frequency ω increases, the drawn sample becomes more similar to a sample drawn from Uniform((−π, π)2).",3.1. Toy Problems,[0],[0]
"That is, the higher ω, the harder to detect the dependency between X and Y .",3.1. Toy Problems,[0],[0]
This problem was studied in Sejdinovic et al. (2013).,3.1. Toy Problems,[0],[0]
Plots of the density for a few values of ω are shown in Figures 6 and 7 in the appendix.,3.1. Toy Problems,[0],[0]
The main characteristic of interest in this problem is the local change in the density function.,3.1. Toy Problems,[0],[0]
3.,3.1. Toy Problems,[0],[0]
Gaussian Sign (GSign).,3.1. Toy Problems,[0],[0]
"In this problem, Y = |Z|∏dxi=1 sgn(Xi), where X ∼ N (0, Idx), sgn(·) is the sign function, and Z ∼ N (0, 1) serves as a source of noise.",3.1. Toy Problems,[0],[0]
"The full interaction of X = (X1, . . .",3.1. Toy Problems,[0],[0]
", Xdx) is what makes the problem challenging.",3.1. Toy Problems,[0],[0]
"That is, Y is dependent on X , yet it is independent of any proper subset of {X1, . . .",3.1. Toy Problems,[0],[0]
", Xd}.",3.1. Toy Problems,[0],[0]
"Thus, simultaneous consideration of all the coordinates of X is required to successfully detect the dependency.
",3.1. Toy Problems,[0],[0]
We fix n = 4000 and vary the problem parameters.,3.1. Toy Problems,[0],[0]
"Each problem is repeated for 300 trials, and the sample is redrawn each time.",3.1. Toy Problems,[0],[0]
The significance level α is set to 0.05.,3.1. Toy Problems,[0],[0]
"The re-
claim a linear runtime to find a global optimum.
sults are shown in Figure 2.",3.1. Toy Problems,[0],[0]
"It can be seen that in the SG problem (Figure 2b) where H0 holds, all the tests achieve roughly correct type-I errors at α = 0.05.",3.1. Toy Problems,[0],[0]
"In particular, we point out that NFSIC-opt’s rejection rate is well controlled as the sample used for testing and the sample used for parameter tuning are independent.",3.1. Toy Problems,[0],[0]
"The rejection rate would have been much higher had we done the optimization and testing on the same sample (i.e., overfitting).",3.1. Toy Problems,[0],[0]
"In the Sin problem, NFSIC-opt achieves high test power for all considered ω = 1, . . .",3.1. Toy Problems,[0],[0]
", 6, highlighting its strength in detecting local changes in the joint density.",3.1. Toy Problems,[0],[0]
The performance of NFSIC-med is significantly lower than that of NFSIC-opt.,3.1. Toy Problems,[0],[0]
This phenomenon clearly emphasizes the importance of the optimization to place the locations at the relevant regions in X×Y .,3.1. Toy Problems,[0],[0]
"RDC has a remarkably high performance in both Sin and GSign (Figure 2c, 2d) despite no parameter tuning.",3.1. Toy Problems,[0],[0]
"The ability to simultaneously consider interacting features of NFSIC-opt is indicated by its superior test power in GSign, especially at the challenging settings of dx = 5, 6.
NFSIC vs. QHSIC.",3.1. Toy Problems,[0],[0]
We observe that NFSIC-opt outperforms the quadratic-time QHSIC in these two problems.,3.1. Toy Problems,[0],[0]
QHSIC is defined as the RKHS norm of the witness function u (see (2)).,3.1. Toy Problems,[0],[0]
"Intuitively, one can think of the RKHS norm as taking into account all the locations (v,w).",3.1. Toy Problems,[0],[0]
"By contrast, the proposed NFSIC evaluates the witness function at J locations.",3.1. Toy Problems,[0],[0]
"If the differences in pxy and pxpy are local (e.g., Sin problem), or there are interacting features (e.g., GSign problem), then only small regions in the space of (X,Y ) are relevant in detecting the difference of pxy and pxpy.",3.1. Toy Problems,[0],[0]
"In these cases, pinpointing exact test locations by the optimization of NFSIC performs well.",3.1. Toy Problems,[0],[0]
"On the other hand, taking into account all possible test locations as done implicitly in QHSIC also integrates over regions where the difference between pxy and pxpy is small, resulting in a weaker indication of dependence.",3.1. Toy Problems,[0],[0]
"Whether QHSIC is better than NFSIC depends heavily on the problem, and there is no one best answer.",3.1. Toy Problems,[0],[0]
"If the difference between pxy and pxpy is large only in localized regions, then the proposed linear time statistic has an advantage.",3.1. Toy Problems,[0],[0]
"If the difference is spatially diffuse, then QHSIC has an advantage.",3.1. Toy Problems,[0],[0]
"No existing work has proposed a procedure to optimally tune kernel parameters for QHSIC; by contrast, NFSIC has a clearly defined objective for parameter tuning.
",3.1. Toy Problems,[0],[0]
"To investigate the sample efficiency of all the tests, we fix dx = dy = 250 in SG, ω = 4 in Sin, dx = 4 in GSign, and increase n. Figure 3 shows the results.",3.1. Toy Problems,[0],[0]
The quadratic dependency on n in QHSIC makes it infeasible both in terms of memory and runtime to consider n larger than 6000 (Figure 3a).,3.1. Toy Problems,[0],[0]
"By constrast, although not the most time-efficient, NFSIC-opt has the highest sample-efficiency for GSign, and for Sin in the low-sample regime, significantly outperforming QHSIC.",3.1. Toy Problems,[0],[0]
"Despite the small additional overhead from the optimization, we are yet able to conduct an accurate test with n = 105, dx = dy = 250 in less than 100 seconds.",3.1. Toy Problems,[0],[0]
We observe in Figure 3b that the two NFSIC variants have correct type-I errors across all sample sizes.,3.1. Toy Problems,[0],[0]
We recall from Theorem 5 that the NFSIC test with random test locations will asymptotically reject H0 if it is false.,3.1. Toy Problems,[0],[0]
"A demonstration of this property is given in Figure 3c, where the test power of NFSIC-med eventually reaches 1 with n higher than 105.",3.1. Toy Problems,[0],[0]
"We now examine the performance of our proposed test on real problems.
",3.2. Real Problems,[0],[0]
Million Song Data (MSD),3.2. Real Problems,[0],[0]
"We consider a subset of the Million Song Data3 (Bertin-Mahieux et al., 2011), in which each song (X) out of 515,345 is represented by 90 features, of which 12 features are timbre average (over all segments) of the song, and 78 features are timbre covariance.",3.2. Real Problems,[0],[0]
Most of the songs are western commercial tracks from 1922 to 2011.,3.2. Real Problems,[0],[0]
The goal is to detect the dependency between each song and its year of release (Y ).,3.2. Real Problems,[0],[0]
"We set α = 0.01, and repeat for 300 trials where the full sample is randomly subsampled to n points in each trial.",3.2. Real Problems,[0],[0]
Other settings are the same as in the toy problems.,3.2. Real Problems,[0],[0]
"To make sure that the type-I error is correct, we use the permutation approach in the NFSIC tests to compute the threshold.",3.2. Real Problems,[0],[0]
Figure 4b shows the test powers as n increases from 500 to 2000.,3.2. Real Problems,[0],[0]
"To simulate the case whereH0 holds in the problem, we permute the sample to break the dependency of X and Y .",3.2. Real Problems,[0],[0]
"The results are shown in Figure 5 in the appendix.
",3.2. Real Problems,[0],[0]
"Evidently, NFSIC-opt has the highest test power among all
3Million Song Data subset: https://archive.ics.",3.2. Real Problems,[0],[0]
"uci.edu/ml/datasets/YearPredictionMSD.
the linear-time tests for all the sample sizes.",3.2. Real Problems,[0],[0]
Its test power is second to only QHSIC.,3.2. Real Problems,[0],[0]
We recall that NFSIC-opt uses half of the sample for parameter tuning.,3.2. Real Problems,[0],[0]
"Thus, at n = 500, the actual sample for testing is 250, which is relatively small.",3.2. Real Problems,[0],[0]
"The fact that there is a vast power gain from 0.4 (NFSIC-med) to 0.8 (NFSIC-opt) at n = 500 suggests that the optimization procedure can perform well even at a lower sample sizes.
",3.2. Real Problems,[0],[0]
Videos and Captions,3.2. Real Problems,[0],[0]
"Our last problem is based on the VideoStory46K4 dataset (Habibian et al., 2014).",3.2. Real Problems,[0],[0]
"The dataset contains 45,826 Youtube videos (X) of an average length of roughly one minute, and their corresponding text captions (Y ) uploaded by the users.",3.2. Real Problems,[0],[0]
Each video is represented as a dx = 2000 dimensional Fisher vector encoding of motion boundary histograms (MBH) descriptors of Wang & Schmid (2013).,3.2. Real Problems,[0],[0]
Each caption is represented as a bag of words with each feature being the frequency of one word.,3.2. Real Problems,[0],[0]
"After filtering only words which occur in at least six video captions, we obtain dy = 1878 words.",3.2. Real Problems,[0],[0]
We examine the test powers as n increases from 2000 to 8000.,3.2. Real Problems,[0],[0]
The results are given in Figure 4.,3.2. Real Problems,[0],[0]
The problem is sufficiently challenging that all linear-time tests achieve a low power at n = 2000.,3.2. Real Problems,[0],[0]
"QHSIC performs exceptionally well on this problem, achieving a maximum power throughout.",3.2. Real Problems,[0],[0]
"NFSIC-opt has the highest sample efficiency among the linear-time tests, showing that the optimization procedure is also practical in a high dimensional setting.
",3.2. Real Problems,[0],[0]
4VideoStory46K dataset: https://ivi.fnwi.uva.nl/ isis/mediamill/datasets/videostory.php.,3.2. Real Problems,[0],[0]
We thank the Gatsby Charitable Foundation for the financial support.,Acknowledgement,[0],[0]
"The major part of this work was carried out while Zoltán Szabó was a research associate at the Gatsby Computational Neuroscience Unit, University College London.",Acknowledgement,[0],[0]
"A new computationally efficient dependence measure, and an adaptive statistical test of independence, are proposed.",abstractText,[0],[0]
"The dependence measure is the difference between analytic embeddings of the joint distribution and the product of the marginals, evaluated at a finite set of locations (features).",abstractText,[0],[0]
"These features are chosen so as to maximize a lower bound on the test power, resulting in a test that is data-efficient, and that runs in linear time (with respect to the sample size n).",abstractText,[0],[0]
"The optimized features can be interpreted as evidence to reject the null hypothesis, indicating regions in the joint domain where the joint distribution and the product of the marginals differ most.",abstractText,[0],[0]
"Consistency of the independence test is established, for an appropriate choice of features.",abstractText,[0],[0]
"In real-world benchmarks, independence tests using the optimized features perform comparably to the state-of-the-art quadratic-time HSIC test, and outperform competing O(n) and O(n log n) tests.",abstractText,[0],[0]
An Adaptive Test of Independence with Analytic Kernel Embeddings,title,[0],[0]
There is a fundamental tension in decision making between choosing the action that has highest expected utility and avoiding “starving” the other actions.,1. Introduction,[0],[0]
"The issue arises in the context of the exploration–exploitation dilemma (Thrun, 1992), non-stationary decision problems (Sutton, 1990), and when interpreting observed decisions (Baker et al., 2007).
",1. Introduction,[0],[0]
"In reinforcement learning, an approach to addressing the tension is the use of softmax operators for value-function optimization, and softmax policies for action selection.",1. Introduction,[0],[0]
"Examples include value-based methods such as SARSA (Rummery & Niranjan, 1994) or expected SARSA (Sutton & Barto, 1998; Van Seijen et al., 2009), and policy-search methods such as REINFORCE (Williams, 1992).
",1. Introduction,[0],[0]
"1Brown University, USA.",1. Introduction,[0],[0]
"Correspondence to: Kavosh Asadi <kavosh@brown.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"An ideal softmax operator is a parameterized set of operators that:
1. has parameter settings that allow it to approximate maximization arbitrarily accurately to perform reward-seeking behavior;
2. is a non-expansion for all parameter settings ensuring convergence to a unique fixed point;
3. is differentiable to make it possible to improve via gradient-based optimization; and
4.",1. Introduction,[0],[0]
"avoids the starvation of non-maximizing actions.
",1. Introduction,[0],[0]
"Let X = x1, . . .",1. Introduction,[0],[0]
", xn be a vector of values.",1. Introduction,[0],[0]
"We define the following operators:
max(X) = max i∈{1,...,n} xi ,
mean(X) = 1
n
n∑
i=1
xi ,
eps (X) = mean(X) + (1− ) max(X) ,
boltzβ(X) =",1. Introduction,[0],[0]
∑n i=1,1. Introduction,[0],[0]
"xi e βxi
∑n i=1 e βxi .
",1. Introduction,[0],[0]
"The first operator, max(X), is known to be a non-expansion (Littman & Szepesvári, 1996).",1. Introduction,[0],[0]
"However, it is non-differentiable (Property 3), and ignores non-maximizing selections (Property 4).
",1. Introduction,[0],[0]
"The next operator, mean(X), computes the average of its inputs.",1. Introduction,[0],[0]
"It is differentiable and, like any operator that takes a fixed convex combination of its inputs, is a non-expansion.",1. Introduction,[0],[0]
"However, it does not allow for maximization (Property 1).
",1. Introduction,[0],[0]
"The third operator eps (X), commonly referred to as epsilon greedy (Sutton & Barto, 1998), interpolates between max and mean.",1. Introduction,[0],[0]
"The operator is a non-expansion, because it is a convex combination of two non-expansion operators.",1. Introduction,[0],[0]
"But it is non-differentiable (Property 3).
",1. Introduction,[0],[0]
The Boltzmann operator boltzβ(X) is differentiable.,1. Introduction,[0],[0]
"It also approximates max as β → ∞, and mean as β → 0.",1. Introduction,[0],[0]
"However, it is not a non-expansion (Property 2), and therefore, prone to misbehavior as will be shown in the next section.
",1. Introduction,[0],[0]
"In the following section, we provide a simple example illustrating why the non-expansion property is important, especially in the context of planning and on-policy learning.",1. Introduction,[0],[0]
We then present a new softmax operator that is similar to the Boltzmann operator yet is a non-expansion.,1. Introduction,[0],[0]
"We prove several critical properties of this new operator, introduce a new softmax policy, and present empirical results.",1. Introduction,[0],[0]
We first show that boltzβ can lead to problematic behavior.,2. Boltzmann Misbehaves,[0],[0]
"To this end, we ran SARSA with Boltzmann softmax policy (Algorithm 1) on the MDP shown in Figure 1.",2. Boltzmann Misbehaves,[0],[0]
The edges are labeled with a transition probability (unsigned) and a reward number (signed).,2. Boltzmann Misbehaves,[0],[0]
"Also, state s2 is a terminal state, so we only consider two action values, namely Q̂(s1, a) and Q̂(s2, b).",2. Boltzmann Misbehaves,[0],[0]
"Recall that the Boltzmann softmax policy assigns the following probability to each action:
π(a|s) = e βQ̂(s,a)
∑ a e βQ̂(s,a) .
",2. Boltzmann Misbehaves,[0],[0]
"Algorithm 1 SARSA with Boltzmann softmax policy Input: initial Q̂(s, a) ∀s ∈",2. Boltzmann Misbehaves,[0],[0]
S ∀a ∈,2. Boltzmann Misbehaves,[0],[0]
"A, α, and β for each episode do
Initialize s a ∼ Boltzmann with parameter β repeat
Take action a, observe r, s′ a ′",2. Boltzmann Misbehaves,[0],[0]
"∼ Boltzmann with parameter β Q̂(s, a)← Q̂(s, a) + α",2. Boltzmann Misbehaves,[0],[0]
"[ r + γQ̂(s′, a′)− Q̂(s, a) ]
s← s′ , a← a′ until s is terminal
end for
",2. Boltzmann Misbehaves,[0],[0]
"In Figure 2, we plot state–action value estimates at the end of each episode of a single run (smoothed by averaging over ten consecutive points).",2. Boltzmann Misbehaves,[0],[0]
We set α = .1 and β = 16.55.,2. Boltzmann Misbehaves,[0],[0]
"The value estimates are unstable.
",2. Boltzmann Misbehaves,[0],[0]
"SARSA is known to converge in the tabular setting using -greedy exploration (Littman & Szepesvári, 1996), under decreasing exploration (Singh et al., 2000), and to a region in the function-approximation setting (Gordon, 2001).",2. Boltzmann Misbehaves,[0],[0]
"There are also variants of the SARSA update rule that converge more generally (Perkins & Precup, 2002; Baird & Moore, 1999; Van Seijen et al., 2009).",2. Boltzmann Misbehaves,[0],[0]
"However, this example is the first, to our knowledge, to show that SARSA fails to converge in the tabular setting with Boltzmann policy.",2. Boltzmann Misbehaves,[0],[0]
The next section provides background for our analysis of the example.,2. Boltzmann Misbehaves,[0],[0]
"A Markov decision process (Puterman, 1994), or MDP, is specified by the tuple 〈S,A,R,P, γ〉, where S is the set of states and A is the set of actions.",3. Background,[0],[0]
"The functions R : S ×A → R and P : S × A× S → [0, 1] denote the reward and transition dynamics of the MDP.",3. Background,[0],[0]
"Finally, γ ∈",3. Background,[0],[0]
"[0, 1), the discount rate, determines the relative importance of immediate reward as opposed to the rewards received in the future.
",3. Background,[0],[0]
A typical approach to finding a good policy is to estimate how good it is to be in a particular state—the state value function.,3. Background,[0],[0]
"The value of a particular state s given a policy π and initial action a is written Qπ(s, a).",3. Background,[0],[0]
"We define the optimal value of a state–action pair Q?(s, a) =",3. Background,[0],[0]
"maxπ Qπ(s, a).",3. Background,[0],[0]
"It is possible to defineQ?(s, a) recursively and as a function of the optimal value of the other state–action pairs:
Q?(s, a) =",3. Background,[0],[0]
"R(s, a)+ ∑
s′∈S γ P(s, a, s′) max a′",3. Background,[0],[0]
"Q?(s′, a′) .
",3. Background,[0],[0]
"Bellman equations, such as the above, are at the core of many reinforcement-learning algorithms such as Value Iteration (Bellman, 1957).",3. Background,[0],[0]
"The algorithm computes the
value of the best policy in an iterative fashion:
Q̂(s, a)←",3. Background,[0],[0]
"R(s, a) +",3. Background,[0],[0]
"γ ∑
s′∈S P(s, a, s′) max a′ Q̂(s′, a′).
",3. Background,[0],[0]
"Regardless of its initial value, Q̂ will converge to Q∗.
Littman & Szepesvári (1996) generalized this algorithm by replacing the max operator by any arbitrary operator ⊗ , resulting in the generalized value iteration (GVI) algorithm with the following update rule:
Q̂(s, a)←",3. Background,[0],[0]
"R(s, a)+γ ∑
s′∈S γP(s, a, s′)
⊗
a′
Q̂(s′, a′).",3. Background,[0],[0]
"(1)
Algorithm 2 GVI algorithm Input: initial Q̂(s, a) ∀s ∈ S",3. Background,[0],[0]
∀a ∈,3. Background,[0],[0]
"A and δ ∈ R+ repeat
diff← 0 for each s ∈ S do
for each a ∈",3. Background,[0],[0]
"A do Qcopy ← Q̂(s, a) Q̂(s, a)←∑s′∈S R(s, a, s′)
+ γP(s, a, s′)⊗ Q̂(s′, .)",3. Background,[0],[0]
"diff← max { diff, |Qcopy − Q̂(s, a)| }
end for end for
until diff < δ
Crucially, convergence of GVI to a unique fixed point follows if operator ⊗ is a non-expansion with respect to the infinity norm: ∣∣∣ ⊗
a
Q̂(s, a)− ⊗
a
Q̂′(s, a)",3. Background,[0],[0]
∣∣∣ ≤,3. Background,[0],[0]
"max
a
∣∣∣Q̂(s, a)− Q̂′(s, a) ∣∣∣,
for any Q̂, Q̂′ and s. As mentioned earlier, the max operator is known to be a non-expansion, as illustrated in Figure 3.",3. Background,[0],[0]
mean and eps operators are also non-expansions.,3. Background,[0],[0]
"Therefore, each of these operators can play the role of ⊗ in GVI, resulting in convergence to the corresponding unique
fixed point.",3. Background,[0],[0]
"However, the Boltzmann softmax operator, boltzβ , is not a non-expansion (Littman, 1996).",3. Background,[0],[0]
Note that we can relate GVI to SARSA by observing that SARSA’s update is a stochastic implementation of GVI’s update.,3. Background,[0],[0]
"Under a Boltzmann softmax policy π, the target of the (expected) SARSA update is the following:
E π
[ r + γQ̂(s′, a′) ∣∣s, a ] =
R(s, a) + γ ∑
s′∈S P(s, a, s′)
∑
a′∈A π(a′|s′)Q̂(s′, a′) ︸",3. Background,[0],[0]
"︷︷ ︸ boltzβ ( Q̂(s′,·) ) .
",3. Background,[0],[0]
This matches the GVI update (1) when ⊗ = boltzβ .,3. Background,[0],[0]
"Although it has been known for a long time that the Boltzmann operator is not a non-expansion (Littman, 1996), we are not aware of a published example of an MDP for which two distinct fixed points exist.",4. Boltzmann Has Multiple Fixed Points,[0],[0]
"The MDP presented in Figure 1 is the first example where, as shown in Figure 4, GVI under boltzβ has two distinct fixed points.",4. Boltzmann Has Multiple Fixed Points,[0],[0]
"We also show, in Figure 5, a vector field visualizing GVI updates under boltzβ=16.55.",4. Boltzmann Has Multiple Fixed Points,[0],[0]
The updates can move the current estimates farther from the fixed points.,4. Boltzmann Has Multiple Fixed Points,[0],[0]
The behavior of SARSA (Figure 2) results from the algorithm stochastically bouncing back and forth between the two fixed points.,4. Boltzmann Has Multiple Fixed Points,[0],[0]
"When the learning algorithm performs a sequence of noisy updates, it moves from a fixed point to the other.",4. Boltzmann Has Multiple Fixed Points,[0],[0]
"As we will show later, planning will also progress extremely slowly near the fixed points.",4. Boltzmann Has Multiple Fixed Points,[0],[0]
The lack of the non-expansion property leads to multiple fixed points and ultimately a misbehavior in learning and planning.,4. Boltzmann Has Multiple Fixed Points,[0],[0]
"We advocate for an alternative softmax operator defined as follows:
mmω(X) = log( 1n
∑n i=1 e ωxi)
ω ,
which can be viewed as a particular instantiation of the quasi-arithmetic mean (Beliakov et al., 2016).",5. Mellowmax and its Properties,[0],[0]
"It can also
be derived from information theoretical principles as a way of regularizing policies with a cost function defined by KL divergence (Todorov, 2006; Rubin et al., 2012; Fox et al., 2016).",5. Mellowmax and its Properties,[0],[0]
"Note that the operator has previously been utilized in other areas, such as power engineering (Safak, 1993).
",5. Mellowmax and its Properties,[0],[0]
"We show that mmω , which we refer to as mellowmax, has the desired properties and that it compares quite favorably to boltzβ in practice.",5. Mellowmax and its Properties,[0],[0]
"We prove that mmω is a non-expansion (Property 2), and therefore, GVI and SARSA under mmω are guaranteed to converge to a unique fixed point.
",5.1. Mellowmax is a Non-Expansion,[0],[0]
"Let X = x1, . . .",5.1. Mellowmax is a Non-Expansion,[0],[0]
", xn and Y = y1, . . .",5.1. Mellowmax is a Non-Expansion,[0],[0]
", yn be two vectors of values.",5.1. Mellowmax is a Non-Expansion,[0],[0]
"Let ∆i = xi − yi for i ∈ {1, . . .",5.1. Mellowmax is a Non-Expansion,[0],[0]
", n} be the difference of the ith components of the two vectors.",5.1. Mellowmax is a Non-Expansion,[0],[0]
"Also, let i∗ be the index with the maximum component-wise difference, i∗ = argmaxi ∆i.",5.1. Mellowmax is a Non-Expansion,[0],[0]
"For simplicity, we assume that i∗ is unique and ω > 0.",5.1. Mellowmax is a Non-Expansion,[0],[0]
"Also, without loss of generality, we assume that xi∗ − yi∗ ≥ 0.",5.1. Mellowmax is a Non-Expansion,[0],[0]
"It follows that:
∣∣mmω(X)−mmω(Y) ∣∣
= ∣∣ log( 1
n
n∑
i=1
",5.1. Mellowmax is a Non-Expansion,[0],[0]
"eωxi)/ω − log( 1 n
n∑
i=1
eωyi)/ω ∣∣
= ∣∣ log 1 n
∑n i=1",5.1. Mellowmax is a Non-Expansion,[0],[0]
"e ωxi
1 n",5.1. Mellowmax is a Non-Expansion,[0],[0]
∑n i=1,5.1. Mellowmax is a Non-Expansion,[0],[0]
"e
ωyi /ω ∣∣
= ∣∣ log
∑n i=1",5.1. Mellowmax is a Non-Expansion,[0],[0]
"e
ω",5.1. Mellowmax is a Non-Expansion,[0],[0]
( yi+∆i ),5.1. Mellowmax is a Non-Expansion,[0],[0]
∑n i=1,5.1. Mellowmax is a Non-Expansion,[0],[0]
"e ωyi /ω ∣∣
≤ ∣∣ log
∑n i=1",5.1. Mellowmax is a Non-Expansion,[0],[0]
"e
ω ( yi+∆i∗ )",5.1. Mellowmax is a Non-Expansion,[0],[0]
∑n i=1,5.1. Mellowmax is a Non-Expansion,[0],[0]
"e ωyi /ω ∣∣
= ∣∣ log e
ω∆i∗ ∑n i=1",5.1. Mellowmax is a Non-Expansion,[0],[0]
"e ωyi
∑n i=1",5.1. Mellowmax is a Non-Expansion,[0],[0]
"e
ωyi /ω ∣∣
= ∣∣ log(eω∆i∗ )/ω ∣∣ = ∣∣∆i∗ ∣∣ = max i ∣∣xi − yi ∣∣ ,
allowing us to conclude that mellowmax is a non-expansion under the infinity norm.",5.1. Mellowmax is a Non-Expansion,[0],[0]
Mellowmax includes parameter settings that allow for maximization (Property 1) as well as for minimization.,5.2. Maximization,[0],[0]
"In particular, as ω goes to infinity, mmω acts like max.
Let m = max(X) and let W = |{xi = m|i ∈",5.2. Maximization,[0],[0]
"{1, . . .",5.2. Maximization,[0],[0]
", n}}|.",5.2. Maximization,[0],[0]
"Note that W ≥ 1 is the number of maximum values (“winners”) in X. Then:
lim ω→∞ mmω(X) = lim ω→∞
log( 1n ∑n i=1 e ωxi)
ω
= lim ω→∞
log( 1ne ωm",5.2. Maximization,[0],[0]
∑n i=1,5.2. Maximization,[0],[0]
"e ω(xi−m))
",5.2. Maximization,[0],[0]
"ω
= lim ω→∞
log( 1ne ωmW )
ω
= lim ω→∞ log(eωm)− log(n) + log(W ) ω
= m+ lim ω→∞",5.2. Maximization,[0],[0]
− log(n),5.2. Maximization,[0],[0]
"+ log(W ) ω = m = max(X) .
",5.2. Maximization,[0],[0]
"That is, the operator acts more and more like pure maximization as the value of ω is increased.",5.2. Maximization,[0],[0]
"Conversely, as ω goes to −∞, the operator approaches the minimum.",5.2. Maximization,[0],[0]
"We can take the derivative of mellowmax with respect to each one of the arguments xi and for any non-zero ω:
∂mmω(X) ∂xi = eωxi∑n i=1 e ωxi ≥ 0 .
",5.3. Derivatives,[0],[0]
"Note that the operator is non-decreasing in each component of X.
Moreover, we can take the derivative of mellowmax with respect to ω.",5.3. Derivatives,[0],[0]
We define nω(X) = log(,5.3. Derivatives,[0],[0]
1n ∑n i=1,5.3. Derivatives,[0],[0]
"e
ωxi) and dω(X)",5.3. Derivatives,[0],[0]
= ω.,5.3. Derivatives,[0],[0]
"Then:
∂nω(X)",5.3. Derivatives,[0],[0]
∂ω,5.3. Derivatives,[0],[0]
"=
∑n i=1",5.3. Derivatives,[0],[0]
"xie ωxi
∑n i=1 e ωxi and ∂dω(X) ∂ω = 1 ,
and so:
∂mmω(X) ∂ω
= ∂nω(X)",5.3. Derivatives,[0],[0]
"∂ω dω(X)− nω(X) ∂dω(X) ∂ω
dω(X)2 ,
ensuring differentiablity of the operator (Property 3).",5.3. Derivatives,[0],[0]
"Because of the division by ω in the definition of mmω , the parameter ω cannot be set to zero.",5.4. Averaging,[0],[0]
"However, we can examine the behavior of mmω as ω approaches zero and show that the operator computes an average in the limit.
",5.4. Averaging,[0],[0]
"Since both the numerator and denominator go to zero as ω goes to zero, we will use L’Hôpital’s rule and the derivative given in the previous section to derive the value in the limit:
lim ω→0 mmω(X) =",5.4. Averaging,[0],[0]
"lim ω→0
log( 1n ∑n i=1 e ωxi)
ω
L’Hôpital = lim
ω→0
1 n",5.4. Averaging,[0],[0]
∑n i=1,5.4. Averaging,[0],[0]
"xie ωxi
1 n",5.4. Averaging,[0],[0]
"∑n i=1 e ωxi
= 1
n
n∑
i=1
xi = mean(X) .
",5.4. Averaging,[0],[0]
"That is, as ω gets closer to zero, mmω(X) approaches the mean of the values in X.",5.4. Averaging,[0],[0]
"As described, mmω computes a value for a list of numbers somewhere between its minimum and maximum.",6. Maximum Entropy Mellowmax Policy,[0],[0]
"However, it is often useful to actually provide a probability distribution over the actions such that (1) a non-zero probability mass is assigned to each action, and (2) the resulting expected value equals the computed value.",6. Maximum Entropy Mellowmax Policy,[0],[0]
"Such a probability distribution can then be used for action selection in algorithms such as SARSA.
",6. Maximum Entropy Mellowmax Policy,[0],[0]
"In this section, we address the problem of identifying such a probability distribution as a maximum entropy problem—over all distributions that satisfy the properties above, pick the one that maximizes information entropy (Cover & Thomas, 2006; Peters et al., 2010).",6. Maximum Entropy Mellowmax Policy,[0],[0]
"We formally define the maximum entropy mellowmax policy of a state s as:
πmm(s) = argmin π
∑ a∈A π(a|s) log ( π(a|s) )",6. Maximum Entropy Mellowmax Policy,[0],[0]
"(2)
subject to { ∑ a∈A π(a|s)Q̂(s, a) = mmω(Q̂(s, .))
π(a|s) ≥ 0∑ a∈A π(a|s) = 1 .
",6. Maximum Entropy Mellowmax Policy,[0],[0]
"Note that this optimization problem is convex and can be solved reliably using any numerical convex optimization library.
",6. Maximum Entropy Mellowmax Policy,[0],[0]
"One way of finding the solution, which leads to an interesting policy form, is to use the method of Lagrange
multipliers.",6. Maximum Entropy Mellowmax Policy,[0],[0]
"Here, the Lagrangian is:
L(π, λ1, λ2) = ∑
a∈A π(a|s) log
( π(a|s) )
",6. Maximum Entropy Mellowmax Policy,[0],[0]
"−λ1 (∑
a∈A π(a|s)− 1
)
−λ2 (∑
a∈A π(a|s)Q̂(s, a)−mmω
( Q̂(s, .) )) .
",6. Maximum Entropy Mellowmax Policy,[0],[0]
"Taking the partial derivative of the Lagrangian with respect to each π(a|s) and setting them to zero, we obtain:
∂L ∂π(a|s) = log ( π(a|s) )",6. Maximum Entropy Mellowmax Policy,[0],[0]
"+1−λ1−λ2Q̂(s, a) = 0 ∀",6. Maximum Entropy Mellowmax Policy,[0],[0]
"a ∈ A .
",6. Maximum Entropy Mellowmax Policy,[0],[0]
"These |A| equations, together with the two linear constraints in (2), form |A| + 2 equations to constrain the |A| + 2 variables π(a|s) ∀a ∈ A and the two Lagrangian multipliers λ1 and λ2.
",6. Maximum Entropy Mellowmax Policy,[0],[0]
"Solving this system of equations, the probability of taking an action under the maximum entropy mellowmax policy has the form:
πmm(a|s) = eβQ̂(s,a)∑ a∈A",6. Maximum Entropy Mellowmax Policy,[0],[0]
"e βQ̂(s,a) ∀a ∈ A ,
where β is a value for which:
∑ a∈A eβ ( Q̂(s,a)−mmωQ̂(s,.) )",6. Maximum Entropy Mellowmax Policy,[0],[0]
"( Q̂(s, a)−mmωQ̂(s, .) )",6. Maximum Entropy Mellowmax Policy,[0],[0]
"= 0 .
",6. Maximum Entropy Mellowmax Policy,[0],[0]
The argument for the existence of a unique root is simple.,6. Maximum Entropy Mellowmax Policy,[0],[0]
"As β → ∞ the term corresponding to the best action dominates, and so, the function is positive.",6. Maximum Entropy Mellowmax Policy,[0],[0]
"Conversely, as β → −∞ the term corresponding to the action with lowest utility dominates, and so the function is negative.",6. Maximum Entropy Mellowmax Policy,[0],[0]
"Finally, by taking the derivative, it is clear that the function is monotonically increasing, allowing us to conclude that there exists only a single root.",6. Maximum Entropy Mellowmax Policy,[0],[0]
"Therefore, we can find β easily using any root-finding algorithm.",6. Maximum Entropy Mellowmax Policy,[0],[0]
"In particular, we use Brent’s method (Brent, 2013) available in the Numpy library of Python.
",6. Maximum Entropy Mellowmax Policy,[0],[0]
"This policy has the same form as Boltzmann softmax, but with a parameter β whose value depends indirectly on ω.",6. Maximum Entropy Mellowmax Policy,[0],[0]
"This mathematical form arose not from the structure of mmω , but from maximizing the entropy.",6. Maximum Entropy Mellowmax Policy,[0],[0]
"One way to view the use of the mellowmax operator, then, is as a form of Boltzmann policy with a temperature parameter chosen adaptively in each state to ensure that the non-expansion property holds.
",6. Maximum Entropy Mellowmax Policy,[0],[0]
"Finally, note that the SARSA update under the maximum entropy mellowmax policy could be thought of as a
stochastic implementation of the GVI update under the mmω operator:
E πmm
[ r + γQ̂(s′, a′) ∣∣s, a ] =
∑ s′∈S",6. Maximum Entropy Mellowmax Policy,[0],[0]
"R(s, a, s′) + γP(s, a, s′) ∑ a′∈A πmm(a ′|s′)Q̂(s′, a′) ]
︸ ︷︷ ︸ mmω",6. Maximum Entropy Mellowmax Policy,[0],[0]
"( Q̂(s′,.) )
",6. Maximum Entropy Mellowmax Policy,[0],[0]
due to the first constraint of the convex optimization problem (2).,6. Maximum Entropy Mellowmax Policy,[0],[0]
"Because mellowmax is a non-expansion, SARSA with the maximum entropy mellowmax policy is guaranteed to converge to a unique fixed point.",6. Maximum Entropy Mellowmax Policy,[0],[0]
"Note also that, similar to other variants of SARSA, the algorithm simply bootstraps using the value of the next state while implementing the new policy.",6. Maximum Entropy Mellowmax Policy,[0],[0]
We observed that in practice computing mellowmax can yield overflow if the exponentiated values are large.,7. Experiments on MDPs,[0],[0]
"In this case, we can safely shift the values by a constant before exponentiating them due to the following equality:
log( 1n ∑n i=1 e ωxi)
ω =",7. Experiments on MDPs,[0],[0]
"c+
log( 1n ∑n i=1",7. Experiments on MDPs,[0],[0]
"e ω(xi−c))
",7. Experiments on MDPs,[0],[0]
"ω .
",7. Experiments on MDPs,[0],[0]
"A value of c = maxi xi usually avoids overflow.
",7. Experiments on MDPs,[0],[0]
We repeat the experiment from Figure 5 for mellowmax with ω = 16.55 to get a vector field.,7. Experiments on MDPs,[0],[0]
"The result, presented in Figure 6, show a rapid and steady convergence towards the unique fixed point.",7. Experiments on MDPs,[0],[0]
"As a result, GVI under mmω can terminate significantly faster than GVI under boltzβ , as illustrated in Figure 7.
",7. Experiments on MDPs,[0],[0]
We present three additional experiments.,7. Experiments on MDPs,[0],[0]
The first experiment investigates the behavior of GVI with the softmax operators on randomly generated MDPs.,7. Experiments on MDPs,[0],[0]
The second experiment evaluates the softmax policies when used in SARSA with a tabular representation.,7. Experiments on MDPs,[0],[0]
"The last
experiment is a policy gradient experiment where a deep neural network, with a softmax output layer, is used to directly represent the policy.",7. Experiments on MDPs,[0],[0]
The example in Figure 1 was created carefully by hand.,7.1. Random MDPs,[0],[0]
It is interesting to know whether such examples are likely to be encountered naturally.,7.1. Random MDPs,[0],[0]
"To this end, we constructed 200 MDPs as follows: We sampled |S| from {2, 3, ..., 10} and |A| from {2, 3, 4, 5} uniformly at random.",7.1. Random MDPs,[0],[0]
"We initialized the transition probabilities by sampling uniformly from [0, .01].",7.1. Random MDPs,[0],[0]
"We then added to each entry, with probability 0.5, Gaussian noise with mean 1 and variance 0.1.",7.1. Random MDPs,[0],[0]
"We next added, with probability 0.1, Gaussian noise with mean 100 and variance 1.",7.1. Random MDPs,[0],[0]
"Finally, we normalized the raw values to ensure that we get a transition matrix.",7.1. Random MDPs,[0],[0]
"We did a similar process for rewards, with the difference that we divided each entry by the maximum entry and multiplied by 0.5 to ensure that Rmax = 0.5 .
",7.1. Random MDPs,[0],[0]
We measured the failure rate of GVI under boltzβ and mmω by stopping GVI when it did not terminate in 1000 iterations.,7.1. Random MDPs,[0],[0]
We also computed the average number of iterations needed before termination.,7.1. Random MDPs,[0],[0]
A summary of results is presented in the table below.,7.1. Random MDPs,[0],[0]
"Mellowmax outperforms Boltzmann based on the three measures provided below.
",7.1. Random MDPs,[0],[0]
"MDPs, no terminate MDPs, > 1 fixed points average iterations
boltzβ 8 of 200 3 of 200 231.65 mmω 0 0 201.32",7.1. Random MDPs,[0],[0]
We evaluated SARSA on the multi-passenger taxi domain introduced by Dearden et al. (1998).,7.2. Multi-passenger Taxi Domain,[0],[0]
"(See Figure 8.)
",7.2. Multi-passenger Taxi Domain,[0],[0]
One challenging aspect of this domain is that it admits many locally optimal policies.,7.2. Multi-passenger Taxi Domain,[0],[0]
Exploration needs to be set carefully to avoid either over-exploring or under-exploring the state space.,7.2. Multi-passenger Taxi Domain,[0],[0]
"Note also that Boltzmann softmax performs remarkably well on this domain, outperforming sophisticated Bayesian
reinforcement-learning algorithms (Dearden et al., 1998).",7.2. Multi-passenger Taxi Domain,[0],[0]
"As shown in Figure 9, SARSA with the epsilon-greedy policy performs poorly.",7.2. Multi-passenger Taxi Domain,[0],[0]
"In fact, in our experiment, the algorithm rarely was able to deliver all the passengers.",7.2. Multi-passenger Taxi Domain,[0],[0]
"However, SARSA with Boltzmann softmax and SARSA with the maximum entropy mellowmax policy achieved significantly higher average reward.",7.2. Multi-passenger Taxi Domain,[0],[0]
"Maximum entropy mellowmax policy is no worse than Boltzmann softmax, here, suggesting that the greater stability does not come at the expense of less effective exploration.",7.2. Multi-passenger Taxi Domain,[0],[0]
"In this section, we evaluate the use of the maximum entropy mellowmax policy in the context of a policy-gradient algorithm.",7.3. Lunar Lander Domain,[0],[0]
"Specifically, we represent a policy by a neural network (discussed below) that maps from states to probabilities over actions.",7.3. Lunar Lander Domain,[0],[0]
A common choice for the activation function of the last layer is the Boltzmann softmax policy.,7.3. Lunar Lander Domain,[0],[0]
"In contrast, we can use maximum entropy mellowmax policy, presented in Section 6, by treating the inputs of the activation function as Q̂ values.
",7.3. Lunar Lander Domain,[0],[0]
"We used the lunar lander domain, from OpenAI Gym (Brockman et al., 2016) as our benchmark.",7.3. Lunar Lander Domain,[0],[0]
A screenshot of the domain is presented in Figure 10.,7.3. Lunar Lander Domain,[0],[0]
"This domain has a continuous state space with 8 dimensions, namely x-y coordinates, x-y velocities, angle and angular velocities, and leg-touchdown sensors.",7.3. Lunar Lander Domain,[0],[0]
There are 4 discrete actions to control 3 engines.,7.3. Lunar Lander Domain,[0],[0]
"The reward is +100 for a safe landing in the designated area, and −100 for a crash.",7.3. Lunar Lander Domain,[0],[0]
There is a small shaping reward for approaching the landing area.,7.3. Lunar Lander Domain,[0],[0]
Using the engines results in a negative reward.,7.3. Lunar Lander Domain,[0],[0]
An episode finishes when the spacecraft crashes or lands.,7.3. Lunar Lander Domain,[0],[0]
"Solving the domain is defined as maintaining mean episode return higher than 200 in 100 consecutive episodes.
",7.3. Lunar Lander Domain,[0],[0]
"The policy in our experiment is represented by a neural network with a hidden layer comprised of 16 units with RELU activation functions, followed by a second layer with 16 units and softmax activation functions.",7.3. Lunar Lander Domain,[0],[0]
We used REINFORCE to train the network.,7.3. Lunar Lander Domain,[0],[0]
"A batch episode size
of 10 was used, as we had stability issues with smaller episode batch sizes.",7.3. Lunar Lander Domain,[0],[0]
"We used the Adam algorithm (Kingma & Ba, 2014) with α = 0.005 and the other parameters as suggested by the paper.",7.3. Lunar Lander Domain,[0],[0]
"We used Keras (Chollet, 2015) and Theano (Team et al., 2016) to implement the neural network architecture.",7.3. Lunar Lander Domain,[0],[0]
"For each softmax policy, we present in Figure 11 the learning curves for different values of their free parameter.",7.3. Lunar Lander Domain,[0],[0]
We further plot average return over all 40000 episodes.,7.3. Lunar Lander Domain,[0],[0]
Mellowmax outperforms Boltzmann at its peak.,7.3. Lunar Lander Domain,[0],[0]
"Softmax operators play an important role in sequential decision-making algorithms.
",8. Related Work,[0],[0]
"In model-free reinforcement learning, they can help strike
a balance between exploration (mean) and exploitation (max).",8. Related Work,[0],[0]
"Decision rules based on epsilon-greedy and Boltzmann softmax, while very simple, often perform surprisingly well in practice, even outperforming more advanced exploration techniques (Kuleshov & Precup, 2014) that require significant approximation for complex domains.",8. Related Work,[0],[0]
"When learning “on policy”, exploration steps can (Rummery & Niranjan, 1994) and perhaps should (John, 1994) become part of the value-estimation process itself.",8. Related Work,[0],[0]
"On-policy algorithms like SARSA can be made to converge to optimal behavior in the limit when the exploration rate and the update operator is gradually moved toward max (Singh et al., 2000).",8. Related Work,[0],[0]
"Our use of softmax in learning updates reflects this point of view and shows that the value-sensitive behavior of Boltzmann exploration can be maintained even as updates are made stable.
",8. Related Work,[0],[0]
Analyses of the behavior of human subjects in choice experiments very frequently use softmax.,8. Related Work,[0],[0]
"Sometimes referred to in the literature as logit choice (Stahl & Wilson, 1994), it forms an important part of the most accurate predictor of human decisions in normal-form games (Wright & Leyton-Brown, 2010), quantal level-k reasoning (QLk).",8. Related Work,[0],[0]
Softmax-based fixed points play a crucial role in this work.,8. Related Work,[0],[0]
"As such, mellowmax could potentially make a good replacement.
",8. Related Work,[0],[0]
"Algorithms for inverse reinforcement learning (IRL), the problem of inferring reward functions from observed behavior (Ng & Russell, 2000), frequently use a Boltzmann operator to avoid assigning zero probability to non-optimal actions and hence assessing an observed sequence as impossible.",8. Related Work,[0],[0]
"Such methods include Bayesian IRL (Ramachandran & Amir, 2007), natural gradient IRL (Neu & Szepesvári, 2007), and maximum likelihood IRL (Babes et al., 2011).",8. Related Work,[0],[0]
"Given the recursive nature of value defined in these problems, mellowmax could be a more stable and efficient choice.
",8. Related Work,[0],[0]
"In linearly solvable MDPs (Todorov, 2006), an operator similar to mellowmax emerges when using an alternative characterization for cost of action selection in MDPs.",8. Related Work,[0],[0]
Inspired by this work Fox et al. (2016) introduced an off-policy G-learning algorithm that uses the operator to perform value-function updates.,8. Related Work,[0],[0]
"Instead of performing off-policy updates, we introduced a convergent variant of SARSA with Boltzmann policy and a state-dependent temperature parameter.",8. Related Work,[0],[0]
This is in contrast to Fox et al. (2016) where an epsilon greedy behavior policy is used.,8. Related Work,[0],[0]
We proposed the mellowmax operator as an alternative to the Boltzmann softmax operator.,9. Conclusion and Future Work,[0],[0]
We showed that mellowmax has several desirable properties and that it works favorably in practice.,9. Conclusion and Future Work,[0],[0]
"Arguably, mellowmax could be used in place of Boltzmann throughout reinforcement-learning research.
",9. Conclusion and Future Work,[0],[0]
"A future direction is to analyze the fixed point of planning, reinforcement-learning, and game-playing algorithms when using the mellowmax operators.",9. Conclusion and Future Work,[0],[0]
"In particular, an interesting analysis could be one that bounds the sub-optimality of the fixed points found by GVI.
",9. Conclusion and Future Work,[0],[0]
"An important future work is to expand the scope of our theoretical understanding to the more general function approximation setting, in which the state space or the action space is large and abstraction techniques are used.",9. Conclusion and Future Work,[0],[0]
Note that the importance of non-expansion in the function approximation case is well-established.,9. Conclusion and Future Work,[0],[0]
"(Gordon, 1995)
",9. Conclusion and Future Work,[0],[0]
"Finally, due to the convexity of mellowmax (Boyd & Vandenberghe, 2004), it is compelling to use it in a gradient-based algorithm in the context of sequential decision making.",9. Conclusion and Future Work,[0],[0]
IRL is a natural candidate given the popularity of softmax in this setting.,9. Conclusion and Future Work,[0],[0]
"The authors gratefully acknowledge the assistance of George D. Konidaris, as well as anonymous ICML reviewers for their outstanding feedback.",10. Acknowledgments,[0],[0]
A softmax operator applied to a set of values acts somewhat like the maximization function and somewhat like an average.,abstractText,[0],[0]
"In sequential decision making, softmax is often used in settings where it is necessary to maximize utility but also to hedge against problems that arise from putting all of one’s weight behind a single maximum utility decision.",abstractText,[0],[0]
"The Boltzmann softmax operator is the most commonly used softmax operator in this setting, but we show that this operator is prone to misbehavior.",abstractText,[0],[0]
"In this work, we study a differentiable softmax operator that, among other properties, is a non-expansion ensuring a convergent behavior in learning and planning.",abstractText,[0],[0]
"We introduce a variant of SARSA algorithm that, by utilizing the new operator, computes a Boltzmann policy with a state-dependent temperature parameter.",abstractText,[0],[0]
We show that the algorithm is convergent and that it performs favorably in practice.,abstractText,[0],[0]
An Alternative Softmax Operator for Reinforcement Learning,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2422–2430 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
2422
An AMR Aligner Tuned by Transition-based Parser
Yijia Liu, Wanxiang Che∗, Bo Zheng, Bing Qin, Ting Liu Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China {yjliu,car,bzheng,qinb,tliu}@ir.hit.edu.cn
Abstract
In this paper, we propose a new rich resource enhanced AMR aligner which produces multiple alignments and a new transition system for AMR parsing along with its oracle parser. Our aligner is further tuned by our oracle parser via picking the alignment that leads to the highestscored achievable AMR graph. Experimental results show that our aligner outperforms the rule-based aligner in previous work by achieving higher alignment F1 score and consistently improving two open-sourced AMR parsers. Based on our aligner and transition system, we develop a transition-based AMR parser that parses a sentence into its AMR graph directly. An ensemble of our parsers with only words and POS tags as input leads to 68.4 Smatch F1 score, which outperforms the parser of Wang and Xue (2017).",text,[0],[0]
"Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic representation which encodes the meaning of a sentence in a rooted and directed graph, whose nodes are abstract semantic concepts and edges are semantic relations between concepts (see Figure 1 for an example).",1 Introduction,[0],[0]
"Parsing a sentence into its AMR graph has drawn a lot of research attention in recent years with a number of parsers being developed (Flanigan et al., 2014; Wang et al., 2015b; Pust et al., 2015; Artzi et al., 2015; Peng et al., 2015; Zhou et al., 2016; Goodman et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Foland and Martin, 2017; Konstas et al., 2017).
",1 Introduction,[0],[0]
The nature of abstracting away the association between a concept and a span of words complicates the training of the AMR parser.,1 Introduction,[0],[0]
"A wordconcept aligner is required to derive such association from the sentence-AMR-graph pair and the
∗*",1 Introduction,[0],[0]
"Email corresponding.
alignment output is then used as reference to train the AMR parser.",1 Introduction,[0],[0]
"In previous works, such alignment is extracted by either greedily applying a set of heuristic rules (Flanigan et al., 2014) or adopting the unsupervised word alignment technique from machine translation (Pourdamghani et al., 2014; Wang and Xue, 2017).
",1 Introduction,[0],[0]
The rule-based aligner – JAMR aligner proposed by Flanigan et al. (2014) is widely used in previous works thanks to its flexibility of incorporating additional linguistic resources like WordNet.,1 Introduction,[0],[0]
"However, achieving good alignments with the JAMR aligner still faces some difficult challenges.",1 Introduction,[0],[0]
The first challenge is deriving an optimal alignment in ambiguous situations.,1 Introduction,[0],[0]
"Taking the sentence-AMR-graph pair in Figure 1 for example, the JAMR aligner doesn’t distinguish between the two “nuclear”s in the sentence and can yield sub-optimal alignment in which the first “nuclear” is aligned to the nucleus˜2 concept.",1 Introduction,[0],[0]
"The second challenge is recalling more semantically matched word-concept pair without harming the
alignment precision.",1 Introduction,[0],[0]
"The JAMR aligner adopts a rule that aligns the word-concept pair which at least have a common longest prefix of 4 characters, but omitting the shorter cases like aligning the word “actions” to the concept act-01 and the semantically matched cases like aligning the word “example” to the concept exemplify-01.",1 Introduction,[0],[0]
The final challenge which is faced by both the rule-based and unsupervised aligners is tuning the alignment with downstream parser learning.,1 Introduction,[0],[0]
Previous works treated the alignment as a fixed input.,1 Introduction,[0],[0]
Its quality is never evaluated and its alternatives are never explored.,1 Introduction,[0],[0]
"All these challenges make the JAMR aligner achieve only an alignment F1 score of about 90% and influence the performance of the trained AMR parsers.
",1 Introduction,[0],[0]
"In this paper, we propose a novel method to solve these challenges and improve the word-toconcept alignment, which further improves the AMR parsing performance.",1 Introduction,[0],[0]
A rule-based aligner and a transition-based oracle AMR parser lie in the core of our method.,1 Introduction,[0],[0]
"For the aligner part, we incorporate rich semantic resources into the JAMR aligner to recall more word-concept pairs and cancel its greedily aligning process.",1 Introduction,[0],[0]
This leads to multiple alignment outputs with higher recall but lower precision.,1 Introduction,[0],[0]
"For the parser part, we propose a new transition system that can parse the raw sentence into AMR graph directly.",1 Introduction,[0],[0]
"Meanwhile, a new oracle algorithm is proposed which produces the best achievable AMR graph from an alignment.",1 Introduction,[0],[0]
"Our aligner is tuned by our oracle parser by feeding the alignments to the oracle parser and picking the one which leads to the highest Smatch F1 score (Cai and Knight, 2013).",1 Introduction,[0],[0]
The chosen alignment is used in downstream training of the AMR parser.,1 Introduction,[0],[0]
"Based on the newly proposed aligner and transition system, we develop a transition-based parser that directly parses a sentence into its AMR graph and it can be easily improved through ensemble thanks to its simplicity.
",1 Introduction,[0],[0]
We conduct experiments on LDC2014T12 dataset.1 Both intrinsic and extrinsic evaluations are performed on our aligner.,1 Introduction,[0],[0]
"In the intrinsic evaluation, our aligner achieves an alignment F1 score of 95.2%.",1 Introduction,[0],[0]
"In the extrinsic evaluation, we replace the JAMR aligner with ours in two opensourced AMR parsers, which leads to consistent improvements on both parsers.",1 Introduction,[0],[0]
"We also evaluate our transition-based parser on the same dataset.
1catalog.ldc.upenn.edu/ldc2014t12
Using both our aligner and ensemble, a score of 68.1 Smatch F1 is achieved without any additional resources, which is comparable to the parser of Wang and Xue (2017).",1 Introduction,[0],[0]
"With additional part-ofspeech (POS) tags, our ensemble parser achieves 68.4 Smatch F1 score and outperforms that of Wang and Xue (2017).
",1 Introduction,[0],[0]
"The contributions of this paper come in two folds:
• We propose a new AMR aligner (§3) which recalls more semantically matched pairs and produces multiple alignments.",1 Introduction,[0],[0]
We also propose a new transition system for AMR parsing (§4.1) and use its oracle (§4.2) to pick the alignment that leads to the highest-scored achievable AMR graph (§4.3).,1 Introduction,[0],[0]
"Both intrinsic and extrinsic evaluations (§5) show the effectiveness of our aligner by achieving higher F1 score and consistently improving two opensourced AMR parsers.
",1 Introduction,[0],[0]
• We build a new transition-based parser (§4.4) upon our aligner and transition system which directly parses a raw sentence into its AMR graph.,1 Introduction,[0],[0]
"Through simple ensemble, our parser achieves 68.4 Smatch F1 score with only words and POS tags as input (§6) and outperforms the parser of Wang and Xue (2017).
",1 Introduction,[0],[0]
Our code and the alignments for LDC2014T12 dataset are publicly available at https:// github.com/Oneplus/tamr,1 Introduction,[0],[0]
AMR Parsers.,2 Related Work,[0],[0]
AMR parsing maps a natural language sentence into its AMR graph.,2 Related Work,[0],[0]
"Most current parsers construct the AMR graph in a two-staged manner which first identifies concepts (nodes in the graph) from the input sentence, then identifies relations (edges in the graph) between the identified concepts.",2 Related Work,[0],[0]
"Flanigan et al. (2014) and their follow-up works (Flanigan et al., 2016; Zhou et al., 2016) model the parsing problem as finding the maximum spanning connected graph.",2 Related Work,[0],[0]
"Wang et al. (2015b) proposes to greedily transduce the dependency tree into AMR graph and a bunch of works (Wang et al., 2015a; Goodman et al., 2016; Wang and Xue, 2017) further improve the transducer’s performance with rich features and imitation learning.2 Transition-based methods
2Wang et al. (2015b) and the follow-up works refer their transducing process as “transition-based”.",2 Related Work,[0],[0]
"However, to dis-
that directly parse an input sentence into its AMR graph have also been studied (Ballesteros and AlOnaizan, 2017; Damonte et al., 2017).",2 Related Work,[0],[0]
"In these works, the concept identification and relation identification are performed jointly.
",2 Related Work,[0],[0]
"An aligner which maps a span of words into its concept serves to the generation of training data for the concept identifier, thus is important to the parser training.",2 Related Work,[0],[0]
"Missing or incorrect alignments lead to poor concept identification, which then hurt the overall AMR parsing performance.",2 Related Work,[0],[0]
"Besides the typical two-staged methods, the aligner also works in some other AMR parsing algorithms like that using syntax-based machine translation (Pust et al., 2015), sequence-to-sequence (Peng et al., 2017; Konstas et al., 2017), Hyperedge Replacement Grammar (Peng et al., 2015) and Combinatory Category Grammar (Artzi et al., 2015).
",2 Related Work,[0],[0]
Previous aligner works solve the alignment problem in two different ways.,2 Related Work,[0],[0]
"The rule-based aligner (Flanigan et al., 2014) defines a set of heuristic rules which align a span of words to the graph fragment and greedily applies these rules.",2 Related Work,[0],[0]
"The unsupervised aligner (Pourdamghani et al., 2014; Wang and Xue, 2017) uncovers the word-toconcept alignment from the linearized AMR graph through EM.",2 Related Work,[0],[0]
"All these approaches yield a single alignment for one sentence and its effect on the downstream parsing is not considered.
",2 Related Work,[0],[0]
"JAMR Aligner (Flanigan et al., 2014).",2 Related Work,[0],[0]
"Two components exist in the JAMR aligner: 1) a set of heuristic rules and 2) a greedy search process.
",2 Related Work,[0],[0]
"The heuristic rules in the JAMR aligner are a set of indicator functions ρ(c, ws,e) which take a concept c and a span of words ws,e starting from s and ending with e as input and return whether they should be aligned.",2 Related Work,[0],[0]
These rules can be categorized into matching rules and updating rules.,2 Related Work,[0],[0]
"The matching rules directly compare c with ws,e and determine if they should be aligned.",2 Related Work,[0],[0]
"The updating rules first retrieve the concept c′ that ws,e aligns, then determine if c and ws,e should be aligned by checking whether c and c′ meet some conditions.",2 Related Work,[0],[0]
"Here, we illustrate how update rules work by applying a rule named Entity Type on the AMR graph in Figure 1 as an example.",2 Related Work,[0],[0]
"When determining if the entity type concept country should be aligned to “North Korea”, the Entity
tinguish their work with that of Damonte et al. (2017) and Ballesteros and Al-Onaizan (2017), we use the term “transduce” instead.
",2 Related Work,[0],[0]
"Type rule first retrieve that this span is aligned to the fragment (name :op1 ""North"" :op2 ""Korea""), then determine if they are aligned by checking if name is the tail concept of country.
",2 Related Work,[0],[0]
The greedy search process applies rules in a manually defined order.,2 Related Work,[0],[0]
"The results are mutually exclusive which means once a graph fragment is aligned by one rule, it cannot be realigned.",2 Related Work,[0],[0]
"By doing so, conflicts between the alignments produced by different rules are resolved.",2 Related Work,[0],[0]
"Flanigan et al. (2014) didn’t talk about the principle of orders but it generally follows the principle that 1) the matching rules have higher priorities than the updating rules, and 2) exact matching rules have higher priorities than the fuzzy matching rules.",2 Related Work,[0],[0]
Error propagates in the greedy search process.,3.1 Enhancing Aligner with Rich Semantic Resources,[0],[0]
An alignment error can lead to future errors because of the dependencies and mutual exclusions between rules.,3.1 Enhancing Aligner with Rich Semantic Resources,[0],[0]
"In the JAMR aligner, rules that recall more alignments but introduce errors are carefully opted out and it influences the aligner’s performance.",3.1 Enhancing Aligner with Rich Semantic Resources,[0],[0]
Our motivation is to use rich semantic resources to recall more alignments.,3.1 Enhancing Aligner with Rich Semantic Resources,[0],[0]
"Instead of resolving the resulted conflicts and errors by greedy search, we keep the multiple alignments produced by the aligner and let a parser decide the best alignment.
",3.1 Enhancing Aligner with Rich Semantic Resources,[0],[0]
"In this paper, we use two kinds of semantic resources to recall more alignments, which include the similarity drawn from Glove embedding (Pennington et al., 2014)3 and the morphosemantic database (Fellbaum et al., 2009) in the WordNet project4.",3.1 Enhancing Aligner with Rich Semantic Resources,[0],[0]
"Two additional matching schemes semantic match and morphological match are proposed as:
Semantic Match.",3.1 Enhancing Aligner with Rich Semantic Resources,[0],[0]
Glove embedding encodes a word into its vector representation.,3.1 Enhancing Aligner with Rich Semantic Resources,[0],[0]
"We define semantic match of a concept as a word in the sentence that has a cosine similarity greater than 0.7 in the embedding space with the concept striping off trailing number (e.g. run-01→ run).
",3.1 Enhancing Aligner with Rich Semantic Resources,[0],[0]
Morphological Match.,3.1 Enhancing Aligner with Rich Semantic Resources,[0],[0]
"Morphosemantic is a database that contains links among derivational
3nlp.stanford.edu/projects/glove/ 4wordnet.princeton.edu/wordnet/
download/standoff/
links connecting noun and verb senses (e.g., “example” and exemplify).",3.1 Enhancing Aligner with Rich Semantic Resources,[0],[0]
"We define morphological match of a concept as a word in the sentence having the (word, concept) link in the database.
",3.1 Enhancing Aligner with Rich Semantic Resources,[0],[0]
"By defining the semantic match and morphological match, we extend the rules in Flanigan et al. (2014) with four additional matching rules as shown in Table 1.",3.1 Enhancing Aligner with Rich Semantic Resources,[0],[0]
"These rules are intended to recall the concepts or entities which either semantically resemble a span of words but differ in the surface form, or match a span of words in their morphological derivation.",3.1 Enhancing Aligner with Rich Semantic Resources,[0],[0]
"Using the rules in the JAMR aligner along with our four extended matching rules, we propose an algorithm to draw multiple alignments from a pair of sentence and AMR graph and it is shown in Algorithm 1.",3.2 Producing Multiple Alignments,[0],[0]
"In this algorithm, Ac denotes the set of candidate alignments for a graph fragment c, in which each alignment is represented as a tuple (s, e, c′) where s denotes the starting position, e denotes the ending position, and c′ denotes the concept that lead to this alignment.",3.2 Producing Multiple Alignments,[0],[0]
"At the beginning, Ac is initialized as an empty set (line 1 to 2).",3.2 Producing Multiple Alignments,[0],[0]
Then all the matching rules are tried to align a span of words to that fragment (line 3 to 7).,3.2 Producing Multiple Alignments,[0],[0]
"After applying all the matching rules, all the updating rules are repeatedly applied until no new alignment is generated in one iteration (line 8 to 16).",3.2 Producing Multiple Alignments,[0],[0]
"During applying the updating rules, we keep track of the dependencies between fragments.",3.2 Producing Multiple Alignments,[0],[0]
"Finally, all the possible combination of the alignments are enumerated without considering the one that violates the fragment dependencies (line 17 to 26).",3.2 Producing Multiple Alignments,[0],[0]
"Our enhanced rule-based aligner produces multiple alignments, and we would like to use our
Algorithm 1: Our alignment algorithm.",4 Transition-based AMR Parser,[0],[0]
"Input: An AMR graph with a set of graph fragments C;
a sentence W ; a set of matching rules PM ; and a set of updating rules PU .
",4 Transition-based AMR Parser,[0],[0]
"Output: a set of alignments A. 1 for c ∈ C do 2 Ac ← ∅; 3 for ρM ∈ PM do 4 for ws,e ← spans(W ) do 5 for c ∈ C do 6 if ρM (c, ws,e) then 7 Ac ← Ac ∪ (s, e, nil);
8 updated← true ; 9 while updated is true do
10 updated← false; 11 for ρU ∈ PU do 12 for c, c′ ∈ C × C do 13 for (s, e, d) ∈ A′c do 14 if ρU (c, ws,e)",4 Transition-based AMR Parser,[0],[0]
"∧ (s, e, c′) /∈",4 Transition-based AMR Parser,[0],[0]
"Ac then 15 Ac ← Ac ∪ (s, e, c′); 16 updated← true;
17 A ← ∅ ; 18 for (a1, ..., ac) ∈ CartesianProduct(A1, ..., A|C|) do 19 legal← true; 20 for a ∈ (a1, ..., ac) do 21 (s, e, c′)← a; 22 (s′, e′, d)← ac′ ; 23 if s 6= s′ ∧ e 6= e′",4 Transition-based AMR Parser,[0],[0]
"then 24 legal← false ;
25 if legal then 26 A ← A∪ (a1, ..., ac);
parser to evaluate their qualities.",4 Transition-based AMR Parser,[0],[0]
A parameterized parser does not accomplish such goal because training its parameters depends on the aligner’s outputs.,4 Transition-based AMR Parser,[0],[0]
A deterministic parser works in this situation but is required to consider the association between concepts and spans.,4 Transition-based AMR Parser,[0],[0]
"This stops the deterministic parsers which build AMR graph only from the derived concepts5 from being used because they do not distinguish alignments that yields to the same set of concepts.6
This discussion shows that to evaluate the quality of an alignment, we need a deterministic (oracle) parser which builds the AMR graph from the raw sentence.",4 Transition-based AMR Parser,[0],[0]
Ballesteros and Al-Onaizan (2017) presented a transition-based parser that directly parses a sentence into its AMR graph.,4 Transition-based AMR Parser,[0],[0]
"A transition system which extends the swap-based dependency parsing system to handle AMR non-projectivities (Damonte et al., 2017) was proposed in their work.
5e.g.",4 Transition-based AMR Parser,[0],[0]
"the reference relation identifier in Flanigan et al. (2014) and the oracle transducer in Wang et al. (2015b).
",4 Transition-based AMR Parser,[0],[0]
"6recall the “nuclear” example in Section 1.
",4 Transition-based AMR Parser,[0],[0]
"Their work presented the possibility for the oracle parser, but their oracle parser was not touched explicitly.",4 Transition-based AMR Parser,[0],[0]
"What’s more, in the non-projective dependency parsing, Choi and McCallum (2013)’s extension to the list-based system (Nivre, 2008) with caching mechanism achieves expected linear time complexity and requires fewer actions to parse a non-projective tree than the swap-based system.",4 Transition-based AMR Parser,[0],[0]
"Their extension to transition-based AMR parsing is worth studying.
",4 Transition-based AMR Parser,[0],[0]
"In this paper, we propose to extend Choi and McCallum (2013)’s transition system to AMR parsing and present the corresponding oracle parser.",4 Transition-based AMR Parser,[0],[0]
The oracle parser is used for tuning our aligner and training our parser.,4 Transition-based AMR Parser,[0],[0]
We also present a comprehensive comparison of our system with that of Ballesteros and Al-Onaizan (2017) in Section 6.3.,4 Transition-based AMR Parser,[0],[0]
"We follow Choi and McCallum (2013) and define a state in our transition system as a quadruple s = (σ, δ, β,A), where σ is a stack holding processed words, δ is a deque holding words popped out of σ that will be pushed back in the future, and β is a buffer holding unprocessed words.",4.1 List-based Extension for AMR Parsing,[0],[0]
A is a set of labeled relations.,4.1 List-based Extension for AMR Parsing,[0],[0]
A set of actions is defined to parse sentence into AMR graph.,4.1 List-based Extension for AMR Parsing,[0],[0]
Table 2 gives a formal illustration of these actions and how they work.,4.1 List-based Extension for AMR Parsing,[0],[0]
"The first five actions in Table 2 are our ex-
tended actions, and they are used to deriving concepts from the input sentence.",4.1 List-based Extension for AMR Parsing,[0],[0]
"Given an alignment and the gold standard AMR graph, we can build the best AMR graph by repeatedly applying one of these actions and this is what we called oracle parser.",4.2 Oracle Parser,[0],[0]
"Before running the oracle parser, we first remove the concepts which aren’t aligned with any span of words from the AMR graph.",4.2 Oracle Parser,[0],[0]
"During running the oracle parser, for a state s = (σ|s0, δ, b0|b1|β, A), our oracle parser decides which action to apply by checking the following conditions one by one.
1.",4.2 Oracle Parser,[0],[0]
"If b0 is a word and it doesn’t align to any concept, perform DROP.
2.",4.2 Oracle Parser,[0],[0]
"If b1 is within a span in the alignment, perform MERGE.
3.",4.2 Oracle Parser,[0],[0]
"If b0 is a word or span and it only aligns to one entity concept c, perform ENTITY(c).
4.",4.2 Oracle Parser,[0],[0]
"If b0 is a word or span and it aligns to one or more concepts, perform CONFIRM(c) where c is the concept b0 aligns and has the longest graph distance to the root.
5.",4.2 Oracle Parser,[0],[0]
"If b0 is a concept and its head concept c has the same alignment as b0, perform NEW(c).
6.",4.2 Oracle Parser,[0],[0]
"If b0 is a concept and there is an unprocessed edge r between s0 and t0, perform LEFT(r) or RIGHT(r) according to r’s direction.
7.",4.2 Oracle Parser,[0],[0]
"If s0 has unprocessed edge, perform CACHE.
8.",4.2 Oracle Parser,[0],[0]
"If s0 doesn’t have unprocessed edge, perform REDUCE.
9. perform SHIFT.
",4.2 Oracle Parser,[0],[0]
"We test our oracle parser on the hand-align data created by Flanigan et al. (2014) and it achieves 97.4 Smatch F1 score.7 Besides the errors resulted from incorrect manual alignments, entity errors made by the limitation of our ENTITY(c) action count a lot.",4.2 Oracle Parser,[0],[0]
Since our ENTITY action directly converts the surface form of a word span into an entity.,4.2 Oracle Parser,[0],[0]
"It cannot correctly generate entity names when they require derivation,8 or where tokenization errors exist.9",4.2 Oracle Parser,[0],[0]
"Using our oracle parser, we tune the aligner by picking the alignment which leads to the highestscored AMR graph from the set of candidates (see Figure 2 for the workflow).",4.3 Tune the Aligner with Oracle Parser,[0],[0]
"When more than one alignment achieve the highest score, we choose the one with the smallest number of actions.",4.3 Tune the Aligner with Oracle Parser,[0],[0]
"Intuitively, choosing the one with the smallest number of actions will encourage structurally coherent alignment10 because coherent alignment requires fewer CACHE actions.",4.3 Tune the Aligner with Oracle Parser,[0],[0]
"Based on our aligner and transition system, we propose a transition-based parser which parse the
7 Since some alignments in hand-align were created on incorrect AMR annotations, we filter out them and only use the correct subset which has 136 pairs of alignment and AMR graph.",4.4 Parsing Model,[0],[0]
"This data is also used in our intrinsic evaluation.
8e.g., “North Koreans” cannot be parsed into (name :op1 ""North"" :op2 ""Korea"")
9e.g., “Wi Sung - lac” cannot be parsed into (name :op1 ""Wi"" :op2 ""Sung-lac"")
10e.g.",4.4 Parsing Model,[0],[0]
"the first “nuclear” aligned to nucleus˜1 in Fig. 1
raw sentence directly into its AMR graph.",4.4 Parsing Model,[0],[0]
"In this paper, we follow Ballesteros and Al-Onaizan (2017) and use StackLSTM (Dyer et al., 2015) to model the states.",4.4 Parsing Model,[0],[0]
The score of a transition action a on state s is calculated as p(a|s) = exp{ga · STACKLSTM(s) +,4.4 Parsing Model,[0],[0]
"ba}∑ a′ exp{ga′ · STACKLSTM(s) + ba′} ,
where STACKLSTM(s) encodes the state s into a vector and ga is the embedding vector of action a.",4.4 Parsing Model,[0],[0]
"We encourage the reader to refer Ballesteros and Al-Onaizan (2017) for more details.
",4.4 Parsing Model,[0],[0]
Ensemble.,4.4 Parsing Model,[0],[0]
"Ensemble has been shown as an effective way of improving the neural model’s performance (He et al., 2017).",4.4 Parsing Model,[0],[0]
"Since the transitionbased parser directly parse a sentence into its AMR graph, ensemble of several parsers is easier compared to the two-staged AMR parsers.",4.4 Parsing Model,[0],[0]
"In this paper, we ensemble the parsers trained with different initialization by averaging their probability distribution over the actions.",4.4 Parsing Model,[0],[0]
We evaluate our aligner on the LDC2014T12 dataset.,5.1 Settings,[0],[0]
"Two kinds of evaluations are carried out including the intrinsic and extrinsic evaluations.
",5.1 Settings,[0],[0]
"For the intrinsic evaluation, we follow Flanigan et al. (2014) and evaluate the F1 score of the alignments produced by our aligner against the manually aligned data created in their work (handalign).",5.1 Settings,[0],[0]
"We also use our oracle parser’s performance as an intrinsic evaluation assuming that better alignment leads to higher scored oracle parser.
",5.1 Settings,[0],[0]
"For the extrinsic evaluation, we plug our alignment into two open-sourced AMR parsers: 1) JAMR (Flanigan et al., 2014, 2016) and 2) CAMR (Wang et al., 2015b,a) and evaluate the final performances of the AMR parsers on both the newswire proportion and the entire dataset of LDC2014T12.",5.1 Settings,[0],[0]
We use the configuration in Flanigan et al. (2016) for JAMR and the configuration in Wang et al. (2015a) without semantic role labeling (SRL) features for CAMR.,5.1 Settings,[0],[0]
Intrinsic Evaluation.,5.2 Results,[0],[0]
"Table 3 shows the intrinsic evaluation results, in which our alignment intrinsically outperforms JAMR aligner by achieving better alignment F1 score and leading to a higher scored oracle parser.
",5.2 Results,[0],[0]
Extrinsic Evaluation.,5.2 Results,[0],[0]
Table 4 shows the results.,5.2 Results,[0],[0]
"From this table, we can see that our alignment consistently improves all the parsers by a margin ranging from 0.5 to 1.7.",5.2 Results,[0],[0]
Both the intrinsic and the extrinsic evaluations show the effectiveness our aligner.,5.2 Results,[0],[0]
"To have a better understanding of our aligner, we conduct ablation test by removing the semantic matching and oracle parser tuning respectively and retrain the JAMR parser on the newswire proportion.",5.3 Ablation,[0],[0]
The results are shown in Table 5.,5.3 Ablation,[0],[0]
"From this table, we can see that removing either of these components harms the performance.",5.3 Ablation,[0],[0]
Removing oracle parser tuning leads to severe performance drop and the score is even lower than that with JAMR aligner.,5.3 Ablation,[0],[0]
We address this observation to that alignment noise is introduced by the semantic matching especially by the word embedding similarity component.,5.3 Ablation,[0],[0]
"Without filtering the noise by our oracle parser, just introducing more matching rules will harm the performance.",5.3 Ablation,[0],[0]
We use the same settings in our aligner extrinsic evaluation for the experiments on our transitionbased parser.,6.1 Settings,[0],[0]
"For the input to the parser, we tried two settings: 1) using only words as input, and 2) using words and POS tags as input.",6.1 Settings,[0],[0]
"Automatic POS tags are assigned with Stanford POS tagger (Manning et al., 2014).",6.1 Settings,[0],[0]
Word embedding from Ling et al. (2015) is used in the same way with Ballesteros and Al-Onaizan (2017).,6.1 Settings,[0],[0]
"To opt
out the effect of different initialization in training the neural network, we run 10 differently seeded runs and report their average performance following Reimers and Gurevych (2017).",6.1 Settings,[0],[0]
Table 6 shows the performance of our transitionbased parser along with comparison to the parsers in the previous works.,6.2 Results,[0],[0]
"When compared with our transition-based counterpart (Ballesteros and AlOnaizan, 2017), our word-only model outperforms theirs using the same JAMR alignment.",6.2 Results,[0],[0]
The same trend is witnessed using words and POS tags as input.,6.2 Results,[0],[0]
"When replacing the JAMR alignments with ours, the parsing performances are improved in the same way as in Table 4, which further confirms the effectiveness of our aligner.
",6.2 Results,[0],[0]
"The second block in Table 6 shows the results of our ensemble parser, in which ensemble significantly improves the performance and more parsers ensembled, more improvements are achieved.",6.2 Results,[0],[0]
An ensemble of 10 parsers with only words as input achieves 68.1 Smatch F1 score which is comparable to the AMR parser of Wang and Xue (2017).,6.2 Results,[0],[0]
"Using the minimal amount of additional syntactic information – POS tags, the performance of the ensemble of 10 parsers is further pushed to 68.4, which surpasses that of Wang and Xue (2017) which relied on named entity recognition (NER) and dependency parsing (DEP).
",6.2 Results,[0],[0]
A further study on the speed shows that our 10 parser ensemble can parse 43 tokens per second which is faster than JAMR (7 tokens/sec.) and CAMR (24 tokens/sec.),6.2 Results,[0],[0]
"thanks to the simplicity of our model and independence of preprocessing, like NER and DEP.11",6.2 Results,[0],[0]
"Al-Onaizan (2017)
",6.3 Comparison to Ballesteros and,[0],[0]
"To explain the improved performance against Ballesteros and Al-Onaizan (2017) in Table 6, we
11In our speed comparison, we also count the time of preprocessing for JAMR and CAMR.",6.3 Comparison to Ballesteros and,[0],[0]
"All the comparison is performed in the same single-threaded settings.
give a comprehensive comparison between our transition system and that of Ballesteros and AlOnaizan (2017).
",6.3 Comparison to Ballesteros and,[0],[0]
Capability.,6.3 Comparison to Ballesteros and,[0],[0]
"In both these two systems, a span of words can only be derived into concept for one time.",6.3 Comparison to Ballesteros and,[0],[0]
“Patch” actions are required to generate new concepts from the one that is aligned to the same span.12 Ballesteros and Al-Onaizan (2017) uses a DEPENDENT action to generate one tail concept for one hop and cannot deal with the cases which have a chain of more than two concepts aligned to the same span.,6.3 Comparison to Ballesteros and,[0],[0]
Our list-based system differs theirs by using a NEW action to deal these cases.,6.3 Comparison to Ballesteros and,[0],[0]
"Since the new concept is pushed onto the buffer, NEW action can be repeatedly applied and used to generate arbitrary concepts that aligned to the same
12 e.g., three concepts in the fragment (person :source (country :name (name :op1 ""North"" :op2 ""Korea""))) are aligned to “North Koreans”.
span.",6.3 Comparison to Ballesteros and,[0],[0]
"On the development set of LDC2014T12, our oracle achieves 91.7 Smatch F1 score over the JAMR alignment, which outperforms Ballesteros and Al-Onaizan (2017)’s oracle (89.5 in their paper) on the same alignment.",6.3 Comparison to Ballesteros and,[0],[0]
"This result confirms that our list-based system is more powerful.
",6.3 Comparison to Ballesteros and,[0],[0]
Number of Actions.,6.3 Comparison to Ballesteros and,[0],[0]
Our list-based system also differs theirs in the number of oracle actions required to parse the same AMR graphs.,6.3 Comparison to Ballesteros and,[0],[0]
We use the oracles from two systems to parse the development set of LDC2014T12 on the same JAMR alignments.,6.3 Comparison to Ballesteros and,[0],[0]
Figure 3 shows the comparison in which our system clearly uses fewer actions (the average number of our system is 63.7 and that of Ballesteros and Al-Onaizan (2017) is 86.4).,6.3 Comparison to Ballesteros and,[0],[0]
Using fewer actions makes the parser learned from the oracle less prone to error propagation.,6.3 Comparison to Ballesteros and,[0],[0]
We attribute the improved performance in Table 6 to this advantage of transition system.,6.3 Comparison to Ballesteros and,[0],[0]
"In this paper, we propose a new AMR aligner which is tuned by a novel transition-based AMR oracle parser.",7 Conclusion,[0],[0]
Our aligner is also enhanced by rich semantic resource and recalls more alignments.,7 Conclusion,[0],[0]
Both the intrinsic and extrinsic evaluations show the effectiveness of our aligner by achieving higher alignment F1 score and consistently improving two open-sourced AMR parsers.,7 Conclusion,[0],[0]
We also develop transition-based AMR parser based on our aligner and transition system and it achieves a performance of 68.4 Smatch F1 score via ensemble with only words and POS tags as input.,7 Conclusion,[0],[0]
We thank the anonymous reviewers for their helpful comments and suggestions.,Acknowledgments,[0],[0]
"This work was
supported by the National Key Basic Research Program of China via grant 2014CB340503 and the National Natural Science Foundation of China (NSFC) via grant 61632011 and 61772153.",Acknowledgments,[0],[0]
"In this paper, we propose a new rich resource enhanced AMR aligner which produces multiple alignments and a new transition system for AMR parsing along with its oracle parser.",abstractText,[0],[0]
Our aligner is further tuned by our oracle parser via picking the alignment that leads to the highestscored achievable AMR graph.,abstractText,[0],[0]
Experimental results show that our aligner outperforms the rule-based aligner in previous work by achieving higher alignment F1 score and consistently improving two open-sourced AMR parsers.,abstractText,[0],[0]
"Based on our aligner and transition system, we develop a transition-based AMR parser that parses a sentence into its AMR graph directly.",abstractText,[0],[0]
"An ensemble of our parsers with only words and POS tags as input leads to 68.4 Smatch F1 score, which outperforms the parser of Wang and Xue (2017).",abstractText,[0],[0]
An AMR Aligner Tuned by Transition-based Parser,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 64–71 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-2011",text,[0],[0]
Action recognition is the task of identifying the action being depicted in a video or still image.,1 Introduction,[0],[0]
"The task is useful for a range of applications such as generating descriptions, image/video retrieval, surveillance, and human–computer interaction.",1 Introduction,[0],[0]
"It has been widely studied in computer vision, often on videos (Nagel, 1994; Forsyth et al., 2005), where motion and temporal information provide cues for recognizing actions (Taylor et al., 2010).",1 Introduction,[0],[0]
"However, many actions are recognizable from still images, see the examples in Figure 1.",1 Introduction,[0],[0]
"Due to the absence of motion cues and temporal features (Ikizler et al., 2008) action recognition from stills is more challenging.",1 Introduction,[0],[0]
Most of the existing work can be categorized into four tasks: (a) action classification (AC); (b) determining human–object interaction (HOI); (c) visual verb sense disambiguation (VSD); and (d) visual semantic role labeling (VSRL).,1 Introduction,[0],[0]
"In Figure 2 we illustrate each of these
tasks and show how they are related to each other.",1 Introduction,[0],[0]
"Until recently, action recognition was studied as action classification on small-scale datasets with a limited number of predefined actions labels (Ikizler et al., 2008; Gupta et al., 2009; Yao and FeiFei, 2010; Everingham et al., 2010; Yao et al., 2011).",1 Introduction,[0],[0]
"Often the labels in action classification tasks are verb phrases or a combination of verb and object such as playing baseball, riding horse.",1 Introduction,[0],[0]
"These datasets have helped in building models and understanding which aspects of an image are important for classifying actions, but most methods are not scalable to larger numbers of actions (Ramanathan et al., 2015).",1 Introduction,[0],[0]
"Action classification models are trained on images annotated with mutually exclusive labels, i.e., the assumption is that only a single label is relevant for a given image.",1 Introduction,[0],[0]
This ignores the fact that actions such as holding bicycle and riding bicycle can co-occur in the same image.,1 Introduction,[0],[0]
"To address these issues and also to understand the range of possible interactions between humans and objects, the human–object interaction (HOI) detection task has been proposed, in which all possible interactions between a human and a given object have to be identified (Le et al., 2014; Chao et al., 2015; Lu et al., 2016).
",1 Introduction,[0],[0]
"However, both action classification and HOI detection do not consider the ambiguity that arises when verbs are used as labels, e.g., the verb play has multiple meanings in different contexts.",1 Introduction,[0],[0]
"On the other hand, action labels consisting of verbobject pairs can miss important generalizations:
64
riding horse and riding elephant both instantiate the same verb semantics, i.e., riding animal.",1 Introduction,[0],[0]
"Thirdly, existing action labels miss generalizations across verbs, e.g., the fact that fixing bike and repairing bike are semantically equivalent, in spite of the use of different verbs.",1 Introduction,[0],[0]
These observations have led authors to argue that actions should be analyzed at the level of verb senses.,1 Introduction,[0],[0]
"Gella et al. (2016) propose the new task of visual verb sense disambiguation (VSD), in which a verb– image pair is annotated with a verb sense taken from an existing lexical database (OntoNotes in this case).",1 Introduction,[0],[0]
"While VSD handles distinction between different verb senses, it does not identify or localize the objects that participate in the action denoted by the verb.",1 Introduction,[0],[0]
"Recent work (Gupta and Malik, 2015; Yatskar et al., 2016) has filled this gap by proposing the task of visual semantic role labeling (VSRL), in which images are labeled with verb frames, and the objects that fill the semantic roles of the frame are identified in the image.
",1 Introduction,[0],[0]
"In this paper, we provide a unified view of action recognition tasks, pointing out their strengths and weaknesses.",1 Introduction,[0],[0]
We survey existing literature and provide insights into existing datasets and models for action recognition tasks.,1 Introduction,[0],[0]
We give an overview of commonly used datasets for action recognition tasks in Table 1 and group them according to subtask.,2 Datasets for Action Recognition,[0],[0]
"We observe that the number of verbs covered in these datasets is often smaller than the number of action labels reported (see Table 1, columns #V and #L) and in many cases the action label involves object reference.",2 Datasets for Action Recognition,[0],[0]
"A few of the first action recognition datasets such as the Ikizler and Willow datasets (Ikizler et al.,
2008; Delaitre et al., 2010) had action labels such as throwing and running; they were taken from the sports domain and exhibited diversity in camera view point, background and resolution.",2 Datasets for Action Recognition,[0],[0]
"Then datasets were created to capture variation in human poses in the sports domain for actions such as tennis serve and cricket bowling; typically features based on poses and body parts were used to build models (Gupta et al., 2009).",2 Datasets for Action Recognition,[0],[0]
"Further datasets were created based on the intuition that object information helps in modeling action recognition (Li and Fei-Fei, 2007; Ikizler-Cinbis and Sclaroff, 2010), which resulted in the use of action labels such as riding horse or riding bike (Everingham et al., 2010; Yao et al., 2011).",2 Datasets for Action Recognition,[0],[0]
"Not only were most of these datasets domain specific, but the labels were also manually selected and mutually exclusive, i.e., two actions cannot co-occur in the same image.",2 Datasets for Action Recognition,[0],[0]
"Also, most of these datasets do not localize objects or identify their semantic roles.",2 Datasets for Action Recognition,[0],[0]
"The limitations with early datasets (small scale, domain specificity, and the use of ad-hoc labels that combine verb and object) have been recently addressed in a number of broad-coverage datasets that offer linguistically motivated labels.",2.1 Identifying Visual Verbs and Verb Senses,[0],[0]
"Often these datasets use existing linguistic resources such as VerbNet (Schuler, 2005), OntoNotes (Hovy et al., 2006) and FrameNet (Baker et al., 1998) to classify verbs and their senses.",2.1 Identifying Visual Verbs and Verb Senses,[0],[0]
"This allows for a more general, semantically motivated treatment of verbs and verb phrases, and also takes into account that not all verbs are depictable.",2.1 Identifying Visual Verbs and Verb Senses,[0],[0]
"For example, abstract verbs such as presuming and acquiring are not depictable at all, while other verbs have both depictable and non-depictable senses: play is non-depictable in playing with emotions, but depictable in playing instrument and playing sport.",2.1 Identifying Visual Verbs and Verb Senses,[0],[0]
"The process of identifying depictable verbs or verb senses is used by Ronchi and Perona (2015), Gella et al. (2016) and Yatskar et al. (2016) to identify visual verbs, visual verb senses, and the semantic roles of the participating objects respectively.",2.1 Identifying Visual Verbs and Verb Senses,[0],[0]
In all the cases the process of identifying visual verbs or senses is carried out by human annotators via crowd-sourcing platforms.,2.1 Identifying Visual Verbs and Verb Senses,[0],[0]
"Visualness labels for 935 OntoNotes verb senses corresponding to 154 verbs is provided by Gella et al. (2016), while Yatskar et al. (2016) provides visualness labels for 9683 FrameNet verbs.",2.1 Identifying Visual Verbs and Verb Senses,[0],[0]
Over the last few years tasks that combine language and vision such as image description and visual question answering have gained much attention.,2.2 Datasets Beyond Action Classification,[0],[0]
"This has led to the creation of new, large datasets such as MSCOCO (Chen et al., 2015) and the VQA dataset (Antol et al., 2015).",2.2 Datasets Beyond Action Classification,[0],[0]
"Although these datasets are not created for action recognition, a number of attempts have been made to use the verbs present in image descriptions to annotate actions.",2.2 Datasets Beyond Action Classification,[0],[0]
"The COCO-a, VerSe and VCOCO-SRL datasets all use the MSCOCO image descriptions to annotate fine-grained aspects of interaction and semantic roles.
HICO:",2.2 Datasets Beyond Action Classification,[0],[0]
The HICO dataset has 47.8k images annotated with 600 categories of human-object interactions with 111 verbs applying to 80 object categories of MSCOCO.,2.2 Datasets Beyond Action Classification,[0],[0]
It is annotated to include diverse interactions for objects and has an average of 6.5 distinct interactions per object category.,2.2 Datasets Beyond Action Classification,[0],[0]
"Unlike other HOI datasets such as TUHOI which label interactions as verbs and ignore senses, the HOI categories of HICO are based on WordNet (Miller, 1995) verb senses.",2.2 Datasets Beyond Action Classification,[0],[0]
The HICO dataset also has multiple annotations per object and it incorporates the information that certain interactions such as riding a bike and holding a bike often co-occur.,2.2 Datasets Beyond Action Classification,[0],[0]
"However, it fails to include annotations to distinguish between multiple senses of a verb.
",2.2 Datasets Beyond Action Classification,[0],[0]
"Visual Genome: The dataset created by Krishna et al. (2016) has dense annotations of objects, at-
tributes, and relationships between objects.",2.2 Datasets Beyond Action Classification,[0],[0]
The Visual Genome dataset contains 105k images with 40k unique relationships between objects.,2.2 Datasets Beyond Action Classification,[0],[0]
"Unlike other HOI datasets such as HICO, visual genome relationships also include prepositions, comparative and prepositional phrases such as near and taller than, making the visual relationship task more generic than action recognition.",2.2 Datasets Beyond Action Classification,[0],[0]
"Krishna et al. (2016) combine all the annotations of objects, relationships, and attributes into directed graphs known as scene graphs.
",2.2 Datasets Beyond Action Classification,[0],[0]
"COCO-a: Ronchi and Perona (2015) present Visual VerbNet (VVN), a list of 140 common visual verbs manually mined from English VerbNet (Schuler, 2005).",2.2 Datasets Beyond Action Classification,[0],[0]
"The coverage of visual verbs in this dataset is not complete, as many visual verbs such as dive, perform and shoot are not included.",2.2 Datasets Beyond Action Classification,[0],[0]
This also highlights a bias in this dataset as the authors relied on occurrence in MSCOCO as a verification step to consider a verb as visual.,2.2 Datasets Beyond Action Classification,[0],[0]
"They annotated 10k images containing human subjects with one of the 140 visual verbs, for 80 MSCOCO objects.",2.2 Datasets Beyond Action Classification,[0],[0]
"This dataset has better coverage of human-object interactions than the HICO dataset despite of missing many visual verbs.
",2.2 Datasets Beyond Action Classification,[0],[0]
VerSe: Gella et al. (2016) created a dataset of 3.5k images sampled from the MSCOCO and TUHOI datasets and annotated it with 90 verbs and their OntoNotes senses to distinguish different verb senses using visual context.,2.2 Datasets Beyond Action Classification,[0],[0]
"This is the first dataset that aims to annotate all visual senses
of a verb.",2.2 Datasets Beyond Action Classification,[0],[0]
"However, the total number of images annotated and number of images for some senses is relatively small, which makes it difficult to use this dataset to train models.",2.2 Datasets Beyond Action Classification,[0],[0]
"The authors further divided their 90 verbs into motion and non-motion verbs according to Levin (1993) verb classes and analyzed visual ambiguity in the task of visual sense disambiguation.
",2.2 Datasets Beyond Action Classification,[0],[0]
VCOCO-SRL: Gupta and Malik (2015) annotated a dataset of 16k person instances in 10k images with 26 verbs and associated objects in the scene with the semantic roles for each action.,2.2 Datasets Beyond Action Classification,[0],[0]
The main aim of the dataset is to build models for visual semantic role labeling in images.,2.2 Datasets Beyond Action Classification,[0],[0]
"This task involves identifying the actions depicted in an image, along with the people and objects that instantiate the semantic roles of the actions.",2.2 Datasets Beyond Action Classification,[0],[0]
"In the VCOCO-SRL dataset, each person instance is annotated with a mean of 2.8 actions simultaneously.
",2.2 Datasets Beyond Action Classification,[0],[0]
"imSitu: Yatskar et al. (2016) annotated a large dataset of 125k images with 504 verbs, 1.7k semantic roles and 11k objects.",2.2 Datasets Beyond Action Classification,[0],[0]
"They used FrameNet verbs, frames and associated objects or scenes with roles to develop the dataset.",2.2 Datasets Beyond Action Classification,[0],[0]
They annotate every image with a single verb and the semantic roles of the objects present in the image.,2.2 Datasets Beyond Action Classification,[0],[0]
VCOCOSRL,2.2 Datasets Beyond Action Classification,[0],[0]
"the is dataset most similar to imSitu, however VCOCO-SRL includes localization information of agents and all objects and provides multiple action annotations per image.",2.2 Datasets Beyond Action Classification,[0],[0]
"On the other hand, imSitu is the dataset that covers highest number of verbs, while also omitting many commonly studied polysemous verbs such as play.",2.2 Datasets Beyond Action Classification,[0],[0]
"With the exception of a few datasets such as COCO-a, VerSe, imSitu all action recognition datasets have manually picked labels or focus on covering actions in specific domains such as sports.",2.3 Diversity in Datasets,[0],[0]
"Alternatively, many datasets only cover actions relevant to specific object categories such as musical instruments, animals and vehicles.",2.3 Diversity in Datasets,[0],[0]
"In the real world, people interact with many more objects and perform actions relevant to a wide range of domains such as personal care, household activities, or socializing.",2.3 Diversity in Datasets,[0],[0]
This limits the diversity and coverage of existing action recognition datasets.,2.3 Diversity in Datasets,[0],[0]
Recently proposed datasets partly handle this issue by using generic linguistic resources to extend the vocabulary of verbs in action labels.,2.3 Diversity in Datasets,[0],[0]
"The diversity issue has also been high-
lighted and addressed in recent video action recognition datasets (Caba Heilbron et al., 2015; Sigurdsson et al., 2016), which include generic household activities.",2.3 Diversity in Datasets,[0],[0]
An analysis of various image description and question answering datasets by Ferraro et al. (2015) shows the bias in the distribution of word categories.,2.3 Diversity in Datasets,[0],[0]
"Image description datasets have a higher distribution of nouns compared to other word categories, indicating that the descriptions are object specific, limiting their usefulness for action-based tasks.",2.3 Diversity in Datasets,[0],[0]
"Template based description generation systems for both videos and images rely on identifying subject–verb–object triples and use language modeling to generate or rank descriptions (Yang et al., 2011; Thomason et al., 2014; Bernardi et al., 2016).",3 Relevant Language and Vision Tasks,[0],[0]
"Understanding actions also plays an important role in question answering, especially when the question is pertaining to an action depicted in the image.",3 Relevant Language and Vision Tasks,[0],[0]
"There are some specifically curated question answering datasets which target human activities or relationships between a pair of objects (Yu et al., 2015).",3 Relevant Language and Vision Tasks,[0],[0]
Mallya and Lazebnik (2016) have shown that systems trained on action recognition datasets could be used to improve the accuracy of visual question answering systems that handle questions related to human activity and human–object relationships.,3 Relevant Language and Vision Tasks,[0],[0]
"Action recognition datasets could be used to learn actions that are visually similar such as interacting with panda and feeding a panda or tickling a baby and calming a baby, which cannot be learned from text alone (Ramanathan et al., 2015).",3 Relevant Language and Vision Tasks,[0],[0]
"Visual semantic role labeling is a crucial step for grounding actions in the physical world (Yang et al., 2016).",3 Relevant Language and Vision Tasks,[0],[0]
"Most of the models proposed for action classification and human–object interaction tasks rely on identifying higher-level visual cues present in the image, including human bodies or body parts (Ikizler et al., 2008; Gupta et al., 2009; Yao et al., 2011; Andriluka et al., 2014), objects (Gupta et al., 2009), and scenes (Li and Fei-Fei, 2007).",4 Action Recognition Models,[0],[0]
"Higherlevel visual cues are obtained through low-level features extracted from the image such as Scale Invariant Feature Transforms (SIFT), Histogram of Oriented Gradients (HOG), and Spatial Envelopes (Gist) features (Lowe, 1999; Dalal and Triggs,
2005).",4 Action Recognition Models,[0],[0]
"These are useful in identifying key points, detecting humans, and scene or background information in images, respectively.",4 Action Recognition Models,[0],[0]
"In addition to identifying humans and objects, the relative position or angle between a human and an object is useful in learning human–object interactions (Le et al., 2014).",4 Action Recognition Models,[0],[0]
"Most of the existing approaches rely on learning supervised classifiers over low-level features to predict action labels.
",4 Action Recognition Models,[0],[0]
"More recent approaches are based on end-toend convolutional neural network architectures which learn visual cues such as objects and image features for action recognition (Chao et al., 2015; Zhou et al., 2016; Mallya and Lazebnik, 2016).",4 Action Recognition Models,[0],[0]
"While most of the action classification models rely solely on visual information, models proposed for human–object interaction or visual relationship detection sometimes combine human and object identification (using visual features) with linguistic knowledge (Le et al., 2014; Krishna et al., 2016; Lu et al., 2016).",4 Action Recognition Models,[0],[0]
"Other work on identifying actions, especially methods that focus on relationships that are infrequent or unseen, utilize word vectors learned on large text corpora as an additional source of information (Lu et al., 2016).",4 Action Recognition Models,[0],[0]
"Similarly, Gella et al. (2016) show that embeddings generated from textual data associated with images (object labels, image descriptions) is useful for visual verb sense disambiguation, and is complementary to visual information.",4 Action Recognition Models,[0],[0]
"Linguistic resources such as WordNet, OntoNotes, and FrameNet play a key role in textual sense disambiguation and semantic role labeling.",5 Discussion,[0],[0]
"The visual action disambiguation and visual semantic role labeling tasks are extensions of their textual counterparts, where context is provided as an image instead of as text.",5 Discussion,[0],[0]
Linguistic resources therefore have to play a key role if we are to make rapid progress in these language and vision tasks.,5 Discussion,[0],[0]
"However, as we have shown in this paper, only a few of the existing datasets for action recognition and related tasks are based on linguistic resources (Chao et al., 2015; Gella et al., 2016; Yatskar et al., 2016).",5 Discussion,[0],[0]
"This is despite the fact that the WordNet noun hierarchy (for example) has played an important role in recent progress in object recognition, by virtue of underlying the ImageNet database, the de-facto standard for this task (Russakovsky et al., 2015).",5 Discussion,[0],[0]
"The success of ImageNet for objects has
in turn helped NLP tasks such as bilingual lexicon induction (Vulić et al., 2016).",5 Discussion,[0],[0]
"In our view, language and vision datasets that are based on the WordNet, OntoNotes, or FrameNet verb sense inventories can play a similar role for tasks such as action recognition or visual semantic role labeling, and ultimately be useful also for more distantly related tasks such as language grounding.
",5 Discussion,[0],[0]
Another argument for linking language and vision datasets with linguistic resources is that this enables us to deploy the datasets in a multilingual setting.,5 Discussion,[0],[0]
"For example a polysemous verb such as ride in English has multiple translations in German and Spanish, depending on the context and the objects involved.",5 Discussion,[0],[0]
"Riding a horse is translated as reiten in German and cabalgar in Spanish, whereas riding a bicycle is translated as fahren in German and pedalear in Spanish.",5 Discussion,[0],[0]
"In contrast, some polysemous verb (e.g., English play) are always translated as the same verb, independent of sense (spielen in German).",5 Discussion,[0],[0]
"Such sense mappings are discoverable from multilingual lexical resources (e.g., BabelNet, Navigli and Ponzetto 2010), which makes it possible to construct language and vision models that are applicable to multiple languages.",5 Discussion,[0],[0]
"This opportunity is lost if language and vision dataset are constructed in isolation, instead of using existing linguistic resources.",5 Discussion,[0],[0]
"In this paper, we have shown the evolution of action recognition datasets and tasks from simple ad-hoc labels to the fine-grained annotation of verb semantics.",6 Conclusions,[0],[0]
"It is encouraging to see the recent increase in datasets that deal with sense ambiguity and annotate semantic roles, while using standard linguistic resources.",6 Conclusions,[0],[0]
"One major remaining issue with existing datasets is their limited coverage, and the skewed distribution of verbs or verb senses.",6 Conclusions,[0],[0]
Another challenge is the inconsistency in annotation schemes and task definitions across datasets.,6 Conclusions,[0],[0]
"For example Chao et al. (2015) used WordNet senses as interaction labels, while Gella et al. (2016) used the more coarsegrained OntoNotes senses.",6 Conclusions,[0],[0]
"Yatskar et al. (2016) used FrameNet frames for semantic role annotation, while Gupta and Malik (2015) used manually curated roles.",6 Conclusions,[0],[0]
"If we are to develop robust, domain independent models, then we need to standardize annotation schemes and use the same linguistic resources across datasets.",6 Conclusions,[0],[0]
"A large amount of recent research has focused on tasks that combine language and vision, resulting in a proliferation of datasets and methods.",abstractText,[0],[0]
"One such task is action recognition, whose applications include image annotation, scene understanding and image retrieval.",abstractText,[0],[0]
"In this survey, we categorize the existing approaches based on how they conceptualize this problem and provide a detailed review of existing datasets, highlighting their diversity as well as advantages and disadvantages.",abstractText,[0],[0]
We focus on recently developed datasets which link visual information with linguistic resources and provide a fine-grained syntactic and semantic analysis of actions in images.,abstractText,[0],[0]
An Analysis of Action Recognition Datasets for Language and Vision Tasks,title,[0],[0]
"Proceedings of NAACL-HLT 2013, pages 95–105, Atlanta, Georgia, 9–14 June 2013. c©2013 Association for Computational Linguistics
structions has been observed to have a substantial effect on language processing. This begs the question of what causes certain constructions to be more or less frequent. A theory of grounding (Phillips, 2010) would suggest that cognitive limitations might cause languages to develop frequent constructions in such a way as to avoid processing costs. This paper studies how current theories of working memory fit into theories of language processing and what influence memory limitations may have over reading times. Measures of such limitations are evaluated on eye-tracking data and the results are compared with predictions made by different theories of processing.",text,[0],[0]
"Frequency effects in language have been isolated and observed in many studies (Trueswell, 1996; Jurafsky, 1996; Hale, 2001; Demberg and Keller, 2008).",1 Introduction,[0],[0]
"These effects are important because they illuminate the ontogeny of language (how individual speakers have acquired language), but they do not answer questions about the phylogeny of language (how the language came to its current form).
",1 Introduction,[0],[0]
Phillips (2010) has hypothesized that grammar rule probabilities may be grounded in memory limitations.,1 Introduction,[0],[0]
"Increased delays in processing centerembedded sentences as the number of embeddings increases, for example, are often explained in terms of a complexity cost associated with maintaining incomplete dependencies in working memory (Gibson, 2000; Lewis and Vasishth, 2005).",1 Introduction,[0],[0]
"Other studies have shown a link between processing delays
and the low frequency of center-embedded constructions like object relatives (Hale, 2001), but they have not explored the source of this low frequency.",1 Introduction,[0],[0]
A grounding hypothesis would claim that the low probability of generating such a structure may arise from an associated memory load.,1 Introduction,[0],[0]
"In this account, while these complexity costs may involve languagespecific concepts such as referent or argument linking, the underlying explanation would be one of memory limitations (Gibson, 2000) or neural activation (Lewis and Vasishth, 2005).
",1 Introduction,[0],[0]
"This paper seeks to explore the different predictions made by these theories on a broad-coverage corpus of eye-tracking data (Kennedy et al., 2003).",1 Introduction,[0],[0]
"In addition, the current experiment seeks to isolate memory effects from frequency effects in the same task.",1 Introduction,[0],[0]
"The results show that memory load measures are a significant factor even when frequency measures are residualized out.
",1 Introduction,[0],[0]
The remainder of this paper is organized as follows: Sections 2 and 3 describe several frequency and memory measures.,1 Introduction,[0],[0]
Section 4 describes a probabilistic hierarchic sequence model that allows all of these measures to be directly computed.,1 Introduction,[0],[0]
Section 5 describes how these measures were used to predict reading time durations on the Dundee eye-tracking corpus.,1 Introduction,[0],[0]
Sections 6 and 7 present results and discuss.,1 Introduction,[0],[0]
"One of the strongest predictors of processing complexity is surprisal (Hale, 2001).",2.1 Surprisal,[0],[0]
"It has been shown in numerous studies to have a strong correlation with reading time durations in eye-tracking and selfpaced reading studies when calculated with a variety
95
of models (Levy, 2008; Roark et al., 2009; Wu et al., 2010).
",2.1 Surprisal,[0],[0]
"Surprisal predicts the integration difficulty that a word xt at time step t presents given the preceding context and is calculated as follows:
surprisal(xt) =",2.1 Surprisal,[0],[0]
"− log2
( ∑
s∈S(x1...xt) P (s)
∑
s∈S(x1...xt−1) P (s)
)
(1)
where S(x1 . . .",2.1 Surprisal,[0],[0]
xt) is the set of syntactic trees whose leaves have x1 . . .,2.1 Surprisal,[0],[0]
xt,2.1 Surprisal,[0],[0]
as a prefix.,2.1 Surprisal,[0],[0]
"1
In essence, surprisal measures how unexpected constructions are in a given context.",2.1 Surprisal,[0],[0]
What it does not provide is an explanation for why certain constructions would be less common and thus more surprising.,2.1 Surprisal,[0],[0]
"Processing difficulty can also be measured in terms of entropy (Shannon, 1948).",2.2 Entropy Reduction,[0],[0]
A larger entropy over a random variable corresponds to greater uncertainty over the observed value it will take.,2.2 Entropy Reduction,[0],[0]
The entropy of a syntactic derivation over the sequence x1 . . .,2.2 Entropy Reduction,[0],[0]
"xt is calculated as:2
H(x1...t) =",2.2 Entropy Reduction,[0],[0]
"∑
s∈S(x1...xt)
−P (s) · log2 P (s) (2)
Reduction in entropy has been found to predict processing complexity (Hale, 2003; Hale, 2006; Roark et al., 2009; Wu et al., 2010; Hale, 2011):
∆H(x1...t) = max(0, H(x1...t−1)−H(x1...t))",2.2 Entropy Reduction,[0],[0]
"(3)
This measures the change in uncertainty about the discourse as each new word is processed.",2.2 Entropy Reduction,[0],[0]
"In Dependency Locality Theory (DLT) (Gibson, 2000), complexity arises from intervening referents introduced between a predicate and its argument.",3.1 Dependency Locality,[0],[0]
"Under the original formulation of DLT, there is a
1The parser in this study uses a beam.",3.1 Dependency Locality,[0],[0]
"However, given high parser accuracy, Roark (2001) showed that calculating complexity metrics over a beam should obtain similar results to the full complexity calculation.",3.1 Dependency Locality,[0],[0]
"2The incremental formulation used here was first proposed in Wu et al. (2010).
",3.1 Dependency Locality,[0],[0]
storage cost for each new referent introduced and an integration cost for each referent intervening in a dependency projection.,3.1 Dependency Locality,[0],[0]
"This is a simplification made for ease of computation, and subsequent work has found DLT to be more accurate cross-linguistically if the intervening elements are structurally defined rather than defined in terms of referents (Kwon et al., 2010).",3.1 Dependency Locality,[0],[0]
"That is, simply having a particular referent intervene in a dependency projection may not have as great an effect on processing complexity as the syntactic construction the referent appears in.",3.1 Dependency Locality,[0],[0]
"Therefore, this work reinterprets the costs of dependency locality to be related to the events of beginning a center embedding (storage) and completing a center embedding (integration).",3.1 Dependency Locality,[0],[0]
"Note that antilocality effects (where longer dependencies are easier to process) have also been observed in some languages, and DLT is unable to account for these phenomena (Vasishth and Lewis, 2006).",3.1 Dependency Locality,[0],[0]
"Processing complexity has also been attributed to confusability (Lewis and Vasishth, 2005) as defined in domain-general cognitive models like ACT-R (Anderson et al., 2004).
",3.2 ACT-R,[0],[0]
ACT-R is based on theories of neural activation.,3.2 ACT-R,[0],[0]
Each new word is encoded and stored in working memory until it is retrieved at a later point for modification before being re-encoded into the parse.,3.2 ACT-R,[0],[0]
"A newly observed sign (word) associatively activates any appropriate arguments from working memory, so multiple similarly appropriate arguments would slow processing as the parser must choose between the highly activated hypotheses.",3.2 ACT-R,[0],[0]
Any intervening signs (words or phrases) that modify a previously encoded sign re-activate it and raise its resting activation potential.,3.2 ACT-R,[0],[0]
"This can ease later retrieval of that sign in what is termed an anti-locality effect, contra predictions of DLT.",3.2 ACT-R,[0],[0]
"In this way, returning out of an embedded clause can actually speed processing by having primed the retrieved sign before it was needed.",3.2 ACT-R,[0],[0]
ACT-R attributes locality phenomena to frequency effects (e.g. unusual constructions) overriding such priming and to activation decay if embedded signs do not prime the target sign through modification (as in parentheticals).,3.2 ACT-R,[0],[0]
"Finally, ACT-R predicts something like DLT’s storage cost due to the need to differentiate each newly encoded sign from

those previously encoded (similarity-based encoding interference) (Lewis et al., 2006).",3.2 ACT-R,[0],[0]
"Current models of working memory in structured tasks are defined in terms of hierarchies of sequential processes, in which superordinate sequences can be interrupted by subordinate sequences and resume when the subordinate sequences have concluded (Botvinick, 2007).",3.3 Hierarchic Sequential Prediction,[0],[0]
"These models rely on temporal cueing as well as content-based cueing to explain how an interrupted sequence may be recalled for continuation.
",3.3 Hierarchic Sequential Prediction,[0],[0]
"Temporal cueing is based on a context of temporal features for the current state (Howard and Kahana, 2002).",3.3 Hierarchic Sequential Prediction,[0],[0]
The temporal context in which the subordinate sequence concludes must be similar enough to the temporal context in which it was initiated to recall where in the superordinate sequence the subordinate sequence occurred.,3.3 Hierarchic Sequential Prediction,[0],[0]
"For example, the act of making breakfast may be interrupted by a phone call.",3.3 Hierarchic Sequential Prediction,[0],[0]
"Once the call is complete, the temporal context is sufficiently similar to when the call began that one is able to continue preparing breakfast.",3.3 Hierarchic Sequential Prediction,[0],[0]
"The association between the current temporal context and the temporal context prior to the interruption is strong enough to cue the next action.
",3.3 Hierarchic Sequential Prediction,[0],[0]
"Temporal cueing is complemented by sequential (content-based) cueing (Botvinick, 2007) in which the content of an individual element is associated with, and thus cues, the following element.",3.3 Hierarchic Sequential Prediction,[0],[0]
"For example, recalling the 20th note of a song is difficult, but when playing the song, each note cues the fol-
lowing note, leading one to play the 20th note without difficulty.
",3.3 Hierarchic Sequential Prediction,[0],[0]
"Hierarchic sequential prediction may be directly applicable to processing syntactic center embeddings (van Schijndel et al., in press).",3.3 Hierarchic Sequential Prediction,[0],[0]
An ongoing parse may be viewed graph-theoretically as one or more connected components of incomplete phrase structure trees (see Figure 1).,3.3 Hierarchic Sequential Prediction,[0],[0]
"Beginning a new subordinate sequence (a center embedding) introduces a new connected component, disjoint from that of the superordinate sequence.",3.3 Hierarchic Sequential Prediction,[0],[0]
"As the subordinate sequence proceeds, the new component gains associated discourse referents, each sequentially cued from the last, until finally it merges with the superordinate connected component at the end of the embedded clause, forming a single connected component representing the parse up to that point.",3.3 Hierarchic Sequential Prediction,[0],[0]
"Since it is not connected to the subordinate connected component prior to merging, the superordinate connected component must be recalled through temporal cueing.
",3.3 Hierarchic Sequential Prediction,[0],[0]
"McElree (2001; 2006) has found that retrieval of any non-focused (or in this case, unconnected) element from memory leads to slower processing.",3.3 Hierarchic Sequential Prediction,[0],[0]
"Therefore, integrating two disjoint connected components should be expected to incur a processing cost due to the need to recall the current state of the superordinate sequence to continue the parse.",3.3 Hierarchic Sequential Prediction,[0],[0]
Such a cost would corroborate a DLT-like theory where integration slows processing.,3.3 Hierarchic Sequential Prediction,[0],[0]
Language processing is typically centered in the left hemisphere of the brain (for right-handed individuals).,3.4 Dynamic Recruitment of Additional Processing Resources,[0],[0]
Just and Varma (2007) provide fMRI results suggesting readers dynamically recruit additional processing resources such as the right-side homologues of the language processing areas of the brain when processing center-embedded constructions.,3.4 Dynamic Recruitment of Additional Processing Resources,[0],[0]
"Once an embedded construction terminates, the reader may still have temporary access to these extra processing resources, which may briefly speed processing.
",3.4 Dynamic Recruitment of Additional Processing Resources,[0],[0]
"This hypothesis would, therefore, predict an encoding cost when a center embedding is initiated.",3.4 Dynamic Recruitment of Additional Processing Resources,[0],[0]
"The resulting inhibition would trigger recruitment of additional processing resources, which would then
allow the rest of the embedded structure to be processed at the usual speed.",3.4 Dynamic Recruitment of Additional Processing Resources,[0],[0]
"Upon completing an embedding, the difficulty arising from memory retrieval (McElree, 2001) would be ameliorated by these extra processing resources, and the reduced processing complexity arising from reduced memory load would yield a temporary facilitation in processing.",3.4 Dynamic Recruitment of Additional Processing Resources,[0],[0]
"No longer requiring the additional resources to cope with the increased embedding, the processor would release them, returning the processor to its usual speed.",3.4 Dynamic Recruitment of Additional Processing Resources,[0],[0]
"Unlike anti-locality, where processing is facilitated in longer passages due to accumulating probabilistic evidence, a model of dynamic recruitment of additional processing resources would predict universal facilitation after a center embedding of any length, modulo frequency effects.",3.4 Dynamic Recruitment of Additional Processing Resources,[0],[0]
"Wu et al. (2010) propose an explicit measure of the difficulty associated with processing centerembedded constructions, which is similar to the predictions of dynamic recruitment and is defined in terms of changes in memory load.",3.5 Embedding Difference,[0],[0]
"They calculate a probabilistically-weighted average embedding depth as follows:
µemb(x1 . . .",3.5 Embedding Difference,[0],[0]
"xt) = ∑
s∈S(x1...xt)
d(s) ·",3.5 Embedding Difference,[0],[0]
"P (s) (4)
where d(s) returns the embedding depth of the derivation s at xt in a variant of a left-corner parsing process.3 Embedding difference may then be derived as:
EmbDiff (x1 . . .",3.5 Embedding Difference,[0],[0]
xt) =µemb(x1 . . .,3.5 Embedding Difference,[0],[0]
"xt)− (5)
µemb(x1 . . .",3.5 Embedding Difference,[0],[0]
"xt−1)
",3.5 Embedding Difference,[0],[0]
This is hypothesized to correlate positively with processing load: increasing the embedding depth increases processing load and decreasing it reduces processing load.,3.5 Embedding Difference,[0],[0]
Note that embedding difference makes the opposite prediction from DLT in that integrating an embedded clause is predicted to speed processing.,3.5 Embedding Difference,[0],[0]
"In fact, the predictions of embedding
3As pointed out by Wu et al. (2010), in practice this can be computed over a beam of potential parses in which case it must be normalized by the total probability of the beam.
difference are such that it may be viewed as an implementation of the predictions of a hierarchic sequential processing model with dynamic recruitment of additional resources.",3.5 Embedding Difference,[0],[0]
"This paper uses a hierarchic sequence model implementation of a left-corner parser variant (van Schijndel et al., in press), which represents connected components of phrase structure trees in hierarchies of hidden random variables.",4 Model,[0],[0]
"This requires, at each time step t:
• a hierarchically-organized set of N connected component states qnt , each consisting of an active sign of category aqn
t , and an awaited sign
of category bqn t , separated by a slash ‘/’; and
• an observed word xt.
",4 Model,[0],[0]
"Each connected component state in this model then represents a contiguous portion of a phrase structure tree (see Figure 1 on preceding page).
",4 Model,[0],[0]
"The operations of this parser can be defined as a deductive system (Shieber et al., 1995) with an input sequence consisting of a top-level connected component state ⊤/⊤, corresponding to an existing discourse context, followed by a sequence of observed words x1, x2, . . .",4 Model,[0],[0]
4,4 Model,[0],[0]
"If an observation xt can attach as the awaited sign of the most recent (most subordinate) connected component a/b, it is hypothesized to do so, turning this incomplete sign into a complete sign a (F–, below); or if the observation can serve as a lower descendant of this awaited sign, it is hypothesized to form the first complete sign a′ in a newly initiated connected component (F+):
a/b xt a b → xt (F–)
a/b xt a/b a′ b + → a′ ... ; a′ → xt (F+)
Then, if either of these complete signs (a or a′ above, matched to a′′ below) can attach as an initial
4A deductive system consists of inferences or productions
of the form: P
Q R, meaning premise P entails conclusion Q ac-
cording to rule R.
⊤/⊤",4 Model,[0],[0]
"the ⊤/⊤, D F+
⊤/⊤, NP/N L– studio
⊤/⊤, NP F–
⊤/⊤, S/VP L–
bought
⊤/⊤, S/VP, V F+
⊤/⊤, S/NP L+
the
⊤/⊤, S/NP, D F+
⊤/⊤, S/NP, NP/N L–
publisher
⊤/⊤, S/NP, NP F–
⊤/⊤, S/NP, D/G L–
’s
⊤/⊤, S/NP, D F–
child of the awaited sign of the immediately superordinate connected component state a/b, it is hypothesized to do so and terminate the subordinate connected component state, with xt as the last observation of the terminated connected component (L+); or if the observation can serve as a lower descendant of this awaited sign, it is hypothesized to remain disjoint and form its own connected component (L–):
a/b a′′
a/b′′ b → a′′ b′′ (L+)
a/b a′′
a/b a′/b′′ b
+ → a′ ... ; a′ → a′′ b′′ (L–)
",4 Model,[0],[0]
These operations can be made probabilistic.,4 Model,[0],[0]
"The probability σ of a transition at time step t is defined in terms of (i) a probability φ of initiating a new connected component state with xt as its first observation, multiplied by (ii) the probability λ of terminating a connected component state with xt as its last observation, multiplied by (iii) the probabilities α and β of generating categories for active and awaited signs aqn
t and bqn t in the resulting most subordinate
connected component state qnt .",4 Model,[0],[0]
"This kind of model can be defined directly on PCFG probabilities and trained to produce state-of-the-art accuracy by using the latent variable annotation of Petrov et al. (2006) (van Schijndel et al., in press).5
",4 Model,[0],[0]
An example parse is shown in Figure 2.,4 Model,[0],[0]
"Since two binary structural decisions (F and L) must be made in order to generate each word, there are four possible structures that may be generated (see Table 1).",4 Model,[0],[0]
The F+L– transition initiates a new level of embedding at word xt and so requires the superordinate state to be encoded for later retrieval (e.g. on observing the in Figure 2).,4 Model,[0],[0]
"The F–L+ transition completes the deepest level of embedding and therefore requires the recall of the current superordinate connected component state with which the
5The model has been shown to achieve an F-score of 87.8, within .2 points of the Petrov and Klein (2007) parser, which obtains an F-score of 88.0 on the same task.",4 Model,[0],[0]
"Because the sequence model is defined over binary-branching phrase structure, both parsers were evaluated on binary-branching phrase structure trees to provide a fair comparison.
",4 Model,[0],[0]
subordinate connected component state will be integrated.,4 Model,[0],[0]
"For example, in Figure 2, upon observing ’s, the parser must use temporal cueing to recall that it is in the middle of processing an NP (to complete an S), which sequentially cues a prediction of N. F–L– transitions complete the awaited sign of the most subordinate state and so sequentially cue a following connected component state at the same tier of the hierarchy.",4 Model,[0],[0]
"For example, in Figure 2, after observing studio, the parser uses the completed NP to sequentially cue the prediction that it has finished the left child of an S. F+L+ transitions locally expand the awaited sign of the most subordinate state and so should also not require any recall or encoding.",4 Model,[0],[0]
"For example, in Figure 2, observing bought while awaiting a VP sequentially cues a prediction of NP.
F+L–, then, loosely corresponds to a storage action under DLT as more hierarchic levels must now be maintained at each future step of the parse.",4 Model,[0],[0]
"As stated before, it differs from DLT in that it is sensitive to the depth of embedding rather than a particular subset of syntactic categories.",4 Model,[0],[0]
Wu et al. (2010) found that increasing the embedding depth led to longer reading times in a self-paced reading experiment.,4 Model,[0],[0]
"In ACT-R terms, F+L– corresponds to an encoding action, potentially causing processing difficulty resulting from the similarity of the current sign to previously encoded signs.
",4 Model,[0],[0]
"F–L+, by contrast, is similar to DLT’s integration action since a subordinate connected component is integrated into the rest of the parse structure.",4 Model,[0],[0]
"This represents a temporal cueing event in which the awaited category of the superordinate connected
component is recalled.",4 Model,[0],[0]
"In contrast to DLT, embedding difference and dynamic recruitment would predict a shorter reading time in the F–L+ case because of the reduction in memory load.",4 Model,[0],[0]
"In an ACT-R framework, reading time durations can increase at the retrieval site because the retrieval causes competition among similarly encoded signs in the context set.",4 Model,[0],[0]
"While it is possible for reading times to decrease when completing a center embedding in ACT-R (Vasishth and Lewis, 2006), this would be expressed as a frequency effect due to certain argument types commonly foreshadowing their predicates (Jaeger et al., 2008).",4 Model,[0],[0]
"Since frequency effects are factored separately from memory effects in this study, ACT-R would predict longer residual (memory-based) reading times when completing an embedding.
",4 Model,[0],[0]
"Predicted correlations to reading times for the F
and L transitions are summarized in Table 2.",4 Model,[0],[0]
"Eye-tracking and reading time data are often used to test complexity measures (Gibson, 2000; Demberg and Keller, 2008; Roark et al., 2009) under the assumption that readers slow down when reading more complex passages.",5 Eye-tracking,[0],[0]
"Readers saccade over portions of text and regress back to preceding text in complex patterns, but studies have correlated certain measures with certain processing constraints (see Clifton et al. 2007 for a review).",5 Eye-tracking,[0],[0]
"For example, the initial length of time fixated on a single word is correlated with word identification time; whereas regression durations after a word is fixated (but prior to a fixation in a new region) are hypothesized to correlate
with integration difficulty.
",5 Eye-tracking,[0],[0]
"Since this work focuses on incremental processing, all processing that occurs up to a given point in the sentence is of interest.",5 Eye-tracking,[0],[0]
"Therefore, in this study, predictions will be compared to go-past durations.",5 Eye-tracking,[0],[0]
"Go-past durations are calculated by summing all fixations in a region of text, including regressions, until a new region is fixated, which accounts for additional processing that may take place after initial lexical access, but before the next region is processed.",5 Eye-tracking,[0],[0]
"For example, if one region ends at word 5 in a sentence, and the next fixation lands on word 8, then the go-past region consists of words 6-8 and the go-past duration sums all fixations until a fixation occurs after word 8.",5 Eye-tracking,[0],[0]
"The measures presented in this paper were evaluated on the Dundee eye-tracking corpus (Kennedy et al., 2003).",6 Evaluation,[0],[0]
The corpus consists of 2388 sentences of naturally occurring news text written in standard British English.,6 Evaluation,[0],[0]
"The corpus also includes eye-tracking data from 10 native English speakers, which provides a test corpus of 260,124 subject-duration pairs of reading time data.",6 Evaluation,[0],[0]
"Of this, any fixated words appearing fewer than 5 times in the training data were considered unknown and were filtered out to obtain accurate predictions.",6 Evaluation,[0],[0]
Fixations on the first or last words of a line were also filtered out to avoid any ‘wrap-up’ effects resulting from preparing to saccade to the beginning of the next line or resulting from orienting to a new line.,6 Evaluation,[0],[0]
"Additionally, following Demberg and Keller (2008), any fixations that skip more than 4 words were attributed to track loss by the eyetracker or lack of attention of the reader and so were excluded from the analysis.",6 Evaluation,[0],[0]
"This left the final evaluation corpus with 151,331 subject-duration pairs.
",6 Evaluation,[0],[0]
"The evaluation consisted of fitting a linear mixedeffects model (Baayen et al., 2008) to reading time durations using the lmer function of the lme4 R package (Bates et al., 2011; R Development Core Team, 2010).",6 Evaluation,[0],[0]
"This allowed by-subject and by-item variation to be included in the initial regression as random intercepts in addition to several baseline predictors.6 Before fitting, the durations extracted from
6Each fixed effect was centered to reduce collinearity.
",6 Evaluation,[0],[0]
"the corpus were log-transformed, producing more normally distributed data to obey the assumptions of linear mixed effects models.7
Included among the fixed effects were the position in the sentence that initiated the go-past region (SENTPOS) and the number of characters in the initiating word (NRCHAR).",6 Evaluation,[0],[0]
"The difficulty of integrating a word may be seen in whether the immediately following word was fixated (NEXTISFIX), and similarly if the immediately previous word was fixated (PREVISFIX)",6 Evaluation,[0],[0]
the current word probably need not be fixated for as long.,6 Evaluation,[0],[0]
"Finally, unigram (LOGPROB) and bigram probabilities are included.",6 Evaluation,[0],[0]
The bigram probabilities are those of the current word given the previous word (LOGFWPROB) and the current word given the following word (LOGBWPROB).,6 Evaluation,[0],[0]
"Fossum and Levy (2012) showed that for n-gram probabilities to be effective predictors on the Dundee corpus, they must be calculated from a wide variety of texts, so following them, this study used the Brown corpus (Francis and Kucera, 1979), the WSJ Sections 02-21 (Marcus et al., 1993), the written text portion of the British National Corpus (BNC Consortium, 2007), and the Dundee corpus (Kennedy et al., 2003).",6 Evaluation,[0],[0]
This amounted to an n-gram training corpus of roughly 87 million words.,6 Evaluation,[0],[0]
"These statistics were smoothed using the SRILM (Stolcke, 2002) implementation of modified Kneser-Ney smoothing (Chen and Goodman, 1998).",6 Evaluation,[0],[0]
"Finally, total surprisal (SURP) was included to account for frequency effects in the baseline.
",6 Evaluation,[0],[0]
"The preceding measures are commonly used in baseline models to fit reading time data (Demberg and Keller, 2008; Frank and Bod, 2011; Fossum and Levy, 2012) and were calculated from the final word of each go-past region.",6 Evaluation,[0],[0]
The following measures create a more sophisticated baseline by accumulating over the entire go-past region to capture what must be integrated into the discourse to continue the parse.,6 Evaluation,[0],[0]
One factor (CWDELTA) simply counts the number of words in each go-past region.,6 Evaluation,[0],[0]
"Cumula-
",6 Evaluation,[0],[0]
"7In particular, these models assume the noise in the data is normally distributed.",6 Evaluation,[0],[0]
Initial exploratory trials showed that the residuals of fitting any sensible baseline also become more normally distributed if the response variable is log-transformed.,6 Evaluation,[0],[0]
"Finally, the directions of the effects remain the same whether or not the reading times are log-transformed, though significance cannot be ascertained without the transform.
",6 Evaluation,[0],[0]
"tive total surprisal (CUMUSURP) and cumulative entropy reduction (ENTRED) give the surprisal (Hale, 2001) and entropy reduction (Hale, 2003) summed over the go-past region.",6 Evaluation,[0],[0]
"To avoid convergence issues, each of the cumulative measures is residualized from the next simpler model in the following order: CWDELTA from the standard baseline, CUMUSURP from the baseline with CWDELTA, and ENTRED from the baseline with all other effects.
",6 Evaluation,[0],[0]
Residualization was accomplished by using the simpler mixed-effects model to fit the measure of interest.,6 Evaluation,[0],[0]
The residuals from that model fit were then used in place of the factor of interest.,6 Evaluation,[0],[0]
All joint interactions were included in the baseline model as well.,6 Evaluation,[0],[0]
"Finally, to account for spillover effects (Just et al., 1982) where processing from a previous region contributes to the following duration, the above baseline predictors from the previous go-past region were included as factors for the current region.
",6 Evaluation,[0],[0]
"Having SURP as a predictor with CUMUSURP may seem redundant, but initial analyses showed SURP was a significant predictor over CUMUSURP when CWDELTA was a separate factor in the baseline (current: p = 2.2 · 10−16 spillover: p = 2 · 10−15) and vice versa (current: p = 2.2 · 10−16 spillover: p = 6 · 10−5).",6 Evaluation,[0],[0]
One reason for this could be that go-past durations conflate complexity experienced when initially fixating on a region with the difficulty experienced during regressions.,6 Evaluation,[0],[0]
"By including both versions of surprisal, the model is able to account for frequency effects occurring in both conditions.
",6 Evaluation,[0],[0]
"This study is only interested in how well the proposed memory-based measures fit the data over the baseline, so to avoid fitting to the test data or weakening the baseline by overfitting to training data, the full baseline was used in the final evaluation.
",6 Evaluation,[0],[0]
Each measure proposed in this paper was summed over go-past regions to make it cumulative and was residualized from all non-spillover factors before being included on top of the full baseline as a main effect.,6 Evaluation,[0],[0]
"Likewise, the spillover version of each proposed measure was residualized from the other spillover factors before being included as a main effect.",6 Evaluation,[0],[0]
Only a single proposed measure (or its spillover corrollary) was included in each model.,6 Evaluation,[0],[0]
The results shown in Table 3 reflect the probability of the full model fit being obtained by the model lacking each factor of interest.,6 Evaluation,[0],[0]
"This was found via posterior sam-
pling of each factor using the Markov chain Monte Carlo implementation of the languageR R package (Baayen, 2008).
",6 Evaluation,[0],[0]
The results indicate that the F+L– and F–L+ measures were both significant predictors of duration as expected.,6 Evaluation,[0],[0]
"Further, F–L– and F+L+, which both simply reflect sequential cueing, were not significant predictors of go-past duration, also as expected.",6 Evaluation,[0],[0]
The fact that F+L– was strongly predictive over the baseline is encouraging as it suggests that memory limitations could provide at least a partial explanation of why certain constructions are less frequent in corpora and thus yield a high surprisal.,7 Discussion and Conclusion,[0],[0]
"Moreover, it indicates that the model corroborates the shared prediction of most of the memory-based models that initiating a new connected component slows processing.
",7 Discussion and Conclusion,[0],[0]
"The fact that F–L+ is predictive but has a negative coefficient could be evidence of anti-locality, or it could be an indication of some sort of processing momentum due to dynamic recruitment of additional processing resources (Just and Varma, 2007).",7 Discussion and Conclusion,[0],[0]
"Since anti-locality is an expectation-based frequency effect, and since this study controlled for frequency effects with n-grams, surprisal, and entropy reduction, an anti-locality explanation would rely on either (i) more precise variants of the metrics used in this study or (ii) other frequency metrics altogether.",7 Discussion and Conclusion,[0],[0]
"Future work could investigate the possibility of anti-locality by looking at the distance between an encoding operation and its corresponding
integration action to see if the integration facilitation observed in this study is driven by longer embeddings or if there is simply a general facilitation effect when completing embeddings.
",7 Discussion and Conclusion,[0],[0]
"The finding of a negative integration cost was previously observed by Wu et al. (2010) as well as Demberg and Keller (2008), although Demberg and Keller calculated it using the original referent-based definitions of Gibson (1998; 2000) and varied which parts of speech counted for calculating integration cost.",7 Discussion and Conclusion,[0],[0]
"Ultimately, Demberg and Keller (2008) concluded that the negative coefficient was evidence that integration cost was not a good broad-coverage predictor of reading times; however, this study has replicated the effect and showed it to be a very strong predictor of reading times, albeit one that is correlated with facilitation rather than inhibition.
",7 Discussion and Conclusion,[0],[0]
"It is interesting that many studies have found negative integration cost using naturalistic stimuli while others have consistently found positive integration cost when using constructed stimuli with multiple center embeddings presented without context (Gibson, 2000; Chen et al., 2005; Kwon et al., 2010).",7 Discussion and Conclusion,[0],[0]
It may be the case that any dynamic recruitment is overwhelmed by the memory demands of multiply center-embedded stimuli.,7 Discussion and Conclusion,[0],[0]
"Alternatively, it may be that the difficulty of processing multiply center-embedded sentences containing ambiguities produces anxiety in subjects, which slows processing at implicit prosodic boundaries (Fodor, 2002; Mitchell et al., 2008).",7 Discussion and Conclusion,[0],[0]
"In any case, the source of this discrepancy presents an attractive target for future research.
",7 Discussion and Conclusion,[0],[0]
"In general, sequential prediction does not seem to present people with any special ease or difficulty as evidenced by the lack of significance of F–L– and F+L+ predictions when frequency effects are factored out.",7 Discussion and Conclusion,[0],[0]
"This supports a theory of sequential, content-based cueing (Botvinick, 2007) that predicts that certain states would directly cue other states and thus avoid recall difficulty.",7 Discussion and Conclusion,[0],[0]
An example of this may be seen in the case of a transitive verb triggering the prediction of a direct object.,7 Discussion and Conclusion,[0],[0]
"This kind of cueing would show up as a frequency effect predicted by surprisal rather than as a memory-based cost, due to frequent occurrences becoming ingrained as a learned skill.",7 Discussion and Conclusion,[0],[0]
"Future work could use these sequential cueing operations to investigate further claims
of the dynamic recruitment hypothesis.",7 Discussion and Conclusion,[0],[0]
"One of the implications of the hypothesis is that recruitment of resources alleviates the initial encoding cost, which allows the parser to continue on as before the embedding.",7 Discussion and Conclusion,[0],[0]
"DLT, on the other hand, predicts that there is a storage cost for maintaining unresolved dependencies during a parse (Gibson, 2000).",7 Discussion and Conclusion,[0],[0]
"By weighting each of the sequential cueing operations with the embedding depth at which it occurs, an experiment may be able to test these two predictions.
",7 Discussion and Conclusion,[0],[0]
This study has shown that measures based on working memory operations have strong predictivity over other previously proposed measures including those associated with frequency effects.,7 Discussion and Conclusion,[0],[0]
This suggests that memory limitations may provide a partial explanation of what gives rise to frequency effects.,7 Discussion and Conclusion,[0],[0]
"Lastly, this paper provides evidence that there is a robust facilitation effect in English that arises from completing center embeddings.
",7 Discussion and Conclusion,[0],[0]
"The hierarchic sequence model, all evaluation scripts, and regression results for all baseline predictors used in this paper are freely available at http://sourceforge.net/projects/modelblocks/.",7 Discussion and Conclusion,[0],[0]
"Thanks to Peter Culicover, Micha Elsner, and three anonymous reviewers for helpful suggestions.",Acknowledgements,[0],[0]
This work was funded by an OSU Department of Linguistics Targeted Investment for Excellence (TIE) grant for collaborative interdisciplinary projects conducted during the academic year 2012-13.,Acknowledgements,[0],[0]
The frequency of words and syntactic constructions has been observed to have a substantial effect on language processing.,abstractText,[0],[0]
This begs the question of what causes certain constructions to be more or less frequent.,abstractText,[0],[0]
"A theory of grounding (Phillips, 2010) would suggest that cognitive limitations might cause languages to develop frequent constructions in such a way as to avoid processing costs.",abstractText,[0],[0]
This paper studies how current theories of working memory fit into theories of language processing and what influence memory limitations may have over reading times.,abstractText,[0],[0]
Measures of such limitations are evaluated on eye-tracking data and the results are compared with predictions made by different theories of processing.,abstractText,[0],[0]
An Analysis of Frequency- and Memory-Based Processing Costs,title,[0],[0]
"erties of training a two-layered ReLU network g(x;w) = ∑K
j=1 σ(w ⊺
j x) with centered d-dimensional spherical Gaussian input x (σ=ReLU). We train our network with gradient descent on w to mimic the output of a teacher network with the same architecture and fixed parameters w∗. We show that its population gradient has an analytical formula, leading to interesting theoretical analysis of critical points and convergence behaviors. First, we prove that critical points outside the hyperplane spanned by the teacher parameters (“out-of-plane“) are not isolated and form manifolds, and characterize inplane critical-point-free regions for two ReLU case. On the other hand, convergence to w∗ for one ReLU node is guaranteed with at least (1 − ǫ)/2 probability, if weights are initialized randomly with standard deviation upper-bounded by
O(ǫ/ √ d), consistent with empirical practice. For network with many ReLU nodes, we prove that an infinitesimal perturbation of weight initialization results in convergence towards w∗ (or its permutation), a phenomenon known as spontaneous symmetric-breaking (SSB) in physics. We assume no independence of ReLU activations. Simulation verifies our findings.",text,[0],[0]
"Despite empirical success of deep learning (e.g., Computer Vision (He et al., 2016; Simonyan & Zisserman, 2015; Szegedy et al., 2015; Krizhevsky et al., 2012), Natural Language Processing (Sutskever et al., 2014) and Speech Recognition (Hinton et al., 2012)), it remains elusive how
1Facebook AI Research.",1. Introduction,[0],[0]
Correspondence to: Yuandong Tian,1. Introduction,[0],[0]
"<yuandong@fb.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
and why simple methods like gradient descent can solve the complicated non-convex optimization during training.,1. Introduction,[0],[0]
"In this paper, we focus on a two-layered ReLU network:
g(x;w) =
K ∑
j=1
σ(w⊺j x), (1)
Here σ(x) = max(x, 0) is the ReLU nonlinearity.",1. Introduction,[0.9999999418814721],"['In this paper, we focus on a two-layered ReLU network: g(x;w) = K ∑ j=1 σ(w⊺j x), (1) Here σ(x) = max(x, 0) is the ReLU nonlinearity.']"
"We consider the setting that a student network is optimized to minimize the l2 distance between its prediction and the supervision provided by a teacher network of the same architecture with fixed parameters w∗. Note that although the network prediction (Eqn. 1) is convex, when coupled with loss (e.g., l2 loss Eqn. 2), the optimization becomes highly non-convex and has exponential number of critical points.
",1. Introduction,[0],[0]
"To analyze it, we introduce a simple analytic formula for population gradient in the case of l2 loss, when inputs x are sampled from zero-mean spherical Gaussian.",1. Introduction,[0],[0]
"Using this formula, critical point and convergence analysis follow.
",1. Introduction,[0.9999999811169652],"['Using this formula, critical point and convergence analysis follow.']"
"For critical points, we show that critical points outside the principal hyperplane (the subspace spanned by w∗) form manifolds.",1. Introduction,[0],[0]
"We also characterize the region in the principal hyperplane that has no critical points, in two ReLU case.
",1. Introduction,[0],[0]
We also analyze the convergence behavior under the population gradient.,1. Introduction,[0],[0]
"Using Lyapunov method (LaSalle & Lefschetz, 1961), for single ReLU case we prove that gradient descent converges to w∗ with at least (1 − ǫ)/2 probability, if initialized randomly with standard deviation
upper-bounded by O(ǫ/ √ d), verifying common initialization techniques (Bottou, 1988; Glorot & Bengio, 2010; He et al., 2015; LeCun et al., 2012).",1. Introduction,[0],[0]
"For multiple ReLU case, when the teacher parameters {wj}Kj=1 form an orthonormal basis, we prove that (1) a symmetric weight initialization gets stuck at a saddle point and (2) a particular infinitesimal perturbation of (1) leads to convergence towards w∗ or its permutation.",1. Introduction,[0],[0]
"The behavior that the population gradient field is invariant under certain symmetry but the solution breaks it, is known as spontaneous symmetry breaking in physics.",1. Introduction,[0],[0]
"Although such behaviors are known practically, to our knowledge, we first formally characterize them in 2-layered ReLU network.",1. Introduction,[0],[0]
"Codes are available 1.
",1. Introduction,[0],[0]
1github.com/yuandong-tian/ICML17_ReLU,1. Introduction,[0],[0]
"For multilayer linear network, many works analyze its critical points and convergence behaviors.",2. Related Works,[0],[0]
"(Saxe et al., 2013) analyzes its dynamics of gradient descent and (Kawaguchi, 2016) shows every local minimum is global.",2. Related Works,[0],[0]
"On the other hand, very few theoretical works have been done for nonlinear networks.",2. Related Works,[0],[0]
"(Mei et al., 2016) shows the global convergence for a single nonlinear node whose derivatives of activation σ′, σ′′, σ′′′ are bounded and σ′ > 0.",2. Related Works,[0],[0]
"Similar to our approach, (Saad & Solla, 1996) also uses the student-teacher setting and analyzes the student dynamics when the teacher’s parameters w∗ are orthonormal.",2. Related Works,[0],[0]
"However, their activation is Gaussian error function erf(x), and only the local behaviors of the two critical points (the initial saddle point near the origin and w∗) are analyzed.",2. Related Works,[0],[0]
"Recent paper (Zhang et al., 2017) analyzes a similar teacher-student setting on 2-layered network when the involved function is harmonic, but it is unclear how the conclusion is generalized to ReLU case.",2. Related Works,[0],[0]
"To our knowledge, our close-form formula for 2-layered ReLU network is novel, as well as the critical point and convergence analysis.",2. Related Works,[0],[0]
"Concurrent work (Brutzkus & Globerson, 2017) proposes the same formula with a different approach, and provides similar convergence analysis for one node.",2. Related Works,[0],[0]
"For multiple nodes, they assume non-overlapping shared weights, a special case of our assumption (Sec. 6.2) that weights are cyclically symmetric and orthonormal.
",2. Related Works,[0],[0]
Many previous works analyze nonlinear network based on the assumption of independent activations: the activations of ReLU (or other nonlinear) nodes are independent of the input and/or mutually independent.,2. Related Works,[0],[0]
"For example, (Choromanska et al., 2015a;b) relates the nonlinear ReLU network with spin-glass models when several assumptions hold, including the assumption of independent activations (A1p and A5u).",2. Related Works,[0],[0]
"(Kawaguchi, 2016) proves that every local minimum in nonlinear network is global based on similar assumptions.",2. Related Works,[0],[0]
"(Soudry & Carmon, 2016) shows the global
optimality of the local minimum in a two-layered ReLU network, when independent multiplicative Bernoulli noise is applied to the activations.",2. Related Works,[0],[0]
"In practice, activations that share the input are highly dependent.",2. Related Works,[0],[0]
"Ignoring such dependency misses important behaviors, and may lead to misleading conclusions.",2. Related Works,[0],[0]
"In this paper, no assumption of independent activations is made.",2. Related Works,[0],[0]
"Instead, we assume input to follow spherical Gaussian distribution, which gives more realistic and interdependent activations during training.
",2. Related Works,[0],[0]
"For sigmoid activation, (Fukumizu & Amari, 2000) gives complicated conditions for a local minimum to be global when adding a new node to a 2-layered network.",2. Related Works,[0],[0]
"(Janzamin et al., 2015) gives guarantees for parameter recovery of a 2-layered network learnt with tensor decomposition.",2. Related Works,[0],[0]
"In comparison, we analyze ReLU networks trained with gradient descent, which is more popular in practice.",2. Related Works,[0],[0]
Denote N as the number of samples and d as the input dimension.,3. Problem Definition,[1.0],['Denote N as the number of samples and d as the input dimension.']
The N -by-d matrix X is the input data and w∗ is the fixed parameter of the teacher network.,3. Problem Definition,[0],[0]
"Given the current estimation w, we have the following l2 loss:
J(w) = 1
2 ‖g(X;w∗)− g(X;w)‖2, (2)
Here we focus on population loss",3. Problem Definition,[0],[0]
"EX [J ], where the input X is assumed to follow spherical Gaussian distribution N (0, I).",3. Problem Definition,[0],[0]
Its gradient is the population gradient EX [∇Jw(w)] (abbrev.,3. Problem Definition,[1.0],['Its gradient is the population gradient EX [∇Jw(w)] (abbrev.']
E,3. Problem Definition,[0],[0]
[∇J ]).,3. Problem Definition,[0],[0]
"In this paper, we study critical points E",3. Problem Definition,[0],[0]
[∇J ] = 0 and vanilla gradient dynamics w t+1 = wt − ηE,3. Problem Definition,[0],[0]
"[∇J(wt)], where η is the learning rate.",3. Problem Definition,[0],[0]
Properties of ReLU.,4. The Analytical Formula,[0],[0]
ReLU nonlinearity has useful properties.,4. The Analytical Formula,[0],[0]
We define the gating function D(w) ≡,4. The Analytical Formula,[0],[0]
diag(Xw > 0) as an N -by-N binary diagonal matrix.,4. The Analytical Formula,[0],[0]
"Its l-th diagonal element is a binary variable showing whether the neuron is activated for sample l. Using this notation, σ(Xw) = D(w)Xw which means D(w) selects the output of a linear neuron, based on their activations.",4. The Analytical Formula,[0],[0]
"Note that D(w) only depends on the direction of w but not its magnitude.
",4. The Analytical Formula,[0],[0]
D(w) is also “transparent” with respect to derivatives.,4. The Analytical Formula,[0],[0]
"For example, at differentiable regions, Jacobianw[σ(Xw)] = σ′(Xw)X = D(w)X .",4. The Analytical Formula,[0],[0]
"This gives a very concise rule for gradient descent update in ReLU networks.
",4. The Analytical Formula,[0],[0]
One ReLU node.,4. The Analytical Formula,[0],[0]
"Given the properties of ReLU, the population gradient E",4. The Analytical Formula,[0],[0]
"[∇J ] can be written as:
E",4. The Analytical Formula,[0],[0]
[∇J ] = EX [X⊺D(w) (D(w)Xw −D(w∗)Xw∗)],4. The Analytical Formula,[0],[0]
"(3) Intuitively, this term vanishes when w → w∗, and should
be around N2 (w − w∗) if the data are evenly distributed, since roughly half of the samples are blocked.",4. The Analytical Formula,[0],[0]
"However, such an estimation fails to capture the nonlinear behavior.
",4. The Analytical Formula,[0],[0]
"If we define Population Gating (PG) function F (e,w) ≡",4. The Analytical Formula,[0],[0]
"X⊺D(e)D(w)Xw, then",4. The Analytical Formula,[0],[0]
"E [∇J ] can be written as:
E",4. The Analytical Formula,[0],[0]
"[∇J ] = E [F (w/‖w‖,w)]− E [F (w/‖w‖,w∗)] .",4. The Analytical Formula,[0],[0]
"(4)
Interestingly, F (e,w) has an analytic formula if the data X follow spherical Gaussian distribution:
Theorem 1",4. The Analytical Formula,[0],[0]
"Denote F (e,w) =",4. The Analytical Formula,[0],[0]
"X⊺D(e)D(w)Xw where e is a unit vector, X =",4. The Analytical Formula,[0],[0]
"[x1,x2, · · · ,xN ]⊺ is the N -by-d data matrix and D(w) = diag(Xw > 0) is a binary diagonal matrix.",4. The Analytical Formula,[0],[0]
"If xi ∼ N (0, I) (and thus bias-free), then:
E [F (e,w)]",4. The Analytical Formula,[0],[0]
"= N
2π",4. The Analytical Formula,[0],[0]
"[(π − θ)w + ‖w‖ sin θe] (5)
where θ = ∠(e,w) ∈",4. The Analytical Formula,[0],[0]
"[0, π] is the angle between e and w.
See the link2 for the proof of all theorems.",4. The Analytical Formula,[0],[0]
Note that we do not require X to be independent between samples.,4. The Analytical Formula,[0],[0]
"Intuitively, the first mass term N2π (π−θ)w aligns with w and is proportional to the amount of activated data whose ReLU are on.",4. The Analytical Formula,[0],[0]
"When θ = 0, the gating function is fully on and half of the data contribute to the term; when θ = π, the gating function is completely switched off.",4. The Analytical Formula,[0],[0]
The gate is controlled by the angle between w and the control signal e.,4. The Analytical Formula,[0],[0]
"The second asymmetric term is aligned with e, and is proportional to the asymmetry of the activated data samples (Fig. 2).
",4. The Analytical Formula,[0],[0]
"Note that the expectation analysis smooths out ReLU and leaves only one singularity at the origin, where E",4. The Analytical Formula,[0],[0]
[∇J ] is not continuous.,4. The Analytical Formula,[0],[0]
"That is, if approaching from different directions towards w = 0, E",4. The Analytical Formula,[0],[0]
[∇J ] is different.,4. The Analytical Formula,[0],[0]
"With the close form of F , E",4. The Analytical Formula,[0],[0]
"[∇J ] also has a close form:
E",4. The Analytical Formula,[0],[0]
"[∇J ] = N 2 (w−w∗)+N 2π
( θw∗ − ‖w ∗‖",4. The Analytical Formula,[0],[0]
"‖w‖ sin θw ) (6)
where θ = ∠(w,w∗) ∈",4. The Analytical Formula,[0],[0]
"[0, π].",4. The Analytical Formula,[0],[0]
"The first term is from linear approximation, while the second term shows the nonlinear behavior.
",4. The Analytical Formula,[0],[0]
"For linear case, D ≡ I (no gating) and thus ∇J ∝",4. The Analytical Formula,[0],[0]
X⊺X(w − w∗).,4. The Analytical Formula,[0],[0]
"For spherical Gaussian input X , EX [X
⊺X] = I and E",4. The Analytical Formula,[0],[0]
[∇J ] ∝ w−w∗.,4. The Analytical Formula,[0],[0]
"Therefore, the dynamics has only one critical point and global convergence follows, which is consistent with its convex nature.
",4. The Analytical Formula,[0],[0]
Extension to other distributions.,4. The Analytical Formula,[0],[0]
"From its definition, E [F (e,w)]",4. The Analytical Formula,[0],[0]
"= E [X⊺D(e)D(w)Xw] is linear to ‖w‖, regardless of the distribution of X .",4. The Analytical Formula,[0],[0]
"On the other hand, isotropy in spherical Gaussian distribution leads to the fact
2http://yuandong-tian.com/ssb-supp.pdf
that E",4. The Analytical Formula,[0],[0]
"[F (e,w)] only depends on angles between vectors.",4. The Analytical Formula,[0],[0]
"For other isotropic distributions, we could similarly derive:
E [F (e,w)] = A(θ)w + ‖w‖B(θ)e (7)
where A(0) = N/2 (gating fully on), A(π) = 0",4. The Analytical Formula,[0],[0]
"(gating fully off), and B(0) = B(π) = 0",4. The Analytical Formula,[0],[0]
(no asymmetry when w and e are aligned).,4. The Analytical Formula,[0],[0]
"Although we focus on spherical Gaussian case, many following analysis, in particular critical point analysis, can also be applied to Eqn. 7.
",4. The Analytical Formula,[0],[0]
Multiple ReLU node.,4. The Analytical Formula,[0],[0]
"For Eqn. 1 that contains K ReLU node, we could similarly write down the population gradient with respect to wj (note that ej = wj/‖wj‖):
E",4. The Analytical Formula,[0],[0]
"[ ∇wjJ ] =
K ∑
j′=1
E [F (ej ,wj′)]− K ∑
j′=1
E [ F (ej ,w ∗ j′) ]
(8)",4. The Analytical Formula,[0],[0]
"By solving Eqn. 8 (the normal equation, E [ ∇wjJ ]
= 0), we could identify all critical points of g(x).",5. Critical Point Analysis,[0],[0]
"However, it is highly nonlinear and cannot be solved easily.",5. Critical Point Analysis,[0],[0]
"In this paper, we provide conditions for critical points using the structure of Eqn. 8.",5. Critical Point Analysis,[0],[0]
"The case study for K = 2 gives examples for saddle points and regions without critical points.
",5. Critical Point Analysis,[0],[0]
"For convenience, we define Π∗ as the Principal Hyperplane spanned by K ground truth weight vectors.",5. Critical Point Analysis,[0],[0]
Note that Π∗ is at most K dimensional.,5. Critical Point Analysis,[0],[0]
"{wj}Kj=1 is said to be in-plane, if all wj ∈ Π∗.",5. Critical Point Analysis,[0],[0]
Otherwise it is out-of-plane.,5. Critical Point Analysis,[0],[0]
"The normal equation {E [ ∇wjJ ] = 0}Kj=1 contain Kd scalar equations and can be written as the following:
Y E⊺ = B∗W ∗⊺ (9)
where Y = diag(sinΘ⊺w̄ − sinΘ∗⊺w̄∗) + (π11⊺ − Θ⊺)diagw̄ and B∗ = π11⊺ − (Θ∗)⊺. Here θ∗j ′
j ≡ ∠(wj ,w ∗ j′), θ j′
j ≡ ∠(wj ,wj′), Θ =",5.1. Normal Equation,[0],[0]
"[θij ] (i-th row, j-th column of Θ is θij) and Θ ∗ =",5.1. Normal Equation,[0],[0]
"[θ∗ij ].
Note that Y and B∗ are both K-by-K matrices that only depend on angles and magnitudes, and hence rotational invariant.",5.1. Normal Equation,[0],[0]
"This leads to the following theorem characterizing the structure of out-of-plane critical points:
Theorem 2 If d ≥ K+2, then out-of-plane critical points (solutions of Eqn. 9) are non-isolated and lie in a manifold.
",5.1. Normal Equation,[0],[0]
The intuition is to construct a rotational matrix that is not identity matrix but keeps Π∗ invariant.,5.1. Normal Equation,[0],[0]
Such matrices form a Lie group L that transforms critical points to critical points.,5.1. Normal Equation,[0],[0]
"Then for any out-of-plane critical point, there is one matrix in L that changes at least one of its weights, yielding a non-isolated different critical point.
",5.1. Normal Equation,[0.9999999726333784],"['Then for any out-of-plane critical point, there is one matrix in L that changes at least one of its weights, yielding a non-isolated different critical point.']"
"Note that Thm. 2 also works for any general isotropic distribution, in which E [F (e,w)] has the form of Eqn. 7.",5.1. Normal Equation,[0],[0]
"This is due to the symmetry of the input X , which in turn affects the geometry of critical points.",5.1. Normal Equation,[0],[0]
"The theorem also explains why we have flat minima (Hochreiter et al., 1995; Dauphin et al., 2014) often occuring in practice.",5.1. Normal Equation,[0],[0]
"To analyze in-plane critical points, it suffices to study gradient projections on Π∗.",5.2. In-Plane Normal Equation,[1.0],"['To analyze in-plane critical points, it suffices to study gradient projections on Π∗.']"
"When {wj} is full-rank, the projections could be achieved by right-multiplying both sides by {ej′}, which gives K2 equations:
M(Θ)w̄",5.2. In-Plane Normal Equation,[0],[0]
"= M∗(Θ,Θ∗)w̄∗ (10)
",5.2. In-Plane Normal Equation,[0],[0]
"This again shows decomposition of angles and magnitudes, and linearity with respect to the norms of weight vectors.",5.2. In-Plane Normal Equation,[0],[0]
Here w̄ =,5.2. In-Plane Normal Equation,[0],[0]
"[‖w1‖, ‖w2‖, . . .",5.2. In-Plane Normal Equation,[0],[0]
", ‖wK‖]⊺ and similarly for w̄
∗. M and M∗ are K2-by-K matrices that only depend on angles.",5.2. In-Plane Normal Equation,[0],[0]
"Entries of M and M∗ are:
mjj′,k =",5.2. In-Plane Normal Equation,[0],[0]
"(π − θkj ) cos θkj′ + sin θkj cos θjj′ (11) m∗jj′,k =",5.2. In-Plane Normal Equation,[0],[0]
(π − θ∗kj ),5.2. In-Plane Normal Equation,[0],[0]
cos,5.2. In-Plane Normal Equation,[0],[0]
"θ∗kj′ + sin θ∗kj cos θjj′ (12)
",5.2. In-Plane Normal Equation,[0],[0]
"Here index j is the j-th column of Eqn. 9, j′ is from projection vector ej′ and k is the k-th weight magnitude.
",5.2. In-Plane Normal Equation,[0],[0]
Diagnoal constraints.,5.2. In-Plane Normal Equation,[0],[0]
"For “diagonal” constraints (j, j) of Eqn. 10, we have cos θjj = 1 and mjj,k = h(θ",5.2. In-Plane Normal Equation,[0],[0]
"k j ),",5.2. In-Plane Normal Equation,[0],[0]
"m ∗ jj,k = h(θ∗kj ), where h(θ) =",5.2. In-Plane Normal Equation,[0],[0]
(π− θ) cos θ+ sin θ.,5.2. In-Plane Normal Equation,[0],[0]
"Therefore, we arrive at the following subset of the constraints:
Mrw̄ = M ∗ r w̄ ∗",5.2. In-Plane Normal Equation,[0],[0]
"(13)
where Mr = h(Θ ⊺) and M∗r = h(Θ ∗⊺) are both K-byK matrices.",5.2. In-Plane Normal Equation,[0],[0]
"Note that if Mr is full-rank, then we could solve w̄ from Eqn. 13 and plug it back in Eqn. 10 to check whether it is indeed a critical point.",5.2. In-Plane Normal Equation,[0],[0]
"This gives necessary conditions for critical points that only depend on angles.
",5.2. In-Plane Normal Equation,[0],[0]
Separable Property.,5.2. In-Plane Normal Equation,[0],[0]
"Interestingly, the plugging back operation leads to conditions that are separable with respect to ground truth weight (Fig. 3).",5.2. In-Plane Normal Equation,[0],[0]
"To see this, we first define the following quantity Ljj′ which is a function between a single (rather than K) ground truth unit weight vector e∗ and all current unit weights {el}Kl=1:
Ljj′({θ∗l },Θ) = m∗jj′",5.2. In-Plane Normal Equation,[0],[0]
"− v⊺M−1r mjj′ (14)
where θ∗l = ∠(e ∗, el) is the angle between e ∗ and el, v = v({θ∗l }) =",5.2. In-Plane Normal Equation,[0],[0]
"[h(θ∗1), . . .",5.2. In-Plane Normal Equation,[0],[0]
", h(θ∗K)]⊺, and m∗jj′ = (π − θ∗j ) cos θ∗j′ + sin θ∗j cos θjj′ (like Eqn. 12).",5.2. In-Plane Normal Equation,[0],[0]
Note that v({θ∗jl }) is the j-th column of M∗r .,5.2. In-Plane Normal Equation,[0],[0]
Fig. 3 illustrates the case when K = 2.,5.2. In-Plane Normal Equation,[0],[0]
"Ljj′ has the following properties:
Proposition 1 Ljj′({θ∗l },Θ) = 0",5.2. In-Plane Normal Equation,[0],[0]
when there exists l so that e∗ = el.,5.2. In-Plane Normal Equation,[0],[0]
"In addition, Ljj({θ∗l },Θ) = 0 always.
Intuitively, Ljj′ characterizes the relative geometric relationship among e∗ and {el}.",5.2. In-Plane Normal Equation,[0],[0]
"It is like determinant of a matrix whose columns are {el} and e∗. With Ljj′ , we have the following necessary conditions for critical points:
Theorem 3",5.2. In-Plane Normal Equation,[0],[0]
"If w̄∗ 6= 0, and for a given parameter w, Ljj′({θ∗kl },Θ) > 0 (or < 0) for all 1 ≤ k ≤ K, then w cannot be a critical point.",5.2. In-Plane Normal Equation,[0],[0]
"In this case, Mr and M ∗ r are 2-by-2 matrices.",5.3. Case study: K = 2 network,[0],[0]
"Here we discuss the case that both w1 and w2 are in Π∗.
Saddle points.",5.3. Case study: K = 2 network,[0],[0]
When θ12 = 0,5.3. Case study: K = 2 network,[0],[0]
"(w1 and w2 are collinear), Mr = π11 ⊺ is singular since e1 and e2 are identical.",5.3. Case study: K = 2 network,[0],[0]
"From Eqn. 9, if θ∗11 = θ ∗2 1 , i.e., they are both aligned with the bisector angle of w∗1 and w ∗ 2 , and πw̄ ⊺ 1 = h (
θ∗1∗2/2 ) (w̄∗)⊺1, then the current solution is a saddle point.",5.3. Case study: K = 2 network,[0],[0]
"Note that this gives one constraint for two weight magnitudes, and thus there exist infinite solutions.
",5.3. Case study: K = 2 network,[0],[0]
Region without critical points.,5.3. Case study: K = 2 network,[0],[0]
We rely on the following conjecture that is verified empirically in an exhaustive manner (Sec. 7.2).,5.3. Case study: K = 2 network,[0],[0]
"It characterizes zero-crossings of a 2D function on a closed region [0, 2π]× [0, π].",5.3. Case study: K = 2 network,[1.0],"['It characterizes zero-crossings of a 2D function on a closed region [0, 2π]× [0, π].']"
"In comparison, in-plane 2 ReLU network has 6 parameters and is more difficult to handle: 8 for w1, w2, w ∗ 1 and w ∗ 2 , minus the rotational and scaling symmetries.
∗ 1
Conjecture 1",5.3. Case study: K = 2 network,[0],[0]
"If e∗ is in the interior of Cone(e1, e2), then L12(θ ∗ 1 , θ ∗ 2 , θ 1 2) > 0.",5.3. Case study: K = 2 network,[0],[0]
"If e ∗ is in the exterior, then L12 < 0.
",5.3. Case study: K = 2 network,[0],[0]
This is also empirically true for L21.,5.3. Case study: K = 2 network,[0],[0]
"Combined with Thm. 3, we know that (Fig. 4):
Theorem 4 If Conjecture 1 is correct, then for 2 ReLU network, (w1,w2) (w1 6= w2) is not a critical point, if they both are in Cone(w∗1,w ∗ 2), or both out of it.
",5.3. Case study: K = 2 network,[0],[0]
"When exact one w∗ is inside Cone(w1,w2), whether (w1,w2) is a critical point remains open.",5.3. Case study: K = 2 network,[1.0],"['When exact one w∗ is inside Cone(w1,w2), whether (w1,w2) is a critical point remains open.']"
Application of Eqn. 5 also yields interesting convergence analysis.,6. Convergence Analysis,[0],[0]
"We focus on infinitesimal analysis, i.e., when learning rate η → 0 and the gradient update becomes a first-order differential equation:
dw/dt = −EX [∇wJ(w)] (15)
Then the populated objective EX [J ] does not increase:
dE",6. Convergence Analysis,[0],[0]
[J ] /dt =,6. Convergence Analysis,[0],[0]
−E [∇J ]⊺ dw/dt = −E,6. Convergence Analysis,[0],[0]
[∇J ]⊺ E,6. Convergence Analysis,[0],[0]
[∇J ] ≤ 0,6. Convergence Analysis,[0],[0]
"(16)
The goal of convergence analysis is to determine specific weight initializations w0 that leads to convergence to w∗ following the gradient descent dynamics (Eqn. 15).",6. Convergence Analysis,[0],[0]
"Using Lyapunov method (LaSalle & Lefschetz, 1961), we show that the gradient dynamics (Eqn. 15) converges to w∗ when w0 ∈",6.1. Single ReLU case,[0],[0]
Ω,6.1. Single ReLU case,[0],[0]
"= {w : ‖w −w∗‖ < ‖w∗‖}:
Theorem 5",6.1. Single ReLU case,[0],[0]
When w0 ∈,6.1. Single ReLU case,[0],[0]
Ω,6.1. Single ReLU case,[0],[0]
"= {w : ‖w −w∗‖ < ‖w∗‖}, following the dynamics of Eqn. 15, the Lyapunov function V (w) = 12‖w",6.1. Single ReLU case,[0],[0]
− w∗‖2 has dV/dt < 0,6.1. Single ReLU case,[0],[0]
"and the system is asymptotically stable and thus wt → w∗ when t → +∞.
The intuition is to represent dV/dt as a 2-by-2 bilinear form of vector [‖w‖, ‖w∗‖], and the bilinear coefficient matrix, as a function of angles, is negative definite (except for w = w∗).",6.1. Single ReLU case,[0],[0]
"Note that similar approaches do not apply to regions including the origin because at the origin, the population gradient is discontinuous.",6.1. Single ReLU case,[0],[0]
"Ω does not include the
origin and for any initialization w0 ∈ Ω, we could always find a slightly smaller subset Ω′δ =",6.1. Single ReLU case,[0],[0]
{w : ‖w − w∗‖ ≤ ‖w∗‖−δ} with δ > 0,6.1. Single ReLU case,[0],[0]
"that covers w0, and apply Lyapunov method within.",6.1. Single ReLU case,[0],[0]
"Note that the global convergence claim in (Mei et al., 2016) for l2 loss does not apply to ReLU, since it requires σ′(x) >",6.1. Single ReLU case,[0],[0]
"0.
",6.1. Single ReLU case,[0],[0]
Random Initialization.,6.1. Single ReLU case,[0],[0]
How to sample w0 ∈ Ω without knowing w∗?,6.1. Single ReLU case,[0],[0]
Uniform sampling around origin with radius r ≥ γ‖w∗‖ for any γ > 1 results in exponentially small success rate (r/‖w∗‖)d ≤ γ−d in high-dimensional space.,6.1. Single ReLU case,[0],[0]
"A better idea is to sample around the origin with very small radius (but not at w = 0), so that Ω looks like a hyperplane near the origin, and thus almost half samples are useful (Fig. 5(a)), as shown in the following theorem:
Theorem 6 The dynamics in Eqn. 6 converges to w∗ with probability at least (1 − ǫ)/2, if the initial value w0 is sampled uniformly from Br = {w : ‖w‖ ≤ r} with r ≤",6.1. Single ReLU case,[0],[0]
"ǫ √
2π d+1‖w∗‖.
",6.1. Single ReLU case,[0],[0]
The idea is to lower-bound the probability of the shaded area (Fig. 5(b)).,6.1. Single ReLU case,[0],[0]
"Thm. 6 gives an explanation for common initialization techniques (Glorot & Bengio, 2010; He et al., 2015; LeCun et al., 2012; Bottou, 1988) that uses random
variables with O(1/ √ d) standard deviation.",6.1. Single ReLU case,[0],[0]
"For multiple ReLUs, Lyapunov method on Eqn. 8 yields no decisive conclusion.",6.2. Multiple ReLU case,[0],[0]
"Here we focus on the symmetric property of Eqn. 8 and discuss a special case, that the teacher parameters {w∗j}Kj=1 and the initial weights {w0j}Kj=1 respect the following symmetry: wj = Pjw and w
∗",6.2. Multiple ReLU case,[0],[0]
"j = Pjw ∗, where Pj is an orthogonal matrix whose collection P ≡ {Pj}Kj=1 forms a group.",6.2. Multiple ReLU case,[0],[0]
"Without loss of generality, we set P1 as the identity.",6.2. Multiple ReLU case,[1.0],"['Without loss of generality, we set P1 as the identity.']"
"Then from Eqn. 8 the population gradient becomes:
E [ ∇wjJ ] =",6.2. Multiple ReLU case,[0],[0]
"PjE [∇w1J ] (17) This means that if all wj and w ∗ j are symmetric under group actions, so does their population gradients.",6.2. Multiple ReLU case,[0],[0]
"There-
fore, the trajectory {wt} also respects the symmetry (i.e., Pjw t 1 = w t j) and we only need to solve one equation for E",6.2. Multiple ReLU case,[0],[0]
"[∇wJ ] instead of K (here e = w/‖w‖):
E",6.2. Multiple ReLU case,[0],[0]
"[∇wJ ] = K ∑
j′=1
E [F (e, Pj′w)]− E [F (e, Pj′w∗)]",6.2. Multiple ReLU case,[0],[0]
"(18)
Eqn. 18 has interesting properties, known as Spontaneous Symmetric-Breaking (SSB) in physics (Brading & Castellani, 2003), in which the equations of motion respect a certain symmetry but its solution breaks it (Fig. 6).",6.2. Multiple ReLU case,[0],[0]
"In our language, despite that the population gradient field E",6.2. Multiple ReLU case,[0],[0]
"[∇wJ ] and the objective E [J ] are invariant to the group transformation P , i.e., for w∗ → Pjw∗, E",6.2. Multiple ReLU case,[0],[0]
[J ] and E,6.2. Multiple ReLU case,[0],[0]
"[∇wJ ] remain the same, its solution is not (Pjw 6= w).",6.2. Multiple ReLU case,[0],[0]
"Furthermore, since P is finite, as we will see, the final solution converges to different permutations of w∗ due to infinitesimal perturbations of initialization.
",6.2. Multiple ReLU case,[0],[0]
"To illustrate such behaviors, consider the following example in which {w∗j}Kj=1 forms an orthonormal basis and under this basis, P is a cyclic group in which Pj circularly shifts dimension by j − 1 (e.g., P2[1, 2, 3]⊺ =",6.2. Multiple ReLU case,[0],[0]
"[3, 1, 2]⊺).",6.2. Multiple ReLU case,[0],[0]
"In this case, if we start with w0 = x0w∗ + ∑
j 6=1 Pjw ∗ j =
[x0, y0, . . .",6.2. Multiple ReLU case,[0],[0]
", y0] under the basis of w∗, then Eqn. 18 is further reduced to a convergent 2D nonlinear dynamics and Thm. 7 holds (Please check Supplementary Materials for the associated close-form of the 2D dynamics):
Theorem 7 For a bias-free two-layered ReLU network g(x;w) = ∑
j σ(w ⊺
j x) that takes spherical Gaussian inputs, if the teacher’s parameters {w∗j} form orthnomal bases, then (1) when the student parameters is initialized to be [x0, y0, . . .",6.2. Multiple ReLU case,[0],[0]
", y0] under the basis of w∗, where (x0, y0) ∈ Ω",6.2. Multiple ReLU case,[0],[0]
"= {x ∈ (0, 1], y ∈",6.2. Multiple ReLU case,[0],[0]
"[0, 1], x > y}, then Eqn. 8 converges to teacher’s parameters {w∗j} (or (x, y) = (1, 0)); (2) when x0 = y0 ∈ (0, 1], then it converges to a saddle point x = y = 1 πK",6.2. Multiple ReLU case,[0],[0]
"( √ K − 1− arccos(1/ √ K) + π).
",6.2. Multiple ReLU case,[0],[0]
Thm. 7 suggests that when w0 =,6.2. Multiple ReLU case,[0],[0]
"[y0, x0, . . .",6.2. Multiple ReLU case,[0],[0]
", y0], the system converges to P2w
∗, etc.",6.2. Multiple ReLU case,[0.9999999730577757],"[', y0], the system converges to P2w ∗, etc.']"
"Since |x0 − y0| can be arbitrarily small, a slightest perturbation around x0 = y0 leads
to a different fixed point Pjw ∗ for some j. Unlike single ReLU case, the initialization in Thm. 7 is w∗-dependent, and serves as an example for the branching behavior.
",6.2. Multiple ReLU case,[0],[0]
"Thm. 7 also suggests that for convergence, x0 and y0 can be arbitrarily small, regardless of the magnitude of w∗, showing a global convergence behavior.",6.2. Multiple ReLU case,[0],[0]
"In comparison, (Saad & Solla, 1996) uses Gaussian error function (σ = erf) as the activation, and only analyzes local behaviors near the two fixed points (origin and w∗).
",6.2. Multiple ReLU case,[0],[0]
"In practice, even with noisy initialization, Eqn. 18 and the original dynamics (Eqn. 8) still converge to w∗ (and its transformations).",6.2. Multiple ReLU case,[0],[0]
"We leave it as a conjecture, whose proof may lead to an initialization technique for 2-layered ReLU that is w∗-independent.
Conjecture 2",6.2. Multiple ReLU case,[0],[0]
"If the initialization w0 = x0w∗ + y0 ∑
j 6=1 Pjw ∗ + ǫ, where ǫ is noise and (x0, y0) ∈ Ω,
then Eqn. 8 also converges to w∗ with high probability.",6.2. Multiple ReLU case,[0],[0]
7.1.,7. Simulations,[0],[0]
"The analytical solution to F (e,w)
We verify E [F (e,w)]",7. Simulations,[0],[0]
= E [X⊺D(e)D(w)Xw] (Eqn. 5) with simulation.,7. Simulations,[0],[0]
"We randomly pick e and w so that their angle ∠(e,w) is uniformly distributed in [0, π].",7. Simulations,[1.0],"['We randomly pick e and w so that their angle ∠(e,w) is uniformly distributed in [0, π].']"
"The analytical formula E [F (e,w)] is compared with F (e,w), which is computed via sampling on the input X that follows spherical Gaussian distribution.",7. Simulations,[0],[0]
"We use relative RMS error: err = ‖E [F (e,w)]",7. Simulations,[0],[0]
"− F (e,w)‖/‖F (e,w)‖. Fig. 7(a) shows the error distribution with respect to angles.",7. Simulations,[0],[0]
"For small θ, the gating function D(w) and D(e) mostly overlap and give a reliable estimation.",7. Simulations,[0],[0]
"When θ → π, D(w) and D(e)overlap less and the variance grows.",7. Simulations,[0],[0]
Note that our convergence analysis operate on θ ∈,7. Simulations,[0],[0]
"[0, π/2] and is not affected.",7. Simulations,[0],[0]
"In the following, we sample angles from [0, π/2].
",7. Simulations,[0],[0]
Fig. 7(a) shows that the formula is more accurate with more samples.,7. Simulations,[0],[0]
"We also examine other zero-mean distributions of X , e.g., U [−1/2, 1/2].",7. Simulations,[1.0],"['We also examine other zero-mean distributions of X , e.g., U [−1/2, 1/2].']"
"As shown in Fig. 7(d), the formula still works for large d. Note that the error is computed up to a global scale, due to different normalization constants in probability distributions.",7. Simulations,[0],[0]
Whether Eqn. 5 applies for more general distributions remains open.,7. Simulations,[0],[0]
Conjecture 1 can be reduced to enumerate a complicated but 2D function via exhaustive sampling.,7.2. Empirical Results in critical point analysis K = 2,[0],[0]
"In comparison, a full optimization of 2-ReLU network constrained on principal hyperplane Π∗ involves 6 parameters (8 parameters minus 2 degrees of symmetry) and is more difficult to handle.",7.2. Empirical Results in critical point analysis K = 2,[0],[0]
Fig. 10 shows that empirically L12 has no extra zerocrossing other than e∗ = e1 or e2.,7.2. Empirical Results in critical point analysis K = 2,[0],[0]
"As shown in Fig. 10(c), we have densely enumerated θ12 ∈",7.2. Empirical Results in critical point analysis K = 2,[0],[0]
"[0, π] and e∗ on a
104 × 104 grid without finding any counterexamples.",7.2. Empirical Results in critical point analysis K = 2,[0],[0]
Fig. 8(a) and (b) shows the 2D vector field in Thm 7.,7.3. Convergence analysis for multiple ReLU nodes,[0],[0]
"Fig. 8(c) shows the 2D trajectory towards convergence to the teacher’s parameters w∗. Interestingly, even when we initialize the weights as [10−3, 0]⊺, whose direction is aligned with w∗ at [1, 0]⊺, the gradient descent still takes detours to reach the destination.",7.3. Convergence analysis for multiple ReLU nodes,[0],[0]
"This is because at the beginning of optimization, all ReLU nodes explain the training error in the same way (both x and y increases); when the “obvious” component is explained, the error pushes some nodes to explain other components.",7.3. Convergence analysis for multiple ReLU nodes,[0],[0]
"Hence, specialization follows (x increases but y decreases).
",7.3. Convergence analysis for multiple ReLU nodes,[0],[0]
"Fig. 9 shows empirical convergence for K ≥ 2, when the initialization deviates from initialization",7.3. Convergence analysis for multiple ReLU nodes,[0],[0]
"[x, y, . . .",7.3. Convergence analysis for multiple ReLU nodes,[0],[0]
", y] in Thm. 7.",7.3. Convergence analysis for multiple ReLU nodes,[0],[0]
"Unless the deviation is large, w converges to w ∗. For more general network g2(x) = ∑K j=1 ajσ(w ⊺
j x), when aj > 0 convergence follows.",7.3. Convergence analysis for multiple ReLU nodes,[0],[0]
"When some aj is negative, the network fails to converge to w∗, even when the student is initialized with the true values {a∗j}Kj=1.",7.3. Convergence analysis for multiple ReLU nodes,[0],[0]
A natural question is whether the proposed method can be extended to multilayer ReLU network.,8. Extension to multilayer ReLU network,[0],[0]
"In this case, there is similar subtraction structure for gradient as Eqn. 3:
Proposition 2",8. Extension to multilayer ReLU network,[0],[0]
Denote,8. Extension to multilayer ReLU network,[0],[0]
"[c] as all nodes in layer c. Denote u ∗ j and uj as the output of node j at layer c of the teacher and student network, then the gradient of the parameters wj immediate under node j ∈",8. Extension to multilayer ReLU network,[0],[0]
"[c] is:
∇wjJ = X⊺c",8. Extension to multilayer ReLU network,[0],[0]
"DjQj ∑
j′∈[c]
(Qj′uj′ −Q∗j′u∗j′) (19)
where Xc is the data fed into node j, Qj and Q ∗",8. Extension to multilayer ReLU network,[0],[0]
j are N - by-N diagonal matrices.,8. Extension to multilayer ReLU network,[0],[0]
For any node k ∈,8. Extension to multilayer ReLU network,[0],[0]
"[c + 1], Qk = ∑
j∈[c] wjkDjQj and similarly for Q ∗",8. Extension to multilayer ReLU network,[0],[0]
"k.
The 2-layered network in this paper is a special case with Qj = Q ∗",8. Extension to multilayer ReLU network,[0],[0]
j = I .,8. Extension to multilayer ReLU network,[0],[0]
"Despite the difficulty that Qj is now depends on the weights of upper layers, and the input Xc is not necessarily Gaussian distributed, Proposition 2 gives a mathematical framework to explore the structure of gradient.",8. Extension to multilayer ReLU network,[0],[0]
"For example, a similar definition of Population Gradi-
ent function is possible.",8. Extension to multilayer ReLU network,[0],[0]
"In this paper, we study the gradient descent dynamics of a 2-layered bias-free ReLU network.",9. Conclusion and Future Work,[0],[0]
The network is trained using gradient descent to reproduce the output of a teacher network with fixed parameters w∗ in the sense of l2 norm.,9. Conclusion and Future Work,[0],[0]
We propose a novel analytic formula for population gradient when the input follows zero-mean spherical Gaussian distribution.,9. Conclusion and Future Work,[0],[0]
This formula leads to interesting critical point and convergence analysis.,9. Conclusion and Future Work,[1.0],['This formula leads to interesting critical point and convergence analysis.']
"Specifically, we show that critical points out of the hyperplane spanned by w∗ are not isolated and form manifolds.",9. Conclusion and Future Work,[0],[0]
"For two ReLU case, we characterize regions that contain no critical points.",9. Conclusion and Future Work,[0],[0]
"For convergence analysis, we show guaranteed convergence for a single ReLU case with random initialization whose stan-
dard deviation is on the order of O(1/ √ d).",9. Conclusion and Future Work,[0],[0]
"For multiple ReLU case, we show that an infinitesimal change of weight initialization leads to convergence to different optima.
",9. Conclusion and Future Work,[0],[0]
Our work opens many future directions.,9. Conclusion and Future Work,[0],[0]
"First, Thm. 2 characterizes the non-isolating nature of critical points in the case of isotropic input distribution, which explains why often practical solutions of NN are degenerated.",9. Conclusion and Future Work,[0],[0]
What if the input distribution has different symmetries?,9. Conclusion and Future Work,[1.0],['What if the input distribution has different symmetries?']
Will such symmetries determine the geometry of critical points?,9. Conclusion and Future Work,[0],[0]
"Second, empirically we see convergence cases that are not covered by the theorems, suggesting the conditions imposed by the theorems can be weaker.",9. Conclusion and Future Work,[1.0],"['Second, empirically we see convergence cases that are not covered by the theorems, suggesting the conditions imposed by the theorems can be weaker.']"
"Finally, how to apply similar analysis to broader distributions and how to generalize the analysis to multiple layers are also open problems.
",9. Conclusion and Future Work,[0.9999999915897397],"['Finally, how to apply similar analysis to broader distributions and how to generalize the analysis to multiple layers are also open problems.']"
"Acknowledgement We thank Léon Bottou, Ruoyu Sun, Jason Lee, Yann Dauphin and Nicolas Usunier for discussions and insightful suggestions.",9. Conclusion and Future Work,[0],[0]
"In this paper, we explore theoretical properties of training a two-layered ReLU network g(x;w) = ∑K j=1 σ(w ⊺ j x) with centered d-dimensional spherical Gaussian input x (σ=ReLU).",abstractText,[0],[0]
"We train our network with gradient descent on w to mimic the output of a teacher network with the same architecture and fixed parameters w. We show that its population gradient has an analytical formula, leading to interesting theoretical analysis of critical points and convergence behaviors.",abstractText,[0],[0]
"First, we prove that critical points outside the hyperplane spanned by the teacher parameters (“out-of-plane“) are not isolated and form manifolds, and characterize inplane critical-point-free regions for two ReLU case.",abstractText,[0],[0]
"On the other hand, convergence to w for one ReLU node is guaranteed with at least (1 − ǫ)/2 probability, if weights are initialized randomly with standard deviation upper-bounded by O(ǫ/ √ d), consistent with empirical practice.",abstractText,[0],[0]
"For network with many ReLU nodes, we prove that an infinitesimal perturbation of weight initialization results in convergence towards w (or its permutation), a phenomenon known as spontaneous symmetric-breaking (SSB) in physics.",abstractText,[0],[0]
We assume no independence of ReLU activations.,abstractText,[0],[0]
Simulation verifies our findings.,abstractText,[0],[0]
An Analytical Formula of Population Gradient for two-layered ReLU network and its Applications in Convergence and Critical Point Analysis,title,[0],[0]
