{"sections": [{"heading": "1. Introduction", "text": "In high dimensional learning problems, a sparse solution is often desired as it has better generalization and interpretation. In order to promote sparse solutions, a regularization term that penalizes for the 1-norm of the vector of parameters is often augmented to an empirical loss term. In regression problems, the empirical loss amounts to sum of the squared differences between the linear predictions and true targets. The task of linear regression with 1-norm penalty is known as Lasso (Tibshirani, 1996). To obtain meaningful solutions for the Lasso, it is required to pick a good value of the `1 regularizer. To automatically choose the best regularization value, algorithms that calculate all possible solutions were developed (Efron et al., 2004; Osborne et al., 2000; Tibshirani & Taylor, 2012).\nThese algorithms find the solution set for all possible regularization values, commonly referred to as the entire reg-\n1Department of Computer Science, Princeton University 2Googol Brain. Correspondence to: Yuanzhi Li <yuanzhil@cs.princeton.edu>.\nProceedings of the 35th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nularization path. The algorithms typically built upon the property that the Lasso regularization path is piecewise linear in the constituents of solution vector. As a result, their running times are also governed by the total number of linear segments. While experiments with real datasets suggest that the number of linear segments is in practice linear in the dimension of the problem (Rosset & Zhu, 2007), worst case settings (Mairal & Yu, 2012) can yield to exponentially many linear segments. The construction of exponentially complex regression problems of (Mairal & Yu, 2012) stands in stark contrast to the aforementioned methods. Provably polynomial path complexity has so far derived in vastly more restricted settings, such as the one described in (Dubiner & Singer, 2011).\nWe bridge the gap between the de facto complexity of the regularization path in real problems and the worst case analysis of the number of linear segments. We show that under fairly general models, the complexity of the entire regularization path is guaranteed to be polynomial in the dimension of the problem. As an important observation, settings which attain the worst case complexity of the regularization path often exhibit fragile algebraic structure. In contrast, natural datasets often comes with noise, which renders those highly frail structures improbable. This approach is called the smoothed analysis, introduced by the seminal paper of (Spielman & Teng, 2009).\nThe core of smoothed analysis is the assumption that the input data is subjected to a small intrinsic noise. Such noise may come from uncertainty in the physical measurements when collecting the data, irrational decisions in human feedback, or simply the rounding errors in the computation process used for obtaining the data. In this model, we let X \u2208 Rn\u00d7d denote the data matrix where n is the number of observations is d is the dimension (number of free parameters). Smoothed analysis assumptions implies that X is the sum of Xh, an unknown fixed matrix, and G, which consists of i.i.d. random samples from a normal distribution with a zero mean and low variance, X = Xh + G. In this view, the data is neither completely random nor completely arbitrary. The smoothed complexity of the problem is measured as the expected complexity taken over the random choices of G. Using this framework, it was proved that the smoothed running time of the simplex, k-means, and the Perceptron algorithm (Spielman & Teng, 2001; Arthur et al.,\n2009; Blum & Dunagan, 2002) is in fact polynomial while the worst case complexity of these problems is exponential.\nWe use the above smoothed analysis model to show that on \u201ctypical\u201d instances, the total number of linear segments of the Lasso\u2019s exact regularization path is polynomial in the problem size with high probability. Informally speaking and omitting technical details, our main result can be stated as follows.\nLet X \u2208 Rn\u00d7d be a data matrix of the form X = Xh +G for any fixed matrix Xh and a random Gaussian matrix G, Gi j \u223c N(0, \u03c32). Then, for an arbitrary vector of targets y \u2208 Rn, with high probability, the total number of linear segments of the Lasso\u2019s exact regularization path for (X, y) is polynomial in n, d, and 1\u03c3 .\nOur result is conceptually different than the one presented in (Mairal & Yu, 2012). Mairal and Yu showed that there exists an approximate regularization path with a small number of linear segments. However, the analysis, while being novel and inspiring, does not shed light on why, in practice, the exact number of linear segments is small as the approximated path is unlikely to coincide with the exact path. Our analysis covers uncharted terrain and different aspects than the approximated path algorithms in (Mairal & Yu, 2012; Giesen et al., 2010). On one hand, we show that when the input data is \u201ctypical\u201d, namely comes from a \u201cnaturally smooth\u201d distribution, then with high probability, the total number of linear segments, of the exact Lasso path, is already polynomially small. This part of our analysis provides theoretical backing to the empirical findings reported in (Rosset & Zhu, 2007). On the other hand, when the input matrix is atypical and induces a high-complexity path, then we can also obtain a low-complexity approximate regularization path by adding a small amount of random noise to the data and then solve the Lasso\u2019s regularization path on the perturbed instance exactly. We also verify our analysis experimentally in section 9. We show that even a tiny amount of perturbation to high-complexity data matrices, results in a dramatic drop in the number of linear segments.\nThe technique used in this paper is morally different from the smoothed analysis obtained for simplex (Spielman & Teng, 2001), k-means (Arthur et al., 2009), and the Perceptron (Blum & Dunagan, 2002), as there is no concrete algorithm involved. We develop a new framework which shows that when the total number of linear segments is excessively high, then we can tightly couple the solutions of the original, smoothed problems to another set of solutions in a manner that does not depend on G. We then use the randomness of G to show that such couplings are unlikely to exist, thus high complexity solutions are rare. We believe that our framework can be extended to other problems such as the regularization path of support vector machines."}, {"heading": "2. Preliminaries", "text": "We use uppercase boldface letters, e.g, X, to denote matrices and lowercase letters x,w to denote vectors, variables, and scalars. We use Xi to denote the i\u2019th column of X. Given a set S, we denote by XS \u2208 Rd\u00d7|S | the sub-matrix of X whose columns are Xi for i \u2208 S. Analogously, XS\u0304 denotes the sub-matrix of X with columns Xi for i < S. For a matrix X \u2208 Rn\u00d7d with n \u2265 d, we use the term smallest (largest) singular value of X to denote the smallest (largest) right singular value of X. We define the generalized sign of a scalar b as follows,\nsign(b) =  +1 b > 0 \u22121 b < 0 0 b = 0 .\nLet y be a vector in Rn and let X = [X1, \u00b7 \u00b7 \u00b7 ,Xd] be a matrix in Rn\u00d7d . The Lasso is the following regression problem,\nw[\u03bb] = argmin w\u2208Rd 1 2 \u2016Xw \u2212 y\u201622 + \u03bb\u2016w\u20161 . (1)\nHere, \u03bb > 0 is the regularization value. The value of \u03bb influences the sparsity level of the solution w[\u03bb]. The larger \u03bb is the sparser the solution is. When X is of full column rank, the solution to (1) is unique. We therefore denote it by w[\u03bb]. We use P = {w[\u03bb] | \u03bb > 0} to denote the set of all possible solution vectors. This set is also referred to as the entire regularization path.\nTo establish out main result we need a few technical lemmas. The first Lemma from (Mairal & Yu, 2012) provides optimality conditions for w[\u03bb]. Lemma 1. Let \u03bb > 0, the w[\u03bb] is the optimal solution iff it satisfies the following conditions,\n1. There exists a vector u[\u03bb] s.t.\nX>(Xw[\u03bb] \u2212 y) = u[\u03bb] .\n2. Each coordinate of u[\u03bb] satisfies, ui[\u03bb] = { \u2212\u03bb sign (wi[\u03bb]) |wi[\u03bb]| > 0 \u2208 [\u2212\u03bb,\u03bb] o.w. .\nLet us denote the sign vector as sign(w[\u03bb]), which is obtained by applying the generalized sign function sign(\u00b7) element-wise to w[\u03bb]. The result of (Mairal & Yu, 2012) shows that P is piecewise linear and unique in the following sense. Lemma 2. Suppose X is of full column rank, then P = {w[\u03bb] | \u03bb > 0} is unique, well-defined, and w[\u03bb] is piecewise linear. Moreover, for any \u03bb1, \u03bb2 > 0, if the sign vectors at \u03bb1 and \u03bb2 are equal, sign(w[\u03bb1]) = sign(w[\u03bb2]), then w[\u03bb1] and w[\u03bb2] are in the same linear segment.\nWe use |P | to denote the total number of linear segments in P. We denote by \u03b1 > 0 the smallest singular value of X. Without loss of generality, as we can rescale X and y accordingly, we assume that \u2016y\u20162 = 1. To obtain our main result, we introduce the following smoothness assumption on the data matrix X. Assumption 3 (Smoothness). X is generated according to,\nX = Xh +G ,\nwhere Xh \u2208 Rn\u00d7d (n \u2265 d) is a fixed unknown matrix with \u2016Xh\u20162 \u2264 1. Each entry of G is an i.i.d. sample from the normal distribution with 0 mean and variance of \u03c32n .\nWe use N(0, \u03c32/n) instead of N(0, \u03c32) for the noise distribution G. This choice implies that when \u03c3 is a constant the spectral norm of G is also a constant in expectation, E[\u2016G\u20162] = O(1), see for instance (Rudelson & Vershynin, 2010). Therefore, the signal-to-noise ratio satisfies,\nE [ \u2016X\u20162\u2016G\u2016\u221212 ] = O(1) .\nIt is convenient to think of \u03c3 as an arbitrary small constant. The analysis presented in the sequel employs a fixed constant c that does not depend on the problem size. We use f (\u00b7) c> poly(\u00b7) (analogously, f (\u00b7) c< poly(\u00b7)) to denote the fact that the function f is everywhere greater (smaller) than a polynomial function up to a multiplicative constant. Equipped with the above conventions and the smoothness assumption, the following lemma, due to (Sankar et al., 2006), characterizes the extremal singular values of X .\nLemma 4. Let \u03b4 > 0. With probability of at least 1 \u2212 \u03b4, the smallest, denoted \u03b1, and largest, denoted \u03b2, right singular values of X satisfy,\n\u03b1 c > \u03b4\u03c3\nd and \u03b2\nc < 1 + \u03c3 log(1/\u03b4) .\nThe bound on \u03b2 lets assume henceforth that \u03b2 is O(1) for any reasonable choices of \u03c3 and \u03b4. In our analysis We describe explicitly dependencies on \u2016X \u2016 for clarification and states the main results with \u03b2 = O(1).\nThe main result of the paper is states in the following theorem.\nTheorem 5 (Lasso\u2019s Smoothed Complexity). Suppose assumption 3 holds for arbitrary n, d \u2208 Z with n \u2265 d and \u03c3 \u2208 (0,1]. Then, with a probability of at least 1 \u2212 \u03b4 (over the random selection of G), the complexity of the Lasso satisfies,\n|P | c< n1.1 (\nd \u03b4\u03c3\n)6 ."}, {"heading": "3. Main Lemmas", "text": "To prove the main theorem, we introduce several properties of X, y,w[\u03bb] and u[\u03bb] that are critical in the analysis of |P |. We then use the smoothness assumption to bound these properties.\nDefinition 1 (Lipschitzness). Let wi[\u03bb] and ui[\u03bb] be the value of the i\u2019th coordinate of w[\u03bb] and u[\u03bb] respectively for i \u2208 [d]. The coordinate-wise Lipschitz parameters of w and u are defined as,\nLw = max i\u2208[d] sup \u03bb>0 \u2202\u03bbwi[\u03bb] , Lu = max i\u2208[d] sup \u03bb>0 \u2202\u03bbui[\u03bb] . By definition, Lw and Lu characterize how much each coordinate of w[\u03bb] and u[\u03bb] can change as we vary the value of \u03bb. We later use the smoothness assumption to show that Lw and Lu are polynomially small. This implies that w[\u03bb] and u[\u03bb] would not change too fast with \u03bb. However, Lipschitzness by itself does not give us a bound on |P | since w[\u03bb] can still oscillate around zero and induce an excessively large number of linear segments. Therefore, we also need the following property which defines the restricted distance between the column space of X and y. Definition 2 (Subspace distance). For any s, \u03b4 > 0, let \u03b3s denote the largest value such that,\nPr [ \u2203v \u2208 Rd\u2212s s.t. XS\u0304v \u2212 y 2 \u2264 \u03b3s] \u2264 \u03b4 , for all S \u2282 [d] of size s.\nThis definition quantifies the distance of y to a subspace spanned by s \u2264 d columns of X. Since y \u2208 Rn, n \u2265 d, and X is smooth, it can be shown that y cannot be too close to the subspace spanned by XS\u0304 . That is, \u03b3s is inversely proportional to a polynomial in n, d,1/\u03c3. We interchangeably use in the following the original matrix X with v \u2208 Rd s.t. vi = 0 for i \u2208 S and XS\u0304 with v \u2208 Rd\u2212s. Using the above properties, we prove the following theorem.\nTheorem 6 (Exact Smooth Complexity). Let X satisfy Assumption 3. Then, for all s \u2208 [d] and \u03b4 > 0, with probability of at least 1 \u2212 \u03b4 the complexity of the Lasso satisfies,\n|P | c< 3s \u00a9\u00ab \u221a\nsnd ( Lw \u03b12 + Lu ) \u03b42\u03c3\u03b3s \u00aa\u00ae\u00ae\u00ac s s\u22121 .\nThe following lemma characterizes the (smoothed) values of Lw , Lu , and \u03b3s . Lemma 7. Let X satisfy Assumption 3. Then with probability of at least 1 \u2212 \u03b4 the following properties hold,\nLw,Lu c < \u221a d \u03b12 , \u03b3s c > \u03c3\u221a dn(d/\u03b4)2/s .\nApplying the bounds on Lw , Lu , \u03b3s , and \u03b1 to Theorem 6 while letting s be a sufficiently large constant, we can directly prove Theorem 5. In Section 5, we use the value of \u03b1 to bound Lw and Lu . In Section 6, we employ the smoothness of X to bound \u03b3s . Finally, in Section 7 we prove Theorem 6."}, {"heading": "4. Proof sketch", "text": "Since \u2016X\u20162 = O(1), there exists a constant \u03bbmax = \u2126(1) such that for \u03bb \u2265 \u03bbmax, w[\u03bb] is the zero vector. Thus, we can divide \u03bb \u2208 [0, \u03bbmax] into \u03bbmax/\u03bd intervals, each of size \u03bd for some (inversely polynomial) small \u03bd. We then show that within every interval the total number of linear segments exceeds a fixed polynomial number with exponentially small probability. The total number of linear segments follows by taking a union bound over all intervals. We need to specifically address the following two questions in the analysis.\nWhat if |P | within an intervals is excessively large?\nWe will show that when there are N linear segments in an interval, then there must be at least log3(N) many coordinates of w[\u03bb] , which we denote as the set S, that change their sign. Since \u03bd is small and w[\u03bb] is a Lipschitz function in \u03bb, we know that those coordinates of w[\u03bb] must be close to zero. Therefore, we can show that w[\u03bb] is close to the optimal solution, v[\u03bb], of the Lasso problem when the entries of coordinates in S are constrained to be exactly zero,\nv[\u03bb] = argmin w\u2208Rd 1 2 \u2016Xw \u2212 y\u201622 + \u03bb\u2016w\u20161s.t.\u2200i \u2208 S : wi = 0\n(2)\nWhat if w[\u03bb] oscillates excessively around v[\u03bb]?\nFrom the optimality condition of u[\u03bb] and the smoothness of u[\u03bb], we also know that the coordinates in S of u[\u03bb]must be close to either \u2212\u03bb or \u03bb. Thus, uS[\u03bb] def = X>S(Xw[\u03bb] \u2212 y) is close to a vector on the scaled hypercube {\u2212\u03bb,\u03bb} |S | . On the other hand, if w[\u03bb] is close to v[\u03bb], we know that X>S(X v[\u03bb] \u2212 y) is also close to a vector in {\u2212\u03bb,\u03bb}\n|S | . However, v[\u03bb] does not depend on XS by construction. Therefore, the residual Xv[\u03bb] \u2212 y also does not depend on XS . Thus, using the randomness etched in XS we can now show that X>S(Xv[\u03bb] \u2212 y) is close to a vector in {\u2212\u03bb,\u03bb}\n|S | with probability which is exponentially small in the size of S. Therefore, we know that w.h.p. the total number of linear segments in this interval is unlikely to be large."}, {"heading": "5. Bounding Lw and Lu", "text": "Recall that we denote the smallest singular value of X by \u03b1. We first show the following lemma regarding pertur-\nbations of strongly convex functions. Also recall that a second-order smooth function f : Rd \u2192 R is \u03b12-strongly convex if \u22072 f (x) \u2265 \u03b12 for all x \u2208 Rd and is L-Lipschitz if \u2016\u2207 f (x)\u20162 \u2264 L for all x \u2208 Rd . Lemma 8 (Perturbation of strongly convex functions I). Let f : Rd \u2192 R be an non-negative, \u03b12-strongly convex function. Let g : Rd \u2192 R be a L-Lipschitz non-negative convex function . For any \u03bb \u2265 0, let z[\u03bb] be the minimizer of f (z) + \u03bbg(z), then we have, dz[\u03bb]d\u03bb 2 \u2264 L\u03b12 . Proof of Lemma 8. For any \u03c4 \u2265 0 and \u03bb \u2265 0, let us abbreviate z = z[\u03bb] and denote \u03b5 = z[\u03bb + \u03c4] \u2212 z. From \u03b12-strong convexity of f at z and the optimality of z at \u03bb, we know that\n1 2 \u03b12\u2016\u03b5\u201622 + f (z) + \u03bbg(z) \u2264 f (z + \u03b5) + \u03bbg(z + \u03b5) . (3)\nMoreover, using the optimality of z + \u03b5 at \u03bb + \u03c4, we know that,\n1 2 \u03b12\u2016\u03b5\u201622 + f (z + \u03b5) + (\u03bb + \u03c4)g(z + \u03b5) \u2264 f (z) + (\u03bb + \u03c4)g(z) . (4)\nSumming Eqs. (3) and (4) and rearranging terms yields,\n\u03b12\u2016\u03b5\u201622 \u2264 \u03c4 (g(z) \u2212 g(z + \u03b5)) \u2264 \u03c4\u2016\u03b5\u20162L ,\nwhere the last inequality is due to the Lipschitzness assumption on g. Therefore, we get that \u2016\u03b5\u20162/\u03c4 \u2264 L/\u03b12. Letting \u03c4 \u2192 0+ completes the proof.\nUsing Lemma 8 with f (w) = 12 \u2016Xw\u2212y\u201622 and g(w) = \u2016w\u20161, we obtain Lipschitz properties for w[\u03bb] and u[\u03bb]. Since we assume that the minimum singular value of X is \u03b1 then f (w) is \u03b12-strongly convex. In addition, the norm of \u2207g(w) is clearly at most \u221a d. To simplify notation, when w[\u03bb] is not differentiable at a point \u03bb, we define dw[\u03bb]/d\u03bb = 0. Due to Lipschitzness and strong convexity all vectors in the subgradient set \u2202\u03bb w[\u03bb] include this particular choice for a subgradient. In summary we get the following corollary.\nCorollary 9 (Lipschitzness of w). For any \u03bb \u2265 0 it holds that, dw[\u03bb]d\u03bb 2 \u2264 \u221a d \u03b12 .\nSince by definition, u[\u03bb] = X>i (Xw[\u03bb] \u2212 y), we obtain a similar corollary for u.\nCorollary 10 (Lipschitzness of u). For any \u03bb \u2265 0 it holds that, du[\u03bb]d\u03bb 2 \u2264 \u2016X\u201622 \u221a d \u03b12 .\nRecall that \u2016X\u20162 = O(1), thus, from the above corollaries we get\nLw,Lu c < \u221a d/\u03b12 . (5)\nWe also use in the next section the following Lemma.\nLemma 11 (Perturbation of strongly convex functions II). Let f : Rd \u2192 R be an \u03b12-strongly convex function and g : Rd \u2192 R an L-Lipschitz convex function. Let z1 and z2 be the minimizers of f (z) and f (z)+ g(z), respectively, then\n\u2016z1 \u2212 z2\u20162 \u2264 L \u03b12 .\nProof of Lemma 11. Let \u03b5 = z2 \u2212 z1. From strong convexity of f and optimality of z1, we know that\n1 2 \u03b12\u2016\u03b5\u201622 + f (z1) \u2264 f (z2) . (6)\nDue to the optimality of z2 for f (z) + g(z) we get,\n1 2 \u03b12\u2016\u03b5\u201622 + f (z2) + g(z2) \u2264 f (z1) + g(z1) . (7)\nSumming Eq. (6) and (7) and rearranging terms gives,\n\u03b12\u2016\u03b5\u201622 \u2264 g(z1) \u2212 g(z2) \u2264 L\u2016\u03b5\u20162 .\nWe therefore get that \u2016\u03b5\u20162 \u2264 L\u03b12 ."}, {"heading": "6. Bounding \u03b3s", "text": "In this section we prove that Assumption 3 also yields a bound on \u03b3s . Lemma 12. Let y \u2208 Sn\u22121 be an arbitrary unit vector. Then for any s \u2208 {10, . . . , d} and a set S \u2282 [d] of size s, the following holds,\nPr [ \u2203v \u2208 Rd\u2212s s.t. XS\u0304v \u2212 y 2 c< \u03c3\u03b4 1s\u221adn ] \u2264 \u03b4 .\nFor brevity, we denote the event above by D\u03b3 where \u03b3 = \u2126 ( \u03c3\u03b4 1 s\u221a\ndn\n) .\nProof. The proof relies on the following simple fact about Gaussian random variables. For any unit vector u, scalar \u03c4 > 0, g \u2208 R, and i \u2208 [d] the following holds,\nPr [ |\u3008u,Xi\u3009 \u2212 g | \u2264 \u03c4 ] \u2264 e\u03c4 \u221a n\n\u03c3 , (8)\nwhere Xi is a (column) vector with elements distributed i.i.d according to N(0, \u03c32/n).\nLet us assume that there exists v \u2208 Rd\u2212s such that XS\u0304v \u2212 y 2 \u2264 \u03b3. From Lemma 4, we know that with probability of at least 1 \u2212 \u03b4, \u2016X\u20162 c < 1. Since we assume that \u2016y\u20162 = 1, it holds that\n\u03b3 \u2265 XS\u0304v \u2212 y 2 \u2265 \u2016y\u20162 \u2212 \u2016Xv\u20162 \u2265 1 \u2212 \u2016v\u20162 \u21d2 \u2016v\u20162 \u2265 1 \u2212 \u03b3 .\nSince \u03b3 is smaller than 1/2, there must exist a coordinate i for which |vi | \u2265 12\u221ad .\nNow, let us fix Gj to its observed values for all j , i. In addition, let us denote by U the subspace spanned by\n{(Xh)i} \u222a {Xj | \u2200 j , i, j \u2208 S} \u222a {y} .\nWe know that U is of dimension d \u2212 s + 1. Consider an orthonormal basis for {u1, \u00b7 \u00b7 \u00b7 ,us\u22121} \u2208 Rn for the subspace span({Xi}) \u2212 U. Suppose there exists v such that XS\u0304v \u2212 y 2 \u2264 \u03b3. Then, for every j \u2208 [s \u2212 1], multiplying by u j yields \u3008u j,Xi\u3009vi \u2264 \u03b3 \u21d2 |\u3008u j,Xi\u3009| \u2264 \u03b3|vi | \u2264 2\u221ad\u03b3 . Note that any pair j , j \u2032, the inner products \u3008u j,Xi\u3009 and \u3008u j\u2032,Xi\u3009 are independent. We now use (8) over all u j and get that D\u03b3 holds with probability of at most,(\n6 \u221a\ndn\u03b3 \u03c3\n)s .\nFinally, choosing \u03b3 = \u2126 ( \u03c3\u03b4 1 s\u221a\ndn\n) completes the proof."}, {"heading": "7. Coupling the solutions", "text": "In this section, we show that when the total number of linear segments in a small interval is excessively large, the optimal solution w[\u03bb] can be coupled with the optimal solution v[\u03bb] of the constrained Lasso problem of (2).\nSign changes. For a given fixed \u03bb0 > 0 and \u03bd > 0, let us denote by \u03b6(i) the number of times that the generalized sign of wi changes as \u03bb increases from \u03bb0 to \u03bb0 + \u03bd. Thus, the total number of linear segments in the interval [\u03bb0, \u03bb0 + \u03bd] is at least \u2211d i=1 \u03b6(i). We prove the following lemmas related to the sign changes. Lemma 13 (Number of sign changes). For any integer N > 0, any \u03bb0, \u03bd > 0, if \u2211d i=1 \u03b6(i) \u2265 N , then there exists at least log3(N) many indices j \u2208 [d] such that \u03b6( j) \u2265 1.\nProof. According to Lemma 2, each linear segment is associated with a unique sign pattern in {\u22121,0,1}d . Since there are N segments, the pigeon hole principle implies that there must exist at least log3(N) many coordinates of w[\u03bb] that change their sign in this interval.\nFrom the Lipschitzness of w[\u03bb] and u[\u03bb], we also obtain the following lemma. Lemma 14 (Sign change\u21d2 small weight). For i \u2208 [d], if \u03b6(i) \u2265 1, then following properties hold: |wi[\u03bb0]| \u2264 Lw\u03bd and | |ui[\u03bb0]| \u2212 \u03bb0 | \u2264 (Lu + 1)\u03bd.\nProof. Since \u03b6(i) \u2265 1, we know that there exists \u03bb \u2208 [\u03bb0, \u03bb0 + \u03bd] such that wi[\u03bb] = 0 and |ui[\u03bb]| = \u03bb. Using Lipschitzness of w[\u03bb]we get |wi[\u03bb0]\u2212wi[\u03bb]| = |wi[\u03bb0]| \u2264 Lw\u03bd. For ui[\u03bb0] we have,\n| |ui[\u03bb0]| \u2212 \u03bb0 | \u2264 | |ui[\u03bb0]| \u2212 |ui[\u03bb]| | + | |ui[\u03bb]| \u2212 \u03bb | + |\u03bb \u2212 \u03bb0 | \u2264 (Lu + 1)\u03bd\nwhich concludes the proof.\nWe use L\u0303u in the sequel as a shorthand for Lu + 1. Based on the two lemmas above we readily get the following corollary. Corollary 15. For any integer N > 0 and \u03bd, \u03bb0 \u2265 0, if\u2211\ni \u03b6(i) \u2265 N , then there exists a subset S \u2286 [d] of cardinality at least log3(N) such that \u2200i \u2208 S, wi[\u03bb0] \u2264 Lw\u03bd and |ui[\u03bb0]| \u2212 \u03bb0 \u2264 L\u0303u\u03bd . Rare events. Let S be defined as in Corollary 15, we next show that if the size of S is too large, then certain rare couplings would take place. Thus, the size of S is likely to be small with high probability. Throughout the rest of the paper we overload notation and denote by XS \u2208 Rn\u00d7d the matrix where each column Xi , for i < S, is replaced with the zero vector. The matrix XS\u0304 is defined analogously. Note that by definition, XS + XS\u0304 = X. Lemma 16 (Size of S). For any fixed \u03bb0, \u03bd > 0, and a set S \u2286 [d], let XS\u0304 \u2208 Rn\u00d7d be defined as above. Let vS\u0304[\u03bb0] be the minimizer,\nvS\u0304[\u03bb0] = arg min v\u2208Rd 1 2 \u2016XS\u0304 v \u2212 y\u201622 + \u03bb0\u2016v\u20161\nAssume that the properties of Corollary 15 hold for a set S. Then, for every j \u2208 S the following inequality holds, X>j (XS\u0304 vS\u0304[\u03bb0] \u2212 y) \u2212 \u03bb0\n\u2264 2 \u221a |S| \u2016X\u201622\n( Lw \u2016X\u201622\u03bd\n\u03b12 + L\u0303u\u03bd\n) .\nWe refer to this event as E(\u03c4)S with parameter \u03c4 = 2 \u221a |S|\u2016X\u201622 ( Lw \u2016X\u201622\u03bd \u03b12 + L\u0303u\u03bd ) .\nProof. We know that the i\u2019th coordinate of vS\u0304[\u03bb0] is zero for all i \u2208 S. Therefore, we need to focus solely on the set of vectors v which are in\nK def= {v \u2208 Rd | \u2200i \u2208 S, vi = 0} .\nSince by definition XS = X\u2212XS\u0304 we can rewrite the original objective as,\n1 2 XS\u0304 w + XS w \u2212 y 22 + \u03bb0\u2016w\u20161 .\nLet wS\u0304[\u03bb0] \u2208 Rd be a vector whose ith coordinate is the ith coordinate of w[\u03bb0] for i < S and is zero otherwise and let wS[\u03bb0] = w \u2212 wS\u0304[\u03bb0]. From the optimality of w[\u03bb0], we know that wS\u0304[\u03bb0] is the minimizer of\nh(w) = 1 2 XS\u0304 w+XS wS[\u03bb0] \u2212 y 22+\u03bb0\u2016w\u20161 s.t. w \u2208 K .\nLet g(w) def= 12 XS\u0304 w \u2212 y 22 +\u03bb0\u2016w\u20161. Expanding terms we get that for every w \u2208 K,\nh(w) \u2212 g(w) = 1 2 \u2016XS wS[\u03bb0]\u201622 \u2212 \u3008XS wS[\u03bb0], y\u3009 + \u2329 XSwS[\u03bb0],XS\u0304 w \u232a ,\nand the gradient of h(w) \u2212 g(w) satisfies \u2207 ((h(w) \u2212 g(w)) 2 \u2264 X>S\u0304 XS wS[\u03bb0] 2 \u2264 \u2016X\u201622\nwS[\u03bb0] 2 . From our assumption that Corollary 15 holds for S, we know that \u2016wS[\u03bb0]\u2016\u221e \u2264 Lw\u03bd which in turn implies that \u2016wS[\u03bb0]\u20162 \u2264 \u221a |S|Lw\u03bd.\nWe can now apply Lemma 11 w.r.t g(w) and h(w) \u2212 g(w) to conclude that vS\u0304[\u03bb0] \u2212 wS\u0304[\u03bb0] 2 \u2264 \u2016X\u201622 \u221a|S|Lw\u03bd\u03b12 . Therefore, for every j \u2208 S the following holds X>j (XS\u0304 vS\u0304[\u03bb0] \u2212 y) \u2212 \u03bb0\n\u2264 X>j (XS\u0304 wS\u0304[\u03bb0] \u2212 y) \u2212 \u03bb0 + \u2016X\u201642 \u221a|S|Lw\u03bd\u03b12 \u2264\nX>j (XS\u0304 w[\u03bb0] \u2212 y) \u2212 \u03bb0 + \u2016X\u201622 (\u221a |S| \u2016X\u201622 Lw \u03bd\n\u03b12 +\n\u221a |S| \u2016wS[\u03bb0]\u2016\u221e ) \u2264 2 \u221a |S| \u2016X\u201622 ( Lw \u2016X\u201622 \u03bd\n\u03b12 + L\u0303u\u03bd\n) ,\nwhich concludes the proof.\nUsing again that \u2016X\u20162 c < 1 we obtain\n\u03c4 c < \u03bd \u221a |S| ( Lw \u03b12 + L\u0303u ) . (9)\nThis means that of gradient of objective scales as the product of the root of the size of S and the length of the interval \u03bd.\nBounding the probability of bad events Next we show that for every fixed set S of sufficiently large cardinality and sufficiently small \u03bd, E(\u03c4)S holds with very small probability if X satisfies the smoothness assumption.\nLemma 17 (Smoothing). For any fixed \u03bb0, \u03bd > 0, \u03c4 \u2265 0, and S \u2286 [d], let us decompose G into G = GS\u0304 +GS as in Lemma 16. Then, the following inequality holds,\nPr GS\n[ E(\u03c4)S GS\u0304 ] \u2264 ( e\u03c4\u221an \u03c3 XS\u0304 vS\u0304[\u03bb0] \u2212 y 2 ) |S | .\nProof. Consider the vector vS\u0304[\u03bb0] defined as in Lemma 16. We know that vS\u0304[\u03bb0] depends only on GS\u0304 but not on GS . Therefore, for any fixed G0, by conditioning on GS\u0304 = G0, we get that for all j \u2208 S\nPr GS [ X>j (XS\u0304vS\u0304[\u03bb0] \u2212 y) \u2212 \u03bb0 \u2264 \u03c4] = Pr\nGS [ ((Xh)j +Gj )> (XS\u0304vS\u0304[\u03bb0] \u2212 y) \u2212 \u03bb0 \u2264 \u03c4] = Pr\nG j [ ((Xh)j +Gj )> (XS\u0304vS\u0304[\u03bb0] \u2212 y) \u2212 \u03bb0 \u2264 \u03c4] \u2264 e\u03c4 \u221a n\n\u03c3 (XS\u0304vS\u0304[\u03bb0] \u2212 y) 2 .\nSince the inequality holds for all j \u2208 S and any G0 the proof is completed."}, {"heading": "8. Proof of Theorem 6", "text": "Recall that we assume that the target vector is of unit norm \u2016y\u20162 = 1. We slightly overload notation and denote by \u03b1(X) the smallest right singular value of X. From the optimality of w[\u03bb], we know that\n1 2 Xw[\u03bb] \u2212 y 22 + \u03bb w[\u03bb] 1 \u2264 12 \u2016y\u201622 \u2264 12 ,\nwhich implies that \u2016Xw[\u03bb] \u2212 y\u20162 \u2264 1. From necessary conditions for optimality we also get,\nX>(Xw[\u03bb] \u2212 y) = u[\u03bb] .\nThus, we have\n\u2016u[\u03bb]\u20162 = X>(Xw[\u03bb] \u2212 y) 2 \u2264 \u2016X\u20162 = O(1) .\nTherefore, there exists a constant \u03bbmax such that implies that w[\u03bb] = 0 for \u03bb \u2265 \u03bbmax. We next employ the randomness of G. Consider a fixed \u03b10 and examine the event that there exists one set S of size at\nleast s such that E(\u03c4)S is true, then it holds that, Pr [( \u2203S : |S| = s,E(\u03c4)S ) \u2229 D\u03b3 \u2229 \u03b1(X) \u2265 \u03b10 ] \u2264\n\u2211 S0\u2286[d], |S0 |=s Pr [ ( E(\u03c4)S ,S = S0 ) \u2229 D\u03b3 \u2229 \u03b1(X) \u2265 \u03b10 ] \u2264\n\u2211 S0\u2286[d], |S0 |=s Pr [ E(\u03c4)S0 \u2229 D\u03b3 \u2229 \u03b1(X) \u2265 \u03b10 ] \u2264 ( d s ) ( e\u03c4 \u221a n \u03c3\u03b3 )s \u2264 ( e\u03c4d \u221a n \u03c3\u03b3 )s ,\nwhere we used the definition of D\u03b3 and the fact that \u03b1 def = \u03b1(X) \u2265 \u03b10 to obtain the last inequality. We now set\n\u03c4 = O ( (\u03b4\u03bd)1/s \u03c3\u03b3\nd \u221a n\n) ,\nwhich in turn implies that (see (9)), \u03bd1\u22121/s c >\n\u03b41/s\u03c3\u03b3 d \u221a ns(Lw/\u03b120+L\u0303u)\n\u21d2 \u03bd c> (\n\u03b41/s\u03c3\u03b3 d \u221a ns(Lw/\u03b120+L\u0303u)\n) s s\u22121\n. and\nobtain Pr [( \u2203S : |S| = s,E(\u03c4)S holds ) \u2229 D\u03b3 \u2229 \u03b1(X) \u2265 \u03b10 ] \u2264 \u03b4\u03bd .\nSince we have at most 1/\u03bd many intervals, taking union bound over all linear segments we get that\nPr [( N(P) \u2265 3 s\n\u03bd\n) \u2229 D\u03b3 \u2229 (\u03b1(X) \u2265 \u03b10) ] \u2264 \u03b4\nFinally, for properly chosen \u03b3 and \u03b10 we also obtain Pr[\u00acD\u03b3 \u222a (\u03b1(X) < \u03b10)] \u2264 2\u03b4 which completes the proof.\nTo recap, there exists a universal constant c such that for all s \u2208 [d] the complexity of the Lasso path is bounded above by,\nc 3s \u00a9\u00ab \u221a\nsnd ( Lw \u03b12 + Lu ) \u03b42\u03c3\u03b3s \u00aa\u00ae\u00ae\u00ac s s\u22121 .\nWe now use the bounds on Lw , Lu , and \u03b1, yielding,\nLw \u03b12 + Lu c < d4.5 \u03b44\u03c34 , \u03b3s c < \u03c3\u03b4 1 s \u221a dn\n\u21d2 |P| c< 3s ( \u221a s n d6\n\u03b46+ 1 s \u03c36\n) s s\u22121\n.\nBy choosing s = O ( log ( nd \u03b4\u03c3 )) we get that\n|P | c< n1.1 (\nd \u03b4\u03c3\n)6 .\nFigure 1. Path complexity as a function of dimension for different levels of smoothing. The theoretical non-smoothed (\u03c3 = 0) complexity is exponential in the size of the problem.\nFigure 2. Path complexity in a regression task that predicts the value of a pixel from its neighboring pixels using the MNIST dataset."}, {"heading": "9. Experiments", "text": "We performed two sets of experiments. Our path following implementation uses Python with Float128 for high accuracy computations. In the first set of experiments, we start with the exponential complexity construction for Xh \u2208 Rd\u00d7d from (Mairal & Yu, 2012), which has (3d + 1)/2 many line segments. We artificially added to each entry of Xh i.i.d. Gaussian noise of mean zero and variance \u03c32. In this setting, the largest value of the entries of Xh is 1 and y is an all-one vector. We show the effect of dimension d and smoothing \u03c3 on N(P). We report the average over 100 random choices for smoothing per Xh. As can be seen from the figure below, even for a tiny amount of entry-wise noisy of 10\u221210, the number of linear segments dramatically shrinks. We also include a full table of results, where 1/SNR denotes \u2212 log10(\u03c3).\nFor the next experiment we use the MNIST data set. We randomly selected n = 1000 images from the data set. We constructed the data matrix X \u2208 Rn\u00d7d2 such that the i\u2019th row of X is a randomly chosen patch from the i\u2019th image of size d \u00d7 d. We cast the center pixel of patch i as the target yi and discard the pixel from X. Thus, the regression task amounts to predicting the center pixel yi using its surrounding pixels X(i). We plot the relation between the size of the patch and the path complexity N(P). Each point in the graph is the average over 100 random samples of patches and images. As can be seen, when the amount of noise in the data is fixed and governed by the data acquisition process (the minimum pixel value is 0 and maximum is 255 for MNIST), the number of linear segments increases barely faster than linearly in the dimension."}, {"heading": "10. Conclusions", "text": "We proved that the smoothed complexity of the Lasso\u2019s regularization path is pragmatically polynomial in the input\nsize. Our analysis contrasts worst case settings for which the Lasso\u2019s complexity is known to be exponential. To illustrate the key idea, we provided analysis when smoothing each entry in the data matrix by adding small amount of Gaussian noise. Although not presented here, our analysis carries over to settings in which the smoothing is performed using other distributions which can be sub-Gaussian, sub-exponential, or even non i.i.d. so long as the rows of G are statistically independent.\nThe nature of smoothed analysis usually imposes a large polynomial factor (Spielman & Teng, 2001), as we do not make any additional assumptions on the hidden matrix Xh. However, we believe that the polynomial degree in our results can be further reduced. For example, in our proof, we used the fact that the condition number of X is \u223c 1d , which is close to being ill-conditioned. For well-conditioned matrices the polynomial bound can be improved to O(d2.1n1.1). This reduction is also valid in settings when n \u2265 2d (see e.g. (Rudelson & Vershynin, 2010) for different behaviors of the condition number of Gaussian random matrices for n = d and n \u2265 2d). Furthermore, when n \u2265 2d, we can also improve \u03b3s to \u2126(1), hence our polynomial dependency can be further reduced to O(d1.6n0.6). A final improvement may stem from Lw and Lu . We actually proved that dw[\u03bb]d\u03bb 2 = O (\u221ad\u03b12 ) ,while we only need to use the infinity norm\ndw[\u03bb]d\u03bb \u221e. We leave these improvements and further generalizations to future research."}], "year": 2018, "references": [{"title": "K-means has polynomial smoothed complexity", "authors": ["Arthur", "David", "Manthey", "Bodo", "R\u00f6glin", "Heiko"], "venue": "In Foundations of Computer Science,", "year": 2009}, {"title": "Smoothed analysis of the perceptron algorithm for linear programming", "authors": ["Blum", "Avrim", "Dunagan", "John"], "venue": "In Proceedings of the thirteenth annual ACM-SIAM symposium on Discrete algorithms,", "year": 2002}, {"title": "Entire relaxation path for maximum entropy problems", "authors": ["Dubiner", "Moshe", "Singer", "Yoram"], "venue": "In Proc. of the 2011 Conference on Empirical Methods in Natural Language Processing,", "year": 2011}, {"title": "Least angle regression", "authors": ["Efron", "Bradley", "Hastie", "Trevor", "Johnstone", "Iain", "Tibshirani", "Robert"], "venue": "The Annals of statistics,", "year": 2004}, {"title": "Approximating parameterized convex optimization problems", "authors": ["Giesen", "Joachim", "Jaggi", "Martin", "Laue", "S\u00f6ren"], "venue": "In European Symposium on Algorithms,", "year": 2010}, {"title": "Complexity analysis of the lasso regularization path", "authors": ["Mairal", "Julien", "Yu", "Bin"], "venue": "In Proceedings of the 29th International Coference on International Conference on Machine Learning,", "year": 2012}, {"title": "A new approach to variable selection in least squares problems", "authors": ["Osborne", "Michael R", "Presnell", "Brett", "Turlach", "Berwin A"], "venue": "IMA Journal of Numerical Analysis,", "year": 2000}, {"title": "Piecewise linear regularized solution paths", "authors": ["Rosset", "Saharon", "Zhu", "Ji"], "venue": "The Annals of Statistics,", "year": 2007}, {"title": "Non-asymptotic theory of random matrices: extreme singular values", "authors": ["Rudelson", "Mark", "Vershynin", "Roman"], "venue": "In Proceedings of the International Congress of Mathematicians 2010 (ICM 2010) (In 4 Volumes) Vol. I: Plenary Lectures and Ceremonies Vols. II\u2013IV: Invited Lectures,", "year": 2010}, {"title": "Smoothed analysis of the condition numbers and growth factors of matrices", "authors": ["Sankar", "Arvind", "Spielman", "Daniel A", "Teng", "Shang-Hua"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "year": 2006}, {"title": "Smoothed analysis of algorithms: Why the simplex algorithm usually takes polynomial time", "authors": ["Spielman", "Daniel", "Teng", "Shang-Hua"], "venue": "In Proceedings of the thirty-third annual ACM symposium on Theory of computing,", "year": 2001}, {"title": "Smoothed analysis: an attempt to explain the behavior of algorithms in practice", "authors": ["Spielman", "Daniel A", "Teng", "Shang-Hua"], "venue": "Communications of the ACM,", "year": 2009}, {"title": "Regression shrinkage and selection via the lasso", "authors": ["Tibshirani", "Robert"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), pp", "year": 1996}, {"title": "Degrees of freedom in lasso problems", "authors": ["Tibshirani", "Ryan J", "Taylor", "Jonathan"], "venue": "The Annals of Statistics,", "year": 2012}], "id": "SP:8c7070616aa82b461a709bbc148a2fdbca44d6ec", "authors": [{"name": "Yuanzhi Li", "affiliations": []}, {"name": "Yoram Singer", "affiliations": []}], "abstractText": "We study the complexity of the entire regularization path for least squares regression with 1-norm penalty, known as the Lasso. Every regression parameter in the Lasso changes linearly as a function of the regularization value. The number of changes is regarded as the Lasso\u2019s complexity. Experimental results using exact path following exhibit polynomial complexity of the Lasso in the problem size. Alas, the path complexity of the Lasso on artificially designed regression problems is exponential. We use smoothed analysis as a mechanism for bridging the gap between worst case settings and the de facto low complexity. Our analysis assumes that the observed data has a tiny amount of intrinsic noise. We then prove that the Lasso\u2019s complexity is polynomial in the problem size.", "title": "The Well-Tempered Lasso"}